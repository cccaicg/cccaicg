{"docstring": "\"\"\"Returns the datasteream of a scaled image.\"\"\"\n", "func_signal": "def scaled_imagedata(imageid, size='150x150'):\n", "code": "url = scaled_imageurl(imageid, size)\nhttp = httplib2.Http()\nresponse, content = http.request(url, 'GET')\nif str(response.status) == '200':\n    return content\nelse:\n    return None", "path": "huimages\\__init__.py", "repo_name": "hudora/huImages", "stars": 64, "license": "other", "language": "python", "size": 189}
{"docstring": "\"\"\"Deletes an image and all associated data.\"\"\"\n\n# delete in CouchDB\n", "func_signal": "def delete_image(imageid):\n", "code": "db = _setup_couchdb()\ntry:\n    doc = db[imageid]\n    db.delete(doc)\nexcept couchdb.client.ResourceNotFound:\n    pass\n# Push data into S3 if needed\nconn = boto.connect_s3()\ns3bucket = conn.get_bucket(S3BUCKET, validate=False)\nk = s3bucket.get_key(imageid)\nif k:\n    k.delete()", "path": "huimages\\__init__.py", "repo_name": "hudora/huImages", "stars": 64, "license": "other", "language": "python", "size": 189}
{"docstring": "\"\"\"Get a connection handler to the CouchDB Database, creating it when needed.\"\"\"\n", "func_signal": "def _setup_couchdb():\n", "code": "server = couchdb.client.Server(COUCHSERVER)\nif COUCHDB_NAME in server:\n    return server[COUCHDB_NAME]\nelse:\n    return server.create(COUCHDB_NAME)", "path": "huimages\\__init__.py", "repo_name": "hudora/huImages", "stars": 64, "license": "other", "language": "python", "size": 189}
{"docstring": "\"\"\"If there is a Problem with an Image, mark it as broken (deleted) in the Database.\"\"\"\n", "func_signal": "def mark_broken(doc_id):\n", "code": "db = couchdb.client.Server(COUCHSERVER)[COUCHDB_NAME]\ndoc = db[doc_id]\ndoc['deleted'] = True\ndb[doc_id] = doc", "path": "huimages\\server.py", "repo_name": "hudora/huImages", "stars": 64, "license": "other", "language": "python", "size": 189}
{"docstring": "\"\"\"Executes imageserver() returning a 500 status code on an exception.\"\"\"\n", "func_signal": "def save_imagserver(environ, start_response):\n", "code": "try:\n    return imagserver(environ, start_response)\nexcept:\n    raise\n    try:\n        start_response('500 OK', [('Content-Type', 'text/plain')])\n    except:\n        pass\n    return ['Error']", "path": "huimages\\server.py", "repo_name": "hudora/huImages", "stars": 64, "license": "other", "language": "python", "size": 189}
{"docstring": "\"\"\"Get a dictionary describing an Image.\"\"\"\n# includes super simple cache\n", "func_signal": "def get_imagedoc(imageid):\n", "code": "global __imagedoc_cache\ndoc = __imagedoc_cache.get(imageid, None)\nif doc:\n    return doc\nif len(__imagedoc_cache) > 5:\n    __imagedoc_cache = {}\ntry:\n    db = _setup_couchdb()\n    doc = db.get(imageid, {})\n    __imagedoc_cache[imageid] = doc\nexcept IOError:\n    # CouchDB not available\n    doc = None\nreturn doc", "path": "huimages\\__init__.py", "repo_name": "hudora/huImages", "stars": 64, "license": "other", "language": "python", "size": 189}
{"docstring": "\"\"\"Save an image title.\"\"\"\n", "func_signal": "def set_title(imageid, newtitle):\n", "code": "db = _setup_couchdb()\ndoc = get_imagedoc(imageid)\nif newtitle and newtitle not in doc.get('title', []):\n    doc.setdefault('title', []).append(newtitle)\ndb[imageid] = doc", "path": "huimages\\__init__.py", "repo_name": "hudora/huImages", "stars": 64, "license": "other", "language": "python", "size": 189}
{"docstring": "\"\"\"Creates an XHTML tag for an Image scaled to <size>.\n\nAdditional keyword arguments are added as attributes to  the <img> tag.\n\n>>> scaled_tag('0eadsaf', alt='neu')\n'<img src=\"http://images.hudora.de/477x600/0eadsaf.jpeg\" width=\"328\" height=\"600\" alt=\"neu\"/>'\n\nIf you don't give an alt tag the system tries to generate one based on image title or\nimage file name. If the environment variable HUIMAGESALTADDITION is set, that text is\nappended to the generated alt-text. This might be helpful for SEO purposes.\n\"\"\"\n", "func_signal": "def scaled_tag(imageid, size='150x150', *args, **kwargs):\n", "code": "ret = ['<img src=\"%s\"' % cgi.escape(scaled_imageurl(imageid, size), True)]\nwidth, height = scaled_dimensions(imageid, size)\nif width and height:\n    ret.append('width=\"%d\" height=\"%d\"' % (width, height))\nret.extend(args)\nfor key, val in list(kwargs.items()):\n    ret.append('%s=\"%s\"' % (cgi.escape(key, True), cgi.escape(val, True)))\nif 'alt' not in kwargs:\n    doc = get_imagedoc(imageid)\n    if doc and 'title' in doc:\n        # use the newest title as alt text\n        ret.append('alt=\"%s%s\"' % (cgi.escape(doc['title'][-1]),\n                                   cgi.escape(os.environ.get('HUIMAGESALTADDITION', ''))))\n    elif doc and '_attachments' in doc and doc['_attachments']:\n        ret.append('alt=\"%s%s\"' % (cgi.escape(doc['_attachments'].keys()[-1].replace('_', ' ')),\n                                   cgi.escape(os.environ.get('HUIMAGESALTADDITION', ''))))\n    else:\n        ret.append('alt=\"%s\"' % cgi.escape(os.environ.get('HUIMAGESALTADDITION', '').strip()))\nret.append('/>')\nreturn ' '.join(ret)", "path": "huimages\\__init__.py", "repo_name": "hudora/huImages", "stars": 64, "license": "other", "language": "python", "size": 189}
{"docstring": "\"\"\"Get the URL where a scaled version of the Image can be accessed.\"\"\"\n", "func_signal": "def scaled_imageurl(imageid, size='150x150'):\n", "code": "doc = get_imagedoc(imageid)\nfilename = imageid + '.jpeg'\nif doc:\n    # generate an url which contains a reference to the title of filename\n    title = ''\n    if 'title' in doc:\n        title = doc['title'][0].replace(' ', '-')\n    elif doc and '_attachments' in doc and doc['_attachments']:\n        title = doc['_attachments'].keys()[-1].replace(' ', '-')\n        if title.lower().endswith('.jpg'):\n            title = title[:-4]\n        if title.lower().endswith('.jpeg'):\n            title = title[:-5]\n    title = cgi.escape(re.sub(u'([^\\w_=,:-])+', '_', title))\n    if title:\n        filename = \"%s/%s.jpeg\" % (imageid, title)\nreturn urlparse.urljoin(IMAGESERVERURL, os.path.join(_sizes.get(size, size), filename))", "path": "huimages\\__init__.py", "repo_name": "hudora/huImages", "stars": 64, "license": "other", "language": "python", "size": 189}
{"docstring": "\"\"\"\nThis will crop the largest block out of the middle of an image\nthat has the same aspect ratio as the given bounding box. No\nblank space will be in the thumbnail, but the image isn't fully\nvisible due to croping.\n\"\"\"\n# origially from\n# http://simon.bofh.ms/cgi-bin/trac-django-projects.cgi/file/stuff/branches/magic-removal/image.py\n", "func_signal": "def _crop_image(width, height, image):\n", "code": "width, height = int(width), int(height)\nlfactor = 1\n(xsize, ysize) = image.size\nif xsize > width and ysize > height:\n    lfactorx = float(width) / float(xsize)\n    lfactory = float(height) / float(ysize)\n    lfactor = max(lfactorx, lfactory)\nnewx = int(float(xsize) * lfactor)\nnewy = int(float(ysize) * lfactor)\nres = image.resize((newx, newy), Image.ANTIALIAS)\nleftx = 0\nlefty = 0\nrightx = newx\nrighty = newy\nif newx > width:\n    leftx += (newx - width) / 2\n    rightx -= (newx - width) / 2\nelif newy > height:\n    lefty += (newy - height) / 2\n    righty -= (newy - height) / 2\nres = res.crop((leftx, lefty, rightx, righty))\nreturn res", "path": "huimages\\server.py", "repo_name": "hudora/huImages", "stars": 64, "license": "other", "language": "python", "size": 189}
{"docstring": "\"\"\"Get the 'previous' ImageID.\"\"\"\n", "func_signal": "def get_previous_imageid(imageid):\n", "code": "db = _setup_couchdb()\nreturn [x.id for x in db.view('_all_docs', startkey=imageid, limit=2, descending=True)][-1]", "path": "huimages\\__init__.py", "repo_name": "hudora/huImages", "stars": 64, "license": "other", "language": "python", "size": 189}
{"docstring": "\"\"\"Returns the dimensions of an image after scaling.\"\"\"\n", "func_signal": "def scaled_dimensions(imageid, size='150x150'):\n", "code": "size = _sizes.get(size, size)\nwidth, height = size.split('x')\nif size.endswith('!'):\n    return (int(width), int(height.rstrip('!')))\n# get current is_width and is_height\ntry:\n    doc = get_imagedoc(imageid)\n    return _scale(width, height, doc['width'], doc['height'])\nexcept:\n    return (None, None)", "path": "huimages\\__init__.py", "repo_name": "hudora/huImages", "stars": 64, "license": "other", "language": "python", "size": 189}
{"docstring": "\"\"\"Returns a random (valid) ImageID.\"\"\"\n", "func_signal": "def get_random_imageid():\n", "code": "db = _setup_couchdb()\nstartkey = base64.b32encode(hashlib.sha1(str(random.random())).digest()).rstrip('=')\nreturn [x.id for x in db.view('all/without_deleted_and_automatic', startkey=startkey, limit=1)][0]", "path": "huimages\\__init__.py", "repo_name": "hudora/huImages", "stars": 64, "license": "other", "language": "python", "size": 189}
{"docstring": "\"\"\"\nThis function will scale an image to a given bounding box. Image\naspect ratios will be conserved and so there might be blank space\nat two sides of the image if the ratio isn't identical to that of\nthe bounding box.\n\nReturns the size of the final image.\n\"\"\"\n# from http://simon.bofh.ms/cgi-bin/trac-django-projects.cgi/file/stuff/branches/magic-removal/image.py\n", "func_signal": "def _scale(want_width, want_height, is_width, is_height):\n", "code": "lfactor = 1\nwant_width, want_height = int(want_width), int(want_height)\nif is_width > want_width and is_height > want_height:\n    lfactorx = float(want_width) / float(is_width)\n    lfactory = float(want_height) / float(is_height)\n    lfactor = min(lfactorx, lfactory)\nelif is_width > want_width:\n    lfactor = float(want_width) / float(is_width)\nelif is_height > want_height:\n    lfactor = float(want_height) / float(is_height)\nreturn (int(float(is_width) * lfactor), int(float(is_height) * lfactor))", "path": "huimages\\__init__.py", "repo_name": "hudora/huImages", "stars": 64, "license": "other", "language": "python", "size": 189}
{"docstring": "\"\"\"Get the length in bytes of an unmodified image.\"\"\"\n", "func_signal": "def get_length(imageid):\n", "code": "doc = get_imagedoc(imageid)\nattachment = doc['_attachments'][list(doc['_attachments'].keys())[0]]\nreturn attachment['length']", "path": "huimages\\__init__.py", "repo_name": "hudora/huImages", "stars": 64, "license": "other", "language": "python", "size": 189}
{"docstring": "\"\"\"Get the 'next' ImageID.\"\"\"\n", "func_signal": "def get_next_imageid(imageid):\n", "code": "db = _setup_couchdb()\nreturn [x.id for x in db.view('_all_docs', startkey=imageid, limit=2)][-1]", "path": "huimages\\__init__.py", "repo_name": "hudora/huImages", "stars": 64, "license": "other", "language": "python", "size": 189}
{"docstring": "\"\"\"Returns a filehandle for the unscaled file related to doc_id.\"\"\"\n\n", "func_signal": "def _get_original_file(doc_id):\n", "code": "cachefilename = os.path.join(CACHEDIR, 'o', doc_id + '.jpeg')\nif os.path.exists(cachefilename):\n    # File exists in the cache\n    return open(cachefilename)\n\n# ensure the needed dirs exist\nif not os.path.exists(os.path.join(CACHEDIR, 'o')):\n    os.makedirs(os.path.join(CACHEDIR, 'o'))\n\n# try to get file from S3\nconn = boto.connect_s3()\ns3bucket = conn.get_bucket(S3BUCKET, validate=False)\nk = s3bucket.get_key(doc_id)\nif k:\n    # write then rename to avoid race conditions\n    tempfilename = tempfile.mktemp(prefix='tmp_%s_%s' % ('o', doc_id), dir=CACHEDIR)\n    k.get_file(open(tempfilename, \"w\"))\n    os.rename(tempfilename, cachefilename)\n    return open(cachefilename)\n\n# try to get it from couchdb\ndb = couchdb.client.Server(COUCHSERVER)[COUCHDB_NAME]\ntry:\n    doc = db[doc_id]\nexcept couchdb.client.ResourceNotFound:\n    return None\n\nfilename = list(doc['_attachments'].keys())[0]\n\n# save original Image in Cache\nfiledata = db.get_attachment(doc_id, filename)\n# write then rename to avoid race conditions\ntempfilename = tempfile.mktemp(prefix='tmp_%s_%s' % ('o', doc_id), dir=CACHEDIR)\nopen(os.path.join(tempfilename), 'w').write(filedata)\nos.rename(tempfilename, cachefilename)\n\n# upload to S3 for migrating form CouchDB to S3\nconn = boto.connect_s3()\nk = s3bucket.get_key(doc_id)\nif not k:\n    k = boto.s3.key.Key(s3bucket)\n    k.key = doc_id\n    k.set_contents_from_filename(cachefilename)\n    k.make_public()\n\nreturn open(cachefilename)", "path": "huimages\\server.py", "repo_name": "hudora/huImages", "stars": 64, "license": "other", "language": "python", "size": 189}
{"docstring": "\"\"\"Simple WSGI complient Server.\"\"\"\n", "func_signal": "def imagserver(environ, start_response):\n", "code": "parts = environ.get('PATH_INFO', '').split('/')\nif len(parts) < 3:\n    start_response('404 Not Found', [('Content-Type', 'text/plain')])\n    return [\"File not found\\n\"]\ntyp, doc_id = parts[1:3]\ndoc_id = doc_id.strip('jpeg.')\nif not typ_re.match(typ):\n    start_response('501 Error', [('Content-Type', 'text/plain')])\n    return [\"Not Implemented\\n\"]\nif not docid_re.match(doc_id):\n    start_response('501 Error', [('Content-Type', 'text/plain')])\n    return [\"Not Implemented\\n\"]\n\nif not os.path.exists(os.path.join(CACHEDIR, typ)):\n    os.makedirs(os.path.join(CACHEDIR, typ))\n\ncachefilename = os.path.join(CACHEDIR, typ, doc_id + '.jpeg')\nif os.path.exists(cachefilename):\n    # serve request from cache\n    start_response('200 OK', [('Content-Type', 'image/jpeg'),\n                              ('Cache-Control', 'max-age=1728000, public'),  # 20 Days\n                              ])\n    return open(cachefilename)\n\n# get data from database\norgfile = _get_original_file(doc_id)\nif not orgfile:\n    start_response('404 Not Found', [('Content-Type', 'text/plain')])\n    return [\"File not found\"]\n\nif typ == 'o':\n    imagefile = orgfile\nelse:\n    width, height = typ.split('x')\n    try:\n        img = Image.open(orgfile)\n        if img.mode != \"RGB\":\n            img = img.convert(\"RGB\")\n        if height.endswith('!'):\n            height = height.strip('!')\n            img = _crop_image(width, height, img)\n        else:\n            img = _scale_image(width, height, img)\n    except IOError:\n        # we assume the source file is broken\n        mark_broken(doc_id)\n        start_response('404 Internal Server Error', [('Content-Type', 'text/plain')])\n        return [\"File not found\"]\n\n    tempfilename = tempfile.mktemp(prefix='tmp_%s_%s' % (typ, doc_id), dir=CACHEDIR)\n    img.save(tempfilename, \"JPEG\")\n    os.rename(tempfilename, cachefilename)\n    # using X-Sendfile could speed this up.\n    imagefile = open(cachefilename)\n\nstart_response('200 OK', [('Content-Type', 'image/jpeg'),\n                          ('Cache-Control', 'max-age=17280000, public'),  # 20 Days\n                          ])\nreturn imagefile", "path": "huimages\\server.py", "repo_name": "hudora/huImages", "stars": 64, "license": "other", "language": "python", "size": 189}
{"docstring": "\"\"\"\nThis function will scale an image to a given bounding box. Image\naspect ratios will be conserved and so there might be blank space\nat two sides of the image if the ratio isn't identical to that of\nthe bounding box.\n\"\"\"\n# originally from\n# http://simon.bofh.ms/cgi-bin/trac-django-projects.cgi/file/stuff/branches/magic-removal/image.py\n", "func_signal": "def _scale_image(width, height, image):\n", "code": "lfactor = 1\nwidth, height = int(width), int(height)\n(xsize, ysize) = image.size\nif xsize > width and ysize > height:\n    lfactorx = float(width) / float(xsize)\n    lfactory = float(height) / float(ysize)\n    lfactor = min(lfactorx, lfactory)\nelif xsize > width:\n    lfactor = float(width) / float(xsize)\nelif ysize > height:\n    lfactor = float(height) / float(ysize)\nres = image.resize((int(float(xsize) * lfactor), int(float(ysize) * lfactor)), Image.ANTIALIAS)\nreturn res", "path": "huimages\\server.py", "repo_name": "hudora/huImages", "stars": 64, "license": "other", "language": "python", "size": 189}
{"docstring": "\"\"\"Updates metadata for an image.\n\ntimestamp should be a datetime object representing the creation time of the image or None.\n\ntitle should be an title for the image.\n\nreferences can be arbitrary data e.g. referencing an article number.\n\ntyp can be the type of the image. So far only 'product_image' is used.\n\"\"\"\n", "func_signal": "def update_metadata(doc_id, timestamp=None, title='', references=None, typ=''):\n", "code": "db = _setup_couchdb()\ndoc = db[doc_id]\n\nif timestamp:\n    timestamp = _datetime2str(datetime.datetime.now())\n    doc['mtime'] = timestamp\n    if 'ctime' not in doc:\n        doc['ctime'] = timestamp\n\nif typ and (typ not in doc.get('types', [])):\n    doc.setdefault('types', []).append(typ)\nif title and title not in doc.get('title', []):\n    doc.setdefault('title', []).append(title)\n\nif references:\n    for key, value in list(references.items()):\n        if value not in doc.get('references', {}).get(key, []):\n            doc.setdefault('references', {}).setdefault(key, []).append(value)\n\ndb[doc_id] = doc\n# clean cache\n__imagedoc_cache = {}\nreturn doc_id", "path": "huimages\\__init__.py", "repo_name": "hudora/huImages", "stars": 64, "license": "other", "language": "python", "size": 189}
{"docstring": "\"\"\"\nreturn a list of (ind0, ind1) such that mask[ind0:ind1].all() is\nTrue and we cover all such regions\n\nTODO: this is a pure python implementation which probably has a much faster numpy impl\n\"\"\"\n\n", "func_signal": "def contiguous_regions(mask):\n", "code": "in_region = None\nboundaries = []\nfor i, val in enumerate(mask):\n    if in_region is None and val:\n        in_region = i\n    elif in_region is not None and not val:\n        boundaries.append((in_region, i))\n        in_region = None\n\nif in_region is not None:\n    boundaries.append((in_region, i+1))\nreturn boundaries", "path": "pbrainlib\\mlabold.py", "repo_name": "nipy/pbrain", "stars": 94, "license": "None", "language": "python", "size": 2440}
{"docstring": "\"\"\"\nReturns the log base 2 of *n* if *n* is a power of 2, zero otherwise.\n\nNote the potential ambiguity if *n* == 1: 2**0 == 1, interpret accordingly.\n\"\"\"\n\n", "func_signal": "def ispower2(n):\n", "code": "bin_n = binary_repr(n)[1:]\nif '1' in bin_n:\n    return 0\nelse:\n    return len(bin_n)", "path": "pbrainlib\\mlabold.py", "repo_name": "nipy/pbrain", "stars": 94, "license": "None", "language": "python", "size": 2440}
{"docstring": "\"\"\"\nReturn a *M* x *N* sparse matrix with *frac* elements randomly\nfilled.\n\"\"\"\n", "func_signal": "def get_sparse_matrix(M,N,frac=0.1):\n", "code": "data = np.zeros((M,N))*0.\nfor i in range(int(M*N*frac)):\n    x = np.random.randint(0,M-1)\n    y = np.random.randint(0,N-1)\n    data[x,y] = np.random.rand()\nreturn data", "path": "pbrainlib\\mlabold.py", "repo_name": "nipy/pbrain", "stars": 94, "license": "None", "language": "python", "size": 2440}
{"docstring": "\"\"\"Construct a mask suitable for string.translate,\nwhich marks letters in charlist as \"t\" and ones not as \"b\" \"\"\"\n", "func_signal": "def mask(charlist):\n", "code": "mask=\"\"\nfor i in range(256):\n        if chr(i) in charlist: mask=mask+\"t\"\n        else: mask=mask+\"b\"\nreturn mask", "path": "eegview\\file_formats.py", "repo_name": "nipy/pbrain", "stars": 94, "license": "None", "language": "python", "size": 2440}
{"docstring": "\"\"\"Mouse button released.\"\"\"\n\n\n", "func_signal": "def OnButtonUp(self, wid, event):\n", "code": "MarkerWindowInteractor.OnButtonUp(self, wid, event)\npntsXYZ = ( self.get_plane_points(self.pwX),\n            self.get_plane_points(self.pwY),\n            self.get_plane_points(self.pwZ))\n\nif hasattr(self, 'self.lastPntsXYZ'):\n    if pntsXYZ != self.lastPntsXYZ:\n        UndoRegistry().push_command(\n            self.set_plane_points_xyz, self.lastPntsXYZ)\n\n\nreturn True", "path": "eegview\\plane_widgets.py", "repo_name": "nipy/pbrain", "stars": 94, "license": "None", "language": "python", "size": 2440}
{"docstring": "\"\"\"\nGiven data vectors *x* and *y*, the slope vector *yp* and a new\nabscissa vector *xi*, the function :func:`stineman_interp` uses\nStineman interpolation to calculate a vector *yi* corresponding to\n*xi*.\n\nHere's an example that generates a coarse sine curve, then\ninterpolates over a finer abscissa::\n\n  x = linspace(0,2*pi,20);  y = sin(x); yp = cos(x)\n  xi = linspace(0,2*pi,40);\n  yi = stineman_interp(xi,x,y,yp);\n  plot(x,y,'o',xi,yi)\n\nThe interpolation method is described in the article A\nCONSISTENTLY WELL BEHAVED METHOD OF INTERPOLATION by Russell\nW. Stineman. The article appeared in the July 1980 issue of\nCreative Computing with a note from the editor stating that while\nthey were:\n\n  not an academic journal but once in a while something serious\n  and original comes in adding that this was\n  \"apparently a real solution\" to a well known problem.\n\nFor *yp* = *None*, the routine automatically determines the slopes\nusing the :func:`slopes` routine.\n\n*x* is assumed to be sorted in increasing order.\n\nFor values ``xi[j] < x[0]`` or ``xi[j] > x[-1]``, the routine\ntries an extrapolation.  The relevance of the data obtained from\nthis, of course, is questionable...\n\nOriginal implementation by Halldor Bjornsson, Icelandic\nMeteorolocial Office, March 2006 halldor at vedur.is\n\nCompletely reworked and optimized for Python by Norbert Nemec,\nInstitute of Theoretical Physics, University or Regensburg, April\n2006 Norbert.Nemec at physik.uni-regensburg.de\n\"\"\"\n\n# Cast key variables as float.\n", "func_signal": "def stineman_interp(xi,x,y,yp=None):\n", "code": "x=np.asarray(x, np.float_)\ny=np.asarray(y, np.float_)\nassert x.shape == y.shape\nN=len(y)\n\nif yp is None:\n    yp = slopes(x,y)\nelse:\n    yp=np.asarray(yp, np.float_)\n\nxi=np.asarray(xi, np.float_)\nyi=np.zeros(xi.shape, np.float_)\n\n# calculate linear slopes\ndx = x[1:] - x[:-1]\ndy = y[1:] - y[:-1]\ns = dy/dx  #note length of s is N-1 so last element is #N-2\n\n# find the segment each xi is in\n# this line actually is the key to the efficiency of this implementation\nidx = np.searchsorted(x[1:-1], xi)\n\n# now we have generally: x[idx[j]] <= xi[j] <= x[idx[j]+1]\n# except at the boundaries, where it may be that xi[j] < x[0] or xi[j] > x[-1]\n\n# the y-values that would come out from a linear interpolation:\nsidx = s.take(idx)\nxidx = x.take(idx)\nyidx = y.take(idx)\nxidxp1 = x.take(idx+1)\nyo = yidx + sidx * (xi - xidx)\n\n# the difference that comes when using the slopes given in yp\ndy1 = (yp.take(idx)- sidx) * (xi - xidx)       # using the yp slope of the left point\ndy2 = (yp.take(idx+1)-sidx) * (xi - xidxp1) # using the yp slope of the right point\n\ndy1dy2 = dy1*dy2\n# The following is optimized for Python. The solution actually\n# does more calculations than necessary but exploiting the power\n# of numpy, this is far more efficient than coding a loop by hand\n# in Python\nyi = yo + dy1dy2 * np.choose(np.array(np.sign(dy1dy2), np.int32)+1,\n                             ((2*xi-xidx-xidxp1)/((dy1-dy2)*(xidxp1-xidx)),\n                              0.0,\n                              1/(dy1+dy2),))\nreturn yi", "path": "pbrainlib\\mlabold.py", "repo_name": "nipy/pbrain", "stars": 94, "license": "None", "language": "python", "size": 2440}
{"docstring": "\"\"\"\nReturn the rank for each element in *x*, return the rank\n0..len(*p*).  Eg if *p* = (25, 50, 75), the return value will be a\nlen(*x*) array with values in [0,1,2,3] where 0 indicates the\nvalue is less than the 25th percentile, 1 indicates the value is\n>= the 25th and < 50th percentile, ... and 3 indicates the value\nis above the 75th percentile cutoff.\n\n*p* is either an array of percentiles in [0..100] or a scalar which\nindicates how many quantiles of data you want ranked.\n\"\"\"\n\n", "func_signal": "def prctile_rank(x, p):\n", "code": "if not cbook.iterable(p):\n    p = np.arange(100.0/p, 100.0, 100.0/p)\nelse:\n    p = np.asarray(p)\n\nif p.max()<=1 or p.min()<0 or p.max()>100:\n    raise ValueError('percentiles should be in range 0..100, not 0..1')\n\nptiles = prctile(x, p)\nreturn np.searchsorted(ptiles, x)", "path": "pbrainlib\\mlabold.py", "repo_name": "nipy/pbrain", "stars": 94, "license": "None", "language": "python", "size": 2440}
{"docstring": "\"\"\"\nGiven a date string and time string from the .axonascii format, return a datetime.datetime instance\n\"\"\"\n", "func_signal": "def get_axondate(self, axon_datestr, axon_timestr):\n", "code": "[datestamp, timestamp] = [axon_datestr, axon_timestr]\ndatestamp = re.sub('\\\"', '', datestamp)\ntimestamp = re.sub('\\\"', '', timestamp)\ndate_split = datestamp.split('/')\ntime_split = timestamp.split(':')\n# mccXXX: this may not properly handle am/pm timestamp differences!\naxondate = datetime.datetime(int(date_split[2]), int(date_split[0]), int(date_split[1]), int(time_split[0]), int(time_split[1]), int(time_split[2])) \nreturn axondate", "path": "eegview\\file_formats.py", "repo_name": "nipy/pbrain", "stars": 94, "license": "None", "language": "python", "size": 2440}
{"docstring": "\"\"\"\nReturn *True* if *s1* and *s2* intersect.\n*s1* and *s2* are defined as::\n\n  s1: (x1, y1), (x2, y2)\n  s2: (x3, y3), (x4, y4)\n\"\"\"\n", "func_signal": "def segments_intersect(s1, s2):\n", "code": "(x1, y1), (x2, y2) = s1\n(x3, y3), (x4, y4) = s2\n\nden = ((y4-y3) * (x2-x1)) - ((x4-x3)*(y2-y1))\n\nn1 = ((x4-x3) * (y1-y3)) - ((y4-y3)*(x1-x3))\nn2 = ((x2-x1) * (y1-y3)) - ((y2-y1)*(x1-x3))\n\nif den == 0:\n    # lines parallel\n    return False\n\nu1 = n1/den\nu2 = n2/den\n\nreturn 0.0 <= u1 <= 1.0 and 0.0 <= u2 <= 1.0", "path": "pbrainlib\\mlabold.py", "repo_name": "nipy/pbrain", "stars": 94, "license": "None", "language": "python", "size": 2440}
{"docstring": "\"\"\"\nCompute exponentials which safely underflow to zero.\n\nSlow, but convenient to use. Note that numpy provides proper\nfloating point exception handling with access to the underlying\nhardware.\n\"\"\"\n\n", "func_signal": "def exp_safe(x):\n", "code": "if type(x) is np.ndarray:\n    return exp(np.clip(x,exp_safe_MIN,exp_safe_MAX))\nelse:\n    return math.exp(x)", "path": "pbrainlib\\mlabold.py", "repo_name": "nipy/pbrain", "stars": 94, "license": "None", "language": "python", "size": 2440}
{"docstring": "\"\"\"\nBuffer up to *nmax* points.\n\"\"\"\n", "func_signal": "def __init__(self, nmax):\n", "code": "self._xa = np.zeros((nmax,), np.float_)\nself._ya = np.zeros((nmax,), np.float_)\nself._xs = np.zeros((nmax,), np.float_)\nself._ys = np.zeros((nmax,), np.float_)\nself._ind = 0\nself._nmax = nmax\nself.dataLim = None\nself.callbackd = {}", "path": "pbrainlib\\mlabold.py", "repo_name": "nipy/pbrain", "stars": 94, "license": "None", "language": "python", "size": 2440}
{"docstring": "\"\"\"\n*points* is a sequence of *x*, *y* points.\n*verts* is a sequence of *x*, *y* vertices of a polygon.\n\nReturn value is a sequence of indices into points for the points\nthat are inside the polygon.\n\"\"\"\n", "func_signal": "def inside_poly(points, verts):\n", "code": "res, =  np.nonzero(nxutils.points_inside_poly(points, verts))\nreturn res", "path": "pbrainlib\\mlabold.py", "repo_name": "nipy/pbrain", "stars": 94, "license": "None", "language": "python", "size": 2440}
{"docstring": "\"\"\"\nReturn *x* and *y* as arrays; their length will be the len of\ndata added or *nmax*.\n\"\"\"\n", "func_signal": "def asarrays(self):\n", "code": "if self._ind<self._nmax:\n    return self._xs[:self._ind], self._ys[:self._ind]\nind = self._ind % self._nmax\n\nself._xa[:self._nmax-ind] = self._xs[ind:]\nself._xa[self._nmax-ind:] = self._xs[:ind]\nself._ya[:self._nmax-ind] = self._ys[ind:]\nself._ya[self._nmax-ind:] = self._ys[:ind]\n\nreturn self._xa, self._ya", "path": "pbrainlib\\mlabold.py", "repo_name": "nipy/pbrain", "stars": 94, "license": "None", "language": "python", "size": 2440}
{"docstring": "\"\"\"\nCompute an FFT phase randomized surrogate of *x*.\n\"\"\"\n", "func_signal": "def fftsurr(x, detrend=detrend_none, window=window_none):\n", "code": "if cbook.iterable(window):\n    x=window*detrend(x)\nelse:\n    x = window(detrend(x))\nz = np.fft.fft(x)\na = 2.*np.pi*1j\nphase = a * np.random.rand(len(x))\nz = z*np.exp(phase)\nreturn np.fft.ifft(z).real", "path": "pbrainlib\\mlabold.py", "repo_name": "nipy/pbrain", "stars": 94, "license": "None", "language": "python", "size": 2440}
{"docstring": "\"\"\"Mouse button released.\"\"\"\n", "func_signal": "def OnButtonUp(self, wid, event):\n", "code": "m = self.get_pointer()\nctrl, shift = self._GetCtrlShift(event)\nself._Iren.SetEventInformationFlipY(m[0], m[1], ctrl, shift,\n                                    chr(0), 0, None)\n\n\nif event.button in self.interactButtons:\n    self.releaseFuncs[event.button]()            \n\ntry: self.releaseHooks[event.button]()\nexcept KeyError: pass\n\nthisCamera = self.get_camera_fpu()\ntry: self.lastCamera\nexcept AttributeError: pass  # this\nelse:\n    if thisCamera != self.lastCamera:\n        UndoRegistry().push_command(self.set_camera, self.lastCamera)\n\nreturn True", "path": "eegview\\plane_widgets.py", "repo_name": "nipy/pbrain", "stars": 94, "license": "None", "language": "python", "size": 2440}
{"docstring": "# get the old limits\n", "func_signal": "def make_plot(self, *args):\n", "code": "self.axes.cla()\nself.axes.grid(True)\n\ntup = self.get_data()\nif tup is None: return \nt, data, dt, label = tup\n#print \"ChannelWin.make_plot(): data[0:100] is \" , data[0:100]\nself.axes.plot(t, data)\nif self.ylim is not None:\n    self.axes.set_ylim(self.ylim)\nself.ylim = self.axes.get_ylim()\nself.axes.set_title('Channel %s'%label)\nself.axes.set_xlabel('time(s)')        \n\nself.axes.yaxis.set_major_formatter(ScalarFormatter())\n\nself.canvas.draw()", "path": "eegview\\mpl_windows.py", "repo_name": "nipy/pbrain", "stars": 94, "license": "None", "language": "python", "size": 2440}
{"docstring": "\"\"\"\n*r* is a numpy record array\n\n*summaryfuncs* is a list of (*attr*, *func*, *outname*) tuples\nwhich will apply *func* to the the array *r*[attr] and assign the\noutput to a new attribute name *outname*.  The returned record\narray is identical to *r*, with extra arrays for each element in\n*summaryfuncs*.\n\n\"\"\"\n\n", "func_signal": "def rec_summarize(r, summaryfuncs):\n", "code": "names = list(r.dtype.names)\narrays = [r[name] for name in names]\n\nfor attr, func, outname in summaryfuncs:\n    names.append(outname)\n    arrays.append(np.asarray(func(r[attr])))\n\nreturn np.rec.fromarrays(arrays, names=names)", "path": "pbrainlib\\mlabold.py", "repo_name": "nipy/pbrain", "stars": 94, "license": "None", "language": "python", "size": 2440}
{"docstring": "#x, y = self.get_pointer()\n", "func_signal": "def get_cursor_position_world(self):\n", "code": "x, y = self.GetEventPosition()\nxyz = [x, y, 0.0]\npicker = vtk.vtkWorldPointPicker()\npicker.Pick(xyz, self.renderer)\nppos = picker.GetPickPosition()\npos = self.get_cursor_position()\nif pos is None: return None\nworld =  self.obs_to_world(pos)\nreturn world", "path": "eegview\\plane_widgets.py", "repo_name": "nipy/pbrain", "stars": 94, "license": "None", "language": "python", "size": 2440}
{"docstring": "# project the point onto the plane, find the distance between\n        # xyz and the projected point, then move the plane along it's\n        # normal that distance\n#todo: undo\n", "func_signal": "def snap_view_to_point(self, xyz):\n", "code": "        move_pw_to_point(self.pwX, xyz)\n        move_pw_to_point(self.pwY, xyz)\n        move_pw_to_point(self.pwZ, xyz)\n        self.Render()\n        EventHandler().notify('observers update plane')", "path": "eegview\\plane_widgets.py", "repo_name": "nipy/pbrain", "stars": 94, "license": "None", "language": "python", "size": 2440}
{"docstring": "# get the old limits\n", "func_signal": "def make_plot(self, *args):\n", "code": "self.axes.cla()\nself.axes.grid(True)\n\ntup = self.get_data()\nif tup is None: return \nt, data, dt, label = tup\n\nself.axes.hist(data, 200)\nself.axes.set_title('Channel %s'%label)\nself.canvas.draw()", "path": "eegview\\mpl_windows.py", "repo_name": "nipy/pbrain", "stars": 94, "license": "None", "language": "python", "size": 2440}
{"docstring": "#self._dev = pycuda.autoinit.device\n#self._dev = drv.Device(dev)\n", "func_signal": "def __init__(self):\n", "code": "self._dev = drv.Context.get_device()\nself._attr = self._dev.get_attributes()\n\nself.max_block_threads = self._attr[_dev_attr.MAX_THREADS_PER_BLOCK]\nself.shared_mem = self._attr[_dev_attr.MAX_SHARED_MEMORY_PER_BLOCK]\nself.warp_size = self._attr[_dev_attr.WARP_SIZE]\nself.max_registers = self._attr[_dev_attr.MAX_REGISTERS_PER_BLOCK]\nself.compute_cap = self._dev.compute_capability()\nself.max_grid_dim = (self._attr[_dev_attr.MAX_GRID_DIM_X],\n                     self._attr[_dev_attr.MAX_GRID_DIM_Y])", "path": "gpustats\\util.py", "repo_name": "dukestats/gpustats", "stars": 83, "license": "bsd-3-clause", "language": "python", "size": 414}
{"docstring": "# quadratic formula, correct fp error\n", "func_signal": "def unvech(v):\n", "code": "rows = .5 * (-1 + np.sqrt(1 + 8 * len(v)))\nrows = int(np.round(rows))\n\nresult = np.zeros((rows, rows))\nresult[np.triu_indices(rows)] = v\nresult = result + result.T\n\n# divide diagonal elements by 2\nresult[np.diag_indices(rows)] /= 2\n\nreturn result", "path": "gpustats\\util.py", "repo_name": "dukestats/gpustats", "stars": 83, "license": "bsd-3-clause", "language": "python", "size": 414}
{"docstring": "# for multivariate pdfs\n", "func_signal": "def _get_mvcaller_code():\n", "code": "path = os.path.join(get_cufiles_path(), 'mvcaller.cu')\nreturn open(path).read()", "path": "gpustats\\codegen.py", "repo_name": "dukestats/gpustats", "stars": 83, "license": "bsd-3-clause", "language": "python", "size": 414}
{"docstring": "# quadratic formula, correct fp error\n", "func_signal": "def unvech(v):\n", "code": "rows = .5 * (-1 + np.sqrt(1 + 8 * len(v)))\nrows = int(np.round(rows))\n\nresult = np.zeros((rows, rows))\nresult[np.triu_indices(rows)] = v\nresult = result + result.T\n\n# divide diagonal elements by 2\nresult[np.diag_indices(rows)] /= 2\n\nreturn result", "path": "old\\util.py", "repo_name": "dukestats/gpustats", "stars": 83, "license": "bsd-3-clause", "language": "python", "size": 414}
{"docstring": "\"\"\"\nPad data to be a multiple of 16 for discrete sampler.\n\"\"\"\n\n", "func_signal": "def pad_data_mult16(data, fill=0):\n", "code": "if type(data) == gpuarray:\n    data = data.get()\n\nn, k = data.shape\n\nkm = int(k/16) + 1\n\nnewk = km*16\nif newk != k:\n    padded_data = np.zeros((n, newk), dtype=np.float32)\n    if fill!=0:\n        padded_data = padded_data + fill\n\n    padded_data[:,:k] = data\n\n    return padded_data\nelse:\n    return prep_ndarray(data)", "path": "gpustats\\util.py", "repo_name": "dukestats/gpustats", "stars": 83, "license": "bsd-3-clause", "language": "python", "size": 414}
{"docstring": "# is float32 and contiguous?\n", "func_signal": "def prep_ndarray(arr):\n", "code": "if not arr.dtype == np.float32 or not arr.flags.contiguous:\n    arr = np.array(arr, dtype=np.float32)\n\nreturn arr", "path": "scripts\\bench.py", "repo_name": "dukestats/gpustats", "stars": 83, "license": "bsd-3-clause", "language": "python", "size": 414}
{"docstring": "\"\"\"\nPad data to avoid bank conflicts on the GPU-- dimension should not be a\nmultiple of the half-warp size (16)\n\"\"\"\n", "func_signal": "def pad_data(data):\n", "code": "if type(data) == gpuarray:\n    data = data.get()\n\nn, k = data.shape\n\nif not k % HALF_WARP:\n    pad_dim = k + 1\nelse:\n    pad_dim = k\n\nif k != pad_dim:\n    padded_data = np.empty((n, pad_dim), dtype=np.float32)\n    padded_data[:, :k] = data\n\n    return padded_data\nelse:\n    return prep_ndarray(data)", "path": "gpustats\\util.py", "repo_name": "dukestats/gpustats", "stars": 83, "license": "bsd-3-clause", "language": "python", "size": 414}
{"docstring": "\"\"\"\n\nParameters\n----------\n\nReturns\n-------\n\n\"\"\"\n", "func_signal": "def make_calls(func, data, devices=None, splits=None):\n", "code": "if splits is None:\n    pass", "path": "gpustats\\multigpu.py", "repo_name": "dukestats/gpustats", "stars": 83, "license": "bsd-3-clause", "language": "python", "size": 414}
{"docstring": "# is float32 and contiguous?\n", "func_signal": "def prep_ndarray(arr):\n", "code": "if not arr.dtype == np.float32 or not arr.flags.contiguous:\n    arr = np.array(arr, dtype=np.float32, order='C')\n\nreturn arr", "path": "gpustats\\util.py", "repo_name": "dukestats/gpustats", "stars": 83, "license": "bsd-3-clause", "language": "python", "size": 414}
{"docstring": "\"\"\"\nIf gpustats (or any other pycuda work) is used inside a \nmultiprocessing.Process, this function must be used inside the\nthread to clean up invalid contexts and create a new one on the \ngiven device. Assumes one GPU per thread.\n\"\"\"\n\n", "func_signal": "def threadSafeInit(device = 0):\n", "code": "import atexit\ndrv.init() # just in case\n\n## clean up all contexts. most will be invalid from\n## multiprocessing fork\nimport os; import sys\nclean = False\nwhile not clean:\n    _old_ctx = drv.Context.get_current()\n    if _old_ctx is None:\n        clean = True\n    else:\n        ## detach: will give warnings to stderr if invalid\n        _old_cerr = os.dup(sys.stderr.fileno())\n        _nl = os.open(os.devnull, os.O_RDWR)\n        os.dup2(_nl, sys.stderr.fileno())\n        _old_ctx.detach() \n        sys.stderr = os.fdopen(_old_cerr, \"wb\")\n        os.close(_nl)\nfrom pycuda.tools import clear_context_caches\nclear_context_caches()\n    \n## init a new device\ndev = drv.Device(device)\nctx = dev.make_context()\n\n## pycuda.autoinit exitfunc is bad now .. delete it\nexit_funcs = atexit._exithandlers\nfor fn in exit_funcs:\n    if hasattr(fn[0], 'func_name'):\n        if fn[0].func_name == '_finish_up':\n            exit_funcs.remove(fn)\n        if fn[0].func_name == 'clean_all_contexts': # avoid duplicates\n            exit_funcs.remove(fn)\n\n## make sure we clean again on exit\natexit.register(clean_all_contexts)", "path": "gpustats\\util.py", "repo_name": "dukestats/gpustats", "stars": 83, "license": "bsd-3-clause", "language": "python", "size": 414}
{"docstring": "# For univariate pdfs\n", "func_signal": "def _get_univcaller_code():\n", "code": "path = os.path.join(get_cufiles_path(), 'univcaller.cu')\nreturn open(path).read()", "path": "gpustats\\codegen.py", "repo_name": "dukestats/gpustats", "stars": 83, "license": "bsd-3-clause", "language": "python", "size": 414}
{"docstring": "\"\"\"\n\n\"\"\"\n", "func_signal": "def _execute_calls(calls):\n", "code": "for call in calls:\n    call.start()\n\nfor call in calls:\n    call.join()", "path": "gpustats\\multigpu.py", "repo_name": "dukestats/gpustats", "stars": 83, "license": "bsd-3-clause", "language": "python", "size": 414}
{"docstring": "\"\"\"\nwill set the order of garray in place\n\"\"\"\n", "func_signal": "def GPUarray_order(garray, order=\"F\"):\n", "code": "if order==\"F\":\n    if garray.flags.f_contiguous:\n        exit\n    else:\n        garray.strides = gpuarray._f_contiguous_strides(\n            garray.dtype.itemsize, garray.shape)\n        garray.flags.f_contiguous = True\n        garray.flags.c_contiguous = False\nelif order==\"C\":\n    if garray.flags.c_contiguous:\n        exit\n    else:\n        garray.strides = gpuarray._c_contiguous_strides(\n            garray.dtype.itemsize, garray.shape)\n        garray.flags.c_contiguous = True\n        garray.flags.f_contiguous = False", "path": "gpustats\\util.py", "repo_name": "dukestats/gpustats", "stars": 83, "license": "bsd-3-clause", "language": "python", "size": 414}
{"docstring": "# can override default name, for transforms. this a hack?\n", "func_signal": "def get_name(self, name=None):\n", "code": "if name is None:\n    name = self.name\n\nreturn name", "path": "gpustats\\codegen.py", "repo_name": "dukestats/gpustats", "stars": 83, "license": "bsd-3-clause", "language": "python", "size": 414}
{"docstring": "\"\"\"\nFor multivariate distributions-- what's the optimal block size given the\ngpu?\n\nParameters\n----------\ndata : ndarray\nparams : ndarray\n\nReturns\n-------\n(data_per, params_per) : (int, int)\n\"\"\"\n#info = DeviceInfo()\n\n", "func_signal": "def tune_blocksize(data, params, func_regs):\n", "code": "max_smem = info.shared_mem * 0.9\nmax_threads = int(info.max_block_threads * 0.5)\nmax_regs = info.max_registers\nmax_grid = int(info.max_grid_dim[0])\n\nparams_per = 64#max_threads\nif (len(params) < params_per):\n    params_per = _next_pow2(len(params), info.max_block_threads)\n\nmin_data_per = data.shape[0] / max_grid;\ndata_per0 = _next_pow2( max( max_threads / params_per, min_data_per ), 512);\ndata_per = data_per0\n\ndef _can_fit(data_per, params_per):\n    ok = compute_shmem(data, params, data_per, params_per) <= max_smem\n    ok = ok and data_per*params_per <= max_threads\n    return ok and func_regs*data_per*params_per <= max_regs\n\nwhile True:\n    while not _can_fit(data_per, params_per):\n        if data_per <= min_data_per:\n            break\n\n        if params_per > 1:\n            # reduce number of parameters first\n            params_per /= 2\n        else:\n            # can't go any further, have to do less data\n            data_per /= 2\n\n    if data_per <= min_data_per:\n        # we failed somehow. start over\n        data_per = 2 * data_per0\n        params_per /= 2\n        continue\n    else:\n        break\n\nwhile _can_fit(2 * data_per, params_per):\n    #if 2 * data_per * params_per < max_threads:\n        data_per *= 2\n    #else:\n        # hit block size limit\n    #    break\n\n#import pdb; pdb.set_trace()\nreturn data_per, params_per", "path": "gpustats\\util.py", "repo_name": "dukestats/gpustats", "stars": 83, "license": "bsd-3-clause", "language": "python", "size": 414}
{"docstring": "'''\n\n\n'''\n", "func_signal": "def pack_pdf_params(mean, chol_sigma, logdet):\n", "code": "k = len(mean)\nmean_len = k\nchol_len = k * (k + 1) / 2\nmch_len = mean_len + chol_len\n\npacked_dim = next_multiple(mch_len + 2, PAD_MULTIPLE)\n\npacked_params = np.empty(packed_dim, dtype=np.float32)\npacked_params[:mean_len] = mean\n\npacked_params[mean_len:mch_len] = chol_sigma[np.tril_indices(k)]\npacked_params[mch_len:mch_len + 2] = 1, logdet\n\nreturn packed_params", "path": "old\\util.py", "repo_name": "dukestats/gpustats", "stars": 83, "license": "bsd-3-clause", "language": "python", "size": 414}
{"docstring": "# get the module for this context\n", "func_signal": "def get_function(self, name):\n", "code": "context = drv.Context.get_current()\ntry:\n    mod = self.pycuda_modules[context]\nexcept KeyError:\n    # if it's a new context, init the module\n    self.pycuda_modules[context] = SourceModule(self.all_code)\n    mod = self.pycuda_modules[context]\nreturn mod.get_function('k_%s' % name)\n#curDevice = drv.Context.get_device()\n#if self.curDevice != curDevice:\n#    self.pycuda_module = SourceModule(self.all_code)\n#    self.curDevice = curDevice\n#return self.pycuda_module.get_function('k_%s' % name)", "path": "gpustats\\codegen.py", "repo_name": "dukestats/gpustats", "stars": 83, "license": "bsd-3-clause", "language": "python", "size": 414}
{"docstring": "\"\"\"\nOutputs the 'opimal' block and grid configuration\nfor the sample discrete kernel.\n\"\"\"\n", "func_signal": "def _tune_sfm(n, stride, func_regs):\n", "code": "from gpustats.util import info\n\n#info = DeviceInfo()\ncomp_cap = info.compute_cap\nmax_smem = info.shared_mem * 0.8\nmax_threads = int(info.max_block_threads * 0.5)\nmax_regs = 0.9 * info.max_registers\n\n# We want smallest dim possible in x dimsension while\n# still reading mem correctly\n\nif comp_cap[0] == 1:\n    xdim = 16\nelse:\n    xdim = 32\n\n\ndef sfm_config_ok(xdim, ydim, stride, func_regs, max_regs, max_smem, max_threads):\n    ok = 4*(xdim*stride + 1*xdim) < max_smem and func_regs*ydim*xdim < max_regs\n    return ok and xdim*ydim <= max_threads\n\nydim = 2\nwhile sfm_config_ok(xdim, ydim, stride, func_regs, max_regs, max_smem, max_threads):\n    ydim += 1\n\nydim -= 1\n\nnblocks = int(n/xdim) + 1\n\nreturn (nblocks,1), (xdim,ydim,1)", "path": "gpustats\\sampler.py", "repo_name": "dukestats/gpustats", "stars": 83, "license": "bsd-3-clause", "language": "python", "size": 414}
{"docstring": "# is float32 and contiguous?\n", "func_signal": "def prep_ndarray(arr):\n", "code": "if not arr.dtype == np.float32 or not arr.flags.contiguous:\n    arr = np.array(arr, dtype=np.float32)\n\nreturn arr", "path": "old\\util.py", "repo_name": "dukestats/gpustats", "stars": 83, "license": "bsd-3-clause", "language": "python", "size": 414}
{"docstring": "\"\"\"\nPad data to avoid bank conflicts on the GPU-- dimension should not be a\nmultiple of the half-warp size (16)\n\"\"\"\n", "func_signal": "def pad_data(data):\n", "code": "n, k = data.shape\n\nif not k % HALF_WARP:\n    pad_dim = k + 1\nelse:\n    pad_dim = k\n\nif k != pad_dim:\n    padded_data = np.empty((n, pad_dim), dtype=np.float32)\n    padded_data[:, :k] = data\n\n    return padded_data\nelse:\n    return prep_ndarray(data)", "path": "old\\util.py", "repo_name": "dukestats/gpustats", "stars": 83, "license": "bsd-3-clause", "language": "python", "size": 414}
{"docstring": "\"\"\" [\">\",\">=\",\"=\",\"<=\",\"<\",\"<>\"] \"\"\"\n\n", "func_signal": "def compare(sign,v1,v2,builder):\n", "code": "if sign == \">\":\n\ti_cod = IPRED_UGT\n\tf_cod = RPRED_OGT\nelif sign == \">=\":\n\ti_cod = IPRED_UGE\n\tf_cod = RPRED_OGE\nelif sign == \"=\":\n\ti_cod = IPRED_EQ\n\tf_cod = RPRED_OEQ\nelif sign == \"<=\":\n\ti_cod = IPRED_ULE\n\tf_cod = RPRED_OLE\nelif sign == \"<\":\n\ti_cod = IPRED_ULT\n\tf_cod = RPRED_OLT\nelif sign == \"<>\":\n\ti_cod = IPRED_NE\n\tf_cod = RPRED_ONE\nelse:\n\treturn c_boolean(False)\n\nif v1.type == types.integer:\n\treturn builder.icmp(i_cod, v1, v2)\nelif v1.type == types.real:\n\treturn builder.fcmp(f_cod, v1, v2)\nelse:\n\treturn c_boolean(False)", "path": "src\\codegen\\helpers.py", "repo_name": "alcides/pascal-in-python", "stars": 85, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"statement_sequence : statement SEMICOLON statement_sequence\n | statement\"\"\"\n", "func_signal": "def p_statement_sequence(t):\n", "code": "if len(t) == 2:\n\tt[0] = t[1]\nelse:\n\tt[0] = Node('statement_list',t[1],t[3])", "path": "src\\rules.py", "repo_name": "alcides/pascal-in-python", "stars": 85, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\" Creates a stub of println \"\"\"\n\n", "func_signal": "def create_write(mod,ln=False):\n", "code": "if ln:\n\tfname = \"writeln\"\nelse:\n\tfname = \"write\"\nprintf = mod.get_function_named(\"printf\")\n\nstring_pointer = Type.pointer(types.int8, 0)\n\nf = mod.add_function(\n\ttypes.function(types.void, (string_pointer,) )\n, fname)\nbb = f.append_basic_block(\"entry\")\t\nbuilder = Builder.new(bb)\nbuilder.call(printf,   (\n\tf.args[0],\n))\n\nif ln:\n\tbuilder.call(printf,   (\n\t\tpointer(builder, c_string(mod,\"\\n\")),\n\t))\nbuilder.ret_void()\nreturn f", "path": "src\\codegen\\helpers.py", "repo_name": "alcides/pascal-in-python", "stars": 85, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\" parameter_list : parameter COMMA parameter_list\n| parameter\"\"\"\n", "func_signal": "def p_parameter_list(t):\n", "code": "if len(t) == 4:\n\tt[0] = Node(\"parameter_list\", t[1], t[3])\nelse:\n\tt[0] = t[1]", "path": "src\\rules.py", "repo_name": "alcides/pascal-in-python", "stars": 85, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\" procedure_heading : PROCEDURE identifier \n| PROCEDURE identifier LPAREN parameter_list RPAREN\"\"\"\n\n", "func_signal": "def p_procedure_heading(t):\n", "code": "if len(t) == 3:\n\tt[0] = Node(\"procedure_head\",t[2])\nelse:\n\tt[0] = Node(\"procedure_head\",t[2],t[4])", "path": "src\\rules.py", "repo_name": "alcides/pascal-in-python", "stars": 85, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\" expression_s : element \n| expression_s psign element\"\"\"\n", "func_signal": "def p_expression_s(t):\n", "code": "if len(t) == 2:\n\tt[0] = t[1]\nelse:\n\tt[0] = Node('op',t[2],t[1],t[3])", "path": "src\\rules.py", "repo_name": "alcides/pascal-in-python", "stars": 85, "license": "None", "language": "python", "size": 188}
{"docstring": "'''\u914d\u5217\u306e\u5148\u982d\u30a2\u30c9\u30ec\u30b9\u3092\u53d6\u5f97\u3059\u308b'''\n", "func_signal": "def gep_first(emit, val):\n", "code": "global int_type\nreturn emit.gep(val, (int_zero, int_zero))", "path": "stuff\\loops.py", "repo_name": "alcides/pascal-in-python", "stars": 85, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"element : identifier\n| real\n| integer\n| string\n| char\n| LPAREN expression RPAREN\n| NOT element\n| function_call_inline\n\"\"\"\n", "func_signal": "def p_element(t):\n", "code": "if len(t) == 2:\n\tt[0] = Node(\"element\",t[1])\nelif len(t) == 3:\n\t# not e\n\tt[0] = Node('not',t[2])\nelse:\n\t# ( e )\n\tt[0] = Node('element',t[2])", "path": "src\\rules.py", "repo_name": "alcides/pascal-in-python", "stars": 85, "license": "None", "language": "python", "size": 188}
{"docstring": "'''\u65b0\u3057\u3044\u6587\u5b57\u5217\u5b9a\u6570\u3092\u751f\u6210\u3059\u308b'''\n", "func_signal": "def new_str_const(val):\n", "code": "global mod, int8_type\nstr = mod.add_global_variable(Type.array(int8_type, len(val) + 1), \"\")\nstr.initializer = Constant.stringz(val)\nreturn str", "path": "stuff\\t.py", "repo_name": "alcides/pascal-in-python", "stars": 85, "license": "None", "language": "python", "size": 188}
{"docstring": "'''\u914d\u5217\u306e\u5148\u982d\u30a2\u30c9\u30ec\u30b9\u3092\u53d6\u5f97\u3059\u308b'''\n", "func_signal": "def gep_first(emit, val):\n", "code": "global int_type\nreturn emit.gep(val, (int_zero, int_zero))", "path": "stuff\\t.py", "repo_name": "alcides/pascal-in-python", "stars": 85, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"variable_declaration_part : VAR variable_declaration_list\n |\n\"\"\"\n", "func_signal": "def p_variable_declaration_part(t):\n", "code": "if len(t) > 1:\n\tt[0] = t[2]", "path": "src\\rules.py", "repo_name": "alcides/pascal-in-python", "stars": 85, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\" param_list : param_list COMMA param\n | param \"\"\"\n", "func_signal": "def p_param_list(t):\n", "code": "if len(t) == 2:\n\tt[0] = t[1]\nelse:\n\tt[0] = Node(\"parameter_list\",t[1],t[3])", "path": "src\\rules.py", "repo_name": "alcides/pascal-in-python", "stars": 85, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\" function_heading : FUNCTION type\n \t| FUNCTION identifier COLON type\n\t| FUNCTION identifier LPAREN parameter_list RPAREN COLON type\"\"\"\n", "func_signal": "def p_function_heading(t):\n", "code": "if len(t) == 3:\n\tt[0] = Node(\"function_head\",t[2])\nelif len(t) == 5:\n\tt[0] = Node(\"function_head\",t[2],t[3])\nelse:\n\tt[0] = Node(\"function_head\",t[2],t[4],t[7])", "path": "src\\rules.py", "repo_name": "alcides/pascal-in-python", "stars": 85, "license": "None", "language": "python", "size": 188}
{"docstring": "'''\u65b0\u3057\u3044\u6587\u5b57\u5217\u5b9a\u6570\u3092\u751f\u6210\u3059\u308b'''\n", "func_signal": "def new_str_const(val):\n", "code": "global mod, int8_type\nstr = mod.add_global_variable(Type.array(int8_type, len(val) + 1), \"\")\nstr.initializer = Constant.stringz(val)\nreturn str", "path": "stuff\\loops.py", "repo_name": "alcides/pascal-in-python", "stars": 85, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"if_statement : IF expression THEN statement ELSE statement\n| IF expression THEN statement\n\"\"\"\n\n", "func_signal": "def p_if_statement(t):\n", "code": "if len(t) == 5:\n\tt[0] = Node('if',t[2],t[4])\nelse:\n\tt[0] = Node('if',t[2],t[4],t[6])", "path": "src\\rules.py", "repo_name": "alcides/pascal-in-python", "stars": 85, "license": "None", "language": "python", "size": 188}
{"docstring": "'''\u6574\u6570\u5b9a\u6570\u5024\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u751f\u6210\u3059\u308b'''\n", "func_signal": "def int_const(val):\n", "code": "global int_type\nreturn Constant.int(int_type, val)", "path": "stuff\\t.py", "repo_name": "alcides/pascal-in-python", "stars": 85, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"procedure_or_function : proc_or_func_declaration SEMICOLON procedure_or_function\n\t| \"\"\"\n\t\n", "func_signal": "def p_procedure_or_function(t):\n", "code": "if len(t) == 4:\n\tt[0] = Node('function_list',t[1],t[3])", "path": "src\\rules.py", "repo_name": "alcides/pascal-in-python", "stars": 85, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"statement : assignment_statement\n | statement_part\n | if_statement\n | while_statement\n | repeat_statement\n | for_statement\n | procedure_or_function_call\n |\n\"\"\"\n", "func_signal": "def p_statement(t):\n", "code": "if len(t) > 1:\n\tt[0] = t[1]", "path": "src\\rules.py", "repo_name": "alcides/pascal-in-python", "stars": 85, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\" expression_m : expression_s\n| expression_m sign expression_s\"\"\"\n", "func_signal": "def p_expression_m(t):\n", "code": "if len(t) == 2:\n\tt[0] = t[1]\nelse:\n\tt[0] = Node('op',t[2],t[1],t[3])", "path": "src\\rules.py", "repo_name": "alcides/pascal-in-python", "stars": 85, "license": "None", "language": "python", "size": 188}
{"docstring": "'''\u6574\u6570\u5b9a\u6570\u5024\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u751f\u6210\u3059\u308b'''\n", "func_signal": "def int_const(val):\n", "code": "global int_type\nreturn Constant.int(int_type, val)", "path": "stuff\\loops.py", "repo_name": "alcides/pascal-in-python", "stars": 85, "license": "None", "language": "python", "size": 188}
{"docstring": "#rv.write(\"%s\\n\" % (mem))\n", "func_signal": "def const_data(self, rv, mem):\n", "code": "ofs = 0\nwhile ofs < len(mem):\n    rv.write(\"%04x: \" % (ofs*4))\n    try:\n        for x in xrange(0, 4):\n            rv.write(\"%08x \" % mem[ofs+x])\n    except IndexError:\n        pass # expected at end of data\n    rv.write(\"\\n\")\n    ofs += 4", "path": "Formatter.py", "repo_name": "laanwj/decuda", "stars": 86, "license": "bsd-3-clause", "language": "python", "size": 662}
{"docstring": "\"\"\"\nint x to four-byte representation to float32\n\"\"\"\n", "func_signal": "def i2f(x):\n", "code": "from struct import pack,unpack\n(x,) = unpack(\"f\", pack(\"I\", x))\nreturn x", "path": "Util.py", "repo_name": "laanwj/decuda", "stars": 86, "license": "bsd-3-clause", "language": "python", "size": 662}
{"docstring": "\"\"\"Parse instruction text\"\"\"\n# [@<pred>] <base>[.<mod1>[.<mod2>...] <dest>[|<dest2>], <src1>, <src2>, ...\n", "func_signal": "def parse(self, text):\n", "code": "text = text.strip()\n\n# Predication\nif text.startswith('@'):\n    # Find and remove predicate string\n    pred_end = text.index(\" \")\n    pred = text[1:pred_end]\n    text = text[pred_end+1:]\n\n    match = re.match(\"(\\!?)\\$p([0-9]+)\\.([0-9a-z]+)\", pred)\n    if not match:\n        raise ValueError(\"Invalid predicate expression %s\" % pred)\n    (neg, pred, cc) = match.groups()\n    # [!]$<pred>.<cc>\n    try:\n        cc = numlookup(condition_codes_rev, cc)\n    except KeyError:\n        raise ValueError(\"Invalid predicate condition code %s\" % cc)\n    if neg == \"!\":\n        # Invert predicate\n        cc = cc ^ 0xF\n    # XXX we should be able to leave out the condition code if we just want\n    # to evaluate the output of a set instruction\n    self.pred_op = cc\n    self.pred = int(pred)\nelse:\n    self.pred_op = 15 # always true\n    self.pred = 0\n\ntry:\n    base_end = text.index(\" \")\n    ins = text[0:base_end]\n    args = text[base_end+1:].split(\",\")\nexcept ValueError:\n    ins = text\n    args = []\nmods = ins.split(\".\")\nself.base = mods[0]\nmods = [\".\"+x for x in mods[1:]]\n\n# Filter out and separate types\ntypes = []\ntypere = re.compile(\"\\.([subf][0-9]+|label)\")\nfor x in mods:\n    if typere.match(x):\n        types.append(x)\n    else:\n        self.modifiers.append(x)\n\n# Parse operands\ntry:\n    dest = args[0]\n    dst_args = [x.strip() for x in dest.split(\"|\")]\nexcept IndexError:\n    dst_args = []\nsrc_args = [x.strip() for x in args[1:]]\n\ni = 0\nfor x in dst_args:\n    op = Operand()\n    if x.startswith(\"$p\"):\n        # predicates have no type specifier\n        type = \"\"\n    else:\n        try:\n            type = types[i]\n        except IndexError:\n            raise ValueError(\"Not enough operand types in instruction\")\n        if len(types)!=1:\n            i += 1\n    op.parse(x, type)\n    self.dst_operands.append(op)\n\nfor x in src_args:\n    op = Operand()\n    if x.startswith(\"$p\"):\n        # predicates have no type specifier\n        type = \"\"\n    else:\n        try:\n            type = types[i]\n        except IndexError:\n            raise ValueError(\"Not enough operand types in instruction\")\n        if len(types)!=1:\n            i += 1\n    op.parse(x, type)\n    self.src_operands.append(op)", "path": "Instruction.py", "repo_name": "laanwj/decuda", "stars": 86, "license": "bsd-3-clause", "language": "python", "size": 662}
{"docstring": "\"\"\"Wrap x in a list if it isn't a list\"\"\"\n", "func_signal": "def wraplist(x):\n", "code": "if isinstance(x, list):\n    return x\nelse:\n    return [x]", "path": "Util.py", "repo_name": "laanwj/decuda", "stars": 86, "license": "bsd-3-clause", "language": "python", "size": 662}
{"docstring": "#rv.write(\"%s\\n\" % (mem))\n", "func_signal": "def const_data(self, rv, mem):\n", "code": "ofs = 0\nwhile ofs < len(mem):\n    rv.write(\"\\x1b[1;30m%04x\\x1b[0m: \" % (ofs*4))\n    try:\n        for x in xrange(0, 4):\n            rv.write(\"%08x \" % mem[ofs+x])\n    except IndexError:\n        pass # expected at end of data\n    rv.write(\"\\n\")\n    ofs += 4", "path": "Formatter.py", "repo_name": "laanwj/decuda", "stars": 86, "license": "bsd-3-clause", "language": "python", "size": 662}
{"docstring": "\"\"\"Load a cubin binary assembly file\"\"\"\n", "func_signal": "def load(name):\n", "code": "f = open(name, \"r\")\n\nex = CubinFile()\ninside = [ex]\nwhile True:\n    line = f.readline()\n    if not line:\n        break\n    line = line[0:-1]\n    if line.strip() == \"\":\n        # Empty line\n        continue\n\n    closebrace = line.rfind(\"}\")\n    openbrace = line.find(\"{\")\n    equalpos = line.find(\"=\")\n    if openbrace != -1:\n        cmd = line[0:openbrace].strip()\n        if closebrace != -1:\n            value = line[openbrace+1:closebrace]\n            setattr(inside[-1], cmd, value)\n        else:\n            #print cmd, \"open\"\n            if cmd == \"code\":\n                kernel = Kernel()\n                inside[-1].kernels.append(kernel)\n                inside.append(kernel)\n            elif cmd == \"bincode\":\n                inst = []\n                inside[-1].bincode = inst\n                inside.append(inst)\n            elif cmd == \"const\":\n                const = Const()\n                inside[-1].const.append(const)\n                inside.append(const)\n            elif cmd == \"mem\":\n                inst = []\n                inside[-1].mem = inst\n                inside.append(inst)\n            elif cmd == \"consts\" or cmd == \"sampler\" or cmd == \"reloc\":\n                # Ignore\n                inside.append(Dummy())\n            elif cmd == \"params_SMEM\":\n                # Ignore\n                inside.append(Dummy())\n            else:\n                raise ValueError(\"Invalid environment %s\" % cmd)\n    elif closebrace != -1:\n        #print inside[-1], \"closed\"\n        inside.pop()\n    elif equalpos != -1:\n        valname = line[0:equalpos].strip()\n        valvalue = line[equalpos+1:].strip()\n        if valname in _numeric:\n            valvalue = int(valvalue)\n        setattr(inside[-1], valname, valvalue)\n    else:\n        # Bincode?\n        inst = line.strip().split(\" \")\n        inst = [int(x,0) for x in inst]\n        inside[-1].extend(inst)\n        #print \"inst\", inst\n\n# Fill in name->kernel map\nfor kernel in ex.kernels:\n    ex.kernels_byname[kernel.name] = kernel\n        \nreturn ex", "path": "CubinFile.py", "repo_name": "laanwj/decuda", "stars": 86, "license": "bsd-3-clause", "language": "python", "size": 662}
{"docstring": "# Predication\n# Condition code\n# What do these mean?\n# do we have a zero bit, sign bit?\n# self.pred_op&3 seems straightforward enough\n# but what is self.pred_op>>2  ?\n", "func_signal": "def dump(self, rv, fmt=Formatter()):\n", "code": "if not self.predicated or self.pred_op == 15 or self.pred_op == None:\n    pass # No self.pred\nelse:\n    fmt.pred(rv, \"@$p%i.%s\" % (self.pred, condition_codes[self.pred_op]))\n#elif self.pred_op == 2:  # 0010\n#    # Execute on false, not on true\n#   fmt.pred(rv, \"@!$p%i\" % self.pred)\n#elif self.pred_op == 5:  # 0101\n#    # Execute on true, not on false\n#    fmt.pred(rv, \"@$p%i\" % self.pred)\n#elif (self.pred_op&3)==2: # xx10  -- seems to be @!$p%i\n#    fmt.pred(rv, \"@!$p%i\" % (self.pred))\n#    self.warnings.append(\"cc is %s\" % condition_codes[self.pred_op])\n#elif (self.pred_op&3)==1: # xx01  -- seems to be @p%i\n#    fmt.pred(rv, \"@$p%i\" % (self.pred))\n#    self.warnings.append(\"cc is %s\" % condition_codes[self.pred_op])\n#elif logic_ops.has_key(self.pred_op):\n#    # unsigned version\n#    fmt.pred(rv, \"@$p%i%s.u\" % (self.pred,logic_ops[self.pred_op]))\n#elif (self.pred_op>=8 and self.pred_op<16) and logic_ops.has_key(self.pred_op-8):\n#    # signed version\n#    fmt.pred(rv, \"@$p%i%s.s\" % (self.pred,logic_ops[self.pred_op-8]))\n#else:\n#    fmt.pred(rv, \"@$p%i.%i\" % (self.pred, self.pred_op))\n#    self.warnings.append(\"cc is %s\" % condition_codes[self.pred_op])\n\n# Base\nif self.base:\n    fmt.base(rv, self.base)\nelif self.system:\n    fmt.base(rv, \"sop.%01x\" % (self.op))\nelse:\n    fmt.base(rv, \"op.%01x%01x\" % (self.op,self.subop))\n# Add instruction modifiers\nfor m in self.modifiers:\n    fmt.modifier(rv, m)\n# Operand types\n# collapse if all are the same\n\n#optypes.extend([x.typestr() for x in self.dst_operands])\n\n# Promote sign of source operands\nsrco = self.dst_operands + self.src_operands\nsrco = [x.clone() for x in srco]\nsign = OP_SIGN_NONE\nfor o in srco:\n    if o.sign != OP_SIGN_NONE:\n        sign = o.sign\nfor o in srco:\n    if o.sign == OP_SIGN_NONE:\n        o.sign = sign\n\n# Add to operand list\noptypes = []\noptypes.extend([x.typestr() for x in srco])\noptypes = [x for x in optypes if x!=\"\"] # Filter empty types (predicates)\noset = set(optypes)\n\nif len(oset) == 1:\n    # There is only one type\n    fmt.types(rv, optypes[0])\nelse:\n    # Show all types\n    fmt.types(rv, \"\".join(optypes))\n# Destination operands\ndst_operands = self.dst_operands[:]\n#if self.ignore_result:\n#    # When ignoring result, only pred register output\n#    dst_operands = [x for x in dst_operands if x.source==OP_SOURCE_PRED_REGISTER]\n# output register 0x7f = bit bucket\n# dst_operands = [x for x in dst_operands if not (x.source==OP_SOURCE_OUTPUT_REGISTER and x.value==0x7f)]\nif len(dst_operands):\n    operands_str = [x.__repr__() for x in dst_operands]\n    fmt.dest_operands(rv, \"|\".join(operands_str))\n# Source operands\nif len(self.src_operands):\n    pre = \"\"\n    if len(self.dst_operands):\n        pre = \", \"\n    operands_str = [x.__repr__() for x in self.src_operands]\n    fmt.src_operands(rv, pre+(\", \".join(operands_str)))\n# Disassembler warnings\nif self.inst != None and self.visited != None:\n    unk0 = self.inst[0] & ~(self.visited[0])\n    if len(self.inst)==2:\n        unk1 = self.inst[1] & ~(self.visited[1])\n    else:\n        unk1 = 0\n    if unk0:\n        fmt.warning(rv, \"unk0 %08x\" % (unk0))\n    if unk1:\n        fmt.warning(rv, \"unk1 %08x\" % (unk1))\n\nfor warn in self.warnings:\n    fmt.warning(rv, warn)", "path": "Instruction.py", "repo_name": "laanwj/decuda", "stars": 86, "license": "bsd-3-clause", "language": "python", "size": 662}
{"docstring": "# Phase 1 -- assemble instructions, fill in addresses\n", "func_signal": "def assemble(self):\n", "code": "bincode = []\nlabel_map = {} \nfor inst in self.instructions:\n    inst.addr = len(bincode)*4   # Fill in addresses\n    inst.assemble()\n    bincode.extend(inst.inst)\n    # Create label map\n    if isinstance(inst, Label):\n        if inst.name in label_map:\n            raise CompilationError(inst.line, \"Duplicate label %s\" % inst.name)\n        label_map[inst.name] = inst\n\n# Phase 2 -- fill in labels\nfor inst in self.instructions:\n    if isinstance(inst, Instruction):\n        dirty = False\n        for o in inst.dst_operands:\n            if o.indirection == OP_INDIRECTION_CODE and o.source == OP_SOURCE_IMMEDIATE:\n                try:\n                    o.value = label_map[o.label].addr\n                except KeyError:\n                    raise CompilationError(inst.line, \"Undefined label %s\" % o.label)\n                dirty = True\n        if dirty:\n            # Relocate\n            inst.assemble()\n            idx = inst.addr>>2\n            bincode[idx:idx+len(inst.inst)] = inst.inst\n\nself.bincode = bincode\n#print self.instructions", "path": "CubinFile.py", "repo_name": "laanwj/decuda", "stars": 86, "license": "bsd-3-clause", "language": "python", "size": 662}
{"docstring": "#rv.write(\"%s\\n\" % (mem))\n", "func_signal": "def const_data(self, rv, mem):\n", "code": "ofs = 0\nwhile ofs < len(mem):\n    rv.write(\"#d.u32 \")\n    rv.write(\", \".join((\"0x%08x\" % x) for x in mem[ofs:ofs+4]))\n    rv.write(\" // %04x\\n\" % (ofs*4))\n    ofs += 4\nrv.write(\"#}\\n\")", "path": "Formatter.py", "repo_name": "laanwj/decuda", "stars": 86, "license": "bsd-3-clause", "language": "python", "size": 662}
{"docstring": "\"\"\"Load a cubin binary assembly file\"\"\"\n", "func_signal": "def load(name):\n", "code": "f = open(name, \"r\")\n\nex = CubinFile()\ninside = [ex]\nwhile True:\n    line = f.readline()\n    if not line:\n        break\n    line = line[0:-1]\n    if line.strip() == \"\":\n        # Empty line\n        continue\n\n    closebrace = line.rfind(\"}\")\n    openbrace = line.find(\"{\")\n    equalpos = line.find(\"=\")\n    if openbrace != -1:\n        cmd = line[0:openbrace].strip()\n        if closebrace != -1:\n            value = line[openbrace+1:closebrace]\n            setattr(inside[-1], cmd, value)\n        else:\n            #print cmd, \"open\"\n            if cmd == \"code\":\n                kernel = Kernel()\n                inside[-1].kernels.append(kernel)\n                inside.append(kernel)\n            elif cmd == \"bincode\":\n                inst = []\n                inside[-1].bincode = inst\n                inside.append(inst)\n            elif cmd == \"consts\" or cmd == \"mem\" or cmd == \"sampler\" or cmd == \"const\":\n                # Ignore\n                inside.append(Dummy())\n            else:\n                raise ValueError(\"Invalid environment %s\" % cmd)\n    elif closebrace != -1:\n        #print inside[-1], \"closed\"\n        inside.pop()\n    elif equalpos != -1:\n        valname = line[0:equalpos].strip()\n        valvalue = line[equalpos+1:].strip()\n        setattr(inside[-1], valname, valvalue)\n    else:\n        # Bincode?\n        inst = line.strip().split(\" \")\n        inst = [int(x,0) for x in inst]\n        inside[-1].extend(inst)\n        #print \"inst\", inst\n        \nreturn ex", "path": "Disass2.py", "repo_name": "laanwj/decuda", "stars": 86, "license": "bsd-3-clause", "language": "python", "size": 662}
{"docstring": "#self.inst  = inst\n\n", "func_signal": "def decode(self):\n", "code": "self.op    = self.bits(0,0xF0000000)\n# Large embedded operand\nif len(self.inst) == 2 and self.bits(1,3)==3:\n    self.immediate = True\nelse:\n    self.immediate = False\n# Flow control instruction\nif self.bits(0,2):\n    self.system = True\nelse:\n    self.system = False\n# Is a second, full word present?\nself.fullinst = (len(self.inst)==2) and not self.immediate\n# Changes some instructions\n# Seems to choose an alternative instruction set, for example,\n# add becomes sub. It is also used on mul24.lo.s32 sometimes\n# but I have no idea what its effect is there.\n# inst.alt = (inst.flags & 1)\n\n# Predication\nif self.fullinst: \n    # Predicated execution\n    self.pred_op = self.bits(1,0x00000F80)\n    self.pred = self.bits(1,0x00003000)\n    # Predication (set)\n    # This should be an operand\n    if self.bits(1,0x0000040):\n        self.dst_operands.append(Operand(\n          OP_TYPE_PRED, OP_SIGN_NONE, 1, OP_SOURCE_PRED_REGISTER, \n          OP_INDIRECTION_NONE, self.bits(1,0x00000030)))\n        \nelse:\n    self.pred_op = None # Don't get in the way\n    \n# Ignore out\n#if self.fullinst:\n#    self.output_reg = self.bits(1,0x0000008)\n#else:\n#    self.output_reg = False\n    \nif self.bits(0,3)==2:\n    self.warnings.append(\"Unknown marker 0.2\")\nif self.fullinst and self.bits(1,3)==2:\n    #self.warnings.append(\"Unknown marker 1.2\")\n    # join point?\n    self.modifiers.append(\".join\")\nif self.fullinst and self.bits(1,3)==1:\n    self.modifiers.append(\".end\")\n    \nif len(self.inst)==1:\n    self.modifiers.append(\".half\")", "path": "Instruction.py", "repo_name": "laanwj/decuda", "stars": 86, "license": "bsd-3-clause", "language": "python", "size": 662}
{"docstring": "\"\"\"Match operand type against operand type rule\"\"\"\n", "func_signal": "def match_type(a,b):\n", "code": "b = (b.type, b.sign, b.size, b.source, b.indirection)\n#print a,b\nfor aa,bb in zip(a,b):\n    if not bb in wraplist(aa):\n        return False\nreturn True", "path": "Assembler.py", "repo_name": "laanwj/decuda", "stars": 86, "license": "bsd-3-clause", "language": "python", "size": 662}
{"docstring": "\"\"\"Alignment shift for data of a certain size\"\"\"\n", "func_signal": "def align_shift(size, val):\n", "code": "if size == 32:\n    if val&3:\n        raise ValueError(\"32 bit operand not aligned to 4 bytes\")\n    return val>>2\nelif size == 16:\n    if val&1:\n        raise ValueError(\"16 bit operand not aligned to 2 bytes\")\n    return val>>1\nelif size == 8:\n    return val>>0\nelse:\n    raise ValueError(\"Invalid data size %i\" % size)", "path": "Assembler.py", "repo_name": "laanwj/decuda", "stars": 86, "license": "bsd-3-clause", "language": "python", "size": 662}
{"docstring": "\"\"\"\nfloat32 x to four-byte representation to uint32\n\"\"\"\n", "func_signal": "def f2i(x):\n", "code": "from struct import pack,unpack\n(x,) = unpack(\"I\", pack(\"f\", x))\nreturn x", "path": "Util.py", "repo_name": "laanwj/decuda", "stars": 86, "license": "bsd-3-clause", "language": "python", "size": 662}
{"docstring": "\"\"\"Write cubin data to f\"\"\"\n# Write intro\n# Write constant segments\n", "func_signal": "def write(self, f):\n", "code": "f.write(\"architecture {sm_10}\\n\")\nf.write(\"abiversion {0}\\n\")\nf.write(\"modname {cubin}\\n\")\n# test zone 0\n#f.write(\"consts  {\\n\")\n#f.write(\"\\tname = ww\\n\")\n#f.write(\"\\tsegname = const\\n\")\n#f.write(\"\\tsegnum = 0\\n\")\n#f.write(\"\\toffset = 0\\n\")\n#f.write(\"\\tbytes = 4\\n\")\n#f.write(\"\\tmem  {\\n\")\n#f.write(\"\\t\\t0x12345678 0x20000010 0x20000020 0x20000030\\n\") # same as for code\n#f.write(\"\\t}\\n\")\n#f.write(\"}\\n\")\n\nfor kernel in self.kernels:\n    f.write(\"code  {\\n\")\n    f.write(\"\\tname = %s\\n\" % kernel.name)\n    f.write(\"\\tlmem = %i\\n\" % kernel.lmem)\n    f.write(\"\\tsmem = %i\\n\" % kernel.smem)\n    f.write(\"\\treg = %i\\n\" % kernel.reg)\n    f.write(\"\\tbar = %i\\n\" % kernel.bar)\n    f.write(\"\\tbincode  {\\n\")\n    # kernel.bincode\n    # up to four 32 bit values per line\n    for i in xrange(0, len(kernel.bincode), 4):\n        f.write(\"\\t\\t\"+(\"\".join([\"0x%08x \" % x for x in kernel.bincode[i:i+4]]))+\"\\n\")\n    f.write(\"\\t}\\n\")\n    # XXX write local constant stuff\n    # test zone 1\n    #f.write(\"\\tconst  {\\n\")\n    #f.write(\"\\t\\tsegname = const\\n\")\n    #f.write(\"\\t\\tsegnum = 1\\n\")\n    #f.write(\"\\t\\toffset = 0\\n\")\n    #f.write(\"\\t\\tbytes = 4\\n\")\n    #f.write(\"\\t\\tmem  {\\n\")\n    #f.write(\"\\t\\t\\t0x56789abc 0x10000010 0x10000020 0x10000030\\n\") # same as for code\n    #f.write(\"\\t\\t}\\n\")\n    #f.write(\"\\t}\\n\")\n\n    f.write(\"}\\n\")", "path": "CubinFile.py", "repo_name": "laanwj/decuda", "stars": 86, "license": "bsd-3-clause", "language": "python", "size": 662}
{"docstring": "\"\"\"\nTry to resolve y as a numberic, then try to\nlook it up a key in dictionary x.\n\"\"\"\n", "func_signal": "def numlookup(x, y):\n", "code": "try:\n    return int(y)\nexcept ValueError:\n    return x[y]", "path": "Util.py", "repo_name": "laanwj/decuda", "stars": 86, "license": "bsd-3-clause", "language": "python", "size": 662}
{"docstring": "\"\"\"Disassemble the cubin instructions in this kernel\"\"\"\n# Phase 1 -- decode instructions\n", "func_signal": "def disassemble(self, rv, formatter=Formatter()):\n", "code": "ptr = 0\ndisa = Disassembler()\ninstructions = []\nwhile ptr < len(self.bincode):\n    base = ptr*4\n    inst = [self.bincode[ptr]]\n    ptr += 1\n    if inst[0] & 1:\n        inst.append(self.bincode[ptr])\n        ptr += 1\n    instructions.append(disa.decode(base, inst))\n\n# Phase 2 -- labels, sort in order of address\nlabel_set = set()\nfor i in instructions:\n    for o in i.dst_operands:\n        if o.indirection == OP_INDIRECTION_CODE and o.source == OP_SOURCE_IMMEDIATE:\n            label_set.add(o.value)\nlabels = list(label_set)\nlabels.sort()\nlabel_map = dict([(l, \"label%i\" % x) for x,l in enumerate(labels)])\n\n# Phase 3 -- fill in labels in program arguments\nfor i in instructions:\n    for o in i.dst_operands:\n        if o.indirection == OP_INDIRECTION_CODE and o.source == OP_SOURCE_IMMEDIATE:\n            o.label = label_map[o.value]\n            \n# Phase 4 -- print\nfor i in instructions:\n    formatter.address(rv, i.address)\n    \n    formatter.bincode(rv, (\" \".join([\"%08x\" % x for x in i.inst])))\n    if i.address in label_map:\n        formatter.label(rv, label_map[i.address])\n    i.dump(rv, formatter)\n    formatter.newline(rv)\n\n# Phase 5 -- print constants\nfor seg in self.const:\n    formatter.const_hdr(rv, seg.segname, seg.segnum, seg.offset, seg.bytes)\n    formatter.const_data(rv, seg.mem)", "path": "CubinFile.py", "repo_name": "laanwj/decuda", "stars": 86, "license": "bsd-3-clause", "language": "python", "size": 662}
{"docstring": "# xx fullinst and print_psize2\n", "func_signal": "def print_psize2(self, rv, inst, fullinst):\n", "code": "if fullinst:\n    x = inst[1] & 0x00008000\nelse:\n    x = inst[0] & 0x00008000\nrv.write(\".\")\nif x:\n    rv.write(\"s32\")\nelse:\n    rv.write(\"u32\")", "path": "Disass2.py", "repo_name": "laanwj/decuda", "stars": 86, "license": "bsd-3-clause", "language": "python", "size": 662}
{"docstring": "\"\"\"Disassemble the cubin instructions in this kernel\"\"\"\n", "func_signal": "def disassemble(self):\n", "code": "rv = StringIO()\nptr = 0\nwhile ptr < len(self.bincode):\n    base = ptr*4\n    inst = [self.bincode[ptr]]\n    ptr += 1\n    if inst[0] & 1:\n        inst.append(self.bincode[ptr])\n        ptr += 1\n    rv.write(\"%04x: %-17s \" % (base, \" \".join([\"%08x\" % x for x in inst])))\n    op    = (inst[0]&0xF0000000)>>28\n    # Misc flags\n    flags = (inst[0]&0x0FC00000)>>22\n    if len(inst)==1:\n        subop = 0\n        twoword = False\n    else:\n        subop = (inst[1]&0xE0000000)>>29\n        twoword = True\n    \n    if len(inst)==2 and (inst[1] & 3)==3:\n        immediate = True\n    else:\n        immediate = False\n    if inst[0] & 2: \n        system = True # flow control\n    else:\n        system = False\n    # Is the second instruction present?\n    fullinst = twoword and not immediate\n    if fullinst: # subsubop\n        lop = (inst[1]&0x001fc000)>>14\n    else:\n        lop = -1\n    # Changes some instructions\n    # Seems to choose an alternative instruction set, for example,\n    # add becomes sub. It is also used on mul24.lo.s32 sometimes\n    # but I have no idea what its effect is there.\n    alt = flags&1\n    \n    print_msize = False\n    print_psize = False\n    print_psize2 = False\n    swap_oper = False\n    type_float = False\n    is_bra = False\n    oper1_t = \"r\" # Operand type u(unknown), r(reg), i(imm), p(pointer)\n    oper2_t = \"r\"\n    oper3_t = \"r\"\n    oper4_t = None\n    \n    # Predication\n    if fullinst:\n        onpred = (inst[1]&0x00000780)>>7\n        pred = (inst[1]&0x00003000)>>12\n        if onpred == 15 or onpred == 0:\n            pass # No pred\n        elif onpred == 2:\n            # Execute on false\n            rv.write(\"@!p%i \" % pred)\n        elif onpred == 5:\n            # Execute on true\n            rv.write(\"@p%i \" % pred)\n        else:\n            # ??\n            rv.write(\"@?%i?p%i \" % (onpred,pred))\n    \n    # Print the main instruction\n    if system:\n        oper1_t = None\n        oper2_t = None\n        oper3_t = None\n        oper4_t = None\n        flags = 0\n        alt = 0\n\n        # Special instructions\n        if op == 0x1:\n            rv.write(\"bra\")\n            is_bra = True\n        elif op == 0x2:\n            rv.write(\"call\")\n            is_bra = True\n        elif op == 0x3:\n            rv.write(\"return\")\n        elif op == 0x8:\n            rv.write(\"bar.sync\") \n            # 0x0003xfff where x is the barrier id, 0-15\n            # The other bits are always like this...\n        elif op == 0x9:\n            rv.write(\"trap\")\n        elif op == 0xA:\n            rv.write(\"join\")  # join point for divergent threads\n            is_bra = True\n        else:\n            rv.write(\"?%i?\" % op)\n    elif op == 0x0:\n        if subop == 0x3: # Load internal value\n            rv.write(\"ldgpu\")\n            if lop == 0:\n                rv.write(\".physid\")\n            elif lop == 1:\n                rv.write(\".clock\")\n            elif lop >= 4 and lop < 8:\n                rv.write(\".pm%i\" % (lop-4))\n            else:\n                rv.write(\".??\")\n        elif subop == 0x6: \n            rv.write(\"ld.offset1.shl\")\n            oper3_t = \"i\"\n        elif subop == 0x7: \n            rv.write(\"st.shared\")\n            # TODO: oper1 extends into oper3 for large values\n            oper4_t = \"r\"\n            oper3_t = None\n            # long offset in word 1\n            # 8 bit reads offset is *4\n            # 16 bit read offset is *2\n            # 32 bit reads offset is *1\n            # always loading at offset 60\n            type = (inst[1]&0x07E00000) >> 21\n            if type == 0:\n                rv.write(\".b16\")\n            elif type == 33:\n                rv.write(\".b32\")\n            elif type == 2:\n                rv.write(\".b8\")\n            else:\n                rv.write(\".?%i?\" % type)\n            if lop == 0:\n                # Absolute offset\n                rv.write(\".abs\")\n            elif lop == 1:\n                # Register offset\n                if offsetr: # Use offset register\n                    oper2_t = \"pro\"\n                else:\n                    oper2_t = \"pr\"\n            oper4_t = None\n            #print offsetr\n            #flags = 0 # flags contain something else?\n        else:\n            rv.write(\"????\")\n    elif op == 0x1:\n        print_psize = True\n        if subop == 0x0:\n            if lop == 0xF or lop == -1:\n                rv.write(\"ld\")\n            \n                if immediate:\n                    oper1_t = None\n                else:\n                    oper2_t = \"r\"\n                    oper3_t = None\n            else:\n                rv.write(\"?%i?\" % lop)\n            \n        elif subop == 0x1:\n            rv.write(\"ld.const\")  # Load const from offset into register\n            # TODO: oper1 extends into oper3 for large values\n            # TODO: use bit inst[1]&0x00400000 to determine segment to load from\n            if (inst[1]&0x00400000):\n                rv.write(\"1\")\n            else:\n                rv.write(\"0\")\n            oper4_t = None\n            oper3_t = None\n            oper1_t = \"pi\"\n        else:\n            rv.write(\"????\")\n    elif op == 0x2:\n        print_psize = True\n        if subop == 0x0:\n            if alt:\n                alt = False # reset alt flag, as we know the effect in this case\n                rv.write(\"sub\")\n            else:\n                rv.write(\"add\")\n            if fullinst and (inst[1]&0x08000000):\n                # .s32 also signifies saturation\n                rv.write(\".sat\")\n            if fullinst:\n                oper4_t = \"r\"\n                oper3_t = None\n        else:\n            rv.write(\"????\")\n    elif op == 0x3:\n        print_psize = True\n        \n        if subop == 0: # seems to be a sub with arguments swapped\n            rv.write(\"neg\")\n            if fullinst: # for some reason, the arguments for this instruction are oper1,oper2,oper4\n                oper3_t = None\n                oper4_t = \"r\"\n        elif subop == 3:\n            rv.write(\"set\")\n            rv.write(lookup(logic_ops, lop))\n        elif subop == 4:\n            rv.write(\"max\")\n        elif subop == 5:\n            rv.write(\"min\")\n        elif subop == 6:\n            rv.write(\"shl\")\n        elif subop == 7:\n            rv.write(\"shr\")\n        else:\n            rv.write(\"????\")\n    elif op == 0x4:\n        #print_psize = True\n        if subop == 0:\n            rv.write(\"mul24\")  # XXX hi/lo word\n            print_psize2 = True # based on bit 1 of lop\n            if lop == 0:\n                print_psize2 = False\n                rv.write(\".u32.u16\")\n            elif lop == 4 or lop == 6 or lop == -1: # no lop specified\n                rv.write(\".lo\")\n            elif lop == 5 or lop == 7:\n                rv.write(\".hi\")\n            else:\n                rv.write(\".?%i?\" % lop)\n            \n        else:\n            rv.write(\"????\")\n    elif op == 0x5:\n        if subop == 0:\n            rv.write(\"sad\") # Sum of Absolute Differences\n            oper4_t = \"r\"\n            print_psize = True\n        else:\n            rv.write(\"????\")\n    elif op == 0x6:\n        if subop == 0:\n            rv.write(\"mad24.u32.u16\")\n            oper4_t = \"r\"\n        elif subop == 3:\n            rv.write(\"mad24.lo.u32\")\n            oper4_t = \"r\"\n        elif subop == 4:\n            rv.write(\"mad24.lo.s32\")\n            oper4_t = \"r\"\n        elif subop == 5:\n            rv.write(\"mad24.lo.sat.s32\")\n            oper4_t = \"r\"\n        elif subop == 6:\n            rv.write(\"mad24.hi.u32\")\n            oper4_t = \"r\"\n        elif subop == 7:\n            rv.write(\"mad24.hi.s32\")\n            oper4_t = \"r\"\n        else:\n            rv.write(\"????\")\n        # mad24.hi.sat.s32 runs over to op==0x7 subop==0x0, but I assume \n        # this is a ptxas bug, as this instruction makes no sense.\n    elif op == 0x9:\n        type_float = True\n        if subop == 0:\n            rv.write(\"rcp\")\n        elif subop == 2:\n            rv.write(\"rsqrt\")\n        elif subop == 3:\n            rv.write(\"lg2\")\n        elif subop == 4:\n            rv.write(\"sin\")\n        elif subop == 5:\n            rv.write(\"cos\")\n        elif subop == 6:\n            rv.write(\"ex2\")\n        else:\n            rv.write(\"????\")\n    elif op == 0xa: # Conversion ops\n        oper3_t = None\n        if subop == 0:\n            rv.write(\"cvt\")\n            types = [\".u16\",\".u32\",\".u8\",\"??\",\".s16\",\".s32\",\".s8\",\"??\"]\n            self.print_psize(rv, inst, fullinst)\n            rv.write(types[lop&7])\n        elif subop == 1:  # Used in div implementation\n            rv.write(\"????\")\n        elif subop == 2:\n            rv.write(\"cvt\")\n            rops = [\".rn\",\".rm\",\".rp\",\".rz\"]\n            types = [\".u16\",\".u32\",\".u8\",\"??\",\".s16\",\".s32\",\".s8\",\"??\"]\n            rv.write(rops[lop>>3])\n            rv.write(\".f32\")\n            rv.write(types[lop&7])\n        elif subop == 3: # saturate or scale?\n            rv.write(\"sat.f32.u32\") # conversion; int 0xFFFFFFFF to float 1.0f\n        elif subop == 4:\n            rv.write(\"cvt\")\n            rops = [\".rni\",\".rmi\",\".rpi\",\".rzi\"]\n            if (lop&7)!=1:\n                rv.write(\"?\") # Unknown conversion\n            rv.write(rops[lop>>3])\n            self.print_psize(rv, inst, fullinst)\n            rv.write(\".f32\")\n        elif subop == 6:\n            rops = [\".rn\",\".rm\",\".rp\",\".rz\"]\n            if (lop&7)!=1:\n                rv.write(\"?\") # Unknown conversion\n            rv.write(\"cvt\")\n            rv.write(lookup(rops, (lop>>3)&3))\n            if (inst[1]&0x08000000):\n                rv.write(\"i\") # integer rounding mode\n            if (lop&32):\n                rv.write(\".sat\")\n            if (lop&64):\n                rv.write(\".abs\")\n            rv.write(\".f32.f32\")\n            \n        elif subop == 7:\n            if lop == 1:\n                rv.write(\"neg\")\n                type_float = True\n            else:\n                rv.write(\"????\")\n        else:\n            rv.write(\"????\")\n    elif op == 0xb:\n        type_float = True\n        if subop == 0:\n            # This determines the size of the operand (16 or 32 bit) for\n            # integer instructions. For floating point instructions it \n            # has a different meaning.\n            if fullinst:\n                sizebit = (inst[1]&0x08000000)\n            else:\n                sizebit = (inst[0]&0x00008000)\n            if sizebit and alt:\n                rv.write(\"????\") # an add too?\n            elif alt:\n                rv.write(\"sub\") # d,a,b -> d = a-b\n            elif sizebit:\n                rv.write(\"sub2\") # arguments reversed d,a,b -> d = b-a\n            else:\n                rv.write(\"add\")\n            if fullinst: # for some reason, the arguments for this instruction are oper1,oper2,oper4\n                oper3_t = None\n                oper4_t = \"r\" \n            #oper3_t = \"i\"\n            alt = False\n        elif subop == 3:\n            rv.write(\"set\")\n            rv.write(lookup(logic_ops, lop))\n        elif subop == 4:\n            rv.write(\"max\")\n        elif subop == 5:\n            rv.write(\"min\")\n        elif subop == 6: # I don't know what this operation does, but is executed before sin, cos, and lg2\n            # Denormalize maybe?\n            rv.write(\"presin\")\n            oper3_t = None\n        else:\n            rv.write(\"????\")\n    elif op == 0xc:\n        if subop == 0:\n            type_float = True\n            rv.write(\"mul\")\n            if lop == 0 or lop == -1:\n                rv.write(\".rn\")\n            elif lop == 3:\n                rv.write(\".rz\")\n            else:\n                rv.write(\".??\")\n        elif subop == 2:\n            oper4_t = \"r\"\n            rv.write(\"slct\") # Select one of both arguments based on value of oper4\n        else:\n            rv.write(\"????\")\n    elif op == 0xd:\n        if subop == 0x0:\n            ssop = (inst[1]&0x0000C000)>>14\n            print_psize = True\n            \n            rv.write(lookup(d0_ops,lop))\n        elif subop == 0x1:\n            rv.write(\"ld.offset0\")\n            # xx inst[0] contains the offset in the area oper1 and oper3 generally are\n            # oper5?\n        elif subop == 0x2:\n            print_msize = True\n            oper1_t = \"pr\"\n            oper2_t = \"r\"\n            oper3_t = None\n            rv.write(\"ld.local\")\n        elif subop == 0x3:\n            print_msize = True\n            swap_oper = True\n            oper1_t = \"pr\"\n            oper2_t = \"r\"\n            oper3_t = None\n            rv.write(\"st.local\")\n        elif subop == 0x4:\n            print_msize = True\n            oper1_t = \"pr\"\n            oper2_t = \"r\"\n            oper3_t = None\n            rv.write(\"ld.global\")\n        elif subop == 0x5:\n            print_msize = True\n            oper1_t = \"pr\"\n            oper2_t = \"r\"\n            oper3_t = None\n            swap_oper = True # swap src and dest\n            rv.write(\"st.global\")\n        elif subop == 0x6: # atomic, ignore output\n            rv.write(\"atom.global.\")\n            rv.write(lookup(atomic_ops,(inst[1]&0x0000003C)>>2))\n            print_msize = True\n        elif subop == 0x7: # atomic, provide out reg\n            rv.write(\"atom.global.out.\")\n            rv.write(lookup(atomic_ops,(inst[1]&0x0000003C)>>2))\n            print_msize = True\n        else:\n            rv.write(\"????\")\n    elif op == 0xe:\n        if subop == 0x0:\n            rv.write(\"mad\")\n            type_float = True\n            oper4_t = \"r\"\n        else:\n            rv.write(\"????\")\n    elif op == 0xf:\n        if subop == 0x0:\n            rv.write(\"tex\")\n            op = (inst[0]&0x03C00000)>>22\n            op2 = (inst[0]&0x04000000)\n            # 0 is also 1d, 1 is also 2d ...\n            if op == 0x8: # tex type\n                rv.write(\".1d.f32.f32\")\n            elif op == 0x9:\n                rv.write(\".2d.f32.f32\")\n            elif op == 0xa:\n                rv.write(\".3d.f32.f32\")\n            elif op == 0xc: \n                rv.write(\".1d.f32.s32\")\n            elif op == 0xe:\n                rv.write(\".2d.f32.s32\")\n            elif op == 0xf:\n                rv.write(\".3d.f32.s32\")\n            else:\n                rv.write(\"?%i?\" % op)\n            if(op2): # Don't know what this bit does\n                rv.write(\".?\")\n            flags = 0 # flags are useless here as the bits contain something else\n            alt = 0\n        elif subop == 0x7:\n            rv.write(\"nop\")\n        else:\n            rv.write(\"????\")\n    else:\n        # unknown ops: 0x7, 0x8\n        rv.write(\"[op %01x subop %01x]\" % (op,subop))\n\n    if fullinst and print_msize: # has second word, not immediate\n        opsize = (inst[1] & 0x00E00000)>>21\n        rv.write(msize[opsize])\n    # size and sign of data, if not immediate\n    if print_psize:\n        self.print_psize(rv, inst, fullinst)\n    if print_psize2:\n        self.print_psize2(rv, inst, fullinst)\n    if type_float:\n        rv.write(\".f32\")\n        \n    rv.write(\" \")\n    if alt:\n        rv.write(\"[alt] \") \n\n    # Extract and print operands\n    if oper3_t != None and (flags&2):\n        # operand 3 comes from constant (in segment 0)\n        oper3_t = \"z\"\n    if fullinst and (inst[1]&0x00400000):\n        #rv.write(\"[co3] \")\n        # operand 4 comes from constant in segment 1\n        if oper4_t != None:\n            oper4_t = \"x\"\n        elif oper3_t != None:\n            oper3_t = \"x\"\n    \n    #if flags&4:\n    #    # operand 1 comes from param\n    #    #rv.write(\"[src from param?] \")\n    #    oper1_t = \"y\" # from param\n    #    oper1 += 0x30\n    # ld.offset flags 10 means:\n    #    multiply src with 4\n    #    offset from \n    if (flags&(~(4|2|1))):\n        rv.write(\"[flags 0x%02x] \" % flags)\n    if fullinst: # Depending on this, source is 7 or 6 bits\n        oper1 = (inst[0]&0x0000FE00)>>9\n        oper2 = (inst[0]&0x000001FC)>>2\n    else:\n        oper1 = (inst[0]&0x00007E00)>>9\n        oper2 = (inst[0]&0x000001FC)>>2\n    if fullinst:\n        oper4 = (inst[1]&0x001fc000)>>14\n    else:\n        oper4 = None\n    # seems that bit 6 of src has another meaning when this is a immediate\n    # instruction (namely \"32 bit\")\n    if oper3_t != None and fullinst and (inst[1]&0x00100000):\n        # Operand 3 is immediate\n        oper3_t = \"i\"\n    #else:\n    #    oper3_t = \"r\"\n    \n    # Process parameter numbers in shared memory\n    if fullinst and (inst[1]&0x00200000) and not print_msize:\n        if (flags&0x10):\n            oper1_t = \"yo\" # from offset reg\n        else:\n            oper1_t = \"y\"\n    elif flags&4:\n        oper1_t = \"yw\"\n        #if oper1 >= 0x30: # Weird mapping for 32 bit\n        #    oper1 += 0x30\n        #else:\n        #    oper1 += 0x10\n\n    if immediate:\n        # Immediate data\n        oper3 = ((inst[1]&0x0FFFFFFC)<<4) | ((inst[0]&0x003F0000)>>16)\n        oper3_t = \"i\"\n        #rv.write(\"[imm %08x]\" % imm)\n    else:\n        oper3 = ((inst[0]&0x003F0000)>>16)\n        #rv.write(\"[imm %02x]\" % imm)\n    if swap_oper: # to, from\n        if oper1_t != None:\n            self.print_oper(rv, oper1, oper1_t)\n        if oper2_t != None:\n            rv.write(\", \")\n            self.print_oper(rv, oper2, oper2_t)\n    else: \n        if oper2_t != None:\n            self.print_oper(rv, oper2, oper2_t)\n        if oper1_t != None:\n            rv.write(\", \")\n            self.print_oper(rv, oper1, oper1_t)\n    if oper3_t != None:\n        rv.write(\", \")\n        #if flipsrc2:\n        #    rv.write(\"-\")\n        self.print_oper(rv, oper3, oper3_t)\n    if oper4 != None and oper4_t != None:\n        rv.write(\", \")\n        self.print_oper(rv, oper4, oper4_t)\n    if is_bra:\n        # Branch instruction, divide address for convenience\n        # I know the address is longer, as CUDA can address up to\n        # 2Mb of kernel instructions, but I have never been \n        # able to generate a kernel this big without crashing the\n        # ptxas. Probably, the higher part is in inst[1].\n        addr = (inst[0]&0x0FFFFE00)>>9\n        if addr&3:\n            rv.write(\"!nonaligned!\")\n        rv.write(\"0x%08x\" % (addr)) \n    elif system:\n        addr = (inst[0]&0x0FFFFE00)>>9\n        rv.write(\"0x%08x \" % (addr)) \n\n    # Offset (integrate this into operand)\n    if flags&0x10:\n        offr = (flags&0x08)>>3\n        rv.write(\" +ofs%i\" % (offr))\n\n    # Predication (set)\n    if fullinst and (inst[1]&0x0000040):\n        pred = (inst[1]&0x00000030)>>4\n        rv.write(\", p%i\" % pred)\n               \n\n    # Bits still unknown \n    # inst[0]:  \n    #   0xF0000000 op\n    #   0x0FC00000 flags\n    #   0x04000000 use offset register\n    #   0x02000000 \\- offset register 0/1\n    #   0x01000000 oper3 from constant\n    #   0x00800000 oper2 from parameter (offset 0x30)\n    #   0x00400000 alternative instruction, ie add->sub\n    #   0x003F0000 oper3 (full instruction)\n    #   0x001F0000 oper3 (short instruction or immediate)\n    #   0x0000FE00 oper2\n    #   0x000001FC oper1\n    #   0x00000002 0=normal, 1=system (flow control)\n    #   0x00000001 0=32bit, 1=64 bit\n    # inst[1]:  \n    #   0xE0000000 subop\n    #   0x08000000 signed/unsigned\n    #   0x04000000 32/16\n    #   0x01000000 oper3 is immediate\n    #   0x00E00000 type, on load instructions\n    #   0x00400000 oper3/4 from constant in segment 1\n    #   0x00200000 oper2 from parameter\n    #   0x001FC000 oper4 or sub-sub op\n    #   0x00003000 predicate to act on\n    #   0x00000400 ?? (usually set, unless predicated)\n    #   0x00000200 execute on pred\n    #   0x00000100 execute on !pred\n    #   0x00000080 ?? (usually set, unless predicated)\n    #   0x00000040 set predicate\n    #   0x00000030 predicate to set\n    #   0x0000003C atomic op\n    #   0x00000003 marker (0=normal,1=end,2???,3=immediate)\n    # to find: predication\n    \n    rv.write(\"\\n\")\nreturn rv.getvalue()", "path": "Disass2.py", "repo_name": "laanwj/decuda", "stars": 86, "license": "bsd-3-clause", "language": "python", "size": 662}
{"docstring": "# Find overall type/sign/size of instruction\n", "func_signal": "def default_oper_type(self, oper_type):\n", "code": "if oper_type == OPER_MSIZE:\n    type = OP_TYPE_INT\n    (sign, size) = _msize[self.bits(1,0x00E00000)]\nelif oper_type == OPER_PSIZE:\n    type = OP_TYPE_INT\n    if self.fullinst:\n        if self.bits(1,0x08000000):\n            sign = OP_SIGN_SIGNED\n        else:\n            sign = OP_SIGN_UNSIGNED\n        if self.bits(1,0x04000000):\n            size = 32\n        else:\n            size = 16\n    else:\n        sign = OP_SIGN_NONE\n        if self.bits(0,0x00008000):\n            size = 32\n        else:\n            size = 16\nelif oper_type == OPER_PSIZE3: # psize, width only\n    type = OP_TYPE_INT\n    sign = OP_SIGN_NONE\n    if self.fullinst:\n        if self.bits(1,0x04000000):\n            size = 32\n        else:\n            size = 16\n    else:\n        if self.bits(0,0x00008000):\n            size = 32\n        else:\n            size = 16\nelif oper_type == OPER_FLOAT:\n    sign = OP_SIGN_NONE\n    size = 32\n    type = OP_TYPE_FLOAT\nelif oper_type == OPER_FLOAT16:\n    sign = OP_SIGN_NONE\n    size = 16\n    type = OP_TYPE_FLOAT\nelif oper_type == OPER_FLOAT64:\n\tsign = OP_SIGN_NONE\n\tsize = 64\n\ttype = OP_TYPE_FLOAT\nelif oper_type == OPER_VARFLOAT:\n    sign = OP_SIGN_NONE\n    if self.bits(1, 0x04000000):\n        size = 32\n    elif self.bits(1, 0x00400000):\n        size = 64\n    else:\n        size = 16\n    type = OP_TYPE_FLOAT\nreturn (sign, size, type)", "path": "Instruction.py", "repo_name": "laanwj/decuda", "stars": 86, "license": "bsd-3-clause", "language": "python", "size": 662}
{"docstring": "\"\"\"\nMake sure a session is open.\n\nIf it's not and autosession is turned on, create a new session automatically.\nIf it's not and autosession is off, raise an exception.\n\"\"\"\n", "func_signal": "def check_session(self):\n", "code": "if self.session is None:\n    if self.autosession:\n        self.open_session()\n    else:\n        msg = \"must open a session before modifying %s\" % self\n        raise RuntimeError(msg)", "path": "simserver\\simserver.py", "repo_name": "RaRe-Technologies/gensim-simserver", "stars": 107, "license": "agpl-3.0", "language": "python", "size": 233}
{"docstring": "# index some docs first\n", "func_signal": "def test_query_id(self):\n", "code": "self.server.drop_index(keep_model=False)\nself.server.train(self.docs, method='lsi')\nself.server.index(self.docs)\n\n# query index by id: return the most similar documents to an already indexed document\ndocid = self.docs[0]['id']\nexpected = [('en__0', 1.0), ('en__2', 0.112942614), ('en__1', 0.09881371),\n            ('en__3', 0.087866522)]\ngot = self.server.find_similar(docid)\nself.check_equal(expected, got)\n\n# same thing, but only get docs with similarity >= 0.3\nexpected = [('en__0', 1.0)]\ngot = self.server.find_similar(docid, min_score=0.3)\nself.check_equal(expected, got)\n\n# same thing, but only get max 3 documents docs with similarity >= 0.2\nexpected = [('en__0', 1.0), ('en__2', 0.112942614)]\ngot = self.server.find_similar(docid, max_results=3, min_score=0.1)\nself.check_equal(expected, got)", "path": "simserver\\test\\test_simserver.py", "repo_name": "RaRe-Technologies/gensim-simserver", "stars": 107, "license": "agpl-3.0", "language": "python", "size": 233}
{"docstring": "\"\"\"Merge two precomputed similarity lists, truncating the result to `clip` most similar items.\"\"\"\n", "func_signal": "def merge_sims(oldsims, newsims, clip=None):\n", "code": "if oldsims is None:\n    result = newsims or []\nelif newsims is None:\n    result = oldsims\nelse:\n    result = sorted(oldsims + newsims, key=lambda item: -item[1])\nif clip is not None:\n    result = result[:clip]\nreturn result", "path": "simserver\\simserver.py", "repo_name": "RaRe-Technologies/gensim-simserver", "stars": 107, "license": "agpl-3.0", "language": "python", "size": 233}
{"docstring": "\"\"\"Drop all indexed documents from the session. Optionally, drop model too.\"\"\"\n", "func_signal": "def drop_index(self, keep_model=True):\n", "code": "self.check_session()\nresult = self.session.drop_index(keep_model)\nif self.autosession:\n    self.commit()\nreturn result", "path": "simserver\\simserver.py", "repo_name": "RaRe-Technologies/gensim-simserver", "stars": 107, "license": "agpl-3.0", "language": "python", "size": 233}
{"docstring": "\"\"\"Update id->pos mapping with new document ids.\"\"\"\n", "func_signal": "def update_ids(self, docids):\n", "code": "logger.info(\"updating %i id mappings\" % len(docids))\nfor docid in docids:\n    if docid is not None:\n        pos = self.id2pos.get(docid, None)\n        if pos is not None:\n            logger.info(\"replacing existing document %r in %s\" % (docid, self))\n            del self.pos2id[pos]\n        self.id2pos[docid] = self.length\n        try:\n            del self.id2sims[docid]\n        except:\n            pass\n    self.length += 1\nself.id2sims.sync()\nself.update_mappings()", "path": "simserver\\simserver.py", "repo_name": "RaRe-Technologies/gensim-simserver", "stars": 107, "license": "agpl-3.0", "language": "python", "size": 233}
{"docstring": "# index some docs first\n", "func_signal": "def test_query_document(self):\n", "code": "self.server.drop_index(keep_model=False)\nself.server.train(self.docs, method='lsi')\nself.server.index(self.docs)\n\n# query index by document text: id is ignored\ndoc = self.docs[0]\ndoc['id'] = None # clear out id; not necessary, just to demonstrate it's not used in query-by-document\nexpected = [('en__0', 1.0), ('en__2', 0.11294261), ('en__1', 0.09881371), ('en__3', 0.087866522)]\ngot = self.server.find_similar(doc)\nself.check_equal(expected, got)\n\n# same thing, but only get docs with similarity >= 0.3\nexpected =  [('en__0', 1.0)]\ngot = self.server.find_similar(doc, min_score=0.3)\nself.check_equal(expected, got)\n\n# same thing, but only get max 3 documents docs with similarity >= 0.2\nexpected =  [('en__0', 1.0), ('en__2', 0.112942614)]\ngot = self.server.find_similar(doc, max_results=3, min_score=0.1)\nself.check_equal(expected, got)", "path": "simserver\\test\\test_simserver.py", "repo_name": "RaRe-Technologies/gensim-simserver", "stars": 107, "license": "agpl-3.0", "language": "python", "size": 233}
{"docstring": "\"\"\"\nAll data will be stored under directory `basename`. If there is a server\nthere already, it will be loaded (resumed).\n\nThe server object is stateless in RAM -- its state is defined entirely by its location.\nThere is therefore no need to store the server object.\n\"\"\"\n", "func_signal": "def __init__(self, basename, use_locks=False):\n", "code": "if not os.path.isdir(basename):\n    raise ValueError(\"%r must be a writable directory\" % basename)\nself.basename = basename\nself.use_locks = use_locks\nself.lock_update = threading.RLock() if use_locks else gensim.utils.nocm\ntry:\n    self.fresh_index = SimIndex.load(self.location('index_fresh'))\nexcept:\n    logger.debug(\"starting a new fresh index\")\n    self.fresh_index = None\ntry:\n    self.opt_index = SimIndex.load(self.location('index_opt'))\nexcept:\n    logger.debug(\"starting a new optimized index\")\n    self.opt_index = None\ntry:\n    self.model = SimModel.load(self.location('model'))\nexcept:\n    self.model = None\nself.payload = SqliteDict(self.location('payload'), autocommit=True, journal_mode=JOURNAL_MODE)\nself.flush(save_index=False, save_model=False, clear_buffer=True)\nlogger.info(\"loaded %s\" % self)", "path": "simserver\\simserver.py", "repo_name": "RaRe-Technologies/gensim-simserver", "stars": 107, "license": "agpl-3.0", "language": "python", "size": 233}
{"docstring": "\"\"\"\nTurn autosession (automatic committing after each modification call) on/off.\nIf value is None, only query the current value (don't change anything).\n\"\"\"\n", "func_signal": "def set_autosession(self, value=None):\n", "code": "if value is not None:\n    self.rollback()\n    self.autosession = value\nreturn self.autosession", "path": "simserver\\simserver.py", "repo_name": "RaRe-Technologies/gensim-simserver", "stars": 107, "license": "agpl-3.0", "language": "python", "size": 233}
{"docstring": "\"\"\"Merge documents from the other index. Update precomputed similarities\nin the process.\"\"\"\n", "func_signal": "def merge(self, other):\n", "code": "other.qindex.normalize, other.qindex.num_best = False, self.topsims\n# update precomputed \"most similar\" for old documents (in case some of\n# the new docs make it to the top-N for some of the old documents)\nlogger.info(\"updating old precomputed values\")\npos, lenself = 0, len(self.qindex)\nfor chunk in self.qindex.iter_chunks():\n    for sims in other.qindex[chunk]:\n        if pos in self.pos2id:\n            # ignore masked entries (deleted, overwritten documents)\n            docid = self.pos2id[pos]\n            sims = self.sims2scores(sims)\n            self.id2sims[docid] = merge_sims(self.id2sims[docid], sims, self.topsims)\n        pos += 1\n        if pos % 10000 == 0:\n            logger.info(\"PROGRESS: updated doc #%i/%i\" % (pos, lenself))\nself.id2sims.sync()\n\nlogger.info(\"merging fresh index into optimized one\")\npos, docids = 0, []\nfor chunk in other.qindex.iter_chunks():\n    for vec in chunk:\n        if pos in other.pos2id: # don't copy deleted documents\n            self.qindex.add_documents([vec])\n            docids.append(other.pos2id[pos])\n        pos += 1\nself.qindex.save()\nself.update_ids(docids)\n\nlogger.info(\"precomputing most similar for the fresh index\")\npos, lenother = 0, len(other.qindex)\nnorm, self.qindex.normalize = self.qindex.normalize, False\ntopsims, self.qindex.num_best = self.qindex.num_best, self.topsims\nfor chunk in other.qindex.iter_chunks():\n    for sims in self.qindex[chunk]:\n        if pos in other.pos2id:\n            # ignore masked entries (deleted, overwritten documents)\n            docid = other.pos2id[pos]\n            self.id2sims[docid] = self.sims2scores(sims)\n        pos += 1\n        if pos % 10000 == 0:\n            logger.info(\"PROGRESS: precomputed doc #%i/%i\" % (pos, lenother))\nself.qindex.normalize, self.qindex.num_best = norm, topsims\nself.id2sims.sync()", "path": "simserver\\simserver.py", "repo_name": "RaRe-Technologies/gensim-simserver", "stars": 107, "license": "agpl-3.0", "language": "python", "size": 233}
{"docstring": "\"\"\"Index documents, in the current session\"\"\"\n", "func_signal": "def index(self, *args, **kwargs):\n", "code": "self.check_session()\nresult = self.session.index(*args, **kwargs)\nif self.autosession:\n    self.commit()\nreturn result", "path": "simserver\\simserver.py", "repo_name": "RaRe-Technologies/gensim-simserver", "stars": 107, "license": "agpl-3.0", "language": "python", "size": 233}
{"docstring": "\"\"\"\nFind the most similar documents to a given vector (=already processed document).\n\"\"\"\n", "func_signal": "def sims_by_vec(self, vec, normalize=None):\n", "code": "if normalize is None:\n    normalize = self.qindex.normalize\nnorm, self.qindex.normalize = self.qindex.normalize, normalize # store old value\nself.qindex.num_best = self.topsims\nsims = self.qindex[vec]\nself.qindex.normalize = norm # restore old value of qindex.normalize\nreturn self.sims2scores(sims)", "path": "simserver\\simserver.py", "repo_name": "RaRe-Technologies/gensim-simserver", "stars": 107, "license": "agpl-3.0", "language": "python", "size": 233}
{"docstring": "\"\"\"Return indexed vector corresponding to document `docid`.\"\"\"\n", "func_signal": "def vec_by_id(self, docid):\n", "code": "pos = self.id2pos[docid]\nreturn self.qindex.vector_by_id(pos)", "path": "simserver\\simserver.py", "repo_name": "RaRe-Technologies/gensim-simserver", "stars": 107, "license": "agpl-3.0", "language": "python", "size": 233}
{"docstring": "\"\"\"Optimize index for faster by-document-id queries.\"\"\"\n", "func_signal": "def optimize(self):\n", "code": "self.check_session()\nresult = self.session.optimize()\nif self.autosession:\n    self.commit()\nreturn result", "path": "simserver\\simserver.py", "repo_name": "RaRe-Technologies/gensim-simserver", "stars": 107, "license": "agpl-3.0", "language": "python", "size": 233}
{"docstring": "\"\"\"Buffer documents, in the current session\"\"\"\n", "func_signal": "def buffer(self, *args, **kwargs):\n", "code": "self.check_session()\nresult = self.session.buffer(*args, **kwargs)\nreturn result", "path": "simserver\\simserver.py", "repo_name": "RaRe-Technologies/gensim-simserver", "stars": 107, "license": "agpl-3.0", "language": "python", "size": 233}
{"docstring": "\"\"\"Ignore all changes made in the latest session (terminate the session).\"\"\"\n", "func_signal": "def rollback(self):\n", "code": "if self.session is not None:\n    logger.info(\"rolling back transaction in %s\" % self)\n    self.session.close()\n    self.session = None\n    self.lock_update.release()\nelse:\n    logger.warning(\"rollback called but there's no open session in %s\" % self)", "path": "simserver\\simserver.py", "repo_name": "RaRe-Technologies/gensim-simserver", "stars": 107, "license": "agpl-3.0", "language": "python", "size": 233}
{"docstring": "\"\"\"Commit all changes, clear all caches.\"\"\"\n", "func_signal": "def flush(self, save_index=False, save_model=False, clear_buffer=False):\n", "code": "if save_index:\n    if self.fresh_index is not None:\n        self.fresh_index.save(self.location('index_fresh'))\n    if self.opt_index is not None:\n        self.opt_index.save(self.location('index_opt'))\nif save_model:\n    if self.model is not None:\n        self.model.save(self.location('model'))\nself.payload.commit()\nif clear_buffer:\n    if hasattr(self, 'fresh_docs'):\n        try:\n            self.fresh_docs.terminate() # erase all buffered documents + file on disk\n        except:\n            pass\n    self.fresh_docs = SqliteDict(journal_mode=JOURNAL_MODE) # buffer defaults to a random location in temp\nself.fresh_docs.sync()", "path": "simserver\\simserver.py", "repo_name": "RaRe-Technologies/gensim-simserver", "stars": 107, "license": "agpl-3.0", "language": "python", "size": 233}
{"docstring": "\"\"\"\nTrain a model, using `fresh_docs` as training corpus.\n\nIf `dictionary` is not specified, it is computed from the documents.\n\n`method` is currently one of \"tfidf\"/\"lsi\"/\"lda\".\n\"\"\"\n# FIXME TODO: use subclassing/injection for different methods, instead of param..\n", "func_signal": "def __init__(self, fresh_docs, dictionary=None, method=None, params=None):\n", "code": "self.method = method\nif params is None:\n    params = {}\nself.params = params\nlogger.info(\"collecting %i document ids\" % len(fresh_docs))\ndocids = fresh_docs.keys()\nlogger.info(\"creating model from %s documents\" % len(docids))\npreprocessed = lambda: (fresh_docs[docid]['tokens'] for docid in docids)\n\n# create id->word (integer->string) mapping\nlogger.info(\"creating dictionary from %s documents\" % len(docids))\nif dictionary is None:\n    dictionary = gensim.corpora.Dictionary(preprocessed())\n    if len(docids) >= 1000:\n        dictionary.filter_extremes(no_below=5, no_above=0.2, keep_n=50000)\n    else:\n        logger.warning(\"training model on only %i documents; is this intentional?\" % len(docids))\n        dictionary.filter_extremes(no_below=0, no_above=1.0, keep_n=50000)\nself.dictionary = dictionary\n\nclass IterableCorpus(object):\n    def __iter__(self):\n        for tokens in preprocessed():\n            yield dictionary.doc2bow(tokens)\n\n    def __len__(self):\n        return len(docids)\ncorpus = IterableCorpus()\n\nif method == 'lsi':\n    logger.info(\"training TF-IDF model\")\n    self.tfidf = gensim.models.TfidfModel(corpus, id2word=self.dictionary)\n    logger.info(\"training LSI model\")\n    tfidf_corpus = self.tfidf[corpus]\n    self.lsi = gensim.models.LsiModel(tfidf_corpus, id2word=self.dictionary, **params)\n    self.lsi.projection.u = self.lsi.projection.u.astype(numpy.float32) # use single precision to save mem\n    self.num_features = len(self.lsi.projection.s)\nelif method == 'lda_tfidf':\n    logger.info(\"training TF-IDF model\")\n    self.tfidf = gensim.models.TfidfModel(corpus, id2word=self.dictionary)\n    logger.info(\"training LDA model\")\n    self.lda = gensim.models.LdaModel(self.tfidf[corpus], id2word=self.dictionary, **params)\n    self.num_features = self.lda.num_topics\nelif method == 'lda':\n    logger.info(\"training TF-IDF model\")\n    self.tfidf = gensim.models.TfidfModel(corpus, id2word=self.dictionary)\n    logger.info(\"training LDA model\")\n    self.lda = gensim.models.LdaModel(corpus, id2word=self.dictionary, **params)\n    self.num_features = self.lda.num_topics\nelif method == 'logentropy':\n    logger.info(\"training a log-entropy model\")\n    self.logent = gensim.models.LogEntropyModel(corpus, id2word=self.dictionary, **params)\n    self.num_features = len(self.dictionary)\nelse:\n    msg = \"unknown semantic method %s\" % method\n    logger.error(msg)\n    raise NotImplementedError(msg)", "path": "simserver\\simserver.py", "repo_name": "RaRe-Technologies/gensim-simserver", "stars": 107, "license": "agpl-3.0", "language": "python", "size": 233}
{"docstring": "\"\"\"\nPrecompute top similarities for all indexed documents. This speeds up\n`find_similar` queries by id (but not queries by fulltext).\n\nInternally, documents are moved from a fresh index (=no precomputed similarities)\nto an optimized index (precomputed similarities). Similarity queries always\nquery both indexes, so this split is transparent to clients.\n\nIf you add documents later via `index`, they go to the fresh index again.\nTo precompute top similarities for these new documents too, simply call\n`optimize` again.\n\n\"\"\"\n", "func_signal": "def optimize(self):\n", "code": "if self.fresh_index is None:\n    logger.warning(\"optimize called but there are no new documents\")\n    return # nothing to do!\n\nif self.opt_index is None:\n    logger.info(\"starting a new optimized index for %s\" % self)\n    self.opt_index = SimIndex(self.location('index_opt'), self.model.num_features)\n\nself.opt_index.merge(self.fresh_index)\nself.fresh_index.terminate() # delete old files\nself.fresh_index = None\nself.flush(save_index=True)", "path": "simserver\\simserver.py", "repo_name": "RaRe-Technologies/gensim-simserver", "stars": 107, "license": "agpl-3.0", "language": "python", "size": 233}
{"docstring": "\"\"\"Convert multiple SimilarityDocuments to vectors (batch version of doc2vec).\"\"\"\n", "func_signal": "def docs2vecs(self, docs):\n", "code": "bows = (self.dictionary.doc2bow(doc['tokens']) for doc in docs)\nif self.method == 'lsi':\n    return self.lsi[self.tfidf[bows]]\nelif self.method == 'lda':\n    return self.lda[bows]\nelif self.method == 'lda_tfidf':\n    return self.lda[self.tfidf[bows]]\nelif self.method == 'logentropy':\n    return self.logent[bows]", "path": "simserver\\simserver.py", "repo_name": "RaRe-Technologies/gensim-simserver", "stars": 107, "license": "agpl-3.0", "language": "python", "size": 233}
{"docstring": "\"\"\"\nUpdate fresh index with new documents (potentially replacing old ones with\nthe same id). `fresh_docs` is a dictionary-like object (=dict, sqlitedict, shelve etc)\nthat maps document_id->document.\n\"\"\"\n", "func_signal": "def index_documents(self, fresh_docs, model):\n", "code": "docids = fresh_docs.keys()\nvectors = (model.docs2vecs(fresh_docs[docid] for docid in docids))\nlogger.info(\"adding %i documents to %s\" % (len(docids), self))\nself.qindex.add_documents(vectors)\nself.qindex.save()\nself.update_ids(docids)", "path": "simserver\\simserver.py", "repo_name": "RaRe-Technologies/gensim-simserver", "stars": 107, "license": "agpl-3.0", "language": "python", "size": 233}
{"docstring": "\"\"\" Save function as At command. \"\"\"\n\n", "func_signal": "def _command(nump=0, *syns):\n", "code": "def decorator(func):\n    name = func.__name__[3:]\n    AT_COMMANDS[name] = func, nump\n    global AT_COMMANDS_DOCS\n    AT_COMMANDS_DOCS += \"\\n\\n\" + (func.__doc__ or \"%s\") % \"/\".join(\n        style(n, fg='yellow') for n in (name,) + syns)\n    for syn in syns or []:\n        AT_COMMANDS[syn] = func, nump\n\n    @wraps(func)\n    def wrapper(*args):\n        return func(*args)\n    return wrapper\n\nreturn decorator", "path": "atmark\\commands.py", "repo_name": "klen/atmark", "stars": 69, "license": "bsd-3-clause", "language": "python", "size": 407}
{"docstring": "\"\"\" %s REGEXP \\t -- filter results by REGEXP. Leave ungrepped \"\"\"\n", "func_signal": "def at_notgrep(arg, regexp):\n", "code": "value = text_type(arg.value)\nif not re.search(regexp, value):\n    return arg.value", "path": "atmark\\commands.py", "repo_name": "klen/atmark", "stars": 69, "license": "bsd-3-clause", "language": "python", "size": 407}
{"docstring": "\"\"\" %s \\t\\t -- filter results by value has length \"\"\"\n", "func_signal": "def at_filter(arg):\n", "code": "if isinstance(arg.value, list):\n    return arg.value or None\nreturn arg.value.strip() or None", "path": "atmark\\commands.py", "repo_name": "klen/atmark", "stars": 69, "license": "bsd-3-clause", "language": "python", "size": 407}
{"docstring": "\"\"\" %s \\t\\t -- Remove ansi colors from string. \"\"\"\n", "func_signal": "def at_nocolor(arg):\n", "code": "value = text_type(arg)\nreturn ANSI.re.sub('', value)", "path": "atmark\\commands.py", "repo_name": "klen/atmark", "stars": 69, "license": "bsd-3-clause", "language": "python", "size": 407}
{"docstring": "\"\"\" Function description. \"\"\"\n", "func_signal": "def take_params(args, nump):\n", "code": "while nump:\n    nump -= 1\n    yield unicode_escape(args.pop(0))", "path": "atmark\\atmark.py", "repo_name": "klen/atmark", "stars": 69, "license": "bsd-3-clause", "language": "python", "size": 407}
{"docstring": "\"\"\" %s REGEXP \\t\\t -- replace in a string/list REGEXP to ''. \"\"\"\n", "func_signal": "def at_kill(arg, regexp):\n", "code": "value = text_type(arg)\nreturn re.sub(regexp, \"\", value)", "path": "atmark\\commands.py", "repo_name": "klen/atmark", "stars": 69, "license": "bsd-3-clause", "language": "python", "size": 407}
{"docstring": "\"\"\" %s REGEXP \\t\\t -- filter results by REGEXP \"\"\"\n", "func_signal": "def at_grep(arg, regexp):\n", "code": "value = text_type(arg.value)\nif re.search(regexp, value):\n    return arg.value", "path": "atmark\\commands.py", "repo_name": "klen/atmark", "stars": 69, "license": "bsd-3-clause", "language": "python", "size": 407}
{"docstring": "\"\"\" %s SEPARATOR \\t -- return a list of the substrings of the string splited by SEPARATOR \"\"\"\n", "func_signal": "def at_split(arg, p):\n", "code": "value = text_type(arg)\ntry:\n    return value.split(p)\nexcept ValueError:\n    return None", "path": "atmark\\commands.py", "repo_name": "klen/atmark", "stars": 69, "license": "bsd-3-clause", "language": "python", "size": 407}
{"docstring": "\"\"\" %s \\t\\t -- make the string is lowercase \"\"\"\n", "func_signal": "def at_lower(arg):\n", "code": "value = text_type(arg)\nreturn value.lower()", "path": "atmark\\commands.py", "repo_name": "klen/atmark", "stars": 69, "license": "bsd-3-clause", "language": "python", "size": 407}
{"docstring": "\"\"\" %s PATTERN -- return the string with trailing PATTERN removed. \"\"\"\n", "func_signal": "def at_rstrip(arg, pattern):\n", "code": "value = text_type(arg)\nreturn value.rstrip(pattern)", "path": "atmark\\commands.py", "repo_name": "klen/atmark", "stars": 69, "license": "bsd-3-clause", "language": "python", "size": 407}
{"docstring": "\"\"\" %s \\t\\t -- same as split by splited a string by whitespace characters \"\"\"\n", "func_signal": "def at_split_(arg):\n", "code": "value = text_type(arg)\nreturn value.split()", "path": "atmark\\commands.py", "repo_name": "klen/atmark", "stars": 69, "license": "bsd-3-clause", "language": "python", "size": 407}
{"docstring": "\"\"\" %s \\t -- capitalize the string. \"\"\"\n", "func_signal": "def at_capitalize(arg):\n", "code": "value = text_type(arg)\nreturn value[0].upper() + value[1:].lower()", "path": "atmark\\commands.py", "repo_name": "klen/atmark", "stars": 69, "license": "bsd-3-clause", "language": "python", "size": 407}
{"docstring": "\"\"\" %s \\t\\t -- make the string is uppercase. \"\"\"\n", "func_signal": "def at_upper(arg):\n", "code": "value = text_type(arg)\nreturn value.upper()", "path": "atmark\\commands.py", "repo_name": "klen/atmark", "stars": 69, "license": "bsd-3-clause", "language": "python", "size": 407}
{"docstring": "\"\"\" %s PATTERN \\t -- return the string with leading and trailing PATTERN removed. \"\"\"\n", "func_signal": "def at_strip(arg, pattern):\n", "code": "value = text_type(arg)\nreturn value.strip(pattern)", "path": "atmark\\commands.py", "repo_name": "klen/atmark", "stars": 69, "license": "bsd-3-clause", "language": "python", "size": 407}
{"docstring": "\"\"\" %s N \\t\\t -- get the N-th element/character from list/string. \"\"\"\n", "func_signal": "def at_index(arg, index):\n", "code": "try:\n    return arg.value[int(index)]\nexcept (ValueError, IndexError):\n    return None", "path": "atmark\\commands.py", "repo_name": "klen/atmark", "stars": 69, "license": "bsd-3-clause", "language": "python", "size": 407}
{"docstring": "\"\"\" %s \\t -- same as strip but trims a whitespaces. \"\"\"\n", "func_signal": "def at_strip_(arg):\n", "code": "value = text_type(arg)\nreturn value.strip()", "path": "atmark\\commands.py", "repo_name": "klen/atmark", "stars": 69, "license": "bsd-3-clause", "language": "python", "size": 407}
{"docstring": "\"\"\" %s FORMAT_STRING \\t -- format and print a string.\n\nSymbol '@' in FORMAT_STRING represents the current value in process of composition of fuctions.\nSymbol '#' in FORMAT_STRING represents the history state.\n    Where   # or #0 -- first state, #<n> (#1, #2) -- state with number n\n\nSynonyms: You can drop `format` function name. This lines are equalent:\n\n    $ ls | @ upper format \"@.BAK\"\n    $ ls | @ upper \"@.BAK\" \"\"\"\n", "func_signal": "def at_format(arg, string):\n", "code": "value = text_type(arg)\nvalue = CURRENT_RE.sub(value, string)\n\ndef get_history(m):\n    index = int(m.group(1) or 0)\n    try:\n        return text_type(arg.history[index])\n    except IndexError:\n        return \"\"\n\nreturn HISTORY_RE.sub(get_history, value)", "path": "atmark\\commands.py", "repo_name": "klen/atmark", "stars": 69, "license": "bsd-3-clause", "language": "python", "size": 407}
{"docstring": "\"\"\" %s FROM TO \\t -- replace in a string/list FROM to TO. \"\"\"\n", "func_signal": "def at_replace(arg, p1, p2):\n", "code": "value = text_type(arg)\nreturn value.replace(p1, p2)", "path": "atmark\\commands.py", "repo_name": "klen/atmark", "stars": 69, "license": "bsd-3-clause", "language": "python", "size": 407}
{"docstring": "\"\"\" %s PATTERN \\t -- return None if arg is equal to PATTERN. \"\"\"\n", "func_signal": "def at_notequal(arg, pattern):\n", "code": "value = text_type(arg)\nif value != pattern:\n    return value", "path": "atmark\\commands.py", "repo_name": "klen/atmark", "stars": 69, "license": "bsd-3-clause", "language": "python", "size": 407}
{"docstring": "\"\"\" %s PATTERN \\t -- return None if arg is not equal to PATTERN. \"\"\"\n", "func_signal": "def at_equal(arg, pattern):\n", "code": "value = text_type(arg)\nif value == pattern:\n    return value", "path": "atmark\\commands.py", "repo_name": "klen/atmark", "stars": 69, "license": "bsd-3-clause", "language": "python", "size": 407}
{"docstring": "\"\"\"Scan the string s for a JSON string. End is the index of the\ncharacter in s after the quote that started the JSON string.\nUnescapes all valid JSON string escape sequences and raises ValueError\non attempt to decode an invalid string. If strict is False then literal\ncontrol characters are allowed in the string.\n\nReturns a tuple of the decoded string and the index of the character in s\nafter the end quote.\"\"\"\n", "func_signal": "def py_scanstring(s, end, encoding=None, strict=True, _b=BACKSLASH, _m=STRINGCHUNK.match):\n", "code": "if encoding is None:\n    encoding = DEFAULT_ENCODING\nchunks = []\n_append = chunks.append\nbegin = end - 1\nwhile 1:\n    chunk = _m(s, end)\n    if chunk is None:\n        raise ValueError(\n            errmsg(\"Unterminated string starting at\", s, begin))\n    end = chunk.end()\n    content, terminator = chunk.groups()\n    # Content is contains zero or more unescaped string characters\n    if content:\n        if not isinstance(content, unicode):\n            content = unicode(content, encoding)\n        _append(content)\n    # Terminator is the end of string, a literal control character,\n    # or a backslash denoting that an escape sequence follows\n    if terminator == '\"':\n        break\n    elif terminator != '\\\\':\n        if strict:\n            msg = \"Invalid control character %r at\" % (terminator,)\n            #msg = \"Invalid control character {0!r} at\".format(terminator)\n            raise ValueError(errmsg(msg, s, end))\n        else:\n            _append(terminator)\n            continue\n    try:\n        esc = s[end]\n    except IndexError:\n        raise ValueError(\n            errmsg(\"Unterminated string starting at\", s, begin))\n    # If not a unicode escape sequence, must be in the lookup table\n    if esc != 'u':\n        try:\n            char = _b[esc]\n        except KeyError:\n            msg = \"Invalid \\\\escape: \" + repr(esc)\n            raise ValueError(errmsg(msg, s, end))\n        end += 1\n    else:\n        # Unicode escape sequence\n        esc = s[end + 1:end + 5]\n        next_end = end + 5\n        if len(esc) != 4:\n            msg = \"Invalid \\\\uXXXX escape\"\n            raise ValueError(errmsg(msg, s, end))\n        uni = int(esc, 16)\n        # Check for surrogate pair on UCS-4 systems\n        if 0xd800 <= uni <= 0xdbff and sys.maxunicode > 65535:\n            msg = \"Invalid \\\\uXXXX\\\\uXXXX surrogate pair\"\n            if not s[end + 5:end + 7] == '\\\\u':\n                raise ValueError(errmsg(msg, s, end))\n            esc2 = s[end + 7:end + 11]\n            if len(esc2) != 4:\n                raise ValueError(errmsg(msg, s, end))\n            uni2 = int(esc2, 16)\n            uni = 0x10000 + (((uni - 0xd800) << 10) | (uni2 - 0xdc00))\n            next_end += 6\n        char = unichr(uni)\n        end = next_end\n    # Append the unescaped character\n    _append(char)\nreturn u''.join(chunks), end", "path": "server\\simplejson\\decoder.py", "repo_name": "netguy204/roku_media_server", "stars": 90, "license": "None", "language": "python", "size": 21934}
{"docstring": "\"\"\"\nParses an HTTP date into a datetime object.\n\n    >>> parsehttpdate('Thu, 01 Jan 1970 01:01:01 GMT')\n    datetime.datetime(1970, 1, 1, 1, 1, 1)\n\"\"\"\n", "func_signal": "def parsehttpdate(string_):\n", "code": "try:\n    t = time.strptime(string_, \"%a, %d %b %Y %H:%M:%S %Z\")\nexcept ValueError:\n    return None\nreturn datetime.datetime(*t[:6])", "path": "server\\web\\net.py", "repo_name": "netguy204/roku_media_server", "stars": 90, "license": "None", "language": "python", "size": 21934}
{"docstring": "\"\"\"\nReturns True if `address` is a valid IPv4 address.\n\n    >>> validipaddr('192.168.1.1')\n    True\n    >>> validipaddr('192.168.1.800')\n    False\n    >>> validipaddr('192.168.1')\n    False\n\"\"\"\n", "func_signal": "def validipaddr(address):\n", "code": "try:\n    octets = address.split('.')\n    if len(octets) != 4:\n        return False\n    for x in octets:\n        if not (0 <= int(x) <= 255):\n            return False\nexcept ValueError:\n    return False\nreturn True", "path": "server\\web\\net.py", "repo_name": "netguy204/roku_media_server", "stars": 90, "license": "None", "language": "python", "size": 21934}
{"docstring": "# Converts 'django.views.news.stories.story_detail' to\n# ['django.views.news.stories', 'story_detail']\n", "func_signal": "def get_mod_func(callback):\n", "code": "try:\n    dot = callback.rindex('.')\nexcept ValueError:\n    return callback, ''\nreturn callback[:dot], callback[dot+1:]", "path": "server\\django\\core\\urlresolvers.py", "repo_name": "netguy204/roku_media_server", "stars": 90, "license": "None", "language": "python", "size": 21934}
{"docstring": "\"\"\"\nReturns either (ip_address, port) or \"/path/to/socket\" from string_\n\n    >>> validaddr('/path/to/socket')\n    '/path/to/socket'\n    >>> validaddr('8000')\n    ('0.0.0.0', 8000)\n    >>> validaddr('127.0.0.1')\n    ('127.0.0.1', 8080)\n    >>> validaddr('127.0.0.1:8000')\n    ('127.0.0.1', 8000)\n    >>> validaddr('fff')\n    Traceback (most recent call last):\n        ...\n    ValueError: fff is not a valid IP address/port\n\"\"\"\n", "func_signal": "def validaddr(string_):\n", "code": "if '/' in string_:\n    return string_\nelse:\n    return validip(string_)", "path": "server\\web\\net.py", "repo_name": "netguy204/roku_media_server", "stars": 90, "license": "None", "language": "python", "size": 21934}
{"docstring": "\"\"\"\nReturns True if `port` is a valid IPv4 port.\n\n    >>> validipport('9000')\n    True\n    >>> validipport('foo')\n    False\n    >>> validipport('1000000')\n    False\n\"\"\"\n", "func_signal": "def validipport(port):\n", "code": "try:\n    if not (0 <= int(port) <= 65535):\n        return False\nexcept ValueError:\n    return False\nreturn True", "path": "server\\web\\net.py", "repo_name": "netguy204/roku_media_server", "stars": 90, "license": "None", "language": "python", "size": 21934}
{"docstring": "\"\"\"\nSets the URLconf for the current thread (overriding the default one in\nsettings). Set to None to revert back to the default.\n\"\"\"\n", "func_signal": "def set_urlconf(urlconf_name):\n", "code": "thread = currentThread()\nif urlconf_name:\n    _urlconfs[thread] = urlconf_name\nelse:\n    # faster than wrapping in a try/except\n    if thread in _urlconfs:\n        del _urlconfs[thread]", "path": "server\\django\\core\\urlresolvers.py", "repo_name": "netguy204/roku_media_server", "stars": 90, "license": "None", "language": "python", "size": 21934}
{"docstring": "\"\"\"\nEncodes `text` for raw use in HTML.\n\n    >>> htmlquote(\"<'&\\\\\">\")\n    '&lt;&#39;&amp;&quot;&gt;'\n\"\"\"\n", "func_signal": "def htmlquote(text):\n", "code": "text = text.replace(\"&\", \"&amp;\") # Must be done first!\ntext = text.replace(\"<\", \"&lt;\")\ntext = text.replace(\">\", \"&gt;\")\ntext = text.replace(\"'\", \"&#39;\")\ntext = text.replace('\"', \"&quot;\")\nreturn text", "path": "server\\web\\net.py", "repo_name": "netguy204/roku_media_server", "stars": 90, "license": "None", "language": "python", "size": 21934}
{"docstring": "\"\"\"Decode a JSON document from ``s`` (a ``str`` or ``unicode`` beginning\nwith a JSON document) and return a 2-tuple of the Python\nrepresentation and the index in ``s`` where the document ended.\n\nThis can be used to decode a JSON document from a string that may\nhave extraneous data at the end.\n\n\"\"\"\n", "func_signal": "def raw_decode(self, s, idx=0):\n", "code": "try:\n    obj, end = self.scan_once(s, idx)\nexcept StopIteration:\n    raise ValueError(\"No JSON object could be decoded\")\nreturn obj, end", "path": "server\\simplejson\\decoder.py", "repo_name": "netguy204/roku_media_server", "stars": 90, "license": "None", "language": "python", "size": 21934}
{"docstring": "\"\"\"\nConvert a string version of a function name to the callable object.\n\nIf the lookup_view is not an import path, it is assumed to be a URL pattern\nlabel and the original string is returned.\n\nIf can_fail is True, lookup_view might be a URL pattern label, so errors\nduring the import fail and the string is returned.\n\"\"\"\n", "func_signal": "def get_callable(lookup_view, can_fail=False):\n", "code": "if not callable(lookup_view):\n    try:\n        # Bail early for non-ASCII strings (they can't be functions).\n        lookup_view = lookup_view.encode('ascii')\n        mod_name, func_name = get_mod_func(lookup_view)\n        if func_name != '':\n            lookup_view = getattr(import_module(mod_name), func_name)\n            if not callable(lookup_view):\n                raise AttributeError(\"'%s.%s' is not a callable.\" % (mod_name, func_name))\n    except (ImportError, AttributeError):\n        if not can_fail:\n            raise\n    except UnicodeEncodeError:\n        pass\nreturn lookup_view", "path": "server\\django\\core\\urlresolvers.py", "repo_name": "netguy204/roku_media_server", "stars": 90, "license": "None", "language": "python", "size": 21934}
{"docstring": "\"\"\"Return the Python representation of ``s`` (a ``str`` or ``unicode``\ninstance containing a JSON document)\n\n\"\"\"\n", "func_signal": "def decode(self, s, _w=WHITESPACE.match):\n", "code": "obj, end = self.raw_decode(s, idx=_w(s, 0).end())\nend = _w(s, end).end()\nif end != len(s):\n    raise ValueError(errmsg(\"Extra data\", s, end, len(s)))\nreturn obj", "path": "server\\simplejson\\decoder.py", "repo_name": "netguy204/roku_media_server", "stars": 90, "license": "None", "language": "python", "size": 21934}
{"docstring": "# regex is a string representing a regular expression.\n# urlconf_name is a string representing the module containing URLconfs.\n", "func_signal": "def __init__(self, regex, urlconf_name, default_kwargs=None, app_name=None, namespace=None):\n", "code": "self.regex = re.compile(regex, re.UNICODE)\nself.urlconf_name = urlconf_name\nif not isinstance(urlconf_name, basestring):\n    self._urlconf_module = self.urlconf_name\nself.callback = None\nself.default_kwargs = default_kwargs or {}\nself.namespace = namespace\nself.app_name = app_name\nself._reverse_dict = None\nself._namespace_dict = None\nself._app_dict = None", "path": "server\\django\\core\\urlresolvers.py", "repo_name": "netguy204/roku_media_server", "stars": 90, "license": "None", "language": "python", "size": 21934}
{"docstring": "# Note that this function is called from _speedups\n", "func_signal": "def errmsg(msg, doc, pos, end=None):\n", "code": "lineno, colno = linecol(doc, pos)\nif end is None:\n    #fmt = '{0}: line {1} column {2} (char {3})'\n    #return fmt.format(msg, lineno, colno, pos)\n    fmt = '%s: line %d column %d (char %d)'\n    return fmt % (msg, lineno, colno, pos)\nendlineno, endcolno = linecol(doc, end)\n#fmt = '{0}: line {1} column {2} - line {3} column {4} (char {5} - {6})'\n#return fmt.format(msg, lineno, colno, endlineno, endcolno, pos, end)\nfmt = '%s: line %d column %d - line %d column %d (char %d - %d)'\nreturn fmt % (msg, lineno, colno, endlineno, endcolno, pos, end)", "path": "server\\simplejson\\decoder.py", "repo_name": "netguy204/roku_media_server", "stars": 90, "license": "None", "language": "python", "size": 21934}
{"docstring": "\"\"\"\nQuotes a string for use in a URL.\n\n    >>> urlquote('://?f=1&j=1')\n    '%3A//%3Ff%3D1%26j%3D1'\n    >>> urlquote(None)\n    ''\n    >>> urlquote(u'\\u203d')\n    '%E2%80%BD'\n\"\"\"\n", "func_signal": "def urlquote(val):\n", "code": "if val is None: return ''\nif not isinstance(val, unicode): val = str(val)\nelse: val = val.encode('utf-8')\nreturn urllib.quote(val)", "path": "server\\web\\net.py", "repo_name": "netguy204/roku_media_server", "stars": 90, "license": "None", "language": "python", "size": 21934}
{"docstring": "\"\"\"\nReturns the root URLconf to use for the current thread if it has been\nchanged from the default one.\n\"\"\"\n", "func_signal": "def get_urlconf(default=None):\n", "code": "thread = currentThread()\nif thread in _urlconfs:\n    return _urlconfs[thread]\nreturn default", "path": "server\\django\\core\\urlresolvers.py", "repo_name": "netguy204/roku_media_server", "stars": 90, "license": "None", "language": "python", "size": 21934}
{"docstring": "# regex is a string representing a regular expression.\n# callback is either a string like 'foo.views.news.stories.story_detail'\n# which represents the path to a module and a view function name, or a\n# callable object (view).\n", "func_signal": "def __init__(self, regex, callback, default_args=None, name=None):\n", "code": "self.regex = re.compile(regex, re.UNICODE)\nif callable(callback):\n    self._callback = callback\nelse:\n    self._callback = None\n    self._callback_str = callback\nself.default_args = default_args or {}\nself.name = name", "path": "server\\django\\core\\urlresolvers.py", "repo_name": "netguy204/roku_media_server", "stars": 90, "license": "None", "language": "python", "size": 21934}
{"docstring": "\"\"\"\nDecodes `text` that's HTML quoted.\n\n    >>> htmlunquote('&lt;&#39;&amp;&quot;&gt;')\n    '<\\\\'&\">'\n\"\"\"\n", "func_signal": "def htmlunquote(text):\n", "code": "text = text.replace(\"&quot;\", '\"')\ntext = text.replace(\"&#39;\", \"'\")\ntext = text.replace(\"&gt;\", \">\")\ntext = text.replace(\"&lt;\", \"<\")\ntext = text.replace(\"&amp;\", \"&\") # Must be done last!\nreturn text", "path": "server\\web\\net.py", "repo_name": "netguy204/roku_media_server", "stars": 90, "license": "None", "language": "python", "size": 21934}
{"docstring": "\"\"\"\nConverts `val` so that it's safe for use in UTF-8 HTML.\n\n    >>> websafe(\"<'&\\\\\">\")\n    '&lt;&#39;&amp;&quot;&gt;'\n    >>> websafe(None)\n    ''\n    >>> websafe(u'\\u203d')\n    '\\\\xe2\\\\x80\\\\xbd'\n\"\"\"\n", "func_signal": "def websafe(val):\n", "code": "if val is None:\n    return ''\nif isinstance(val, unicode):\n    val = val.encode('utf-8')\nval = str(val)\nreturn htmlquote(val)", "path": "server\\web\\net.py", "repo_name": "netguy204/roku_media_server", "stars": 90, "license": "None", "language": "python", "size": 21934}
{"docstring": "\"\"\"\nSets the script prefix for the current thread.\n\"\"\"\n", "func_signal": "def set_script_prefix(prefix):\n", "code": "if not prefix.endswith('/'):\n    prefix += '/'\n_prefixes[currentThread()] = prefix", "path": "server\\django\\core\\urlresolvers.py", "repo_name": "netguy204/roku_media_server", "stars": 90, "license": "None", "language": "python", "size": 21934}
{"docstring": "\"\"\"\nReturns a list of the SQL statements used to flush the database.\n\nIf only_django is True, then only table names that have associated Django\nmodels and are in INSTALLED_APPS will be included.\n\"\"\"\n", "func_signal": "def sql_flush(style, connection, only_django=False):\n", "code": "if only_django:\n    tables = connection.introspection.django_table_names(only_existing=True)\nelse:\n    tables = connection.introspection.table_names()\nstatements = connection.ops.sql_flush(style, tables, connection.introspection.sequence_list())\nreturn statements", "path": "server\\django\\core\\management\\sql.py", "repo_name": "netguy204/roku_media_server", "stars": 90, "license": "None", "language": "python", "size": 21934}
{"docstring": "\"\"\"Increase sum of n-gram terms\n\n\"\"\"\n", "func_signal": "def increaseGramSum(self, n, value):\n", "code": "key = self._meta_prefix + ('%s-gram-sum' % n)\nreturn self.db.redis.incr(key, value)", "path": "loso\\lexicon.py", "repo_name": "fangpenlin/loso", "stars": 82, "license": "bsd-3-clause", "language": "python", "size": 118}
{"docstring": "# extracting the tarball\n", "func_signal": "def _install(tarball):\n", "code": "tmpdir = tempfile.mkdtemp()\nlog.warn('Extracting in %s', tmpdir)\nold_wd = os.getcwd()\ntry:\n    os.chdir(tmpdir)\n    tar = tarfile.open(tarball)\n    _extractall(tar)\n    tar.close()\n\n    # going in the directory\n    subdir = os.path.join(tmpdir, os.listdir(tmpdir)[0])\n    os.chdir(subdir)\n    log.warn('Now working in %s', subdir)\n\n    # installing\n    log.warn('Installing Distribute')\n    if not _python_cmd('setup.py', 'install'):\n        log.warn('Something went wrong during the installation.')\n        log.warn('See the error message above.')\nfinally:\n    os.chdir(old_wd)", "path": "distribute_setup.py", "repo_name": "fangpenlin/loso", "stars": 82, "license": "bsd-3-clause", "language": "python", "size": 118}
{"docstring": "\"\"\"Iterate English terms from Chinese text\n\n\"\"\"\n", "func_signal": "def iterEnglishTerms(text):\n", "code": "terms = []\nparts = text.split()\nfor part in parts:\n    for term in re.finditer(eng_term_pattern, part):\n        terms.append(term.group(0))\nreturn terms", "path": "loso\\lexicon.py", "repo_name": "fangpenlin/loso", "stars": 82, "license": "bsd-3-clause", "language": "python", "size": 118}
{"docstring": "\"\"\"Get statistics of this category\n\n\"\"\"\n", "func_signal": "def getStats(self):\n", "code": "stats = dict(\n    gram=self.gram,\n    total_sum=0,\n    total_variety=0\n)\nfor n in xrange(1, self.gram + 1):\n    sum = self.getGramSum(n)\n    variety = self.getGramVariety(n)\n    sum_key = '%sgram_sum' % n\n    variety_key = '%sgram_variety' % n\n    stats[sum_key] = sum\n    stats[variety_key] = variety\n    stats['total_sum'] += sum\n    stats['total_variety'] += variety\nreturn stats", "path": "loso\\lexicon.py", "repo_name": "fangpenlin/loso", "stars": 82, "license": "bsd-3-clause", "language": "python", "size": 118}
{"docstring": "\"\"\"Clean lexicon up\n\n\"\"\"\n", "func_signal": "def clean(self):\n", "code": "categories = self.getCategoryList()\nif categories:\n    for name in categories:\n        c = self.getCategory(name)\n        c.clean()\nself.logger.info('Clean lexicon database, %s categories', \n                 len(categories))", "path": "loso\\lexicon.py", "repo_name": "fangpenlin/loso", "stars": 82, "license": "bsd-3-clause", "language": "python", "size": 118}
{"docstring": "# extracting the tarball\n", "func_signal": "def _build_egg(egg, tarball, to_dir):\n", "code": "tmpdir = tempfile.mkdtemp()\nlog.warn('Extracting in %s', tmpdir)\nold_wd = os.getcwd()\ntry:\n    os.chdir(tmpdir)\n    tar = tarfile.open(tarball)\n    _extractall(tar)\n    tar.close()\n\n    # going in the directory\n    subdir = os.path.join(tmpdir, os.listdir(tmpdir)[0])\n    os.chdir(subdir)\n    log.warn('Now working in %s', subdir)\n\n    # building an egg\n    log.warn('Building a Distribute egg in %s', to_dir)\n    _python_cmd('setup.py', '-q', 'bdist_egg', '--dist-dir', to_dir)\n\nfinally:\n    os.chdir(old_wd)\n# returning the result\nlog.warn(egg)\nif not os.path.exists(egg):\n    raise IOError('Could not build the egg.')", "path": "distribute_setup.py", "repo_name": "fangpenlin/loso", "stars": 82, "license": "bsd-3-clause", "language": "python", "size": 118}
{"docstring": "\"\"\"Get category and return \n\n\"\"\"\n", "func_signal": "def getCategory(self, name):\n", "code": "if name not in self.getCategoryList():\n    return \ncategory = self._categories_cache.get(name)\nif category:\n    return category\ncategory = LexiconCategory(self, name)\nself._categories_cache[name] = category\nreturn category", "path": "loso\\lexicon.py", "repo_name": "fangpenlin/loso", "stars": 82, "license": "bsd-3-clause", "language": "python", "size": 118}
{"docstring": "\"\"\"Will backup the file then patch it\"\"\"\n", "func_signal": "def _patch_file(path, content):\n", "code": "existing_content = open(path).read()\nif existing_content == content:\n    # already patched\n    log.warn('Already patched.')\n    return False\nlog.warn('Patching...')\n_rename_path(path)\nf = open(path, 'w')\ntry:\n    f.write(content)\nfinally:\n    f.close()\nreturn True", "path": "distribute_setup.py", "repo_name": "fangpenlin/loso", "stars": 82, "license": "bsd-3-clause", "language": "python", "size": 118}
{"docstring": "\"\"\"Get variety of n-gram terms\n\n\"\"\"\n", "func_signal": "def getGramVariety(self, n):\n", "code": "key = '%s-gram-variety' % n\nreturn int(self.getMeta(key) or 0)", "path": "loso\\lexicon.py", "repo_name": "fangpenlin/loso", "stars": 82, "license": "bsd-3-clause", "language": "python", "size": 118}
{"docstring": "\"\"\"Install or upgrade setuptools and EasyInstall\"\"\"\n", "func_signal": "def main(argv, version=DEFAULT_VERSION):\n", "code": "tarball = download_setuptools()\n_install(tarball)", "path": "distribute_setup.py", "repo_name": "fangpenlin/loso", "stars": 82, "license": "bsd-3-clause", "language": "python", "size": 118}
{"docstring": "\"\"\"Get count of a term\n\n\"\"\"\n", "func_signal": "def getTerm(self, term):\n", "code": "key = self._lexicon_prefix + term\nreturn self.db.redis.get(key)", "path": "loso\\lexicon.py", "repo_name": "fangpenlin/loso", "stars": 82, "license": "bsd-3-clause", "language": "python", "size": 118}
{"docstring": "\"\"\"Get score of a term\n\n\"\"\"\n", "func_signal": "def _getTermScore(self, term, ngram, categories):\n", "code": "score = 0.00000001\nfor c in categories:\n    count = int(c.getTerm(term) or 0)\n    n = len(term)\n    sum = int(c.getGramSum(n) or 0)\n    variety = int(c.getGramVariety(n) or 0)\n    if not variety:\n        v = 1\n    else:\n        v = sum/float(variety)\n        v *= v\n    score += count/v\nreturn score", "path": "loso\\lexicon.py", "repo_name": "fangpenlin/loso", "stars": 82, "license": "bsd-3-clause", "language": "python", "size": 118}
{"docstring": "\"\"\"Add a category and return\n\n\"\"\"\n", "func_signal": "def addCategory(self, name):\n", "code": "category = self._categories_cache.get(name)\nif category:\n    return category\ncategory = LexiconCategory(self, name)\ncategory.init(self.ngram)\nself._categories_cache[name] = category\nreturn category", "path": "loso\\lexicon.py", "repo_name": "fangpenlin/loso", "stars": 82, "license": "bsd-3-clause", "language": "python", "size": 118}
{"docstring": "\"\"\"Split article into sentences by delimiters\n\n\"\"\"\n", "func_signal": "def splitSentence(text, delimiters=None):\n", "code": "if delimiters is None:\n    delimiters = default_delimiters\n\nsentence = []\nfor c in text:\n    if c in delimiters:\n        yield ''.join(sentence)\n        sentence = []\n    else:\n        sentence.append(c)\nyield ''.join(sentence)", "path": "loso\\lexicon.py", "repo_name": "fangpenlin/loso", "stars": 82, "license": "bsd-3-clause", "language": "python", "size": 118}
{"docstring": "\"\"\"Iterate n-gram terms in given text and return a generator. \n\nAll English in \nterms will be lower case. All first and last terms in a sentence will be\nemitted another term in the result with 'B' and 'E' prefix.\n\nFor example:\n\n'C1C2C3'\n\nin uni-gram term will be emitted as\n\n['C1', 'BC1', 'C2', 'C3', 'EC3']\n\nwhere the C1, C2 and C3 are Chinese words. \n\n\"\"\"\n", "func_signal": "def iterTerms(n, text, emmit_head_tail=False):\n", "code": "for sentence in splitSentence(text):\n    first = True\n    term = None\n    for term in util.ngram(n, sentence):\n        term = term.lower()\n        yield term\n        if first:\n            if emmit_head_tail:\n                yield 'B' + term\n            first = False\n    if term is not None:\n        if emmit_head_tail:\n            yield 'E' + term", "path": "loso\\lexicon.py", "repo_name": "fangpenlin/loso", "stars": 82, "license": "bsd-3-clause", "language": "python", "size": 118}
{"docstring": "\"\"\"Increase variety of n-gram terms\n\n\"\"\"\n", "func_signal": "def increaseGramVariety(self, n, value):\n", "code": "key = self._meta_prefix + ('%s-gram-variety' % n)\nreturn self.db.redis.incr(key, value)", "path": "loso\\lexicon.py", "repo_name": "fangpenlin/loso", "stars": 82, "license": "bsd-3-clause", "language": "python", "size": 118}
{"docstring": "\"\"\"Clean all value of this category\n\n\"\"\"\n# remove terms\n", "func_signal": "def clean(self):\n", "code": "terms = self.getTermList()\nkeys = [self._lexicon_prefix + term for term in terms]\nself.db.redis.delete(*keys)\n\n# remove meta keys\nfor n in self.gram:\n    self.db.redis.delete(self._meta_prefix + ('%s-gram-sum' % n))\n    self.db.redis.delete(self._meta_prefix + ('%s-gram-variety' % n))\nself.db.redis.delete(self._meta_prefix + 'gram')\n\n# remove this category from category set\nself.db.redis.srem(self.db._category_set_key, self.name)\n\nself.logger.info('Clean category %r, %d terms are deleted', \n                 self.name, len(terms))", "path": "loso\\lexicon.py", "repo_name": "fangpenlin/loso", "stars": 82, "license": "bsd-3-clause", "language": "python", "size": 118}
{"docstring": "\"\"\"Increase value of a term\n\n\"\"\"\n# increase number\n", "func_signal": "def increaseTerm(self, term, delta=1):\n", "code": "key = self._lexicon_prefix + term\nself.db.redis.incr(key, delta)\n# add to terms set\nself.db.redis.sadd(self._terms_key, term)", "path": "loso\\lexicon.py", "repo_name": "fangpenlin/loso", "stars": 82, "license": "bsd-3-clause", "language": "python", "size": 118}
{"docstring": "\"\"\"Split text into terms, categories is a list of category to read\nlexicon data from, if it is empty, it means to get data from all\ncategories\n\n\"\"\"\n", "func_signal": "def splitTerms(self, text, categories=None):\n", "code": "all_category = self.getCategoryList()\nif not categories:\n    categories = all_category\nc_list = []\nfor name in categories:\n    c = self.getCategory(name)\n    if not c:\n        self.logger.error('Category %s not exist', name)\n        continue\n    c_list.append(c)\ngrams = []\nfor n in xrange(1, self.ngram+1):\n    terms = []\n    for term in util.ngram(n, text):\n        score = self._getTermScore(term, n, c_list)\n        self.logger.debug('Term=%s, Score=%s', term, score)\n        terms.append((term, score))\n    grams.append(terms)\nterms, best_score = findBestSegment(grams)\nself.logger.debug('Best score: %s', best_score)\nreturn terms", "path": "loso\\lexicon.py", "repo_name": "fangpenlin/loso", "stars": 82, "license": "bsd-3-clause", "language": "python", "size": 118}
{"docstring": "\"\"\"Initialize category in database\n\n\"\"\"\n# add to category set\n", "func_signal": "def init(self, ngram=4):\n", "code": "if not self.db.redis.sadd(self.db._category_set_key, self.name):\n    # already exists\n    self.logger.info('Category %s already exists', self.name)\n    return\nself.setMeta('gram', ngram)\nfor n in xrange(ngram):\n    self.increaseGramSum(n, 0)\n    self.increaseGramVariety(n, 0)\nself.logger.info('Add category %s (gram=%s)', self.name, ngram)", "path": "loso\\lexicon.py", "repo_name": "fangpenlin/loso", "stars": 82, "license": "bsd-3-clause", "language": "python", "size": 118}
{"docstring": "\"\"\"Sets up the initial relations between this element and\nother elements.\"\"\"\n", "func_signal": "def setup(self, parent=None, previous=None):\n", "code": "self.parent = parent\nself.previous = previous\nself.next = None\nself.previousSibling = None\nself.nextSibling = None\nif self.parent and self.parent.contents:\n    self.previousSibling = self.parent.contents[-1]\n    self.previousSibling.nextSibling = self", "path": "BeautifulSoup.py", "repo_name": "initpy/selficious", "stars": 93, "license": "None", "language": "python", "size": 1341}
{"docstring": "\"\"\"Handle a processing instruction as a ProcessingInstruction\nobject, possibly one with a %SOUP-ENCODING% slot into which an\nencoding will be plugged later.\"\"\"\n", "func_signal": "def handle_pi(self, text):\n", "code": "if text[:3] == \"xml\":\n    text = u\"xml version='1.0' encoding='%SOUP-ENCODING%'\"\nself._toStringSubclass(text, ProcessingInstruction)", "path": "BeautifulSoup.py", "repo_name": "initpy/selficious", "stars": 93, "license": "None", "language": "python", "size": 1341}
{"docstring": "\"\"\"Returns the parents of this Tag that match the given\ncriteria.\"\"\"\n\n", "func_signal": "def findParents(self, name=None, attrs={}, limit=None, **kwargs):\n", "code": "return self._findAll(name, attrs, None, limit, self.parentGenerator,\n                     **kwargs)", "path": "BeautifulSoup.py", "repo_name": "initpy/selficious", "stars": 93, "license": "None", "language": "python", "size": 1341}
{"docstring": "\"\"\"This method routes method call requests to either the SGMLParser\nsuperclass or the Tag superclass, depending on the method name.\"\"\"\n#print \"__getattr__ called on %s.%s\" % (self.__class__, methodName)\n\n", "func_signal": "def __getattr__(self, methodName):\n", "code": "if methodName.startswith('start_') or methodName.startswith('end_') \\\n       or methodName.startswith('do_'):\n    return SGMLParser.__getattr__(self, methodName)\nelif not methodName.startswith('__'):\n    return Tag.__getattr__(self, methodName)\nelse:\n    raise AttributeError", "path": "BeautifulSoup.py", "repo_name": "initpy/selficious", "stars": 93, "license": "None", "language": "python", "size": 1341}
{"docstring": "#print \"Matching %s against %s\" % (markup, matchAgainst)\n", "func_signal": "def _matches(self, markup, matchAgainst):\n", "code": "result = False\nif matchAgainst is True:\n    result = markup is not None\nelif callable(matchAgainst):\n    result = matchAgainst(markup)\nelse:\n    #Custom match methods take the tag as an argument, but all\n    #other ways of matching match the tag name as a string.\n    if isinstance(markup, Tag):\n        markup = markup.name\n    if markup and not isinstance(markup, basestring):\n        markup = unicode(markup)\n    #Now we know that chunk is either a string, or None.\n    if hasattr(matchAgainst, 'match'):\n        # It's a regexp object.\n        result = markup and matchAgainst.search(markup)\n    elif hasattr(matchAgainst, '__iter__'): # list-like\n        result = markup in matchAgainst\n    elif hasattr(matchAgainst, 'items'):\n        result = markup.has_key(matchAgainst)\n    elif matchAgainst and isinstance(markup, basestring):\n        if isinstance(markup, unicode):\n            matchAgainst = unicode(matchAgainst)\n        else:\n            matchAgainst = str(matchAgainst)\n\n    if not result:\n        result = matchAgainst == markup\nreturn result", "path": "BeautifulSoup.py", "repo_name": "initpy/selficious", "stars": 93, "license": "None", "language": "python", "size": 1341}
{"docstring": "#print \"Push\", tag.name\n", "func_signal": "def pushTag(self, tag):\n", "code": "if self.currentTag:\n    self.currentTag.contents.append(tag)\nself.tagStack.append(tag)\nself.currentTag = self.tagStack[-1]", "path": "BeautifulSoup.py", "repo_name": "initpy/selficious", "stars": 93, "license": "None", "language": "python", "size": 1341}
{"docstring": "\"\"\"Beautiful Soup can detect a charset included in a META tag,\ntry to convert the document to that charset, and re-parse the\ndocument from the beginning.\"\"\"\n", "func_signal": "def start_meta(self, attrs):\n", "code": "httpEquiv = None\ncontentType = None\ncontentTypeIndex = None\ntagNeedsEncodingSubstitution = False\n\nfor i in range(0, len(attrs)):\n    key, value = attrs[i]\n    key = key.lower()\n    if key == 'http-equiv':\n        httpEquiv = value\n    elif key == 'content':\n        contentType = value\n        contentTypeIndex = i\n\nif httpEquiv and contentType: # It's an interesting meta tag.\n    match = self.CHARSET_RE.search(contentType)\n    if match:\n        if (self.declaredHTMLEncoding is not None or\n            self.originalEncoding == self.fromEncoding):\n            # An HTML encoding was sniffed while converting\n            # the document to Unicode, or an HTML encoding was\n            # sniffed during a previous pass through the\n            # document, or an encoding was specified\n            # explicitly and it worked. Rewrite the meta tag.\n            def rewrite(match):\n                return match.group(1) + \"%SOUP-ENCODING%\"\n            newAttr = self.CHARSET_RE.sub(rewrite, contentType)\n            attrs[contentTypeIndex] = (attrs[contentTypeIndex][0],\n                                       newAttr)\n            tagNeedsEncodingSubstitution = True\n        else:\n            # This is our first pass through the document.\n            # Go through it again with the encoding information.\n            newCharset = match.group(3)\n            if newCharset and newCharset != self.originalEncoding:\n                self.declaredHTMLEncoding = newCharset\n                self._feed(self.declaredHTMLEncoding)\n                raise StopParsing\n            pass\ntag = self.unknown_starttag(\"meta\", attrs)\nif tag and tagNeedsEncodingSubstitution:\n    tag.containsSubstitutions = True", "path": "BeautifulSoup.py", "repo_name": "initpy/selficious", "stars": 93, "license": "None", "language": "python", "size": 1341}
{"docstring": "#print \"Start tag %s: %s\" % (name, attrs)\n", "func_signal": "def unknown_starttag(self, name, attrs, selfClosing=0):\n", "code": "if self.quoteStack:\n    #This is not a real tag.\n    #print \"<%s> is not real!\" % name\n    attrs = ''.join([' %s=\"%s\"' % (x, y) for x, y in attrs])\n    self.handle_data('<%s%s>' % (name, attrs))\n    return\nself.endData()\n\nif not self.isSelfClosingTag(name) and not selfClosing:\n    self._smartPop(name)\n\nif self.parseOnlyThese and len(self.tagStack) <= 1 \\\n       and (self.parseOnlyThese.text or not self.parseOnlyThese.searchTag(name, attrs)):\n    return\n\ntag = Tag(self, name, attrs, self.currentTag, self.previous)\nif self.previous:\n    self.previous.next = tag\nself.previous = tag\nself.pushTag(tag)\nif selfClosing or self.isSelfClosingTag(name):\n    self.popTag()\nif name in self.QUOTE_TAGS:\n    #print \"Beginning quote (%s)\" % name\n    self.quoteStack.append(name)\n    self.literal = 1\nreturn tag", "path": "BeautifulSoup.py", "repo_name": "initpy/selficious", "stars": 93, "license": "None", "language": "python", "size": 1341}
{"docstring": "\"\"\"Handle entity references as data, possibly converting known\nHTML and/or XML entity references to the corresponding Unicode\ncharacters.\"\"\"\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "data = None\nif self.convertHTMLEntities:\n    try:\n        data = unichr(name2codepoint[ref])\n    except KeyError:\n        pass\n\nif not data and self.convertXMLEntities:\n        data = self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref)\n\nif not data and self.convertHTMLEntities and \\\n    not self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref):\n        # TODO: We've got a problem here. We're told this is\n        # an entity reference, but it's not an XML entity\n        # reference or an HTML entity reference. Nonetheless,\n        # the logical thing to do is to pass it through as an\n        # unrecognized entity reference.\n        #\n        # Except: when the input is \"&carol;\" this function\n        # will be called with input \"carol\". When the input is\n        # \"AT&T\", this function will be called with input\n        # \"T\". We have no way of knowing whether a semicolon\n        # was present originally, so we don't know whether\n        # this is an unknown entity or just a misplaced\n        # ampersand.\n        #\n        # The more common case is a misplaced ampersand, so I\n        # escape the ampersand and omit the trailing semicolon.\n        data = \"&amp;%s\" % ref\nif not data:\n    # This case is different from the one above, because we\n    # haven't already gone through a supposedly comprehensive\n    # mapping of entities to Unicode characters. We might not\n    # have gone through any mapping at all. So the chances are\n    # very high that this is a real entity, and not a\n    # misplaced ampersand.\n    data = \"&%s;\" % ref\nself.handle_data(data)", "path": "BeautifulSoup.py", "repo_name": "initpy/selficious", "stars": 93, "license": "None", "language": "python", "size": 1341}
{"docstring": "\"\"\"Replace the contents of the tag with a string\"\"\"\n", "func_signal": "def setString(self, string):\n", "code": "self.clear()\nself.append(string)", "path": "BeautifulSoup.py", "repo_name": "initpy/selficious", "stars": 93, "license": "None", "language": "python", "size": 1341}
{"docstring": "\"\"\"Turns a list of maps, lists, or scalars into a single map.\nUsed to build the SELF_CLOSING_TAGS, NESTABLE_TAGS, and\nNESTING_RESET_TAGS maps out of lists and partial maps.\"\"\"\n", "func_signal": "def buildTagMap(default, *args):\n", "code": "built = {}\nfor portion in args:\n    if hasattr(portion, 'items'):\n        #It's a map. Merge it.\n        for k,v in portion.items():\n            built[k] = v\n    elif hasattr(portion, '__iter__'): # is a list\n        #It's a list. Map each item to the default.\n        for k in portion:\n            built[k] = default\n    else:\n        #It's a scalar. Map it to the default.\n        built[portion] = default\nreturn built", "path": "BeautifulSoup.py", "repo_name": "initpy/selficious", "stars": 93, "license": "None", "language": "python", "size": 1341}
{"docstring": "\"\"\"Returns the closest sibling to this Tag that matches the\ngiven criteria and appears before this Tag in the document.\"\"\"\n", "func_signal": "def findPreviousSibling(self, name=None, attrs={}, text=None, **kwargs):\n", "code": "return self._findOne(self.findPreviousSiblings, name, attrs, text,\n                     **kwargs)", "path": "BeautifulSoup.py", "repo_name": "initpy/selficious", "stars": 93, "license": "None", "language": "python", "size": 1341}
{"docstring": "\"\"\"Encodes an object to a string in some encoding, or to Unicode.\n.\"\"\"\n", "func_signal": "def toEncoding(self, s, encoding=None):\n", "code": "if isinstance(s, unicode):\n    if encoding:\n        s = s.encode(encoding)\nelif isinstance(s, str):\n    if encoding:\n        s = s.encode(encoding)\n    else:\n        s = unicode(s)\nelse:\n    if encoding:\n        s  = self.toEncoding(str(s), encoding)\n    else:\n        s = unicode(s)\nreturn s", "path": "BeautifulSoup.py", "repo_name": "initpy/selficious", "stars": 93, "license": "None", "language": "python", "size": 1341}
{"docstring": "\"\"\"Pops the tag stack up to and including the most recent\ninstance of the given tag. If inclusivePop is false, pops the tag\nstack up to but *not* including the most recent instqance of\nthe given tag.\"\"\"\n#print \"Popping to %s\" % name\n", "func_signal": "def _popToTag(self, name, inclusivePop=True):\n", "code": "if name == self.ROOT_TAG_NAME:\n    return\n\nnumPops = 0\nmostRecentTag = None\nfor i in range(len(self.tagStack)-1, 0, -1):\n    if name == self.tagStack[i].name:\n        numPops = len(self.tagStack)-i\n        break\nif not inclusivePop:\n    numPops = numPops - 1\n\nfor i in range(0, numPops):\n    mostRecentTag = self.popTag()\nreturn mostRecentTag", "path": "BeautifulSoup.py", "repo_name": "initpy/selficious", "stars": 93, "license": "None", "language": "python", "size": 1341}
{"docstring": "\"\"\"Adds a certain piece of text to the tree as a NavigableString\nsubclass.\"\"\"\n", "func_signal": "def _toStringSubclass(self, text, subclass):\n", "code": "self.endData()\nself.handle_data(text)\nself.endData(subclass)", "path": "BeautifulSoup.py", "repo_name": "initpy/selficious", "stars": 93, "license": "None", "language": "python", "size": 1341}
{"docstring": "\"\"\"Returns true iff this tag has the same name, the same attributes,\nand the same contents (recursively) as the given tag.\n\nNOTE: right now this will return false if two tags have the\nsame attributes in a different order. Should this be fixed?\"\"\"\n", "func_signal": "def __eq__(self, other):\n", "code": "if other is self:\n    return True\nif not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):\n    return False\nfor i in range(0, len(self.contents)):\n    if self.contents[i] != other.contents[i]:\n        return False\nreturn True", "path": "BeautifulSoup.py", "repo_name": "initpy/selficious", "stars": 93, "license": "None", "language": "python", "size": 1341}
{"docstring": "\"\"\"Changes a MS smart quote character to an XML or HTML\nentity.\"\"\"\n", "func_signal": "def _subMSChar(self, orig):\n", "code": "sub = self.MS_CHARS.get(orig)\nif isinstance(sub, tuple):\n    if self.smartQuotesTo == 'xml':\n        sub = '&#x%s;' % sub[1]\n    else:\n        sub = '&%s;' % sub[0]\nreturn sub", "path": "BeautifulSoup.py", "repo_name": "initpy/selficious", "stars": 93, "license": "None", "language": "python", "size": 1341}
{"docstring": "# Convert the document to Unicode.\n", "func_signal": "def _feed(self, inDocumentEncoding=None, isHTML=False):\n", "code": "markup = self.markup\nif isinstance(markup, unicode):\n    if not hasattr(self, 'originalEncoding'):\n        self.originalEncoding = None\nelse:\n    dammit = UnicodeDammit\\\n             (markup, [self.fromEncoding, inDocumentEncoding],\n              smartQuotesTo=self.smartQuotesTo, isHTML=isHTML)\n    markup = dammit.unicode\n    self.originalEncoding = dammit.originalEncoding\n    self.declaredHTMLEncoding = dammit.declaredHTMLEncoding\nif markup:\n    if self.markupMassage:\n        if not hasattr(self.markupMassage, \"__iter__\"):\n            self.markupMassage = self.MARKUP_MASSAGE\n        for fix, m in self.markupMassage:\n            markup = fix.sub(m, markup)\n        # TODO: We get rid of markupMassage so that the\n        # soup object can be deepcopied later on. Some\n        # Python installations can't copy regexes. If anyone\n        # was relying on the existence of markupMassage, this\n        # might cause problems.\n        del(self.markupMassage)\nself.reset()\n\nSGMLParser.feed(self, markup)\n# Close out any unfinished strings and close all the open tags.\nself.endData()\nwhile self.currentTag.name != self.ROOT_TAG_NAME:\n    self.popTag()", "path": "BeautifulSoup.py", "repo_name": "initpy/selficious", "stars": 93, "license": "None", "language": "python", "size": 1341}
{"docstring": "\"\"\"Given a document, tries to detect its XML encoding.\"\"\"\n", "func_signal": "def _detectEncoding(self, xml_data, isHTML=False):\n", "code": "xml_encoding = sniffed_xml_encoding = None\ntry:\n    if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n        # EBCDIC\n        xml_data = self._ebcdic_to_ascii(xml_data)\n    elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n        # UTF-16BE\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') \\\n             and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16BE with BOM\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n        # UTF-16LE\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and \\\n             (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16LE with BOM\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n        # UTF-32BE\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n        # UTF-32LE\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n        # UTF-32BE with BOM\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n        # UTF-32LE with BOM\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n    elif xml_data[:3] == '\\xef\\xbb\\xbf':\n        # UTF-8 with BOM\n        sniffed_xml_encoding = 'utf-8'\n        xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n    else:\n        sniffed_xml_encoding = 'ascii'\n        pass\nexcept:\n    xml_encoding_match = None\nxml_encoding_match = re.compile(\n    '^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>').match(xml_data)\nif not xml_encoding_match and isHTML:\n    regexp = re.compile('<\\s*meta[^>]+charset=([^>]*?)[;\\'\">]', re.I)\n    xml_encoding_match = regexp.search(xml_data)\nif xml_encoding_match is not None:\n    xml_encoding = xml_encoding_match.groups()[0].lower()\n    if isHTML:\n        self.declaredHTMLEncoding = xml_encoding\n    if sniffed_xml_encoding and \\\n       (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',\n                         'iso-10646-ucs-4', 'ucs-4', 'csucs4',\n                         'utf-16', 'utf-32', 'utf_16', 'utf_32',\n                         'utf16', 'u16')):\n        xml_encoding = sniffed_xml_encoding\nreturn xml_data, xml_encoding, sniffed_xml_encoding", "path": "BeautifulSoup.py", "repo_name": "initpy/selficious", "stars": 93, "license": "None", "language": "python", "size": 1341}
{"docstring": "\"\"\"This method fixes a bug in Python's SGMLParser.\"\"\"\n", "func_signal": "def convert_charref(self, name):\n", "code": "try:\n    n = int(name)\nexcept ValueError:\n    return\nif not 0 <= n <= 127 : # ASCII ends at 127, not 255\n    return\nreturn self.convert_codepoint(n)", "path": "BeautifulSoup.py", "repo_name": "initpy/selficious", "stars": 93, "license": "None", "language": "python", "size": 1341}
{"docstring": "'''\u6bcf\u6b21\u4efb\u52a1\u53ea\u8dd1\u4e00\u5e74\u7684'''\n", "func_signal": "def mtime_beat():\n", "code": "y_list = []\ny = get_year() + 1  # \u8981\u6293\u53d6\u7684\u5e74\u4efd\ndebug('Fetch Year: {} starting...'.format(y))\ninstance = fetch(y, 1)\npage = get_movie_pages(instance)\nif page is None:\n    warn('Movie\"page has not fetched')\n    # \u6267\u884c\u95f4\u9694\u81ea\u9002\u5e94\n    if scheduler.get_interval < TASK_BEAT * 7:\n        scheduler.change_interval(incr=True)\n    return\nids = get_movie_ids(instance)\nif ids is None:\n    # \u95f4\u9694\u81ea\u9002\u5e94\u4e5f\u4e0d\u80fd\u592a\u5927\n    warn('Movie has not fetched')\n    if scheduler.get_interval < TASK_BEAT * 7:\n        scheduler.change_interval(incr=True)\n    return\n# \u5f53\u4efb\u52a1\u7ee7\u7eed\u80fd\u6267\u884c\u7684\u65f6\u5019,\u56de\u5230\u9ed8\u8ba4\u7684\u95f4\u9694\nif scheduler.get_interval > TASK_BEAT:\n    debug('Interval back to default')\n    scheduler.change_interval(TASK_BEAT)\ny_list.extend(ids)\nif not y_list:\n    # \u672c\u5e74\u6ca1\u6709\u7535\u5f71\n    debug('Year: {} has not movie'.format(y))\n    YearFinished(year=y).save()\n    sleep2()\n    return mtime_beat()\nif page > 1:\n    p = 2\n    while p <= page:\n        instance = fetch(y, p)\n        debug('Fetch Year:{} Page:{}'.format(y, p))\n        ids = get_movie_ids(instance)\n        if ids is None:\n            # \u95f4\u9694\u81ea\u9002\u5e94\u4e5f\u4e0d\u80fd\u592a\u5927\n            if scheduler.get_interval < TASK_BEAT * 7:\n                scheduler.change_interval(incr=True)\n                # \u51fa\u73b0\u9700\u8981\u9a8c\u8bc1\u7801 \u624b\u52a8\u8f93\u5165\u6216\u8005\u7b49\u5f85\u4e00\u6bb5\u65f6\u95f4\u540e\u91cd\u8bd5,\u76f4\u5230\u80fd\u6b63\u5e38\u4f7f\u7528\n                sleep2(VERIFY_INTERVAL)\n                continue\n            ids = []\n        y_list.extend(ids)\n        p += 1\n        sleep2()\nobj = IdFinished.objects(year=y).first()\nif obj is not None:\n    has_finished = obj.ids\nelse:\n    has_finished = []\nto_process = get_unfinished(has_finished, y_list)\n# \u7ed9\u76f8\u5e94\u961f\u5217\u6dfb\u52a0\u4efb\u52a1\nfor payload in group(to_process, TASK_BEAT_NUM):\n    for task in ['Fullcredits', 'Movie', 'Comment', 'Character',\n                 'MicroComment', 'Scenes', 'Awards', 'Plot',\n                 'Details']:\n        debug('Push payload: {} to {} Queue'.format(payload, task))\n        try:\n            Message(year=y, task=task, payload=payload).save()\n            # Hack\u4e00\u4e0b\n            #Message.objects.get_or_create(year=y, task=task, payload=payload)\n        except NotUniqueError:\n            debug('Duplicate insert: [{}], payload: {}'.format(task, payload))\n# \u5f53\u524d\u5e74\u4efd\u6570\u636e\u5df2\u7ecf\u5165MQ\nYearFinished(year=y).save()\ndebug('Year: {} done'.format(y))", "path": "beat.py", "repo_name": "dongweiming/Mtime", "stars": 98, "license": "None", "language": "python", "size": 204}
{"docstring": "# This places terminating in the global namespace of the worker subprocesses.\n# This allows the worker function to access `terminating` even though it is\n# not passed as an argument to the function.\n", "func_signal": "def initializer(terminating_):\n", "code": "global terminating\nterminating = terminating_", "path": "worker.py", "repo_name": "dongweiming/Mtime", "stars": 98, "license": "None", "language": "python", "size": 204}
{"docstring": "'''\u83b7\u53d6\u7f51\u5361\u7684ip\u5730\u5740'''\n", "func_signal": "def get_ip_address(ifname=IFNAME):\n", "code": "s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\nreturn socket.inet_ntoa(fcntl.ioctl(\n    s.fileno(),\n    0x8915,\n    struct.pack('256s', ifname[:15])\n)[20:24])", "path": "utils.py", "repo_name": "dongweiming/Mtime", "stars": 98, "license": "None", "language": "python", "size": 204}
{"docstring": "'''\u8bbe\u7f6elogging\u8bb0\u5f55\u65e5\u5fd7'''\n", "func_signal": "def logger():\n", "code": "FORMAT = '%(asctime)-15s %(clientip)-15s %(levelname)-8s %(module)-20s %(funcName)-15s %(message)s'  # noqa\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\nformatter = Formatter(fmt=FORMAT, datefmt=DATE_FORMAT)\nhandler = StreamHandler()\nsockethandler = logging.handlers.SocketHandler(SERVER_HOST,\n                                               logging.handlers.DEFAULT_TCP_LOGGING_PORT)  # noqa\nhandler.setFormatter(formatter)\nfor_logger = getLogger('Tencent')\nfor_logger.setLevel(DEBUG)\nfor_logger.addHandler(handler)\nfor_logger.addHandler(sockethandler)\nreturn for_logger", "path": "log.py", "repo_name": "dongweiming/Mtime", "stars": 98, "license": "None", "language": "python", "size": 204}
{"docstring": "'''\u8c03\u7528\u7c7b'''\n", "func_signal": "def __call__(self):\n", "code": "self.page = self.spider()\nif self.page is None:\n    return\nhasnext = self.xpath() is not None\nself.d['movieid'] = self.id\nreturn self.d, hasnext", "path": "parse.py", "repo_name": "dongweiming/Mtime", "stars": 98, "license": "None", "language": "python", "size": 204}
{"docstring": "'''\u62bd\u8c61\u4ee3\u7801\u505a\u591a\u9879\u6b63\u5219\u5339\u914d'''\n", "func_signal": "def checkmatch(regex, instance, type=int):\n", "code": "match = regex.findall(instance.content)\nif not match:\n    return 0\nelse:\n    return type(match[0])", "path": "parse.py", "repo_name": "dongweiming/Mtime", "stars": 98, "license": "None", "language": "python", "size": 204}
{"docstring": "'''sleep\u4e00\u5b9a\u65f6\u95f4'''\n", "func_signal": "def sleep2(interval=None):\n", "code": "num = interval if interval is not None else INTERVAL\ntime.sleep(num)", "path": "utils.py", "repo_name": "dongweiming/Mtime", "stars": 98, "license": "None", "language": "python", "size": 204}
{"docstring": "# init beat\n", "func_signal": "def init_task_db():\n", "code": "task = Task.objects.get_or_create(type='beat')[0]\ntask.update(set__interval=TASK_BEAT, set__last_run_at=datetime.now())\nworker = Task.objects.get_or_create(type='worker')[0]\nworker.update(set__interval=TASK_WORKER, set__last_run_at=datetime.now())", "path": "init.py", "repo_name": "dongweiming/Mtime", "stars": 98, "license": "None", "language": "python", "size": 204}
{"docstring": "'''\u6839\u636e\u5e74\u4efd\u4ece\u524d\u5411\u540e,\u83b7\u53d6\u5f53\u524d\u8981\u6267\u884c\u7684\u7b2c\u4e00\u4e2a\u5e74\u4efd(min)'''\n", "func_signal": "def get_year():\n", "code": "obj = YearFinished.objects\nif obj:\n    c_year = obj.first()\n    return c_year.year\nelse:\n    return MIN_YEAR - 1", "path": "beat.py", "repo_name": "dongweiming/Mtime", "stars": 98, "license": "None", "language": "python", "size": 204}
{"docstring": "'''Modify from rom http://pastebin.com/zYPWHnc6'''\n", "func_signal": "def get_user_agent():\n", "code": "platform = random.choice(['Macintosh', 'Windows', 'X11'])\nif platform == 'Macintosh':\n    os = random.choice(['68K', 'PPC'])\nelif platform == 'Windows':\n    os = random.choice(['Win3.11', 'WinNT3.51', 'WinNT4.0',\n                        'Windows NT 5.0', 'Windows NT 5.1',\n                        'Windows NT 5.2', 'Windows NT 6.0',\n                        'Windows NT 6.1', 'Windows NT 6.2',\n                        'Win95', 'Win98', 'Win 9x 4.90', 'WindowsCE'])\nelif platform == 'X11':\n    os = random.choice(['Linux i686', 'Linux x86_64'])\n\nbrowser = random.choice(['chrome', 'firefox', 'ie'])\nif browser == 'chrome':\n    webkit = str(random.randint(500, 599))\n    version = str(random.randint(0, 24)) + '.0' + \\\n        str(random.randint(0, 1500)) + '.' + \\\n        str(random.randint(0, 999))\n    return 'Mozilla/5.0 (' + os + ') AppleWebKit/' + webkit + \\\n        '.0 (KHTML, live Gecko) Chrome/' + version + ' Safari/' + webkit\nelif browser == 'firefox':\n    year = str(random.randint(2000, 2012))\n    month = random.randint(1, 12)\n    if month < 10:\n        month = '0' + str(month)\n    else:\n        month = str(month)\n    day = random.randint(1, 30)\n    if day < 10:\n        day = '0' + str(day)\n    else:\n        day = str(day)\n    gecko = year + month + day\n    version = random.choice(map(lambda x: str(x) + '.0', range(1, 16)))\n    return 'Mozilla/5.0 (' + os + '; rv:' + version + ') Gecko/' + \\\n        gecko + ' Firefox/' + version\nelif browser == 'ie':\n    version = str(random.randint(1, 10)) + '.0'\n    engine = str(random.randint(1, 5)) + '.0'\n    option = random.choice([True, False])\n    if option:\n        token = random.choice(['.NET CLR', 'SV1', 'Tablet PC', 'WOW64',\n                               'Win64; IA64', 'Win64; x64']) + '; '\n    elif option is False:\n        token = ''\n    return 'Mozilla/5.0 (compatible; MSIE ' + version + '; ' + os + \\\n        '; ' + token + 'Trident/' + engine + ')'", "path": "utils.py", "repo_name": "dongweiming/Mtime", "stars": 98, "license": "None", "language": "python", "size": 204}
{"docstring": "# \u8bf7\u6c42\u5934\u589e\u52a0cc\n", "func_signal": "def spider(self):\n", "code": "s = Spider(additional_headers={'Cache-Control': 'max-age=0'})\ntry:\n    s.fetch(self.url)\nexcept HTTPError as e:\n    # \u68c0\u67e5\u8be5\u7535\u5f71\u76f8\u5173\u9875\u9762\u662f\u5426\u5b58\u5728\n    if e.msg == 'Not Found':\n        return\n# \u56e0\u4e3a\u4e2d\u6587\u88ab\u7f16\u7801\u6210utf-8\u4e4b\u540e\u53d8\u6210'/u2541'\u4e4b\u7c7b\u7684\u5f62\u5f0f\uff0clxml\u4e00\u9047\u5230\"/\"\u5c31\u4f1a\u8ba4\u4e3a\u5176\u6807\u7b7e\u7ed3\u675f\nreturn etree.HTML(s.content.decode('utf-8'))", "path": "parse.py", "repo_name": "dongweiming/Mtime", "stars": 98, "license": "None", "language": "python", "size": 204}
{"docstring": "'''\u83b7\u53d6\u5f53\u524d\u5e74\u4efd\u5305\u542b\u7535\u5f71\u7684\u9875\u6570'''\n", "func_signal": "def get_movie_pages(instance):\n", "code": "try:\n    return max([int(i[1]) for i in\n                movie_page_regex.findall(instance.content)])\nexcept ValueError:\n    # \u53ea\u6709\u4e00\u9875\n    if mtime_vcodeValid_regex.search(instance.content):\n        return\n    return 1", "path": "parse.py", "repo_name": "dongweiming/Mtime", "stars": 98, "license": "None", "language": "python", "size": 204}
{"docstring": "'''\u5b9a\u65f6\u8c03\u5ea6\u51fd\u6570'''\n", "func_signal": "def periodic(scheduler, action, actionargs=()):\n", "code": "scheduler.start(1, periodic,\n                (scheduler, action, actionargs))\naction(*actionargs)", "path": "control.py", "repo_name": "dongweiming/Mtime", "stars": 98, "license": "None", "language": "python", "size": 204}
{"docstring": "'''\u83b7\u53d6\u7535\u5f71\u5728mtime\u7684\u552f\u4e00id'''\n", "func_signal": "def get_movie_ids(instance):\n", "code": "if mtime_vcodeValid_regex.search(instance.content):\n    return\nreturn movie_regex.findall(instance.content)", "path": "parse.py", "repo_name": "dongweiming/Mtime", "stars": 98, "license": "None", "language": "python", "size": 204}
{"docstring": "'''\u722c\u53d6\u957f\u8bc4\u8bba\u9875'''\n", "func_signal": "def get_content(self):\n", "code": "ret = self.spider()\nall = ret.xpath('//div[@class=\"db_mediacont db_commentcont\"]/p')\ncontents = []\nfor elem in all:\n    istext = elem.xpath('text()')\n    if istext:\n        if istext[0].strip():\n            # \u6587\u672c, \u5426\u5219\u7a7a\u884c\n            cur_type = 'text'\n            content = istext[0].strip()\n        else:\n            continue\n    isembed = elem.xpath('embed')\n    if isembed:\n        # \u5185\u5d4cflash\u4e4b\u7c7b\n        cur_type = 'embed'\n        content = str(isembed[0].attrib)\n    isimage = elem.xpath('img')\n    if isimage:\n        # \u56fe\u7247\n        cur_type = 'image'\n        image = []\n        for i in isimage:\n            image.append(i.attrib['src'])\n        content = ','.join(image)\n    contents.append({'type': cur_type, 'content': content})\nreturn contents", "path": "parse.py", "repo_name": "dongweiming/Mtime", "stars": 98, "license": "None", "language": "python", "size": 204}
{"docstring": "'''\u641c\u96c6\u5404client\u65e5\u5fd7\u5230\u670d\u52a1\u7aef'''\n", "func_signal": "def handle_log(socket, address):\n", "code": "chunk = socket.recv(4)\nif len(chunk) < 4:\n    return\nslen = struct.unpack('>L', chunk)[0]\nchunk = socket.recv(slen)\nwhile len(chunk) < slen:\n    chunk = chunk + socket.recv(slen - len(chunk))\nobj = pickle.loads(chunk)\nrecord = makeLogRecord(obj)\nname = record.name\nlogger = getLogger(name)\nlogger.handle(record)\nsocket.close()", "path": "log.py", "repo_name": "dongweiming/Mtime", "stars": 98, "license": "None", "language": "python", "size": 204}
{"docstring": "'''\u901a\u8fc7\u4e2d\u6587\u7c7b\u578b\u7684\u6587\u672c\u89e3\u6790\u6210datetime\u7c7b\u578b\u7684\u65e5\u671f\u7ed3\u679c'''\n", "func_signal": "def make_datetime(text):\n", "code": "make = lambda t: datetime(int(t[0]), int(t[1]), int(t[2]))\nt = date_regex.findall(text)\nif t:\n    if len(t) == 1:\n        return make(t[0])\n    else:\n        return [make(i) for i in t]\nelse:\n    return datetime.now()", "path": "parse.py", "repo_name": "dongweiming/Mtime", "stars": 98, "license": "None", "language": "python", "size": 204}
{"docstring": "'''\u6539\u53d8\u4efb\u52a1\u6267\u884c\u7684\u95f4\u9694'''\n", "func_signal": "def change_interval(self, interval=None, incr=False, decr=False):\n", "code": "if incr:\n    interval = self.task.interval * 2\nelif decr:\n    interval = self.task.interval / 2\nelif interval is not None:\n    interval = interval\ndebug('Change interval to: ' + str(interval))\nself.task.update(set__interval=interval)", "path": "control.py", "repo_name": "dongweiming/Mtime", "stars": 98, "license": "None", "language": "python", "size": 204}
{"docstring": "# \u9ed8\u8ba4\u7684\u5934\u4fe1\u606f\n", "func_signal": "def http_request(self, req):\n", "code": "req.add_header('Accept-Encoding', 'gzip, deflate')\nreq.add_header('User-Agent', get_user_agent())\nreq.add_header('Accept-Language',\n               'zh-cn,zh;q=0.8,en-us;q=0.5,en;q=0.3')\nif self.additional_headers is not None:\n    req.headers.update(self.additional_headers)\nif self.cookiejar is not None:\n    self.cookiejar.add_cookie_header(req)\nreturn req", "path": "spider.py", "repo_name": "dongweiming/Mtime", "stars": 98, "license": "None", "language": "python", "size": 204}
{"docstring": "'''\u5217\u8868\u5206\u7ec4: \u6bcf\u7ec4size\u4e2a'''\n", "func_signal": "def group(seq, size):\n", "code": "l = len(seq)\nfor i in range(0, l, size):\n    yield seq[i:i + size]", "path": "utils.py", "repo_name": "dongweiming/Mtime", "stars": 98, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"Test Bind9 normalization\"\"\"\n", "func_signal": "def test_bind9(self):\n", "code": "self.aS(\"Oct 22 01:27:16 pluto named: client 192.168.198.130#4532: bad zone transfer request: 'www.abc.com/IN': non-authoritative zone (NOTAUTH)\",\n        {'event_id' : \"zone_transfer_bad\",\n         'zone' : \"www.abc.com\",\n         'source_ip' : '192.168.198.130',\n         'class' : 'IN',\n         'program' : 'named'})\nself.aS(\"Oct 22 01:27:16 pluto named: general: notice: client 10.10.4.4#39583: query: tpf.qa.ifr.lan IN SOA +\",\n        {'event_id' : \"client_query\",\n         'domain' : \"tpf.qa.ifr.lan\",\n         'category' : \"general\",\n         'severity' : \"notice\",\n         'class' : \"IN\",\n         'source_ip' : \"10.10.4.4\",\n         'program' : 'named'})\nself.aS(\"Oct 22 01:27:16 pluto named: createfetch: 126.92.194.77.zen.spamhaus.org A\",\n        {'event_id' : \"fetch_request\",\n         'domain' : \"126.92.194.77.zen.spamhaus.org\",\n         'program' : 'named'})", "path": "tests\\test_log_samples.py", "repo_name": "wallix/pylogsparser", "stars": 123, "license": "lgpl-2.1", "language": "python", "size": 978}
{"docstring": "\"\"\"Testing CISCO ASA logs\"\"\"\n", "func_signal": "def test_cisco_asa(self):\n", "code": "self.aS(\"\"\"<168>Mar 05 2010 11:06:12 ciscoasa : %ASA-6-305011: Built dynamic TCP translation from 14net:14.36.103.220/300 to 172net:172.18.254.146/55\"\"\",\n       {'program': 'cisco-asa',\n        'severity_code': '6',\n        'event_id': '305011',\n        'date': datetime(2010, 3, 5, 11, 6, 12),\n        'taxonomy': 'firewall',\n        'outbound_int': '172net',\n        'dest_port': '55'})\nself.aS(\"\"\"<168>Jul 02 2006 07:33:45 ciscoasa : %ASA-6-302013: Built outbound TCP connection 8300517 for outside:64.156.4.191/110 (64.156.4.191/110) to inside:192.168.8.12/3109 (xxx.xxx.185.142/11310)\"\"\",\n       {'program': 'cisco-asa',\n        'severity_code': '6',\n        'event_id': '302013',\n        'date': datetime(2006, 7, 2, 7, 33, 45),\n        'taxonomy': 'firewall',\n        'outbound_int': 'inside',\n        'dest_ip': '192.168.8.12'})", "path": "tests\\test_log_samples.py", "repo_name": "wallix/pylogsparser", "stars": 123, "license": "lgpl-2.1", "language": "python", "size": 978}
{"docstring": "\"\"\"Testing Win2003 security audit logs (english)\"\"\"\n", "func_signal": "def test_eventlogW3EN(self):\n", "code": "self.aS(\"\"\"<13>Nov 21 16:28:40 w2003en MSWinEventLog\t0\\tSecurity\\t129\\tWed Nov 21 16:28:40\\t2012\\t592\\tSecurity\\tSYSTEM\\tUser\\tSuccess Audit\\tW2003EN\\tDetailed Tracking\\tA new process has been created:     New Process ID: 1536     Image File Name: C:\\WINDOWS\\system32\\wpabaln.exe     Creator Process ID: 540     User Name: W2003EN$     Domain: WORKGROUP     Logon ID: (0x0,0x3E7)\\t99\"\"\",\n       {\n         'criticality': '0',\n         'eventlog_id': '592',\n         'eventlog_source': 'Security',\n         'eventlog_name': 'Security',\n         'source_host': 'W2003EN',\n         'eventlog_type': 'Success Audit',\n         'program' : 'EventLog',\n         'md5_checksum' : '99',\n         'eventlog_description': \"\"\"A new process has been created:     New Process ID: 1536     Image File Name: C:\\WINDOWS\\system32\\wpabaln.exe     Creator Process ID: 540     User Name: W2003EN$     Domain: WORKGROUP     Logon ID: (0x0,0x3E7)\"\"\",\n         \"file_name\" : \"C:\\WINDOWS\\system32\\wpabaln.exe\",\n         \"user\" : \"W2003EN$\",\n         \"domain\" : \"WORKGROUP\",\n         \"logon_id\" : \"(0x0,0x3E7)\",\n         \"pid\" : \"1536\",})         \n\nself.aS(\"\"\"<13>Nov 21 17:45:05 w2003en MSWinEventLog 1\\tSecurity\\t233\\tWed Nov 21 17:44:59\\t2012\\t529\\tSecurity\\tSYSTEM\\tUser\\tFailure Audit\\tW2003EN\\tLogon/Logoff\\tLogon Failure:     Reason: Unknown user name or bad password     User Name: Administrator     Domain: W2003EN     Logon Type: 2     Logon Process: User32       Authentication Package: Negotiate     Workstation Name: W2003EN     Caller User Name: W2003EN$     Caller Domain: WORKGROUP     Caller Logon ID: (0x0,0x3E7)     Caller Process ID: 484     Transited Services: -     Source Network Address: 127.0.0.1     Source Port: 0\\t206\"\"\", \n       {'criticality': '1',\n         'eventlog_id': '529',\n         'eventlog_source': 'Security',\n         'eventlog_name': 'Security',\n         'source_host': 'W2003EN',\n         'eventlog_type': 'Failure Audit',\n         'program' : 'EventLog',\n         'md5_checksum' : '206',\n         \"user\" : \"Administrator\",\n         \"domain\" : \"W2003EN\",\n         \"logon_type\" : \"2\",\n         \"method\" : \"Interactive\",\n         \"authentication_package\" : 'Negotiate',\n         \"dest_host\" : \"W2003EN\",\n         \"caller\" : \"W2003EN$\",\n         \"caller_domain\" : \"WORKGROUP\",\n         \"caller_logon_id\" : \"(0x0,0x3E7)\",\n         \"status\" : \"failure\",\n         \"reason\" : \"Unknown user name or bad password\",\n         \"source_ip\" : \"127.0.0.1\",\n         \"source_port\" : \"0\",\n         'eventlog_description': \"\"\"Logon Failure:     Reason: Unknown user name or bad password     User Name: Administrator     Domain: W2003EN     Logon Type: 2     Logon Process: User32       Authentication Package: Negotiate     Workstation Name: W2003EN     Caller User Name: W2003EN$     Caller Domain: WORKGROUP     Caller Logon ID: (0x0,0x3E7)     Caller Process ID: 484     Transited Services: -     Source Network Address: 127.0.0.1     Source Port: 0\"\"\", })  \n\nself.aS(\"\"\"<13>Nov 21 17:45:25 w2003en MSWinEventLog\t1\\tSecurity\\t237\\tWed Nov 21 17:45:25\\t2012\\t576\\tSecurity\\tAdministrator\\tUser\\tSuccess Audit\\tW2003EN\\tPrivilege Use\\tSpecial privileges assigned to new logon:     User Name: Administrator     Domain: W2003EN     Logon ID: (0x0,0x3A092)     Privileges: SeSecurityPrivilege   SeBackupPrivilege   SeRestorePrivilege   SeTakeOwnershipPrivilege   SeDebugPrivilege   SeSystemEnvironmentPrivilege   SeLoadDriverPrivilege   SeImpersonatePrivilege\\t210\"\"\", \n       {'criticality': '1',\n         'eventlog_id': '576',\n         'eventlog_source': 'Security',\n         'eventlog_name': 'Security',\n         'source_host': 'W2003EN',\n         'eventlog_type': 'Success Audit',\n         'program' : 'EventLog',\n         'md5_checksum' : '210',\n         \"user\" : \"Administrator\",\n         \"domain\" : \"W2003EN\",\n         \"logon_id\" : \"(0x0,0x3A092)\",\n         \"privileges\" : \"SeSecurityPrivilege   SeBackupPrivilege   SeRestorePrivilege   SeTakeOwnershipPrivilege   SeDebugPrivilege   SeSystemEnvironmentPrivilege   SeLoadDriverPrivilege   SeImpersonatePrivilege\",\n         'eventlog_description': \"\"\"Special privileges assigned to new logon:     User Name: Administrator     Domain: W2003EN     Logon ID: (0x0,0x3A092)     Privileges: SeSecurityPrivilege   SeBackupPrivilege   SeRestorePrivilege   SeTakeOwnershipPrivilege   SeDebugPrivilege   SeSystemEnvironmentPrivilege   SeLoadDriverPrivilege   SeImpersonatePrivilege\"\"\", })", "path": "tests\\test_log_samples.py", "repo_name": "wallix/pylogsparser", "stars": 123, "license": "lgpl-2.1", "language": "python", "size": 978}
{"docstring": "\"\"\"Test Exchange 2003 message tracking log normalization\"\"\"\n", "func_signal": "def test_MSExchange2003MTL(self):\n", "code": "self.aS(\"\"\"2012-11-29\\t10:42:15 GMT\\t10.10.40.254\\tmx2.wallix.com\\t-\\tEXCHANGE\\t10.10.46.74\\tmhuin@exchange.wallix.fr\\t1019\\t353399610.5513.1354185742534.JavaMail.root@zimbra.ifr.lan\\t0\\t0\\t2532\\t1\\t2012-11-29 10:42:15 GMT\\t0\\tVersion: 6.0.3790.4675\\t-\\t-\\tmatthieu.huin@wallix.com\\t-\"\"\",\n        {\n         'source_host': 'mx2.wallix.com',\n         'source_ip': '10.10.40.254',\n         'partner' : '-',\n         'creation_time': '2012-11-29 10:42:15',\n         'date': datetime(2012, 11, 29, 10, 42, 15),\n         'message_recipient' : 'mhuin@exchange.wallix.fr',\n         'event': 'SMTP submit message to AQ',\n         'event_id': '1019',\n         'internal_message_id': '353399610.5513.1354185742534.JavaMail.root@zimbra.ifr.lan',\n         'priority' : '0',\n         'recipient_status' : '0',\n         'len' : '2532',\n         'recipient_count' : '1',\n         'encryption': '0',\n         'service_version' : \"Version: 6.0.3790.4675\",\n         'linked_message_id' : '-',\n         'message_subject' : \"-\",\n         \"message_sender\" : \"matthieu.huin@wallix.com\",\n         'program': 'MS Exchange 2003 Message Tracking',\n         'dest_host': 'EXCHANGE',\n         'dest_ip': '10.10.46.74',})", "path": "tests\\test_log_samples.py", "repo_name": "wallix/pylogsparser", "stars": 123, "license": "lgpl-2.1", "language": "python", "size": 978}
{"docstring": "\"\"\" Verify that lognormalizer does not extract\nsyslog header as tags when syslog normalizer is deactivated.\n\"\"\"\n", "func_signal": "def test_006_normalizer_test_a_syslog_log_with_syslog_deactivate(self):\n", "code": "testlog = {'raw': 'Jul 18 08:55:35 naruto app[3245]: body message'}\nln = LogNormalizer(self.normalizer_path)\nactive_n = ln.get_active_normalizers()\nto_deactivate = [n for n in active_n.keys() if n.find('syslog') >= 0]\nfor n in to_deactivate:\n    del active_n[n]\nln.set_active_normalizers(active_n)\nln.reload()\nln.lognormalize(testlog)\nself.assertTrue('uuid' in testlog.keys())\nself.assertFalse('date' in testlog.keys())\nself.assertFalse('program' in testlog.keys())", "path": "tests\\test_lognormalizer.py", "repo_name": "wallix/pylogsparser", "stars": 123, "license": "lgpl-2.1", "language": "python", "size": 978}
{"docstring": "\"\"\"Testing openLDAP logs\"\"\"\n", "func_signal": "def test_openLDAP(self):\n", "code": "self.aS(\"\"\"Jun 12 11:18:47 openLDAP slapd[870]: conn=1007 op=0 RESULT tag=97 err=53 text=unauthenticated bind (DN with no password) disallowed\"\"\",\n       {'program': 'slapd',\n        'source': 'openLDAP',\n        'connection_id' : '1007',\n        'operation_id' : '0',\n        'action' : 'RESULT',\n        'tag_code' : '97',\n        'error_code': '53',\n        'response_type' : 'Bind',\n        'status' : 'Service error - unwilling to perform',\n        'reason' : 'unauthenticated bind (DN with no password) disallowed',\n        })\nself.aS('Jun 12 11:14:20 openLDAP slapd[870]: conn=1002 op=0 SRCH base=\"\" scope=0 deref=0 filter=\"(objectClass=*)\"',\n       {'program': 'slapd',\n        'source': 'openLDAP',\n        'connection_id' : '1002',\n        'operation_id' : '0',\n        'action' : 'SRCH',\n        'deref' : '0',\n        'scope_code': '0',\n        'scope' : 'base',\n        'filter' : '(objectClass=*)',\n        })\nself.aS('Jun 11 15:52:37 openLDAP slapd[1814]: conn=1012 fd=14 ACCEPT from IP=10.10.4.7:39450 (IP=10.10.4.250:389)',\n       {'program': 'slapd',\n        'source': 'openLDAP',\n        'connection_id' : '1012',\n        'socket_id' : '14',\n        'action' : 'ACCEPT',\n        'source_ip' : '10.10.4.7',\n        'source_port': '39450',\n        'local_ip' : '10.10.4.250',\n        'local_port' : '389',\n        })", "path": "tests\\test_log_samples.py", "repo_name": "wallix/pylogsparser", "stars": 123, "license": "lgpl-2.1", "language": "python", "size": 978}
{"docstring": "\"\"\"Test SSHd normalization\"\"\"\n", "func_signal": "def test_sshd(self):\n", "code": "self.aS(\"<40>Dec 26 10:32:40 naruto sshd[2274]: Failed password for bernat from 127.0.0.1 port 37234 ssh2\",\n        { 'program': 'sshd',\n          'action': 'fail',\n          'user': 'bernat',\n          'method': 'password',\n          'source_ip': '127.0.0.1' })\nself.aS(\"<40>Dec 26 10:32:40 naruto sshd[2274]: Failed password for invalid user jfdghfg from 127.0.0.1 port 37234 ssh2\",\n        { 'program': 'sshd',\n          'action': 'fail',\n          'user': 'jfdghfg',\n          'method': 'password',\n          'source_ip': '127.0.0.1' })\nself.aS(\"<40>Dec 26 10:32:40 naruto sshd[2274]: Failed none for invalid user kgjfk from 127.0.0.1 port 37233 ssh2\",\n        { 'program': 'sshd',\n          'action': 'fail',\n          'user': 'kgjfk',\n          'method': 'none',\n          'source_ip': '127.0.0.1' })\nself.aS(\"<40>Dec 26 10:32:40 naruto sshd[2274]: Accepted password for bernat from 127.0.0.1 port 37234 ssh2\",\n        { 'program': 'sshd',\n          'action': 'accept',\n          'user': 'bernat',\n          'method': 'password',\n          'source_ip': '127.0.0.1' })\nself.aS(\"<40>Dec 26 10:32:40 naruto sshd[2274]: Accepted publickey for bernat from 192.168.251.2 port 60429 ssh2\",\n        { 'program': 'sshd',\n          'action': 'accept',\n          'user': 'bernat',\n          'method': 'publickey',\n          'source_ip': '192.168.251.2' })\n# See http://www.ossec.net/en/attacking-loganalysis.html\nself.aS(\"<40>Dec 26 10:32:40 naruto sshd[2274]: Failed password for invalid user myfakeuser from 10.1.1.1 port 123 ssh2 from 192.168.50.65 port 34813 ssh2\",\n       { 'program': 'sshd',\n          'action': 'fail',\n          'user': 'myfakeuser from 10.1.1.1 port 123 ssh2',\n          'method': 'password',\n          'source_ip': '192.168.50.65' })", "path": "tests\\test_log_samples.py", "repo_name": "wallix/pylogsparser", "stars": 123, "license": "lgpl-2.1", "language": "python", "size": 978}
{"docstring": "\"\"\"Test syslog logs with a timezone info\"\"\"\n", "func_signal": "def test_syslog_with_timezone(self):\n", "code": "try:\n    # Linux/Unix specific I guess ?\n    localtz = pytz.timezone(file('/etc/timezone').read()[:-1])\nexcept:\n    self.skipTest(\"Could not find local timezone, skipping test\")    \n# Tokyo drift\ntokyo = pytz.timezone('Asia/Tokyo')\noffset = _get_timeoffset(tokyo,localtz)\n# first test the past\nnow = datetime.now() + offset + timedelta(hours=-2)\nself.aS(\"<40>%s neo kernel: tun_wallix: Disabled Privacy Extensions\" % now.strftime(\"%b %d %H:%M:%S\"),\n        {'body': 'tun_wallix: Disabled Privacy Extensions',\n         'severity': 'emerg',\n         'severity_code' : '0',\n         'facility': 'syslog',\n         'facility_code' : '5',\n         'source': 'neo',\n         'program': 'kernel',\n         'date': now.replace(microsecond=0)},\n         tzinfo = 'Asia/Tokyo')\n# then fight the future\nnow = datetime.now() + offset + timedelta(hours=+2)\nself.aS(\"<40>%s neo kernel: tun_wallix: Disabled Privacy Extensions\" % now.strftime(\"%b %d %H:%M:%S\"),\n        {'body': 'tun_wallix: Disabled Privacy Extensions',\n         'severity': 'emerg',\n         'severity_code' : '0',\n         'facility': 'syslog',\n         'facility_code' : '5',\n         'source': 'neo',\n         'program': 'kernel',\n         'date': now.replace(microsecond=0, year=now.year-1)},\n         tzinfo = 'Asia/Tokyo')\n# and finally, without the tz info ?\nnow = datetime.now() + offset\ntotal_seconds = (offset.microseconds + (offset.seconds + offset.days * 24 * 3600) * 10**6) / 10**6\n# New in python 2.7\n#total_seconds = offset.total_seconds()\nif total_seconds > 60:\n    d = now.replace(microsecond=0, year=now.year-1)\nelse:\n    d = now.replace(microsecond=0)\nself.aS(\"<40>%s neo kernel: tun_wallix: Disabled Privacy Extensions\" % now.strftime(\"%b %d %H:%M:%S\"),\n        {'body': 'tun_wallix: Disabled Privacy Extensions',\n         'severity': 'emerg',\n         'severity_code' : '0',\n         'facility': 'syslog',\n         'facility_code' : '5',\n         'source': 'neo',\n         'program': 'kernel',\n         'date': d})\n# Operation Anchorage\nanchorage = pytz.timezone('America/Anchorage')\noffset = _get_timeoffset(anchorage,localtz)\n# first test the past\nnow = datetime.now() + offset + timedelta(hours=-2)\nself.aS(\"<40>%s neo kernel: tun_wallix: Disabled Privacy Extensions\" % now.strftime(\"%b %d %H:%M:%S\"),\n        {'body': 'tun_wallix: Disabled Privacy Extensions',\n         'severity': 'emerg',\n         'severity_code' : '0',\n         'facility': 'syslog',\n         'facility_code' : '5',\n         'source': 'neo',\n         'program': 'kernel',\n         'date': now.replace(microsecond=0)},\n         tzinfo = 'America/Anchorage')\n# then fight the future\nnow = datetime.now() + offset + timedelta(hours=+2)\nself.aS(\"<40>%s neo kernel: tun_wallix: Disabled Privacy Extensions\" % now.strftime(\"%b %d %H:%M:%S\"),\n        {'body': 'tun_wallix: Disabled Privacy Extensions',\n         'severity': 'emerg',\n         'severity_code' : '0',\n         'facility': 'syslog',\n         'facility_code' : '5',\n         'source': 'neo',\n         'program': 'kernel',\n         'date': now.replace(microsecond=0, year=now.year-1)},\n         tzinfo = 'America/Anchorage')\n# and finally, without the tz info ?\nnow = datetime.now() + offset\ntotal_seconds = (offset.microseconds + (offset.seconds + offset.days * 24 * 3600) * 10**6) / 10**6\n# New in python 2.7\n#total_seconds = offset.total_seconds()\nif total_seconds > 60:\n    d = now.replace(microsecond=0, year=now.year-1)\nelse:\n    d = now.replace(microsecond=0)\nself.aS(\"<40>%s neo kernel: tun_wallix: Disabled Privacy Extensions\" % now.strftime(\"%b %d %H:%M:%S\"),\n        {'body': 'tun_wallix: Disabled Privacy Extensions',\n         'severity': 'emerg',\n         'severity_code' : '0',\n         'facility': 'syslog',\n         'facility_code' : '5',\n         'source': 'neo',\n         'program': 'kernel',\n         'date': d})", "path": "tests\\test_log_samples.py", "repo_name": "wallix/pylogsparser", "stars": 123, "license": "lgpl-2.1", "language": "python", "size": 978}
{"docstring": "\"\"\"Test VMware ESX 4.x and VMware ESXi 4.x log normalization\"\"\"\n", "func_signal": "def test_vmwareESX4_ESXi4(self):\n", "code": "self.aS(\"\"\"[2011-09-05 16:06:30.016 F4CD1B90 verbose 'Locale' opID=996867CC-000002A6] Default resource used for 'host.SystemIdentificationInfo.IdentifierType.ServiceTag.summary' expected in module 'enum'.\"\"\",\n\t{'date': datetime(2011, 9, 5, 16, 6, 30),\n \t 'numeric': 'F4CD1B90',\n \t 'level': 'verbose',\n \t 'alpha': 'Locale',\n \t 'body': 'Default resource used for \\'host.SystemIdentificationInfo.IdentifierType.ServiceTag.summary\\' expected in module \\'enum\\'.'})\n\nself.aS(\"\"\"sysboot: Executing 'kill -TERM 314'\"\"\",\n\t{'body': 'Executing \\'kill -TERM 314\\''})", "path": "tests\\test_log_samples.py", "repo_name": "wallix/pylogsparser", "stars": 123, "license": "lgpl-2.1", "language": "python", "size": 978}
{"docstring": "\"\"\" Verify we can can deal with multiple normalizer paths.\n\"\"\"\n", "func_signal": "def test_008_normalizer_multiple_paths(self):\n", "code": "fdir = tempfile.mkdtemp()\nsdir = tempfile.mkdtemp()\nfor f in os.listdir(self.normalizer_path):\n    path_f = os.path.join(self.normalizer_path, f)\n    if os.path.isfile(path_f):\n        shutil.copyfile(path_f, os.path.join(fdir, f))\nshutil.move(os.path.join(fdir, 'postfix.xml'), \n            os.path.join(sdir, 'postfix.xml'))\nln = LogNormalizer([fdir, sdir])\nsource = ln.get_normalizer_source('postfix-0.99')\nself.assertEquals(XMLfromstring(source).getroottree().getroot().get('name'), 'postfix')\nself.assertTrue(ln.get_normalizer_path('postfix-0.99').__contains__(os.path.basename(sdir)))\nself.assertTrue(ln.get_normalizer_path('syslog-1.0').__contains__(os.path.basename(fdir)))\nxml_src = ln.get_normalizer_source('syslog-1.0')\nos.unlink(os.path.join(fdir, 'syslog.xml'))\nln.reload()\nself.assertRaises(ValueError, ln.get_normalizer_path, 'syslog-1.0')\nln.update_normalizer(xml_src, dir_path = sdir)\nself.assertTrue(ln.get_normalizer_path('syslog-1.0').__contains__(os.path.basename(sdir)))\nshutil.rmtree(fdir)\nshutil.rmtree(sdir)", "path": "tests\\test_lognormalizer.py", "repo_name": "wallix/pylogsparser", "stars": 123, "license": "lgpl-2.1", "language": "python", "size": 978}
{"docstring": "\"\"\" Verify that normalizer deactivation is working.\n\"\"\"\n", "func_signal": "def test_002_deactivate_normalizer(self):\n", "code": "ln = LogNormalizer(self.normalizer_path)\nactive_n = ln.get_active_normalizers()\nto_deactivate = active_n.keys()[:2]\nfor to_d in to_deactivate:\n    del active_n[to_d]\nln.set_active_normalizers(active_n)\nln.reload()\nself.assertEqual(len([an[0] for an in ln.get_active_normalizers().items() if an[1]]), len(ln)-2)\nself.assertEqual(len(ln._cache), len(ln)-2)", "path": "tests\\test_lognormalizer.py", "repo_name": "wallix/pylogsparser", "stars": 123, "license": "lgpl-2.1", "language": "python", "size": 978}
{"docstring": "\"\"\"args is a list of ordered date elements, from month and day (both \nmandatory) to eventual second. The function gives the most sensible \nyear for that set of values, so that the date is not set in the future.\"\"\"\n", "func_signal": "def get_sensible_year(*args):\n", "code": "year = int(datetime.now().year)\nd = datetime(year, *args)\nif d > datetime.now():\n    return year - 1\nreturn year", "path": "tests\\test_commonElements.py", "repo_name": "wallix/pylogsparser", "stars": 123, "license": "lgpl-2.1", "language": "python", "size": 978}
{"docstring": "\"\"\"Testing Win2008 security audit logs (english)\"\"\"\n", "func_signal": "def test_eventlogW8EN(self):\n", "code": "self.aS(u\"\"\"<13>Nov 21 17:45:25 WIN-D7NM05T4KNM    MSWinEventLog    1\\tSecurity\\t399\\tThu Jan 31 13:18:31\\t2013\\t4624\\tMicrosoft-Windows-Security-Auditing\\tWIN-D7NM05T4KNM\\Administrator\\tN/A\\tSuccess Audit\\tWIN-D7NM05T4KNM\\tLogon\\tAn account was successfully logged on. Subject: Security ID: S-1-5-18 Account Name: WIN-D7NM05T4KNM$ Account Domain: WORKGROUP Logon ID: 0x3e7 Logon Type: 2 New Logon: Security ID: S-1-5-21-2218251928-2375033965-419438225-500 Account Name: Administrator Account Domain: WIN-D7NM05T4KNM Logon ID: 0xd30cd Logon GUID: {00000000-0000-0000-0000-000000000000} Process Information: Process ID: 0x5e0 Process Name: C:\\Windows\\System32\\winlogon.exe Network Information: Workstation Name: WIN-D7NM05T4KNM Source Network Address: 127.0.0.1 Source Port: 0 Detailed Authentication Information: Logon Process: User32 Authentication Package: Negotiate Transited Services: - Package Name (NTLM only): - Key Length: 0 This event is generated when a logon session is created. It is generated on the computer that was accessed. The subject fields indicate the account on the local system which requested the logon. This is most commonly a service such as the Server service, or a local process such as Winlogon.exe or Services.exe. The logon type field indicates the kind of logon that occurred. The most common types are 2 (interactive) and 3 (network). The New Logon fields indicate the account for whom the new logon was created, i.e. the account that was logged on. The network fields indicate where a remote logon request originated. Workstation name is not always available and may be left blank in some cases. The authentication information fields provide detailed information about this specific logon request. - Logon GUID is a unique identifier that can be used to correlate this event with a KDC event. - Transited services indicate which intermediate services have participated in this logon request. - Package name indicates which sub-protocol was used among the NTLM protocols. - Key length indicates the length of the generated session key. This will be 0 if no session key was requested.\\t271\"\"\",\n        {\n         'criticality': '1',\n         'eventlog_id': '4624',\n         'eventlog_source': 'Security',\n         'eventlog_name': 'Microsoft-Windows-Security-Auditing',\n         'source_host': 'WIN-D7NM05T4KNM',\n         'eventlog_type': 'Success Audit',\n         'program' : 'EventLog',\n         'md5_checksum' : '271',\n         'eventlog_description': \"\"\"An account was successfully logged on. Subject: Security ID: S-1-5-18 Account Name: WIN-D7NM05T4KNM$ Account Domain: WORKGROUP Logon ID: 0x3e7 Logon Type: 2 New Logon: Security ID: S-1-5-21-2218251928-2375033965-419438225-500 Account Name: Administrator Account Domain: WIN-D7NM05T4KNM Logon ID: 0xd30cd Logon GUID: {00000000-0000-0000-0000-000000000000} Process Information: Process ID: 0x5e0 Process Name: C:\\Windows\\System32\\winlogon.exe Network Information: Workstation Name: WIN-D7NM05T4KNM Source Network Address: 127.0.0.1 Source Port: 0 Detailed Authentication Information: Logon Process: User32 Authentication Package: Negotiate Transited Services: - Package Name (NTLM only): - Key Length: 0 This event is generated when a logon session is created. It is generated on the computer that was accessed. The subject fields indicate the account on the local system which requested the logon. This is most commonly a service such as the Server service, or a local process such as Winlogon.exe or Services.exe. The logon type field indicates the kind of logon that occurred. The most common types are 2 (interactive) and 3 (network). The New Logon fields indicate the account for whom the new logon was created, i.e. the account that was logged on. The network fields indicate where a remote logon request originated. Workstation name is not always available and may be left blank in some cases. The authentication information fields provide detailed information about this specific logon request. - Logon GUID is a unique identifier that can be used to correlate this event with a KDC event. - Transited services indicate which intermediate services have participated in this logon request. - Package name indicates which sub-protocol was used among the NTLM protocols. - Key length indicates the length of the generated session key. This will be 0 if no session key was requested.\"\"\",\n         'security_id': 'S-1-5-18',\n         'old_user': 'WIN-D7NM05T4KNM$',\n         'old_domain': 'WORKGROUP',\n         'old_logon_id': '0x3e7',\n         'logon_type':'2',\n         'method': 'Interactive',\n         'security_id': 'S-1-5-21-2218251928-2375033965-419438225-500',\n         'user': 'Administrator',\n         'domain': 'WIN-D7NM05T4KNM',\n         'logon_id': '0xd30cd',\n         'logon_guid': '{00000000-0000-0000-0000-000000000000}',\n         'pid': '0x5e0',\n         'process_name': 'C:\\Windows\\System32\\winlogon.exe',\n         'workstation_name':'WIN-D7NM05T4KNM',\n         'source_ip': '127.0.0.1',\n         'source_port': '0',\n         'logon_process': 'User32',\n         'authentification_package': 'Negotiate',\n         'transited_services': '-',\n         'package_name': '-',\n         'key_length': '0',\n         'status' : 'success'\n         })\n\nself.aS(u\"\"\"<13>Nov 21 17:45:25 WIN-D7NM05T4KNM    MSWinEventLog    1\\tSecurity\\t392\\tThu Jan 31 13:18:27\\t2013\\t4625\\tMicrosoft-Windows-Security-Auditing\\tWIN-D7NM05T4KNM\\Administrator\\tN/A\\tFailure Audit\\tWIN-D7NM05T4KNM\\tLogon\\tAn account failed to log on. Subject: Security ID: S-1-5-18 Account Name: WIN-D7NM05T4KNM$ Account Domain: WORKGROUP Logon ID: 0x3e7 Logon Type: 2 Account For Which Logon Failed: Security ID: S-1-0-0 Account Name: Administrator Account Domain: WIN-D7NM05T4KNM Failure Information: Failure Reason: Unknown user name or bad password. Status: 0xc000006d Sub Status: 0xc000006a Process Information: Caller Process ID: 0x5e0 Caller Process Name: C:\\Windows\\System32\\winlogon.exe Network Information: Workstation Name: WIN-D7NM05T4KNM Source Network Address: 127.0.0.1 Source Port: 0 Detailed Authentication Information: Logon Process: User32 Authentication Package: Negotiate Transited Services: - Package Name (NTLM only): - Key Length: 0 This event is generated when a logon request fails. It is generated on the computer where access was attempted. The Subject fields indicate the account on the local system which requested the logon. This is most commonly a service such as the Server service, or a local process such as Winlogon.exe or Services.exe. The Logon Type field indicates the kind of logon that was requested. The most common types are 2 (interactive) and 3 (network). The Process Information fields indicate which account and process on the system requested the logon. The Network Information fields indicate where a remote logon request originated. Workstation name is not always available and may be left blank in some cases. The authentication information fields provide detailed information about this specific logon request. - Transited services indicate which intermediate services have participated in this logon request. - Package name indicates which sub-protocol was used among the NTLM protocols. - Key length indicates the length of the generated session key. This will be 0 if no session key was requested.\\t268\"\"\",\n        {\n         'criticality': '1',\n         'eventlog_id': '4625',\n         'eventlog_source': 'Security',\n         'eventlog_name': 'Microsoft-Windows-Security-Auditing',\n         'source_host': 'WIN-D7NM05T4KNM',\n         'eventlog_type': 'Failure Audit',\n         'program' : 'EventLog',\n         'md5_checksum' : '268',\n         'eventlog_description': \"\"\"An account failed to log on. Subject: Security ID: S-1-5-18 Account Name: WIN-D7NM05T4KNM$ Account Domain: WORKGROUP Logon ID: 0x3e7 Logon Type: 2 Account For Which Logon Failed: Security ID: S-1-0-0 Account Name: Administrator Account Domain: WIN-D7NM05T4KNM Failure Information: Failure Reason: Unknown user name or bad password. Status: 0xc000006d Sub Status: 0xc000006a Process Information: Caller Process ID: 0x5e0 Caller Process Name: C:\\Windows\\System32\\winlogon.exe Network Information: Workstation Name: WIN-D7NM05T4KNM Source Network Address: 127.0.0.1 Source Port: 0 Detailed Authentication Information: Logon Process: User32 Authentication Package: Negotiate Transited Services: - Package Name (NTLM only): - Key Length: 0 This event is generated when a logon request fails. It is generated on the computer where access was attempted. The Subject fields indicate the account on the local system which requested the logon. This is most commonly a service such as the Server service, or a local process such as Winlogon.exe or Services.exe. The Logon Type field indicates the kind of logon that was requested. The most common types are 2 (interactive) and 3 (network). The Process Information fields indicate which account and process on the system requested the logon. The Network Information fields indicate where a remote logon request originated. Workstation name is not always available and may be left blank in some cases. The authentication information fields provide detailed information about this specific logon request. - Transited services indicate which intermediate services have participated in this logon request. - Package name indicates which sub-protocol was used among the NTLM protocols. - Key length indicates the length of the generated session key. This will be 0 if no session key was requested.\"\"\",\n         'old_security_id': 'S-1-5-18',\n         'old_user': 'WIN-D7NM05T4KNM$',\n         'old_domain': 'WORKGROUP',\n         'old_logon_id': '0x3e7',\n         'logon_type': '2',\n         'method': 'Interactive',                   \n         'security_id': 'S-1-0-0',\n         'user': 'Administrator',\n         'domain': 'WIN-D7NM05T4KNM',\n         'failure_reason': 'Unknown user name or bad password.',\n         'failure_status': '0xc000006d',\n         'failure_sub_status': '0xc000006a',\n         'caller_pid': '0x5e0',\n         'caller_process_name': 'C:\\Windows\\System32\\winlogon.exe',\n         'workstation_name': 'WIN-D7NM05T4KNM',\n         'source_ip': '127.0.0.1',\n         'source_port':'0',\n         'logon_process': 'User32',\n         'authentification_package': 'Negotiate',\n         'transited_services': '-',\n         'package_name' : '-',\n         'key_length': '0',\n         'status': 'failure'           \n         })\n\nself.aS(u\"\"\"<13>Nov 21 17:45:25 WIN-D7NM05T4KNM    MSWinEventLog    1\\tSecurity\\t384\\tThu Jan 31 13:18:19\\t2013\\t4634\\tMicrosoft-Windows-Security-Auditing\\tWIN-D7NM05T4KNM\\Administrator\\tN/A\\tSuccess Audit\\tWIN-D7NM05T4KNM\\tLogoff\\tAn account was logged off. Subject: Security ID: S-1-5-21-2218251928-2375033965-419438225-500 Account Name: Administrator Account Domain: WIN-D7NM05T4KNM Logon ID: 0xa2a99 Logon Type: 2 This event is generated when a logon session is destroyed. It may be positively correlated with a logon event using the Logon ID value. Logon IDs are only unique between reboots on the same computer.\\t260\"\"\",\n        {\n         'criticality': '1',\n         'eventlog_id': '4634',\n         'eventlog_source': 'Security',\n         'eventlog_name': 'Microsoft-Windows-Security-Auditing',\n         'source_host': 'WIN-D7NM05T4KNM',\n         'eventlog_type': 'Success Audit',\n         'program' : 'EventLog',\n         'md5_checksum' : '260',\n         'eventlog_description': \"\"\"An account was logged off. Subject: Security ID: S-1-5-21-2218251928-2375033965-419438225-500 Account Name: Administrator Account Domain: WIN-D7NM05T4KNM Logon ID: 0xa2a99 Logon Type: 2 This event is generated when a logon session is destroyed. It may be positively correlated with a logon event using the Logon ID value. Logon IDs are only unique between reboots on the same computer.\"\"\",\n         'security_id': 'S-1-5-21-2218251928-2375033965-419438225-500',\n         'user': 'Administrator',\n         'domain': 'WIN-D7NM05T4KNM',\n         'logon_id': '0xa2a99',\n         'logon_type' : '2',\n         'method': 'Interactive'                   \n         })\n\nself.aS(u\"\"\"<13>Nov 21 17:45:25 WIN-D7NM05T4KNM    MSWinEventLog    1\\tSecurity\\t378\\tThu Jan 31 13:18:18\\t2013\\t4647\\tMicrosoft-Windows-Security-Auditing\\tWIN-D7NM05T4KNM\\Administrator\\tN/A\\tSuccess Audit\\tWIN-D7NM05T4KNM\\tLogoff\\tUser initiated logoff: Subject: Security ID: S-1-5-21-2218251928-2375033965-419438225-500 Account Name: Administrator Account Domain: WIN-D7NM05T4KNM Logon ID: 0xa2a99 This event is generated when a logoff is initiated. No further user-initiated activity can occur. This event can be interpreted as a logoff event.\\t255\"\"\",\n        {'criticality': '1',\n         'eventlog_id': '4647',\n         'eventlog_source': 'Security',\n         'eventlog_name': 'Microsoft-Windows-Security-Auditing',\n         'source_host': 'WIN-D7NM05T4KNM',\n         'eventlog_type': 'Success Audit',\n         'program' : 'EventLog',\n         'md5_checksum' : '255',\n         'eventlog_description':\"\"\"User initiated logoff: Subject: Security ID: S-1-5-21-2218251928-2375033965-419438225-500 Account Name: Administrator Account Domain: WIN-D7NM05T4KNM Logon ID: 0xa2a99 This event is generated when a logoff is initiated. No further user-initiated activity can occur. This event can be interpreted as a logoff event.\"\"\",\n         'security_id': 'S-1-5-21-2218251928-2375033965-419438225-500',\n         'user' : 'Administrator',\n         'domain': 'WIN-D7NM05T4KNM',\n         'logon_id': '0xa2a99',\n         \n         })\n\nself.aS(u\"\"\"<13>Nov 21 17:45:25 WIN-D7NM05T4KNM    MSWinEventLog    1\\tSecurity\\t398\\tThu Jan 31 13:18:31\\t2013\\t4648\\tMicrosoft-Windows-Security-Auditing\\tWIN-D7NM05T4KNM\\Administrator\\tN/A\\tSuccess Audit\\tWIN-D7NM05T4KNM\\tLogon\\tA logon was attempted using explicit credentials. Subject: Security ID: S-1-5-18 Account Name: WIN-D7NM05T4KNM$ Account Domain: WORKGROUP Logon ID: 0x3e7 Logon GUID: {00000000-0000-0000-0000-000000000000} Account Whose Credentials Were Used: Account Name: Administrator Account Domain: WIN-D7NM05T4KNM Logon GUID: {00000000-0000-0000-0000-000000000000} Target Server: Target Server Name: localhost Additional Information: localhost Process Information: Process ID: 0x5e0 Process Name: C:\\Windows\\System32\\winlogon.exe Network Information: Network Address: 127.0.0.1 Port: 0 This event is generated when a process attempts to log on an account by explicitly specifying that account\u2019s credentials. This most commonly occurs in batch-type configurations such as scheduled tasks, or when using the RUNAS command.\\t270\"\"\",\n        {'criticality': '1',\n         'eventlog_id': '4648',\n         'eventlog_source': 'Security',\n         'eventlog_name': 'Microsoft-Windows-Security-Auditing',\n         'source_host': 'WIN-D7NM05T4KNM',\n         'eventlog_type': 'Success Audit',\n         'program' : 'EventLog',\n         'md5_checksum' : '270',\n         'eventlog_description': unicode(\"\"\"A logon was attempted using explicit credentials. Subject: Security ID: S-1-5-18 Account Name: WIN-D7NM05T4KNM$ Account Domain: WORKGROUP Logon ID: 0x3e7 Logon GUID: {00000000-0000-0000-0000-000000000000} Account Whose Credentials Were Used: Account Name: Administrator Account Domain: WIN-D7NM05T4KNM Logon GUID: {00000000-0000-0000-0000-000000000000} Target Server: Target Server Name: localhost Additional Information: localhost Process Information: Process ID: 0x5e0 Process Name: C:\\Windows\\System32\\winlogon.exe Network Information: Network Address: 127.0.0.1 Port: 0 This event is generated when a process attempts to log on an account by explicitly specifying that account\u2019s credentials. This most commonly occurs in batch-type configurations such as scheduled tasks, or when using the RUNAS command.\"\"\", \"utf_8\"),\n         'security_id': 'S-1-5-18',\n         'user': 'WIN-D7NM05T4KNM$',\n         'domain': 'WORKGROUP',\n         'logon_id': '0x3e7',\n         'logon_guid': '{00000000-0000-0000-0000-000000000000}',\n         'credentials_account_name': 'Administrator',                  \n         'credentials_account_domain': 'WIN-D7NM05T4KNM',\n         'credentials_logon_guid': '{00000000-0000-0000-0000-000000000000}',\n         'target_server_name': 'localhost',\n         'additional_information': 'localhost',\n         'pid': '0x5e0',\n         'process_name': 'C:\\Windows\\System32\\winlogon.exe',\n         'address': '127.0.0.1',\n         'port': '0',\n         'status': 'failure',\n         })", "path": "tests\\test_log_samples.py", "repo_name": "wallix/pylogsparser", "stars": 123, "license": "lgpl-2.1", "language": "python", "size": 978}
{"docstring": "\"\"\"Testing time formatting callbacks. This is boilerplate code.\"\"\"\n# so far only time related callbacks were written. If it changes, list\n# here non related functions to skip in this test.\n", "func_signal": "def generic_time_callback_test(instance, cb):\n", "code": "instance.assertTrue(cb in instance.cb.keys())\nDATES_TO_TEST = [ datetime.utcnow() + timedelta(-1),\n                  datetime.utcnow() + timedelta(-180),\n                  datetime.utcnow() + timedelta(1), # will always be considered as in the future unless you're testing on new year's eve...\n                ]\n# The pattern translation list. Order is important !\ntranslations = [ (\"YYYY\", \"%Y\"),\n                 (\"YY\"  , \"%y\"),\n                 (\"DDD\" , \"%a\"),        # localized day\n                 (\"DD\"  , \"%d\"),        # day with eventual leading 0\n                 (\"dd\"  , \"%d\"),        \n                 (\"MMM\" , \"%b\"),        # localized month\n                 (\"MM\"  , \"%m\"),        # month number with eventual leading 0\n                 (\"hh\"  , \"%H\"),\n                 (\"mm\"  , \"%M\"),\n                 (\"ss\"  , \"%S\") ]\npattern = cb\nfor old, new in translations:\n    pattern = pattern.replace(old, new)\n# special cases\nif pattern == \"ISO8601\":\n    pattern = \"%Y-%m-%dT%H:%M:%SZ\"\nfor d in DATES_TO_TEST:\n    if pattern == \"EPOCH\":\n        #value = d.strftime('%s') + \".%i\" % (d.microsecond/1000)\n        # Fix for windows strftime('%s'), and python timedelta total_seconds not exists in 2.6\n        td = d - datetime(1970, 1, 1)\n        total_seconds_since_epoch = (td.microseconds + (td.seconds + td.days * 24 * 3600) * 10**6) / 10**6\n        value = str(total_seconds_since_epoch) + \".%i\" % (d.microsecond/1000)\n        #\n        expected_result = datetime.utcfromtimestamp(float(value))\n    else:\n        value = d.strftime(pattern)\n        expected_result = datetime.strptime(value, pattern)\n        # Deal with time formats that don't define a year explicitly\n        if \"%y\" not in pattern.lower():\n            expected_year = get_sensible_year(*expected_result.timetuple()[1:-3])\n            expected_result = expected_result.replace(year = expected_year)\n    log = {}\n    instance.cb[cb](value, log)\n    instance.assertTrue(\"date\" in log.keys())\n    instance.assertEqual(log['date'], expected_result)", "path": "tests\\test_commonElements.py", "repo_name": "wallix/pylogsparser", "stars": 123, "license": "lgpl-2.1", "language": "python", "size": 978}
{"docstring": "\"\"\"Testing MSExchangeIS Mailbox Store logs for exchange 2003\"\"\"\n", "func_signal": "def test_MSExchangeISMailboxStore2003(self):\n", "code": "self.aS(u\"\"\"<13>Dec  3 18:14:15 exchange.q-ass.lan MSWinEventLog 1\\tApplication\\t127867\\tMon Dec 03 18:14:13\\t2012\\t1016\\tMSExchangeIS Mailbox Store\\tUnknown User\\tN/A\\tSuccess Audit\\tEXCHANGE\\tOuvertures de session\\tL'utilisateur Windows 2000 Q-ASS\\mhu s'est connect\u00e9 \u00e0 la bo\u00eete aux lettres scr@exchange.wallix.fr et n'est pas le compte principal Windows 2000 de cette bo\u00eete aux lettres.      Pour plus d'informations, visitez le site http://www.microsoft.com/contentredirect.asp.\\t711\"\"\",\n       {\n         'criticality': '1',\n         'eventlog_id': '1016',\n         'eventlog_source': 'Application',\n         'eventlog_name': 'MSExchangeIS Mailbox Store',\n         'source_host': 'EXCHANGE',\n         'eventlog_type': 'Success Audit',\n         'program' : 'EventLog',\n         'md5_checksum' : '711',\n         'eventlog_description': u\"\"\"L'utilisateur Windows 2000 Q-ASS\\mhu s'est connect\u00e9 \u00e0 la bo\u00eete aux lettres scr@exchange.wallix.fr et n'est pas le compte principal Windows 2000 de cette bo\u00eete aux lettres.      Pour plus d'informations, visitez le site http://www.microsoft.com/contentredirect.asp.\"\"\",\n         \"user\" : u\"Q-ASS\\\\mhu\",\n         \"mailbox_owner\" : \"scr@exchange.wallix.fr\",\n         })", "path": "tests\\test_log_samples.py", "repo_name": "wallix/pylogsparser", "stars": 123, "license": "lgpl-2.1", "language": "python", "size": 978}
{"docstring": "\"\"\"Converts a windows UTC timestamp (increments of 100 nanoseconds since Jan 1, 1601)\ninto a Unix EPOCH timestamp.\n\n@param winTimestamp : the windows timestamp\"\"\"\n\n", "func_signal": "def winUTC2UnixTimestamp(winTimestamp):\n", "code": "a = int(winTimestamp)\nunixts = (a / 10000000) - 11644473600\nreturn datetime.fromtimestamp(unixts).isoformat()", "path": "logsparser\\extras\\windows.py", "repo_name": "wallix/pylogsparser", "stars": 123, "license": "lgpl-2.1", "language": "python", "size": 978}
{"docstring": "\"\"\"Testing xferlog formatted logs\"\"\"\n", "func_signal": "def test_xferlog(self):\n", "code": "self.aS(\"Thu Sep 2 09:52:00 2004 50 192.168.20.10 896242 /home/test/file1.tgz b _ o r suporte ftp 0 * c \",\n        {'transfer_time' : '50',\n         'source_ip' : '192.168.20.10',\n         'len' : '896242',\n         'filename' : '/home/test/file1.tgz',\n         'transfer_type_code' : 'b',\n         'special_action_flag' : '_',\n         'direction_code' : 'o',\n         'access_mode_code' : 'r',\n         'completion_status_code' : 'c',\n         'authentication_method_code' : '0',\n         'transfer_type' : 'binary',\n         'special_action' : 'none',\n         'direction' : 'outgoing',\n         'access_mode' : 'real',\n         'completion_status' : 'complete',\n         'authentication_method' : 'none',\n         'user' : 'suporte',\n         'service_name' : 'ftp',\n         'authenticated_user_id' : '*',\n         'program' : 'ftpd',\n         'date' : datetime(2004,9,2,9,52),})\nself.aS(\"Tue Dec 27 11:24:23 2011 1 127.0.0.1 711074 /home/mhu/Documents/Brooks,_Max_-_World_War_Z.mobi b _ o r mhu ftp 0 * c\",\n        {'transfer_time' : '1',\n         'source_ip' : '127.0.0.1',\n         'len' : '711074',\n         'filename' : '/home/mhu/Documents/Brooks,_Max_-_World_War_Z.mobi',\n         'transfer_type_code' : 'b',\n         'special_action_flag' : '_',\n         'direction_code' : 'o',\n         'access_mode_code' : 'r',\n         'completion_status_code' : 'c',\n         'authentication_method_code' : '0',\n         'transfer_type' : 'binary',\n         'special_action' : 'none',\n         'direction' : 'outgoing',\n         'access_mode' : 'real',\n         'completion_status' : 'complete',\n         'authentication_method' : 'none',\n         'user' : 'mhu',\n         'service_name' : 'ftp',\n         'authenticated_user_id' : '*',\n         'program' : 'ftpd',\n         'date' : datetime(2011,12,27,11,24,23),})", "path": "tests\\test_log_samples.py", "repo_name": "wallix/pylogsparser", "stars": 123, "license": "lgpl-2.1", "language": "python", "size": 978}
{"docstring": "\"\"\"Test Exchange 2007 message tracking log normalization\"\"\"\n", "func_signal": "def test_MSExchange2007MTL(self):\n", "code": "self.aS(\"\"\"2010-04-19T12:29:07.390Z,10.10.14.73,WIN2K3DC,,WIN2K3DC,\"MDB:ada3d2c3-6f32-45db-b1ee-a68dbcc86664, Mailbox:68cf09c1-1344-4639-b013-3c6f8a588504, Event:1440, MessageClass:IPM.Note, CreationTime:2010-04-19T12:28:51.312Z, ClientType:User\",,STOREDRIVER,SUBMIT,,<C6539E897AEDFA469FE34D029FB708D43495@win2k3dc.qa.ifr.lan>,,,,,,,Coucou !,user7@qa.ifr.lan,,\"\"\",\n        {'mdb': 'ada3d2c3-6f32-45db-b1ee-a68dbcc86664',\n         'source_host': 'WIN2K3DC',\n         'source_ip': '10.10.14.73',\n         'client_type': 'User',\n         'creation_time': 'Mon Apr 19 12:28:51 2010',\n         'date': datetime(2010, 4, 19, 12, 29, 7, 390000),\n         'event': '1440',\n         'event_id': 'SUBMIT',\n         'exchange_source': 'STOREDRIVER',\n         'mailbox': '68cf09c1-1344-4639-b013-3c6f8a588504',\n         'message_class': 'IPM.Note',\n         'message_id': 'C6539E897AEDFA469FE34D029FB708D43495@win2k3dc.qa.ifr.lan',\n         'message_subject': 'Coucou !',\n         'program': 'MS Exchange 2007 Message Tracking',\n         'dest_host': 'WIN2K3DC'})", "path": "tests\\test_log_samples.py", "repo_name": "wallix/pylogsparser", "stars": 123, "license": "lgpl-2.1", "language": "python", "size": 978}
{"docstring": "\"\"\" Verify that we have all normalizer\nactivated when we instanciate LogNormalizer with\nan activate dict empty.\n\"\"\"\n", "func_signal": "def test_001_all_normalizers_activated(self):\n", "code": "ln = LogNormalizer(self.normalizer_path)\nself.assertTrue(len(ln))\nself.assertEqual(len([an[0] for an in ln.get_active_normalizers() if an[1]]), len(ln))\nself.assertEqual(len(ln._cache), len(ln))", "path": "tests\\test_lognormalizer.py", "repo_name": "wallix/pylogsparser", "stars": 123, "license": "lgpl-2.1", "language": "python", "size": 978}
{"docstring": "\"\"\"Testing tagTypes' accuracy\"\"\"\n", "func_signal": "def test_010_test_tagTypes(self):\n", "code": "self.assertTrue(self.tagTypes['EpochTime'].compiled_regexp.match('12934824.134'))\nself.assertTrue(self.tagTypes['EpochTime'].compiled_regexp.match('12934824'))\nself.assertTrue(self.tagTypes['syslogDate'].compiled_regexp.match('Jan 23 10:23:45'))\nself.assertTrue(self.tagTypes['syslogDate'].compiled_regexp.match('Oct  6 23:05:10'))\nself.assertTrue(self.tagTypes['URL'].compiled_regexp.match('http://www.wallix.org'))\nself.assertTrue(self.tagTypes['URL'].compiled_regexp.match('https://mysecuresite.com/?myparam=myvalue&myotherparam=myothervalue'))\nself.assertTrue(self.tagTypes['Email'].compiled_regexp.match('mhu@wallix.com'))\nself.assertTrue(self.tagTypes['Email'].compiled_regexp.match('matthieu.huin@wallix.com'))\nself.assertTrue(self.tagTypes['Email'].compiled_regexp.match('John-Fitzgerald.Willis@super-duper.institution.withlotsof.subdomains.org'))\nself.assertTrue(self.tagTypes['IP'].compiled_regexp.match('192.168.1.1'))\nself.assertTrue(self.tagTypes['IP'].compiled_regexp.match('255.255.255.0'))\n# shouldn't match ...\nself.assertTrue(self.tagTypes['IP'].compiled_regexp.match('999.888.777.666'))\nself.assertTrue(self.tagTypes['MACAddress'].compiled_regexp.match('0e:88:6a:4b:00:ff'))\nself.assertTrue(self.tagTypes['ZuluTime'].compiled_regexp.match('2012-12-21'))\nself.assertTrue(self.tagTypes['ZuluTime'].compiled_regexp.match('2012-12-21T12:34:56.99'))", "path": "tests\\test_commonElements.py", "repo_name": "wallix/pylogsparser", "stars": 123, "license": "lgpl-2.1", "language": "python", "size": 978}
{"docstring": "\"\"\"\n    Returns a list of frequencies with the magnitudes higher\n    than a given threshold.\n\"\"\"\n\n", "func_signal": "def getFilteredFFT(self, FFT, duration, threshold):\n", "code": "significantFreqs = []\nfor i in range(len(FFT)):\n    power_spectrum = scipy.absolute(FFT[i]) * scipy.absolute(FFT[i])\n    if power_spectrum > threshold:\n        significantFreqs.append(i / duration)\n\nreturn significantFreqs", "path": "first_peaks_method.py", "repo_name": "Agerrr/Automated_Music_Transcription", "stars": 106, "license": "mit", "language": "python", "size": 7839}
{"docstring": "\"\"\"\n    Given clustered frequencies finds a mean of each cluster.\n\"\"\"\n\n", "func_signal": "def getClustersMeans(self, clusters):\n", "code": "means = []\nfor bin, freqs in clusters.iteritems():\n    means.append(sum(freqs) / len(freqs))\nreturn means", "path": "first_peaks_method.py", "repo_name": "Agerrr/Automated_Music_Transcription", "stars": 106, "license": "mit", "language": "python", "size": 7839}
{"docstring": "\"\"\"\n    Returns the closest number that is smaller than n that is a power of 2.\n\"\"\"\n\n", "func_signal": "def get_next_power_2(n):\n", "code": "power = 1\nwhile (power < n):\n    power *= 2\nif power > 1:\n    return power / 2\nelse:\n    return 1", "path": "highest_peak_method.py", "repo_name": "Agerrr/Automated_Music_Transcription", "stars": 106, "license": "mit", "language": "python", "size": 7839}
{"docstring": "\"\"\"\n    Returns number of channels of a given sound file.\n\"\"\"\n\n", "func_signal": "def get_channels_no(sound_file):\n", "code": "wr = wave.open(sound_file, 'r')\nnchannels, sampwidth, framerate, nframes, comptype, compname = wr.getparams()\nreturn nchannels", "path": "first_peaks_method.py", "repo_name": "Agerrr/Automated_Music_Transcription", "stars": 106, "license": "mit", "language": "python", "size": 7839}
{"docstring": "\"\"\"\n    The algorithm for calculating midi notes from a given wav file.\n\"\"\"\n\n", "func_signal": "def detect_MIDI_notes(self):\n", "code": "(framerate, sample) = wav.read(self.wav_file)\n# We need to change the 2 channels into one because STFT works only\n# for 1 channel. We could also do STFT for each channel separately.\nmonoChannel = sample.mean(axis=1)\nduration = getDuration(self.wav_file)\nmidi_notes = []\n\n# Consider only files with a duration longer than 0.2 seconds.\nif duration > 0.18:\n    frequency_power = self.calculateFFT(duration, framerate, monoChannel)\n    filtered_frequencies = [f for (f, p) in frequency_power]\n    #self.plot_power_spectrum(frequency_power)\n    #self.plot_power_spectrum_dB(frequency_power)\n    f0_candidates = self.get_pitch_candidates_remove_highest_peak(frequency_power)\n    midi_notes = self.matchWithMIDINotes(f0_candidates)\nreturn midi_notes", "path": "highest_peak_method.py", "repo_name": "Agerrr/Automated_Music_Transcription", "stars": 106, "license": "mit", "language": "python", "size": 7839}
{"docstring": "\"\"\"\n    Calculates FFT for a given sound wave.\n    Considers only frequencies with the magnitudes higher than\n    a given threshold.\n\"\"\"\n\n", "func_signal": "def calculateFFT(self, duration, framerate, sample):\n", "code": "fft_length = int(duration * framerate)\n\nfft_length = get_next_power_2(fft_length)\nFFT = numpy.fft.fft(sample, n=fft_length)\n\n''' ADJUSTING THRESHOLD '''\nthreshold = 0\npower_spectra = []\nfor i in range(len(FFT) / 2):\n    power_spectrum = scipy.absolute(FFT[i]) * scipy.absolute(FFT[i])\n    if power_spectrum > threshold:\n        threshold = power_spectrum\n    power_spectra.append(power_spectrum)\nthreshold *= 0.1\n\nbinResolution = float(framerate) / float(fft_length)\nfrequency_power = []\n# For each bin calculate the corresponding frequency.\nfor k in range(len(FFT) / 2):\n    binFreq = k * binResolution\n\n    if binFreq > self.minFreqConsidered and binFreq < self.maxFreqConsidered:\n        power_spectrum = power_spectra[k]\n        #dB = 10*math.log10(power_spectrum)\n        if power_spectrum > threshold:\n            frequency_power.append((binFreq, power_spectrum))\n\nreturn frequency_power", "path": "highest_peak_method.py", "repo_name": "Agerrr/Automated_Music_Transcription", "stars": 106, "license": "mit", "language": "python", "size": 7839}
{"docstring": "\"\"\"\n    The algorithm for calculating midi notes from a given wav file.\n\"\"\"\n\n", "func_signal": "def detect_MIDI_notes(self):\n", "code": "(framerate, sample) = wav.read(self.wav_file)\nif get_channels_no(self.wav_file) > 1:\n    sample = sample.mean(axis=1)\nduration = getDuration(self.wav_file)\nmidi_notes = []\n\n# Consider only files with a duration longer than 0.18 seconds.\nif duration > 0.18:\n    FFT, filteredFreqs, maxFreq, magnitudes, significant_freq = self.calculateFFT(duration, framerate, sample)\n    #plotPowerSpectrum(FFT, filteredFreqs, 1000)\n    clusters = self.clusterFrequencies(filteredFreqs)\n    averagedClusters = self.getClustersMeans(clusters)\n    f0_candidates = self.getF0Candidates(averagedClusters)\n    midi_notes = self.matchWithMIDINotes(f0_candidates)\n\n    '''\n    OCTAVE CORRECTION METHOD\n    '''\n    '''\n\n    # Include a note with a significant magnitude:\n    # if its magnitude is higher than the sum of magnitudes\n    # of all other spectral peaks\n    # include it in the list of detected notes and\n    # remove the note that's octave lower than this one\n    # if it was also detected.\n    if significant_freq > 0:\n        significant_midi_notes = self.matchWithMIDINotes([\n            significant_freq])\n        significant_midi_note = significant_midi_notes[0]\n        if significant_midi_note not in midi_notes:\n            midi_notes.append(significant_midi_note)\n            midi_notes = self.remove_lower_octave(\n                significant_midi_note, midi_notes)\n    '''\n\nreturn midi_notes", "path": "first_peaks_method.py", "repo_name": "Agerrr/Automated_Music_Transcription", "stars": 106, "license": "mit", "language": "python", "size": 7839}
{"docstring": "\"\"\"\n    Returns the duration of a given sound file.\n\"\"\"\n\n", "func_signal": "def getDuration(sound_file):\n", "code": "wr = wave.open(sound_file, 'r')\nnchannels, sampwidth, framerate, nframes, comptype, compname =  wr.getparams()\nreturn nframes / float(framerate)", "path": "first_peaks_method.py", "repo_name": "Agerrr/Automated_Music_Transcription", "stars": 106, "license": "mit", "language": "python", "size": 7839}
{"docstring": "\"\"\"\n    Check if a number is prime.\n\"\"\"\n\n# make sure n is a positive integer\n", "func_signal": "def is_Prime(n):\n", "code": "n = abs(int(n))\n# 0 and 1 are not primes\nif n < 2:\n    return False\n# 2 is the only even prime number\nif n == 2:\n    return True\n# all other even numbers are not primes\nif not n & 1:\n    return False\n# range starts with 3 and only needs to go up the squareroot of n\n# for all odd numbers\nfor x in range(3, int(n ** 0.5) + 1, 2):\n    if n % x == 0:\n        return False\nreturn True", "path": "highest_peak_method.py", "repo_name": "Agerrr/Automated_Music_Transcription", "stars": 106, "license": "mit", "language": "python", "size": 7839}
{"docstring": "\"\"\"\n    Given frequency_power pairs and an f0 candidate remove\n    all possible harmonics of this f0 candidate.\n\"\"\"\n\n# If an integer frequency is a multiple of another frequency\n# then it is its harmonic. This constant was found empirically.\n# TODO: This constant may change for inharmonic frequencies!!!\n", "func_signal": "def filterOutHarmonics(self, frequency_power, f0_candidate):\n", "code": "REMAINDER_THRESHOLD = 0.2\n\ndef is_multiple(f, f0):\n    return abs(round(f / f0) - f / f0) < REMAINDER_THRESHOLD\n\nreturn [(f, p) for (f, p) in frequency_power if not is_multiple(f, f0_candidate)]", "path": "highest_peak_method.py", "repo_name": "Agerrr/Automated_Music_Transcription", "stars": 106, "license": "mit", "language": "python", "size": 7839}
{"docstring": "\"\"\"\n    Calculates distance between frequencies taking into account that\n    the frequencies of pitches increase logarithmically.\n\"\"\"\n\n", "func_signal": "def calcDistance(self, freq1, freq2):\n", "code": "difference = abs(freq1 - freq2)\nlog = math.log((freq1 + freq2) / 2)\nreturn difference / log", "path": "first_peaks_method.py", "repo_name": "Agerrr/Automated_Music_Transcription", "stars": 106, "license": "mit", "language": "python", "size": 7839}
{"docstring": "\"\"\"\n    Given frequencies and an f0 candidate remove\n    all possible harmonics of this f0 candidate.\n\"\"\"\n\n# If an integer frequency is a multiple of another frequency\n# then it is its harmonic. This constant was found empirically.\n", "func_signal": "def filterOutHarmonics(self, frequencies, f0_candidate):\n", "code": "REMAINDER_THRESHOLD = 0.2\n\ndef is_multiple(f, f0):\n    return abs(round(f / f0) - f / f0) < REMAINDER_THRESHOLD\n\nreturn [f for f in frequencies if not is_multiple(f, f0_candidate)]", "path": "first_peaks_method.py", "repo_name": "Agerrr/Automated_Music_Transcription", "stars": 106, "license": "mit", "language": "python", "size": 7839}
{"docstring": "\"\"\"\n    Returns the duration of a given sound file.\n\"\"\"\n\n", "func_signal": "def getDuration(sound_file):\n", "code": "wr = wave.open(sound_file, 'r')\nnchannels, sampwidth, framerate, nframes, comptype, compname =  wr.getparams()\nreturn nframes / float(framerate)", "path": "highest_peak_method.py", "repo_name": "Agerrr/Automated_Music_Transcription", "stars": 106, "license": "mit", "language": "python", "size": 7839}
{"docstring": "\"\"\"\n    Calculates and plots the power spectrum of a given sound wave.\n\"\"\"\n\n", "func_signal": "def plotPowerSpectrum(FFT, binFrequencies, maxFreq):\n", "code": "T = int(maxFreq)\npylab.figure('Power spectrum')\npylab.plot(binFrequencies[:T], scipy.absolute(FFT[:T]) * scipy.absolute(FFT[:T]),)\npylab.xlabel('Frequency (Hz)')\npylab.ylabel('Power spectrum (|X[k]|^2)')\npylab.show()", "path": "first_peaks_method.py", "repo_name": "Agerrr/Automated_Music_Transcription", "stars": 106, "license": "mit", "language": "python", "size": 7839}
{"docstring": "\"\"\"\n    Returns the frame rate of a given sound file.\n\"\"\"\n\n", "func_signal": "def getFrameRate(sound_file):\n", "code": "wr = wave.open(sound_file, 'r')\nnchannels, sampwidth, framerate, nframes, comptype, compname = wr.getparams()\nreturn framerate", "path": "first_peaks_method.py", "repo_name": "Agerrr/Automated_Music_Transcription", "stars": 106, "license": "mit", "language": "python", "size": 7839}
{"docstring": "\"\"\"\n    Calculates FFT for a given sound wave.\n    Considers only frequencies with the magnitudes higher than\n    a given threshold.\n\"\"\"\n\n", "func_signal": "def calculateFFT(self, duration, framerate, sample):\n", "code": "fft_length = int(duration * framerate)\n# For the FFT to work much faster take the length that is a power of 2.\nfft_length = get_next_power_2(fft_length)\nFFT = numpy.fft.fft(sample, n=fft_length)\n\n''' ADJUSTING THRESHOLD - HIGHEST SPECTRAL PEAK METHOD'''\nthreshold = 0\npower_spectra = []\nfrequency_bin_with_max_spectrum = 0\nfor i in range(len(FFT) / 2):\n    power_spectrum = scipy.absolute(FFT[i]) * scipy.absolute(FFT[i])\n    if power_spectrum > threshold:\n        threshold = power_spectrum\n        frequency_bin_with_max_spectrum = i\n    power_spectra.append(power_spectrum)\nmax_power_spectrum = threshold\nthreshold *= 0.1\n\nbinFrequencies = []\nmagnitudes = []\nbinResolution = float(framerate) / float(fft_length)\nsum_of_significant_spectra = 0\n# For each bin calculate the corresponding frequency.\nfor k in range(len(FFT)):\n    binFreq = k * binResolution\n\n    # Truncating the FFT so we consider only hearable frequencies.\n    if binFreq > self.maxFreqConsidered:\n        FFT = FFT[:k]\n        break\n    elif binFreq > self.minFreqConsidered:\n        # Consider only the frequencies\n        # with magnitudes higher than the threshold.\n        power_spectrum = power_spectra[k]\n        if power_spectrum > threshold:\n            magnitudes.append(power_spectrum)\n            binFrequencies.append(binFreq)\n\n            # Sum all significant power spectra\n            # except the max power spectrum.\n            if power_spectrum != max_power_spectrum:\n                sum_of_significant_spectra += power_spectrum\n\nsignificant_freq = 0.0\n\nif max_power_spectrum > sum_of_significant_spectra:\n    significant_freq = frequency_bin_with_max_spectrum * binResolution\n\n# Max. frequency considered after truncating.\n# maxFreq = rate without truncating.\nmaxFreq = len(FFT) / duration\n\nreturn (FFT, binFrequencies, maxFreq, magnitudes, significant_freq)", "path": "first_peaks_method.py", "repo_name": "Agerrr/Automated_Music_Transcription", "stars": 106, "license": "mit", "language": "python", "size": 7839}
{"docstring": "\"\"\"\n    Plots a given sound wave.\n\"\"\"\n\n", "func_signal": "def plotSoundWave(rate, sample):\n", "code": "t = scipy.linspace(0, 2, 2*rate, endpoint=False)\npylab.figure('Sound wave')\nT = int(0.0001*rate)\npylab.plot(t[:T], sample[:T],)\npylab.show()", "path": "first_peaks_method.py", "repo_name": "Agerrr/Automated_Music_Transcription", "stars": 106, "license": "mit", "language": "python", "size": 7839}
{"docstring": "\"\"\"\n    Calculates and plots the magnitude spectrum of a given sound wave.\n\"\"\"\n\n", "func_signal": "def plotMagnitudeSpectrogram(self, rate, sample, framesz, hop):\n", "code": "X = self.STFT(sample, rate, framesz, hop)\n\n# Plot the magnitude spectrogram.\npylab.figure('Magnitude spectrogram')\npylab.imshow(scipy.absolute(X.T), origin='lower', aspect='auto',\n             interpolation='nearest')\npylab.xlabel('Time')\npylab.ylabel('Frequency')\npylab.show()", "path": "first_peaks_method.py", "repo_name": "Agerrr/Automated_Music_Transcription", "stars": 106, "license": "mit", "language": "python", "size": 7839}
{"docstring": "\"\"\"\n    Clusters frequencies.\n\"\"\"\n\n", "func_signal": "def clusterFrequencies(self, freqs):\n", "code": "if len(freqs) == 0:\n    return {}\nclusteredFreqs = {}\nbin = 0\nclusteredFreqs[0] = [freqs[0]]\nfor i in range(len(freqs) - 1):\n    dist = self.calcDistance(freqs[i], freqs[i + 1])\n    if dist < 2.0:\n        clusteredFreqs[bin].append(freqs[i + 1])\n    else:\n        bin += 1\n        clusteredFreqs[bin] = [freqs[i + 1]]\n\nreturn clusteredFreqs", "path": "first_peaks_method.py", "repo_name": "Agerrr/Automated_Music_Transcription", "stars": 106, "license": "mit", "language": "python", "size": 7839}
{"docstring": "\"\"\"\n    Given frequencies, frequency magnitudes and an f0 candidate\n    return the partials and magnitudes of this f0 candidate.\n\"\"\"\n\n", "func_signal": "def find_partials(self, frequencies, f0_candidate, magnitudes):\n", "code": "REMAINDER_THRESHOLD = 0.05\n\ndef is_multiple(f, f0):\n    return abs(round(f / f0) - f / f0) < REMAINDER_THRESHOLD\n\npartials = []\npartial_magnitudes = []\nfor i in range(len(frequencies)):\n    if is_multiple(frequencies[i], f0_candidate):\n        partials.append(frequencies[i])\n        partial_magnitudes.append(magnitudes[i])\nreturn (partials, partial_magnitudes)", "path": "first_peaks_method.py", "repo_name": "Agerrr/Automated_Music_Transcription", "stars": 106, "license": "mit", "language": "python", "size": 7839}
{"docstring": "\"\"\"x\u3092\u5165\u529b\u3057\u305f\u3068\u304d\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u51fa\u529b\u3092\u8a08\u7b97\n\u96a0\u308c\u30e6\u30cb\u30c3\u30c8\u306e\u51fa\u529b\u3082\u4e00\u7dd2\u306b\u8fd4\u3059\"\"\"\n# \u914d\u5217\u306b\u5909\u63db\u3057\u3066\u5148\u982d\u306b\u30d0\u30a4\u30a2\u30b9\u306e1\u3092\u633f\u5165\n", "func_signal": "def output(x, w1, w2):\n", "code": "x = np.insert(x, 0, 1)\nz = np.zeros(NUM_HIDDEN)\ny = np.zeros(NUM_OUTPUT)\n# \u9806\u4f1d\u64ad\u3067\u51fa\u529b\u3092\u8a08\u7b97\nfor j in range(NUM_HIDDEN):\n    a = np.zeros(NUM_HIDDEN)\n    a[j] = np.dot(w1[j, :], x)\n    z[j] = np.tanh(a[j])\nfor k in range(NUM_OUTPUT):\n    y[k] = np.dot(w2[k, :], z)\nreturn y, z", "path": "ch5\\animation.py", "repo_name": "aidiary/PRML", "stars": 119, "license": "mit", "language": "python", "size": 13598}
{"docstring": "\"\"\"\u4e8c\u4e57\u8aa4\u5dee\u548c\u3092\u8a08\u7b97\u3059\u308b\"\"\"\n", "func_signal": "def sum_of_squares_error(xlist, tlist, w1, w2):\n", "code": "error = 0.0\nfor n in range(N):\n    z = np.zeros(NUM_HIDDEN)\n    y = np.zeros(NUM_OUTPUT)\n    # \u30d0\u30a4\u30a2\u30b9\u306e1\u3092\u5148\u982d\u306b\u633f\u5165\n    x = np.insert(xlist[n], 0, 1)\n    # \u9806\u4f1d\u64ad\u3067\u51fa\u529b\u3092\u8a08\u7b97\n    for j in range(NUM_HIDDEN):\n        a = np.zeros(NUM_HIDDEN)\n        a[j] = np.dot(w1[j, :], x)\n        z[j] = np.tanh(a[j])\n    for k in range(NUM_OUTPUT):\n        y[k] = np.dot(w2[k, :], z)\n    # \u4e8c\u4e57\u8aa4\u5dee\u3092\u8a08\u7b97\n    for k in range(NUM_OUTPUT):\n        error += 0.5 * (y[k] - tlist[n, k]) * (y[k] - tlist[n, k])\nreturn error", "path": "ch5\\animation.py", "repo_name": "aidiary/PRML", "stars": 119, "license": "mit", "language": "python", "size": 13598}
{"docstring": "# \u30af\u30e9\u30b91\u3068\u30af\u30e9\u30b92\u306e\u6c7a\u5b9a\u5883\u754c\u306e\u76f4\u7dda\u306e\u65b9\u7a0b\u5f0f\n", "func_signal": "def f1(x1, W_t):\n", "code": "a = - ((W_t[0,1] - W_t[1,1]) / (W_t[0,2] - W_t[1,2]))\nb = - ((W_t[0,0] - W_t[1,0]) / (W_t[0,2] - W_t[1,2]))\nreturn a * x1 + b", "path": "ch4\\least_squares_multiclass.py", "repo_name": "aidiary/PRML", "stars": 119, "license": "mit", "language": "python", "size": 13598}
{"docstring": "# \u6c7a\u5b9a\u5883\u754c\u306e\u76f4\u7dda\u306e\u65b9\u7a0b\u5f0f\n", "func_signal": "def f(x1, W_t):\n", "code": "a = - ((W_t[0,1] - W_t[1,1]) / (W_t[0,2] - W_t[1,2]))\nb = - (W_t[0,0] - W_t[1,0]) / (W_t[0,2] - W_t[1,2])\nreturn a * x1 + b", "path": "ch4\\least_squares.py", "repo_name": "aidiary/PRML", "stars": 119, "license": "mit", "language": "python", "size": 13598}
{"docstring": "\"\"\"\n\u7279\u5fb4X1\u3068\u7279\u5fb4X2\u3092\u7d44\u307f\u5408\u308f\u305b\u305fdegree\u6b21\u306e\u9805\u307e\u3067\u7279\u5fb4\u3092\u30c7\u30fc\u30bf\u306b\u8ffd\u52a0\n\u30d0\u30a4\u30a2\u30b9\u9805\u306b\u5bfe\u5fdc\u3059\u308b\u30c7\u30fc\u30bf1\u3082\u8ffd\u52a0\n\"\"\"\n# \u30c7\u30fc\u30bf\u884c\u5217\u306b1\u3092\u8ffd\u52a0\n", "func_signal": "def mapFeature(X1, X2, degree=6):\n", "code": "m = X1.shape[0]\nX = np.ones((m, 1))\nfor i in range(1, degree + 1):\n    for j in range(0, i + 1):\n        newX = (X1 ** (i - j) * X2 ** j).reshape((m, 1))\n        X = np.hstack((X, newX))\nreturn X", "path": "ch4\\logistic_regression_reg.py", "repo_name": "aidiary/PRML", "stars": 119, "license": "mit", "language": "python", "size": 13598}
{"docstring": "# \u8a13\u7df4\u30c7\u30fc\u30bf\n", "func_signal": "def main():\n", "code": "xlist = np.linspace(0, 1, 10)\ntlist = np.sin(2 * np.pi * xlist) + np.random.normal(0, 0.2, xlist.size)\n\n# \u30d9\u30a4\u30ba\u66f2\u7dda\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u3092\u7528\u3044\u3066\u4e88\u6e2c\u5206\u5e03\u3092\u6c42\u3081\u308b\n# \u884c\u5217S\u3092\u8a08\u7b97\nsums = matrix(zeros((M + 1, M + 1)))\nfor n in range(len(xlist)):\n    sums += phi(xlist[n]) * phi(xlist[n]).transpose()\nI = matrix(np.identity(M + 1))\nS_inv = ALPHA * I + BETA * sums\nS = S_inv.getI()\n\n# \u9023\u7d9a\u95a2\u6570\u306e\u30d7\u30ed\u30c3\u30c8\u7528X\u5024\nxs = np.linspace(0, 1, 500)\nideal = np.sin(2 * np.pi * xs)\nmeans = []\nuppers = []\nlowers = []\nfor x in xs:\n    m = mean(x, xlist, tlist, S)[0,0]       # \u4e88\u6e2c\u5206\u5e03\u306e\u5e73\u5747\n    s = np.sqrt(variance(x, xlist, S)[0,0]) # \u4e88\u6e2c\u5206\u5e03\u306e\u6a19\u6e96\u504f\u5dee\n    u = m + s                               # \u5e73\u5747 + \u6a19\u6e96\u504f\u5dee\n    l = m - s                               # \u5e73\u5747 - \u6a19\u6e96\u504f\u5dee\n    means.append(m)\n    uppers.append(u)\n    lowers.append(l)\n\nplot(xlist, tlist, 'bo')  # \u8a13\u7df4\u30c7\u30fc\u30bf\nplot(xs, ideal, 'g-')     # \u7406\u60f3\u66f2\u7dda\nplot(xs, means, 'r-')     # \u4e88\u6e2c\u30e2\u30c7\u30eb\u306e\u5e73\u5747\nplot(xs, uppers, 'r--')   # +sigma\nplot(xs, lowers, 'r--')   # -sigma\nxlim(0.0, 1.0)\nylim(-1.5, 1.5)\nshow()", "path": "ch1\\bayes_fitting.py", "repo_name": "aidiary/PRML", "stars": 119, "license": "mit", "language": "python", "size": 13598}
{"docstring": "# positive\u30af\u30e9\u30b9\u306e\u30c7\u30fc\u30bf\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n", "func_signal": "def plotData(X, y):\n", "code": "positive = [i for i in range(len(y)) if y[i] == 1]\n# negative\u30af\u30e9\u30b9\u306e\u30c7\u30fc\u30bf\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\nnegative = [i for i in range(len(y)) if y[i] == 0]\n\nplt.scatter(X[positive, 0], X[positive, 1], c='red', marker='o', label=\"positive\")\nplt.scatter(X[negative, 0], X[negative, 1], c='blue', marker='o', label=\"negative\")", "path": "ch4\\logistic_regression_reg.py", "repo_name": "aidiary/PRML", "stars": 119, "license": "mit", "language": "python", "size": 13598}
{"docstring": "\"\"\"\u591a\u5909\u91cf\u30ac\u30a6\u30b9\u95a2\u6570\"\"\"\n", "func_signal": "def gaussian(x, mean, cov):\n", "code": "temp1 = 1 / ((2 * np.pi) ** (x.size/2.0))\ntemp2 = 1 / (np.linalg.det(cov) ** 0.5)\ntemp3 = - 0.5 * np.dot(np.dot(x - mean, np.linalg.inv(cov)), x - mean)\nreturn temp1 * temp2 * np.exp(temp3)", "path": "ch9\\gmm_em.py", "repo_name": "aidiary/PRML", "stars": 119, "license": "mit", "language": "python", "size": 13598}
{"docstring": "\"\"\"\u8a13\u7df4\u30c7\u30fc\u30bf\u304b\u3089\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u63a8\u5b9a\uff08\u6b63\u5247\u5316\u3042\u308a\uff09\"\"\"\n# M\u6b21\u591a\u9805\u5f0f\u306e\u3068\u304d\u306fM+1\u500b\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u3042\u308b\n", "func_signal": "def estimate(xlist, tlist, lam):\n", "code": "A = []\nfor i in range(M + 1):\n    for j in range(M + 1):\n        temp = (xlist ** (i + j)).sum()\n        if i == j: temp += lam  # \u5bfe\u89d2\u6210\u5206\u306b\u03bb\u3092\u8db3\u3059\n        A.append(temp)\nA = array(A).reshape(M + 1, M + 1)\n\nT = []\nfor i in range(M + 1):\n    T.append(((xlist ** i) * tlist).sum())\nT = array(T)\n\n# \u30d1\u30e9\u30e1\u30fc\u30bfw\u306f\u7dda\u5f62\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\nwlist = np.linalg.solve(A, T)\n\nreturn wlist", "path": "ch1\\regularization.py", "repo_name": "aidiary/PRML", "stars": 119, "license": "mit", "language": "python", "size": 13598}
{"docstring": "# \u30af\u30e9\u30b92\u3068\u30af\u30e9\u30b93\u306e\u6c7a\u5b9a\u5883\u754c\u306e\u76f4\u7dda\u306e\u65b9\u7a0b\u5f0f\n", "func_signal": "def f2(x1, W_t):\n", "code": "a = - ((W_t[1,1] - W_t[2,1]) / (W_t[1,2] - W_t[2,2]))\nb = - ((W_t[1,0] - W_t[2,0]) / (W_t[1,2] - W_t[2,2]))\nreturn a * x1 + b", "path": "ch4\\least_squares_multiclass.py", "repo_name": "aidiary/PRML", "stars": 119, "license": "mit", "language": "python", "size": 13598}
{"docstring": "# positive\u30af\u30e9\u30b9\u306e\u30c7\u30fc\u30bf\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n", "func_signal": "def plotData(X, y):\n", "code": "positive = [i for i in range(len(y)) if y[i] == 1]\n# negative\u30af\u30e9\u30b9\u306e\u30c7\u30fc\u30bf\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\nnegative = [i for i in range(len(y)) if y[i] == 0]\n\nplt.scatter(X[positive, 0], X[positive, 1], c='red', marker='o', label=\"positive\")\nplt.scatter(X[negative, 0], X[negative, 1], c='blue', marker='o', label=\"negative\")", "path": "ch4\\logistic_regression.py", "repo_name": "aidiary/PRML", "stars": 119, "license": "mit", "language": "python", "size": 13598}
{"docstring": "\"\"\"\n\u30c7\u30fc\u30bf\u304b\u3089\u30e9\u30f3\u30c0\u30e0\u306b100\u30b5\u30f3\u30d7\u30eb\u9078\u3093\u3067\u53ef\u8996\u5316\n\u753b\u50cf\u30c7\u30fc\u30bf\u306f8x8\u30d4\u30af\u30bb\u30eb\u3092\u4eee\u5b9a\n\"\"\"\n# \u30e9\u30f3\u30c0\u30e0\u306b100\u30b5\u30f3\u30d7\u30eb\u9078\u3076\n", "func_signal": "def displayData(X):\n", "code": "sel = np.random.permutation(X.shape[0])\nsel = sel[:100]\nX = X[sel, :]\nfor index, data in enumerate(X):\n    pyplot.subplot(10, 10, index + 1)\n    pyplot.axis('off')\n    image = data.reshape((28, 28))\n    pyplot.imshow(image, cmap=pyplot.cm.gray_r,\n                 interpolation='nearest')\npyplot.show()", "path": "ch5\\mlp_cg.py", "repo_name": "aidiary/PRML", "stars": 119, "license": "mit", "language": "python", "size": 13598}
{"docstring": "# \u4e8c\u4e57\u8aa4\u5dee\u95a2\u6570\u3067\u306f\u306a\u304f\u3001\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u8aa4\u5dee\u95a2\u6570\u3092\u4f7f\u7528\n", "func_signal": "def computeCost(X, y, theta):\n", "code": "h = sigmoid(np.dot(X, theta))\nJ = (1.0 / m) * np.sum(-y * safe_log(h) - (1 - y) * safe_log(1 - h))\nreturn J", "path": "ch4\\logistic_regression.py", "repo_name": "aidiary/PRML", "stars": 119, "license": "mit", "language": "python", "size": 13598}
{"docstring": "\"\"\"\u8a13\u7df4\u30c7\u30fc\u30bf\u304b\u3089\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u63a8\u5b9a\"\"\"\n# M\u6b21\u591a\u9805\u5f0f\u306e\u3068\u304d\u306fM+1\u500b\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u3042\u308b\n", "func_signal": "def estimate(xlist, tlist):\n", "code": "A = []\nfor i in range(M + 1):\n    for j in range(M + 1):\n        temp = (xlist ** (i + j)).sum()\n        A.append(temp)\nA = array(A).reshape(M + 1, M + 1)\n\nT = []\nfor i in range(M + 1):\n    T.append(((xlist ** i) * tlist).sum())\nT = array(T)\n\n# \u30d1\u30e9\u30e1\u30fc\u30bfw\u306f\u7dda\u5f62\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\nwlist = np.linalg.solve(A, T)\n\nreturn wlist", "path": "ch1\\curve_fitting.py", "repo_name": "aidiary/PRML", "stars": 119, "license": "mit", "language": "python", "size": 13598}
{"docstring": "\"\"\"\u5bfe\u6570\u5c24\u5ea6\u95a2\u6570\"\"\"\n", "func_signal": "def likelihood(X, mean, cov, pi):\n", "code": "summation = 0.0\nfor n in range(len(X)):\n    temp = 0.0\n    for k in range(K):\n        temp += pi[k] * gaussian(X[n], mean[k], cov[k])\n    summation += np.log(temp)\nreturn summation", "path": "ch9\\gmm_em.py", "repo_name": "aidiary/PRML", "stars": 119, "license": "mit", "language": "python", "size": 13598}
{"docstring": "\"\"\"\n(-epsilon_init, +epsilon_init) \u306e\u7bc4\u56f2\u3067\n\u91cd\u307f\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u521d\u671f\u5316\u3057\u305f\u91cd\u307f\u884c\u5217\u3092\u8fd4\u3059\n\"\"\"\n# \u5165\u529b\u3068\u306a\u308b\u5c64\u306b\u306f\u30d0\u30a4\u30a2\u30b9\u9805\u304c\u5165\u308b\u306e\u3067+1\u304c\u5fc5\u8981\u306a\u306e\u3067\u6ce8\u610f\n", "func_signal": "def randInitializeWeights(L_in, L_out):\n", "code": "epsilon_init = 0.12\nW = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\nreturn W", "path": "ch5\\mlp_cg.py", "repo_name": "aidiary/PRML", "stars": 119, "license": "mit", "language": "python", "size": 13598}
{"docstring": "# \u6c7a\u5b9a\u5883\u754c\u306e\u76f4\u7dda\u306e\u65b9\u7a0b\u5f0f\n", "func_signal": "def f(x1, W_t):\n", "code": "a = - ((W_t[0,1] - W_t[1,1]) / (W_t[0,2] - W_t[1,2]))\nb = - (W_t[0,0] - W_t[1,0]) / (W_t[0,2] - W_t[1,2])\nreturn a * x1 + b", "path": "ch4\\least_squares_with_noise.py", "repo_name": "aidiary/PRML", "stars": 119, "license": "mit", "language": "python", "size": 13598}
{"docstring": "\"\"\"\u30b3\u30b9\u30c8\u95a2\u6570\u306e\u5404\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u306e\u504f\u5fae\u5206\u306e\u30ea\u30b9\u30c8\u3092\u8fd4\u3059\"\"\"\n", "func_signal": "def gradient(theta, *args):\n", "code": "X, y, lam = args\nh = sigmoid(np.dot(X, theta))\ngrad = np.zeros(theta.shape[0])\ngrad[0] = (1.0 / m) * np.sum(h - y)\ngrad[1:] = (1.0 / m) * np.dot(X[:,1:].T, h - y) + (lam / m) * theta[1:]\nreturn grad", "path": "ch4\\logistic_regression_reg.py", "repo_name": "aidiary/PRML", "stars": 119, "license": "mit", "language": "python", "size": 13598}
{"docstring": "\"\"\"\u6b63\u5247\u5316\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u30b3\u30b9\u30c8\u95a2\u6570\"\"\"\n", "func_signal": "def J(theta, *args):\n", "code": "def safe_log(x, minval=0.0000000001):\n    return np.log(x.clip(min=minval))\nX, y, lam = args\n# \u4e8c\u4e57\u8aa4\u5dee\u95a2\u6570\u3067\u306f\u306a\u304f\u3001\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u8aa4\u5dee\u95a2\u6570\u3092\u4f7f\u7528\nh = sigmoid(np.dot(X, theta))\nreturn (1.0 / m) * np.sum(-y * safe_log(h) - (1 - y) * safe_log(1 - h)) + lam / (2 * m) * np.sum(theta[1:] ** 2)", "path": "ch4\\logistic_regression_reg.py", "repo_name": "aidiary/PRML", "stars": 119, "license": "mit", "language": "python", "size": 13598}
{"docstring": "\"\"\"\u30c7\u30fc\u30bf\u884c\u5217X\u3092\u5c5e\u6027\u3054\u3068\u306b\u6a19\u6e96\u5316\u3057\u305f\u30c7\u30fc\u30bf\u3092\u8fd4\u3059\"\"\"\n# \u5c5e\u6027\u306e\u6570\uff08=\u5217\u306e\u6570\uff09\n", "func_signal": "def scale(X):\n", "code": "col = X.shape[1]\n\n# \u5c5e\u6027\u3054\u3068\u306b\u5e73\u5747\u5024\u3068\u6a19\u6e96\u504f\u5dee\u3092\u8a08\u7b97\nmu = np.mean(X, axis=0)\nsigma = np.std(X, axis=0)\n\n# \u5c5e\u6027\u3054\u3068\u30c7\u30fc\u30bf\u3092\u6a19\u6e96\u5316\nfor i in range(col):\n    X[:,i] = (X[:,i] - mu[i]) / sigma[i]\n\nreturn X", "path": "ch9\\gmm_em.py", "repo_name": "aidiary/PRML", "stars": 119, "license": "mit", "language": "python", "size": 13598}
{"docstring": "\"\"\"\nLookup for nearest existing file\n\n\"\"\"\n", "func_signal": "def any_filepath_field(field, **kwargs):\n", "code": "def get_some_file(path):\n    subdirs, files = [], []\n    for entry in os.listdir(path):\n        entry_path = os.path.join(path, entry)\n        if os.path.isdir(entry_path):\n            subdirs.append(entry_path)\n        else:\n            if not field.match or re.match(field.match,entry):\n                files.append(entry_path)\n\n    if files:\n        return random.choice(files)\n    \n    if field.recursive:\n        for subdir in subdirs:\n            result = get_some_file(subdir)\n            if result:\n                return result\n\nresult = get_some_file(field.path)\nif result is None and not field.null:\n    raise TypeError(\"Can't found file in %s for non nullable FilePathField\" % field.path)\nreturn result", "path": "django_any\\models.py", "repo_name": "kmmbvnr/django-any", "stars": 74, "license": "mit", "language": "python", "size": 314}
{"docstring": "\"\"\"\nShortcut for creating Users\n\nPermissions could be a list of permission names\n\nIf not specified, creates active, non superuser \nand non staff user\n\"\"\"\n\n", "func_signal": "def any_user(password=None, permissions=[], groups=[], **kwargs):\n", "code": "is_active = kwargs.pop('is_active', True)\nis_superuser = kwargs.pop('is_superuser', False)\nis_staff = kwargs.pop('is_staff', False)\n\nuser = any_model(User, is_active = is_active, is_superuser = is_superuser,\n                 is_staff = is_staff, **kwargs)\n\nfor group_name in groups :\n    group = Group.objects.get(name=group_name)\n    user.groups.add(group)\n\nfor permission_name in permissions:\n    app_label, codename = permission_name.split('.')\n    permission = Permission.objects.get(\n        content_type__app_label=app_label,\n        codename=codename)\n    user.user_permissions.add(permission)\n\nif password:\n    user.set_password(password)\n\nuser.save()\nreturn user", "path": "django_any\\contrib\\auth.py", "repo_name": "kmmbvnr/django-any", "stars": 74, "license": "mit", "language": "python", "size": 314}
{"docstring": "\"\"\"\nReturn random value for EmailField\n\n>>> result = any_field(models.EmailField())\n>>> type(result)\n<type 'str'>\n>>> re.match(r\"(?:^|\\s)[-a-z0-9_.]+@(?:[-a-z0-9]+\\.)+[a-z]{2,6}(?:\\s|$)\", result, re.IGNORECASE) is not None\nTrue\n\"\"\"\n", "func_signal": "def any_email_field(field, **kwargs):\n", "code": "return \"%s@%s.%s\" % (xunit.any_string(max_length=10),\n                     xunit.any_string(max_length=10),\n                     xunit.any_string(min_length=2, max_length=3))", "path": "django_any\\models.py", "repo_name": "kmmbvnr/django-any", "stars": 74, "license": "mit", "language": "python", "size": 314}
{"docstring": "\"\"\"\nReturn random value for TimeField\n>>> result = any_field(models.TimeField())\n>>> type(result)\n<type 'datetime.time'>\n\"\"\"\n", "func_signal": "def any_time_field(field, **kwargs):\n", "code": "return time(\n    xunit.any_int(min_value=0, max_value=23),\n    xunit.any_int(min_value=0, max_value=59),\n    xunit.any_int(min_value=0, max_value=59))", "path": "django_any\\models.py", "repo_name": "kmmbvnr/django-any", "stars": 74, "license": "mit", "language": "python", "size": 314}
{"docstring": "\"\"\"\nReturn random value for FloatField\n\n>>> result = any_field(models.FloatField())\n>>> type(result)\n<type 'float'>\n\"\"\"\n", "func_signal": "def any_float_field(field, **kwargs):\n", "code": "min_value = kwargs.get('min_value', 1)\nmax_value = kwargs.get('max_value', 100)\nprecision = kwargs.get('precision', 3)\nreturn xunit.any_float(min_value=min_value, max_value=max_value, precision=precision)", "path": "django_any\\models.py", "repo_name": "kmmbvnr/django-any", "stars": 74, "license": "mit", "language": "python", "size": 314}
{"docstring": "\"\"\"\nReturn random value for PositiveSmallIntegerField\n>>> result = any_field(models.PositiveSmallIntegerField())\n>>> type(result)\n<type 'int'>\n>>> result < 256, result > 0\n(True, True)\n\"\"\"\n", "func_signal": "def any_positivesmallinteger_field(field, **kwargs):\n", "code": "min_value = kwargs.get('min_value', 1)\nmax_value = kwargs.get('max_value', 255)\nreturn xunit.any_int(min_value=min_value, max_value=max_value)", "path": "django_any\\models.py", "repo_name": "kmmbvnr/django-any", "stars": 74, "license": "mit", "language": "python", "size": 314}
{"docstring": "\"\"\"\nReturn random value for SmallIntegerValue\n>>> result = any_field(models.SmallIntegerField())\n>>> type(result)\n<type 'int'>\n>>> result > -256, result < 256\n(True, True)\n\"\"\"\n", "func_signal": "def any_smallinteger_field(field, **kwargs):\n", "code": "min_value = kwargs.get('min_value', -255)\nmax_value = kwargs.get('max_value', 255)\nreturn xunit.any_int(min_value=min_value, max_value=max_value)", "path": "django_any\\models.py", "repo_name": "kmmbvnr/django-any", "stars": 74, "license": "mit", "language": "python", "size": 314}
{"docstring": "\"\"\"\nSelection from field.choices\n\n>>> CHOICES = [('YNG', 'Child'), ('OLD', 'Parent')]\n>>> result = any_field(models.CharField(max_length=3, choices=CHOICES))\n>>> result in ['YNG', 'OLD']\nTrue\n\"\"\"\n", "func_signal": "def any_field_choices(function):\n", "code": "def wrapper(field, **kwargs):\n    if field.choices:\n        return random.choice(list(valid_choices(field.choices)))\n    return function(field, **kwargs)\n\nreturn wrapper", "path": "django_any\\models.py", "repo_name": "kmmbvnr/django-any", "stars": 74, "license": "mit", "language": "python", "size": 314}
{"docstring": "\"\"\"\nLookup for nearest existing file\n\n\"\"\"\n", "func_signal": "def any_file_field(field, **kwargs):\n", "code": "def get_some_file(path):\n    subdirs, files = field.storage.listdir(path)\n\n    if files:\n        result_file = random.choice(files)\n        instance = field.storage.open(\"%s/%s\" % (path, result_file)).file\n        return FieldFile(instance, field, result_file)\n\n    for subdir in subdirs:\n        result = get_some_file(\"%s/%s\" % (path, subdir))\n        if result:\n            return result\n        \nresult = get_some_file(field.upload_to)\n\nif result is None and not field.null:\n    raise TypeError(\"Can't found file in %s for non nullable FileField\" % field.upload_to)\nreturn result", "path": "django_any\\models.py", "repo_name": "kmmbvnr/django-any", "stars": 74, "license": "mit", "language": "python", "size": 314}
{"docstring": "\"\"\"\nReturn random value for URLField\n>>> result = any_field(models.URLField())\n>>> from django.core.validators import URLValidator\n>>> re.match(URLValidator.regex, result) is not None\nTrue\n\"\"\"\n", "func_signal": "def any_url_field(field, **kwargs):\n", "code": "url = kwargs.get('url')\n\nif not url:\n    verified = [validator for validator in field.validators \\\n                if isinstance(validator, validators.URLValidator) and \\\n                validator.verify_exists == True]\n    if verified:\n        url = choice(['http://news.yandex.ru/society.html',\n                      'http://video.google.com/?hl=en&tab=wv',\n                      'http://www.microsoft.com/en/us/default.aspx',\n                      'http://habrahabr.ru/company/opera/',\n                      'http://www.apple.com/support/hardware/',\n                      'http://ya.ru',\n                      'http://google.com',\n                      'http://fr.wikipedia.org/wiki/France'])\n    else:\n        url = \"http://%s.%s/%s\" % (\n            xunit.any_string(max_length=10),\n            xunit.any_string(min_length=2, max_length=3),\n            xunit.any_string(max_length=20))\n\nreturn url", "path": "django_any\\models.py", "repo_name": "kmmbvnr/django-any", "stars": 74, "license": "mit", "language": "python", "size": 314}
{"docstring": "\"\"\"\nAn positive integer\n\n>>> result = any_field(models.PositiveIntegerField())\n>>> type(result)\n<type 'int'>\n>>> result > 0\nTrue\n\"\"\"\n", "func_signal": "def any_positiveinteger_field(field, **kwargs):\n", "code": "min_value = kwargs.get('min_value', 1)\nmax_value = kwargs.get('max_value', 9999)\nreturn xunit.any_int(min_value=min_value, max_value=max_value)", "path": "django_any\\models.py", "repo_name": "kmmbvnr/django-any", "stars": 74, "license": "mit", "language": "python", "size": 314}
{"docstring": "\"\"\"\nSometimes return None if field could be blank\n\"\"\"\n", "func_signal": "def any_field_blank(function):\n", "code": "def wrapper(field, **kwargs):\n    if kwargs.get('isnull', False):\n        return None\n\n    if field.blank and random.random < 0.1:\n        return None\n    return function(field, **kwargs)\nreturn wrapper", "path": "django_any\\models.py", "repo_name": "kmmbvnr/django-any", "stars": 74, "license": "mit", "language": "python", "size": 314}
{"docstring": "\"\"\"\nReturn random value for IPAddressField\n>>> result = any_field(models.IPAddressField())\n>>> type(result)\n<type 'str'>\n>>> from django.core.validators import ipv4_re\n>>> re.match(ipv4_re, result) is not None\nTrue\n\"\"\"\n", "func_signal": "def any_ipaddress_field(field, **kwargs):\n", "code": "nums = [str(xunit.any_int(min_value=0, max_value=255)) for _ in xrange(0, 4)]\nreturn \".\".join(nums)", "path": "django_any\\models.py", "repo_name": "kmmbvnr/django-any", "stars": 74, "license": "mit", "language": "python", "size": 314}
{"docstring": "\"\"\"\nReturn random value for IntegerField\n>>> result = any_field(models.IntegerField())\n>>> type(result)\n<type 'int'>\n\"\"\"\n", "func_signal": "def any_integer_field(field, **kwargs):\n", "code": "min_value = kwargs.get('min_value', -10000)\nmax_value = kwargs.get('max_value', 10000)\nreturn xunit.any_int(min_value=min_value, max_value=max_value)", "path": "django_any\\models.py", "repo_name": "kmmbvnr/django-any", "stars": 74, "license": "mit", "language": "python", "size": 314}
{"docstring": "\"\"\"\nReturn random value for DateField,\nskips auto_now and auto_now_add fields\n\n>>> result = any_field(models.DateField())\n>>> type(result)\n<type 'datetime.date'>\n\"\"\"\n", "func_signal": "def any_date_field(field, **kwargs):\n", "code": "if field.auto_now or field.auto_now_add:\n    return None\nfrom_date = kwargs.get('from_date', date(1990, 1, 1))\nto_date = kwargs.get('to_date', date.today())\nreturn xunit.any_date(from_date=from_date, to_date=to_date)", "path": "django_any\\models.py", "repo_name": "kmmbvnr/django-any", "stars": 74, "license": "mit", "language": "python", "size": 314}
{"docstring": "\"\"\"\nReturn random value for SlugField\n>>> result = any_field(models.SlugField())\n>>> type(result)\n<type 'str'>\n>>> from django.core.validators import slug_re\n>>> re.match(slug_re, result) is not None\nTrue\n\"\"\"\n", "func_signal": "def any_slug_field(field, **kwargs):\n", "code": "letters = ascii_letters + digits + '_-'\nreturn xunit.any_string(letters = letters, max_length = field.max_length)", "path": "django_any\\models.py", "repo_name": "kmmbvnr/django-any", "stars": 74, "license": "mit", "language": "python", "size": 314}
{"docstring": "\"\"\"\nReturn random value for CharField\n\n>>> result = any_field(models.CharField(max_length=10))\n>>> type(result)\n<type 'str'>\n\"\"\"\n", "func_signal": "def any_char_field(field, **kwargs):\n", "code": "min_length = kwargs.get('min_length', 1)\nmax_length = kwargs.get('max_length', field.max_length)\nreturn xunit.any_string(min_length=min_length, max_length=max_length)", "path": "django_any\\models.py", "repo_name": "kmmbvnr/django-any", "stars": 74, "license": "mit", "language": "python", "size": 314}
{"docstring": "\"\"\"\nReturn random value for CharField\n\n>>> result = any_field(models.CommaSeparatedIntegerField(max_length=10))\n>>> type(result)\n<type 'str'>\n>>> [int(num) for num in result.split(',')] and 'OK'\n'OK'\n\"\"\"\n", "func_signal": "def any_commaseparatedinteger_field(field, **kwargs):\n", "code": "nums_count = field.max_length/2\nnums = [str(xunit.any_int(min_value=0, max_value=9)) for _ in xrange(0, nums_count)]\nreturn \",\".join(nums)", "path": "django_any\\models.py", "repo_name": "kmmbvnr/django-any", "stars": 74, "license": "mit", "language": "python", "size": 314}
{"docstring": "\"\"\"\nReturn random value for BigIntegerField\n\n>>> result = any_field(models.BigIntegerField())\n>>> type(result)\n<type 'long'>\n\"\"\"\n", "func_signal": "def any_biginteger_field(field, **kwargs):\n", "code": "min_value = kwargs.get('min_value', 1)\nmax_value = kwargs.get('max_value', 10**10)\nreturn long(xunit.any_int(min_value=min_value, max_value=max_value))", "path": "django_any\\models.py", "repo_name": "kmmbvnr/django-any", "stars": 74, "license": "mit", "language": "python", "size": 314}
{"docstring": "\"\"\"\nReturn random value for DecimalField\n\n>>> result = any_field(models.DecimalField(max_digits=5, decimal_places=2))\n>>> type(result)\n<class 'decimal.Decimal'>\n\"\"\"\n", "func_signal": "def any_decimal_field(field, **kwargs):\n", "code": "min_value = kwargs.get('min_value', 0)\nmax_value = kwargs.get('max_value',\n                       Decimal('%s.%s' % ('9'*(field.max_digits-field.decimal_places),\n                                          '9'*field.decimal_places)))\ndecimal_places = kwargs.get('decimal_places', field.decimal_places)\nreturn xunit.any_decimal(min_value=min_value, max_value=max_value,\n                         decimal_places = decimal_places)", "path": "django_any\\models.py", "repo_name": "kmmbvnr/django-any", "stars": 74, "license": "mit", "language": "python", "size": 314}
{"docstring": "\"\"\" selects the appropriate player from type field of the medialist and computes\n      the parameters for that type\n      selected track is a dictionary for the track/show\n\"\"\"\n", "func_signal": "def _play_selected_track(self,selected_track):\n", "code": "self.canvas.delete(ALL)\nif self.show['progress']==\"manual\":\n    self._display_eggtimer(self.resource('mediashow','m04'))\n\n# is menu required\nif self.show['has-child']==\"yes\":\n    enable_child=True\nelse:\n    enable_child=False\n\n#dispatch track by type\nself.player=None\nself.shower=None\ntrack_type = selected_track['type']\nself.mon.log(self,\"Track type is: \"+ track_type)\n\nif track_type==\"video\":\n    # create a videoplayer\n    track_file=self.complete_path(selected_track)\n    self.player=VideoPlayer(self.canvas,self.show,selected_track)\n    self.player.play(track_file,\n                                self.end_player,\n                                self.ready_callback,\n                                enable_menu=enable_child)\n                                \nelif track_type==\"image\":\n    track_file=self.complete_path(selected_track)\n    # images played from menus don't have children\n    self.player=ImagePlayer(self.canvas,self.show,selected_track)\n    self.player.play(track_file,\n                            self.end_player,\n                            self.ready_callback,\n                            enable_menu=enable_child)\n                            \nelif track_type==\"message\":\n    # bit odd because MessagePlayer is used internally to display text. \n    text=selected_track['text']\n    self.player=MessagePlayer(self.canvas,self.show,selected_track)\n    self.player.play(text,\n                            self.end_player,\n                            self.ready_callback,\n                            enable_menu=enable_child\n                            )\n \n \nelif track_type==\"show\":\n    # get the show from the showlist\n    index = self.showlist.index_of_show(selected_track['sub-show'])\n    if index >=0:\n        self.showlist.select(index)\n        selected_show=self.showlist.selected_show()\n    else:\n        self.mon.err(self,\"Show not found in showlist: \"+ selected_track['sub-show'])\n        self._end('error',\"Unknown show\")\n        \n    if selected_show['type']==\"mediashow\":    \n        self.shower= MediaShow(selected_show,\n                                                        self.canvas,\n                                                        self.showlist,\n                                                        self.pp_home,\n                                                        self.pp_profile)\n        self.shower.play(self.end_shower,top=False,command=self._direction)\n\n    elif selected_show['type']==\"liveshow\":    \n        self.shower= LiveShow(selected_show,\n                                                        self.canvas,\n                                                        self.showlist,\n                                                        self.pp_home,\n                                                        self.pp_profile)\n        self.shower.play(self.end_shower,top=False,command='nil')\n    \n    elif selected_show['type']==\"menu\":\n        self.shower= MenuShow(selected_show,\n                                                self.canvas,\n                                                self.showlist,\n                                                self.pp_home,\n                                                self.pp_profile)\n        self.shower.play(self.end_shower,top=False,command='nil')\n        \n    else:\n        self.mon.err(self,\"Unknown Show Type: \"+ selected_show['type'])\n        self._end('error'\"Unknown show type\")  \n    \nelse:\n    self.mon.err(self,\"Unknown Track Type: \"+ track_type)\n    self._end('error',\"Unknown track type\")", "path": "pp_mediashow.py", "repo_name": "KenT2/pipresents", "stars": 65, "license": "other", "language": "python", "size": 12444}
{"docstring": "#initialise all the state machine variables\n#self.iteration = 0                             # for debugging\n", "func_signal": "def _start_play_state_machine(self,track):\n", "code": "self._stop_required_signal=False     # signal that user has pressed stop\nself.play_state=VideoPlayer._STARTING\n\n#play the selected track\noptions=self.omx_audio+\" \"+self.cd['omx-other-options']+\" \"\nself.omx.play(track,options)\n# and start polling for state changes\nself._tick_timer=self.canvas.after(50, self._play_state_machine)", "path": "pp_videoplayer.py", "repo_name": "KenT2/pipresents", "stars": 65, "license": "other", "language": "python", "size": 12444}
{"docstring": "\"\"\"reads options from options file to interface\"\"\"\n", "func_signal": "def read(self):\n", "code": "config=ConfigParser.ConfigParser()\nconfig.read(self.options_file)\n\nself.pp_home_dir =config.get('config','home',0)\nself.initial_media_dir =config.get('config','media',0)", "path": "pp_editor.py", "repo_name": "KenT2/pipresents", "stars": 65, "license": "other", "language": "python", "size": 12444}
{"docstring": "\"\"\" selects the appropriate player from type field of the medialist and computes\n      the parameters for that type\n      selected_track is a dictionary for the track/show\n\"\"\"\n# self.canvas.delete(ALL)\n\n# is menu required\n", "func_signal": "def _play_selected_track(self,selected_track):\n", "code": "if self.show['has-child']==\"yes\":\n    enable_child=True\nelse:\n    enable_child=False\n\n#dispatch track by type\nself.player=None\nself.shower=None\ntrack_type = selected_track['type']\nself.mon.log(self,\"Track type is: \"+ track_type)\n                              \nif track_type==\"image\":\n    track_file=self.complete_path(selected_track)\n    # images played from menus don't have children\n    self.player=ImagePlayer(self.canvas,self.show,selected_track)\n    self.player.play(track_file,\n                            self.end_player,\n                            self.ready_callback,\n                            enable_menu=enable_child)\nelif track_type==\"video\":\n    # create a videoplayer\n    track_file=self.complete_path(selected_track)\n    self.player=VideoPlayer(self.canvas,self.show,selected_track)\n    self.player.play(track_file,\n                                self.end_player,\n                                self.ready_callback,\n                                enable_menu=enable_child)\n    \nelif track_type==\"show\":\n    # get the show from the showlist\n    index = self.showlist.index_of_show(selected_track['sub-show'])\n    if index >=0:\n        self.showlist.select(index)\n        selected_show=self.showlist.selected_show()\n    else:\n        self.mon.err(self,\"Show not found in showlist: \"+ selected_track['sub-show'])\n        self._stop(\"Unknown show\")\n        \n    if selected_show['type']==\"mediashow\":    \n        self.shower= MediaShow(selected_show,\n                                                        self.canvas,\n                                                        self.showlist,\n                                                        self.pp_home,\n                                                        self.pp_profile)\n        self.shower.play(self.end_shower,top=False,command='nil')\n\n    \n    elif selected_show['type']==\"menu\":\n        self.shower= MenuShow(selected_show,\n                                                self.canvas,\n                                                self.showlist,\n                                                self.pp_home,\n                                                self.pp_profile)\n        self.shower.play(self.end_shower,top=False,command='nil')\n        \n    else:\n        self.mon.err(self,\"Unknown Show Type: \"+ selected_show['type'])\n        self._stop(\"Unknown show type\")  \n                                                                    \nelse:\n    self.mon.err(self,\"Unknown Track Type: \"+ track_type)\n    self._stop(\"Unknown track type\")", "path": "pp_liveshow.py", "repo_name": "KenT2/pipresents", "stars": 65, "license": "other", "language": "python", "size": 12444}
{"docstring": "\"\"\" reads the command line options and returns a dictionary of them\"\"\"\n", "func_signal": "def ed_options():\n", "code": "parser = argparse.ArgumentParser(description = 'Pi Presents Editor')\nparser.add_argument( '-d','--debug', \n                                      action='store_true',\n                                      help='Debug output to terminal window')\nargs=parser.parse_args()\nreturn  vars(args)", "path": "pp_options.py", "repo_name": "KenT2/pipresents", "stars": 65, "license": "other", "language": "python", "size": 12444}
{"docstring": "#turn screen blanking back on\n", "func_signal": "def tidy_up(self):\n", "code": "if self.options['noblank']==True:\n    call([\"xset\",\"s\", \"on\"])\n    call([\"xset\",\"s\", \"+dpms\"])\nif self.options['gpio']==True:\n    self.buttons.kill()\n#close logging files \nself.mon.finish()", "path": "pipresents.py", "repo_name": "KenT2/pipresents", "stars": 65, "license": "other", "language": "python", "size": 12444}
{"docstring": "\"\"\"\nuser clicks on a medialst in a profile so try and select it.\n\"\"\"\n# needs forgiving int for possible tkinter upgrade\n", "func_signal": "def select_medialist(self,event):\n", "code": "if len(self.medialists)>0:\n    self.current_medialists_index=int(event.widget.curselection()[0])\n    self.current_medialist=MediaList()\n    if not self.current_medialist.open_list(self.pp_profile_dir+ os.sep + self.medialists[self.current_medialists_index],self.current_showlist.sissue()):\n        self.mon.err(self,\"medialist is a different version to showlist: \"+ self.medialists[self.current_medialists_index])\n        self.app_exit()        \n    self.refresh_tracks_display()\n    self.refresh_medialists_display()", "path": "pp_editor.py", "repo_name": "KenT2/pipresents", "stars": 65, "license": "other", "language": "python", "size": 12444}
{"docstring": "\"\"\" save the output of the options edit dialog to file\"\"\"\n", "func_signal": "def save_options(self):\n", "code": "config=ConfigParser.ConfigParser()\nconfig.add_section('config')\nconfig.set('config','home',self.e_home.get())\nconfig.set('config','media',self.e_media.get())\nwith open(self.options_file, 'wb') as optionsfile:\n    config.write(optionsfile)", "path": "pp_editor.py", "repo_name": "KenT2/pipresents", "stars": 65, "license": "other", "language": "python", "size": 12444}
{"docstring": "\"\"\" reads the command line options and returns a dictionary of them\"\"\"\n", "func_signal": "def command_options():\n", "code": "parser = argparse.ArgumentParser(description = 'Pi presents presentation package')\nparser.add_argument( '-b','--noblank', \n                                      action='store_true',\n                                      help='Disable screen blanking.')\nparser.add_argument( '-f','--fullscreen', nargs='?', default='partial', const='',choices=['partial','left','right','top','bottom'],\n                                      help='Full screen, <top,bottom,left,right>')\nparser.add_argument( '-g','--gpio', \n                                      action='store_true',\n                                      help='Use GPIO')\nparser.add_argument( '-v','--verify', \n                                      action='store_true',\n                                      help='Verify Profile')\nparser.add_argument( '-d','--debug', \n                                      action='store_true',\n                                      help='Debug output to terminal window')\nparser.add_argument( '-o','--home', nargs='?', default='', const='',\n                                      help='Path to pp_home')\nparser.add_argument( '-l','--liveshow', nargs='?', default='', const='',\n                                      help='Directory for live tracks')\nparser.add_argument( '-p','--profile', nargs='?', default='', const='',\n                                      help='Profile')\nargs=parser.parse_args()\nreturn  vars(args)", "path": "pp_options.py", "repo_name": "KenT2/pipresents", "stars": 65, "license": "other", "language": "python", "size": 12444}
{"docstring": "\"\"\"edit the options then read them from file\"\"\"\n", "func_signal": "def edit_options(self):\n", "code": "eo = OptionsDialog(self.root, self.options.options_file,'Edit Options')\nif eo.result==True: self.init()", "path": "pp_editor.py", "repo_name": "KenT2/pipresents", "stars": 65, "license": "other", "language": "python", "size": 12444}
{"docstring": "#save the extra args to instance variables\n", "func_signal": "def __init__(self, parent, title, label, default):\n", "code": "self.label_1 = label\nself.default_1 = default     \n#and call the base class _init_which uses the args in body\ntkSimpleDialog.Dialog.__init__(self, parent, title)", "path": "pp_editor.py", "repo_name": "KenT2/pipresents", "stars": 65, "license": "other", "language": "python", "size": 12444}
{"docstring": "# unpause to start playing\n", "func_signal": "def show(self):\n", "code": "self._process.send('p')\nself.paused = False", "path": "pp_omxdriver.py", "repo_name": "KenT2/pipresents", "stars": 65, "license": "other", "language": "python", "size": 12444}
{"docstring": "\"\"\"\n        canvas - the canvas onto which the image is to be drawn\n        cd - configuration dictionary for the show from which player was called\n\"\"\"\n\n", "func_signal": "def __init__(self,canvas,cd,track_params):\n", "code": "self.mon=Monitor()\nself.mon.on()\n\nself.canvas=canvas\nself.cd=cd\nself.track_params=track_params", "path": "pp_messageplayer.py", "repo_name": "KenT2/pipresents", "stars": 65, "license": "other", "language": "python", "size": 12444}
{"docstring": "# define options for Editor\n", "func_signal": "def __init__(self,app_dir):\n", "code": "        self.pp_home_dir =\"\"   #home directory containing profile to be edited.\n        self.initial_media_dir =\"\"   #initial directory for open playlist      \n        self.debug = False  # print debug information to terminal", "path": "pp_editor.py", "repo_name": "KenT2/pipresents", "stars": 65, "license": "other", "language": "python", "size": 12444}
{"docstring": "# instantiate the subclass attributes\n", "func_signal": "def __init__(self, parent, options_file, title=None, ):\n", "code": "self.options_file=options_file\n\n# init the super class\ntkSimpleDialog.Dialog.__init__(self, parent, title)", "path": "pp_editor.py", "repo_name": "KenT2/pipresents", "stars": 65, "license": "other", "language": "python", "size": 12444}
{"docstring": "# send signal to stop the track to the state machine\n", "func_signal": "def _stop_omx(self):\n", "code": "self.mon.log(self,\"         >stop omx received from state machine\")\nif self.play_state==VideoPlayer._PLAYING:\n    self.omx.stop()\n    return True\nelse:\n    self.mon.log(self,\"!<stop rejected\")\n    return False", "path": "pp_videoplayer.py", "repo_name": "KenT2/pipresents", "stars": 65, "license": "other", "language": "python", "size": 12444}
{"docstring": "# send signal to stop the track to the state machine\n", "func_signal": "def _stop(self):\n", "code": "self.mon.log(self,\">stop received\")\nself._stop_required_signal=True", "path": "pp_videoplayer.py", "repo_name": "KenT2/pipresents", "stars": 65, "license": "other", "language": "python", "size": 12444}
{"docstring": "#  complete path of the filename of the selected entry\n", "func_signal": "def complete_path(self,selected_track):\n", "code": "track_file = selected_track['location']\nif track_file[0]==\"+\":\n        track_file=self.pp_home+track_file[1:]\nself.mon.log(self,\"Track to play is: \"+ track_file)\nreturn track_file", "path": "pp_liveshow.py", "repo_name": "KenT2/pipresents", "stars": 65, "license": "other", "language": "python", "size": 12444}
{"docstring": "'''add standard button box.\noverride to get rid of key bindings which cause trouble with text widget\n'''\n\n", "func_signal": "def buttonbox(self):\n", "code": "box = Frame(self)\n\nw = Button(box, text=\"OK\", width=10, command=self.ok, default=ACTIVE)\nw.pack(side=LEFT, padx=5, pady=5)\nw = Button(box, text=\"Cancel\", width=10, command=self.cancel)\nw.pack(side=LEFT, padx=5, pady=5)\n\n#self.bind(\"<Return>\", self.ok)\n#self.bind(\"<Escape>\", self.cancel)\n\nbox.pack()", "path": "pp_editor.py", "repo_name": "KenT2/pipresents", "stars": 65, "license": "other", "language": "python", "size": 12444}
{"docstring": "#  complete path of the filename of the selected entry\n", "func_signal": "def complete_path(self,selected_track):\n", "code": "track_file = selected_track['location']\nif track_file[0]==\"+\":\n        track_file=self.pp_home+track_file[1:]\nself.mon.log(self,\"Track to play is: \"+ track_file)\nreturn track_file", "path": "pp_mediashow.py", "repo_name": "KenT2/pipresents", "stars": 65, "license": "other", "language": "python", "size": 12444}
{"docstring": "# Strip away articles, unless there's only an article\n", "func_signal": "def test_normalize_list():\n", "code": "eq_(normalize_list('the dog'), ['dog'])\neq_(normalize_list('the'), ['the'])\n\n# strip out pluralization\neq_(normalize_list('big dogs'), ['big', 'dog'])", "path": "tests\\test_nltk_morphy.py", "repo_name": "commonsense/metanl", "stars": 84, "license": "mit", "language": "python", "size": 77197}
{"docstring": "\"\"\"\nGet the command for running the basic FreeLing pipeline in the\nspecified language.\n\nThe options we choose are:\n\n    -f data/freeling/<language>.cfg\n        load our custom configuration for the language\n    --fsplit data/freeling/generic_splitter.dat\n        don't do any special handling of ends of sentences\n\"\"\"\n", "func_signal": "def _get_command(self):\n", "code": "return ['analyze', '-f', self.configfile, '--fsplit',\n        self.splitterfile]", "path": "metanl\\freeling.py", "repo_name": "commonsense/metanl", "stars": 84, "license": "mit", "language": "python", "size": 77197}
{"docstring": "\"\"\"\nIn English, return the third segment of the record.\n\nIn other languages, this segment contains one letter for the part of\nspeech, plus densely-encoded features that we really have no way to\nuse. Return just the part-of-speech letter.\n\"\"\"\n", "func_signal": "def get_record_pos(self, record):\n", "code": "if self.lang == 'en':\n    return record[2]\nelse:\n    return record[2][0]", "path": "metanl\\freeling.py", "repo_name": "commonsense/metanl", "stars": 84, "license": "mit", "language": "python", "size": 77197}
{"docstring": "\"\"\"\nRun text through the external process, and get a list of lists\n(\"records\") that contain the analysis of each word.\n\"\"\"\n", "func_signal": "def analyze(self, text):\n", "code": "try:\n    text = render_safe(text).strip()\n    if not text:\n        return []\n    chunks = text.split('\\n')\n    results = []\n    for chunk_text in chunks:\n        if chunk_text.strip():\n            textbytes = (chunk_text + '\\n').encode('utf-8')\n            self.send_input(textbytes)\n            out_line = ''\n            while True:\n                out_line = self.receive_output_line()\n                out_line = out_line.decode('utf-8')\n\n                if out_line == '\\n':\n                    break\n\n                record = out_line.strip('\\n').split(' ')\n                results.append(record)\n    return results\nexcept ProcessError:\n    self.restart_process()\n    return self.analyze(text)", "path": "metanl\\freeling.py", "repo_name": "commonsense/metanl", "stars": 84, "license": "mit", "language": "python", "size": 77197}
{"docstring": "\"\"\"\nCreate the process by running the specified command.\n\"\"\"\n", "func_signal": "def _get_process(self):\n", "code": "command = self._get_command()\nreturn subprocess.Popen(command, bufsize=-1, close_fds=True,\n                        stdout=subprocess.PIPE,\n                        stdin=subprocess.PIPE)", "path": "metanl\\extprocess.py", "repo_name": "commonsense/metanl", "stars": 84, "license": "mit", "language": "python", "size": 77197}
{"docstring": "\"\"\"\nGiven a record, get the word's part of speech.\n\nHere we're going to return MeCab's part of speech (written in\nJapanese), though if it's a stopword we prefix the part of speech\nwith '~'.\n\"\"\"\n", "func_signal": "def get_record_pos(self, record):\n", "code": "if self.is_stopword_record(record):\n    return '~' + record.pos\nelse:\n    return record.pos", "path": "metanl\\mecab.py", "repo_name": "commonsense/metanl", "stars": 84, "license": "mit", "language": "python", "size": 77197}
{"docstring": "\"\"\"\nUntokenizing a text undoes the tokenizing operation, restoring\npunctuation and spaces to the places that people expect them to be.\n\nIdeally, `untokenize(tokenize(text))` should be identical to `text`,\nexcept for line breaks.\n\"\"\"\n", "func_signal": "def untokenize(words):\n", "code": "text = ' '.join(words)\nstep1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .', '...')\nstep2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\nstep3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\nstep4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\nstep5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n    \"can not\", \"cannot\")\nstep6 = step5.replace(\" ` \", \" '\")\nreturn step6.strip()", "path": "metanl\\token_utils.py", "repo_name": "commonsense/metanl", "stars": 84, "license": "mit", "language": "python", "size": 77197}
{"docstring": "\"\"\"\nGet a canonical list representation of text, with words\nseparated and reduced to their base forms.\n\nTODO: use the cache.\n\"\"\"\n", "func_signal": "def normalize_list(self, text, cache=None):\n", "code": "words = []\nanalysis = self.analyze(text)\nfor record in analysis:\n    if not self.is_stopword_record(record):\n        words.append(self.get_record_root(record))\nif not words:\n    # Don't discard stopwords if that's all you've got\n    words = [self.get_record_token(record) for record in analysis]\nreturn words", "path": "metanl\\extprocess.py", "repo_name": "commonsense/metanl", "stars": 84, "license": "mit", "language": "python", "size": 77197}
{"docstring": "\"\"\"\nGiven a MeCab record, return the root word.\n\"\"\"\n", "func_signal": "def get_record_root(self, record):\n", "code": "if record.root == '*':\n    return record.surface\nelse:\n    return record.root", "path": "metanl\\mecab.py", "repo_name": "commonsense/metanl", "stars": 84, "license": "mit", "language": "python", "size": 77197}
{"docstring": "\"\"\"\nGiven some text, return a sequence of (stem, pos, text) triples as\nappropriate for the reader. `pos` can be as general or specific as\nnecessary (for example, it might label all parts of speech, or it might\nonly distinguish function words from others).\n\nTwitter-style hashtags and at-mentions have the stem and pos they would\nhave without the leading # or @. For instance, if the reader's triple\nfor \"thing\" is ('thing', 'NN', 'things'), then \"#things\" would come out\nas ('thing', 'NN', '#things').\n\"\"\"\n", "func_signal": "def tag_and_stem(self, text, cache=None):\n", "code": "analysis = self.analyze(text)\ntriples = []\n\nfor record in analysis:\n    root = self.get_record_root(record)\n    token = self.get_record_token(record)\n\n    if token:\n        if unicode_is_punctuation(token):\n            triples.append((token, '.', token))\n        else:\n            pos = self.get_record_pos(record)\n            triples.append((root, pos, token))\nreturn triples", "path": "metanl\\extprocess.py", "repo_name": "commonsense/metanl", "stars": 84, "license": "mit", "language": "python", "size": 77197}
{"docstring": "\"\"\"\nClean up by closing the pipe.\n\"\"\"\n", "func_signal": "def __del__(self):\n", "code": "if hasattr(self, '_process'):\n    self._process.stdin.close()", "path": "metanl\\extprocess.py", "repo_name": "commonsense/metanl", "stars": 84, "license": "mit", "language": "python", "size": 77197}
{"docstring": "\"\"\"\nGiven a record, get the word's part of speech.\n\nThis default implementation simply distinguishes stopwords from\nnon-stopwords.\n\"\"\"\n", "func_signal": "def get_record_pos(self, record):\n", "code": "if self.is_stopword_record(record):\n    return 'STOP'\nelse:\n    return 'TERM'", "path": "metanl\\extprocess.py", "repo_name": "commonsense/metanl", "stars": 84, "license": "mit", "language": "python", "size": 77197}
{"docstring": "\"\"\"\nReturn two things about each character:\n\n- Its transliterated value (in Roman characters, if it's a kana)\n- A class of characters indicating how it affects the romanization\n\"\"\"\n", "func_signal": "def get_kana_info(char):\n", "code": "try:\n    name = unicodedata.name(char)\nexcept ValueError:\n    return char, NOT_KANA\n\n# The names we're dealing with will probably look like\n# \"KATAKANA CHARACTER ZI\".\nif (name.startswith('HIRAGANA LETTER') or\n    name.startswith('KATAKANA LETTER') or\n    name.startswith('KATAKANA-HIRAGANA')):\n    names = name.split()\n    syllable = str_func(names[-1].lower())\n\n    if name.endswith('SMALL TU'):\n        # The small tsu (\u3063) doubles the following consonant.\n        # It'll show up as 't' on its own.\n        return 't', SMALL_TSU\n    elif names[-1] == 'N':\n        return 'n', NN\n    elif names[1] == 'PROLONGED':\n        # The prolongation marker doubles the previous vowel.\n        # It'll show up as '_' on its own.\n        return '_', PROLONG\n    elif names[-2] == 'SMALL':\n        # Small characters tend to modify the sound of the previous\n        # kana. If they can't modify anything, they're appended to\n        # the letter 'x' instead.\n        if syllable.startswith('y'):\n            return 'x' + syllable, SMALL_Y\n        else:\n            return 'x' + syllable, SMALL\n\n    return syllable, KANA\nelse:\n    if char in ROMAN_PUNCTUATION_TABLE:\n        char = ROMAN_PUNCTUATION_TABLE[char]\n    return char, NOT_KANA", "path": "metanl\\mecab.py", "repo_name": "commonsense/metanl", "stars": 84, "license": "mit", "language": "python", "size": 77197}
{"docstring": "\"\"\"\nStore the actual process in _process. If it doesn't exist yet, create\nit.\n\"\"\"\n", "func_signal": "def process(self):\n", "code": "if hasattr(self, '_process'):\n    return self._process\nelse:\n    self._process = self._get_process()\n    return self._process", "path": "metanl\\extprocess.py", "repo_name": "commonsense/metanl", "stars": 84, "license": "mit", "language": "python", "size": 77197}
{"docstring": "\"\"\"\nRuns a line of text through MeCab, and returns the results as a\nlist of lists (\"records\") that contain the MeCab analysis of each\nword.\n\"\"\"\n", "func_signal": "def analyze(self, text):\n", "code": "try:\n    self.process  # make sure things are loaded\n    text = render_safe(text).replace('\\n', ' ').lower()\n    results = []\n    for chunk in string_pieces(text):\n        self.send_input((chunk + '\\n').encode('utf-8'))\n        while True:\n            out_line = self.receive_output_line().decode('utf-8')\n            if out_line == 'EOS\\n':\n                break\n\n            word, info = out_line.strip('\\n').split('\\t')\n            record_parts = [word] + info.split(',')\n\n            # Pad the record out to have 10 parts if it doesn't\n            record_parts += [None] * (10 - len(record_parts))\n            record = MeCabRecord(*record_parts)\n\n            # special case for detecting nai -> n\n            if (record.surface == '\u3093' and\n                record.conjugation == '\u4e0d\u5909\u5316\u578b'):\n                # rebuild the record so that record.root is 'nai'\n                record_parts[MeCabRecord._fields.index('root')] = '\u306a\u3044'\n                record = MeCabRecord(*record_parts)\n\n            results.append(record)\n    return results\nexcept ProcessError:\n    self.restart_process()\n    return self.analyze(text)", "path": "metanl\\mecab.py", "repo_name": "commonsense/metanl", "stars": 84, "license": "mit", "language": "python", "size": 77197}
{"docstring": "\"\"\"\nGiven some text, extract phrases of up to 2 content words,\nand map their normalized form to the complete phrase.\n\"\"\"\n", "func_signal": "def extract_phrases(self, text):\n", "code": "analysis = self.analyze(text)\nfor pos1 in range(len(analysis)):\n    rec1 = analysis[pos1]\n    if not self.is_stopword_record(rec1):\n        yield self.get_record_root(rec1), rec1[0]\n        for pos2 in range(pos1 + 1, len(analysis)):\n            rec2 = analysis[pos2]\n            if not self.is_stopword_record(rec2):\n                roots = [self.get_record_root(rec1),\n                         self.get_record_root(rec2)]\n                pieces = [analysis[i][0] for i in range(pos1, pos2+1)]\n                term = ' '.join(roots)\n                phrase = ''.join(pieces)\n                yield term, phrase\n                break", "path": "metanl\\extprocess.py", "repo_name": "commonsense/metanl", "stars": 84, "license": "mit", "language": "python", "size": 77197}
{"docstring": "\"\"\"\nUse MeCab to turn any text into its phonetic spelling, as katakana\nseparated by spaces.\n\"\"\"\n", "func_signal": "def to_kana(text):\n", "code": "records = MECAB.analyze(text)\nkana = []\nfor record in records:\n    if record.pronunciation:\n        kana.append(record.pronunciation)\n    elif record.reading:\n        kana.append(record.reading)\n    else:\n        kana.append(record.surface)\nreturn ' '.join(k for k in kana if k)", "path": "metanl\\mecab.py", "repo_name": "commonsense/metanl", "stars": 84, "license": "mit", "language": "python", "size": 77197}
{"docstring": "\"\"\"\nTakes a (unicode) string and yields pieces of it that are at most `maxlen`\ncharacters, trying to break it at punctuation/whitespace. This is an\nimportant step before using a tokenizer with a maximum buffer size.\n\"\"\"\n", "func_signal": "def string_pieces(s, maxlen=1024):\n", "code": "if not s:\n    return\ni = 0\nwhile True:\n    j = i + maxlen\n    if j >= len(s):\n        yield s[i:]\n        return\n    # Using \"j - 1\" keeps boundary characters with the left chunk\n    while unicodedata.category(s[j - 1]) not in BOUNDARY_CATEGORIES:\n        j -= 1\n        if j == i:\n            # No boundary available; oh well.\n            j = i + maxlen\n            break\n    yield s[i:j]\n    i = j", "path": "metanl\\token_utils.py", "repo_name": "commonsense/metanl", "stars": 84, "license": "mit", "language": "python", "size": 77197}
{"docstring": "\"\"\"\nTest if a token is made entirely of Unicode characters of the following\nclasses:\n\n- P: punctuation\n- S: symbols\n- Z: separators\n- M: combining marks\n- C: control characters\n\n>>> unicode_is_punctuation('word')\nFalse\n>>> unicode_is_punctuation('\u3002')\nTrue\n>>> unicode_is_punctuation('-')\nTrue\n>>> unicode_is_punctuation('-3')\nFalse\n>>> unicode_is_punctuation('\u3042')\nFalse\n\"\"\"\n", "func_signal": "def unicode_is_punctuation(text):\n", "code": "for char in str_func(text):\n    category = unicodedata.category(char)[0]\n    if category not in 'PSZMC':\n        return False\nreturn True", "path": "metanl\\extprocess.py", "repo_name": "commonsense/metanl", "stars": 84, "license": "mit", "language": "python", "size": 77197}
{"docstring": "\"\"\"\nDetermine whether a single MeCab record represents a stopword.\n\nThis mostly determines words to strip based on their parts of speech.\nIf common_words is set to True (default), it will also strip common\nverbs and nouns such as \u304f\u308b and \u3088\u3046. If more_stopwords is True, it\nwill look at the sub-part of speech to remove more categories.\n\"\"\"\n# preserve negations\n", "func_signal": "def is_stopword_record(self, record):\n", "code": "if record.root == '\u306a\u3044':\n    return False\nreturn (\n    record.pos in STOPWORD_CATEGORIES or\n    record.subclass1 in STOPWORD_CATEGORIES or\n    record.root in STOPWORD_ROOTS\n)", "path": "metanl\\mecab.py", "repo_name": "commonsense/metanl", "stars": 84, "license": "mit", "language": "python", "size": 77197}
{"docstring": "\"\"\"\nDecorator that will cause the function to be executed on the proper\nredis node. In the case of a Connection failure, it will attempt to find\na new master node and perform the action there.\n\"\"\"\n", "func_signal": "def executeOnNode(func):\n", "code": "@wraps(func)\ndef wrapper(self, key, *args, **kwargs):\n    node = self.get_node_for_key(key)\n    nodeFunc = getattr(node.connection, func.__name__)\n    try:\n        return nodeFunc(key, *args, **kwargs)\n    except ConnectionError:\n        # if it fails a second time, then sentinel hasn't caught up, so\n        # we have no choice but to fail for real.\n        node = self.get_master(node)\n        nodeFunc = getattr(node.connection, func.__name__)\n        return nodeFunc(key, *args, **kwargs)\nreturn wrapper", "path": "disredis\\disredis_client\\client.py", "repo_name": "SawdustSoftware/disredis", "stars": 80, "license": "other", "language": "python", "size": 185}
{"docstring": "\"\"\"\nReturns the current master for a node. If it's different from the\npassed in node, update our node list accordingly.\n\"\"\"\n", "func_signal": "def get_master(self, node):\n", "code": "host, port = self._execute_sentinel_command(\"get-master-addr-by-name\",\n    node.name)\nif host == node.host and port == node.port:\n    return node\nnewNode = Node(node.name, host, port)\nself.nodes[self.nodes.index(node)] = newNode\nreturn newNode", "path": "disredis\\disredis_client\\client.py", "repo_name": "SawdustSoftware/disredis", "stars": 80, "license": "other", "language": "python", "size": 185}
{"docstring": "\"\"\"\nReturning the master of a node that is failed should return a new node\nand reset the one in the nodes list.\n\"\"\"\n", "func_signal": "def test_get_master_different_host(self):\n", "code": "self.client.sentinel.masters[0] = [\"name\", \"node1\", \"ip\", \"2.2.3.4\",\n    \"port\", \"1\"]\nnode = self.client.nodes[0]\nmaster = self.client.get_master(node)\nself.assertNotEqual(node, master)\nself.assertEqual(master, self.client.nodes[0])\nself.assertEqual(master.host, \"2.2.3.4\")", "path": "disredis\\disredis_client\\test_client.py", "repo_name": "SawdustSoftware/disredis", "stars": 80, "license": "other", "language": "python", "size": 185}
{"docstring": "\"\"\"\nConnect to a sentinel, accounting for sentinels that fail.\n\"\"\"\n", "func_signal": "def _connect(self):\n", "code": "while True:\n    try:\n        address = self.sentinel_addresses.pop(0)\n        logger = logging.getLogger('custommade_logging')\n        logger.info(\"Connecting to Sentinel %s\" % address)\n        host, port = address.split(\":\")\n        self.sentinel = self.redis_client_class(host, int(port))\n        self.sentinel_addresses.append(address)\n        break\n    except ConnectionError:\n        if not self.sentinel_addresses:\n            raise\n    except IndexError:\n        raise ConnectionError(\"Out of available Sentinel addresses!\")", "path": "disredis\\disredis_client\\client.py", "repo_name": "SawdustSoftware/disredis", "stars": 80, "license": "other", "language": "python", "size": 185}
{"docstring": "\"\"\"\nRun a command on a sentinel, but fail over to the next sentinel in the\nlist if there's a connection problem.\n\"\"\"\n", "func_signal": "def _execute_sentinel_command(self, *args, **kwargs):\n", "code": "while True:\n    try:\n        if self.sentinel is None:\n            self._connect()\n        return self.sentinel.execute_command(\"SENTINEL\", *args,\n            **kwargs)\n    except ConnectionError:\n        self.sentinel = None\n        if self.sentinel_addresses:\n            self.sentinel_addresses.pop()  # pull the current connection off\n        else:\n            raise", "path": "disredis\\disredis_client\\client.py", "repo_name": "SawdustSoftware/disredis", "stars": 80, "license": "other", "language": "python", "size": 185}
{"docstring": "\"\"\"\nThis function uses an sha1 hash to distribute keys across available\nnodes.\n\"\"\"\n", "func_signal": "def test_get_node_for_key(self):\n", "code": "self.assertEqual(self.client.get_node_for_key(\"1\"),\n    self.client.nodes[1])\nself.assertEqual(self.client.get_node_for_key(\"2\"),\n    self.client.nodes[0])\nself.assertEqual(self.client.get_node_for_key(\"3\"),\n    self.client.nodes[1])", "path": "disredis\\disredis_client\\test_client.py", "repo_name": "SawdustSoftware/disredis", "stars": 80, "license": "other", "language": "python", "size": 185}
{"docstring": "\"\"\"Return the real key name in redis storage\n@return string\n\"\"\"\n", "func_signal": "def get_real_stored_key(self, session_key):\n", "code": "prefix = settings.SESSION_REDIS_PREFIX\nif not prefix:\n    return session_key\nreturn ':'.join([prefix, session_key])", "path": "disredis\\disredis_sessions\\session.py", "repo_name": "SawdustSoftware/disredis", "stars": 80, "license": "other", "language": "python", "size": 185}
{"docstring": "\"\"\"\nReturning the master of a node that is failed should return a new node\nand reset the one in the nodes list.\n\"\"\"\n", "func_signal": "def test_get_master_different_port(self):\n", "code": "self.client.sentinel.masters[0] = [\"name\", \"node1\", \"ip\", \"1.2.3.4\",\n    \"port\", \"11\"]\nnode = self.client.nodes[0]\nmaster = self.client.get_master(node)\nself.assertNotEqual(node, master)\nself.assertEqual(master, self.client.nodes[0])\nself.assertEqual(master.port, \"11\")", "path": "disredis\\disredis_client\\test_client.py", "repo_name": "SawdustSoftware/disredis", "stars": 80, "license": "other", "language": "python", "size": 185}
{"docstring": "\"\"\"\nIncrements the value at key ``name`` by floating ``amount``.\nIf no key exists, the value will be initialized as ``amount``\n\"\"\"\n\n", "func_signal": "def incrbyfloat(self, name, amount=1.0):\n", "code": "\n\"Returns a list of keys matching ``pattern``\"\nraise NotImplementedError(\"Not supported for disredis.\")", "path": "disredis\\disredis_client\\client.py", "repo_name": "SawdustSoftware/disredis", "stars": 80, "license": "other", "language": "python", "size": 185}
{"docstring": "\"\"\"\nEnclosing a part of the key in {} will cause it to be used as the\nsharding key. This is future-compatible with Redis Sentinel.\n\"\"\"\n", "func_signal": "def test_set_with_specified_key(self):\n", "code": "self.client.set(\"test{1}\", \"foo\")\nself.assertEqual(self.client.get(\"test{1}\"), \"foo\")\nself.assertEqual(self.client.nodes[0].connection.data, {})\nself.assertEqual(self.client.nodes[1].connection.data, {\"test{1}\":\"foo\"})\nself.client.set(\"other{1}\", \"bar\")\nself.assertEqual(self.client.get(\"other{1}\"), \"bar\")\nself.assertEqual(self.client.nodes[0].connection.data, {})\nself.assertEqual(self.client.nodes[1].connection.data, {\"test{1}\":\"foo\",\n    \"other{1}\":\"bar\"})\nself.client.set(\"test{2}\", \"baz\")\nself.assertEqual(self.client.get(\"test{2}\"), \"baz\")\nself.assertEqual(self.client.nodes[0].connection.data, {\"test{2}\":\"baz\"})\nself.assertEqual(self.client.nodes[1].connection.data, {\"test{1}\":\"foo\",\n    \"other{1}\":\"bar\"})", "path": "disredis\\disredis_client\\test_client.py", "repo_name": "SawdustSoftware/disredis", "stars": 80, "license": "other", "language": "python", "size": 185}
{"docstring": "\"\"\"\nIf a connection gives a ConnectionError, switch to the backup.\n\"\"\"\n", "func_signal": "def test_set_get_failover(self):\n", "code": "failed = self.client.nodes[1] \nfailed.connection.fail = True\nself.client.sentinel.masters[1] = [\"name\", \"node2\", \"ip\", \"1.2.3.4\",\n    \"port\", \"11\"]        \nself.client.set(\"test\", \"foo\")\nself.assertNotEqual(failed, self.client.nodes[1])\nself.assertEqual(self.client.get(\"test\"), \"foo\")\nself.assertEqual(self.client.nodes[0].connection.data, {})\nself.assertEqual(self.client.nodes[1].connection.data, {\"test\":\"foo\"})", "path": "disredis\\disredis_client\\test_client.py", "repo_name": "SawdustSoftware/disredis", "stars": 80, "license": "other", "language": "python", "size": 185}
{"docstring": "\"\"\"\nIf all sentinels fail, a ConnectionError is raised.\n\"\"\"\n", "func_signal": "def test_get_master_failed_sentinels(self):\n", "code": "MockStrictRedis.fail = True\ntry:\n    self.assertRaises(ConnectionError, self.client.get_master,\n        self.client.nodes[0])\nfinally:\n    MockStrictRedis.fail = False", "path": "disredis\\disredis_client\\test_client.py", "repo_name": "SawdustSoftware/disredis", "stars": 80, "license": "other", "language": "python", "size": 185}
{"docstring": "\"\"\"\nReturns a node for the given key. Keys with {} in them will be sharded\nbased on only the string between the brackets. This is for future\ncompatibility with Redis Cluster (and is also a nice feature to have).\n\"\"\"\n", "func_signal": "def get_node_for_key(self, key):\n", "code": "if \"{\" in key and \"}\" in key:\n    key = key[key.index(\"{\") + 1: key.index(\"}\")]\nreturn self.nodes[int(sha1(key).hexdigest(), 16) % len(self.nodes)]", "path": "disredis\\disredis_client\\client.py", "repo_name": "SawdustSoftware/disredis", "stars": 80, "license": "other", "language": "python", "size": 185}
{"docstring": "\"\"\"\nget_nodes is called from __init__, so check that the nodes were added.\n\"\"\"\n", "func_signal": "def test_get_nodes(self):\n", "code": "self.assertEqual(len(self.client.nodes), 2)\nself.assertEqual(self.client.nodes[0].name, \"node1\")\nself.assertEqual(self.client.nodes[1].name, \"node2\")\nself.assertEqual(self.client.nodes[0].host, \"1.2.3.4\")\nself.assertEqual(self.client.nodes[1].host, \"1.2.3.4\")\nself.assertEqual(self.client.nodes[0].port, \"1\")\nself.assertEqual(self.client.nodes[1].port, \"2\")", "path": "disredis\\disredis_client\\test_client.py", "repo_name": "SawdustSoftware/disredis", "stars": 80, "license": "other", "language": "python", "size": 185}
{"docstring": "\"\"\"\nReturn the value at key ``name``, or None if the key doesn't exist\n\"\"\"\n\n", "func_signal": "def get(self, name):\n", "code": "\n\"\"\"\nReturn the value at key ``name``, raises a KeyError if the key\ndoesn't exist.\n\"\"\"\nvalue = self.get(name)\nif value:\n    return value\nraise KeyError(name)", "path": "disredis\\disredis_client\\client.py", "repo_name": "SawdustSoftware/disredis", "stars": 80, "license": "other", "language": "python", "size": 185}
{"docstring": "\"\"\"\nAny time a sentinel fails, the client should use the next one in line.\n\"\"\"\n", "func_signal": "def test_get_master_failed_sentinel(self):\n", "code": "self.client.sentinel.fail = True\nself.assertEqual(self.client.get_master(self.client.nodes[0]),\n    self.client.nodes[0])", "path": "disredis\\disredis_client\\test_client.py", "repo_name": "SawdustSoftware/disredis", "stars": 80, "license": "other", "language": "python", "size": 185}
{"docstring": "\"\"\"\nReturning the master of a node that isn't failed should return the\nsame object.\n\"\"\"\n", "func_signal": "def test_get_master(self):\n", "code": "self.assertEqual(self.client.get_master(self.client.nodes[0]),\n    self.client.nodes[0])", "path": "disredis\\disredis_client\\test_client.py", "repo_name": "SawdustSoftware/disredis", "stars": 80, "license": "other", "language": "python", "size": 185}
{"docstring": "\"\"\"\nRetrieve the list of nodes and their masters from the Sentinel server.\n\"\"\"\n", "func_signal": "def _get_nodes(self):\n", "code": "masterList = self._execute_sentinel_command(\"MASTERS\")\nself.nodes = []\nfor master in masterList:\n    info = dict(zip(master[::2], master[1::2]))\n    self.nodes.append(Node(info[\"name\"], info[\"ip\"], info[\"port\"]))", "path": "disredis\\disredis_client\\client.py", "repo_name": "SawdustSoftware/disredis", "stars": 80, "license": "other", "language": "python", "size": 185}
{"docstring": "\"\"\"\nTest setting a key on the master and retrieving it.\n\"\"\"\n", "func_signal": "def test_set_get(self):\n", "code": "self.client.set(\"test\", \"foo\")\nself.assertEqual(self.client.get(\"test\"), \"foo\")\nself.assertEqual(self.client.nodes[0].connection.data, {})\nself.assertEqual(self.client.nodes[1].connection.data, {\"test\":\"foo\"})", "path": "disredis\\disredis_client\\test_client.py", "repo_name": "SawdustSoftware/disredis", "stars": 80, "license": "other", "language": "python", "size": 185}
{"docstring": "\"\"\"\nReturn a Publish/Subscribe object. With this object, you can\nsubscribe to channels and listen for messages that get published to\nthem.\n\"\"\"\n\n    #### SERVER INFORMATION ####\n", "func_signal": "def pubsub(self, shard_hint=None):\n", "code": "\n\"Tell the Redis server to rewrite the AOF file from data in memory.\"\nraise NotImplementedError(\"Not supported for disredis.\")", "path": "disredis\\disredis_client\\client.py", "repo_name": "SawdustSoftware/disredis", "stars": 80, "license": "other", "language": "python", "size": 185}
{"docstring": "\"\"\"Returns the closest sibling to this Tag that matches the\ngiven criteria and appears after this Tag in the document.\"\"\"\n", "func_signal": "def findNextSibling(self, name=None, attrs={}, text=None, **kwargs):\n", "code": "return self._findOne(self.findNextSiblings, name, attrs, text,\n                     **kwargs)", "path": "BeautifulSoup.py", "repo_name": "albertsun/Intro-Data-Journalism-With-Python", "stars": 72, "license": "None", "language": "python", "size": 1847}
{"docstring": "#print \"Push\", tag.name\n", "func_signal": "def pushTag(self, tag):\n", "code": "if self.currentTag:\n    self.currentTag.contents.append(tag)\nself.tagStack.append(tag)\nself.currentTag = self.tagStack[-1]", "path": "BeautifulSoup.py", "repo_name": "albertsun/Intro-Data-Journalism-With-Python", "stars": 72, "license": "None", "language": "python", "size": 1847}
{"docstring": "\"\"\"Returns true iff this tag has the same name, the same attributes,\nand the same contents (recursively) as the given tag.\n\nNOTE: right now this will return false if two tags have the\nsame attributes in a different order. Should this be fixed?\"\"\"\n", "func_signal": "def __eq__(self, other):\n", "code": "if other is self:\n    return True\nif not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):\n    return False\nfor i in range(0, len(self.contents)):\n    if self.contents[i] != other.contents[i]:\n        return False\nreturn True", "path": "BeautifulSoup.py", "repo_name": "albertsun/Intro-Data-Journalism-With-Python", "stars": 72, "license": "None", "language": "python", "size": 1847}
{"docstring": "'''Given a string and its encoding, decodes the string into Unicode.\n%encoding is a string recognized by encodings.aliases'''\n\n# strip Byte Order Mark (if present)\n", "func_signal": "def _toUnicode(self, data, encoding):\n", "code": "if (len(data) >= 4) and (data[:2] == '\\xfe\\xff') \\\n       and (data[2:4] != '\\x00\\x00'):\n    encoding = 'utf-16be'\n    data = data[2:]\nelif (len(data) >= 4) and (data[:2] == '\\xff\\xfe') \\\n         and (data[2:4] != '\\x00\\x00'):\n    encoding = 'utf-16le'\n    data = data[2:]\nelif data[:3] == '\\xef\\xbb\\xbf':\n    encoding = 'utf-8'\n    data = data[3:]\nelif data[:4] == '\\x00\\x00\\xfe\\xff':\n    encoding = 'utf-32be'\n    data = data[4:]\nelif data[:4] == '\\xff\\xfe\\x00\\x00':\n    encoding = 'utf-32le'\n    data = data[4:]\nnewdata = unicode(data, encoding)\nreturn newdata", "path": "BeautifulSoup.py", "repo_name": "albertsun/Intro-Data-Journalism-With-Python", "stars": 72, "license": "None", "language": "python", "size": 1847}
{"docstring": "\"\"\"Given a document, tries to detect its XML encoding.\"\"\"\n", "func_signal": "def _detectEncoding(self, xml_data, isHTML=False):\n", "code": "xml_encoding = sniffed_xml_encoding = None\ntry:\n    if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n        # EBCDIC\n        xml_data = self._ebcdic_to_ascii(xml_data)\n    elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n        # UTF-16BE\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') \\\n             and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16BE with BOM\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n        # UTF-16LE\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and \\\n             (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16LE with BOM\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n        # UTF-32BE\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n        # UTF-32LE\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n        # UTF-32BE with BOM\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n        # UTF-32LE with BOM\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n    elif xml_data[:3] == '\\xef\\xbb\\xbf':\n        # UTF-8 with BOM\n        sniffed_xml_encoding = 'utf-8'\n        xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n    else:\n        sniffed_xml_encoding = 'ascii'\n        pass\nexcept:\n    xml_encoding_match = None\nxml_encoding_match = re.compile(\n    '^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>').match(xml_data)\nif not xml_encoding_match and isHTML:\n    regexp = re.compile('<\\s*meta[^>]+charset=([^>]*?)[;\\'\">]', re.I)\n    xml_encoding_match = regexp.search(xml_data)\nif xml_encoding_match is not None:\n    xml_encoding = xml_encoding_match.groups()[0].lower()\n    if isHTML:\n        self.declaredHTMLEncoding = xml_encoding\n    if sniffed_xml_encoding and \\\n       (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',\n                         'iso-10646-ucs-4', 'ucs-4', 'csucs4',\n                         'utf-16', 'utf-32', 'utf_16', 'utf_32',\n                         'utf16', 'u16')):\n        xml_encoding = sniffed_xml_encoding\nreturn xml_data, xml_encoding, sniffed_xml_encoding", "path": "BeautifulSoup.py", "repo_name": "albertsun/Intro-Data-Journalism-With-Python", "stars": 72, "license": "None", "language": "python", "size": 1847}
{"docstring": "\"\"\"Returns true iff the given string is the name of a\nself-closing tag according to this parser.\"\"\"\n", "func_signal": "def isSelfClosingTag(self, name):\n", "code": "return self.SELF_CLOSING_TAGS.has_key(name) \\\n       or self.instanceSelfClosingTags.has_key(name)", "path": "BeautifulSoup.py", "repo_name": "albertsun/Intro-Data-Journalism-With-Python", "stars": 72, "license": "None", "language": "python", "size": 1847}
{"docstring": "\"\"\"Beautiful Soup can detect a charset included in a META tag,\ntry to convert the document to that charset, and re-parse the\ndocument from the beginning.\"\"\"\n", "func_signal": "def start_meta(self, attrs):\n", "code": "httpEquiv = None\ncontentType = None\ncontentTypeIndex = None\ntagNeedsEncodingSubstitution = False\n\nfor i in range(0, len(attrs)):\n    key, value = attrs[i]\n    key = key.lower()\n    if key == 'http-equiv':\n        httpEquiv = value\n    elif key == 'content':\n        contentType = value\n        contentTypeIndex = i\n\nif httpEquiv and contentType: # It's an interesting meta tag.\n    match = self.CHARSET_RE.search(contentType)\n    if match:\n        if (self.declaredHTMLEncoding is not None or\n            self.originalEncoding == self.fromEncoding):\n            # An HTML encoding was sniffed while converting\n            # the document to Unicode, or an HTML encoding was\n            # sniffed during a previous pass through the\n            # document, or an encoding was specified\n            # explicitly and it worked. Rewrite the meta tag.\n            def rewrite(match):\n                return match.group(1) + \"%SOUP-ENCODING%\"\n            newAttr = self.CHARSET_RE.sub(rewrite, contentType)\n            attrs[contentTypeIndex] = (attrs[contentTypeIndex][0],\n                                       newAttr)\n            tagNeedsEncodingSubstitution = True\n        else:\n            # This is our first pass through the document.\n            # Go through it again with the encoding information.\n            newCharset = match.group(3)\n            if newCharset and newCharset != self.originalEncoding:\n                self.declaredHTMLEncoding = newCharset\n                self._feed(self.declaredHTMLEncoding)\n                raise StopParsing\n            pass\ntag = self.unknown_starttag(\"meta\", attrs)\nif tag and tagNeedsEncodingSubstitution:\n    tag.containsSubstitutions = True", "path": "BeautifulSoup.py", "repo_name": "albertsun/Intro-Data-Journalism-With-Python", "stars": 72, "license": "None", "language": "python", "size": 1847}
{"docstring": "#print \"Matching %s against %s\" % (markup, matchAgainst)\n", "func_signal": "def _matches(self, markup, matchAgainst):\n", "code": "result = False\nif matchAgainst is True:\n    result = markup is not None\nelif callable(matchAgainst):\n    result = matchAgainst(markup)\nelse:\n    #Custom match methods take the tag as an argument, but all\n    #other ways of matching match the tag name as a string.\n    if isinstance(markup, Tag):\n        markup = markup.name\n    if markup and not isinstance(markup, basestring):\n        markup = unicode(markup)\n    #Now we know that chunk is either a string, or None.\n    if hasattr(matchAgainst, 'match'):\n        # It's a regexp object.\n        result = markup and matchAgainst.search(markup)\n    elif hasattr(matchAgainst, '__iter__'): # list-like\n        result = markup in matchAgainst\n    elif hasattr(matchAgainst, 'items'):\n        result = markup.has_key(matchAgainst)\n    elif matchAgainst and isinstance(markup, basestring):\n        if isinstance(markup, unicode):\n            matchAgainst = unicode(matchAgainst)\n        else:\n            matchAgainst = str(matchAgainst)\n\n    if not result:\n        result = matchAgainst == markup\nreturn result", "path": "BeautifulSoup.py", "repo_name": "albertsun/Intro-Data-Journalism-With-Python", "stars": 72, "license": "None", "language": "python", "size": 1847}
{"docstring": "\"\"\"Setting tag[key] sets the value of the 'key' attribute for the\ntag.\"\"\"\n", "func_signal": "def __setitem__(self, key, value):\n", "code": "self._getAttrMap()\nself.attrMap[key] = value\nfound = False\nfor i in range(0, len(self.attrs)):\n    if self.attrs[i][0] == key:\n        self.attrs[i] = (key, value)\n        found = True\nif not found:\n    self.attrs.append((key, value))\nself._getAttrMap()[key] = value", "path": "BeautifulSoup.py", "repo_name": "albertsun/Intro-Data-Journalism-With-Python", "stars": 72, "license": "None", "language": "python", "size": 1847}
{"docstring": "\"\"\"Returns the closest sibling to this Tag that matches the\ngiven criteria and appears before this Tag in the document.\"\"\"\n", "func_signal": "def findPreviousSibling(self, name=None, attrs={}, text=None, **kwargs):\n", "code": "return self._findOne(self.findPreviousSiblings, name, attrs, text,\n                     **kwargs)", "path": "BeautifulSoup.py", "repo_name": "albertsun/Intro-Data-Journalism-With-Python", "stars": 72, "license": "None", "language": "python", "size": 1847}
{"docstring": "\"\"\"Pops the tag stack up to and including the most recent\ninstance of the given tag. If inclusivePop is false, pops the tag\nstack up to but *not* including the most recent instqance of\nthe given tag.\"\"\"\n#print \"Popping to %s\" % name\n", "func_signal": "def _popToTag(self, name, inclusivePop=True):\n", "code": "if name == self.ROOT_TAG_NAME:\n    return\n\nnumPops = 0\nmostRecentTag = None\nfor i in range(len(self.tagStack)-1, 0, -1):\n    if name == self.tagStack[i].name:\n        numPops = len(self.tagStack)-i\n        break\nif not inclusivePop:\n    numPops = numPops - 1\n\nfor i in range(0, numPops):\n    mostRecentTag = self.popTag()\nreturn mostRecentTag", "path": "BeautifulSoup.py", "repo_name": "albertsun/Intro-Data-Journalism-With-Python", "stars": 72, "license": "None", "language": "python", "size": 1847}
{"docstring": "\"\"\"Turns a list of maps, lists, or scalars into a single map.\nUsed to build the SELF_CLOSING_TAGS, NESTABLE_TAGS, and\nNESTING_RESET_TAGS maps out of lists and partial maps.\"\"\"\n", "func_signal": "def buildTagMap(default, *args):\n", "code": "built = {}\nfor portion in args:\n    if hasattr(portion, 'items'):\n        #It's a map. Merge it.\n        for k,v in portion.items():\n            built[k] = v\n    elif hasattr(portion, '__iter__'): # is a list\n        #It's a list. Map each item to the default.\n        for k in portion:\n            built[k] = default\n    else:\n        #It's a scalar. Map it to the default.\n        built[portion] = default\nreturn built", "path": "BeautifulSoup.py", "repo_name": "albertsun/Intro-Data-Journalism-With-Python", "stars": 72, "license": "None", "language": "python", "size": 1847}
{"docstring": "\"\"\"Recursively destroys the contents of this tree.\"\"\"\n", "func_signal": "def decompose(self):\n", "code": "self.extract()\nif len(self.contents) == 0:\n    return\ncurrent = self.contents[0]\nwhile current is not None:\n    next = current.next\n    if isinstance(current, Tag):\n        del current.contents[:]\n    current.parent = None\n    current.previous = None\n    current.previousSibling = None\n    current.next = None\n    current.nextSibling = None\n    current = next", "path": "BeautifulSoup.py", "repo_name": "albertsun/Intro-Data-Journalism-With-Python", "stars": 72, "license": "None", "language": "python", "size": 1847}
{"docstring": "#print \"End tag %s\" % name\n", "func_signal": "def unknown_endtag(self, name):\n", "code": "if self.quoteStack and self.quoteStack[-1] != name:\n    #This is not a real end tag.\n    #print \"</%s> is not real!\" % name\n    self.handle_data('</%s>' % name)\n    return\nself.endData()\nself._popToTag(name)\nif self.quoteStack and self.quoteStack[-1] == name:\n    self.quoteStack.pop()\n    self.literal = (len(self.quoteStack) > 0)", "path": "BeautifulSoup.py", "repo_name": "albertsun/Intro-Data-Journalism-With-Python", "stars": 72, "license": "None", "language": "python", "size": 1847}
{"docstring": "\"\"\"Used in a call to re.sub to replace HTML, XML, and numeric\nentities with the appropriate Unicode characters. If HTML\nentities are being converted, any unrecognized entities are\nescaped.\"\"\"\n", "func_signal": "def _convertEntities(self, match):\n", "code": "x = match.group(1)\nif self.convertHTMLEntities and x in name2codepoint:\n    return unichr(name2codepoint[x])\nelif x in self.XML_ENTITIES_TO_SPECIAL_CHARS:\n    if self.convertXMLEntities:\n        return self.XML_ENTITIES_TO_SPECIAL_CHARS[x]\n    else:\n        return u'&%s;' % x\nelif len(x) > 0 and x[0] == '#':\n    # Handle numeric entities\n    if len(x) > 1 and x[1] == 'x':\n        return unichr(int(x[2:], 16))\n    else:\n        return unichr(int(x[1:]))\n\nelif self.escapeUnrecognizedEntities:\n    return u'&amp;%s;' % x\nelse:\n    return u'&%s;' % x", "path": "BeautifulSoup.py", "repo_name": "albertsun/Intro-Data-Journalism-With-Python", "stars": 72, "license": "None", "language": "python", "size": 1847}
{"docstring": "\"\"\"Encodes an object to a string in some encoding, or to Unicode.\n.\"\"\"\n", "func_signal": "def toEncoding(self, s, encoding=None):\n", "code": "if isinstance(s, unicode):\n    if encoding:\n        s = s.encode(encoding)\nelif isinstance(s, str):\n    if encoding:\n        s = s.encode(encoding)\n    else:\n        s = unicode(s)\nelse:\n    if encoding:\n        s  = self.toEncoding(str(s), encoding)\n    else:\n        s = unicode(s)\nreturn s", "path": "BeautifulSoup.py", "repo_name": "albertsun/Intro-Data-Journalism-With-Python", "stars": 72, "license": "None", "language": "python", "size": 1847}
{"docstring": "\"\"\"Replace the contents of the tag with a string\"\"\"\n", "func_signal": "def setString(self, string):\n", "code": "self.clear()\nself.append(string)", "path": "BeautifulSoup.py", "repo_name": "albertsun/Intro-Data-Journalism-With-Python", "stars": 72, "license": "None", "language": "python", "size": 1847}
{"docstring": "\"\"\"Sets up the initial relations between this element and\nother elements.\"\"\"\n", "func_signal": "def setup(self, parent=None, previous=None):\n", "code": "self.parent = parent\nself.previous = previous\nself.next = None\nself.previousSibling = None\nself.nextSibling = None\nif self.parent and self.parent.contents:\n    self.previousSibling = self.parent.contents[-1]\n    self.previousSibling.nextSibling = self", "path": "BeautifulSoup.py", "repo_name": "albertsun/Intro-Data-Journalism-With-Python", "stars": 72, "license": "None", "language": "python", "size": 1847}
{"docstring": "\"\"\"Handle entity references as data, possibly converting known\nHTML and/or XML entity references to the corresponding Unicode\ncharacters.\"\"\"\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "data = None\nif self.convertHTMLEntities:\n    try:\n        data = unichr(name2codepoint[ref])\n    except KeyError:\n        pass\n\nif not data and self.convertXMLEntities:\n        data = self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref)\n\nif not data and self.convertHTMLEntities and \\\n    not self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref):\n        # TODO: We've got a problem here. We're told this is\n        # an entity reference, but it's not an XML entity\n        # reference or an HTML entity reference. Nonetheless,\n        # the logical thing to do is to pass it through as an\n        # unrecognized entity reference.\n        #\n        # Except: when the input is \"&carol;\" this function\n        # will be called with input \"carol\". When the input is\n        # \"AT&T\", this function will be called with input\n        # \"T\". We have no way of knowing whether a semicolon\n        # was present originally, so we don't know whether\n        # this is an unknown entity or just a misplaced\n        # ampersand.\n        #\n        # The more common case is a misplaced ampersand, so I\n        # escape the ampersand and omit the trailing semicolon.\n        data = \"&amp;%s\" % ref\nif not data:\n    # This case is different from the one above, because we\n    # haven't already gone through a supposedly comprehensive\n    # mapping of entities to Unicode characters. We might not\n    # have gone through any mapping at all. So the chances are\n    # very high that this is a real entity, and not a\n    # misplaced ampersand.\n    data = \"&%s;\" % ref\nself.handle_data(data)", "path": "BeautifulSoup.py", "repo_name": "albertsun/Intro-Data-Journalism-With-Python", "stars": 72, "license": "None", "language": "python", "size": 1847}
{"docstring": "\"\"\"Adds a certain piece of text to the tree as a NavigableString\nsubclass.\"\"\"\n", "func_signal": "def _toStringSubclass(self, text, subclass):\n", "code": "self.endData()\nself.handle_data(text)\nself.endData(subclass)", "path": "BeautifulSoup.py", "repo_name": "albertsun/Intro-Data-Journalism-With-Python", "stars": 72, "license": "None", "language": "python", "size": 1847}
{"docstring": "\"\"\"Draw a line around original_widget.\"\"\"\n\n", "func_signal": "def __init__(self, original_widget):\n", "code": "tlcorner=None; tline=None; lline=None\ntrcorner=None; blcorner=None; rline=None\nbline=None; brcorner=None\n\ndef use_attr( a, t ):\n    if a is not None:\n        t = urwid.AttrWrap(t, a)\n    return t\n    \ntline = use_attr( tline, Divider(utf8decode(\"\u2500\")))\nbline = use_attr( bline, Divider(utf8decode(\"\u2500\")))\nlline = use_attr( lline, SolidFill(utf8decode(\"\u2502\")))\nrline = use_attr( rline, SolidFill(utf8decode(\"\u2502\")))\ntlcorner = use_attr( tlcorner, Text(utf8decode(\"\u250c\")))\ntrcorner = use_attr( trcorner, Text(utf8decode(\"\u2510\")))\nblcorner = use_attr( blcorner, Text(utf8decode(\"\u2514\")))\nbrcorner = use_attr( brcorner, Text(utf8decode(\"\u2518\")))\ntop = Columns([ ('fixed', 1, tlcorner),\n    tline, ('fixed', 1, trcorner) ])\nmiddle = Columns( [('fixed', 1, lline),\n    original_widget, ('fixed', 1, rline)], box_columns = [0,2],\n    focus_column = 1)\nbottom = Columns([ ('fixed', 1, blcorner),\n    bline, ('fixed', 1, brcorner) ])\npile = Pile([('flow',top),middle,('flow',bottom)],\n    focus_item = 1)\n\nWidgetDecoration.__init__(self, original_widget)\nWidgetWrap.__init__(self, pile)", "path": "urwid\\graphics.py", "repo_name": "Hebo/reddit-cli", "stars": 69, "license": "None", "language": "python", "size": 249}
{"docstring": "\"\"\"\nnormal -- attribute for uncomplete part of progress bar\ncomplete -- attribute for complete part of progress bar\ncurrent -- current progress\ndone -- progress amount at 100%\nsatt -- attribute for smoothed part of bar where the foreground\n    of satt corresponds to the normal part and the\n    background corresponds to the complete part.  If satt\n    is None then no smoothing will be done.\n\"\"\"\n", "func_signal": "def __init__(self, normal, complete, current=0, done=100, satt=None):\n", "code": "self.normal = normal\nself.complete = complete\nself.current = current\nself.done = done\nself.satt = satt", "path": "urwid\\graphics.py", "repo_name": "Hebo/reddit-cli", "stars": 69, "license": "None", "language": "python", "size": 249}
{"docstring": "\"\"\"\nStore bar data, bargraph top and horizontal line positions.\n\nbardata -- a list of bar values.\ntop -- maximum value for segments within bardata\nhlines -- None or a bar value marking horizontal line positions\n\nbar values are [ segment1, segment2, ... ] lists where top is \nthe maximal value corresponding to the top of the bar graph and\nsegment1, segment2, ... are the values for the top of each \nsegment of this bar.  Simple bar graphs will only have one\nsegment in each bar value.\n\nEg: if top is 100 and there is a bar value of [ 80, 30 ] then\nthe top of this bar will be at 80% of full height of the graph\nand it will have a second segment that starts at 30%.\n\"\"\"\n", "func_signal": "def set_data(self, bardata, top, hlines=None):\n", "code": "if hlines is not None:\n    hlines = hlines[:] # shallow copy\n    hlines.sort()\nself.data = bardata, top, hlines\nself._invalidate()", "path": "urwid\\graphics.py", "repo_name": "Hebo/reddit-cli", "stars": 69, "license": "None", "language": "python", "size": 249}
{"docstring": "\"\"\"\nRender the progress bar.\n\"\"\"\n", "func_signal": "def render(self, size, focus=False):\n", "code": "(maxcol,) = size\npercent = int( self.current*100/self.done )\nif percent < 0: percent = 0\nif percent > 100: percent = 100\n    \ntxt=Text( str(percent)+\" %\", 'center', 'clip' )\nc = txt.render((maxcol,))\n\ncf = float( self.current ) * maxcol / self.done\nccol = int( cf )\ncs = 0\nif self.satt is not None:\n    cs = int((cf - ccol) * 8)\nif ccol < 0 or (ccol == 0 and cs == 0):\n    c._attr = [[(self.normal,maxcol)]]\nelif ccol >= maxcol:\n    c._attr = [[(self.complete,maxcol)]]\nelif cs and c._text[0][ccol] == \" \":\n    t = c._text[0]\n    cenc = self.eighths[cs].encode(\"utf-8\")\n    c._text[0] = t[:ccol]+cenc+t[ccol+1:]\n    a = []\n    if ccol > 0:\n        a.append( (self.complete, ccol) )\n    a.append((self.satt,len(cenc)))\n    if maxcol-ccol-1 > 0:\n        a.append( (self.normal, maxcol-ccol-1) )\n    c._attr = [a]\n    c._cs = [[(None, len(c._text[0]))]]\nelse:\n    c._attr = [[(self.complete,ccol),\n        (self.normal,maxcol-ccol)]]\nreturn c", "path": "urwid\\graphics.py", "repo_name": "Hebo/reddit-cli", "stars": 69, "license": "None", "language": "python", "size": 249}
{"docstring": "\"\"\"\nset_scale( [(label1 position, label1 markup),...], top )\nlabel position -- 0 < position < top for the y position\nlabel markup -- text markup for this label\ntop -- top y position\n\"\"\"\n\n", "func_signal": "def set_scale(self, labels, top):\n", "code": "labels = labels[:] # shallow copy\nlabels.sort()\nlabels.reverse()\nself.pos = []\nself.txt = []\nfor y, markup in labels:\n    self.pos.append(y)\n    self.txt.append( Text(markup) )\nself.top = top", "path": "urwid\\graphics.py", "repo_name": "Hebo/reddit-cli", "stars": 69, "license": "None", "language": "python", "size": 249}
{"docstring": "\"\"\"\nRender BarGraph.\n\"\"\"\n", "func_signal": "def render(self, size, focus=False):\n", "code": "(maxcol, maxrow) = size\ndisp = self.calculate_display( (maxcol,maxrow) )\n\ncombinelist = []\nfor y_count, row in disp:\n    l = []\n    for bar_type, width in row:\n        if type(bar_type) == type(()):\n            if len(bar_type) == 3:\n                # vertical eighths\n                fg,bg,k = bar_type\n                a = self.satt[(fg,bg)]\n                t = self.eighths[k] * width\n            else:\n                # horizontal lines\n                bg,k = bar_type\n                a = self.hatt[bg]\n                t = self.hlines[k] * width\n        else:\n            a = self.attr[bar_type]\n            t = self.char[bar_type] * width\n        l.append( (a, t) )\n    c = Text(l).render( (maxcol,) )\n    assert c.rows() == 1, \"Invalid characters in BarGraph!\"\n    combinelist += [(c, None, False)] * y_count\n    \ncanv = CanvasCombine(combinelist)\nreturn canv", "path": "urwid\\graphics.py", "repo_name": "Hebo/reddit-cli", "stars": 69, "license": "None", "language": "python", "size": 249}
{"docstring": "\"\"\"\nCalculate display data.\n\"\"\"\n", "func_signal": "def calculate_display(self, size):\n", "code": "(maxcol, maxrow) = size\nbardata, top, hlines = self.get_data( (maxcol, maxrow) )\nwidths = self.calculate_bar_widths( (maxcol, maxrow), bardata )\n\nif self.use_smoothed():\n    disp = calculate_bargraph_display(bardata, top, widths,\n        maxrow * 8 )\n    disp = self.smooth_display( disp )\n    \nelse:\n    disp = calculate_bargraph_display(bardata, top, widths,\n        maxrow )\n\nif hlines:\n    disp = self.hlines_display( disp, top, hlines, maxrow )\n\nreturn disp", "path": "urwid\\graphics.py", "repo_name": "Hebo/reddit-cli", "stars": 69, "license": "None", "language": "python", "size": 249}
{"docstring": "\"\"\"Return screenshots as a list of HTML fragments.\"\"\"\n", "func_signal": "def screenshot_collect():\n", "code": "l = HtmlGenerator.fragments\nHtmlGenerator.fragments = []\nreturn l", "path": "urwid\\html_fragment.py", "repo_name": "Hebo/reddit-cli", "stars": 69, "license": "None", "language": "python", "size": 249}
{"docstring": "\"\"\"\nAdd hlines to display structure represented as bar_type tuple\nvalues:\n(bg, 0-5)\nbg is the segment that has the hline on it\n0-5 is the hline graphic to use where 0 is a regular underscore\nand 1-5 are the UTF-8 horizontal scan line characters.\n\"\"\"\n", "func_signal": "def hlines_display(self, disp, top, hlines, maxrow ):\n", "code": "if self.use_smoothed():\n    shiftr = 0\n    r = [    (0.2, 1),\n        (0.4, 2),\n        (0.6, 3),\n        (0.8, 4),\n        (1.0, 5),]\nelse:\n    shiftr = 0.5\n    r = [    (1.0, 0), ]\n\n# reverse the hlines to match screen ordering\nrhl = []\nfor h in hlines:\n    rh = float(top-h) * maxrow / top - shiftr\n    if rh < 0:\n        continue\n    rhl.append(rh)\n    \n# build a list of rows that will have hlines\nhrows = []\nlast_i = -1\nfor rh in rhl:\n    i = int(rh)\n    if i == last_i:\n        continue\n    f = rh-i\n    for spl, chnum in r:\n        if f < spl:\n            hrows.append( (i, chnum) )\n            break\n    last_i = i\n\n# fill hlines into disp data\ndef fill_row( row, chnum ):\n    rout = []\n    for bar_type, width in row:\n        if (type(bar_type) == type(0) and \n                len(self.hatt) > bar_type ):\n            rout.append( ((bar_type, chnum), width))\n            continue\n        rout.append( (bar_type, width))\n    return rout\n    \no = []\nk = 0\nrnum = 0\nfor y_count, row in disp:\n    if k >= len(hrows):\n        o.append( (y_count, row) )\n        continue\n    end_block = rnum + y_count\n    while k < len(hrows) and hrows[k][0] < end_block:\n        i, chnum = hrows[k]\n        if i-rnum > 0:\n            o.append( (i-rnum, row) )\n        o.append( (1, fill_row( row, chnum ) ) )\n        rnum = i+1\n        k += 1\n    if rnum < end_block:\n        o.append( (end_block-rnum, row) )\n        rnum = end_block\n\n#assert 0, o\nreturn o", "path": "urwid\\graphics.py", "repo_name": "Hebo/reddit-cli", "stars": 69, "license": "None", "language": "python", "size": 249}
{"docstring": "\"\"\"\nSet a preferred bar width for calculate_bar_widths to use.\n\nwidth -- width of bar or None for automatic width adjustment\n\"\"\"\n", "func_signal": "def set_bar_width(self, width):\n", "code": "assert width is None or width > 0\nself.bar_width = width\nself._invalidate()", "path": "urwid\\graphics.py", "repo_name": "Hebo/reddit-cli", "stars": 69, "license": "None", "language": "python", "size": 249}
{"docstring": "\"\"\"Return the next list of keypresses in HtmlGenerator.keys.\"\"\"\n", "func_signal": "def get_input(self, raw_keys=False):\n", "code": "if not self.keys:\n    raise ExitMainLoop()\nif raw_keys:\n    return (self.keys.pop(0), [])\nreturn self.keys.pop(0)", "path": "urwid\\html_fragment.py", "repo_name": "Hebo/reddit-cli", "stars": 69, "license": "None", "language": "python", "size": 249}
{"docstring": "\"\"\"\nRender GraphVScale.\n\"\"\"\n", "func_signal": "def render(self, size, focus=False):\n", "code": "(maxcol, maxrow) = size\npl = scale_bar_values( self.pos, self.top, maxrow )\n\ncombinelist = []\nrows = 0\nfor p, t in zip(pl, self.txt):\n    p -= 1\n    if p >= maxrow: break\n    if p < rows: continue\n    c = t.render((maxcol,))\n    if p > rows:\n        run = p-rows\n        c = CompositeCanvas(c)\n        c.pad_trim_top_bottom(run, 0)\n    rows += c.rows()\n    combinelist.append((c, None, False))\nc = CanvasCombine(combinelist)\nif maxrow - rows:\n    c.pad_trim_top_bottom(0, maxrow - rows)\nreturn c", "path": "urwid\\graphics.py", "repo_name": "Hebo/reddit-cli", "stars": 69, "license": "None", "language": "python", "size": 249}
{"docstring": "\"\"\"\nmarkup -- same as Text widget markup\nfont -- instance of a Font class\n\"\"\"\n", "func_signal": "def __init__(self, markup, font):\n", "code": "self.set_font(font)\nself.set_text(markup)", "path": "urwid\\graphics.py", "repo_name": "Hebo/reddit-cli", "stars": 69, "license": "None", "language": "python", "size": 249}
{"docstring": "\"\"\"\nCreate a bar graph with the passed display characteristics.\nsee set_segment_attributes for a description of the parameters.\n\"\"\"\n\n", "func_signal": "def __init__(self, attlist, hatt=None, satt=None):\n", "code": "self.set_segment_attributes( attlist, hatt, satt )\nself.set_data([], 1, None)\nself.set_bar_width(None)", "path": "urwid\\graphics.py", "repo_name": "Hebo/reddit-cli", "stars": 69, "license": "None", "language": "python", "size": 249}
{"docstring": "\"\"\"\nReturn a list of bar widths, one for each bar in data.\n\nIf self.bar_width is None this implementation will stretch \nthe bars across the available space specified by maxcol.\n\"\"\"\n", "func_signal": "def calculate_bar_widths(self, size, bardata):\n", "code": "(maxcol, maxrow) = size\n\nif self.bar_width is not None:\n    return [self.bar_width] * min(\n        len(bardata), maxcol/self.bar_width )\n\nif len(bardata) >= maxcol:\n    return [1] * maxcol\n\nwidths = []\ngrow = maxcol\nremain = len(bardata)\nfor row in bardata:\n    w = int(float(grow) / remain + 0.5)\n    widths.append(w)\n    grow -= w\n    remain -= 1\nreturn widths", "path": "urwid\\graphics.py", "repo_name": "Hebo/reddit-cli", "stars": 69, "license": "None", "language": "python", "size": 249}
{"docstring": "\"\"\"\nCreate canvas containing an ASCII version of the Python\nLogo and store it.\n\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "blu = AttrSpec('light blue', 'default')\nyel = AttrSpec('yellow', 'default')\nwidth = 17\nself._canvas = Text([\n    (blu, \"     ______\\n\"),\n    (blu, \"   _|_o__  |\"), (yel, \"__\\n\"),\n    (blu, \"  |   _____|\"), (yel, \"  |\\n\"),\n    (blu, \"  |__|  \"), (yel, \"______|\\n\"),\n    (yel, \"     |____o_|\")]).render((width,))", "path": "urwid\\graphics.py", "repo_name": "Hebo/reddit-cli", "stars": 69, "license": "None", "language": "python", "size": 249}
{"docstring": "\"\"\"\nReturn (bardata, top, hlines)\n\nThis function is called by render to retrieve the data for\nthe graph. It may be overloaded to create a dynamic bar graph.\n\nThis implementation will truncate the bardata list returned \nif not all bars will fit within maxcol.\n\"\"\"\n", "func_signal": "def _get_data(self, size):\n", "code": "(maxcol, maxrow) = size\nbardata, top, hlines = self.data\nwidths = self.calculate_bar_widths((maxcol,maxrow),bardata)\n\nif len(bardata) > len(widths):\n    return bardata[:len(widths)], top, hlines\n\nreturn bardata, top, hlines", "path": "urwid\\graphics.py", "repo_name": "Hebo/reddit-cli", "stars": 69, "license": "None", "language": "python", "size": 249}
{"docstring": "\"\"\"\nReturn the pre-rendered canvas.\n\"\"\"\n", "func_signal": "def render(self, size, focus=False):\n", "code": "fixed_size(size)\nreturn self._canvas", "path": "urwid\\graphics.py", "repo_name": "Hebo/reddit-cli", "stars": 69, "license": "None", "language": "python", "size": 249}
{"docstring": "\"\"\"\ncurrent -- current progress\n\"\"\"\n", "func_signal": "def set_completion(self, current ):\n", "code": "self.current = current\nself._invalidate()", "path": "urwid\\graphics.py", "repo_name": "Hebo/reddit-cli", "stars": 69, "license": "None", "language": "python", "size": 249}
{"docstring": "\"\"\"\nDisable caching on this bargraph because get_data_fn needs\nto be polled to get the latest data.\n\"\"\"\n", "func_signal": "def nocache_bargraph_get_data(self, get_data_fn):\n", "code": "self.render = nocache_widget_render_instance(self)\nself._get_data = get_data_fn", "path": "urwid\\graphics.py", "repo_name": "Hebo/reddit-cli", "stars": 69, "license": "None", "language": "python", "size": 249}
{"docstring": "\"\"\"\nload FeatureVector using (feat, weight) pairs from file\n\"\"\"\n", "func_signal": "def load(self, input_file):\n", "code": "import codecs\nimport os.path\n\nif not os.path.exists(input_file):\n    logger.error('file does not exist: %s' % input_file)\n\nself.clear()\nwith codecs.open(input_file, 'r', 'utf-8') as infile:\n    for line in infile:\n        line = line.strip()\n        if line.startswith('#'): continue\n        k, v = line.split('\\t')\n        new_k = tuple(k.split())\n        self.setdefault(new_k, float(v))\n        \nreturn self", "path": "src\\fei\\model\\feat_vec.py", "repo_name": "summarization/semantic_summ", "stars": 74, "license": "None", "language": "python", "size": 284}
{"docstring": "\"\"\"\nrun logistic regression using creg\ntrain_iter: iterator of (dict, float) pairs over train set\n\"\"\"\n", "func_signal": "def trainLR(self, train_iter, l1=0.01):\n", "code": "logger.debug('start training LR model...')\nimport creg #@UnresolvedImport\ntrain_data = creg.CategoricalDataset(train_iter)\nself.model = creg.LogisticRegression()\nself.model.fit(train_data, l1)\nself.model_l1 = l1\nreturn", "path": "src\\fei\\model\\edge_filter.py", "repo_name": "summarization/semantic_summ", "stars": 74, "license": "None", "language": "python", "size": 284}
{"docstring": "\"\"\"\nReturn a list of files in dir_path.\n\n\"\"\"\n\n", "func_signal": "def list_files(dir_path, recursive=True):\n", "code": "for root, dirs, files in os.walk(dir_path):\n    file_list = [os.path.join(root, f) for f in files]\n    if recursive:\n        for dir in dirs:\n            dir = os.path.join(root, dir)\n            file_list.extend(list_files(dir, recursive=True))\n    return file_list", "path": "src\\fei\\eval\\pyrouge\\utils\\file_utils.py", "repo_name": "summarization/semantic_summ", "stars": 74, "license": "None", "language": "python", "size": 284}
{"docstring": "\"\"\"\noverload operator '*'\nmultiply feature vector with a scaling factor (float)\n\"\"\"\n", "func_signal": "def __mul__(self, scaling):\n", "code": "if not type(scaling) == float: return\nself.scaling *= scaling\nreturn self", "path": "src\\fei\\model\\feat_vec.py", "repo_name": "summarization/semantic_summ", "stars": 74, "license": "None", "language": "python", "size": 284}
{"docstring": "\"\"\"\nconsiders only unique triples\n\"\"\"\n\n", "func_signal": "def getCoverageStats(models_triples, docs_triples, docs_extended_triples=None, docs_concepts=None):\n", "code": "counts = defaultdict(lambda: defaultdict(int))\n\n# important to iterate through model files\nfor topic in models_triples:\n    \n    m_triples = models_triples[topic]\n    m_triples_exact = set()\n    \n    # get triples from documents\n    d_triples = docs_triples[topic]\n    d_triples_exact = set(CompareTriplesExactMatch(t) for t in d_triples)\n    d_triples_wo_rel = set(CompareTriplesWoRelation(t) for t in d_triples)\n    counts[topic]['uniq_d_triples'] = len(d_triples_exact)\n    \n    # get extended triples\n    d_extended_triples = None\n    if docs_extended_triples is not None: \n        d_extended_triples = docs_extended_triples[topic]\n        d_extended_triples = set(CompareTriplesWoRelation(t) for t in d_extended_triples)\n    \n    # get concepts\n    d_concepts = None\n    if docs_concepts is not None:\n        d_concepts = docs_concepts[topic]\n\n    for t in m_triples:\n        \n        counts[topic]['num_m_triples'] += 1\n        t_exact = CompareTriplesExactMatch(t)\n        if t_exact in m_triples_exact: continue\n        \n        m_triples_exact.add(t_exact)\n        counts[topic]['uniq_m_triples'] += 1\n         \n        # if there is exact match\n        if t_exact in d_triples_exact:\n            counts[topic]['exact_match'] += 1\n            \n        else: # if only two concepts match (w/o relation)\n            t_wo_rel = CompareTriplesWoRelation(t)\n            if t_wo_rel in d_triples_wo_rel:\n                counts[topic]['wo_relation'] += 1\n                \n            else: # if two concepts are in same sentence\n                if d_extended_triples is not None and t_wo_rel in d_extended_triples:\n                    counts[topic]['same_sent'] += 1\n                    \n                else: # if two concepts are in different sentences\n                    if d_concepts is not None and t.concept1 in d_concepts and t.concept2 in d_concepts:\n                        counts[topic]['diff_sent'] += 1\n                        \n                    else: # at least one concept is not in document set\n                        counts[topic]['no_match'] += 1\n\nnum_topics = 0\nnum_uniq_m_triples = 0.0\nnum_uniq_d_triples = 0.0\nper_uniq_m_triples = 0.0\nper_exact_match = 0.0\nper_wo_relation = 0.0\nper_same_sent = 0.0\nper_diff_sent = 0.0\nper_no_match = 0.0\n\nfor topic in counts:\n    num_topics += 1\n    num_uniq_m_triples += counts[topic]['uniq_m_triples']\n    num_uniq_d_triples += counts[topic]['uniq_d_triples']\n    per_uniq_m_triples += counts[topic]['uniq_m_triples']/float(counts[topic]['num_m_triples'])\n    per_exact_match += counts[topic]['exact_match']/float(counts[topic]['uniq_m_triples'])\n    per_wo_relation += counts[topic]['wo_relation']/float(counts[topic]['uniq_m_triples'])\n    per_same_sent += counts[topic]['same_sent']/float(counts[topic]['uniq_m_triples'])\n    per_diff_sent += counts[topic]['diff_sent']/float(counts[topic]['uniq_m_triples'])\n    per_no_match += counts[topic]['no_match']/float(counts[topic]['uniq_m_triples'])\n\nnum_uniq_m_triples /= num_topics\nnum_uniq_d_triples /= num_topics\nper_uniq_m_triples /= num_topics\nper_exact_match /= num_topics\nper_wo_relation /= num_topics\nper_same_sent /= num_topics\nper_diff_sent /= num_topics\nper_no_match /= num_topics\n\nresult = '''\n------\nnumber of total files: %d\nnumber of unique triples per summary: %.1f\nnumber of unique triples per document set: %.1f\npercentage of unique triples among all triples in summary: %.1f%%\n------\nunique summary triples (exact match): %.1f%%\nunique summary triples (w/o relation): %.1f%%\nunique summary triples (same sentence): %.1f%%\nunique summary triples (diff sentence): %.1f%%\nunique summary triples (no match): %.1f%%\n''' % (num_topics, num_uniq_m_triples, num_uniq_d_triples, per_uniq_m_triples*100,\n       per_exact_match*100, per_wo_relation*100, per_same_sent*100, per_diff_sent*100, per_no_match*100)\n\nreturn result", "path": "src\\fei\\preprocess\\stats_coverage.py", "repo_name": "summarization/semantic_summ", "stars": 74, "license": "None", "language": "python", "size": 284}
{"docstring": "\"\"\"\noverload operator '+=', merge keys in both vectors\n\"\"\"\n", "func_signal": "def __iadd__(self, other):\n", "code": "if type(other) == FeatureVector: # handle FeatureVector\n    for k, v in other.iteritems():\n        self[k] = self.get(k, 0.0) + v\n\nif type(other) == CompositeFeatureVector: # handle CompositeFeatureVector\n    feat_vec, scaling = other.feat_vec, other.scaling\n    for k, v in feat_vec.iteritems():\n        self[k] = self.get(k, 0.0) + v * scaling\n\nreturn self", "path": "src\\fei\\model\\feat_vec.py", "repo_name": "summarization/semantic_summ", "stars": 74, "license": "None", "language": "python", "size": 284}
{"docstring": "\"\"\"\noverload operator '-=', merge keys in both vectors\n\"\"\"\n# handle both FeatureVector and CompositeFeatureVector\n", "func_signal": "def __isub__(self, other):\n", "code": "if type(other) == FeatureVector or type(other) == CompositeFeatureVector: \n    self += -1.0 * other\n\nreturn self", "path": "src\\fei\\model\\feat_vec.py", "repo_name": "summarization/semantic_summ", "stars": 74, "license": "None", "language": "python", "size": 284}
{"docstring": "\"\"\"\nreturn fired features\n\"\"\"\n", "func_signal": "def extract(self, edge, add_feat=True):\n", "code": "relation = edge.relation\nnode1 = edge.node1\nnode2 = edge.node2\n\ndep_node1 = str(len(node1.graphIdx.split('.')))\ndep_node2 = str(len(node2.graphIdx.split('.')))\n\nactive_percepts = {}\n\nactive_percepts.update(self.fireFeat('relation', relation, 1, add_feat))\nactive_percepts.update(self.fireFeat('dep_node1', dep_node1, 1, add_feat))\nactive_percepts.update(self.fireFeat('dep_node2', dep_node2, 1, add_feat))\n\nif re.search(r'-\\d{2}$', node1.concept):\n    active_percepts.update(self.fireFeat('verb_node1', '', 1, add_feat))\nif re.search(r'-\\d{2}$', node2.concept):\n    active_percepts.update(self.fireFeat('verb_node2', '', 1, add_feat))\n\nconcept1 = re.sub(r'-\\d{2}$', '', node1.concept)\nconcept2 = re.sub(r'-\\d{2}$', '', node2.concept)\n\nactive_percepts.update(self.fireFeat('concept1', concept1, 1, add_feat))\nactive_percepts.update(self.fireFeat('concept2', concept2, 1, add_feat))\n\nreturn active_percepts", "path": "src\\fei\\model\\edge_filter.py", "repo_name": "summarization/semantic_summ", "stars": 74, "license": "None", "language": "python", "size": 284}
{"docstring": "\"\"\"\nextract list of triples from JAMR parsed file\n\"\"\"\n", "func_signal": "def getExtendedTriples(input_file):\n", "code": "extended_triples = []\n\ncached_concepts = []\ncached_indices = {}\nline_num = -1 # start from 0\nbase_filename = os.path.basename(input_file)\n\nwith codecs.open(input_file, 'r', 'utf-8') as infile:\n    for line in infile:\n        line = line.strip()\n        \n        if line.startswith('# ::snt'): \n            line_num += 1\n            continue\n        \n        if line == '' and cached_indices:\n            for i, c1 in enumerate(cached_concepts):\n                for j, c2 in enumerate(cached_concepts):\n                    if j <= i: continue\n                    t = Triple(c1, c2, '', base_filename, line_num, tokens=None)\n                    extended_triples.append(t)\n            cached_concepts = []\n            cached_indices = {}\n            continue\n            \n        if line.startswith('(') and line.endswith(')'):\n            line = line[1:-1] # remove parentheses\n            triple = re.sub(r'[0-9A-Za-z\\-]+ \\/ ', '', line)\n            \n            try:\n                c1, _, c2 = triple.split(', ')\n                idx1, idx2 = re.findall(r'([0-9A-Za-z]+) \\/', line)\n                \n                if idx1 not in cached_indices:\n                    cached_indices[idx1] = 1\n                    cached_concepts.append(c1.lower())\n                    \n                if idx2 not in cached_indices:\n                    cached_indices[idx2] = 1\n                    cached_concepts.append(c2.lower())\n                    \n            except ValueError: pass\n            \nreturn extended_triples", "path": "src\\fei\\preprocess\\stats_jamr.py", "repo_name": "summarization/semantic_summ", "stars": 74, "license": "None", "language": "python", "size": 284}
{"docstring": "# write feature mapping\n", "func_signal": "def writeFiles(self, output_dir):\n", "code": "import os.path\nimport codecs\n\noutput_file_percepts = os.path.join(output_dir, 'percepts')\nwith codecs.open(output_file_percepts, 'w', 'utf-8') as outfile:\n    for percept, index in sorted(self.percept_indices.iteritems(), key=lambda x: x[1]):\n        count = self.percept_counts[index]\n        outfile.write('%d\\t%d\\t%s\\n' % (index, count, '\\t'.join([str(w) for w in percept])))\n\n# write model weights\noutput_file_weights = os.path.join(output_dir, 'weights_' + str(self.model_l1))\nwith codecs.open(output_file_weights, 'w', 'utf-8') as outfile:\n    for label in sorted(self.model.weights):\n        for fname, w in sorted(self.model.weights[label].iteritems(), \n                               key=lambda x: x[1], reverse=True):\n            outfile.write('%s\\t%s\\t%f\\n' % (label, fname, w))\n\n# write model posteriors\noutput_file_posteriors = os.path.join(output_dir, 'posteriors')\nwith codecs.open(output_file_posteriors, 'w', 'utf-8') as outfile:\n    for posteriors in self.model_posteriors_iter:\n        outfile.write('%s\\n' % '\\t'.join([str(p) for p in posteriors]))\n\nreturn", "path": "src\\fei\\model\\edge_filter.py", "repo_name": "summarization/semantic_summ", "stars": 74, "license": "None", "language": "python", "size": 284}
{"docstring": "\"\"\"\nreturn new vector containing the same keys as in 'other' vector\n\"\"\"\n", "func_signal": "def slice(self, other):\n", "code": "if not type(other) == FeatureVector: return\nnew_vec = FeatureVector()\nfor k, _ in other.iteritems():\n    new_vec[k] = self.get(k, 0.0)\n    \nreturn new_vec", "path": "src\\fei\\model\\feat_vec.py", "repo_name": "summarization/semantic_summ", "stars": 74, "license": "None", "language": "python", "size": 284}
{"docstring": "\"\"\"\ndot product of two vectors, return float\n\"\"\"\n", "func_signal": "def dot(self, other):\n", "code": "if not type(other) == FeatureVector: return\n(sht_vec, lng_vec) = (self, other) if len(self) < len(other) else (other, self)\n\nscore = 0.0\nfor k, v in sht_vec.iteritems(): \n    score += lng_vec.get(k, 0.0) * v\n        \nreturn score", "path": "src\\fei\\model\\feat_vec.py", "repo_name": "summarization/semantic_summ", "stars": 74, "license": "None", "language": "python", "size": 284}
{"docstring": "\"\"\"\nget triples from JAMR parsed file\n\"\"\"\n", "func_signal": "def getTriples(input_file):\n", "code": "base_filename = os.path.basename(input_file)\nline_num = -1 # start from 0\ntriples = []\n\nwith codecs.open(input_file, 'r', 'utf-8') as infile:\n    for line in infile:\n        line = line.strip()\n        \n        if line.startswith('# ::snt'): \n            line_num += 1\n            continue\n\n        if line.startswith('(') and line.endswith(')'):\n            line = line[1:-1] # remove parentheses\n            triple = re.sub(r'[0-9A-Za-z\\-]+ \\/ ', '', line)\n            \n            try:\n                c1, r, c2 = triple.split(', ')\n                t = Triple(c1.lower(), c2.lower(), r.lower(), base_filename, line_num, tokens=None)\n                triples.append(t)\n                \n            except ValueError: pass\n            \nreturn triples", "path": "src\\fei\\preprocess\\stats_jamr.py", "repo_name": "summarization/semantic_summ", "stars": 74, "license": "None", "language": "python", "size": 284}
{"docstring": "\"\"\"\nFeature pruning\n\"\"\"\n# percepts to be removed\n", "func_signal": "def prune(self, train_data, cutoff=1):\n", "code": "deleted_percept_ids = set()\n\nfor percept_id, count in self.percept_counts.items():\n    if count < cutoff:\n        deleted_percept_ids.add(percept_id)\n        # update percept_indices, no need to prune test_data\n        del self.percept_indices[self.percepts[percept_id]]\n\n# update training data\nfor active_percepts in train_data:\n    for percept_id in deleted_percept_ids:\n        active_percepts.pop(percept_id, None)\nreturn", "path": "src\\fei\\model\\edge_filter.py", "repo_name": "summarization/semantic_summ", "stars": 74, "license": "None", "language": "python", "size": 284}
{"docstring": "\"\"\"\noverload operator '*'\nmultiply feature vector with a scaling factor (float)\n\"\"\"\n", "func_signal": "def __mul__(self, scaling):\n", "code": "if not type(scaling) == float: return\nreturn CompositeFeatureVector(self, scaling)", "path": "src\\fei\\model\\feat_vec.py", "repo_name": "summarization/semantic_summ", "stars": 74, "license": "None", "language": "python", "size": 284}
{"docstring": "\"\"\"\nParse xml and convert to a canonical string representation so we don't\nhave to worry about semantically meaningless differences\n\n\"\"\"\n", "func_signal": "def xml_equal(xml_file1, xml_file2):\n", "code": "def canonical(xml_file):\n    # poor man's canonicalization, since we don't want to install\n    # external packages just for unittesting\n    s = et.tostring(et.parse(xml_file).getroot()).decode(\"UTF-8\")\n    s = re.sub(\"[\\n|\\t]*\", \"\", s)\n    s = re.sub(\"\\s+\", \" \", s)\n    s = \"\".join(sorted(s)).strip()\n    return s\n\nreturn canonical(xml_file1) == canonical(xml_file2)", "path": "src\\fei\\eval\\pyrouge\\utils\\file_utils.py", "repo_name": "summarization/semantic_summ", "stars": 74, "license": "None", "language": "python", "size": 284}
{"docstring": "\"\"\"\nget concepts from file\n\"\"\"\n", "func_signal": "def getConcepts(input_file):\n", "code": "concepts = mset() # use Counter for concepts\ncached_indices = {}\n\nwith codecs.open(input_file, 'r', 'utf-8') as infile:\n    for line in infile:\n        line = line.strip()\n\n        if line == '' and cached_indices:\n            cached_indices = {}\n            continue\n            \n        if line.startswith('(') and line.endswith(')'):\n            line = line[1:-1] # remove parentheses\n            triple = re.sub(r'[0-9A-Za-z\\-]+ \\/ ', '', line)\n            \n            try:\n                c1, _, c2 = triple.split(', ')\n                idx1, idx2 = re.findall(r'([0-9A-Za-z]+) \\/', line)\n                \n                if idx1 not in cached_indices:\n                    cached_indices[idx1] = 1\n                    concepts[c1.lower()] += 1\n                    \n                if idx2 not in cached_indices:\n                    cached_indices[idx2] = 1\n                    concepts[c2.lower()] += 1\n                    \n            except ValueError: pass\n            \nreturn concepts", "path": "src\\fei\\preprocess\\stats_jamr.py", "repo_name": "summarization/semantic_summ", "stars": 74, "license": "None", "language": "python", "size": 284}
{"docstring": "\"\"\"\noverload operator '*' (right-hand-side equivalent to __mul__)\nmultiply feature vector with a scaling factor (float)\n\"\"\"\n", "func_signal": "def __rmul__(self, scaling):\n", "code": "if not type(scaling) == float: return\nself.scaling *= scaling\nreturn self", "path": "src\\fei\\model\\feat_vec.py", "repo_name": "summarization/semantic_summ", "stars": 74, "license": "None", "language": "python", "size": 284}
{"docstring": "\"\"\"\ncalculate average number of concepts in summary and document set\n\"\"\"\n", "func_signal": "def getConceptStats(models_concepts, docs_concepts):\n", "code": "num_topics = 0\nnum_uniq_m_concepts = 0.0\nnum_uniq_d_concepts = 0.0\nper_covered_m_concepts = 0.0\n\n# important to iterate through model files\nfor topic in models_concepts:\n    num_topics += 1\n    num_uniq_m_concepts += len(models_concepts[topic])\n    num_uniq_d_concepts += len(docs_concepts[topic])\n    \n    m_concepts = set(models_concepts[topic])\n    d_concepts = set(docs_concepts[topic])\n    intersect = m_concepts & d_concepts\n    per_covered_m_concepts += len(intersect)*100/len(m_concepts)\n    \nnum_uniq_m_concepts /= num_topics\nnum_uniq_d_concepts /= num_topics\nper_covered_m_concepts /= num_topics\n\nresult = '''\n------\nnumber of total files: %d\nnumber of unique concepts per summary: %.1f\nnumber of unique concepts per document set: %.1f\npercentage of covered summary concepts: %.1f%%\n''' % (num_topics, num_uniq_m_concepts, num_uniq_d_concepts, per_covered_m_concepts)\n\nreturn result", "path": "src\\fei\\preprocess\\stats_coverage.py", "repo_name": "summarization/semantic_summ", "stars": 74, "license": "None", "language": "python", "size": 284}
{"docstring": "\"\"\"\noverload operator '*' (right-hand-side equivalent to __mul__)\nmultiply feature vector with a scaling factor (float)\n\"\"\"\n", "func_signal": "def __rmul__(self, scaling):\n", "code": "if not type(scaling) == float: return\nreturn CompositeFeatureVector(self, scaling)", "path": "src\\fei\\model\\feat_vec.py", "repo_name": "summarization/semantic_summ", "stars": 74, "license": "None", "language": "python", "size": 284}
{"docstring": "# Find pivot index\n", "func_signal": "def next_lexographic(n):\n", "code": "idx = len(n) - 2\n\nwhile idx > -1 and n[idx] >= n[idx + 1]:\n    idx -= 1\n\n\nif idx > -1:\n    j = len(n) - 1\n    \n    while j > idx and n[idx] >= n[j]:\n        j -= 1\n    \n    n[idx], n[j] = n[j], n[idx]\n    return n[: idx + 1] + sorted(n[idx + 1:])\n\nelse:\n    zeros = n.count(0) + 1\n    n = sorted(x for x in n if x != 0)\n    return [n[0]] + [0] * zeros + n[1:]", "path": "codeeval\\044_following-integer.py", "repo_name": "ssmehta/programming-challanges", "stars": 81, "license": "None", "language": "python", "size": 473}
{"docstring": "\"\"\"Delete the given variable if it exists in the current transactional block.\"\"\"\n", "func_signal": "def unset(self, var):\n", "code": "if var in self._data:\n    self.set(var, None)", "path": "thumbtack\\simple-database\\simple-database.py", "repo_name": "ssmehta/programming-challanges", "stars": 81, "license": "None", "language": "python", "size": 473}
{"docstring": "\"\"\"\nUse Rabin-Miller algorithm to return True (n is probably prime)\nor False (n is definitely composite).\nhttp://en.wikibooks.org/wiki/Algorithm_Implementation/Mathematics/Primality_Testing\n\"\"\"\n\n", "func_signal": "def is_probable_prime(n, k = 7):\n", "code": "if n < 6:  # assuming n >= 0 in all cases... shortcut small cases here\n    return [False, False, True, True, False, True][n]\n\nelif n & 1 == 0:\n    return False\n\nelse:\n    s, d = 0, n - 1\n\n    while d & 1 == 0:\n        s, d = s + 1, d >> 1\n\n# Use random.randint(2, n - 2) for very large numbers\nfor a in random.sample(range(2, n - 2), min(n - 4, k)):\n    x = pow(a, d, n)\n\n    if x != 1 and x + 1 != n:\n        for r in range(1, s):\n            x = pow(x, 2, n)\n\n            if x == 1:\n                return False  # composite for sure\n            elif x == n - 1:\n                a = 0  # so we know loop didn't continue to end\n                break  # could be strong liar, try another a\n    \n        if a:\n            return False  # composite if we reached end of this loop\n\nreturn True", "path": "hackerrank\\mathematics\\prime-sum\\prime-sum.py", "repo_name": "ssmehta/programming-challanges", "stars": 81, "license": "None", "language": "python", "size": 473}
{"docstring": "\"\"\"Sets a value for the given variable in the database, storing the\nprevious value in the current transaction block if any blocks are open.\nSince no type information is given, we keep the value as a string.\"\"\"\n\n# Update the current transactional block if it exists and if a rollback\n# is not currently being performed\n", "func_signal": "def set(self, var, value, rollback = False):\n", "code": "if not rollback and self._transaction_blocks:\n    if var in self._data:\n        # Record the old variable value only if it is newly changed\n        # since beginning the latest transactional block\n        if var not in self._transaction_blocks[-1]:\n            self._transaction_blocks[-1][var] = self._data[var]\n    else:\n        # Record that the value was previously unset\n        self._transaction_blocks[-1][var] = None\n\n# Set the value in the database\nif value == None:\n    del self._data[var]\nelse:\n    self._data[var] = value", "path": "thumbtack\\simple-database\\simple-database.py", "repo_name": "ssmehta/programming-challanges", "stars": 81, "license": "None", "language": "python", "size": 473}
{"docstring": "\"\"\"\nCalculates the prime factorization of an integer, returning a dictionary of each prime factor\nand its frequency in the factorization.\n\"\"\"\n", "func_signal": "def get_prime_factors(n):\n", "code": "factors = {}\nif n <= 1: return {}\n\nwhile n != 1:\n    if is_prime(n):\n        factors[n] = 1\n        break\n    \n    i = 2\n    while i <= n:\n        j = 0\n        while n % i == 0 and n != 1:\n            j += 1\n            n //= i\n        \n        if j > 0:\n            factors[i] = j\n            break\n        i += 1\n\nreturn factors", "path": "projecteuler\\euler.py", "repo_name": "ssmehta/programming-challanges", "stars": 81, "license": "None", "language": "python", "size": 473}
{"docstring": "# http://stackoverflow.com/a/10733621/406772\n", "func_signal": "def postponed_sieve():\n", "code": "yield 2; yield 3; yield 5; yield 7;\n\ndef add(D, x, s):\n    while x in D:\n        x += s\n    D[x] = s\n\n# ActiveState Recipe 2002\nD = {}\nps = (p for p in postponed_sieve())\np = next(ps) and next(ps)\nq = p * p\nc = 9\n\nwhile True:\n    if c not in D:\n        if c < q:\n            yield c\n        else:\n            add(D, c + 2 * p, 2 * p)\n            p = next(ps)\n            q = p * p\n    else:\n        s = D.pop(c)\n        add(D, c + s, s)\n    c += 2", "path": "codeeval\\063_counting-primes.py", "repo_name": "ssmehta/programming-challanges", "stars": 81, "license": "None", "language": "python", "size": 473}
{"docstring": "\"\"\"Calculate the nth Zeckendorf number from a precalculated set of\nFibonacci numbers.  Given in a function instead for the speedup\nrequired to complete the challenge.\nSee: http://stackoverflow.com/q/11241523/406772\"\"\"\n\n", "func_signal": "def zeckendorf_number(n):\n", "code": "result = 0\n\nfor i in reversed(range(len(FIBONACCI))):\n    if n >= FIBONACCI[i]:\n        result += POWERS[i]\n        n -= FIBONACCI[i]\n\nreturn result", "path": "hackerrank\\mathematics\\chandrima-and-xor\\chandrima-and-xor.py", "repo_name": "ssmehta/programming-challanges", "stars": 81, "license": "None", "language": "python", "size": 473}
{"docstring": "\"\"\"\nThe greatest common divisor function, calculating the largest integer that\ndivides both n and m.\n\"\"\"\n\n", "func_signal": "def gcd(n, m):\n", "code": "d_n, d_m = get_prime_factors(n), get_prime_factors(m)\n\nreturn reduce(op.mul, [k * min(d_n[k], d_m[k]) for k in d_n if k in d_m], 1)", "path": "projecteuler\\euler.py", "repo_name": "ssmehta/programming-challanges", "stars": 81, "license": "None", "language": "python", "size": 473}
{"docstring": "\"\"\"http://stackoverflow.com/a/15391420/406772\"\"\"\n\n", "func_signal": "def isqrt(n):\n", "code": "x = n\ny = (n + 1) // 2\n\nwhile y < x:\n    x = y\n    y = (x + n // x) // 2\n\nreturn x", "path": "hackerrank\\mathematics\\dance-class\\dance-class-v1.py", "repo_name": "ssmehta/programming-challanges", "stars": 81, "license": "None", "language": "python", "size": 473}
{"docstring": "\"\"\"Computes the string similarity using a simplified Z Algorithm:\nhttp://codeforces.com/blog/entry/3107\nhttp://binfalse.de/2010/09/advanced-searching-via-z-algorithm/\n\"\"\"\n\n", "func_signal": "def string_similarity(s):\n", "code": "l, r, n = 0, 0, len(s)\nz = [0] * n\n\nfor i in range(1, n):\n    if i <= r and z[i - l] < r - i + 1:\n        z[i] = z[i - l]\n    \n    else:\n        # Update Z-box bounds\n        l = i\n        if i > r:\n            r = i\n        \n        while r < n and s[r - l] == s[r]:\n            r += 1\n        \n        z[i] = r - l\n        r -= 1\n        \n# Return the total number of string similarities\nreturn n + sum(z)", "path": "hackerrank\\algorithms\\string-similarity\\string-similarity-v2.py", "repo_name": "ssmehta/programming-challanges", "stars": 81, "license": "None", "language": "python", "size": 473}
{"docstring": "# http://stackoverflow.com/a/10733621/406772\n", "func_signal": "def postponed_sieve():\n", "code": "yield 2; yield 3; yield 5; yield 7;\n\ndef add(D, x, s):\n    while x in D:\n        x += s\n    D[x] = s\n\n# ActiveState Recipe 2002\nD = {}\nps = (p for p in postponed_sieve())\np = next(ps) and next(ps)\nq = p * p\nc = 9\n\nwhile True:\n    if c not in D:\n        if c < q:\n            yield c\n        else:\n            add(D, c + 2 * p, 2 * p)\n            p = next(ps)\n            q = p * p\n    else:\n        s = D.pop(c)\n        add(D, c + s, s)\n    c += 2", "path": "projecteuler\\euler.py", "repo_name": "ssmehta/programming-challanges", "stars": 81, "license": "None", "language": "python", "size": 473}
{"docstring": "\"\"\"Returns whether the given integer is prime\"\"\"\n\n", "func_signal": "def is_prime(n):\n", "code": "if n < 2:\n    return False\nelif n == 2 or n == 3 or n == 5:\n    return True\nelif n % 2 == 0 or n % 3 == 0 or n % 5 == 0:\n    return False\n\ni = 6\nsqrt_n = int(math.ceil(math.sqrt(n)))\n\nwhile i <= sqrt_n + 1:\n    if n % (i - 1) == 0 or n % (i + 1) == 0:\n        return False\n    i += 6\nreturn True", "path": "projecteuler\\euler.py", "repo_name": "ssmehta/programming-challanges", "stars": 81, "license": "None", "language": "python", "size": 473}
{"docstring": "\"\"\"Returns whether the given integer is prime\"\"\"\n\n", "func_signal": "def is_prime(n):\n", "code": "if n < 2:\n    return False\nelif n == 2 or n == 3 or n == 5:\n    return True\nelif n % 2 == 0 or n % 3 == 0 or n % 5 == 0:\n    return False\n\ni = 6\nsqrt_n = int(math.ceil(math.sqrt(n)))\n\nwhile i <= sqrt_n + 1:\n    if n % (i - 1) == 0 or n % (i + 1) == 0:\n        return False\n    i += 6\nreturn True", "path": "hackerrank\\mathematics\\identify-smith-numbers\\identify-smith-numbers.py", "repo_name": "ssmehta/programming-challanges", "stars": 81, "license": "None", "language": "python", "size": 473}
{"docstring": "\"\"\"\nCalculates the prime factorization of an integer, returning a dictionary of each prime factor\nand its frequency in the factorization.\n\"\"\"\n", "func_signal": "def get_prime_factors(n):\n", "code": "factors = {}\nif n <= 1: return {}\n\nwhile n != 1:\n    if is_prime(n):\n        factors[n] = 1\n        break\n    \n    i = 2\n    while i <= n:\n        j = 0\n        while n % i == 0 and n != 1:\n            j += 1\n            n //= i\n        \n        if j > 0:\n            factors[i] = j\n            break\n        i += 1\n\nreturn factors", "path": "hackerrank\\mathematics\\identify-smith-numbers\\identify-smith-numbers.py", "repo_name": "ssmehta/programming-challanges", "stars": 81, "license": "None", "language": "python", "size": 473}
{"docstring": "# Compute limited, sorted suffix array and LCP between successive suffixes\n# Followed techniques used by http://stackoverflow.com/a/13574862\n\n", "func_signal": "def string_similarity(s):\n", "code": "suffixes = sorted(s[i:] for i in range(len(s)) if s[i] == s[0])\nlcp = list(map(len, map(os.path.commonprefix, pairwise(suffixes))))\n\nidx = suffixes.index(s)\nsimilarities = len(s)\n\n# Back count\nif idx > 0:\n    similarities += lcp[idx - 1]\n    min_count = lcp[idx - 1]\n    \n    for i in reversed(range(idx - 1)):\n        if lcp[i] < min_count:\n            min_count = lcp[i]\n        similarities += min_count\n\n# Forward count\nif idx < len(lcp):\n    similarities += lcp[idx]\n    min_count = lcp[idx]\n    \n    for i in range(idx + 1, len(lcp)):\n        if lcp[i] < min_count:\n            min_count = lcp[i]\n        similarities += min_count\n\nreturn(similarities)", "path": "hackerrank\\algorithms\\string-similarity\\string-similarity-v1.py", "repo_name": "ssmehta/programming-challanges", "stars": 81, "license": "None", "language": "python", "size": 473}
{"docstring": "# http://stackoverflow.com/a/10733621/406772\n", "func_signal": "def postponed_sieve():\n", "code": "yield 2; yield 3; yield 5; yield 7;\n\ndef add(D, x, s):\n    while x in D:\n        x += s\n    D[x] = s\n\n# ActiveState Recipe 2002\nD = {}\nps = (p for p in postponed_sieve())\np = next(ps) and next(ps)\nq = p * p\nc = 9\n\nwhile True:\n    if c not in D:\n        if c < q:\n            yield c\n        else:\n            add(D, c + 2 * p, 2 * p)\n            p = next(ps)\n            q = p * p\n    else:\n        s = D.pop(c)\n        add(D, c + s, s)\n    c += 2", "path": "codeeval\\004_sum-of-primes.py", "repo_name": "ssmehta/programming-challanges", "stars": 81, "license": "None", "language": "python", "size": 473}
{"docstring": "\"\"\"Returns whether the given integer is prime\"\"\"\n\n", "func_signal": "def is_prime(n):\n", "code": "if n < 2:\n    return False\nelif n == 2 or n == 3:\n    return True\nelif n % 2 == 0 or n % 3 == 0:\n    return False\n\ni = 6\nsqrt_n = int(math.ceil(math.sqrt(n)))\n\nwhile i <= sqrt_n + 1:\n    if n % (i - 1) == 0 or n % (i + 1) == 0:\n        return False\n    i += 6\nreturn True", "path": "codeeval\\003_prime-palindrome.py", "repo_name": "ssmehta/programming-challanges", "stars": 81, "license": "None", "language": "python", "size": 473}
{"docstring": "\"\"\"Revert the changes made since beginning the latest transactional block.\"\"\"\n", "func_signal": "def rollback(self):\n", "code": "if self._transaction_blocks:\n    for k, v in self._transaction_blocks[-1].items():\n        self.set(k, v, rollback = True)\n    \n    self._transaction_blocks.pop()\nelse:\n    return 'NO TRANSACTION'", "path": "thumbtack\\simple-database\\simple-database.py", "repo_name": "ssmehta/programming-challanges", "stars": 81, "license": "None", "language": "python", "size": 473}
{"docstring": "# Remove duplicate characters while preserving order\n", "func_signal": "def substitution(key, alphabet):\n", "code": "key = list(collections.OrderedDict.fromkeys(key))\n\n# Create column structure of substitution key\nsub = [list(key)] + chunks([x for x in alphabet if x not in key], len(key))\n\n# Transpose and join sublists\nsub = sorted(''.join(x) for x in itertools.zip_longest(*sub, fillvalue = ''))\n\n# Return a single string as a substitution key to the given alphabet\nreturn ''.join(sub)", "path": "hackerrank\\algorithms\\keyword-transposition-cipher\\keyword-transposition-cipher.py", "repo_name": "ssmehta/programming-challanges", "stars": 81, "license": "None", "language": "python", "size": 473}
{"docstring": "\"\"\"Input n>=6, Returns a list of primes, 2 <= p < n\nhttp://stackoverflow.com/a/2068548/406772\"\"\"\n\n", "func_signal": "def rwh_primes2(n):\n", "code": "if n < 3:\n    return []\nelif n == 3:\n    return [2]\nelif n < 6:\n    return [2, 3]\n\ncorrection = (n % 6 > 1)\nn = {0: n, 1: n - 1, 2: n + 4, 3: n + 3, 4: n + 2, 5: n + 1}[n % 6]\n\nsieve = [True] * (n // 3)\nsieve[0] = False\n\nfor i in range(int(n**0.5) // 3 + 1):\n    if sieve[i]:\n        k = 3 * i + 1 | 1\n        sieve[((k * k) // 3) :: 2 * k] = [False] * ((n // 6 - (k * k) // 6 - 1) // k + 1)\n        sieve[(k * k + 4 * k - 2 * k * (i & 1)) // 3 :: 2 * k] = [False] * ((n // 6 - (k * k + 4 * k - 2 * k * (i & 1)) // 6 - 1) // k + 1)\n\nreturn [2, 3] + [3 * i + 1 | 1 for i in range(1, n // 3 - correction) if sieve[i]]", "path": "hackerrank\\mathematics\\gcd-product\\gcd-product.py", "repo_name": "ssmehta/programming-challanges", "stars": 81, "license": "None", "language": "python", "size": 473}
{"docstring": "\"\"\"\nReading socket and receiving message from server. Check the CRC32.\n\"\"\"\n", "func_signal": "def recv_message(self):\n", "code": "packet_length_data = self.sock.recv(4)  # reads how many bytes to read\n\nif len(packet_length_data) < 4:\n    raise Exception(\"Nothing in the socket!\")\npacket_length = struct.unpack(\"<I\", packet_length_data)[0]\npacket = self.sock.recv(packet_length - 4)  # read the rest of bytes from socket\n\n# check the CRC32\nif not crc32(packet_length_data + packet[0:-4]) == struct.unpack('<I', packet[-4:])[0]:\n    raise Exception(\"CRC32 was not correct!\")\nx = struct.unpack(\"<I\", packet[:4])\nauth_key_id = packet[4:12]\nif auth_key_id == b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00':\n    # No encryption - Plain text\n    (message_id, message_length) = struct.unpack(\"<8sI\", packet[12:24])\n    data = packet[24:24+message_length]\nelif auth_key_id == self.auth_key_id:\n    message_key = packet[12:28]\n    encrypted_data = packet[28:-4]\n    aes_key, aes_iv = self.aes_calculate(message_key, direction=\"from server\")\n    decrypted_data = crypt.ige_decrypt(encrypted_data, aes_key, aes_iv)\n    assert decrypted_data[0:8] == self.server_salt\n    assert decrypted_data[8:16] == self.session_id\n    message_id = decrypted_data[16:24]\n    seq_no = struct.unpack(\"<I\", decrypted_data[24:28])[0]\n    message_data_length = struct.unpack(\"<I\", decrypted_data[28:32])[0]\n    data = decrypted_data[32:32+message_data_length]\nelse:\n    raise Exception(\"Got unknown auth_key id\")\nreturn data", "path": "mtproto.py", "repo_name": "griganton/telepy_old", "stars": 74, "license": "None", "language": "python", "size": 733}
{"docstring": "# Deal with py2 and py3 differences\n", "func_signal": "def __init__(self):\n", "code": "try: # this only works in py2.7\n  import configparser\nexcept ImportError:\n  import ConfigParser as configparser\nimport mtproto\n\nself._config = configparser.ConfigParser()\n# Check if credentials is correctly loaded (when it doesn't read anything it returns [])\nif not self._config.read('credentials'):\n  print(\"File 'credentials' seems to not exist.\")\n  exit(-1)\nip = self._config.get('App data', 'ip_address')\nport = self._config.getint('App data', 'port')\n\nself._session = mtproto.Session(ip, port)\nself._session.create_auth_key()\n\nself._salt = future_salts = self._session.method_call('get_future_salts', num=3)", "path": "classes\\telepy.py", "repo_name": "griganton/telepy_old", "stars": 74, "license": "None", "language": "python", "size": 733}
{"docstring": "'''\nhistory <peer> [limit]\nprints history (and marks it as read). Default limit = 40\n'''\n", "func_signal": "def do_history(self, peer, limit=40):\n", "code": "if peer is '':\n  print('no peer have specified')\n  return\nargs = peer.split()\nif len(args) not in (1,2) :\n  print('not appropriate number of arguments : ', peer)\n  return\nif len(args) is 2:\n  if not args[1].isdecimal() or int(args[1]) < 1:\n    print('not a valid limit:', args[1])\n  limit = int(args[1])\nprint(peer)\nprint(limit)", "path": "classes\\shell.py", "repo_name": "griganton/telepy_old", "stars": 74, "license": "None", "language": "python", "size": 733}
{"docstring": "'''\nchat_info <chat>\nprints info about chat\n'''\n", "func_signal": "def do_chat_info(self, arg):\n", "code": "arg=arg.split()\nif len(arg) is 1:\n  print ('chat_info called with ', arg[0])", "path": "classes\\shell.py", "repo_name": "griganton/telepy_old", "stars": 74, "license": "None", "language": "python", "size": 733}
{"docstring": "# http://stackoverflow.com/questions/2068372/fastest-way-to-list-all-primes-below-n-in-python/3035188#3035188\n#\"\"\" Input N>=6, Returns a list of primes, 2 <= p < N \"\"\"\n", "func_signal": "def primesbelow(N):\n", "code": "correction = N % 6 > 1\nN = {0:N, 1:N-1, 2:N+4, 3:N+3, 4:N+2, 5:N+1}[N%6]\nsieve = [True] * (N // 3)\nsieve[0] = False\nfor i in range(int(N ** .5) // 3 + 1):\n    if sieve[i]:\n        k = (3 * i + 1) | 1\n        sieve[k*k // 3::2*k] = [False] * ((N//6 - (k*k)//6 - 1)//k + 1)\n        sieve[(k*k + 4*k - 2*k*(i%2)) // 3::2*k] = [False] * ((N // 6 - (k*k + 4*k - 2*k*(i%2))//6 - 1) // k + 1)\nreturn [2, 3] + [(3 * i + 1) | 1 for i in range(1, N//3 - correction) if sieve[i]]", "path": "prime.py", "repo_name": "griganton/telepy_old", "stars": 74, "license": "None", "language": "python", "size": 733}
{"docstring": "\"\"\"\n:type bytes_io: io.BytesIO object\n\"\"\"\n", "func_signal": "def deserialize(bytes_io, type_=None, subtype=None):\n", "code": "assert isinstance(bytes_io, io.BytesIO)\n\n# Built-in bare types\nif   type_ == 'int':    x = struct.unpack('<i', bytes_io.read(4))[0]\nelif type_ == '#':      x = struct.unpack('<I', bytes_io.read(4))[0]\nelif type_ == 'long':   x = struct.unpack('<q', bytes_io.read(8))[0]\nelif type_ == 'double': x = struct.unpack('<d', bytes_io.read(8))[0]\nelif type_ == 'int128': x = bytes_io.read(16)\nelif type_ == 'int256': x = bytes_io.read(32)\nelif type_ == 'string' or type_ == 'bytes':\n    l = struct.unpack('<B', bytes_io.read(1))[0]\n    assert l <= 254  # In general, 0xFF byte is not allowed here\n    if l == 254:\n        # We have a long string\n        long_len = struct.unpack('<I', bytes_io.read(3)+b'\\x00')[0]\n        x = bytes_io.read(long_len)\n        bytes_io.read(-long_len % 4)  # skip padding bytes\n    else:\n        # We have a short string\n        x = bytes_io.read(l)\n        bytes_io.read(-(l+1) % 4)  # skip padding bytes\n    assert isinstance(x, bytes)\nelif type_ == 'vector':\n    assert subtype is not None\n    count = struct.unpack('<l', bytes_io.read(4))[0]\n    x = [deserialize(bytes_io, type_=subtype) for i in range(count)]\nelse:\n    # known types\n    try:\n    # Bare types\n        tl_elem = tl.constructor_type[type_]\n    except KeyError:\n        # Boxed types\n        i = struct.unpack('<i', bytes_io.read(4))[0]  # read type ID\n        try:\n            tl_elem = tl.constructor_id[i]\n        except KeyError:\n            # Unknown type\n            raise Exception(\"Could not extract type: %s\" % type_)\n\n    base_boxed_types = [\"Vector t\", \"Int\", \"Long\", \"Double\", \"String\", \"Int128\", \"Int256\"]\n    if tl_elem.type in base_boxed_types:\n        x = deserialize(bytes_io, type_=tl_elem.predicate, subtype=subtype)\n    else:  # other types\n        x = TLObject(tl_elem)\n        for arg in tl_elem.params:\n            x[arg['name']] = deserialize(bytes_io, type_=arg['type'], subtype=arg['subtype'])\nreturn x", "path": "TL.py", "repo_name": "griganton/telepy_old", "stars": 74, "license": "None", "language": "python", "size": 733}
{"docstring": "# convert first word(command name) to lower and return it as line\n", "func_signal": "def precmd(self, line):\n", "code": "line = line.lstrip()\nblank_pos = line.find(' ')\nif blank_pos < 0: return line.lower()\nreturn line[:blank_pos].lower() + ' ' + line[blank_pos+1:]", "path": "classes\\shell.py", "repo_name": "griganton/telepy_old", "stars": 74, "license": "None", "language": "python", "size": 733}
{"docstring": "# creating socket\n", "func_signal": "def __init__(self, ip, port, auth_key=None, server_salt=None):\n", "code": "self.sock = socket.socket()\nself.sock.connect((ip, port))\nself.number = 0\nself.timedelta = 0\nself.session_id = os.urandom(8)\nself.auth_key = auth_key\nself.auth_key_id = SHA.new(self.auth_key).digest()[-8:] if self.auth_key else None\nself.sock.settimeout(5.0)\nself.MAX_RETRY = 5;\nself.AUTH_MAX_RETRY = 5;", "path": "mtproto.py", "repo_name": "griganton/telepy_old", "stars": 74, "license": "None", "language": "python", "size": 733}
{"docstring": "\"\"\"Given a key, given an iv, and message\n do whatever operation asked in the operation field.\n Operation will be checked for: \"decrypt\" and \"encrypt\" strings.\n Returns the message encrypted/decrypted.\n message must be a multiple by 16 bytes (for division in 16 byte blocks)\n key must be 32 byte\n iv must be 32 byte (it's not internally used in AES 256 ECB, but it's\n needed for IGE)\"\"\"\n", "func_signal": "def ige(message, key, iv, operation=\"decrypt\"):\n", "code": "if type(message) == long:\n    message = number.long_to_bytes(message)\nif type(key) == long:\n    key = number.long_to_bytes(key)\nif type(iv) == long:\n    iv = number.long_to_bytes(iv)\n\nif len(key) != 32:\n    raise ValueError(\"key must be 32 bytes long (was \" +\n                     str(len(key)) + \" bytes)\")\nif len(iv) != 32:\n    raise ValueError(\"iv must be 32 bytes long (was \" +\n                     str(len(iv)) + \" bytes)\")\n\ncipher = AES.new(key, AES.MODE_ECB, iv)\nblocksize = cipher.block_size\nif len(message) % blocksize != 0:\n    raise ValueError(\"message must be a multiple of 16 bytes (try adding \" +\n                    str(16 - len(message) % 16) + \" bytes of padding)\")\n\nivp = iv[0:blocksize]\nivp2 = iv[blocksize:]\n\nciphered = None\n\nfor i in range(0, len(message), blocksize):\n    indata = message[i:i+blocksize]\n    if operation == \"decrypt\":\n        xored = xor_stuff(indata, ivp2)\n        decrypt_xored = cipher.decrypt(xored)\n        outdata = xor_stuff(decrypt_xored, ivp)\n        ivp = indata\n        ivp2 = outdata\n    elif operation == \"encrypt\":\n        xored = xor_stuff(indata, ivp)\n        encrypt_xored = cipher.encrypt(xored)\n        outdata = xor_stuff(encrypt_xored, ivp2)\n        ivp = outdata\n        ivp2 = indata\n    else:\n        raise ValueError(\"operation must be either 'decrypt' or 'encrypt'\")\n\n    if ciphered is None:\n        ciphered = outdata\n    else:\n        ciphered_ba = bytearray(ciphered)\n        ciphered_ba.extend(outdata)\n        if version_info >= MIN_SUPPORTED_PY3_VERSION:\n            ciphered = bytes(ciphered_ba)\n        else:\n            ciphered = str(ciphered_ba)\n\nreturn ciphered", "path": "tests\\ige.py", "repo_name": "griganton/telepy_old", "stars": 74, "license": "None", "language": "python", "size": 733}
{"docstring": "''' read the file as bytes. :return b'' on file not exist '''\n", "func_signal": "def read_bytes(self):\n", "code": "if not exists(self._path): return b''\n# buf = b''\nwith open(self._path, 'r+b') as file:\n  return file.read()\n# return buf", "path": "classes\\file.py", "repo_name": "griganton/telepy_old", "stars": 74, "license": "None", "language": "python", "size": 733}
{"docstring": "\"\"\"Given a str_bytes (so str())  like\ntmp_aes_key_hex = '\\xf0\\x11(\\x08\\x87\\xc7\\xbb\\x01\\xdf\\x0f\\xc4\\xe1x0\\xe0\\xb9\\x1f\\xbb\\x8b\\xe4\\xb2&|\\xb9\\x85\\xae%\\xf3;RrS'\nConvert it back to it's uppercase string representation, like:\ntmp_aes_key_str = \"F011280887C7BB01DF0FC4E17830E0B91FBB8BE4B2267CB985AE25F33B527253\" \"\"\"\n", "func_signal": "def str_bytes_to_hex_string(val):\n", "code": "if version_info >= MIN_SUPPORTED_PY3_VERSION:\n    return str(hexlify(val).upper())\nreturn val.encode(\"hex\").upper()", "path": "tests\\ige.py", "repo_name": "griganton/telepy_old", "stars": 74, "license": "None", "language": "python", "size": 733}
{"docstring": "\"\"\"Given a key, given an iv, and message\n do whatever operation asked in the operation field.\n Operation will be checked for: \"decrypt\" and \"encrypt\" strings.\n Returns the message encrypted/decrypted.\n message must be a multiple by 16 bytes (for division in 16 byte blocks)\n key must be 32 byte\n iv must be 32 byte (it's not internally used in AES 256 ECB, but it's\n needed for IGE)\"\"\"\n", "func_signal": "def _ige(message, key, iv, operation=\"decrypt\"):\n", "code": "message = bytes(message)\nif len(key) != 32:\n    raise ValueError(\"key must be 32 bytes long (was \" +\n                     str(len(key)) + \" bytes)\")\nif len(iv) != 32:\n    raise ValueError(\"iv must be 32 bytes long (was \" +\n                     str(len(iv)) + \" bytes)\")\n\ncipher = AES.new(key, AES.MODE_ECB, iv)\nblocksize = cipher.block_size\n\nif len(message) % blocksize != 0:\n    raise ValueError(\"message must be a multiple of 16 bytes (try adding \" +\n                     str(16 - len(message) % 16) + \" bytes of padding)\")\n\nivp = iv[0:blocksize]\nivp2 = iv[blocksize:]\n\nciphered = bytes()\n\nfor i in range(0, len(message), blocksize):\n    indata = message[i:i+blocksize]\n    if operation == \"decrypt\":\n        xored = strxor(indata, ivp2)\n        decrypt_xored = cipher.decrypt(xored)\n        outdata = strxor(decrypt_xored, ivp)\n        ivp = indata\n        ivp2 = outdata\n    elif operation == \"encrypt\":\n        xored = strxor(indata, ivp)\n        encrypt_xored = cipher.encrypt(xored)\n        outdata = strxor(encrypt_xored, ivp2)\n        ivp = outdata\n        ivp2 = indata\n    else:\n        raise ValueError(\"operation must be either 'decrypt' or 'encrypt'\")\n    ciphered += outdata\nreturn ciphered", "path": "crypt.py", "repo_name": "griganton/telepy_old", "stars": 74, "license": "None", "language": "python", "size": 733}
{"docstring": "\"\"\"\nFunction to visualize byte streams. Split into bytes, print to console.\n:param bs: BYTE STRING\n\"\"\"\n", "func_signal": "def vis(bs):\n", "code": "bs = bytearray(bs)\nsymbols_in_one_line = 8\nn = len(bs) // symbols_in_one_line\ni = 0\nfor i in range(n):\n    print(str(i*symbols_in_one_line)+\" | \"+\" \".join([\"%02X\" % b for b in bs[i*symbols_in_one_line:(i+1)*symbols_in_one_line]])) # for every 8 symbols line\nif not len(bs) % symbols_in_one_line == 0:\n    print(str((i+1)*symbols_in_one_line)+\" | \"+\" \".join([\"%02X\" % b for b in bs[(i+1)*symbols_in_one_line:]])+\"\\n\") # for last line", "path": "mtproto.py", "repo_name": "griganton/telepy_old", "stars": 74, "license": "None", "language": "python", "size": 733}
{"docstring": "\"\"\"\nFunction to visualize byte streams. Split into bytes, print to console.\n:param bs: BYTE STRING\n\"\"\"\n", "func_signal": "def vis(bs):\n", "code": "bs = bytearray(bs)\nsymbols_in_one_line = 8\nn = len(bs) // symbols_in_one_line\nfor i in range(n):\n    print(str(i*symbols_in_one_line)+\" | \"+\" \".join([\"%02X\" % b for b in bs[i*symbols_in_one_line:(i+1)*symbols_in_one_line]])) # for every 8 symbols line\nif not len(bs) % symbols_in_one_line == 0:\n    print(str((i+1)*symbols_in_one_line)+\" | \"+\" \".join([\"%02X\" % b for b in bs[(i+1)*symbols_in_one_line:]])+\"\\n\") # for last line", "path": "tests\\research_decrypt_mode.py", "repo_name": "griganton/telepy_old", "stars": 74, "license": "None", "language": "python", "size": 733}
{"docstring": "'''\ncreate_group_chat <chat topic> <user1> <user2> <user3> ...\ncreates a groupchat with users, use chat_add_user to add more users\n'''\n", "func_signal": "def do_create_group_chat(self, chat_topic, user1, user2, user3):\n", "code": "print(chat_topic)\nprint(user1,user2,user3)\n\npass", "path": "classes\\shell.py", "repo_name": "griganton/telepy_old", "stars": 74, "license": "None", "language": "python", "size": 733}
{"docstring": "\"\"\"XOR applied to every element of a with every element of b.\nDepending on python version and depeding on input some arrangements need to be done.\"\"\"\n", "func_signal": "def xor_stuff(a, b):\n", "code": "if version_info < MIN_SUPPORTED_PY3_VERSION:\n    if len(a) > len(b):\n        return \"\".join([chr(ord(x) ^ ord(y)) for (x, y) in zip(a[:len(b)], b)])\n    else:\n        return \"\".join([chr(ord(x) ^ ord(y)) for (x, y) in zip(a, b[:len(a)])])\nelse:\n    if type(a) == str and type(b) == bytes:# cipher.encrypt returns string\n        return bytes(ord(x) ^ y for x, y in zip(a, b))\n    elif type(a) == bytes and type(b) == str:\n        return bytes(x ^ ord(y) for x, y in zip(a, b))\n    else:\n        return bytes(x ^ y for x, y in zip(a, b))", "path": "tests\\ige.py", "repo_name": "griganton/telepy_old", "stars": 74, "license": "None", "language": "python", "size": 733}
{"docstring": "'''  truncates the file and create new with :param bytes.\n :return number of bytes written'''\n", "func_signal": "def write_bytes(self, bytes):\n", "code": "with open(self._path, 'w+b') as file:\n  return file.write(bytes)", "path": "classes\\file.py", "repo_name": "griganton/telepy_old", "stars": 74, "license": "None", "language": "python", "size": 733}
{"docstring": "\"\"\"\nForming the message frame and sending message to server\n:param message: byte string to send\n\"\"\"\n\n", "func_signal": "def send_message(self, message_data):\n", "code": "message_id = struct.pack('<Q', int((time()+self.timedelta)*2**30)*4)\n\nif self.auth_key is None or self.server_salt is None:\n    # Unencrypted data send\n    message = (b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00' +\n               message_id +\n               struct.pack('<I', len(message_data)) +\n               message_data)\nelse:\n    # Encrypted data send\n    encrypted_data = (self.server_salt +\n                      self.session_id +\n                      message_id +\n                      struct.pack('<II', self.number, len(message_data)) +\n                      message_data)\n    message_key = SHA.new(encrypted_data).digest()[-16:]\n    padding = os.urandom((-len(encrypted_data)) % 16)\n    print(len(encrypted_data+padding))\n    aes_key, aes_iv = self.aes_calculate(message_key)\n\n    message = (self.auth_key_id + message_key +\n               crypt.ige_encrypt(encrypted_data+padding, aes_key, aes_iv))\n\nstep1 = struct.pack('<II', len(message)+12, self.number) + message\nstep2 = step1 + struct.pack('<I', crc32(step1))\nself.sock.send(step2)\nself.number += 1", "path": "mtproto.py", "repo_name": "griganton/telepy_old", "stars": 74, "license": "None", "language": "python", "size": 733}
{"docstring": "''' try to remove the file '''\n", "func_signal": "def remove(self):\n", "code": "try:\n  os.remove(self._path)\nexcept FileNotFoundError: pass", "path": "classes\\file.py", "repo_name": "griganton/telepy_old", "stars": 74, "license": "None", "language": "python", "size": 733}
{"docstring": "# http://en.wikipedia.org/wiki/Miller-Rabin_primality_test#Algorithm_and_running_time\n", "func_signal": "def isprime(n, precision=7):\n", "code": "if n == 1 or n % 2 == 0:\n    return False\nelif n < 1:\n    raise ValueError(\"Out of bounds, first argument must be > 0\")\nelif n < _smallprimeset:\n    return n in smallprimeset\n\n\nd = n - 1\ns = 0\nwhile d % 2 == 0:\n    d //= 2\n    s += 1\n\nfor repeat in range(precision):\n    a = random.randrange(2, n - 2)\n    x = pow(a, d, n)\n\n    if x == 1 or x == n - 1: continue\n\n    for r in range(s - 1):\n        x = pow(x, 2, n)\n        if x == 1: return False\n        if x == n - 1: break\n    else: return False\n\nreturn True", "path": "prime.py", "repo_name": "griganton/telepy_old", "stars": 74, "license": "None", "language": "python", "size": 733}
{"docstring": "\"\"\"Determine whether or not the current function is a CSP\nprocess.\n\"\"\"\n", "func_signal": "def is_process(self, decorators):\n", "code": "for decorator in decorators:\n    if (decorator.name == 'process' or decorator.name == 'forever'):\n        return True\nreturn False", "path": "csp\\lint\\processes.py", "repo_name": "futurecore/python-csp", "stars": 106, "license": "gpl-2.0", "language": "python", "size": 5073}
{"docstring": "\"\"\"Visit function definition.\n\"\"\"\n\n# If this function definition is not a CSP process, ignore it.\n", "func_signal": "def visitFunction(self, node):\n", "code": "if (node.decorators is None or\n    self.is_process(node.decorators) is None):            \n    return\n\n# Store useful information about this process.\nself.current_process = node.name\nself.current_process_lineno = node.lineno\n\n# 'I001':'Function is a CSP process or server process',\nexstatic.cspwarnings.create_error(self.filename,\n                                  self.current_process_lineno,\n                                  self.current_process,\n                                  'I001')\n\n# 'W004':'@process or @forever applied to method (rather than function)'\nif 'self' in node.argnames:\n    exstatic.cspwarnings.create_error(self.filename,\n                                      self.current_process_lineno,\n                                      self.current_process,\n                                      'W004')\n\nreturn", "path": "csp\\lint\\processes.py", "repo_name": "futurecore/python-csp", "stars": 106, "license": "gpl-2.0", "language": "python", "size": 5073}
{"docstring": "\"\"\"Convert dict of annotations to xml.\n\"\"\"\n", "func_signal": "def _annote2xml(self):\n", "code": "x = '<annote lineno=' + str(self.lineno) + ' '\nfor key, val in self.annote.values():\n    x+= str(key) + '=' + str(val) + ' '\nreturn x + '/>\\n'", "path": "exstatic\\icode.py", "repo_name": "futurecore/python-csp", "stars": 106, "license": "gpl-2.0", "language": "python", "size": 5073}
{"docstring": "\"\"\"\nreadset =\nwriteset = cout\n\"\"\"\n", "func_signal": "def send100(cout):\n", "code": "for i in range(100):\n    print('send100() is sending {0}'.format(i))\n    cout.write(i)\nreturn", "path": "test\\testcsp.py", "repo_name": "futurecore/python-csp", "stars": 106, "license": "gpl-2.0", "language": "python", "size": 5073}
{"docstring": "\"\"\"\nreadset = self.chan\nwriteset =\n\"\"\"\n", "func_signal": "def send(self, msg):\n", "code": "self.chan.write(msg)\nreturn", "path": "test\\testcsp.py", "repo_name": "futurecore/python-csp", "stars": 106, "license": "gpl-2.0", "language": "python", "size": 5073}
{"docstring": "\"\"\"\nreadset = cin1, cin2, cin3\nwriteset =\n\"\"\"\n", "func_signal": "def testAltRRep(cin1, cin2, cin3):\n", "code": "gen = Alt(cin1, cin2, cin3) * 3\nprint(next(gen))\nprint(next(gen))\nprint(next(gen))\nreturn", "path": "test\\testcsp.py", "repo_name": "futurecore/python-csp", "stars": 106, "license": "gpl-2.0", "language": "python", "size": 5073}
{"docstring": "\"\"\"\nreadset =\nwriteset = cout\n\"\"\"\n", "func_signal": "def sendAlt(cout, num):\n", "code": "t = Timer()\nt.sleep(1)\ncout.write(num)\nreturn", "path": "test\\testcsp.py", "repo_name": "futurecore/python-csp", "stars": 106, "license": "gpl-2.0", "language": "python", "size": 5073}
{"docstring": "\"\"\"\nreadset = flibble, foo\nwriteset = outchan, foo\n\"\"\"\n", "func_signal": "def fact(outchan):\n", "code": "n = 1\nf = 1\nwhile True:\n    if n == 1:\n        outchan.write(1)\n    else:\n        f = reduce(operator.mul, list(range(1, n)))\n        outchan.write(f)\n    n += 1\n    yield", "path": "test\\test_forever.py", "repo_name": "futurecore/python-csp", "stars": 106, "license": "gpl-2.0", "language": "python", "size": 5073}
{"docstring": "\"\"\"\n@param lineno line number\n@param annote dictionary of annotations\n\"\"\"\n", "func_signal": "def __init__(self, lineno, annote):\n", "code": "self.annote = annote\nself.lineno = lineno\nreturn", "path": "exstatic\\icode.py", "repo_name": "futurecore/python-csp", "stars": 106, "license": "gpl-2.0", "language": "python", "size": 5073}
{"docstring": "\"\"\"\nreadset =\nwriteset = cout\n\"\"\"\n", "func_signal": "def ParcalculateRowColumnProduct(cout, A, row, B, col):\n", "code": "product = 0\nfor i in range(len(A[row])):\n    product  += A[row][i] * B[i][col]\ncout.write((row,col,product))", "path": "examples\\matrix\\matrix.py", "repo_name": "futurecore/python-csp", "stars": 106, "license": "gpl-2.0", "language": "python", "size": 5073}
{"docstring": "\"\"\"\nreadset = cin1, cin2, cin3\nwriteset =\n\"\"\"\n# For obvious reasons, SKIP cannot go first \n", "func_signal": "def testAlt3(cin1, cin2, cin3):\n", "code": "alt = Alt(cin1, cin2, cin3, Skip())\nnumeric = 0\nwhile numeric < 3:\n    print('*** TestAlt3 selecting...')        \n    val = alt.pri_select()\n    if isinstance(val, int): numeric +=1\n    print('* Got this from Alt:' + str(val))\nreturn", "path": "test\\testcsp.py", "repo_name": "futurecore/python-csp", "stars": 106, "license": "gpl-2.0", "language": "python", "size": 5073}
{"docstring": "\"\"\"\nreadset = cin1, cin2, cin3\nwriteset =\n\"\"\"\n", "func_signal": "def testAltLRep(cin1, cin2, cin3):\n", "code": "gen = 3 * Alt(cin1, cin2, cin3)\nprint(next(gen))\nprint(next(gen))\nprint(next(gen))\nreturn", "path": "test\\testcsp.py", "repo_name": "futurecore/python-csp", "stars": 106, "license": "gpl-2.0", "language": "python", "size": 5073}
{"docstring": "\"\"\"\nreadset = cin1, cin2, cin3\nwriteset =\n\"\"\"\n", "func_signal": "def testAlt2(cin1, cin2, cin3):\n", "code": "alt = Alt(Skip(), cin1, cin2, cin3)\nnumeric = 0 \nwhile numeric < 3:\n    print('*** TestAlt2 selecting...')\n    val = alt.select()\n    if isinstance(val, int): numeric +=1\n    print('* Got this from Alt:' + str(val))\nreturn", "path": "test\\testcsp.py", "repo_name": "futurecore/python-csp", "stars": 106, "license": "gpl-2.0", "language": "python", "size": 5073}
{"docstring": "\"\"\"\nreadset = cin\nwriteset =\n\"\"\"\n", "func_signal": "def recv100(cin):\n", "code": "for i in range(100):\n    data = cin.read()\n    print('recv100() has received {0}'.format(str(data)))\nreturn", "path": "test\\testcsp.py", "repo_name": "futurecore/python-csp", "stars": 106, "license": "gpl-2.0", "language": "python", "size": 5073}
{"docstring": "\"\"\"\nreadset = cin\nwriteset =\n\"\"\"\n", "func_signal": "def recv(cin):\n", "code": "for i in range(5):\n    data = cin.read()\n    print('recv() has received {0}'.format(str(data)))\nreturn", "path": "test\\testcsp.py", "repo_name": "futurecore/python-csp", "stars": 106, "license": "gpl-2.0", "language": "python", "size": 5073}
{"docstring": "\"\"\"\nreadset =\nwriteset = cout\n\"\"\"\n", "func_signal": "def send(cout):\n", "code": "for i in range(5):\n    print('send() is sending {0}'.format(i))\n    cout.write(i)\nreturn", "path": "test\\testcsp.py", "repo_name": "futurecore/python-csp", "stars": 106, "license": "gpl-2.0", "language": "python", "size": 5073}
{"docstring": "\"\"\"\nreadset = self.chan\nwriteset =\n\"\"\"\n", "func_signal": "def recv(self):\n", "code": "print(self.chan.read())\nreturn", "path": "test\\testcsp.py", "repo_name": "futurecore/python-csp", "stars": 106, "license": "gpl-2.0", "language": "python", "size": 5073}
{"docstring": "\"\"\"\nreadset = cin\nwriteset =\n\"\"\"\n", "func_signal": "def testAlt1(cin):\n", "code": "alt = Alt(cin)\nnumeric = 0 \nwhile numeric < 1:\n    print('*** TestAlt1 selecting...')\n    val = alt.select()\n    if isinstance(val, int): numeric += 1 \n    print('* Got this from Alt:' + str(val))\nreturn", "path": "test\\testcsp.py", "repo_name": "futurecore/python-csp", "stars": 106, "license": "gpl-2.0", "language": "python", "size": 5073}
{"docstring": "\"\"\"\nreadset = cin1, cin2, cin3\nwriteset =\n\"\"\"\n", "func_signal": "def testAlt4(cin1, cin2, cin3):\n", "code": "alt = Alt(Skip(), cin1, cin2, cin3)\nnumeric = 0\nwhile numeric < 3:\n    print('*** TestAlt4 selecting...')        \n    val = alt.fair_select()\n    if isinstance(val, int): numeric +=1\n    print('* Got this from Alt:' + str(val))\nreturn", "path": "test\\testcsp.py", "repo_name": "futurecore/python-csp", "stars": 106, "license": "gpl-2.0", "language": "python", "size": 5073}
{"docstring": "\"\"\"\nreadset = cin1, cin2\nwriteset =\n\"\"\"\n", "func_signal": "def testOr(cin1, cin2):\n", "code": "print(cin1 | cin2)\nprint(cin1 | cin2)\nreturn", "path": "test\\testcsp.py", "repo_name": "futurecore/python-csp", "stars": 106, "license": "gpl-2.0", "language": "python", "size": 5073}
{"docstring": "\"\"\" add fonts part to root\n    return {font.crc => index}\n\"\"\"\n\n", "func_signal": "def _write_fonts(self):\n", "code": "fonts = SubElement(self._root, 'fonts')\n\n# default\nfont_node = SubElement(fonts, 'font')\nSubElement(font_node, 'sz', {'val':'11'})\nSubElement(font_node, 'color', {'theme':'1'})\nSubElement(font_node, 'name', {'val':'Calibri'})\nSubElement(font_node, 'family', {'val':'2'})\nSubElement(font_node, 'scheme', {'val':'minor'})\n\n# others\ntable = {}\nindex = 1\nfor st in self._style_list:\n    if hash(st.font) != hash(style.DEFAULTS.font) and hash(st.font) not in table:\n        table[hash(st.font)] = str(index)\n        font_node = SubElement(fonts, 'font')\n        SubElement(font_node, 'sz', {'val':str(st.font.size)})\n        SubElement(font_node, 'color', {'rgb':str(st.font.color.index)})\n        SubElement(font_node, 'name', {'val':st.font.name})\n        SubElement(font_node, 'family', {'val':'2'})\n        # Don't write the 'scheme' element because it appears to prevent\n        # the font name from being applied in Excel.\n        #SubElement(font_node, 'scheme', {'val':'minor'})\n        if st.font.bold:\n            SubElement(font_node, 'b')\n        if st.font.italic:\n            SubElement(font_node, 'i')\n        if st.font.underline == 'single':\n            SubElement(font_node, 'u')\n        else:\n            SubElement(font_node, 'u', {'val':st.font.underline})\n\n        index += 1\n\nfonts.attrib[\"count\"] = str(index)\nreturn table", "path": "dataproxy\\vendor\\openpyxl-1.5.7\\openpyxl\\writer\\styles.py", "repo_name": "okfn/dataproxy", "stars": 74, "license": "other", "language": "python", "size": 779}
{"docstring": "# print >> self.logfile, \"_locate_stream\", base, sec_size, start_sid, size\n", "func_signal": "def _locate_stream(self, mem, base, sat, sec_size, start_sid, size):\n", "code": "s = start_sid\nif s < 0:\n    raise CompDocError(\"_locate_stream: start_sid (%d) is -ve\" % start_sid)\np = -99 # dummy previous SID\nstart_pos = -9999\nend_pos = -8888\nslices = []\nwhile s >= 0:\n    if s == p+1:\n        # contiguous sectors\n        end_pos += sec_size\n    else:\n        # start new slice\n        if p >= 0:\n            # not first time\n            slices.append((start_pos, end_pos))\n        start_pos = base + s * sec_size\n        end_pos = start_pos + sec_size\n    p = s\n    s = sat[s]\nassert s == EOCSID\n# print >> self.logfile, len(slices) + 1, \"slices\"\nif not slices:\n    # The stream is contiguous ... just what we like!\n    return (mem, start_pos, size)\nslices.append((start_pos, end_pos))\nreturn (''.join([mem[start_pos:end_pos] for start_pos, end_pos in slices]), 0, size)", "path": "dataproxy\\xlrd\\compdoc.py", "repo_name": "okfn/dataproxy", "stars": 74, "license": "other", "language": "python", "size": 779}
{"docstring": "'''od.popitem() -> (k, v), return and remove a (key, value) pair.\nPairs are returned in LIFO order if last is true or FIFO order if false.\n\n'''\n", "func_signal": "def popitem(self, last=True):\n", "code": "if not self:\n    raise KeyError('dictionary is empty')\nroot = self.__root\nif last:\n    link = root[0]\n    link_prev = link[0]\n    link_prev[1] = root\n    root[0] = link_prev\nelse:\n    link = root[1]\n    link_next = link[1]\n    root[1] = link_next\n    link_next[0] = root\nkey = link[2]\ndel self.__map[key]\nvalue = dict.pop(self, key)\nreturn key, value", "path": "dataproxy\\vendor\\openpyxl-1.5.7\\openpyxl\\shared\\compat\\odict.py", "repo_name": "okfn/dataproxy", "stars": 74, "license": "other", "language": "python", "size": 779}
{"docstring": "\"\"\"Create a password hash from a given string.\n\nThis method is based on the algorithm provided by\nDaniel Rentz of OpenOffice and the PEAR package\nSpreadsheet_Excel_Writer by Xavier Noguer <xnoguer@rezebra.com>.\n\n\"\"\"\n", "func_signal": "def hash_password(plaintext_password=''):\n", "code": "password = 0x0000\ni = 1\nfor char in plaintext_password:\n    value = ord(char) << i\n    rotated_bits = value >> 15\n    value &= 0x7fff\n    password ^= (value | rotated_bits)\n    i += 1\npassword ^= len(plaintext_password)\npassword ^= 0xCE4B\nreturn str(hex(password)).upper()[2:]", "path": "dataproxy\\vendor\\openpyxl-1.5.7\\openpyxl\\shared\\password_hasher.py", "repo_name": "okfn/dataproxy", "stars": 74, "license": "other", "language": "python", "size": 779}
{"docstring": "\"\"\" write styles combinations based on ids found in tables \"\"\"\n\n# writing the cellXfs\n", "func_signal": "def _write_cell_xfs(self, number_format_table, fonts_table, fills_table, borders_table):\n", "code": "cell_xfs = SubElement(self._root, 'cellXfs',\n    {'count':'%d' % (len(self._style_list) + 1)})\n\n# default\ndef _get_default_vals():\n    return dict(numFmtId='0', fontId='0', fillId='0',\n        xfId='0', borderId='0')\n\nSubElement(cell_xfs, 'xf', _get_default_vals())\n\nfor st in self._style_list:\n    vals = _get_default_vals()\n\n    if hash(st.font) != hash(style.DEFAULTS.font):\n        vals['fontId'] = fonts_table[hash(st.font)]\n        vals['applyFont'] = '1'\n\n    if hash(st.borders) != hash(style.DEFAULTS.borders):\n        vals['borderId'] = borders_table[hash(st.borders)]\n        vals['applyBorder'] = '1'\n\n    if hash(st.fill) != hash(style.DEFAULTS.fill):\n        vals['fillId'] = fills_table[hash(st.fill)]\n        vals['applyFillId'] = '1'\n\n    if st.number_format != style.DEFAULTS.number_format:\n        vals['numFmtId'] = '%d' % number_format_table[st.number_format]\n        vals['applyNumberFormat'] = '1'\n\n    if hash(st.alignment) != hash(style.DEFAULTS.alignment):\n        vals['applyAlignment'] = '1'\n\n    node = SubElement(cell_xfs, 'xf', vals)\n\n    if hash(st.alignment) != hash(style.DEFAULTS.alignment):\n        alignments = {}\n\n        for align_attr in ['horizontal', 'vertical']:\n            if hash(getattr(st.alignment, align_attr)) != hash(getattr(style.DEFAULTS.alignment, align_attr)):\n                alignments[align_attr] = getattr(st.alignment, align_attr)\n\n            if hash(st.alignment.wrap_text) != hash(style.DEFAULTS.alignment.wrap_text):\n                alignments['wrapText'] = '1'\n            \n            if st.alignment.text_rotation != 0: \n                alignments['textRotation'] = '%s' % st.alignment.text_rotation\n\n        SubElement(node, 'alignment', alignments)", "path": "dataproxy\\vendor\\openpyxl-1.5.7\\openpyxl\\writer\\styles.py", "repo_name": "okfn/dataproxy", "stars": 74, "license": "other", "language": "python", "size": 779}
{"docstring": "'''od.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\n'''\n", "func_signal": "def pop(self, key, default=__marker):\n", "code": "if key in self:\n    result = self[key]\n    del self[key]\n    return result\nif default is self.__marker:\n    raise KeyError(key)\nreturn default", "path": "dataproxy\\vendor\\openpyxl-1.5.7\\openpyxl\\shared\\compat\\odict.py", "repo_name": "okfn/dataproxy", "stars": 74, "license": "other", "language": "python", "size": 779}
{"docstring": "# move to info\n", "func_signal": "def validate(self, res):\n", "code": "if res.year is not None:\n    res.year = self.convertyear(res.year)\nif res.tzoffset == 0 and not res.tzname or res.tzname == 'Z':\n    res.tzname = \"UTC\"\n    res.tzoffset = 0\nelif res.tzoffset != 0 and res.tzname and self.utczone(res.tzname):\n    res.tzoffset = 0\nreturn True", "path": "dataproxy\\vendor\\python-dateutil-1.5\\dateutil\\parser.py", "repo_name": "okfn/dataproxy", "stars": 74, "license": "other", "language": "python", "size": 779}
{"docstring": "\"\"\"Parse a I[.F] seconds value into (seconds, microseconds).\"\"\"\n", "func_signal": "def _parsems(value):\n", "code": "if \".\" not in value:\n    return int(value), 0\nelse:\n    i, f = value.split(\".\")\n    return int(i), int(f.ljust(6, \"0\")[:6])", "path": "dataproxy\\vendor\\python-dateutil-1.5\\dateutil\\parser.py", "repo_name": "okfn/dataproxy", "stars": 74, "license": "other", "language": "python", "size": 779}
{"docstring": "\"\"\" return shape coordinates in percentages (left, top, right, bottom) \"\"\"\n\n", "func_signal": "def get_coordinates(self):\n", "code": "(x1, y1), (x2, y2) = self.coordinates\n\ndrawing_width = pixels_to_EMU(self._chart.drawing.width)\ndrawing_height = pixels_to_EMU(self._chart.drawing.height)\nplot_width = drawing_width * self._chart.width\nplot_height = drawing_height * self._chart.height\n\nmargin_left = self._chart._get_margin_left() * drawing_width\nxunit = plot_width / self._chart.get_x_units()\n\nmargin_top = self._chart._get_margin_top() * drawing_height\nyunit = self._chart.get_y_units()\n\nx_start = (margin_left + (float(x1) * xunit)) / drawing_width\ny_start = (margin_top + plot_height - (float(y1) * yunit)) / drawing_height\n\nx_end = (margin_left + (float(x2) * xunit)) / drawing_width\ny_end = (margin_top + plot_height - (float(y2) * yunit)) / drawing_height\n\ndef _norm_pct(pct):\n    \"\"\" force shapes to appear by truncating too large sizes \"\"\"\n    if pct > 1: pct = 1\n    elif pct < 0: pct = 0\n    return pct\n\n# allow user to specify y's in whatever order\n# excel expect y_end to be lower\nif y_end < y_start:\n    y_end, y_start = y_start, y_end\n\nreturn (_norm_pct(x_start), _norm_pct(y_start),\n    _norm_pct(x_end), _norm_pct(y_end))", "path": "dataproxy\\vendor\\openpyxl-1.5.7\\openpyxl\\drawing.py", "repo_name": "okfn/dataproxy", "stars": 74, "license": "other", "language": "python", "size": 779}
{"docstring": "'''OD.fromkeys(S[, v]) -> New ordered dictionary with keys from S\nand values equal to v (which defaults to None).\n\n'''\n", "func_signal": "def fromkeys(cls, iterable, value=None):\n", "code": "d = cls()\nfor key in iterable:\n    d[key] = value\nreturn d", "path": "dataproxy\\vendor\\openpyxl-1.5.7\\openpyxl\\shared\\compat\\odict.py", "repo_name": "okfn/dataproxy", "stars": 74, "license": "other", "language": "python", "size": 779}
{"docstring": "# dent is the 128-byte directory entry\n", "func_signal": "def __init__(self, DID, dent, DEBUG=0):\n", "code": "self.DID = DID\n# (cbufsize, self.etype, self.colour, self.left_DID, self.right_DID,\n# self.root_DID,\n# self.first_SID,\n# self.tot_size) = \\\n#     unpack('<HBBiii16x4x8x8xii4x', dent[64:128])\n(cbufsize, self.etype, self.colour, self.left_DID, self.right_DID,\nself.root_DID) = \\\n    unpack('<HBBiii', dent[64:80])\n(self.first_SID, self.tot_size) = \\\n    unpack('<ii', dent[116:124])\nif cbufsize == 0:\n    self.name = u''\nelse:\n    self.name = unicode(dent[0:cbufsize-2], 'utf_16_le') # omit the trailing U+0000\nself.children = [] # filled in later\nself.parent = -1 # indicates orphan; fixed up later\nself.tsinfo = unpack('<IIII', dent[100:116])\nif DEBUG:\n    self.dump(DEBUG)", "path": "dataproxy\\xlrd\\compdoc.py", "repo_name": "okfn/dataproxy", "stars": 74, "license": "other", "language": "python", "size": 779}
{"docstring": "\"\"\"Write the theme xml.\"\"\"\n", "func_signal": "def write_theme():\n", "code": "xml_node = fromstring(\n        '<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\\n'\n\n        '<a:theme xmlns:a=\"http://schemas.openxmlformats.org/'\n            'drawingml/2006/main\" name=\"Office Theme\">'\n        '<a:themeElements>'\n\n        '<a:clrScheme name=\"Office\">'\n        '<a:dk1><a:sysClr val=\"windowText\" lastClr=\"000000\"/></a:dk1>'\n        '<a:lt1><a:sysClr val=\"window\" lastClr=\"FFFFFF\"/></a:lt1>'\n        '<a:dk2><a:srgbClr val=\"1F497D\"/></a:dk2>'\n        '<a:lt2><a:srgbClr val=\"EEECE1\"/></a:lt2>'\n        '<a:accent1><a:srgbClr val=\"4F81BD\"/></a:accent1>'\n        '<a:accent2><a:srgbClr val=\"C0504D\"/></a:accent2>'\n        '<a:accent3><a:srgbClr val=\"9BBB59\"/></a:accent3>'\n        '<a:accent4><a:srgbClr val=\"8064A2\"/></a:accent4>'\n        '<a:accent5><a:srgbClr val=\"4BACC6\"/></a:accent5>'\n        '<a:accent6><a:srgbClr val=\"F79646\"/></a:accent6>'\n        '<a:hlink><a:srgbClr val=\"0000FF\"/></a:hlink>'\n        '<a:folHlink><a:srgbClr val=\"800080\"/></a:folHlink>'\n        '</a:clrScheme>'\n\n        '<a:fontScheme name=\"Office\">'\n        '<a:majorFont>'\n        '<a:latin typeface=\"Cambria\"/>'\n        '<a:ea typeface=\"\"/>'\n        '<a:cs typeface=\"\"/>'\n        '<a:font script=\"Jpan\" typeface=\"\uff2d\uff33 \uff30\u30b4\u30b7\u30c3\u30af\"/>'\n        '<a:font script=\"Hang\" typeface=\"\ub9d1\uc740 \uace0\ub515\"/>'\n        '<a:font script=\"Hans\" typeface=\"\u5b8b\u4f53\"/>'\n        '<a:font script=\"Hant\" typeface=\"\u65b0\u7d30\u660e\u9ad4\"/>'\n        '<a:font script=\"Arab\" typeface=\"Times New Roman\"/>'\n        '<a:font script=\"Hebr\" typeface=\"Times New Roman\"/>'\n        '<a:font script=\"Thai\" typeface=\"Tahoma\"/>'\n        '<a:font script=\"Ethi\" typeface=\"Nyala\"/>'\n        '<a:font script=\"Beng\" typeface=\"Vrinda\"/>'\n        '<a:font script=\"Gujr\" typeface=\"Shruti\"/>'\n        '<a:font script=\"Khmr\" typeface=\"MoolBoran\"/>'\n        '<a:font script=\"Knda\" typeface=\"Tunga\"/>'\n        '<a:font script=\"Guru\" typeface=\"Raavi\"/>'\n        '<a:font script=\"Cans\" typeface=\"Euphemia\"/>'\n        '<a:font script=\"Cher\" typeface=\"Plantagenet Cherokee\"/>'\n        '<a:font script=\"Yiii\" typeface=\"Microsoft Yi Baiti\"/>'\n        '<a:font script=\"Tibt\" typeface=\"Microsoft Himalaya\"/>'\n        '<a:font script=\"Thaa\" typeface=\"MV Boli\"/>'\n        '<a:font script=\"Deva\" typeface=\"Mangal\"/>'\n        '<a:font script=\"Telu\" typeface=\"Gautami\"/>'\n        '<a:font script=\"Taml\" typeface=\"Latha\"/>'\n        '<a:font script=\"Syrc\" typeface=\"Estrangelo Edessa\"/>'\n        '<a:font script=\"Orya\" typeface=\"Kalinga\"/>'\n        '<a:font script=\"Mlym\" typeface=\"Kartika\"/>'\n        '<a:font script=\"Laoo\" typeface=\"DokChampa\"/>'\n        '<a:font script=\"Sinh\" typeface=\"Iskoola Pota\"/>'\n        '<a:font script=\"Mong\" typeface=\"Mongolian Baiti\"/>'\n        '<a:font script=\"Viet\" typeface=\"Times New Roman\"/>'\n        '<a:font script=\"Uigh\" typeface=\"Microsoft Uighur\"/>'\n        '</a:majorFont>'\n        '<a:minorFont>'\n        '<a:latin typeface=\"Calibri\"/>'\n        '<a:ea typeface=\"\"/>'\n        '<a:cs typeface=\"\"/>'\n        '<a:font script=\"Jpan\" typeface=\"\uff2d\uff33 \uff30\u30b4\u30b7\u30c3\u30af\"/>'\n        '<a:font script=\"Hang\" typeface=\"\ub9d1\uc740 \uace0\ub515\"/>'\n        '<a:font script=\"Hans\" typeface=\"\u5b8b\u4f53\"/>'\n        '<a:font script=\"Hant\" typeface=\"\u65b0\u7d30\u660e\u9ad4\"/>'\n        '<a:font script=\"Arab\" typeface=\"Arial\"/>'\n        '<a:font script=\"Hebr\" typeface=\"Arial\"/>'\n        '<a:font script=\"Thai\" typeface=\"Tahoma\"/>'\n        '<a:font script=\"Ethi\" typeface=\"Nyala\"/>'\n        '<a:font script=\"Beng\" typeface=\"Vrinda\"/>'\n        '<a:font script=\"Gujr\" typeface=\"Shruti\"/>'\n        '<a:font script=\"Khmr\" typeface=\"DaunPenh\"/>'\n        '<a:font script=\"Knda\" typeface=\"Tunga\"/>'\n        '<a:font script=\"Guru\" typeface=\"Raavi\"/>'\n        '<a:font script=\"Cans\" typeface=\"Euphemia\"/>'\n        '<a:font script=\"Cher\" typeface=\"Plantagenet Cherokee\"/>'\n        '<a:font script=\"Yiii\" typeface=\"Microsoft Yi Baiti\"/>'\n        '<a:font script=\"Tibt\" typeface=\"Microsoft Himalaya\"/>'\n        '<a:font script=\"Thaa\" typeface=\"MV Boli\"/>'\n        '<a:font script=\"Deva\" typeface=\"Mangal\"/>'\n        '<a:font script=\"Telu\" typeface=\"Gautami\"/>'\n        '<a:font script=\"Taml\" typeface=\"Latha\"/>'\n        '<a:font script=\"Syrc\" typeface=\"Estrangelo Edessa\"/>'\n        '<a:font script=\"Orya\" typeface=\"Kalinga\"/>'\n        '<a:font script=\"Mlym\" typeface=\"Kartika\"/>'\n        '<a:font script=\"Laoo\" typeface=\"DokChampa\"/>'\n        '<a:font script=\"Sinh\" typeface=\"Iskoola Pota\"/>'\n        '<a:font script=\"Mong\" typeface=\"Mongolian Baiti\"/>'\n        '<a:font script=\"Viet\" typeface=\"Arial\"/>'\n        '<a:font script=\"Uigh\" typeface=\"Microsoft Uighur\"/>'\n        '</a:minorFont>'\n        '</a:fontScheme>'\n\n        '<a:fmtScheme name=\"Office\">'\n        '<a:fillStyleLst>'\n        '<a:solidFill><a:schemeClr val=\"phClr\"/></a:solidFill>'\n        '<a:gradFill rotWithShape=\"1\"><a:gsLst>'\n        '<a:gs pos=\"0\"><a:schemeClr val=\"phClr\"><a:tint val=\"50000\"/>'\n        '<a:satMod val=\"300000\"/></a:schemeClr></a:gs>'\n        '<a:gs pos=\"35000\"><a:schemeClr val=\"phClr\"><a:tint val=\"37000\"/>'\n        '<a:satMod val=\"300000\"/></a:schemeClr></a:gs>'\n        '<a:gs pos=\"100000\"><a:schemeClr val=\"phClr\"><a:tint val=\"15000\"/>'\n        '<a:satMod val=\"350000\"/></a:schemeClr></a:gs></a:gsLst>'\n        '<a:lin ang=\"16200000\" scaled=\"1\"/></a:gradFill>'\n        '<a:gradFill rotWithShape=\"1\"><a:gsLst>'\n        '<a:gs pos=\"0\"><a:schemeClr val=\"phClr\"><a:shade val=\"51000\"/>'\n        '<a:satMod val=\"130000\"/></a:schemeClr></a:gs>'\n        '<a:gs pos=\"80000\"><a:schemeClr val=\"phClr\"><a:shade val=\"93000\"/>'\n        '<a:satMod val=\"130000\"/></a:schemeClr></a:gs>'\n        '<a:gs pos=\"100000\"><a:schemeClr val=\"phClr\">'\n        '<a:shade val=\"94000\"/>'\n        '<a:satMod val=\"135000\"/></a:schemeClr></a:gs></a:gsLst>'\n        '<a:lin ang=\"16200000\" scaled=\"0\"/></a:gradFill></a:fillStyleLst>'\n        '<a:lnStyleLst>'\n        '<a:ln w=\"9525\" cap=\"flat\" cmpd=\"sng\" algn=\"ctr\">'\n        '<a:solidFill><a:schemeClr val=\"phClr\"><a:shade val=\"95000\"/>'\n        '<a:satMod val=\"105000\"/></a:schemeClr></a:solidFill>'\n        '<a:prstDash val=\"solid\"/></a:ln>'\n        '<a:ln w=\"25400\" cap=\"flat\" cmpd=\"sng\" algn=\"ctr\"><a:solidFill>'\n        '<a:schemeClr val=\"phClr\"/></a:solidFill>'\n        '<a:prstDash val=\"solid\"/></a:ln>'\n        '<a:ln w=\"38100\" cap=\"flat\" cmpd=\"sng\" algn=\"ctr\"><a:solidFill>'\n        '<a:schemeClr val=\"phClr\"/></a:solidFill>'\n        '<a:prstDash val=\"solid\"/></a:ln></a:lnStyleLst>'\n        '<a:effectStyleLst><a:effectStyle><a:effectLst>'\n        '<a:outerShdw blurRad=\"40000\" dist=\"20000\" dir=\"5400000\" '\n            'rotWithShape=\"0\"><a:srgbClr val=\"000000\">'\n        '<a:alpha val=\"38000\"/></a:srgbClr></a:outerShdw></a:effectLst>'\n        '</a:effectStyle><a:effectStyle><a:effectLst>'\n        '<a:outerShdw blurRad=\"40000\" dist=\"23000\" dir=\"5400000\" '\n            'rotWithShape=\"0\"><a:srgbClr val=\"000000\">'\n        '<a:alpha val=\"35000\"/></a:srgbClr></a:outerShdw></a:effectLst>'\n        '</a:effectStyle><a:effectStyle><a:effectLst>'\n        '<a:outerShdw blurRad=\"40000\" dist=\"23000\" dir=\"5400000\" '\n            'rotWithShape=\"0\"><a:srgbClr val=\"000000\">'\n        '<a:alpha val=\"35000\"/></a:srgbClr></a:outerShdw></a:effectLst>'\n        '<a:scene3d><a:camera prst=\"orthographicFront\">'\n        '<a:rot lat=\"0\" lon=\"0\" rev=\"0\"/></a:camera>'\n        '<a:lightRig rig=\"threePt\" dir=\"t\">'\n        '<a:rot lat=\"0\" lon=\"0\" rev=\"1200000\"/></a:lightRig>'\n        '</a:scene3d><a:sp3d><a:bevelT w=\"63500\" h=\"25400\"/>'\n        '</a:sp3d></a:effectStyle></a:effectStyleLst>'\n        '<a:bgFillStyleLst><a:solidFill><a:schemeClr val=\"phClr\"/>'\n        '</a:solidFill><a:gradFill rotWithShape=\"1\"><a:gsLst>'\n        '<a:gs pos=\"0\"><a:schemeClr val=\"phClr\"><a:tint val=\"40000\"/>'\n        '<a:satMod val=\"350000\"/></a:schemeClr></a:gs>'\n        '<a:gs pos=\"40000\"><a:schemeClr val=\"phClr\"><a:tint val=\"45000\"/>'\n        '<a:shade val=\"99000\"/><a:satMod val=\"350000\"/>'\n        '</a:schemeClr></a:gs>'\n        '<a:gs pos=\"100000\"><a:schemeClr val=\"phClr\">'\n        '<a:shade val=\"20000\"/><a:satMod val=\"255000\"/>'\n        '</a:schemeClr></a:gs></a:gsLst>'\n        '<a:path path=\"circle\">'\n        '<a:fillToRect l=\"50000\" t=\"-80000\" r=\"50000\" b=\"180000\"/>'\n        '</a:path>'\n        '</a:gradFill><a:gradFill rotWithShape=\"1\"><a:gsLst>'\n        '<a:gs pos=\"0\"><a:schemeClr val=\"phClr\"><a:tint val=\"80000\"/>'\n        '<a:satMod val=\"300000\"/></a:schemeClr></a:gs>'\n        '<a:gs pos=\"100000\"><a:schemeClr val=\"phClr\">'\n        '<a:shade val=\"30000\"/><a:satMod val=\"200000\"/>'\n        '</a:schemeClr></a:gs></a:gsLst>'\n        '<a:path path=\"circle\">'\n        '<a:fillToRect l=\"50000\" t=\"50000\" r=\"50000\" b=\"50000\"/></a:path>'\n        '</a:gradFill></a:bgFillStyleLst></a:fmtScheme>'\n        '</a:themeElements>'\n        '<a:objectDefaults/><a:extraClrSchemeLst/>'\n        '</a:theme>')\nreturn get_document_content(xml_node)", "path": "dataproxy\\vendor\\openpyxl-1.5.7\\openpyxl\\writer\\theme.py", "repo_name": "okfn/dataproxy", "stars": 74, "license": "other", "language": "python", "size": 779}
{"docstring": "# self is a Book instance.\n", "func_signal": "def xf_epilogue(self):\n", "code": "self._xf_epilogue_done = 1\nnum_xfs = len(self.xf_list)\nblah = DEBUG or self.verbosity >= 3\nblah1 = DEBUG or self.verbosity >= 1\nif blah:\n    fprintf(self.logfile, \"xf_epilogue called ...\\n\")\n\ndef check_same(book_arg, xf_arg, parent_arg, attr):\n    # the _arg caper is to avoid a Warning msg from Python 2.1 :-(\n    if getattr(xf_arg, attr) != getattr(parent_arg, attr):\n        fprintf(book_arg.logfile,\n            \"NOTE !!! XF[%d] parent[%d] %s different\\n\",\n            xf_arg.xf_index, parent_arg.xf_index, attr)\n\nfor xfx in xrange(num_xfs):\n    xf = self.xf_list[xfx]\n    if not self.format_map.has_key(xf.format_key):\n        msg = \"ERROR *** XF[%d] unknown format key (%d, 0x%04x)\\n\"\n        fprintf(self.logfile, msg,\n                xf.xf_index, xf.format_key, xf.format_key)\n        xf.format_key = 0\n    cellty_from_fmtty = {\n        FNU: XL_CELL_NUMBER,\n        FUN: XL_CELL_NUMBER,\n        FGE: XL_CELL_NUMBER,\n        FDT: XL_CELL_DATE,\n        FTX: XL_CELL_NUMBER, # Yes, a number can be formatted as text.\n        }\n    fmt = self.format_map[xf.format_key]\n    cellty = cellty_from_fmtty[fmt.type]\n    self._xf_index_to_xl_type_map[xf.xf_index] = cellty\n    # Now for some assertions etc\n    if not self.formatting_info:\n        continue\n    if xf.is_style:\n        continue\n    if not(0 <= xf.parent_style_index < num_xfs):\n        fprintf(self.logfile,\n            \"WARNING *** XF[%d]: is_style=%d but parent_style_index=%d\\n\",\n            xf.xf_index, xf.is_style, xf.parent_style_index)\n        # make it conform\n        xf.parent_style_index = 0\n    if self.biff_version >= 30:\n        assert xf.parent_style_index != xf.xf_index\n        assert self.xf_list[xf.parent_style_index].is_style\n        if blah1 and xf.parent_style_index > xf.xf_index:\n            fprintf(self.logfile,\n                \"NOTE !!! XF[%d]: parent_style_index is %d; out of order?\\n\",\n                xf.xf_index, xf.parent_style_index)\n        parent = self.xf_list[xf.parent_style_index]\n        if not xf._alignment_flag and not parent._alignment_flag:\n            if blah1: check_same(self, xf, parent, 'alignment')\n        if not xf._background_flag and not parent._background_flag:\n            if blah1: check_same(self, xf, parent, 'background')\n        if not xf._border_flag and not parent._border_flag:\n            if blah1: check_same(self, xf, parent, 'border')\n        if not xf._protection_flag and not parent._protection_flag:\n            if blah1: check_same(self, xf, parent, 'protection')\n        if not xf._format_flag and not parent._format_flag:\n            if blah1 and xf.format_key != parent.format_key:\n                fprintf(self.logfile,\n                    \"NOTE !!! XF[%d] fmtk=%d, parent[%d] fmtk=%r\\n%r / %r\\n\",\n                    xf.xf_index, xf.format_key, parent.xf_index, parent.format_key,\n                    self.format_map[xf.format_key].format_str,\n                    self.format_map[parent.format_key].format_str)\n        if not xf._font_flag and not parent._font_flag:\n            if blah1 and xf.font_index != parent.font_index:\n                fprintf(self.logfile,\n                    \"NOTE !!! XF[%d] fontx=%d, parent[%d] fontx=%r\\n\",\n                    xf.xf_index, xf.font_index, parent.xf_index, parent.font_index)", "path": "dataproxy\\xlrd\\formatting.py", "repo_name": "okfn/dataproxy", "stars": 74, "license": "other", "language": "python", "size": 779}
{"docstring": "# print >> self.logfile, \"_get_stream\", base, sec_size, start_sid, size\n", "func_signal": "def _get_stream(self, mem, base, sat, sec_size, start_sid, size=None, name=''):\n", "code": "sectors = []\ns = start_sid\nif size is None:\n    # nothing to check against\n    while s >= 0:\n        start_pos = base + s * sec_size\n        sectors.append(mem[start_pos:start_pos+sec_size])\n        try:\n            s = sat[s]\n        except IndexError:\n            raise CompDocError(\n                \"OLE2 stream %r: sector allocation table invalid entry (%d)\" %\n                (name, s)\n                )\n    assert s == EOCSID\nelse:\n    todo = size\n    while s >= 0:\n        start_pos = base + s * sec_size\n        grab = sec_size\n        if grab > todo:\n            grab = todo\n        todo -= grab\n        sectors.append(mem[start_pos:start_pos+grab])\n        try:\n            s = sat[s]\n        except IndexError:\n            raise CompDocError(\n                \"OLE2 stream %r: sector allocation table invalid entry (%d)\" %\n                (name, s)\n                )\n    assert s == EOCSID\n    if todo != 0:\n        print >> self.logfile, \\\n            \"WARNING *** OLE2 stream %r: expected size %d, actual size %d\" \\\n            % (name, size, size - todo)\nreturn ''.join(sectors)", "path": "dataproxy\\xlrd\\compdoc.py", "repo_name": "okfn/dataproxy", "stars": 74, "license": "other", "language": "python", "size": 779}
{"docstring": "\"\"\"Setup DataProxy tests\"\"\"\n\n# Create application\n", "func_signal": "def setup(self):\n", "code": "self.wsgiapp = JsonpDataProxy(100000)\nself.app = TestApp(self.wsgiapp)\n\n# Define named requests for easier (re)cofiguration and reuse.\nself.requests = {\n        \"no_type\": \"url=http://foo.com/foo\",\n         \"unknown_type\": \"url=http://foo.com/foo.undefined\",\n         \"valid_csv\": \"url=https://raw.github.com/datasets/cofog/master/data/cofog.csv\",\n         \"valid_csv_with_decimal\": \"url=https://raw.github.com/datasets/gold-prices/master/data/data.csv\",\n         \"valid_csv_limit\": {\n             \"url\": \"https://raw.github.com/datasets/cofog/master/data/cofog.csv\",\n                    \"max-results\": 3,\n                    \"format\": \"json\"\n                    },\n         \"csv_json\": \"url=https://raw.github.com/datasets/cofog/master/data/cofog.csv&format=json\",\n         \"valid_xls\": \"url=http://oee.nrcan.gc.ca/corporate/statistics/neud/dpa/tablestrends2/id_ca_28_e.xls\",\n         \"valid_xlsx\": \"url=https://github.com/okfn/dataconverters/raw/master/testdata/xls/simple.xlsx&format=json\",\n         \"untyped_csv\": {\n                \"url\": \"http://openeconomics.net/store/8d7d4770-e1d1-11db-9f7e-00145101c316/data\",\n                \"type\": \"csv\"\n            },\n         \"redirect_csv\": \"url=http://www.archive.org/download/ckan-cofog/cofog.csv\",\n         \"valid_tsv\": \"url=https://raw.github.com/okfn/dataconverters/master/testdata/tsv/simple.tsv\"\n        }", "path": "test\\test_app.py", "repo_name": "okfn/dataproxy", "stars": 74, "license": "other", "language": "python", "size": 779}
{"docstring": "'''od.update(E, **F) -> None.  Update od from dict/iterable E and F.\n\nIf E is a dict instance, does:           for k in E: od[k] = E[k]\nIf E has a .keys() method, does:         for k in E.keys(): od[k] = E[k]\nOr if E is an iterable of items, does:   for k, v in E: od[k] = v\nIn either case, this is followed by:     for k, v in F.items(): od[k] = v\n\n'''\n", "func_signal": "def update(*args, **kwds):\n", "code": "if len(args) > 2:\n    raise TypeError('update() takes at most 2 positional '\n                    'arguments (%d given)' % (len(args),))\nelif not args:\n    raise TypeError('update() takes at least 1 argument (0 given)')\nself = args[0]\n# Make progressively weaker assumptions about \"other\"\nother = ()\nif len(args) == 2:\n    other = args[1]\nif isinstance(other, dict):\n    for key in other:\n        self[key] = other[key]\nelif hasattr(other, 'keys'):\n    for key in other.keys():\n        self[key] = other[key]\nelse:\n    for key, value in other:\n        self[key] = value\nfor key, value in kwds.items():\n    self[key] = value", "path": "dataproxy\\vendor\\openpyxl-1.5.7\\openpyxl\\shared\\compat\\odict.py", "repo_name": "okfn/dataproxy", "stars": 74, "license": "other", "language": "python", "size": 779}
{"docstring": "\"\"\" return (x, y, w, h) in EMU \"\"\"\n\n", "func_signal": "def get_emu_dimensions(self):\n", "code": "return (pixels_to_EMU(self.left), pixels_to_EMU(self.top),\n    pixels_to_EMU(self._width), pixels_to_EMU(self._height))", "path": "dataproxy\\vendor\\openpyxl-1.5.7\\openpyxl\\drawing.py", "repo_name": "okfn/dataproxy", "stars": 74, "license": "other", "language": "python", "size": 779}
{"docstring": "'''od.__eq__(y) <==> od==y.  Comparison to another OD is order-sensitive\nwhile comparison to a regular mapping is order-insensitive.\n\n'''\n", "func_signal": "def __eq__(self, other):\n", "code": "if isinstance(other, OrderedDict):\n    return len(self) == len(other) and self.items() == other.items()\nreturn dict.__eq__(self, other)", "path": "dataproxy\\vendor\\openpyxl-1.5.7\\openpyxl\\shared\\compat\\odict.py", "repo_name": "okfn/dataproxy", "stars": 74, "license": "other", "language": "python", "size": 779}
{"docstring": "# Return matching DirNode instance, or None\n", "func_signal": "def _dir_search(self, path, storage_DID=0):\n", "code": "head = path[0]\ntail = path[1:]\ndl = self.dirlist\nfor child in dl[storage_DID].children:\n    if dl[child].name.lower() == head.lower():\n        et = dl[child].etype\n        if et == 2:\n            return dl[child]\n        if et == 1:\n            if not tail:\n                raise CompDocError(\"Requested component is a 'storage'\")\n            return self._dir_search(tail, child)\n        dl[child].dump(1)\n        raise CompDocError(\"Requested stream is not a 'user stream'\")\nreturn None", "path": "dataproxy\\xlrd\\compdoc.py", "repo_name": "okfn/dataproxy", "stars": 74, "license": "other", "language": "python", "size": 779}
{"docstring": "'''Initialize an ordered dictionary.  Signature is the same as for\nregular dictionaries, but keyword arguments are not recommended\nbecause their insertion order is arbitrary.\n\n'''\n", "func_signal": "def __init__(self, *args, **kwds):\n", "code": "if len(args) > 1:\n    raise TypeError('expected at most 1 arguments, got %d' % len(args))\ntry:\n    self.__root\nexcept AttributeError:\n    self.__root = root = []                     # sentinel node\n    root[:] = [root, root, None]\n    self.__map = {}\nself.__update(*args, **kwds)", "path": "dataproxy\\vendor\\openpyxl-1.5.7\\openpyxl\\shared\\compat\\odict.py", "repo_name": "okfn/dataproxy", "stars": 74, "license": "other", "language": "python", "size": 779}
{"docstring": "\"\"\"\nCreate a new L{UniformSample}.\n\n@type reservoir_size: C{int}\n@param reservoir_size: the number of params to keep in the sampling reservoir\n\"\"\"\n", "func_signal": "def __init__(self, reservoir_size):\n", "code": "self.values = [0 for x in xrange(reservoir_size)]\nself.clear()", "path": "yunomi\\stats\\uniform_sample.py", "repo_name": "dreid/yunomi", "stars": 108, "license": "other", "language": "python", "size": 228}
{"docstring": "\"\"\"\nGets a timer based on a key, creates a new one if it does not exist.\n\n@param key: name of the metric\n@type key: C{str}\n\n@return: L{Timer}\n\"\"\"\n", "func_signal": "def timer(self, key):\n", "code": "if key not in self._timers:\n    self._timers[key] = Timer()\nreturn self._timers[key]", "path": "yunomi\\core\\metrics_registry.py", "repo_name": "dreid/yunomi", "stars": 108, "license": "other", "language": "python", "size": 228}
{"docstring": "\"\"\"\nUpdates the I{self.values} at a random index with the given value.\n\n@type value: C{int} or C{float}\n@param value: the new value to be added\n\"\"\"\n", "func_signal": "def update(self, value):\n", "code": "self.count += 1\nif self.count <= len(self.values):\n    self.values[self.count - 1] = value\nelse:\n    r = UniformSample.next_long(self.count)\n    if r < len(self.values):\n        self.values[r] = value", "path": "yunomi\\stats\\uniform_sample.py", "repo_name": "dreid/yunomi", "stars": 108, "license": "other", "language": "python", "size": 228}
{"docstring": "\"\"\"\nCreates a statistical snapshot from the current set of values.\n\"\"\"\n", "func_signal": "def get_snapshot(self):\n", "code": "copy = []\nfor i in xrange(self.size()):\n    copy.append(self.values[i])\nreturn Snapshot(copy)", "path": "yunomi\\stats\\uniform_sample.py", "repo_name": "dreid/yunomi", "stars": 108, "license": "other", "language": "python", "size": 228}
{"docstring": "\"\"\"\nReturns the value at the given quantile.\n\n@type quantile: C{float}\n@param quantile: a given quantile in M{[0...1]}\n\n@rtype: C{int} or C{float}\n@return: the value in the distribution at the specified I{quantile}\n\"\"\"\n", "func_signal": "def get_value(self, quantile):\n", "code": "assert quantile >= 0.0 and quantile <= 1.0,\\\n    \"{0} is not in [0...1]\".format(quantile)\nif len(self.values) == 0:\n    return 0.0\n\npos = quantile * (len(self.values) + 1)\n\nif pos < 1:\n    return self.values[0]\nif pos >= len(self.values):\n    return self.values[len(self.values) -1]\n\nlower = self.values[int(pos) - 1]\nupper = self.values[int(pos)]\nreturn lower + (pos - floor(pos)) * (upper - lower)", "path": "yunomi\\stats\\snapshot.py", "repo_name": "dreid/yunomi", "stars": 108, "license": "other", "language": "python", "size": 228}
{"docstring": "\"\"\"\nClears the sample, setting all values to zero.\n\"\"\"\n", "func_signal": "def clear(self):\n", "code": "self.values = [0 for x in xrange(len(self.values))]\nself.count = 0", "path": "yunomi\\stats\\uniform_sample.py", "repo_name": "dreid/yunomi", "stars": 108, "license": "other", "language": "python", "size": 228}
{"docstring": "\"\"\"\nCreate a new EWMA with a specific smoothing constant.\n\n@type period: C{int}\n@param period: the time it takes to reach a given significance level\n@type interval: C{int}\n@param interval: the expected tick interval, defaults to 5s\n\"\"\"\n", "func_signal": "def __init__(self, period, interval=None):\n", "code": "self.initialized = False\nself._period = period\nself._interval = (interval or EWMA.INTERVAL)\nself._uncounted = 0.0\nself._rate = 0.0\nself._last_tick = time()", "path": "yunomi\\stats\\ewma.py", "repo_name": "dreid/yunomi", "stars": 108, "license": "other", "language": "python", "size": 228}
{"docstring": "\"\"\"\nUpdates the L{Histogram} and marks the L{Meter}.\n\n@type duration: C{int}\n@param duration: the duration of an event\n\"\"\"\n", "func_signal": "def update(self, duration):\n", "code": "if duration >= 0:\n    self.histogram.update(duration)\n    self.meter.mark()", "path": "yunomi\\core\\timer.py", "repo_name": "dreid/yunomi", "stars": 108, "license": "other", "language": "python", "size": 228}
{"docstring": "\"\"\"\nDecorator to the rate at which a function is called.\n\n@param fn: the function to be decorated\n@type fn: C{func}\n\n@return: the decorated function\n@rtype: C{func}\n\"\"\"\n", "func_signal": "def meter_calls(fn):\n", "code": "@wraps(fn)\ndef wrapper(*args):\n    meter(\"%s_calls\" % fn.__name__).mark()\n    try:\n        return fn(*args)\n    except:\n        raise\nreturn wrapper", "path": "yunomi\\core\\metrics_registry.py", "repo_name": "dreid/yunomi", "stars": 108, "license": "other", "language": "python", "size": 228}
{"docstring": "\"\"\"\nL{Histogram.get_snapshot}\n\"\"\"\n", "func_signal": "def get_snapshot(self):\n", "code": "values = self.histogram.get_snapshot().get_values()\nreturn Snapshot(values)", "path": "yunomi\\core\\timer.py", "repo_name": "dreid/yunomi", "stars": 108, "license": "other", "language": "python", "size": 228}
{"docstring": "\"\"\"\nMark the passage of time and decay the current rate accordingly.\n\"\"\"\n", "func_signal": "def tick(self):\n", "code": "prev = self._last_tick\nnow = time()\ninterval = now - prev\n\ninstant_rate = self._uncounted / interval\nself._uncounted = 0\n\nif self.initialized:\n    self._rate += (self._alpha(interval) * (instant_rate - self._rate))\nelse:\n    self._rate = instant_rate\n    self.initialized = True\n\nself._last_tick = now", "path": "yunomi\\stats\\ewma.py", "repo_name": "dreid/yunomi", "stars": 108, "license": "other", "language": "python", "size": 228}
{"docstring": "\"\"\"\nGets a meter based on a key, creates a new one if it does not exist.\n\n@param key: name of the metric\n@type key: C{str}\n\n@return: L{Meter}\n\"\"\"\n", "func_signal": "def meter(self, key):\n", "code": "if key not in self._meters:\n    self._meters[key] = Meter()\nreturn self._meters[key]", "path": "yunomi\\core\\metrics_registry.py", "repo_name": "dreid/yunomi", "stars": 108, "license": "other", "language": "python", "size": 228}
{"docstring": "\"\"\"\nGets a counter based on a key, creates a new one if it does not exist.\n\n@param key: name of the metric\n@type key: C{str}\n\n@return: L{Counter}\n\"\"\"\n", "func_signal": "def counter(self, key):\n", "code": "if key not in self._counters:\n    self._counters[key] = Counter()\nreturn self._counters[key]", "path": "yunomi\\core\\metrics_registry.py", "repo_name": "dreid/yunomi", "stars": 108, "license": "other", "language": "python", "size": 228}
{"docstring": "\"\"\"\nGets a histogram based on a key, creates a new one if it does not exist.\n\n@param key: name of the metric\n@type key: C{str}\n\n@return: L{Histogram}\n\"\"\"\n", "func_signal": "def histogram(self, key, biased=False):\n", "code": "if key not in self._histograms:\n    if biased:\n        self._histograms[key] = Histogram.get_biased()\n    else:\n        self._histograms[key] = Histogram.get_uniform()\n\nreturn self._histograms[key]", "path": "yunomi\\core\\metrics_registry.py", "repo_name": "dreid/yunomi", "stars": 108, "license": "other", "language": "python", "size": 228}
{"docstring": "\"\"\"\nDecorator to time the execution of the function.\n\n@param fn: the function to be decorated\n@type fn: C{func}\n\n@return: the decorated function\n@rtype: C{func}\n\"\"\"\n", "func_signal": "def time_calls(fn):\n", "code": "@wraps(fn)\ndef wrapper(*args):\n    _timer = timer(\"%s_calls\" % fn.__name__)\n    start = time()\n    try:\n        return fn(*args)\n    except:\n        raise\n    finally:\n        _timer.update(time() - start)\nreturn wrapper", "path": "yunomi\\core\\metrics_registry.py", "repo_name": "dreid/yunomi", "stars": 108, "license": "other", "language": "python", "size": 228}
{"docstring": "\"\"\"\nCreate a new L{Snapshot} with the given values.\n\n@type values: C{dict}\n@param values: an unordered set of values in the sample\n\"\"\"\n", "func_signal": "def __init__(self, values):\n", "code": "self.values = list(values)\nself.values.sort()", "path": "yunomi\\stats\\snapshot.py", "repo_name": "dreid/yunomi", "stars": 108, "license": "other", "language": "python", "size": 228}
{"docstring": "\"\"\"\nReturns the rate in counts per second. Calls L{EWMA.tick} when the\nelapsed time is greater than L{EWMA.INTERVAL}.\n\n@rtype: C{float}\n@return: the rate\n\"\"\"\n", "func_signal": "def get_rate(self):\n", "code": "if time() - self._last_tick >= self._interval:\n    self.tick()\nreturn self._rate", "path": "yunomi\\stats\\ewma.py", "repo_name": "dreid/yunomi", "stars": 108, "license": "other", "language": "python", "size": 228}
{"docstring": "\"\"\"\nCreates a new L{Timer} instance.\n\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self.histogram = Histogram.get_biased()\nself.meter = Meter(\"calls\")", "path": "yunomi\\core\\timer.py", "repo_name": "dreid/yunomi", "stars": 108, "license": "other", "language": "python", "size": 228}
{"docstring": "\"\"\"\nDecorator to check the distribution of return values of a function.\n\n@param fn: the function to be decorated\n@type fn: C{func}\n\n@return: the decorated function\n@rtype: C{func}\n\"\"\"\n", "func_signal": "def hist_calls(fn):\n", "code": "@wraps(fn)\ndef wrapper(*args):\n    _histogram = histogram(\"%s_calls\" % fn.__name__)\n    try:\n        rtn = fn(*args)\n        if type(rtn) in (int, float):\n            _histogram.update(rtn)\n        return rtn\n    except:\n        raise\nreturn wrapper", "path": "yunomi\\core\\metrics_registry.py", "repo_name": "dreid/yunomi", "stars": 108, "license": "other", "language": "python", "size": 228}
{"docstring": "\"\"\"\nCreates a new L{MetricsRegistry} instance.\n\"\"\"\n", "func_signal": "def __init__(self, clock=time):\n", "code": "self._timers = {}\nself._meters = {}\nself._counters = {}\nself._histograms = {}\n\nself._clock = clock", "path": "yunomi\\core\\metrics_registry.py", "repo_name": "dreid/yunomi", "stars": 108, "license": "other", "language": "python", "size": 228}
{"docstring": "\"\"\"Retrieve the configuration from the parser options.\"\"\"\n", "func_signal": "def __init__(self, options):\n", "code": "if options.verbosity == 0:\n    log.setLevel(logging.ERROR)\nelif options.verbosity == 1:\n    log.setLevel(logging.INFO)\nelif options.verbosity == 2:\n    log.setLevel(logging.DEBUG)\n    log.debug(\"Debug logging on\")\nself.delete = options.delete\nif options.attributes:\n    self.requested_attributes = set(options.attributes)\nelse:\n    self.requested_attributes = set()\nself.dry_run = options.dry_run\nself.update = options.update\nif self.update:\n    self.requested_attributes.add(\"mtime\")\nself.recursive = options.recursive\nif options.exclude_files:\n    self.exclude_files = re.compile(options.exclude_files)\nelse:\n    # An unmatchable regex, to save us from checking if this is set. Hopefully it's\n    # not too slow.\n    self.exclude_files = re.compile(\"^$\")\nif options.include_files:\n    self.include_files = re.compile(options.include_files)\n    if not self.exclude_files:\n        self.exclude_files = re.compile(\"\")\nelse:\n    self.include_files = re.compile(\"^$\")\nif options.exclude_dirs:\n    self.exclude_dirs = re.compile(options.exclude_dirs)\nelse:\n    self.exclude_dirs = re.compile(\"^$\")\nif options.include_dirs:\n    self.include_dirs = re.compile(options.include_dirs)\n    if not self.exclude_dirs:\n        self.exclude_dirs = re.compile(\"\")\nelse:\n    self.include_dirs = re.compile(\"^$\")\n\n# access to remaining options and any options\n# that were set by plugins.\nself.full_options = options\n\nself.exclude_attributes = set()", "path": "omnisync\\configuration.py", "repo_name": "skorokithakis/omnisync", "stars": 88, "license": "None", "language": "python", "size": 531}
{"docstring": "\"\"\"Remove the specified file.\"\"\"\n", "func_signal": "def remove(self, url):\n", "code": "filename = self._get_filename(url)\nif filename not in self._filesystem or self._filesystem[filename] is None:\n    return False\ndel self._filesystem[filename]\nreturn True", "path": "omnisync\\transports\\virtual.py", "repo_name": "skorokithakis/omnisync", "stars": 88, "license": "None", "language": "python", "size": 531}
{"docstring": "\"\"\"Create a Progress instance. totalitems must be the total number of\nitems we intend to process, so the class knows how far we've gone.\"\"\"\n", "func_signal": "def __init__(self, totalitems, timeasstring = True):\n", "code": "self._totalitems = totalitems\nself._starttime = time.time()\nself._timeasstring = timeasstring", "path": "omnisync\\progress.py", "repo_name": "skorokithakis/omnisync", "stars": 88, "license": "None", "language": "python", "size": 531}
{"docstring": "\"\"\"Return a string detailing the current progress.\"\"\"\n", "func_signal": "def progressstring(self, itemnumber):\n", "code": "timings = self.progress(itemnumber)\nif itemnumber == self._totalitems:\n    return \"Done in %s, processed %s items.        \\n\" % (timings[0], timings[4])\nelse:\n    return \"Progress: %s/%s, %s%%, %s/%s items.\\r\" % timings", "path": "omnisync\\progress.py", "repo_name": "skorokithakis/omnisync", "stars": 88, "license": "None", "language": "python", "size": 531}
{"docstring": "\"\"\"Retrieve a directory listing of the given location.\n\nReturns a list of (url, attribute_dict) tuples if the\ngiven URL is a directory, False otherwise.\n\"\"\"\n# Add a slash so we don't have to remove it from the start of the subpaths.\n", "func_signal": "def listdir(self, url):\n", "code": "url = urlfunctions.append_slash(url)\nfilename = self._get_filename(url, False)\nfiles = set()\nfor key in self._filesystem:\n    # Check the length to prevent returning the directory itself.\n    if key.startswith(filename) and len(key) > len(filename):\n        subpath = key[len(filename):]\n        if \"/\" not in subpath:\n            # Add the subpath in the set as is, because there are no lower levels.\n            files.add(subpath)\n        else:\n            files.add(subpath[:subpath.find(\"/\")])\nreturn [FileObject(self, url + x,) for x in files]", "path": "omnisync\\transports\\virtual.py", "repo_name": "skorokithakis/omnisync", "stars": 88, "license": "None", "language": "python", "size": 531}
{"docstring": "\"\"\"Write _data_ to the open file.\"\"\"\n", "func_signal": "def write(self, data):\n", "code": "if self._file_handle is None:\n    return IOError, \"No file is open.\"\nself._filesystem[self._file_handle][\"size\"] += len(data)", "path": "omnisync\\transports\\virtual.py", "repo_name": "skorokithakis/omnisync", "stars": 88, "license": "None", "language": "python", "size": 531}
{"docstring": "\"\"\"Close the open file.\"\"\"\n", "func_signal": "def close(self):\n", "code": "if self._file_handle:\n    self._file_handle.close()\n    self._file_handle = None", "path": "omnisync\\transports\\s3.py", "repo_name": "skorokithakis/omnisync", "stars": 88, "license": "None", "language": "python", "size": 531}
{"docstring": "\"\"\"Unpickle the filesystem dictionary.\"\"\"\n", "func_signal": "def connect(self, url, config):\n", "code": "self._storage = urlfunctions.url_split(url).hostname\n# If the storage is in-memory only, don't do anything.\nif self._storage == \"memory\":\n    return\ntry:\n    pickled_file = open(self._storage, \"rb\")\nexcept IOError:\n    return\nself._filesystem = pickle.load(pickled_file)\npickled_file.close()", "path": "omnisync\\transports\\virtual.py", "repo_name": "skorokithakis/omnisync", "stars": 88, "license": "None", "language": "python", "size": 531}
{"docstring": "\"\"\"Create a directory.\"\"\"\n", "func_signal": "def mkdir(self, url):\n", "code": "filename = self._get_filename(url)\nif filename not in self._filesystem:\n    return IOError, \"A directory with the specified name already exists.\"\nself._filesystem[filename] = None", "path": "omnisync\\transports\\virtual.py", "repo_name": "skorokithakis/omnisync", "stars": 88, "license": "None", "language": "python", "size": 531}
{"docstring": "\"\"\"Do nothing.\"\"\"\n# TODO: Set ACL.\n\n", "func_signal": "def setattr(self, url, attributes):\n", "code": "\n\"\"\"Return True if a given path exists, False otherwise.\"\"\"\nfilename = self._get_filename(url)\n# If we're looking for the root, return True.\nif filename == \"\":\n    return True\nreturn Key(self._bucket, filename).exists()", "path": "omnisync\\transports\\s3.py", "repo_name": "skorokithakis/omnisync", "stars": 88, "license": "None", "language": "python", "size": 531}
{"docstring": "\"\"\"Retrieve the local filename from a given URL.\"\"\"\n", "func_signal": "def _get_filename(self, url):\n", "code": "url = urlfunctions.append_slash(url, False)\nsplit_url = urlfunctions.url_split(url, uses_hostname=self.uses_hostname)\nreturn urlfunctions.prepend_slash(split_url.path, False)", "path": "omnisync\\transports\\s3.py", "repo_name": "skorokithakis/omnisync", "stars": 88, "license": "None", "language": "python", "size": 531}
{"docstring": "\"\"\"Remove the specified directory non-recursively.\"\"\"\n", "func_signal": "def rmdir(self, url):\n", "code": "filename = self._get_filename(url)\nif self._filesystem[filename] is not None:\n    return False\nif self.listdir(url):\n    return False\nelse:\n    del self._filesystem[filename]\n    return True", "path": "omnisync\\transports\\virtual.py", "repo_name": "skorokithakis/omnisync", "stars": 88, "license": "None", "language": "python", "size": 531}
{"docstring": "\"\"\"Pickle the filesystem to a file for persistence.\"\"\"\n# If the storage is in-memory only, don't do anything.\n", "func_signal": "def disconnect(self):\n", "code": "if self._storage == \"memory\":\n    return\npickled_file = open(self._storage, \"wb\")\npickle.dump(self._filesystem, pickled_file)\npickled_file.close()", "path": "omnisync\\transports\\virtual.py", "repo_name": "skorokithakis/omnisync", "stars": 88, "license": "None", "language": "python", "size": 531}
{"docstring": "\"\"\"We have progressed itemnumber items, so return our completion\npercentage, items/total items, total time and projected total\ntime.\"\"\"\n", "func_signal": "def progress(self, itemnumber):\n", "code": "elapsed = time.time() - self._starttime\n# Multiply by 1.0 to force conversion to long.\npercentcomplete = (1.0 * itemnumber) / self._totalitems\ntry:\n    total = int(elapsed / percentcomplete)\nexcept ZeroDivisionError:\n    total = 0\nif self._timeasstring:\n    return ({\"elapsed_time\": timetostr(elapsed),\n            \"total_time\": timetostr(total),\n            \"percentage\": int(percentcomplete * 100),\n            \"item\": itemnumber,\n            \"items\": self._totalitems})\nelse:\n    return ({\"elapsed_time\": int(elapsed),\n            \"total_time\": int(total),\n            \"percentage\": int(percentcomplete * 100),\n            \"item\": itemnumber,\n            \"items\": self._totalitems})", "path": "omnisync\\progress.py", "repo_name": "skorokithakis/omnisync", "stars": 88, "license": "None", "language": "python", "size": 531}
{"docstring": "\"\"\"Read _size_ bytes from the open file.\"\"\"\n", "func_signal": "def read(self, size):\n", "code": "if self._file_handle is None:\n    return IOError, \"No file is open.\"\nif self._bytes_read + size < self._filesystem[self._file_handle][\"size\"]:\n    self._bytes_read += size\n    return \" \" * size\nelse:\n    bytes_read = self._bytes_read\n    self._bytes_read = self._filesystem[self._file_handle][\"size\"]\n    return \" \" * (self._filesystem[self._file_handle][\"size\"] - bytes_read)", "path": "omnisync\\transports\\virtual.py", "repo_name": "skorokithakis/omnisync", "stars": 88, "license": "None", "language": "python", "size": 531}
{"docstring": "\"\"\"Retrieve as many file attributes as we can, at the very *least* the requested ones.\n\nReturns a dictionary of {\"attribute\": \"value\"}, or {\"attribute\": None} if the file does\nnot exist.\n\"\"\"\n", "func_signal": "def getattr(self, url, attributes):\n", "code": "try:\n    attrs = self._filesystem[self._get_filename(url)]\nexcept KeyError:\n    return {\"size\": None}\nif attrs is None:\n    # Directories have no attributes in our virtual FS.\n    return {\"size\": None}\nelse:\n    return attrs", "path": "omnisync\\transports\\virtual.py", "repo_name": "skorokithakis/omnisync", "stars": 88, "license": "None", "language": "python", "size": 531}
{"docstring": "\"\"\"Where we're going, we don't *need* directories.\"\"\"\n\n", "func_signal": "def mkdir(self, url):\n", "code": "\n\"\"\"Retrieve a directory listing of the given location.\n\nReturns a list of (url, attribute_dict) tuples if the given URL is a directory,\nFalse otherwise. URLs should be absolute, including protocol, etc.\nattribute_dict is a dictionary of {key: value} pairs for any applicable\nattributes from (\"size\", \"mtime\", \"atime\", \"ctime\", \"isdir\").\n\"\"\"\nurl = urlfunctions.append_slash(url, True)\nurl = urlfunctions.url_split(url)\npath = urlfunctions.prepend_slash(url.path, False)\ndir_list = self._bucket.list(prefix=path, delimiter=\"/\")\nfile_list = []\nfor item in dir_list:\n    # Prepend a slash by convention.\n    url.path = \"/\" + item.name\n    # list() returns directories ending with a slash.\n    file_obj = FileObject(self, urlfunctions.url_join(url),\n                                {\"isdir\": item.name.endswith(\"/\")})\n    if not file_obj.isdir:\n        file_obj.size = item.size\n    else:\n        file_obj.size = 0\n    file_list.append(file_obj)\nreturn file_list", "path": "omnisync\\transports\\s3.py", "repo_name": "skorokithakis/omnisync", "stars": 88, "license": "None", "language": "python", "size": 531}
{"docstring": "\"\"\"Convert seconds to D:H:M:S format (whichever applicable).\"\"\"\n", "func_signal": "def timetostr(duration):\n", "code": "duration = int(duration)\ntimelist = [duration / 86400, (duration / 3600) % 24]\ntimestring = \"\"\nprintall = False\nfor item in timelist:\n    printall |= item\n    if printall:\n        timestring += str(item).zfill(2) + \":\"\ntimestring += str((duration / 60) % 60).zfill(2) + \":\" + str(duration % 60).zfill(2)\nreturn timestring", "path": "omnisync\\progress.py", "repo_name": "skorokithakis/omnisync", "stars": 88, "license": "None", "language": "python", "size": 531}
{"docstring": "\"\"\"Retrieve the local filename from a given URL.\"\"\"\n# Remove the trailing slash as a convention unless specified otherwise.\n", "func_signal": "def _get_filename(self, url, remove_slash=True):\n", "code": "if remove_slash:\n    urlfunctions.append_slash(url, False)\nfilename = urlfunctions.url_split(url).path\nif filename == \"\":\n    filename = \"/\"\nreturn filename", "path": "omnisync\\transports\\virtual.py", "repo_name": "skorokithakis/omnisync", "stars": 88, "license": "None", "language": "python", "size": 531}
{"docstring": "\"\"\"Return True if the given URL is a directory, False if it is a file or\n   does not exist.\"\"\"\n", "func_signal": "def isdir(self, url):\n", "code": "filename = self._get_filename(url)\nif filename not in self._filesystem:\n    return False\nreturn self._filesystem[filename] is None", "path": "omnisync\\transports\\virtual.py", "repo_name": "skorokithakis/omnisync", "stars": 88, "license": "None", "language": "python", "size": 531}
{"docstring": "\"\"\" ensure_tenant_exists when tenant does not exist, check mode \"\"\"\n# Setup\n", "func_signal": "def test_ensure_tenant_exists_when_absent_check():\n", "code": "keystone = setup_tenant_user_role()\nkeystone.tenants.create = mock.Mock(return_value=mock.Mock(\n    id=\"7c310f797aa045898e2884a975ab32ab\"))\ncheck_mode = True\n\n# Code under test\n(changed, id) = keystone_user.ensure_tenant_exists(keystone, \"bar\",\n                \"The bar tenant\", check_mode)\n\n# Assertions\nassert changed\nassert_is_none(id)\nassert not keystone.tenants.create.called", "path": "openstack_modules\\tests\\test_keystone_user.py", "repo_name": "gc3-uzh-ch/ansible-playbooks", "stars": 107, "license": "None", "language": "python", "size": 790}
{"docstring": "\"\"\" ensure_role_exists when role does not exist yet \"\"\"\n# Setup\n", "func_signal": "def test_ensure_role_exists_when_role_is_absent():\n", "code": "keystone = setup_tenant_user_role()\nkeystone.roles.create = mock.Mock(return_value=mock.Mock(\n    id=\"40b14f9c2d114b38b3f6bced49a792b8\"))\nkeystone.roles.roles_for_user = mock.Mock(return_value=[])\ncheck_mode = False\n\n# Code under test\nuser = \"johndoe\"\ntenant = \"acme\"\nrole = \"webuser\"\n(changed, id) = keystone_user.ensure_role_exists(keystone, user, tenant,\n                                                role, check_mode)\n\n# Assertions\nassert changed\nassert_equal(id, \"40b14f9c2d114b38b3f6bced49a792b8\")\nkeystone.roles.create.assert_called_with(\"webuser\")", "path": "openstack_modules\\tests\\test_keystone_user.py", "repo_name": "gc3-uzh-ch/ansible-playbooks", "stars": 107, "license": "None", "language": "python", "size": 790}
{"docstring": "\"\"\" dispatch with tenant only\"\"\"\n# Setup\n", "func_signal": "def test_dispatch_tenant_when_present(mock_ensure_tenant_exists):\n", "code": "keystone = setup_tenant_user_role()\nmock_ensure_tenant_exists.return_value = (True,\n                                   \"34469137412242129cd908e384717794\")\n\n# Code under test\nres = keystone_user.dispatch(keystone, tenant=\"bar\",\n                       tenant_description=\"This is a bar\")\n\n# Assertions\nmock_ensure_tenant_exists.assert_called_with(keystone, \"bar\",\n                                            \"This is a bar\", False)\nassert_equal(res,\n    dict(changed=True, id=\"34469137412242129cd908e384717794\"))", "path": "openstack_modules\\tests\\test_keystone_user.py", "repo_name": "gc3-uzh-ch/ansible-playbooks", "stars": 107, "license": "None", "language": "python", "size": 790}
{"docstring": "\"\"\" ensure_user_exists when user exists, check mode\"\"\"\n# Setup\n", "func_signal": "def test_ensure_user_exists_when_present_check():\n", "code": "keystone = setup_tenant_user_role()\ncheck_mode = True\n\n# Code under test\n(changed, id) = keystone_user.ensure_user_exists(keystone,\n                             user_name=\"johndoe\",\n                             password=\"12345\",\n                             email=\"johndoe@example.com\",\n                             tenant_name=\"acme\",\n                             check_mode=check_mode)\n\n# Assertions\nassert not changed\nassert_equal(id, \"24073d9426ab4bc59527955d7c486179\")", "path": "openstack_modules\\tests\\test_keystone_user.py", "repo_name": "gc3-uzh-ch/ansible-playbooks", "stars": 107, "license": "None", "language": "python", "size": 790}
{"docstring": "\"\"\" ensure_user_exists when user exists\"\"\"\n# Setup\n", "func_signal": "def test_ensure_user_exists_when_present():\n", "code": "keystone = setup_tenant_user_role()\ncheck_mode = False\n\n# Code under test\n(changed, id) = keystone_user.ensure_user_exists(keystone,\n                             user_name=\"johndoe\",\n                             password=\"12345\",\n                             email=\"johndoe@example.com\",\n                             tenant_name=\"acme\",\n                             check_mode=check_mode)\n\n# Assertions\nassert not changed\nassert_equal(id, \"24073d9426ab4bc59527955d7c486179\")", "path": "openstack_modules\\tests\\test_keystone_user.py", "repo_name": "gc3-uzh-ch/ansible-playbooks", "stars": 107, "license": "None", "language": "python", "size": 790}
{"docstring": "\"\"\" ensure_role_exists when role exists and is associated properly \"\"\"\n# Setup\n", "func_signal": "def test_ensure_role_exists_when_present():\n", "code": "keystone = setup_tenant_user_role()\nrole = mock.Mock()\nrole.id = \"34a699ab89d04c38894bbf3d998e5229\"\nrole.name = \"admin\"\nkeystone.roles.roles_for_user = mock.Mock(return_value=[role])\ncheck_mode = False\n\n# Code under test\nuser = \"johndoe\"\ntenant = \"acme\"\nrole = \"admin\"\n(changed, id) = keystone_user.ensure_role_exists(keystone, user, tenant,\n                                                 role, check_mode)\n\n# Assertions\nassert not changed\nassert_equal(id, \"34a699ab89d04c38894bbf3d998e5229\")\nassert not keystone.roles.create.called", "path": "openstack_modules\\tests\\test_keystone_user.py", "repo_name": "gc3-uzh-ch/ansible-playbooks", "stars": 107, "license": "None", "language": "python", "size": 790}
{"docstring": "\"\"\" ensure_user_exists when user does not exist, check mode\"\"\"\n# Setup\n", "func_signal": "def test_ensure_user_exists_when_absent_check():\n", "code": "keystone = setup_tenant_user_role()\nuser = mock.Mock()\nuser.id = \"5ce4b6ef2e814a4897907cc6db879536\"\nuser.name = \"skippyjonjones\"\nuser.email = \"sjj@example.com\"\nkeystone.users.create = mock.Mock(return_value=user)\ncheck_mode = True\n\n# Code under test\n(changed, id) = keystone_user.ensure_user_exists(keystone,\n                             user_name=\"skippyjonjones\",\n                             password=\"1234567\",\n                             email=\"sjj@example.com\",\n                             tenant_name=\"acme\",\n                             check_mode=check_mode)\n\n# Assertions\nassert changed\nassert_is_none(id)\nassert not keystone.users.create.called", "path": "openstack_modules\\tests\\test_keystone_user.py", "repo_name": "gc3-uzh-ch/ansible-playbooks", "stars": 107, "license": "None", "language": "python", "size": 790}
{"docstring": "\"\"\"Create a tenant, user, and role\"\"\"\n", "func_signal": "def setup_tenant_user_role():\n", "code": "keystone = mock.MagicMock()\n\ntenant = mock.Mock()\ntenant.id = \"21b505b9cbf84bdfba60dc08cc2a4b8d\"\ntenant.name = \"acme\"\ntenant.description = \"The acme tenant\"\nkeystone.tenants.list = mock.Mock(return_value=[tenant])\n\nuser = mock.Mock()\nuser.id = \"24073d9426ab4bc59527955d7c486179\"\nuser.name = \"johndoe\"\nkeystone.users.list = mock.Mock(return_value=[user])\n\nrole = mock.Mock()\nrole.id = \"34a699ab89d04c38894bbf3d998e5229\"\nrole.name = \"admin\"\nkeystone.roles.list = mock.Mock(return_value=[role])\n\nreturn keystone", "path": "openstack_modules\\tests\\test_keystone_user.py", "repo_name": "gc3-uzh-ch/ansible-playbooks", "stars": 107, "license": "None", "language": "python", "size": 790}
{"docstring": "\"\"\" dispatch with tenant and user\"\"\"\n# Setup\n", "func_signal": "def test_dispatch_user_when_present(mock_ensure_user_exists):\n", "code": "keystone = setup_tenant_user_role()\nmock_ensure_user_exists.return_value = (True,\n                                   \"0a6f3697fc314279b1a22c61d40c0919\")\n\n# Code under test\nres = keystone_user.dispatch(keystone, tenant=\"acme\", user=\"root\",\n                             email=\"admin@example.com\",\n                             password=\"12345\")\n\n# Assertions\nmock_ensure_user_exists.assert_called_with(keystone, \"root\",\n                                           \"12345\", \"admin@example.com\",\n                                           \"acme\", False)\n\nassert_equal(res,\n    dict(changed=True, id=\"0a6f3697fc314279b1a22c61d40c0919\"))", "path": "openstack_modules\\tests\\test_keystone_user.py", "repo_name": "gc3-uzh-ch/ansible-playbooks", "stars": 107, "license": "None", "language": "python", "size": 790}
{"docstring": "\"\"\" ensure_role_exists when role does not exist yet, check mode \"\"\"\n# Setup\n", "func_signal": "def test_ensure_role_exists_when_role_is_absent_check():\n", "code": "keystone = setup_tenant_user_role()\nkeystone.roles.create = mock.Mock(return_value=mock.Mock(\n    id=\"40b14f9c2d114b38b3f6bced49a792b8\"))\nkeystone.roles.roles_for_user = mock.Mock(return_value=[])\ncheck_mode = True\n\n# Code under test\nuser = \"johndoe\"\ntenant = \"acme\"\nrole = \"webuser\"\n(changed, id) = keystone_user.ensure_role_exists(keystone, user, tenant,\n                                                role, check_mode)\n\n# Assertions\nassert changed\nassert_equal(id, None)\nassert not keystone.roles.create.called", "path": "openstack_modules\\tests\\test_keystone_user.py", "repo_name": "gc3-uzh-ch/ansible-playbooks", "stars": 107, "license": "None", "language": "python", "size": 790}
{"docstring": "\"\"\" dispatch with tenant, user and role\"\"\"\n", "func_signal": "def test_dispatch_role_present(mock_ensure_role_exists):\n", "code": "keystone = setup_tenant_user_role()\nmock_ensure_role_exists.return_value = (True,\n                                   \"7df22b53d9c4405f92032c802178a31e\")\n\n# Code under test\nres = keystone_user.dispatch(keystone, tenant=\"acme\", user=\"root\",\n                             role=\"admin\")\n\n# Assertions\nmock_ensure_role_exists.assert_called_with(keystone, \"root\",\n                                           \"acme\", \"admin\", False)\nassert_equal(res,\n    dict(changed=True, id=\"7df22b53d9c4405f92032c802178a31e\"))", "path": "openstack_modules\\tests\\test_keystone_user.py", "repo_name": "gc3-uzh-ch/ansible-playbooks", "stars": 107, "license": "None", "language": "python", "size": 790}
{"docstring": "\"\"\" ensure_tenant_exists with a change in description \"\"\"\n# Setup\n", "func_signal": "def test_change_tenant_description():\n", "code": "keystone = setup_tenant_user_role()\n\n# Code under test\n(changed, id) = keystone_user.ensure_tenant_exists(keystone, \"acme\",\n                \"The foo tenant with a description change\", False)\n\n# Assertions\nassert changed\nassert_equal(id, \"21b505b9cbf84bdfba60dc08cc2a4b8d\")", "path": "openstack_modules\\tests\\test_keystone_user.py", "repo_name": "gc3-uzh-ch/ansible-playbooks", "stars": 107, "license": "None", "language": "python", "size": 790}
{"docstring": "\"\"\" tenant_exists when tenant does exist\"\"\"\n# Setup\n", "func_signal": "def test_tenant_exists_when_present():\n", "code": "keystone = setup_tenant_user_role()\n\n# Code under test\nassert keystone_user.tenant_exists(keystone, \"acme\")", "path": "openstack_modules\\tests\\test_keystone_user.py", "repo_name": "gc3-uzh-ch/ansible-playbooks", "stars": 107, "license": "None", "language": "python", "size": 790}
{"docstring": "\"\"\" ensure_tenant_exists when tenant does not exist \"\"\"\n# Setup\n", "func_signal": "def test_ensure_tenant_exists_when_absent():\n", "code": "keystone = setup_tenant_user_role()\nkeystone.tenants.create = mock.Mock(return_value=mock.Mock(\n    id=\"7c310f797aa045898e2884a975ab32ab\"))\ncheck_mode = False\n\n# Code under test\n(changed, id) = keystone_user.ensure_tenant_exists(keystone, \"bar\",\n                \"The bar tenant\", check_mode)\n\n# Assertions\nassert changed\nassert_equal(id, \"7c310f797aa045898e2884a975ab32ab\")\nkeystone.tenants.create.assert_called_with(tenant_name=\"bar\",\n                                           description=\"The bar tenant\",\n                                           enabled=True)", "path": "openstack_modules\\tests\\test_keystone_user.py", "repo_name": "gc3-uzh-ch/ansible-playbooks", "stars": 107, "license": "None", "language": "python", "size": 790}
{"docstring": "\"\"\" ensure_tenant_exists when tenant does exists \"\"\"\n\n# Setup\n", "func_signal": "def test_ensure_tenant_exists_when_present():\n", "code": "keystone = setup_tenant_user_role()\ncheck_mode = False\n\n# Code under test\n(changed, id) = keystone_user.ensure_tenant_exists(keystone, \"acme\",\n                \"The acme tenant\", check_mode)\n\n# Assertions\nassert not changed\nassert_equal(id, \"21b505b9cbf84bdfba60dc08cc2a4b8d\")", "path": "openstack_modules\\tests\\test_keystone_user.py", "repo_name": "gc3-uzh-ch/ansible-playbooks", "stars": 107, "license": "None", "language": "python", "size": 790}
{"docstring": "\"\"\" ensure_tenant_exists when tenant does exists, check mode \"\"\"\n\n# Setup\n", "func_signal": "def test_ensure_tenant_exists_when_present_check():\n", "code": "keystone = setup_tenant_user_role()\ncheck_mode = True\n\n# Code under test\n(changed, id) = keystone_user.ensure_tenant_exists(keystone, \"acme\",\n                \"The acme tenant\", check_mode)\n\n# Assertions\nassert not changed\nassert_equal(id, \"21b505b9cbf84bdfba60dc08cc2a4b8d\")", "path": "openstack_modules\\tests\\test_keystone_user.py", "repo_name": "gc3-uzh-ch/ansible-playbooks", "stars": 107, "license": "None", "language": "python", "size": 790}
{"docstring": "\"\"\" tenant_exists when tenant does not exist\"\"\"\n# Setup\n", "func_signal": "def test_tenant_exists_when_absent():\n", "code": "keystone = setup_tenant_user_role()\n\n# Code under test\nassert not keystone_user.tenant_exists(keystone, \"bar\")", "path": "openstack_modules\\tests\\test_keystone_user.py", "repo_name": "gc3-uzh-ch/ansible-playbooks", "stars": 107, "license": "None", "language": "python", "size": 790}
{"docstring": "\"\"\" ensure_role_exists when role exists but not associated yet, check mode\n\"\"\"\n# Setup\n", "func_signal": "def test_ensure_role_exists_when_role_is_present_but_not_associated_check():\n", "code": "keystone = setup_tenant_user_role()\nkeystone.roles.roles_for_user = mock.Mock(return_value=[])\ncheck_mode = True\n\n# Code under test\nuser = \"johndoe\"\ntenant = \"acme\"\nrole = \"admin\"\n(changed, id) = keystone_user.ensure_role_exists(keystone, user, tenant,\n                                                role, check_mode)\n\n# Assertions\nassert changed\nassert_is_none(id)\nassert not keystone.roles.create.called", "path": "openstack_modules\\tests\\test_keystone_user.py", "repo_name": "gc3-uzh-ch/ansible-playbooks", "stars": 107, "license": "None", "language": "python", "size": 790}
{"docstring": "\"\"\" ensure_user_exists when user does not exist\"\"\"\n# Setup\n", "func_signal": "def test_ensure_user_exists_when_absent():\n", "code": "keystone = setup_tenant_user_role()\nuser = mock.Mock()\nuser.id = \"5ce4b6ef2e814a4897907cc6db879536\"\nuser.name = \"skippyjonjones\"\nuser.email = \"sjj@example.com\"\nkeystone.users.create = mock.Mock(return_value=user)\ncheck_mode = False\n\n# Code under test\n(changed, id) = keystone_user.ensure_user_exists(keystone,\n                             user_name=\"skippyjonjones\",\n                             password=\"1234567\",\n                             email=\"sjj@example.com\",\n                             tenant_name=\"acme\",\n                             check_mode=check_mode)\n\n# Assertions\nassert changed\nassert_equal(id, \"5ce4b6ef2e814a4897907cc6db879536\")\nkeystone.users.create.assert_called_with(\n    name='skippyjonjones',\n    password='1234567',\n    email='sjj@example.com',\n    tenant_id='21b505b9cbf84bdfba60dc08cc2a4b8d')", "path": "openstack_modules\\tests\\test_keystone_user.py", "repo_name": "gc3-uzh-ch/ansible-playbooks", "stars": 107, "license": "None", "language": "python", "size": 790}
{"docstring": "\"\"\" ensure_role_exists when role exists and is associated properly,\n    check mode \"\"\"\n# Setup\n", "func_signal": "def test_ensure_role_exists_when_present_check():\n", "code": "keystone = setup_tenant_user_role()\nrole = mock.Mock()\nrole.id = \"34a699ab89d04c38894bbf3d998e5229\"\nrole.name = \"admin\"\nkeystone.roles.roles_for_user = mock.Mock(return_value=[role])\ncheck_mode = True\n\n# Code under test\nuser = \"johndoe\"\ntenant = \"acme\"\nrole = \"admin\"\n(changed, id) = keystone_user.ensure_role_exists(keystone, user, tenant,\n                                                 role, check_mode)\n\n# Assertions\nassert not changed\nassert_equal(id, \"34a699ab89d04c38894bbf3d998e5229\")\nassert not keystone.roles.create.called", "path": "openstack_modules\\tests\\test_keystone_user.py", "repo_name": "gc3-uzh-ch/ansible-playbooks", "stars": 107, "license": "None", "language": "python", "size": 790}
{"docstring": "\"\"\"calculates digest response (MD5 and qop)\"\"\"\n", "func_signal": "def prove_auth(app, req):\n", "code": "auth = req.authorization\n\naccount = app.db.accounts.find_one({'email': auth.username})\n_A1 = account['passwd'] if account else standard_b64encode(urandom(16))\n\nif str(auth.get('qop', '')) == 'auth':\n    A2 = ':'.join([auth.nonce, auth.nc, auth.cnonce, 'auth',\n                   md5(req.method + ':' + auth.uri)])\n    return md5(_A1 + ':' + A2)\nelse:\n    # compatibility with RFC 2069: https://tools.ietf.org/html/rfc2069\n    A2 = ':'.join([auth.nonce, md5(req.method + ':' + auth.uri)])\n    return md5(_A1 + ':' + A2)", "path": "regenwolken\\utils.py", "repo_name": "posativ/regenwolken", "stars": 112, "license": "other", "language": "python", "size": 657}
{"docstring": "\"\"\"Upload a file, when the client has a valid session key.\n\n-- http://developer.getcloudapp.com/upload-file\"\"\"\n\n", "func_signal": "def index():\n", "code": "db, fs = current_app.db, current_app.fs\nconfig, sessions = current_app.config, current_app.sessions\n\nif request.method == 'POST' and not request.accept_mimetypes.accept_html:\n\n    try:\n        account = sessions.pop(request.form.get('key'))['account']\n    except KeyError:\n        abort(401)\n\n    acc = db.accounts.find_one({'email': account})\n    source = request.headers.get('User-Agent', 'Regenschirm++/1.0').split(' ', 1)[0]\n    privacy = request.form.get('acl', acc['private_items'])\n\n    _id = fs.upload_file(config, account, request.files.get('file'), source, privacy)\n\n    items = acc['items']\n    items.append(_id)\n    db.accounts.update({'_id': acc['_id']}, {'$set': {'items': items}}, upsert=False)\n\n    obj = fs.get(_id)\n\n    if obj is None:\n        abort(400)\n    else:\n        return jsonify(Item(obj, config, urlscheme(request)))\nelse:\n    users = db.accounts.find().count()\n    files = fs.gfs._GridFS__files.count()\n    size = sum([f['length'] for f in fs.gfs._GridFS__files.find()])\n    hits = sum([f['view_counter'] for f in fs.mdb.find()])\n\n    if request.args.get('format') == 'csv':\n        fields = [('users', users), ('files', files), ('size', size), ('hits', hits)]\n        return Response('\\n'.join('%s,%s' % field for field in fields), 200)\n\n    return Response(render_template(\"index.html\", **locals()), 200, content_type=\"text/html\")", "path": "regenwolken\\views.py", "repo_name": "posativ/regenwolken", "stars": 112, "license": "other", "language": "python", "size": 657}
{"docstring": "\"\"\"JSON-compatible dict representing Item.\n\n    href:           used for renaming -> http://developer.getcloudapp.com/rename-item\n    name:           item's name, taken from filename\n    private:        requires auth when viewing\n    subscribed:     true or false, when paid for \"Pro\"\n    url:            url to this file\n    content_url:    <unknown>\n    item_type:      image, bookmark, ... there are more\n    view_counter:   obviously\n    icon:           some picture to display `item_type`\n    remote_url:     <unknown>, href + quoted name\n    thumbnail_url:  <url to thumbnail, when used?>\n    redirect_url:   redirection url in bookmark items\n    source:         client name\n    created_at:     timestamp created - '%Y-%m-%dT%H:%M:%SZ' UTC\n    updated_at:     timestamp updated - '%Y-%m-%dT%H:%M:%SZ' UTC\n    deleted_at:     timestamp deleted - '%Y-%m-%dT%H:%M:%SZ' UTC\n\"\"\"\n\n", "func_signal": "def Item(obj, conf, scheme='http'):\n", "code": "x = {}\nif isinstance(obj, dict):\n    obj = Struct(**obj)\n\nresult = {\n    \"href\": \"%s://%s/items/%s\" % (scheme, conf['HOSTNAME'], obj._id),\n    \"private\": obj.private,\n    \"subscribed\": True,\n    \"item_type\": obj.item_type,\n    \"view_counter\": obj.view_counter,\n    \"icon\": \"%s://%s/images/item_types/%s.png\" % (scheme, conf['HOSTNAME'], obj.item_type),\n    \"source\": obj.source,\n    \"created_at\": strftime('%Y-%m-%dT%H:%M:%SZ', gmtime()),\n    \"updated_at\": strftime('%Y-%m-%dT%H:%M:%SZ', gmtime()),\n    \"deleted_at\": None }\n\nif obj.item_type == 'bookmark':\n    x['name'] = obj.name\n    x['url'] = scheme + '://' + conf['HOSTNAME'] + '/' + obj.short_id\n    x['content_url'] = x['url'] + '/content'\n    x['remote_url'] = None\n    x['redirect_url'] = obj.redirect_url\nelse:\n    x['name'] = obj.filename\n    x['url'] = scheme + '://' + conf['HOSTNAME'] + '/' + obj.short_id\n    x['content_url'] = x['url'] + '/' + secure_filename(obj.filename)\n    x['remote_url'] = x['url'] + '/' + url_quote(obj.filename)\n    x['thumbnail_url'] = x['url'] # TODO: thumbails\n    x['redirect_url'] = None\n\ntry:\n    x['created_at'] = obj.created_at\n    x['updated_at'] = obj.updated_at\n    x['deleted_at'] = obj.deleted_at\n    if obj.deleted_at:\n        x['icon'] = scheme + \"://\" + conf['HOSTNAME'] + \"/images/item_types/trash.png\"\nexcept AttributeError:\n    pass\n\nresult.update(x)\nreturn result", "path": "regenwolken\\specs.py", "repo_name": "posativ/regenwolken", "stars": 112, "license": "other", "language": "python", "size": 657}
{"docstring": "\"\"\"Yet another URL shortener. This implementation prefixes bookmarks with\na dash (-) so\n\n-- http://developer.getcloudapp.com/bookmark-link\"\"\"\n\n", "func_signal": "def bookmark():\n", "code": "conf, db = current_app.config, current_app.db\n\n# XXX move logic into mongonic.py\ndef insert(name, redirect_url):\n\n    acc = db.accounts.find_one({'email': request.authorization.username})\n\n    _id = str(getrandbits(32))\n    retry_count = 1\n    short_id_length = conf['SHORT_ID_MIN_LENGTH']\n\n    while True:\n        short_id = slug(short_id_length)\n        if not db.items.find_one({'short_id': short_id}):\n            break\n        else:\n            retry_count += 1\n            if retry_count > 3:\n                short_id_length += 1\n                retry_count = 1\n\n    x = {\n        'account': request.authorization.username,\n        'name': name,\n        '_id': _id,\n        'short_id': slug(short_id_length),\n        'redirect_url': redirect_url,\n        'item_type': 'bookmark',\n        'view_counter': 0,\n        'private': request.form.get('acl', acc['private_items'])\n            if conf['ALLOW_PRIVATE_BOOKMARKS'] else False,\n        'source': request.headers.get('User-Agent', 'Regenschirm++/1.0').split(' ', 1)[0],\n        'created_at': strftime('%Y-%m-%dT%H:%M:%SZ', gmtime()),\n        'updated_at': strftime('%Y-%m-%dT%H:%M:%SZ', gmtime()),\n    }\n\n    item = Item(x, conf, urlscheme(request))\n    db.items.insert(x)\n\n    items = acc['items']\n    items.append(_id)\n    db.accounts.update({'_id': acc['_id']}, {'$set': {'items': items}}, upsert=False)\n\n    return item\n\ntry:\n    data = json.loads(request.data)\n    data = data['item']\nexcept (ValueError, KeyError):\n    return ('Unprocessable Entity', 422)\n\nif isinstance(data, list):\n    return json.dumps([insert(d['name'], d['redirect_url']) for d in data])\nelse:\n    return jsonify(insert(data['name'], data['redirect_url']))", "path": "regenwolken\\views.py", "repo_name": "posativ/regenwolken", "stars": 112, "license": "other", "language": "python", "size": 657}
{"docstring": "'''pretty-print filesize.\nhttp://blogmag.net/blog/read/38/Print_human_readable_file_size'''\n", "func_signal": "def ppsize(num):\n", "code": "for x in ['bytes','KiB','MiB','GiB','TB']:\n    if num < 1024.0:\n        return \"%3.2f %s\" % (num, x)\n    num /= 1024.0", "path": "regenwolken\\utils.py", "repo_name": "posativ/regenwolken", "stars": 112, "license": "other", "language": "python", "size": 657}
{"docstring": "\"\"\"return the current scheme (HTTP or HTTPS)\"\"\"\n\n", "func_signal": "def urlscheme(request):\n", "code": "if request.url.startswith('https://'):\n    return 'https'\nreturn request.headers.get('X-Forwarded-Proto', 'http')", "path": "regenwolken\\utils.py", "repo_name": "posativ/regenwolken", "stars": 112, "license": "other", "language": "python", "size": 657}
{"docstring": "\"\"\"JSON-compatible dict representing cloudapp's account\n\n    domain:           custom domain, only in Pro available\n    domain_home_page: http://developer.getcloudapp.com/view-domain-details\n    private_items:    http://developer.getcloudapp.com/change-default-security\n    subscribed:       Pro feature, custom domain... we don't need this.\n    alpha:            <unkown> wtf?\n    created_at:       timestamp created - '%Y-%m-%dT%H:%M:%SZ' UTC\n    updated_at:       timestamp updated - '%Y-%m-%dT%H:%M:%SZ' UTC\n    activated_at:     timestamp account activated, per default None\n    items:            (not official) list of items by this account\n    email:            username of this account, characters can be any\n                      of \"a-zA-Z0-9.- @\" and no digit-only name is allowed\n    password:         password, md5(username + ':' + realm + ':' + passwd)\n\"\"\"\n\n", "func_signal": "def Account(account, conf, **kw):\n", "code": "result = {\n    'id': account['id'],\n    'domain': conf['HOSTNAME'],\n    'domain_home_page': None,\n    'private_items': False,\n    'subscribed': True,\n    'subscription_expires_at': '2112-12-21',\n    'alpha': False,\n    'created_at': strftime('%Y-%m-%dT%H:%M:%SZ', gmtime()),\n    'updated_at': strftime('%Y-%m-%dT%H:%M:%SZ', gmtime()),\n    'activated_at': None,\n    \"items\": [],\n    'email': account['email'],\n    'passwd': A1(account['email'], account['passwd'])\n}\n\nresult.update(kw)\nreturn result", "path": "regenwolken\\specs.py", "repo_name": "posativ/regenwolken", "stars": 112, "license": "other", "language": "python", "size": 657}
{"docstring": "\"\"\"returns bookmark or file either as direct download with human-readable,\noriginal filename or inline display using whitelisting\"\"\"\n\n", "func_signal": "def blob(short_id, filename):\n", "code": "fs = current_app.fs\n\nobj = fs.get(short_id=short_id)\nif obj is None or getattr(obj, 'deleted_at', None):\n    abort(404)\n\n# views++\nfs.inc_count(obj._id)\n\nif obj.item_type == 'bookmark':\n    return redirect(obj.redirect_url)\nelif not obj.content_type.split('/', 1)[0] in ['image', 'text']:\n    return Response(obj, content_type=obj.content_type, headers={'Content-Disposition':\n                'attachment; filename=\"%s\"' % basename(obj.filename)})\nreturn Response(obj, content_type=obj.content_type)", "path": "regenwolken\\views.py", "repo_name": "posativ/regenwolken", "stars": 112, "license": "other", "language": "python", "size": 657}
{"docstring": "\"\"\"Generates a new key for the upload process.  Timeout after 60 minutes!\n\n-- http://developer.getcloudapp.com/upload-file\n-- http://developer.getcloudapp.com/upload-file-with-specific-privacy\"\"\"\n\n", "func_signal": "def items_new():\n", "code": "acc = current_app.db.accounts.find_one({'email': request.authorization.username})\nParseResult = urlparse(request.url)\nprivacy = 'private' if acc['private_items'] else 'public-read'\n\nif not ParseResult.query == '':\n    query = dict([part.split('=', 1) for part in ParseResult.query.split('&')])\n    privacy = 'private' if query.get('item[private]', None) else 'public-read'\n\n\nkey = current_app.sessions.new(request.authorization.username)\nres = { \"url\": urlscheme(request) + '://' + current_app.config['HOSTNAME'],\n      \"max_upload_size\": current_app.config['MAX_CONTENT_LENGTH'],\n      \"params\": { \"acl\": privacy,\n                  \"key\": key\n                },\n    }\n\nreturn jsonify(res)", "path": "regenwolken\\views.py", "repo_name": "posativ/regenwolken", "stars": 112, "license": "other", "language": "python", "size": 657}
{"docstring": "\"\"\"Show current item count and other statistics.\n\n-- http://developer.getcloudapp.com/view-account-stats\"\"\"\n\n", "func_signal": "def account_stats():\n", "code": "email = request.authorization.username\nitems = current_app.db.accounts.find_one({'email': email})['items']\nviews = 0\nfor item in items:\n    views += current_app.db.items.find_one({'_id': item})['view_counter']\n\nreturn jsonify({'items': len(items), 'views': views})", "path": "regenwolken\\views.py", "repo_name": "posativ/regenwolken", "stars": 112, "license": "other", "language": "python", "size": 657}
{"docstring": "\"\"\"login decorator using HTTP Digest Authentication.  Pattern based on\nhttp://flask.pocoo.org/docs/patterns/viewdecorators/\n\n-- http://developer.getcloudapp.com/usage/#authentication\"\"\"\n\n", "func_signal": "def login(f):\n", "code": "app = flask.current_app\n\ndef dec(*args, **kwargs):\n    \"\"\"This decorater function will send an authenticate header, if none\n    is present and denies access, if HTTP Digest Auth failed.\"\"\"\n\n    request = flask.request\n    usehtml = request.accept_mimetypes.accept_html\n\n    if not request.authorization:\n        response = Response(\n            'Unauthorized', 401,\n            content_type='text/html; charset=utf-8' if usehtml else 'application/json'\n        )\n        response.www_authenticate.set_digest(\n            'Application', algorithm='MD5',\n            nonce=standard_b64encode(urandom(32)),\n            qop=('auth', ), opaque='%x' % getrandbits(128))\n        return response\n    else:\n        account = app.db.accounts.find_one({'email': request.authorization.username})\n        if account and account['activated_at'] == None:\n            return Response('[ \"Your account hasn\\'t been activated. Please ' \\\n                            + 'check your email and activate your account.\" ]', 409)\n        elif prove_auth(app, request) != request.authorization.response:\n            return Response('Forbidden', 403)\n    return f(*args, **kwargs)\nreturn dec", "path": "regenwolken\\utils.py", "repo_name": "posativ/regenwolken", "stars": 112, "license": "other", "language": "python", "size": 657}
{"docstring": "\"\"\"rename/delete/change privacy of an item.\n\n-- http://developer.getcloudapp.com/rename-item\n-- http://developer.getcloudapp.com/delete-item\n-- http://developer.getcloudapp.com/change-security-of-item\"\"\"\n\n", "func_signal": "def items_edit(object_id):\n", "code": "conf, db, fs = current_app.config, current_app.db, current_app.fs\nitem = db.items.find_one({'account': request.authorization.username,\n                          '_id': object_id})\nif not item:\n    abort(404)\n\nif request.method == 'DELETE':\n    item['deleted_at'] = strftime('%Y-%m-%dT%H:%M:%SZ', gmtime())\nelif request.method == 'PUT':\n    try:\n        data = json.loads(request.data)['item']\n        key, value = data.items()[0]\n        if not key in ['private', 'name', 'deleted_at']: raise ValueError\n    except ValueError:\n        return ('Unprocessable Entity', 422)\n\n    if key == 'name' and item['item_type'] != 'bookmark':\n        item['filename'] = value\n    elif key == 'private' and item['item_type'] == 'bookmark' and value \\\n    and not conf['ALLOW_PRIVATE_BOOKMARKS']:\n        pass\n    else:\n        item[key] = value\n\n    item['updated_at'] = strftime('%Y-%m-%dT%H:%M:%SZ', gmtime())\n\ndb.items.save(item)\nitem = fs.get(item['_id'])\nreturn jsonify(Item(item, conf, urlscheme(request)))", "path": "regenwolken\\views.py", "repo_name": "posativ/regenwolken", "stars": 112, "license": "other", "language": "python", "size": 657}
{"docstring": "\"\"\"generate png thumbnails\"\"\"\n\n", "func_signal": "def thumbnail(fp, size=128, bs=2048):\n", "code": "p = ImageFile.Parser()\n\ntry:\n    while True:\n        s = fp.read(bs)\n        if not s:\n            break\n        p.feed(s)\n\n    img = p.close()\n    img.thumbnail((size, size))\n    op = io.BytesIO()\n    img.save(op, 'PNG')\n    op.seek(0)\n    return op.read().encode('base64')\nexcept IOError:\n    raise", "path": "regenwolken\\utils.py", "repo_name": "posativ/regenwolken", "stars": 112, "license": "other", "language": "python", "size": 657}
{"docstring": "\"\"\"returns 128px thumbnail, when possible and cached for 30 minutes,\notherwise item_type icons.\"\"\"\n\n# th = cache.get('thumb-'+short_id)\n# if th: return Response(standard_b64decode(th), 200, content_type='image/png')\n\n", "func_signal": "def thumb(short_id):\n", "code": "rv = current_app.fs.get(short_id=short_id)\nif rv is None or getattr(obj, 'deleted_at', None):\n    abort(404)\n\nif rv.item_type == 'image' and current_app.config['THUMBNAILS']:\n    try:\n        th = thumbnail(rv)\n        # cache.set('thumb-'+short_id, th)\n        return Response(standard_b64decode(th), 200, content_type='image/png')\n    except IOError:\n        pass\nreturn Response(open('wolken/static/images/item_types/%s.png' % rv.item_type),\n                200, content_type='image/png')", "path": "regenwolken\\views.py", "repo_name": "posativ/regenwolken", "stars": 112, "license": "other", "language": "python", "size": 657}
{"docstring": "\"\"\"Return account details and/or update given keys.\n\n-- http://developer.getcloudapp.com/view-account-details\n-- http://developer.getcloudapp.com/change-default-security\n-- http://developer.getcloudapp.com/change-email\n-- http://developer.getcloudapp.com/change-password\n\nPUT: accepts every new password (stored in plaintext) and similar to /register\nno digit-only \"email\" address is allowed.\"\"\"\n\n", "func_signal": "def account():\n", "code": "conf, db = current_app.config, current_app.db\naccount = db.accounts.find_one({'email': request.authorization.username})\n\nif request.method == 'GET':\n    return jsonify(clear(account))\n\ntry:\n    _id = account['_id']\n    data = json.loads(request.data)['user']\nexcept ValueError:\n    return ('Unprocessable Entity', 422)\n\nif len(data.keys()) == 1 and 'private_items' in data:\n    db.accounts.update({'_id': _id}, {'$set': {'private_items': data['private_items']}})\n    account['private_items'] = data['private_items']\nelif len(data.keys()) == 2 and 'current_password' in data:\n    if not account['passwd'] == A1(account['email'], data['current_password']):\n        return abort(403)\n\n    if 'email' in data:\n        if filter(lambda c: not c in conf['ALLOWED_CHARS'], data['email']) \\\n        or data['email'].isdigit(): # no numbers allowed\n            abort(400)\n        if db.accounts.find_one({'email': data['email']}) and \\\n        account['email'] != data['email']:\n            return ('User already exists', 406)\n\n        new = {'email': data['email'],\n               'passwd': A1(data['email'], data['current_password'])}\n        db.accounts.update({'_id': _id}, {'$set': new})\n        account['email'] = new['email']\n        account['passwd'] = new['passwd']\n\n    elif 'password' in data:\n        passwd = A1(account['email'], data['password'])\n        db.accounts.update({'_id': _id}, {'$set': {'passwd': passwd}})\n        account['passwd'] = passwd\n\n    else:\n        abort(400)\n\ndb.accounts.update({'_id': account['_id']}, {'$set':\n        {'updated_at': strftime('%Y-%m-%dT%H:%M:%SZ', gmtime())}})\n\nreturn jsonify(clear(account))", "path": "regenwolken\\views.py", "repo_name": "posativ/regenwolken", "stars": 112, "license": "other", "language": "python", "size": 657}
{"docstring": "\"\"\"Registration of new users (no digits-only usernames are allowed), if\nPUBLIC_REGISTRATION is set to True new accounts are instantly activated. Otherwise\nyou have to do it manually via `manage.py activate $USER`.\n\n-- http://developer.getcloudapp.com/register\"\"\"\n\n", "func_signal": "def register():\n", "code": "conf, db = current_app.config, current_app.db\n\nif len(request.data) > 200:\n    return ('Request Entity Too Large', 413)\ntry:\n    d = json.loads(request.data)\n    email = d['user']['email']\n    if email.isdigit(): raise ValueError # no numbers as username allowed\n    passwd = d['user']['password']\nexcept (ValueError, KeyError):\n    return ('Bad Request', 422)\n\n# TODO: allow more characters, unicode -> ascii, before filter\nif filter(lambda c: not c in conf['ALLOWED_CHARS'], email):\n    return ('Bad Request', 422)\n\nif db.accounts.find_one({'email': email}) != None:\n    return ('User already exists', 406)\n\nif not db.accounts.find_one({\"_id\":\"autoinc\"}):\n    db.accounts.insert({\"_id\":\"_inc\", \"seq\": 1})\n\naccount = Account({'email': email, 'passwd': passwd,\n                   'id': db.accounts.find_one({'_id': '_inc'})['seq']}, conf)\ndb.accounts.update({'_id': '_inc'}, {'$inc': {'seq': 1}})\nif conf['PUBLIC_REGISTRATION']:\n    account['activated_at'] = strftime('%Y-%m-%dT%H:%M:%SZ', gmtime())\n\naccount['_id'] = account['id']\ndb.accounts.insert(account)\n\nreturn (jsonify(clear(account)), 201)", "path": "regenwolken\\views.py", "repo_name": "posativ/regenwolken", "stars": 112, "license": "other", "language": "python", "size": 657}
{"docstring": "\"\"\"Show items from user.  Optional query parameters:\n\n        - page (int)     - default: 1\n        - per_page (int) - default: 5\n        - type (str)     - default: None, filter by image, bookmark, text,\n                                         archive, audio, video, or unknown\n        - deleted (bool) - default: False, show trashed items\n\n-- http://developer.getcloudapp.com/list-items\"\"\"\n\n", "func_signal": "def items():\n", "code": "db, fs = current_app.db, current_app.fs\n\nParseResult = urlparse(request.url)\nparams = {'per_page': '5', 'page': '1', 'type': None, 'deleted': False,\n          'source': None}\n\nif not ParseResult.query == '':\n    query = dict([part.split('=', 1) for part in ParseResult.query.split('&')])\n    params.update(query)\n\nlisting = []\ntry:\n    pp = int(params['per_page'])\n    page = int(params['page'])\n    email = request.authorization.username\nexcept (ValueError, KeyError):\n    abort(400)\n\nquery = {'account': email}\nif params['type'] != None:\n    query['item_type'] = params['type']\nif params['deleted'] == False:\n    query['deleted_at'] = None\nif params['source'] != None:\n    query['source'] = {'$regex': '^' + unquote(params['source'])}\n\nitems = db.items.find(query)\nfor item in items.sort('updated_at', DESCENDING)[pp*(page-1):pp*page]:\n    listing.append(Item(fs.get(_id=item['_id']),\n                        current_app.config, urlscheme(request)))\nreturn json.dumps(listing[::-1])", "path": "regenwolken\\views.py", "repo_name": "posativ/regenwolken", "stars": 112, "license": "other", "language": "python", "size": 657}
{"docstring": "\"\"\"Uses heuristics to guess whether the given file is text or binary,\nby reading a single block of bytes from the file. If more than 30% of\nthe chars in the block are non-text, or there are NUL ('\\x00') bytes in\nthe block, assume this is a binary file.\n\n-- via http://eli.thegreenplace.net/2011/10/19/perls-guess-if-file-is-text-or-binary-implemented-in-python/\"\"\"\n\n# A function that takes an integer in the 8-bit range and returns a\n# single-character byte object in py3 / a single-character string in py2.\n", "func_signal": "def istext(self):\n", "code": "int2byte = (lambda x: bytes((x,))) if sys.version_info > (3, 0) else chr\n\nblocksize = 512\nchars = b''.join(int2byte(i) for i in range(32, 127)) + b'\\n\\r\\t\\f\\b'\n\nblock = self.read(blocksize); self.seek(0)\nif b'\\x00' in block:\n    # Files with null bytes are binary\n    return False\nelif not block:\n    # An empty file is considered a valid text file\n    return True\n\n# Use translate's 'deletechars' argument to efficiently remove all\n# occurrences of chars from the block\nnontext = block.translate(None, chars)\nreturn float(len(nontext)) / len(block) <= 0.30", "path": "regenwolken\\specs.py", "repo_name": "posativ/regenwolken", "stars": 112, "license": "other", "language": "python", "size": 657}
{"docstring": "\"\"\"No official API call yet.  Trash items marked as deleted. Usage:\ncurl -u user:pw --digest -H \"Accept: application/json\" -X POST http://my.cl.ly/items/trash\"\"\"\n\n", "func_signal": "def trash():\n", "code": "empty = current_app.db.items.find(\n    {'account': request.authorization.username, 'deleted_at': {'$ne': None}})\n\nfor item in empty:\n    current_app.fs.delete(item)\n\nreturn '', 200", "path": "regenwolken\\views.py", "repo_name": "posativ/regenwolken", "stars": 112, "license": "other", "language": "python", "size": 657}
{"docstring": "\"\"\"converts human-readable time deltas to datetime.timedelta.\n>>> tdelta(3w 12m) == datetime.timedelta(weeks=3, minutes=12)\"\"\"\n\n", "func_signal": "def tdelta(input):\n", "code": "keys = ['weeks', 'days', 'hours', 'minutes']\nregex = ''.join(['((?P<%s>\\d+)%s ?)?' % (k, k[0]) for k in keys])\nkwargs = {}\nfor k,v in re.match(regex, input).groupdict(default='0').items():\n    kwargs[k] = int(v)\nreturn timedelta(**kwargs)", "path": "regenwolken\\manage.py", "repo_name": "posativ/regenwolken", "stars": 112, "license": "other", "language": "python", "size": 657}
{"docstring": "# Lossy compression is hard to test, since we don't know to which degree\n# \"compressible\" items will be compressed. This test only checks that\n# non-compressible things stay the same.\n", "func_signal": "def testLossy(self):\n", "code": "src = textwrap.dedent(\"\"\"\n    def foo(a: int) -> float raises IndexError\n    def foo(a: str) -> complex raises AssertionError\n\"\"\")\nflags = optimize.OptimizeFlags(lossy=True, use_abcs=True, max_union=4,\n                               remove_mutable=False)\nself.AssertSourceEquals(\n    optimize.Optimize(self.Parse(src), flags),\n    src)", "path": "optimize_test.py", "repo_name": "google/pytypedecl", "stars": 66, "license": "other", "language": "python", "size": 735}
{"docstring": "\"\"\"template : LBRACKET template_items RBRACKET\"\"\"\n", "func_signal": "def p_template(self, p):\n", "code": "p[0] = p[2]\n# Verify we don't have duplicate identifiers.\nnames = [template.name for template in p[2]]\nfor name in names:\n  if names.count(name) > 1:\n    make_syntax_error(self, 'Duplicate name %s' % name, p)", "path": "parse\\parser.py", "repo_name": "google/pytypedecl", "stars": 66, "license": "other", "language": "python", "size": 735}
{"docstring": "\"\"\"'Decorate' a given class. Only stores it for later.\"\"\"\n", "func_signal": "def __call__(self, cls):\n", "code": "self._mapping[cls.__name__] = cls\nreturn cls", "path": "parse\\decorate.py", "repo_name": "google/pytypedecl", "stars": 66, "license": "other", "language": "python", "size": 735}
{"docstring": "\"\"\"version_expr : NAME RBRACKET NUMBER\"\"\"\n", "func_signal": "def p_version_expr_lt(self, p):\n", "code": "self.lexer.CancelRBracket()\nCheckStringIsPython(self, p[1], p)\np[0] = self.python_version > p[3].AsVersion(self, p)", "path": "parse\\parser.py", "repo_name": "google/pytypedecl", "stars": 66, "license": "other", "language": "python", "size": 735}
{"docstring": "\"\"\"version_expr : NAME EQ NUMBER\"\"\"\n", "func_signal": "def p_version_expr_eq(self, p):\n", "code": "CheckStringIsPython(self, p[1], p)\np[0] = self.python_version == p[3].AsVersion(self, p)", "path": "parse\\parser.py", "repo_name": "google/pytypedecl", "stars": 66, "license": "other", "language": "python", "size": 735}
{"docstring": "\"\"\"Replace a tree of nodes with nodes registered as replacements.\n\nThis will walk the tree and replace each class with a class of the same\nname previously registered by using this class as a class decorator.\n\nArgs:\n  node: A pytd node.\n\nReturns:\n  A new tree, with given nodes taken over by their replacement classes.\n\"\"\"\n", "func_signal": "def Visit(self, node):\n", "code": "mapping = self._mapping\n\n# Build a visitor that performs the old_class -> new_class mapping:\nclass Visitor(object):\n  name_to_class = mapping\n  for name, new_cls in mapping.iteritems():\n\n    def Visit(self, node):\n      # Python doesn't allow us to build this as a closure, so we have to\n      # use the clunky way of retrieving the replacement class.\n      cls = self.name_to_class[node.__class__.__name__]\n      return cls(*node)\n    locals()[\"Visit\" + name] = Visit\nreturn node.Visit(Visitor())", "path": "parse\\decorate.py", "repo_name": "google/pytypedecl", "stars": 66, "license": "other", "language": "python", "size": 735}
{"docstring": "# This is a test for intermediate data. See AbsorbMutableParameters class\n# pydoc about how AbsorbMutableParameters works on methods.\n", "func_signal": "def testAbsorbMutableParametersFromMethods(self):\n", "code": "src = textwrap.dedent(\"\"\"\n    class MyClass<T>:\n        def append<NEW>(self, x: NEW) -> ?:\n            self := MyClass<T or NEW>\n\"\"\")\nexpected = textwrap.dedent(\"\"\"\n    class MyClass<T>:\n        def append<NEW>(self: MyClass<T or NEW>, x: NEW) -> ?\n\"\"\")\ntree = self.Parse(src)\nnew_tree = tree.Visit(optimize.AbsorbMutableParameters())\nnew_tree = new_tree.Visit(optimize.CombineContainers())\nself.AssertSourceEquals(new_tree, expected)", "path": "optimize_test.py", "repo_name": "google/pytypedecl", "stars": 66, "license": "other", "language": "python", "size": 735}
{"docstring": "\"\"\"Initialize.\n\nParameters:\n  version: A tuple of three numbers: (major, minor, micro).\n           E.g. (3,4,0).\n  kwargs: Additional parameters to pass to yacc.yacc().\n\"\"\"\n# TODO: Don't generate the lex/yacc tables each time. This should\n#                  be done by a separate program that imports this module\n#                  and calls yacc.yacc(write_tables=True,\n#                  outputdir=$GENFILESDIR, tabmodule='pytypedecl_parser')\n#                  and similar params for lex.lex(...).  Then:\n#                    import pytypdecl_parser\n#                    self.parser = yacc.yacc(tabmodule=pytypedecl_parser)\n#                  [might also need optimize=True]\n", "func_signal": "def __init__(self, version=None, **kwargs):\n", "code": "self.lexer = PyLexer()\nself.tokens = self.lexer.tokens\nself.python_version = version or DEFAULT_VERSION\n\nself.parser = yacc.yacc(\n    start='start',  # warning: ply ignores this\n    module=self,\n    debug=False,\n    write_tables=False,\n    # debuglog=yacc.PlyLogger(sys.stderr),\n    # errorlog=yacc.NullLogger(),  # If you really want to suppress messages\n    **kwargs)", "path": "parse\\parser.py", "repo_name": "google/pytypedecl", "stars": 66, "license": "other", "language": "python", "size": 735}
{"docstring": "\"\"\"funcdef : DEF NAME template LPAREN params RPAREN return raises signature maybe_body\"\"\"\n#            1   2    3        4      5      6      7      8      9         10\n# TODO: Output a warning if we already encountered a signature\n#              with these types (but potentially different argument names)\n", "func_signal": "def p_funcdef(self, p):\n", "code": "if p[2] == '__init__' and isinstance(p[7], pytd.AnythingType):\n  # for __init__, the default return value is None\n  ret = pytd.NamedType('NoneType')\nelse:\n  ret = p[7]\nsignature = pytd.Signature(params=tuple(p[5].required), return_type=ret,\n                           exceptions=tuple(p[8]), template=tuple(p[3]),\n                           has_optional=p[5].has_optional)\nfor mutator in p[10]:\n  signature = signature.Visit(mutator)\n  if not mutator.successful:\n    make_syntax_error(self, 'No parameter named %s' % mutator.name, p)\np[0] = NameAndSig(name=p[2], signature=signature)", "path": "parse\\parser.py", "repo_name": "google/pytypedecl", "stars": 66, "license": "other", "language": "python", "size": 735}
{"docstring": "\"\"\"version_expr : NAME NE NUMBER\"\"\"\n", "func_signal": "def p_version_expr_ne(self, p):\n", "code": "CheckStringIsPython(self, p[1], p)\np[0] = self.python_version != p[3].AsVersion(self, p)", "path": "parse\\parser.py", "repo_name": "google/pytypedecl", "stars": 66, "license": "other", "language": "python", "size": 735}
{"docstring": "\"\"\"classdef : CLASS NAME template parents COLON INDENT class_funcs DEDENT\"\"\"\n#             1     2    3        4       5     6\n", "func_signal": "def p_classdef(self, p):\n", "code": "funcdefs = [x for x in p[7] if isinstance(x, NameAndSig)]\nconstants = [x for x in p[7] if isinstance(x, pytd.Constant)]\nif (set(f.name for f in funcdefs) | set(c.name for c in constants) !=\n    set(d.name for d in p[7])):\n  # TODO: raise a syntax error right when the identifier is defined.\n  raise make_syntax_error(self, 'Duplicate identifier(s)', p)\n# Check that template parameter names are unique:\ntemplate_names = {t.name for t in p[3]}\nfor _, sig in funcdefs:\n  for t in sig.template:\n    if t.name in template_names:\n      raise make_syntax_error(self, 'Duplicate template parameter %s' %\n                              t.name, p)\n\nif p[4] == [pytd.NothingType()]:\n  bases = ()\nelse:\n  # Everything implicitly subclasses \"object\"\n  bases = tuple(p[4]) or (pytd.NamedType('object'),)\ncls = pytd.Class(name=p[2], parents=bases,\n                 methods=tuple(MergeSignatures(funcdefs)),\n                 constants=tuple(constants), template=tuple(p[3]))\np[0] = cls.Visit(visitors.AdjustSelf())", "path": "parse\\parser.py", "repo_name": "google/pytypedecl", "stars": 66, "license": "other", "language": "python", "size": 735}
{"docstring": "\"\"\"unit : alldefs\"\"\"\n", "func_signal": "def p_unit(self, p):\n", "code": "funcdefs = [x for x in p[1] if isinstance(x, NameAndSig)]\nconstants = [x for x in p[1] if isinstance(x, pytd.Constant)]\nclasses = [x for x in p[1] if isinstance(x, pytd.Class)]\nall_names = (list(set(f.name for f in funcdefs)) +\n             [c.name for c in constants] +\n             [c.name for c in classes])\nduplicates = [name\n              for name, count in collections.Counter(all_names).items()\n              if count >= 2]\nif duplicates:\n  make_syntax_error(\n      self, 'Duplicate top-level identifier(s):' + ', '.join(duplicates), p)\np[0] = pytd.TypeDeclUnit(name=None,  # replaced later, in Parse\n                         constants=tuple(constants),\n                         functions=tuple(MergeSignatures(funcdefs)),\n                         classes=tuple(classes),\n                         modules=())", "path": "parse\\parser.py", "repo_name": "google/pytypedecl", "stars": 66, "license": "other", "language": "python", "size": 735}
{"docstring": "\"\"\"version_expr : NAME LE NUMBER\"\"\"\n", "func_signal": "def p_version_expr_le(self, p):\n", "code": "CheckStringIsPython(self, p[1], p)\np[0] = self.python_version <= p[3].AsVersion(self, p)", "path": "parse\\parser.py", "repo_name": "google/pytypedecl", "stars": 66, "license": "other", "language": "python", "size": 735}
{"docstring": "\"\"\"type : NAME LBRACKET parameters RBRACKET\"\"\"\n", "func_signal": "def p_type_homogeneous(self, p):\n", "code": "if len(p[3]) == 1:\n  element_type, = p[3]\n  p[0] = pytd.HomogeneousContainerType(base_type=pytd.NamedType(p[1]),\n                                       parameters=(element_type,))\nelse:\n  p[0] = pytd.GenericType(base_type=pytd.NamedType(p[1]), parameters=p[3])", "path": "parse\\parser.py", "repo_name": "google/pytypedecl", "stars": 66, "license": "other", "language": "python", "size": 735}
{"docstring": "\"\"\"This is *supposed* to yield integers...\"\"\"\n", "func_signal": "def _BadGen():\n", "code": "for num in [1, 2, 3.1414926, 4]:\n  yield num", "path": "tests\\generics.py", "repo_name": "google/pytypedecl", "stars": 66, "license": "other", "language": "python", "size": 735}
{"docstring": "\"\"\"Convert a parser error into a SyntaxError and throw it.\"\"\"\n# SyntaxError(msg, (filename, lineno, offset, line))\n# is output in a nice format by traceback.print_exception\n# TODO: add test cases for this (including beginning/end of file,\n#                  lexer error, parser error)\n# TODO: Add test cases for all the various places where this function\n#              is used (duplicate detection etc.)\n\n", "func_signal": "def make_syntax_error(parser_or_tokenizer, msg, p):\n", "code": "if isinstance(p, yacc.YaccProduction):\n  # TODO: pretty-print lexpos / lineno\n  lexpos = p.lexpos(1)\n  lineno = p.lineno(1)\n  # TODO: The code below only works in the tokenizer, not in the\n  # parser. Additionally, ply's yacc catches SyntaxError, but has broken\n  # error handling (so we throw a SystemError for the time being).\n  raise SystemError(msg, parser_or_tokenizer.filename, (lexpos, lineno))\nelif p is None:\n  raise SystemError(msg, parser_or_tokenizer.filename)\n\n# Convert the lexer's offset to an offset within the line with the error\n# TODO: use regexp to split on r'[\\r\\n]' (for Windows, old MacOS):\nlast_line_offset = parser_or_tokenizer.data.rfind('\\n', 0, p.lexpos) + 1\nline, _, _ = parser_or_tokenizer.data[last_line_offset:].partition('\\n')\n\nraise SyntaxError(msg,\n                  (parser_or_tokenizer.filename,\n                   p.lineno, p.lexpos - last_line_offset + 1, line))", "path": "parse\\parser.py", "repo_name": "google/pytypedecl", "stars": 66, "license": "other", "language": "python", "size": 735}
{"docstring": "\"\"\"Given a list of pytd function signature declarations, group them by name.\n\nConverts a list of NameAndSignature items to a list of Functions (grouping\nsignatures by name).\n\nArguments:\n  signatures: A list of tuples (name, signature).\n\nReturns:\n  A list of instances of pytd.Function.\n\"\"\"\n\n", "func_signal": "def MergeSignatures(signatures):\n", "code": "name_to_signatures = collections.OrderedDict()\n\nfor name, signature in signatures:\n  if name not in name_to_signatures:\n    name_to_signatures[name] = []\n  name_to_signatures[name].append(signature)\n\nreturn [pytd.Function(name, tuple(signatures))\n        for name, signatures in name_to_signatures.items()]", "path": "parse\\parser.py", "repo_name": "google/pytypedecl", "stars": 66, "license": "other", "language": "python", "size": 735}
{"docstring": "\"\"\"version_expr : NAME GE NUMBER\"\"\"\n", "func_signal": "def p_version_expr_ge(self, p):\n", "code": "CheckStringIsPython(self, p[1], p)\np[0] = self.python_version >= p[3].AsVersion(self, p)", "path": "parse\\parser.py", "repo_name": "google/pytypedecl", "stars": 66, "license": "other", "language": "python", "size": 735}
{"docstring": "\"\"\"version_expr : NAME LBRACKET NUMBER\"\"\"\n", "func_signal": "def p_version_expr_gt(self, p):\n", "code": "self.lexer.CancelLBracket()\nCheckStringIsPython(self, p[1], p)\np[0] = self.python_version < p[3].AsVersion(self, p)", "path": "parse\\parser.py", "repo_name": "google/pytypedecl", "stars": 66, "license": "other", "language": "python", "size": 735}
{"docstring": "# TODO: See comments with TypeDeclParser about generating the\n#                  $GENFILESDIR/pytypedecl_lexer.py and using it by\n#                  calling lex.lex(lextab=pytypedecl_lexer)\n", "func_signal": "def __init__(self):\n", "code": "self.lexer = lex.lex(module=self, debug=False)\nself.default_get_token = self.lexer.token\n# TODO: Is there a better way to use a custom lexer.token() function?\nself.lexer.token = self.get_token\nself.lexer.escaping = False", "path": "parse\\parser.py", "repo_name": "google/pytypedecl", "stars": 66, "license": "other", "language": "python", "size": 735}
{"docstring": "# 1 byte is represented by two characters in the hexString, so internally we need to calculate the offset in nibbles\n", "func_signal": "def parse_table_btree_leaf_cell(page_hex_string, page_offset, cell_pointers, free_block_pointer):\n", "code": "page_offset_in_bytes = page_offset # store for log reasons only\npage_offset = page_offset * 2 # now dealing with nibbles because we treat a string (1 character = 1 nibble)\ndb_page_size_in_bytes = _sqliteFileHandler.DB_PAGESIZE_IN_BYTES\nusable_page_space = db_page_size_in_bytes - _sqliteFileHandler.DB_RESERVED_SPACE\n\n_adel_log.log(\"parse_table_btree_leaf_cell:      ----> parsing b-tree leaf cell at offset %(page_offset_in_bytes)s....\" % vars(), 4)\n\n# Get total number of bytes of payload\nbytes_of_payload_tuple = _sqliteVarInt.parse_next_var_int(page_hex_string[page_offset:(page_offset + 18)]) # a variable integer can be maximum 9 byte (= 18 nibbles) long\nbytes_of_payload = bytes_of_payload_tuple[0]\n_adel_log.log(\"parse_table_btree_leaf_cell:            OK - payload is %(bytes_of_payload)s bytes long\" % vars(), 4)\n# Get row_id\nrow_id_string = page_hex_string[(page_offset + (bytes_of_payload_tuple[1] * 2)):(page_offset + (bytes_of_payload_tuple[1] + 9) * 2)]\nrow_id_tuple = _sqliteVarInt.parse_next_var_int(row_id_string)\nrow_id = row_id_tuple[0]\n_adel_log.log(\"parse_table_btree_leaf_cell:      ----> extracting contents for row_id %(row_id)s....\" % vars(), 4)\n\n# Check for overflow pages and append content of those pages, if any\n# Calculate the overflow limits for table b-tree leaf cell\nremaining_page_space = db_page_size_in_bytes - page_offset_in_bytes\nif (bytes_of_payload > (remaining_page_space)):\n    # We expext content to overflow, because there is not enough space left on this page\n    _adel_log.log(\"parse_table_btree_leaf_cell:            OK - payload is too large for this page, there are overflow pages\" % vars(), 4)\n\n    # Check at which position the next cell starts\n    next_cell = usable_page_space\n    for cell_pointer in cell_pointers:\n        if (cell_pointer > page_offset_in_bytes) and (cell_pointer < next_cell):\n            next_cell = cell_pointer\n\n    # Check at which position the next freeblock starts (we ignore theoretically possible freebytes in this case,\n    # Because we expect no freebyte at the end of a cell that overflows to another page\n    next_free_block = usable_page_space\n    free_blocks = parse_free_blocks(page_hex_string, free_block_pointer)\n    for free_block in free_blocks:\n        if (free_block[0] > page_offset_in_bytes) and (free_block[0] < next_free_block):\n            next_free_block = free_block[0]\n\n    # Get the end of this record: either closest following cell or closest following freeblock or end of page\n    end_of_record = usable_page_space\n    # Check of the end of this record is given through a following cell\n    if (next_cell != usable_page_space) and ((next_cell <= next_free_block) or (next_free_block == usable_page_space)):\n        # next element is not end of page but a cell\n        end_of_record = next_cell\n    # Check of the end of this record is given through a following free block\n    if (next_free_block != usable_page_space) and ((next_free_block < next_cell) or (next_cell == usable_page_space)):\n        # Next element is not end of page but a free block\n        end_of_record = next_free_block\n\n    # Cut record hex string from the beginning to the offset of the next following element\n    record_hex_string = page_hex_string[(page_offset + ((bytes_of_payload_tuple[1] + row_id_tuple[1]) * 2)):(end_of_record * 2)]\n    record_hex_string_length = len(record_hex_string) / 2 # string length is count in nibbles, we need bytes here\n\n    # Save overflow page pointer at the end of record hex string\n    first_overflow_page_number = int(record_hex_string[((record_hex_string_length - 4) * 2):(record_hex_string_length * 2)], 16)\n    _adel_log.log(\"parse_table_btree_leaf_cell:      ----> parsing overflow page chain beginning at page %(first_overflow_page_number)s....\" % vars(), 4)\n    # Cut off overflow page number from record_hex_string\n    record_hex_string = record_hex_string[(0):((record_hex_string_length - 4) * 2)]\n\n    first_overflow_page_string = _sqliteFileHandler.read_page(first_overflow_page_number)\n    # Ensure that read page could retrieve an existing page\n    if (first_overflow_page_string == \"\"):\n        _adel_log.log(\"parse_table_btree_leaf_cell: ERROR - invalid overflow page pointer, cannot reference first overflow page: \" + str(first_overflow_page_number), 1)\n        return []\n    # Append content from overflow pages\n    record_hex_string += parse_overflow_page_chain(first_overflow_page_string)\n\n    # Ensure correct length of string (maybe not all bytes of the last overflow page in the chain contain content)\n    record_hex_string_length = len(record_hex_string) / 2 # string length is count in nibbles, we need bytes here\n    if (bytes_of_payload < record_hex_string_length):\n        # Cut record hex string again\n        record_hex_string = record_hex_string[:bytes_of_payload * 2]\nelse:\n    # The entire payload is stored on this page\n    record_hex_string = page_hex_string[(page_offset + ((bytes_of_payload_tuple[1] + row_id_tuple[1]) * 2)):(page_offset + ((bytes_of_payload_tuple[1] + row_id_tuple[1] + bytes_of_payload_tuple[0]) * 2))]\n\n# Parse the record\nread_content_list = parse_record(record_hex_string)\n# Build the resulting list (including the row_id used sqlite internally)\ncell_content_list = []\ncell_content_list.append(row_id)\nfor element in range(len(read_content_list)):\n    cell_content_list.append(read_content_list[element])\n# Return results\n_adel_log.log(\"parse_table_btree_leaf_cell:            OK - returning list of cell contents\", 4)\n_adel_log.log(\"parse_table_btree_leaf_cell:      ----> b-tree leaf cell at offset %(page_offset_in_bytes)s parsed\" % vars(), 4)\nreturn cell_content_list", "path": "_sqlitePageParser.py", "repo_name": "mspreitz/ADEL", "stars": 126, "license": "gpl-3.0", "language": "python", "size": 58694}
{"docstring": "######################################################################################################################################################\n#                                                           definition of Tuples                                                                     #\n######################################################################################################################################################\n# SMARTPHONE INFORMATION\n## Database Name\n", "func_signal": "def phone_info(backup_dir, os_version, xml_dir, handheld_id, config):\n", "code": "db_file_info_1 = backup_dir + \"/\" +  config.file_info1_db_name\nACCOUNTS_TABLE_NUMBER = config.file_info1_table_num\n######################################################################################################################################################\n## Table Design\n#### [['row_id', 'INTEGER'], ['account_name', 'TEXT'], ['account_type', 'TEXT']]\n## Tuple Definition -> the integers in this tuple represent the corresponding columns of the database table of the above table design\nACCOUNTS_ENTRIES = config.account_entry_list\n######################################################################################################################################################\n## Database Name\ndb_file_info_2 = backup_dir + \"/\" + config.file_info2_db_name\n######################################################################################################################################################\n## Table Name\n#### meta\n## corresponding table number in the database\nMETA_TABLE_NUMBER = config.file_info2_table_num\n## Table Design\n#### [['key', 'TEXT'], ['value', 'TEXT']]\n## Tuple Definition -> the integers in this tuple represent the corresponding columns of the database table of the above table design\nMETA_ENTRY = config.meta_entry_list[0] # only one element \n######################################################################################################################################################\n## Database Name\ndb_file_info_3 = backup_dir + \"/\" + config.file_info3_db_name\n######################################################################################################################################################\n## Table Name\n#### secure\n## corresponding table number in the database\nID_TABLE_NUMBER = config.file_info3_table_num\n## Table Design\n#### [['_id', 'INTEGER'],['name', 'TEXT'],['value', 'TEXT']]\n## Tuple Definition -> the integers in this tuple represent the corresponding columns of the database table of the above table design\nID_ENTRIES = config.settings_entry_list\n######################################################################################################################################################\n_adel_log.log(\"parseDBs:      ----> starting to parse \\033[0;32msmartphone info\\033[m\", 0)\nif os.path.isfile(db_file_info_1):\n    try:\n        result_list_1 = _sqliteParser.parse_db(db_file_info_1)\n        account_name = str(result_list_1[ACCOUNTS_TABLE_NUMBER][1][ACCOUNTS_ENTRIES[0]])\n        account_type = str(result_list_1[ACCOUNTS_TABLE_NUMBER][1][ACCOUNTS_ENTRIES[1]])\n    except:\n        _adel_log.log(\"analyzeDBs:    ----> can't get required data from \" + db_file_info_1.split(\"/\")[2] + \"! Please check manually for account data.\", 1)\n        account_name = \"not available\"\n        account_type = \"not available\"\nelse:\n    account_name = \"not available\"\n    account_type = \"not available\"\nif os.path.isfile(db_file_info_2):\n    try:\n        result_list_2 = _sqliteParser.parse_db(db_file_info_2)\n        imsi = str(result_list_2[META_TABLE_NUMBER][1][META_ENTRY])\n    except:\n        _adel_log.log(\"analyzeDBs:    ----> can't get required data from \" + db_file_info_2.split(\"/\")[2] + \"! Please check manually for IMSI.\", 1)\n        imsi = \"not available\"\nelse:\n    imsi = \"not available\"\nif os.path.isfile(db_file_info_3):\n    try:\n        result_list_3 = _sqliteParser.parse_db(db_file_info_3)\n        android_id = str(result_list_3[ID_TABLE_NUMBER][ID_ENTRIES[1]][ID_ENTRIES[0]])\n    except:\n        _adel_log.log(\"analyzeDBs:    ----> can't get required data from \" + db_file_info_3.split(\"/\")[2] + \"! Please check manually for android ID.\", 1)\n        android_id = \"not available\"\nelse:\n    android_id = \"not available\"\nmodel = subprocess.Popen(['adb', 'shell', 'getprop', 'ro.product.model'], stdout=subprocess.PIPE).communicate(0)[0]\nphone_info_list = [account_name, account_type, imsi, android_id, handheld_id, model, os_version]\n_xmlParser.smartphone_info_to_xml(xml_dir, phone_info_list)", "path": "_analyzeDB.py", "repo_name": "mspreitz/ADEL", "stars": 126, "license": "gpl-3.0", "language": "python", "size": 58694}
{"docstring": "# Determine next varInt length\n", "func_signal": "def parse_next_var_int(hex_string):\n", "code": "hex_string_length = len(hex_string)\nif hex_string_length < 2:\n    return None\n\nvar_int_length = 2\n# Length cannot be longer than 18 and cannot not be longer than the string itself\n# take the next byte if the current byte starts with 1, e.g. like \"10000000\" (this is how sqlite3 variable integers are defined)\nwhile (((int(hex_string[(var_int_length - 2):var_int_length], 16) & 10000000) >> 7) == 1) and (var_int_length < 18) and (var_int_length < (hex_string_length - 1)): # -1 due to strings with odd length\n    var_int_length += 2\n\n# Determine next varInt value\nbit_string = \"\"\nposition = 1\nwhile position <= var_int_length:\n    nibbleValue = int(hex_string[(position - 1):position], 16)\n    if (position % 2) == 0 or position == 17:\n        # This is the second nibble of a byte, so take all bits\n        bit_string += _helpersBinaryOperations.get_bitstring(nibbleValue, 4)\n    else:\n        # This is not the first nibble of the ninth byte of the variable integer, so take only 3 bits\n        bit_string += _helpersBinaryOperations.get_bitstring((nibbleValue & 7), 3)\n    position += 1\n    var_int_value = _helpersBinaryOperations.bin_to_int(bit_string)   \n\n# Return tuple of value and length\nreturn [var_int_value, (var_int_length / 2)]", "path": "_sqliteVarInt.py", "repo_name": "mspreitz/ADEL", "stars": 126, "license": "gpl-3.0", "language": "python", "size": 58694}
{"docstring": "# Check whether there are any free blocks on this page\n", "func_signal": "def parse_free_blocks(page_hex_strings, next_free_block_pointer):\n", "code": "if (next_free_block_pointer == 0):\n    # No free blocks on this page\n    return []\n\n# Parse the free block list\n_adel_log.log(\"parse_free_blocks:              ----> parsing b-tree page free block chain....\", 4)\nfree_blocks = []\nwhile (next_free_block_pointer != 0):\n    # We have a free block in the chain\n    free_blocks.append([next_free_block_pointer, int(page_hex_strings[((next_free_block_pointer + 2) * 2):((next_free_block_pointer + 4) * 2)], 16)])\n    _adel_log.log(\"parse_free_blocks:                    OK - append free block tuple to list [offset, length]: %(next_free_block_pointer)s\" % vars(), 4)\n    next_free_block_pointer = int(page_hex_strings[(next_free_block_pointer * 2):((next_free_block_pointer + 2) * 2)], 16)\n\n# Return results\n_adel_log.log(\"parse_free_blocks:                    OK - returning list of free block pointers: %(free_blocks)s\" % vars(), 3)\n_adel_log.log(\"parse_free_blocks:              ----> b-tree page free block chain parsed\", 4)\nreturn free_blocks", "path": "_sqlitePageParser.py", "repo_name": "mspreitz/ADEL", "stars": 126, "license": "gpl-3.0", "language": "python", "size": 58694}
{"docstring": "# Standard databases\n", "func_signal": "def process(self):\n", "code": "databases = self.phone.getElementsByTagName(\"databases\")[0] \nself.process_calls(databases.getElementsByTagName(\"call_logs\")[0])\nself.process_sms(databases.getElementsByTagName(\"sms\")[0])\nself.process_calendar(databases.getElementsByTagName(\"calendar\")[0])\nself.process_contacts(databases.getElementsByTagName(\"contacts\")[0])\nself.process_smartphone_info(self.phone.getElementsByTagName(\"smartphone_information\")[0])\n# Apps databases\napps = self.xml_doc.getElementsByTagName(\"apps\")[0] \nself.process_facebook(apps.getElementsByTagName(\"facebook\")[0])\nself.process_twitter(apps.getElementsByTagName(\"twitter\")[0])", "path": "_processXmlConfig.py", "repo_name": "mspreitz/ADEL", "stars": 126, "license": "gpl-3.0", "language": "python", "size": 58694}
{"docstring": "# initial checks\n", "func_signal": "def parse_content_entry(serial_type, record_hex_string, content_offset):\n", "code": "if serial_type < 0:\n\t_adel_log.log(\"getEntryContent: WARNING! invalid serial type (must be >= 0): %(serial_type)s\" % vars(), 2)", "path": "_sqlitePageParser.py", "repo_name": "mspreitz/ADEL", "stars": 126, "license": "gpl-3.0", "language": "python", "size": 58694}
{"docstring": "# DB_NAME\n", "func_signal": "def process_calls(self, calls):\n", "code": "call_db_name = calls.getElementsByTagName(\"db_name\")[0]\nif call_db_name == []:\n    if self.os_version < 200:\n        self.db_file_call_logs = \"contacts.db\"\n    else:\n        self.db_file_call_logs = \"contacts2.db\"\nelse:\n    self.db_file_call_logs = call_db_name.firstChild.data\n# TABLE_NUM\ncall_table_num = calls.getElementsByTagName(\"table_num\")[0]\nself.call_log_table_num = int(call_table_num.firstChild.data)\n# POSITIONS\nself.call_log_entry_positions = self.tree_to_list(calls.getElementsByTagName(\"call_log_entry_positions\")[0])", "path": "_processXmlConfig.py", "repo_name": "mspreitz/ADEL", "stars": 126, "license": "gpl-3.0", "language": "python", "size": 58694}
{"docstring": "# First digit seems to be in steps of 1/6 EV.\n# Does the third value mean the step size?  It is usually 6,\n# but it is 12 for the ExposureDifference.\n#\n# Check for an error condition that could cause a crash.\n# This only happens if something has gone really wrong in\n# reading the Nikon MakerNote.\n", "func_signal": "def nikon_ev_bias(seq):\n", "code": "if len(seq) < 4 : return \"\"\n#\nif seq == [252, 1, 6, 0]:\n    return \"-2/3 EV\"\nif seq == [253, 1, 6, 0]:\n    return \"-1/2 EV\"\nif seq == [254, 1, 6, 0]:\n    return \"-1/3 EV\"\nif seq == [0, 1, 6, 0]:\n    return \"0 EV\"\nif seq == [2, 1, 6, 0]:\n    return \"+1/3 EV\"\nif seq == [3, 1, 6, 0]:\n    return \"+1/2 EV\"\nif seq == [4, 1, 6, 0]:\n    return \"+2/3 EV\"\n# Handle combinations not in the table.\na = seq[0]\n# Causes headaches for the +/- logic, so special case it.\nif a == 0:\n    return \"0 EV\"\nif a > 127:\n    a = 256 - a\n    ret_str = \"-\"\nelse:\n    ret_str = \"+\"\nb = seq[2]\t# Assume third value means the step size\nwhole = a / b\na = a % b\nif whole != 0:\n    ret_str = ret_str + str(whole) + \" \"\nif a == 0:\n    ret_str = ret_str + \"EV\"\nelse:\n    r = Ratio(a, b)\n    ret_str = ret_str + r.__repr__() + \" EV\"\nreturn ret_str", "path": "_exif.py", "repo_name": "mspreitz/ADEL", "stars": 126, "license": "gpl-3.0", "language": "python", "size": 58694}
{"docstring": "# Parse the page header\n", "func_signal": "def parse_table_btree_interior_page(page_hex_string, page_offset):\n", "code": "header = parse_btree_page_header(page_hex_string, page_offset)\n\n# Ensure that we deal with a correct page\nheader_length = len(header)\nif (header_length != 7 or header[0] != 5):\n    # No valid header_length\n    _adel_log.log(\"parse_table_btree_interior_page: ERROR - invalid page type in table b-tree interior page header\", 1)\n    _adel_log.log(\"                             Page header was said to start at page offset: \" + str(page_offset), 1)\n    _adel_log.log(\"                             Printing page content....\", 1)\n    _adel_log.log(page_hex_string, 1)\n    return []\n\n# initialize resulting list\ncontent_list = []\n# Initialize node list\nnode_pointers = []\n# Initialize page content list\npage_contents = []\n# Parse cell pointer array\ncell_pointers = parse_cell_pointer_array(page_hex_string, (page_offset + header[header_length - 1]), header[2])\n# Parse cells\nfor cell_pointer in cell_pointers:\n    node_pointers.append(parse_table_btree_interior_cell(page_hex_string, cell_pointer))\n# This is an interior page, thus we append the right-most pointer as well\nnode_pointers.append([header[5], 0])\n\n# Iterate through every node\nfor node_tuple in node_pointers:\n    _adel_log.log(\"parse_table_btree_interior_page:  ----> fetching child page to parse, page number: \" + str(node_tuple[0]) + \"....\", 3)\n    child_page = _sqliteFileHandler.read_page(node_tuple[0])\n\n    # Ensure we fetched a valid page\n    if (child_page == \"\"):\n        _adel_log.log(\"parse_table_btree_interior_page: ERROR - invalid node tuple detected, cannot reference child page pointer: \" + str(node_tuple), 1)\n        continue\n    # Parse child pages\n    page_contents = parse_table_btree_page(child_page, 0)\n    for page_content in page_contents:\n        content_list.append(page_content)\n    _adel_log.log(\"parse_table_btree_interior_page:  ----> child page parsed, page number: \" + str(node_tuple[0]) + \"....\", 4)\n\nreturn content_list", "path": "_sqlitePageParser.py", "repo_name": "mspreitz/ADEL", "stars": 126, "license": "gpl-3.0", "language": "python", "size": 58694}
{"docstring": "######################################################################################################################################################\n#                                                           definition of Tuples                                                                     #\n######################################################################################################################################################\n# CALENDAR ENTRIES\n## Database Name\n", "func_signal": "def analyze_calendar(backup_dir, os_version, xml_dir, config):\n", "code": "db_file_calendar = backup_dir + \"/\" + config.db_file_calendar\n######################################################################################################################################################\n## Table Name\n#### calendars\n## corresponding table number in the database\nCALENDAR_NAME_TABLE_NUMBER = config.calendar_name_table_num\n## Table Design\n#### ['_id', 'INTEGER'], ['account_name', 'TEXT'], ['account_type', 'TEXT'], ['_sync_id', 'TEXT'], ['dirty', 'INTEGER'], ['mutators', 'TEXT'], \n#    ['name', 'TEXT'], ['calendar_displayName', 'TEXT'], ['calendar_color', 'INTEGER'], ['calendar_color_index', 'TEXT'], ['calendar_access_level', 'INTEGER'], \n#    ['visible', 'INTEGER'], ['sync_events', 'INTEGER'], ['calendar_location', 'TEXT'], ['calendar_timezone', 'TEXT'], ['ownerAccount', 'TEXT'], \n#    ['isPrimary', 'INTEGER'], ['canOrganizerRespond', 'INTEGER'], ['canModifyTimeZone', 'INTEGER'], ['canPartiallyUpdate', 'INTEGER'], \n#    ['maxReminders', 'INTEGER'], ['allowedReminders', 'TEXT'], \"1'\", ['allowedAvailability', 'TEXT'], \"1'\", ['allowedAttendeeTypes', 'TEXT'], '1', \"2'\", \n#    ['deleted', 'INTEGER'], ['cal_sync1', 'TEXT'], ['cal_sync2', 'TEXT'], ['cal_sync3', 'TEXT'], ['cal_sync4', 'TEXT'], ['cal_sync5', 'TEXT'], \n#    ['cal_sync6', 'TEXT'], ['cal_sync7', 'TEXT'], ['cal_sync8', 'TEXT'], ['cal_sync9', 'TEXT'], ['cal_sync10', 'TEXT']\n## Tuple Definition -> the integers in this tuple represent the corresponding columns of the database table of the above table design\nCALENDARS_NAME_LIST = config.calendar_name_list\n######################################################################################################################################################\n## Table Name\n#### events\n## corresponding table number in the database\nCALENDAR_EVENTS_TABLE_NUMBER = config.calendar_events_table_num\n## Table Design\n#### ['_id', 'INTEGER'], ['_sync_id', 'TEXT'], ['dirty', 'INTEGER'], ['mutators', 'TEXT'], ['lastSynced', 'INTEGER'], ['calendar_id', 'INTEGER'], \n#    ['title', 'TEXT'], ['eventLocation', 'TEXT'], ['description', 'TEXT'], ['eventColor', 'INTEGER'], ['eventColor_index', 'TEXT'], \n#    ['eventStatus', 'INTEGER'], ['selfAttendeeStatus', 'INTEGER'], ['dtstart', 'INTEGER'], ['dtend', 'INTEGER'], ['eventTimezone', 'TEXT'], \n#    ['duration', 'TEXT'], ['allDay', 'INTEGER'], ['accessLevel', 'INTEGER'], ['availability', 'INTEGER'], ['hasAlarm', 'INTEGER'], \n#    ['hasExtendedProperties', 'INTEGER'], ['rrule', 'TEXT'], ['rdate', 'TEXT'], ['exrule', 'TEXT'], ['exdate', 'TEXT'], ['original_id', 'INTEGER'], \n#    ['original_sync_id', 'TEXT'], ['originalInstanceTime', 'INTEGER'], ['originalAllDay', 'INTEGER'], ['lastDate', 'INTEGER'], ['hasAttendeeData', 'INTEGER'], \n#    ['guestsCanModify', 'INTEGER'], ['guestsCanInviteOthers', 'INTEGER'], ['guestsCanSeeGuests', 'INTEGER'], ['organizer', 'STRING'], \n#    ['isOrganizer', 'INTEGER'], ['deleted', 'INTEGER'], ['eventEndTimezone', 'TEXT'], ['customAppPackage', 'TEXT'], ['customAppUri', 'TEXT'], ['uid2445', 'TEXT'], \n#    ['sync_data1', 'TEXT'], ['sync_data2', 'TEXT'], ['sync_data3', 'TEXT'], ['sync_data4', 'TEXT'], ['sync_data5', 'TEXT'], ['sync_data6', 'TEXT'], \n#    ['sync_data7', 'TEXT'], ['sync_data8', 'TEXT'], ['sync_data9', 'TEXT'], ['sync_data10', 'TEXT']\n## Tuple Definition -> the integers in this tuple represent the corresponding columns of the database table of the above table design\nCALENDAR_EVENTS_LIST = config.calendar_events_list\n######################################################################################################################################################\nif os.path.isfile(db_file_calendar):\n    _adel_log.log(\"parseDBs:      ----> starting to parse \\033[0;32mcalendar entries\\033[m\", 0)\n    calendar_list = []\n    try:\n        result_list = _sqliteParser.parse_db(db_file_calendar)\n        for i in range(1, len(result_list[CALENDAR_EVENTS_TABLE_NUMBER])):\n            if str(result_list[CALENDAR_EVENTS_TABLE_NUMBER][i][CALENDAR_EVENTS_LIST[6]]) != \"None\":\n                end = time.strftime(\"%a, %d %B %Y %H:%M:%S\", time.gmtime(result_list[CALENDAR_EVENTS_TABLE_NUMBER][i][CALENDAR_EVENTS_LIST[6]] / 1000.0))\n            else:\n                end = time.strftime(\"%a, %d %B %Y %H:%M:%S\", time.gmtime(result_list[CALENDAR_EVENTS_TABLE_NUMBER][i][CALENDAR_EVENTS_LIST[5]] / 1000.0))\n            # calendar_list = [[id, calendarName, title, eventLocation, description, allDay, start, end, hasAlarm],[......],......]\n            calendar_list.append([\n                str(result_list[CALENDAR_EVENTS_TABLE_NUMBER][i][CALENDAR_EVENTS_LIST[0]]),\n                str(result_list[CALENDAR_NAME_TABLE_NUMBER][result_list[CALENDAR_EVENTS_TABLE_NUMBER][i][CALENDAR_EVENTS_LIST[1]]][CALENDARS_NAME_LIST[1]]),\n                string_replace(str(result_list[CALENDAR_EVENTS_TABLE_NUMBER][i][CALENDAR_EVENTS_LIST[2]])),\n                string_replace(str(result_list[CALENDAR_EVENTS_TABLE_NUMBER][i][CALENDAR_EVENTS_LIST[3]])),\n                string_replace(str(result_list[CALENDAR_EVENTS_TABLE_NUMBER][i][CALENDAR_EVENTS_LIST[4]])),\n                str(result_list[CALENDAR_EVENTS_TABLE_NUMBER][i][CALENDAR_EVENTS_LIST[7]]),\n                time.strftime(\"%a, %d %B %Y %H:%M:%S\", time.gmtime(result_list[CALENDAR_EVENTS_TABLE_NUMBER][i][CALENDAR_EVENTS_LIST[5]] / 1000.0)),\n                end,\n                str(result_list[CALENDAR_EVENTS_TABLE_NUMBER][i][CALENDAR_EVENTS_LIST[8]])\n                ])\n    except:\n        _adel_log.log(\"analyzeDBs:    ----> it seems that there are no entries in the calendar or that the database has an unsupported encoding!\", 1)\n    _xmlParser.calendar_to_xml(xml_dir, calendar_list)\nelse:\n    _adel_log.log(\"analyzeDBs:    ----> database file \" + db_file_calendar.split(\"/\")[2] + \" missing!\", 1)", "path": "_analyzeDB.py", "repo_name": "mspreitz/ADEL", "stars": 126, "license": "gpl-3.0", "language": "python", "size": 58694}
{"docstring": "# DB filename is in _analyzeDBs\n# Twitter user\n", "func_signal": "def process_twitter(self, twitter):\n", "code": "user = twitter.getElementsByTagName(\"twitter_user\")[0]\n## TABLE_NUM\nself.twitter_user_table_num = int(user.getElementsByTagName(\"table_num\")[0].firstChild.data)\n## TWITTER USER LIST\nself.twitter_user_list = self.tree_to_list(user.getElementsByTagName(\"twitter_user_positions\")[0])\n# Statuses\nstatuses = twitter.getElementsByTagName(\"statuses\")[0]\n## TABLE_NUM\nself.statuses_table_num = int(statuses.getElementsByTagName(\"table_num\")[0].firstChild.data)\n## TWITTER STATUSES\nself.twitter_statuses_list = self.tree_to_list(statuses.getElementsByTagName(\"status_positions\")[0])", "path": "_processXmlConfig.py", "repo_name": "mspreitz/ADEL", "stars": 126, "license": "gpl-3.0", "language": "python", "size": 58694}
{"docstring": "# Parse the page header\n", "func_signal": "def parse_table_btree_leaf_page(page_hex_string, page_offset):\n", "code": "header = parse_btree_page_header(page_hex_string, page_offset)\n\n# Ensure that we deal with a correct page\nheaderLength = len(header)\nif (headerLength != 6 or header[0] != 13):\n    # no valid headerLength\n    _adel_log.log(\"parse_table_btree_leaf_page: ERROR - invalid page type in table b-tree leaf page header\", 1)\n    _adel_log.log(\"                         Page header was said to start at page offset: \" + str(page_offset), 1)\n    _adel_log.log(\"                         Printing page content....\", 1)\n    _adel_log.log(page_hex_string, 1)\n    return []\n\n# Initialize resulting list\ncontent_list = []\n\n# Parse cell pointer array\ncell_pointers = parse_cell_pointer_array(page_hex_string, (page_offset + header[headerLength - 1]), header[2])\n\n# parse cells\nfor cell_pointer in cell_pointers:\n    content_list.append(parse_table_btree_leaf_cell(page_hex_string, cell_pointer, cell_pointers, header[1]))\n\nreturn content_list", "path": "_sqlitePageParser.py", "repo_name": "mspreitz/ADEL", "stars": 126, "license": "gpl-3.0", "language": "python", "size": 58694}
{"docstring": "######################################################################################################################################################\n#                                                           definition of Tuples                                                                     #\n######################################################################################################################################################\n# SMS MMS Messages\n## Database Name\n", "func_signal": "def analyze_sms_mss(backup_dir, os_version, xml_dir, config):\n", "code": "db_file_sms = backup_dir + \"/\" + config.db_file_sms\n######################################################################################################################################################\n## Table Name\n#### sms\n## corresponding table number in the database\nSMS_TABLE_NUMBER = config.sms_table_num\n## Table Design\n#### [[['_id', 'INTEGER'], ['thread_id', 'INTEGER'], ['address', 'TEXT'], ['person', 'INTEGER'], ['date', 'INTEGER'], ['date_sent', 'INTEGER'],  \n#      ['protocol', 'INTEGER'], ['read', 'INTEGER'], ['status', 'INTEGER'], ['type', 'INTEGER'], ['reply_path_present', 'INTEGER'], ['subject', 'TEXT'], \n#      ['body', 'TEXT'], ['service_center', 'TEXT'], ['locked', 'INTEGER'], ['error_code', 'INTEGER'], ['seen', 'INTEGER']]\n## Tuple Definition -> the integers in this tuple represent the corresponding columns of the database table of the above table design\nSMS_ENTRIES_LIST = config.sms_entry_positions\n######################################################################################################################################################\nif os.path.isfile(db_file_sms):\n    _adel_log.log(\"parseDBs:      ----> starting to parse \\033[0;32mSMS messages\\033[m\", 0)\n    sms_list = []\n    try:\n        result_list = _sqliteParser.parse_db(db_file_sms)\n        for i in range(1, len(result_list[SMS_TABLE_NUMBER])):\n            # sms_list = [[id, thread_id, number, person, date, read, type, subject, body],[......],......]\n            sms_list.append([\n                str(result_list[SMS_TABLE_NUMBER][i][SMS_ENTRIES_LIST[0]]),\n                str(result_list[SMS_TABLE_NUMBER][i][SMS_ENTRIES_LIST[1]]),\n                str(result_list[SMS_TABLE_NUMBER][i][SMS_ENTRIES_LIST[2]]),\n                str(result_list[SMS_TABLE_NUMBER][i][SMS_ENTRIES_LIST[3]]),\n                time.strftime(\"%a, %d %B %Y %H:%M:%S\", time.gmtime(result_list[SMS_TABLE_NUMBER][i][SMS_ENTRIES_LIST[4]] / 1000.0)),\n                str(result_list[SMS_TABLE_NUMBER][i][SMS_ENTRIES_LIST[5]]),\n                str(result_list[SMS_TABLE_NUMBER][i][SMS_ENTRIES_LIST[6]]),\n                str(result_list[SMS_TABLE_NUMBER][i][SMS_ENTRIES_LIST[7]]),\n                string_replace(str(result_list[SMS_TABLE_NUMBER][i][SMS_ENTRIES_LIST[8]]))\n                ])\n    except:\n        _adel_log.log(\"analyzeDBs:    ----> it seems that there are no SMS or MMS messages or that the database has an unsupported encoding!\", 1)\n    _xmlParser.sms_messages_to_xml(xml_dir, sms_list)\nelse:\n    _adel_log.log(\"analyzeDBs:    ----> database file \" + db_file_sms.split(\"/\")[2] + \" missing!\", 1)", "path": "_analyzeDB.py", "repo_name": "mspreitz/ADEL", "stars": 126, "license": "gpl-3.0", "language": "python", "size": 58694}
{"docstring": "# Info 1 \n", "func_signal": "def process_smartphone_info(self, info):\n", "code": "info1 = info.getElementsByTagName(\"info1\")[0]\ndb_filename = info1.getElementsByTagName(\"db_name\")\nif db_filename == \"\":\n    if os_version < 200:\n        self.file_info1_db_name = \"contacts.db\"\n        self.file_info1_table_num = 19\n    else:\n        if os_version > 220:\n            self.file_info1_db_name = \"accounts.db\"\n            self.file_info1_table_num = 1\n        else:\n            self.file_info1_db_name = \"contacts2.db\"\n            self.file_info1_table_num = 19\nelse:\n    self.file_info1_db_name = db_filename[0].firstChild.data\n    self.file_info1_table_num = int(info1.getElementsByTagName(\"table_num\")[0].firstChild.data)\naccounts = info1.getElementsByTagName(\"accounts\")[0]\nself.account_entry_list = self.tree_to_list(accounts.getElementsByTagName(\"accounts_positions\")[0])\n# Info 2\ninfo2 = info.getElementsByTagName(\"info2\")[0]\ndb_filename = info2.getElementsByTagName(\"db_name\")\nself.file_info2_table_num = int(info2.getElementsByTagName(\"table_num\")[0].firstChild.data)\nif db_filename == \"\":\n            self.file_info2_db_name = \"accounts.db\"\nelse:\n    self.file_info2_db_name = db_filename[0].firstChild.data\nmeta = info2.getElementsByTagName(\"meta\")[0]\nself.meta_entry_list = self.tree_to_list(meta.getElementsByTagName(\"meta_positions\")[0])\n# Info 3\ninfo3 = info.getElementsByTagName(\"info3\")[0]\ndb_filename = info3.getElementsByTagName(\"db_name\")\nself.file_info3_table_num = int(info3.getElementsByTagName(\"table_num\")[0].firstChild.data)\nif db_filename == \"\":\n            self.file_info3_db_name = \"settings.db\"\nelse:\n    self.file_info3_db_name = db_filename[0].firstChild.data\nsettings = info3.getElementsByTagName(\"settings\")[0]\nself.settings_entry_list = self.tree_to_list(settings.getElementsByTagName(\"settings_positions\")[0])", "path": "_processXmlConfig.py", "repo_name": "mspreitz/ADEL", "stars": 126, "license": "gpl-3.0", "language": "python", "size": 58694}
{"docstring": "# DB_NAME\n", "func_signal": "def process_calendar(self, calendar):\n", "code": "calendar_db_name = calendar.getElementsByTagName(\"db_name\")[0]\nif calendar_db_name == []:\n    self.db_file_calendar = \"calendar.db\"\nelse:\n    self.db_file_calendar = calendar_db_name.firstChild.data\n# Name Table\nname_table = calendar.getElementsByTagName(\"name_table\")[0]\n## TABLE_NUM\nself.calendar_name_table_num = int(name_table.getElementsByTagName(\"table_num\")[0].firstChild.data)\n## POSITIONS\nself.calendar_name_list = self.tree_to_list(name_table.getElementsByTagName(\"calendar_name_positions\")[0])\n# Event Table\nevent_table = calendar.getElementsByTagName(\"events\")[0]\n## TABLE_NUM\nself.calendar_events_table_num = int(event_table.getElementsByTagName(\"table_num\")[0].firstChild.data)\n## POSITIONS\nself.calendar_events_list = self.tree_to_list(event_table.getElementsByTagName(\"calendar_event_positions\")[0])", "path": "_processXmlConfig.py", "repo_name": "mspreitz/ADEL", "stars": 126, "license": "gpl-3.0", "language": "python", "size": 58694}
{"docstring": "# 1 byte is represented by two characters in the hexString, so internally we need to calculate the offset in nibbles\n", "func_signal": "def parse_btree_page_header(page_hex_string, page_offset):\n", "code": "page_offset = page_offset * 2\n\n# Parse sqlite b-tree header structure\n_adel_log.log(\"parse_btree_page_header:         ----> parsing b-tree page header structure....\", 4)\n# B-tree header byte 0: b-tree page type\nbtree_page_type = int(page_hex_string[(page_offset + 0):(page_offset + 2)], 16)\nif btree_page_type in (2, 5, 10, 13):\n    _adel_log.log(\"parse_btree_page_header:               OK - sqlite b-tree page type (must be 2,5,10 or 13): %(btree_page_type)s\" % vars(), 4)\nelse:\n    _adel_log.log(\"parse_btree_page_header: WARNING! - invalid sqlite b-tree page type (must be 2,5,10 or 13): %(btree_page_type)s\" % vars(), 2)\n# B-tree header bytes 1-2: bytes offset into the page of the first freeblock\nbtree_number_of_bytes_offset_in_first_free_block = int(page_hex_string[(page_offset + 2):(page_offset + 6)], 16)\n_adel_log.log(\"parse_btree_page_header:               OK - bytes offset into the page of the first freeblock: %(btree_number_of_bytes_offset_in_first_free_block)s\" % vars(), 4)\n# B-tree header bytes 3-4: number of cells on this page\nbtree_number_of_cells = int(page_hex_string[(page_offset + 6):(page_offset + 10)], 16)\n_adel_log.log(\"parse_btree_page_header:               OK - number of cells on this page: %(btree_number_of_cells)s\" % vars(), 4)\n# B-tree header bytes 5-6: offset of the first byte of cell content area\nbtree_offset_of_first_byte_content = int(page_hex_string[(page_offset + 10):(page_offset + 14)], 16)\n_adel_log.log(\"parse_btree_page_header:               OK - offset of the first byte of cell content area: %(btree_offset_of_first_byte_content)s\" % vars(), 4)\n# B-tree header byte 7: number of fragmented free bytes\nbtree_number_of_fragmented_free_bytes = int(page_hex_string[(page_offset + 14):(page_offset + 16)], 16)\n_adel_log.log(\"parse_btree_page_header:               OK - number of fragmented free bytes: %(btree_number_of_fragmented_free_bytes)s\" % vars(), 4)\n\n# Build list of well defined header elements\nheader_elements = [btree_page_type, btree_number_of_bytes_offset_in_first_free_block, btree_number_of_cells, btree_offset_of_first_byte_content, btree_number_of_fragmented_free_bytes]\n\n# check for optional header element 6: right-most pointer\n# b-tree header bytes 8-11: right-most pointer if page is an interior b-tree page\nif btree_page_type == 2 or btree_page_type == 5:\n    btree_right_most_pointer = int(page_hex_string[(page_offset + 16):(page_offset + 24)], 16)\n    header_elements.append(btree_right_most_pointer)\n    length = 12\n    _adel_log.log(\"parse_btree_page_header:               OK - right-most pointer: %(btree_right_most_pointer)s\" % vars(), 4)\nelse:\n    length = 8\n    _adel_log.log(\"parse_btree_page_header:               OK - page is a b-tree leaf page and thus does not include a right-most pointer\", 4)\n\n# Return list of header elements\nheader_elements.append(length)\n_adel_log.log(\"parse_btree_page_header:               OK - returning list of header elements: %(header_elements)s\" % vars(), 3)\n_adel_log.log(\"parse_btree_page_header:         ----> b-tree page header structure parsed\", 4)\nreturn header_elements", "path": "_sqlitePageParser.py", "repo_name": "mspreitz/ADEL", "stars": 126, "license": "gpl-3.0", "language": "python", "size": 58694}
{"docstring": "# DB_NAME\n", "func_signal": "def process_sms(self, sms):\n", "code": "sms_db_name = sms.getElementsByTagName(\"db_name\")[0]\nif sms_db_name == []:\n    self.db_file_sms = \"mmssms.db\"\nelse:\n    self.db_file_sms = sms_db_name.firstChild.data\n# TABLE_NUM\nsms_table_num = sms.getElementsByTagName(\"table_num\")[0]\nif sms_table_num == []:\n    if (os_version < 233):\n        self.sms_table_num = 6\n    else:\n        self.sms_table_num = 7\nelse:\n    self.sms_table_num = int(sms_table_num.firstChild.data)\n# POSITIONS\nself.sms_entry_positions = self.tree_to_list(sms.getElementsByTagName(\"sms_entry_positions\")[0])", "path": "_processXmlConfig.py", "repo_name": "mspreitz/ADEL", "stars": 126, "license": "gpl-3.0", "language": "python", "size": 58694}
{"docstring": "######################################################################################################################################################\n#                                                           definition of Tuples                                                                     #\n######################################################################################################################################################\n# CONTACTS ENTRIES\n## Database Name\n", "func_signal": "def analyze_contacts(backup_dir, os_version, xml_dir, config):\n", "code": "db_file_contacts = backup_dir + \"/\" + config.db_file_contacts\n######################################################################################################################################################\n## Table Name\n#### contacts\n## corresponding table number in the database\nCONTACTS_TABLE_NUMBER = config.contacts_normal_table_num\n## Table Design\n#### [['_id', 'INTEGER'], ['name_raw_contact_id', 'INTEGER'], ['photo_id', 'INTEGER'], ['photo_file_id', 'INTEGER'], ['custom_ringtone', 'TEXT'], \n#    ['send_to_voicemail', 'INTEGER'], ['times_contacted', 'INTEGER'], ['last_time_contacted', 'INTEGER'], ['starred', 'INTEGER'], ['pinned', 'INTEGER'], \n#    ['has_phone_number', 'INTEGER'], ['lookup', 'TEXT'], ['status_update_id', 'INTEGER'], ['contact_last_updated_timestamp', 'INTEGER']]\n## Tuple Definition -> the integers in this tuple represent the corresponding columns of the database table of the above table design\nCONTACTS_LIST = config.contacts_normal_list\n######################################################################################################################################################\n## Table Name\n#### raw_contacts\n## corresponding table number in the database\nRAW_CONTACTS_TABLE_NUMBER = config.raw_contacts_table_num\n## Table Design\n#### [['_id', 'INTEGER'], ['account_id', 'INTEGER'], ['sourceid', 'TEXT'], ['raw_contact_is_read_only', 'INTEGER'], ['version', 'INTEGER'], \n#    ['dirty', 'INTEGER'], ['deleted', 'INTEGER'], ['contact_id', 'INTEGER'], ['aggregation_mode', 'INTEGER'], ['aggregation_needed', 'INTEGER'], \n#    ['custom_ringtone', 'TEXT'], ['send_to_voicemail', 'INTEGER'], ['times_contacted', 'INTEGER'], ['last_time_contacted', 'INTEGER'], \n#    ['starred', 'INTEGER'], ['pinned', 'INTEGER'], ['display_name', 'TEXT'], ['display_name_alt', 'TEXT'], ['display_name_source', 'INTEGER'], \n#    ['phonetic_name', 'TEXT'], ['phonetic_name_style', 'TEXT'], ['sort_key', 'TEXT'], ['phonebook_label', 'TEXT'], ['phonebook_bucket', 'INTEGER'], \n#    ['sort_key_alt', 'TEXT'], ['phonebook_label_alt', 'TEXT'], ['phonebook_bucket_alt', 'INTEGER'], ['name_verified', 'INTEGER'], ['sync1', 'TEXT'], \n#    ['sync2', 'TEXT'], ['sync3', 'TEXT'], ['sync4', 'TEXT']]    \n## Tuple Definition -> the integers in this tuple represent the corresponding columns of the database table of the above table design\nRAW_CONTACTS_LIST = config.raw_contacts_list\n\nRAW_CONTACTS_LIST_DELETED = config.raw_contacts_deleted_list\n######################################################################################################################################################\n## Table Name\n#### data\n## corresponding table number in the database\nDATA_TABLE_NUMBER = config.data_table_num\n## Table Design\n#### [['_id', 'INTEGER'], ['package_id', 'INTEGER'], ['mimetype_id', 'INTEGER'], ['raw_contact_id', 'INTEGER'], ['is_read_only', 'INTEGER'], \n#    ['is_primary', 'INTEGER'], ['is_super_primary', 'INTEGER'], ['data_version', 'INTEGER'], ['data1', 'TEXT'], ['data2', 'TEXT'], ['data3', 'TEXT'], \n#    ['data4', 'TEXT'], ['data5', 'TEXT'], ['data6', 'TEXT'], ['data7', 'TEXT'], ['data8', 'TEXT'], ['data9', 'TEXT'], ['data10', 'TEXT'], \n#    ['data11', 'TEXT'], ['data12', 'TEXT'], ['data13', 'TEXT'], ['data14', 'TEXT'], ['data15', 'TEXT'], ['data_sync1', 'TEXT'], ['data_sync2', 'TEXT'], \n#    ['data_sync3', 'TEXT'], ['data_sync4', 'TEXT']]\n## Tuple Definition -> the integers in this tuple represent the corresponding columns of the database table of the above table design\nDATA_ENTRY_LIST = config.data_entry_list\n######################################################################################################################################################\n## Table Name\n#### mimetypes\n## corresponding table number in the database\nMIME_TABLE_NUMBER = config.mime_table_num\n## Table Design and matching mimetypes for the contact entries\n#### [['_id', 'INTEGER'], ['mimetype', 'TEXT']]\n#    [1, 'vnd.android.cursor.item/email_v2'], [2, 'vnd.android.cursor.item/im'], [3, 'vnd.android.cursor.item/nickname'], \n#    [4, 'vnd.android.cursor.item/organization'], [5, 'vnd.android.cursor.item/phone_v2'], [6, 'vnd.android.cursor.item/sip_address'], \n#    [7, 'vnd.android.cursor.item/name'], [8, 'vnd.android.cursor.item/postal-address_v2'], [9, 'vnd.android.cursor.item/identity'], \n#    [10, 'vnd.android.cursor.item/photo'], [11, 'vnd.android.cursor.item/group_membership']]\nMIME_TYPE_LIST = config.mime_type_entry_list\n######################################################################################################################################################\nif os.path.isfile(db_file_contacts):\n    _adel_log.log(\"parseDBs:      ----> starting to parse \\033[0;32maddress book entries\\033[m\", 0)\n    contacts_list = []\n    try:\n        result_list = _sqliteParser.parse_db(db_file_contacts)\n        for i in range(1, len(result_list[CONTACTS_TABLE_NUMBER])):\n            email = \"None\"\n            url = \"None\"\n            lastname = \"None\"\n            firstname = \"None\"\n            number = \"None\"\n            address = \"None\"\n            company = \"None\"\n            # convert the date and time of the last call to this contact\n            if result_list[CONTACTS_TABLE_NUMBER][i][CONTACTS_LIST[3]] == 0:\n                last_time_contacted = \"NEVER\"\n            else:\n                last_time_contacted = time.strftime(\"%a, %d %B %Y %H:%M:%S\", time.gmtime(result_list[CONTACTS_TABLE_NUMBER][i][CONTACTS_LIST[3]] / 1000.0))\n            # search the display name and other data for this contact\n            for j in range(1, len(result_list[RAW_CONTACTS_TABLE_NUMBER])):\n                # display name\n                if result_list[RAW_CONTACTS_TABLE_NUMBER][j][RAW_CONTACTS_LIST[0]] == result_list[CONTACTS_TABLE_NUMBER][i][CONTACTS_LIST[0]]:\n                    display_name = string_replace(str(result_list[RAW_CONTACTS_TABLE_NUMBER][j][RAW_CONTACTS_LIST[1]]))\n                    # other data like name, phone number, etc. => identified by mimetype\n                    for k in range(1, len(result_list[DATA_TABLE_NUMBER])):\n                        if result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[1]] == j and result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[0]] == MIME_TYPE_LIST[3]:\n                            lastname = string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[4]]))\n                            firstname = string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[3]]))\n                            break;\n                        else:\n                            continue;\n                    for k in range(1, len(result_list[DATA_TABLE_NUMBER])):\n                        if result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[1]] == j and result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[0]] == MIME_TYPE_LIST[0]:\n                            if email == \"None\":\n                                email = string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[2]]))\n                            else:\n                                email = email + \";\" + string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[2]]))\n                        else:\n                            continue;\n                    for k in range(1, len(result_list[DATA_TABLE_NUMBER])):\n                        if result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[1]] == j and result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[0]] == MIME_TYPE_LIST[4]:\n                            if url == \"None\":\n                                url = string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[2]]))\n                            else:\n                                url = url + \";\" + string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[2]]))\n                        else:\n                            continue;\n                    for k in range(1, len(result_list[DATA_TABLE_NUMBER])):\n                        if result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[1]] == j and result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[0]] == MIME_TYPE_LIST[2]:\n                            if number == \"None\":\n                                number = string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[2]]))\n                            else:\n                                number = number + \";\" + string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[2]]))\n                        else:\n                            continue;\n                    for k in range(1, len(result_list[DATA_TABLE_NUMBER])):\n                        if result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[1]] == j and result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[0]] == MIME_TYPE_LIST[1]:\n                            if \"\\n\" in string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[5]])):\n                                _adel_log.log(\"analyzeDBs:    ----> check formatting of postal address for contact with id \" + str(result_list[CONTACTS_TABLE_NUMBER][i][CONTACTS_LIST[0]]) + \"!\", 2)\n                                address = string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[5]]))\n                            else:\n                                if address == \"None\":\n                                    address = string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[5]])) + \";\" + string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[8]])) + \";\" + string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[10]])) + \";\" + string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[11]]))\n                                else:\n                                    address = address + \";\" + string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[2]]))\n                        else:\n                            continue;\n                    for k in range(1, len(result_list[DATA_TABLE_NUMBER])):\n                        if result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[1]] == j and result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[0]] == MIME_TYPE_LIST[5]:\n                            if company == \"None\":\n                                company = string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[2]]))\n                            else:\n                                company = company + \";\" + string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[2]]))\n                        else:\n                            continue;\n                    break;\n                else:\n                    continue;\n            # contacts_list = [[id, photo_id, times_contacted, last_time_contacted, starred, number, display_name, lastname, firstname, company, email, url, address],[......],.....]\n            contacts_list.append([\n                str(result_list[CONTACTS_TABLE_NUMBER][i][CONTACTS_LIST[0]]),\n                str(result_list[CONTACTS_TABLE_NUMBER][i][CONTACTS_LIST[1]]),\n                str(result_list[CONTACTS_TABLE_NUMBER][i][CONTACTS_LIST[2]]),\n                last_time_contacted,\n                str(result_list[CONTACTS_TABLE_NUMBER][i][CONTACTS_LIST[4]]),\n                number,\n                display_name,\n                lastname,\n                firstname,\n                company,\n                email,\n                url,\n                address\n                ])\n        # search for deleted entries and gather information\n        for j in range(1, len(result_list[RAW_CONTACTS_TABLE_NUMBER])):\n            del_email = \"None\"\n            del_url = \"None\"\n            del_lastname = \"None\"\n            del_firstname = \"None\"\n            del_number = \"None\"\n            del_address = \"None\"\n            del_company = \"None\"\n            if result_list[RAW_CONTACTS_TABLE_NUMBER][j][RAW_CONTACTS_LIST_DELETED[1]] == 1:\n                _adel_log.log(\"analyzeDBs:    ----> found deleted contact entry!\", 3)\n                # convert the date and time of the last call to this contact\n                if result_list[RAW_CONTACTS_TABLE_NUMBER][j][RAW_CONTACTS_LIST_DELETED[3]] == None:\n                    del_last_time_contacted = \"NEVER\"\n                else:\n                    del_last_time_contacted = time.strftime(\"%a, %d %B %Y %H:%M:%S\", time.gmtime(result_list[RAW_CONTACTS_TABLE_NUMBER][j][RAW_CONTACTS_LIST_DELETED[3]] / 1000.0))\n                del_times_contacted = str(result_list[RAW_CONTACTS_TABLE_NUMBER][j][RAW_CONTACTS_LIST_DELETED[2]])\n                del_starred = str(result_list[RAW_CONTACTS_TABLE_NUMBER][j][RAW_CONTACTS_LIST_DELETED[4]])\n                del_display_name = string_replace(str(result_list[RAW_CONTACTS_TABLE_NUMBER][j][RAW_CONTACTS_LIST_DELETED[5]]))\n                # other data like name, phone number, etc. => identified by mimetype\n                for k in range(1, len(result_list[DATA_TABLE_NUMBER])):\n                    if result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[1]] == j and result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[0]] == MIME_TYPE_LIST[3]:\n                        del_lastname = string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[4]]))\n                        del_firstname = string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[3]]))\n                        break;\n                    else:\n                        continue;\n                for k in range(1, len(result_list[DATA_TABLE_NUMBER])):\n                    if result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[1]] == j and result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[0]] == MIME_TYPE_LIST[0]:\n                        if del_email == \"None\":\n                            del_email = string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[2]]))\n                        else:\n                            del_email = del_email + \";\" + string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[2]]))\n                    else:\n                        continue;\n                for k in range(1, len(result_list[DATA_TABLE_NUMBER])):\n                    if result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[1]] == j and result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[0]] == MIME_TYPE_LIST[4]:\n                        if del_url == \"None\":\n                            del_url = string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[2]]))\n                        else:\n                            del_url = del_url + \";\" + string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[2]]))\n                    else:\n                        continue;\n                for k in range(1, len(result_list[DATA_TABLE_NUMBER])):\n                    if result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[1]] == j and result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[0]] == MIME_TYPE_LIST[2]:\n                        if del_number == \"None\":\n                            del_number = string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[2]]))\n                        else:\n                            del_number = del_number + \";\" + string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[2]]))\n                    else:\n                        continue;\n                for k in range(1, len(result_list[DATA_TABLE_NUMBER])):\n                    if result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[1]] == j and result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[0]] == MIME_TYPE_LIST[1]:\n                        if \"\\n\" in string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[5]])):\n                            _adel_log.log(\"analyzeDBs:    ----> check formatting of postal address for contact with id \" + str(result_list[CONTACTS_TABLE_NUMBER][i][CONTACTS_LIST[0]]) + \"!\", 2)\n                            del_address = string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[5]]))\n                        else:\n                            if del_address == \"None\":\n                                del_address = string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[5]])) + \";\" + string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[8]])) + \";\" + string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[10]])) + \";\" + string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[11]]))\n                            else:\n                                del_address = del_address + \";\" + string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[2]]))\n                    else:\n                        continue;\n                for k in range(1, len(result_list[DATA_TABLE_NUMBER])):\n                    if result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[1]] == j and result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[0]] == MIME_TYPE_LIST[5]:\n                        if del_company == \"None\":\n                            del_company = string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[2]]))\n                        else:\n                            del_company = del_company + \";\" + string_replace(str(result_list[DATA_TABLE_NUMBER][k][DATA_ENTRY_LIST[2]]))\n                    else:\n                        continue;\n            else:\n                continue;\n            # contacts_list = [[id, photo_id, times_contacted, last_time_contacted, starred, number, display_name, lastname, firstname, company, email, url, address],[......],.....]\n            contacts_list.append([\n                \"DELETED\",\n                \"DELETED\",\n                del_times_contacted,\n                del_last_time_contacted,\n                del_starred,\n                del_number,\n                del_display_name,\n                del_lastname,\n                del_firstname,\n                del_company,\n                del_email,\n                del_url,\n                del_address\n                ])\n    except:\n        _adel_log.log(\"analyzeDBs:    ----> it seems that the contact list is empty or that the database has an unsupported encoding!\", 1)\n    _xmlParser.contacts_to_xml(xml_dir, contacts_list)\nelse:\n    _adel_log.log(\"analyzeDBs:    ----> database file \" + db_file_contacts.split(\"/\")[2] + \" missing!\", 1)", "path": "_analyzeDB.py", "repo_name": "mspreitz/ADEL", "stars": 126, "license": "gpl-3.0", "language": "python", "size": 58694}
{"docstring": "# 1 byte is represented by two characters in the hexString, so internally we need to calculate the offset in nibbles\n", "func_signal": "def parse_table_btree_interior_cell(page_hex_string, page_offset):\n", "code": "page_offset_in_bytes = page_offset # store for log reasons only\npage_offset = page_offset * 2 # now dealing with nibbles because we treat a string (1 character = 1 nibble)\n\n_adel_log.log(\"parse_table_btree_interior_cell:  ----> parsing b-tree interior cell at offset %(page_offset_in_bytes)s....\" % vars(), 4)\n\n# Get total number of bytes of payload\nleft_child_pointer = int(page_hex_string[page_offset:(page_offset + (4 * 2))], 16)\n_adel_log.log(\"parse_table_btree_interior_cell:        OK - left child pointer is: %(left_child_pointer)s\" % vars(), 4)\n# Get row_id\nrow_id_string = page_hex_string[(page_offset + (4 * 2)):(page_offset + ((4 + 9) * 2))]\nrow_id_tuple = _sqliteVarInt.parse_next_var_int(row_id_string)\nrow_id = row_id_tuple[0]\n_adel_log.log(\"parse_table_btree_interior_cell:  ----> row_id (index) is: %(row_id)s....\" % vars(), 4)\n\n# Build tuple of node contents\nnode_tuple = [left_child_pointer, row_id]\n_adel_log.log(\"parse_table_btree_interior_cell:        OK - returning tuple of node content: %(node_tuple)s\" % vars(), 4)\n_adel_log.log(\"parse_table_btree_interior_cell:  ----> b-tree interior cell at offset %(page_offset_in_bytes)s parsed\" % vars(), 4)\nreturn node_tuple", "path": "_sqlitePageParser.py", "repo_name": "mspreitz/ADEL", "stars": 126, "license": "gpl-3.0", "language": "python", "size": 58694}
{"docstring": "# Backup the SQLite files\n", "func_signal": "def dumpDBs(file_dir, os_version, device_name):\n", "code": "_adel_log.log(\"dumpDBs:       ----> dumping all SQLite databases....\", 0)\n_dumpFiles.get_SQLite_files(file_dir, os_version, device_name)\n_adel_log.log(\"dumpDBs:       ----> all SQLite databases dumped\", 0)\n_adel_log.log(\"\", 3)", "path": "adel.py", "repo_name": "mspreitz/ADEL", "stars": 126, "license": "gpl-3.0", "language": "python", "size": 58694}
{"docstring": "\"\"\"\nGenerates headers to avoid any page caching in the browser.\nUseful for highly dynamic sites.\n\nReturns a unicode string of headers.\n\"\"\"\n", "func_signal": "def no_cache_headers(self):\n", "code": "return u\"\".join([u\"Expires: Tue, 03 Jul 2001 06:00:00 GMT\",\n    strftime(\"Last-Modified: %a, %d %b %y %H:%M:%S %Z\").decode(\"utf-8\"),\n    u\"Cache-Control: no-store, no-cache, must-revalidate, max-age=0\",\n    u\"Cache-Control: post-check=0, pre-check=0\",\n    u\"Pragma: no-cache\",\n])", "path": "appengine_utilities\\sessions.py", "repo_name": "joerussbowman/gaeutilities", "stars": 78, "license": "bsd-3-clause", "language": "python", "size": 1131}
{"docstring": "\"\"\"\nAdds a keyname/value for session to the datastore and memcache\n\nReturns the key from the datastore put or u\"dirty\"\n\"\"\"\n# update or insert in datastore\n", "func_signal": "def put(self):\n", "code": "try:\n    return_val = db.put(self)\n    self.dirty = False\nexcept:\n    return_val = u\"dirty\"\n    self.dirty = True\n\n# update or insert in memcache\nmc_items = memcache.get(u\"_AppEngineUtilities_SessionData_%s\" % \\\n    (str(self.session.key())))\nif mc_items:\n    value_updated = False\n    for item in mc_items:\n        if value_updated == True:\n            break\n        if item.keyname == self.keyname:\n            item.content = self.content\n            item.model = self.model\n            memcache.set(u\"_AppEngineUtilities_SessionData_%s\" % \\\n                (str(self.session.key())), mc_items)\n            value_updated = True\n            break\n    if value_updated == False:\n        mc_items.append(self)\n        memcache.set(u\"_AppEngineUtilities_SessionData_%s\" % \\\n            (str(self.session.key())), mc_items)\nreturn return_val", "path": "appengine_utilities\\sessions.py", "repo_name": "joerussbowman/gaeutilities", "stars": 78, "license": "bsd-3-clause", "language": "python", "size": 1131}
{"docstring": "\"\"\"\nReturns all the items stored in a session. Queries memcache first\nand will try the datastore next.\n\"\"\"\n", "func_signal": "def get_items(self):\n", "code": "items = memcache.get(u\"_AppEngineUtilities_SessionData_%s\" % \\\n    (str(self.key())))\nif items:\n    for item in items:\n        if item.deleted == True:\n            item.delete()\n            items.remove(item)\n    return items\n\nquery = _AppEngineUtilities_SessionData.all()\nquery.filter(u\"session\", self)\nresults = query.fetch(1000)\nreturn results", "path": "appengine_utilities\\sessions.py", "repo_name": "joerussbowman/gaeutilities", "stars": 78, "license": "bsd-3-clause", "language": "python", "size": 1131}
{"docstring": "\"\"\"\nDelete's the current session, creating a new one.\n\nReturns True\n\"\"\"\n", "func_signal": "def flush(self):\n", "code": "self._delete_session()\nself.__init__()\nreturn True", "path": "appengine_utilities\\sessions.py", "repo_name": "joerussbowman/gaeutilities", "stars": 78, "license": "bsd-3-clause", "language": "python", "size": 1131}
{"docstring": "\"\"\"\nDelete expired sessions from the datastore.\n\nThis is a class method which can be used by applications for\nmaintenance if they don't want to use the built in session\ncleaning.\n\nArgs:\n  count: The amount of session to clean.\n  session_expire_time: The age in seconds to determine outdated\n                       sessions.\n\nReturns True on completion\n\"\"\"\n", "func_signal": "def clean_old_sessions(cls, session_expire_time, count=50):\n", "code": "duration = datetime.timedelta(seconds=session_expire_time)\nsession_age = datetime.datetime.now() - duration\nquery = _AppEngineUtilities_Session.all()\nquery.filter(u\"last_activity <\", session_age)\nresults = query.fetch(50)\nfor result in results:\n    result.delete()\nreturn True", "path": "appengine_utilities\\sessions.py", "repo_name": "joerussbowman/gaeutilities", "stars": 78, "license": "bsd-3-clause", "language": "python", "size": 1131}
{"docstring": "\"\"\"\nDeletes all sessions and session data from the data store. This\ndoes not delete the entities from memcache (yet). Depending on the\namount of sessions active in your datastore, this request could\ntimeout before completion and may have to be called multiple times.\n\nNOTE: This can not delete cookie only sessions as it has no way to\naccess them. It will only delete datastore writer sessions.\n\nReturns True on completion.\n\"\"\"\n", "func_signal": "def delete_all_sessions(cls):\n", "code": "all_sessions_deleted = False\n\nwhile not all_sessions_deleted:\n    query = _AppEngineUtilities_Session.all()\n    results = query.fetch(75)\n    if len(results) is 0:\n        all_sessions_deleted = True\n    else:\n        for result in results:\n            result.delete()\nreturn True", "path": "appengine_utilities\\sessions.py", "repo_name": "joerussbowman/gaeutilities", "stars": 78, "license": "bsd-3-clause", "language": "python", "size": 1131}
{"docstring": "\"\"\"\nReturns either the value for the keyname or a default value\npassed.\n\nArgs:\n    keyname: keyname to look up\n    default: (optional) value to return on keyname miss\n\nReturns value of keyname, or default, or None\n\"\"\"\n", "func_signal": "def get(self, keyname, default = None):\n", "code": "try:\n    return self.__getitem__(keyname)\nexcept KeyError:\n    if default is not None:\n        return default\n    return None", "path": "appengine_utilities\\sessions.py", "repo_name": "joerussbowman/gaeutilities", "stars": 78, "license": "bsd-3-clause", "language": "python", "size": 1131}
{"docstring": "\"\"\"\nReturn size of session.\n\"\"\"\n# check memcache first\n", "func_signal": "def __len__(self):\n", "code": "if hasattr(self, u\"session\"):\n    results = self._get()\n    if results is not None:\n        return len(results) + len(self.cookie_vals)\n    else:\n        return 0\nreturn len(self.cookie_vals)", "path": "appengine_utilities\\sessions.py", "repo_name": "joerussbowman/gaeutilities", "stars": 78, "license": "bsd-3-clause", "language": "python", "size": 1131}
{"docstring": "\"\"\"\nReturns either the value for the keyname or a default value\npassed. If keyname lookup is a miss, the keyname is set with\na value of default.\n\nArgs:\n    keyname: keyname to look up\n    default: (optional) value to return on keyname miss\n\nReturns value of keyname, or default, or None\n\"\"\"\n", "func_signal": "def setdefault(self, keyname, default = None):\n", "code": "try:\n    return self.__getitem__(keyname)\nexcept KeyError:\n    if default is not None:\n        self.__setitem__(keyname, default)\n        return default\n    return None", "path": "appengine_utilities\\sessions.py", "repo_name": "joerussbowman/gaeutilities", "stars": 78, "license": "bsd-3-clause", "language": "python", "size": 1131}
{"docstring": "\"\"\"\nDelete item from session data, ignoring exceptions if\nnecessary.\n\nArgs:\n    keyname: The keyname of the object to delete.\n    throw_exception: false if exceptions are to be ignored.\nReturns:\n    Nothing.\n\"\"\"\n", "func_signal": "def delete_item(self, keyname, throw_exception=False):\n", "code": "if throw_exception:\n    self.__delitem__(keyname)\n    return None\nelse:\n    try:\n        self.__delitem__(keyname)\n    except KeyError:\n        return None", "path": "appengine_utilities\\sessions.py", "repo_name": "joerussbowman/gaeutilities", "stars": 78, "license": "bsd-3-clause", "language": "python", "size": 1131}
{"docstring": "\"\"\"\nDeletes an entity from the session in memcache and the datastore\n\nReturns True\n\"\"\"\n", "func_signal": "def delete(self):\n", "code": "try:\n    db.delete(self)\nexcept:\n    self.deleted = True\nmc_items = memcache.get(u\"_AppEngineUtilities_SessionData_%s\" % \\\n    (str(self.session.key())))\nvalue_handled = False\nfor item in mc_items:\n    if value_handled == True:\n        break\n    if item.keyname == self.keyname:\n        if self.deleted == True:\n            item.deleted = True\n        else:\n            mc_items.remove(item)\n        memcache.set(u\"_AppEngineUtilities_SessionData_%s\" % \\\n            (str(self.session.key())), mc_items)\nreturn True", "path": "appengine_utilities\\sessions.py", "repo_name": "joerussbowman/gaeutilities", "stars": 78, "license": "bsd-3-clause", "language": "python", "size": 1131}
{"docstring": "\"\"\"\nprivate method\n\nValidate the keyname, making sure it is set and not a reserved name.\n\nReturns the validated keyname.\n\"\"\"\n", "func_signal": "def _validate_key(self, keyname):\n", "code": "if keyname is None:\n    raise ValueError(\n        u\"You must pass a keyname for the session data content.\"\n    )\nelif keyname in (u\"sid\", u\"flash\"):\n    raise ValueError(u\"%s is a reserved keyname.\" % keyname)\n\nif type(keyname) != type([str, unicode]):\n    return unicode(keyname)\nreturn keyname", "path": "appengine_utilities\\sessions.py", "repo_name": "joerussbowman/gaeutilities", "stars": 78, "license": "bsd-3-clause", "language": "python", "size": 1131}
{"docstring": "\"\"\"\nWill return the session entity from the datastore if one\nexists, otherwise will return None (as in the case of cookie writer\nsession.\n\"\"\"\n", "func_signal": "def get_ds_entity(self):\n", "code": "if hasattr(self, u\"session\"):\n    return self.session\nreturn None", "path": "appengine_utilities\\sessions.py", "repo_name": "joerussbowman/gaeutilities", "stars": 78, "license": "bsd-3-clause", "language": "python", "size": 1131}
{"docstring": "\"\"\"\nExtends put so that it writes vaules to memcache as well as the\ndatastore, and keeps them in sync, even when datastore writes fails.\n\nReturns the session object.\n\"\"\"\n", "func_signal": "def put(self):\n", "code": "try:\n    memcache.set(u\"_AppEngineUtilities_Session_%s\" % \\\n        (str(self.key())), self)\nexcept:\n    # new session, generate a new key, which will handle the\n    # put and set the memcache\n    db.put(self)\n\nself.last_activity = datetime.datetime.now()\n\ntry:\n    self.dirty = False\n    db.put(self)\n    memcache.set(u\"_AppEngineUtilities_Session_%s\" % \\\n        (str(self.key())), self)\nexcept:\n    self.dirty = True\n    memcache.set(u\"_AppEngineUtilities_Session_%s\" % \\\n        (str(self.key())), self)\n\nreturn self", "path": "appengine_utilities\\sessions.py", "repo_name": "joerussbowman/gaeutilities", "stars": 78, "license": "bsd-3-clause", "language": "python", "size": 1131}
{"docstring": "\"\"\"\nReturns a list object of just values in the session.\n\"\"\"\n", "func_signal": "def values(self):\n", "code": "v = []\nfor k in self:\n    v.append(self[k])\nreturn v", "path": "appengine_utilities\\sessions.py", "repo_name": "joerussbowman/gaeutilities", "stars": 78, "license": "bsd-3-clause", "language": "python", "size": 1131}
{"docstring": "\"\"\"\nGenerates headers to avoid any page caching in the browser.\nUseful for highly dynamic sites.\n\nReturns a unicode string of headers.\n\"\"\"\n", "func_signal": "def no_cache_headers(self):\n", "code": "return u\"\".join([u\"Expires: Tue, 03 Jul 2001 06:00:00 GMT\",\n    strftime(\"Last-Modified: %a, %d %b %y %H:%M:%S %Z\").decode(\"utf-8\"),\n    u\"Cache-Control: no-store, no-cache, must-revalidate, max-age=0\",\n    u\"Cache-Control: post-check=0, pre-check=0\",\n    u\"Pragma: no-cache\",\n])", "path": "appengine_utilities\\flash.py", "repo_name": "joerussbowman/gaeutilities", "stars": 78, "license": "bsd-3-clause", "language": "python", "size": 1131}
{"docstring": "\"\"\"\nReturns a single session data item from the memcache or datastore\n\nArgs:\n    keyname: keyname of the session data object\n\nReturns the session data object if it exists, otherwise returns None\n\"\"\"\n", "func_signal": "def get_item(self, keyname = None):\n", "code": "mc = memcache.get(u\"_AppEngineUtilities_SessionData_%s\" % \\\n    (str(self.key())))\nif mc:\n    for item in mc:\n        if item.keyname == keyname:\n            if item.deleted == True:\n                item.delete()\n                return None\n            return item\nquery = _AppEngineUtilities_SessionData.all()\nquery.filter(u\"session = \", self)\nquery.filter(u\"keyname = \", keyname)\nresults = query.fetch(1)\nif len(results) > 0:\n    memcache.set(u\"_AppEngineUtilities_SessionData_%s\" % \\\n        (str(self.key())), self.get_items_ds())\n    return results[0]\nreturn None", "path": "appengine_utilities\\sessions.py", "repo_name": "joerussbowman/gaeutilities", "stars": 78, "license": "bsd-3-clause", "language": "python", "size": 1131}
{"docstring": "\"\"\"\nCheck if an item is in the session data.\n\nArgs:\n    keyname: The keyname being searched.\n\"\"\"\n", "func_signal": "def __contains__(self, keyname):\n", "code": "try:\n    self.__getitem__(keyname)\nexcept KeyError:\n    return False\nreturn True", "path": "appengine_utilities\\sessions.py", "repo_name": "joerussbowman/gaeutilities", "stars": 78, "license": "bsd-3-clause", "language": "python", "size": 1131}
{"docstring": "\"\"\"\nUpdates with key/value pairs from b, overwriting existing keys\n\nReturns None\n\"\"\"\n", "func_signal": "def update(self, *dicts):\n", "code": "for dict in dicts:\n    for k in dict:\n        self._put(k, dict[k])\nreturn None", "path": "appengine_utilities\\sessions.py", "repo_name": "joerussbowman/gaeutilities", "stars": 78, "license": "bsd-3-clause", "language": "python", "size": 1131}
{"docstring": "\"\"\"\nSet item in session data.\n\nArgs:\n    keyname: They keyname of the mapping.\n    value: The value of mapping.\n\"\"\"\n\n", "func_signal": "def __setitem__(self, keyname, value):\n", "code": "if self.integrate_flash and (keyname == u\"flash\"):\n    self.flash.msg = value\nelse:\n    keyname = self._validate_key(keyname)\n    self.cache[keyname] = value\n    return self._put(keyname, value)", "path": "appengine_utilities\\sessions.py", "repo_name": "joerussbowman/gaeutilities", "stars": 78, "license": "bsd-3-clause", "language": "python", "size": 1131}
{"docstring": "\"\"\"\nSave the state of node with a given regularity to the given\nfilename.\n\nArgs:\n    fname: File name to save retularly to\n    frequencey: Frequency in seconds that the state should be saved.\n                By default, 10 minutes.\n\"\"\"\n", "func_signal": "def saveStateRegularly(self, fname, frequency=600):\n", "code": "loop = LoopingCall(self.saveState, fname)\nloop.start(frequency)\nreturn loop", "path": "subspace\\network.py", "repo_name": "cpacia/Subspace", "stars": 82, "license": "mit", "language": "python", "size": 22919}
{"docstring": "\"\"\"\nRemove a list of peer ids from this heap.  Note that while this\nheap retains a constant visible size (based on the iterator), it's\nactual size may be quite a bit larger than what's exposed.  Therefore,\nremoval of nodes may not change the visible size as previously added\nnodes suddenly become visible.\n\"\"\"\n", "func_signal": "def remove(self, peerIDs):\n", "code": "peerIDs = set(peerIDs)\nif len(peerIDs) == 0:\n    return\nnheap = []\nfor distance, node in self.heap:\n    if node.id not in peerIDs:\n        heapq.heappush(nheap, (distance, node))\nself.heap = nheap", "path": "subspace\\node.py", "repo_name": "cpacia/Subspace", "stars": 82, "license": "mit", "language": "python", "size": 22919}
{"docstring": "\"\"\"\nOpenSSL random function\n\"\"\"\n", "func_signal": "def rand(self, size):\n", "code": "buffer = self.malloc(0, size)\n# This pyelliptic library, by default, didn't check the return value of RAND_bytes. It is \n# evidently possible that it returned an error and not-actually-random data. However, in\n# tests on various operating systems, while generating hundreds of gigabytes of random \n# strings of various sizes I could not get an error to occur. Also Bitcoin doesn't check\n# the return value of RAND_bytes either. \n# Fixed in Bitmessage version 0.4.2 (in source code on 2013-10-13)\nwhile self.RAND_bytes(buffer, size) != 1:\n    import time\n    time.sleep(1)\nreturn buffer.raw", "path": "subspace\\pyelliptic\\openssl.py", "repo_name": "cpacia/Subspace", "stars": 82, "license": "mit", "language": "python", "size": 22919}
{"docstring": "\"\"\"\nGet a :class:`list` of (ip, port) :class:`tuple` pairs suitable for use as an argument\nto the bootstrap method.\n\nThe server should have been bootstrapped\nalready - this is just a utility for getting some neighbors and then\nstoring them if this server is going down for a while.  When it comes\nback up, the list of nodes can be used to bootstrap.\n\"\"\"\n", "func_signal": "def bootstrappableNeighbors(self):\n", "code": "neighbors = self.protocol.router.findNeighbors(self.node)\nreturn [ tuple(n)[-2:] for n in neighbors ]", "path": "subspace\\network.py", "repo_name": "cpacia/Subspace", "stars": 82, "license": "mit", "language": "python", "size": 22919}
{"docstring": "\"\"\"\nLoad the state of this node (the alpha/ksize/id/immediate neighbors)\nfrom a cache file with the given fname.\n\"\"\"\n", "func_signal": "def loadState(self, fname, addrs, seeds):\n", "code": "with open(fname, 'r') as f:\n    data = pickle.load(f)\ns = Server(data['ksize'], data['alpha'], data['id'])\nif len(data['neighbors']) > 0:\n    data[\"neighbors\"].extend(addrs)\n    s.bootstrap(data['neighbors'], seeds)\nreturn s", "path": "subspace\\network.py", "repo_name": "cpacia/Subspace", "stars": 82, "license": "mit", "language": "python", "size": 22919}
{"docstring": "\"\"\"\nGet the internet visible IP's of this node as other nodes see it.\n\nReturns:\n    A `list` of IP's.  If no one can be contacted, then the `list` will be empty.\n\"\"\"\n", "func_signal": "def inetVisibleIP(self):\n", "code": "def handle(results):\n    ips = [ result[1][0] for result in results if result[0] ]\n    self.log.debug(\"other nodes think our ip is %s\" % str(ips))\n    return ips\n\nds = []\nfor neighbor in self.bootstrappableNeighbors():\n    ds.append(self.protocol.stun(neighbor))\nreturn defer.gatherResults(ds).addCallback(handle)", "path": "subspace\\network.py", "repo_name": "cpacia/Subspace", "stars": 82, "license": "mit", "language": "python", "size": 22919}
{"docstring": "\"\"\"\nreturns a create_string_buffer (ctypes)\n\"\"\"\n", "func_signal": "def malloc(self, data, size):\n", "code": "buffer = None\nif data != 0:\n    if sys.version_info.major == 3 and isinstance(data, type('')):\n        data = data.encode()\n    buffer = self.create_string_buffer(data, size)\nelse:\n    buffer = self.create_string_buffer(size)\nreturn buffer", "path": "subspace\\pyelliptic\\openssl.py", "repo_name": "cpacia/Subspace", "stars": 82, "license": "mit", "language": "python", "size": 22919}
{"docstring": "\"\"\"\nreturns the name of a elliptic curve with his id\n\"\"\"\n", "func_signal": "def get_curve_by_id(self, id):\n", "code": "res = None\nfor i in self.curves:\n    if self.curves[i] == id:\n        res = i\n        break\nif res is None:\n    raise Exception(\"Unknown curve\")\nreturn res", "path": "subspace\\pyelliptic\\openssl.py", "repo_name": "cpacia/Subspace", "stars": 82, "license": "mit", "language": "python", "size": 22919}
{"docstring": "\"\"\"\nBuild the wrapper\n\"\"\"\n", "func_signal": "def __init__(self, library):\n", "code": "self._lib = ctypes.CDLL(library)\n\nself.pointer = ctypes.pointer\nself.c_int = ctypes.c_int\nself.byref = ctypes.byref\nself.create_string_buffer = ctypes.create_string_buffer\n\nself.BN_new = self._lib.BN_new\nself.BN_new.restype = ctypes.c_void_p\nself.BN_new.argtypes = []\n\nself.BN_free = self._lib.BN_free\nself.BN_free.restype = None\nself.BN_free.argtypes = [ctypes.c_void_p]\n\nself.BN_num_bits = self._lib.BN_num_bits\nself.BN_num_bits.restype = ctypes.c_int\nself.BN_num_bits.argtypes = [ctypes.c_void_p]\n\nself.BN_bn2bin = self._lib.BN_bn2bin\nself.BN_bn2bin.restype = ctypes.c_int\nself.BN_bn2bin.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\nself.BN_bin2bn = self._lib.BN_bin2bn\nself.BN_bin2bn.restype = ctypes.c_void_p\nself.BN_bin2bn.argtypes = [ctypes.c_void_p, ctypes.c_int,\n                           ctypes.c_void_p]\n\nself.EC_KEY_free = self._lib.EC_KEY_free\nself.EC_KEY_free.restype = None\nself.EC_KEY_free.argtypes = [ctypes.c_void_p]\n\nself.EC_KEY_new_by_curve_name = self._lib.EC_KEY_new_by_curve_name\nself.EC_KEY_new_by_curve_name.restype = ctypes.c_void_p\nself.EC_KEY_new_by_curve_name.argtypes = [ctypes.c_int]\n\nself.EC_KEY_generate_key = self._lib.EC_KEY_generate_key\nself.EC_KEY_generate_key.restype = ctypes.c_int\nself.EC_KEY_generate_key.argtypes = [ctypes.c_void_p]\n\nself.EC_KEY_check_key = self._lib.EC_KEY_check_key\nself.EC_KEY_check_key.restype = ctypes.c_int\nself.EC_KEY_check_key.argtypes = [ctypes.c_void_p]\n\nself.EC_KEY_get0_private_key = self._lib.EC_KEY_get0_private_key\nself.EC_KEY_get0_private_key.restype = ctypes.c_void_p\nself.EC_KEY_get0_private_key.argtypes = [ctypes.c_void_p]\n\nself.EC_KEY_get0_public_key = self._lib.EC_KEY_get0_public_key\nself.EC_KEY_get0_public_key.restype = ctypes.c_void_p\nself.EC_KEY_get0_public_key.argtypes = [ctypes.c_void_p]\n\nself.EC_KEY_get0_group = self._lib.EC_KEY_get0_group\nself.EC_KEY_get0_group.restype = ctypes.c_void_p\nself.EC_KEY_get0_group.argtypes = [ctypes.c_void_p]\n\nself.EC_POINT_get_affine_coordinates_GFp = self._lib.EC_POINT_get_affine_coordinates_GFp\nself.EC_POINT_get_affine_coordinates_GFp.restype = ctypes.c_int\nself.EC_POINT_get_affine_coordinates_GFp.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\nself.EC_KEY_set_private_key = self._lib.EC_KEY_set_private_key\nself.EC_KEY_set_private_key.restype = ctypes.c_int\nself.EC_KEY_set_private_key.argtypes = [ctypes.c_void_p,\n                                        ctypes.c_void_p]\n\nself.EC_KEY_set_public_key = self._lib.EC_KEY_set_public_key\nself.EC_KEY_set_public_key.restype = ctypes.c_int\nself.EC_KEY_set_public_key.argtypes = [ctypes.c_void_p,\n                                       ctypes.c_void_p]\n\nself.EC_KEY_set_group = self._lib.EC_KEY_set_group\nself.EC_KEY_set_group.restype = ctypes.c_int\nself.EC_KEY_set_group.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\nself.EC_POINT_set_affine_coordinates_GFp = self._lib.EC_POINT_set_affine_coordinates_GFp\nself.EC_POINT_set_affine_coordinates_GFp.restype = ctypes.c_int\nself.EC_POINT_set_affine_coordinates_GFp.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\nself.EC_POINT_new = self._lib.EC_POINT_new\nself.EC_POINT_new.restype = ctypes.c_void_p\nself.EC_POINT_new.argtypes = [ctypes.c_void_p]\n\nself.EC_POINT_free = self._lib.EC_POINT_free\nself.EC_POINT_free.restype = None\nself.EC_POINT_free.argtypes = [ctypes.c_void_p]\n\nself.BN_CTX_free = self._lib.BN_CTX_free\nself.BN_CTX_free.restype = None\nself.BN_CTX_free.argtypes = [ctypes.c_void_p]\n\nself.EC_POINT_mul = self._lib.EC_POINT_mul\nself.EC_POINT_mul.restype = None\nself.EC_POINT_mul.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\nself.EC_KEY_set_private_key = self._lib.EC_KEY_set_private_key\nself.EC_KEY_set_private_key.restype = ctypes.c_int\nself.EC_KEY_set_private_key.argtypes = [ctypes.c_void_p,\n                                        ctypes.c_void_p]\n\nself.ECDH_OpenSSL = self._lib.ECDH_OpenSSL\nself._lib.ECDH_OpenSSL.restype = ctypes.c_void_p\nself._lib.ECDH_OpenSSL.argtypes = []\n\nself.BN_CTX_new = self._lib.BN_CTX_new\nself._lib.BN_CTX_new.restype = ctypes.c_void_p\nself._lib.BN_CTX_new.argtypes = []\n\nself.ECDH_set_method = self._lib.ECDH_set_method\nself._lib.ECDH_set_method.restype = ctypes.c_int\nself._lib.ECDH_set_method.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\nself.ECDH_compute_key = self._lib.ECDH_compute_key\nself.ECDH_compute_key.restype = ctypes.c_int\nself.ECDH_compute_key.argtypes = [ctypes.c_void_p,\n                                  ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p]\n\nself.EVP_CipherInit_ex = self._lib.EVP_CipherInit_ex\nself.EVP_CipherInit_ex.restype = ctypes.c_int\nself.EVP_CipherInit_ex.argtypes = [ctypes.c_void_p,\n                                   ctypes.c_void_p, ctypes.c_void_p]\n\nself.EVP_CIPHER_CTX_new = self._lib.EVP_CIPHER_CTX_new\nself.EVP_CIPHER_CTX_new.restype = ctypes.c_void_p\nself.EVP_CIPHER_CTX_new.argtypes = []\n\n# Cipher\nself.EVP_aes_128_cfb128 = self._lib.EVP_aes_128_cfb128\nself.EVP_aes_128_cfb128.restype = ctypes.c_void_p\nself.EVP_aes_128_cfb128.argtypes = []\n\nself.EVP_aes_256_cfb128 = self._lib.EVP_aes_256_cfb128\nself.EVP_aes_256_cfb128.restype = ctypes.c_void_p\nself.EVP_aes_256_cfb128.argtypes = []\n\nself.EVP_aes_128_cbc = self._lib.EVP_aes_128_cbc\nself.EVP_aes_128_cbc.restype = ctypes.c_void_p\nself.EVP_aes_128_cbc.argtypes = []\n\nself.EVP_aes_256_cbc = self._lib.EVP_aes_256_cbc\nself.EVP_aes_256_cbc.restype = ctypes.c_void_p\nself.EVP_aes_256_cbc.argtypes = []\n\n#self.EVP_aes_128_ctr = self._lib.EVP_aes_128_ctr\n#self.EVP_aes_128_ctr.restype = ctypes.c_void_p\n#self.EVP_aes_128_ctr.argtypes = []\n\n#self.EVP_aes_256_ctr = self._lib.EVP_aes_256_ctr\n#self.EVP_aes_256_ctr.restype = ctypes.c_void_p\n#self.EVP_aes_256_ctr.argtypes = []\n\nself.EVP_aes_128_ofb = self._lib.EVP_aes_128_ofb\nself.EVP_aes_128_ofb.restype = ctypes.c_void_p\nself.EVP_aes_128_ofb.argtypes = []\n\nself.EVP_aes_256_ofb = self._lib.EVP_aes_256_ofb\nself.EVP_aes_256_ofb.restype = ctypes.c_void_p\nself.EVP_aes_256_ofb.argtypes = []\n\nself.EVP_bf_cbc = self._lib.EVP_bf_cbc\nself.EVP_bf_cbc.restype = ctypes.c_void_p\nself.EVP_bf_cbc.argtypes = []\n\nself.EVP_bf_cfb64 = self._lib.EVP_bf_cfb64\nself.EVP_bf_cfb64.restype = ctypes.c_void_p\nself.EVP_bf_cfb64.argtypes = []\n\nself.EVP_rc4 = self._lib.EVP_rc4\nself.EVP_rc4.restype = ctypes.c_void_p\nself.EVP_rc4.argtypes = []\n\nself.EVP_CIPHER_CTX_cleanup = self._lib.EVP_CIPHER_CTX_cleanup\nself.EVP_CIPHER_CTX_cleanup.restype = ctypes.c_int\nself.EVP_CIPHER_CTX_cleanup.argtypes = [ctypes.c_void_p]\n\nself.EVP_CIPHER_CTX_free = self._lib.EVP_CIPHER_CTX_free\nself.EVP_CIPHER_CTX_free.restype = None\nself.EVP_CIPHER_CTX_free.argtypes = [ctypes.c_void_p]\n\nself.EVP_CipherUpdate = self._lib.EVP_CipherUpdate\nself.EVP_CipherUpdate.restype = ctypes.c_int\nself.EVP_CipherUpdate.argtypes = [ctypes.c_void_p,\n                                  ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int]\n\nself.EVP_CipherFinal_ex = self._lib.EVP_CipherFinal_ex\nself.EVP_CipherFinal_ex.restype = ctypes.c_int\nself.EVP_CipherFinal_ex.argtypes = [ctypes.c_void_p,\n                                    ctypes.c_void_p, ctypes.c_void_p]\n\nself.EVP_DigestInit = self._lib.EVP_DigestInit\nself.EVP_DigestInit.restype = ctypes.c_int\nself._lib.EVP_DigestInit.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\nself.EVP_DigestInit_ex = self._lib.EVP_DigestInit_ex\nself.EVP_DigestInit_ex.restype = ctypes.c_int\nself._lib.EVP_DigestInit_ex.argtypes = 3 * [ctypes.c_void_p]\n\nself.EVP_DigestUpdate = self._lib.EVP_DigestUpdate\nself.EVP_DigestUpdate.restype = ctypes.c_int\nself.EVP_DigestUpdate.argtypes = [ctypes.c_void_p,\n                                  ctypes.c_void_p, ctypes.c_int]\n\nself.EVP_DigestFinal = self._lib.EVP_DigestFinal\nself.EVP_DigestFinal.restype = ctypes.c_int\nself.EVP_DigestFinal.argtypes = [ctypes.c_void_p,\n                                 ctypes.c_void_p, ctypes.c_void_p]\n\nself.EVP_DigestFinal_ex = self._lib.EVP_DigestFinal_ex\nself.EVP_DigestFinal_ex.restype = ctypes.c_int\nself.EVP_DigestFinal_ex.argtypes = [ctypes.c_void_p,\n                                    ctypes.c_void_p, ctypes.c_void_p]\n\nself.EVP_ecdsa = self._lib.EVP_ecdsa\nself._lib.EVP_ecdsa.restype = ctypes.c_void_p\nself._lib.EVP_ecdsa.argtypes = []\n\nself.ECDSA_sign = self._lib.ECDSA_sign\nself.ECDSA_sign.restype = ctypes.c_int\nself.ECDSA_sign.argtypes = [ctypes.c_int, ctypes.c_void_p,\n                            ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\nself.ECDSA_verify = self._lib.ECDSA_verify\nself.ECDSA_verify.restype = ctypes.c_int\nself.ECDSA_verify.argtypes = [ctypes.c_int, ctypes.c_void_p,\n                              ctypes.c_int, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p]\n\nself.EVP_MD_CTX_create = self._lib.EVP_MD_CTX_create\nself.EVP_MD_CTX_create.restype = ctypes.c_void_p\nself.EVP_MD_CTX_create.argtypes = []\n\nself.EVP_MD_CTX_init = self._lib.EVP_MD_CTX_init\nself.EVP_MD_CTX_init.restype = None\nself.EVP_MD_CTX_init.argtypes = [ctypes.c_void_p]\n\nself.EVP_MD_CTX_destroy = self._lib.EVP_MD_CTX_destroy\nself.EVP_MD_CTX_destroy.restype = None\nself.EVP_MD_CTX_destroy.argtypes = [ctypes.c_void_p]\n\nself.RAND_bytes = self._lib.RAND_bytes\nself.RAND_bytes.restype = ctypes.c_int\nself.RAND_bytes.argtypes = [ctypes.c_void_p, ctypes.c_int]\n\n\nself.EVP_sha256 = self._lib.EVP_sha256\nself.EVP_sha256.restype = ctypes.c_void_p\nself.EVP_sha256.argtypes = []\n\nself.i2o_ECPublicKey = self._lib.i2o_ECPublicKey\nself.i2o_ECPublicKey.restype = ctypes.c_void_p\nself.i2o_ECPublicKey.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\nself.EVP_sha512 = self._lib.EVP_sha512\nself.EVP_sha512.restype = ctypes.c_void_p\nself.EVP_sha512.argtypes = []\n\nself.HMAC = self._lib.HMAC\nself.HMAC.restype = ctypes.c_void_p\nself.HMAC.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int,\n                      ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p]\n\ntry:\n    self.PKCS5_PBKDF2_HMAC = self._lib.PKCS5_PBKDF2_HMAC\nexcept:\n    # The above is not compatible with all versions of OSX.\n    self.PKCS5_PBKDF2_HMAC = self._lib.PKCS5_PBKDF2_HMAC_SHA1\n    \nself.PKCS5_PBKDF2_HMAC.restype = ctypes.c_int\nself.PKCS5_PBKDF2_HMAC.argtypes = [ctypes.c_void_p, ctypes.c_int,\n                                   ctypes.c_void_p, ctypes.c_int,\n                                   ctypes.c_int, ctypes.c_void_p,\n                                   ctypes.c_int, ctypes.c_void_p]\n\nself._set_ciphers()\nself._set_curves()", "path": "subspace\\pyelliptic\\openssl.py", "repo_name": "cpacia/Subspace", "stars": 82, "license": "mit", "language": "python", "size": 22919}
{"docstring": "\"\"\"\nRefresh buckets that haven't had any lookups in the last hour\n(per section 2.3 of the paper).\n\"\"\"\n", "func_signal": "def refreshTable(self):\n", "code": "ds = []\nfor id in self.protocol.getRefreshIDs():\n    node = Node(id)\n    nearest = self.protocol.router.findNeighbors(node, self.alpha)\n    spider = NodeSpiderCrawl(self.protocol, node, nearest)\n    ds.append(spider.find())\n\ndef republishKeys(_):\n    ds = []\n    # Republish keys older than one hour\n    for key, value in self.storage.iteritemsOlderThan(3600):\n        ds.append(self.set(key, value))\n    return defer.gatherResults(ds)\n\nreturn defer.gatherResults(ds).addCallback(republishKeys)", "path": "subspace\\network.py", "repo_name": "cpacia/Subspace", "stars": 82, "license": "mit", "language": "python", "size": 22919}
{"docstring": "\"\"\"\nGet a key if the network has it.\n\nReturns:\n    :class:`None` if not found, the value otherwise.\n\"\"\"\n", "func_signal": "def get(self, key):\n", "code": "node = Node(key)\nnearest = self.protocol.router.findNeighbors(node)\nif len(nearest) == 0:\n    self.log.warning(\"There are no known neighbors to get key %s\" % key)\n    return defer.succeed(None)\nelif len(key) != 64 or all(c in string.hexdigits for c in key) is not True:\n    self.log.warning(\"Invalid key\")\n    return defer.succeed(None)\nspider = ValueSpiderCrawl(self.protocol, node, nearest, self.ksize, self.alpha)\nreturn spider.find()", "path": "subspace\\network.py", "repo_name": "cpacia/Subspace", "stars": 82, "license": "mit", "language": "python", "size": 22919}
{"docstring": "\"\"\"\nMake a node.  Created a random id if not specified.\n\"\"\"\n", "func_signal": "def mknode(id=None, ip=None, port=None, intid=None):\n", "code": "if intid is not None:\n    id = pack('>l', intid)\nid = id or hashlib.sha1(str(random.getrandbits(255))).digest()\nreturn Node(id, ip, port)", "path": "subspace\\tests\\utils.py", "repo_name": "cpacia/Subspace", "stars": 82, "license": "mit", "language": "python", "size": 22919}
{"docstring": "\"\"\"\nCompute the key and the message with HMAC SHA512\n\"\"\"\n", "func_signal": "def hmac_sha512(k, m):\n", "code": "key = OpenSSL.malloc(k, len(k))\nd = OpenSSL.malloc(m, len(m))\nmd = OpenSSL.malloc(0, 64)\ni = OpenSSL.pointer(OpenSSL.c_int(0))\nOpenSSL.HMAC(OpenSSL.EVP_sha512(), key, len(k), d, len(m), md, i)\nreturn md.raw", "path": "subspace\\pyelliptic\\hash.py", "repo_name": "cpacia/Subspace", "stars": 82, "license": "mit", "language": "python", "size": 22919}
{"docstring": "\"\"\"\nreturns the OpenSSL cipher instance\n\"\"\"\n", "func_signal": "def get_cipher(self, name):\n", "code": "if name not in self.cipher_algo:\n    raise Exception(\"Unknown cipher\")\nreturn self.cipher_algo[name]", "path": "subspace\\pyelliptic\\openssl.py", "repo_name": "cpacia/Subspace", "stars": 82, "license": "mit", "language": "python", "size": 22919}
{"docstring": "\"\"\"\nGet ids to search for to keep old buckets up to date.\n\"\"\"\n", "func_signal": "def getRefreshIDs(self):\n", "code": "ids = []\nfor bucket in self.router.getLonelyBuckets():\n    ids.append(random.randint(*bucket.range))\nreturn ids", "path": "subspace\\tests\\utils.py", "repo_name": "cpacia/Subspace", "stars": 82, "license": "mit", "language": "python", "size": 22919}
{"docstring": "\"\"\"\nGiven the result of a DeferredList of calls to peers, ensure that at least\none of them was contacted and responded with a Truthy result.\n\"\"\"\n", "func_signal": "def _anyRespondSuccess(self, responses):\n", "code": "for deferSuccess, result in responses:\n    peerReached, peerResponse = result\n    if deferSuccess and peerReached and peerResponse:\n        return True\nreturn False", "path": "subspace\\network.py", "repo_name": "cpacia/Subspace", "stars": 82, "license": "mit", "language": "python", "size": 22919}
{"docstring": "\"\"\"\nConstructor.\n\n@param node: The node to measure all distnaces from.\n@param maxsize: The maximum size that this heap can grow to.\n\"\"\"\n", "func_signal": "def __init__(self, node, maxsize):\n", "code": "self.node = node\nself.heap = []\nself.contacted = set()\nself.maxsize = maxsize", "path": "subspace\\node.py", "repo_name": "cpacia/Subspace", "stars": 82, "license": "mit", "language": "python", "size": 22919}
{"docstring": "\"\"\"\nBootstrap the server by connecting to other known nodes in the network.\n\nArgs:\n    addrs: A `list` of (ip, port) `tuple` pairs.  Note that only IP addresses\n           are acceptable - hostnames will cause an error.\n    seeds: A 'list' ssl seeds formatted as \"hostname:port\". We will try to bootstrap\n           using addrs first an if that fails we will query the ssl seeds.\n\"\"\"\n", "func_signal": "def bootstrap(self, addrs, seeds):\n", "code": "def initTable(results):\n        nodes = []\n        for addr, result in results.items():\n            if result[0]:\n                nodes.append(Node(result[1], addr[0], addr[1]))\n        if len(nodes) < 1:\n            return self.bootstrap([], seeds)\n        spider = NodeSpiderCrawl(self.protocol, self.node, nodes, self.ksize, self.alpha)\n        return spider.find()\n\n# if the transport hasn't been initialized yet, wait a second\nif self.protocol.transport is None:\n    return task.deferLater(reactor, 1, self.bootstrap, addrs, seeds)\n\nif len(addrs) > 0:\n    ds = {}\n    for addr in addrs:\n        ds[addr] = self.protocol.ping(addr, self.node.id)\n    return deferredDict(ds).addCallback(initTable)\nelse:\n    ds = {}\n    def formatResult(results):\n        tup_list = []\n        for s, nodes in results.items():\n            tup_list.extend(nodes)\n        ds = {}\n        for addr in tup_list:\n            ds[addr] = self.protocol.ping(addr, self.node.id)\n        return deferredDict(ds).addCallback(initTable)\n\n    for seed in seeds:\n        ds[seed] = self.querySeed(seed)\n    return deferredDict(ds).addCallback(formatResult)", "path": "subspace\\network.py", "repo_name": "cpacia/Subspace", "stars": 82, "license": "mit", "language": "python", "size": 22919}
{"docstring": "\"\"\"\nBy default, max age is a week.\n\"\"\"\n", "func_signal": "def __init__(self, ttl=604800):\n", "code": "self.data = OrderedDict()\nself.ttl = ttl", "path": "subspace\\storage.py", "repo_name": "cpacia/Subspace", "stars": 82, "license": "mit", "language": "python", "size": 22919}
{"docstring": "\"\"\"\nSave the state of this node (the alpha/ksize/id/immediate neighbors)\nto a cache file with the given fname.\n\"\"\"\n", "func_signal": "def saveState(self, fname):\n", "code": "data = { 'ksize': self.ksize,\n         'alpha': self.alpha,\n         'id': self.node.id,\n         'neighbors': self.bootstrappableNeighbors() }\nif len(data['neighbors']) == 0:\n    self.log.warning(\"No known neighbors, so not writing to cache.\")\n    return\nwith open(fname, 'w') as f:\n    pickle.dump(data, f)", "path": "subspace\\network.py", "repo_name": "cpacia/Subspace", "stars": 82, "license": "mit", "language": "python", "size": 22919}
{"docstring": "# Deletes a drive\n", "func_signal": "def ex_destroy_drive(self, drive_uuid):\n", "code": "response = self.connection.request(\n    action='/drives/%s/destroy' % (drive_uuid),\n    method='POST'\n)\nreturn response.status == 204", "path": "libcloud\\compute\\drivers\\elastichosts.py", "repo_name": "cloudkick/libcloud", "stars": 114, "license": "apache-2.0", "language": "python", "size": 1609}
{"docstring": "# Returns a list of active (running) nodes\n", "func_signal": "def list_nodes(self):\n", "code": "response = self.connection.request(action='/servers/info').object\n\nnodes = []\nfor data in response:\n    node = self._to_node(data)\n    nodes.append(node)\n\nreturn nodes", "path": "libcloud\\compute\\drivers\\elastichosts.py", "repo_name": "cloudkick/libcloud", "stars": 114, "license": "apache-2.0", "language": "python", "size": 1609}
{"docstring": "# Reboots the node\n", "func_signal": "def reboot_node(self, node):\n", "code": "response = self.connection.request(\n    action='/servers/%s/reset' % (node.id),\n    method='POST'\n)\nreturn response.status == 204", "path": "libcloud\\compute\\drivers\\elastichosts.py", "repo_name": "cloudkick/libcloud", "stars": 114, "license": "apache-2.0", "language": "python", "size": 1609}
{"docstring": "\"\"\"Get SubjectAltNames\n\nRetrieve 'subjectAltName' attributes from cert data structure\n\"\"\"\n", "func_signal": "def _get_subject_alt_names(self, cert):\n", "code": "if 'subjectAltName' not in cert:\n    values = []\nelse:\n    values = [value\n              for field, value in cert['subjectAltName']\n              if field == 'DNS']\nreturn values", "path": "libcloud\\httplib_ssl.py", "repo_name": "cloudkick/libcloud", "stars": 114, "license": "apache-2.0", "language": "python", "size": 1609}
{"docstring": "\"\"\"\n@type name: C{str}\n@param name: Container name (must be unique).\n\n@type extra: C{dict}\n@param extra: Extra attributes.\n\n@type driver: C{StorageDriver}\n@param driver: StorageDriver instance.\n\"\"\"\n\n", "func_signal": "def __init__(self, name, extra, driver):\n", "code": "self.name = name\nself.extra = extra or {}\nself.driver = driver", "path": "libcloud\\storage\\base.py", "repo_name": "cloudkick/libcloud", "stars": 114, "license": "apache-2.0", "language": "python", "size": 1609}
{"docstring": "# An exception would indicate failure\n", "func_signal": "def test_reboot_node(self):\n", "code": "node = self.driver.list_nodes()[0]\nself.driver.reboot_node(node)", "path": "test\\compute\\test_linode.py", "repo_name": "cloudkick/libcloud", "stars": 114, "license": "apache-2.0", "language": "python", "size": 1609}
{"docstring": "# should return a node object\n", "func_signal": "def test_create_node_response(self):\n", "code": "node = self.driver.create_node(name=\"node-name\",\n                 location=self.driver.list_locations()[0],\n                 size=self.driver.list_sizes()[0],\n                 image=self.driver.list_images()[0],\n                 auth=NodeAuthPassword(\"foobar\"))\nself.assertTrue(isinstance(node[0], Node))", "path": "test\\compute\\test_linode.py", "repo_name": "cloudkick/libcloud", "stars": 114, "license": "apache-2.0", "language": "python", "size": 1609}
{"docstring": "# Kills the server immediately\n", "func_signal": "def destroy_node(self, node):\n", "code": "response = self.connection.request(\n    action='/servers/%s/destroy' % (node.id),\n    method='POST'\n)\nreturn response.status == 204", "path": "libcloud\\compute\\drivers\\elastichosts.py", "repo_name": "cloudkick/libcloud", "stars": 114, "license": "apache-2.0", "language": "python", "size": 1609}
{"docstring": "# Changes the configuration of the running server\n", "func_signal": "def ex_set_node_configuration(self, node, **kwargs):\n", "code": "valid_keys = ('^name$', '^parent$', '^cpu$', '^smp$', '^mem$',\n              '^boot$', '^nic:0:model$', '^nic:0:dhcp',\n              '^nic:1:model$', '^nic:1:vlan$', '^nic:1:mac$',\n              '^vnc:ip$', '^vnc:password$', '^vnc:tls',\n              '^ide:[0-1]:[0-1](:media)?$',\n              '^scsi:0:[0-7](:media)?$', '^block:[0-7](:media)?$')\n\ninvalid_keys = []\nfor key in kwargs.keys():\n    matches = False\n    for regex in valid_keys:\n        if re.match(regex, key):\n            matches = True\n            break\n    if not matches:\n        invalid_keys.append(key)\n\nif invalid_keys:\n    raise ElasticHostsException(\n        'Invalid configuration key specified: %s'\n        % (',' .join(invalid_keys))\n    )\n\nresponse = self.connection.request(\n    action='/servers/%s/set' % (node.id), data=json.dumps(kwargs),\n    method='POST'\n)\n\nreturn (response.status == 200 and response.body != '')", "path": "libcloud\\compute\\drivers\\elastichosts.py", "repo_name": "cloudkick/libcloud", "stars": 114, "license": "apache-2.0", "language": "python", "size": 1609}
{"docstring": "# Sends the ACPI power-down event\n", "func_signal": "def ex_shutdown_node(self, node):\n", "code": "response = self.connection.request(\n    action='/servers/%s/shutdown' % (node.id),\n    method='POST'\n)\nreturn response.status == 204", "path": "libcloud\\compute\\drivers\\elastichosts.py", "repo_name": "cloudkick/libcloud", "stars": 114, "license": "apache-2.0", "language": "python", "size": 1609}
{"docstring": "\"\"\"Setup Verify SSL or not\n\nReads security module's VERIFY_SSL_CERT and toggles whether\nthe class overrides the connect() class method or runs the\ninherited httplib.HTTPSConnection connect()\n\"\"\"\n", "func_signal": "def _setup_verify(self):\n", "code": "self.verify = libcloud.security.VERIFY_SSL_CERT\n\nif self.verify:\n    self._setup_ca_cert()\nelse:\n    warnings.warn(libcloud.security.VERIFY_SSL_DISABLED_MSG)", "path": "libcloud\\httplib_ssl.py", "repo_name": "cloudkick/libcloud", "stars": 114, "license": "apache-2.0", "language": "python", "size": 1609}
{"docstring": "\"\"\"Setup CA Certs\n\nSearch in CA_CERTS_PATH for valid candidates and\nreturn first match.  Otherwise, complain about certs\nnot being available.\n\"\"\"\n", "func_signal": "def _setup_ca_cert(self):\n", "code": "if not self.verify:\n    return\n\nca_certs_available = [cert\n                      for cert in libcloud.security.CA_CERTS_PATH\n                      if os.path.exists(cert)]\nif ca_certs_available:\n    # use first available certificate\n    self.ca_cert = ca_certs_available[0]\nelse:\n    # no certificates found; toggle verify to False\n    warnings.warn(libcloud.security.CA_CERTS_UNAVAILABLE_MSG)\n    self.ca_cert = None\n    self.verify = False", "path": "libcloud\\httplib_ssl.py", "repo_name": "cloudkick/libcloud", "stars": 114, "license": "apache-2.0", "language": "python", "size": 1609}
{"docstring": "# Returns a list of available pre-installed system drive images\n", "func_signal": "def list_images(self, location=None):\n", "code": "images = []\nfor key, value in STANDARD_DRIVES.iteritems():\n    image = NodeImage(\n        id=value['uuid'],\n        name=value['description'],\n        driver=self.connection.driver,\n        extra={\n            'size_gunzipped': value['size_gunzipped']\n        }\n    )\n    images.append(image)\n\nreturn images", "path": "libcloud\\compute\\drivers\\elastichosts.py", "repo_name": "cloudkick/libcloud", "stars": 114, "license": "apache-2.0", "language": "python", "size": 1609}
{"docstring": "\"\"\"Creates a ElasticHosts instance\n\nSee L{NodeDriver.create_node} for more keyword args.\n\n@keyword    name: String with a name for this new node (required)\n@type       name: C{string}\n\n@keyword    smp: Number of virtual processors or None to calculate\n                 based on the cpu speed\n@type       smp: C{int}\n\n@keyword    nic_model: e1000, rtl8139 or virtio\n                       (if not specified, e1000 is used)\n@type       nic_model: C{string}\n\n@keyword    vnc_password: If set, the same password is also used for\n                          SSH access with user toor,\n                          otherwise VNC access is disabled and\n                          no SSH login is possible.\n@type       vnc_password: C{string}\n\"\"\"\n", "func_signal": "def create_node(self, **kwargs):\n", "code": "size = kwargs['size']\nimage = kwargs['image']\nsmp = kwargs.get('smp', 'auto')\nnic_model = kwargs.get('nic_model', 'e1000')\nvnc_password = ssh_password = kwargs.get('vnc_password', None)\n\nif nic_model not in ('e1000', 'rtl8139', 'virtio'):\n    raise ElasticHostsException('Invalid NIC model specified')\n\n# check that drive size is not smaller then pre installed image size\n\n# First we create a drive with the specified size\ndrive_data = {}\ndrive_data.update({'name': kwargs['name'],\n                   'size': '%sG' % (kwargs['size'].disk)})\n\nresponse = self.connection.request(action='/drives/create',\n                                   data=json.dumps(drive_data),\n                                   method='POST').object\n\nif not response:\n    raise ElasticHostsException('Drive creation failed')\n\ndrive_uuid = response['drive']\n\n# Then we image the selected pre-installed system drive onto it\nresponse = self.connection.request(\n    action='/drives/%s/image/%s/gunzip' % (drive_uuid, image.id),\n    method='POST'\n)\n\nif response.status != 204:\n    raise ElasticHostsException('Drive imaging failed')\n\n# We wait until the drive is imaged and then boot up the node\n# (in most cases, the imaging process shouldn't take longer\n# than a few minutes)\nresponse = self.connection.request(\n    action='/drives/%s/info' % (drive_uuid)\n).object\nimaging_start = time.time()\nwhile response.has_key('imaging'):\n    response = self.connection.request(\n        action='/drives/%s/info' % (drive_uuid)\n    ).object\n    elapsed_time = time.time() - imaging_start\n    if (response.has_key('imaging')\n        and elapsed_time >= IMAGING_TIMEOUT):\n        raise ElasticHostsException('Drive imaging timed out')\n    time.sleep(1)\n\nnode_data = {}\nnode_data.update({'name': kwargs['name'],\n                  'cpu': size.cpu,\n                  'mem': size.ram,\n                  'ide:0:0': drive_uuid,\n                  'boot': 'ide:0:0',\n                  'smp': smp})\nnode_data.update({'nic:0:model': nic_model, 'nic:0:dhcp': 'auto'})\n\nif vnc_password:\n    node_data.update({'vnc:ip': 'auto', 'vnc:password': vnc_password})\n\nresponse = self.connection.request(\n    action='/servers/create', data=json.dumps(node_data),\n    method='POST'\n).object\n\nif isinstance(response, list):\n    nodes = [self._to_node(node, ssh_password) for node in response]\nelse:\n    nodes = self._to_node(response, ssh_password)\n\nreturn nodes", "path": "libcloud\\compute\\drivers\\elastichosts.py", "repo_name": "cloudkick/libcloud", "stars": 114, "license": "apache-2.0", "language": "python", "size": 1609}
{"docstring": "\"\"\"Get Common Name\n\nRetrieve 'commonName' attribute from cert data structure\n\"\"\"\n", "func_signal": "def _get_common_name(self, cert):\n", "code": "if 'subject' not in cert:\n    return None\nvalues = [value[0][1]\n          for value in cert['subject']\n          if value[0][0] == 'commonName']\nreturn values", "path": "libcloud\\httplib_ssl.py", "repo_name": "cloudkick/libcloud", "stars": 114, "license": "apache-2.0", "language": "python", "size": 1609}
{"docstring": "\"\"\"Verify hostname against peer cert\n\nCheck both commonName and entries in subjectAltName, using a\nrudimentary glob to dns regex check to find matches\n\"\"\"\n", "func_signal": "def _verify_hostname(self, hostname, cert):\n", "code": "common_name = self._get_common_name(cert)\nalt_names = self._get_subject_alt_names(cert)\n\n# replace * with alphanumeric and dash\n# replace . with literal .\nvalid_patterns = [\n    re.compile(\n        pattern.replace(\n            r\".\", r\"\\.\"\n        ).replace(\n            r\"*\", r\"[0-9A-Za-z]+\"\n        )\n    )\n    for pattern\n    in (set(common_name) | set(alt_names))\n]\n\nreturn any(\n    pattern.search(hostname)\n    for pattern in valid_patterns\n)", "path": "libcloud\\httplib_ssl.py", "repo_name": "cloudkick/libcloud", "stars": 114, "license": "apache-2.0", "language": "python", "size": 1609}
{"docstring": "\"\"\"\nCreate a new node, and start deployment.\n\n@keyword    enable_root: If true, root password will be set to\n                         vnc_password (this will enable SSH access)\n                         and default 'toor' account will be deleted.\n@type       enable_root: C{bool}\n\nFor detailed description and keywords args, see\nL{NodeDriver.deploy_node}.\n\"\"\"\n", "func_signal": "def deploy_node(self, **kwargs):\n", "code": "image = kwargs['image']\nvnc_password = kwargs.get('vnc_password', None)\nenable_root = kwargs.get('enable_root', False)\n\nif not vnc_password:\n    raise ValueError('You need to provide vnc_password argument '\n                     'if you want to use deployment')\n\nif (image in STANDARD_DRIVES\n    and STANDARD_DRIVES[image]['supports_deployment']):\n    raise ValueError('Image %s does not support deployment'\n                     % (image.id))\n\nif enable_root:\n    script = (\"unset HISTFILE;\"\n              \"echo root:%s | chpasswd;\"\n              \"sed -i '/^toor.*$/d' /etc/passwd /etc/shadow;\"\n              \"history -c\") % vnc_password\n    root_enable_script = ScriptDeployment(script=script,\n                                          delete=True)\n    deploy = kwargs.get('deploy', None)\n    if deploy:\n        if (isinstance(deploy, ScriptDeployment)\n            or isinstance(deploy, SSHKeyDeployment)):\n            deployment = MultiStepDeployment([deploy,\n                                              root_enable_script])\n        elif isinstance(deploy, MultiStepDeployment):\n            deployment = deploy\n            deployment.add(root_enable_script)\n    else:\n        deployment = root_enable_script\n\n    kwargs['deploy'] = deployment\n\nif not kwargs.get('ssh_username', None):\n    kwargs['ssh_username'] = 'toor'\n\nreturn super(ElasticHostsBaseNodeDriver, self).deploy_node(**kwargs)", "path": "libcloud\\compute\\drivers\\elastichosts.py", "repo_name": "cloudkick/libcloud", "stars": 114, "license": "apache-2.0", "language": "python", "size": 1609}
{"docstring": "\"\"\"Create a new SoftLayer node\n\nSee L{NodeDriver.create_node} for more keyword args.\n@keyword    ex_domain: e.g. libcloud.org\n@type       ex_domain: C{string}\n\"\"\"\n", "func_signal": "def create_node(self, **kwargs):\n", "code": "name = kwargs['name']\nimage = kwargs['image']\nsize = kwargs['size']\ndomain = kwargs.get('ex_domain')\nlocation = kwargs['location']\nif domain == None:\n    if name.find(\".\") != -1:\n        domain = name[name.find('.')+1:]\n\nif domain == None:\n    # TODO: domain is a required argument for the Sofylayer API, but it\n    # it shouldn't be.\n    domain = \"exmaple.com\"\n\nres = {'prices': SL_TEMPLATES[size.id]['prices']}\nres['packageId'] = DEFAULT_PACKAGE\nres['prices'].append({'id': image.id})  # Add OS to order\nres['location'] = location.id\nres['complexType'] = 'SoftLayer_Container_Product_Order_Virtual_Guest'\nres['quantity'] = 1\nres['useHourlyPricing'] = True\nres['virtualGuests'] = [\n    {\n        'hostname': name,\n        'domain': domain\n    }\n]\n\nres = self.connection.request(\n    \"SoftLayer_Product_Order\",\n    \"placeOrder\",\n    res\n)\n\norder_id = res['orderId']\nraw_node = self._get_order_information(order_id)\n\nreturn self._to_node(raw_node)", "path": "libcloud\\compute\\drivers\\softlayer.py", "repo_name": "cloudkick/libcloud", "stars": 114, "license": "apache-2.0", "language": "python", "size": 1609}
{"docstring": "# An exception would indicate failure\n", "func_signal": "def test_destroy_node(self):\n", "code": "node = self.driver.list_nodes()[0]\nself.driver.destroy_node(node)", "path": "test\\compute\\test_linode.py", "repo_name": "cloudkick/libcloud", "stars": 114, "license": "apache-2.0", "language": "python", "size": 1609}
{"docstring": "\"\"\"Connect\n\nChecks if verification is toggled; if not, just call\nhttplib.HTTPSConnection's connect\n\"\"\"\n", "func_signal": "def connect(self):\n", "code": "if not self.verify:\n    return httplib.HTTPSConnection.connect(self)\n\n# otherwise, create a connection and verify the hostname\n# use socket.create_connection (in 2.6+) if possible\nif getattr(socket, 'create_connection', None):\n    sock = socket.create_connection((self.host, self.port),\n                                    self.timeout)\nelse:\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sock.connect((self.host, self.port))\nself.sock = ssl.wrap_socket(sock,\n                            self.key_file,\n                            self.cert_file,\n                            cert_reqs=ssl.CERT_REQUIRED,\n                            ca_certs=self.ca_cert,\n                            ssl_version=ssl.PROTOCOL_TLSv1)\ncert = self.sock.getpeercert()\nif not self._verify_hostname(self.host, cert):\n    raise ssl.SSLError('Failed to verify hostname')", "path": "libcloud\\httplib_ssl.py", "repo_name": "cloudkick/libcloud", "stars": 114, "license": "apache-2.0", "language": "python", "size": 1609}
{"docstring": "# Currently horribly inefficient\n", "func_signal": "def __eq__(self, other):\n", "code": "if not isinstance(other, NodeSelection):\n    return False # todo: should be NotImplemented?\nreturn set(self) == set(other)", "path": "garlicsim\\garlicsim\\data_structures\\node_selection.py", "repo_name": "cool-RR/GarlicSim", "stars": 68, "license": "None", "language": "python", "size": 16556}
{"docstring": "'''Decorate `function` to use this context manager when it's called.'''\n", "func_signal": "def __call__(self, function):\n", "code": "def inner(function_, *args, **kwargs):\n    with self:\n        return function_(*args, **kwargs)\nreturn decorator_tools.decorator(inner, function)", "path": "garlicsim\\garlicsim\\general_misc\\context_manager.py", "repo_name": "cool-RR/GarlicSim", "stars": 68, "license": "None", "language": "python", "size": 16556}
{"docstring": "'''Test `SleekCallArgs` on unhashable arguments.'''\n", "func_signal": "def test_unhashable():\n", "code": "sca_dict = {}\n\nargs = ([1, 2], {1: [1, 2]}, set(['a', 1]))\nsca1 = SleekCallArgs(sca_dict, f, *args)\nhash(sca1)\nsca_dict[sca1] = 'meow'\ndel args\ngc.collect()\n# GCed because there's a `set` in `args`, and it's weakreffable:\nassert len(sca_dict) == 0\n\nkwargs = {\n    'a': {1: 2},\n    'b': [\n        set(),\n        set(\n            (\n                frozenset((3, 4)),\n            )\n        )\n    ]\n}\nsca2 = SleekCallArgs(sca_dict, f, **kwargs)\nhash(sca2)\nsca_dict[sca2] = 'meow'\ndel kwargs\ngc.collect()\n# Not GCed because all objects in `kwargs` are not weakreffable:\nassert len(sca_dict) == 1", "path": "garlicsim\\test_garlicsim\\test_general_misc\\test_sleek_refs\\test_sleek_call_args.py", "repo_name": "cool-RR/GarlicSim", "stars": 68, "license": "None", "language": "python", "size": 16556}
{"docstring": "# preparation\n", "func_signal": "def MyContextManager():\n", "code": "try:\n    yield\nfinally:\n    pass # cleanup", "path": "garlicsim\\garlicsim\\general_misc\\context_manager.py", "repo_name": "cool-RR/GarlicSim", "stars": 68, "license": "None", "language": "python", "size": 16556}
{"docstring": "'''Trim a docstring, removing redundant tabs.'''\n", "func_signal": "def docstring_trim(docstring):\n", "code": "if not docstring:\n    return ''\n# Convert tabs to spaces (following the normal Python rules)\n# and split into a list of lines:\nlines = docstring.expandtabs().splitlines()\n# Determine minimum indentation (first line doesn't count):\nindent = sys.maxint\nfor line in lines[1:]:\n    stripped = line.lstrip()\n    if stripped:\n        indent = min(indent, len(line) - len(stripped))\n# Remove indentation (first line is special):\ntrimmed = [lines[0].strip()]\nif indent < sys.maxint:\n    for line in lines[1:]:\n        trimmed.append(line[indent:].rstrip())\n# Strip off trailing and leading blank lines:\nwhile trimmed and not trimmed[-1]:\n    trimmed.pop()\nwhile trimmed and not trimmed[0]:\n    trimmed.pop(0)\n    \nreturn '\\n'.join(trimmed)", "path": "garlicsim\\garlicsim\\general_misc\\string_tools.py", "repo_name": "cool-RR/GarlicSim", "stars": 68, "license": "None", "language": "python", "size": 16556}
{"docstring": "'''\nConstruct the `NodeSelection`.\n\n`ranges` is a list of node ranges that this selection will be made of.\n'''\n", "func_signal": "def __init__(self, ranges=()):\n", "code": "self.ranges = [ranges] if isinstance(ranges, NodeRange) else \\\n              list(ranges)", "path": "garlicsim\\garlicsim\\data_structures\\node_selection.py", "repo_name": "cool-RR/GarlicSim", "stars": 68, "license": "None", "language": "python", "size": 16556}
{"docstring": "'''Cleanup after suite execution.'''\n    \n\n", "func_signal": "def __exit__(self, type_=None, value=None, traceback=None):\n", "code": "\n'''\nInitialize a `ContextManager` made from a lone generator function.\n'''\nself._ContextManager__args = args\nself._ContextManager__kwargs = kwargs\nself._ContextManager__generators = []", "path": "garlicsim\\garlicsim\\general_misc\\context_manager.py", "repo_name": "cool-RR/GarlicSim", "stars": 68, "license": "None", "language": "python", "size": 16556}
{"docstring": "'''\nReturn whether `cls` is `ContextManager`.\n\nIt's an ugly method, but unfortunately it's necessary because at one\npoint we want to test if a class is `ContextManager` before\n`ContextManager` is defined in this module.\n'''\n\n", "func_signal": "def __is_the_base_context_manager_class(cls):\n", "code": "return (\n    (cls.__name__ == 'ContextManager') and\n    (cls.__module__ == 'garlicsim.general_misc.context_manager') and\n    (cls.mro() == [cls, object])\n)", "path": "garlicsim\\garlicsim\\general_misc\\context_manager.py", "repo_name": "cool-RR/GarlicSim", "stars": 68, "license": "None", "language": "python", "size": 16556}
{"docstring": "# preparation\n", "func_signal": "def MyContextManager():\n", "code": "try:\n    yield\nfinally:\n    pass # cleanup", "path": "garlicsim\\garlicsim\\general_misc\\context_manager.py", "repo_name": "cool-RR/GarlicSim", "stars": 68, "license": "None", "language": "python", "size": 16556}
{"docstring": "'''\nConvert a string from camelcase to spacecase.\n\nExample: camelcase_to_underscore('HelloWorld') == 'Hello world'\n'''\n", "func_signal": "def camelcase_to_spacecase(s):\n", "code": "if s == '': return s\ncharacter_process = lambda c: (' ' + c.lower()) if c.isupper() else c\nreturn s[0] + ''.join(character_process(c) for c in s[1:])", "path": "garlicsim\\garlicsim\\general_misc\\string_tools.py", "repo_name": "cool-RR/GarlicSim", "stars": 68, "license": "None", "language": "python", "size": 16556}
{"docstring": "'''\nCompact the `NodeSelection`.\n\nThis'll make it use the minimum number of node ranges while still\ncontaining exactly the same nodes.\n'''\n", "func_signal": "def compact(self):\n", "code": "for node_range in self.ranges:\n    node_range._sanity_check()\n\ntry:\n    while True:\n        self.__partially_compact()\nexcept CompletelyCompact:\n    return", "path": "garlicsim\\garlicsim\\data_structures\\node_selection.py", "repo_name": "cool-RR/GarlicSim", "stars": 68, "license": "None", "language": "python", "size": 16556}
{"docstring": "'''Perform union between two `NodeSelections` and return the result.'''\n", "func_signal": "def __or__(self, other):\n", "code": "assert isinstance(other, NodeSelection)\nreturn NodeSelection(self.ranges + other.ranges)", "path": "garlicsim\\garlicsim\\data_structures\\node_selection.py", "repo_name": "cool-RR/GarlicSim", "stars": 68, "license": "None", "language": "python", "size": 16556}
{"docstring": "'''Iterate over the nodes that are members of this `NodeSelection`.'''\n", "func_signal": "def __iter__(self):\n", "code": "for node_range in self.ranges:\n    for node in node_range:\n        yield node", "path": "garlicsim\\garlicsim\\data_structures\\node_selection.py", "repo_name": "cool-RR/GarlicSim", "stars": 68, "license": "None", "language": "python", "size": 16556}
{"docstring": "'''Try to make the NodeSelection a bit more compact.'''\n# todo: consider defining a canonic decomposition of a node selection\n# to node ranges. This will make a few things easier, like checking\n# equality.\n", "func_signal": "def __partially_compact(self):\n", "code": "first, second = None, None\nfor (r1, r2) in sequence_tools.combinations(self.ranges, 2):\n    if r1.head in r2:\n        second, first = r1, r2\n        break\n    elif r2.head in r1:\n        first, second = r1, r2\n        break\n    else:\n        pass\nif first is not None and second is not None:\n    if second.tail in first:\n        pass\n    else: # second.tail not in first\n        for current in second:\n            if current not in first:\n                break\n        if current.parent is first.tail:\n            self.ranges.remove(first)\n            new_range = NodeRange(head=first.head, tail=second.tail)\n        else:\n            new_range = NodeRange(head=current, tail=second.tail)\n        self.ranges.append(new_range)\n      \n    self.ranges.remove(second)\n    return\nelse:\n    raise CompletelyCompact", "path": "garlicsim\\garlicsim\\data_structures\\node_selection.py", "repo_name": "cool-RR/GarlicSim", "stars": 68, "license": "None", "language": "python", "size": 16556}
{"docstring": "'''\nPrepare for suite execution.\n\nThis is used as `__enter__` for context managers that use a\n`manage_context` function.\n'''\n", "func_signal": "def __enter_using_manage_context(self):\n", "code": "if not hasattr(self, '_ContextManager__generators'):\n    self._ContextManager__generators = []\n\nnew_generator = self.manage_context(\n    *getattr(self, '_ContextManager__args', ()),\n    **getattr(self, '_ContextManager__kwargs', {})\n)\nassert isinstance(new_generator, types.GeneratorType)\nself._ContextManager__generators.append(new_generator)\n\n\ntry:\n    generator_return_value = new_generator.next()\n    return self if (generator_return_value is SelfHook) else \\\n           generator_return_value\n\nexcept StopIteration:\n    raise RuntimeError(\"The generator didn't yield even one time; it \"\n                       \"must yield one time exactly.\")", "path": "garlicsim\\garlicsim\\general_misc\\context_manager.py", "repo_name": "cool-RR/GarlicSim", "stars": 68, "license": "None", "language": "python", "size": 16556}
{"docstring": "'''Test the basic workings of `SleekCallArgs`.'''\n", "func_signal": "def test():\n", "code": "sca_dict = {}\n\nargs = (1, 2)\nsca1 = SleekCallArgs(sca_dict, f, *args)\nsca_dict[sca1] = 'meow'\ndel args\ngc.collect()\nassert len(sca_dict) == 1\n\nargs = (1, A())\nsca2 = SleekCallArgs(sca_dict, f, *args)\nsca_dict[sca2] = 'meow'\ndel args\ngc.collect()\nassert len(sca_dict) == 1", "path": "garlicsim\\test_garlicsim\\test_general_misc\\test_sleek_refs\\test_sleek_call_args.py", "repo_name": "cool-RR/GarlicSim", "stars": 68, "license": "None", "language": "python", "size": 16556}
{"docstring": "'''\nCreate a new `ContextManager`.\n\nThis can work in two ways, depending on which arguments are given:\n\n 1. The classic `type.__call__` way. If `name, bases, namespace` are\n    passed in, `type.__call__` will be used normally.\n    \n 2. As a decorator for a generator function. For example:\n    \n        @ContextManagerType\n        def MyContextManager():\n            # preparation\n            try:\n                yield\n            finally:\n                pass # cleanup\n                \n    What happens here is that the function (in this case\n    `MyContextManager`) is passed directly into\n    `ContextManagerTypeType.__call__`. So we create a new\n    `ContextManager` subclass for it, and use the original generator as\n    its `.manage_context` function.\n                \n'''\n", "func_signal": "def __call__(cls, *args):\n", "code": "if len(args) == 1:\n    (function,) = args\n    assert callable(function)\n    name = function.__name__\n    bases = (ContextManager,)\n    namespace_dict = {\n        'manage_context': staticmethod(function),\n        '__init__': ContextManager.\\\n                    _ContextManager__init_lone_manage_context\n    }\n    return super(ContextManagerTypeType, cls).__call__(\n        name,\n        bases,\n        namespace_dict\n    )\n    \nelse:\n    return super(ContextManagerTypeType, cls).__call__(*args)", "path": "garlicsim\\garlicsim\\general_misc\\context_manager.py", "repo_name": "cool-RR/GarlicSim", "stars": 68, "license": "None", "language": "python", "size": 16556}
{"docstring": "'''Shallow-copy the `NodeSelection`.'''\n", "func_signal": "def copy(self):\n", "code": "klass = type(self)\nreturn klass((node_range.copy() for node_range in self.ranges))", "path": "garlicsim\\garlicsim\\data_structures\\node_selection.py", "repo_name": "cool-RR/GarlicSim", "stars": 68, "license": "None", "language": "python", "size": 16556}
{"docstring": "'''\nConvert a string from camelcase to underscore.\n\nExample: camelcase_to_underscore('HelloWorld') == 'hello_world'\n'''\n", "func_signal": "def camelcase_to_underscore(s):\n", "code": "return re.sub('(((?<=[a-z])[A-Z])|([A-Z](?![A-Z]|$)))', '_\\\\1', s).\\\n       lower().strip('_')", "path": "garlicsim\\garlicsim\\general_misc\\string_tools.py", "repo_name": "cool-RR/GarlicSim", "stars": 68, "license": "None", "language": "python", "size": 16556}
{"docstring": "'''\nCreate either `ContextManager` itself or a subclass of it.\n\nFor subclasses of `ContextManager`, if a `manage_context` method is\navailable, we will use `__enter__` and `__exit__` that will use the\ngenerator returned by `manage_context`.\n'''\n", "func_signal": "def __new__(mcls, name, bases, namespace):\n", "code": "if 'manage_context' in namespace:\n    manage_context = namespace['manage_context']\n    if '__enter__' in namespace:\n        raise Exception(\n            'You defined both an `__enter__` method and a '\n            '`manage_context` method-- That is unallowed. You need to '\n            '*either* define a `manage_context` method *or* an '\n            '`__enter__` and `__exit__` pair.'\n        )\n    if '__exit__' in namespace:\n        raise Exception(\n            'You defined both an `__exit__` method and a '\n            '`manage_context` method-- That is unallowed. You need to '\n            '*either* define a `manage_context` method *or* an '\n            '`__enter__` and `__exit__` pair.'\n        )\n    namespace['__enter__'] = \\\n        ContextManager._ContextManager__enter_using_manage_context\n    namespace['__exit__'] = \\\n        ContextManager._ContextManager__exit_using_manage_context\n    \nresult_class = super(ContextManagerType, mcls).__new__(\n    mcls,\n    name,\n    bases,\n    namespace\n)\n\n\nif (not result_class.__is_the_base_context_manager_class()) and \\\n   ('manage_context' not in namespace) and \\\n   hasattr(result_class, 'manage_context'):\n    \n    # What this `if` just checked for is: Is this a class that doesn't\n    # define `manage_context`, but whose base context manager class\n    # *does* define `manage_context`?\n    #\n    # If so, we need to be careful. It's okay for this class to be\n    # using the enter/exit pair provided by the base `manage_context`;\n    # It's also okay for this class to override these with its own\n    # `__enter__` and `__exit__` implementations; but it's *not* okay\n    # for this class to define just one of these methods, say\n    # `__enter__`, because then it will not have an `__exit__` to work\n    # with.\n    \n    our_enter_uses_manage_context = (\n        getattr(result_class.__enter__, 'im_func',\n        result_class.__enter__) == ContextManager.\\\n        _ContextManager__enter_using_manage_context.im_func\n    )\n    \n    our_exit_uses_manage_context = (\n        getattr(result_class.__exit__, 'im_func',\n        result_class.__exit__) == ContextManager.\\\n        _ContextManager__exit_using_manage_context.im_func\n    )\n    \n    if our_exit_uses_manage_context and not \\\n       our_enter_uses_manage_context:\n        \n        assert '__enter__' in namespace\n    \n        raise Exception(\"The %s class defines an `__enter__` method, \"\n                        \"but not an `__exit__` method; we cannot use \"\n                        \"the `__exit__` method of its base context \"\n                        \"manager class because it uses the \"\n                        \"`manage_context` generator function.\" %\n                        result_class)\n\n    \n    if our_enter_uses_manage_context and not \\\n       our_exit_uses_manage_context:\n        \n        assert '__exit__' in namespace\n        \n        raise Exception(\"The %s class defines an `__exit__` method, \"\n                        \"but not an `__enter__` method; we cannot use \"\n                        \"the `__enter__` method of its base context \"\n                        \"manager class because it uses the \"\n                        \"`manage_context` generator function.\" %\n                        result_class)\n    \nreturn result_class", "path": "garlicsim\\garlicsim\\general_misc\\context_manager.py", "repo_name": "cool-RR/GarlicSim", "stars": 68, "license": "None", "language": "python", "size": 16556}
{"docstring": "\"\"\" check if we have to test a legacy account or not \"\"\"\n", "func_signal": "def clean_password(self):\n", "code": "if 'password' in self.cleaned_data:\n    if not self.user.check_password(self.cleaned_data['password']):\n        self.test_openid = True\nreturn self.cleaned_data['password']", "path": "django_authopenid\\forms.py", "repo_name": "nicksergeant/snipt-old", "stars": 84, "license": "other", "language": "python", "size": 670}
{"docstring": "# Stylize the code.\n", "func_signal": "def code_stylized(value):\n", "code": "code_stylized = highlight(value.code, get_lexer_by_name(value.lexer, encoding='UTF-8'), HtmlFormatter())\nreturn code_stylized", "path": "snippet\\templatetags\\code_stylized.py", "repo_name": "nicksergeant/snipt-old", "stars": 84, "license": "other", "language": "python", "size": 670}
{"docstring": "\"\"\" test if password is valid for this user \"\"\"\n", "func_signal": "def clean_password(self):\n", "code": "if 'username' in self.cleaned_data and \\\n        'password' in self.cleaned_data:\n    self.user_cache =  authenticate(\n            username = self.cleaned_data['username'], \n            password = self.cleaned_data['password']\n    )\n    if self.user_cache is None:\n        raise forms.ValidationError(_(\"Please enter a valid \\\n            username and password. Note that both fields are \\\n            case-sensitive.\"))\n    elif self.user_cache.is_active == False:\n        raise forms.ValidationError(_(\"This account is inactive.\"))\n    return self.cleaned_data['password']", "path": "django_authopenid\\forms.py", "repo_name": "nicksergeant/snipt-old", "stars": 84, "license": "other", "language": "python", "size": 670}
{"docstring": "\"\"\" test if username is valid and exist in database \"\"\"\n", "func_signal": "def clean_username(self):\n", "code": "if 'username' in self.cleaned_data:\n    if not username_re.search(self.cleaned_data['username']):\n        raise forms.ValidationError(_(\"Usernames can only contain \\\n            letters, numbers and underscores\"))\n    try:\n        user = User.objects.get(\n                username__exact = self.cleaned_data['username']\n                )\n    except User.DoesNotExist:\n        return self.cleaned_data['username']\n    except User.MultipleObjectsReturned:\n        raise forms.ValidationError(u'There is already more than one \\\n            account registered with that username. Please try \\\n            another.')\n    raise forms.ValidationError(_(\"This username is already \\\n        taken. Please choose another.\"))", "path": "django_authopenid\\forms.py", "repo_name": "nicksergeant/snipt-old", "stars": 84, "license": "other", "language": "python", "size": 670}
{"docstring": "\"\"\"\nHandle new user signups.\n\"\"\"\n", "func_signal": "def signup(request):\n", "code": "signup_form = UserCreationForm(request.POST)\n\n# If the signup form has been submitted and is valid, save the new user and log the user in, otherwise present the form (with errors, if necessary).\nif signup_form.is_valid():\n    signup_form.save()\n    \n    # Go ahead and log the new user in after we've created them.  Convenience!\n    user = authenticate(username=request.POST['username'], password=request.POST['password1'])\n    auth_login(request, user)\n\n    return HttpResponseRedirect('/')\nelse:\n    return render_to_response(\"registration/signup.html\", {\n        'signup_form' : signup_form,\n        'signup' : True,\n        'data': request.POST\n    }, context_instance=RequestContext(request))", "path": "views.py", "repo_name": "nicksergeant/snipt-old", "stars": 84, "license": "other", "language": "python", "size": 670}
{"docstring": "\"\"\" validate if email exist in database\n:return: raise error if it exist \"\"\"\n", "func_signal": "def clean_email(self):\n", "code": "if 'email' in self.cleaned_data:\n    try:\n        user = User.objects.get(email = self.cleaned_data['email'])\n    except User.DoesNotExist:\n        return self.cleaned_data['email']\n    except User.MultipleObjectsReturned:\n        raise forms.ValidationError(u'There is already more than one \\\n            account registered with that e-mail address. Please try \\\n            another.')\n    raise forms.ValidationError(u'This email is already registered \\\n            in our database. Please choose another.')\nreturn self.cleaned_data['email']", "path": "django_authopenid\\forms.py", "repo_name": "nicksergeant/snipt-old", "stars": 84, "license": "other", "language": "python", "size": 670}
{"docstring": "\"\"\" validate next \"\"\"\n", "func_signal": "def clean_next(self):\n", "code": "if 'next' in self.cleaned_data and self.cleaned_data['next'] != \"\":\n    self.cleaned_data['next'] = clean_next(self.cleaned_data['next'])\n    return self.cleaned_data['next']", "path": "django_authopenid\\forms.py", "repo_name": "nicksergeant/snipt-old", "stars": 84, "license": "other", "language": "python", "size": 670}
{"docstring": "# Adding model 'Ad'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_table('ad_ad', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('title', self.gf('django.db.models.fields.TextField')()),\n            ('url', self.gf('django.db.models.fields.TextField')()),\n            ('image', self.gf('django.db.models.fields.files.ImageField')(max_length=100)),\n            ('tags', self.gf('tagging.fields.TagField')(default='')),\n        ))\n        db.send_create_signal('ad', ['Ad'])", "path": "ad\\migrations\\0001_initial.py", "repo_name": "nicksergeant/snipt-old", "stars": 84, "license": "other", "language": "python", "size": 670}
{"docstring": "\"\"\" get user for this username \"\"\"\n", "func_signal": "def clean_username(self):\n", "code": "if 'username' in self.cleaned_data:\n    try:\n        self.user_cache = User.objects.get(\n                username = self.cleaned_data['username'])\n    except:\n        raise forms.ValidationError(_(\"Incorrect username.\"))\nreturn self.cleaned_data['username']", "path": "django_authopenid\\forms.py", "repo_name": "nicksergeant/snipt-old", "stars": 84, "license": "other", "language": "python", "size": 670}
{"docstring": "\"\"\" check if email don't exist \"\"\"\n", "func_signal": "def clean_email(self):\n", "code": "if 'email' in self.cleaned_data:\n    if self.user.email != self.cleaned_data['email']:\n        try:\n            user = User.objects.get(email = self.cleaned_data['email'])\n        except User.DoesNotExist:\n            return self.cleaned_data['email']\n        except User.MultipleObjectsReturned:\n            raise forms.ValidationError(u'There is already more than one \\\n                account registered with that e-mail address. Please try \\\n                another.')\n        raise forms.ValidationError(u'This email is already registered \\\n            in our database. Please choose another.')\nreturn self.cleaned_data['email']", "path": "django_authopenid\\forms.py", "repo_name": "nicksergeant/snipt-old", "stars": 84, "license": "other", "language": "python", "size": 670}
{"docstring": "\"\"\" check if we have to test a legacy account or not \"\"\"\n", "func_signal": "def clean_password(self):\n", "code": "if 'password' in self.cleaned_data:\n    if not self.user.check_password(self.cleaned_data['password']):\n        self.test_openid = True\nreturn self.cleaned_data['password']", "path": "django_authopenid\\forms.py", "repo_name": "nicksergeant/snipt-old", "stars": 84, "license": "other", "language": "python", "size": 670}
{"docstring": "# Adding model 'FavSnipt'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_table('favsnipt_favsnipt', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('snipt', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['snippet.Snippet'])),\n            ('user', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['auth.User'])),\n            ('created', self.gf('django.db.models.fields.DateTimeField')(auto_now_add=True, blank=True)),\n        ))\n        db.send_create_signal('favsnipt', ['FavSnipt'])", "path": "favsnipt\\migrations\\0001_initial.py", "repo_name": "nicksergeant/snipt-old", "stars": 84, "license": "other", "language": "python", "size": 670}
{"docstring": "\"\"\"\nValidates that the two password inputs match.\n\"\"\"\n", "func_signal": "def clean_password2(self):\n", "code": "if 'password1' in self.cleaned_data and \\\n        'password2' in self.cleaned_data and \\\n   self.cleaned_data['password1'] == self.cleaned_data['password2']:\n    return self.cleaned_data['password2']\nraise forms.ValidationError(_(\"new passwords do not match\"))", "path": "django_authopenid\\forms.py", "repo_name": "nicksergeant/snipt-old", "stars": 84, "license": "other", "language": "python", "size": 670}
{"docstring": "\"\"\"\nAn AJAX view for saving new snipts.\n\nIf we sent along an id of '0', it means we're adding a new snipt, so use the appropriate form.\nOtherwise, create an instance form from the model for that specific id.\n\"\"\"\n", "func_signal": "def save(request):\n", "code": "try:\n    request.POST['id']\nexcept:\n    raise Http404()\nif request.POST['id'] != '0':\n    submitted_snippet = SnippetForm(request.POST, instance=Snippet.objects.get(id=request.POST['id'], user=request.user.id))\nelse:\n    submitted_snippet = SnippetForm(request.POST)\nif submitted_snippet.is_valid():\n    \n    # Halt the form submission until we add the proper user to the values.\n    submitted_snippet = submitted_snippet.save(commit=False)\n    submitted_snippet.user = request.user\n    if request.POST['id'] == '0':\n        submitted_snippet.slug = SlugifyUniquely(submitted_snippet.description, Snippet)\n        submitted_snippet.key = md5.new(submitted_snippet.slug).hexdigest()\n    \n    submitted_snippet.save()\n    \n    \"\"\"\n    Grab the tags for the newly created snippet.\n    For each tag, append the details of this tag to a new list which will be appended to the JSON response.\n    \n    Why?  Because if we simply send along the list of tag objects, simplejson won't know how to serialize it (rightly so).\n    \"\"\"\n    submitted_snippet.tags_list = []\n    tags_list = submitted_snippet.get_tags()\n    for tag in tags_list:\n        submitted_snippet.tags_list.append({\n            'id': tag.id,\n            'tag': escape(str(tag)),\n        })\n    \n    # Construct the data object we'll send back to the client so the new snipt can appear immediately.\n    data = {\n        'success': True,\n        'id': submitted_snippet.id,\n        'code': escape(submitted_snippet.code),\n        'code_stylized': stylize(submitted_snippet.code, submitted_snippet.lexer),\n        'description': escape(submitted_snippet.description),\n        'tags': escape(submitted_snippet.tags),\n        'tags_list': submitted_snippet.tags_list,\n        'lexer': submitted_snippet.lexer,\n        'public': submitted_snippet.public,\n        'key': submitted_snippet.key,\n        'slug': submitted_snippet.slug,\n        'username': request.user.username,\n        'created_date': submitted_snippet.created.strftime(\"%b %d\"),\n        'created_time': submitted_snippet.created.strftime(\"%I:%M %p\").replace(\"AM\",\"a.m.\").replace(\"PM\",\"p.m.\")\n    }\nelse:\n    data = {\n        'success': False\n    }\n    \n# Use simplejson to convert the Python object to a true JSON object (though it's already pretty close).\nreturn HttpResponse(simplejson.dumps(data), mimetype='application/json')", "path": "views.py", "repo_name": "nicksergeant/snipt-old", "stars": 84, "license": "other", "language": "python", "size": 670}
{"docstring": "\"\"\" validate next url \"\"\"\n", "func_signal": "def clean_next(self):\n", "code": "if 'next' in self.cleaned_data and \\\n        self.cleaned_data['next'] != \"\":\n    self.cleaned_data['next'] = clean_next(self.cleaned_data['next'])\n    return self.cleaned_data['next']", "path": "django_authopenid\\forms.py", "repo_name": "nicksergeant/snipt-old", "stars": 84, "license": "other", "language": "python", "size": 670}
{"docstring": "\"\"\"\nReturn a list of tags that the user has used.\n\"\"\"\n", "func_signal": "def tags(request):\n", "code": "user_tags_list = Tag.objects.usage_for_queryset(Snippet.objects.filter(user=request.user.id).order_by('-created'), counts=True)\n\n\"\"\"\nFor each tag, append the details of this tag to a new list which will be appended to the JSON response.\n\nWhy?  Because if we simply send along the list of tag objects, simplejson won't know how to serialize it (rightly so).\n\"\"\"\ntags_list = []\nfor tag in user_tags_list:\n    tags_list.append({\n        'id': tag.id,\n        'tag': escape(str(tag)),\n        'count': escape(str(tag.count))\n    })\ndata = {\n    'tags_list': tags_list\n}\nreturn HttpResponse(simplejson.dumps(data), mimetype='application/json')", "path": "views.py", "repo_name": "nicksergeant/snipt-old", "stars": 84, "license": "other", "language": "python", "size": 670}
{"docstring": "\"\"\" return openid object from response \"\"\"\n", "func_signal": "def from_openid_response(openid_response):\n", "code": "issued = int(time.time())\nsreg_resp = sreg.SRegResponse.fromSuccessResponse(openid_response) \\\n        or []\n\nreturn OpenID(\n    openid_response.identity_url, issued, openid_response.signed_fields, \n     dict(sreg_resp)\n)", "path": "django_authopenid\\util.py", "repo_name": "nicksergeant/snipt-old", "stars": 84, "license": "other", "language": "python", "size": 670}
{"docstring": "\"\"\" validate username \"\"\"\n", "func_signal": "def clean_username(self):\n", "code": "if 'username' in self.cleaned_data:\n    if not username_re.search(self.cleaned_data['username']):\n        raise forms.ValidationError(_(\"Usernames can only contain \\\n            letters, numbers and underscores\"))\n    try:\n        user = User.objects.get(\n                username__exact = self.cleaned_data['username']\n        )\n    except User.DoesNotExist:\n        raise forms.ValidationError(_(\"This username don't exist. \\\n                Please choose another.\"))\n    except User.MultipleObjectsReturned:\n        raise forms.ValidationError(u'Somehow, that username is in \\\n            use for multiple accounts. Please contact us to get this \\\n            problem resolved.')\n    return self.cleaned_data['username']", "path": "django_authopenid\\forms.py", "repo_name": "nicksergeant/snipt-old", "stars": 84, "license": "other", "language": "python", "size": 670}
{"docstring": "\"\"\"\" test if password is valid for this username \"\"\"\n", "func_signal": "def clean_password(self):\n", "code": "if 'username' in self.cleaned_data and \\\n        'password' in self.cleaned_data:\n    self.user_cache =  authenticate(\n            username=self.cleaned_data['username'], \n            password=self.cleaned_data['password']\n    )\n    if self.user_cache is None:\n        raise forms.ValidationError(_(\"Please enter a valid \\\n            username and password. Note that both fields are \\\n            case-sensitive.\"))\n    elif self.user_cache.is_active == False:\n        raise forms.ValidationError(_(\"This account is inactive.\"))\n    return self.cleaned_data['password']", "path": "django_authopenid\\forms.py", "repo_name": "nicksergeant/snipt-old", "stars": 84, "license": "other", "language": "python", "size": 670}
{"docstring": "\"\"\"\nValidates that the two password inputs match.\n\n\"\"\"\n", "func_signal": "def clean_password2(self):\n", "code": "if 'password1' in self.cleaned_data and \\\n        'password2' in self.cleaned_data and \\\n        self.cleaned_data['password1'] == \\\n        self.cleaned_data['password2']:\n    return self.cleaned_data['password2']\nraise forms.ValidationError(u'You must type the same password each \\\n        time')", "path": "django_authopenid\\forms.py", "repo_name": "nicksergeant/snipt-old", "stars": 84, "license": "other", "language": "python", "size": 670}
{"docstring": "\"\"\"\u968f\u673a\u751f\u6210\u5b57\u7b26\u4e32\"\"\"\n", "func_signal": "def entropy(bytes):\n", "code": "s = \"\"\nfor i in range(bytes):\n    s += chr(randint(0, 255))\nreturn s", "path": "kademlia\\utils.py", "repo_name": "wuzhenda/simDHT", "stars": 94, "license": "mit", "language": "python", "size": 122}
{"docstring": "\"\"\"\n\u6536\u5230\u8bf7\u6c42\u7c7b\u578b\u7684\u6570\u636e\u540e, \u667a\u80fd\u8c03\u7528DHT\u670d\u52a1\u5668\u7aef\u76f8\u5173\u5904\u7406\u51fd\u6570\n\"\"\"\n", "func_signal": "def queryReceived(self, res, address):\n", "code": "try:\n    self.queryActions[res[\"q\"]](res, address)        \nexcept KeyError:\n    pass", "path": "kademlia\\krpc.py", "repo_name": "wuzhenda/simDHT", "stars": 94, "license": "mit", "language": "python", "size": 122}
{"docstring": "\"\"\"\n\u6536\u5230\u8bf7\u6c42\u7c7b\u578b\u7684\u6570\u636e, \u76f4\u63a5\u8c03\u7528\u5904\u7406find_node\u56de\u5e94\u7684\u65b9\u6cd5\u5373\u53ef, \n\u56e0\u4e3a\u722c\u866b\u5ba2\u6237\u7aef\u53ea\u5b9e\u73b0\u4e86find_node\u8bf7\u6c42.\n\"\"\"\n", "func_signal": "def responseReceived(self, res, address):\n", "code": "try:\n    self.findNodeHandle(res)\nexcept KeyError:\n    pass", "path": "kademlia\\krpc.py", "repo_name": "wuzhenda/simDHT", "stars": 94, "license": "mit", "language": "python", "size": 122}
{"docstring": "\"\"\"\n\u56de\u5e94get_peers\u8bf7\u6c42, \u5dee\u4e0d\u591a\u8ddffindNodeReceived\u4e00\u6837, \u53ea\u56de\u590dnodes. \u61d2\u5f97\u7ef4\u62a4peer\u4fe1\u606f\n\"\"\"\n", "func_signal": "def getPeersReceived(self, res, address):\n", "code": "try:\n    infohash = res[\"a\"][\"info_hash\"]\n    closeNodes = self.table.findCloseNodes(infohash, 16)\n    if not closeNodes: return\n\n    nid = res[\"a\"][\"id\"]\n    h = sha1()\n    h.update(infohash+nid)\n    token = h.hexdigest()[:TOKEN_LENGTH]\n    msg = {\n        \"t\": res[\"t\"],\n        \"y\": \"r\",\n        \"r\": {\"id\": self.table.nid, \"nodes\": encodeNodes(closeNodes), \"token\": token}\n    }\n    (ip, port) = address\n    self.table.append(KNode(nid, ip, port))\n    self.sendResponse(msg, address)\nexcept KeyError:\n    pass", "path": "kademlia\\kdht.py", "repo_name": "wuzhenda/simDHT", "stars": 94, "license": "mit", "language": "python", "size": 122}
{"docstring": "\"\"\"\n\u66f4\u65b0\u6307\u5b9anode\u6240\u5728bucket\u6700\u540e\u8bbf\u95ee\u65f6\u95f4\n\"\"\"\n", "func_signal": "def touchBucket(self, target):\n", "code": "try:\n    self.buckets[self.bucketIndex(target)].touch()\nexcept IndexError:\n    pass", "path": "kademlia\\ktable.py", "repo_name": "wuzhenda/simDHT", "stars": 94, "license": "mit", "language": "python", "size": 122}
{"docstring": "\"\"\"\n\u5237\u65b0\u8def\u7531\u8868\n\n\u9047\u5230\u4e0d\"\u65b0\u9c9c\"\u7684bucket\u65f6, \u968f\u673a\u9009\u4e00\u4e2anode, \u53d1\u9001find_node\n\"\"\"\n", "func_signal": "def refreshRoutingTable(self):\n", "code": "for bucket in self.table:\n    if bucket.isFresh(): continue\n\n    node = bucket.random()\n    if node is None: continue #\u5982\u679c\u8be5bucket\u65e0node, \u7ee7\u7eed\u4e0b\u4e00\u4e2a\n\n    reactor.callLater(NEXT_FIND_NODE_INTERVAL, self.findNode, (node.ip, node.port))", "path": "kademlia\\kdht.py", "repo_name": "wuzhenda/simDHT", "stars": 94, "license": "mit", "language": "python", "size": 122}
{"docstring": "\"\"\"\n\u6570\u636e\u63a5\u6536\n\"\"\"\n", "func_signal": "def datagramReceived(self, data, address):\n", "code": "try:\n    res = bdecode(data)\n    self.actionSwitch[res[\"y\"]](res, address)     \nexcept(BTL.BTFailure, KeyError):\n    pass", "path": "kademlia\\krpc.py", "repo_name": "wuzhenda/simDHT", "stars": 94, "license": "mit", "language": "python", "size": 122}
{"docstring": "\"\"\"\n\u5904\u7406find_node\u56de\u5e94\u6570\u636e\n\"\"\"\n", "func_signal": "def findNodeHandle(self, res):\n", "code": "try:\n    self.table.touchBucket(res[\"r\"][\"id\"])\n    \n    nodes = decodeNodes(res[\"r\"][\"nodes\"])\n    for node in nodes:\n        (nid, ip, port) = node\n        if nid == self.table.nid: continue #\u4e0d\u5b58\u81ea\u5df1\n        self.table.append(KNode(nid, ip, port))\n        self.lastFind = time() #\u6700\u540e\u8bf7\u6c42\u65f6\u95f4\n\n        #\"\u7b49\u5f85\"NEXT_FIND_NODE_INTERVAL\u65f6\u95f4\u540e, \u8fdb\u884c\u4e0b\u4e00\u4e2afind_node\n        reactor.callLater(NEXT_FIND_NODE_INTERVAL, self.findNode, (ip, port))\nexcept KeyError:\n    pass", "path": "kademlia\\kdht.py", "repo_name": "wuzhenda/simDHT", "stars": 94, "license": "mit", "language": "python", "size": 122}
{"docstring": "\"\"\"\u89e3\u6790\u57df\u540d\"\"\"\n\n", "func_signal": "def resolve(self, host, port):\n", "code": "def callback(ip, port):\n    \"\"\"\u89e3\u6790\u6210\u529f\u540e, \u5f00\u59cb\u53d1\u9001find_node\"\"\"\n    self.findNode((ip, port))\n\ndef errback(failure, host, port):\n    \"\"\"\u89e3\u6790\u5931\u8d25, \u518d\u7ee7\u7eed\u89e3\u6790, \u76f4\u5230\u6210\u529f\u4e3a\u6b62\"\"\"\n    self.resolve(host, port)\n\nd = reactor.resolve(host)\nd.addCallback(callback, port)\nd.addErrback(errback, host, port)", "path": "kademlia\\kdht.py", "repo_name": "wuzhenda/simDHT", "stars": 94, "license": "mit", "language": "python", "size": 122}
{"docstring": "\"\"\"\n\u9632\u6b62find_node\u8bf7\u6c42\u505c\u6b62\u800c\u6253\u9020. \u505c\u6b62\u540e, \u518d\u91cd\u65b0\u52a0\u5165DHT\u7f51\u7edc\n\"\"\"\n", "func_signal": "def rejoinNetwork(self):\n", "code": "if ( self.lastFind - time() ) > FIND_TIMEOUT:\n    self.joinNetwork()", "path": "kademlia\\kdht.py", "repo_name": "wuzhenda/simDHT", "stars": 94, "license": "mit", "language": "python", "size": 122}
{"docstring": "\"\"\"\n\u627e\u51fa\u79bb\u76ee\u6807node ID\u6216infohash\u6700\u8fd1\u7684\u524dn\u4e2anode\n\"\"\"\n", "func_signal": "def findCloseNodes(self, target, n=K):\n", "code": "nodes = []\nif len(self.buckets) == 0: return nodes\n\nindex = self.bucketIndex(target)\ntry:\n    nodes = self.buckets[index].nodes\n    min = index - 1\n    max = index + 1\n\n    while len(nodes) < n and ((min >= 0) or (max < len(self.buckets))):\n        #\u5982\u679c\u8fd8\u80fd\u5f80\u524d\u8d70\n        if min >= 0:\n            nodes.extend(self.buckets[min].nodes)\n\n        #\u5982\u679c\u8fd8\u80fd\u5f80\u540e\u8d70\n        if max < len(self.buckets):\n            nodes.extend(self.buckets[max].nodes)\n\n        min -= 1\n        max += 1\n\n    #\u6309\u5f02\u6216\u503c\u4ece\u5c0f\u5230\u5927\u6392\u5e8f\n    num = intify(target)\n    nodes.sort(lambda a, b, num=num: cmp(num^intify(a.nid), num^intify(b.nid)))\n    return nodes[:n]\nexcept IndexError:\n    return nodes", "path": "kademlia\\ktable.py", "repo_name": "wuzhenda/simDHT", "stars": 94, "license": "mit", "language": "python", "size": 122}
{"docstring": "\"\"\"\n\u56de\u5e94find_node\u8bf7\u6c42\n\"\"\"\n", "func_signal": "def findNodeReceived(self, res, address):\n", "code": "try:\n    target = res[\"a\"][\"target\"]\n    closeNodes = self.table.findCloseNodes(target, 16)\n    if not closeNodes: return\n\n    msg = {\n        \"t\": res[\"t\"],\n        \"y\": \"r\",\n        \"r\": {\"id\": self.table.nid, \"nodes\": encodeNodes(closeNodes)}\n    }\n    nid = res[\"a\"][\"id\"]\n    (ip, port) = address\n    self.table.append(KNode(nid, ip, port))\n    self.sendResponse(msg, address)\nexcept KeyError:\n    pass", "path": "kademlia\\kdht.py", "repo_name": "wuzhenda/simDHT", "stars": 94, "license": "mit", "language": "python", "size": 122}
{"docstring": "\"\"\"\u968f\u673a\u9009\u62e9\u4e00\u4e2anode\"\"\"\n", "func_signal": "def random(self):\n", "code": "if len(self.nodes) == 0:\n    return None\nreturn self.nodes[randint(0, len(self.nodes)-1)]", "path": "kademlia\\ktable.py", "repo_name": "wuzhenda/simDHT", "stars": 94, "license": "mit", "language": "python", "size": 122}
{"docstring": "\"\"\"\n\u79cd\u5b50\u4e0b\u8f7d, \u53ef\u4ee5\u901a\u8fc7\u8fc5\u96f7\u79cd\u5b50, \u79cd\u5b50\u534f\u8bae, libtorrent\u4e0b\u8f7d\n\"\"\"\n", "func_signal": "def downloadTorrent(self, ip, port, infohash):\n", "code": "self.f.write(\"%s %s %s\\n\" % (ip, port, infohash.encode(\"hex\")))\nself.f.flush()", "path": "simDHT.py", "repo_name": "wuzhenda/simDHT", "stars": 94, "license": "mit", "language": "python", "size": 122}
{"docstring": "\"\"\"\u52a0\u5165DHT\u7f51\u7edc\"\"\"\n", "func_signal": "def joinNetwork(self):\n", "code": "for address in BOOTSTRAP_NODES:\n    self.resolve(address[0], address[1])\nreactor.callLater(KRPC_TIMEOUT, self.joinFailHandle)", "path": "kademlia\\kdht.py", "repo_name": "wuzhenda/simDHT", "stars": 94, "license": "mit", "language": "python", "size": 122}
{"docstring": "\"\"\"\u628a4\u5b57\u8282\u6574\u578b\u8f6c\u6362\u4e3aipv4\"\"\"\n", "func_signal": "def numToDottedQuad(n):\n", "code": "d = 256 * 256 * 256\nq = []\nwhile d > 0:\n    m, n = divmod(n, d)\n    q.append(str(m))\n    d /= 256\nreturn '.'.join(q)", "path": "kademlia\\utils.py", "repo_name": "wuzhenda/simDHT", "stars": 94, "license": "mit", "language": "python", "size": 122}
{"docstring": "\"\"\"\u628a20\u5b57\u8282\u7684hash\u503c\u8f6c\u6362\u4e3a\u6570\u5b57\"\"\"\n", "func_signal": "def intify(hstr):\n", "code": "assert len(hstr) == 20\nreturn long(hstr.encode('hex'), 16)", "path": "kademlia\\utils.py", "repo_name": "wuzhenda/simDHT", "stars": 94, "license": "mit", "language": "python", "size": 122}
{"docstring": "\"\"\"\n\u62c6\u8868\n\nindex\u662f\u5f85\u62c6\u5206\u7684bucket(old bucket)\u7684\u6240\u5728\u7d22\u5f15\u503c. \n\u5047\u8bbe\u8fd9\u4e2aold bucket\u7684min:0, max:16. \u62c6\u5206\u8be5old bucket\u7684\u8bdd, \u5206\u754c\u70b9\u662f8, \u7136\u540e\u628aold bucket\u7684max\u6539\u4e3a8, min\u8fd8\u662f0. \n\u521b\u5efa\u4e00\u4e2a\u65b0\u7684bucket, new bucket\u7684min=8, max=16.\n\u7136\u540e\u6839\u636e\u7684old bucket\u4e2d\u7684\u5404\u4e2anode\u7684nid, \u770b\u770b\u662f\u5c5e\u4e8e\u54ea\u4e2abucket\u7684\u8303\u56f4\u91cc, \u5c31\u88c5\u5230\u5bf9\u5e94\u7684bucket\u91cc. \n\u5404\u56de\u5404\u5bb6,\u5404\u627e\u5404\u5988.\nnew bucket\u7684\u6240\u5728\u7d22\u5f15\u503c\u5c31\u5728old bucket\u540e\u9762, \u5373index+1, \u628a\u65b0\u7684bucket\u63d2\u5165\u5230\u8def\u7531\u8868\u91cc.        \n\"\"\"\n", "func_signal": "def splitBucket(self, index):\n", "code": "old = self.buckets[index]\npoint = old.max - (old.max - old.min)/2\nnew = KBucket(point, old.max)\nold.max = point\nself.buckets.insert(index + 1, new)\nfor node in old.nodes[:]:\n    if new.inRange(node.nid):\n        new.append(node)\n        old.remove(node)", "path": "kademlia\\ktable.py", "repo_name": "wuzhenda/simDHT", "stars": 94, "license": "mit", "language": "python", "size": 122}
{"docstring": "\"\"\"\u628aipv4\u8f6c\u6362\u4e3a4\u5b57\u8282\u6574\u578b\"\"\"\n", "func_signal": "def dottedQuadToNum(ip):\n", "code": "hexn = ''.join([\"%02X\" % long(i) for i in ip.split('.')])\nreturn long(hexn, 16)", "path": "kademlia\\utils.py", "repo_name": "wuzhenda/simDHT", "stars": 94, "license": "mit", "language": "python", "size": 122}
{"docstring": "\"\"\"\n\u56de\u5e94ping\u8bf7\u6c42\n\"\"\"\n", "func_signal": "def pingReceived(self, res, address):\n", "code": "try:\n    nid = res[\"a\"][\"id\"]\n    msg = {\n        \"t\": res[\"t\"],\n        \"y\": \"r\",\n        \"r\": {\"id\": self.table.nid}\n    }\n    (ip, port) = address\n    self.table.append(KNode(nid, ip, port))\n    self.sendResponse(msg, address)\nexcept KeyError:\n    pass", "path": "kademlia\\kdht.py", "repo_name": "wuzhenda/simDHT", "stars": 94, "license": "mit", "language": "python", "size": 122}
{"docstring": "\"\"\"\nCreate an instance of a Shell\nThis call takes over the current terminal by calling curses.initscr()\nSets global shell state, including size information, menus, stickers,\nthe header, and the prompt.\n\nKwargs:\nscriptfile - the name of the script file to run. If not None and the\n             file exists, the script will be immediately run.\n\"\"\"\n", "func_signal": "def __init__(self, scriptfile=None):\n", "code": "self._register_sigint_handler()\n\nself.script_lines = self._parse_script_file(scriptfile)\nself.script_counter = 0\nself.scriptfile = \"\"\n\nself.stdscr = curses.initscr()\nself.stdscr.keypad(1)\n\nself.platform = self._get_platform()\n\n# holds the backlog of shell output\nself.backbuffer = []\nself.height,self.width = self.stdscr.getmaxyx()\n\n# the list of menus in the shell app\nself.menus = []\n# the currently visible stickers in the app\nself.stickers = []\n\n# should the command menu be shown\nself.should_show_help = True\n# for commands with only positional args, show the\n# name of the next argument as the user types\nself.should_show_hint = False\n\n# dictionary of functions to call on key events\n# keys are chars representing the pressed keys\nself.keyevent_hooks = {}\n\n# the text to stick in the upper left corner of the window\nself.header = \"\"\nself._header_bottom = 0\nself._header_right = 0\nself._header_right_margin = 50\n\nself.prompt = \"> \"", "path": "candela\\shell.py", "repo_name": "emmettbutler/candela", "stars": 71, "license": "gpl-3.0", "language": "python", "size": 417}
{"docstring": "\"\"\"\nGet the help string for the current menu.\n\nThis string contains a preformatted list of commands and their\ndescriptions from the current menu.\n\"\"\"\n", "func_signal": "def get_helpstring(self):\n", "code": "_menu = self.get_menu()\nif not _menu:\n    return\n\nhelpstring = \"\\n\\n\" + _menu.title + \"\\n\" + \"-\"*20 + \"\\n\" + _menu.options()\nreturn helpstring", "path": "candela\\shell.py", "repo_name": "emmettbutler/candela", "stars": 71, "license": "gpl-3.0", "language": "python", "size": 417}
{"docstring": "\"\"\"\nPrint the menu help box for the current menu\n\"\"\"\n", "func_signal": "def _print_help(self):\n", "code": "_helpstring = self.get_helpstring()\nif not _helpstring:\n    return\nhelpstrings = [\" %s\" % a for a in _helpstring.split(\"\\n\")]\nht = 0\nlongest = len(max(helpstrings, key=len))\n_x = self._header_right + self._header_right_margin\nif _x + longest > self.width:\n    _x = self.width - longest - 1\nfor line in helpstrings:\n    self.stdscr.addstr(ht, _x, line + \" \"*15)\n    ht += 1", "path": "candela\\shell.py", "repo_name": "emmettbutler/candela", "stars": 71, "license": "gpl-3.0", "language": "python", "size": 417}
{"docstring": "\"\"\"\nPrint all current stickers at the appropriate positions\n\"\"\"\n", "func_signal": "def _print_stickers(self):\n", "code": "for text,pos in self.stickers:\n    _y,_x = pos\n    if _x + len(text) > self.width:\n        _x = self.width - len(text) - 1\n    self.stdscr.addstr(_y, _x, text)", "path": "candela\\shell.py", "repo_name": "emmettbutler/candela", "stars": 71, "license": "gpl-3.0", "language": "python", "size": 417}
{"docstring": "\"\"\"\nClear the bottom line and re-print the given string on that line\n\nArgs:\nbuff    - The line to print on the cleared bottom line\n\"\"\"\n", "func_signal": "def _redraw_buffer(self, buff):\n", "code": "self.stdscr.addstr(self.height-1, 0, \" \"*(self.width-3))\nself.stdscr.addstr(self.height-1, 0, \"%s%s\" % (self.prompt, buff))", "path": "candela\\shell.py", "repo_name": "emmettbutler/candela", "stars": 71, "license": "gpl-3.0", "language": "python", "size": 417}
{"docstring": "\"\"\"\nPlace, change, or remove a sticker from the shell window.\n\nCandela has the concept of a sticker - a small block of text that\nis \"stuck\" to the window. They can be used to convey persistent\ninformation to the shell user.\n\nIf only output is specified, this creates a new sticker with the string\noutput. If output and new_output are specified, and there is an existing\nsticker whose text is the same as output, this will replace that\nsticker's text with new_output.\n\nArgs:\noutput      - The text of the sticker to manipulate\n\nKwargs:\nnew_output  - The text that will replace the text of the chosen sticker\npos         - The (y, x) tuple indicating where to place the sticker\n\"\"\"\n", "func_signal": "def sticker(self, output, new_output=\"\", pos=None):\n", "code": "if len(self.stickers) > 0:\n    sort = sorted(self.stickers, key=lambda x: x[1][0], reverse=True)\n    ht = sort[0][1][0]+1\nelse:\n    ht = 3\n\npos = pos or (ht, self.width - 20)\n\nmatch = None\nfor text,_pos in self.stickers:\n    if output == text:\n        match = (text,_pos)\n        break\nif match:\n    self.remove_sticker(match[0])\n\nsticker = (new_output or output, match[1] if match else pos)\nself.stickers.append(sticker)\n\nself._update_screen()", "path": "candela\\shell.py", "repo_name": "emmettbutler/candela", "stars": 71, "license": "gpl-3.0", "language": "python", "size": 417}
{"docstring": "\"\"\"\nPrint the previously printed output above the current command line.\n\ncandela.shell.Shell stores previously printed commands and output\nin a backbuffer. Like a normal shell, it handles printing these lines\nin reverse order to allow the user to see their past work.\n\"\"\"\n", "func_signal": "def _print_backbuffer(self):\n", "code": "rev = list(self.backbuffer)\nrev.reverse()\n\nfor i, tup in zip(range(len(rev)), rev):\n    string, iscommand = tup\n    ypos = self.height-2-i\n    if ypos > 0:\n        printstring = string\n        if iscommand:\n            printstring = \"%s%s\" % (self.prompt, string)\n        self.stdscr.addstr(ypos,0,printstring)", "path": "candela\\shell.py", "repo_name": "emmettbutler/candela", "stars": 71, "license": "gpl-3.0", "language": "python", "size": 417}
{"docstring": "\"\"\"\nSubstitute for _input used when reading from a script.\nReturns the next command from the script being read.\n\"\"\"\n", "func_signal": "def _script_in(self):\n", "code": "if not self.script_lines:\n    return None\n\nif self.script_counter < len(self.script_lines):\n    command = self.script_lines[self.script_counter]\n    self.script_counter += 1\nelse:\n    command = None\nreturn command", "path": "candela\\shell.py", "repo_name": "emmettbutler/candela", "stars": 71, "license": "gpl-3.0", "language": "python", "size": 417}
{"docstring": "\"\"\"\nGet the current menu as a Menu\n\"\"\"\n", "func_signal": "def get_menu(self):\n", "code": "if not self.menus: return\ntry:\n    return [a for a in self.menus if a.name == self.menu][0]\nexcept:\n    return", "path": "candela\\shell.py", "repo_name": "emmettbutler/candela", "stars": 71, "license": "gpl-3.0", "language": "python", "size": 417}
{"docstring": "\"\"\"\nSet up the global shell state necessary to run a script from a file\n\nArgs:\nscriptfile - the string name of the file containing the script.\n             paths are relative to system cwd\n\"\"\"\n", "func_signal": "def runscript(self, scriptfile):\n", "code": "self.script_lines = self._parse_script_file(scriptfile)\nself.script_counter = 0", "path": "candela\\shell.py", "repo_name": "emmettbutler/candela", "stars": 71, "license": "gpl-3.0", "language": "python", "size": 417}
{"docstring": "\"\"\"\nHandle user input on the shell window.\nWorks similarly to python's raw_input().\nTakes a prompt and returns the raw string entered before the return key\nby the user.\n\nThe input is returned withnewlines stripped.\n\nArgs:\nprompt  - The text to display prompting the user to enter text\n\"\"\"\n", "func_signal": "def _input(self, prompt):\n", "code": "self.put(prompt)\nkeyin = ''\nbuff = ''\nhist_counter = 1\nwhile keyin != 10:\n    keyin = self.stdscr.getch()\n    _y,_x = self.stdscr.getyx()\n    index = _x - len(self.prompt)\n    #self.stdscr.addstr(20, 70, str(keyin))  # for debugging\n    try:\n        if chr(keyin) in self.keyevent_hooks.keys():\n            cont = self.keyevent_hooks[chr(keyin)](chr(keyin), buff)\n            if cont == False:\n                continue\n    except:\n        pass\n    if keyin in [127, 263]:  # backspaces\n        del_lo, del_hi = self._get_backspace_indices()\n        buff = buff[:index+del_lo] + buff[index+del_hi:]\n        self._redraw_buffer(buff)\n        self.stdscr.move(_y, max(_x+del_lo, len(self.prompt)))\n    elif keyin in [curses.KEY_UP, curses.KEY_DOWN]:  # up and down arrows\n        hist_counter,buff = self._process_history_command(keyin, hist_counter)\n    elif keyin in [curses.KEY_LEFT, curses.KEY_RIGHT]:  # left, right arrows\n        if keyin == curses.KEY_LEFT:\n            newx = max(_x - 1, len(self.prompt))\n        elif keyin == curses.KEY_RIGHT:\n            newx = min(_x + 1, len(buff) + len(self.prompt))\n        self.stdscr.move(_y, newx)\n    elif keyin == curses.KEY_F1:  # F1\n        curses.endwin()\n        sys.exit()\n    elif keyin in [9]:  # tab\n        choices = self._tabcomplete(buff)\n        if len(choices) == 1:\n            if len(buff.split()) == 1 and not buff.endswith(' '):\n                buff = choices[0]\n            else:\n                if len(buff.split()) != 1 and not buff.endswith(' '):\n                    buff = ' '.join(buff.split()[:-1])\n                if buff.endswith(' '):\n                    buff += choices[0]\n                else:\n                    buff += ' ' + choices[0]\n        elif len(choices) > 1:\n            self.put(\"    \".join(choices))\n        elif len(choices) == 0:\n            pass\n        self._redraw_buffer(buff)\n    elif keyin >= 32 and keyin <= 126:  # ascii input\n        buff = buff[:index-1] + chr(keyin) + buff[index-1:]\n        self._redraw_buffer(buff)\n        self.stdscr.move(_y, min(_x, len(buff) + len(self.prompt)))\n        if self.should_show_hint and keyin == 32:\n            command = self._get_command(buff)\n            if hasattr(command, 'definition') and '-' not in command.definition:\n                try:\n                    nextarg = command.definition.split()[len(buff.split())]\n                    self.stdscr.addstr(_y, _x+1, nextarg)\n                    self.stdscr.move(_y, _x)\n                except:\n                    pass\nself.put(buff, command=True)\nself.stdscr.refresh()\nreturn buff", "path": "candela\\shell.py", "repo_name": "emmettbutler/candela", "stars": 71, "license": "gpl-3.0", "language": "python", "size": 417}
{"docstring": "\"\"\"\nCreate a new thread, run func in the thread for a max of\ntimeout_duration seconds\nThis is useful for blocking operations that must be performed\nafter the next window refresh.\nFor example, if a command should set a sticker when it starts executing\nand then clear that sticker when it's done, simply using the following\nwill not work:\n\ndef _run(*args, **kwargs):\n    self.sticker(\"Hello!\")\n    # do things...\n    self.remove_sticker(\"Hello!\")\n\nThis is because the sticker is both added and removed in the same\nrefresh loop of the window. Put another way, the sticker is added and\nremoved before the window gets redrawn.\n\ndefer() can be used to get around this by scheduling the sticker\nto be removed shortly after the next window refresh, like so:\n\ndef _run(*args, **kwargs):\n    self.sticker(\"Hello!\")\n    # do things...\n    def clear_sticker():\n        time.sleep(.1)\n        self.remove_sticker(\"Hello!\")\n    self.defer(clear_sticker)\n\nArgs:\nfunc        - The callback function to run in the new thread\n\nKwargs:\nargs        - The arguments to pass to the threaded function\nkwargs      - The keyword arguments to pass to the threaded function\ntimeout_duration - the amount of time in seconds to wait before\n                   killing the thread\ndefault     - The value to return in case of a timeout\n\"\"\"\n", "func_signal": "def defer(self, func, args=(), kwargs={}, timeout_duration=10, default=None):\n", "code": "class InterruptableThread(threading.Thread):\n    def __init__(self):\n        threading.Thread.__init__(self)\n        self.result = default\n    def run(self):\n        self.result = func(*args, **kwargs)\nit = InterruptableThread()\nit.start()\nit.join(timeout_duration)\nif it.isAlive():\n    return it.result\nelse:\n    return it.result", "path": "candela\\shell.py", "repo_name": "emmettbutler/candela", "stars": 71, "license": "gpl-3.0", "language": "python", "size": 417}
{"docstring": "\"\"\"\nGet the command instance referenced by string in the current input buffer\n\nArgs:\nbuff    - The string version of the current command input buffer\n\nReturn:\nThe Command instance corresponding to the buffer command\n\"\"\"\n", "func_signal": "def _get_command(self, buff):\n", "code": "menu = self.get_menu()\ncommands = []\nif menu:\n    commands = menu.commands\nif len(commands) == 0:\n    self.put(\"No commands found. Maybe you forgot to set self.menus or self.menu?\")\n    self.put(\"Hint: use F1 to quit\")\nfor command in commands:\n    if command.name == buff.split()[0] or buff.split()[0] in command.aliases:\n        return command\nreturn None", "path": "candela\\shell.py", "repo_name": "emmettbutler/candela", "stars": 71, "license": "gpl-3.0", "language": "python", "size": 417}
{"docstring": "\"\"\"\nPrint the output string on the bottom line of the shell window\nAlso pushes the backbuffer up the screen by the number of lines\nin output.\n\nArgs:\noutput  - The string to print. May contain newlines\n\nKwargs:\ncommand - False if the string was not a user-entered command,\n          True otherwise (users of Candela should always use False)\n\"\"\"\n", "func_signal": "def put(self, output, command=False):\n", "code": "self._update_screen()\n\nif not output:\n    return\n\noutput = str(output)\n\n_x,_y = (self.height-1, 0)\n\nlines = []\nfor line in output.split('\\n'):\n    if len(line) > self.width - 3:\n        for line in textwrap.wrap(line, self.width-3):\n            lines.append(line)\n    else:\n        lines.append(line)\n\nfor line in lines:\n    # put the line\n    self.stdscr.addstr(_x, _y, line)\n\n    # add it to backbuffer\n    backbuf_string = line\n    to_append = (backbuf_string, command)\n    if line != self.prompt:\n        index = 0\n        if len(self.backbuffer) >= 200:\n            index = 1\n        self.backbuffer = self.backbuffer[index:] + [to_append]", "path": "candela\\shell.py", "repo_name": "emmettbutler/candela", "stars": 71, "license": "gpl-3.0", "language": "python", "size": 417}
{"docstring": "\"\"\"\nGet the next command from the backbuffer and return it\nAlso return the modified buffer counter.\n\nArgs:\nkeyin           - The key just pressed\nhist_counter    - The current position in the backbuffer\n\"\"\"\n", "func_signal": "def _process_history_command(self, keyin, hist_counter):\n", "code": "hist_commands = [(s,c) for s,c in self.backbuffer if c]\nif not hist_commands:\n    return hist_counter, \"\"\n\nbuff = hist_commands[-hist_counter][0]\n\nself.stdscr.addstr(self.height-1, 0, \" \"*(self.width-3))\nself.stdscr.addstr(self.height-1, 0, \"%s%s\" % (self.prompt, buff))\n\nif keyin == curses.KEY_UP and hist_counter < len(hist_commands):\n    hist_counter += 1\nelif keyin == curses.KEY_DOWN and hist_counter > 0:\n    hist_counter -= 1\nreturn hist_counter, buff", "path": "candela\\shell.py", "repo_name": "emmettbutler/candela", "stars": 71, "license": "gpl-3.0", "language": "python", "size": 417}
{"docstring": "\"\"\"\nRemove all scrollback text from the window\n\"\"\"\n", "func_signal": "def clear(self):\n", "code": "backbuffer = list(self.backbuffer)\nprintstring = \"\\n\"\nfor i in range(self.height):\n    self.put(printstring)", "path": "candela\\shell.py", "repo_name": "emmettbutler/candela", "stars": 71, "license": "gpl-3.0", "language": "python", "size": 417}
{"docstring": "\"\"\"\nPrint the header in the appropriate position\n\"\"\"\n", "func_signal": "def _print_header(self):\n", "code": "ht = 0\nfor line in self.header.split(\"\\n\"):\n    self.stdscr.addstr(ht, 0, line + (\" \"*self._header_right_margin))\n    if len(line) > self._header_right:\n        self._header_right = len(line)\n    ht += 1\nself.stdscr.addstr(ht, 0, \" \"*(self._header_right+self._header_right_margin))\nself._header_bottom = ht\nself.mt_width = self._header_right + 49", "path": "candela\\shell.py", "repo_name": "emmettbutler/candela", "stars": 71, "license": "gpl-3.0", "language": "python", "size": 417}
{"docstring": "\"\"\"\nOpen a file if it exists and return its contents as a list of lines\n\nArgs:\nfilename - the file to attempt to open\n\"\"\"\n", "func_signal": "def _parse_script_file(self, filename):\n", "code": "self.scriptfile = filename\ntry:\n    f = open(filename, 'r')\n    script_lines = f.readlines()\n    script_lines = [a.strip('\\n') for a in script_lines]\n    f.close()\nexcept Exception as e:\n    return\nreturn script_lines", "path": "candela\\shell.py", "repo_name": "emmettbutler/candela", "stars": 71, "license": "gpl-3.0", "language": "python", "size": 417}
{"docstring": "\"\"\"\nThe main shell IO loop.\nThe sequence of events is as follows:\n    get an input command\n    split into tokens\n    find matching command\n    validate tokens for command\n    run command\n\nThis loop can be broken out of only with by a command returning\nconstants.CHOICE_QUIT or by pressing F1\n\"\"\"\n", "func_signal": "def main_loop(self):\n", "code": "ret_choice = None\nwhile ret_choice != constants.CHOICE_QUIT:\n    success = True\n    ret_choice = constants.CHOICE_INVALID\n    choice = self._script_in()\n    if choice:\n        self.put(\"%s%s\" % (self.prompt, choice))\n    else:\n        choice = self._input(self.prompt)\n    tokens = choice.split()\n    if len(tokens) == 0:\n        self.put(\"\\n\")\n        continue\n    command = self._get_command(choice)\n    if not command:\n        self.put(\"Invalid command - no match\")\n        continue\n    try:\n        args, kwargs = command.parse_command(tokens)\n        success, message = command.validate(*args, **kwargs)\n        if not success:\n            self.put(message)\n        else:\n            ret_choice = command.run(*args, **kwargs)\n            if ret_choice == constants.CHOICE_INVALID:\n                self.put(\"Invalid command\")\n            else:\n                menus = [a.name for a in self.menus]\n                if str(ret_choice).lower() in menus:\n                    self.menu = ret_choice.lower()\n                else:\n                    self.put(\"New menu '%s' not found\" % ret_choice.lower())\n    except Exception as e:\n        self.put(e)\nreturn self", "path": "candela\\shell.py", "repo_name": "emmettbutler/candela", "stars": 71, "license": "gpl-3.0", "language": "python", "size": 417}
{"docstring": "\"\"\"\nGet a list of possible completions for the current buffer\n\nIf the current buffer doesn't contain a valid command, see if the\nbuffer is a prefix of any valid commands. If so, return those as possible\ncompletions. Otherwise, delegate the completion finding to the command object.\n\nArgs:\nbuff    - The string buffer representing the current unfinished command input\n\nReturn:\nA list of completion strings for the current token in the command\n\"\"\"\n", "func_signal": "def _tabcomplete(self, buff):\n", "code": "menu = self.get_menu()\ncommands = []\nif menu:\n    commands = menu.commands\noutput = []\nif len(buff.split()) <= 1 and ' ' not in buff:\n    for command in commands:\n        if command.name.startswith(buff):\n            output.append(command.name)\n        for alias in command.aliases:\n            if alias.startswith(buff):\n                output.append(alias)\nelse:\n    command = self._get_command(buff)\n    if command:\n        output = command._tabcomplete(buff)\nreturn output", "path": "candela\\shell.py", "repo_name": "emmettbutler/candela", "stars": 71, "license": "gpl-3.0", "language": "python", "size": 417}
{"docstring": "# Popup menu for the Sent page\n", "func_signal": "def init_sent_popup_menu(self):\n", "code": "self.ui.sentContextMenuToolbar = QtGui.QToolBar()\n# Actions\nself.actionTrashSentMessage = self.ui.sentContextMenuToolbar.addAction(\n    _translate(\n        \"MainWindow\", \"Move to Trash\"), self.on_action_SentTrash)\nself.actionSentClipboard = self.ui.sentContextMenuToolbar.addAction(\n    _translate(\n        \"MainWindow\", \"Copy destination address to clipboard\"),\n    self.on_action_SentClipboard)\nself.actionForceSend = self.ui.sentContextMenuToolbar.addAction(\n    _translate(\n        \"MainWindow\", \"Force send\"), self.on_action_ForceSend)\nself.ui.tableWidgetSent.setContextMenuPolicy(\n    QtCore.Qt.CustomContextMenu)\nself.connect(self.ui.tableWidgetSent, QtCore.SIGNAL(\n    'customContextMenuRequested(const QPoint&)'),\n             self.on_context_menuSent)\n# self.popMenuSent = QtGui.QMenu( self )\n# self.popMenuSent.addAction( self.actionSentClipboard )\n# self.popMenuSent.addAction( self.actionTrashSentMessage )", "path": "bitmessageqt\\__init__.py", "repo_name": "bitxbay/BitXBay", "stars": 67, "license": "gpl-3.0", "language": "python", "size": 69116}
{"docstring": "#check and send tx for cancel deal here\n", "func_signal": "def cancelagree(self,ides,tx):\n", "code": "messageText3 = MyForm.sh[ides]\ntry:\n    start = messageText3.index('{amount{') + len('{amount{')\n    end = messageText3.index('}amount}', start)\n    amount = messageText3[start:end]\nexcept ValueError:\n    error=\"\"\namount = float(amount)\ntry:\n    start = messageText3.index('{escrowaddr1{') + len('{escrowaddr1{')\n    end = messageText3.index('}escrowaddr1}', start)\n    esc1 = messageText3[start:end]\nexcept ValueError:\n    error=\"\"\ntry:\n    start = messageText3.index('{escrowaddr2{') + len('{escrowaddr2{')\n    end = messageText3.index('}escrowaddr2}', start)\n    esc2 = messageText3[start:end]\nexcept ValueError:\n    error=\"\"\ntry:\n    start = messageText3.index('{escrowaddr3{') + len('{escrowaddr3{')\n    end = messageText3.index('}escrowaddr3}', start)\n    esc3 = messageText3[start:end]\nexcept ValueError:\n    error=\"\"\n\ntry:\n    start = messageText3.index('{txid1{') + len('{txid1{')\n    end = messageText3.index('}txid1}', start)\n    txid1 = messageText3[start:end]\nexcept ValueError:\n    error=\"\"\n\ntry:\n    start = messageText3.index('{txid2{') + len('{txid2{')\n    end = messageText3.index('}txid2}', start)\n    txid2 = messageText3[start:end]\nexcept ValueError:\n    error=\"\"\n\ntry:\n    start = messageText3.index('{txid3{') + len('{txid3{')\n    end = messageText3.index('}txid3}', start)\n    txid3 = messageText3[start:end]\nexcept ValueError:\n    error=\"\"\n\ntry:\n    start = messageText3.index('{maddr1{') + len('{maddr1{')\n    end = messageText3.index('}maddr1}', start)\n    maddr1 = messageText3[start:end]\nexcept ValueError:\n    error=\"\"\n\ntry:\n    start = messageText3.index('{badd1{') + len('{badd1{')\n    end = messageText3.index('}badd1}', start)\n    badd1 = messageText3[start:end]\nexcept ValueError:\n    error=\"\"\n\ntry:\n    start = messageText3.index('{badd2{') + len('{badd2{')\n    end = messageText3.index('}badd2}', start)\n    badd2 = messageText3[start:end]\nexcept ValueError:\n    error=\"\"\n\ntry:\n    start = messageText3.index('{badd3{') + len('{badd3{')\n    end = messageText3.index('}badd3}', start)\n    badd3 = messageText3[start:end]\nexcept ValueError:\n    error=\"\"\n\ntry:\n    start = messageText3.index('{redeem1{') + len('{redeem1{')\n    end = messageText3.index('}redeem1}', start)\n    redeem1 = messageText3[start:end]\nexcept ValueError:\n    error=\"\"\n\ntry:\n    start = messageText3.index('{redeem2{') + len('{redeem2{')\n    end = messageText3.index('}redeem2}', start)\n    redeem2 = messageText3[start:end]\nexcept ValueError:\n    error=\"\"\n\ntry:\n    start = messageText3.index('{redeem3{') + len('{redeem3{')\n    end = messageText3.index('}redeem3}', start)\n    redeem3 = messageText3[start:end]\nexcept ValueError:\n    error=\"\"\n\nif self.electrumon:\n    scam = False\n    try:\n        sz = sys.getsizeof(tx)/1000\n        sz = float(sz)\n        fee = 0.0006 + 0.0001 * sz\n        decodedtx = MyForm.conn.decoderawtransaction(tx)\n        for i in decodedtx[\"inputs\"]:\n            if i[\"prevout_hash\"] != txid1 and i[\"prevout_hash\"] != txid2 and i[\"prevout_hash\"] != txid3:\n                scam = True\n                break\n        for i in decodedtx[\"outputs\"]:\n            if i[\"address\"] == badd1:\n                b = float(i[\"value\"])\n                b = b*0.00000001\n                if amount * 0.05 < 0.0001:\n                    s = 0.0001\n                else:\n                    s = amount * 0.05\n                if b >= amount-0.00001 + s - fee:\n                    my = MyForm.conn.validateaddress(i[\"address\"])\n                    if my.ismine:\n                        scam = False\n                else:\n                    scam = True\n                    break\n    except:\n        self.statusBar().showMessage(_translate(\n                        \"MainWindow\", \"Error: Bitcoin-qt/bitcoind/Electrum don't work correctly.\"))\n        scam = True\n    if scam == False:\n        addresses = {esc1:badd1,esc2:badd2,esc3:badd3}\n        redeem = {esc1:redeem1,esc2:redeem2,esc3:redeem3}\n        signedtx = str(MyForm.conn.signb(tx,addresses,redeem))\n        return signedtx\nelse:\n    scam = False\n    try:\n        sz = sys.getsizeof(tx)/1000\n        sz = float(sz)\n        fee = 0.0006 + 0.0001 * sz\n        decodedtx = MyForm.conn.decoderawtransaction(tx)\n        for i in decodedtx[\"vin\"]:\n            if i[\"txid\"] != txid1 and i[\"txid\"] != txid2 and i[\"txid\"] != txid3:\n                scam = True\n                break\n        for i in decodedtx[\"vout\"]:\n            if i[\"addresses\"][0] == badd1:\n                b = float(i[\"value\"])\n                b = b\n                if amount * 0.05 < 0.0001:\n                    s = 0.0001\n                else:\n                    s = amount * 0.05\n                if b >= amount-0.00001 + s - fee:\n                    my = MyForm.conn.validateaddress(i[\"addresses\"][0])\n                    if my.ismine:\n                        scam = False\n                else:\n                    scam = True\n                    break\n    except:\n        self.statusBar().showMessage(_translate(\n                        \"MainWindow\", \"Error: Bitcoin-qt/bitcoind/Electrum don't work correctly.\"))\n        scam = True\n    if scam == False:\n        signedtx = str(MyForm.conn.signrawtransaction(tx))\n        return signedtx\n\nreturn signedtx", "path": "bitmessageqt\\__init__.py", "repo_name": "bitxbay/BitXBay", "stars": 67, "license": "gpl-3.0", "language": "python", "size": 69116}
{"docstring": "\"\"\"The curve of points satisfying y^2 = x^3 + a*x + b (mod p).\"\"\"\n", "func_signal": "def __init__( self, p, a, b ):\n", "code": "self.__p = p\nself.__a = a\nself.__b = b", "path": "electru\\ecdsa\\ellipticcurve.py", "repo_name": "bitxbay/BitXBay", "stars": 67, "license": "gpl-3.0", "language": "python", "size": 69116}
{"docstring": "# Initialize the Blacklist or Whitelist table\n", "func_signal": "def loadBlackWhiteList(self):\n", "code": "listType = shared.config.get('bitmessagesettings', 'blackwhitelist')\nif listType == 'black':\n    queryreturn = sqlQuery('''SELECT label, address, enabled FROM blacklist''')\nelse:\n    queryreturn = sqlQuery('''SELECT label, address, enabled FROM whitelist''')\nfor row in queryreturn:\n    label, address, enabled = row\n    self.ui.tableWidgetBlacklist.insertRow(0)\n    newItem = QtGui.QTableWidgetItem(unicode(label, 'utf-8'))\n    if not enabled:\n        newItem.setTextColor(QtGui.QColor(128, 128, 128))\n    newItem.setIcon(avatarize(address))\n    self.ui.tableWidgetBlacklist.setItem(0, 0, newItem)\n    newItem = QtGui.QTableWidgetItem(address)\n    newItem.setFlags(\n        QtCore.Qt.ItemIsSelectable | QtCore.Qt.ItemIsEnabled)\n    if not enabled:\n        newItem.setTextColor(QtGui.QColor(128, 128, 128))\n    self.ui.tableWidgetBlacklist.setItem(0, 1, newItem)", "path": "bitmessageqt\\__init__.py", "repo_name": "bitxbay/BitXBay", "stars": 67, "license": "gpl-3.0", "language": "python", "size": 69116}
{"docstring": "\"\"\"We expect that on curve c, (x1,y1) + (x2, y2 ) = (x3, y3).\"\"\"\n", "func_signal": "def test_add( c, x1, y1, x2,  y2, x3, y3 ):\n", "code": "p1 = Point( c, x1, y1 )\np2 = Point( c, x2, y2 )\np3 = p1 + p2\nprint_(\"%s + %s = %s\" % ( p1, p2, p3 ), end=' ')\nif p3.x() != x3 or p3.y() != y3:\n  raise FailedTest(\"Failure: should give (%d,%d).\" % ( x3, y3 ))\nelse:\n  print_(\" Good.\")", "path": "electru\\ecdsa\\ellipticcurve.py", "repo_name": "bitxbay/BitXBay", "stars": 67, "license": "gpl-3.0", "language": "python", "size": 69116}
{"docstring": "\"\"\" Find a quadratic residue (mod p) of 'a'. p\nmust be an odd prime.\n\nSolve the congruence of the form:\nx^2 = a (mod p)\nAnd returns x. Note that p - x is also a root.\n\n0 is returned is no square root exists for\nthese a and p.\n\nThe Tonelli-Shanks algorithm is used (except\nfor some simple cases in which the solution\nis known from an identity). This algorithm\nruns in polynomial time (unless the\ngeneralized Riemann hypothesis is false).\n\"\"\"\n# Simple cases\n#\n", "func_signal": "def modular_sqrt(a, p):\n", "code": "if legendre_symbol(a, p) != 1:\n    return 0\nelif a == 0:\n    return 0\nelif p == 2:\n    return p\nelif p % 4 == 3:\n    return pow(a, (p + 1) / 4, p)\n\n# Partition p-1 to s * 2^e for an odd s (i.e.\n# reduce all the powers of 2 from p-1)\n#\ns = p - 1\ne = 0\nwhile s % 2 == 0:\n    s /= 2\n    e += 1\n    \n# Find some 'n' with a legendre symbol n|p = -1.\n# Shouldn't take long.\n#\nn = 2\nwhile legendre_symbol(n, p) != -1:\n    n += 1\n    \n# Here be dragons!\n# Read the paper \"Square roots from 1; 24, 51,\n# 10 to Dan Shanks\" by Ezra Brown for more\n# information\n#\n\n# x is a guess of the square root that gets better\n# with each iteration.\n# b is the \"fudge factor\" - by how much we're off\n# with the guess. The invariant x^2 = ab (mod p)\n# is maintained throughout the loop.\n# g is used for successive powers of n to update\n# both a and b\n# r is the exponent - decreases with each update\n#\nx = pow(a, (s + 1) / 2, p)\nb = pow(a, s, p)\ng = pow(n, s, p)\nr = e\n\nwhile True:\n    t = b\n    m = 0\n    for m in xrange(r):\n        if t == 1:\n            break\n        t = pow(t, 2, p)\n        \n    if m == 0:\n        return x\n    \n    gs = pow(g, 2 ** (r - m - 1), p)\n    g = (gs * gs) % p\n    x = (x * gs) % p\n    b = (b * g) % p\n    r = m", "path": "electru\\build\\lib\\electrum\\msqr.py", "repo_name": "bitxbay/BitXBay", "stars": 67, "license": "gpl-3.0", "language": "python", "size": 69116}
{"docstring": "# Popup menu for the Your Identities tab\n", "func_signal": "def init_address_popup_menu2(self):\n", "code": "self.ui.addressContextMenuToolbar3 = QtGui.QToolBar()\n# Actions\nself.actionClipboard3 = self.ui.addressContextMenuToolbar3.addAction(\n    _translate(\n        \"MainWindow\", \"Copy address to clipboard\"),\n    self.on_action_YourAddressClipboard2)\nself.ui.bitcoinaddresses.setContextMenuPolicy(\n    QtCore.Qt.CustomContextMenu)\nself.connect(self.ui.bitcoinaddresses, QtCore.SIGNAL(\n    'customContextMenuRequested(const QPoint&)'),\n             self.on_context_menuYourAddress2)\nself.popMenu3 = QtGui.QMenu(self)\nself.popMenu3.addAction(self.actionClipboard3)", "path": "bitmessageqt\\__init__.py", "repo_name": "bitxbay/BitXBay", "stars": 67, "license": "gpl-3.0", "language": "python", "size": 69116}
{"docstring": "# Popup menu for the Your Identities tab\n", "func_signal": "def init_identities_popup_menu(self):\n", "code": "self.ui.addressContextMenuToolbar = QtGui.QToolBar()\n# Actions\nself.actionNew = self.ui.addressContextMenuToolbar.addAction(_translate(\n    \"MainWindow\", \"New\"), self.on_action_YourIdentitiesNew)\nself.actionEnable = self.ui.addressContextMenuToolbar.addAction(\n    _translate(\n        \"MainWindow\", \"Enable\"), self.on_action_YourIdentitiesEnable)\nself.actionDisable = self.ui.addressContextMenuToolbar.addAction(\n    _translate(\n        \"MainWindow\", \"Disable\"), self.on_action_YourIdentitiesDisable)\nself.actionSetAvatar = self.ui.addressContextMenuToolbar.addAction(\n    _translate(\n        \"MainWindow\", \"Set avatar...\"),\n    self.on_action_YourIdentitiesSetAvatar)\nself.actionClipboard = self.ui.addressContextMenuToolbar.addAction(\n    _translate(\n        \"MainWindow\", \"Copy address to clipboard\"),\n    self.on_action_YourIdentitiesClipboard)\nself.actionSpecialAddressBehavior = self.ui.addressContextMenuToolbar.addAction(\n    _translate(\n        \"MainWindow\", \"Special address behavior...\"),\n    self.on_action_SpecialAddressBehaviorDialog)\nself.ui.tableWidgetYourIdentities.setContextMenuPolicy(\n    QtCore.Qt.CustomContextMenu)\nself.connect(self.ui.tableWidgetYourIdentities, QtCore.SIGNAL(\n    'customContextMenuRequested(const QPoint&)'),\n             self.on_context_menuYourIdentities)\nself.popMenu = QtGui.QMenu(self)\nself.popMenu.addAction(self.actionNew)\nself.popMenu.addSeparator()\nself.popMenu.addAction(self.actionClipboard)\nself.popMenu.addSeparator()\nself.popMenu.addAction(self.actionEnable)\nself.popMenu.addAction(self.actionDisable)\nself.popMenu.addAction(self.actionSetAvatar)\nself.popMenu.addAction(self.actionSpecialAddressBehavior)", "path": "bitmessageqt\\__init__.py", "repo_name": "bitxbay/BitXBay", "stars": 67, "license": "gpl-3.0", "language": "python", "size": 69116}
{"docstring": "#when merchant wait for main payment and do actions if it come\n", "func_signal": "def updateescrows(self):\n", "code": "chk3payment = shelve.open(\"chk3pay.slv\")\nif len(chk3payment) > 0:\n    for key in chk3payment:\n        try:\n            val = chk3payment[key]\n            try:\n                received = self.recievefrom2(val[0], val[1], val[2])\n            except:\n                received = 0\n\n\n            if float(received) >= (float(val[2])-0.00001):\n                sh2 = MyForm.sh2\n\n                if sh2.has_key(key):\n                    messageText2 = sh2[key]\n                    try:\n                        start = messageText2.index('{amount{') + len('{amount{')\n                        end = messageText2.index('}amount}', start)\n                        amount15 = messageText2[start:end]\n                    except ValueError:\n                        error = ''\n                        amount15=\"\"\n                    try:\n                        start = messageText2.index('{id{') + len('{id{')\n                        end = messageText2.index('}id}', start)\n                        accnt = messageText2[start:end]\n                    except ValueError:\n                        error = ''\n                        accnt=\"\"\n                    try:\n                        start = messageText2.index('{cont2{') + len('{cont2{')\n                        end = messageText2.index('}cont2}', start)\n                        toadd = messageText2[start:end]\n                    except ValueError:\n                        error = ''\n                    if sh2[key][0:29] == \"beta02{status{started-buyer-6\":\n                        sh2[key] = \"beta02\"+\"{status{\"+\"started-buyer69\"+\"}status}\"+str(messageText2[37:])\n\n                    del chk3payment[key]\n                    chk3payment.sync()\n                    sh2.sync()\n                    self.rendertextbrowser3()\n                else:\n                    del chk3payment[key]\n                    chk3payment.sync()\n                sh2.sync()\n        except:\n            error=\"\"\nchk3payment.close()\n\n\n\n\n\n#when buyer wait for first insurance payment and do actions if it come\nchk2payment = shelve.open(\"chk2pay.slv\")\nif len(chk2payment) > 0:\n    for key in chk2payment:\n        try:\n            val = chk2payment[key]\n            try:\n                received = self.recievefrom(val[0], val[1], val[2])\n            except:\n                received = 0\n\n            if float(received) >= float(val[2])/21:\n                sh = MyForm.sh\n                if sh.has_key(key):\n                    messageText2 = sh[key]\n                    blc = 0\n                    try:\n                        blc = MyForm.conn.getbalance()\n                    except:\n                        blc = 0\n                        sendResult1 = False\n                    try:\n                        start = messageText2.index('{amount{') + len('{amount{')\n                        end = messageText2.index('}amount}', start)\n                        amount15 = messageText2[start:end]\n                    except ValueError:\n                        error = ''\n                        amount15=\"\"\n                    try:\n                        start = messageText2.index('{id{') + len('{id{')\n                        end = messageText2.index('}id}', start)\n                        accnt = messageText2[start:end]\n                    except ValueError:\n                        error = ''\n                        accnt=\"\"\n                    try:\n                        start = messageText2.index('{cont2{') + len('{cont2{')\n                        end = messageText2.index('}cont2}', start)\n                        toadd = messageText2[start:end]\n                    except ValueError:\n                        error = ''\n                    try:\n                        start = messageText2.index('{cont{') + len('{cont{')\n                        end = messageText2.index('}cont}', start)\n                        fromadd = messageText2[start:end]\n                    except ValueError:\n                        error = ''\n                    try:\n                        start = messageText2.index('{escrowaddr2{') + len('{escrowaddr2{')\n                        end = messageText2.index('}escrowaddr2}', start)\n                        esc2 = messageText2[start:end]\n                    except ValueError:\n                        error = ''\n                        esc2=\"\"\n                    try:\n                        start = messageText2.index('{escrowaddr3{') + len('{escrowaddr3{')\n                        end = messageText2.index('}escrowaddr3}', start)\n                        esc3 = messageText2[start:end]\n                    except ValueError:\n                        error = ''\n                        esc3=\"\"\n                    try:\n                        if float(amount15)<0.0001:\n                            inssumm = 0.0001\n                        else:\n                            inssumm = float(amount15)\n                        if MyForm.conn.isempty:\n                            txid3 = MyForm.conn.sendtoaddress(esc3, inssumm)\n                        else:\n                            txs = MyForm.conn.getaddressunspent(esc3)\n                            txid3 = txs[tx_hash]\n                    except:\n                        error = \"\"\n                        txid3 = \"\"\n\n                    if txid3 != \"\" and 'code' not in txid3:\n                        message = \"beta02\"+\"{status{\"+\"started-buyer-6\"+\"}status}\"+str(messageText2[37:])+\"{txid3{\"+str(txid3)+\"}txid3}\"\n                        sh[key] = message\n                        sh.sync()\n                        self.buyerpay1(message, fromadd, toadd)\n                        del chk2payment[key]\n                        chk2payment.sync()\n                    else:\n                        message = \"beta02\"+\"{status{\"+\"started-buyer06\"+\"}status}\"+str(messageText2[37:])\n                        sh[key] = message\n                        sh.sync()\n                    self.rendertextbrowser2()\n                else:\n                    del chk2payment[key]\n                    chk2payment.sync()\n\n                sh.sync()\n        except:\n            error=\"\"\nchk2payment.close()\n\n\n#when merchant wait for first insurance payment and do actions if it come\nchk1payment = shelve.open(\"chk1pay.slv\")\nif len(chk1payment) > 0:\n    for key in chk1payment:\n        try:\n            val = chk1payment[key]\n            try:\n                received = self.recievefrom(val[0], val[1], val[2])\n            except:\n                received = 0\n\n\n            if float(received) >= float(val[2])/21:\n                sh2 = MyForm.sh2\n                if sh2.has_key(key):\n                    messageText2 = sh2[key]\n                    blc = 0\n                    try:\n                        blc = MyForm.conn.getbalance()\n                    except:\n                        blc = 0\n                        sendResult1 = False\n                    try:\n                        start = messageText2.index('{amount{') + len('{amount{')\n                        end = messageText2.index('}amount}', start)\n                        amount15 = messageText2[start:end]\n                    except ValueError:\n                        error = ''\n                        amount15=\"\"\n                    try:\n                        start = messageText2.index('{id{') + len('{id{')\n                        end = messageText2.index('}id}', start)\n                        accnt = messageText2[start:end]\n                    except ValueError:\n                        error = ''\n                        accnt=\"\"\n                    try:\n                        start = messageText2.index('{cont2{') + len('{cont2{')\n                        end = messageText2.index('}cont2}', start)\n                        toadd = messageText2[start:end]\n                    except ValueError:\n                        error = ''\n                    try:\n                        start = messageText2.index('{cont{') + len('{cont{')\n                        end = messageText2.index('}cont}', start)\n                        fromadd = messageText2[start:end]\n                    except ValueError:\n                        error = ''\n                    try:\n                        start = messageText2.index('{escrowaddr2{') + len('{escrowaddr2{')\n                        end = messageText2.index('}escrowaddr2}', start)\n                        esc2 = messageText2[start:end]\n                    except ValueError:\n                        error = ''\n                        esc2=\"\"\n                    try:\n                        start = messageText2.index('{escrowaddr3{') + len('{escrowaddr3{')\n                        end = messageText2.index('}escrowaddr3}', start)\n                        esc3 = messageText2[start:end]\n                    except ValueError:\n                        error = ''\n                        esc3=\"\"\n                    try:\n                        if float(amount15)*0.05<0.0001:\n                            inssumm = 0.0001\n                        else:\n                            inssumm = float(amount15)*0.05\n                        if  MyForm.conn.isempty:\n                            txid2 = MyForm.conn.sendtoaddress(esc2, inssumm)\n                        else:\n                            txs = MyForm.conn.getaddressunspent(esc2)\n                            txid2 = txs[tx_hash]\n                    except:\n                        error = \"\"\n                        txid2 = \"\"\n\n                    if txid2 != \"\" and 'code' not in txid2:\n                        message = \"beta02\" + \"{status{\" + \"started-buyer-5\" + \"}status}\" + str(messageText2[37:]) + \"{txid2{\" + str(txid2) + \"}txid2}\"\n                        sh2[key] = message\n                        sh2.sync()\n                        self.buyerpay1(message, toadd, fromadd)\n                        del chk1payment[key]\n                        chk1payment.sync()\n                    else:\n                        message = \"beta02\"+\"{status{\"+\"started-buyer05\"+\"}status}\"+str(messageText2[37:])\n                        sh2[key] = message\n\n\n                    sh2.sync()\n                    self.rendertextbrowser3()\n                else:\n                    del chk1payment[key]\n                    chk1payment.sync()\n\n                    sh2.sync()\n                sh2.sync()\n\n        except:\n            error=\"\"\n\nchk1payment.close()", "path": "bitmessageqt\\__init__.py", "repo_name": "bitxbay/BitXBay", "stars": 67, "license": "gpl-3.0", "language": "python", "size": 69116}
{"docstring": "# Popup menu for the Inbox tab\n", "func_signal": "def init_inbox_popup_menu(self):\n", "code": "self.ui.inboxContextMenuToolbar = QtGui.QToolBar()\n# Actions\nself.actionReply = self.ui.inboxContextMenuToolbar.addAction(_translate(\n    \"MainWindow\", \"Reply\"), self.on_action_InboxReply)\nself.actionAddSenderToAddressBook = self.ui.inboxContextMenuToolbar.addAction(\n    _translate(\n        \"MainWindow\", \"Add sender to your Address Book\"),\n    self.on_action_InboxAddSenderToAddressBook)\nself.actionTrashInboxMessage = self.ui.inboxContextMenuToolbar.addAction(\n    _translate(\"MainWindow\", \"Move to Trash\"),\n    self.on_action_InboxTrash)\nself.actionForceHtml = self.ui.inboxContextMenuToolbar.addAction(\n    _translate(\n        \"MainWindow\", \"View HTML code as formatted text\"),\n    self.on_action_InboxMessageForceHtml)\nself.actionSaveMessageAs = self.ui.inboxContextMenuToolbar.addAction(\n    _translate(\n        \"MainWindow\", \"Save message as...\"),\n    self.on_action_InboxSaveMessageAs)\nself.actionMarkUnread = self.ui.inboxContextMenuToolbar.addAction(\n    _translate(\n        \"MainWindow\", \"Mark Unread\"), self.on_action_InboxMarkUnread)\nself.ui.tableWidgetInbox.setContextMenuPolicy(\n    QtCore.Qt.CustomContextMenu)\nself.connect(self.ui.tableWidgetInbox, QtCore.SIGNAL(\n    'customContextMenuRequested(const QPoint&)'),\n             self.on_context_menuInbox)\nself.popMenuInbox = QtGui.QMenu(self)\nself.popMenuInbox.addAction(self.actionForceHtml)\nself.popMenuInbox.addAction(self.actionMarkUnread)\nself.popMenuInbox.addSeparator()\nself.popMenuInbox.addAction(self.actionReply)\nself.popMenuInbox.addAction(self.actionAddSenderToAddressBook)\nself.popMenuInbox.addSeparator()\nself.popMenuInbox.addAction(self.actionSaveMessageAs)\nself.popMenuInbox.addAction(self.actionTrashInboxMessage)", "path": "bitmessageqt\\__init__.py", "repo_name": "bitxbay/BitXBay", "stars": 67, "license": "gpl-3.0", "language": "python", "size": 69116}
{"docstring": "\"\"\" Compute the Legendre symbol a|p using\nEuler's criterion. p is a prime, a is\nrelatively prime to p (if p divides\na, then a|p = 0)\n\nReturns 1 if a has a square root modulo\np, -1 otherwise.\n\"\"\"\n", "func_signal": "def legendre_symbol(a, p):\n", "code": "ls = pow(a, (p - 1) / 2, p)\nreturn -1 if ls == p - 1 else ls", "path": "electru\\build\\lib\\electrum\\msqr.py", "repo_name": "bitxbay/BitXBay", "stars": 67, "license": "gpl-3.0", "language": "python", "size": 69116}
{"docstring": "\"\"\"Add one point to another point.\"\"\"\n\n# X9.62 B.3:\n\n", "func_signal": "def __add__( self, other ):\n", "code": "if other == INFINITY: return self\nif self == INFINITY: return other\nassert self.__curve == other.__curve\nif self.__x == other.__x:\n  if ( self.__y + other.__y ) % self.__curve.p() == 0:\n    return INFINITY\n  else:\n    return self.double()\n\np = self.__curve.p()\n\nl = ( ( other.__y - self.__y ) * \\\n      numbertheory.inverse_mod( other.__x - self.__x, p ) ) % p\n\nx3 = ( l * l - self.__x - other.__x ) % p\ny3 = ( l * ( self.__x - x3 ) - self.__y ) % p\n\nreturn Point( self.__curve, x3, y3 )", "path": "electru\\ecdsa\\ellipticcurve.py", "repo_name": "bitxbay/BitXBay", "stars": 67, "license": "gpl-3.0", "language": "python", "size": 69116}
{"docstring": "\"\"\"Multiply a point by an integer.\"\"\"\n\n", "func_signal": "def __mul__( self, other ):\n", "code": "def leftmost_bit( x ):\n  assert x > 0\n  result = 1\n  while result <= x: result = 2 * result\n  return result // 2\n\ne = other\nif self.__order: e = e % self.__order\nif e == 0: return INFINITY\nif self == INFINITY: return INFINITY\nassert e > 0\n\n# From X9.62 D.3.2:\n\ne3 = 3 * e\nnegative_self = Point( self.__curve, self.__x, -self.__y, self.__order )\ni = leftmost_bit( e3 ) // 2\nresult = self\n# print_(\"Multiplying %s by %d (e3 = %d):\" % ( self, other, e3 ))\nwhile i > 1:\n  result = result.double()\n  if ( e3 & i ) != 0 and ( e & i ) == 0: result = result + self\n  if ( e3 & i ) == 0 and ( e & i ) != 0: result = result + negative_self\n  # print_(\". . . i = %d, result = %s\" % ( i, result ))\n  i = i // 2\n\nreturn result", "path": "electru\\ecdsa\\ellipticcurve.py", "repo_name": "bitxbay/BitXBay", "stars": 67, "license": "gpl-3.0", "language": "python", "size": 69116}
{"docstring": "\"\"\"curve, x, y, order; order (optional) is the order of this point.\"\"\"\n", "func_signal": "def __init__( self, curve, x, y, order = None ):\n", "code": "self.__curve = curve\nself.__x = x\nself.__y = y\nself.__order = order\n# self.curve is allowed to be None only for INFINITY:\nif self.__curve: assert self.__curve.contains_point( x, y )\nif order: assert self * order == INFINITY", "path": "electru\\ecdsa\\ellipticcurve.py", "repo_name": "bitxbay/BitXBay", "stars": 67, "license": "gpl-3.0", "language": "python", "size": 69116}
{"docstring": "\"\"\"Import module, returning the module after the last dot.\"\"\"\n", "func_signal": "def _import_module(name):\n", "code": "__import__(name)\nreturn sys.modules[name]", "path": "electru\\ecdsa\\six.py", "repo_name": "bitxbay/BitXBay", "stars": 67, "license": "gpl-3.0", "language": "python", "size": 69116}
{"docstring": "\"\"\"Return a new point that is twice the old.\"\"\"\n\n", "func_signal": "def double( self ):\n", "code": "if self == INFINITY:\n  return INFINITY\n\n# X9.62 B.3:\n\np = self.__curve.p()\na = self.__curve.a()\n\nl = ( ( 3 * self.__x * self.__x + a ) * \\\n      numbertheory.inverse_mod( 2 * self.__y, p ) ) % p\n\nx3 = ( l * l - 2 * self.__x ) % p\ny3 = ( l * ( self.__x - x3 ) - self.__y ) % p\n\nreturn Point( self.__curve, x3, y3 )", "path": "electru\\ecdsa\\ellipticcurve.py", "repo_name": "bitxbay/BitXBay", "stars": 67, "license": "gpl-3.0", "language": "python", "size": 69116}
{"docstring": "# detect backspace\n", "func_signal": "def edit_str(self, target, c, is_num=False):\n", "code": "if c in [8, 127, 263] and target:\n    target = target[:-1]\nelif not is_num or curses.unctrl(c) in '0123456789.':\n    target += curses.unctrl(c)\nreturn target", "path": "electru\\build\\lib\\electrum_gui\\text.py", "repo_name": "bitxbay/BitXBay", "stars": 67, "license": "gpl-3.0", "language": "python", "size": 69116}
{"docstring": "# Popup menu for the Subscriptions page\n", "func_signal": "def init_subscriptions_popup_menu(self):\n", "code": "self.ui.subscriptionsContextMenuToolbar = QtGui.QToolBar()\n# Actions\nself.actionsubscriptionsNew = self.ui.subscriptionsContextMenuToolbar.addAction(\n    _translate(\"MainWindow\", \"New\"), self.on_action_SubscriptionsNew)\nself.actionsubscriptionsDelete = self.ui.subscriptionsContextMenuToolbar.addAction(\n    _translate(\"MainWindow\", \"Delete\"),\n    self.on_action_SubscriptionsDelete)\nself.actionsubscriptionsClipboard = self.ui.subscriptionsContextMenuToolbar.addAction(\n    _translate(\"MainWindow\", \"Copy address to clipboard\"),\n    self.on_action_SubscriptionsClipboard)\nself.actionsubscriptionsEnable = self.ui.subscriptionsContextMenuToolbar.addAction(\n    _translate(\"MainWindow\", \"Enable\"),\n    self.on_action_SubscriptionsEnable)\nself.actionsubscriptionsDisable = self.ui.subscriptionsContextMenuToolbar.addAction(\n    _translate(\"MainWindow\", \"Disable\"),\n    self.on_action_SubscriptionsDisable)\nself.actionsubscriptionsSetAvatar = self.ui.subscriptionsContextMenuToolbar.addAction(\n    _translate(\"MainWindow\", \"Set avatar...\"),\n    self.on_action_SubscriptionsSetAvatar)\nself.ui.tableWidgetSubscriptions.setContextMenuPolicy(\n    QtCore.Qt.CustomContextMenu)\nself.connect(self.ui.tableWidgetSubscriptions, QtCore.SIGNAL(\n    'customContextMenuRequested(const QPoint&)'),\n             self.on_context_menuSubscriptions)\nself.popMenuSubscriptions = QtGui.QMenu(self)\nself.popMenuSubscriptions.addAction(self.actionsubscriptionsNew)\nself.popMenuSubscriptions.addAction(self.actionsubscriptionsDelete)\nself.popMenuSubscriptions.addSeparator()\nself.popMenuSubscriptions.addAction(self.actionsubscriptionsEnable)\nself.popMenuSubscriptions.addAction(self.actionsubscriptionsDisable)\nself.popMenuSubscriptions.addAction(self.actionsubscriptionsSetAvatar)\nself.popMenuSubscriptions.addSeparator()\nself.popMenuSubscriptions.addAction(self.actionsubscriptionsClipboard)", "path": "bitmessageqt\\__init__.py", "repo_name": "bitxbay/BitXBay", "stars": 67, "license": "gpl-3.0", "language": "python", "size": 69116}
{"docstring": "# Popup menu for the Address Book page\n", "func_signal": "def init_addressbook_popup_menu(self):\n", "code": "self.ui.addressBookContextMenuToolbar = QtGui.QToolBar()\n# Actions\nself.actionAddressBookSend = self.ui.addressBookContextMenuToolbar.addAction(\n    _translate(\n        \"MainWindow\", \"Send message to this address\"),\n    self.on_action_AddressBookSend)\nself.actionAddressBookClipboard = self.ui.addressBookContextMenuToolbar.addAction(\n    _translate(\n        \"MainWindow\", \"Copy address to clipboard\"),\n    self.on_action_AddressBookClipboard)\nself.actionAddressBookSubscribe = self.ui.addressBookContextMenuToolbar.addAction(\n    _translate(\n        \"MainWindow\", \"Subscribe to this address\"),\n    self.on_action_AddressBookSubscribe)\nself.actionAddressBookSetAvatar = self.ui.addressBookContextMenuToolbar.addAction(\n    _translate(\n        \"MainWindow\", \"Set avatar...\"),\n    self.on_action_AddressBookSetAvatar)\nself.actionAddressBookNew = self.ui.addressBookContextMenuToolbar.addAction(\n    _translate(\n        \"MainWindow\", \"Add New Address\"), self.on_action_AddressBookNew)\nself.actionAddressBookDelete = self.ui.addressBookContextMenuToolbar.addAction(\n    _translate(\n        \"MainWindow\", \"Delete\"), self.on_action_AddressBookDelete)\nself.ui.tableWidgetAddressBook.setContextMenuPolicy(\n    QtCore.Qt.CustomContextMenu)\nself.connect(self.ui.tableWidgetAddressBook, QtCore.SIGNAL(\n    'customContextMenuRequested(const QPoint&)'),\n             self.on_context_menuAddressBook)\nself.popMenuAddressBook = QtGui.QMenu(self)\nself.popMenuAddressBook.addAction(self.actionAddressBookSend)\nself.popMenuAddressBook.addAction(self.actionAddressBookClipboard)\nself.popMenuAddressBook.addAction(self.actionAddressBookSubscribe)\nself.popMenuAddressBook.addAction(self.actionAddressBookSetAvatar)\nself.popMenuAddressBook.addSeparator()\nself.popMenuAddressBook.addAction(self.actionAddressBookNew)\nself.popMenuAddressBook.addAction(self.actionAddressBookDelete)", "path": "bitmessageqt\\__init__.py", "repo_name": "bitxbay/BitXBay", "stars": 67, "license": "gpl-3.0", "language": "python", "size": 69116}
{"docstring": "\"\"\"Execute code in a namespace.\"\"\"\n", "func_signal": "def exec_(_code_, _globs_=None, _locs_=None):\n", "code": "if _globs_ is None:\n    frame = sys._getframe(1)\n    _globs_ = frame.f_globals\n    if _locs_ is None:\n        _locs_ = frame.f_locals\n    del frame\nelif _locs_ is None:\n    _locs_ = _globs_\nexec(\"\"\"exec _code_ in _globs_, _locs_\"\"\")", "path": "electru\\ecdsa\\six.py", "repo_name": "bitxbay/BitXBay", "stars": 67, "license": "gpl-3.0", "language": "python", "size": 69116}
{"docstring": "# key: document identifier\n# value: document contents\n", "func_signal": "def mapper(record):\n", "code": "matrix = record[0]\nrow, col = record[1:3]\nvalue = record[3]\nfor j in range(5):\n    if matrix == 'a':\n        mr.emit_intermediate((row, j), (col, matrix, value))\n    else:\n        mr.emit_intermediate((j, col), (row, matrix, value))", "path": "solutions\\multiply.py", "repo_name": "marcelcaraciolo/big-data-tutorial", "stars": 66, "license": "None", "language": "python", "size": 61486}
{"docstring": "# key: word\n# value: list of occurrence counts\n", "func_signal": "def reducer(key, list_of_values):\n", "code": "person, friend = key\nif len(list_of_values) == 1:\n   mr.emit((person, friend))\n   mr.emit((friend, person))\n   \n#total = sum(list_of_values)\n#mr.emit((person, total))", "path": "solutions\\asymmetric_friendships.py", "repo_name": "marcelcaraciolo/big-data-tutorial", "stars": 66, "license": "None", "language": "python", "size": 61486}
{"docstring": "# key: word\n# value: list of occurrence counts\n", "func_signal": "def reducer(key, list_of_values):\n", "code": "list_of_values = list(set(list_of_values))\nmr.emit((key, list_of_values))", "path": "solutions\\inverted_index.py", "repo_name": "marcelcaraciolo/big-data-tutorial", "stars": 66, "license": "None", "language": "python", "size": 61486}
{"docstring": "# key: document identifier\n# value: document contents\n", "func_signal": "def mapper(record):\n", "code": "key = record[0]\nvalue = record[1]\nwords = value.split()\nfor w in words:\n  mr.emit_intermediate(w, 1)", "path": "solutions\\wordcount.py", "repo_name": "marcelcaraciolo/big-data-tutorial", "stars": 66, "license": "None", "language": "python", "size": 61486}
{"docstring": "# key: document identifier\n# value: document contents\n", "func_signal": "def mapper(record):\n", "code": "identifier = record[0]\nsequence = record[1]\nmr.emit_intermediate(sequence[:-10], identifier)", "path": "solutions\\mrJob_example\\assignment3\\unique_trims.py", "repo_name": "marcelcaraciolo/big-data-tutorial", "stars": 66, "license": "None", "language": "python", "size": 61486}
{"docstring": "# key: document identifier\n# value: document contents\n", "func_signal": "def mapper(record):\n", "code": "identifier = record[0]\nsequence = record[1]\nmr.emit_intermediate(sequence[:-10], identifier)", "path": "solutions\\unique_trims.py", "repo_name": "marcelcaraciolo/big-data-tutorial", "stars": 66, "license": "None", "language": "python", "size": 61486}
{"docstring": "# key: word\n# value: list of occurrence counts\n", "func_signal": "def reducer(key, list_of_values):\n", "code": "total = 0\nfor v in list_of_values:\n  total += v\nmr.emit((key, total))", "path": "solutions\\wordcount.py", "repo_name": "marcelcaraciolo/big-data-tutorial", "stars": 66, "license": "None", "language": "python", "size": 61486}
{"docstring": "'''\nGet the TOP N recommendations for user.\n\nInput ({friend1,  [(numberOfMutualFriends, friend, explanations),\n                   (numberOfMutualFriends2, friend, explanations)]}):\n\n    \"fabio\", [1,\"marcel\",['jonas']]\n    \"marcel\", [[2,\"carol\",['maria','jose']]\n            , [1,\"fabio\",['jonas']], [1,\"fabiola\",['maria']],\n              [1,\"patricia\",['amanda']],\n                [1,\"paula\",['amanda']]\n    \"fabiola\", [1,\"marcel\",['maria']]\n    \"patricia\", [1,\"marcel\",['amanda']]\n    \"paula\", [[1,\"marcel\"]]\n    \"carol\", [[2,\"marcel\"]]\n\nOutput ({friend1,  [(numberOfMutualFriends, friend),\n                   (numberOfMutualFriends2, friend)]}):\n\n    Ex: Get the top 3 suggestions.\n\n    \"fabio\", [[1,\"marcel\"]]\n    \"marcel\", [[2,\"carol\"], [1,\"fabio\"], [1,\"fabiola\"]]\n    \"fabiola\", [[1,\"marcel\"]]\n    \"patricia\", [[1,\"marcel\"]]\n    \"paula\", [1,\"marcel\",['amanda']]\n    \"carol\", [2,\"marcel\",['maria','jose']]\n'''\n", "func_signal": "def top_recommendations(self, key, values):\n", "code": "recommendations = []\nfor score, item, explanations in values:\n    recommendations.append((item, score, explanations))\n\nyield key, sorted(recommendations, key=lambda k: -k[1])[:TOP_N]", "path": "solutions\\friend_recommender_exp.py", "repo_name": "marcelcaraciolo/big-data-tutorial", "stars": 66, "license": "None", "language": "python", "size": 61486}
{"docstring": "# key: document identifier\n# value: document contents\n", "func_signal": "def mapper(record):\n", "code": "key = record[0]\nvalue = record[1]\nwords = value.split(' ')\nfor w in words:\n  mr.emit_intermediate(w, key)", "path": "solutions\\inverted_index.py", "repo_name": "marcelcaraciolo/big-data-tutorial", "stars": 66, "license": "None", "language": "python", "size": 61486}
{"docstring": "# key: document identifier\n# value: document contents\n", "func_signal": "def mapper(record):\n", "code": "person = record[0]\nfriend = record[1]\nf = sorted([person, friend])\nmr.emit_intermediate(tuple(f), 1)", "path": "solutions\\asymmetric_friendships.py", "repo_name": "marcelcaraciolo/big-data-tutorial", "stars": 66, "license": "None", "language": "python", "size": 61486}
{"docstring": "# key: document identifier\n# value: document contents\n", "func_signal": "def mapper(record):\n", "code": "key = record[0]\nvalue = record[1]\nwords = value.split()\nfor w in words:\n  mr.emit_intermediate(w, 1)", "path": "solutions\\mrJob_example\\assignment3\\wordcount.py", "repo_name": "marcelcaraciolo/big-data-tutorial", "stars": 66, "license": "None", "language": "python", "size": 61486}
{"docstring": "# key: document identifier\n# value: document contents\n", "func_signal": "def mapper(record):\n", "code": "table = record[0]\norder_id = record[1]\ndata = record[2:]\nmr.emit_intermediate(order_id, (table, data))", "path": "solutions\\join.py", "repo_name": "marcelcaraciolo/big-data-tutorial", "stars": 66, "license": "None", "language": "python", "size": 61486}
{"docstring": "# key: document identifier\n# value: document contents\n", "func_signal": "def mapper(record):\n", "code": "person = record[0]\nfriend = record[1]\nf = sorted([person, friend])\nmr.emit_intermediate(tuple(f), 1)", "path": "solutions\\mrJob_example\\assignment3\\asymmetric_friendships.py", "repo_name": "marcelcaraciolo/big-data-tutorial", "stars": 66, "license": "None", "language": "python", "size": 61486}
{"docstring": "# key: word\n# value: list of occurrence counts\n", "func_signal": "def reducer(key, list_of_values):\n", "code": "person = key\ntotal = sum(list_of_values)\nmr.emit((person, total))", "path": "solutions\\friend_count.py", "repo_name": "marcelcaraciolo/big-data-tutorial", "stars": 66, "license": "None", "language": "python", "size": 61486}
{"docstring": "# key: document identifier\n# value: document contents\n", "func_signal": "def mapper(record):\n", "code": "matrix = record[0]\nrow, col = record[1:3]\nvalue = record[3]\nfor j in range(5):\n    if matrix == 'a':\n        mr.emit_intermediate((row, j), (col, matrix, value))\n    else:\n        mr.emit_intermediate((j, col), (row, matrix, value))", "path": "solutions\\mrJob_example\\assignment3\\multiply.py", "repo_name": "marcelcaraciolo/big-data-tutorial", "stars": 66, "license": "None", "language": "python", "size": 61486}
{"docstring": "'''\nPrepare the dataset to yield the source and\nget the top suggestions.\n\nInput ({[friend1, friend2],\n        (numberOfMutualFriends,[mutual1, mutual2, ... mutualn]}):\n\n    [\"fabio\", \"marcel\"]     (1, ['jonas'])\n    [\"fabiola\", \"marcel\"]   (1, ['maria'])\n    [\"marcel\", \"patricia\"]  (1, ['amanda'])\n    [\"marcel\", \"paula\"]     (1, ['amanda'])\n    [\"carol\", \"marcel\"]     (2, ['maria','jose'])\n\nOutput ({friend1,  [numberOfMutualFriends, friend2,\n                        [mutual1, mutual2, ... mutualn]]},\n        {friend2,  [numberOfMutualFriends, friend1,\n                        [mutual1, mutual2, ... mutualn]]}):\n\n    \"fabio\", [1,\"marcel\",['jonas']]\n    \"marcel\", [1,\"fabio\",['jonas']]\n    \"fabiola\", [1,\"marcel\",['maria']]\n    \"marcel\", [1,\"fabiola\",['maria']]\n    \"marcel\", [1,\"patricia\",['amanda']]\n    \"patricia\", [1,\"marcel\",['amanda']]\n    \"marcel\", [1,\"paula\",['amanda']]\n    \"paula\", [1,\"marcel\",['amanda']]\n    \"marcel\", [2,\"carol\",['maria','jose']]\n    \"carol\", [2,\"marcel\",['maria','jose']]\n\n'''\n", "func_signal": "def count_max_of_mutual_friends(self, key, value):\n", "code": "f1, f2 = key\nvalue, explanations = value\nyield f1, (int(value), f2, explanations)\nyield f2, (int(value), f1, explanations)", "path": "solutions\\friend_recommender_exp.py", "repo_name": "marcelcaraciolo/big-data-tutorial", "stars": 66, "license": "None", "language": "python", "size": 61486}
{"docstring": "# key: word\n# value: list of occurrence counts\n", "func_signal": "def reducer(key, list_of_values):\n", "code": "person, friend = key\nif len(list_of_values) == 1:\n   mr.emit((person, friend))\n   mr.emit((friend, person))\n   \n#total = sum(list_of_values)\n#mr.emit((person, total))", "path": "solutions\\mrJob_example\\assignment3\\asymmetric_friendships.py", "repo_name": "marcelcaraciolo/big-data-tutorial", "stars": 66, "license": "None", "language": "python", "size": 61486}
{"docstring": "'''\nSplit input to obtain the pair of friends\n\nInput (source -> {\u201cfriend1\u201d, \u201cfriend2\u201d, \u201cfriend3\u201d}):\n    marcel,jonas,maria,jose,amanda\n\nOutput ({[source, friend1], (-1,source)};\n        {[friend1, friend2],(1,source)};):\n\n    [\"jonas\", \"marcel\"]    -1,'marcel'\n    [\"jonas\", \"maria\"]      1,'marcel',\n    [\"jonas\", \"jose\"]       1,'marcel'\n    [\"amanda\", \"jonas\"]     1,'marcel'\n    [\"marcel\", \"maria\"]    -1,'marcel'\n    [\"jose\", \"maria\"]       1, 'marcel'\n    [\"amanda\", \"maria\"]     1, 'marcel'\n    [\"jose\", \"marcel\"]      -1, 'marcel'\n    [\"amanda\", \"jose\"]      1, 'marcel'\n    [\"amanda\", \"marcel\"]    -1, 'marcel'\n\n'''\n", "func_signal": "def map_input(self, key, line):\n", "code": "input = line.split(',')\nuser_id, item_ids = input[0], input[1:]\nfor i in range(len(item_ids)):\n    f1 = item_ids[i]\n    if user_id < f1:\n        yield (user_id, f1), (-1, user_id)\n    else:\n        yield (f1, user_id), (-1, user_id)\n\n    for j in range(i + 1, len(item_ids)):\n        f2 = item_ids[j]\n        if f1 < f2:\n            yield (f1, f2), (1, user_id)\n        else:\n            yield (f2, f1), (1, user_id)", "path": "solutions\\friend_recommender_exp.py", "repo_name": "marcelcaraciolo/big-data-tutorial", "stars": 66, "license": "None", "language": "python", "size": 61486}
{"docstring": "# key: document identifier\n# value: document contents\n", "func_signal": "def mapper(record):\n", "code": "person = record[0]\nfriend = record[1]\nmr.emit_intermediate(person, 1)", "path": "solutions\\friend_count.py", "repo_name": "marcelcaraciolo/big-data-tutorial", "stars": 66, "license": "None", "language": "python", "size": 61486}
{"docstring": "# key: word\n# value: list of occurrence counts\n", "func_signal": "def reducer(key, list_of_values):\n", "code": "total = 0\nfor v in list_of_values:\n  total += v\nmr.emit((key, total))", "path": "solutions\\mrJob_example\\assignment3\\wordcount.py", "repo_name": "marcelcaraciolo/big-data-tutorial", "stars": 66, "license": "None", "language": "python", "size": 61486}
{"docstring": "# define default configs\n", "func_signal": "def __init__(self, configs):\n", "code": "self.config = {\n    'linenums': [None, \"Use lines numbers. True=yes, False=no, None=auto\"],\n    'force_linenos' : [False, \"Depreciated! Use 'linenums' instead. Force line numbers - Default: False\"],\n    'guess_lang' : [True, \"Automatic language detection - Default: True\"],\n    'css_class' : [\"codehilite\",\n                   \"Set class name for wrapper <div> - Default: codehilite\"],\n    'pygments_style' : ['default', 'Pygments HTML Formatter Style (Colorscheme) - Default: default'],\n    'noclasses': [False, 'Use inline styles instead of CSS classes - Default false']\n    }\n\n# Override defaults with user settings\nfor key, value in configs:\n    # convert strings to booleans\n    if value == 'True': value = True\n    if value == 'False': value = False\n    if value == 'None': value = None\n\n    if key == 'force_linenos':\n        warnings.warn('The \"force_linenos\" config setting'\n            ' to the CodeHilite extension is deprecrecated.'\n            ' Use \"linenums\" instead.', PendingDeprecationWarning)\n        if value:\n            # Carry 'force_linenos' over to new 'linenos'.\n            self.setConfig('linenums', True)\n\n    self.setConfig(key, value)", "path": "lib\\markdown\\extensions\\codehilite.py", "repo_name": "akngs/ecogwiki", "stars": 78, "license": "other", "language": "python", "size": 5111}
{"docstring": "# .pub BBS1 -> .pub BBS2\n", "func_signal": "def test_change_page_of_published_page(self):\n", "code": "self.update_page(u'.pub BBS1\\nHello', u'Hello')\npage = self.update_page(u'.pub BBS2\\nHello', u'Hello')\nself.assertIsNotNone(page.published_at)\nself.assertEqual('BBS2', page.published_to)", "path": "tests\\test_blog.py", "repo_name": "akngs/ecogwiki", "stars": 78, "license": "other", "language": "python", "size": 5111}
{"docstring": "# .pub BBS -> (null)\n", "func_signal": "def test_unpublish_published_page(self):\n", "code": "self.update_page(u'.pub BBS\\nHello', u'Hello')\npage = self.update_page(u'Hello', u'Hello')\nself.assertIsNone(page.published_at)\nself.assertEqual(None, page.published_to)", "path": "tests\\test_blog.py", "repo_name": "akngs/ecogwiki", "stars": 78, "license": "other", "language": "python", "size": 5111}
{"docstring": "\"\"\"\nDetermines language of a code block from shebang line and whether said\nline should be removed or left in place. If the sheband line contains a\npath (even a single /) then it is assumed to be a real shebang line and\nleft alone. However, if no path is given (e.i.: #!python or :::python)\nthen it is assumed to be a mock shebang for language identifitation of a\ncode fragment and removed from the code block prior to processing for\ncode highlighting. When a mock shebang (e.i: #!python) is found, line\nnumbering is turned on. When colons are found in place of a shebang\n(e.i.: :::python), line numbering is left in the current state - off\nby default.\n\n\"\"\"\n\n", "func_signal": "def _getLang(self):\n", "code": "import re\n\n#split text into lines\nlines = self.src.split(\"\\n\")\n#pull first line to examine\nfl = lines.pop(0)\n\nc = re.compile(r'''\n    (?:(?:^::+)|(?P<shebang>^[#]!))\t# Shebang or 2 or more colons.\n    (?P<path>(?:/\\w+)*[/ ])?        # Zero or 1 path\n    (?P<lang>[\\w+-]*)               # The language\n    ''',  re.VERBOSE)\n# search first line for shebang\nm = c.search(fl)\nif m:\n    # we have a match\n    try:\n        self.lang = m.group('lang').lower()\n    except IndexError:\n        self.lang = None\n    if m.group('path'):\n        # path exists - restore first line\n        lines.insert(0, fl)\n    if self.linenums is None and m.group('shebang'):\n        # Overridable and Shebang exists - use line numbers\n        self.linenums = True\nelse:\n    # No match\n    lines.insert(0, fl)\n\nself.src = \"\\n\".join(lines).strip(\"\\n\")", "path": "lib\\markdown\\extensions\\codehilite.py", "repo_name": "akngs/ecogwiki", "stars": 78, "license": "other", "language": "python", "size": 5111}
{"docstring": "\"\"\" Find code blocks and store in htmlStash. \"\"\"\n", "func_signal": "def run(self, root):\n", "code": "blocks = root.getiterator('pre')\nfor block in blocks:\n    children = block.getchildren()\n    if len(children) == 1 and children[0].tag == 'code':\n        code = CodeHilite(children[0].text,\n                    linenums=self.config['linenums'],\n                    guess_lang=self.config['guess_lang'],\n                    css_class=self.config['css_class'],\n                    style=self.config['pygments_style'],\n                    noclasses=self.config['noclasses'],\n                    tab_length=self.markdown.tab_length)\n        placeholder = self.markdown.htmlStash.store(code.hilite(),\n                                                    safe=True)\n        # Clear codeblock in etree instance\n        block.clear()\n        # Change to p element which will later\n        # be removed when inserting raw html\n        block.tag = 'p'\n        block.text = placeholder", "path": "lib\\markdown\\extensions\\codehilite.py", "repo_name": "akngs/ecogwiki", "stars": 78, "license": "other", "language": "python", "size": 5111}
{"docstring": "# .pub BBS -> .pub\n", "func_signal": "def test_remove_page_of_published_page(self):\n", "code": "self.update_page(u'.pub BBS\\nHello', u'Hello')\npage = self.update_page(u'.pub\\nHello', u'Hello')\nself.assertIsNotNone(page.published_at)\nself.assertEqual(None, page.published_to)", "path": "tests\\test_blog.py", "repo_name": "akngs/ecogwiki", "stars": 78, "license": "other", "language": "python", "size": 5111}
{"docstring": "\"\"\"Check HTTP response status is expected.\n\nArgs:\n  status: HTTP response status. int.\n  expected: a list of expected statuses. A list of ints.\n  headers: HTTP response headers.\n\nRaises:\n  AuthorizationError: if authorization failed.\n  NotFoundError: if an object that's expected to exist doesn't.\n  TimeoutError: if HTTP request timed out.\n  ServerError: if server experienced some errors.\n  FatalError: if any other unexpected errors occurred.\n\"\"\"\n", "func_signal": "def check_status(status, expected, headers=None):\n", "code": "if status in expected:\n  return\n\nmsg = ('Expect status %r from Google Storage. But got status %d. Response '\n       'headers: %r' %\n       (expected, status, headers))\n\nif status == httplib.UNAUTHORIZED:\n  raise AuthorizationError(msg)\nelif status == httplib.FORBIDDEN:\n  raise ForbiddenError(msg)\nelif status == httplib.NOT_FOUND:\n  raise NotFoundError(msg)\nelif status == httplib.REQUEST_TIMEOUT:\n  raise TimeoutError(msg)\nelif status == httplib.REQUESTED_RANGE_NOT_SATISFIABLE:\n  raise InvalidRange(msg)\nelif status >= 500:\n  raise ServerError(msg)\nelse:\n  raise FatalError(msg)", "path": "lib\\cloudstorage\\errors.py", "repo_name": "akngs/ecogwiki", "stars": 78, "license": "other", "language": "python", "size": 5111}
{"docstring": "# links in hierarchical title and body\n", "func_signal": "def _parse_outlinks(self):\n", "code": "dicts = [\n    {'%s/relatedTo' % self.itemtype: [path[0] for path in self.paths[:-1]]},\n    md_wikilink.parse_wikilinks(self.itemtype, WikiPage.remove_metadata(self.body)),\n]\n\n# links in structured data\nfor name, value in self.data.items():\n    if type(value) is list:\n        dicts += [self._schema_item_to_links(name, v) for v in value]\n    else:\n        dicts.append(self._schema_item_to_links(name, value))\n\n# merge\nmerged = merge_dicts(dicts, force_list=True)\n\n# exclude links to this page\nreturn dict((k, v) for k, v in merged.items()\n            if not((type(v) == list and self.title in v) or self.title == v))", "path": "models\\wiki_page.py", "repo_name": "akngs/ecogwiki", "stars": 78, "license": "other", "language": "python", "size": 5111}
{"docstring": "\"\"\"Returns all links ordered by score\"\"\"\n\n# related links\n", "func_signal": "def link_scoretable(self):\n", "code": "related_links_scoretable = self.related_links\n\n# in/out links\ninlinks = reduce(lambda a, b: a + b, self.inlinks.values(), [])\noutlinks = reduce(lambda a, b: a + b, self.outlinks.values(), [])\ninout_links = set(inlinks + outlinks).difference(related_links_scoretable.keys())\ninout_links_len = len(inout_links)\ninout_score = 1.0 / inout_links_len if inout_links_len != 0 else 0.0\ninout_links_scoretable = dict(zip(inout_links, [inout_score] * inout_links_len))\n\nscoretable = dict(inout_links_scoretable.items() + related_links_scoretable.items())\nsorted_scoretable = sorted(scoretable.iteritems(),\n                           key=operator.itemgetter(1),\n                           reverse=True)\nreturn OrderedDict(sorted_scoretable)", "path": "models\\wiki_page.py", "repo_name": "akngs/ecogwiki", "stars": 78, "license": "other", "language": "python", "size": 5111}
{"docstring": "\"\"\"Update related_links score table by random walk\"\"\"\n", "func_signal": "def update_related_links(self, max_distance=5):\n", "code": "if len(self.outlinks) == 0:\n    return False\n\n# random walk\nscore_table = self.related_links\nupdated = WikiPage._update_related_links(self, self, 0.1, score_table, max_distance)\nif not updated:\n    return False\n\nself.related_links = score_table\nself.normalize_related_links()\nreturn True", "path": "models\\wiki_page.py", "repo_name": "akngs/ecogwiki", "stars": 78, "license": "other", "language": "python", "size": 5111}
{"docstring": "# handle added links\n", "func_signal": "def _update_inlinks(self, added_outlinks, removed_outlinks):\n", "code": "updates = []\nfor rel, titles in added_outlinks.items():\n    for title in titles:\n        page = WikiPage.get_by_title(title, follow_redirect=True)\n        page.add_inlink(self.title, rel)\n        updates.append(page)\n\nif updates:\n    ndb.put_multi(updates)\n    for page in updates:\n        caching.del_rendered_body(page.title)\n        caching.del_hashbangs(page.title)\n\n# handle removed links\nupdates = []\ndeletes = []\nfor rel, titles in removed_outlinks.items():\n    for title in titles:\n        page = WikiPage.get_by_title(title, follow_redirect=True)\n        page.del_inlink(self.title, rel)\n        if len(page.inlinks) == 0 and page.revision == 0 and page.key:\n            deletes.append(page.key)\n        else:\n            updates.append(page)\n\nif updates:\n    ndb.put_multi(updates)\nif deletes:\n    ndb.delete_multi(deletes)\nfor page in updates + deletes:\n    caching.del_rendered_body(page.title)\n    caching.del_hashbangs(page.title)", "path": "models\\wiki_page.py", "repo_name": "akngs/ecogwiki", "stars": 78, "license": "other", "language": "python", "size": 5111}
{"docstring": "\"\"\"\nPass code to the [Pygments](http://pygments.pocoo.org/) highliter with\noptional line numbers. The output should then be styled with css to\nyour liking. No styles are applied by default - only styling hooks\n(i.e.: <span class=\"k\">).\n\nreturns : A string of html.\n\n\"\"\"\n\n", "func_signal": "def hilite(self):\n", "code": "self.src = self.src.strip('\\n')\n\nif self.lang is None:\n    self._getLang()\n\nif pygments:\n    try:\n        lexer = get_lexer_by_name(self.lang)\n    except ValueError:\n        try:\n            if self.guess_lang:\n                lexer = guess_lexer(self.src)\n            else:\n                lexer = TextLexer()\n        except ValueError:\n            lexer = TextLexer()\n    formatter = HtmlFormatter(linenos=self.linenums,\n                              cssclass=self.css_class,\n                              style=self.style,\n                              noclasses=self.noclasses)\n    return highlight(self.src, lexer, formatter)\nelse:\n    # just escape and build markup usable by JS highlighting libs\n    txt = self.src.replace('&', '&amp;')\n    txt = txt.replace('<', '&lt;')\n    txt = txt.replace('>', '&gt;')\n    txt = txt.replace('\"', '&quot;')\n    classes = []\n    if self.lang:\n        classes.append('language-%s' % self.lang)\n    if self.linenums:\n        classes.append('linenums')\n    class_str = ''\n    if classes:\n        class_str = ' class=\"%s\"' % ' '.join(classes) \n    return '<pre class=\"%s\"><code%s>%s</code></pre>\\n'% \\\n                (self.css_class, class_str, txt)", "path": "lib\\markdown\\extensions\\codehilite.py", "repo_name": "akngs/ecogwiki", "stars": 78, "license": "other", "language": "python", "size": 5111}
{"docstring": "# do not update if the body is not changed\n", "func_signal": "def _update_content_all(self, body, base_revision, comment, user, force_update, dont_create_rev, dont_defer):\n", "code": "if not force_update and self.body == body:\n    return False\n\nnow = datetime.now()\n\n# validate and prepare new contents\nnew_data, new_md = self.validate_new_content(base_revision, body, user)\nnew_body = self._merge_if_needed(base_revision, body)\n\n# get old data and metadata\ntry:\n    old_md = self.metadata.copy()\nexcept ValueError:\n    old_md = {}\n\ntry:\n    old_data = self.data.copy()\nexcept ValueError:\n    old_data = {}\n\n# delete caches\ncaching.del_rendered_body(self.title)\ncaching.del_hashbangs(self.title)\ncaching.del_metadata(self.title)\ncaching.del_data(self.title)\n\n# update model and save\nself.body = new_body\nself.modifier = user\nself.description = PageOperationMixin.make_description(new_body)\nself.acl_read = new_md.get('read', '')\nself.acl_write = new_md.get('write', '')\nself.comment = comment\nself.itemtype_path = schema.get_itemtype_path(new_md['schema'])\nself._update_pub_state(new_md, old_md)\nif not dont_create_rev:\n    self.revision += 1\nif not force_update:\n    self.updated_at = now\nself.put()\n\n# create revision\nif not dont_create_rev:\n    rev_key = self._rev_key()\n    rev = WikiPageRevision(parent=rev_key, title=self.title, body=self.body,\n                           created_at=self.updated_at, revision=self.revision,\n                           comment=self.comment, modifier=self.modifier,\n                           acl_read=self.acl_read, acl_write=self.acl_write)\n    rev.put()\n\n# update inlinks, outlinks and schema data index\nself.update_links_and_data(old_md.get('redirect'), new_md.get('redirect'), old_data, new_data, dont_defer)\n\n# delete config cache\nif self.title == '.config':\n    caching.del_config()\n\n# delete title cache if it's a new page\nif self.revision == 1:\n    caching.del_titles()\n\nreturn True", "path": "models\\wiki_page.py", "repo_name": "akngs/ecogwiki", "stars": 78, "license": "other", "language": "python", "size": 5111}
{"docstring": "# parse\n", "func_signal": "def search(cls, expression):\n", "code": "parsed = search.parse_expression(expression)\n\n# evaluate\npos, neg = parsed['pos'], parsed['neg']\npos_pages = [cls.get_by_title(t, True) for t in pos]\nneg_pages = [cls.get_by_title(t, True) for t in neg]\nscoretable = search.evaluate(\n    dict((page.title, page.link_scoretable) for page in pos_pages),\n    dict((page.title, page.link_scoretable) for page in neg_pages)\n)\n\nreturn scoretable", "path": "models\\wiki_page.py", "repo_name": "akngs/ecogwiki", "stars": 78, "license": "other", "language": "python", "size": 5111}
{"docstring": "\"\"\"Updates outlinks of this page and inlinks of target pages\"\"\"\n# 1. process \"redirect\" metadata\n", "func_signal": "def update_links(self, old_redir, new_redir):\n", "code": "self._update_redirected_links(new_redir, old_redir)\n\n# 2. update inlinks\ncur_outlinks = self.outlinks\nnew_outlinks = {}\nfor rel, titles in self._parse_outlinks().items():\n    new_outlinks[rel] = list({WikiPage.get_by_title(t, follow_redirect=True).title for t in titles})\n\nif self.acl_read:\n    # delete all inlinks of target pages if the source page has a read restriction\n    added_outlinks = {}\n    removed_outlinks = cur_outlinks\nelse:\n    # update all inlinks of target pages\n    added_outlinks = {}\n    for rel, titles in new_outlinks.items():\n        added_outlinks[rel] = titles\n        if rel in cur_outlinks:\n            added_outlinks[rel] = set(added_outlinks[rel]).difference(cur_outlinks[rel])\n    removed_outlinks = {}\n    for rel, titles in cur_outlinks.items():\n        removed_outlinks[rel] = titles\n        if rel in new_outlinks:\n            removed_outlinks[rel] = set(removed_outlinks[rel]).difference(new_outlinks[rel])\n\nself._update_inlinks(added_outlinks, removed_outlinks)\n\n# 3. update outlinks of this page\n[new_outlinks[rel].sort() for rel in new_outlinks.keys()]\nself.outlinks = new_outlinks\nself.put()", "path": "models\\wiki_page.py", "repo_name": "akngs/ecogwiki", "stars": 78, "license": "other", "language": "python", "size": 5111}
{"docstring": "\"\"\" Add HilitePostprocessor to Markdown instance. \"\"\"\n", "func_signal": "def extendMarkdown(self, md, md_globals):\n", "code": "hiliter = HiliteTreeprocessor(md)\nhiliter.config = self.getConfigs()\nmd.treeprocessors.add(\"hilite\", hiliter, \"<inline\")\n\nmd.registerExtension(self)", "path": "lib\\markdown\\extensions\\codehilite.py", "repo_name": "akngs/ecogwiki", "stars": 78, "license": "other", "language": "python", "size": 5111}
{"docstring": "\"\"\" Override existing Processors. \"\"\"\n", "func_signal": "def extendMarkdown(self, md, md_globals):\n", "code": "md.parser.blockprocessors['olist'] = SaneOListProcessor(md.parser)\nmd.parser.blockprocessors['ulist'] = SaneUListProcessor(md.parser)", "path": "lib\\markdown\\extensions\\sane_lists.py", "repo_name": "akngs/ecogwiki", "stars": 78, "license": "other", "language": "python", "size": 5111}
{"docstring": "# skip until find matching index\n", "func_signal": "def replacer(m):\n", "code": "cur_index['value'] += 1\nif cur_index['value'] != index:\n    return m.group(0)\n\n# replace\nreturn m.group(1) + content + m.group(3) + '\\n' + m.group(0)", "path": "models\\wiki_page.py", "repo_name": "akngs/ecogwiki", "stars": 78, "license": "other", "language": "python", "size": 5111}
{"docstring": "# skip until find matching index\n", "func_signal": "def replacer(m):\n", "code": "cur_index['value'] += 1\nif cur_index['value'] != index:\n    return m.group(0)\n\n# replace\nreturn u'[x]' if content == u'1' else u'[ ]'", "path": "models\\wiki_page.py", "repo_name": "akngs/ecogwiki", "stars": 78, "license": "other", "language": "python", "size": 5111}
{"docstring": "\"\"\"Change in/out links of self and related pages according to new redirect metadata\"\"\"\n", "func_signal": "def _update_redirected_links(self, new_redir, old_redir):\n", "code": "if old_redir == new_redir:\n    return\n\nsource = WikiPage.get_by_title(old_redir, follow_redirect=True) if old_redir else self\nif len(source.inlinks) == 0:\n    return\n\ntarget = WikiPage.get_by_title(new_redir, follow_redirect=True) if new_redir else self\n\nupdates = [source, target]\nfor rel, titles in source.inlinks.items():\n    for t in titles:\n        page = WikiPage.get_by_title(t)\n        page.del_outlink(source.title, rel)\n        page.add_outlink(target.title, rel)\n        updates.append(page)\n\n    target.add_inlinks(source.inlinks[rel], rel)\n    del source.inlinks[rel]\n\nndb.put_multi(updates)\nfor page in updates:\n    caching.del_rendered_body(page.title)\n    caching.del_hashbangs(page.title)", "path": "models\\wiki_page.py", "repo_name": "akngs/ecogwiki", "stars": 78, "license": "other", "language": "python", "size": 5111}
{"docstring": "\"\"\" Gets people with most followers. \"\"\"\n\n", "func_signal": "def get_popular_people(followed, num):\n", "code": "dict_num_followers = {}\nfor node in followed.keys():\n    dict_num_followers[node] = len(followed[node])\n\npopular_people = sorted(dict_num_followers,\n    key=dict_num_followers.__getitem__,\n    reverse=True)\n\nreturn popular_people[0 : num]", "path": "fb_suggest_missing_link\\main.py", "repo_name": "FindBoat/Kaggle", "stars": 86, "license": "None", "language": "python", "size": 17633}
{"docstring": "\"\"\" Running on the test file. \"\"\"\n\n", "func_signal": "def spring_brother(training_file, test_file, submission_file):\n", "code": "y, meta_data = utilities.read_training_file(training_file)\nids, meta_data_test = utilities.read_test_file(test_file)\n\nx_train, x_test = feature_selection.generate_features(meta_data,\n    y, meta_data_test)\n\nclf = classification.random_forest(x_train, y, None, None)\n\np = classification.get_prob(clf, x_test)\nutilities.write_submission_file(submission_file, ids, p)", "path": "photo_quality_prediction\\main.py", "repo_name": "FindBoat/Kaggle", "stars": 86, "license": "None", "language": "python", "size": 17633}
{"docstring": "\"\"\" Extracts all the data includes rating, track, user etc. given\nan artist id. \"\"\"\n", "func_signal": "def extract_rating(data, artist):\n", "code": "ratings = []\nfor row in data:\n    if row[0] == artist:\n        ratings.append(row)\nreturn ratings", "path": "music_rating\\music_rating.py", "repo_name": "FindBoat/Kaggle", "stars": 86, "license": "None", "language": "python", "size": 17633}
{"docstring": "\"\"\" Reads data from file. \"\"\"\n", "func_signal": "def read_data(path, ignore_header=True,  max_line=-1):\n", "code": "csv_file_object = csv.reader(open(path, 'rb'))\nif ignore_header:\n    header = csv_file_object.next()\nx = []\nfor row in csv_file_object:\n    if max_line >= 0 and len(row) >= max_line:\n        break\n    x.append(row)\nreturn x", "path": "music_rating\\music_rating.py", "repo_name": "FindBoat/Kaggle", "stars": 86, "license": "None", "language": "python", "size": 17633}
{"docstring": "\"\"\" Generate user features from users.csv.\nFeatures include sex, age, questions.\"\"\"\n", "func_signal": "def generate_user_features():\n", "code": "profiles = read_data(__users__)\nindices_features = [0, 1, 2] + range(8, 27)\nuser_map = {}\nfor row in profiles:\n    features = []\n    for index in indices_features:\n        features.append(row[index])\n    if features[1] == 'Male':\n        features[1] = 0\n    else:\n        features[1] = 1\n    user_map[features[0]] = features[1 : :]\nreturn user_map", "path": "music_rating\\music_rating.py", "repo_name": "FindBoat/Kaggle", "stars": 86, "license": "None", "language": "python", "size": 17633}
{"docstring": "\"\"\" Calculate the standard deviation. \"\"\"\n\n", "func_signal": "def std(iterable, avg):\n", "code": "std = 0.0\nfor n in iterable:\n    std += (n - avg) ** 2\nreturn math.sqrt(std)", "path": "photo_quality_prediction\\feature_selection.py", "repo_name": "FindBoat/Kaggle", "stars": 86, "license": "None", "language": "python", "size": 17633}
{"docstring": "\"\"\" Generates features for each (artist, user) pair from word.csv. \"\"\"\n", "func_signal": "def generate_artist_user_pref():\n", "code": "words = read_data(__words__)\nartist_user_pref = {}\nfor row in words:\n    artist_user = (row[0], row[1])\n    pref = row[4 : :]\n    for i in range(len(pref)):\n        if pref[i] == '':\n            pref[i] = 0.0\n        else:\n            pref[i] = float(pref[i])\n    if len(pref) == 82:\n        pref.append(0)\n    artist_user_pref[artist_user] = pref\nreturn artist_user_pref", "path": "music_rating\\music_rating.py", "repo_name": "FindBoat/Kaggle", "stars": 86, "license": "None", "language": "python", "size": 17633}
{"docstring": "\"\"\" Creates features for a given pair of nodes. \"\"\"\n\n# Level 1 features.\n", "func_signal": "def get_features(follow, followed, n1, n2):\n", "code": "does_follow = 0\nif n1 in follow[n2]:\n    does_follow = 1\n\n# Level 2 features.\nfollowees_follow = set.intersection(follow[n1], followed[n2])\npercent_followees_follow = 0.0\nif len(follow[n1]) > 0:\n    percent_followees_follow = 1.0 * len(followees_follow) / len(follow[n1])\n\nfollowees_followed = set.intersection(follow[n1], follow[n2])\npercent_followees_followed = 0.0\nif len(follow[n1]) > 0:\n    percent_followees_followed = 1.0 * len(followees_followed) \\\n        / len(follow[n1])\n\nfollowers_follow = set.intersection(followed[n1], followed[n2])\npercent_followers_follow = 0.0\nif len(followed[n1]) > 0:\n    percent_followers_follow = 1.0 * len(followers_follow) \\\n        / len(followed[n1])\n\nfollowers_followed = set.intersection(followed[n1], follow[n2])\npercent_followers_followed = 0.0\nif len(followed[n1]) > 0:\n    percent_followers_followed = 1.0 * len(followers_followed) \\\n        / len(followed[n1])\n\nreturn [does_follow, percent_followees_follow, percent_followees_followed,\n        percent_followers_follow, percent_followers_followed]", "path": "fb_suggest_missing_link\\rank.py", "repo_name": "FindBoat/Kaggle", "stars": 86, "license": "None", "language": "python", "size": 17633}
{"docstring": "\"\"\" Fills empty features with averages and scales the data.\"\"\"\n", "func_signal": "def preprocess_feature(user_map):\n", "code": "num_features = 21\nmean = get_mean(user_map, num_features)\nstd = get_std(user_map, num_features, mean)\n# Scaling.\nfor key in user_map.keys():\n    features = user_map[key]\n    for i in range(len(features)):\n        if features[i] == '':\n            features[i] = 0.0\n        else:\n            features[i] = (float(features[i]) - mean[i]) / std[i]", "path": "music_rating\\music_rating.py", "repo_name": "FindBoat/Kaggle", "stars": 86, "license": "None", "language": "python", "size": 17633}
{"docstring": "\"\"\" Creates a map which maps a key to its average value. \"\"\"\n\n", "func_signal": "def create_key_avg_map(k_v_pairs):\n", "code": "key_avg_map = {}\nfor pair in k_v_pairs:\n    k = pair[0]\n    v = pair[1]\n    if k not in key_avg_map:\n        key_avg_map[k] = [v, 1]\n    else:\n        key_avg_map[k][0] += v\n        key_avg_map[k][1] += 1\n\nfor key in key_avg_map.keys():\n    key_avg_map[key] = float(key_avg_map[key][0]) / key_avg_map[key][1]\n\nreturn key_avg_map", "path": "photo_quality_prediction\\feature_selection.py", "repo_name": "FindBoat/Kaggle", "stars": 86, "license": "None", "language": "python", "size": 17633}
{"docstring": "\"\"\" Generates features for classifier. \"\"\"\n\n# Generate maps.\n", "func_signal": "def generate_features(meta_data_train, y_train, meta_data_test):\n", "code": "name_score_map, desc_score_map, caption_score_map, word_score_map = \\\n    generate_text_score_map(meta_data_train, y_train)\ngeo_score_map, lat_score_map, lon_score_map = generate_geo_score_map(\n    meta_data_train, y_train)\nshape_score_map, size_score_map, width_score_map, height_score_map = \\\n        generate_size_score_map(meta_data_train, y_train)\n\n# Genearte text features.\ntext_features_train = generate_text_features(meta_data_train,\n    name_score_map, desc_score_map, caption_score_map, word_score_map)\ntext_features_test = generate_text_features(meta_data_test,\n    name_score_map, desc_score_map, caption_score_map, word_score_map)\n\n# Generates geo features.\ngeo_features_train = generate_geo_features(meta_data_train, geo_score_map,\n    lat_score_map, lon_score_map)\ngeo_features_test = generate_geo_features(meta_data_test, geo_score_map,\n    lat_score_map, lon_score_map)\n\n# Generates size features\nsize_features_train = generate_size_features(meta_data_train,\n    shape_score_map, size_score_map, width_score_map, height_score_map)\nsize_features_test = generate_size_features(meta_data_test,\n    shape_score_map, size_score_map, width_score_map, height_score_map)\n\n# Combines all features.\nx_train = []\nfor i in range(len(text_features_train)):\n    x_train.append(text_features_train[i] + size_features_train[i] \\\n         + geo_features_train[i])\n\nx_test = []\nfor i in range(len(text_features_test)):\n    x_test.append(text_features_test[i] + size_features_test[i] \\\n        + geo_features_test[i])\n\nreturn (x_train, x_test)", "path": "photo_quality_prediction\\feature_selection.py", "repo_name": "FindBoat/Kaggle", "stars": 86, "license": "None", "language": "python", "size": 17633}
{"docstring": "\"\"\" Calculate average rating for each artist. \"\"\"\n", "func_signal": "def generate_artist_mean(data):\n", "code": "artist_mean = {}\nartist_rate = {}\nfor row in data:\n    artist = row[0]\n    rate = row[3]\n    if artist_rate.has_key(artist):\n        artist_rate[artist].append(float(rate))\n    else:\n        artist_rate[artist] = [float(rate)]\nfor key in artist_rate.keys():\n    artist_mean[key] = sum(artist_rate[key]) / len(artist_rate[key])\nreturn artist_mean", "path": "music_rating\\music_rating.py", "repo_name": "FindBoat/Kaggle", "stars": 86, "license": "None", "language": "python", "size": 17633}
{"docstring": "\"\"\" Calculating RMSE error. \"\"\"\n", "func_signal": "def rmse(real_value, predict_value):\n", "code": "rmse = 0.0\nfor i in range(real_value.shape[0]):\n    rmse += (real_value[i] - predict_value[i]) ** 2\nrmse = math.sqrt(rmse / real_value.shape[0])\nreturn rmse", "path": "music_rating\\music_rating.py", "repo_name": "FindBoat/Kaggle", "stars": 86, "license": "None", "language": "python", "size": 17633}
{"docstring": "\"\"\" Calculates the average value of a map. \"\"\"\n\n", "func_signal": "def get_map_avg(k_v_map):\n", "code": "avg = 0.0\nfor key in k_v_map.keys():\n    avg += k_v_map[key]\nreturn float(avg) / len(k_v_map)", "path": "photo_quality_prediction\\feature_selection.py", "repo_name": "FindBoat/Kaggle", "stars": 86, "license": "None", "language": "python", "size": 17633}
{"docstring": "\"\"\" Gets candidates for node to suggest follow. \"\"\"\n\n", "func_signal": "def get_candidates(follow, followed, node):\n", "code": "nodes_exclude = follow[node].copy()\nnodes_exclude.add(node)\n\nl1_candidates = get_surroundings(follow, followed, [node])\nl2_candidates = get_surroundings(follow, followed, l1_candidates)\nl3_candidates = get_surroundings(follow, followed, l2_candidates)\n\ncandidates = set()\ncandidates.update(l1_candidates)\ncandidates.update(l2_candidates)\ncandidates.update(l3_candidates)\n\ncandidates.difference_update(nodes_exclude)\nreturn candidates", "path": "fb_suggest_missing_link\\candidate.py", "repo_name": "FindBoat/Kaggle", "stars": 86, "license": "None", "language": "python", "size": 17633}
{"docstring": "\"\"\" Gets all followers and followees of a given sets of nodes. \"\"\"\n\n", "func_signal": "def get_surroundings(follow, followed, nodes):\n", "code": "followers_and_followees = set()\nfor node in nodes:\n    followers_and_followees.update(follow[node])\n    followers_and_followees.update(followed[node])\nreturn followers_and_followees", "path": "fb_suggest_missing_link\\candidate.py", "repo_name": "FindBoat/Kaggle", "stars": 86, "license": "None", "language": "python", "size": 17633}
{"docstring": "\"\"\" Gets the probability of being good. \"\"\"\n\n", "func_signal": "def get_prob(clf, x):\n", "code": "prob = array(clf.predict_proba(x))\nreturn prob[:, 1]", "path": "photo_quality_prediction\\classification.py", "repo_name": "FindBoat/Kaggle", "stars": 86, "license": "None", "language": "python", "size": 17633}
{"docstring": "\"\"\" Divides data into training set and cross validation set. \"\"\"\n\n", "func_signal": "def prepare_data(x, y, size=0.3, state=0):\n", "code": "x_train, x_cv, y_train, y_cv = cross_validation.train_test_split(\n    x, y, test_size=size, random_state=state)\n\nreturn (x_train, y_train, x_cv, y_cv)", "path": "photo_quality_prediction\\classification.py", "repo_name": "FindBoat/Kaggle", "stars": 86, "license": "None", "language": "python", "size": 17633}
{"docstring": "\"\"\" Produces training set, cross validation set and test set. \"\"\"\n\n", "func_signal": "def get_data(data_file, test_file):\n", "code": "raw_data = utilities.read_file(data_file, True)\ntest_data = utilities.read_file(test_file, True)\nx = array(raw_data, float64)\ny = x[:, 0]\nx = x[:, 1 : :]\nx_train, x_cv, y_train, y_cv = cross_validation.train_test_split(\n    x, y, test_size=0.3, random_state=None)\nx = array(test_data, float64)\ny_test = x[:, 0]\nx_test = x[:, 1 : :]\n\nreturn (x_train, y_train, x_cv, y_cv, x_test, y_test)", "path": "fb_suggest_missing_link\\rank.py", "repo_name": "FindBoat/Kaggle", "stars": 86, "license": "None", "language": "python", "size": 17633}
{"docstring": "\"\"\" Ranks the candidates based on the chance they will be followed. \"\"\"\n\n", "func_signal": "def rank_candidates(follow, followed, clf, node, candidates):\n", "code": "if not candidates:\n    return []\n\n# Generates feature matrix.\ncandidates = list(candidates)\nx_candidates = []\nfor candidate in candidates:\n    features = get_features(follow, followed, node, candidate)\n    x_candidates.append(features)\n\n# Uses classifier to estimate probability.\ncandidate_score = {}\nprob = clf.predict_proba(x_candidates)\nfor i in range(len(candidates)):\n    candidate_score[candidates[i]] = prob[i][1]\n\n# Ranks candidates based on the score\nreturn  sorted(candidate_score, key=candidate_score.__getitem__,\n    reverse=True)", "path": "fb_suggest_missing_link\\rank.py", "repo_name": "FindBoat/Kaggle", "stars": 86, "license": "None", "language": "python", "size": 17633}
{"docstring": "\"\"\"\nGet the previous valid month.\n\"\"\"\n", "func_signal": "def get_previous_month(self, date):\n", "code": "first_day, last_day = _month_bounds(date)\nprev = (first_day - datetime.timedelta(days=1)).replace(day=1)\nreturn _get_next_prev_month(self, prev, is_previous=True, use_first_day=True)", "path": "class_based_views\\dates.py", "repo_name": "bfirsh/django-class-based-views", "stars": 79, "license": "None", "language": "python", "size": 146}
{"docstring": "\"\"\"\nTest a view can only be called once.\n\"\"\"\n", "func_signal": "def test_calling_more_than_once(self):\n", "code": "request = self.rf.get('/')\nview = InstanceView.as_view()\nself.assertNotEqual(view(request), view(request))", "path": "class_based_views\\tests\\tests\\base.py", "repo_name": "bfirsh/django-class-based-views", "stars": 79, "license": "None", "language": "python", "size": 146}
{"docstring": "\"\"\"\nGet the next valid day.\n\"\"\"\n", "func_signal": "def get_next_day(self, date):\n", "code": "next = date + datetime.timedelta(days=1)\nreturn _get_next_prev_month(self, next, is_previous=False, use_first_day=False)", "path": "class_based_views\\dates.py", "repo_name": "bfirsh/django-class-based-views", "stars": 79, "license": "None", "language": "python", "size": 146}
{"docstring": "\"\"\"\nGet the queryset to look an objects up against. May not be called if\n`get_dated_items` is overridden.\n\"\"\"\n", "func_signal": "def get_queryset(self):\n", "code": "if self.queryset is None:\n    raise ImproperlyConfigured(u\"%(cls)s is missing a queryset. Define \"\n                               u\"%(cls)s.queryset, or override \"\n                               u\"%(cls)s.get_dated_items().\"\n                               % {'cls': self.__class__.__name__})\nreturn self.queryset._clone()", "path": "class_based_views\\dates.py", "repo_name": "bfirsh/django-class-based-views", "stars": 79, "license": "None", "language": "python", "size": 146}
{"docstring": "\"\"\"\nTest a view which only allows GET doesn't allow other methods.\n\"\"\"\n", "func_signal": "def test_get_only(self):\n", "code": "self._assert_simple(SimpleView.as_view()(self.rf.get('/')))\nself.assertEqual(SimpleView.as_view()(self.rf.post('/')).status_code, 405)\nself.assertEqual(SimpleView.as_view()(\n    self.rf.get('/', REQUEST_METHOD='FAKE')\n).status_code, 405)", "path": "class_based_views\\tests\\tests\\base.py", "repo_name": "bfirsh/django-class-based-views", "stars": 79, "license": "None", "language": "python", "size": 146}
{"docstring": "\"\"\"\nReturn (date_list, items, extra_context) for this request.\n\"\"\"\n", "func_signal": "def get_dated_items(self, year, week):\n", "code": "date_field = self.get_date_field()\ndate = _date_from_string(year, '%Y', '0', '%w', week, '%U')\n\n# Construct a date-range lookup.\nfirst_day = date\nlast_day = date + datetime.timedelta(days=7)\nlookup_kwargs = {\n    '%s__gte' % date_field: first_day,\n    '%s__lt' % date_field: last_day,\n}\n\nallow_future = self.get_allow_future()\nqs = self.get_dated_queryset(allow_future=allow_future, **lookup_kwargs)\n\nreturn (None, qs, {'week': date})", "path": "class_based_views\\dates.py", "repo_name": "bfirsh/django-class-based-views", "stars": 79, "license": "None", "language": "python", "size": 146}
{"docstring": "\"\"\"\nSimilar to parent class, but returns the request object as soon as it\nhas created it.\n\"\"\"\n", "func_signal": "def request(self, **request):\n", "code": "environ = {\n    'HTTP_COOKIE': self.cookies,\n    'PATH_INFO': '/',\n    'QUERY_STRING': '',\n    'REQUEST_METHOD': 'GET',\n    'SCRIPT_NAME': '',\n    'SERVER_NAME': 'testserver',\n    'SERVER_PORT': 80,\n    'SERVER_PROTOCOL': 'HTTP/1.1',\n}\nenviron.update(self.defaults)\nenviron.update(request)\nreturn WSGIRequest(environ)", "path": "class_based_views\\tests\\utils.py", "repo_name": "bfirsh/django-class-based-views", "stars": 79, "license": "None", "language": "python", "size": 146}
{"docstring": "\"\"\"\nGet the name of the date field to be used to filter by.\n\"\"\"\n", "func_signal": "def get_date_field(self):\n", "code": "if self.date_field is None:\n    raise ImproperlyConfigured(u\"%s.date_field is required.\" \n                               % self.__class__.__name__)\nreturn self.date_field", "path": "class_based_views\\dates.py", "repo_name": "bfirsh/django-class-based-views", "stars": 79, "license": "None", "language": "python", "size": 146}
{"docstring": "\"\"\"\nHelper: get a datetime.date object given a format string and a year,\nmonth, and possibly day; raise a 404 for an invalid date.\n\"\"\"\n", "func_signal": "def _date_from_string(year, year_format, month, month_format, day='', day_format='', delim='__'):\n", "code": "format = delim.join((year_format, month_format, day_format))\ndatestr = delim.join((year, month, day))\ntry:\n    return datetime.date(*time.strptime(datestr, format)[:3])\nexcept ValueError:\n    raise Http404(u\"Invalid date string '%s' given format '%s'\" \n                  % (datestr, format))", "path": "class_based_views\\dates.py", "repo_name": "bfirsh/django-class-based-views", "stars": 79, "license": "None", "language": "python", "size": 146}
{"docstring": "\"\"\"\nGet the context for this view.\n\"\"\"\n", "func_signal": "def get_context(self, queryset):\n", "code": "context = {\n    'object_list': queryset,\n}\ntemplate_object_name = self.get_template_object_name(queryset)\nif template_object_name is not None:\n    context[template_object_name] = queryset\nreturn context", "path": "class_based_views\\list.py", "repo_name": "bfirsh/django-class-based-views", "stars": 79, "license": "None", "language": "python", "size": 146}
{"docstring": "\"\"\"\nGet the object this request displays.\n\"\"\"\n", "func_signal": "def get_object(self, year, month, day, pk=None, slug=None):\n", "code": "date = _date_from_string(year, '%Y',\n                         month, self.get_month_format(),\n                         day, self.get_day_format())\n\nqs = self.get_queryset()\n\nif not self.get_allow_future() and date > datetime.date.today():\n    raise Http404(u\"Future %s not available because %s.allow_future is \"\n                  u\"False.\"\n                  % (qs.model._meta.verbose_name_plural, self.__class__.__name__))\n\n# Filter down a queryset from self.queryset using the date from the\n# URL. This'll get passed as the queryset to DetailView.get_object,\n# which'll handle the 404\ndate_field = self.get_date_field()\nfield = qs.model._meta.get_field(date_field)\nlookup = _date_lookup_for_field(field, date)\nqs = qs.filter(**lookup)\n\nreturn super(DateDetailView, self).get_object(pk=pk, slug=slug, queryset=qs)", "path": "class_based_views\\dates.py", "repo_name": "bfirsh/django-class-based-views", "stars": 79, "license": "None", "language": "python", "size": 146}
{"docstring": "\"\"\"\nGet a date list by calling `queryset.dates()`, checking along the way\nfor empty lists that aren't allowed.\n\"\"\"\n", "func_signal": "def get_date_list(self, queryset, date_type):\n", "code": "date_field = self.get_date_field()\nallow_empty = self.get_allow_empty()\n\ndate_list = queryset.dates(date_field, date_type)[::-1]\nif date_list is not None and not date_list and not allow_empty:\n    raise Http404(u\"No %s available\"\n                  % queryset.model._meta.verbose_name_plural)\n\nreturn date_list", "path": "class_based_views\\dates.py", "repo_name": "bfirsh/django-class-based-views", "stars": 79, "license": "None", "language": "python", "size": 146}
{"docstring": "\"\"\"\nGet a queryset properly filtered according to `allow_future` and any\nextra lookup kwargs.\n\"\"\"\n", "func_signal": "def get_dated_queryset(self, allow_future=False, **lookup):\n", "code": "qs = self.get_queryset().filter(**lookup)\ndate_field = self.get_date_field()\nallow_future = allow_future or self.get_allow_future()\nallow_empty = self.get_allow_empty()\n\nif not allow_future:\n    qs = qs.filter(**{'%s__lte' % date_field: datetime.datetime.now()})\n\nif not allow_empty and not qs:\n    raise Http404(u\"No %s available\"\n                  % qs.model._meta.verbose_name_plural)\nreturn qs", "path": "class_based_views\\dates.py", "repo_name": "bfirsh/django-class-based-views", "stars": 79, "license": "None", "language": "python", "size": 146}
{"docstring": "\"\"\"\nGet the context. Must return a Context (or subclass) instance.\n\"\"\"\n", "func_signal": "def get_context(self, items, date_list, context=None):\n", "code": "context = super(DateView, self).get_context(items)\ncontext['date_list'] = date_list\nreturn context", "path": "class_based_views\\dates.py", "repo_name": "bfirsh/django-class-based-views", "stars": 79, "license": "None", "language": "python", "size": 146}
{"docstring": "\"\"\"\nGet the list of items for this view. This must be an interable, and may\nbe a queryset (in which qs-specific behavior will be enabled).\n\"\"\"\n", "func_signal": "def get_queryset(self):\n", "code": "if hasattr(self, 'queryset') and self.queryset is not None:\n    queryset = self.queryset\nelse:\n    raise ImproperlyConfigured(u\"'%s' must define 'queryset'\"\n                               % self.__class__.__name__)\nif hasattr(queryset, '_clone'):\n    queryset = queryset._clone()\nreturn queryset", "path": "class_based_views\\list.py", "repo_name": "bfirsh/django-class-based-views", "stars": 79, "license": "None", "language": "python", "size": 146}
{"docstring": "\"\"\"\nHelper: return the first and last days of the month for the given date.\n\"\"\"\n", "func_signal": "def _month_bounds(date):\n", "code": "first_day = date.replace(day=1)\nif first_day.month == 12:\n    last_day = first_day.replace(year=first_day.year + 1, month=1)\nelse:\n    last_day = first_day.replace(month=first_day.month + 1)\n\nreturn first_day, last_day", "path": "class_based_views\\dates.py", "repo_name": "bfirsh/django-class-based-views", "stars": 79, "license": "None", "language": "python", "size": 146}
{"docstring": "\"\"\"\nGet the next valid month.\n\"\"\"\n", "func_signal": "def get_next_month(self, date):\n", "code": "first_day, last_day = _month_bounds(date)\nnext = (last_day + datetime.timedelta(days=1)).replace(day=1)\nreturn _get_next_prev_month(self, next, is_previous=False, use_first_day=True)", "path": "class_based_views\\dates.py", "repo_name": "bfirsh/django-class-based-views", "stars": 79, "license": "None", "language": "python", "size": 146}
{"docstring": "\"\"\"\nReturn (date_list, items, extra_context) for this request.\n\"\"\"\n", "func_signal": "def get_dated_items(self):\n", "code": "qs = self.get_dated_queryset()\ndate_list = self.get_date_list(qs, 'year')\nnum_latest = self.get_num_latest()\n\nif date_list and num_latest:\n    latest = qs.order_by('-'+self.get_date_field())[:num_latest]\nelse:\n    latest = None\n\nreturn (date_list, latest, {})", "path": "class_based_views\\dates.py", "repo_name": "bfirsh/django-class-based-views", "stars": 79, "license": "None", "language": "python", "size": 146}
{"docstring": "# Create a new book in the future\n", "func_signal": "def test_year_view_allow_future(self):\n", "code": "year = datetime.date.today().year + 1\nb = Book.objects.create(name=\"The New New Testement\", pages=600, pubdate=datetime.date(year, 1, 1))\nres = self.client.get('/dates/books/%s/' % year)\nself.assertEqual(res.status_code, 404)\n\nres = self.client.get('/dates/books/%s/allow_empty/' % year)\nself.assertEqual(res.status_code, 200)\nself.assertEqual(list(res.context['books']), [])\n\nres = self.client.get('/dates/books/%s/allow_future/' % year)\nself.assertEqual(res.status_code, 200)\nself.assertEqual(list(res.context['date_list']), [datetime.datetime(year, 1, 1)])", "path": "class_based_views\\tests\\tests\\dates.py", "repo_name": "bfirsh/django-class-based-views", "stars": 79, "license": "None", "language": "python", "size": 146}
{"docstring": "\"\"\"\nReturn a list of template names to be used for the request. Must return\na list. May not be called if get_template is overridden.\n\"\"\"\n", "func_signal": "def get_template_names(self, items):\n", "code": "return super(DateView, self).get_template_names(\n    items,\n    suffix=self._template_name_suffix\n)", "path": "class_based_views\\dates.py", "repo_name": "bfirsh/django-class-based-views", "stars": 79, "license": "None", "language": "python", "size": 146}
{"docstring": "\"\"\"Returns a single reply.\"\"\"\n", "func_signal": "def get_reply(request, reply_id):\n", "code": "r = Post.query.get(reply_id)\nif r is None or r.is_question:\n    raise NotFound()\nreturn dict(reply=r)", "path": "solace\\views\\api.py", "repo_name": "mitsuhiko/solace", "stars": 64, "license": "other", "language": "python", "size": 952}
{"docstring": "\"\"\"Model signalling\"\"\"\n", "func_signal": "def test_model_signals(self):\n", "code": "from solace.models import User, session\nmodel_changes = []\ndef listen(changes):\n    model_changes.append(changes)\nsignals.after_models_committed.connect(listen)\n\nme = User('A_USER', 'a-user@example.com')\nself.assertEqual(model_changes, [])\nsession.rollback()\nself.assertEqual(model_changes, [])\nme = User('A_USER', 'a-user@example.com')\nself.assertEqual(model_changes, [])\nsession.commit()\nself.assertEqual(model_changes, [[(me, 'insert')]])\ndel model_changes[:]\nsession.delete(me)\nsession.commit()\nself.assertEqual(model_changes, [[(me, 'delete')]])", "path": "solace\\tests\\signals.py", "repo_name": "mitsuhiko/solace", "stars": 64, "license": "other", "language": "python", "size": 952}
{"docstring": "\"\"\"Returns a list of users.  You can retrieve up to 50 users at\nonce.  Each user has the same format as a call to \"get user\".\n\n==== Parameters ====\n\n* {{{limit}}} \u2014 the number of items to load at once.  Defaults to\n                10, maximum allowed number is 50.\n* {{{offset}}} \u2014 the offset of the returned list.  Defaults to 0\n\"\"\"\n", "func_signal": "def list_users(request):\n", "code": "offset = max(0, request.args.get('offset', type=int) or 0)\nlimit = max(0, min(50, request.args.get('limit', 10, type=int)))\nq = User.query.order_by(User.username)\ncount = q.count()\nq = q.limit(limit).offset(offset)\nreturn dict(users=q.all(), total_count=count,\n            limit=limit, offset=offset)", "path": "solace\\views\\api.py", "repo_name": "mitsuhiko/solace", "stars": 64, "license": "other", "language": "python", "size": 952}
{"docstring": "\"\"\"Temporary signal subscriptions\"\"\"\n", "func_signal": "def test_temporary_subscriptions(self):\n", "code": "called = []\nsig = signals.Signal('FOO')\ndef foo():\n    called.append(True)\nsig.emit()\nwith signals.temporary_connection(foo, sig):\n    sig.emit()\nsig.emit()\nself.assertEqual(len(called), 1)", "path": "solace\\tests\\signals.py", "repo_name": "mitsuhiko/solace", "stars": 64, "license": "other", "language": "python", "size": 952}
{"docstring": "\"\"\"Renders a template into a string.\"\"\"\n", "func_signal": "def render_template(template_name, **context):\n", "code": "template = jinja_env.get_template(template_name)\ncontext['request'] = Request.current\ncontext['theme'] = get_theme()\ncontext['auth_system'] = get_auth_system()\nreturn template.render(context)", "path": "solace\\templating.py", "repo_name": "mitsuhiko/solace", "stars": 64, "license": "other", "language": "python", "size": 952}
{"docstring": "\"\"\"Splits up a path into individual components.  If one of the\ncomponents is unsafe on the file system, `None` is returned:\n\n>>> from solace.templating import split_path_safely\n>>> split_path_safely(\"foo/bar/baz\")\n['foo', 'bar', 'baz']\n>>> split_path_safely(\"foo/bar/baz/../meh\")\n>>> split_path_safely(\"/meh/muh\")\n['meh', 'muh']\n>>> split_path_safely(\"/blafasel/.muh\")\n['blafasel', '.muh']\n>>> split_path_safely(\"/blafasel/./x\")\n['blafasel', 'x']\n\"\"\"\n", "func_signal": "def split_path_safely(template):\n", "code": "pieces = []\nfor piece in template.split('/'):\n    if path.sep in piece \\\n       or (path.altsep and path.altsep in piece) or \\\n       piece == path.pardir:\n        return None\n    elif piece and piece != '.':\n        pieces.append(piece)\nreturn pieces", "path": "solace\\templating.py", "repo_name": "mitsuhiko/solace", "stars": 64, "license": "other", "language": "python", "size": 952}
{"docstring": "\"\"\"Generates an ASCII-only slug.\"\"\"\n", "func_signal": "def slugify(text, delim=u'-'):\n", "code": "result = []\nfor word in _punctuation_re.split(text.lower()):\n    word = _punctuation_re.sub(u'', word.encode('translit/long'))\n    if word:\n        result.append(word)\nreturn unicode(delim.join(result))", "path": "solace\\utils\\support.py", "repo_name": "mitsuhiko/solace", "stars": 64, "license": "other", "language": "python", "size": 952}
{"docstring": "\"\"\"Lists all questions or all questions in a section.\"\"\"\n", "func_signal": "def list_questions(request):\n", "code": "q = Topic.query.order_by(Topic.date.desc())\nif request.view_lang is not None:\n    q = q.filter_by(locale=request.view_lang)\noffset = max(0, request.args.get('offset', type=int) or 0)\nlimit = max(0, min(50, request.args.get('limit', 10, type=int)))\ncount = q.count()\nq = q.limit(limit).offset(offset)\nreturn dict(questions=q.all(), total_count=count,\n            limit=limit, offset=offset)", "path": "solace\\views\\api.py", "repo_name": "mitsuhiko/solace", "stars": 64, "license": "other", "language": "python", "size": 952}
{"docstring": "\"\"\"Simple signal subscriptions\"\"\"\n", "func_signal": "def test_simple_subscriptions(self):\n", "code": "sig = signals.Signal('FOO', ('a', 'b'))\nself.assertEqual(repr(sig), 'FOO')\nself.assertEqual(sig.args, ('a', 'b'))\n\ncalled = []\ndef foo(a, b):\n    called.append((a, b))\n\nsig.emit(a=1, b=2)\nself.assertEqual(called, [])\n\nsig.connect(foo)\nsig.emit(a=1, b=2)\nself.assertEqual(called, [(1, 2)])\n\ndel foo\ngc.collect()\n\nsig.emit(a=3, b=4)\nself.assertEqual(called, [(1, 2)])", "path": "solace\\tests\\signals.py", "repo_name": "mitsuhiko/solace", "stars": 64, "license": "other", "language": "python", "size": 952}
{"docstring": "\"\"\"Signal pickling\"\"\"\n", "func_signal": "def test_pickle(self):\n", "code": "x = pickle.loads(pickle.dumps(TEST_SIGNAL))\nself.assert_(x is TEST_SIGNAL)", "path": "solace\\tests\\signals.py", "repo_name": "mitsuhiko/solace", "stars": 64, "license": "other", "language": "python", "size": 952}
{"docstring": "\"\"\"Return a macro from a template.\"\"\"\n", "func_signal": "def get_macro(template_name, macro_name):\n", "code": "template = jinja_env.get_template(template_name)\nreturn getattr(template.module, macro_name)", "path": "solace\\templating.py", "repo_name": "mitsuhiko/solace", "stars": 64, "license": "other", "language": "python", "size": 952}
{"docstring": "\"\"\"Returns a single question and the replies.\"\"\"\n", "func_signal": "def get_question(request, question_id):\n", "code": "t = Topic.query.get(question_id)\nif t is None:\n    raise NotFound()\nreturn dict(question=t, replies=t.replies)", "path": "solace\\views\\api.py", "repo_name": "mitsuhiko/solace", "stars": 64, "license": "other", "language": "python", "size": 952}
{"docstring": "\"\"\"Signal introspection\"\"\"\n", "func_signal": "def test_signal_introspection(self):\n", "code": "sig = signals.Signal('sig')\nself.assertEqual(sig.get_connections(), set())\n\ndef on_foo():\n    pass\nclass Foo(object):\n    def f(self):\n        pass\nf = Foo()\n\nsig.connect(on_foo)\nsig.connect(f.f)\n\nself.assertEqual(sig.get_connections(), set([on_foo, f.f]))\n\nsig.disconnect(on_foo)\nself.assertEqual(sig.get_connections(), set([f.f]))\n\ndel f\ngc.collect()\nself.assertEqual(sig.get_connections(), set())", "path": "solace\\tests\\signals.py", "repo_name": "mitsuhiko/solace", "stars": 64, "license": "other", "language": "python", "size": 952}
{"docstring": "\"\"\"Broadcast signals\"\"\"\n", "func_signal": "def test_broadcasting(self):\n", "code": "on_signal = []\ndef listen(signal, args):\n    on_signal.append((signal, args))\nsignals.broadcast.connect(listen)\n\nsig = signals.Signal('sig', ['foo'])\nsig.emit(foo=42)\n\nself.assertEqual(on_signal, [(sig, {'foo': 42})])", "path": "solace\\tests\\signals.py", "repo_name": "mitsuhiko/solace", "stars": 64, "license": "other", "language": "python", "size": 952}
{"docstring": "\"\"\"Looks up a user by username or user id and returns it.  If the user\nis looked up by id, a plus symbol has to be prefixed to the ID.\n\"\"\"\n", "func_signal": "def get_user(request, username=None, user_id=None):\n", "code": "if username is not None:\n    user = User.query.filter_by(username=username).first()\nelse:\n    user = User.query.get(user_id)\nif user is None:\n    raise NotFound()\nreturn dict(user=user)", "path": "solace\\views\\api.py", "repo_name": "mitsuhiko/solace", "stars": 64, "license": "other", "language": "python", "size": 952}
{"docstring": "\"\"\"After a config change this unloads the theme to refresh it.\"\"\"\n", "func_signal": "def refresh_theme():\n", "code": "global _theme\n_theme = None\n\n# if we have a cache, clear it.  This makes sure that imports no\n# longer point to the old theme's layout files etc.\ncache = jinja_env.cache\nif cache:\n    cache.clear()", "path": "solace\\templating.py", "repo_name": "mitsuhiko/solace", "stars": 64, "license": "other", "language": "python", "size": 952}
{"docstring": "\"\"\"Returns a single badge.\"\"\"\n", "func_signal": "def get_badge(request, identifier):\n", "code": "badge = badges_by_id.get(identifier)\nif badge is None:\n    raise NotFound()\nreturn dict(badge=badge)", "path": "solace\\views\\api.py", "repo_name": "mitsuhiko/solace", "stars": 64, "license": "other", "language": "python", "size": 952}
{"docstring": "\"\"\"Opens a resource from the static folder as fd.\"\"\"\n", "func_signal": "def open_resource(self, filename):\n", "code": "pieces = split_path_safely(filename)\nif pieces is not None:\n    fn = path.join(self.folder, 'static', *pieces)\n    if path.isfile(fn):\n        return open(fn, 'rb')", "path": "solace\\templating.py", "repo_name": "mitsuhiko/solace", "stars": 64, "license": "other", "language": "python", "size": 952}
{"docstring": "\"\"\"Weak method signal subscriptions\"\"\"\n", "func_signal": "def test_weak_method_subscriptions(self):\n", "code": "called = []\nclass Foo(object):\n    def foo(self, a):\n        called.append(a)\nf = Foo()\n\nsig = signals.Signal('FOO', ('a',))\nsig.connect(f.foo)\nsig.emit(a=42)\n\nself.assertEqual(called, [42])\nsig.disconnect(f.foo)\n\ndel f\ngc.collect()\n\nsig.emit(a=23)\nself.assertEqual(called, [42])", "path": "solace\\tests\\signals.py", "repo_name": "mitsuhiko/solace", "stars": 64, "license": "other", "language": "python", "size": 952}
{"docstring": "\"\"\"Returns the specified theme of the one from the config.  If the\ntheme does not exist, `None` is returned.\n\"\"\"\n", "func_signal": "def get_theme(name=None):\n", "code": "global _theme\nset_theme = False\nwith _theme_lock:\n    if name is None:\n        if _theme is not None:\n            return _theme\n        name = settings.THEME\n        set_theme = True\n    for folder in chain(settings.THEME_PATH, DEFAULT_THEME_PATH):\n        theme_dir = path.join(folder, name)\n        if path.isfile(path.join(theme_dir, 'theme.ini')):\n            rv = Theme(theme_dir)\n            if set_theme:\n                _theme = rv\n            return rv", "path": "solace\\templating.py", "repo_name": "mitsuhiko/solace", "stars": 64, "license": "other", "language": "python", "size": 952}
{"docstring": "# print(\"Input: \", end=\"\")\n# print(value)\n", "func_signal": "def hex_converter(self, value):\n", "code": "value = hex(value)\nvalue = value[2:4]\nif (int(value,16) < 16):\n    value = \"0\" + value\n# print(\"Output: \", end=\"\")\n# print(value)\nreturn(value)", "path": "Clients\\Python\\sp_controller\\serialcommunication.py", "repo_name": "jeroendoggen/Arduino-serial-messaging", "stars": 77, "license": "gpl-2.0", "language": "python", "size": 2026}
{"docstring": "\"\"\"TODO: Generate the exit value for the application.\"\"\"\n", "func_signal": "def exit_value(self):\n", "code": "if (self.errors == 0):\n    return 0\nelse:\n    return 42", "path": "Clients\\Python\\sp_controller\\controller.py", "repo_name": "jeroendoggen/Arduino-serial-messaging", "stars": 77, "license": "gpl-2.0", "language": "python", "size": 2026}
{"docstring": "\"\"\"Get the node id\"\"\"\n", "func_signal": "def get_node_id(self, packet):\n", "code": "if packet[3] == 'N':\n    return self.hex_to_dec(packet[4:6])\nelse:\n    return 0", "path": "Clients\\Python\\sp_controller\\serialparser.py", "repo_name": "jeroendoggen/Arduino-serial-messaging", "stars": 77, "license": "gpl-2.0", "language": "python", "size": 2026}
{"docstring": "\"\"\"Print the program flow\"\"\"\n", "func_signal": "def programflow(serial_port):\n", "code": "print (\"\")\nprint (\"Program flow: \")\nprint (\" 1. Connect to the Arduino on port: \" + serial_port)\nprint (\" 2. Start polling for commands\")\nprint (\" 3. Start writing incoming data to the terminal\")\nprint (\" repeat forever... \")\nprint (\"\")", "path": "Clients\\Python\\sp_controller\\infoprinter.py", "repo_name": "jeroendoggen/Arduino-serial-messaging", "stars": 77, "license": "gpl-2.0", "language": "python", "size": 2026}
{"docstring": "\"\"\"Start serial communication thread\"\"\"\n", "func_signal": "def start_serial_communication_threads(self):\n", "code": "self.serial_write_thread.daemon = True\nself.serial_write_thread.start()\nself.serial_read_thread.daemon = True\nself.serial_read_thread.start()", "path": "Clients\\Python\\sp_controller\\controller.py", "repo_name": "jeroendoggen/Arduino-serial-messaging", "stars": 77, "license": "gpl-2.0", "language": "python", "size": 2026}
{"docstring": "\"\"\"Get the payload\"\"\"\n", "func_signal": "def get_payload(self, packet):\n", "code": "if packet[9] == 'P':\n    return self.hex_to_dec(packet[10:12])\nelse:\n    return 0", "path": "Clients\\Python\\sp_controller\\serialparser.py", "repo_name": "jeroendoggen/Arduino-serial-messaging", "stars": 77, "license": "gpl-2.0", "language": "python", "size": 2026}
{"docstring": "\"\"\"Connect to serial port\"\"\"\n", "func_signal": "def serial_connect(self, serial_port, baudrate):\n", "code": "try:\n    self.check_baudrate(baudrate)\n    self.ser = serial.Serial(serial_port, baudrate)\n    self.ser.flush()\n    self.logger.info(self.identification\n                     + \" : Connected to serial port: \"\n                     + serial_port\n                     + \" with speed:\" + str(baudrate))\n    print(self.identification\n          + \" : connected to serial port: \", end=\"\")\n    print(serial_port, end=\"\")\n    print(\" with speed: \", end=\"\")\n    print(baudrate)\nexcept:\n    self.logger.critical(\"Unable to connect to serial port: \"\n                         + serial_port\n                         + \" with speed:\"\n                         + str(baudrate))\n    print(\"Unable to connect to serial port: \", end=\"\")\n    print(serial_port, end=\"\")\n    print(\" with speed: \", end=\"\")\n    print(baudrate)\n    sys.exit(1)", "path": "Clients\\Python\\sp_controller\\serialcommunication.py", "repo_name": "jeroendoggen/Arduino-serial-messaging", "stars": 77, "license": "gpl-2.0", "language": "python", "size": 2026}
{"docstring": "\"\"\"Get the sensor id\"\"\"\n", "func_signal": "def get_sensor_id(self, packet):\n", "code": "if packet[6] == 'I':\n    return self.hex_to_dec(packet[7:9])\nelse:\n    return 0", "path": "Clients\\Python\\sp_controller\\serialparser.py", "repo_name": "jeroendoggen/Arduino-serial-messaging", "stars": 77, "license": "gpl-2.0", "language": "python", "size": 2026}
{"docstring": "\"\"\"Print an overview of the startup info\"\"\"\n", "func_signal": "def startup_info(serial_port):\n", "code": "top()\nprogramflow(serial_port)", "path": "Clients\\Python\\sp_controller\\infoprinter.py", "repo_name": "jeroendoggen/Arduino-serial-messaging", "stars": 77, "license": "gpl-2.0", "language": "python", "size": 2026}
{"docstring": "\"\"\"The main thread for the controller\"\"\"\n", "func_signal": "def run(self):\n", "code": "self.start_serial_communication_threads()\nwhile True:\n    try:\n        time.sleep(2)\n        self.send_command()\n    except KeyboardInterrupt:\n        print(\"Program stopped by keyboard interrupt\")\n        sys.exit(1)", "path": "Clients\\Python\\sp_controller\\controller.py", "repo_name": "jeroendoggen/Arduino-serial-messaging", "stars": 77, "license": "gpl-2.0", "language": "python", "size": 2026}
{"docstring": "\"\"\"Send commands from the queue to the serial ports\"\"\"\n", "func_signal": "def send_commands(self):\n", "code": "if not self.cmd_queue.empty():\n    command = self.cmd_queue.get()\n    #print(command)\n    #self.cmd_queue.join\n    command_id = command[0]\n    payload = command[1]\n    node_id = command[2]\n    self.send_command(command_id, payload, node_id)", "path": "Clients\\Python\\sp_controller\\serialcommunication.py", "repo_name": "jeroendoggen/Arduino-serial-messaging", "stars": 77, "license": "gpl-2.0", "language": "python", "size": 2026}
{"docstring": "\"\"\"This function will be called when the thread starts\"\"\"\n", "func_signal": "def run(self):\n", "code": "while True:\n    time.sleep(self.sleeptime)", "path": "Clients\\Python\\sp_controller\\serialcommunication.py", "repo_name": "jeroendoggen/Arduino-serial-messaging", "stars": 77, "license": "gpl-2.0", "language": "python", "size": 2026}
{"docstring": "\"\"\"Parse a packet\"\"\"\n", "func_signal": "def parse(self, packet):\n", "code": "self.now = datetime.datetime.now()\nif self.validate_packet(packet):\n    if self.get_packet_type(packet) == 0x01:  # \n        return(self.parse_data(packet))\n    if self.get_packet_type(packet) == 0x02:  # COMMAND_REPLY\n        return(self.parse_data(packet))\nelse:\n    return 0", "path": "Clients\\Python\\sp_controller\\serialparser.py", "repo_name": "jeroendoggen/Arduino-serial-messaging", "stars": 77, "license": "gpl-2.0", "language": "python", "size": 2026}
{"docstring": "# Just send some dummy packets (with a counter for the payload)           \n", "func_signal": "def send_command(self):\n", "code": "self.cmd_queue.put((1, self.payload_counter, 2))\nprint(\"A command has been placed in the queue\")\n\nself.payload_counter += 1 \nif (self.payload_counter is 255):\n    self.payload_counter = 0", "path": "Clients\\Python\\sp_controller\\controller.py", "repo_name": "jeroendoggen/Arduino-serial-messaging", "stars": 77, "license": "gpl-2.0", "language": "python", "size": 2026}
{"docstring": "\"\"\"Parse the data\"\"\"\n#        print (self.get_sensor_id(packet),\n#                \" becomes \",\n#                self.converter.serial_to_db(self.get_sensor_id(packet)))\n", "func_signal": "def parse_data(self, packet):\n", "code": "return(self.get_node_id(packet),\n       self.get_sensor_id(packet),\n       self.get_payload(packet),\n       str(self.now))", "path": "Clients\\Python\\sp_controller\\serialparser.py", "repo_name": "jeroendoggen/Arduino-serial-messaging", "stars": 77, "license": "gpl-2.0", "language": "python", "size": 2026}
{"docstring": "\"\"\"Get the packet type\"\"\"\n", "func_signal": "def get_packet_type(self, packet):\n", "code": "if self.validate_packet(packet):\n    if packet[0] == 'T':\n        return self.hex_to_dec(packet[1:3])\n    else:\n        return 0", "path": "Clients\\Python\\sp_controller\\serialparser.py", "repo_name": "jeroendoggen/Arduino-serial-messaging", "stars": 77, "license": "gpl-2.0", "language": "python", "size": 2026}
{"docstring": "\"\"\"Print the top of the cli message\"\"\"\n", "func_signal": "def top():\n", "code": "print (\"\")\ndouble_line()\nprint (\"Starting sp_controller...\")", "path": "Clients\\Python\\sp_controller\\infoprinter.py", "repo_name": "jeroendoggen/Arduino-serial-messaging", "stars": 77, "license": "gpl-2.0", "language": "python", "size": 2026}
{"docstring": "\"\"\"Get one serial packet (one line of data)\"\"\"\n", "func_signal": "def get_packet(self):\n", "code": "packet = self.read_line()[:-1]\nprint(\"Incoming packet: \" + packet)  # Print packet (HEX format)\npacket = self.parser.parse(packet)\nprint(packet)  # Print out all info about the packet (human readable)\nprint(\"\")      # Print empty line\nif packet != 0:\n    self.sensordata_queue.put(packet)", "path": "Clients\\Python\\sp_controller\\serialcommunication.py", "repo_name": "jeroendoggen/Arduino-serial-messaging", "stars": 77, "license": "gpl-2.0", "language": "python", "size": 2026}
{"docstring": "\"\"\"Get the parity 'quality check'\"\"\"\n", "func_signal": "def get_quality_check(self, packet):\n", "code": "if packet[12] == 'Q':\n    return self.hex_to_dec(packet[13:15])\nelse:\n    return 0", "path": "Clients\\Python\\sp_controller\\serialparser.py", "repo_name": "jeroendoggen/Arduino-serial-messaging", "stars": 77, "license": "gpl-2.0", "language": "python", "size": 2026}
{"docstring": "\"\"\"Validate an incoming packet using parity control\"\"\"\n", "func_signal": "def validate_packet(self, packet):\n", "code": "if len(packet) == 15:\n    self.received_parity = self.get_quality_check(packet)\n    self.calculated_parity = (int(packet[1:3], 16)\n              ^ int(packet[4:6], 16)\n              ^ int(packet[7:9], 16)\n              ^ int(packet[10:12], 16))", "path": "Clients\\Python\\sp_controller\\serialparser.py", "repo_name": "jeroendoggen/Arduino-serial-messaging", "stars": 77, "license": "gpl-2.0", "language": "python", "size": 2026}
{"docstring": "\"\"\"\nRestart gunicorn worker processes for the project.\n\"\"\"\n", "func_signal": "def restart():\n", "code": "pid_path = \"%s/gunicorn.pid\" % env.proj_path\nif exists(pid_path):\n    sudo(\"kill -HUP `cat %s`\" % pid_path)\nelse:\n    start_args = (env.proj_name, env.proj_name)\n    sudo(\"supervisorctl start %s:gunicorn_%s\" % start_args)", "path": "fabfile.py", "repo_name": "hypertexthero/fn", "stars": 68, "license": "other", "language": "python", "size": 733}
{"docstring": "\"\"\"\nChecks for changes in the requirements file across an update,\nand gets new requirements if changes have occurred.\n\"\"\"\n", "func_signal": "def update_changed_requirements():\n", "code": "reqs_path = join(env.proj_path, env.reqs_path)\nget_reqs = lambda: run(\"cat %s\" % reqs_path, show=False)\nold_reqs = get_reqs() if env.reqs_path else \"\"\nyield\nif old_reqs:\n    new_reqs = get_reqs()\n    if old_reqs == new_reqs:\n        # Unpinned requirements should always be checked.\n        for req in new_reqs.split(\"\\n\"):\n            if req.startswith(\"-e\"):\n                if \"@\" not in req:\n                    # Editable requirement without pinned commit.\n                    break\n            elif req.strip() and not req.startswith(\"#\"):\n                if not set(\">=<\") & set(req):\n                    # PyPI requirement without version.\n                    break\n        else:\n            # All requirements are pinned.\n            return\n    pip(\"-r %s/%s\" % (env.proj_path, env.reqs_path))", "path": "fabfile.py", "repo_name": "hypertexthero/fn", "stars": 68, "license": "other", "language": "python", "size": 733}
{"docstring": "\"\"\"\nPrompts for the database password if unknown.\n\"\"\"\n", "func_signal": "def db_pass():\n", "code": "if not env.db_pass:\n    env.db_pass = getpass(\"Enter the database password: \")\nreturn env.db_pass", "path": "fabfile.py", "repo_name": "hypertexthero/fn", "stars": 68, "license": "other", "language": "python", "size": 733}
{"docstring": "# Deleting model 'Link'\n", "func_signal": "def backwards(self, orm):\n", "code": "db.delete_table(u'main_link')\n\n# Deleting model 'Profile'\ndb.delete_table(u'main_profile')", "path": "main\\migrations\\0001_initial.py", "repo_name": "hypertexthero/fn", "stars": 68, "license": "other", "language": "python", "size": 733}
{"docstring": "\"\"\"\nReturns the live STATIC_ROOT directory.\n\"\"\"\n", "func_signal": "def static():\n", "code": "return python(\"from django.conf import settings;\"\n              \"print settings.STATIC_ROOT\", show=False).split(\"\\n\")[-1]", "path": "fabfile.py", "repo_name": "hypertexthero/fn", "stars": 68, "license": "other", "language": "python", "size": 733}
{"docstring": "\"\"\"\nRuns commands within the project's directory.\n\"\"\"\n", "func_signal": "def project():\n", "code": "with virtualenv():\n    with cd(env.proj_dirname):\n        yield", "path": "fabfile.py", "repo_name": "hypertexthero/fn", "stars": 68, "license": "other", "language": "python", "size": 733}
{"docstring": "\"\"\"\nReverts project state to the last deploy.\nWhen a deploy is performed, the current state of the project is\nbacked up. This includes the last commit checked out, the database,\nand all static files. Calling rollback will revert all of these to\ntheir state prior to the last deploy.\n\"\"\"\n", "func_signal": "def rollback():\n", "code": "with project():\n    with update_changed_requirements():\n        update = \"git checkout\" if env.git else \"hg up -C\"\n        run(\"%s `cat last.commit`\" % update)\n    with cd(join(static(), \"..\")):\n        run(\"tar -xf %s\" % join(env.proj_path, \"last.tar\"))\n    restore(\"last.db\")\nrestart()", "path": "fabfile.py", "repo_name": "hypertexthero/fn", "stars": 68, "license": "other", "language": "python", "size": 733}
{"docstring": "# Adding model 'Link'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.create_table(u'main_link', (\n    (u'id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n    ('comments_count', self.gf('django.db.models.fields.IntegerField')(default=0)),\n    ('keywords_string', self.gf('django.db.models.fields.CharField')(max_length=500, blank=True)),\n    ('rating_count', self.gf('django.db.models.fields.IntegerField')(default=0)),\n    ('rating_sum', self.gf('django.db.models.fields.IntegerField')(default=0)),\n    ('rating_average', self.gf('django.db.models.fields.FloatField')(default=0)),\n    ('site', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['sites.Site'])),\n    ('title', self.gf('django.db.models.fields.CharField')(max_length=500)),\n    ('slug', self.gf('django.db.models.fields.CharField')(max_length=2000, null=True, blank=True)),\n    ('_meta_title', self.gf('django.db.models.fields.CharField')(max_length=500, null=True, blank=True)),\n    ('description', self.gf('django.db.models.fields.TextField')(blank=True)),\n    ('gen_description', self.gf('django.db.models.fields.BooleanField')(default=True)),\n    ('created', self.gf('django.db.models.fields.DateTimeField')(null=True)),\n    ('updated', self.gf('django.db.models.fields.DateTimeField')(null=True)),\n    ('status', self.gf('django.db.models.fields.IntegerField')(default=2)),\n    ('publish_date', self.gf('django.db.models.fields.DateTimeField')(null=True, blank=True)),\n    ('expiry_date', self.gf('django.db.models.fields.DateTimeField')(null=True, blank=True)),\n    ('short_url', self.gf('django.db.models.fields.URLField')(max_length=200, null=True, blank=True)),\n    ('in_sitemap', self.gf('django.db.models.fields.BooleanField')(default=True)),\n    ('user', self.gf('django.db.models.fields.related.ForeignKey')(related_name='links', to=orm['auth.User'])),\n    ('link', self.gf('django.db.models.fields.URLField')(max_length=200)),\n    ('keywords', self.gf('mezzanine.generic.fields.KeywordsField')(object_id_field='object_pk', to=orm['generic.AssignedKeyword'], frozen_by_south=True)),\n    ('rating', self.gf('mezzanine.generic.fields.RatingField')(object_id_field='object_pk', to=orm['generic.Rating'], frozen_by_south=True)),\n    ('comments', self.gf('mezzanine.generic.fields.CommentsField')(object_id_field='object_pk', to=orm['generic.ThreadedComment'], frozen_by_south=True)),\n))\ndb.send_create_signal(u'main', ['Link'])\n\n# Adding model 'Profile'\ndb.create_table(u'main_profile', (\n    (u'id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n    ('password', self.gf('django.db.models.fields.CharField')(max_length=128)),\n    ('last_login', self.gf('django.db.models.fields.DateTimeField')(default=datetime.datetime.now)),\n    ('user', self.gf('django.db.models.fields.related.OneToOneField')(to=orm['auth.User'], unique=True)),\n    ('website', self.gf('django.db.models.fields.URLField')(max_length=200, blank=True)),\n    ('bio', self.gf('django.db.models.fields.TextField')(blank=True)),\n    ('karma', self.gf('django.db.models.fields.IntegerField')(default=0)),\n    ('is_active', self.gf('django.db.models.fields.BooleanField')(default=True)),\n    ('is_admin', self.gf('django.db.models.fields.BooleanField')(default=False)),\n))\ndb.send_create_signal(u'main', ['Profile'])", "path": "main\\migrations\\0001_initial.py", "repo_name": "hypertexthero/fn", "stars": 68, "license": "other", "language": "python", "size": 733}
{"docstring": "\"\"\"\nRuns a shell comand on the remote server.\n\"\"\"\n", "func_signal": "def run(command, show=True):\n", "code": "if show:\n    print_command(command)\nwith hide(\"running\"):\n    return _run(command)", "path": "fabfile.py", "repo_name": "hypertexthero/fn", "stars": 68, "license": "other", "language": "python", "size": 733}
{"docstring": "\"\"\"\nInstalls the base system and Python requirements for the entire server.\n\"\"\"\n", "func_signal": "def install():\n", "code": "locale = \"LC_ALL=%s\" % env.locale\nwith hide(\"stdout\"):\n    if locale not in sudo(\"cat /etc/default/locale\"):\n        sudo(\"update-locale %s\" % locale)\n        run(\"exit\")\nsudo(\"apt-get update -y -q\")\napt(\"nginx libjpeg-dev python-dev python-setuptools git-core \"\n    \"postgresql libpq-dev memcached supervisor\")\nsudo(\"easy_install pip\")\nsudo(\"pip install virtualenv mercurial\")", "path": "fabfile.py", "repo_name": "hypertexthero/fn", "stars": 68, "license": "other", "language": "python", "size": 733}
{"docstring": "\"\"\"\nEach time a rating is saved, check its value and modify the\nprofile karma for the related object's user accordingly.\nSince ratings are either +1/-1, if a rating is being edited,\nwe can assume that the existing rating is in the other direction,\nso we multiply the karma modifier by 2.\n\"\"\"\n", "func_signal": "def karma(sender, **kwargs):\n", "code": "rating = kwargs[\"instance\"]\nvalue = int(rating.value)\nif not kwargs[\"created\"]:\n    value *= 2\ncontent_object = rating.content_object\nif rating.user != content_object.user:\n    queryset = Profile.objects.filter(user=content_object.user)\n    queryset.update(karma=models.F(\"karma\") + value)", "path": "main\\models.py", "repo_name": "hypertexthero/fn", "stars": 68, "license": "other", "language": "python", "size": 733}
{"docstring": "\"\"\"\nRuns commands within the project's virtualenv.\n\"\"\"\n", "func_signal": "def virtualenv():\n", "code": "with cd(env.venv_path):\n    with prefix(\"source %s/bin/activate\" % env.venv_path):\n        yield", "path": "fabfile.py", "repo_name": "hypertexthero/fn", "stars": 68, "license": "other", "language": "python", "size": 733}
{"docstring": "\"\"\"\nRuns Python code in the project's virtual environment, with Django loaded.\n\"\"\"\n", "func_signal": "def python(code, show=True):\n", "code": "setup = \"import os; os.environ[\\'DJANGO_SETTINGS_MODULE\\']=\\'settings\\';\"\nfull_code = 'python -c \"%s%s\"' % (setup, code.replace(\"`\", \"\\\\\\`\"))\nwith project():\n    result = run(full_code, show=False)\n    if show:\n        print_command(code)\nreturn result", "path": "fabfile.py", "repo_name": "hypertexthero/fn", "stars": 68, "license": "other", "language": "python", "size": 733}
{"docstring": "\"\"\"\nRuns SQL against the project's database.\n\"\"\"\n", "func_signal": "def psql(sql, show=True):\n", "code": "out = postgres('psql -c \"%s\"' % sql)\nif show:\n    print_command(sql)\nreturn out", "path": "fabfile.py", "repo_name": "hypertexthero/fn", "stars": 68, "license": "other", "language": "python", "size": 733}
{"docstring": "\"\"\"\nInstalls one or more Python packages within the virtual environment.\n\"\"\"\n", "func_signal": "def pip(packages):\n", "code": "with virtualenv():\n    return sudo(\"pip install %s\" % packages)", "path": "fabfile.py", "repo_name": "hypertexthero/fn", "stars": 68, "license": "other", "language": "python", "size": 733}
{"docstring": "\"\"\"\nReturns each of the templates with env vars injected.\n\"\"\"\n", "func_signal": "def get_templates():\n", "code": "injected = {}\nfor name, data in templates.items():\n    injected[name] = dict([(k, v % env) for k, v in data.items()])\nreturn injected", "path": "fabfile.py", "repo_name": "hypertexthero/fn", "stars": 68, "license": "other", "language": "python", "size": 733}
{"docstring": "\"\"\"\nBlow away the current project.\n\"\"\"\n", "func_signal": "def remove():\n", "code": "if exists(env.venv_path):\n    sudo(\"rm -rf %s\" % env.venv_path)\nfor template in get_templates().values():\n    remote_path = template[\"remote_path\"]\n    if exists(remote_path):\n        sudo(\"rm %s\" % remote_path)\npsql(\"DROP DATABASE %s;\" % env.proj_name)\npsql(\"DROP USER %s;\" % env.proj_name)", "path": "fabfile.py", "repo_name": "hypertexthero/fn", "stars": 68, "license": "other", "language": "python", "size": 733}
{"docstring": "\"\"\"\nRuns a command as sudo.\n\"\"\"\n", "func_signal": "def sudo(command, show=True):\n", "code": "if show:\n    print_command(command)\nwith hide(\"running\"):\n    return _sudo(command)", "path": "fabfile.py", "repo_name": "hypertexthero/fn", "stars": 68, "license": "other", "language": "python", "size": 733}
{"docstring": "\"\"\"\nInstalls everything required on a new system and deploy.\nFrom the base software, up to the deployed project.\n\"\"\"\n", "func_signal": "def all():\n", "code": "install()\nif create():\n    deploy()", "path": "fabfile.py", "repo_name": "hypertexthero/fn", "stars": 68, "license": "other", "language": "python", "size": 733}
{"docstring": "\"\"\"\nRuns the given command as the postgres user.\n\"\"\"\n", "func_signal": "def postgres(command):\n", "code": "show = not command.startswith(\"psql\")\nreturn run(\"sudo -u root sudo -u postgres %s\" % command, show=show)", "path": "fabfile.py", "repo_name": "hypertexthero/fn", "stars": 68, "license": "other", "language": "python", "size": 733}
{"docstring": "\"\"\"Returns a searcher for the existing index.\"\"\"\n", "func_signal": "def searcher(self):\n", "code": "if not self._searcher:\n    self._searcher = self.index.searcher()\nreturn self._searcher", "path": "src\\whoosh\\writing.py", "repo_name": "tallstreet/Whoosh-AppEngine", "stars": 87, "license": "apache-2.0", "language": "python", "size": 419}
{"docstring": "\"\"\"Adds the contents of another segment to this one. This is used\nto merge existing segments into the new one before deleting them.\n\n:ix: The index.Index object containing the segment to merge.\n:segment: The index.Segment object to merge into this one.\n\"\"\"\n\n", "func_signal": "def add_segment(self, ix, segment):\n", "code": "start_doc = self.max_doc\nhas_deletions = segment.has_deletions()\n\nif has_deletions:\n    doc_map = {}\n\n# Merge document info\ndocnum = 0\nschema = ix.schema\n\ndoc_reader = reading.DocReader(ix.storage, segment, schema)\ntry:\n    vectored_fieldnums = ix.schema.vectored_fields()\n    if vectored_fieldnums:\n        doc_reader._open_vectors()\n        inv = doc_reader.vector_table\n        outv = self.vector_table\n    \n    ds = SegmentWriter.DocumentState(self._scorable_fields)\n    for docnum in xrange(0, segment.max_doc):\n        if not segment.is_deleted(docnum):\n            ds.stored_fields = doc_reader[docnum]\n            ds.field_lengths = doc_reader.doc_field_lengths(docnum)\n            \n            if has_deletions:\n                doc_map[docnum] = self.max_doc\n            \n            for fieldnum in vectored_fieldnums:\n                if (docnum, fieldnum) in inv:\n                    tables.copy_data(inv, (docnum, fieldnum),\n                                     outv, (self.max_doc, fieldnum),\n                                     postings = True)\n            \n            self._write_doc_entry(ds)\n            self.max_doc += 1\n        \n        docnum += 1\n\n    # Add field length totals\n    for fieldnum, total in segment.field_length_totals.iteritems():\n        self.field_length_totals[fieldnum] += total\n\nfinally:\n    doc_reader.close()\n\n# Merge terms\nterm_reader = reading.TermReader(ix.storage, segment, ix.schema)\ntry:\n    for fieldnum, text, _, _ in term_reader:\n        for docnum, data in term_reader.postings(fieldnum, text):\n            if has_deletions:\n                newdoc = doc_map[docnum]\n            else:\n                newdoc = start_doc + docnum\n            \n            self.pool.add_posting(fieldnum, text, newdoc, data)\nfinally:\n    term_reader.close()", "path": "src\\whoosh\\writing.py", "repo_name": "tallstreet/Whoosh-AppEngine", "stars": 87, "license": "apache-2.0", "language": "python", "size": 419}
{"docstring": "\"\"\"Finishes writing the segment (flushes the posting pool out to disk) and\ncloses all open files.\n\"\"\"\n\n", "func_signal": "def close(self):\n", "code": "if self._doc_state.active:\n    raise IndexingError(\"Called SegmentWriter.close() with a document still opened\")\n\nself._flush_pool()\n\nself.doclength_table.close()\n\nself.docs_table.close()\nself.term_table.close()\n\nif self.vector_table:\n    self.vector_table.close()", "path": "src\\whoosh\\writing.py", "repo_name": "tallstreet/Whoosh-AppEngine", "stars": 87, "license": "apache-2.0", "language": "python", "size": 419}
{"docstring": "\"\"\"Encodes a posting as a string, for sorting.\"\"\"\n\n", "func_signal": "def encode_posting(fieldNum, text, doc, data):\n", "code": "return \"\".join([struct.pack(\"!i\", fieldNum),\n                text.encode(\"utf8\"),\n                chr(0),\n                struct.pack(\"!i\", doc),\n                dumps(data)\n                ])", "path": "src\\whoosh\\postpool.py", "repo_name": "tallstreet/Whoosh-AppEngine", "stars": 87, "license": "apache-2.0", "language": "python", "size": 419}
{"docstring": "\"\"\"This policy merges small segments, where small is\ndefined using a heuristic based on the fibonacci sequence.\n\"\"\"\n\n", "func_signal": "def MERGE_SMALL(ix, writer, segments):\n", "code": "newsegments = index.SegmentSet()\nsorted_segment_list = sorted((s.doc_count_all(), s) for s in segments)\ntotal_docs = 0\nfor i, (count, seg) in enumerate(sorted_segment_list):\n    if count > 0:\n        total_docs += count\n        if total_docs < fib(i + 5):\n            writer.add_segment(ix, seg)\n        else:\n            newsegments.append(seg)\nreturn newsegments", "path": "src\\whoosh\\writing.py", "repo_name": "tallstreet/Whoosh-AppEngine", "stars": 87, "license": "apache-2.0", "language": "python", "size": 419}
{"docstring": "\"\"\"This policy merges all existing segments.\n\"\"\"\n", "func_signal": "def OPTIMIZE(ix, writer, segments):\n", "code": "for seg in segments:\n    writer.add_segment(ix, seg)\nreturn index.SegmentSet()", "path": "src\\whoosh\\writing.py", "repo_name": "tallstreet/Whoosh-AppEngine", "stars": 87, "license": "apache-2.0", "language": "python", "size": 419}
{"docstring": "\"\"\"Decodes an encoded posting string into a\n(field_number, text, document_number, data) tuple.\n\"\"\"\n\n", "func_signal": "def decode_posting(posting):\n", "code": "field_num = struct.unpack(\"!i\", posting[:_int_size])[0]\n\nzero = posting.find(chr(0), _int_size)\ntext = posting[_int_size:zero].decode(\"utf8\")\n\ndocstart = zero + 1\ndocend = docstart + _int_size\ndoc = struct.unpack(\"!i\", posting[docstart:docend])[0]\n\ndata = loads(posting[docend:])\n\nreturn field_num, text, doc, data", "path": "src\\whoosh\\postpool.py", "repo_name": "tallstreet/Whoosh-AppEngine", "stars": 87, "license": "apache-2.0", "language": "python", "size": 419}
{"docstring": "# Clears and refills the buffer.\n\n# If this reader is exhausted, do nothing.\n", "func_signal": "def _fill(self):\n", "code": "if self.finished:\n    return\n\n# Clear the buffer.\nbuffer = self.buffer = []\n\n# Reset the index at which the next() method\n# reads from the buffer.\nself.pointer = 0\n\n# How much we've read so far.\nso_far = 0\ncount = self.count\n\nwhile so_far < self.buffer_size:\n    if count <= 0:\n        break\n    p = self.stream.read_string()\n    buffer.append(p)\n    so_far += len(p)\n    count -= 1\n\nself.count = count", "path": "src\\whoosh\\postpool.py", "repo_name": "tallstreet/Whoosh-AppEngine", "stars": 87, "license": "apache-2.0", "language": "python", "size": 419}
{"docstring": "# Iterating the PostingPool object performs a merge sort of\n# the runs that have been written to disk and yields the\n# sorted, decoded postings.\n\n", "func_signal": "def __iter__(self):\n", "code": "if self.finished:\n    raise Exception(\"Tried to iterate on PostingPool twice\")\n\nrun_count = len(self.runs)\nif self.postings and run_count == 0:\n    # Special case: we never accumulated enough postings to flush\n    # to disk, so the postings are still in memory: just yield\n    # them from there.\n    \n    self.postings.sort()\n    for p in self.postings:\n        yield decode_posting(p)\n    return\n\nif not self.postings and run_count == 0:\n    # No postings at all\n    return\n\nif self.postings:\n    self._flush_run()\n    run_count = len(self.runs)\n\n#This method does an external merge to yield postings\n#from the (n > 1) runs built up during indexing and\n#merging.\n\n# Divide up the posting pool's memory limit between the\n# number of runs plus an output buffer.\nmax_chunk_size = int(self.limit / (run_count + 1))\n\nrun_readers = [RunReader(run_file, count, max_chunk_size)\n               for run_file, count in self.runs]\n\nfor decoded_posting in merge(run_readers, max_chunk_size):\n    yield decoded_posting\n\nfor rr in run_readers:\n    assert rr.count == 0\n    rr.close()\n\n# And we're done.\nself.finished = True", "path": "src\\whoosh\\postpool.py", "repo_name": "tallstreet/Whoosh-AppEngine", "stars": 87, "license": "apache-2.0", "language": "python", "size": 419}
{"docstring": "\"\"\"Returns the underlying SegmentWriter object.\"\"\"\n\n", "func_signal": "def segment_writer(self):\n", "code": "if not self._segment_writer:\n    self._segment_writer = SegmentWriter(self.index, self.postlimit,\n                                         self.term_blocksize,\n                                         self.doc_blocksize, self.vector_blocksize)\nreturn self._segment_writer", "path": "src\\whoosh\\writing.py", "repo_name": "tallstreet/Whoosh-AppEngine", "stars": 87, "license": "apache-2.0", "language": "python", "size": 419}
{"docstring": "\"\"\"Returns an index.Segment object for the segment being written.\"\"\"\n", "func_signal": "def segment(self):\n", "code": "return index.Segment(self.name, self.max_doc, self.max_weight,\n                     dict(self.field_length_totals))", "path": "src\\whoosh\\writing.py", "repo_name": "tallstreet/Whoosh-AppEngine", "stars": 87, "license": "apache-2.0", "language": "python", "size": 419}
{"docstring": "#: Whether a document is currently in progress\n", "func_signal": "def reset(self):\n", "code": "self.active = False\n#: Maps field names to stored field contents for this document\nself.stored_fields = {}\n#: Keeps track of the last field that was added\nself.prev_fieldnum = None\n#: Keeps track of field lengths in this document\nself.field_lengths = array(DOCLENGTH_TYPE, [0] * len(self._scorable_fields))", "path": "src\\whoosh\\writing.py", "repo_name": "tallstreet/Whoosh-AppEngine", "stars": 87, "license": "apache-2.0", "language": "python", "size": 419}
{"docstring": "\"\"\"Adds or replaces a document. At least one of the fields for which you\nsupply values must be marked as 'unique' in the index's schema.\n\nThe keyword arguments map field names to the values to index/store.\n\nFor fields that are both indexed and stored, you can specify an alternate\nvalue to store using a keyword argument in the form \"_stored_<fieldname>\".\nFor example, if you have a field named \"title\" and you want to index the\ntext \"a b c\" but store the text \"e f g\", use keyword arguments like this::\n\n    update_document(title=u\"a b c\", _stored_title=u\"e f g\")\n\"\"\"\n\n# Check which of the supplied fields are unique\n", "func_signal": "def update_document(self, **fields):\n", "code": "unique_fields = [name for name, field\n                 in self.index.schema.fields()\n                 if name in fields and field.unique]\nif not unique_fields:\n    raise IndexingError(\"None of the fields in %r are unique\" % fields.keys())\n\n# Delete documents in which the supplied unique fields match\ns = self.searcher()\nfor name in unique_fields:\n    self.delete_by_term(name, fields[name])\n\n# Add the given fields\nself.add_document(**fields)", "path": "src\\whoosh\\writing.py", "repo_name": "tallstreet/Whoosh-AppEngine", "stars": 87, "license": "apache-2.0", "language": "python", "size": 419}
{"docstring": "# Test update with multiple unique keys\n", "func_signal": "def test_update(self):\n", "code": "SAMPLE_DOCS = [{\"id\": u\"test1\", \"path\": u\"/test/1\", \"text\": u\"Hello\"},\n               {\"id\": u\"test2\", \"path\": u\"/test/2\", \"text\": u\"There\"},\n               {\"id\": u\"test3\", \"path\": u\"/test/3\", \"text\": u\"Reader\"},\n               ]\n\nschema = fields.Schema(id=fields.ID(unique=True, stored=True),\n                       path=fields.ID(unique=True, stored=True),\n                       text=fields.TEXT)\nix = self.make_index(\"testindex\", schema)\ntry:\n    writer = ix.writer()\n    for doc in SAMPLE_DOCS:\n        writer.add_document(**doc)\n    writer.commit()\n    \n    writer = ix.writer()\n    writer.update_document(**{\"id\": u\"test2\",\n                              \"path\": u\"test/1\",\n                              \"text\": u\"Replacement\"})\n    writer.commit()\n    ix.close()\nfinally:\n    self.destroy_index(\"testindex\")", "path": "tests\\test_indexing.py", "repo_name": "tallstreet/Whoosh-AppEngine", "stars": 87, "license": "apache-2.0", "language": "python", "size": 419}
{"docstring": "# This method pulls postings out of the posting pool (built up\n# as documents are added) and writes them to the posting file.\n# Each time it encounters a posting for a new term, it writes\n# the previous term to the term index (by waiting to write the\n# term entry, we can easily count the document frequency and\n# sum the terms by looking at the postings).\n\n", "func_signal": "def _flush_pool(self):\n", "code": "term_table = self.term_table\n\nwrite_posting_method = None\ncurrent_fieldnum = None # Field number of the current term\ncurrent_text = None # Text of the current term\nfirst = True\ncurrent_weight = 0\n\n# Loop through the postings in the pool.\n# Postings always come out of the pool in field number/alphabetic order.\nfor fieldnum, text, docnum, data in self.pool:\n    # If we're starting a new term, reset everything\n    if write_posting_method is None or fieldnum > current_fieldnum or text > current_text:\n        if fieldnum != current_fieldnum:\n            write_posting_method = self.schema.field_by_number(fieldnum).format.write_postvalue\n        \n        # If we've already written at least one posting, write the\n        # previous term to the index.\n        if not first:\n            term_table.add_row((current_fieldnum, current_text), current_weight)\n            \n            if current_weight > self.max_weight:\n                self.max_weight = current_weight\n        \n        # Reset term variables\n        current_fieldnum = fieldnum\n        current_text = text\n        current_weight = 0\n        first = False\n    \n    elif fieldnum < current_fieldnum or (fieldnum == current_fieldnum and text < current_text):\n        # This should never happen!\n        raise Exception(\"Postings are out of order: %s:%s .. %s:%s\" %\n                        (current_fieldnum, current_text, fieldnum, text))\n    \n    current_weight += term_table.write_posting(docnum, data, write_posting_method)\n\n# Finish up the last term\nif not first:\n    term_table.add_row((current_fieldnum, current_text), current_weight)\n    if current_weight > self.max_weight:\n        self.max_weight = current_weight", "path": "src\\whoosh\\writing.py", "repo_name": "tallstreet/Whoosh-AppEngine", "stars": 87, "license": "apache-2.0", "language": "python", "size": 419}
{"docstring": "\"\"\"Finishes writing and unlocks the index.\n\n:mergetype: How to merge existing segments. One of\n    writing.NO_MERGE, writing.MERGE_SMALL, or writing.OPTIMIZE.\n\"\"\"\n\n", "func_signal": "def commit(self, mergetype = MERGE_SMALL):\n", "code": "self._close_searcher()\nif self._segment_writer or mergetype is OPTIMIZE:\n    self._merge_segments(mergetype)\nself.index.commit(self.segments)\nself._finish()", "path": "src\\whoosh\\writing.py", "repo_name": "tallstreet/Whoosh-AppEngine", "stars": 87, "license": "apache-2.0", "language": "python", "size": 419}
{"docstring": "\"\"\"\n:stream: the file from which to read.\n:count: the number of postings in the stream.\n:buffer_size: the size (in bytes) of the read buffer to use.\n\"\"\"\n\n", "func_signal": "def __init__(self, stream, count, buffer_size):\n", "code": "self.stream = stream\nself.count = count\nself.buffer_size = buffer_size\n\nself.buffer = []\nself.pointer = 0\nself.finished = False", "path": "src\\whoosh\\postpool.py", "repo_name": "tallstreet/Whoosh-AppEngine", "stars": 87, "license": "apache-2.0", "language": "python", "size": 419}
{"docstring": "# Initialize a list of terms we're \"current\"ly\n# looking at, by taking the first posting from\n# each buffer.\n#\n# The format of the list is\n# [(\"encoded_posting\", reader_number), ...]\n#\n# The list is sorted, and the runs are already\n# sorted, so the first term in this list should\n# be the absolute \"lowest\" term.\n\n", "func_signal": "def merge(run_readers, max_chunk_size):\n", "code": "current = [(r.next(), i) for i, r\n           in enumerate(run_readers)]\nheapify(current)\n\n# The number of active readers (readers with more\n# postings to available), initially equal\n# to the total number of readers/buffers.\n\nactive = len(run_readers)\n\n# Initialize the output buffer, and a variable to\n# keep track of the output buffer size. This buffer\n# accumulates postings from the various buffers in\n# proper sorted order.\n\noutput = []\noutputBufferSize = 0\n\nwhile active > 0:\n    # Get the first (\"encoded_posting\", reader_number)\n    # pair and add it to the output buffer.\n    \n    p, i = current[0]\n    output.append(p)\n    outputBufferSize += len(p)\n    \n    # If the output buffer is full, \"flush\" it by yielding\n    # the accumulated postings back to the parent writer\n    # and clearing the output buffer.\n    \n    if outputBufferSize > max_chunk_size:\n        for p in output:\n            yield decode_posting(p)\n        output = []\n        outputBufferSize = 0\n    \n    # We need to replace the posting we just added to the output\n    # by getting the next posting from the same buffer.\n    \n    if run_readers[i] is not None:\n        # Take the first posting from buffer i and insert it into the\n        # \"current\" list in sorted order.\n        # The current list must always stay sorted, so the first item\n        # is always the lowest.\n        \n        p = run_readers[i].next()\n        if p:\n            heapreplace(current, (p, i))\n        else:\n            heappop(current)\n            active -= 1\n\n# If there are still terms in the \"current\" list after all the\n# readers are empty, dump them into the output buffer.\n\nif len(current) > 0:\n    output.extend([p for p, i in current])\n\n# If there's still postings in the output buffer, yield\n# them all to the parent writer.\n\nif len(output) > 0:\n    for p in output:\n        yield decode_posting(p)", "path": "src\\whoosh\\postpool.py", "repo_name": "tallstreet/Whoosh-AppEngine", "stars": 87, "license": "apache-2.0", "language": "python", "size": 419}
{"docstring": "\"\"\"\n:limit: the maximum amount of memory to use at once\n    for adding postings and the merge sort.\n\"\"\"\n\n", "func_signal": "def __init__(self, limit):\n", "code": "self.limit = limit\nself.size = 0\nself.postings = []\nself.finished = False\n\nself.runs = []\nself.count = 0", "path": "src\\whoosh\\postpool.py", "repo_name": "tallstreet/Whoosh-AppEngine", "stars": 87, "license": "apache-2.0", "language": "python", "size": 419}
{"docstring": "\"\"\"Adds the contents of another Index object to this segment.\nThis currently does NO checking of whether the schemas match up.\n\"\"\"\n\n", "func_signal": "def add_index(self, other_ix):\n", "code": "for seg in other_ix.segments:\n    self.add_segment(other_ix, seg)", "path": "src\\whoosh\\writing.py", "repo_name": "tallstreet/Whoosh-AppEngine", "stars": 87, "license": "apache-2.0", "language": "python", "size": 419}
{"docstring": "# test that heartbeats go out\n", "func_signal": "def test_handle_message_3(server, monkeypatch):\n", "code": "server, _, _ = server\nmsg = dict(term=27, id='uuid', type='ae')\nsa = Mock()\nmonkeypatch.setattr(server, 'send_ae', sa)\nserver.role = 'leader'\nserver.last_update = 0\nrpc = arbrpc(**msg)\nserver.handle_message(rpc, None)\nassert sa.called == True", "path": "tests\\unit\\test_server.py", "repo_name": "kurin/py-raft", "stars": 91, "license": "unlicense", "language": "python", "size": 485}
{"docstring": "# each log is recorded\n", "func_signal": "def test_handle_msg_follower_ae5(server):\n", "code": "server, _, _ = server\nlogs = {\n        34: dict(index=34, term=27, committed=True, msgid='one', msg={}),\n        35: dict(index=35, term=27, committed=True, msgid='two', msg={}),\n        36: dict(index=36, term=27, committed=True, msgid='three', msg={}),\n        37: dict(index=37, term=27, committed=True, msgid='four', msg={}),\n    }\nmsg = dict(term=27, id='otherobj', entries=logs,\n           previdx=33, prevterm=26, commitidx=5)\nserver.log.add = a = Mock()\nserver.handle_msg_follower_ae(msg)\nfor l in logs:\n    a.assert_any_call(logs[l])", "path": "tests\\unit\\test_server.py", "repo_name": "kurin/py-raft", "stars": 91, "license": "unlicense", "language": "python", "size": 485}
{"docstring": "# return if bad uuid\n", "func_signal": "def test_handle_msg_follower_ae0(server):\n", "code": "server, _, _ = server\nmsg = dict(id='randomobj')\nassert server.handle_msg_follower_ae(msg) == None", "path": "tests\\unit\\test_server.py", "repo_name": "kurin/py-raft", "stars": 91, "license": "unlicense", "language": "python", "size": 485}
{"docstring": "# client response RPC\n# qid = query id, ans is arbitrary data\n", "func_signal": "def cr_rpc_ack(self, qid, info=None):\n", "code": "rpc = {\n    'type': 'cr_ack',\n    'id': qid,\n    'info': info\n}\nreturn msgpack.packb(rpc)", "path": "raft\\server.py", "repo_name": "kurin/py-raft", "stars": 91, "license": "unlicense", "language": "python", "size": 485}
{"docstring": "# client query rpc\n", "func_signal": "def cq_rpc(self, data, msgid):\n", "code": "rpc = {\n    'type': 'cq',\n    'id': msgid,\n    'data': data\n}\nreturn msgpack.packb(rpc)", "path": "raft\\client.py", "repo_name": "kurin/py-raft", "stars": 91, "license": "unlicense", "language": "python", "size": 485}
{"docstring": "# commitidx is handled correctly\n", "func_signal": "def test_handle_msg_follower_ae4(server):\n", "code": "server, _, _ = server\nmsg = dict(term=27, id='otherobj', entries=[],\n           previdx=32, prevterm=25, commitidx=5)\nserver.commitidx = 8\nserver.log.force_commit = fc = Mock()\nserver.handle_msg_follower_ae(msg)\nassert fc.called == False\nmsg['commitidx'] = 12\nserver.handle_msg_follower_ae(msg)\nfc.assert_called_with(12)", "path": "tests\\unit\\test_server.py", "repo_name": "kurin/py-raft", "stars": 91, "license": "unlicense", "language": "python", "size": 485}
{"docstring": "# we (a follower) just learned that one or more\n# logs were committed, *and* we are in the middle of an\n# update.  check to see if that was phase 2 of the update,\n# and remove old hosts if so\n", "func_signal": "def check_update_committed(self):\n", "code": "umsg = self.log.get_by_uuid(self.update_uuid)\nif umsg['index'] > self.commitidx:\n    # isn't yet committed\n    return\ndata = umsg['msg']\nif data['phase'] == 2:\n    self.oldpeers = None\n    self.update_uuid = None\n    if not self.uuid in self.all_peers():\n        self.running = False", "path": "raft\\server.py", "repo_name": "kurin/py-raft", "stars": 91, "license": "unlicense", "language": "python", "size": 485}
{"docstring": "# return if term < current term\n# we test that by making sure that the\n# server.last_update variable is never updated\n", "func_signal": "def test_handle_msg_follower_ae1(server):\n", "code": "server, _, _ = server\nserver.last_update = None\nmsg = dict(term=25, id='otherobj', entries=[],\n           previdx=32, prevterm=25, commitidx=5)\nserver.handle_msg_follower_ae(msg)\nassert server.last_update == None", "path": "tests\\unit\\test_server.py", "repo_name": "kurin/py-raft", "stars": 91, "license": "unlicense", "language": "python", "size": 485}
{"docstring": "# client response RPC\n# qid = query id, ans is arbitrary data\n# if the qid is None, we make one up and\n# return it when we ack it\n", "func_signal": "def cr_rpc(self, qid, ans):\n", "code": "rpc = {\n    'type': 'cr',\n    'id': qid,\n    'data': ans\n}\nreturn msgpack.packb(rpc)", "path": "raft\\server.py", "repo_name": "kurin/py-raft", "stars": 91, "license": "unlicense", "language": "python", "size": 485}
{"docstring": "# someone else was elected during our candidacy\n", "func_signal": "def handle_msg_candidate_ae(self, msg):\n", "code": "term = msg['term']\nuuid = msg['id']\nif not self.valid_peer(uuid):\n    return\nif term < self.term:\n    # illegitimate, toss it\n    return\nself.role = 'follower'\nself.handle_msg_follower_ae(msg)", "path": "raft\\server.py", "repo_name": "kurin/py-raft", "stars": 91, "license": "unlicense", "language": "python", "size": 485}
{"docstring": "# test that failures decease our msg index\n", "func_signal": "def test_handle_msg_leader_ae_reply_1(server, monkeypatch):\n", "code": "server, _, _ = server\nserver.next_index = {}\nserver.next_index['otherobj'] = 12\nmsg = dict(id='otherobj', success=False, index=12)\nserver.handle_msg_leader_ae_reply(msg)\nassert server.next_index['otherobj'] == 11", "path": "tests\\unit\\test_server.py", "repo_name": "kurin/py-raft", "stars": 91, "license": "unlicense", "language": "python", "size": 485}
{"docstring": "# we're in an update; see if the update msg\n# has committed, and go to phase 2 or finish\n", "func_signal": "def possible_update_commit(self):\n", "code": "if not self.log.is_committed_by_uuid(self.update_uuid):\n    # it hasn't\n    return\numsg = self.log.get_by_uuid(self.update_uuid)\ndata = copy.deepcopy(umsg['msg'])\nif data['phase'] == 1 and self.newpeers:\n    # the *first* phase of the update has been committed\n    # new leaders are guaranteed to be in the union of the\n    # old and new configs.  now update the configuration\n    # to the new one only.\n    data['phase'] = 2\n    newid = uuid.uuid4().hex\n    self.update_uuid = newid\n    data['id'] = newid\n    self.oldpeers = self.peers\n    self.peers = self.newpeers\n    self.newpeers = None\n    logentry = log.logentry(self.term, newid, data)\n    self.log.add(logentry)\nelse:\n    # the *second* phase is now committed.  tell all our\n    # current peers about the successful commit, drop\n    # the old config entirely and, if necessary, step down\n    self.send_ae()  # send this to peers who might be about to dispeer\n    self.oldpeers = None\n    self.update_uuid = None\n    if not self.uuid in self.peers:\n        self.running = False", "path": "raft\\server.py", "repo_name": "kurin/py-raft", "stars": 91, "license": "unlicense", "language": "python", "size": 485}
{"docstring": "# got a new message\n# update our term if applicable, and dispatch the message\n# to the appropriate handler.  finally, if we are still\n# (or have become) the leader, send out heartbeats\n", "func_signal": "def handle_message(self, msg, addr):\n", "code": "try:\n    msg = msgpack.unpackb(msg, use_list=False, encoding='utf-8')\nexcept msgpack.UnpackException:\n    return\nmtype = msg['type']\nterm = msg.get('term', None)\nmsg['src'] = addr\nuuid = msg.get('id', None)\n# no matter what, if our term is old, update and step down\nif term and term > self.term and self.valid_peer(uuid):\n    # okay, well, only if it's from a valid source\n    self.term = term\n    self.voted = None\n    self.role = 'follower'\nmname = 'handle_msg_%s_%s' % (self.role, mtype)\nif hasattr(self, mname):\n    getattr(self, mname)(msg)\nif self.role == 'leader' and time.time() - self.last_update > 0.3:\n    self.send_ae()", "path": "raft\\server.py", "repo_name": "kurin/py-raft", "stars": 91, "license": "unlicense", "language": "python", "size": 485}
{"docstring": "# bootstrap packets solve the problem of how we find the\n# id of our peers.  we don't want to have to copy uuids around\n# when they could just mail them to each other.\n", "func_signal": "def handle_msg_follower_bootstrap(self, msg):\n", "code": "print(msg)\nprint(self.peers)", "path": "raft\\server.py", "repo_name": "kurin/py-raft", "stars": 91, "license": "unlicense", "language": "python", "size": 485}
{"docstring": "# test that terms get updated correctly\n", "func_signal": "def test_handle_message_2(server):\n", "code": "server, _, _ = server\nassert server.role == 'follower'\n# first, send a message that should *not* update anything\nmsg = dict(term=29, id='uuid', type='null')\nrpc = arbrpc(**msg)\nserver.handle_message(rpc, None)\nassert server.term == 27\n# okay, now send a message that should update\nserver.role = 'leader'\nmsg['id'] = 'otherobj'\nrpc = arbrpc(**msg)\nserver.handle_message(rpc, None)\nassert server.term == 29\nassert server.voted == None\nassert server.role == 'follower'", "path": "tests\\unit\\test_server.py", "repo_name": "kurin/py-raft", "stars": 91, "license": "unlicense", "language": "python", "size": 485}
{"docstring": "# failure when log entries are bad\n", "func_signal": "def test_handle_msg_follower_ae3(server):\n", "code": "server, _, _ = server\nmsg = dict(term=27, id='otherobj', entries=[],\n           previdx=32, prevterm=27, commitidx=5)\nrpc = server.ae_rpc_reply(32, 27, False)\nserver.send_to_peer = stp = Mock()\nserver.handle_msg_follower_ae(msg)\nstp.assert_called_with(rpc, 'otherobj')", "path": "tests\\unit\\test_server.py", "repo_name": "kurin/py-raft", "stars": 91, "license": "unlicense", "language": "python", "size": 485}
{"docstring": "# we are a leader who has received an ae ack\n# if the update was rejected, it's because the follower\n# has an incorrect log entry, so send an update for that\n# log entry as well\n# if the update succeeded, record that in the log and,\n# if the log has been recorded by enough followers, mark\n# it committed.\n", "func_signal": "def handle_msg_leader_ae_reply(self, msg):\n", "code": "uuid = msg['id']\nif not self.valid_peer(uuid):\n    return\nsuccess = msg['success']\nindex = msg['index']\nif success:\n    self.next_index[uuid] = index\n    if self.log.get_commit_index() < index:\n        self.msg_recorded(msg)\nelse:\n    # exponentially reduce the index for peers\n    # this way if they're only missing a couple log entries,\n    # we only have to send 2 or 4, but if they're missing\n    # a couple thousand we'll find out in less than 2k round\n    # trips\n    oldidx = self.next_index.get(uuid, 0)\n    diff = self.log.maxindex() - oldidx\n    diff = max(diff, 1)\n    oldidx -= diff\n    self.next_index[uuid] = max(oldidx, 0)", "path": "raft\\server.py", "repo_name": "kurin/py-raft", "stars": 91, "license": "unlicense", "language": "python", "size": 485}
{"docstring": "# we're a leader and we just got an ack from\n# a follower who might have been the one to\n# commit an entry\n", "func_signal": "def msg_recorded(self, msg):\n", "code": "term = msg['term']\nindex = msg['index']\nuuid = msg['id']\nself.log.add_ack(index, term, uuid)\nif self.log.num_acked(index) >= self.quorum() and term == self.term:\n    self.log.commit(index, term)\n    assert index >= self.commitidx\n    oldidx = self.commitidx\n    self.commitidx = index\n    if self.update_uuid:\n        # if there's an update going on, see if our commit\n        # is actionable\n        self.possible_update_commit()\n    # otherwise just see what messages are now runnable\n    self.run_committed_messages(oldidx)", "path": "raft\\server.py", "repo_name": "kurin/py-raft", "stars": 91, "license": "unlicense", "language": "python", "size": 485}
{"docstring": "# don't vote for a different candidate!\n", "func_signal": "def handle_msg_candidate_rv(self, msg):\n", "code": "uuid = msg['id']\nif self.uuid == uuid:\n    # huh\n    return\nif not self.valid_peer(uuid):\n    return\nrpc = self.rv_rpc_reply(False)\nself.send_to_peer(rpc, uuid)", "path": "raft\\server.py", "repo_name": "kurin/py-raft", "stars": 91, "license": "unlicense", "language": "python", "size": 485}
{"docstring": "# bad uuids get rejected\n", "func_signal": "def test_handle_msg_leader_ae_reply_0(server):\n", "code": "server, _, _ = server\nmsg = dict(id='badid')\nassert server.handle_msg_leader_ae_reply(msg) == None\nmsg['id'] = 'otherobj'\nwith pytest.raises(KeyError):\n    server.handle_msg_leader_ae_reply(msg)", "path": "tests\\unit\\test_server.py", "repo_name": "kurin/py-raft", "stars": 91, "license": "unlicense", "language": "python", "size": 485}
{"docstring": "\"\"\"\nGroup by `column` and run `aggregate` function on `chooser` column.\n\"\"\"\n", "func_signal": "def uniquify_by(self, column, chooser=None, aggregate='MAX'):\n", "code": "self.group_by.append(column)\nif chooser:\n    i = self.columns.index(chooser)\n    self.columns[i] = '{0}({1})'.format(aggregate, self.columns[i])", "path": "rash\\utils\\sqlconstructor.py", "repo_name": "tkf/rash", "stars": 83, "license": "gpl-3.0", "language": "python", "size": 614}
{"docstring": "\"\"\"\nJoin `source`.\n\n>>> sc = SQLConstructor('main', ['c1', 'c2'])\n>>> sc.join('sub', 'JOIN', 'main.id = sub.id')\n>>> (sql, params, keys) = sc.compile()\n>>> sql\n'SELECT c1, c2 FROM main JOIN sub ON main.id = sub.id'\n\nIt is possible to pass another `SQLConstructor` as a source.\n\n>>> sc = SQLConstructor('main', ['c1', 'c2'])\n>>> sc.add_or_matches('{0} = {1}', 'c1', [111])\n>>> subsc = SQLConstructor('sub', ['d1', 'd2'])\n>>> subsc.add_or_matches('{0} = {1}', 'd1', ['abc'])\n>>> sc.join(subsc, 'JOIN', 'main.id = sub.id')\n>>> sc.add_column('d1')\n>>> (sql, params, keys) = sc.compile()\n>>> print(sql)                     # doctest: +NORMALIZE_WHITESPACE\nSELECT c1, c2, d1 FROM main\nJOIN ( SELECT d1, d2 FROM sub WHERE (d1 = ?) )\nON main.id = sub.id\nWHERE (c1 = ?)\n\n`params` is set appropriately to include parameters for joined\nsource:\n\n>>> params\n['abc', 111]\n\nNote that `subsc.compile` is called when `sc.join(subsc, ...)`\nis called.  Therefore, calling `subsc.add_<predicate>` does not\neffect `sc`.\n\n:type source: str or SQLConstructor\n:arg  source: table\n:type     op: str\n:arg      op: operation (e.g., 'JOIN')\n:type     on: str\n:arg      on: on clause.  `source` (\"right\" source) can be\n              referred using `{r}` formatting field.\n\n\"\"\"\n", "func_signal": "def join(self, source, op='LEFT JOIN', on=''):\n", "code": "if isinstance(source, SQLConstructor):\n    (sql, params, _) = source.compile()\n    self.join_params.extend(params)\n    jsrc = '( {0} )'.format(sql)\n    if source.table_alias:\n        jsrc += ' AS ' + source.table_alias\n        on = on.format(r=source.table_alias)\nelse:\n    jsrc = source\n    on = on.format(r=source)\nconstraint = 'ON {0}'.format(on) if on else ''\nself.join_source = ' '.join([self.join_source, op, jsrc, constraint])", "path": "rash\\utils\\sqlconstructor.py", "repo_name": "tkf/rash", "stars": 83, "license": "gpl-3.0", "language": "python", "size": 614}
{"docstring": "\"\"\"\nConcatenate `conditions` with `operator` and wrap it by ().\n\nIt returns a string in a list or empty list, if `conditions` is empty.\n\n\"\"\"\n", "func_signal": "def concat_expr(operator, conditions):\n", "code": "expr = \" {0} \".format(operator).join(conditions)\nreturn [\"({0})\".format(expr)] if expr else []", "path": "rash\\utils\\sqlconstructor.py", "repo_name": "tkf/rash", "stars": 83, "license": "gpl-3.0", "language": "python", "size": 614}
{"docstring": "\"\"\"\nGenerate session ID based on HOST, TTY, PID [#]_ and start time.\n\n:type data: dict\n:rtype: str\n\n.. [#] PID of the shell, i.e., PPID of this Python process.\n\n\"\"\"\n", "func_signal": "def generate_session_id(data):\n", "code": "host = data['environ']['HOST']\ntty = data['environ'].get('TTY') or 'NO_TTY'\nreturn ':'.join(map(str, [\n    host, tty, os.getppid(), data['start']]))", "path": "rash\\record.py", "repo_name": "tkf/rash", "stars": 83, "license": "gpl-3.0", "language": "python", "size": 614}
{"docstring": "\"\"\"\nReturn elements in `iterative` including `num` before and after elements.\n\n>>> ''.join(include_context(lambda x: x == '!', 2, 'bb!aa__bb!aa'))\n'bb!aabb!aa'\n\n\"\"\"\n", "func_signal": "def include_context(predicate, num, iterative):\n", "code": "(it0, it1, it2) = itertools.tee(iterative, 3)\npsf = _forward_shifted_predicate(predicate, num, it1)\npsb = _backward_shifted_predicate(predicate, num, it2)\nreturn (e for (e, pf, pb) in zip(it0, psf, psb) if pf or pb)", "path": "rash\\utils\\iterutils.py", "repo_name": "tkf/rash", "stars": 83, "license": "gpl-3.0", "language": "python", "size": 614}
{"docstring": "\"\"\"\nMake directory at `path` if it does not exist.\n\"\"\"\n", "func_signal": "def mkdirp(path):\n", "code": "if not os.path.isdir(path):\n    os.makedirs(path)", "path": "rash\\utils\\pathutils.py", "repo_name": "tkf/rash", "stars": 83, "license": "gpl-3.0", "language": "python", "size": 614}
{"docstring": "\"\"\"\nReturn required fields in `format_string`.\n\n>>> sorted(formatter_keys('{1} {key}'))\n['1', 'key']\n\n\"\"\"\n", "func_signal": "def formatter_keys(format_string):\n", "code": "from string import Formatter\nreturn (tp[1] for tp in Formatter().parse(format_string))", "path": "rash\\search.py", "repo_name": "tkf/rash", "stars": 83, "license": "gpl-3.0", "language": "python", "size": 614}
{"docstring": "\"\"\"\nReturn \\\"os.ttyname(0 or 1 or 2)\\\".\n\"\"\"\n", "func_signal": "def get_tty():\n", "code": "for i in range(3):\n    try:\n        return os.ttyname(i)\n        break\n    except OSError:\n        pass", "path": "rash\\record.py", "repo_name": "tkf/rash", "stars": 83, "license": "gpl-3.0", "language": "python", "size": 614}
{"docstring": "\"\"\"\nAdd OR conditions to match to `params`.  See `add_and_matches`.\n\"\"\"\n", "func_signal": "def add_or_matches(self, matcher, lhs, params, numq=1, flatten=None):\n", "code": "params = self._adapt_params(params)\nqs = ['?'] * numq\nflatten = flatten or self._default_flatten(numq)\nexpr = repeat(adapt_matcher(matcher)(lhs, *qs), len(params))\nself.conditions.extend(concat_expr('OR', expr))\nself.params.extend(flatten(params))", "path": "rash\\utils\\sqlconstructor.py", "repo_name": "tkf/rash", "stars": 83, "license": "gpl-3.0", "language": "python", "size": 614}
{"docstring": "\"\"\"\nGet environment variables from :data:`os.environ`.\n\n:type keys: [str]\n:rtype: dict\n\nSome additional features.\n\n* If 'HOST' is not in :data:`os.environ`, this function\n  automatically fetch it using :meth:`platform.node`.\n* If 'TTY' is not in :data:`os.environ`, this function\n  automatically fetch it using :meth:`os.ttyname`.\n* Set 'RASH_SPENV_TERMINAL' if needed.\n\n\"\"\"\n", "func_signal": "def get_environ(keys):\n", "code": "items = ((k, os.environ.get(k)) for k in keys)\nsubenv = dict((k, v) for (k, v) in items if v is not None)\nneedset = lambda k: k in keys and not subenv.get(k)\n\ndef setifnonempty(key, value):\n    if value:\n        subenv[key] = value\n\nif needset('HOST'):\n    import platform\n    subenv['HOST'] = platform.node()\nif needset('TTY'):\n    setifnonempty('TTY', get_tty())\nif needset('RASH_SPENV_TERMINAL'):\n    from .utils.termdetection import detect_terminal\n    setifnonempty('RASH_SPENV_TERMINAL', detect_terminal())\nreturn subenv", "path": "rash\\record.py", "repo_name": "tkf/rash", "stars": 83, "license": "gpl-3.0", "language": "python", "size": 614}
{"docstring": "\"\"\"\nMove whole WHERE clause to a column named `column`.\n\"\"\"\n", "func_signal": "def move_where_clause_to_column(self, column='condition', key=None):\n", "code": "if self.conditions:\n    expr = \" AND \".join(self.conditions)\n    params = self.params\n    self.params = []\n    self.conditions = []\nelse:\n    expr = '1'\n    params = []\nself.add_column('({0}) AS {1}'.format(expr, column),\n                key or column,\n                params)", "path": "rash\\utils\\sqlconstructor.py", "repo_name": "tkf/rash", "stars": 83, "license": "gpl-3.0", "language": "python", "size": 614}
{"docstring": "\"\"\"\nAttach file handler to RASH logger.\n\n:type cfstore: rash.config.ConfigStore\n\n\"\"\"\n", "func_signal": "def setup_daemon_log_file(cfstore):\n", "code": "level = loglevel(cfstore.daemon_log_level)\nhandler = logging.FileHandler(filename=cfstore.daemon_log_path)\nhandler.setLevel(level)\nlogger.setLevel(level)\nlogger.addHandler(handler)", "path": "rash\\log.py", "repo_name": "tkf/rash", "stars": 83, "license": "gpl-3.0", "language": "python", "size": 614}
{"docstring": "\"\"\"\nAdd AND conditions to match to `params`.\n\n:type matcher: str or callable\n:arg  matcher: if `str`, `matcher.format` is used.\n:type     lhs: str\n:arg      lhs: the first argument to `matcher`.\n:type  params: list\n:arg   params: each element should be able to feed into sqlite '?'.\n:type    numq: int\n:arg     numq: number of parameters for each condition.\n:type flatten: None or callable\n:arg  flatten: when `numq > 1`, it should return a list of\n               length `numq * len(params)`.\n\n\"\"\"\n", "func_signal": "def add_and_matches(self, matcher, lhs, params, numq=1, flatten=None):\n", "code": "params = self._adapt_params(params)\nqs = ['?'] * numq\nflatten = flatten or self._default_flatten(numq)\nexpr = repeat(adapt_matcher(matcher)(lhs, *qs), len(params))\nself.conditions.extend(expr)\nself.params.extend(flatten(params))", "path": "rash\\utils\\sqlconstructor.py", "repo_name": "tkf/rash", "stars": 83, "license": "gpl-3.0", "language": "python", "size": 614}
{"docstring": "\"\"\"\nConvert any representation of `level` to an int appropriately.\n\n:type level: int or str\n:rtype: int\n\n>>> loglevel('DEBUG') == logging.DEBUG\nTrue\n>>> loglevel(10)\n10\n>>> loglevel(None)\nTraceback (most recent call last):\n  ...\nValueError: None is not a proper log level.\n\n\"\"\"\n", "func_signal": "def loglevel(level):\n", "code": "if isinstance(level, str):\n    level = getattr(logging, level.upper())\nelif isinstance(level, int):\n    pass\nelse:\n    raise ValueError('{0!r} is not a proper log level.'.format(level))\nreturn level", "path": "rash\\log.py", "repo_name": "tkf/rash", "stars": 83, "license": "gpl-3.0", "language": "python", "size": 614}
{"docstring": "\"\"\"\nReturn elements in `iterative` including `num`-after elements.\n\n>>> list(include_after(lambda x: x == 'b', 2, 'abcbcde'))\n['b', 'c', 'b', 'c', 'd']\n\n\"\"\"\n", "func_signal": "def include_after(predicate, num, iterative):\n", "code": "(it0, it1) = itertools.tee(iterative)\nps = _forward_shifted_predicate(predicate, num, it1)\nreturn (e for (e, p) in zip(it0, ps) if p)", "path": "rash\\utils\\iterutils.py", "repo_name": "tkf/rash", "stars": 83, "license": "gpl-3.0", "language": "python", "size": 614}
{"docstring": "\"\"\"\nCompile SQL and return 3-tuple ``(sql, params, keys)``.\n\nExample usage::\n\n    (sql, params, keys) = sc.compile()\n    for row in cursor.execute(sql, params):\n        record = dict(zip(keys, row))\n\n\"\"\"\n", "func_signal": "def compile(self):\n", "code": "params = self.column_params + self.join_params + self.params\nif self.limit and self.limit >= 0:\n    self.sql_limit = 'LIMIT ?'\n    params += [self.limit]\nreturn (self.sql, params, self.keys)", "path": "rash\\utils\\sqlconstructor.py", "repo_name": "tkf/rash", "stars": 83, "license": "gpl-3.0", "language": "python", "size": 614}
{"docstring": "\"\"\"\nRecord shell history.\n\"\"\"\n", "func_signal": "def record_run(record_type, print_session_id, **kwds):\n", "code": "if print_session_id and record_type != 'init':\n    raise RuntimeError(\n        '--print-session-id should be used with --record-type=init')\n\ncfstore = ConfigStore()\n# SOMEDAY: Pass a list of environment variables to shell by \"rash\n# init\" and don't read configuration in \"rash record\" command.  It\n# is faster.\nconfig = cfstore.get_config()\nenvkeys = config.record.environ[record_type]\njson_path = os.path.join(cfstore.record_path,\n                         record_type,\n                         time.strftime('%Y-%m-%d-%H%M%S.json'))\nmkdirp(os.path.dirname(json_path))\n\n# Command line options directly map to record keys\ndata = dict((k, v) for (k, v) in kwds.items() if v is not None)\ndata.update(\n    environ=get_environ(envkeys),\n)\n\n# Automatically set some missing variables:\ndata.setdefault('cwd', getcwd())\nif record_type in ['command', 'exit']:\n    data.setdefault('stop', int(time.time()))\nelif record_type in ['init']:\n    data.setdefault('start', int(time.time()))\n\nif print_session_id:\n    data['session_id'] = generate_session_id(data)\n    print(data['session_id'])\n\nwith open(json_path, 'w') as fp:\n    json.dump(data, fp)", "path": "rash\\record.py", "repo_name": "tkf/rash", "stars": 83, "license": "gpl-3.0", "language": "python", "size": 614}
{"docstring": "\"\"\"\nSearch command history.\n\n\"\"\"\n", "func_signal": "def search_run(output, **kwds):\n", "code": "from .config import ConfigStore\nfrom .database import DataBase\nfrom .query import expand_query, preprocess_kwds\n\ncfstore = ConfigStore()\nkwds = expand_query(cfstore.get_config(), kwds)\nformat = get_formatter(**kwds)\nfmtkeys = formatter_keys(format)\ncandidates = set([\n    'command_count', 'success_count', 'success_ratio', 'program_count'])\nkwds['additional_columns'] = candidates & set(fmtkeys)\n\ndb = DataBase(cfstore.db_path)\nfor crec in db.search_command_record(**preprocess_kwds(kwds)):\n    output.write(format.format(**crec.__dict__))", "path": "rash\\search.py", "repo_name": "tkf/rash", "stars": 83, "license": "gpl-3.0", "language": "python", "size": 614}
{"docstring": "\"\"\"\nReturn elements in `iterative` including `num`-before elements.\n\n>>> list(include_before(lambda x: x == 'd', 2, 'abcded'))\n['b', 'c', 'd', 'e', 'd']\n\n\"\"\"\n", "func_signal": "def include_before(predicate, num, iterative):\n", "code": "(it0, it1) = itertools.tee(iterative)\nps = _backward_shifted_predicate(predicate, num, it1)\nreturn (e for (e, p) in zip(it0, ps) if p)", "path": "rash\\utils\\iterutils.py", "repo_name": "tkf/rash", "stars": 83, "license": "gpl-3.0", "language": "python", "size": 614}
{"docstring": "\"\"\"\nTrue if `iterative` returns at least one element.\n\n>>> nonempty(iter([1]))\nTrue\n>>> nonempty(iter([]))\nFalse\n\n\"\"\"\n", "func_signal": "def nonempty(iterative):\n", "code": "for _ in iterative:\n    return True\nreturn False", "path": "rash\\utils\\iterutils.py", "repo_name": "tkf/rash", "stars": 83, "license": "gpl-3.0", "language": "python", "size": 614}
{"docstring": "\"\"\"Returns an observable sequence that terminates with an exception, \nusing the specified scheduler to send out the single OnError message.\n    \n1 - res = rx.Observable.throw_exception(new Error('Error'));\n2 - res = rx.Observable.throw_exception(new Error('Error'), Rx.Scheduler.timeout);\n     \nKeyword arguments:\nexception -- An object used for the sequence's termination.\nscheduler -- Scheduler to send the exceptional termination call on. If\n    not specified, defaults to ImmediateScheduler.\n    \nReturns the observable sequence that terminates exceptionally with the \nspecified exception object.\n\"\"\"\n", "func_signal": "def throw_exception(cls, exception, scheduler=None):\n", "code": "scheduler = scheduler or immediate_scheduler\n\nexception = Exception(exception) if type(exception) is Exception else exception\n\ndef subscribe(observer):\n    def action(scheduler, state):\n        observer.on_error(exception)\n\n    return scheduler.schedule(action)\nreturn AnonymousObservable(subscribe)", "path": "rx\\linq\\observable_creation.py", "repo_name": "Reactive-Extensions/RxPy", "stars": 102, "license": "apache-2.0", "language": "python", "size": 520}
{"docstring": "\"\"\"Advances the scheduler's clock to the specified time, running all \nwork till that point.\n\nKeyword arguments:\ntime -- Absolute time to advance the scheduler's clock to.\n\"\"\"\n", "func_signal": "def advance_to(self, time):\n", "code": "print (\"advance_to()\")\nnext = None\nif self.comparer(self.clock, time) >= 0:\n    raise ArgumentOutOfRangeException()\n\nif not self.is_enabled:\n    self.is_enabled = True\n    while self.is_enabled:\n        next = self.get_next()\n        if next and self.comparer(next.duetime, time) <= 0:\n            if self.comparer(next.duetime, self.clock) > 0:\n                self.clock = next.duetime\n\n            next.invoke()\n        else:\n            self.is_enabled = False\n    \n    self.clock = time", "path": "rx\\concurrency\\virtualtimescheduler.py", "repo_name": "Reactive-Extensions/RxPy", "stars": 102, "license": "apache-2.0", "language": "python", "size": 520}
{"docstring": "\"\"\"Advances the scheduler's clock by the specified relative time.\n\nKeyword arguments:\ntime -- Relative time to advance the scheduler's clock by.\n\"\"\"\n", "func_signal": "def sleep(self, time):\n", "code": "dt = self.add(self.clock, time)\n\nif self.comparer(self.clock, dt) >= 0:\n    raise ArgumentOutOfRangeException()\n\nself.clock = dt", "path": "rx\\concurrency\\virtualtimescheduler.py", "repo_name": "Reactive-Extensions/RxPy", "stars": 102, "license": "apache-2.0", "language": "python", "size": 520}
{"docstring": "\"\"\"Converts timespan to from datetime/timedelta to milliseconds\"\"\"\n\n", "func_signal": "def to_relative(cls, timespan):\n", "code": "if isinstance(timespan, datetime):\n    timespan = timespan - datetime.fromtimestamp(0)\nif isinstance(timespan, timedelta):\n    timespan = int(timespan.total_seconds()*1000)\nreturn timespan", "path": "rx\\testing\\testscheduler.py", "repo_name": "Reactive-Extensions/RxPy", "stars": 102, "license": "apache-2.0", "language": "python", "size": 520}
{"docstring": "#print (\"VirtualTimeScheduler:schedule_absolute:run(%s)\" % repr(state1))\n", "func_signal": "def run(scheduler, state1):\n", "code": "self.queue.remove(si)\n\n#print (\"running action\", action.__doc__)\nreturn action(scheduler, state1)", "path": "rx\\concurrency\\virtualtimescheduler.py", "repo_name": "Reactive-Extensions/RxPy", "stars": 102, "license": "apache-2.0", "language": "python", "size": 520}
{"docstring": "\"\"\"Constructs an observable sequence that depends on a resource object,\nwhose lifetime is tied to the resulting observable sequence's lifetime.\n      \n1 - res = Rx.Observable.using(function () { return new AsyncSubject(); }, function (s) { return s; });\n    \nKeyword arguments:\nresource_factory -- Factory function to obtain a resource object.\nobservable_factory -- Factory function to obtain an observable sequence\n    that depends on the obtained resource.\n     \nReturns an observable sequence whose lifetime controls the lifetime of \nthe dependent resource object.\n\"\"\"\n", "func_signal": "def using(cls, resource_factory, observable_factory):\n", "code": "def subscribe(observer):\n    disposable = Disposable.empty()\n    try:\n        resource = resource_factory()\n        if resource:\n            disposable = resource\n        \n        source = observable_factory(resource)\n    except Exception as exception:\n        d = Observable.throw_exception(exception).subscribe(observer)\n        return CompositeDisposable(d, disposable)\n    \n    return CompositeDisposable(source.subscribe(observer), disposable)\nreturn AnonymousObservable(subscribe)", "path": "rx\\linq\\observable_creation.py", "repo_name": "Reactive-Extensions/RxPy", "stars": 102, "license": "apache-2.0", "language": "python", "size": 520}
{"docstring": "#print \"Trampoline:run\"\n", "func_signal": "def run(self):\n", "code": "while self.queue.length > 0:\n    item = self.queue.dequeue()\n    if not item.is_cancelled():\n        diff = item.duetime - Scheduler.now()\n        while diff > timedelta(0):\n            seconds = diff.seconds + diff.microseconds / 1E6 + diff.days * 86400\n            log.info(\"Trampoline:run(), Sleeping: %s\" % seconds)\n            time.sleep(seconds)\n            diff = item.duetime - Scheduler.now()\n        \n        if not item.is_cancelled():\n            #print(\"item.invoke\")\n            item.invoke()", "path": "rx\\concurrency\\currentthreadscheduler.py", "repo_name": "Reactive-Extensions/RxPy", "stars": 102, "license": "apache-2.0", "language": "python", "size": 520}
{"docstring": "\"\"\"Returns the next scheduled item to be executed.\"\"\"\n", "func_signal": "def get_next(self):\n", "code": "while self.queue.length > 0:\n    next = self.queue.peek()\n    if next.is_cancelled():\n        self.queue.dequeue()\n    else:\n        return next\n\nreturn None", "path": "rx\\concurrency\\virtualtimescheduler.py", "repo_name": "Reactive-Extensions/RxPy", "stars": 102, "license": "apache-2.0", "language": "python", "size": 520}
{"docstring": "\"\"\"Starts the virtual time scheduler.\"\"\"\n", "func_signal": "def start(self):\n", "code": "next = None\nif not self.is_enabled:\n    self.is_enabled = True\n    while self.is_enabled:\n        #print (self.clock)\n        next = self.get_next()\n        if next:\n            if self.comparer(next.duetime, self.clock) > 0:\n                self.clock = next.duetime\n                log.info(\"VirtualTimeScheduler.start(), clock: %s\" % self.clock)\n            #else:\n            #    print (\"skipping\", next.duetime, self.clock)\n            \n            #print (\"Invoke: \", self.clock, next.action)\n            next.invoke()\n        else:\n            self.is_enabled = False", "path": "rx\\concurrency\\virtualtimescheduler.py", "repo_name": "Reactive-Extensions/RxPy", "stars": 102, "license": "apache-2.0", "language": "python", "size": 520}
{"docstring": "\"\"\"Performs the task of cleaning up resources.\"\"\"\n\n", "func_signal": "def dispose(self):\n", "code": "if not self.is_disposed:\n    self.action()\n    self.is_disposed = True", "path": "rx\\disposables\\disposable.py", "repo_name": "Reactive-Extensions/RxPy", "stars": 102, "license": "apache-2.0", "language": "python", "size": 520}
{"docstring": "\"\"\"Creates a new virtual time scheduler with the specified initial \nclock value and absolute time comparer.\n\nKeyword arguments:\ninitial_clock -- Initial value for the clock.\ncomparer -- Comparer to determine causality of events based on absolute time.\n\"\"\"\n\n", "func_signal": "def __init__(self, initial_clock=0, comparer=None):\n", "code": "self.clock = initial_clock\nself.comparer = comparer\nself.is_enabled = False\nself.queue = PriorityQueue(1024)", "path": "rx\\concurrency\\virtualtimescheduler.py", "repo_name": "Reactive-Extensions/RxPy", "stars": 102, "license": "apache-2.0", "language": "python", "size": 520}
{"docstring": "\"\"\"Generates an observable sequence of integral numbers within a \nspecified range, using the specified scheduler to send out observer \nmessages.\n     \n1 - res = Rx.Observable.range(0, 10);\n2 - res = Rx.Observable.range(0, 10, Rx.Scheduler.timeout);\n    \nKeyword arguments:\nstart -- The value of the first integer in the sequence.\ncount -- The number of sequential integers to generate.\nscheduler -- [Optional] Scheduler to run the generator loop on. If not \n    specified, defaults to Scheduler.currentThread.\n    \nReturns an observable sequence that contains a range of sequential \nintegral numbers.\n\"\"\"\n", "func_signal": "def range(cls, start, count, scheduler=None):\n", "code": "scheduler = scheduler or current_thread_scheduler\n\ndef subscribe(observer):\n    def action(scheduler, i):\n        #print(\"Observable:range:subscribe:action\", scheduler, i)\n        if i < count:\n            observer.on_next(start + i)\n            scheduler(i + 1)\n        else:\n            #print \"completed\"\n            observer.on_completed()\n        \n    return scheduler.schedule_recursive(action, 0)\nreturn AnonymousObservable(subscribe)", "path": "rx\\linq\\observable_creation.py", "repo_name": "Reactive-Extensions/RxPy", "stars": 102, "license": "apache-2.0", "language": "python", "size": 520}
{"docstring": "\"\"\"Creates a notification callback from an observer.\n\nReturns the action that forwards its input notification to the underlying observer.\n\"\"\"\n", "func_signal": "def to_notifier(self):\n", "code": "observer = self\n\ndef func(notifier):\n    return notifier.accept(observer)\nreturn func", "path": "rx\\observer.py", "repo_name": "Reactive-Extensions/RxPy", "stars": 102, "license": "apache-2.0", "language": "python", "size": 520}
{"docstring": "#print(\"TimeoutScheduler:schedule_now()\")\n", "func_signal": "def schedule(self, action, state=None):\n", "code": "scheduler = self\ndisposable = SingleAssignmentDisposable()\n\ndef interval():\n    #print (\"TimeoutScheduler:schedule.interval()\")\n    disposable.disposable = action(scheduler, state)\nself.timer = Timer(0, interval)\nself.timer.start()\ndef dispose():\n    #print (\"TimeoutScheduler:schedule.dispose()\")\n    self.timer.cancel()\nreturn CompositeDisposable(disposable, Disposable(dispose))", "path": "rx\\concurrency\\timeoutscheduler.py", "repo_name": "Reactive-Extensions/RxPy", "stars": 102, "license": "apache-2.0", "language": "python", "size": 520}
{"docstring": "\"\"\"Schedules an action to be executed at the specified virtual time.\n\nKeyword arguments:\ndueime -- Absolute virtual time at which to execute the action.\naction -- Action to be executed.\nstate -- State passed to the action to be executed.\n\nReturns disposable object used to cancel the scheduled action (best effort).\n\"\"\"\n\n", "func_signal": "def schedule_absolute(self, duetime, action, state=None):\n", "code": "log.debug(\"TestScheduler.schedule_absolute(duetime=%s, state=%s)\" % (duetime, state))\n\nduetime = duetime if isinstance(duetime, int) else self.to_relative(duetime)\n\nif duetime <= self.clock:\n    duetime = self.clock + 1\n\nreturn super(TestScheduler, self).schedule_absolute(duetime, action, state)", "path": "rx\\testing\\testscheduler.py", "repo_name": "Reactive-Extensions/RxPy", "stars": 102, "license": "apache-2.0", "language": "python", "size": 520}
{"docstring": "\"\"\"Returns a non-terminating observable sequence, which can be used to \ndenote an infinite duration (e.g. when using reactive joins).\n     \nReturns an observable sequence whose observers will never get called.\n\"\"\"\n", "func_signal": "def never(cls):\n", "code": "def subscribe(observer):\n    return Disposable.empty()\n\nreturn AnonymousObservable(subscribe)", "path": "rx\\linq\\observable_creation.py", "repo_name": "Reactive-Extensions/RxPy", "stars": 102, "license": "apache-2.0", "language": "python", "size": 520}
{"docstring": "\"\"\"Converts an array to an observable sequence, using an optional \nscheduler to enumerate the array.\n    \n1 - res = rx.Observable.from_array([1,2,3])\n2 - res = rx.Observable.from_array([1,2,3], rx.Scheduler.timeout)\n     \nKeyword arguments:\nscheduler -- [Optional] Scheduler to run the enumeration of the input sequence on.\n     \nReturns the observable sequence whose elements are pulled from the \ngiven enumerable sequence.\n\"\"\"\n\n", "func_signal": "def from_array(cls, array, scheduler=None):\n", "code": "scheduler = scheduler or current_thread_scheduler\n\ndef subscribe(observer):\n    count = 0\n    \n    def action(action1, state=None):\n        nonlocal count\n        \n        if count < len(array):\n            observer.on_next(array[count])\n            count += 1\n            action1(action)\n        else:\n            observer.on_completed()\n        \n    return scheduler.schedule_recursive(action)\nreturn AnonymousObservable(subscribe)", "path": "rx\\linq\\observable_creation.py", "repo_name": "Reactive-Extensions/RxPy", "stars": 102, "license": "apache-2.0", "language": "python", "size": 520}
{"docstring": "\"\"\"Schedules an action to be executed at duetime. Return the disposable\nobject used to cancel the scheduled action (best effort)\n\nKeyword arguments:\ndue_time -- Relative time after which to execute the action.\naction -- Action to be executed.\nstate -- [Optional] State passed to the action to be executed.\n\"\"\"\n", "func_signal": "def schedule_relative(self, duetime, action, state=None):\n", "code": "log.debug(\"VirtualTimeScheduler.schedule_relative(duetime=%s, state=%s)\" % (duetime, state))\n\nrunat = self.add(self.clock, self.to_relative(duetime))\nreturn self.schedule_absolute(runat, action, state)", "path": "rx\\concurrency\\virtualtimescheduler.py", "repo_name": "Reactive-Extensions/RxPy", "stars": 102, "license": "apache-2.0", "language": "python", "size": 520}
{"docstring": "\"\"\"Advances the scheduler's clock by the specified relative time, \nrunning all work scheduled for that timespan.\n\nKeyword arguments:\ntime -- Relative time to advance the scheduler's clock by.\n\"\"\"\n", "func_signal": "def advance_by(self, time):\n", "code": "log.debug(\"VirtualTimeScheduler.advance_by(time=%s)\" % time)\n\ndt = self.add(self.clock, time)\nif self.comparer(self.clock, dt) >= 0:\n    raise ArgumentOutOfRangeException()\n\nreturn self.advance_to(dt)", "path": "rx\\concurrency\\virtualtimescheduler.py", "repo_name": "Reactive-Extensions/RxPy", "stars": 102, "license": "apache-2.0", "language": "python", "size": 520}
{"docstring": "\"\"\"Returns an empty observable sequence, using the specified scheduler \nto send out the single OnCompleted message.\n     \n1 - res = rx.Observable.empty()\n2 - res = rx.Observable.empty(Rx.Scheduler.timeout)\n    \nscheduler -- Scheduler to send the termination call on.\n    \nreturs an observable sequence with no elements.\n\"\"\"\n\n", "func_signal": "def empty(cls, scheduler=None):\n", "code": "scheduler = scheduler or immediate_scheduler\n    \ndef subscribe(observer):\n    def action(scheduler, state):\n        observer.on_completed()\n\n    return scheduler.schedule(action)\nreturn AnonymousObservable(subscribe)", "path": "rx\\linq\\observable_creation.py", "repo_name": "Reactive-Extensions/RxPy", "stars": 102, "license": "apache-2.0", "language": "python", "size": 520}
{"docstring": "# Erase character\n", "func_signal": "def csi_ECH(self, p):\n", "code": "p = self.vt100_parse_params(p, [1])\nn = min(self.w - self.cx, max(1, p[0]))\nself.clear(self.cy, self.cx, self.cy + 1, self.cx + n);", "path": "webshell.py", "repo_name": "xypiie/WebShell", "stars": 100, "license": "gpl-2.0", "language": "python", "size": 208}
{"docstring": "# Set VT100 mode\n", "func_signal": "def vt100_setmode(self, p, state):\n", "code": "p = self.vt100_parse_params(p, [], False)\nfor m in p:\n\tif m == '4':\n\t\t# Insertion replacement mode\n\t\tself.vt100_mode_insert = state\n\telif m == '20':\n\t\t# Linefeed/new line mode\n\t\tself.vt100_mode_lfnewline = state\n\telif m == '?1':\n\t\t# Cursor key mode\n\t\tself.vt100_mode_cursorkey = state\n\telif m == '?3':\n\t\t# Column mode\n\t\tif self.vt100_mode_column_switch:\n\t\t\tif state:\n\t\t\t\tself.w = 132\n\t\t\telse:\n\t\t\t\tself.w = 80\n\t\t\tself.reset_screen()\n\telif m == '?5':\n\t\t# Screen mode\n\t\tself.vt100_mode_inverse = state\n\telif m == '?6':\n\t\t# Region origin mode\n\t\tself.vt100_mode_origin = state\n\t\tif state:\n\t\t\tself.cursor_set(self.scroll_area_y0, 0)\n\t\telse:\n\t\t\tself.cursor_set(0, 0)\n\telif m == '?7':\n\t\t# Autowrap mode\n\t\tself.vt100_mode_autowrap = state\n\telif m == '?25':\n\t\t# Text cursor enable mode\n\t\tself.vt100_mode_cursor = state\n\telif m == '?40':\n\t\t# Column switch control\n\t\tself.vt100_mode_column_switch = state\n\telif m == '?47':\n\t\t# Alternate screen mode\n\t\tif ((state and not self.vt100_mode_alt_screen) or \n\t\t\t(not state and self.vt100_mode_alt_screen)):\n\t\t\tself.screen, self.screen2 = self.screen2, self.screen\n\t\t\tself.vt100_saved, self.vt100_saved2 = self.vt100_saved2, self.vt100_saved\n\t\tself.vt100_mode_alt_screen = state\n\telif m == '?67':\n\t\t# Backspace/delete\n\t\tself.vt100_mode_backspace = state", "path": "webshell.py", "repo_name": "xypiie/WebShell", "stars": 100, "license": "gpl-2.0", "language": "python", "size": 208}
{"docstring": "# Erase in display\n", "func_signal": "def csi_ED(self, p):\n", "code": "p = self.vt100_parse_params(p, ['0'], False)\nif p[0] == '0':\n\tself.clear(self.cy, self.cx, self.h, self.w)\nelif p[0] == '1':\n\tself.clear(0, 0, self.cy + 1, self.cx + 1)\nelif p[0] == '2':\n\tself.clear(0, 0, self.h, self.w)", "path": "webshell.py", "repo_name": "xypiie/WebShell", "stars": 100, "license": "gpl-2.0", "language": "python", "size": 208}
{"docstring": "# Select charset\n", "func_signal": "def vt100_charset_select(self, g, charset):\n", "code": "self.vt100_charset_g[g] = charset\nself.vt100_charset_update()", "path": "webshell.py", "repo_name": "xypiie/WebShell", "stars": 100, "license": "gpl-2.0", "language": "python", "size": 208}
{"docstring": "# Store cursor\n", "func_signal": "def esc_DECSC(self):\n", "code": "self.vt100_saved = {}\nself.vt100_saved['cx'] = self.cx\nself.vt100_saved['cy'] = self.cy\nself.vt100_saved['attr'] = self.attr\nself.vt100_saved['charset_g_sel'] = self.vt100_charset_g_sel\nself.vt100_saved['charset_g'] = self.vt100_charset_g[:]\nself.vt100_saved['mode_autowrap'] = self.vt100_mode_autowrap\nself.vt100_saved['mode_origin'] = self.vt100_mode_origin", "path": "webshell.py", "repo_name": "xypiie/WebShell", "stars": 100, "license": "gpl-2.0", "language": "python", "size": 208}
{"docstring": "# Set top and bottom margins\n", "func_signal": "def csi_DECSTBM(self, p):\n", "code": "p = self.vt100_parse_params(p, [1, self.h])\nself.scroll_area_set(p[0] - 1, p[1])\nif self.vt100_mode_origin:\n\tself.cursor_set(self.scroll_area_y0, 0)\nelse:\n\tself.cursor_set(0, 0)", "path": "webshell.py", "repo_name": "xypiie/WebShell", "stars": 100, "license": "gpl-2.0", "language": "python", "size": 208}
{"docstring": "# Cursor down\n", "func_signal": "def csi_CUD(self, p):\n", "code": "p = self.vt100_parse_params(p, [1])\nself.cursor_down(max(1, p[0]))", "path": "webshell.py", "repo_name": "xypiie/WebShell", "stars": 100, "license": "gpl-2.0", "language": "python", "size": 208}
{"docstring": "# Cursor left\n", "func_signal": "def csi_CUB(self, p):\n", "code": "p = self.vt100_parse_params(p, [1])\nself.cursor_left(max(1, p[0]))", "path": "webshell.py", "repo_name": "xypiie/WebShell", "stars": 100, "license": "gpl-2.0", "language": "python", "size": 208}
{"docstring": "# Tabulation clear\n", "func_signal": "def csi_TBC(self, p):\n", "code": "p = self.vt100_parse_params(p, ['0'], False)\nif p[0] == '0':\n\tself.csi_CTC('2')\nelif p[0] == '3':\n\tself.csi_CTC('5')", "path": "webshell.py", "repo_name": "xypiie/WebShell", "stars": 100, "license": "gpl-2.0", "language": "python", "size": 208}
{"docstring": "# Device status report\n", "func_signal": "def csi_DSR(self, p):\n", "code": "p = self.vt100_parse_params(p, ['0'], False)\nif p[0] == '5':\n\tself.vt100_out = \"\\x1b[0n\"\nelif p[0] == '6':\n\tx = self.cx + 1\n\ty = self.cy + 1\n\tself.vt100_out = '\\x1b[%d;%dR' % (y, x)\t\nelif p[0] == '7':\n\tself.vt100_out = 'WebShell'\nelif p[0] == '8':\n\tself.vt100_out = version\nelif p[0] == '?6':\n\tx = self.cx + 1\n\ty = self.cy + 1\n\tself.vt100_out = '\\x1b[?%d;%dR' % (y, x)\t\nelif p[0] == '?15':\n\tself.vt100_out = '\\x1b[?13n'\nelif p[0] == '?25':\n\tself.vt100_out = '\\x1b[?20n'\nelif p[0] == '?26':\n\tself.vt100_out = '\\x1b[?27;1n'\nelif p[0] == '?53':\n\tself.vt100_out = '\\x1b[?53n'", "path": "webshell.py", "repo_name": "xypiie/WebShell", "stars": 100, "license": "gpl-2.0", "language": "python", "size": 208}
{"docstring": "# Cursor character absolute\n", "func_signal": "def csi_CHA(self, p):\n", "code": "p = self.vt100_parse_params(p, [1])\nself.cursor_set_x(p[0] - 1)", "path": "webshell.py", "repo_name": "xypiie/WebShell", "stars": 100, "license": "gpl-2.0", "language": "python", "size": 208}
{"docstring": "# Save cursor position\n", "func_signal": "def csi_SCP(self, p):\n", "code": "self.vt100_saved_cx = self.cx\nself.vt100_saved_cy = self.cy", "path": "webshell.py", "repo_name": "xypiie/WebShell", "stars": 100, "license": "gpl-2.0", "language": "python", "size": 208}
{"docstring": "# Request terminal parameters\n", "func_signal": "def csi_DECREQTPARM(self, p):\n", "code": "p = self.vt100_parse_params(p, [], False)\nif p[0] == '0':\n\tself.vt100_out = \"\\x1b[2;1;1;112;112;1;0x\"\nelif p[0] == '1':\n\tself.vt100_out = \"\\x1b[3;1;1;112;112;1;0x\"", "path": "webshell.py", "repo_name": "xypiie/WebShell", "stars": 100, "license": "gpl-2.0", "language": "python", "size": 208}
{"docstring": "# Set Linux signal handler\n", "func_signal": "def __init__(self, cmd = None, env_term = None):\n", "code": "uname = commands.getoutput('uname')\nif uname == 'Linux':\n\tself.sigchldhandler = signal.signal(signal.SIGCHLD, signal.SIG_IGN)\n# Session\nself.session = {}\nself.cmd = cmd\nself.env_term = env_term\n# Synchronize methods\nself.lock = threading.RLock()\nfor name in ['proc_keepalive', 'proc_buryall',\n\t'proc_read', 'proc_write', 'proc_dump', 'proc_getalive']:\n\torig = getattr(self, name)\n\tsetattr(self, name, SynchronizedMethod(self.lock, orig))\n# Supervisor thread\nself.signal_stop = 0\nself.thread = threading.Thread(target = self.proc_thread)\nself.thread.start()", "path": "webshell.py", "repo_name": "xypiie/WebShell", "stars": 100, "license": "gpl-2.0", "language": "python", "size": 208}
{"docstring": "# Cursor tabulation control\n", "func_signal": "def csi_CTC(self, p):\n", "code": "p = self.vt100_parse_params(p, ['0'], False)\nfor m in p:\n\tif m == '0':\n\t\ttry:\n\t\t\tts = self.tab_stops.index(self.cx)\n\t\texcept ValueError:\n\t\t\ttab_stops = self.tab_stops\n\t\t\ttab_stops.append(self.cx)\n\t\t\ttab_stops.sort()\n\t\t\tself.tab_stops = tab_stops\n\telif m == '2':\n\t\ttry:\n\t\t\tself.tab_stops.remove(self.cx)\n\t\texcept ValueError:\n\t\t\tpass\n\telif m == '5':\n\t\tself.tab_stops = [0]", "path": "webshell.py", "repo_name": "xypiie/WebShell", "stars": 100, "license": "gpl-2.0", "language": "python", "size": 208}
{"docstring": "# Delete characters\n", "func_signal": "def csi_DCH(self, p):\n", "code": "p = self.vt100_parse_params(p, [1])\nself.scroll_line_left(self.cy, self.cx, max(1, p[0]))", "path": "webshell.py", "repo_name": "xypiie/WebShell", "stars": 100, "license": "gpl-2.0", "language": "python", "size": 208}
{"docstring": "# Stop supervisor thread\n", "func_signal": "def stop(self):\n", "code": "self.signal_stop = 1\nself.thread.join()", "path": "webshell.py", "repo_name": "xypiie/WebShell", "stars": 100, "license": "gpl-2.0", "language": "python", "size": 208}
{"docstring": "# Invoke active character set\n", "func_signal": "def vt100_charset_set(self, g):\n", "code": "self.vt100_charset_g_sel = g\nself.vt100_charset_update()", "path": "webshell.py", "repo_name": "xypiie/WebShell", "stars": 100, "license": "gpl-2.0", "language": "python", "size": 208}
{"docstring": "# Line position absolute\n", "func_signal": "def csi_VPA(self, p):\n", "code": "p = self.vt100_parse_params(p, [1])\nself.cursor_set_y(p[0] - 1)", "path": "webshell.py", "repo_name": "xypiie/WebShell", "stars": 100, "license": "gpl-2.0", "language": "python", "size": 208}
{"docstring": "# Insert character\n", "func_signal": "def csi_ICH(self, p):\n", "code": "p = self.vt100_parse_params(p, [1])\nself.scroll_line_right(self.cy, self.cx, p[0])", "path": "webshell.py", "repo_name": "xypiie/WebShell", "stars": 100, "license": "gpl-2.0", "language": "python", "size": 208}
{"docstring": "\"\"\"\nSearch for places near an IP address, within a radius (in\nkilometers).\n\nThe server uses guesses the latitude and longitude from the\nipaddr and then does the same thing as search(), using that\nguessed latitude and longitude.\n\"\"\"\n", "func_signal": "def search_by_ip(self, ipaddr, radius=None, query=None, category=None, num=None):\n", "code": "if not is_valid_ip(ipaddr):\n    raise ValueError(\"Address %s is not a valid IP\" % ipaddr)\nif (radius and not is_numeric(radius)):\n    raise ValueError(\"Radius must be numeric.\")\nif (query and not isinstance(query, basestring)):\n    raise ValueError(\"Query must be a string.\")\nif (category and not isinstance(category, basestring)):\n    raise ValueError(\"Category must be a string.\")\nif (num and not is_numeric(num)):\n    raise ValueError(\"Num parameter must be numeric.\")\n\nif isinstance(query, unicode):\n    query = query.encode('utf-8')\nif isinstance(category, unicode):\n    category = category.encode('utf-8')\n\nkwargs = { }\nif radius:\n    kwargs['radius'] = radius\nif query:\n    kwargs['q'] = query\nif category:\n    kwargs['category'] = category\nif num:\n    kwargs['num'] = num\n\nendpoint = self._endpoint('search_by_ip', ipaddr=ipaddr)\n\nresult = self._request(endpoint, 'GET', data=kwargs)[1]\n\nfc = json_decode(result)\nreturn [Feature.from_dict(f) for f in fc['features']]", "path": "simplegeo\\places\\places_10.py", "repo_name": "simplegeo/python-simplegeo", "stars": 105, "license": "None", "language": "python", "size": 343}
{"docstring": "# Test that all features expected are received\n", "func_signal": "def test_expected_features_are_received(self):\n", "code": "for (point, response) in self.known_requests:\n    for expected_feature in point['expected_response']['features']:\n        found_expected_feature = False\n        for received_feature in response['features']:\n            if expected_feature == received_feature:\n                found_expected_feature = True\n        self.assertTrue(found_expected_feature, 'Could not find expected feature in response for point %s,%s:\\n%s' % (point['lat'], point['lon'], expected_feature))", "path": "consumption\\test_context.py", "repo_name": "simplegeo/python-simplegeo", "stars": 105, "license": "None", "language": "python", "size": 343}
{"docstring": "# Ensure that we don't have multiple features with specified type/category/subcategory configurations.\n", "func_signal": "def test_duplicate_categories(self):\n", "code": "dupe_classifiers = [{'type': 'Region',\n                     'category': 'Subnational',\n                      'subcategory': 'State'},\n                     {'type': 'Region',\n                      'category': 'Time Zone',\n                      'subcategory': ''},\n                     {'type': 'Region',\n                      'category': 'National',\n                      'subcategory': ''},\n                     {'type': 'Region',\n                      'category': 'Urban Area',\n                      'subcategory': ''},\n                     {'type': 'Region',\n                      'category': 'US Census',\n                      'subcategory': 'Tract'},\n                     {'type': 'Region',\n                      'category': 'Neighborhood',\n                      'subcategory': ''},\n                     {'type': 'Region',\n                      'category': 'Administrative',\n                      'subcategory': 'County'},\n                     {'type': 'Region',\n                      'category': 'Municiple',\n                      'subcategory': 'City'},\n                     {'type': 'Region',\n                      'category': 'School District',\n                      'subcategory': 'Unified'}]\nfor (point, response) in self.known_requests:\n    for dupe_classifier in dupe_classifiers:\n        instances_found = 0\n        for feature in response['features']:\n            for classifier in feature['classifiers']:\n                if 'type' in classifier and 'category' in classifier and 'subcategory' in classifier:\n                    if dupe_classifier['type'] == classifier['type'] and dupe_classifier['category'] == classifier['category'] and dupe_classifier['subcategory'] == classifier['subcategory']:\n                        instances_found += 1\n                        self.assertTrue(instances_found <= 1, 'Found dupe for type/categories %s/%s/%s for point %s,%s' % (dupe_classifier['type'], dupe_classifier['category'], dupe_classifier['subcategory'], point['lat'], point['lon']))", "path": "consumption\\test_context.py", "repo_name": "simplegeo/python-simplegeo", "stars": 105, "license": "None", "language": "python", "size": 343}
{"docstring": "\"\"\"Create a new feature, returns the simplegeohandle. \"\"\"\n", "func_signal": "def add_feature(self, feature):\n", "code": "endpoint = self._endpoint('create')\nif hasattr(feature, 'id'):\n    # only simplegeohandles or None should be stored in self.id\n    assert is_simplegeohandle(feature.id)\n    raise ValueError('A feature cannot be added to the Places database when it already has a simplegeohandle: %s' % (feature.id,))\njsonrec = feature.to_json()\nresp, content = self._request(endpoint, \"POST\", jsonrec)\nif resp['status'] != \"202\":\n    raise APIError(int(resp['status']), content, resp)\ncontentobj = json_decode(content)\nif not contentobj.has_key('id'):\n    raise APIError(int(resp['status']), content, resp)\nhandle = contentobj['id']\nassert is_simplegeohandle(handle)\nreturn handle", "path": "simplegeo\\places\\places_10.py", "repo_name": "simplegeo/python-simplegeo", "stars": 105, "license": "None", "language": "python", "size": 343}
{"docstring": "\"\"\"Search for places near a lat/lon, within a radius (in kilometers).\"\"\"\n", "func_signal": "def search(self, lat, lon, radius=None, query=None, category=None, num=None):\n", "code": "_assert_valid_lat(lat)\n_assert_valid_lon(lon)\nif (radius and not is_numeric(radius)):\n    raise ValueError(\"Radius must be numeric.\")\nif (query and not isinstance(query, basestring)):\n    raise ValueError(\"Query must be a string.\")\nif (category and not isinstance(category, basestring)):\n    raise ValueError(\"Category must be a string.\")\nif (num and not is_numeric(num)):\n    raise ValueError(\"Num parameter must be numeric.\")\n\nif isinstance(query, unicode):\n    query = query.encode('utf-8')\nif isinstance(category, unicode):\n    category = category.encode('utf-8')\n\nkwargs = { }\nif radius:\n    kwargs['radius'] = radius\nif query:\n    kwargs['q'] = query\nif category:\n    kwargs['category'] = category\nif num:\n    kwargs['num'] = num\n\nendpoint = self._endpoint('search', lat=lat, lon=lon)\n\nresult = self._request(endpoint, 'GET', data=kwargs)[1]\n\nfc = json_decode(result)\nreturn [Feature.from_dict(f) for f in fc['features']]", "path": "simplegeo\\places\\places_10.py", "repo_name": "simplegeo/python-simplegeo", "stars": 105, "license": "None", "language": "python", "size": 343}
{"docstring": "# Tests random requests and known requests.\n", "func_signal": "def test_duplicate_handles(self):\n", "code": "for (point, response) in self.known_requests + self.random_requests:\n    for (i, feature) in enumerate(response['features']):\n        for (j, possible_duplicate_feature) in enumerate(response['features']):\n            if i != j:\n                self.assertNotEqual(feature['handle'], possible_duplicate_feature['handle'], 'Found duplicate handle %s for point %s,%s' % (feature['handle'], point['lat'], point['lon']))\n                # Test for dupes in the first 25 characters of the handle.\n                self.assertNotEqual(feature['handle'][:25], possible_duplicate_feature['handle'][:25], 'Found duplicate *base* handle %s for point %s,%s' % (feature['handle'][:25], point['lat'], point['lon']))", "path": "consumption\\test_context.py", "repo_name": "simplegeo/python-simplegeo", "stars": 105, "license": "None", "language": "python", "size": 343}
{"docstring": "\"\"\"Delete a Places feature.\"\"\"\n", "func_signal": "def delete_feature(self, simplegeohandle):\n", "code": "if not is_simplegeohandle(simplegeohandle):\n    raise ValueError(\"simplegeohandle is required to match \"\n                     \"the regex %s\" % SIMPLEGEOHANDLE_RSTR)\nendpoint = self._endpoint('feature', simplegeohandle=simplegeohandle)\nreturn self._request(endpoint, 'DELETE')[1]", "path": "simplegeo\\places\\places_10.py", "repo_name": "simplegeo/python-simplegeo", "stars": 105, "license": "None", "language": "python", "size": 343}
{"docstring": "\"\"\" _request() is required to return the exact string that the HTTP\nserver sent to it -- no transforming it, such as by json-decoding and\nthen constructing a Record. \"\"\"\n\n", "func_signal": "def test_dont_Recordify_results(self):\n", "code": "EXAMPLE_RECORD_JSONSTR=json.dumps({ 'geometry' : { 'type' : 'Point', 'coordinates' : [D('10.0'), D('11.0')] }, 'id' : 'my_id', 'type' : 'Feature', 'properties' : { 'key' : 'value'  , 'type' : 'object' } })\n\nmockhttp = mock.Mock()\nmockhttp.request.return_value = ({'status': '200', 'content-type': 'application/json', }, EXAMPLE_RECORD_JSONSTR)\nself.client.http = mockhttp\nres = self.client._request(\"http://thing\", 'POST')[1]\nself.failUnlessEqual(res, EXAMPLE_RECORD_JSONSTR)", "path": "simplegeo\\test\\client\\test_client.py", "repo_name": "simplegeo/python-simplegeo", "stars": 105, "license": "None", "language": "python", "size": 343}
{"docstring": "\"\"\"Update a Places feature.\"\"\"\n", "func_signal": "def update_feature(self, feature):\n", "code": "endpoint = self._endpoint('feature', simplegeohandle=feature.id)\nreturn self._request(endpoint, 'POST', feature.to_json())[1]", "path": "simplegeo\\places\\places_10.py", "repo_name": "simplegeo/python-simplegeo", "stars": 105, "license": "None", "language": "python", "size": 343}
{"docstring": "# Invesco Field should have the demographic data.\n", "func_signal": "def test_demographics(self):\n", "code": "point = self.known_points['invesco_field']\nresponse = point['response']\n\nself.assertTrue('demographics' in response, 'Demographics not found in response for point %s,%s' % (point['lat'], point['lon']))\nself.assertTrue('metro_score' in response['demographics'], 'metro_score not found in demographics section for point %s,%s' % (point['lat'], point['lon']))\nself.assertTrue(0 <= int(response['demographics']['metro_score']) <= 10, 'Invalid value \"%s\" for metro_score in response for point %s,%s' % (response['demographics']['metro_score'], point['lat'], point['lon']))", "path": "consumption\\test_context.py", "repo_name": "simplegeo/python-simplegeo", "stars": 105, "license": "None", "language": "python", "size": 343}
{"docstring": "\"\"\"Return the GeoJSON representation of a feature.\"\"\"\n", "func_signal": "def get_feature(self, place_id):\n", "code": "(headers, response) = self._request(\n    self._endpoint('feature', place_id=place_id), 'GET')\nreturn self._respond(headers, response)", "path": "simplegeo\\places\\places_12.py", "repo_name": "simplegeo/python-simplegeo", "stars": 105, "license": "None", "language": "python", "size": 343}
{"docstring": "# Test that all features received are expected\n", "func_signal": "def test_received_features_are_expected(self):\n", "code": "for (point, response) in self.known_requests:\n    for received_feature in response['features']:\n        found_received_feature = False\n        for expected_feature in point['expected_response']['features']:\n            if received_feature == expected_feature:\n                found_received_feature = True\n        self.assertTrue(found_received_feature, 'Could not find received feature in response for point %s,%s:\\n%s' % (point['lat'], point['lon'], received_feature))", "path": "consumption\\test_context.py", "repo_name": "simplegeo/python-simplegeo", "stars": 105, "license": "None", "language": "python", "size": 343}
{"docstring": "# Invesco Field should have the weather data.\n", "func_signal": "def test_weather(self):\n", "code": "point = self.known_points['invesco_field']\nresponse = point['response']\n\nself.assertTrue('weather' in response, 'Weather not contained in response for point %s,%s' % (point['lat'], point['lon']))\nself.assertTrue('temperature' in response['weather'], 'Temperature not found in weather in response for point %s,%s' % (point['lat'], point['lon']))\nself.assertEqual(response['weather']['temperature'][-1:], 'F', 'Temperature value %s does not end in F in response for %s,%s' % (response['weather']['temperature'], point['lat'], point['lon']))", "path": "consumption\\test_context.py", "repo_name": "simplegeo/python-simplegeo", "stars": 105, "license": "None", "language": "python", "size": 343}
{"docstring": "\"\"\"\nFor the meaning of strict_lon_validation, please see the function\nis_valid_lon().\n\"\"\"\n", "func_signal": "def deep_validate_lat_lon(struc, strict_lon_validation=False):\n", "code": "if not isinstance(struc, (list, tuple, set)):\n    raise TypeError('argument is required to be a sequence (of sequences of...) numbers, not: %s :: %s' % (struc, type(struc)))\nif is_numeric(struc[0]):\n    if not len(struc) == 2:\n        raise TypeError(\"The leaf element of this structure is required to be a tuple of length 2 (to hold a lat and lon).\")\n\n    _assert_valid_lat(struc[0])\n    _assert_valid_lon(struc[1], strict=strict_lon_validation)\nelse:\n    for sub in struc:\n        deep_validate_lat_lon(sub, strict_lon_validation=strict_lon_validation)\nreturn True", "path": "simplegeo\\util.py", "repo_name": "simplegeo/python-simplegeo", "stars": 105, "license": "None", "language": "python", "size": 343}
{"docstring": "\"\"\"\nLongitude is technically defined as extending from -180 to\n180. However in practice people sometimes prefer to use longitudes\nwhich have \"wrapped around\" past 180. For example, if you are\ndrawing a polygon around modern-day Russia almost all of it is in\nthe Eastern Hemisphere, which means its longitudes are almost all\npositive numbers, but the easternmost part of it (Big Diomede\nIsland) lies a few degrees east of the International Date Line,\nand it is sometimes more convenient to describe it as having\nlongitude 190.9 instead of having longitude -169.1.\n\nIf strict=True then is_valid_lon() requires a number to be in\n[-180..180] to be considered a valid longitude. If strict=False\n(the default) then it requires the number to be in [-360..360].\n\"\"\"\n", "func_signal": "def is_valid_lon(x, strict=False):\n", "code": "if strict:\n    return is_numeric(x) and (x <= 180) and (x >= -180)\nelse:\n    return is_numeric(x) and (x <= 360) and (x >= -360)", "path": "simplegeo\\util.py", "repo_name": "simplegeo/python-simplegeo", "stars": 105, "license": "None", "language": "python", "size": 343}
{"docstring": "\"\"\" _request() is required to return the exact string that the HTTP\nserver sent to it -- no transforming it, such as by json-decoding. \"\"\"\n\n", "func_signal": "def test_dont_json_decode_results(self):\n", "code": "mockhttp = mock.Mock()\nmockhttp.request.return_value = ({'status': '200', 'content-type': 'application/json', }, '{ \"Hello\": \"I am a string. \\xe2\\x9d\\xa4\" }'.decode('utf-8'))\nself.client.http = mockhttp\nres = self.client._request(\"http://thing\", 'POST')[1]\nself.failUnlessEqual(res, '{ \"Hello\": \"I am a string. \\xe2\\x9d\\xa4\" }'.decode('utf-8'))", "path": "simplegeo\\test\\client\\test_client.py", "repo_name": "simplegeo/python-simplegeo", "stars": 105, "license": "None", "language": "python", "size": 343}
{"docstring": "\"\"\"Return a translated address using Google Translate API.\nSee http://goo.gl/HXJvu for list of language codes.\"\"\"\n", "func_signal": "def get_translated_address_from_feature(feature, translate_to='ru'):\n", "code": "feature = feature.to_dict()\naddress = feature['properties']['address']\nlangpair = '%s|%s'%('en',translate_to)\nbase_url = 'http://ajax.googleapis.com/ajax/services/language/translate?'\nparams = urllib.urlencode( (('v',1.0),\n                            ('q',address),\n                            ('langpair',langpair),) )\nurl = base_url+params\ncontent = urlopen(url).read()\nstart_idx = content.find('\"translatedText\":\"')+18\ntranslation = content[start_idx:]\nend_idx = translation.find('\"}, \"')\ntranslation = translation[:end_idx]\nreturn translation", "path": "simplegeo\\contrib\\google.py", "repo_name": "simplegeo/python-simplegeo", "stars": 105, "license": "None", "language": "python", "size": 343}
{"docstring": "\"\"\"\nReturns a GeoJSON object, including having its coordinates in\nGeoJSON standad order (lon, lat) instead of SimpleGeo standard\norder (lat, lon).\n\"\"\"\n", "func_signal": "def to_dict(self):\n", "code": "d = {\n    'type': 'Feature',\n    'geometry': {\n        'type': self.geomtype,\n        'coordinates': deep_swap(self.coordinates)\n    },\n    'properties': copy.deepcopy(self.properties),\n}\n\nif hasattr(self, 'id'):\n    d['id'] = self.id\n\nreturn d", "path": "simplegeo\\models.py", "repo_name": "simplegeo/python-simplegeo", "stars": 105, "license": "None", "language": "python", "size": 343}
{"docstring": "\"\"\"\nSearch for places near your IP address, within a radius (in\nkilometers).\n\nThe server gets the IP address from the HTTP connection (this\nmay be the IP address of your device or of a firewall, NAT, or\nHTTP proxy device between you and the server), and then does\nthe same thing as search_by_ip(), using that IP address.\n\"\"\"\n", "func_signal": "def search_by_my_ip(self, radius=None, query=None, category=None, num=None):\n", "code": "if (radius and not is_numeric(radius)):\n    raise ValueError(\"Radius must be numeric.\")\nif (query and not isinstance(query, basestring)):\n    raise ValueError(\"Query must be a string.\")\nif (category and not isinstance(category, basestring)):\n    raise ValueError(\"Category must be a string.\")\nif (num and not is_numeric(num)):\n    raise ValueError(\"Num parameter must be numeric.\")\n\nif isinstance(query, unicode):\n    query = query.encode('utf-8')\nif isinstance(category, unicode):\n    category = category.encode('utf-8')\n\nkwargs = { }\nif radius:\n    kwargs['radius'] = radius\nif query:\n    kwargs['q'] = query\nif category:\n    kwargs['category'] = category\nif num:\n    kwargs['num'] = num\n\nendpoint = self._endpoint('search_by_my_ip')\n\nresult = self._request(endpoint, 'GET', data=kwargs)[1]\n\nfc = json_decode(result)\nreturn [Feature.from_dict(f) for f in fc['features']]", "path": "simplegeo\\places\\places_10.py", "repo_name": "simplegeo/python-simplegeo", "stars": 105, "license": "None", "language": "python", "size": 343}
{"docstring": "\"\"\"Fulltext search for places.\"\"\"\n", "func_signal": "def search_text(self, query=None, category=None, limit=None, start=None):\n", "code": "if (query and not isinstance(query, basestring)):\n    raise ValueError(\"Query must be a string.\")\nif (category and not isinstance(category, basestring)):\n    raise ValueError(\"Category must be a string.\")\nif (limit and not is_numeric(limit)):\n    raise ValueError(\"Limit parameter must be numeric.\")\nif (start and not is_numeric(start)):\n    raise ValueError(\"Start parameter must be numeric.\")\n\nif isinstance(query, unicode):\n    query = query.encode('utf-8')\nif isinstance(category, unicode):\n    category = category.encode('utf-8')\n\nkwargs = { }\nif query:\n    kwargs['q'] = query\nif category:\n    kwargs['category'] = category\nif limit:\n    kwargs['limit'] = limit\nif start:\n    kwargs['start'] = start\n\nreturn self._respond(*self._request(self._endpoint('search_text'),\n                                    'GET', data=kwargs))", "path": "simplegeo\\places\\places_12.py", "repo_name": "simplegeo/python-simplegeo", "stars": 105, "license": "None", "language": "python", "size": 343}
{"docstring": "\"\"\"Add a mouse event to events.\nParams:\n  event: the event info\n  value: 2=motion, 1=down, 0=up\n\"\"\"\n", "func_signal": "def _handle_mouse(self, event, value):\n", "code": "if value == 2:\n  self.events.append(XEvent('EV_MOV',\n      0, 0, (event.root_x, event.root_y)))\nelif event.detail in [4, 5]:\n  if event.detail == 5:\n    value = -1\n  else:\n    value = 1\n  self.events.append(XEvent('EV_REL',\n      0, XEvents._butn_to_code.get(event.detail, 'BTN_%d' % event.detail), value))\nelse:\n  self.events.append(XEvent('EV_KEY',\n      0, XEvents._butn_to_code.get(event.detail, 'BTN_%d' % event.detail), value))", "path": "src\\keymon\\xlib.py", "repo_name": "critiqjo/key-mon", "stars": 95, "license": "apache-2.0", "language": "python", "size": 1708}
{"docstring": "\"\"\"Add the options to the optparse instance and parse command line\nArgs:\n  desc: Description to use for the program.\n  args: Args for testing or sys.args[1:] otherwise\n\"\"\"\n", "func_signal": "def parse_args(self, desc, args=None):\n", "code": "parser = optparse.OptionParser(desc)\nfor dest in self._options_order:\n  opt = self._options[dest]\n  opt.add_to_parser(parser)\n\nself._opt_ret, self._other_args = parser.parse_args(args)\nfor opt in self._options.values():\n  opt.set_from_optparse(self._opt_ret, args)", "path": "src\\keymon\\options.py", "repo_name": "critiqjo/key-mon", "stars": 95, "license": "apache-2.0", "language": "python", "size": 1708}
{"docstring": "\"\"\"If possible the button is passed on.\"\"\"\n", "func_signal": "def _defer_to(self, old_name):\n", "code": "if not self.defer_to:\n  return\nself.defer_to.switch_to(old_name)\nself.defer_to.switch_to_default()", "path": "src\\keymon\\two_state_image.py", "repo_name": "critiqjo/key-mon", "stars": 95, "license": "apache-2.0", "language": "python", "size": 1708}
{"docstring": "\"\"\"Set the value via attribute name.\nArgs:\n  attr: attribute name ('_value', or '_temp_value')\n  val: value to set\n\"\"\"\n", "func_signal": "def _set_attr_value(self, attr, val):\n", "code": "old_val = getattr(self, attr)\nif val is None:\n  setattr(self, attr, val)\nelif self._type == 'int':\n  setattr(self, attr, int(val))\nelif self._type == 'float':\n  setattr(self, attr, float(val))\nelif self._type == 'bool':\n  if isinstance(val, basestring):\n    if val.lower() in ('false', 'off', 'no', '0'):\n      setattr(self, attr, False)\n    elif val.lower() in ('true', 'on', 'yes', '1'):\n      setattr(self, attr, True)\n    else:\n      raise OptionException('Unable to convert %s to bool' % val)\n  else:\n    setattr(self, attr, bool(val))\nelse:\n  setattr(self, attr, val)\nself._dirty = old_val != getattr(self, attr)", "path": "src\\keymon\\options.py", "repo_name": "critiqjo/key-mon", "stars": 95, "license": "apache-2.0", "language": "python", "size": 1708}
{"docstring": "\"\"\"Sort of a idle event.\n\nReturns True if image has been changed.\n\"\"\"\n", "func_signal": "def empty_event(self):\n", "code": "if self.count_down is None:\n  return\ndelta = time.time() - self.count_down\nif delta > self.timeout_secs:\n  if self.normal.replace('_EMPTY', '') in ('SHIFT', 'ALT', 'CTRL', 'META') and \\\n      self.really_pressed:\n    return\n  self.count_down = None\n  self._switch_to(self.normal)\n  return True", "path": "src\\keymon\\two_state_image.py", "repo_name": "critiqjo/key-mon", "stars": 95, "license": "apache-2.0", "language": "python", "size": 1708}
{"docstring": "\"\"\"Booleans need special handling.\"\"\"\n", "func_signal": "def _add_bool_to_parser(self, parser):\n", "code": "args = []\nif self._opt_short:\n  args.append(self._opt_short)\nif self._opt_long:\n  args.append(self._opt_long)\nparser.add_option(action='store_true', default=self._default,\n  dest=self._dest, help=self._help, *args)\n\nif self._ini_group:\n  # Only need the --no version if it could be saved to ini file.\n  parser.add_option('--no' + self._opt_long.lstrip('-'),\n          action='store_false',\n          dest=self._dest, help=_('Opposite of %s') % self._opt_long)", "path": "src\\keymon\\options.py", "repo_name": "critiqjo/key-mon", "stars": 95, "license": "apache-2.0", "language": "python", "size": 1708}
{"docstring": "\"\"\"Switch to image with this name.\"\"\"\n", "func_signal": "def switch_to(self, name):\n", "code": "if self.current != self.normal and self.defer_to:\n  self._defer_to(self.current)\n  # Make sure defer_to image will only start counting timeout after self\n  # image has timed out.\n  if self.count_down:\n    self.defer_to.count_down = self.count_down + self.timeout_secs\n  else:\n    self.defer_to.count_down += self.timeout_secs\nself._switch_to(name)", "path": "src\\keymon\\two_state_image.py", "repo_name": "critiqjo/key-mon", "stars": 95, "license": "apache-2.0", "language": "python", "size": 1708}
{"docstring": "\"\"\"Returns the next event in queue, or None if none.\"\"\"\n", "func_signal": "def next_event(self):\n", "code": "if self.events:\n  return self.events.pop(0)\nreturn None", "path": "src\\keymon\\xlib.py", "repo_name": "critiqjo/key-mon", "stars": 95, "license": "apache-2.0", "language": "python", "size": 1708}
{"docstring": "\"\"\"Reset ini file to defaults.\"\"\"\n", "func_signal": "def reset_to_defaults(self):\n", "code": "for opt in self._options.values():\n  if not opt.ini_group:\n    continue\n  opt.reset_to_default()", "path": "src\\keymon\\options.py", "repo_name": "critiqjo/key-mon", "stars": 95, "license": "apache-2.0", "language": "python", "size": 1708}
{"docstring": "\"\"\"Stop listening to events.\"\"\"\n", "func_signal": "def stop_listening(self):\n", "code": "if not self._listening:\n  return\nself.local_display.record_disable_context(self.ctx)\nself.local_display.flush()\nself.local_display.close()\nself._listening = False\nself.join(0.05)", "path": "src\\keymon\\xlib.py", "repo_name": "critiqjo/key-mon", "stars": 95, "license": "apache-2.0", "language": "python", "size": 1708}
{"docstring": "\"\"\"Reset to the default value.\"\"\"\n", "func_signal": "def reset_to_default(self):\n", "code": "self._set_value(self._default)\nself._set_temp_value(None)", "path": "src\\keymon\\options.py", "repo_name": "critiqjo/key-mon", "stars": 95, "license": "apache-2.0", "language": "python", "size": 1708}
{"docstring": "\"\"\"Start the countdown now.\"\"\"\n", "func_signal": "def reset_time_if_pressed(self):\n", "code": "if self.is_pressed():\n  self.count_down = time.time()", "path": "src\\keymon\\two_state_image.py", "repo_name": "critiqjo/key-mon", "stars": 95, "license": "apache-2.0", "language": "python", "size": 1708}
{"docstring": "\"\"\"Parser an ini file from fp, which is file-like class.\"\"\"\n\n", "func_signal": "def write_ini(self, fp):\n", "code": "config = ConfigParser.SafeConfigParser()\nfor opt in self._options.values():\n  if not opt.ini_group:\n    continue\n  if not config.has_section(opt.ini_group):\n    config.add_section(opt.ini_group)\n\n  if opt.ini_value is not None:\n    config.set(opt.ini_group, opt.ini_name, opt.ini_value)\nconfig.write(fp)", "path": "src\\keymon\\options.py", "repo_name": "critiqjo/key-mon", "stars": 95, "license": "apache-2.0", "language": "python", "size": 1708}
{"docstring": "\"\"\"Parser an ini file from fp, which is file-like class.\"\"\"\n\n", "func_signal": "def parse_ini(self, fp):\n", "code": "config = ConfigParser.SafeConfigParser()\nconfig.readfp(fp)\nchecker = {}\nfor opt in self._options.values():\n  if opt.ini_group:\n    checker[opt.ini_group + '-' + opt.ini_name] = True\n    if (config.has_section(opt.ini_group) and\n        config.has_option(opt.ini_group, opt.ini_name)):\n      opt.value = config.get(opt.ini_group, opt.ini_name)\n      LOG.info('From ini getting %s.%s = %s', opt.ini_group, opt.ini_name,\n          opt.value)\nfor section in config.sections():\n  for name, value in config.items(section):\n    combined_name = section + '-' + name\n    if not combined_name in checker:\n      LOG.info('Unknown option %r in section [%s]', name, section)\n      # we no longer throw an error to be backward compatible", "path": "src\\keymon\\options.py", "repo_name": "critiqjo/key-mon", "stars": 95, "license": "apache-2.0", "language": "python", "size": 1708}
{"docstring": "\"\"\"Value to store in the ini, always a string.\"\"\"\n", "func_signal": "def ini_value(self):\n", "code": "if self._value is None:\n  return None\nif self._type == 'bool':\n  if self._value is True:\n    return '1'\n  else:\n    return '0'\nelse:\n  return str(self._value)", "path": "src\\keymon\\options.py", "repo_name": "critiqjo/key-mon", "stars": 95, "license": "apache-2.0", "language": "python", "size": 1708}
{"docstring": "\"\"\"Image from pixbufs has changed, reset.\"\"\"\n", "func_signal": "def reset_image(self, showit=True):\n", "code": "self.showit = showit\nself._switch_to(self.normal)\nself.showit = True", "path": "src\\keymon\\two_state_image.py", "repo_name": "critiqjo/key-mon", "stars": 95, "license": "apache-2.0", "language": "python", "size": 1708}
{"docstring": "\"\"\"Try and set an option from optparse.\nArgs:\n  opts: options as returned from parse_args()\n  args: arguments as returned bys sys.args.\n\"\"\"\n", "func_signal": "def set_from_optparse(self, opts, args):\n", "code": "if not self._opt_short and not self._opt_long:\n  return\n\n# Was this option actually passed on the command line?\nfound = False\nif args:\n  for arg in args:\n    if self._type == 'bool' and arg.startswith('--no'):\n      arg = '--' + arg[4:]\n    # Remove the --x=123, if any\n    arg = arg.split('=')[0]\n    if arg == self._opt_short or arg == self._opt_long:\n      found = True\n      break\n\nif hasattr(opts, self._dest):\n  opt_val = getattr(opts, self._dest)\n  if not self._ini_name:\n    # For commands like --version which don't have stored values\n    self._set_value(opt_val)\n  if found:\n    self._set_temp_value(opt_val)", "path": "src\\keymon\\options.py", "repo_name": "critiqjo/key-mon", "stars": 95, "license": "apache-2.0", "language": "python", "size": 1708}
{"docstring": "\"\"\"Internal, switch to image with this name even if same.\"\"\"\n", "func_signal": "def _switch_to(self, name):\n", "code": "self.set_from_pixbuf(self.pixbufs.get(name))\nself.current = name\nself.count_down = None\nif self.showit:\n  self.show()", "path": "src\\keymon\\two_state_image.py", "repo_name": "critiqjo/key-mon", "stars": 95, "license": "apache-2.0", "language": "python", "size": 1708}
{"docstring": "\"\"\"Return the value.\"\"\"\n", "func_signal": "def get_value(self):\n", "code": "if self._temp_value is not None:\n  return self._temp_value\nreturn self._value", "path": "src\\keymon\\options.py", "repo_name": "critiqjo/key-mon", "stars": 95, "license": "apache-2.0", "language": "python", "size": 1708}
{"docstring": "\"\"\"Setup the key lookups.\"\"\"\n# set locale to default C locale, see Issue 77.\n# Use setlocale(None) to get curent locale instead of getlocal.\n# See Issue 125 and http://bugs.python.org/issue1699853.\n", "func_signal": "def _setup_lookup(self):\n", "code": "OLD_CTYPE = locale.setlocale(locale.LC_CTYPE, None)\nlocale.setlocale(locale.LC_CTYPE, 'C')\nfor name in dir(XK):\n  if name[:3] == \"XK_\":\n    code = getattr(XK, name)\n    self.keycode_to_symbol[code] = 'KEY_' + name[3:].upper()\nlocale.setlocale(locale.LC_CTYPE, OLD_CTYPE)\nself.keycode_to_symbol[65027] = 'KEY_ISO_LEVEL3_SHIFT'\nself.keycode_to_symbol[269025062] = 'KEY_BACK'\nself.keycode_to_symbol[269025063] = 'KEY_FORWARD'\nself.keycode_to_symbol[16777215] = 'KEY_CAPS_LOCK'\nself.keycode_to_symbol[269025067] = 'KEY_WAKEUP'\n# Multimedia keys\nself.keycode_to_symbol[269025042] = 'KEY_AUDIOMUTE'\nself.keycode_to_symbol[269025041] = 'KEY_AUDIOLOWERVOLUME'\nself.keycode_to_symbol[269025043] = 'KEY_AUDIORAISEVOLUME'\nself.keycode_to_symbol[269025047] = 'KEY_AUDIONEXT'\nself.keycode_to_symbol[269025044] = 'KEY_AUDIOPLAY'\nself.keycode_to_symbol[269025046] = 'KEY_AUDIOPREV'\nself.keycode_to_symbol[269025045] = 'KEY_AUDIOSTOP'\n# Turkish / F layout\nself.keycode_to_symbol[699] = 'KEY_GBREVE'   # scancode = 26 / 18\nself.keycode_to_symbol[697] = 'KEY_IDOTLESS' # scancode = 23 / 19\nself.keycode_to_symbol[442] = 'KEY_SCEDILLA' # scancode = 39 / 40", "path": "src\\keymon\\xlib.py", "repo_name": "critiqjo/key-mon", "stars": 95, "license": "apache-2.0", "language": "python", "size": 1708}
{"docstring": "\"\"\"\nConstructor of FoulingStatistics class.\n\n:param play: Type of teams playing in matches.\n:type play: string\n:param base_uri: Base URI of API.\n:type base_uri: string\n:param auth: Authentication credential.\n:type auth: tuple\n\"\"\"\n", "func_signal": "def __init__(self, play, base_uri, auth):\n", "code": "super(FoulingStatistics, self).__init__()\n\nstatistic = \"fouls\"\n\nself.cards = MatchStatisticsResource(play, statistic, \"cards\", base_uri, auth)\nself.wins = MatchStatisticsResource(play, statistic, \"wins\", base_uri, auth)", "path": "soccermetrics\\rest\\resources\\statistics.py", "repo_name": "soccermetrics/soccermetrics-client-py", "stars": 71, "license": "mit", "language": "python", "size": 9102}
{"docstring": "\"\"\"Verify national team match statistics endpoints without match or record IDs.\"\"\"\n", "func_signal": "def test_natl_statistics_endpoints(self):\n", "code": "self.assertEqual(self.client.natl.stats.crosses.corners.endpoint, '/v1/national/stats/crosses/corners')\nself.assertEqual(self.client.natl.stats.crosses.totals.endpoint, '/v1/national/stats/crosses/totals')\n\nself.assertEqual(self.client.natl.stats.defense.actions.endpoint, '/v1/national/stats/defense/actions')\nself.assertEqual(self.client.natl.stats.defense.blocks.endpoint, '/v1/national/stats/defense/blocks')\nself.assertEqual(self.client.natl.stats.defense.clearances.endpoint, '/v1/national/stats/defense/clearances')\nself.assertEqual(self.client.natl.stats.defense.goalline.endpoint, '/v1/national/stats/defense/goalline')\nself.assertEqual(self.client.natl.stats.defense.tackles.endpoint, '/v1/national/stats/defense/tackles')\n\nself.assertEqual(self.client.natl.stats.fouls.cards.endpoint, '/v1/national/stats/fouls/cards')\nself.assertEqual(self.client.natl.stats.fouls.wins.endpoint, '/v1/national/stats/fouls/wins')\n\nself.assertEqual(self.client.natl.stats.goals.assists.endpoint, '/v1/national/stats/goals/assists')\nself.assertEqual(self.client.natl.stats.goals.bodyparts.endpoint, '/v1/national/stats/goals/bodyparts')\nself.assertEqual(self.client.natl.stats.goals.locations.endpoint, '/v1/national/stats/goals/locations')\nself.assertEqual(self.client.natl.stats.goals.penalties.endpoint, '/v1/national/stats/goals/penalties')\nself.assertEqual(self.client.natl.stats.goals.totals.endpoint, '/v1/national/stats/goals/totals')\n\nself.assertEqual(self.client.natl.stats.goalkeeper.actions.endpoint, '/v1/national/stats/goalkeeper/actions')\nself.assertEqual(self.client.natl.stats.goalkeeper.goals.endpoint, '/v1/national/stats/goalkeeper/goals')\nself.assertEqual(self.client.natl.stats.goalkeeper.shots.endpoint, '/v1/national/stats/goalkeeper/shots')\nself.assertEqual(self.client.natl.stats.goalkeeper.saves.endpoint, '/v1/national/stats/goalkeeper/saves')\n\nself.assertEqual(self.client.natl.stats.passes.directions.endpoint, '/v1/national/stats/passes/directions')\nself.assertEqual(self.client.natl.stats.passes.lengths.endpoint, '/v1/national/stats/passes/lengths')\nself.assertEqual(self.client.natl.stats.passes.locations.endpoint, '/v1/national/stats/passes/locations')\nself.assertEqual(self.client.natl.stats.passes.totals.endpoint, '/v1/national/stats/passes/totals')\n\nself.assertEqual(self.client.natl.stats.setpieces.corners.endpoint, '/v1/national/stats/setpieces/corners')\nself.assertEqual(self.client.natl.stats.setpieces.freekicks.endpoint, '/v1/national/stats/setpieces/freekicks')\nself.assertEqual(self.client.natl.stats.setpieces.throwins.endpoint, '/v1/national/stats/setpieces/throwins')\n\nself.assertEqual(self.client.natl.stats.shots.bodyparts.endpoint, '/v1/national/stats/shots/bodyparts')\nself.assertEqual(self.client.natl.stats.shots.locations.endpoint, '/v1/national/stats/shots/locations')\nself.assertEqual(self.client.natl.stats.shots.plays.endpoint, '/v1/national/stats/shots/plays')\nself.assertEqual(self.client.natl.stats.shots.totals.endpoint, '/v1/national/stats/shots/totals')\n\nself.assertEqual(self.client.natl.stats.touches.duels.endpoint, '/v1/national/stats/touches/duels')\nself.assertEqual(self.client.natl.stats.touches.locations.endpoint, '/v1/national/stats/touches/locations')\nself.assertEqual(self.client.natl.stats.touches.totals.endpoint, '/v1/national/stats/touches/totals')", "path": "tests\\test_endpoints.py", "repo_name": "soccermetrics/soccermetrics-client-py", "stars": 71, "license": "mit", "language": "python", "size": 9102}
{"docstring": "\"\"\"Verify club match micro-event endpoints without match or record IDs.\"\"\"\n", "func_signal": "def test_club_events_endpoints(self):\n", "code": "self.assertEqual(self.client.club.events.all.endpoint, '/v1/clubs/events/all')\nself.assertEqual(self.client.club.events.touches.endpoint, '/v1/clubs/events/touches')\nself.assertEqual(self.client.club.events.actions.endpoint, '/v1/clubs/events/actions')", "path": "tests\\test_endpoints.py", "repo_name": "soccermetrics/soccermetrics-client-py", "stars": 71, "license": "mit", "language": "python", "size": 9102}
{"docstring": "\"\"\"\nConstructor of PassingStatistics class.\n\n:param play: Type of teams playing in matches.\n:type play: string\n:param base_uri: Base URI of API.\n:type base_uri: string\n:param auth: Authentication credential.\n:type auth: tuple\n\"\"\"\n", "func_signal": "def __init__(self, play, base_uri, auth):\n", "code": "super(PassingStatistics, self).__init__()\n\nstatistic = \"passes\"\n\nself.directions = MatchStatisticsResource(play, statistic, \"directions\", base_uri, auth)\nself.lengths = MatchStatisticsResource(play, statistic, \"lengths\", base_uri, auth)\nself.locations = MatchStatisticsResource(play, statistic, \"locations\", base_uri, auth)\nself.totals = MatchStatisticsResource(play, statistic, \"totals\", base_uri, auth)", "path": "soccermetrics\\rest\\resources\\statistics.py", "repo_name": "soccermetrics/soccermetrics-client-py", "stars": 71, "license": "mit", "language": "python", "size": 9102}
{"docstring": "\"\"\"\nConstructor of MatchStatistics class.\n\n:param play: Type of teams playing in matches.\n:type play: string\n:param base_uri: Base URI of API.\n:type base_uri: string\n:param auth: Authentication credential.\n:type auth: tuple\n\"\"\"\n\n", "func_signal": "def __init__(self, play, base_uri, auth):\n", "code": "self.crosses = CrossingStatistics(play, base_uri, auth)\nself.defense = DefensiveStatistics(play, base_uri, auth)\nself.fouls = FoulingStatistics(play, base_uri, auth)\nself.goals = GoalStatistics(play, base_uri, auth)\nself.goalkeeper = GoalkeepingStatistics(play, base_uri, auth)\nself.passes = PassingStatistics(play, base_uri, auth)\nself.setpieces = SetPieceStatistics(play, base_uri, auth)\nself.shots = ShotStatistics(play, base_uri, auth)\nself.touches = TouchStatistics(play, base_uri, auth)", "path": "soccermetrics\\rest\\resources\\statistics.py", "repo_name": "soccermetrics/soccermetrics-client-py", "stars": 71, "license": "mit", "language": "python", "size": 9102}
{"docstring": "\"\"\"Constructor of SoccermetricsRestException.\n\n:param status: HTTP status code\n:type status: int\n:param uri: URI sent when exception was raised\n:type uri: string\n:param msg: Detailed error message\n:type msg: string or \"\"\n\"\"\"\n", "func_signal": "def __init__(self,status,uri,msg=\"\"):\n", "code": "self.uri = uri\nself.status = status\nself.msg = msg", "path": "soccermetrics\\__init__.py", "repo_name": "soccermetrics/soccermetrics-client-py", "stars": 71, "license": "mit", "language": "python", "size": 9102}
{"docstring": "\"\"\"\nConstructor of GoalStatistics class.\n\n:param play: Type of teams playing in matches.\n:type play: string\n:param base_uri: Base URI of API.\n:type base_uri: string\n:param auth: Authentication credential.\n:type auth: tuple\n\"\"\"\n", "func_signal": "def __init__(self, play, base_uri, auth):\n", "code": "super(GoalStatistics, self).__init__()\n\nstatistic = \"goals\"\n\nself.assists = MatchStatisticsResource(play, statistic, \"assists\", base_uri, auth)\nself.bodyparts = MatchStatisticsResource(play, statistic, \"bodyparts\", base_uri, auth)\nself.locations = MatchStatisticsResource(play, statistic, \"locations\", base_uri, auth)\nself.penalties = MatchStatisticsResource(play, statistic, \"penalties\", base_uri, auth)\nself.totals = MatchStatisticsResource(play, statistic, \"totals\", base_uri, auth)", "path": "soccermetrics\\rest\\resources\\statistics.py", "repo_name": "soccermetrics/soccermetrics-client-py", "stars": 71, "license": "mit", "language": "python", "size": 9102}
{"docstring": "\"\"\"Verify club match resource endpoints without match ID.\"\"\"\n", "func_signal": "def test_club_match_endpoints(self):\n", "code": "self.assertEqual(self.client.club.information.EndpointURI(), '/v1/clubs/matches/info')\nself.assertEqual(self.client.club.lineups.EndpointURI(), '/v1/clubs/matches/lineups')\nself.assertEqual(self.client.club.conditions.EndpointURI(), '/v1/clubs/matches/conditions')\nself.assertEqual(self.client.club.goals.EndpointURI(), '/v1/clubs/matches/goals')\nself.assertEqual(self.client.club.penalties.EndpointURI(), '/v1/clubs/matches/penalties')\nself.assertEqual(self.client.club.offenses.EndpointURI(), '/v1/clubs/matches/offenses')\nself.assertEqual(self.client.club.substitutions.EndpointURI(), '/v1/clubs/matches/substitutions')\nself.assertEqual(self.client.club.shootouts.EndpointURI(), '/v1/clubs/matches/shootouts')", "path": "tests\\test_endpoints.py", "repo_name": "soccermetrics/soccermetrics-client-py", "stars": 71, "license": "mit", "language": "python", "size": 9102}
{"docstring": "\"\"\"Verify national team match micro-event endpoints without match or record IDs.\"\"\"\n", "func_signal": "def test_natl_events_endpoints(self):\n", "code": "self.assertEqual(self.client.natl.events.all.endpoint, '/v1/national/events/all')\nself.assertEqual(self.client.natl.events.touches.endpoint, '/v1/national/events/touches')\nself.assertEqual(self.client.natl.events.actions.endpoint, '/v1/national/events/actions')", "path": "tests\\test_endpoints.py", "repo_name": "soccermetrics/soccermetrics-client-py", "stars": 71, "license": "mit", "language": "python", "size": 9102}
{"docstring": "\"\"\"\nConstructor of CrossingStatistics class.\n\n:param play: Type of teams playing in matches.\n:type play: string\n:param base_uri: Base URI of API.\n:type base_uri: string\n:param auth: Authentication credential.\n:type auth: tuple\n\"\"\"\n", "func_signal": "def __init__(self, play, base_uri, auth):\n", "code": "super(CrossingStatistics, self).__init__()\n\nstatistic = \"crosses\"\n\nself.corners = MatchStatisticsResource(play, statistic, \"corners\", base_uri, auth)\nself.totals = MatchStatisticsResource(play, statistic, \"totals\", base_uri, auth)", "path": "soccermetrics\\rest\\resources\\statistics.py", "repo_name": "soccermetrics/soccermetrics-client-py", "stars": 71, "license": "mit", "language": "python", "size": 9102}
{"docstring": "\"\"\"Verify club match statistics endpoints without match or record IDs.\"\"\"\n", "func_signal": "def test_club_statistics_endpoints(self):\n", "code": "self.assertEqual(self.client.club.stats.crosses.corners.endpoint, '/v1/clubs/stats/crosses/corners')\nself.assertEqual(self.client.club.stats.crosses.totals.endpoint, '/v1/clubs/stats/crosses/totals')\n\nself.assertEqual(self.client.club.stats.defense.actions.endpoint, '/v1/clubs/stats/defense/actions')\nself.assertEqual(self.client.club.stats.defense.blocks.endpoint, '/v1/clubs/stats/defense/blocks')\nself.assertEqual(self.client.club.stats.defense.clearances.endpoint, '/v1/clubs/stats/defense/clearances')\nself.assertEqual(self.client.club.stats.defense.goalline.endpoint, '/v1/clubs/stats/defense/goalline')\nself.assertEqual(self.client.club.stats.defense.tackles.endpoint, '/v1/clubs/stats/defense/tackles')\n\nself.assertEqual(self.client.club.stats.fouls.cards.endpoint, '/v1/clubs/stats/fouls/cards')\nself.assertEqual(self.client.club.stats.fouls.wins.endpoint, '/v1/clubs/stats/fouls/wins')\n\nself.assertEqual(self.client.club.stats.goals.assists.endpoint, '/v1/clubs/stats/goals/assists')\nself.assertEqual(self.client.club.stats.goals.bodyparts.endpoint, '/v1/clubs/stats/goals/bodyparts')\nself.assertEqual(self.client.club.stats.goals.locations.endpoint, '/v1/clubs/stats/goals/locations')\nself.assertEqual(self.client.club.stats.goals.penalties.endpoint, '/v1/clubs/stats/goals/penalties')\nself.assertEqual(self.client.club.stats.goals.totals.endpoint, '/v1/clubs/stats/goals/totals')\n\nself.assertEqual(self.client.club.stats.goalkeeper.actions.endpoint, '/v1/clubs/stats/goalkeeper/actions')\nself.assertEqual(self.client.club.stats.goalkeeper.goals.endpoint, '/v1/clubs/stats/goalkeeper/goals')\nself.assertEqual(self.client.club.stats.goalkeeper.shots.endpoint, '/v1/clubs/stats/goalkeeper/shots')\nself.assertEqual(self.client.club.stats.goalkeeper.saves.endpoint, '/v1/clubs/stats/goalkeeper/saves')\n\nself.assertEqual(self.client.club.stats.passes.directions.endpoint, '/v1/clubs/stats/passes/directions')\nself.assertEqual(self.client.club.stats.passes.lengths.endpoint, '/v1/clubs/stats/passes/lengths')\nself.assertEqual(self.client.club.stats.passes.locations.endpoint, '/v1/clubs/stats/passes/locations')\nself.assertEqual(self.client.club.stats.passes.totals.endpoint, '/v1/clubs/stats/passes/totals')\n\nself.assertEqual(self.client.club.stats.setpieces.corners.endpoint, '/v1/clubs/stats/setpieces/corners')\nself.assertEqual(self.client.club.stats.setpieces.freekicks.endpoint, '/v1/clubs/stats/setpieces/freekicks')\nself.assertEqual(self.client.club.stats.setpieces.throwins.endpoint, '/v1/clubs/stats/setpieces/throwins')\n\nself.assertEqual(self.client.club.stats.shots.bodyparts.endpoint, '/v1/clubs/stats/shots/bodyparts')\nself.assertEqual(self.client.club.stats.shots.locations.endpoint, '/v1/clubs/stats/shots/locations')\nself.assertEqual(self.client.club.stats.shots.plays.endpoint, '/v1/clubs/stats/shots/plays')\nself.assertEqual(self.client.club.stats.shots.totals.endpoint, '/v1/clubs/stats/shots/totals')\n\nself.assertEqual(self.client.club.stats.touches.duels.endpoint, '/v1/clubs/stats/touches/duels')\nself.assertEqual(self.client.club.stats.touches.locations.endpoint, '/v1/clubs/stats/touches/locations')\nself.assertEqual(self.client.club.stats.touches.totals.endpoint, '/v1/clubs/stats/touches/totals')", "path": "tests\\test_endpoints.py", "repo_name": "soccermetrics/soccermetrics-client-py", "stars": 71, "license": "mit", "language": "python", "size": 9102}
{"docstring": "\"\"\"\nConstructor of MatchStatisticsResource class.\n\n:param play: Type of teams playing in matches.\n:type play: string\n:param statistic: Statistic type.\n:type statistic: string\n:param resource: Name of resource.\n:type resource: string\n:param base_uri: Base URI of API.\n:type base_uri: string\n:param auth: Authentication credential.\n:type auth: tuple\n\"\"\"\n", "func_signal": "def __init__(self, play, statistic, resource, base_uri, auth):\n", "code": "super(MatchStatisticsResource, self).__init__(base_uri,auth)\n\nself.endpoint += \"/%s/stats/%s/%s\" % (play, statistic, resource)", "path": "soccermetrics\\rest\\resources\\statistics.py", "repo_name": "soccermetrics/soccermetrics-client-py", "stars": 71, "license": "mit", "language": "python", "size": 9102}
{"docstring": "\"\"\"\nConstructor of TouchStatistics class.\n\n:param play: Type of teams playing in matches.\n:type play: string\n:param base_uri: Base URI of API.\n:type base_uri: string\n:param auth: Authentication credential.\n:type auth: tuple\n\"\"\"\n", "func_signal": "def __init__(self, play, base_uri, auth):\n", "code": "super(TouchStatistics, self).__init__()\n\nstatistic = \"touches\"\n\nself.duels = MatchStatisticsResource(play, statistic, \"duels\", base_uri, auth)\nself.locations = MatchStatisticsResource(play, statistic, \"locations\", base_uri, auth)\nself.totals = MatchStatisticsResource(play, statistic, \"totals\", base_uri, auth)", "path": "soccermetrics\\rest\\resources\\statistics.py", "repo_name": "soccermetrics/soccermetrics-client-py", "stars": 71, "license": "mit", "language": "python", "size": 9102}
{"docstring": "\"\"\"Verify national team match resource endpoints without match ID.\"\"\"\n", "func_signal": "def test_national_team_match_endpoints(self):\n", "code": "self.assertEqual(self.client.natl.information.EndpointURI(), '/v1/national/matches/info')\nself.assertEqual(self.client.natl.lineups.EndpointURI(), '/v1/national/matches/lineups')\nself.assertEqual(self.client.natl.conditions.EndpointURI(), '/v1/national/matches/conditions')\nself.assertEqual(self.client.natl.goals.EndpointURI(), '/v1/national/matches/goals')\nself.assertEqual(self.client.natl.penalties.EndpointURI(), '/v1/national/matches/penalties')\nself.assertEqual(self.client.natl.offenses.EndpointURI(), '/v1/national/matches/offenses')\nself.assertEqual(self.client.natl.substitutions.EndpointURI(), '/v1/national/matches/substitutions')\nself.assertEqual(self.client.natl.shootouts.EndpointURI(), '/v1/national/matches/shootouts')", "path": "tests\\test_endpoints.py", "repo_name": "soccermetrics/soccermetrics-client-py", "stars": 71, "license": "mit", "language": "python", "size": 9102}
{"docstring": "\"\"\"Verify personnel resource endpoints without ID.\"\"\"\n", "func_signal": "def test_personnel_endpoints(self):\n", "code": "self.assertEqual(self.client.players.endpoint, '/v1/personnel/players')\nself.assertEqual(self.client.managers.endpoint, '/v1/personnel/managers')\nself.assertEqual(self.client.referees.endpoint, '/v1/personnel/referees')", "path": "tests\\test_endpoints.py", "repo_name": "soccermetrics/soccermetrics-client-py", "stars": 71, "license": "mit", "language": "python", "size": 9102}
{"docstring": "\"\"\"Verify analytics endpoints without match ID.\"\"\"\n", "func_signal": "def test_analytics_endpoints(self):\n", "code": "self.assertEqual(self.client.analytics.state.EndpointURI(), '/v1/analytics/match/state')\nself.assertEqual(self.client.analytics.segment.EndpointURI(), '/v1/analytics/match/segment')\nself.assertEqual(self.client.analytics.tsr.EndpointURI(), '/v1/analytics/match/tsr')", "path": "tests\\test_endpoints.py", "repo_name": "soccermetrics/soccermetrics-client-py", "stars": 71, "license": "mit", "language": "python", "size": 9102}
{"docstring": "\"\"\"Verify validation resource endpoints without ID.\"\"\"\n", "func_signal": "def test_validation_endpoints(self):\n", "code": "self.assertEqual(self.client.validation.phases.endpoint, \"/v1/phases\")\nself.assertEqual(self.client.validation.groupRounds.endpoint, '/v1/grouprounds')\nself.assertEqual(self.client.validation.knockoutRounds.endpoint, '/v1/knockoutrounds')\nself.assertEqual(self.client.validation.confederations.endpoint, '/v1/confederations')\nself.assertEqual(self.client.validation.countries.endpoint, '/v1/countries')\nself.assertEqual(self.client.validation.competitions.endpoint, '/v1/competitions')\nself.assertEqual(self.client.validation.domesticCompetitions.endpoint, '/v1/domestic_competitions')\nself.assertEqual(self.client.validation.intlCompetitions.endpoint, '/v1/intl_competitions')\nself.assertEqual(self.client.validation.seasons.endpoint, '/v1/seasons')\nself.assertEqual(self.client.validation.teams.endpoint, '/v1/teams')\nself.assertEqual(self.client.validation.venues.endpoint, '/v1/venues')\nself.assertEqual(self.client.validation.timezones.endpoint, '/v1/timezones')\nself.assertEqual(self.client.validation.nameOrder.endpoint, '/v1/name_order')\nself.assertEqual(self.client.validation.persons.endpoint, '/v1/persons')\nself.assertEqual(self.client.validation.positions.endpoint, '/v1/positions')\nself.assertEqual(self.client.validation.fouls.endpoint, '/v1/fouls')\nself.assertEqual(self.client.validation.cards.endpoint, '/v1/cards')\nself.assertEqual(self.client.validation.bodyparts.endpoint, '/v1/bodyparts')\nself.assertEqual(self.client.validation.shotevents.endpoint, '/v1/shotevents')\nself.assertEqual(self.client.validation.penaltyOutcomes.endpoint, '/v1/penalty_outcomes')\nself.assertEqual(self.client.validation.actions.endpoint, '/v1/actions')\nself.assertEqual(self.client.validation.modifiers.endpoint, '/v1/modifiers')\nself.assertEqual(self.client.validation.modifierCategories.endpoint, '/v1/modifier_categories')\nself.assertEqual(self.client.validation.weather.endpoint, '/v1/weather')\nself.assertEqual(self.client.validation.surfaces.endpoint, '/v1/surfaces')", "path": "tests\\test_endpoints.py", "repo_name": "soccermetrics/soccermetrics-client-py", "stars": 71, "license": "mit", "language": "python", "size": 9102}
{"docstring": "\"\"\"\nConstructor of DefensiveStatistics class.\n\n:param play: Type of teams playing in matches.\n:type play: string\n:param base_uri: Base URI of API.\n:type base_uri: string\n:param auth: Authentication credential.\n:type auth: tuple\n\"\"\"\n", "func_signal": "def __init__(self, play, base_uri, auth):\n", "code": "super(DefensiveStatistics, self).__init__()\n\nstatistic = \"defense\"\n\nself.actions = MatchStatisticsResource(play, statistic, \"actions\", base_uri, auth)\nself.blocks = MatchStatisticsResource(play, statistic, \"blocks\", base_uri, auth)\nself.clearances = MatchStatisticsResource(play, statistic, \"clearances\", base_uri, auth)\nself.goalline = MatchStatisticsResource(play, statistic, \"goalline\", base_uri, auth)\nself.tackles = MatchStatisticsResource(play, statistic, \"tackles\", base_uri, auth)", "path": "soccermetrics\\rest\\resources\\statistics.py", "repo_name": "soccermetrics/soccermetrics-client-py", "stars": 71, "license": "mit", "language": "python", "size": 9102}
{"docstring": "\"\"\"\nConstructor of GoalkeepingStatistics class.\n\n:param play: Type of teams playing in matches.\n:type play: string\n:param base_uri: Base URI of API.\n:type base_uri: string\n:param auth: Authentication credential.\n:type auth: tuple\n\"\"\"\n", "func_signal": "def __init__(self, play, base_uri, auth):\n", "code": "super(GoalkeepingStatistics, self).__init__()\n\nstatistic = \"goalkeeper\"\n\nself.actions = MatchStatisticsResource(play, statistic, \"actions\", base_uri, auth)\nself.goals = MatchStatisticsResource(play, statistic, \"goals\", base_uri, auth)\nself.shots = MatchStatisticsResource(play, statistic, \"shots\", base_uri, auth)\nself.saves = MatchStatisticsResource(play, statistic, \"saves\", base_uri, auth)", "path": "soccermetrics\\rest\\resources\\statistics.py", "repo_name": "soccermetrics/soccermetrics-client-py", "stars": 71, "license": "mit", "language": "python", "size": 9102}
{"docstring": "\"\"\"\nConstructor of ShotStatistics class.\n\n:param play: Type of teams playing in matches.\n:type play: string\n:param base_uri: Base URI of API.\n:type base_uri: string\n:param auth: Authentication credential.\n:type auth: tuple\n\"\"\"\n", "func_signal": "def __init__(self, play, base_uri, auth):\n", "code": "super(ShotStatistics, self).__init__()\n\nstatistic = \"shots\"\n\nself.bodyparts = MatchStatisticsResource(play, statistic, \"bodyparts\", base_uri, auth)\nself.locations = MatchStatisticsResource(play, statistic, \"locations\", base_uri, auth)\nself.plays = MatchStatisticsResource(play, statistic, \"plays\", base_uri, auth)\nself.totals = MatchStatisticsResource(play, statistic, \"totals\", base_uri, auth)", "path": "soccermetrics\\rest\\resources\\statistics.py", "repo_name": "soccermetrics/soccermetrics-client-py", "stars": 71, "license": "mit", "language": "python", "size": 9102}
{"docstring": "\"\"\"\nReturns the key with the highest value.\n\"\"\"\n", "func_signal": "def argMax(self):\n", "code": "if len(self.keys()) == 0: return None\nall = self.items()\nvalues = [x[1] for x in all]\nmaxIndex = values.index(max(values))\nreturn all[maxIndex][0]", "path": "Project 3 - Reinforcement Learning\\util.py", "repo_name": "filR/edX-CS188.1x-Artificial-Intelligence", "stars": 85, "license": "None", "language": "python", "size": 363}
{"docstring": "\"\"\"\nCoordinates are flipped from the input format to the (x,y) convention here\n\nThe shape of the maze.  Each character\nrepresents a different type of object.\n % - Wall\n . - Food\n o - Capsule\n G - Ghost\n P - Pacman\nOther characters are ignored.\n\"\"\"\n", "func_signal": "def processLayoutText(self, layoutText):\n", "code": "maxY = self.height - 1\nfor y in range(self.height):\n    for x in range(self.width):\n        layoutChar = layoutText[maxY - y][x]\n        self.processLayoutChar(x, y, layoutChar)\nself.agentPositions.sort()\nself.agentPositions = [ ( i == 0, pos) for i, pos in self.agentPositions]", "path": "Project 1 - Search in Pacman\\layout.py", "repo_name": "filR/edX-CS188.1x-Artificial-Intelligence", "stars": 85, "license": "None", "language": "python", "size": 363}
{"docstring": "\"\"\"\nTurns a matrix into a list of coordinates matching the specified value\n\"\"\"\n", "func_signal": "def matrixAsList( matrix, value = True ):\n", "code": "rows, cols = len( matrix ), len( matrix[0] )\ncells = []\nfor row in range( rows ):\n    for col in range( cols ):\n        if matrix[row][col] == value:\n            cells.append( ( row, col ) )\nreturn cells", "path": "Project 3 - Reinforcement Learning\\util.py", "repo_name": "filR/edX-CS188.1x-Artificial-Intelligence", "stars": 85, "license": "None", "language": "python", "size": 363}
{"docstring": "\"\"\"\nReturns a list of keys sorted by their values.  Keys\nwith the highest values will appear first.\n\n>>> a = Counter()\n>>> a['first'] = -2\n>>> a['second'] = 4\n>>> a['third'] = 1\n>>> a.sortedKeys()\n['second', 'third', 'first']\n\"\"\"\n", "func_signal": "def sortedKeys(self):\n", "code": "sortedItems = self.items()\ncompare = lambda x, y:  sign(y[1] - x[1])\nsortedItems.sort(cmp=compare)\nreturn [x[0] for x in sortedItems]", "path": "Project 3 - Reinforcement Learning\\util.py", "repo_name": "filR/edX-CS188.1x-Artificial-Intelligence", "stars": 85, "license": "None", "language": "python", "size": 363}
{"docstring": "\"\"\"\nSubtracting a counter from another gives a counter with the union of all keys and\ncounts of the second subtracted from counts of the first.\n\n>>> a = Counter()\n>>> b = Counter()\n>>> a['first'] = -2\n>>> a['second'] = 4\n>>> b['first'] = 3\n>>> b['third'] = 1\n>>> (a - b)['first']\n-5\n\"\"\"\n", "func_signal": "def __sub__( self, y ):\n", "code": "addend = Counter()\nfor key in self:\n    if key in y:\n        addend[key] = self[key] - y[key]\n    else:\n        addend[key] = self[key]\nfor key in y:\n    if key in self:\n        continue\n    addend[key] = -1 * y[key]\nreturn addend", "path": "Project 3 - Reinforcement Learning\\util.py", "repo_name": "filR/edX-CS188.1x-Artificial-Intelligence", "stars": 85, "license": "None", "language": "python", "size": 363}
{"docstring": "\"\"\"\nMultiplying two counters gives the dot product of their vectors where\neach unique label is a vector element.\n\n>>> a = Counter()\n>>> b = Counter()\n>>> a['first'] = -2\n>>> a['second'] = 4\n>>> b['first'] = 3\n>>> b['second'] = 5\n>>> a['third'] = 1.5\n>>> a['fourth'] = 2.5\n>>> a * b\n14\n\"\"\"\n", "func_signal": "def __mul__(self, y ):\n", "code": "sum = 0\nx = self\nif len(x) > len(y):\n    x,y = y,x\nfor key in x:\n    if key not in y:\n        continue\n    sum += x[key] * y[key]\nreturn sum", "path": "Project 3 - Reinforcement Learning\\util.py", "repo_name": "filR/edX-CS188.1x-Artificial-Intelligence", "stars": 85, "license": "None", "language": "python", "size": 363}
{"docstring": "\"\"\"\nnormalize a vector or counter by dividing each value by the sum of all values\n\"\"\"\n", "func_signal": "def normalize(vectorOrCounter):\n", "code": "normalizedCounter = Counter()\nif type(vectorOrCounter) == type(normalizedCounter):\n    counter = vectorOrCounter\n    total = float(counter.totalCount())\n    if total == 0: return counter\n    for key in counter.keys():\n        value = counter[key]\n        normalizedCounter[key] = value / total\n    return normalizedCounter\nelse:\n    vector = vectorOrCounter\n    s = float(sum(vector))\n    if s == 0: return vector\n    return [el / s for el in vector]", "path": "Project 3 - Reinforcement Learning\\util.py", "repo_name": "filR/edX-CS188.1x-Artificial-Intelligence", "stars": 85, "license": "None", "language": "python", "size": 363}
{"docstring": "\"\"\"\nFinds the nearest grid point to a position (discretizes).\n\"\"\"\n", "func_signal": "def nearestPoint( pos ):\n", "code": "( current_row, current_col ) = pos\n\ngrid_row = int( current_row + 0.5 )\ngrid_col = int( current_col + 0.5 )\nreturn ( grid_row, grid_col )", "path": "Project 3 - Reinforcement Learning\\util.py", "repo_name": "filR/edX-CS188.1x-Artificial-Intelligence", "stars": 85, "license": "None", "language": "python", "size": 363}
{"docstring": "\"\"\"\nEdits the counter such that the total count of all\nkeys sums to 1.  The ratio of counts for all keys\nwill remain the same. Note that normalizing an empty\nCounter will result in an error.\n\"\"\"\n", "func_signal": "def normalize(self):\n", "code": "total = float(self.totalCount())\nif total == 0: return\nfor key in self.keys():\n    self[key] = self[key] / total", "path": "Project 3 - Reinforcement Learning\\util.py", "repo_name": "filR/edX-CS188.1x-Artificial-Intelligence", "stars": 85, "license": "None", "language": "python", "size": 363}
{"docstring": "\"\"\"\nAdding two counters gives a counter with the union of all keys and\ncounts of the second added to counts of the first.\n\n>>> a = Counter()\n>>> b = Counter()\n>>> a['first'] = -2\n>>> a['second'] = 4\n>>> b['first'] = 3\n>>> b['third'] = 1\n>>> (a + b)['first']\n1\n\"\"\"\n", "func_signal": "def __add__( self, y ):\n", "code": "addend = Counter()\nfor key in self:\n    if key in y:\n        addend[key] = self[key] + y[key]\n    else:\n        addend[key] = self[key]\nfor key in y:\n    if key in self:\n        continue\n    addend[key] = y[key]\nreturn addend", "path": "Project 3 - Reinforcement Learning\\util.py", "repo_name": "filR/edX-CS188.1x-Artificial-Intelligence", "stars": 85, "license": "None", "language": "python", "size": 363}
{"docstring": "\"\"\"\nMethod to format the exception message, this is more complicated because\nwe need to cgi.escape the traceback but wrap the exception in a <pre> tag\n\"\"\"\n", "func_signal": "def addExceptionMessage(self, q, inst, traceback):\n", "code": "self.fail('FAIL: Exception raised: %s' % inst)\nself.addMessage('')\nfor line in traceback.format_exc().split('\\n'):\n    self.addMessage(line)", "path": "Project 2 - Multi-Agent Pacman\\grading.py", "repo_name": "filR/edX-CS188.1x-Artificial-Intelligence", "stars": 85, "license": "None", "language": "python", "size": 363}
{"docstring": "\"\"\"\nInverts a matrix stored as a list of lists.\n\"\"\"\n", "func_signal": "def arrayInvert(array):\n", "code": "result = [[] for i in array]\nfor outer in array:\n    for inner in range(len(outer)):\n        result[inner].append(outer[inner])\nreturn result", "path": "Project 3 - Reinforcement Learning\\util.py", "repo_name": "filR/edX-CS188.1x-Artificial-Intelligence", "stars": 85, "license": "None", "language": "python", "size": 363}
{"docstring": "\"\"\"\nIncrements all elements of keys by the same count.\n\n>>> a = Counter()\n>>> a.incrementAll(['one','two', 'three'], 1)\n>>> a['one']\n1\n>>> a['two']\n1\n\"\"\"\n", "func_signal": "def incrementAll(self, keys, count):\n", "code": "for key in keys:\n    self[key] += count", "path": "Project 3 - Reinforcement Learning\\util.py", "repo_name": "filR/edX-CS188.1x-Artificial-Intelligence", "stars": 85, "license": "None", "language": "python", "size": 363}
{"docstring": "\"\"\"\nDivides all counts by divisor\n\"\"\"\n", "func_signal": "def divideAll(self, divisor):\n", "code": "divisor = float(divisor)\nfor key in self:\n    self[key] /= divisor", "path": "Project 3 - Reinforcement Learning\\util.py", "repo_name": "filR/edX-CS188.1x-Artificial-Intelligence", "stars": 85, "license": "None", "language": "python", "size": 363}
{"docstring": "\"\"\"\n  Gives the probability of a value under a discrete distribution\n  defined by (distributions, values).\n\"\"\"\n", "func_signal": "def getProbability(value, distribution, values):\n", "code": "total = 0.0\nfor prob, val in zip(distribution, values):\n    if val == value:\n        total += prob\nreturn total", "path": "Project 3 - Reinforcement Learning\\util.py", "repo_name": "filR/edX-CS188.1x-Artificial-Intelligence", "stars": 85, "license": "None", "language": "python", "size": 363}
{"docstring": "\"\"\"\nAdding another counter to a counter increments the current counter\nby the values stored in the second counter.\n\n>>> a = Counter()\n>>> b = Counter()\n>>> a['first'] = -2\n>>> a['second'] = 4\n>>> b['first'] = 3\n>>> b['third'] = 1\n>>> a += b\n>>> a['first']\n1\n\"\"\"\n", "func_signal": "def __radd__(self, y):\n", "code": "for key, value in y.items():\n    self[key] += value", "path": "Project 3 - Reinforcement Learning\\util.py", "repo_name": "filR/edX-CS188.1x-Artificial-Intelligence", "stars": 85, "license": "None", "language": "python", "size": 363}
{"docstring": "# FIXME: restored old behaviour to check against old results better\n# FIXED: restored to stable behaviour\n", "func_signal": "def push(self, item, priority):\n", "code": "entry = (priority, self.count, item)\n# entry = (priority, item)\nheapq.heappush(self.heap, entry)\nself.count += 1", "path": "Project 3 - Reinforcement Learning\\util.py", "repo_name": "filR/edX-CS188.1x-Artificial-Intelligence", "stars": 85, "license": "None", "language": "python", "size": 363}
{"docstring": "## Window ##\n", "func_signal": "def __initGUI(self, win):\n", "code": "self.win = win\n\n## Initialize Frame ##\nwin.grid()\nself.dec = -.5\nself.inc = .5\nself.tickTime = 0.1\n\n## Epsilon Button + Label ##\nself.setupSpeedButtonAndLabel(win)\n\nself.setupEpsilonButtonAndLabel(win)\n\n## Gamma Button + Label ##\nself.setUpGammaButtonAndLabel(win)\n\n## Alpha Button + Label ##\nself.setupAlphaButtonAndLabel(win)\n\n## Exit Button ##\n#self.exit_button = Tkinter.Button(win,text='Quit', command=self.exit)\n#self.exit_button.grid(row=0, column=9)\n\n## Simulation Buttons ##", "path": "Project 3 - Reinforcement Learning\\graphicsCrawlerDisplay.py", "repo_name": "filR/edX-CS188.1x-Artificial-Intelligence", "stars": 85, "license": "None", "language": "python", "size": 363}
{"docstring": "\"\"\"\nReturns 1 or -1 depending on the sign of x\n\"\"\"\n", "func_signal": "def sign( x ):\n", "code": "if( x >= 0 ):\n    return 1\nelse:\n    return -1", "path": "Project 3 - Reinforcement Learning\\util.py", "repo_name": "filR/edX-CS188.1x-Artificial-Intelligence", "stars": 85, "license": "None", "language": "python", "size": 363}
{"docstring": "# If we have SIGALRM signal, use it to cause an exception if and\n# when this function runs too long.  Otherwise check the time taken\n# after the method has returned, and throw an exception then.\n", "func_signal": "def __call__(self, *args, **keyArgs):\n", "code": "if hasattr(signal, 'SIGALRM'):\n    old = signal.signal(signal.SIGALRM, self.handle_timeout)\n    signal.alarm(self.timeout)\n    try:\n        result = self.function(*args, **keyArgs)\n    finally:\n        signal.signal(signal.SIGALRM, old)\n    signal.alarm(0)\nelse:\n    startTime = time.time()\n    result = self.function(*args, **keyArgs)\n    timeElapsed = time.time() - startTime\n    if timeElapsed >= self.timeout:\n        self.handle_timeout(None, None)\nreturn result", "path": "Project 3 - Reinforcement Learning\\util.py", "repo_name": "filR/edX-CS188.1x-Artificial-Intelligence", "stars": 85, "license": "None", "language": "python", "size": 363}
{"docstring": "# noon, -7 hours off UTC, as America/Los_Angeles in summer\n", "func_signal": "def test_getstate(self):\n", "code": "tz = Timezone.generate(\"America/Los_Angeles\")\nw = Wait(43200, tz)\n\nassert w.__getstate__() == (43200, tz.soul)", "path": "pygs\\test\\unit_test\\test_wait.py", "repo_name": "bmander/graphserver", "stars": 122, "license": "other", "language": "python", "size": 35348}
{"docstring": "#Split each line segment in the linestring into segment smaller than max_section_length\n", "func_signal": "def split_line_string(points, max_section_length):\n", "code": "    split_segs = []\n    for (lng1, lat1), (lng2,lat2) in cons(points):\n        split_seg = list(split_line_segment(lng1, lat1, lng2, lat2, max_section_length))\n        split_segs.append( split_seg )\n#String together the sub linestrings into a single linestring\n    ret = []\n    segstart_s = 0\n    for i, split_seg in enumerate(split_segs):\n        for x, y, s in split_seg[:-1]:\n            ret.append( (x, y, s+segstart_s) )\n        \n        if i==len(split_segs)-1:\n            x, y, s = split_seg[-1]\n            ret.append( (x, y, s+segstart_s) )\n        \n        segstart_s += split_seg[-1][2]\n            \n    return ret", "path": "pygs\\graphserver\\ext\\ned\\elevation\\elevation.py", "repo_name": "bmander/graphserver", "stars": 122, "license": "other", "language": "python", "size": 35348}
{"docstring": "\"\"\"generates (vertex1_label, vertex2_label, edgepayload) from osmdb\"\"\"\n\n", "func_signal": "def edges_from_osmdb(osmdb, vertex_namespace, slogs, profiledb=None):\n", "code": "street_id_counter = 0\nstreet_names = {}\n\n# for each edge in the osmdb\nfor i, (id, parent_id, node1, node2, distance, geom, tags) in enumerate( osmdb.edges() ):\n        \n    # Find rise/fall of edge, if profiledb is given\n    rise=0\n    fall=0\n    if profiledb:\n        profile = profiledb.get( id )\n        if profile:\n            rise, fall = get_rise_and_fall( profile )\n    \n    # insert end vertices of edge to graph\n    vertex1_label = \"%s-%s\"%(vertex_namespace,node1)\n    vertex2_label = \"%s-%s\"%(vertex_namespace,node2)\n            \n    # create ID for the way's street\n    street_name = tags.get(\"name\")\n    if street_name is None:\n        street_id_counter += 1\n        street_id = street_id_counter\n    else:\n        if street_name not in street_names:\n            street_id_counter += 1\n            street_names[street_name] = street_id_counter\n        street_id = street_names[street_name]\n    \n    # Create edges to be inserted into graph\n    s1 = Street( id, distance, rise, fall )\n    s2 = Street( id, distance, fall, rise, reverse_of_source=True )\n    s1.way = street_id\n    s2.way = street_id\n    \n    # See if the way's highway tag is penalized with a 'slog' value; if so, set it in the edges\n    slog = slogs.get( tags.get(\"highway\") )\n    if slog:\n        s1.slog = s2.slog = slog\n    \n    # Add the forward edge and the return edge if the edge is not oneway", "path": "pygs\\graphserver\\compiler\\gdb_import_osm.py", "repo_name": "bmander/graphserver", "stars": 122, "license": "other", "language": "python", "size": 35348}
{"docstring": "# Create stoptimes table\n", "func_signal": "def create_table(cc, gtfs_basename, header):\n", "code": "sqlite_field_definitions = [\"%s %s\"%(field_name, field_type if field_type else \"TEXT\") for field_name, field_type, field_converter in header]\ncc.execute(\"create table %s (%s)\"%(gtfs_basename,\",\".join(sqlite_field_definitions)))", "path": "pygs\\graphserver\\ext\\gtfs\\gtfsdb.py", "repo_name": "bmander/graphserver", "stars": 122, "license": "other", "language": "python", "size": 35348}
{"docstring": "\"\"\"get a vertex, edge after adding a single segment\"\"\"\n\n", "func_signal": "def test_one(self):\n", "code": "ee = Edge(self.aa, self.bb, self.ep)\nself.path.addSegment( self.bb, ee )\n\n# out of bounds\nself.assertRaises( IndexError, self.path.getVertex, -1 )\nself.assertRaises( IndexError, self.path.getEdge, -1 )\n\n# vertices in bounds\nself.assertEqual( self.path.getVertex(0).soul, self.aa.soul )\nself.assertEqual( self.path.getVertex(1).soul, self.bb.soul )\n\n# edges in bounds\nself.assertEqual( self.path.getEdge(0).soul, ee.soul )\n\n# out of bounds again\nself.assertRaises( IndexError, self.path.getVertex, 2 )\nself.assertRaises( IndexError, self.path.getEdge, 1 )", "path": "pygs\\test\\unit_test\\test_path.py", "repo_name": "bmander/graphserver", "stars": 122, "license": "other", "language": "python", "size": 35348}
{"docstring": "\"\"\"Create a path object without crashing\"\"\"\n", "func_signal": "def test_path_new(self):\n", "code": "path = Path( Vertex(\"A\") )\n\nself.assertTrue( path )", "path": "pygs\\test\\unit_test\\test_path.py", "repo_name": "bmander/graphserver", "stars": 122, "license": "other", "language": "python", "size": 35348}
{"docstring": "# Attempts to get a cursor using the current connection to the db. If we've found ourselves in a different thread\n# than that which the connection was made in, re-make the connection.\n\n", "func_signal": "def get_cursor(self):\n", "code": "try:\n    ret = self.conn.cursor()\nexcept sqlite3.ProgrammingError:\n    self.conn = sqlite3.connect(self.dbname)\n    ret = self.conn.cursor()\n    \nreturn ret", "path": "pygs\\graphserver\\ext\\gtfs\\gtfsdb.py", "repo_name": "bmander/graphserver", "stars": 122, "license": "other", "language": "python", "size": 35348}
{"docstring": "\"\"\"behave appropriately when asking for an out-of-bounds index\"\"\"\n\n# test out of bounds values\n", "func_signal": "def test_none(self):\n", "code": "self.assertRaises( IndexError, self.path.getVertex, -1 )\nself.assertRaises( IndexError, self.path.getVertex, 1 )\nself.assertRaises( IndexError, self.path.getVertex, 10 )\n\nself.assertRaises( IndexError, self.path.getEdge, -1 )\nself.assertRaises( IndexError, self.path.getEdge, 0 )\nself.assertRaises( IndexError, self.path.getEdge, 1 )\nself.assertRaises( IndexError, self.path.getEdge, 10 )\n\n# if you don't add any segments, there's still a single vertex in the path\nself.assertEquals( self.path.getVertex( 0 ).soul, self.aa.soul )", "path": "pygs\\test\\unit_test\\test_path.py", "repo_name": "bmander/graphserver", "stars": 122, "license": "other", "language": "python", "size": 35348}
{"docstring": "\"\"\"getSize returns ten after ten entries\"\"\"\n\n", "func_signal": "def test_ten(self):\n", "code": "for i in range(10):\n    aa = Vertex(\"AA\")\n    bb = Vertex(\"BB\")\n    payload = Link()\n    self.path.addSegment( bb, Edge(aa, bb, payload) )\n    \nself.assertEquals( self.path.num_elements, 10 )", "path": "pygs\\test\\unit_test\\test_path.py", "repo_name": "bmander/graphserver", "stars": 122, "license": "other", "language": "python", "size": 35348}
{"docstring": "\"\"\"ShortestPathTree.path_retro works on a trivial graph\"\"\"\n\n", "func_signal": "def test_path_retro_basic(self):\n", "code": "vertices, edges = self.spt.path_retro( \"B\" )\n\nself.assertEqual( vertices[0].label , self.B.label )\nself.assertEqual( vertices[1].label , self.A.label )\nself.assertEqual( edges[0].payload.name , self.a.payload.name )", "path": "pygs\\test\\unit_test\\test_spt.py", "repo_name": "bmander/graphserver", "stars": 122, "license": "other", "language": "python", "size": 35348}
{"docstring": "\"\"\"check the previous day for viable departures\"\"\"\n\n# the service calendar has two weekdays, back to back\n", "func_signal": "def test_check_yesterday(self):\n", "code": "sc = ServiceCalendar()\nsc.add_period( 0, 3600*24, [\"WKDY\"] )\nsc.add_period( 3600*24, 2*3600*24, [\"WKDY\"] )\n\n# the timezone lasts for two days and has no offset\n# this is just boilerplate\ntz = Timezone()\ntz.add_period( TimezonePeriod(0, 2*3600*24, 0) )\n\n# tripboard runs on weekdays for agency 0\nal = TripAlight( \"WKDY\", sc, tz, 0 )\n\n# one alighting - one second before midnight\nal.add_alighting( \"1\", 86400-1, 0 )\n\n# our starting state is midnight between the two days\ns0 = State(1, 86400)\n\n# it should be one second after the last alighting \ns1 = al.walk_back( s0, WalkOptions() )\nself.assertEquals( s1.time, 86399 )", "path": "pygs\\test\\unit_test\\test_tripalight.py", "repo_name": "bmander/graphserver", "stars": 122, "license": "other", "language": "python", "size": 35348}
{"docstring": "# get shape_id of trip\n", "func_signal": "def shape_between(self, trip_id, stop_sequence1, stop_sequence2):\n", "code": "shape_id = list(self.execute( \"SELECT shape_id FROM trips WHERE trip_id=?\", (trip_id,) ))[0][0]\n\nif shape_id is None:\n    return self.shape_from_stops( trip_id, stop_sequence1, stop_sequence2 )\n\nquery = \"\"\"SELECT min(shape_dist_traveled), max(shape_dist_traveled)\n             FROM stop_times\n             WHERE trip_id=? and (stop_sequence = ? or stop_sequence = ?)\"\"\"\nt_min, t_max = list(self.execute( query, (trip_id, stop_sequence1, stop_sequence2) ))[0]\n\nif t_min is None or \\\n   ( hasattr(t_min,\"strip\") and t_min.strip()==\"\" ) or \\\n   t_max is None or \\\n   ( hasattr(t_max,\"strip\") and t_max.strip()==\"\" ) :\n    return self.shape_from_stops( trip_id, stop_sequence1, stop_sequence2 )\n        \nret = []\nfor (lon1, lat1, dist1), (lon2, lat2, dist2) in cons(self.shape(shape_id)):\n    if between( t_min, dist1, dist2 ):\n        percent_along = (t_min-dist1)/float((dist2-dist1)) if dist2!=dist1 else 0\n        lat = lat1+percent_along*(lat2-lat1)\n        lon = lon1+percent_along*(lon2-lon1)\n        ret.append( (lon, lat) )\n\n    if between( dist2, t_min, t_max ):\n        ret.append( (lon2, lat2) )\n        \n    if between( t_max, dist1, dist2):\n        percent_along = (t_max-dist1)/float((dist2-dist1)) if dist2!=dist1 else 0\n        lat = lat1+percent_along*(lat2-lat1)\n        lon = lon1+percent_along*(lon2-lon1)\n        ret.append( (lon, lat) )\n        \nreturn ret", "path": "pygs\\graphserver\\ext\\gtfs\\gtfsdb.py", "repo_name": "bmander/graphserver", "stars": 122, "license": "other", "language": "python", "size": 35348}
{"docstring": "# noon, -7 hours off UTC, as America/Los_Angeles in summer\n", "func_signal": "def test_august(self):\n", "code": "tz = Timezone.generate(\"America/Los_Angeles\")\nw = Wait(43200, tz)\n\n# one calendar, noon august 27, America/Los_Angeles\ns = State(1, 1219863600)\n\nassert w.walk(s, WalkOptions()).time == 1219863600\n\n# one calendar, 11:55 AM August 27 2008, America/Los_Angeles\ns = State(1, 1219863300)\nassert w.walk(s, WalkOptions()).time == 1219863600\nassert w.walk(s, WalkOptions()).weight == 300", "path": "pygs\\test\\unit_test\\test_wait.py", "repo_name": "bmander/graphserver", "stars": 122, "license": "other", "language": "python", "size": 35348}
{"docstring": "\"\"\"getSize returns one after one entry\"\"\"\n\n", "func_signal": "def test_one(self):\n", "code": "bb = Vertex(\"BB\")\nee = Edge(self.aa, bb, Link())\n\nself.path.addSegment( bb, ee )\n\nself.assertEqual( self.path.num_elements, 1 )", "path": "pygs\\test\\unit_test\\test_path.py", "repo_name": "bmander/graphserver", "stars": 122, "license": "other", "language": "python", "size": 35348}
{"docstring": "# Split line segment defined by (x1, y1, x2, y2) into a set of points \n# (x,y,displacement) spaced less than max_section_length apart\n\n", "func_signal": "def split_line_segment(lng1, lat1, lng2, lat2, max_section_length):\n", "code": "if lng1==lng2 and lat1==lat2:\n    yield [lng1, lat1, 0]\n    yield [lng2, lat2, 0]\n    return\n\nstreet_len = vincenty(lat1, lng1, lat2, lng2)\nn_sections = int(street_len/max_section_length)+1\n\ngeolen = ((lat2-lat1)**2 + (lng2-lng1)**2)**0.5\nsection_len = geolen/n_sections\nstreet_vector = (lng2-lng1, lat2-lat1)\nunit_vector = [x/geolen for x in street_vector]\n\nfor i in range(n_sections+1):\n    vec = [x*section_len*i for x in unit_vector]\n    vec = [lng1+vec[0], lat1+vec[1], (street_len/n_sections)*i]\n    yield vec", "path": "pygs\\graphserver\\ext\\ned\\elevation\\elevation.py", "repo_name": "bmander/graphserver", "stars": 122, "license": "other", "language": "python", "size": 35348}
{"docstring": "\"\"\"header is iterable of (fieldname, fieldtype, processing_function). For example, ((\"stop_sequence\", \"INTEGER\", int),). \n\"TEXT\" is default fieldtype. Default processing_function is lambda x:x\"\"\"\n\n", "func_signal": "def load_gtfs_table_to_sqlite(fp, gtfs_basename, cc, header=None, verbose=False):\n", "code": "ur = UTF8TextFile( fp )\nrd = csv.reader( ur )\n\n# create map of field locations in gtfs header to field locations as specified by the table definition\ngtfs_header = [x.strip() for x in rd.next()]\n\nprint(gtfs_header)\n\ngtfs_field_indices = dict(zip(gtfs_header, range(len(gtfs_header))))\n\nfield_name_locations = [gtfs_field_indices[field_name] if field_name in gtfs_field_indices else None for field_name, field_type, field_converter in header]\nfield_converters = [field_definition[2] for field_definition in header]\nfield_operator = list(zip(field_name_locations, field_converters))\n\n# populate stoptimes table\ninsert_template = 'insert into %s (%s) values (%s)'%(gtfs_basename,\",\".join([x[0] for x in header]), \",\".join([\"?\"]*len(header)))\nprint( insert_template )\nfor i, line in withProgress(enumerate(rd), 5000):\n    # carry on quietly if there's a blank line in the csv\n    if line == []:\n        continue\n    \n    _line = []\n    for i, converter in field_operator:\n        if i<len(line) and i is not None and line[i].strip() != \"\":\n            if converter:\n                _line.append( converter(line[i].strip()) )\n            else:\n                _line.append( line[i].strip() )\n        else:\n            _line.append( None )\n            \n    cc.execute(insert_template, _line)", "path": "pygs\\graphserver\\ext\\gtfs\\gtfsdb.py", "repo_name": "bmander/graphserver", "stars": 122, "license": "other", "language": "python", "size": 35348}
{"docstring": "\"\"\"Path is empty right after first created\"\"\"\n", "func_signal": "def test_path_empty(self):\n", "code": "pp = Path( Vertex(\"A\") )\n\nself.assertEqual( pp.num_elements, 0 )", "path": "pygs\\test\\unit_test\\test_path.py", "repo_name": "bmander/graphserver", "stars": 122, "license": "other", "language": "python", "size": 35348}
{"docstring": "\"\"\"get a vertex, edge after adding two segments\"\"\"\n\n", "func_signal": "def test_two(self):\n", "code": "ee1 = Edge(self.aa, self.bb, Link())\nee2 = Edge(self.bb, self.aa, Link())\nself.path.addSegment( self.bb, ee1 )\nself.path.addSegment( self.aa, ee2 )\n\n# out of bounds\nself.assertRaises( IndexError, self.path.getVertex, -1 )\nself.assertRaises( IndexError, self.path.getEdge, -1 )\n\n# vertices in bounds\nself.assertEqual( self.path.getVertex(0).soul, self.aa.soul )\nself.assertEqual( self.path.getVertex(1).soul, self.bb.soul )\nself.assertEqual( self.path.getVertex(2).soul, self.aa.soul )\n\n# edges in bounds\nself.assertEqual( self.path.getEdge(0).soul, ee1.soul )\nself.assertEqual( self.path.getEdge(1).soul, ee2.soul )\n\n# out of bounds again\nself.assertRaises( IndexError, self.path.getVertex, 3 )\nself.assertRaises( IndexError, self.path.getEdge, 2 )", "path": "pygs\\test\\unit_test\\test_path.py", "repo_name": "bmander/graphserver", "stars": 122, "license": "other", "language": "python", "size": 35348}
{"docstring": "# initiate the Path Struct with a C constructor\n", "func_signal": "def __new__(cls, init_size=50, expand_delta=50):\n", "code": "soul = lgs.vecNew( init_size, expand_delta )\n\n# wrap an instance of this class around that pointer\nreturn cls.from_address( soul )", "path": "pygs\\graphserver\\vector.py", "repo_name": "bmander/graphserver", "stars": 122, "license": "other", "language": "python", "size": 35348}
{"docstring": "\"\"\"vertices gettable after resizing\"\"\"\n\n# the path length right before a vector expansion\n", "func_signal": "def test_expand(self):\n", "code": "pathlen = 50\n\n# make a bunch of fake segments\nsegments = []\nfor i in range(pathlen):\n    vv = Vertex(str(i))\n    ee = Edge( vv, vv, Link() )\n    segments.append( (vv, ee) )\n\n# add those segments to the path\nfor vv, ee in segments:\n    self.path.addSegment( vv, ee ) \n    \n# check that they're alright\n# check the odd-duck vertex\nself.assertEqual( self.path.getVertex(0).label, \"A\" )\n\n# check the bunch of fake segments added\nfor i in range(1, pathlen+1):\n    self.assertEqual( i-1, int(self.path.getVertex(i).label) )\n    \n#\n# getting towards the real test - add a segment after the vectors have\n# been expanded\n#\n\n# add it\nvv = Vertex(\"B\")\nee = Edge(vv, vv, Link())\nself.path.addSegment( vv, ee )\n\n# get it\nself.assertEqual( self.path.getVertex( 51 ).label, \"B\" )", "path": "pygs\\test\\unit_test\\test_path.py", "repo_name": "bmander/graphserver", "stars": 122, "license": "other", "language": "python", "size": 35348}
{"docstring": "\"\"\"Return a list of the string names in the enum.\n\nThese are returned in the order they were defined in the .proto file.\n\"\"\"\n\n", "func_signal": "def keys(self):\n", "code": "return [value_descriptor.name\n        for value_descriptor in self._enum_type.values]", "path": "Lib\\google\\protobuf\\internal\\enum_type_wrapper.py", "repo_name": "mwielgoszewski/burp-protobuf-decoder", "stars": 100, "license": "None", "language": "python", "size": 380}
{"docstring": "\"\"\"Adds the FileDescriptorProto and its types to this database.\n\nArgs:\n  file_desc_proto: The FileDescriptorProto to add.\n\"\"\"\n\n", "func_signal": "def Add(self, file_desc_proto):\n", "code": "self._file_desc_protos_by_file[file_desc_proto.name] = file_desc_proto\npackage = file_desc_proto.package\nfor message in file_desc_proto.message_type:\n  self._file_desc_protos_by_symbol.update(\n      (name, file_desc_proto) for name in _ExtractSymbols(message, package))\nfor enum in file_desc_proto.enum_type:\n  self._file_desc_protos_by_symbol[\n      '.'.join((package, enum.name))] = file_desc_proto", "path": "Lib\\google\\protobuf\\descriptor_database.py", "repo_name": "mwielgoszewski/burp-protobuf-decoder", "stars": 100, "license": "None", "language": "python", "size": 380}
{"docstring": "\"\"\"Returns a string containing the name of an enum value.\"\"\"\n", "func_signal": "def Name(self, number):\n", "code": "if number in self._enum_type.values_by_number:\n  return self._enum_type.values_by_number[number].name\nraise ValueError('Enum %s has no name defined for value %d' % (\n    self._enum_type.name, number))", "path": "Lib\\google\\protobuf\\internal\\enum_type_wrapper.py", "repo_name": "mwielgoszewski/burp-protobuf-decoder", "stars": 100, "license": "None", "language": "python", "size": 380}
{"docstring": "\"\"\"Generates and returns a method that can be set for a service methods.\n\nArgs:\n  method: Descriptor of the service method for which a method is to be\n    generated.\n\nReturns:\n  A method that can be added to the service class.\n\"\"\"\n", "func_signal": "def _GenerateNonImplementedMethod(self, method):\n", "code": "return lambda inst, rpc_controller, request, callback: (\n    self._NonImplementedMethod(method.name, rpc_controller, callback))", "path": "Lib\\google\\protobuf\\service_reflection.py", "repo_name": "mwielgoszewski/burp-protobuf-decoder", "stars": 100, "license": "None", "language": "python", "size": 380}
{"docstring": "\"\"\"Inits EnumTypeWrapper with an EnumDescriptor.\"\"\"\n", "func_signal": "def __init__(self, enum_type):\n", "code": "self._enum_type = enum_type\nself.DESCRIPTOR = enum_type;", "path": "Lib\\google\\protobuf\\internal\\enum_type_wrapper.py", "repo_name": "mwielgoszewski/burp-protobuf-decoder", "stars": 100, "license": "None", "language": "python", "size": 380}
{"docstring": "\"\"\"Creates a message service class.\n\nArgs:\n  name: Name of the class (ignored, but required by the metaclass\n    protocol).\n  bases: Base classes of the class being constructed.\n  dictionary: The class dictionary of the class being constructed.\n    dictionary[_DESCRIPTOR_KEY] must contain a ServiceDescriptor object\n    describing this protocol service type.\n\"\"\"\n# Don't do anything if this class doesn't have a descriptor. This happens\n# when a service class is subclassed.\n", "func_signal": "def __init__(cls, name, bases, dictionary):\n", "code": "if GeneratedServiceType._DESCRIPTOR_KEY not in dictionary:\n  return\ndescriptor = dictionary[GeneratedServiceType._DESCRIPTOR_KEY]\nservice_builder = _ServiceBuilder(descriptor)\nservice_builder.BuildService(cls)", "path": "Lib\\google\\protobuf\\service_reflection.py", "repo_name": "mwielgoszewski/burp-protobuf-decoder", "stars": 100, "license": "None", "language": "python", "size": 380}
{"docstring": "\"\"\"Returns the class of the request protocol message.\n\nArgs:\n  method_descriptor: Descriptor of the method for which to return the\n    request protocol message class.\n\nReturns:\n  A class that represents the input protocol message of the specified\n  method.\n\"\"\"\n", "func_signal": "def _GetRequestClass(self, method_descriptor):\n", "code": "if method_descriptor.containing_service != self.descriptor:\n  raise RuntimeError(\n      'GetRequestClass() given method descriptor for wrong service type.')\nreturn method_descriptor.input_type._concrete_class", "path": "Lib\\google\\protobuf\\service_reflection.py", "repo_name": "mwielgoszewski/burp-protobuf-decoder", "stars": 100, "license": "None", "language": "python", "size": 380}
{"docstring": "\"\"\"Creates a message service stub class.\n\nArgs:\n  name: Name of the class (ignored, here).\n  bases: Base classes of the class being constructed.\n  dictionary: The class dictionary of the class being constructed.\n    dictionary[_DESCRIPTOR_KEY] must contain a ServiceDescriptor object\n    describing this protocol service type.\n\"\"\"\n", "func_signal": "def __init__(cls, name, bases, dictionary):\n", "code": "super(GeneratedServiceStubType, cls).__init__(name, bases, dictionary)\n# Don't do anything if this class doesn't have a descriptor. This happens\n# when a service stub is subclassed.\nif GeneratedServiceStubType._DESCRIPTOR_KEY not in dictionary:\n  return\ndescriptor = dictionary[GeneratedServiceStubType._DESCRIPTOR_KEY]\nservice_stub_builder = _ServiceStubBuilder(descriptor)\nservice_stub_builder.BuildServiceStub(cls)", "path": "Lib\\google\\protobuf\\service_reflection.py", "repo_name": "mwielgoszewski/burp-protobuf-decoder", "stars": 100, "license": "None", "language": "python", "size": 380}
{"docstring": "\"\"\"Support the pickle protocol.\"\"\"\n", "func_signal": "def __setstate__(self, state):\n", "code": "self.__init__()\nself.ParseFromString(state['serialized'])", "path": "Lib\\google\\protobuf\\message.py", "repo_name": "mwielgoszewski/burp-protobuf-decoder", "stars": 100, "license": "None", "language": "python", "size": 380}
{"docstring": "\"\"\"Return a list of the (name, value) pairs of the enum.\n\nThese are returned in the order they were defined in the .proto file.\n\"\"\"\n", "func_signal": "def items(self):\n", "code": "return [(value_descriptor.name, value_descriptor.number)\n        for value_descriptor in self._enum_type.values]", "path": "Lib\\google\\protobuf\\internal\\enum_type_wrapper.py", "repo_name": "mwielgoszewski/burp-protobuf-decoder", "stars": 100, "license": "None", "language": "python", "size": 380}
{"docstring": "\"\"\"Like MergeFromString(), except we clear the object first.\"\"\"\n", "func_signal": "def ParseFromString(self, serialized):\n", "code": "self.Clear()\nself.MergeFromString(serialized)", "path": "Lib\\google\\protobuf\\message.py", "repo_name": "mwielgoszewski/burp-protobuf-decoder", "stars": 100, "license": "None", "language": "python", "size": 380}
{"docstring": "\"\"\"Constructs the stub class.\n\nArgs:\n  cls: The class that will be constructed.\n\"\"\"\n\n", "func_signal": "def BuildServiceStub(self, cls):\n", "code": "def _ServiceStubInit(stub, rpc_channel):\n  stub.rpc_channel = rpc_channel\nself.cls = cls\ncls.__init__ = _ServiceStubInit\nfor method in self.descriptor.methods:\n  setattr(cls, method.name, self._GenerateStubMethod(method))", "path": "Lib\\google\\protobuf\\service_reflection.py", "repo_name": "mwielgoszewski/burp-protobuf-decoder", "stars": 100, "license": "None", "language": "python", "size": 380}
{"docstring": "\"\"\"Returns the value coresponding to the given enum name.\"\"\"\n", "func_signal": "def Value(self, name):\n", "code": "if name in self._enum_type.values_by_name:\n  return self._enum_type.values_by_name[name].number\nraise ValueError('Enum %s has no value defined for name %s' % (\n    self._enum_type.name, name))", "path": "Lib\\google\\protobuf\\internal\\enum_type_wrapper.py", "repo_name": "mwielgoszewski/burp-protobuf-decoder", "stars": 100, "license": "None", "language": "python", "size": 380}
{"docstring": "\"\"\"Return a list of the integer values in the enum.\n\nThese are returned in the order they were defined in the .proto file.\n\"\"\"\n\n", "func_signal": "def values(self):\n", "code": "return [value_descriptor.number\n        for value_descriptor in self._enum_type.values]", "path": "Lib\\google\\protobuf\\internal\\enum_type_wrapper.py", "repo_name": "mwielgoszewski/burp-protobuf-decoder", "stars": 100, "license": "None", "language": "python", "size": 380}
{"docstring": "\"\"\"Pulls out all the symbols from a descriptor proto.\n\nArgs:\n  desc_proto: The proto to extract symbols from.\n  package: The package containing the descriptor type.\n\nYields:\n  The fully qualified name found in the descriptor.\n\"\"\"\n\n", "func_signal": "def _ExtractSymbols(desc_proto, package):\n", "code": "message_name = '.'.join((package, desc_proto.name))\nyield message_name\nfor nested_type in desc_proto.nested_type:\n  for symbol in _ExtractSymbols(nested_type, message_name):\n    yield symbol\n  for enum_type in desc_proto.enum_type:\n    yield '.'.join((message_name, enum_type.name))", "path": "Lib\\google\\protobuf\\descriptor_database.py", "repo_name": "mwielgoszewski/burp-protobuf-decoder", "stars": 100, "license": "None", "language": "python", "size": 380}
{"docstring": "\"\"\"Copies the content of the specified message into the current message.\n\nThe method clears the current message and then merges the specified\nmessage using MergeFrom.\n\nArgs:\n  other_msg: Message to copy into the current one.\n\"\"\"\n", "func_signal": "def CopyFrom(self, other_msg):\n", "code": "if self is other_msg:\n  return\nself.Clear()\nself.MergeFrom(other_msg)", "path": "Lib\\google\\protobuf\\message.py", "repo_name": "mwielgoszewski/burp-protobuf-decoder", "stars": 100, "license": "None", "language": "python", "size": 380}
{"docstring": "\"\"\"Returns a type checker for a message field of the specified types.\n\nArgs:\n  cpp_type: C++ type of the field (see descriptor.py).\n  field_type: Protocol message field type (see descriptor.py).\n\nReturns:\n  An instance of TypeChecker which can be used to verify the types\n  of values assigned to a field of the specified type.\n\"\"\"\n", "func_signal": "def GetTypeChecker(cpp_type, field_type):\n", "code": "if (cpp_type == _FieldDescriptor.CPPTYPE_STRING and\n    field_type == _FieldDescriptor.TYPE_STRING):\n  return UnicodeValueChecker()\nreturn _VALUE_CHECKERS[cpp_type]", "path": "Lib\\google\\protobuf\\internal\\type_checkers.py", "repo_name": "mwielgoszewski/burp-protobuf-decoder", "stars": 100, "license": "None", "language": "python", "size": 380}
{"docstring": "\"\"\"The body of all methods in the generated service class.\n\nArgs:\n  method_name: Name of the method being executed.\n  rpc_controller: RPC controller used to execute this method.\n  callback: A callback which will be invoked when the method finishes.\n\"\"\"\n", "func_signal": "def _NonImplementedMethod(self, method_name, rpc_controller, callback):\n", "code": "rpc_controller.SetFailed('Method %s not implemented.' % method_name)\ncallback(None)", "path": "Lib\\google\\protobuf\\service_reflection.py", "repo_name": "mwielgoszewski/burp-protobuf-decoder", "stars": 100, "license": "None", "language": "python", "size": 380}
{"docstring": "\"\"\"Returns the class of the response protocol message.\n\nArgs:\n  method_descriptor: Descriptor of the method for which to return the\n    response protocol message class.\n\nReturns:\n  A class that represents the output protocol message of the specified\n  method.\n\"\"\"\n", "func_signal": "def _GetResponseClass(self, method_descriptor):\n", "code": "if method_descriptor.containing_service != self.descriptor:\n  raise RuntimeError(\n      'GetResponseClass() given method descriptor for wrong service type.')\nreturn method_descriptor.output_type._concrete_class", "path": "Lib\\google\\protobuf\\service_reflection.py", "repo_name": "mwielgoszewski/burp-protobuf-decoder", "stars": 100, "license": "None", "language": "python", "size": 380}
{"docstring": "\"\"\"Constructs the service class.\n\nArgs:\n  cls: The class that will be constructed.\n\"\"\"\n\n# CallMethod needs to operate with an instance of the Service class. This\n# internal wrapper function exists only to be able to pass the service\n# instance to the method that does the real CallMethod work.\n", "func_signal": "def BuildService(self, cls):\n", "code": "def _WrapCallMethod(srvc, method_descriptor,\n                    rpc_controller, request, callback):\n  return self._CallMethod(srvc, method_descriptor,\n                   rpc_controller, request, callback)\nself.cls = cls\ncls.CallMethod = _WrapCallMethod\ncls.GetDescriptor = staticmethod(lambda: self.descriptor)\ncls.GetDescriptor.__doc__ = \"Returns the service descriptor.\"\ncls.GetRequestClass = self._GetRequestClass\ncls.GetResponseClass = self._GetResponseClass\nfor method in self.descriptor.methods:\n  setattr(cls, method.name, self._GenerateNonImplementedMethod(method))", "path": "Lib\\google\\protobuf\\service_reflection.py", "repo_name": "mwielgoszewski/burp-protobuf-decoder", "stars": 100, "license": "None", "language": "python", "size": 380}
{"docstring": "\"\"\"\nmocksignature(func, mock=None, skipfirst=False)\n\nCreate a new function with the same signature as `func` that delegates\nto `mock`. If `skipfirst` is True the first argument is skipped, useful\nfor methods where `self` needs to be omitted from the new function.\n\nIf you don't pass in a `mock` then one will be created for you.\n\nThe mock is set as the `mock` attribute of the returned function for easy\naccess.\n\n`mocksignature` can also be used with classes. It copies the signature of\nthe `__init__` method.\n\nWhen used with callable objects (instances) it copies the signature of the\n`__call__` method.\n\"\"\"\n", "func_signal": "def mocksignature(func, mock=None, skipfirst=False):\n", "code": "if mock is None:\n    mock = Mock()\nsignature, func = _getsignature(func, skipfirst)\nsrc = \"lambda %(signature)s: _mock_(%(signature)s)\" % {\n    'signature': signature\n}\n\nfuncopy = eval(src, dict(_mock_=mock))\n_copy_func_details(func, funcopy)\n_setup_func(funcopy, mock)\nreturn funcopy", "path": "unittests\\mock.py", "repo_name": "HenrikDK/youtube-xbmc-plugin", "stars": 89, "license": "None", "language": "python", "size": 2814}
{"docstring": "\"\"\"\n@param [String] secret in the form of base32\n@option options digits [Integer] (6)\n    Number of integers in the OTP\n    Google Authenticate only supports 6 currently\n@option options digest [Callable] (hashlib.sha1)\n    Digest used in the HMAC\n    Google Authenticate only supports 'sha1' currently\n@returns [OTP] OTP instantiation\n\"\"\"\n", "func_signal": "def __init__(self, s, digits=6, digest=hashlib.sha1):\n", "code": "self.digits = digits\nself.digest = digest\nself.secret = s", "path": "integrationtests\\pyotp\\otp.py", "repo_name": "HenrikDK/youtube-xbmc-plugin", "stars": 89, "license": "None", "language": "python", "size": 2814}
{"docstring": "\"\"\"Unpatch the dict.\"\"\"\n", "func_signal": "def __exit__(self, *args):\n", "code": "self._unpatch_dict()\nreturn False", "path": "unittests\\mock.py", "repo_name": "HenrikDK/youtube-xbmc-plugin", "stars": 89, "license": "None", "language": "python", "size": 2814}
{"docstring": "\"\"\"assert that the mock was called with the specified arguments.\n\nRaises an AssertionError if the args and keyword args passed in are\ndifferent to the last call to the mock.\"\"\"\n", "func_signal": "def assert_called_with(_mock_self, *args, **kwargs):\n", "code": "self = _mock_self\nif self.call_args is None:\n    expected = self._format_mock_call_signature(args, kwargs)\n    raise AssertionError('Expected call: %s\\nNot called' % (expected,))\n\nif self.call_args != (args, kwargs):\n    msg = self._format_mock_failure_message(args, kwargs)\n    raise AssertionError(msg)", "path": "unittests\\mock.py", "repo_name": "HenrikDK/youtube-xbmc-plugin", "stars": 89, "license": "None", "language": "python", "size": 2814}
{"docstring": "# could specify parent?\n", "func_signal": "def _create_proxy(entry, self):\n", "code": "def create_mock():\n    m = MagicMock(name=entry, _new_name=entry, _new_parent=self)\n    setattr(self, entry, m)\n    _set_return_value(self, m, entry)\n    return m\nreturn MagicProxy(create_mock)", "path": "unittests\\mock.py", "repo_name": "HenrikDK/youtube-xbmc-plugin", "stars": 89, "license": "None", "language": "python", "size": 2814}
{"docstring": "# every instance has its own class\n# so we can create magic methods on the\n# class without stomping on other mocks\n", "func_signal": "def __new__(cls, *args, **kw):\n", "code": "new = type(cls.__name__, (cls,), {'__doc__': cls.__doc__})\nreturn object.__new__(new)", "path": "unittests\\mock.py", "repo_name": "HenrikDK/youtube-xbmc-plugin", "stars": 89, "license": "None", "language": "python", "size": 2814}
{"docstring": "\"\"\"\nAccepts either a Unix timestamp integer or a Time object.\nTime objects will be adjusted to UTC automatically\n@param [Time/Integer] time the time to generate an OTP for\n\"\"\"\n", "func_signal": "def at(self, for_time):\n", "code": "if not isinstance(for_time, datetime.datetime):\n    for_time = datetime.datetime.fromtimestamp(int(for_time))\nreturn self.generate_otp(self.timecode(for_time))", "path": "integrationtests\\pyotp\\totp.py", "repo_name": "HenrikDK/youtube-xbmc-plugin", "stars": 89, "license": "None", "language": "python", "size": 2814}
{"docstring": "# Max recursion of 5\n", "func_signal": "def _getfullAlgoCode( self, mainFunName, playerData, recDepth = 0, allLocalFunNamesTab=[], allLocalVarNamesTab=[] ):\n", "code": "if 5 <= recDepth:\n    self.common.log('_getfullAlgoCode: Maximum recursion depth exceeded')\n    return\n\nfunBody = self._getLocalFunBody( mainFunName, playerData)\nif '' != funBody:\n    funNames = self._getAllLocalSubFunNames(funBody)\n    if len(funNames):\n        for funName in funNames:\n            funName_=funName.replace('$','_S_')\n            if funName not in allLocalFunNamesTab:\n                funBody=funBody.replace(funName,funName_)\n                allLocalFunNamesTab.append(funName)\n                self.common.log(\"Add local function %s to known functions\" % mainFunName)\n                funBody = self._getfullAlgoCode( funName, playerData, recDepth + 1, allLocalFunNamesTab ) + \"\\n\" + funBody\n\n    varNames = self._extractLocalVarNames(funBody)\n    if len(varNames):\n        for varName in varNames:\n            self.common.log(\"Found local var object: \" + str(varName))\n            self.common.log(\"Known vars: \" + str(allLocalVarNamesTab))\n            if varName not in allLocalVarNamesTab:\n                self.common.log(\"Adding local var object %s to known objects\" % varName)\n                allLocalVarNamesTab.append(varName)\n                funBody = self._getLocalVarObjBody( varName, playerData ) + \"\\n\" + funBody\n\n    # conver code from javascript to python\n    funBody = self._jsToPy(funBody)\n    return '\\n' + funBody + '\\n'\nreturn funBody", "path": "plugin\\YouTubePlayer.py", "repo_name": "HenrikDK/youtube-xbmc-plugin", "stars": 89, "license": "None", "language": "python", "size": 2814}
{"docstring": "\"\"\"Given an object, return True if the object is callable.\nFor classes, return True if instances would be callable.\"\"\"\n", "func_signal": "def _instance_callable(obj):\n", "code": "if not isinstance(obj, ClassTypes):\n    # already an instance\n    return hasattr(obj, '__call__')\n\nklass = obj\n# uses __bases__ instead of __mro__ so that we work with old style classes\nif '__call__' in klass.__dict__:\n    return True\nfor base in klass.__bases__:\n    if _instance_callable(base):\n        return True\nreturn False", "path": "unittests\\mock.py", "repo_name": "HenrikDK/youtube-xbmc-plugin", "stars": 89, "license": "None", "language": "python", "size": 2814}
{"docstring": "\"\"\"Add a spec to a mock. `spec` can either be an object or a\nlist of strings. Only attributes on the `spec` can be fetched as\nattributes from the mock.\n\nIf `spec_set` is True then only attributes on the spec can be set.\"\"\"\n", "func_signal": "def mock_add_spec(self, spec, spec_set=False):\n", "code": "self._mock_add_spec(spec, spec_set)\nself._mock_set_magics()", "path": "unittests\\mock.py", "repo_name": "HenrikDK/youtube-xbmc-plugin", "stars": 89, "license": "None", "language": "python", "size": 2814}
{"docstring": "\"\"\"Perform the patch.\"\"\"\n", "func_signal": "def __enter__(self):\n", "code": "new, spec, spec_set = self.new, self.spec, self.spec_set\nautospec, kwargs = self.autospec, self.kwargs\nnew_callable = self.new_callable\n\noriginal, local = self.get_original()\n\nif new is DEFAULT and autospec is False:\n    inherit = False\n    if spec_set == True:\n        spec_set = original\n    elif spec == True:\n        # set spec to the object we are replacing\n        spec = original\n\n    if (spec or spec_set) is not None:\n        if isinstance(original, ClassTypes):\n            # If we're patching out a class and there is a spec\n            inherit = True\n\n    Klass = MagicMock\n    _kwargs = {}\n    if new_callable is not None:\n        Klass = new_callable\n    elif (spec or spec_set) is not None:\n        if not _callable(spec or spec_set):\n            Klass = NonCallableMagicMock\n\n    if spec is not None:\n        _kwargs['spec'] = spec\n    if spec_set is not None:\n        _kwargs['spec_set'] = spec_set\n\n    # add a name to mocks\n    if (isinstance(Klass, type) and\n        issubclass(Klass, NonCallableMock) and self.attribute):\n        _kwargs['name'] = self.attribute\n\n    _kwargs.update(kwargs)\n    new = Klass(**_kwargs)\n\n    if inherit and _is_instance_mock(new):\n        # we can only tell if the instance should be callable if the\n        # spec is not a list\n        if (not _is_list(spec or spec_set) and not\n            _instance_callable(spec or spec_set)):\n            Klass = NonCallableMagicMock\n\n        _kwargs.pop('name')\n        new.return_value = Klass(_new_parent=new, _new_name='()',\n                                 **_kwargs)\nelif autospec is not False:\n    # spec is ignored, new *must* be default, spec_set is treated\n    # as a boolean. Should we check spec is not None and that spec_set\n    # is a bool? mocksignature should also not be used. Should we\n    # check this?\n    if new is not DEFAULT:\n        raise TypeError(\n            \"autospec creates the mock for you. Can't specify \"\n            \"autospec and new.\"\n        )\n    spec_set = bool(spec_set)\n    if autospec is True:\n        autospec = original\n    new = create_autospec(autospec, spec_set=spec_set, configure=kwargs,\n                          _name=self.attribute)\nelif kwargs:\n    # can't set keyword args when we aren't creating the mock\n    # XXXX If new is a Mock we could call new.configure_mock(**kwargs)\n    raise TypeError(\"Can't pass kwargs to a mock we aren't creating\")\n\nnew_attr = new\nif self.mocksignature:\n    new_attr = mocksignature(original, new)\n\nself.temp_original = original\nself.is_local = local\nsetattr(self.target, self.attribute, new_attr)\nif self.attribute_name is not None:\n    extra_args = {}\n    if self.new is DEFAULT:\n        extra_args[self.attribute_name] =  new\n    for patching in self.additional_patchers:\n        arg = patching.__enter__()\n        if patching.new is DEFAULT:\n            extra_args.update(arg)\n    return extra_args\n\nreturn new", "path": "unittests\\mock.py", "repo_name": "HenrikDK/youtube-xbmc-plugin", "stars": 89, "license": "None", "language": "python", "size": 2814}
{"docstring": "\"\"\"assert the mock has been called with the specified arguments.\n\nThe assert passes if the mock has *ever* been called, unlike\n`assert_called_with` and `assert_called_once_with` that only pass if\nthe call is the most recent one.\"\"\"\n", "func_signal": "def assert_any_call(self, *args, **kwargs):\n", "code": "kall = call(*args, **kwargs)\nif not kall in self.call_args_list:\n    expected_string = self._format_mock_call_signature(args, kwargs)\n    raise AssertionError(\n        '%s call not found, possible calls are: \\r\\n\\r\\n %s' % (expected_string, repr(self.call_args_list)) \n    )", "path": "unittests\\mock.py", "repo_name": "HenrikDK/youtube-xbmc-plugin", "stars": 89, "license": "None", "language": "python", "size": 2814}
{"docstring": "\"\"\"\n@option options [Integer] interval (30) the time interval in seconds for OTP\n    This defaults to 30 which is standard.\n\"\"\"\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "self.interval = kwargs.pop('interval', 30)\nsuper(TOTP, self).__init__(*args, **kwargs)", "path": "integrationtests\\pyotp\\totp.py", "repo_name": "HenrikDK/youtube-xbmc-plugin", "stars": 89, "license": "None", "language": "python", "size": 2814}
{"docstring": "\"\"\"\nReturns the provisioning URI for the OTP\nThis can then be encoded in a QR Code and used\nto provision the Google Authenticator app\n@param [String] name of the account\n@return [String] provisioning uri\n\"\"\"\n", "func_signal": "def provisioning_uri(self, name):\n", "code": "return 'otpauth://totp/%(name)s?secret=%(secret)s' % {\n    'name': urllib.quote(name, safe='@'),\n    'secret': self.secret,\n}", "path": "integrationtests\\pyotp\\totp.py", "repo_name": "HenrikDK/youtube-xbmc-plugin", "stars": 89, "license": "None", "language": "python", "size": 2814}
{"docstring": "# can't use self in-case a function / method we are mocking uses self\n# in the signature\n", "func_signal": "def __call__(_mock_self, *args, **kwargs):\n", "code": "_mock_self._mock_check_sig(*args, **kwargs)\nreturn _mock_self._mock_call(*args, **kwargs)", "path": "unittests\\mock.py", "repo_name": "HenrikDK/youtube-xbmc-plugin", "stars": 89, "license": "None", "language": "python", "size": 2814}
{"docstring": "# creates a function with signature (*args, **kwargs) that delegates to a\n# mock. It still does signature checking by calling a lambda with the same\n# signature as the original. This is effectively mocksignature2.\n", "func_signal": "def _set_signature(mock, original):\n", "code": "if not _callable(original):\n    return\n\nskipfirst = isinstance(original, ClassTypes)\nresult = _getsignature2(original, skipfirst)\nif result is None:\n    # was a C function (e.g. object().__init__ ) that can't be mocked\n    return\n\nsignature, func = result\n\nsrc = \"lambda %s: None\" % signature\ncontext = {'_mock_': mock}\nchecksig = eval(src, context)\n_copy_func_details(func, checksig)\n\nname = original.__name__\nif not _isidentifier(name):\n    name = 'funcopy'\ncontext = {'checksig': checksig, 'mock': mock}\nsrc = \"\"\"def %s(*args, **kwargs):\nchecksig(*args, **kwargs)\nreturn mock(*args, **kwargs)\"\"\" % name\nexec (src, context)\nfuncopy = context[name]\n_setup_func(funcopy, mock)\nreturn funcopy", "path": "unittests\\mock.py", "repo_name": "HenrikDK/youtube-xbmc-plugin", "stars": 89, "license": "None", "language": "python", "size": 2814}
{"docstring": "\"\"\"assert that the mock was called exactly once and with the specified\narguments.\"\"\"\n", "func_signal": "def assert_called_once_with(_mock_self, *args, **kwargs):\n", "code": "self = _mock_self\nif not self.call_count == 1:\n    msg = (\"Expected to be called once. Called %s times.\" %\n           self.call_count)\n    raise AssertionError(msg)\nreturn self.assert_called_with(*args, **kwargs)", "path": "unittests\\mock.py", "repo_name": "HenrikDK/youtube-xbmc-plugin", "stars": 89, "license": "None", "language": "python", "size": 2814}
{"docstring": "\"\"\"For a call object that represents multiple calls, `call_list`\nreturns a list of all the intermediate calls as well as the\nfinal call.\"\"\"\n", "func_signal": "def call_list(self):\n", "code": "vals = []\nthing = self\nwhile thing is not None:\n    if thing.from_kall:\n        vals.append(thing)\n    thing = thing.parent\nreturn _CallList(reversed(vals))", "path": "unittests\\mock.py", "repo_name": "HenrikDK/youtube-xbmc-plugin", "stars": 89, "license": "None", "language": "python", "size": 2814}
{"docstring": "\"\"\"Add a spec to a mock. `spec` can either be an object or a\nlist of strings. Only attributes on the `spec` can be fetched as\nattributes from the mock.\n\nIf `spec_set` is True then only attributes on the spec can be set.\"\"\"\n", "func_signal": "def mock_add_spec(self, spec, spec_set=False):\n", "code": "self._mock_add_spec(spec, spec_set)\nself._mock_set_magics()", "path": "unittests\\mock.py", "repo_name": "HenrikDK/youtube-xbmc-plugin", "stars": 89, "license": "None", "language": "python", "size": 2814}
{"docstring": "# don't use a with here (backwards compatability with Python 2.4)\n", "func_signal": "def patched(*args, **keywargs):\n", "code": "extra_args = []\nentered_patchers = []\n\n# can't use try...except...finally because of Python 2.4\n# compatibility\ntry:\n    try:\n        for patching in patched.patchings:\n            arg = patching.__enter__()\n            entered_patchers.append(patching)\n            if patching.attribute_name is not None:\n                keywargs.update(arg)\n            elif patching.new is DEFAULT:\n                extra_args.append(arg)\n\n        args += tuple(extra_args)\n        return func(*args, **keywargs)\n    except:\n        if (patching not in entered_patchers and\n            _is_started(patching)):\n            # the patcher may have been started, but an exception\n            # raised whilst entering one of its additional_patchers\n            entered_patchers.append(patching)\n        # re-raise the exception\n        raise\nfinally:\n    for patching in reversed(entered_patchers):\n        patching.__exit__()", "path": "unittests\\mock.py", "repo_name": "HenrikDK/youtube-xbmc-plugin", "stars": 89, "license": "None", "language": "python", "size": 2814}
{"docstring": "\"\"\" This method is used to underlap an image under the current image\nHave a look at the documentation of the 'load' method for the meanings of 'resource_name' and 'value'\n\"\"\"\n", "func_signal": "def prepend(self, resource_name, value):\n", "code": "if resource_name == \"File\":\n    self.loader(value)", "path": "amsn2\\gui\\front_ends\\qt4\\image.py", "repo_name": "drf/amsn2", "stars": 107, "license": "None", "language": "python", "size": 4676}
{"docstring": "# TODO : load the accounts from disk and all settings\n# then show the login window if autoconnect is disabled\n\n", "func_signal": "def mainWindowShown(self):\n", "code": "self._main.setTitle(\"aMSN 2 - Loading\")\n\n\nsplash = self._gui.gui.aMSNSplashScreen(self, self._main)\nimage = ImageView()\nimage.load(\"Filename\",\"/path/to/image/here\")\n\nsplash.setImage(image)\nsplash.setText(\"Loading...\")\nsplash.show()\n\nlogin = self._gui.gui.aMSNLoginWindow(self, self._main)\n\nlogin.setAccounts(self._account_manager.getAvailableAccountViews())\n\nsplash.hide()\nself._main.setTitle(\"aMSN 2 - Login\")\nlogin.show()\n\nmenu = self.createMainMenuView()\nself._main.setMenu(menu)", "path": "amsn2\\core\\amsn.py", "repo_name": "drf/amsn2", "stars": 107, "license": "None", "language": "python", "size": 4676}
{"docstring": "# Acquire the lock to do modifications\n", "func_signal": "def __repaint(self):\n", "code": "with self._mod_lock:\n    self._win.clear()\n    (y, x) = self._stdscr.getmaxyx()\n    self._win.move(0,1)\n    available = y\n    gso = []\n    for g in self._groups_order:\n        available -= 1\n        available -= len(self._groups[g].contact_ids)\n        gso.append(g)\n        if available <= 0:\n            break\n    gso.reverse()\n    available = y\n    i = 0\n    for g in gso:\n        if self._groups[g] is not None:\n            available -= 1\n            cids = self._groups[g].contact_ids\n            cids = cids[:available]\n            cids.reverse()\n            for c in cids:\n                if self._contacts.has_key(c) and self._contacts[c]['cView'] is not None:\n                    if i == y - self._selected:\n                        self._win.bkgdset(curses.color_pair(1))\n                    self._win.insstr(str(self._contacts[c]['cView'].name))\n                    self._win.bkgdset(curses.color_pair(0))\n                    self._win.insch(' ')\n                    self._win.insch(curses.ACS_HLINE)\n                    self._win.insch(curses.ACS_HLINE)\n                    self._win.insch(curses.ACS_LLCORNER)\n                    self._win.insertln()\n                    self._win.bkgdset(curses.color_pair(0))\n                    i += 1\n            if i == y - self._selected:\n                self._win.bkgdset(curses.color_pair(1))\n            self._win.insstr(str(self._groups[g].name))\n            self._win.bkgdset(curses.color_pair(0))\n            self._win.insch(' ')\n            self._win.insch(curses.ACS_LLCORNER)\n            self._win.insertln()\n            i += 1\n    self._win.border()\n    self._win.refresh()\n    self._modified = False", "path": "amsn2\\gui\\front_ends\\curses\\contact_list.py", "repo_name": "drf/amsn2", "stars": 107, "license": "None", "language": "python", "size": 4676}
{"docstring": "\"\"\" This method is used to overlap an image on the current image\nHave a look at the documentation of the 'load' method for the meanings of 'resource_name' and 'value'\n\"\"\"\n", "func_signal": "def append(self, resource_name, value):\n", "code": "if resource_name == \"File\":\n    self.loader(value)", "path": "amsn2\\gui\\front_ends\\qt4\\image.py", "repo_name": "drf/amsn2", "stars": 107, "license": "None", "language": "python", "size": 4676}
{"docstring": "\"\"\"\nCreate a new aMSN Core. It takes an options class as argument\nwhich has a variable for each option the core is supposed to received.\nThis is easier done using optparse.\nThe options supported are :\n   options.account = the account's username to use\n   options.password = the account's password to use\n   options.front_end = the front end's name to use\n   options.debug = whether or not to enable debug output\n\"\"\"\n", "func_signal": "def __init__(self, options):\n", "code": "self.p2s = {papyon.Presence.ONLINE:\"online\",\n            papyon.Presence.BUSY:\"busy\",\n            papyon.Presence.IDLE:\"idle\",\n            papyon.Presence.AWAY:\"away\",\n            papyon.Presence.BE_RIGHT_BACK:\"brb\",\n            papyon.Presence.ON_THE_PHONE:\"phone\",\n            papyon.Presence.OUT_TO_LUNCH:\"lunch\",\n            papyon.Presence.INVISIBLE:\"hidden\",\n            papyon.Presence.OFFLINE:\"offline\"}\nself.Presence = papyon.Presence\n\nself._event_manager = aMSNEventManager(self)\nself._options = options\n\nself._gui_name = None\nself._gui = None\nself._loop = None\nself._main = None\nself._account = None\nself.loadUI(self._options.front_end)\n\nself._backend_manager = aMSNBackendManager(self)\nself._account_manager = aMSNAccountManager(self, options)\nself._theme_manager = aMSNThemeManager(self)\nself._contactlist_manager = aMSNContactListManager(self)\nself._oim_manager = aMSNOIMManager(self)\nself._conversation_manager = aMSNConversationManager(self)\nself._personalinfo_manager = aMSNPersonalInfoManager(self)\n\n# TODO: redirect the logs somewhere, something like ctrl-s ctrl-d for amsn-0.9x\nlogging.basicConfig(level=logging.WARNING)\n\nif self._options.debug_protocol:\n    papyon_logger.setLevel(logging.DEBUG)\nelse:\n    papyon_logger.setLevel(logging.WARNING)\n\nif self._options.debug_amsn2:\n    logger.setLevel(logging.DEBUG)\nelse:\n    logger.setLevel(logging.WARNING)", "path": "amsn2\\core\\amsn.py", "repo_name": "drf/amsn2", "stars": 107, "license": "None", "language": "python", "size": 4676}
{"docstring": "# Get the next event from the queue.\n", "func_signal": "def processEvents(self, timeout=100):\n", "code": "if timeout < 0:\n    eventTimeout = NSDate.distantPast()\nelif timeout == 0:\n    eventTimeout = NSDate.distantFuture()\nelse:\n    eventTimeout = NSDate.dateWithTimeIntervalSinceNow_(float(timeout/1000.0))\n\n# NSAnyEventMask = 0xffffffff - http://osdir.com/ml/python.pyobjc.devel/2003-10/msg00130.html\nevent = self.nextEventMatchingMask_untilDate_inMode_dequeue_( \\\n    0xffffffff, \\\n    eventTimeout, \\\n    NSDefaultRunLoopMode , \\\n    True)\n\n# Process event if we have one. (python None == cocoa nil)\nif event != None:\n    self.sendEvent_(event)\n    return True\n\nreturn False", "path": "amsn2\\gui\\front_ends\\cocoa\\main_loop.py", "repo_name": "drf/amsn2", "stars": 107, "license": "None", "language": "python", "size": 4676}
{"docstring": "# Acquire the lock to do modifications\n", "func_signal": "def groupUpdated(self, gView):\n", "code": "with self._mod_lock:\n    if self._groups.has_key(gView.uid):\n        if self._groups[gView.uid] is not None:\n            #Delete contacts\n            for c in self._groups[gView.uid].contact_ids:\n                if c not in gView.contact_ids:\n                    if self._contacts[c]['refs'] == 1:\n                        self._contacts.delete(c)\n                    else:\n                        self._contacts[c]['refs'] -= 1\n        #Add contacts\n        for c in gView.contact_ids:\n            if not self._contacts.has_key(c):\n                self._contacts[c] = {'cView': None, 'refs': 1}\n                continue\n            #If contact wasn't already there, increment reference count\n            if self._groups[gView.uid] is None or c not in self._groups[gView.uid].contact_ids:\n                self._contacts[c]['refs'] += 1\n        self._groups[gView.uid] = gView\n        self._modified = True\n\n        # Notify waiting threads that we modified something\n        self._mod_lock.notify()", "path": "amsn2\\gui\\front_ends\\curses\\contact_list.py", "repo_name": "drf/amsn2", "stars": 107, "license": "None", "language": "python", "size": 4676}
{"docstring": "\"\"\"\n@type core: aMSNCore\n@type gui_name: str\n\"\"\"\n\n", "func_signal": "def __init__(self, core, gui_name):\n", "code": "self._core = core\nself._name = gui_name\n   \t\t\nif GUIManager.frontEndExists(self._name) is False:\n    raise InvalidFrontEndException(\"Invalid Front End. Available front ends are : \" + str(GUIManager.listFrontEnds()))\nelse:\n    self.gui = GUIManager.front_ends[self._name]\n    self.gui = self.gui.load()", "path": "amsn2\\gui\\gui.py", "repo_name": "drf/amsn2", "stars": 107, "license": "None", "language": "python", "size": 4676}
{"docstring": "\"\"\" This will allow the core to change the current window's main menu\n@type menu: MenuView\n\"\"\"\n", "func_signal": "def setMenu(self, menu):\n", "code": "chldn = self.main_menu.get_children()\nif len(chldn) is not 0:\n    for chl in chldn:\n        self.main_menu.remove(chl)\ncommon.createMenuItemsFromView(self.main_menu, menu.items)\nself.main_menu.show()", "path": "amsn2\\gui\\front_ends\\gtk\\main.py", "repo_name": "drf/amsn2", "stars": 107, "license": "None", "language": "python", "size": 4676}
{"docstring": "#ecore.main_loop_glib_integrate()\n", "func_signal": "def run(self):\n", "code": "mainloop = gobject.MainLoop(is_running=True)\ncontext = mainloop.get_context()\n\ndef glib_context_iterate():\n    iters = 0\n    while iters < 10 and context.pending():\n        context.iteration()\n        iters += 1\n    return True\n\n# Every 100ms, call an iteration of the glib main context loop\n# to allow the protocol context loop to work\necore.timer_add(0.1, glib_context_iterate)\n\n#equals elementary.run()\necore.main_loop_begin()", "path": "amsn2\\gui\\front_ends\\efl\\main_loop.py", "repo_name": "drf/amsn2", "stars": 107, "license": "None", "language": "python", "size": 4676}
{"docstring": "# Acquire the lock to do modifications\n", "func_signal": "def contactListUpdated(self, clView):\n", "code": "with self._mod_lock:\n    # TODO: Implement it to sort groups\n    for g in self._groups_order:\n        if g not in clView.group_ids:\n            self._groups.delete(g)\n    for g in clView.group_ids:\n        if not g in self._groups_order:\n            self._groups[g] = None\n    self._groups_order = clView.group_ids\n    self._modified = True\n\n    # Notify waiting threads that we modified something\n    self._mod_lock.notify()", "path": "amsn2\\gui\\front_ends\\curses\\contact_list.py", "repo_name": "drf/amsn2", "stars": 107, "license": "None", "language": "python", "size": 4676}
{"docstring": "# Acquire the lock to do modifications\n", "func_signal": "def contactUpdated(self, cView):\n", "code": "with self._mod_lock:\n    if self._contacts.has_key(cView.uid):\n        self._contacts[cView.uid]['cView'] = cView\n        self._modified = True\n\n        # Notify waiting threads that we modified something\n        self._mod_lock.notify()", "path": "amsn2\\gui\\front_ends\\curses\\contact_list.py", "repo_name": "drf/amsn2", "stars": 107, "license": "None", "language": "python", "size": 4676}
{"docstring": "\"\"\" contact_uid is the Id of the contact to invite \"\"\"\n", "func_signal": "def inviteContact(self, contact_uid):\n", "code": "c = self._core._contactlist_manager.getContact(contact_uid)\nself._conv.invite_user(contact.papyon_contact)", "path": "amsn2\\core\\conversation.py", "repo_name": "drf/amsn2", "stars": 107, "license": "None", "language": "python", "size": 4676}
{"docstring": "#TODO: messageView\n", "func_signal": "def onMessageReceived(self, message, sender_uid=None, formatting=None):\n", "code": "mv = MessageView()\nif sender_uid is None:\n    mv.sender.appendStringView(self._core._personalinfo_manager._personalinfoview.nick)\nelse:\n    c = self._core._contactlist_manager.getContact(sender_uid)\n    mv.sender_icon = c.icon\n    mv.message_type = MessageView.MESSAGE_OUTGOING\n    mv.sender.appendStringView(c.nickname)\nmv.msg = message\nself._convWidget.onMessageReceived(mv, formatting)", "path": "amsn2\\core\\conversation.py", "repo_name": "drf/amsn2", "stars": 107, "license": "None", "language": "python", "size": 4676}
{"docstring": "\"\"\" msg is a StringView \"\"\"\n# for the moment, no smiley substitution... (TODO)\n", "func_signal": "def sendMessage(self, msg, formatting=None):\n", "code": "self.onMessageReceived(msg, formatting=formatting)\nmessage = papyon.ConversationMessage(str(msg), formatting)\nself._conv.send_text_message(message)", "path": "amsn2\\core\\conversation.py", "repo_name": "drf/amsn2", "stars": 107, "license": "None", "language": "python", "size": 4676}
{"docstring": "# TODO image, ...\n", "func_signal": "def myInfoUpdated(self, view):\n", "code": "self._myview = view\nnk = view.nick\nself.ui.nickName.setText(str(nk))\nmessage = str(view.psm)+' '+str(view.current_media)\nself.ui.statusMessage.setText('<i>'+message+'</i>')\n# TODO Add a combobox like the gtk ui?\n#self.ui.statusCombo.currentIndex(self.status_values[view.presence])", "path": "amsn2\\gui\\front_ends\\qt4\\contact_list.py", "repo_name": "drf/amsn2", "stars": 107, "license": "None", "language": "python", "size": 4676}
{"docstring": "#TODO: filter objects\n", "func_signal": "def on_profile_msn_object_changed(self):\n", "code": "if self._client.profile.msn_object._type is papyon.p2p.MSNObjectType.DISPLAY_PICTURE:\n    self._personalinfo_manager.onDPUpdated(self._client.profile.msn_object)", "path": "amsn2\\protocol\\events\\profile.py", "repo_name": "drf/amsn2", "stars": 107, "license": "None", "language": "python", "size": 4676}
{"docstring": "\"\"\" create the chat widget for the 'parent' window, but don't attach to\nit.\"\"\"\n", "func_signal": "def __init__(self, amsn_conversation, parent, contacts_uid):\n", "code": "self._main=parent._main\nself._uid=md5.new(str(random.random())).hexdigest()\nself._main.send(\"newChatWidget\",[self._uid])\nself._main.addListener(\"sendMessage\",self.sendMessage)\nself._amsn_conversation=amsn_conversation", "path": "amsn2\\gui\\front_ends\\web\\chat_window.py", "repo_name": "drf/amsn2", "stars": 107, "license": "None", "language": "python", "size": 4676}
{"docstring": "# TODO Create and set text/values to controls.\n#status list\n", "func_signal": "def __create_controls(self):\n", "code": "self.status_values = {}\nself.status_dict = {}\nstatus_n = 0\nfor key in self._amsn_core.p2s:\n    name = self._amsn_core.p2s[key]\n    if (name == 'offline'): continue\n    self.status_values[name] = status_n\n    self.status_dict[str.capitalize(name)] = name\n    status_n = status_n +1\n# If we add a combobox like the gtk ui, uncomment this.\n#self.ui.comboStatus.addItem(str.capitalize(name))", "path": "amsn2\\gui\\front_ends\\qt4\\contact_list.py", "repo_name": "drf/amsn2", "stars": 107, "license": "None", "language": "python", "size": 4676}
{"docstring": "\"\"\"\n@type account: aMSNAccount\n@type state: L{papyon.event.ClientState}\n@param state: New state of the Client.\n\"\"\"\n\n", "func_signal": "def connectionStateChanged(self, account, state):\n", "code": "status_str = \\\n{\n    papyon.event.ClientState.CONNECTING : 'Connecting to server...',\n    papyon.event.ClientState.CONNECTED : 'Connected',\n    papyon.event.ClientState.AUTHENTICATING : 'Authenticating...',\n    papyon.event.ClientState.AUTHENTICATED : 'Password accepted',\n    papyon.event.ClientState.SYNCHRONIZING : 'Please wait while your contact list\\nis being downloaded...',\n    papyon.event.ClientState.SYNCHRONIZED : 'Contact list downloaded successfully.\\nHappy Chatting'\n}\n\nif state in status_str:\n    account.login.onConnecting((state + 1)/ 7., status_str[state])\nelif state == papyon.event.ClientState.OPEN:\n    clwin = self._gui.gui.aMSNContactListWindow(self, self._main)\n    clwin.account = account\n    account.clwin = clwin\n    account.login.hide()\n    self._main.setTitle(\"aMSN 2\")\n    account.clwin.show()\n    account.login = None\n\n    self._personalinfo_manager.setAccount(account)\n    self._contactlist_manager.onCLDownloaded(account.client.address_book)", "path": "amsn2\\core\\amsn.py", "repo_name": "drf/amsn2", "stars": 107, "license": "None", "language": "python", "size": 4676}
{"docstring": "\"\"\"\nSimple example - list google docs documents\n\"\"\"\n", "func_signal": "def test_page1(request):\n", "code": "if TOKEN_VAR in request.session:\n    con = HTTPSConnection(\"www.google.com\")\n    con.putrequest('GET', '/m8/feeds/contacts/vchakoshy@gmail.com/full')\n    con.putheader('Authorization', 'AuthSub token=\"%s\"' % request.session[TOKEN_VAR])\n    con.endheaders()\n    con.send('')\n    r2 = con.getresponse()\n    dane = r2.read()\n    soup = BeautifulStoneSoup(dane)\n    dane = soup.prettify()\n    return render(request, 'pin/a.html', {'dane': dane})\nelse:\n    return render(request, 'pin/a.html', {'dane': 'bad bad'})", "path": "pin\\views_test.py", "repo_name": "vchakoshy/django-pin", "stars": 84, "license": "None", "language": "python", "size": 536}
{"docstring": "# Adding field 'Post.cnt_comment'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.add_column('pin_post', 'cnt_comment',\n              self.gf('django.db.models.fields.IntegerField')(default=-1, blank=True),\n              keep_default=False)", "path": "pin\\migrations\\0016_auto__add_field_post_cnt_comment.py", "repo_name": "vchakoshy/django-pin", "stars": 84, "license": "None", "language": "python", "size": 536}
{"docstring": "# Adding field 'Post.cnt_like'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.add_column('pin_post', 'cnt_like',\n              self.gf('django.db.models.fields.IntegerField')(default=-1, blank=True),\n              keep_default=False)", "path": "pin\\migrations\\0017_auto__add_field_post_cnt_like.py", "repo_name": "vchakoshy/django-pin", "stars": 84, "license": "None", "language": "python", "size": 536}
{"docstring": "# Adding model 'Comments'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.create_table('pin_comments', (\n    ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n    ('text', self.gf('django.db.models.fields.TextField')()),\n    ('date', self.gf('django.db.models.fields.DateTimeField')(auto_now_add=True, blank=True)),\n    ('ip', self.gf('django.db.models.fields.IPAddressField')(default='127.0.0.1', max_length=15)),\n    ('public', self.gf('django.db.models.fields.BooleanField')(default=False)),\n    ('post', self.gf('django.db.models.fields.related.ForeignKey')(related_name='comment_post', to=orm['pin.Post'])),\n    ('user', self.gf('django.db.models.fields.related.ForeignKey')(related_name='comment_sender', to=orm['auth.User'])),\n))\ndb.send_create_signal('pin', ['Comments'])", "path": "pin\\migrations\\0008_auto__add_comments.py", "repo_name": "vchakoshy/django-pin", "stars": 84, "license": "None", "language": "python", "size": 536}
{"docstring": "#gd = datetime.datetime.strptime(value, '%Y-%m-%d %H:%M:%S')\n", "func_signal": "def jalali_mysql_date(value):\n", "code": "gd = value\ncal = Calverter()\njd = cal.gregorian_to_jd(gd.year, gd.month, gd.day)\n\nd = cal.jd_to_jalali(jd)\nd = \"%s/%s/%s\" % (d[0], d[1], d[2])\nreturn d", "path": "pin\\templatetags\\pin_tags.py", "repo_name": "vchakoshy/django-pin", "stars": 84, "license": "None", "language": "python", "size": 536}
{"docstring": "# User chose to not deal with backwards NULL issues for 'Comments.text'\n", "func_signal": "def backwards(self, orm):\n", "code": "        raise RuntimeError(\"Cannot reverse this migration. 'Comments.text' and its values cannot be restored.\")\n# User chose to not deal with backwards NULL issues for 'Comments.date'\n        raise RuntimeError(\"Cannot reverse this migration. 'Comments.date' and its values cannot be restored.\")\n        # Adding field 'Comments.ip'\n        db.add_column('pin_comments', 'ip',\n                      self.gf('django.db.models.fields.IPAddressField')(default='127.0.0.1', max_length=15),\n                      keep_default=False)\n\n# User chose to not deal with backwards NULL issues for 'Comments.post'\n        raise RuntimeError(\"Cannot reverse this migration. 'Comments.post' and its values cannot be restored.\")\n        # Adding field 'Comments.public'\n        db.add_column('pin_comments', 'public',\n                      self.gf('django.db.models.fields.BooleanField')(default=False),\n                      keep_default=False)\n# Deleting field 'Comments.comment'\n        db.delete_column('pin_comments', 'comment')\n# Deleting field 'Comments.submit_date'\n        db.delete_column('pin_comments', 'submit_date')\n# Deleting field 'Comments.ip_address'\n        db.delete_column('pin_comments', 'ip_address')\n# Deleting field 'Comments.is_public'\n        db.delete_column('pin_comments', 'is_public')\n# Deleting field 'Comments.object_pk'\n        db.delete_column('pin_comments', 'object_pk_id')", "path": "pin\\migrations\\0010_auto__del_field_comments_text__del_field_comments_date__del_field_comm.py", "repo_name": "vchakoshy/django-pin", "stars": 84, "license": "None", "language": "python", "size": 536}
{"docstring": "\"\"\"\nAuthenticate against Google GData service\n\"\"\"\n", "func_signal": "def gdata_required(f):\n", "code": "def wrap(request, *args, **kwargs):\n    if TOKEN_IN_GET not in request.GET and TOKEN_VAR not in request.session:\n        # no token at all, request one-time-token\n        # next: where to redirect\n        # scope: what service you want to get access to\n        return HttpResponseRedirect(\"https://www.google.com/accounts/AuthSubRequest?next=http://127.0.0.1:8000/pin/test_page&scope=https://www.google.com/m8/feeds&session=1\")\n    elif TOKEN_VAR not in request.session and TOKEN_IN_GET in request.GET:\n        # request session token using one-time-token\n        conn = HTTPSConnection(\"www.google.com\")\n        conn.putrequest('GET', '/accounts/AuthSubSessionToken')\n        conn.putheader('Authorization', 'AuthSub token=\"%s\"' % request.GET[TOKEN_IN_GET])\n        conn.endheaders()\n        conn.send(' ')\n        r = conn.getresponse()\n        if str(r.status) == '200':\n            token = r.read()\n            token = token.split('=')[1]\n            token = token.replace('', '')\n            request.session[TOKEN_VAR] = token\n    return f(request, *args, **kwargs)\nwrap.__doc__=f.__doc__\nwrap.__name__=f.__name__\nreturn wrap", "path": "pin\\views_test.py", "repo_name": "vchakoshy/django-pin", "stars": 84, "license": "None", "language": "python", "size": 536}
{"docstring": "# Deleting field 'Post.height'\n", "func_signal": "def backwards(self, orm):\n", "code": "db.delete_column('pin_post', 'height')\n\n# Deleting field 'Post.width'\ndb.delete_column('pin_post', 'width')", "path": "pin\\migrations\\0019_auto__add_field_post_height__add_field_post_width.py", "repo_name": "vchakoshy/django-pin", "stars": 84, "license": "None", "language": "python", "size": 536}
{"docstring": "# Removing index on 'Comments', fields ['ip_address']\n", "func_signal": "def backwards(self, orm):\n", "code": "db.delete_index('pin_comments', ['ip_address'])\n\n# Removing index on 'Comments', fields ['reported']\ndb.delete_index('pin_comments', ['reported'])", "path": "pin\\migrations\\0020_auto.py", "repo_name": "vchakoshy/django-pin", "stars": 84, "license": "None", "language": "python", "size": 536}
{"docstring": "\"\"\"\nRemoves all newline characters from a block of text.\n\"\"\"\n# First normalize the newlines using Django's nifty utility\n", "func_signal": "def remove_newlines(text):\n", "code": "normalized_text = normalize_newlines(text)\n# Then simply remove the newlines like so.\nreturn mark_safe(normalized_text.replace('\\n', ' '))", "path": "pin\\templatetags\\pin_tags.py", "repo_name": "vchakoshy/django-pin", "stars": 84, "license": "None", "language": "python", "size": 536}
{"docstring": "# Adding field 'Post.height'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.add_column('pin_post', 'height',\n              self.gf('django.db.models.fields.IntegerField')(default=-1, blank=True),\n              keep_default=False)\n\n# Adding field 'Post.width'\ndb.add_column('pin_post', 'width',\n              self.gf('django.db.models.fields.IntegerField')(default=-1, blank=True),\n              keep_default=False)", "path": "pin\\migrations\\0019_auto__add_field_post_height__add_field_post_width.py", "repo_name": "vchakoshy/django-pin", "stars": 84, "license": "None", "language": "python", "size": 536}
{"docstring": "# Removing unique constraint on 'Likes', fields ['post', 'user']\n", "func_signal": "def backwards(self, orm):\n", "code": "db.delete_unique('pin_likes', ['post_id', 'user_id'])\n\n# Removing unique constraint on 'Stream', fields ['following', 'user', 'post']\ndb.delete_unique('pin_stream', ['following_id', 'user_id', 'post_id'])\n\n# Deleting model 'Category'\ndb.delete_table('pin_category')\n\n# Deleting model 'Post'\ndb.delete_table('pin_post')\n\n# Deleting model 'Follow'\ndb.delete_table('pin_follow')\n\n# Deleting model 'Stream'\ndb.delete_table('pin_stream')\n\n# Deleting model 'Likes'\ndb.delete_table('pin_likes')\n\n# Deleting model 'Notify'\ndb.delete_table('pin_notify')\n\n# Removing M2M table for field actors on 'Notify'\ndb.delete_table('pin_notify_actors')\n\n# Deleting model 'App_data'\ndb.delete_table('pin_app_data')", "path": "pin\\migrations\\0001_initial.py", "repo_name": "vchakoshy/django-pin", "stars": 84, "license": "None", "language": "python", "size": 536}
{"docstring": "# Adding field 'Likes.ip'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.add_column('pin_likes', 'ip',\n              self.gf('django.db.models.fields.IPAddressField')(default='127.0.0.1', max_length=15),\n              keep_default=False)", "path": "pin\\migrations\\0002_auto__add_field_likes_ip.py", "repo_name": "vchakoshy/django-pin", "stars": 84, "license": "None", "language": "python", "size": 536}
{"docstring": "# Adding field 'Comments.reported'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.add_column('pin_comments', 'reported',\n              self.gf('django.db.models.fields.BooleanField')(default=False),\n              keep_default=False)", "path": "pin\\migrations\\0009_auto__add_field_comments_reported.py", "repo_name": "vchakoshy/django-pin", "stars": 84, "license": "None", "language": "python", "size": 536}
{"docstring": "# Deleting field 'Comments.text'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.delete_column('pin_comments', 'text')\n\n# Deleting field 'Comments.date'\ndb.delete_column('pin_comments', 'date')\n\n# Deleting field 'Comments.ip'\ndb.delete_column('pin_comments', 'ip')\n\n# Deleting field 'Comments.post'\ndb.delete_column('pin_comments', 'post_id')\n\n# Deleting field 'Comments.public'\ndb.delete_column('pin_comments', 'public')\n\n# Adding field 'Comments.comment'\ndb.add_column('pin_comments', 'comment',\n              self.gf('django.db.models.fields.TextField')(default=''),\n              keep_default=False)\n\n# Adding field 'Comments.submit_date'\ndb.add_column('pin_comments', 'submit_date',\n              self.gf('django.db.models.fields.DateTimeField')(auto_now_add=True, default=datetime.datetime(2013, 7, 18, 0, 0), blank=True),\n              keep_default=False)\n\n# Adding field 'Comments.ip_address'\ndb.add_column('pin_comments', 'ip_address',\n              self.gf('django.db.models.fields.IPAddressField')(default='127.0.0.1', max_length=15),\n              keep_default=False)\n\n# Adding field 'Comments.is_public'\ndb.add_column('pin_comments', 'is_public',\n              self.gf('django.db.models.fields.BooleanField')(default=False),\n              keep_default=False)\n\n# Adding field 'Comments.object_pk'\ndb.add_column('pin_comments', 'object_pk',\n              self.gf('django.db.models.fields.related.ForeignKey')(default=1, related_name='comment_post', to=orm['pin.Post']),\n              keep_default=False)", "path": "pin\\migrations\\0010_auto__del_field_comments_text__del_field_comments_date__del_field_comm.py", "repo_name": "vchakoshy/django-pin", "stars": 84, "license": "None", "language": "python", "size": 536}
{"docstring": "# Adding model 'Category'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.create_table('pin_category', (\n    ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n    ('title', self.gf('django.db.models.fields.CharField')(max_length=250)),\n    ('image', self.gf('django.db.models.fields.files.ImageField')(default='', max_length=100)),\n))\ndb.send_create_signal('pin', ['Category'])\n\n# Adding model 'Post'\ndb.create_table('pin_post', (\n    ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n    ('text', self.gf('django.db.models.fields.TextField')(blank=True)),\n    ('image', self.gf('django.db.models.fields.CharField')(max_length=500)),\n    ('create_date', self.gf('django.db.models.fields.DateField')(auto_now_add=True, blank=True)),\n    ('create', self.gf('django.db.models.fields.DateTimeField')(auto_now_add=True, blank=True)),\n    ('timestamp', self.gf('django.db.models.fields.IntegerField')(default=1347546432, db_index=True)),\n    ('user', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['auth.User'])),\n    ('like', self.gf('django.db.models.fields.IntegerField')(default=0)),\n    ('url', self.gf('django.db.models.fields.CharField')(max_length=2000, blank=True)),\n    ('status', self.gf('django.db.models.fields.IntegerField')(default=0, blank=True)),\n    ('device', self.gf('django.db.models.fields.IntegerField')(default=1, blank=True)),\n    ('hash', self.gf('django.db.models.fields.CharField')(db_index=True, max_length=32, blank=True)),\n    ('actions', self.gf('django.db.models.fields.IntegerField')(default=1, blank=True)),\n    ('is_ads', self.gf('django.db.models.fields.BooleanField')(default=False)),\n    ('category', self.gf('django.db.models.fields.related.ForeignKey')(default=1, to=orm['pin.Category'])),\n))\ndb.send_create_signal('pin', ['Post'])\n\n# Adding model 'Follow'\ndb.create_table('pin_follow', (\n    ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n    ('follower', self.gf('django.db.models.fields.related.ForeignKey')(related_name='follower', to=orm['auth.User'])),\n    ('following', self.gf('django.db.models.fields.related.ForeignKey')(related_name='following', to=orm['auth.User'])),\n))\ndb.send_create_signal('pin', ['Follow'])\n\n# Adding model 'Stream'\ndb.create_table('pin_stream', (\n    ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n    ('following', self.gf('django.db.models.fields.related.ForeignKey')(related_name='stream_following', to=orm['auth.User'])),\n    ('user', self.gf('django.db.models.fields.related.ForeignKey')(related_name='user', to=orm['auth.User'])),\n    ('post', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['pin.Post'])),\n    ('date', self.gf('django.db.models.fields.IntegerField')(default=0)),\n))\ndb.send_create_signal('pin', ['Stream'])\n\n# Adding unique constraint on 'Stream', fields ['following', 'user', 'post']\ndb.create_unique('pin_stream', ['following_id', 'user_id', 'post_id'])\n\n# Adding model 'Likes'\ndb.create_table('pin_likes', (\n    ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n    ('user', self.gf('django.db.models.fields.related.ForeignKey')(related_name='pin_post_user_like', to=orm['auth.User'])),\n    ('post', self.gf('django.db.models.fields.related.ForeignKey')(related_name='post_item', to=orm['pin.Post'])),\n))\ndb.send_create_signal('pin', ['Likes'])\n\n# Adding unique constraint on 'Likes', fields ['post', 'user']\ndb.create_unique('pin_likes', ['post_id', 'user_id'])\n\n# Adding model 'Notify'\ndb.create_table('pin_notify', (\n    ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n    ('post', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['pin.Post'])),\n    ('user', self.gf('django.db.models.fields.related.ForeignKey')(related_name='userid', to=orm['auth.User'])),\n    ('text', self.gf('django.db.models.fields.CharField')(max_length=500)),\n    ('seen', self.gf('django.db.models.fields.BooleanField')(default=False)),\n    ('type', self.gf('django.db.models.fields.IntegerField')(default=1)),\n    ('date', self.gf('django.db.models.fields.DateTimeField')(auto_now_add=True, blank=True)),\n))\ndb.send_create_signal('pin', ['Notify'])\n\n# Adding M2M table for field actors on 'Notify'\ndb.create_table('pin_notify_actors', (\n    ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),\n    ('notify', models.ForeignKey(orm['pin.notify'], null=False)),\n    ('user', models.ForeignKey(orm['auth.user'], null=False))\n))\ndb.create_unique('pin_notify_actors', ['notify_id', 'user_id'])\n\n# Adding model 'App_data'\ndb.create_table('pin_app_data', (\n    ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n    ('name', self.gf('django.db.models.fields.CharField')(max_length=250)),\n    ('file', self.gf('django.db.models.fields.files.FileField')(max_length=100)),\n    ('version', self.gf('django.db.models.fields.CharField')(max_length=50)),\n    ('current', self.gf('django.db.models.fields.BooleanField')(default=True)),\n))\ndb.send_create_signal('pin', ['App_data'])", "path": "pin\\migrations\\0001_initial.py", "repo_name": "vchakoshy/django-pin", "stars": 84, "license": "None", "language": "python", "size": 536}
{"docstring": "# Adding field 'Post.show_in_default'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.add_column('pin_post', 'show_in_default',\n              self.gf('django.db.models.fields.BooleanField')(default=False),\n              keep_default=False)", "path": "pin\\migrations\\0005_auto__add_field_post_show_in_default.py", "repo_name": "vchakoshy/django-pin", "stars": 84, "license": "None", "language": "python", "size": 536}
{"docstring": "# Adding field 'Post.view'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.add_column('pin_post', 'view',\n              self.gf('django.db.models.fields.IntegerField')(default=0),\n              keep_default=False)", "path": "pin\\migrations\\0003_auto__add_field_post_view.py", "repo_name": "vchakoshy/django-pin", "stars": 84, "license": "None", "language": "python", "size": 536}
{"docstring": "''' raw_data: if True, upfile is a HttpRequest object with raw post data\n    as the file, rather than a Django UploadedFile from request.FILES '''\n", "func_signal": "def save_upload(uploaded, filename, raw_data):\n", "code": "try:\n    from io import FileIO, BufferedWriter\n    with BufferedWriter(FileIO(\"%s/pin/temp/o/%s\" % (MEDIA_ROOT, filename), \"wb\")) as dest:\n\n        if raw_data:\n            foo = uploaded.read(1024)\n            while foo:\n                dest.write(foo)\n                foo = uploaded.read(1024)\n\n        else:\n            for c in uploaded.chunks():\n                dest.write(c)\n        return True\nexcept IOError:\n    # could not open the file most likely\n    return False", "path": "pin\\views_user.py", "repo_name": "vchakoshy/django-pin", "stars": 84, "license": "None", "language": "python", "size": 536}
{"docstring": "# Adding index on 'Comments', fields ['reported']\n", "func_signal": "def forwards(self, orm):\n", "code": "db.create_index('pin_comments', ['reported'])\n\n# Adding index on 'Comments', fields ['ip_address']\ndb.create_index('pin_comments', ['ip_address'])", "path": "pin\\migrations\\0020_auto.py", "repo_name": "vchakoshy/django-pin", "stars": 84, "license": "None", "language": "python", "size": 536}
{"docstring": "\"\"\"\nUse this method to apply an iterable of filters to\na stream. If lexer is given it's forwarded to the\nfilter, otherwise the filter receives `None`.\n\"\"\"\n\n", "func_signal": "def apply_filters(stream, filters, lexer=None):\n", "code": "def _apply(filter_, stream):\n    for token in filter_.filter(lexer, stream):\n        yield token\n\nfor filter_ in filters:\n    stream = _apply(filter_, stream)\nreturn stream", "path": "sqlparse\\lexer.py", "repo_name": "freewizard/SublimeFormatSQL", "stars": 111, "license": "other", "language": "python", "size": 113}
{"docstring": "\"\"\"Returns the type of a statement.\n\nThe returned value is a string holding an upper-cased reprint of\nthe first DML or DDL keyword. If the first token in this group\nisn't a DML or DDL keyword \"UNKNOWN\" is returned.\n\"\"\"\n", "func_signal": "def get_type(self):\n", "code": "first_token = self.token_first()\nif first_token is None:\n    # An \"empty\" statement that either has not tokens at all\n    # or only whitespace tokens.\n    return 'UNKNOWN'\nelif first_token.ttype in (T.Keyword.DML, T.Keyword.DDL):\n    return first_token.value.upper()\nelse:\n    return 'UNKNOWN'", "path": "sqlparse\\sql.py", "repo_name": "freewizard/SublimeFormatSQL", "stars": 111, "license": "other", "language": "python", "size": 113}
{"docstring": "\"\"\"Returns the previous token relative to *idx*.\n\nIf *skip_ws* is ``True`` (the default) whitespace tokens are ignored.\n``None`` is returned if there's no previous token.\n\"\"\"\n", "func_signal": "def token_prev(self, idx, skip_ws=True):\n", "code": "if idx is None:\n    return None\nif not isinstance(idx, int):\n    idx = self.token_index(idx)\nwhile idx != 0:\n    idx -= 1\n    if self.tokens[idx].is_whitespace() and skip_ws:\n        continue\n    return self.tokens[idx]", "path": "sqlparse\\sql.py", "repo_name": "freewizard/SublimeFormatSQL", "stars": 111, "license": "other", "language": "python", "size": 113}
{"docstring": "\"\"\"Checks whether the token matches the given arguments.\n\n*ttype* is a token type. If this token doesn't match the given token\ntype.\n*values* is a list of possible values for this token. The values\nare OR'ed together so if only one of the values matches ``True``\nis returned. Except for keyword tokens the comparison is\ncase-sensitive. For convenience it's ok to pass in a single string.\nIf *regex* is ``True`` (default is ``False``) the given values are\ntreated as regular expressions.\n\"\"\"\n", "func_signal": "def match(self, ttype, values, regex=False):\n", "code": "type_matched = self.ttype is ttype\nif not type_matched or values is None:\n    return type_matched\nif isinstance(values, str):\n    values = set([values])\nif regex:\n    if self.ttype is T.Keyword:\n        values = set([re.compile(v, re.IGNORECASE) for v in values])\n    else:\n        values = set([re.compile(v) for v in values])\n    for pattern in values:\n        if pattern.search(self.value):\n            return True\n    return False\nelse:\n    if self.ttype in T.Keyword:\n        values = set([v.upper() for v in values])\n        return self.value.upper() in values\n    else:\n        return self.value in values", "path": "sqlparse\\sql.py", "repo_name": "freewizard/SublimeFormatSQL", "stars": 111, "license": "other", "language": "python", "size": 113}
{"docstring": "\"\"\"Split *sql* into single statements.\n\nReturns a list of strings.\n\"\"\"\n", "func_signal": "def split(sql):\n", "code": "stack = engine.FilterStack()\nstack.split_statements = True\nreturn [unicode(stmt) for stmt in stack.run(sql)]", "path": "sqlparse\\__init__.py", "repo_name": "freewizard/SublimeFormatSQL", "stars": 111, "license": "other", "language": "python", "size": 113}
{"docstring": "\"\"\"Returns the first child token.\n\nIf *ignore_whitespace* is ``True`` (the default), whitespace\ntokens are ignored.\n\"\"\"\n", "func_signal": "def token_first(self, ignore_whitespace=True):\n", "code": "for token in self.tokens:\n    if ignore_whitespace and token.is_whitespace():\n        continue\n    return token\nreturn None", "path": "sqlparse\\sql.py", "repo_name": "freewizard/SublimeFormatSQL", "stars": 111, "license": "other", "language": "python", "size": 113}
{"docstring": "\"\"\"Parse sql and return a list of statements.\n\n*sql* is a single string containting one or more SQL statements.\n\nReturns a tuple of :class:`~sqlparse.sql.Statement` instances.\n\"\"\"\n", "func_signal": "def parse(sql):\n", "code": "stack = engine.FilterStack()\nstack.full_analyze()\nreturn tuple(stack.run(sql))", "path": "sqlparse\\__init__.py", "repo_name": "freewizard/SublimeFormatSQL", "stars": 111, "license": "other", "language": "python", "size": 113}
{"docstring": "\"\"\"Generator yielding ungrouped tokens.\n\nThis method is recursively called for all child tokens.\n\"\"\"\n", "func_signal": "def flatten(self):\n", "code": "for token in self.tokens:\n    if isinstance(token, TokenList):\n        for item in token.flatten():\n            yield item\n    else:\n        yield token", "path": "sqlparse\\sql.py", "repo_name": "freewizard/SublimeFormatSQL", "stars": 111, "license": "other", "language": "python", "size": 113}
{"docstring": "\"\"\"\nSplit ``text`` into (tokentype, text) pairs.\n\n``stack`` is the inital stack (default: ``['root']``)\n\"\"\"\n", "func_signal": "def get_tokens_unprocessed(self, text, stack=('root',)):\n", "code": "pos = 0\ntokendefs = self._tokens  # see __call__, pylint:disable=E1101\nstatestack = list(stack)\nstatetokens = tokendefs[statestack[-1]]\nknown_names = {}\nwhile 1:\n    for rexmatch, action, new_state in statetokens:\n        m = rexmatch(text, pos)\n        if m:\n            # print rex.pattern\n            value = m.group()\n            if value in known_names:\n                yield pos, known_names[value], value\n            elif type(action) is tokens._TokenType:\n                yield pos, action, value\n            elif hasattr(action, '__call__'):\n                ttype, value = action(value)\n                known_names[value] = ttype\n                yield pos, ttype, value\n            else:\n                for item in action(self, m):\n                    yield item\n            pos = m.end()\n            if new_state is not None:\n                # state transition\n                if isinstance(new_state, tuple):\n                    for state in new_state:\n                        if state == '#pop':\n                            statestack.pop()\n                        elif state == '#push':\n                            statestack.append(statestack[-1])\n                        else:\n                            statestack.append(state)\n                elif isinstance(new_state, int):\n                    # pop\n                    del statestack[new_state:]\n                elif new_state == '#push':\n                    statestack.append(statestack[-1])\n                else:\n                    assert False, \"wrong state def: %r\" % new_state\n                statetokens = tokendefs[statestack[-1]]\n            break\n    else:\n        try:\n            if text[pos] == '\\n':\n                # at EOL, reset state to \"root\"\n                pos += 1\n                statestack = ['root']\n                statetokens = tokendefs['root']\n                yield pos, tokens.Text, u'\\n'\n                continue\n            yield pos, tokens.Error, text[pos]\n            pos += 1\n        except IndexError:\n            break", "path": "sqlparse\\lexer.py", "repo_name": "freewizard/SublimeFormatSQL", "stars": 111, "license": "other", "language": "python", "size": 113}
{"docstring": "\"\"\"Return all tokens between (and including) start and end.\n\nIf *exclude_end* is ``True`` (default is ``False``) the end token\nis included too.\n\"\"\"\n# FIXME(andi): rename exclude_end to inlcude_end\n", "func_signal": "def tokens_between(self, start, end, exclude_end=False):\n", "code": "if exclude_end:\n    offset = 0\nelse:\n    offset = 1\nend_idx = self.token_index(end) + offset\nstart_idx = self.token_index(start)\nreturn self.tokens[start_idx:end_idx]", "path": "sqlparse\\sql.py", "repo_name": "freewizard/SublimeFormatSQL", "stars": 111, "license": "other", "language": "python", "size": 113}
{"docstring": "\"\"\"Returns ``True`` if *other* is in this tokens ancestry.\"\"\"\n", "func_signal": "def has_ancestor(self, other):\n", "code": "parent = self.parent\nwhile parent:\n    if parent == other:\n        return True\n    parent = parent.parent\nreturn False", "path": "sqlparse\\sql.py", "repo_name": "freewizard/SublimeFormatSQL", "stars": 111, "license": "other", "language": "python", "size": 113}
{"docstring": "\"\"\"Replace tokens by an instance of *grp_cls*.\"\"\"\n", "func_signal": "def group_tokens(self, grp_cls, tokens, ignore_ws=False):\n", "code": "idx = self.token_index(tokens[0])\nif ignore_ws:\n    while tokens and tokens[-1].is_whitespace():\n        tokens = tokens[:-1]\nfor t in tokens:\n    self.tokens.remove(t)\ngrp = grp_cls(tokens)\nfor token in tokens:\n    token.parent = grp\ngrp.parent = self\nself.tokens.insert(idx, grp)\nreturn grp", "path": "sqlparse\\sql.py", "repo_name": "freewizard/SublimeFormatSQL", "stars": 111, "license": "other", "language": "python", "size": 113}
{"docstring": "\"\"\"\nReturn an iterable of (tokentype, value) pairs generated from\n`text`. If `unfiltered` is set to `True`, the filtering mechanism\nis bypassed even if filters are defined.\n\nAlso preprocess the text, i.e. expand tabs and strip it if\nwanted and applies registered filters.\n\"\"\"\n", "func_signal": "def get_tokens(self, text, unfiltered=False):\n", "code": "if not isinstance(text, str):\n    if self.encoding == 'guess':\n        try:\n            text = text.decode('utf-8')\n            if text.startswith(u'\\ufeff'):\n                text = text[len(u'\\ufeff'):]\n        except UnicodeDecodeError:\n            text = text.decode('latin1')\n    else:\n        text = text.decode(self.encoding)\nif self.stripall:\n    text = text.strip()\nelif self.stripnl:\n    text = text.strip('\\n')\nif self.tabsize > 0:\n    text = text.expandtabs(self.tabsize)", "path": "sqlparse\\lexer.py", "repo_name": "freewizard/SublimeFormatSQL", "stars": 111, "license": "other", "language": "python", "size": 113}
{"docstring": "\"\"\"Returns the alias for this identifier or ``None``.\"\"\"\n", "func_signal": "def get_alias(self):\n", "code": "kw = self.token_next_match(0, T.Keyword, 'AS')\nif kw is not None:\n    alias = self.token_next(self.token_index(kw))\n    if alias is None:\n        return None\nelse:\n    next_ = self.token_next_by_instance(0, Identifier)\n    if next_ is None:\n        return None\n    alias = next_\nif isinstance(alias, Identifier):\n    return alias.get_name()\nelse:\n    return alias.to_unicode()", "path": "sqlparse\\sql.py", "repo_name": "freewizard/SublimeFormatSQL", "stars": 111, "license": "other", "language": "python", "size": 113}
{"docstring": "# TODO(andi) Comment types should be unified, see related issue38\n", "func_signal": "def _get_next_comment(self, tlist):\n", "code": "token = tlist.token_next_by_instance(0, sql.Comment)\nif token is None:\n    token = tlist.token_next_by_type(0, T.Comment)\nreturn token", "path": "sqlparse\\filters.py", "repo_name": "freewizard/SublimeFormatSQL", "stars": 111, "license": "other", "language": "python", "size": 113}
{"docstring": "\"\"\"Returns ``True`` if this token is within *group_cls*.\n\nUse this method for example to check if an identifier is within\na function: ``t.within(sql.Function)``.\n\"\"\"\n", "func_signal": "def within(self, group_cls):\n", "code": "parent = self.parent\nwhile parent:\n    if isinstance(parent, group_cls):\n        return True\n    parent = parent.parent\nreturn False", "path": "sqlparse\\sql.py", "repo_name": "freewizard/SublimeFormatSQL", "stars": 111, "license": "other", "language": "python", "size": 113}
{"docstring": "\"\"\"Returns the typecast or ``None`` of this object as a string.\"\"\"\n", "func_signal": "def get_typecast(self):\n", "code": "marker = self.token_next_match(0, T.Punctuation, '::')\nif marker is None:\n    return None\nnext_ = self.token_next(self.token_index(marker), False)\nif next_ is None:\n    return None\nreturn next_.to_unicode()", "path": "sqlparse\\sql.py", "repo_name": "freewizard/SublimeFormatSQL", "stars": 111, "license": "other", "language": "python", "size": 113}
{"docstring": "\"\"\"Returns the identifiers.\n\nWhitespaces and punctuations are not included in this list.\n\"\"\"\n", "func_signal": "def get_identifiers(self):\n", "code": "return [x for x in self.tokens\n        if not x.is_whitespace() and not x.match(T.Punctuation, ',')]", "path": "sqlparse\\sql.py", "repo_name": "freewizard/SublimeFormatSQL", "stars": 111, "license": "other", "language": "python", "size": 113}
{"docstring": "\"\"\"Returns the next token relative to *idx*.\n\nIf *skip_ws* is ``True`` (the default) whitespace tokens are ignored.\n``None`` is returned if there's no next token.\n\"\"\"\n", "func_signal": "def token_next(self, idx, skip_ws=True):\n", "code": "if idx is None:\n    return None\nif not isinstance(idx, int):\n    idx = self.token_index(idx)\nwhile idx < len(self.tokens) - 1:\n    idx += 1\n    if self.tokens[idx].is_whitespace() and skip_ws:\n        continue\n    return self.tokens[idx]", "path": "sqlparse\\sql.py", "repo_name": "freewizard/SublimeFormatSQL", "stars": 111, "license": "other", "language": "python", "size": 113}
{"docstring": "\"\"\"Return name of the parent object if any.\n\nA parent object is identified by the first occuring dot.\n\"\"\"\n", "func_signal": "def get_parent_name(self):\n", "code": "dot = self.token_next_match(0, T.Punctuation, '.')\nif dot is None:\n    return None\nprev_ = self.token_prev(self.token_index(dot))\nif prev_ is None:  # something must be verry wrong here..\n    return None\nreturn prev_.value", "path": "sqlparse\\sql.py", "repo_name": "freewizard/SublimeFormatSQL", "stars": 111, "license": "other", "language": "python", "size": 113}
{"docstring": "#get remaining nonces\n", "func_signal": "def getRangeFromUnit(self, size):\n", "code": "        noncesLeft = self.currentUnit.nonces - self.currentUnit.base\n# Flag indicating if the WorkUnit was depeleted by this request\n        depleted = False\n#if there are enough nonces to fill the full reqest\n        if noncesLeft >= size:\n            nr = NonceRange(self.currentUnit, self.currentUnit.base, size)\n    #check if this uses up the rest of the WorkUnit\n            if size >= noncesLeft:\n                depleted = True\n            else:\n                self.currentUnit.base += size\n#otherwise send whatever is left\n        else:\n            nr = NonceRange(\n                self.currentUnit, self.currentUnit.base, noncesLeft)\n            depleted = True\n#return the range\n        return nr, depleted", "path": "phoenix2\\core\\WorkQueue.py", "repo_name": "phoenix2/phoenix", "stars": 109, "license": "None", "language": "python", "size": 183}
{"docstring": "\"\"\"Change the interval at which to poll the getwork() function.\"\"\"\n", "func_signal": "def setInterval(self, interval):\n", "code": "self.askInterval = interval\nself._startCall()", "path": "phoenix2\\backend\\RPCProtocol.py", "repo_name": "phoenix2/phoenix", "stars": 109, "license": "None", "language": "python", "size": 183}
{"docstring": "\"\"\"Sends a result to the server, returning a Deferred that fires with\na bool to indicate whether or not the work was accepted.\n\"\"\"\n\n# Must be a 128-byte response, but the last 48 are typically ignored.\n", "func_signal": "def sendResult(self, result):\n", "code": "result += '\\x00'*48\n\nd = self.poller.call('getwork', [result.encode('hex')])\n\ndef errback(*ignored):\n    return False # ANY error while turning in work is a Bad Thing(TM).\n\n#we need to return the result, not the headers\ndef callback(x):\n    try:\n        (headers, accepted) = x\n    except TypeError:\n        self.runCallback('debug',\n                'TypeError in RPC sendResult callback:')\n        self.runCallback('debug', str(x))\n        return False\n\n    if (not accepted):\n        self.handleRejectReason(headers)\n\n    return accepted\n\nd.addCallback(callback)\nd.addErrback(errback)\nreturn d", "path": "phoenix2\\backend\\RPCProtocol.py", "repo_name": "phoenix2/phoenix", "stars": 109, "license": "None", "language": "python", "size": 183}
{"docstring": "\"\"\"Attempt to load JSON-RPC data.\"\"\"\n\n", "func_signal": "def parse(cls, data):\n", "code": "response = json.loads(data)\ntry:\n    message = response['error']['message']\nexcept (KeyError, TypeError):\n    pass\nelse:\n    raise ServerMessage(message)\n\nreturn response.get('result')", "path": "phoenix2\\backend\\RPCProtocol.py", "repo_name": "phoenix2/phoenix", "stars": 109, "license": "None", "language": "python", "size": 183}
{"docstring": "# Called 5 seconds before any work expires in order to fetch more\n", "func_signal": "def checkWork(self):\n", "code": "if self.checkQueue():\n    if self.core.connection:\n        self.core.requestWork()", "path": "phoenix2\\core\\WorkQueue.py", "repo_name": "phoenix2/phoenix", "stars": 109, "license": "None", "language": "python", "size": 183}
{"docstring": "\"\"\"Given a 512-bit (64-byte) block of (little-endian byteswapped) data,\ncalculate a Bitcoin-style midstate. (That is, if SHA-256 were little-endian\nand only hashed the first block of input.)\n\"\"\"\n", "func_signal": "def calculateMidstate(data, state=None, rounds=None):\n", "code": "if len(data) != 64:\n    raise ValueError('data must be 64 bytes long')\n\nw = list(struct.unpack('<IIIIIIIIIIIIIIII', data))\n\nif state is not None:\n    if len(state) != 32:\n        raise ValueError('state must be 32 bytes long')\n    a,b,c,d,e,f,g,h = struct.unpack('<IIIIIIII', state)\nelse:\n    a = A0\n    b = B0\n    c = C0\n    d = D0\n    e = E0\n    f = F0\n    g = G0\n    h = H0\n\nconsts = K if rounds is None else K[:rounds]\nfor k in consts:\n    s0 = rotateright(a,2) ^ rotateright(a,13) ^ rotateright(a,22)\n    s1 = rotateright(e,6) ^ rotateright(e,11) ^ rotateright(e,25)\n    ma = (a&b) ^ (a&c) ^ (b&c)\n    ch = (e&f) ^ ((~e)&g)\n\n    h = addu32(h,w[0],k,ch,s1)\n    d = addu32(d,h)\n    h = addu32(h,ma,s0)\n\n    a,b,c,d,e,f,g,h = h,a,b,c,d,e,f,g\n\n    s0 = rotateright(w[1],7) ^ rotateright(w[1],18) ^ (w[1] >> 3)\n    s1 = rotateright(w[14],17) ^ rotateright(w[14],19) ^ (w[14] >> 10)\n    w.append(addu32(w[0], s0, w[9], s1))\n    w.pop(0)\n\nif rounds is None:\n    a = addu32(a, A0)\n    b = addu32(b, B0)\n    c = addu32(c, C0)\n    d = addu32(d, D0)\n    e = addu32(e, E0)\n    f = addu32(f, F0)\n    g = addu32(g, G0)\n    h = addu32(h, H0)\n\nreturn struct.pack('<IIIIIIII', a, b, c, d, e, f, g, h)", "path": "phoenix2\\util\\Midstate.py", "repo_name": "phoenix2/phoenix", "stars": 109, "license": "None", "language": "python", "size": 183}
{"docstring": "\"\"\"Cease server communications immediately. The client is probably not\nreusable, so it's probably best not to try.\n\"\"\"\n\n", "func_signal": "def disconnect(self):\n", "code": "self._deactivateCallbacks()\nself.disconnected = True\nself.poller.setInterval(None)\nself.poller.closeConnection()\nif self.longPoller:\n    self.longPoller.stop()\n    self.longPoller = None", "path": "phoenix2\\backend\\RPCProtocol.py", "repo_name": "phoenix2/phoenix", "stars": 109, "license": "None", "language": "python", "size": 183}
{"docstring": "\"\"\"Begin requesting data from the LP server, if we aren't already...\"\"\"\n", "func_signal": "def start(self):\n", "code": "if self.polling:\n    return\nself.polling = True\nself._request()", "path": "phoenix2\\backend\\RPCProtocol.py", "repo_name": "phoenix2/phoenix", "stars": 109, "license": "None", "language": "python", "size": 183}
{"docstring": "# Don't expire WorkUnits if idle and queue empty\n", "func_signal": "def workExpire(self, wu):\n", "code": "if (self.core.idle) and (len(self.queue) <= 1):\n    return\n\n# Remove the WorkUnit from queue\nif len(self.queue) > 0:\n    iSize = len(self.queue)\n    if not (len(self.queue) == 1 and (self.currentUnit is None)):\n        try:\n            self.queue.remove(wu)\n        except ValueError: pass\n    if self.currentUnit == wu:\n        self.currentUnit = None\n\n    # Check queue size\n    if self.checkQueue() and (iSize != len(self.queue)):\n        if self.core.connection:\n            self.core.connection.requestWork()\n\n    # Flag the WorkUnit as stale\n    wu.stale()\nelse:\n    # Check back again later if we didn't expire the work\n    reactor.callLater(5, self.workExpire, wu)", "path": "phoenix2\\core\\WorkQueue.py", "repo_name": "phoenix2/phoenix", "stars": 109, "license": "None", "language": "python", "size": 183}
{"docstring": "\"\"\"Format this log for appearance in the console.\"\"\"\n\n", "func_signal": "def formatConsole(self, logger, fullDate=False):\n", "code": "timeformat = '%m/%d/%Y %H:%M:%S' if fullDate else '%H:%M:%S'\n\noutput = '[%s] ' % time.strftime(timeformat, time.localtime(self.time))\nif self.kernelif:\n    output += '[%s] ' % self.kernelif.getName()\n\noutput += self.getMsg(logger.isVerbose())\n\nreturn output", "path": "phoenix2\\core\\PhoenixLogger.py", "repo_name": "phoenix2/phoenix", "stars": 109, "license": "None", "language": "python", "size": 183}
{"docstring": "\"\"\"Stop polling. This LongPoller probably shouldn't be reused.\"\"\"\n", "func_signal": "def stop(self):\n", "code": "self.polling = False\nself.closeConnection()", "path": "phoenix2\\backend\\RPCProtocol.py", "repo_name": "phoenix2/phoenix", "stars": 109, "license": "None", "language": "python", "size": 183}
{"docstring": "\"\"\"Call the specified remote function.\"\"\"\n\n", "func_signal": "def call(self, method, params=[]):\n", "code": "body = json.dumps({'method': method, 'params': params, 'id': 1})\npath = self.root.url.path or '/'\nif self.root.url.query:\n    path += '?' + self.root.url.query\nresponse = yield self.doRequest(\n    self.root.url,\n    'POST',\n    path,\n    body,\n    {\n        'Authorization': self.root.auth,\n        'User-Agent': self.root.version,\n        'Content-Type': 'application/json',\n        'X-Work-Identifier': '1',\n        'X-Mining-Extensions': self.root.EXTENSIONS\n    })\n\n(headers, data) = response\nresult = self.parse(data)\ndefer.returnValue((headers, result))", "path": "phoenix2\\backend\\RPCProtocol.py", "repo_name": "phoenix2/phoenix", "stars": 109, "license": "None", "language": "python", "size": 183}
{"docstring": "#check if this work matches the previous block\n", "func_signal": "def storeWork(self, aw):\n", "code": "        if (self.lastBlock is not None) and (aw.identifier == self.lastBlock):\n            self.logger.debug('Server gave work from the previous '\n                              'block, ignoring.')\n            #if the queue is too short request more work\n            if self.checkQueue():\n                if self.core.connection:\n                    self.core.connection.requestWork()\n            return\n#create a WorkUnit\n        work = WorkUnit(aw)\n        reactor.callLater(max(60, aw.time - 1) - self.queueDelay,\n                            self.checkWork)\n        reactor.callLater(max(60, aw.time - 1), self.workExpire, work)\n#check if there is a new block, if so reset queue\n        newBlock = (aw.identifier != self.block)\n        if newBlock:\n            self.queue.clear()\n            self.currentUnit = None\n            self.lastBlock = self.block\n            self.block = aw.identifier\n            self.logger.debug(\"New block (WorkQueue)\")\n#add new WorkUnit to queue\n        if work.data and work.target and work.midstate and work.nonces:\n            self.queue.append(work)\n#if the queue is too short request more work\n        workRequested = False\n        if self.checkQueue():\n            if self.core.connection:\n                self.core.connection.requestWork()\n                workRequested = True\n#if there is a new block notify kernels that their work is now stale\n        if newBlock:\n            for callback in self.staleCallbacks:\n                callback()\n            self.staleCallbacks = []\n        self.staleCallbacks.append(work.stale)\n#check if there are deferred WorkUnit requests pending\n        #since requests to fetch a WorkUnit can add additional deferreds to\n        #the queue, cache the size beforehand to avoid infinite loops.\n        for i in range(len(self.deferredQueue)):\n            df = self.deferredQueue.popleft()\n            d = self.fetchUnit(workRequested)\n            d.chainDeferred(df)\n#clear the idle flag since we just added work to queue\n        self.core.reportIdle(False)", "path": "phoenix2\\core\\WorkQueue.py", "repo_name": "phoenix2/phoenix", "stars": 109, "license": "None", "language": "python", "size": 183}
{"docstring": "\"\"\"Call the callback on the handler, if it's there, specifying args.\"\"\"\n\n", "func_signal": "def runCallback(self, callback, *args):\n", "code": "if not self.callbacksActive:\n    return\n\nfunc = getattr(self.handler, 'on' + callback.capitalize(), None)\nif callable(func):\n    func(*args)", "path": "phoenix2\\backend\\ClientBase.py", "repo_name": "phoenix2/phoenix", "stars": 109, "license": "None", "language": "python", "size": 183}
{"docstring": "\"\"\"Apply any kernel-specific metadata.\"\"\"\n", "func_signal": "def applyMeta(self):\n", "code": "self.interface.setMeta('kernel', 'phatk2 r%s' % self.REVISION)\nself.interface.setMeta('device',\n                        self.device.name.replace('\\x00','').strip())\nself.interface.setMeta('cores', self.device.max_compute_units)", "path": "phoenix2\\plugins\\phatk2\\__init__.py", "repo_name": "phoenix2/phoenix", "stars": 109, "license": "None", "language": "python", "size": 183}
{"docstring": "\"\"\"Run a getwork request immediately.\"\"\"\n\n", "func_signal": "def ask(self):\n", "code": "if self.currentAsk and not self.currentAsk.called:\n     return\nself._stopCall()\n\nself.currentAsk = self.call('getwork')\n\ndef errback(failure):\n    try:\n        if failure.check(ServerMessage):\n            self.root.runCallback('msg', failure.getErrorMessage())\n        self.root._failure()\n    finally:\n        self._startCall()\n\nself.currentAsk.addErrback(errback)\n\ndef callback(x):\n    try:\n        try:\n            (headers, result) = x\n        except TypeError:\n            return\n        self.root.handleWork(result, headers)\n        self.root.handleHeaders(headers)\n    finally:\n        self._startCall()\nself.currentAsk.addCallback(callback)", "path": "phoenix2\\backend\\RPCProtocol.py", "repo_name": "phoenix2/phoenix", "stars": 109, "license": "None", "language": "python", "size": 183}
{"docstring": "# This function exists as a workaround: If the connection is closed,\n# we also want to kill the response to allow the socket to die, but\n# httplib doesn't keep the response hanging around at all, so we need\n# to intercept its creation (hence this function) and store it.\n", "func_signal": "def __makeResponse(self, *args, **kwargs):\n", "code": "self.__response = httplib.HTTPResponse(*args, **kwargs)\nreturn self.__response", "path": "phoenix2\\backend\\RPCProtocol.py", "repo_name": "phoenix2/phoenix", "stars": 109, "license": "None", "language": "python", "size": 183}
{"docstring": "\"\"\"Format a positive integer in a more readable fashion.\"\"\"\n", "func_signal": "def formatNumber(cls, n):\n", "code": "if n < 0:\n    raise ValueError('can only format positive integers')\nprefixes = 'KMGTP'\nwhole = str(int(n))\ndecimal = ''\ni = 0\nwhile len(whole) > 3:\n    if i + 1 < len(prefixes):\n        decimal = '.%s' % whole[-3:-1]\n        whole = whole[:-3]\n        i += 1\n    else:\n        break\nreturn '%s%s %s' % (whole, decimal, prefixes[i])", "path": "phoenix2\\core\\PhoenixLogger.py", "repo_name": "phoenix2/phoenix", "stars": 109, "license": "None", "language": "python", "size": 183}
{"docstring": "# This class method is for analyzing how well a kernel will support a\n# specific device to help Phoenix automatically choose kernels.\n# See doc/cpu.py for further details.\n\n# Make sure we only deal with OpenCL devices.\n", "func_signal": "def analyzeDevice(cls, devid):\n", "code": "if devid.startswith('cl:'):\n    (platform, device) = cls.getDevice(devid)\n\n    if (platform is not None) and (device is not None):\n        # Get the device name\n        name = device.name.replace('\\x00','').strip()\n\n        # Check if the device is a CPU\n        if device.get_info(cl.device_info.TYPE) == cl.device_type.CPU:\n            return (1, {'name': name, 'aggression': 0},\n                        [devid, 'cpu:0'])\n\n        # Check if the device has CUDA support\n        ids = devid.split(':',3)\n        if 'nvidia cuda' in platform.name.lower():\n            return (1, {'name': (name + ' ' + ids[2]),\n                    'aggression': 3, }, [devid, 'cuda:' + ids[2]])\n\n        # Check if the device supports BFI_INT\n        if (device.extensions.find('cl_amd_media_ops') != -1):\n            supported = False\n            for whitelisted in WHITELIST:\n                if name in whitelisted:\n                    supported = True\n\n            if supported:\n                return (3, {'name': (name + ' ' + ids[2]),\n                        'bfi_int': True, 'vectors': True}, [devid])\n\n        # Otherwise just use a safe default config\n        return (1, {'name': (name + ' ' + ids[2]),\n                'aggression': 3}, [devid])\n    else:\n        return (0, {}, [devid])\nelse:\n    return (0, {}, [devid])", "path": "phoenix2\\plugins\\phatk2\\__init__.py", "repo_name": "phoenix2/phoenix", "stars": 109, "license": "None", "language": "python", "size": 183}
{"docstring": "\"\"\"RPC clients do not support meta. Ignore.\"\"\"\n\n", "func_signal": "def setMeta(self, var, value):\n", "code": "\nif version is not None:\n    self.version = '%s/%s' % (shortname, version)\nelse:\n    self.version = shortname", "path": "phoenix2\\backend\\RPCProtocol.py", "repo_name": "phoenix2/phoenix", "stars": 109, "license": "None", "language": "python", "size": 183}
{"docstring": "\"\"\"Select the next completion.\"\"\"\n\n", "func_signal": "def select_next(self):\n", "code": "row = min(self.get_selected() + 1, len(self.store) - 1)\nselection = self.view.get_selection()\nselection.unselect_all()\nselection.select_path(row)\nself.view.scroll_to_cell(row)\nself.text_buffer.set_text(self.completions[self.get_selected()]['info'])", "path": "pythoncodecompletion\\pythoncodecompletion.py", "repo_name": "fenrrir/geditpycompletion", "stars": 79, "license": "None", "language": "python", "size": 398}
{"docstring": "\"\"\"Set the completions to display.\"\"\"\n\n", "func_signal": "def set_completions(self, completions):\n", "code": "self.completions = completions\nself.completions.reverse()\nself.resize(1, 1)\nself.store.clear()\nfor completion in completions:\n    self.store.append([unicode(completion['abbr'])])\nself.view.columns_autosize()\nself.view.get_selection().select_path(0)\nself.text_buffer.set_text(self.completions[self.get_selected()]['info'])", "path": "pythoncodecompletion\\pythoncodecompletion.py", "repo_name": "fenrrir/geditpycompletion", "stars": 79, "license": "None", "language": "python", "size": 398}
{"docstring": "# Revert color back to normal\n", "func_signal": "def deactivate(self):\n", "code": "default_bg_color = self.parent.get_style().bg[gtk.STATE_NORMAL]\nself.modify_bg(gtk.STATE_NORMAL, default_bg_color)\nself.unset_flags(gtk.CAN_FOCUS)\nself.active = False\nkeybinding = self.getKeybinding()\n\nif not keybinding:\n    self._label.set_text(DEFAULT_TEXT)\nelse:\n    self._label.set_text(keybinding)", "path": "pythoncodecompletion\\keybindingwidget.py", "repo_name": "fenrrir/geditpycompletion", "stars": 79, "license": "None", "language": "python", "size": 398}
{"docstring": "\"\"\"\nReturns a tuple with the keybinding used to do code completion from the\nconfiguration file, e.g. {\"alt\" : True, \"ctrl\" : True, \"key\" : \"space\"}.\n\"\"\"\n", "func_signal": "def getKeybindingCompleteTuple():\n", "code": "global __keybindingCompleteTuple\n# Return cached result\nif len(__keybindingCompleteTuple) != 0:\n    return __keybindingCompleteTuple\n    \n# Parse keybinding\nalt = False\nctrl = False\nshift = False\nkey = \"\"\nkeybinding = getKeybindingComplete().split('+')\nkeybindingTuple = {\n    MODIFIER_CTRL : False,\n    MODIFIER_ALT : False,\n    MODIFIER_SHIFT : False,\n    KEY : \"\"\n}\n\nfor s in keybinding:\n    s = s.lower()\n    if s == MODIFIER_ALT:\n        keybindingTuple[MODIFIER_ALT] = True\n    elif s == MODIFIER_CTRL:\n        keybindingTuple[MODIFIER_CTRL] = True\n    elif s == MODIFIER_SHIFT:\n        keybindingTuple[MODIFIER_SHIFT] = True\n    else:\n        keybindingTuple[KEY] = s \n\n__keybindingCompleteTuple = keybindingTuple\n\nreturn __keybindingCompleteTuple", "path": "pythoncodecompletion\\configuration.py", "repo_name": "fenrrir/geditpycompletion", "stars": 79, "license": "None", "language": "python", "size": 398}
{"docstring": "\"\"\"Hide the completion window.\"\"\"\n\n", "func_signal": "def hide_popup(self):\n", "code": "self.popup.hide()\nself.completes = None\nself.completions = None", "path": "pythoncodecompletion\\pythoncodecompletion.py", "repo_name": "fenrrir/geditpycompletion", "stars": 79, "license": "None", "language": "python", "size": 398}
{"docstring": "\"\"\"Hide the completion window and return False.\"\"\"\n\n", "func_signal": "def cancel(self):\n", "code": "self.hide_popup()\nreturn False", "path": "pythoncodecompletion\\pythoncodecompletion.py", "repo_name": "fenrrir/geditpycompletion", "stars": 79, "license": "None", "language": "python", "size": 398}
{"docstring": "\"\"\"Connect the document and view in tab.\"\"\"\n\n", "func_signal": "def on_window_tab_added(self, window, tab):\n", "code": "context = tab.get_view().get_pango_context()\nfont_desc = context.get_font_description()\nself.popup.set_font_description(font_desc)\nself.connect_view(tab.get_view())", "path": "pythoncodecompletion\\pythoncodecompletion.py", "repo_name": "fenrrir/geditpycompletion", "stars": 79, "license": "None", "language": "python", "size": 398}
{"docstring": "\"\"\"Find completions and display them.\"\"\"\n\n", "func_signal": "def display_completions(self, view, event):\n", "code": "doc = view.get_buffer()\ninsert = doc.get_iter_at_mark(doc.get_insert())\nstart = insert.copy()\nwhile start.backward_char():\n    char = unicode(start.get_char())\n    if not self.re_alpha.match(char) and not char == \".\":\n        start.forward_char()\n        break\nincomplete = unicode(doc.get_text(start, insert))\nincomplete += unicode(event.string)\nif incomplete.isdigit():\n    return self.cancel()\ncompletes =  complete( doc.get_text(*doc.get_bounds()), incomplete, insert.get_line())\nif not completes:\n    return self.cancel()\nself.completes = completes\n\nif \".\" in incomplete:\n    incompletelist = incomplete.split('.')\n    newword = incompletelist[-1]\n    self.completions = list(x['abbr'][len(newword):] for x in completes)\n    length = len(newword)\nelse:\n    self.completions = list(x['abbr'][len(incomplete):] for x in completes)\n    length = len(incomplete)\nfor x in completes:\n    x['completion'] = x['abbr'][length:]\nwindow = gtk.TEXT_WINDOW_TEXT\nrect = view.get_iter_location(insert)\nx, y = view.buffer_to_window_coords(window, rect.x, rect.y)\nx, y = view.translate_coordinates(self.window, x, y)\nself.show_popup(completes, x, y)", "path": "pythoncodecompletion\\pythoncodecompletion.py", "repo_name": "fenrrir/geditpycompletion", "stars": 79, "license": "None", "language": "python", "size": 398}
{"docstring": "\"\"\"Select the previous completion.\"\"\"\n\n", "func_signal": "def select_previous(self):\n", "code": "row = max(self.get_selected() - 1, 0)\nselection = self.view.get_selection()\nselection.unselect_all()\nselection.select_path(row)\nself.view.scroll_to_cell(row)\nself.text_buffer.set_text(self.completions[self.get_selected()]['info'])", "path": "pythoncodecompletion\\pythoncodecompletion.py", "repo_name": "fenrrir/geditpycompletion", "stars": 79, "license": "None", "language": "python", "size": 398}
{"docstring": "\"\"\"\nSaves a string with the keybinding used to do code completion to the gconf\nentry, e.g. \"ctrl+alt+space\".\n\"\"\"\n", "func_signal": "def setKeybindingComplete(keybinding):\n", "code": "global __keybindingComplete\nglobal __keybindingCompleteTuple\n__client.set_string(GCONF_KEYBINDING_COMPLETE, keybinding)\n__keybindingComplete = keybinding\n__keybindingCompleteTuple = {}", "path": "pythoncodecompletion\\configuration.py", "repo_name": "fenrrir/geditpycompletion", "stars": 79, "license": "None", "language": "python", "size": 398}
{"docstring": "\"\"\"Deactivate plugin.\"\"\"\n\n", "func_signal": "def deactivate(self, window):\n", "code": "widgets = [window]\nwidgets.append(window.get_views())\nwidgets.append(window.get_documents())\nfor widget in widgets:\n    handler_ids = widget.get_data(self.name)\n    for handler_id in handler_ids:\n        widget.disconnect(handler_id)\n    widget.set_data(self.name, None)\nself.hide_popup()\nself.popup = None\nself.window = None", "path": "pythoncodecompletion\\pythoncodecompletion.py", "repo_name": "fenrrir/geditpycompletion", "stars": 79, "license": "None", "language": "python", "size": 398}
{"docstring": "\"\"\"Creates and displays a ConfigurationDialog.\"\"\"\n", "func_signal": "def create_configure_dialog(self):\n", "code": "dlg = configurationdialog.ConfigurationDialog()\nreturn dlg", "path": "pythoncodecompletion\\pythoncodecompletion.py", "repo_name": "fenrrir/geditpycompletion", "stars": 79, "license": "None", "language": "python", "size": 398}
{"docstring": "\"\"\"Initialize the frame and scroller around the tree view.\"\"\"\n\n", "func_signal": "def init_frame(self):\n", "code": "scroller = gtk.ScrolledWindow()\nscroller.set_policy(gtk.POLICY_AUTOMATIC, gtk.POLICY_NEVER)\nscroller.add(self.view)\nframe = gtk.Frame()\nframe.set_shadow_type(gtk.SHADOW_OUT)\nhbox = gtk.HBox()\nhbox.add(scroller)\n\nscroller_text = gtk.ScrolledWindow() \nscroller_text.set_policy(gtk.POLICY_AUTOMATIC, gtk.POLICY_AUTOMATIC)\nscroller_text.add(self.text)\nhbox.add(scroller_text)\nframe.add(hbox)\nself.add(frame)", "path": "pythoncodecompletion\\pythoncodecompletion.py", "repo_name": "fenrrir/geditpycompletion", "stars": 79, "license": "None", "language": "python", "size": 398}
{"docstring": "\"\"\"Show the completion window.\"\"\"\n\n", "func_signal": "def show_popup(self, completions, x, y):\n", "code": "root_x, root_y = self.window.get_position()\nself.popup.move(root_x + x + 24, root_y + y + 44)\nself.popup.set_completions(completions)\nself.popup.show_all()", "path": "pythoncodecompletion\\pythoncodecompletion.py", "repo_name": "fenrrir/geditpycompletion", "stars": 79, "license": "None", "language": "python", "size": 398}
{"docstring": "\"\"\"Get the selected row.\"\"\"\n\n", "func_signal": "def get_selected(self):\n", "code": "selection = self.view.get_selection()\nreturn selection.get_selected_rows()[1][0][0]", "path": "pythoncodecompletion\\pythoncodecompletion.py", "repo_name": "fenrrir/geditpycompletion", "stars": 79, "license": "None", "language": "python", "size": 398}
{"docstring": "\"\"\"Connect to view's signals.\"\"\"\n\n", "func_signal": "def connect_view(self, view):\n", "code": "handler_ids = []\ncallback = self.on_view_key_press_event\nhandler_id = view.connect(\"key-press-event\", callback)\nhandler_ids.append(handler_id)\nview.set_data(self.name, handler_ids)", "path": "pythoncodecompletion\\pythoncodecompletion.py", "repo_name": "fenrrir/geditpycompletion", "stars": 79, "license": "None", "language": "python", "size": 398}
{"docstring": "\"\"\"Initialize the tree view listing the completions.\"\"\"\n\n", "func_signal": "def init_tree_view(self):\n", "code": "self.store = gtk.ListStore(gobject.TYPE_STRING)\nself.view = gtk.TreeView(self.store)\nrenderer = gtk.CellRendererText()\ncolumn = gtk.TreeViewColumn(\"\", renderer, text=0)\nself.view.append_column(column)\nself.view.set_enable_search(False)\nself.view.set_headers_visible(False)\nself.view.set_rules_hint(True)\nselection = self.view.get_selection()\nselection.set_mode(gtk.SELECTION_SINGLE)\nself.view.set_size_request(200, 200)\nself.view.connect('row-activated', self.row_activated)", "path": "pythoncodecompletion\\pythoncodecompletion.py", "repo_name": "fenrrir/geditpycompletion", "stars": 79, "license": "None", "language": "python", "size": 398}
{"docstring": "\"\"\"\nReturns a string with the keybinding used to do code completion from the\nconfiguration file, e.g. \"ctrl+alt+space\"\n\"\"\"\n", "func_signal": "def getKeybindingComplete():\n", "code": "global __keybindingComplete\n# Get keybinding from cache, then gconf or else use default.\nif len(__keybindingComplete) == 0:\n    keybinding = __client.get_string(GCONF_KEYBINDING_COMPLETE)\n    __keybindingCompleteTuple = {} # Invalidate cache\n    if not keybinding:\n        __keybindingComplete = DEFAULT_KEYBINDING_COMPLETE\n    else:\n        __keybindingComplete = keybinding\n\nreturn __keybindingComplete", "path": "pythoncodecompletion\\configuration.py", "repo_name": "fenrrir/geditpycompletion", "stars": 79, "license": "None", "language": "python", "size": 398}
{"docstring": "\"\"\"Display the completion window or complete the current word.\"\"\"\n", "func_signal": "def on_view_key_press_event(self, view, event):\n", "code": "active_doc = self.window.get_active_document()\nif active_doc is None or active_doc.get_mime_type() != 'text/x-python':\n    return self.cancel()\n\n# FIXME This might result in a clash with other plugins eg. snippets\n# FIXME This code is not portable! \n#  The \"Alt\"-key might be mapped to something else\n# TODO Find out which keybinding are already in use.\nkeybinding = configuration.getKeybindingCompleteTuple()\nctrl_pressed = (event.state & gtk.gdk.CONTROL_MASK) == gtk.gdk.CONTROL_MASK\nalt_pressed = (event.state & gtk.gdk.MOD1_MASK) == gtk.gdk.MOD1_MASK\nshift_pressed = (event.state & gtk.gdk.SHIFT_MASK) == gtk.gdk.SHIFT_MASK\nkeyval = gtk.gdk.keyval_from_name(keybinding[configuration.KEY])\nkey_pressed = (event.keyval == keyval)\n\n# It's ok if a key is pressed and it's needed or\n# if a key is not pressed if it isn't needed.\nctrl_ok = not (keybinding[configuration.MODIFIER_CTRL] ^ ctrl_pressed )\nalt_ok =  not (keybinding[configuration.MODIFIER_ALT] ^ alt_pressed )\nshift_ok = not (keybinding[configuration.MODIFIER_SHIFT] ^ shift_pressed )\n\nif ctrl_ok and alt_ok and shift_ok and key_pressed or event.keyval == gtk.keysyms.period:\n    return self.display_completions(view, event)\n\nreturn self.cancel()", "path": "pythoncodecompletion\\pythoncodecompletion.py", "repo_name": "fenrrir/geditpycompletion", "stars": 79, "license": "None", "language": "python", "size": 398}
{"docstring": "\"\"\"Activate plugin.\"\"\"\n\n", "func_signal": "def activate(self, window):\n", "code": "self.window = window\nself.popup = CompletionWindow(window, self.complete)\nhandler_ids = []\ncallback = self.on_window_tab_added\nhandler_id = window.connect(\"tab-added\", callback)\nhandler_ids.append(handler_id)\nwindow.set_data(self.name, handler_ids)\nfor view in window.get_views():\n    self.connect_view(view)", "path": "pythoncodecompletion\\pythoncodecompletion.py", "repo_name": "fenrrir/geditpycompletion", "stars": 79, "license": "None", "language": "python", "size": 398}
{"docstring": "\"\"\"Return string used for indenting Body lines.\"\"\"\n", "func_signal": "def get_body_indent(body):\n", "code": "et = int(vim.eval(\"getbufvar(%s,'&et')\" %body))\nif et:\n    ts = int(vim.eval(\"getbufvar(%s,'&ts')\" %body))\n    return ' '*ts\nelse:\n    return '\\t'", "path": "autoload\\voom\\voom_mode_python.py", "repo_name": "vim-scripts/VOoM", "stars": 99, "license": "None", "language": "python", "size": 985}
{"docstring": "\"\"\"Return (tlines, bnodes, levels) for Body lines blines.\nblines is either Vim buffer object (Body) or list of buffer lines.\n\"\"\"\n", "func_signal": "def hook_makeOutline(VO, blines):\n", "code": "Z = len(blines)\ntlines, bnodes, levels = [], [], []\ntlines_add, bnodes_add, levels_add = tlines.append, bnodes.append, levels.append\nfor i in xrange(Z):\n    bline = blines[i]\n    h = bline.lstrip('\\t')\n    # line is a Task\n    if h.startswith('- '):\n        head = h[2:]\n        mark = ' '\n    # line is a Project\n    # the \"in\" test is for efficiency sake in case there is lots of Notes\n    elif h.endswith(':') or (':' in h and project_match(h)):\n        head = h\n        mark = 'x'\n    else:\n        continue\n    lev = len(bline) - len(h) + 1\n\n    tline = ' %s%s|%s' %(mark, '. '*(lev-1), head)\n    tlines_add(tline)\n    bnodes_add(i+1)\n    levels_add(lev)\nreturn (tlines, bnodes, levels)", "path": "autoload\\voom\\voom_mode_taskpaper.py", "repo_name": "vim-scripts/VOoM", "stars": 99, "license": "None", "language": "python", "size": 985}
{"docstring": "\"\"\"Return (tree_head, bodyLines).\ntree_head is new headline string in Tree buffer (text after |).\nbodyLines is list of lines to insert in Body buffer.\n\"\"\"\n", "func_signal": "def hook_newHeadline(VO, level, blnum, tlnum):\n", "code": "tree_head = 'NewHeadline'\nif level >= MAX:\n    C = CHAR\nelse:\n    C = CHAR * (MAX - level + 1)\nbodyLines = ['=%s %s =%s' %(C, tree_head, C), '']\nreturn (tree_head, bodyLines)", "path": "autoload\\voom\\voom_mode_dokuwiki.py", "repo_name": "vim-scripts/VOoM", "stars": 99, "license": "None", "language": "python", "size": 985}
{"docstring": "\"\"\"Return (tree_head, bodyLines).\ntree_head is new headline string in Tree buffer (text after |).\nbodyLines is list of lines to insert in Body buffer.\n\"\"\"\n", "func_signal": "def hook_newHeadline(VO, level, blnum, tlnum):\n", "code": "tree_head = 'NewHeadline'\nbodyLines = ['%s- %s' %('\\t'*(level-1), tree_head),]\nreturn (tree_head, bodyLines)", "path": "autoload\\voom\\voom_mode_taskpaper.py", "repo_name": "vim-scripts/VOoM", "stars": 99, "license": "None", "language": "python", "size": 985}
{"docstring": "\"\"\"Return (tlines, bnodes, levels) for Body lines blines.\nblines is either Vim buffer object (Body) or list of buffer lines.\n\"\"\"\n", "func_signal": "def hook_makeOutline(VO, blines):\n", "code": "Z = len(blines)\ntlines, bnodes, levels = [], [], []\ntlines_add, bnodes_add, levels_add = tlines.append, bnodes.append, levels.append\nfor i in xrange(Z):\n    bline = blines[i]\n    if not ('</h' in bline or '</H' in bline):\n        continue\n    m = headline_search(bline)\n    if not m:\n        continue\n    lev = int(m.group(1))\n    head = m.group(2)\n    # delete all html tags\n    head = html_tag_sub('',head)\n    tline = '  %s|%s' %('. '*(lev-1), head.strip())\n    tlines_add(tline)\n    bnodes_add(i+1)\n    levels_add(lev)\nreturn (tlines, bnodes, levels)", "path": "autoload\\voom\\voom_mode_html.py", "repo_name": "vim-scripts/VOoM", "stars": 99, "license": "None", "language": "python", "size": 985}
{"docstring": "\"\"\"Increase of decrease level number of Body headline by levDelta.\"\"\"\n", "func_signal": "def hook_changeLevBodyHead(VO, h, levDelta):\n", "code": "if levDelta==0: return h\nm = headline_match(h)\nlevel = len(m.group(1))\nreturn '%s%s' %('*'*(level+levDelta), h[m.end(1):])", "path": "autoload\\voom\\voom_mode_viki.py", "repo_name": "vim-scripts/VOoM", "stars": 99, "license": "None", "language": "python", "size": 985}
{"docstring": "# this is instead of hook_changeLevBodyHead()\n\n# Based on Markdown mode function.\n# Inserts blank separator lines if missing.\n\n#print oop, levDelta, blnum1, tlnum1, blnum2, tlnum2, tlnumCut, blnumCut\n", "func_signal": "def hook_doBodyAfterOop(VO, oop, levDelta, blnum1, tlnum1, blnum2, tlnum2, blnumCut, tlnumCut):\n", "code": "Body = VO.Body\nZ = len(Body)\nbnodes, levels = VO.bnodes, VO.levels\nENC = VO.enc\n\n# blnum1 blnum2 is first and last lnums of Body region pasted, inserted\n# during up/down, or promoted/demoted.\nif blnum1:\n    assert blnum1 == bnodes[tlnum1-1]\n    if tlnum2 < len(bnodes):\n        assert blnum2 == bnodes[tlnum2]-1\n    else:\n        assert blnum2 == Z\n\n# blnumCut is Body lnum after which a region was removed during 'cut',\n# 'up', 'down'. Need this to check if there is blank line between nodes\n# used to be separated by the cut/moved region.\nif blnumCut:\n    if tlnumCut < len(bnodes):\n        assert blnumCut == bnodes[tlnumCut]-1\n    else:\n        assert blnumCut == Z\n\n# Total number of added lines minus number of deleted lines.\nb_delta = 0\n\n### After 'cut' or 'up': insert blank line if there is none\n# between the nodes used to be separated by the cut/moved region.\nif DO_BLANKS and (oop=='cut' or oop=='up') and (0 < blnumCut < Z) and Body[blnumCut-1].strip():\n    Body[blnumCut:blnumCut] = ['']\n    update_bnodes(VO, tlnumCut+1 ,1)\n    b_delta+=1\n\nif oop=='cut':\n    return\n\n### Make sure there is blank line after the last node in the region:\n# insert blank line after blnum2 if blnum2 is not blank, that is insert\n# blank line before bnode at tlnum2+1.\nif DO_BLANKS and blnum2 < Z and Body[blnum2-1].strip():\n    Body[blnum2:blnum2] = ['']\n    update_bnodes(VO, tlnum2+1 ,1)\n    b_delta+=1\n\n### Change levels and/or formats of headlines in the affected region.\n# Always do this after Paste, even if level is unchanged -- format can\n# be different when pasting from other outlines.\n# Examine each headline, from bottom to top, and change level and/or format.\n# To change from 1-style to 2-style:\n#   strip ='s, strip whitespace;\n#   insert underline.\n# To change from 2-style to 1-style:\n#   delete underline;\n#   insert ='s.\n# Update bnodes after inserting or deleting a line.\n#\n# NOTE: bnode can be [[AAA]] or [AAA] line, we check for that and adjust it\n# to point to the headline text line\n#\n#   1-style          2-style\n#\n#            L0            L0             Body[bln-2]\n#   == head  L1      head  L1   <--bnode  Body[bln-1] (not always the actual bnode)\n#            L2      ----  L2             Body[bln]\n#            L3            L3             Body[bln+1]\n\nif levDelta or oop=='paste':\n    for i in xrange(tlnum2, tlnum1-1, -1):\n        # required level (VO.levels has been updated)\n        lev = levels[i-1]\n        # current level from which to change to lev\n        lev_ = lev - levDelta\n\n        # Body headline (bnode) and the next line\n        bln = bnodes[i-1]\n        L1 = Body[bln-1].rstrip()\n        # bnode can point to the tompost [AAA] or [[AAA]] line\n        # increment bln until the actual headline (title line) is found\n        while L1.startswith('[') and L1.endswith(']'):\n            bln += 1\n            L1 = Body[bln-1].rstrip()\n        # the underline line\n        if bln < len(Body):\n            L2 = Body[bln].rstrip()\n        else:\n            L2 = ''\n\n        # get current headline format\n        hasOne, hasOneClose = False, VO.useOneClose\n        theHead = L1\n        if L1.startswith('='):\n            m = HEAD_MATCH(L1)\n            if m:\n                hasOne = True\n                # headline without ='s but with whitespace around it preserved\n                theHead = m.group(2)\n                theclose = m.group(3)\n                if theclose:\n                    hasOneClose = True\n                    theHead += theclose.rstrip('=')\n                else:\n                    hasOneClose = False\n\n        # get desired headline format\n        if oop=='paste':\n            if lev > 5:\n                useOne = True\n            else:\n                useOne = VO.useOne\n            useOneClose = VO.useOneClose\n        elif lev < 6 and lev_ < 6:\n            useOne = hasOne\n            useOneClose = hasOneClose\n        elif lev > 5 and lev_ > 5:\n            useOne = True\n            useOneClose = hasOneClose\n        elif lev < 6 and lev_ > 5:\n            useOne = VO.useOne\n            useOneClose = VO.useOneClose\n        elif lev > 5 and lev_ < 6:\n            useOne = True\n            useOneClose = hasOneClose\n        else:\n            assert False\n        #print useOne, hasOne, ';', useOneClose, hasOneClose\n\n        ### change headline level and/or format\n        # 2-style unchanged, only adjust level of underline\n        if not useOne and not hasOne:\n            if not levDelta: continue\n            Body[bln] = LEVELS_ADS[lev]*len(L2)\n        # 1-style unchanged, adjust level of ='s and add/remove closing ='s\n        elif useOne and hasOne:\n            # no format change, there are closing ='s\n            if useOneClose and hasOneClose:\n                if not levDelta: continue\n                Body[bln-1] = '%s%s%s' %('='*lev, theHead, '='*lev)\n            # no format change, there are no closing ='s\n            elif not useOneClose and not hasOneClose:\n                if not levDelta: continue\n                Body[bln-1] = '%s%s' %('='*lev, theHead)\n            # add closing ='s\n            elif useOneClose and not hasOneClose:\n                Body[bln-1] = '%s%s %s' %('='*lev, theHead.rstrip(), '='*lev)\n            # remove closing ='s\n            elif not useOneClose and hasOneClose:\n                Body[bln-1] = '%s%s' %('='*lev, theHead.rstrip())\n        # insert underline, remove ='s\n        elif not useOne and hasOne:\n            L1 = theHead.strip()\n            Body[bln-1] = L1\n            # insert underline\n            Body[bln:bln] = [LEVELS_ADS[lev]*len(L1.decode(ENC,'replace'))]\n            update_bnodes(VO, i+1, 1)\n            b_delta+=1\n        # remove underline, insert ='s\n        elif useOne and not hasOne:\n            if useOneClose:\n                Body[bln-1] = '%s %s %s' %('='*lev, theHead.strip(), '='*lev)\n            else:\n                Body[bln-1] = '%s %s' %('='*lev, theHead.strip())\n            # delete underline\n            Body[bln:bln+1] = []\n            update_bnodes(VO, i+1, -1)\n            b_delta-=1\n\n### Make sure first headline is preceded by a blank line.\nblnum1 = bnodes[tlnum1-1]\nif DO_BLANKS and blnum1 > 1 and Body[blnum1-2].strip():\n    Body[blnum1-1:blnum1-1] = ['']\n    update_bnodes(VO, tlnum1 ,1)\n    b_delta+=1\n\n### After 'down' : insert blank line if there is none\n# between the nodes used to be separated by the moved region.\nif DO_BLANKS and oop=='down' and (0 < blnumCut < Z) and Body[blnumCut-1].strip():\n    Body[blnumCut:blnumCut] = ['']\n    update_bnodes(VO, tlnumCut+1 ,1)\n    b_delta+=1\n\nassert len(Body) == Z + b_delta", "path": "autoload\\voom\\voom_mode_asciidoc.py", "repo_name": "vim-scripts/VOoM", "stars": 99, "license": "None", "language": "python", "size": 985}
{"docstring": "\"\"\"Return (tree_head, bodyLines).\ntree_head is new headline string in Tree buffer (text after |).\nbodyLines is list of lines to insert in Body buffer.\n\"\"\"\n", "func_signal": "def hook_newHeadline(VO, level, blnum, tlnum):\n", "code": "tree_head = 'NewHeadline'\nbodyLines = ['%s %s' %('*'*level, tree_head), '']\nreturn (tree_head, bodyLines)", "path": "autoload\\voom\\voom_mode_viki.py", "repo_name": "vim-scripts/VOoM", "stars": 99, "license": "None", "language": "python", "size": 985}
{"docstring": "\"\"\"Update VO.bnodes by adding/substracting delta to each bnode\nstarting with bnode at tlnum and to the end.\n\"\"\"\n", "func_signal": "def update_bnodes(VO, tlnum, delta):\n", "code": "bnodes = VO.bnodes\nfor i in xrange(tlnum, len(bnodes)+1):\n    bnodes[i-1] += delta", "path": "autoload\\voom\\voom_mode_asciidoc.py", "repo_name": "vim-scripts/VOoM", "stars": 99, "license": "None", "language": "python", "size": 985}
{"docstring": "# this is instead of hook_changeLevBodyHead()\n#print oop, levDelta, blnum1, tlnum1, blnum2, tlnum2, tlnumCut, blnumCut\n", "func_signal": "def hook_doBodyAfterOop(VO, oop, levDelta, blnum1, tlnum1, blnum2, tlnum2, blnumCut, tlnumCut):\n", "code": "Body = VO.Body\nZ = len(Body)\n\nind = get_body_indent(VO.body)\n# levDelta is wrong when pasting because hook_makeOutline() looks at relative indent\n# determine level of pasted region from indent of its first line\nif oop=='paste':\n    bline1 = Body[blnum1-1]\n    lev = (len(bline1) - len(bline1.lstrip())) / len(ind) + 1\n    levDelta = VO.levels[tlnum1-1] - lev\n\nif not levDelta: return\n\nindent = abs(levDelta) * ind\n#--- copied from voom_mode_thevimoutliner.py -----------------------------\nif blnum1:\n    assert blnum1 == VO.bnodes[tlnum1-1]\n    if tlnum2 < len(VO.bnodes):\n        assert blnum2 == VO.bnodes[tlnum2]-1\n    else:\n        assert blnum2 == Z\n\n# dedent (if possible) or indent every non-blank line in Body region blnum1,blnum2\nblines = []\nfor i in xrange(blnum1-1,blnum2):\n    line = Body[i]\n    if not line.strip():\n        blines.append(line)\n        continue\n    if levDelta > 0:\n        line = '%s%s' %(indent,line)\n    elif levDelta < 0 and line.startswith(indent):\n        line = line[len(indent):]\n    blines.append(line)\n\n# replace Body region\nBody[blnum1-1:blnum2] = blines\nassert len(Body)==Z", "path": "autoload\\voom\\voom_mode_python.py", "repo_name": "vim-scripts/VOoM", "stars": 99, "license": "None", "language": "python", "size": 985}
{"docstring": "\"\"\"Return (tlines, bnodes, levels) for Body lines blines.\nblines is either Vim buffer object (Body) or list of buffer lines.\n\"\"\"\n", "func_signal": "def hook_makeOutline(VO, blines):\n", "code": "Z = len(blines)\ntlines, bnodes, levels = [], [], []\ntlines_add, bnodes_add, levels_add = tlines.append, bnodes.append, levels.append\n\n#ignore_lnums, func_lnums = get_lnums_from_tokenize(blines)\ntry:\n    ignore_lnums, func_lnums = get_lnums_from_tokenize(blines)\nexcept (IndentationError, tokenize.TokenError):\n    vim.command(\"call voom#ErrorMsg('VOoM: EXCEPTION WHILE PARSING PYTHON OUTLINE')\")\n    # DO NOT print to sys.stderr -- triggers Vim error when default stderr (no PyLog)\n    #traceback.print_exc()  --this goes to sys.stderr\n    #print traceback.format_exc() --ok but no highlighting\n    lines = traceback.format_exc().replace(\"'\",\"''\").split('\\n')\n    for ln in lines:\n        vim.command(\"call voom#ErrorMsg('%s')\" %ln)\n    return (['= |!!!ERROR: OUTLINE IS INVALID'], [1], [1])\n\nisHead = False # True if current line is a headline\nindents = [0,] # indents of previous levels\nfuncLevels = [] # levels of previous def or class\nindentError = '' # inconsistent indent\nisDecor = 0 # keeps track of decorators, set to lnum of the first decorator\nX = ' ' # char in Tree's column 2 (marks)\nfor i in xrange(Z):\n    bnode = i + 1\n    if bnode in ignore_lnums: continue\n    bline = blines[i]\n    bline_s = bline.strip()\n    if not bline_s: continue\n    if bline_s.startswith('#'):\n        # ignore comment lines consisting only of #, -, =, spaces, tabs (separators, pretty headers)\n        if not bline_s.lstrip('# \\t-='): continue\n        isComment = True\n    else:\n        isComment = False\n    bline_ls = bline.lstrip()\n\n    # compute indent and level\n    indent = len(bline) - len(bline_ls)\n    if indent > indents[-1]:\n        indents.append(indent)\n    elif indent < indents[-1]:\n        while indents and (indents[-1] > indent):\n            indents.pop()\n        if indents[-1]==indent:\n            indentError = ''\n        else:\n            indentError = '!!! '\n    lev = len(indents)\n\n    # First line after the end of a class or def block.\n    if funcLevels and lev <= funcLevels[-1]:\n        isHead = True\n        while funcLevels and funcLevels[-1] >= lev:\n            funcLevels.pop()\n    # First line of a class or def block.\n    if bnode in func_lnums:\n        isHead = True\n        if isDecor:\n            bnode = isDecor\n            isDecor = 0\n            X = 'd'\n        if not funcLevels or (lev > funcLevels[-1]):\n            funcLevels.append(lev)\n    # Line after a decorator. Not a def or class.\n    elif isDecor:\n        # ingore valid lines between the first decorator and function/class\n        if bline_s.startswith('@') or isComment or not bline_s:\n            isHead = False\n            continue\n        # Invalid line after a decorator (should be syntax error): anything\n        # other than another decorator, comment, blank line, def/class.\n        # If it looks like a headline, let it be a headline.\n        else:\n            isDecor = 0\n    # Decorator line (the first one if a group of several).\n    elif bline_s.startswith('@'):\n        isDecor = bnode\n        isHead = False\n        continue\n    # Special comment line (unconditional headline). Not a separator or pretty header line.\n    elif isComment:\n        if bline_s.startswith('###') or bline_s.startswith('#--') or bline_s.startswith('#=='):\n            isHead = True\n\n    if isHead:\n        ##########################################\n        # Take care of pretty headers like this. #\n        ##########################################\n        if isComment:\n            # add preceding lines to the current node if they consist only of #, =, -, whitespace\n            while bnode > 1:\n                bline_p = blines[bnode-2].lstrip()\n                if not bline_p.startswith('#') or bline_p.lstrip('# \\t-='):\n                    break\n                else:\n                    bnode -= 1\n        # the end\n        isHead = False\n        tline = ' %s%s|%s%s' %(X, '. '*(lev-1), indentError, bline_s)\n        X = ' '\n        tlines_add(tline)\n        bnodes_add(bnode)\n        levels_add(lev)\n\nreturn (tlines, bnodes, levels)", "path": "autoload\\voom\\voom_mode_python.py", "repo_name": "vim-scripts/VOoM", "stars": 99, "license": "None", "language": "python", "size": 985}
{"docstring": "\"\"\"Return (tree_head, bodyLines).\ntree_head is new headline string in Tree buffer (text after |).\nbodyLines is list of lines to insert in Body buffer.\n\"\"\"\n", "func_signal": "def hook_newHeadline(VO, level, blnum, tlnum):\n", "code": "tree_head = 'NewHeadline'\nbodyLines = ['<h%s>%s</h%s>' %(level, tree_head, level), '']\nreturn (tree_head, bodyLines)", "path": "autoload\\voom\\voom_mode_html.py", "repo_name": "vim-scripts/VOoM", "stars": 99, "license": "None", "language": "python", "size": 985}
{"docstring": "# this is instead of hook_changeLevBodyHead()\n", "func_signal": "def hook_doBodyAfterOop(VO, oop, levDelta, blnum1, tlnum1, blnum2, tlnum2, blnumCut, tlnumCut):\n", "code": "if not levDelta: return\n\nindent = abs(levDelta) * '\\t'\n\nBody = VO.Body\nZ = len(Body)\n\n# ---- identical to voom_mode_python.py code ----------------------------\nif blnum1:\n    assert blnum1 == VO.bnodes[tlnum1-1]\n    if tlnum2 < len(VO.bnodes):\n        assert blnum2 == VO.bnodes[tlnum2]-1\n    else:\n        assert blnum2 == Z\n\n# dedent (if possible) or indent every non-blank line in Body region blnum1,blnum2\nblines = []\nfor i in xrange(blnum1-1,blnum2):\n    line = Body[i]\n    if not line.strip():\n        blines.append(line)\n        continue\n    if levDelta > 0:\n        line = '%s%s' %(indent,line)\n    elif levDelta < 0 and line.startswith(indent):\n        line = line[len(indent):]\n    blines.append(line)\n\n# replace Body region\nBody[blnum1-1:blnum2] = blines\nassert len(Body)==Z", "path": "autoload\\voom\\voom_mode_taskpaper.py", "repo_name": "vim-scripts/VOoM", "stars": 99, "license": "None", "language": "python", "size": 985}
{"docstring": "# this is instead of hook_changeLevBodyHead()\n#print oop, levDelta, blnum1, tlnum1, blnum2, tlnum2, tlnumCut, blnumCut\n", "func_signal": "def hook_doBodyAfterOop(VO, oop, levDelta, blnum1, tlnum1, blnum2, tlnum2, blnumCut, tlnumCut):\n", "code": "Body = VO.Body\nZ = len(Body)\nbnodes, levels = VO.bnodes, VO.levels\n\n# blnum1 blnum2 is first and last lnums of Body region pasted, inserted\n# during up/down, or promoted/demoted.\nif blnum1:\n    assert blnum1 == bnodes[tlnum1-1]\n    if tlnum2 < len(bnodes):\n        assert blnum2 == bnodes[tlnum2]-1\n    else:\n        assert blnum2 == Z\n\n# blnumCut is Body lnum after which a region was removed during 'cut'\nif blnumCut:\n    if tlnumCut < len(bnodes):\n        assert blnumCut == bnodes[tlnumCut]-1\n    else:\n        assert blnumCut == Z\n\n### Change levels and/or sections of headlines in the affected region.\nif not levDelta:\n    return\n\n# Examine each headline in the affected region from top to bottom.\n# Change levels.\n# Correct levels that exceed the MAX: set them to MAX.\ninvalid_levs = [] # tree lnums of nodes with level > MAX\nfor i in xrange(tlnum1, tlnum2+1):\n    # required level based on new VO.levels, can be disallowed\n    lev_ = levels[i-1]\n    # Body line\n    bln = bnodes[i-1]\n    L = Body[bln-1] # original Body headline line\n\n    if lev_ <= MAX:\n        n = MAX - lev_ + 1\n    # MAX level exceeded\n    else:\n        n = 1\n        invalid_levs.append(i)\n        levels[i-1] = MAX # correct VO.levels\n        # don't change Body line if level is already at MAX\n        if lev_ - levDelta == MAX:\n            continue\n    m = headline_match(L)\n    # set Body line\n    # don't bother changing closing CHARs if there are too many of them\n    if len(m.group(4)) <= MAX+1:\n        Body[bln-1] = '%s=%s%s=%s' %(m.group(1), CHAR * n, m.group(3), CHAR * n)\n    else:\n        Body[bln-1] = '%s=%s%s' %(m.group(1), CHAR * n, L[m.end(2):])\n\n### --- the end ---\nif invalid_levs:\n    vim.command(\"call voom#ErrorMsg('VOoM (dokuwiki): Disallowed levels have been corrected after ''%s''')\" %oop)\n    invalid_levs = ', '.join(['%s' %i for i in invalid_levs])\n    vim.command(\"call voom#ErrorMsg('     level set to maximum (%s) for nodes: %s')\" %(MAX, invalid_levs))", "path": "autoload\\voom\\voom_mode_dokuwiki.py", "repo_name": "vim-scripts/VOoM", "stars": 99, "license": "None", "language": "python", "size": 985}
{"docstring": "\"\"\"Increase of decrease level number of Body headline by levDelta.\"\"\"\n", "func_signal": "def hook_changeLevBodyHead(VO, h, levDelta):\n", "code": "if levDelta==0: return h\nm = headline_search(h)\nlevel = int(m.group(1))\nlev = level+levDelta\nreturn '%s%s%s%s%s' %(h[:m.start(1)], lev, h[m.end(1):m.start(3)], lev, h[m.end(3):])", "path": "autoload\\voom\\voom_mode_html.py", "repo_name": "vim-scripts/VOoM", "stars": 99, "license": "None", "language": "python", "size": 985}
{"docstring": "\"\"\"Return (tree_head, bodyLines).\ntree_head is new headline string in Tree buffer (text after |).\nbodyLines is list of lines to insert in Body buffer.\n\"\"\"\n", "func_signal": "def hook_newHeadline(VO, level, blnum, tlnum):\n", "code": "tree_head = '### NewHeadline'\nindent = get_body_indent(VO.body)\nbody_head = '%s%s' %(indent*(level-1), tree_head)\nreturn (tree_head, [body_head])", "path": "autoload\\voom\\voom_mode_python.py", "repo_name": "vim-scripts/VOoM", "stars": 99, "license": "None", "language": "python", "size": 985}
{"docstring": "\"\"\"Return dicts. Keys are Body lnums.\nThe main purpose is to get list of lnums to ignore: multi-line strings and\nexpressions.\n\"\"\"\n# lnums to ignore: multi-line strings and expressions other than the first line\n", "func_signal": "def get_lnums_from_tokenize(blines):\n", "code": "ignore_lnums = {}\n# lnums of 'class' and 'def' tokens\nfunc_lnums = {}\n\ninName = False\n\nfor tok in tokenize.generate_tokens(BLines(blines).readline):\n    toktype, toktext, (srow, scol), (erow, ecol), line = tok\n    #print token.tok_name[toktype], tok\n    if toktype == NAME:\n        if not inName:\n            inName = True\n            srow_name = srow\n        if toktext in ('def','class'):\n            func_lnums[srow] = toktext\n    elif toktype == NEWLINE and inName:\n        inName = False\n        if srow_name != erow:\n            for i in xrange(srow_name+1, erow+1):\n                ignore_lnums[i] = 0\n    elif toktype == STRING:\n        if srow != erow:\n            for i in xrange(srow+1, erow+1):\n                ignore_lnums[i] = 0\n\nreturn (ignore_lnums, func_lnums)", "path": "autoload\\voom\\voom_mode_python.py", "repo_name": "vim-scripts/VOoM", "stars": 99, "license": "None", "language": "python", "size": 985}
{"docstring": "\"\"\"Return (tlines, bnodes, levels) for Body lines blines.\nblines is either Vim buffer object (Body) or list of buffer lines.\n\"\"\"\n", "func_signal": "def hook_makeOutline(VO, blines):\n", "code": "Z = len(blines)\ntlines, bnodes, levels = [], [], []\ntlines_add, bnodes_add, levels_add = tlines.append, bnodes.append, levels.append\nisFenced = False # EndOfRegion match object when inside a region\nfor i in xrange(Z):\n    bline = blines[i]\n\n    if isFenced:\n        if re.match(isFenced, bline):\n            isFenced = False\n        continue\n\n    if bline.lstrip().startswith('#') and '<<' in bline:\n        r_m = region_match(bline)\n        if r_m and r_m.group(1) != 'Region':\n            isFenced = '^\\s*%s\\s*$' %re.escape(r_m.group(3) or '')\n            continue\n    elif not bline.startswith('*'):\n        continue\n\n    m = headline_match(bline)\n    if not m:\n        continue\n    lev = len(m.group(1))\n    head = bline[lev:].strip()\n    tline = '  %s|%s' %('. '*(lev-1), head)\n    tlines_add(tline)\n    bnodes_add(i+1)\n    levels_add(lev)\nreturn (tlines, bnodes, levels)", "path": "autoload\\voom\\voom_mode_viki.py", "repo_name": "vim-scripts/VOoM", "stars": 99, "license": "None", "language": "python", "size": 985}
{"docstring": "\"\"\"Return (tlines, bnodes, levels) for Body lines blines.\nblines is either Vim buffer object (Body) or list of buffer lines.\n\"\"\"\n", "func_signal": "def hook_makeOutline(VO, blines):\n", "code": "ENC = VO.enc\nZ = len(blines)\ntlines, bnodes, levels = [], [], []\ntlines_add, bnodes_add, levels_add = tlines.append, bnodes.append, levels.append\n\n# trailing whitespace is always removed with rstrip()\n# if headline is precedeed by [AAA] and/or [[AAA]], bnode is set to their lnum\n#\n# 1-style, overrides 2-style\n#  [[AAA]]       L3, blines[i-2]\n#  [yyy]         L2, blines[i-1]\n#  == head ==    L1, blines[i]   -- current line, closing = are optional\n#\n# 2-style (underline)\n#  [[AAA]]       L4, blines[i-3]\n#  [yyy]         L3, blines[i-2]\n#  head          L2, blines[i-1] -- title line, many restrictions on the format\n#  ----          L1, blines[i]   -- current line\n\n\n# Set this the first time a headline with level 1-5 is encountered.\n# 0 or 1 -- False, use 2-style (default); 2 -- True, use 1-style\nuseOne = 0\n# Set this the first time headline in 1-style is encountered.\n# 0 or 1 -- True, use closing ='s (default); 2 -- False, do not use closing ='s\nuseOneClose = 0\n\nisHead = False\nisFenced = False # True if inside DelimitedBlock, the value is the char\nheadI = -2 # idx of the last line that is part of a headline\nblockI = -2 # idx of the last line where a DelimitedBlock ended\nm = None # match object for 1-style regex\n\nfor i in xrange(Z):\n    L1 = blines[i].rstrip()\n    if not L1 or not L1[0] in CHARS:\n        continue\n    ch = L1[0]\n\n    if isFenced:\n        if isFenced==ch and len(L1)>3 and L1.lstrip(ch)=='':\n            isFenced = False\n            blockI = i\n        continue\n\n    # 1-style headline\n    if ch == '=' and L1.strip('='):\n        m = HEAD_MATCH(L1)\n        if m:\n            isHead = True\n            headI_ = headI\n            headI = i\n            lev = len(m.group(1))\n            head = m.group(2).strip()\n            bnode = i+1\n\n    # current line is an underline\n    # the previous, underlined line (L2) is not a headline if it:\n    #   is not exactly the length of underline +/- 2\n    #   is already part of in the previous headline\n    #   looks like an underline or a delimited block line\n    #   is [[AAA]] or [AAA] (BlockID or Attribute List)\n    #   starts with . (Block Title, they have no level)\n    #   starts with // (comment line)\n    #   starts with tab (don't know why, spaces are ok)\n    #   is only 1 chars (avoids confusion with --, as in Vim syntax, not as in AsciiDoc)\n    if not isHead and ch in ADS_LEVELS and L1.lstrip(ch)=='' and i > 0:\n        L2 = blines[i-1].rstrip()\n        z2 = len(L2.decode(ENC,'replace'))\n        z1 = len(L1)\n        if (L2 and\n              (-3 < z2 - z1 < 3) and z1 > 1 and z2 > 1 and\n              headI != i-1 and\n              not ((L2[0] in CHARS) and L2.lstrip(L2[0])=='') and\n              not (L2.startswith('[') and L2.endswith(']')) and\n              not L2.startswith('.') and\n              not L2.startswith('\\t') and\n              not (L2.startswith('//') and not L2.startswith('///'))\n              ):\n            isHead = True\n            headI_ = headI\n            headI = i\n            lev = ADS_LEVELS[ch]\n            head = L2.strip()\n            bnode = i # lnum of previous line (L2)\n\n    if isHead and bnode > 1:\n        # decrement bnode if preceding lines are [[AAA]] or [AAA] lines\n        # that is set bnode to the topmost [[AAA]] or [AAA] line number\n        j_ = bnode-2 # idx of line before the title line\n        L3 = blines[bnode-2].rstrip()\n        while L3.startswith('[') and L3.endswith(']'):\n            bnode -= 1\n            if bnode > 1:\n                L3 = blines[bnode-2].rstrip()\n            else:\n                break\n\n        # headline must be preceded by a blank line unless:\n        #   it's line 1 (j == -1)\n        #   headline is preceded by [AAA] or [[AAA]] lines (j != j_)\n        #   previous line is a headline (headI_ == j)\n        #   previous line is the end of a DelimitedBlock (blockI == j)\n        j = bnode-2\n        if DO_BLANKS and j==j_ and j > -1:\n            L3 = blines[j].rstrip()\n            if L3 and headI_ != j and blockI != j:\n                # skip over any adjacent comment lines\n                while L3.startswith('//') and not L3.startswith('///'):\n                    j -= 1\n                    if j > -1:\n                        L3 = blines[j].rstrip()\n                    else:\n                        L3 = ''\n                if L3 and headI_ != j and blockI != j:\n                    isHead = False\n                    headI = headI_\n\n    # start of DelimitedBlock\n    if not isHead and ch in BLOCK_CHARS and len(L1)>3 and L1.lstrip(ch)=='':\n        isFenced = ch\n        continue\n\n    if isHead:\n        isHead = False\n        # save style info for first headline and first 1-style headline\n        if not useOne and lev < 6:\n            if m:\n                useOne = 2\n            else:\n                useOne = 1\n        if not useOneClose and m:\n            if m.group(3):\n                useOneClose = 1\n            else:\n                useOneClose = 2\n        # make outline\n        tline = '  %s|%s' %('. '*(lev-1), head)\n        tlines_add(tline)\n        bnodes_add(bnode)\n        levels_add(lev)\n\n# don't clobber these when parsing clipboard during Paste\n# which is the only time blines is not Body\nif blines is VO.Body:\n    VO.useOne = useOne == 2\n    VO.useOneClose = useOneClose < 2\n\nreturn (tlines, bnodes, levels)", "path": "autoload\\voom\\voom_mode_asciidoc.py", "repo_name": "vim-scripts/VOoM", "stars": 99, "license": "None", "language": "python", "size": 985}
{"docstring": "\"\"\"Return (tlines, bnodes, levels) for Body lines blines.\nblines is either Vim buffer object (Body) or list of buffer lines.\n\"\"\"\n", "func_signal": "def hook_makeOutline(VO, blines):\n", "code": "Z = len(blines)\ntlines, bnodes, levels = [], [], []\ntlines_add, bnodes_add, levels_add = tlines.append, bnodes.append, levels.append\nfor i in xrange(Z):\n    if not blines[i].lstrip().startswith(CHAR):\n        continue\n    bline = blines[i]\n    m = headline_match(bline)\n    if not m:\n        continue\n    n = len(m.group(2))\n    if n > MAX:\n        lev = 1\n    else:\n        lev = MAX - n + 2\n    head = m.group(3).strip()\n    tline = '  %s|%s' %('. '*(lev-1), head)\n    tlines_add(tline)\n    bnodes_add(i+1)\n    levels_add(lev)\nreturn (tlines, bnodes, levels)", "path": "autoload\\voom\\voom_mode_dokuwiki.py", "repo_name": "vim-scripts/VOoM", "stars": 99, "license": "None", "language": "python", "size": 985}
{"docstring": "\"\"\"\nForce Clamd to reload signature database\n\nreturn: (string) \"RELOADING\"\n\nMay raise:\n  - ConnectionError: in case of communication problem\n\"\"\"\n\n\n", "func_signal": "def reload(self):\n", "code": "try:\n    self._init_socket()\n    self._send_command('RELOAD')\n    result = self._recv_response()\n    self._close_socket()\n    \nexcept socket.error:\n    raise ConnectionError('Could probably not reload signature database')\n\nreturn result", "path": "pyclamd.py", "repo_name": "kholia/checksec", "stars": 66, "license": "None", "language": "python", "size": 1986}
{"docstring": "\"\"\"Import module, returning the module after the last dot.\"\"\"\n", "func_signal": "def _import_module(name):\n", "code": "__import__(name)\nreturn sys.modules[name]", "path": "six.py", "repo_name": "kholia/checksec", "stars": 66, "license": "None", "language": "python", "size": 1986}
{"docstring": "\"\"\"\nUnix Socket Class initialisation\n\nfilename (string) : unix socket filename or None to get the socket from /etc/clamav/clamd.conf\ntimeout (float or None) : socket timeout\n\"\"\"\n\n# try to get unix socket from clamd.conf\n", "func_signal": "def __init__(self, filename=None, timeout=None):\n", "code": "if filename is None:\n    with open('/etc/clamav/clamd.conf', 'r') as conffile:\n        for line in conffile.readlines():\n            try:\n                if line.strip().split()[0] == 'LocalSocket':\n                    filename = line.strip().split()[1]\n                    break\n            except IndexError:\n                pass\n                    \n        else:\n            raise ConnectionError('Could not find clamd unix socket from /etc/clamav/clamd.conf')\n\nassert isinstance(filename, str), 'Wrong type for [file], should be a string [was {0}]'.format(type(file))\nassert isinstance(timeout, (float, int)) or timeout is None, 'Wrong type for [timeout], should be either None or a float [was {0}]'.format(type(timeout))\n\n_ClamdGeneric.__init__(self)\n\nself.unix_socket = filename\nself.timeout = timeout\n\n# tests the socket\nself._init_socket()\nself._close_socket()\n\nreturn", "path": "pyclamd.py", "repo_name": "kholia/checksec", "stars": 66, "license": "None", "language": "python", "size": 1986}
{"docstring": "\"\"\"\nNetwork Class initialisation\nhost (string) : hostname or ip address\nport (int) : TCP port\ntimeout (float or None) : socket timeout\n\"\"\"\n    \n", "func_signal": "def __init__(self, host='127.0.0.1', port=3310, timeout=None):\n", "code": "assert isinstance(host, str), 'Wrong type for [host], should be a string [was {0}]'.format(type(host))\nassert isinstance(port, int), 'Wrong type for [port], should be an int [was {0}]'.format(type(port))\nassert isinstance(timeout, (float, int)) or timeout is None, 'Wrong type for [timeout], should be either None or a float [was {0}]'.format(type(timeout))\n\n_ClamdGeneric.__init__(self)\n\nself.host = host\nself.port = port\nself.timeout = timeout\n\n# tests the socket\nself._init_socket()\nself._close_socket()\n\nreturn", "path": "pyclamd.py", "repo_name": "kholia/checksec", "stars": 66, "license": "None", "language": "python", "size": 1986}
{"docstring": "\"\"\"Deprecated API - use ClamdNetworkSocket instead.\"\"\"\n", "func_signal": "def init_network_socket(host='127.0.0.1', port=3310, timeout=None):\n", "code": "global socketinst\nsocketinst = ClamdNetworkSocket(host=host, port=port, timeout=timeout)", "path": "pyclamd.py", "repo_name": "kholia/checksec", "stars": 66, "license": "None", "language": "python", "size": 1986}
{"docstring": "\"\"\"\nreceive multiple line response from clamd and strip all whitespace characters\n\"\"\"\n", "func_signal": "def _recv_response_multiline(self):\n", "code": "response = ''\nc = '...'\nwhile c != '':\n    try:\n        data = self.clamd_socket.recv(4096)\n        try:\n            c = bytes.decode(data).strip()\n        except UnicodeDecodeError:\n            response = data.strip()\n    except socket.error:\n        break\n        \n    response += '{0}\\n'.format(c)\nreturn response", "path": "pyclamd.py", "repo_name": "kholia/checksec", "stars": 66, "license": "None", "language": "python", "size": 1986}
{"docstring": "\"\"\"\nForce Clamd to shutdown and exit\n\nreturn: nothing\n\nMay raise:\n  - ConnectionError: in case of communication problem\n\"\"\"\n", "func_signal": "def shutdown(self):\n", "code": "try:\n    self._init_socket()\n    self._send_command('SHUTDOWN')\n    self._recv_response()\n    self._close_socket()\nexcept socket.error:\n    raise ConnectionError('Could probably not shutdown clamd')", "path": "pyclamd.py", "repo_name": "kholia/checksec", "stars": 66, "license": "None", "language": "python", "size": 1986}
{"docstring": "\"\"\"\nScan a file or directory given by filename and stop on first virus or error found.\nScan with archive support enabled.\n\nfile (string) : filename or directory (MUST BE ABSOLUTE PATH !)\n\nreturn either :\n  - (dict): {filename1: \"virusname\"}\n  - None: if no virus found\n\nMay raise :\n  - ConnectionError: in case of communication problem\n  - socket.timeout: if timeout has expired\n\"\"\"\n\n", "func_signal": "def scan_file(self, file):\n", "code": "assert isinstance(file, str), 'Wrong type for [file], should be a string [was {0}]'.format(type(file))\n\ntry:\n    self._init_socket()\n    self._send_command('SCAN {0}'.format(file))\nexcept socket.error:\n    raise ConnectionError('Unable to scan {0}'.format(file))\n\nresult='...'\ndr={}\nwhile result:\n    try:\n        result = self._recv_response()\n    except socket.error:\n        raise ConnectionError('Unable to scan {0}'.format(file))\n\n    if len(result) > 0:\n        filename, reason, status = self._parse_response(result)\n\n        if status == 'ERROR':\n            dr[filename] = ('ERROR', '{0}'.format(reason))\n            return dr\n            \n        elif status == 'FOUND':\n            dr[filename] = ('FOUND', '{0}'.format(reason))\n\nself._close_socket()\nif not dr:\n    return None\nreturn dr", "path": "pyclamd.py", "repo_name": "kholia/checksec", "stars": 66, "license": "None", "language": "python", "size": 1986}
{"docstring": "\"\"\"Deprecated API - use ClamdUnixSocket instead.\"\"\"\n", "func_signal": "def init_unix_socket(filename=None):\n", "code": "global socketinst\nsocketinst = ClamdUnixSocket(filename=filename)", "path": "pyclamd.py", "repo_name": "kholia/checksec", "stars": 66, "license": "None", "language": "python", "size": 1986}
{"docstring": "\"\"\"\nScan a file or directory given by filename\nDo not stop on error or virus found.\nScan with archive support enabled.\n\nfile (string): filename or directory (MUST BE ABSOLUTE PATH !)\n\nreturn either :\n  - (dict): {filename1: ('FOUND', 'virusname'), filename2: ('ERROR', 'reason')}\n  - None: if no virus found\n\nMay raise:\n  - ConnectionError: in case of communication problem\n\"\"\"\n", "func_signal": "def contscan_file(self, file):\n", "code": "assert isinstance(file, str), 'Wrong type for [file], should be a string [was {0}]'.format(type(file))\n\ntry:\n    self._init_socket()\n    self._send_command('CONTSCAN {0}'.format(file))\nexcept socket.error:\n    raise ConnectionError('Unable to scan  {0}'.format(file))\n\nresult='...'\ndr={}\nwhile result:\n    try:\n        result = self._recv_response()\n    except socket.error:\n        raise ConnectionError('Unable to scan  {0}'.format(file))\n\n    if len(result) > 0:\n        for resline in result.splitlines():\n            filename, reason, status = self._parse_response(resline)\n            \n            if status == 'ERROR':\n                dr[filename] = ('ERROR', '{0}'.format(reason))\n            \n            elif status == 'FOUND':\n                dr[filename] = ('FOUND', '{0}'.format(reason))\n\nself._close_socket()\nif not dr:\n    return None\nreturn dr", "path": "pyclamd.py", "repo_name": "kholia/checksec", "stars": 66, "license": "None", "language": "python", "size": 1986}
{"docstring": "\"\"\"\n`man clamd` recommends to prefix commands with z, but we will use \\n\nterminated strings, as python<->clamd has some problems with \\0x00\n\"\"\"\n", "func_signal": "def _send_command(self, cmd):\n", "code": "try:\n    cmd = str.encode('n{0}\\n'.format(cmd))\nexcept UnicodeDecodeError:\n    cmd = 'n{0}\\n'.format(cmd)\nself.clamd_socket.send(cmd)\nreturn", "path": "pyclamd.py", "repo_name": "kholia/checksec", "stars": 66, "license": "None", "language": "python", "size": 1986}
{"docstring": "# print('Processing file:', filename)\n\n", "func_signal": "def process_file(filename):\n", "code": "debug_paths = []\nwith open(filename, 'rb') as f:\n    elffile = ELFFile(f)\n\n    if not elffile.has_dwarf_info():\n        for section in elffile.iter_sections():\n            name = bytes2str(section.name)\n            # print(name)\n\n            # first try to find \".note.gnu.build-id\" in ELF itself\n            # uint32 name_size; /* size of the name */\n            # uint32 hash_size; /* size of the hash */\n            # uint32 identifier; /* NT_GNU_BUILD_ID == 0x3 */\n            # char   name[name_size]; /* the name \"GNU\" */\n            # char   hash[hash_size]; /* the hash */\n            #\n            # objdump -s -j .note.gnu.build-id /usr/bin/openssl\n            if name == \".note.gnu.build-id\":\n                data = section.data()\n                hash = data[16:]\n                value = binascii.hexlify(hash).decode(\"ascii\")\n                # print(value)\n                # a value of \"0834ce567a2d57deed6706e28fa29225cf043e16\"\n                # implies that we will have a path which looks like,\n                # /usr/lib/debug/.build-id/08/34ce5...25cf043e16.debug\n                path = os.path.join(value[0:2], value[2:] + \".debug\")\n                # print(path)\n                debug_paths.append(path)\n            # A filename, with any leading directory components removed,\n            # followed by a zero byte, zero to three bytes of padding, as\n            # needed to reach the next four-byte boundary within the\n            # section, and a four-byte CRC checksum, stored in the same\n            # endianness used for the executable file itself.\n            #\n            # objdump -s -j .gnu_debuglink /usr/bin/openssl\n            if name == \".gnu_debuglink\":\n                data = section.data()\n                fdata = data[0:data.find(b\"\\x00\")]\n                debug_paths.append(fdata.decode(\"utf-8\"))\n    else:\n        # this file itself has the DWARF information, must be my lucky day!\n        get_producer(filename)\n\n    # get_dwarf_info returns a DWARFInfo context object, which is the\n    # starting point for all DWARF-based processing in pyelftools.\n    for path in debug_paths:\n        # So, for example, suppose you ask gdb to debug /usr/bin/ls, which\n        # has a debug link that specifies the file ls.debug, and a build ID\n        # whose value in hex is abcdef1234. If the list of the global debug\n        # directories includes /usr/lib/debug, then gdb will look for the\n        # following debug information files, in the indicated order:\n        #\n        # /usr/lib/debug/.build-id/ab/cdef1234.debug\n        # /usr/bin/ls.debug\n        # /usr/bin/.debug/ls.debug\n        # /usr/lib/debug/usr/bin/ls.debug.\n\n        rpath = os.path.join(\"/usr/lib/debug/.build-id\", path)\n        if os.path.isfile(rpath):\n            producer = get_producer(rpath)  # got producer, is one enough?\n            if producer:\n                print(producer)\n                continue\n\n        # `cwd` + \"/usr/lib/debug/.build-id\" is our hack ;)\n        debug_prefixes = [\"/usr/lib/debug/.build-id/\", \"/usr/bin/\",\n                          \"/usr/lib/debug/usr/bin/\"]\n\n        for prefix in debug_prefixes:\n            rpath = os.path.join(prefix, path)\n            if os.path.isfile(rpath):\n                get_producer(rpath)", "path": "producer.py", "repo_name": "kholia/checksec", "stars": 66, "license": "None", "language": "python", "size": 1986}
{"docstring": "\"\"\"\nclose clamd socket\n\"\"\"\n", "func_signal": "def _close_socket(self):\n", "code": "self.clamd_socket.close()\nreturn", "path": "pyclamd.py", "repo_name": "kholia/checksec", "stars": 66, "license": "None", "language": "python", "size": 1986}
{"docstring": "\"\"\"Remove item from six.moves.\"\"\"\n", "func_signal": "def remove_move(name):\n", "code": "try:\n    delattr(_MovedItems, name)\nexcept AttributeError:\n    try:\n        del moves.__dict__[name]\n    except KeyError:\n        raise AttributeError(\"no such move, %r\" % (name,))", "path": "six.py", "repo_name": "kholia/checksec", "stars": 66, "license": "None", "language": "python", "size": 1986}
{"docstring": "\"\"\"\ninternal use only\n\"\"\"\n", "func_signal": "def _init_socket(self):\n", "code": "try:\n    self.clamd_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    self.clamd_socket.connect((self.host, self.port))\n    self.clamd_socket.settimeout(self.timeout)\n\nexcept socket.error:\n    raise ConnectionError('Could not reach clamd using network ({0}, {1})'.format(self.host, self.port))\n\nreturn", "path": "pyclamd.py", "repo_name": "kholia/checksec", "stars": 66, "license": "None", "language": "python", "size": 1986}
{"docstring": "\"\"\"\nScan a buffer\n\nbuffer_to_test (string): buffer to scan\n\nreturn either:\n  - (dict): {filename1: \"virusname\"}\n  - None: if no virus found\n\nMay raise :\n  - BufferTooLongError: if the buffer size exceeds clamd limits\n  - ConnectionError: in case of communication problem\n\"\"\"\n", "func_signal": "def scan_stream(self, buffer_to_test):\n", "code": "try:\n    self._init_socket()\n    self._send_command('INSTREAM')\n\n    max_chunk_size = 1024 # MUST be < StreamMaxLength in /etc/clamav/clamd.conf\n\n    chunks_left = buffer_to_test\n    while len(chunks_left)>0:\n        chunk = chunks_left[:max_chunk_size]\n        chunks_left = chunks_left[max_chunk_size:]\n\n        size = bytes.decode(struct.pack('!L', len(chunk)))\n        self.clamd_socket.send(str.encode('{0}{1}'.format(size, chunk)))\n\n    self.clamd_socket.send(struct.pack('!L', 0))\n        \n    \nexcept socket.error:\n    raise ConnectionError('Unable to scan stream')\n\n\nresult='...'\ndr={}\nwhile result:\n    try:\n        result = self._recv_response()\n    except socket.error:\n        raise ConnectionError('Unable to scan stream')\n\n    if len(result) > 0:\n        \n        if result == 'INSTREAM size limit exceeded. ERROR':\n            raise BufferTooLongError(result)\n\n        filename, reason, status = self._parse_response(result)\n       \n        if status == 'ERROR':\n            dr[filename] = ('ERROR', '{0}'.format(reason))\n            \n        elif status == 'FOUND':\n            dr[filename] = ('FOUND', '{0}'.format(reason))\n\nself._close_socket()\nif not dr:\n    return None\nreturn dr", "path": "pyclamd.py", "repo_name": "kholia/checksec", "stars": 66, "license": "None", "language": "python", "size": 1986}
{"docstring": "\"\"\"\nThis is for internal use\n\"\"\"\n", "func_signal": "def _non_regression_test():\n", "code": "import doctest\ndoctest.testmod()\nreturn", "path": "pyclamd.py", "repo_name": "kholia/checksec", "stars": 66, "license": "None", "language": "python", "size": 1986}
{"docstring": "\"\"\"\nScan a file or directory given by filename using multiple threads (faster on SMP machines).\nDo not stop on error or virus found.\nScan with archive support enabled.\n\nfile (string): filename or directory (MUST BE ABSOLUTE PATH !)\n\nreturn either :\n  - (dict): {filename1: ('FOUND', 'virusname'), filename2: ('ERROR', 'reason')}\n  - None: if no virus found\n\nMay raise:\n  - ConnectionError: in case of communication problem\n\"\"\"\n", "func_signal": "def multiscan_file(self, file):\n", "code": "assert isinstance(file, str), 'Wrong type for [file], should be a string [was {0}]'.format(type(file))\n\ntry:\n    self._init_socket()\n    self._send_command('MULTISCAN {0}'.format(file))\nexcept socket.error:\n    raise ConnectionError('Unable to scan {0}'.format(file))\n\nresult='...'\ndr={}\nwhile result:\n    try:\n        result = self._recv_response()\n    except socket.error:\n        raise ConnectionError('Unable to scan {0}'.format(file))\n\n    if len(result) > 0:\n        for resline in result.splitlines():\n            filename, reason, status = self._parse_response(resline)\n\n            if status == 'ERROR':\n                dr[filename] = ('ERROR', '{0}'.format(reason))\n            \n            elif status == 'FOUND':\n                dr[filename] = ('FOUND', '{0}'.format(reason))\n\nself._close_socket()\nif not dr:\n    return None\nreturn dr", "path": "pyclamd.py", "repo_name": "kholia/checksec", "stars": 66, "license": "None", "language": "python", "size": 1986}
{"docstring": "\"\"\"\nSend a PING to the clamav server, which should reply\nby a PONG.\n\nreturn: True if the server replies to PING\n\nMay raise:\n  - ConnectionError: if the server do not reply by PONG\n\"\"\"\n\n", "func_signal": "def ping(self):\n", "code": "self._init_socket()\n\ntry:\n    self._send_command('PING')\n    result = self._recv_response()\n    self._close_socket()\nexcept socket.error:\n    raise ConnectionError('Could not ping clamd server')\n\nif result == 'PONG':\n    return True\nelse:\n    raise ConnectionError('Could not ping clamd server [{0}]'.format(result))\nreturn", "path": "pyclamd.py", "repo_name": "kholia/checksec", "stars": 66, "license": "None", "language": "python", "size": 1986}
{"docstring": "\"\"\"\nThis is for internal use\n\"\"\"\n", "func_signal": "def _print_doc():\n", "code": "import os\nos.system('pydoc ./{0}.py'.format(__name__))\nreturn", "path": "pyclamd.py", "repo_name": "kholia/checksec", "stars": 66, "license": "None", "language": "python", "size": 1986}
{"docstring": "\"\"\"Regularizes all tokens for each sentence in each paragraph.\"\"\"\n", "func_signal": "def regularize_text(self):\n", "code": "if not self.paragraphs:\n    self.tokenize_sentences()\nfor i, para in enumerate(self.paragraphs):\n    for j, sent in enumerate(para.sentence_tokens):\n        self.paragraphs[i].sentence_tokens[j] = regularize(sent)\n    # Remove empty sentences\n    self.paragraphs[i].sentence_tokens = [x for x in self.\n                                          paragraphs[i].sentence_tokens\n                                          if x]", "path": "causeofwhy\\wiki_dump_reader.py", "repo_name": "bwbaugh/causeofwhy", "stars": 103, "license": "other", "language": "python", "size": 287}
{"docstring": "\"\"\"Remove wiki markup leaving just the plain-text.\"\"\"\n# First fix wiktioanry links that aren't being handled properly\n# by the WikiExtractor library.\n", "func_signal": "def remove_markup(self):\n", "code": "wikt = r\"\\[{2,}wikt:[^\\|]+\\|([^\\]]+)\\]{2,}\"\nself.text = re.sub(wikt, r'\\1', self.text)\nbroken_wikt = r\"{{broken wikt link\\|([^\\|}]+)(?:\\|([^}]+))?}{2,}\"\nself.text = re.sub(broken_wikt, r'\\1', self.text)\n# Use the WikiExtractor library to finish processing\nself.text = WikiExtractor.clean(self.text)\nself.text = '\\n'.join(WikiExtractor.compact(self.text))", "path": "causeofwhy\\wiki_dump_reader.py", "repo_name": "bwbaugh/causeofwhy", "stars": 103, "license": "other", "language": "python", "size": 287}
{"docstring": "\"\"\"Initialize the Page object.\"\"\"\n", "func_signal": "def __init__(self, ID, title, text, start=None):\n", "code": "self.ID = ID\nself.title = title\nself.text = text\nself.start = start\nself.paragraphs = None\nself.token_count = None\nself.cosine_sim = None", "path": "causeofwhy\\wiki_dump_reader.py", "repo_name": "bwbaugh/causeofwhy", "stars": 103, "license": "other", "language": "python", "size": 287}
{"docstring": "\"\"\"Initialize the Paragraph object.\"\"\"\n", "func_signal": "def __init__(self, text):\n", "code": "self.text = text\nself.sentences = None\nself.sentence_tokens = None", "path": "causeofwhy\\wiki_dump_reader.py", "repo_name": "bwbaugh/causeofwhy", "stars": 103, "license": "other", "language": "python", "size": 287}
{"docstring": "\"\"\"Tokenize the sentence list in the paragraphs into list of tokens.\"\"\"\n", "func_signal": "def tokenize_sentences(self):\n", "code": "if not self.paragraphs:\n    self.segment_sentences()\nfor paragraph in self.paragraphs:\n    paragraph.tokenize_sentences()", "path": "causeofwhy\\wiki_dump_reader.py", "repo_name": "bwbaugh/causeofwhy", "stars": 103, "license": "other", "language": "python", "size": 287}
{"docstring": "\"\"\"Creates a string including ID, title, and original text.\"\"\"\n", "func_signal": "def __str__(self):\n", "code": "self.preprocess()\nf = StringIO()\nf.write('=' * 79 + '\\n')\nf.write(str(self.ID) + ' ' + self.title + '\\n')\nf.write('-' * 79 + '\\n')\nf.write(self.text.encode('utf-8') + '\\n')\nf.write('=' * 79 + '\\n')\noutput = f.getvalue()\nf.close()\nreturn output", "path": "causeofwhy\\wiki_dump_reader.py", "repo_name": "bwbaugh/causeofwhy", "stars": 103, "license": "other", "language": "python", "size": 287}
{"docstring": "\"\"\"Gets user query and any args and sends to an AnswerEngine.\"\"\"\n", "func_signal": "def get(self):\n", "code": "self.query = self.get_argument('q')\nnum_top = int(self.get_argument('top', default=5))\nself.num = int(self.get_argument('num', default=100))\nstart = int(self.get_argument('start', default=0))\nlch = float(self.get_argument('lch', default=2.16))\nself.log_training = bool(self.get_argument('train', default=False))\nself.ans_eng = AnswerEngine(self.index, self.query, start, num_top, lch)\nself.pool.apply_async(answer_engine.get_answers, (self.ans_eng,),\n                      callback=self.callback)", "path": "causeofwhy\\web.py", "repo_name": "bwbaugh/causeofwhy", "stars": 103, "license": "other", "language": "python", "size": 287}
{"docstring": "\"\"\"Parses a Wikipedia dump file and yields individual pages.\"\"\"\n", "func_signal": "def page_generator(file_obj, offset=None):\n", "code": "state = title = ID = text = start = None\npos = next_pos = 0\nfor line in file_obj:\n    # Keep track of file pos for later start of page seeking\n    pos = next_pos\n    next_pos += len(line)\n    line = line.decode('utf-8')\n    if state is None:\n        if '<page>' in line:\n            state = 'page'\n            start = pos\n    elif state == 'page':\n        title = re.search(r'<title>(.*?)</title>', line)\n        if title:\n            state = 'title'\n            title = title.group(1)\n    elif state == 'title':\n        ID = re.search(r'<id>(\\d+)</id>', line)\n        if ID:\n            state = 'id'\n            ID = ID.group(1)\n    elif state == 'id':\n        if line.endswith('</text>\\n'):\n            text = re.search(r'<text[^>]*>(.*?)</text>', line).group(1)\n            state = 'done'\n        else:\n            text = re.search(r'<text.*?>', line)\n            if text:\n                text = [line[text.end():]]\n                state = 'text'\n    elif state == 'text':\n        if line.endswith('</text>\\n'):\n            text.append(line[:-8])\n            text = ''.join(text)\n            state = 'done'\n        else:\n            text.append(line)\n    if state == 'done':\n        state = None\n        if bad_page(title, text):\n            continue\n        else:\n            yield Page(int(ID), title, text, start)", "path": "causeofwhy\\wiki_dump_reader.py", "repo_name": "bwbaugh/causeofwhy", "stars": 103, "license": "other", "language": "python", "size": 287}
{"docstring": "\"\"\"Convenience method that removed markup does unidecode.\"\"\"\n", "func_signal": "def preprocess(self):\n", "code": "self.remove_markup()\nself.unidecode()", "path": "causeofwhy\\wiki_dump_reader.py", "repo_name": "bwbaugh/causeofwhy", "stars": 103, "license": "other", "language": "python", "size": 287}
{"docstring": "\"\"\"Segment each Paragraph into a list of sentences.\"\"\"\n", "func_signal": "def segment_sentences(self):\n", "code": "if not self.paragraphs:\n    self.segment_paragraphs()\nfor paragraph in self.paragraphs:\n    paragraph.segment_sentences()", "path": "causeofwhy\\wiki_dump_reader.py", "repo_name": "bwbaugh/causeofwhy", "stars": 103, "license": "other", "language": "python", "size": 287}
{"docstring": "\"\"\"Count the frequency of text's tokens in a bag-of-words style.\"\"\"\n", "func_signal": "def count_tokens(self):\n", "code": "self.token_count = collections.defaultdict(int)\nfor paragraph in self.paragraphs:\n    for sentence in paragraph.sentence_tokens:\n        for token in sentence:\n            self.token_count[str(token)] += 1\nself.token_count = [(token, count) for (token, count) in\\\n                    sorted(self.token_count.iteritems(),\n                           key=operator.itemgetter(1),\n                           reverse=True)]", "path": "causeofwhy\\wiki_dump_reader.py", "repo_name": "bwbaugh/causeofwhy", "stars": 103, "license": "other", "language": "python", "size": 287}
{"docstring": "\"\"\"Stores references to the processing pool and Index object.\"\"\"\n", "func_signal": "def initialize(self):\n", "code": "self.pool = self.application.settings.get('pool')\nself.index = self.application.settings.get('index')", "path": "causeofwhy\\web.py", "repo_name": "bwbaugh/causeofwhy", "stars": 103, "license": "other", "language": "python", "size": 287}
{"docstring": "\"\"\"Renders a list of computed answers as a response to the query.\"\"\"\n", "func_signal": "def callback(self, args):\n", "code": "answers, ir_query_tagged = args\n# Display result\nself.render(\"answer.html\",\n            query=self.query,\n            ir_query=' '.join(self.ans_eng.ir_query),\n            ir_query_tagged=ir_query_tagged,\n            num_pages=self.ans_eng.num_pages,\n            num_answers=len(answers),\n            answers=answers[:self.num])\n# Log answer details\nif self.log_training:\n    with open('log_training.txt'.format(self.num), mode='a') as f:\n        f.write('\\t'.join([' '.join(self.ans_eng.ir_query),\n                           self.query]) + '\\t0')\n        for rank, answer in enumerate(answers, start=1):\n            f.write('\\t' + '\\t'.join([str(rank)] + [str(x) for x in\n                                                    answer._features]))\n        f.write('\\n')", "path": "causeofwhy\\web.py", "repo_name": "bwbaugh/causeofwhy", "stars": 103, "license": "other", "language": "python", "size": 287}
{"docstring": "\"\"\"Convert non-ascii to closest ASCII equivalent.\"\"\"\n", "func_signal": "def unidecode(self):\n", "code": "self.title = unidecode(self.title).strip()\nself.text = unidecode(self.text).strip()", "path": "causeofwhy\\wiki_dump_reader.py", "repo_name": "bwbaugh/causeofwhy", "stars": 103, "license": "other", "language": "python", "size": 287}
{"docstring": "\"\"\"Sets default values for each incoming request.\"\"\"\n", "func_signal": "def prepare(self):\n", "code": "self.query = None\nself.ans_eng = None", "path": "causeofwhy\\web.py", "repo_name": "bwbaugh/causeofwhy", "stars": 103, "license": "other", "language": "python", "size": 287}
{"docstring": "\"\"\"Tokenize each sentence in the list into a list of tokens.\"\"\"\n", "func_signal": "def tokenize_sentences(self):\n", "code": "if not self.sentences:\n    self.segment_sentences()\nself.sentence_tokens = tokenizer.batch_tokenize(self.sentences)", "path": "causeofwhy\\wiki_dump_reader.py", "repo_name": "bwbaugh/causeofwhy", "stars": 103, "license": "other", "language": "python", "size": 287}
{"docstring": "\"\"\"Uses heuristics to see if a page shouldn't be processed.\"\"\"\n", "func_signal": "def bad_page(title, text):\n", "code": "for term in title_start_with_terms:\n    if title[:len(term)].upper() == term:\n        return True\nfor term in title_end_with_terms:\n    if title[-len(term):].upper() == term:\n        return True\nif len(text) <= page_length_limit:\n    return True\nfor term in text_start_with_terms:\n    if term == text[:len(term)].upper():\n        return True\nfor term in text_last_terms:\n    if term in text[-8000:].upper():\n        return True\nreturn False", "path": "causeofwhy\\wiki_dump_reader.py", "repo_name": "bwbaugh/causeofwhy", "stars": 103, "license": "other", "language": "python", "size": 287}
{"docstring": "\"\"\"Segment the Paragraph text into a list of sentences.\"\"\"\n# Sentence segmentation\n", "func_signal": "def segment_sentences(self):\n", "code": "if LINE_SEPARATOR in self.text:\n    self.sentences = [sent for sent in self.text.split(LINE_SEPARATOR)]\nelse:\n    self.sentences = sent_detector.tokenize(self.text,\n                                            realign_boundaries=True)", "path": "causeofwhy\\wiki_dump_reader.py", "repo_name": "bwbaugh/causeofwhy", "stars": 103, "license": "other", "language": "python", "size": 287}
{"docstring": "\"\"\"Starts the web server as a user interface to the system.\"\"\"\n", "func_signal": "def main(index, port=8080):\n", "code": "pool = multiprocessing.Pool(NUMBER_OF_PROCESSES)\n# Give each pool initial piece of work so that they initialize.\nans_eng = AnswerEngine(index, 'bird sing', 0, 1, 2.16)\nfor x in xrange(NUMBER_OF_PROCESSES * 2):\n    pool.apply_async(answer_engine.get_answers, (ans_eng,))\ndel ans_eng\napplication = tornado.web.Application([\n    (r\"/\", MainHandler),\n    (r\"/cause/\", QueryHandler),\n    ], template_path=os.path.join(os.path.dirname(__file__), \"templates\"),\n    static_path=os.path.join(os.path.dirname(__file__), \"static\"),\n    index=index,\n    pool=pool)\nhttp_server = tornado.httpserver.HTTPServer(application, xheaders=True)\nhttp_server.listen(port)\ntornado.ioloop.IOLoop.instance().start()", "path": "causeofwhy\\web.py", "repo_name": "bwbaugh/causeofwhy", "stars": 103, "license": "other", "language": "python", "size": 287}
{"docstring": "\"\"\"Yields individual pages from a generated plain-text corpus file.\"\"\"\n", "func_signal": "def plain_page_generator(file_obj):\n", "code": "title = ID = text = None\npos = next_pos = 0\nfor line in file_obj:\n    # Keep track of file pos for later start of page seeking\n    pos = next_pos\n    next_pos += len(line)\n    line = line.decode('utf-8')\n    ID, title, text = line.split('\\t')\n    yield Page(int(ID), title, text, pos)", "path": "causeofwhy\\wiki_dump_reader.py", "repo_name": "bwbaugh/causeofwhy", "stars": 103, "license": "other", "language": "python", "size": 287}
{"docstring": "\"\"\"\nparameters: \n    x: the value s.t. lin_proj*N = x; scalar\n    lin_proj: the vector to project the sample unto (n,)\n    covariance: the covariance matrix of the unconditional samples (nxn)\n    n_samples: the number of samples to return\n    \nreturns:\n    ( n x n_samples ) numpy array \n    \n\"\"\"\n", "func_signal": "def sample_normal_given_projection(  covariance, x, lin_proj, n_samples=1):\n", "code": "variance = np.dot( np.dot( lin_proj.T, covariance), lin_proj )\n\n#normalize our variables s.t. lin_proj*N is N(0,1)\n\nsigma_lin = np.dot(covariance, lin_proj[:,None])\ncond_mu = ( sigma_lin.T*x/variance ).flatten()\ncond_covar = covariance - np.dot( sigma_lin, sigma_lin.T )/ variance\n\n_samples = np.random.multivariate_normal( cond_mu, cond_covar, size = (n_samples) )\nreturn ( _samples )", "path": "MonteCarlo\\sample_normal_given_projection.py", "repo_name": "CamDavidsonPilon/Python-Numerics", "stars": 98, "license": "None", "language": "python", "size": 310}
{"docstring": "\"\"\"\ncovariance matrix to correlation matrix.\n\"\"\"\n", "func_signal": "def cov2corr( A ):\n", "code": "d = np.sqrt(A.diagonal())\nA = ((A.T/d).T)/d\n#A[ np.diag_indices(A.shape[0]) ] = np.ones( A.shape[0] )\nreturn A", "path": "utils\\cov2corr.py", "repo_name": "CamDavidsonPilon/Python-Numerics", "stars": 98, "license": "None", "language": "python", "size": 310}
{"docstring": "\"\"\"\nFit the model to some data. Estimates the transition and intial probabilities.\nInput:\n    Data: a (nxt) numpy array of n samples, each t unit long. The data must have a specific \n        form to be read in where each possible emission is enumerated starting from 0 \n        (called encoded data).\n    encoded: a boolean representing if the data is encoded. If not, a naive EncodingScheme will be used.\n\n   \n\"\"\"\n\n", "func_signal": "def fit(self, data, encoded=True):\n", "code": "self._fit_init(data, encoded)\nlist_series_length = range(1, self.len_trials)\n\nfor encoded_series in data:\n    \n    self.init_probs_estimate[ encoded_series[0] ] += 1\n    for j in list_series_length:\n        self.trans_probs_estimate[ encoded_series[j-1], encoded_series[j] ] += 1 \n\n    self.number_of_series += 1\nself.init_probs_estimate = self._normalize( self.init_probs_estimate )\nself.trans_probs_estimate = self._normalize( self.trans_probs_estimate )", "path": "MultinomialMarkovAndEncoding\\multinomialMM.py", "repo_name": "CamDavidsonPilon/Python-Numerics", "stars": 98, "license": "None", "language": "python", "size": 310}
{"docstring": "#check if x,y are same shape\n", "func_signal": "def _corr(x,y, remove_outliers=False):\n", "code": "n = x.shape[0]\nif x.var()==0 or y.var()==0:\n    return 0\nelse:\n    if remove_outliers:\n        ee = EllipticEnvelope(store_precision = False, contamination=0.05)\n        ee.fit( np.concatenate( [x[:,None],y[:,None] ], axis=1) )\n        c = ee.covariance_\n        return c[0,1]/np.sqrt( c[0,0]*c[1,1] )\n    return np.dot( x - x.mean(), y - y.mean() ) / np.sqrt(( x.var()*y.var() ))/ n", "path": "MachineLearningScikitLearn\\maxCorrelationTransformer.py", "repo_name": "CamDavidsonPilon/Python-Numerics", "stars": 98, "license": "None", "language": "python", "size": 310}
{"docstring": "\"\"\"\ncovariance matrix to correlation matrix.\n\"\"\"\n", "func_signal": "def cov2corr( A ):\n", "code": "d = np.sqrt(A.diagonal())\nA = ((A.T/d).T)/d\n#A[ np.diag_indices(A.shape[0]) ] = np.ones( A.shape[0] )\nreturn A", "path": "MachineLearningScikitLearn\\maxCorrelationTransformer.py", "repo_name": "CamDavidsonPilon/Python-Numerics", "stars": 98, "license": "None", "language": "python", "size": 310}
{"docstring": "#K and X are a list, K is increasing positions, min(K)>0\n# TODO\n", "func_signal": "def __sample_conditional( self, K, X):\n", "code": "sample = np.empty( (1, self.len_trials) )\nfor i,k in enumerate(K):\n    substr = self._sample_conditional( X[i] )\n    pass", "path": "MultinomialMarkovAndEncoding\\multinomialMM.py", "repo_name": "CamDavidsonPilon/Python-Numerics", "stars": 98, "license": "None", "language": "python", "size": 310}
{"docstring": "\"\"\"\nchecks if word is a swear word, or a missing spelling of swear word.\n\"\"\"\n", "func_signal": "def isswear( word, max_distance = 1):\n", "code": "word = word.lower()\ndl = lambda x: dl_distance(x, word) <= max_distance\nreturn any( map(dl, swear_list) )", "path": "DamerauLevenshteinDistance\\example.py", "repo_name": "CamDavidsonPilon/Python-Numerics", "stars": 98, "license": "None", "language": "python", "size": 310}
{"docstring": "\"\"\"\nBandits is an object that can be called like bandits.pull(choice) and returns a 0 or 1.\n\n\n\"\"\"\n", "func_signal": "def fit(self, bandits, trials = 10 ):\n", "code": "n_bandits = len( bandits )\nself.n_pulls = np.zeros( n_bandits )\nself.n_successes = np.zeros( n_bandits )\nself.prior_distibutions = np.array( [self.prior_alpha, self.prior_beta])*np.ones( (n_bandits, 2 ) )\n\nfor i in xrange(trials):\n\n    choice = np.argmax( self.betad.rvs( self.prior_distibutions[:,0] + self.n_successes,\n                                        self.prior_distibutions[:,1] + self.n_pulls - self.n_successes ) )\n    outcome = bandits.pull(choice)\n    self.n_pulls[choice] += 1\n    self.n_successes[choice] += outcome\n\nself.posterior_alpha =  self.prior_distibutions[:,0] + self.n_successes\nself.posterior_beta = self.prior_distibutions[:,1] + self.n_pulls - self.n_successes\nreturn", "path": "MachineLearningScikitLearn\\BayesianBandit.py", "repo_name": "CamDavidsonPilon/Python-Numerics", "stars": 98, "license": "None", "language": "python", "size": 310}
{"docstring": "#normalizes the array to sum to one. The array should be semi-positive\n", "func_signal": "def _normalize(self, array ):\n", "code": "try:\n    #2d?\n    return array.astype(\"float\")/array.sum(1)[:,None]\nexcept: \n    #oh, 1d\n    return array.astype(\"float\")/array.sum()", "path": "MultinomialMarkovAndEncoding\\multinomialMM.py", "repo_name": "CamDavidsonPilon/Python-Numerics", "stars": 98, "license": "None", "language": "python", "size": 310}
{"docstring": "#Sample the process, but at position k, put x (or put NOT x).\n", "func_signal": "def sample_conditional(self, k, x, negate=False):\n", "code": "sample = np.empty( (1, self.len_trials) )\nnegate = int(negate) #0 or 1\nsample[0,0] = np.argmax( np.random.multinomial( 1, self.init_probs_estimate ) )\nfor i in range(1, k + negate):\n    A = np.linalg.matrix_power( self.trans_probs_estimate, k-i )\n    if not negate:\n        p = self.trans_probs_estimate[ sample[0,i-1], :]*A[:, x ]\n    else:\n        p = self.trans_probs_estimate[ sample[0,i-1], :]*(1-A[:, x ])\n\n    p = self._normalize(p)\n    sample[0, i] = np.argmax( np.random.multinomial( 1, p ) )\n\nif not negate:\n    sample[0, k] = x\n    \n\nfor i in range(k+ 1, self.len_trials):\n                sample[0, i] = np.argmax(np.random.multinomial( 1, self.trans_probs_estimate[ sample[0,i-1],: ] ) )\nreturn sample", "path": "MultinomialMarkovAndEncoding\\multinomialMM.py", "repo_name": "CamDavidsonPilon/Python-Numerics", "stars": 98, "license": "None", "language": "python", "size": 310}
{"docstring": "\"\"\"\n    M: the constant s.t. sample_g*M >= target_f for all x\n    sample_g: a scipy.stats frozen random variable.\n    target_f: a 1-d integrable, positive function\n\"\"\"\n", "func_signal": "def __init__(self, target_f, sample_g, M):\n", "code": "self.target_f = target_f\nself.sample_g = sample_g\nself.uniform = stats.uniform\nself.M = M", "path": "MonteCarlo\\sampling_methods.py", "repo_name": "CamDavidsonPilon/Python-Numerics", "stars": 98, "license": "None", "language": "python", "size": 310}
{"docstring": "\"\"\"\nThis computes the partial-correlation between X and Y, with covariates Z.\n\"\"\"\n", "func_signal": "def partial_correlation(X, Y, Z):\n", "code": "lr1 = LR()\nlr2 = LR()\nlr1.fit(Z,X)\nlr2.fit(Z,Y)\n\nreturn np.corrcoef( Y - lr1.predict(Z), X - lr2.predict(Z) )[0,1]", "path": "MachineLearningScikitLearn\\maxCorrelationTransformer.py", "repo_name": "CamDavidsonPilon/Python-Numerics", "stars": 98, "license": "None", "language": "python", "size": 310}
{"docstring": "\"\"\"\ndf: a dataframe \nignore: an iterable of columns to not make quad features out of.\n\nreturns:\n    a copied dataframe with quadratic features, including squares of variables if squares == True.\n\n\"\"\"\n", "func_signal": "def create_pairwise_data( df, ignore = [], squares = True):\n", "code": "n,d = df.shape\ncolumns = df.columns.diff( ignore )\n    \ndf = df.copy()\n\niterator = combinations_with_replacement if squares else combinations  \n\nfor x,y in iterator( columns, 2):\n    df[ x + \"__times__\" + y ] = df[x]*df[y]\n    \n    \nreturn df", "path": "utils\\dataframe_pairwise_feature_gen.py", "repo_name": "CamDavidsonPilon/Python-Numerics", "stars": 98, "license": "None", "language": "python", "size": 310}
{"docstring": "\"\"\"numerically unstable for large dimensions\"\"\"\n", "func_signal": "def cdf2pdf( f, u, delta=0.001, kwargs={} ):\n", "code": "def _wrapper(*args):\n    u = np.array(args)\n    return f(u, **kwargs)\nn = u.shape[0]\np= _pdf( _wrapper, u, delta)\nreturn np.exp( np.log(p) - n*np.log( delta ) )\n#return p / delta**n", "path": "NumericalDerivatives\\diff.py", "repo_name": "CamDavidsonPilon/Python-Numerics", "stars": 98, "license": "None", "language": "python", "size": 310}
{"docstring": "\"\"\"\nSample the learned model n times.\n\n\"\"\"\n", "func_signal": "def sample(self, n=1):\n", "code": "samples = np.empty( (n, self.len_trials) )\nfor i in range(n):\n    samples[i,:] = self._sample()\nreturn samples", "path": "MultinomialMarkovAndEncoding\\multinomialMM.py", "repo_name": "CamDavidsonPilon/Python-Numerics", "stars": 98, "license": "None", "language": "python", "size": 310}
{"docstring": "\"\"\"\nThis is the main function.\n    style: either \"min\" or \"max\", get the min or maximum price respectively.\n    F: the final payoff function\n    sigma_max, sigma_min: the max and min volatility\n    delta_t: the length of time step\n    r: the risk-free rate\n    S_0: the initial price of the underlying\n    n: the time step\n    j: position in tree\n    N: the number of time steps. I'd keep this not too large, else you stack overflow lol.\n\"\"\"\n", "func_signal": "def price( style, F, sigma_max, sigma_min, delta_t, r, S_0, n, j, N):\n", "code": "if n == N:\n    return F( Snj(S_0, n, j, sigma_max, r, delta_t ) )\n\nt = sigma_max*np.sqrt(delta_t)/2\nl = (1-t)*price(style,F, sigma_max, sigma_min, delta_t, r, S_0, n+1, j+1, N) + \\\n    (1+t)*price(style,F, sigma_max, sigma_min, delta_t, r, S_0, n+1, j-1, N) - \\\n    2*price(style,F, sigma_max, sigma_min, delta_t, r, S_0, n+1, j, N)\n\nc = 0.5 if (1-2*(style==\"min\"))*l >= 0 else sigma_min**2/(2*sigma_max**2)\n\nreturn  np.exp( -r*delta_t)*( price(style, F, sigma_max, sigma_min, delta_t, r, S_0, n+1, j, N) + c*l )", "path": "DiscreteOptionPricing\\price_bounds.py", "repo_name": "CamDavidsonPilon/Python-Numerics", "stars": 98, "license": "None", "language": "python", "size": 310}
{"docstring": "#initalize\n", "func_signal": "def euler(self, t, n):\n", "code": "P,N = self._init(t,n)\n\nfor i in xrange(1,int(N)):\n    x = P[:, i-1]\n    P[:,i] = x + self.drift(x)*self.delta + self.diffusion(x)*np.sqrt(self.delta)*np.random.randn( n )\n\nreturn P", "path": "DiscreteSDE\\discreteSDE.py", "repo_name": "CamDavidsonPilon/Python-Numerics", "stars": 98, "license": "None", "language": "python", "size": 310}
{"docstring": "\"\"\"\nperforms the Gramm-Shmidt process to orthonormalize a a matrix of vectors.\nX: vectors to orthonormalize are rows.\nReturns Y, same shape as X, and with orthonormal rows.\n\"\"\"\n", "func_signal": "def gs(X):\n", "code": "Y = np.zeros_like(X)\nfor i in range(len(X)):\n    temp_vec = X[i]\n    for j in range(i) :\n        proj_vec = proj(Y[j,:], X[i])\n        temp_vec = temp_vec - proj_vec\n    Y[i,:] = temp_vec/np.sqrt( np.dot( temp_vec,temp_vec ) )\nreturn Y", "path": "MonteCarlo\\grammschmidt.py", "repo_name": "CamDavidsonPilon/Python-Numerics", "stars": 98, "license": "None", "language": "python", "size": 310}
{"docstring": "\"\"\"\nPlots contours for non-evenly spaced data.\nx,y,z must be 1d arrays.\nlines = # of contour lines (default 18 )\nlinewidth = line width of lines (default 2 )\n\"\"\"\n\n", "func_signal": "def contour(x,y,z, linewidth = 2, labels = None):\n", "code": "assert x.shape[0] == y.shape[0] == z.shape[0], \"arrays x,y,z must be the same size\"\n\n#make a grid that surrounds x,y support\nxi = np.linspace(x.min(),x.max(),100)\nyi = np.linspace(y.min(),y.max(),100)\n# grid the data.\nzi = griddata((x, y), z, (xi[None,:], yi[:,None]), method='cubic')\n# contour the gridded data, plotting dots at the randomly spaced data points.\nplt.figure()\nCS = plt.contour(xi,yi,zi,linewidth=2)\nplt.clabel(CS, inline=1, fontsize=10)\n\nif labels:\n    plt.xlabel(labels[0])\n    plt.ylabel(labels[1])\n# plot data points.\nplt.scatter(x,y,c=z,s=60, alpha = 0.7, edgecolors = \"none\")\nplt.xlim(x.min(),x.max())\nplt.ylim(y.min(),y.max())\nplt.show()", "path": "utils\\contour_irregular_data.py", "repo_name": "CamDavidsonPilon/Python-Numerics", "stars": 98, "license": "None", "language": "python", "size": 310}
{"docstring": "\"\"\"Calculate the Damerau-Levenshtein distance between sequences.\n\nThis distance is the number of additions, deletions, substitutions,\nand transpositions needed to transform the first sequence into the\nsecond. Although generally used with strings, any sequences of\ncomparable objects will work.\n\nTranspositions are exchanges of *consecutive* characters; all other\noperations are self-explanatory.\n\nThis implementation is O(N*M) time and O(M) space, for N and M the\nlengths of the two sequences.\n\n>>> dameraulevenshtein('ba', 'abc')\n2\n>>> dameraulevenshtein('fee', 'deed')\n2\n\nIt works with arbitrary sequences too:\n>>> dameraulevenshtein('abcd', ['b', 'a', 'c', 'd', 'e'])\n2\n\"\"\"\n# codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n# Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1 matrix.\n# However, only the current and two previous rows are needed at once,\n# so we only store those.\n", "func_signal": "def dameraulevenshtein(seq1, seq2):\n", "code": "oneago = None\nthisrow = range(1, len(seq2) + 1) + [0]\nfor x in xrange(len(seq1)):\n    # Python lists wrap around for negative indices, so put the\n    # leftmost column at the *end* of the list. This matches with\n    # the zero-indexed strings and saves extra calculation.\n    twoago, oneago, thisrow = oneago, thisrow, [0] * len(seq2) + [x + 1]\n    for y in xrange(len(seq2)):\n        delcost = oneago[y] + 1\n        addcost = thisrow[y - 1] + 1\n        subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n        thisrow[y] = min(delcost, addcost, subcost)\n        # This block deals with transpositions\n        if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n            and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n            thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\nreturn thisrow[len(seq2) - 1]", "path": "DamerauLevenshteinDistance\\dameraulevenshtein.py", "repo_name": "CamDavidsonPilon/Python-Numerics", "stars": 98, "license": "None", "language": "python", "size": 310}
{"docstring": "# For Gzip and Bz2 Tests: fail with a ReadError on an uncompressed file.\n", "func_signal": "def test_fail_comp(self):\n", "code": "if self.mode == \"r:\":\n    return\nself.assertRaises(tarfile.ReadError, tarfile.open, tarname, self.mode)\nfobj = open(tarname, \"rb\")\nself.assertRaises(tarfile.ReadError, tarfile.open, fileobj=fobj, mode=self.mode)", "path": "python\\Lib\\test\\test_tarfile.py", "repo_name": "smart-mobile-software/gitstack", "stars": 124, "license": "other", "language": "python", "size": 94495}
{"docstring": "# Test for issue6123: Allow opening empty archives.\n# This test guarantees that tarfile.open() does not treat an empty\n# file as an empty tar archive.\n", "func_signal": "def test_null_tarfile(self):\n", "code": "open(tmpname, \"wb\").close()\nself.assertRaises(tarfile.ReadError, tarfile.open, tmpname, self.mode)\nself.assertRaises(tarfile.ReadError, tarfile.open, tmpname)", "path": "python\\Lib\\test\\test_tarfile.py", "repo_name": "smart-mobile-software/gitstack", "stars": 124, "license": "other", "language": "python", "size": 94495}
{"docstring": "\"\"\"Class method to parse get_comment_list/count/form and return a Node.\"\"\"\n", "func_signal": "def handle_token(cls, parser, token):\n", "code": "tokens = token.contents.split()\nif tokens[1] != 'for':\n    raise template.TemplateSyntaxError(\"Second argument in %r tag must be 'for'\" % tokens[0])\n\n# {% get_whatever for obj as varname %}\nif len(tokens) == 5:\n    if tokens[3] != 'as':\n        raise template.TemplateSyntaxError(\"Third argument in %r must be 'as'\" % tokens[0])\n    return cls(\n        object_expr = parser.compile_filter(tokens[2]),\n        as_varname = tokens[4],\n    )\n\n# {% get_whatever for app.model pk as varname %}\nelif len(tokens) == 6:\n    if tokens[4] != 'as':\n        raise template.TemplateSyntaxError(\"Fourth argument in %r must be 'as'\" % tokens[0])\n    return cls(\n        ctype = BaseCommentNode.lookup_content_type(tokens[2], tokens[0]),\n        object_pk_expr = parser.compile_filter(tokens[3]),\n        as_varname = tokens[5]\n    )\n\nelse:\n    raise template.TemplateSyntaxError(\"%r tag requires 4 or 5 arguments\" % tokens[0])", "path": "python\\Lib\\site-packages\\django\\contrib\\comments\\templatetags\\comments.py", "repo_name": "smart-mobile-software/gitstack", "stars": 124, "license": "other", "language": "python", "size": 94495}
{"docstring": "\"\"\"\nRedirect to a given URL.\n\nThe given url may contain dict-style string formatting, which will be\ninterpolated against the params in the URL.  For example, to redirect from\n``/foo/<id>/`` to ``/bar/<id>/``, you could use the following URLconf::\n\n    urlpatterns = patterns('',\n        ('^foo/(?P<id>\\d+)/$', 'django.views.generic.simple.redirect_to', {'url' : '/bar/%(id)s/'}),\n    )\n\nIf the given url is ``None``, a HttpResponseGone (410) will be issued.\n\nIf the ``permanent`` argument is False, then the response will have a 302\nHTTP status code. Otherwise, the status code will be 301.\n\nIf the ``query_string`` argument is True, then the GET query string\nfrom the request is appended to the URL.\n\n\"\"\"\n", "func_signal": "def redirect_to(request, url, permanent=True, query_string=False, **kwargs):\n", "code": "args = request.META.get('QUERY_STRING', '')\n\nif url is not None:\n    if kwargs:\n        url = url % kwargs\n\n    if args and query_string:\n        url = \"%s?%s\" % (url, args)\n\n    klass = permanent and HttpResponsePermanentRedirect or HttpResponseRedirect\n    return klass(url)\nelse:\n    logger.warning('Gone: %s', request.path,\n                extra={\n                    'status_code': 410,\n                    'request': request\n                })\n    return HttpResponseGone()", "path": "python\\Lib\\site-packages\\django\\views\\generic\\simple.py", "repo_name": "smart-mobile-software/gitstack", "stars": 124, "license": "other", "language": "python", "size": 94495}
{"docstring": "# Test for bug #1543303.\n", "func_signal": "def test_stream_padding(self):\n", "code": "tar = tarfile.open(tmpname, self.mode)\ntar.close()\n\nif self.mode.endswith(\"gz\"):\n    fobj = gzip.GzipFile(tmpname)\n    data = fobj.read()\n    fobj.close()\nelif self.mode.endswith(\"bz2\"):\n    dec = bz2.BZ2Decompressor()\n    data = open(tmpname, \"rb\").read()\n    data = dec.decompress(data)\n    self.assertTrue(len(dec.unused_data) == 0,\n            \"found trailing data\")\nelse:\n    fobj = open(tmpname, \"rb\")\n    data = fobj.read()\n    fobj.close()\n\nself.assertTrue(data.count(\"\\0\") == tarfile.RECORDSIZE,\n                 \"incorrect zero padding\")", "path": "python\\Lib\\test\\test_tarfile.py", "repo_name": "smart-mobile-software/gitstack", "stars": 124, "license": "other", "language": "python", "size": 94495}
{"docstring": "# Test if extractall works properly when tarfile contains broken\n# symlinks\n", "func_signal": "def test_extractall_broken_symlinks(self):\n", "code": "tempdir = os.path.join(TEMPDIR, \"testsymlinks\")\ntemparchive = os.path.join(TEMPDIR, \"testsymlinks.tar\")\nos.mkdir(tempdir)\ntry:\n    source_file = os.path.join(tempdir,'source')\n    target_file = os.path.join(tempdir,'symlink')\n    with open(source_file,'w') as f:\n        f.write('something\\n')\n    os.symlink(source_file, target_file)\n    tar = tarfile.open(temparchive,'w')\n    tar.add(target_file, arcname=os.path.basename(target_file))\n    tar.close()\n    # remove the real file\n    os.unlink(source_file)\n    # Let's extract it to the location which contains the symlink\n    tar = tarfile.open(temparchive,'r')\n    # this should not raise OSError: [Errno 17] File exists\n    try:\n        tar.extractall(path=tempdir)\n    except OSError:\n        self.fail(\"extractall failed with broken symlinked files\")\n    finally:\n        tar.close()\nfinally:\n    os.unlink(temparchive)\n    shutil.rmtree(tempdir)", "path": "python\\Lib\\test\\test_tarfile.py", "repo_name": "smart-mobile-software/gitstack", "stars": 124, "license": "other", "language": "python", "size": 94495}
{"docstring": "# Test if the IOError exception is passed through properly.\n", "func_signal": "def test_exception(self):\n", "code": "with self.assertRaises(Exception) as exc:\n    with tarfile.open(tarname) as tar:\n        raise IOError\nself.assertIsInstance(exc.exception, IOError,\n                      \"wrong exception raised in context manager\")\nself.assertTrue(tar.closed, \"context manager failed\")", "path": "python\\Lib\\test\\test_tarfile.py", "repo_name": "smart-mobile-software/gitstack", "stars": 124, "license": "other", "language": "python", "size": 94495}
{"docstring": "# __exit__() must write end-of-archive blocks, i.e. call\n# TarFile.close() if there was no error.\n", "func_signal": "def test_eof(self):\n", "code": "with tarfile.open(tmpname, \"w\"):\n    pass\nself.assertNotEqual(os.path.getsize(tmpname), 0,\n        \"context manager wrote no end-of-archive block\")", "path": "python\\Lib\\test\\test_tarfile.py", "repo_name": "smart-mobile-software/gitstack", "stars": 124, "license": "other", "language": "python", "size": 94495}
{"docstring": "\"\"\"Class method to parse render_comment_list and return a Node.\"\"\"\n", "func_signal": "def handle_token(cls, parser, token):\n", "code": "tokens = token.contents.split()\nif tokens[1] != 'for':\n    raise template.TemplateSyntaxError(\"Second argument in %r tag must be 'for'\" % tokens[0])\n\n# {% render_comment_list for obj %}\nif len(tokens) == 3:\n    return cls(object_expr=parser.compile_filter(tokens[2]))\n\n# {% render_comment_list for app.models pk %}\nelif len(tokens) == 4:\n    return cls(\n        ctype = BaseCommentNode.lookup_content_type(tokens[2], tokens[0]),\n        object_pk_expr = parser.compile_filter(tokens[3])\n    )", "path": "python\\Lib\\site-packages\\django\\contrib\\comments\\templatetags\\comments.py", "repo_name": "smart-mobile-software/gitstack", "stars": 124, "license": "other", "language": "python", "size": 94495}
{"docstring": "# Test if extractall works properly when tarfile contains symlinks\n", "func_signal": "def test_extractall_symlinks(self):\n", "code": "tempdir = os.path.join(TEMPDIR, \"testsymlinks\")\ntemparchive = os.path.join(TEMPDIR, \"testsymlinks.tar\")\nos.mkdir(tempdir)\ntry:\n    source_file = os.path.join(tempdir,'source')\n    target_file = os.path.join(tempdir,'symlink')\n    with open(source_file,'w') as f:\n        f.write('something\\n')\n    os.symlink(source_file, target_file)\n    tar = tarfile.open(temparchive,'w')\n    tar.add(source_file, arcname=os.path.basename(source_file))\n    tar.add(target_file, arcname=os.path.basename(target_file))\n    tar.close()\n    # Let's extract it to the location which contains the symlink\n    tar = tarfile.open(temparchive,'r')\n    # this should not raise OSError: [Errno 17] File exists\n    try:\n        tar.extractall(path=tempdir)\n    except OSError:\n        self.fail(\"extractall failed with symlinked files\")\n    finally:\n        tar.close()\nfinally:\n    os.unlink(temparchive)\n    shutil.rmtree(tempdir)", "path": "python\\Lib\\test\\test_tarfile.py", "repo_name": "smart-mobile-software/gitstack", "stars": 124, "license": "other", "language": "python", "size": 94495}
{"docstring": "# The fields from the pax header have priority over the\n# TarInfo.\n", "func_signal": "def test_pax_extended_header(self):\n", "code": "pax_headers = {u\"path\": u\"foo\", u\"uid\": u\"123\"}\n\ntar = tarfile.open(tmpname, \"w\", format=tarfile.PAX_FORMAT, encoding=\"iso8859-1\")\nt = tarfile.TarInfo()\nt.name = u\"\"     # non-ASCII\nt.uid = 8**8        # too large\nt.pax_headers = pax_headers\ntar.addfile(t)\ntar.close()\n\ntar = tarfile.open(tmpname, encoding=\"iso8859-1\")\nt = tar.getmembers()[0]\nself.assertEqual(t.pax_headers, pax_headers)\nself.assertEqual(t.name, \"foo\")\nself.assertEqual(t.uid, 123)", "path": "python\\Lib\\test\\test_tarfile.py", "repo_name": "smart-mobile-software/gitstack", "stars": 124, "license": "other", "language": "python", "size": 94495}
{"docstring": "# Test for issue6123: Allow opening empty archives.\n# This test checks if tarfile.open() is able to open an empty tar\n# archive successfully. Note that an empty tar archive is not the\n# same as an empty file!\n", "func_signal": "def test_empty_tarfile(self):\n", "code": "tarfile.open(tmpname, self.mode.replace(\"r\", \"w\")).close()\ntry:\n    tar = tarfile.open(tmpname, self.mode)\n    tar.getnames()\nexcept tarfile.ReadError:\n    self.fail(\"tarfile.open() failed on empty archive\")\nself.assertListEqual(tar.getmembers(), [])", "path": "python\\Lib\\test\\test_tarfile.py", "repo_name": "smart-mobile-software/gitstack", "stars": 124, "license": "other", "language": "python", "size": 94495}
{"docstring": "# Test old style dirtype member (bug #1336623):\n# Old V7 tars create directory members using an AREGTYPE\n# header with a \"/\" appended to the filename field.\n", "func_signal": "def test_v7_dirtype(self):\n", "code": "tarinfo = self.tar.getmember(\"misc/dirtype-old-v7\")\nself.assertTrue(tarinfo.type == tarfile.DIRTYPE,\n        \"v7 dirtype failed\")", "path": "python\\Lib\\test\\test_tarfile.py", "repo_name": "smart-mobile-software/gitstack", "stars": 124, "license": "other", "language": "python", "size": 94495}
{"docstring": "# The __enter__() method is supposed to raise IOError\n# if the TarFile object is already closed.\n", "func_signal": "def test_closed(self):\n", "code": "tar = tarfile.open(tarname)\ntar.close()\nwith self.assertRaises(IOError):\n    with tar:\n        pass", "path": "python\\Lib\\test\\test_tarfile.py", "repo_name": "smart-mobile-software/gitstack", "stars": 124, "license": "other", "language": "python", "size": 94495}
{"docstring": "# Test if extractall works properly when tarfile contains symlinks\n", "func_signal": "def test_extractall_hardlinks(self):\n", "code": "tempdir = os.path.join(TEMPDIR, \"testsymlinks\")\ntemparchive = os.path.join(TEMPDIR, \"testsymlinks.tar\")\nos.mkdir(tempdir)\ntry:\n    source_file = os.path.join(tempdir,'source')\n    target_file = os.path.join(tempdir,'symlink')\n    with open(source_file,'w') as f:\n        f.write('something\\n')\n    os.link(source_file, target_file)\n    tar = tarfile.open(temparchive,'w')\n    tar.add(source_file, arcname=os.path.basename(source_file))\n    tar.add(target_file, arcname=os.path.basename(target_file))\n    tar.close()\n    # Let's extract it to the location which contains the symlink\n    tar = tarfile.open(temparchive,'r')\n    # this should not raise OSError: [Errno 17] File exists\n    try:\n        tar.extractall(path=tempdir)\n    except OSError:\n        self.fail(\"extractall failed with linked files\")\n    finally:\n        tar.close()\nfinally:\n    os.unlink(temparchive)\n    shutil.rmtree(tempdir)", "path": "python\\Lib\\test\\test_tarfile.py", "repo_name": "smart-mobile-software/gitstack", "stars": 124, "license": "other", "language": "python", "size": 94495}
{"docstring": "\"\"\"\nRender a given template with any extra URL parameters in the context as\n``{{ params }}``.\n\"\"\"\n", "func_signal": "def direct_to_template(request, template, extra_context=None, mimetype=None, **kwargs):\n", "code": "if extra_context is None: extra_context = {}\ndictionary = {'params': kwargs}\nfor key, value in extra_context.items():\n    if callable(value):\n        dictionary[key] = value()\n    else:\n        dictionary[key] = value\nc = RequestContext(request, dictionary)\nt = loader.get_template(template)\nreturn HttpResponse(t.render(c), content_type=mimetype)", "path": "python\\Lib\\site-packages\\django\\views\\generic\\simple.py", "repo_name": "smart-mobile-software/gitstack", "stars": 124, "license": "other", "language": "python", "size": 94495}
{"docstring": "# Test reading of longname (bug #1471427).\n", "func_signal": "def test_read_longname(self):\n", "code": "longname = self.subdir + \"/\" + \"123/\" * 125 + \"longname\"\ntry:\n    tarinfo = self.tar.getmember(longname)\nexcept KeyError:\n    self.fail(\"longname not found\")\nself.assertTrue(tarinfo.type != tarfile.DIRTYPE, \"read longname as dirtype\")", "path": "python\\Lib\\test\\test_tarfile.py", "repo_name": "smart-mobile-software/gitstack", "stars": 124, "license": "other", "language": "python", "size": 94495}
{"docstring": "# The same name will be added as a REGTYPE every\n# time regardless of st_nlink.\n", "func_signal": "def test_add_twice(self):\n", "code": "tarinfo = self.tar.gettarinfo(self.foo)\nself.assertTrue(tarinfo.type == tarfile.REGTYPE,\n        \"add file as regular failed\")", "path": "python\\Lib\\test\\test_tarfile.py", "repo_name": "smart-mobile-software/gitstack", "stars": 124, "license": "other", "language": "python", "size": 94495}
{"docstring": "# Test TarFile's ignore_zeros option.\n", "func_signal": "def test_ignore_zeros(self):\n", "code": "if self.mode.endswith(\":gz\"):\n    _open = gzip.GzipFile\nelif self.mode.endswith(\":bz2\"):\n    _open = bz2.BZ2File\nelse:\n    _open = open\n\nfor char in ('\\0', 'a'):\n    # Test if EOFHeaderError ('\\0') and InvalidHeaderError ('a')\n    # are ignored correctly.\n    fobj = _open(tmpname, \"wb\")\n    fobj.write(char * 1024)\n    fobj.write(tarfile.TarInfo(\"foo\").tobuf())\n    fobj.close()\n\n    tar = tarfile.open(tmpname, mode=\"r\", ignore_zeros=True)\n    self.assertListEqual(tar.getnames(), [\"foo\"],\n            \"ignore_zeros=True should have skipped the %r-blocks\" % char)\n    tar.close()", "path": "python\\Lib\\test\\test_tarfile.py", "repo_name": "smart-mobile-software/gitstack", "stars": 124, "license": "other", "language": "python", "size": 94495}
{"docstring": "# Create a pathname that has one component representable using\n# iso8859-1 and the other only in iso8859-15.\n", "func_signal": "def test_error_handler_utf8(self):\n", "code": "self._create_unicode_name(u\"/\")\n\ntar = tarfile.open(tmpname, format=self.format, encoding=\"iso8859-1\",\n        errors=\"utf-8\")\nself.assertEqual(tar.getnames()[0], \"/\" + u\"\".encode(\"utf8\"))", "path": "python\\Lib\\test\\test_tarfile.py", "repo_name": "smart-mobile-software/gitstack", "stars": 124, "license": "other", "language": "python", "size": 94495}
{"docstring": "\"\"\"\nObtain the limit with the given UUID.  Ensures that the\ncurrent limit list is loaded.\n\n:param key: The UUID of the desired limit.\n\"\"\"\n\n", "func_signal": "def __getitem__(self, key):\n", "code": "self.recheck_limits()\nreturn self.limit_map[key]", "path": "turnstile\\compactor.py", "repo_name": "klmitch/turnstile", "stars": 104, "license": "apache-2.0", "language": "python", "size": 871}
{"docstring": "\"\"\"\nGet a bucket key to compact.  If none are available, returns\nNone.  This uses a configured lock to ensure that the bucket\nkey is popped off the sorted set in an atomic fashion.\n\n:param now: The current time, as a float.  Used to ensure the\n            bucket key has been aged sufficiently to be\n            quiescent.\n\n:returns: A bucket key ready for compaction, or None if no\n          bucket keys are available or none have aged\n          sufficiently.\n\"\"\"\n\n", "func_signal": "def get(self, now):\n", "code": "with self.lock:\n    items = self.db.zrangebyscore(self.key, 0, now - self.min_age,\n                                  start=0, num=1)\n    # Did we get any items?\n    if not items:\n        return None\n\n    # Drop the item we got\n    item = items[0]\n    self.db.zrem(item)\n\n    return item", "path": "turnstile\\compactor.py", "repo_name": "klmitch/turnstile", "stars": 104, "license": "apache-2.0", "language": "python", "size": 871}
{"docstring": "\"\"\"\nInitialize a LimitContainer.  This sets up an appropriate\ncontrol daemon, as well as providing a container for the\nlimits themselves.\n\n:param conf: A turnstile.config.Config instance containing the\n             configuration for the ControlDaemon.\n:param db: A database handle for the Redis database.\n\"\"\"\n\n", "func_signal": "def __init__(self, conf, db):\n", "code": "self.conf = conf\nself.db = db\nself.limits = []\nself.limit_map = {}\nself.limit_sum = None\n\n# Initialize the control daemon\nif conf.to_bool(conf['control'].get('remote', 'no'), False):\n    self.control_daemon = remote.RemoteControlDaemon(self, conf)\nelse:\n    self.control_daemon = control.ControlDaemon(self, conf)\n\n# Now start the control daemon\nself.control_daemon.start()", "path": "turnstile\\compactor.py", "repo_name": "klmitch/turnstile", "stars": 104, "license": "apache-2.0", "language": "python", "size": 871}
{"docstring": "\"\"\"\nRe-check that the cached limits are the current limits.\n\"\"\"\n\n", "func_signal": "def recheck_limits(self):\n", "code": "limit_data = self.control_daemon.get_limits()\n\ntry:\n    # Get the new checksum and list of limits\n    new_sum, new_limits = limit_data.get_limits(self.limit_sum)\n\n    # Convert the limits list into a list of objects\n    lims = database.limits_hydrate(self.db, new_limits)\n\n    # Save the new data\n    self.limits = lims\n    self.limit_map = dict((lim.uuid, lim) for lim in lims)\n    self.limit_sum = new_sum\nexcept control.NoChangeException:\n    # No changes to process; just keep going...\n    return\nexcept Exception:\n    # Log an error\n    LOG.exception(\"Could not load limits\")\n\n    # Get our error set and publish channel\n    control_args = self.conf['control']\n    error_key = control_args.get('errors_key', 'errors')\n    error_channel = control_args.get('errors_channel', 'errors')\n\n    # Get an informative message\n    msg = \"Failed to load limits: \" + traceback.format_exc()\n\n    # Store the message into the error set.  We use a set\n    # here because it's likely that more than one node\n    # will generate the same message if there is an error,\n    # and this avoids an explosion in the size of the set.\n    with utils.ignore_except():\n        self.db.sadd(error_key, msg)\n\n    # Publish the message to a channel\n    with utils.ignore_except():\n        self.db.publish(error_channel, msg)", "path": "turnstile\\compactor.py", "repo_name": "klmitch/turnstile", "stars": 104, "license": "apache-2.0", "language": "python", "size": 871}
{"docstring": "\"\"\"\nGet a bucket key to compact.  If none are available, returns\nNone.  This uses a Lua script to ensure that the bucket key is\npopped off the sorted set in an atomic fashion.\n\n:param now: The current time, as a float.  Used to ensure the\n            bucket key has been aged sufficiently to be\n            quiescent.\n\n:returns: A bucket key ready for compaction, or None if no\n          bucket keys are available or none have aged\n          sufficiently.\n\"\"\"\n\n", "func_signal": "def get(self, now):\n", "code": "items = self.script(keys=[self.key], args=[now - self.min_age])\nreturn items[0] if items else None", "path": "turnstile\\compactor.py", "repo_name": "klmitch/turnstile", "stars": 104, "license": "apache-2.0", "language": "python", "size": 871}
{"docstring": "\"\"\"\nGiven a configuration and database, select and return an\nappropriate instance of a subclass of GetBucketKey.  This will\nensure that both client and server support are available for\nthe Lua script feature of Redis, and if not, a lock will be\nused.\n\n:param config: A dictionary of compactor options.\n:param db: A database handle for the Redis database.\n\n:returns: An instance of a subclass of GetBucketKey, dependent\n          on the support for the Lua script feature of Redis.\n\"\"\"\n\n# Make sure that the client supports register_script()\n", "func_signal": "def factory(cls, config, db):\n", "code": "if not hasattr(db, 'register_script'):\n    LOG.debug(\"Redis client does not support register_script()\")\n    return GetBucketKeyByLock(config, db)\n\n# OK, the client supports register_script(); what about the\n# server?\ninfo = db.info()\nif version_greater('2.6', info['redis_version']):\n    LOG.debug(\"Redis server supports register_script()\")\n    return GetBucketKeyByScript(config, db)\n\n# OK, use our fallback...\nLOG.debug(\"Redis server does not support register_script()\")\nreturn GetBucketKeyByLock(config, db)", "path": "turnstile\\compactor.py", "repo_name": "klmitch/turnstile", "stars": 104, "license": "apache-2.0", "language": "python", "size": 871}
{"docstring": "\"\"\"\nRetrieve the next bucket key to compact.  If no buckets are\navailable for compacting, sleeps for a given period of time\nand tries again.\n\n:returns: The bucket key to compact.\n\"\"\"\n\n", "func_signal": "def __call__(self):\n", "code": "while True:\n    now = time.time()\n\n    # Drop all items older than max_age; they're no longer\n    # quiesced, since the compactor logic will cause new\n    # summarize records to be generated.  No lock is needed...\n    self.db.zremrangebyscore(self.key, 0, now - self.max_age)\n\n    # Get an item and return it\n    item = self.get(now)\n    if item:\n        LOG.debug(\"Next bucket to compact: %s\" % item)\n        return item\n\n    # If we didn't get one, idle\n    LOG.debug(\"No buckets to compact; sleeping for %s seconds\" %\n              self.idle_sleep)\n    time.sleep(self.idle_sleep)", "path": "turnstile\\compactor.py", "repo_name": "klmitch/turnstile", "stars": 104, "license": "apache-2.0", "language": "python", "size": 871}
{"docstring": "\"\"\"\nA helper to retrieve an integer value from a given dictionary\ncontaining string values.  If the requested value is not present\nin the dictionary, or if it cannot be converted to an integer, a\ndefault value will be returned instead.\n\n:param config: The dictionary containing the desired value.\n:param key: The dictionary key for the desired value.\n:param default: The default value to return, if the key isn't set\n                in the dictionary, or if the value set isn't a\n                legal integer value.\n\n:returns: The desired integer value.\n\"\"\"\n\n", "func_signal": "def get_int(config, key, default):\n", "code": "try:\n    return int(config[key])\nexcept (KeyError, ValueError):\n    return default", "path": "turnstile\\compactor.py", "repo_name": "klmitch/turnstile", "stars": 104, "license": "apache-2.0", "language": "python", "size": 871}
{"docstring": "\"\"\"\nPerform the compaction operation.  This reads in the bucket\ninformation from the database, builds a compacted bucket record,\ninserts that record in the appropriate place in the database, then\nremoves outdated updates.\n\n:param db: A database handle for the Redis database.\n:param buck_key: A turnstile.limits.BucketKey instance containing\n                 the bucket key.\n:param limit: The turnstile.limits.Limit object corresponding to\n              the bucket.\n\"\"\"\n\n# Suck in the bucket records and generate our bucket\n", "func_signal": "def compact_bucket(db, buck_key, limit):\n", "code": "records = db.lrange(str(buck_key), 0, -1)\nloader = limits.BucketLoader(limit.bucket_class, db, limit,\n                             str(buck_key), records, stop_summarize=True)\n\n# We now have the bucket loaded in; generate a 'bucket' record\nbuck_record = msgpack.dumps(dict(bucket=loader.bucket.dehydrate(),\n                                 uuid=str(uuid.uuid4())))\n\n# Now we need to insert it into the record list\nresult = db.linsert(str(buck_key), 'after', loader.last_summarize_rec,\n                    buck_record)\n\n# Were we successful?\nif result < 0:\n    # Insert failed; we'll try again when max_age is hit\n    LOG.warning(\"Bucket compaction on %s failed; will retry\" % buck_key)\n    return\n\n# OK, we have confirmed that the compacted bucket record has been\n# inserted correctly; now all we need to do is trim off the\n# outdated update records\ndb.ltrim(str(buck_key), loader.last_summarize_idx + 1, -1)", "path": "turnstile\\compactor.py", "repo_name": "klmitch/turnstile", "stars": 104, "license": "apache-2.0", "language": "python", "size": 871}
{"docstring": "\"\"\"\nUtility function to issue a command to all Turnstile instances.\n\n:param db: The database handle.\n:param channel: The control channel all Turnstile instances are\n                listening on.\n:param command: The command, as plain text.  Currently, only\n                'reload' and 'ping' are recognized.\n\nAll remaining arguments are treated as arguments for the command;\nthey will be stringified and sent along with the command to the\ncontrol channel.  Note that ':' is an illegal character in\narguments, but no warnings will be issued if it is used.\n\"\"\"\n\n# Build the command we're sending\n", "func_signal": "def command(db, channel, command, *args):\n", "code": "cmd = [command]\ncmd.extend(str(a) for a in args)\n\n# Send it out\ndb.publish(channel, ':'.join(cmd))", "path": "turnstile\\database.py", "repo_name": "klmitch/turnstile", "stars": 104, "license": "apache-2.0", "language": "python", "size": 871}
{"docstring": "\"\"\"\nAdds tag and index to the path; they will be popped off when\nthe corresponding 'with' statement exits.\n\n:param tag: The element tag\n:param idx: If not None, the integer index of the element\n            within its parent.  Not included in the path\n            element if None.\n\"\"\"\n\n", "func_signal": "def node(self, tag, idx):\n", "code": "if idx is not None:\n    self.path.append(\"%s[%d]\" % (tag, idx))\nelse:\n    self.path.append(tag)\nreturn self", "path": "tests\\unit\\utils.py", "repo_name": "klmitch/turnstile", "stars": 104, "license": "apache-2.0", "language": "python", "size": 871}
{"docstring": "\"\"\"\nThe compactor daemon.  This fuction watches the sorted set\ncontaining bucket keys that need to be compacted, performing the\nnecessary compaction.\n\n:param conf: A turnstile.config.Config instance containing the\n             configuration for the compactor daemon.  Note that a\n             ControlDaemon is also started, so appropriate\n             configuration for that must also be present, as must\n             appropriate Redis connection information.\n\"\"\"\n\n# Get the database handle\n", "func_signal": "def compactor(conf):\n", "code": "db = conf.get_database('compactor')\n\n# Get the limits container\nlimit_map = LimitContainer(conf, db)\n\n# Get the compactor configuration\nconfig = conf['compactor']\n\n# Make sure compaction is enabled\nif get_int(config, 'max_updates', 0) <= 0:\n    # We'll just warn about it, since they could be running\n    # the compactor with a different configuration file\n    LOG.warning(\"Compaction is not enabled.  Enable it by \"\n                \"setting a positive integer value for \"\n                \"'compactor.max_updates' in the configuration.\")\n\n# Select the bucket key getter\nkey_getter = GetBucketKey.factory(config, db)\n\nLOG.info(\"Compactor initialized\")\n\n# Now enter our loop\nwhile True:\n    # Get a bucket key to compact\n    try:\n        buck_key = limits.BucketKey.decode(key_getter())\n    except ValueError as exc:\n        # Warn about invalid bucket keys\n        LOG.warning(\"Error interpreting bucket key: %s\" % exc)\n        continue\n\n    # Ignore version 1 keys--they can't be compacted\n    if buck_key.version < 2:\n        continue\n\n    # Get the corresponding limit class\n    try:\n        limit = limit_map[buck_key.uuid]\n    except KeyError:\n        # Warn about missing limits\n        LOG.warning(\"Unable to compact bucket for limit %s\" %\n                    buck_key.uuid)\n        continue\n\n    LOG.debug(\"Compacting bucket %s\" % buck_key)\n\n    # OK, we now have the limit (which we really only need for\n    # the bucket class); let's compact the bucket\n    try:\n        compact_bucket(db, buck_key, limit)\n    except Exception:\n        LOG.exception(\"Failed to compact bucket %s\" % buck_key)\n    else:\n        LOG.debug(\"Finished compacting bucket %s\" % buck_key)", "path": "turnstile\\compactor.py", "repo_name": "klmitch/turnstile", "stars": 104, "license": "apache-2.0", "language": "python", "size": 871}
{"docstring": "\"\"\"\nRetrieve the given configuration option.  Configuration\noptions that can be queried this way are those that are\nspecified without prefix in the paste.ini file, or which are\nspecified in the '[turnstile]' section of the configuration\nfile.  Raises an AttributeError if the given option does not\nexist.\n\"\"\"\n\n", "func_signal": "def __getattr__(self, key):\n", "code": "try:\n    return self._config.get(None, {})[key]\nexcept KeyError:\n    raise AttributeError('%r object has no attribute %r' %\n                         (self.__class__.__name__, key))", "path": "turnstile\\config.py", "repo_name": "klmitch/turnstile", "stars": 104, "license": "apache-2.0", "language": "python", "size": 871}
{"docstring": "\"\"\"\nInitializes a Config object.  A default is provided for the\n\"status\" configuration.\n\n:param conf_dict: Optional.  Should specify a dictionary\n                  containing the configuration drawn from the\n                  paste.ini file.  If a 'config' key is\n                  present in the dict, configuration will\n                  additionally be drawn from the specified INI\n                  file; configuration from the INI file will\n                  override configuration drawn from this dict.\n:param conf_file: Optional.  Should specify the name of a file\n                  containing further configuration.  If a\n                  conf_dict is also provided, values drawn\n                  from this file will override values from the\n                  conf_dict, as well as any additional file\n                  specified by the 'config' key.\n\nFor configuration files, values in the '[turnstile]' section\ncorrespond to prefix-less values in the dictionary, with the\nexception that the 'config' value is ignored.\n\"\"\"\n\n", "func_signal": "def __init__(self, conf_dict=None, conf_file=None):\n", "code": "self._config = {\n    None: {\n        'status': '413 Request Entity Too Large',\n    },\n}\n\n# Handle passed-in dict (middleware)\nif conf_dict:\n    for key, value in conf_dict.items():\n        outer, _sep, inner = key.partition('.')\n\n        # Deal with prefix-less keys\n        if not inner:\n            outer, inner = None, outer\n\n        # Make sure we have a place to put them\n        self._config.setdefault(outer, {})\n        self._config[outer][inner] = value\n\nconf_files = []\n\n# Were we to look aside to a configuration file?\nif 'config' in self._config[None]:\n    conf_files.append(self._config[None]['config'])\n\n# Were we asked to load a specific file in addition?\nif conf_file:\n    conf_files.append(conf_file)\n\n# Parse configuration files\nif conf_files:\n    cp = ConfigParser.SafeConfigParser()\n    cp.read(conf_files)\n\n    # Each section corresponds to a top-level in the config\n    for sect in cp.sections():\n        # Transform [turnstile] section\n        outer = None if sect == 'turnstile' else sect\n\n        self._config.setdefault(outer, {})\n\n        # Merge in the options from the section\n        self._config[outer].update(dict(cp.items(sect)))", "path": "turnstile\\config.py", "repo_name": "klmitch/turnstile", "stars": 104, "license": "apache-2.0", "language": "python", "size": 871}
{"docstring": "\"\"\"\nInitialize a connection to the Redis database.\n\"\"\"\n\n# Determine the client class to use\n", "func_signal": "def initialize(config):\n", "code": "if 'redis_client' in config:\n    client = utils.find_entrypoint('turnstile.redis_client',\n                                   config['redis_client'], required=True)\nelse:\n    client = redis.StrictRedis\n\n# Extract relevant connection information from the configuration\nkwargs = {}\nfor cfg_var, type_ in REDIS_CONFIGS.items():\n    if cfg_var in config:\n        kwargs[cfg_var] = type_(config[cfg_var])\n\n# Make sure we have at a minimum the hostname\nif 'host' not in kwargs and 'unix_socket_path' not in kwargs:\n    raise redis.ConnectionError(\"No host specified for redis database\")\n\n# Look up the connection pool configuration\ncpool_class = None\ncpool = {}\nextra_kwargs = {}\nfor key, value in config.items():\n    if key.startswith('connection_pool.'):\n        _dummy, _sep, varname = key.partition('.')\n        if varname == 'connection_class':\n            cpool[varname] = utils.find_entrypoint(\n                'turnstile.connection_class', value, required=True)\n        elif varname == 'max_connections':\n            cpool[varname] = int(value)\n        elif varname == 'parser_class':\n            cpool[varname] = utils.find_entrypoint(\n                'turnstile.parser_class', value, required=True)\n        else:\n            cpool[varname] = value\n    elif key not in REDIS_CONFIGS and key not in REDIS_EXCLUDES:\n        extra_kwargs[key] = value\nif cpool:\n    cpool_class = redis.ConnectionPool\n\n# Use custom connection pool class if requested...\nif 'connection_pool' in config:\n    cpool_class = utils.find_entrypoint('turnstile.connection_pool',\n                                        config['connection_pool'],\n                                        required=True)\n\n# If we're using a connection pool, we'll need to pass the keyword\n# arguments to that instead of to redis\nif cpool_class:\n    cpool.update(kwargs)\n\n    # Use a custom connection class?\n    if 'connection_class' not in cpool:\n        if 'unix_socket_path' in cpool:\n            if 'host' in cpool:\n                del cpool['host']\n            if 'port' in cpool:\n                del cpool['port']\n\n            cpool['path'] = cpool['unix_socket_path']\n            del cpool['unix_socket_path']\n\n            cpool['connection_class'] = redis.UnixDomainSocketConnection\n        else:\n            cpool['connection_class'] = redis.Connection\n\n    # Build the connection pool to use and set up to pass it into\n    # the redis constructor...\n    kwargs = dict(connection_pool=cpool_class(**cpool))\n\n# Build and return the database\nkwargs.update(extra_kwargs)\nreturn client(**kwargs)", "path": "turnstile\\database.py", "repo_name": "klmitch/turnstile", "stars": 104, "license": "apache-2.0", "language": "python", "size": 871}
{"docstring": "\"\"\"Compare two XML strings.\"\"\"\n\n", "func_signal": "def compare_xml(expected, actual):\n", "code": "expected = etree.fromstring(expected)\nif isinstance(actual, basestring):\n    actual = etree.fromstring(actual)\n\nstate = XMLMatchState()\nresult = _compare_node(expected, actual, state, None)\n\nif result is False:\n    raise AssertionError(\"%s: XML does not match\" % state)\nelif result is not True:\n    return result", "path": "tests\\unit\\utils.py", "repo_name": "klmitch/turnstile", "stars": 104, "license": "apache-2.0", "language": "python", "size": 871}
{"docstring": "\"\"\"Recursively compares nodes within the XML tree.\"\"\"\n\n# Start by comparing the tags\n", "func_signal": "def _compare_node(expected, actual, state, idx):\n", "code": "if expected.tag != actual.tag:\n    raise AssertionError(\"s: XML tag mismatch at index %d: \"\n                         \"expected tag <%s>; actual tag <%s>\" %\n                         (state, idx, expected.tag, actual.tag))\n\nwith state.node(expected.tag, idx):\n    # Compare the attribute keys\n    expected_attrs = set(expected.attrib.keys())\n    actual_attrs = set(actual.attrib.keys())\n    if expected_attrs != actual_attrs:\n        expected = ', '.join(sorted(expected_attrs - actual_attrs))\n        actual = ', '.join(sorted(actual_attrs - expected_attrs))\n        raise AssertionError(\"%s: XML attributes mismatch: \"\n                             \"keys only in expected: %s; \"\n                             \"keys only in actual: %s\" %\n                             (state, expected, actual))\n\n    # Compare the attribute values\n    for key in expected_attrs:\n        expected_value = expected.attrib[key]\n        actual_value = actual.attrib[key]\n\n        if 'DONTCARE' in (expected_value, actual_value):\n            continue\n        elif expected_value != actual_value:\n            raise AssertionError(\"%s: XML attribute value mismatch: \"\n                                 \"expected value of attribute %s: %r; \"\n                                 \"actual value: %r\" %\n                                 (state, key, expected_value,\n                                  actual_value))\n\n    # Compare the contents of the node\n    if len(expected) == 0 and len(actual) == 0:\n        # No children, compare text values\n        if ('DONTCARE' not in (expected.text, actual.text) and\n                expected.text != actual.text):\n            raise AssertionError(\"%s: XML text value mismatch: \"\n                                 \"expected text value: %r; \"\n                                 \"actual value: %r\" %\n                                 (state, expected.text, actual.text))\n    else:\n        expected_idx = 0\n        actual_idx = 0\n        while (expected_idx < len(expected) and\n               actual_idx < len(actual)):\n            # Ignore comments and processing instructions\n            # TODO(Vek): may interpret PIs in the future, to\n            # allow for, say, arbitrary ordering of some\n            # elements\n            if (expected[expected_idx].tag in\n                    (etree.Comment, etree.ProcessingInstruction)):\n                expected_idx += 1\n                continue\n\n            # Compare the nodes\n            result = _compare_node(expected[expected_idx],\n                                   actual[actual_idx], state,\n                                   actual_idx)\n            if result is not True:\n                return result\n\n            # Step on to comparing the next nodes...\n            expected_idx += 1\n            actual_idx += 1\n\n        # Make sure we consumed all nodes in actual\n        if actual_idx < len(actual):\n            raise AssertionError(\"%s: XML unexpected child element \"\n                                 \"<%s> present at index %d\" %\n                                 (state, actual[actual_idx].tag,\n                                  actual_idx))\n\n        # Make sure we consumed all nodes in expected\n        if expected_idx < len(expected):\n            for node in expected[expected_idx:]:\n                if (node.tag in\n                        (etree.Comment, etree.ProcessingInstruction)):\n                    continue\n\n                raise AssertionError(\"%s: XML expected child element \"\n                                     \"<%s> not present at index %d\" %\n                                     (state, node.tag, actual_idx))\n\n# The nodes match\nreturn True", "path": "tests\\unit\\utils.py", "repo_name": "klmitch/turnstile", "stars": 104, "license": "apache-2.0", "language": "python", "size": 871}
{"docstring": "\"\"\"\nConvenience function for obtaining a handle to the Redis\ndatabase.  By default, uses the connection options from the\n'[redis]' section.  However, if the override parameter is\ngiven, it specifies a section containing overrides for the\nRedis connection info; the keys will all be prefixed with\n'redis.'.  For example, in the following configuration file:\n\n    [redis]\n    host = 10.0.0.1\n    password = s3cureM3!\n\n    [control]\n    redis.host = 127.0.0.1\n\nA call to get_database() would return a handle for the redis\ndatabase on 10.0.0.1, while a call to get_database('control')\nwould return a handle for the redis database on 127.0.0.1; in\nboth cases, the database password would be 's3cureM3!'.\n\"\"\"\n\n# Grab the database connection arguments\n", "func_signal": "def get_database(self, override=None):\n", "code": "redis_args = self['redis']\n\n# If we have an override, read some overrides from that\n# section\nif override:\n    redis_args = redis_args.copy()\n    for key, value in self[override].items():\n        if not key.startswith('redis.'):\n            continue\n        key = key[len('redis.'):]\n        if value:\n            redis_args[key] = value\n        else:\n            redis_args.pop(key, None)\n\n# Return the redis database connection\nreturn database.initialize(redis_args)", "path": "turnstile\\config.py", "repo_name": "klmitch/turnstile", "stars": 104, "license": "apache-2.0", "language": "python", "size": 871}
{"docstring": "\"\"\"\nInitialize a GetBucketKey instance.\n\n:param config: A dictionary of compactor options.\n:param db: A database handle for the Redis database.\n\"\"\"\n\n", "func_signal": "def __init__(self, config, db):\n", "code": "self.db = db\nself.key = config.get('compactor_key', 'compactor')\nself.max_age = get_int(config, 'max_age', 600)\nself.min_age = get_int(config, 'min_age', 30)\nself.idle_sleep = get_int(config, 'sleep', 5)", "path": "turnstile\\compactor.py", "repo_name": "klmitch/turnstile", "stars": 104, "license": "apache-2.0", "language": "python", "size": 871}
{"docstring": "\"\"\"\nInitialize a GetBucketKeyByLock instance.\n\n:param config: A dictionary of compactor options.\n:param db: A database handle for the Redis database.\n\"\"\"\n\n", "func_signal": "def __init__(self, config, db):\n", "code": "super(GetBucketKeyByLock, self).__init__(config, db)\n\nlock_key = config.get('compactor_lock', 'compactor_lock')\ntimeout = get_int(config, 'compactor_timeout', 30)\nself.lock = db.lock(lock_key, timeout=timeout)\n\nLOG.debug(\"Using GetBucketKeyByLock as bucket key getter\")", "path": "turnstile\\compactor.py", "repo_name": "klmitch/turnstile", "stars": 104, "license": "apache-2.0", "language": "python", "size": 871}
{"docstring": "\"\"\"\n    Called when a device has been added to the system. If the device\n    is a volume with a video DVD the \"video-found\" signal is emitted.\n\"\"\"\n", "func_signal": "def device_added(self, udi):\n", "code": "dev_obj = self.bus.get_object(\"org.freedesktop.Hal\", udi)\ndev = dbus.Interface(dev_obj, \"org.freedesktop.Hal.Device\")\nif dev.PropertyExists(\"block.device\"):\n    block = dev.GetProperty(\"block.device\")\n    if self.drives.has_key(block):\n        if dev.PropertyExists(\"volume.disc.is_videodvd\"):\n            if dev.GetProperty(\"volume.disc.is_videodvd\"):\n                label = dev.GetProperty(\"volume.label\")\n                self.drives[block].video = True\n                self.drives[block].video_udi = udi\n                self.drives[block].label = label\n                self.emit(\"disc-found\", self.drives[block], label)\nelif dev.PropertyExists(\"video4linux.device\"):\n    device = dev.GetProperty(\"video4linux.device\")\n    capture_device = V4LDevice(udi, dev)\n    self.capture_devices[device] = capture_device\n    self.emit(\"v4l-capture-found\", capture_device)", "path": "arista\\inputs\\haldisco.py", "repo_name": "danielgtaylor/arista", "stars": 121, "license": "lgpl-2.1", "language": "python", "size": 3427}
{"docstring": "\"\"\"\ntransform a value in nanoseconds into a human-readable string\n\"\"\"\n", "func_signal": "def _time_to_string(self, value):\n", "code": "ms = value / gst.MSECOND\nsec = ms / 1000\nms = ms % 1000\nmin = sec / 60\nsec = sec % 60\nreturn \"%2dm %2ds %3d\" % (min, sec, ms)", "path": "arista\\discoverer.py", "repo_name": "danielgtaylor/arista", "stars": 121, "license": "lgpl-2.1", "language": "python", "size": 3427}
{"docstring": "\"\"\"\n    Handle a udev event.\n\"\"\"\n", "func_signal": "def event(self, client, action, device):\n", "code": "return {\n    \"add\": self.device_added,\n    \"change\": self.device_changed,\n    \"remove\": self.device_removed,\n}.get(action, lambda x,y: None)(device, device.get_subsystem())", "path": "arista\\inputs\\udevdisco.py", "repo_name": "danielgtaylor/arista", "stars": 121, "license": "lgpl-2.1", "language": "python", "size": 3427}
{"docstring": "\"\"\"\n    Create a new DVDFinder and attach to the DBus system bus to find\n    device information through HAL.\n\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self.__gobject_init__()\nself.bus = dbus.SystemBus()\nself.hal_obj = self.bus.get_object(\"org.freedesktop.Hal\",\n                                   \"/org/freedesktop/Hal/Manager\")\nself.hal = dbus.Interface(self.hal_obj, \"org.freedesktop.Hal.Manager\")\n\nself.drives = {}\nself.capture_devices = {}\n\nudis = self.hal.FindDeviceByCapability(\"storage.cdrom\")\nfor udi in udis:\n    dev_obj = self.bus.get_object(\"org.freedesktop.Hal\", udi)\n    dev = dbus.Interface(dev_obj, \"org.freedesktop.Hal.Device\")\n    if dev.GetProperty(\"storage.cdrom.dvd\"):\n        #print \"Found DVD drive!\"\n        block = dev.GetProperty(\"block.device\")\n        self.drives[block] = DVDDevice(udi, dev)\n\nudis = self.hal.FindDeviceByCapability(\"volume.disc\")\nfor udi in udis:\n    dev_obj = self.bus.get_object(\"org.freedesktop.Hal\", udi)\n    dev = dbus.Interface(dev_obj, \"org.freedesktop.Hal.Device\")\n    if dev.PropertyExists(\"volume.disc.is_videodvd\"):\n        if dev.GetProperty(\"volume.disc.is_videodvd\"):\n            block = dev.GetProperty(\"block.device\")\n            label = dev.GetProperty(\"volume.label\")\n            if self.drives.has_key(block):\n                self.drives[block].video = True\n                self.drives[block].video_udi = udi\n                self.drives[block].label = label\n\nudis = self.hal.FindDeviceByCapability(\"video4linux\")\nfor udi in udis:\n    dev_obj = self.bus.get_object(\"org.freedesktop.Hal\", udi)\n    dev = dbus.Interface(dev_obj, \"org.freedesktop.Hal.Device\")\n    if dev.QueryCapability(\"video4linux.video_capture\"):\n        device = dev.GetProperty(\"video4linux.device\")\n        self.capture_devices[device] = V4LDevice(udi, dev)\n\nself.hal.connect_to_signal(\"DeviceAdded\", self.device_added)\nself.hal.connect_to_signal(\"DeviceRemoved\", self.device_removed)", "path": "arista\\inputs\\haldisco.py", "repo_name": "danielgtaylor/arista", "stars": 121, "license": "lgpl-2.1", "language": "python", "size": 3427}
{"docstring": "\"\"\"\n    Called when a device has changed. If the change represents a disc\n    being inserted or removed, fire the disc-found or disc-lost signals\n    respectively.\n\"\"\"\n", "func_signal": "def device_changed(self, device, subsystem):\n", "code": "if subsystem == \"block\" and device.has_property(\"ID_CDROM\"):\n    block = device.get_device_file()\n    dvd_device = self.drives[block]\n    media_changed = dvd_device.media != device.has_property(\"ID_FS_TYPE\")\n    dvd_device.device = device\n    if media_changed:\n        if dvd_device.media:\n            self.emit(\"disc-found\", dvd_device, dvd_device.nice_label)\n        else:\n            self.emit(\"disc-lost\", dvd_device, dvd_device.nice_label)", "path": "arista\\inputs\\udevdisco.py", "repo_name": "danielgtaylor/arista", "stars": 121, "license": "lgpl-2.1", "language": "python", "size": 3427}
{"docstring": "\"\"\"\n    Get a list of paths that are searched for installed resources.\n    \n    @rtype: list\n    @return: A list of paths to search in the order they will be searched\n\"\"\"\n", "func_signal": "def get_search_paths():\n", "code": "return [\n    # Current path, useful for development:\n    os.getcwd(),\n    # User home directory:\n    os.path.expanduser(os.path.join(\"~\", \".arista\")),\n    # User-installed:\n    os.path.join(sys.prefix, \"local\", \"share\", \"arista\"),\n    # System-installed:\n    os.path.join(sys.prefix, \"share\", \"arista\"),\n    # The following allows stuff like virtualenv to work!\n    os.path.join(os.path.join(os.path.dirname(os.path.dirname(__file__)), \"share\", \"arista\")),\n]", "path": "arista\\utils.py", "repo_name": "danielgtaylor/arista", "stars": 121, "license": "lgpl-2.1", "language": "python", "size": 3427}
{"docstring": "\"\"\"\n    Get a nice label that looks like \"The Big Lebowski\" if a video\n    disk is found, otherwise the model name.\n    \n    @type label: string\n    @param label: Use this label instead of the disk label.\n    @rtype: string\n    @return: The nicely formatted label.\n\"\"\"\n", "func_signal": "def nice_label(self, label=None):\n", "code": "if not label:\n    label = self.label\n    \nif label:\n    words = [word.capitalize() for word in label.split(\"_\")]\n    return \" \".join(words)\nelse:\n    return self.product", "path": "arista\\inputs\\haldisco.py", "repo_name": "danielgtaylor/arista", "stars": 121, "license": "lgpl-2.1", "language": "python", "size": 3427}
{"docstring": "\"\"\"Find the information on the given file asynchronously\"\"\"\n", "func_signal": "def discover(self):\n", "code": "_log.debug(_(\"Discovering %(filename)s\") % {\n    \"filename\": self.filename\n})\nself.debug(\"starting discovery\")\nif self.finished:\n    self.emit('discovered', False)\n    return\n\nself.bus = self.get_bus()\nself.bus.add_signal_watch()\nself.bus.connect(\"message\", self._bus_message_cb)\n\n# 3s timeout\nself._timeoutid = gobject.timeout_add(3000, self._timed_out_or_eos)\n\nself.info(\"setting to PLAY\")\nif not self.set_state(gst.STATE_PLAYING):\n    self._finished()", "path": "arista\\discoverer.py", "repo_name": "danielgtaylor/arista", "stars": 121, "license": "lgpl-2.1", "language": "python", "size": 3427}
{"docstring": "\"\"\"\n    Get the video4linux version of this device.\n\"\"\"\n", "func_signal": "def version(self):\n", "code": "if self.device.has_property(\"ID_V4L_VERSION\"):\n    return self.device.get_property(\"ID_V4L_VERSION\")\nelse:\n    # Default to version 2\n    return \"2\"", "path": "arista\\inputs\\udevdisco.py", "repo_name": "danielgtaylor/arista", "stars": 121, "license": "lgpl-2.1", "language": "python", "size": 3427}
{"docstring": "\"\"\"\n   Get a human-friendly time description.\n\"\"\"\n", "func_signal": "def get_friendly_time(seconds):\n", "code": "hours = seconds / (60 * 60)\nseconds = seconds % (60 * 60)\nminutes = seconds / 60\nseconds = seconds % 60\n\nreturn \"%(hours)02d:%(minutes)02d:%(seconds)02d\" % {\n   \"hours\": hours,\n   \"minutes\": minutes,\n   \"seconds\": seconds,\n}", "path": "arista\\utils.py", "repo_name": "danielgtaylor/arista", "stars": 121, "license": "lgpl-2.1", "language": "python", "size": 3427}
{"docstring": "\"\"\"\n    Initialize the arista module. You MUST call this method after\n    importing.\n\"\"\"\n", "func_signal": "def init():\n", "code": "import discoverer\nimport dvd\nimport inputs\nimport presets\nimport queue\nimport transcoder\nimport utils", "path": "arista\\__init__.py", "repo_name": "danielgtaylor/arista", "stars": 121, "license": "lgpl-2.1", "language": "python", "size": 3427}
{"docstring": "\"\"\"\n    Create a new input device.\n    \n    @type udi: string\n    @param udi: The HAL device identifier for this device.\n    @type interface: dbus.Interface\n    @param interface: The Hal.Device DBus interface for this device.\n\"\"\"\n", "func_signal": "def __init__(self, udi, interface):\n", "code": "self.udi = udi\nself.interface = interface\nself.product = self.interface.GetProperty(\"info.product\")", "path": "arista\\inputs\\haldisco.py", "repo_name": "danielgtaylor/arista", "stars": 121, "license": "lgpl-2.1", "language": "python", "size": 3427}
{"docstring": "# Check if we have the info, if not, return and we will be called\n# again to check in 100ms.\n", "func_signal": "def run(self):\n", "code": "if self.proc.poll() is not None:\n    if self.proc.returncode == 0:\n        # TODO: is there a safer way to do this?\n        exec(self.proc.stdout.read())\n        self.emit(\"ready\", lsdvd)\n    \n    return False\n\nreturn True", "path": "arista\\dvd.py", "repo_name": "danielgtaylor/arista", "stars": 121, "license": "lgpl-2.1", "language": "python", "size": 3427}
{"docstring": "\"\"\"\n    Get a path, searching first in the current directory, then the user's\n    home directory, then sys.prefix, then sys.prefix + \"local\".\n    \n        >>> get_path(\"presets\", \"computer.json\")\n        '/usr/share/arista/presets/computer.json'\n        >>> get_path(\"presets\", \"my_cool_preset.json\")\n        '/home/dan/.arista/presets/my_cool_preset.json'\n    \n    @type parts: str\n    @param parts: The parts of the path to get that you would normally \n                  send to os.path.join\n    @type default: bool\n    @param default: A default value to return rather than raising IOError\n    @rtype: str\n    @return: The full path to the relative path passed in\n    @raise IOError: The path cannot be found in any location\n\"\"\"\n", "func_signal": "def get_path(*parts, **kwargs):\n", "code": "path = os.path.join(*parts)\n\nfor search in get_search_paths():\n    full = os.path.join(search, path)\n    if os.path.exists(full):\n        return full\nelse:\n    if \"default\" in kwargs:\n        return kwargs[\"default\"]\n        \n    raise IOError(_(\"Can't find %(path)s in any known prefix!\") % {\n        \"path\": path,\n    })", "path": "arista\\utils.py", "repo_name": "danielgtaylor/arista", "stars": 121, "license": "lgpl-2.1", "language": "python", "size": 3427}
{"docstring": "\"\"\"\n    Create a new DVDFinder and attach to the udev system to listen for\n    events.\n\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self.__gobject_init__()\n\nself.client = gudev.Client([\"video4linux\", \"block\"])\n\nself.drives = {}\nself.capture_devices = {}\n\nfor device in self.client.query_by_subsystem(\"video4linux\"):\n    block = device.get_device_file()\n    self.capture_devices[block] = V4LDevice(device)\n\nfor device in self.client.query_by_subsystem(\"block\"):\n    if device.has_property(\"ID_CDROM\"):\n        block = device.get_device_file()\n        self.drives[block] = DVDDevice(device)\n\nself.client.connect(\"uevent\", self.event)", "path": "arista\\inputs\\udevdisco.py", "repo_name": "danielgtaylor/arista", "stars": 121, "license": "lgpl-2.1", "language": "python", "size": 3427}
{"docstring": "\"\"\"\n    Get a path that can be written to. This uses the same logic as get_path\n    above, but instead of checking for the existence of a path it checks\n    to see if the current user has write accces.\n    \n        >>>> get_write_path(\"presets\", \"foo.json\")\n        '/home/dan/.arista/presets/foo.json'\n    \n    @type parts: str\n    @param parts: The parts of the path to get that you would normally \n                  send to os.path.join\n    @type default: bool\n    @param default: A default value to return rather than raising IOError\n    @rtype: str\n    @return: The full path to the relative path passed in\n    @raise IOError: The path cannot be written to in any location\n\"\"\"\n", "func_signal": "def get_write_path(*parts, **kwargs):\n", "code": "path = os.path.join(*parts)\n\nfor search in get_search_paths()[1:]:\n    full = os.path.join(search, path)\n    \n    # Find part of path that exists\n    test = full\n    while not os.path.exists(test):\n        test = os.path.dirname(test)\n    \n    if os.access(test, os.W_OK):\n        if not os.path.exists(os.path.dirname(full)):\n            os.makedirs(os.path.dirname(full))\n            \n        return full\nelse:\n    if \"default\" in kwargs:\n        return kwargs[\"default\"]\n    \n    raise IOError(_(\"Can't find %(path)s that can be written to!\") % {\n        \"path\": path,\n    })", "path": "arista\\utils.py", "repo_name": "danielgtaylor/arista", "stars": 121, "license": "lgpl-2.1", "language": "python", "size": 3427}
{"docstring": "\"\"\"\n    Called when a menu item is clicked. Start a transcode job for\n    the selected device and preset, and show the user the progress.\n\"\"\"\n", "func_signal": "def callback(self, menu, files, device_name, preset_name):\n", "code": "command = \"arista-gtk --simple -d %s -p \\\"%s\\\" %s &\" % (device_name, preset_name, \" \".join([\"\\\"%s\\\"\" % f for f in files]))\nos.system(command)", "path": "arista-nautilus.py", "repo_name": "danielgtaylor/arista", "stars": 121, "license": "lgpl-2.1", "language": "python", "size": 3427}
{"docstring": "# Does the file contain got audio or video ?\n", "func_signal": "def _new_decoded_pad_cb(self, dbin, pad, extra=None):\n", "code": "caps = pad.get_caps()\ngst.info(\"caps:%s\" % caps.to_string())\nif \"audio\" in caps.to_string():\n    self.is_audio = True\nelif \"video\" in caps.to_string():\n    self.is_video = True\nelse:\n    self.warning(\"got a different caps.. %s\" % caps.to_string())\n    return\n#if is_last and not self.is_video and not self.is_audio:\n#    self.debug(\"is last, not video or audio\")\n#    self._finished(False)\n#    return\n# we connect a fakesink to the new pad...\npad.info(\"adding queue->fakesink\")\nfakesink = gst.element_factory_make(\"fakesink\", \"fakesink%d-%s\" % \n    (self.sinknumber, \"audio\" in caps.to_string() and \"audio\" or \"video\"))\nself.sinknumber += 1\nqueue = gst.element_factory_make(\"queue\")\n# we want the queue to buffer up to the specified amount of data \n# before outputting. This enables us to cope with formats \n# that don't create their source pads straight away, \n# but instead wait for the first buffer of that stream.\n# The specified time must be greater than the input file\n# frame interleave for the discoverer to work properly.\nqueue.props.min_threshold_time = int(self._max_interleave * gst.SECOND)\nqueue.props.max_size_time = int(2 * self._max_interleave * gst.SECOND)\nqueue.props.max_size_bytes = 0\n\n# If durations are bad on the buffers (common for video decoders), we'll\n# never reach the min_threshold_time or max_size_time. So, set a\n# max size in buffers, and if reached, disable the min_threshold_time.\n# This ensures we don't fail to discover with various ffmpeg \n# demuxers/decoders that provide bogus (or no) duration.\nqueue.props.max_size_buffers = int(100 * self._max_interleave)\ndef _disable_min_threshold_cb(queue):\n    queue.props.min_threshold_time = 0\n    queue.disconnect(signal_id)\nsignal_id = queue.connect('overrun', _disable_min_threshold_cb)\n\nself.add(fakesink, queue)\nqueue.link(fakesink)\nsinkpad = fakesink.get_pad(\"sink\")\nqueuepad = queue.get_pad(\"sink\")\n# ... and connect a callback for when the caps are fixed\nsinkpad.connect(\"notify::caps\", self._notify_caps_cb)\nif pad.link(queuepad):\n    pad.warning(\"##### Couldn't link pad to queue\")\nqueue.set_state(gst.STATE_PLAYING)\nfakesink.set_state(gst.STATE_PLAYING)\ngst.info('finished here')", "path": "arista\\discoverer.py", "repo_name": "danielgtaylor/arista", "stars": 121, "license": "lgpl-2.1", "language": "python", "size": 3427}
{"docstring": "\"\"\"\n    Called when a device has been removed from the signal. If the\n    device is a volume with a video DVD the \"video-lost\" signal is\n    emitted.\n\"\"\"\n", "func_signal": "def device_removed(self, udi):\n", "code": "for block, drive in self.drives.items():\n    if drive.video_udi == udi:\n        drive.video = False\n        drive.udi = \"\"\n        label = drive.label\n        drive.label = \"\"\n        self.emit(\"disc-lost\", drive, label)\n        break\n\nfor device, capture in self.capture_devices.items():\n    if capture.udi == udi:\n        self.emit(\"v4l-capture-lost\", self.capture_devices[device])\n        del self.capture_devices[device]\n        break", "path": "arista\\inputs\\haldisco.py", "repo_name": "danielgtaylor/arista", "stars": 121, "license": "lgpl-2.1", "language": "python", "size": 3427}
{"docstring": "\"\"\"\n    This method is called anytime one or more files are selected and\n    the right-click menu is invoked. If we are looking at a media\n    file then let's show the new menu item!\n\"\"\"\n# Check if this is actually a media file and it is local\n", "func_signal": "def get_file_items(self, window, files):\n", "code": "for f in files:\n    if f.get_mime_type() not in SUPPORTED_FORMATS:\n        return\n    \n    if not f.get_uri().startswith(\"file://\"):\n        return\n\n# Create the new menu item, with a submenu of devices each with a \n# submenu of presets for that particular device.\nmenu = nautilus.MenuItem('Nautilus::convert_media',\n                         _('Convert for device'),\n                         _('Convert this media using a device preset'))\n\ndevices = nautilus.Menu()\nmenu.set_submenu(devices)\n\npresets = arista.presets.get().items()\nfor shortname, device in sorted(presets, lambda x,y: cmp(x[1].name, y[1].name)):\n    item = nautilus.MenuItem(\"Nautilus::convert_to_%s\" % shortname,\n                             device.name,\n                             device.description)\n    \n    presets = nautilus.Menu()\n    item.set_submenu(presets)\n    \n    for preset_name, preset in device.presets.items():\n        preset_item = nautilus.MenuItem(\n                \"Nautilus::convert_to_%s_%s\" % (shortname, preset.name),\n                preset.name,\n                \"%s: %s\" % (device.name, preset.name))\n        preset_item.connect(\"activate\", self.callback,\n                            [f.get_uri()[7:] for f in files],\n                            shortname, preset.name)\n        presets.append_item(preset_item)\n    \n    devices.append_item(item)\n\nreturn menu,", "path": "arista-nautilus.py", "repo_name": "danielgtaylor/arista", "stars": 121, "license": "lgpl-2.1", "language": "python", "size": 3427}
{"docstring": "\"\"\"\nRegister a callable with arguments to be called at the given frequency.\nThe frequency must be one of the above constants.\n\"\"\"\n", "func_signal": "def register(self, frequency, to_call, args=(), kwargs={}):\n", "code": "if not FREQUENCIES[frequency]:\n    raise ValueError(\"The frequency is invalid. it must be from the defined list.\")\nif frequency == NEVER: return # Don't create an entry for something that never happens\nentry = (to_call, args, kwargs)\ntry:\n    self.entries[frequency].append(entry)\nexcept KeyError:\n    self.entries[frequency] = [entry]", "path": "stockandflow\\periodic.py", "repo_name": "jesseh/django-stockandflow", "stars": 82, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"\nValues are either given or extracted from the request.\n\"\"\"\n", "func_signal": "def __init__(self, process, request=None, stock=None):\n", "code": "if stock is None:\n    stock_slug = request.GET.get(\"stock_slug\", \"\")\n    self.stock = process.stock_lookup[stock_slug] # raises a key error if invalid stoc\nelse:\n    self.stock = stock", "path": "stockandflow\\views.py", "repo_name": "jesseh/django-stockandflow", "stars": 82, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"\nGenerate a series of flows from a choices tuple array.\n\"\"\"\n", "func_signal": "def gen_flows_from_choice(choice, flow_event_model, choice_stocks):\n", "code": "rv = []\ni = 0\ntry:\n    while(True):\n        up = choice[i+1]\n        down = choice[i]\n        up_stock_slug = up[0]\n        down_stock_slug = down[0]\n        up_slug = \"rising_%s\" % up_stock_slug\n        up_name = \"Rising to %s\" % up[1].lower()\n        down_slug = \"dropping_%s\" % down_stock_slug\n        down_name = \"Dropping to %s\" % down[1].lower()\n        rv.append(Flow(slug=up_slug, name=up_name, flow_event_model=flow_event_model,\n                       sources=[choice_stocks[down_stock_slug]],\n                       sinks=[choice_stocks[up_stock_slug]]))\n        rv.append(Flow(slug=down_slug, name=down_name, flow_event_model=flow_event_model,\n                       sources=[choice_stocks[up_stock_slug]],\n                       sinks=[choice_stocks[down_stock_slug]]))\n        i += 1\nexcept IndexError:\n    pass\nreturn rv", "path": "example\\stocksandflows\\profiles_sandf.py", "repo_name": "jesseh/django-stockandflow", "stars": 82, "license": "None", "language": "python", "size": 160}
{"docstring": "# Adding model 'StockRecord'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_table('stockandflow_stockrecord', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('stock', self.gf('django.db.models.fields.SlugField')(max_length=50, db_index=True)),\n            ('timestamp', self.gf('django.db.models.fields.DateTimeField')(auto_now_add=True, blank=True)),\n            ('count', self.gf('django.db.models.fields.PositiveIntegerField')()),\n        ))\n        db.send_create_signal('stockandflow', ['StockRecord'])\n\n        # Adding model 'PeriodicSchedule'\n        db.create_table('stockandflow_periodicschedule', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('frequency', self.gf('django.db.models.fields.SlugField')(max_length=50, db_index=True)),\n            ('last_run_timestamp', self.gf('django.db.models.fields.DateTimeField')(null=True)),\n            ('call_count', self.gf('django.db.models.fields.IntegerField')(default=0, null=True)),\n        ))\n        db.send_create_signal('stockandflow', ['PeriodicSchedule'])", "path": "stockandflow\\migrations\\0001_initial.py", "repo_name": "jesseh/django-stockandflow", "stars": 82, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"\nRun the entries for a given frequency.\n\"\"\"\n", "func_signal": "def run_entries_for_frequency(self, frequency):\n", "code": "self.log(\"Running %s entries.\" % frequency)\ncall_count = 0\nfor to_call, args, kwargs in self.entries.get(frequency, []):\n    self.log(\"Running '%s'.\" % to_call.func_name)\n    message = to_call(*args, **kwargs)\n    self.log(message)\n    call_count += 1\nreturn call_count", "path": "stockandflow\\periodic.py", "repo_name": "jesseh/django-stockandflow", "stars": 82, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"\nCreate a proxy of a model that can be used to represents a stock or a flow in an\nadmin site.\n\nDjango requires that either the module or an app label be set, so adding the new \nmodel to an existing module is necessary.\n\"\"\"\n\n", "func_signal": "def create_proxy_model(self, represents, base_model, module):\n", "code": "class_name = represents.__class__.__name__\nname = represents.name.title().replace(\" \",\"\") + class_name\nclass Meta:\n    proxy = True\n    verbose_name_plural = \"%02d. %s: %s\" % (self.next_reg_sequence(), class_name, \n                                          represents.name.capitalize())\nattrs = {\n    '__module__': module,\n    '__str__': represents.name,\n    'Meta': Meta,\n}\nrv = type(name, (base_model,), attrs)\nreturn rv", "path": "stockandflow\\admin.py", "repo_name": "jesseh/django-stockandflow", "stars": 82, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"\nIf valid_redirect is not defined it will just reload the current URL with an updated\nGET query string.\n\nNOTE: THE IS AN UNTESTED WORK IN PROGRESS\n\"\"\"\n", "func_signal": "def form(self, request, valid_redirect=None):\n", "code": "if request.method == 'POST':\n    form = FacetForm(self, request.POST)\n    if form.is_valid():\n        # Process the data in form.cleaned_data\n        # Find the facet that has a value, set the slug and value, update the querydict and redirect.\n        for slug, value in enumerate(form.cleaned_data):\n            if value:\n                self.slug = slug\n                self.value = value\n                break\n        if valid_redirect:\n            url = valid_redirect\n        else:\n            url = request.path_info\n            #Update the GET params\n        try:\n            query_str = self.update_query_dict(request.GET.copy()).urlencode()\n        except ValueError:\n            query_str = None\n        if query_str: \n            url += \"?%s\" % query_str\n        return redirect(url)\nelse:\n    form = FacetForm(self)\nreturn form", "path": "stockandflow\\views.py", "repo_name": "jesseh/django-stockandflow", "stars": 82, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"\nSet the values relevant to this object in the query dict.\n\"\"\"\n", "func_signal": "def update_query_dict(self, query_dict):\n", "code": "query_dict[\"stock_slug\"] = self.stock.slug\nreturn query_dict", "path": "stockandflow\\views.py", "repo_name": "jesseh/django-stockandflow", "stars": 82, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"\nSet the values relevant to this object in the query dict.\n\"\"\"\n", "func_signal": "def update_query_dict(self, query_dict):\n", "code": "query_dict = self.stock_selection.update_query_dict(query_dict)\nif self.facet_selection:\n    query_dict = self.facet_selection.update_query_dict(query_dict)\nquery_dict[\"index\"] = self.index\nreturn query_dict", "path": "stockandflow\\views.py", "repo_name": "jesseh/django-stockandflow", "stars": 82, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"\nAssign a sequence number for registrations so that they can be ordered in the display\n\"\"\"\n", "func_signal": "def next_reg_sequence(self):\n", "code": "self.registration_sequence += 1\nreturn self.registration_sequence", "path": "stockandflow\\admin.py", "repo_name": "jesseh/django-stockandflow", "stars": 82, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"\nDynamically create an admin model class that can be registered in the\nadmin site to represent a stock or flow.\n\n - The queryset extracts the records that are included in the stock or\n   flow.\n - The attrs dict become the properties of the class.\n - The action_mixins provide the a way to include sets of admin actions\n   in the resulting class.\n\"\"\"\n\n", "func_signal": "def create_model_admin(self, represents, queryset, attrs={}, action_mixins=[]):\n", "code": "class_name = represents.__class__.__name__\nname = represents.name.title().replace(\" \",\"\") + class_name + 'Admin'\ninherits = tuple([admin.ModelAdmin] + action_mixins)\nret_class = type(name, inherits, attrs)\nret_class.queryset = MethodType(lambda self, request: queryset, None, ret_class)\n# Block add and delete permissions because stocks and flows are read only\nret_class.has_add_permission = MethodType(lambda self, request: False, None, ret_class)\nret_class.has_delete_permission = MethodType(lambda self, request, obj=None: \n                                             False, None, ret_class)\n\n# Collect all the mixed in actions\nall_actions = []\nreduce(lambda a, cls: a.extend(cls.actions), action_mixins, all_actions)\nret_class.actions = all_actions\nret_class.actions_on_bottom = True\nreturn ret_class", "path": "stockandflow\\admin.py", "repo_name": "jesseh/django-stockandflow", "stars": 82, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"\nUpdate the request's GET query string with the values in this object\nand return the resulting query url encoded string.\n\"\"\"\n", "func_signal": "def query_str(self):\n", "code": "qd = QueryDict(\"\", mutable=True)\nqd = self.update_query_dict(qd)\nreturn qd.urlencode()", "path": "stockandflow\\views.py", "repo_name": "jesseh/django-stockandflow", "stars": 82, "license": "None", "language": "python", "size": 160}
{"docstring": "# Deleting model 'StockRecord'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.delete_table('stockandflow_stockrecord')\n\n        # Deleting model 'PeriodicSchedule'\n        db.delete_table('stockandflow_periodicschedule')", "path": "stockandflow\\migrations\\0001_initial.py", "repo_name": "jesseh/django-stockandflow", "stars": 82, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"\nReturn the next or previous in the sequence based on the step_amount.\nIf cur_object_id and slug are None then the previous object is index +\nstep.\n\nIf current_object or slug is not None and if the object at index is not\nequal to current object's id or slug then previous will have the same\nindex, so simply return self.\n\nRaises StopIteration if the next index is invalid.\n\"\"\"\n", "func_signal": "def _step(self, step_amount, current_object_id, current_slug, slug_field):\n", "code": "if not current_object_id:\n    self.index = 0\n    rv = self\nelif not self.object_at_index:\n    raise StopIteration\nelif current_object_id != self.object_at_index.id:\n    rv = self\nelif current_slug is not None:\n    if slug_field is None:\n        raise ValueError(\"There must be a slug_field give if current_slug is given.\")\n    if current_slug != self.get_object()[slug_field]:\n        rv = self\nelse:\n    stepped_index = self.index + step_amount\n    if stepped_index < 0:\n        raise StopIteration\n    rv = StockSequencer(self.stock_selection, self.facet_selection, stepped_index)\nif not rv.object_at_index:\n    raise StopIteration\nreturn rv", "path": "stockandflow\\views.py", "repo_name": "jesseh/django-stockandflow", "stars": 82, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"\nValues are either given or extracted from the request.\n\"\"\"\n", "func_signal": "def __init__(self, request=None, facet_slug=None, facet_value=None):\n", "code": "if request:\n    self.slug = request.GET.get(\"facet_slug\", \"\")\n    self.value = request.GET.get(\"facet_value\", \"\")\nif facet_slug is not None:\n    self.slug = facet_slug\nif facet_value is not None:\n    self.value = facet_value", "path": "stockandflow\\views.py", "repo_name": "jesseh/django-stockandflow", "stars": 82, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"\nRemove the delete_selected action because these are proxy models.\nThe action can be added back in for a given model stock or flow.\n\"\"\"\n", "func_signal": "def __init__(self, *args):\n", "code": "super(StockAndFlowAdminSite, self). __init__(*args)\nself.disable_action('delete_selected')", "path": "stockandflow\\admin.py", "repo_name": "jesseh/django-stockandflow", "stars": 82, "license": "None", "language": "python", "size": 160}
{"docstring": "# Get the facet select defined by the request.\n", "func_signal": "def all_stock_sequencers(self, facet_selection=None):\n", "code": "stock_seqs = []\nfor stock in self.stocks:\n    stock_selection = StockSelection(self, stock=stock)\n    try:\n        stock_seqs.append(StockSequencer(stock_selection, facet_selection))\n    except StopIteration:\n        pass\nreturn stock_seqs", "path": "stockandflow\\views.py", "repo_name": "jesseh/django-stockandflow", "stars": 82, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"\nFeed a geckoboard line chart. The options that can be set in a GET\nquery are points (integer), x_label (string), y_label (string), color\n(string).\n\"\"\"\n", "func_signal": "def stock_line_chart(request, slug):\n", "code": "points = int(request.GET.get(\"points\", 50))\nx_label = request.GET.get(\"x_label\", \"\")\ny_label = request.GET.get(\"y_label\", slug.capitalize())\ncolor = request.GET.get(\"color\", None)\nrecords = list(StockRecord.objects.filter(stock=slug).values_list('count', flat=True)[:points])\nrecords.reverse()\nif color: return ( records, x_label, y_label, color)\nreturn ( records, x_label, y_label)", "path": "stockandflow\\views.py", "repo_name": "jesseh/django-stockandflow", "stars": 82, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"\nSet the values relevant to this object in the query dict.\n\"\"\"\n", "func_signal": "def update_query_dict(self, query_dict):\n", "code": "query_dict[\"facet_slug\"] = self.slug\nquery_dict[\"facet_value\"] = self.value\nreturn query_dict", "path": "stockandflow\\views.py", "repo_name": "jesseh/django-stockandflow", "stars": 82, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"\nRun the schedule by checking if now is a higher period than the period\nof the last call for each frequency, and if so then run all the entries\nfor that frequency.\n\nThe period is determined by looking at the minutes since the epock, so\nit is safe to run this function repeatedly and it will still only run\nthe entries for each frequency once per period.\n\"\"\"\n", "func_signal": "def run(self):\n", "code": "now = datetime.now()\nnow_seconds = int(time.mktime(now.utctimetuple()))\nself.log(\"Starting to run at %s.\" % now)\nperiod_mins_to_freq = dict((period, freq) for freq, period in FREQUENCIES.iteritems())\nfor period_mins in sorted(period_mins_to_freq.keys()):\n    if period_mins == 0: continue # Skip the never frequency\n    freq = period_mins_to_freq[period_mins]\n    to_run, created = PeriodicSchedule.objects.get_or_create(frequency=freq,\n            defaults={\"last_run_timestamp\": datetime.now(), \"call_count\": 0})\n    if created:\n        self.log(\"Not running %s frequency because it was just created.\" % freq)\n        continue # Don't run just after creation because now may be mid-period\n    last_run_timestamp = to_run.last_run_timestamp\n    last_run_count = to_run.call_count\n    if not last_run_timestamp:\n        self.log(\"Giving defualt timestamp for %s\" % freq)\n        last_run_timestamp = datetime(1901,1,1)\n        last_run_count = 0\n    #Check for if this is overlapping a previous run\n    elif to_run.call_count is None:\n        self.log(\"Not running %s frequency because of an overlap.\" % freq)\n        self.overlap_warning(freq, now)\n    last_seconds = int(time.mktime(last_run_timestamp.utctimetuple()))\n    now_period = now_seconds / 60 / period_mins\n    last_period = last_seconds / 60 / period_mins\n    if now_period > last_period:\n        # Set that this is running in the database\n        to_run.last_run_timestamp = now\n        to_run.call_count = None #Mark to catch an overlap\n        to_run.save()\n        call_count = self.run_entries_for_frequency(freq)\n        just_ran = PeriodicSchedule.objects.get(frequency=freq)\n        if just_ran.last_run_timestamp == now:\n            just_ran.call_count = call_count\n            just_ran.save()\n        else:\n            self.log(\"The run at %s has been overlapped.\" % freq)\n            # don't save the call count when there has been an overlap\n    else:\n        self.log(\"Not running %s because it is within the period\" % freq)", "path": "stockandflow\\periodic.py", "repo_name": "jesseh/django-stockandflow", "stars": 82, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\" Adds the current state to the undo stack \"\"\"\n", "func_signal": "def UndoAdd(self, saveState):\n", "code": "self.UndoStack.append(saveState)\nself.menuItems[JetDefs.MNU_UNDO].Enable(True)", "path": "Android.app\\sdk\\tools\\Jet\\JetCreator\\JetCreator.py", "repo_name": "willfarrell/Browsers", "stars": 93, "license": "None", "language": "python", "size": 314209}
{"docstring": "\"\"\"Bth; Get MIDI info\"\"\"\n\n", "func_signal": "def GetMidiInfo(midiFile):\n", "code": "class midiData(object):\n\tdef __init__ (self):\n\t\tself.err = 1\n\t\tself.endMbt = \"0:0:0\"\n\t\tself.totalTicks = 0\n\t\tself.maxTracks = 0\n\t\tself.maxMeasures = 0\n\t\tself.maxBeats = 0\n\t\tself.maxTicks = 0\n\t\tself.totalTicks = 0\n\t\tself.timebase = None\n\t\tself.ppqn = 0\n\t\tself.beats_per_measure = 0\n\t\tself.trackList = []\n\t\t\nmd = midiData()\n\ntry:\n\tm = MIDIFile(midiFile, 'rb')\n\tm.ReadFromStream()\n\t\n\tfor track in m.tracks:\n\t\tif track.channel is not None:\n\t\t\tempty = False \n\t\t\ttrk = track.channel + 1\n\t\telse:\n\t\t\tempty = True\t\n\t\t\ttrk = ''\t\t\n\t\tmd.trackList.append(trackGrid(track.trackNum, trk, track.name, empty))\n\t\t\t\n\tmd.endMbt = m.timebase.ConvertTicksToMBT(m.end_of_file)\n\tmd.endMbtStr = \"%d:%d:%d\" % (md.endMbt[0], md.endMbt[1], md.endMbt[2])\n\tmd.maxMeasures = md.endMbt[0]\n\tmd.maxBeats = 4\n\tmd.maxTicks = m.timebase.ppqn\n\tmd.maxTracks = m.num_tracks\n\tmd.totalTicks = m.end_of_file\n\tmd.timebase = m.timebase\n\tmd.ppqn = m.timebase.ppqn\n\tmd.beats_per_measure = m.timebase.beats_per_measure\n\n\t#add above if more added\n\tmd.err = 0\n\t\n\tm.close()\nexcept:\n\traise\n\tpass\n\nreturn md", "path": "Android.app\\sdk\\tools\\Jet\\JetCreator\\midifile.py", "repo_name": "willfarrell/Browsers", "stars": 93, "license": "None", "language": "python", "size": 314209}
{"docstring": "\"\"\" Creates the toolbar \"\"\"\n", "func_signal": "def createToolbar(self):\n", "code": "toolbar = self.CreateToolBar()\ntoolbar.SetToolBitmapSize((32,32))\nself.toolItems = {}\nfor eachTool in JetDefs.TOOLBAR_SPEC:\n    if eachTool[0] == '-':\n        toolbar.AddSeparator()\n    else:\n        b = __import__(eachTool[1])\n        bitMap = b.getBitmap()\n        self.toolItems[eachTool[0]] = toolbar.AddLabelTool(-1, label=eachTool[0], \n                bitmap=bitMap, \n                             shortHelp=eachTool[0], longHelp=eachTool[2])\n        self.Bind(wx.EVT_TOOL, getattr(self, eachTool[3]) , self.toolItems[eachTool[0]])\ntoolbar.Realize()", "path": "Android.app\\sdk\\tools\\Jet\\JetCreator\\JetCreator.py", "repo_name": "willfarrell/Browsers", "stars": 93, "license": "None", "language": "python", "size": 314209}
{"docstring": "# default parameters\n", "func_signal": "def DeleteEvents (self, start_index, end_index, move_meta_events=None):\n", "code": "if start_index is None:\n\tstart_index = 0\nif end_index is None:\n\tend_index = len(self)\n\n#print(\"\\n\")\n#for evt in self[start_index:end_index]:\n#\tprint(\"%d %s\" % (evt.ticks, evt))\n\n# delete events\ndelete_count = 0\nmove_count = 0\nfor event in self[start_index:end_index]:\n\t#Bth; Added this so we always get clip end events; clips that ended on last measure wouldn't end on repeat\n\tif (event.msg_type == CONTROL_CHANGE) and \\\n\t        (event.controller == JET_EVENT_TRIGGER_CLIP) and \\\n\t        ((event.value & 0x40) != 0x40):\n\t\tpass\n\telse:\n\t\tif (move_meta_events is None) or (event.msg_type != META_EVENT):\n\t\t\tself.remove(event)\n\t\t\tdelete_count += 1\n\t\t\t\n\t\t# move meta-events\n\t\telse:\n\t\t\tevent.ticks = move_meta_events\n\t\t\tmove_count += 1\n\t\t\nmidi_file_logger.debug('DeleteEvents: deleted %d events in range(%s:%s)' % (delete_count, start_index, end_index))\nmidi_file_logger.debug('DeleteEvents: moved %d events in range(%s:%s)' % (move_count, start_index, end_index))", "path": "Android.app\\sdk\\tools\\Jet\\JetCreator\\midifile.py", "repo_name": "willfarrell/Browsers", "stars": 93, "license": "None", "language": "python", "size": 314209}
{"docstring": "\"\"\" Selects a segment by segment name \"\"\"\n", "func_signal": "def SelectSegment(self, segName):\n", "code": "itm = self.segList.FindItem(-1, segName)\nself.segList.EnsureVisible(itm)\nClearRowSelections(self.segList)\nSetRowSelection(self.segList, itm, True)", "path": "Android.app\\sdk\\tools\\Jet\\JetCreator\\JetCreator.py", "repo_name": "willfarrell/Browsers", "stars": 93, "license": "None", "language": "python", "size": 314209}
{"docstring": "\"\"\"Process an event and save any changes in controller values\"\"\"\n# process control changes\n", "func_signal": "def Event (self, event):\n", "code": "if event.msg_type == CONTROL_CHANGE:\n\tself.ControlChange(event)\nelif event.msg_type == CHANNEL_PRESSURE:\n\tself.PressureChange(event)\nelif event.msg_type == PROGRAM_CHANGE:\n\tself.ProgramChange(event)\nelif event.msg_type == PITCH_BEND:\n\tself.PitchBendChange(event)", "path": "Android.app\\sdk\\tools\\Jet\\JetCreator\\midifile.py", "repo_name": "willfarrell/Browsers", "stars": 93, "license": "None", "language": "python", "size": 314209}
{"docstring": "\"\"\"Reset controllers to default.\"\"\"\n", "func_signal": "def ResetControllers (self, channel):\n", "code": "self.controllers[channel] = DEFAULT_CONTROLLER_VALUES\nself.rpns[channel] = DEFAULT_RPN_VALUES\nself.pressure[channel] = 0", "path": "Android.app\\sdk\\tools\\Jet\\JetCreator\\midifile.py", "repo_name": "willfarrell/Browsers", "stars": 93, "license": "None", "language": "python", "size": 314209}
{"docstring": "\"\"\"Monitor control change.\"\"\"\n", "func_signal": "def ControlChange (self, event):\n", "code": "controller = event.controller\nif controller in MONITOR_CONTROLLERS:\n\tchannel = event.channel\n\tself.controllers[channel][controller] = event.value\n\tif (controller == CTRL_RPN_DATA_MSB) or (controller == CTRL_RPN_DATA_LSB):\n\t\trpn = (self.controllers[channel][CTRL_RPN_MSB] << 7) + self.controllers[channel][CTRL_RPN_LSB]\n\t\tif rpn in MONITOR_RPNS:\n\t\t\tvalue = (self.controllers[channel][CTRL_RPN_DATA_MSB] << 7) + self.controllers[channel][CTRL_RPN_DATA_LSB]\n\t\t\tself.rpns[channel][rpn] = value\n\n# reset controllers\nelif event.controller == CTRL_RESET_CONTROLLERS:\n\tself.ResetControllers[event.channel]", "path": "Android.app\\sdk\\tools\\Jet\\JetCreator\\midifile.py", "repo_name": "willfarrell/Browsers", "stars": 93, "license": "None", "language": "python", "size": 314209}
{"docstring": "\"\"\" These are screen updates called for from within the thread.  Communication \n    is via a postevent call.\n\"\"\"\n", "func_signal": "def OnJetStatusUpdate(self, evt):\n", "code": "if evt.mode == JetDefs.PST_PLAY:\n    segName = getColumnText(self.segList, evt.data, 0)\n    self.LoadEventsForSeg(segName)\n    self.log.SetValue(segName)\n    ClearRowSelections(self.segList)\n    SetRowSelection(self.segList, evt.data, True)\nelif evt.mode == JetDefs.PST_UPD_LOCATION:\n    self.graph.UpdateLocation(evt.data)\nelif evt.mode == 3:\n    self.graph.UpdateLocation(0)\n    ClearRowSelections(self.segList)\n    self.eventList.DeleteAllItems()\n    self.SetKeepPlayingFlag(False)\n    self.btnPlay.SetLabel(JetDefs.BUT_PLAY)\n    self.btnPause.SetLabel(JetDefs.BUT_PAUSE)", "path": "Android.app\\sdk\\tools\\Jet\\JetCreator\\JetCreator.py", "repo_name": "willfarrell/Browsers", "stars": 93, "license": "None", "language": "python", "size": 314209}
{"docstring": "\"\"\" Imports a jet archive file \"\"\"\n", "func_signal": "def OnJetImportArchive(self, event):\n", "code": "defDir = IniGetValue(JetDefs.JETCREATOR_INI, JetDefs.INI_DEFAULTDIRS, JetDefs.ARCHIVE_FILE_SPEC, 'str', str(os.getcwd()))\ndialog = wx.FileDialog(None, JetDefs.IMPORT_ARCHIVE_PROMPT, defDir, \"\", JetDefs.ARCHIVE_FILE_SPEC, wx.OPEN)\nif dialog.ShowModal() == wx.ID_OK:\n    IniSetValue(JetDefs.JETCREATOR_INI, JetDefs.INI_DEFAULTDIRS, JetDefs.ARCHIVE_FILE_SPEC, str(FileJustPath(dialog.GetPath())))\n    defDir = IniGetValue(JetDefs.JETCREATOR_INI, JetDefs.INI_DEFAULTDIRS, JetDefs.ARCHIVE_FILE_SPEC + \"Dir\", 'str', str(os.getcwd()))\n    dlg1 = wx.DirDialog(self, JetDefs.IMPORT_ARCHIVEDIR_PROMPT, style=wx.DD_DEFAULT_STYLE, defaultPath=defDir)\n    if dlg1.ShowModal() == wx.ID_OK:\n        IniSetValue(JetDefs.JETCREATOR_INI, JetDefs.INI_DEFAULTDIRS, JetDefs.ARCHIVE_FILE_SPEC + \"Dir\", str(FileJustPath(dlg1.GetPath())))\n        if YesNo(JetDefs.MAIN_IMPORTTITLE, JetDefs.MAIN_IMPORTMSG % (dialog.GetPath(),dlg1.GetPath()), False):\n            projectPath = dlg1.GetPath()\n            zipFile = dialog.GetPath()\n            z = __import__('zipfile')\n            \n            if not z.is_zipfile(zipFile):\n                wx.MessageBox(JetDefs.IMPORT_ARCHIVE_NO_JTC)\n            else:\n                zip = z.ZipFile(zipFile, 'r')\n                \n                jtcFile = \"\"\n                fileList = zip.namelist()\n                \n                isArchive = False\n                for myFile in fileList:\n                    if myFile == 'JetArchive':\n                        isArchive = True\n                        break\n                if not isArchive:\n                    wx.MessageBox(JetDefs.IMPORT_NOT_JET_ARCHIVE)\n                else:\n                    for myFile in fileList:\n                        if FileJustExt(myFile) == '.JTC':\n                            jtcFile = myFile\n                            break\n                    if jtcFile == \"\":\n                        wx.MessageBox(JetDefs.IMPORT_ARCHIVE_NO_JTC)\n                    else:\n                        for name in zip.namelist(): \n                            ext = FileJustExt(name)\n                            if ext == '.MID' or ext == '.DLS'  or ext == '.JET':\n                                file(FileFixPath(projectPath + \"/\" + name), 'wb').write(zip.read(name))\n                            else:\n                                if len(ext) > 0 and ext != '.DS_STORE':\n                                    file(FileFixPath(projectPath + \"/\" + name), 'w').write(zip.read(name))\n                    zip.close()\n                    self.currentJetConfigFile = FileFixPath(projectPath + \"/\") + jtcFile\n                    self.jet_file = JetFile(self.currentJetConfigFile, \"\")\n                    \n                    #fix paths\n                    self.jet_file.config.filename = FileJustRoot(self.currentJetConfigFile) + \".JET\"\n                    for index, segment in enumerate(self.jet_file.segments):\n                        self.jet_file.segments[index].filename = FileFixPath(projectPath + \"/\" + segment.filename)\n                        if segment.dlsfile > \"\":\n                            self.jet_file.segments[index].dlsfile = FileFixPath(projectPath + \"/\" + segment.dlsfile)\n                        self.jet_file.segments[index].output = FileFixPath(projectPath + \"/\" + segment.output)                        \n                    \n                    for index, library in enumerate(self.jet_file.libraries):\n                        self.jet_file.libraries[index] = FileFixPath(projectPath + \"/\" + library)\n                        \n                    if ValidateConfig(self.jet_file):\n                        self.jet_file.SaveJetConfig(self.currentJetConfigFile)\n                        self.jet_file.WriteJetFileFromConfig(self.currentJetConfigFile)\n                        self.jet_file = JetFile(self.currentJetConfigFile , \"\")\n                        self.SetCurrentFile(self.currentJetConfigFile)\n\n    dlg1.Destroy()\ndialog.Destroy()", "path": "Android.app\\sdk\\tools\\Jet\\JetCreator\\JetCreator.py", "repo_name": "willfarrell/Browsers", "stars": 93, "license": "None", "language": "python", "size": 314209}
{"docstring": "# copy events and sort them by ticks/sequence#\n", "func_signal": "def MergeEvents (self, events):\n", "code": "self.extend(events)\nself.SortEvents()", "path": "Android.app\\sdk\\tools\\Jet\\JetCreator\\midifile.py", "repo_name": "willfarrell/Browsers", "stars": 93, "license": "None", "language": "python", "size": 314209}
{"docstring": "\"\"\" Loads default properties from the a configuration file \"\"\"\n", "func_signal": "def LoadDefaultProperties(self):\n", "code": "self.jet_file.config.copyright = IniGetValue(JetDefs.JETCREATOR_INI, JetDefs.INI_PREF_SECTION, JetDefs.F_COPYRIGHT)\nself.jet_file.config.chase_controllers = IniGetValue(JetDefs.JETCREATOR_INI, JetDefs.INI_PREF_SECTION, JetDefs.F_CHASECONTROLLERS, 'bool', True)\nself.jet_file.config.delete_empty_tracks = IniGetValue(JetDefs.JETCREATOR_INI, JetDefs.INI_PREF_SECTION, JetDefs.F_DELETEEMPTYTRACKS, 'bool', False)", "path": "Android.app\\sdk\\tools\\Jet\\JetCreator\\JetCreator.py", "repo_name": "willfarrell/Browsers", "stars": 93, "license": "None", "language": "python", "size": 314209}
{"docstring": "\"\"\" Sets a flag to communicate playing state to the play thread \"\"\"\n", "func_signal": "def SetKeepPlayingFlag(self, val):\n", "code": "with self.playerLock:\n    self.keepPlaying = val", "path": "Android.app\\sdk\\tools\\Jet\\JetCreator\\JetCreator.py", "repo_name": "willfarrell/Browsers", "stars": 93, "license": "None", "language": "python", "size": 314209}
{"docstring": "\"\"\" Calls the dialog box to update the current event \"\"\"\n", "func_signal": "def OnEventUpdate(self, event):\n", "code": "if self.currentSegmentName is None:\n    return\n\nif self.currentEventName  is None:\n    return\n\nsegment = self.jet_file.GetSegment(self.currentSegmentName)\nif segment == None:\n    return\n\ncurEvent = copy.deepcopy(self.jet_file.GetEvent(self.currentSegmentName, self.currentEventName))\nif curEvent == None:\n    return\n\nsaveState = JetState(self.jet_file, self.currentSegmentIndex, self.currentEventIndex)\n\n#only want the event we are editing to show up in graph\neditSegment = copy.deepcopy(segment)\neditSegment.jetevents = []\neditSegment.jetevents.append(curEvent)\n\ndlg = EventEdit(JetDefs.MAIN_REVEVENTTITLE, self.currentJetConfigFile)\ndlg.SetSegment(editSegment)\ndlg.SetValue(JetDefs.F_ENAME, curEvent.event_name)\ndlg.SetValue(JetDefs.F_ETYPE, curEvent.event_type)\ndlg.SetValue(JetDefs.F_ESTART, curEvent.event_start)\ndlg.SetValue(JetDefs.F_EEND, curEvent.event_end)\ndlg.SetValue(JetDefs.F_ETRACK, curEvent.track_num)\ndlg.SetValue(JetDefs.F_ECHANNEL, curEvent.channel_num)\ndlg.SetValue(JetDefs.F_EEVENTID, curEvent.event_id)\ndlg.OnEventSelect()   \n\nresult = dlg.ShowModal()\nif result == wx.ID_OK:\n    if dlg.GetValue(JetDefs.F_ETYPE) == JetDefs.E_EOS:\n        dlg.SetValue(JetDefs.F_ESTART, dlg.GetValue(JetDefs.F_EEND))\n        \n    self.jet_file.UpdateEvent(self.currentSegmentName, \n                              self.currentEventName, \n                             dlg.GetValue(JetDefs.F_ENAME),\n                             dlg.GetValue(JetDefs.F_ETYPE), \n                             dlg.GetValue(JetDefs.F_EEVENTID), \n                             dlg.GetValue(JetDefs.F_ETRACK), \n                             dlg.GetValue(JetDefs.F_ECHANNEL), \n                             dlg.GetValue(JetDefs.F_ESTART),\n                             dlg.GetValue(JetDefs.F_EEND))\n\n    if len(dlg.lstReplicate) > 0:\n        if dlg.chkReplaceMatching:\n            self.jet_file.DeleteEventsMatchingPrefix(self.currentSegmentName, dlg.replicatePrefix) \n        \n        for replicate in dlg.lstReplicate:\n            self.jet_file.AddEvent(self.currentSegmentName, replicate[0],\n                                 dlg.GetValue(JetDefs.F_ETYPE), \n                                 dlg.GetValue(JetDefs.F_EEVENTID), \n                                 dlg.GetValue(JetDefs.F_ETRACK), \n                                 dlg.GetValue(JetDefs.F_ECHANNEL), \n                                 mbtFct(replicate[1],-1),\n                                 mbtFct(replicate[2],-1))\n        self.SelectSegment(self.currentSegmentName)\n        self.SelectEvent(dlg.lstReplicate[0][0])\n    else:            \n        self.SelectSegment(self.currentSegmentName)\n        self.SelectEvent(dlg.GetValue(JetDefs.F_ENAME))\n    self.UndoAdd(saveState)\ndlg.Destroy()", "path": "Android.app\\sdk\\tools\\Jet\\JetCreator\\JetCreator.py", "repo_name": "willfarrell/Browsers", "stars": 93, "license": "None", "language": "python", "size": 314209}
{"docstring": "\"\"\"Save MIDI data to new file.\"\"\"\n", "func_signal": "def SaveAs (self, filename, offset=0, filters=None):\n", "code": "output_file = MIDIFile(filename, 'wb')\nself.Write(output_file, offset, filters)\noutput_file.close()", "path": "Android.app\\sdk\\tools\\Jet\\JetCreator\\midifile.py", "repo_name": "willfarrell/Browsers", "stars": 93, "license": "None", "language": "python", "size": 314209}
{"docstring": "\"\"\" Loads the authoring guidelines file \"\"\"\n", "func_signal": "def OnHelpJetGuidelines(self, event):\n", "code": "import webbrowser\nwebbrowser.open(JetDefs.MAIN_HELPGUIDELINESFILE)\nreturn", "path": "Android.app\\sdk\\tools\\Jet\\JetCreator\\JetCreator.py", "repo_name": "willfarrell/Browsers", "stars": 93, "license": "None", "language": "python", "size": 314209}
{"docstring": "\"\"\" Adds the current state the the redo stack \"\"\"\n", "func_signal": "def RedoAdd(self, saveState):\n", "code": "self.RedoStack.append(saveState)\nself.menuItems[JetDefs.MNU_REDO].Enable(True)", "path": "Android.app\\sdk\\tools\\Jet\\JetCreator\\JetCreator.py", "repo_name": "willfarrell/Browsers", "stars": 93, "license": "None", "language": "python", "size": 314209}
{"docstring": "\"\"\" Move event(s) \"\"\"\n", "func_signal": "def OnEventsMove(self, event):\n", "code": "if self.currentSegmentName is None:\n    return\n\nif self.currentEventName  is None:\n    return\n\nsegment = self.jet_file.GetSegment(self.currentSegmentName)\nif segment == None:\n    return\n\ncurEvent = self.jet_file.GetEvent(self.currentSegmentName, self.currentEventName)\nif curEvent == None:\n    return\n\nlstMoveItems = []\ncount = 0\nitem = self.eventList.GetFirstSelected()\nwhile item != -1:\n    lstMoveItems.append((getColumnText(self.eventList,item,0), mbtFct(getColumnText(self.eventList,item,2),-1), mbtFct(getColumnText(self.eventList,item,3),-1), segment.end))\n    count = count + 1\n    item = self.eventList.GetNextSelected(item)\n\nif count == 0:\n    InfoMsg(\"Move\", \"Select one or more items to move.\")\n    return\n\ndlg = JetMove(\"Move Events\")\ndlg.lstMoveItems = lstMoveItems\nresult = dlg.ShowModal()\nif result == wx.ID_OK:\n    if len(dlg.lstMoveMbt) > 0:\n        saveState = JetState(self.jet_file, self.currentSegmentIndex, self.currentEventIndex)\n        \n        for moveitem in dlg.lstMoveMbt:\n            self.jet_file.MoveEvent(self.currentSegmentName, moveitem[0], moveitem[1], moveitem[2])\n        \n        self.SelectSegment(self.currentSegmentName)\n        self.LoadEventsForSeg(self.currentSegmentName)\n        \n        self.UndoAdd(saveState)\n        \ndlg.Destroy()", "path": "Android.app\\sdk\\tools\\Jet\\JetCreator\\JetCreator.py", "repo_name": "willfarrell/Browsers", "stars": 93, "license": "None", "language": "python", "size": 314209}
{"docstring": "\"\"\" Initializes the screen layout \"\"\"\n", "func_signal": "def initLayout(self):\n", "code": "panel = wx.Panel(self, -1)\n\nhboxMain = wx.BoxSizer(wx.HORIZONTAL)\n\nleftPanel = wx.Panel(panel, -1)\nleftTopPanel = wx.Panel(leftPanel, -1)\nleftBotPanel = wx.Panel(leftPanel, -1)\nrightPanel = wx.Panel(panel, -1)\n\nself.segList = JetCheckListCtrl(rightPanel)\nfor title, width, fld in JetDefs.SEGMENT_GRID:\n    self.segList.AddCol(title, width)\n\nself.eventList = JetListCtrl(rightPanel)\nfor title, width, fld in JetDefs.CLIPS_GRID:\n    self.eventList.AddCol(title, width)\n\nself.eventList.Bind(wx.EVT_LIST_ITEM_SELECTED, self.OnEventListClick)\nself.segList.Bind(wx.EVT_LIST_ITEM_SELECTED, self.OnSegListClick)\nself.segList.Bind(wx.EVT_LIST_ITEM_ACTIVATED, self.OnSegmentUpdate)\nself.eventList.Bind(wx.EVT_LIST_ITEM_ACTIVATED, self.OnEventUpdate)\n\nself.segList.BindCheckBox(self.OnSegmentChecked)\n\nBUT_SIZE = (95, 25)\nself.btnAddSeg = wx.Button(leftTopPanel, -1, JetDefs.BUT_ADD, size=BUT_SIZE)\nself.btnRevSeg = wx.Button(leftTopPanel, -1, JetDefs.BUT_REVISE, size=BUT_SIZE)\nself.btnDelSeg = wx.Button(leftTopPanel, -1, JetDefs.BUT_DELETE, size=BUT_SIZE)\nself.btnMoveSeg = wx.Button(leftTopPanel, -1, JetDefs.BUT_MOVE, size=BUT_SIZE)\n\nself.btnQueueAll = wx.Button(leftTopPanel, -1, JetDefs.BUT_QUEUEALL, size=BUT_SIZE)\nself.btnDequeueAll = wx.Button(leftTopPanel, -1, JetDefs.BUT_DEQUEUEALL, size=BUT_SIZE)\nself.btnPlay = wx.Button(leftTopPanel, -1, JetDefs.BUT_PLAY, size=BUT_SIZE)\nself.btnPause = wx.Button(leftTopPanel, -1, JetDefs.BUT_PAUSE, size=BUT_SIZE)\nself.btnAudition = wx.Button(leftTopPanel, -1, JetDefs.BUT_AUDITION, size=BUT_SIZE)\n\nself.btnAddEvt = wx.Button(leftBotPanel, -1, JetDefs.BUT_ADD, size=BUT_SIZE)\nself.btnRevEvt = wx.Button(leftBotPanel, -1, JetDefs.BUT_REVISE, size=BUT_SIZE)\nself.btnDelEvt = wx.Button(leftBotPanel, -1, JetDefs.BUT_DELETE, size=BUT_SIZE)\nself.btnMoveEvents = wx.Button(leftBotPanel, -1, JetDefs.BUT_MOVE, size=BUT_SIZE)\n\nself.Bind(wx.EVT_BUTTON, self.OnSegmentAdd, id=self.btnAddSeg.GetId())\nself.Bind(wx.EVT_BUTTON, self.OnSegmentUpdate, id=self.btnRevSeg.GetId())\nself.Bind(wx.EVT_BUTTON, self.OnSegmentDelete, id=self.btnDelSeg.GetId())\nself.Bind(wx.EVT_BUTTON, self.OnSegmentsMove, id=self.btnMoveSeg.GetId())\n\nself.Bind(wx.EVT_BUTTON, self.OnSelectAll, id=self.btnQueueAll.GetId())\nself.Bind(wx.EVT_BUTTON, self.OnDeselectAll, id=self.btnDequeueAll.GetId())\nself.Bind(wx.EVT_BUTTON, self.OnPlay, id=self.btnPlay.GetId())\nself.Bind(wx.EVT_BUTTON, self.OnPause, id=self.btnPause.GetId())\nself.Bind(wx.EVT_BUTTON, self.OnAudition, id=self.btnAudition.GetId())\n\nself.Bind(wx.EVT_BUTTON, self.OnEventAdd, id=self.btnAddEvt.GetId())\nself.Bind(wx.EVT_BUTTON, self.OnEventUpdate, id=self.btnRevEvt.GetId())\nself.Bind(wx.EVT_BUTTON, self.OnEventDelete, id=self.btnDelEvt.GetId())\nself.Bind(wx.EVT_BUTTON, self.OnEventsMove, id=self.btnMoveEvents.GetId())\n\nBORDER = 5\nBUT_SPACE = 3\nvBoxLeftTop = wx.BoxSizer(wx.VERTICAL)\nvBoxLeftBot = wx.BoxSizer(wx.VERTICAL)\n\nvBoxLeftTop.Add(self.btnAddSeg, 0, wx.TOP, BORDER)\nvBoxLeftTop.Add(self.btnRevSeg, 0, wx.TOP, BUT_SPACE)\nvBoxLeftTop.Add(self.btnDelSeg, 0, wx.TOP, BUT_SPACE)\nvBoxLeftTop.Add(self.btnMoveSeg, 0, wx.TOP, BUT_SPACE)\nvBoxLeftTop.Add((-1, 12))\nvBoxLeftTop.Add(self.btnQueueAll, 0, wx.TOP, BUT_SPACE)\nvBoxLeftTop.Add(self.btnDequeueAll, 0, wx.TOP, BUT_SPACE)\nvBoxLeftTop.Add(self.btnPlay, 0, wx.TOP, BUT_SPACE)\nvBoxLeftTop.Add(self.btnPause, 0, wx.TOP, BUT_SPACE)\nvBoxLeftTop.Add(self.btnAudition, 0, wx.TOP, BUT_SPACE)\n\nvBoxLeftBot.Add(self.btnAddEvt, 0)\nvBoxLeftBot.Add(self.btnRevEvt, 0, wx.TOP, BUT_SPACE)\nvBoxLeftBot.Add(self.btnDelEvt, 0, wx.TOP, BUT_SPACE)\nvBoxLeftBot.Add(self.btnMoveEvents, 0, wx.TOP, BUT_SPACE)\n\nleftTopPanel.SetSizer(vBoxLeftTop)\nleftBotPanel.SetSizer(vBoxLeftBot)\n\nvboxLeft = wx.BoxSizer(wx.VERTICAL)\nvboxLeft.Add(leftTopPanel, 1, wx.EXPAND)\nvboxLeft.Add(leftBotPanel, 1, wx.EXPAND)\nvboxLeft.Add((-1, 25))\n\nleftPanel.SetSizer(vboxLeft)\n\nself.log = wx.TextCtrl(rightPanel, -1)\nself.graph = SegmentGraph(rightPanel, size=(-1, 50))\n\nvboxRight = wx.BoxSizer(wx.VERTICAL)\nvboxRight.Add(self.segList, 4, wx.EXPAND | wx.TOP, BORDER)\nvboxRight.Add((-1, 10))\nvboxRight.Add(self.eventList, 3, wx.EXPAND | wx.TOP, BORDER)\nvboxRight.Add((-1, 10))\nvboxRight.Add(self.log, 0, wx.EXPAND)\nvboxRight.Add((-1, 5))\nvboxRight.Add(self.graph, 1, wx.EXPAND)\nvboxRight.Add((-1, 10))\n\nrightPanel.SetSizer(vboxRight)\n\nhboxMain.Add(leftPanel, 0, wx.EXPAND | wx.RIGHT | wx.LEFT, BORDER)\nhboxMain.Add(rightPanel, 1, wx.EXPAND)\nhboxMain.Add((BORDER, -1))\n\npanel.SetSizer(hboxMain)\n\npnlGraph = wx.Panel(leftBotPanel, -1)\ngraphSizer1 = wx.BoxSizer(wx.VERTICAL)\npnlGraph.SetSizer(graphSizer1)\n\ngraphBox = wx.StaticBox(pnlGraph, wx.ID_ANY, label='Graph')\ngraphSizer2 = wx.StaticBoxSizer(graphBox, wx.VERTICAL)\n\nself.chkGraphLabels = wx.CheckBox(pnlGraph, -1, JetDefs.GRAPH_LBLS)\nself.chkGraphClips = wx.CheckBox(pnlGraph, -1, JetDefs.GRAPH_TRIGGER)\nself.chkGraphAppEvts = wx.CheckBox(pnlGraph, -1, JetDefs.GRAPH_APP)\n\ngraphSizer2.Add(self.chkGraphLabels, 0, wx.TOP, BUT_SPACE)\ngraphSizer2.Add(self.chkGraphClips, 0, wx.TOP, BUT_SPACE)\ngraphSizer2.Add(self.chkGraphAppEvts, 0, wx.TOP | wx.BOTTOM, BUT_SPACE)\ngraphSizer1.Add((-1, 10))\ngraphSizer1.Add(graphSizer2)\n\nvBoxLeftBot.Add(pnlGraph, 0, wx.TOP, BUT_SPACE)\n\nself.Bind(wx.EVT_CHECKBOX, self.OnSetGraphOptions, id=self.chkGraphLabels.GetId())\nself.Bind(wx.EVT_CHECKBOX, self.OnSetGraphOptions, id=self.chkGraphClips.GetId())\nself.Bind(wx.EVT_CHECKBOX, self.OnSetGraphOptions, id=self.chkGraphAppEvts.GetId())", "path": "Android.app\\sdk\\tools\\Jet\\JetCreator\\JetCreator.py", "repo_name": "willfarrell/Browsers", "stars": 93, "license": "None", "language": "python", "size": 314209}
{"docstring": "\"\"\"Parse the MIDI file creating a list of properties, tracks,\nand events based on the contents of the file.\n\n\"\"\"\n\n# determine file size - without using os.stat\n", "func_signal": "def ReadFromStream (self, start_offset=0, file_size=None):\n", "code": "if file_size == None:\n\tself.start_offset = start_offset\n\tself.seek(0,2)\n\tfile_size = self.tell() - self.start_offset\n\tself.seek(start_offset,0)\nelse:\n\tfile_size = file_size\n\n# for error recovery\nself.last_good_event = None\nself.error_loc = None\n\n# read the file header - verify it's an SMF file\nbytes = self.read(struct.calcsize(SMF_HEADER_FMT))\nriff_tag, self.hdr_len, self.format, self.num_tracks, self.timebase.ppqn = struct.unpack(SMF_HEADER_FMT, bytes)\nmidi_file_logger.debug('SMF header\\n  Tag:       %s\\n  HeaderLen: %d\\n  Format:    %d\\n  NumTracks: %d\\n  PPQN:      %d\\n' % \\\n\t(riff_tag, self.hdr_len, self.format, self.num_tracks, self.timebase.ppqn))\n\n# sanity check on header\nif (riff_tag != SMF_RIFF_TAG) or (self.format not in range(2)):\n\traise MIDIFileException(self, MSG_NOT_SMF_FILE)\n\n# check for odd header size\nif self.hdr_len + 8 != struct.calcsize(SMF_HEADER_FMT):\n\tself.Warning('SMF file has unusual header size: %d bytes' % self.hdr_len)\n\n# read each of the tracks\noffset = start_offset + self.hdr_len + 8\nself.tracks = []\nself.end_of_file = 0\nfor i in range(self.num_tracks):\n\t#print(\"Track: %d\" % i)\n\n\t# parse the track\n\ttrack = MIDITrack()\n\tlength = track.ReadFromStream(self, offset, file_size)\n\ttrack.trackNum = i\n\t\n\tself.tracks.append(track)\n\n\t# calculate offset to next track\n\toffset += length + 8\n\n\t# determine time of last event\n\tself.end_of_file = max(self.end_of_file, track.end_of_track)\n\n# if start_offset is zero, the final offset should match the file length\nif (offset - start_offset) != file_size:\n\tself.Warning('SMF file size is incorrect - should be %d, was %d' % (file_size, offset))", "path": "Android.app\\sdk\\tools\\Jet\\JetCreator\\midifile.py", "repo_name": "willfarrell/Browsers", "stars": 93, "license": "None", "language": "python", "size": 314209}
{"docstring": "\"\"\"\nReturn the number of documents that match a given find query.\n\nEXAMPLES::\n\n    >>>                 \n\"\"\"\n", "func_signal": "def count(self, *args, **kwds):\n", "code": "kwds['_count'] = True\ncmd = self._find_cmd(*args, **kwds)\nreturn self.database(cmd)[0]", "path": "nosqlite.py", "repo_name": "williamstein/nosqlite", "stars": 69, "license": "None", "language": "python", "size": 251}
{"docstring": "\"\"\"\nEXAMPLES::\n\n    >>> s = server(); c = client(s.port)\n    >>> c._coerce_(False)\n    0\n    >>> c._coerce_(True)\n    1\n    >>> c._coerce_('lkjdf')\n    'lkjdf'\n    >>> c._coerce_(2.5)\n    2.5\n    >>> c._coerce_([1,2])\n    '__pickleeJxrYIot...'\n\"\"\"\n", "func_signal": "def _coerce_(self, x):\n", "code": "if isinstance(x, bool):\n    x = int(x)\nelif isinstance(x, (str, int, long, float)):\n    pass\nelif x is None:\n    pass\nelif is_Integer(x) and x.nbits()<32:\n    x = int(x)\nelif is_RealNumber(x) and x.prec()==53:\n    return float(x)\nelif isinstance(x, unicode):\n    return str(x)\nelse:\n    x = '__pickle' + base64.b64encode(zlib.compress(cPickle.dumps(x, 2)))\nreturn x", "path": "nosqlite.py", "repo_name": "williamstein/nosqlite", "stars": 69, "license": "None", "language": "python", "size": 251}
{"docstring": "\"\"\"\nINPUTS:\n- database -- a Database object\n- name -- string, name of this collection\n\nEXAMPLES::\n\n    >>> s = server(); db = client(s.port).database\n    >>> C = db.mycoll; C\n    Collection 'database.mycoll'\n    >>> type(C)\n    <class '__main__.Collection'>\n    >>> C.database\n    Database 'database'\n    >>> C.name\n    'mycoll'\n\"\"\"\n", "func_signal": "def __init__(self, database, name):\n", "code": "self.database = database\nself.name = str(name)", "path": "nosqlite.py", "repo_name": "williamstein/nosqlite", "stars": 69, "license": "None", "language": "python", "size": 251}
{"docstring": "\"\"\"\nEXAMPLES::\n\n    >>> \n\"\"\"\n", "func_signal": "def drop_index(self, **kwds):\n", "code": "cols, index_name = self._index_pattern(kwds)\ncmd = 'DROP INDEX IF EXISTS \"%s\"'%index_name\nself.database(cmd)", "path": "nosqlite.py", "repo_name": "williamstein/nosqlite", "stars": 69, "license": "None", "language": "python", "size": 251}
{"docstring": "\"\"\"\nCopy documents from self into the given collection.  The query\nand fields are specified exactly as for the find command.\n\nINPUT:\n- collection -- a Collection or string (that names a collection).\n\nEXAMPLES::\n\n    >>> s = server(); db = client(s.port).database; C = db.C\n    >>> C.insert([{'a':5, 'b':10, 'x':15}, {'x':20, 'y':30}])\n    >>> C.copy('foo')\n    >>> list(db.foo)\n    [{'y': 30, 'x': 20}, {'a': 5, 'x': 15, 'b': 10}]\n\"\"\"\n", "func_signal": "def copy(self, collection, query='', fields=None, **kwds):\n", "code": "if isinstance(collection, str):\n    collection = self.database.__getattr__(collection)\n# which columns we want to copy\nfields = self._columns() if fields is None else fields\n# which are already in other collection\nother = collection._columns()\n# which are missing\ncols = set(fields).difference(other)\nif len(other) == 0:\n    # other collection hasn't been created yet\n    collection._create(cols)\nelif cols:\n    # need to add some columns to other collection\n    collection._add_columns(cols)\n# now recipient table has all needed columns, so do the insert in one go.\nc = ','.join(['\"%s\"'%x for x in fields])\ncmd = 'INSERT INTO \"%s\" (%s) SELECT %s FROM \"%s\" %s'%(\n    collection.name, c, c, self.name, self._where_clause(query, kwds))\nself.database(cmd)", "path": "nosqlite.py", "repo_name": "williamstein/nosqlite", "stars": 69, "license": "None", "language": "python", "size": 251}
{"docstring": "\"\"\"\nA list of all of the collections in this database.\n\nNOTE: This is not a list of the names of collections but of\nthe actual collections themselves.\n\nEXAMPLES::\n\n    >>> s = server(); db = client(s.port).database\n    >>> db.col1.insert({'a':0}); db.my_col2.insert({'a':10})\n    >>> v = db.collections(); v\n    [Collection 'database.col1', Collection 'database.my_col2']\n    >>> v[0].find_one()\n    {'a': 0}\n\"\"\"\n", "func_signal": "def collections(self):\n", "code": "cmd = \"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name\"\nreturn [Collection(self, x[0]) for x in self(cmd)]", "path": "nosqlite.py", "repo_name": "williamstein/nosqlite", "stars": 69, "license": "None", "language": "python", "size": 251}
{"docstring": "# first, call the original implementation which returns\n# True if all OK so far\n", "func_signal": "def parse_request(myself):\n", "code": "if SimpleXMLRPCRequestHandler.parse_request(myself):\n    # next we authenticate\n    if self.authenticate(myself.headers):\n        return True\n    else:\n        # if authentication fails, tell the client\n        myself.send_error(401, 'Authentication failed')\nreturn False", "path": "nosqlite.py", "repo_name": "williamstein/nosqlite", "stars": 69, "license": "None", "language": "python", "size": 251}
{"docstring": "\"\"\"\nEXAMPLES::\n\n    >>> \n\"\"\"\n", "func_signal": "def _add_columns(self, new_columns):\n", "code": "self._validate_column_names(new_columns)        \nfor col in new_columns:\n    try:\n        self.database('ALTER TABLE \"%s\" ADD COLUMN \"%s\"'%(self.name, col))\n    except xmlrpclib.Fault:\n        # TODO: make it into a single transaction...\n        # The above could safely fail if another client tried\n        # to add at the same time and made the relevant\n        # column. Ignore error here and deal with it later.\n        pass", "path": "nosqlite.py", "repo_name": "williamstein/nosqlite", "stars": 69, "license": "None", "language": "python", "size": 251}
{"docstring": "\"\"\"\nImport data into self from the given csvfile.  If columns is\nNone, then the first row of the cvsfile must be headers that\nspecify the keys.  If columns is not None, then the first row\nis assumed to be data. \n\nINPUT:\n- csvfile -- string or readable file\n- delimiter -- string (default: ' ')\n- quotechar -- string (default: '|')\n- columns -- None or list of strings (column headings)\n\nEXAMPLES::\n\n    >>>         \n\"\"\"\n", "func_signal": "def import_csv(self, csvfile, columns=None, delimiter=' ', quotechar='|'):\n", "code": "if isinstance(csvfile, str):\n    csvfile = open(csvfile, 'rb')\nimport csv\nR = csv.reader(csvfile, delimiter=delimiter, quotechar=quotechar)\nif columns is None:\n    columns = R.next()\nd = []\nfor x in R:\n    z = {}\n    for i in range(len(x)):\n        y = x[i]\n        if y != '':\n            if y.isdigit():\n                y = eval(y)\n            else:\n                v = y.split('.')\n                if len(v) == 2 and v[0].isdigit() and v[1].isdigit():\n                    y = eval(y)\n            z[columns[i]] = y\n    d.append(z)\nself.insert(d)", "path": "nosqlite.py", "repo_name": "williamstein/nosqlite", "stars": 69, "license": "None", "language": "python", "size": 251}
{"docstring": "\"\"\"\nEXAMPLES::\n\n    >>> \n\"\"\"\n", "func_signal": "def delete(self, query='', **kwds):\n", "code": "if not query and len(kwds) == 0:\n    if len(self._columns()) == 0:\n        # nothing to do, since table wasn't created yet.\n        return\n    # just drop the table\n    cmd = 'DROP TABLE \"%s\"'%self.name\nelse:\n    cmd = 'DELETE FROM \"%s\" %s'%(self.name, self._where_clause(query, kwds))\nself.database(cmd)", "path": "nosqlite.py", "repo_name": "williamstein/nosqlite", "stars": 69, "license": "None", "language": "python", "size": 251}
{"docstring": "\"\"\"\nReturn the number of documents in this collection.\n\nEXAMPLES::\n\n    >>> s = server(); C = client(s.port).database.mycol\n    >>> len(C)\n    0\n    >>> C.insert([{'a':i} for i in range(100)])\n    >>> len(C)\n    100\n\"\"\"\n", "func_signal": "def __len__(self):\n", "code": "try:\n    cmd = 'SELECT COUNT(*) FROM \"%s\"'%self.name\n    return int(self.database(cmd)[0][0])\nexcept RuntimeError:\n    if len(self._columns()) == 0:\n        return 0\n    raise", "path": "nosqlite.py", "repo_name": "williamstein/nosqlite", "stars": 69, "license": "None", "language": "python", "size": 251}
{"docstring": "\"\"\"\nEXAMPLES::\n\n    >>> \n\"\"\"\n", "func_signal": "def drop_indexes(self):\n", "code": "cmd = \"SELECT * FROM sqlite_master WHERE type='index' and tbl_name='%s'\"%self.name\nfor x in self.database(cmd):\n    if x[1].startswith('idx___'):\n        self.database('DROP INDEX IF EXISTS \"%s\"'%x[1])", "path": "nosqlite.py", "repo_name": "williamstein/nosqlite", "stars": 69, "license": "None", "language": "python", "size": 251}
{"docstring": "\"\"\"\nEXAMPLES::\n\n    >>> \n\"\"\"\n", "func_signal": "def _columns(self):\n", "code": "a = self.database('PRAGMA table_info(\"%s\")'%self.name)\nif a is None:\n    return []\nreturn [x[1] for x in a]", "path": "nosqlite.py", "repo_name": "williamstein/nosqlite", "stars": 69, "license": "None", "language": "python", "size": 251}
{"docstring": "\"\"\"\nCreate this table for the first time with the given columns.\n\nINPUT:\n- columns -- a nonempty list of strings\n\nEXAMPLES::\n\n    >>> s = server(); C = client(s.port).database.mycol\n    >>> C._create(['a', 'b', 'c'])\n    >>> C.columns()\n    ['a', 'b', 'c']\n\"\"\"\n", "func_signal": "def _create(self, columns):\n", "code": "self._validate_column_names(columns)\nself.database('CREATE TABLE IF NOT EXISTS \"%s\" (%s)'%(self.name, ', '.join('\"%s\"'%s for s in columns)))", "path": "nosqlite.py", "repo_name": "williamstein/nosqlite", "stars": 69, "license": "None", "language": "python", "size": 251}
{"docstring": "\"\"\"\nEXAMPLES::\n\n    >>> s = server(); c = client(s.port)\n    >>> z = c._coerce_([1,2])\n    >>> c._coerce_back_(z)\n    [1, 2]\n\"\"\"\n", "func_signal": "def _coerce_back_(self, x):\n", "code": "if isinstance(x, (str, unicode)) and x.startswith('__pickle'):\n    return cPickle.loads(zlib.decompress(base64.b64decode(x[8:])))\nreturn x", "path": "nosqlite.py", "repo_name": "williamstein/nosqlite", "stars": 69, "license": "None", "language": "python", "size": 251}
{"docstring": "\"\"\"\nExport all documents in self to the given csvfile.  The first row\nof the cvsfile will be headers that specify the keys.\n\nINPUT:\n- csvfile -- string or readable file\n- delimiter -- string (default: ' ')\n- quotechar -- string (default: '|')\n- order_by -- string (default: None)\n\nEXAMPLES::\n\n    >>> \n\"\"\"\n", "func_signal": "def export_csv(self, csvfile, delimiter=' ', quotechar='|', order_by=None, write_columns=True):\n", "code": "if isinstance(csvfile, str):\n    csvfile = open(csvfile, 'wb')\nimport csv\nW = csv.writer(csvfile, delimiter=delimiter, quotechar=quotechar, quoting=csv.QUOTE_MINIMAL)\ncmd = 'SELECT * FROM \"%s\" '%self.name\nif order_by is not None:\n    cmd += ' ORDER BY %s'%order_by\nif write_columns:\n    W.writerow(self.columns())\nfor x in self.database(cmd):\n    W.writerow(['%r'%a for a in x])", "path": "nosqlite.py", "repo_name": "williamstein/nosqlite", "stars": 69, "license": "None", "language": "python", "size": 251}
{"docstring": "\"\"\"\nGroup the list d into a list of sublists with constant keys.\n\nINPUT:\n- d -- a list of dictionaries\nOUTPUT:\n- a list of lists of dictionaries with constant keys\n\nEXAMPLES::\n\n    >>> from nosqlite import _constant_key_grouping\n    >>> _constant_key_grouping([{'a':5,'b':7}, {'a':10,'c':4}, {'a':5, 'b':8}])\n    [[{'a': 5, 'b': 7}, {'a': 5, 'b': 8}], [{'a': 10, 'c': 4}]]\n\"\"\"\n", "func_signal": "def _constant_key_grouping(d):\n", "code": "x = {}\nfor a in d:\n    k = tuple(a.keys())\n    if x.has_key(k):\n        x[k].append(a)\n    else:\n        x[k] = [a]\nreturn x.values()", "path": "nosqlite.py", "repo_name": "williamstein/nosqlite", "stars": 69, "license": "None", "language": "python", "size": 251}
{"docstring": "\"\"\"\nRename this collection to the given new name.\n\nINPUT:\n- new_name -- string\n\nEXAMPLES::\n\n    >>> s = server(); db = client(s.port).database; C = db.C\n    >>> C.name\n    'C'\n    >>> C.insert([{'a':5, 'b':10, 'x':15}, {'x':20, 'y':30}])\n    >>> C.rename('collection2')\n    >>> C.name\n    'collection2'\n    >>> C\n    Collection 'database.collection2'\n    >>> C = db.collection2\n    >>> list(C)\n    [{'y': 30, 'x': 20}, {'a': 5, 'x': 15, 'b': 10}]\n    >>> list(db.C)\n    []\n\"\"\"\n", "func_signal": "def rename(self, new_name):\n", "code": "cmd = \"ALTER TABLE %s RENAME TO %s\"%(self.name, new_name)\nself.database(cmd)\nself.name = new_name", "path": "nosqlite.py", "repo_name": "williamstein/nosqlite", "stars": 69, "license": "None", "language": "python", "size": 251}
{"docstring": "\"\"\"\nEXAMPLES::\n\n    >>> \n\"\"\"\n", "func_signal": "def indexes(self):\n", "code": "cmd = \"SELECT * FROM sqlite_master WHERE type='index' and tbl_name='%s' ORDER BY name\"%self.name\nv = []\nfor x in self.database(cmd):\n    d = {}\n    for a in x[1].split('___')[2:]:\n        if a.endswith('ASC'):\n            d[a[:-3]] = 1\n        else:\n            d[a[:-4]] = -1\n    v.append(d)\nreturn v", "path": "nosqlite.py", "repo_name": "williamstein/nosqlite", "stars": 69, "license": "None", "language": "python", "size": 251}
{"docstring": "\"\"\"\nEXAMPLES::\n\n    >>> s = server(); c = client(s.port)\n    >>> db = c.database; db\n    Database 'database'\n    >>> type(db)\n    <class '__main__.Database'>\n    >>> db.client\n    nosqlite client connected to port ...\n    >>> db.name\n    'database'\n\"\"\"\n", "func_signal": "def __init__(self, client, name):\n", "code": "self.client = client\nself.name = str(name)", "path": "nosqlite.py", "repo_name": "williamstein/nosqlite", "stars": 69, "license": "None", "language": "python", "size": 251}
{"docstring": "\"\"\"Pretty print the dictionary 'params'\n\nParameters\n----------\nparams: dict\n    The dictionary to pretty print\n\noffset: int\n    The offset in characters to add at the begin of each line.\n\nprinter:\n    The function to convert entries to strings, typically\n    the builtin str or repr\n\n\"\"\"\n# Do a multi-line justified repr:\n", "func_signal": "def _pprint(params, offset=0, printer=repr):\n", "code": "options = np.get_printoptions()\nnp.set_printoptions(precision=5, threshold=64, edgeitems=2)\nparams_list = list()\nthis_line_length = offset\nline_sep = ',\\n' + (1 + offset // 2) * ' '\nfor i, (k, v) in enumerate(sorted((params.iteritems()))):\n    if type(v) is float:\n        # use str for representing floating point numbers\n        # this way we get consistent representation across\n        # architectures and versions.\n        this_repr = '%s=%s' % (k, str(v))\n    else:\n        # use repr of the rest\n        this_repr = '%s=%s' % (k, printer(v))\n    if len(this_repr) > 500:\n        this_repr = this_repr[:300] + '...' + this_repr[-100:]\n    if i > 0:\n        if (this_line_length + len(this_repr) >= 75 or '\\n' in this_repr):\n            params_list.append(line_sep)\n            this_line_length = len(line_sep)\n        else:\n            params_list.append(', ')\n            this_line_length += 2\n    params_list.append(this_repr)\n    this_line_length += len(this_repr)\n\nnp.set_printoptions(**options)\nlines = ''.join(params_list)\n# Strip trailing space to avoid nightmare in doctests\nlines = '\\n'.join(l.rstrip(' ') for l in lines.split('\\n'))\nreturn lines", "path": "crab\\utils\\format.py", "repo_name": "python-recsys/crab", "stars": 126, "license": "other", "language": "python", "size": 583}
{"docstring": "\"\"\" Check that the pairwise Manhattan distances computation\"\"\"\n#Idepontent Test\n", "func_signal": "def test_manhattan_distances():\n", "code": "X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nD = manhattan_distances(X, X)\nassert_array_almost_equal(D, [[1.]])\n\n#Vector x Non Vector\nX = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nY = [[]]\nassert_raises(ValueError, manhattan_distances, X, Y)\n\n#Vector A x Vector B\nX = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nY = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0]]\nD = manhattan_distances(X, Y)\nassert_array_almost_equal(D, [[0.25]])\n\n#BUG FIX: How to fix for multi-dimm arrays\n\n#Vector N x 1\nX = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nY = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0]]\nD = manhattan_distances(X, Y)\nassert_array_almost_equal(D, [[0.25], [0.25]])\n\n#N-Dimmensional Vectors\nX = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nY = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nD = manhattan_distances(X, Y)\nassert_array_almost_equal(D, [[1., 1.], [1., 1.]])\n\nX = [[0, 1], [1, 1]]\nD = manhattan_distances(X, X)\nassert_array_almost_equal(D, [[1., 0.5], [0.5, 1.]])\n\nX = [[0, 1], [1, 1]]\nY = [[0, 0]]\nD = manhattan_distances(X, Y)\nassert_array_almost_equal(D, [[0.5], [0.]])\n\n#Test Sparse Matrices\nX = csr_matrix(X)\nY = csr_matrix(Y)\nassert_raises(ValueError, manhattan_distances, X, Y)", "path": "crab\\metrics\\tests\\test_pairwise.py", "repo_name": "python-recsys/crab", "stars": 126, "license": "other", "language": "python", "size": 583}
{"docstring": "\"\"\" Turns a numpy matrix (any n-dimensional array) into tuples.\"\"\"\n", "func_signal": "def tuplify(X):\n", "code": "s = X.shape\nif len(s) > 1:\n    # Tuplify each sub-array in the input.\n    return tuple(tuplify(row) for row in X)\nelse:\n    # Single dimension input, just return tuple of contents.\n    return tuple(r for r in X)", "path": "crab\\metrics\\tests\\test_pairwise.py", "repo_name": "python-recsys/crab", "stars": 126, "license": "other", "language": "python", "size": 583}
{"docstring": "\"\"\"Dot product that handle the sparse matrix case correctly\"\"\"\n", "func_signal": "def safe_sparse_dot(a, b, dense_output=False):\n", "code": "from scipy import sparse\nif sparse.issparse(a) or sparse.issparse(b):\n    ret = a * b\n    if dense_output and hasattr(ret, \"toarray\"):\n        ret = ret.toarray()\n    return ret\nelse:\n    return np.dot(a, b)", "path": "crab\\utils\\extmath.py", "repo_name": "python-recsys/crab", "stars": 126, "license": "other", "language": "python", "size": 583}
{"docstring": "\"\"\" Ensures that checks return valid sparse matrices. \"\"\"\n", "func_signal": "def test_check_sparse_arrays():\n", "code": "rng = np.random.RandomState(0)\nXA = rng.random_sample((5, 4))\nXA_sparse = csr_matrix(XA)\nXB = rng.random_sample((5, 4))\nXB_sparse = csr_matrix(XB)\nXA_checked, XB_checked = check_pairwise_arrays(XA_sparse, XB_sparse)\nassert_equal(XA_sparse, XA_checked)\nassert_equal(XB_sparse, XB_checked)", "path": "crab\\metrics\\tests\\test_pairwise.py", "repo_name": "python-recsys/crab", "stars": 126, "license": "other", "language": "python", "size": 583}
{"docstring": "\"\"\" Check that the pairwise Pearson distances computation\"\"\"\n#Idepontent Test\n", "func_signal": "def test_pearson_correlation():\n", "code": "X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nD = pearson_correlation(X, X)\nassert_array_almost_equal(D, [[1.]])\n\n#Vector x Non Vector\nX = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nY = [[]]\nassert_raises(ValueError, pearson_correlation, X, Y)\n\n#Vector A x Vector B\nX = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nY = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0]]\nD = pearson_correlation(X, Y)\nassert_array_almost_equal(D, [[0.3960590]])\n\n#Vector N x 1\nX = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nY = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0]]\nD = pearson_correlation(X, Y)\nassert_array_almost_equal(D, [[0.3960590], [0.3960590]])\n\n#N-Dimmensional Vectors\nX = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nY = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nD = pearson_correlation(X, Y)\nassert_array_almost_equal(D, [[0.3960590, 1.], [0.3960590, 1.]])\n\nX = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [3.0, 3.5, 1.5, 5.0, 3.5, 3.0]]\nD = pearson_correlation(X, X)\nassert_array_almost_equal(D, [[1., 0.39605902], [0.39605902, 1.]])\n\nX = [[1.0, 0.0], [1.0, 1.0]]\nY = [[0.0, 0.0]]\nD = pearson_correlation(X, Y)\nassert_array_almost_equal(D, [[np.nan], [np.nan]])\n\n#Test Sparse Matrices\nX = csr_matrix(X)\nY = csr_matrix(Y)\nassert_raises(ValueError, pearson_correlation, X, Y)", "path": "crab\\metrics\\tests\\test_pairwise.py", "repo_name": "python-recsys/crab", "stars": 126, "license": "other", "language": "python", "size": 583}
{"docstring": "\"\"\"Set the parameters of the recommenders.\n\nThe method works on simple reccommenders as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n\nReturns\n-------\nself\n\"\"\"\n", "func_signal": "def set_params(self, **params):\n", "code": "if not params:\n    # Simple optimisation to gain speed (inspect is slow)\n    return self\nvalid_params = self.get_params(deep=True)\nfor key, value in params.iteritems():\n    split = key.split('__', 1)\n    if len(split) > 1:\n        # nested objects case\n        name, sub_name = split\n        if not name in valid_params:\n            raise ValueError('Invalid param %s for reccommender %s' %\n                             (name, self))\n        sub_object = valid_params[name]\n        sub_object.set_params(**{sub_name: value})\n    else:\n        # simple objects case\n        if not key in valid_params:\n            raise ValueError('Invalid param %s ' 'for reccommender %s'\n                             % (key, self.__class__.__name__))\n        setattr(self, key, value)\nreturn self", "path": "crab\\base.py", "repo_name": "python-recsys/crab", "stars": 126, "license": "other", "language": "python", "size": 583}
{"docstring": "\"\"\"Like numpy.atleast_2d, but converts sparse matrices to CSR format\n\nAlso, converts np.matrix to np.ndarray.\n\"\"\"\n", "func_signal": "def atleast2d_or_csr(X, dtype=None, order=None, copy=False):\n", "code": "return _atleast2d_or_sparse(X, dtype, order, copy, sparse.csr_matrix,\n                            \"tocsr\")", "path": "crab\\utils\\validation.py", "repo_name": "python-recsys/crab", "stars": 126, "license": "other", "language": "python", "size": 583}
{"docstring": "\"\"\"Convert X to an array or sparse matrix.\n\nPrevents copying X when possible; sparse matrices are passed through.\"\"\"\n", "func_signal": "def safe_asarray(X, dtype=None, order=None):\n", "code": "if sparse.issparse(X):\n    assert_all_finite(X.data)\nelse:\n    X = np.asarray(X, dtype, order)\n    assert_all_finite(X)\nreturn X", "path": "crab\\utils\\validation.py", "repo_name": "python-recsys/crab", "stars": 126, "license": "other", "language": "python", "size": 583}
{"docstring": "\"\"\"Like numpy.atleast_2d, but converts sparse matrices to CSC format.\n\nAlso, converts np.matrix to np.ndarray.\n\"\"\"\n", "func_signal": "def atleast2d_or_csc(X, dtype=None, order=None, copy=False):\n", "code": "return _atleast2d_or_sparse(X, dtype, order, copy, sparse.csc_matrix,\n                            \"tocsc\")", "path": "crab\\utils\\validation.py", "repo_name": "python-recsys/crab", "stars": 126, "license": "other", "language": "python", "size": 583}
{"docstring": "\"\"\" Ensure an error is raised if the dimensions are different. \"\"\"\n", "func_signal": "def test_check_different_dimensions():\n", "code": "XA = np.resize(np.arange(45), (5, 9))\nXB = np.resize(np.arange(32), (4, 8))\nassert_raises(ValueError, check_pairwise_arrays, XA, XB)", "path": "crab\\metrics\\tests\\test_pairwise.py", "repo_name": "python-recsys/crab", "stars": 126, "license": "other", "language": "python", "size": 583}
{"docstring": "\"\"\"Get parameter names for the estimator\"\"\"\n", "func_signal": "def _get_param_names(cls):\n", "code": "try:\n    # fetch the constructor or the original constructor before\n    # deprecation wrapping if any\n    init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n\n    # introspect the constructor arguments to find the model parameters\n    # to represent\n    args, varargs, kw, default = inspect.getargspec(init)\n    if not varargs is None:\n        raise RuntimeError('crab recommenders should always '\n                           'specify their parameters in the signature'\n                           ' of their init (no varargs).')\n    # Remove 'self'\n    # XXX: This is going to fail if the init is a staticmethod, but\n    # who would do this?\n    args.pop(0)\nexcept TypeError:\n    # No explicit __init__\n    args = []\nargs.sort()\nreturn args", "path": "crab\\base.py", "repo_name": "python-recsys/crab", "stars": 126, "license": "other", "language": "python", "size": 583}
{"docstring": "\"\"\"Check that the pairwise euclidian distances computation\"\"\"\n#Idepontent Test\n", "func_signal": "def test_euclidean_distances():\n", "code": "X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nD = euclidean_distances(X, X)\nassert_array_almost_equal(D, [[0.]])\n\nX = [[3.0, -2.0]]\nD = euclidean_distances(X, X, inverse=True)\nassert_array_almost_equal(D, [[1.]])\n\nX = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nD = euclidean_distances(X, X, inverse=False)\nassert_array_almost_equal(D, [[0.]])\n\n#Inverse Test\nX = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nD = euclidean_distances(X, X, inverse=True)\nassert_array_almost_equal(D, [[1.]])\n\n#Vector x Non Vector\nX = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nY = [[]]\nassert_raises(ValueError, euclidean_distances, X, Y)\n\n#Vector A x Vector B\nX = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nY = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0]]\nD = euclidean_distances(X, Y)\nassert_array_almost_equal(D, [[2.39791576]])\n\n#Inverse vector (mahout check)\nX = [[3.0, -2.0]]\nY = [[-3.0, 2.0]]\nD = euclidean_distances(X, Y, inverse=True)\nassert_array_almost_equal(D, [[0.13736056]])\n\n#Inverse vector (oreilly check)\nX = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nY = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0]]\nD = euclidean_distances(X, Y, inverse=True, squared=True)\nassert_array_almost_equal(D, [[0.14814815]])\n\n#Vector N x 1\nX = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nY = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0]]\nD = euclidean_distances(X, Y)\nassert_array_almost_equal(D, [[2.39791576], [2.39791576]])\n\n#N-Dimmensional Vectors\nX = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nY = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nD = euclidean_distances(X, Y)\nassert_array_almost_equal(D, [[2.39791576, 0.], [2.39791576, 0.]])\n\nX = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [3.0, 3.5, 1.5, 5.0, 3.5, 3.0]]\nD = euclidean_distances(X, X)\nassert_array_almost_equal(D, [[0., 2.39791576], [2.39791576, 0.]])\n\nX = [[1.0, 0.0], [1.0, 1.0]]\nY = [[0.0, 0.0]]\nD = euclidean_distances(X, Y)\nassert_array_almost_equal(D, [[1.], [1.41421356]])\n\n#Test Sparse Matrices\nX = csr_matrix(X)\nY = csr_matrix(Y)\nD = euclidean_distances(X, Y)\nassert_array_almost_equal(D, [[1.], [1.41421356]])", "path": "crab\\metrics\\tests\\test_pairwise.py", "repo_name": "python-recsys/crab", "stars": 126, "license": "other", "language": "python", "size": 583}
{"docstring": "\"\"\" Check that the pairwise Pearson distances computation\"\"\"\n#Idepontent Test\n", "func_signal": "def test_adjusted_cosine():\n", "code": "X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nEFV = [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\nD = adjusted_cosine(X, X, EFV)\nassert_array_almost_equal(D, [[1.]])\n\n#Vector x Non Vector\nX = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nY = [[]]\nEFV = [[]]\nassert_raises(ValueError, adjusted_cosine, X, Y, EFV)\n\n#Vector A x Vector B\nX = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nY = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0]]\nEFV = [[2.0, 2.0, 2.0, 2.0, 2.0, 2.0]]\nD = adjusted_cosine(X, Y, EFV)\nassert_array_almost_equal(D, [[0.80952381]])\n\n#Vector N x 1\nX = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nY = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0]]\nEFV = [[2.0, 2.0, 2.0, 2.0, 2.0, 2.0]]\nD = adjusted_cosine(X, Y, EFV)\nassert_array_almost_equal(D, [[0.80952381], [0.80952381]])\n\n#N-Dimmensional Vectors\nX = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nY = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]\nEFV = [[2.0, 2.0, 2.0, 2.0, 2.0, 2.0]]\nD = adjusted_cosine(X, Y, EFV)\nassert_array_almost_equal(D, [[0.80952381, 1.], [0.80952381, 1.]])\n\nX = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [3.0, 3.5, 1.5, 5.0, 3.5, 3.0]]\nEFV = [[2.0, 2.0, 2.0, 2.0, 2.0, 2.0]]\nD = adjusted_cosine(X, X, EFV)\nassert_array_almost_equal(D, [[1., 0.80952381], [0.80952381, 1.]])\n\nX = [[1.0, 0.0], [1.0, 1.0]]\nY = [[0.0, 0.0]]\nEFV = [[0.0, 0.0]]\nD = adjusted_cosine(X, Y, EFV)\nassert_array_almost_equal(D, [[np.nan], [np.nan]])\n\n#Test Sparse Matrices\nX = csr_matrix(X)\nY = csr_matrix(Y)\nEFV = csr_matrix(EFV)\nassert_raises(ValueError, adjusted_cosine, X, Y, EFV)", "path": "crab\\metrics\\tests\\test_pairwise.py", "repo_name": "python-recsys/crab", "stars": 126, "license": "other", "language": "python", "size": 583}
{"docstring": "\"\"\" Ensures that checks return valid tuples. \"\"\"\n", "func_signal": "def test_check_tuple_input():\n", "code": "rng = np.random.RandomState(0)\nXA = rng.random_sample((5, 4))\nXA_tuples = tuplify(XA)\nXB = rng.random_sample((5, 4))\nXB_tuples = tuplify(XB)\nXA_checked, XB_checked = check_pairwise_arrays(XA_tuples, XB_tuples)\nassert_equal(XA_tuples, XA_checked)\nassert_equal(XB_tuples, XB_checked)", "path": "crab\\metrics\\tests\\test_pairwise.py", "repo_name": "python-recsys/crab", "stars": 126, "license": "other", "language": "python", "size": 583}
{"docstring": "\"\"\"Returns at least 2-d array with data from X\"\"\"\n", "func_signal": "def array2d(X, dtype=None, order=None, copy=False):\n", "code": "if sparse.issparse(X):\n    raise TypeError('A sparse matrix was passed, but dense data '\n                    'is required. Use X.toarray() to convert to dense.')\nX_2d = np.asarray(np.atleast_2d(X), dtype=dtype, order=order)\n_assert_all_finite(X_2d)\nif X is X_2d and copy:\n    X_2d = safe_copy(X_2d)\nreturn X_2d", "path": "crab\\utils\\validation.py", "repo_name": "python-recsys/crab", "stars": 126, "license": "other", "language": "python", "size": 583}
{"docstring": "\"\"\" Check that the pairwise Tanimoto distances computation\"\"\"\n#Idepontent Test\n", "func_signal": "def test_tanimoto_distances():\n", "code": "X = [['a', 'b', 'c']]\nD = tanimoto_coefficient(X, X)\nassert_array_almost_equal(D, [[1.]])\n\n#Vector x Non Vector\nX = [['a', 'b', 'c']]\nY = [[]]\nD = tanimoto_coefficient(X, Y)\nassert_array_almost_equal(D, [[0.]])\n\n#Vector A x Vector B\nX = [[1, 2, 3, 4]]\nY = [[2, 3]]\nD = tanimoto_coefficient(X, Y)\nassert_array_almost_equal(D, [[0.5]])\n\n#BUG FIX: How to fix for multi-dimm arrays\n\n#Vector N x 1\nX = [['a', 'b', 'c', 'd'], ['e', 'f', 'g']]\nY = [['a', 'b', 'c', 'k']]\nD = tanimoto_coefficient(X, Y)\nassert_array_almost_equal(D, [[0.6], [0.]])\n\n#N-Dimmensional Vectors\nX = [['a', 'b', 'c', 'd'], ['e', 'f', 'g']]\nY = [['a', 'b', 'c', 'd'], ['e', 'f', 'g']]\nD = tanimoto_coefficient(X, Y)\nassert_array_almost_equal(D, [[1., 0.], [0., 1.]])\n\nX = [[0, 1], [1, 2]]\nD = tanimoto_coefficient(X, X)\nassert_array_almost_equal(D, [[1., 0.33333333], [0.33333333, 1.]])\n\nX = [[0, 1], [1, 2]]\nY = [[0, 3]]\nD = tanimoto_coefficient(X, Y)\nassert_array_almost_equal(D, [[0.3333333], [0.]])\n\n#Test Sparse Matrices\nX = csr_matrix(X)\nY = csr_matrix(Y)\nassert_raises(ValueError, tanimoto_coefficient, X, Y)", "path": "crab\\metrics\\tests\\test_pairwise.py", "repo_name": "python-recsys/crab", "stars": 126, "license": "other", "language": "python", "size": 583}
{"docstring": "\"\"\"Get parameters for the recommender\n\nParameters\n----------\ndeep: boolean, optional\n    If True, will return the parameters for this reccommender and\n    contained subobjects that are recommenders.\n\nReturns\n-------\nparams : mapping of string to any\n    Parameter names mapped to their values.\n\"\"\"\n", "func_signal": "def get_params(self, deep=True):\n", "code": "out = dict()\nfor key in self._get_param_names():\n    # catch deprecation warnings\n    with warnings.catch_warnings(record=True) as w:\n        value = getattr(self, key, None)\n    if len(w) and w[0].category == DeprecationWarning:\n        # if the parameter is deprecated, don't show it\n        continue\n\n    # XXX: should we rather test if instance of recommender?\n    if deep and hasattr(value, 'get_params'):\n        deep_items = value.get_params().items()\n        out.update((key + '__' + k, val) for k, val in deep_items)\n    out[key] = value\nreturn out", "path": "crab\\base.py", "repo_name": "python-recsys/crab", "stars": 126, "license": "other", "language": "python", "size": 583}
{"docstring": "\"\"\" Check that the pairwise Jaccard distances computation\"\"\"\n#Idepontent Test\n", "func_signal": "def test_jaccard_distances():\n", "code": "X = [['a', 'b', 'c']]\nD = jaccard_coefficient(X, X)\nassert_array_almost_equal(D, [[1.]])\n\n#Vector x Non Vector\nX = [['a', 'b', 'c']]\nY = [[]]\nD = jaccard_coefficient(X, Y)\nassert_array_almost_equal(D, [[0.]])\n\n#Vector A x Vector B\nX = [[1, 2, 3, 4]]\nY = [[2, 3]]\nD = jaccard_coefficient(X, Y)\nassert_array_almost_equal(D, [[0.5]])\n\n#BUG FIX: How to fix for multi-dimm arrays\n\n#Vector N x 1\nX = [['a', 'b', 'c', 'd'], ['e', 'f', 'g']]\nY = [['a', 'b', 'c', 'k']]\nD = jaccard_coefficient(X, Y)\nassert_array_almost_equal(D, [[0.6], [0.]])\n\n#N-Dimmensional Vectors\nX = [['a', 'b', 'c', 'd'], ['e', 'f', 'g']]\nY = [['a', 'b', 'c', 'd'], ['e', 'f', 'g']]\nD = jaccard_coefficient(X, Y)\nassert_array_almost_equal(D, [[1., 0.], [0., 1.]])\n\nX = [[0, 1], [1, 2]]\nD = jaccard_coefficient(X, X)\nassert_array_almost_equal(D, [[1., 0.33333333], [0.33333333, 1.]])\n\nX = [[0, 1], [1, 2]]\nY = [[0, 3]]\nD = jaccard_coefficient(X, Y)\nassert_array_almost_equal(D, [[0.33333333], [0.]])\n\n#Test Sparse Matrices\nX = csr_matrix(X)\nY = csr_matrix(Y)\nassert_raises(ValueError, jaccard_coefficient, X, Y)", "path": "crab\\metrics\\tests\\test_pairwise.py", "repo_name": "python-recsys/crab", "stars": 126, "license": "other", "language": "python", "size": 583}
{"docstring": "\"\"\" Check that the pairwise Sorensen distances computation\"\"\"\n#Idepontent Test\n", "func_signal": "def test_sorensen_distances():\n", "code": "X = [['a', 'b', 'c']]\nD = sorensen_coefficient(X, X)\nassert_array_almost_equal(D, [[1.]])\n\n#Vector x Non Vector\nX = [['a', 'b', 'c']]\nY = [[]]\nD = sorensen_coefficient(X, Y)\nassert_array_almost_equal(D, [[0.]])\n\n#Vector A x Vector B\nX = [[1, 2, 3, 4]]\nY = [[2, 3]]\nD = sorensen_coefficient(X, Y)\nassert_array_almost_equal(D, [[0.666666]])\n\n#BUG FIX: How to fix for multi-dimm arrays\n\n#Vector N x 1\nX = [['a', 'b', 'c', 'd'], ['e', 'f', 'g']]\nY = [['a', 'b', 'c', 'k']]\nD = sorensen_coefficient(X, Y)\nassert_array_almost_equal(D, [[0.75], [0.]])\n\n#N-Dimmensional Vectors\nX = [['a', 'b', 'c', 'd'], ['e', 'f', 'g']]\nY = [['a', 'b', 'c', 'd'], ['e', 'f', 'g']]\nD = sorensen_coefficient(X, Y)\nassert_array_almost_equal(D, [[1., 0.], [0., 1.]])\n\nX = [[0, 1], [1, 2]]\nD = sorensen_coefficient(X, X)\nassert_array_almost_equal(D, [[1., 0.5], [0.5, 1.]])\n\nX = [[0, 1], [1, 2]]\nY = [[0, 0]]\nD = sorensen_coefficient(X, Y)\nassert_array_almost_equal(D, [[0.5], [0.]])", "path": "crab\\metrics\\tests\\test_pairwise.py", "repo_name": "python-recsys/crab", "stars": 126, "license": "other", "language": "python", "size": 583}
{"docstring": "\"\"\"The distance between two points in a grid based on a strictly horizontal\nand/or vertical path (that is, along the grid lines as opposed to the\ndiagonal or \"as the crow flies\" distance.  The Manhattan distance is the\nsimple sum of the horizontal and vertical components, whereas the diagonal\ndistance might be computed by applying the Pythagorean theorem.\n\nParameters:\n    vector1: The vector you want to compare\n    vector2: The second vector you want to compare\n    args: optional arguments\n\nThe value returned is in [0,1].\n\n\"\"\"\n# Content Mode\n", "func_signal": "def sim_manhattan(vector1, vector2, **args):\n", "code": "if type(vector1) == type({}):\n    nP1P2 = len([item  for item in vector1 if item in vector2])\n    distance = sum([abs(vector1[key] - vector2[key])\n                    for key in vector1 if key in vector2])\nelse:\n    nP1P2 = len(vector1)\n    distance = sum([abs(vector1[i] - vector2[i])\n                    for i in xrange(len(vector1))])\n\nif nP1P2 > 0:\n    return 1 - (float(distance) / nP1P2)\nelse:\n    return 0.0", "path": "crab\\similarities\\similarity_distance.py", "repo_name": "marcelcaraciolo/crab", "stars": 85, "license": "None", "language": "python", "size": 830}
{"docstring": "''' UserBasedRecommender Class Constructor\n\n    `model` is the data source model\n\n    `neighborhood` is the neighborhood strategy for computing the most\n    similar users.\n\n    `similarity` is the class used for computing the similarities over\n    the users.\n\n    `capper` a normalizer for Maximum/Minimum Preferences range.\n\n'''\n", "func_signal": "def __init__(self, model, similarity, neighborhood, capper=True):\n", "code": "UserBasedRecommender.__init__(self, model)\nself.neighborhood = neighborhood\nself.similarity = similarity\nself.capper = capper", "path": "crab\\recommender\\recommender.py", "repo_name": "marcelcaraciolo/crab", "stars": 85, "license": "None", "language": "python", "size": 830}
{"docstring": "'''\nGo back and prune irrelevant diffs. Irrelevant means here, represented\nby one data point, so possibly unreliable\n'''\n", "func_signal": "def pruneDiffs(self):\n", "code": "for item1 in self._freqs.keys():\n    for item2 in self._freqs[item1].keys():\n        if self._freqs[item1][item2] <= 1:\n            del self._freqs[item1][item2]\n            del self._diffStorage[item1][item2]\n            if len(self._diffStorage[item1]) == 0:\n                break", "path": "crab\\recommender\\utils.py", "repo_name": "marcelcaraciolo/crab", "stars": 85, "license": "None", "language": "python", "size": 830}
{"docstring": "#SIMILARITY BY RATES.\n", "func_signal": "def setUp(self):\n", "code": "movies={'Marcel Caraciolo': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,\n 'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5, \n 'The Night Listener': 3.0},\n'Luciana Nunes': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5, \n 'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0, \n 'You, Me and Dupree': 3.5}, \n'Leopoldo Pires': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,\n 'Superman Returns': 3.5, 'The Night Listener': 4.0},\n'Lorena Abreu': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,\n 'The Night Listener': 4.5, 'Superman Returns': 4.0, \n 'You, Me and Dupree': 2.5},\n'Steve Gates': {'Lady in  the Water': 3.0, 'Snakes on a Plane': 4.0, \n 'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0,\n 'You, Me and Dupree': 2.0}, \n'Sheldom': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,\n 'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5},\n'Penny Frewman': {'Snakes on a Plane':4.5,'You, Me and Dupree':1.0,'Superman Returns':4.0},\n'Maria Gabriela': {}}\n\nself.model = DictDataModel(movies)\nself.similarity = UserSimilarity(self.model,sim_euclidian)", "path": "crab\\tests\\test_topitems.py", "repo_name": "marcelcaraciolo/crab", "stars": 85, "license": "None", "language": "python", "size": 830}
{"docstring": "#@TODO: How to improve this architecture for estimatePreference as a\n# method for topMatches.\n", "func_signal": "def estimatePreference(self, **args):\n", "code": "userID = args.get('thingID', None) or args.get('userID', None)\notherUserID = args.get('otherUserID', None)\nsimilarity = args.get('similarity', self.similarity)\n\n# Don't consider the user itself as possible most similar user\nif userID == otherUserID:\n    return None\n\nestimated = similarity.getSimilarity(userID, otherUserID)\nreturn estimated", "path": "crab\\neighborhood\\neighborhood.py", "repo_name": "marcelcaraciolo/crab", "stars": 85, "license": "None", "language": "python", "size": 830}
{"docstring": "# SIMILARITY BY RATES.\n", "func_signal": "def setUp(self):\n", "code": "self.movies = {\n        'Marcel Caraciolo': {\n            'Lady in the Water': 2.5,\n            'Snakes on a Plane': 3.5,\n            'Just My Luck': 3.0,\n            'Superman Returns': 3.5,\n            'You, Me and Dupree': 2.5,\n            'The Night Listener': 3.0},\n        'Luciana Nunes': {\n            'Lady in the Water': 3.0,\n            'Snakes on a Plane': 3.5,\n            'Just My Luck': 1.5,\n            'Superman Returns': 5.0,\n            'The Night Listener': 3.0,\n            'You, Me and Dupree': 3.5},\n        'Leopoldo Pires': {\n            'Lady in the Water': 2.5,\n            'Snakes on a Plane': 3.0,\n            'Superman Returns': 3.5,\n            'The Night Listener': 4.0},\n        'Lorena Abreu': {\n            'Snakes on a Plane': 3.5,\n            'Just My Luck': 3.0,\n            'The Night Listener': 4.5,\n            'Superman Returns': 4.0,\n            'You, Me and Dupree': 2.5},\n        'Steve Gates': {\n            'Lady in the Water': 3.0,\n            'Snakes on a Plane': 4.0,\n            'Just My Luck': 2.0,\n            'Superman Returns': 3.0,\n            'The Night Listener': 3.0,\n            'You, Me and Dupree': 2.0},\n        'Sheldom': {\n            'Lady in the Water': 3.0,\n            'Snakes on a Plane': 4.0,\n            'The Night Listener': 3.0,\n            'Superman Returns': 5.0,\n            'You, Me and Dupree': 3.5},\n        'Penny Frewman': {\n            'Snakes on a Plane': 4.5,\n            'You, Me and Dupree': 1.0,\n            'Superman Returns': 4.0},\n        'Maria Gabriela': {}}", "path": "crab\\tests\\models_test.py", "repo_name": "marcelcaraciolo/crab", "stars": 85, "license": "None", "language": "python", "size": 830}
{"docstring": "#SIMILARITY BY RATES.\n", "func_signal": "def setUp(self):\n", "code": "movies={'Marcel Caraciolo': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,\n 'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5, \n 'The Night Listener': 3.0},\n'Luciana Nunes': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5, \n 'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0, \n 'You, Me and Dupree': 3.5}, \n'Leopoldo Pires': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,\n 'Superman Returns': 3.5, 'The Night Listener': 4.0},\n'Lorena Abreu': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,\n 'The Night Listener': 4.5, 'Superman Returns': 4.0, \n 'You, Me and Dupree': 2.5},\n'Steve Gates': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \n 'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0,\n 'You, Me and Dupree': 2.0}, \n'Sheldom': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,\n 'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5},\n'Penny Frewman': {'Snakes on a Plane':4.5,'You, Me and Dupree':1.0,'Superman Returns':4.0},\n'Maria Gabriela': {}}\n\nself.model = DictDataModel(movies)", "path": "crab\\tests\\test_recommender.py", "repo_name": "marcelcaraciolo/crab", "stars": 85, "license": "None", "language": "python", "size": 830}
{"docstring": "#SIMILARITY BY RATES.\n", "func_signal": "def setUp(self):\n", "code": "movies={'Marcel Caraciolo': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,\n 'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5, \n 'The Night Listener': 3.0},\n'Luciana Nunes': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5, \n 'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0, \n 'You, Me and Dupree': 3.5}, \n'Leopoldo Pires': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,\n 'Superman Returns': 3.5, 'The Night Listener': 4.0},\n'Lorena Abreu': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,\n 'The Night Listener': 4.5, 'Superman Returns': 4.0, \n 'You, Me and Dupree': 2.5},\n'Steve Gates': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \n 'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0,\n 'You, Me and Dupree': 2.0}, \n'Sheldom': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,\n 'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5},\n'Penny Frewman': {'Snakes on a Plane':4.5,'You, Me and Dupree':1.0,'Superman Returns':4.0},\n'Maria Gabriela': {}}\n\nself.model = DictDataModel(movies)\nself.similarity = ItemSimilarity(self.model,sim_euclidian)\nself.strategy = PreferredItemsNeighborhoodStrategy()", "path": "crab\\tests\\test_recommender.py", "repo_name": "marcelcaraciolo/crab", "stars": 85, "license": "None", "language": "python", "size": 830}
{"docstring": "\"\"\" The constructor of Similarity class\n\n`model` defines the data model where data is fetched.\n\n`distance` The similarity measured (function) between two vectors.\n\nIf `numBest` is left unspecified, similarity queries return a full list\n(one float for every item in the model, including the query item).\n\nIf `numBest` is set, queries return `numBest` most similar items, as a\nsorted list.\n\n\"\"\"\n", "func_signal": "def __init__(self, model, distance, numBest=None):\n", "code": "self.model = model\nself.distance = distance\nself.numBest = numBest", "path": "crab\\interfaces.py", "repo_name": "marcelcaraciolo/crab", "stars": 85, "license": "None", "language": "python", "size": 830}
{"docstring": "'''\nUserBasedRecommender Class Constructor\n\n`model` is the data source model\n\n`itemStrategy` is the candidate item strategy for computing the most\nsimilar items.\n\n`similarity` is the class used for computing the similarities over\nthe items.\n\n`capper` a normalizer for Maximum/Minimum Preferences range.\n'''\n", "func_signal": "def __init__(self, model, similarity, itemStrategy, capper=True):\n", "code": "ItemBasedRecommender.__init__(self, model)\nself.strategy = itemStrategy\nself.similarity = similarity\nself.capper = capper", "path": "crab\\recommender\\recommender.py", "repo_name": "marcelcaraciolo/crab", "stars": 85, "license": "None", "language": "python", "size": 830}
{"docstring": "'''\nThe S\u00f8rensen index, also known as S\u00f8rensen\u2019s similarity coefficient, is a\nstatistic used for comparing the similarity of two samples.  It was\ndeveloped by the botanist Thorvald S\u00f8rensen and published in 1948.[1] See\nthe link: http://en.wikipedia.org/wiki/S%C3%B8rensen_similarity_index\n\nThis is intended for \"binary\" data sets where a user either expresses a\ngeneric \"yes\" preference for an item or has no preference. The actual\npreference values do not matter here, only their presence or absence.\n\nParameters:\n    vector1: The vector you want to compare\n    vector2: The second vector you want to compare\n    args: optional arguments\n\nThe value returned is in [0,1].\n\n'''\n", "func_signal": "def sim_sorensen(vector1, vector2, **args):\n", "code": "nP1P2 = len([item  for item in vector1 if item in vector2])\n\nif len(vector1) + len(vector2) == 0:\n    return 0.0\n\nreturn float(2.0 * nP1P2 / (len(vector1) + len(vector2)))", "path": "crab\\similarities\\similarity_distance.py", "repo_name": "marcelcaraciolo/crab", "stars": 85, "license": "None", "language": "python", "size": 830}
{"docstring": "#SIMILARITY BY RATES.\n", "func_signal": "def setUp(self):\n", "code": "movies={'Marcel Caraciolo': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,\n 'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5, \n 'The Night Listener': 3.0},\n'Luciana Nunes': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5, \n 'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0, \n 'You, Me and Dupree': 3.5}, \n'Leopoldo Pires': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,\n 'Superman Returns': 3.5, 'The Night Listener': 4.0},\n'Lorena Abreu': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,\n 'The Night Listener': 4.5, 'Superman Returns': 4.0, \n 'You, Me and Dupree': 2.5},\n'Steve Gates': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \n 'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0,\n 'You, Me and Dupree': 2.0}, \n'Sheldom': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,\n 'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5},\n'Penny Frewman': {'Snakes on a Plane':4.5,'You, Me and Dupree':1.0,'Superman Returns':4.0},\n'Maria Gabriela': {}}\n\nself.model = DictDataModel(movies)\nself.similarity = UserSimilarity(self.model,sim_euclidian)\nself.neighbor = NearestNUserNeighborhood(self.similarity,self.model,4,0.0)\nself.similarity_item = ItemSimilarity(self.model,sim_euclidian)\nself.strategy = PreferredItemsNeighborhoodStrategy()", "path": "crab\\tests\\test_evaluator.py", "repo_name": "marcelcaraciolo/crab", "stars": 85, "license": "None", "language": "python", "size": 830}
{"docstring": "'''\n  An implementation of a \"similarity\" based on the Tanimoto coefficient,\nor extended Jaccard coefficient.\n\nThis is intended for \"binary\" data sets where a user either expresses a\ngeneric \"yes\" preference for an item or has no preference. The actual\npreference values do not matter here, only their presence or absence.\n\nParameters:\n    the prefs: The preferences in dict format.\n    person1: The user profile you want to compare\n    person2: The second user profile you want to compare\n\nThe value returned is in [0,1].\n\n'''\n", "func_signal": "def sim_tanimoto(vector1, vector2, **args):\n", "code": "simP1P2 = [item for item in vector1 if item in vector2]\nif len(simP1P2) == 0:\n    return 0.0\n\nreturn float(len(simP1P2)) / (len(vector1) + len(vector2) - len(simP1P2))", "path": "crab\\similarities\\similarity_distance.py", "repo_name": "marcelcaraciolo/crab", "stars": 85, "license": "None", "language": "python", "size": 830}
{"docstring": "#SIMILARITY BY RATES.\n", "func_signal": "def setUp(self):\n", "code": "movies={'Marcel Caraciolo': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,\n 'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5, \n 'The Night Listener': 3.0},\n'Luciana Nunes': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5, \n 'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0, \n 'You, Me and Dupree': 3.5}, \n'Leopoldo Pires': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,\n 'Superman Returns': 3.5, 'The Night Listener': 4.0},\n'Lorena Abreu': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,\n 'The Night Listener': 4.5, 'Superman Returns': 4.0, \n 'You, Me and Dupree': 2.5},\n'Steve Gates': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \n 'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0,\n 'You, Me and Dupree': 2.0}, \n'Sheldom': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,\n 'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5},\n'Penny Frewman': {'Snakes on a Plane':4.5,'You, Me and Dupree':1.0,'Superman Returns':4.0},\n'Maria Gabriela': {}}\n\nself.model = DictDataModel(movies)\nself.similarity = UserSimilarity(self.model,sim_euclidian)\nself.neighbor = NearestNUserNeighborhood(self.similarity,self.model,4,0.0)", "path": "crab\\tests\\test_recommender.py", "repo_name": "marcelcaraciolo/crab", "stars": 85, "license": "None", "language": "python", "size": 830}
{"docstring": "''' Base Constructor Class '''\n", "func_signal": "def __init__(self, similarity, dataModel, samplingRate):\n", "code": "self.model = dataModel\nself.samplingRate = samplingRate\nself.similarity = similarity", "path": "crab\\interfaces.py", "repo_name": "marcelcaraciolo/crab", "stars": 85, "license": "None", "language": "python", "size": 830}
{"docstring": "'''\nSee http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.14.5962 and\nhttp://tdunning.blogspot.com/2008/03/surprise-and-coincidence.html .\n\nParameters:\n    n : Total  Number of items\n    vector1: The vector you want to compare\n    vector2: The second vector you want to compare\n    args: optional arguments\n\nThe value returned is in [0,1].\n'''\n\n", "func_signal": "def sim_loglikehood(n, vector1, vector2, **args):\n", "code": "def safeLog(d):\n    if d <= 0.0:\n        return 0.0\n    else:\n        return log(d)\n\ndef logL(p, k, n):\n    return k * safeLog(p) + (n - k) * safeLog(1.0 - p)\n\ndef twoLogLambda(k1, k2, n1, n2):\n    p = (k1 + k2) / (n1 + n2)\n    return 2.0 * (logL(k1 / n1, k1, n1) + logL(k2 / n2, k2, n2)\n                  - logL(p, k1, n1) - logL(p, k2, n2))\n\n# Using Content Mode.\nif type(vector1) == type({}):\n    simP1P2 = {}\n    [simP1P2.update({item: 1}) for item in vector1 if item in vector2]\n\n    if len(simP1P2) == 0:\n        return 0.0\n\n    nP1P2 = len(simP1P2)\n    nP1 = len(vector1)\n    nP2 = len(vector2)\nelse:\n    nP1P2 = len([item  for item in vector1 if item in vector2])\n\n    if nP1P2 == 0:\n        return 0.0\n\n    nP1 = len(vector1)\n    nP2 = len(vector2)\n\nif (nP1 - nP1P2 == 0)  or (n - nP2 == 0):\n    return 1.0\n\nlogLikeliHood = twoLogLambda(float(nP1P2), float(nP1 - nP1P2),\n                             float(nP2), float(n - nP2))\n\nreturn 1.0 - 1.0 / (1.0 + float(logLikeliHood))", "path": "crab\\similarities\\similarity_distance.py", "repo_name": "marcelcaraciolo/crab", "stars": 85, "license": "None", "language": "python", "size": 830}
{"docstring": "''' Return the most similar users to the given userID'''\n# Sampling\n", "func_signal": "def userNeighborhood(self, userID, rescorer=None):\n", "code": "userIDs = self.getSampleUserIDs()\n\nif not userIDs:\n    return []\n\nrec_users = topUsers(userID, userIDs, self.numUsers,\n        self.estimatePreference, self.similarity, rescorer)\n\nreturn rec_users", "path": "crab\\neighborhood\\neighborhood.py", "repo_name": "marcelcaraciolo/crab", "stars": 85, "license": "None", "language": "python", "size": 830}
{"docstring": "#SIMILARITY BY RATES.\n", "func_signal": "def setUp(self):\n", "code": "movies={'Marcel Caraciolo': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,\n 'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5, \n 'The Night Listener': 3.0},\n'Luciana Nunes': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5, \n 'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0, \n 'You, Me and Dupree': 3.5}, \n'Leopoldo Pires': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,\n 'Superman Returns': 3.5, 'The Night Listener': 4.0},\n'Lorena Abreu': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,\n 'The Night Listener': 4.5, 'Superman Returns': 4.0, \n 'You, Me and Dupree': 2.5},\n'Steve Gates': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \n 'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0,\n 'You, Me and Dupree': 2.0}, \n'Sheldom': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,\n 'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5},\n'Penny Frewman': {'Snakes on a Plane':4.5,'You, Me and Dupree':1.0,'Superman Returns':4.0},\n'Maria Gabriela': {}}\n\nself.model = DictDataModel(movies)\nself.similarity = UserSimilarity(self.model,sim_euclidian)", "path": "crab\\tests\\test_neighborhood.py", "repo_name": "marcelcaraciolo/crab", "stars": 85, "license": "None", "language": "python", "size": 830}
{"docstring": "'''\nLike  sim_pearson , but compares relative ranking of preference values\ninstead of preference values themselves. That is, each user's preferences\nare sorted and then assign a rank as their preference value, with 1 being\nassigned to the least preferred item.\n\nParameters:\n    vector1: The vector you want to compare\n    vector2: The second vector you want to compare\n    args: optional arguments\n\nThe value returned is in [0,1].\n\n'''\n\n", "func_signal": "def sim_spearman(vector1, vector2, **args):\n", "code": "if type(vector1) == type([]):\n    raise TypeError('It still not yet implemented.')\n\nsimP1 = {}\nsimP2 = {}\nrank = 1.0\n\n# First order from the lowest to greatest value.\nvector1_items = sorted(vector1.items(), lambda x, y: cmp(x[1], y[1]))\nvector2_items = sorted(vector2.items(), lambda x, y: cmp(x[1], y[1]))\n\nfor key, value in vector1_items:\n    if key in vector2:\n        simP1.update({key: rank})\n        rank += 1\n\nrank = 1.0\nfor key, values in vector2_items:\n    if key in vector2:\n        simP2.update({key: rank})\n        rank += 1\n\nsumDiffSq = 0.0\nfor key, rank in simP1.items():\n    if key in simP2:\n        sumDiffSq += pow((rank - simP2[key]), 2.0)\n\nn = len(simP1)\n\nif n == 0:\n    return 0.0\n\nreturn 1.0 - ((6.0 * sumDiffSq) / (n * (n * n - 1)))", "path": "crab\\similarities\\similarity_distance.py", "repo_name": "marcelcaraciolo/crab", "stars": 85, "license": "None", "language": "python", "size": 830}
{"docstring": "'''\nAn implementation of a \"similarity\" based on the Euclidean \"distance\"\nbetween two vectors X and Y. Thinking of items as dimensions and\npreferences as points along those dimensions, a distance is computed using\nall items (dimensions) where both users have expressed a preference for\nthat item. This is simply the square root of the sum of the squares of\ndifferences in position (preference) along each dimension. The similarity\nis then computed as 1 / (1 + distance), so the resulting values are in the\nrange (0,1].\n\nParameters:\n    vector1: The vector you want to compare\n    vector2: The second vector you want to compare\n    args: optional arguments\n\nThe value returned is in [0,1].\n'''\n# Using Content Mode.\n", "func_signal": "def sim_euclidian(vector1, vector2, **args):\n", "code": "if type(vector1) == type({}):\n    sim = {}\n    [sim.update({item:1}) for item in vector1 if item in vector2]\n\n    if len(sim) == 0.0:\n        return 0.0\n\n    sum_of_squares = sum([pow(vector1[item] - vector2[item], 2.0)\n                       for item in vector1 if item in vector2])\nelse:\n    # Using Value Mode.\n    if len(vector1) != len(vector2):\n        raise ValueError('Dimmensions vector1 != Dimmensions vector2')\n\n    sum_of_squares = sum([pow(vector1[i] - vector2[i], 2.0)\n                          for i in range(len(vector1))])\n\n    if not sum_of_squares:\n        return 0.0\n\nreturn 1 / (1 + sqrt(sum_of_squares))", "path": "crab\\similarities\\similarity_distance.py", "repo_name": "marcelcaraciolo/crab", "stars": 85, "license": "None", "language": "python", "size": 830}
{"docstring": "\"\"\" Load a bottle application from a module and make sure that the import\n    does not affect the current default application, but returns a separate\n    application object. See :func:`load` for the target parameter. \"\"\"\n", "func_signal": "def load_app(target):\n", "code": "global NORUN; NORUN, nr_old = True, NORUN\ntry:\n    tmp = default_app.push() # Create a new \"default application\"\n    rv = load(target) # Import the target module\n    return rv if callable(rv) else tmp\nfinally:\n    default_app.remove(tmp) # Remove the temporary added default application\n    NORUN = nr_old", "path": "framework\\bottle.py", "repo_name": "ecmendenhall/DaveDaveFind", "stars": 74, "license": "other", "language": "python", "size": 178}
{"docstring": "''' Returns a copy of self. '''\n", "func_signal": "def copy(self):\n", "code": "copy = Response()\ncopy.status = self.status\ncopy._headers = dict((k, v[:]) for (k, v) in self._headers.items())\nreturn copy", "path": "framework\\bottle.py", "repo_name": "ecmendenhall/DaveDaveFind", "stars": 74, "license": "other", "language": "python", "size": 178}
{"docstring": "''' Escape HTML special characters ``&<>`` and quotes ``'\"``. '''\n", "func_signal": "def html_escape(string):\n", "code": "return string.replace('&','&amp;').replace('<','&lt;').replace('>','&gt;')\\\n             .replace('\"','&quot;').replace(\"'\",'&#039;')", "path": "framework\\bottle.py", "repo_name": "ecmendenhall/DaveDaveFind", "stars": 74, "license": "other", "language": "python", "size": 178}
{"docstring": "\"\"\"\nValidates and manipulates keyword arguments by user defined callables.\nHandles ValueError and missing arguments by raising HTTPError(403).\n\"\"\"\n", "func_signal": "def validate(**vkargs):\n", "code": "depr('Use route wildcard filters instead.')\ndef decorator(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kargs):\n        for key, value in vkargs.iteritems():\n            if key not in kargs:\n                abort(403, 'Missing parameter: %s' % key)\n            try:\n                kargs[key] = value(kargs[key])\n            except ValueError:\n                abort(403, 'Wrong parameter format for: %s' % key)\n        return func(*args, **kargs)\n    return wrapper\nreturn decorator", "path": "framework\\bottle.py", "repo_name": "ecmendenhall/DaveDaveFind", "stars": 74, "license": "other", "language": "python", "size": 178}
{"docstring": "'''\nGet a rendered template as a string iterator.\nYou can use a name, a filename or a template string as first parameter.\nTemplate rendering arguments can be passed as dictionaries\nor directly (as keyword arguments).\n'''\n", "func_signal": "def template(*args, **kwargs):\n", "code": "tpl = args[0] if args else None\ntemplate_adapter = kwargs.pop('template_adapter', SimpleTemplate)\nif tpl not in TEMPLATES or DEBUG:\n    settings = kwargs.pop('template_settings', {})\n    lookup = kwargs.pop('template_lookup', TEMPLATE_PATH)\n    if isinstance(tpl, template_adapter):\n        TEMPLATES[tpl] = tpl\n        if settings: TEMPLATES[tpl].prepare(**settings)\n    elif \"\\n\" in tpl or \"{\" in tpl or \"%\" in tpl or '$' in tpl:\n        TEMPLATES[tpl] = template_adapter(source=tpl, lookup=lookup, **settings)\n    else:\n        TEMPLATES[tpl] = template_adapter(name=tpl, lookup=lookup, **settings)\nif not TEMPLATES[tpl]:\n    abort(500, 'Template (%s) not found' % tpl)\nfor dictarg in args[1:]: kwargs.update(dictarg)\nreturn TEMPLATES[tpl].render(kwargs)", "path": "framework\\bottle.py", "repo_name": "ecmendenhall/DaveDaveFind", "stars": 74, "license": "other", "language": "python", "size": 178}
{"docstring": "\"\"\" A dict-like SimpleCookie instance. This should not be used directly.\n    See :meth:`set_cookie`. \"\"\"\n", "func_signal": "def COOKIES(self):\n", "code": "depr('The COOKIES dict is deprecated. Use `set_cookie()` instead.') # 0.10\nif not self._cookies:\n    self._cookies = SimpleCookie()\nreturn self._cookies", "path": "framework\\bottle.py", "repo_name": "ecmendenhall/DaveDaveFind", "stars": 74, "license": "other", "language": "python", "size": 178}
{"docstring": "''' True if the request was triggered by a XMLHttpRequest. This only\n    works with JavaScript libraries that support the `X-Requested-With`\n    header (most of the popular libraries do). '''\n", "func_signal": "def is_xhr(self):\n", "code": "requested_with = self.environ.get('HTTP_X_REQUESTED_WITH','')\nreturn requested_with.lower() == 'xmlhttprequest'", "path": "framework\\bottle.py", "repo_name": "ecmendenhall/DaveDaveFind", "stars": 74, "license": "other", "language": "python", "size": 178}
{"docstring": "''' Trigger a hook and return a list of results. '''\n", "func_signal": "def trigger(self, name, *a, **ka):\n", "code": "hooks = self.hooks[name]\nif ka.pop('reversed', False): hooks = hooks[::-1]\nreturn [hook(*a, **ka) for hook in hooks]", "path": "framework\\bottle.py", "repo_name": "ecmendenhall/DaveDaveFind", "stars": 74, "license": "other", "language": "python", "size": 178}
{"docstring": "\"\"\" Open a file in a safe way and return :exc:`HTTPResponse` with status\n    code 200, 305, 401 or 404. Set Content-Type, Content-Encoding,\n    Content-Length and Last-Modified header. Obey If-Modified-Since header\n    and HEAD requests.\n\"\"\"\n", "func_signal": "def static_file(filename, root, mimetype='auto', download=False):\n", "code": "root = os.path.abspath(root) + os.sep\nfilename = os.path.abspath(os.path.join(root, filename.strip('/\\\\')))\nheader = dict()\n\nif not filename.startswith(root):\n    return HTTPError(403, \"Access denied.\")\nif not os.path.exists(filename) or not os.path.isfile(filename):\n    return HTTPError(404, \"File does not exist.\")\nif not os.access(filename, os.R_OK):\n    return HTTPError(403, \"You do not have permission to access this file.\")\n\nif mimetype == 'auto':\n    mimetype, encoding = mimetypes.guess_type(filename)\n    if mimetype: header['Content-Type'] = mimetype\n    if encoding: header['Content-Encoding'] = encoding\nelif mimetype:\n    header['Content-Type'] = mimetype\n\nif download:\n    download = os.path.basename(filename if download == True else download)\n    header['Content-Disposition'] = 'attachment; filename=\"%s\"' % download\n\nstats = os.stat(filename)\nheader['Content-Length'] = stats.st_size\nlm = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime(stats.st_mtime))\nheader['Last-Modified'] = lm\n\nims = request.environ.get('HTTP_IF_MODIFIED_SINCE')\nif ims:\n    ims = parse_date(ims.split(\";\")[0].strip())\nif ims is not None and ims >= int(stats.st_mtime):\n    header['Date'] = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime())\n    return HTTPResponse(status=304, header=header)\n\nbody = '' if request.method == 'HEAD' else open(filename, 'rb')\nreturn HTTPResponse(body, header=header)", "path": "framework\\bottle.py", "repo_name": "ecmendenhall/DaveDaveFind", "stars": 74, "license": "other", "language": "python", "size": 178}
{"docstring": "''' An instance of :class:`HeaderDict`, a case-insensitive dict-like\n    view on the response headers. '''\n", "func_signal": "def headers(self):\n", "code": "self.__dict__['headers'] = hdict = HeaderDict()\nhdict.dict = self._headers\nreturn hdict", "path": "framework\\bottle.py", "repo_name": "ecmendenhall/DaveDaveFind", "stars": 74, "license": "other", "language": "python", "size": 178}
{"docstring": "\"\"\" Cookies parsed into a :class:`FormsDict`. Signed cookies are NOT\n    decoded. Use :meth:`get_cookie` if you expect signed cookies. \"\"\"\n", "func_signal": "def cookies(self):\n", "code": "cookies = SimpleCookie(self.environ.get('HTTP_COOKIE',''))\ncookies = list(cookies.values())[:self.MAX_PARAMS]\nreturn FormsDict((c.key, c.value) for c in cookies)", "path": "framework\\bottle.py", "repo_name": "ecmendenhall/DaveDaveFind", "stars": 74, "license": "other", "language": "python", "size": 178}
{"docstring": "\"\"\" Parse rfc2617 HTTP authentication header string (basic) and return (user,pass) tuple or None\"\"\"\n", "func_signal": "def parse_auth(header):\n", "code": "try:\n    method, data = header.split(None, 1)\n    if method.lower() == 'basic':\n        #TODO: Add 2to3 save base64[encode/decode] functions.\n        user, pwd = touni(base64.b64decode(tob(data))).split(':',1)\n        return user, pwd\nexcept (KeyError, ValueError):\n    return None", "path": "framework\\bottle.py", "repo_name": "ecmendenhall/DaveDaveFind", "stars": 74, "license": "other", "language": "python", "size": 178}
{"docstring": "\"\"\" Change the debug level.\nThere is only one debug level supported at the moment.\"\"\"\n", "func_signal": "def debug(mode=True):\n", "code": "global DEBUG\nDEBUG = bool(mode)", "path": "framework\\bottle.py", "repo_name": "ecmendenhall/DaveDaveFind", "stars": 74, "license": "other", "language": "python", "size": 178}
{"docstring": "''' Encode and sign a pickle-able object. Return a (byte) string '''\n", "func_signal": "def cookie_encode(data, key):\n", "code": "msg = base64.b64encode(pickle.dumps(data, -1))\nsig = base64.b64encode(hmac.new(tob(key), msg).digest())\nreturn tob('!') + sig + tob('?') + msg", "path": "framework\\bottle.py", "repo_name": "ecmendenhall/DaveDaveFind", "stars": 74, "license": "other", "language": "python", "size": 178}
{"docstring": "''' Return a (target, url_agrs) tuple or raise HTTPError(400/404/405). '''\n", "func_signal": "def match(self, environ):\n", "code": "path, targets, urlargs = environ['PATH_INFO'] or '/', None, {}\nif path in self.static:\n    targets = self.static[path]\nelse:\n    for combined, rules in self.dynamic:\n        match = combined.match(path)\n        if not match: continue\n        getargs, targets = rules[match.lastindex - 1]\n        urlargs = getargs(path) if getargs else {}\n        break\n\nif not targets:\n    raise HTTPError(404, \"Not found: \" + repr(environ['PATH_INFO']))\nmethod = environ['REQUEST_METHOD'].upper()\nif method in targets:\n    return targets[method], urlargs\nif method == 'HEAD' and 'GET' in targets:\n    return targets['GET'], urlargs\nif 'ANY' in targets:\n    return targets['ANY'], urlargs\nallowed = [verb for verb in targets if verb != 'ANY']\nif 'GET' in allowed and 'HEAD' not in allowed:\n    allowed.append('HEAD')\nraise HTTPError(405, \"Method not allowed.\",\n                header=[('Allow',\",\".join(allowed))])", "path": "framework\\bottle.py", "repo_name": "ecmendenhall/DaveDaveFind", "stars": 74, "license": "other", "language": "python", "size": 178}
{"docstring": "\"\"\" The client IP as a string. Note that this information can be forged\n    by malicious clients. \"\"\"\n", "func_signal": "def remote_addr(self):\n", "code": "route = self.remote_route\nreturn route[0] if route else None", "path": "framework\\bottle.py", "repo_name": "ecmendenhall/DaveDaveFind", "stars": 74, "license": "other", "language": "python", "size": 178}
{"docstring": "\"\"\" Return a decorator that attaches a callback to a hook. \"\"\"\n", "func_signal": "def hook(self, name):\n", "code": "def wrapper(func):\n    self.hooks.add(name, func)\n    return func\nreturn wrapper", "path": "framework\\bottle.py", "repo_name": "ecmendenhall/DaveDaveFind", "stars": 74, "license": "other", "language": "python", "size": 178}
{"docstring": "\"\"\" Decorator: Register an output handler for a HTTP error code\"\"\"\n", "func_signal": "def error(self, code=500):\n", "code": "def wrapper(handler):\n    self.error_handler[int(code)] = handler\n    return handler\nreturn wrapper", "path": "framework\\bottle.py", "repo_name": "ecmendenhall/DaveDaveFind", "stars": 74, "license": "other", "language": "python", "size": 178}
{"docstring": "\"\"\" File uploads parsed from an `url-encoded` or `multipart/form-data`\n    encoded POST or PUT request body. The values are instances of\n    :class:`cgi.FieldStorage`. The most important attributes are:\n\n    filename\n        The filename, if specified; otherwise None; this is the client\n        side filename, *not* the file name on which it is stored (that's\n        a temporary file you don't deal with)\n    file\n        The file(-like) object from which you can read the data.\n    value\n        The value as a *string*; for file uploads, this transparently\n        reads the file every time you request the value. Do not do this\n        on big files.\n\"\"\"\n", "func_signal": "def files(self):\n", "code": "files = FormsDict()\nfor name, item in self.POST.iterallitems():\n    if hasattr(item, 'filename'):\n        files[name] = item\nreturn files", "path": "framework\\bottle.py", "repo_name": "ecmendenhall/DaveDaveFind", "stars": 74, "license": "other", "language": "python", "size": 178}
{"docstring": "''' The :attr:`query_string` parsed into a :class:`FormsDict`. These\n    values are sometimes called \"URL arguments\" or \"GET parameters\", but\n    not to be confused with \"URL wildcards\" as they are provided by the\n    :class:`Router`. '''\n", "func_signal": "def query(self):\n", "code": "pairs = parse_qsl(self.query_string, keep_blank_values=True)\nget = self.environ['bottle.get'] = FormsDict()\nfor key, value in pairs[:self.MAX_PARAMS]:\n    get[key] = value\nreturn get", "path": "framework\\bottle.py", "repo_name": "ecmendenhall/DaveDaveFind", "stars": 74, "license": "other", "language": "python", "size": 178}
{"docstring": "\"\"\"\nRender particles as screen space circles (looks like spheres, but projection is always a circle instead of an ellipse).\n\"\"\"\n", "func_signal": "def render_point_sprites(self, shader, enable_depth_test=True):\n", "code": "with shader:\n    if enable_depth_test:\n        glDepthMask(GL_TRUE)\n        glEnable(GL_DEPTH_TEST)\n    else:\n        glDisable(GL_DEPTH_TEST)\n    \n    # this lets use define the size of the point in the shader\n    glEnable(GL_VERTEX_PROGRAM_POINT_SIZE)\n\n    # this enables rendering of small squares\n    glEnable(GL_POINT_SPRITE)\n    # this enables coordinates on the small squares\n    glTexEnvi(GL_POINT_SPRITE, GL_COORD_REPLACE, GL_TRUE)\n    self._render_particles()\n    glDisable(GL_POINT_SPRITE);", "path": "src\\fluid_rendering\\fluid_renderer.py", "repo_name": "benma/pysph", "stars": 102, "license": "None", "language": "python", "size": 980}
{"docstring": "\"\"\"\nCreate and populate opencl device buffers.\n\"\"\"\n", "func_signal": "def cl_init_data(self):\n", "code": "N = self.N\n\nmf = cl.mem_flags\nctx = self.ctx\n\n# sizes of float32, uint32\nsf = np.nbytes[np.float32]\nsi = np.nbytes[np.uint32]\n\n# constant params made available to the kernels\n# can be updated though by copying a new set of parameters to the device \nparams = struct.pack('Iff',\n                     np.uint32(N),\n                     np.float32(self.dt),\n                     np.float32(self.mass))\nself.params_cl = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=params)\n\n\nif self.gl_interop:\n    # share positions with opengl (for rendering)\n    # use shared gl buffer\n    self.position_vbo.bind() # added to test on ATI\n    try:\n        self.position_cl = cl.GLBuffer(ctx, mf.READ_WRITE, int(self.position_vbo.buffers[0]))\n    except AttributeError: # pyopengl-accelerate is installed, only single buffer available\n        self.position_cl = cl.GLBuffer(ctx, mf.READ_WRITE, int(self.position_vbo.buffer))\n    self.position_vbo.unbind() # added to test on ATI\n    self.cl_gl_objects = [self.position_cl]\nelse:\n    # create new device buffer\n    self.position_cl = cl.Buffer(ctx, mf.READ_WRITE | mf.COPY_HOST_PTR, hostbuf=self.position)\n\nself.position_sorted_cl = cl.Buffer(ctx, mf.READ_WRITE, size=4*N*sf)\n\nself.density_cl = cl.Buffer(ctx, mf.READ_ONLY, size=N*sf)\nself.pressure_cl = cl.Buffer(ctx, mf.READ_WRITE, size=N*sf)\n\nself.velocity_cl = cl.Buffer(ctx, mf.READ_WRITE | mf.COPY_HOST_PTR, hostbuf=self.velocity)\nself.velocity_sorted_cl = cl.Buffer(ctx, mf.READ_WRITE, size=4*N*sf)\nself.acceleration_cl = cl.Buffer(ctx, mf.READ_WRITE, size=4*N*sf)\n\np2 = self.p2\n\n# grid_hash and grid_index will be sorted, and the sort algorithm can only sort arrays that have lengths that are powers of two.\n# prefill grid_hash_cl with the largest uint32 (=unused) so that the ascending sort will make the unused elements end up at the end.\ngrid_hash_preset = np.zeros((p2, 1), dtype=np.uint32)\ngrid_hash_preset.fill(-1) # largest uint32\nself.grid_hash_cl = cl.Buffer(ctx, mf.READ_WRITE | mf.COPY_HOST_PTR, hostbuf=grid_hash_preset)\n\nself.grid_index_cl = cl.Buffer(ctx, mf.READ_WRITE, size=p2*si)\nself.cell_start_cl = cl.Buffer(ctx, mf.READ_WRITE, size=self.total_number_of_cells*si)\nself.cell_end_cl = cl.Buffer(ctx, mf.READ_WRITE, size=self.total_number_of_cells*si)\n\nself.queue.finish()", "path": "src\\sph\\sph.py", "repo_name": "benma/pysph", "stars": 102, "license": "None", "language": "python", "size": 980}
{"docstring": "# for now: vertex and fragment shader in same source\n", "func_signal": "def __init__(self, code, entry=None):\n", "code": "assert self.vertex ^ self.fragment, \"Use one of CGVertexShader, CGFragmentShader or CGVertexFragmentShader.\"\nself.entry = entry or self.default_entry\nself.context = _create_context()\nself.profile = create_profile(CG_GL_VERTEX if self.vertex else CG_GL_FRAGMENT)\nprogram = cg.cgCreateProgram(\n    self.context, # Cg runtime context\n    CG_SOURCE, # Program in human-readable form\n    code.encode('ascii'), # string of source code\n    self.profile, # Profile: OpenGL ARB vertex program\n    entry.encode('ascii'),  # Entry function name\n    None # No extra compiler options\n    )\nself.error_prefix = '%s shader, entry = %s' % (\"vertex\" if self.vertex else \"fragment\", entry)\n\nself.check_error(\"creating program from string\")\ncg_gl.cgGLLoadProgram(program)\nself.check_error(\"loading program\")\nself.program = program", "path": "src\\cg\\cg_shader.py", "repo_name": "benma/pysph", "stars": 102, "license": "None", "language": "python", "size": 980}
{"docstring": "\"\"\"\nDo some work so we can efficiently find neighbours using a grid.\n\"\"\"\n", "func_signal": "def assign_cells(self):\n", "code": "prg = self.prg\nqueue = self.queue\n\n# compute hashes\n\nprg.computeHash(queue, (self.N,), None,\n                self.position_cl,\n                self.grid_hash_cl,\n                self.grid_index_cl,\n                self.params_cl).wait()\n\n# sort particles based on hash, ascending.\nself.radix_sort.sort(self.grid_hash_cl, self.grid_index_cl, self.p2)\n\n\n# find cell start / cell end\nglobal_size = (self.p2 - self.p2 % self.wg_size, )\nlocal_size = (self.wg_size, )\n\nprg.memset(queue, global_size, local_size,\n           self.cell_start_cl,\n           np.uint32(-1),\n           np.uint32(self.total_number_of_cells)).wait()\n\nprg.reorderDataAndFindCellStart(queue, global_size, local_size,\n                                self.cell_start_cl,\n                                self.cell_end_cl,\n                                self.grid_hash_cl,\n                                self.grid_index_cl,\n                                self.position_cl,\n                                self.position_sorted_cl,\n                                self.velocity_cl,\n                                self.velocity_sorted_cl,\n                                self.params_cl).wait()", "path": "src\\sph\\sph.py", "repo_name": "benma/pysph", "stars": 102, "license": "None", "language": "python", "size": 980}
{"docstring": "\"\"\"\nDefine context and queue.\n\"\"\"\n", "func_signal": "def cl_init_context(self):\n", "code": "device = self.cl_pick_device()\nplatform = device.platform\nadditional_properties = []\n\nfrom pyopencl.tools import get_gl_sharing_context_properties\nadditional_properties = get_gl_sharing_context_properties()\n# Some OSs prefer clCreateContextFromType, some prefer clCreateContext. Try both.\ntry: \n    self.ctx = cl.Context(properties=[(cl.context_properties.PLATFORM, platform)] + additional_properties)\nexcept:\n    self.ctx = cl.Context(properties=[(cl.context_properties.PLATFORM, platform)] + additional_properties, devices=[device])\nself.queue = cl.CommandQueue(self.ctx, device=device, properties=cl.command_queue_properties.OUT_OF_ORDER_EXEC_MODE_ENABLE)", "path": "src\\fluid_rendering\\fluid_renderer.py", "repo_name": "benma/pysph", "stars": 102, "license": "None", "language": "python", "size": 980}
{"docstring": "\"\"\"\nFinished rendering to the render target.\n\"\"\"\n", "func_signal": "def unbind(self):\n", "code": "glPopAttrib()\nglBindFramebuffer(GL_FRAMEBUFFER, 0)", "path": "src\\render_target.py", "repo_name": "benma/pysph", "stars": 102, "license": "None", "language": "python", "size": 980}
{"docstring": "\"\"\"\nRenders the thickness map into the thickness target.\n\"\"\"\n# render thickness map to thickness target\n", "func_signal": "def _create_thickness_map(self):\n", "code": "with self.thickness_target:\n    # render with additive blending (the more particles on top of each other, the thicker the fluid is).\n    glEnable(GL_BLEND)\n    glBlendFunc(GL_ONE, GL_ONE)\n    # no depth test, we want to render them all to determine thickness.\n    self.render_point_sprites(self.thickness_shader, enable_depth_test=False)\n    glDisable(GL_BLEND)\n\n# gaussian blur of thickness map (makes the circle-artefacts coming from rendering\n# the particles as circles disappear).\nif self.blur_thickness_map:\n    # blur thickness map, vertical pass\n    with self.thickness2_target:\n        with self.thickness_blur_shader.blur(BlurShader.VERTICAL):\n            self.render_texture(self.thickness_target.texture)\n            \n    # blur thickness map, horizontal pass\n    with self.thickness_target:\n        with self.thickness_blur_shader.blur(BlurShader.HORIZONTAL):\n            self.render_texture(self.thickness2_target.texture)", "path": "src\\fluid_rendering\\fluid_renderer.py", "repo_name": "benma/pysph", "stars": 102, "license": "None", "language": "python", "size": 980}
{"docstring": "\"\"\"\nInitalize the initial positions/velocities of the particles.\nUsing self.N particles.\n\"\"\"\n", "func_signal": "def initialize_positions(self):\n", "code": "i = 0\nposition = self.position\nspacing0 = self.spacing0\n\n# arrange particles in one large and two small cubes\n# (N needs to be a cubic number)\nn = int((self.N/1.25)**(1/3.)+0.5)\nfor x in range(n):\n    for y in range(n):\n        for z in range(n):\n            position[i, 0] = (x + 0.5) * spacing0\n            position[i, 1] = (y + 0.5) * spacing0 + 0.01*self.boxsize[1]\n            position[i, 2] = self.boxsize[2] - (z + 0.5) * spacing0\n            position[i, 3] = 0\n            i += 1\n\nfor x in range(n//2):\n    for y in range(n//2):\n        for z in range(n//2):\n            position[i, 0] = (x + 0.5) * spacing0 + 0.01*self.boxsize[0]\n            position[i, 1] = self.boxsize[1] - (y + 0.5) * spacing0 \n            position[i, 2] = self.boxsize[2] - (z + 0.5) * spacing0\n            position[i, 3] = 0\n            i += 1\n\nfor x in range(n//2):\n    for y in range(n//2):\n        for z in range(n//2):\n            position[i, 0] = self.boxsize[0] - (x + 0.5) * spacing0\n            position[i, 1] = (y + 0.5) * spacing0 + 0.15*self.boxsize[1]\n            position[i, 2] = (z + 0.5) * spacing0\n            position[i, 3] = 0\n            i += 1", "path": "src\\sph\\sph.py", "repo_name": "benma/pysph", "stars": 102, "license": "None", "language": "python", "size": 980}
{"docstring": "\"\"\"\nCopies the positions (and velocities) from host to device\n\"\"\"\n", "func_signal": "def set_positions(self):\n", "code": "self.position_vbo.set_array(self.position)\ncl.enqueue_copy(self.queue, self.velocity_cl, self.velocity).wait()", "path": "src\\sph\\sph.py", "repo_name": "benma/pysph", "stars": 102, "license": "None", "language": "python", "size": 980}
{"docstring": "\"\"\"\nForce update of the parameters. Note that parameter updates automatically\nhappen during bind (Cg Reference Manual, see cgSetParameterSettingMode).\n\"\"\"\n", "func_signal": "def update_parameters(self):\n", "code": "cg.cgUpdateProgramParameters(self.program)\nself.check_error(\"updating parameters\")", "path": "src\\cg\\cg_shader.py", "repo_name": "benma/pysph", "stars": 102, "license": "None", "language": "python", "size": 980}
{"docstring": "\"\"\"\ndir: 0 for descending sort, 1 for ascending.\n\"\"\"\n", "func_signal": "def _sort(self, d_dst_key, d_dst_val, d_src_key, d_src_val, batch, array_length, dir):\n", "code": "assert array_length >= 2\n\nqueue = self.queue\nprg = self.prg\nlocal_size_limit = self.local_size_limit\n\n\ndef int_log2(L):\n    if not L:\n        return 0\n\n    log2 = 0\n    while L & 1 == 0:\n        L >>= 1\n        log2 += 1\n    return log2\n\n\n# only power-of-two array lengths are supported\nlog2L = int_log2(array_length)\nassert 2**log2L == array_length\n\nif array_length <= local_size_limit:\n    assert (batch * array_length) % local_size_limit == 0\n\n    local_work_size = (local_size_limit // 2, )\n    global_work_size = (batch * array_length // 2, )\n\n    kernel_args = (\n        d_dst_key,\n        d_dst_val,\n        d_src_key,\n        d_src_val,\n        np.uint32(array_length),\n        np.uint32(dir),\n        )\n\n    prg.bitonicSortLocal(queue, global_work_size, local_work_size, *kernel_args)\n    queue.finish()\nelse:\n    # launch bitonicSortLocal1\n\n    local_work_size = (local_size_limit // 2, )\n    global_work_size = (batch * array_length // 2, )\n\n    kernel_args = (\n        d_dst_key,\n        d_dst_val,\n        d_src_key,\n        d_src_val,\n        )\n\n    prg.bitonicSortLocal1(queue, global_work_size, local_work_size, *kernel_args)\n    queue.finish()\n\n    size = 2*local_size_limit\n    while size <= array_length:\n        stride = size // 2\n        while stride > 0:\n            if stride >= local_size_limit:\n                # launch bitonicMergeGlobal\n\n                local_work_size = (local_size_limit // 4, )\n                global_work_size = (batch * array_length // 2, )\n\n                kernel_args = (\n                    d_dst_key,\n                    d_dst_val,\n                    d_src_key,\n                    d_src_val,\n                    np.uint32(array_length),\n                    np.uint32(size),\n                    np.uint32(stride),\n                    np.uint32(dir),\n                    )\n\n                prg.bitonicMergeGlobal(queue, global_work_size, local_work_size, *kernel_args)\n                queue.finish()\n            else:\n                # launch bitonicMergeLocal\n                local_work_size = (local_size_limit // 2, )\n                global_work_size = (batch * array_length // 2, )\n\n                kernel_args = (\n                    d_dst_key,\n                    d_dst_val,\n                    d_src_key,\n                    d_src_val,\n                    np.uint32(array_length),\n                    np.uint32(stride),\n                    np.uint32(size),\n                    np.uint32(dir),\n                    )\n\n                prg.bitonicMergeLocal(queue, global_work_size, local_work_size, *kernel_args)\n                queue.finish()\n\n            stride >>= 1\n\n        size <<= 1", "path": "src\\sph\\bitonic_sort\\bitonic_sort.py", "repo_name": "benma/pysph", "stars": 102, "license": "None", "language": "python", "size": 980}
{"docstring": "\"\"\"\nCopies the position buffer from device to host.\n\"\"\"\n", "func_signal": "def get_position(self):\n", "code": "assert not self.gl_interop, \"currently not working with gl interop\"\ncl.enqueue_copy(self.queue, self.position, self.position_cl)\nreturn self.position", "path": "src\\sph\\sph.py", "repo_name": "benma/pysph", "stars": 102, "license": "None", "language": "python", "size": 980}
{"docstring": "\"\"\"\nDefine context and queue.\n\"\"\"\n", "func_signal": "def cl_init_context(self):\n", "code": "device = self.cl_pick_device()\nplatform = device.platform\nadditional_properties = []\nif self.gl_interop:\n    from pyopencl.tools import get_gl_sharing_context_properties\n    additional_properties = get_gl_sharing_context_properties()\n# Some OSs prefer clCreateContextFromType, some prefer clCreateContext. Try both.\ntry: \n    self.ctx = cl.Context(properties=[(cl.context_properties.PLATFORM, platform)] + additional_properties)\nexcept:\n    self.ctx = cl.Context(properties=[(cl.context_properties.PLATFORM, platform)] + additional_properties, devices=[device])\nself.queue = cl.CommandQueue(self.ctx, device=device, properties=cl.command_queue_properties.OUT_OF_ORDER_EXEC_MODE_ENABLE)", "path": "src\\sph\\sph.py", "repo_name": "benma/pysph", "stars": 102, "license": "None", "language": "python", "size": 980}
{"docstring": "\"\"\"\nsize: (width, height).\n\"\"\"\n", "func_signal": "def __init__(self, size, clear_color=(0,0,0,1)):\n", "code": "self.size = size\nself.clear_color = clear_color\nwidth, height = size\n\n\n\n# create texture for color output\nself.texture = self.create_texture(size)\n\n# create renderbuffer for depth\nrb = glGenRenderbuffers(1)\nglBindRenderbuffer(GL_RENDERBUFFER, rb)\nglRenderbufferStorage(GL_RENDERBUFFER, GL_DEPTH_COMPONENT, width, height)\nglBindRenderbuffer(GL_RENDERBUFFER, 0)\n\n# create frame buffer\nself.fbo = glGenFramebuffers(1)\nglBindFramebuffer(GL_FRAMEBUFFER, self.fbo)\n# attach color texture\nglFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, self.texture, 0)\n# attach depth buffer\nglFramebufferRenderbuffer(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, GL_RENDERBUFFER, rb)\n\ncheckFramebufferStatus()\n#assert glCheckFramebufferStatus(GL_FRAMEBUFFER) == GL_FRAMEBUFFER_COMPLETE\n\nglBindFramebuffer(GL_FRAMEBUFFER, 0)", "path": "src\\render_target.py", "repo_name": "benma/pysph", "stars": 102, "license": "None", "language": "python", "size": 980}
{"docstring": "\"\"\"\nBind frame buffer. Subsequent rendering goes into the render target.\n\"\"\"\n", "func_signal": "def bind(self):\n", "code": "glBindFramebuffer(GL_FRAMEBUFFER, self.fbo)\nglPushAttrib(GL_COLOR_BUFFER_BIT | GL_VIEWPORT_BIT | GL_ENABLE_BIT)\nglViewport(0, 0, *self.size)\nif self.clear_color:\n    glClearColor(*self.clear_color)\n    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)", "path": "src\\render_target.py", "repo_name": "benma/pysph", "stars": 102, "license": "None", "language": "python", "size": 980}
{"docstring": "\"\"\"\nAdvance simulation.\n\"\"\"\n", "func_signal": "def step(self):\n", "code": "prg = self.prg\nqueue = self.queue\n\nif self.gl_interop:\n    cl.enqueue_acquire_gl_objects(queue, self.cl_gl_objects)\n\n## one simulation step consists of four steps:\n## step 1) assign particles to cells in the uniform grid\n## step 2) compute densities\n## step 3) compute accelerations from forces\n## step 4) move particles according to accelerations.\n## for now, simple collision detection is done in the last step.\n\n# step 1)\nif self.use_grid:\n    self.assign_cells()\n\nglobal_size = self.global_size\nlocal_size = self.local_size\n\n# if we use a grid based neighbour search, we need to pass\n# some additional arguments.\ngrid_args = [self.grid_index_cl,\n             self.cell_start_cl,\n             self.cell_end_cl] if self.use_grid else []\n\n# step 2)\nprg.stepDensity(queue, global_size, local_size, \n                self.position_sorted_cl if self.use_grid and self.reorder else self.position_cl,\n                self.density_cl,\n                self.pressure_cl,\n                self.params_cl, *grid_args).wait()\n\n# step 3)\nprg.stepForces(queue, global_size, local_size,\n               self.position_sorted_cl if self.use_grid and self.reorder else self.position_cl,\n               self.velocity_sorted_cl if self.use_grid and self.reorder else self.velocity_cl,\n               self.acceleration_cl,\n               self.density_cl,\n               self.pressure_cl,\n               self.params_cl, *grid_args).wait()\n\n# step 4)\nprg.stepMove(queue, global_size, local_size,\n             self.position_cl,\n             self.velocity_cl,\n             self.acceleration_cl,\n             self.params_cl).wait()\n\nif self.gl_interop:\n    cl.enqueue_release_gl_objects(queue, self.cl_gl_objects)", "path": "src\\sph\\sph.py", "repo_name": "benma/pysph", "stars": 102, "license": "None", "language": "python", "size": 980}
{"docstring": "\"\"\"\nInitialize opencl context, queue and device buffers.\n\"\"\"\n", "func_signal": "def cl_init(self):\n", "code": "if self.gl_interop:\n    # create a opengl vertex buffer object we can can share the particle positions\n    # with opengl (for rendering).\n    from OpenGL.arrays import vbo\n    from OpenGL import GL\n    self.position_vbo = vbo.VBO(data=self.position, usage=GL.GL_DYNAMIC_DRAW, target=GL.GL_ARRAY_BUFFER)\n\nif self.gl_interop:\n    # need to bind positions here, not sure why\n    self.position_vbo.bind()\n    self.cl_init_context()\n    self.position_vbo.unbind()\nelse:\n    self.cl_init_context()\n# load opencl code\ncur_dir = os.path.dirname(os.path.abspath(__file__))\nfrom mako.template import Template\nwith open('%s/sph.cl' % cur_dir) as f:\n\n    code = str(Template(f.read()).render(\n        # constant parameters, replaced in code before\n        # it is compiled\n        use_grid=self.use_grid,\n        reorder=self.use_grid and self.reorder,\n        local_hash_size=self.wg_size+1,\n        h=self.h,\n        number_of_cells=self.number_of_cells,\n        boxsize=self.boxsize,\n        density0=self.density0,\n        k=self.k,\n        viscosity=self.viscosity,\n        ))\n    self.prg = cl.Program(self.ctx, code).build()\n\nself.radix_sort = RadixSort(self.ctx, self.queue, self.p2, np.uint32)\n\nself.cl_init_data()", "path": "src\\sph\\sph.py", "repo_name": "benma/pysph", "stars": 102, "license": "None", "language": "python", "size": 980}
{"docstring": "\"\"\"\nprojection_matrix: a 4x4 numpy array describing the projection matrix in use.\n\"\"\"\n", "func_signal": "def __init__(self, window_size, fluid_simulator, projection_matrix):\n", "code": "self.window_size = window_size\nself.fluid_simulator = fluid_simulator\nself.projection_matrix = projection_matrix\n\nself.position_vbo = fluid_simulator.position_vbo\nself.N = fluid_simulator.N\n\n\nradius = fluid_simulator.h * 0.25\n\n# blur parameters tuned to this example. should be generalized.\n# should make the radius/sigma dependent on the largest screen-space particle size\n# so that it looks good from any distance.\nself.thickness_blur_shader = BlurShader(size=window_size, radius=20, sigma=5)\n\nself.ball_shader = FluidShader(window_size, fluid_simulator.boxsize, radius, entry_fragment=\"ballFragment\")\nself.depth_shader = FluidShader(window_size, fluid_simulator.boxsize, radius*1.5, entry_fragment=\"depthFragment\")\nself.final_shader = FluidShader(window_size, fluid_simulator.boxsize, radius, entry_vertex=\"vertexPass\", entry_fragment=\"finalFragment\")\nself.tex_shader = FluidShader(window_size, fluid_simulator.boxsize, radius, entry_vertex=\"vertexPass\", entry_fragment=\"texFragment\")\nself.thickness_shader = FluidShader(window_size, fluid_simulator.boxsize, radius*3, entry_fragment=\"thicknessFragment\")\n\nfrom render_target import RenderTarget, RenderTargetR32F\n\n # alternating between the two in the smoothing iteration\nself.depth_target = RenderTargetR32F(window_size)\nself.depth2_target = RenderTargetR32F(window_size)\n\nself.test_target = RenderTarget(window_size)\n\n# alternating between the two for the horizontal/vertical blur pass\nself.thickness_target = RenderTargetR32F(window_size)\nself.thickness2_target = RenderTargetR32F(window_size)\n\nself.cl_init()", "path": "src\\fluid_rendering\\fluid_renderer.py", "repo_name": "benma/pysph", "stars": 102, "license": "None", "language": "python", "size": 980}
{"docstring": "\"\"\"\nRender particles as a fluid surface (Simon Green's screen space rendering).\n\"\"\"\n", "func_signal": "def render_advanced(self):\n", "code": "self._create_thickness_map()\n\n# render depth to depth target\nwith self.depth_target:\n    self.render_point_sprites(self.depth_shader)\n\n# smooth depth texture\nif self.smooth_depth:\n    cl.enqueue_acquire_gl_objects(self.queue, self.cl_gl_objects)\n    local_size = self.cl_local_size\n    for i in range(self.smoothing_iterations):\n        # alternate between writing to depth2_target and depth1_target\n        # (can't read from and write to the same texture at the same time).\n        args = (np.float32(self.smoothing_dt),\n                np.float32(self.smoothing_z_contrib),)\n        self.prg.curvatureFlow(self.queue, self.window_size, local_size, self.depth_cl, self.depth2_cl, *args).wait()\n        self.prg.curvatureFlow(self.queue, self.window_size, local_size, self.depth2_cl, self.depth_cl, *args).wait()\n    cl.enqueue_release_gl_objects(self.queue, self.cl_gl_objects)\n\n\nif self.render_mean_curvature:\n    cl.enqueue_acquire_gl_objects(self.queue, self.cl_gl_objects)\n    self.prg.test(self.queue, self.window_size, self.cl_local_size, self.depth_cl, self.test_cl).wait()\n    cl.enqueue_release_gl_objects(self.queue, self.cl_gl_objects)\n\n    with self.tex_shader:\n        self.render_texture(self.test_target.texture)\n        \n    return\n\n# # testing\n# cl.enqueue_acquire_gl_objects(self.queue, [self.depth_cl, self.test_cl])\n# self.prg.test(self.queue, self.depth_target.size, self.cl_local_size, self.depth_cl, self.test_cl).wait()\n# with self.tex_shader:\n#     self.render_texture(self.test_target.texture)\n#     #self.render_texture(self.depth_target.texture)\n# cl.enqueue_release_gl_objects(self.queue, [self.depth_cl, self.test_cl])\n# return\n\n# use alpha blending\nglEnable (GL_BLEND);\nglBlendFunc (GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA);\n\n# bind thickness texture\nglActiveTexture(GL_TEXTURE0+1)\nglBindTexture(GL_TEXTURE_2D, self.thickness_target.texture)\nwith self.final_shader:\n    self.render_texture(self.depth_target.texture)\nglActiveTexture(GL_TEXTURE0)\n\nglDisable(GL_BLEND)", "path": "src\\fluid_rendering\\fluid_renderer.py", "repo_name": "benma/pysph", "stars": 102, "license": "None", "language": "python", "size": 980}
{"docstring": "\"\"\"\nRender a full screen texture.\nstage: which texture unit to make active.\n\"\"\"\n", "func_signal": "def render_texture(self, texture, stage=0):\n", "code": "glDisable(GL_CULL_FACE)\n\nglActiveTexture(GL_TEXTURE0+stage)\nglBindTexture(GL_TEXTURE_2D, texture)\n\nglBegin(GL_QUADS)\n\nglTexCoord2f(0.0, 0.0);\nglVertex3f(-1.0, -1.0, 0.0)\n\nglTexCoord2f(1.0, 0.0);\nglVertex3f(1.0, -1.0, 0.0)\n\nglTexCoord2f(1.0, 1.0);\nglVertex3f(1.0, 1.0, 0.0)\n\nglTexCoord2f(0.0, 1.0);\nglVertex3f(-1.0, 1.0, 0.0)    \n\nglEnd()\n\nglBindTexture(GL_TEXTURE_2D, 0)\nglActiveTexture(GL_TEXTURE0)", "path": "src\\fluid_rendering\\fluid_renderer.py", "repo_name": "benma/pysph", "stars": 102, "license": "None", "language": "python", "size": 980}
{"docstring": "# opening database\n", "func_signal": "def getMediaItem(self, zpk):\n", "code": "\t\ttry:    \n\t\t\tself.tempdb = sqlite3.connect(self.fname_contacts)\n\t\texcept:\n\t\t\tprint(\"\\nUnexpected error: %s\"%sys.exc_info()[1])\n\t\t\tself.close()\n\t\t\n\t\t# query results are retrieved as a namedtuple\n\t\t# (this step must be before cursor instantiation)\n\t\tself.tempdb.row_factory = namedtuple_factory                        \t\n\t\tself.tempcur = self.tempdb.cursor()\n# ZATTACHMENT table\n\t\tquery = \"SELECT * FROM ZATTACHMENT WHERE Z_PK=?;\"\n\t\tself.tempcur.execute(query, [zpk])\n\t\tmedia = self.tempcur.fetchone()\n\t\t\n\t\t# closing database\n\t\tself.tempdb.close()\n# a namedtuple is returned\t\t\n\t\treturn media", "path": "ipba2-plugins\\plg_viber.py", "repo_name": "PicciMario/iPhone-Backup-Analyzer-2", "stars": 95, "license": "None", "language": "python", "size": 3550}
{"docstring": "# opening database\n", "func_signal": "def getLocation(self, zpk):\n", "code": "\t\ttry:    \n\t\t\tself.tempdb = sqlite3.connect(self.fname_contacts)\n\t\texcept:\n\t\t\tprint(\"\\nUnexpected error: %s\"%sys.exc_info()[1])\n\t\t\tself.close()\n\t\t\n\t\t# query results are retrieved as a namedtuple\n\t\t# (this step must be before cursor instantiation)\n\t\tself.tempdb.row_factory = namedtuple_factory                        \t\n\t\tself.tempcur = self.tempdb.cursor()\n# ZVIBERLOCATION table\n\t\tquery = \"SELECT * FROM ZVIBERLOCATION WHERE Z_PK=?;\"\n\t\tself.tempcur.execute(query, [zpk])\n\t\tlocation = self.tempcur.fetchone()\n\t\t\n\t\t# closing database\n\t\tself.tempdb.close()\n# a namedtuple is returned\t\t\n\t\treturn location", "path": "ipba2-plugins\\plg_viber.py", "repo_name": "PicciMario/iPhone-Backup-Analyzer-2", "stars": 95, "license": "None", "language": "python", "size": 3550}
{"docstring": "# opening database\n", "func_signal": "def getMsgContact(self, zpk):\n", "code": "\t\ttry:    \n\t\t\tself.tempdb = sqlite3.connect(self.fname_contacts)\n\t\texcept:\n\t\t\tprint(\"\\nUnexpected error: %s\"%sys.exc_info()[1])\n\t\t\tself.close()\n\t\t\n\t\t# query results are retrieved as a namedtuple\n\t\t# (this step must be before cursor instantiation)\n\t\tself.tempdb.row_factory = namedtuple_factory                        \t\n\t\tself.tempcur = self.tempdb.cursor()\n# ZPHONENUMBERINDEX - ZABCONTACT\n\t\tzphonenumberindex_columns = \"ZPHONENUMBERINDEX.ZCANONIZEDPHONENUM\"\n\t\tzabcontact_columns = \"ZABCONTACT.ZMAINNAME, ZABCONTACT.ZPREFIXNAME\"\n\t\tconditions = \"ZPHONENUMBERINDEX.Z_PK=? AND ZPHONENUMBERINDEX.ZCONTACT = ZABCONTACT.Z_PK;\"\n\t\tquery = \"SELECT \" + zphonenumberindex_columns + \", \" + zabcontact_columns + \" FROM ZPHONENUMBERINDEX, ZABCONTACT WHERE \" + conditions\n\t\tself.tempcur.execute(query, [zpk])\n\t\tmsgcontact = self.tempcur.fetchone()\n\t\t\n\t\t# closing database\n\t\tself.tempdb.close()\n# a namedtuple is returned\t\t\n\t\treturn msgcontact", "path": "ipba2-plugins\\plg_viber.py", "repo_name": "PicciMario/iPhone-Backup-Analyzer-2", "stars": 95, "license": "None", "language": "python", "size": 3550}
{"docstring": "# refresh the window\n\t\t#(the selected row is highligthed and the chats table appears disabled)\n", "func_signal": "def getMsgs(self, zconversation):\n", "code": "\t\tQtGui.QApplication.processEvents()\n# opening database\n\t\ttry:    \n\t\t\tself.tempdb = sqlite3.connect(self.fname_contacts)\n\t\texcept:\n\t\t\tprint(\"\\nUnexpected error: %s\"%sys.exc_info()[1])\n\t\t\tself.close()\n\t\t\n\t\t# query results are retrieved as a namedtuple\n\t\t# (this step must be before cursor instantiation)\n\t\tself.tempdb.row_factory = namedtuple_factory                        \t\n\t\tself.tempcur = self.tempdb.cursor()\n\t\t\t\t\n\t\t# ** ZVIBERMESSAGE ** - ZPHONENUMBERINDEX\n\t\tzvibermessage_columns = \"ZVIBERMESSAGE.Z_PK, ZVIBERMESSAGE.ZATTACHMENT, ZVIBERMESSAGE.ZLOCATION, ZVIBERMESSAGE.ZPHONENUMINDEX, ZVIBERMESSAGE.ZDATE, ZVIBERMESSAGE.ZSTATEDATE, ZVIBERMESSAGE.ZSTATE, ZVIBERMESSAGE.ZTEXT\"\n\t\tconditions = \"ZVIBERMESSAGE.ZCONVERSATION=?;\"\n\t\tquery = \"SELECT \" + zvibermessage_columns + \" FROM ZVIBERMESSAGE WHERE \" + conditions\n\t\tself.tempcur.execute(query, [zconversation])\n\t\tmessages = self.tempcur.fetchall()\n\t\t\n\t\t# closing database\n\t\tself.tempdb.close()\n# a namedtuple is returned\t\t\n\t\treturn messages", "path": "ipba2-plugins\\plg_viber.py", "repo_name": "PicciMario/iPhone-Backup-Analyzer-2", "stars": 95, "license": "None", "language": "python", "size": 3550}
{"docstring": "\"\"\"\nSaves the list of MRU files to the application settings file.\n\"\"\"\n", "func_signal": "def mruSaveList(self):\n", "code": "qs = QSettings()\n\nqs.remove(\"mru\")\nqs.beginWriteArray(\"mru\")\nfor i, m in enumerate(self.mru_list):\n\t(path, description) = m\n\tqs.setArrayIndex(i)\n\tqs.setValue(\"path\", path)\n\tqs.setValue(\"description\", description)\nqs.endArray()", "path": "main.py", "repo_name": "PicciMario/iPhone-Backup-Analyzer-2", "stars": 95, "license": "None", "language": "python", "size": 3550}
{"docstring": "# opening database\n", "func_signal": "def getCalls(self):\n", "code": "\t\ttry:          \n\t\t\tself.tempdb = sqlite3.connect(self.fname_contacts)\n\t\texcept:\n\t\t\tprint(\"\\nUnexpected error: %s\"%sys.exc_info()[1])\n\t\t\tself.close()\n# query results are retrieved as a namedtuple\n\t\t# (this step must be before cursor instantiation)\n\t\tself.tempdb.row_factory = namedtuple_factory                        \t\n\t\tself.tempcur = self.tempdb.cursor()\n# ** ZRECENT - ZRECENTSLINE ** -> ZPHONENUMBERINDEX -> ZABCONTACT\n\t\tzrecent_columns = \"ZRECENT.ZDURATION, ZRECENT.ZDATE, ZRECENT.ZCALLTYPE\"\n\t\tzabcontact_columns = \"ZABCONTACT.ZMAINNAME, ZABCONTACT.ZPREFIXNAME\"\n\t\tconditions = \"ZRECENT.ZRECENTSLINE = ZRECENTSLINE.Z_PK AND ZRECENTSLINE.ZPHONENUMINDEX = ZPHONENUMBERINDEX.Z_PK AND ZPHONENUMBERINDEX.ZCONTACT = ZABCONTACT.Z_PK;\"\n\t\tquery = \"SELECT \" + zrecent_columns + \", \" + zabcontact_columns + \" FROM ZRECENT, ZRECENTSLINE, ZPHONENUMBERINDEX, ZABCONTACT WHERE \" + conditions\n\t\tself.tempcur.execute(query)\n\t\t# fetches calls namedtuple\n\t\tcalls = self.tempcur.fetchall()\n\t\t\t\n\t\t# closing database\n\t\tself.tempdb.close()\n# a namedtuple is returned\t\t\n\t\treturn calls", "path": "ipba2-plugins\\plg_viber.py", "repo_name": "PicciMario/iPhone-Backup-Analyzer-2", "stars": 95, "license": "None", "language": "python", "size": 3550}
{"docstring": "\"\"\"\nLoads the list of MRU files from the pplication settings file.\n\"\"\"\n", "func_signal": "def mruLoadList(self):\n", "code": "self.mru_list = list()\nqs = QSettings()\n\ncount = qs.beginReadArray(\"mru\")\nfor i in range(count):\n\tqs.setArrayIndex(i)\n\tpath = qs.value(\"path\")\n\tdescription = qs.value(\"description\")\n\tif os.path.exists(path):\n\t\tself.mru_list.append((path,description))\nqs.endArray()", "path": "main.py", "repo_name": "PicciMario/iPhone-Backup-Analyzer-2", "stars": 95, "license": "None", "language": "python", "size": 3550}
{"docstring": "# opening database\n", "func_signal": "def getContacts(self):\n", "code": "\t\ttry:          \n\t\t\tself.tempdb = sqlite3.connect(self.fname_contacts)\n\t\texcept:\n\t\t\tprint(\"\\nUnexpected error: %s\"%sys.exc_info()[1])\n\t\t\tself.close()\n# query results are retrieved as a namedtuple\n\t\t# (this step must be before cursor instantiation)\n\t\tself.tempdb.row_factory = namedtuple_factory                        \t\n\t\tself.tempcur = self.tempdb.cursor()\n# ** ZABCONTACT - ZPHONENUMBERINDEX **\n\t\tzphonenumberindex_columns = \"ZPHONENUMBERINDEX.Z_PK, ZPHONENUMBERINDEX.ZREGISTRATIONDATE, ZPHONENUMBERINDEX.ZCANONIZEDPHONENUM, ZPHONENUMBERINDEX.ZICONID\"\n\t\tzabcontact_columns = \"ZABCONTACT.ZISVIBERICON, ZABCONTACT.ZMODIFCATIONDATE, ZABCONTACT.ZMAINNAME, ZABCONTACT.ZPREFIXNAME\"\n\t\tconditions = \"ZPHONENUMBERINDEX.ZISVIBER = 1 AND ZPHONENUMBERINDEX.ZCONTACT = ZABCONTACT.Z_PK;\"\n\t\tquery = \"SELECT \" + zphonenumberindex_columns + \", \" + zabcontact_columns + \" FROM ZPHONENUMBERINDEX, ZABCONTACT WHERE \" + conditions\n\t\tself.tempcur.execute(query)\n\t\t# fetches contacts namedtuple\n\t\tcontacts = self.tempcur.fetchall()\n\t\t\t\n\t\t# closing database\n\t\tself.tempdb.close()\n# a namedtuple is returned\t\t\n\t\treturn contacts", "path": "ipba2-plugins\\plg_viber.py", "repo_name": "PicciMario/iPhone-Backup-Analyzer-2", "stars": 95, "license": "None", "language": "python", "size": 3550}
{"docstring": "# if timestamp is not like \"304966548\", but like \"306350664.792749\",\n# then just use the numbers in front of the \".\"\n", "func_signal": "def formatDate(self, mactime):\n", "code": "mactime = str(mactime)\nif mactime.find(\".\") > -1:\n\tmactime = mactime[:mactime.find(\".\")]\ndate_time = datetime.fromtimestamp(int(mactime)+11323*60*1440)\nreturn date_time", "path": "ipba2-plugins\\plg_viber.py", "repo_name": "PicciMario/iPhone-Backup-Analyzer-2", "stars": 95, "license": "None", "language": "python", "size": 3550}
{"docstring": "\"\"\"\nGiven an iDevice \"Product Type\" (e.g. \"iPhone3,1\") returns the product name (e.g. \"iPhone 4\").\nReturns \"Unknown Model (type)\" if the model is not recognized.\n\"\"\"\n", "func_signal": "def getIDeviceProductName(self,product_type):\n", "code": "types = { \"iPad1,1\":\"iPad 1\",\n\t\t\"iPad2,1\":\"iPad 2\",\n\t\t\"iPad2,2\":\"iPad 2\",\n\t\t\"iPad2,3\":\"iPad 2\",\n\t\t\"iPad2,4\":\"iPad 2\",\n\t\t\"iPad3,1\":\"iPad 3\",\n\t\t\"iPad3,3\":\"iPad 3\",\n\t\t\"iPad3,2\":\"iPad 3\",\n\t\t\"iPad3,4\":\"iPad 4\",\n\t\t\"iPad2,5\":\"iPad mini\",\n\t\t\"iPad2,6\":\"iPad mini\",\n\t\t\"iPad2,7\":\"iPad mini\",\n\t\t\"iPhone1,1\":\"iPhone 1\",\n\t\t\"iPhone1,2\":\"iPhone 3G\",\n\t\t\"iPhone2,1\":\"iPhone 3GS\",\n\t\t\"iPhone3,1\":\"iPhone 4\",\n\t\t\"iPhone3,3\":\"iPhone 4\",\n\t\t\"iPhone4,1\":\"iPhone 4S\",\n\t\t\"iPhone5,1\":\"iPhone 5\",\n\t\t\"iPhone5,2\":\"iPhone 5\",\n\t\t\"iPhone5,1\":\"iPhone 5\",\n\t\t\"iPhone6,1\":\"iPhone 5S\",\n\t\t\"iPod1,1\":\"iPod touch 1\",\n\t\t\"iPod2,1\":\"iPod touch 2\",\n\t\t\"iPod3,1\":\"iPod touch 3\",\n\t\t\"iPod4,1\":\"iPod touch 4\",\n\t\t\"iPod5,1\":\"iPod touch 5\" }\n\nif product_type in types:\n\treturn types[product_type]\n\nreturn \"Unknown Model (%s)\" % (product_type)", "path": "main.py", "repo_name": "PicciMario/iPhone-Backup-Analyzer-2", "stars": 95, "license": "None", "language": "python", "size": 3550}
{"docstring": "# check for existence \n", "func_signal": "def readMagic(self, item_realpath):\n", "code": "\t\tif (os.path.exists(item_realpath) == 0):\n\t\t\treturn None\n\t\t\n\t\t# print file type (from magic numbers)\n\t\tfilemagic = magic.file(item_realpath)\n\t\treturn filemagic", "path": "main.py", "repo_name": "PicciMario/iPhone-Backup-Analyzer-2", "stars": 95, "license": "None", "language": "python", "size": 3550}
{"docstring": "\"\"\"\nUpdates the \"File\" menu, adds a menu item for\neach item in the MRU list.\n\"\"\"\n# Clear the current menu items of MRU files\n# (all menu items after the separator)\n", "func_signal": "def mruUpdateFileMenu(self):\n", "code": "items = self.ui.menuFile.actions()\nfound_separator = False\nfor i, item in enumerate(items):\n\tif found_separator:\n\t\tself.ui.menuFile.removeAction(item)\n\tif (not found_separator) and item==self.ui.separatorMRUList:\n\t\tfound_separator = True\n\n# Re-create MRU list\n# (Menu item for each MRU item)\nself.ui.separatorMRUList.setVisible(len(self.mru_list) != 0)\n\nfor i, m in enumerate(self.mru_list):\n\t(path, description) = m;\n\n\ttext = \"%d %s\" % (i + 1, description)\n\tif i < 9:\n\t\ttext = '&' + text\n\n\taction = self.ui.menuFile.addAction(text)\n\taction.triggered.connect(lambda p=path: self.openBackup(p))", "path": "main.py", "repo_name": "PicciMario/iPhone-Backup-Analyzer-2", "stars": 95, "license": "None", "language": "python", "size": 3550}
{"docstring": "\"\"\"\nUsage:\ncon.row_factory = namedtuple_factory\n\"\"\"\n", "func_signal": "def namedtuple_factory(cursor, row):\n", "code": "fields = [col[0] for col in cursor.description]\nRow = namedtuple(\"Row\", fields)\nreturn Row(*row)", "path": "ipba2-plugins\\plg_viber.py", "repo_name": "PicciMario/iPhone-Backup-Analyzer-2", "stars": 95, "license": "None", "language": "python", "size": 3550}
{"docstring": "\"\"\"\nPresent the \"Select Directory\" dialog to the user, then open the iOS backup.\n\"\"\"\n", "func_signal": "def openBackupGUI(self):\n", "code": "newBackupPath = QtGui.QFileDialog.getExistingDirectory(self, \"Open Directory\", \"\", QtGui.QFileDialog.ShowDirsOnly | QtGui.QFileDialog.DontResolveSymlinks);\n\nif (newBackupPath == None):\n\treturn\n\nif (len(newBackupPath) == 0):\n\treturn\n\nself.openBackup(newBackupPath)", "path": "main.py", "repo_name": "PicciMario/iPhone-Backup-Analyzer-2", "stars": 95, "license": "None", "language": "python", "size": 3550}
{"docstring": "# progress window\n", "func_signal": "def getMsgsThreaded(self, zconversation):\n", "code": "\t\tprogress = QtGui.QProgressDialog(\"Querying the database ...\", \"Abort\", 0, 0, self)\n\t\tprogress.setWindowTitle(\"Viber Browser ...\")\n\t\tprogress.setWindowModality(QtCore.Qt.WindowModal)\n\t\tprogress.setMinimumDuration(0)\n\t\tprogress.setCancelButton(None)\n\t\tprogress.show()\n\t\t\t\t\n\t\t# ** ZVIBERMESSAGE ** - ZPHONENUMBERINDEX\n\t\tzvibermessage_columns = \"ZVIBERMESSAGE.Z_PK, ZVIBERMESSAGE.ZATTACHMENT, ZVIBERMESSAGE.ZLOCATION, ZVIBERMESSAGE.ZPHONENUMINDEX, ZVIBERMESSAGE.ZDATE, ZVIBERMESSAGE.ZSTATEDATE, ZVIBERMESSAGE.ZSTATE, ZVIBERMESSAGE.ZTEXT\"\n\t\tconditions = \"ZVIBERMESSAGE.ZCONVERSATION=?;\"\n\t\tquery = \"SELECT \" + zvibermessage_columns + \" FROM ZVIBERMESSAGE WHERE \" + conditions\n\n\t\t# call a thread to query the db showing a progress bar\n\t\tqueryTh = ThreadedQuery(self.fname_contacts,query,[zconversation])\n\t\tqueryTh.start()                \n\t\twhile queryTh.isAlive():\n\t\t\tQtGui.QApplication.processEvents()\n\n\t\tprogress.close()               \n\t\tmessages = queryTh.getResult()\n# a namedtuple is returned\t\t\n\t\treturn messages", "path": "ipba2-plugins\\plg_viber.py", "repo_name": "PicciMario/iPhone-Backup-Analyzer-2", "stars": 95, "license": "None", "language": "python", "size": 3550}
{"docstring": "# opening database\n", "func_signal": "def getChats(self):\n", "code": "\t\ttry:    \n\t\t\tself.tempdb = sqlite3.connect(self.fname_contacts)\n\t\texcept:\n\t\t\tprint(\"\\nUnexpected error: %s\"%sys.exc_info()[1])\n\t\t\tself.close()\n\t\t\n\t\t# query results are retrieved as a namedtuple\n\t\t# (this step must be before cursor instantiation)\n\t\tself.tempdb.row_factory = namedtuple_factory                        \t\n\t\tself.tempcur = self.tempdb.cursor()\n# ** ZCONVERSATION  ** -> Z_3PHONENUMINDEXES -> ZPHONENUMBERINDEX -> ZABCONTACT\n\t\tzconversation_columns = \"ZCONVERSATION.Z_PK, ZCONVERSATION.ZDATE, ZCONVERSATION.ZUNREADCOUNT, ZCONVERSATION.ZGROUPID, ZCONVERSATION.ZNAME\"\n\t\tzabcontact_columns = \"ZABCONTACT.ZMAINNAME, ZABCONTACT.ZPREFIXNAME\"\n\t\tconditions = \"ZCONVERSATION.Z_PK = Z_3PHONENUMINDEXES.Z_3CONVERSATIONS AND Z_3PHONENUMINDEXES.Z_5PHONENUMINDEXES = ZPHONENUMBERINDEX.Z_PK AND ZPHONENUMBERINDEX.ZCONTACT = ZABCONTACT.Z_PK;\"\n\t\tquery = \"SELECT \" + zconversation_columns + \", \" + zabcontact_columns + \" FROM ZCONVERSATION, Z_3PHONENUMINDEXES, ZPHONENUMBERINDEX, ZABCONTACT WHERE \" + conditions\n\t\tself.tempcur.execute(query)                \n\t\t# fetches chats namedtuple\n\t\tchats = self.tempcur.fetchall()\n\t\t\t\n\t\t# closing database\n\t\tself.tempdb.close()\n# a namedtuple is returned\t\t\n\t\treturn chats", "path": "ipba2-plugins\\plg_viber.py", "repo_name": "PicciMario/iPhone-Backup-Analyzer-2", "stars": 95, "license": "None", "language": "python", "size": 3550}
{"docstring": "# opening database\n", "func_signal": "def populateUI(self):\n", "code": "\t\tself.tempdb = sqlite3.connect(self.filename)\n\t\tself.tempdb.row_factory = sqlite3.Row\n\t\tself.tempcur = self.tempdb.cursor()\n\t\n\t\t# populating tree with Safari Bookmarks\n\t\tself.insertBookmark(None, 0)\n# closing database\n\t\tself.tempdb.close()", "path": "ipba2-plugins\\plg_safbookmarks.py", "repo_name": "PicciMario/iPhone-Backup-Analyzer-2", "stars": 95, "license": "None", "language": "python", "size": 3550}
{"docstring": "\"\"\"\nUpdate the \"Window\" menus - with the list of active MDI sub windows.\n\"\"\"\n# Clear the current menu items of window titles\n# (all menu items after the separator)\n", "func_signal": "def updateWindowMenu(self):\n", "code": "items = self.ui.menuWindow.actions()\nfound_separator = False\nfor i, item in enumerate(items):\n\tif found_separator:\n\t\tself.ui.menuWindow.removeAction(item)\n\tif (not found_separator) and item==self.ui.separatorWindowList:\n\t\tfound_separator = True\n\n# Re-create list Windows\n# (Menu item for each MDI Sub window)\nwindows = self.ui.mdiArea.subWindowList()\nself.ui.separatorWindowList.setVisible(len(windows) != 0)\n\nfor i, window in enumerate(windows):\n\tchild = window.widget()\n\n\ttext = \"%d %s\" % (i + 1, child.windowTitle())\n\tif i < 9:\n\t\ttext = '&' + text\n\n\taction = self.ui.menuWindow.addAction(text)\n\taction.setCheckable(True)\n\taction.setChecked(window is self.ui.mdiArea.currentSubWindow())\n\t#NOTE: the \"w=window\" trick is used to work-around the lambda evaluation limitation,\n\t#      see here: http://stackoverflow.com/a/1107260\n\taction.triggered.connect(lambda w=window: self.ui.mdiArea.setActiveSubWindow(w))", "path": "main.py", "repo_name": "PicciMario/iPhone-Backup-Analyzer-2", "stars": 95, "license": "None", "language": "python", "size": 3550}
{"docstring": "# managing \"standard\" files\n", "func_signal": "def openSelectedPlist(self):\n", "code": "\t\tcurrentSelectedElement = self.ui.fileTree.currentItem()\n\t\tif (currentSelectedElement): pass\n\t\telse: return\n\n\t\tif (currentSelectedElement.text(1) == \"X\"):\t\n\t\t\trealFileName = os.path.join(self.backup_path, currentSelectedElement.text(0))\n\t\t\ttitle = currentSelectedElement.text(0) + \" - Plist Viewer\"\n\t\telse:\n\t\t\telement = self.getSelectedFileData()\n\t\t\tif (element == None): return\n\t\t\trealFileName = os.path.join(self.backup_path, element['fileid'])\n\t\t\ttitle = element['file_name'] + \" - Plist Viewer\"\n\t\n\t\tnewWidget = PlistWidget(realFileName)\n\t\tnewWidget.setTitle(title)", "path": "main.py", "repo_name": "PicciMario/iPhone-Backup-Analyzer-2", "stars": 95, "license": "None", "language": "python", "size": 3550}
{"docstring": "\"\"\"\nUpdate the list of Most Recently Used archives, with a newly opened archive.\n\ndescription = The \"Display name\" of the open archive (e.g. \"MyPhone (iPhone3)\")\npath = The filesystem path of the archive.\n\nThe function updates the MRU list,\nthe application settings file, and the \"File\" Menu.\n\"\"\"\n\n# Remove the path, if it already exists\n", "func_signal": "def mruAddArchive(self, description, path):\n", "code": "tmp = filter(lambda x: x[0] == path, self.mru_list)\nif len(tmp)>0:\n\tself.mru_list.remove(tmp[0])\n\n# Add the new archive, to the beginning of the list\nself.mru_list.insert(0, ( path, description ))\n\nself.mruSaveList()\nself.mruUpdateFileMenu()", "path": "main.py", "repo_name": "PicciMario/iPhone-Backup-Analyzer-2", "stars": 95, "license": "None", "language": "python", "size": 3550}
{"docstring": "\"\"\"Returns first registered (main) rule as url.\n\"\"\"\n", "func_signal": "def url(self):\n", "code": "try:\n    return url_for(\"%s.%s_%s\" % (self.admin.endpoint,\n        self.endpoint, self.rules.lists()[0][0]))\n        # Cause OrderedMultiDict.keys() doesn't preserve order...\nexcept IndexError:\n    raise Exception('`AdminModule` must provide at list one rule.')", "path": "flask_dashed\\admin.py", "repo_name": "jeanphix/Flask-Dashed", "stars": 127, "license": "None", "language": "python", "size": 1115}
{"docstring": "\"\"\"Returns all parent hierarchy as list. Usefull for breadcrumbs.\n\"\"\"\n", "func_signal": "def parents(self):\n", "code": "if self.parent:\n    parents = list(self.parent.parents)\n    parents.append(self.parent)\n    return parents\nelse:\n    return []", "path": "flask_dashed\\admin.py", "repo_name": "jeanphix/Flask-Dashed", "stars": 127, "license": "None", "language": "python", "size": 1115}
{"docstring": "\"\"\"Deletes object.\n\n:param object: The object to delete\n\"\"\"\n", "func_signal": "def delete_object(self, object):\n", "code": "self.db_session.delete(object)\nself.db_session.commit()", "path": "flask_dashed\\ext\\sqlalchemy.py", "repo_name": "jeanphix/Flask-Dashed", "stars": 127, "license": "None", "language": "python", "size": 1115}
{"docstring": "\"\"\"Adds object list rule to current app.\n\"\"\"\n", "func_signal": "def default_rules(self):\n", "code": "return [\n    ('/', 'list', self.list_view.as_view('short_title', self)),\n    ('/page/<page>', 'listpaged', self.list_view.as_view('short_title',\n        self)),\n    ('/new', 'new', self.form_view.as_view('short_title', self)),\n    ('/<pk>/edit', 'edit', self.form_view.as_view('short_title',\n        self)),\n    ('/<pk>/delete', 'delete', self.delete_view.as_view('short_title',\n        self)),\n]", "path": "flask_dashed\\admin.py", "repo_name": "jeanphix/Flask-Dashed", "stars": 127, "license": "None", "language": "python", "size": 1115}
{"docstring": "\"\"\"Counts filtered list.\n\n:param search: The string for quick search\n\"\"\"\n", "func_signal": "def count_list(self, search=None):\n", "code": "query = self._get_filtered_query(self.list_query_factory, search)\nreturn query.count()", "path": "flask_dashed\\ext\\sqlalchemy.py", "repo_name": "jeanphix/Flask-Dashed", "stars": 127, "license": "None", "language": "python", "size": 1115}
{"docstring": "\"\"\"Deletes object at given pk.\n\n:param pk: The primary key\n\"\"\"\n", "func_signal": "def get(self, pk):\n", "code": "obj = self.admin_module.get_object(pk)\nself.admin_module.delete_object(obj)\nflash(\"Object successfully deleted\", \"success\")\nreturn redirect(get_next_or(url_for(\".%s_%s\" %\n    (self.admin_module.endpoint, 'list'))))", "path": "flask_dashed\\views.py", "repo_name": "jeanphix/Flask-Dashed", "stars": 127, "license": "None", "language": "python", "size": 1115}
{"docstring": "\"\"\"Gives a way to secure specific url path.\n\n:param http_code: The response http code when False\n\"\"\"\n", "func_signal": "def secure(self, http_code=403):\n", "code": "def decorator(f):\n    self.admin.add_path_security(self.url_path, f, http_code)\n    return f\nreturn decorator", "path": "flask_dashed\\admin.py", "repo_name": "jeanphix/Flask-Dashed", "stars": 127, "license": "None", "language": "python", "size": 1115}
{"docstring": "\"\"\"Checks security for specific and path.\n\n:param path: The path to check\n\"\"\"\n", "func_signal": "def check_path_security(self, path):\n", "code": "for key in self.secure_functions.iterkeys():\n    if path.startswith(\"%s%s\" % (self.url_prefix, key)):\n        for function, http_code in self.secure_functions.getlist(key):\n            if not function():\n                return abort(http_code)", "path": "flask_dashed\\admin.py", "repo_name": "jeanphix/Flask-Dashed", "stars": 127, "license": "None", "language": "python", "size": 1115}
{"docstring": "\"\"\"Merges all view_args and request args then update with\nuser args.\n\n:param update: The user args\n\"\"\"\n", "func_signal": "def compute_args(request, update={}):\n", "code": "args = request.view_args.copy()\nargs = dict(dict(request.args.to_dict(flat=True)), **args)\nargs = dict(args, **update)\nreturn args", "path": "flask_dashed\\views.py", "repo_name": "jeanphix/Flask-Dashed", "stars": 127, "license": "None", "language": "python", "size": 1115}
{"docstring": "\"\"\"\"Returns actions for object as and tuple list.\n\n:param object: The object\n\"\"\"\n", "func_signal": "def get_actions_for_object(self, object):\n", "code": "return [\n    ('edit', 'edit', 'Edit object', url_for(\n        \"%s.%s_edit\" % (self.admin.blueprint.name, self.endpoint),\n        pk=object.id)),\n    ('delete', 'delete', 'Delete object', url_for(\n        \"%s.%s_delete\" % (self.admin.blueprint.name, self.endpoint),\n        pk=object.id)),\n]", "path": "flask_dashed\\ext\\sqlalchemy.py", "repo_name": "jeanphix/Flask-Dashed", "stars": 127, "license": "None", "language": "python", "size": 1115}
{"docstring": "\"\"\"Registers all module rules after initialization.\n\"\"\"\n", "func_signal": "def _register_rules(self):\n", "code": "if not hasattr(self, 'default_rules'):\n    raise NotImplementedError('Admin module class must provide'\n        + ' default_rules')\nfor rule, endpoint, view_func in self.default_rules:\n    self.add_url_rule(rule, endpoint, view_func)", "path": "flask_dashed\\admin.py", "repo_name": "jeanphix/Flask-Dashed", "stars": 127, "license": "None", "language": "python", "size": 1115}
{"docstring": "\"\"\"Displays form.\n\n:param pk: The object primary key\n\"\"\"\n", "func_signal": "def get(self, pk=None):\n", "code": "obj = self.object\nif pk and obj is None:\n    abort(404)\nis_new = pk is None\nform = self.admin_module.get_form(obj)\nreturn  render_template(\n    self.admin_module.edit_template,\n    admin=self.admin_module.admin,\n    module=self.admin_module,\n    object=obj,\n    form=form,\n    is_new=is_new\n)", "path": "flask_dashed\\views.py", "repo_name": "jeanphix/Flask-Dashed", "stars": 127, "license": "None", "language": "python", "size": 1115}
{"docstring": "\"\"\"Returns object related attributes, as it's a template filter None\nis return when attribute doesn't exists.\n\neg::\n\n    a = object()\n    a.b = object()\n    a.b.c = 1\n    recursive_getattr(a, 'b.c') => 1\n    recursive_getattr(a, 'b.d') => None\n\"\"\"\n", "func_signal": "def recursive_getattr(obj, attr):\n", "code": "try:\n    if \".\" not in attr:\n            return getattr(obj, attr)\n    else:\n        l = attr.split('.')\n        return recursive_getattr(getattr(obj, l[0]), '.'.join(l[1:]))\nexcept AttributeError:\n    return None", "path": "flask_dashed\\admin.py", "repo_name": "jeanphix/Flask-Dashed", "stars": 127, "license": "None", "language": "python", "size": 1115}
{"docstring": "\"\"\"Gives a way to secure specific url path.\n\n:param endpoint: The endpoint to protect\n:param http_code: The response http code when False\n\"\"\"\n", "func_signal": "def secure_endpoint(self, endpoint,  http_code=403):\n", "code": "def decorator(f):\n    self._secure_enpoint(endpoint, f, http_code)\n    return f\nreturn decorator", "path": "flask_dashed\\admin.py", "repo_name": "jeanphix/Flask-Dashed", "stars": 127, "license": "None", "language": "python", "size": 1115}
{"docstring": "\"\"\"Saves object.\n\n:param object: The object to save\n\"\"\"\n", "func_signal": "def save_object(self, obj):\n", "code": "self.db_session.add(obj)\nself.db_session.commit()", "path": "flask_dashed\\ext\\sqlalchemy.py", "repo_name": "jeanphix/Flask-Dashed", "stars": 127, "license": "None", "language": "python", "size": 1115}
{"docstring": "\"\"\"Displays object list.\n\n:param page: The current page index\n\"\"\"\n", "func_signal": "def get(self, page=1):\n", "code": "page = int(page)\nsearch = request.args.get('search', None)\norder_by = request.args.get('orderby', None)\norder_direction = request.args.get('orderdir', None)\ncount = self.admin_module.count_list(search=search)\nreturn  render_template(\n    self.admin_module.list_template,\n    admin=self.admin_module.admin,\n    module=self.admin_module,\n    objects=self.admin_module.get_object_list(\n        search=search,\n        offset=self.admin_module.list_per_page * (page - 1),\n        limit=self.admin_module.list_per_page,\n        order_by_name=order_by,\n        order_by_direction=order_direction,\n    ),\n    count=count,\n    current_page=page,\n    pages=self.iter_pages(count, page),\n    compute_args=compute_args\n)", "path": "flask_dashed\\views.py", "repo_name": "jeanphix/Flask-Dashed", "stars": 127, "license": "None", "language": "python", "size": 1115}
{"docstring": "\"\"\"Secures view function.\n\"\"\"\n", "func_signal": "def secure(endpoint, function, http_code):\n", "code": "def decorator(view_func):\n    @wraps(view_func)\n    def _wrapped_view(self, *args, **kwargs):\n        if not function(self, *args, **kwargs):\n            return abort(http_code)\n        return view_func(self, *args, **kwargs)\n    return _wrapped_view\nreturn decorator", "path": "flask_dashed\\views.py", "repo_name": "jeanphix/Flask-Dashed", "stars": 127, "license": "None", "language": "python", "size": 1115}
{"docstring": "\"\"\"Secure enpoint view function via `secure` decorator.\n\n:param enpoint: The endpoint to secure\n:param secure_function: The function to check\n:param http_code: The response http code when False.\n\"\"\"\n", "func_signal": "def _secure_enpoint(self, endpoint, secure_function, http_code):\n", "code": "rule, endpoint, view_func = self.rules.get(endpoint)\nview_func.view_class.dispatch_request =\\\n    secure(endpoint, secure_function, http_code)(\n        view_func.view_class.dispatch_request)", "path": "flask_dashed\\admin.py", "repo_name": "jeanphix/Flask-Dashed", "stars": 127, "license": "None", "language": "python", "size": 1115}
{"docstring": "\"\"\"Gets object required by the form.\n\n:param pk: The object primary key\n\"\"\"\n", "func_signal": "def object(self):\n", "code": "if not hasattr(self, '_object'):\n    if 'pk' in request.view_args:\n        self._object = self.admin_module.get_object(\n            request.view_args['pk'])\n    else:\n        self._object = self.admin_module.create_object()\nreturn self._object", "path": "flask_dashed\\views.py", "repo_name": "jeanphix/Flask-Dashed", "stars": 127, "license": "None", "language": "python", "size": 1115}
{"docstring": "\"\"\"Returns the url path relative to admin one.\n\"\"\"\n", "func_signal": "def url_path(self):\n", "code": "if self.parent:\n    return self.parent.url_path + self.url_prefix\nelse:\n    return self.url_prefix", "path": "flask_dashed\\admin.py", "repo_name": "jeanphix/Flask-Dashed", "stars": 127, "license": "None", "language": "python", "size": 1115}
{"docstring": "\"\"\"Dict-like items() that returns a list of name-value tuples from the jar.\nSee keys() and values(). Allows client-code to call \"dict(RequestsCookieJar)\nand get a vanilla python dict of key value pairs.\"\"\"\n", "func_signal": "def items(self):\n", "code": "items = []\nfor cookie in iter(self):\n    items.append((cookie.name, cookie.value))\nreturn items", "path": "Kindle Bookstore\\alp\\request\\requests\\cookies.py", "repo_name": "phyllisstein/Workflows", "stars": 70, "license": "None", "language": "python", "size": 5177}
{"docstring": "\"\"\"Produce an appropriate Cookie header string to be sent with `request`, or None.\"\"\"\n", "func_signal": "def get_cookie_header(jar, request):\n", "code": "r = MockRequest(request)\njar.add_cookie_header(r)\nreturn r.get_new_headers().get('Cookie')", "path": "Kindle Bookstore\\alp\\request\\requests\\cookies.py", "repo_name": "phyllisstein/Workflows", "stars": 70, "license": "None", "language": "python", "size": 5177}
{"docstring": "\"\"\"Remove item from six.moves.\"\"\"\n", "func_signal": "def remove_move(name):\n", "code": "try:\n    delattr(_MovedItems, name)\nexcept AttributeError:\n    try:\n        del moves.__dict__[name]\n    except KeyError:\n        raise AttributeError(\"no such move, %r\" % (name,))", "path": "Kindle Bookstore UK\\PyAl\\Request\\requests\\packages\\urllib3\\packages\\six.py", "repo_name": "phyllisstein/Workflows", "stars": 70, "license": "None", "language": "python", "size": 5177}
{"docstring": "\"\"\"Convert a Morsel object into a Cookie containing the one k/v pair.\"\"\"\n", "func_signal": "def morsel_to_cookie(morsel):\n", "code": "c = create_cookie(\n    name=morsel.key,\n    value=morsel.value,\n    version=morsel['version'] or 0,\n    port=None,\n    port_specified=False,\n    domain=morsel['domain'],\n    domain_specified=bool(morsel['domain']),\n    domain_initial_dot=morsel['domain'].startswith('.'),\n    path=morsel['path'],\n    path_specified=bool(morsel['path']),\n    secure=bool(morsel['secure']),\n    expires=morsel['max-age'] or morsel['expires'],\n    discard=False,\n    comment=morsel['comment'],\n    comment_url=bool(morsel['comment']),\n    rest={'HttpOnly': morsel['httponly']},\n    rfc2109=False,)\nreturn c", "path": "Kindle Bookstore\\alp\\request\\requests\\cookies.py", "repo_name": "phyllisstein/Workflows", "stars": 70, "license": "None", "language": "python", "size": 5177}
{"docstring": "\"\"\"Closes all adapters and as such the session\"\"\"\n", "func_signal": "def close(self):\n", "code": "for _, v in self.adapters.items():\n    v.close()", "path": "Yourls Stats\\alp\\request\\requests\\sessions.py", "repo_name": "phyllisstein/Workflows", "stars": 70, "license": "None", "language": "python", "size": 5177}
{"docstring": "\"\"\"Returns a CookieJar from a key/value dictionary.\n\n:param cookie_dict: Dict of key/values to insert into CookieJar.\n\"\"\"\n", "func_signal": "def cookiejar_from_dict(cookie_dict, cookiejar=None):\n", "code": "if cookiejar is None:\n    cookiejar = RequestsCookieJar()\n\nif cookie_dict is not None:\n    for name in cookie_dict:\n        cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))\nreturn cookiejar", "path": "Kindle Bookstore\\alp\\request\\requests\\cookies.py", "repo_name": "phyllisstein/Workflows", "stars": 70, "license": "None", "language": "python", "size": 5177}
{"docstring": "\"\"\"Utility method to list all the paths in the jar.\"\"\"\n", "func_signal": "def list_paths(self):\n", "code": "paths = []\nfor cookie in iter(self):\n    if cookie.path not in paths:\n        paths.append(cookie.path)\nreturn paths", "path": "Kindle Bookstore\\alp\\request\\requests\\cookies.py", "repo_name": "phyllisstein/Workflows", "stars": 70, "license": "None", "language": "python", "size": 5177}
{"docstring": "\"\"\"Extract the cookies from the response into a CookieJar.\n\n:param jar: cookielib.CookieJar (not necessarily a RequestsCookieJar)\n:param request: our own requests.Request object\n:param response: urllib3.HTTPResponse object\n\"\"\"\n# the _original_response field is the wrapped httplib.HTTPResponse object,\n", "func_signal": "def extract_cookies_to_jar(jar, request, response):\n", "code": "req = MockRequest(request)\n# pull out the HTTPMessage with the headers and put it in the mock:\nres = MockResponse(response._original_response.msg)\njar.extract_cookies(res, req)", "path": "Kindle Bookstore\\alp\\request\\requests\\cookies.py", "repo_name": "phyllisstein/Workflows", "stars": 70, "license": "None", "language": "python", "size": 5177}
{"docstring": "\"\"\"Utility method to list all the domains in the jar.\"\"\"\n", "func_signal": "def list_domains(self):\n", "code": "domains = []\nfor cookie in iter(self):\n    if cookie.domain not in domains:\n        domains.append(cookie.domain)\nreturn domains", "path": "Kindle Bookstore\\alp\\request\\requests\\cookies.py", "repo_name": "phyllisstein/Workflows", "stars": 70, "license": "None", "language": "python", "size": 5177}
{"docstring": "\"\"\"Takes as an argument an optional domain and path and returns a plain old\nPython dict of name-value pairs of cookies that meet the requirements.\"\"\"\n", "func_signal": "def get_dict(self, domain=None, path=None):\n", "code": "dictionary = {}\nfor cookie in iter(self):\n    if (domain is None or cookie.domain == domain) and (path is None\n                                        or cookie.path == path):\n        dictionary[cookie.name] = cookie.value\nreturn dictionary", "path": "Kindle Bookstore\\alp\\request\\requests\\cookies.py", "repo_name": "phyllisstein/Workflows", "stars": 70, "license": "None", "language": "python", "size": 5177}
{"docstring": "#: A case-insensitive dictionary of headers to be sent on each\n        #: :class:`Request <Request>` sent from this\n        #: :class:`Session <Session>`.\n", "func_signal": "def __init__(self):\n", "code": "        self.headers = default_headers()\n#: Default Authentication tuple or object to attach to\n        #: :class:`Request <Request>`.\n        self.auth = None\n#: Dictionary mapping protocol to the URL of the proxy (e.g.\n        #: {'http': 'foo.bar:3128'}) to be used on each\n        #: :class:`Request <Request>`.\n        self.proxies = {}\n#: Event-handling hooks.\n        self.hooks = default_hooks()\n#: Dictionary of querystring data to attach to each\n        #: :class:`Request <Request>`. The dictionary values may be lists for\n        #: representing multivalued query parameters.\n        self.params = {}\n#: Stream response content default.\n        self.stream = False\n#: SSL Verification default.\n        self.verify = True\n#: SSL certificate default.\n        self.cert = None\n#: Maximum number of redirects allowed. If the request exceeds this\n        #: limit, a :class:`TooManyRedirects` exception is raised.\n        self.max_redirects = DEFAULT_REDIRECT_LIMIT\n#: Should we trust the environment?\n        self.trust_env = True\n# Set up a CookieJar to be used by default\n        self.cookies = cookiejar_from_dict({})\n# Default connection adapters.\n        self.adapters = {}\n        self.mount('http://', HTTPAdapter())\n        self.mount('https://', HTTPAdapter())", "path": "Yourls Stats\\alp\\request\\requests\\sessions.py", "repo_name": "phyllisstein/Workflows", "stars": 70, "license": "None", "language": "python", "size": 5177}
{"docstring": "\"\"\"Dict-like set() that also supports optional domain and path args in\norder to resolve naming collisions from using one cookie jar over\nmultiple domains.\"\"\"\n# support client code that unsets cookies by assignment of a None value:\n", "func_signal": "def set(self, name, value, **kwargs):\n", "code": "if value is None:\n    remove_cookie_by_name(self, name, domain=kwargs.get('domain'), path=kwargs.get('path'))\n    return\n\nif isinstance(value, Morsel):\n    c = morsel_to_cookie(value)\nelse:\n    c = create_cookie(name, value, **kwargs)\nself.set_cookie(c)\nreturn c", "path": "Kindle Bookstore\\alp\\request\\requests\\cookies.py", "repo_name": "phyllisstein/Workflows", "stars": 70, "license": "None", "language": "python", "size": 5177}
{"docstring": "\"\"\"Unlike a normal CookieJar, this class is pickleable.\"\"\"\n", "func_signal": "def __setstate__(self, state):\n", "code": "self.__dict__.update(state)\nif '_cookies_lock' not in self.__dict__:\n    self._cookies_lock = threading.RLock()", "path": "Kindle Bookstore\\alp\\request\\requests\\cookies.py", "repo_name": "phyllisstein/Workflows", "stars": 70, "license": "None", "language": "python", "size": 5177}
{"docstring": "\"\"\"Sends a HEAD request. Returns :class:`Response` object.\n\n:param url: URL for the new :class:`Request` object.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n\"\"\"\n\n", "func_signal": "def head(self, url, **kwargs):\n", "code": "kwargs.setdefault('allow_redirects', False)\nreturn self.request('HEAD', url, **kwargs)", "path": "Yourls Stats\\alp\\request\\requests\\sessions.py", "repo_name": "phyllisstein/Workflows", "stars": 70, "license": "None", "language": "python", "size": 5177}
{"docstring": "\"\"\"Send a given PreparedRequest.\"\"\"\n# Set defaults that the hooks can utilize to ensure they always have\n# the correct parameters to reproduce the previous request.\n", "func_signal": "def send(self, request, **kwargs):\n", "code": "kwargs.setdefault('stream', self.stream)\nkwargs.setdefault('verify', self.verify)\nkwargs.setdefault('cert', self.cert)\nkwargs.setdefault('proxies', self.proxies)\n\n# It's possible that users might accidentally send a Request object.\n# Guard against that specific failure case.\nif getattr(request, 'prepare', None):\n    raise ValueError('You can only send PreparedRequests.')\n\n# Set up variables needed for resolve_redirects and dispatching of\n# hooks\nallow_redirects = kwargs.pop('allow_redirects', True)\nstream = kwargs.get('stream')\ntimeout = kwargs.get('timeout')\nverify = kwargs.get('verify')\ncert = kwargs.get('cert')\nproxies = kwargs.get('proxies')\nhooks = request.hooks\n\n# Get the appropriate adapter to use\nadapter = self.get_adapter(url=request.url)\n\n# Start time (approximately) of the request\nstart = datetime.utcnow()\n# Send the request\nr = adapter.send(request, **kwargs)\n# Total elapsed time of the request (approximately)\nr.elapsed = datetime.utcnow() - start\n\n# Response manipulation hooks\nr = dispatch_hook('response', hooks, r, **kwargs)\n\n# Persist cookies\nextract_cookies_to_jar(self.cookies, request, r.raw)\n\n# Redirect resolving generator.\ngen = self.resolve_redirects(r, request, stream=stream,\n                             timeout=timeout, verify=verify, cert=cert,\n                             proxies=proxies)\n\n# Resolve redirects if allowed.\nhistory = [resp for resp in gen] if allow_redirects else []\n\n# Shuffle things around if there's history.\nif history:\n    # Insert the first (original) request at the start\n    history.insert(0, r)\n    # Get the last request made\n    r = history.pop()\n    r.history = tuple(history)\n\nreturn r", "path": "Yourls Stats\\alp\\request\\requests\\sessions.py", "repo_name": "phyllisstein/Workflows", "stars": 70, "license": "None", "language": "python", "size": 5177}
{"docstring": "\"\"\"Import module, returning the module after the last dot.\"\"\"\n", "func_signal": "def _import_module(name):\n", "code": "__import__(name)\nreturn sys.modules[name]", "path": "Kindle Bookstore UK\\PyAl\\Request\\requests\\packages\\urllib3\\packages\\six.py", "repo_name": "phyllisstein/Workflows", "stars": 70, "license": "None", "language": "python", "size": 5177}
{"docstring": "\"\"\"Unsets a cookie by name, by default over all domains and paths.\n\nWraps CookieJar.clear(), is O(n).\n\"\"\"\n", "func_signal": "def remove_cookie_by_name(cookiejar, name, domain=None, path=None):\n", "code": "clearables = []\nfor cookie in cookiejar:\n    if cookie.name == name:\n        if domain is None or domain == cookie.domain:\n            if path is None or path == cookie.path:\n                clearables.append((cookie.domain, cookie.path, cookie.name))\n\nfor domain, path, name in clearables:\n    cookiejar.clear(domain, path, name)", "path": "Kindle Bookstore\\alp\\request\\requests\\cookies.py", "repo_name": "phyllisstein/Workflows", "stars": 70, "license": "None", "language": "python", "size": 5177}
{"docstring": "\"\"\"__get_item__ and get call _find_no_duplicates -- never used in Requests internally.\nTakes as args name and optional domain and path. Returns a cookie.value.\nThrows KeyError if cookie is not found and CookieConflictError if there are\nmultiple cookies that match name and optionally domain and path.\"\"\"\n", "func_signal": "def _find_no_duplicates(self, name, domain=None, path=None):\n", "code": "toReturn = None\nfor cookie in iter(self):\n    if cookie.name == name:\n        if domain is None or cookie.domain == domain:\n            if path is None or cookie.path == path:\n                if toReturn is not None:  # if there are multiple cookies that meet passed in criteria\n                    raise CookieConflictError('There are multiple cookies with name, %r' % (name))\n                toReturn = cookie.value  # we will eventually return this as long as no cookie conflict\n\nif toReturn:\n    return toReturn\nraise KeyError('name=%r, domain=%r, path=%r' % (name, domain, path))", "path": "Kindle Bookstore\\alp\\request\\requests\\cookies.py", "repo_name": "phyllisstein/Workflows", "stars": 70, "license": "None", "language": "python", "size": 5177}
{"docstring": "\"\"\"Merges kwarg dictionaries.\n\nIf a local key in the dictionary is set to None, it will be removed.\n\"\"\"\n\n", "func_signal": "def merge_kwargs(local_kwarg, default_kwarg):\n", "code": "if default_kwarg is None:\n    return local_kwarg\n\nif isinstance(local_kwarg, str):\n    return local_kwarg\n\nif local_kwarg is None:\n    return default_kwarg\n\n# Bypass if not a dictionary (e.g. timeout)\nif not hasattr(default_kwarg, 'items'):\n    return local_kwarg\n\ndefault_kwarg = from_key_val_list(default_kwarg)\nlocal_kwarg = from_key_val_list(local_kwarg)\n\n# Update new values in a case-insensitive way\ndef get_original_key(original_keys, new_key):\n    \"\"\"\n    Finds the key from original_keys that case-insensitive matches new_key.\n    \"\"\"\n    for original_key in original_keys:\n        if key.lower() == original_key.lower():\n            return original_key\n    return new_key\n\nkwargs = default_kwarg.copy()\noriginal_keys = kwargs.keys()\nfor key, value in local_kwarg.items():\n    kwargs[get_original_key(original_keys, key)] = value\n\n# Remove keys that are set to None.\nfor (k, v) in local_kwarg.items():\n    if v is None:\n        del kwargs[k]\n\nreturn kwargs", "path": "Yourls Stats\\alp\\request\\requests\\sessions.py", "repo_name": "phyllisstein/Workflows", "stars": 70, "license": "None", "language": "python", "size": 5177}
{"docstring": "\"\"\"Sends a GET request. Returns :class:`Response` object.\n\n:param url: URL for the new :class:`Request` object.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n\"\"\"\n\n", "func_signal": "def get(self, url, **kwargs):\n", "code": "kwargs.setdefault('allow_redirects', True)\nreturn self.request('GET', url, **kwargs)", "path": "Yourls Stats\\alp\\request\\requests\\sessions.py", "repo_name": "phyllisstein/Workflows", "stars": 70, "license": "None", "language": "python", "size": 5177}
{"docstring": "\"\"\"Accepts filename string,\n    and numpy array.\n    Saves png image representing array.\n    Returns whether it was saved.\n\"\"\"\n", "func_signal": "def array_to_file(filename, a):\n", "code": "a = normalize_array(a)\ni = Image.fromarray(a.astype('uint8'))\nreturn i.save(filename)", "path": "pca\\display_network.py", "repo_name": "jperla/neural", "stars": 98, "license": "None", "language": "python", "size": 2170}
{"docstring": "\"\"\"Accepts flat 1-D vector theta.\n    Pulls out the weight vectors and returns them for \n        sparse autoencoding.\n\"\"\"\n#TODO: jperla: generalize\n", "func_signal": "def unflatten_params(theta, hidden_size, visible_size):\n", "code": "hv = hidden_size * visible_size\nW1 = theta[:hv].reshape(hidden_size, visible_size)\nW2 = theta[hv:2*hv].reshape(visible_size, hidden_size)\nb1 = theta[2*hv:2*hv+hidden_size]\nb2 = theta[2*hv+hidden_size:]\nreturn W1, W2, b1, b2", "path": "rae\\sparse_autoencoder.py", "repo_name": "jperla/neural", "stars": 98, "license": "None", "language": "python", "size": 2170}
{"docstring": "\"\"\"Accepts filename.\n    Reads in MNIST data.\n    Returns 3-tuple of training, validation, and test set.\n\"\"\"\n", "func_signal": "def read_mnist_file(filename):\n", "code": "with gzip.open(filename, 'rb') as f:\n    train_set, valid_set, test_set = cPickle.load(f)\nreturn train_set, valid_set, test_set", "path": "deep\\sample_images.py", "repo_name": "jperla/neural", "stars": 98, "license": "None", "language": "python", "size": 2170}
{"docstring": "\"\"\"Accepts an array of images.\n    images.ndim = (xdim, ydim, num_images)\n    Also accepts the size of the sample, a 2-tuple, (xdim, ydim).\n   Returns an array of flattened images.\n        Will be a (size[0]*size[1]) x num_samples size array.\n\"\"\"\n", "func_signal": "def sample(images, num_samples, size=(8,8), norm=(0.1, 0.9)):\n", "code": "d = np.array([random_sample(images, size) for i in xrange(num_samples)]).T\nif norm is not None:\n    output = normalize_data(d, norm)\nelse:\n    output = d\nreturn output", "path": "deep\\sample_images.py", "repo_name": "jperla/neural", "stars": 98, "license": "None", "language": "python", "size": 2170}
{"docstring": "\"\"\"\n% softmaxTrain Train a softmax model with the given parameters on the given\n% data. Returns softmaxOptTheta, a vector containing the trained parameters\n% for the model.\n%\n% input_size: the size of an input vector x^(i)\n% num_classes: the number of classes \n% weight_decay: weight decay parameter\n% input_data: an N by M matrix containing the input data, such that\n%            inputData(:, c) is the cth input\n% labels: M by 1 matrix containing the class labels for the\n%            corresponding inputs. labels(c) is the class label for\n%            the cth input\n%  max_iter: number of iterations to train for\n\"\"\"\n# initialize parameters\n", "func_signal": "def softmax_train(input_size, num_classes, weight_decay, data, labels, max_iter):\n", "code": "theta = 0.005 * np.random.randn(num_classes * input_size)\n\nsc = lambda x: softmax_cost(x, num_classes,\n                               input_size,\n                               weight_decay,\n                               data,\n                               labels)\n\n# Train!\ntrained, cost, d = scipy.optimize.lbfgsb.fmin_l_bfgs_b(sc, theta,\n                                                       maxfun=max_iter,\n                                                       m=100,\n                                                       factr=1.0,\n                                                       pgtol=1e-20,\n                                                       iprint=1)\nreturn trained.reshape((num_classes, input_size))", "path": "deep\\softmax.py", "repo_name": "jperla/neural", "stars": 98, "license": "None", "language": "python", "size": 2170}
{"docstring": "\"\"\"Accepts numpy array, and norm_max integer of max value.\n   Returns an array normalized to go from 0 to norm_max.\n\"\"\"\n", "func_signal": "def normalize_array(a, norm_max=255):\n", "code": "c = a - np.min(a.flatten())\nc = c / np.max(c)\ncentered = c * norm_max\nreturn centered", "path": "pca\\display_network.py", "repo_name": "jperla/neural", "stars": 98, "license": "None", "language": "python", "size": 2170}
{"docstring": "\"\"\"Accepts number of hidde states in sparse encoder,\n        and number of input states in sparse encoder..\n   Initialize parameters randomly based on layer sizes.\n   Returns a new flat array of size 2*visisble_size + hidden_size\n\"\"\"\n", "func_signal": "def initialize_params(hidden_size, visible_size):\n", "code": "assert hidden_size <= visible_size\n\n#we'll choose weights uniformly from the interval [-r, r]\nr  = np.sqrt(6) / np.sqrt(hidden_size + visible_size + 1)\nW1 = np.random.rand(hidden_size, visible_size) * 2 * r - r\nW2 = np.random.rand(visible_size, hidden_size) * 2 * r - r\n\nb1 = np.zeros(hidden_size)\nb2 = np.zeros(visible_size)\n\n\"\"\"\n% Convert weights and bias gradients to the vector form.\n% This step will \"unroll\" (flatten and concatenate together) all \n% your parameters into a vector, which can then be used with minFunc. \n\"\"\"\n#TODO: jperla: make this a function\nreturn neurolib.flatten_params(W1, W2, b1, b2)", "path": "rae\\sparse_autoencoder.py", "repo_name": "jperla/neural", "stars": 98, "license": "None", "language": "python", "size": 2170}
{"docstring": "\"\"\"\n% num_classes - the number of classes \n% input_size - the size N of the input vector\n% weight_decay - weight decay parameter\n% data - the N x M input matrix, where each column data(:, i) corresponds to\n%        a single test set\n% labels - an M x 1 matrix containing the labels corresponding for the input data\n\"\"\"\n# Unroll the parameters from theta\n", "func_signal": "def softmax_cost(theta, num_classes, input_size, weight_decay, data, labels):\n", "code": "theta = theta.reshape((num_classes, input_size))\nnum_data = data.shape[1]\nassert theta.shape == (num_classes, input_size)\n\nassert data.shape == (input_size, num_data)\n# efficiently build large sparse matrix\nij = np.array([[i, l] for i,l in enumerate(labels)]).T\nground_truth = scipy.sparse.coo_matrix((np.ones(num_data), ij),\n                            shape=(num_data, num_classes)).todense().T\nassert ground_truth.shape == (num_classes, num_data)\n\n# calculate cost function\nfull_prediction = np.dot(theta, data)\nmax_prediction = np.max(full_prediction, axis=0)\nshrunk_prediction = full_prediction - max_prediction\nexp_shrunk_prediction = np.exp(shrunk_prediction) \nprediction = (exp_shrunk_prediction / \n                                np.sum(exp_shrunk_prediction, axis=0))\nassert full_prediction.shape == (num_classes, num_data)\nassert max_prediction.shape == (num_data,)\nassert shrunk_prediction.shape == (num_classes, num_data)\nassert exp_shrunk_prediction.shape == (num_classes, num_data)\nassert prediction.shape == (num_classes, num_data)\n\nlog_term = np.log(prediction)\ncost = -1 * np.sum(np.multiply(ground_truth, log_term)) / num_data\ncost += (0.5 * weight_decay) * np.sum(theta**2)\n# done calculating cost function\n\n# calculate gradient of cost function\ntheta_grad = np.zeros(theta.shape)\n\n#import pdb; pdb.set_trace()\ngp = ground_truth - prediction\ntheta_grad = np.dot(data, gp.T).T * (-1.0 / num_data)\ntheta_grad += weight_decay * theta\nassert theta_grad.shape == theta.shape\n\nreturn cost, np.array(theta_grad).flatten()", "path": "deep\\softmax.py", "repo_name": "jperla/neural", "stars": 98, "license": "None", "language": "python", "size": 2170}
{"docstring": "\"\"\"Accepts numpy array, and norm_max integer of max value.\n   Returns an array normalized to go from 0 to norm_max.\n\"\"\"\n", "func_signal": "def normalize_array(a, norm_max=255):\n", "code": "c = a - np.min(a.flatten())\nc = c / np.max(c)\ncentered = c * norm_max\nreturn centered", "path": "selftaught\\display_network.py", "repo_name": "jperla/neural", "stars": 98, "license": "None", "language": "python", "size": 2170}
{"docstring": "\"\"\"Accepts two 1-d arrays (N,)-shape with integer class labels.\n    Arrays must be of same size, and one predicts \n        the other class labels.\n    Returns a 2-d array of NxN size.\n\"\"\"\n", "func_signal": "def confusion_matrix(a, b):\n", "code": "N = a.shape[0]\nassert (N,) == a.shape == b.shape\n\ncm = np.zeros((N, N))\nfrom itertools import izip\n#TODO: jperla: might be faster with sparse matrices\nfor i, j in izip(a, b):\n    cm[i,j] += 1\nreturn cm", "path": "rae\\scratch.py", "repo_name": "jperla/neural", "stars": 98, "license": "None", "language": "python", "size": 2170}
{"docstring": "\"\"\"Accepts two 2-d arrays of predicted and gold-truth class label\n    integers from multinomial distribution.\n    Returns 4-tuple of reals of (precision, recall, accuracy, F1 score)\n\"\"\"\n", "func_signal": "def  getAccuracy(predicted, gold):\n", "code": "cm = confusion_matrix(gold, predicted)\nn = len(gold)\n\nrowsums = np.max(np.vstack([np.sum(cm, axis=0), np.repeat(1e-5, n)]),\n                 axis=0)\ncolsums = np.max(np.vstack([np.sum(cm, axis=1), np.repeat(1e-5, n)]),\n                 axis=0)\n\nprecision = np.mean(np.diag(cm) / colsums)\nrecall = np.mean(np.diag(cm) / rowsums)\naccuracy = np.sum(np.diag(cm)) / np.sum(cm)\nf1 = 2 * precision * recall / (precision + recall)\nreturn precision, recall, accuracy, f1", "path": "rae\\scratch.py", "repo_name": "jperla/neural", "stars": 98, "license": "None", "language": "python", "size": 2170}
{"docstring": "\"\"\"Accepts filename string,\n    2-d numpy array of images,\n    and padding (default 1) number of black pixels between images.\n    Each column of images is a filter. \n\n    This function visualizes filters in matrix images. \n    It will reshape each column into a square image and visualizes\n        on each cell of the visualization panel. \n\n    Returns True on success.\n\n    % TODO: jperla:\n    % All other parameters are optional, usually you do not need to worry\n    % about it.\n    % opt_normalize: whether we need to normalize the filter so that all of\n    % them can have similar contrast. Default value is true.\n    % opt_graycolor: whether we use gray as the heat map. Default is true.\n    % cols: how many columns are there in the display. Default value is the\n    % squareroot of the number of columns in A.\n    % opt_colmajor: you can switch convention to row major for A. In that\n    % case, each row of A is a filter. Default value is false.\n    warning off all\n\n    Also accepts num_samples integer.  Takes first num_samples images\n        if there are too many.\n\"\"\"\n", "func_signal": "def display_network(filename, images, padding=1, num_samples=200):\n", "code": "if images.shape[1] > num_samples:\n    images = images[:,:num_samples]\n\n# first figure out the shape and size of everything\ns, n = images.shape\nd = int(math.sqrt(s))\nassert d * d == s, 'Images must be square'\ncols = int(math.sqrt(n))\nrows = int(n / cols) + (1 if n % cols > 0 else 0)\n\n# black background in output\np = padding\noutput = np.zeros((p + rows * (d + p), p + cols * (d + p)))\noutput += np.min(images.flatten())\n# then fill in the output\nfor i in xrange(n):\n    r,c = int(i / cols), i % cols\n    image = images[:,i]\n    image.shape = (d,d)\n    x,y = (r*(d+p))+p, (c*(d+p))+p\n    output[x:x+d,y:y+d] = image\n\n# and save it \nreturn array_to_file(filename, output)", "path": "pca\\display_network.py", "repo_name": "jperla/neural", "stars": 98, "license": "None", "language": "python", "size": 2170}
{"docstring": "\"\"\"\n    Accepts esize integer of embedded representation size.\n        vsize integer of input size into autoencoder,\n            (typically a multiple of esize).\n        cat_size integer of dimensionality of multinomial categories\n        dictionary_length integer of number of words in vocab.\n    Returns one flat array of parameters initialized randomly.\n\n    Initialize parameters randomly based on layer sizes.\n\"\"\"\n#We'll choose weights uniformly from the interval [-r, r]\n", "func_signal": "def initialize_params(esize, vsize, cat_size, dictionary_length):\n", "code": "r  = np.sqrt(6) / np.sqrt(esize + vsize + 1)\n\nW1 = np.random.rand(esize, vsize) * 2 * r - r;\nW2 = np.random.rand(esize, vsize) * 2 * r - r;\nW3 = np.random.rand(vsize, esize) * 2 * r - r;\nW4 = np.random.rand(vsize, esize) * 2 * r - r;\n\nWe = 1e-3 * (np.random.rand(esize, dictionary_length) * 2 * r - r)\n\nWcat = np.random.rand(cat_size, esize) * 2 * r - r;\n\nb1 = np.zeros(esize, 1);\nb2 = np.zeros(vsize, 1);\nb3 = np.zeros(vsize, 1);\nbcat = np.zeros(cat_size, 1);\n\n# Convert weights and bias gradients to the vector form.\n# This step will \"unroll\" (flatten and concatenate together) all\n# your parameters into a vector, which can then be used with minFunc.\nreturn neurolib.flatten_params(W1, W2, W3, W4, b1, b2, b3, Wcat, bcat, We)", "path": "rae\\scratch.py", "repo_name": "jperla/neural", "stars": 98, "license": "None", "language": "python", "size": 2170}
{"docstring": "\"\"\"Accepts theta 1-d array of parameters,\n    has_Wcat boolean of whether parameters have Wcat,bcat in it,\n    esize integer size of embedded word vector size\n    cat_size is number of dimensions in binary array \n        (1 for on binary categories, >1 for multinomial categories)\n    dictionary_length integer number of words in dictionary\n    Returns 10 arrays of all parameters for RAEs.\n\"\"\"\n", "func_signal": "def get_W(theta, has_Wcat, esize, cat_size, dictionary_length):\n", "code": "Wcat = []\nbcat = []\nvsize = esize * 2;\nwsize = esize*(vsize/2)\n\nW1, W2, W3, W4, b1, b2, b3, rest = np.split(theta, np.cumsum([wsize]*4 + [esize]*3))\n\nif has_Wcat:\n    Wcat, bcat, We = np.split(rest, np.cumsum([cat_size*esize, cat_size]))\n    Wcat.reshape(cat_size, esize)\n    assert bcat.shape == cat_size\nelse:\n    We = rest\n\nW1.reshape(esize, vsize/2)\nW2.reshape(esize, vsize/2)\nW3.reshape(vsize/2, esize)\nW4.reshape(vsize/2, esize)\nWe.reshape(esize, dictionary_length)\n\nassert b1.shape == b2.shape == b3.shape == (esize,)\n                     \nreturn (W1, W2, W3, W4, b1, b2, b3, Wcat, bcat, We)", "path": "rae\\scratch.py", "repo_name": "jperla/neural", "stars": 98, "license": "None", "language": "python", "size": 2170}
{"docstring": "\"\"\"Accepts a 2x1 vector x: (x1, x2).\n    Returns the value of h(x) at x, \n        and its true gradient (partial derivatives w/ respect to x1, x2).\n    h(x1, x2) = x1^2 + 3*x1*x2\n\"\"\"\n", "func_signal": "def simple_quadratic(x):\n", "code": "value = x[0]**2 + (3 * x[0] * x[1])\n\ngrad = np.zeros((2,));\ngrad[0]  = (2 * x[0]) + (3 * x[1])\ngrad[1]  = (3 * x[0])\nreturn value, grad", "path": "softmax\\test_numerical_gradient.py", "repo_name": "jperla/neural", "stars": 98, "license": "None", "language": "python", "size": 2170}
{"docstring": "\"\"\"Accepts an array of images.\n    images.ndim = (xdim, ydim, num_images)\n    Also accepts the size of the sample, a 2-tuple.\n   Returns a flattened array of a random patch of a random image.\n        Will be a (size[0]*size[1]) x 1 size array.\n\"\"\"\n", "func_signal": "def random_sample(images, size=(8,8)):\n", "code": "num_images = images.shape[2]\nimage = images[:,:,randrange(num_images)]\n\nx,y = size\nmx, my = tuple(np.array(image.shape) - np.array(size))\nrx, ry = randrange(mx), randrange(my)\npatch = image[rx:rx+x,ry:ry+y]\nreturn patch.flatten()", "path": "deep\\sample_images.py", "repo_name": "jperla/neural", "stars": 98, "license": "None", "language": "python", "size": 2170}
{"docstring": "\"\"\"Accepts columns of data, norm 2-tuple.\n   Zero-centers the patches.\n\n   Squash data to [norm[0], norm[1]] since we use \n       sigmoid as the activation function in the output layer\n\"\"\"\n", "func_signal": "def normalize_data(patches, norm):\n", "code": "assert norm[1] > norm[0]\n# Remove DC (mean of images). \npatches = patches - np.mean(patches)\nassert np.allclose(np.mean(patches), 0)\n\n# Truncate to +/-3 standard deviations and scale to -1 to 1\npstd = 3 * np.std(patches)\npatches = np.maximum(np.minimum(patches, pstd), -pstd) / pstd;\nassert np.all(-1 <= patches) and np.all(patches <= 1)\n\n# Rescale from [-1,1] to [0.1,0.9]\npatches = ((patches + 1) * ((norm[1] - norm[0])/2.0)) + norm[0];\n\nassert np.all(norm[0] <= patches) and np.all(patches <= norm[1])\nreturn patches", "path": "deep\\sample_images.py", "repo_name": "jperla/neural", "stars": 98, "license": "None", "language": "python", "size": 2170}
{"docstring": "\"\"\"Accepts filename string,\n    and numpy array.\n    Saves png image representing array.\n    Returns whether it was saved.\n\"\"\"\n", "func_signal": "def array_to_file(filename, a):\n", "code": "a = normalize_array(a)\ni = Image.fromarray(a.astype('uint8'))\nreturn i.save(filename)", "path": "selftaught\\display_network.py", "repo_name": "jperla/neural", "stars": 98, "license": "None", "language": "python", "size": 2170}
{"docstring": "\"\"\"Accepts a string.\n    String points to file which is a matlab matrix .mat file.\n    Loads the file and extracts the images in the first key that\n        begins with \"IMAGES\".\n\"\"\"\n", "func_signal": "def load_matlab_images(matlab_filename):\n", "code": "d = scipy.io.loadmat(matlab_filename)\nkey = [k for k in d.keys() if k.startswith('IMAGES')][0]\nimages = d[key]\nreturn images", "path": "deep\\sample_images.py", "repo_name": "jperla/neural", "stars": 98, "license": "None", "language": "python", "size": 2170}
{"docstring": "\"\"\"Accepts filename string,\n    2-d numpy array of images,\n    and padding (default 1) number of black pixels between images.\n    Each column of images is a filter. \n\n    This function visualizes filters in matrix images. \n    It will reshape each column into a square image and visualizes\n        on each cell of the visualization panel. \n\n    Returns True on success.\n\n    % TODO: jperla:\n    % All other parameters are optional, usually you do not need to worry\n    % about it.\n    % opt_normalize: whether we need to normalize the filter so that all of\n    % them can have similar contrast. Default value is true.\n    % opt_graycolor: whether we use gray as the heat map. Default is true.\n    % cols: how many columns are there in the display. Default value is the\n    % squareroot of the number of columns in A.\n    % opt_colmajor: you can switch convention to row major for A. In that\n    % case, each row of A is a filter. Default value is false.\n    warning off all\n\n    Also accepts num_samples integer.  Takes first num_samples images\n        if there are too many.\n\"\"\"\n", "func_signal": "def display_network(filename, images, padding=1, num_samples=200):\n", "code": "if images.shape[1] > num_samples:\n    images = images[:,:num_samples]\n\n# first figure out the shape and size of everything\ns, n = images.shape\nd = int(math.sqrt(s))\nassert d * d == s, 'Images must be square'\ncols = int(math.sqrt(n))\nrows = int(n / cols) + (1 if n % cols > 0 else 0)\n\n# black background in output\np = padding\noutput = np.zeros((p + rows * (d + p), p + cols * (d + p)))\noutput += np.min(images.flatten())\n# then fill in the output\nfor i in xrange(n):\n    r,c = int(i / cols), i % cols\n    image = images[:,i]\n    image.shape = (d,d)\n    x,y = (r*(d+p))+p, (c*(d+p))+p\n    output[x:x+d,y:y+d] = image\n\n# and save it \nreturn array_to_file(filename, output)", "path": "selftaught\\display_network.py", "repo_name": "jperla/neural", "stars": 98, "license": "None", "language": "python", "size": 2170}
{"docstring": "\"\"\"Write a string into the output stream.\"\"\"\n", "func_signal": "def write(self, x):\n", "code": "stream = self.stream_stack[-1]\nif self._new_lines:\n    if self.indentation >= 0:\n        if not self._first_write:\n            stream.write('\\n' * self._new_lines)\n        self._first_write = False\n        stream.write(' ' * (self.indentation * self._indentation))\n    self._new_lines = 0\nif isinstance(x, unicode):\n    x = x.encode('utf-8')\nstream.write(x)", "path": "templatetk\\jscompiler.py", "repo_name": "mitsuhiko/templatetk", "stars": 67, "license": "other", "language": "python", "size": 254}
{"docstring": "\"\"\"Compiles an AST node to bytecode\"\"\"\n", "func_signal": "def compile_ast(ast, filename='<string>'):\n", "code": "if isinstance(filename, unicode):\n    filename = filename.encode('utf-8')\n\n# XXX: this is here for debugging purposes during development.\nif os.environ.get('TEMPLATETK_AST_DEBUG'):\n    from astutil.codegen import to_source\n    print >> sys.stderr, '-' * 80\n    ast = to_source(ast)\n    print >> sys.stderr, ast\n    print >> sys.stderr, '-' * 80\n\nreturn compile(ast, filename, 'exec')", "path": "templatetk\\bcinterp.py", "repo_name": "mitsuhiko/templatetk", "stars": 67, "license": "other", "language": "python", "size": 254}
{"docstring": "\"\"\"Return the visitor function for this node or `None` if no visitor\nexists for this node.  In that case the generic visit function is\nused instead.\n\"\"\"\n", "func_signal": "def get_visitor(self, node):\n", "code": "visitor = self._visitor_cache.get(node.__class__)\nif visitor is None:\n    method = 'visit_' + node.__class__.__name__\n    visitor = getattr(self, method, None)\n    self._visitor_cache[node.__class__] = visitor\nreturn visitor", "path": "templatetk\\nodeutils.py", "repo_name": "mitsuhiko/templatetk", "stars": 67, "license": "other", "language": "python", "size": 254}
{"docstring": "\"\"\"Python requires filenames to be strings.\"\"\"\n", "func_signal": "def encode_filename(filename):\n", "code": "if isinstance(filename, unicode):\n    return filename.encode('utf-8')\nreturn filename", "path": "templatetk\\bcinterp.py", "repo_name": "mitsuhiko/templatetk", "stars": 67, "license": "other", "language": "python", "size": 254}
{"docstring": "\"\"\"Set the line numbers of the node and children.\"\"\"\n", "func_signal": "def set_lineno(self, lineno, override=False):\n", "code": "todo = deque([self])\nwhile todo:\n    node = todo.popleft()\n    if 'lineno' in node.attributes:\n        if node.lineno is None or override:\n            node.lineno = lineno\n    todo.extend(node.iter_child_nodes())\nreturn self", "path": "templatetk\\nodes.py", "repo_name": "mitsuhiko/templatetk", "stars": 67, "license": "other", "language": "python", "size": 254}
{"docstring": "\"\"\"Set the config for all nodes.\"\"\"\n", "func_signal": "def set_config(self, config):\n", "code": "todo = deque([self])\nwhile todo:\n    node = todo.popleft()\n    node.config = config\n    todo.extend(node.iter_child_nodes())\nreturn self", "path": "templatetk\\nodes.py", "repo_name": "mitsuhiko/templatetk", "stars": 67, "license": "other", "language": "python", "size": 254}
{"docstring": "# XXX: For intercepting this it would be necessary to extract the\n# rightmost part of the dotted expression in node.node so that the\n# owner can be preserved for JavaScript (this)\n", "func_signal": "def visit_Call(self, node, fstate):\n", "code": "self.visit(node.node, fstate)\nself.writer.write('(')\nfor idx, arg in enumerate(node.args):\n    if idx:\n        self.writer.write(', ')\n    self.visit(arg, fstate)\nself.writer.write(')')\n\nif node.kwargs or node.dyn_args or node.dyn_kwargs:\n    raise NotImplementedError('Dynamic calls or keyword arguments '\n                              'not available with javascript')", "path": "templatetk\\jscompiler.py", "repo_name": "mitsuhiko/templatetk", "stars": 67, "license": "other", "language": "python", "size": 254}
{"docstring": "\"\"\"This has to be called just before doing any modifications on the\nscoped code and after all inner frames were visisted.  This is required\nbecause the injected code will also need the knowledge of the inner\nframes to know which identifiers are required to lookup in the outer\nframe that the outer frame might not need.\n\"\"\"\n", "func_signal": "def inject_scope_code(self, fstate, body):\n", "code": "before = []\n\nfor alias, old_name in fstate.required_aliases.iteritems():\n    before.append(ast.Assign([ast.Name(alias, ast.Store())],\n                              ast.Name(old_name, ast.Load())))\nfor inner_func in fstate.inner_functions:\n    before.extend(inner_func)\n\n# at that point we know about the inner states and can see if any\n# of them need variables we do not have yet assigned and we have to\n# resolve for them.\nfor target, sourcename in fstate.iter_required_lookups():\n    before.append(ast.Assign([ast.Name(target, ast.Store())],\n        self.make_call('rtstate.lookup_var',\n                       [ast.Str(sourcename)])))\n\ndummy_yield = []\nif fstate.buffer is None:\n    dummy_yield.append(ast.If(ast.Num(0),\n        [ast.Expr(ast.Yield(ast.Num(0)))], []))\nbody[:] = before + body + dummy_yield", "path": "templatetk\\asttransform.py", "repo_name": "mitsuhiko/templatetk", "stars": 67, "license": "other", "language": "python", "size": 254}
{"docstring": "\"\"\"Adds missing locations so that the code becomes compilable\nby the Python interpreter.\n\"\"\"\n", "func_signal": "def fix_missing_locations(node):\n", "code": "def _fix(node, lineno, col_offset):\n    if 'lineno' in node._attributes:\n        if getattr(node, 'lineno', None) is None:\n            node.lineno = lineno\n        else:\n            lineno = node.lineno\n    if 'col_offset' in node._attributes:\n        if getattr(node, 'col_offset', None) is None:\n            node.col_offset = col_offset\n        else:\n            col_offset = node.col_offset\n    for child in ast.iter_child_nodes(node):\n        _fix(child, lineno, col_offset)\n_fix(node, 1, 0)\nreturn node", "path": "templatetk\\asttransform.py", "repo_name": "mitsuhiko/templatetk", "stars": 67, "license": "other", "language": "python", "size": 254}
{"docstring": "# XXX: better defaults maybe\n", "func_signal": "def getattr(self, obj, attribute):\n", "code": "try:\n    return getattr(obj, str(attribute))\nexcept (UnicodeError, AttributeError):\n    try:\n        return obj[attribute]\n    except (TypeError, LookupError):\n         Undefined()", "path": "templatetk\\config.py", "repo_name": "mitsuhiko/templatetk", "stars": 67, "license": "other", "language": "python", "size": 254}
{"docstring": "\"\"\"As transformers may return lists in some places this method\ncan be used to enforce a list as return value.\n\"\"\"\n", "func_signal": "def visit_list(self, node, *args, **kwargs):\n", "code": "rv = self.visit(node, *args, **kwargs)\nif not isinstance(rv, list):\n    rv = [rv]\nreturn rv", "path": "templatetk\\nodeutils.py", "repo_name": "mitsuhiko/templatetk", "stars": 67, "license": "other", "language": "python", "size": 254}
{"docstring": "\"\"\"This method iterates over all fields that are defined and yields\n``(key, value)`` tuples.  Per default all fields are returned, but\nit's possible to limit that to some fields by providing the `only`\nparameter or to exclude some using the `exclude` parameter.  Both\nshould be sets or tuples of field names.\n\"\"\"\n", "func_signal": "def iter_fields(self, exclude=None, only=None):\n", "code": "for name in self.fields:\n    if (exclude is only is None) or \\\n       (exclude is not None and name not in exclude) or \\\n       (only is not None and name in only):\n        try:\n            yield name, getattr(self, name)\n        except AttributeError:\n            pass", "path": "templatetk\\nodes.py", "repo_name": "mitsuhiko/templatetk", "stars": 67, "license": "other", "language": "python", "size": 254}
{"docstring": "\"\"\"Called if no explicit visitor function exists for a node.\"\"\"\n", "func_signal": "def generic_visit(self, node, *args, **kwargs):\n", "code": "for node in node.iter_child_nodes():\n    self.visit(node, *args, **kwargs)", "path": "templatetk\\nodeutils.py", "repo_name": "mitsuhiko/templatetk", "stars": 67, "license": "other", "language": "python", "size": 254}
{"docstring": "\"\"\"Gets a template from cache or if it's not there, it will newly\nload it and cache it.\n\"\"\"\n", "func_signal": "def get_template(self, template_name):\n", "code": "template_name = self.config.join_path(self.template_name,\n                                      template_name)\n\nif template_name in self.template_cache:\n    return self.template_cache[template_name]\nrv = self.config.get_template(template_name)\nself.template_cache[template_name] = rv\nreturn rv", "path": "templatetk\\runtime.py", "repo_name": "mitsuhiko/templatetk", "stars": 67, "license": "other", "language": "python", "size": 254}
{"docstring": "\"\"\"Converts a template node to a python AST ready for compilation.\"\"\"\n", "func_signal": "def to_ast(node):\n", "code": "transformer = ASTTransformer(node.config)\nreturn transformer.transform(node)", "path": "templatetk\\asttransform.py", "repo_name": "mitsuhiko/templatetk", "stars": 67, "license": "other", "language": "python", "size": 254}
{"docstring": "\"\"\"Find the first node of a given type.  If no such node exists the\nreturn value is `None`.\n\"\"\"\n", "func_signal": "def find(self, node_type):\n", "code": "for result in self.find_all(node_type):\n    return result", "path": "templatetk\\nodes.py", "repo_name": "mitsuhiko/templatetk", "stars": 67, "license": "other", "language": "python", "size": 254}
{"docstring": "\"\"\"Visit a node.\"\"\"\n", "func_signal": "def visit(self, node, *args, **kwargs):\n", "code": "f = self.get_visitor(node)\nif f is not None:\n    return f(node, *args, **kwargs)\nreturn self.generic_visit(node, *args, **kwargs)", "path": "templatetk\\nodeutils.py", "repo_name": "mitsuhiko/templatetk", "stars": 67, "license": "other", "language": "python", "size": 254}
{"docstring": "\"\"\"Can unpack tuples to target names without raising exceptions.  This\nis used by the compiled as helper function in case the config demands\nthis behavior.\n\"\"\"\n", "func_signal": "def lenient_unpack_helper(config, iterable, targets):\n", "code": "try:\n    values = tuple(iterable)\nexcept TypeError:\n    if not config.allow_noniter_unpacking:\n        raise\n    return recursive_make_undefined(config, targets)\n\nif config.strict_tuple_unpacking:\n    return values\n\nreturn _unpack_tuple_silent(config, values, targets)", "path": "templatetk\\bcinterp.py", "repo_name": "mitsuhiko/templatetk", "stars": 67, "license": "other", "language": "python", "size": 254}
{"docstring": "# TODO: also allow assignments to tuples\n", "func_signal": "def visit_Assign(self, node, fstate):\n", "code": "return self.make_assign(node.target, self.visit(node.node, fstate),\n                        fstate, lineno=node.lineno)", "path": "templatetk\\asttransform.py", "repo_name": "mitsuhiko/templatetk", "stars": 67, "license": "other", "language": "python", "size": 254}
{"docstring": "\"\"\"Make this info and evaluated template generator into a module.\"\"\"\n", "func_signal": "def make_module(self, gen):\n", "code": "body = list(gen)\nreturn self.config.make_module(self.template_name, self.exports,\n                               body)", "path": "templatetk\\runtime.py", "repo_name": "mitsuhiko/templatetk", "stars": 67, "license": "other", "language": "python", "size": 254}
{"docstring": "# tzrange._isdst() was using a date() rather than a datetime().\n# Issue reported by Lennart Regebro.\n", "func_signal": "def testBrokenIsDstHandling(self):\n", "code": "dt = datetime(2007,8,6,4,10, tzinfo=tzutc())\nself.assertEquals(dt.astimezone(tz=gettz(\"GMT+2\")),\n                  datetime(2007,8,6,6,10, tzinfo=tzstr(\"GMT+2\")))", "path": "test.py", "repo_name": "paxan/python-dateutil", "stars": 84, "license": "other", "language": "python", "size": 1173}
{"docstring": "# tzstr(\"GMT+2\") improperly considered daylight saving time.\n# Issue reported by Lennart Regebro.\n", "func_signal": "def testGMTHasNoDaylight(self):\n", "code": "dt = datetime(2007,8,6,4,10)\nself.assertEquals(gettz(\"GMT+2\").dst(dt), timedelta(0))", "path": "test.py", "repo_name": "paxan/python-dateutil", "stars": 84, "license": "other", "language": "python", "size": 1173}
{"docstring": "# Tests a problem reported by Adam Ryan.\n", "func_signal": "def testYearDayBug(self):\n", "code": "self.assertEqual(date(2010, 1, 1)+relativedelta(yearday=15),\n                 date(2010, 1, 15))", "path": "test.py", "repo_name": "paxan/python-dateutil", "stars": 84, "license": "other", "language": "python", "size": 1173}
{"docstring": "# Another nice test. The last days of week number 52/53\n# may be in the next year.\n", "func_signal": "def testWeeklyByWeekNoAndWeekDayLarge(self):\n", "code": "self.assertEqual(list(rrule(WEEKLY,\n                      count=3,\n                      byweekno=52,\n                      byweekday=SU,\n                      dtstart=parse(\"19970902T090000\"))),\n                 [datetime(1997, 12, 28, 9, 0),\n                  datetime(1998, 12, 27, 9, 0),\n                  datetime(2000, 1, 2, 9, 0)])", "path": "test.py", "repo_name": "paxan/python-dateutil", "stars": 84, "license": "other", "language": "python", "size": 1173}
{"docstring": "# This explores a bug found by Mathieu Bridon.\n", "func_signal": "def testSecondlyByHourAndMinuteAndSecondBug(self):\n", "code": "self.assertEqual(list(rrule(SECONDLY,\n                      count=3,\n                      bysecond=(0,),\n                      byminute=(1,),\n                      dtstart=parse(\"20100322120100\"))),\n                 [datetime(2010, 3, 22, 12, 1),\n                  datetime(2010, 3, 22, 13, 1),\n                  datetime(2010, 3, 22, 14, 1)])", "path": "test.py", "repo_name": "paxan/python-dateutil", "stars": 84, "license": "other", "language": "python", "size": 1173}
{"docstring": "# That's a nice one. The first days of week number one\n# may be in the last year.\n", "func_signal": "def testYearlyByWeekNoAndWeekDay(self):\n", "code": "self.assertEqual(list(rrule(YEARLY,\n                      count=3,\n                      byweekno=1,\n                      byweekday=MO,\n                      dtstart=parse(\"19970902T090000\"))),\n                 [datetime(1997, 12, 29, 9, 0),\n                  datetime(1999, 1, 4, 9, 0),\n                  datetime(2000, 1, 3, 9, 0)])", "path": "test.py", "repo_name": "paxan/python-dateutil", "stars": 84, "license": "other", "language": "python", "size": 1173}
{"docstring": "# That's a nice one. The first days of week number one\n# may be in the last year.\n", "func_signal": "def testWeeklyByWeekNoAndWeekDay(self):\n", "code": "self.assertEqual(list(rrule(WEEKLY,\n                      count=3,\n                      byweekno=1,\n                      byweekday=MO,\n                      dtstart=parse(\"19970902T090000\"))),\n                 [datetime(1997, 12, 29, 9, 0),\n                  datetime(1999, 1, 4, 9, 0),\n                  datetime(2000, 1, 3, 9, 0)])", "path": "test.py", "repo_name": "paxan/python-dateutil", "stars": 84, "license": "other", "language": "python", "size": 1173}
{"docstring": "\"\"\"dayofweek == 0 means Sunday, whichweek 5 means last instance\"\"\"\n", "func_signal": "def picknthweekday(year, month, dayofweek, hour, minute, whichweek):\n", "code": "first = datetime.datetime(year, month, 1, hour, minute)\nweekdayone = first.replace(day=((dayofweek-first.isoweekday())%7+1))\nfor n in xrange(whichweek):\n    dt = weekdayone+(whichweek-n)*ONEWEEK\n    if dt.month == month:\n        return dt", "path": "dateutil\\tzwin.py", "repo_name": "paxan/python-dateutil", "stars": 84, "license": "other", "language": "python", "size": 1173}
{"docstring": "# That's a nice one. The first days of week number one\n# may be in the last year.\n", "func_signal": "def testMonthlyByWeekNoAndWeekDay(self):\n", "code": "self.assertEqual(list(rrule(MONTHLY,\n                      count=3,\n                      byweekno=1,\n                      byweekday=MO,\n                      dtstart=parse(\"19970902T090000\"))),\n                 [datetime(1997, 12, 29, 9, 0),\n                  datetime(1999, 1, 4, 9, 0),\n                  datetime(2000, 1, 3, 9, 0)])", "path": "test.py", "repo_name": "paxan/python-dateutil", "stars": 84, "license": "other", "language": "python", "size": 1173}
{"docstring": "# Another nice test. The last days of week number 52/53\n# may be in the next year.\n", "func_signal": "def testDailyByWeekNoAndWeekDayLarge(self):\n", "code": "self.assertEqual(list(rrule(DAILY,\n                      count=3,\n                      byweekno=52,\n                      byweekday=SU,\n                      dtstart=parse(\"19970902T090000\"))),\n                 [datetime(1997, 12, 28, 9, 0),\n                  datetime(1998, 12, 27, 9, 0),\n                  datetime(2000, 1, 2, 9, 0)])", "path": "test.py", "repo_name": "paxan/python-dateutil", "stars": 84, "license": "other", "language": "python", "size": 1173}
{"docstring": "# This is interesting because the TH(-3) ends up before\n# the TU(3).\n", "func_signal": "def testYearlyByMonthAndNWeekDayLarge(self):\n", "code": "self.assertEqual(list(rrule(YEARLY,\n                      count=3,\n                      bymonth=(1,3),\n                      byweekday=(TU(3),TH(-3)),\n                      dtstart=parse(\"19970902T090000\"))),\n                 [datetime(1998, 1, 15, 9, 0),\n                  datetime(1998, 1, 20, 9, 0),\n                  datetime(1998, 3, 12, 9, 0)])", "path": "test.py", "repo_name": "paxan/python-dateutil", "stars": 84, "license": "other", "language": "python", "size": 1173}
{"docstring": "# This timezone has an offset of 5992 seconds in 1900-01-01.\n", "func_signal": "def testRoundNonFullMinutes(self):\n", "code": "tz = tzfile(StringIO(base64.decodestring(self.EUROPE_HELSINKI)))\nself.assertEquals(str(datetime(1900,1,1,0,0, tzinfo=tz)),\n                  \"1900-01-01 00:00:00+01:40\")", "path": "test.py", "repo_name": "paxan/python-dateutil", "stars": 84, "license": "other", "language": "python", "size": 1173}
{"docstring": "# Another nice test. The last days of week number 52/53\n# may be in the next year.\n", "func_signal": "def testMonthlyByWeekNoAndWeekDayLarge(self):\n", "code": "self.assertEqual(list(rrule(MONTHLY,\n                      count=3,\n                      byweekno=52,\n                      byweekday=SU,\n                      dtstart=parse(\"19970902T090000\"))),\n                 [datetime(1997, 12, 28, 9, 0),\n                  datetime(1998, 12, 27, 9, 0),\n                  datetime(2000, 1, 2, 9, 0)])", "path": "test.py", "repo_name": "paxan/python-dateutil", "stars": 84, "license": "other", "language": "python", "size": 1173}
{"docstring": "# Every mask is 7 days longer to handle cross-year weekly periods.\n", "func_signal": "def rebuild(self, year, month):\n", "code": "rr = self.rrule\nif year != self.lastyear:\n    self.yearlen = 365+calendar.isleap(year)\n    self.nextyearlen = 365+calendar.isleap(year+1)\n    firstyday = datetime.date(year, 1, 1)\n    self.yearordinal = firstyday.toordinal()\n    self.yearweekday = firstyday.weekday()\n\n    wday = datetime.date(year, 1, 1).weekday()\n    if self.yearlen == 365:\n        self.mmask = M365MASK\n        self.mdaymask = MDAY365MASK\n        self.nmdaymask = NMDAY365MASK\n        self.wdaymask = WDAYMASK[wday:]\n        self.mrange = M365RANGE\n    else:\n        self.mmask = M366MASK\n        self.mdaymask = MDAY366MASK\n        self.nmdaymask = NMDAY366MASK\n        self.wdaymask = WDAYMASK[wday:]\n        self.mrange = M366RANGE\n\n    if not rr._byweekno:\n        self.wnomask = None\n    else:\n        self.wnomask = [0]*(self.yearlen+7)\n        #no1wkst = firstwkst = self.wdaymask.index(rr._wkst)\n        no1wkst = firstwkst = (7-self.yearweekday+rr._wkst)%7\n        if no1wkst >= 4:\n            no1wkst = 0\n            # Number of days in the year, plus the days we got\n            # from last year.\n            wyearlen = self.yearlen+(self.yearweekday-rr._wkst)%7\n        else:\n            # Number of days in the year, minus the days we\n            # left in last year.\n            wyearlen = self.yearlen-no1wkst\n        div, mod = divmod(wyearlen, 7)\n        numweeks = div+mod//4\n        for n in rr._byweekno:\n            if n < 0:\n                n += numweeks+1\n            if not (0 < n <= numweeks):\n                continue\n            if n > 1:\n                i = no1wkst+(n-1)*7\n                if no1wkst != firstwkst:\n                    i -= 7-firstwkst\n            else:\n                i = no1wkst\n            for j in range(7):\n                self.wnomask[i] = 1\n                i += 1\n                if self.wdaymask[i] == rr._wkst:\n                    break\n        if 1 in rr._byweekno:\n            # Check week number 1 of next year as well\n            # TODO: Check -numweeks for next year.\n            i = no1wkst+numweeks*7\n            if no1wkst != firstwkst:\n                i -= 7-firstwkst\n            if i < self.yearlen:\n                # If week starts in next year, we\n                # don't care about it.\n                for j in range(7):\n                    self.wnomask[i] = 1\n                    i += 1\n                    if self.wdaymask[i] == rr._wkst:\n                        break\n        if no1wkst:\n            # Check last week number of last year as\n            # well. If no1wkst is 0, either the year\n            # started on week start, or week number 1\n            # got days from last year, so there are no\n            # days from last year's last week number in\n            # this year.\n            if -1 not in rr._byweekno:\n                lyearweekday = datetime.date(year-1,1,1).weekday()\n                lno1wkst = (7-lyearweekday+rr._wkst)%7\n                lyearlen = 365+calendar.isleap(year-1)\n                if lno1wkst >= 4:\n                    lno1wkst = 0\n                    lnumweeks = 52+(lyearlen+\n                                   (lyearweekday-rr._wkst)%7)%7//4\n                else:\n                    lnumweeks = 52+(self.yearlen-no1wkst)%7//4\n            else:\n                lnumweeks = -1\n            if lnumweeks in rr._byweekno:\n                for i in range(no1wkst):\n                    self.wnomask[i] = 1\n\nif (rr._bynweekday and\n    (month != self.lastmonth or year != self.lastyear)):\n    ranges = []\n    if rr._freq == YEARLY:\n        if rr._bymonth:\n            for month in rr._bymonth:\n                ranges.append(self.mrange[month-1:month+1])\n        else:\n            ranges = [(0, self.yearlen)]\n    elif rr._freq == MONTHLY:\n        ranges = [self.mrange[month-1:month+1]]\n    if ranges:\n        # Weekly frequency won't get here, so we may not\n        # care about cross-year weekly periods.\n        self.nwdaymask = [0]*self.yearlen\n        for first, last in ranges:\n            last -= 1\n            for wday, n in rr._bynweekday:\n                if n < 0:\n                    i = last+(n+1)*7\n                    i -= (self.wdaymask[i]-wday)%7\n                else:\n                    i = first+(n-1)*7\n                    i += (7-self.wdaymask[i]+wday)%7\n                if first <= i <= last:\n                    self.nwdaymask[i] = 1\n\nif rr._byeaster:\n    self.eastermask = [0]*(self.yearlen+7)\n    eyday = easter.easter(year).toordinal()-self.yearordinal\n    for offset in rr._byeaster:\n        self.eastermask[eyday+offset] = 1\n\nself.lastyear = year\nself.lastmonth = month", "path": "dateutil\\rrule.py", "repo_name": "paxan/python-dateutil", "stars": 84, "license": "other", "language": "python", "size": 1173}
{"docstring": "# Another nice test. The last days of week number 52/53\n# may be in the next year.\n", "func_signal": "def testYearlyByWeekNoAndWeekDayLarge(self):\n", "code": "self.assertEqual(list(rrule(YEARLY,\n                      count=3,\n                      byweekno=52,\n                      byweekday=SU,\n                      dtstart=parse(\"19970902T090000\"))),\n                 [datetime(1997, 12, 28, 9, 0),\n                  datetime(1998, 12, 27, 9, 0),\n                  datetime(2000, 1, 2, 9, 0)])", "path": "test.py", "repo_name": "paxan/python-dateutil", "stars": 84, "license": "other", "language": "python", "size": 1173}
{"docstring": "\"\"\"\nJust making sure we don't barf while unfolding.\n\"\"\"\n", "func_signal": "def testICalUnfolding(self):\n", "code": "for string in (\"ho: hum\\r\\n frotz\", \"ho: hum\\r\\n\\tfrotz\"):\n    tz = tzical(StringIO(string))\n    self.assertEqual(dict(), tz._vtz)", "path": "test.py", "repo_name": "paxan/python-dateutil", "stars": 84, "license": "other", "language": "python", "size": 1173}
{"docstring": "# We need to handle cross-year weeks here.\n", "func_signal": "def wdayset(self, year, month, day):\n", "code": "set = [None]*(self.yearlen+7)\ni = datetime.date(year, month, day).toordinal()-self.yearordinal\nstart = i\nfor j in range(7):\n    set[i] = i\n    i += 1\n    #if (not (0 <= i < self.yearlen) or\n    #    self.wdaymask[i] == self.rrule._wkst):\n    # This will cross the year boundary, if necessary.\n    if self.wdaymask[i] == self.rrule._wkst:\n        break\nreturn set, start, i", "path": "dateutil\\rrule.py", "repo_name": "paxan/python-dateutil", "stars": 84, "license": "other", "language": "python", "size": 1173}
{"docstring": "# That's a nice one. The first days of week number one\n# may be in the last year.\n", "func_signal": "def testDailyByWeekNoAndWeekDay(self):\n", "code": "self.assertEqual(list(rrule(DAILY,\n                      count=3,\n                      byweekno=1,\n                      byweekday=MO,\n                      dtstart=parse(\"19970902T090000\"))),\n                 [datetime(1997, 12, 29, 9, 0),\n                  datetime(1999, 1, 4, 9, 0),\n                  datetime(2000, 1, 3, 9, 0)])", "path": "test.py", "repo_name": "paxan/python-dateutil", "stars": 84, "license": "other", "language": "python", "size": 1173}
{"docstring": "\"\"\"Convert a registry key's values to a dictionary.\"\"\"\n", "func_signal": "def valuestodict(key):\n", "code": "dict = {}\nsize = _winreg.QueryInfoKey(key)[1]\nfor i in range(size):\n    data = _winreg.EnumValue(key, i)\n    dict[data[0]] = data[1]\nreturn dict", "path": "dateutil\\tzwin.py", "repo_name": "paxan/python-dateutil", "stars": 84, "license": "other", "language": "python", "size": 1173}
{"docstring": "\"\"\"Return a list of all time zones known to the system.\"\"\"\n", "func_signal": "def list():\n", "code": "handle = _winreg.ConnectRegistry(None, _winreg.HKEY_LOCAL_MACHINE)\ntzkey = _winreg.OpenKey(handle, TZKEYNAME)\nresult = [_winreg.EnumKey(tzkey, i)\n          for i in range(_winreg.QueryInfoKey(tzkey)[0])]\ntzkey.Close()\nhandle.Close()\nreturn result", "path": "dateutil\\tzwin.py", "repo_name": "paxan/python-dateutil", "stars": 84, "license": "other", "language": "python", "size": 1173}
{"docstring": "\"\"\"\n:type lcClass: LanguageComponent class\n:type label: string\n:type parent: QWidget\n\"\"\"\n", "func_signal": "def __init__(self, lcClass, label, parent):\n", "code": "super(VideoCollectionFunction0Widget, self).__init__(parent)\n\nself.model = lambda: lcClass()\n\nself.setSizePolicy(QSizePolicy.Fixed, QSizePolicy.Fixed)\n\nlayout = QHBoxLayout()\nicon = QLabel(self)\nicon.setPixmap(QPixmap(\"res/video-collection-64-64.png\"))\nlayout.addWidget(icon)\nlayout.addWidget(QLabel(label, self))\n\nself.setLayout(layout)", "path": "app\\ui\\language.py", "repo_name": "CalumJEadie/part-ii-individual-project-dev", "stars": 68, "license": "None", "language": "python", "size": 8285}
{"docstring": "\"\"\"\n:type sceneWidget: QWidget\n\"\"\"\n", "func_signal": "def deleteScene(self, sceneWidget):\n", "code": "sceneIndex = self._layout.indexOf(sceneWidget)\ngapBefore = self._layout.itemAt(sceneIndex-1).widget()\n\nself._layout.removeWidget(gapBefore)\ngapBefore.setParent(None)\n\nself._layout.removeWidget(sceneWidget)\nsceneWidget.setParent(None)\nself._scenes.remove(sceneWidget)\n\nself._postScriptChangeEvent()", "path": "app\\ui\\language.py", "repo_name": "CalumJEadie/part-ii-individual-project-dev", "stars": 68, "license": "None", "language": "python", "size": 8285}
{"docstring": "\"\"\"\n:rtype: models.language.TextExpression\n\"\"\"\n", "func_signal": "def model(self):\n", "code": "if self.isFull():\n    return self._child.model()\nelse:\n    return language.TextGap()", "path": "app\\ui\\language.py", "repo_name": "CalumJEadie/part-ii-individual-project-dev", "stars": 68, "license": "None", "language": "python", "size": 8285}
{"docstring": "\"\"\"\nUse highlight levels so that many mechanisms can alter\nhighlighting at the same time.\n\n0 - no highlight\n>0 - highlight\n\"\"\"\n", "func_signal": "def increaseHighlight(self):\n", "code": "self._highlightLevel += 1\nif self._highlightLevel > 0:\n    self.setStyleSheet(\"background: orange\")", "path": "app\\ui\\language.py", "repo_name": "CalumJEadie/part-ii-individual-project-dev", "stars": 68, "license": "None", "language": "python", "size": 8285}
{"docstring": "\"\"\"\n:type getExpression: language.GetVariableExpression\n\"\"\"\n", "func_signal": "def __init__(self, getExpression, parent):\n", "code": "super(GetWidget, self).__init__(parent)\n\nself.setSizePolicy(QSizePolicy.Fixed, QSizePolicy.Fixed)\n\nself._type = getExpression.type\nself._name = QComboBox(self)\n## Allow user to add and edit names\n#self._name.setEditable(True)\nfor name in TYPE_TO_VARIABLE_NAMES[self._type]:\n    self._name.addItem(name)\nself._name.setCurrentIndex(self._name.findText(getExpression.name))\nself._registerChangeSignal(self._name.currentIndexChanged)\n\nlayout = QHBoxLayout()\nlayout.addWidget(QLabel(\"get\"))\nlayout.addWidget(self._name)\n\nself.setLayout(layout)", "path": "app\\ui\\language.py", "repo_name": "CalumJEadie/part-ii-individual-project-dev", "stars": 68, "license": "None", "language": "python", "size": 8285}
{"docstring": "\"\"\"\n:type youtubeSearch: language.YoutubeSearch\n\"\"\"\n\n", "func_signal": "def __init__(self, youtubeSearch, parent):\n", "code": "super(YoutubeSearchWidget, self).__init__(parent)\n\nself.setSizePolicy(QSizePolicy.Fixed, QSizePolicy.Fixed)\n\nself._query = TextGapWidget(youtubeSearch.query, self)\n\nlayout = QHBoxLayout()\nicon = QLabel(self)\nicon.setPixmap(QPixmap(\"res/video-collection-64-64.png\"))\nlayout.addWidget(icon)\nlayout.addWidget(QLabel(\"search\", self))\nlayout.addWidget(self._query)\n\nself.setLayout(layout)", "path": "app\\ui\\language.py", "repo_name": "CalumJEadie/part-ii-individual-project-dev", "stars": 68, "license": "None", "language": "python", "size": 8285}
{"docstring": "\"\"\"\n:type videoCollectionRandom: language.YoutubeVideoCollectionRandom\n\"\"\"\n\n", "func_signal": "def __init__(self, videoCollectionRandom, parent):\n", "code": "super(YoutubeVideoCollectionRandomWidget, self).__init__(parent)\n\nself.setSizePolicy(QSizePolicy.Fixed, QSizePolicy.Fixed)\n\nself._videoCollection = VideoCollectionGapWidget(videoCollectionRandom.video_collection, self)\n\nlayout = QHBoxLayout()\nicon = QLabel(self)\nicon.setPixmap(QPixmap(\"res/video-64-64.png\"))\nlayout.addWidget(icon)\nlayout.addWidget(QLabel(\"random\\nfrom\", self))\nlayout.addWidget(self._videoCollection)\n\nself.setLayout(layout)", "path": "app\\ui\\language.py", "repo_name": "CalumJEadie/part-ii-individual-project-dev", "stars": 68, "license": "None", "language": "python", "size": 8285}
{"docstring": "\"\"\"\n:rtype: models.language.IfScene\n\"\"\"\n", "func_signal": "def model(self):\n", "code": "return language.IfScene(\n    self.title(),\n    self.comment(),\n    self.question(),\n    self.true_scene_sequence(),\n    self.false_scene_sequence()\n)", "path": "app\\ui\\language.py", "repo_name": "CalumJEadie/part-ii-individual-project-dev", "stars": 68, "license": "None", "language": "python", "size": 8285}
{"docstring": "\"\"\"\n:type videoRandomComment: language.YoutubeVideoRandomComment\n\"\"\"\n\n", "func_signal": "def __init__(self, videoRandomComment, parent):\n", "code": "super(YoutubeVideoRandomCommentWidget, self).__init__(parent)\n\nself.setSizePolicy(QSizePolicy.Fixed, QSizePolicy.Fixed)\n\nself._video = VideoGapWidget(videoRandomComment.video, self)\n\nlayout = QHBoxLayout()\nlayout.addWidget(QLabel(\"random\\ncomment\\nfrom\", self))\nlayout.addWidget(self._video)\n\nself.setLayout(layout)", "path": "app\\ui\\language.py", "repo_name": "CalumJEadie/part-ii-individual-project-dev", "stars": 68, "license": "None", "language": "python", "size": 8285}
{"docstring": "\"\"\"\n:type ro: boolean\n\"\"\"\n", "func_signal": "def setReadOnly(self, ro):\n", "code": "self._operand1.setReadOnly(ro)\nself._operand2.setReadOnly(ro)", "path": "app\\ui\\language.py", "repo_name": "CalumJEadie/part-ii-individual-project-dev", "stars": 68, "license": "None", "language": "python", "size": 8285}
{"docstring": "\"\"\"\n:rtype: models.language.NumberValue\n\"\"\"\n", "func_signal": "def model(self):\n", "code": "operator = self._operator.currentText()\nreturn self.OPERATORS[operator](\n    self._operand1.model(),\n    self._operand2.model()\n)", "path": "app\\ui\\language.py", "repo_name": "CalumJEadie/part-ii-individual-project-dev", "stars": 68, "license": "None", "language": "python", "size": 8285}
{"docstring": "\"\"\"\n:rtype: models.language.VideoCollectionExpression\n\"\"\"\n", "func_signal": "def model(self):\n", "code": "if self.isFull():\n    return self._child.model()\nelse:\n    return language.VideoGap()", "path": "app\\ui\\language.py", "repo_name": "CalumJEadie/part-ii-individual-project-dev", "stars": 68, "license": "None", "language": "python", "size": 8285}
{"docstring": "\"\"\"\n:rtype: models.language.WhileScene\n\"\"\"\n", "func_signal": "def model(self):\n", "code": "return language.WhileScene(\n    \"Example Repeat Scene\",\n    \"\",\n    language.TextGap(),\n    language.SceneSequence([])\n)", "path": "app\\ui\\language.py", "repo_name": "CalumJEadie/part-ii-individual-project-dev", "stars": 68, "license": "None", "language": "python", "size": 8285}
{"docstring": "\"\"\"\n:rtype: models.language.VideoScene\n\"\"\"\n", "func_signal": "def model(self):\n", "code": "return language.VideoScene(\n    \"Example Video Scene\",\n    \"Displays Gangnam Style video for 10 seconds from offset 0 seconds.\",\n    language.NumberValue(10),\n    language.CommandSequence([]),\n    language.CommandSequence([]),\n    language.NumberValue(0),\n    language.VideoValue(\"http://www.youtube.com/watch?v=9bZkp7q19f0\")\n)", "path": "app\\ui\\language.py", "repo_name": "CalumJEadie/part-ii-individual-project-dev", "stars": 68, "license": "None", "language": "python", "size": 8285}
{"docstring": "\"\"\"\n:rtype: models.language.WhileScene\n\"\"\"\n", "func_signal": "def model(self):\n", "code": "return language.WhileScene(\n    self.title(),\n    self.comment(),\n    self.question(),\n    self.scene_sequence()\n)", "path": "app\\ui\\language.py", "repo_name": "CalumJEadie/part-ii-individual-project-dev", "stars": 68, "license": "None", "language": "python", "size": 8285}
{"docstring": "\"\"\"\n:rtype: models.language.GetRandomNumberBetweenInterval\n\"\"\"\n", "func_signal": "def model(self):\n", "code": "return language.GetRandomNumberBetweenInterval(\n    self._operand1.model(),\n    self._operand2.model()\n)", "path": "app\\ui\\language.py", "repo_name": "CalumJEadie/part-ii-individual-project-dev", "stars": 68, "license": "None", "language": "python", "size": 8285}
{"docstring": "\"\"\"\n:type sceneSequence: language.SceneSequence\n\"\"\"\n\n", "func_signal": "def __init__(self, sceneSequence, parent):\n", "code": "super(SceneSequenceWidget, self).__init__(parent)\n\nself.setAcceptDrops(True)\nself._readOnly = False\n\nself._scenes = []\nself._endGap = SceneGapWidget(self)\n\nself._layout = QVBoxLayout()\nself._layout.addSpacing(10)\nself._layout.addWidget(self._endGap, alignment=Qt.AlignHCenter)\nfor scene in sceneSequence.scenes:\n    self.addScene(scene)\nself._layout.addStretch(10)\n\nself.setLayout(self._layout)", "path": "app\\ui\\language.py", "repo_name": "CalumJEadie/part-ii-individual-project-dev", "stars": 68, "license": "None", "language": "python", "size": 8285}
{"docstring": "\"\"\"\n:type text: string\n:param parent: Used to call back for modifying items.\n\"\"\"\n", "func_signal": "def __init__(self, text, parent):\n", "code": "super(ListGapWidget, self).__init__(text, parent)\nself.setAcceptDrops(True)\n# Preferred - The sizeHint() is best, but the widget can be shrunk and still be useful.\nself.setSizePolicy(QSizePolicy.Preferred, QSizePolicy.Fixed)\nself.setWordWrap(True)\nself.setAlignment(Qt.AlignHCenter)\n\nself._readOnly = False", "path": "app\\ui\\language.py", "repo_name": "CalumJEadie/part-ii-individual-project-dev", "stars": 68, "license": "None", "language": "python", "size": 8285}
{"docstring": "\"\"\"\nRemoves language compoment currently in gap.\n\n:raises RuntimeError: If gap is not currently occupied.\n\"\"\"\n", "func_signal": "def emptyGap(self):\n", "code": "if not self.isFull():\n    raise RuntimeError(\"Gap is currently not occupied.\")\n\nself._emptyGap()\n\nself._postScriptChangeEvent()", "path": "app\\ui\\language.py", "repo_name": "CalumJEadie/part-ii-individual-project-dev", "stars": 68, "license": "None", "language": "python", "size": 8285}
{"docstring": "\"\"\"\n:type child: language.VideoExpression\n\"\"\"\n", "func_signal": "def __init__(self, child, parent):\n", "code": "super(VideoGapWidget, self).__init__(child, parent)\nlabel = QLabel(self)\nlabel.setPixmap(QPixmap(\"res/video-64-64.png\"))\nself.addWidget(label)", "path": "app\\ui\\language.py", "repo_name": "CalumJEadie/part-ii-individual-project-dev", "stars": 68, "license": "None", "language": "python", "size": 8285}
{"docstring": "'''Returns a list of existing stores. The returned names can then be\nused to call get_storage().\n'''\n# Filter out any storages used by xbmcswift2 so caller doesn't corrupt\n# them.\n", "func_signal": "def list_storages(self):\n", "code": "return [name for name in os.listdir(self.storage_path)\n        if not name.startswith('.')]", "path": "xbmcswift2\\xbmcmixin.py", "repo_name": "jbeluch/xbmcswift2", "stars": 81, "license": "gpl-3.0", "language": "python", "size": 898}
{"docstring": "'''Adds ListItems to the XBMC interface. Each item in the\nprovided list should either be instances of xbmcswift2.ListItem,\nor regular dictionaries that will be passed to\nxbmcswift2.ListItem.from_dict. Returns the list of ListItems.\n\n:param items: An iterable of items where each item is either a\n              dictionary with keys/values suitable for passing to\n              :meth:`xbmcswift2.ListItem.from_dict` or an instance of\n              :class:`xbmcswift2.ListItem`.\n'''\n", "func_signal": "def add_items(self, items):\n", "code": "_items = [self._listitemify(item) for item in items]\ntuples = [item.as_tuple() for item in _items]\nxbmcplugin.addDirectoryItems(self.handle, tuples, len(tuples))\n\n# We need to keep track internally of added items so we can return them\n# all at the end for testing purposes\nself.added_items.extend(_items)\n\n# Possibly need an if statement if only for debug mode\nreturn _items", "path": "xbmcswift2\\xbmcmixin.py", "repo_name": "jbeluch/xbmcswift2", "stars": 81, "license": "gpl-3.0", "language": "python", "size": 898}
{"docstring": "'''Returns a storage for the given name. The returned storage is a\nfully functioning python dictionary and is designed to be used that\nway. It is usually not necessary for the caller to load or save the\nstorage manually. If the storage does not already exist, it will be\ncreated.\n\n.. seealso:: :class:`xbmcswift2.TimedStorage` for more details.\n\n:param name: The name  of the storage to retrieve.\n:param file_format: Choices are 'pickle', 'csv', and 'json'. Pickle is\n                    recommended as it supports python objects.\n\n                    .. note:: If a storage already exists for the given\n                              name, the file_format parameter is\n                              ignored. The format will be determined by\n                              the existing storage file.\n:param TTL: The time to live for storage items specified in minutes or None\n            for no expiration. Since storage items aren't expired until a\n            storage is loaded form disk, it is possible to call\n            get_storage() with a different TTL than when the storage was\n            created. The currently specified TTL is always honored.\n'''\n\n", "func_signal": "def get_storage(self, name='main', file_format='pickle', TTL=None):\n", "code": "if not hasattr(self, '_unsynced_storages'):\n    self._unsynced_storages = {}\nfilename = os.path.join(self.storage_path, name)\ntry:\n    storage = self._unsynced_storages[filename]\n    log.debug('Loaded storage \"%s\" from memory', name)\nexcept KeyError:\n    if TTL:\n        TTL = timedelta(minutes=TTL)\n\n    try:\n        storage = TimedStorage(filename, file_format, TTL)\n    except ValueError:\n        # Thrown when the storage file is corrupted and can't be read.\n        # Prompt user to delete storage.\n        choices = ['Clear storage', 'Cancel']\n        ret = xbmcgui.Dialog().select('A storage file is corrupted. It'\n                                      ' is recommended to clear it.',\n                                      choices)\n        if ret == 0:\n            os.remove(filename)\n            storage = TimedStorage(filename, file_format, TTL)\n        else:\n            raise Exception('Corrupted storage file at %s' % filename)\n\n    self._unsynced_storages[filename] = storage\n    log.debug('Loaded storage \"%s\" from disk', name)\nreturn storage", "path": "xbmcswift2\\xbmcmixin.py", "repo_name": "jbeluch/xbmcswift2", "stars": 81, "license": "gpl-3.0", "language": "python", "size": 898}
{"docstring": "'''Write the dict to disk'''\n", "func_signal": "def sync(self):\n", "code": "if self.flag == 'r':\n    return\nfilename = self.filename\ntempname = filename + '.tmp'\nfileobj = open(tempname, 'wb' if self.file_format == 'pickle' else 'w')\ntry:\n    self.dump(fileobj)\nexcept Exception:\n    os.remove(tempname)\n    raise\nfinally:\n    fileobj.close()\nshutil.move(tempname, self.filename)    # atomic commit\nif self.mode is not None:\n    os.chmod(self.filename, self.mode)", "path": "xbmcswift2\\storage.py", "repo_name": "jbeluch/xbmcswift2", "stars": 81, "license": "gpl-3.0", "language": "python", "size": 898}
{"docstring": "'''Load the dict from the file object'''\n# try formats from most restrictive to least restrictive\n", "func_signal": "def load(self, fileobj):\n", "code": "for loader in (pickle.load, json.load, csv.reader):\n    fileobj.seek(0)\n    try:\n        return self.initial_update(loader(fileobj))\n    except Exception as e:\n        pass\nraise ValueError('File not in a supported format')", "path": "xbmcswift2\\storage.py", "repo_name": "jbeluch/xbmcswift2", "stars": 81, "license": "gpl-3.0", "language": "python", "size": 898}
{"docstring": "'''Takes a url or a listitem to be played. Used in conjunction with a\nplayable list item with a path that calls back into your addon.\n\n:param item: A playable list item or url. Pass None to alert XBMC of a\n             failure to resolve the item.\n\n             .. warning:: When using set_resolved_url you should ensure\n                          the initial playable item (which calls back\n                          into your addon) doesn't have a trailing\n                          slash in the URL. Otherwise it won't work\n                          reliably with XBMC's PlayMedia().\n:param subtitles: A URL to a remote subtitles file or a local filename\n                  for a subtitles file to be played along with the\n                  item.\n'''\n", "func_signal": "def set_resolved_url(self, item=None, subtitles=None):\n", "code": "if self._end_of_directory:\n    raise Exception('Current XBMC handle has been removed. Either '\n                    'set_resolved_url(), end_of_directory(), or '\n                    'finish() has already been called.')\nself._end_of_directory = True\n\nsucceeded = True\nif item is None:\n    # None item indicates the resolve url failed.\n    item = {}\n    succeeded = False\n\nif isinstance(item, basestring):\n    # caller is passing a url instead of an item dict\n    item = {'path': item}\n\nitem = self._listitemify(item)\nitem.set_played(True)\nxbmcplugin.setResolvedUrl(self.handle, succeeded,\n                          item.as_xbmc_listitem())\n\n# call to _add_subtitles must be after setResolvedUrl\nif subtitles:\n    self._add_subtitles(subtitles)\nreturn [item]", "path": "xbmcswift2\\xbmcmixin.py", "repo_name": "jbeluch/xbmcswift2", "stars": 81, "license": "gpl-3.0", "language": "python", "size": 898}
{"docstring": "'''Creates an xbmcswift2.ListItem if the provided value for item is a\ndict. If item is already a valid xbmcswift2.ListItem, the item is\nreturned unmodified.\n'''\n", "func_signal": "def _listitemify(self, item):\n", "code": "info_type = self.info_type if hasattr(self, 'info_type') else 'video'\n\n# Create ListItems for anything that is not already an instance of\n# ListItem\nif not hasattr(item, 'as_tuple'):\n    if 'info_type' not in item.keys():\n        item['info_type'] = info_type\n    item = xbmcswift2.ListItem.from_dict(**item)\nreturn item", "path": "xbmcswift2\\xbmcmixin.py", "repo_name": "jbeluch/xbmcswift2", "stars": 81, "license": "gpl-3.0", "language": "python", "size": 898}
{"docstring": "'''A wrapper for `xbmcplugin.addSortMethod()\n<http://mirrors.xbmc.org/docs/python-docs/xbmcplugin.html#-addSortMethod>`_.\nYou can use ``dir(xbmcswift2.SortMethod)`` to list all available sort\nmethods.\n\n:param sort_method: A valid sort method. You can provided the constant\n                    from xbmcplugin, an attribute of SortMethod, or a\n                    string name. For instance, the following method\n                    calls are all equivalent:\n\n                    * ``plugin.add_sort_method(xbmcplugin.SORT_METHOD_TITLE)``\n                    * ``plugin.add_sort_metohd(SortMethod.TITLE)``\n                    * ``plugin.add_sort_method('title')``\n:param label2_mask: A mask pattern for label2. See the `XBMC\n                    documentation\n                    <http://mirrors.xbmc.org/docs/python-docs/xbmcplugin.html#-addSortMethod>`_\n                    for more information.\n'''\n", "func_signal": "def add_sort_method(self, sort_method, label2_mask=None):\n", "code": "try:\n    # Assume it's a string and we need to get the actual int value\n    sort_method = SortMethod.from_string(sort_method)\nexcept AttributeError:\n    # sort_method was already an int (or a bad value)\n    pass\n\nif label2_mask:\n    xbmcplugin.addSortMethod(self.handle, sort_method, label2_mask)\nelse:\n    xbmcplugin.addSortMethod(self.handle, sort_method)", "path": "xbmcswift2\\xbmcmixin.py", "repo_name": "jbeluch/xbmcswift2", "stars": 81, "license": "gpl-3.0", "language": "python", "size": 898}
{"docstring": "'''Attempts to return a view_mode_id for a given view_mode\ntaking into account the current skin. If not view_mode_id can\nbe found, None is returned. 'thumbnail' is currently the only\nsuppported view_mode.\n'''\n", "func_signal": "def get_view_mode_id(self, view_mode):\n", "code": "view_mode_ids = VIEW_MODES.get(view_mode.lower())\nif view_mode_ids:\n    return view_mode_ids.get(xbmc.getSkinDir())\nreturn None", "path": "xbmcswift2\\xbmcmixin.py", "repo_name": "jbeluch/xbmcswift2", "stars": 81, "license": "gpl-3.0", "language": "python", "size": 898}
{"docstring": "'''Displays a temporary notification message to the user. If\ntitle is not provided, the plugin name will be used. To have a\nblank title, pass '' for the title argument. The delay argument\nis in milliseconds.\n'''\n", "func_signal": "def notify(self, msg='', title=None, delay=5000, image=''):\n", "code": "if not msg:\n    log.warning('Empty message for notification dialog')\nif title is None:\n    title = self.addon.getAddonInfo('name')\nxbmc.executebuiltin('XBMC.Notification(\"%s\", \"%s\", \"%s\", \"%s\")' %\n                    (msg, title, delay, image))", "path": "xbmcswift2\\xbmcmixin.py", "repo_name": "jbeluch/xbmcswift2", "stars": 81, "license": "gpl-3.0", "language": "python", "size": 898}
{"docstring": "'''TTL if provided should be a datetime.timedelta. Any entries\nolder than the provided TTL will be removed upon load and upon item\naccess.\n'''\n", "func_signal": "def __init__(self, filename, file_format='pickle', TTL=None):\n", "code": "self.TTL = TTL\n_Storage.__init__(self, filename, file_format=file_format)", "path": "xbmcswift2\\storage.py", "repo_name": "jbeluch/xbmcswift2", "stars": 81, "license": "gpl-3.0", "language": "python", "size": 898}
{"docstring": "'''Handles the writing of the dict to the file object'''\n", "func_signal": "def dump(self, fileobj):\n", "code": "if self.file_format == 'csv':\n    csv.writer(fileobj).writerows(self.raw_dict().items())\nelif self.file_format == 'json':\n    json.dump(self.raw_dict(), fileobj, separators=(',', ':'))\nelif self.file_format == 'pickle':\n    pickle.dump(dict(self.raw_dict()), fileobj, 2)\nelse:\n    raise NotImplementedError('Unknown format: ' +\n                              repr(self.file_format))", "path": "xbmcswift2\\storage.py", "repo_name": "jbeluch/xbmcswift2", "stars": 81, "license": "gpl-3.0", "language": "python", "size": 898}
{"docstring": "'''Returns the localized string from strings.xml for the given\nstringid.\n'''\n", "func_signal": "def get_string(self, stringid):\n", "code": "stringid = int(stringid)\nif not hasattr(self, '_strings'):\n    self._strings = {}\nif not stringid in self._strings:\n    self._strings[stringid] = self.addon.getLocalizedString(stringid)\nreturn self._strings[stringid]", "path": "xbmcswift2\\xbmcmixin.py", "repo_name": "jbeluch/xbmcswift2", "stars": 81, "license": "gpl-3.0", "language": "python", "size": 898}
{"docstring": "'''Returns the settings value for the provided key.\nIf converter is str, unicode, bool or int the settings value will be\nreturned converted to the provided type.\nIf choices is an instance of list or tuple its item at position of the\nsettings value be returned.\n.. note:: It is suggested to always use unicode for text-settings\n          because else xbmc returns utf-8 encoded strings.\n\n:param key: The id of the setting defined in settings.xml.\n:param converter: (Optional) Choices are str, unicode, bool and int.\n:param converter: (Optional) Choices are instances of list or tuple.\n\nExamples:\n    * ``plugin.get_setting('per_page', int)``\n    * ``plugin.get_setting('password', unicode)``\n    * ``plugin.get_setting('force_viewmode', bool)``\n    * ``plugin.get_setting('content', choices=('videos', 'movies'))``\n'''\n#TODO: allow pickling of settings items?\n# TODO: STUB THIS OUT ON CLI\n", "func_signal": "def get_setting(self, key, converter=None, choices=None):\n", "code": "value = self.addon.getSetting(id=key)\nif converter is str:\n    return value\nelif converter is unicode:\n    return value.decode('utf-8')\nelif converter is bool:\n    return value == 'true'\nelif converter is int:\n    return int(value)\nelif isinstance(choices, (list, tuple)):\n    return choices[int(value)]\nelif converter is None:\n    log.warning('No converter provided, unicode should be used, '\n                'but returning str value')\n    return value\nelse:\n    raise TypeError('Acceptable converters are str, unicode, bool and '\n                    'int. Acceptable choices are instances of list '\n                    ' or tuple.')", "path": "xbmcswift2\\xbmcmixin.py", "repo_name": "jbeluch/xbmcswift2", "stars": 81, "license": "gpl-3.0", "language": "python", "size": 898}
{"docstring": "'''Adds the provided list of items to the specified playlist.\nAvailable playlists include *video* and *music*.\n'''\n", "func_signal": "def add_to_playlist(self, items, playlist='video'):\n", "code": "playlists = {'music': 0, 'video': 1}\nassert playlist in playlists.keys(), ('Playlist \"%s\" is invalid.' %\n                                      playlist)\nselected_playlist = xbmc.PlayList(playlists[playlist])\n\n_items = []\nfor item in items:\n    if not hasattr(item, 'as_xbmc_listitem'):\n        if 'info_type' in item.keys():\n            log.warning('info_type key has no affect for playlist '\n                        'items as the info_type is inferred from the '\n                        'playlist type.')\n        # info_type has to be same as the playlist type\n        item['info_type'] = playlist\n        item = xbmcswift2.ListItem.from_dict(**item)\n    _items.append(item)\n    selected_playlist.add(item.get_path(), item.as_xbmc_listitem())\nreturn _items", "path": "xbmcswift2\\xbmcmixin.py", "repo_name": "jbeluch/xbmcswift2", "stars": 81, "license": "gpl-3.0", "language": "python", "size": 898}
{"docstring": "'''Acceptable formats are 'csv', 'json' and 'pickle'.'''\n", "func_signal": "def __init__(self, filename, file_format='pickle'):\n", "code": "self._items = {}\n_PersistentDictMixin.__init__(self, filename, file_format=file_format)", "path": "xbmcswift2\\storage.py", "repo_name": "jbeluch/xbmcswift2", "stars": 81, "license": "gpl-3.0", "language": "python", "size": 898}
{"docstring": "'''Displays the keyboard input window to the user. If the user does not\ncancel the modal, the value entered by the user will be returned.\n\n:param default: The placeholder text used to prepopulate the input field.\n:param heading: The heading for the window. Defaults to the current\n                addon's name. If you require a blank heading, pass an\n                empty string.\n:param hidden: Whether or not the input field should be masked with\n               stars, e.g. a password field.\n'''\n", "func_signal": "def keyboard(self, default=None, heading=None, hidden=False):\n", "code": "if heading is None:\n    heading = self.addon.getAddonInfo('name')\nif default is None:\n    default = ''\nkeyboard = xbmc.Keyboard(default, heading, hidden)\nkeyboard.doModal()\nif keyboard.isConfirmed():\n    return keyboard.getText()", "path": "xbmcswift2\\xbmcmixin.py", "repo_name": "jbeluch/xbmcswift2", "stars": 81, "license": "gpl-3.0", "language": "python", "size": 898}
{"docstring": "'''A decorator that will cache the output of the wrapped function. The\nkey used for the cache is the function name as well as the `*args` and\n`**kwargs` passed to the function.\n\n:param TTL: time to live in minutes\n\n.. note:: For route caching, you should use\n          :meth:`xbmcswift2.Plugin.cached_route`.\n'''\n", "func_signal": "def cached(self, TTL=60 * 24):\n", "code": "def decorating_function(function):\n    # TODO test this method\n    storage = self.get_storage(self._function_cache_name, file_format='pickle',\n                               TTL=TTL)\n    kwd_mark = 'f35c2d973e1bbbc61ca60fc6d7ae4eb3'\n\n    @wraps(function)\n    def wrapper(*args, **kwargs):\n        key = (function.__name__, kwd_mark,) + args\n        if kwargs:\n            key += (kwd_mark,) + tuple(sorted(kwargs.items()))\n\n        try:\n            result = storage[key]\n            log.debug('Storage hit for function \"%s\" with args \"%s\" '\n                      'and kwargs \"%s\"', function.__name__, args,\n                      kwargs)\n        except KeyError:\n            log.debug('Storage miss for function \"%s\" with args \"%s\" '\n                      'and kwargs \"%s\"', function.__name__, args,\n                      kwargs)\n            result = function(*args, **kwargs)\n            storage[key] = result\n            storage.sync()\n        return result\n    return wrapper\nreturn decorating_function", "path": "xbmcswift2\\xbmcmixin.py", "repo_name": "jbeluch/xbmcswift2", "stars": 81, "license": "gpl-3.0", "language": "python", "size": 898}
{"docstring": "'''Adds subtitles to playing video.\n\n:param subtitles: A URL to a remote subtitles file or a local filename\n                  for a subtitles file.\n\n.. warning:: You must start playing a video before calling this method\n             or it will loop for an indefinite length.\n'''\n# This method is named with an underscore to suggest that callers pass\n# the subtitles argument to set_resolved_url instead of calling this\n# method directly. This is to ensure a video is played before calling\n# this method.\n", "func_signal": "def _add_subtitles(self, subtitles):\n", "code": "player = xbmc.Player()\nfor _ in xrange(30):\n    if player.isPlaying():\n        break\n    time.sleep(1)\nelse:\n    raise Exception('No video playing. Aborted after 30 seconds.')\n\nplayer.setSubtitles(subtitles)", "path": "xbmcswift2\\xbmcmixin.py", "repo_name": "jbeluch/xbmcswift2", "stars": 81, "license": "gpl-3.0", "language": "python", "size": 898}
{"docstring": "'''Initially fills the underlying dictionary with keys, values and\ntimestamps.\n'''\n", "func_signal": "def initial_update(self, mapping):\n", "code": "for key, val in mapping.items():\n    _, timestamp = val\n    if not self.TTL or (datetime.utcnow() -\n        datetime.utcfromtimestamp(timestamp) < self.TTL):\n        self.__setitem__(key, val, raw=True)", "path": "xbmcswift2\\storage.py", "repo_name": "jbeluch/xbmcswift2", "stars": 81, "license": "gpl-3.0", "language": "python", "size": 898}
{"docstring": "\"\"\"Iterate through the edges in this edgelist.\n\nSample usage:\nfor edge in edgelist: \n    print edge\n\n\"\"\"\n\n", "func_signal": "def __iter__(self):\n", "code": "for src, dests in enumerate(self._outgoing):\n    for dest in dests:\n        yield (src, dest)", "path": "src\\pebl\\network.py", "repo_name": "abhik/pebl", "stars": 102, "license": "other", "language": "python", "size": 2127}
{"docstring": "# score. alter data. score. alter data (back to original). score. \n# 1st and last scores should be same.\n", "func_signal": "def test_alterdata_scoring(self):\n", "code": "self.neteval1.score_network()\nscore1 = self.neteval1._score_network_with_tempdata()\noldval = self.neteval1.data.observations[0][2]\nself.neteval1._alter_data(0, 2, 1)\nself.neteval1._score_network_with_tempdata()\nself.neteval1._alter_data(0, 2, oldval)\nscore2 = self.neteval1._score_network_with_tempdata()\n\nassert score1 == score2, \"Altering and unaltering data leaves score unchanged.\"", "path": "src\\pebl\\test\\test_evaluator.py", "repo_name": "abhik/pebl", "stars": 102, "license": "other", "language": "python", "size": 2127}
{"docstring": "\"\"\"Add multiple edges.\"\"\"\n\n", "func_signal": "def add_many(self, edges):\n", "code": "for src,dest in edges:\n    if dest not in self._outgoing[src]: \n        insort(self._outgoing[src], dest)\n        insort(self._incoming[dest], src)", "path": "src\\pebl\\network.py", "repo_name": "abhik/pebl", "stars": 102, "license": "other", "language": "python", "size": 2127}
{"docstring": "\"\"\"Returns a posterior object for this result.\"\"\"\n", "func_signal": "def posterior(self):\n", "code": "return posterior.from_sorted_scored_networks(\n            self.nodes, \n            list(reversed(self.networks))\n)", "path": "src\\pebl\\result.py", "repo_name": "abhik/pebl", "stars": 102, "license": "other", "language": "python", "size": 2127}
{"docstring": "# alter data. check dirtynodes.\n", "func_signal": "def test_alterdata_dirtynodes(self):\n", "code": "self.neteval1.score_network()   # to initialize datastructures\nself.neteval1._alter_data(0, 2, 1)\nassert set(self.neteval1.data_dirtynodes) == set([2,3,4]), \"Altering data dirties affected nodes.\"", "path": "src\\pebl\\test\\test_evaluator.py", "repo_name": "abhik/pebl", "stars": 102, "license": "other", "language": "python", "size": 2127}
{"docstring": "\"\"\"Add a network and score to the results.\"\"\"\n", "func_signal": "def add_network(self, net, score):\n", "code": "nets = self.networks\nnethashes = self.nethashes\nnethash = hash(net.edges)\n\nif self.size == 0 or len(nets) < self.size:\n    if nethash not in nethashes:\n        snet = _ScoredNetwork(copy(net.edges), score)\n        insort(nets, snet)\n        nethashes[nethash] = 1\nelif score > nets[0].score and nethash not in nethashes:\n    nethashes.pop(hash(nets[0].edges))\n    nets.remove(nets[0])\n\n    snet = _ScoredNetwork(copy(net.edges), score)\n    insort(nets, snet)\n    nethashes[nethash] = 1", "path": "src\\pebl\\result.py", "repo_name": "abhik/pebl", "stars": 102, "license": "other", "language": "python", "size": 2127}
{"docstring": "\"\"\"Set or get edges as two adjacency lists.\n\nProperty returns/accepts two adjacency lists for outgoing and incoming\nedges respectively. Each adjacency list if a list of sets.\n\n\"\"\"\n\n", "func_signal": "def adjacency_lists():\n", "code": "def fget(self):\n    return self._outgoing, self._incoming\n\ndef fset(self, adjlists):\n    if len(adjlists) is not 2:\n        raise Exception(\"Specify both outgoing and incoming lists.\")\n   \n    # adjlists could be any iterable. convert to list of lists\n    _outgoing, _incoming = adjlists\n    self._outgoing = [list(lst) for lst in _outgoing]\n    self._incoming = [list(lst) for lst in _incoming]\n\nreturn locals()", "path": "src\\pebl\\network.py", "repo_name": "abhik/pebl", "stars": 102, "license": "other", "language": "python", "size": 2127}
{"docstring": "\"\"\"Create a html report for the given result.\"\"\"\n\n", "func_signal": "def htmlreport(self, result_, outdir, numnetworks=10):\n", "code": "def jsonize_run(r):\n    return {\n        'start': time.asctime(time.localtime(r.start)),\n        'end': time.asctime(time.localtime(r.end)),\n        'runtime': round((r.end - r.start)/60, 3),\n        'host': r.host\n    }\n\npjoin = os.path.join\n\n# make outdir if it does not exist\nif not os.path.exists(outdir):\n    os.makedirs(outdir)\n\n# copy static files to outdir\nstaticdir = resource_filename('pebl', 'resources/htmlresult')\nshutil.copy2(pjoin(staticdir, 'index.html'), outdir)\nshutil.copytree(pjoin(staticdir, 'lib'), pjoin(outdir, 'lib'))\n       \n# change outdir to outdir/data\noutdir = pjoin(outdir, 'data')\nos.mkdir(outdir)\n\n# get networks and scores\npost = result_.posterior\nnumnetworks = numnetworks if len(post) >= numnetworks else len(post)\ntopscores = post.scores[:numnetworks]\nnorm_topscores = exp(rescale_logvalues(topscores))\n\n# create json-able datastructure\nresultsdata = {\n    'topnets_normscores': [round(s,3) for s in norm_topscores],\n    'topnets_scores': [round(s,3) for s in topscores],\n    'runs': [jsonize_run(r) for r in result_.runs],\n} \n\n# write out results related data (in json format)\nopen(pjoin(outdir, 'result.data.js'), 'w').write(\n    \"resultdata=\" + simplejson.dumps(resultsdata)\n)\n\n# create network images\ntop = post[0]\ntop.layout()\nfor i,net in enumerate(post[:numnetworks]):\n    self.network_image(\n        net, \n        pjoin(outdir, \"%s.png\" % i), \n        pjoin(outdir, \"%s-common.png\" % i), \n        top.node_positions\n    )\n\n# create consensus network images\ncm = post.consensus_matrix\nfor threshold in xrange(10):\n   self.consensus_network_image(\n        post.consensus_network(threshold/10.0),\n        pjoin(outdir, \"consensus.%s.png\" % threshold),\n        cm, top.node_positions\n    )\n        \n# create score plot\nself.plot(post.scores, pjoin(outdir, \"scores.png\"))", "path": "src\\pebl\\result.py", "repo_name": "abhik/pebl", "stars": 102, "license": "other", "language": "python", "size": 2127}
{"docstring": "\"\"\"Check whether an edge exists in the edgelist.\n\nSample usage:\nif (4,5) in edges: \n    print \"edge exists!\"\n\n\"\"\"\n", "func_signal": "def __contains__(self, edge):\n", "code": "src, dest = edge\n\ntry:\n    return dest in self._outgoing[src]\nexcept IndexError:\n    return False", "path": "src\\pebl\\network.py", "repo_name": "abhik/pebl", "stars": 102, "license": "other", "language": "python", "size": 2127}
{"docstring": "\"\"\"Returns a merged result object.\n\nExample::\n\n    merge(result1, result2, result3)\n    results = [result1, result2, result3]\n    merge(results)\n    merge(*results)\n\n\"\"\"\n", "func_signal": "def merge(*args):\n", "code": "results = flatten(args)\nif len(results) is 1:\n    return results[0]\n\n# create new result object\nnewresults = LearnerResult()\nnewresults.data = results[0].data\nnewresults.nodes = results[0].nodes\n\n# merge all networks, remove duplicates, then sort\nallnets = list(set([net for net in flatten(r.networks for r in results)]))\nallnets.sort()\nnewresults.networks = allnets\nnewresults.nethashes = dict([(net, 1) for net in allnets])\n\n# merge run statistics\nif hasattr(results[0], 'runs'):\n    newresults.runs = flatten([r.runs for r in results]) \nelse:\n    newresults.runs = []\n\nreturn newresults", "path": "src\\pebl\\result.py", "repo_name": "abhik/pebl", "stars": 102, "license": "other", "language": "python", "size": 2127}
{"docstring": "\"\"\"Creates a random network with the given set of nodes.\n\nCan specify required_edges and prohibited_edges to control the resulting\nrandom network.  \n\n\"\"\"\n\n", "func_signal": "def random_network(nodes, required_edges=[], prohibited_edges=[]):\n", "code": "def _randomize(net, density=None):\n    n_nodes = len(net.nodes)\n    density = density or 1.0/n_nodes\n    max_attempts = 50\n\n    for attempt in xrange(max_attempts):\n        # create an random adjacency matrix with given density\n        adjmat = N.random.rand(n_nodes, n_nodes)\n        adjmat[adjmat >= (1.0-density)] = 1\n        adjmat[adjmat < 1] = 0\n        \n        # add required edges\n        for src,dest in required_edges:\n            adjmat[src][dest] = 1\n\n        # remove prohibited edges\n        for src,dest in prohibited_edges:\n            adjmat[src][dest] = 0\n\n        # remove self-loop edges (those along the diagonal)\n        adjmat = N.invert(N.identity(n_nodes).astype(bool))*adjmat\n        \n        # set the adjaceny matrix and check for acyclicity\n        net.edges.adjacency_matrix = adjmat.astype(bool)\n\n        if net.is_acyclic():\n            return net\n\n    # got here without finding a single acyclic network.\n    # so try with a less dense network\n    return _randomize(density/2)\n\n# -----------------------\n\nnet = Network(nodes)\n_randomize(net)\nreturn net", "path": "src\\pebl\\network.py", "repo_name": "abhik/pebl", "stars": 102, "license": "other", "language": "python", "size": 2127}
{"docstring": "\"\"\"Returns the log likelihood of the given network.\n\nSimilar to the loglikelihood method of a Conditional Probability\nDistribution.  \n\n\"\"\"\n\n", "func_signal": "def loglikelihood(self, net):\n", "code": "adjmat = net.edges.adjacency_matrix\n\n# if any of the mustexist or mustnotexist constraints are violated,\n# return negative infinity\nif (not (adjmat | self.mustexist).all()) or \\\n   (adjmat & self.mustnotexist).any():\n    return NEGINF\n\n# if any custom constraints are violated, return negative infinity\nif self.constraints and not all(c(adjmat) for c in self.constraints):\n    return NEGINF\n\nloglike = 0.0\nif self.energy_matrix != None:\n    energy = N.sum(adjmat * self.energy_matrix) \n    loglike = -self.weight * energy\n\nreturn loglike", "path": "src\\pebl\\prior.py", "repo_name": "abhik/pebl", "stars": 102, "license": "other", "language": "python", "size": 2127}
{"docstring": "\"\"\"Creates a Network.\n\nnodes is a list of pebl.data.Variable instances.\nedges can be:\n\n    * an EdgeSet instance\n    * a list of edge tuples\n    * an adjacency matrix (as boolean numpy.ndarray instance)\n    * string representation (see Network.as_string() for format)\n\n\"\"\"\n\n", "func_signal": "def __init__(self, nodes, edges=None):\n", "code": "self.nodes = nodes\nself.nodeids = range(len(nodes))\n\n# add edges\nif isinstance(edges, EdgeSet):\n    self.edges = edges\nelif isinstance(edges, N.ndarray):\n    self.edges = EdgeSet(len(edges))\n    self.edges.adjacency_matrix = edges    \nelse:\n    self.edges = EdgeSet(len(self.nodes))\n    if isinstance(edges, list):\n        self.edges.add_many(edges)\n    elif isinstance(edges, str) and edges:\n        edges = edges.split(';')\n        edges = [tuple([int(n) for n in e.split(',')]) for e in edges]\n        self.edges.add_many(edges)", "path": "src\\pebl\\network.py", "repo_name": "abhik/pebl", "stars": 102, "license": "other", "language": "python", "size": 2127}
{"docstring": "\"\"\"Saves network as a dot file.\"\"\"\n\n", "func_signal": "def as_dotfile(self, filename):\n", "code": "f = file(filename, 'w')\nf.write(self.as_dotstring())\nf.close()", "path": "src\\pebl\\network.py", "repo_name": "abhik/pebl", "stars": 102, "license": "other", "language": "python", "size": 2127}
{"docstring": "\"\"\"Set or get edges as an adjacency matrix.\n\nThe adjacency matrix is a boolean numpy.ndarray instance.\n\n\"\"\"\n\n", "func_signal": "def adjacency_matrix():\n", "code": "def fget(self):\n    size = len(self._outgoing)\n    adjmat = N.zeros((size, size), dtype=bool)\n    selfedges = list(self)\n    if selfedges:\n        adjmat[unzip(selfedges)] = True\n    return adjmat\n\ndef fset(self, adjmat):\n    self.clear()\n    for edge in zip(*N.nonzero(adjmat)):\n        self.add(edge)\n\nreturn locals()", "path": "src\\pebl\\network.py", "repo_name": "abhik/pebl", "stars": 102, "license": "other", "language": "python", "size": 2127}
{"docstring": "\"\"\"Uses a depth-first search (dfs) to detect cycles.\"\"\"\n\n", "func_signal": "def is_acyclic_python(self, roots=None):\n", "code": "def _isacyclic(tovisit, visited):\n    if tovisit.intersection(visited):\n        # already visited a node we need to visit. thus, cycle!\n        return False\n\n    for n in tovisit:\n        # check children for cycles\n        if not _isacyclic(set(children(n)), visited.union([n])):\n            return False\n\n    # got here without returning false, so no cycles below rootnodes\n    return True\n\n#---------------\n\nchildren = self.edges.children\nroots = set(roots) if roots else set(range(len(self.nodes)))\nreturn _isacyclic(roots, set())", "path": "src\\pebl\\network.py", "repo_name": "abhik/pebl", "stars": 102, "license": "other", "language": "python", "size": 2127}
{"docstring": "\"\"\"Save the result to a python pickle file.\n\nThe result can be later read using the result.fromfile function.\n\"\"\"\n\n", "func_signal": "def tofile(self, filename=None):\n", "code": "filename = filename or config.get('result.filename')\nwith open(filename, 'w') as fp:\n    cPickle.dump(self, fp)", "path": "src\\pebl\\result.py", "repo_name": "abhik/pebl", "stars": 102, "license": "other", "language": "python", "size": 2127}
{"docstring": "\"\"\"Uses a depth-first search (dfs) to detect cycles.\"\"\"\n\n", "func_signal": "def is_acyclic(self, roots=None):\n", "code": "roots = list(roots) if roots else self.nodeids\nif _network:\n    return _network.is_acyclic(self.edges._outgoing, roots, [])\nelse:\n    return self.is_acyclic_python(roots)", "path": "src\\pebl\\network.py", "repo_name": "abhik/pebl", "stars": 102, "license": "other", "language": "python", "size": 2127}
{"docstring": "\"\"\"Returns network as a dot-formatted string\"\"\"\n\n", "func_signal": "def as_dotstring(self):\n", "code": "def node(n, position):\n    s = \"\\t\\\"%s\\\"\" % n.name\n    if position:\n        x,y = position\n        s += \" [pos=\\\"%d,%d\\\"]\" % (x,y)\n    return s + \";\"\n\n\nnodes = self.nodes\npositions = self.node_positions if hasattr(self, 'node_positions') \\\n                                else [None for n in nodes]\n\nreturn \"\\n\".join(\n    [\"digraph G {\"] + \n    [node(n, pos) for n,pos in zip(nodes, positions)] + \n    [\"\\t\\\"%s\\\" -> \\\"%s\\\";\" % (nodes[src].name, nodes[dest].name) \n        for src,dest in self.edges] +\n    [\"}\"]\n)", "path": "src\\pebl\\network.py", "repo_name": "abhik/pebl", "stars": 102, "license": "other", "language": "python", "size": 2127}
{"docstring": "\"\"\"Performs a maximum-entropy discretization of data in-place.\n\nRequirements for this implementation:\n\n    1. Try to make all bins equal sized (maximize the entropy)\n    2. If datum x==y in the original dataset, then disc(x)==disc(y) \n       For example, all datapoints with value 3.245 discretize to 1\n       even if it violates requirement 1.\n    3. Number of bins reflects only the non-missing data.\n \n Example:\n\n     input:  [3,7,4,4,4,5]\n     output: [0,1,0,0,0,1]\n    \n     Note that all 4s discretize to 0, which makes bin sizes unequal. \n\n Example: \n\n     input:  [1,2,3,4,2,1,2,3,1,x,x,x]\n     output: [0,1,2,2,1,0,1,2,0,0,0,0]\n\n     Note that the missing data ('x') gets put in the bin with 0.0.\n\n\"\"\"\n\n# includevars can be an atom or list\n", "func_signal": "def maximum_entropy_discretize(indata, includevars=None, excludevars=[], numbins=3):\n", "code": "includevars = as_list(includevars) \n   \n# determine the variables to discretize\nincludevars = includevars or range(indata.variables.size)\nincludevars = [v for v in includevars if v not in excludevars]\n   \nfor v in includevars:\n    # \"_nm\" means \"no missing\"\n    vdata = indata.observations[:,v]\n    vmiss = indata.missing[:,v]\n    vdata_nm = vdata[-vmiss]\n    argsorted = vdata_nm.argsort()\n\n    if len(vdata_nm):\n        # Find bin edges (cutpoints) using no-missing \n        binsize = len(vdata_nm)//numbins\n        binedges = [vdata_nm[argsorted[binsize*b - 1]] for b in range(numbins)][1:]\n        # Discretize full data. Missings get added to bin with 0.0.\n        indata.observations[:,v] = N.searchsorted(binedges, vdata)\n\n    oldvar = indata.variables[v]\n    newvar = data.DiscreteVariable(oldvar.name, numbins)\n    newvar.__dict__.update(oldvar.__dict__) # copy any other data attached to variable\n    newvar.arity = numbins\n    indata.variables[v] = newvar\n\n# if discretized all variables, then cast observations to int\nif len(includevars) == indata.variables.size:\n    indata.observations = indata.observations.astype(int)\n\nreturn indata", "path": "src\\pebl\\discretizer.py", "repo_name": "abhik/pebl", "stars": 102, "license": "other", "language": "python", "size": 2127}
{"docstring": "# start off in a known, mildly interesting state\n", "func_signal": "def test_update_tags_with_none(self):\n", "code": "Tag.objects.update_tags(self.dead_parrot, 'foo bar baz')\ntags = Tag.objects.get_for_object(self.dead_parrot)\nself.assertEquals(len(tags), 3)\nself.failUnless(get_tag('bar') in tags)\nself.failUnless(get_tag('baz') in tags)\nself.failUnless(get_tag('foo') in tags)\n\nTag.objects.update_tags(self.dead_parrot, None)\ntags = Tag.objects.get_for_object(self.dead_parrot)\nself.assertEquals(len(tags), 0)", "path": "tagging\\tests\\tests.py", "repo_name": "brosner/django-tagging", "stars": 115, "license": "other", "language": "python", "size": 306}
{"docstring": "\"\"\"\nUpdate tags associated with an object.\n\"\"\"\n", "func_signal": "def update_tags(self, obj, tag_names):\n", "code": "ctype = ContentType.objects.get_for_model(obj)\ncurrent_tags = list(self.filter(items__content_type__pk=ctype.pk,\n                                items__object_id=obj.pk))\nupdated_tag_names = parse_tag_input(tag_names)\nif settings.FORCE_LOWERCASE_TAGS:\n    updated_tag_names = [t.lower() for t in updated_tag_names]\n\n# Remove tags which no longer apply\ntags_for_removal = [tag for tag in current_tags \\\n                    if tag.name not in updated_tag_names]\nif len(tags_for_removal):\n    TaggedItem._default_manager.filter(content_type__pk=ctype.pk,\n                                       object_id=obj.pk,\n                                       tag__in=tags_for_removal).delete()\n# Add new tags\ncurrent_tag_names = [tag.name for tag in current_tags]\nfor tag_name in updated_tag_names:\n    if tag_name not in current_tag_names:\n        tag, created = self.get_or_create(name=tag_name)\n        TaggedItem._default_manager.create(tag=tag, object=obj)", "path": "tagging\\models.py", "repo_name": "brosner/django-tagging", "stars": 115, "license": "other", "language": "python", "size": 306}
{"docstring": "\"\"\"\nObtain a list of tags associated with instances of a model\ncontained in the given queryset.\n\nIf ``counts`` is True, a ``count`` attribute will be added to\neach tag, indicating how many times it has been used against\nthe Model class in question.\n\nIf ``min_count`` is given, only tags which have a ``count``\ngreater than or equal to ``min_count`` will be returned.\nPassing a value for ``min_count`` implies ``counts=True``.\n\"\"\"\n\n", "func_signal": "def usage_for_queryset(self, queryset, counts=False, min_count=None):\n", "code": "if getattr(queryset.query, 'get_compiler', None):\n    # Django 1.2+\n    compiler = queryset.query.get_compiler(using='default')\n    extra_joins = ' '.join(compiler.get_from_clause()[0][1:])\n    where, params = queryset.query.where.as_sql(\n        compiler.quote_name_unless_alias, compiler.connection\n    )\nelse:\n    # Django pre-1.2\n    extra_joins = ' '.join(queryset.query.get_from_clause()[0][1:])\n    where, params = queryset.query.where.as_sql()\n\nif where:\n    extra_criteria = 'AND %s' % where\nelse:\n    extra_criteria = ''\nreturn self._get_usage(queryset.model, counts, min_count, extra_joins, extra_criteria, params)", "path": "tagging\\models.py", "repo_name": "brosner/django-tagging", "stars": 115, "license": "other", "language": "python", "size": 306}
{"docstring": "# Once again, with feeling (strings)\n", "func_signal": "def test_related_for_model_with_tag_strings_as_input(self):\n", "code": "related_tags = Tag.objects.related_for_model('bar', Parrot, counts=True)\nrelevant_attribute_list = [(tag.name, tag.count) for tag in related_tags]\nself.assertEquals(len(relevant_attribute_list), 3)\nself.failUnless((u'baz', 1) in relevant_attribute_list)\nself.failUnless((u'foo', 1) in relevant_attribute_list)\nself.failUnless((u'ter', 2) in relevant_attribute_list)\n\nrelated_tags = Tag.objects.related_for_model('bar', Parrot, min_count=2)\nrelevant_attribute_list = [(tag.name, tag.count) for tag in related_tags]\nself.assertEquals(len(relevant_attribute_list), 1)\nself.failUnless((u'ter', 2) in relevant_attribute_list)\n\nrelated_tags = Tag.objects.related_for_model('bar', Parrot, counts=False)\nrelevant_attribute_list = [tag.name for tag in related_tags]\nself.assertEquals(len(relevant_attribute_list), 3)\nself.failUnless(u'baz' in relevant_attribute_list)\nself.failUnless(u'foo' in relevant_attribute_list)\nself.failUnless(u'ter' in relevant_attribute_list)\n\nrelated_tags = Tag.objects.related_for_model(['bar', 'ter'], Parrot, counts=True)\nrelevant_attribute_list = [(tag.name, tag.count) for tag in related_tags]\nself.assertEquals(len(relevant_attribute_list), 1)\nself.failUnless((u'baz', 1) in relevant_attribute_list)\n\nrelated_tags = Tag.objects.related_for_model(['bar', 'ter', 'baz'], Parrot, counts=True)\nrelevant_attribute_list = [(tag.name, tag.count) for tag in related_tags]\nself.assertEquals(len(relevant_attribute_list), 0)", "path": "tagging\\tests\\tests.py", "repo_name": "brosner/django-tagging", "stars": 115, "license": "other", "language": "python", "size": 306}
{"docstring": "\"\"\" Test with double-quoted multiple words.\n    A completed quote will trigger this.  Unclosed quotes are ignored. \"\"\"\n    \n", "func_signal": "def test_with_double_quoted_multiple_words(self):\n", "code": "self.assertEquals(parse_tag_input('\"one'), [u'one'])\nself.assertEquals(parse_tag_input('\"one two'), [u'one', u'two'])\nself.assertEquals(parse_tag_input('\"one two three'), [u'one', u'three', u'two'])\nself.assertEquals(parse_tag_input('\"one two\"'), [u'one two'])\nself.assertEquals(parse_tag_input('a-one \"a-two and a-three\"'),\n    [u'a-one', u'a-two and a-three'])", "path": "tagging\\tests\\tests.py", "repo_name": "brosner/django-tagging", "stars": 115, "license": "other", "language": "python", "size": 306}
{"docstring": "\"\"\"\nCreate a ``QuerySet`` containing instances of the specified\nmodel associated with *all* of the given list of tags.\n\"\"\"\n", "func_signal": "def get_intersection_by_model(self, queryset_or_model, tags):\n", "code": "tags = get_tag_list(tags)\ntag_count = len(tags)\nqueryset, model = get_queryset_and_model(queryset_or_model)\n\nif not tag_count:\n    return model._default_manager.none()\n\nmodel_table = qn(model._meta.db_table)\n# This query selects the ids of all objects which have all the\n# given tags.\nquery = \"\"\"\nSELECT %(model_pk)s\nFROM %(model)s, %(tagged_item)s\nWHERE %(tagged_item)s.content_type_id = %(content_type_id)s\n  AND %(tagged_item)s.tag_id IN (%(tag_id_placeholders)s)\n  AND %(model_pk)s = %(tagged_item)s.object_id\nGROUP BY %(model_pk)s\nHAVING COUNT(%(model_pk)s) = %(tag_count)s\"\"\" % {\n    'model_pk': '%s.%s' % (model_table, qn(model._meta.pk.column)),\n    'model': model_table,\n    'tagged_item': qn(self.model._meta.db_table),\n    'content_type_id': ContentType.objects.get_for_model(model).pk,\n    'tag_id_placeholders': ','.join(['%s'] * tag_count),\n    'tag_count': tag_count,\n}\n\ncursor = connection.cursor()\ncursor.execute(query, [tag.pk for tag in tags])\nobject_ids = [row[0] for row in cursor.fetchall()]\nif len(object_ids) > 0:\n    return queryset.filter(pk__in=object_ids)\nelse:\n    return model._default_manager.none()", "path": "tagging\\models.py", "repo_name": "brosner/django-tagging", "stars": 115, "license": "other", "language": "python", "size": 306}
{"docstring": "\"\"\"\nPerform the custom SQL query for ``usage_for_model`` and\n``usage_for_queryset``.\n\"\"\"\n", "func_signal": "def _get_usage(self, model, counts=False, min_count=None, extra_joins=None, extra_criteria=None, params=None):\n", "code": "if min_count is not None: counts = True\n\nmodel_table = qn(model._meta.db_table)\nmodel_pk = '%s.%s' % (model_table, qn(model._meta.pk.column))\nquery = \"\"\"\nSELECT DISTINCT %(tag)s.id, %(tag)s.name%(count_sql)s\nFROM\n    %(tag)s\n    INNER JOIN %(tagged_item)s\n        ON %(tag)s.id = %(tagged_item)s.tag_id\n    INNER JOIN %(model)s\n        ON %(tagged_item)s.object_id = %(model_pk)s\n    %%s\nWHERE %(tagged_item)s.content_type_id = %(content_type_id)s\n    %%s\nGROUP BY %(tag)s.id, %(tag)s.name\n%%s\nORDER BY %(tag)s.name ASC\"\"\" % {\n    'tag': qn(self.model._meta.db_table),\n    'count_sql': counts and (', COUNT(%s)' % model_pk) or '',\n    'tagged_item': qn(TaggedItem._meta.db_table),\n    'model': model_table,\n    'model_pk': model_pk,\n    'content_type_id': ContentType.objects.get_for_model(model).pk,\n}\n\nmin_count_sql = ''\nif min_count is not None:\n    min_count_sql = 'HAVING COUNT(%s) >= %%s' % model_pk\n    params.append(min_count)\n\ncursor = connection.cursor()\ncursor.execute(query % (extra_joins, extra_criteria, min_count_sql), params)\ntags = []\nfor row in cursor.fetchall():\n    t = self.model(*row[:2])\n    if counts:\n        t.count = row[2]\n    tags.append(t)\nreturn tags", "path": "tagging\\models.py", "repo_name": "brosner/django-tagging", "stars": 115, "license": "other", "language": "python", "size": 306}
{"docstring": "# start off in a known, mildly interesting state\n", "func_signal": "def test_add_tag(self):\n", "code": "Tag.objects.update_tags(self.dead_parrot, 'foo bar baz')\ntags = Tag.objects.get_for_object(self.dead_parrot)\nself.assertEquals(len(tags), 3)\nself.failUnless(get_tag('bar') in tags)\nself.failUnless(get_tag('baz') in tags)\nself.failUnless(get_tag('foo') in tags)\n\n# try to add a tag that already exists\nTag.objects.add_tag(self.dead_parrot, 'foo')\ntags = Tag.objects.get_for_object(self.dead_parrot)\nself.assertEquals(len(tags), 3)\nself.failUnless(get_tag('bar') in tags)\nself.failUnless(get_tag('baz') in tags)\nself.failUnless(get_tag('foo') in tags)\n\n# now add a tag that doesn't already exist\nTag.objects.add_tag(self.dead_parrot, 'zip')\ntags = Tag.objects.get_for_object(self.dead_parrot)\nself.assertEquals(len(tags), 4)\nself.failUnless(get_tag('zip') in tags)\nself.failUnless(get_tag('bar') in tags)\nself.failUnless(get_tag('baz') in tags)\nself.failUnless(get_tag('foo') in tags)", "path": "tagging\\tests\\tests.py", "repo_name": "brosner/django-tagging", "stars": 115, "license": "other", "language": "python", "size": 306}
{"docstring": "\"\"\" Test forcing tags to lowercase. \"\"\"\n\n", "func_signal": "def test_force_lowercase_tags(self):\n", "code": "settings.FORCE_LOWERCASE_TAGS = True\n\nTag.objects.update_tags(self.dead_parrot, 'foO bAr Ter')\ntags = Tag.objects.get_for_object(self.dead_parrot)\nself.assertEquals(len(tags), 3)\nfoo_tag = get_tag('foo')\nbar_tag = get_tag('bar')\nter_tag = get_tag('ter')\nself.failUnless(foo_tag in tags)\nself.failUnless(bar_tag in tags)\nself.failUnless(ter_tag in tags)\n\nTag.objects.update_tags(self.dead_parrot, 'foO bAr baZ')\ntags = Tag.objects.get_for_object(self.dead_parrot)\nbaz_tag = get_tag('baz')\nself.assertEquals(len(tags), 3)\nself.failUnless(bar_tag in tags)\nself.failUnless(baz_tag in tags)\nself.failUnless(foo_tag in tags)\n\nTag.objects.add_tag(self.dead_parrot, 'FOO')\ntags = Tag.objects.get_for_object(self.dead_parrot)\nself.assertEquals(len(tags), 3)\nself.failUnless(bar_tag in tags)\nself.failUnless(baz_tag in tags)\nself.failUnless(foo_tag in tags)\n\nTag.objects.add_tag(self.dead_parrot, 'Zip')\ntags = Tag.objects.get_for_object(self.dead_parrot)\nself.assertEquals(len(tags), 4)\nzip_tag = get_tag('zip')\nself.failUnless(bar_tag in tags)\nself.failUnless(baz_tag in tags)\nself.failUnless(foo_tag in tags)\nself.failUnless(zip_tag in tags)\n\nf1 = FormTest.objects.create()\nf1.tags = u'TEST5'\nf1.save()\ntags = Tag.objects.get_for_object(f1)\ntest5_tag = get_tag('test5')\nself.assertEquals(len(tags), 1)\nself.failUnless(test5_tag in tags)\nself.assertEquals(f1.tags, u'test5')", "path": "tagging\\tests\\tests.py", "repo_name": "brosner/django-tagging", "stars": 115, "license": "other", "language": "python", "size": 306}
{"docstring": "# Issue 50 - Get by non-existent tag\n", "func_signal": "def test_get_by_nonexistent_tag(self):\n", "code": "parrots = TaggedItem.objects.get_by_model(Parrot, 'argatrons')\nself.assertEquals(len(parrots), 0)", "path": "tagging\\tests\\tests.py", "repo_name": "brosner/django-tagging", "stars": 115, "license": "other", "language": "python", "size": 306}
{"docstring": "# Ensure that automatically created forms use TagField\n", "func_signal": "def test_tag_field_in_modelform(self):\n", "code": "class TestForm(forms.ModelForm):\n    class Meta:\n        model = FormTest\n        \nform = TestForm()\nself.assertEquals(form.fields['tags'].__class__.__name__, 'TagField')", "path": "tagging\\tests\\tests.py", "repo_name": "brosner/django-tagging", "stars": 115, "license": "other", "language": "python", "size": 306}
{"docstring": "\"\"\" Double quotes can contain commas \"\"\"\n", "func_signal": "def test_tags_with_double_quotes_can_contain_commas(self):\n", "code": "self.assertEquals(parse_tag_input('a-one \"a-two, and a-three\"'),\n    [u'a-one', u'a-two, and a-three'])\nself.assertEquals(parse_tag_input('\"two\", one, one, two, \"one\"'),\n    [u'one', u'two'])", "path": "tagging\\tests\\tests.py", "repo_name": "brosner/django-tagging", "stars": 115, "license": "other", "language": "python", "size": 306}
{"docstring": "\"\"\" Test with comma-delimited multiple words.\n    An unquoted comma in the input will trigger this. \"\"\"\n    \n", "func_signal": "def test_with_comma_delimited_multiple_words(self):\n", "code": "self.assertEquals(parse_tag_input(',one'), [u'one'])\nself.assertEquals(parse_tag_input(',one two'), [u'one two'])\nself.assertEquals(parse_tag_input(',one two three'), [u'one two three'])\nself.assertEquals(parse_tag_input('a-one, a-two and a-three'),\n    [u'a-one', u'a-two and a-three'])", "path": "tagging\\tests\\tests.py", "repo_name": "brosner/django-tagging", "stars": 115, "license": "other", "language": "python", "size": 306}
{"docstring": "\"\"\"\nObtain a list of tags related to a given list of tags - that\nis, other tags used by items which have all the given tags.\n\nIf ``counts`` is True, a ``count`` attribute will be added to\neach tag, indicating the number of items which have it in\naddition to the given list of tags.\n\nIf ``min_count`` is given, only tags which have a ``count``\ngreater than or equal to ``min_count`` will be returned.\nPassing a value for ``min_count`` implies ``counts=True``.\n\"\"\"\n", "func_signal": "def related_for_model(self, tags, model, counts=False, min_count=None):\n", "code": "if min_count is not None: counts = True\ntags = get_tag_list(tags)\ntag_count = len(tags)\ntagged_item_table = qn(TaggedItem._meta.db_table)\nquery = \"\"\"\nSELECT %(tag)s.id, %(tag)s.name%(count_sql)s\nFROM %(tagged_item)s INNER JOIN %(tag)s ON %(tagged_item)s.tag_id = %(tag)s.id\nWHERE %(tagged_item)s.content_type_id = %(content_type_id)s\n  AND %(tagged_item)s.object_id IN\n  (\n      SELECT %(tagged_item)s.object_id\n      FROM %(tagged_item)s, %(tag)s\n      WHERE %(tagged_item)s.content_type_id = %(content_type_id)s\n        AND %(tag)s.id = %(tagged_item)s.tag_id\n        AND %(tag)s.id IN (%(tag_id_placeholders)s)\n      GROUP BY %(tagged_item)s.object_id\n      HAVING COUNT(%(tagged_item)s.object_id) = %(tag_count)s\n  )\n  AND %(tag)s.id NOT IN (%(tag_id_placeholders)s)\nGROUP BY %(tag)s.id, %(tag)s.name\n%(min_count_sql)s\nORDER BY %(tag)s.name ASC\"\"\" % {\n    'tag': qn(self.model._meta.db_table),\n    'count_sql': counts and ', COUNT(%s.object_id)' % tagged_item_table or '',\n    'tagged_item': tagged_item_table,\n    'content_type_id': ContentType.objects.get_for_model(model).pk,\n    'tag_id_placeholders': ','.join(['%s'] * tag_count),\n    'tag_count': tag_count,\n    'min_count_sql': min_count is not None and ('HAVING COUNT(%s.object_id) >= %%s' % tagged_item_table) or '',\n}\n\nparams = [tag.pk for tag in tags] * 2\nif min_count is not None:\n    params.append(min_count)\n\ncursor = connection.cursor()\ncursor.execute(query, params)\nrelated = []\nfor row in cursor.fetchall():\n    tag = self.model(*row[:2])\n    if counts is True:\n        tag.count = row[2]\n    related.append(tag)\nreturn related", "path": "tagging\\models.py", "repo_name": "brosner/django-tagging", "stars": 115, "license": "other", "language": "python", "size": 306}
{"docstring": "\"\"\"\nAssociates the given object with a tag.\n\"\"\"\n", "func_signal": "def add_tag(self, obj, tag_name):\n", "code": "tag_names = parse_tag_input(tag_name)\nif not len(tag_names):\n    raise AttributeError(_('No tags were given: \"%s\".') % tag_name)\nif len(tag_names) > 1:\n    raise AttributeError(_('Multiple tags were given: \"%s\".') % tag_name)\ntag_name = tag_names[0]\nif settings.FORCE_LOWERCASE_TAGS:\n    tag_name = tag_name.lower()\ntag, created = self.get_or_create(name=tag_name)\nctype = ContentType.objects.get_for_model(obj)\nTaggedItem._default_manager.get_or_create(\n    tag=tag, content_type=ctype, object_id=obj.pk)", "path": "tagging\\models.py", "repo_name": "brosner/django-tagging", "stars": 115, "license": "other", "language": "python", "size": 306}
{"docstring": "\"\"\" Test with simple space-delimited tags. \"\"\"\n\n", "func_signal": "def test_with_simple_space_delimited_tags(self):\n", "code": "self.assertEquals(parse_tag_input('one'), [u'one'])\nself.assertEquals(parse_tag_input('one two'), [u'one', u'two'])\nself.assertEquals(parse_tag_input('one two three'), [u'one', u'three', u'two'])\nself.assertEquals(parse_tag_input('one one two two'), [u'one', u'two'])", "path": "tagging\\tests\\tests.py", "repo_name": "brosner/django-tagging", "stars": 115, "license": "other", "language": "python", "size": 306}
{"docstring": "\"\"\" Test with naughty input. \"\"\"\n\n# Bad users! Naughty users!\n", "func_signal": "def test_with_naughty_input(self):\n", "code": "self.assertEquals(parse_tag_input(None), [])\nself.assertEquals(parse_tag_input(''), [])\nself.assertEquals(parse_tag_input('\"'), [])\nself.assertEquals(parse_tag_input('\"\"'), [])\nself.assertEquals(parse_tag_input('\"' * 7), [])\nself.assertEquals(parse_tag_input(',,,,,,'), [])\nself.assertEquals(parse_tag_input('\",\",\",\",\",\",\",\"'), [u','])\nself.assertEquals(parse_tag_input('a-one \"a-two\" and \"a-three'),\n    [u'a-one', u'a-three', u'a-two', u'and'])", "path": "tagging\\tests\\tests.py", "repo_name": "brosner/django-tagging", "stars": 115, "license": "other", "language": "python", "size": 306}
{"docstring": "\"\"\"\nRetrieves ``ContentType`` and content objects for the given list of\n``TaggedItems``, grouping the retrieval of content objects by model\ntype to reduce the number of queries executed.\n\nThis results in ``number_of_content_types + 1`` queries rather than\nthe ``number_of_tagged_items * 2`` queries you'd get by iterating\nover the list and accessing each item's ``object`` attribute.\n\nA ``select_related_for`` argument can be used to specify a list of\nof model names (corresponding to the ``model`` field of a\n``ContentType``) for which ``select_related`` should be used when\nretrieving model instances.\n\"\"\"\n", "func_signal": "def fetch_content_objects(tagged_items, select_related_for=None):\n", "code": "if select_related_for is None: select_related_for = []\n\n# Group content object pks by their content type pks\nobjects = {}\nfor item in tagged_items:\n    objects.setdefault(item.content_type_id, []).append(item.object_id)\n\n# Retrieve content types and content objects in bulk\ncontent_types = ContentType._default_manager.in_bulk(objects.keys())\nfor content_type_pk, object_pks in objects.iteritems():\n    model = content_types[content_type_pk].model_class()\n    if content_types[content_type_pk].model in select_related_for:\n        objects[content_type_pk] = model._default_manager.select_related().in_bulk(object_pks)\n    else:\n        objects[content_type_pk] = model._default_manager.in_bulk(object_pks)\n\n# Set content types and content objects in the appropriate cache\n# attributes, so accessing the 'content_type' and 'object'\n# attributes on each tagged item won't result in further database\n# hits.\nfor item in tagged_items:\n    item._object_cache = objects[item.content_type_id][item.object_id]\n    item._content_type_cache = content_types[item.content_type_id]", "path": "tagging\\generic.py", "repo_name": "brosner/django-tagging", "stars": 115, "license": "other", "language": "python", "size": 306}
{"docstring": "# This fails on Oracle because it has no support for a 'LIMIT' clause.\n# See http://asktom.oracle.com/pls/asktom/f?p=100:11:0::::P11_QUESTION_ID:127412348064\n\n# ask for no more than 1 result\n", "func_signal": "def test_get_related_objects_of_same_model_limited_number_of_results(self):\n", "code": "related_objects = TaggedItem.objects.get_related(self.l1, Link, num=1)\nself.assertEquals(len(related_objects), 1)\nself.failUnless(self.l2 in related_objects)", "path": "tagging\\tests\\tests.py", "repo_name": "brosner/django-tagging", "stars": 115, "license": "other", "language": "python", "size": 306}
{"docstring": "\"\"\"\nObtain a list of tags associated with instances of the given\nModel class.\n\nIf ``counts`` is True, a ``count`` attribute will be added to\neach tag, indicating how many times it has been used against\nthe Model class in question.\n\nIf ``min_count`` is given, only tags which have a ``count``\ngreater than or equal to ``min_count`` will be returned.\nPassing a value for ``min_count`` implies ``counts=True``.\n\nTo limit the tags (and counts, if specified) returned to those\nused by a subset of the Model's instances, pass a dictionary\nof field lookups to be applied to the given Model as the\n``filters`` argument.\n\"\"\"\n", "func_signal": "def usage_for_model(self, model, counts=False, min_count=None, filters=None):\n", "code": "if filters is None: filters = {}\n\nqueryset = model._default_manager.filter()\nfor f in filters.items():\n    queryset.query.add_filter(f)\nusage = self.usage_for_queryset(queryset, counts, min_count)\n\nreturn usage", "path": "tagging\\models.py", "repo_name": "brosner/django-tagging", "stars": 115, "license": "other", "language": "python", "size": 306}
{"docstring": "'''\ninverse transform onset coefs bacl to onset patterns\n'''\n\n", "func_signal": "def _op_inv_transform(self,coefs):\n", "code": "D1 = self.en_num_bands \nD2 = self.num_log_bands\n\nif coefs.ndim == 2:\n    N = coefs.shape[0]\n    recon = (np.dot(coefs, self.op_basis.T) + self.op_mean).reshape((N,D1,D2))\nelif coefs.ndim == 1:\n    recon = (np.dot(coefs, self.op_basis.T) + self.op_mean).reshape((D1,D2))\n\nreturn recon", "path": "examples\\music_recommendation\\msdtools.py", "repo_name": "egonina/pycasp", "stars": 65, "license": "None", "language": "python", "size": 11952}
{"docstring": "'''\nCompute log-spaced triangular filter matrix\ninput:\nbands=(lofreq,highfreq,numbands)\n'''\n\n", "func_signal": "def _filter_weights_interp(self,sample_rate,fft_length,bands=(1./1.5,13.3,25)):\n", "code": "fs = sample_rate\nnfft = fft_length\nlofreq = bands[0] \nhifreq = bands[1]\nnbands = bands[2]\n\n\nfft_freqs = np.linspace(0,fs/2.,nfft/2+1)\nfft_freqs[0] = np.finfo('float32').tiny\n\nlog_freqs = np.log(fft_freqs)\nlog_centers = np.linspace(np.log(lofreq),np.log(hifreq),nbands)\nlog_bandwidth = log_centers[1] - log_centers[0]\n# gaussian decays to 0.5 after half bandwidth\nsigma2 = log_bandwidth**2 / np.log(16)\n\ncenter = np.exp(log_centers)\n\nweights = np.zeros((nbands,nfft/2+1))\n\n# construct triangular-shaped filters\nfor i in range(nbands):\n    weights[i] = np.exp(-(log_freqs-log_centers[i])**2/sigma2)\n    weights[i][weights[i] < 0.1*weights[i].max()] = 0\n\nweights /= fft_length\n\nreturn weights, center", "path": "examples\\music_recommendation\\msdtools.py", "repo_name": "egonina/pycasp", "stars": 65, "license": "None", "language": "python", "size": 11952}
{"docstring": "'''\ncompute p-norm params (mean and std)\nusing distance stats between ref and others\n\ninput:\nref - reference vector\nothers - test comparison vectors\n\noutput:\nmu - mean distance\nstd - std of distance\n'''\n", "func_signal": "def p_norm_params_single(ref, others):\n", "code": "da = distanceArray(ref, others)\n\nmu = np.mean(da)\nsigma = np.std(da)\n\nreturn mu, sigma", "path": "examples\\music_recommendation\\msdtools.py", "repo_name": "egonina/pycasp", "stars": 65, "license": "None", "language": "python", "size": 11952}
{"docstring": "'''\ncompute a p-normed distance array between ref and others\n\ninput:\nref - target feature vector\nothers - [D x N] matrix of D-length features vectors\nref_mean - p-norm mean for ref\nothers_mean - array of p-norm means for others\nref_std - p-norm std for ref\nothers_std - array of p-norm std's for others\n\noutput:\npdist - p-normed distance array between ref and others\n'''\n\n", "func_signal": "def p_norm_distance_single(ref, others, ref_mean, others_mean, ref_std, others_std):\n", "code": "da = np.array(distanceArray(ref, others), dtype=np.float32)\n\npdist = 0.5 * ( (da - ref_mean)*(1./ref_std) + (da - others_mean)/others_std )\n\nreturn pdist", "path": "examples\\music_recommendation\\msdtools.py", "repo_name": "egonina/pycasp", "stars": 65, "license": "None", "language": "python", "size": 11952}
{"docstring": "'''\ncompute inverse 2d type II DCT along axis=1,2\n\ninput:\nnorm - {'ortho',None}\n'''\n\n\n", "func_signal": "def _idct2d(self,coeffs,norm='ortho'):\n", "code": "if coeffs.ndim == 3:\n    y_idct = fftpack.idct(coeffs,axis=2,norm=norm)\n    x_idct = fftpack.idct(y_idct,axis=1,norm=norm)\nelif coeffs.ndim == 2:\n    y_idct = fftpack.idct(coeffs,axis=1,norm=norm)\n    x_idct = fftpack.idct(y_idct,axis=0,norm=norm)\n\n\nreturn x_idct", "path": "examples\\music_recommendation\\msdtools.py", "repo_name": "egonina/pycasp", "stars": 65, "license": "None", "language": "python", "size": 11952}
{"docstring": "''' output a range of frames w.r.t. the absolute frame number\n(not including end frame)'''\n", "func_signal": "def getFrameRange(self,start,end):\n", "code": "lowLimit = self.count - self.numFrames - 1\nhighLimit = self.count\nif (start < lowLimit) or (start > highLimit) or (end < lowLimit) or (end > highLimit):\n    pdb.set_trace()\n    return None\nif start >= end:\n    return None\n\n# find contiguous region within buffer\noffset = self.count - self.curr\nstart -= offset\nend -= offset\nwhile start < 0:\n    start += self.numFrames\n    end += self.numFrames\n\nreturn self.data[start:end]", "path": "examples\\music_recommendation\\msdtools.py", "repo_name": "egonina/pycasp", "stars": 65, "license": "None", "language": "python", "size": 11952}
{"docstring": "'''\ncompute onset patterns from echo nest timbre features\n'''\n#print 'computing onset patterns with chunk size %u' % chunk_size\n#t0 = time.time()\n", "func_signal": "def _compute_op(self,timbre_features,timbre_time,song_duration):\n", "code": "spectrogram = self._en_inverse_pca(timbre_features,timbre_time,song_duration)\n#t1 = time.time()\nonset_spectrogram = self._unsharp_mask(spectrogram)\n#t2 = time.time()\nperiodicities = self._compute_periodicities(onset_spectrogram)\n#t3 = time.time()\n\n\n#total_time = t3 - t0\n#print '\\ntotal time %g sec' % total_time\n#t_pca = t1 - t0\n#print 'inverse pca %g sec' % t_pca\n#t_unsharp = t2 - t1\n#print 'unsharp mask %g sec' % t_unsharp\n#t_periods = t3 - t2\n#print 'periodicities %g sec' % t_periods\n\nreturn periodicities", "path": "examples\\music_recommendation\\msdtools.py", "repo_name": "egonina/pycasp", "stars": 65, "license": "None", "language": "python", "size": 11952}
{"docstring": "'''\ngenerate a parameters file for rhythm_features() function\n'''\n\n# setup rhythm parameters from file loaded from disk\n#frame_width = rhythm_params['frame_width']\n#frame_rate = 1./frame_width\n#window_width_frames = rhythm_params['window_width_frames']\n#fftLength = rhythm_params['fftLength']\n#max_lag_frames = rhythm_params['max_lag_frames']\n#hops_per_window = rhythm_params['hops_per_window']\n#window_hop_frames = window_width_frames/hops_per_window\n#log_weights = rhythm_params['log_weights']\n#timbre_cov_inv = rhythm_params['timbre_cov_inv']\n#pca_matrix = rhythm_params['pca_matrix']\n\n", "func_signal": "def rhythm_features_gen_params():\n", "code": "frame_width = 1024/44100. #23ms\n\nwindow_width = 3 #sec\npadded_window_width = 6 #sec\nhops_per_window = 4 \nmax_lag = 3 #sec\n\nwindow_width_frames = hops_per_window * int(np.round(window_width / frame_width / hops_per_window))\nwindow_hop_frames = window_width_frames/hops_per_window\nmax_lag_frames = int(np.round(max_lag / frame_width))\nfftLength = nextPow2(int(np.round(max(padded_window_width / frame_width, max_lag_frames))))\n\nmahalanobis = True\n\nmin_bpm = 20\nmax_bpm = 800\nnum_bands = 25\nweights, centers = filterWeights(1./frame_width,max_lag_frames,fftLength,(min_bpm,max_bpm,num_bands))\n\n# load matrix from disk\nfp = open('timbre_stats.pkl','rb')\ntimbre_stats = pkl.load(fp)\nfp.close()\nfp = open('onset_spectrum_stats.pkl','rb')\nonset_spectrum_stats = pkl.load(fp)\nfp.close()\n\nrhythm_params = {'frame_width':frame_width, 'window_width_frames':window_width_frames, \n        'fftLength':fftLength,\n        'max_lag_frames':max_lag_frames, 'hops_per_window':hops_per_window, \n        'log_weights':{'weights':weights,'centers':centers},\n        'timbre_stats':timbre_stats,\n        'onset_spectrum_stats':onset_spectrum_stats,\n        }\n\nname = raw_input('filename to save rhythm params [rhythm_params.pkl]:')\nif name is '':\n    name = 'rhythm_params.pkl'\n    \nfp = open(name,'wb')\npkl.dump(rhythm_params,fp,protocol=-1)\nfp.close()\n\n        \nreturn rhythm_params", "path": "examples\\music_recommendation\\msdtools.py", "repo_name": "egonina/pycasp", "stars": 65, "license": "None", "language": "python", "size": 11952}
{"docstring": "'''\ncomputes a squared-euclidean distance matrix between each pair of feature vectors\n\ninput:\nfeatures - [D x N] matrix containing D-dimensional feature column vectors\noutput:\ndmatrix - [N x N] distance matrix (zeroes on diagonal)\n'''\n\n", "func_signal": "def distanceMatrix(features):\n", "code": "N = features.shape[1]\nx = features\nenergies = np.tile(np.sum(x * x,axis=0),[N,1])\ndots = np.dot(x.T,x)\n\ndmatrix = energies + energies.T - 2*dots\ndmatrix[dmatrix < 0] = 0\ndmatrix = np.sqrt(dmatrix)\n\nreturn dmatrix", "path": "examples\\music_recommendation\\msdtools.py", "repo_name": "egonina/pycasp", "stars": 65, "license": "None", "language": "python", "size": 11952}
{"docstring": "# === TRAIN SVM ===\n", "func_signal": "def train_SVM(svm_train_feats, svm_labels):\n", "code": "svm = SVM()\nsvm.train(svm_train_feats, svm_labels, \"linear\") \nreturn svm", "path": "examples\\speaker_id\\speaker_id.py", "repo_name": "egonina/pycasp", "stars": 65, "license": "None", "language": "python", "size": 11952}
{"docstring": "'''\ntransform onset patterns to onset coefs using PCA\n'''\n", "func_signal": "def _op_transform(self,periodicity):\n", "code": "D = self.en_num_bands * self.num_log_bands\nif periodicity.ndim == 3:\n    N = periodicity.shape[0]\n    coefs = np.dot(periodicity.reshape((N,D)) - self.op_mean, self.op_basis)\nelif periodicity.ndim == 2:\n    coefs = np.dot(periodicity.reshape(D) - self.op_mean, self.op_basis) \n\nreturn coefs", "path": "examples\\music_recommendation\\msdtools.py", "repo_name": "egonina/pycasp", "stars": 65, "license": "None", "language": "python", "size": 11952}
{"docstring": "'''\nsave params to file\ninput:\nparams_file - file name to save to\n'''\n\n", "func_signal": "def save_params(self,params_file):\n", "code": "params = {}\n\nparams['en_pca_mean'] = self.en_pca_mean\nparams['en_inv_pca'] = self.en_inv_pca\nparams['en_frame_time'] = self.en_frame_time\nparams['en_num_bands'] = self.en_num_bands\nparams['en_attack_frames'] = self.en_attack_frames\nparams['en_decay_frames'] = self.en_decay_frames\n\nparams['onset_filter'] = self.onset_filter\n\nparams['num_log_bands'] = self.num_log_bands\nparams['periodicity_centers'] = self.periodicity_centers\nparams['block_size'] = self.block_size\nparams['block_padded_size'] = self.block_padded_size\nparams['block_hop_size'] = self.block_hop_size\nparams['block_window'] = self.block_window\n\nparams['log_weights'] = self.log_weights\nparams['periodicity_normalization'] = self.periodicity_normalization\n\nparams['op_mean'] = self.op_mean\nparams['op_corr'] = self.op_corr\n#params['coef_energy'] = self.coef_energy\nparams['total_blocks'] = self.total_blocks\n#params['dct_coef_inds'] = self.dct_coef_inds\nparams['op_basis'] = self.op_basis\n\n\nparams['chunk_size'] = self.chunk_size\n\nfp = open(params_file,'wb')\npkl.dump(params,fp,protocol=-1)\nfp.close()", "path": "examples\\music_recommendation\\msdtools.py", "repo_name": "egonina/pycasp", "stars": 65, "license": "None", "language": "python", "size": 11952}
{"docstring": "# TODO: figure out a less hacky way to do this... :(\n# stack the svm training features\n", "func_signal": "def concat_svm_features(speaker1_svm_feats, speaker2_svm_feats):\n", "code": "svm_train_feats = np.hstack((speaker1_svm_feats[0].reshape(1, M*D),\\\n                             speaker1_svm_feats[1].reshape(1, M*D),\\\n                             speaker1_svm_feats[2].reshape(1, M*D),\\\n                             speaker1_svm_feats[3].reshape(1, M*D),\\\n                             speaker1_svm_feats[4].reshape(1, M*D),\\\n                             speaker2_svm_feats[0].reshape(1, M*D),\\\n                             speaker2_svm_feats[1].reshape(1, M*D),\\\n                             speaker2_svm_feats[2].reshape(1, M*D),\\\n                             speaker2_svm_feats[3].reshape(1, M*D),\\\n                             speaker2_svm_feats[4].reshape(1, M*D))).reshape(10, M*D, order='F')\nreturn svm_train_feats", "path": "examples\\speaker_id\\speaker_id.py", "repo_name": "egonina/pycasp", "stars": 65, "license": "None", "language": "python", "size": 11952}
{"docstring": "'''\ncompute p-norm params (means and std's)\nusing distance stats between each pair of feature vectors\n\ninput:\nfeatures - [D x N] matrix of D-dim feature vectors\n\noutput:\nmu - mean distances\nstd - std of distances\n'''\n\n", "func_signal": "def p_norm_params(features):\n", "code": "da = distanceMatrix(features)\n\nmu = np.mean(da,axis=1)\nsigma = np.std(da,axis=1)\n\nreturn mu, sigma", "path": "examples\\music_recommendation\\msdtools.py", "repo_name": "egonina/pycasp", "stars": 65, "license": "None", "language": "python", "size": 11952}
{"docstring": "\"\"\"\nCompute non-uniform triangular filter matrix\ninput:\nbands=(lowbpm,highbpm,numbands)\n\noutput:\nweights - matrix [bands[2],fftLength]\n\"\"\"\n\n", "func_signal": "def filterWeightsTriangular(sampleRate,fftLength=512,bands=(20,800,25)):\n", "code": "fs = sampleRate\nnfft = fftLength\nlofreq = bands[0] \nhifreq = bands[1]\nnbands = bands[2]\n\n\nfft_freqs = np.linspace(0,60*fs/2.,nfft/2+1) #in bpm\nf_first = max(np.nonzero(fft_freqs >= lofreq)[0][0] - 1, 0)\nf_last = min(np.nonzero(fft_freqs <= hifreq)[0][-1] + 1, fft_freqs.size-1)\nf_scaled = np.log(fft_freqs[f_first:f_last+1])\nfidx = np.zeros(nbands+2,dtype='int') #band locations in fft_freqs\nbandedges = np.linspace(f_scaled[0],f_scaled[-1],nbands+2) #band edges in f_scaled\n\n# find boundaries of filters\n# bandwidth of filters must be increasing\nbandwidth = 1\nfidx[0] = f_first\nfor i in range(1,fidx.size):\n    fidx[i] = np.argmin(np.abs(f_scaled - bandedges[i])) + f_first\n    if i > 0:\n        d = fidx[i]-fidx[i-1]\n        if d < bandwidth:\n            fidx[i] = min(fidx[i-1] + bandwidth, fft_freqs.size-1)\n        else:\n            bandwidth = d\n\nfreqs = fft_freqs[fidx]\n#print 'band edges'\n#print fidx\n#print freqs\nlower = freqs[0:-2]\ncenter = freqs[1:-1]\nupper = freqs[2:]\n\n#pdb.set_trace()\n\n\n\n\nnbands = np.sum(upper <= np.max(fft_freqs))\nweights = np.zeros((nbands,nfft/2+1),dtype='float32')\n\n\nfor i in range(nbands):\n    weights[i] = \\\n            ((fft_freqs > lower[i]) & (fft_freqs <= center[i])) * \\\n            (fft_freqs-lower[i])/(center[i]-lower[i]) + \\\n            ((fft_freqs > center[i]) & (fft_freqs <= upper[i])) * \\\n            (upper[i]-fft_freqs)/(upper[i]-center[i])\n\n# normalize weight by bandwidth\nrow_scale = np.tile(1./np.sum(weights,axis=1),[weights.shape[1],1]).T\nweights *= row_scale\n\n\nreturn weights, center", "path": "examples\\music_recommendation\\msdtools.py", "repo_name": "egonina/pycasp", "stars": 65, "license": "None", "language": "python", "size": 11952}
{"docstring": "'''get a frame with respect to last (most recent) frame\n(0 is most recent frame)'''\n", "func_signal": "def get(self,ind):\n", "code": "if -ind >= self.numFrames:\n    raise IndexError\nind = self.curr - 1 + ind + self.numFrames\nreturn self.data[ind]", "path": "examples\\music_recommendation\\msdtools.py", "repo_name": "egonina/pycasp", "stars": 65, "license": "None", "language": "python", "size": 11952}
{"docstring": "\"\"\"\nCompute non-uniform triangular filter matrix\ninput:\nbands=(lowbpm,highbpm,numbands)\n\noutput:\nweights - matrix [bands[2],fftLength]\n\"\"\"\n\n", "func_signal": "def filterWeights(sampleRate,sigLength,fftLength,bands=(20,800,25),shape='gaussian'):\n", "code": "fs = sampleRate\nnfft = fftLength\nlofreq = bands[0] \nhifreq = bands[1]\nnbands = bands[2]\n\n\nfft_freqs = np.linspace(0,60*fs/2.,nfft/2+1).astype('float32') #in bpm\nf_first = max(np.nonzero(fft_freqs >= lofreq)[0][0] - 1, 0)\nf_last = min(np.nonzero(fft_freqs <= hifreq)[0][-1] + 1, fft_freqs.size-1)\nf_scaled = np.log(fft_freqs[f_first:f_last+1])\nbandedges = np.linspace(f_scaled[0],f_scaled[-1],nbands+2).astype('float32') #band edges in f_scaled\nlog_bandwidth = bandedges[2] - bandedges[0]\n\n\nweights = np.zeros((nbands,nfft/2+1),dtype='float32')\n\n\nif shape is 'cosine':\n    # raised cosine window\n    for i in range(nbands):\n        mask = np.logical_and(f_scaled >= bandedges[i], f_scaled <= bandedges[i+2])\n        weights[i][mask] =  0.5 * (1 + np.cos(2*np.pi*(f_scaled[mask] - bandedges[i+1])/log_bandwidth))\nelif shape is 'gaussian':\n    # std equal to 1/4 log_bandwidth\n    sigma2 = (log_bandwidth**2)/16\n    for i in range(nbands):\n        weights[i][f_first:f_last+1] =   np.exp(-(f_scaled - bandedges[i+1])**2/(2*sigma2))\n\n#normalize weights by bandwidth\nrow_scale = np.tile(1./np.sqrt(np.sum(weights*weights,axis=1)),[weights.shape[1],1]).T\nweights *= row_scale\n\ncenters = np.exp(bandedges[1:-1])\n\nreturn weights, centers", "path": "examples\\music_recommendation\\msdtools.py", "repo_name": "egonina/pycasp", "stars": 65, "license": "None", "language": "python", "size": 11952}
{"docstring": "'''\ncompute a p-normed distance array between pairs of features\n\ninput:\nfeatures - [D x N] matrix of D-dim feature vectors\nmu - array of p-norm means\nsigma - array of p-norm std's\n\noutput:\npdist - p-normed distance matrix between ref and others\n'''\n\n", "func_signal": "def p_norm_distance(features, mu, sigma):\n", "code": "da = distanceMatrix(features)\nda -= mu\nda *= (1./sigma)\n\npdist = 0.5 * (da + da.T)\n\nreturn pdist", "path": "examples\\music_recommendation\\msdtools.py", "repo_name": "egonina/pycasp", "stars": 65, "license": "None", "language": "python", "size": 11952}
{"docstring": "'''\ninvert Echo Nest PCA timbre transformation\n'''\n", "func_signal": "def _en_inverse_pca(self,timbre_features,timbre_time,song_duration,fast_interp=None):\n", "code": "timbre_frames = np.round(timbre_time / self.en_frame_time).astype('uint32')\nduration_frames = int(np.ceil(song_duration / self.en_frame_time))\n\nspectral_segments = (np.dot(self.en_inv_pca,timbre_features[1:]) + timbre_features[0]).T + self.en_pca_mean\nspectral_segments = spectral_segments.reshape((-1,self.en_num_bands))\n\nspectrogram = np.zeros((duration_frames,self.en_num_bands),dtype='float32')\nfor i in xrange(len(timbre_frames)):\n    attack_start = timbre_frames[i] # inclusive\n    attack_end = attack_start + self.en_attack_frames # exclusive\n    decay_start = attack_end # inclusive\n\n    if i+1 < len(timbre_frames):\n        decay_end = timbre_frames[i+1] # exclusive\n    else:\n        decay_end = duration_frames\n\n    segment_start = i * (self.en_attack_frames + self.en_decay_frames)\n    spectrogram[attack_start:attack_end] = spectral_segments[segment_start:segment_start+self.en_attack_frames]\n\n    # linearly interpolate decay to fill remainder of segment time\n    segment_mid = segment_start + self.en_attack_frames\n    decay = spectral_segments[segment_mid:segment_mid+self.en_decay_frames]\n    decay = np.vstack((decay,2*decay[-1]-decay[-2])) # extrapolate linearly by one frame\n    num_new_decay_frames = decay_end - decay_start\n    if fast_interp == 'linear':\n        slope = ((decay[-1] - decay[0])/(num_new_decay_frames+1)).reshape((1,-1))\n        initial = decay[0].reshape((1,-1))\n        new_decay_ind = np.arange(num_new_decay_frames+1).reshape((-1,1))\n        resampled_decay = (slope * new_decay_ind) + initial\n\n    #elif fast_interp == 'exponential':\n    #    initial = decay[0].reshape((1,-1))\n    #    final = decay[-1].reshape((1,-1))\n    #    tau = (num_new_decay_frames+1)/(np.log(initial)-np.log(final)).reshape((1,-1))\n    #    inv_tau  = 1./tau\n    #    new_decay_ind = np.arange(num_new_decay_frames+1).reshape((-1,1))\n    #    resampled_decay = initial * np.exp(-new_decay_ind * inv_tau)\n\n    elif fast_interp is None:\n        #decay_ind = np.arange(0,self.en_decay_frames+1).astype('float32')\n        decay_ind = np.linspace(0,self.en_decay_frames,self.en_decay_frames+1)\n        new_decay_ind = np.linspace(0,self.en_decay_frames,num_new_decay_frames+1).astype('float32')\n        resampled_decay = interp1d(decay_ind,decay,axis=0,kind='linear')(new_decay_ind)\n\n\n\n    spectrogram[decay_start:decay_end] = resampled_decay[:-1]\n\nreturn spectrogram", "path": "examples\\music_recommendation\\msdtools.py", "repo_name": "egonina/pycasp", "stars": 65, "license": "None", "language": "python", "size": 11952}
{"docstring": "'''\ncompute 2d type II DCT along axis=1,2\n\ndct is separable so we compute it using two 1-d dct's\n\ninput:\nnorm - {'ortho',None}\n'''\n\n", "func_signal": "def _dct2d(self,periodicities,norm='ortho'):\n", "code": "if periodicities.ndim == 3:\n    x_dct = fftpack.dct(periodicities,axis=1,norm=norm)\n    y_dct = fftpack.dct(x_dct,axis=2,norm=norm)\nelif periodicities.ndim == 2:\n    x_dct = fftpack.dct(periodicities,axis=0,norm=norm)\n    y_dct = fftpack.dct(x_dct,axis=1,norm=norm)\n\n\nreturn y_dct", "path": "examples\\music_recommendation\\msdtools.py", "repo_name": "egonina/pycasp", "stars": 65, "license": "None", "language": "python", "size": 11952}
{"docstring": "'''\nReturn a list of available services (keystone services-list)\n\nCLI Example:\n\n.. code-block:: bash\n\n    salt '*' keystone.service_list\n'''\n", "func_signal": "def service_list(profile=None, **connection_args):\n", "code": "kstone = auth(profile, **connection_args)\nret = {}\nfor service in kstone.services.list():\n    ret[service.name] = {'id': service.id,\n                         'name': service.name,\n                         'description': service.description,\n                         'type': service.type}\nreturn ret", "path": "file_root\\_modules\\keystone.py", "repo_name": "CSSCorp/openstack-automation", "stars": 74, "license": "gpl-2.0", "language": "python", "size": 763}
{"docstring": "'''\nDelete a tenant (keystone tenant-delete)\n\nCLI Examples:\n\n.. code-block:: bash\n\n    salt '*' keystone.tenant_delete c965f79c4f864eaaa9c3b41904e67082\n    salt '*' keystone.tenant_delete tenant_id=c965f79c4f864eaaa9c3b41904e67082\n    salt '*' keystone.tenant_delete name=demo\n'''\n", "func_signal": "def tenant_delete(tenant_id=None, name=None, profile=None, **connection_args):\n", "code": "kstone = auth(profile, **connection_args)\nif name:\n    for tenant in kstone.tenants.list():\n        if tenant.name == name:\n            tenant_id = tenant.id\n            break\nif not tenant_id:\n    return {'Error': 'Unable to resolve tenant id'}\nkstone.tenants.delete(tenant_id)\nret = 'Tenant ID {0} deleted'.format(tenant_id)\nif name:\n\n    ret += ' ({0})'.format(name)\nreturn ret", "path": "file_root\\_modules\\keystone.py", "repo_name": "CSSCorp/openstack-automation", "stars": 74, "license": "gpl-2.0", "language": "python", "size": 763}
{"docstring": "'''\nDelete endpoints of an Openstack service\n\nCLI Examples:\n\n.. code-block:: bash\n\n    salt '*' keystone.endpoint_delete nova\n'''\n", "func_signal": "def endpoint_delete(service, profile=None, **connection_args):\n", "code": "kstone = auth(profile, **connection_args)\nendpoint = endpoint_get(service, profile, **connection_args)\nif not endpoint or 'Error' in endpoint:\n    return {'Error': 'Could not find any endpoints for the service'}\nkstone.endpoints.delete(endpoint['id'])\nendpoint = endpoint_get(service, profile, **connection_args)\nif not endpoint or 'Error' in endpoint:\n    return True", "path": "file_root\\_modules\\keystone.py", "repo_name": "CSSCorp/openstack-automation", "stars": 74, "license": "gpl-2.0", "language": "python", "size": 763}
{"docstring": "'''\nEnsure that the service doesn't exist in Keystone catalog\n\nname\n    The name of the service that should not exist\n'''\n", "func_signal": "def service_absent(name, profile=None, **connection_args):\n", "code": "ret = {'name': name,\n       'changes': {},\n       'result': True,\n       'comment': 'Service \"{0}\" is already absent'.format(name)}\n\n# Check if service is present\nrole = __salt__['keystone.service_get'](name=name,\n                                        profile=profile,\n                                        **connection_args)\nif 'Error' not in role:\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'Service \"{0}\" will be deleted'.format(name)\n        ret['changes']['Service'] = 'Will be deleted'\n        return ret\n    # Delete service\n    __salt__['keystone.service_delete'](name=name,\n                                        profile=profile,\n                                        **connection_args)\n    ret['comment'] = 'Service \"{0}\" has been deleted'.format(name)\n    ret['changes']['Service'] = 'Deleted'\n\nreturn ret", "path": "file_root\\_states\\keystone.py", "repo_name": "CSSCorp/openstack-automation", "stars": 74, "license": "gpl-2.0", "language": "python", "size": 763}
{"docstring": "'''\nTemplate for writing list functions\nReturn a list of available items (keystone items-list)\n\nCLI Example:\n\n.. code-block:: bash\n\n    salt '*' keystone.item_list\n'''\n", "func_signal": "def _item_list(profile=None, **connection_args):\n", "code": "kstone = auth(profile, **connection_args)\nret = []\nfor item in kstone.items.list():\n    ret.append(item.__dict__)\n    #ret[item.name] = {\n    #        'id': item.id,\n    #        'name': item.name,\n    #        }\nreturn ret\n\n#The following is a list of functions that need to be incorporated in the\n#keystone module. This list should be updated as functions are added.\n#\n#endpoint-create     Create a new endpoint associated with a service\n#endpoint-delete     Delete a service endpoint\n#discover            Discover Keystone servers and show authentication\n#                    protocols and\n#bootstrap           Grants a new role to a new user on a new tenant, after\n#                    creating each.", "path": "file_root\\_modules\\keystone.py", "repo_name": "CSSCorp/openstack-automation", "stars": 74, "license": "gpl-2.0", "language": "python", "size": 763}
{"docstring": "''''\nEnsures that the keystone role exists\n\nname\n    The name of the role that should be present\n'''\n", "func_signal": "def role_present(name, profile=None, **connection_args):\n", "code": "ret = {'name': name,\n       'changes': {},\n       'result': True,\n       'comment': 'Role \"{0}\" already exists'.format(name)}\n\n# Check if role is already present\nrole = __salt__['keystone.role_get'](name=name, profile=profile,\n                                     **connection_args)\n\nif 'Error' not in role:\n    return ret\nelse:\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'Role \"{0}\" will be added'.format(name)\n        ret['changes']['Role'] = 'Will be created'\n        return ret\n    # Create role\n    __salt__['keystone.role_create'](name, profile=profile,\n                                     **connection_args)\n    ret['comment'] = 'Role \"{0}\" has been added'.format(name)\n    ret['changes']['Role'] = 'Created'\nreturn ret", "path": "file_root\\_states\\keystone.py", "repo_name": "CSSCorp/openstack-automation", "stars": 74, "license": "gpl-2.0", "language": "python", "size": 763}
{"docstring": "'''\nReturn a list of available roles (keystone role-list)\n\nCLI Example:\n\n.. code-block:: bash\n\n    salt '*' keystone.role_list\n'''\n", "func_signal": "def role_list(profile=None, **connection_args):\n", "code": "kstone = auth(profile, **connection_args)\nret = {}\nfor role in kstone.roles.list():\n    ret[role.name] = {'id': role.id,\n                      'name': role.name}\nreturn ret", "path": "file_root\\_modules\\keystone.py", "repo_name": "CSSCorp/openstack-automation", "stars": 74, "license": "gpl-2.0", "language": "python", "size": 763}
{"docstring": "'''\nDelete a user (keystone user-delete)\n\nCLI Examples:\n\n.. code-block:: bash\n\n    salt '*' keystone.user_delete c965f79c4f864eaaa9c3b41904e67082\n    salt '*' keystone.user_delete user_id=c965f79c4f864eaaa9c3b41904e67082\n    salt '*' keystone.user_delete name=nova\n'''\n", "func_signal": "def user_delete(user_id=None, name=None, profile=None, **connection_args):\n", "code": "kstone = auth(profile, **connection_args)\nif name:\n    for user in kstone.users.list():\n        if user.name == name:\n            user_id = user.id\n            break\nif not user_id:\n    return {'Error': 'Unable to resolve user id'}\nkstone.users.delete(user_id)\nret = 'User ID {0} deleted'.format(user_id)\nif name:\n\n    ret += ' ({0})'.format(name)\nreturn ret", "path": "file_root\\_modules\\keystone.py", "repo_name": "CSSCorp/openstack-automation", "stars": 74, "license": "gpl-2.0", "language": "python", "size": 763}
{"docstring": "'''\nSet up keystone credentials\n\nOnly intended to be used within Keystone-enabled modules\n'''\n\n", "func_signal": "def auth(profile=None, **connection_args):\n", "code": "if profile:\n    prefix = profile + \":keystone.\"\nelse:\n    prefix = \"keystone.\"\n\n# look in connection_args first, then default to config file\ndef get(key, default=None):\n    return connection_args.get('connection_' + key,\n        __salt__['config.get'](prefix + key, default))\n\nuser = get('user', 'admin')\npassword = get('password', 'ADMIN')\ntenant = get('tenant', 'admin')\ntenant_id = get('tenant_id')\nauth_url = get('auth_url', 'http://127.0.0.1:35357/v2.0/')\ninsecure = get('insecure', False)\ntoken = get('token')\nendpoint = get('endpoint', 'http://127.0.0.1:35357/v2.0')\n\nif token:\n    kwargs = {'token': token,\n              'endpoint': endpoint}\nelse:\n    kwargs = {'username': user,\n              'password': password,\n              'tenant_name': tenant,\n              'tenant_id': tenant_id,\n              'auth_url': auth_url}\n    # 'insecure' keyword not supported by all v2.0 keystone clients\n    #   this ensures it's only passed in when defined\n    if insecure:\n        kwargs['insecure'] = True\n\nreturn client.Client(**kwargs)", "path": "file_root\\_modules\\keystone.py", "repo_name": "CSSCorp/openstack-automation", "stars": 74, "license": "gpl-2.0", "language": "python", "size": 763}
{"docstring": "'''\nEnsure that a Physical Device is not being used by lvm\n\nname\n    The device name to initialize.\n'''\n", "func_signal": "def pv_absent(name):\n", "code": "ret = {'changes': {},\n       'comment': '',\n       'name': name,\n       'result': True}\n\nif not __salt__['lvm.pvdisplay'](name):\n    ret['comment'] = 'Physical Volume {0} does not exist'.format(name)\nelif __opts__['test']:\n    ret['comment'] = 'Physical Volume {0} is set to be removed'.format(name)\n    ret['result'] = None\n    return ret\nelse:\n    changes = __salt__['lvm.pvremove'](name)\n\n    if __salt__['lvm.pvdisplay'](name):\n        ret['comment'] = 'Failed to remove Physical Volume {0}'.format(name)\n        ret['result'] = False\n    else:\n        ret['comment'] = 'Removed Physical Volume {0}'.format(name)\n        ret['changes'] = changes\nreturn ret", "path": "file_root\\_states\\lvm.py", "repo_name": "CSSCorp/openstack-automation", "stars": 74, "license": "gpl-2.0", "language": "python", "size": 763}
{"docstring": "'''\nReturn a specific services (keystone service-get)\n\nCLI Examples:\n\n.. code-block:: bash\n\n    salt '*' keystone.service_get c965f79c4f864eaaa9c3b41904e67082\n    salt '*' keystone.service_get service_id=c965f79c4f864eaaa9c3b41904e67082\n    salt '*' keystone.service_get name=nova\n'''\n", "func_signal": "def service_get(service_id=None, name=None, profile=None, **connection_args):\n", "code": "kstone = auth(profile, **connection_args)\nret = {}\nif name:\n    for service in kstone.services.list():\n        if service.name == name:\n            service_id = service.id\n            break\nif not service_id:\n    return {'Error': 'Unable to resolve service id'}\nservice = kstone.services.get(service_id)\nret[service.name] = {'id': service.id,\n                     'name': service.name,\n                     'type': service.type,\n                     'description': service.description}\nreturn ret", "path": "file_root\\_modules\\keystone.py", "repo_name": "CSSCorp/openstack-automation", "stars": 74, "license": "gpl-2.0", "language": "python", "size": 763}
{"docstring": "'''\nOnly load the module if lvm is installed\n'''\n", "func_signal": "def __virtual__():\n", "code": "if salt.utils.which('lvm'):\n    return 'lvm'\nreturn False", "path": "file_root\\_states\\lvm.py", "repo_name": "CSSCorp/openstack-automation", "stars": 74, "license": "gpl-2.0", "language": "python", "size": 763}
{"docstring": "'''\nEnsure that the keystone user is absent.\n\nname\n    The name of the user that should not exist\n'''\n", "func_signal": "def user_absent(name, profile=None, **connection_args):\n", "code": "ret = {'name': name,\n       'changes': {},\n       'result': True,\n       'comment': 'User \"{0}\" is already absent'.format(name)}\n\n# Check if user is present\nuser = __salt__['keystone.user_get'](name=name, profile=profile,\n                                     **connection_args)\nif 'Error' not in user:\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'User \"{0}\" will be deleted'.format(name)\n        ret['changes']['User'] = 'Will be deleted'\n        return ret\n    # Delete that user!\n    __salt__['keystone.user_delete'](name=name, profile=profile,\n                                     **connection_args)\n    ret['comment'] = 'User \"{0}\" has been deleted'.format(name)\n    ret['changes']['User'] = 'Deleted'\n\nreturn ret", "path": "file_root\\_states\\keystone.py", "repo_name": "CSSCorp/openstack-automation", "stars": 74, "license": "gpl-2.0", "language": "python", "size": 763}
{"docstring": "'''\nEnsure that the endpoint for a service doesn't exist in Keystone catalog\n\nname\n    The name of the service whose endpoints should not exist\n'''\n", "func_signal": "def endpoint_absent(name, profile=None, **connection_args):\n", "code": "ret = {'name': name,\n       'changes': {},\n       'result': True,\n       'comment': 'endpoint for service \"{0}\" is already absent'.format(name)}\n\n# Check if service is present\nendpoint = __salt__['keystone.endpoint_get'](name,\n                                             profile=profile,\n                                             **connection_args)\nif not endpoint:\n    return ret\nelse:\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'Endpoint for service \"{0}\" will be deleted'.format(name)\n        ret['changes']['endpoint'] = 'Will be deleted'\n        return ret\n    # Delete service\n    __salt__['keystone.endpoint_delete'](name,\n                                         profile=profile,\n                                         **connection_args)\n    ret['comment'] = 'Endpoint for service \"{0}\" has been deleted'.format(name)\n    ret['changes']['endpoint'] = 'Deleted'\nreturn ret", "path": "file_root\\_states\\keystone.py", "repo_name": "CSSCorp/openstack-automation", "stars": 74, "license": "gpl-2.0", "language": "python", "size": 763}
{"docstring": "'''\nTemplate for writing list functions\nReturn a list of available items (glance items-list)\n\nCLI Example:\n\n.. code-block:: bash\n\n    salt '*' glance.item_list\n'''\n", "func_signal": "def _item_list(profile=None, **connection_args):\n", "code": "nt_ks = _auth(profile, **connection_args)\nret = []\nfor item in nt_ks.items.list():\n    ret.append(item.__dict__)\n    # ret[item.name] = {\n    #        'name': item.name,\n    #    }\nreturn ret", "path": "file_root\\_modules\\glance.py", "repo_name": "CSSCorp/openstack-automation", "stars": 74, "license": "gpl-2.0", "language": "python", "size": 763}
{"docstring": "'''\nSet up keystone credentials\n'''\n", "func_signal": "def _auth(profile=None, **connection_args):\n", "code": "kstone = __salt__['keystone.auth'](profile, **connection_args)\ntoken = kstone.auth_token\nendpoint = kstone.service_catalog.url_for(\n    service_type='image',\n    endpoint_type='publicURL',\n    )\n\nreturn client.Client('1', endpoint, token=token)", "path": "file_root\\_modules\\glance.py", "repo_name": "CSSCorp/openstack-automation", "stars": 74, "license": "gpl-2.0", "language": "python", "size": 763}
{"docstring": "'''\nReturn a list of available tenants (keystone tenants-list)\n\nCLI Example:\n\n.. code-block:: bash\n\n    salt '*' keystone.tenant_list\n'''\n", "func_signal": "def tenant_list(profile=None, **connection_args):\n", "code": "kstone = auth(profile, **connection_args)\nret = {}\nfor tenant in kstone.tenants.list():\n    ret[tenant.name] = {'id': tenant.id,\n                        'name': tenant.name,\n                        'description': tenant.description,\n                        'enabled': tenant.enabled}\nreturn ret", "path": "file_root\\_modules\\keystone.py", "repo_name": "CSSCorp/openstack-automation", "stars": 74, "license": "gpl-2.0", "language": "python", "size": 763}
{"docstring": "'''\nOnly load this module if keystone\nis installed on this minion.\n'''\n", "func_signal": "def __virtual__():\n", "code": "if HAS_KEYSTONE:\n    return 'keystone'\nreturn False", "path": "file_root\\_modules\\keystone.py", "repo_name": "CSSCorp/openstack-automation", "stars": 74, "license": "gpl-2.0", "language": "python", "size": 763}
{"docstring": "'''\nDelete a service from Keystone service catalog\n\nCLI Examples:\n\n.. code-block:: bash\n\n    salt '*' keystone.service_delete c965f79c4f864eaaa9c3b41904e67082\n    salt '*' keystone.service_delete name=nova\n'''\n", "func_signal": "def service_delete(service_id=None, name=None, profile=None, **connection_args):\n", "code": "kstone = auth(profile, **connection_args)\nif name:\n    service_id = service_get(name=name, profile=profile,\n                             **connection_args)[name]['id']\nservice = kstone.services.delete(service_id)\nreturn 'Keystone service ID \"{0}\" deleted'.format(service_id)", "path": "file_root\\_modules\\keystone.py", "repo_name": "CSSCorp/openstack-automation", "stars": 74, "license": "gpl-2.0", "language": "python", "size": 763}
{"docstring": "'''\nCreate an image (glance image-create)\n\nCLI Example:\n\n.. code-block:: bash\n\n    salt '*' glance.image_create name=f16-jeos is_public=true \\\\\n             disk_format=qcow2 container_format=ovf \\\\\n             copy_from=http://berrange.fedorapeople.org/images/ \\\\\n             2012-02-29/f16-x86_64-openstack-sda.qcow2\n\nFor all possible values, run ``glance help image-create`` on the minion.\n'''\n", "func_signal": "def image_create(profile=None, **connection_args):\n", "code": "nt_ks = _auth(profile, **connection_args)\nfields = dict(\n    filter(\n        lambda x: x[0] in glanceclient.v1.images.CREATE_PARAMS,\n        connection_args.items()\n    )\n)\n\nimage = nt_ks.images.create(**fields)\nreturn image_show(id=str(image.id), profile=profile, **connection_args)", "path": "file_root\\_modules\\glance.py", "repo_name": "CSSCorp/openstack-automation", "stars": 74, "license": "gpl-2.0", "language": "python", "size": 763}
{"docstring": "# coerce test examples to be N-by-2 scipy array\n", "func_signal": "def predict(self, X, k=1):\n", "code": "X = sp.array(X, ndmin=2)\n# number of test examples\nN = X.shape[0]\n\n# create empty vector for predictions\nyhat = sp.zeros(N)\n\n# use the cdist function to quickly compute distances\n# between all test and training examples\nD = spat.distance.cdist(X, self.X, 'euclidean')\n\nfor i in range(N):\n    # grab the indices of the k closest points\n    ndx = D[i,:].argsort()[:k]\n\n    # take a majority vote over the nearest points' labels\n    yhat[i] = mode(self.y[ndx])[0]\n    \nreturn yhat", "path": "code\\image_data\\knn.py", "repo_name": "drewconway/strata_bootcamp", "stars": 94, "license": "None", "language": "python", "size": 101415}
{"docstring": "\"\"\"Convert an email string on the type 'Ann Smith <ann.smith@email.com>'\nto only 'ann.smith@email.com'\n\"\"\"\n# Need to clean up the messy CSV data to ge the graph right\n", "func_signal": "def getEmail(email_string):\n", "code": "if email_string.find(\"<\") > -1:\n    email_address=re.split(\"[<>]\",email_string)     # First, extract Address from brackets\n    address_index=map(lambda x: x.find(\"@\"), email_address) # Find where address is\n    address_index=map(lambda y: y>0, address_index).index(True) \n    email_address=email_address[address_index]  # Get address string\n    email_address=email_address.replace('\"','') # Do final string cleaning\n    return email_address.strip()\nelse:\n    return email_string", "path": "code\\text_data\\email_graph\\email_viz.py", "repo_name": "drewconway/strata_bootcamp", "stars": 94, "license": "None", "language": "python", "size": 101415}
{"docstring": "# Create a NetworkX graph object \n", "func_signal": "def main():\n", "code": "gmail_graph=graphFromCSV(\"../email_analysis/email_graph.csv\")\n\n# Draw entire graph\nfull_graph=plt.figure(figsize=(10,10))\nnx.draw_spring(gmail_graph, arrows=False, node_size=50, with_labels=False)\nplt.savefig(\"../../../images/graphs/full_graph.png\")\n\n# Draw ann9enigma@gmail.com's Ego graph\nego_plot(gmail_graph, \"ann9enigma@gmail.com\", \"../../../images/graphs/ann9enigma_ego.png\")\n\n# Create a lattice plot of all weakly connected components\ngmail_components=nx.weakly_connected_component_subgraphs(gmail_graph)\nlattice_plot(gmail_components, \"../../../images/graphs/gmail_subgraphs.png\")", "path": "code\\text_data\\email_graph\\email_viz.py", "repo_name": "drewconway/strata_bootcamp", "stars": 94, "license": "None", "language": "python", "size": 101415}
{"docstring": "# memorize training data\n", "func_signal": "def add_examples(self, X, y):\n", "code": "try:\n    self.X = sp.vstack((self.X, X))\n    self.y = sp.concatenate((self.y, y))\nexcept AttributeError:\n    self.X = X\n    self.y = y", "path": "code\\image_data\\knn.py", "repo_name": "drewconway/strata_bootcamp", "stars": 94, "license": "None", "language": "python", "size": 101415}
{"docstring": "\"\"\"\nplot a montage of the examples specified in ndx (as rows of X)\n\"\"\"\n\n", "func_signal": "def plot_digits(X, ndx, ncol=50, width=IM_WIDTH, cmap=cm.gray):\n", "code": "row = 0\ncol = 0\nfor i in range(ndx.shape[0]):\n    \n    plt.figimage(reshape_digit(X, ndx[i]),\n                 xo=width*col, yo=width*row,\n                 origin='upper', cmap=cmap)\n\n    col += 1\n    if col % ncol == 0:\n        row += 1\n        col = 0", "path": "code\\image_data\\digits.py", "repo_name": "drewconway/strata_bootcamp", "stars": 94, "license": "None", "language": "python", "size": 101415}
{"docstring": "\"\"\"\nlist_folders: lists all folders in the e-mail account\n\"\"\"\n", "func_signal": "def list_folders(self):\n", "code": "(status, folder_list) = self.m.list()\n\nfolder_names = []\nfor f in folder_list:\n    label = f.split('\"')[-2] # HACKY: parsing this can get messy\n    folder_names.append(label)\n    \nreturn folder_names", "path": "code\\text_data\\email_analysis\\gmail\\__init__.py", "repo_name": "drewconway/strata_bootcamp", "stars": 94, "license": "None", "language": "python", "size": 101415}
{"docstring": "\"\"\"\nget_message: retrieve a message from a folder by id\n\"\"\"\n", "func_signal": "def get_message(self, message_id):\n", "code": "(resp, data) = self.m.fetch(message_id, \"(RFC822)\") # fetching the mail, \"`(RFC822)`\" means \"get the whole stuff\", but you can ask for headers only, etc\nreturn data[0][1]", "path": "code\\text_data\\email_analysis\\gmail\\__init__.py", "repo_name": "drewconway/strata_bootcamp", "stars": 94, "license": "None", "language": "python", "size": 101415}
{"docstring": "\"\"\"Draw the ego graph for a given node 'n' \"\"\"\n", "func_signal": "def ego_plot(graph, n, file_path):\n", "code": "ego_plot=plt.figure(figsize=(10,10))\nego=nx.ego_graph(graph,n) # Get ego graph for n\n\n# Draw graph\npos=nx.spring_layout(ego, iterations=5000)\nnx.draw_networkx_nodes(ego,pos,node_color='b',node_size=100)\nnx.draw_networkx_edges(ego,pos)\n# Create label offset\nlabel_pos=dict([(a,[b[0],b[1]+0.03]) for (a,b) in pos.items()])\nnx.draw_networkx_labels(ego, pos=label_pos,font_color=\"darkgreen\")\n\n# Draw ego as large and red\nnx.draw_networkx_nodes(ego,pos,nodelist=[n],node_size=300,node_color='r', font_color=\"darkgreen\")\nplt.savefig(file_path)", "path": "code\\text_data\\email_graph\\email_viz.py", "repo_name": "drewconway/strata_bootcamp", "stars": 94, "license": "None", "language": "python", "size": 101415}
{"docstring": "# run over red, green, and blue channels\n", "func_signal": "def rgb_hist(I, ax, bins=256):\n", "code": "    channels = ('r','g','b')\n    for i, color in enumerate(channels):\n        # get count pixel intensities for this channel\n        counts, bins, patches = plt.hist(I[:,:,i].flatten(), bins=bins, normed=True, visible=False)\n    # hack: choose mid-point of bins as centers\n        centers = bins[:-1] + sp.diff(bins)/2\n    # line plot with fill\n        plt.plot(centers, counts, color=color)\n        ax.fill_between(centers, 0, counts, color=color, alpha=0.25)\n# hack for matlab's axes('square') function\n    # http://www.mail-archive.com/matplotlib-users@lists.sourceforge.net/msg08388.html\n    plt.axis('tight')\n    ax.set_aspect(1./ax.get_data_ratio())", "path": "code\\image_data\\imgtools.py", "repo_name": "drewconway/strata_bootcamp", "stars": 94, "license": "None", "language": "python", "size": 101415}
{"docstring": "# coerce test examples to be N-by-2 scipy array\n", "func_signal": "def predict(self, X, k=1):\n", "code": "X = sp.array(X, ndmin=2)\n# number of test examples\nN = X.shape[0]\n\n# create empty vector for predictions\nyhat = sp.zeros(N)\n\n# use the kd-tree query function to quickly lookup nearest\n# neighbors\nD, ndx = self.kdtree.query(X, k=k)\nfor i in range(N):\n    # take a majority vote over the nearest points' labels\n    yhat[i] = mode(self.y[ndx])[0]\n\nreturn yhat", "path": "code\\image_data\\knn.py", "repo_name": "drewconway/strata_bootcamp", "stars": 94, "license": "None", "language": "python", "size": 101415}
{"docstring": "\"\"\"\nCreates a lattice style plot of all graph components\n\"\"\"\n", "func_signal": "def lattice_plot(component_list, file_path):\n", "code": "graph_fig=plt.figure(figsize=(20,10))    # Create figure\n\n# Set the number of rows in the plot based on an odd or\n# even number of components  \nnum_components=len(component_list)\nif num_components % 2 > 0:\n    num_cols=(num_components/2)+1\nelse:\n    num_cols=num_components/2\n\n# Plot subgraphs, with centrality annotation\nplot_count=1\nfor G in component_list:\n    # Find actor in each component with highest degree\n    in_cent=nx.degree(G)\n    in_cent=[(b,a) for (a,b) in in_cent.items()]\n    in_cent.sort()\n    in_cent.reverse()\n    high_in=in_cent[0][1]\n    \n    # Plot with annotation\n    plt.subplot(2,num_cols,plot_count)\n    nx.draw_spring(G, node_size=35, with_labels=False)\n    plt.text( 0,-.1,\"Highest degree: \"+high_in, color=\"darkgreen\")\n    plot_count+=1\n\nplt.savefig(file_path)", "path": "code\\text_data\\email_graph\\email_viz.py", "repo_name": "drewconway/strata_bootcamp", "stars": 94, "license": "None", "language": "python", "size": 101415}
{"docstring": "\"\"\"\nCreate a NetworkX graph object from a csv file\n\"\"\"\n# Create NetworkX object for storing graph\n", "func_signal": "def graphFromCSV(file_path, create_using=nx.DiGraph()):\n", "code": "csv_graph = create_using\n\n# Create reader CSV reader object from file path\ncsv_file=open(file_path, \"rb\")\ncsv_reader=csv.reader(csv_file)\n\n#  Add rows from CSV file as \nfor row in csv_reader:\n    clean_edges=map(getEmail, row)\n    csv_graph.add_edge(clean_edges[0], clean_edges[1])\n\n# Return graph object\nreturn csv_graph", "path": "code\\text_data\\email_graph\\email_viz.py", "repo_name": "drewconway/strata_bootcamp", "stars": 94, "license": "None", "language": "python", "size": 101415}
{"docstring": "\"\"\"\nprobability: prob that an item is in a category\n\"\"\"\n", "func_signal": "def probability(self, item, category):\n", "code": "category_prob = self.get_category_count(category) / sum(self.category_count.values())\nreturn self.document_probability(item, category) * category_prob", "path": "code\\text_data\\email_analysis\\email_classify.py", "repo_name": "drewconway/strata_bootcamp", "stars": 94, "license": "None", "language": "python", "size": 101415}
{"docstring": "# number of examples\n", "func_signal": "def train_test_split(X, y, frac=0.8):\n", "code": "N = y.shape[0]\n\n# shuffle example digits\nndx = sp.random.permutation(N)\nX = X[ndx,:]\ny = y[ndx,:]\n\n# number of training examples as fraction of total\nNtrain = int(frac*N)\n\n# split data into training and test sets\nXtrain = X[:Ntrain,:]\nytrain = y[:Ntrain]\n\nXtest = X[Ntrain:,:]\nytest = y[Ntrain:]\n\nreturn (Xtrain, ytrain, Xtest, ytest)", "path": "code\\image_data\\mltools.py", "repo_name": "drewconway/strata_bootcamp", "stars": 94, "license": "None", "language": "python", "size": 101415}
{"docstring": "\"\"\"\nplot the i-th example (row of X) as an image\n\"\"\"\n\n", "func_signal": "def plot_digit(X, i, width=IM_WIDTH):\n", "code": "I = reshape_digit(X, i, width)\nplt.imshow(I, cmap=cm.gray, interpolation='nearest')", "path": "code\\image_data\\digits.py", "repo_name": "drewconway/strata_bootcamp", "stars": 94, "license": "None", "language": "python", "size": 101415}
{"docstring": "# escape query\n", "func_signal": "def yql_public(query):\n", "code": "query_str = urlencode({'q': query, 'format': 'json'})\n\n# fetch results\nurl = '%s?%s' % (YQL_PUBLIC, query_str)\nresult = urlopen(url)\n\n# parse json and return\nreturn json.load(result)['query']['results']", "path": "code\\image_data\\simpleyql.py", "repo_name": "drewconway/strata_bootcamp", "stars": 94, "license": "None", "language": "python", "size": 101415}
{"docstring": "\"\"\"\nA function to graphically check a random distribution's\nfit to a theoretical normal.\n\"\"\"\n", "func_signal": "def plot_normal(random_numbers, path=\"\"):\n", "code": "fig=plt.figure(figsize = (8,6)) # Create a figure to plot in (good habit)\n# Histogram of random numbers with 25 bins\nn, bins, pataches = plt.hist(random_numbers,normed=True,bins=25,alpha=0.75)  \n# Add \"best fit\" line\ny = norm.pdf(bins)\nplt.plot(bins,y,\"r-\")\n# Save plot\nplt.xlabel(\"Random numbers\")\nplt.ylabel(\"Density\")\nplt.title(\"My first matplotlib visualization!\")\nplt.savefig(path)", "path": "code\\text_data\\first_viz\\first_viz.py", "repo_name": "drewconway/strata_bootcamp", "stars": 94, "license": "None", "language": "python", "size": 101415}
{"docstring": "\"\"\"\nget_message_ids: get the list of message ids in the folder\n\"\"\"\n", "func_signal": "def get_message_ids(self, folder_name=None):\n", "code": "if folder_name:\n    self.select_folder(folder_name)\n\ntry:\n    response, items = self.m.search(None, \"ALL\")        \n    items = items[0].split() \n    return items\nexcept imaplib.IMAP4.error:\n    return []", "path": "code\\text_data\\email_analysis\\gmail\\__init__.py", "repo_name": "drewconway/strata_bootcamp", "stars": 94, "license": "None", "language": "python", "size": 101415}
{"docstring": "\"\"\"\nget_all_messages_from_folder: convenience method to loop through and return all messages in a folder\n\"\"\"\n", "func_signal": "def get_all_messages_from_folder(self, folder_name=None):\n", "code": "message_data = []\nfor message_id in self.get_message_ids(folder_name):\n    message_data.append(self.get_message(message_id))\n    \nreturn message_data", "path": "code\\text_data\\email_analysis\\gmail\\__init__.py", "repo_name": "drewconway/strata_bootcamp", "stars": 94, "license": "None", "language": "python", "size": 101415}
{"docstring": "# hack from http://www.meliza.org/itoaeky/graphics.php?itemid=35\n# to provide functionality similar to matlab's accumarray\n\n", "func_signal": "def accumarray(i, j, val=None):\n", "code": "if not val:\n    val = sp.ones(len(i))\n\nreturn coo_matrix( (val, (i, j)) ).todense()", "path": "code\\image_data\\mltools.py", "repo_name": "drewconway/strata_bootcamp", "stars": 94, "license": "None", "language": "python", "size": 101415}
{"docstring": "#\u5bfc\u5165\u6dd8\u5b9d\u4ea7\u54c1\n", "func_signal": "def import_taobao_product(dbname, uid, app_key, rsp):\n", "code": "line = rsp.packet.msg.import_taobao_product\npool = openerp.pooler.get_pool(dbname)\ncr = pool.db.cursor()\ntry:\n    shop = pool.get('taobao.shop')._get(cr, uid, args = [('taobao_app_key','=',app_key)])\n    top = TOP(shop.taobao_app_key, shop.taobao_app_secret, shop.taobao_session_key)\n    taobao_product_obj = pool.get('taobao.product')\n\n    skus = taobao_product_obj._top_item_skus_get(shop, top, line.get(\"taobao_num_iid\", None))\n    if skus:\n        for sku in skus:\n            taobao_product_obj._get_create_product(pool, cr, uid, shop, top,\n                    taobao_num_iid = line.get('taobao_num_iid', None),\n                    taobao_sku_id = sku.sku_id,\n\n                    taobao_product_category_id = line.get('taobao_product_category_id', None),\n                    taobao_product_supplier = line.get('taobao_product_supplier', None),\n                    taobao_product_warehouse_id = line.get('taobao_product_warehouse_id', None),\n                    taobao_product_location_id = line.get('taobao_product_location_id', None),\n                    taobao_product_cost_method = line.get('taobao_product_cost_method', None),\n                    taobao_product_type = line.get('taobao_product_type', None),\n                    taobao_product_supply_method = line.get('taobao_product_supply_method', None),\n                    taobao_product_procure_method = line.get('taobao_product_procure_method', None),\n                    taobao_product_min_qty = line.get('taobao_product_min_qty', None),\n                    taobao_product_max_qty = line.get('taobao_product_max_qty', None),\n                    taobao_product_uom = line.get('taobao_product_uom', None),\n                    is_update_stock = line.get('is_update_stock', None),\n\n                    )\n    else:\n        taobao_product_obj._get_create_product(pool, cr, uid, shop, top,\n                taobao_num_iid = line.get('taobao_num_iid', None),\n\n                taobao_product_category_id = line.get('taobao_product_category_id', None),\n                taobao_product_supplier = line.get('taobao_product_supplier', None),\n                taobao_product_warehouse_id = line.get('taobao_product_warehouse_id', None),\n                taobao_product_location_id = line.get('taobao_product_location_id', None),\n                taobao_product_cost_method = line.get('taobao_product_cost_method', None),\n                taobao_product_type = line.get('taobao_product_type', None),\n                taobao_product_supply_method = line.get('taobao_product_supply_method', None),\n                taobao_product_procure_method = line.get('taobao_product_procure_method', None),\n                taobao_product_min_qty = line.get('taobao_product_min_qty', None),\n                taobao_product_max_qty = line.get('taobao_product_max_qty', None),\n                taobao_product_uom = line.get('taobao_product_uom', None),\n                is_update_stock = line.get('is_update_stock', None),\n\n                )\n\nfinally:\n    cr.close()", "path": "taobao\\taobao_product.py", "repo_name": "buke/openerp-taobao", "stars": 126, "license": "other", "language": "python", "size": 595}
{"docstring": "#\u5356\u5bb6\u53d1\u8d27\n", "func_signal": "def TaobaoTradeSellerShip(dbname, uid, app_key, rsp):\n", "code": "notify_trade = rsp.packet.msg.notify_trade\npool = openerp.pooler.get_pool(dbname)\ncr = pool.db.cursor()\ntry:\n    shop = pool.get('taobao.shop')._get(cr, uid, args = [('taobao_app_key','=',app_key)])\n    top = TOP(shop.taobao_app_key, shop.taobao_app_secret, shop.taobao_session_key)\n    sale_order_obj = pool.get('sale.order')\n    sale_order_instance = sale_order_obj._taobao_save_fullinfo(pool, cr, uid, notify_trade.tid, shop, top)\n    if sale_order_instance and sale_order_instance.taobao_trade_status in ['WAIT_SELLER_SEND_GOODS', 'WAIT_BUYER_CONFIRM_GOODS', 'TRADE_FINISHED', 'TRADE_BUYER_SIGNED']:\n        sale_order_obj._taobao_confirm_order(pool, cr, uid, [sale_order_instance.id])\n        sale_order_obj._taobao_order_ship(pool, cr, uid, [sale_order_instance.id], top)\n        sale_order_obj._taobao_create_invoice(pool, cr, uid, [sale_order_instance.id])\n    cr.commit()\nfinally:\n    cr.close()\n#TODO send sms to client", "path": "taobao\\taobao_order.py", "repo_name": "buke/openerp-taobao", "stars": 126, "license": "other", "language": "python", "size": 595}
{"docstring": "\"\"\"\n\u542f\u52a8 taobao worker \u7ebf\u7a0b\n\"\"\"\n", "func_signal": "def _start_worker_thread(self, cr, uid):\n", "code": "for i in range(int(config.get('taobao_worker_thread_limit', 4))):\n    thread_name = 'taobao_worker_%s' % str(i)\n    thread_exits = False\n    for thread in threading.enumerate():\n        if thread.getName() == thread_name:\n            thread_exits = True\n            break;\n\n    if not thread_exits:\n        from taobao_base import mq_server\n        t = threading.Thread(target=mq_server, args = [], name=thread_name)\n        t.setDaemon(True)\n        t.start()\n\n    time.sleep(50/1000)", "path": "taobao\\taobao_shop.py", "repo_name": "buke/openerp-taobao", "stars": 126, "license": "other", "language": "python", "size": 595}
{"docstring": "\"\"\"Release this job back into the ready queue.\"\"\"\n", "func_signal": "def release(self, priority=None, delay=0):\n", "code": "if self.reserved:\n    self.conn.release(self.jid, priority or self._priority(), delay)\n    self.reserved = False", "path": "taobao\\libs\\beanstalkc.py", "repo_name": "buke/openerp-taobao", "stars": 126, "license": "other", "language": "python", "size": 595}
{"docstring": "\"\"\"Delete this job.\"\"\"\n", "func_signal": "def delete(self):\n", "code": "self.conn.delete(self.jid)\nself.reserved = False", "path": "taobao\\libs\\beanstalkc.py", "repo_name": "buke/openerp-taobao", "stars": 126, "license": "other", "language": "python", "size": 595}
{"docstring": "#\u4fee\u6539\u8ba2\u5355\u4fe1\u606f\uff08SKU\u7b49\uff09\n", "func_signal": "def TaobaoTradeChanged(dbname, uid, app_key, rsp):\n", "code": "notify_trade = rsp.packet.msg.notify_trade\npool = openerp.pooler.get_pool(dbname)\ncr = pool.db.cursor()\ntry:\n    shop = pool.get('taobao.shop')._get(cr, uid, args = [('taobao_app_key','=',app_key)])\n    top = TOP(shop.taobao_app_key, shop.taobao_app_secret, shop.taobao_session_key)\n    sale_order_obj = pool.get('sale.order')\n    sale_order_instance = sale_order_obj._taobao_save_fullinfo(pool, cr, uid, notify_trade.tid, shop, top)\n    if sale_order_instance and sale_order_instance.taobao_trade_status in ['WAIT_SELLER_SEND_GOODS', 'WAIT_BUYER_CONFIRM_GOODS', 'TRADE_FINISHED', 'TRADE_BUYER_SIGNED']:\n        sale_order_obj._taobao_confirm_order(pool, cr, uid, [sale_order_instance.id])\n    cr.commit()\nfinally:\n    cr.close()", "path": "taobao\\taobao_order.py", "repo_name": "buke/openerp-taobao", "stars": 126, "license": "other", "language": "python", "size": 595}
{"docstring": "#\u4fee\u6539\u4ea4\u6613\u6536\u8d27\u5730\u5740\n", "func_signal": "def TaobaoTradeLogisticsAddressChanged(dbname, uid, app_key, rsp):\n", "code": "notify_trade = rsp.packet.msg.notify_trade\npool = openerp.pooler.get_pool(dbname)\ncr = pool.db.cursor()\ntry:\n    shop = pool.get('taobao.shop')._get(cr, uid, args = [('taobao_app_key','=',app_key)])\n    top = TOP(shop.taobao_app_key, shop.taobao_app_secret, shop.taobao_session_key)\n    sale_order_obj = pool.get('sale.order')\n    sale_order_obj._taobao_save_fullinfo(pool, cr, uid, notify_trade.tid, shop, top)\n    cr.commit()\nfinally:\n    cr.close()", "path": "taobao\\taobao_order.py", "repo_name": "buke/openerp-taobao", "stars": 126, "license": "other", "language": "python", "size": 595}
{"docstring": "\"\"\"Touch this reserved job, requesting more time to work on it before\nit expires.\"\"\"\n", "func_signal": "def touch(self):\n", "code": "if self.reserved:\n    self.conn.touch(self.jid)", "path": "taobao\\libs\\beanstalkc.py", "repo_name": "buke/openerp-taobao", "stars": 126, "license": "other", "language": "python", "size": 595}
{"docstring": "\"\"\"Connect to beanstalkd server.\"\"\"\n", "func_signal": "def connect(self):\n", "code": "self._socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nself._socket.settimeout(self._connect_timeout)\nSocketError.wrap(self._socket.connect, (self.host, self.port))\nself._socket.settimeout(None)\nself._socket_file = self._socket.makefile('rb')", "path": "taobao\\libs\\beanstalkc.py", "repo_name": "buke/openerp-taobao", "stars": 126, "license": "other", "language": "python", "size": 595}
{"docstring": "#\u4e70\u5bb6\u4ed8\u6b3e\n", "func_signal": "def TaobaoTradeBuyerPay(dbname, uid, app_key, rsp):\n", "code": "notify_trade = rsp.packet.msg.notify_trade\npool = openerp.pooler.get_pool(dbname)\ncr = pool.db.cursor()\ntry:\n    shop = pool.get('taobao.shop')._get(cr, uid, args = [('taobao_app_key','=',app_key)])\n    top = TOP(shop.taobao_app_key, shop.taobao_app_secret, shop.taobao_session_key)\n    sale_order_obj = pool.get('sale.order')\n    sale_order_instance = sale_order_obj._taobao_save_fullinfo(pool, cr, uid, notify_trade.tid, shop, top)\n    if sale_order_instance and sale_order_instance.taobao_trade_status in ['WAIT_SELLER_SEND_GOODS', 'WAIT_BUYER_CONFIRM_GOODS', 'TRADE_FINISHED', 'TRADE_BUYER_SIGNED']:\n        sale_order_obj._taobao_confirm_order(pool, cr, uid, [sale_order_instance.id])\n\n    cr.commit()\nfinally:\n    cr.close()", "path": "taobao\\taobao_order.py", "repo_name": "buke/openerp-taobao", "stars": 126, "license": "other", "language": "python", "size": 595}
{"docstring": "\"\"\"Bury a job, by job id.\"\"\"\n", "func_signal": "def bury(self, jid, priority=DEFAULT_PRIORITY):\n", "code": "self._interact('bury %d %d\\r\\n' % (jid, priority),\n               ['BURIED'],\n               ['NOT_FOUND'])", "path": "taobao\\libs\\beanstalkc.py", "repo_name": "buke/openerp-taobao", "stars": 126, "license": "other", "language": "python", "size": 595}
{"docstring": "#\u521b\u5efa\u4ea4\u6613\n", "func_signal": "def TaobaoTradeCreate(dbname, uid, app_key, rsp):\n", "code": "notify_trade = rsp.packet.msg.notify_trade\npool = openerp.pooler.get_pool(dbname)\ncr = pool.db.cursor()\ntry:\n    shop = pool.get('taobao.shop')._get(cr, uid, args = [('taobao_app_key','=',app_key)])\n    top = TOP(shop.taobao_app_key, shop.taobao_app_secret, shop.taobao_session_key)\n    sale_order_obj = pool.get('sale.order')\n    sale_order_obj._taobao_save_fullinfo(pool, cr, uid, notify_trade.tid, shop, top)\n    cr.commit()\nfinally:\n    cr.close()", "path": "taobao\\taobao_order.py", "repo_name": "buke/openerp-taobao", "stars": 126, "license": "other", "language": "python", "size": 595}
{"docstring": "#\u5173\u95ed\u4ea4\u6613\n", "func_signal": "def TaobaoTradeClose(dbname, uid, app_key, rsp):\n", "code": "notify_trade = rsp.packet.msg.notify_trade\npool = openerp.pooler.get_pool(dbname)\ncr = pool.db.cursor()\ntry:\n    shop = pool.get('taobao.shop')._get(cr, uid, args = [('taobao_app_key','=',app_key)])\n    top = TOP(shop.taobao_app_key, shop.taobao_app_secret, shop.taobao_session_key)\n    sale_order_obj = pool.get('sale.order')\n    sale_order_obj._taobao_save_fullinfo(pool, cr, uid, notify_trade.tid, shop, top)\n    sale_order_obj._taobao_cancel_order(pool, cr, uid, sale_order_obj.search(cr, uid, [('origin','=','TB%s' % notify_trade.tid)]))\n\n    cr.commit()\nfinally:\n    cr.close()", "path": "taobao\\taobao_order.py", "repo_name": "buke/openerp-taobao", "stars": 126, "license": "other", "language": "python", "size": 595}
{"docstring": "\"\"\"Return a dict of stats about a job, by job id.\"\"\"\n", "func_signal": "def stats_job(self, jid):\n", "code": "return self._interact_yaml('stats-job %d\\r\\n' % jid,\n                           ['OK'],\n                           ['NOT_FOUND'])", "path": "taobao\\libs\\beanstalkc.py", "repo_name": "buke/openerp-taobao", "stars": 126, "license": "other", "language": "python", "size": 595}
{"docstring": "\"\"\"Pause a tube for a given delay time, in seconds.\"\"\"\n", "func_signal": "def pause_tube(self, name, delay):\n", "code": "self._interact('pause-tube %s %d\\r\\n' %(name, delay),\n               ['PAUSED'],\n               ['NOT_FOUND'])", "path": "taobao\\libs\\beanstalkc.py", "repo_name": "buke/openerp-taobao", "stars": 126, "license": "other", "language": "python", "size": 595}
{"docstring": "\"\"\"Put a job into the current tube. Returns job id.\"\"\"\n", "func_signal": "def put(self, body, priority=DEFAULT_PRIORITY, delay=0, ttr=DEFAULT_TTR):\n", "code": "assert isinstance(body, str), 'Job body must be a str instance'\njid = self._interact_value(\n        'put %d %d %d %d\\r\\n%s\\r\\n' %\n            (priority, delay, ttr, len(body), body),\n        ['INSERTED', 'BURIED'], ['JOB_TOO_BIG'])\nreturn int(jid)", "path": "taobao\\libs\\beanstalkc.py", "repo_name": "buke/openerp-taobao", "stars": 126, "license": "other", "language": "python", "size": 595}
{"docstring": "#\u6784\u9020\u53c2\u6570\n", "func_signal": "def execute(self, method_name, **kwargs):\n", "code": "params = {}\nfor k, v in kwargs.iteritems():\n    if v: params[k.lower()] = v\nparams['app_key'] = self.app_key\nparams['v'] = '2.0'\nparams['sign_method'] = 'md5',\nparams['format'] = 'json'\nparams['partner_id'] = 'openerp_top_1.0'\nparams['timestamp'] = self._get_timestamp()\nparams['method'] = method_name\nparams['session'] = self.session\nparams['sign'] = self._sign(params)\n\n#_logger.info('%s(%s) response send -' % (method_name, params))\ncurl_rsp = self._get_top_resp(self.top_url, params)\nif not curl_rsp: return None\nrsp = json.loads(curl_rsp, strict=False, object_hook =lambda x: _O(x))\nif rsp.has_key('error_response'):\n    error_code = rsp['error_response']['code']\n    if 'sub_msg' in rsp['error_response']:\n        msg = rsp['error_response']['sub_msg']\n    else:\n        msg = rsp['error_response']['msg']\n\n    raise TOPException(error_code, msg)\nelse:\n    rsp = rsp[method_name.replace('.','_')[7:] + '_response']\n    if not rsp: return None\n    _logger.info('%s(%s) response OK -' % (method_name, kwargs))\n    return rsp", "path": "taobao\\taobao_top.py", "repo_name": "buke/openerp-taobao", "stars": 126, "license": "other", "language": "python", "size": 595}
{"docstring": "\"\"\"Return a dict of stats about a given tube.\"\"\"\n", "func_signal": "def stats_tube(self, name):\n", "code": "return self._interact_yaml('stats-tube %s\\r\\n' % name,\n                          ['OK'],\n                          ['NOT_FOUND'])", "path": "taobao\\libs\\beanstalkc.py", "repo_name": "buke/openerp-taobao", "stars": 126, "license": "other", "language": "python", "size": 595}
{"docstring": "\"\"\"\n\u521b\u5efa\u94fe\u63a5 taobao stream \u7ebf\u7a0b\n\"\"\"\n#\u521b\u5efastream\u7ebf\u7a0b\n", "func_signal": "def _create_stream_thread(self, cr, uid, thread_name, shop):\n", "code": "stream_thread_exits = False\nfor thread in threading.enumerate():\n    if thread.getName() == thread_name:\n        stream_thread_exits = True\n        break;\n\nif not stream_thread_exits:\n    # check last discard_info\n    global CHECK_DISCARD_THREAD_START\n    if not CHECK_DISCARD_THREAD_START.get(shop.taobao_app_key, False):\n        threading.Thread(target=self._check_discard_info, args=[cr.dbname, uid, shop.taobao_app_key]).start()\n        CHECK_DISCARD_THREAD_START[shop.taobao_app_key] = True\n\n    #start taobao stream\n    stream_id = ''.join([config['xmlrpc_interface'] or '0.0.0.0', ':', str(config['xmlrpc_port']), '/', thread_name])\n\n    t = threading.Thread(target=TOP(shop.taobao_app_key, shop.taobao_app_secret, shop.taobao_session_key).stream, args=[cr.dbname, uid, stream_id, shop.taobao_user_id, shop.taobao_nick], name=thread_name)\n    t.setDaemon(True)\n    t.start()", "path": "taobao\\taobao_shop.py", "repo_name": "buke/openerp-taobao", "stars": 126, "license": "other", "language": "python", "size": 595}
{"docstring": "\"\"\"Stop watching a given tube.\"\"\"\n", "func_signal": "def ignore(self, name):\n", "code": "try:\n    return int(self._interact_value('ignore %s\\r\\n' % name,\n                                    ['WATCHING'],\n                                    ['NOT_IGNORED']))\nexcept CommandFailed:\n    return 1", "path": "taobao\\libs\\beanstalkc.py", "repo_name": "buke/openerp-taobao", "stars": 126, "license": "other", "language": "python", "size": 595}
{"docstring": "\"\"\"\nFind the most likely assignment to labels given parameters using the\nViterbi algorithm.\n\"\"\"\n", "func_signal": "def slow_predict(self,log_potential,N,K,debug=False):\n", "code": "g0 = log_potential[0,0]\ng  = log_potential[1:]\n\nB = np.ones((N,K), dtype=np.int32) * -1\n# compute max-marginals and backtrace matrix\nV = g0\nfor t in xrange(1,N):\n\tU = np.empty(K)\n\tfor y in xrange(K):\n\t\tw = V + g[t-1,:,y]\n\t\tB[t,y] = b = w.argmax()\n\t\tU[y] = w[b]\n\tV = U\n# extract the best path by brack-tracking\ny = V.argmax()\ntrace = []\nfor t in reversed(xrange(N)):\n\ttrace.append(y)\n\ty = B[t, y]\ntrace.reverse()\nreturn trace", "path": "crf.py", "repo_name": "shawntan/python-crf", "stars": 100, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"Plug in system. Register additional pickling functions if modules already loaded\"\"\"\n", "func_signal": "def inject_addons(self):\n", "code": "self.inject_numpy()\nself.inject_timeseries()\nself.inject_email()", "path": "cloudpickle.py", "repo_name": "shawntan/python-crf", "stars": 100, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\" Registered with the dispatch to handle all function types.\n\nDetermines what kind of function obj is (e.g. lambda, defined at\ninteractive prompt, etc) and handles the pickling appropriately.\n\"\"\"\n", "func_signal": "def save_function(self, obj, name=None, pack=struct.pack):\n", "code": "write = self.write\n\nname = obj.__name__\nmodname = pickle.whichmodule(obj, name)\n#print 'which gives %s %s %s' % (modname, obj, name)\ntry:\n    themodule = sys.modules[modname]\nexcept KeyError: # eval'd items such as namedtuple give invalid items for their function __module__\n    modname = '__main__'\n\nif modname == '__main__':\n    themodule = None\n\nif themodule:\n    self.modules.add(themodule)\n\nif not self.savedDjangoEnv:\n    #hack for django - if we detect the settings module, we transport it\n    django_settings = os.environ.get('DJANGO_SETTINGS_MODULE', '')\n    if django_settings:\n        django_mod = sys.modules.get(django_settings)\n        if django_mod:\n            cloudLog.debug('Transporting django settings %s during save of %s', django_mod, name)\n            self.savedDjangoEnv = True\n            self.modules.add(django_mod)\n            write(pickle.MARK)\n            self.save_reduce(django_settings_load, (django_mod.__name__,), obj=django_mod)\n            write(pickle.POP_MARK)\n\n\n# if func is lambda, def'ed at prompt, is in main, or is nested, then\n# we'll pickle the actual function object rather than simply saving a\n# reference (as is done in default pickler), via save_function_tuple.\nif islambda(obj) or obj.func_code.co_filename == '<stdin>' or themodule == None:\n    #Force server to import modules that have been imported in main\n    modList = None\n    if themodule == None and not self.savedForceImports:\n        mainmod = sys.modules['__main__']\n        if useForcedImports and hasattr(mainmod,'___pyc_forcedImports__'):\n            modList = list(mainmod.___pyc_forcedImports__)\n        self.savedForceImports = True\n    self.save_function_tuple(obj, modList)\n    return\nelse:   # func is nested\n    klass = getattr(themodule, name, None)\n    if klass is None or klass is not obj:\n        self.save_function_tuple(obj, [themodule])\n        return\n\nif obj.__dict__:\n    # essentially save_reduce, but workaround needed to avoid recursion\n    self.save(_restore_attr)\n    write(pickle.MARK + pickle.GLOBAL + modname + '\\n' + name + '\\n')\n    self.memoize(obj)\n    self.save(obj.__dict__)\n    write(pickle.TUPLE + pickle.REDUCE)\nelse:\n    write(pickle.GLOBAL + modname + '\\n' + name + '\\n')\n    self.memoize(obj)", "path": "cloudpickle.py", "repo_name": "shawntan/python-crf", "stars": 100, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"  Pickles an actual func object.\n\nA func comprises: code, globals, defaults, closure, and dict.  We\nextract and save these, injecting reducing functions at certain points\nto recreate the func object.  Keep in mind that some of these pieces\ncan contain a ref to the func itself.  Thus, a naive save on these\npieces could trigger an infinite loop of save's.  To get around that,\nwe first create a skeleton func object using just the code (this is\nsafe, since this won't contain a ref to the func), and memoize it as\nsoon as it's created.  The other stuff can then be filled in later.\n\"\"\"\n", "func_signal": "def save_function_tuple(self, func, forced_imports):\n", "code": "save = self.save\nwrite = self.write\n\n# save the modules (if any)\nif forced_imports:\n    write(pickle.MARK)\n    save(_modules_to_main)\n    #print 'forced imports are', forced_imports\n\n    forced_names = map(lambda m: m.__name__, forced_imports)\n    save((forced_names,))\n\n    #save((forced_imports,))\n    write(pickle.REDUCE)\n    write(pickle.POP_MARK)\n\ncode, f_globals, defaults, closure, dct, base_globals = self.extract_func_data(func)\n\nsave(_fill_function)  # skeleton function updater\nwrite(pickle.MARK)    # beginning of tuple that _fill_function expects\n\n# create a skeleton function object and memoize it\nsave(_make_skel_func)\nsave((code, len(closure), base_globals))\nwrite(pickle.REDUCE)\nself.memoize(func)\n\n# save the rest of the func data needed by _fill_function\nsave(f_globals)\nsave(defaults)\nsave(closure)\nsave(dct)\nwrite(pickle.TUPLE)\nwrite(pickle.REDUCE)  # applies _fill_function on the tuple", "path": "cloudpickle.py", "repo_name": "shawntan/python-crf", "stars": 100, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nSave a module as an import\n\"\"\"\n#print 'try save import', obj.__name__\n", "func_signal": "def save_module(self, obj, pack=struct.pack):\n", "code": "self.modules.add(obj)\nself.save_reduce(subimport,(obj.__name__,), obj=obj)", "path": "cloudpickle.py", "repo_name": "shawntan/python-crf", "stars": 100, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nSave a code object\n\"\"\"\n#print 'try to save codeobj: ', obj\n", "func_signal": "def save_codeobject(self, obj, pack=struct.pack):\n", "code": "args = (\n    obj.co_argcount, obj.co_nlocals, obj.co_stacksize, obj.co_flags, obj.co_code,\n    obj.co_consts, obj.co_names, obj.co_varnames, obj.co_filename, obj.co_name,\n    obj.co_firstlineno, obj.co_lnotab, obj.co_freevars, obj.co_cellvars\n)\nself.save_reduce(types.CodeType, args, obj=obj)", "path": "cloudpickle.py", "repo_name": "shawntan/python-crf", "stars": 100, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nAxes:\n0 - T or time or sequence index\n1 - y' or previous label\n2 - y  or current  label\n3 - f(y',y,x_vec,i) for i s\n\"\"\"\n", "func_signal": "def all_features(self,x_vec):\n", "code": "result = np.zeros((len(x_vec)+1,len(self.labels),len(self.labels),len(self.ft_fun)))\nfor i in range(len(x_vec)+1):\n\tfor j,yp in enumerate(self.labels):\n\t\tfor k,y in enumerate(self.labels):\n\t\t\tfor l,f in enumerate(self.ft_fun):\n\t\t\t\tresult[i,j,k,l] = f(yp,y,x_vec,i)\nreturn result", "path": "crf.py", "repo_name": "shawntan/python-crf", "stars": 100, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"itemgetter serializer (needed for namedtuple support)\na bit of a pain as we need to read ctypes internals\"\"\"\n", "func_signal": "def save_itemgetter(self, obj):\n", "code": "class ItemGetterType(ctypes.Structure):\n    _fields_ = PyObject_HEAD + [\n        ('nitems', ctypes.c_size_t),\n        ('item', ctypes.py_object)\n    ]\n\n\nitemgetter_obj = ctypes.cast(ctypes.c_void_p(id(obj)), ctypes.POINTER(ItemGetterType)).contents\nreturn self.save_reduce(operator.itemgetter, (itemgetter_obj.item,))", "path": "cloudpickle.py", "repo_name": "shawntan/python-crf", "stars": 100, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\" Fills in the rest of function data into the skeleton function object\n    that were created via _make_skel_func().\n     \"\"\"\n", "func_signal": "def _fill_function(func, globals, defaults, closure, dict):\n", "code": "func.func_globals.update(globals)\nfunc.func_defaults = defaults\nfunc.func_dict = dict\n\nif len(closure) != len(func.func_closure):\n    raise pickle.UnpicklingError(\"closure lengths don't match up\")\nfor i in range(len(closure)):\n    _change_cell_value(func.func_closure[i], closure[i])\n\nreturn func", "path": "cloudpickle.py", "repo_name": "shawntan/python-crf", "stars": 100, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"Returns a 3 element tuple describing the xrange start, step, and len\nrespectively\n\nNote: Only guarentees that elements of xrange are the same. parameters may\nbe different.\ne.g. xrange(1,1) is interpretted as xrange(0,0); both behave the same\nthough w/ iteration\n\"\"\"\n\n", "func_signal": "def xrange_params(xrangeobj):\n", "code": "xrange_len = len(xrangeobj)\nif not xrange_len: #empty\n    return (0,1,0)\nstart = xrangeobj[0]\nif xrange_len == 1: #one element\n    return start, 1, 1\nreturn (start, xrangeobj[1] - xrangeobj[0], xrange_len)", "path": "cloudpickle.py", "repo_name": "shawntan/python-crf", "stars": 100, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nFind the most likely assignment to labels given parameters using the\nViterbi algorithm.\n\"\"\"\n", "func_signal": "def argmax(X):\n", "code": "N,K,_ = X.shape\ng0 = X[0,0]\ng  = X[1:]\n\nB = ones((N,K), dtype=int32) * -1\n# compute max-marginals and backtrace matrix\nV = g0\nfor t in xrange(1,N):\n\tU = empty(K)\n\tfor y in xrange(K):\n\t\tw = V + g[t-1,:,y]\n\t\tB[t,y] = b = w.argmax()\n\t\tU[y] = w[b]\n\tV = U\n# extract the best path by brack-tracking\ny = V.argmax()\ntrace = []\nfor t in reversed(xrange(N)):\n\ttrace.append(y)\n\ty = B[t, y]\ntrace.reverse()\nreturn trace", "path": "test.py", "repo_name": "shawntan/python-crf", "stars": 100, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"Handle bugs with pickling scikits timeseries\"\"\"\n", "func_signal": "def inject_timeseries(self):\n", "code": "tseries = sys.modules.get('scikits.timeseries.tseries')\nif not tseries or not hasattr(tseries, 'Timeseries'):\n    return\nself.dispatch[tseries.Timeseries] = self.__class__.save_timeseries", "path": "cloudpickle.py", "repo_name": "shawntan/python-crf", "stars": 100, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"Inner logic to save instance. Based off pickle.save_inst\nSupports __transient__\"\"\"\n", "func_signal": "def save_inst_logic(self, obj):\n", "code": "cls = obj.__class__\n\nmemo  = self.memo\nwrite = self.write\nsave  = self.save\n\nif hasattr(obj, '__getinitargs__'):\n    args = obj.__getinitargs__()\n    len(args) # XXX Assert it's a sequence\n    pickle._keep_alive(args, memo)\nelse:\n    args = ()\n\nwrite(pickle.MARK)\n\nif self.bin:\n    save(cls)\n    for arg in args:\n        save(arg)\n    write(pickle.OBJ)\nelse:\n    for arg in args:\n        save(arg)\n    write(pickle.INST + cls.__module__ + '\\n' + cls.__name__ + '\\n')\n\nself.memoize(obj)\n\ntry:\n    getstate = obj.__getstate__\nexcept AttributeError:\n    stuff = obj.__dict__\n    #remove items if transient\n    if hasattr(obj, '__transient__'):\n        transient = obj.__transient__\n        stuff = stuff.copy()\n        for k in list(stuff.keys()):\n            if k in transient:\n                del stuff[k]\nelse:\n    stuff = getstate()\n    pickle._keep_alive(stuff, memo)\nsave(stuff)\nwrite(pickle.BUILD)", "path": "cloudpickle.py", "repo_name": "shawntan/python-crf", "stars": 100, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"Block email LazyImporters from being saved\"\"\"\n", "func_signal": "def inject_email(self):\n", "code": "email = sys.modules.get('email')\nif not email:\n    return\nself.dispatch[email.LazyImporter] = self.__class__.save_unsupported", "path": "cloudpickle.py", "repo_name": "shawntan/python-crf", "stars": 100, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\" Calculate matrix of backward unnormalized log-probabilities. \"\"\"\n", "func_signal": "def backward(g, N, K):\n", "code": "b = np.zeros((N,K))\nfor t in reversed(xrange(0,N-1)):\n\tby = b[t+1,:]\n\tfor yp in xrange(K):\n\t\tb[t,yp] = misc.logsumexp(by + g[t,yp,:])\nreturn b", "path": "test.py", "repo_name": "shawntan/python-crf", "stars": 100, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\" Creates a skeleton function object that contains just the provided\n    code and the correct number of cells in func_closure.  All other\n    func attributes (e.g. func_globals) are empty.\n\"\"\"\n#build closure (cells):\n", "func_signal": "def _make_skel_func(code, num_closures, base_globals = None):\n", "code": "if not ctypes:\n    raise Exception('ctypes failed to import; cannot build function')\n\ncellnew = ctypes.pythonapi.PyCell_New\ncellnew.restype = ctypes.py_object\ncellnew.argtypes = (ctypes.py_object,)\ndummy_closure = tuple(map(lambda i: cellnew(None), range(num_closures)))\n\nif base_globals is None:\n    base_globals = {}\nbase_globals['__builtins__'] = __builtins__\n\nreturn types.FunctionType(code, base_globals,\n                          None, None, dummy_closure)", "path": "cloudpickle.py", "repo_name": "shawntan/python-crf", "stars": 100, "license": "None", "language": "python", "size": 248}
{"docstring": "#betas = np.NINF*np.ones((M.shape[0],M.shape[1]))\n", "func_signal": "def backward(self,M,end=-1):\n", "code": "betas = np.zeros((M.shape[0],M.shape[1]))\nbeta  = betas[-1]\nbeta[end] = 0\nfor i in reversed(range(M.shape[0]-1)):\n\tbeta = betas[i] = log_dot_mv(M[i+1],beta)\nbeta = log_dot_mv(M[0],beta)\nreturn (betas,beta)", "path": "crf.py", "repo_name": "shawntan/python-crf", "stars": 100, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"Hack function for saving numpy ufunc objects\"\"\"\n", "func_signal": "def save_ufunc(self, obj):\n", "code": "name = obj.__name__\nfor tst_mod_name in self.numpy_tst_mods:\n    tst_mod = sys.modules.get(tst_mod_name, None)\n    if tst_mod:\n        if name in tst_mod.__dict__:\n            self.save_reduce(_getobject, (tst_mod_name, name))\n            return\nraise pickle.PicklingError('cannot save %s. Cannot resolve what module it is defined in' % str(obj))", "path": "cloudpickle.py", "repo_name": "shawntan/python-crf", "stars": 100, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"Generate image from string representation\"\"\"\n", "func_signal": "def _generateImage(size, mode, str_rep):\n", "code": "import Image\ni = Image.new(mode, size)\ni.fromstring(str_rep)\nreturn i", "path": "cloudpickle.py", "repo_name": "shawntan/python-crf", "stars": 100, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nCalculate matrix of forward unnormalized log-probabilities.\n\na[i,y] log of the sum of scores of all sequences from 0 to i where\nthe label at position i is y.\n\"\"\"\n", "func_signal": "def forward(g0, g, N, K):\n", "code": "a = np.zeros((N,K))\na[0,:] = g0\nfor t in xrange(1,N):\n\tayp = a[t-1,:]\n\tfor y in xrange(K):\n\t\ta[t,y] = misc.logsumexp(ayp + g[t-1,:,y])\nreturn a", "path": "test.py", "repo_name": "shawntan/python-crf", "stars": 100, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"Produces a file object from source.\n\nsource can be either a file object, local filename or a string.\n\n\"\"\"\n# Already a file object\n", "func_signal": "def openStream(self, source):\n", "code": "if hasattr(source, 'read'):\n    stream = source\nelse:\n    stream = BytesIO(source)\n\ntry:\n    stream.seek(stream.tell())\nexcept:\n    stream = BufferedStream(stream)\n\nreturn stream", "path": "lib\\html5lib\\inputstream.py", "repo_name": "JinnLynn/alfred-workflows", "stars": 112, "license": "other", "language": "python", "size": 4928}
{"docstring": "\"\"\" Returns a string of characters from the stream up to but not\nincluding any character in 'characters' or EOF. 'characters' must be\na container that supports the 'in' method and iteration over its\ncharacters.\n\"\"\"\n\n# Use a cache of regexps to find the required characters\n", "func_signal": "def charsUntil(self, characters, opposite=False):\n", "code": "try:\n    chars = charsUntilRegEx[(characters, opposite)]\nexcept KeyError:\n    if __debug__:\n        for c in characters:\n            assert(ord(c) < 128)\n    regex = \"\".join([\"\\\\x%02x\" % ord(c) for c in characters])\n    if not opposite:\n        regex = \"^%s\" % regex\n    chars = charsUntilRegEx[(characters, opposite)] = re.compile(\"[%s]+\" % regex)\n\nrv = []\n\nwhile True:\n    # Find the longest matching prefix\n    m = chars.match(self.chunk, self.chunkOffset)\n    if m is None:\n        # If nothing matched, and it wasn't because we ran out of chunk,\n        # then stop\n        if self.chunkOffset != self.chunkSize:\n            break\n    else:\n        end = m.end()\n        # If not the whole chunk matched, return everything\n        # up to the part that didn't match\n        if end != self.chunkSize:\n            rv.append(self.chunk[self.chunkOffset:end])\n            self.chunkOffset = end\n            break\n    # If the whole remainder of the chunk matched,\n    # use it all and read the next chunk\n    rv.append(self.chunk[self.chunkOffset:])\n    if not self.readChunk():\n        # Reached EOF\n        break\n\nr = \"\".join(rv)\nreturn r", "path": "lib\\html5lib\\inputstream.py", "repo_name": "JinnLynn/alfred-workflows", "stars": 112, "license": "other", "language": "python", "size": 4928}
{"docstring": "\"\"\"The new-style print function.\"\"\"\n", "func_signal": "def print_(*args, **kwargs):\n", "code": "fp = kwargs.pop(\"file\", sys.stdout)\nif fp is None:\n    return\ndef write(data):\n    if not isinstance(data, basestring):\n        data = str(data)\n    fp.write(data)\nwant_unicode = False\nsep = kwargs.pop(\"sep\", None)\nif sep is not None:\n    if isinstance(sep, unicode):\n        want_unicode = True\n    elif not isinstance(sep, str):\n        raise TypeError(\"sep must be None or a string\")\nend = kwargs.pop(\"end\", None)\nif end is not None:\n    if isinstance(end, unicode):\n        want_unicode = True\n    elif not isinstance(end, str):\n        raise TypeError(\"end must be None or a string\")\nif kwargs:\n    raise TypeError(\"invalid keyword arguments to print()\")\nif not want_unicode:\n    for arg in args:\n        if isinstance(arg, unicode):\n            want_unicode = True\n            break\nif want_unicode:\n    newline = unicode(\"\\n\")\n    space = unicode(\" \")\nelse:\n    newline = \"\\n\"\n    space = \" \"\nif sep is None:\n    sep = space\nif end is None:\n    end = newline\nfor i, arg in enumerate(args):\n    if i:\n        write(sep)\n    write(arg)\nwrite(end)", "path": "lib\\six\\__init__.py", "repo_name": "JinnLynn/alfred-workflows", "stars": 112, "license": "other", "language": "python", "size": 4928}
{"docstring": "\"\"\"Look for a sequence of bytes at the start of a string. If the bytes\nare found return True and advance the position to the byte after the\nmatch. Otherwise return False and leave the position alone\"\"\"\n", "func_signal": "def matchBytes(self, bytes):\n", "code": "p = self.position\ndata = self[p:p + len(bytes)]\nrv = data.startswith(bytes)\nif rv:\n    self.position += len(bytes)\nreturn rv", "path": "lib\\html5lib\\inputstream.py", "repo_name": "JinnLynn/alfred-workflows", "stars": 112, "license": "other", "language": "python", "size": 4928}
{"docstring": "\"\"\"Import module, returning the module after the last dot.\"\"\"\n", "func_signal": "def _import_module(name):\n", "code": "__import__(name)\nreturn sys.modules[name]", "path": "lib\\six\\__init__.py", "repo_name": "JinnLynn/alfred-workflows", "stars": 112, "license": "other", "language": "python", "size": 4928}
{"docstring": "# We are only interested in <meta> tags\n", "func_signal": "def set_up_substitutions(self, tag):\n", "code": "if tag.name != 'meta':\n    return False\n\nhttp_equiv = tag.get('http-equiv')\ncontent = tag.get('content')\ncharset = tag.get('charset')\n\n# We are interested in <meta> tags that say what encoding the\n# document was originally in. This means HTML 5-style <meta>\n# tags that provide the \"charset\" attribute. It also means\n# HTML 4-style <meta> tags that provide the \"content\"\n# attribute and have \"http-equiv\" set to \"content-type\".\n#\n# In both cases we will replace the value of the appropriate\n# attribute with a standin object that can take on any\n# encoding.\nmeta_encoding = None\nif charset is not None:\n    # HTML 5 style:\n    # <meta charset=\"utf8\">\n    meta_encoding = charset\n    tag['charset'] = CharsetMetaAttributeValue(charset)\n\nelif (content is not None and http_equiv is not None\n      and http_equiv.lower() == 'content-type'):\n    # HTML 4 style:\n    # <meta http-equiv=\"content-type\" content=\"text/html; charset=utf8\">\n    tag['content'] = ContentMetaAttributeValue(content)\n\nreturn (meta_encoding is not None)", "path": "lib\\bs4\\builder\\__init__.py", "repo_name": "JinnLynn/alfred-workflows", "stars": 112, "license": "other", "language": "python", "size": 4928}
{"docstring": "\"\"\"Produces a file object from source.\n\nsource can be either a file object, local filename or a string.\n\n\"\"\"\n# Already a file object\n", "func_signal": "def openStream(self, source):\n", "code": "if hasattr(source, 'read'):\n    stream = source\nelse:\n    stream = StringIO(source)\n\nreturn stream", "path": "lib\\html5lib\\inputstream.py", "repo_name": "JinnLynn/alfred-workflows", "stars": 112, "license": "other", "language": "python", "size": 4928}
{"docstring": "# First look for a BOM\n# This will also read past the BOM if present\n", "func_signal": "def detectEncoding(self, parseMeta=True, chardet=True):\n", "code": "encoding = self.detectBOM()\nconfidence = \"certain\"\n# If there is no BOM need to look for meta elements with encoding\n# information\nif encoding is None and parseMeta:\n    encoding = self.detectEncodingMeta()\n    confidence = \"tentative\"\n# Guess with chardet, if avaliable\nif encoding is None and chardet:\n    confidence = \"tentative\"\n    try:\n        try:\n            from charade.universaldetector import UniversalDetector\n        except ImportError:\n            from chardet.universaldetector import UniversalDetector\n        buffers = []\n        detector = UniversalDetector()\n        while not detector.done:\n            buffer = self.rawStream.read(self.numBytesChardet)\n            assert isinstance(buffer, bytes)\n            if not buffer:\n                break\n            buffers.append(buffer)\n            detector.feed(buffer)\n        detector.close()\n        encoding = detector.result['encoding']\n        self.rawStream.seek(0)\n    except ImportError:\n        pass\n# If all else fails use the default encoding\nif encoding is None:\n    confidence = \"tentative\"\n    encoding = self.defaultEncoding\n\n# Substitute for equivalent encodings:\nencodingSub = {\"iso-8859-1\": \"windows-1252\"}\n\nif encoding.lower() in encodingSub:\n    encoding = encodingSub[encoding.lower()]\n\nreturn encoding, confidence", "path": "lib\\html5lib\\inputstream.py", "repo_name": "JinnLynn/alfred-workflows", "stars": 112, "license": "other", "language": "python", "size": 4928}
{"docstring": "# Only one character is allowed to be ungotten at once - it must\n# be consumed again before any further call to unget\n", "func_signal": "def unget(self, char):\n", "code": "if char is not None:\n    if self.chunkOffset == 0:\n        # unget is called quite rarely, so it's a good idea to do\n        # more work here if it saves a bit of work in the frequently\n        # called char and charsUntil.\n        # So, just prepend the ungotten character onto the current\n        # chunk:\n        self.chunk = char + self.chunk\n        self.chunkSize += 1\n    else:\n        self.chunkOffset -= 1\n        assert self.chunk[self.chunkOffset] == char", "path": "lib\\html5lib\\inputstream.py", "repo_name": "JinnLynn/alfred-workflows", "stars": 112, "license": "other", "language": "python", "size": 4928}
{"docstring": "# Someone picked the wrong compile option\n# You lose\n", "func_signal": "def characterErrorsUCS2(self, data):\n", "code": "skip = False\nfor match in invalid_unicode_re.finditer(data):\n    if skip:\n        continue\n    codepoint = ord(match.group())\n    pos = match.start()\n    # Pretty sure there should be endianness issues here\n    if utils.isSurrogatePair(data[pos:pos + 2]):\n        # We have a surrogate pair!\n        char_val = utils.surrogatePairToCodepoint(data[pos:pos + 2])\n        if char_val in non_bmp_invalid_codepoints:\n            self.errors.append(\"invalid-codepoint\")\n        skip = True\n    elif (codepoint >= 0xD800 and codepoint <= 0xDFFF and\n          pos == len(data) - 1):\n        self.errors.append(\"invalid-codepoint\")\n    else:\n        skip = False\n        self.errors.append(\"invalid-codepoint\")", "path": "lib\\html5lib\\inputstream.py", "repo_name": "JinnLynn/alfred-workflows", "stars": 112, "license": "other", "language": "python", "size": 4928}
{"docstring": "\"\"\"Attempts to detect at BOM at the start of the stream. If\nan encoding can be determined from the BOM return the name of the\nencoding otherwise return None\"\"\"\n", "func_signal": "def detectBOM(self):\n", "code": "bomDict = {\n    codecs.BOM_UTF8: 'utf-8',\n    codecs.BOM_UTF16_LE: 'utf-16-le', codecs.BOM_UTF16_BE: 'utf-16-be',\n    codecs.BOM_UTF32_LE: 'utf-32-le', codecs.BOM_UTF32_BE: 'utf-32-be'\n}\n\n# Go to beginning of file and read in 4 bytes\nstring = self.rawStream.read(4)\nassert isinstance(string, bytes)\n\n# Try detecting the BOM using bytes from the string\nencoding = bomDict.get(string[:3])         # UTF-8\nseek = 3\nif not encoding:\n    # Need to detect UTF-32 before UTF-16\n    encoding = bomDict.get(string)         # UTF-32\n    seek = 4\n    if not encoding:\n        encoding = bomDict.get(string[:2])  # UTF-16\n        seek = 2\n\n# Set the read position past the BOM if one was found, otherwise\n# set it to the start of the stream\nself.rawStream.seek(encoding and seek or 0)\n\nreturn encoding", "path": "lib\\html5lib\\inputstream.py", "repo_name": "JinnLynn/alfred-workflows", "stars": 112, "license": "other", "language": "python", "size": 4928}
{"docstring": "\"\"\"Execute code in a namespace.\"\"\"\n", "func_signal": "def exec_(_code_, _globs_=None, _locs_=None):\n", "code": "if _globs_ is None:\n    frame = sys._getframe(1)\n    _globs_ = frame.f_globals\n    if _locs_ is None:\n        _locs_ = frame.f_locals\n    del frame\nelif _locs_ is None:\n    _locs_ = _globs_\nexec(\"\"\"exec _code_ in _globs_, _locs_\"\"\")", "path": "lib\\six\\__init__.py", "repo_name": "JinnLynn/alfred-workflows", "stars": 112, "license": "other", "language": "python", "size": 4928}
{"docstring": "\"\"\"Returns (line, col) of the current position in the stream.\"\"\"\n", "func_signal": "def position(self):\n", "code": "line, col = self._position(self.chunkOffset)\nreturn (line + 1, col)", "path": "lib\\html5lib\\inputstream.py", "repo_name": "JinnLynn/alfred-workflows", "stars": 112, "license": "other", "language": "python", "size": 4928}
{"docstring": "\"\"\"Replaces class=\"foo bar\" with class=[\"foo\", \"bar\"]\n\nModifies its input in place.\n\"\"\"\n", "func_signal": "def _replace_cdata_list_attribute_values(self, tag_name, attrs):\n", "code": "if not attrs:\n    return attrs\nif self.cdata_list_attributes:\n    universal = self.cdata_list_attributes.get('*', [])\n    tag_specific = self.cdata_list_attributes.get(\n        tag_name.lower(), None)\n    for attr in attrs.keys():\n        if attr in universal or (tag_specific and attr in tag_specific):\n            # We have a \"class\"-type attribute whose string\n            # value is a whitespace-separated list of\n            # values. Split it into a list.\n            value = attrs[attr]\n            if isinstance(value, basestring):\n                values = whitespace_re.split(value)\n            else:\n                # html5lib sometimes calls setAttributes twice\n                # for the same tag when rearranging the parse\n                # tree. On the second call the attribute value\n                # here is already a list.  If this happens,\n                # leave the value alone rather than trying to\n                # split it again.\n                values = value\n            attrs[attr] = values\nreturn attrs", "path": "lib\\bs4\\builder\\__init__.py", "repo_name": "JinnLynn/alfred-workflows", "stars": 112, "license": "other", "language": "python", "size": 4928}
{"docstring": "# \u5ffd\u7565\u6389\u505a\u79cd\u7684\u4efb\u52a1\n", "func_signal": "def parseTasks(self, tasks):\n", "code": "exclude_status = ['seeding']\ntasks = filter(lambda t: t['status'] not in exclude_status, tasks)\n\nhr = lambda s: dslibHumanReadable(s)\nparsed_tasks = []\nfor task in tasks:\n    transfer = task['additional']['transfer']\n    detail = task['additional']['detail']\n    try:\n        progress = float(transfer['size_downloaded']) / float(task['size'])\n    except:\n        progress = 0.0\n    status_desc = task['status'].title()\n    if task['status'] == 'error':\n        status_desc = '{}: {}'.format(status_desc, task['status_extra']['error_detail'])\n    new_task = {\n        'title'                 : task['title'],\n        'id'                    : task['id'],\n        'type'                  : task['type'],\n        'type_desc'             : 'eMule' if task['type']=='emule' else task['type'].upper(),\n        'status'                : task['status'],\n        'status_desc'           : status_desc,\n        'username'              : task['username'],\n        'uri'                   : detail['uri'],\n        'size'                  : util.toInt(task['size']),\n        'size_downloaded'       : util.toInt(transfer['size_downloaded']),\n        'size_uploaded'         : util.toInt(transfer['size_uploaded']),\n        'speed_download'        : util.toInt(transfer['speed_download']),\n        'speed_upload'          : util.toInt(transfer['speed_upload']),\n        'hr_size'               : hr(task['size']),\n        'hr_size_downloaded'    : hr(transfer['size_downloaded']),\n        'hr_size_uploaded'      : hr(transfer['size_uploaded']),\n        'hr_speed_download'     : hr(transfer['speed_download']),\n        'hr_speed_upload'       : hr(transfer['speed_upload']),\n        'progress'              : progress,\n        'progress_percent'      : '{:.2%}'.format(progress)\n    }\n    new_task.update({\n        'desc'  : '{hr_size:10}{hr_size_downloaded:10}{progress_percent:8} DL: {hr_speed_download:8} UL: {hr_speed_upload:8}{type_desc:8}{status_desc:15}'.format(**new_task)\n        })\n    parsed_tasks.append(new_task)\nreturn sorted(parsed_tasks, key=lambda t:t['title'].lower())", "path": "src\\download-station\\ds.py", "repo_name": "JinnLynn/alfred-workflows", "stars": 112, "license": "other", "language": "python", "size": 4928}
{"docstring": "\"\"\"Remove item from six.moves.\"\"\"\n", "func_signal": "def remove_move(name):\n", "code": "try:\n    delattr(_MovedItems, name)\nexcept AttributeError:\n    try:\n        del moves.__dict__[name]\n    except KeyError:\n        raise AttributeError(\"no such move, %r\" % (name,))", "path": "lib\\six\\__init__.py", "repo_name": "JinnLynn/alfred-workflows", "stars": 112, "license": "other", "language": "python", "size": 4928}
{"docstring": "\"\"\"Initialises the HTMLInputStream.\n\nHTMLInputStream(source, [encoding]) -> Normalized stream from source\nfor use by html5lib.\n\nsource can be either a file-object, local filename or a string.\n\nThe optional encoding parameter must be a string that indicates\nthe encoding.  If specified, that encoding will be used,\nregardless of any BOM or later declaration (such as in a meta\nelement)\n\nparseMeta - Look for a <meta> element containing encoding information\n\n\"\"\"\n# Raw Stream - for unicode objects this will encode to utf-8 and set\n#              self.charEncoding as appropriate\n", "func_signal": "def __init__(self, source, encoding=None, parseMeta=True, chardet=True):\n", "code": "self.rawStream = self.openStream(source)\n\nHTMLUnicodeInputStream.__init__(self, self.rawStream)\n\nself.charEncoding = (codecName(encoding), \"certain\")\n\n# Encoding Information\n# Number of bytes to use when looking for a meta element with\n# encoding information\nself.numBytesMeta = 512\n# Number of bytes to use when using detecting encoding using chardet\nself.numBytesChardet = 100\n# Encoding to use if no other information can be found\nself.defaultEncoding = \"windows-1252\"\n\n# Detect encoding iff no explicit \"transport level\" encoding is supplied\nif (self.charEncoding[0] is None):\n    self.charEncoding = self.detectEncoding(parseMeta, chardet)\n\n# Call superclass\nself.reset()", "path": "lib\\html5lib\\inputstream.py", "repo_name": "JinnLynn/alfred-workflows", "stars": 112, "license": "other", "language": "python", "size": 4928}
{"docstring": "\"\"\"Copy TreeBuilders from the given module into this module.\"\"\"\n# I'm fairly sure this is not the best way to do this.\n", "func_signal": "def register_treebuilders_from(module):\n", "code": "this_module = sys.modules['bs4.builder']\nfor name in module.__all__:\n    obj = getattr(module, name)\n\n    if issubclass(obj, TreeBuilder):\n        setattr(this_module, name, obj)\n        this_module.__all__.append(name)\n        # Register the builder while we're at it.\n        this_module.builder_registry.register(obj)", "path": "lib\\bs4\\builder\\__init__.py", "repo_name": "JinnLynn/alfred-workflows", "stars": 112, "license": "other", "language": "python", "size": 4928}
{"docstring": "\"\"\"string - the data to work on for encoding detection\"\"\"\n", "func_signal": "def __init__(self, data):\n", "code": "self.data = EncodingBytes(data)\nself.encoding = None", "path": "lib\\html5lib\\inputstream.py", "repo_name": "JinnLynn/alfred-workflows", "stars": 112, "license": "other", "language": "python", "size": 4928}
{"docstring": "\"\"\"Register a treebuilder based on its advertised features.\"\"\"\n", "func_signal": "def register(self, treebuilder_class):\n", "code": "for feature in treebuilder_class.features:\n    self.builders_for_feature[feature].insert(0, treebuilder_class)\nself.builders.insert(0, treebuilder_class)", "path": "lib\\bs4\\builder\\__init__.py", "repo_name": "JinnLynn/alfred-workflows", "stars": 112, "license": "other", "language": "python", "size": 4928}
{"docstring": "\"\"\"\nCreates a formatted ``HTMLCalendar``. \nArgument ``format`` can be one of ``month``, ``year``, or ``yearpage``\nKeyword arguments are collected and passed into ``HTMLCalendar.formatmonth``, \n``HTMLCalendar.formatyear``, and ``HTMLCalendar.formatyearpage``\n\n    Syntax::\n        \n        {% calendar month [year] [month] %}\n        {% calendar year [year] %}\n        {% calendar yearpage [year] %}\n        \n    Example::\n    \n        {% calendr month 2009 10 %}\n\"\"\"\n", "func_signal": "def calendar(format, *args, **kwargs):\n", "code": "cal = HTMLCalendar(kwargs.pop('firstweekday', 0))\nreturn getattr(cal, 'format%s' % format)(*args, **kwargs)", "path": "native_tags\\contrib\\cal.py", "repo_name": "justquick/django-native-tags", "stars": 87, "license": "bsd-3-clause", "language": "python", "size": 253}
{"docstring": "\"\"\"\nDoes looping while setting a context variable work\n\"\"\"\n", "func_signal": "def test_loops_work(self):\n", "code": "t = \"{% for i in items %}{% add 1 i as roomba %}{{roomba}}{% endfor %}\"\no = render(t, {'items':[1,2,3]})\nself.assertEqual(o, \"234\")", "path": "example_project\\app\\tests.py", "repo_name": "justquick/django-native-tags", "stars": 87, "license": "bsd-3-clause", "language": "python", "size": 253}
{"docstring": "\"\"\"\nApplies reStructuredText conversion to a string, and returns the\nHTML.\n\n\"\"\"\n", "func_signal": "def restructuredtext(text, **kwargs):\n", "code": "from docutils import core\nparts = core.publish_parts(source=text,\n                           writer_name='html4css1',\n                           **kwargs)\nreturn parts['fragment']", "path": "native_tags\\contrib\\_markup.py", "repo_name": "justquick/django-native-tags", "stars": 87, "license": "bsd-3-clause", "language": "python", "size": 253}
{"docstring": "\"\"\"\nTry to resolve the varialbe in a context\nIf ``resolve`` is ``False``, only string variables are returned\n\"\"\"\n", "func_signal": "def lookup(parser, var, context, resolve=True, apply_filters=True):\n", "code": "if resolve:\n    try:\n        return Variable(var).resolve(context)\n    except VariableDoesNotExist:\n        if apply_filters and var.find('|') > -1:\n            return parser.compile_filter(var).resolve(context)\n        return Constant(var)\n    except TypeError:\n        # already resolved\n        return var\nreturn var", "path": "native_tags\\nodes.py", "repo_name": "justquick/django-native-tags", "stars": 87, "license": "bsd-3-clause", "language": "python", "size": 253}
{"docstring": "\"\"\"\nRetrieves a random object from a given model, and stores it in a\ncontext variable.\n\nSyntax::\n\n    {% get_random_object [app_name].[model_name] as [varname] %}\n\nExample::\n\n    {% get_random_object comments.freecomment as random_comment %}\n\n\"\"\"\n", "func_signal": "def get_random_object(model):\n", "code": "try:\n    return _get_model(model)._default_manager.order_by('?')[0]\nexcept IndexError:\n    return ''", "path": "native_tags\\contrib\\generic_content.py", "repo_name": "justquick/django-native-tags", "stars": 87, "license": "bsd-3-clause", "language": "python", "size": 253}
{"docstring": "\"\"\"\nParse an RSS or Atom feed and render a given number of its items\ninto HTML.\n\nIt is **highly** recommended that you use `Django's template\nfragment caching`_ to cache the output of this tag for a\nreasonable amount of time (e.g., one hour); polling a feed too\noften is impolite, wastes bandwidth and may lead to the feed\nprovider banning your IP address.\n\n.. _Django's template fragment caching: http://www.djangoproject.com/documentation/cache/#template-fragment-caching\n\nArguments should be:\n\n1. The URL of the feed to parse.\n\n2. The name of a template to use for rendering the results into HTML.\n\n3. The number of items to render (if not supplied, renders all\n   items in the feed).\n       \nThe template used to render the results will receive two variables:\n\n``items``\n    A list of dictionaries representing feed items, each with\n    'title', 'summary', 'link' and 'date' members.\n\n``feed``\n    The feed itself, for pulling out arbitrary attributes.\n\nRequires the Universal Feed Parser, which can be obtained at\nhttp://feedparser.org/. See `its documentation`_ for details of the\nparsed feed object.\n\n.. _its documentation: http://feedparser.org/docs/\n\nSyntax::\n\n    {% include_feed [feed_url] [num_items] [template_name] %}\n\nExample::\n\n    {% include_feed \"http://www2.ljworld.com/rss/headlines/\" 10 feed_includes/ljworld_headlines.html %}\n\n\"\"\"\n", "func_signal": "def include_feed(feed_url, *args):\n", "code": "feed = feedparser.parse(feed_url)\nitems = []\n\nif len(args) == 2:\n    # num_items, template_name\n    num_items, template_name = args\nelif len(args) == 1:\n    # template_name\n    num_items, template_name = None, args[0]\nelse:\n    raise TemplateSyntaxError(\"'include_feed' tag takes either two or three arguments\")\n    \nnum_items = int(num_items) or len(feed['entries'])\nfor i in range(num_items):\n    pub_date = feed['entries'][i].updated_parsed\n    published = datetime.date(pub_date[0], pub_date[1], pub_date[2])\n    items.append({ 'title': feed['entries'][i].title,\n                   'summary': feed['entries'][i].summary,\n                   'link': feed['entries'][i].link,\n                   'date': published })\nreturn template_name, { 'items': items, 'feed': feed }", "path": "native_tags\\contrib\\feeds.py", "repo_name": "justquick/django-native-tags", "stars": 87, "license": "bsd-3-clause", "language": "python", "size": 253}
{"docstring": "\"\"\"\nReturn a list of the results of applying the function to the items of\nthe argument sequence(s).  \n\nFunctions may be registered with ``native_tags`` \nor can be ``builtins`` or from the ``operator`` module\n\nIf more than one sequence is given, the\nfunction is called with an argument list consisting of the corresponding\nitem of each sequence, substituting None for missing values when not all\nsequences have the same length.  If the function is None, return a list of\nthe items of the sequence (or a list of tuples if more than one sequence).\n\nSyntax::\n\n    {% map [function] [sequence] %}        \n    {% map [function] [item1 item2 ...] %}\n\nFor example::\n\n    {% map sha1 hello world %}\n    \ncalculates::\n    \n    [sha1(hello), sha1(world)]\n\n\"\"\"\n\n", "func_signal": "def do_map(func_name, *sequence):\n", "code": "if len(sequence)==1:\n    sequence = sequence[0]\nreturn map(get_func(func_name, False), sequence)", "path": "native_tags\\contrib\\mapreduce.py", "repo_name": "justquick/django-native-tags", "stars": 87, "license": "bsd-3-clause", "language": "python", "size": 253}
{"docstring": "\"\"\"\nApplies Markdown conversion to a string, and returns the HTML.\n\n\"\"\"\n", "func_signal": "def markdown(text, **kwargs):\n", "code": "import markdown\nreturn markdown.markdown(text, **kwargs)", "path": "native_tags\\contrib\\_markup.py", "repo_name": "justquick/django-native-tags", "stars": 87, "license": "bsd-3-clause", "language": "python", "size": 253}
{"docstring": "\"\"\"\nAdd a function to the registry by name\n\"\"\"\n", "func_signal": "def register(self, bucket, name_or_func, func=None):\n", "code": "assert bucket in self, 'Bucket %s is unknown' % bucket\nif func is None and hasattr(name_or_func, '__name__'):\n    name = name_or_func.__name__\n    func = name_or_func\nelif func:\n    name = name_or_func\nif name in self[bucket]:\n    raise AlreadyRegistered('The function %s is already registered' % name)\n\nself[bucket][name] = func", "path": "native_tags\\registry.py", "repo_name": "justquick/django-native-tags", "stars": 87, "license": "bsd-3-clause", "language": "python", "size": 253}
{"docstring": "\"\"\"\nProcess several nodes inside a single block\nBlock functions take ``context``, ``nodelist`` as first arguments\nIf the second to last argument is ``as``, the rendered result is stored in the context and is named whatever the last argument is.\n\nSyntax::\n\n    {% [block] [var args...] [name=value kwargs...] [as varname] %}\n        ... nodelist ...\n    {% end[block] %}\n\nExamples::\n\n    {% render_block as rendered_output %}\n        {{ request.path }}/blog/{{ blog.slug }}\n    {% endrender_block %}\n\n    {% highlight_block python %}\n        import this\n    {% endhighlight_block %}\n\n\"\"\"\n", "func_signal": "def do_block(parser, token):\n", "code": "name, args, kwargs = get_signature(token, contextable=True)\nkwargs['nodelist'] = parser.parse(('end%s' % name,))\nparser.delete_first_token()\nreturn BlockNode(parser, name, *args, **kwargs)", "path": "native_tags\\nodes.py", "repo_name": "justquick/django-native-tags", "stars": 87, "license": "bsd-3-clause", "language": "python", "size": 253}
{"docstring": "\"\"\"\nApplies text-to-HTML conversion to a string, and returns the\nHTML.\n\n\"\"\"\n", "func_signal": "def __call__(self, text, **kwargs):\n", "code": "if 'filter_name' in kwargs:\n    filter_name = kwargs['filter_name']\n    del kwargs['filter_name']\n    filter_kwargs = {}\nelse:\n    from django.conf import settings\n    filter_name, filter_kwargs = settings.MARKUP_FILTER\nif filter_name is None:\n    return text\nif filter_name not in self._filters:\n    raise ValueError(\"'%s' is not a registered markup filter. Registered filters are: %s.\" % (filter_name,\n                                                                                               ', '.join(self._filters.iterkeys())))\nfilter_func = self._filters[filter_name]\nfilter_kwargs.update(**kwargs)\nreturn filter_func(text, **filter_kwargs)", "path": "native_tags\\contrib\\_markup.py", "repo_name": "justquick/django-native-tags", "stars": 87, "license": "bsd-3-clause", "language": "python", "size": 253}
{"docstring": "\"\"\"\nApply a function of two arguments cumulatively to the items of a sequence,\nfrom left to right, so as to reduce the sequence to a single value.\n\nFunctions may be registered with ``native_tags`` \nor can be ``builtins`` or from the ``operator`` module\n\nSyntax::\n\n    {% reduce [function] [sequence] %}        \n    {% reduce [function] [item1 item2 ...] %}\n\nFor example::\n\n    {% reduce add 1 2 3 4 5 %}\n    \ncalculates::\n\n    ((((1+2)+3)+4)+5) = 15\n\"\"\"\n", "func_signal": "def do_reduce(func_name, *sequence):\n", "code": "if len(sequence)==1:\n    sequence = sequence[0]\nreturn reduce(get_func(func_name), sequence)", "path": "native_tags\\contrib\\mapreduce.py", "repo_name": "justquick/django-native-tags", "stars": 87, "license": "bsd-3-clause", "language": "python", "size": 253}
{"docstring": "\"\"\"\nCompares passed arguments. \nAttached functions should return boolean ``True`` or ``False``.\nIf the attached function returns ``True``, the first node list is rendered.\nIf the attached function returns ``False``, the second optional node list is rendered (part after the ``{% else %}`` statement). \nIf the last argument in the tag is ``negate``, then the opposite node list is rendered (like an ``ifnot`` tag).\n\nSyntax::\n\n    {% if_[comparison] [var args...] [name=value kwargs...] [negate] %}\n        {# first node list in here #}\n    {% else %}\n        {# second optional node list in here #}\n    {% endif_[comparison] %}\n\n\nSupported comparisons are ``match``, ``find``, ``startswith``, ``endswith``,\n``less``, ``less_or_equal``, ``greater`` and ``greater_or_equal`` and many more.\nCheckout the :ref:`contrib-index` for more examples\n\nExamples::\n\n    {% if_less some_object.id 3 %}\n    {{ some_object }} has an id less than 3.\n    {% endif_less %}\n\n    {% if_match request.path '^/$' %}\n    Welcome home\n    {% endif_match %}\n\n\"\"\"\n", "func_signal": "def do_comparison(parser, token):\n", "code": "name, args, kwargs = get_signature(token, comparison=True)\nname = name.replace('if_if', 'if')\nend_tag = 'end' + name\nkwargs['nodelist_true'] = parser.parse(('else', end_tag))\ntoken = parser.next_token()\nif token.contents == 'else':\n    kwargs['nodelist_false'] = parser.parse((end_tag,))\n    parser.delete_first_token()\nelse:\n    kwargs['nodelist_false'] = template.NodeList()\nif name.startswith('if_'):\n    name = name.split('if_')[1]\nreturn ComparisonNode(parser, name, *args, **kwargs)", "path": "native_tags\\nodes.py", "repo_name": "justquick/django-native-tags", "stars": 87, "license": "bsd-3-clause", "language": "python", "size": 253}
{"docstring": "\"\"\"\nGets a unique SHA1 cache key for any call to a native tag.\nUse args and kwargs in hash so that the same arguments use the same key\n\"\"\"\n", "func_signal": "def get_cache_key(bucket, name, args, kwargs):\n", "code": "u = ''.join(map(str, (bucket, name, args, kwargs)))\nreturn 'native_tags.%s' % sha_constructor(u).hexdigest()", "path": "native_tags\\nodes.py", "repo_name": "justquick/django-native-tags", "stars": 87, "license": "bsd-3-clause", "language": "python", "size": 253}
{"docstring": "\"\"\"\nRetrieves the latest ``num`` objects from a given model, in that\nmodel's default ordering, and stores them in a context variable.\nThe optional field argument specifies which field to get_latest_by, otherwise the model's default is used\n\nSyntax::\n\n    {% get_latest_objects [app_name].[model_name] [num] [field] as [varname] %}\n\nExample::\n\n    {% get_latest_objects comments.freecomment 5 submitted_date as latest_comments %}\n\n\"\"\"\n", "func_signal": "def get_latest_objects(model, num, field='?'):\n", "code": "model = _get_model(model)\nif field == '?':\n    field = model._meta.get_latest_by and '-%s' % model._meta.get_latest_by or field\nreturn model._default_manager.order_by(field)[:int(num)]", "path": "native_tags\\contrib\\generic_content.py", "repo_name": "justquick/django-native-tags", "stars": 87, "license": "bsd-3-clause", "language": "python", "size": 253}
{"docstring": "\"\"\"\nRemove the function from the registry by name\n\"\"\"\n", "func_signal": "def unregister(self, bucket, name):\n", "code": "assert bucket in self, 'Bucket %s is unknown' % bucket\nif not name in self[bucket]:\n    raise NotRegistered('The function %s is not registered' % name)\ndel self[bucket][name]", "path": "native_tags\\registry.py", "repo_name": "justquick/django-native-tags", "stars": 87, "license": "bsd-3-clause", "language": "python", "size": 253}
{"docstring": "\"\"\"\nApplies Textile conversion to a string, and returns the HTML.\n\nThis is simply a pass-through to the ``textile`` template filter\nincluded in ``django.contrib.markup``, which works around issues\nPyTextile has with Unicode strings. If you're not using Django but\nwant to use Textile with ``MarkupFormatter``, you'll need to\nsupply your own Textile filter.\n\n\"\"\"\n", "func_signal": "def textile(text, **kwargs):\n", "code": "from django.contrib.markup.templatetags.markup import textile\nreturn textile(text)", "path": "native_tags\\contrib\\_markup.py", "repo_name": "justquick/django-native-tags", "stars": 87, "license": "bsd-3-clause", "language": "python", "size": 253}
{"docstring": "\"\"\"\nGets the signature tuple for any native tag\ncontextable searchs for ``as`` variable to update context\ncomparison if true uses ``negate`` (p) to ``not`` the result (~p)\nreturns (``tag_name``, ``args``, ``kwargs``)\n\"\"\"\n", "func_signal": "def get_signature(token, contextable=False, comparison=False):\n", "code": "bits = split(token.contents)\nargs, kwargs = (), {}\nif comparison and bits[-1] == 'negate':\n    kwargs['negate'] = True\n    bits = bits[:-1]\nif contextable and len(bits) > 2 and bits[-2] == 'as':\n    kwargs['varname'] = bits[-1]\n    bits = bits[:-2]\nkwarg_re = re.compile(r'^([-\\w]+)\\=(.*)$')\nfor bit in bits[1:]:\n    match = kwarg_re.match(bit)\n    if match:\n        kwargs[str(match.group(1))] = force_unicode(match.group(2))\n    else:\n        args += (bit,)\nreturn bits[0], args, kwargs", "path": "native_tags\\nodes.py", "repo_name": "justquick/django-native-tags", "stars": 87, "license": "bsd-3-clause", "language": "python", "size": 253}
{"docstring": "\"\"\"\nPerforms a defined function on the passed arguments.\nNormally this returns the output of the function into the template.\nIf the second to last argument is ``as``, the result of the function is stored in the context and is named whatever the last argument is.\n\nSyntax::\n\n    {% [function] [var args...] [name=value kwargs...] [as varname] %}\n\nExamples::\n\n    {% search '^(\\d{3})$' 800 as match %}\n\n    {% map sha1 hello world %}\n\n\"\"\"\n", "func_signal": "def do_function(parser, token):\n", "code": "name, args, kwargs = get_signature(token, True, True)\nreturn FunctionNode(parser, name, *args, **kwargs)", "path": "native_tags\\nodes.py", "repo_name": "justquick/django-native-tags", "stars": 87, "license": "bsd-3-clause", "language": "python", "size": 253}
{"docstring": "\"\"\"\nRetrieves a specific object from a given model by primary-key\nlookup, and stores it in a context variable.\n\nSyntax::\n\n    {% retrieve_object [app_name].[model_name] [lookup kwargs] as [varname] %}\n\nExample::\n\n    {% retrieve_object flatpages.flatpage pk=12 as my_flat_page %}\n\n\"\"\"\n", "func_signal": "def retrieve_object(model, *args, **kwargs):\n", "code": "if len(args) == 1:\n    kwargs.update({'pk': args[0]})\n_model = _get_model(model)\ntry:\n    return _model._default_manager.get(**kwargs)\nexcept _model.DoesNotExist:\n    return ''", "path": "native_tags\\contrib\\generic_content.py", "repo_name": "justquick/django-native-tags", "stars": 87, "license": "bsd-3-clause", "language": "python", "size": 253}
{"docstring": "# Return True if lock is owned by current_thread.\n# This method is called only if __lock doesn't have _is_owned().\n", "func_signal": "def _is_owned(self):\n", "code": "if self._lock.acquire(False):\n    self._lock.release()\n    return False\nelse:\n    return True", "path": "evergreen\\locks.py", "repo_name": "saghul/evergreen", "stars": 114, "license": "mit", "language": "python", "size": 1212}
{"docstring": "\"\"\"Return the approximate size of the queue (not reliable!).\"\"\"\n", "func_signal": "def qsize(self):\n", "code": "self.mutex.acquire()\nn = self._qsize()\nself.mutex.release()\nreturn n", "path": "evergreen\\queue.py", "repo_name": "saghul/evergreen", "stars": 114, "license": "mit", "language": "python", "size": 1212}
{"docstring": "\"\"\"Remove and return an item from the queue.\n\nIf optional args 'block' is true and 'timeout' is None (the default),\nblock if necessary until an item is available. If 'timeout' is\na positive number, it blocks at most 'timeout' seconds and raises\nthe Empty exception if no item was available within that time.\nOtherwise ('block' is false), return an item if one is immediately\navailable, else raise the Empty exception ('timeout' is ignored\nin that case).\n\"\"\"\n", "func_signal": "def get(self, block=True, timeout=None):\n", "code": "self.not_empty.acquire()\ntry:\n    if not block:\n        if not self._qsize():\n            raise Empty\n    elif timeout is None:\n        while not self._qsize():\n            self.not_empty.wait()\n    elif timeout < 0:\n        raise ValueError(\"'timeout' must be a positive number\")\n    else:\n        if not self._qsize():\n            self.not_empty.wait(timeout)\n            if not self._qsize():\n                raise Empty\n    item = self._get()\n    self.not_full.notify()\n    return item\nfinally:\n    self.not_empty.release()", "path": "evergreen\\queue.py", "repo_name": "saghul/evergreen", "stars": 114, "license": "mit", "language": "python", "size": 1212}
{"docstring": "# An internal error was detected.  The barrier is set to\n# a broken state all parties awakened.\n", "func_signal": "def _break(self):\n", "code": "self._state = -2\nself._cond.notify_all()", "path": "evergreen\\locks.py", "repo_name": "saghul/evergreen", "stars": 114, "license": "mit", "language": "python", "size": 1212}
{"docstring": "\"\"\"Indicate that a formerly enqueued task is complete.\n\nUsed by Queue consumer tasks.  For each get() used to fetch a task,\na subsequent call to task_done() tells the queue that the processing\non the task is complete.\n\nIf a join() is currently blocking, it will resume when all items\nhave been processed (meaning that a task_done() call was received\nfor every item that had been put() into the queue).\n\nRaises a ValueError if called more times than there were items\nplaced in the queue.\n\"\"\"\n", "func_signal": "def task_done(self):\n", "code": "self.all_tasks_done.acquire()\ntry:\n    unfinished = self.unfinished_tasks - 1\n    if unfinished <= 0:\n        if unfinished < 0:\n            raise ValueError('task_done() called too many times')\n        self.all_tasks_done.notify_all()\n    self.unfinished_tasks = unfinished\nfinally:\n    self.all_tasks_done.release()", "path": "evergreen\\queue.py", "repo_name": "saghul/evergreen", "stars": 114, "license": "mit", "language": "python", "size": 1212}
{"docstring": "\"\"\"Put an item into the queue.\n\nIf optional args 'block' is true and 'timeout' is None (the default),\nblock if necessary until a free slot is available. If 'timeout' is\na positive number, it blocks at most 'timeout' seconds and raises\nthe Full exception if no free slot was available within that time.\nOtherwise ('block' is false), put an item on the queue if a free slot\nis immediately available, else raise the Full exception ('timeout'\nis ignored in that case).\n\"\"\"\n", "func_signal": "def put(self, item, block=True, timeout=None):\n", "code": "self.not_full.acquire()\ntry:\n    if self.maxsize > 0:\n        if not block:\n            if self._qsize() >= self.maxsize:\n                raise Full\n        elif timeout is None:\n            while self._qsize() >= self.maxsize:\n                self.not_full.wait()\n        elif timeout < 0:\n            raise ValueError(\"'timeout' must be a positive number\")\n        else:\n            if self._qsize() >= self.maxsize:\n                self.not_full.wait(timeout)\n                if self._qsize() >= self.maxsize:\n                    raise Full\n    self._put(item)\n    self.unfinished_tasks += 1\n    self.not_empty.notify()\nfinally:\n    self.not_full.release()", "path": "evergreen\\queue.py", "repo_name": "saghul/evergreen", "stars": 114, "license": "mit", "language": "python", "size": 1212}
{"docstring": "\"\"\"If the timeout is pending, cancel it.  If not using\nTimeouts in ``with`` statements, always call cancel() in a\n``finally`` after the block of code that is getting timed out.\nIf not canceled, the timeout will be raised later on, in some\nunexpected section of the application.\"\"\"\n", "func_signal": "def cancel(self):\n", "code": "if self._timer is not None:\n    self._timer.cancel()\n    self._timer = None", "path": "evergreen\\timeout.py", "repo_name": "saghul/evergreen", "stars": 114, "license": "mit", "language": "python", "size": 1212}
{"docstring": "\"\"\"Schedule the timeout.  This is called on construction, so\nit should not be called explicitly, unless the timer has been\ncanceled.\"\"\"\n", "func_signal": "def start(self):\n", "code": "assert not self._timer, '%r is already started; to restart it, cancel it first' % self\nloop = evergreen.current.loop\ncurrent = evergreen.current.task\nif self.seconds is None or self.seconds < 0:\n    # \"fake\" timeout (never expires)\n    self._timer = None\nelif self.exception is None or isinstance(self.exception, bool):\n    # timeout that raises self\n    self._timer = loop.call_later(self.seconds, self._timer_cb, current.throw, self)\nelse:\n    # regular timeout with user-provided exception\n    self._timer = loop.call_later(self.seconds, self._timer_cb, current.throw, self.exception)", "path": "evergreen\\timeout.py", "repo_name": "saghul/evergreen", "stars": 114, "license": "mit", "language": "python", "size": 1212}
{"docstring": "\"\"\" This returns an unpatched version of a module.\"\"\"\n# note that it's not necessary to temporarily install unpatched\n# versions of all patchable modules during the import of the\n# module; this is because none of them import each other, except\n# for threading which imports thread\n", "func_signal": "def original(modname):\n", "code": "original_name = '__original_module_' + modname\nif original_name in sys.modules:\n    return sys.modules.get(original_name)\n\n# re-import the \"pure\" module and store it in the global _originals\n# dict; be sure to restore whatever module had that name already\nsaver = SysModulesSaver((modname,))\nsys.modules.pop(modname, None)\ntry:\n    real_mod = __import__(modname, {}, {}, modname.split('.')[:-1])\n    # save a reference to the unpatched module so it doesn't get lost\n    sys.modules[original_name] = real_mod\nfinally:\n    saver.restore()\n\nreturn sys.modules[original_name]", "path": "evergreen\\patcher.py", "repo_name": "saghul/evergreen", "stars": 114, "license": "mit", "language": "python", "size": 1212}
{"docstring": "\"\"\"Blocks until all items in the Queue have been gotten and processed.\n\nThe count of unfinished tasks goes up whenever an item is added to the\nqueue. The count goes down whenever a consumer task calls task_done()\nto indicate the item was retrieved and all work on it is complete.\n\nWhen the count of unfinished tasks drops to zero, join() unblocks.\n\"\"\"\n", "func_signal": "def join(self):\n", "code": "self.all_tasks_done.acquire()\ntry:\n    while self.unfinished_tasks:\n        self.all_tasks_done.wait()\nfinally:\n    self.all_tasks_done.release()", "path": "evergreen\\queue.py", "repo_name": "saghul/evergreen", "stars": 114, "license": "mit", "language": "python", "size": 1212}
{"docstring": "\"\"\"Return the number of threads currently waiting at the barrier.\"\"\"\n# We don't need synchronization here since this is an ephemeral result\n# anyway.  It returns the correct value in the steady state.\n", "func_signal": "def n_waiting(self):\n", "code": "if self._state == 0:\n    return self._count\nreturn 0", "path": "evergreen\\locks.py", "repo_name": "saghul/evergreen", "stars": 114, "license": "mit", "language": "python", "size": 1212}
{"docstring": "\"\"\"Globally patches certain system modules to be 'cooperaive'.\n\nThe keyword arguments afford some control over which modules are patched.\nIf no keyword arguments are supplied, all possible modules are patched.\nIf keywords are set to True, only the specified modules are patched.  E.g.,\n``monkey_patch(socket=True, select=True)`` patches only the select and\nsocket modules.  Most arguments patch the single module of the same name\n(os, time, select).  The exception is socket, which also patches the ssl\nmodule if present.\n\nIt's safe to call monkey_patch multiple times.\n\"\"\"\n", "func_signal": "def patch(**on):\n", "code": "accepted_args = set(('select', 'socket', 'time'))\ndefault_on = on.pop(\"all\", None)\nfor k in on.keys():\n    if k not in accepted_args:\n        raise TypeError(\"patch() got an unexpected keyword argument %r\" % k)\nif default_on is None:\n    default_on = not (True in list(on.values()))\nfor modname in accepted_args:\n    on.setdefault(modname, default_on)\n\nmodules_to_patch = []\nif on['select'] and not already_patched.get('select'):\n    modules_to_patch += _select_modules()\n    already_patched['select'] = True\nif on['socket'] and not already_patched.get('socket'):\n    modules_to_patch += _socket_modules()\n    already_patched['socket'] = True\nif on['time'] and not already_patched.get('time'):\n    modules_to_patch += _time_modules()\n    already_patched['time'] = True\n\nimp.acquire_lock()\ntry:\n    for name, mod in modules_to_patch:\n        orig_mod = sys.modules.get(name)\n        if orig_mod is None:\n            orig_mod = __import__(name)\n        for attr_name in mod.__patched__:\n            patched_attr = getattr(mod, attr_name, None)\n            if patched_attr is not None:\n                setattr(orig_mod, attr_name, patched_attr)\nfinally:\n    imp.release_lock()", "path": "evergreen\\patcher.py", "repo_name": "saghul/evergreen", "stars": 114, "license": "mit", "language": "python", "size": 1212}
{"docstring": "\"\"\"Return True if the queue is full, False otherwise (not reliable!).\n\nThis method is likely to be removed at some point.  Use qsize() >= n\nas a direct substitute, but be aware that either approach risks a race\ncondition where a queue can shrink before the result of full() or\nqsize() can be used.\n\n\"\"\"\n", "func_signal": "def full(self):\n", "code": "self.mutex.acquire()\nn = 0 < self.maxsize <= self._qsize()\nself.mutex.release()\nreturn n", "path": "evergreen\\queue.py", "repo_name": "saghul/evergreen", "stars": 114, "license": "mit", "language": "python", "size": 1212}
{"docstring": "\"\"\"Place the barrier into a 'broken' state.\n\nUseful in case of error.  Any currently waiting threads and threads\nattempting to 'wait()' will have BrokenBarrierError raised.\n\n\"\"\"\n", "func_signal": "def abort(self):\n", "code": "with self._cond:\n    self._break()", "path": "evergreen\\locks.py", "repo_name": "saghul/evergreen", "stars": 114, "license": "mit", "language": "python", "size": 1212}
{"docstring": "\"\"\"Reset the barrier to the initial state.\n\nAny threads currently waiting will get the BrokenBarrier exception\nraised.\n\n\"\"\"\n", "func_signal": "def reset(self):\n", "code": "with self._cond:\n    if self._count > 0:\n        if self._state == 0:\n            #reset the barrier, waking up threads\n            self._state = -1\n        elif self._state == -2:\n            #was broken, set it to reset state\n            #which clears when the last thread exits\n            self._state = -1\n    else:\n        self._state = 0\n    self._cond.notify_all()", "path": "evergreen\\locks.py", "repo_name": "saghul/evergreen", "stars": 114, "license": "mit", "language": "python", "size": 1212}
{"docstring": "\"\"\"Create a barrier, initialised to 'parties' threads.\n\n'action' is a callable which, when supplied, will be called by one of\nthe threads after they have all entered the barrier and just prior to\nreleasing them all. If a 'timeout' is provided, it is uses as the\ndefault for all subsequent 'wait()' calls.\n\n\"\"\"\n", "func_signal": "def __init__(self, parties, action=None, timeout=None):\n", "code": "self._cond = Condition(Lock())\nself._action = action\nself._timeout = timeout\nself._parties = parties\nself._state = 0   # 0 filling, 1, draining, -1 resetting, -2 broken\nself._count = 0", "path": "evergreen\\locks.py", "repo_name": "saghul/evergreen", "stars": 114, "license": "mit", "language": "python", "size": 1212}
{"docstring": "\"\"\"Saves the named modules to the object.\"\"\"\n", "func_signal": "def save(self, *module_names):\n", "code": "for modname in module_names:\n    self._saved[modname] = sys.modules.get(modname, None)", "path": "evergreen\\patcher.py", "repo_name": "saghul/evergreen", "stars": 114, "license": "mit", "language": "python", "size": 1212}
{"docstring": "\"\"\"Return True if the queue is empty, False otherwise (not reliable!).\n\nThis method is likely to be removed at some point.  Use qsize() == 0\nas a direct substitute, but be aware that either approach risks a race\ncondition where a queue can grow before the result of empty() or\nqsize() can be used.\n\nTo create code that needs to wait for all queued tasks to be\ncompleted, the preferred technique is to use the join() method.\n\n\"\"\"\n", "func_signal": "def empty(self):\n", "code": "self.mutex.acquire()\nn = not self._qsize()\nself.mutex.release()\nreturn n", "path": "evergreen\\queue.py", "repo_name": "saghul/evergreen", "stars": 114, "license": "mit", "language": "python", "size": 1212}
{"docstring": "\"\"\"Copy properties from *source* (assumed to be a module) to\n*destination* (assumed to be a dict).\n\n*ignore* lists properties that should not be thusly copied.\n*srckeys* is a list of keys to copy, if the source's __all__ is\nuntrustworthy.\n\"\"\"\n", "func_signal": "def slurp_properties(source, destination, ignore=[], srckeys=None):\n", "code": "if srckeys is None:\n    srckeys = source.__all__\ndestination.update(dict([(name, getattr(source, name))\n                          for name in srckeys\n                            if not (name.startswith('__') or name in ignore)\n                        ]))", "path": "evergreen\\patcher.py", "repo_name": "saghul/evergreen", "stars": 114, "license": "mit", "language": "python", "size": 1212}
{"docstring": "\"\"\"Wait for the barrier.\n\nWhen the specified number of threads have started waiting, they are all\nsimultaneously awoken. If an 'action' was provided for the barrier, one\nof the threads will have executed that callback prior to returning.\nReturns an individual index number from 0 to 'parties-1'.\n\n\"\"\"\n", "func_signal": "def wait(self, timeout=None):\n", "code": "if timeout is None:\n    timeout = self._timeout\nwith self._cond:\n    self._enter()    # Block while the barrier drains.\n    index = self._count\n    self._count += 1\n    try:\n        if index + 1 == self._parties:\n            # We release the barrier\n            self._release()\n        else:\n            # We wait until someone releases us\n            self._wait(timeout)\n        return index\n    finally:\n        self._count -= 1\n        # Wake up any threads waiting for barrier to drain.\n        self._exit()", "path": "evergreen\\locks.py", "repo_name": "saghul/evergreen", "stars": 114, "license": "mit", "language": "python", "size": 1212}
{"docstring": "\"\"\"\nCreates a new string using @format to highlight matching substrings\nof @other in @main.\n\nReturns: a formatted str\n\n>>> formatCommonSubstrings('hi there dude', 'hi dude')\n'<b>hi </b>there <b>dude</b>'\n\"\"\"\n", "func_signal": "def formatCommonSubstrings(main, other, format = '<b>%s</b>'):\n", "code": "length = 0\nresult = ''\nmatch_pos = last_main_cut = 0\nlower_main = main.lower()\nother = other.lower()\n\nfor pos in range(len(other)):\n    matchedTermination = False\n    for length in range(1, 1 + len(other) - pos + 1):\n        tmp_match_pos  = _index(lower_main, other[pos:pos + length])\n        if tmp_match_pos < 0:\n            length -= 1\n            matchedTermination = False\n            break\n        else:\n            matchedTermination = True\n            match_pos = tmp_match_pos\n    if matchedTermination:\n        length -= 1\n    if 0 < length:\n        # There is a match starting at match_pos with positive length\n        skipped = main[last_main_cut:match_pos - last_main_cut]\n        matched = main[match_pos:match_pos + length]\n        if len(skipped) + len(matched) < len(main):\n            remainder = formatCommonSubstrings(\n                main[match_pos + length:],\n                other[pos + length:],\n                format)\n        else:\n            remainder = ''\n        result = '%s%s%s' % (skipped, format % matched, remainder)\n        break\n\nif result == '':\n    # No matches\n    result = main\n\nreturn result", "path": "legacy-plugins\\gotofile\\relevance.py", "repo_name": "ivyl/gedit-mate", "stars": 99, "license": "None", "language": "python", "size": 876}
{"docstring": "# create the modal\n", "func_signal": "def _add_panel(self):\n", "code": "self._store = Gtk.TreeStore(GObject.TYPE_STRING,    # icon\n                            GObject.TYPE_STRING,    # name\n                            GObject.TYPE_STRING,    # uri\n                            GObject.TYPE_INT)       # editable \n\n# create the treeview\nself._treeview = Gtk.TreeView.new_with_model(self._store)   \ncolumn = Gtk.TreeViewColumn(\"Favorite\")\ncell = Gtk.CellRendererPixbuf()\ncolumn.pack_start(cell, False)\ncolumn.add_attribute(cell, \"stock-id\", 0)\ncell = Gtk.CellRendererText()\nself._edit_cell = cell\ncell.connect(\"edited\", self.on_cell_edited)\ncolumn.pack_start(cell, True)\ncolumn.add_attribute(cell, \"text\", 1)\ncolumn.add_attribute(cell, \"editable\", 3)\nself._treeview.append_column(column)\nself._treeview.set_tooltip_column(2)\nself._treeview.set_headers_visible(False)\n\nscrolled = Gtk.ScrolledWindow()\nscrolled.set_policy(Gtk.PolicyType.AUTOMATIC, Gtk.PolicyType.AUTOMATIC)\nscrolled.add(self._treeview)\n\nself._panel_widget = Gtk.VBox(homogeneous=False, spacing=2)\nself._panel_widget.pack_start(scrolled, True, True, 0)\nself._panel_widget.show_all()\nself._create_popup_menu()\n\n# add the panel\nfilename = os.path.join(ICON_DIR, 'gedit-favorites.png')\nicon = Gtk.Image.new_from_file(filename)\npanel = self.window.get_side_panel()\npanel.add_item(self._panel_widget, \"FavoritesPlugin\", \"Favorites\", icon)\n\n# create popup\n\n\n# drag and drop not working in GTK+ 3.0. A patch has been committed.\nself._treeview.set_reorderable(True) \n\"\"\"\ntargets = [('MY_TREE_MODEL_ROW', Gtk.TargetFlags.SAME_WIDGET, 0),]\nself._treeview.enable_model_drag_source(Gdk.ModifierType.BUTTON1_MASK, \n                                        targets,\n                                        Gdk.DragAction.DEFAULT |\n                                        Gdk.DragAction.MOVE)\n\"\"\"\n# connect signals\nself._treeview.connect(\"row-activated\", self.on_row_activated)\nself._treeview.connect(\"button-press-event\", self.on_button_press_event)", "path": "plugins\\favorites\\plugin.py", "repo_name": "ivyl/gedit-mate", "stars": 99, "license": "None", "language": "python", "size": 876}
{"docstring": "\"\"\" Activate plugin. \"\"\"\n", "func_signal": "def do_activate(self):\n", "code": "self._add_panel()\nself._add_ui() \nself.do_update_state()\nself.load_from_xml()", "path": "plugins\\favorites\\plugin.py", "repo_name": "ivyl/gedit-mate", "stars": 99, "license": "None", "language": "python", "size": 876}
{"docstring": "\"\"\" Recursive function to add elements from the XML to the treeview. \"\"\"\n", "func_signal": "def _load_element(self, parent_iter, element):\n", "code": "if element.tag == \"folder\":\n    new_iter = self._add_favorites_folder(parent_iter, element.attrib['name'])\n    for subelement in element:\n        self._load_element(new_iter, subelement)\nelif element.tag == \"uri\":\n    self._add_favorites_uri(parent_iter, element.text)", "path": "plugins\\favorites\\plugin.py", "repo_name": "ivyl/gedit-mate", "stars": 99, "license": "None", "language": "python", "size": 876}
{"docstring": "\"\"\" Add new parsers here. \"\"\"\n", "func_signal": "def register_parsers(self, window):\n", "code": "self.tabwatch.defaultparser = CTagsParser()\nself.tabwatch.register_parser(\"Python\",PythonParser(window))\nself.tabwatch.register_parser(\"Ruby\",RubyParser())\nself.tabwatch.register_parser(\"Diff\",DiffParser())\n\nhtmlParser = geditHTMLParser()\nself.tabwatch.register_parser(\"HTML\",htmlParser)\nself.tabwatch.register_parser(\"XML\",htmlParser)", "path": "legacy-plugins\\classbrowser\\__init__.py", "repo_name": "ivyl/gedit-mate", "stars": 99, "license": "None", "language": "python", "size": 876}
{"docstring": "\"\"\" Create a new untitled folder. \"\"\"\n", "func_signal": "def on_new_folder_activate(self, action, data=None):\n", "code": "selection = self._treeview.get_selection()\nmodel, tree_iter = selection.get_selected()\nif tree_iter:\n    uri = model.get_value(tree_iter, 2)\n    if uri is not None:\n        parent_iter = model.iter_parent(tree_iter)\n    else:\n        parent_iter = tree_iter\nelse:\n    parent_iter = None\nnew_iter = self._add_favorites_folder(parent_iter, \"Untitled\")\nself._begin_edit_at_iter(new_iter)", "path": "plugins\\favorites\\plugin.py", "repo_name": "ivyl/gedit-mate", "stars": 99, "license": "None", "language": "python", "size": 876}
{"docstring": "\"\"\" Deactivate plugin. \"\"\"\n", "func_signal": "def do_deactivate(self):\n", "code": "self._remove_panel()\nself._remove_ui()\nself._save_to_xml()", "path": "plugins\\favorites\\plugin.py", "repo_name": "ivyl/gedit-mate", "stars": 99, "license": "None", "language": "python", "size": 876}
{"docstring": "\"\"\" \nAdd a new favorite URI to the treeview under parent_iter. If the URI\nalready exists, it will simply be selected.\n\"\"\"\n", "func_signal": "def _add_favorites_uri(self, parent_iter, uri):\n", "code": "exists = self._select_uri_in_treeview(self._store.get_iter_first(), uri)\nif not exists:\n    name = os.path.basename(uri)\n    self._store.append(parent_iter, (self.FAVORITE_ICON, name, uri, 0))", "path": "plugins\\favorites\\plugin.py", "repo_name": "ivyl/gedit-mate", "stars": 99, "license": "None", "language": "python", "size": 876}
{"docstring": "\"\"\" Recursively find URI in treeview and select it or return False. \"\"\"\n", "func_signal": "def _select_uri_in_treeview(self, tree_iter, uri):\n", "code": "model = self._store\nwhile tree_iter:\n    row_uri = model.get_value(tree_iter, 2)\n    if row_uri:\n        if row_uri == uri:\n            path = model.get_path(tree_iter)\n            self._treeview.expand_to_path(path)\n            self._treeview.set_cursor(path, None, False)\n            return True\n    if model.iter_has_child(tree_iter):\n        child_iter = model.iter_children(tree_iter)\n        exists = self._select_uri_in_treeview(child_iter, uri)\n        if exists:\n            return exists\n    tree_iter = model.iter_next(tree_iter)\nreturn False", "path": "plugins\\favorites\\plugin.py", "repo_name": "ivyl/gedit-mate", "stars": 99, "license": "None", "language": "python", "size": 876}
{"docstring": "\"\"\"\nKeyword arguments:\n\t    on_activated -- Callback for when a row is activated.\n\"\"\"\n", "func_signal": "def __init__(self, on_activated):\n", "code": "super(output_pane, self).__init__()\n\t\t\nself.set_policy(gtk.POLICY_NEVER, gtk.POLICY_AUTOMATIC);\nself.set_shadow_type(gtk.SHADOW_IN)\n\nself._box = _output_box(on_activated)\n\t\t\nself.add_with_viewport(self._box)\nself._box.show()\n\nself.target_uri = None", "path": "legacy-plugins\\html-tidy\\output_pane.py", "repo_name": "ivyl/gedit-mate", "stars": 99, "license": "None", "language": "python", "size": 876}
{"docstring": "\"\"\"\nFinds the shortest substring of @s that contains all characters of query\nin order.\n\n@s: a str to search\n@query: a str query to search for\n\nReturns: a two-item tuple containing the start and end indicies of\n         the match.  No match returns (-1,-1).\n\"\"\"\n", "func_signal": "def _findBestMatch(s, query):\n", "code": "if len(query) == 0:\n    return 0, 0\n\nindex = -1\nbestMatch = -1, -1\n\n# Find the last instance of the last character of the query\n# since we never need to search beyond that\nlastChar = len(s) - 1\nwhile lastChar >= 0 and s[lastChar] != query[-1]:\n    lastChar -= 1\n\n# No instance of the character?\nif lastChar == -1:\n    return bestMatch\n\n# Loop through each instance of the first character in query\nindex = _index(s, query[0], index + 1, lastChar - index)\nwhile index >= 0:\n    # Is there room for a match?\n    if index > (lastChar + 1 - len(query)):\n        break\n    \n    # Look for the best match in the tail\n    # We know the first char matches, so we dont check it.\n    cur = index + 1\n    qcur = 1\n    while (qcur < len(query)) and (cur < len(s)):\n        if query[qcur] == s[cur]:\n            qcur += 1\n        cur += 1\n    \n    if ((qcur == len(query)) \\\n    and (((cur - index) < (bestMatch[1] - bestMatch[0])) \\\n    or (bestMatch[0] == -1))):\n        bestMatch = (index, cur)\n    \n    if index == (len(s) - 1):\n        break\n    \n    index = _index(s, query[0], index + 1, lastChar - index)\n    \nreturn bestMatch", "path": "legacy-plugins\\gotofile\\relevance.py", "repo_name": "ivyl/gedit-mate", "stars": 99, "license": "None", "language": "python", "size": 876}
{"docstring": "\"\"\" Load the favorites into the treeview from an XML file. \"\"\"\n", "func_signal": "def load_from_xml(self):\n", "code": "self._store.clear()\nfilename = os.path.join(DATA_DIR, \"favorites.xml\")\nxml = ET.parse(filename)\nroot = xml.getroot()\nfor element in root:\n    self._load_element(None, element)", "path": "plugins\\favorites\\plugin.py", "repo_name": "ivyl/gedit-mate", "stars": 99, "license": "None", "language": "python", "size": 876}
{"docstring": "\"\"\" Create the popup menu used by the treeview. \"\"\"\n", "func_signal": "def _create_popup_menu(self):\n", "code": "manager = Gtk.UIManager()\nself._popup_actions = Gtk.ActionGroup(\"TreeGlobalActions\")\nself._popup_actions.add_actions([\n    ('NewFolder', Gtk.STOCK_DIRECTORY, \"New _Folder\", \n        None, \"Add a folder.\", \n        self.on_new_folder_activate),\n    ('Open', Gtk.STOCK_OPEN, \"_Open\", \n        None, \"Open document.\", \n        self.on_open_activate),\n    ('Rename', None, \"_Rename\", \n        None, \"Rename the item.\", \n        self.on_rename_activate),\n    ('Remove', Gtk.STOCK_REMOVE, \"Re_move\", \n        None, \"Remove the item.\", \n        self.on_remove_activate),\n])\nmanager.insert_action_group(self._popup_actions)\nui_file = os.path.join(DATA_DIR, 'popmenu.ui')\nmanager.add_ui_from_file(ui_file)\nself._popup = manager.get_widget(\"/FavoritesPopup\")", "path": "plugins\\favorites\\plugin.py", "repo_name": "ivyl/gedit-mate", "stars": 99, "license": "None", "language": "python", "size": 876}
{"docstring": "\"\"\"\nClear all rows.\n\"\"\"\n", "func_signal": "def clear(self):\n", "code": "self._box.clear()\n\nself.target_uri = None", "path": "legacy-plugins\\html-tidy\\output_pane.py", "repo_name": "ivyl/gedit-mate", "stars": 99, "license": "None", "language": "python", "size": 876}
{"docstring": "\"\"\" Update UI to reflect current state. \"\"\"\n", "func_signal": "def do_update_state(self):\n", "code": "if self.window.get_active_document():\n    self._file_actions.set_sensitive(True)\nelse:\n    self._file_actions.set_sensitive(False)", "path": "plugins\\favorites\\plugin.py", "repo_name": "ivyl/gedit-mate", "stars": 99, "license": "None", "language": "python", "size": 876}
{"docstring": "\"\"\"\nLooks for the index of @char in @s starting at @index for count bytes.\n\nReturns: int containing the offset of @char.  -1 if @char is not found.\n\n>>> _index('hi', 'i', 0, 2)\n1\n\"\"\"\n", "func_signal": "def _index(s, char, index = 0, count = -1):\n", "code": "if count >= 0:\n    s = s[index:index + count]\nelse:\n    s = s[index:]\n\ntry:\n    return index + s.index(char)\nexcept ValueError:\n    return -1", "path": "legacy-plugins\\gotofile\\relevance.py", "repo_name": "ivyl/gedit-mate", "stars": 99, "license": "None", "language": "python", "size": 876}
{"docstring": "\"\"\" Removes the side panel \"\"\"\n", "func_signal": "def _remove_panel(self):\n", "code": "if self._panel_widget:\n    panel = self.window.get_side_panel()\n    panel.remove_item(self._panel_widget)", "path": "plugins\\favorites\\plugin.py", "repo_name": "ivyl/gedit-mate", "stars": 99, "license": "None", "language": "python", "size": 876}
{"docstring": "\"\"\" Recursively open all URIs under tree_iter. \"\"\"\n", "func_signal": "def _open_uris_at_iter(self, tree_iter):\n", "code": "model = self._store\nwhile tree_iter:\n    uri = model.get_value(tree_iter, 2)\n    if uri:\n        self._open_uri(uri)\n    if model.iter_has_child(tree_iter):\n        child_iter = model.iter_children(tree_iter)\n        self._open_uris_at_iter(child_iter)\n    tree_iter = model.iter_next(tree_iter)", "path": "plugins\\favorites\\plugin.py", "repo_name": "ivyl/gedit-mate", "stars": 99, "license": "None", "language": "python", "size": 876}
{"docstring": "\"\"\" Remove the 'Project' menu from the Gedit menubar. \"\"\"\n", "func_signal": "def _remove_ui(self):\n", "code": "manager = self.window.get_ui_manager()\nmanager.remove_ui(self._ui_merge_id)\nmanager.remove_action_group(self._file_actions)\nmanager.ensure_update()", "path": "plugins\\favorites\\plugin.py", "repo_name": "ivyl/gedit-mate", "stars": 99, "license": "None", "language": "python", "size": 876}
{"docstring": "\"\"\" Merge the 'Project' menu into the Gedit menubar. \"\"\"\n", "func_signal": "def _add_ui(self):\n", "code": "ui_file = os.path.join(DATA_DIR, 'menu.ui')\nmanager = self.window.get_ui_manager()\n\nself._file_actions = Gtk.ActionGroup(\"FavoritesFile\")\nself._file_actions.add_actions([\n    ('AddToFavorites', self.FOLDER_ICON, \"A_dd to Favorites\", \n        None, \"Add document to favorites.\", \n        self.on_add_to_favorites_activate),\n])\nself._file_actions.set_sensitive(False)\nmanager.insert_action_group(self._file_actions)       \n\nself._ui_merge_id = manager.add_ui_from_file(ui_file)\nmanager.ensure_update()", "path": "plugins\\favorites\\plugin.py", "repo_name": "ivyl/gedit-mate", "stars": 99, "license": "None", "language": "python", "size": 876}
{"docstring": "\"\"\"\nSet offset and count into result set, and optionally set max-matches and cutoff limits.\n\"\"\"\n", "func_signal": "def SetLimits (self, offset, limit, maxmatches=0, cutoff=0):\n", "code": "assert ( type(offset) in [int,long] and 0<=offset<16777216 )\nassert ( type(limit) in [int,long] and 0<limit<16777216 )\nassert(maxmatches>=0)\nself._offset = offset\nself._limit = limit\nif maxmatches>0:\n\tself._maxmatches = maxmatches\nif cutoff>=0:\n\tself._cutoff = cutoff", "path": "sphinxapi.py", "repo_name": "koch-t/KV78Turbo-OVAPI", "stars": 82, "license": "None", "language": "python", "size": 1211}
{"docstring": "\"\"\"\nClear groupby settings (for multi-queries).\n\"\"\"\n", "func_signal": "def ResetGroupBy (self):\n", "code": "self._groupby = ''\nself._groupfunc = SPH_GROUPBY_DAY\nself._groupsort = '@group desc'\nself._groupdistinct = ''", "path": "sphinxapi.py", "repo_name": "koch-t/KV78Turbo-OVAPI", "stars": 82, "license": "None", "language": "python", "size": 1211}
{"docstring": "\"\"\"\nUpdate given attribute values on given documents in given indexes.\nReturns amount of updated documents (0 or more) on success, or -1 on failure.\n\n'attrs' must be a list of strings.\n'values' must be a dict with int key (document ID) and list of int values (new attribute values).\noptional boolean parameter 'mva' points that there is update of MVA attributes.\nIn this case the 'values' must be a dict with int key (document ID) and list of lists of int values\n(new MVA attribute values).\n\nExample:\n\tres = cl.UpdateAttributes ( 'test1', [ 'group_id', 'date_added' ], { 2:[123,1000000000], 4:[456,1234567890] } )\n\"\"\"\n", "func_signal": "def UpdateAttributes ( self, index, attrs, values, mva=False ):\n", "code": "assert ( isinstance ( index, str ) )\nassert ( isinstance ( attrs, list ) )\nassert ( isinstance ( values, dict ) )\nfor attr in attrs:\n\tassert ( isinstance ( attr, str ) )\nfor docid, entry in values.items():\n\tAssertUInt32(docid)\n\tassert ( isinstance ( entry, list ) )\n\tassert ( len(attrs)==len(entry) )\n\tfor val in entry:\n\t\tif mva:\n\t\t\tassert ( isinstance ( val, list ) )\n\t\t\tfor vals in val:\n\t\t\t\tAssertInt32(vals)\n\t\telse:\n\t\t\tAssertInt32(val)\n\n# build request\nreq = [ pack('>L',len(index)), index ]\n\nreq.append ( pack('>L',len(attrs)) )\nmva_attr = 0\nif mva: mva_attr = 1\nfor attr in attrs:\n\treq.append ( pack('>L',len(attr)) + attr )\n\treq.append ( pack('>L', mva_attr ) )\n\nreq.append ( pack('>L',len(values)) )\nfor docid, entry in values.items():\n\treq.append ( pack('>Q',docid) )\n\tfor val in entry:\n\t\tval_len = val\n\t\tif mva: val_len = len ( val )\n\t\treq.append ( pack('>L',val_len ) )\n\t\tif mva:\n\t\t\tfor vals in val:\n\t\t\t\treq.append ( pack ('>L',vals) )\n\n# connect, send query, get response\nsock = self._Connect()\nif not sock:\n\treturn None\n\nreq = ''.join(req)\nlength = len(req)\nreq = pack ( '>2HL', SEARCHD_COMMAND_UPDATE, VER_COMMAND_UPDATE, length ) + req\nwrote = sock.send ( req )\n\nresponse = self._GetResponse ( sock, VER_COMMAND_UPDATE )\nif not response:\n\treturn -1\n\n# parse response\nupdated = unpack ( '>L', response[0:4] )[0]\nreturn updated", "path": "sphinxapi.py", "repo_name": "koch-t/KV78Turbo-OVAPI", "stars": 82, "license": "None", "language": "python", "size": 1211}
{"docstring": "\"\"\"\nBind per-field weights by name; expects (name,field_weight) dictionary as argument.\n\"\"\"\n", "func_signal": "def SetFieldWeights (self, weights):\n", "code": "assert(isinstance(weights,dict))\nfor key,val in weights.items():\n\tassert(isinstance(key,str))\n\tAssertUInt32 ( val )\nself._fieldweights = weights", "path": "sphinxapi.py", "repo_name": "koch-t/KV78Turbo-OVAPI", "stars": 82, "license": "None", "language": "python", "size": 1211}
{"docstring": "\"\"\"\nConnect to searchd server, and generate keywords list for a given query.\nReturns None on failure, or a list of keywords on success.\n\"\"\"\n", "func_signal": "def BuildKeywords ( self, query, index, hits ):\n", "code": "assert ( isinstance ( query, str ) )\nassert ( isinstance ( index, str ) )\nassert ( isinstance ( hits, int ) )\n\n# build request\nreq = [ pack ( '>L', len(query) ) + query ]\nreq.append ( pack ( '>L', len(index) ) + index )\nreq.append ( pack ( '>L', hits ) )\n\n# connect, send query, get response\nsock = self._Connect()\nif not sock:\n\treturn None\n\nreq = ''.join(req)\nlength = len(req)\nreq = pack ( '>2HL', SEARCHD_COMMAND_KEYWORDS, VER_COMMAND_KEYWORDS, length ) + req\nwrote = sock.send ( req )\n\nresponse = self._GetResponse ( sock, VER_COMMAND_KEYWORDS )\nif not response:\n\treturn None\n\n# parse response\nres = []\n\nnwords = unpack ( '>L', response[0:4] )[0]\np = 4\nmax_ = len(response)\n\nwhile nwords>0 and p<max_:\n\tnwords -= 1\n\n\tlength = unpack ( '>L', response[p:p+4] )[0]\n\tp += 4\n\ttokenized = response[p:p+length]\n\tp += length\n\n\tlength = unpack ( '>L', response[p:p+4] )[0]\n\tp += 4\n\tnormalized = response[p:p+length]\n\tp += length\n\n\tentry = { 'tokenized':tokenized, 'normalized':normalized }\n\tif hits:\n\t\tentry['docs'], entry['hits'] = unpack ( '>2L', response[p:p+8] )\n\t\tp += 8\n\n\tres.append ( entry )\n\nif nwords>0 or p>max_:\n\tself._error = 'incomplete reply'\n\treturn None\n\nreturn res", "path": "sphinxapi.py", "repo_name": "koch-t/KV78Turbo-OVAPI", "stars": 82, "license": "None", "language": "python", "size": 1211}
{"docstring": "\"\"\"\nConnect to searchd server and generate exceprts from given documents.\n\"\"\"\n", "func_signal": "def BuildExcerpts (self, docs, index, words, opts=None):\n", "code": "if not opts:\n\topts = {}\nif isinstance(words,unicode):\n\twords = words.encode('utf-8')\n\nassert(isinstance(docs, list))\nassert(isinstance(index, str))\nassert(isinstance(words, str))\nassert(isinstance(opts, dict))\n\nsock = self._Connect()\n\nif not sock:\n\treturn None\n\n# fixup options\nopts.setdefault('before_match', '<b>')\nopts.setdefault('after_match', '</b>')\nopts.setdefault('chunk_separator', ' ... ')\nopts.setdefault('html_strip_mode', 'index')\nopts.setdefault('limit', 256)\nopts.setdefault('limit_passages', 0)\nopts.setdefault('limit_words', 0)\nopts.setdefault('around', 5)\nopts.setdefault('start_passage_id', 1)\nopts.setdefault('passage_boundary', 'none')\n\n# build request\n# v.1.0 req\n\nflags = 1 # (remove spaces)\nif opts.get('exact_phrase'):\tflags |= 2\nif opts.get('single_passage'):\tflags |= 4\nif opts.get('use_boundaries'):\tflags |= 8\nif opts.get('weight_order'):\tflags |= 16\nif opts.get('query_mode'):\t\tflags |= 32\nif opts.get('force_all_words'):\tflags |= 64\nif opts.get('load_files'):\t\tflags |= 128\nif opts.get('allow_empty'):\t\tflags |= 256\nif opts.get('emit_zones'):\t\tflags |= 512\nif opts.get('load_files_scattered'):\tflags |= 1024\n\n# mode=0, flags\nreq = [pack('>2L', 0, flags)]\n\n# req index\nreq.append(pack('>L', len(index)))\nreq.append(index)\n\n# req words\nreq.append(pack('>L', len(words)))\nreq.append(words)\n\n# options\nreq.append(pack('>L', len(opts['before_match'])))\nreq.append(opts['before_match'])\n\nreq.append(pack('>L', len(opts['after_match'])))\nreq.append(opts['after_match'])\n\nreq.append(pack('>L', len(opts['chunk_separator'])))\nreq.append(opts['chunk_separator'])\n\nreq.append(pack('>L', int(opts['limit'])))\nreq.append(pack('>L', int(opts['around'])))\n\nreq.append(pack('>L', int(opts['limit_passages'])))\nreq.append(pack('>L', int(opts['limit_words'])))\nreq.append(pack('>L', int(opts['start_passage_id'])))\nreq.append(pack('>L', len(opts['html_strip_mode'])))\nreq.append((opts['html_strip_mode']))\nreq.append(pack('>L', len(opts['passage_boundary'])))\nreq.append((opts['passage_boundary']))\n\n# documents\nreq.append(pack('>L', len(docs)))\nfor doc in docs:\n\tif isinstance(doc,unicode):\n\t\tdoc = doc.encode('utf-8')\n\tassert(isinstance(doc, str))\n\treq.append(pack('>L', len(doc)))\n\treq.append(doc)\n\nreq = ''.join(req)\n\n# send query, get response\nlength = len(req)\n\n# add header\nreq = pack('>2HL', SEARCHD_COMMAND_EXCERPT, VER_COMMAND_EXCERPT, length)+req\nwrote = sock.send(req)\n\nresponse = self._GetResponse(sock, VER_COMMAND_EXCERPT )\nif not response:\n\treturn []\n\n# parse response\npos = 0\nres = []\nrlen = len(response)\n\nfor i in range(len(docs)):\n\tlength = unpack('>L', response[pos:pos+4])[0]\n\tpos += 4\n\n\tif pos+length > rlen:\n\t\tself._error = 'incomplete reply'\n\t\treturn []\n\n\tres.append(response[pos:pos+length])\n\tpos += length\n\nreturn res", "path": "sphinxapi.py", "repo_name": "koch-t/KV78Turbo-OVAPI", "stars": 82, "license": "None", "language": "python", "size": 1211}
{"docstring": "\"\"\"\nSet range filter.\nOnly match records if 'attribute' value is beetwen 'min_' and 'max_' (inclusive).\n\"\"\"\n", "func_signal": "def SetFilterRange (self, attribute, min_, max_, exclude=0 ):\n", "code": "assert(isinstance(attribute, str))\nAssertInt32(min_)\nAssertInt32(max_)\nassert(min_<=max_)\n\nself._filters.append ( { 'type':SPH_FILTER_RANGE, 'attr':attribute, 'exclude':exclude, 'min':min_, 'max':max_ } )", "path": "sphinxapi.py", "repo_name": "koch-t/KV78Turbo-OVAPI", "stars": 82, "license": "None", "language": "python", "size": 1211}
{"docstring": "\"\"\"\nConnect to searchd server and run given search query.\nReturns None on failure; result set hash on success (see documentation for details).\n\"\"\"\n", "func_signal": "def Query (self, query, index='*', comment=''):\n", "code": "assert(len(self._reqs)==0)\nself.AddQuery(query,index,comment)\nresults = self.RunQueries()\nself._reqs = [] # we won't re-run erroneous batch\n\nif not results or len(results)==0:\n\treturn None\nself._error = results[0]['error']\nself._warning = results[0]['warning']\nif results[0]['status'] == SEARCHD_ERROR:\n\treturn None\nreturn results[0]", "path": "sphinxapi.py", "repo_name": "koch-t/KV78Turbo-OVAPI", "stars": 82, "license": "None", "language": "python", "size": 1211}
{"docstring": "\"\"\"\nClear all filters (for multi-queries).\n\"\"\"\n", "func_signal": "def ResetFilters (self):\n", "code": "self._filters = []\nself._anchor = {}", "path": "sphinxapi.py", "repo_name": "koch-t/KV78Turbo-OVAPI", "stars": 82, "license": "None", "language": "python", "size": 1211}
{"docstring": "\"\"\"\nGet the status\n\"\"\"\n\n# connect, send query, get response\n", "func_signal": "def Status ( self ):\n", "code": "sock = self._Connect()\nif not sock:\n\treturn None\n\nreq = pack ( '>2HLL', SEARCHD_COMMAND_STATUS, VER_COMMAND_STATUS, 4, 1 )\nwrote = sock.send ( req )\n\nresponse = self._GetResponse ( sock, VER_COMMAND_STATUS )\nif not response:\n\treturn None\n\n# parse response\nres = []\n\np = 8\nmax_ = len(response)\n\nwhile p<max_:\n\tlength = unpack ( '>L', response[p:p+4] )[0]\n\tk = response[p+4:p+length+4]\n\tp += 4+length\n\tlength = unpack ( '>L', response[p:p+4] )[0]\n\tv = response[p+4:p+length+4]\n\tp += 4+length\n\tres += [[k, v]]\n\nreturn res", "path": "sphinxapi.py", "repo_name": "koch-t/KV78Turbo-OVAPI", "stars": 82, "license": "None", "language": "python", "size": 1211}
{"docstring": "\"\"\"\nRun queries batch.\nReturns None on network IO failure; or an array of result set hashes on success.\n\"\"\"\n", "func_signal": "def RunQueries (self):\n", "code": "if len(self._reqs)==0:\n\tself._error = 'no queries defined, issue AddQuery() first'\n\treturn None\n\nsock = self._Connect()\nif not sock:\n\treturn None\n\nreq = ''.join(self._reqs)\nlength = len(req)+8\nreq = pack('>HHLLL', SEARCHD_COMMAND_SEARCH, VER_COMMAND_SEARCH, length, 0, len(self._reqs))+req\nsock.send(req)\n\nresponse = self._GetResponse(sock, VER_COMMAND_SEARCH)\nif not response:\n\treturn None\n\nnreqs = len(self._reqs)\n\n# parse response\nmax_ = len(response)\np = 0\n\nresults = []\nfor i in range(0,nreqs,1):\n\tresult = {}\n\tresults.append(result)\n\n\tresult['error'] = ''\n\tresult['warning'] = ''\n\tstatus = unpack('>L', response[p:p+4])[0]\n\tp += 4\n\tresult['status'] = status\n\tif status != SEARCHD_OK:\n\t\tlength = unpack('>L', response[p:p+4])[0]\n\t\tp += 4\n\t\tmessage = response[p:p+length]\n\t\tp += length\n\n\t\tif status == SEARCHD_WARNING:\n\t\t\tresult['warning'] = message\n\t\telse:\n\t\t\tresult['error'] = message\n\t\t\tcontinue\n\n\t# read schema\n\tfields = []\n\tattrs = []\n\n\tnfields = unpack('>L', response[p:p+4])[0]\n\tp += 4\n\twhile nfields>0 and p<max_:\n\t\tnfields -= 1\n\t\tlength = unpack('>L', response[p:p+4])[0]\n\t\tp += 4\n\t\tfields.append(response[p:p+length])\n\t\tp += length\n\n\tresult['fields'] = fields\n\n\tnattrs = unpack('>L', response[p:p+4])[0]\n\tp += 4\n\twhile nattrs>0 and p<max_:\n\t\tnattrs -= 1\n\t\tlength = unpack('>L', response[p:p+4])[0]\n\t\tp += 4\n\t\tattr = response[p:p+length]\n\t\tp += length\n\t\ttype_ = unpack('>L', response[p:p+4])[0]\n\t\tp += 4\n\t\tattrs.append([attr,type_])\n\n\tresult['attrs'] = attrs\n\n\t# read match count\n\tcount = unpack('>L', response[p:p+4])[0]\n\tp += 4\n\tid64 = unpack('>L', response[p:p+4])[0]\n\tp += 4\n\n\t# read matches\n\tresult['matches'] = []\n\twhile count>0 and p<max_:\n\t\tcount -= 1\n\t\tif id64:\n\t\t\tdoc, weight = unpack('>QL', response[p:p+12])\n\t\t\tp += 12\n\t\telse:\n\t\t\tdoc, weight = unpack('>2L', response[p:p+8])\n\t\t\tp += 8\n\n\t\tmatch = { 'id':doc, 'weight':weight, 'attrs':{} }\n\t\tfor i in range(len(attrs)):\n\t\t\tif attrs[i][1] == SPH_ATTR_FLOAT:\n\t\t\t\tmatch['attrs'][attrs[i][0]] = unpack('>f', response[p:p+4])[0]\n\t\t\telif attrs[i][1] == SPH_ATTR_BIGINT:\n\t\t\t\tmatch['attrs'][attrs[i][0]] = unpack('>q', response[p:p+8])[0]\n\t\t\t\tp += 4\n\t\t\telif attrs[i][1] == SPH_ATTR_STRING:\n\t\t\t\tslen = unpack('>L', response[p:p+4])[0]\n\t\t\t\tp += 4\n\t\t\t\tmatch['attrs'][attrs[i][0]] = ''\n\t\t\t\tif slen>0:\n\t\t\t\t\tmatch['attrs'][attrs[i][0]] = response[p:p+slen]\n\t\t\t\tp += slen-4\n\t\t\telif attrs[i][1] == SPH_ATTR_MULTI:\n\t\t\t\tmatch['attrs'][attrs[i][0]] = []\n\t\t\t\tnvals = unpack('>L', response[p:p+4])[0]\n\t\t\t\tp += 4\n\t\t\t\tfor n in range(0,nvals,1):\n\t\t\t\t\tmatch['attrs'][attrs[i][0]].append(unpack('>L', response[p:p+4])[0])\n\t\t\t\t\tp += 4\n\t\t\t\tp -= 4\n\t\t\telif attrs[i][1] == SPH_ATTR_MULTI64:\n\t\t\t\tmatch['attrs'][attrs[i][0]] = []\n\t\t\t\tnvals = unpack('>L', response[p:p+4])[0]\n\t\t\t\tnvals = nvals/2\n\t\t\t\tp += 4\n\t\t\t\tfor n in range(0,nvals,1):\n\t\t\t\t\tmatch['attrs'][attrs[i][0]].append(unpack('>q', response[p:p+8])[0])\n\t\t\t\t\tp += 8\n\t\t\t\tp -= 4\n\t\t\telse:\n\t\t\t\tmatch['attrs'][attrs[i][0]] = unpack('>L', response[p:p+4])[0]\n\t\t\tp += 4\n\n\t\tresult['matches'].append ( match )\n\n\tresult['total'], result['total_found'], result['time'], words = unpack('>4L', response[p:p+16])\n\n\tresult['time'] = '%.3f' % (result['time']/1000.0)\n\tp += 16\n\n\tresult['words'] = []\n\twhile words>0:\n\t\twords -= 1\n\t\tlength = unpack('>L', response[p:p+4])[0]\n\t\tp += 4\n\t\tword = response[p:p+length]\n\t\tp += length\n\t\tdocs, hits = unpack('>2L', response[p:p+8])\n\t\tp += 8\n\n\t\tresult['words'].append({'word':word, 'docs':docs, 'hits':hits})\n\nself._reqs = []\nreturn results", "path": "sphinxapi.py", "repo_name": "koch-t/KV78Turbo-OVAPI", "stars": 82, "license": "None", "language": "python", "size": 1211}
{"docstring": "\"\"\"\nSet grouping attribute and function.\n\"\"\"\n", "func_signal": "def SetGroupBy ( self, attribute, func, groupsort='@group desc' ):\n", "code": "assert(isinstance(attribute, str))\nassert(func in [SPH_GROUPBY_DAY, SPH_GROUPBY_WEEK, SPH_GROUPBY_MONTH, SPH_GROUPBY_YEAR, SPH_GROUPBY_ATTR, SPH_GROUPBY_ATTRPAIR] )\nassert(isinstance(groupsort, str))\n\nself._groupby = attribute\nself._groupfunc = func\nself._groupsort = groupsort", "path": "sphinxapi.py", "repo_name": "koch-t/KV78Turbo-OVAPI", "stars": 82, "license": "None", "language": "python", "size": 1211}
{"docstring": "\"\"\"\nCreate a new client object, and fill defaults.\n\"\"\"\n", "func_signal": "def __init__ (self):\n", "code": "self._host\t\t\t= 'localhost'\t\t\t\t\t# searchd host (default is \"localhost\")\nself._port\t\t\t= 9312\t\t\t\t\t\t\t# searchd port (default is 9312)\nself._path\t\t\t= None\t\t\t\t\t\t\t# searchd unix-domain socket path\nself._socket\t\t= None\nself._offset\t\t= 0\t\t\t\t\t\t\t\t# how much records to seek from result-set start (default is 0)\nself._limit\t\t\t= 20\t\t\t\t\t\t\t# how much records to return from result-set starting at offset (default is 20)\nself._mode\t\t\t= SPH_MATCH_ALL\t\t\t\t\t# query matching mode (default is SPH_MATCH_ALL)\nself._weights\t\t= []\t\t\t\t\t\t\t# per-field weights (default is 1 for all fields)\nself._sort\t\t\t= SPH_SORT_RELEVANCE\t\t\t# match sorting mode (default is SPH_SORT_RELEVANCE)\nself._sortby\t\t= ''\t\t\t\t\t\t\t# attribute to sort by (defualt is \"\")\nself._min_id\t\t= 0\t\t\t\t\t\t\t\t# min ID to match (default is 0)\nself._max_id\t\t= 0\t\t\t\t\t\t\t\t# max ID to match (default is UINT_MAX)\nself._filters\t\t= []\t\t\t\t\t\t\t# search filters\nself._groupby\t\t= ''\t\t\t\t\t\t\t# group-by attribute name\nself._groupfunc\t\t= SPH_GROUPBY_DAY\t\t\t\t# group-by function (to pre-process group-by attribute value with)\nself._groupsort\t\t= '@group desc'\t\t\t\t\t# group-by sorting clause (to sort groups in result set with)\nself._groupdistinct\t= ''\t\t\t\t\t\t\t# group-by count-distinct attribute\nself._maxmatches\t= 1000\t\t\t\t\t\t\t# max matches to retrieve\nself._cutoff\t\t= 0\t\t\t\t\t\t\t\t# cutoff to stop searching at\nself._retrycount\t= 0\t\t\t\t\t\t\t\t# distributed retry count\nself._retrydelay\t= 0\t\t\t\t\t\t\t\t# distributed retry delay\nself._anchor\t\t= {}\t\t\t\t\t\t\t# geographical anchor point\nself._indexweights\t= {}\t\t\t\t\t\t\t# per-index weights\nself._ranker\t\t= SPH_RANK_PROXIMITY_BM25\t\t# ranking mode\nself._rankexpr\t\t= ''\t\t\t\t\t\t\t# ranking expression for SPH_RANK_EXPR\nself._maxquerytime\t= 0\t\t\t\t\t\t\t\t# max query time, milliseconds (default is 0, do not limit)\nself._timeout = 1.0\t\t\t\t\t\t\t\t\t\t# connection timeout\nself._fieldweights\t= {}\t\t\t\t\t\t\t# per-field-name weights\nself._overrides\t\t= {}\t\t\t\t\t\t\t# per-query attribute values overrides\nself._select\t\t= '*'\t\t\t\t\t\t\t# select-list (attributes or expressions, with optional aliases)\n\nself._error\t\t\t= ''\t\t\t\t\t\t\t# last error message\nself._warning\t\t= ''\t\t\t\t\t\t\t# last warning message\nself._reqs\t\t\t= []\t\t\t\t\t\t\t# requests array for multi-query", "path": "sphinxapi.py", "repo_name": "koch-t/KV78Turbo-OVAPI", "stars": 82, "license": "None", "language": "python", "size": 1211}
{"docstring": "\"\"\"\nSet sorting mode.\n\"\"\"\n", "func_signal": "def SetSortMode ( self, mode, clause='' ):\n", "code": "assert ( mode in [SPH_SORT_RELEVANCE, SPH_SORT_ATTR_DESC, SPH_SORT_ATTR_ASC, SPH_SORT_TIME_SEGMENTS, SPH_SORT_EXTENDED, SPH_SORT_EXPR] )\nassert ( isinstance ( clause, str ) )\nself._sort = mode\nself._sortby = clause", "path": "sphinxapi.py", "repo_name": "koch-t/KV78Turbo-OVAPI", "stars": 82, "license": "None", "language": "python", "size": 1211}
{"docstring": "\"\"\"\nAdd query to batch.\n\"\"\"\n# build request\n", "func_signal": "def AddQuery (self, query, index='*', comment=''):\n", "code": "req = []\nreq.append(pack('>4L', self._offset, self._limit, self._mode, self._ranker))\nif self._ranker==SPH_RANK_EXPR:\n\treq.append(pack('>L', len(self._rankexpr)))\n\treq.append(self._rankexpr)\nreq.append(pack('>L', self._sort))\nreq.append(pack('>L', len(self._sortby)))\nreq.append(self._sortby)\n\nif isinstance(query,unicode):\n\tquery = query.encode('utf-8')\nassert(isinstance(query,str))\n\nreq.append(pack('>L', len(query)))\nreq.append(query)\n\nreq.append(pack('>L', len(self._weights)))\nfor w in self._weights:\n\treq.append(pack('>L', w))\nreq.append(pack('>L', len(index)))\nreq.append(index)\nreq.append(pack('>L',1)) # id64 range marker\nreq.append(pack('>Q', self._min_id))\nreq.append(pack('>Q', self._max_id))\n\n# filters\nreq.append ( pack ( '>L', len(self._filters) ) )\nfor f in self._filters:\n\treq.append ( pack ( '>L', len(f['attr'])) + f['attr'])\n\tfiltertype = f['type']\n\treq.append ( pack ( '>L', filtertype))\n\tif filtertype == SPH_FILTER_VALUES:\n\t\treq.append ( pack ('>L', len(f['values'])))\n\t\tfor val in f['values']:\n\t\t\treq.append ( pack ('>q', val))\n\telif filtertype == SPH_FILTER_RANGE:\n\t\treq.append ( pack ('>2q', f['min'], f['max']))\n\telif filtertype == SPH_FILTER_FLOATRANGE:\n\t\treq.append ( pack ('>2f', f['min'], f['max']))\n\treq.append ( pack ( '>L', f['exclude'] ) )\n\n# group-by, max-matches, group-sort\nreq.append ( pack ( '>2L', self._groupfunc, len(self._groupby) ) )\nreq.append ( self._groupby )\nreq.append ( pack ( '>2L', self._maxmatches, len(self._groupsort) ) )\nreq.append ( self._groupsort )\nreq.append ( pack ( '>LLL', self._cutoff, self._retrycount, self._retrydelay)) \nreq.append ( pack ( '>L', len(self._groupdistinct)))\nreq.append ( self._groupdistinct)\n\n# anchor point\nif len(self._anchor) == 0:\n\treq.append ( pack ('>L', 0))\nelse:\n\tattrlat, attrlong = self._anchor['attrlat'], self._anchor['attrlong']\n\tlatitude, longitude = self._anchor['lat'], self._anchor['long']\n\treq.append ( pack ('>L', 1))\n\treq.append ( pack ('>L', len(attrlat)) + attrlat)\n\treq.append ( pack ('>L', len(attrlong)) + attrlong)\n\treq.append ( pack ('>f', latitude) + pack ('>f', longitude))\n\n# per-index weights\nreq.append ( pack ('>L',len(self._indexweights)))\nfor indx,weight in self._indexweights.items():\n\treq.append ( pack ('>L',len(indx)) + indx + pack ('>L',weight))\n\n# max query time\nreq.append ( pack ('>L', self._maxquerytime) ) \n\n# per-field weights\nreq.append ( pack ('>L',len(self._fieldweights) ) )\nfor field,weight in self._fieldweights.items():\n\treq.append ( pack ('>L',len(field)) + field + pack ('>L',weight) )\n\n# comment\nreq.append ( pack('>L',len(comment)) + comment )\n\n# attribute overrides\nreq.append ( pack('>L', len(self._overrides)) )\nfor v in self._overrides.values():\n\treq.extend ( ( pack('>L', len(v['name'])), v['name'] ) )\n\treq.append ( pack('>LL', v['type'], len(v['values'])) )\n\tfor id, value in v['values'].iteritems():\n\t\treq.append ( pack('>Q', id) )\n\t\tif v['type'] == SPH_ATTR_FLOAT:\n\t\t\treq.append ( pack('>f', value) )\n\t\telif v['type'] == SPH_ATTR_BIGINT:\n\t\t\treq.append ( pack('>q', value) )\n\t\telse:\n\t\t\treq.append ( pack('>l', value) )\n\n# select-list\nreq.append ( pack('>L', len(self._select)) )\nreq.append ( self._select )\n\n# send query, get response\nreq = ''.join(req)\n\nself._reqs.append(req)\nreturn", "path": "sphinxapi.py", "repo_name": "koch-t/KV78Turbo-OVAPI", "stars": 82, "license": "None", "language": "python", "size": 1211}
{"docstring": "\"\"\"\nBind per-index weights by name; expects (name,index_weight) dictionary as argument.\n\"\"\"\n", "func_signal": "def SetIndexWeights (self, weights):\n", "code": "assert(isinstance(weights,dict))\nfor key,val in weights.items():\n\tassert(isinstance(key,str))\n\tAssertUInt32(val)\nself._indexweights = weights", "path": "sphinxapi.py", "repo_name": "koch-t/KV78Turbo-OVAPI", "stars": 82, "license": "None", "language": "python", "size": 1211}
{"docstring": "\"\"\"\nSet searchd server host and port.\n\"\"\"\n", "func_signal": "def SetServer (self, host, port = None):\n", "code": "assert(isinstance(host, str))\nif host.startswith('/'):\n\tself._path = host\n\treturn\nelif host.startswith('unix://'):\n\tself._path = host[7:]\n\treturn\nassert(isinstance(port, int))\nself._host = host\nself._port = port\nself._path = None", "path": "sphinxapi.py", "repo_name": "koch-t/KV78Turbo-OVAPI", "stars": 82, "license": "None", "language": "python", "size": 1211}
{"docstring": "\"\"\"\nSet values set filter.\nOnly match records where 'attribute' value is in given 'values' set.\n\"\"\"\n", "func_signal": "def SetFilter ( self, attribute, values, exclude=0 ):\n", "code": "assert(isinstance(attribute, str))\nassert iter(values)\n\nfor value in values:\n\tAssertInt32 ( value )\n\nself._filters.append ( { 'type':SPH_FILTER_VALUES, 'attr':attribute, 'exclude':exclude, 'values':values } )", "path": "sphinxapi.py", "repo_name": "koch-t/KV78Turbo-OVAPI", "stars": 82, "license": "None", "language": "python", "size": 1211}
{"docstring": "\"\"\"\nSet ranking mode.\n\"\"\"\n", "func_signal": "def SetRankingMode ( self, ranker, rankexpr='' ):\n", "code": "assert(ranker>=0 and ranker<SPH_RANK_TOTAL)\nself._ranker = ranker\nself._rankexpr = rankexpr", "path": "sphinxapi.py", "repo_name": "koch-t/KV78Turbo-OVAPI", "stars": 82, "license": "None", "language": "python", "size": 1211}
{"docstring": "\"\"\"\nINTERNAL METHOD, DO NOT CALL. Gets and checks response packet from searchd server.\n\"\"\"\n", "func_signal": "def _GetResponse (self, sock, client_ver):\n", "code": "(status, ver, length) = unpack('>2HL', sock.recv(8))\nresponse = ''\nleft = length\nwhile left>0:\n\tchunk = sock.recv(left)\n\tif chunk:\n\t\tresponse += chunk\n\t\tleft -= len(chunk)\n\telse:\n\t\tbreak\n\nif not self._socket:\n\tsock.close()\n\n# check response\nread = len(response)\nif not response or read!=length:\n\tif length:\n\t\tself._error = 'failed to read searchd response (status=%s, ver=%s, len=%s, read=%s)' \\\n\t\t\t% (status, ver, length, read)\n\telse:\n\t\tself._error = 'received zero-sized searchd response'\n\treturn None\n\n# check status\nif status==SEARCHD_WARNING:\n\twend = 4 + unpack ( '>L', response[0:4] )[0]\n\tself._warning = response[4:wend]\n\treturn response[wend:]\n\nif status==SEARCHD_ERROR:\n\tself._error = 'searchd error: '+response[4:]\n\treturn None\n\nif status==SEARCHD_RETRY:\n\tself._error = 'temporary searchd error: '+response[4:]\n\treturn None\n\nif status!=SEARCHD_OK:\n\tself._error = 'unknown status code %d' % status\n\treturn None\n\n# check version\nif ver<client_ver:\n\tself._warning = 'searchd command v.%d.%d older than client\\'s v.%d.%d, some options might not work' \\\n\t\t% (ver>>8, ver&0xff, client_ver>>8, client_ver&0xff)\n\nreturn response", "path": "sphinxapi.py", "repo_name": "koch-t/KV78Turbo-OVAPI", "stars": 82, "license": "None", "language": "python", "size": 1211}
{"docstring": "# Codigo para dinamizar a criacao de instancias de entidade,\n# aplicando os valores dos atributos na instanciacao\n", "func_signal": "def __init__(self, **kwargs):\n", "code": "for k, v in kwargs.items():\n    setattr(self, k, v)\n\n# Adiciona o objeto \u00e0 fonte de dados informada\nif not self._fonte_dados:\n    from fonte_dados import _fonte_dados\n    self._fonte_dados = _fonte_dados\n\nself._fonte_dados.adicionar_objeto(self)", "path": "pynfe\\entidades\\base.py", "repo_name": "marinho/PyNFe", "stars": 127, "license": "other", "language": "python", "size": 508}
{"docstring": "# FIXME: descobrir forma de evitar o uso do libxml2 neste processo\n\n# Ativa as fun\u00e7\u00f5es de an\u00e1lise de arquivos XML FIXME\n", "func_signal": "def _ativar_funcoes_criptograficas(self):\n", "code": "libxml2.initParser()\nlibxml2.substituteEntitiesDefault(1)\n\n# Ativa as fun\u00e7\u00f5es da API de criptografia\nxmlsec.init()\nxmlsec.cryptoAppInit(None)\nxmlsec.cryptoInit()", "path": "pynfe\\processamento\\assinatura.py", "repo_name": "marinho/PyNFe", "stars": 127, "license": "other", "language": "python", "size": 508}
{"docstring": "# Carrega o XML do arquivo\n", "func_signal": "def verificar_arquivo(self, caminho_arquivo):\n", "code": "raiz = etree.parse(caminho_arquivo)\nreturn self.verificar_etree(raiz)", "path": "pynfe\\processamento\\assinatura.py", "repo_name": "marinho/PyNFe", "stars": 127, "license": "other", "language": "python", "size": 508}
{"docstring": "# Converte etree para string\n", "func_signal": "def _antes_de_assinar_ou_verificar(self, raiz):\n", "code": "xml = etree.tostring(raiz, xml_declaration=True, encoding='utf-8')\n\n# Ativa fun\u00e7\u00f5es criptogr\u00e1ficas\nself._ativar_funcoes_criptograficas()\n\n# Colocamos o texto no avaliador XML FIXME: descobrir forma de evitar o uso do libxml2 neste processo\ndoc_xml = libxml2.parseMemory(xml, len(xml))\n    \n# Cria o contexto para manipula\u00e7\u00e3o do XML via sintaxe XPATH\nctxt = doc_xml.xpathNewContext()\nctxt.xpathRegisterNs(u'sig', NAMESPACE_SIG)\n    \n# Separa o n\u00f3 da assinatura\nnoh_assinatura = ctxt.xpathEval(u'//*/sig:Signature')[0]\n    \n# Buscamos a chave no arquivo do certificado\nchave = xmlsec.cryptoAppKeyLoad(\n        filename=str(self.certificado.caminho_arquivo),\n        format=xmlsec.KeyDataFormatPkcs12,\n        pwd=str(self.senha),\n        pwdCallback=None,\n        pwdCallbackCtx=None,\n        )\n    \n# Cria a vari\u00e1vel de chamada (callable) da fun\u00e7\u00e3o de assinatura\nassinador = xmlsec.DSigCtx()\n    \n# Atribui a chave ao assinador\nassinador.signKey = chave\n\nreturn doc_xml, ctxt, noh_assinatura, assinador", "path": "pynfe\\processamento\\assinatura.py", "repo_name": "marinho/PyNFe", "stars": 127, "license": "other", "language": "python", "size": 508}
{"docstring": "\"\"\"Gera o(s) arquivo(s) de Nofa Fiscal eletronica no padrao oficial da SEFAZ\ne Receita Federal, para ser(em) enviado(s) para o webservice ou para ser(em)\narmazenado(s) em cache local.\"\"\"\n\n# No raiz do XML de saida\n", "func_signal": "def exportar(self, destino=None, retorna_string=False, **kwargs):\n", "code": "raiz = etree.Element('NFe', xmlns=\"http://www.portalfiscal.inf.br/nfe\")\n\n# Carrega lista de Notas Fiscais\nnotas_fiscais = self._fonte_dados.obter_lista(_classe=NotaFiscal, **kwargs)\n\nfor nf in notas_fiscais:\n    raiz.append(self._serializar_notas_fiscal(nf, retorna_string=False))\n\nif retorna_string:\n    return etree.tostring(raiz, pretty_print=True)\nelse:\n    return raiz", "path": "pynfe\\processamento\\serializacao.py", "repo_name": "marinho/PyNFe", "stars": 127, "license": "other", "language": "python", "size": 508}
{"docstring": "# TODO\n", "func_signal": "def obter_pais_por_codigo(codigo):\n", "code": "if codigo == '1058':\n    return 'Brasil'", "path": "pynfe\\utils\\__init__.py", "repo_name": "marinho/PyNFe", "stars": 127, "license": "other", "language": "python", "size": 508}
{"docstring": "# Extrai a tag do elemento raiz\n", "func_signal": "def assinar_etree(self, raiz, retorna_xml=False):\n", "code": "tipo = extrair_tag(raiz.getroot())\n\n# doctype compat\u00edvel com o tipo da tag raiz\nif tipo == u'NFe':\n    doctype = u'<!DOCTYPE NFe [<!ATTLIST infNFe Id ID #IMPLIED>]>'\nelif tipo == u'inutNFe':\n    doctype = u'<!DOCTYPE inutNFe [<!ATTLIST infInut Id ID #IMPLIED>]>'\nelif tipo == u'cancNFe':\n    doctype = u'<!DOCTYPE cancNFe [<!ATTLIST infCanc Id ID #IMPLIED>]>'\nelif tipo == u'DPEC':\n    doctype = u'<!DOCTYPE DPEC [<!ATTLIST infDPEC Id ID #IMPLIED>]>'\n\n# Tag de assinatura\nif raiz.getroot().find('Signature') is None:\n    signature = etree.Element(\n            '{%s}Signature'%NAMESPACE_SIG,\n            URI=raiz.getroot().getchildren()[0].attrib['Id'],\n            nsmap={'sig': NAMESPACE_SIG},\n            )\n\n    signed_info = etree.SubElement(signature, '{%s}SignedInfo'%NAMESPACE_SIG)\n    etree.SubElement(signed_info, 'CanonicalizationMethod', Algorithm=\"http://www.w3.org/TR/2001/REC-xml-c14n-20010315\")\n    etree.SubElement(signed_info, 'SignatureMethod', Algorithm=\"http://www.w3.org/2000/09/xmldsig#rsa-sha1\")\n\n    reference = etree.SubElement(signed_info, '{%s}Reference'%NAMESPACE_SIG, URI=raiz.getroot().getchildren()[0].attrib['Id'])\n    transforms = etree.SubElement(reference, 'Transforms', URI=raiz.getroot().getchildren()[0].attrib['Id'])\n    etree.SubElement(transforms, 'Transform', Algorithm=\"http://www.w3.org/2000/09/xmldsig#enveloped-signature\")\n    etree.SubElement(transforms, 'Transform', Algorithm=\"http://www.w3.org/TR/2001/REC-xml-c14n-20010315\")\n    etree.SubElement(reference, '{%s}DigestMethod'%NAMESPACE_SIG, Algorithm=\"http://www.w3.org/2000/09/xmldsig#sha1\")\n    digest_value = etree.SubElement(reference, '{%s}DigestValue'%NAMESPACE_SIG)\n\n    signature_value = etree.SubElement(signature, '{%s}SignatureValue'%NAMESPACE_SIG)\n\n    key_info = etree.SubElement(signature, '{%s}KeyInfo'%NAMESPACE_SIG)\n    x509_data = etree.SubElement(key_info, '{%s}X509Data'%NAMESPACE_SIG)\n    x509_certificate = etree.SubElement(x509_data, '{%s}X509Certificate'%NAMESPACE_SIG)\n\n    raiz.getroot().insert(0, signature)\n\n# Acrescenta a tag de doctype (como o lxml nao suporta alteracao do doctype,\n# converte para string para faze-lo)\nxml = etree.tostring(raiz, xml_declaration=True, encoding='utf-8')\n\nif xml.find('<!DOCTYPE ') == -1:\n    pos = xml.find('>') + 1\n    xml = xml[:pos] + doctype + xml[pos:]\n    #raiz = etree.parse(StringIO(xml))\n\ndoc_xml, ctxt, noh_assinatura, assinador = self._antes_de_assinar_ou_verificar(raiz)\n\n# Realiza a assinatura\nassinador.sign(noh_assinatura)\n    \n# Coloca na inst\u00e2ncia Signature os valores calculados\ndigest_value.text = ctxt.xpathEval(u'//sig:DigestValue')[0].content.replace(u'\\n', u'')\nsignature_value.text = ctxt.xpathEval(u'//sig:SignatureValue')[0].content.replace(u'\\n', u'')\n\n# Provavelmente retornar\u00e3o v\u00e1rios certificados, j\u00e1 que o xmlsec inclui a cadeia inteira\ncertificados = ctxt.xpathEval(u'//sig:X509Data/sig:X509Certificate')\nx509_certificate.text = certificados[len(certificados)-1].content.replace(u'\\n', u'')\n    \nresultado = assinador.status == xmlsec.DSigStatusSucceeded\n\n# Gera o XML para retornar\nxml = doc_xml.serialize()\n\n# Limpa objetos da memoria e desativa fun\u00e7\u00f5es criptogr\u00e1ficas\nself._depois_de_assinar_ou_verificar(doc_xml, ctxt, assinador)\n\nif retorna_xml:\n    return xml\nelse:\n    return etree.parse(StringIO(xml))", "path": "pynfe\\processamento\\assinatura.py", "repo_name": "marinho/PyNFe", "stars": 127, "license": "other", "language": "python", "size": 508}
{"docstring": "\"\"\" Decode font file embedded in this script and create file \"\"\"\n", "func_signal": "def decodeFontFile(data, file):\n", "code": "from zlib import decompress\nfrom base64 import decodestring\nfrom os.path import exists\n\n# If the font file is missing\nif not exists(file):\n   # Write font file\n   open (file, \"wb\").write(decompress(decodestring(data)))", "path": "pynfe\\utils\\bar_code_128.py", "repo_name": "marinho/PyNFe", "stars": 127, "license": "other", "language": "python", "size": 508}
{"docstring": "\"\"\" Create the binary code\nreturn a string which contains \"0\" for white bar, \"1\" for black bar \"\"\"\n\n", "func_signal": "def makeCode(self, code):\n", "code": "current_charset = None\npos=sum=0\nskip=False\nfor c in range(len(code)):\n    if skip:\n        skip=False\n        continue\n  \n    #Only switch to char set C if next four chars are digits\n    if len(code[c:]) >=4 and code[c:c+4].isdigit() and current_charset!=self.CharSetC or \\\n       len(code[c:]) >=2 and code[c:c+2].isdigit() and current_charset==self.CharSetC:     \n       #If char set C = current and next two chars ar digits, keep C \n       if current_charset!=self.CharSetC:\n           #Switching to Character set C\n           if pos:\n               strCode += self.ValueEncodings[current_charset['Code C']]\n               sum  += pos * current_charset['Code C']\n           else:\n               strCode= self.ValueEncodings[self.CharSetC['START C']]\n               sum = self.CharSetC['START C']\n           current_charset= self.CharSetC\n           pos+=1\n    elif self.CharSetB.has_key(code[c]) and current_charset!=self.CharSetB and \\\n         not(self.CharSetA.has_key(code[c]) and current_charset==self.CharSetA): \n       #If char in chrset A = current, then just keep that\n       # Switching to Character set B\n       if pos:\n           strCode += self.ValueEncodings[current_charset['Code B']]\n           sum  += pos * current_charset['Code B']\n       else:\n           strCode= self.ValueEncodings[self.CharSetB['START B']]\n           sum = self.CharSetB['START B']\n       current_charset= self.CharSetB\n       pos+=1\n    elif self.CharSetA.has_key(code[c]) and current_charset!=self.CharSetA and \\\n         not(self.CharSetB.has_key(code[c]) and current_charset==self.CharSetB): \n       # if char in chrset B== current, then just keep that\n       # Switching to Character set A\n       if pos:\n           strCode += self.ValueEncodings[current_charset['Code A']]\n           sum  += pos * current_charset['Code A']\n       else:\n           strCode += self.ValueEncodings[self.CharSetA['START A']]\n           sum = self.CharSetA['START A']\n       current_charset= self.CharSetA\n       pos+=1\n\n    if current_charset==self.CharSetC:\n       val= self.CharSetC[code[c:c+2]]\n       skip=True\n    else:\n       val=current_charset[code[c]]\n\n    sum += pos * val\n    strCode += self.ValueEncodings[val]\n    pos+=1\n                  \n#Checksum\nchecksum= sum % 103\n      \nstrCode +=  self.ValueEncodings[checksum]\n              \n#The stop character\nstrCode += self.ValueEncodings[current_charset['STOP']]\n              \n#Termination bar\nstrCode += \"11\"\n      \nreturn strCode", "path": "pynfe\\utils\\bar_code_128.py", "repo_name": "marinho/PyNFe", "stars": 127, "license": "other", "language": "python", "size": 508}
{"docstring": "\"\"\" Execute all tests \"\"\"\n", "func_signal": "def test():\n", "code": "testWithChecksum()\ntestImage()", "path": "pynfe\\utils\\bar_code_128.py", "repo_name": "marinho/PyNFe", "stars": 127, "license": "other", "language": "python", "size": 508}
{"docstring": "# Monta 'Id' da tag raiz <infNFe>\n# Ex.: NFe35080599999090910270550010000000011518005123\n", "func_signal": "def identificador_unico(self):\n", "code": "return \"NFe%(uf)s%(ano)s%(mes)s%(cnpj)s%(mod)s%(serie)s%(nNF)s%(tpEmis)s%(cNF)s%(cDV)s\"%{\n        'uf': CODIGOS_ESTADOS[self.uf],\n        'ano': self.data_emissao.strftime('%y'),\n        'mes': self.data_emissao.strftime('%m'),\n        'cnpj': so_numeros(self.emitente.cnpj),\n        'mod': self.modelo,\n        'serie': str(self.serie).zfill(3),\n        'nNF': str(self.numero_nf).zfill(9),\n        'tpEmis': str(self.forma_emissao),\n        'cNF': self.codigo_numerico_aleatorio.zfill(8),\n        'cDV': self.dv_codigo_numerico_aleatorio,\n        }", "path": "pynfe\\entidades\\notafiscal.py", "repo_name": "marinho/PyNFe", "stars": 127, "license": "other", "language": "python", "size": 508}
{"docstring": "''' Desativa as fun\u00e7\u00f5es criptogr\u00e1ficas e de an\u00e1lise XML\nAs fun\u00e7\u00f5es devem ser chamadas aproximadamente na ordem inversa da ativa\u00e7\u00e3o\n'''\n\n# Shutdown xmlsec-crypto library\n", "func_signal": "def _desativar_funcoes_criptograficas(self):\n", "code": "xmlsec.cryptoShutdown()\n\n# Shutdown crypto library\nxmlsec.cryptoAppShutdown()\n\n# Shutdown xmlsec library\nxmlsec.shutdown()\n\n# Shutdown LibXML2 FIXME: descobrir forma de evitar o uso do libxml2 neste processo\nlibxml2.cleanupParser()", "path": "pynfe\\processamento\\assinatura.py", "repo_name": "marinho/PyNFe", "stars": 127, "license": "other", "language": "python", "size": 508}
{"docstring": "\"\"\" Test bar code with checksum \"\"\"\n", "func_signal": "def testWithChecksum():\n", "code": "bar = Code128()\nassert(bar.makeCode('HI345678')=='11010010000110001010001100010001010111011110100010110001110001011011000010100100001001101100011101011')", "path": "pynfe\\utils\\bar_code_128.py", "repo_name": "marinho/PyNFe", "stars": 127, "license": "other", "language": "python", "size": 508}
{"docstring": "# Inicializa vari\u00e1vel que armazena os objetos contidos na Fonte de Dados\n", "func_signal": "def __init__(self, objetos=None):\n", "code": "if objetos:\n    self._objetos = objetos\nelse:\n    self._objetos = []", "path": "pynfe\\entidades\\fonte_dados.py", "repo_name": "marinho/PyNFe", "stars": 127, "license": "other", "language": "python", "size": 508}
{"docstring": "# Carrega o XML do arquivo\n", "func_signal": "def assinar_arquivo(self, caminho_arquivo, salva=True):\n", "code": "raiz = etree.parse(caminho_arquivo)\n\n# Efetua a assinatura\nxml = self.assinar_etree(raiz, retorna_xml=True)\n\n# Grava XML assinado no arquivo\nif salva:\n    fp = file(caminho_arquivo, 'w')\n    fp.write(xml)\n    fp.close()\n\nreturn xml", "path": "pynfe\\processamento\\assinatura.py", "repo_name": "marinho/PyNFe", "stars": 127, "license": "other", "language": "python", "size": 508}
{"docstring": "'''Valida um documento lxml diretamente.\nArgumentos:\n    xml_doc - documento etree\n    xsd_file - caminho para o arquivo xsd\n    use_assert - levantar exce\u00e7\u00e3o caso documento n\u00e3o valide?\n'''\n", "func_signal": "def validar_etree(self, xml_doc, xsd_file, use_assert=False):\n", "code": "xsd_filepath = get_xsd(xsd_file)\n\ntry:\n    # checa se o schema ja existe no cache\n    xsd_schema = self.MEM_CACHE[xsd_filepath]\nexcept:\n    # l\u00ea xsd e atualiza cache\n    xsd_doc = etree.parse(xsd_filepath)\n    xsd_schema = etree.XMLSchema(xsd_doc)\n    self.MEM_CACHE[xsd_file] = xsd_schema\nreturn use_assert and xsd_schema.assertValid(xml_doc) \\\n       or xsd_schema.validate(xml_doc)", "path": "pynfe\\processamento\\validacao.py", "repo_name": "marinho/PyNFe", "stars": 127, "license": "other", "language": "python", "size": 508}
{"docstring": "# TODO: fazer UF ser opcional\n", "func_signal": "def obter_municipio_por_codigo(codigo, uf):\n", "code": "municipios = carregar_arquivo_municipios(uf)\n\nreturn municipios[codigo]", "path": "pynfe\\utils\\__init__.py", "repo_name": "marinho/PyNFe", "stars": 127, "license": "other", "language": "python", "size": 508}
{"docstring": "\"\"\" Test images generation with PIL \"\"\"\n", "func_signal": "def testImage():\n", "code": "bar = Code128()\nbar.getImage(\"9782212110708\",50,\"gif\")\nbar.getImage(\"978221211070\",50,\"png\")", "path": "pynfe\\utils\\bar_code_128.py", "repo_name": "marinho/PyNFe", "stars": 127, "license": "other", "language": "python", "size": 508}
{"docstring": "\"\"\" Get an image with PIL library \nvalue code barre value\nheight height in pixel of the bar code\nextension image file extension\"\"\"\n", "func_signal": "def getImage(self, value, height = 50, extension = \"PNG\"):\n", "code": "import Image, ImageFont, ImageDraw\nfrom string import lower, upper\n\n# Create a missing font file\ndecodeFontFile(courB08_pil ,\"courB08.pil\")\ndecodeFontFile(courB08_pbm ,\"courB08.pbm\")\n\n# Get the bar code list\nbits = self.makeCode(value)\n\n# Create a new image\nposition = 8\nim = Image.new(\"1\",(len(bits)+position,height))\n\n# Load font\nfont = ImageFont.load(\"courB08.pil\")\n\n# Create drawer\ndraw = ImageDraw.Draw(im)\n\n# Erase image\ndraw.rectangle(((0,0),(im.size[0],im.size[1])),fill=256)\n\n# Draw text\ndraw.text((0, height-9), value, font=font, fill=0)\n\n# Draw the bar codes\nfor bit in range(len(bits)):\n   if bits[bit] == '1':\n      draw.rectangle(((bit+position,0),(bit+position,height-10)),fill=0)\n      \n# Save the result image\nim.save(value+\".\"+lower(extension), upper(extension))", "path": "pynfe\\utils\\bar_code_128.py", "repo_name": "marinho/PyNFe", "stars": 127, "license": "other", "language": "python", "size": 508}
{"docstring": "# Libera a mem\u00f3ria do assinador; isso \u00e9 necess\u00e1rio, pois na verdade foi feita uma chamada\n# a uma fun\u00e7\u00e3o em C cujo c\u00f3digo n\u00e3o \u00e9 gerenciado pelo Python\n", "func_signal": "def _depois_de_assinar_ou_verificar(self, doc_xml, ctxt, assinador):\n", "code": "assinador.destroy()\nctxt.xpathFreeContext()\ndoc_xml.freeDoc()\n\n# E, por fim, desativa todas as fun\u00e7\u00f5es ativadas anteriormente\nself._desativar_funcoes_criptograficas()", "path": "pynfe\\processamento\\assinatura.py", "repo_name": "marinho/PyNFe", "stars": 127, "license": "other", "language": "python", "size": 508}
{"docstring": "\"\"\"\nImports all the carriers for current instance\n\n:param cursor: Database cursor\n:param user: ID of current user\n:param ids: List of ids of records for this model\n:param context: Application context\n\"\"\"\n", "func_signal": "def import_carriers(self, cursor, user, ids, context):\n", "code": "instance_obj = self.pool.get('magento.instance')\nmagento_carrier_obj = self.pool.get('magento.instance.carrier')\n\ninstance = instance_obj.browse(\n    cursor, user, context.get('active_id')\n)\ncontext.update({\n    'magento_instance': instance.id\n})\n\nwith OrderConfig(\n    instance.url, instance.api_user, instance.api_key\n) as order_config_api:\n    mag_carriers = order_config_api.get_shipping_methods()\n\nmagento_carrier_obj.create_all_using_magento_data(\n    cursor, user, mag_carriers, context\n)", "path": "wizard\\import_carriers.py", "repo_name": "openlabs/magento_integration", "stars": 81, "license": "None", "language": "python", "size": 4108}
{"docstring": "\"\"\"\nTests import of sale order with bundle product using magento data\nThis tests that the duplication of BoMs doesnot happen\n\"\"\"\n", "func_signal": "def test_0033_import_sale_order_with_bundle_product_check_duplicate(self):\n", "code": "sale_obj = POOL.get('sale.order')\npartner_obj = POOL.get('res.partner')\nproduct_obj = POOL.get('product.product')\ncategory_obj = POOL.get('product.category')\nmagento_order_state_obj = POOL.get('magento.order_state')\nwebsite_obj = POOL.get('magento.instance.website')\n\nwith Transaction().start(DB_NAME, USER, CONTEXT) as txn:\n    self.setup_defaults(txn)\n    context = deepcopy(CONTEXT)\n    context.update({\n        'magento_instance': self.instance_id1,\n        'magento_store_view': self.store_view_id,\n        'magento_website': self.website_id1,\n    })\n\n    website = website_obj.browse(\n        txn.cursor, txn.user, self.website_id1, txn.context\n    )\n\n    if settings.MOCK:\n        order_states = load_json('order-states', 'all')\n    else:\n        with OrderConfig(*settings.ARGS) as order_config_api:\n            order_states = order_config_api.get_states()\n\n    magento_order_state_obj.create_all_using_magento_data(\n        txn.cursor, txn.user, order_states, context=context\n    )\n\n    if settings.MOCK:\n        category_tree = load_json('categories', 'category_tree')\n    else:\n        with magento.Category(*settings.ARGS) as category_api:\n            category_tree = category_api.tree(\n                website.magento_root_category_id\n            )\n\n    category_obj.create_tree_using_magento_data(\n        txn.cursor, txn.user, category_tree, context\n    )\n\n    if settings.MOCK:\n        order_data = load_json('orders', '300000001')\n\n        with patch(\n            'magento.Customer', mock_customer_api(), create=True\n        ):\n            partner_obj.find_or_create_using_magento_id(\n                txn.cursor, txn.user, order_data['customer_id'],\n                context\n            )\n\n        # Create sale order using magento data\n        with patch('magento.Product', mock_product_api(), create=True):\n            order = sale_obj.find_or_create_using_magento_data(\n                txn.cursor, txn.user, order_data, context=context\n            )\n    else:\n        with magento.Order(*settings.ARGS) as order_api:\n            orders = [\n                order_api.info(order['increment_id'])\n                    for order in order_api.list()\n            ]\n            for order in orders:\n                if filter(\n                    lambda item: item['product_type'] == 'bundle',\n                    order['items']\n                ):\n                    order_data = order\n\n        partner_obj.find_or_create_using_magento_id(\n            txn.cursor, txn.user, order_data['customer_id'], context\n        )\n\n        # Create sale order using magento data\n        order = sale_obj.find_or_create_using_magento_data(\n            txn.cursor, txn.user, order_data, context=context\n        )\n\n    if settings.MOCK:\n        product_data = load_json('products', '158')\n    else:\n        with magento.Product(*settings.ARGS) as product_api:\n            product_list = product_api.list()\n            for product in product_list:\n                if product['type'] == 'bundle':\n                    product_data = product_api.info(\n                        product=product['product_id'],\n                    )\n                    break\n\n    # There should be a BoM for the bundle product\n    product = product_obj.find_or_create_using_magento_id(\n        txn.cursor, txn.user, product_data['product_id'], context\n    )\n    self.assertTrue(product.bom_ids)\n    self.assertEqual(\n        len(product.bom_ids[0].bom_lines),\n        len(order.order_line)\n    )\n\n    if settings.MOCK:\n        order_data = load_json('orders', '300000001-a')\n\n        # Create sale order using magento data\n        with patch('magento.Product', mock_product_api(), create=True):\n            order = sale_obj.find_or_create_using_magento_data(\n                txn.cursor, txn.user, order_data, context=context\n            )\n    else:\n        with magento.Order(*settings.ARGS) as order_api:\n            orders = [\n                order_api.info(order['increment_id'])\n                    for order in order_api.list()\n            ]\n            for order in orders:\n                for item in order['items']:\n                    if item['product_type'] == 'bundle':\n                        order_data = order\n                        break\n\n        order = sale_obj.find_or_create_using_magento_data(\n            txn.cursor, txn.user, order_data, context=context\n        )\n\n    # There should be a BoM for the bundle product\n    product = product_obj.find_or_create_using_magento_id(\n        txn.cursor, txn.user, product_data['product_id'], context\n    )\n    self.assertTrue(product.bom_ids)\n    self.assertTrue(\n        len(product.bom_ids[0].bom_lines), len(order.order_line)\n    )", "path": "tests\\test_sale.py", "repo_name": "openlabs/magento_integration", "stars": 81, "license": "None", "language": "python", "size": 4108}
{"docstring": "\"\"\"\nOpen view for sales imported from the magento store view\n\n:param cursor: Database cursor\n:param user: ID of current user\n:param sale_ids: List of sale ids\n:param context: Application context\n:return: Tree view for sales\n\"\"\"\n", "func_signal": "def open_sales(self, cursor, user, sale_ids, context):\n", "code": "ir_model_data = self.pool.get('ir.model.data')\n\ntree_res = ir_model_data.get_object_reference(\n    cursor, user, 'sale', 'view_order_tree'\n)\ntree_id = tree_res and tree_res[1] or False\n\nreturn {\n    'name': _('Magento Sale Orders'),\n    'view_type': 'form',\n    'view_mode': 'form,tree',\n    'res_model': 'sale.order',\n    'views': [(tree_id, 'tree')],\n    'context': context,\n    'type': 'ir.actions.act_window',\n    'domain': [('id', 'in', sale_ids)]\n}", "path": "wizard\\import_orders.py", "repo_name": "openlabs/magento_integration", "stars": 81, "license": "None", "language": "python", "size": 4108}
{"docstring": "\"\"\"\nTest the import and creation of sale order states for an instance when\norder state is unknown\n\"\"\"\n", "func_signal": "def test_0006_import_sale_order_states(self):\n", "code": "magento_order_state_obj = POOL.get('magento.order_state')\n\nwith Transaction().start(DB_NAME, USER, CONTEXT) as txn:\n    self.setup_defaults(txn)\n    context = deepcopy(CONTEXT)\n    context.update({\n        'magento_instance': self.instance_id1,\n    })\n\n    states_before_import = magento_order_state_obj.search(\n        txn.cursor, txn.user, [], context=context\n    )\n    states = magento_order_state_obj.create_all_using_magento_data(\n        txn.cursor, txn.user, {'something': 'something'},\n        context=context\n    )\n    states_after_import = magento_order_state_obj.search(\n        txn.cursor, txn.user, [], context=context\n    )\n\n    self.assertTrue(states_after_import > states_before_import)\n\n    for state in states:\n        self.assertEqual(\n            state.instance.id, context['magento_instance']\n        )", "path": "tests\\test_sale.py", "repo_name": "openlabs/magento_integration", "stars": 81, "license": "None", "language": "python", "size": 4108}
{"docstring": "\"\"\"\nTests import of sale order with tax\n\"\"\"\n", "func_signal": "def test_0039_import_sale_with_tax(self):\n", "code": "sale_obj = POOL.get('sale.order')\npartner_obj = POOL.get('res.partner')\ncategory_obj = POOL.get('product.category')\ntax_obj = POOL.get('account.tax')\nmagento_order_state_obj = POOL.get('magento.order_state')\nwebsite_obj = POOL.get('magento.instance.website')\n\nwith Transaction().start(DB_NAME, USER, CONTEXT) as txn:\n    self.setup_defaults(txn)\n    context = deepcopy(CONTEXT)\n    tax_obj.create(txn.cursor, txn.user, {\n        'name': 'VAT',\n        'amount': float('0.20'),\n        'used_on_magento': True\n    })\n    context.update({\n        'magento_instance': self.instance_id1,\n        'magento_store_view': self.store_view_id,\n        'magento_website': self.website_id1,\n    })\n\n    website = website_obj.browse(\n        txn.cursor, txn.user, self.website_id1, txn.context\n    )\n\n    if settings.MOCK:\n        order_states = load_json('order-states', 'all')\n    else:\n        with OrderConfig(*settings.ARGS) as order_config_api:\n            order_states = order_config_api.get_states()\n\n    magento_order_state_obj.create_all_using_magento_data(\n        txn.cursor, txn.user, order_states, context=context\n    )\n\n    if settings.MOCK:\n        category_tree = load_json('categories', 'category_tree')\n    else:\n        with magento.Category(*settings.ARGS) as category_api:\n            category_tree = category_api.tree(\n                website.magento_root_category_id\n            )\n    category_obj.create_tree_using_magento_data(\n        txn.cursor, txn.user, category_tree, context\n    )\n\n    if settings.MOCK:\n        order_data = load_json('orders', '100000005')\n\n        with patch(\n                'magento.Customer', mock_customer_api(), create=True):\n            partner_obj.find_or_create_using_magento_id(\n                txn.cursor, txn.user, order_data['customer_id'],\n                context\n            )\n\n        # Create sale order using magento data\n        with patch('magento.Product', mock_product_api(), create=True):\n            order = sale_obj.find_or_create_using_magento_data(\n                txn.cursor, txn.user, order_data, context=context\n            )\n    else:\n        with magento.Order(*settings.ARGS) as order_api:\n            orders = [\n                order_api.info(order['increment_id'])\n                    for order in order_api.list()\n            ]\n            for order in orders:\n                if order.get('tax_amount'):\n                    order_data = order\n                    break\n        partner_obj.find_or_create_using_magento_id(\n            txn.cursor, txn.user, order_data['customer_id'], context\n        )\n\n        # Create sale order using magento data\n        order = sale_obj.find_or_create_using_magento_data(\n            txn.cursor, txn.user, order_data, context=context\n        )\n\n    self.assertEqual(\n        order.amount_total, float(order_data['base_grand_total'])\n    )\n\n    # Item lines + shipping line should be equal to lines on openerp\n    self.assertEqual(\n        len(order.order_line), len(order_data['items']) + 1\n    )", "path": "tests\\test_sale.py", "repo_name": "openlabs/magento_integration", "stars": 81, "license": "None", "language": "python", "size": 4108}
{"docstring": "\"\"\"\nTests if currency can be searched using magento code\n\"\"\"\n", "func_signal": "def test_0010_search_currency_with_valid_code(self):\n", "code": "currency_obj = POOL.get('res.currency')\n\nwith Transaction().start(DB_NAME, USER, CONTEXT) as txn:\n\n    code = 'USD'\n\n    currency_id, = currency_obj.search(\n        txn.cursor, txn.user, [\n            ('name', '=', code)\n        ], context=txn.context\n    )\n\n    self.assertEqual(\n        currency_obj.search_using_magento_code(\n            txn.cursor, txn.user, code, txn.context\n        ).id,\n        currency_id\n    )", "path": "tests\\test_currency.py", "repo_name": "openlabs/magento_integration", "stars": 81, "license": "None", "language": "python", "size": 4108}
{"docstring": "\"\"\"\nImport sale orders from magento for the current store view.\n\n:param cursor: Database cursor\n:param user: ID of current user\n:param ids: List of ids of records for this model\n:param context: Application context\n\"\"\"\n", "func_signal": "def import_orders(self, cursor, user, ids, context):\n", "code": "store_view_obj = self.pool.get('magento.store.store_view')\n\nstore_view = store_view_obj.browse(\n    cursor, user, context.get('active_id')\n)\n\nsales = store_view_obj.import_orders_from_store_view(\n    cursor, user, store_view, context\n)\n\nreturn self.open_sales(cursor, user, map(int, sales), context)", "path": "wizard\\import_orders.py", "repo_name": "openlabs/magento_integration", "stars": 81, "license": "None", "language": "python", "size": 4108}
{"docstring": "\"\"\"\nSearches for country with given magento code.\n\n:param cursor: Database cursor\n:param user: ID of current user\n:param code: ISO code of country\n:param context: Application context\n:return: Browse record of country if found else raises error\n\"\"\"\n", "func_signal": "def search_using_magento_code(self, cursor, user, code, context):\n", "code": "country_ids = self.search(\n    cursor, user, [('code', '=', code)], context=context\n)\n\nif not country_ids:\n    raise osv.except_osv(\n        _('Not Found!'),\n        _('Country with ISO code %s does not exists.' % code)\n    )\n\ncountry = self.browse(\n    cursor, user, country_ids[0], context=context\n)\nreturn country", "path": "country.py", "repo_name": "openlabs/magento_integration", "stars": 81, "license": "None", "language": "python", "size": 4108}
{"docstring": "\"\"\"\nTest If all carriers are being imported from magento\n\"\"\"\n", "func_signal": "def test_0040_import_carriers(self):\n", "code": "magento_carrier_obj = POOL.get('magento.instance.carrier')\n\nwith Transaction().start(DB_NAME, USER, CONTEXT) as txn:\n    self.setup_defaults(txn)\n\n    context = deepcopy(CONTEXT)\n    context.update({\n        'magento_instance': self.instance_id1,\n    })\n\n    carriers_before_import = magento_carrier_obj.search(\n        txn.cursor, txn.user, [], context=context\n    )\n\n    if settings.MOCK:\n        mag_carriers = load_json('carriers', 'shipping_methods')\n    else:\n        with OrderConfig(*settings.ARGS) as order_config_api:\n            mag_carriers = order_config_api.get_shipping_methods()\n\n    carriers = magento_carrier_obj.create_all_using_magento_data(\n        txn.cursor, txn.user, mag_carriers, context=context\n    )\n    carriers_after_import = magento_carrier_obj.search(\n        txn.cursor, txn.user, [], context=context\n    )\n\n    self.assertTrue(carriers_after_import > carriers_before_import)\n    for carrier in carriers:\n        self.assertEqual(\n            carrier.instance.id, context['magento_instance']\n        )", "path": "tests\\test_sale.py", "repo_name": "openlabs/magento_integration", "stars": 81, "license": "None", "language": "python", "size": 4108}
{"docstring": "\"\"\"\nTests that if last shipment export time is there then shipment status\nare exported for shipments delivered after last shipment export time\n\"\"\"\n", "func_signal": "def test_0080_export_shipment_status_with_last_export_date_case2(self):\n", "code": "sale_obj = POOL.get('sale.order')\npartner_obj = POOL.get('res.partner')\ncategory_obj = POOL.get('product.category')\nmagento_order_state_obj = POOL.get('magento.order_state')\nstore_view_obj = POOL.get('magento.store.store_view')\ncarrier_obj = POOL.get('delivery.carrier')\nproduct_obj = POOL.get('product.product')\nmagento_carrier_obj = POOL.get('magento.instance.carrier')\npicking_obj = POOL.get('stock.picking')\nwebsite_obj = POOL.get('magento.instance.website')\n\nwith Transaction().start(DB_NAME, USER, CONTEXT) as txn:\n    self.setup_defaults(txn)\n    context = deepcopy(CONTEXT)\n    context.update({\n        'magento_instance': self.instance_id1,\n        'magento_store_view': self.store_view_id,\n        'magento_website': self.website_id1,\n    })\n\n    store_view = store_view_obj.browse(\n        txn.cursor, txn.user, self.store_view_id, txn.context\n    )\n\n    website = website_obj.browse(\n        txn.cursor, txn.user, self.website_id1, txn.context\n    )\n\n    if settings.MOCK:\n        order_states = load_json('order-states', 'all')\n    else:\n        with OrderConfig(*settings.ARGS) as order_config_api:\n            order_states = order_config_api.get_states()\n\n    magento_order_state_obj.create_all_using_magento_data(\n        txn.cursor, txn.user, order_states, context=context\n    )\n\n    if settings.MOCK:\n        category_tree = load_json('categories', 'category_tree')\n    else:\n        with magento.Category(*settings.ARGS) as category_api:\n            category_tree = category_api.tree(\n                website.magento_root_category_id\n            )\n\n    category_obj.create_tree_using_magento_data(\n        txn.cursor, txn.user, category_tree, context\n    )\n\n    if settings.MOCK:\n        order_data = load_json('orders', '100000001')\n        mag_carriers = load_json('carriers', 'shipping_methods')\n    else:\n        with magento.Order(*settings.ARGS) as order_api:\n            orders = order_api.list()\n            order_data = order_api.info(orders[0]['increment_id'])\n        with OrderConfig(*settings.ARGS) as order_config_api:\n            mag_carriers = order_config_api.get_shipping_methods()\n\n    if settings.MOCK:\n        with patch(\n                'magento.Customer', mock_customer_api(), create=True):\n            partner = partner_obj.find_or_create_using_magento_id(\n                txn.cursor, txn.user, order_data['customer_id'], context\n            )\n\n        # Create sale order using magento data\n        with patch(\n                'magento.Product', mock_product_api(), create=True):\n            order = sale_obj.find_or_create_using_magento_data(\n                txn.cursor, txn.user, order_data, context=context\n            )\n    else:\n        partner = partner_obj.find_or_create_using_magento_id(\n            txn.cursor, txn.user, order_data['customer_id'], context\n        )\n\n        # Create sale order using magento data\n        order = sale_obj.find_or_create_using_magento_data(\n            txn.cursor, txn.user, order_data, context=context\n        )\n\n    magento_carrier_obj.create_all_using_magento_data(\n        txn.cursor, txn.user, mag_carriers, context=context\n    )\n\n    product_id = product_obj.search(\n        txn.cursor, txn.user, [], context=context\n    )[0]\n\n    # Create carrier\n    carrier_id = carrier_obj.create(\n        txn.cursor, txn.user, {\n            'name': 'DHL',\n            'partner_id': partner.id,\n            'product_id': product_id,\n        }, context=context\n    )\n\n    # Set carrier for sale order\n    sale_obj.write(\n        txn.cursor, txn.user, order.id, {\n            'carrier_id': carrier_id\n        }, context=context\n    )\n    order = sale_obj.browse(\n        txn.cursor, txn.user, order.id, context\n    )\n\n    # Set picking as delivered\n    picking_obj.action_assign(\n        txn.cursor, txn.user, map(int, order.picking_ids)\n    )\n    picking_obj.action_process(\n        txn.cursor, txn.user, map(int, order.picking_ids)\n    )\n    picking_obj.action_done(\n        txn.cursor, txn.user, map(int, order.picking_ids)\n    )\n\n    pickings = picking_obj.browse(\n        txn.cursor, txn.user, map(int, order.picking_ids),\n        context=context\n    )\n\n    export_date = datetime.date.today() - relativedelta(days=1)\n    store_view_obj.write(\n        txn.cursor, txn.user, store_view.id, {\n            'last_shipment_export_time': export_date.strftime(\n                DEFAULT_SERVER_DATETIME_FORMAT\n            )\n        }, context=context\n    )\n    store_view = store_view_obj.browse(\n        txn.cursor, txn.user, store_view.id, context=context\n    )\n\n    # Since write date is greater than last shipment export time. It\n    # should export shipment status successfully\n    for picking in pickings:\n        self.assertTrue(\n            picking.write_date >= store_view.last_shipment_export_time\n        )\n\n    if settings.MOCK:\n        with patch(\n                'magento.Shipment', mock_shipment_api(), create=True):\n            # Export shipment status\n            store_view_obj.export_shipment_status_to_magento(\n                txn.cursor, txn.user, store_view, context=context\n            )\n    else:\n        store_view_obj.export_shipment_status_to_magento(\n            txn.cursor, txn.user, store_view, context=context\n        )", "path": "tests\\test_sale.py", "repo_name": "openlabs/magento_integration", "stars": 81, "license": "None", "language": "python", "size": 4108}
{"docstring": "\"\"\"\nOpen view for products for which tier prices have been exported\n\n:param cursor: Database cursor\n:param user: ID of current user\n:param product_ids: List of product ids\n:param context: Application context\n:return: Tree view for products\n\"\"\"\n", "func_signal": "def open_products(self, cursor, user, product_ids, context):\n", "code": "ir_model_data = self.pool.get('ir.model.data')\n\ntree_res = ir_model_data.get_object_reference(\n    cursor, user, 'product', 'product_product_tree_view'\n)\ntree_id = tree_res and tree_res[1] or False\n\nreturn {\n    'name': _('Products with tier prices exported to Magento'),\n    'view_type': 'form',\n    'view_mode': 'tree,form',\n    'res_model': 'product.product',\n    'views': [(tree_id, 'tree')],\n    'context': context,\n    'type': 'ir.actions.act_window',\n    'domain': [('id', 'in', product_ids)]\n}", "path": "wizard\\export_tier_prices.py", "repo_name": "openlabs/magento_integration", "stars": 81, "license": "None", "language": "python", "size": 4108}
{"docstring": "\"\"\"\nTests if Tracking information is being updated for shipments\n\"\"\"\n", "func_signal": "def test_0060_export_shipment_status_with_tracking_info(self):\n", "code": "sale_obj = POOL.get('sale.order')\npartner_obj = POOL.get('res.partner')\ncategory_obj = POOL.get('product.category')\nmagento_order_state_obj = POOL.get('magento.order_state')\nstore_view_obj = POOL.get('magento.store.store_view')\ncarrier_obj = POOL.get('delivery.carrier')\nproduct_obj = POOL.get('product.product')\nmagento_carrier_obj = POOL.get('magento.instance.carrier')\npicking_obj = POOL.get('stock.picking')\nwebsite_obj = POOL.get('magento.instance.website')\n\nwith Transaction().start(DB_NAME, USER, CONTEXT) as txn:\n    self.setup_defaults(txn)\n    context = deepcopy(CONTEXT)\n    context.update({\n        'magento_instance': self.instance_id1,\n        'magento_store_view': self.store_view_id,\n        'magento_website': self.website_id1,\n    })\n\n    store_view = store_view_obj.browse(\n        txn.cursor, txn.user, self.store_view_id, txn.context\n    )\n\n    website = website_obj.browse(\n        txn.cursor, txn.user, self.website_id1, txn.context\n    )\n\n    if settings.MOCK:\n        order_states = load_json('order-states', 'all')\n    else:\n        with OrderConfig(*settings.ARGS) as order_config_api:\n            order_states = order_config_api.get_states()\n\n    magento_order_state_obj.create_all_using_magento_data(\n        txn.cursor, txn.user, order_states, context=context\n    )\n\n    if settings.MOCK:\n        category_tree = load_json('categories', 'category_tree')\n    else:\n        with magento.Category(*settings.ARGS) as category_api:\n            category_tree = category_api.tree(\n                website.magento_root_category_id\n            )\n\n    category_obj.create_tree_using_magento_data(\n        txn.cursor, txn.user, category_tree, context\n    )\n\n    if settings.MOCK:\n        order_data = load_json('orders', '100000001')\n        mag_carriers = load_json('carriers', 'shipping_methods')\n\n        with patch(\n            'magento.Customer', mock_customer_api(), create=True\n        ):\n            partner = partner_obj.find_or_create_using_magento_id(\n                txn.cursor, txn.user, order_data['customer_id'],\n                context\n            )\n\n        # Create sale order using magento data\n        with patch('magento.Product', mock_product_api(), create=True):\n            order = sale_obj.find_or_create_using_magento_data(\n                txn.cursor, txn.user, order_data, context=context\n            )\n    else:\n        with magento.Order(*settings.ARGS) as order_api:\n            orders = [\n                order_api.info(order['increment_id'])\n                    for order in order_api.list()\n            ]\n            for order in orders:\n                for item in order['items']:\n                    if item['product_type'] == 'bundle':\n                        order_data = order\n                        break\n\n        partner = partner_obj.find_or_create_using_magento_id(\n            txn.cursor, txn.user, order_data['customer_id'], context\n        )\n\n        # Create sale order using magento data\n        order = sale_obj.find_or_create_using_magento_data(\n            txn.cursor, txn.user, order_data, context=context\n        )\n        with OrderConfig(*settings.ARGS) as order_config_api:\n            mag_carriers = order_config_api.get_shipping_methods()\n\n    magento_carrier_obj.create_all_using_magento_data(\n        txn.cursor, txn.user, mag_carriers, context=context\n    )\n\n    product_id = product_obj.search(\n        txn.cursor, txn.user, [], context=context\n    )[0]\n\n    # Create carrier\n    carrier_id = carrier_obj.create(\n        txn.cursor, txn.user, {\n            'name': 'DHL',\n            'partner_id': partner.id,\n            'product_id': product_id,\n        }, context=context\n    )\n\n    # Set carrier for sale order\n    sale_obj.write(\n        txn.cursor, txn.user, order.id, {\n            'carrier_id': carrier_id\n        }, context=context\n    )\n    order = sale_obj.browse(\n        txn.cursor, txn.user, order.id, context\n    )\n\n    # Set picking as delivered\n    picking_obj.action_assign(\n        txn.cursor, txn.user, map(int, order.picking_ids)\n    )\n    picking_obj.action_process(\n        txn.cursor, txn.user, map(int, order.picking_ids)\n    )\n    picking_obj.action_done(\n        txn.cursor, txn.user, map(int, order.picking_ids)\n    )\n\n    with patch('magento.Shipment', mock_shipment_api(), create=True):\n        # Export shipment status\n        shipments = store_view_obj.export_shipment_status_to_magento(\n            txn.cursor, txn.user, store_view, context=context\n        )\n\n        # Export Tracking info\n        self.assertEqual(\n            store_view_obj.export_tracking_info_to_magento(\n                txn.cursor, txn.user, shipments[0], context=context\n            ),\n            True\n        )", "path": "tests\\test_sale.py", "repo_name": "openlabs/magento_integration", "stars": 81, "license": "None", "language": "python", "size": 4108}
{"docstring": "\"\"\"\nTests if shipments status is being exported for all the shipments\nrelated to store view\n\"\"\"\n", "func_signal": "def test_0050_export_shipment(self):\n", "code": "sale_obj = POOL.get('sale.order')\npartner_obj = POOL.get('res.partner')\ncategory_obj = POOL.get('product.category')\nmagento_order_state_obj = POOL.get('magento.order_state')\nstore_view_obj = POOL.get('magento.store.store_view')\ncarrier_obj = POOL.get('delivery.carrier')\nproduct_obj = POOL.get('product.product')\nmagento_carrier_obj = POOL.get('magento.instance.carrier')\npicking_obj = POOL.get('stock.picking')\nwebsite_obj = POOL.get('magento.instance.website')\n\nwith Transaction().start(DB_NAME, USER, CONTEXT) as txn:\n    self.setup_defaults(txn)\n    context = deepcopy(CONTEXT)\n    context.update({\n        'magento_instance': self.instance_id1,\n        'magento_store_view': self.store_view_id,\n        'magento_website': self.website_id1,\n    })\n\n    store_view = store_view_obj.browse(\n        txn.cursor, txn.user, self.store_view_id, txn.context\n    )\n\n    website = website_obj.browse(\n        txn.cursor, txn.user, self.website_id1, txn.context\n    )\n\n    if settings.MOCK:\n        order_states = load_json('order-states', 'all')\n    else:\n        with OrderConfig(*settings.ARGS) as order_config_api:\n            order_states = order_config_api.get_states()\n\n    magento_order_state_obj.create_all_using_magento_data(\n        txn.cursor, txn.user, order_states, context=context\n    )\n\n    if settings.MOCK:\n        category_tree = load_json('categories', 'category_tree')\n    else:\n        with magento.Category(*settings.ARGS) as category_api:\n            category_tree = category_api.tree(\n                website.magento_root_category_id\n            )\n\n    category_obj.create_tree_using_magento_data(\n        txn.cursor, txn.user, category_tree, context\n    )\n\n    if settings.MOCK:\n        order_data = load_json('orders', '100000001')\n        mag_carriers = load_json('carriers', 'shipping_methods')\n\n        with patch(\n            'magento.Customer', mock_customer_api(), create=True\n        ):\n            partner = partner_obj.find_or_create_using_magento_id(\n                txn.cursor, txn.user, order_data['customer_id'],\n                context\n            )\n\n        # Create sale order using magento data\n        with patch('magento.Product', mock_product_api(), create=True):\n            order = sale_obj.find_or_create_using_magento_data(\n                txn.cursor, txn.user, order_data, context=context\n            )\n    else:\n        with magento.Order(*settings.ARGS) as order_api:\n            orders = order_api.list()\n            order_data = order_api.info(orders[0]['increment_id'])\n        with OrderConfig(*settings.ARGS) as order_config_api:\n            mag_carriers = order_config_api.get_shipping_methods()\n\n        partner = partner_obj.find_or_create_using_magento_id(\n            txn.cursor, txn.user, order_data['customer_id'], context\n        )\n\n        # Create sale order using magento data\n        order = sale_obj.find_or_create_using_magento_data(\n            txn.cursor, txn.user, order_data, context=context\n        )\n\n    magento_carrier_obj.create_all_using_magento_data(\n        txn.cursor, txn.user, mag_carriers, context=context\n    )\n\n    product_id = product_obj.search(\n        txn.cursor, txn.user, [], context=context\n    )[0]\n\n    # Create carrier\n    carrier_id = carrier_obj.create(\n        txn.cursor, txn.user, {\n            'name': 'DHL',\n            'partner_id': partner.id,\n            'product_id': product_id,\n        }, context=context\n    )\n\n    # Set carrier for sale order\n    sale_obj.write(\n        txn.cursor, txn.user, order.id, {\n            'carrier_id': carrier_id\n        }, context=context\n    )\n    order = sale_obj.browse(\n        txn.cursor, txn.user, order.id, context\n    )\n\n    # Set picking as delivered\n    picking_obj.action_assign(\n        txn.cursor, txn.user, map(int, order.picking_ids)\n    )\n    picking_obj.action_process(\n        txn.cursor, txn.user, map(int, order.picking_ids)\n    )\n    picking_obj.action_done(\n        txn.cursor, txn.user, map(int, order.picking_ids)\n    )\n\n    pickings = picking_obj.browse(\n        txn.cursor, txn.user, map(int, order.picking_ids),\n        context=context\n    )\n\n    for picking in pickings:\n        self.assertFalse(picking.magento_increment_id)\n\n    if settings.MOCK:\n\n        with patch(\n                'magento.Shipment', mock_shipment_api(), create=True):\n            store_view_obj.export_shipment_status_to_magento(\n                txn.cursor, txn.user, store_view, context=context\n            )\n    else:\n        store_view_obj.export_shipment_status_to_magento(\n            txn.cursor, txn.user, store_view, context=context\n        )\n\n    pickings = picking_obj.browse(\n        txn.cursor, txn.user, map(int, order.picking_ids),\n        context=context\n    )\n\n    for picking in pickings:\n        self.assertTrue(picking.magento_increment_id)", "path": "tests\\test_sale.py", "repo_name": "openlabs/magento_integration", "stars": 81, "license": "None", "language": "python", "size": 4108}
{"docstring": "\"\"\"Test the import and creation of sale order states for an instance\n\"\"\"\n", "func_signal": "def test_0005_import_sale_order_states(self):\n", "code": "magento_order_state_obj = POOL.get('magento.order_state')\n\nwith Transaction().start(DB_NAME, USER, CONTEXT) as txn:\n    self.setup_defaults(txn)\n    context = deepcopy(CONTEXT)\n    context.update({\n        'magento_instance': self.instance_id1,\n    })\n\n    states_before_import = magento_order_state_obj.search(\n        txn.cursor, txn.user, [], context=context\n    )\n    if settings.MOCK:\n        order_states = load_json('order-states', 'all')\n    else:\n        with OrderConfig(*settings.ARGS) as order_config_api:\n            order_states = order_config_api.get_states()\n\n    states = magento_order_state_obj.create_all_using_magento_data(\n        txn.cursor, txn.user, order_states,\n        context=context\n    )\n\n    states_after_import = magento_order_state_obj.search(\n        txn.cursor, txn.user, [], context=context\n    )\n\n    self.assertTrue(states_after_import > states_before_import)\n\n    for state in states:\n        self.assertEqual(\n            state.instance.id, context['magento_instance']\n        )", "path": "tests\\test_sale.py", "repo_name": "openlabs/magento_integration", "stars": 81, "license": "None", "language": "python", "size": 4108}
{"docstring": "\"\"\"\nTests import of sale order using magento data\n\"\"\"\n", "func_signal": "def test_0010_import_sale_order_with_products(self):\n", "code": "sale_obj = POOL.get('sale.order')\npartner_obj = POOL.get('res.partner')\ncategory_obj = POOL.get('product.category')\nmagento_order_state_obj = POOL.get('magento.order_state')\nwebsite_obj = POOL.get('magento.instance.website')\n\nwith Transaction().start(DB_NAME, USER, CONTEXT) as txn:\n    self.setup_defaults(txn)\n    context = deepcopy(CONTEXT)\n    context.update({\n        'magento_instance': self.instance_id1,\n        'magento_store_view': self.store_view_id,\n        'magento_website': self.website_id1,\n    })\n\n    website = website_obj.browse(\n        txn.cursor, txn.user, self.website_id1, txn.context\n    )\n\n    if settings.MOCK:\n        order_states = load_json('order-states', 'all')\n    else:\n        with OrderConfig(*settings.ARGS) as order_config_api:\n            order_states = order_config_api.get_states()\n\n    magento_order_state_obj.create_all_using_magento_data(\n        txn.cursor, txn.user, order_states,\n        context=context\n    )\n\n    if settings.MOCK:\n        category_tree = load_json('categories', 'category_tree')\n    else:\n        with magento.Category(*settings.ARGS) as category_api:\n            category_tree = category_api.tree(\n                website.magento_root_category_id\n            )\n\n    category_obj.create_tree_using_magento_data(\n        txn.cursor, txn.user, category_tree, context\n    )\n\n    orders_before_import = sale_obj.search(\n        txn.cursor, txn.user, [], context=context\n    )\n\n    if settings.MOCK:\n        order_data = load_json('orders', '100000001')\n\n        with patch(\n                'magento.Customer', mock_customer_api(), create=True):\n            partner_obj.find_or_create_using_magento_id(\n                txn.cursor, txn.user, order_data['customer_id'], context\n            )\n\n        # Create sale order using magento data\n        with patch(\n                'magento.Product', mock_product_api(), create=True):\n            order = sale_obj.find_or_create_using_magento_data(\n                txn.cursor, txn.user, order_data, context=context\n            )\n    else:\n        with magento.Order(*settings.ARGS) as order_api:\n            orders = order_api.list()\n            order_data = order_api.info(orders[0]['increment_id'])\n\n        partner_obj.find_or_create_using_magento_id(\n            txn.cursor, txn.user, order_data['customer_id'], context\n        )\n        order = sale_obj.find_or_create_using_magento_data(\n            txn.cursor, txn.user, order_data, context=context\n        )\n\n    self.assertEqual(order.state, 'manual')\n\n    orders_after_import = sale_obj.search(\n        txn.cursor, txn.user, [], context=context\n    )\n    self.assertTrue(orders_after_import > orders_before_import)\n\n    # Item lines + shipping line should be equal to lines on openerp\n    self.assertEqual(\n        len(order.order_line), len(order_data['items']) + 1\n    )\n\n    self.assertEqual(\n        order.amount_total, float(order_data['base_grand_total'])\n    )", "path": "tests\\test_sale.py", "repo_name": "openlabs/magento_integration", "stars": 81, "license": "None", "language": "python", "size": 4108}
{"docstring": "\"\"\"\nTests import of sale order with bundle product using magento data\n\"\"\"\n", "func_signal": "def test_0030_import_sale_order_with_bundle_product(self):\n", "code": "sale_obj = POOL.get('sale.order')\npartner_obj = POOL.get('res.partner')\nproduct_obj = POOL.get('product.product')\ncategory_obj = POOL.get('product.category')\nmagento_order_state_obj = POOL.get('magento.order_state')\nwebsite_obj = POOL.get('magento.instance.website')\n\nwith Transaction().start(DB_NAME, USER, CONTEXT) as txn:\n    self.setup_defaults(txn)\n    context = deepcopy(CONTEXT)\n    lines = []\n    context.update({\n        'magento_instance': self.instance_id1,\n        'magento_store_view': self.store_view_id,\n        'magento_website': self.website_id1,\n    })\n\n    website = website_obj.browse(\n        txn.cursor, txn.user, self.website_id1, txn.context\n    )\n\n    if settings.MOCK:\n        order_states = load_json('order-states', 'all')\n    else:\n        with OrderConfig(*settings.ARGS) as order_config_api:\n            order_states = order_config_api.get_states()\n\n    magento_order_state_obj.create_all_using_magento_data(\n        txn.cursor, txn.user, order_states,\n        context=context\n    )\n\n    if settings.MOCK:\n        category_tree = load_json('categories', 'category_tree')\n    else:\n        with magento.Category(*settings.ARGS) as category_api:\n            category_tree = category_api.tree(\n                website.magento_root_category_id\n            )\n\n    category_obj.create_tree_using_magento_data(\n        txn.cursor, txn.user, category_tree, context\n    )\n\n    orders_before_import = sale_obj.search(\n        txn.cursor, txn.user, [], context=context\n    )\n\n    if settings.MOCK:\n        order_data = load_json('orders', '300000001')\n\n        with patch(\n            'magento.Customer', mock_customer_api(), create=True\n        ):\n            partner_obj.find_or_create_using_magento_id(\n                txn.cursor, txn.user, order_data['customer_id'],\n                context\n            )\n\n        # Create sale order using magento data\n        with patch('magento.Product', mock_product_api(), create=True):\n            order = sale_obj.find_or_create_using_magento_data(\n                txn.cursor, txn.user, order_data, context=context\n            )\n    else:\n        with magento.Order(*settings.ARGS) as order_api:\n            orders = [\n                order_api.info(order['increment_id'])\n                    for order in order_api.list()\n            ]\n            for order in orders:\n                if filter(\n                    lambda item: item['product_type'] == 'bundle',\n                    order['items']\n                ):\n                    order_data = order\n\n        partner_obj.find_or_create_using_magento_id(\n            txn.cursor, txn.user, order_data['customer_id'], context\n        )\n        order = sale_obj.find_or_create_using_magento_data(\n            txn.cursor, txn.user, order_data, context=context\n        )\n\n    self.assertEqual(order.state, 'manual')\n    self.assertTrue('bundle' in order.order_line[0].magento_notes)\n\n    orders_after_import = sale_obj.search(\n        txn.cursor, txn.user, [], context=context\n    )\n    self.assertTrue(orders_after_import > orders_before_import)\n\n    for item in order_data['items']:\n        if not item['parent_item_id']:\n            lines.append(item)\n\n        # If the product is a child product of a bundle product, do not\n        # create a separate line for this.\n        if 'bundle_option' in item['product_options'] and \\\n                item['parent_item_id']:\n            continue\n\n    # Item lines + shipping line should be equal to lines on openerp\n    self.assertEqual(len(order.order_line), len(lines) + 1)\n\n    self.assertEqual(\n        order.amount_total, float(order_data['base_grand_total'])\n    )\n\n    if settings.MOCK:\n        product_data = load_json('products', '158')\n    else:\n        with magento.Product(*settings.ARGS) as product_api:\n            product_list = product_api.list()\n            for product in product_list:\n                if product['type'] == 'simple':\n                    product_data = product_api.info(\n                        product=product['product_id'],\n                    )\n\n    # There should be a BoM for the bundle product\n    product = product_obj.find_or_create_using_magento_id(\n        txn.cursor, txn.user, product_data['product_id'], context\n    )\n    self.assertTrue(product.bom_ids)\n    self.assertEqual(\n        len(product.bom_ids[0].bom_lines), len(order.order_line)\n    )\n\n    self.assertEqual(\n        len(order.picking_ids[0].move_lines), len(order.order_line)\n    )", "path": "tests\\test_sale.py", "repo_name": "openlabs/magento_integration", "stars": 81, "license": "None", "language": "python", "size": 4108}
{"docstring": "\"\"\"\nTests finding and creating order using increment id\n\"\"\"\n", "func_signal": "def test_0020_find_or_create_order_using_increment_id(self):\n", "code": "sale_obj = POOL.get('sale.order')\npartner_obj = POOL.get('res.partner')\ncategory_obj = POOL.get('product.category')\nmagento_order_state_obj = POOL.get('magento.order_state')\nwebsite_obj = POOL.get('magento.instance.website')\n\nwith Transaction().start(DB_NAME, USER, CONTEXT) as txn:\n    self.setup_defaults(txn)\n    context = deepcopy(CONTEXT)\n    context.update({\n        'magento_instance': self.instance_id1,\n        'magento_store_view': self.store_view_id,\n        'magento_website': self.website_id1,\n    })\n\n    website = website_obj.browse(\n        txn.cursor, txn.user, self.website_id1, txn.context\n    )\n\n    if settings.MOCK:\n        order_states = load_json('order-states', 'all')\n    else:\n        with OrderConfig(*settings.ARGS) as order_config_api:\n            order_states = order_config_api.get_states()\n\n    magento_order_state_obj.create_all_using_magento_data(\n        txn.cursor, txn.user, order_states,\n        context=context\n    )\n\n    if settings.MOCK:\n        category_tree = load_json('categories', 'category_tree')\n    else:\n        with magento.Category(*settings.ARGS) as category_api:\n            category_tree = category_api.tree(\n                website.magento_root_category_id\n            )\n\n    category_obj.create_tree_using_magento_data(\n        txn.cursor, txn.user, category_tree, context\n    )\n\n    orders_before_import = sale_obj.search(\n        txn.cursor, txn.user, [], context=context\n    )\n\n    if settings.MOCK:\n        order_data = load_json('orders', '100000001')\n\n        with patch(\n                'magento.Customer', mock_customer_api(), create=True):\n            partner_obj.find_or_create_using_magento_id(\n                txn.cursor, txn.user, order_data['customer_id'],\n                context\n            )\n\n        # Create sale order using magento increment_id\n        with nested(\n            patch('magento.Product', mock_product_api(), create=True),\n            patch('magento.Order', mock_order_api(), create=True),\n        ):\n            order = sale_obj.find_or_create_using_magento_increment_id(\n                txn.cursor, txn.user, order_data['increment_id'],\n                context=context\n            )\n    else:\n        with magento.Order(*settings.ARGS) as order_api:\n            orders = order_api.list()\n            order_data = order_api.info(orders[0]['increment_id'])\n\n        partner_obj.find_or_create_using_magento_id(\n            txn.cursor, txn.user, order_data['customer_id'],\n            context\n        )\n        order = sale_obj.find_or_create_using_magento_increment_id(\n            txn.cursor, txn.user, order_data['increment_id'],\n            context=context\n        )\n\n    orders_after_import = sale_obj.search(\n        txn.cursor, txn.user, [], context=context\n    )\n    self.assertTrue(orders_after_import > orders_before_import)\n\n    # Item lines + shipping line should be equal to lines on openerp\n    self.assertEqual(\n        len(order.order_line), len(order_data['items']) + 1\n    )\n    self.assertEqual(\n        order.amount_total, float(order_data['base_grand_total'])\n    )", "path": "tests\\test_sale.py", "repo_name": "openlabs/magento_integration", "stars": 81, "license": "None", "language": "python", "size": 4108}
{"docstring": "\"\"\"\nTest suite\n\"\"\"\n", "func_signal": "def suite():\n", "code": "_suite = unittest.TestSuite()\n_suite.addTests([\n    unittest.TestLoader().loadTestsFromTestCase(TestCurrency),\n])\nreturn _suite", "path": "tests\\test_currency.py", "repo_name": "openlabs/magento_integration", "stars": 81, "license": "None", "language": "python", "size": 4108}
{"docstring": "\"\"\"\nTests if error is raised for searching currency with invalid code\n\"\"\"\n", "func_signal": "def test_0020_search_currency_with_invalid_code(self):\n", "code": "currency_obj = POOL.get('res.currency')\n\nwith Transaction().start(DB_NAME, USER, CONTEXT) as txn:\n\n    code = 'abc'\n\n    with self.assertRaises(Exception):\n        currency_obj.search_using_magento_code(\n            txn.cursor, txn.user, code, txn.context\n        )", "path": "tests\\test_currency.py", "repo_name": "openlabs/magento_integration", "stars": 81, "license": "None", "language": "python", "size": 4108}
{"docstring": "\"\"\"\nTests import of sale order with bundle product using magento data\nOne of the children of the bundle is bought separately too\nMake sure that the lines are created correctly\n\"\"\"\n", "func_signal": "def test_0036_import_sale_with_bundle_plus_child_separate(self):\n", "code": "sale_obj = POOL.get('sale.order')\npartner_obj = POOL.get('res.partner')\ncategory_obj = POOL.get('product.category')\nmagento_order_state_obj = POOL.get('magento.order_state')\nwebsite_obj = POOL.get('magento.instance.website')\n\nwith Transaction().start(DB_NAME, USER, CONTEXT) as txn:\n    self.setup_defaults(txn)\n    context = deepcopy(CONTEXT)\n    lines = []\n    context.update({\n        'magento_instance': self.instance_id1,\n        'magento_store_view': self.store_view_id,\n        'magento_website': self.website_id1,\n    })\n\n    website = website_obj.browse(\n        txn.cursor, txn.user, self.website_id1, txn.context\n    )\n\n    if settings.MOCK:\n        order_states = load_json('order-states', 'all')\n    else:\n        with OrderConfig(*settings.ARGS) as order_config_api:\n            order_states = order_config_api.get_states()\n\n    magento_order_state_obj.create_all_using_magento_data(\n        txn.cursor, txn.user, order_states, context=context\n    )\n\n    if settings.MOCK:\n        category_tree = load_json('categories', 'category_tree')\n    else:\n        with magento.Category(*settings.ARGS) as category_api:\n            category_tree = category_api.tree(\n                website.magento_root_category_id\n            )\n\n    category_obj.create_tree_using_magento_data(\n        txn.cursor, txn.user, category_tree, context\n    )\n\n    if settings.MOCK:\n        order_data = load_json('orders', '100000004')\n\n        with patch(\n            'magento.Customer', mock_customer_api(), create=True\n        ):\n            partner_obj.find_or_create_using_magento_id(\n                txn.cursor, txn.user, order_data['customer_id'],\n                context\n            )\n\n        # Create sale order using magento data\n        with patch('magento.Product', mock_product_api(), create=True):\n            order = sale_obj.find_or_create_using_magento_data(\n                txn.cursor, txn.user, order_data, context=context\n            )\n    else:\n        with magento.Order(*settings.ARGS) as order_api:\n            orders = [\n                order_api.info(order['increment_id'])\n                    for order in order_api.list()\n            ]\n            for order in orders:\n                if filter(\n                    lambda item: item['product_type'] == 'bundle',\n                    order['items']\n                ):\n                    order_data = order\n\n        partner_obj.find_or_create_using_magento_id(\n            txn.cursor, txn.user, order_data['customer_id'], context\n        )\n\n        # Create sale order using magento data\n        order = sale_obj.find_or_create_using_magento_data(\n            txn.cursor, txn.user, order_data, context=context\n        )\n\n    self.assertEqual(\n        order.amount_total, float(order_data['base_grand_total'])\n    )\n\n    for item in order_data['items']:\n        if not item['parent_item_id']:\n            lines.append(item)\n\n        # If the product is a child product of a bundle product, do not\n        # create a separate line for this.\n        if 'bundle_option' in item['product_options'] and \\\n                item['parent_item_id']:\n            continue\n\n    # Item lines + shipping line should be equal to lines on openerp\n    self.assertEqual(\n        len(order.order_line), len(lines) + 1\n    )", "path": "tests\\test_sale.py", "repo_name": "openlabs/magento_integration", "stars": 81, "license": "None", "language": "python", "size": 4108}
{"docstring": "\"\"\"Unregister hook by name.\n\n:param name: name of the hook to unregister, or a\n    :class:`durian.event.Hook` class with a valid ``name`` attribute.\n\n:raises celery.exceptions.NotRegistered: if the hook has not\n    been registered.\n\n\"\"\"\n", "func_signal": "def unregister(self, name):\n", "code": "if hasattr(name, \"name\"):\n    name = name.name\nif name not in self.data:\n    raise self.NotRegistered(\n            \"Hook with name %s is not registered.\" % name)\ndel self.data[name]", "path": "durian\\registry.py", "repo_name": "ask/durian", "stars": 64, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\"Add listener with an instantiated :attr:`config_form`.\n\n:param form: An instance of :attr:`config_form`.\n:param match: Optional event filter match dict.\n\n\"\"\"\n", "func_signal": "def add_listener_by_form(self, form, match=None):\n", "code": "if not hasattr(form, \"cleaned_data\"):\n    form.is_valid()\nconfig = dict(form.cleaned_data)\nurl = config.pop(\"url\")\nreturn Listener.objects.create(hook=self.name, url=url,\n                               match=match, config=config)", "path": "durian\\event.py", "repo_name": "ask/durian", "stars": 64, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\"Simple listener destination URL to dump out the payload and\nrequest to stderr.\"\"\"\n", "func_signal": "def debug(request):\n", "code": "sys.stderr.write(str(request.get_full_path()) + \"\\n\")\nsys.stderr.write(str(request.raw_post_data) + \"\\n\")\nsys.stderr.write(str(request.POST) + \"\\n\")\nsys.stderr.write(str(deserialize(request.raw_post_data)) + \"\\n\")\nreturn HttpResponse(\"Thanks!\")", "path": "durian\\views.py", "repo_name": "ask/durian", "stars": 64, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\"Converts a list of ``(name, kind, what)`` tuples to a match dict.\nWhere name is the field to match, kind is a matchable number mapping to\n``CONST_TO_MATCHABLE``.\n\nPossible types are: ``CONDITION_EXACT``, ``CONDITION_STARTSWITH``,\n                    ``CONDITION_ENDSWITH, ``CONDITION_CONTAINS``.\n\n\n\nProbably best explained by an example:\n\n    >>> mtuplelist = [(\"name\", CONDITION_ENDSWITH, \"Constanza\"),\n    ...               (\"zipcode\", CONDITION_STARTSWITH, \"70\")]\n    >>> mtuplelist_to_matchdict(mtuplelist)\n    {\"name\": Endswith(\"Constanza\"), \"zipcode\": Startswith(\"70\")}\n\n\"\"\"\n", "func_signal": "def mtuplelist_to_matchdict(mtuplelist):\n", "code": "return dict((name, const_to_matchable(kind, what) or what)\n                for name, kind, what in mtuplelist\n                    if int(kind) != CONDITION_PASS)", "path": "durian\\match\\__init__.py", "repo_name": "ask/durian", "stars": 64, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\"How we filter events.\n\n:param sender: The sender of the signal.\n\n:param payload: The signal data.\n\n:param match: The match dictionary, or ``None``.\n\n\"\"\"\n", "func_signal": "def event_filter(self, sender, payload, match):\n", "code": "if not match:\n    return True\nreturn deepmatch(match, payload)", "path": "durian\\event.py", "repo_name": "ask/durian", "stars": 64, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\"View to create a new hook.\"\"\"\n", "func_signal": "def create(request, template_name=\"durian/create_hook.html\"):\n", "code": "context = RequestContext(request)\nif request.method == \"POST\":\n    hook = get_hook_or_404(request.POST[\"type\"])\n    config_form = hook.config_form(request.POST)\n    if config_form.is_valid():\n        matchdict = hook.apply_match_forms(request.POST)\n        hook.add_listener_by_form(config_form, match=matchdict)\n        return HttpResponse(\"Listener Created!\")\nelse:\n    hook = get_hook_or_404(request.GET[\"type\"])\n    config_form = hook.config_form()\n\nmatch_forms = hook.get_match_forms()\ncontext[\"title\"] = _(\"Create %s Listener\" % (\n                            hook.verbose_name.capitalize()))\ncontext[\"hook_type\"] = hook.name\ncontext[\"hook_name\"] = hook.verbose_name\ncontext[\"match_forms\"] = match_forms\ncontext[\"config_form\"] = config_form\nreturn render_to_response(template_name, context_instance=context)", "path": "durian\\views.py", "repo_name": "ask/durian", "stars": 64, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\"Select hook to create.\"\"\"\n", "func_signal": "def select(request, template_name=\"durian/select_hook.html\"):\n", "code": "context = RequestContext(request)\ncontext[\"title\"] = _(\"Select event\")\ncontext[\"select_hook_form\"] = create_select_hook_form()\nreturn render_to_response(template_name, context_instance=context)", "path": "durian\\views.py", "repo_name": "ask/durian", "stars": 64, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\"Register a hook in the hook registry.\n\n:param hook: The hook to register.\n\n:raises AlreadyRegistered: if the task is already registered.\n\n\"\"\"\n# instantiate class if not already.\n", "func_signal": "def register(self, hook):\n", "code": "hook = hook() if isclass(hook) else hook\n\nname = hook.name\nif name in self.data:\n    raise self.AlreadyRegistered(\n            \"Hook with name %s is already registered.\" % name)\n\nself.data[name] = hook", "path": "durian\\registry.py", "repo_name": "ask/durian", "stars": 64, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\"With a needle dictionary, recursively match all its keys to\nthe haystack dictionary.\"\"\"\n", "func_signal": "def deepmatch(needle, haystack):\n", "code": "stream = deque([(needle, haystack)])\n\nwhile True:\n    atom_left, atom_right = stream.pop()\n    for key, value in atom_left.items():\n        if isinstance(value, dict):\n            if key not in atom_right:\n                return False\n            stream.append((value, atom_right[key]))\n        else:\n            if atom_right.get(key) != value:\n                return False\n    if not stream:\n        break\nreturn True", "path": "durian\\match\\strategy.py", "repo_name": "ask/durian", "stars": 64, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\"With a list of supported arguments, generate a list of match\nforms for these.\n\nE.g. if the supported arguments is ``[\"name\", \"address\"]``, it will\ngenerate forms like these::\n\n    Name: SELECT:[any|exact|starts with|ends with|contains] [ query ]\n    Address: SELECT:[any|exact|starts with|ends with|contains] [ query ]\n\nWhen these form are feeded with data they can be converted to a match\ndict like:\n\n    >>> {\"name\": Startswith(\"foo\"), \"address\": Endswith(\"New York\")}\n\n\"\"\"\n\n", "func_signal": "def create_match_forms(name, provides_args):\n", "code": "def gen_field_for_name(name):\n    return {\"%s_cond\" % name: forms.ChoiceField(label=name,\n                                          choices=MATCHABLE_CHOICES,\n                                          widget=forms.Select()),\n            \"%s_query\" % name: forms.CharField(label=\"\",\n                                               required=False,\n                                               initial=\"\"),\n    }\n\ndef gen_form_for_field(field):\n    dict_ = gen_field_for_name(field)\n    dict_[\"_condition_for\"] = field\n    return type(name + field, (BaseMatchForm, ), dict_)\n\nreturn dict((field, gen_form_for_field(field))\n                for field in provides_args)", "path": "durian\\forms.py", "repo_name": "ask/durian", "stars": 64, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\"The keyword arguments sent to the celery task.\"\"\"\n", "func_signal": "def task_keywords(self):\n", "code": "return {\"retry\": self.retry,\n        \"max_retries\": self.max_retries,\n        \"fail_silently\": self.fail_silently,\n        \"timeout\": self.timeout}", "path": "durian\\event.py", "repo_name": "ask/durian", "stars": 64, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\"Trigger hook by sending payload as POST data.\"\"\"\n\n", "func_signal": "def send(request, hook_type):\n", "code": "if request.method != \"POST\":\n    return HttpResponseNotAllowed(_(\"Method not allowed: %s\") % (\n        request.method))\n\npayload = dict(request.POST.copy())\nsender = request.META\n\nhook = get_hook_or_404(hook_type)\nhook.send(sender=sender, **payload)", "path": "durian\\views.py", "repo_name": "ask/durian", "stars": 64, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\"With data recieved by request, convert to a list of match\ntuples.\"\"\"\n", "func_signal": "def apply_match_forms(self, data):\n", "code": "mtuplelist = [match_form(data).field_to_mtuple()\n                 for match_form in self.match_forms.values()]\nreturn mtuplelist_to_matchdict(mtuplelist)", "path": "durian\\event.py", "repo_name": "ask/durian", "stars": 64, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\"Add listener for this signal.\n\n:param url: The url the listener is listening on.\n:keyword match: The even filter match dict.\n:keyword \\*\\*config: Hook specific listener configuration.\n\n\"\"\"\n", "func_signal": "def add_listener(self, url, match={}, **config):\n", "code": "return Listener.objects.create(hook=self, url=url, match=match,\n                               **dict(config))", "path": "durian\\event.py", "repo_name": "ask/durian", "stars": 64, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\"Dynamically create a form that has a ``ChoiceField`` listing all the\nhook types registered in the hook registry.\"\"\"\n\n", "func_signal": "def create_select_hook_form(*args, **kwargs):\n", "code": "class SelectHookForm(forms.Form):\n    type = forms.ChoiceField(choices=hooks.as_choices())\n\nreturn SelectHookForm(*args, **kwargs)", "path": "durian\\forms.py", "repo_name": "ask/durian", "stars": 64, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\"Return the hook registry as a choices tuple for use\nwithin Django models and forms.\"\"\"\n", "func_signal": "def as_choices(self):\n", "code": "dict_types = dict((type.name, type)\n                for type in self.data.values())\nsorted_names = sorted(dict_types.keys())\nreturn [(type.name, type.verbose_name.capitalize())\n            for name, type in dict_types.items()]", "path": "durian\\registry.py", "repo_name": "ask/durian", "stars": 64, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\"Send signal and dispatch to all listeners.\n\n:param sender: The sender of the signal. Either a specific object\n    or ``None``.\n\n:param payload: The data to pass on to listeners. Usually the keys\n    described in :attr:`provides_args` and any additional keys you'd\n    want to provide.\n\n\"\"\"\n", "func_signal": "def send(self, sender, **payload):\n", "code": "payload = self.prepare_payload(sender, payload)\napply_ = curry(self._send_signal, sender, payload)\nreturn map(apply_, self.get_listeners(sender, payload))", "path": "durian\\event.py", "repo_name": "ask/durian", "stars": 64, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\"Initialize the match forms with data recived by a request.\n\n:returns: A list of instantiated match forms.\n\n\"\"\"\n", "func_signal": "def get_match_forms(self, **kwargs):\n", "code": "return [match_form(**kwargs)\n            for match_form in self.match_forms.values()]", "path": "durian\\event.py", "repo_name": "ask/durian", "stars": 64, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\"Get a list of all the listeners who wants this signal.\"\"\"\n", "func_signal": "def get_listeners(self, sender, payload):\n", "code": "possible_targets = Listener.objects.filter(hook=self.name)\nreturn [target for target in possible_targets\n            if self.event_filter(sender, payload, target.match)]", "path": "durian\\event.py", "repo_name": "ask/durian", "stars": 64, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\"Convert the form to a match tuple.\"\"\"\n", "func_signal": "def field_to_mtuple(self):\n", "code": "if not hasattr(self, \"cleaned_data\"):\n    if not self.is_valid():\n        # FIXME\n        raise Exception(\"FORM IS NOT VALID: %s\" % self.errors)\n\nfield = self._condition_for\nreturn (field,\n        self.cleaned_data[\"%s_cond\" % field],\n        self.cleaned_data[\"%s_query\" % field])", "path": "durian\\forms.py", "repo_name": "ask/durian", "stars": 64, "license": "None", "language": "python", "size": 197}
