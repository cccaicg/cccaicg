{"docstring": "\"\"\"\nWrites a PDF file with the given text to be used as an \"overlay\".\n\"\"\"\n", "func_signal": "def write_text_overlay(text):\n", "code": "try:\n    from reportlab.platypus import SimpleDocTemplate, Spacer, Paragraph\n    from reportlab.lib import styles, enums, colors, units\nexcept ImportError:\n    app.logger.debug(\"No report lab found, not generating text overlay\")\n    return None\n\nstyles = styles.getSampleStyleSheet()\nstyle = styles[\"Normal\"]\nstyle.fontSize = 100\nstyle.leading = 110\nstyle.alignment = enums.TA_CENTER\ngray = colors.slategrey\ngray.alpha = 0.5\nstyle.textColor = gray\n\nf = StringIO()\ndoc = SimpleDocTemplate(f)\ndoc.build([Spacer(1, 3.5 * units.inch), Paragraph(text, style)])\n\nreturn f", "path": "pdfserver\\util.py", "repo_name": "cburgmer/pdfserver", "stars": 17, "license": "other", "language": "python", "size": 386}
{"docstring": "# TODO get proper file name\n", "func_signal": "def _respond_with_pdf(output):\n", "code": "response = Response(content_type='application/pdf')\nresponse.headers.add('Content-Disposition',\n                     'attachment; filename=combined.pdf')\n\nresponse.data = output\n\napp.logger.debug(\"Wrote %d bytes\" % len(response.data))\n\nreturn response", "path": "pdfserver\\views.py", "repo_name": "cburgmer/pdfserver", "stars": 17, "license": "other", "language": "python", "size": 386}
{"docstring": "\"\"\"\nIterates over all divisors of the given value smaller or equal to the\nvalue's square root in decreasing order.\n\"\"\"\n", "func_signal": "def lower_divisor_iterator(value):\n", "code": "i = int(math.sqrt(value))\nwhile i > 0:\n    if value % i == 0:\n        yield i\n    i -= 1", "path": "pdfserver\\util.py", "repo_name": "cburgmer/pdfserver", "stars": 17, "license": "other", "language": "python", "size": 386}
{"docstring": "\"\"\"Clean up old results.\"\"\"\n", "func_signal": "def clean(cls):\n", "code": "oldest_keep_datetime = datetime.utcnow() - timedelta(\n                            seconds=app.config['TASK_RESULT_EXPIRES'])\nfor task_result in cls.all().filter('available =', False):\n    TaskResult.delete(task_result)\nfor task_result in cls.all().filter('created <', oldest_keep_datetime):\n    TaskResult.delete(task_result)", "path": "pdfserver\\gaetask.py", "repo_name": "cburgmer/pdfserver", "stars": 17, "license": "other", "language": "python", "size": 386}
{"docstring": "\"\"\"Creates the database\"\"\"\n", "func_signal": "def createdb():\n", "code": "print >>sys.stderr, \"Creating database...\",\n\nfrom pdfserver import models, faketask, database\ndatabase.init_db()\n\nprint >>sys.stderr, \"done\"", "path": "manage.py", "repo_name": "cburgmer/pdfserver", "stars": 17, "license": "other", "language": "python", "size": 386}
{"docstring": "# Make sure our Test string is available in the original document\n", "func_signal": "def test_build_result_is_downloadable(self):\n", "code": "pdf = PdfFileReader(self.get_pdf_stream())\nassert 'Test' in pdf.getPage(0).extractText()\n\nrv = self.app.get('/')\nself.assertEquals(rv.status_code, 200)\n\nrv = self.app.post('/handleform',\n                   data={'file': (self.get_pdf_stream(), 'test.pdf')})\n\nrv = self.combine_and_download()\n\npdf_download = PdfFileReader(StringIO(rv.data))\nself.assert_('Test' in pdf_download.getPage(0).extractText())\n\nself.clean_up()", "path": "pdfserver\\tests.py", "repo_name": "cburgmer/pdfserver", "stars": 17, "license": "other", "language": "python", "size": 386}
{"docstring": "# Make sure our Test string is available in the original document\n", "func_signal": "def test_watermark(self):\n", "code": "pdf = PdfFileReader(self.get_pdf_stream())\nassert 'Test' in pdf.getPage(0).extractText()\nassert 'TEST_WATERMARK' not in pdf.getPage(0).extractText()\n\nrv = self.combine_and_download(text_overlay='TEST_WATERMARK')\n\npdf_download = PdfFileReader(StringIO(rv.data))\nself.assert_('Test' in pdf_download.getPage(0).extractText())\nself.assert_('TEST_WATERMARK' in pdf_download.getPage(0).extractText())\n\nself.clean_up()", "path": "pdfserver\\tests.py", "repo_name": "cburgmer/pdfserver", "stars": 17, "license": "other", "language": "python", "size": 386}
{"docstring": "\"\"\"Return a mapping for each id given by POST data.\n\nAppends missing ids to the end of the given order.\n\"\"\"\n", "func_signal": "def _order_files(files):\n", "code": "files_order = []\n\nfile_id_map = dict((upload.id, upload) for upload in files)\n\n# Get user selected order from form\norder = request.form.getlist('file[]')\nfor id in order:\n    try:\n        id = int(id)\n    except ValueError:\n        continue\n    if id and id in file_id_map:\n        files_order.append(file_id_map[id])\n        del file_id_map[id]\n\n# Append missing ids\nfiles_order.extend(file_id_map.values())\n\nreturn files_order", "path": "pdfserver\\views.py", "repo_name": "cburgmer/pdfserver", "stars": 17, "license": "other", "language": "python", "size": 386}
{"docstring": "# Make sure our Test string is available in the original document\n", "func_signal": "def test_rotation_different_to_unrotated(self):\n", "code": "pdf = PdfFileReader(self.get_pdf_stream())\nassert 'Test' in pdf.getPage(0).extractText()\n\nrv = self.app.get('/')\nself.assertEquals(rv.status_code, 200)\n\nrv = self.app.post('/handleform',\n                   data={'file': (self.get_pdf_stream(), 'test.pdf')})\n\n# Start build without rotation\nrv = self.combine_and_download()\ncontent_no_rotation = rv.data\n\n# Start build with rotation\nrv = self.combine_and_download(rotate='180')\ncontent = rv.data\n\nself.assert_(content_no_rotation != content)\n\nself.clean_up()", "path": "pdfserver\\tests.py", "repo_name": "cburgmer/pdfserver", "stars": 17, "license": "other", "language": "python", "size": 386}
{"docstring": "\"\"\"\nNon-Javascript result page\n\"\"\"\n", "func_signal": "def result_page(task_id):\n", "code": "if task_id not in session.get('tasks', []):\n    app.logger.debug(\"Valid tasks %r\" % session.get('tasks', []))\n    raise NotFound()\n\nparam = {'task_id': task_id,\n         'ready': handle_pdfs_task.AsyncResult(task_id).ready()}\nreturn render_template('download.html', **param)", "path": "pdfserver\\views.py", "repo_name": "cburgmer/pdfserver", "stars": 17, "license": "other", "language": "python", "size": 386}
{"docstring": "# Make sure our Test string is available in the original document\n", "func_signal": "def test_two_documents(self):\n", "code": "pdf = PdfFileReader(self.get_pdf_stream())\nassert 'Test' in pdf.getPage(0).extractText()\nassert pdf.getNumPages() == 1\n\nrv = self.app.get('/')\nself.assertEquals(rv.status_code, 200)\n\nrv = self.app.post('/handleform',\n                   data={'file': (self.get_pdf_stream(), 'test.pdf')})\nrv = self.app.post('/handleform',\n                   data={'file': (self.get_pdf_stream(), 'test2.pdf')})\n\nrv = self.combine_and_download(pages_sheet='2')\n\npdf_download = PdfFileReader(StringIO(rv.data))\nself.assert_('Test' in pdf_download.getPage(0).extractText())\nself.assertEquals(pdf_download.getNumPages(), 2)\n\nself.clean_up()", "path": "pdfserver\\tests.py", "repo_name": "cburgmer/pdfserver", "stars": 17, "license": "other", "language": "python", "size": 386}
{"docstring": "# Build a document with two pages\n", "func_signal": "def test_two_on_one_page(self):\n", "code": "pdf = PdfFileReader(self.get_pdf_stream())\noutput = PdfFileWriter()\noutput.addPage(pdf.getPage(0))\noutput.addPage(pdf.getPage(0))\nassert output.getNumPages() == 2\nassert output.getPage(0).extractText().count('Test') ==  1\nbuf = StringIO()\noutput.write(buf)\nbuf.seek(0)\n\nrv = self.app.get('/')\nself.assertEquals(rv.status_code, 200)\n\nrv = self.app.post('/handleform',\n                   data={'file': (buf, 'test.pdf')})\n\nrv = self.combine_and_download(pages_sheet='2')\n\npdf_download = PdfFileReader(StringIO(rv.data))\nself.assertEquals(pdf_download.getPage(0).extractText().count('Test'),\n                  2)\nself.assertEquals(pdf_download.getNumPages(), 1)\n\nself.clean_up()", "path": "pdfserver\\tests.py", "repo_name": "cburgmer/pdfserver", "stars": 17, "license": "other", "language": "python", "size": 386}
{"docstring": "# Make sure our Test string is available in the original document\n", "func_signal": "def test_rotation_identity(self):\n", "code": "pdf = PdfFileReader(self.get_pdf_stream())\nassert 'Test' in pdf.getPage(0).extractText()\n\nrv = self.app.get('/')\nself.assertEquals(rv.status_code, 200)\n\nrv = self.app.post('/handleform',\n                   data={'file': (self.get_pdf_stream(), 'test.pdf')})\n\n# Start build without rotation\nrv = self.combine_and_download(rotate='360')\ncontent_full = rv.data\n\n# Start build with rotation\nrv = self.combine_and_download(rotate='180')\ncontent_half = rv.data\n\nself.clean_up()\n\n# Upload rotated\nrv = self.app.post('/handleform',\n                   data={'file': (StringIO(content_half), 'test.pdf')})\n\n# Start build with rotation\nrv = self.combine_and_download(rotate='180')\ncontent_two_half = rv.data\n\nself.assert_(content_two_half == content_full,\n             '\\n'.join(difflib.ndiff(content_two_half.split('\\n'),\n                                     content_full.split('\\n'))))\n\nself.clean_up()", "path": "pdfserver\\tests.py", "repo_name": "cburgmer/pdfserver", "stars": 17, "license": "other", "language": "python", "size": 386}
{"docstring": "\"\"\"\nCalculates the optimal placement on a sheet for the given number of pages.\n\nEstimates:\n    - Rotation (0\u00b0 / 90\u00b0)\n    - Scaling factor (< 1)\n    - Cell structure (width x height)\n\"\"\"\n", "func_signal": "def _get_optimal_placement(width, height, page_count):\n", "code": "rotation = scaling_factor = None\ncell_x_count = cell_y_count = None\n\n# Check all possible decompositions\n# TODO don't start at square root, as that would asume next to quadratic\n#   layout of the page, choose a decomposition next to the page's ratio\nfor lower_divisor in lower_divisor_iterator(page_count):\n    higher_divisor = page_count / lower_divisor\n\n    # Check A x B with A < B\n    rot1, scal1 = _get_rotation_scaling(width, height,\n                                        lower_divisor, higher_divisor)\n    # Check B x A with A < B\n    rot2, scal2 = _get_rotation_scaling(width, height,\n                                        higher_divisor, lower_divisor)\n\n    scal, rot, x, y = max((scal1, rot1, lower_divisor, higher_divisor),\n                          (scal2, rot2, higher_divisor, lower_divisor))\n\n    if scaling_factor is None or scaling_factor < scal:\n        scaling_factor = scal\n        rotation = rot\n        cell_x_count = x\n        cell_y_count = y\n    else:\n        # Once the scaling factor decreases in both directions we won't get\n        #   any better\n        break\n\nreturn rotation, scaling_factor, cell_x_count, cell_y_count", "path": "pdfserver\\util.py", "repo_name": "cburgmer/pdfserver", "stars": 17, "license": "other", "language": "python", "size": 386}
{"docstring": "\"\"\"Checks if the result hasn't expired yet.\"\"\"\n", "func_signal": "def available(self):\n", "code": "task_result = self._get_result_from_db()\nreturn task_result.is_available()", "path": "pdfserver\\gaetask.py", "repo_name": "cburgmer/pdfserver", "stars": 17, "license": "other", "language": "python", "size": 386}
{"docstring": "# Make sure our Test string is available in the original document\n", "func_signal": "def test_rotation_maintains_text(self):\n", "code": "pdf = PdfFileReader(self.get_pdf_stream())\nassert 'Test' in pdf.getPage(0).extractText()\n\nrv = self.app.get('/')\nself.assertEquals(rv.status_code, 200)\n\nrv = self.app.post('/handleform',\n                   data={'file': (self.get_pdf_stream(), 'test.pdf')})\n\nrv = self.combine_and_download(rotate='90')\n\npdf_download = PdfFileReader(StringIO(rv.data))\nself.assert_('Test' in pdf_download.getPage(0).extractText())\n\nself.clean_up()", "path": "pdfserver\\tests.py", "repo_name": "cburgmer/pdfserver", "stars": 17, "license": "other", "language": "python", "size": 386}
{"docstring": "\"\"\"Decorator for generating a task similar to Celery.\"\"\"\n", "func_signal": "def task(func):\n", "code": "@wraps(func, assigned=(\"__module__\", \"__name__\"))\ndef run(self, *args, **kwargs):\n    return func(*args, **kwargs)\n\n# TODO name should be set through class\n#   (see metaclass in celery.task.base.TaskType)\nname = func.__module__ + '.' + func.__name__ \ncls_dict = dict(run=run,\n                name=name,\n                __module__=func.__module__,\n                __doc__=func.__doc__)\nt = type(func.__name__, (GAETask, ), cls_dict)()\ntasks[name] = t\nreturn t", "path": "pdfserver\\gaetask.py", "repo_name": "cburgmer/pdfserver", "stars": 17, "license": "other", "language": "python", "size": 386}
{"docstring": "# Get user defined order\n", "func_signal": "def main_table():\n", "code": "files = _order_files(_get_uploads())\n\nreturn jsonify(content=render_template('uploads.html',\n                                       uploads=files))", "path": "pdfserver\\views.py", "repo_name": "cburgmer/pdfserver", "stars": 17, "license": "other", "language": "python", "size": 386}
{"docstring": "# Make sure our Test string is available in the original document\n", "func_signal": "def test_one_page(self):\n", "code": "pdf = PdfFileReader(self.get_pdf_stream())\nassert 'Test' in pdf.getPage(0).extractText()\nassert pdf.getNumPages() == 1\n\nrv = self.app.get('/')\nself.assertEquals(rv.status_code, 200)\n\nrv = self.app.post('/handleform',\n                   data={'file': (self.get_pdf_stream(), 'test.pdf')})\n\nrv = self.combine_and_download(pages_sheet='2')\n\npdf_download = PdfFileReader(StringIO(rv.data))\nself.assert_('Test' in pdf_download.getPage(0).extractText())\nself.assertEquals(pdf_download.getNumPages(), 1)\n\nself.clean_up()", "path": "pdfserver\\tests.py", "repo_name": "cburgmer/pdfserver", "stars": 17, "license": "other", "language": "python", "size": 386}
{"docstring": "# HACK\n", "func_signal": "def replace_text(cls, page, text, replace):\n", "code": "from pyPdf.pdf import ContentStream, PageObject\nfrom pyPdf.generic import TextStringObject, NameObject\ncontent = ContentStream(page[\"/Contents\"].getObject(), page.pdf)\nfor idx in range(len(content.operations)):\n    operands, operator = content.operations[idx]\n    if operator == 'Tj':\n        operands[0] = TextStringObject(operands[0].replace(text,\n                                                           replace))\nnew_page = PageObject.createBlankPage(page.pdf)\nnew_page.mergePage(page)\nnew_page[NameObject('/Contents')] = content\nreturn new_page", "path": "pdfserver\\tests.py", "repo_name": "cburgmer/pdfserver", "stars": 17, "license": "other", "language": "python", "size": 386}
{"docstring": "\"\"\"Make sure a content-type is set.\"\"\"\n", "func_signal": "def test_content_type(self):\n", "code": "body = {'foo': 'bar'}\nresp = self.resp_class(body=body)\nself.assert_(\n    resp.content_type.startswith('application/json'))", "path": "dream\\tests\\test_dream.py", "repo_name": "simplegeo/dream", "stars": 25, "license": "other", "language": "python", "size": 271}
{"docstring": "\"\"\"Make sure functions aren't decorated when added.\"\"\"\n", "func_signal": "def test_expose_method_decorates(self):\n", "code": "url, method = ('/shop', 'GET')\nold_len = len(self.app.map[method]._patterns)\nf = lambda request: None\nself.app.expose(url, method)(f)\nself.assert_(self.app.map[method]._patterns[url][1] is f)", "path": "dream\\tests\\test_dream.py", "repo_name": "simplegeo/dream", "stars": 25, "license": "other", "language": "python", "size": 271}
{"docstring": "\"\"\"Non-error exceptions shouldn't get mangled a traceback.\"\"\"\n", "func_signal": "def test_client_error_exceptions(self):\n", "code": "ex = exc.HTTPBadRequest()\nresp = self.app._mangle_response(Request(_env()), ex)\nself.assertTrue(resp is not ex)", "path": "dream\\tests\\test_dream.py", "repo_name": "simplegeo/dream", "stars": 25, "license": "other", "language": "python", "size": 271}
{"docstring": "\"\"\"Make sure webob exception statuses are preserved.\"\"\"\n", "func_signal": "def test_status(self):\n", "code": "not_found = exc.HTTPNotFound(\"Sorry\")\nresp = self.func(Request(_env()), not_found)\nself.assertEqual(not_found.status, resp.status)", "path": "dream\\tests\\test_dream.py", "repo_name": "simplegeo/dream", "stars": 25, "license": "other", "language": "python", "size": 271}
{"docstring": "\"\"\"Make sure the cookie is included.\"\"\"\n", "func_signal": "def test_has_cookie(self):\n", "code": "req = Request(_env())\nresp = self.func(req, Exception(\"foo\"))\nbody = json.loads(resp.body)\nself.assertTrue('request_id' in body)\nself.assertEqual(body['request_id'], req.id)", "path": "dream\\tests\\test_dream.py", "repo_name": "simplegeo/dream", "stars": 25, "license": "other", "language": "python", "size": 271}
{"docstring": "\"\"\"Make sure tracebacks are included when they're lists.\"\"\"\n", "func_signal": "def test_traceback_list(self):\n", "code": "ex = Exception(\"foo\")\nex.__traceback__ = traceback.extract_stack()\n\nresp = _debug_exception_to_reponse(Request(_env()), ex)\nbody = json.loads(resp.body)\nself.assertTrue('traceback' in body)\nself.assertNotEqual(body['traceback'], [\"No traceback available.\"])", "path": "dream\\tests\\test_dream.py", "repo_name": "simplegeo/dream", "stars": 25, "license": "other", "language": "python", "size": 271}
{"docstring": "\"\"\"Make sure patterns are added to the correct maps.\"\"\"\n", "func_signal": "def test_expose_method_maps(self):\n", "code": "url, method = ('/cheese', 'GET')\nold_len = len(self.app.map[method]._patterns)\nf = lambda request: None\nself.app.expose(url, method)(f)\nself.assert_(len(self.app.map[method]._patterns) > old_len)\nself.assert_(url in self.app.map[method]._patterns)\nself.assert_(self.app.map[method]._patterns[url][1] is f)", "path": "dream\\tests\\test_dream.py", "repo_name": "simplegeo/dream", "stars": 25, "license": "other", "language": "python", "size": 271}
{"docstring": "\"\"\"Make sure there is no traceback.\"\"\"\n", "func_signal": "def test_no_traceback(self):\n", "code": "resp = self.func(Request(_env()), Exception(\"foo\"))\nself.assertFalse('traceback' in json.loads(resp.body))", "path": "dream\\tests\\test_dream.py", "repo_name": "simplegeo/dream", "stars": 25, "license": "other", "language": "python", "size": 271}
{"docstring": "\"\"\"Make sure the original method is preserved.\"\"\"\n", "func_signal": "def test_expose_method(self):\n", "code": "f = lambda request: None\nf_ = self.app.expose('/test_expose')(f)\nself.assert_(f_ is f)", "path": "dream\\tests\\test_dream.py", "repo_name": "simplegeo/dream", "stars": 25, "license": "other", "language": "python", "size": 271}
{"docstring": "\"\"\"Make sure exceptions are handled properly.\"\"\"\n", "func_signal": "def test_exceptions(self):\n", "code": "exc = ValueError(\"Expected some cheese.\")\nresp = self.app._mangle_response(Request(_env()), exc)\nbody = json.loads(resp.body)\nself.assertTrue(body['detail'].startswith('Caught exception ' +\n                                          str(type(exc))))", "path": "dream\\tests\\test_dream.py", "repo_name": "simplegeo/dream", "stars": 25, "license": "other", "language": "python", "size": 271}
{"docstring": "\"\"\"Make sure the message from a non-HTTPException is elided.\"\"\"\n", "func_signal": "def test_exception_message(self):\n", "code": "msg = \"Something went terribly wrong\"\nresp = self.func(Request(_env()), ValueError(msg))\nself.assertNotEqual(json.loads(resp.body)['detail'], msg)", "path": "dream\\tests\\test_dream.py", "repo_name": "simplegeo/dream", "stars": 25, "license": "other", "language": "python", "size": 271}
{"docstring": "\"\"\"Make sure request objects are generated.\"\"\"\n", "func_signal": "def test_generates_request(self):\n", "code": "runs = []\napp = App()\n\n@app.expose(\"/foo\")\ndef test_f(request):\n    runs.append(True)\n    self.assert_(isinstance(request, Request))\n    return Response()\n\n(req, resp) = app.route({'REQUEST_METHOD': 'GET', 'PATH_INFO': \"/foo\"})\nself.assert_(len(runs) == 1)", "path": "dream\\tests\\test_dream.py", "repo_name": "simplegeo/dream", "stars": 25, "license": "other", "language": "python", "size": 271}
{"docstring": "\"\"\"Make sure there's error detail.\"\"\"\n", "func_signal": "def test_has_detail(self):\n", "code": "resp = self.func(Request(_env()), Exception(\"foo\"))\nbody = json.loads(resp.body)\nself.assertTrue('detail' in body)", "path": "dream\\tests\\test_dream.py", "repo_name": "simplegeo/dream", "stars": 25, "license": "other", "language": "python", "size": 271}
{"docstring": "\"\"\"Make sure the request uses Unicode.\"\"\"\n", "func_signal": "def test_unicode_request(self):\n", "code": "env = {'QUERY_STRING': 'q=\u00fc'}\napp = App()\n\n@app.expose(\"/foo\")\ndef __endpoint__(request):\n    self.assertTrue(isinstance(request.GET, webob.UnicodeMultiDict))\n\napp.route({'REQUEST_METHOD': 'GET',\n           'PATH_INFO': \"/foo\"})", "path": "dream\\tests\\test_dream.py", "repo_name": "simplegeo/dream", "stars": 25, "license": "other", "language": "python", "size": 271}
{"docstring": "\"\"\"Make sure the message from a HTTPException is preserved.\"\"\"\n", "func_signal": "def test_httpexception_message(self):\n", "code": "msg = \"foo\"\nresp = self.func(Request(_env()), exc.HTTPBadRequest(msg))\nself.assertEqual(json.loads(resp.body)['detail'], msg)", "path": "dream\\tests\\test_dream.py", "repo_name": "simplegeo/dream", "stars": 25, "license": "other", "language": "python", "size": 271}
{"docstring": "\"\"\"Make sure exceptions are handled properly based on debug.\"\"\"\n", "func_signal": "def test_nondebug_exceptions(self):\n", "code": "app = App(debug=False)\nresp = app._mangle_response(\n    Request(_env()), exc.HTTPInternalServerError(\"Whops\"))\nself.assertFalse('traceback' in json.loads(resp.body))", "path": "dream\\tests\\test_dream.py", "repo_name": "simplegeo/dream", "stars": 25, "license": "other", "language": "python", "size": 271}
{"docstring": "\"\"\"Non-error exceptions shouldn't get mangled a traceback.\"\"\"\n", "func_signal": "def test_server_error_exceptions(self):\n", "code": "ex = exc.HTTPInternalServerError()\nresp = self.app._mangle_response(Request(_env()), ex)\nself.assertTrue(resp is not ex)", "path": "dream\\tests\\test_dream.py", "repo_name": "simplegeo/dream", "stars": 25, "license": "other", "language": "python", "size": 271}
{"docstring": "\"\"\"Make sure hidden functions don't show up in endpoints.\"\"\"\n", "func_signal": "def test_endpoints(self):\n", "code": "app = App()\n\n@app.expose('/endpoint')\n@hidden\ndef endpoint(request):\n    return Response()\n\nself.assertFalse(app.endpoints())", "path": "dream\\tests\\test_dream.py", "repo_name": "simplegeo/dream", "stars": 25, "license": "other", "language": "python", "size": 271}
{"docstring": "\"\"\"Test the decorator itself.\"\"\"\n\n", "func_signal": "def test_decorator(self):\n", "code": "func = lambda: None\nfunc_ = hidden(func)\nself.assertTrue(func is func_)\nself.assertTrue(hasattr(func_, '__hidden__'))\nself.assertTrue(func.__hidden__)", "path": "dream\\tests\\test_dream.py", "repo_name": "simplegeo/dream", "stars": 25, "license": "other", "language": "python", "size": 271}
{"docstring": "\"\"\"Make sure JSONResponses with no body work.\"\"\"\n", "func_signal": "def test_no_body(self):\n", "code": "resp = self.resp_class(status=304)\nself.assertEqual(resp.body, \"\")", "path": "dream\\tests\\test_dream.py", "repo_name": "simplegeo/dream", "stars": 25, "license": "other", "language": "python", "size": 271}
{"docstring": "\"\"\"Returns a list of change logs in the system.\"\"\"\n", "func_signal": "def read(self, startdate=None, enddate=None):\n", "code": "params = {\n    'startdate': startdate,\n    'enddate': enddate,\n}\nreturn self._req_json(\"change_logs\", params)", "path": "contrib\\cloudkick_api\\endpoints.py", "repo_name": "loggly/alertbirds-community-edition", "stars": 21, "license": "other", "language": "python", "size": 1209}
{"docstring": "\"\"\"Destroys this http client, freeing any file descriptors used.\nNot needed in normal use, but may be helpful in unittests that\ncreate and destroy http clients.  No other methods may be called\non the AsyncHTTPClient after close().\n\"\"\"\n", "func_signal": "def close(self):\n", "code": "if self._async_clients[self.io_loop] is self:\n    del self._async_clients[self.io_loop]", "path": "contrib\\tornado\\httpclient.py", "repo_name": "loggly/alertbirds-community-edition", "stars": 21, "license": "other", "language": "python", "size": 1209}
{"docstring": "\"\"\"Returns a form field for a ``db.DateProperty``.\"\"\"\n", "func_signal": "def convert_DateProperty(model, prop, kwargs):\n", "code": "if prop.auto_now or prop.auto_now_add:\n    return None\n\nreturn f.DateField(format='%Y-%m-%d', **kwargs)", "path": "contrib\\wtforms\\ext\\appengine\\db.py", "repo_name": "loggly/alertbirds-community-edition", "stars": 21, "license": "other", "language": "python", "size": 1209}
{"docstring": "\"\"\"\nReturns an ``IntegerField``, applying the ``db.IntegerProperty`` range\nlimits.\n\"\"\"\n", "func_signal": "def get_IntegerField(kwargs):\n", "code": "v = validators.NumberRange(min=-0x8000000000000000, max=0x7fffffffffffffff)\nkwargs['validators'].append(v)\nreturn f.IntegerField(**kwargs)", "path": "contrib\\wtforms\\ext\\appengine\\db.py", "repo_name": "loggly/alertbirds-community-edition", "stars": 21, "license": "other", "language": "python", "size": 1209}
{"docstring": "\"\"\"Returns a form field for a ``db.LinkProperty``.\"\"\"\n", "func_signal": "def convert_LinkProperty(model, prop, kwargs):\n", "code": "kwargs['validators'].append(validators.url())\nreturn get_TextField(kwargs)", "path": "contrib\\wtforms\\ext\\appengine\\db.py", "repo_name": "loggly/alertbirds-community-edition", "stars": 21, "license": "other", "language": "python", "size": 1209}
{"docstring": "\"\"\"Converts a `tornado.httpserver.HTTPRequest` to a WSGI environment.\n\"\"\"\n", "func_signal": "def environ(request):\n", "code": "hostport = request.host.split(\":\")\nif len(hostport) == 2:\n    host = hostport[0]\n    port = int(hostport[1])\nelse:\n    host = request.host\n    port = 443 if request.protocol == \"https\" else 80\nenviron = {\n    \"REQUEST_METHOD\": request.method,\n    \"SCRIPT_NAME\": \"\",\n    \"PATH_INFO\": urllib.unquote(request.path),\n    \"QUERY_STRING\": request.query,\n    \"REMOTE_ADDR\": request.remote_ip,\n    \"SERVER_NAME\": host,\n    \"SERVER_PORT\": str(port),\n    \"SERVER_PROTOCOL\": request.version,\n    \"wsgi.version\": (1, 0),\n    \"wsgi.url_scheme\": request.protocol,\n    \"wsgi.input\": BytesIO(escape.utf8(request.body)),\n    \"wsgi.errors\": sys.stderr,\n    \"wsgi.multithread\": False,\n    \"wsgi.multiprocess\": True,\n    \"wsgi.run_once\": False,\n}\nif \"Content-Type\" in request.headers:\n    environ[\"CONTENT_TYPE\"] = request.headers.pop(\"Content-Type\")\nif \"Content-Length\" in request.headers:\n    environ[\"CONTENT_LENGTH\"] = request.headers.pop(\"Content-Length\")\nfor key, value in request.headers.iteritems():\n    environ[\"HTTP_\" + key.replace(\"-\", \"_\").upper()] = value\nreturn environ", "path": "contrib\\tornado\\wsgi.py", "repo_name": "loggly/alertbirds-community-edition", "stars": 21, "license": "other", "language": "python", "size": 1209}
{"docstring": "\"\"\"Returns a form field for a ``db.ReferenceProperty``.\"\"\"\n", "func_signal": "def convert_ReferenceProperty(model, prop, kwargs):\n", "code": "kwargs['reference_class'] = prop.reference_class\nreturn ReferencePropertyField(**kwargs)", "path": "contrib\\wtforms\\ext\\appengine\\db.py", "repo_name": "loggly/alertbirds-community-edition", "stars": 21, "license": "other", "language": "python", "size": 1209}
{"docstring": "\"\"\"Restarts the process automatically when a module is modified.\n\nWe run on the I/O loop, and restarting is a destructive operation,\nso will terminate any pending requests.\n\"\"\"\n", "func_signal": "def start(io_loop=None, check_time=500):\n", "code": "io_loop = io_loop or ioloop.IOLoop.instance()\nmodify_times = {}\ncallback = functools.partial(_reload_on_update, io_loop, modify_times)\nscheduler = ioloop.PeriodicCallback(callback, check_time, io_loop=io_loop)\nscheduler.start()", "path": "contrib\\tornado\\autoreload.py", "repo_name": "loggly/alertbirds-community-edition", "stars": 21, "license": "other", "language": "python", "size": 1209}
{"docstring": "# Encodings here are tricky:  Headers are latin1, bodies can be\n# anything (we use utf8 by default).\n", "func_signal": "def test_multipart_form(self):\n", "code": "response = self.raw_fetch([\n        b(\"POST /multipart HTTP/1.0\"),\n        b(\"Content-Type: multipart/form-data; boundary=1234567890\"),\n        u\"X-Header-encoding-test: \\u00e9\".encode(\"latin1\"),\n        ],\n                          b(\"\\r\\n\").join([\n            b(\"Content-Disposition: form-data; name=argument\"),\n            b(\"\"),\n            u\"\\u00e1\".encode(\"utf-8\"),\n            b(\"--1234567890\"),\n            u'Content-Disposition: form-data; name=\"files\"; filename=\"\\u00f3\"'.encode(\"utf8\"),\n            b(\"\"),\n            u\"\\u00fa\".encode(\"utf-8\"),\n            b(\"--1234567890\"),\n            b(\"\"),\n            b(\"\"),\n            ]))\ndata = json_decode(response.body)\nself.assertEqual(u\"\\u00e9\", data[\"header\"])\nself.assertEqual(u\"\\u00e1\", data[\"argument\"])\nself.assertEqual(u\"\\u00f3\", data[\"filename\"])\nself.assertEqual(u\"\\u00fa\", data[\"filebody\"])", "path": "contrib\\tornado\\test\\httpserver_test.py", "repo_name": "loggly/alertbirds-community-edition", "stars": 21, "license": "other", "language": "python", "size": 1209}
{"docstring": "\"\"\"Executes a request, returning an `HTTPResponse`.\n\nThe request may be either a string URL or an `HTTPRequest` object.\nIf it is a string, we construct an `HTTPRequest` using any additional\nkwargs: ``HTTPRequest(request, **kwargs)``\n\nIf an error occurs during the fetch, we raise an `HTTPError`.\n\"\"\"\n", "func_signal": "def fetch(self, request, **kwargs):\n", "code": "def callback(response):\n    self._response = response\n    self._io_loop.stop()\nself._async_client.fetch(request, callback, **kwargs)\nself._io_loop.start()\nresponse = self._response\nself._response = None\nresponse.rethrow()\nreturn response", "path": "contrib\\tornado\\httpclient.py", "repo_name": "loggly/alertbirds-community-edition", "stars": 21, "license": "other", "language": "python", "size": 1209}
{"docstring": "\"\"\"Returns the amount of time it took for this request to execute.\"\"\"\n", "func_signal": "def request_time(self):\n", "code": "if self._finish_time is None:\n    return time.time() - self._start_time\nelse:\n    return self._finish_time - self._start_time", "path": "contrib\\tornado\\wsgi.py", "repo_name": "loggly/alertbirds-community-edition", "stars": 21, "license": "other", "language": "python", "size": 1209}
{"docstring": "\"\"\"Returns a form field for a ``db.EmailProperty``.\"\"\"\n", "func_signal": "def convert_EmailProperty(model, prop, kwargs):\n", "code": "kwargs['validators'].append(validators.email())\nreturn get_TextField(kwargs)", "path": "contrib\\wtforms\\ext\\appengine\\db.py", "repo_name": "loggly/alertbirds-community-edition", "stars": 21, "license": "other", "language": "python", "size": 1209}
{"docstring": "\"\"\"Returns a form field for a ``db.TimeProperty``.\"\"\"\n", "func_signal": "def convert_TimeProperty(model, prop, kwargs):\n", "code": "if prop.auto_now or prop.auto_now_add:\n    return None\n\nreturn f.DateTimeField(format='%H:%M:%S', **kwargs)", "path": "contrib\\wtforms\\ext\\appengine\\db.py", "repo_name": "loggly/alertbirds-community-edition", "stars": 21, "license": "other", "language": "python", "size": 1209}
{"docstring": "\"\"\"Parses the given WSGI environ to construct the request.\"\"\"\n", "func_signal": "def __init__(self, environ):\n", "code": "self.method = environ[\"REQUEST_METHOD\"]\nself.path = urllib.quote(environ.get(\"SCRIPT_NAME\", \"\"))\nself.path += urllib.quote(environ.get(\"PATH_INFO\", \"\"))\nself.uri = self.path\nself.arguments = {}\nself.query = environ.get(\"QUERY_STRING\", \"\")\nif self.query:\n    self.uri += \"?\" + self.query\n    arguments = cgi.parse_qs(self.query)\n    for name, values in arguments.iteritems():\n        values = [v for v in values if v]\n        if values: self.arguments[name] = values\nself.version = \"HTTP/1.1\"\nself.headers = httputil.HTTPHeaders()\nif environ.get(\"CONTENT_TYPE\"):\n    self.headers[\"Content-Type\"] = environ[\"CONTENT_TYPE\"]\nif environ.get(\"CONTENT_LENGTH\"):\n    self.headers[\"Content-Length\"] = environ[\"CONTENT_LENGTH\"]\nfor key in environ:\n    if key.startswith(\"HTTP_\"):\n        self.headers[key[5:].replace(\"_\", \"-\")] = environ[key]\nif self.headers.get(\"Content-Length\"):\n    self.body = environ[\"wsgi.input\"].read(\n        int(self.headers[\"Content-Length\"]))\nelse:\n    self.body = \"\"\nself.protocol = environ[\"wsgi.url_scheme\"]\nself.remote_ip = environ.get(\"REMOTE_ADDR\", \"\")\nif environ.get(\"HTTP_HOST\"):\n    self.host = environ[\"HTTP_HOST\"]\nelse:\n    self.host = environ[\"SERVER_NAME\"]\n\n# Parse request body\nself.files = {}\ncontent_type = self.headers.get(\"Content-Type\", \"\")\nif content_type.startswith(\"application/x-www-form-urlencoded\"):\n    for name, values in cgi.parse_qs(self.body).iteritems():\n        self.arguments.setdefault(name, []).extend(values)\nelif content_type.startswith(\"multipart/form-data\"):\n    if 'boundary=' in content_type:\n        boundary = content_type.split('boundary=',1)[1]\n        if boundary:\n            httputil.parse_multipart_form_data(\n                utf8(boundary), self.body, self.arguments, self.files)\n    else:\n        logging.warning(\"Invalid multipart/form-data\")\n\nself._start_time = time.time()\nself._finish_time = None", "path": "contrib\\tornado\\wsgi.py", "repo_name": "loggly/alertbirds-community-edition", "stars": 21, "license": "other", "language": "python", "size": 1209}
{"docstring": "\"\"\"\nReturns a form field for a single model property.\n\n:param model:\n    The ``db.Model`` class that contains the property.\n:param prop:\n    The model property: a ``db.Property`` instance.\n:param field_args:\n    Optional keyword arguments to construct the field.\n\"\"\"\n", "func_signal": "def convert(self, model, prop, field_args):\n", "code": "kwargs = {\n    'label': prop.name.replace('_', ' ').title(),\n    'default': prop.default_value(),\n    'validators': [],\n}\nif field_args:\n    kwargs.update(field_args)\n\nif prop.required:\n    kwargs['validators'].append(validators.required())\n\nif prop.choices:\n    # Use choices in a select field.\n    kwargs['choices'] = [(v, v) for v in prop.choices]\n    return f.SelectField(**kwargs)\nelse:\n    converter = self.converters.get(type(prop).__name__, None)\n    if converter is not None:\n        return converter(model, prop, kwargs)", "path": "contrib\\wtforms\\ext\\appengine\\db.py", "repo_name": "loggly/alertbirds-community-edition", "stars": 21, "license": "other", "language": "python", "size": 1209}
{"docstring": "\"\"\"Returns a form field for a ``db.DateTimeProperty``.\"\"\"\n", "func_signal": "def convert_DateTimeProperty(model, prop, kwargs):\n", "code": "if prop.auto_now or prop.auto_now_add:\n    return None\n\nreturn f.DateTimeField(format='%Y-%m-%d %H:%M:%S', **kwargs)", "path": "contrib\\wtforms\\ext\\appengine\\db.py", "repo_name": "loggly/alertbirds-community-edition", "stars": 21, "license": "other", "language": "python", "size": 1209}
{"docstring": "'''\nWe're here for one of several reasons:\n1. Initial visit, not logged in, going to '/' (show welcome page)\n2. Initial visit, no User object stored yet in App Engine datastore (returning from google.com)\n3. Return visit, everything normal\n4. Return visit, no subdomain in URL (so going to '/')\n5. User object stored, but access token not yet written (returning from loggly.com)\n6. User object stored, but access token not yet written (App Engine threw exception when returning from Loggly, etc.)\n7. User entered a subdomain, then hit the back button\n'''\n", "func_signal": "def get(self, subdomain = None):\n", "code": "args = self.template_args\nif not google_users.get_current_user():\n    # case 1\n    logging.info({'module': 'main', 'path': 'case 1'})\n    args['login_url'] = google_users.create_login_url(self.request.uri)\n    return self.render('splash.html', **args)\n\nif not UserManager.current_user_exists():\n    # case 2\n    logging.info({'module': 'main', 'path': 'case 2'})\n    args['form'] = SubdomainForm()\n    return self.render('subdomain.html', **args)\n\nuser = UserManager.get_current_user()\nif subdomain:\n    args['subdomain'] = subdomain\nelse:\n    args['subdomain'] = user.subdomain\n\nif self.get_argument('oauth_verifier', None) or not user.access_token_key:\n    # cases 5, 6, or 7\n    logging.info({'module': 'main', 'path': 'case 5, 6 or 7'})\n\n    oauth_client = lib.oauth.Client(user.subdomain)\n \n    if self.get_argument('oauth_verifier', None):\n        # case 5 (just in case)\n        user.oauth_verifier = self.get_argument('oauth_verifier')\n        user.put()\n    elif not user.oauth_verifier:\n        # case 7\n        request_token = oauth_client.generate_token(user.request_token_key, user.request_token_secret)\n        url = oauth_client.get_authorize_url(request_token)\n        return self.redirect(url)\n\n    request_token = oauth_client.generate_token(user.request_token_key, user.request_token_secret)\n    request_token.verifier = user.oauth_verifier\n\n    access_token = oauth_client.get_access_token(request_token)\n    \n    # store the access token for all future requests\n    user.access_token_key = access_token.key\n    user.access_token_secret = access_token.secret\n    user.new_user = False\n    user.put()\n\n    search_result = json.loads(oauth_client.make_request(access_token, 'http://%s.%s/api/facets/date?q=*&from=NOW-1YEAR' % \\\n        (user.subdomain, config.LOGGLY_DOMAIN), 'GET'))\n    logging.info(json.dumps({'module': 'main', 'user': user.email, 'event': 'new user', 'loggly_events_found': search_result['numFound']}))\n    if search_result['numFound'] == 0:\n        return self.render('nodata.html', **args)\n\nif not subdomain:\n    # case 4\n    logging.info({'module': 'main', 'path': 'case 4'})\n    # this must come after the others so we don't lose the OAuth parameters from the URL\n    return self.redirect('/%s' % user.subdomain)\n\n# case 3 or 5\nlogging.info({'module': 'main', 'path': 'case 3 or 5'})\nif self.request.uri[-1] == '/':\n    return self.redirect(self.request.uri[:-1])\n\nargs['alerts'] = AlertManager.get_all_alerts(user.subdomain)\nself.render('main.html', **args)", "path": "controllers\\main.py", "repo_name": "loggly/alertbirds-community-edition", "stars": 21, "license": "other", "language": "python", "size": 1209}
{"docstring": "# Testing keys were generated with:\n# openssl req -new -keyout tornado/test/test.key -out tornado/test/test.crt -nodes -days 3650 -x509\n", "func_signal": "def get_httpserver_options(self):\n", "code": "test_dir = os.path.dirname(__file__)\nreturn dict(ssl_options=dict(\n        certfile=os.path.join(test_dir, 'test.crt'),\n        keyfile=os.path.join(test_dir, 'test.key')))", "path": "contrib\\tornado\\test\\httpserver_test.py", "repo_name": "loggly/alertbirds-community-edition", "stars": 21, "license": "other", "language": "python", "size": 1209}
{"docstring": "\"\"\"Configures the AsyncHTTPClient subclass to use.\n\nAsyncHTTPClient() actually creates an instance of a subclass.\nThis method may be called with either a class object or the\nfully-qualified name of such a class (or None to use the default,\nSimpleAsyncHTTPClient)\n\nIf additional keyword arguments are given, they will be passed\nto the constructor of each subclass instance created.  The\nkeyword argument max_clients determines the maximum number of\nsimultaneous fetch() operations that can execute in parallel\non each IOLoop.  Additional arguments may be supported depending\non the implementation class in use.\n\nExample::\n\n   AsyncHTTPClient.configure(\"tornado.curl_httpclient.CurlAsyncHTTPClient\")\n\"\"\"\n", "func_signal": "def configure(impl, **kwargs):\n", "code": "if isinstance(impl, (unicode, bytes_type)):\n    impl = import_object(impl)\nif impl is not None and not issubclass(impl, AsyncHTTPClient):\n    raise ValueError(\"Invalid AsyncHTTPClient implementation\")\nAsyncHTTPClient._impl_class = impl\nAsyncHTTPClient._impl_kwargs = kwargs", "path": "contrib\\tornado\\httpclient.py", "repo_name": "loggly/alertbirds-community-edition", "stars": 21, "license": "other", "language": "python", "size": 1209}
{"docstring": "\"\"\"Returns the status of a set of checks, filtered based on statuses\n\nKeyword arguments:\n    overall_check_statuses -- Filter only checks with warning,\n                              error, or recovery messages\n    check_id -- Filter the statuses based on the check id\n    monitor_id -- Filter based on the monitor id\n    query -- Filter based on a query string\n    include_metrics -- Include the metrics with the response\n\n\"\"\"\n", "func_signal": "def read(self, **kwargs):\n", "code": "valid_params = ['overall_check_statuses', 'check_id',\n                'monitor_id', 'query', 'include_metrics']\nparams = dict([(k,v) for k, v in kwargs.iteritems()\n                        if k in valid_params])\n\nreturn self._req_json(\"status/nodes\", params)", "path": "contrib\\cloudkick_api\\endpoints.py", "repo_name": "loggly/alertbirds-community-edition", "stars": 21, "license": "other", "language": "python", "size": 1209}
{"docstring": "\"\"\"\nconstruct and return a radius header.\n_code, _id, _type, _auth_data should be selfexplanatory\n\"\"\"\n", "func_signal": "def pack_eap(_code, _id, _type, _auth_data):\n", "code": "_len = 5 + len(_auth_data)\n_eap = RADIUS_H3C.EAP(\n            code = _code,\n            id = _id,\n            len = _len,\n            type = _type,\n            data = _auth_data\n        )\nreturn _eap", "path": "pyh3c\\h3cPack.py", "repo_name": "houqp/pyh3c", "stars": 20, "license": "None", "language": "python", "size": 206}
{"docstring": "\"\"\"\nhandler for success\n\"\"\"\n", "func_signal": "def success_handler(self, ether, callback=do_nothing, data=None):\n", "code": "self.h3cStatus.auth_success = True\n\nself.callback_caller(callback, data)\n\n#call after_auth_succ functions registered by plugins\nfor plugin in self.plugins_loaded:\n    getattr(plugin, 'after_auth_succ')(self)", "path": "pyh3c\\pyh3c.py", "repo_name": "houqp/pyh3c", "stars": 20, "license": "None", "language": "python", "size": 206}
{"docstring": "\"\"\"\nconstruct and return a radius header.\n_code, _id should be selfexplanatory\n_len is the length of eap Packet\n_eap is a RADIUS_H3C.EAP object, not a string\n\"\"\"\n", "func_signal": "def pack_radius(_code, _id, _eap=None):\n", "code": "if not _eap:\n    _radius = RADIUS_H3C(\n                code = _code,\n                id = _id,\n                len = 0,\n                data = \"\"\n            )\nelse:\n    _radius = RADIUS_H3C(\n                code = _code,\n                id = _id,\n                len = _eap.len,\n                data = str(_eap)\n            )\nreturn _radius", "path": "pyh3c\\h3cPack.py", "repo_name": "houqp/pyh3c", "stars": 20, "license": "None", "language": "python", "size": 206}
{"docstring": "\"\"\"\nsend logoff packet\n\"\"\"\n", "func_signal": "def logoff(self, callback=do_nothing, data=None):\n", "code": "logoff_radius = pack_radius(0x01, 0x02)\nlogoff_packet = pack_ether(self.h3cStatus.cli_hwadd, self.h3cStatus.ser_hwadd, logoff_radius)\n\nself.sender.send(str(logoff_packet))", "path": "pyh3c\\pyh3c.py", "repo_name": "houqp/pyh3c", "stars": 20, "license": "None", "language": "python", "size": 206}
{"docstring": "\"\"\" \nReceived allocated response, send authentication result\n\"\"\"\n", "func_signal": "def allocated_handler(self, ether, callback=do_nothing, data=None):\n", "code": "auth_re = False\nradius = RADIUS_H3C(ether.data)\neap = RADIUS_H3C.EAP(radius.data)\n#@TODO handle username and password here  13.03 2012 (houqp)\n\n#auth_data = '%s%s%s' % ( chr(len(self.h3cSrvStatus.user_pass)), self.h3cSrvStatus.user_pass, self.h3cSrvStatus.user_name )\n#allocated_eap = pack_eap(0x02, eap.id, 0x07, auth_data)\n#allocated_radius = pack_radius(0x01, 0x00, allocated_eap)\n#allocated_packet = pack_ether(self.h3cSrvStatus.cli_hwadd, self.h3cSrvStatus.srv_hwadd, allocated_radius)\n#self.sender.send(str(allocated_packet))\nif auth_re:\n    self.send_auth_success(ether)\nelse:\n    self.send_auth_fail(ether)\nself.callback_caller(callback, (ether,auth_re))", "path": "pyh3c\\pyh3c_srv.py", "repo_name": "houqp/pyh3c", "stars": 20, "license": "None", "language": "python", "size": 206}
{"docstring": "\"\"\"Unpack packet header fields from buf, and set self.data.\"\"\"\n", "func_signal": "def unpack(self, buf):\n", "code": "for k, v in itertools.izip(self.__hdr_fields__,\n    struct.unpack(self.__hdr_fmt__, buf[:self.__hdr_len__])):\n    setattr(self, k, v)\nself.data = buf[self.__hdr_len__:]", "path": "pyh3c\\dpktMini\\dpkt.py", "repo_name": "houqp/pyh3c", "stars": 20, "license": "None", "language": "python", "size": 206}
{"docstring": "\"\"\"Return a hexdump output string of the given buffer.\"\"\"\n", "func_signal": "def hexdump(buf, length=16):\n", "code": "n = 0\nres = []\nwhile buf:\n    line, buf = buf[:length], buf[length:]\n    hexa = ' '.join(['%02x' % ord(x) for x in line])\n    line = line.translate(__vis_filter)\n    res.append('  %04d:  %-*s %s' % (n, length * 3, hexa, line))\n    n += length\nreturn '\\n'.join(res)", "path": "pyh3c\\dpktMini\\dpkt.py", "repo_name": "houqp/pyh3c", "stars": 20, "license": "None", "language": "python", "size": 206}
{"docstring": "\"\"\"\nparse arguments\n\"\"\"\n", "func_signal": "def read_args(self):\n", "code": "desc = \"PyH3C - A H3C client written in Python.\"\nparser = argparse.ArgumentParser(description=desc)\n\nparser.add_argument('-u', '--user', type=str, \n        metavar='user_name', dest='user_name', action='store', \n        help=\"User name for your account.\")\n\nparser.add_argument('-p', '--pass', type=str, \n        metavar='password', dest='user_pass', action='store', \n        help=\"Password for your account.\")\n\nparser.add_argument('-D', '--dhcp', type=str, \n        metavar='dhcp_command', dest='dhcp_command', action='store', \n        help=\"DHCP command for acquiring IP after authentication.\")\n\nparser.add_argument('-d', '--dev', type=str, \n        metavar='dev', dest='dev', action='store', \n        help=\"Ethernet interface used to connect to the internet.\")\n\nparser.add_argument('-g', '--debug', \n        dest='debug_on', action='store_true', \n        help=\"Turn on debug to see dump content.\")\n\nparser.add_argument('-k', '--kill', \n        dest='kill_on', action='store_true', \n        help=\"If there is another PyH3C instance running, kill it before start.\")\n\nargs = parser.parse_args(namespace=self.h3cStatus)\nreturn", "path": "pyh3c\\pyh3c.py", "repo_name": "houqp/pyh3c", "stars": 20, "license": "None", "language": "python", "size": 206}
{"docstring": "\"\"\"\nSetup lock file in /tmp/pyh3c.lock in which pid is written \n\"\"\"\n", "func_signal": "def set_up_lock(self):\n", "code": "try:\n    lock = open(self.lock_file)\nexcept IOError:\n    lock = open(self.lock_file, 'w')\n    lock.write(str(os.getpid()))\n    lock.close()\n    return 1\nreturn 0", "path": "pyh3c\\pyh3c.py", "repo_name": "houqp/pyh3c", "stars": 20, "license": "None", "language": "python", "size": 206}
{"docstring": "\"\"\"\nSetup lock file in /tmp/pyh3c_srv.lock in which pid is written \n\"\"\"\n", "func_signal": "def set_up_lock(self):\n", "code": "try:\n    lock = open(self.lock_file)\nexcept IOError:\n    lock = open(self.lock_file, 'w')\n    lock.write(str(os.getpid()))\n    lock.close()\n    return 1\nreturn 0", "path": "pyh3c\\pyh3c_srv.py", "repo_name": "houqp/pyh3c", "stars": 20, "license": "None", "language": "python", "size": 206}
{"docstring": "\"\"\"\nwrite current configuration to pyh3c.conf\n\"\"\"\n", "func_signal": "def save_config(self):\n", "code": "if not self.parser:\n    self.parser = ConfigParser.SafeConfigParser()\n\n#try:\n    #fp = open('/etc/pyh3c.conf', 'r+')\n#except IOError:\n    #fp = open('/etc/pyh3c.conf', 'w')\n\nif not self.parser.has_section('sys_conf'):\n    self.parser.add_section('sys_conf')\nif not self.parser.has_section('account'):\n    self.parser.add_section('account')\n\nself.parser.set('sys_conf', 'dev', self.dev)\nself.parser.set('sys_conf', 'dhcp_command', self.dhcp_command)\nself.parser.set('sys_conf', 'ping_target', self.ping_target)\nself.parser.set('sys_conf', 'ping_interval', str(self.ping_interval))\nself.parser.set('sys_conf', 'ping_tolerence', str(self.ping_tolerence))\nself.parser.set('sys_conf', 'ping_after_reauth', str(self.ping_after_reauth))\nself.parser.set('account', 'user_name', self.user_name)\nself.parser.set('account', 'user_pass', self.user_pass)\n\n#ConfigParser module will delete all comments, here is a dirty hack\n#@TODO@: fix the ConfigParser module, or use cfgparse module\ntry:\n    os.unlink('/etc/pyh3c.conf')\nexcept OSError:\n    pass\nfp = open('/etc/pyh3c.conf', 'w')\nself.parser.write(fp)\nfp = fp.close()\nreturn", "path": "pyh3c\\h3cStatus.py", "repo_name": "houqp/pyh3c", "stars": 20, "license": "None", "language": "python", "size": 206}
{"docstring": "\"\"\"\nGet the list of all devices, return a list\n\"\"\"\n", "func_signal": "def get_devices(self):\n", "code": "devs = []\ntry:\n    libdnet = __import__('dnet')\nexcept ImportError:\n    libdnet = __import__('dumbnet')\nintf = libdnet.intf()\ndef add_dev(dict, arg):\n    arg.append(dict['name'])\n    return\nintf.loop(add_dev, devs)\nreturn devs", "path": "pyh3c\\pyh3c_srv.py", "repo_name": "houqp/pyh3c", "stars": 20, "license": "None", "language": "python", "size": 206}
{"docstring": "\"\"\"\nGet the list of all devices, return a list\n\"\"\"\n", "func_signal": "def get_devices(self):\n", "code": "devs = []\ntry:\n    libdnet = __import__('dnet')\nexcept ImportError:\n    libdnet = __import__('dumbnet')\nintf = libdnet.intf()\ndef add_dev(dict, arg):\n    arg.append(dict['name'])\n    return\nintf.loop(add_dev, devs)\nreturn devs", "path": "pyh3c\\pyh3c.py", "repo_name": "houqp/pyh3c", "stars": 20, "license": "None", "language": "python", "size": 206}
{"docstring": "\"\"\"\nstart the authentication\n\"\"\"\n# manually construct the header because it's special\n", "func_signal": "def send_start(self, callback=do_nothing, data=None):\n", "code": "start_radius = RADIUS_H3C(\n            code = 1,\n            id = 1,\n            len = 0,\n            data = '\\x00'\n        )\nstart_packet = pack_ether(self.h3cStatus.cli_hwadd, '\\xff\\xff\\xff\\xff\\xff\\xff', start_radius)\n\n#call before_auth functions registered by plugins\nfor plugin in self.plugins_loaded:\n    getattr(plugin, 'before_auth')(self)\n\nself.sender.send(str(start_packet))\n\nself.callback_caller(callback, data)", "path": "pyh3c\\pyh3c.py", "repo_name": "houqp/pyh3c", "stars": 20, "license": "None", "language": "python", "size": 206}
{"docstring": "\"\"\" \nresponse user_name to server\n\"\"\"\n", "func_signal": "def identity_handler(self, ether, callback=do_nothing, data=None):\n", "code": "self.h3cStatus.ser_hwadd = ether.src\nradius = RADIUS_H3C(ether.data)\neap = RADIUS_H3C.EAP(radius.data)\nidentity_eap = pack_eap(0x02, eap.id, 0x01, self.h3cStatus.user_name)\nidentity_radius = pack_radius(0x01, 0x00, identity_eap)\nidentity_packet = pack_ether(self.h3cStatus.cli_hwadd, self.h3cStatus.ser_hwadd, identity_radius)\n\nself.sender.send(str(identity_packet))\n\nself.callback_caller(callback, data)", "path": "pyh3c\\pyh3c.py", "repo_name": "houqp/pyh3c", "stars": 20, "license": "None", "language": "python", "size": 206}
{"docstring": "\"\"\" \nReceived identity response, send allocated request\n\"\"\"\n", "func_signal": "def identity_handler(self, ether, callback=do_nothing, data=None):\n", "code": "radius = RADIUS_H3C(ether.data)\neap = RADIUS_H3C.EAP(radius.data)\nallocated_eap = pack_eap(0x01, eap.id + 1, 0x07, '\\x00')\nallocated_radius = pack_radius(0x01, 0x00, allocated_eap)\nallocated_packet = pack_ether(self.h3cSrvStatus.srv_hwadd, ether.src, allocated_radius)\nself.sender.send(str(allocated_packet))\nself.callback_caller(callback, data)", "path": "pyh3c\\pyh3c_srv.py", "repo_name": "houqp/pyh3c", "stars": 20, "license": "None", "language": "python", "size": 206}
{"docstring": "\"\"\"\nparse arguments\n\"\"\"\n", "func_signal": "def read_args(self):\n", "code": "desc = \"PyH3CSrv - A H3C authentication server written in Python.\"\nparser = argparse.ArgumentParser(description=desc)\n\nparser.add_argument('-d', '--dev', type=str, \n        metavar='dev', dest='dev', action='store', \n        help=\"Ethernet interface used to connect to the internet.\")\n\nparser.add_argument('-g', '--debug', \n        dest='debug_on', action='store_true', \n        help=\"Turn on debug to see dump content.\")\n\nparser.add_argument('-k', '--kill', \n        dest='kill_on', action='store_true', \n        help=\"If there is another PyH3CSrv instance running, kill it before start.\")\n\nargs = parser.parse_args(namespace=self.h3cSrvStatus)", "path": "pyh3c\\pyh3c_srv.py", "repo_name": "houqp/pyh3c", "stars": 20, "license": "None", "language": "python", "size": 206}
{"docstring": "\"\"\"\nhandler for failed authentication\n\"\"\"\n", "func_signal": "def failure_handler(self, ether, callback=do_nothing, data=None):\n", "code": "self.h3cStatus.auth_success = False\n\nself.callback_caller(callback, data)\n\n#call after_auth_succ functions registered by plugins\nfor plugin in self.plugins_loaded:\n    getattr(plugin, 'after_auth_fail')(self)", "path": "pyh3c\\pyh3c.py", "repo_name": "houqp/pyh3c", "stars": 20, "license": "None", "language": "python", "size": 206}
{"docstring": "\"\"\"\nconstruct and return a radius header.\n_src is the source address of the frame, in binary\n_dst is the destination address of the frame, in binary\n_radius is a RADIUS_H3C object, not a string\n\"\"\"\n", "func_signal": "def pack_ether(_src, _dst, _radius):\n", "code": "_ether = dpktMini.ethernet.Ethernet(\n            src = _src,\n            dst = _dst,\n            type = 0x888e,\n            data = str(_radius)\n        )\nreturn _ether", "path": "pyh3c\\h3cPack.py", "repo_name": "houqp/pyh3c", "stars": 20, "license": "None", "language": "python", "size": 206}
{"docstring": "\"\"\" \nReceived start request, send identity request to client\n\"\"\"\n", "func_signal": "def start_request_handler(self, ether, callback=do_nothing, data=None):\n", "code": "self.h3cSrvStatus.cli_hwadd = ether.src\nidentity_eap = pack_eap(0x01, 0x02, 0x01, '\\x00')\nidentity_radius = pack_radius(0x01, 0x00, identity_eap)\nidentity_packet = pack_ether(self.h3cSrvStatus.srv_hwadd, self.h3cSrvStatus.cli_hwadd, identity_radius)\nself.sender.send(str(identity_packet))\nself.callback_caller(callback, data)", "path": "pyh3c\\pyh3c_srv.py", "repo_name": "houqp/pyh3c", "stars": 20, "license": "None", "language": "python", "size": 206}
{"docstring": "\"\"\"\narglist : argument\n        | arglist COMMA argument\n\"\"\"\n", "func_signal": "def p_arglist(self, t):\n", "code": "if len(t) == 2:\n    t[0] = [t[1]]\nelse:\n    t[0] = t[1] + [t[3]]", "path": "shore\\parser.py", "repo_name": "alex/Shore", "stars": 19, "license": "None", "language": "python", "size": 208}
{"docstring": "\"\"\"\nstatements : statement\n           | NEWLINE\n           | statements statement\n           | statements NEWLINE\n\"\"\"\n", "func_signal": "def p_statements(self, t):\n", "code": "if len(t) == 2:\n    if t[1] == \"\\n\":\n        t[0] = []\n    else:\n        t[0] = [t[1]]\nelse:\n    if t[2] == \"\\n\":\n        t[0] = t[1]\n    else:\n        t[0] = t[1] + [t[2]]", "path": "shore\\parser.py", "repo_name": "alex/Shore", "stars": 19, "license": "None", "language": "python", "size": 208}
{"docstring": "\"\"\"\nexpression : NUMBER DOT NUMBER\n           | NUMBER DOT\n           | DOT NUMBER\n\"\"\"\n", "func_signal": "def p_expression_float(self, t):\n", "code": "if len(t) == 4:\n    t[0] = ast.FloatNode(\"%s.%s\" % (t[1], t[3]))\nelse:\n    if t[1] == \".\":\n        t[0] = ast.FloatNode(\"0.%s\" % t[2])\n    else:\n        t[0] = ast.FloatNode(\"%s.0\" % t[1])", "path": "shore\\parser.py", "repo_name": "alex/Shore", "stars": 19, "license": "None", "language": "python", "size": 208}
{"docstring": "# TODO: Default args.\n", "func_signal": "def matches(self, arguments):\n", "code": "if len(self.arguments) != len(arguments):\n    return False\nfor expected, received in zip(self.arguments, arguments):\n    if not expected[1].compatible(received[1]):\n        return False\nreturn True", "path": "shore\\builtins.py", "repo_name": "alex/Shore", "stars": 19, "license": "None", "language": "python", "size": 208}
{"docstring": "\"\"\"\nexpression_or_none : expression\n                   |\n\"\"\"\n", "func_signal": "def p_expression_or_none(self, t):\n", "code": "if len(t) == 2:\n    t[0] = t[1]\nelse:\n    t[0] = None", "path": "shore\\parser.py", "repo_name": "alex/Shore", "stars": 19, "license": "None", "language": "python", "size": 208}
{"docstring": "# TODO: subclasses\n", "func_signal": "def compatible(self, type):\n", "code": "return type is None or (self.__class__ == type.__class__ and\n    all(self.templates[k].compatible(type.templates[k]) for k in\n        self.templated_over)\n    )", "path": "shore\\builtins.py", "repo_name": "alex/Shore", "stars": 19, "license": "None", "language": "python", "size": 208}
{"docstring": "\"\"\"\nclass_definition : CLASS LBRACE templates RBRACE NAME LPAR templates RPAR COLON suite\n                 | CLASS NAME LPAR templates RPAR COLON suite\n\"\"\"\n", "func_signal": "def p_class_definition(self, t):\n", "code": "if len(t) == 11:\n    t[0] = ast.ClassNode(t[5], t[3], t[7], t[10])\nelse:\n    t[0] = ast.ClassNode(t[2], [], t[4], t[7])", "path": "shore\\parser.py", "repo_name": "alex/Shore", "stars": 19, "license": "None", "language": "python", "size": 208}
{"docstring": "\"\"\"\nargument : expression\n         | NAME EQUAL expression\n\"\"\"\n", "func_signal": "def p_argument(self, t):\n", "code": "if len(t) == 2:\n    t[0] = (None, t[1])\nelse:\n    t[0] = (t[1], t[3])", "path": "shore\\parser.py", "repo_name": "alex/Shore", "stars": 19, "license": "None", "language": "python", "size": 208}
{"docstring": "\"\"\"\nparams : param\n       | params param\n\"\"\"\n", "func_signal": "def p_params(self, t):\n", "code": "if len(t) == 2:\n    t[0] = [t[1]]\nelse:\n    t[0] = t[1] + [t[2]]", "path": "shore\\parser.py", "repo_name": "alex/Shore", "stars": 19, "license": "None", "language": "python", "size": 208}
{"docstring": "\"\"\"\nelifs : ELIF expression COLON suite\n      | elifs ELIF expression COLON suite\n\"\"\"\n", "func_signal": "def p_elifs(self, t):\n", "code": "if len(t) == 5:\n    t[0] = [(t[2], t[4])]\nelse:\n    t[0] = t[1] + [(t[3], t[5])]", "path": "shore\\parser.py", "repo_name": "alex/Shore", "stars": 19, "license": "None", "language": "python", "size": 208}
{"docstring": "\"\"\"\nexpression : TRUE\n           | FALSE\n\"\"\"\n", "func_signal": "def p_expression_bool(self, t):\n", "code": "val = {\n    \"True\": True,\n    \"False\": False,\n}\nt[0] = ast.BooleanNode(val[t[1]])", "path": "shore\\parser.py", "repo_name": "alex/Shore", "stars": 19, "license": "None", "language": "python", "size": 208}
{"docstring": "\"\"\"\nslice : expression_or_none COLON expression_or_none COLON expression_or_none\n      | expression_or_none COLON expression_or_none\n\"\"\"\n", "func_signal": "def p_slice(self, t):\n", "code": "if len(t) == 6:\n    t[0] = ast.SliceNode(t[1], t[3], t[5])\nelse:\n    t[0] = ast.SliceNode(t[1], t[3], None)", "path": "shore\\parser.py", "repo_name": "alex/Shore", "stars": 19, "license": "None", "language": "python", "size": 208}
{"docstring": "\"\"\"\ntemplate : NAME\n         | NAME LBRACE templates RBRACE\n\"\"\"\n", "func_signal": "def p_template(self, t):\n", "code": "if len(t) == 2:\n    t[0] = ast.NameNode(t[1])\nelse:\n    t[0] = ast.TemplateNode(ast.NameNode(t[1]), t[3])", "path": "shore\\parser.py", "repo_name": "alex/Shore", "stars": 19, "license": "None", "language": "python", "size": 208}
{"docstring": "\"\"\"\nparam : template NAME\n      | template NAME EQUAL expression\n\"\"\"\n", "func_signal": "def p_param(self, t):\n", "code": "if len(t) == 3:\n    t[0] = (t[2], t[1], None)\nelse:\n    t[0] = (t[2], t[1], t[4])", "path": "shore\\parser.py", "repo_name": "alex/Shore", "stars": 19, "license": "None", "language": "python", "size": 208}
{"docstring": "\"\"\"\nif_statement : IF expression COLON suite elifs ELSE COLON suite\n\"\"\"\n", "func_signal": "def p_if_statement_elifs_else(self, t):\n", "code": "t[0] = ast.IfNode(\n    [(t[2], t[4])] + t[5],\n    t[8]\n)", "path": "shore\\parser.py", "repo_name": "alex/Shore", "stars": 19, "license": "None", "language": "python", "size": 208}
{"docstring": "\"\"\"\nif_statement : IF expression COLON suite elifs\n\"\"\"\n", "func_signal": "def p_if_statement_elifs(self, t):\n", "code": "t[0] = ast.IfNode(\n    [(t[2], t[4])] + t[5],\n    None\n)", "path": "shore\\parser.py", "repo_name": "alex/Shore", "stars": 19, "license": "None", "language": "python", "size": 208}
{"docstring": "\"\"\"\nfunction_definition : template DEF LBRACE templates RBRACE NAME parameters COLON suite\n                    | template DEF NAME parameters COLON suite\n                    | DEF LBRACE templates RBRACE NAME parameters COLON suite\n                    | DEF NAME parameters COLON suite\n\"\"\"\n", "func_signal": "def p_function_definition(self, t):\n", "code": "if len(t) == 10:\n    t[0] = ast.FunctionNode(t[6], t[4], t[1], t[7], t[9])\nelif len(t) == 7:\n    t[0] = ast.FunctionNode(t[3], [], t[1], t[4], t[6])\nelif len(t) == 9:\n    t[0] = ast.FunctionNode(t[5], t[3], None, t[6], t[8])\nelif len(t) == 6:\n    t[0] = ast.FunctionNode(t[2], [], None, t[3], t[5])", "path": "shore\\parser.py", "repo_name": "alex/Shore", "stars": 19, "license": "None", "language": "python", "size": 208}
{"docstring": "\"\"\"\ntemplates : template\n          | templates COMMA template\n\"\"\"\n", "func_signal": "def p_templates(self, t):\n", "code": "if len(t) == 2:\n    t[0] = [t[1]]\nelse:\n    t[0] = t[1] + [t[3]]", "path": "shore\\parser.py", "repo_name": "alex/Shore", "stars": 19, "license": "None", "language": "python", "size": 208}
{"docstring": "\"\"\"\nexpression : expression IN expression %prec COMPARISON\n           | expression NOT IN expression %prec COMPARISON\n\"\"\"\n", "func_signal": "def p_expression_in(self, t):\n", "code": "if len(t) == 4:\n    t[0] = ast.ContainsNode(t[1], t[3])\nelse:\n    t[0] = ast.UnaryOpNode(ast.ContainsNode(t[1], t[4]), \"not\")", "path": "shore\\parser.py", "repo_name": "alex/Shore", "stars": 19, "license": "None", "language": "python", "size": 208}
{"docstring": "\"\"\"\ndecorator : AT dotted_name NEWLINE\n          | AT dotted_name LPAR arglist RPAR NEWLINE\n\"\"\"\n\n", "func_signal": "def p_decorator(self, t):\n", "code": "\n\"\"\"\nparameters : LPAR RPAR\n           | LPAR params RPAR\n\"\"\"\nif len(t) == 3:\n    t[0] = []\nelse:\n    t[0] = t[2]", "path": "shore\\parser.py", "repo_name": "alex/Shore", "stars": 19, "license": "None", "language": "python", "size": 208}
{"docstring": "\"\"\"Check results for a given site: praise, issues and both.\"\"\"\n", "func_signal": "def test_single_site(self):\n", "code": "for i in xrange(NUM_SITES):\n    for sentiment in [\"happy\", \"sad\", None]:\n        params = dict(url_='www%i.example.com' % i, protocol='http')\n        view_url = reverse('single_site', kwargs=params)\n        if sentiment is not None:\n            view_url += '?sentiment=%s' % sentiment\n        r = self.client.get(view_url)\n        eq_(r.status_code, 200)\n        assert_true(len(r.content) > 0)", "path": "apps\\website_issues\\tests.py", "repo_name": "fwenzel/reporter", "stars": 25, "license": "None", "language": "python", "size": 2425}
{"docstring": "\"\"\"Create a bunch of clusters for the given summary.\"\"\"\n", "func_signal": "def make_clusters(summary, type, numcomments):\n", "code": "numcreated = 0\nfor csize in [NUM_PRAISE - NUM_ISSUES, NUM_ISSUES]:\n    if numcreated >= numcomments:\n        break\n    cluster = Cluster(site_summary=summary, size=csize)\n    for i in xrange(csize):\n        if i == 0:\n            cluster.save()\n        c = make_comment(cluster, csize, i, type)\n        if i == 0:\n            cluster.primary_description = c.description\n            cluster.primary_comment = c\n            cluster.save()\n    numcreated += csize", "path": "apps\\website_issues\\tests.py", "repo_name": "fwenzel/reporter", "stars": 25, "license": "None", "language": "python", "size": 2425}
{"docstring": "\"\"\"Test how HTTP, about:, chrome:// sites are shown to the user\"\"\"\n", "func_signal": "def test_url_display(self):\n", "code": "test_domains = (\n    ('http://example.com', 'example.com'),\n    ('https://example.net:8080/abc', 'example.net:8080'),\n    ('about:config', 'about:config'),\n    ('chrome://something/exciting', 'chrome://something/exciting'),\n)\nfor domain, expected in test_domains:\n    eq_(helpers.strip_protocol(domain), expected)", "path": "apps\\website_issues\\tests.py", "repo_name": "fwenzel/reporter", "stars": 25, "license": "None", "language": "python", "size": 2425}
{"docstring": "\"\"\"Check each site theme page.\"\"\"\n", "func_signal": "def test_site_theme(self):\n", "code": "clusters = Cluster.objects.all()\nfor c in clusters:\n    params = dict(theme_id=c.id)\n    r = self.client.get(reverse('site_theme', kwargs=params))\n    eq_(r.status_code, 200)\n    assert_true(len(r.content) > 0)", "path": "apps\\website_issues\\tests.py", "repo_name": "fwenzel/reporter", "stars": 25, "license": "None", "language": "python", "size": 2425}
{"docstring": "\"\"\"Convert a locale code into a human readable locale name.\"\"\"\n", "func_signal": "def locale_name(locale, native=False, default=_lazy('Unknown')):\n", "code": "if locale in product_details.languages:\n    return product_details.languages[locale][\n        native and 'native' or 'English']\nelse:\n    return default", "path": "apps\\feedback\\helpers.py", "repo_name": "fwenzel/reporter", "stars": 25, "license": "None", "language": "python", "size": 2425}
{"docstring": "\"\"\"\nTest that we return the proper value of unknown that sphinx is expecting.\n\"\"\"\n", "func_signal": "def test_extract_filters_unknown():\n", "code": "_, _, metas = extract_filters(dict(platform='unknown'))\neq_(metas['platform'], 0)", "path": "apps\\search\\tests\\test_client.py", "repo_name": "fwenzel/reporter", "stars": 25, "license": "None", "language": "python", "size": 2425}
{"docstring": "\"\"\"Test normalization from urls to sites.\"\"\"\n", "func_signal": "def test_normalize_url(self):\n", "code": "test_urls = (\n    ('http://www.example.com', 'http://example.com'),\n    ('http://example.com', 'http://example.com'),\n    ('http://example.com/the/place/to/be', 'http://example.com'),\n    ('https://example.net:8080', 'https://example.net:8080'),\n    ('https://example.net:8080/abc', 'https://example.net:8080'),\n    ('https://me@example.com:8080/xyz', 'https://example.com:8080'),\n    ('https://me:pass@example.com:8080/z', 'https://example.com:8080'),\n    ('about:config', 'about:config'),\n    ('chrome://something/exciting', 'chrome://something/exciting'),\n)\nfor url, expected in test_urls:\n    eq_(utils.normalize_url(url), expected)", "path": "apps\\website_issues\\tests.py", "repo_name": "fwenzel/reporter", "stars": 25, "license": "None", "language": "python", "size": 2425}
{"docstring": "\"\"\"Quickly check if sites works in general.\"\"\"\n", "func_signal": "def test_sites(self):\n", "code": "r = self.client.get(reverse('website_issues'))\neq_(r.status_code, 200)\nassert_true(len(r.content) > 0)", "path": "apps\\website_issues\\tests.py", "repo_name": "fwenzel/reporter", "stars": 25, "license": "None", "language": "python", "size": 2425}
{"docstring": "\"\"\"Non-existent platform will not cause an error.\"\"\"\n", "func_signal": "def test_invalid_platform(self):\n", "code": "r = self.client.get(reverse('website_issues'), {\"platform\": \"bogus\"})\neq_(r.status_code, 200)\nassert_true(len(r.content) > 0)", "path": "apps\\website_issues\\tests.py", "repo_name": "fwenzel/reporter", "stars": 25, "license": "None", "language": "python", "size": 2425}
{"docstring": "\"\"\"\nRequests to front page and submission pages should forward mobile\nusers to mobile site.\n\"\"\"\n", "func_signal": "def test_mobile_device_detection(self, mock):\n", "code": "fake_mobile_domain = 'mymobiledomain.example.com'\n\ndef side_effect(*args, **kwargs):\n    class FakeSite(object):\n        id = settings.MOBILE_SITE_ID\n        domain = fake_mobile_domain\n    return FakeSite()\nmock.side_effect = side_effect\n\n# URLs that should allow Mobile detection\nurls = (\n    reverse('dashboard'),\n    reverse('feedback'),\n)\n\n# User Agent Patterns: (UA, forward: true/false?)\nua_patterns = (\n    # Fx\n    ('Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; '\n     'rv:1.9.2.13) Gecko/20101203 Firefox/3.6.13', False),\n    # MSIE\n    ('Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)',\n     False),\n    # Fennec\n    ('Mozilla/5.0 (X11; U; Linux armv6l; fr; rv:1.9.1b1pre) Gecko/'\n     '20081005220218 Gecko/2008052201 Fennec/0.9pre', True),\n    # iPod touch\n    ('Mozilla/5.0 (iPod; U; CPU like Mac OS X; en) AppleWebKit/420.1 '\n     '(KHTML, like Gecko) Version/3.0 Mobile/3A101a Safari/419.3',\n     True),\n)\nfor test_url in urls:\n    for ua, forward_this in ua_patterns:\n        r = self.client.get(test_url, HTTP_USER_AGENT=ua)\n        if forward_this:\n            eq_(r.status_code, 302, test_url)\n            assert r['Location'].find(fake_mobile_domain) >= 0\n        else:\n            assert (r.status_code == 200 or  # Page is served, or:\n                    r.status_code / 100 == 3 and  # some redirect...\n                    # ... but not to the mobile domain.\n                    r['Location'].find(fake_mobile_domain) == -1)", "path": "apps\\input\\tests\\test_decorators.py", "repo_name": "fwenzel/reporter", "stars": 25, "license": "None", "language": "python", "size": 2425}
{"docstring": "\"\"\"Test domain extraction from URLs, for HTTP, about:, chrome.\"\"\"\n", "func_signal": "def test_domain_protocol(self):\n", "code": "test_domains = (\n    ('http://example.com', 'http', 'example.com'),\n    ('https://example.net:8080/abc', 'https', 'example.net:8080'),\n    ('about:config', 'about', 'config'),\n    ('chrome://something/exciting', 'chrome', 'something/exciting'),\n)\nfor url, protocol, domain in test_domains:\n    eq_(helpers.protocol(url), protocol)\n    eq_(helpers.domain(url), domain)", "path": "apps\\website_issues\\tests.py", "repo_name": "fwenzel/reporter", "stars": 25, "license": "None", "language": "python", "size": 2425}
{"docstring": "\"\"\"\nIf sphinx tells us there's an error, make sure we raise a\nSearchError.\nThis is unexpected, so we mock the behavior.\n\"\"\"\n", "func_signal": "def test_result_errors(self, rq):\n", "code": "rq.return_value = [dict(error='you lose')]\nself.assertRaises(SearchError, query)", "path": "apps\\search\\tests\\test_client.py", "repo_name": "fwenzel/reporter", "stars": 25, "license": "None", "language": "python", "size": 2425}
{"docstring": "\"\"\"Ensure date filters are applied in app time (= PST), not UTC.\"\"\"\n", "func_signal": "def test_date_filter_timezone():\n", "code": "dates = dict(date_start=datetime.date(2010, 1, 1),\n             date_end=datetime.date(2010, 1, 31))\n_, ranges, _ = extract_filters(dates)\neq_(ranges['created'][0], 1262332800)  # 8:00 UTC on 1/1/2010\n# 8:00 UTC on 2/1/2010 (sic, to include all of the last day)\neq_(ranges['created'][1], 1265011200)", "path": "apps\\search\\tests\\test_client.py", "repo_name": "fwenzel/reporter", "stars": 25, "license": "None", "language": "python", "size": 2425}
{"docstring": "\"\"\"\nRenders a smiley.\n\nStyle can be \"sad\" or \"happy\".\n\"\"\"\n", "func_signal": "def smiley(style, page=None):\n", "code": "if not style in ('happy', 'sad'):\n    return ''\nif style == 'happy':  # positive smiley\n    character = '&#9786;'\n    title = _('Praise')\nelse:  # negative smiley\n    character = '&#9785;'\n    title = _('Issue')\nreturn jinja2.Markup(\n    u'<span title=\"%s\" class=\"smiley %s %s\">%s</span>' % (\n        title, style, page, character))", "path": "apps\\feedback\\helpers.py", "repo_name": "fwenzel/reporter", "stars": 25, "license": "None", "language": "python", "size": 2425}
{"docstring": "\"\"\"Forward mobile requests to the same path on the mobile domain.\"\"\"\n\n", "func_signal": "def forward_mobile(f):\n", "code": "@wraps(f)\ndef wrapped(request, *args, **kwargs):\n    if (settings.SITE_ID == settings.DESKTOP_SITE_ID and\n        MOBILE_DEVICE_PATTERN.search(\n            request.META.get('HTTP_USER_AGENT', ''))):\n        mobile_site = Site.objects.get(id=settings.MOBILE_SITE_ID)\n        target = '%s://%s%s' % ('https' if request.is_secure() else 'http',\n                                mobile_site.domain, request.path)\n        if request.GET:\n            target = '%s?%s' % (target, urllib.urlencode(request.GET))\n\n        response = HttpResponseRedirect(target)\n        response['Vary'] = 'User-Agent'\n        return response\n\n    return f(request, *args, **kwargs)\n\nreturn wrapped", "path": "apps\\input\\decorators.py", "repo_name": "fwenzel/reporter", "stars": 25, "license": "None", "language": "python", "size": 2425}
{"docstring": "\"\"\"\nIf sphinx has no results, but gives us a weird result, let's return an\nempty list.\n\"\"\"\n", "func_signal": "def test_result_empty(self, rq):\n", "code": "rq.return_value = [dict(error=None)]\neq_(query(), [])", "path": "apps\\search\\tests\\test_client.py", "repo_name": "fwenzel/reporter", "stars": 25, "license": "None", "language": "python", "size": 2425}
{"docstring": "\"\"\"Any query should return results in rev-chron order.\"\"\"\n", "func_signal": "def test_default_ordering(self):\n", "code": "r = query()\ndates = [o.created for o in r]\neq_(dates, sorted(dates, reverse=True), \"These aren't revchron.\")\n\nr = query('Firefox')\ndates = [o.created for o in r]\neq_(dates, sorted(dates, reverse=True), \"These aren't revchron.\")", "path": "apps\\search\\tests\\test_client.py", "repo_name": "fwenzel/reporter", "stars": 25, "license": "None", "language": "python", "size": 2425}
{"docstring": "\"\"\"Single site for nonexistent url gives 404.\"\"\"\n", "func_signal": "def test_nonexistant_site(self):\n", "code": "for sentiment in [\"happy\", \"sad\", None]:\n    params = dict(url_='this.is.nonexistent.com', protocol='http')\n    view_url = reverse('single_site', kwargs=params)\n    if sentiment is not None:\n        view_url += '?sentiment=%s' % sentiment\n    r = self.client.get(view_url)\n    eq_(r.status_code, 404)\n    assert_true(len(r.content) > 0)", "path": "apps\\website_issues\\tests.py", "repo_name": "fwenzel/reporter", "stars": 25, "license": "None", "language": "python", "size": 2425}
{"docstring": "\"\"\"\nCache an entire page with a cache prefix based on the Site ID and\n(optionally) the GET parameters.\n\"\"\"\n# If the first argument is a callable, we've used the decorator without\n# args.\n", "func_signal": "def cache_page(cache_timeout=None, use_get=False, **kwargs):\n", "code": "if callable(cache_timeout):\n    f = cache_timeout\n    return cache_page()(f)\n\nif cache_timeout is None:\n    cache_timeout = settings.CACHE_DEFAULT_PERIOD\n\ndef key_prefix(request):\n    prefix = '%ss%d:' % (settings.CACHE_PREFIX, settings.SITE_ID)\n    if use_get:\n        prefix += md5_constructor(str(request.GET)).hexdigest()\n    return prefix\n\ndef wrap(f):\n    @wraps(f)\n    @cache_page_with_prefix(cache_timeout, key_prefix)\n    def cached_view(request, *args, **kwargs):\n        return f(request, *args, **kwargs)\n    return cached_view\nreturn wrap", "path": "apps\\input\\decorators.py", "repo_name": "fwenzel/reporter", "stars": 25, "license": "None", "language": "python", "size": 2425}
{"docstring": "\"\"\"Test urlparser for chrome and about URLs.\"\"\"\n\n# about:*\n", "func_signal": "def test_urlparse(self):\n", "code": "url = 'about:config'\np = utils.urlparse(url)\neq_(p.scheme, 'about')\neq_(p.netloc, 'config')\neq_(p.geturl(), url)\n\n# chrome\nurl = 'chrome://somewhere/special'\np = utils.urlparse(url)\neq_(p.scheme, 'chrome')\neq_(p.netloc, 'somewhere')\neq_(p.path, 'special')\neq_(p.geturl(), url)\n\n# HTTP (unchanged from Python)\nurl = 'http://example.com/something'\np = utils.urlparse(url)\neq_(p, urlparse_.urlparse(url))", "path": "apps\\website_issues\\tests.py", "repo_name": "fwenzel/reporter", "stars": 25, "license": "None", "language": "python", "size": 2425}
{"docstring": "\"\"\"Posts are present in the API XML.\"\"\"\n", "func_signal": "def testNumPosts(self):\n", "code": "for log in self.logs:\n    assert len(log.posts) > 0, \"No posts found!\"", "path": "tumblrtest.py", "repo_name": "braveulysses/tumblr-api", "stars": 18, "license": "mit", "language": "python", "size": 195}
{"docstring": "\"\"\"No posts are of an unrecognized type.\"\"\"\n", "func_signal": "def testNoUnknownPosts(self):\n", "code": "types = [ 'regular', \n    'link', \n    'quote', \n    'photo', \n    'conversation', \n    'video', \n    'audio' \n]\nfor log in self.logs:\n    for post in log.posts:\n        assert post.type in types", "path": "tumblrtest.py", "repo_name": "braveulysses/tumblr-api", "stars": 18, "license": "mit", "language": "python", "size": 195}
{"docstring": "\"\"\"Photo URLs should in fact be URLs.\"\"\"\n", "func_signal": "def testPhotoUrls(self):\n", "code": "for url in self.log.posts[0].urls.values():\n    assert isUrl(url)", "path": "tumblrtest.py", "repo_name": "braveulysses/tumblr-api", "stars": 18, "license": "mit", "language": "python", "size": 195}
{"docstring": "\"\"\"Tumblelog's title can be determined.\"\"\"\n", "func_signal": "def testHasTitle(self):\n", "code": "for log in self.logs:\n    assert log.title is not None and log.title != ''", "path": "tumblrtest.py", "repo_name": "braveulysses/tumblr-api", "stars": 18, "license": "mit", "language": "python", "size": 195}
{"docstring": "\"\"\"Redirect via HTTP 307 is recorded.\"\"\"\n", "func_signal": "def testHTTP307(self):\n", "code": "log = tumblr.parse(self.urlTemporaryRedirect)\nassert (log.http_response.previous.status == 307) and (log.http_response['content-location'] == self.urlRedirectDestination)", "path": "tumblrtest.py", "repo_name": "braveulysses/tumblr-api", "stars": 18, "license": "mit", "language": "python", "size": 195}
{"docstring": "\"\"\"Error thrown if content-type application/xhtml+xml used.\"\"\"\n", "func_signal": "def testContentTypeApplicationXhtmlXml(self):\n", "code": "try:\n    tumblr.parse(self.urlContentTypeApplicationXhtmlXml)\nexcept tumblr.UnsupportedContentTypeError:\n    pass\nelse:\n    self.fail(\"Expected an UnsupportedContentTypeError!\")", "path": "tumblrtest.py", "repo_name": "braveulysses/tumblr-api", "stars": 18, "license": "mit", "language": "python", "size": 195}
{"docstring": "\"\"\"Error thrown if URL has been removed (HTTP 410).\"\"\"\n", "func_signal": "def testHTTP410Gone(self):\n", "code": "try:\n    tumblr.parse(self.urlGone)\nexcept tumblr.URLGoneError:\n    pass\nelse:\n    self.fail(\"Expected a URLGoneError!\")", "path": "tumblrtest.py", "repo_name": "braveulysses/tumblr-api", "stars": 18, "license": "mit", "language": "python", "size": 195}
{"docstring": "\"\"\"Error thrown if URL cannot be found (HTTP 404).\"\"\"\n", "func_signal": "def testHTTP404NotFound(self):\n", "code": "try:\n    tumblr.parse(self.urlNotFound)\nexcept tumblr.URLNotFoundError:\n    pass\nelse:\n    self.fail(\"Expected a URLNotFoundError!\")", "path": "tumblrtest.py", "repo_name": "braveulysses/tumblr-api", "stars": 18, "license": "mit", "language": "python", "size": 195}
{"docstring": "\"\"\"An open file object can be passed to the parser.\"\"\"\n", "func_signal": "def testOpenFile(self):\n", "code": "f = open(self.filename, 'r')\nlog = tumblr.parse(f)\nf.close()\nassert log.title == 'golden hours'", "path": "tumblrtest.py", "repo_name": "braveulysses/tumblr-api", "stars": 18, "license": "mit", "language": "python", "size": 195}
{"docstring": "\"\"\"Error thrown if XML is not well-formed.\"\"\"\n", "func_signal": "def testMalformedXML(self):\n", "code": "try:\n    tumblr.parse(self.urlMalformed)\nexcept tumblr.TumblrParseError:\n    pass\nelse:\n    fail(\"Expected a TumblrParseError for malformed XML!\")", "path": "tumblrtest.py", "repo_name": "braveulysses/tumblr-api", "stars": 18, "license": "mit", "language": "python", "size": 195}
{"docstring": "\"\"\"Typical Content-Type with charset can be parsed.\"\"\"\n", "func_signal": "def testContentTypeAndCharset(self):\n", "code": "ct = 'application/xml; charset=utf-8'\ncontentType, charset = tumblr._parse_content_type(ct)\nassert (contentType == 'application/xml') and (charset == 'utf-8')", "path": "tumblrtest.py", "repo_name": "braveulysses/tumblr-api", "stars": 18, "license": "mit", "language": "python", "size": 195}
{"docstring": "\"\"\"Should accept HTTP content-type text/xml.\"\"\"\n", "func_signal": "def testContentTypeTextXml(self):\n", "code": "log = tumblr.parse(self.urlContentTypeTextXml)\n# Just do anything\nassert log.name == u'demo'", "path": "tumblrtest.py", "repo_name": "braveulysses/tumblr-api", "stars": 18, "license": "mit", "language": "python", "size": 195}
{"docstring": "\"\"\"Redirect via HTTP 301 is recorded.\"\"\"\n", "func_signal": "def testHTTP301MovedPermanently(self):\n", "code": "log = tumblr.parse(self.urlMovedPermanently)\nassert (log.http_response.previous.status == 301) and (log.http_response['content-location'] == self.urlRedirectDestination)", "path": "tumblrtest.py", "repo_name": "braveulysses/tumblr-api", "stars": 18, "license": "mit", "language": "python", "size": 195}
{"docstring": "\"\"\"Redirect via HTTP 302 is recorded.\"\"\"\n", "func_signal": "def testHTTP302Found(self):\n", "code": "log = tumblr.parse(self.urlFound)\nassert (log.http_response.previous.status == 302) and (log.http_response['content-location'] == self.urlRedirectDestination)", "path": "tumblrtest.py", "repo_name": "braveulysses/tumblr-api", "stars": 18, "license": "mit", "language": "python", "size": 195}
{"docstring": "\"\"\"Typical Content-Type with no charset can be parsed.\"\"\"\n", "func_signal": "def testContentTypeWithoutCharset(self):\n", "code": "ct = 'text/xml'\ncontentType, charset = tumblr._parse_content_type(ct)\nassert (contentType == 'text/xml') and (charset is None)", "path": "tumblrtest.py", "repo_name": "braveulysses/tumblr-api", "stars": 18, "license": "mit", "language": "python", "size": 195}
{"docstring": "\"\"\"Error thrown if URL is forbidden (HTTP 403).\"\"\"\n", "func_signal": "def testHTTP403Forbidden(self):\n", "code": "try:\n    tumblr.parse(self.urlForbidden)\nexcept tumblr.URLForbiddenError:\n    pass\nelse:\n    self.fail(\"Expected a URLForbiddenError!\")", "path": "tumblrtest.py", "repo_name": "braveulysses/tumblr-api", "stars": 18, "license": "mit", "language": "python", "size": 195}
{"docstring": "\"\"\"Should accept HTTP content-type application/xml.\"\"\"\n", "func_signal": "def testContentTypeApplicationXml(self):\n", "code": "log = tumblr.parse(self.urlContentTypeApplicationXml)\n# Just do anything\nassert log.name == u'demo'", "path": "tumblrtest.py", "repo_name": "braveulysses/tumblr-api", "stars": 18, "license": "mit", "language": "python", "size": 195}
{"docstring": "\"\"\"Error thrown if HTTP 503 occurs.\"\"\"\n", "func_signal": "def testHTTP503ServiceUnavailableError(self):\n", "code": "try:\n    tumblr.parse(self.urlServiceUnavailable)\nexcept tumblr.ServiceUnavailableError:\n    pass\nelse:\n    self.fail(\"Expected a ServiceUnavailableError!\")", "path": "tumblrtest.py", "repo_name": "braveulysses/tumblr-api", "stars": 18, "license": "mit", "language": "python", "size": 195}
{"docstring": "\"\"\"Error thrown if HTTP 500 occurs.\"\"\"\n", "func_signal": "def testHTTP500ServerError(self):\n", "code": "try:\n    tumblr.parse(self.urlServerError)\nexcept tumblr.InternalServerError:\n    pass\nelse:\n    self.fail(\"Expected an InternalServerError!\")", "path": "tumblrtest.py", "repo_name": "braveulysses/tumblr-api", "stars": 18, "license": "mit", "language": "python", "size": 195}
{"docstring": "\"\"\"Attempts to determine if the given string is really an HTTP URL.\n\nThis is a quick-and-dirty test that just looks for an http protocol handler.\"\"\"\n", "func_signal": "def isUrl(str):\n", "code": "u = urlparse.urlparse(str)\nif u[0] == 'http' or u[0] == 'https':\n    return True\nelse:\n    return False", "path": "tumblrtest.py", "repo_name": "braveulysses/tumblr-api", "stars": 18, "license": "mit", "language": "python", "size": 195}
{"docstring": "\"\"\"\nCheck if any column of the tic-tac-toe board is complete\n\n:return (bool): True if any column is complete, False otherwise\n\"\"\"\n", "func_signal": "def is_any_column_complete(self):\n", "code": "for i in range(3):\n    val = self.board[0][i]\n    if val != self.EMPTY and self.board[1][i] == val and self.board[2][i] == val:\n        return True\n\nreturn False", "path": "aima\\core\\environment\\tictactoe.py", "repo_name": "mushketyk/aima-python", "stars": 17, "license": "None", "language": "python", "size": 265}
{"docstring": "\"\"\"\nGet number of queens attacking on a specified location\n\n:param location (XYLocation):\n:return (int): number of attacking queens\n\"\"\"\n", "func_signal": "def get_number_of_attacks_on(self, location):\n", "code": "return self.number_of_horizontal_attacks_on(location) + self.number_of_vertical_attacks_on(location) + \\\n       self.number_of_diagonal_attacks_on(location)", "path": "aima\\core\\environment\\nqueens.py", "repo_name": "mushketyk/aima-python", "stars": 17, "license": "None", "language": "python", "size": 265}
{"docstring": "# handle command line args for test discovery\n", "func_signal": "def _do_discovery(argv, verbosity, Loader):\n", "code": "parser = optparse.OptionParser()\nparser.add_option('-v', '--verbose', dest='verbose', default=False,\n                  help='Verbose output', action='store_true')\nparser.add_option('-s', '--start-directory', dest='start', default='.',\n                  help=\"Directory to start discovery ('.' default)\")\nparser.add_option('-p', '--pattern', dest='pattern', default='test*.py',\n                  help=\"Pattern to match tests ('test*.py' default)\")\nparser.add_option('-t', '--top-level-directory', dest='top', default=None,\n                  help='Top level directory of project (defaults to start directory)')\n\noptions, args = parser.parse_args(argv)\nif len(args) > 3:\n    _usage_exit()\n\nfor name, value in zip(('start', 'pattern', 'top'), args):\n    setattr(options, name, value)\n\nif options.verbose:\n    verbosity = 2\n\nstart_dir = options.start\npattern = options.pattern\ntop_level_dir = options.top\n\nloader = Loader()\nreturn loader.discover(start_dir, pattern, top_level_dir), verbosity", "path": "discover.py", "repo_name": "mushketyk/aima-python", "stars": 17, "license": "None", "language": "python", "size": 265}
{"docstring": "\"\"\"Return a suite of all tests cases contained in testCaseClass\"\"\"\n", "func_signal": "def loadTestsFromTestCase(self, testCaseClass):\n", "code": "if issubclass(testCaseClass, unittest.TestSuite):\n    raise TypeError(\"Test cases should not be derived from TestSuite.\"\n                    \" Maybe you meant to derive from TestCase?\")\ntestCaseNames = self.getTestCaseNames(testCaseClass)\nif not testCaseNames and hasattr(testCaseClass, 'runTest'):\n    testCaseNames = ['runTest']\nloaded_suite = self.suiteClass(map(testCaseClass, testCaseNames))\nreturn loaded_suite", "path": "discover.py", "repo_name": "mushketyk/aima-python", "stars": 17, "license": "None", "language": "python", "size": 265}
{"docstring": "\"\"\"\nReturn number of queens on a board\n\n:return (int): number of queens on a board\n\"\"\"\n", "func_signal": "def get_number_of_queens_on_board(self):\n", "code": "counter = 0\n\nfor r in range(0, self.size):\n    for c in range(0, self.size):\n        if self.squares[r][c] == self.QUEEN:\n            counter += 1\nreturn counter", "path": "aima\\core\\environment\\nqueens.py", "repo_name": "mushketyk/aima-python", "stars": 17, "license": "None", "language": "python", "size": 265}
{"docstring": "# import __main__ triggers code re-execution\n", "func_signal": "def collector():\n", "code": "__main__ = sys.modules['__main__']\nsetupDir = os.path.abspath(os.path.dirname(__main__.__file__))\nreturn defaultTestLoader.discover(setupDir)", "path": "discover.py", "repo_name": "mushketyk/aima-python", "stars": 17, "license": "None", "language": "python", "size": 265}
{"docstring": "\"\"\"Return a sorted sequence of method names found within testCaseClass\n\"\"\"\n", "func_signal": "def getTestCaseNames(self, testCaseClass):\n", "code": "def isTestMethod(attrname, testCaseClass=testCaseClass,\n                 prefix=self.testMethodPrefix):\n    return attrname.startswith(prefix) and \\\n        hasattr(getattr(testCaseClass, attrname), '__call__')\ntestFnNames = list(filter(isTestMethod, dir(testCaseClass)))\nif self.sortTestMethodsUsing:\n    testFnNames.sort(key=_CmpToKey(self.sortTestMethodsUsing))\nreturn testFnNames", "path": "discover.py", "repo_name": "mushketyk/aima-python", "stars": 17, "license": "None", "language": "python", "size": 265}
{"docstring": "\"\"\"\nGet number of attacking pairs\n\n:return (int): number of attacking pairs\n\"\"\"\n", "func_signal": "def get_number_of_attacking_pairs(self):\n", "code": "result = 0\nfor location in self.get_queen_positions():\n    result += self.get_number_of_attacks_on(location)\n\nreturn result / 2", "path": "aima\\core\\environment\\nqueens.py", "repo_name": "mushketyk/aima-python", "stars": 17, "license": "None", "language": "python", "size": 265}
{"docstring": "\"\"\"\nCheck if any of tic-tac-toe diagonals is complete\n\n:return (bool): True if any diagoanl is complete, False otherwise\n\"\"\"\n", "func_signal": "def is_diagonal_complete(self):\n", "code": "val = self.board[0][0]\n\nif val != self.EMPTY and self.board[1][1] == val and self.board[2][2] == val:\n    return True\n\nval = self.board[0][2]\nif val != self.EMPTY and self.board[1][1] == val and self.board[2][0] == val:\n    return True\n\nreturn False", "path": "aima\\core\\environment\\tictactoe.py", "repo_name": "mushketyk/aima-python", "stars": 17, "license": "None", "language": "python", "size": 265}
{"docstring": "\"\"\"Return a suite of all tests cases found using the given sequence\nof string specifiers. See 'loadTestsFromName()'.\n\"\"\"\n", "func_signal": "def loadTestsFromNames(self, names, module=None):\n", "code": "suites = [self.loadTestsFromName(name, module) for name in names]\nreturn self.suiteClass(suites)", "path": "discover.py", "repo_name": "mushketyk/aima-python", "stars": 17, "license": "None", "language": "python", "size": 265}
{"docstring": "\"\"\"\nGet positions of queens on a board\n\n:return (list of XYLocation): list of locations of queens\n\"\"\"\n", "func_signal": "def get_queen_positions(self):\n", "code": "locations = []\n\nfor r in range(0, self.size):\n    for c in range(0, self.size):\n        if self.squares[r][c] == self.QUEEN:\n            locations.append(XYLocation(c, r))\nreturn locations", "path": "aima\\core\\environment\\nqueens.py", "repo_name": "mushketyk/aima-python", "stars": 17, "license": "None", "language": "python", "size": 265}
{"docstring": "\"\"\"Find and return all test modules from the specified start\ndirectory, recursing into subdirectories to find them. Only test files\nthat match the pattern will be loaded. (Using shell style pattern\nmatching.)\n\nAll test modules must be importable from the top level of the project.\nIf the start directory is not the top level directory then the top\nlevel directory must be specified separately.\n\nIf a test package name (directory with '__init__.py') matches the\npattern then the package will be checked for a 'load_tests' function. If\nthis exists then it will be called with loader, tests, pattern.\n\nIf load_tests exists then discovery does  *not* recurse into the package,\nload_tests is responsible for loading all tests in the package.\n\nThe pattern is deliberately not stored as a loader attribute so that\npackages can continue discovery themselves. top_level_dir is stored so\nload_tests does not need to pass this argument in to loader.discover().\n\"\"\"\n", "func_signal": "def discover(self, start_dir, pattern='test*.py', top_level_dir=None):\n", "code": "set_implicit_top = False\nif top_level_dir is None and self._top_level_dir is not None:\n    # make top_level_dir optional if called from load_tests in a package\n    top_level_dir = self._top_level_dir\nelif top_level_dir is None:\n    set_implicit_top = True\n    top_level_dir = start_dir\n\ntop_level_dir = os.path.abspath(top_level_dir)\n\nif not top_level_dir in sys.path:\n    # all test modules must be importable from the top level directory\n    # should we *unconditionally* put the start directory in first\n    # in sys.path to minimise likelihood of conflicts between installed\n    # modules and development versions?\n    sys.path.insert(0, top_level_dir)\nself._top_level_dir = top_level_dir\n\nis_not_importable = False\nif os.path.isdir(os.path.abspath(start_dir)):\n    start_dir = os.path.abspath(start_dir)\n    if start_dir != top_level_dir:\n        is_not_importable = not os.path.isfile(os.path.join(start_dir, '__init__.py'))\nelse:\n    # support for discovery from dotted module names\n    try:\n        __import__(start_dir)\n    except ImportError:\n        is_not_importable = True\n    else:\n        the_module = sys.modules[start_dir]\n        top_part = start_dir.split('.')[0]\n        start_dir = os.path.abspath(os.path.dirname((the_module.__file__)))\n        if set_implicit_top:\n            self._top_level_dir = os.path.abspath(os.path.dirname(os.path.dirname(sys.modules[top_part].__file__)))\n            sys.path.remove(top_level_dir)\n\nif is_not_importable:\n    raise ImportError('Start directory is not importable: %r' % start_dir)\n\ntests = list(self._find_tests(start_dir, pattern))\nreturn self.suiteClass(tests)", "path": "discover.py", "repo_name": "mushketyk/aima-python", "stars": 17, "license": "None", "language": "python", "size": 265}
{"docstring": "\"\"\"\nRemove all queens from a board.\n\n:return: None\n\"\"\"\n", "func_signal": "def clean(self):\n", "code": "for r in range(0, self.size):\n    for c in range(0, self.size):\n        self.squares[r][c] = 0", "path": "aima\\core\\environment\\nqueens.py", "repo_name": "mushketyk/aima-python", "stars": 17, "license": "None", "language": "python", "size": 265}
{"docstring": "\"\"\"Used by discovery. Yields test suites it loads.\"\"\"\n", "func_signal": "def _find_tests(self, start_dir, pattern):\n", "code": "paths = os.listdir(start_dir)\n\nfor path in paths:\n    full_path = os.path.join(start_dir, path)\n    if os.path.isfile(full_path):\n        if not VALID_MODULE_NAME.match(path):\n            # valid Python identifiers only\n            continue\n        if not self._match_path(path, full_path, pattern):\n            continue\n        # if the test file matches, load it\n        name = self._get_name_from_path(full_path)\n        try:\n            module = self._get_module_from_name(name)\n        except:\n            yield _make_failed_import_test(name, self.suiteClass)\n        else:\n            mod_file = os.path.abspath(getattr(module, '__file__', full_path))\n            realpath = os.path.splitext(mod_file)[0]\n            fullpath_noext = os.path.splitext(full_path)[0]\n            if realpath.lower() != fullpath_noext.lower():\n                module_dir = os.path.dirname(realpath)\n                mod_name = os.path.splitext(os.path.basename(full_path))[0]\n                expected_dir = os.path.dirname(full_path)\n                msg = (\"%r module incorrectly imported from %r. Expected %r. \"\n                       \"Is this module globally installed?\")\n                raise ImportError(msg % (mod_name, module_dir, expected_dir))\n            yield self.loadTestsFromModule(module)\n    elif os.path.isdir(full_path):\n        if not os.path.isfile(os.path.join(full_path, '__init__.py')):\n            continue\n\n        load_tests = None\n        tests = None\n        if fnmatch(path, pattern):\n            # only check load_tests if the package directory itself matches the filter\n            name = self._get_name_from_path(full_path)\n            package = self._get_module_from_name(name)\n            load_tests = getattr(package, 'load_tests', None)\n            tests = self.loadTestsFromModule(package, use_load_tests=False)\n\n        if load_tests is None:\n            if tests is not None:\n                # tests loaded from package file\n                yield tests\n            # recurse into the package\n            for test in self._find_tests(full_path, pattern):\n                yield test\n        else:\n            try:\n                yield load_tests(self, tests, pattern)\n            except:\n                ExceptionClass, e = sys.exc_info()[:2]\n                if not isinstance(e, Exception):\n                    # for BaseException exceptions\n                    raise\n                yield _make_failed_load_tests(package.__name__, e,\n                                              self.suiteClass)", "path": "discover.py", "repo_name": "mushketyk/aima-python", "stars": 17, "license": "None", "language": "python", "size": 265}
{"docstring": "\"\"\"\nCheck if square is under attack of other queen\n\n:param location (XYLocation): location to check\n:return (bool): True if location is under attack, False otherwise\n\"\"\"\n", "func_signal": "def is_square_under_attack(self, location):\n", "code": "return self.is_square_horizontally_attacked(location) or self.is_square_vertically_attacked(location) or \\\n       self.is_square_diagonally_attacked(location)", "path": "aima\\core\\environment\\nqueens.py", "repo_name": "mushketyk/aima-python", "stars": 17, "license": "None", "language": "python", "size": 265}
{"docstring": "\"\"\"\nCheck if any row of the tic-tac-toe board is complete\n\n:return (bool): True if any row is complete, False otherwise\n\"\"\"\n", "func_signal": "def is_any_row_complete(self):\n", "code": "for i in range(3):\n    val = self.board[i][0]\n    if val != self.EMPTY and self.board[i][1] == val and self.board[i][2] == val:\n        return True\n\nreturn False", "path": "aima\\core\\environment\\tictactoe.py", "repo_name": "mushketyk/aima-python", "stars": 17, "license": "None", "language": "python", "size": 265}
{"docstring": "\"\"\"Return a relative version of a path\"\"\"\n        \n", "func_signal": "def relpath(path, start=os.path.curdir):\n", "code": "if not path:\n    raise ValueError(\"no path specified\")\nstart_list = os.path.abspath(start).split(os.path.sep)\npath_list = os.path.abspath(path).split(os.path.sep)\nif start_list[0].lower() != path_list[0].lower():\n    unc_path, rest = os.path.splitunc(path)\n    unc_start, rest = os.path.splitunc(start)\n    if bool(unc_path) ^ bool(unc_start):\n        raise ValueError(\"Cannot mix UNC and non-UNC paths (%s and %s)\"\n                                                            % (path, start))\n    else:\n        raise ValueError(\"path is on drive %s, start on drive %s\"\n                                            % (path_list[0], start_list[0]))\n# Work out how much of the filepath is shared by start and path.\nfor i in range(min(len(start_list), len(path_list))):\n    if start_list[i].lower() != path_list[i].lower():\n        break\nelse:\n    i += 1\n        \nrel_list = [os.path.pardir] * (len(start_list)-i) + path_list[i:]\nif not rel_list:\n    return os.path.curdir\nreturn os.path.join(*rel_list)", "path": "discover.py", "repo_name": "mushketyk/aima-python", "stars": 17, "license": "None", "language": "python", "size": 265}
{"docstring": "\"\"\"\nMove queen from location from_location to location to_location\n\n:param from_location (XYLocation): - location to move queen from\n:param to_location (XYLocation): - location to move queen to\n:return: None\n\"\"\"\n", "func_signal": "def move_queen(self, from_location, to_location):\n", "code": "if self.queen_exists_at(from_location) and (not self.queen_exists_at(to_location)):\n    self.remove_queen_from(from_location)\n    self.add_queen_at(to_location)", "path": "aima\\core\\environment\\nqueens.py", "repo_name": "mushketyk/aima-python", "stars": 17, "license": "None", "language": "python", "size": 265}
{"docstring": "\"\"\"Return a suite of all tests cases contained in the given module\"\"\"\n", "func_signal": "def loadTestsFromModule(self, module, use_load_tests=True):\n", "code": "tests = []\nfor name in dir(module):\n    obj = getattr(module, name)\n    if isinstance(obj, type) and issubclass(obj, unittest.TestCase):\n        tests.append(self.loadTestsFromTestCase(obj))\n\nload_tests = getattr(module, 'load_tests', None)\ntests = self.suiteClass(tests)\nif use_load_tests and load_tests is not None:\n    try:\n        return load_tests(self, tests, None)\n    except:\n        ExceptionClass, e = sys.exc_info()[:2]\n        if not isinstance(e, Exception):\n            # for BaseException exceptions\n            raise\n        return _make_failed_load_tests(module.__name__, e,\n                                       self.suiteClass)\nreturn tests", "path": "discover.py", "repo_name": "mushketyk/aima-python", "stars": 17, "license": "None", "language": "python", "size": 265}
{"docstring": "\"\"\"\nClean board and set queens in specified locations.\n\n:param locations (list of XYLocation): locatios to set queens to.\n:return: None\n\"\"\"\n", "func_signal": "def set_board(self, locations):\n", "code": "self.clean()\nfor location in locations:\n    self.add_queen_at(location)", "path": "aima\\core\\environment\\nqueens.py", "repo_name": "mushketyk/aima-python", "stars": 17, "license": "None", "language": "python", "size": 265}
{"docstring": "\"\"\"Returns the closest sibling to this Tag that matches the\ngiven criteria and appears before this Tag in the document.\"\"\"\n", "func_signal": "def findPreviousSibling(self, name=None, attrs={}, text=None, **kwargs):\n", "code": "return self._findOne(self.findPreviousSiblings, name, attrs, text,\n                     **kwargs)", "path": "lib\\BeautifulSoup.py", "repo_name": "balle/chaosmap", "stars": 16, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Returns the parents of this Tag that match the given\ncriteria.\"\"\"\n\n", "func_signal": "def findParents(self, name=None, attrs={}, limit=None, **kwargs):\n", "code": "return self._findAll(name, attrs, None, limit, self.parentGenerator,\n                     **kwargs)", "path": "lib\\BeautifulSoup.py", "repo_name": "balle/chaosmap", "stars": 16, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Destructively rips this element out of the tree.\"\"\"\n", "func_signal": "def extract(self):\n", "code": "if self.parent:\n    try:\n        del self.parent.contents[self.parent.index(self)]\n    except ValueError:\n        pass\n\n#Find the two elements that would be next to each other if\n#this element (and any children) hadn't been parsed. Connect\n#the two.\nlastChild = self._lastRecursiveChild()\nnextElement = lastChild.next\n\nif self.previous:\n    self.previous.next = nextElement\nif nextElement:\n    nextElement.previous = self.previous\nself.previous = None\nlastChild.next = None\n\nself.parent = None\nif self.previousSibling:\n    self.previousSibling.nextSibling = self.nextSibling\nif self.nextSibling:\n    self.nextSibling.previousSibling = self.previousSibling\nself.previousSibling = self.nextSibling = None\nreturn self", "path": "lib\\BeautifulSoup.py", "repo_name": "balle/chaosmap", "stars": 16, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Initializes a map representation of this tag's attributes,\nif not already initialized.\"\"\"\n", "func_signal": "def _getAttrMap(self):\n", "code": "if not getattr(self, 'attrMap'):\n    self.attrMap = {}\n    for (key, value) in self.attrs:\n        self.attrMap[key] = value\nreturn self.attrMap", "path": "lib\\BeautifulSoup.py", "repo_name": "balle/chaosmap", "stars": 16, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "#print \"Push\", tag.name\n", "func_signal": "def pushTag(self, tag):\n", "code": "if self.currentTag:\n    self.currentTag.contents.append(tag)\nself.tagStack.append(tag)\nself.currentTag = self.tagStack[-1]", "path": "lib\\BeautifulSoup.py", "repo_name": "balle/chaosmap", "stars": 16, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Extract all children.\"\"\"\n", "func_signal": "def clear(self):\n", "code": "for child in self.contents[:]:\n    child.extract()", "path": "lib\\BeautifulSoup.py", "repo_name": "balle/chaosmap", "stars": 16, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Encodes an object to a string in some encoding, or to Unicode.\n.\"\"\"\n", "func_signal": "def toEncoding(self, s, encoding=None):\n", "code": "if isinstance(s, unicode):\n    if encoding:\n        s = s.encode(encoding)\nelif isinstance(s, str):\n    if encoding:\n        s = s.encode(encoding)\n    else:\n        s = unicode(s)\nelse:\n    if encoding:\n        s  = self.toEncoding(str(s), encoding)\n    else:\n        s = unicode(s)\nreturn s", "path": "lib\\BeautifulSoup.py", "repo_name": "balle/chaosmap", "stars": 16, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Used in a call to re.sub to replace HTML, XML, and numeric\nentities with the appropriate Unicode characters. If HTML\nentities are being converted, any unrecognized entities are\nescaped.\"\"\"\n", "func_signal": "def _convertEntities(self, match):\n", "code": "x = match.group(1)\nif self.convertHTMLEntities and x in name2codepoint:\n    return unichr(name2codepoint[x])\nelif x in self.XML_ENTITIES_TO_SPECIAL_CHARS:\n    if self.convertXMLEntities:\n        return self.XML_ENTITIES_TO_SPECIAL_CHARS[x]\n    else:\n        return u'&%s;' % x\nelif len(x) > 0 and x[0] == '#':\n    # Handle numeric entities\n    if len(x) > 1 and x[1] == 'x':\n        return unichr(int(x[2:], 16))\n    else:\n        return unichr(int(x[1:]))\n\nelif self.escapeUnrecognizedEntities:\n    return u'&amp;%s;' % x\nelse:\n    return u'&%s;' % x", "path": "lib\\BeautifulSoup.py", "repo_name": "balle/chaosmap", "stars": 16, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Explicitly send END to stop the lookup process and disconnect\"\"\"\n", "func_signal": "def disconnect(self):\n", "code": "if not self._connected: return\n\nself._sendline(\"END\")\nself._disconnect()\nself._connected=False", "path": "lib\\cymruwhois.py", "repo_name": "balle/chaosmap", "stars": 16, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Generate sublists from an iterator\n>>> list(iterwindow(iter(range(10)),11))\n[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]\n>>> list(iterwindow(iter(range(10)),9))\n[[0, 1, 2, 3, 4, 5, 6, 7, 8], [9]]\n>>> list(iterwindow(iter(range(10)),5))\n[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n>>> list(iterwindow(iter(range(10)),3))\n[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n>>> list(iterwindow(iter(range(10)),1))\n[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]]\n\"\"\"\n\n", "func_signal": "def iterwindow(l, slice=50):\n", "code": "assert(slice > 0)\na=[]\n\nfor x in l:\n    if len(a) >= slice :\n        yield a\n        a=[]\n    a.append(x)\n\nif a:\n    yield a", "path": "lib\\cymruwhois.py", "repo_name": "balle/chaosmap", "stars": 16, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Explicitly connect and send BEGIN to start the lookup process\"\"\"\n", "func_signal": "def _begin(self):\n", "code": "self._connect()\nself._sendline(\"BEGIN\")\nself._readline() #discard the message \"Bulk mode; one IP per line. [2005-08-02 18:54:55 GMT]\"\nself._sendline(\"PREFIX\")\nself._sendline(\"COUNTRYCODE\")\nself._sendline(\"NOTRUNC\")\nself._connected=True", "path": "lib\\cymruwhois.py", "repo_name": "balle/chaosmap", "stars": 16, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Adds a certain piece of text to the tree as a NavigableString\nsubclass.\"\"\"\n", "func_signal": "def _toStringSubclass(self, text, subclass):\n", "code": "self.endData()\nself.handle_data(text)\nself.endData(subclass)", "path": "lib\\BeautifulSoup.py", "repo_name": "balle/chaosmap", "stars": 16, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Returns true iff the given string is the name of a\nself-closing tag according to this parser.\"\"\"\n", "func_signal": "def isSelfClosingTag(self, name):\n", "code": "return self.SELF_CLOSING_TAGS.has_key(name) \\\n       or self.instanceSelfClosingTags.has_key(name)", "path": "lib\\BeautifulSoup.py", "repo_name": "balle/chaosmap", "stars": 16, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "#print \"End tag %s\" % name\n", "func_signal": "def unknown_endtag(self, name):\n", "code": "if self.quoteStack and self.quoteStack[-1] != name:\n    #This is not a real end tag.\n    #print \"</%s> is not real!\" % name\n    self.handle_data('</%s>' % name)\n    return\nself.endData()\nself._popToTag(name)\nif self.quoteStack and self.quoteStack[-1] == name:\n    self.quoteStack.pop()\n    self.literal = (len(self.quoteStack) > 0)", "path": "lib\\BeautifulSoup.py", "repo_name": "balle/chaosmap", "stars": 16, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Returns the closest sibling to this Tag that matches the\ngiven criteria and appears after this Tag in the document.\"\"\"\n", "func_signal": "def findNextSibling(self, name=None, attrs={}, text=None, **kwargs):\n", "code": "return self._findOne(self.findNextSiblings, name, attrs, text,\n                     **kwargs)", "path": "lib\\BeautifulSoup.py", "repo_name": "balle/chaosmap", "stars": 16, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Returns true iff this tag has the same name, the same attributes,\nand the same contents (recursively) as the given tag.\n\nNOTE: right now this will return false if two tags have the\nsame attributes in a different order. Should this be fixed?\"\"\"\n", "func_signal": "def __eq__(self, other):\n", "code": "if other is self:\n    return True\nif not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):\n    return False\nfor i in range(0, len(self.contents)):\n    if self.contents[i] != other.contents[i]:\n        return False\nreturn True", "path": "lib\\BeautifulSoup.py", "repo_name": "balle/chaosmap", "stars": 16, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Handle a processing instruction as a ProcessingInstruction\nobject, possibly one with a %SOUP-ENCODING% slot into which an\nencoding will be plugged later.\"\"\"\n", "func_signal": "def handle_pi(self, text):\n", "code": "if text[:3] == \"xml\":\n    text = u\"xml version='1.0' encoding='%SOUP-ENCODING%'\"\nself._toStringSubclass(text, ProcessingInstruction)", "path": "lib\\BeautifulSoup.py", "repo_name": "balle/chaosmap", "stars": 16, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Returns the closest parent of this Tag that matches the given\ncriteria.\"\"\"\n# NOTE: We can't use _findOne because findParents takes a different\n# set of arguments.\n", "func_signal": "def findParent(self, name=None, attrs={}, **kwargs):\n", "code": "r = None\nl = self.findParents(name, attrs, 1)\nif l:\n    r = l[0]\nreturn r", "path": "lib\\BeautifulSoup.py", "repo_name": "balle/chaosmap", "stars": 16, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Replace the contents of the tag with a string\"\"\"\n", "func_signal": "def setString(self, string):\n", "code": "self.clear()\nself.append(string)", "path": "lib\\BeautifulSoup.py", "repo_name": "balle/chaosmap", "stars": 16, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Pops the tag stack up to and including the most recent\ninstance of the given tag. If inclusivePop is false, pops the tag\nstack up to but *not* including the most recent instqance of\nthe given tag.\"\"\"\n#print \"Popping to %s\" % name\n", "func_signal": "def _popToTag(self, name, inclusivePop=True):\n", "code": "if name == self.ROOT_TAG_NAME:\n    return\n\nnumPops = 0\nmostRecentTag = None\nfor i in range(len(self.tagStack)-1, 0, -1):\n    if name == self.tagStack[i].name:\n        numPops = len(self.tagStack)-i\n        break\nif not inclusivePop:\n    numPops = numPops - 1\n\nfor i in range(0, numPops):\n    mostRecentTag = self.popTag()\nreturn mostRecentTag", "path": "lib\\BeautifulSoup.py", "repo_name": "balle/chaosmap", "stars": 16, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\" Override eq so we can do better than object id comparisons \"\"\"\n", "func_signal": "def __eq__(self,other):\n", "code": "if other == None:\n\treturn False\nreturn self.name == other.name", "path": "crawler.py", "repo_name": "petterw/crawler", "stars": 21, "license": "gpl-3.0", "language": "python", "size": 129}
{"docstring": "# MK1996 says, 'If a %xx encoded octet is encountered it is unencoded \n# prior to comparison, unless it is the \"/\" character, which has \n# special meaning in a path.'\n", "func_signal": "def _unquote_path(path):\n", "code": "path = re.sub(\"%2[fF]\", \"\\n\", path)\npath = urllib_unquote(path)\nreturn path.replace(\"\\n\", \"%2F\")", "path": "robotexclusionrulesparser.py", "repo_name": "petterw/crawler", "stars": 21, "license": "gpl-3.0", "language": "python", "size": 129}
{"docstring": "# I have to exec() this code because the Python 2 syntax is invalid\n# under Python 3 and vice-versa.\n", "func_signal": "def _raise_error(error, message):\n", "code": "if PY_MAJOR_VERSION == 2:\n    #raise error, message\n    s = \"raise error, message\"\nelse:\n    #raise error(message)\n    s = \"raise error(message)\"\n    \nexec(s)", "path": "robotexclusionrulesparser.py", "repo_name": "petterw/crawler", "stars": 21, "license": "gpl-3.0", "language": "python", "size": 129}
{"docstring": "\"\"\" Claim a crawl time \"\"\"\n", "func_signal": "def claim_next_crawl_time(self):\n", "code": "if self.rp.is_expired():\n\tself.setup_robots_txt()\nif not self.parsed_robots_txt:\n\tself.parse_robots_txt()\nself.last_crawl_time = self.last_crawl_time + self.crawl_delay\nreturn self.last_crawl_time", "path": "crawler.py", "repo_name": "petterw/crawler", "stars": 21, "license": "gpl-3.0", "language": "python", "size": 129}
{"docstring": "\"\"\" Assigns score to a document, used for sorting retrieval queue to find next\n\turls to crawl. This is probably not the best ranking method but hey.. WIP^TM\n\"\"\"\n# if nobody thinks the site is worth linking to, then who are we to argue?\n", "func_signal": "def rank(self, document):\n", "code": "if len(document.referrers) == 0:\n\treturn 0.0\n\n# assign a score based on who links to the document\nancestor_linkjuice = 0.0\nfor referrer in document.referrers:\n\tif referrer != None and self.links[referrer.url] != None and not document.domain == referrer.domain:\n\t\tancestor_linkjuice += 1.0/(len(self.links[referrer.url]))\n\nreturn ancestor_linkjuice/len(self.results)", "path": "crawler.py", "repo_name": "petterw/crawler", "stars": 21, "license": "gpl-3.0", "language": "python", "size": 129}
{"docstring": "\"\"\" Suspends crawl to file and returns filename \"\"\"\n", "func_signal": "def suspend(self):\n", "code": "if not os.path.exists(self.outputdir):\n\tos.makedirs(self.outputdir)\nthisrun = self.outputdir + os.sep + str(int(self.start_stop_tuples[0][0]))\nif not os.path.exists(thisrun):\n\tos.makedirs(thisrun)\n\n# store stop time\nself.start_stop_tuples.append( (self.start_stop_tuples.pop()[0], time.time()) )\nfilename = thisrun + os.sep + str(int(time.time()))+\".suspended_crawl\"\nf = open(filename,\"w\")\npickle.dump(self, f)\nf.close()\nreturn filename", "path": "crawler.py", "repo_name": "petterw/crawler", "stars": 21, "license": "gpl-3.0", "language": "python", "size": 129}
{"docstring": "\"\"\" Start retrieval of document.\n\n\tTreats crawl-delay as a *guideline*, as tasks scheduled for a specific\n\ttime may be executed later. But on average, it will be obeyed.\n\n\tThis is a limitation of the crawler, and should be fixed.\n\n\tTo really obey crawl-delay, a safety margin should be added, and made into\n\ta task expiration time.\n\"\"\"\n\n# check if we're allowed to crawl\n", "func_signal": "def retrieve(self):\n", "code": "if self.domain.allows_crawling(self.url):\n\t# farm out crawl job\n\tself.crawl_time = self.domain.claim_next_crawl_time()\n\tself.task = fetcher.fetch_document.apply_async(args = [ self.url, self.domain.crawler.useragent_string],\n\t\t\t\t\t\t\t\t\t\t\t\t   eta = self.crawl_time)\n\nelse:\n\t# Blocked by robots.txt\n\tself.blocked = True", "path": "crawler.py", "repo_name": "petterw/crawler", "stars": 21, "license": "gpl-3.0", "language": "python", "size": 129}
{"docstring": "\"\"\"Returns a float representing the crawl delay specified for this \nuser agent, or None if the crawl delay was unspecified or not a float.\n\"\"\"\n# See is_allowed() comment about the explicit unicode conversion.\n", "func_signal": "def get_crawl_delay(self, user_agent):\n", "code": "if (PY_MAJOR_VERSION < 3) and (not isinstance(user_agent, unicode)):\n    user_agent = user_agent.decode()\n    \nfor ruleset in self.__rulesets:\n    if ruleset.does_user_agent_match(user_agent):\n        return ruleset.crawl_delay\n        \nreturn None", "path": "robotexclusionrulesparser.py", "repo_name": "petterw/crawler", "stars": 21, "license": "gpl-3.0", "language": "python", "size": 129}
{"docstring": "\"\"\" Comparison override to be able to use class sensibly in queues \"\"\"\n", "func_signal": "def __eq__(self, other):\n", "code": "if other == None:\n\treturn False\nreturn self.url == other.url", "path": "crawler.py", "repo_name": "petterw/crawler", "stars": 21, "license": "gpl-3.0", "language": "python", "size": 129}
{"docstring": "\"\"\" Consider adding urls from result to crawling queue \"\"\"\n", "func_signal": "def process_results(self):\n", "code": "for doc in self.result_queue:\n\tself.result_queue.remove(doc)\n\tif len(doc.get_contents()[2].result) > 0:\n\t\tself.new_links = True\n\t\tfor url_tuple in doc.get_contents()[2].result:\n\t\t\tif self.links.get(doc.url)==None:\n\t\t\t\tself.links[doc.url] = [url_tuple[0]]\n\t\t\telse:\n\t\t\t\tself.links[doc.url] = self.links[doc.url] + [url_tuple[0]]\n\t\t\tself.add_url(url_tuple)", "path": "crawler.py", "repo_name": "petterw/crawler", "stars": 21, "license": "gpl-3.0", "language": "python", "size": 129}
{"docstring": "# config\n", "func_signal": "def __init__(self, name, crawler):\n", "code": "self.name = name\nself.crawler = crawler\n# counters for statistics\nself.downloaded = 0\nself.downloaded_count = 0\n# robots.txt handling\nself.crawl_delay = timedelta(seconds=crawler.default_crawl_delay)\nself.last_crawl_time = datetime.now() - self.crawl_delay\nself.rp = robotexclusionrulesparser.RobotExclusionRulesParser()\nself.robots_txt_task = fetcher.FakeAsyncResult(ready=False)\nself.parsed_robots_txt = False\nself.setup_robots_txt()", "path": "crawler.py", "repo_name": "petterw/crawler", "stars": 21, "license": "gpl-3.0", "language": "python", "size": 129}
{"docstring": "\"\"\"Attempts to fetch the URL requested which should refer to a \nrobots.txt file, e.g. http://example.com/robots.txt.\n\"\"\"\n\n# ISO-8859-1 is the default encoding for text files per the specs for\n# HTTP 1.0 (RFC 1945 sec 3.6.1) and HTTP 1.1 (RFC 2616 sec 3.7.1).\n# ref: http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.7.1\n", "func_signal": "def fetch(self, url):\n", "code": "encoding = \"iso-8859-1\"\ncontent = \"\"\nexpires_header = None\ncontent_type_header = None\nself._response_code = None\nself._source_url = url\n\nif self.user_agent:\n    req = urllib_request.Request(url, None, \n                                 { 'User-Agent' : self.user_agent })\nelse:\n    req = urllib_request.Request(url)\n    \ntry:\n    f = urllib_request.urlopen(req)\n    content = f.read(MAX_FILESIZE)\n    # As of Python 2.5, f.info() looks like it returns the HTTPMessage\n    # object created during the connection. \n    expires_header = f.info().get(\"expires\")\n    content_type_header = f.info().get(\"Content-Type\")\n    # As of Python 2.4, this file-like object reports the response \n    # code, too. \n    if hasattr(f, \"code\"):\n        self._response_code = f.code\n    else:\n        self._response_code = 200\n    f.close()\nexcept urllib_error.URLError:\n    # This is a slightly convoluted way to get the error instance,\n    # but it works under Python 2 & 3. \n    error_instance = sys.exc_info()\n    if len(error_instance) > 1:\n        error_instance = error_instance[1]\n    if hasattr(error_instance, \"code\"):\n        self._response_code = error_instance.code\n        \n# MK1996 section 3.4 says, \"...robots should take note of Expires \n# header set by the origin server. If no cache-control directives \n# are present robots should default to an expiry of 7 days\".\n\n# This code is lazy and looks at the Expires header but not \n# Cache-Control directives.\nself.expiration_date = None\nif self._response_code >= 200 and self._response_code < 300:\n    # All's well.\n    if expires_header:\n        self.expiration_date = email_utils.parsedate_tz(expires_header)\n        \n        if self.expiration_date:\n            # About time zones -- the call to parsedate_tz() returns a\n            # 10-tuple with the time zone offset in the 10th element. \n            # There are 3 valid formats for HTTP dates, and one of \n            # them doesn't contain time zone information. (UTC is \n            # implied since all HTTP header dates are UTC.) When given\n            # a date that lacks time zone information, parsedate_tz() \n            # returns None in the 10th element. mktime_tz() interprets\n            # None in the 10th (time zone) element to mean that the \n            # date is *local* time, not UTC. \n            # Therefore, if the HTTP timestamp lacks time zone info \n            # and I run that timestamp through parsedate_tz() and pass\n            # it directly to mktime_tz(), I'll get back a local \n            # timestamp which isn't what I want. To fix this, I simply\n            # convert a time zone of None to zero. It's much more \n            # difficult to explain than to fix. =)\n            # ref: http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.3.1\n            if self.expiration_date[9] == None: \n                self.expiration_date = self.expiration_date[:9] + (0,)\n        \n            self.expiration_date = email_utils.mktime_tz(self.expiration_date)\n            if self.use_local_time: \n                # I have to do a little more converting to get this \n                # UTC timestamp into localtime.\n                self.expiration_date = time.mktime(time.gmtime(self.expiration_date)) \n        #else:\n            # The expires header was garbage.\n\nif not self.expiration_date: self.expiration_date = self._now() + SEVEN_DAYS\n\nif (self._response_code >= 200) and (self._response_code < 300):\n    # All's well.\n    media_type, encoding = _parse_content_type_header(content_type_header)\n    # RFC 2616 sec 3.7.1 -- \n    # When no explicit charset parameter is provided by the sender, \n    # media subtypes  of the \"text\" type are defined to have a default\n    # charset value of \"ISO-8859-1\" when received via HTTP.\n    # http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.7.1\n    if not encoding: \n        encoding = \"iso-8859-1\"\nelif self._response_code in (401, 403):\n    # 401 or 403 ==> Go away or I will taunt you a second time! \n    # (according to MK1996)\n    content = \"User-agent: *\\nDisallow: /\\n\"\nelif self._response_code == 404:\n    # No robots.txt ==> everyone's welcome\n    content = \"\"\nelse:        \n    # Uh-oh. I punt this up to the caller. \n    _raise_error(urllib_error.URLError, self._response_code)\n\nif ((PY_MAJOR_VERSION == 2) and isinstance(content, str)) or \\\n   ((PY_MAJOR_VERSION > 2)  and (not isinstance(content, str))):\n    # This ain't Unicode yet! It needs to be.\n    \n    # Unicode decoding errors are another point of failure that I punt \n    # up to the caller.\n    try:\n        content = content.decode(encoding)\n    except UnicodeError:\n        _raise_error(UnicodeError,\n        \"Robots.txt contents are not in the encoding expected (%s).\" % encoding)\n    except (LookupError, ValueError):\n        # LookupError ==> Python doesn't have a decoder for that encoding.\n        # One can also get a ValueError here if the encoding starts with \n        # a dot (ASCII 0x2e). See Python bug 1446043 for details. This \n        # bug was supposedly fixed in Python 2.5.\n        _raise_error(UnicodeError,\n                \"I don't understand the encoding \\\"%s\\\".\" % encoding)\n\n# Now that I've fetched the content and turned it into Unicode, I \n# can parse it.\nself.parse(content)", "path": "robotexclusionrulesparser.py", "repo_name": "petterw/crawler", "stars": 21, "license": "gpl-3.0", "language": "python", "size": 129}
{"docstring": "\"\"\" take our async result and parse it (blocking) \"\"\"\n", "func_signal": "def parse_robots_txt(self):\n", "code": "self.rp.parse(self.robots_txt_task.wait()[1])\nif self.rp.get_crawl_delay(self.crawler.robots_txt_name) != None:\n\tself.crawl_delay = max(timedelta(seconds = self.rp.get_crawl_delay(self.crawler.robots_txt_name)), self.crawl_delay)\nself.parsed_robots_txt = True", "path": "crawler.py", "repo_name": "petterw/crawler", "stars": 21, "license": "gpl-3.0", "language": "python", "size": 129}
{"docstring": "\"\"\" Can we crawl this url? \"\"\"\n", "func_signal": "def allows_crawling(self, url):\n", "code": "if not self.crawler.obey_robots_txt:\n\treturn True\n# if robots.txt is expired, refresh it\nif self.rp.is_expired():\n\tself.setup_robots_txt()\n# some more thought needs to go into how to avoid blocking on robots.txt\nif not self.parsed_robots_txt:\n\tself.parse_robots_txt()\ntry:\n\treturn self.rp.is_allowed(self.crawler.robots_txt_name, urlparse.urlparse(url)[2])\nexcept UnicodeDecodeError:\n\treturn False", "path": "crawler.py", "repo_name": "petterw/crawler", "stars": 21, "license": "gpl-3.0", "language": "python", "size": 129}
{"docstring": "\"\"\" Reads suspended crawl from file and returns crawler object. Restart\n\tcrawling with .crawl()\n\n\tRemember to handle file exceptions..\n\"\"\"\n", "func_signal": "def resume(suspended_crawl):\n", "code": "crawler = pickle.load(open(suspended_crawl))\n\n# store start time\ncrawler.start_stop_tuples.append( (time.time(), -1) )\nreturn crawler", "path": "crawler.py", "repo_name": "petterw/crawler", "stars": 21, "license": "gpl-3.0", "language": "python", "size": 129}
{"docstring": "\"\"\"Parses the passed string as a set of robots.txt rules.\"\"\"\n", "func_signal": "def parse(self, s):\n", "code": "self._sitemap = None\nself.__rulesets = [ ]\n\nif (PY_MAJOR_VERSION > 2) and (isinstance(s, bytes) or isinstance(s, bytearray)) or \\\n   (PY_MAJOR_VERSION == 2) and (not isinstance(s, unicode)):            \n    s = s.decode(\"iso-8859-1\")\n    \n# Normalize newlines.\ns = _end_of_line_regex.sub(\"\\n\", s)\n\nlines = s.split(\"\\n\")\n\nprevious_line_was_a_user_agent = False\ncurrent_ruleset = None\n\nfor line in lines:\n    line = line.strip()\n    \n    if line and line[0] == '#':\n        # \"Lines containing only a comment are discarded completely, \n        # and therefore do not indicate a record boundary.\" (MK1994)\n        pass\n    else:\n        # Remove comments\n        i = line.find(\"#\")\n        if i != -1: line = line[:i]\n\n        line = line.strip()\n        \n        if not line:\n            # An empty line indicates the end of a ruleset.\n            if current_ruleset and current_ruleset.is_not_empty():\n                self.__rulesets.append(current_ruleset)\n            \n            current_ruleset = None\n            previous_line_was_a_user_agent = False\n        else:\n            # Each non-empty line falls into one of six categories:\n            # 1) User-agent: blah blah blah\n            # 2) Disallow: blah blah blah\n            # 3) Allow: blah blah blah\n            # 4) Crawl-delay: blah blah blah\n            # 5) Sitemap: blah blah blah\n            # 6) Everything else\n            # 1 - 5 are interesting and I find them with the regex \n            # below. Category 6 I discard as directed by the MK1994 \n            # (\"Unrecognised headers are ignored.\")\n            # Note that 4 & 5 are specific to GYM2008 syntax, but \n            # respecting them here is not a problem. They're just \n            # additional information the the caller is free to ignore.\n            matches = _directive_regex.findall(line)\n            \n            # Categories 1 - 5 produce two matches, #6 produces none.\n            if matches:\n                field, data = matches[0]\n                field = field.lower()\n                data = _scrub_data(data)\n\n                # Matching \"useragent\" is a deviation from the \n                # MK1994/96 which permits only \"user-agent\".\n                if field in (\"useragent\", \"user-agent\"):\n                    if previous_line_was_a_user_agent:\n                        # Add this UA to the current ruleset \n                        if current_ruleset and data:\n                            current_ruleset.add_robot_name(data)\n                    else:\n                        # Save the current ruleset and start a new one.\n                        if current_ruleset and current_ruleset.is_not_empty():\n                            self.__rulesets.append(current_ruleset)\n                        #else:\n                            # (is_not_empty() == False) ==> malformed \n                            # robots.txt listed a UA line but provided\n                            # no name or didn't provide any rules \n                            # for a named UA.\n                        current_ruleset = _Ruleset()\n                        if data: \n                            current_ruleset.add_robot_name(data)\n                    \n                    previous_line_was_a_user_agent = True\n                elif field == \"allow\":\n                    previous_line_was_a_user_agent = False\n                    if current_ruleset:\n                        current_ruleset.add_allow_rule(data)\n                elif field == \"sitemap\":\n                    previous_line_was_a_user_agent = False\n                    self._sitemap = data\n                elif field == \"crawl-delay\":\n                    # Only Yahoo documents the syntax for Crawl-delay.\n                    # ref: http://help.yahoo.com/l/us/yahoo/search/webcrawler/slurp-03.html\n                    previous_line_was_a_user_agent = False\n                    if current_ruleset:\n                        try:\n                            current_ruleset.crawl_delay = float(data)\n                        except ValueError:\n                            # Invalid crawl-delay -- ignore.\n                            pass\n                else:\n                    # This is a disallow line\n                    previous_line_was_a_user_agent = False\n                    if current_ruleset:\n                        current_ruleset.add_disallow_rule(data)\n\nif current_ruleset and current_ruleset.is_not_empty():\n    self.__rulesets.append(current_ruleset)\n    \n# Now that I have all the rulesets, I want to order them in a way \n# that makes comparisons easier later. Specifically, any ruleset that \n# contains the default user agent '*' should go at the end of the list\n# so that I only apply the default as a last resort. According to \n# MK1994/96, there should only be one ruleset that specifies * as the \n# user-agent, but you know how these things go.\nnot_defaults = [r for r in self.__rulesets if not r.is_default()]\ndefaults = [r for r in self.__rulesets if r.is_default()]\n\nself.__rulesets = not_defaults + defaults", "path": "robotexclusionrulesparser.py", "repo_name": "petterw/crawler", "stars": 21, "license": "gpl-3.0", "language": "python", "size": 129}
{"docstring": "\"\"\" Download and parse robots.txt if we care about it \"\"\"\n", "func_signal": "def setup_robots_txt(self):\n", "code": "if self.crawler.obey_robots_txt:\n\tself.parsed_robots_txt = False\n\ttry:\n\t\t# This should be made async later\n\t\tself.robots_txt_task = fetcher.fetch_robots_txt.apply_async(\n\t\t\targs = ['http://'+self.name+'/robots.txt', self.crawler.useragent_string])\n\t\t#self.rp.fetch()\n\texcept:\n\t\t# if we couldn't get robots.txt, that's just too bad.. :p\n\t\tpass", "path": "crawler.py", "repo_name": "petterw/crawler", "stars": 21, "license": "gpl-3.0", "language": "python", "size": 129}
{"docstring": "\"\"\" Consider crawling some new urls from queue: \"\"\"\n\n", "func_signal": "def start_new_retrievals(self):\n", "code": "if self.document_fetchers - len(self.in_progress_queue) > 0:\n\t\n\tif self.new_links:\n\t\tself.candidate_queue.sort(key=self.rank)\n\t\tself.new_links = False\n\t\n\tfor domain in self.robots_txt_wait_queue:\n\t\tif domain.robots_txt_task.ready():\n\t\t\tself.robots_txt_wait_queue.remove(domain)\n\t\t\tdomain.parse_robots_txt()\n\t\n\tfor doc in self.candidate_queue:\t\n\t\t# if we're not allowed to crawl the site before the next crawl management pass,\n\t\t# skip it to avoid starving the crawling process with waiting workers\n\t\tif doc.domain.robots_txt_in_place():\n\t\t\tif self.within_scheduling_scope(doc):\n\t\t\t\tself.candidate_queue.remove(doc)\n\t\t\t\tdoc.retrieve()\n\t\t\t\tself.in_progress_queue.append(doc)\n\t\telse:\n\t\t\tif not doc.domain in self.robots_txt_wait_queue and self.robots_txt_fetchers - len(self.robots_txt_wait_queue) > 0:\n\t\t\t\tdoc.domain.setup_robots_txt()\n\t\t\t\tself.robots_txt_wait_queue.append(doc.domain)\n\t\tif self.document_fetchers - len(self.in_progress_queue) == 0:\n\t\t\tbreak", "path": "crawler.py", "repo_name": "petterw/crawler", "stars": 21, "license": "gpl-3.0", "language": "python", "size": 129}
{"docstring": "\"\"\" add a url to the retrieval queue, without starting to download it\n\tThis creates a Domain instance if the domain hasn't been seen before,\n\tto keep track of crawl intervals, robot exclusion etc. It also\n\tupdates some internal data so that statistics may be calculated,\n\tand so that a more accurate rank score can be given to other urls.\n\"\"\"\n#avoid parsing more than once\n", "func_signal": "def add_url(self, url_tuple):\n", "code": "parts = urlparse.urlparse(url_tuple[0])\n#if it's a url we want to visit\nif parts[0] in self.schemes and (len(self.crawl_domains) == 0 or (parts[1] in self.crawl_domains)):\n\t# extract domain\n\tdname = parts[1]\n\n\t#if we've never seen this domain\n\tif self.domains.get(dname) == None:\n\t\t# create a Domain instance\n\t\tself.domains[dname] = Domain(name = dname, crawler = self)\n\t\t# and an array to keep track of urls for this domain\n\t\tself.urls[dname] = []\n\n\t# if this url is new to us\n\tif self.results.get(url_tuple[0]) == None:\n\t\t#add to urls for domain\n\t\tself.urls[dname].append(url_tuple[0])\n\t\t# create a Document instance for url\n\t\tdocument =  Document(url_tuple[0],\n\t\t\t\t\t\t\t self.results[url_tuple[1]],\n\t\t\t\t\t\t\t self.domains[dname])\n\t\t# add url to list of candidates for retrieval\n\t\tself.candidate_queue.append(document)\n\t\t# keep track of Document\n\t\tself.results[url_tuple[0]] = document\n\telse:\n\t\t#if we have seen this url before, we just track the extra incoming link\n\t\tself.results.get(url_tuple[0]).add_referrer(self.results[url_tuple[1]])", "path": "crawler.py", "repo_name": "petterw/crawler", "stars": 21, "license": "gpl-3.0", "language": "python", "size": 129}
{"docstring": "# Data is either a path or user agent name; i.e. the data portion of a \n# robots.txt line. Scrubbing it consists of (a) removing extraneous \n# whitespace, (b) turning tabs into spaces (path and UA names should not \n# contain tabs), and (c) stripping control characters which, like tabs, \n# shouldn't be present. (See MK1996 section 3.3 \"Formal Syntax\".)\n", "func_signal": "def _scrub_data(s):\n", "code": "s = _control_characters_regex.sub(\"\", s)\ns = s.replace(\"\\t\", \" \")\nreturn s.strip()", "path": "robotexclusionrulesparser.py", "repo_name": "petterw/crawler", "stars": 21, "license": "gpl-3.0", "language": "python", "size": 129}
{"docstring": "'''Creates a new issue: add the issue to the issues file, add the issue\nnumber to the issues-open file, and increment the last issue number in\nissues-last.\n\n:param path: The path to the .gitli directory.\n:param title: The title of the issue.\n:param verbose: If True, ask the user for the issue type and milestone.\n'''\n", "func_signal": "def new_issue(path, title, verbose=False):\n", "code": "with open(join(path, LAST), 'r', encoding='utf-8') as last:\n    issue_number = int(last.read().strip()) + 1\n\nttype = ask_type(verbose)\nmilestone = ask_milestone(path, verbose)\n\nwith open(join(path, ISSUES), 'a', encoding='utf-8') as issues:\n    issues.write('{0}\\n{1}\\n{2}\\n{3}\\n'.format(issue_number, title,\n        ttype, milestone))\n\nadd_open(path, issue_number)\n\nwith open(join(path, LAST), 'w', encoding='utf-8') as last:\n    last.write('{0}'.format(issue_number))", "path": "gitli.py", "repo_name": "bartdag/gitli", "stars": 16, "license": "other", "language": "python", "size": 162}
{"docstring": "'''Removes an issue from the issues file.\n\n:param path: The path to the .gitli directory.\n:param issue_number: The number of the issue to remove.\n'''\n", "func_signal": "def remove_an_issue(path, issue_number):\n", "code": "issues = get_issues(path, [], [], [], [])\nwith open(join(path, ISSUES), 'w', encoding='utf-8') as issues_file:\n    for issue in issues:\n        if issue[0] != issue_number:\n            issues_file.write('{0}\\n{1}\\n{2}\\n{3}\\n'.format(issue[0],\n                issue[1], issue[2], issue[3]))", "path": "gitli.py", "repo_name": "bartdag/gitli", "stars": 16, "license": "other", "language": "python", "size": 162}
{"docstring": "'''Reopens an issue by adding its number back to the issues-open file.\nIf the issue is already opened, this operation does nothing.\n\n:param path: The path to the .gitli directory.\n:param issue_number: The number of the issue to reopen.\n'''\n# To make sure that we don't add the issue twice... that would be bad\n", "func_signal": "def reopen_issue(path, issue_number):\n", "code": "remove_open(path, issue_number)\nadd_open(path, issue_number)", "path": "gitli.py", "repo_name": "bartdag/gitli", "stars": 16, "license": "other", "language": "python", "size": 162}
{"docstring": "'''Prints information about an issue.\n\n:param path: The path to the .gitli directory.\n:param issue_number: The number of the issue to display.\n:param bcolor: A BColors instance to colorize the output.\n'''\n", "func_signal": "def show_issue(path, issue_number, bcolor=BColors()):\n", "code": "issue = get_issue(path, issue_number)\nif issue is not None:\n    open_issues = get_open_issues(path)\n    print_issues([issue], open_issues, bcolor)\nelse:\n    print('Issue #{0} not found'.format(issue_number))", "path": "gitli.py", "repo_name": "bartdag/gitli", "stars": 16, "license": "other", "language": "python", "size": 162}
{"docstring": "'''Return a tuple (issue_number, title, issue_type, milestone).\n\n:param path: The path to the .gitli directory.\n:param issue_number: The number of the issue to retrieve.\n:rtype: A tuple representing the issue or None if not found.\n'''\n\n", "func_signal": "def get_issue(path, issue_number):\n", "code": "with open(join(path, ISSUES), 'r', encoding='utf-8') as issues_file:\n    lines = issues_file.readlines()\nsize = len(lines)\nindex = 0\nissue = None\nwhile index < size:\n    issue = (\n        lines[index].strip(),\n        lines[index + 1].strip(),\n        int(lines[index + 2].strip()),\n        lines[index + 3].strip())\n    if issue[0] == issue_number:\n        break\n    else:\n        issue = None\n        index += 4\n\nreturn issue", "path": "gitli.py", "repo_name": "bartdag/gitli", "stars": 16, "license": "other", "language": "python", "size": 162}
{"docstring": "'''Asks the user what milestone to associate the issue with.\n\n:param path: The path to the .gitli directory.\n:param verbose: If False, the default milestone is returned without asking\nthe user.\n:param default: The default milestone. If None, the current milestone is\nprovided as the default.\n:rtype: The milestone selected by th euser or the default one if verbose\nis False.\n'''\n", "func_signal": "def ask_milestone(path, verbose=False, default=None):\n", "code": "if default is None:\n    with open(join(path, CURRENT), 'r', encoding='utf-8') as current:\n        current_value = current.read()\nelse:\n    current_value = default\n\nif not verbose:\n    return current_value\n\nmilestone = rinput('Milestone: [{0}]: '.format(current_value)).strip()\nif not milestone:\n    milestone = current_value\n\nreturn milestone", "path": "gitli.py", "repo_name": "bartdag/gitli", "stars": 16, "license": "other", "language": "python", "size": 162}
{"docstring": "'''Returns a list of issues that match the filters.\n[(issue_number, title, issue_type, milestone)]\n\n:param path: The path to the .gitli directory.\n:param filters: A list of filters, [str]. e.g., 'close', '0.1', 'task'.\n:param open_issues: A list of the issue numbers that are open. [str]\n:param milestones: A list of milestones, [str], that the issue must be\nassociated with. If empty, the issue milestone is not checked.\n:param itypes: A list of issue types, [str], used to filter the issue. If\nthe list is empty, the issue type is not checked.\n:rtype: A list of issue tuples matching the filters.\n'''\n", "func_signal": "def get_issues(path, filters, open_issues, milestones, itypes):\n", "code": "with open(join(path, ISSUES), 'r', encoding='utf-8') as issues_file:\n    lines = issues_file.readlines()\nissues = []\nsize = len(lines)\nindex = 0\nwhile index < size:\n    issue = (\n        lines[index].strip(),\n        lines[index + 1].strip(),\n        int(lines[index + 2].strip()),\n        lines[index + 3].strip())\n    if filter_issues(issue, filters, open_issues, milestones, itypes):\n        issues.append(issue)\n    index += 4\n\nreturn issues", "path": "gitli.py", "repo_name": "bartdag/gitli", "stars": 16, "license": "other", "language": "python", "size": 162}
{"docstring": "'''Asks the user what type of issue to create.\n\n:verbose: If False, the default type is returned without asking the user.\n:default: The default issue type.\n:rtype: The issue type selected by the user or the default one if verbose\nis False.\n'''\n", "func_signal": "def ask_type(verbose=False, default=1):\n", "code": "if not verbose:\n    return 1\n\nttype = rinput('Issue type: 1-Task, 2-Bug, 3-Enhancement [{0}]: '\\\n        .format(default))\nttype = ttype.strip().lower()\n\nif not ttype:\n    return default\nelif ttype in ('1', 'task'):\n    return 1\nelif ttype in ('2', 'bug'):\n    return 2\nelif ttype in ('3', 'enhancement'):\n    return 3\nelse:\n    return 1", "path": "gitli.py", "repo_name": "bartdag/gitli", "stars": 16, "license": "other", "language": "python", "size": 162}
{"docstring": "'''\n:rtype: True if gitli.color is on in the git config.\n'''\n", "func_signal": "def is_colored_output():\n", "code": "try:\n    value = check_output(COLOR).strip().lower().decode('utf-8')\n    return value in ('auto', 'on', 'true')\nexcept Exception:\n    return False", "path": "gitli.py", "repo_name": "bartdag/gitli", "stars": 16, "license": "other", "language": "python", "size": 162}
{"docstring": "'''Updates the milestone of all open issues.\n\n:param path: The path to the .gitli directory.\n:param milestone: The new milestone\n'''\n", "func_signal": "def move_issues(path, milestone):\n", "code": "open_issues = get_open_issues(path)\nissues = get_issues(path, [], [], [], [])\n\nwith open(join(path, ISSUES), 'w', encoding='utf-8') as issues_file:\n    for (number, title, itype, imilestone) in issues:\n        if number in open_issues:\n            imilestone = milestone\n        issues_file.write('{0}\\n{1}\\n{2}\\n{3}\\n'.format(\n                number,\n                title,\n                itype,\n                imilestone\n                ))", "path": "gitli.py", "repo_name": "bartdag/gitli", "stars": 16, "license": "other", "language": "python", "size": 162}
{"docstring": "'''Changes the current milestone by overwriting the issues-current file.\n\n:param path: The path of the .gitli directory.\n:param milestone: The new current milestone, e.g., '0.1'\n:param up: If True, all open issues will be moved to the new current\nmilestone.\n'''\n", "func_signal": "def edit_milestone(path, milestone, up):\n", "code": "if milestone:\n    current_path = join(path, CURRENT)\n    with open(current_path, 'w', encoding='utf-8') as current:\n        current.write(milestone)\nif up:\n    move_issues(path, milestone)", "path": "gitli.py", "repo_name": "bartdag/gitli", "stars": 16, "license": "other", "language": "python", "size": 162}
{"docstring": "'''Indicate whether or not an issue should be displayed (True) or not\n(False).\n:param issue: The issue tuple to filter.\n(issue_number, title, issue_type, milestone)\n:param filters: A list of filters, [str]. e.g., 'close', '0.1', 'task'.\n:param open_issues: A list of the issue numbers that are open. [str]\n:param milestones: A list of milestones, [str], that the issue must be\nassociated with. If empty, the issue milestone is not checked.\n:param itypes: A list of issue types, [str], used to filter the issue. If\nthe list is empty, the issue type is not checked.\n:rtype: True if the issue passes all filters and can be displayed. False\notherwise.\n'''\n", "func_signal": "def filter_issues(issue, filters, open_issues, milestones, itypes):\n", "code": "if 'open' in filters and issue[0] not in open_issues:\n    return False\n\nif 'close' in filters and issue[0] in open_issues:\n    return False\n\nif len(milestones) > 0 and issue[3] not in milestones:\n    return False\n\nif len(itypes) > 0 and ITYPES[issue[2] - 1].lower() not in itypes:\n    return False\n\nreturn True", "path": "gitli.py", "repo_name": "bartdag/gitli", "stars": 16, "license": "other", "language": "python", "size": 162}
{"docstring": "'''\n:rtype: The default list filter specified in the git config or\nDEFAULT_LIST_FILTER.\n'''\n", "func_signal": "def get_default_list_filter():\n", "code": "try:\n    value = check_output(LIST)\n    if not value:\n        return DEFAULT_LIST_FILTER\n    else:\n        return value.strip().lower().decode('utf-8')\nexcept Exception:\n    return DEFAULT_LIST_FILTER", "path": "gitli.py", "repo_name": "bartdag/gitli", "stars": 16, "license": "other", "language": "python", "size": 162}
{"docstring": "'''Prints a list of issues matching the provided filters.\n\n:param path: The path to the .gitli directory.\n:param filters: A list of filters such as ['open', '0.1', 'task']\n:param bcolor: An instance of the BColors class to colorize the output.\n'''\n", "func_signal": "def list_issues(path, filters=None, bcolor=BColors()):\n", "code": "if filters is None or len(filters) == 0:\n    filters = [get_default_list_filter()]\nelse:\n    filters = [ifilter.strip().lower() for ifilter in filters]\n\nif 'all' in filters:\n    filters = []\n\nopen_issues = get_open_issues(path)\n\nitypes = [ifilter for ifilter in filters if ifilter in\n    ('task', 'bug', 'enhancement')]\n\nmilestones = [ifilter for ifilter in filters if ifilter not in\n    ('open', 'close', 'task', 'bug', 'enhancement')]\n\nissues = get_issues(path, filters, open_issues, milestones, itypes)\n\nprint_issues(issues, open_issues, bcolor)\n\n# Useful for testing\nreturn issues", "path": "gitli.py", "repo_name": "bartdag/gitli", "stars": 16, "license": "other", "language": "python", "size": 162}
{"docstring": "'''Enables the user to edit an issue by asking several questions (title,\nissue type, milestone).\n\n:param path: The path to the .gitli directory.\n:param issue_number: The number of the issue to edit.\n'''\n", "func_signal": "def edit_issue(path, issue_number):\n", "code": "issues = get_issues(path, [], [], [], [])\nissue = None\nindex = -1\nfor i, temp_issue in enumerate(issues):\n    if temp_issue[0] == issue_number:\n        issue = temp_issue\n        index = i\n\nif issue is None:\n    print('Issue #{0} unknown'.format(issue_number))\n    return\nelse:\n    title = rinput('Enter a new title (enter nothing to keep the same): ')\n    if not title.strip():\n        title = issue[1]\n    ttype = ask_type(True, issue[2])\n    milestone = ask_milestone(path, True, issue[3])\n    new_issue = (issue_number, title, ttype, milestone)\n\n    with open(join(path, ISSUES), 'w', encoding='utf-8') as issues_file:\n        for i, temp_issue in enumerate(issues):\n            if i != index:\n                issues_file.write('{0}\\n{1}\\n{2}\\n{3}\\n'.format(\n                    temp_issue[0],\n                    temp_issue[1],\n                    temp_issue[2],\n                    temp_issue[3]))\n            else:\n                issues_file.write('{0}\\n{1}\\n{2}\\n{3}\\n'.format(\n                    new_issue[0],\n                    new_issue[1],\n                    new_issue[2],\n                    new_issue[3]))", "path": "gitli.py", "repo_name": "bartdag/gitli", "stars": 16, "license": "other", "language": "python", "size": 162}
{"docstring": "'''Remove an issue number from the issues-open file.\n\n:param path: The path to the .gitli directory.\n:param issue_number: The issue to close.\n'''\n", "func_signal": "def remove_open(path, issue_number):\n", "code": "with open(join(path, OPEN), 'r', encoding='utf-8') as iopen:\n    issues = iopen.read().split(OSEPARATOR)\n\nnew_issues = OSEPARATOR.join((issue for issue in issues if issue !=\n    issue_number))\n\nwith open(join(path, OPEN), 'w', encoding='utf-8') as iopen:\n    iopen.write(new_issues)", "path": "gitli.py", "repo_name": "bartdag/gitli", "stars": 16, "license": "other", "language": "python", "size": 162}
{"docstring": "'''Removes an issue by removing its number for the issues-open file and all\nits information from the issues file.\n\n:param path: The path of the .gitli directory.\n:param issue_number: The number of the issue to remove.\n'''\n", "func_signal": "def remove_issue(path, issue_number):\n", "code": "remove_open(path, issue_number)\nremove_an_issue(path, issue_number)", "path": "gitli.py", "repo_name": "bartdag/gitli", "stars": 16, "license": "other", "language": "python", "size": 162}
{"docstring": "'''Prints the current milestone.\n\n:param path: The path of the .gitli directory.\n'''\n", "func_signal": "def show_milestone(path):\n", "code": "current_path = join(path, CURRENT)\nwith open(current_path, 'r', encoding='utf-8') as current:\n    milestone = current.read().strip()\n\nprint('The current milestone is {0}'.format(milestone))", "path": "gitli.py", "repo_name": "bartdag/gitli", "stars": 16, "license": "other", "language": "python", "size": 162}
{"docstring": "'''\n:param path: The path to the .gitli directory.\n:rtype: A list of issue numbers that are open.\n'''\n", "func_signal": "def get_open_issues(path):\n", "code": "with open(join(path, OPEN), 'r', encoding='utf-8') as iopen:\n    issues = iopen.read().split(OSEPARATOR)\n\nreturn issues", "path": "gitli.py", "repo_name": "bartdag/gitli", "stars": 16, "license": "other", "language": "python", "size": 162}
{"docstring": "'''Prints the issues on stdout.\n[(issue_number, title, issue_type, milestone)]\n\n:param issues: The list of tuples representing the issues to print.\n:param open_issues: The list of the issue numbers that are open.\n:param bcolor: An instance of the BColors class used to colorize the\noutput.\n'''\n", "func_signal": "def print_issues(issues, open_issues, bcolor):\n", "code": "for (number, title, type_id, milestone) in issues:\n    if number in open_issues:\n        open_text = 'open'\n        color = bcolor.YELLOW\n    else:\n        open_text = 'closed'\n        color = bcolor.GREEN\n\n    milestone_text = '[' + milestone + ']'\n    type_text = '[' + ITYPES[type_id - 1] + ']'\n\n    print('{5}#{0:<4}{9} {6}{1:<48}{9} {7}{2:<6} {3:<7}{9} - {8}{4}{9}'\n        .format(number, title, type_text, milestone_text, open_text,\n        bcolor.CYAN, bcolor.WHITE, bcolor.BLUE, color, bcolor.ENDC))", "path": "gitli.py", "repo_name": "bartdag/gitli", "stars": 16, "license": "other", "language": "python", "size": 162}
{"docstring": "\"\"\"Constructor.\n\nArgs:\n  host: The hostname the connection was made to.\n  cert: The SSL certificate (as a dictionary) the host returned.\n\"\"\"\n", "func_signal": "def __init__(self, host, cert, reason):\n", "code": "httplib.HTTPException.__init__(self)\nself.host = host\nself.cert = cert\nself.reason = reason", "path": "lib\\google_appengine\\lib\\fancy_urllib\\fancy_urllib\\__init__.py", "repo_name": "andrewxhill/MOL", "stars": 19, "license": "None", "language": "python", "size": 91871}
{"docstring": "# TODO(frew): When we drop support for <2.6 (in the far distant future),\n# change this to socket.create_connection.\n", "func_signal": "def connect(self):\n", "code": "self.sock = _create_connection((self.host, self.port))\n\nif self._tunnel_host:\n  self._tunnel()\n\n# ssl and FakeSocket got deprecated. Try for the new hotness of wrap_ssl,\n# with fallback. Note: Since can_validate_certs() just checks for the\n# ssl module, it's equivalent to attempting to import ssl from\n# the function, but doesn't require a dynamic import, which doesn't\n# play nicely with dev_appserver.\nif can_validate_certs():\n  self.sock = ssl.wrap_socket(self.sock,\n                              keyfile=self.key_file,\n                              certfile=self.cert_file,\n                              ca_certs=self.ca_certs,\n                              cert_reqs=self.cert_reqs)\n\n  if self.cert_reqs & ssl.CERT_REQUIRED:\n    cert = self.sock.getpeercert()\n    hostname = self.host.split(':', 0)[0]\n    if not self._validate_certificate_hostname(cert, hostname):\n      raise InvalidCertificateException(hostname, cert,\n                                        'hostname mismatch')\nelse:\n  ssl_socket = socket.ssl(self.sock,\n                          keyfile=self.key_file,\n                          certfile=self.cert_file)\n  self.sock = httplib.FakeSocket(self.sock, ssl_socket)", "path": "lib\\google_appengine\\lib\\fancy_urllib\\fancy_urllib\\__init__.py", "repo_name": "andrewxhill/MOL", "stars": 19, "license": "None", "language": "python", "size": 91871}
{"docstring": "\"\"\"Implementation of MailServer::SendToAdmins().\n\nLogs email message.  Contents of attachments are not shown, only\ntheir sizes.\n\nGiven the difficulty of determining who the actual sender\nis, Sendmail and SMTP are disabled for this action.\n\nArgs:\n  request: The message to send, a SendMailRequest.\n  response: The send response, a SendMailResponse.\n  log: Log function to send log information.  Used for dependency\n    injection.\n\"\"\"\n", "func_signal": "def _SendToAdmins(self, request, response, log=logging.info):\n", "code": "self._GenerateLog('SendToAdmins', request, log)\n\nif self._smtp_host and self._enable_sendmail:\n  log('Both SMTP and sendmail are enabled.  Ignoring sendmail.')", "path": "lib\\google_appengine\\google\\appengine\\api\\mail_stub.py", "repo_name": "andrewxhill/MOL", "stars": 19, "license": "None", "language": "python", "size": 91871}
{"docstring": "\"\"\"Validates a subnet.\"\"\"\n", "func_signal": "def Validate(self, value, unused_key=None):\n", "code": "if value is None:\n  raise validation.MissingAttribute('subnet must be specified')\nif not isinstance(value, basestring):\n  raise validation.ValidationError('subnet must be a string, not \\'%r\\'' %\n                                   type(value))\ntry:\n  ipaddr.IPNetwork(value)\nexcept ValueError:\n  raise validation.ValidationError('%s is not a valid IPv4 or IPv6 subnet' %\n                                   value)\n\n\nparts = value.split('/')\nif len(parts) == 2 and not re.match('^[0-9]+$', parts[1]):\n  raise validation.ValidationError('Prefix length of subnet %s must be an '\n                                   'integer (quad-dotted masks are not '\n                                   'supported)' % value)\n\nreturn value", "path": "lib\\google_appengine\\google\\appengine\\api\\dosinfo.py", "repo_name": "andrewxhill/MOL", "stars": 19, "license": "None", "language": "python", "size": 91871}
{"docstring": "\"\"\"Trivial implementation of URLFetchService::Fetch().\n\nArgs:\n  request: the fetch to perform, a URLFetchRequest\n  response: the fetch response, a URLFetchResponse\n\"\"\"\n\n\n", "func_signal": "def _Dynamic_Fetch(self, request, response):\n", "code": "if len(request.url()) >= _MAX_URL_LENGTH:\n  logging.error('URL is too long: %s...' % request.url()[:50])\n  raise apiproxy_errors.ApplicationError(\n      urlfetch_service_pb.URLFetchServiceError.INVALID_URL)\n\n(protocol, host, path, query, fragment) = urlparse.urlsplit(request.url())\n\npayload = None\nif request.method() == urlfetch_service_pb.URLFetchRequest.GET:\n  method = 'GET'\nelif request.method() == urlfetch_service_pb.URLFetchRequest.POST:\n  method = 'POST'\n  payload = request.payload()\nelif request.method() == urlfetch_service_pb.URLFetchRequest.HEAD:\n  method = 'HEAD'\nelif request.method() == urlfetch_service_pb.URLFetchRequest.PUT:\n  method = 'PUT'\n  payload = request.payload()\nelif request.method() == urlfetch_service_pb.URLFetchRequest.DELETE:\n  method = 'DELETE'\nelse:\n  logging.error('Invalid method: %s', request.method())\n  raise apiproxy_errors.ApplicationError(\n    urlfetch_service_pb.URLFetchServiceError.UNSPECIFIED_ERROR)\n\nif not (protocol == 'http' or protocol == 'https'):\n  logging.error('Invalid protocol: %s', protocol)\n  raise apiproxy_errors.ApplicationError(\n    urlfetch_service_pb.URLFetchServiceError.INVALID_URL)\n\nif not host:\n  logging.error('Missing host.')\n\n\n  raise apiproxy_errors.ApplicationError(\n      urlfetch_service_pb.URLFetchServiceError.FETCH_ERROR)\n\nsanitized_headers = self._SanitizeHttpHeaders(_UNTRUSTED_REQUEST_HEADERS,\n                                              request.header_list())\nrequest.clear_header()\nrequest.header_list().extend(sanitized_headers)\ndeadline = _API_CALL_DEADLINE\nif request.has_deadline():\n  deadline = request.deadline()\nvalidate_certificate = _API_CALL_VALIDATE_CERTIFICATE_DEFAULT\nif request.has_mustvalidateservercertificate():\n  validate_certificate = request.mustvalidateservercertificate()\n\nself._RetrieveURL(request.url(), payload, method,\n                  request.header_list(), request, response,\n                  follow_redirects=request.followredirects(),\n                  deadline=deadline,\n                  validate_certificate=validate_certificate)", "path": "lib\\google_appengine\\google\\appengine\\api\\urlfetch_stub.py", "repo_name": "andrewxhill/MOL", "stars": 19, "license": "None", "language": "python", "size": 91871}
{"docstring": "\"\"\"Validates that a given hostname is valid for an SSL certificate.\n\nArgs:\n  cert: A dictionary representing an SSL certificate.\n  hostname: The hostname to test.\nReturns:\n  bool: Whether or not the hostname is valid for this certificate.\n\"\"\"\n", "func_signal": "def _validate_certificate_hostname(self, cert, hostname):\n", "code": "hosts = self._get_valid_hosts_for_cert(cert)\nfor host in hosts:\n  # Convert the glob-style hostname expression (eg, '*.google.com') into a\n  # valid regular expression.\n  host_re = host.replace('.', '\\.').replace('*', '[^.]*')\n  if re.search('^%s$' % (host_re,), hostname, re.I):\n    return True\nreturn False", "path": "lib\\google_appengine\\lib\\fancy_urllib\\fancy_urllib\\__init__.py", "repo_name": "andrewxhill/MOL", "stars": 19, "license": "None", "language": "python", "size": 91871}
{"docstring": "\"\"\"\n:param index: the :class:`whoosh.index.Index` to write to.\n:param delay: the delay (in seconds) between attempts to instantiate\n    the actual writer.\n:param writerargs: an optional dictionary specifying keyword arguments\n    to to be passed to the index's ``writer()`` method.\n\"\"\"\n\n", "func_signal": "def __init__(self, index, delay=0.25, writerargs=None):\n", "code": "threading.Thread.__init__(self)\nself.running = False\nself.index = index\nself.writerargs = writerargs or {}\nself.delay = delay\nself.events = []\ntry:\n    self.writer = self.index.writer(**self.writerargs)\nexcept LockError:\n    self.writer = None", "path": "lib\\google_appengine\\lib\\whoosh\\whoosh\\writing.py", "repo_name": "andrewxhill/MOL", "stars": 19, "license": "None", "language": "python", "size": 91871}
{"docstring": "\"\"\"Send MIME message via SMTP.\n\nConnects to SMTP server and sends MIME message.  If user is supplied\nwill try to login to that server to send as authenticated.  Does not\ncurrently support encryption.\n\nArgs:\n  mime_message: MimeMessage to send.  Create using ToMIMEMessage.\n  smtp_lib: Class of SMTP library.  Used for dependency injection.\n\"\"\"\n\n", "func_signal": "def _SendSMTP(self, mime_message, smtp_lib=smtplib.SMTP):\n", "code": "smtp = smtp_lib()\ntry:\n  smtp.connect(self._smtp_host, self._smtp_port)\n  if self._smtp_user:\n    smtp.login(self._smtp_user, self._smtp_password)\n\n\n  tos = [mime_message[to] for to in ['To', 'Cc', 'Bcc'] if mime_message[to]]\n  smtp.sendmail(mime_message['From'], tos, mime_message.as_string())\nfinally:\n  smtp.quit()", "path": "lib\\google_appengine\\google\\appengine\\api\\mail_stub.py", "repo_name": "andrewxhill/MOL", "stars": 19, "license": "None", "language": "python", "size": 91871}
{"docstring": "\"\"\"Cleans \"unsafe\" headers from the HTTP request/response.\n\nArgs:\n  untrusted_headers: set of untrusted headers names\n  headers: list of string pairs, first is header name and the second is header's value\n\"\"\"\n", "func_signal": "def _SanitizeHttpHeaders(self, untrusted_headers, headers):\n", "code": "prohibited_headers = [h.key() for h in headers\n                      if h.key().lower() in untrusted_headers]\nif prohibited_headers:\n  logging.warn('Stripped prohibited headers from URLFetch request: %s',\n               prohibited_headers)\nreturn (h for h in headers if h.key().lower() not in untrusted_headers)", "path": "lib\\google_appengine\\google\\appengine\\api\\urlfetch_stub.py", "repo_name": "andrewxhill/MOL", "stars": 19, "license": "None", "language": "python", "size": 91871}
{"docstring": "\"\"\"Generate a list of log messages representing sent mail.\n\nArgs:\n  message: Message to write to log.\n  log: Log function of type string -> None\n\"\"\"\n", "func_signal": "def _GenerateLog(self, method, message, log):\n", "code": "log('MailService.%s' % method)\nlog('  From: %s' % message.sender())\n\n\nfor address in message.to_list():\n  log('  To: %s' % address)\nfor address in message.cc_list():\n  log('  Cc: %s' % address)\nfor address in message.bcc_list():\n  log('  Bcc: %s' % address)\n\nif message.replyto():\n  log('  Reply-to: %s' % message.replyto())\n\n\nlog('  Subject: %s' % message.subject())\n\n\nif message.has_textbody():\n  log('  Body:')\n  log('    Content-type: text/plain')\n  log('    Data length: %d' % len(message.textbody()))\n  if self._show_mail_body:\n    log('-----\\n' + message.textbody() + '\\n-----')\n\n\nif message.has_htmlbody():\n  log('  Body:')\n  log('    Content-type: text/html')\n  log('    Data length: %d' % len(message.htmlbody()))\n  if self._show_mail_body:\n    log('-----\\n' + message.htmlbody() + '\\n-----')\n\n\nfor attachment in message.attachment_list():\n  log('  Attachment:')\n  log('    File name: %s' % attachment.filename())\n  log('    Data length: %s' % len(attachment.data()))", "path": "lib\\google_appengine\\google\\appengine\\api\\mail_stub.py", "repo_name": "andrewxhill/MOL", "stars": 19, "license": "None", "language": "python", "size": 91871}
{"docstring": "\"\"\"Deletes any documents containing \"term\" in the \"fieldname\" field.\nThis is useful when you have an indexed field containing a unique ID\n(such as \"pathname\") for each document.\n\n:returns: the number of documents deleted.\n\"\"\"\n\n", "func_signal": "def delete_by_term(self, fieldname, text, searcher=None):\n", "code": "from whoosh.query import Term\n\nq = Term(fieldname, text)\nreturn self.delete_by_query(q, searcher=searcher)", "path": "lib\\google_appengine\\lib\\whoosh\\whoosh\\writing.py", "repo_name": "andrewxhill/MOL", "stars": 19, "license": "None", "language": "python", "size": 91871}
{"docstring": "\"\"\"The keyword arguments map field names to the values to index/store.\n\nNote that this method will only replace a *committed* document;\ncurrently it cannot replace documents you've added to the IndexWriter\nbut haven't yet committed. For example, if you do this:\n\n>>> writer.update_document(unique_id=u\"1\", content=u\"Replace me\")\n>>> writer.update_document(unique_id=u\"1\", content=u\"Replacement\")\n\n...this will add two documents with the same value of ``unique_id``,\ninstead of the second document replacing the first.\n\nFor fields that are both indexed and stored, you can specify an\nalternate value to store using a keyword argument in the form\n\"_stored_<fieldname>\". For example, if you have a field named \"title\"\nand you want to index the text \"a b c\" but store the text \"e f g\", use\nkeyword arguments like this::\n\n    writer.update_document(title=u\"a b c\", _stored_title=u\"e f g\")\n\"\"\"\n\n# Delete the set of documents matching the unique terms\n", "func_signal": "def update_document(self, **fields):\n", "code": "unique_fields = self._unique_fields(fields)\nwith self.searcher() as s:\n    for docnum in s._find_unique([(name, fields[name])\n                                  for name in unique_fields]):\n        self.delete_document(docnum)\n\n# Add the given fields\nself.add_document(**fields)", "path": "lib\\google_appengine\\lib\\whoosh\\whoosh\\writing.py", "repo_name": "andrewxhill/MOL", "stars": 19, "license": "None", "language": "python", "size": 91871}
{"docstring": "# This block is copied wholesale from Python2.6 urllib2.\n# It is idempotent, so the superclass method call executes as normal\n# if invoked.\n", "func_signal": "def proxy_open(self, req, proxy, type):\n", "code": "orig_type = req.get_type()\nproxy_type, user, password, hostport = self._parse_proxy(proxy)\nif proxy_type is None:\n  proxy_type = orig_type\nif user and password:\n  user_pass = \"%s:%s\" % (urllib2.unquote(user), urllib2.unquote(password))\n  creds = base64.b64encode(user_pass).strip()\n  # Later calls overwrite earlier calls for the same header\n  req.add_header(\"Proxy-authorization\", \"Basic \" + creds)\nhostport = urllib2.unquote(hostport)\nreq.set_proxy(hostport, proxy_type)\n# This condition is the change\nif orig_type == \"https\":\n  return None\n\nreturn urllib2.ProxyHandler.proxy_open(self, req, proxy, type)", "path": "lib\\google_appengine\\lib\\fancy_urllib\\fancy_urllib\\__init__.py", "repo_name": "andrewxhill/MOL", "stars": 19, "license": "None", "language": "python", "size": 91871}
{"docstring": "\"\"\"Return (scheme, user, password, host/port) given a URL or an authority.\n\nIf a URL is supplied, it must have an authority (host:port) component.\nAccording to RFC 3986, having an authority component means the URL must\nhave two slashes after the scheme:\n\n>>> _parse_proxy('file:/ftp.example.com/')\nTraceback (most recent call last):\nValueError: proxy URL with no authority: 'file:/ftp.example.com/'\n\nThe first three items of the returned tuple may be None.\n\nExamples of authority parsing:\n\n>>> _parse_proxy('proxy.example.com')\n(None, None, None, 'proxy.example.com')\n>>> _parse_proxy('proxy.example.com:3128')\n(None, None, None, 'proxy.example.com:3128')\n\nThe authority component may optionally include userinfo (assumed to be\nusername:password):\n\n>>> _parse_proxy('joe:password@proxy.example.com')\n(None, 'joe', 'password', 'proxy.example.com')\n>>> _parse_proxy('joe:password@proxy.example.com:3128')\n(None, 'joe', 'password', 'proxy.example.com:3128')\n\nSame examples, but with URLs instead:\n\n>>> _parse_proxy('http://proxy.example.com/')\n('http', None, None, 'proxy.example.com')\n>>> _parse_proxy('http://proxy.example.com:3128/')\n('http', None, None, 'proxy.example.com:3128')\n>>> _parse_proxy('http://joe:password@proxy.example.com/')\n('http', 'joe', 'password', 'proxy.example.com')\n>>> _parse_proxy('http://joe:password@proxy.example.com:3128')\n('http', 'joe', 'password', 'proxy.example.com:3128')\n\nEverything after the authority is ignored:\n\n>>> _parse_proxy('ftp://joe:password@proxy.example.com/rubbish:3128')\n('ftp', 'joe', 'password', 'proxy.example.com')\n\nTest for no trailing '/' case:\n\n>>> _parse_proxy('http://joe:password@proxy.example.com')\n('http', 'joe', 'password', 'proxy.example.com')\n\n\"\"\"\n", "func_signal": "def _parse_proxy(self, proxy):\n", "code": "scheme, r_scheme = splittype(proxy)\nif not r_scheme.startswith(\"/\"):\n  # authority\n  scheme = None\n  authority = proxy\nelse:\n  # URL\n  if not r_scheme.startswith(\"//\"):\n    raise ValueError(\"proxy URL with no authority: %r\" % proxy)\n  # We have an authority, so for RFC 3986-compliant URLs (by ss 3.\n  # and 3.3.), path is empty or starts with '/'\n  end = r_scheme.find(\"/\", 2)\n  if end == -1:\n    end = None\n  authority = r_scheme[2:end]\nuserinfo, hostport = splituser(authority)\nif userinfo is not None:\n  user, password = splitpasswd(userinfo)\nelse:\n  user = password = None\nreturn scheme, user, password, hostport", "path": "lib\\google_appengine\\lib\\fancy_urllib\\fancy_urllib\\__init__.py", "repo_name": "andrewxhill/MOL", "stars": 19, "license": "None", "language": "python", "size": 91871}
{"docstring": "# Check which of the supplied fields are unique\n", "func_signal": "def _unique_fields(self, fields):\n", "code": "unique_fields = [name for name, field in self.schema.items()\n                 if name in fields and field.unique]\nif not unique_fields:\n    raise IndexingError(\"None of the fields in %r\"\n                        \" are unique\" % fields.keys())\nreturn unique_fields", "path": "lib\\google_appengine\\lib\\whoosh\\whoosh\\writing.py", "repo_name": "andrewxhill/MOL", "stars": 19, "license": "None", "language": "python", "size": 91871}
{"docstring": "\"\"\"Load a dos.yaml file or string and return a DosInfoExternal object.\n\nArgs:\n  dos_info: The contents of a dos.yaml file as a string, or an open file\n    object.\n\nReturns:\n  A DosInfoExternal instance which represents the contents of the parsed yaml\n  file.\n\nRaises:\n  MalformedDosConfiguration: The yaml file contains multiple blacklist\n    sections.\n  yaml_errors.EventError: An error occured while parsing the yaml file.\n\"\"\"\n", "func_signal": "def LoadSingleDos(dos_info):\n", "code": "builder = yaml_object.ObjectBuilder(DosInfoExternal)\nhandler = yaml_builder.BuilderHandler(builder)\nlistener = yaml_listener.EventListener(handler)\nlistener.Parse(dos_info)\n\nparsed_yaml = handler.GetResults()\nif not parsed_yaml:\n  return DosInfoExternal()\nif len(parsed_yaml) > 1:\n  raise MalformedDosConfiguration('Multiple blacklist: sections '\n                                  'in configuration.')\nreturn parsed_yaml[0]", "path": "lib\\google_appengine\\google\\appengine\\api\\dosinfo.py", "repo_name": "andrewxhill/MOL", "stars": 19, "license": "None", "language": "python", "size": 91871}
{"docstring": "\"\"\"Update CRC-32C checksum with data.\n\nArgs:\n  crc: 32-bit checksum to update as long.\n  data: byte array, string or iterable over bytes.\n\nReturns:\n  32-bit updated CRC-32C as long.\n\"\"\"\n\n", "func_signal": "def crc_update(crc, data):\n", "code": "if type(data) != array.array or data.itemsize != 1:\n  buf = array.array(\"B\", data)\nelse:\n  buf = data\n\ncrc = crc ^ _MASK\nfor b in buf:\n  table_index = (crc ^ b) & 0xff\n  crc = (CRC_TABLE[table_index] ^ (crc >> 8)) & _MASK\nreturn crc ^ _MASK", "path": "lib\\google_appengine\\google\\appengine\\api\\files\\crc32c.py", "repo_name": "andrewxhill/MOL", "stars": 19, "license": "None", "language": "python", "size": 91871}
{"docstring": "\"\"\"Returns a list of valid host globs for an SSL certificate.\n\nArgs:\n  cert: A dictionary representing an SSL certificate.\nReturns:\n  list: A list of valid host globs.\n\"\"\"\n", "func_signal": "def _get_valid_hosts_for_cert(self, cert):\n", "code": "if 'subjectAltName' in cert:\n  return [x[1] for x in cert['subjectAltName'] if x[0].lower() == 'dns']\nelse:\n  # Return a list of commonName fields\n  return [x[0][1] for x in cert['subject']\n          if x[0][0].lower() == 'commonname']", "path": "lib\\google_appengine\\lib\\fancy_urllib\\fancy_urllib\\__init__.py", "repo_name": "andrewxhill/MOL", "stars": 19, "license": "None", "language": "python", "size": 91871}
{"docstring": "# We need to inspect the HTML generated by the fancy 500 debug view but\n# the test client ignores it, so we send it explicitly.\n", "func_signal": "def raises_template_does_not_exist(request):\n", "code": "try:\n    return render_to_response('i_dont_exist.html')\nexcept TemplateDoesNotExist:\n    return technical_500_response(request, *sys.exc_info())", "path": "lib\\google_appengine\\lib\\django_1_2\\tests\\regressiontests\\views\\views.py", "repo_name": "andrewxhill/MOL", "stars": 19, "license": "None", "language": "python", "size": 91871}
{"docstring": "\"\"\"\nCalls create_object generic view with a custom form class.\n\"\"\"\n", "func_signal": "def custom_create(request):\n", "code": "class SlugChangingArticleForm(forms.ModelForm):\n    \"\"\"Custom form class to overwrite the slug.\"\"\"\n\n    class Meta:\n        model = Article\n\n    def save(self, *args, **kwargs):\n        self.instance.slug = 'some-other-slug'\n        return super(SlugChangingArticleForm, self).save(*args, **kwargs)\n\nreturn create_object(request,\n    post_save_redirect='/views/create_update/view/article/%(slug)s/',\n    form_class=SlugChangingArticleForm)", "path": "lib\\google_appengine\\lib\\django_1_2\\tests\\regressiontests\\views\\views.py", "repo_name": "andrewxhill/MOL", "stars": 19, "license": "None", "language": "python", "size": 91871}
{"docstring": "\"\"\"Parse a primitive regexp.\"\"\"\n", "func_signal": "def parse_prim(self):\n", "code": "c = self.get()\nif c == '.':\n  re = AnyBut(\"\\n\")\nelif c == '^':\n  re = Bol\nelif c == '$':\n  re = Eol\nelif c == '(':\n  re = self.parse_alt()\n  self.expect(')')\nelif c == '[':\n  re = self.parse_charset()\n  self.expect(']')\nelse:\n  if c == '\\\\':\n    c = self.get()\n  re = Char(c)\nreturn re", "path": "Plex\\Traditional.py", "repo_name": "bluemoon/nlp", "stars": 18, "license": "None", "language": "python", "size": 192}
{"docstring": "\"\"\"Add a new state to the machine and return it.\"\"\"\n", "func_signal": "def new_state(self):\n", "code": "s = Node()\nn = self.next_state_number\nself.next_state_number = n + 1\ns.number = n\nself.states.append(s)\nreturn s", "path": "Plex\\Machines.py", "repo_name": "bluemoon/nlp", "stars": 18, "license": "None", "language": "python", "size": 192}
{"docstring": "\"\"\"\nScanner(lexicon, stream, name = '')\n\n  |lexicon| is a Plex.Lexicon instance specifying the lexical tokens\n  to be recognised.\n\n  |stream| can be a file object or anything which implements a\n  compatible read() method.\n\n  |name| is optional, and may be the name of the file being\n  scanned or any other identifying string.\n\"\"\"\n", "func_signal": "def __init__(self, lexicon, stream, name = ''):\n", "code": "self.lexicon = lexicon\nself.stream = stream\nself.name = name\nself.queue = []\nself.initial_state = None\nself.begin('')\nself.next_pos = 0\nself.cur_pos = 0\nself.cur_line_start = 0\nself.cur_char = BOL\nself.input_state = 1", "path": "Plex\\Scanners.py", "repo_name": "bluemoon/nlp", "stars": 18, "license": "None", "language": "python", "size": 192}
{"docstring": "\"\"\"Make this an accepting state with the given action. If \nthere is already an action, choose the action with highest\npriority.\"\"\"\n", "func_signal": "def set_action(self, action, priority):\n", "code": "if priority > self.action_priority:\n  self.action = action\n  self.action_priority = priority", "path": "Plex\\Machines.py", "repo_name": "bluemoon/nlp", "stars": 18, "license": "None", "language": "python", "size": 192}
{"docstring": "#print \"Destroying\", self ###\n", "func_signal": "def __del__(self):\n", "code": "for state in self.states:\n  state.destroy()", "path": "Plex\\Machines.py", "repo_name": "bluemoon/nlp", "stars": 18, "license": "None", "language": "python", "size": 192}
{"docstring": "\"\"\"Parse a set of alternative regexps.\"\"\"\n", "func_signal": "def parse_alt(self):\n", "code": "re = self.parse_seq()\nif self.c == '|':\n  re_list = [re]\n  while self.c == '|':\n    self.next()\n    re_list.append(self.parse_seq())\n  re = apply(Alt, tuple(re_list))\nreturn re", "path": "Plex\\Traditional.py", "repo_name": "bluemoon/nlp", "stars": 18, "license": "None", "language": "python", "size": 192}
{"docstring": "# Preinitialise the list of empty transitions, because\n# the nfa-to-dfa algorithm needs it\n#self.transitions = {'':[]}\n", "func_signal": "def __init__(self):\n", "code": "self.transitions = TransitionMap()\nself.action_priority = LOWEST_PRIORITY", "path": "Plex\\Machines.py", "repo_name": "bluemoon/nlp", "stars": 18, "license": "None", "language": "python", "size": 192}
{"docstring": "\"\"\"Parse a sequence of regexps.\"\"\"\n", "func_signal": "def parse_seq(self):\n", "code": "re_list = []\nwhile not self.end and not self.c in \"|)\":\n  re_list.append(self.parse_mod())\nreturn apply(Seq, tuple(re_list))", "path": "Plex\\Traditional.py", "repo_name": "bluemoon/nlp", "stars": 18, "license": "None", "language": "python", "size": 192}
{"docstring": "\"\"\"Raise exception to signal syntax error in regexp.\"\"\"\n", "func_signal": "def error(self, mess):\n", "code": "raise RegexpSyntaxError(\"Syntax error in regexp %s at position %d: %s\" % (\n  repr(self.s), self.i, mess))", "path": "Plex\\Traditional.py", "repo_name": "bluemoon/nlp", "stars": 18, "license": "None", "language": "python", "size": 192}
{"docstring": "## memory has to be a dictionary\n", "func_signal": "def do_action(self, s_tuple, memory):\n", "code": "self.resolve_words(s_tuple, memory)\n\nif s_tuple[1] == 'eq':\n    self.doActionEqual(s_tuple, memory)\nif s_tuple[1] == 'ap':\n    self.doActionAppend(s_tuple, memory)", "path": "rule_engine.py", "repo_name": "bluemoon/nlp", "stars": 18, "license": "None", "language": "python", "size": 192}
{"docstring": "\"\"\"Set the current state of the scanner to the named state.\"\"\"\n", "func_signal": "def begin(self, state_name):\n", "code": "self.initial_state = (\n  self.lexicon.get_initial_state(state_name))\nself.state_name = state_name", "path": "Plex\\Scanners.py", "repo_name": "bluemoon/nlp", "stars": 18, "license": "None", "language": "python", "size": 192}
{"docstring": "\"\"\"\nRead the next input sequence recognised by the machine\nand return (text, action). Returns ('', None) on end of\nfile.\n\"\"\"\n", "func_signal": "def scan_a_token(self):\n", "code": "self.start_pos = self.cur_pos\nself.start_line = self.cur_line\nself.start_col = self.cur_pos - self.cur_line_start", "path": "Plex\\Scanners.py", "repo_name": "bluemoon/nlp", "stars": 18, "license": "None", "language": "python", "size": 192}
{"docstring": "\"\"\"\nCalled from an action procedure, causes |value| to be returned\nas the token value from read(). If |text| is supplied, it is\nreturned in place of the scanned text.\n\nproduce() can be called more than once during a single call to an action\nprocedure, in which case the tokens are queued up and returned one\nat a time by subsequent calls to read(), until the queue is empty,\nwhereupon scanning resumes.\n\"\"\"\n", "func_signal": "def produce(self, value, text = None):\n", "code": "if text is None:\n  text = self.text\nself.queue.append((value, text))", "path": "Plex\\Scanners.py", "repo_name": "bluemoon/nlp", "stars": 18, "license": "None", "language": "python", "size": 192}
{"docstring": "\"\"\"Advance to the next char.\"\"\"\n", "func_signal": "def next(self):\n", "code": "s = self.s\ni = self.i = self.i + 1\nif i < len(s):\n  self.c = s[i]\nelse:\n  self.c = ''\n  self.end = 1", "path": "Plex\\Traditional.py", "repo_name": "bluemoon/nlp", "stars": 18, "license": "None", "language": "python", "size": 192}
{"docstring": "\"\"\"Look ahead n chars.\"\"\"\n", "func_signal": "def lookahead(self, n):\n", "code": "j = self.i + n\nif j < len(self.s):\n  return self.s[j]\nelse:\n  return ''", "path": "Plex\\Traditional.py", "repo_name": "bluemoon/nlp", "stars": 18, "license": "None", "language": "python", "size": 192}
{"docstring": "\"\"\"\nExpect to find character |c| at current position.\nRaises an exception otherwise.\n\"\"\"\n", "func_signal": "def expect(self, c):\n", "code": "if self.c == c:\n  self.next()\nelse:\n  self.error(\"Missing %s\" % repr(c))", "path": "Plex\\Traditional.py", "repo_name": "bluemoon/nlp", "stars": 18, "license": "None", "language": "python", "size": 192}
{"docstring": "\"\"\"Parse a primitive regexp followed by *, +, ? modifiers.\"\"\"\n", "func_signal": "def parse_mod(self):\n", "code": "re = self.parse_prim()\nwhile not self.end and self.c in \"*+?\":\n  if self.c == '*':\n    re = Rep(re)\n  elif self.c == '+':\n    re = Rep1(re)\n  else: # self.c == '?'\n    re = Opt(re)\n  self.next()\nreturn re", "path": "Plex\\Traditional.py", "repo_name": "bluemoon/nlp", "stars": 18, "license": "None", "language": "python", "size": 192}
{"docstring": "\"\"\"\nRun the machine until no more transitions are possible.\n\"\"\"\n", "func_signal": "def run_machine(self):\n", "code": "self.state = self.initial_state\nself.backup_state = None\nwhile self.transition():\n  pass\nreturn self.back_up()", "path": "Plex\\Scanners.py", "repo_name": "bluemoon/nlp", "stars": 18, "license": "None", "language": "python", "size": 192}
{"docstring": "\"\"\"\nRead the next lexical token from the stream and return a\ntuple (value, text), where |value| is the value associated with\nthe token as specified by the Lexicon, and |text| is the actual\nstring read from the stream. Returns (None, '') on end of file.\n\"\"\"\n", "func_signal": "def read(self):\n", "code": "queue = self.queue\nwhile not queue:\n  self.text, action = self.scan_a_token()\n  if action is None:\n    self.produce(None)\n    self.eof()\n  else:\n    value = action.perform(self, self.text)\n    if value is not None:\n      self.produce(value)\nresult = queue[0]\ndel queue[0]\nreturn result", "path": "Plex\\Scanners.py", "repo_name": "bluemoon/nlp", "stars": 18, "license": "None", "language": "python", "size": 192}
{"docstring": "#print \"Destroying\", self ###\n", "func_signal": "def destroy(self):\n", "code": "self.transitions = None\nself.action = None\nself.epsilon_closure = None", "path": "Plex\\Machines.py", "repo_name": "bluemoon/nlp", "stars": 18, "license": "None", "language": "python", "size": 192}
{"docstring": "#first with standard settings\n", "func_signal": "def test_view1_de(self):\n", "code": "translation.activate('de')\nresponse = self.client.get('/')\nself.assertRedirects(response, '/?lang=de')\nresponse = self.client.get('/?lang=de')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'de')\nself.assertEqual(response.request['QUERY_STRING'], 'lang=de')\n#exclude the url \napp_settings.URLI18N_INCLUDE_PATHS = []\nresponse = self.client.get('/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'de')\nself.assertEqual(response.request['QUERY_STRING'], '')\napp_settings.URLI18N_INCLUDE_PATHS = self.included_paths", "path": "urli18n\\tests\\__init__.py", "repo_name": "torte/django-urli18n", "stars": 24, "license": "mit", "language": "python", "size": 445}
{"docstring": "#first with standard settings\n", "func_signal": "def test_view2_en(self):\n", "code": "translation.activate('en')\nresponse = self.client.get('/home/')\nself.assertRedirects(response, '/home/?lang=en')\nresponse = self.client.get('/home/?lang=en')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'en')\nself.assertEqual(response.request['QUERY_STRING'], 'lang=en')\n#change the standard behavior\napp_settings.URLI18N_ALWAYS_SHOW_LANGUAGE = False\nresponse = self.client.get('/home/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'en')\nself.assertEqual(response.request['QUERY_STRING'], '')\n#exclude the url \napp_settings.URLI18N_ALWAYS_SHOW_LANGUAGE = True\napp_settings.URLI18N_INCLUDE_PATHS = []\nresponse = self.client.get('/home/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'en')\nself.assertEqual(response.request['QUERY_STRING'], '')\napp_settings.URLI18N_INCLUDE_PATHS = self.included_paths", "path": "urli18n\\tests\\__init__.py", "repo_name": "torte/django-urli18n", "stars": 24, "license": "mit", "language": "python", "size": 445}
{"docstring": "\"\"\"Process the request of according to following conditions:\n\n- Only processes ``GET`` request, since others will not be displayed in the address bar anyway\n- Only processes ``GET`` request for url path's which are not provided in ``URLI18N_EXCLUDE_PATHS`` setting. If a path is provided in ``URLI18N_EXCLUDE_PATHS`` setting it will simply not transform the url (though change the language)\n- If the url query string does not include the language parameter it will build a new url from given path and query string and redirect to it \n- If ``URLI18N_ALWAYS_SHOW_LANGUAGE`` setting is set to ``True`` (the default) it will always show the language in the query string, if  ``URLI18N_ALWAYS_SHOW_LANGUAGE`` setting is set to ``False`` it will show the language query string only for languages which are not the default language (set via the django setting for ``LANGUAGE_CODE``)\n- If user is not navigating on the page but coming from a source which is not the domain of this project it will change the language directly when provided in the url or if not provided use the current activated language (given by process_request of django.middleware.locale.LocaleMiddleware)\n\nArgs:\n    - ``request``: the django request object to process\n    \nReturns:\n    - Either a redirect response to the right path with leading language shortcut or the view response for the view attached to the url of the path\n\"\"\"\n", "func_signal": "def process_request(self, request):\n", "code": "path = request.path_info\nif request.method == 'GET' and utils.is_included_path(path):\n    full_path = request.get_full_path()\n    querystring_name = app_settings.URLI18N_QUERYSTRING_NAME\n    \n    path_parts, querystring_parts, language_querystring, language_querystring_position = utils.break_full_path(full_path)\n    \n    language = translation.get_language()\n    host = request.get_host()\n    redirect_to = None\n    regex_ref = re.compile('^http[s]?://%s' % host, re.UNICODE)\n    referer = request.META.get('HTTP_REFERER', None)\n    language_shortcuts = [lang[0] for lang in settings.LANGUAGES]\n    \n    if not referer or regex_ref.match(referer) is None and language_querystring:\n        #change the language according to the path if\n        #its provided in the path, else try to use the\n        #last activated language\n        language_from_querystring = language_querystring.replace('%s=' % querystring_name,'')\n        if language_from_querystring in language_shortcuts:\n            translation.activate(language_from_querystring)\n            request.LANGUAGE_CODE = translation.get_language()\n            language = translation.get_language()\n    \n    #reconstruct the language_querystring\n    language_querystring = '%s=%s' % (querystring_name, language)\n    \n    if app_settings.URLI18N_ALWAYS_SHOW_LANGUAGE is False\\\n    and settings.LANGUAGE_CODE == language:\n        #dont' rewrite urls for the default language if setting is set\n        if language_querystring_position is not None:\n            querystring_parts.pop(language_querystring_position)\n            full_path = utils.reconstruct_full_path(path_parts[0], querystring_parts)\n        if full_path != request.get_full_path():\n            return shortcuts.redirect(full_path)\n    else:\n        if language_querystring_position is not None\\\n        and language_querystring != querystring_parts[language_querystring_position]:\n            querystring_parts[language_querystring_position] = language_querystring\n            redirect_to = utils.reconstruct_full_path(path_parts[0], querystring_parts)\n        elif language_querystring_position is None:\n            querystring_parts.append(language_querystring)\n            redirect_to = utils.reconstruct_full_path(path_parts[0], querystring_parts)\n        if redirect_to is not None:\n            return shortcuts.redirect(redirect_to)", "path": "urli18n\\middleware.py", "repo_name": "torte/django-urli18n", "stars": 24, "license": "mit", "language": "python", "size": 445}
{"docstring": "\"\"\"Processes every ``process_view`` method from\nall middleware's in ``settings.MIDDLEWARE_CLASSES``.\nSince ``process_view`` calls are never made when calling \na view (and returning its response) from a middleware class \n``process_request`` method this helper will do the job instead.\n\nArgs:\n    - ``middleware_object``: the current middleware object\n    - ``request``: the current django request object, this will be passed to the ``process_view`` calls\n    - ``view_func``: the view function which will be passed to the ``process_view`` calls\n    - ``view_args``: the view arguments which will be passed to the ``process_view`` calls\n    - ``view_kwargs``: the view keyword arguments which will be passed to the ``process_view`` calls\n   \n   Returns:\n    - if a ``process_view`` call returns an HttpResponse it will return this response, else None\n\"\"\"\n", "func_signal": "def process_missing_views(middleware_object, request, view_func, view_args, view_kwargs):\n", "code": "middlware_instances = _init_middleware_classes(settings.MIDDLEWARE_CLASSES)\nfor mw_instance in middlware_instances:\n    if hasattr(mw_instance, 'process_view'):\n        process_result = mw_instance.process_view(request, view_func, view_args, view_kwargs)\n        if process_result is not None:\n            return process_result\nreturn None", "path": "urli18n\\utils.py", "repo_name": "torte/django-urli18n", "stars": 24, "license": "mit", "language": "python", "size": 445}
{"docstring": "#first with standard settings\n", "func_signal": "def test_view1_zhcn(self):\n", "code": "translation.activate('zh-cn')\nresponse = self.client.get('/')\nself.assertRedirects(response, '/?lang=zh-cn')\nresponse = self.client.get('/?lang=zh-cn')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'zh-cn')\nself.assertEqual(response.request['QUERY_STRING'], 'lang=zh-cn')\n#exclude the url \napp_settings.URLI18N_INCLUDE_PATHS = []\nresponse = self.client.get('/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'zh-cn')\nself.assertEqual(response.request['QUERY_STRING'], '')\napp_settings.URLI18N_INCLUDE_PATHS = self.included_paths", "path": "urli18n\\tests\\__init__.py", "repo_name": "torte/django-urli18n", "stars": 24, "license": "mit", "language": "python", "size": 445}
{"docstring": "\"\"\"Private helper function transform a regular expression\ngiven in ``app_settings.URLI18N_INCLUDE_PATHS`` for\nchecking. This can be performed in strict or non-strict mode.\nIf strict mode it will make sure the given expression ends with\n``/$``. In non strict mode it will simply not care how the\nexpression ends. In both cases it will transform an expression\nto start with ``^/`` though.\n\nParams:\n    - ``path_expression``: a regular expression to check against a url path\n    - ``strict_mode``: a boolean indicating string or non-strict mode. See explainations above.\n    \nReturns:\n    - ``path_expression`` (modified if needed)\n\"\"\"\n", "func_signal": "def _edit_path_exp(path_expression, strict_mode=True):\n", "code": "if not path_expression.startswith('^/'):\n    if path_expression.startswith('^'):\n        path_expression = path_expression.replace('^', '^/')\n    elif path_expression.startswith('/'):\n        path_expression = '^%s' % path_expression\n    else:\n        path_expression = '^/%s' % path_expression\nif strict_mode is True:\n    if not path_expression.endswith('/$'):\n        if path_expression.endswith('$'):\n            path_expression = path_expression.replace('$', '/$')\n        elif path_expression.endswith('/'):\n            path_expression = '%s$' % path_expression\n        else:\n            path_expression = '%s/$' % path_expression\nreturn path_expression", "path": "urli18n\\utils.py", "repo_name": "torte/django-urli18n", "stars": 24, "license": "mit", "language": "python", "size": 445}
{"docstring": "#first with standard settings\n", "func_signal": "def test_view4_de(self):\n", "code": "translation.activate('de')\nresponse = self.client.get('/articles/2011/02/28/')\nself.assertRedirects(response, '/articles/2011/02/28/?lang=de')\nresponse = self.client.get('/articles/2011/02/28/?lang=de')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'de')\nself.assertEqual(response.request['QUERY_STRING'], 'lang=de')\n#exclude the url \napp_settings.URLI18N_INCLUDE_PATHS = []\nresponse = self.client.get('/articles/2011/02/28/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'de')\nself.assertEqual(response.request['QUERY_STRING'], '')\napp_settings.URLI18N_INCLUDE_PATHS = self.included_paths", "path": "urli18n\\tests\\__init__.py", "repo_name": "torte/django-urli18n", "stars": 24, "license": "mit", "language": "python", "size": 445}
{"docstring": "#first with standard settings\n", "func_signal": "def test_view2_zhcn(self):\n", "code": "translation.activate('zh-cn')\nresponse = self.client.get('/home/')\nself.assertRedirects(response, '/home/?lang=zh-cn')\nresponse = self.client.get('/home/?lang=zh-cn')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'zh-cn')\nself.assertEqual(response.request['QUERY_STRING'], 'lang=zh-cn')\n#exclude the url \napp_settings.URLI18N_INCLUDE_PATHS = []\nresponse = self.client.get('/home/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'zh-cn')\nself.assertEqual(response.request['QUERY_STRING'], '')\napp_settings.URLI18N_INCLUDE_PATHS = self.included_paths", "path": "urli18n\\tests\\__init__.py", "repo_name": "torte/django-urli18n", "stars": 24, "license": "mit", "language": "python", "size": 445}
{"docstring": "#first with standard settings\n", "func_signal": "def test_view3_zhcn(self):\n", "code": "translation.activate('zh-cn')\nresponse = self.client.get('/articles/2011/01/')\nself.assertRedirects(response, '/articles/2011/01/?lang=zh-cn')\nresponse = self.client.get('/articles/2011/01/?lang=zh-cn')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'zh-cn')\nself.assertEqual(response.request['QUERY_STRING'], 'lang=zh-cn')\n#exclude the url \napp_settings.URLI18N_INCLUDE_PATHS = []\nresponse = self.client.get('/articles/2011/01/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'zh-cn')\nself.assertEqual(response.request['QUERY_STRING'], '')\napp_settings.URLI18N_INCLUDE_PATHS = self.included_paths", "path": "urli18n\\tests\\__init__.py", "repo_name": "torte/django-urli18n", "stars": 24, "license": "mit", "language": "python", "size": 445}
{"docstring": "#first with standard settings\n", "func_signal": "def test_view4_zhcn(self):\n", "code": "translation.activate('zh-cn')\nresponse = self.client.get('/articles/2011/02/28/')\nself.assertRedirects(response, '/articles/2011/02/28/?lang=zh-cn')\nresponse = self.client.get('/articles/2011/02/28/?lang=zh-cn')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'zh-cn')\nself.assertEqual(response.request['QUERY_STRING'], 'lang=zh-cn')\n#exclude the url \napp_settings.URLI18N_INCLUDE_PATHS = []\nresponse = self.client.get('/articles/2011/02/28/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'zh-cn')\nself.assertEqual(response.request['QUERY_STRING'], '')\napp_settings.URLI18N_INCLUDE_PATHS = self.included_paths", "path": "urli18n\\tests\\__init__.py", "repo_name": "torte/django-urli18n", "stars": 24, "license": "mit", "language": "python", "size": 445}
{"docstring": "#first with standard settings\n", "func_signal": "def test_view1_de(self):\n", "code": "translation.activate('de')\nresponse = self.client.get('/')\nself.assertRedirects(response, '/de/')\nresponse = self.client.get('/de/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'de')\n#exclude the url \napp_settings.URLI18N_INCLUDE_PATHS = []\nresponse = self.client.get('/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'de')\napp_settings.URLI18N_INCLUDE_PATHS = self.included_paths", "path": "urli18n\\tests\\__init__.py", "repo_name": "torte/django-urli18n", "stars": 24, "license": "mit", "language": "python", "size": 445}
{"docstring": "#first with standard settings\n", "func_signal": "def test_view3_en(self):\n", "code": "translation.activate('en')\nresponse = self.client.get('/articles/2011/01/')\nself.assertRedirects(response, '/articles/2011/01/?lang=en')\nresponse = self.client.get('/articles/2011/01/?lang=en')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'en')\nself.assertEqual(response.request['QUERY_STRING'], 'lang=en')\n#change the standard behavior\napp_settings.URLI18N_ALWAYS_SHOW_LANGUAGE = False\nresponse = self.client.get('/articles/2011/01/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'en')\nself.assertEqual(response.request['QUERY_STRING'], '')\n#exclude the url \napp_settings.URLI18N_ALWAYS_SHOW_LANGUAGE = True\napp_settings.URLI18N_INCLUDE_PATHS = []\nresponse = self.client.get('/articles/2011/01/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'en')\nself.assertEqual(response.request['QUERY_STRING'], '')\napp_settings.URLI18N_INCLUDE_PATHS = self.included_paths", "path": "urli18n\\tests\\__init__.py", "repo_name": "torte/django-urli18n", "stars": 24, "license": "mit", "language": "python", "size": 445}
{"docstring": "\"\"\"Utility function which reconstruct the full_path\naccording to new paremeters.\n\nArgs:\n    - ``path``: the absolute path without the querystring\n    - ``querystring_parts``: the parts of the querystring as a list, similiar to ``querystring_parts`` from ``break_full_path`` function\n\nReturns:\n    - the reconstructed full path with query string\n\"\"\"\n", "func_signal": "def reconstruct_full_path(path, querystring_parts):\n", "code": "full_path = path\nif querystring_parts:\n    full_path = '?'.join([path, '&'.join(querystring_parts)])\nreturn full_path", "path": "urli18n\\utils.py", "repo_name": "torte/django-urli18n", "stars": 24, "license": "mit", "language": "python", "size": 445}
{"docstring": "#first with standard settings\n", "func_signal": "def test_view1_en(self):\n", "code": "translation.activate('en')\nresponse = self.client.get('/')\nself.assertRedirects(response, '/en/')\nresponse = self.client.get('/en/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'en')\n#change the standard behavior\napp_settings.URLI18N_ALWAYS_SHOW_LANGUAGE = False\nresponse = self.client.get('/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'en')\n#exclude the url \napp_settings.URLI18N_ALWAYS_SHOW_LANGUAGE = True\napp_settings.URLI18N_INCLUDE_PATHS = []\nresponse = self.client.get('/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'en')\napp_settings.URLI18N_INCLUDE_PATHS = self.included_paths", "path": "urli18n\\tests\\__init__.py", "repo_name": "torte/django-urli18n", "stars": 24, "license": "mit", "language": "python", "size": 445}
{"docstring": "\"\"\"Processes every ``process_request`` method from\nall middleware's which follow this middleware within \n``settings.MIDDLEWARE_CLASSES``\n\nArgs:\n    - ``middleware_object``: the current middleware object\n    - ``request``: the current django request object, this will be passed to the ``process_request`` calls\n\nReturns:\n    - if a ``process_request`` call returns an HttpResponse it will return this response, else None\n\"\"\"\n", "func_signal": "def process_missing_requests(middleware_object, request):\n", "code": "middleware_string = _create_middleware_string(middleware_object)\nmiddleware_tuple = []\nmiddleware_appeared = False\nfor middleware_path in settings.MIDDLEWARE_CLASSES:\n    if middleware_appeared is True:\n        middleware_tuple.append(middleware_path)\n    if middleware_appeared is False and middleware_path==middleware_string:\n        middleware_appeared = True\nmiddlware_instances = _init_middleware_classes(middleware_tuple)\nfor mw_instance in middlware_instances:\n    if hasattr(mw_instance, 'process_request'):\n        process_result = mw_instance.process_request(request)\n        if process_result is not None:\n            return process_result\nreturn None", "path": "urli18n\\utils.py", "repo_name": "torte/django-urli18n", "stars": 24, "license": "mit", "language": "python", "size": 445}
{"docstring": "#first with standard settings\n", "func_signal": "def test_view4_en(self):\n", "code": "translation.activate('en')\nresponse = self.client.get('/articles/2011/02/28/')\nself.assertRedirects(response, '/articles/2011/02/28/?lang=en')\nresponse = self.client.get('/articles/2011/02/28/?lang=en')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'en')\nself.assertEqual(response.request['QUERY_STRING'], 'lang=en')\n#change the standard behavior\napp_settings.URLI18N_ALWAYS_SHOW_LANGUAGE = False\nresponse = self.client.get('/articles/2011/02/28/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'en')\nself.assertEqual(response.request['QUERY_STRING'], '')\n#exclude the url \napp_settings.URLI18N_ALWAYS_SHOW_LANGUAGE = True\napp_settings.URLI18N_INCLUDE_PATHS = []\nresponse = self.client.get('/articles/2011/02/28/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'en')\nself.assertEqual(response.request['QUERY_STRING'], '')\napp_settings.URLI18N_INCLUDE_PATHS = self.included_paths", "path": "urli18n\\tests\\__init__.py", "repo_name": "torte/django-urli18n", "stars": 24, "license": "mit", "language": "python", "size": 445}
{"docstring": "#first with standard settings\n", "func_signal": "def test_view1_zhcn(self):\n", "code": "translation.activate('zh-cn')\nresponse = self.client.get('/')\nself.assertRedirects(response, '/zh-cn/')\nresponse = self.client.get('/zh-cn/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'zh-cn')\n#exclude the url \napp_settings.URLI18N_INCLUDE_PATHS = []\nresponse = self.client.get('/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'zh-cn')\napp_settings.URLI18N_INCLUDE_PATHS = self.included_paths", "path": "urli18n\\tests\\__init__.py", "repo_name": "torte/django-urli18n", "stars": 24, "license": "mit", "language": "python", "size": 445}
{"docstring": "#first with standard settings\n", "func_signal": "def test_view1_en(self):\n", "code": "translation.activate('en')\nresponse = self.client.get('/')\nself.assertRedirects(response, '/?lang=en')\nresponse = self.client.get('/?lang=en')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'en')\nself.assertEqual(response.request['QUERY_STRING'], 'lang=en')\n#change the standard behavior\napp_settings.URLI18N_ALWAYS_SHOW_LANGUAGE = False\nresponse = self.client.get('/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'en')\nself.assertEqual(response.request['QUERY_STRING'], '')\n#exclude the url \napp_settings.URLI18N_ALWAYS_SHOW_LANGUAGE = True\napp_settings.URLI18N_INCLUDE_PATHS = []\nresponse = self.client.get('/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'en')\nself.assertEqual(response.request['QUERY_STRING'], '')\napp_settings.URLI18N_INCLUDE_PATHS = self.included_paths", "path": "urli18n\\tests\\__init__.py", "repo_name": "torte/django-urli18n", "stars": 24, "license": "mit", "language": "python", "size": 445}
{"docstring": "\"\"\"Helper to determine if a given path should be\ntransformed to the show the language in the url. This\nis checked against ``URLI18N_INCLUDE_PATHS`` in \n``app_settings``. The ``MEDIA_URL`` and ``STATIC_URL``\nfrom ``django.conf.settings`` are excluded automatically.\n\nParams:\n    - ``path``: the url path which should be checked\n    - ``strict_mode``: a boolean which indicates if the path should be checked strict or non-strict. This affects check within the function ``_edit_path_exp`` only.\n\nReturns:\n    A Boolean: ``True`` if the given url path should be included,\n    ``False`` otherwise.\n\"\"\"\n", "func_signal": "def is_included_path(path, strict_mode=True):\n", "code": "path = urlparse.urlparse(path).path\nmedia_url = getattr(settings, 'MEDIA_URL')\nstatic_url = getattr(settings, 'STATIC_URL')\nif not (media_url and path.startswith(media_url))\\\nand not (static_url and path.startswith(static_url)):\n    for p in app_settings.URLI18N_INCLUDE_PATHS:\n        p = _edit_path_exp(p, strict_mode)\n        regex_path = re.compile(p, re.UNICODE)\n        if regex_path.match(path):\n            return True\nreturn False", "path": "urli18n\\utils.py", "repo_name": "torte/django-urli18n", "stars": 24, "license": "mit", "language": "python", "size": 445}
{"docstring": "#first with standard settings\n", "func_signal": "def test_view2_de(self):\n", "code": "translation.activate('de')\nresponse = self.client.get('/home/')\nself.assertRedirects(response, '/de/home/')\nresponse = self.client.get('/de/home/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'de')\n#exclude the url \napp_settings.URLI18N_INCLUDE_PATHS = []\nresponse = self.client.get('/home/')\nself.assertEqual(response.status_code, 200)\nself.assertEqual(translation.get_language(), 'de')\napp_settings.URLI18N_INCLUDE_PATHS = self.included_paths", "path": "urli18n\\tests\\__init__.py", "repo_name": "torte/django-urli18n", "stars": 24, "license": "mit", "language": "python", "size": 445}
{"docstring": "'''Gets the dimensions of the document\\' viewbox.'''\n", "func_signal": "def getviewbox(self):\n", "code": "svg = self.doc.xpath(\"/svg:svg\", namespaces=self.nss)[0]\nreturn svg.attrib.get(\"viewBox\").split(\" \")", "path": "bin\\img2xml\\svg.py", "repo_name": "hcayless/img2xml", "stars": 19, "license": "None", "language": "python", "size": 8887}
{"docstring": "'''Gets the document height.'''\n", "func_signal": "def getheight(self):\n", "code": "svg = self.doc.xpath(\"/svg:svg\", namespaces=self.nss)[0]\nreturn int(svg.attrib.get(\"height\").rstrip(\"pt\"))", "path": "bin\\img2xml\\svg.py", "repo_name": "hcayless/img2xml", "stars": 19, "license": "None", "language": "python", "size": 8887}
{"docstring": "'''Used in removing paths from an svg:g container. Converts the \nsupplied path to the document\\'s coordinate system.'''\n", "func_signal": "def transformpath(self, path):\n", "code": "points = path.split()\nresult = \"\"\npt = \"\"\ntf = \"all\"\nfor point in points:\n  if pt == \"\":\n    pt = point\n  else:\n    if point.isalpha():\n      pt += \" %s\" % point\n    else:\n      pt += point\n  if not pt.isalpha():\n    if pt[0].isalpha():\n      result += \" %s\" % pt[0]\n      if pt[0].islower():\n        function = \"scale\"\n      else:\n        function = \"all\"\n      pt = pt[1:]\n    if pt.find(\",\") > 0:\n      if pt[len(pt) - 1].isalpha():\n        p = self.transformpoint(pt[0:len(pt) - 1], function)\n        result += \" %s,%s\" % p\n        result += pt[len(pt) - 1]\n      else:\n        p = self.transformpoint(pt, function)\n        result += \" %s,%s \" % p\n      pt = \"\"\n    else:\n      pt += \",\"\n  else:\n    result += \" %s\" % point\n    if pt.islower():\n      function = \"scale\"\n    else:\n      function = \"all\"\nreturn result", "path": "bin\\img2xml\\svg.py", "repo_name": "hcayless/img2xml", "stars": 19, "license": "None", "language": "python", "size": 8887}
{"docstring": "'''Adds a background image to the SVG document.'''\n", "func_signal": "def add_image(self, url):\n", "code": "width = self.doc.xpath(\"//svg:svg/@width\", namespaces=self.nss)[0].rstrip(\"pt\")\nheight = self.doc.xpath(\"//svg:svg/@height\", namespaces=self.nss)[0].rstrip(\"pt\")\nimage = etree.Element(self.NS + \"image\", nsmap={None: self.SVG_NS, \"xlink\": self.XLINK_NS})\nimage.set(self.XLNS + \"href\", url)\nimage.set(self.XLNS + \"actuate\", \"onLoad\")\nimage.set(\"width\", width)\nimage.set(\"height\", height)\nself.doc.getroot().insert(0, image)", "path": "bin\\img2xml\\svg.py", "repo_name": "hcayless/img2xml", "stars": 19, "license": "None", "language": "python", "size": 8887}
{"docstring": "'''Determines whether this rectangle intersects with the supplied one.'''\n", "func_signal": "def intersects(self, rect):\n", "code": "if self.contains(rect.tl):\n  return True\nif self.contains(rect.tr):\n  return True\nif self.contains(rect.bl):\n  return True\nif self.contains(rect.br):\n  return True\nreturn False", "path": "bin\\img2xml\\shapes.py", "repo_name": "hcayless/img2xml", "stars": 19, "license": "None", "language": "python", "size": 8887}
{"docstring": "'''Since potrace wraps all svg:paths in an svg:g, this function unwraps them\nand converts their path data to the document\\s coordinate system.'''\n", "func_signal": "def ungroup(self):\n", "code": "g = self.doc.xpath(\"/svg:svg/svg:g\", namespaces=self.nss)[0]\nfor path in g.xpath(\"svg:path\", namespaces=self.nss):\n  g.remove(path)\n  path.attrib['d'] = self.transformpath(path.attrib['d'])\n  self.doc.getroot().append(path)\nself.doc.getroot().remove(g)", "path": "bin\\img2xml\\svg.py", "repo_name": "hcayless/img2xml", "stars": 19, "license": "None", "language": "python", "size": 8887}
{"docstring": "'''Gets the lower X bound of the document.'''\n", "func_signal": "def getxmin(self):\n", "code": "result = self.getviewbox()\nreturn int(result[0])", "path": "bin\\img2xml\\svg.py", "repo_name": "hcayless/img2xml", "stars": 19, "license": "None", "language": "python", "size": 8887}
{"docstring": "'''Adds an svg:rect with the supplied characteristics.'''\n", "func_signal": "def add_rectangle(self, rect, fill, opacity):\n", "code": "style = \"fill:%s;fill-opacity:1;opacity:%s\" % (fill, opacity)\nself.doc.getroot().append(etree.Element(self.NS + \"rect\", x=str(rect.tl.x), y=str(rect.tl.y), width=str(rect.width()), height=str(rect.height()), id=rect.id, style=style,  nsmap=self.NSMAP))", "path": "bin\\img2xml\\svg.py", "repo_name": "hcayless/img2xml", "stars": 19, "license": "None", "language": "python", "size": 8887}
{"docstring": "'''Gets the path information in the svg:path/@d attribute.'''\n", "func_signal": "def getpathdata(self):\n", "code": "paths = self.doc.xpath(\"//svg:path\", namespaces=self.nss)\nresult = []\nfor path in paths:\n  result.append((path.get('id'), path.get('d')))\nreturn result", "path": "bin\\img2xml\\svg.py", "repo_name": "hcayless/img2xml", "stars": 19, "license": "None", "language": "python", "size": 8887}
{"docstring": "'''Gets the lower Y bound of the document.'''\n", "func_signal": "def getymin(self):\n", "code": "result = self.getviewbox()\nreturn int(result[1])", "path": "bin\\img2xml\\svg.py", "repo_name": "hcayless/img2xml", "stars": 19, "license": "None", "language": "python", "size": 8887}
{"docstring": "'''Retrieves a list of Rectangles based on the <svg:rect> elements in the document.'''\n", "func_signal": "def getrects(self):\n", "code": "rects = self.doc.xpath(\"//svg:rect\", namespaces=self.nss)\nresult = []\nfor r in rects:\n  x = float(r.get('x'))\n  y = float(r.get('y'))\n  w = float(r.get('width'))\n  h = float(r.get('height'))\n  result.append(Rectangle({'tl':(x, y), 'tr':(x, y+w), 'bl':(x+h, y), 'br':(x+h, y+w), 'id':r.get('id')}))\nreturn result", "path": "bin\\img2xml\\svg.py", "repo_name": "hcayless/img2xml", "stars": 19, "license": "None", "language": "python", "size": 8887}
{"docstring": "'''Determines whether the supplied point is inside this rectangle.'''\n", "func_signal": "def contains(self, point):\n", "code": "if hasattr(point, 'x'):\n  return (self.tl.x <= point.x <= self.br.x) & (self.tl.y <= point.y <= self.br.y)\nelse:\n  return self.contains(self.tl) and self.contains(self.tr) and self.contains(self.bl) and self.contains(self.br)", "path": "bin\\img2xml\\shapes.py", "repo_name": "hcayless/img2xml", "stars": 19, "license": "None", "language": "python", "size": 8887}
{"docstring": "'''applies the supplied function (scale, translate, or all) to the given point \nusing the matrices extracted from the containing group.'''\n", "func_signal": "def transformpoint(self, point, function=\"all\"):\n", "code": "p = point.split(\",\")\ntp = numpy.matrix( [[float(p[0])], [float(p[1])], [1]])\nmatrices = self.getmatrices()\n\nif (function == \"scale\" or function == \"all\") and matrices.has_key('scale'):\n  tp = matrices['scale'] * tp\nif (function == \"translate\" or function == \"all\") and matrices.has_key('translate'):\n  tp = matrices['translate'] * tp\nif function == \"all\" and matrices.has_key('matrix'):\n  tp = matrices['matrix'] * tp\nreturn (float(tp[0]), float(tp[1]))", "path": "bin\\img2xml\\svg.py", "repo_name": "hcayless/img2xml", "stars": 19, "license": "None", "language": "python", "size": 8887}
{"docstring": "'''Analyses all of the svg:paths in the document and returns a list of bounding polygons.'''\n", "func_signal": "def analysepaths(self):\n", "code": "polygons = []\npcount = 0\nfor pathdata in self.doc.xpath(\"//svg:path\", namespaces=self.nss):\n  pcount = pcount + 1\n  path = pathdata.xpath(\"@d\")[0].replace(\"\\n\", ' ').split()\n  index = 0\n  polygon = []\n  current = \"\"\n  start = \"\"\n  for p in path:\n    if p.find('m') == 0:\n      if \"\" != start:\n        current = start\n      if p != 'm':\n        if index == 0:\n          pair = \"%s,%s\" % (p.lstrip('m'), path[index + 1])\n          polygon.append(pair)\n          current = pair\n          start = current\n      else:\n        if index == 0:\n          current = path[index + 1]\n        else:\n          current = self.absolute_coords(path[index + 1], current)\n        start = current\n        polygon.append(current)\n    if p.find('M') == 0:\n      if p != 'M':\n        pair = \"%s,%s\" % (p.lstrip('M'), path[index + 1])\n        current = pair\n      else:\n        current = path[index + 1]\n      start = current\n      polygon.append(current)\n    if p.find('l') == 0:\n      if p != 'l':\n        pair = \"%s,%s\" % (p.lstrip('l'), path[index + 1])\n        current = self.absolute_coords(pair, current)\n        polygon.append(current)\n      else:\n        i = index\n        while re.match(\"[-.0-9]+,[-.0-9z]+\", path[i+1]):\n          current = self.absolute_coords(path[i + 1].rstrip('z'), current)\n          polygon.append(current)\n          if path[i+1].find('z') > 0:\n            break\n          i = i + 1\n    if p.find('L') == 0:\n      if p != 'L':\n        pair = \"%s,%s\" % (p.lstrip('L'), path[index + 1])\n        current = pair\n      else:\n        current = path[index + 1]\n      polygon.append(current)\n    if p.find('c') == 0:\n      if p != 'c':\n        pair = \"%s,%s\" % (p.lstrip('c'), path[index + 1])\n        current = self.absolute_coords(pair, current)\n        polygon.append(current)\n        i = index + 2\n        while path[i].isdigit():\n          c2 = path[i + 1]\n          if c2.find('z') > 0:\n            pair = \"%s,%s\" % (path[i], c2.rstrip('z'))\n          else :\n            pair = \"%s,%s\" % (path[i], c2)\n          current = self.absolute_coords(pair, current)\n          polygon.append(current)\n          if c2.find('z') > 0:\n            break\n          i = i + 2\n      else:\n        i = index + 1\n        c = 0\n        while re.match(\"[-.0-9]+,[-.0-9z]+\", path[i]):\n          if c == 2:\n            current = self.absolute_coords(path[i].rstrip('z'), current)\n            polygon.append(current)\n            if path[i].find('z') > 0:\n              break\n            c = 0\n          else:\n            polygon.append(self.absolute_coords(path[i].rstrip('z'), current))\n            c = c + 1\n          i = i + 1\n    if p.find('C') == 0:\n      if p != 'C':\n        pair = \"%s,%s\" % (p.lstrip('C'), path[index + 1])\n        polygon.append(pair)\n        pair = \"%s,%s\" % (path[index + 2], path[index + 3])\n        polygon.append(pair)\n        pair = \"%s,%s\" % (path[index + 4], path[index + 5])\n        current = pair\n        polygon.append(current)\n      else:\n        polygon.append(path[index + 1])\n        polygon.append(path[index + 2])\n        current = path[index + 3]\n        polygon.append(current)\n    index = index + 1\n  pid = pathdata.xpath(\"@id\")\n  if not(pid):\n    pid = \"p%s\" % str(pcount).rjust(5, \"0\")\n  polygons.append((pid[0], polygon))\nreturn polygons", "path": "bin\\img2xml\\svg.py", "repo_name": "hcayless/img2xml", "stars": 19, "license": "None", "language": "python", "size": 8887}
{"docstring": "'''Merges the supplied rectangle with the current one to produce a larger rectangle.'''\n", "func_signal": "def merge(self, rect):\n", "code": "if self.tr == None:\n  self.tl = rect.tl\n  self.tr = rect.tr\n  self.bl = rect.bl\n  self.br = rect.br\nelse:\n  tl1 = self.tl\n  tl2 = rect.tl\n  br1 = self.br\n  br2 = rect.br\n  top = float(tl1.y)\n  if top > float(tl2.y):\n    top = float(tl2.y)\n  left = float(tl1.x)\n  if left > float(tl2.x):\n    left = float(tl2.x)\n  bottom = float(br1.y)\n  if bottom < float(br2.y):\n    bottom = float(br2.y)\n  right = float(br1.x)\n  if right < float(br2.x):\n    right = float(br2.x)\n  self.tr = Point(right, top)\n  self.tl = Point(left, top)\n  self.br = Point(right, bottom)\n  self.bl = Point(left, bottom)", "path": "bin\\img2xml\\shapes.py", "repo_name": "hcayless/img2xml", "stars": 19, "license": "None", "language": "python", "size": 8887}
{"docstring": "'''Converts relative coordinates to absolute.'''\n", "func_signal": "def absolute_coords(self, coords, current):\n", "code": "pair = coords.split(\",\")\ncpair = current.split(\",\")\npair[0] = str(float(pair[0]) + float(cpair[0]))\npair[1] = str(float(pair[1]) + float(cpair[1]))\nreturn \",\".join(pair)", "path": "bin\\img2xml\\svg.py", "repo_name": "hcayless/img2xml", "stars": 19, "license": "None", "language": "python", "size": 8887}
{"docstring": "'''Returns a list of values in the dictionary for the given range.'''\n", "func_signal": "def getvalues(range, dict):\n", "code": "result = []\nfor k in range:\n  result.append(dict[k])\nreturn result", "path": "bin\\line_detector.py", "repo_name": "hcayless/img2xml", "stars": 19, "license": "None", "language": "python", "size": 8887}
{"docstring": "'''Gets the document width.'''\n", "func_signal": "def getwidth(self):\n", "code": "svg = self.doc.xpath(\"/svg:svg\", namespaces=self.nss)[0]\nreturn int(svg.attrib.get(\"width\").rstrip(\"pt\"))", "path": "bin\\img2xml\\svg.py", "repo_name": "hcayless/img2xml", "stars": 19, "license": "None", "language": "python", "size": 8887}
{"docstring": "'''Given a polygon, return the bounding rectangle'''\n#print polygon\n", "func_signal": "def extract_rectangle(polygon):\n", "code": "pair = polygon[0].split(',')\nleft = float(pair[0])\nright = left\ntop = float(pair[1])\nbottom = top\nfor p in polygon:\n  pair = p.split(',')\n  x = float(pair[0])\n  y = float(pair[1])\n  if x < left:\n    left = x\n  if x > right:\n    right = x\n  if y < top:\n    top = y\n  if y > bottom:\n    bottom = y\ntr = (right, top)\ntl = (left, top)\nbr = (right,bottom)\nbl = (left,bottom)\nrectangle = img2xml.shapes.Rectangle({'tr': tr, 'tl': tl, 'br': br, 'bl': bl})\nreturn rectangle", "path": "bin\\line_detector.py", "repo_name": "hcayless/img2xml", "stars": 19, "license": "None", "language": "python", "size": 8887}
{"docstring": "'''Returns a numpy matrix derived from the svg:g element\\'s @transform attribute.'''\n", "func_signal": "def getmatrices(self):\n", "code": "if False == hasattr(self, \"matrices\"):\n  self.matrices = {}\n  g = self.doc.xpath(\"/svg:svg/svg:g\", namespaces=self.nss)\n  if g:\n    t = g[0].get(\"transform\")\n    if t:\n      transforms = t.split(\" \")\n      fm = False\n      #print transforms\n      for f in transforms:\n        if f.find(\"matrix\") >= 0:\n          #print \"Found matrix\"\n          m = f.lstrip(\"matrix(\").rstrip(\")\")\n          m = m.split(\",\")\n          self.matrices['matrix'] = numpy.matrix( [[float(m[0]),float(m[2]),float(m[4])],[float(m[1]),float(m[3]),float(m[5])],[0,0,1]])\n          continue\n        if f.find(\"translate\") >= 0:\n          #print \"Found translate\"\n          l = f.lstrip(\"translate(\").rstrip(\")\")\n          translate = l.split(\",\")\n          self.matrices['translate'] = numpy.matrix([[1,0,float(translate[0])],[0,1,float(translate[1])],[0,0,1]])\n          continue\n        if f.find(\"scale\") >= 0:\n          #print \"Found scale\"\n          s = f.lstrip(\"scale(\").rstrip(\")\")\n          scale = s.split(\",\")\n          self.matrices['scale'] = numpy.matrix([[float(scale[0]),0,0],[0,float(scale[1]),0],[0,0,1]])\n          continue\nif len(self.matrices) == 0:\n  self.matrices['matrix'] = numpy.matrix([[1,0,0],[0,1,0],[0,0,1]])\nreturn self.matrices", "path": "bin\\img2xml\\svg.py", "repo_name": "hcayless/img2xml", "stars": 19, "license": "None", "language": "python", "size": 8887}
{"docstring": "\"\"\"Build root authorized keys file based on current ACLs.\"\"\"\n", "func_signal": "def gen_sudoers(self, entry, metadata):\n", "code": "superusers = self.repository.entries['superusers'].data.split()\ntry:\n    rootlike = [line.split(':', 1) for line in \\\n                self.repository.entries['rootlike'].data.split()]\n    superusers += [user for (user, host) in rootlike \\\n                   if host == metadata.hostname.split('.')[0]]\nexcept:\n    pass\nentry.text = self.repository.entries['static.sudoers'].data\nentry.text += \"\".join([\"%s ALL=(ALL) ALL\\n\" % uname \\\n                       for uname in superusers])\nperms = {'owner': 'root',\n         'group': 'root',\n         'perms': '0440'}\n[entry.attrib.__setitem__(key, value) for (key, value) \\\n in list(perms.items())]", "path": "src\\lib\\Server\\Plugins\\Account.py", "repo_name": "solj/bcfg2-old", "stars": 26, "license": "other", "language": "python", "size": 8612}
{"docstring": "\"\"\"Build password file from cached yp data.\"\"\"\n", "func_signal": "def from_yp_cb(self, entry, metadata):\n", "code": "fname = entry.attrib['name'].split('/')[-1]\nentry.text = self.repository.entries[\"static.%s\" % (fname)].data\nentry.text += self.repository.entries[\"dyn.%s\" % (fname)].data\nperms = {'owner': 'root',\n         'group': 'root',\n         'perms': '0644'}\n[entry.attrib.__setitem__(key, value) for (key, value) in \\\n list(perms.items())]", "path": "src\\lib\\Server\\Plugins\\Account.py", "repo_name": "solj/bcfg2-old", "stars": 26, "license": "other", "language": "python", "size": 8612}
{"docstring": "\"\"\"A genshi filter that removes comments from the stream.\"\"\"\n", "func_signal": "def removecomment(stream):\n", "code": "for kind, data, pos in stream:\n    if kind is genshi.core.COMMENT:\n        continue\n    yield kind, data, pos", "path": "src\\lib\\Server\\Plugins\\Cfg.py", "repo_name": "solj/bcfg2-old", "stars": 26, "license": "other", "language": "python", "size": 8612}
{"docstring": "\"\"\"return a list of all entries pertinent\nto a client => [base, delta1, delta2]\n\"\"\"\n", "func_signal": "def get_pertinent_entries(self, entry, metadata):\n", "code": "matching = [ent for ent in list(self.entries.values()) if \\\n            ent.specific.matches(metadata)]\nmatching.sort(key=operator.attrgetter('specific'))\n# base entries which apply to a client\n# (e.g. foo, foo.G##_groupname, foo.H_hostname)\nbase_files = [matching.index(m) for m in matching\n              if not m.specific.delta]\nif not base_files:\n    logger.error(\"No base file found for %s\" % entry.get('name'))\n    raise Bcfg2.Server.Plugin.PluginExecutionError\nbase = min(base_files)\nused = matching[:base + 1]\nused.reverse()\nreturn used", "path": "src\\lib\\Server\\Plugins\\Cfg.py", "repo_name": "solj/bcfg2-old", "stars": 26, "license": "other", "language": "python", "size": 8612}
{"docstring": "\"\"\"Build limits entries based on current ACLs.\"\"\"\n", "func_signal": "def gen_limits_cb(self, entry, metadata):\n", "code": "entry.text = self.repository.entries[\"static.limits.conf\"].data\nsuperusers = self.repository.entries[\"superusers\"].data.split()\nuseraccess = [line.split(':') for line in \\\n              self.repository.entries[\"useraccess\"].data.split()]\nusers = [user for (user, host) in \\\n         useraccess if host == metadata.hostname.split('.')[0]]\nperms = {'owner': 'root',\n         'group': 'root',\n         'perms': '0600'}\n[entry.attrib.__setitem__(key, value) for (key, value) in \\\n list(perms.items())]\nentry.text += \"\".join([\"%s hard maxlogins 1024\\n\" % uname for uname in superusers + users])\nif \"*\" not in users:\n    entry.text += \"* hard maxlogins 0\\n\"", "path": "src\\lib\\Server\\Plugins\\Account.py", "repo_name": "solj/bcfg2-old", "stars": 26, "license": "other", "language": "python", "size": 8612}
{"docstring": "# dont use the sysv constructor\n", "func_signal": "def __init__(self, logger, setup, config):\n", "code": "Bcfg2.Client.Tools.PkgTool.__init__(self, logger, setup, config)\nnoaskfile = tempfile.NamedTemporaryFile()\nself.noaskname = noaskfile.name\ntry:\n    noaskfile.write(Bcfg2.Client.Tools.SYSV.noask)\nexcept:\n    pass", "path": "src\\lib\\Client\\Tools\\Blast.py", "repo_name": "solj/bcfg2-old", "stars": 26, "license": "other", "language": "python", "size": 8612}
{"docstring": "\"\"\"Remove specified Sysv packages.\"\"\"\n", "func_signal": "def RemovePackages(self, packages):\n", "code": "names = [pkg.get('name') for pkg in packages]\nself.logger.info(\"Removing packages: %s\" % (names))\nself.cmd.run(\"/usr/sbin/pkgrm -a %s -n %s\" % \\\n             (self.noaskname, names))\nself.RefreshPackages()\nself.extra = self.FindExtraPackages()", "path": "src\\lib\\Client\\Tools\\SYSV.py", "repo_name": "solj/bcfg2-old", "stars": 26, "license": "other", "language": "python", "size": 8612}
{"docstring": "'''return a list of candidate pull locations'''\n", "func_signal": "def list_accept_choices(self, entry, metadata):\n", "code": "used = self.get_pertinent_entries(entry, metadata)\nret = []\nif used:\n    ret.append(used[0].specific)\nif not ret[0].hostname:\n    ret.append(Bcfg2.Server.Plugin.Specificity(hostname=metadata.hostname))\nreturn ret", "path": "src\\lib\\Server\\Plugins\\Cfg.py", "repo_name": "solj/bcfg2-old", "stars": 26, "license": "other", "language": "python", "size": 8612}
{"docstring": "\"\"\"Verify package for entry.\"\"\"\n", "func_signal": "def VerifyPackage(self, entry, modlist, checksums=True):\n", "code": "if not 'version' in entry.attrib:\n    self.logger.info(\"Cannot verify unversioned package %s\" %\n                     (entry.attrib['name']))\n    return False\npkgname = entry.get('name')\nif self.pkg_cache.has_key(pkgname):\n    if self._newapi:\n        is_installed = self.pkg_cache[pkgname].is_installed\n    else:\n        is_installed = self.pkg_cache[pkgname].isInstalled\nif not self.pkg_cache.has_key(pkgname) or not is_installed:\n    self.logger.info(\"Package %s not installed\" % (entry.get('name')))\n    entry.set('current_exists', 'false')\n    return False\n\npkg = self.pkg_cache[pkgname]\nif self._newapi:\n    installed_version = pkg.installed.version\n    candidate_version = pkg.candidate.version\nelse:\n    installed_version = pkg.installedVersion\n    candidate_version = pkg.candidateVersion\nif entry.get('version') == 'auto':\n    if self._newapi:\n        is_upgradable = self.pkg_cache._depcache.is_upgradable(pkg._pkg)\n    else:\n        is_upgradable = self.pkg_cache._depcache.IsUpgradable(pkg._pkg)\n    if is_upgradable:\n        desiredVersion = candidate_version\n    else:\n        desiredVersion = installed_version\nelif entry.get('version') == 'any':\n    desiredVersion = installed_version\nelse:\n    desiredVersion = entry.get('version')\nif desiredVersion != installed_version:\n    entry.set('current_version', installed_version)\n    entry.set('qtext', \"Modify Package %s (%s -> %s)? (y/N) \" % \\\n              (entry.get('name'), entry.get('current_version'),\n               desiredVersion))\n    return False\nelse:\n    # version matches\n    if not self.setup['quick'] and entry.get('verify', 'true') == 'true' \\\n           and checksums:\n        pkgsums = self.VerifyDebsums(entry, modlist)\n        return pkgsums\n    return True", "path": "src\\lib\\Client\\Tools\\APT.py", "repo_name": "solj/bcfg2-old", "stars": 26, "license": "other", "language": "python", "size": 8612}
{"docstring": "\"\"\"Deal with extra configuration detected.\"\"\"\n", "func_signal": "def Remove(self, packages):\n", "code": "pkgnames = \" \".join([pkg.get('name') for pkg in packages])\nself.pkg_cache = apt.cache.Cache()\nif len(packages) > 0:\n    self.logger.info('Removing packages:')\n    self.logger.info(pkgnames)\n    for pkg in pkgnames.split(\" \"):\n        try:\n            if self._newapi:\n                self.pkg_cache[pkg].mark_delete(purge=True)\n            else:\n                self.pkg_cache[pkg].markDelete(purge=True)\n        except:\n            if self._newapi:\n                self.pkg_cache[pkg].mark_delete()\n            else:\n                self.pkg_cache[pkg].markDelete()\n    try:\n        self.pkg_cache.commit()\n    except SystemExit:\n        # thank you python-apt 0.6\n        pass\n    self.pkg_cache = apt.cache.Cache()\n    self.modified += packages\n    self.extra = self.FindExtra()", "path": "src\\lib\\Client\\Tools\\APT.py", "repo_name": "solj/bcfg2-old", "stars": 26, "license": "other", "language": "python", "size": 8612}
{"docstring": "\"\"\"Find extra packages.\"\"\"\n", "func_signal": "def FindExtra(self):\n", "code": "packages = [entry.get('name') for entry in self.getSupportedEntries()]\nif self._newapi:\n    extras = [(p.name, p.installed.version) for p in self.pkg_cache\n              if p.is_installed and p.name not in packages]\nelse:\n    extras = [(p.name, p.installedVersion) for p in self.pkg_cache\n              if p.isInstalled and p.name not in packages]\nreturn [Bcfg2.Client.XML.Element('Package', name=name, \\\n                                 type='deb', version=version) \\\n                                 for (name, version) in extras]", "path": "src\\lib\\Client\\Tools\\APT.py", "repo_name": "solj/bcfg2-old", "stars": 26, "license": "other", "language": "python", "size": 8612}
{"docstring": "\"\"\"Refresh memory hashes of packages.\"\"\"\n", "func_signal": "def RefreshPackages(self):\n", "code": "self.installed = {}\n# Build list of packages\nlines = self.cmd.run(\"/usr/bin/pkginfo -x\")[1]\nwhile lines:\n    # Splitting on whitespace means that packages with spaces in\n    # their version numbers don't work right.  Found this with\n    # IBM TSM software with package versions like\n    #           \"Version 6 Release 1 Level 0.0\"\n    # Should probably be done with a regex but this works.\n    version = lines.pop().split(') ')[1]\n    pkg = lines.pop().split()[0]\n    self.installed[pkg] = version", "path": "src\\lib\\Client\\Tools\\SYSV.py", "repo_name": "solj/bcfg2-old", "stars": 26, "license": "other", "language": "python", "size": 8612}
{"docstring": "# entries are name -> (modified, correct, start, desired, end)\n# not sure we can get all of this from old format stats\n", "func_signal": "def statistics_from_old_stats(self, metadata, xdata):\n", "code": "t1 = time.time()\nentries = dict([('Package', dict()),\n                ('Service', dict()), ('Path', dict())])\nextra = dict([('Package', dict()), ('Service', dict()),\n              ('Path', dict())])\nbad = []\nstate = xdata.find('.//Statistics')\ncorrect = state.get('state') == 'clean'\nrevision = u_str(state.get('revision', '-1'))\nfor entry in state.find('.//Bad'):\n    data = [False, False, u_str(entry.get('name'))] \\\n           + build_snap_ent(entry)\n    if entry.tag in ftypes:\n        etag = 'Path'\n    else:\n        etag = entry.tag\n    entries[etag][entry.get('name')] = data\nfor entry in state.find('.//Modified'):\n    if entry.tag in ftypes:\n        etag = 'Path'\n    else:\n        etag = entry.tag\n    if entry.get('name') in entries[etag]:\n        data = [True, False, u_str(entry.get('name'))] + \\\n               build_snap_ent(entry)\n    else:\n        data = [True, False, u_str(entry.get('name'))] + \\\n               build_snap_ent(entry)\nfor entry in state.find('.//Extra'):\n    if entry.tag in datafields:\n        data = build_snap_ent(entry)[1]\n        ename = u_str(entry.get('name'))\n        data['name'] = ename\n        extra[entry.tag][ename] = data\n    else:\n        print(\"extra\", entry.tag, entry.get('name'))\nt2 = time.time()\nsnap = Snapshot.from_data(self.session, correct, revision,\n                          metadata, entries, extra)\nself.session.add(snap)\nself.session.commit()\nt3 = time.time()\nlogger.info(\"Snapshot storage took %fs\" % (t3 - t2))\nreturn True", "path": "src\\lib\\Server\\Plugins\\Snapshots.py", "repo_name": "solj/bcfg2-old", "stars": 26, "license": "other", "language": "python", "size": 8612}
{"docstring": "\"\"\" Build local data structures \"\"\"\n", "func_signal": "def Index(self):\n", "code": "for section in self.sections():\n    self.remove_section(section)\nself.read(self.name)\nif self.pkg_obj.sources.loaded:\n    # only reload Packages plugin if sources have been loaded.\n    # otherwise, this is getting called on server startup, and\n    # we have to wait until all sources have been indexed\n    # before we can call Packages.Reload()\n    self.pkg_obj.Reload()", "path": "src\\lib\\Server\\Plugins\\Packages\\PackagesConfig.py", "repo_name": "solj/bcfg2-old", "stars": 26, "license": "other", "language": "python", "size": 8612}
{"docstring": "\"\"\"Ensures file contains all necessary attributes in order \"\"\"\n", "func_signal": "def checkformat(values, indices):\n", "code": "filelist = [pair[0] for pair in values]\n\n#    lines = len(filelist)\n\nfilelist = filelist[indices[0]:]\n\nfor index in indices:\n    if filelist[0:13] != host_attribs:\n        # figure out what to do here\n        return False\n    else:\n        # process rest of host attributes\n        try:\n            next = filelist[1:].index('hostname')\n            remaining = filelist[13:next + 1]\n            filelist = filelist[next + 1:]\n        except:\n            remaining = filelist[13:]\n        needfields = ['mac_addr', 'hdwr_type', 'ip_addr']\n        if [item for item in needfields if item not in remaining]:\n            return False\nreturn True", "path": "tools\\batchadd.py", "repo_name": "solj/bcfg2-old", "stars": 26, "license": "other", "language": "python", "size": 8612}
{"docstring": "# it looks like you can't install arbitrary versions of software\n# out of the pkg cache, we will still need to call apt-get\n", "func_signal": "def Install(self, packages, states):\n", "code": "ipkgs = []\nbad_pkgs = []\nfor pkg in packages:\n    if not self.pkg_cache.has_key(pkg.get('name')):\n        self.logger.error(\"APT has no information about package %s\" % (pkg.get('name')))\n        continue\n    if pkg.get('version') in ['auto', 'any']:\n        if self._newapi:\n            ipkgs.append(\"%s=%s\" % (pkg.get('name'),\n                                    self.pkg_cache[pkg.get('name')].candidate.version))\n        else:\n            ipkgs.append(\"%s=%s\" % (pkg.get('name'),\n                                    self.pkg_cache[pkg.get('name')].candidateVersion))\n        continue\n    if self._newapi:\n        avail_vers = [x.ver_str for x in \\\n                      self.pkg_cache[pkg.get('name')]._pkg.version_list]\n    else:\n        avail_vers = [x.VerStr for x in \\\n                      self.pkg_cache[pkg.get('name')]._pkg.VersionList]\n    if pkg.get('version') in avail_vers:\n        ipkgs.append(\"%s=%s\" % (pkg.get('name'), pkg.get('version')))\n        continue\n    else:\n        self.logger.error(\"Package %s: desired version %s not in %s\" \\\n                          % (pkg.get('name'), pkg.get('version'),\n                             avail_vers))\n    bad_pkgs.append(pkg.get('name'))\nif bad_pkgs:\n    self.logger.error(\"Cannot find correct versions of packages:\")\n    self.logger.error(bad_pkgs)\nif not ipkgs:\n    return\nrc = self.cmd.run(self.pkgcmd % (\" \".join(ipkgs)))[0]\nif rc:\n    self.logger.error(\"APT command failed\")\nself.pkg_cache = apt.cache.Cache()\nself.extra = self.FindExtra()\nfor package in packages:\n    states[package] = self.VerifyPackage(package, [], checksums=False)\n    if states[package]:\n        self.modified.append(package)", "path": "src\\lib\\Client\\Tools\\APT.py", "repo_name": "solj/bcfg2-old", "stars": 26, "license": "other", "language": "python", "size": 8612}
{"docstring": "\"\"\"Read Bazaar revision information for the Bcfg2 repository.\"\"\"\n", "func_signal": "def get_revision(self):\n", "code": "try:\n    working_tree = WorkingTree.open(self.datastore)\n    revision = str(working_tree.branch.revno())\n    if working_tree.has_changes(working_tree.basis_tree()) or working_tree.unknowns():\n        revision += \"+\"\nexcept errors.NotBranchError:\n    logger.error(\"Failed to read Bazaar branch; disabling Bazaar support\")\n    raise Bcfg2.Server.Plugin.PluginInitError\nreturn revision", "path": "src\\lib\\Server\\Plugins\\Bzr.py", "repo_name": "solj/bcfg2-old", "stars": 26, "license": "other", "language": "python", "size": 8612}
{"docstring": "\"\"\"Verify Package status for entry.\"\"\"\n", "func_signal": "def VerifyPackage(self, entry, modlist):\n", "code": "if not entry.get('version'):\n    self.logger.info(\"Insufficient information of Package %s; cannot Verify\" % entry.get('name'))\n    return False\n\ndesiredVersion = entry.get('version')\nif desiredVersion == 'any':\n    desiredVersion = self.installed.get(entry.get('name'), desiredVersion)\n\ncmdrc = self.cmd.run(\"/usr/bin/pkginfo -q -v \\\"%s\\\" %s\" % \\\n                     (desiredVersion, entry.get('name')))[0]\n\nif cmdrc != 0:\n    if entry.get('name') in self.installed:\n        self.logger.debug(\"Package %s version incorrect: have %s want %s\" \\\n                          % (entry.get('name'), self.installed[entry.get('name')],\n                             desiredVersion))\n    else:\n        self.logger.debug(\"Package %s not installed\" % (entry.get(\"name\")))\nelse:\n    if self.setup['quick'] or entry.attrib.get('verify', 'true') == 'false':\n        return True\n    (vstat, odata) = self.cmd.run(\"/usr/sbin/pkgchk -n %s\" % (entry.get('name')))\n    if vstat == 0:\n        return True\n    else:\n        output = [line for line in odata if line[:5] == 'ERROR']\n        if len([name for name in output if name.split()[-1] not in modlist]):\n            self.logger.debug(\"Package %s content verification failed\" % \\\n                              (entry.get('name')))\n        else:\n            return True\nreturn False", "path": "src\\lib\\Client\\Tools\\SYSV.py", "repo_name": "solj/bcfg2-old", "stars": 26, "license": "other", "language": "python", "size": 8612}
{"docstring": "\"\"\"Build root authorized keys file based on current ACLs.\"\"\"\n", "func_signal": "def gen_root_keys_cb(self, entry, metadata):\n", "code": "superusers = self.repository.entries['superusers'].data.split()\ntry:\n    rootlike = [line.split(':', 1) for line in \\\n                self.repository.entries['rootlike'].data.split()]\n    superusers += [user for (user, host) in rootlike \\\n                   if host == metadata.hostname.split('.')[0]]\nexcept:\n    pass\nrdata = self.repository.entries\nentry.text = \"\".join([rdata[\"%s.key\" % user].data for user \\\n                      in superusers if \\\n                      (\"%s.key\" % user) in rdata])\nperms = {'owner': 'root',\n         'group': 'root',\n         'perms': '0600'}\n[entry.attrib.__setitem__(key, value) for (key, value) \\\n in list(perms.items())]", "path": "src\\lib\\Server\\Plugins\\Account.py", "repo_name": "solj/bcfg2-old", "stars": 26, "license": "other", "language": "python", "size": 8612}
{"docstring": "# check that default value isn't cooked\n", "func_signal": "def test_cook(self):\n", "code": "o1 = Bcfg2.Options.Option('foo', 'test4', cook=Bcfg2.Options.bool_cook)\no1.parse([], [])\nassert o1.value == 'test4'\no2 = Bcfg2.Options.Option('foo', False, cmd='-F')\no2.parse([('-F', '')], [])\nassert o2.value == True", "path": "testsuite\\TestOptions.py", "repo_name": "solj/bcfg2-old", "stars": 26, "license": "other", "language": "python", "size": 8612}
{"docstring": "# Adding model 'Agg'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_table('ratings_agg', (\n            ('id', models.AutoField(primary_key=True)),\n            ('target_ct', models.ForeignKey(orm['contenttypes.ContentType'], db_index=True)),\n            ('target_id', models.PositiveIntegerField(_('Object ID'), db_index=True)),\n            ('time', models.DateField(_('Time'))),\n            ('people', models.IntegerField(_('People'))),\n            ('amount', models.DecimalField(_('Amount'), max_digits=10, decimal_places=2)),\n            ('period', models.CharField(_('Period'), max_length=\"1\")),\n            ('detract', models.IntegerField(_('Detract'), default=0, max_length=1)),\n        ))\n        db.send_create_signal('django_ratings', ['Agg'])\n# Adding model 'ModelWeight'\n        db.create_table('ratings_modelweight', (\n            ('id', models.AutoField(primary_key=True)),\n            ('content_type', models.OneToOneField(orm['contenttypes.ContentType'])),\n            ('weight', models.IntegerField(_('Weight'), default=1)),\n            ('owner_field', models.CharField(_('Owner field'), max_length=30)),\n        ))\n        db.send_create_signal('django_ratings', ['ModelWeight'])\n# Adding model 'TotalRate'\n        db.create_table('ratings_totalrate', (\n            ('id', models.AutoField(primary_key=True)),\n            ('target_ct', models.ForeignKey(orm['contenttypes.ContentType'], db_index=True)),\n            ('target_id', models.PositiveIntegerField(_('Object ID'), db_index=True)),\n            ('amount', models.DecimalField(_('Amount'), max_digits=10, decimal_places=2)),\n        ))\n        db.send_create_signal('django_ratings', ['TotalRate'])\n# Adding model 'Rating'\n        db.create_table('ratings_rating', (\n            ('id', models.AutoField(primary_key=True)),\n            ('target_ct', models.ForeignKey(orm['contenttypes.ContentType'], db_index=True)),\n            ('target_id', models.PositiveIntegerField(_('Object ID'), db_index=True)),\n            ('time', models.DateTimeField(_('Time'), default=datetime.now, editable=False)),\n            ('user', models.ForeignKey(orm['auth.User'], null=True, blank=True)),\n            ('amount', models.DecimalField(_('Amount'), max_digits=10, decimal_places=2)),\n            ('ip_address', models.CharField(_('IP Address'), max_length=\"15\", blank=True)),\n        ))\n        db.send_create_signal('django_ratings', ['Rating'])", "path": "django_ratings\\migrations\\0001_initial.py", "repo_name": "ella/django-ratings", "stars": 31, "license": "bsd-3-clause", "language": "python", "size": 153}
{"docstring": "\"\"\"\ntransfer data from table Rating to table Agg\n\"\"\"\n", "func_signal": "def transfer_data():\n", "code": "logger.info(\"transfer_data BEGIN\")\ntimenow = datetime.now()\nfor t in sorted(TIMES_ALL.keys(), reverse=True):\n    TIME_DELTA = t\n    time_agg = timenow - timedelta(seconds=TIME_DELTA)\n    Rating.objects.move_rate_to_agg(time_agg, TIMES_ALL[t])\ntransfer_agg_to_agg()\ntransfer_agg_to_totalrate()\nlogger.info(\"transfer_data END\")", "path": "django_ratings\\aggregation.py", "repo_name": "ella/django-ratings", "stars": 31, "license": "bsd-3-clause", "language": "python", "size": 153}
{"docstring": "\"\"\"\nReturns whether object was rated by current user\n\nRating can fail later on db query, this checks user cookies\n\"\"\"\n", "func_signal": "def get_was_rated(request, ct, target):\n", "code": "if isinstance(ct, ContentType):\n    ct = ct.id\nif isinstance(target, models.Model):\n    target = target.pk\nreturn '%s:%s' % (ct, target) in _get_cookie(request)", "path": "django_ratings\\views.py", "repo_name": "ella/django-ratings", "stars": 31, "license": "bsd-3-clause", "language": "python", "size": 153}
{"docstring": "\"\"\"\nCoppy aggregated Agg data to table Agg\n\ntime_limit: limit for time of transfering data\n\ntime_format: format for date_trunc\n\"\"\"\n", "func_signal": "def move_agg_to_agg(self, time_limit, time_format):\n", "code": "qn = connection.ops.quote_name\ndate_trunc = connection.ops.date_trunc_sql\n\nsql = '''INSERT INTO %(agg_table)s\n            (detract, period, people, amount, time, target_ct_id, target_id)\n         SELECT\n            1, %%s, SUM(people), SUM(amount), %(truncated_date)s, target_ct_id, target_id\n         FROM\n            %(agg_table)s\n         WHERE\n            time <= %%s AND detract = 0\n         GROUP BY\n            target_ct_id, target_id, %(truncated_date)s''' % {\n    'agg_table' : qn(Agg._meta.db_table),\n    'truncated_date': date_trunc(time_format, qn('time')),\n}\n\ncursor = connection.cursor()\ncursor.execute(sql, (time_format[0], time_limit,))\nself.filter(time__lte=time_limit, detract=0).delete()", "path": "django_ratings\\models.py", "repo_name": "ella/django-ratings", "stars": 31, "license": "bsd-3-clause", "language": "python", "size": 153}
{"docstring": "# Adding model 'UserKarma'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_table('django_ratings_userkarma', (\n            ('user', models.ForeignKey(orm['auth.User'], primary_key=True)),\n            ('karma', models.DecimalField(_('Karma'), max_digits=10, decimal_places=2)),\n        ))\n        db.send_create_signal('django_ratings', ['UserKarma'])", "path": "django_ratings\\migrations\\0003_added_user_karma.py", "repo_name": "ella/django-ratings", "stars": 31, "license": "bsd-3-clause", "language": "python", "size": 153}
{"docstring": "# Deleting model 'Agg'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.delete_table('ratings_agg')\n# Deleting model 'ModelWeight'\n        db.delete_table('ratings_modelweight')\n# Deleting model 'TotalRate'\n        db.delete_table('ratings_totalrate')\n# Deleting model 'Rating'\n        db.delete_table('ratings_rating')", "path": "django_ratings\\migrations\\0001_initial.py", "repo_name": "ella/django-ratings", "stars": 31, "license": "bsd-3-clause", "language": "python", "size": 153}
{"docstring": "\"\"\"\nMarks target as rated\n\nAdds object content_type and id to RATINGS_COOKIE_NAME cookie\n\"\"\"\n", "func_signal": "def set_was_rated(request, response, ct, target):\n", "code": "cook = _get_cookie(request)\nif len(cook) > RATINGS_MAX_COOKIE_LENGTH:\n    cook = cook[1:]\ncook.append('%s:%s' % (ct.id, target.id))\nexpires = datetime.strftime(datetime.utcnow() + timedelta(seconds=RATINGS_MAX_COOKIE_AGE), \"%a, %d-%b-%Y %H:%M:%S GMT\")\ndomain = settings.SESSION_COOKIE_DOMAIN\nresponse.set_cookie(RATINGS_COOKIE_NAME, value=','.join(cook),\n        max_age=RATINGS_MAX_COOKIE_AGE, expires=expires,  path='/',\n        domain=domain, secure=None)", "path": "django_ratings\\views.py", "repo_name": "ella/django-ratings", "stars": 31, "license": "bsd-3-clause", "language": "python", "size": 153}
{"docstring": "\"\"\"\nTransfer aggregation data from table Agg to table TotalRate\n\"\"\"\n", "func_signal": "def transfer_agg_to_totalrate():\n", "code": "logger.info(\"transfer_agg_to_totalrate BEGIN\")\nif TotalRate.objects.count() != 0:\n    TotalRate.objects.all().delete()\nAgg.objects.agg_to_totalrate()\nlogger.info(\"transfer_agg_to_totalrate END\")", "path": "django_ratings\\aggregation.py", "repo_name": "ella/django-ratings", "stars": 31, "license": "bsd-3-clause", "language": "python", "size": 153}
{"docstring": "\"\"\"\nReturns cookie split by ','\n\"\"\"\n", "func_signal": "def _get_cookie(request):\n", "code": "try:\n    return request.COOKIES[RATINGS_COOKIE_NAME].split(',')\nexcept KeyError:\n    # Cookie not set\n    return []", "path": "django_ratings\\views.py", "repo_name": "ella/django-ratings", "stars": 31, "license": "bsd-3-clause", "language": "python", "size": 153}
{"docstring": "\"\"\"\nModified save() method that checks for duplicit entries.\n\"\"\"\n", "func_signal": "def save(self, **kwargs):\n", "code": "if not self.pk:\n    # fail silently on inserting duplicate ratings\n    if self.user:\n        try:\n            Rating.objects.get(target_ct=self.target_ct, target_id=self.target_id, user=self.user)\n            return\n        except Rating.DoesNotExist:\n            pass\n    elif (self.ip_address and Rating.objects.filter(\n                target_ct=self.target_ct,\n                target_id=self.target_id,\n                user__isnull=True,\n                ip_address=self.ip_address ,\n                time__gte=(self.time or datetime.now()) - timedelta(seconds=MINIMAL_ANONYMOUS_IP_DELAY)\n            ).count() > 0):\n        return\n    # denormalize the total rate\n    cnt = TotalRate.objects.filter(target_ct=self.target_ct, target_id=self.target_id).update(amount=models.F('amount')+self.amount)\n    if cnt == 0:\n        tr = TotalRate.objects.create(target_ct=self.target_ct, target_id=self.target_id, amount=self.amount)\n\n\nsuper(Rating, self).save(**kwargs)", "path": "django_ratings\\models.py", "repo_name": "ella/django-ratings", "stars": 31, "license": "bsd-3-clause", "language": "python", "size": 153}
{"docstring": "\"\"\"\nReturns rating normalized from min to top rounded to step\n\n- no score (0) is always avarage (0)\n- worst score gets always min\n- best score gets always top\n- results between 0 and top should be uniformly distributed\n\"\"\"\n", "func_signal": "def get_normalized_rating(self, obj, top, step=None):\n", "code": "total = self.get_for_object(obj)\nif total == 0:\n    return Decimal(\"0\").quantize(step or 1)\n\n# Handle positive and negative score separately\nop_lt = \"lt\"\nop_gt = \"gt\"\nref = top\nif total < 0:\n    op_lt = \"gt\"\n    op_gt = \"lt\"\n    ref = -top\n\nct = ContentType.objects.get_for_model(obj)\nmore = self.filter(target_ct=ct, **{'amount__' + op_lt: total, 'amount__' + op_gt: 0 }).count()\ntotal = self.filter(target_ct=ct, **{'amount__' + op_gt: 0 }).count()\n\nif total == 0:\n    # First rating\n    percentil = Decimal(0)\nelse:\n    percentil = Decimal(more) / Decimal(total)\n\nresult = percentil * ref\nif step:\n    result = (result / step).quantize(1) * step\nresult = max(-top, result)\nresult = min(top, result)\nreturn result.quantize(1)", "path": "django_ratings\\models.py", "repo_name": "ella/django-ratings", "stars": 31, "license": "bsd-3-clause", "language": "python", "size": 153}
{"docstring": "\"\"\"\nCoppy aggregated Rating to table Agg\n\ntime_limit: limit for time of transfering data\n\ntime_format: format for date_trunc\n\"\"\"\n", "func_signal": "def move_rate_to_agg(self, time_limit, time_format):\n", "code": "qn = connection.ops.quote_name\ndate_trunc = connection.ops.date_trunc_sql\n\nsql = '''INSERT INTO %(agg_table)s\n            (detract, period, people, amount, time, target_ct_id, target_id)\n         SELECT\n            0, %%s, COUNT(*), SUM(amount), %(truncated_date)s, target_ct_id, target_id\n         FROM %(rating_table)s\n         WHERE time <= %%s\n         GROUP BY target_ct_id, target_id, %(truncated_date)s''' % {\n    'rating_table' : qn(Rating._meta.db_table),\n    'agg_table' : qn(Agg._meta.db_table),\n    'truncated_date': date_trunc(time_format, qn('time')),\n}\n\ncursor = connection.cursor()\ncursor.execute(sql, (time_format[0], time_limit,))\nself.filter(time__lte=time_limit).delete()", "path": "django_ratings\\models.py", "repo_name": "ella/django-ratings", "stars": 31, "license": "bsd-3-clause", "language": "python", "size": 153}
{"docstring": "\"\"\"\nReturn the agg rating for a given object.\n\nParams:\n        obj: object to work with\n\"\"\"\n", "func_signal": "def get_for_object(self, obj):\n", "code": "content_type = ContentType.objects.get_for_model(obj)\ntry:\n    return self.values('amount').get(target_ct=content_type, target_id=obj.pk)['amount']\nexcept self.model.DoesNotExist:\n    return 0", "path": "django_ratings\\models.py", "repo_name": "ella/django-ratings", "stars": 31, "license": "bsd-3-clause", "language": "python", "size": 153}
{"docstring": "\"\"\"\nTransfer aggregation data from table Agg to table TotalRate\n\"\"\"\n", "func_signal": "def agg_to_totalrate(self):\n", "code": "qn = connection.ops.quote_name\n\nsql = '''INSERT INTO %(tab_tr)s\n            (amount, target_ct_id, target_id)\n         SELECT\n            SUM(amount), target_ct_id, target_id\n         FROM\n            %(tab_agg)s\n         GROUP BY\n            target_ct_id, target_id''' % {\n    'tab_agg' : qn(Agg._meta.db_table),\n    'tab_tr' : qn(TotalRate._meta.db_table)\n}\n\ncursor = connection.cursor()\ncursor.execute(sql, ())", "path": "django_ratings\\models.py", "repo_name": "ella/django-ratings", "stars": 31, "license": "bsd-3-clause", "language": "python", "size": 153}
{"docstring": "\"\"\"\nReturn the rating for a given object.\n\nParams:\n    obj: object to work with\n\"\"\"\n", "func_signal": "def get_for_object(self, obj):\n", "code": "content_type = ContentType.objects.get_for_model(obj)\naggs = self.filter(target_ct=content_type, target_id=obj.pk).aggregate(amount_sum=models.Sum('amount'))['amount_sum']\nif aggs is None:\n    return 0\nreturn aggs", "path": "django_ratings\\models.py", "repo_name": "ella/django-ratings", "stars": 31, "license": "bsd-3-clause", "language": "python", "size": 153}
{"docstring": "\"\"\"\nView for ella custom urls\n\nExpects rating in POST\n\"\"\"\n# TODO: how to use django forms together with  content_type and objct from context\n", "func_signal": "def rate(request, bits, context):\n", "code": "try:\n    plusminus = Decimal(request.POST['rating'])\nexcept KeyError:\n    raise Http404\n# Allow only ratings in <-1;1> interval\nplusminus = plusminus.max(Decimal(\"-1\")).min(Decimal(\"1\"))\nreturn do_rate(\n    request,\n    context['content_type'],\n    context['object'],\n    plusminus\n)", "path": "django_ratings\\views.py", "repo_name": "ella/django-ratings", "stars": 31, "license": "bsd-3-clause", "language": "python", "size": 153}
{"docstring": "\"\"\"\naggregation data from table Agg to table Agg\n\"\"\"\n", "func_signal": "def transfer_agg_to_agg():\n", "code": "logger.info(\"transfer_agg_to_agg BEGIN\")\ntimenow = datetime.now()\nfor t in TIMES_ALL:\n    TIME_DELTA = t\n    time_agg = timenow - timedelta(seconds=TIME_DELTA)\n    Agg.objects.move_agg_to_agg(time_agg, TIMES_ALL[t])\nAgg.objects.agg_assume()\nlogger.info(\"transfer_agg_to_agg END\")", "path": "django_ratings\\aggregation.py", "repo_name": "ella/django-ratings", "stars": 31, "license": "bsd-3-clause", "language": "python", "size": 153}
{"docstring": "\"\"\"\nGet the owner of given model, return None if there is no owner or the\nmodel class is not registered.\n\"\"\"\n", "func_signal": "def get_owner(self, instance):\n", "code": "if instance.__class__ in self._registry:\n    owner_getter, weight = self._registry[instance.__class__]\n    return owner_getter(instance), weight\n\nreturn None", "path": "django_ratings\\karma.py", "repo_name": "ella/django-ratings", "stars": 31, "license": "bsd-3-clause", "language": "python", "size": 153}
{"docstring": "# Adding model 'ModelWeight'\n", "func_signal": "def backwards(self, orm):\n", "code": "db.create_table('ratings_modelweight', (\n    ('id', models.AutoField(primary_key=True)),\n    ('content_type', models.OneToOneField(orm['contenttypes.ContentType'])),\n    ('weight', models.IntegerField(_('Weight'), default=1)),\n    ('owner_field', models.CharField(_('Owner field'), max_length=30)),\n))\ndb.send_create_signal('django_ratings', ['ModelWeight'])", "path": "django_ratings\\migrations\\0002_removed_model_weight.py", "repo_name": "ella/django-ratings", "stars": 31, "license": "bsd-3-clause", "language": "python", "size": 153}
{"docstring": "\"\"\"\nReturn count objects with the highest rating.\n\nParams:\n    count: number of objects to return\n    mods: if specified, limit the result to given model classes\n\"\"\"\n", "func_signal": "def get_top_objects(self, count, mods=[]):\n", "code": "qset = self.order_by('-amount')\nkw = {}\nif mods:\n    kw['target_ct__in'] = [ContentType.objects.get_for_model(m).pk for m in mods]\nreturn [o.target for o in qset.filter(**kw)[:count]]", "path": "django_ratings\\models.py", "repo_name": "ella/django-ratings", "stars": 31, "license": "bsd-3-clause", "language": "python", "size": 153}
{"docstring": "\"\"\"List of required extensive properties within the simulator\"\"\"\n", "func_signal": "def GetReqExtensivePropertyNames():\n", "code": "global _reqExtProps\nreturn _reqExtProps", "path": "sim\\solver\\Variables.py", "repo_name": "jonathanxavier/sim42", "stars": 26, "license": "None", "language": "python", "size": 3693}
{"docstring": "\"\"\" return object (compound) matching number or name\"\"\"\n", "func_signal": "def GetObject(self, name):\n", "code": "try:\n    # if it is a number, use that\n    idx = int(name)\n    vals = self.GetValues()\n    return vals[idx]\nexcept:\n    try:\n        idx = self._cmpList.GetParent().GetCompoundNumber(name)\n        vals = self.GetValues()\n        return vals[idx]\n    except:\n        return None", "path": "sim\\solver\\Variables.py", "repo_name": "jonathanxavier/sim42", "stars": 26, "license": "None", "language": "python", "size": 3693}
{"docstring": "\"\"\"Load the type list with types and default units\"\"\"\n", "func_signal": "def LoadSetTypeList(self):\n", "code": "lb = self.GetSetTypeList()\nlb.Clear()\nset = self.set\n\n# map type name, unit id tuples from the set to types\ntypes = map(lambda x, u=self.units: (u.GetTypeName(x[0]),x[1]), set.items())\n#sort on the names\ntypes.sort(lambda x, y: cmp(x[0], y[0]))\nself.setTypeIDs = []\nfor type in types:\n    # get typeName and unit from the tuple\n    typeName = type[0]\n    unit = self.units.GetUnitWithID(type[1])\n    pad = 20 - len(typeName)\n    lb.Append(typeName + pad*' ' + unit.name)\n    self.setTypeIDs.append(unit.typeID)\n\nif self.typeID:\n    sel = self.setTypeIDs.index(self.typeID)\nelse:\n    sel = 0\n    self.typeID = self.setTypeIDs[0]\nlb.SetSelection(sel)\nself.LoadUseUnit(self.setTypeIDs[sel])", "path": "vmgunits\\unitmgr\\editset.py", "repo_name": "jonathanxavier/sim42", "stars": 26, "license": "None", "language": "python", "size": 3693}
{"docstring": "\"\"\"Only BasicProperties\"\"\"\n", "func_signal": "def __setitem__(self, key, item):\n", "code": "if not isinstance(item, BasicProperty): return\ndict.__setitem__(self, key, item)", "path": "sim\\solver\\Variables.py", "repo_name": "jonathanxavier/sim42", "stars": 26, "license": "None", "language": "python", "size": 3693}
{"docstring": "\"\"\"List of required array properties within the simulator\"\"\"\n", "func_signal": "def GetReqArrayPropertyNames():\n", "code": "global _reqArrayProps\nreturn _reqArrayProps", "path": "sim\\solver\\Variables.py", "repo_name": "jonathanxavier/sim42", "stars": 26, "license": "None", "language": "python", "size": 3693}
{"docstring": "\"\"\"Returns names of props with self.GetCalcStatus()==FIXED_V\"\"\"\n", "func_signal": "def GetNamesOfKnownFixedVars(self, type=None):\n", "code": "vars = []\nfor i in self.items():\n    if(i[1].GetCalcStatus() & FIXED_V and\n           i[1].GetValue() != None):\n        if type == None or i[1].GetType().calcType & type:\n            vars.append(i[0])\nreturn vars", "path": "sim\\solver\\Variables.py", "repo_name": "jonathanxavier/sim42", "stars": 26, "license": "None", "language": "python", "size": 3693}
{"docstring": "#Don't change anything if type is wrong\n", "func_signal": "def SetCalcType(self, calcType):\n", "code": "if (calcType == EXTENSIVE_PROP or calcType == INTENSIVE_PROP or calcType == INTENSIVE_PROP | CANFLASH_PROP):\n    self._type.calcType = calcType", "path": "sim\\solver\\Variables.py", "repo_name": "jonathanxavier/sim42", "stars": 26, "license": "None", "language": "python", "size": 3693}
{"docstring": "\"\"\"\nclean up before deleting\n\"\"\"\n", "func_signal": "def CleanUp(self):\n", "code": "self._myPort = None\nself._type = None", "path": "sim\\solver\\Variables.py", "repo_name": "jonathanxavier/sim42", "stars": 26, "license": "None", "language": "python", "size": 3693}
{"docstring": "\"\"\"return list units with type typeID\"\"\"\n", "func_signal": "def UnitsByType(self, typeID):\n", "code": "itemList = self.myUnitSystem.UnitsByType(typeID)\nreturn self.SetActiveUnitList(itemList)", "path": "vmgunits\\vmgUnitsCom.py", "repo_name": "jonathanxavier/sim42", "stars": 26, "license": "None", "language": "python", "size": 3693}
{"docstring": "# Given a numberic value in the default unit, returns a list\n# containing the same values in all different units\n", "func_signal": "def ValueList(self, typeId, value):\n", "code": "baseValue = self.ConvertFromDefaultToBase(typeId, value)\nresults = []\nunits = []\nfor unitSetName in self.myUnitSystem.GetSetNames():\n    us = self.myUnitSystem.GetUnitSet(unitSetName)\n    unitItem = self.myUnitSystem.GetUnit(us, typeId)\n    if not unitItem.name in units:\n        units.append(unitItem.name)\n        results.append(unitItem.ConvertFromBase(baseValue))\nreturn [results, units]", "path": "vmgunits\\vmgUnitsCom.py", "repo_name": "jonathanxavier/sim42", "stars": 26, "license": "None", "language": "python", "size": 3693}
{"docstring": "\"\"\"Init dictionary with basic and common properties.\"\"\"\n", "func_signal": "def __init__(self, dict=None, port=None):\n", "code": "dict.__init__(self, dict)\n\nfor i in GetReqIntensivePropertyNames():\n    self[i] = BasicProperty(i, port)\nfor i in GetReqExtensivePropertyNames():\n    self[i] = BasicProperty(i, port)", "path": "sim\\solver\\Variables.py", "repo_name": "jonathanxavier/sim42", "stars": 26, "license": "None", "language": "python", "size": 3693}
{"docstring": "\"\"\"\njust set parent\n\"\"\"\n", "func_signal": "def __init__(self, parent):\n", "code": "list.__init__(self)\nself._parent = parent", "path": "sim\\solver\\Variables.py", "repo_name": "jonathanxavier/sim42", "stars": 26, "license": "None", "language": "python", "size": 3693}
{"docstring": "\"\"\"\nset the scaleFactor, minValue and maxValue from values\nif not all values are supplied, the missing ones are not changed\ncalcstatus is ignored\n\"\"\"\n", "func_signal": "def SetValues(self, values, calcStatus):\n", "code": "try:\n    if values[0]: self.scaleFactor = float(values[0])\n    if values[1] != None and values[1] != 'None': self.minValue = float(values[1])\n    if values[2] != None and values[2] != 'None': self.maxValue = float(values[2])\nexcept IndexError:\n    pass", "path": "sim\\solver\\Variables.py", "repo_name": "jonathanxavier/sim42", "stars": 26, "license": "None", "language": "python", "size": 3693}
{"docstring": "\"\"\"\nfor name = Values return scaleFactor, minValue, maxValue\n\"\"\"\n", "func_signal": "def GetObject(self, name):\n", "code": "if name == 'Values':\n    return [self.scaleFactor, self.minValue, self.maxValue]", "path": "sim\\solver\\Variables.py", "repo_name": "jonathanxavier/sim42", "stars": 26, "license": "None", "language": "python", "size": 3693}
{"docstring": "\"\"\"The only value of the dictionary is varTypeName\"\"\"\n", "func_signal": "def __init__(self, dict=None, port=None, varTypeName=ENERGY_VAR):\n", "code": "dict.__init__(self, dict)\n\nself[varTypeName] = BasicProperty(varTypeName, port)", "path": "sim\\solver\\Variables.py", "repo_name": "jonathanxavier/sim42", "stars": 26, "license": "None", "language": "python", "size": 3693}
{"docstring": "\"\"\"used to assign a value to the property\"\"\"\n", "func_signal": "def SetValue(self, value, calcStatus=CALCULATED_V):\n", "code": "port = self._myPort\n\nif not port:\n    self._value = value\n    self._calcStatus = calcStatus\n    return\n     \nif calcStatus & FIXED_V:\n    \n    estimateAll = False\n    #For now, estimating anything will flip the state of the port\n    if calcStatus & ESTIMATED_V:\n        if port.state != Ports.ESTIMATEALL_STATE:\n            port.SetState(Ports.ESTIMATEALL_STATE)\n            estimateAll = True\n        \n    if port.state == Ports.ESTIMATEALL_STATE:\n        #Every spec will be forced as estimate\n        calcStatus |= ESTIMATED_V\n    elif port.state == Ports.FIXALL_STATE:\n        #Every spec is forced to be a spec\n        calcStatus &= ~ESTIMATED_V\n        \n    #Make sure it is a number\n    if value == None: calcStatus = UNKNOWN_V\n    else: value = float(value)\n    \n    #Same status, same value, just leave\n    if self._calcStatus == calcStatus and value == self._value:\n        return\n    \n    #If an estimate, then notify the port or else, clear the ESTIMATED_V bit\n    if calcStatus & ESTIMATED_V:\n        port.SetEstimated()\n    elif self._calcStatus & ESTIMATED_V:\n        self._calcStatus = self._calcStatus & ~ ESTIMATED_V\n        port.CheckEstimated()\n        \n    #Dirty hack for zero flow\n    if value == 0.0:\n        if self._type.name in (MOLEFLOW_VAR, MASSFLOW_VAR, VOLFLOW_VAR, STDVOLFLOW_VAR, STDGASVOLFLOW_VAR):\n            value = TINIEST_FLOW\n            \n    #Flag it as new\n    self._calcStatus = calcStatus | NEW_V\n    self._value = value\n    port.PropertyModified(self, calcStatus)\n    \n    if estimateAll:\n        port.AllPropsAsEstimates()\n    \nelif calcStatus == UNKNOWN_V:\n    if value != None:\n        raise SimError('SetValueUnknownNotNone')\n    if self._calcStatus & UNKNOWN_V:\n        return  # already unknown\n    \n    self._calcStatus = UNKNOWN_V | NEW_V\n    self._value = None\n    port.PropertyModified(self, UNKNOWN_V)\n\nelif calcStatus & (CALCULATED_V | PASSED_V):\n    # ignore attempts to calculate or pass unknown values\n    if value == None: return\n    # is there already a value? GetValue won't return new fixed values\n    isNew =  self._calcStatus & NEW_V\n    isFixed = self._calcStatus & FIXED_V\n    if self.GetValue() != None or (isNew and isFixed):\n        # is this a forget call?\n        if not self._myPort.GetParentOp().IsForgetting():\n            self.CheckTolerance(value)\n            # Just keep old value\n    else:\n        self._calcStatus = NEW_V | calcStatus\n        if value == 0.0:\n            if self._type.name in (MOLEFLOW_VAR, MASSFLOW_VAR, VOLFLOW_VAR, STDVOLFLOW_VAR, STDGASVOLFLOW_VAR):\n                value = TINIEST_FLOW\n        self._value = value\n        port.PropertyModified(self, calcStatus)\n    \nelse:\n    raise SimError('InvalidCalcStatusInSet')", "path": "sim\\solver\\Variables.py", "repo_name": "jonathanxavier/sim42", "stars": 26, "license": "None", "language": "python", "size": 3693}
{"docstring": "\"\"\"Returns names of props with self.GetCalcStatus()=!= UNKNOWN_V.\n\ntype -- filters by type if desired(i.e. intensive or extensive)\n\n\"\"\"\n", "func_signal": "def GetNamesOfKnownVars(self, type=None):\n", "code": "vars = []\nfor i in self.items():\n    if i[1].GetCalcStatus() != UNKNOWN_V:\n        if type == None or i[1].GetType().calcType & type:\n            vars.append(i[0])\nreturn vars", "path": "sim\\solver\\Variables.py", "repo_name": "jonathanxavier/sim42", "stars": 26, "license": "None", "language": "python", "size": 3693}
{"docstring": "\"\"\" return object (compound) matching number or name\"\"\"\n", "func_signal": "def GetObject(self, name):\n", "code": "try:\n    # if it is a number, use that\n    idx = int(name)\n    vals = self.GetValues()\n    return vals[idx]\nexcept:\n    try:\n        idx = self._cmpList.GetParent().GetCompoundNumber(name)\n        vals = self.GetValues()\n        return vals[idx]\n    except:\n        return None", "path": "sim\\solver\\Variables.py", "repo_name": "jonathanxavier/sim42", "stars": 26, "license": "None", "language": "python", "size": 3693}
{"docstring": "\"\"\"Sets the list of requiered intensive properties\"\"\"\n", "func_signal": "def SetReqExtensivePropertyNames(propList):\n", "code": "global _reqExtProps\ntry: _reqExtProps = tuple(propList)\nexcept: pass", "path": "sim\\solver\\Variables.py", "repo_name": "jonathanxavier/sim42", "stars": 26, "license": "None", "language": "python", "size": 3693}
{"docstring": "\"\"\"Return how many dimensions the value has (scalar, vector, or array)\"\"\"\n", "func_signal": "def GetRank(self):\n", "code": "if self._value == None:\n    return None\nelse:\n    return Numeric.rank(self._value)", "path": "sim\\solver\\Variables.py", "repo_name": "jonathanxavier/sim42", "stars": 26, "license": "None", "language": "python", "size": 3693}
{"docstring": "\"\"\"Here we backups master values in <name>_<ATTR_BACKUP_SUFFIX>\nand overrides the default fields with their translated values using\ninstance set_language.\n\"\"\"\n", "func_signal": "def change_fields(self, instance):\n", "code": "trans_opts = instance._translation_model._transmeta\n\n# backup master value on <name>_<suffix> attribute\napply(lambda name: setattr(instance,\n                           '_'.join((name, ATTR_BACKUP_SUFFIX)),\n                           getattr(instance, name, None)),\n       trans_opts.translatable_fields)\n\nlanguages = filter(None, getattr(instance,\n                                 CURRENT_LANGUAGES, '').split('_'))\nimplicit = self.lang\nif implicit and implicit in languages:\n    instance.switch_language(implicit) # switch to implicit language\nsetattr(instance, CURRENT_LANGUAGES, languages)\nreturn instance", "path": "model_i18n\\query.py", "repo_name": "gonz/django-model-i18n", "stars": 26, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\" Gets default project language from settings.TRANSLATIONS_DEFAULT_LANGUAGE\nif defined, or uses LANGUAGE_CODE\n\"\"\"\n", "func_signal": "def get_default_language():\n", "code": "return getattr(settings, 'TRANSLATIONS_DEFAULT_LANGUAGE', None) or \\\n       getattr(settings, 'LANGUAGE_CODE', None)", "path": "model_i18n\\utils.py", "repo_name": "gonz/django-model-i18n", "stars": 26, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\" Also clone custom_joins attribute (if any) when cloning a\nquery object \"\"\"\n", "func_signal": "def MP_clone(self, *args, **kwargs):\n", "code": "query = dj_clone(self, *args, **kwargs) # django\nif hasattr(self, 'custom_joins'):\n    query.custom_joins = self.custom_joins[:]\nreturn query", "path": "model_i18n\\patches.py", "repo_name": "gonz/django-model-i18n", "stars": 26, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\" Init method.\nArgs:\n    model: translatable model\n    lang: language desired\n\"\"\"\n", "func_signal": "def __init__(self, model, lang):\n", "code": "self.model = model\n\ntrans_model = model._translation_model\ntrans_opts = trans_model._transmeta\n\nalias = 'translation_%s' % lang\nself.data = { alias: lang }\n\n# Join data\nrelated_col  = trans_opts.master_field_name\ntrans_table  = trans_model._meta.db_table\ntrans_fk     = trans_model._meta.get_field(related_col).column\nmaster_table = model._meta.db_table\nmaster_pk    = model._meta.pk.column\n\nwhere = '%(m_table)s.%(m_pk)s = %(alias)s.%(t_fk)s %(and)s '\\\n        '%(alias)s.%(t_lang)s = \"%(lang)s\"' % {\n            'm_table': QN(master_table),\n            'm_pk':  QN(master_pk),\n            'and': AND,\n            'alias': alias,\n            't_fk': QN(trans_fk),\n            't_lang': QN(trans_opts.language_field_name),\n            'lang': lang }\nsuper(TransJoin, self).__init__(**{ alias: (trans_table, where) })", "path": "model_i18n\\query.py", "repo_name": "gonz/django-model-i18n", "stars": 26, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\" Build joins and add them to queries \"\"\"\n", "func_signal": "def add_to_query(self, query, used_aliases):\n", "code": "if self.joins:\n    if not hasattr(query, 'custom_joins'):\n        query.custom_joins = []\n    jtype = self.JOIN_TYPE\n    query.custom_joins += [\n        \" %s %s AS %s ON %s\" % (jtype, table, alias, where)\n            for alias, (table, where) in self.joins.iteritems()\n                if alias not in used_aliases ]", "path": "model_i18n\\query.py", "repo_name": "gonz/django-model-i18n", "stars": 26, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\" Returns application version \"\"\"\n", "func_signal": "def get_version():\n", "code": "version = '%s.%s' % (VERSION[0], VERSION[1])\nif VERSION[2]:\n    version = '%s.%s' % (version, VERSION[2])\nif VERSION[3:] == ('alpha', 0):\n    version = '%s pre-alpha' % version\nelse:\n    if VERSION[3] != 'final':\n        version = '%s %s %s' % (version, VERSION[3], VERSION[4])\nreturn version", "path": "model_i18n\\__init__.py", "repo_name": "gonz/django-model-i18n", "stars": 26, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\nPatch for master model's managers.\n    * model.objects.set_language: Sets the current language.\n    * model.objects.get_query_set: All querysets are TransQuerySet types\n\"\"\"\n# Backup get_query_set to use in translation get_query_set\n", "func_signal": "def setup_manager(self, manager):\n", "code": "manager.get_query_set_orig = manager.get_query_set\nfor method_name in ('get_query_set', 'set_language'):\n    # Add translation method into the manager instance\n    setattr(manager, method_name,\n        new.instancemethod(getattr(managers, method_name), manager, manager.__class__))", "path": "model_i18n\\translator.py", "repo_name": "gonz/django-model-i18n", "stars": 26, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\" _clone override, setups languages requested and current \nselected language\"\"\"\n", "func_signal": "def _clone(self, *args, **kwargs):\n", "code": "clone = super(TransQuerySet, self)._clone()\nclone.lang = self.lang\nclone.languages = self.languages\nreturn clone", "path": "model_i18n\\query.py", "repo_name": "gonz/django-model-i18n", "stars": 26, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\nSets everything up for the given master model using a set of\nregistration options (ModelTranslation attributes).\n  \nIf a translation class isn't given, it will use ModelTranslation (the\ndefault translation options). If keyword arguments are given -- e.g.,\nfields -- they'll overwrite the translation_class attributes.\n\"\"\"\n", "func_signal": "def register(self, master_model, translation_class=None, **options):\n", "code": "if master_model in self._registry:\n    raise AlreadyRegistered('The model \"%s\" has is already registered for translation' % master_model.__name__)\n  \n# If not translation_class given use default options.\nif not translation_class:\n    translation_class = ModelTranslation\n# If we got **options then dynamically construct a subclass of translation_class with those **options.\nif options:\n    translation_class = type('%sTranslation' % master_model.__name__, (translation_class,), options)\n\n# Validate the translation_class (just in debug mode).\nif settings.DEBUG:\n    from model_i18n.validation import validate\n    validate(translation_class, master_model)\n\nopts = translation_class(master_model)\n\n# Set up master_model as a multilingual model using translation_class options\ntranslation_model = self.create_translation_model(master_model, opts)\nmodels.register_models(master_model._meta.app_label, translation_model)\nself.setup_master_model(master_model, translation_model) # This probably will become a class method soon.\nsetup_admin(master_model, translation_model) # Setup django-admin support\n\n# Register the multilingual model and the used translation_class.\nself._registry[master_model] = opts", "path": "model_i18n\\translator.py", "repo_name": "gonz/django-model-i18n", "stars": 26, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\" Invokes QuerySet iterator method and tries to change instance\nattributes with translated values if any translation was retrieved\n\"\"\"\n", "func_signal": "def iterator(self):\n", "code": "for obj in super(TransQuerySet, self).iterator():\n    yield self.change_fields(obj)", "path": "model_i18n\\query.py", "repo_name": "gonz/django-model-i18n", "stars": 26, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Change view for i18n values for current instance. This is a\nsimplified django-admin change view which displays i18n fields\nfor current model/id.\"\"\"\n", "func_signal": "def i18n_change_view(instance, request, obj_id, language):\n", "code": "opts = instance.model._meta\nobj = instance.get_object(request, obj_id)\n\nif not instance.has_change_permission(request, obj):\n    raise PermissionDenied\n\nif obj is None:\n    msg = _('%(name)s object with primary key %(key)r does not exist.')\n    raise Http404(msg % {'name': force_unicode(opts.verbose_name),\n                         'key': escape(obj_id)})\n\nif language not in dict(settings.LANGUAGES):\n    raise Http404(_('Incorrect language %(lang)s') % {'lang': language})\n\nmaster_language = get_translation_opt(obj, 'master_language')\nif language == master_language:\n    # redirect to instance admin on default language\n    return HttpResponseRedirect('../')\n\nfields = get_translation_opt(obj, 'translatable_fields')\nlang_field = get_translation_opt(obj, 'language_field_name')\nmaster_field = get_translation_opt(obj, 'master_field_name')\n\ntry:\n    trans = obj.translations.get(**{lang_field: language})\nexcept obj._translation_model.DoesNotExist: # new translation\n    trans = obj._translation_model(**{lang_field: language,\n                                      master_field: obj})\n\nModelForm = modelform_factory(obj._translation_model, fields=fields)\n\nif request.method == 'POST':\n    form = ModelForm(instance=trans, data=request.POST,\n                     files=request.FILES)\n    if form.is_valid():\n        form.save()\n        return HttpResponseRedirect(request.path)\nelse:\n    form = ModelForm(instance=trans)\n\nadminform = admin.helpers.AdminForm(form, [(None, {'fields': fields})],\n                                    {}, None)\n\ncontext = {\n    'title': _('Translation %s') % force_unicode(opts.verbose_name),\n    'adminform': adminform, 'original': obj,\n    'is_popup': request.REQUEST.has_key('_popup'),\n    'errors': admin.helpers.AdminErrorList(form, None),\n    'root_path': instance.admin_site.root_path,\n    'app_label': opts.app_label, 'trans': True, 'lang': language,\n    'current_language': dict(settings.LANGUAGES)[language],\n    # override some values to provide an useful template\n    'add': False, 'change': True,\n    'has_change_permission_orig': True, # backup\n    'has_add_permission': False, 'has_change_permission': False,\n    'has_delete_permission': False, # hide delete link for now\n    'has_file_field': True, 'save_as': False, 'opts': instance.model._meta,\n}\n\nctx = RequestContext(request, current_app=instance.admin_site.name)\nreturn render_to_response(CHANGE_TRANSLATION_TPL, context,\n                          context_instance=ctx)", "path": "model_i18n\\admin.py", "repo_name": "gonz/django-model-i18n", "stars": 26, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\nSets up master model and its managers for working with translations:\n\nMaster model:\n    * master_model._translation_model: Translation model\n    * master_model.switch_language: language switcher\n\nManagers:\n    * See setup_manager\n\"\"\"\n# Master model\n", "func_signal": "def setup_master_model(self, master_model, translation_model):\n", "code": "master_model._translation_model = translation_model\nmaster_model.switch_language = switch_language\n# Managers\n# FIXME: We probably should we add a translation option to ignore some\n# manager (so users can create non multilingual managers)\n# XXX: Not sure what to do with _meta.abtract_managers\nfor c, fname, manager in master_model._meta.concrete_managers:\n    self.setup_manager(manager)", "path": "model_i18n\\translator.py", "repo_name": "gonz/django-model-i18n", "stars": 26, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\" AND operator, useful to request more than one language\nin a single query:\n    TransJoin(...) & TransJoin(...)\n\"\"\"\n", "func_signal": "def __and__(self, right):\n", "code": "if isinstance(right, TransJoin) and self.model == right.model:\n    self.data.update(right.data)\nreturn super(TransJoin, self).__and__(right)", "path": "model_i18n\\query.py", "repo_name": "gonz/django-model-i18n", "stars": 26, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\" Adds any non-master new languages in parameter to requested\nlanguages list (self.languages) and build the new query joins rules\n\nWe do not do anything if no new languages were passed\n\n`language` parameter will be set as implicit language (self.lang)\n           if passed\n\"\"\"\n", "func_signal": "def get_translations(self, languages, language=None):\n", "code": "if language and language not in languages:\n    languages.append(language)\n\n# filter added languages and master language\nmaster = get_master_language(self.model)\nnew = set((lang for lang in languages\n                if lang and lang != master)) - self.languages\n\nif language not in (self.lang, master): # set implicit language\n    self.lang = language\n\nif new: # if there's any language to add\n    rules = [ TransJoin(self.model, lang) for lang in new ]\n    join = reduce(operator.and_, rules) if len(rules) > 1 else rules[0]\n    self.languages |= new\n    return self.filter(join)\nreturn self", "path": "model_i18n\\query.py", "repo_name": "gonz/django-model-i18n", "stars": 26, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\nDelegates join to QOuterJoins and adds the needed fields to \nselect list. The translateable fields will be in the form:\n    <master model attribute name>_<language code>.\nAlso current_languages attribute will be added with translation\nlanguage codes joined by '_'.\n\"\"\"\n# resolve joins\n", "func_signal": "def add_to_query(self, query, used_aliases):\n", "code": "super(TransJoin, self).add_to_query(query, used_aliases)\ntrans_pk = QN(self.model._translation_model._meta.pk.column)\nfields = self.model._translation_model._transmeta.translatable_fields\n\n# add joined columns needed\nselect = {}\nfor alias, lang in self.data.iteritems():\n    alias = QN(alias)\n    select['id_%s' % lang] = '%s.%s' % (alias, trans_pk)\n    select.update(('%s_%s' % (name, lang),\n                   '%s.%s' % (alias, QN(name)))\n                        for name in fields)\nselect[CURRENT_LANGUAGES] = '\"%s\"' % '_'.join(self.data.itervalues())\nquery.add_extra(select, None, None, None, None, None)", "path": "model_i18n\\query.py", "repo_name": "gonz/django-model-i18n", "stars": 26, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\" AND operator. Useful to setup several joins rules \"\"\"\n", "func_signal": "def __and__(self, right):\n", "code": "if not isinstance(right, QOuterJoins):\n    return super(QOuterJoins, self).__and__(right)\nself.joins.update(right.joins)\nreturn self", "path": "model_i18n\\query.py", "repo_name": "gonz/django-model-i18n", "stars": 26, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"  Ensures the configuration module gets imported when importing model_i18n. \"\"\"\n# This is an idea from haystack app. We need to run the code that\n# follows only once, no matter how many times the main module is imported.\n# We'll look through the stack to see if we appear anywhere and simply\n# return if we do, allowing the original call to finish.\n", "func_signal": "def _load_conf(*args, **kwargs):\n", "code": "stack = inspect.stack()\nfor stack_info in stack[1:]:\n    if '_load_conf' in stack_info[3]:\n        return\n\nif not hasattr(settings, 'MODEL_I18N_CONF'):\n    raise ImproperlyConfigured('You must define the MODEL_I18N_CONF setting, it should be a python module path string, for example \"myproject.i18n_conf\"')\nif not hasattr(settings, 'MODEL_I18N_MASTER_LANGUAGE'):\n    raise ImproperlyConfigured('You must define the MODEL_I18N_MASTER_LANGUAGE setting.')\n\n# Import config module\nimport_module(settings.MODEL_I18N_CONF)", "path": "model_i18n\\__init__.py", "repo_name": "gonz/django-model-i18n", "stars": 26, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Here we overrides the default fields with their translated\nvalues. We keep the default if there's no value in the translated\nfield or more than one language was requested.\n    instance.switch_language('es')\n        will load attribute values for 'es' language\n    instance.switch_language()\n        will load attribute values for master default language\n\"\"\"\n", "func_signal": "def switch_language(instance, lang=None):\n", "code": "current_languages = getattr(instance, CURRENT_LANGUAGES, None)\ncurrent = getattr(instance, CURRENT_LANGUAGE, None)\n\nif current_languages: # any translation?\n    trans_meta = instance._translation_model._transmeta\n    fields = trans_meta.translatable_fields\n    if not lang or lang == trans_meta.master_language: # use defaults\n        for name in fields:\n            value = getattr(instance, '_'.join((name, ATTR_BACKUP_SUFFIX)),\n                            None)\n            setattr(instance, name, value)\n    elif lang in current_languages and lang != current: # swtich language\n        for name in fields:\n            value = getattr(instance, '_'.join((name, lang)), None)\n            if value is not None: # Ignore None, means not translated\n                setattr(instance, name, value)\n    setattr(instance, CURRENT_LANGUAGE, lang)", "path": "model_i18n\\translator.py", "repo_name": "gonz/django-model-i18n", "stars": 26, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Admin get_urls override to add i18n edition view. Last url is\nfor django-admin change view, it's a bit gredy, so we kept it back\nto the end.\"\"\"\n# original urls\n", "func_signal": "def get_urls(instance):\n", "code": "urls = instance.get_urls_orig()\nreturn urls[:-1] + patterns('', \n            url(r'^(?P<obj_id>\\d+)/(?P<language>[a-z]{2})/$',\n                instance.i18n_change_view),\n            urls[-1])", "path": "model_i18n\\admin.py", "repo_name": "gonz/django-model-i18n", "stars": 26, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"\nEach kwargs entry describes an LEFT OUTER join rule,\nkeys will be join aliases while values must be tuples\ncontaining table to join with and a where like clause\nwhich defines the join.\n\nkwargs = { \"join_alias\": (\"table\", \"clause\"),\n           ... }\n\"\"\"\n", "func_signal": "def __init__(self, **kwargs):\n", "code": "super(Q, self).__init__()\nself.joins = kwargs", "path": "model_i18n\\query.py", "repo_name": "gonz/django-model-i18n", "stars": 26, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Creates a new urllib request.\"\"\"\n\n", "func_signal": "def _CreateRequest(self, url, data=None, method=None):\n", "code": "if method == \"DELETE\":\n  req = DELETERequest(url, data=data)\nelif method == \"PUT\":\n  req = PUTRequest(url, data=data)\nelse:\n  req = urllib2.Request(url, data=data)\n\nif self.host_override:\n  req.add_header(\"Host\", self.host_override)\nfor key, value in self.extra_headers.iteritems():\n  req.add_header(key, value)\nreturn req", "path": "documents_service_support.py", "repo_name": "jessegrosjean/Documents.com.client.python", "stars": 26, "license": "None", "language": "python", "size": 126}
{"docstring": "\"\"\"Inits a diff_match_patch object with default settings.\nRedefine these in your program to override the defaults.\n\"\"\"\n\n# Number of seconds to map a diff before giving up (0 for infinity).\n", "func_signal": "def __init__(self):\n", "code": "self.Diff_Timeout = 1.0\n# Cost of an empty edit operation in terms of edit characters.\nself.Diff_EditCost = 4\n# The size beyond which the double-ended diff activates.\n# Double-ending is twice as fast, but less accurate.\nself.Diff_DualThreshold = 32\n# At what point is no match declared (0.0 = perfection, 1.0 = very loose).\nself.Match_Threshold = 0.5\n# How far to search for a match (0 = exact location, 1000+ = broad match).\n# A match this many characters away from the expected location will add\n# 1.0 to the score (0.0 is a perfect match).\nself.Match_Distance = 1000\n# Chunk size for context length.\nself.Patch_Margin = 4\n\n# How many bits in a number?\n# Python has no maximum, thus to disable patch splitting set to 0.\n# However to avoid long patches in certain pathological cases, use 32.\n# Multiple short patches (using native ints) are much faster than long ones.\nself.Match_MaxBits = 32", "path": "diff_match_patch.py", "repo_name": "jessegrosjean/Documents.com.client.python", "stars": 26, "license": "None", "language": "python", "size": 126}
{"docstring": "# Handle delete cases.\n", "func_signal": "def sync(self, controller):\n", "code": "if self.is_deleted_from_server:\n\tif self.has_local_edits():\n\t\tself.is_deleted_from_server = False\n\t\treturn self.POST(controller)\n\telse:\n\t\tcontroller.delete_document(self)\n\t\treturn None\nelif self.is_deleted_from_client:\n\tif self.has_server_edits():\n\t\tself.is_deleted_from_client = False\n\t\treturn self.GET(controller)\n\telse:\n\t\treturn self.DELETE(controller)\n\n# Handle create cases.\nif not self.is_server_document():\n\treturn self.POST(controller)\nelif self.is_inserted_from_server():\n\treturn self.GET(controller)\n\n# Handle changes.\nif self.has_local_edits():\n\treturn self.PUT(controller)\nelif self.has_server_edits():\n\treturn self.GET(controller)\n\nreturn None", "path": "documents_controller.py", "repo_name": "jessegrosjean/Documents.com.client.python", "stars": 26, "license": "None", "language": "python", "size": 126}
{"docstring": "# Note that this function is called from _speedups\n", "func_signal": "def errmsg(msg, doc, pos, end=None):\n", "code": "lineno, colno = linecol(doc, pos)\nif end is None:\n    #fmt = '{0}: line {1} column {2} (char {3})'\n    #return fmt.format(msg, lineno, colno, pos)\n    fmt = '%s: line %d column %d (char %d)'\n    return fmt % (msg, lineno, colno, pos)\nendlineno, endcolno = linecol(doc, end)\n#fmt = '{0}: line {1} column {2} - line {3} column {4} (char {5} - {6})'\n#return fmt.format(msg, lineno, colno, endlineno, endcolno, pos, end)\nfmt = '%s: line %d column %d - line %d column %d (char %d - %d)'\nreturn fmt % (msg, lineno, colno, endlineno, endcolno, pos, end)", "path": "simplejson\\decoder.py", "repo_name": "jessegrosjean/Documents.com.client.python", "stars": 26, "license": "None", "language": "python", "size": 126}
{"docstring": "\"\"\"Scan the string s for a JSON string. End is the index of the\ncharacter in s after the quote that started the JSON string.\nUnescapes all valid JSON string escape sequences and raises ValueError\non attempt to decode an invalid string. If strict is False then literal\ncontrol characters are allowed in the string.\n\nReturns a tuple of the decoded string and the index of the character in s\nafter the end quote.\"\"\"\n", "func_signal": "def py_scanstring(s, end, encoding=None, strict=True, _b=BACKSLASH, _m=STRINGCHUNK.match):\n", "code": "if encoding is None:\n    encoding = DEFAULT_ENCODING\nchunks = []\n_append = chunks.append\nbegin = end - 1\nwhile 1:\n    chunk = _m(s, end)\n    if chunk is None:\n        raise ValueError(\n            errmsg(\"Unterminated string starting at\", s, begin))\n    end = chunk.end()\n    content, terminator = chunk.groups()\n    # Content is contains zero or more unescaped string characters\n    if content:\n        if not isinstance(content, unicode):\n            content = unicode(content, encoding)\n        _append(content)\n    # Terminator is the end of string, a literal control character,\n    # or a backslash denoting that an escape sequence follows\n    if terminator == '\"':\n        break\n    elif terminator != '\\\\':\n        if strict:\n            msg = \"Invalid control character %r at\" % (terminator,)\n            #msg = \"Invalid control character {0!r} at\".format(terminator)\n            raise ValueError(errmsg(msg, s, end))\n        else:\n            _append(terminator)\n            continue\n    try:\n        esc = s[end]\n    except IndexError:\n        raise ValueError(\n            errmsg(\"Unterminated string starting at\", s, begin))\n    # If not a unicode escape sequence, must be in the lookup table\n    if esc != 'u':\n        try:\n            char = _b[esc]\n        except KeyError:\n            msg = \"Invalid \\\\escape: \" + repr(esc)\n            raise ValueError(errmsg(msg, s, end))\n        end += 1\n    else:\n        # Unicode escape sequence\n        esc = s[end + 1:end + 5]\n        next_end = end + 5\n        if len(esc) != 4:\n            msg = \"Invalid \\\\uXXXX escape\"\n            raise ValueError(errmsg(msg, s, end))\n        uni = int(esc, 16)\n        # Check for surrogate pair on UCS-4 systems\n        if 0xd800 <= uni <= 0xdbff and sys.maxunicode > 65535:\n            msg = \"Invalid \\\\uXXXX\\\\uXXXX surrogate pair\"\n            if not s[end + 5:end + 7] == '\\\\u':\n                raise ValueError(errmsg(msg, s, end))\n            esc2 = s[end + 7:end + 11]\n            if len(esc2) != 4:\n                raise ValueError(errmsg(msg, s, end))\n            uni2 = int(esc2, 16)\n            uni = 0x10000 + (((uni - 0xd800) << 10) | (uni2 - 0xdc00))\n            next_end += 6\n        char = unichr(uni)\n        end = next_end\n    # Append the unescaped character\n    _append(char)\nreturn u''.join(chunks), end", "path": "simplejson\\decoder.py", "repo_name": "jessegrosjean/Documents.com.client.python", "stars": 26, "license": "None", "language": "python", "size": 126}
{"docstring": "\"\"\"Compute and return the source text (all equalities and deletions).\n\nArgs:\n  diffs: Array of diff tuples.\n\nReturns:\n  Source text.\n\"\"\"\n", "func_signal": "def diff_text1(self, diffs):\n", "code": "text = []\nfor (op, data) in diffs:\n  if op != self.DIFF_INSERT:\n    text.append(data)\nreturn \"\".join(text)", "path": "diff_match_patch.py", "repo_name": "jessegrosjean/Documents.com.client.python", "stars": 26, "license": "None", "language": "python", "size": 126}
{"docstring": "\"\"\"Locate the best instance of 'pattern' in 'text' near 'loc' using the\nBitap algorithm.\n\nArgs:\n  text: The text to search.\n  pattern: The pattern to search for.\n  loc: The location to search around.\n\nReturns:\n  Best match index or -1.\n\"\"\"\n# Python doesn't have a maxint limit, so ignore this check.\n#if self.Match_MaxBits != 0 and len(pattern) > self.Match_MaxBits:\n#  raise ValueError(\"Pattern too long for this application.\")\n\n# Initialise the alphabet.\n", "func_signal": "def match_bitap(self, text, pattern, loc):\n", "code": "s = self.match_alphabet(pattern)\n\ndef match_bitapScore(e, x):\n  \"\"\"Compute and return the score for a match with e errors and x location.\n  Accesses loc and pattern through being a closure.\n\n  Args:\n    e: Number of errors in match.\n    x: Location of match.\n\n  Returns:\n    Overall score for match (0.0 = good, 1.0 = bad).\n  \"\"\"\n  accuracy = float(e) / len(pattern)\n  proximity = abs(loc - x)\n  if not self.Match_Distance:\n    # Dodge divide by zero error.\n    return proximity and 1.0 or accuracy\n  return accuracy + proximity / float(self.Match_Distance)\n\n# Highest score beyond which we give up.\nscore_threshold = self.Match_Threshold\n# Is there a nearby exact match? (speedup)\nbest_loc = text.find(pattern, loc)\nif best_loc != -1:\n  score_threshold = min(match_bitapScore(0, best_loc), score_threshold)\n# What about in the other direction? (speedup)\nbest_loc = text.rfind(pattern, loc + len(pattern))\nif best_loc != -1:\n  score_threshold = min(match_bitapScore(0, best_loc), score_threshold)\n\n# Initialise the bit arrays.\nmatchmask = 1 << (len(pattern) - 1)\nbest_loc = -1\n\nbin_max = len(pattern) + len(text)\n# Empty initialization added to appease pychecker.\nlast_rd = None\nfor d in xrange(len(pattern)):\n  # Scan for the best match each iteration allows for one more error.\n  # Run a binary search to determine how far from 'loc' we can stray at\n  # this error level.\n  bin_min = 0\n  bin_mid = bin_max\n  while bin_min < bin_mid:\n    if match_bitapScore(d, loc + bin_mid) <= score_threshold:\n      bin_min = bin_mid\n    else:\n      bin_max = bin_mid\n    bin_mid = (bin_max - bin_min) / 2 + bin_min\n\n  # Use the result from this iteration as the maximum for the next.\n  bin_max = bin_mid\n  start = max(1, loc - bin_mid + 1)\n  finish = min(loc + bin_mid, len(text)) + len(pattern)\n\n  rd = range(finish + 1)\n  rd.append((1 << d) - 1)\n  for j in xrange(finish, start - 1, -1):\n    if len(text) <= j - 1:\n      # Out of range.\n      charMatch = 0\n    else:\n      charMatch = s.get(text[j - 1], 0)\n    if d == 0:  # First pass: exact match.\n      rd[j] = ((rd[j + 1] << 1) | 1) & charMatch\n    else:  # Subsequent passes: fuzzy match.\n      rd[j] = ((rd[j + 1] << 1) | 1) & charMatch | (\n          ((last_rd[j + 1] | last_rd[j]) << 1) | 1) | last_rd[j + 1]\n    if rd[j] & matchmask:\n      score = match_bitapScore(d, j - 1)\n      # This match will almost certainly be better than any existing match.\n      # But check anyway.\n      if score <= score_threshold:\n        # Told you so.\n        score_threshold = score\n        best_loc = j - 1\n        if best_loc > loc:\n          # When passing loc, don't exceed our current distance from loc.\n          start = max(1, 2 * loc - best_loc)\n        else:\n          # Already passed loc, downhill from here on in.\n          break\n  # No hope for a (better) match at greater error levels.\n  if match_bitapScore(d + 1, loc) > score_threshold:\n    break\n  last_rd = rd\nreturn best_loc", "path": "diff_match_patch.py", "repo_name": "jessegrosjean/Documents.com.client.python", "stars": 26, "license": "None", "language": "python", "size": 126}
{"docstring": "\"\"\"Determine the common prefix of two strings.\n\nArgs:\n  text1: First string.\n  text2: Second string.\n\nReturns:\n  The number of characters common to the start of each string.\n\"\"\"\n# Quick check for common null cases.\n", "func_signal": "def diff_commonPrefix(self, text1, text2):\n", "code": "if not text1 or not text2 or text1[0] != text2[0]:\n  return 0\n# Binary search.\n# Performance analysis: http://neil.fraser.name/news/2007/10/09/\npointermin = 0\npointermax = min(len(text1), len(text2))\npointermid = pointermax\npointerstart = 0\nwhile pointermin < pointermid:\n  if text1[pointerstart:pointermid] == text2[pointerstart:pointermid]:\n    pointermin = pointermid\n    pointerstart = pointermin\n  else:\n    pointermax = pointermid\n  pointermid = int((pointermax - pointermin) / 2 + pointermin)\nreturn pointermid", "path": "diff_match_patch.py", "repo_name": "jessegrosjean/Documents.com.client.python", "stars": 26, "license": "None", "language": "python", "size": 126}
{"docstring": "\"\"\"Initializes with an empty list of diffs.\n\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self.diffs = []\nself.start1 = None\nself.start2 = None\nself.length1 = 0\nself.length2 = 0", "path": "diff_match_patch.py", "repo_name": "jessegrosjean/Documents.com.client.python", "stars": 26, "license": "None", "language": "python", "size": 126}
{"docstring": "\"\"\"Emmulate GNU diff's format.\nHeader: @@ -382,8 +481,9 @@\nIndicies are printed as 1-based, not 0-based.\n\nReturns:\n  The GNU diff string.\n\"\"\"\n", "func_signal": "def __str__(self):\n", "code": "if self.length1 == 0:\n  coords1 = str(self.start1) + \",0\"\nelif self.length1 == 1:\n  coords1 = str(self.start1 + 1)\nelse:\n  coords1 = str(self.start1 + 1) + \",\" + str(self.length1)\nif self.length2 == 0:\n  coords2 = str(self.start2) + \",0\"\nelif self.length2 == 1:\n  coords2 = str(self.start2 + 1)\nelse:\n  coords2 = str(self.start2 + 1) + \",\" + str(self.length2)\ntext = [\"@@ -\", coords1, \" +\", coords2, \" @@\\n\"]\n# Escape the body of the patch with %xx notation.\nfor (op, data) in self.diffs:\n  if op == diff_match_patch.DIFF_INSERT:\n    text.append(\"+\")\n  elif op == diff_match_patch.DIFF_DELETE:\n    text.append(\"-\")\n  elif op == diff_match_patch.DIFF_EQUAL:\n    text.append(\" \")\n  # High ascii will raise UnicodeDecodeError.  Use Unicode instead.\n  data = data.encode(\"utf-8\")\n  text.append(urllib.quote(data, \"!~*'();/?:@&=+$,# \") + \"\\n\")\nreturn \"\".join(text)", "path": "diff_match_patch.py", "repo_name": "jessegrosjean/Documents.com.client.python", "stars": 26, "license": "None", "language": "python", "size": 126}
{"docstring": "\"\"\"Return the Python representation of ``s`` (a ``str`` or ``unicode``\ninstance containing a JSON document)\n\n\"\"\"\n", "func_signal": "def decode(self, s, _w=WHITESPACE.match):\n", "code": "obj, end = self.raw_decode(s, idx=_w(s, 0).end())\nend = _w(s, end).end()\nif end != len(s):\n    raise ValueError(errmsg(\"Extra data\", s, end, len(s)))\nreturn obj", "path": "simplejson\\decoder.py", "repo_name": "jessegrosjean/Documents.com.client.python", "stars": 26, "license": "None", "language": "python", "size": 126}
{"docstring": "\"\"\"Save the cookie jar after authentication.\"\"\"\n", "func_signal": "def _Authenticate(self):\n", "code": "if cert_file_available and not uses_cert_verification:\n  logger.warn(\"ssl module not found. Without this the identity of the \"\n              \"remote host cannot be verified, and connections are NOT \"\n              \"secure. To fix this, please install the ssl module from \"\n              \"http://pypi.python.org/pypi/ssl\")\nsuper(HttpRpcServer, self)._Authenticate()\nif self.cookie_jar.filename is not None and self.save_cookies:\n  logger.info(\"Saving authentication cookies to %s\" %\n              self.cookie_jar.filename)\n  self.cookie_jar.save()", "path": "documents_service_support.py", "repo_name": "jessegrosjean/Documents.com.client.python", "stars": 26, "license": "None", "language": "python", "size": 126}
{"docstring": "\"\"\"Given an array of patches, return another array that is identical.\n\nArgs:\n  patches: Array of patch objects.\n\nReturns:\n  Array of patch objects.\n\"\"\"\n", "func_signal": "def patch_deepCopy(self, patches):\n", "code": "patchesCopy = []\nfor patch in patches:\n  patchCopy = patch_obj()\n  # No need to deep copy the tuples since they are immutable.\n  patchCopy.diffs = patch.diffs[:]\n  patchCopy.start1 = patch.start1\n  patchCopy.start2 = patch.start2\n  patchCopy.length1 = patch.length1\n  patchCopy.length2 = patch.length2\n  patchesCopy.append(patchCopy)\nreturn patchesCopy", "path": "diff_match_patch.py", "repo_name": "jessegrosjean/Documents.com.client.python", "stars": 26, "license": "None", "language": "python", "size": 126}
{"docstring": "# test in/out equivalence and parsing\n", "func_signal": "def test_parse(self):\n", "code": "res = json.loads(JSON)\nout = json.dumps(res)\nself.assertEquals(res, json.loads(out))", "path": "simplejson\\tests\\test_pass3.py", "repo_name": "jessegrosjean/Documents.com.client.python", "stars": 26, "license": "None", "language": "python", "size": 126}
{"docstring": "\"\"\"Work from the middle back to the start to determine the path.\n\nArgs:\n  v_map: Array of paths.\n  text1: Old string fragment to be diffed.\n  text2: New string fragment to be diffed.\n\nReturns:\n  Array of diff tuples.\n\"\"\"\n", "func_signal": "def diff_path1(self, v_map, text1, text2):\n", "code": "path = []\nx = len(text1)\ny = len(text2)\nlast_op = None\nfor d in xrange(len(v_map) - 2, -1, -1):\n  while True:\n    if (x - 1, y) in v_map[d]:\n      x -= 1\n      if last_op == self.DIFF_DELETE:\n        path[0] = (self.DIFF_DELETE, text1[x] + path[0][1])\n      else:\n        path[:0] = [(self.DIFF_DELETE, text1[x])]\n      last_op = self.DIFF_DELETE\n      break\n    elif (x, y - 1) in v_map[d]:\n      y -= 1\n      if last_op == self.DIFF_INSERT:\n        path[0] = (self.DIFF_INSERT, text2[y] + path[0][1])\n      else:\n        path[:0] = [(self.DIFF_INSERT, text2[y])]\n      last_op = self.DIFF_INSERT\n      break\n    else:\n      x -= 1\n      y -= 1\n      assert text1[x] == text2[y], (\"No diagonal.  \" +\n          \"Can't happen. (diff_path1)\")\n      if last_op == self.DIFF_EQUAL:\n        path[0] = (self.DIFF_EQUAL, text1[x] + path[0][1])\n      else:\n        path[:0] = [(self.DIFF_EQUAL, text1[x])]\n      last_op = self.DIFF_EQUAL\nreturn path", "path": "diff_match_patch.py", "repo_name": "jessegrosjean/Documents.com.client.python", "stars": 26, "license": "None", "language": "python", "size": 126}
{"docstring": "\"\"\"Find the differences between two texts.  Simplifies the problem by\n  stripping any common prefix or suffix off the texts before diffing.\n\nArgs:\n  text1: Old string to be diffed.\n  text2: New string to be diffed.\n  checklines: Optional speedup flag.  If present and false, then don't run\n    a line-level diff first to identify the changed areas.\n    Defaults to true, which does a faster, slightly less optimal diff.\n\nReturns:\n  Array of changes.\n\"\"\"\n\n# Check for equality (speedup)\n", "func_signal": "def diff_main(self, text1, text2, checklines=True):\n", "code": "if text1 == text2:\n  return [(self.DIFF_EQUAL, text1)]\n\n# Trim off common prefix (speedup)\ncommonlength = self.diff_commonPrefix(text1, text2)\ncommonprefix = text1[:commonlength]\ntext1 = text1[commonlength:]\ntext2 = text2[commonlength:]\n\n# Trim off common suffix (speedup)\ncommonlength = self.diff_commonSuffix(text1, text2)\nif commonlength == 0:\n  commonsuffix = ''\nelse:\n  commonsuffix = text1[-commonlength:]\n  text1 = text1[:-commonlength]\n  text2 = text2[:-commonlength]\n\n# Compute the diff on the middle block\ndiffs = self.diff_compute(text1, text2, checklines)\n\n# Restore the prefix and suffix\nif commonprefix:\n  diffs[:0] = [(self.DIFF_EQUAL, commonprefix)]\nif commonsuffix:\n  diffs.append((self.DIFF_EQUAL, commonsuffix))\nself.diff_cleanupMerge(diffs)\nreturn diffs", "path": "diff_match_patch.py", "repo_name": "jessegrosjean/Documents.com.client.python", "stars": 26, "license": "None", "language": "python", "size": 126}
{"docstring": "# Several optimizations were made that skip over calls to\n# the whitespace regex, so this test is designed to try and\n# exercise the uncommon cases. The array cases are already covered.\n", "func_signal": "def test_decoder_optimizations(self):\n", "code": "rval = json.loads('{   \"key\"    :    \"value\"    ,  \"k\":\"v\"    }')\nself.assertEquals(rval, {\"key\":\"value\", \"k\":\"v\"})", "path": "simplejson\\tests\\test_decode.py", "repo_name": "jessegrosjean/Documents.com.client.python", "stars": 26, "license": "None", "language": "python", "size": 126}
{"docstring": "\"\"\"Work from the middle back to the end to determine the path.\n\nArgs:\n  v_map: Array of paths.\n  text1: Old string fragment to be diffed.\n  text2: New string fragment to be diffed.\n\nReturns:\n  Array of diff tuples.\n\"\"\"\n", "func_signal": "def diff_path2(self, v_map, text1, text2):\n", "code": "path = []\nx = len(text1)\ny = len(text2)\nlast_op = None\nfor d in xrange(len(v_map) - 2, -1, -1):\n  while True:\n    if (x - 1, y) in v_map[d]:\n      x -= 1\n      if last_op == self.DIFF_DELETE:\n        path[-1] = (self.DIFF_DELETE, path[-1][1] + text1[-x - 1])\n      else:\n        path.append((self.DIFF_DELETE, text1[-x - 1]))\n      last_op = self.DIFF_DELETE\n      break\n    elif (x, y - 1) in v_map[d]:\n      y -= 1\n      if last_op == self.DIFF_INSERT:\n        path[-1] = (self.DIFF_INSERT, path[-1][1] + text2[-y - 1])\n      else:\n        path.append((self.DIFF_INSERT, text2[-y - 1]))\n      last_op = self.DIFF_INSERT\n      break\n    else:\n      x -= 1\n      y -= 1\n      assert text1[-x - 1] == text2[-y - 1], (\"No diagonal.  \" +\n          \"Can't happen. (diff_path2)\")\n      if last_op == self.DIFF_EQUAL:\n        path[-1] = (self.DIFF_EQUAL, path[-1][1] + text1[-x - 1])\n      else:\n        path.append((self.DIFF_EQUAL, text1[-x - 1]))\n      last_op = self.DIFF_EQUAL\nreturn path", "path": "diff_match_patch.py", "repo_name": "jessegrosjean/Documents.com.client.python", "stars": 26, "license": "None", "language": "python", "size": 126}
{"docstring": "\"\"\"Take a list of patches and return a textual representation.\n\nArgs:\n  patches: Array of patch objects.\n\nReturns:\n  Text representation of patches.\n\"\"\"\n", "func_signal": "def patch_toText(self, patches):\n", "code": "text = []\nfor patch in patches:\n  text.append(str(patch))\nreturn \"\".join(text)", "path": "diff_match_patch.py", "repo_name": "jessegrosjean/Documents.com.client.python", "stars": 26, "license": "None", "language": "python", "size": 126}
{"docstring": "\"\"\"Merge a set of patches onto the text.  Return a patched text, as well\nas a list of true/false values indicating which patches were applied.\n\nArgs:\n  patches: Array of patch objects.\n  text: Old text.\n\nReturns:\n  Two element Array, containing the new text and an array of boolean values.\n\"\"\"\n", "func_signal": "def patch_apply(self, patches, text):\n", "code": "if not patches:\n  return (text, [], [])\n\n# Deep copy the patches so that no changes are made to originals.\npatches = self.patch_deepCopy(patches)\n\nnullPadding = self.patch_addPadding(patches)\ntext = nullPadding + text + nullPadding\nself.patch_splitMax(patches)\n\n# delta keeps track of the offset between the expected and actual location\n# of the previous patch.  If there are patches expected at positions 10 and\n# 20, but the first patch was found at 12, delta is 2 and the second patch\n# has an effective expected position of 22.\ndelta = 0\nresults = []\nresults_patches = []\nfor patch in patches:\n  expected_loc = patch.start2 + delta\n  text1 = self.diff_text1(patch.diffs)\n  start_loc = self.match_main(text, text1, expected_loc)\n  if start_loc == -1:\n    # No match found.  :(\n    results.append(False)\n    results_patches.append(patch)\n  else:\n    # Found a match.  :)\n    results.append(True)\n    results_patches.append(patch)\n    delta = start_loc - expected_loc\n    text2 = text[start_loc : start_loc + len(text1)]\n    if text1 == text2:\n      # Perfect match, just shove the replacement text in.\n      text = (text[:start_loc] + self.diff_text2(patch.diffs) +\n                  text[start_loc + len(text1):])\n    else:\n      # Imperfect match.\n      # Run a diff to get a framework of equivalent indicies.\n      diffs = self.diff_main(text1, text2, False)\n      self.diff_cleanupSemanticLossless(diffs)\n      index1 = 0\n      for (op, data) in patch.diffs:\n        if op != self.DIFF_EQUAL:\n          index2 = self.diff_xIndex(diffs, index1)\n        if op == self.DIFF_INSERT:  # Insertion\n          text = text[:start_loc + index2] + data + text[start_loc +\n                                                         index2:]\n        elif op == self.DIFF_DELETE:  # Deletion\n          text = text[:start_loc + index2] + text[start_loc +\n              self.diff_xIndex(diffs, index1 + len(data)):]\n        if op != self.DIFF_DELETE:\n          index1 += len(data)\n# Strip the padding off.\ntext = text[len(nullPadding):-len(nullPadding)]\nreturn (text, results, results_patches)", "path": "diff_match_patch.py", "repo_name": "jessegrosjean/Documents.com.client.python", "stars": 26, "license": "None", "language": "python", "size": 126}
{"docstring": "# called for each start tag\n# attrs is a list of (attr, value) tuples\n# e.g. for <pre class='screen'>, tag='pre', attrs=[('class', 'screen')]\n", "func_signal": "def unknown_starttag(self, tag, attrs):\n", "code": "if _debug: sys.stderr.write('_BaseHTMLProcessor, unknown_starttag, tag=%s\\n' % tag)\nuattrs = []\n# thanks to Kevin Marks for this breathtaking hack to deal with (valid) high-bit attribute values in UTF-8 feeds\nfor key, value in attrs:\n    if type(value) != type(u''):\n        value = unicode(value, self.encoding)\n    uattrs.append((unicode(key, self.encoding), value))\nstrattrs = u''.join([u' %s=\"%s\"' % (key, value) for key, value in uattrs]).encode(self.encoding)\nif tag in self.elements_no_end_tag:\n    self.pieces.append('<%(tag)s%(strattrs)s />' % locals())\nelse:\n    self.pieces.append('<%(tag)s%(strattrs)s>' % locals())", "path": "improviser\\qtGUI\\feedparser.py", "repo_name": "bspaans/improviser", "stars": 25, "license": "None", "language": "python", "size": 258}
{"docstring": "'''Parse a string according to the OnBlog 8-bit date format'''\n", "func_signal": "def _parse_date_onblog(dateString):\n", "code": "m = _korean_onblog_date_re.match(dateString)\nif not m: return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': m.group(4), 'minute': m.group(5), 'second': m.group(6),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('OnBlog date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "improviser\\qtGUI\\feedparser.py", "repo_name": "bspaans/improviser", "stars": 25, "license": "None", "language": "python", "size": 258}
{"docstring": "'''Parse an RFC822, RFC1123, RFC2822, or asctime-style date'''\n", "func_signal": "def _parse_date_rfc822(dateString):\n", "code": "data = dateString.split()\nif data[0][-1] in (',', '.') or data[0].lower() in rfc822._daynames:\n    del data[0]\nif len(data) == 4:\n    s = data[3]\n    i = s.find('+')\n    if i > 0:\n        data[3:] = [s[:i], s[i+1:]]\n    else:\n        data.append('')\n    dateString = \" \".join(data)\nif len(data) < 5:\n    dateString += ' 00:00:00 GMT'\ntm = rfc822.parsedate_tz(dateString)\nif tm:\n    return time.gmtime(rfc822.mktime_tz(tm))", "path": "improviser\\qtGUI\\feedparser.py", "repo_name": "bspaans/improviser", "stars": 25, "license": "None", "language": "python", "size": 258}
{"docstring": "'''Strips DOCTYPE from XML document, returns (rss_version, stripped_data)\n\nrss_version may be 'rss091n' or None\nstripped_data is the same XML document, minus the DOCTYPE\n'''\n", "func_signal": "def _stripDoctype(data):\n", "code": "entity_pattern = re.compile(r'<!ENTITY([^>]*?)>', re.MULTILINE)\ndata = entity_pattern.sub('', data)\ndoctype_pattern = re.compile(r'<!DOCTYPE([^>]*?)>', re.MULTILINE)\ndoctype_results = doctype_pattern.findall(data)\ndoctype = doctype_results and doctype_results[0] or ''\nif doctype.lower().count('netscape'):\n    version = 'rss091n'\nelse:\n    version = None\ndata = doctype_pattern.sub('', data)\nreturn version, data", "path": "improviser\\qtGUI\\feedparser.py", "repo_name": "bspaans/improviser", "stars": 25, "license": "None", "language": "python", "size": 258}
{"docstring": "'''Return the Time Zone Designator as an offset in seconds from UTC.'''\n", "func_signal": "def __extract_tzd(m):\n", "code": "if not m:\n    return 0\ntzd = m.group('tzd')\nif not tzd:\n    return 0\nif tzd == 'Z':\n    return 0\nhours = int(m.group('tzdhours'))\nminutes = m.group('tzdminutes')\nif minutes:\n    minutes = int(minutes)\nelse:\n    minutes = 0\noffset = (hours*60 + minutes) * 60\nif tzd[0] == '+':\n    return -offset\nreturn offset", "path": "improviser\\qtGUI\\feedparser.py", "repo_name": "bspaans/improviser", "stars": 25, "license": "None", "language": "python", "size": 258}
{"docstring": "# utility method to be called by descendants\n", "func_signal": "def normalize_attrs(self, attrs):\n", "code": "attrs = [(k.lower(), v) for k, v in attrs]\nattrs = [(k, k in ('rel', 'type') and v.lower() or v) for k, v in attrs]\nreturn attrs", "path": "improviser\\qtGUI\\feedparser.py", "repo_name": "bspaans/improviser", "stars": 25, "license": "None", "language": "python", "size": 258}
{"docstring": "'''Parse a string according to a Greek 8-bit date format.'''\n", "func_signal": "def _parse_date_greek(dateString):\n", "code": "m = _greek_date_format_re.match(dateString)\nif not m: return\ntry:\n    wday = _greek_wdays[m.group(1)]\n    month = _greek_months[m.group(3)]\nexcept:\n    return\nrfc822date = '%(wday)s, %(day)s %(month)s %(year)s %(hour)s:%(minute)s:%(second)s %(zonediff)s' % \\\n             {'wday': wday, 'day': m.group(2), 'month': month, 'year': m.group(4),\\\n              'hour': m.group(5), 'minute': m.group(6), 'second': m.group(7),\\\n              'zonediff': m.group(8)}\nif _debug: sys.stderr.write('Greek date parsed as: %s\\n' % rfc822date)\nreturn _parse_date_rfc822(rfc822date)", "path": "improviser\\qtGUI\\feedparser.py", "repo_name": "bspaans/improviser", "stars": 25, "license": "None", "language": "python", "size": 258}
{"docstring": "# called for each block of plain text, i.e. outside of any tag and\n# not containing any character or entity references\n", "func_signal": "def handle_data(self, text, escape=1):\n", "code": "if not self.elementstack: return\nif escape and self.contentparams.get('type') == 'application/xhtml+xml':\n    text = _xmlescape(text)\nself.elementstack[-1][2].append(text)", "path": "improviser\\qtGUI\\feedparser.py", "repo_name": "bspaans/improviser", "stars": 25, "license": "None", "language": "python", "size": 258}
{"docstring": "'''Parse a variety of ISO-8601-compatible formats like 20040105'''\n", "func_signal": "def _parse_date_iso8601(dateString):\n", "code": "m = None\nfor _iso8601_match in _iso8601_matches:\n    m = _iso8601_match(dateString)\n    if m: break\nif not m: return\nif m.span() == (0, 0): return\nparams = m.groupdict()\nordinal = params.get('ordinal', 0)\nif ordinal:\n    ordinal = int(ordinal)\nelse:\n    ordinal = 0\nyear = params.get('year', '--')\nif not year or year == '--':\n    year = time.gmtime()[0]\nelif len(year) == 2:\n    # ISO 8601 assumes current century, i.e. 93 -> 2093, NOT 1993\n    year = 100 * int(time.gmtime()[0] / 100) + int(year)\nelse:\n    year = int(year)\nmonth = params.get('month', '-')\nif not month or month == '-':\n    # ordinals are NOT normalized by mktime, we simulate them\n    # by setting month=1, day=ordinal\n    if ordinal:\n        month = 1\n    else:\n        month = time.gmtime()[1]\nmonth = int(month)\nday = params.get('day', 0)\nif not day:\n    # see above\n    if ordinal:\n        day = ordinal\n    elif params.get('century', 0) or \\\n             params.get('year', 0) or params.get('month', 0):\n        day = 1\n    else:\n        day = time.gmtime()[2]\nelse:\n    day = int(day)\n# special case of the century - is the first year of the 21st century\n# 2000 or 2001 ? The debate goes on...\nif 'century' in params.keys():\n    year = (int(params['century']) - 1) * 100 + 1\n# in ISO 8601 most fields are optional\nfor field in ['hour', 'minute', 'second', 'tzhour', 'tzmin']:\n    if not params.get(field, None):\n        params[field] = 0\nhour = int(params.get('hour', 0))\nminute = int(params.get('minute', 0))\nsecond = int(params.get('second', 0))\n# weekday is normalized by mktime(), we can ignore it\nweekday = 0\n# daylight savings is complex, but not needed for feedparser's purposes\n# as time zones, if specified, include mention of whether it is active\n# (e.g. PST vs. PDT, CET). Using -1 is implementation-dependent and\n# and most implementations have DST bugs\ndaylight_savings_flag = 0\ntm = [year, month, day, hour, minute, second, weekday,\n      ordinal, daylight_savings_flag]\n# ISO 8601 time zone adjustments\ntz = params.get('tz')\nif tz and tz != 'Z':\n    if tz[0] == '-':\n        tm[3] += int(params.get('tzhour', 0))\n        tm[4] += int(params.get('tzmin', 0))\n    elif tz[0] == '+':\n        tm[3] -= int(params.get('tzhour', 0))\n        tm[4] -= int(params.get('tzmin', 0))\n    else:\n        return None\n# Python's time.mktime() is a wrapper around the ANSI C mktime(3c)\n# which is guaranteed to normalize d/m/y/h/m/s.\n# Many implementations have bugs, but we'll pretend they don't.\nreturn time.localtime(time.mktime(tm))", "path": "improviser\\qtGUI\\feedparser.py", "repo_name": "bspaans/improviser", "stars": 25, "license": "None", "language": "python", "size": 258}
{"docstring": "# called for each entity reference, e.g. for '&copy;', ref will be 'copy'\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "if not self.elementstack: return\nif _debug: sys.stderr.write('entering handle_entityref with %s\\n' % ref)\nif ref in ('lt', 'gt', 'quot', 'amp', 'apos'):\n    text = '&%s;' % ref\nelse:\n    # entity resolution graciously donated by Aaron Swartz\n    def name2cp(k):\n        import htmlentitydefs\n        if hasattr(htmlentitydefs, 'name2codepoint'): # requires Python 2.3\n            return htmlentitydefs.name2codepoint[k]\n        k = htmlentitydefs.entitydefs[k]\n        if k.startswith('&#') and k.endswith(';'):\n            return int(k[2:-1]) # not in latin-1\n        return ord(k)\n    try: name2cp(ref)\n    except KeyError: text = '&%s;' % ref\n    else: text = unichr(name2cp(ref)).encode('utf-8')\nself.elementstack[-1][2].append(text)", "path": "improviser\\qtGUI\\feedparser.py", "repo_name": "bspaans/improviser", "stars": 25, "license": "None", "language": "python", "size": 258}
{"docstring": "'''Get the character encoding of the XML document\n\nhttp_headers is a dictionary\nxml_data is a raw string (not Unicode)\n\nThis is so much trickier than it sounds, it's not even funny.\nAccording to RFC 3023 ('XML Media Types'), if the HTTP Content-Type\nis application/xml, application/*+xml,\napplication/xml-external-parsed-entity, or application/xml-dtd,\nthe encoding given in the charset parameter of the HTTP Content-Type\ntakes precedence over the encoding given in the XML prefix within the\ndocument, and defaults to 'utf-8' if neither are specified.  But, if\nthe HTTP Content-Type is text/xml, text/*+xml, or\ntext/xml-external-parsed-entity, the encoding given in the XML prefix\nwithin the document is ALWAYS IGNORED and only the encoding given in\nthe charset parameter of the HTTP Content-Type header should be\nrespected, and it defaults to 'us-ascii' if not specified.\n\nFurthermore, discussion on the atom-syntax mailing list with the\nauthor of RFC 3023 leads me to the conclusion that any document\nserved with a Content-Type of text/* and no charset parameter\nmust be treated as us-ascii.  (We now do this.)  And also that it\nmust always be flagged as non-well-formed.  (We now do this too.)\n\nIf Content-Type is unspecified (input was local file or non-HTTP source)\nor unrecognized (server just got it totally wrong), then go by the\nencoding given in the XML prefix of the document and default to\n'iso-8859-1' as per the HTTP specification (RFC 2616).\n\nThen, assuming we didn't find a character encoding in the HTTP headers\n(and the HTTP Content-type allowed us to look in the body), we need\nto sniff the first few bytes of the XML data and try to determine\nwhether the encoding is ASCII-compatible.  Section F of the XML\nspecification shows the way here:\nhttp://www.w3.org/TR/REC-xml/#sec-guessing-no-ext-info\n\nIf the sniffed encoding is not ASCII-compatible, we need to make it\nASCII compatible so that we can sniff further into the XML declaration\nto find the encoding attribute, which will tell us the true encoding.\n\nOf course, none of this guarantees that we will be able to parse the\nfeed in the declared character encoding (assuming it was declared\ncorrectly, which many are not).  CJKCodecs and iconv_codec help a lot;\nyou should definitely install them if you can.\nhttp://cjkpython.i18n.org/\n'''\n\n", "func_signal": "def _getCharacterEncoding(http_headers, xml_data):\n", "code": "def _parseHTTPContentType(content_type):\n    '''takes HTTP Content-Type header and returns (content type, charset)\n\n    If no charset is specified, returns (content type, '')\n    If no content type is specified, returns ('', '')\n    Both return parameters are guaranteed to be lowercase strings\n    '''\n    content_type = content_type or ''\n    content_type, params = cgi.parse_header(content_type)\n    return content_type, params.get('charset', '').replace(\"'\", '')\n\nsniffed_xml_encoding = ''\nxml_encoding = ''\ntrue_encoding = ''\nhttp_content_type, http_encoding = _parseHTTPContentType(http_headers.get('content-type'))\n# Must sniff for non-ASCII-compatible character encodings before\n# searching for XML declaration.  This heuristic is defined in\n# section F of the XML specification:\n# http://www.w3.org/TR/REC-xml/#sec-guessing-no-ext-info\ntry:\n    if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n        # EBCDIC\n        xml_data = _ebcdic_to_ascii(xml_data)\n    elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n        # UTF-16BE\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16BE with BOM\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n        # UTF-16LE\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16LE with BOM\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n        # UTF-32BE\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n        # UTF-32LE\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n        # UTF-32BE with BOM\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n        # UTF-32LE with BOM\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n    elif xml_data[:3] == '\\xef\\xbb\\xbf':\n        # UTF-8 with BOM\n        sniffed_xml_encoding = 'utf-8'\n        xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n    else:\n        # ASCII-compatible\n        pass\n    xml_encoding_match = re.compile('^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>').match(xml_data)\nexcept:\n    xml_encoding_match = None\nif xml_encoding_match:\n    xml_encoding = xml_encoding_match.groups()[0].lower()\n    if sniffed_xml_encoding and (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode', 'iso-10646-ucs-4', 'ucs-4', 'csucs4', 'utf-16', 'utf-32', 'utf_16', 'utf_32', 'utf16', 'u16')):\n        xml_encoding = sniffed_xml_encoding\nacceptable_content_type = 0\napplication_content_types = ('application/xml', 'application/xml-dtd', 'application/xml-external-parsed-entity')\ntext_content_types = ('text/xml', 'text/xml-external-parsed-entity')\nif (http_content_type in application_content_types) or \\\n   (http_content_type.startswith('application/') and http_content_type.endswith('+xml')):\n    acceptable_content_type = 1\n    true_encoding = http_encoding or xml_encoding or 'utf-8'\nelif (http_content_type in text_content_types) or \\\n     (http_content_type.startswith('text/')) and http_content_type.endswith('+xml'):\n    acceptable_content_type = 1\n    true_encoding = http_encoding or 'us-ascii'\nelif http_content_type.startswith('text/'):\n    true_encoding = http_encoding or 'us-ascii'\nelif http_headers and (not http_headers.has_key('content-type')):\n    true_encoding = xml_encoding or 'iso-8859-1'\nelse:\n    true_encoding = xml_encoding or 'utf-8'\nreturn true_encoding, http_encoding, xml_encoding, sniffed_xml_encoding, acceptable_content_type", "path": "improviser\\qtGUI\\feedparser.py", "repo_name": "bspaans/improviser", "stars": 25, "license": "None", "language": "python", "size": 258}
{"docstring": "\"\"\"Prepares and returns the next block in the movement.\\\nReturns None if there is no next block.\"\"\"\n", "func_signal": "def get_next_block(self):\n", "code": "if len(self.blocks) > 0:\n\tif self.loop >= 0:\n\t\tr = self.get_block()\n\t\tr.progressions = self.get_progression()\n\t\tself.b_counter += 1\n\t\tif self.b_counter == len(self.blocks):\n\t\t\tself.b_counter = 0\n\t\t\tself.loop -= 1\n\t\treturn r\nreturn None", "path": "improviser\\Movements\\Movement.py", "repo_name": "bspaans/improviser", "stars": 25, "license": "None", "language": "python", "size": 258}
{"docstring": "'''Parse a string according to the MS SQL date format'''\n", "func_signal": "def _parse_date_mssql(dateString):\n", "code": "m = _mssql_date_re.match(dateString)\nif not m: return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': m.group(4), 'minute': m.group(5), 'second': m.group(6),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('MS SQL date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "improviser\\qtGUI\\feedparser.py", "repo_name": "bspaans/improviser", "stars": 25, "license": "None", "language": "python", "size": 258}
{"docstring": "'''Parse a string according to a Hungarian 8-bit date format.'''\n", "func_signal": "def _parse_date_hungarian(dateString):\n", "code": "m = _hungarian_date_format_re.match(dateString)\nif not m: return\ntry:\n    month = _hungarian_months[m.group(2)]\n    day = m.group(3)\n    if len(day) == 1:\n        day = '0' + day\n    hour = m.group(4)\n    if len(hour) == 1:\n        hour = '0' + hour\nexcept:\n    return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': month, 'day': day,\\\n             'hour': hour, 'minute': m.group(5),\\\n             'zonediff': m.group(6)}\nif _debug: sys.stderr.write('Hungarian date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "improviser\\qtGUI\\feedparser.py", "repo_name": "bspaans/improviser", "stars": 25, "license": "None", "language": "python", "size": 258}
{"docstring": "# Check if\n# - server requires digest auth, AND\n# - we tried (unsuccessfully) with basic auth, AND\n# - we're using Python 2.3.3 or later (digest auth is irreparably broken in earlier versions)\n# If all conditions hold, parse authentication information\n# out of the Authorization header we sent the first time\n# (for the username and password) and the WWW-Authenticate\n# header the server sent back (for the realm) and retry\n# the request with the appropriate digest auth headers instead.\n# This evil genius hack has been brought to you by Aaron Swartz.\n", "func_signal": "def http_error_401(self, req, fp, code, msg, headers):\n", "code": "host = urlparse.urlparse(req.get_full_url())[1]\ntry:\n    assert sys.version.split()[0] >= '2.3.3'\n    assert base64 != None\n    user, passw = base64.decodestring(req.headers['Authorization'].split(' ')[1]).split(':')\n    realm = re.findall('realm=\"([^\"]*)\"', headers['WWW-Authenticate'])[0]\n    self.add_password(realm, host, user, passw)\n    retry = self.http_error_auth_reqed('www-authenticate', host, req, headers)\n    self.reset_retry_count()\n    return retry\nexcept:\n    return self.http_error_default(req, fp, code, msg, headers)", "path": "improviser\\qtGUI\\feedparser.py", "repo_name": "bspaans/improviser", "stars": 25, "license": "None", "language": "python", "size": 258}
{"docstring": "'''Parse a string according to the Nate 8-bit date format'''\n", "func_signal": "def _parse_date_nate(dateString):\n", "code": "m = _korean_nate_date_re.match(dateString)\nif not m: return\nhour = int(m.group(5))\nampm = m.group(4)\nif (ampm == _korean_pm):\n    hour += 12\nhour = str(hour)\nif len(hour) == 1:\n    hour = '0' + hour\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': hour, 'minute': m.group(6), 'second': m.group(7),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('Nate date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "improviser\\qtGUI\\feedparser.py", "repo_name": "bspaans/improviser", "stars": 25, "license": "None", "language": "python", "size": 258}
{"docstring": "# called for each block of plain text, i.e. outside of any tag and\n# not containing any character or entity references\n# Store the original text verbatim.\n", "func_signal": "def handle_data(self, text):\n", "code": "if _debug: sys.stderr.write('_BaseHTMLProcessor, handle_text, text=%s\\n' % text)\nself.pieces.append(text)", "path": "improviser\\qtGUI\\feedparser.py", "repo_name": "bspaans/improviser", "stars": 25, "license": "None", "language": "python", "size": 258}
{"docstring": "# called for each character reference, e.g. for '&#160;', ref will be '160'\n", "func_signal": "def handle_charref(self, ref):\n", "code": "if not self.elementstack: return\nref = ref.lower()\nif ref in ('34', '38', '39', '60', '62', 'x22', 'x26', 'x27', 'x3c', 'x3e'):\n    text = '&#%s;' % ref\nelse:\n    if ref[0] == 'x':\n        c = int(ref[1:], 16)\n    else:\n        c = int(ref)\n    text = unichr(c).encode('utf-8')\nself.elementstack[-1][2].append(text)", "path": "improviser\\qtGUI\\feedparser.py", "repo_name": "bspaans/improviser", "stars": 25, "license": "None", "language": "python", "size": 258}
{"docstring": "'''Changes an XML data stream on the fly to specify a new encoding\n\ndata is a raw sequence of bytes (not Unicode) that is presumed to be in %encoding already\nencoding is a string recognized by encodings.aliases\n'''\n", "func_signal": "def _toUTF8(data, encoding):\n", "code": "if _debug: sys.stderr.write('entering _toUTF8, trying encoding %s\\n' % encoding)\n# strip Byte Order Mark (if present)\nif (len(data) >= 4) and (data[:2] == '\\xfe\\xff') and (data[2:4] != '\\x00\\x00'):\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-16be':\n            sys.stderr.write('trying utf-16be instead\\n')\n    encoding = 'utf-16be'\n    data = data[2:]\nelif (len(data) >= 4) and (data[:2] == '\\xff\\xfe') and (data[2:4] != '\\x00\\x00'):\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-16le':\n            sys.stderr.write('trying utf-16le instead\\n')\n    encoding = 'utf-16le'\n    data = data[2:]\nelif data[:3] == '\\xef\\xbb\\xbf':\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-8':\n            sys.stderr.write('trying utf-8 instead\\n')\n    encoding = 'utf-8'\n    data = data[3:]\nelif data[:4] == '\\x00\\x00\\xfe\\xff':\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-32be':\n            sys.stderr.write('trying utf-32be instead\\n')\n    encoding = 'utf-32be'\n    data = data[4:]\nelif data[:4] == '\\xff\\xfe\\x00\\x00':\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-32le':\n            sys.stderr.write('trying utf-32le instead\\n')\n    encoding = 'utf-32le'\n    data = data[4:]\nnewdata = unicode(data, encoding)\nif _debug: sys.stderr.write('successfully converted %s data to unicode\\n' % encoding)\ndeclmatch = re.compile('^<\\?xml[^>]*?>')\nnewdecl = '''<?xml version='1.0' encoding='utf-8'?>'''\nif declmatch.search(newdata):\n    newdata = declmatch.sub(newdecl, newdata)\nelse:\n    newdata = newdecl + u'\\n' + newdata\nreturn newdata.encode('utf-8')", "path": "improviser\\qtGUI\\feedparser.py", "repo_name": "bspaans/improviser", "stars": 25, "license": "None", "language": "python", "size": 258}
{"docstring": "# override internal declaration handler to handle CDATA blocks\n", "func_signal": "def parse_declaration(self, i):\n", "code": "if _debug: sys.stderr.write('entering parse_declaration\\n')\nif self.rawdata[i:i+9] == '<![CDATA[':\n    k = self.rawdata.find(']]>', i)\n    if k == -1: k = len(self.rawdata)\n    self.handle_data(_xmlescape(self.rawdata[i+9:k]), 0)\n    return k+3\nelse:\n    k = self.rawdata.find('>', i)\n    return k+1", "path": "improviser\\qtGUI\\feedparser.py", "repo_name": "bspaans/improviser", "stars": 25, "license": "None", "language": "python", "size": 258}
{"docstring": "\"\"\"Return unindented function body\"\"\"\n", "func_signal": "def get_body(pyfunction):\n", "code": "scope = pyfunction.get_scope()\npymodule = pyfunction.get_module()\nstart, end = get_body_region(pyfunction)\nreturn fix_indentation(pymodule.source_code[start:end], 0)", "path": "parser\\rope\\refactor\\sourceutils.py", "repo_name": "davidsansome/pyqtc", "stars": 27, "license": "None", "language": "python", "size": 7286}
{"docstring": "\"\"\"Validate files and folders contained in this folder\n\nIt validates all of the files and folders contained in this\nfolder if some observers are interested in them.\n\n\"\"\"\n", "func_signal": "def validate(self, folder):\n", "code": "for observer in list(self.observers):\n    observer.validate(folder)", "path": "parser\\rope\\base\\project.py", "repo_name": "davidsansome/pyqtc", "stars": 27, "license": "None", "language": "python", "size": 7286}
{"docstring": "\"\"\"Report that the contents of file at `path` was changed\n\nThe new contents of file is retrieved by reading the file.\n\n\"\"\"\n", "func_signal": "def report_change(project, path, old_content):\n", "code": "resource = path_to_resource(project, path)\nif resource is None:\n    return\nfor observer in list(project.observers):\n    observer.resource_changed(resource)\nif project.pycore.automatic_soa:\n    rope.base.pycore.perform_soa_on_changed_scopes(project, resource,\n                                                   old_content)", "path": "parser\\rope\\base\\libutils.py", "repo_name": "davidsansome/pyqtc", "stars": 27, "license": "None", "language": "python", "size": 7286}
{"docstring": "\"\"\"Get a resource in a project.\n\n`resource_name` is the path of a resource in a project.  It is\nthe path of a resource relative to project root.  Project root\nfolder address is an empty string.  If the resource does not\nexist a `exceptions.ResourceNotFound` exception would be\nraised.  Use `get_file()` and `get_folder()` when you need to\nget nonexistent `Resource`\\s.\n\n\"\"\"\n", "func_signal": "def get_resource(self, resource_name):\n", "code": "path = self._get_resource_path(resource_name)\nif not os.path.exists(path):\n    raise exceptions.ResourceNotFoundError(\n        'Resource <%s> does not exist' % resource_name)\nelif os.path.isfile(path):\n    return File(self, resource_name)\nelif os.path.isdir(path):\n    return Folder(self, resource_name)\nelse:\n    raise exceptions.ResourceNotFoundError('Unknown resource '\n                                           + resource_name)", "path": "parser\\rope\\base\\project.py", "repo_name": "davidsansome/pyqtc", "stars": 27, "license": "None", "language": "python", "size": 7286}
{"docstring": "\"\"\"Perform static object analysis on all python files in the project\n\nNote that this might be really time consuming.\n\"\"\"\n", "func_signal": "def analyze_modules(project, task_handle=taskhandle.NullTaskHandle()):\n", "code": "resources = project.pycore.get_python_files()\njob_set = task_handle.create_jobset('Analyzing Modules', len(resources))\nfor resource in resources:\n    job_set.started_job(resource.path)\n    project.pycore.analyze_module(resource)\n    job_set.finished_job()", "path": "parser\\rope\\base\\libutils.py", "repo_name": "davidsansome/pyqtc", "stars": 27, "license": "None", "language": "python", "size": 7286}
{"docstring": "\"\"\"Return the current `JobSet`\"\"\"\n", "func_signal": "def current_jobset(self):\n", "code": "if self.job_sets:\n    return self.job_sets[-1]", "path": "parser\\rope\\base\\taskhandle.py", "repo_name": "davidsansome/pyqtc", "stars": 27, "license": "None", "language": "python", "size": 7286}
{"docstring": "\"\"\"Change the indentation of `code` to `new_indents`\"\"\"\n", "func_signal": "def fix_indentation(code, new_indents):\n", "code": "min_indents = find_minimum_indents(code)\nreturn indent_lines(code, new_indents - min_indents)", "path": "parser\\rope\\refactor\\sourceutils.py", "repo_name": "davidsansome/pyqtc", "stars": 27, "license": "None", "language": "python", "size": 7286}
{"docstring": "\"\"\"Return the real path of `path`\n\nIs equivalent to ``realpath(abspath(expanduser(path)))``.\n\n\"\"\"\n# there is a bug in cygwin for os.path.abspath() for abs paths\n", "func_signal": "def _realpath(path):\n", "code": "if sys.platform == 'cygwin':\n    if path[1:3] == ':\\\\':\n        return path\n    return os.path.abspath(os.path.expanduser(path))\nreturn os.path.realpath(os.path.abspath(os.path.expanduser(path)))", "path": "parser\\rope\\base\\project.py", "repo_name": "davidsansome/pyqtc", "stars": 27, "license": "None", "language": "python", "size": 7286}
{"docstring": "\"\"\"Remove a registered `ResourceObserver`\"\"\"\n", "func_signal": "def remove_observer(self, observer):\n", "code": "if observer in self.observers:\n    self.observers.remove(observer)", "path": "parser\\rope\\base\\project.py", "repo_name": "davidsansome/pyqtc", "stars": 27, "license": "None", "language": "python", "size": 7286}
{"docstring": "\"\"\"Get the resource at path\n\nYou only need to specify `type` if `path` does not exist.  It can\nbe either 'file' or 'folder'.  If the type is `None` it is assumed\nthat the resource already exists.\n\nNote that this function uses `Project.get_resource()`,\n`Project.get_file()`, and `Project.get_folder()` methods.\n\n\"\"\"\n", "func_signal": "def path_to_resource(project, path, type=None):\n", "code": "project_path = relative(project.address, path)\nif project_path is None:\n    project_path = rope.base.project._realpath(path)\n    project = rope.base.project.get_no_project()\nif type is None:\n    return project.get_resource(project_path)\nif type == 'file':\n    return project.get_file(project_path)\nif type == 'folder':\n    return project.get_folder(project_path)\nreturn None", "path": "parser\\rope\\base\\libutils.py", "repo_name": "davidsansome/pyqtc", "stars": 27, "license": "None", "language": "python", "size": 7286}
{"docstring": "# Changing occurrences\n", "func_signal": "def _dest_module_changes(self, dest):\n", "code": "pymodule = self.pycore.resource_to_pyobject(dest)\nsource = self.tools.rename_in_module(self.old_name, pymodule)\npymodule = self.tools.new_pymodule(pymodule, source)\n\nmoving, imports = self._get_moving_element_with_imports()\nsource = self.tools.remove_old_imports(pymodule)\npymodule = self.tools.new_pymodule(pymodule, source)\npymodule, has_changed = self._add_imports2(pymodule, imports)\n\nmodule_with_imports = self.import_tools.module_imports(pymodule)\nsource = pymodule.source_code\nif module_with_imports.imports:\n    start = pymodule.lines.get_line_end(\n        module_with_imports.imports[-1].end_line - 1)\n    result = source[:start + 1] + '\\n\\n'\nelse:\n    result = ''\n    start = -1\nresult += moving + source[start + 1:]\n\n# Organizing imports\nsource = result\npymodule = self.pycore.get_string_module(source, dest)\nsource = self.import_tools.organize_imports(pymodule, sort=False,\n                                            unused=False)\nreturn ChangeContents(dest, source)", "path": "parser\\rope\\refactor\\move.py", "repo_name": "davidsansome/pyqtc", "stars": 27, "license": "None", "language": "python", "size": 7286}
{"docstring": "\"\"\"Approximate block start\"\"\"\n", "func_signal": "def get_block_start(lines, lineno, maximum_indents=80):\n", "code": "pattern = get_block_start_patterns()\nfor i in range(lineno, 0, -1):\n    match = pattern.search(lines.get_line(i))\n    if match is not None and \\\n       count_line_indents(lines.get_line(i)) <= maximum_indents:\n        striped = match.string.lstrip()\n        # Maybe we're in a list comprehension or generator expression\n        if i > 1 and striped.startswith('if') or striped.startswith('for'):\n            bracs = 0\n            for j in range(i, min(i + 5, lines.length() + 1)):\n                for c in lines.get_line(j):\n                    if c == '#':\n                        break\n                    if c in '[(':\n                        bracs += 1\n                    if c in ')]':\n                        bracs -= 1\n                        if bracs < 0:\n                            break\n                if bracs < 0:\n                    break\n            if bracs < 0:\n                continue\n        return i\nreturn 1", "path": "parser\\rope\\base\\codeanalyze.py", "repo_name": "davidsansome/pyqtc", "stars": 27, "license": "None", "language": "python", "size": 7286}
{"docstring": "\"\"\"Interrupts the refactoring\"\"\"\n", "func_signal": "def stop(self):\n", "code": "if self.interrupts:\n    self.stopped = True\n    self._inform_observers()", "path": "parser\\rope\\base\\taskhandle.py", "repo_name": "davidsansome/pyqtc", "stars": 27, "license": "None", "language": "python", "size": 7286}
{"docstring": "\"\"\"A factory for creating Move objects\n\nBased on `resource` and `offset`, return one of `MoveModule`,\n`MoveGlobal` or `MoveMethod` for performing move refactoring.\n\n\"\"\"\n", "func_signal": "def create_move(project, resource, offset=None):\n", "code": "if offset is None:\n    return MoveModule(project, resource)\nthis_pymodule = project.pycore.resource_to_pyobject(resource)\npyname = evaluate.eval_location(this_pymodule, offset)\nif pyname is None:\n    raise exceptions.RefactoringError(\n        'Move only works on classes, functions, modules and methods.')\npyobject = pyname.get_object()\nif isinstance(pyobject, pyobjects.PyModule) or \\\n   isinstance(pyobject, pyobjects.PyPackage):\n    return MoveModule(project, pyobject.get_resource())\nif isinstance(pyobject, pyobjects.PyFunction) and \\\n   isinstance(pyobject.parent, pyobjects.PyClass):\n    return MoveMethod(project, resource, offset)\nif isinstance(pyobject, pyobjects.PyDefinedObject) and \\\n   isinstance(pyobject.parent, pyobjects.PyModule):\n    return MoveGlobal(project, resource, offset)\nraise exceptions.RefactoringError(\n    'Move only works on global classes/functions, modules and methods.')", "path": "parser\\rope\\refactor\\move.py", "repo_name": "davidsansome/pyqtc", "stars": 27, "license": "None", "language": "python", "size": 7286}
{"docstring": "\"\"\"Return a list of ``(name, level)`` tuples for assigned names\n\nThe `level` is `None` for simple assignments and is a list of\nnumbers for tuple assignments for example in::\n\n  a, (b, c) = x\n\nThe levels for for `a` is ``[0]``, for `b` is ``[1, 0]`` and for\n`c` is ``[1, 1]``.\n\n\"\"\"\n", "func_signal": "def get_name_levels(node):\n", "code": "visitor = _NodeNameCollector()\nast.walk(node, visitor)\nreturn visitor.names", "path": "parser\\rope\\base\\astutils.py", "repo_name": "davidsansome/pyqtc", "stars": 27, "license": "None", "language": "python", "size": 7286}
{"docstring": "\"\"\"Return the definition location of the python name at `offset`\n\nA `Location` object is returned if the definition location can be\ndetermined, otherwise ``None`` is returned.\n\"\"\"\n", "func_signal": "def find_definition(project, code, offset, resource=None, maxfixes=1):\n", "code": "fixer = fixsyntax.FixSyntax(project.pycore, code, resource, maxfixes)\nmain_module = fixer.get_pymodule()\npyname = fixer.pyname_at(offset)\nif pyname is not None:\n    module, lineno = pyname.get_definition_location()\n    name = rope.base.worder.Worder(code).get_word_at(offset)\n    if lineno is not None:\n        start = module.lines.get_line_start(lineno)\n        def check_offset(occurrence):\n            if occurrence.offset < start:\n                return False\n        pyname_filter = occurrences.PyNameFilter(pyname)\n        finder = occurrences.Finder(project.pycore, name,\n                                    [check_offset, pyname_filter])\n        for occurrence in finder.find_occurrences(pymodule=module):\n            return Location(occurrence)", "path": "parser\\rope\\contrib\\findit.py", "repo_name": "davidsansome/pyqtc", "stars": 27, "license": "None", "language": "python", "size": 7286}
{"docstring": "\"\"\"Should initialize _starts and _ends attributes\"\"\"\n", "func_signal": "def _init_logicals(self):\n", "code": "size = self.lines.length() + 1\nself._starts = [None] * size\nself._ends = [None] * size\nfor start, end in self._generate(self.lines):\n    self._starts[start] = True\n    self._ends[end] = True", "path": "parser\\rope\\base\\codeanalyze.py", "repo_name": "davidsansome/pyqtc", "stars": 27, "license": "None", "language": "python", "size": 7286}
{"docstring": "\"\"\"Construct a TaskHandle\n\nIf `interrupts` is `False` the task won't be interrupted by\ncalling `TaskHandle.stop()`.\n\n\"\"\"\n", "func_signal": "def __init__(self, name='Task', interrupts=True):\n", "code": "self.name = name\nself.interrupts = interrupts\nself.stopped = False\nself.job_sets = []\nself.observers = []", "path": "parser\\rope\\base\\taskhandle.py", "repo_name": "davidsansome/pyqtc", "stars": 27, "license": "None", "language": "python", "size": 7286}
{"docstring": "\"\"\"This function is called before opening the project\"\"\"\n\n# Specify which files and folders to ignore in the project.\n# Changes to ignored resources are not added to the history and\n# VCSs.  Also they are not returned in `Project.get_files()`.\n# Note that ``?`` and ``*`` match all characters but slashes.\n# '*.pyc': matches 'test.pyc' and 'pkg/test.pyc'\n# 'mod*.pyc': matches 'test/mod1.pyc' but not 'mod/1.pyc'\n# '.svn': matches 'pkg/.svn' and all of its children\n# 'build/*.o': matches 'build/lib.o' but not 'build/sub/lib.o'\n# 'build//*.o': matches 'build/lib.o' and 'build/sub/lib.o'\n", "func_signal": "def set_prefs(prefs):\n", "code": "prefs['ignored_resources'] = ['*.pyc', '*~', '.ropeproject',\n                              '.hg', '.svn', '_svn', '.git']\n\n# Specifies which files should be considered python files.  It is\n# useful when you have scripts inside your project.  Only files\n# ending with ``.py`` are considered to be python files by\n# default.\n#prefs['python_files'] = ['*.py']\n\n# Custom source folders:  By default rope searches the project\n# for finding source folders (folders that should be searched\n# for finding modules).  You can add paths to that list.  Note\n# that rope guesses project source folders correctly most of the\n# time; use this if you have any problems.\n# The folders should be relative to project root and use '/' for\n# separating folders regardless of the platform rope is running on.\n# 'src/my_source_folder' for instance.\n#prefs.add('source_folders', 'src')\n\n# You can extend python path for looking up modules\n#prefs.add('python_path', '~/python/')\n\n# Should rope save object information or not.\nprefs['save_objectdb'] = True\nprefs['compress_objectdb'] = False\n\n# If `True`, rope analyzes each module when it is being saved.\nprefs['automatic_soa'] = True\n# The depth of calls to follow in static object analysis\nprefs['soa_followed_calls'] = 0\n\n# If `False` when running modules or unit tests \"dynamic object\n# analysis\" is turned off.  This makes them much faster.\nprefs['perform_doa'] = True\n\n# Rope can check the validity of its object DB when running.\nprefs['validate_objectdb'] = True\n\n# How many undos to hold?\nprefs['max_history_items'] = 32\n\n# Shows whether to save history across sessions.\nprefs['save_history'] = True\nprefs['compress_history'] = False\n\n# Set the number spaces used for indenting.  According to\n# :PEP:`8`, it is best to use 4 spaces.  Since most of rope's\n# unit-tests use 4 spaces it is more reliable, too.\nprefs['indent_size'] = 4\n\n# Builtin and c-extension modules that are allowed to be imported\n# and inspected by rope.\nprefs['extension_modules'] = []\n\n# Add all standard c-extensions to extension_modules list.\nprefs['import_dynload_stdmods'] = True\n\n# If `True` modules with syntax errors are considered to be empty.\n# The default value is `False`; When `False` syntax errors raise\n# `rope.base.exceptions.ModuleSyntaxError` exception.\nprefs['ignore_syntax_errors'] = False\n\n# If `True`, rope ignores unresolvable imports.  Otherwise, they\n# appear in the importing namespace.\nprefs['ignore_bad_imports'] = False", "path": "parser\\rope\\base\\default_config.py", "repo_name": "davidsansome/pyqtc", "stars": 27, "license": "None", "language": "python", "size": 7286}
{"docstring": "\"\"\"Return the start and end offsets of function body\"\"\"\n", "func_signal": "def get_body_region(defined):\n", "code": "scope = defined.get_scope()\npymodule = defined.get_module()\nlines = pymodule.lines\nnode = defined.get_ast()\nstart_line = node.lineno\nif defined.get_doc() is None:\n    start_line = node.body[0].lineno\nelif len(node.body) > 1:\n    start_line = node.body[1].lineno\nstart = lines.get_line_start(start_line)\nscope_start = pymodule.logical_lines.logical_line_in(scope.start)\nif scope_start[1] >= start_line:\n    # a one-liner!\n    # XXX: what if colon appears in a string\n    start = pymodule.source_code.index(':', start) + 1\n    while pymodule.source_code[start].isspace():\n        start += 1\nend = min(lines.get_line_end(scope.end) + 1, len(pymodule.source_code))\nreturn start, end", "path": "parser\\rope\\refactor\\sourceutils.py", "repo_name": "davidsansome/pyqtc", "stars": 27, "license": "None", "language": "python", "size": 7286}
{"docstring": "#print \"Push\", tag.name\n", "func_signal": "def pushTag(self, tag):\n", "code": "if self.currentTag:\n    self.currentTag.contents.append(tag)\nself.tagStack.append(tag)\nself.currentTag = self.tagStack[-1]", "path": "bin\\hypertags\\BeautifulSoup.py", "repo_name": "mikaelj/limp", "stars": 20, "license": "None", "language": "python", "size": 10237}
{"docstring": "\"\"\"Initializes a map representation of this tag's attributes,\nif not already initialized.\"\"\"\n", "func_signal": "def _getAttrMap(self):\n", "code": "if not getattr(self, 'attrMap'):\n    self.attrMap = {}\n    for (key, value) in self.attrs:\n        self.attrMap[key] = value\nreturn self.attrMap", "path": "bin\\hypertags\\BeautifulSoup.py", "repo_name": "mikaelj/limp", "stars": 20, "license": "None", "language": "python", "size": 10237}
{"docstring": "\"\"\"Destructively rips this element out of the tree.\"\"\"\n", "func_signal": "def extract(self):\n", "code": "if self.parent:\n    try:\n        self.parent.contents.remove(self)\n    except ValueError:\n        pass\n\n#Find the two elements that would be next to each other if\n#this element (and any children) hadn't been parsed. Connect\n#the two.\nlastChild = self._lastRecursiveChild()\nnextElement = lastChild.next\n\nif self.previous:\n    self.previous.next = nextElement\nif nextElement:\n    nextElement.previous = self.previous\nself.previous = None\nlastChild.next = None\n\nself.parent = None\nif self.previousSibling:\n    self.previousSibling.nextSibling = self.nextSibling\nif self.nextSibling:\n    self.nextSibling.previousSibling = self.previousSibling\nself.previousSibling = self.nextSibling = None\nreturn self", "path": "bin\\hypertags\\BeautifulSoup.py", "repo_name": "mikaelj/limp", "stars": 20, "license": "None", "language": "python", "size": 10237}
{"docstring": "\"\"\"Changes a MS smart quote character to an XML or HTML\nentity.\"\"\"\n", "func_signal": "def _subMSChar(self, orig):\n", "code": "sub = self.MS_CHARS.get(orig)\nif type(sub) == types.TupleType:\n    if self.smartQuotesTo == 'xml':\n        sub = '&#x%s;' % sub[1]\n    else:\n        sub = '&%s;' % sub[0]\nreturn sub", "path": "bin\\hypertags\\BeautifulSoup.py", "repo_name": "mikaelj/limp", "stars": 20, "license": "None", "language": "python", "size": 10237}
{"docstring": "\"\"\"Handle a processing instruction as a ProcessingInstruction\nobject, possibly one with a %SOUP-ENCODING% slot into which an\nencoding will be plugged later.\"\"\"\n", "func_signal": "def handle_pi(self, text):\n", "code": "if text[:3] == \"xml\":\n    text = u\"xml version='1.0' encoding='%SOUP-ENCODING%'\"\nself._toStringSubclass(text, ProcessingInstruction)", "path": "bin\\hypertags\\BeautifulSoup.py", "repo_name": "mikaelj/limp", "stars": 20, "license": "None", "language": "python", "size": 10237}
{"docstring": "\"\"\"Treat a bogus SGML declaration as raw data. Treat a CDATA\ndeclaration as a CData object.\"\"\"\n", "func_signal": "def parse_declaration(self, i):\n", "code": "j = None\nif self.rawdata[i:i+9] == '<![CDATA[':\n     k = self.rawdata.find(']]>', i)\n     if k == -1:\n         k = len(self.rawdata)\n     data = self.rawdata[i+9:k]\n     j = k+3\n     self._toStringSubclass(data, CData)\nelse:\n    try:\n        j = SGMLParser.parse_declaration(self, i)\n    except SGMLParseError:\n        toHandle = self.rawdata[i:]\n        self.handle_data(toHandle)\n        j = i + len(toHandle)\nreturn j", "path": "bin\\hypertags\\BeautifulSoup.py", "repo_name": "mikaelj/limp", "stars": 20, "license": "None", "language": "python", "size": 10237}
{"docstring": "#print \"Matching %s against %s\" % (markup, matchAgainst)\n", "func_signal": "def _matches(self, markup, matchAgainst):\n", "code": "result = False\nif matchAgainst == True and type(matchAgainst) == types.BooleanType:\n    result = markup != None\nelif callable(matchAgainst):\n    result = matchAgainst(markup)\nelse:\n    #Custom match methods take the tag as an argument, but all\n    #other ways of matching match the tag name as a string.\n    if isinstance(markup, Tag):\n        markup = markup.name\n    if markup and not isString(markup):\n        markup = unicode(markup)\n    #Now we know that chunk is either a string, or None.\n    if hasattr(matchAgainst, 'match'):\n        # It's a regexp object.\n        result = markup and matchAgainst.search(markup)\n    elif isList(matchAgainst):\n        result = markup in matchAgainst\n    elif hasattr(matchAgainst, 'items'):\n        result = markup.has_key(matchAgainst)\n    elif matchAgainst and isString(markup):\n        if isinstance(markup, unicode):\n            matchAgainst = unicode(matchAgainst)\n        else:\n            matchAgainst = str(matchAgainst)\n\n    if not result:\n        result = matchAgainst == markup\nreturn result", "path": "bin\\hypertags\\BeautifulSoup.py", "repo_name": "mikaelj/limp", "stars": 20, "license": "None", "language": "python", "size": 10237}
{"docstring": "\"\"\"Returns the closest sibling to this Tag that matches the\ngiven criteria and appears after this Tag in the document.\"\"\"\n", "func_signal": "def findNextSibling(self, name=None, attrs={}, text=None, **kwargs):\n", "code": "return self._findOne(self.findNextSiblings, name, attrs, text,\n                     **kwargs)", "path": "bin\\hypertags\\BeautifulSoup.py", "repo_name": "mikaelj/limp", "stars": 20, "license": "None", "language": "python", "size": 10237}
{"docstring": "\"\"\"Encodes an object to a string in some encoding, or to Unicode.\n.\"\"\"\n", "func_signal": "def toEncoding(self, s, encoding=None):\n", "code": "if isinstance(s, unicode):\n    if encoding:\n        s = s.encode(encoding)\nelif isinstance(s, str):\n    if encoding:\n        s = s.encode(encoding)\n    else:\n        s = unicode(s)\nelse:\n    if encoding:\n        s  = self.toEncoding(str(s), encoding)\n    else:\n        s = unicode(s)\nreturn s", "path": "bin\\hypertags\\BeautifulSoup.py", "repo_name": "mikaelj/limp", "stars": 20, "license": "None", "language": "python", "size": 10237}
{"docstring": "\"\"\"Returns the closest sibling to this Tag that matches the\ngiven criteria and appears before this Tag in the document.\"\"\"\n", "func_signal": "def findPreviousSibling(self, name=None, attrs={}, text=None, **kwargs):\n", "code": "return self._findOne(self.findPreviousSiblings, name, attrs, text,\n                     **kwargs)", "path": "bin\\hypertags\\BeautifulSoup.py", "repo_name": "mikaelj/limp", "stars": 20, "license": "None", "language": "python", "size": 10237}
{"docstring": "\"\"\"Returns the closest parent of this Tag that matches the given\ncriteria.\"\"\"\n# NOTE: We can't use _findOne because findParents takes a different\n# set of arguments.\n", "func_signal": "def findParent(self, name=None, attrs={}, **kwargs):\n", "code": "r = None\nl = self.findParents(name, attrs, 1)\nif l:\n    r = l[0]\nreturn r", "path": "bin\\hypertags\\BeautifulSoup.py", "repo_name": "mikaelj/limp", "stars": 20, "license": "None", "language": "python", "size": 10237}
{"docstring": "# Convert the document to Unicode.\n", "func_signal": "def _feed(self, inDocumentEncoding=None):\n", "code": "markup = self.markup\nif isinstance(markup, unicode):\n    if not hasattr(self, 'originalEncoding'):\n        self.originalEncoding = None\nelse:\n    dammit = UnicodeDammit\\\n             (markup, [self.fromEncoding, inDocumentEncoding],\n              smartQuotesTo=self.smartQuotesTo)\n    markup = dammit.unicode\n    self.originalEncoding = dammit.originalEncoding\nif markup:\n    if self.markupMassage:\n        if not isList(self.markupMassage):\n            self.markupMassage = self.MARKUP_MASSAGE\n        for fix, m in self.markupMassage:\n            markup = fix.sub(m, markup)\n        # TODO: We get rid of markupMassage so that the\n        # soup object can be deepcopied later on. Some\n        # Python installations can't copy regexes. If anyone\n        # was relying on the existence of markupMassage, this\n        # might cause problems.\n        del(self.markupMassage)\nself.reset()\n\nSGMLParser.feed(self, markup)\n# Close out any unfinished strings and close all the open tags.\nself.endData()\nwhile self.currentTag.name != self.ROOT_TAG_NAME:\n    self.popTag()", "path": "bin\\hypertags\\BeautifulSoup.py", "repo_name": "mikaelj/limp", "stars": 20, "license": "None", "language": "python", "size": 10237}
{"docstring": "\"\"\"Recursively destroys the contents of this tree.\"\"\"\n", "func_signal": "def decompose(self):\n", "code": "contents = [i for i in self.contents]\nfor i in contents:\n    if isinstance(i, Tag):\n        i.decompose()\n    else:\n        i.extract()\nself.extract()", "path": "bin\\hypertags\\BeautifulSoup.py", "repo_name": "mikaelj/limp", "stars": 20, "license": "None", "language": "python", "size": 10237}
{"docstring": "\"\"\"This method routes method call requests to either the SGMLParser\nsuperclass or the Tag superclass, depending on the method name.\"\"\"\n#print \"__getattr__ called on %s.%s\" % (self.__class__, methodName)\n\n", "func_signal": "def __getattr__(self, methodName):\n", "code": "if methodName.find('start_') == 0 or methodName.find('end_') == 0 \\\n       or methodName.find('do_') == 0:\n    return SGMLParser.__getattr__(self, methodName)\nelif methodName.find('__') != 0:\n    return Tag.__getattr__(self, methodName)\nelse:\n    raise AttributeError", "path": "bin\\hypertags\\BeautifulSoup.py", "repo_name": "mikaelj/limp", "stars": 20, "license": "None", "language": "python", "size": 10237}
{"docstring": "\"\"\"Turns a list of maps, lists, or scalars into a single map.\nUsed to build the SELF_CLOSING_TAGS, NESTABLE_TAGS, and\nNESTING_RESET_TAGS maps out of lists and partial maps.\"\"\"\n", "func_signal": "def buildTagMap(default, *args):\n", "code": "built = {}\nfor portion in args:\n    if hasattr(portion, 'items'):\n        #It's a map. Merge it.\n        for k,v in portion.items():\n            built[k] = v\n    elif isList(portion):\n        #It's a list. Map each item to the default.\n        for k in portion:\n            built[k] = default\n    else:\n        #It's a scalar. Map it to the default.\n        built[portion] = default\nreturn built", "path": "bin\\hypertags\\BeautifulSoup.py", "repo_name": "mikaelj/limp", "stars": 20, "license": "None", "language": "python", "size": 10237}
{"docstring": "\"\"\"Returns true iff this tag has the same name, the same attributes,\nand the same contents (recursively) as the given tag.\n\nNOTE: right now this will return false if two tags have the\nsame attributes in a different order. Should this be fixed?\"\"\"\n", "func_signal": "def __eq__(self, other):\n", "code": "if not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):\n    return False\nfor i in range(0, len(self.contents)):\n    if self.contents[i] != other.contents[i]:\n        return False\nreturn True", "path": "bin\\hypertags\\BeautifulSoup.py", "repo_name": "mikaelj/limp", "stars": 20, "license": "None", "language": "python", "size": 10237}
{"docstring": "\"\"\"Given a document, tries to detect its XML encoding.\"\"\"\n", "func_signal": "def _detectEncoding(self, xml_data):\n", "code": "xml_encoding = sniffed_xml_encoding = None\ntry:\n    if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n        # EBCDIC\n        xml_data = self._ebcdic_to_ascii(xml_data)\n    elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n        # UTF-16BE\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') \\\n             and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16BE with BOM\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n        # UTF-16LE\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and \\\n             (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16LE with BOM\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n        # UTF-32BE\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n        # UTF-32LE\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n        # UTF-32BE with BOM\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n        # UTF-32LE with BOM\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n    elif xml_data[:3] == '\\xef\\xbb\\xbf':\n        # UTF-8 with BOM\n        sniffed_xml_encoding = 'utf-8'\n        xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n    else:\n        sniffed_xml_encoding = 'ascii'\n        pass\n    xml_encoding_match = re.compile \\\n                         ('^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>')\\\n                         .match(xml_data)\nexcept:\n    xml_encoding_match = None\nif xml_encoding_match:\n    xml_encoding = xml_encoding_match.groups()[0].lower()\n    if sniffed_xml_encoding and \\\n       (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',\n                         'iso-10646-ucs-4', 'ucs-4', 'csucs4',\n                         'utf-16', 'utf-32', 'utf_16', 'utf_32',\n                         'utf16', 'u16')):\n        xml_encoding = sniffed_xml_encoding\nreturn xml_data, xml_encoding, sniffed_xml_encoding", "path": "bin\\hypertags\\BeautifulSoup.py", "repo_name": "mikaelj/limp", "stars": 20, "license": "None", "language": "python", "size": 10237}
{"docstring": "#print \"End tag %s\" % name\n", "func_signal": "def unknown_endtag(self, name):\n", "code": "if self.quoteStack and self.quoteStack[-1] != name:\n    #This is not a real end tag.\n    #print \"</%s> is not real!\" % name\n    self.handle_data('</%s>' % name)\n    return\nself.endData()\nself._popToTag(name)\nif self.quoteStack and self.quoteStack[-1] == name:\n    self.quoteStack.pop()\n    self.literal = (len(self.quoteStack) > 0)", "path": "bin\\hypertags\\BeautifulSoup.py", "repo_name": "mikaelj/limp", "stars": 20, "license": "None", "language": "python", "size": 10237}
{"docstring": "\"\"\"Adds a certain piece of text to the tree as a NavigableString\nsubclass.\"\"\"\n", "func_signal": "def _toStringSubclass(self, text, subclass):\n", "code": "self.endData()\nself.handle_data(text)\nself.endData(subclass)", "path": "bin\\hypertags\\BeautifulSoup.py", "repo_name": "mikaelj/limp", "stars": 20, "license": "None", "language": "python", "size": 10237}
{"docstring": "\"\"\"Returns true iff the given string is the name of a\nself-closing tag according to this parser.\"\"\"\n", "func_signal": "def isSelfClosingTag(self, name):\n", "code": "return self.SELF_CLOSING_TAGS.has_key(name) \\\n       or self.instanceSelfClosingTags.has_key(name)", "path": "bin\\hypertags\\BeautifulSoup.py", "repo_name": "mikaelj/limp", "stars": 20, "license": "None", "language": "python", "size": 10237}
{"docstring": "\"\"\"A partir de lista de senten\u00e7as anotadas, onde cada senten\u00e7a \u00e9 uma lista de pares ordenados do tipo de ('palavra','N'), escreve corpus em arquivo de texto 'nome' em que os tokens s\u00e3o etiquetados da forma can\u00f4nica palavra/N.\"\"\"\n#f=open(os.path.join(USUARIO,nome),\"w\")\n", "func_signal": "def escreve_corpus(lista_de_sentencas,nome):\n", "code": "f=open(nome,\"w\") # salva por defeito no diret\u00f3rio de trabalho\nc=1 # inicializa\u00e7\u00e3o de um contador de palavras\nfor sentenca in lista_de_sentencas:\n    for palavra in sentenca:\n        f.write(\"%s/%s<%d> \" % (palavra[0],\n                    palavra[1], c ) )\n        c+=1\n    f.write(\"\\n\")\n    f.write(\"\\n====================\\n\") # recurso para fase de teste\nf.close()", "path": "Aelius\\AnotaCorpus.py", "repo_name": "CompLin/Aelius", "stars": 19, "license": "other", "language": "python", "size": 6469}
{"docstring": "\"\"\"A partir de um arquivo de senten\u00e7as cujos chunks do tipo NP est\u00e3o anotados no formato IOB, retorna uma lista de \u00e1rvores do tipo nltk.Tree.\n\"\"\"\n", "func_signal": "def IOB2trees(arquivo):\n", "code": "linhas=open(arquivo).read().strip().split(\"\\n\\n\")\nreturn [nltk.chunk.conllstr2tree(c) for c in linhas]", "path": "Aelius\\Chunking.py", "repo_name": "CompLin/Aelius", "stars": 19, "license": "other", "language": "python", "size": 6469}
{"docstring": "\"\"\"Retorna etiquetador a partir do modelo e arquitetura especificados. O par\u00e2metro 'arquitetura' tem como default 'nltk' e pode assumir tamb\u00e9m um dos seguintes valores: 'hunpos', 'stanford' ou 'mxpost'. Nesse \u00faltimo caso, \u00e9 constru\u00edda e retornada inst\u00e2ncia de etiquetador de um desses tipos, invocando o construtor das classes HunposTagger, StanfordTagger e MXPOSTTagger, respectivamente. A codifica\u00e7\u00e3o dos modelos deve ser utf-8. Caso a arquitetura seja nltk, assume-se que se trata de inst\u00e2ncia de etiquetador do NLTK armazenada em formato bin\u00e1rio (extens\u00e3o do arquivo '.pkl' ou '.pickle', por exemplo), sendo retornada inst\u00e2ncia de etiquetador do NLTK armazenada em formato bin\u00e1rio.\"\"\"\n", "func_signal": "def abre_etiquetador(modelo,arquitetura=\"nltk\"):\n", "code": "if arquitetura == \"nltk\":\n    f=open(modelo,\"rb\")\n    etiquetador=load(f)\n    f.close()\n    return etiquetador\nif arquitetura ==\"hunpos\":\n    return nltk.tag.HunposTagger(modelo,encoding=\"utf-8\")\nif arquitetura ==\"stanford\":\n    return nltk.tag.StanfordTagger(modelo,encoding=\"utf-8\")\nif arquitetura ==\"mxpost\":\n    return MXPOSTTagger(modelo,encoding=\"utf-8\")", "path": "Aelius\\AnotaCorpus.py", "repo_name": "CompLin/Aelius", "stars": 19, "license": "other", "language": "python", "size": 6469}
{"docstring": "\"\"\"Extrai de arquivo a distribui\u00e7\u00e3o de freq\u00fc\u00eancias de um corpus de\nrefer\u00eancia\n\"\"\"\n", "func_signal": "def FreqCorpusRef(arquivo=MODELO):\n", "code": "f=open(arquivo,\"rb\")\nreturn pickle.load(f)", "path": "Aelius\\CalculaEstatisticasLexicais.py", "repo_name": "CompLin/Aelius", "stars": 19, "license": "other", "language": "python", "size": 6469}
{"docstring": "\"\"\"Esta fun\u00e7\u00e3o transforma unicode em uma lista de tokens unicode.\"\"\"\n", "func_signal": "def toquenizaSentenca(sentenca):\n", "code": "t=\"\" # inicializamos uma cadeia vazia\nfor c in sentenca:\n\t# a cada volta do la\u00e7o, t \u00e9 atualizada\n\tif c in SINAIS: # se c \u00e9 sinal de pontua\u00e7\u00e3o, um espa\u00e7o \u00e9 inserido de ambos os lados, o resultado sendo concatenado a t\n\t\tt=\"%s %s \" % (t,c) \n\telse: # se c n\u00e3o \u00e9 sinal de pontua\u00e7\u00e3o, c \u00e9 simplesmente concatenado a t\n\t\tt=\"%s%s\" % (t,c)\nreturn t.split()", "path": "Aelius\\Toqueniza.py", "repo_name": "CompLin/Aelius", "stars": 19, "license": "other", "language": "python", "size": 6469}
{"docstring": "\"\"\"Esta fun\u00e7\u00e3o primeiro anota as senten\u00e7as com o TAGGER2, para depois utilizar seu output para separar as contra\u00e7\u00f5es.\n\n>>> tokens1=AnotaCorpus.TOK_PORT.tokenize(AnotaCorpus.EXEMPLO)\n>>> tokens1\n[u'Os', u'candidatos', u'classific\\xe1veis', u'dos', u'cursos', u'de', u'Sistemas', u'de', u'Informa\\xe7\\xe3o', u'poder\\xe3o', u'ocupar', u'as', u'vagas', u'remanescentes', u'do', u'Curso', u'de', u'Engenharia', u'de', u'Software', u'.']\n>>> AnotaCorpus.toqueniza_contracoes([tokens1])\n[[u'Os', u'candidatos', u'classific\\xe1veis', u'de', u'os', u'cursos', u'de', u'Sistemas', u'de', u'Informa\\xe7\\xe3o', u'poder\\xe3o', u'ocupar', u'as', u'vagas', u'remanescentes', u'de', u'o', u'Curso', u'de', u'Engenharia', u'de', u'Software', u'.']]\n>>> \n\"\"\"\n", "func_signal": "def toqueniza_contracoes(sentencas):\n", "code": "sents=anota_sentencas(sentencas,TAGGER2)\nsents=decodifica_sentencas_anotadas(sents)\nsents=[expande_contracoes(sent) for sent in sents]\nreturn [nltk.tag.untag(sent) for sent in sents]", "path": "Aelius\\AnotaCorpus.py", "repo_name": "CompLin/Aelius", "stars": 19, "license": "other", "language": "python", "size": 6469}
{"docstring": "'''Maiusculiza o in\u00edcio de cada senten\u00e7a do par\u00e1grafo.\n'''\n", "func_signal": "def formata_paragrafos(paras):\n", "code": "paragrafos=[]\nfor p in paras:\n    maiusculiza_inicio(p)", "path": "Aelius\\AnotaCorpus.py", "repo_name": "CompLin/Aelius", "stars": 19, "license": "other", "language": "python", "size": 6469}
{"docstring": "'''Extrai \u00edndice da palavra que se encontra no in\u00edcio de uma senten\u00e7a\ndada, sob a forma de uma lista de tokens, com cada sinal de pontua\u00e7\u00e3o, tal como\ndefinido na vari\u00e1vel global pontuacao,representando um token. \nCaso iniciada por sinal de pontua\u00e7\u00e3o, esse \u00e9 ignorado.\n'''\n", "func_signal": "def extrai_palavra_inicial(sentenca):\n", "code": "comprimento=len(sentenca)\nif comprimento < 1:\n\treturn -1\n\n\nelse:\n\tinicio=0\n\tindice=0\n\tcondicao=True\n\t\n\twhile condicao and comprimento > indice:\n\t\t# print sentenca\n\t\t# print len(sentenca), indice \n\t\tif sentenca[indice] in pontuacao: # pressup\u00f5e entrada toquenizada\n\t\t\tinicio+=1\n\t\telse:\n\t\t\tcondicao=False\n\t\t\treturn inicio\n\t\tindice+=1\nreturn -1", "path": "Aelius\\ProcessaNomesProprios.py", "repo_name": "CompLin/Aelius", "stars": 19, "license": "other", "language": "python", "size": 6469}
{"docstring": "\"\"\"Escreve em arquivo senten\u00e7as no formato unicode, anotadas, dadas como listas de pares (w,t), onde w \u00e9 uma palavra e t, uma etiqueta.\n\"\"\"\n", "func_signal": "def escreve_corpus(sentencas_anotadas,nome_do_arquivo):\n", "code": "sep1=\"\\t\" # separador entre palavra e etiqueta\nsep2=\"\\n\" # separador de pares de palavra e etiqueta\nsep3= \"\\n\" # separador de senten\u00e7as\narquivo=open(nome_do_arquivo,\"w\")\nfor s in tagged_sents:\n\tfor w,t in s:\n\t\tarquivo.write(\"%s%s%s%s\" % (w.encode(\"utf-8\"),sep1,t.encode(\"utf-8\"),sep2))\n\tarquivo.write(sep3)\narquivo.close()", "path": "Aelius\\Extras.py", "repo_name": "CompLin/Aelius", "stars": 19, "license": "other", "language": "python", "size": 6469}
{"docstring": "# inicializa\u00e7\u00e3o de contadores para palavras, senten\u00e7as e par\u00e1grafos\n", "func_signal": "def escreve_formato_xml(paras,nome,capitulo=\"1\"):\n", "code": "c,s,p=1,1,1\nf=open(os.path.join(DESTINO,nome),\"w\")\nf.write('<div type=\"chap\" n=\"%s\">' % capitulo)\nfor para in paras:\n    f.write('<p n=\"%d\">' % p)\n    for sentenca in para:\n        f.write('<s n=\"%d\">' %s)\n        for palavra in sentenca:\n            f.write('<w xml:id=\"w%d\" type=\"%s\">%s</w> ' % (c, palavra[1],palavra[0] ) )\n            c+=1\n        f.write('</s>')\n        s+=1\n    f.write('</p>')\n    p+=1\nf.write('</div>')\nf.close()", "path": "Aelius\\AnotaCorpus.py", "repo_name": "CompLin/Aelius", "stars": 19, "license": "other", "language": "python", "size": 6469}
{"docstring": "\"\"\"A partir de um arquivo de senten\u00e7as cujos chunks do tipo NP est\u00e3o anotados no formato IOB, retorna uma lista de \u00e1rvores do tipo nltk.Tree.\n\"\"\"\n", "func_signal": "def IOB2trees(arquivo):\n", "code": "linhas=open(arquivo).read().strip().split(\"\\n\\n\")\nreturn [nltk.chunk.conllstr2tree(c) for c in linhas]", "path": "Chunking.py", "repo_name": "CompLin/Aelius", "stars": 19, "license": "other", "language": "python", "size": 6469}
{"docstring": "\"\"\"A partir de cadeia dada, retorna outra cadeia em que os sinais de\npontua\u00e7\u00e3o da vari\u00e1vel global SINAIS est\u00e3o separados das palavras,\nexceto no caso dos itens listados na vari\u00e1vel global ABREVIATURAS.\"\"\"\n", "func_signal": "def ToquenizaPontuacao(sentenca=SENT):\n", "code": "tokens=toquenizaSentenca(sentenca)\nprocessaAbreviaturas(tokens)\ntokens=[t.encode(\"utf-8\") for t in tokens]\nreturn \" \".join(tokens)", "path": "Aelius\\Toqueniza.py", "repo_name": "CompLin/Aelius", "stars": 19, "license": "other", "language": "python", "size": 6469}
{"docstring": "\"\"\"Esta fun\u00e7\u00e3o transforma arquivo cujas linhas s\u00e3o pares do tipo\n\netiqueta1 etiqueta\n\nem um dicion\u00e1rio em que o valor de dic[etiqueta1] \u00e9 etiqueta2.\n\"\"\"\n", "func_signal": "def constroiDicionarioDeArquivo(arquivo=ARQUIVO):\n", "code": "dic={}\nf=open(arquivo,\"rU\")\nfor linha in f:\n\tchave,valor=linha.strip().split()\n\tdic[chave]=valor\nreturn dic", "path": "Aelius\\SimplificaEtiquetas.py", "repo_name": "CompLin/Aelius", "stars": 19, "license": "other", "language": "python", "size": 6469}
{"docstring": "\"\"\"Esta fun\u00e7\u00e3o separa as palavras dos sinais de pontua\u00e7\u00e3o, contidos\npor defeito na lista retornada pela fun\u00e7\u00e3o DefinePontuacao().\n\"\"\"\n", "func_signal": "def SeparaPontuacao(texto, pontuacao=DefinePontuacao(excluir=[\"-\"])):\n", "code": "s=\"\"\nfor c in linha:\n    if c in pontuacao:\n        s=\"%s %s\" % (s,c)\n    else:\n        s=\"%s%s\" % (s,c)\nreturn s", "path": "Aelius\\CalculaEstatisticasLexicais.py", "repo_name": "CompLin/Aelius", "stars": 19, "license": "other", "language": "python", "size": 6469}
{"docstring": "'''Maiusculiza as palavras min\u00fasculas no in\u00edcio de senten\u00e7a.\n'''\n", "func_signal": "def maiusculiza_inicio(lista_de_sentencas):\n", "code": "for indice in range(len(lista_de_sentencas)):\n    palavras,etiquetas= separa_palavras_de_etiquetas(lista_de_sentencas[indice])\n    palavras=maiusculiza_inicio_de_sentenca(palavras)\n    lista_de_sentencas[indice]=zip(palavras,etiquetas)", "path": "Aelius\\AnotaCorpus.py", "repo_name": "CompLin/Aelius", "stars": 19, "license": "other", "language": "python", "size": 6469}
{"docstring": "\"\"\"Output \u00e9 a lista de senten\u00e7as anotadas automaticamente,\ngold \u00e9 a lista de senten\u00e7as anotadas por humano; tagged_words \u00e9 a\nquantidade de tokens anotados. Se o valor da vari\u00e1vel global VERBOSE\nfor True, os erros s\u00e3o armazenados na vari\u00e1vel global LISTA_DE_ERROS.\n\"\"\"\n", "func_signal": "def avalia(output,gold,tagged_words):\n", "code": "erros=0\ndel LISTA_DE_ERROS[:]\nfor m in range(len(output)):\n    for n in range(len(output[m])):\n        out=output[m][n]\n        g=gold[m][n]\n        if out[1] != g[1]: \n            erros+=1", "path": "Aelius\\Avalia.py", "repo_name": "CompLin/Aelius", "stars": 19, "license": "other", "language": "python", "size": 6469}
{"docstring": "\"\"\"Esta fun\u00e7\u00e3o converte uma etiqueta do LXTagger em uma etiqueta do CHPTB ou no estilo deste corpus, conforme um dicion\u00e1rio que mapeia um conjunto de etiquetas em outro. Caso uma etiqueta n\u00e3o esteja inclu\u00edda como chave no dicion\u00e1rio, a fun\u00e7\u00e3o retorna simplesmente retorna a etiqueta dada como entrada.\n\"\"\"\n", "func_signal": "def LXTagger2CHPTB(etiqueta):\n", "code": "dicionario=constroiDicionarioDeArquivo()\nt=dicionario.get(etiqueta,0)\nif t:\n\treturn t\nelse:\n\treturn etiqueta", "path": "Aelius\\SimplificaEtiquetas.py", "repo_name": "CompLin/Aelius", "stars": 19, "license": "other", "language": "python", "size": 6469}
{"docstring": "\"\"\"Toma uma senten\u00e7a anotada, sob a forma de uma lista de duplas (palavra,etiqueta), conforme o sistema do Aelius (baseado no do Corpus Hist\u00f3rico do Portugu\u00eas Tycho Brahe) e retorna uma vers\u00e3o em que os tokens cujas etiquetas contenham o s\u00edmbolo '+' s\u00e3o expandidos pela fun\u00e7\u00e3o expande(). Por exemplo, o par ('na', 'P+D-F') desdobra-se nos dois pares ('em','P') e (u'a', u'D-F').\n\n>>> from Aelius import ExpandeContracoes, AnotaCorpus\n>>> sent=AnotaCorpus.EXEMPLO\n>>> print sent\nOs candidatos classific\u00e1veis dos cursos de Sistemas de Informa\u00e7\u00e3o poder\u00e3o ocupar as vagas remanescentes do Curso de Engenharia de Software.\n>>> tokens1=AnotaCorpus.TOK_PORT.tokenize(sent)\n>>> anotados=AnotaCorpus.anota_sentencas([tokens1],AnotaCorpus.TAGGER)\n>>> anotados\n[[('Os', 'D-P'), ('candidatos', 'N-P'), ('classific\\xc3\\xa1veis', 'ADJ-G-P'), ('dos', 'P+D-P'), ('cursos', 'N-P'), ('de', 'P'), ('Sistemas', 'NPR-P'), ('de', 'P'), ('Informa\\xc3\\xa7\\xc3\\xa3o', 'NPR'), ('poder\\xc3\\xa3o', 'VB-R'), ('ocupar', 'VB'), ('as', 'D-F-P'), ('vagas', 'ADJ-F-P'), ('remanescentes', 'ADJ-G-P'), ('do', 'P+D'), ('Curso', 'NPR'), ('de', 'P'), ('Engenharia', 'NPR'), ('de', 'P'), ('Software', 'NPR'), ('.', '.')]]\n>>> ExpandeContracoes.expande_contracoes(anotados[0])\n[('Os', 'D-P'), ('candidatos', 'N-P'), ('classific\\xc3\\xa1veis', 'ADJ-G-P'), ('de', 'P'), ('os', 'D-P'), ('cursos', 'N-P'), ('de', 'P'), ('Sistemas', 'NPR-P'), ('de', 'P'), ('Informa\\xc3\\xa7\\xc3\\xa3o', 'NPR'), ('poder\\xc3\\xa3o', 'VB-R'), ('ocupar', 'VB'), ('as', 'D-F-P'), ('vagas', 'ADJ-F-P'), ('remanescentes', 'ADJ-G-P'), ('de', 'P'), ('o', 'D'), ('Curso', 'NPR'), ('de', 'P'), ('Engenharia', 'NPR'), ('de', 'P'), ('Software', 'NPR'), ('.', '.')]\n\"\"\"\n", "func_signal": "def expande_contracoes(sentenca):\n", "code": "lista=[]\nfor w,t in sentenca:\n\t# \u00e9 preciso assegurar que a etiqueta n\u00e3o seja apenas \"+\", atribu\u00edda, no sistema do Aelius, ao h\u00edfen\n\tif len(t) > 2 and \"+\" in t:\n\t\tt1,t2=t.split(\"+\")\n\t\tw1,w2=expande(w)\n\t\tlista.extend([(w1,t1),(w2,t2)])\n\telse:\n\t\tif not w==\"-\": # exclus\u00e3o de h\u00edfen como token\n\t\t\tlista.append((w,t))\nreturn lista", "path": "Aelius\\expandecontracoes.py", "repo_name": "CompLin/Aelius", "stars": 19, "license": "other", "language": "python", "size": 6469}
{"docstring": "\"\"\"Esta fun\u00e7\u00e3o permite incluir ou excluir elementos da lista \npr\u00e9-definida de sinais de pontua\u00e7\u00e3o de Python.\n\"\"\"\n", "func_signal": "def DefinePontuacao(incluir=[],excluir=[]):\n", "code": "from string import punctuation as punct\npunct=[p for p in punct]\nif incluir:\n    punct.extend(incluir)\nif excluir:\n    punct=[p for p in punct if p not in excluir]\nreturn punct", "path": "Aelius\\CalculaEstatisticasLexicais.py", "repo_name": "CompLin/Aelius", "stars": 19, "license": "other", "language": "python", "size": 6469}
{"docstring": "\"\"\"Esta fun\u00e7\u00e3o toma contra\u00e7\u00e3o em unicode como entrada e a expande, retornando uma dupla.\n\"\"\"\n", "func_signal": "def expande(s):\n", "code": "if s.lower().startswith(\"d\"):\n\treturn (\"%se\" % s[0],s[1:])\nelif s.lower().startswith(\"n\"):\n\treturn (\"em\",s[1:]) # talvez seja necess\u00e1rio converter em mai\u00fascula em alguns casos\nelif s.lower() == (\"\u00e0\".decode(\"utf-8\")):\n\treturn (\"a\",\"a\" )\nelif s.lower() == (\"\u00e0s\".decode(\"utf-8\")):\n\treturn (\"a\",\"as\" )\nelif s.lower() == (\"ao\"):\n\treturn (\"a\",\"o\" )\nelif s.lower() == (\"aos\"):\n\treturn (\"a\",\"os\" )\nelif s.lower().startswith(\"\u00e0\".decode(\"utf-8\")):\n\treturn (\"a\",\"a%s\" % s[1:])\nelif s.lower().startswith(\"p\"):\n\treturn (\"por\",\"%s\" % s[3:])\nelif s.lower() == (\"comigo\"):\n\treturn (\"com\",\"mim\" )\nelif s.lower() == (\"contigo\"):\n\treturn (\"com\",\"ti\" )\nelif s.lower() == (\"consigo\"):\n\treturn (\"com\",\"si\" )\nelif s.lower() == (\"conosco\"):\n\treturn (\"com\",\"n\u00f3s\".decode(\"utf-8\"))\nelif s.lower() == (\"convosco\"):\n\treturn (\"com\",\"v\u00f3s\".decode(\"utf-8\") )\nelif s.lower().startswith(\"-m\") or s.lower().startswith(\"-t\"):\n\treturn (\"%se\" % s[:2],\"%s\" % s[2:] )\nelif s.lower().startswith(\"-lh\"): # n\u00e3o leva em conta \"lhes\" + \"o\" etc.; forma expandida \u00e9 sempre \"lhe\"\n\treturn (\"%se\" % s[:3],\"%s\" % s[3:] )\nelse:\n\treturn s,\"CL\"", "path": "Aelius\\expandecontracoes.py", "repo_name": "CompLin/Aelius", "stars": 19, "license": "other", "language": "python", "size": 6469}
{"docstring": "\"\"\" Parse the given document and write the tags to a gtk.TreeModel.\n\nThe parser uses the ctags command from the shell to create a ctags file,\nthen parses the file, and finally populates a treemodel. \"\"\"\n# refactoring noise    \n", "func_signal": "def _parse_doc_to_model(self):\n", "code": "doc = self.document\nls = self.model        \nls.clear()\ntmpfile = self._generate_tagfile_from_document(doc)\nif tmpfile is None: return ls\n\n# A list of lists. Matches the order found in tag files.\n# identifier, path to file, line number, type, and then more magical things\ntokenlist = [] \nh = open(tmpfile)\nfor r in h.readlines():\n    tokens = r.strip().split(\"\\t\")\n    if tokens[0][:2] == \"!_\": continue\n\n    # convert line numbers to an int\n    tokens[2] =  int(filter( lambda x: x in '1234567890', tokens[2] ))\n    \n    # prepend container elements, append member elements. Do this to\n    # make sure that container elements are created first.\n    if self._is_container(tokens): tokenlist = [tokens] + tokenlist\n    else: tokenlist.append(tokens)\nh.close()\n\n# add tokens to the treestore---------------------------------------\ncontainers = { None: None } # keep dict: token's name -> treeiter\n\n# iterate through the list of tags, \n# Note: Originally sorted by line number, bit it did break some\n# formatting in c\nfor tokens in tokenlist:\n\n    # skip enums\n    #if self.__get_type(tokens) in 'de': continue\n\n    # append current token to parent iter, or to trunk when there is none\n    parent = self._get_parent(tokens)\n    \n    if parent in containers: node = containers[parent]\n    else:\n        # create a dummy element in case the parent doesn't exist\n        node = ls.append( None, [parent,\"\",0,\"\"] )\n        containers[parent] = node\n    \n    # escape blanks in file path\n    tokens[1] = str( gnomevfs.get_uri_from_local_path(tokens[1]) )\n    \n    # make sure tokens[4] contains type code\n    if len(tokens) == 3: tokens.append(\"\")\n    else: tokens[3] = self.__get_type(tokens)\n    \n    # append to treestore\n    it = ls.append( node, tokens[:4] )\n    \n    # if this element was a container, remember its treeiter\n    if self._is_container(tokens):\n        containername = self._get_container_name(tokens)\n        containers[ containername ] = it\n    \n# remove temp file\nos.remove(tmpfile)", "path": "classbrowser\\parser_ctags.py", "repo_name": "FooBarWidget/gedit-class-browser-plugin", "stars": 16, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\" Usually, we can assume that the parent's name is the same\n    as the name of the token. In some cases (typedefs), this\n    doesn't work (see Issue 13) \"\"\"\n\n", "func_signal": "def _get_container_name(self, tokrow):\n", "code": "if self.__get_type(tokrow) == \"t\":\n    try:\n        t = tokrow[4]\n        a = t.split(\":\")\n        return a[ len(a)-1 ]\n    except:\n        pass\nreturn tokrow[0]", "path": "classbrowser\\parser_ctags.py", "repo_name": "FooBarWidget/gedit-class-browser-plugin", "stars": 16, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\" Returns a char representing the token type or False if none were found.\n\nAccording to the ctags docs, possible types are:\n\t\tc\tclass name\n\t\td\tdefine (from #define XXX)\n\t\te\tenumerator\n\t\tf\tfunction or method name\n\t\tF\tfile name\n\t\tg\tenumeration name\n\t\tm\tmember (of structure or class data)\n\t\tp\tfunction prototype\n\t\ts\tstructure name\n\t\tt\ttypedef\n\t\tu\tunion name\n\t\tv\tvariable        \n\"\"\"\n\n# squeeze\n", "func_signal": "def _get_type(self, string):\n", "code": "s = re.sub(' +', ' ', string)\n\n\nif s.find(\"class\") >= 0: return \"c\"\nif s.find(\"public function\") >= 0: return \"m\"\nif re.search(\"private(.*)function\",s): return \"m_priv\"\nif re.search(\"protected(.*)function\",s): return \"m_prot\"\nif re.search(\"var(.*)\\$\",s): return \"v_pubvar\"\nif re.search(\"private(.*)\\$\",s): return \"v_privvar\"\nif re.search(\"protected(.*)\\$\",s): return \"v_protvar\"\n\nif s.find(\"function\") >= 0: return \"f\"\nreturn \"v\"", "path": "classbrowser\\parser_etags.py", "repo_name": "FooBarWidget/gedit-class-browser-plugin", "stars": 16, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\" Parse a string containing a function or class definition and return\n    a tuple containing information about the function, or None if the\n    parsing failed.\n\n    Example: \n        \"#def foo(bar):\" would return :\n        {'comment':True,'type':\"def\",'name':\"foo\",'params':\"bar\" } \"\"\"\n\n", "func_signal": "def functionTokenFromString(string):\n", "code": "try:\n    e = r\"([# ]*?)([a-zA-Z0-9_]+)( +)([a-zA-Z0-9_]+)(.*)\"\n    r = re.match(e,string).groups()\n    token = Token()\n    token.comment = '#' in r[0]\n    token.type = r[1]\n    token.name = r[3]\n    token.params = r[4]\n    token.original = string\n    return token\nexcept: return None # return None to skip if unable to parse", "path": "classbrowser\\parser_python.py", "repo_name": "FooBarWidget/gedit-class-browser-plugin", "stars": 16, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\" filestr is a string, could be *.* or explicit paths \"\"\"\n\n# create tempfile\n", "func_signal": "def _generate_tagfile(self, filestr, options = \"-n\"):\n", "code": "h, tmpfile = tempfile.mkstemp()\nos.close(h)\n\n# launch ctags\ncommand = \"ctags %s -f \\\"%s\\\" %s\"%(options,tmpfile,filestr)\nos.system(command)\n\nreturn tmpfile", "path": "classbrowser\\parser_ctags.py", "repo_name": "FooBarWidget/gedit-class-browser-plugin", "stars": 16, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\" Append a class attribute to the class a given token belongs to. \"\"\"\n\n# get next parent class\n", "func_signal": "def __appendClassAttribute(self, token, attrName, linenumber):\n", "code": "while token.type != \"class\":\n    token = token.parent\n    if not token: return   \n    \n# make sure attribute is not set yet\nfor i in token.attributes:\n    if i.name == attrName: return\n             \n# append a new attribute\nattr = Token()\nattr.type = \"attribute\"\nattr.name = attrName\nattr.start = linenumber\nattr.end = linenumber\nattr.pythonfile = self\ntoken.attributes.append(attr)", "path": "classbrowser\\parser_python.py", "repo_name": "FooBarWidget/gedit-class-browser-plugin", "stars": 16, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\" Render the browser cell according to the token it represents. \"\"\"\n", "func_signal": "def cellrenderer(self, column, ctr, model, it):\n", "code": "tok = model.get_value(it,0)\nname = tok.name\ncolour = options.singleton().colours[ \"function\" ]\nif tok.type == \"class\":\n    name = \"class \"+tok.name\n    colour = options.singleton().colours[ \"class\" ]\nctr.set_property(\"text\", name)\nctr.set_property(\"foreground-gdk\", colour)", "path": "classbrowser\\parser_cstyle.py", "repo_name": "FooBarWidget/gedit-class-browser-plugin", "stars": 16, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\" \nCreate a gtk.TreeModel with the class elements of the document\n\nThe parser uses the ctags command from the shell to create a ctags file,\nthen parses the file, and finally populates a treemodel.\n\"\"\"\n    \n", "func_signal": "def parse(self, doc):\n", "code": "self.rubyfile = RubyFile(doc)\nself.rubyfile.parse(options.singleton().verbose)\nself.__browsermodel = gtk.TreeStore(gobject.TYPE_PYOBJECT)\nfor child in self.rubyfile.children:\n    self.appendTokenToBrowser(child,None)\nreturn self.__browsermodel", "path": "classbrowser\\parser_ruby.py", "repo_name": "FooBarWidget/gedit-class-browser-plugin", "stars": 16, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\" get the token at the specified line number \"\"\"\n", "func_signal": "def getTokenAtLine(self, line):\n", "code": "for token in self.tokens:\n    if token.start <= line and token.end > line:\n        return token\nreturn None", "path": "classbrowser\\parser_python.py", "repo_name": "FooBarWidget/gedit-class-browser-plugin", "stars": 16, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\" Create a gtk.TreeModel with the tags of the document.\n \nThe TreeModel contains:\n   token name, source file path, line in the source file, type code\n\nIf the second str contains an empty string, it means that\nthe element has no 'physical' position in a file (see get_tag_position)   \"\"\"\n\n", "func_signal": "def parse(self, doc):\n", "code": "self.model = gtk.TreeStore(str,str,int,str) # see __parse_to_model\nself.model.set_sort_column_id(2,gtk.SORT_ASCENDING)\nself.document = doc\n\nif os.system(\"ctags --version >/dev/null\") != 0:\n    self.model.append( None, [\"Please install ctags!\",\"\",0,\"\"] )\n    return self.model\nelse:\n    self._parse_doc_to_model()\n    return self.model", "path": "classbrowser\\parser_ctags.py", "repo_name": "FooBarWidget/gedit-class-browser-plugin", "stars": 16, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\" Parse a string containing a function or class definition and return\n    a tuple containing information about the function, or None if the\n    parsing failed.\n\n    Example: \n        \"#def foo(bar):\" would return :\n        {'comment':True,'type':\"def\",'name':\"foo\",'params':\"bar\" } \"\"\"\n\n", "func_signal": "def tokenFromString(string):\n", "code": "try:\n    e = r\"([# ]*?)([a-zA-Z0-9_]+)( +)([a-zA-Z0-9_\\?\\!<>\\+=\\.]+)(.*)\"\n    r = re.match(e,string).groups()\n    token = Token()\n    token.comment = '#' in r[0]\n    token.type = r[1]\n    token.name = r[3]\n    token.params = r[4]\n    token.original = string\n    return token\nexcept: return None # return None to skip if unable to parse\n\ndef test():\n    pass", "path": "classbrowser\\parser_ruby.py", "repo_name": "FooBarWidget/gedit-class-browser-plugin", "stars": 16, "license": "None", "language": "python", "size": 197}
{"docstring": "# write changes to gconf\n", "func_signal": "def __del__(self):\n", "code": "client = gconf.client_get_default()\nclient.set_bool(self.__gconfDir+\"/verbose\", self.verbose)\nclient.set_bool(self.__gconfDir+\"/autocollapse\", self.autocollapse)\nclient.set_bool(self.__gconfDir+\"/jumpToTagOnMiddleClick\", self.jumpToTagOnMiddleClick)\nfor i in self.colours:\n    client.set_string(self.__gconfDir+\"/colour_\"+i, self.color_to_hex(self.colours[i]))", "path": "classbrowser\\options.py", "repo_name": "FooBarWidget/gedit-class-browser-plugin", "stars": 16, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\" Get the line number where this token's declaration, including all\n    its children, finishes. Use it for copy operations.\"\"\"\n", "func_signal": "def get_endline(self):\n", "code": "if len(self.children) > 0:\n    return self.children[-1].get_endline()\nreturn self.end\n\ndef test_nested():\n    pass", "path": "classbrowser\\parser_ruby.py", "repo_name": "FooBarWidget/gedit-class-browser-plugin", "stars": 16, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\" \nCreate a gtk.TreeModel with the class elements of the document\n\nThe parser uses the ctags command from the shell to create a ctags file,\nthen parses the file, and finally populates a treemodel.\n\"\"\"\n    \n", "func_signal": "def parse(self, doc):\n", "code": "self.pythonfile = PythonFile(doc)\nself.pythonfile.parse(options.singleton().verbose)\nself.__browsermodel = gtk.TreeStore(gobject.TYPE_PYOBJECT)\nfor child in self.pythonfile.children:\n    self.appendTokenToBrowser(child,None)\nreturn self.__browsermodel", "path": "classbrowser\\parser_python.py", "repo_name": "FooBarWidget/gedit-class-browser-plugin", "stars": 16, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\" Try to get the class a token is in. \"\"\"\n    \n", "func_signal": "def get_toplevel_class(self):\n", "code": "if self.type == \"class\":\n    return self    \n\nif self.parent is not None:\n    tc = self.parent.get_toplevel_class()\n    if tc is None or tc.type == \"file\": return self #hack\n    else: return tc\n        \nreturn None", "path": "classbrowser\\parser_python.py", "repo_name": "FooBarWidget/gedit-class-browser-plugin", "stars": 16, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\" Returns a char representing the token type or False if none were found.\n\nAccording to the ctags docs, possible types are:\n    c    class name\n    d    define (from #define XXX)\n    e    enumerator\n    f    function or method name\n    F    file name\n    g    enumeration name\n    m    member (of structure or class data)\n    p    function prototype\n    s    structure name\n    t    typedef\n    u    union name\n    v    variable        \n\"\"\"\n", "func_signal": "def __get_type(self, tokrow):\n", "code": "if len(tokrow) == 3: return\nfor i in tokrow[3:]:\n    if len(i) == 1: return i # most common case: just one char\n    elif i[:4] == \"kind\": return i[5:]\nreturn ' '", "path": "classbrowser\\parser_ctags.py", "repo_name": "FooBarWidget/gedit-class-browser-plugin", "stars": 16, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\" get the token at the specified line number \"\"\"\n", "func_signal": "def getTokenAtLine(self, line):\n", "code": "for token in self.tokens:\n    if token.start <= line and token.end > line:\n        return self.__findInnermostTokenAtLine(token, line)\nreturn None", "path": "classbrowser\\parser_ruby.py", "repo_name": "FooBarWidget/gedit-class-browser-plugin", "stars": 16, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\" class, enumerations, structs and unions are considerer containers.\n    See Issue 13 for some issues we had with this.\n\"\"\"\n", "func_signal": "def _is_container(self, tokrow):\n", "code": "if self.__get_type(tokrow) in 'cgsut': return True\nreturn False", "path": "classbrowser\\parser_ctags.py", "repo_name": "FooBarWidget/gedit-class-browser-plugin", "stars": 16, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\" Get the line number where this token's declaration, including all\n    its children, finishes. Use it for copy operations.\"\"\"\n", "func_signal": "def get_endline(self):\n", "code": "if len(self.children) > 0:\n    return self.children[-1].get_endline()\nreturn self.end\n\ndef test_nested():\n    pass", "path": "classbrowser\\parser_python.py", "repo_name": "FooBarWidget/gedit-class-browser-plugin", "stars": 16, "license": "None", "language": "python", "size": 197}
{"docstring": "\"\"\" Return a treepath to the tag at the given line number, or None if a\ntag can't be found.\n\"\"\"\n\n", "func_signal": "def get_tag_at_line(self, model, doc, linenumber):\n", "code": "if doc is None: return\n\nself.minline = -1\nself.tagpath = None\n    \ndef loopfunc(model, path, it):\n    if model.get_value(it,1) != doc.get_uri(): return\n    l = model.get_value(it,2)\n    if l >= self.minline and l <= linenumber+1:\n        self.tagpath = path\n        self.minline = l\n\n# recursively loop through the treestore\nmodel.foreach(loopfunc)\n\nif self.tagpath is None:\n    it = model.get_iter_root()\n    return model.get_path(it)\n\nreturn self.tagpath", "path": "classbrowser\\parser_ctags.py", "repo_name": "FooBarWidget/gedit-class-browser-plugin", "stars": 16, "license": "None", "language": "python", "size": 197}
{"docstring": "# Deleting field 'Task.priority'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.delete_column('scraper_task', 'priority')\n# Deleting field 'Task.domain'\n        db.delete_column('scraper_task', 'domain')", "path": "djangoscraper\\migrations\\0003_added_domain_and_priority.py", "repo_name": "taras/djangoscraper", "stars": 17, "license": "None", "language": "python", "size": 109}
{"docstring": "'''Gets list of start_urls for the task and returns them'''\n", "func_signal": "def get_start_urls(self):\n", "code": "urls = []\nif self.has_task() and self.task.start_urls:\n    urls = self.task.start_urls.splitlines()\nreturn urls", "path": "djangoscraper\\spiders\\taskspider.py", "repo_name": "taras/djangoscraper", "stars": 17, "license": "None", "language": "python", "size": 109}
{"docstring": "''' Gets the next pending task and returns it '''\n", "func_signal": "def fetch_task(self, id=None, name=None):\n", "code": "if name:\n    task = Task.objects.filter(domain=self.domain_name, locked=0, completed=0, name=name).order_by('priority')\nelif id:\n    task = Task.objects.filter(domain=self.domain_name, locked=0, completed=0, id=id).order_by('priority')\nelse:\n    task = Task.objects.filter(domain=self.domain_name, locked=0, completed=0).order_by('priority')            \n\nif task:\n    task = task[0]\n    task.locked = 1\n    task.save()\n    return task\nreturn None", "path": "djangoscraper\\spiders\\taskspider.py", "repo_name": "taras/djangoscraper", "stars": 17, "license": "None", "language": "python", "size": 109}
{"docstring": "# Deleting field 'Task.name'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.delete_column('scraper_task', 'name')\n# Adding field 'Task.callback'\n        db.add_column('scraper_task', 'callback', models.CharField(max_length=255))", "path": "djangoscraper\\migrations\\0005_renamed_callback_to_name.py", "repo_name": "taras/djangoscraper", "stars": 17, "license": "None", "language": "python", "size": 109}
{"docstring": "# Deleting field 'Task.start'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.delete_column('scraper_task', 'start')\n# Deleting field 'Task.finish'\n        db.delete_column('scraper_task', 'finish')", "path": "djangoscraper\\migrations\\0008_added_start_finish_fields.py", "repo_name": "taras/djangoscraper", "stars": 17, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"\nConverts a timedelta into a nicely readable string.\n\n    >>> td = timedelta(days = 77, seconds = 5)\n    >>> print readable_timedelta(td)\n    two months\n\"\"\"\n", "func_signal": "def stringify(td):\n", "code": "seconds = td.days * 3600 * 24 + td.seconds\namount, unit_name = seconds_in_units(seconds)\n\n# Localize it.\ni18n_amount = amount_to_str(amount, unit_name)\ni18n_unit = unit_names[LANG][unit_name][1]\nif amount == 1:\n    i18n_unit = unit_names[LANG][unit_name][0]\nreturn \"%s %s\" % (i18n_amount, i18n_unit)", "path": "djangoscraper\\utils\\timetext.py", "repo_name": "taras/djangoscraper", "stars": 17, "license": "None", "language": "python", "size": 109}
{"docstring": "''' Crawl task on specific spider '''\n", "func_signal": "def crawl(self, spider, task):\n", "code": "spider.load(task)\nscrapymanager.crawl(*spider.start_urls)", "path": "djangoscraper\\commands\\run.py", "repo_name": "taras/djangoscraper", "stars": 17, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"Tests that data remains the same when saved to and fetched from the database.\"\"\"\n", "func_signal": "def testDataIntegriry(self):\n", "code": "for value in self.testing_data:\n\tmodel_test = TestingModel(pickle_field=value)\n\tmodel_test.save()\n\tmodel_test = TestingModel.objects.get(id__exact=model_test.id)\n\tself.assertEquals(value, model_test.pickle_field)\n\tmodel_test.delete()", "path": "djangoscraper\\fields\\test.py", "repo_name": "taras/djangoscraper", "stars": 17, "license": "None", "language": "python", "size": 109}
{"docstring": "# Adding field 'Task.start'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.add_column('scraper_task', 'start', models.DateTimeField(null=True, blank=True))\n# Adding field 'Task.finish'\n        db.add_column('scraper_task', 'finish', models.DateTimeField(null=True, blank=True))", "path": "djangoscraper\\migrations\\0008_added_start_finish_fields.py", "repo_name": "taras/djangoscraper", "stars": 17, "license": "None", "language": "python", "size": 109}
{"docstring": "# Adding field 'Task.priority'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.add_column('scraper_task', 'priority', models.FloatField())\n# Adding field 'Task.domain'\n        db.add_column('scraper_task', 'domain', models.CharField(max_length=255))", "path": "djangoscraper\\migrations\\0003_added_domain_and_priority.py", "repo_name": "taras/djangoscraper", "stars": 17, "license": "None", "language": "python", "size": 109}
{"docstring": "'''\nReturn true if pending tasks for this domain exist.\nPending task is a task that is not locked and not completed.\n'''\n", "func_signal": "def has_pending(self, name=None):\n", "code": "if name:\n    pending = bool(Task.objects.filter(domain=self.domain_name, locked=0, completed=0, name=name))\nelse:\n    pending = bool(Task.objects.filter(domain=self.domain_name, locked=0, completed=0))\n\nreturn pending", "path": "djangoscraper\\spiders\\taskspider.py", "repo_name": "taras/djangoscraper", "stars": 17, "license": "None", "language": "python", "size": 109}
{"docstring": "# Adding model 'Task'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_table('scraper_task', (\n            ('errors', models.TextField()),\n            ('locked', models.BooleanField(default=0)),\n            ('description', models.TextField()),\n            ('created', models.DateTimeField(auto_now_add=True)),\n            ('completed', models.BooleanField(default=0)),\n            ('args', PickledObjectField()),\n            ('callback', models.CharField(max_length=255)),\n            ('result', models.TextField()),\n            ('identifier', models.CharField(max_length=255)),\n            ('id', models.AutoField(primary_key=True)),\n        ))\n        db.send_create_signal('scraper', ['Task'])", "path": "djangoscraper\\migrations\\0002_initial.py", "repo_name": "taras/djangoscraper", "stars": 17, "license": "None", "language": "python", "size": 109}
{"docstring": "# Adding field 'Task.errors'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.add_column('scraper_task', 'errors', models.TextField(blank=True))\n# Adding field 'Task.result'\n        db.add_column('scraper_task', 'result', models.TextField(blank=True))", "path": "djangoscraper\\migrations\\0009_removed_result_and_errors_fields.py", "repo_name": "taras/djangoscraper", "stars": 17, "license": "None", "language": "python", "size": 109}
{"docstring": "# This is the Swedish hack. The Swedish language has two words for\n# \"one\" - \"en\" and \"ett\". Sometimes \"en\" is used and other times\n# \"ett\" is used. For the word \"r,\" \"ett\" is used instead of \"en.\"\n# No doubt other languages contain similar weirdness.\n", "func_signal": "def amount_to_str(amount, unit_name):\n", "code": "if amount == 1 and unit_name == \"year\" and LANG == \"sv\":\n    return \"ett\"\nif amount in num_repr[LANG]:\n    return num_repr[LANG][amount]\nreturn str(amount)", "path": "djangoscraper\\utils\\timetext.py", "repo_name": "taras/djangoscraper", "stars": 17, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"\nReturns a tuple containing the most appropriate unit for the\nnumber of seconds supplied and the value in that units form.\n\n    >>> seconds_in_units(7700)\n    (2, 'hour')\n\"\"\"\n", "func_signal": "def seconds_in_units(seconds):\n", "code": "unit_limits = [(\"year\", 365 * 24 * 3600),\n               (\"month\", 30 * 24 * 3600),\n               (\"week\", 7 * 24 * 3600),\n               (\"day\", 24 * 3600),\n               (\"hour\", 3600),\n               (\"minute\", 60)]\nfor unit_name, limit in unit_limits:\n    if seconds >= limit:\n        amount = int(round(float(seconds) / limit))\n        return amount, unit_name\nreturn seconds, \"second\"", "path": "djangoscraper\\utils\\timetext.py", "repo_name": "taras/djangoscraper", "stars": 17, "license": "None", "language": "python", "size": 109}
{"docstring": "# Deleting field 'Task.errors'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.delete_column('scraper_task', 'errors')\n# Deleting field 'Task.result'\n        db.delete_column('scraper_task', 'result')", "path": "djangoscraper\\migrations\\0009_removed_result_and_errors_fields.py", "repo_name": "taras/djangoscraper", "stars": 17, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"Tests that lookups can be performed on data once stored in the database.\"\"\"\n", "func_signal": "def testLookups(self):\n", "code": "for value in self.testing_data:\n\tmodel_test = TestingModel(pickle_field=value)\n\tmodel_test.save()\n\tself.assertEquals(value, TestingModel.objects.get(pickle_field__exact=value).pickle_field)\n\tmodel_test.delete()", "path": "djangoscraper\\fields\\test.py", "repo_name": "taras/djangoscraper", "stars": 17, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"\nReturns a bytestring version of 's', encoded as specified in\n'encoding'.\n\"\"\"\n", "func_signal": "def smart_str(s, encoding='utf-8', errors='strict'):\n", "code": "if not isinstance(s, basestring):\n    try:\n        return str(s)\n    except UnicodeEncodeError:\n        return unicode(s).encode(encoding, errors)\nelif isinstance(s, unicode):\n    return s.encode(encoding, errors)\nelif s and encoding != 'utf-8':\n    return s.decode('utf-8', errors).encode(encoding, errors)\nelse:\n    return s", "path": "djangoscraper\\utils\\string.py", "repo_name": "taras/djangoscraper", "stars": 17, "license": "None", "language": "python", "size": 109}
{"docstring": "# Adding field 'Task.name'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.add_column('scraper_task', 'name', models.CharField(max_length=255))\n# Deleting field 'Task.callback'\n        db.delete_column('scraper_task', 'callback')", "path": "djangoscraper\\migrations\\0005_renamed_callback_to_name.py", "repo_name": "taras/djangoscraper", "stars": 17, "license": "None", "language": "python", "size": 109}
{"docstring": "# Changing field 'Task.errors'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.alter_column('scraper_task', 'errors', models.TextField())\n# Changing field 'Task.description'\n        db.alter_column('scraper_task', 'description', models.TextField())\n# Changing field 'Task.args'\n        db.alter_column('scraper_task', 'args', PickledObjectField())\n# Changing field 'Task.result'\n        db.alter_column('scraper_task', 'result', models.TextField())", "path": "djangoscraper\\migrations\\0007_adadded_blank_to_text_fields.py", "repo_name": "taras/djangoscraper", "stars": 17, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"Add options specific to this tool.\"\"\"\n\n", "func_signal": "def setOptions(self, optparser):\n", "code": "Tool.setOptions(self, optparser)\noptparser.add_option(\"-r\", \"--renderer\", type=\"string\",\n                     default=\"aqsis\",\n                     help=\"Renderer to use\")\noptparser.add_option(\"-I\", \"--include\", action=\"append\", default=[],\n                     help=\"Add include path for shader compilation\")\noptparser.add_option(\"-B\", \"--bake\", action=\"store_true\", default=False,\n                     help=\"Bake a texture map\")", "path": "render.py", "repo_name": "yamins81/cgkit", "stars": 18, "license": "other", "language": "python", "size": 3286}
{"docstring": "\"\"\"Set a new value for a particular hat.\n\nThis method sends a JOYSTICK_HAT event.\n\n\\param hat (\\c int) Hat index\n\\param x (\\c int) X value\n\\param y (\\c int) Y value        \n\"\"\"\n", "func_signal": "def setHat(self, hat, x, y):\n", "code": "self.hat[hat]=(x,y)\ne = JoystickHatEvent(self.id, hat, x, y)\neventManager().event(JOYSTICK_HAT, e)", "path": "cgkit\\joystick.py", "repo_name": "yamins81/cgkit", "stars": 18, "license": "other", "language": "python", "size": 3286}
{"docstring": "\"\"\"Create and initialize the job directory.\n\njobType is the name of the job procedure that should be created. Any\nadditional keyword arguments are passed to the constructor of the job\nprocedure.\nReturns the job directory.\n\"\"\"\n\n", "func_signal": "def _initJobDir(self, jobType, **params):\n", "code": "if type(jobType) is not str:\n    raise TypeError(\"Job type must be a string\")\n\n# Create an instance of the job procedure to validate the parameters and get the job label\njobProc = self._instantiateJobProc(jobType, **params)\n\n# Create the root job directory...\njobDir = self._createJobDir()\n# This handle is only used for retrieving file locations (so the root is set to None)\njobHandle = JobHandle(jobDir, None)\n\n# Call the job procedure's postCreate method\njobProc.postCreate(jobDir)\njobProc._jobDir = jobDir\n\n# Write the label\nlabel = jobProc.label\nif label is not None:\n    f = open(jobHandle.labelFile, \"wt\")\n    f.write(label)\n    f.close()\n\n# Write the job procedure description. \n# If this job node is the job root then write the file under a temporary\n# name (so that the job won't get picked up yet)\nprocDefFile = jobHandle.procDefFile\nif self is self._jobRoot:\n    procDefFile += \"_tmp\"\nself._writeJobDef(procDefFile, jobType, params)\n\nreturn jobDir", "path": "cgkit\\jobqueue\\jobqueue.py", "repo_name": "yamins81/cgkit", "stars": 18, "license": "other", "language": "python", "size": 3286}
{"docstring": "\"\"\"Read the job queue config file.\n\nUpdates the local attributes that are initialized from the config file.\n\"\"\"\n", "func_signal": "def _readConfigFile(self, fileName):\n", "code": "self._logger.info(\"Read config file %s\"%fileName)\ncp = configparser.ConfigParser()\n# Override the optionxform method so that it returns the input string\n# unmodified. We need case-senstivity as some options are actually\n# argument names for the job procedures.\ncp.optionxform = str\ncp.read(fileName)\n\n# This dictionary defines the valid options that may appear in the\n# [main] section. The values are the default values which also \n# define the valid type of the variable.\ncfgDict = {\"keepJobsInRepository\":self.keepJobsInRepository,\n           \"useSymLinks\":self.useSymLinks}\n\n# Check if there are unknown options on the main section and issue\n# warnings if there are...\nif cp.has_section(\"main\"):\n    knownOptNames = {}\n    for key in cfgDict.keys():\n        knownOptNames[key.lower()] = 1\n    opts = cp.options(\"main\")\n    for opt in opts:\n        if opt.lower() not in knownOptNames:\n            self._logger.warning('Unknown job queue config variable \"%s\" in file %s\\n'%(opt, fileName))\n\n# Get the values from the main section. The config variable values\n# are set as local attributes.\nfor key in cfgDict.keys():\n    defaultVal = cfgDict[key]\n    val = defaultVal\n    if cp.has_option(\"main\", key):\n        if type(defaultVal) is bool:\n            val = cp.getboolean(\"main\", key)\n        else:\n            raise TypeError(\"Internal error: Unknown config var type\")\n    setattr(self, key, val)\n\n# Read the proc defaults\nfor section in cp.sections():\n    if section.endswith(\" proc\"):\n        defaultParams = {}\n        procName = section[:-5]\n        for option in cp.options(section):\n            val = cp.get(section, option)\n            defaultParams[option] = val\n        self._defaultProcParams[procName] = defaultParams", "path": "cgkit\\jobqueue\\jobqueue.py", "repo_name": "yamins81/cgkit", "stars": 18, "license": "other", "language": "python", "size": 3286}
{"docstring": "\"\"\"Set the state of a particular button.\n\nThis method sends either a JOYSTICK_BUTTON_DOWN or\nJOYSTICK_BUTTON_UP event.\n\n\\param button (\\c int) Button index\n\\param value (\\c bool) State of the button (True = pressed)\n\"\"\"\n", "func_signal": "def setButton(self, button, value):\n", "code": "self.button[button] = value\ne = JoystickButtonEvent(self.id, button)\nif value:\n    eventManager().event(JOYSTICK_BUTTON_DOWN, e)                \nelse:\n    eventManager().event(JOYSTICK_BUTTON_UP, e)", "path": "cgkit\\joystick.py", "repo_name": "yamins81/cgkit", "stars": 18, "license": "other", "language": "python", "size": 3286}
{"docstring": "\"\"\"Return a button state.\n\n\\return Button state (\\c bool)\n\"\"\"\n", "func_signal": "def getButton(self, button):\n", "code": "if button>=self.numbuttons:\n    return False\n\nreturn self.button[button]", "path": "cgkit\\joystick.py", "repo_name": "yamins81/cgkit", "stars": 18, "license": "other", "language": "python", "size": 3286}
{"docstring": "\"\"\"Constructor.\n\njobQueue is the JobQueue object that this job is associated with.\njobType is a string containing the name of the job class that should\nbe instantiated.\nparams are the job parameters which must be passed as keyword\narguments.\n\"\"\"\n", "func_signal": "def __init__(self, jobQueue, jobType, **params):\n", "code": "if not isinstance(jobQueue, JobQueue):\n    raise TypeError(\"jobQueue must be a JobQueue object\")\nself._jobQueue = jobQueue\n# This counter is used to create the job directory names in the job repository dir\nself._subJobCounter = 0\n# Initial value for the redundant dependency removal part\nself._nextDepNr = 0\n# This flag determines whether the jobs will be kept in the job repository\n# or moved out into the dependency hierarchy when _writeDependencies()\n# is called on any job. \nself._keepJobsInRepository = self._jobQueue.keepJobsInRepository\n# Can we use sym links or do we have to emulate them?\nself._useSymLinks = self._jobQueue.useSymLinks\n\nJob.__init__(self, self, jobType, **params)\n\n# Overwrite the isInsideRepository flag (as the root is never inside the repo)\nself._isInsideRepository = False", "path": "cgkit\\jobqueue\\jobqueue.py", "repo_name": "yamins81/cgkit", "stars": 18, "license": "other", "language": "python", "size": 3286}
{"docstring": "\"\"\"Create a link at dst pointing to src.\n\"\"\"\n", "func_signal": "def _linkJobDir(self, src, dst):\n", "code": "if not src.startswith(self._location):\n    raise ValueError(\"Cannot link directories outside the job root\")\n\nif self._useSymLinks:\n    # Create a sym link\n    os.symlink(src, dst)\nelse:\n    # Create a file that contains the target path\n    f = open(dst, \"wt\")\n    f.write(\"[link]\\ntarget=$JOBROOT%s\\n\"%src[len(self._location):])\n    f.close()", "path": "cgkit\\jobqueue\\jobqueue.py", "repo_name": "yamins81/cgkit", "stars": 18, "license": "other", "language": "python", "size": 3286}
{"docstring": "\"\"\"Check if a job with the given sub-jobs is ready to run.\n\nsubJobs is a sequence of JobHandle objects (obtained from job.listSubJobs()).\nReturns True when all jobs in subJobs have been finished without\nan error.\n\"\"\"\n", "func_signal": "def _isReady(self, subJobs):\n", "code": "for job in subJobs:\n    if not job.isFinished() or job.hasError():\n        return False\nreturn True", "path": "cgkit\\jobqueue\\jobqueue.py", "repo_name": "yamins81/cgkit", "stars": 18, "license": "other", "language": "python", "size": 3286}
{"docstring": "\"\"\"Create a new sub-job.\n\n*jobType* is the name of the job procedure that should be created. Any\nadditional keyword arguments are passed to the constructor of the job\nprocedure.\n\nReturns a :class:`Job<jobqueue.Job>` object that represents the newly created job.\n\nThis method is equivalent to creating a job object manually and\ncalling ``addDependency(job)``.\n\"\"\"\n", "func_signal": "def createJob(self, jobType, **params):\n", "code": "job = Job(self._jobRoot, jobType, **params)\nself.addDependency(job)\nreturn job", "path": "cgkit\\jobqueue\\jobqueue.py", "repo_name": "yamins81/cgkit", "stars": 18, "license": "other", "language": "python", "size": 3286}
{"docstring": "\"\"\"Determine the name of the job directory and create it.\n\"\"\"\n# Create the root job directory...\n", "func_signal": "def _createJobDir(self):\n", "code": "for i in range(10):\n    jobDir = self._jobQueue._newJobDir()\n    try:\n        os.mkdir(jobDir)\n        break\n    except OSError:\n        pass\nelse:\n    raise JobQueueError(\"Failed to create a new job directory\")\n\nreturn jobDir", "path": "cgkit\\jobqueue\\jobqueue.py", "repo_name": "yamins81/cgkit", "stars": 18, "license": "other", "language": "python", "size": 3286}
{"docstring": "\"\"\"Set the middle position of an axis or all axes.\n\nSets the joystick position that will be mapped to the value 0.0.\nThis means, if the joystick outputs the (uncalibrated) value\n\\a mid, then the calibrated value will just be 0.0.\n\n\\param axis (\\c int) Axis index (None = all axes)\n\\param mid (\\c float) Middle value (None = take current value)\n\"\"\"\n", "func_signal": "def setAxisMiddle(self, axis=None, mid=None):\n", "code": "if axis==None:\n    axes = range(self.numaxes)\nelse:\n    axes = [axis]\n\nfor i in axes:\n    if mid==None:\n        mid = self.axis[i]\n\n    self.axis_mid[i] = mid", "path": "cgkit\\joystick.py", "repo_name": "yamins81/cgkit", "stars": 18, "license": "other", "language": "python", "size": 3286}
{"docstring": "\"\"\"Return the output file name.\n\noutput is the 'output' parameter.\n\"\"\"\n# No output?\n", "func_signal": "def outputName(self, output):\n", "code": "if output==None:\n    return \"\"\n# Is output a string? (i.e. the output file name)\nelif isinstance(output, types.StringTypes):\n    return output\n# User specified output specs? (output must already be a list\n# of specs)\nelse:\n    if len(output)>0:\n        return output[0][0]\n    else:\n        return \"\"", "path": "render.py", "repo_name": "yamins81/cgkit", "stars": 18, "license": "other", "language": "python", "size": 3286}
{"docstring": "\"\"\"Create a new directory for a sub-job.\n\nNote: The sub-job may be anywhere in the final dependency graph.\n\"\"\"\n", "func_signal": "def _createSubJobDir(self):\n", "code": "self._subJobCounter += 1\n# The directory where all sub-jobs are stored\nsubJobRepo = self._getSubJobRepo()\n# Create the repo dir if it doesn't already exist\nif not os.path.exists(subJobRepo):\n    os.mkdir(subJobRepo)\n# Create the actual sub-job directory\nsubJobDir = os.path.join(self._location, \".jobs\", \"j%d\"%self._subJobCounter)\nos.mkdir(subJobDir)\nreturn subJobDir", "path": "cgkit\\jobqueue\\jobqueue.py", "repo_name": "yamins81/cgkit", "stars": 18, "license": "other", "language": "python", "size": 3286}
{"docstring": "\"\"\"Find the next job that should be processed.\n\njobs is a sequence of JobHandle objects. The method returns a JobHandle\nobject that is the \"deepest\" job that is in waiting state.\n\"\"\"\n# Ignore any job that is not currently waiting\n", "func_signal": "def _findNextWaitingJob(self, jobs):\n", "code": "jobs = filter(lambda job: job.isWaiting(), jobs)\n\n# Search for the first waiting sub-job...\nfor job in jobs:\n    subJobs = job.listSubJobs()\n    if self._isReady(subJobs):\n        return job\n            \n    # The job is not ready, so try to pick one of the sub-jobs...\n    j = self._findNextWaitingJob(subJobs)\n    if j is not None:\n        return j\n\nreturn None", "path": "cgkit\\jobqueue\\jobqueue.py", "repo_name": "yamins81/cgkit", "stars": 18, "license": "other", "language": "python", "size": 3286}
{"docstring": "\"\"\"Append the frame number to all outputs.\n\nThe return value is the same than output except that the file names\nhave an appended frame number.\n\"\"\"\n# No output?\n", "func_signal": "def appendFrameNr(self, output, framenr):\n", "code": "if output==None:\n    return None\n# Is output a string? (i.e. the output file name)\nelif isinstance(output, types.StringTypes):\n    return self._appendFrameNr(output, framenr)\n# User specified output specs? (output must already be a list\n# of specs)\nelse:\n    res = []\n    for name,type,mode,params in output:\n        name = self._appendFrameNr(name,framenr)\n        res.append((name,type,mode,params))\n    return res", "path": "render.py", "repo_name": "yamins81/cgkit", "stars": 18, "license": "other", "language": "python", "size": 3286}
{"docstring": "\"\"\"Create a new top-level job.\n\n*jobType* is the name of the job procedure that should be created. Any\nadditional keyword arguments are passed to the constructor of the job\nprocedure.\n\nReturns a :class:`JobRoot<cgkit.jobqueue.jobqueue.JobRoot>` object that\nrepresents the newly created job.\nSub-jobs can be created by calling :meth:`createJob()<cgkit.jobqueue.jobqueue.JobRoot.createJob>`\non the returned job root.\n\nAfter the entire job hierarchy has been created, the\n:meth:`activate()<cgkit.jobqueue.jobqueue.JobRoot.activate>`\nmethod must be called on the job root, otherwise the job will not\nbe processed.\n\"\"\"\n", "func_signal": "def createJobRoot(self, jobType=None, **params):\n", "code": "jobRoot = JobRoot(self, jobType, **params) \nreturn jobRoot", "path": "cgkit\\jobqueue\\jobqueue.py", "repo_name": "yamins81/cgkit", "stars": 18, "license": "other", "language": "python", "size": 3286}
{"docstring": "\"\"\"Return an axis value.\n\n\\return Calibrated axis value (\\c float).\n\"\"\"\n", "func_signal": "def getAxis(self, axis):\n", "code": "if axis>=self.numaxes:\n    return 0.0\n    \nreturn self._calibratedAxis(axis, self.axis[axis])", "path": "cgkit\\joystick.py", "repo_name": "yamins81/cgkit", "stars": 18, "license": "other", "language": "python", "size": 3286}
{"docstring": "\"\"\"Append the frame number to a file name.\n\nReturns the new file name.\n\"\"\"\n", "func_signal": "def _appendFrameNr(self, filename, framenr):\n", "code": "name, ext = os.path.splitext(filename)\nreturn \"%s%04d%s\"%(name, framenr, ext)", "path": "render.py", "repo_name": "yamins81/cgkit", "stars": 18, "license": "other", "language": "python", "size": 3286}
{"docstring": "\"\"\"Constructor.\n\njobRoot is the JobRoot object that this job belongs to.\njobType is a string containing the name of the job class that should\nbe instantiated.\nparams are the job parameters which must be passed as keyword\narguments.\n\"\"\"\n", "func_signal": "def __init__(self, jobRoot, jobType, **params):\n", "code": "if not isinstance(jobRoot, JobRoot):\n    raise TypeError(\"jobRoot must be a JobRoot object\")\nif not isinstance(jobType, basestring):\n    raise TypeError(\"Job type must be a string\")\n\n# The root of the job\nself._jobRoot = jobRoot\n# The job proc class name\nself._jobType = jobType\n# The job proc parameters\nself._params = params\n# A list of Job object which this job depends on\nself._dependencies = []\n\n# This is used later on to detect cycles\nself._marked = False\n# The maximum distance to a leaf (job nodes without a dependency \n# always have a value of 0). This is used to sort the dependencies\n# with respect to the longest path in that sub-tree which is necessary\n# to remove redundant dependencies.\nself._maxLeafDist = 0\n# This is used later on to remove redundant dependencies\nself._depNr = 0\n\n# The job location on disk.\nself._location = self._initJobDir(jobType, **params)\n# Indicates whether the job directory is inside the repository or not\n# (all jobs begin in the repository except for the root job)\nself._isInsideRepository = True", "path": "cgkit\\jobqueue\\jobqueue.py", "repo_name": "yamins81/cgkit", "stars": 18, "license": "other", "language": "python", "size": 3286}
{"docstring": "\"\"\" Page title must be a WikiWord.\n\"\"\"\n", "func_signal": "def clean_title(self):\n", "code": "title = self.cleaned_data['title']\nif not wikiword_pattern.match(title):\n    raise forms.ValidationError(_('Must be a WikiWord.'))\n\nreturn title", "path": "wiki\\forms.py", "repo_name": "brosner/django-wikiapp", "stars": 16, "license": "mit", "language": "python", "size": 99}
{"docstring": "\"\"\" Revert the article to a previuos state, by revision number.\n\"\"\"\n", "func_signal": "def revert_to(self, revision, editor_ip, editor=None):\n", "code": "changeset = self.changeset_set.get(revision=revision)\nchangeset.reapply(editor_ip, editor)", "path": "wiki\\models.py", "repo_name": "brosner/django-wikiapp", "stars": 16, "license": "mit", "language": "python", "size": 99}
{"docstring": "\"\"\"Returns the text rendered by the Creole markup.\n\"\"\"\n", "func_signal": "def creole(text, **kw):\n", "code": "if Creole is None and settings.DEBUG:\n    raise template.TemplateSyntaxError(\"Error in creole filter: \"\n        \"The Creole library isn't installed, try easy_install Creoleparser.\")\nparser = CreoleParser(dialect=dialect)\nreturn parser.render(text)", "path": "wiki\\templatetags\\wiki_markup.py", "repo_name": "brosner/django-wikiapp", "stars": 16, "license": "mit", "language": "python", "size": 99}
{"docstring": "\"\"\" Mark the Article as 'removed'. If the article is\nalready removed, delete it.\nReturns True if the article was deleted, False when just marked\nas removed.\n\"\"\"\n", "func_signal": "def remove(self):\n", "code": "if self.removed:\n    self.delete()\n    return True\nelse:\n    self.removed = True\n    self.save()\n    return False", "path": "wiki\\models.py", "repo_name": "brosner/django-wikiapp", "stars": 16, "license": "mit", "language": "python", "size": 99}
{"docstring": "\"\"\" Send a message to the user if there is another user\nediting this article.\n\"\"\"\n", "func_signal": "def create_message(self, request):\n", "code": "if not self.is_mine(request):\n    user = request.user\n    user.message_set.create(\n        message=self.message_template%self.created_at)", "path": "wiki\\views.py", "repo_name": "brosner/django-wikiapp", "stars": 16, "license": "mit", "language": "python", "size": 99}
{"docstring": "\"\"\" Display an the body of an article, rendered with the right markup.\n\n- content_attr is the article attribute that will be rendered.\n- markup_attr is the article atribure with the markup that used\n  on the article.\n\nUse examples on templates:\n\n    {# article have a content and markup attributes #}\n    {% render_content article %}\n\n    {# article have a body and markup atributes #}\n    {% render_content article 'body' %}\n\n    {# we want to display the  summary instead #}\n    {% render_content article 'summary' %}\n\n    {# post have a tease and a markup_style attributes #}\n    {% render_content post 'tease' 'markup_style' %}\n\n    {# essay have a content and markup_lang attributes #}\n    {% render_content essay 'content' 'markup_lang' %}\n\n\"\"\"\n", "func_signal": "def render_content(article, content_attr='content', markup_attr='markup'):\n", "code": "return {\n    'content': getattr(article, content_attr),\n    'markup': getattr(article, markup_attr)\n}", "path": "wiki\\templatetags\\wiki_markup.py", "repo_name": "brosner/django-wikiapp", "stars": 16, "license": "mit", "language": "python", "size": 99}
{"docstring": "\"\"\"\nRender the ReStructuredText into html. Will pre-render template code first.\n\nExample:\n::\n\n    {% restructuredtext %}\n        ===================================\n        To: {{ send_to }}\n        ===================================\n        {% include \"email_form.rst\" %}\n    {% endrestructuredtext %}\n\n\"\"\"\n", "func_signal": "def rest_tag(parser, token):\n", "code": "nodelist = parser.parse(('endrestructuredtext',))\nparser.delete_first_token()\nreturn ReStructuredTextNode(nodelist)", "path": "wiki\\templatetags\\restructuredtext.py", "repo_name": "brosner/django-wikiapp", "stars": 16, "license": "mit", "language": "python", "size": 99}
{"docstring": "\"\"\" Returns the real user IP, even if behind a proxy.\nSet BEHIND_PROXY to True in your settings if Django is\nrunning behind a proxy.\n\"\"\"\n", "func_signal": "def get_real_ip(request):\n", "code": "if getattr(settings, 'BEHIND_PROXY', False):\n    return request.META['HTTP_X_FORWARDED_FOR']\nreturn request.META['REMOTE_ADDR']", "path": "wiki\\views.py", "repo_name": "brosner/django-wikiapp", "stars": 16, "license": "mit", "language": "python", "size": 99}
{"docstring": "\"\"\"Create a 'diff' from txt1 to txt2.\"\"\"\n", "func_signal": "def diff(txt1, txt2):\n", "code": "patch = dmp.patch_make(txt1, txt2)\nreturn dmp.patch_toText(patch)", "path": "wiki\\models.py", "repo_name": "brosner/django-wikiapp", "stars": 16, "license": "mit", "language": "python", "size": 99}
{"docstring": "\"\"\" Return True if the user have permission to edit Articles,\nFalse otherwise.\n\"\"\"\n", "func_signal": "def has_write_perm(user, group, is_member):\n", "code": "if (group is None) or (is_member is None) or is_member(user, group):\n    return True\nreturn False", "path": "wiki\\views.py", "repo_name": "brosner/django-wikiapp", "stars": 16, "license": "mit", "language": "python", "size": 99}
{"docstring": "\"\"\" Return the Article to this revision.\n\"\"\"\n\n# XXX Would be better to exclude reverted revisions\n#     and revisions previous/next to reverted ones\n", "func_signal": "def reapply(self, editor_ip, editor):\n", "code": "next_changes = self.article.changeset_set.filter(\n    revision__gt=self.revision).order_by('-revision')\n\narticle = self.article\n\ncontent = None\nfor changeset in next_changes:\n    if content is None:\n        content = article.content\n    patch = dmp.patch_fromText(changeset.content_diff)\n    content = dmp.patch_apply(patch, content)[0]\n\n    changeset.reverted = True\n    changeset.save()\n\nold_content = article.content\nold_title = article.title\nold_markup = article.markup\n\narticle.content = content\narticle.title = changeset.old_title\narticle.markup = changeset.old_markup\narticle.save()\n\narticle.new_revision(\n    old_content=old_content, old_title=old_title,\n    old_markup=old_markup,\n    comment=\"Reverted to revision #%s\" % self.revision,\n    editor_ip=editor_ip, editor=editor)\n\nself.save()\n\nif None not in (notification, self.editor):\n    notification.send([self.editor], \"wiki_revision_reverted\",\n                      {'revision': self, 'article': self.article})", "path": "wiki\\models.py", "repo_name": "brosner/django-wikiapp", "stars": 16, "license": "mit", "language": "python", "size": 99}
{"docstring": "\"\"\" Return the ContentType of the object's model.\n\"\"\"\n", "func_signal": "def get_ct(obj):\n", "code": "return ContentType.objects.get(app_label=obj._meta.app_label,\n                               model=obj._meta.module_name)", "path": "wiki\\utils.py", "repo_name": "brosner/django-wikiapp", "stars": 16, "license": "mit", "language": "python", "size": 99}
{"docstring": "\"\"\" Return True if the user has permission to *read*\nArticles, False otherwise.\n\"\"\"\n", "func_signal": "def has_read_perm(user, group, is_member, is_private):\n", "code": "if (group is None) or (is_member is None) or is_member(user, group):\n    return True\nif (is_private is not None) and is_private(group):\n    return False\nreturn True", "path": "wiki\\views.py", "repo_name": "brosner/django-wikiapp", "stars": 16, "license": "mit", "language": "python", "size": 99}
{"docstring": "''' Returns a HTML representation of the diff.\n'''\n\n# well, it *will* be the old content\n", "func_signal": "def display_diff(self):\n", "code": "old_content = self.article.content\n\n# newer non-reverted revisions of this article, starting from this\nnewer_changesets = ChangeSet.non_reverted_objects.filter(\n    article=self.article,\n    revision__gte=self.revision)\n\n# apply all patches to get the content of this revision\nfor i, changeset in enumerate(newer_changesets):\n    patches = dmp.patch_fromText(changeset.content_diff)\n    if len(newer_changesets) == i+1:\n        # we need to compare with the next revision after the change\n        next_rev_content = old_content\n    old_content = dmp.patch_apply(patches, old_content)[0]\n\ndiffs = dmp.diff_main(old_content, next_rev_content)\nreturn dmp.diff_prettyHtml(diffs)", "path": "wiki\\models.py", "repo_name": "brosner/django-wikiapp", "stars": 16, "license": "mit", "language": "python", "size": 99}
{"docstring": "\"\"\"\nReturns an absolute URL matching given url name with its parameters,\ngiven the articles group and (optional) article and revision number.\n\nThis is a way to define links that aren't tied to our URL configuration::\n\n    {% wikiurl edit group article %}\n\nThe first argument is a url name, without the ``wiki_`` prefix.\n\nFor example if you have a view ``app_name.client`` taking client's id and\nthe corresponding line in a URLconf looks like this::\n\n    url('^edit/(\\w+)/$', 'wiki.edit_article', name='wiki_edit')\n\nand this app's URLconf is included into the project's URLconf under some\npath::\n\n    url('^groups/(?P<group_slug>\\w+)/mywiki/', include('wiki.urls'), kwargs)\n\nthen in a template you can create a link to edit a certain article like this::\n\n    {% wikiurl edit group article %}\n\nThe URL will look like ``groups/some_group/mywiki/edit/WikiWord/``.\n\nThis tag is also able to set a context variable instead of returning the\nfound URL by specifying it with the 'as' keyword::\n\n    {% wikiurl edit group article as wiki_article_url %}\n\n\"\"\"\n", "func_signal": "def wikiurl(parser, token):\n", "code": "bits = token.contents.split(' ')\nkwargs = {}\nif len(bits) == 3: # {% wikiurl url_name group %}\n    url_name = bits[1]\n    group = parser.compile_filter(bits[2])\nelif len(bits) == 4: # {% wikiurl url_name group article %}\n    url_name = bits[1]\n    group = parser.compile_filter(bits[2])\n    kwargs['article'] = parser.compile_filter(bits[3])\nelif len(bits) == 5: # {% wikiurl url_name group as var %} or {% wikiurl url_name group article revision %}\n    url_name = bits[1]\n    group = parser.compile_filter(bits[2])\n    if bits[3] == \"as\":\n        kwargs['asvar'] = bits[4]\n    else:\n        kwargs['article'] = parser.compile_filter(bits[3])\n        kwargs['revision'] = parser.compile_filter(bits[4])\nelif len(bits) == 6: # {% wikiurl url_name group article as var %}\n    if bits[4] == \"as\":\n        raise TemplateSyntaxError(\"4th argument to %s should be 'as'\" % bits[0])\n    url_name = bits[1]\n    group = parser.compile_filter(bits[2])\n    kwargs['article'] = parser.compile_filter(bits[3])\n    kwargs['asvar'] = parser.compile_filter(bits[5])\nelif len(bits) == 7: # {% wikiurl url_name group article revision as var %}\n    url_name = bits[1]\n    group = parser.compile_filter(bits[2])\n    kwargs['article'] = parser.compile_filter(bits[3])\n    kwargs['revision'] = parser.compile_filter(bits[4])\n    kwargs['asvar'] = parser.compile_filter(bits[6])\nelse:\n    raise TemplateSyntaxError(\"wrong number of arguments to %s\" % bits[0])\nreturn WikiURLNode(url_name, group, **kwargs)", "path": "wiki\\templatetags\\wikiurl.py", "repo_name": "brosner/django-wikiapp", "stars": 16, "license": "mit", "language": "python", "size": 99}
{"docstring": "\"\"\" Saves the article with a new revision.\n\"\"\"\n", "func_signal": "def save(self, force_insert=False, force_update=False):\n", "code": "if self.id is None:\n    try:\n        self.revision = ChangeSet.objects.filter(\n            article=self.article).latest().revision + 1\n    except self.DoesNotExist:\n        self.revision = 1\nsuper(ChangeSet, self).save(force_insert, force_update)", "path": "wiki\\models.py", "repo_name": "brosner/django-wikiapp", "stars": 16, "license": "mit", "language": "python", "size": 99}
{"docstring": "## RED_FLAG: need to catch the explicit exceptions and not a catch all...\n", "func_signal": "def restructuredtext_has_errors(value, do_raise=False):\n", "code": "try:\n    restructuredparts(value, halt_level=2, traceback=1)\n    return False\nexcept:\n    if do_raise:\n        raise\nreturn True", "path": "wiki\\templatetags\\restructuredtext.py", "repo_name": "brosner/django-wikiapp", "stars": 16, "license": "mit", "language": "python", "size": 99}
{"docstring": "# 0 - Extra data\n", "func_signal": "def save(self):\n", "code": "editor_ip = self.cleaned_data['user_ip']\ncomment = self.cleaned_data['comment']\n\n# 1 - Get the old stuff before saving\nif self.instance.id is None:\n    old_title = old_content = old_markup = ''\n    new = True\nelse:\n    old_title = self.instance.title\n    old_content = self.instance.content\n    old_markup = self.instance.markup\n    new = False\n\n# 2 - Save the Article\narticle = super(ArticleForm, self).save()\n\n# 3 - Set creator and group\neditor = getattr(self, 'editor', None)\ngroup = getattr(self, 'group', None)\nif new:\n    article.creator_ip = editor_ip\n    if editor is not None:\n        article.creator = editor\n        article.group = group\n    article.save()\n\n# 4 - Create new revision\nchangeset = article.new_revision(\n    old_content, old_title, old_markup,\n    comment, editor_ip, editor)\n\nreturn article, changeset", "path": "wiki\\forms.py", "repo_name": "brosner/django-wikiapp", "stars": 16, "license": "mit", "language": "python", "size": 99}
{"docstring": "\"\"\"The django version of this markup filter has an issue when only one title or subtitle is supplied in that\nthey are dropped from the markup. This is due to the use of 'fragment' instead of something like 'html_body'.\nWe do not want to use 'html_body' either due to some header/footer stuff we want to prevent, but we want to\nkeep the title and subtitle. So we include them if present.\"\"\"\n", "func_signal": "def restructuredtext(value, **overrides):\n", "code": "parts = restructuredparts(value, **overrides)\nif not isinstance(parts, dict):\n    return value\nreturn parts[\"html_body\"]", "path": "wiki\\templatetags\\restructuredtext.py", "repo_name": "brosner/django-wikiapp", "stars": 16, "license": "mit", "language": "python", "size": 99}
{"docstring": "\"\"\" Display a <h1> title for the wiki, with a link to the group main page.\n\"\"\"\n", "func_signal": "def wiki_title(group):\n", "code": "return {'group_name': group.name,\n        'group_type': group._meta.verbose_name.title(),\n        'group_url': group.get_absolute_url()}", "path": "wiki\\templatetags\\wiki_tags.py", "repo_name": "brosner/django-wikiapp", "stars": 16, "license": "mit", "language": "python", "size": 99}
{"docstring": "\"\"\"\nRun all physical checks on a raw input line.\n\"\"\"\n", "func_signal": "def check_physical(self, line):\n", "code": "self.physical_line = line\nif self.indent_char is None and len(line) and line[0] in ' \\t':\n    self.indent_char = line[0]\nfor name, check, argument_names in self.physical_checks:\n    result = self.run_check(check, argument_names)\n    if result is not None:\n        offset, text = result\n        self.report_error(self.line_number, offset, text, check)", "path": "tools\\pep8.py", "repo_name": "swc/comicnamer", "stars": 18, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nThe {}.has_key() method will be removed in the future version of\nPython. Use the 'in' operation instead, like:\nd = {\"a\": 1, \"b\": 2}\nif \"b\" in d:\n    print d[\"b\"]\n\"\"\"\n", "func_signal": "def python_3000_has_key(logical_line):\n", "code": "pos = logical_line.find('.has_key(')\nif pos > -1:\n    return pos, \"W601 .has_key() is deprecated, use 'in'\"", "path": "tools\\pep8.py", "repo_name": "swc/comicnamer", "stars": 18, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Returns list of files found at path\n\"\"\"\n", "func_signal": "def findFiles(self):\n", "code": "if os.path.isfile(self.path):\n    if self._checkExtension(self.path):\n        return [os.path.abspath(self.path)]\n    else:\n        return []\nelif os.path.isdir(self.path):\n    return self._findFilesInPath(self.path)\nelse:\n    raise InvalidPath(\"%s is not a valid file/directory\" % self.path)", "path": "comicnamer\\utils.py", "repo_name": "swc/comicnamer", "stars": 18, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Takes an array of paths, returns all files found\n\"\"\"\n", "func_signal": "def findFiles(paths):\n", "code": "valid_files = []\n\nfor cfile in paths:\n    cur = FileFinder(\n        cfile,\n        with_extension = Config['valid_extensions'],\n        recursive = Config['recursive'])\n\n    try:\n        valid_files.extend(cur.findFiles())\n    except InvalidPath:\n        warn(\"Invalid path: %s\" % cfile)\n\nif len(valid_files) == 0:\n    raise NoValidFilesFoundError()\n\n# Remove duplicate files (all paths from FileFinder are absolute)\nvalid_files = list(set(valid_files))\n\nreturn valid_files", "path": "comicnamer\\main.py", "repo_name": "swc/comicnamer", "stars": 18, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Main comicnamer function, takes an array of paths, does stuff.\n\"\"\"\n# Warn about move_files function\n", "func_signal": "def comicnamer(paths):\n", "code": "if Config['move_files_enable']:\n    import warnings\n    warnings.warn(\"The move_files feature is still under development. \"\n        \"Be very careful with it.\\n\"\n        \"It has not been heavily tested, and is not recommended for \"\n        \"general use yet.\")\n\np(\"#\" * 20)\np(\"# Starting comicnamer\")\n\nissues_found = []\n\nfor cfile in findFiles(paths):\n    cfile = cfile.decode(\"utf-8\")\n    parser = FileParser(cfile)\n    try:\n        issue = parser.parse()\n    except InvalidFilename:\n        warn(\"Invalid filename %s\" % cfile)\n    else:\n        issues_found.append(issue)\n\nif len(issues_found) == 0:\n    raise NoValidFilesFoundError()\n\np(\"# Found %d issue\" % len(issues_found) + (\"s\" * (len(issues_found) > 1)))\n\n# Sort issues by volume name and issue number\nissues_found.sort(key = lambda x: (x.volumename, x.issuenumbers))\n\ncomicvine_instance = Comicvine(\n    interactive=not Config['select_first'])\n\nfor issue in issues_found:\n    processFile(comicvine_instance, issue)\n    p('')\n\np(\"#\" * 20)\np(\"# Done\")", "path": "comicnamer\\main.py", "repo_name": "swc/comicnamer", "stars": 18, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nJCR: Trailing whitespace is superfluous.\n\"\"\"\n", "func_signal": "def trailing_whitespace(physical_line):\n", "code": "physical_line = physical_line.rstrip('\\n') # chr(10), newline\nphysical_line = physical_line.rstrip('\\r') # chr(13), carriage return\nphysical_line = physical_line.rstrip('\\x0c') # chr(12), form feed, ^L\nstripped = physical_line.rstrip()\nif physical_line != stripped:\n    return len(stripped), \"W291 trailing whitespace\"", "path": "tools\\pep8.py", "repo_name": "swc/comicnamer", "stars": 18, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nCheck all Python source files in this directory and all subdirectories.\n\"\"\"\n", "func_signal": "def input_dir(dirname):\n", "code": "dirname = dirname.rstrip('/')\nif excluded(dirname):\n    return\nfor root, dirs, files in os.walk(dirname):\n    if options.verbose:\n        message('directory ' + root)\n    options.counters['directories'] = \\\n        options.counters.get('directories', 0) + 1\n    dirs.sort()\n    for subdir in dirs:\n        if excluded(subdir):\n            dirs.remove(subdir)\n    files.sort()\n    for filename in files:\n        input_file(os.path.join(root, filename))", "path": "tools\\pep8.py", "repo_name": "swc/comicnamer", "stars": 18, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nCheck if options.ignore contains a prefix of the error code.\n\"\"\"\n", "func_signal": "def ignore_code(code):\n", "code": "for ignore in options.ignore:\n    if code.startswith(ignore):\n        return True", "path": "tools\\pep8.py", "repo_name": "swc/comicnamer", "stars": 18, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nCompound statements (multiple statements on the same line) are\ngenerally discouraged.\n\"\"\"\n", "func_signal": "def compound_statements(logical_line):\n", "code": "line = logical_line\nfound = line.find(':')\nif -1 < found < len(line) - 1:\n    before = line[:found]\n    if (before.count('{') <= before.count('}') and # {'a': 1} (dict)\n        before.count('[') <= before.count(']') and # [1:2] (slice)\n        not re.search(r'\\blambda\\b', before)):     # lambda x: x\n        return found, \"E701 multiple statements on one line (colon)\"\nfound = line.find(';')\nif -1 < found:\n    return found, \"E702 multiple statements on one line (semicolon)\"", "path": "tools\\pep8.py", "repo_name": "swc/comicnamer", "stars": 18, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nReport an error, according to options.\n\"\"\"\n", "func_signal": "def report_error(self, line_number, offset, text, check):\n", "code": "if options.quiet == 1 and not self.file_errors:\n    message(self.filename)\nself.file_errors += 1\ncode = text[:4]\noptions.counters[code] = options.counters.get(code, 0) + 1\noptions.messages[code] = text[5:]\nif options.quiet:\n    return\nif options.testsuite:\n    base = os.path.basename(self.filename)[:4]\n    if base == code:\n        return\n    if base[0] == 'E' and code[0] == 'W':\n        return\nif ignore_code(code):\n    return\nif options.counters[code] == 1 or options.repeat:\n    message(\"%s:%s:%d: %s\" %\n            (self.filename, line_number, offset + 1, text))\n    if options.show_source:\n        line = self.lines[line_number - 1]\n        message(line.rstrip())\n        message(' ' * offset + '^')\n    if options.show_pep8:\n        message(check.__doc__.lstrip('\\n').rstrip())", "path": "tools\\pep8.py", "repo_name": "swc/comicnamer", "stars": 18, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Takes a list of issue names, formats them into a string.\nIf two names are supplied, such as \"Pilot (1)\" and \"Pilot (2)\", the\nreturned string will be \"Pilot (1-2)\"\n\nIf two different issue names are found, such as \"The first\", and\n\"Something else\" it will return \"The first, Something else\"\n\"\"\"\n", "func_signal": "def formatIssueName(names, join_with):\n", "code": "if len(names) == 1:\n    return names[0]\n\nfound_names = []\nnumbers = []\nfor cname in names:\n    number = re.match(\"(.*) \\(([0-9]+)\\)$\", cname)\n    if number:\n        issname, issno = number.group(1), number.group(2)\n        if len(found_names) > 0 and issname not in found_names:\n            return join_with.join(names)\n        found_names.append(issname)\n        numbers.append(int(issno))\n    else:\n        # An issue didn't match\n        return join_with.join(names)\n\nnames = []\nstart, end = min(numbers), max(numbers)\nnames.append(\"%s (%d-%d)\" % (found_names[0], start, end))\nreturn join_with.join(names)", "path": "comicnamer\\utils.py", "repo_name": "swc/comicnamer", "stars": 18, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nUses the following config options:\nfilename_with_issue # Filename when issue name is found\nfilename_without_issue # Filename when no issue can be found\nissue_single # formatting for a single issue number\nissue_separator # used to join multiple issue numbers\n\"\"\"\n# Format issue number into string, or a list\n", "func_signal": "def generateFilename(self):\n", "code": "issno = Config['issue_single'] % self.issuenumbers[0]\n\n# Data made available to config'd output file format\nif self.extension is None:\n    prep_extension = ''\nelse:\n    prep_extension = '.%s' % self.extension\n\nissdata = {\n    'volumename': self.volumename,\n    'issue': issno,\n    'issuename': self.issuename,\n    'ext': prep_extension}\n\nif (self.issuename is None or (isinstance(self.issuename, list) and self.issuename[0] is None)):\n    fname = Config['filename_without_issue'] % issdata\nelse:\n    if isinstance(self.issuename, list):\n        issdata['issuename'] = formatIssueName(\n            self.issuename,\n            join_with = Config['multiiss_join_name_with']\n        )\n\n    fname = Config['filename_with_issue'] % issdata\n\nreturn makeValidFilename(\n    fname,\n    normalize_unicode = Config['normalize_unicode_filenames'],\n    windows_safe = Config['windows_safe_filenames'],\n    replace_with = Config['replace_invalid_characters_with'])", "path": "comicnamer\\utils.py", "repo_name": "swc/comicnamer", "stars": 18, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nReplace contents with 'xxx' to prevent syntax matching.\n\n>>> mute_string('\"abc\"')\n'\"xxx\"'\n>>> mute_string(\"'''abc'''\")\n\"'''xxx'''\"\n>>> mute_string(\"r'abc'\")\n\"r'xxx'\"\n\"\"\"\n", "func_signal": "def mute_string(text):\n", "code": "start = 1\nend = len(text) - 1\n# String modifiers (e.g. u or r)\nif text.endswith('\"'):\n    start += text.index('\"')\nelif text.endswith(\"'\"):\n    start += text.index(\"'\")\n# Triple quotes\nif text.endswith('\"\"\"') or text.endswith(\"'''\"):\n    start += 2\n    end -= 2\nreturn text[:start] + 'x' * (end - start) + text[end:]", "path": "tools\\pep8.py", "repo_name": "swc/comicnamer", "stars": 18, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nGet the next line from the input buffer.\n\"\"\"\n", "func_signal": "def readline(self):\n", "code": "self.line_number += 1\nif self.line_number > len(self.lines):\n    return ''\nreturn self.lines[self.line_number - 1]", "path": "tools\\pep8.py", "repo_name": "swc/comicnamer", "stars": 18, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nReturn the amount of indentation.\nTabs are expanded to the next multiple of 8.\n\n>>> expand_indent('    ')\n4\n>>> expand_indent('\\\\t')\n8\n>>> expand_indent('    \\\\t')\n8\n>>> expand_indent('       \\\\t')\n8\n>>> expand_indent('        \\\\t')\n16\n\"\"\"\n", "func_signal": "def expand_indent(line):\n", "code": "result = 0\nfor char in line:\n    if char == '\\t':\n        result = result / 8 * 8 + 8\n    elif char == ' ':\n        result += 1\n    else:\n        break\nreturn result", "path": "tools\\pep8.py", "repo_name": "swc/comicnamer", "stars": 18, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Cleans up volume name by removing any . and _\ncharacters, along with any trailing hyphens.\n\nIs basically equivalent to replacing all _ and . with a\nspace, but handles decimal numbers in string, for example:\n\n>>> cleanRegexedvolumeName(\"an.example.1.0.test\")\n'an example 1.0 test'\n>>> cleanRegexedvolumeName(\"an_example_1.0_test\")\n'an example 1.0 test'\n\"\"\"\n", "func_signal": "def cleanRegexedvolumeName(volumename):\n", "code": "volumename = re.sub(\"(\\D)[.](\\D)\", \"\\\\1 \\\\2\", volumename)\nvolumename = re.sub(\"(\\D)[.]\", \"\\\\1 \", volumename)\nvolumename = re.sub(\"[.](\\D)\", \" \\\\1\", volumename)\nvolumename = volumename.replace(\"_\", \" \")\nvolumename = re.sub(\"-$\", \"\", volumename)\nreturn volumename.strip()", "path": "comicnamer\\utils.py", "repo_name": "swc/comicnamer", "stars": 18, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nJCR: The last line should have a newline.\n\"\"\"\n", "func_signal": "def missing_newline(physical_line):\n", "code": "if physical_line.rstrip() == physical_line:\n    return len(physical_line), \"W292 no newline at end of file\"", "path": "tools\\pep8.py", "repo_name": "swc/comicnamer", "stars": 18, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nAvoid extraneous whitespace in the following situations:\n\n- Immediately inside parentheses, brackets or braces.\n\n- Immediately before a comma, semicolon, or colon.\n\"\"\"\n", "func_signal": "def extraneous_whitespace(logical_line):\n", "code": "line = logical_line\nfor char in '([{':\n    found = line.find(char + ' ')\n    if found > -1:\n        return found + 1, \"E201 whitespace after '%s'\" % char\nfor char in '}])':\n    found = line.find(' ' + char)\n    if found > -1 and line[found - 1] != ',':\n        return found, \"E202 whitespace before '%s'\" % char\nfor char in ',;:':\n    found = line.find(' ' + char)\n    if found > -1:\n        return found, \"E203 whitespace before '%s'\" % char", "path": "tools\\pep8.py", "repo_name": "swc/comicnamer", "stars": 18, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Format issue number(s) into string, using configured values\n\"\"\"\n", "func_signal": "def formatIssueNumbers(issuenumbers):\n", "code": "if len(issuenumbers) == 1:\n    issno = Config['issue_single'] % issuenumbers[0]\nelse:\n    issno = Config['issue_separator'].join(\n        Config['issue_single'] % x for x in issuenumbers)\n\nreturn issno", "path": "comicnamer\\utils.py", "repo_name": "swc/comicnamer", "stars": 18, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Constructs the location to move/copy the file\n\"\"\"\n\n# Calls makeValidFilename on volume name, as it must valid for a filename\n", "func_signal": "def getDestinationFolder(issue):\n", "code": "destdir = Config['move_files_destination'] % {\n    'volumename': makeValidFilename(issue.volumename),\n    'issuenumbers': makeValidFilename(formatIssueNumbers(issue.issuenumbers))\n}\nreturn destdir", "path": "comicnamer\\main.py", "repo_name": "swc/comicnamer", "stars": 18, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"compute entropy of rhyming pairs\"\"\"\n", "func_signal": "def rhyming_entropy(stanzaschemes, stanzas):\n", "code": "pairs=defaultdict(int)\ntotalpairs=0.0\nfor scheme, stanza in zip(stanzaschemes, stanzas):\n    for i, (schemei, wordi) in enumerate(zip(scheme, stanza)):\n        for (schemej, wordj) in zip(scheme[i+1:], stanza[i+1:]):\n            totalpairs+=1\n            if schemei==schemej:\n                if wordi<=wordj:\n                    pairs[(wordi, wordj)]+=1\n                else:\n                    pairs[(wordj, wordi)]+=1\n#normalize\nfor pair in pairs:\n    pairs[pair]=pairs[pair]/totalpairs\n#compute entropy\nreturn sum(map(lambda paircount: -paircount*math.log(paircount, 2), pairs.values()))", "path": "evaluate.py", "repo_name": "sravanareddy/rhymediscovery", "stars": 16, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"initialize probs according to simple measure of orthographic similarity\"\"\"\n", "func_signal": "def init_basicortho_ttable(words):\n", "code": "n=len(words)\nt_table=numpy.zeros((n, n+1)) \n\n#initialize P(c|r) accordingly\nfor r, w in enumerate(words):\n    for c, v in enumerate(words):\n        if c<r:\n            t_table[r, c]=t_table[c, r]  #similarity is symmetric\n        else:\n            t_table[r, c]=basic_word_sim(w, v)+0.001  #for backoff\n    t_table[r, n]=random.random()  #no estimate for P(r|no history)\n\n#normalize\nfor c in range(n+1):\n    tot=float(sum(t_table[:, c]))\n    for r in range(n):\n        t_table[r, c]=t_table[r, c]/tot\n\nreturn t_table", "path": "findschemes.py", "repo_name": "sravanareddy/rhymediscovery", "stars": 16, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"transform poem into lists of rhymesets as given by rhyme scheme\"\"\"\n", "func_signal": "def get_rhymelists(poem, scheme):\n", "code": "rhymelists=defaultdict(list)\nfor poemword, schemeword in zip(poem, scheme):\n    rhymelists[schemeword].append(poemword)\nreturn map(sorted, rhymelists.values())", "path": "evaluate.py", "repo_name": "sravanareddy/rhymediscovery", "stars": 16, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"transform stanza into ordered lists of rhymesets as given by rhyme scheme\"\"\"\n", "func_signal": "def get_rhymelists(stanza, scheme):\n", "code": "rhymelists=defaultdict(list)\nfor stanzaword, schemeword in zip(stanza, scheme):\n    rhymelists[schemeword].append(stanzaword)\nreturn rhymelists.values()", "path": "findschemes.py", "repo_name": "sravanareddy/rhymediscovery", "stars": 16, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"write gold standard\"\"\"\n", "func_signal": "def save_gold_std(stanzaschemes, poemschemes, poems, filename):\n", "code": "o=codecs.open(filename, 'w', 'utf-8')\nschemectr=0\nfor pi, poem in enumerate(poems):\n    for stanza in poem:\n        stanzascheme=stanzaschemes[schemectr]\n        poemscheme=poemschemes[schemectr]\n        o.write('POEM'+str(pi)+' '+unicode(' '.join(stanza))+'\\n')\n        o.write(' '.join(stanzascheme)+'\\n')\n        o.write(' '.join(poemscheme)+'\\n\\n')\n        schemectr+=1\no.close()", "path": "evaluate.py", "repo_name": "sravanareddy/rhymediscovery", "stars": 16, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Load raw stanzas from gold standard file\"\"\"\n", "func_signal": "def load_stanzas(filename):\n", "code": "f=codecs.open(filename, 'r', 'utf8').readlines()\nstanzas=[]\nfor i, line in enumerate(f):\n    line=line.split()\n    if i%4==0:\n        stanzas.append(line[1:])\nreturn stanzas", "path": "findschemes.py", "repo_name": "sravanareddy/rhymediscovery", "stars": 16, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"get all words\"\"\"\n", "func_signal": "def get_wordset(stanzas):\n", "code": "words=sorted(list(set(reduce(lambda x, y: x+y, stanzas))))\nreturn words", "path": "findschemes.py", "repo_name": "sravanareddy/rhymediscovery", "stars": 16, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"write rhyme schemes at convergence\"\"\"\n", "func_signal": "def show_rhymes(probs, stanzas, schemes, outfile):\n", "code": "o=codecs.open(outfile, 'w', 'utf8')\nfor stanza, stanzaprobs in zip(stanzas, probs):\n    #scheme with highest probability\n    bestscheme=schemes[len(stanza)][numpy.argmax(numpy.array(stanzaprobs))]\n    o.write(' '.join(stanza)+'\\n')\n    o.write(' '.join(map(str, bestscheme))+'\\n\\n')\no.close()", "path": "findschemes.py", "repo_name": "sravanareddy/rhymediscovery", "stars": 16, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"normalize posterior probs\"\"\"\n", "func_signal": "def e_norm_post(probs):\n", "code": "normprobs=[]\nfor stanzaprobs in probs:\n    tot=sum(stanzaprobs)\n    if tot>0:\n        normstanzaprobs=map(lambda myprob: myprob/tot, stanzaprobs)\n    else:\n        normstanzaprobs=stanzaprobs[:]\n    normprobs.append(normstanzaprobs)\nreturn normprobs", "path": "findschemes.py", "repo_name": "sravanareddy/rhymediscovery", "stars": 16, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"normalize counts to get conditional probs\"\"\"\n", "func_signal": "def m_norm_frac(tc_table, n, rprobs):\n", "code": "t_table=numpy.zeros((n, n+1))\n\nfor c in range(n+1):\n    tot=sum(tc_table[:, c])\n    if tot==0:\n        continue\n    for r in range(n):\n        t_table[r, c]=tc_table[r, c]/tot\n\n\ntotrprob=sum(rprobs.values())\nfor scheme in rprobs:\n    rprobs[scheme]=rprobs[scheme]/totrprob\n\n\nreturn [t_table, rprobs]", "path": "findschemes.py", "repo_name": "sravanareddy/rhymediscovery", "stars": 16, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"find 'less naive' baseline (most common scheme of a given length in subcorpus)\"\"\"\n", "func_signal": "def lessnaive(gold_schemes):\n", "code": "best_schemes=defaultdict(lambda : defaultdict(int))\nfor g in gold_schemes:\n    best_schemes[len(g)][tuple(g)]+=1\n\nm=sum(map(len, best_schemes.values()))\n\nfor i in best_schemes:\n    best_schemes[i]=list(max(best_schemes[i].items(), key=lambda x:x[1])[0])\n\nnaive_schemes=[]\nfor g in gold_schemes:\n    naive_schemes.append(best_schemes[len(g)])\nreturn naive_schemes", "path": "evaluate.py", "repo_name": "sravanareddy/rhymediscovery", "stars": 16, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"generate scheme from seed\"\"\"\n", "func_signal": "def gen_pattern(seed, n):\n", "code": "seed=map(int, seed)\nif n%len(seed)>0:\n    return \"Error!\"\npattern=seed[:]\nincrement=max(seed)\nwhile len(pattern)<n:\n    pattern+=map(lambda seedunit: seedunit+increment, seed)\n    increment=max(pattern)\nreturn map(str, pattern)", "path": "evaluate.py", "repo_name": "sravanareddy/rhymediscovery", "stars": 16, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"find fractional pseudocounts (maximization step)\"\"\"\n", "func_signal": "def m_frac_counts(words, stanzas, schemes, normprobs):\n", "code": "n=len(words)\ntc_table=numpy.zeros((n, n+1))\nrprobs=defaultdict(float)\nfor stanza, stanzaprobs in zip(stanzas, normprobs):\n    myschemes=schemes[len(stanza)]\n    for myscheme, myprob in zip(myschemes, stanzaprobs):\n\n        rprobs[tuple(myscheme)]+=myprob  \n\n        rhymelists=get_rhymelists(stanza, myscheme)\n        for rhymelist in rhymelists:\n            for i, w in enumerate(rhymelist):\n                r=words.index(w)\n                tc_table[r, n]+=myprob\n                for v in rhymelist[:i]+rhymelist[i+1:]:\n                    c=words.index(v)\n                    tc_table[r, c]+=myprob\n\n\nreturn [tc_table, rprobs]", "path": "findschemes.py", "repo_name": "sravanareddy/rhymediscovery", "stars": 16, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"get all words\"\"\"\n", "func_signal": "def get_wordset(poems):\n", "code": "words=sorted(list(set(reduce(lambda x, y: x+y, poems))))\nreturn words", "path": "evaluate.py", "repo_name": "sravanareddy/rhymediscovery", "stars": 16, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"load rhyme schemes from pickled file\"\"\"\n", "func_signal": "def load_schemes(schemefile):\n", "code": "schemes=pickle.load(open(schemefile))\nfor i in schemes:\n    schemes[i]=map(lambda x:map(int, x[0].split()), schemes[i])  #remove freq and convert to list of integers\nreturn schemes", "path": "findschemes.py", "repo_name": "sravanareddy/rhymediscovery", "stars": 16, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"initialize (normalized) theta uniformly\"\"\"\n", "func_signal": "def init_uniform_ttable(words):\n", "code": "n=len(words)\nt_table=numpy.zeros((n, n+1))  \nuni_prob=1/float(n)\nfor c in range(n+1):\n    for r in range(n):\n        t_table[r, c]=uni_prob   \nreturn t_table", "path": "findschemes.py", "repo_name": "sravanareddy/rhymediscovery", "stars": 16, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"compute entropy of rhyme schemes\"\"\"\n", "func_signal": "def scheme_entropy(stanzaschemes, stanzas):\n", "code": "schemes=defaultdict(float)\nfor scheme, stanza in zip(stanzaschemes, stanzas):\n    schemes[tuple(scheme)]+=1.0\n#normalize\ntotal=len(stanzaschemes)\nfor scheme in schemes:\n    schemes[scheme]=schemes[scheme]/total\n#compute entropy\nreturn sum(map(lambda schemecount: -schemecount*math.log(schemecount, 2), schemes.values()))", "path": "evaluate.py", "repo_name": "sravanareddy/rhymediscovery", "stars": 16, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"posterior prob of a scheme for a stanza, with prob of every word in rhymelist rhyming with all one before it\"\"\"\n", "func_signal": "def post_prob_scheme(t_table, words, stanza, myscheme):\n", "code": "myprob=1.0\nn=len(words)\nrhymelists=get_rhymelists(stanza, myscheme)\nplen=len(stanza)\nfor rhymelist in rhymelists:\n    for i, w in enumerate(rhymelist):            \n        r=words.index(w)\n        if i==0:  #first word, use P(w|x)\n            myprob=myprob*t_table[r, n]\n        else:\n            for v in rhymelist[:i]:  #history\n                c=words.index(v)\n                myprob*=t_table[r, c]\nif myprob==0 and len(stanza)>30: #probably underflow\n    myprob=1e-300\nreturn myprob", "path": "findschemes.py", "repo_name": "sravanareddy/rhymediscovery", "stars": 16, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"find naive baseline (most common scheme of a given length)?\"\"\"\n", "func_signal": "def naive(gold_schemes):\n", "code": "dist = pickle.load(open('allschemes.pickle'))\nbest_schemes={}\nfor i in dist:\n    if dist[i]==[]:\n        continue\n    best_schemes[i]=(max(dist[i], key=lambda x:x[1])[0]).split()\n\nnaive_schemes=[]\nfor g in gold_schemes:\n    naive_schemes.append(best_schemes[len(g)])\nreturn naive_schemes", "path": "evaluate.py", "repo_name": "sravanareddy/rhymediscovery", "stars": 16, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"assign equal prob to every scheme\"\"\"\n", "func_signal": "def init_uniform_r(schemes):\n", "code": "rprobs={}\nnumschemes=float(sum(map(len, schemes.values())))\nuni_prob=1/numschemes\n\nfor i in schemes:\n    for scheme in schemes[i]:\n        rprobs[tuple(scheme)]=uni_prob\n\nreturn rprobs", "path": "findschemes.py", "repo_name": "sravanareddy/rhymediscovery", "stars": 16, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"\nKeeps a reference to the client instance\nwhen its session has been created.\n\nSubscribes to the ``main`` channel of the bus.\n\"\"\"\n", "func_signal": "def ready(self, client):\n", "code": "self.client = client\nself.bus.subscribe(\"main\", self._process)", "path": "headstock\\lib\\cot.py", "repo_name": "Lawouach/headstock", "stars": 18, "license": "None", "language": "python", "size": 464}
{"docstring": "\"\"\"\nHandles the session elements received by the server.\n\nIf the type of the response is `result` it raises\n:class:`headstock.error.HeadstockAuthenticationSuccess` instance\nhandled by the client indicating the session is ready.\n\nOtherwise returns the session stanza indicating the\nclient wishes to start a session.\n\"\"\"\n", "func_signal": "def handle_session(self, e):\n", "code": "if e.xml_parent and e.xml_parent.get_attribute_value('type') == 'result':\n    raise HeadstockSessionBound()\n\niq = Stanza.set_iq(stanza_id=generate_unique())\nE(u'session', namespace=XMPP_SESSION_NS, parent=iq)\n\nreturn iq", "path": "headstock\\stream.py", "repo_name": "Lawouach/headstock", "stars": 18, "license": "None", "language": "python", "size": 464}
{"docstring": "\"\"\"\nCreates and returns a stream header string\n\"\"\"\n", "func_signal": "def stream_header(self):\n", "code": "return '<stream:stream xmlns:stream=\"%s\" xmlns=\"jabber:client\" version=\"1.0\" to=\"%s\">' % (XMPP_STREAM_NS,\n                                                                                          self.jid.domain)", "path": "headstock\\stream.py", "repo_name": "Lawouach/headstock", "stars": 18, "license": "None", "language": "python", "size": 464}
{"docstring": "\"\"\"\nHandles the authentication by computing the\nchallenge for the provided credentials.\n\"\"\"\n", "func_signal": "def handle_challenge(self, e):\n", "code": "response_token = None\nparams = challenge_to_dict(e.xml_text)\n# Handling 'rspauth' token in DIGEST-MD5\n# See section 2.1.3 of RFC 2831\nif 'rspauth' not in params:\n    digest_uri = 'xmpp/%s' % self.jid.domain\n    password = self.password\n    response_token = compute_digest_response(params, self.jid.node,\n                                             password, digest_uri=digest_uri)\n    \nreturn E(u'response', content=response_token, namespace=XMPP_SASL_NS)", "path": "headstock\\stream.py", "repo_name": "Lawouach/headstock", "stars": 18, "license": "None", "language": "python", "size": 464}
{"docstring": "\"\"\"\nGenerator that will yield each managed\nstanza one by one.\n\"\"\"\n", "func_signal": "def stanzas(self):\n", "code": "for stanza in self._stanzas:\n    yield stanza", "path": "headstock\\lib\\cot.py", "repo_name": "Lawouach/headstock", "stars": 18, "license": "None", "language": "python", "size": 464}
{"docstring": "\"\"\"\nLoads a cot script given by ``path`` and returns the parsed\nstanzas as a list of dictionaries.\n\nThe dictionnaries have two numeric keys:\n\n* 0 which represents the stanzas to be sent\n* 1 representing the expected stanzas\n\nThe values of the dictionnaries are also dictionaries\nwith keys being tuples (stanza_id, stanza_type). For each\nkey the associated value is a list of `bridge.Element` instances,\neach representing a stanza.\n\"\"\"\n", "func_signal": "def load(path):\n", "code": "script = file(path, 'r')\n\nmode = SEND_MODE\n\ninside_command = False\nbuf = []\n\nstanzas = []\ncurrent_test_name = None\n\nfor line in script:\n    line = line.strip()\n    if not inside_command:\n        if not line:\n            continue\n    \n        if line.startswith('#'):\n            continue\n        \n        m = command_regex.match(line)\n        if not m:\n            continue\n    \n        result = m.groups()\n        command = result[0] or result[2]\n        if command == 'send':\n            current_test_name = result[1]\n            stanzas.append({SEND_MODE: {}, EXPECT_MODE: {}})\n            mode = SEND_MODE\n        elif command == 'expect':\n            mode = EXPECT_MODE\n        inside_command = True\n    elif line.endswith('}') and inside_command:\n        inside_command = False\n        xml = u'<root xmlns=\"%s\">' % XMPP_CLIENT_NS\n        xml += ''.join(buf)\n        xml += u'</root>'\n        root = E.load(xml).xml_root\n        \n        for child in root.xml_children:\n            child.xml_parent = None\n            stanza_id = child.get_attribute_value('id')\n            stanza_type = child.get_attribute_value('type')\n            stanzas[-1][mode][(stanza_id, stanza_type)] = (current_test_name, child)\n\n        buf = []\n    else:\n        buf.append(line)\n\nscript.close()\nreturn stanzas", "path": "headstock\\lib\\cot.py", "repo_name": "Lawouach/headstock", "stars": 18, "license": "None", "language": "python", "size": 464}
{"docstring": "\"\"\"\nHandle the JID binding request by returning\nthe full JID.\n\"\"\"\n", "func_signal": "def handle_binding(self, e):\n", "code": "iq = Stanza.set_iq(stanza_id=generate_unique())\nbind = E(u'bind', namespace=XMPP_BIND_NS, parent=iq)\nif self.jid.resource != None:\n    E(u'resource', content=self.jid.resource,\n      namespace=XMPP_BIND_NS, parent=bind)\n\nreturn iq", "path": "headstock\\stream.py", "repo_name": "Lawouach/headstock", "stars": 18, "license": "None", "language": "python", "size": 464}
{"docstring": "\"\"\"\nReturns the client domain.\n\"\"\"\n", "func_signal": "def hostname(self):\n", "code": "if self.client:\n    return self.client.jid.domain", "path": "headstock\\lib\\cot.py", "repo_name": "Lawouach/headstock", "stars": 18, "license": "None", "language": "python", "size": 464}
{"docstring": "\"\"\"\nCreates and returns a stream header string\n\"\"\"\n", "func_signal": "def stream_header(self):\n", "code": "return '<stream:stream xmlns:stream=\"%s\" xmlns=\"jabber:component:accept\" to=\"%s\">' % (XMPP_STREAM_NS,\n                                                                                      self.jid.domain)", "path": "headstock\\stream.py", "repo_name": "Lawouach/headstock", "stars": 18, "license": "None", "language": "python", "size": 464}
{"docstring": "\"\"\"\nRemoves the BOM from the provided `text`.\n\"\"\"\n", "func_signal": "def remove_BOM(text):\n", "code": "if codecs.BOM_UTF8.decode(\"utf-8\") in text:\n    return text.replace(codecs.BOM_UTF8.decode(\"utf-8\"), '')\nif codecs.BOM.decode(\"utf-16\") in text:\n    return text.replace(codecs.BOM.decode(\"utf-16\"), '')\nif codecs.BOM_BE.decode(\"utf-16-be\") in text:\n    return text.replace(codecs.BOM_BE.decode(\"utf-16-be\"), '')\nif codecs.BOM_LE.decode(\"utf-16-le\") in text:\n    return text.replace(codecs.BOM_LE.decode(\"utf-16-le\"), '')\n\nreturn text", "path": "headstock\\lib\\utils.py", "repo_name": "Lawouach/headstock", "stars": 18, "license": "None", "language": "python", "size": 464}
{"docstring": "\"\"\"\nGenerates a random and pseudo-unique string.\n\n``seed`` None - if `None` the seed is generated\nfrom the current time and a random value.\n\"\"\"\n", "func_signal": "def generate_unique(seed=None):\n", "code": "if not seed:\n    seed = str(time() * random())\nreturn unicode(abs(hash(sha(seed).hexdigest())))", "path": "headstock\\lib\\utils.py", "repo_name": "Lawouach/headstock", "stars": 18, "license": "None", "language": "python", "size": 464}
{"docstring": "\"\"\"\nCreates and returns a :class:`bridge.Element` instance from a\n:class:`headstock.lib.stanza.Stanza` instance.\n\n``e`` :class:`headstock.lib.stanza.Stanza` instance\n\n``parent`` :class:`bridge.Element` instance to which\nattached the generated element.\n\"\"\"\n", "func_signal": "def to_element(e, parent=None):\n", "code": "attributes = {}\nif e.from_jid:\n    attributes[u'from'] = unicode(e.from_jid)\nif e.to_jid:\n    attributes[u'to'] = unicode(e.to_jid)\nif e.type:\n    attributes[u'type'] = e.type\nif e.stanza_id:\n    attributes[u'id'] = e.stanza_id\n    \nstanza = E(e.stanza, attributes=attributes, namespace=e.ns,\n           parent=parent)\n\nif e.lang:\n    A(u'lang', value=e.lang, prefix=XML_PREFIX,\n      namespace=XML_NS, parent=stanza)\n\nreturn stanza", "path": "headstock\\lib\\stanza.py", "repo_name": "Lawouach/headstock", "stars": 18, "license": "None", "language": "python", "size": 464}
{"docstring": "\"\"\"\nHelper method that generates a :class:`headstock.lib.stanza.Stanza` instance\nof a IQ stanza with a `result` type.\n\"\"\"\n", "func_signal": "def result_iq(from_jid=None, to_jid=None, stanza_id=None):\n", "code": "return Stanza.to_element(Stanza(u'iq', from_jid=from_jid, to_jid=to_jid,\n                                type=u'result', stanza_id=stanza_id))", "path": "headstock\\lib\\stanza.py", "repo_name": "Lawouach/headstock", "stars": 18, "license": "None", "language": "python", "size": 464}
{"docstring": "\"\"\"\nSends the next available stanza and register\nthe `_handle_response` method to each expected stanza.\n\"\"\"\n", "func_signal": "def _next(self):\n", "code": "self._last = 0\nself.processing = True\n\nstanza_id, stanza_type = self.sendable.next()\n\nself.current_test_name, stanza = self.current[SEND_MODE][(stanza_id, stanza_type)]\nstanza = stanza.xml(omit_declaration=True).replace(\"$(hostname)\",\n                                                   self.hostname)\ne = E.load(stanza).xml_root\n\nfor stanza_id, stanza_type in self.current[EXPECT_MODE]:\n    self.keys.append((stanza_id, stanza_type))\n    self.client.register_on_iq(self._handle_response,\n                               id=stanza_id, type=stanza_type,\n                               once=True)\nself._last = time.time()\nself.client.send_stanza(e)", "path": "headstock\\lib\\cot.py", "repo_name": "Lawouach/headstock", "stars": 18, "license": "None", "language": "python", "size": 464}
{"docstring": "\"\"\"\nXMPP handler for stream features.\n\nIt will:\n\n* return immediatly if the element has no children.\n* initiates the TLS negociation (from the stream point\nof view) if `self.tls` is `True` and the feature has a\n`<starttls /> child.\n* initiates the authentication based on the supported\nmechanisms or abort if none is found.        \n\"\"\"\n", "func_signal": "def handle_features(self, e):\n", "code": "if not e.xml_children:\n    return\n\nif self.use_tls and e.has_child('starttls', XMPP_TLS_NS):\n    return \"<starttls xmlns='%s' />\" % XMPP_TLS_NS\n\n# We don't actually handle registration here\n# but if the register module has been loaded\n# we do not want to interfere by trying to authenticate straight away either\nif self.register and e.has_child('register', \"http://jabber.org/features/iq-register\"):\n    return\n\nmech = e.get_child('mechanisms', XMPP_SASL_NS)\nmechanisms = []\nif mech:\n    mechanisms = []\n    for m in mech.xml_children:\n        if m.is_mixed_content():\n            mechanisms.append(m.collapse(separator=''))\n        else:\n            mechanisms.append(m.xml_text)\n\nmechanism = None\n\n# Always favour DIGEST-MD5 if supported by receiving entity\nif u'DIGEST-MD5' in mechanisms:\n    mechanism = u'DIGEST-MD5'\n    token = None\nelif u'PLAIN' in mechanisms:\n    mechanism = u'PLAIN'\n    email = '%s@%s' % (self.jid.node, self.jid.domain)\n    password = self.password\n    token = generate_credential(email, self.jid.node, password)\nelif u'X-GOOGLE-TOKEN' in mechanisms:\n    mechanism = u'X-GOOGLE-TOKEN'\n    password = self.password\n    token = perform_authentication(self.jid.node, password)\nelif u'ANONYMOUS' in mechanisms:            \n    mechanism = u'ANONYMOUS'\n    token = None\nelse:\n    # We don't support any of the proposed mechanism\n    # let's abort the SASL exchange\n    return E(u'abort', namespace=XMPP_SASL_NS)\n\nreturn E(u'auth', content=token,\n         attributes={u'mechanism': mechanism},\n         namespace=XMPP_SASL_NS)", "path": "headstock\\stream.py", "repo_name": "Lawouach/headstock", "stars": 18, "license": "None", "language": "python", "size": 464}
{"docstring": "\"\"\"\nCreates and returns the IQ stanza to query the entity's roster.\n\"\"\"\n", "func_signal": "def ask_roster(self):\n", "code": "iq = Stanza.get_iq(from_jid=unicode(self.jid), stanza_id=generate_unique())\nE(u'query', namespace=XMPP_ROSTER_NS, parent=iq)   \n\nreturn iq", "path": "headstock\\stream.py", "repo_name": "Lawouach/headstock", "stars": 18, "license": "None", "language": "python", "size": 464}
{"docstring": "\"\"\"\nDecorator to wrap a callable so that it can be used as\na XMPP handler by the headstock client.\n\nIt will set various attributes to the wrapped callable:\n\n* handler: set to `True` to indicate the callable can participate to the dispatching\n* xmpp_local_name: XML element name\n* xmpp_ns: XML element namespace\n* fire_once: if `True`, this handler will be removed once it has been used.\n* forget: if set to `True`, the dispatched element will be automatically deleted once the handler has been called.\n\n``name`` XMPP stanza name\n\n``ns`` XMPP stanza namespace\n\n``once`` False - flag indicating if the handler should\nbe called only once and unregistered automatically\n\n``forget`` True - flag indicating if the dispatched\n:class:`bridge.Element` instance should be automatically\nforgotten once dispatched.\n\"\"\"\n", "func_signal": "def xmpphandler(name, ns, once=False, forget=True):\n", "code": "def wrapper(func):\n    func.handler = True\n    func.fire_once = once\n    func.forget = forget\n    func.xmpp_local_name = name\n    func.xmpp_ns = ns\n    return func\nreturn wrapper", "path": "headstock\\__init__.py", "repo_name": "Lawouach/headstock", "stars": 18, "license": "None", "language": "python", "size": 464}
{"docstring": "\"\"\"\nHelper method that generates a :class:`headstock.lib.stanza.Stanza` instance\nof a IQ stanza with a `set` type.\n\"\"\"\n", "func_signal": "def set_iq(from_jid=None, to_jid=None, stanza_id=None):\n", "code": "return Stanza.to_element(Stanza(u'iq', from_jid=from_jid, to_jid=to_jid,\n                                type=u'set', stanza_id=stanza_id))", "path": "headstock\\lib\\stanza.py", "repo_name": "Lawouach/headstock", "stars": 18, "license": "None", "language": "python", "size": 464}
{"docstring": "\"\"\"\nLoads a new cot script and adds it to the pool\nof managed stanza.\n\"\"\"\n", "func_signal": "def add_cot_script(self, cot_script):\n", "code": "stanzas = CotScript.load(cot_script)\nself._stanzas = itertools.chain(self._stanzas, stanzas)", "path": "headstock\\lib\\cot.py", "repo_name": "Lawouach/headstock", "stars": 18, "license": "None", "language": "python", "size": 464}
{"docstring": "\"\"\"\nHelper method that generates a :class:`headstock.lib.stanza.Stanza` instance\nof a IQ stanza with a `error` type.\n\"\"\"\n", "func_signal": "def error_iq(from_jid=None, to_jid=None, stanza_id=None):\n", "code": "return Stanza.to_element(Stanza(u'iq', from_jid=from_jid, to_jid=to_jid,\n                                type=u'error', stanza_id=stanza_id))", "path": "headstock\\lib\\stanza.py", "repo_name": "Lawouach/headstock", "stars": 18, "license": "None", "language": "python", "size": 464}
{"docstring": "\"\"\"\nserializes a CSSValue\n\"\"\"\n", "func_signal": "def do_css_CSSValue(self, cssvalue):\n", "code": "if not cssvalue:\n    return u''\nelse:\n    sep = u',%s' % self.prefs.listItemSpacer\n    out = []\n    for part in cssvalue.seq:\n        if hasattr(part, 'cssText'):\n            # comments or CSSValue if a CSSValueList\n            out.append(part.cssText)\n        elif isinstance(part, basestring) and part == u',':\n            out.append(sep)\n        else:\n            out.append(part)\n    return (u''.join(out)).strip()", "path": "cssutils\\serialize.py", "repo_name": "pib/PyBrowser", "stars": 18, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"\ncomma-separated list of Selectors\n\"\"\"\n", "func_signal": "def do_css_SelectorList(self, selectorlist):\n", "code": "if selectorlist.seq and self._wellformed(selectorlist) and\\\n                        self._valid(selectorlist):\n    out = []\n    sep = u',%s' % self.prefs.listItemSpacer\n    for part in selectorlist.seq:\n        if hasattr(part, 'cssText'):\n            out.append(part.cssText)", "path": "cssutils\\serialize.py", "repo_name": "pib/PyBrowser", "stars": 18, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"\nindent a block like a CSSStyleDeclaration to the given level\nwhich may be higher than self._level (e.g. for CSSStyleDeclaration)\n\"\"\"\n", "func_signal": "def _indentblock(self, text, level):\n", "code": "if not self.prefs.lineSeparator:\n    return text\nreturn self.prefs.lineSeparator.join(\n    [u'%s%s' % (level * self.prefs.indent, line)\n        for line in text.split(self.prefs.lineSeparator)]\n)", "path": "cssutils\\serialize.py", "repo_name": "pib/PyBrowser", "stars": 18, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"\na single selector including comments\n\nTODO: style combinators like + >\n\"\"\"\n", "func_signal": "def do_css_Selector(self, selector):\n", "code": "if selector.seq and self._wellformed(selector) and\\\n                        self._valid(selector):\n    out = []\n    for part in selector.seq:\n        if hasattr(part, 'cssText'):\n            out.append(part.cssText)\n        else:\n            if type(part) == dict:\n                out.append(part['value'])\n            else:\n                out.append(part)\n    return u''.join(out)\nelse: \n    return u''", "path": "cssutils\\serialize.py", "repo_name": "pib/PyBrowser", "stars": 18, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"\nmediaText\n    simple value or comma-separated list of media\n\nDOMException\n\n- SYNTAX_ERR: (MediaQuery)\n  Raised if the specified string value has a syntax error and is\n  unparsable.\n- NO_MODIFICATION_ALLOWED_ERR: (self)\n  Raised if this media list is readonly.\n\"\"\"\n", "func_signal": "def _setMediaText(self, mediaText):\n", "code": "self._checkReadonly()\nvalid = True\ntokenizer = self._tokenize2(mediaText)\nnewseq = []\n\nexpected = None\nwhile True:\n    # find all upto and including next \",\", EOF or nothing\n    mqtokens = self._tokensupto2(tokenizer, listseponly=True)\n    if mqtokens:\n        if self._tokenvalue(mqtokens[-1]) == ',':\n            expected = mqtokens.pop()\n        else:\n            expected = None\n\n        mq = MediaQuery(mqtokens)\n        if mq.valid:\n            newseq.append(mq)\n        else:\n            valid = False\n            self._log.error(u'MediaList: Invalid MediaQuery: %s' %\n                            self._valuestr(mqtokens))\n    else:\n        break\n\n# post condition\nif expected:\n    valid = False\n    self._log.error(u'MediaList: Cannot end with \",\".')\n\nif valid:\n    del self[:]\n    for mq in newseq:\n        self.appendMedium(mq)", "path": "cssutils\\stylesheets\\medialist.py", "repo_name": "pib/PyBrowser", "stars": 18, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"\nStyle declaration of CSSStyleRule\n\"\"\"\n", "func_signal": "def do_css_CSSStyleDeclaration(self, style, separator=None):\n", "code": "if len(style.seq) > 0 and self._wellformed(style) and\\\n                        self._valid(style):\n    if separator is None:\n        separator = self.prefs.lineSeparator\n\n    if self.prefs.keepAllProperties:\n        parts = style.seq\n    else:\n        # find distinct names\n        nnames = set()\n        for x in style.seq:\n            if isinstance(x, cssutils.css.Property):\n                nnames.add(x.normalname)\n        # filter list\n        parts = []\n        for x in reversed(style.seq):\n            if isinstance(x, cssutils.css.Property):\n                if x.normalname in nnames:\n                    parts.append(x)\n                    nnames.remove(x.normalname)\n            else:\n                parts.append(x)\n        parts.reverse()\n\n    out = []\n    for (i, part) in enumerate(parts):\n        if isinstance(part, cssutils.css.CSSComment):\n            # CSSComment\n            if self.prefs.keepComments:\n                out.append(part.cssText)\n                out.append(separator)\n        elif isinstance(part, cssutils.css.Property):\n            # PropertySimilarNameList\n            out.append(self.do_Property(part))\n            if not (self.prefs.omitLastSemicolon and i==len(parts)-1):\n                out.append(u';')\n            out.append(separator)\n        else:\n            # other?\n            out.append(part)\n\n    if out and out[-1] == separator:\n        del out[-1]\n\n    return u''.join(out)\n\nelse:\n    return u''", "path": "cssutils\\serialize.py", "repo_name": "pib/PyBrowser", "stars": 18, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"\nused by all styledeclarations to get the propertyname used\ndependent on prefs setting defaultPropertyName\n\"\"\"\n", "func_signal": "def _getpropertyname(self, property, actual):\n", "code": "if self.prefs.defaultPropertyName and \\\n   not self.prefs.keepAllProperties:\n    return property.normalname\nelse:\n    return actual", "path": "cssutils\\serialize.py", "repo_name": "pib/PyBrowser", "stars": 18, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"\nserializes CSSMediaRule\n\n+ CSSComments\n\"\"\"\n", "func_signal": "def do_CSSMediaRule(self, rule):\n", "code": "rulesout = []\nfor r in rule.cssRules:\n    rtext = r.cssText\n    if rtext:\n        # indent each line of cssText\n        rulesout.append(self._indentblock(rtext, self._level + 1))\n        rulesout.append(self.prefs.lineSeparator)\n\nif not self.prefs.keepEmptyRules and not u''.join(rulesout).strip() or\\\n   self._noinvalids(rule.media):\n    return u''", "path": "cssutils\\serialize.py", "repo_name": "pib/PyBrowser", "stars": 18, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"\nserializes CSSFontFaceRule\n\nstyle\n    CSSStyleDeclaration\n\n+ CSSComments\n\"\"\"\n", "func_signal": "def do_CSSFontFaceRule(self, rule):\n", "code": "self._level += 1\ntry:\n    styleText = self.do_css_CSSStyleDeclaration(rule.style)\nfinally:\n    self._level -= 1\n\nif not styleText or self._noinvalids(rule):\n    return u''\n\nbefore = []\nfor x in rule.seq:\n    if hasattr(x, 'cssText'):\n        before.append(x.cssText)\n    else:\n        # TODO: resolve\n        raise SyntaxErr('serializing CSSFontFaceRule: unexpected %r' % x)\nif before:\n    before = u' '.join(before).strip()\n    if before:\n        before = u' %s' % before\nelse:\n    before = u''\n\nreturn u'%s%s {%s%s%s%s}' % (\n    self._getatkeyword(rule, u'@font-face'),\n    before,\n    self.prefs.lineSeparator,\n    self._indentblock(styleText, self._level + 1),\n    self.prefs.lineSeparator,\n    (self._level + 1) * self.prefs.indent\n    )", "path": "cssutils\\serialize.py", "repo_name": "pib/PyBrowser", "stars": 18, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"\nmediaText\n    unicodestring of parsable comma separared media\n    or a list of media\n\"\"\"\n", "func_signal": "def __init__(self, mediaText=None, readonly=False):\n", "code": "super(MediaList, self).__init__()\nself.valid = True\n\nif isinstance(mediaText, list):\n    mediaText = u','.join(mediaText)\n\nif mediaText:\n    self.mediaText = mediaText\n\nself._readonly = readonly", "path": "cssutils\\stylesheets\\medialist.py", "repo_name": "pib/PyBrowser", "stars": 18, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"\nserializes CSSStyleRule\n\nselectorList\nstyle\n\n+ CSSComments\n\"\"\"\n", "func_signal": "def do_CSSStyleRule(self, rule):\n", "code": "selectorText = self.do_css_SelectorList(rule.selectorList)\nif not selectorText or self._noinvalids(rule):\n    return u''\nself._level += 1\nstyleText = u''\ntry:\n    styleText = self.do_css_CSSStyleDeclaration(rule.style)\nfinally:\n    self._level -= 1\nif not styleText:\n        if self.prefs.keepEmptyRules:\n            return u'%s%s{}' % (selectorText,\n                                self.prefs.paranthesisSpacer)\nelse:\n    return u'%s%s{%s%s%s%s}' % (\n        selectorText,\n        self.prefs.paranthesisSpacer,\n        self.prefs.lineSeparator,\n        self._indentblock(styleText, self._level + 1),\n        self.prefs.lineSeparator,\n        (self._level + 1) * self.prefs.indent)", "path": "cssutils\\serialize.py", "repo_name": "pib/PyBrowser", "stars": 18, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"\na Properties priority \"!\" S* \"important\"\n\"\"\"\n", "func_signal": "def do_Property_priority(self, priorityseq):\n", "code": "out = []\nfor part in priorityseq:\n    if hasattr(part, 'cssText'): # comments\n        out.append(u' ')\n        out.append(part.cssText)\n        out.append(u' ')\n    else:\n        out.append(part)\nreturn u''.join(out).strip()", "path": "cssutils\\serialize.py", "repo_name": "pib/PyBrowser", "stars": 18, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"\n(DOM)\nAdds the medium newMedium to the end of the list. If the newMedium\nis already used, it is first removed.\n\nnewMedium\n    a string or a MediaQuery object\n\nreturns if newMedium is valid\n\nDOMException\n\n- INVALID_CHARACTER_ERR: (self)\n  If the medium contains characters that are invalid in the\n  underlying style language.\n- NO_MODIFICATION_ALLOWED_ERR: (self)\n  Raised if this list is readonly.\n\"\"\"\n", "func_signal": "def appendMedium(self, newMedium):\n", "code": "newMedium = self.__prepareset(newMedium)\n\nif newMedium:\n    mts = [self._normalize(mq.mediaType) for mq in self]\n    newmt = self._normalize(newMedium.mediaType)\n\n    if newmt in mts:\n        self.deleteMedium(newmt)\n        self.seq.append(newMedium)\n    elif u'all' == newmt:\n        # remove all except handheld (Opera)\n        h = None\n        for mq in self:\n            if mq.mediaType == u'handheld':\n                h = mq\n        del self[:]\n        self.seq.append(newMedium)\n        if h:\n            self.append(h)\n    elif u'all' in mts:\n        if u'handheld' == newmt:\n            self.seq.append(newMedium)\n    else:\n        self.seq.append(newMedium)\n\n    return True\n\nelse:\n    return False", "path": "cssutils\\stylesheets\\medialist.py", "repo_name": "pib/PyBrowser", "stars": 18, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"\nreturns uri enclosed in \" if necessary\n\"\"\"\n", "func_signal": "def _uri(self, uri):\n", "code": "if CSSSerializer.__notinurimatcher(uri):\n    return '\"%s\"' % uri\nelse:\n    return uri", "path": "cssutils\\serialize.py", "repo_name": "pib/PyBrowser", "stars": 18, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"\nserializes CSSNamespaceRule\n\nuri\n    string\nprefix\n    string\n\n+ CSSComments\n\"\"\"\n", "func_signal": "def do_CSSNamespaceRule(self, rule):\n", "code": "if not rule.namespaceURI or self._noinvalids(rule):\n    return u''\n\nout = [u'%s' % self._getatkeyword(rule, u'@namespace')]\nfor part in rule.seq:\n    if rule.prefix == part and part != u'':\n        out.append(u' %s' % part)\n    elif rule.namespaceURI == part:\n        out.append(u' \"%s\"' % self._escapestring(part))\n    elif hasattr(part, 'cssText'): # comments\n        out.append(part.cssText)\nreturn u'%s;' % u''.join(out)", "path": "cssutils\\serialize.py", "repo_name": "pib/PyBrowser", "stars": 18, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"\nchecks if valid items only and if yes it item is valid\n\"\"\"\n", "func_signal": "def _valid(self, x):\n", "code": "return not self.prefs.validOnly or (self.prefs.validOnly and\n                                    hasattr(x, 'valid') and\n                                    x.valid)", "path": "cssutils\\serialize.py", "repo_name": "pib/PyBrowser", "stars": 18, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"\ncomma-separated list of media, default is 'all'\n\nIf \"all\" is in the list, every other media *except* \"handheld\" will\nbe stripped. This is because how Opera handles CSS for PDAs.\n\"\"\"\n", "func_signal": "def do_stylesheets_medialist(self, medialist):\n", "code": "if len(medialist) == 0:\n    return u'all'\nelse:\n    sep = u',%s' % self.prefs.listItemSpacer\n    return sep.join(\n                (mq.mediaText for mq in medialist))", "path": "cssutils\\serialize.py", "repo_name": "pib/PyBrowser", "stars": 18, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"\nused by all @rules to get the keyword used\ndependent on prefs setting defaultAtKeyword\n\"\"\"\n", "func_signal": "def _getatkeyword(self, rule, default):\n", "code": "if self.prefs.defaultAtKeyword:\n    return default\nelse:\n    return rule.atkeyword", "path": "cssutils\\serialize.py", "repo_name": "pib/PyBrowser", "stars": 18, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"\nchecks if wellformed items only and if yes it item is wellformed\n\"\"\"\n", "func_signal": "def _wellformed(self, x):\n", "code": "return self.prefs.wellformedOnly and hasattr(x, 'wellformed') and\\\n       x.wellformed", "path": "cssutils\\serialize.py", "repo_name": "pib/PyBrowser", "stars": 18, "license": "None", "language": "python", "size": 312}
{"docstring": "# used by appendSelector and __setitem__\n", "func_signal": "def __prepareset(self, newMedium):\n", "code": "self._checkReadonly()\n\nif not isinstance(newMedium, MediaQuery):\n    newMedium = MediaQuery(newMedium)\n\nif newMedium.valid:\n    return newMedium", "path": "cssutils\\stylesheets\\medialist.py", "repo_name": "pib/PyBrowser", "stars": 18, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"Evaluates a single value against the training data.\n\nArgs:\n    value: List-like object with same dimensionality used for training\n        or the result of using convert_value if converted=True.\n    converted: If True then the input is in the correct internal format\n\nReturns:\n    Sorted (descending) list of (confidence, label)\n\"\"\"\n", "func_signal": "def predict(self, value, converted=False):\n", "code": "if not converted:\n    value = self.convert_value(value)\nconf = svmlight.classify(self._m, [(0, value)])[0]\nreturn [(math.fabs(conf), cmp(conf, 0))]", "path": "classipy\\classifiers\\svm_light.py", "repo_name": "bwhite/classipy", "stars": 18, "license": "None", "language": "python", "size": 2212}
{"docstring": "\"\"\"Generates a confusion matrix based on classifier test results.\n\nArgs:\n    test_results: Iterable of (true, pred) labels.\n\nReturns:\n    Confusion matrix in the form conf[true_label][pred_label]\n\"\"\"\n", "func_signal": "def gen_confusion(test_results):\n", "code": "confusion = {}\nfor true_label, pred_label in test_results:\n    try:\n        confusion[true_label][pred_label] += 1\n    except KeyError:\n        try:\n            confusion[true_label][pred_label] = 1\n        except KeyError:\n            confusion[true_label] = {pred_label: 1}\nreturn confusion", "path": "classipy\\validation.py", "repo_name": "bwhite/classipy", "stars": 18, "license": "None", "language": "python", "size": 2212}
{"docstring": "# free memory created by C to avoid memory leak\n", "func_signal": "def __del__(self):\n", "code": "if hasattr(self, '__createfrom__') and self.__createfrom__ == 'C':\n\tliblinear.free_and_destroy_model(pointer(self))", "path": "classipy\\classifiers\\liblinear\\linear.py", "repo_name": "bwhite/classipy", "stars": 18, "license": "None", "language": "python", "size": 2212}
{"docstring": "\"\"\"Convert values to zero mean and unit variance\n\nValues that have zero variance are replaced with 0 instead of\nNaN and warnings are suppressed.\n\nArgs:\n    label_values: Iterator of (label, value)\n\nReturns:\n    List of label_values that have been scaled in each dimension to zero\n    mean and unit variance\n\"\"\"\n", "func_signal": "def whiten(label_values):\n", "code": "label_values = list(label_values)\nvalues = [x[1] for x in label_values]\nprev_err = np.seterr(all='ignore')\nvalue_mean = np.mean(values, 0)\nvalue_std = np.std(values, 0)\nout = [(x, np.nan_to_num((y - value_mean) / value_std)) for x, y in label_values]\nnp.seterr(**prev_err)\nreturn out", "path": "classipy\\validation.py", "repo_name": "bwhite/classipy", "stars": 18, "license": "None", "language": "python", "size": 2212}
{"docstring": "\"\"\"Generates statistics given a square confusion matrix.\n\nArgs:\n    confusion: A square sparse confusion matrix in the form of a dict of\n        dicts such that confusion[true_label][pred_label].  All values are\n        expected to be integers, missing values are taken as zeros.\n\nReturns:\n    A dictionary of performance statistics (precision, recall, accuracy)\n\"\"\"\n", "func_signal": "def confusion_stats(confusion):\n", "code": "overall_total = 0.\noverall_correct = 0.\nprecision = {}\nrecall = {}\nf1 = {}\nmiss_rate = {}\ntps = {}\nfns = {}\nfps = {}\ntotal_true = {}\ntotal_pred = {}\noverall_total = sum([sum(x.values()) for x in confusion.values()])\nfor true_label in confusion:\n    # Generate base level statistics\n    # row_sum is num of true examples for the cur label\n    row_sum = sum(confusion[true_label].values())\n    # col_sum is num of predicted examples for the cur label\n    col_sum = sum([confusion[x][true_label]\n                   for x in confusion if true_label in confusion[x]])\n    try:\n        # Num True == Predict == cur class\n        tp = confusion[true_label][true_label]\n    except KeyError:\n        tp = 0\n    fn = row_sum - tp  # Num True == cur class and Predict != cur class\n    fp = col_sum - tp  # Num True != cur class and Predict == cur class\n    total_true[true_label] = row_sum\n    total_pred[true_label] = col_sum\n    tps[true_label] = tp\n    fps[true_label] = fp\n    fns[true_label] = fn\n    overall_correct += tp\n    # Generate relevant output statistics\n    try:\n        precision[true_label] = tp / float(tp + fp)\n    except ZeroDivisionError:\n        precision[true_label] = float('nan')\n    try:\n        recall[true_label] = tp / float(tp + fn)\n    except ZeroDivisionError:\n        recall[true_label] = float('nan')\n    f1[true_label] = 2. * precision[true_label] * recall[true_label]\n    try:\n        f1[true_label] /= (precision[true_label] + recall[true_label])\n    except ZeroDivisionError:\n        f1[true_label] = float('nan')\n    miss_rate[true_label] = 1 - recall[true_label]\ntry:\n    accuracy = overall_correct / float(overall_total)\nexcept ZeroDivisionError:\n    accuracy = float('nan')\nreturn {'accuracy': accuracy, 'precision': precision, 'recall': recall,\n        'tp': tps, 'fp': fps, 'fn': fns, 'total_true': total_true,\n        'total_pred': total_pred, 'miss_rate': miss_rate, 'f1': f1,\n        'confusion': confusion}", "path": "classipy\\validation.py", "repo_name": "bwhite/classipy", "stars": 18, "license": "None", "language": "python", "size": 2212}
{"docstring": "\"\"\"\nArgs:\n    num_points: Number of points to generate\n\"\"\"\n# Here we make a few fake classes and see if the classifier can get it\n", "func_signal": "def data_generator(num_points):\n", "code": "cgens = [[(.2, .4), (0, 1)], [(.3, .6), (0, 1)], [(.3, .6), (0, 1)]]\nprint(cgens)\nout = []\nfor x in range(num_points):\n    label = random.randint(0, len(cgens) - 1)\n    value = [np.random.uniform(x, y) for x, y in cgens[label]]\n    if label == 2:\n        value.append(label)\n    else:\n        value.append(0)\n    out.append((label, value))\nreturn out", "path": "examples\\rand_forest_run.py", "repo_name": "bwhite/classipy", "stars": 18, "license": "None", "language": "python", "size": 2212}
{"docstring": "\"\"\"\ntoPyModel(model_ptr) -> svm_model\n\nConvert a ctypes POINTER(svm_model) to a Python svm_model\n\"\"\"\n", "func_signal": "def toPyModel(model_ptr):\n", "code": "if bool(model_ptr) == False:\n\traise ValueError(\"Null pointer\")\nm = model_ptr.contents\nm.__createfrom__ = 'C'\nreturn m", "path": "thirdparty\\libsvm-3.1\\python\\svm.py", "repo_name": "bwhite/classipy", "stars": 18, "license": "None", "language": "python", "size": 2212}
{"docstring": "\"\"\"Classifies the provided values and generates stats based on the labels.\n\nArgs:\n    classifier: A classifier instance that conforms to the BinaryClassifier\n        spec.\n    label_values: Iterable of tuples of label and list-like objects.\n        Example: [(label, value), ...]\n    class_selector: Function that takes classifier output and returns a\n        label. If None (default) then use first class label (highest\n        confidence).\n    converted: True then the input is in the correct internal format.\n\nReturns:\n    A dictionary of performance statistics.\n\"\"\"\n", "func_signal": "def evaluate(classifier, label_values, class_selector=None, converted=False):\n", "code": "if class_selector == None:\n    class_selector = lambda x: x[0][1]\ntest_results = [(label, class_selector(classifier.predict(value, converted=converted)))\n                for label, value in label_values]\nconfusion = gen_confusion(test_results)\nreturn confusion_stats(confusion)", "path": "classipy\\validation.py", "repo_name": "bwhite/classipy", "stars": 18, "license": "None", "language": "python", "size": 2212}
{"docstring": "\"\"\"Build a model.\n\nArgs:\n    label_values: Iterable of tuples of label and list-like objects\n        Example: [(label, value), ...]\n        or the result of using convert_label_values if converted=True.\n    converted: If True then the input is in the correct internal format\nReturns:\n    self\n\"\"\"\n", "func_signal": "def train(self, label_values, converted=False):\n", "code": "if not converted:\n    label_values = self.convert_label_values(label_values)\nif not isinstance(label_values, list):\n    label_values = list(label_values)\nself._m = svmlight.learn(label_values, type='classification',\n                         verbosity=1)\nreturn self", "path": "classipy\\classifiers\\svm_light.py", "repo_name": "bwhite/classipy", "stars": 18, "license": "None", "language": "python", "size": 2212}
{"docstring": "\"\"\"Classifies the provided values and generates stats based on  the labels.\n\nAssumes labels are either -1 or 1.\n\nArgs:\n    classifier: A classifier instance that conforms to the BinaryClassifier\n        spec.\n    label_values: Iterable of tuples of label and list-like objects.\n        Example: [(label, value), ...]\n    samples: If None (default) then use every point.  Else select this many\n        uniform samples.\n\nReturns:\n    A dictionary where key is threshold and value is performance stats dict\n\"\"\"\n", "func_signal": "def confidence_stats(classifier, label_values, samples=None):\n", "code": "conf = lambda x: x[0][0] * x[0][1]\n\ntest_confidences = [(label, conf(classifier.predict(value)))\n                    for label, value in label_values]\nconfs = [x[1] for x in test_confidences]\nif samples == None:\n    confs += [float('-inf'), float('inf')]\nelse:\n    min_conf = min(confs)\n    max_conf = max(confs)\n    step = (max_conf - min_conf) / float(samples)\n    confs = np.arange(min_conf - step, max_conf + step, step).tolist()\nthresh_stats = {}\nfor conf_thresh in confs:\n    mkclass = lambda x: -1 if x < conf_thresh else 1\n    test_results = ((x[0], mkclass(x[1])) for x in test_confidences)\n    confusion = gen_confusion(test_results)\n    thresh_stats[conf_thresh] = confusion_stats(confusion)\nreturn thresh_stats", "path": "classipy\\validation.py", "repo_name": "bwhite/classipy", "stars": 18, "license": "None", "language": "python", "size": 2212}
{"docstring": "\"\"\"Stores the training data internally.\n\nArgs:\n    label_values: Iterable of tuples of label and list-like objects\n        Example: [(label, value), ...]\n        or the result of using convert_label_values if converted=True.\n    converted: If True then the input is in the correct internal format\nReturns:\n    self\n\"\"\"\n", "func_signal": "def train(self, label_values, converted=False):\n", "code": "if not converted:\n    label_values = self.convert_label_values(label_values)\nlabels, values = zip(*list(label_values))\nself._labels = labels\nself._values = values\nreturn self", "path": "classipy\\classifiers\\knn.py", "repo_name": "bwhite/classipy", "stars": 18, "license": "None", "language": "python", "size": 2212}
{"docstring": "\"\"\"\nReturns:\n    Version as a pair of ints (major, minor)\n\nRaises:\n    ImportError: Can't load cython or find version\n\"\"\"\n", "func_signal": "def get_cython_version():\n", "code": "import Cython.Compiler.Main\nmatch = re.search('^([0-9]+)\\.([0-9]+)',\n                  Cython.Compiler.Main.Version.version)\ntry:\n    return map(int, match.groups())\nexcept AttributeError:\n    raise ImportError", "path": "setup.py", "repo_name": "bwhite/classipy", "stars": 18, "license": "None", "language": "python", "size": 2212}
{"docstring": "\"\"\"Classifies the provided values and generates stats based on  the labels.\n\nArgs:\n    classifiers: A list of classifiers that conforms to the BinaryClassifier spec.\n    label_values: Iterable of tuples of label and list-like objects.\n        Example: [(label, value), ...]\n    class_selectors: List of functions (one per classifier, if less then\n        use default) that take classifier output and return label. If None\n        (default) then use first class label (highest confidence).\n\nReturns:\n    A dictionary of performance statistics.\n\"\"\"\n", "func_signal": "def multi_evaluate(classifiers, label_values, class_selectors=None):\n", "code": "if class_selectors == None:\n    class_selectors = [lambda x: x[0][1]]\nclassifier_output = []\nfor sel, cls in itertools.izip_longest(class_selectors, classifiers):\n    classifier_output.append(evaluate(cls, label_values, sel))\nreturn {'classifier_output': classifier_output}", "path": "classipy\\validation.py", "repo_name": "bwhite/classipy", "stars": 18, "license": "None", "language": "python", "size": 2212}
{"docstring": "\"\"\"Evaluates a single value against the training data.\n\nNOTE: Confidence is currently set to 0!\n\nArgs:\n    value: List-like object with same dimensionality used for training\n        or the result of using convert_value if converted=True.\n    converted: True then the input is in the correct internal format.\n\nReturns:\n    Sorted (descending) list of (confidence, label)\n\"\"\"\n", "func_signal": "def predict(self, value, converted=False):\n", "code": "if not converted:\n    value = self.convert_value(value)\nval = self._m.decision_function(np.array([value]))[0]\nif val >= 0:\n    return [(float(np.abs(val)), self._labels[1])]\nreturn [(float(np.abs(val)), self._labels[0])]", "path": "classipy\\classifiers\\svm_scikit.py", "repo_name": "bwhite/classipy", "stars": 18, "license": "None", "language": "python", "size": 2212}
{"docstring": "# NOTE(brandyn): This is to disable importing matplotlib which is unnecessary and causes dependency hell\n", "func_signal": "def _make_model(self):\n", "code": "import scipy.io.matlab.streams  # Needed by pyinstaller\nwith hide_modules(['matplotlib']):\n    from sklearn import svm\n    import sklearn.utils.arraybuilder\nkw = dict(self._param)\nkw.setdefault('kernel', 'linear')  # NOTE(brandyn): Default to linear\nself._m = svm.SVC(**kw)", "path": "classipy\\classifiers\\svm_scikit.py", "repo_name": "bwhite/classipy", "stars": 18, "license": "None", "language": "python", "size": 2212}
{"docstring": "\"\"\"Classifies the provided values and generates stats based on  the labels.\n\nArgs:\n    classifier: A classifier instance that conforms to the BinaryClassifier\n        spec.\n    label_values: Iterable of tuples of label and list-like objects.\n        Example: [(label, value), ...]\n    class_selector: Function that takes classifier output and returns a\n        label. If None (default) then use first class label (highest\n        confidence).\n\nReturns:\n    An iterator of values that correspond to false positives.\n\"\"\"\n", "func_signal": "def hard_negatives(classifier, label_values, class_selector=None):\n", "code": "if class_selector == None:\n    class_selector = lambda x: x[0][1]\nfor label, value in label_values:\n    if label != class_selector(classifier.predict(value)):\n        yield value", "path": "classipy\\validation.py", "repo_name": "bwhite/classipy", "stars": 18, "license": "None", "language": "python", "size": 2212}
{"docstring": "\"\"\"\ntoPyModel(model_ptr) -> model\n\nConvert a ctypes POINTER(model) to a Python model\n\"\"\"\n", "func_signal": "def toPyModel(model_ptr):\n", "code": "if bool(model_ptr) == False:\n\traise ValueError(\"Null pointer\")\nm = model_ptr.contents\nm.__createfrom__ = 'C'\nreturn m", "path": "classipy\\classifiers\\liblinear\\linear.py", "repo_name": "bwhite/classipy", "stars": 18, "license": "None", "language": "python", "size": 2212}
{"docstring": "\"\"\"Converts an iterable of values to a list of to_type.\n\nArgs:\n    label_values: Iterable of tuples of label and list-like objects.\n        Example: [(label, value), ...]\n\nReturns:\n    An iterable of label_values in the type specified by to_type.\n\"\"\"\n", "func_signal": "def convert_label_values(cls, label_values, *args, **kw):\n", "code": "return ((label, cls.convert_value(value, *args, **kw))\n        for label, value in label_values)", "path": "classipy\\classifiers\\base.py", "repo_name": "bwhite/classipy", "stars": 18, "license": "None", "language": "python", "size": 2212}
{"docstring": "\"\"\"Converts value to an efficient representation.\n\nArgs:\n    value: A value in a valid input type.\n\nReturns:\n    Value in an efficient representation.\n\"\"\"\n", "func_signal": "def convert_value(cls, value, *args, **kw):\n", "code": "value = super(SVMLight, cls).convert_value(value, to_type=list,\n                                           *args, **kw)\nreturn [(ind + 1, val) for ind, val in enumerate(value)]", "path": "classipy\\classifiers\\svm_light.py", "repo_name": "bwhite/classipy", "stars": 18, "license": "None", "language": "python", "size": 2212}
{"docstring": "\"\"\"Converts value to to_type.\n\nArgs:\n    value: A value in a valid to_type.\n    to_type: Type to convert to.\n\nReturns:\n    Value in the type specified by to_type.\n\"\"\"\n", "func_signal": "def convert_value(cls, value, to_type=np.ndarray):\n", "code": "if isinstance(value, to_type):  # Same type, quit early\n    return value\nif to_type == np.ndarray: # If it needs to be numpy\n    return np.array(value)\nif isinstance(value, np.ndarray): # We know to_type isn't numpy\n    value = value.tolist()\nif to_type == tuple:\n    return tuple(value)\nif to_type == list:\n    return list(value)", "path": "classipy\\classifiers\\base.py", "repo_name": "bwhite/classipy", "stars": 18, "license": "None", "language": "python", "size": 2212}
{"docstring": "\"\"\" Test error back call raises exception \"\"\"\n", "func_signal": "def test_callable_exception(self):\n", "code": "def f(*args, **kwargs):\n    raise TestException('broken')\n\nself.sc = TestableScheduledCall(self.clock, f)\n\ndef errback(fail):\n    self.assert_(fail.check(TestException),\n                 u'Expecting a TestException failure')\n\ndef callback(result):\n    self.fail('Callback should not be called')\n\nd = self.sc.start(SimpleSchedule(0.1))\nd.addCallbacks(callback, errback)\nself.clock.pump([0.1]*3)", "path": "txscheduling\\tests\\task.py", "repo_name": "benliles/TxScheduling", "stars": 20, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\" Test all days of the month get next day \"\"\"\n", "func_signal": "def testNextDay(self):\n", "code": "self.assertEqual(self.schedule.getNextEntry(datetime(2008, 9, 1, 23, \n                                                     59, 00, 00)),\n                 datetime(2008,9,3,00,00,00,00))\n    \nself.assertEqual(self.schedule.getNextEntry(datetime(2008, 9, 15, 23, \n                                                     59, 00, 00)),\n                 datetime(2008,9,17,00,00,00,00))\n    \nself.assertEqual(self.schedule.getNextEntry(datetime(2008, 9, 16, 23, \n                                                     59, 00, 00)),\n                 datetime(2008,9,17,00,00,00,00))\n    \nself.assertEqual(self.schedule.getNextEntry(datetime(2008, 9, 17, 23, \n                                                     59, 00, 00)),\n                 datetime(2008,9,19,00,00,00,00))", "path": "txscheduling\\tests\\cron.py", "repo_name": "benliles/TxScheduling", "stars": 20, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\" No calls before scheduled delay on long running task \"\"\"\n", "func_signal": "def test_no_calls(self):\n", "code": "self.sc.start(SimpleSchedule(1))\nself.assertEqual(self.callable.count, 0,\n                 u'Callable should not be called before time has passed')\nself.assertEqual(self.started.count, 0,\n                 u'Callable should not be called before time has passed')\n\nself.clock.pump([0.9])\nself.assertEqual(self.callable.count, 0,\n                 u'Callable should not be called before sufficient time '\n                 'has passed')\nself.assertEqual(self.started.count, 0,\n                 u'Callable should not be called before sufficient time '\n                 'has passed')\nself.sc.stop()\nself.assertEqual(self.callable.count, 0,\n                 u'Callable should not be called after stopping')\nself.assertEqual(self.started.count, 0,\n                 u'Callable should not be called after stopping')\n\nself.clock.pump([0.1]*50)\nself.assertEqual(self.callable.count, 0,\n                 u'Callable should not be called after stopping')\nself.assertEqual(self.started.count, 0,\n                 u'Callable should not be called after stopping')", "path": "txscheduling\\tests\\task.py", "repo_name": "benliles/TxScheduling", "stars": 20, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\" Verify calls at proper times on long running task \"\"\"\n", "func_signal": "def test_calls(self):\n", "code": "self.sc.start(SimpleSchedule(2))\nself.assertEqual(self.callable.count, 0,\n                 u'Callable should not be called before time has passed')\nself.assertEqual(self.started.count, 0,\n                 u'Callable should not be called before time has passed')\n\n# Time will be approx. 2.1\nself.clock.pump([0.1]*21)\nself.assertEqual(self.callable.count, 0,\n                 u'Callable should be running now: %f' % (self.clock.rightNow,))\nself.assertEqual(self.started.count, 1,\n                 u'Callable should be running now: %f' % (self.clock.rightNow,))\n\n# Time will be approx. 3.2\nself.clock.pump([0.1]*11)\nself.assertEqual(self.callable.count, 1,\n                 u'Callable should be called once now: %f' % (self.clock.rightNow,))\nself.assertEqual(self.started.count, 1,\n                 u'Callable should be called once now: %f' % (self.clock.rightNow,))\n\n# Time will be approx. 3.9\nself.clock.pump([0.1]*7)\nself.assertEqual(self.callable.count, 1,\n                 u'Callable should be called once now: %f' % (self.clock.rightNow,))\nself.assertEqual(self.started.count, 1,\n                 u'Callable should be called once now: %f' % (self.clock.rightNow,))\n\n# Time will be approx 5.2\nself.clock.pump([0.1]*13)\nself.assertEqual(self.callable.count, 1,\n                 u'Callable should be running now: %f' % (self.clock.rightNow,))\nself.assertEqual(self.started.count, 2,\n                 u'Callable should be running now: %f' % (self.clock.rightNow,))\n\n# Time will be approx. 6.4\nself.clock.pump([0.1]*12)\nself.assertEqual(self.callable.count, 2,\n                 u'Callable should be called twice now: %f' % (self.clock.rightNow,))\nself.assertEqual(self.started.count, 2,\n                 u'Callable should be called twice now: %f' % (self.clock.rightNow,))\n\n\nself.sc.stop()\nself.assertEqual(self.callable.count, 2,\n                 u'Callable should not be called after stopping: %f' % (self.clock.rightNow,))\nself.assertEqual(self.started.count, 2,\n                 u'Callable should not be called after stopping: %f' % (self.clock.rightNow,))\n\n\nself.clock.pump([0.1]*50)\nself.assertEqual(self.callable.count, 2,\n                 u'Callable should not be called after stopping: %f' % (self.clock.rightNow,))\nself.assertEqual(self.started.count, 2,\n                 u'Callable should not be called after stopping: %f' % (self.clock.rightNow,))", "path": "txscheduling\\tests\\task.py", "repo_name": "benliles/TxScheduling", "stars": 20, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\" Schedule the next iteration of this scheduled call. \"\"\"\n", "func_signal": "def _reschedule(self):\n", "code": "if self.call is None:\n    delay = self.schedule.getDelayForNext()\n    self._lastTime = self.clock.seconds() + delay\n    self.call = self.clock.callLater(delay, self)", "path": "txscheduling\\task.py", "repo_name": "benliles/TxScheduling", "stars": 20, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\" Test an invalid start call to a running ScheduledCall \"\"\"\n", "func_signal": "def test_startingStarted(self):\n", "code": "self.sc.start(SimpleSchedule())\nself.assertRaises(AssertionError,self.sc.start,SimpleSchedule())\nself.sc.stop()", "path": "txscheduling\\tests\\task.py", "repo_name": "benliles/TxScheduling", "stars": 20, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\" No calls before scheduled delay \"\"\"\n", "func_signal": "def test_no_calls(self):\n", "code": "self.sc.start(SimpleSchedule(1))\nself.assertEqual(self.callable.count, 0,\n                 u'Callable should not be called before time has passed')\nself.clock.pump([0.9])\nself.assertEqual(self.callable.count, 0,\n                 u'Callable should not be called before sufficient time '\n                 'has passed')\nself.sc.stop()\nself.assertEqual(self.callable.count, 0,\n                 u'Callable should not be called after stopping')\nself.clock.pump([0.1]*50)\nself.assertEqual(self.callable.count, 0,\n                 u'Callable should not be called after stopping')", "path": "txscheduling\\tests\\task.py", "repo_name": "benliles/TxScheduling", "stars": 20, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\"\nParse a standard cron string (minus the command) and return them as a\ndictionary. The syntax for this was pulled from the\nWikipedia page: http://en.wikipedia.org/wiki/Cron\nCurrently, there is no support for textual days of the week\n(i.e. Monday,Tuesday).\n\nExamples:\n\n>>> parseCronLine('* * * * *') == {\n...     'hours': range(0,24),\n...     'doms': range(1,32),\n...     'minutes': range(0,60),\n...     'dows': range(0,7),\n...      'months': range(1,13)}\nTrue\n\nCron lines must have 5 whitespace separated entries\n>>> parseCronLine('* * * *')\nTraceback (most recent call last):\n...\nInvalidCronLine: Improper number of elements encountered: 4\n\nCron lines cannot be empty\n>>> parseCronLine('')\nTraceback (most recent call last):\n...\nInvalidCronLine: Empty cron line provided\n\nCron lines must be a string\n>>> parseCronLine(True)\nTraceback (most recent call last):\n...\nInvalidCronLine: Cron line must be a string\n\n\"\"\"\n", "func_signal": "def parseCronLine(line):\n", "code": "if not line:\n    raise InvalidCronLine('Empty cron line provided')\n\nif not isinstance(line, basestring):\n    raise InvalidCronLine('Cron line must be a string')\n\nline = re.split('\\s+',line.strip())\n\nif len(line) != 5:\n    raise InvalidCronLine('Improper number of elements encountered: %s' % len(line)) \n  \nschedule = {}\n  \nschedule['minutes'] = parseCronEntry(line[0],0,59)\nschedule['hours']   = parseCronEntry(line[1],0,23)\nschedule['doms']    = parseCronEntry(line[2],1,31)\nschedule['months']  = parseCronEntry(line[3],1,12)\nschedule['dows']    = parseCronEntry(line[4],0,6)\n  \nreturn schedule", "path": "txscheduling\\cron.py", "repo_name": "benliles/TxScheduling", "stars": 20, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\" Test that positional arguments are passed through properly \"\"\"\n", "func_signal": "def test_postional_arguments(self):\n", "code": "def f(*args, **kwargs):\n    self.assertEqual(len(args),3)\n    self.assertEqual(len(kwargs),0)\n    self.assertEqual(args[0],'a')\n    self.assertEqual(args[1],True)\n    self.assertEqual(args[2],'c')\n\nself.sc = TestableScheduledCall(self.clock, f, 'a', True, 'c')\nd = self.sc.start(SimpleSchedule(0.1))\nself.clock.pump([0.1,0.05])\nself.sc.stop()", "path": "txscheduling\\tests\\task.py", "repo_name": "benliles/TxScheduling", "stars": 20, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\" Stop running function. \"\"\"\n", "func_signal": "def stop(self):\n", "code": "assert self.running, (\"Tried to stop a ScheduledCall that was \"\n                      \"not running.\")\nself.running = False\nif self.call is not None:\n    self.call.cancel()\n    self.call = None\n    d, self.deferred = self.deferred, None\n    d.callback(self)", "path": "txscheduling\\task.py", "repo_name": "benliles/TxScheduling", "stars": 20, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\"Parse a single cron entry for something like hours or minutes from a cron\nscheduling line.  The given min and max are used to verify that results are\nin the proper range. The following formats are supports:\n* => All in the available range\n*/5 => Only those values in the available range that are divisible by five\n1-5 => The range of 1-5\nAnd any combination of the above using commas to separate the entries.\n\nExamples:\n\nA single entry\n>>> parseCronEntry('0',0,12)\n[0]\n\nA star entry\n>>> parseCronEntry('*',0,3)\n[0, 1, 2, 3]\n\nA range entry\n>>> parseCronEntry('3-5',0,12)\n[3, 4, 5]\n\nA frequency entry\n>>> parseCronEntry('*/3',0,12)\n[0, 3, 6, 9, 12]\n\nA frequency that doesn't match the minimum\n>>> parseCronEntry('*/3',1,12)\n[3, 6, 9, 12]\n\nA ranged frequency entry\n>>> parseCronEntry('3-9/3',0,12)\n[3, 6, 9]\n\nA ranged frequency entry where frequency doesn't match the range\n>>> parseCronEntry('2-10/3',0,12)\n[3, 6, 9]\n\nA set of entries\n>>> parseCronEntry('1,3,5',1,12)\n[1, 3, 5]\n\nA set of ranges\n>>> parseCronEntry('1-3,6-9',1,12)\n[1, 2, 3, 6, 7, 8, 9]\n\nA complex entry\n>>> parseCronEntry('*/5,1,12-14,22-28/3',1,30)\n[1, 5, 10, 12, 13, 14, 15, 20, 24, 25, 27, 30]\n\nMinimum argument must be convertible to an integer\n>>> parseCronEntry('1','a', 5000)\nTraceback (most recent call last):\n  ...\nValueError: minimum and maximum should be convertible to integers\n\nMaximum argument must be convertible to an integer\n>>> parseCronEntry('1',5, 'a')\nTraceback (most recent call last):\n  ...\nValueError: minimum and maximum should be convertible to integers\n\nMinimum argument must not be negative\n>>> parseCronEntry('1',-100,100)\nTraceback (most recent call last):\n  ...\nValueError: minimum must be non-negative\n\nMinimum argument must be less than the maximum argument\n>>> parseCronEntry('1',1,0)\nTraceback (most recent call last):\n  ...\nValueError: minimum must be less than or equal to maximum\n\nEntries are required\n>>> parseCronEntry(None, 1, 5)\nTraceback (most recent call last):\n  ...\nInvalidCronEntry: Empty cron entry\n\n>>> parseCronEntry('', 1, 5)\nTraceback (most recent call last):\n  ...\nInvalidCronEntry: Empty cron entry\n\nEntries must be a string\n>>> parseCronEntry([1], 1, 5)\nTraceback (most recent call last):\n  ...\nInvalidCronEntry: Cron entry must be a string\n\nEntries must be inside of the allowed range\n>>> parseCronEntry('0', 1, 5)\nTraceback (most recent call last):\n  ...\nInvalidCronEntry: Value, 0-0, out of allowed range: 1-5\n    \n>>> parseCronEntry('6', 1, 5)\nTraceback (most recent call last):\n  ...\nInvalidCronEntry: Value, 6-6, out of allowed range: 1-5\n\n>>> parseCronEntry('-6', 1, 5)\nTraceback (most recent call last):\n  ...\nInvalidCronEntry: Value, -6--6, out of allowed range: 1-5\n\nRanges must be in minimum-maximum order\n>>> parseCronEntry('3-1', 1, 5)\nTraceback (most recent call last):\n  ...\nInvalidCronEntry: Invalid range or step specified: 3-1\n\nRange frequency must be valid for range size\n>>> parseCronEntry('1-5/40', 1, 5)\nTraceback (most recent call last):\n  ...\nInvalidCronEntry: Invalid range or step specified: 1-5/40\n\nNegative frequencies are not allowed\n>>> parseCronEntry('1-5/-2', 1, 5)\nTraceback (most recent call last):\n  ...\nInvalidCronEntry: Invalid cron entry\n\n>>> parseCronEntry('this is garbage',1,5)\nTraceback (most recent call last):\n  ...\nInvalidCronEntry: Invalid cron entry\n\n\"\"\"\n  \n", "func_signal": "def parseCronEntry(entry,min,max):\n", "code": "if not entry:\n    raise InvalidCronEntry('Empty cron entry')\n\nif not isinstance(entry, basestring):\n    raise InvalidCronEntry('Cron entry must be a string')\n\ntry:\n    min = int(min)\n    max = int(max)\nexcept ValueError:\n    raise ValueError('minimum and maximum should be convertible to integers')\n  \nif min > max:\n    raise ValueError('minimum must be less than or equal to maximum')\n  \nif min < 0:\n    raise ValueError('minimum must be non-negative')\n  \ntry:\n    entry = entry.split(',')\nexcept AttributeError:\n    raise InvalidCronEntry('Cron entry must be a string')\n  \ntotal = set()\n  \nfor e in entry:\n    try:\n        int_val = int(e)\n    except ValueError:\n        pass\n    else:\n        total.add(int_val)\n        continue\n    \n    begin = None\n    end = None\n    step = 1\n    \n    if e == '*':\n        begin = min\n        end = max + 1\n    \n    if begin is None:\n        #If this match works, then it is of the form */int\n        match = _cronStepRe.search(e)\n    \n        if not match is None:\n            begin = min\n            end = max + 1\n            step = int(match.group('step'))\n        \n    if begin is None:\n        match = _cronRangeRe.search(e)\n    \n        if not match is None:\n            begin = int(match.group('begin'))\n            end = int(match.group('end')) + 1\n            step = 1\n    \n    if begin is None:\n        match = _cronRangeStepRe.search(e)\n      \n        if not match is None:\n            begin = int(match.group('begin'))\n            end = int(match.group('end')) + 1\n            step = int(match.group('step'))\n    \n    if (begin is not None and begin < end and step > 0 and \n        begin >= min and end <= max + 1):\n        \n        # need to align the start properly\n        while begin % step != 0 and begin < end:\n            begin += 1\n      \n        if begin == end and begin % step != 0:\n            raise InvalidCronEntry('Invalid range or step specified: %s' % (e))\n      \n        total.update(range(begin,end,step))\n    elif not begin is None:\n        raise InvalidCronEntry('Invalid range or step specified: %s' % (e))\n  \nif len(total) == 0:\n    raise InvalidCronEntry('Invalid cron entry')\n  \ntotal = list(total)\ntotal.sort()\n  \nif total[0] < min or total[len(total)-1] > max:\n    raise InvalidCronEntry('Value, %s-%s, out of allowed range: %s-%s' % (total[0],total[len(total)-1],min,max))\n  \nreturn total", "path": "txscheduling\\cron.py", "repo_name": "benliles/TxScheduling", "stars": 20, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\" Verify calls at proper times \"\"\"\n", "func_signal": "def test_calls(self):\n", "code": "self.sc.start(SimpleSchedule(1))\nself.assertEqual(self.callable.count, 0,\n                 u'Callable should not be called before time has passed')\nself.clock.pump([0.1]*11)\nself.assertEqual(self.callable.count, 1,\n                 u'Callable should be called once now')\n\nself.clock.pump([0.1]*10)\nself.assertEqual(self.callable.count, 2,\n                 u'Callable should be called twice now')\nself.sc.stop()\nself.assertEqual(self.callable.count, 2,\n                 u'Callable should not be called after stopping')\nself.clock.pump([0.1]*50)\nself.assertEqual(self.callable.count, 2,\n                 u'Callable should not be called after stopping')", "path": "txscheduling\\tests\\task.py", "repo_name": "benliles/TxScheduling", "stars": 20, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\" Test start deferred callback on stop \"\"\"\n", "func_signal": "def test_testStopCallback(self):\n", "code": "def cb(result):\n    self.assertIdentical(result, self.sc)\n\nd = self.sc.start(SimpleSchedule())\nd.addCallback(cb)\nself.sc.stop()", "path": "txscheduling\\tests\\task.py", "repo_name": "benliles/TxScheduling", "stars": 20, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\" Test that mixed arguments are passed through properly \"\"\"\n", "func_signal": "def test_mixed_arguments(self):\n", "code": "def f(*args, **kwargs):\n    self.assertEqual(len(args),2)\n    self.assertEqual(len(kwargs),3)\n    self.assertEqual(args[0], 'p1')\n    self.assertEqual(args[1], False)\n    self.assertEqual(kwargs['kw1'],'a')\n    self.assertEqual(kwargs['kw2'],True)\n    self.assertEqual(kwargs['kw3'],'c')\n\nself.sc = TestableScheduledCall(self.clock, f, 'p1', False, kw1='a',\n                                kw3='c', kw2=True)\nd = self.sc.start(SimpleSchedule(0.1))\nself.clock.pump([0.1,0.05])\nself.sc.stop()", "path": "txscheduling\\tests\\task.py", "repo_name": "benliles/TxScheduling", "stars": 20, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\" Test error back call invalid arguments \"\"\"\n", "func_signal": "def test_callable_invalid_arguments(self):\n", "code": "def f(required, *args, **kwargs):\n    raise TestException('broken')\n\nself.sc = TestableScheduledCall(self.clock, f)\n\ndef errback(fail):\n    self.assert_(fail.check(TypeError),\n                 u'Expecting a TypeError failure')\n\ndef callback(result):\n    self.fail('Callback should not be called')\n\nd = self.sc.start(SimpleSchedule(0.1))\nd.addCallbacks(callback, errback)\nself.clock.pump([0.1]*3)", "path": "txscheduling\\tests\\task.py", "repo_name": "benliles/TxScheduling", "stars": 20, "license": "None", "language": "python", "size": 99}
{"docstring": "# If the current month is a valid option, try to parse for the next valid day\n", "func_signal": "def _getNextMonth(self, current):\n", "code": "if current.month in self._months:\n    try:\n        return self._getNextDay(current)\n    except NoMatch:\n        pass\n    \n# Find the next month if it occurs in the current year\nfor month in self._months:\n    if month > current.month:\n        return self._getFirstDay(current.replace(month=month,\n                                                 day=1,\n                                                 hour=self._hours[0],\n                                                 minute=self._minutes[0]))\n    \n# No remaining months this year, go to next year\nreturn self._getFirstDay(current.replace(year=current.year+1,\n                                         month=self._months[0],\n                                         day=1,\n                                         hour=self._hours[0],\n                                         minute=self._minutes[0]))", "path": "txscheduling\\cron.py", "repo_name": "benliles/TxScheduling", "stars": 20, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\" Test that named arguments are passed through properly \"\"\"\n", "func_signal": "def test_named_arguments(self):\n", "code": "def f(*args, **kwargs):\n    self.assertEqual(len(args),0)\n    self.assertEqual(len(kwargs),3)\n    self.assertEqual(kwargs['kw1'],'a')\n    self.assertEqual(kwargs['kw2'],True)\n    self.assertEqual(kwargs['kw3'],'c')\n\nself.sc = TestableScheduledCall(self.clock, f, kw1='a', kw3='c', kw2=True,)\nd = self.sc.start(SimpleSchedule(0.1))\nself.clock.pump([0.1,0.05])\nself.sc.stop()", "path": "txscheduling\\tests\\task.py", "repo_name": "benliles/TxScheduling", "stars": 20, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\" CronSchedule equality testing \"\"\"\n", "func_signal": "def test_equality(self):\n", "code": "self.assertEqual(self.schedule,CronSchedule('* * * * *'))\nself.assertEqual(self.schedule == 'blah',False)", "path": "txscheduling\\tests\\cron.py", "repo_name": "benliles/TxScheduling", "stars": 20, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\" Test all days of the month get next month \"\"\"\n", "func_signal": "def testNextMonth(self):\n", "code": "self.assertEqual(self.schedule.getNextEntry(datetime(2008, 6, 30, 23, \n                                                     59, 00, 00)),\n                 datetime(2008,7,2,00,00,00,00))\n    \nself.assertEqual(self.schedule.getNextEntry(datetime(2008, 8, 29, 23, \n                                                     59, 00, 00)),\n                 datetime(2008,9,1,00,00,00,00))\n    \nself.assertEqual(self.schedule.getNextEntry(datetime(2008, 9, 29, 23, \n                                                     59, 00, 00)),\n                 datetime(2008,10,1,00,00,00,00))", "path": "txscheduling\\tests\\cron.py", "repo_name": "benliles/TxScheduling", "stars": 20, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\" Test next day with Sunday DOW entry \"\"\"\n", "func_signal": "def test_getFirstDayWithSundayDOW(self):\n", "code": "schedule = CronSchedule('* * * * 0,3,5')\n    \nself.assertEqual(schedule.getNextEntry(datetime(2008,8,31,23,59,00,00)),\n                 datetime(2008,9,3,00,00,00,00))", "path": "txscheduling\\tests\\cron.py", "repo_name": "benliles/TxScheduling", "stars": 20, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\"Decodes a JSON-encoded string into a Python object.\n\nIf 'strict' is set to True, then those strings that are not\nentirely strictly conforming to JSON will result in a\nJSONDecodeError exception.\n\nThe input string can be either a python string or a python unicode\nstring.  If it is already a unicode string, then it is assumed\nthat no character set decoding is required.\n\nHowever, if you pass in a non-Unicode text string (i.e., a python\ntype 'str') then an attempt will be made to auto-detect and decode\nthe character encoding.  This will be successful if the input was\nencoded in any of UTF-8, UTF-16 (BE or LE), or UTF-32 (BE or LE),\nand of course plain ASCII works too.\n\nNote though that if you know the character encoding, then you\nshould convert to a unicode string yourself, or pass it the name\nof the 'encoding' to avoid the guessing made by the auto\ndetection, as with\n\n    python_object = demjson.decode( input_bytes, encoding='utf8' )\n\nOptional keywords arguments must be of the form\n    allow_xxxx=True/False\nor\n    prevent_xxxx=True/False\nwhere each will allow or prevent the specific behavior, after the\nevaluation of the 'strict' argument.  For example, if strict=True\nthen by also passing 'allow_comments=True' then comments will be\nallowed.  If strict=False then prevent_comments=True will allow\neverything except comments.\n\n\"\"\"\n# Initialize the JSON object\n", "func_signal": "def decode( txt, strict=False, encoding=None, **kw ):\n", "code": "j = JSON( strict=strict )\nfor keyword, value in kw.items():\n    if keyword.startswith('allow_'):\n        behavior = keyword[6:]\n        allow = bool(value)\n    elif keyword.startswith('prevent_'):\n        behavior = keyword[8:]\n        allow = not bool(value)\n    else:\n        raise ValueError('unknown keyword argument', keyword)\n    if allow:\n        j.allow(behavior)\n    else:\n        j.prevent(behavior)\n\n# Convert the input string into unicode if needed.\nif isinstance(txt,unicode):\n    unitxt = txt\nelse:\n    if encoding is None:\n        unitxt = auto_unicode_decode( txt )\n    else:\n        cdk = None # codec\n        decoder = None\n        import codecs\n        try:\n            cdk = codecs.lookup(encoding)\n        except LookupError:\n            encoding = encoding.lower()\n            decoder = None\n            if encoding.startswith('utf-32') \\\n                   or encoding.startswith('ucs4') \\\n                   or encoding.startswith('ucs-4'):\n                # Python doesn't natively have a UTF-32 codec, but JSON\n                # requires that it be supported.  So we must decode these\n                # manually.\n                if encoding.endswith('le'):\n                    decoder = utf32le_decode\n                elif encoding.endswith('be'):\n                    decoder = utf32be_decode\n                else:\n                    if txt.startswith( codecs.BOM_UTF32_BE ):\n                        decoder = utf32be_decode\n                        txt = txt[4:]\n                    elif txt.startswith( codecs.BOM_UTF32_LE ):\n                        decoder = utf32le_decode\n                        txt = txt[4:]\n                    else:\n                        if encoding.startswith('ucs'):\n                            raise JSONDecodeError('UCS-4 encoded string must start with a BOM')\n                        decoder = utf32be_decode # Default BE for UTF, per unicode spec\n            elif encoding.startswith('ucs2') or encoding.startswith('ucs-2'):\n                # Python has no UCS-2, but we can simulate with\n                # UTF-16.  We just need to force us to not try to\n                # encode anything past the BMP.\n                encoding = 'utf-16'\n\n        if decoder:\n            unitxt = decoder(txt)\n        elif encoding:\n            unitxt = txt.decode(encoding)\n        else:\n            raise JSONDecodeError('this python has no codec for this character encoding',encoding)\n\n    # Check that the decoding seems sane.  Per RFC 4627 section 3:\n    #    \"Since the first two characters of a JSON text will\n    #    always be ASCII characters [RFC0020], ...\"\n    #\n    # This check is probably not necessary, but it allows us to\n    # raise a suitably descriptive error rather than an obscure\n    # syntax error later on.\n    #\n    # Note that the RFC requirements of two ASCII characters seems\n    # to be an incorrect statement as a JSON string literal may\n    # have as it's first character any unicode character.  Thus\n    # the first two characters will always be ASCII, unless the\n    # first character is a quotation mark.  And in non-strict\n    # mode we can also have a few other characters too.\n    if len(unitxt) > 2:\n        first, second = unitxt[:2]\n        if first in '\"\\'':\n            pass # second can be anything inside string literal\n        else:\n            if ((ord(first) < 0x20 or ord(first) > 0x7f) or \\\n                (ord(second) < 0x20 or ord(second) > 0x7f)) and \\\n                (not j.isws(first) and not j.isws(second)):\n                # Found non-printable ascii, must check unicode\n                # categories to see if the character is legal.\n                # Only whitespace, line and paragraph separators,\n                # and format control chars are legal here.\n                import unicodedata\n                catfirst = unicodedata.category(unicode(first))\n                catsecond = unicodedata.category(unicode(second))\n                if catfirst not in ('Zs','Zl','Zp','Cf') or \\\n                       catsecond not in ('Zs','Zl','Zp','Cf'):\n                    raise JSONDecodeError('the decoded string is gibberish, is the encoding correct?',encoding)\n# Now ready to do the actual decoding\nobj = j.decode( unitxt )\nreturn obj", "path": "deadparrot\\lib\\demjson.py", "repo_name": "gabrielfalcao/dead-parrot", "stars": 16, "license": "None", "language": "python", "size": 381}
{"docstring": "\"\"\"Skips whitespace.\n\"\"\"\n", "func_signal": "def skipws(self, txt, i=0, imax=None, skip_comments=True):\n", "code": "if not self._allow_comments and not self._allow_unicode_whitespace:\n    if imax is None:\n        imax = len(txt)\n    while i < imax and txt[i] in ' \\r\\n\\t':\n        i += 1\n    return i\nelse:\n    return self.skipws_any(txt, i, imax, skip_comments)", "path": "deadparrot\\lib\\demjson.py", "repo_name": "gabrielfalcao/dead-parrot", "stars": 16, "license": "None", "language": "python", "size": 381}
{"docstring": "\"\"\"Encodes a Unicode string into a UTF-32BE encoded byte string.\"\"\"\n", "func_signal": "def utf32be_encode( obj, errors='strict' ):\n", "code": "import struct\ntry:\n    import cStringIO as sio\nexcept ImportError:\n    import StringIO as sio\nf = sio.StringIO()\nwrite = f.write\npack = struct.pack\nfor c in obj:\n    n = ord(c)\n    if 0xD800 <= n <= 0xDFFF: # surrogate codepoints are prohibited by UTF-32\n        if errors == 'ignore':\n            continue\n        elif errors == 'replace':\n            n = ord('?')\n        else:\n            cname = 'U+%04X'%n\n            raise UnicodeError('UTF-32 can not encode surrogate characters',cname)\n    write( pack('>L', n) )\nreturn f.getvalue()", "path": "deadparrot\\lib\\demjson.py", "repo_name": "gabrielfalcao/dead-parrot", "stars": 16, "license": "None", "language": "python", "size": 381}
{"docstring": "\"\"\"Decodes a JSON-endoded string into a Python object.\"\"\"\n", "func_signal": "def decode(self, txt):\n", "code": "if self._allow_unicode_format_control_chars:\n    txt = self.strip_format_control_chars(txt)\nr = self.decodeobj(txt, 0, only_object_or_array=not self._allow_any_type_at_start)\nif not r:\n    raise JSONDecodeError('can not decode value',txt)\nelse:\n    obj, i = r\n    i = self.skipws(txt, i)\n    if i < len(txt):\n        raise JSONDecodeError('unexpected or extra text',txt[i:])\nreturn obj", "path": "deadparrot\\lib\\demjson.py", "repo_name": "gabrielfalcao/dead-parrot", "stars": 16, "license": "None", "language": "python", "size": 381}
{"docstring": "\"\"\"Encodes a Python string into a JSON string literal.\n\n\"\"\"\n# Must handle instances of UserString specially in order to be\n# able to use ord() on it's simulated \"characters\".\n", "func_signal": "def encode_string(self, s):\n", "code": "import UserString\nif isinstance(s, (UserString.UserString, UserString.MutableString)):\n    def tochar(c):\n        return c.data\nelse:\n    # Could use \"lambda c:c\", but that is too slow.  So we set to None\n    # and use an explicit if test inside the loop.\n    tochar = None\n\nchunks = []\nchunks.append('\"')\nrevesc = self._rev_escapes\nasciiencodable = self._asciiencodable\nencunicode = self._encode_unicode_as_escapes\ni = 0\nimax = len(s)\nwhile i < imax:\n    if tochar:\n        c = tochar(s[i])\n    else:\n        c = s[i]\n    cord = ord(c)\n    if cord < 256 and asciiencodable[cord] and isinstance(encunicode, bool):\n        # Contiguous runs of plain old printable ASCII can be copied\n        # directly to the JSON output without worry (unless the user\n        # has supplied a custom is-encodable function).\n        j = i\n        i += 1\n        while i < imax:\n            if tochar:\n                c = tochar(s[i])\n            else:\n                c = s[i]\n            cord = ord(c)\n            if cord < 256 and asciiencodable[cord]:\n                i += 1\n            else:\n                break\n        chunks.append( unicode(s[j:i]) )\n    elif revesc.has_key(c):\n        # Has a shortcut escape sequence, like \"\\n\"\n        chunks.append(revesc[c])\n        i += 1\n    elif cord <= 0x1F:\n        # Always unicode escape ASCII-control characters\n        chunks.append(r'\\u%04x' % cord)\n        i += 1\n    elif 0xD800 <= cord <= 0xDFFF:\n        # A raw surrogate character!  This should never happen\n        # and there's no way to include it in the JSON output.\n        # So all we can do is complain.\n        cname = 'U+%04X' % cord\n        raise JSONEncodeError('can not include or escape a Unicode surrogate character',cname)\n    elif cord <= 0xFFFF:\n        # Other BMP Unicode character\n        if isinstance(encunicode, bool):\n            doesc = encunicode\n        else:\n            doesc = encunicode( c )\n        if doesc:\n            chunks.append(r'\\u%04x' % cord)\n        else:\n            chunks.append( c )\n        i += 1\n    else: # ord(c) >= 0x10000\n        # Non-BMP Unicode\n        if isinstance(encunicode, bool):\n            doesc = encunicode\n        else:\n            doesc = encunicode( c )\n        if doesc:\n            for surrogate in unicode_as_surrogate_pair(c):\n                chunks.append(r'\\u%04x' % ord(surrogate))\n        else:\n            chunks.append( c )\n        i += 1\nchunks.append('\"')\nreturn ''.join( chunks )", "path": "deadparrot\\lib\\demjson.py", "repo_name": "gabrielfalcao/dead-parrot", "stars": 16, "license": "None", "language": "python", "size": 381}
{"docstring": "\"\"\"Intermediate-level JSON decoder.\n\nTakes a string and a starting index, and returns a two-tuple consting\nof a Python object and the index of the next unparsed character.\n\nIf there is no value at all (empty string, etc), the None is\nreturned instead of a tuple.\n\n\"\"\"\n", "func_signal": "def decodeobj(self, txt, i=0, imax=None, identifier_as_string=False, only_object_or_array=False):\n", "code": "if imax is None:\n    imax = len(txt)\nobj = None\ni = self.skipws(txt, i, imax)\nif i >= imax:\n    raise JSONDecodeError('Unexpected end of input')\nc = txt[i]\n\nif c == '[' or c == '{':\n    obj, i = self.decode_composite(txt, i, imax)\nelif only_object_or_array:\n    raise JSONDecodeError('JSON document must start with an object or array type only', txt[i:i+20])\nelif c == '\"' or c == '\\'':\n    obj, i = self.decode_string(txt, i, imax)\nelif c.isdigit() or c in '.+-':\n    obj, i = self.decode_number(txt, i, imax)\nelif c.isalpha() or c in'_$':\n    j = i\n    while j < imax and (txt[j].isalnum() or txt[j] in '_$'):\n        j += 1\n    kw = txt[i:j]\n    if kw == 'null':\n        obj, i = None, j\n    elif kw == 'true':\n        obj, i = True, j\n    elif kw == 'false':\n        obj, i = False, j\n    elif kw == 'undefined':\n        if self._allow_undefined_values:\n            obj, i = undefined, j\n        else:\n            raise JSONDecodeError('strict JSON does not allow undefined elements',txt[i:])\n    elif kw == 'NaN' or kw == 'Infinity':\n        obj, i = self.decode_number(txt, i)\n    else:\n        if identifier_as_string:\n            obj, i = self.decode_javascript_identifier(kw), j\n        else:\n            raise JSONDecodeError('unknown keyword or identifier',kw)\nelse:\n    raise JSONDecodeError('can not decode value',txt[i:])\nreturn obj, i", "path": "deadparrot\\lib\\demjson.py", "repo_name": "gabrielfalcao/dead-parrot", "stars": 16, "license": "None", "language": "python", "size": 381}
{"docstring": "\"\"\"Creates a JSON encoder/decoder object.\n\nIf 'strict' is set to True, then only strictly-conforming JSON\noutput will be produced.  Note that this means that some types\nof values may not be convertable and will result in a\nJSONEncodeError exception.\n\nIf 'compactly' is set to True, then the resulting string will\nhave all extraneous white space removed; if False then the\nstring will be \"pretty printed\" with whitespace and indentation\nadded to make it more readable.\n\nIf 'escape_unicode' is set to True, then all non-ASCII characters\nwill be represented as a unicode escape sequence; if False then\nthe actual real unicode character will be inserted if possible.\n\nThe 'escape_unicode' can also be a function, which when called\nwith a single argument of a unicode character will return True\nif the character should be escaped or False if it should not.\n\nIf you wish to extend the encoding to ba able to handle\nadditional types, you should subclass this class and override\nthe encode_default() method.\n\n\"\"\"\n", "func_signal": "def __init__(self, strict=False, compactly=True, escape_unicode=False):\n", "code": "import sys\nself._set_strictness(strict)\nself._encode_compactly = compactly\ntry:\n    # see if we were passed a predicate function\n    b = escape_unicode(u'A')\n    self._encode_unicode_as_escapes = escape_unicode\nexcept (ValueError, NameError, TypeError):\n    # Just set to True or False.  We could use lambda x:True\n    # to make it more consistent (always a function), but it\n    # will be too slow, so we'll make explicit tests later.\n    self._encode_unicode_as_escapes = bool(escape_unicode)\nself._sort_dictionary_keys = True\n\n# The following is a boolean map of the first 256 characters\n# which will quickly tell us which of those characters never\n# need to be escaped.\n\nself._asciiencodable = [32 <= c < 128 and not self._rev_escapes.has_key(chr(c))\n                      for c in range(0,255)]", "path": "deadparrot\\lib\\demjson.py", "repo_name": "gabrielfalcao/dead-parrot", "stars": 16, "license": "None", "language": "python", "size": 381}
{"docstring": "\"\"\"Encodes the Python boolean into a JSON Boolean literal.\"\"\"\n", "func_signal": "def encode_boolean(self, b):\n", "code": "if bool(b):\n    return 'true'\nreturn 'false'", "path": "deadparrot\\lib\\demjson.py", "repo_name": "gabrielfalcao/dead-parrot", "stars": 16, "license": "None", "language": "python", "size": 381}
{"docstring": "\"\"\"Intermediate-level JSON decoder for composite literal types (array and object).\n\nTakes text and a starting index, and returns either a Python list or\ndictionary and the index of the next unparsed character.\n\n\"\"\"\n", "func_signal": "def decode_composite(self, txt, i=0, imax=None):\n", "code": "if imax is None:\n    imax = len(txt)\ni = self.skipws(txt, i, imax)\nstarti = i\nif i >= imax or txt[i] not in '{[':\n    raise JSONDecodeError('composite object must start with \"[\" or \"{\"',txt[i:])\nif txt[i] == '[':\n    isdict = False\n    closer = ']'\n    obj = []\nelse:\n    isdict = True\n    closer = '}'\n    obj = {}\ni += 1 # skip opener\ni = self.skipws(txt, i, imax)\n\nif i < imax and txt[i] == closer:\n    # empty composite\n    i += 1\n    done = True\nelse:\n    saw_value = False   # set to false at beginning and after commas\n    done = False\n    while i < imax:\n        i = self.skipws(txt, i, imax)\n        if i < imax and (txt[i] == ',' or txt[i] == closer):\n            c = txt[i]\n            i += 1\n            if c == ',':\n                if not saw_value:\n                    # no preceeding value, an elided (omitted) element\n                    if isdict:\n                        raise JSONDecodeError('can not omit elements of an object (dictionary)')\n                    if self._allow_omitted_array_elements:\n                        if self._allow_undefined_values:\n                            obj.append( undefined )\n                        else:\n                            obj.append( None )\n                    else:\n                        raise JSONDecodeError('strict JSON does not permit omitted array (list) elements',txt[i:])\n                saw_value = False\n                continue\n            else: # c == closer\n                if not saw_value and not self._allow_trailing_comma_in_literal:\n                    if isdict:\n                        raise JSONDecodeError('strict JSON does not allow a final comma in an object (dictionary) literal',txt[i-2:])\n                    else:\n                        raise JSONDecodeError('strict JSON does not allow a final comma in an array (list) literal',txt[i-2:])\n                done = True\n                break\n\n        # Decode the item\n        if isdict and self._allow_nonstring_keys:\n            r = self.decodeobj(txt, i, identifier_as_string=True)\n        else:\n            r = self.decodeobj(txt, i, identifier_as_string=False)\n        if r:\n            if saw_value:\n                # two values without a separating comma\n                raise JSONDecodeError('values must be separated by a comma', txt[i:r[1]])\n            saw_value = True\n            i = self.skipws(txt, r[1], imax)\n            if isdict:\n                key = r[0]  # Ref 11.1.5\n                if not isstringtype(key):\n                    if isnumbertype(key):\n                        if not self._allow_nonstring_keys:\n                            raise JSONDecodeError('strict JSON only permits string literals as object properties (dictionary keys)',txt[starti:])\n                    else:\n                        raise JSONDecodeError('object properties (dictionary keys) must be either string literals or numbers',txt[starti:])\n                if i >= imax or txt[i] != ':':\n                    raise JSONDecodeError('object property (dictionary key) has no value, expected \":\"',txt[starti:])\n                i += 1\n                i = self.skipws(txt, i, imax)\n                rval = self.decodeobj(txt, i)\n                if rval:\n                    i = self.skipws(txt, rval[1], imax)\n                    obj[key] = rval[0]\n                else:\n                    raise JSONDecodeError('object property (dictionary key) has no value',txt[starti:])\n            else: # list\n                obj.append( r[0] )\n        else: # not r\n            if isdict:\n                raise JSONDecodeError('expected a value, or \"}\"',txt[i:])\n            elif not self._allow_omitted_array_elements:\n                raise JSONDecodeError('expected a value or \"]\"',txt[i:])\n            else:\n                raise JSONDecodeError('expected a value, \",\" or \"]\"',txt[i:])\n    # end while\nif not done:\n    if isdict:\n        raise JSONDecodeError('object literal (dictionary) is not terminated',txt[starti:])\n    else:\n        raise JSONDecodeError('array literal (list) is not terminated',txt[starti:])\nreturn obj, i", "path": "deadparrot\\lib\\demjson.py", "repo_name": "gabrielfalcao/dead-parrot", "stars": 16, "license": "None", "language": "python", "size": 381}
{"docstring": "\"\"\"Takes a string and tries to convert it to a Unicode string.\n\nThis will return a Python unicode string type corresponding to the\ninput string (either str or unicode).  The character encoding is\nguessed by looking for either a Unicode BOM prefix, or by the\nrules specified by RFC 4627.  When in doubt it is assumed the\ninput is encoded in UTF-8 (the default for JSON).\n\n\"\"\"\n", "func_signal": "def auto_unicode_decode( s ):\n", "code": "if isinstance(s, unicode):\n    return s\nif len(s) < 4:\n    return s.decode('utf8')  # not enough bytes, assume default of utf-8\n# Look for BOM marker\nimport codecs\nbom2 = s[:2]\nbom4 = s[:4]\na, b, c, d = map(ord, s[:4])  # values of first four bytes\nif bom4 == codecs.BOM_UTF32_LE:\n    encoding = 'utf-32le'\n    s = s[4:]\nelif bom4 == codecs.BOM_UTF32_BE:\n    encoding = 'utf-32be'\n    s = s[4:]\nelif bom2 == codecs.BOM_UTF16_LE:\n    encoding = 'utf-16le'\n    s = s[2:]\nelif bom2 == codecs.BOM_UTF16_BE:\n    encoding = 'utf-16be'\n    s = s[2:]\n# No BOM, so autodetect encoding used by looking at first four bytes\n# according to RFC 4627 section 3.\nelif a==0 and b==0 and c==0 and d!=0: # UTF-32BE\n    encoding = 'utf-32be'\nelif a==0 and b!=0 and c==0 and d!=0: # UTF-16BE\n    encoding = 'utf-16be'\nelif a!=0 and b==0 and c==0 and d==0: # UTF-32LE\n    encoding = 'utf-32le'\nelif a!=0 and b==0 and c!=0 and d==0: # UTF-16LE\n    encoding = 'utf-16le'\nelse: #if a!=0 and b!=0 and c!=0 and d!=0: # UTF-8\n    # JSON spec says default is UTF-8, so always guess it\n    # if we can't guess otherwise\n    encoding = 'utf8'\n# Make sure the encoding is supported by Python\ntry:\n    cdk = codecs.lookup(encoding)\nexcept LookupError:\n    if encoding.startswith('utf-32') \\\n           or encoding.startswith('ucs4') \\\n           or encoding.startswith('ucs-4'):\n        # Python doesn't natively have a UTF-32 codec, but JSON\n        # requires that it be supported.  So we must decode these\n        # manually.\n        if encoding.endswith('le'):\n            unis = utf32le_decode(s)\n        else:\n            unis = utf32be_decode(s)\n    else:\n        raise JSONDecodeError('this python has no codec for this character encoding',encoding)\nelse:\n    # Convert to unicode using a standard codec\n    unis = s.decode(encoding)\nreturn unis", "path": "deadparrot\\lib\\demjson.py", "repo_name": "gabrielfalcao/dead-parrot", "stars": 16, "license": "None", "language": "python", "size": 381}
{"docstring": "#print 'encode_helper(chunklist=%r, obj=%r, nest_level=%r)'%(chunklist,obj,nest_level)\n", "func_signal": "def encode_helper(self, chunklist, obj, nest_level):\n", "code": "if hasattr(obj, 'json_equivalent'):\n    json = self.encode_equivalent( obj, nest_level=nest_level )\n    if json is not None:\n        chunklist.append( json )\n        return\nif obj is None:\n    chunklist.append( self.encode_null() )\nelif obj is undefined:\n    if self._allow_undefined_values:\n        chunklist.append( self.encode_undefined() )\n    else:\n        raise JSONEncodeError('strict JSON does not permit \"undefined\" values')\nelif isinstance(obj, bool):\n    chunklist.append( self.encode_boolean(obj) )\nelif isinstance(obj, (int,long,float,complex)) or \\\n         (decimal and isinstance(obj, decimal.Decimal)):\n    chunklist.append( self.encode_number(obj) )\nelif isinstance(obj, basestring) or isstringtype(obj):\n    chunklist.append( self.encode_string(obj) )\nelse:\n    self.encode_composite(chunklist, obj, nest_level)", "path": "deadparrot\\lib\\demjson.py", "repo_name": "gabrielfalcao/dead-parrot", "stars": 16, "license": "None", "language": "python", "size": 381}
{"docstring": "\"\"\"Decodes an octal string into it's integer value.\"\"\"\n", "func_signal": "def decode_octal( octalstring ):\n", "code": "n = 0\nfor c in octalstring:\n    if '0' <= c <= '7':\n        d = ord(c) - ord('0')\n    else:\n        raise JSONDecodeError('not an octal number',octalstring)\n    # Could use ((n << 3 ) | d), but python 2.3 issues a FutureWarning.\n    n = (n * 8) + d\nreturn n", "path": "deadparrot\\lib\\demjson.py", "repo_name": "gabrielfalcao/dead-parrot", "stars": 16, "license": "None", "language": "python", "size": 381}
{"docstring": "\"\"\"Try to return the Nan, Infinity, and -Infinity float values.\n\nThis is unnecessarily complex because there is no standard\nplatform- independent way to do this in Python as the language\n(opposed to some implementation of it) doesn't discuss\nnon-numbers.  We try various strategies from the best to the\nworst.\n\nIf this Python interpreter uses the IEEE 754 floating point\nstandard then the returned values will probably be real instances\nof the 'float' type.  Otherwise a custom class object is returned\nwhich will attempt to simulate the correct behavior as much as\npossible.\n\n\"\"\"\n", "func_signal": "def _nonnumber_float_constants():\n", "code": "try:\n    # First, try (mostly portable) float constructor.  Works under\n    # Linux x86 (gcc) and some Unices.\n    nan = float('nan')\n    inf = float('inf')\n    neginf = float('-inf')\nexcept ValueError:\n    try:\n        # Try the AIX (PowerPC) float constructors\n        nan = float('NaNQ')\n        inf = float('INF')\n        neginf = float('-INF')\n    except ValueError:\n        try:\n            # Next, try binary unpacking.  Should work under\n            # platforms using IEEE 754 floating point.\n            import struct, sys\n            xnan = '7ff8000000000000'.decode('hex')  # Quiet NaN\n            xinf = '7ff0000000000000'.decode('hex')\n            xcheck = 'bdc145651592979d'.decode('hex') # -3.14159e-11\n            # Could use float.__getformat__, but it is a new python feature,\n            # so we use sys.byteorder.\n            if sys.byteorder == 'big':\n                nan = struct.unpack('d', xnan)[0]\n                inf = struct.unpack('d', xinf)[0]\n                check = struct.unpack('d', xcheck)[0]\n            else:\n                nan = struct.unpack('d', xnan[::-1])[0]\n                inf = struct.unpack('d', xinf[::-1])[0]\n                check = struct.unpack('d', xcheck[::-1])[0]\n            neginf = - inf\n            if check != -3.14159e-11:\n                raise ValueError('Unpacking raw IEEE 754 floats does not work')\n        except (ValueError, TypeError):\n            # Punt, make some fake classes to simulate.  These are\n            # not perfect though.  For instance nan * 1.0 == nan,\n            # as expected, but 1.0 * nan == 0.0, which is wrong.\n            class nan(float):\n                \"\"\"An approximation of the NaN (not a number) floating point number.\"\"\"\n                def __repr__(self): return 'nan'\n                def __str__(self): return 'nan'\n                def __add__(self,x): return self\n                def __radd__(self,x): return self\n                def __sub__(self,x): return self\n                def __rsub__(self,x): return self\n                def __mul__(self,x): return self\n                def __rmul__(self,x): return self\n                def __div__(self,x): return self\n                def __rdiv__(self,x): return self\n                def __divmod__(self,x): return (self,self)\n                def __rdivmod__(self,x): return (self,self)\n                def __mod__(self,x): return self\n                def __rmod__(self,x): return self\n                def __pow__(self,exp): return self\n                def __rpow__(self,exp): return self\n                def __neg__(self): return self\n                def __pos__(self): return self\n                def __abs__(self): return self\n                def __lt__(self,x): return False\n                def __le__(self,x): return False\n                def __eq__(self,x): return False\n                def __neq__(self,x): return True\n                def __ge__(self,x): return False\n                def __gt__(self,x): return False\n                def __complex__(self,*a): raise NotImplementedError('NaN can not be converted to a complex')\n            if decimal:\n                nan = decimal.Decimal('NaN')\n            else:\n                nan = nan()\n            class inf(float):\n                \"\"\"An approximation of the +Infinity floating point number.\"\"\"\n                def __repr__(self): return 'inf'\n                def __str__(self): return 'inf'\n                def __add__(self,x): return self\n                def __radd__(self,x): return self\n                def __sub__(self,x): return self\n                def __rsub__(self,x): return self\n                def __mul__(self,x):\n                    if x is neginf or x < 0:\n                        return neginf\n                    elif x == 0:\n                        return nan\n                    else:\n                        return self\n                def __rmul__(self,x): return self.__mul__(x)\n                def __div__(self,x):\n                    if x == 0:\n                        raise ZeroDivisionError('float division')\n                    elif x < 0:\n                        return neginf\n                    else:\n                        return self\n                def __rdiv__(self,x):\n                    if x is inf or x is neginf or x is nan:\n                        return nan\n                    return 0.0\n                def __divmod__(self,x):\n                    if x == 0:\n                        raise ZeroDivisionError('float divmod()')\n                    elif x < 0:\n                        return (nan,nan)\n                    else:\n                        return (self,self)\n                def __rdivmod__(self,x):\n                    if x is inf or x is neginf or x is nan:\n                        return (nan, nan)\n                    return (0.0, x)\n                def __mod__(self,x):\n                    if x == 0:\n                        raise ZeroDivisionError('float modulo')\n                    else:\n                        return nan\n                def __rmod__(self,x):\n                    if x is inf or x is neginf or x is nan:\n                        return nan\n                    return x\n                def __pow__(self, exp):\n                    if exp == 0:\n                        return 1.0\n                    else:\n                        return self\n                def __rpow__(self, x):\n                    if -1 < x < 1: return 0.0\n                    elif x == 1.0: return 1.0\n                    elif x is nan or x is neginf or x < 0:\n                        return nan\n                    else:\n                        return self\n                def __neg__(self): return neginf\n                def __pos__(self): return self\n                def __abs__(self): return self\n                def __lt__(self,x): return False\n                def __le__(self,x):\n                    if x is self:\n                        return True\n                    else:\n                        return False\n                def __eq__(self,x):\n                    if x is self:\n                        return True\n                    else:\n                        return False\n                def __neq__(self,x):\n                    if x is self:\n                        return False\n                    else:\n                        return True\n                def __ge__(self,x): return True\n                def __gt__(self,x): return True\n                def __complex__(self,*a): raise NotImplementedError('Infinity can not be converted to a complex')\n            if decimal:\n                inf = decimal.Decimal('Infinity')\n            else:\n                inf = inf()\n            class neginf(float):\n                \"\"\"An approximation of the -Infinity floating point number.\"\"\"\n                def __repr__(self): return '-inf'\n                def __str__(self): return '-inf'\n                def __add__(self,x): return self\n                def __radd__(self,x): return self\n                def __sub__(self,x): return self\n                def __rsub__(self,x): return self\n                def __mul__(self,x):\n                    if x is self or x < 0:\n                        return inf\n                    elif x == 0:\n                        return nan\n                    else:\n                        return self\n                def __rmul__(self,x): return self.__mul__(self)\n                def __div__(self,x):\n                    if x == 0:\n                        raise ZeroDivisionError('float division')\n                    elif x < 0:\n                        return inf\n                    else:\n                        return self\n                def __rdiv__(self,x):\n                    if x is inf or x is neginf or x is nan:\n                        return nan\n                    return -0.0\n                def __divmod__(self,x):\n                    if x == 0:\n                        raise ZeroDivisionError('float divmod()')\n                    elif x < 0:\n                        return (nan,nan)\n                    else:\n                        return (self,self)\n                def __rdivmod__(self,x):\n                    if x is inf or x is neginf or x is nan:\n                        return (nan, nan)\n                    return (-0.0, x)\n                def __mod__(self,x):\n                    if x == 0:\n                        raise ZeroDivisionError('float modulo')\n                    else:\n                        return nan\n                def __rmod__(self,x):\n                    if x is inf or x is neginf or x is nan:\n                        return nan\n                    return x\n                def __pow__(self,exp):\n                    if exp == 0:\n                        return 1.0\n                    else:\n                        return self\n                def __rpow__(self, x):\n                    if x is nan or x is inf or x is inf:\n                        return nan\n                    return 0.0\n                def __neg__(self): return inf\n                def __pos__(self): return self\n                def __abs__(self): return inf\n                def __lt__(self,x): return True\n                def __le__(self,x): return True\n                def __eq__(self,x):\n                    if x is self:\n                        return True\n                    else:\n                        return False\n                def __neq__(self,x):\n                    if x is self:\n                        return False\n                    else:\n                        return True\n                def __ge__(self,x):\n                    if x is self:\n                        return True\n                    else:\n                        return False\n                def __gt__(self,x): return False\n                def __complex__(self,*a): raise NotImplementedError('-Infinity can not be converted to a complex')\n            if decimal:\n                neginf = decimal.Decimal('-Infinity')\n            else:\n                neginf = neginf(0)\nreturn nan, inf, neginf", "path": "deadparrot\\lib\\demjson.py", "repo_name": "gabrielfalcao/dead-parrot", "stars": 16, "license": "None", "language": "python", "size": 381}
{"docstring": "\"\"\"Determines if the given character is considered a line terminator.\n\nRef. ECMAScript section 7.3\n\n\"\"\"\n", "func_signal": "def islineterm(self, c):\n", "code": "if c == '\\r' or c == '\\n':\n    return True\nif c == u'\\u2028' or c == u'\\u2029': # unicodedata.category(c) in  ['Zl', 'Zp']\n    return True\nreturn False", "path": "deadparrot\\lib\\demjson.py", "repo_name": "gabrielfalcao/dead-parrot", "stars": 16, "license": "None", "language": "python", "size": 381}
{"docstring": "\"\"\"Encodes just dictionaries, lists, or sequences.\n\nBasically handles any python type for which iter() can create\nan iterator object.\n\nThis method is not intended to be called directly.  Use the\nencode() method instead.\n\n\"\"\"\n#print 'encode_complex_helper(chunklist=%r, obj=%r, nest_level=%r)'%(chunklist,obj,nest_level)\n", "func_signal": "def encode_composite(self, chunklist, obj, nest_level):\n", "code": "try:\n    # Is it a dictionary or UserDict?  Try iterkeys method first.\n    it = obj.iterkeys()\nexcept AttributeError:\n    try:\n        # Is it a sequence?  Try to make an iterator for it.\n        it = iter(obj)\n    except TypeError:\n        it = None\nif it is not None:\n    # Does it look like a dictionary?  Check for a minimal dict or\n    # UserDict interface.\n    isdict = hasattr(obj, '__getitem__') and hasattr(obj, 'keys')\n    compactly = self._encode_compactly\n    if isdict:\n        chunklist.append('{')\n        if compactly:\n            dictcolon = ':'\n        else:\n            dictcolon = ' : '\n    else:\n        chunklist.append('[')\n    #print nest_level, 'opening sequence:', repr(chunklist)\n    if not compactly:\n        indent0 = '  ' * nest_level\n        indent = '  ' * (nest_level+1)\n        chunklist.append(' ')\n    sequence_chunks = []  # use this to allow sorting afterwards if dict\n    try: # while not StopIteration\n        numitems = 0\n        while True:\n            obj2 = it.next()\n            if obj2 is obj:\n                raise JSONEncodeError('trying to encode an infinite sequence',obj)\n            if isdict and not isstringtype(obj2):\n                # Check JSON restrictions on key types\n                if isnumbertype(obj2):\n                    if not self._allow_nonstring_keys:\n                        raise JSONEncodeError('object properties (dictionary keys) must be strings in strict JSON',obj2)\n                else:\n                    raise JSONEncodeError('object properties (dictionary keys) can only be strings or numbers in ECMAScript',obj2)\n\n            # Encode this item in the sequence and put into item_chunks\n            item_chunks = []\n            self.encode_helper( item_chunks, obj2, nest_level=nest_level+1 )\n            if isdict:\n                item_chunks.append(dictcolon)\n                obj3 = obj[obj2]\n                self.encode_helper(item_chunks, obj3, nest_level=nest_level+2)\n\n            #print nest_level, numitems, 'item:', repr(obj2)\n            #print nest_level, numitems, 'sequence_chunks:', repr(sequence_chunks)\n            #print nest_level, numitems, 'item_chunks:', repr(item_chunks)\n            #extend_list_with_sep(sequence_chunks, item_chunks)\n            sequence_chunks.append(item_chunks)\n            #print nest_level, numitems, 'new sequence_chunks:', repr(sequence_chunks)\n            numitems += 1\n    except StopIteration:\n        pass\n\n    if isdict and self._sort_dictionary_keys:\n        sequence_chunks.sort()  # Note sorts by JSON repr, not original Python object\n    if compactly:\n        sep = ','\n    else:\n        sep = ',\\n' + indent\n\n    #print nest_level, 'closing sequence'\n    #print nest_level, 'chunklist:', repr(chunklist)\n    #print nest_level, 'sequence_chunks:', repr(sequence_chunks)\n    extend_and_flatten_list_with_sep( chunklist, sequence_chunks, sep )\n    #print nest_level, 'new chunklist:', repr(chunklist)\n\n    if not compactly:\n        if numitems > 1:\n            chunklist.append('\\n' + indent0)\n        else:\n            chunklist.append(' ')\n    if isdict:\n        chunklist.append('}')\n    else:\n        chunklist.append(']')\nelse: # Can't create an iterator for the object\n    json2 = self.encode_default( obj, nest_level=nest_level )\n    chunklist.append( json2 )", "path": "deadparrot\\lib\\demjson.py", "repo_name": "gabrielfalcao/dead-parrot", "stars": 16, "license": "None", "language": "python", "size": 381}
{"docstring": "\"\"\"Changes the strictness behavior.\n\nPass True to be very strict about JSON syntax, or False to be looser.\n\"\"\"\n", "func_signal": "def _set_strictness(self, strict):\n", "code": "self._allow_any_type_at_start = not strict\nself._allow_all_numeric_signs = not strict\nself._allow_comments = not strict\nself._allow_control_char_in_string = not strict\nself._allow_hex_numbers = not strict\nself._allow_initial_decimal_point = not strict\nself._allow_js_string_escapes = not strict\nself._allow_non_numbers = not strict\nself._allow_nonescape_characters = not strict  # \"\\z\" -> \"z\"\nself._allow_nonstring_keys = not strict\nself._allow_omitted_array_elements = not strict\nself._allow_single_quoted_strings = not strict\nself._allow_trailing_comma_in_literal = not strict\nself._allow_undefined_values = not strict\nself._allow_unicode_format_control_chars = not strict\nself._allow_unicode_whitespace = not strict\n# Always disable this by default\nself._allow_octal_numbers = False", "path": "deadparrot\\lib\\demjson.py", "repo_name": "gabrielfalcao/dead-parrot", "stars": 16, "license": "None", "language": "python", "size": 381}
{"docstring": "\"\"\"Filters out all Unicode format control characters from the string.\n\nECMAScript permits any Unicode \"format control characters\" to\nappear at any place in the source code.  They are to be\nignored as if they are not there before any other lexical\ntokenization occurs.  Note that JSON does not allow them.\n\nRef. ECMAScript section 7.1.\n\n\"\"\"\n", "func_signal": "def strip_format_control_chars(self, txt):\n", "code": "import unicodedata\ntxt2 = filter( lambda c: unicodedata.category(unicode(c)) != 'Cf',\n               txt )\nreturn txt2", "path": "deadparrot\\lib\\demjson.py", "repo_name": "gabrielfalcao/dead-parrot", "stars": 16, "license": "None", "language": "python", "size": 381}
{"docstring": "\"\"\"This method is used to encode user-defined class objects.\n\nThe object being encoded should have a json_equivalent()\nmethod defined which returns another equivalent object which\nis easily JSON-encoded.  If the object in question has no\njson_equivalent() method available then None is returned\ninstead of a string so that the encoding will attempt the next\nstrategy.\n\nIf a caller wishes to disable the calling of json_equivalent()\nmethods, then subclass this class and override this method\nto just return None.\n\n\"\"\"\n", "func_signal": "def encode_equivalent( self, obj, nest_level=0 ):\n", "code": "if hasattr(obj, 'json_equivalent') \\\n       and callable(getattr(obj,'json_equivalent')):\n    obj2 = obj.json_equivalent()\n    if obj2 is obj:\n        # Try to prevent careless infinite recursion\n        raise JSONEncodeError('object has a json_equivalent() method that returns itself',obj)\n    json2 = self.encode( obj2, nest_level=nest_level )\n    return json2\nelse:\n    return None", "path": "deadparrot\\lib\\demjson.py", "repo_name": "gabrielfalcao/dead-parrot", "stars": 16, "license": "None", "language": "python", "size": 381}
{"docstring": "\"\"\"Prevent the specified behavior (turn on a strictness check).\n\nThe list of all possible behaviors is available in the behaviors property.\nYou can see which behaviors are currently prevented by accessing the\nprevented_behaviors property.\n\n\"\"\"\n", "func_signal": "def prevent(self, behavior):\n", "code": "p = '_allow_' + behavior\nif hasattr(self, p):\n    setattr(self, p, False)\nelse:\n    raise AttributeError('Behavior is not known',behavior)", "path": "deadparrot\\lib\\demjson.py", "repo_name": "gabrielfalcao/dead-parrot", "stars": 16, "license": "None", "language": "python", "size": 381}
{"docstring": "\"\"\"Intermediate-level decoder for JSON string literals.\n\nTakes a string and a starting index, and returns a Python\nstring (or unicode string) and the index of the next unparsed\ncharacter.\n\n\"\"\"\n", "func_signal": "def decode_string(self, s, i=0, imax=None):\n", "code": "if imax is None:\n    imax = len(s)\nif imax < i+2 or s[i] not in '\"\\'':\n    raise JSONDecodeError('string literal must be properly quoted',s[i:])\ncloser = s[i]\nif closer == '\\'' and not self._allow_single_quoted_strings:\n    raise JSONDecodeError('string literals must use double quotation marks in strict JSON',s[i:])\ni += 1 # skip quote\nif self._allow_js_string_escapes:\n    escapes = self._escapes_js\nelse:\n    escapes = self._escapes_json\nccallowed = self._allow_control_char_in_string\nchunks = []\n_append = chunks.append\ndone = False\nhigh_surrogate = None\nwhile i < imax:\n    c = s[i]\n    # Make sure a high surrogate is immediately followed by a low surrogate\n    if high_surrogate and (i+1 >= imax or s[i:i+2] != '\\\\u'):\n        raise JSONDecodeError('High unicode surrogate must be followed by a low surrogate',s[i:])\n    if c == closer:\n        i += 1 # skip end quote\n        done = True\n        break\n    elif c == '\\\\':\n        # Escaped character\n        i += 1\n        if i >= imax:\n            raise JSONDecodeError('escape in string literal is incomplete',s[i-1:])\n        c = s[i]\n\n        if '0' <= c <= '7' and self._allow_octal_numbers:\n            # Handle octal escape codes first so special \\0 doesn't kick in yet.\n            # Follow Annex B.1.2 of ECMAScript standard.\n            if '0' <= c <= '3':\n                maxdigits = 3\n            else:\n                maxdigits = 2\n            for k in range(i, i+maxdigits+1):\n                if k >= imax or s[k] not in octaldigits:\n                    break\n            n = decode_octal(s[i:k])\n            if n < 128:\n                _append( chr(n) )\n            else:\n                _append( unichr(n) )\n            i = k\n            continue\n\n        if escapes.has_key(c):\n            _append(escapes[c])\n            i += 1\n        elif c == 'u' or c == 'x':\n            i += 1\n            if c == 'u':\n                digits = 4\n            else: # c== 'x'\n                if not self._allow_js_string_escapes:\n                    raise JSONDecodeError(r'string literals may not use the \\x hex-escape in strict JSON',s[i-1:])\n                digits = 2\n            if i+digits >= imax:\n                raise JSONDecodeError('numeric character escape sequence is truncated',s[i-1:])\n            n = decode_hex( s[i:i+digits] )\n            if high_surrogate:\n                # Decode surrogate pair and clear high surrogate\n                _append( surrogate_pair_as_unicode( high_surrogate, unichr(n) ) )\n                high_surrogate = None\n            elif n < 128:\n                # ASCII chars always go in as a str\n                _append( chr(n) )\n            elif 0xd800 <= n <= 0xdbff: # high surrogate\n                if imax < i + digits + 2 or s[i+digits] != '\\\\' or s[i+digits+1] != 'u':\n                    raise JSONDecodeError('High unicode surrogate must be followed by a low surrogate',s[i-2:])\n                high_surrogate = unichr(n)  # remember until we get to the low surrogate\n            elif 0xdc00 <= n <= 0xdfff: # low surrogate\n                raise JSONDecodeError('Low unicode surrogate must be proceeded by a high surrogate',s[i-2:])\n            else:\n                # Other chars go in as a unicode char\n                _append( unichr(n) )\n            i += digits\n        else:\n            # Unknown escape sequence\n            if self._allow_nonescape_characters:\n                _append( c )\n                i += 1\n            else:\n                raise JSONDecodeError('unsupported escape code in JSON string literal',s[i-1:])\n    elif ord(c) <= 0x1f: # A control character\n        if self.islineterm(c):\n            raise JSONDecodeError('line terminator characters must be escaped inside string literals',s[i:])\n        elif ccallowed:\n            _append( c )\n            i += 1\n        else:\n            raise JSONDecodeError('control characters must be escaped inside JSON string literals',s[i:])\n    else: # A normal character; not an escape sequence or end-quote.\n        # Find a whole sequence of \"safe\" characters so we can append them\n        # all at once rather than one a time, for speed.\n        j = i\n        i += 1\n        while i < imax and s[i] not in unsafe_string_chars and s[i] != closer:\n            i += 1\n        _append(s[j:i])\nif not done:\n    raise JSONDecodeError('string literal is not terminated with a quotation mark',s)\ns = ''.join( chunks )\nreturn s, i", "path": "deadparrot\\lib\\demjson.py", "repo_name": "gabrielfalcao/dead-parrot", "stars": 16, "license": "None", "language": "python", "size": 381}
{"docstring": "\"\"\"Only for minimized automata. Returns a topologically ordered list of\nall the states that induce a finite language. Runs in linear time.\n\"\"\"\n#Step 1: Build the states' profiles\n", "func_signal": "def pluck_leaves(self):\n", "code": "loops    = self.state_hash(0)\ninbound  = self.state_hash(list)\noutbound = self.state_hash(list)\nfor state in self.states:\n    for c in self.alphabet:\n        next = self.delta(state, c)\n        inbound[next].append(state)\n        outbound[state].append(next)\n        if state == next:\n            loops[state] += 1\n#Step 2: Add sink state to to_pluck\nto_pluck = []\nfor state in self.states:\n    if len(outbound[state]) == loops[state]:\n        if not state in self.accepts:\n            #prints(\"Adding '%s' to be plucked\" % state)\n            to_pluck.append(state)\n#Step 3: Pluck!\nplucked = []\nwhile len(to_pluck):\n    state = to_pluck.pop()\n    #prints(\"Plucking %s\" % state)\n    plucked.append(state)\n    for incoming in inbound[state]:\n        #prints(\"Deleting %s->%s edge\" % (incoming, state))\n        outbound[incoming].remove(state)\n        if (len(outbound[incoming]) == 0) and (incoming != state):\n            to_pluck.append(incoming)\n            #prints(\"Adding '%s' to be plucked\" % incoming)\nplucked.reverse()\nreturn plucked", "path": "DFA.py", "repo_name": "reverie/python-automata", "stars": 27, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"The inputs to the class are as follows:\n - states: An iterable containing the states of the DFA. States must be immutable.\n - alphabet: An iterable containing the symbols in the DFA's alphabet. Symbols must be immutable.\n - delta: A complete function from [states]x[alphabets]->[states].\n - start: The state at which the DFA begins operation.\n - accepts: A list containing the \"accepting\" or \"final\" states of the DFA.\n\nMaking delta a function rather than a transition table makes it much easier to define certain DFAs. \nIf you want to use a transition table, you can just do this:\n delta = lambda q,c: transition_table[q][c]\nOne caveat is that the function should not depend on the value of 'states' or 'accepts', since\nthese may be modified during minimization.\n\nFinally, the names of states and inputs should be hashable. This generally means strings, numbers,\nor tuples of hashables.\n\"\"\"\n", "func_signal": "def __init__(self, states, alphabet, delta, start, accepts):\n", "code": "self.states = set(states)\nself.start = start\nself.delta = delta\nself.accepts = set(accepts)\nself.alphabet = set(alphabet)\nself.current_state = start", "path": "DFA.py", "repo_name": "reverie/python-automata", "stars": 27, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Creates a hash with one key for every state in the DFA, and\nall values initialized to the 'value' passed.\n\"\"\"\n", "func_signal": "def state_hash(self, value):\n", "code": "d = {}\nfor state in self.states:\n    if callable(value):\n        d[state] = value()\n    else:\n        d[state] = value\nreturn d", "path": "DFA.py", "repo_name": "reverie/python-automata", "stars": 27, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Given a partition of the DFA's states into equivalence classes,\ncollapses every equivalence class into a single \"representative\" state.\nReturns the hash mapping each old state to its new representative.\n\"\"\"\n", "func_signal": "def collapse(self, partition):\n", "code": "new_states = []\nnew_start = None\nnew_delta = None\nnew_accepts = []\n#alphabet stays the same\nnew_current_state = None\nstate_map = {}\n#build new_states, new_start, new_current_state:\nfor state_class in partition:\n    representative = state_class[0]\n    new_states.append(representative)\n    for state in state_class:\n        state_map[state] = representative\n        if state == self.start:\n            new_start = representative\n        if state == self.current_state:\n            new_current_state = representative\n#build new_accepts:\nfor acc in self.accepts:\n    if acc in new_states:\n        new_accepts.append(acc)\n#build new_delta:\ntransitions = {}\nfor state in new_states:\n    transitions[state] = {}\n    for alpha in self.alphabet:\n        transitions[state][alpha] = state_map[self.delta(state, alpha)]\nnew_delta = (lambda s, a: transitions[s][a])\nself.states = new_states\nself.start = new_start\nself.delta = new_delta\nself.accepts = new_accepts\nself.current_state = new_current_state\nreturn state_map", "path": "DFA.py", "repo_name": "reverie/python-automata", "stars": 27, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Constructs a random DFA with \"states_size\" states and \"alphabet_size\" inputs. Each \ntransition destination is chosen uniformly at random, so the resultant DFA may have \nunreachable states. The optional \"acceptance\" parameter indicates what fraction of \nthe states should be accepting.\n\"\"\"\n", "func_signal": "def random(states_size, alphabet_size, acceptance=0.5):\n", "code": "import random\nstates = range(states_size)\nstart = 0\nalphabet = range(alphabet_size)\naccepts = random.sample(states, int(acceptance*states_size))\ntt = {}\nfor q in states:\n    tt[q] = {}\n    for c in alphabet:\n        tt[q][c] = random.choice(states)\ndelta = lambda q, c: tt[q][c]\nreturn DFA(states, alphabet, delta, start, accepts)", "path": "DFA.py", "repo_name": "reverie/python-automata", "stars": 27, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Indicates whether q1 and q2 only have finitely many distinguishing strings.\"\"\"\n", "func_signal": "def states_fd_equivalent(self, q1, q2):\n", "code": "d1 = DFA(states=self.states, start=q1, accepts=self.accepts, delta=self.delta, alphabet=self.alphabet)\nd2 = DFA(states=self.states, start=q2, accepts=self.accepts, delta=self.delta, alphabet=self.alphabet)\nsd_dfa = symmetric_difference(d1, d2)\nreturn sd_dfa.is_finite()", "path": "DFA.py", "repo_name": "reverie/python-automata", "stars": 27, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"A generalized cross-product constructor over two DFAs. \nThe third argument is a binary boolean function f; a state (q1, q2) in the final\nDFA accepts if f(A[q1],A[q2]), where A indicates the acceptance-value of the state.\n\"\"\"\n", "func_signal": "def cross_product(D1, D2, accept_method):\n", "code": "assert(D1.alphabet == D2.alphabet)\nstates = []\nfor s1 in D1.states:\n    for s2 in D2.states:\n        states.append((s1,s2))\nstart = (D1.start, D2.start)\ndef delta(state_pair, char):\n    next_D1 = D1.delta(state_pair[0], char)\n    next_D2 = D2.delta(state_pair[1], char)\n    return (next_D1, next_D2)\nalphabet = copy(D1.alphabet)\naccepts = []\nfor (s1, s2) in states:\n    a1 = s1 in D1.accepts\n    a2 = s2 in D2.accepts\n    if accept_method(a1, a2):\n        accepts.append((s1, s2))\nreturn DFA(states=states, start=start, delta=delta, accepts=accepts, alphabet=alphabet)", "path": "DFA.py", "repo_name": "reverie/python-automata", "stars": 27, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Returns the set of states reachable from given state q0. The optional\nparameter \"inclusive\" indicates that q0 should always be included.\n\"\"\"\n", "func_signal": "def reachable_from(self, q0, inclusive=True):\n", "code": "reached = self.state_hash(False)\nif inclusive:\n    reached[q0] = True\nto_process = [q0]\nwhile len(to_process):\n    q = to_process.pop()\n    for c in self.alphabet:\n        next = self.delta(q, c)\n        if reached[next] == False:\n            reached[next] = True\n            to_process.append(next)\nreturn filter(lambda q: reached[q], self.states)", "path": "DFA.py", "repo_name": "reverie/python-automata", "stars": 27, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Returns a partition of the states into finite-difference equivalence clases, using\nthe experimental O(n^2) algorithm.\"\"\"\n", "func_signal": "def f_equivalence_classes(self):\n", "code": "sd = symmetric_difference(self, self)\nself_pairs = [(x, x) for x in self.states]\nfd_equiv_pairs = sd.right_finite_states(self_pairs)\nsets = UnionFind()\nfor state in self.states:\n    sets.make_set(state)\nfor (state1, state2) in fd_equiv_pairs:\n    set1, set2 = sets.find(state1), sets.find(state2)\n    if set1 != set2:\n        sets.union(set1, set2)\nstate_classes = sets.as_lists()\nreturn state_classes", "path": "DFA.py", "repo_name": "reverie/python-automata", "stars": 27, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Constructs an unminimized DFA accepting the given finite language.\"\"\"\n", "func_signal": "def from_word_list(language, alphabet):\n", "code": "accepts = language\nstart = ''\nsink = 'sink'\nstates = [start, sink]\nfor word in language:\n    for i in range(len(word)):\n        prefix = word[:i+1]\n        if prefix not in states:\n            states.append(prefix)\nfwl = copy(states)\ndef delta(q, c):\n    next = q+c\n    if next in fwl:\n        return next\n    else:\n        return sink\nreturn DFA(states=states, alphabet=alphabet, delta=delta, start=start, accepts=accepts)", "path": "DFA.py", "repo_name": "reverie/python-automata", "stars": 27, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Updates the DFA's current state based on an iterable of inputs.\"\"\"\n", "func_signal": "def input_sequence(self, char_sequence):\n", "code": "for char in char_sequence:\n    self.input(char)", "path": "DFA.py", "repo_name": "reverie/python-automata", "stars": 27, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Constructs an unminimized DFA recognizing the union of the languages of two given DFAs.\"\"\"\n", "func_signal": "def union(D1, D2):\n", "code": "f = bool.__or__\nreturn cross_product(D1, D2, f)", "path": "DFA.py", "repo_name": "reverie/python-automata", "stars": 27, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Returns a dictionary mapping each state to its distance from the starting state.\"\"\"\n", "func_signal": "def levels(self):\n", "code": "levels = {}\nseen = [self.start]\nlevels[self.start] = 0\nlevel_number = 0\nlevel_states = [self.start]\nwhile len(level_states):\n    next_level_states = []\n    next_level_number = level_number + 1\n    for q in level_states:\n        for c in self.alphabet:\n            next = self.delta(q, c)\n            if next not in seen:\n                seen.append(next)\n                levels[next] = next_level_number\n                next_level_states.append(next)\n    level_states = next_level_states\n    level_number = next_level_number\nreturn levels", "path": "DFA.py", "repo_name": "reverie/python-automata", "stars": 27, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Returns a partition of self.states into Myhill-Nerode equivalence classes.\"\"\"\n", "func_signal": "def mn_classes(self):\n", "code": "changed = True\nclasses = []\nif self.accepts != []:\n    classes.append(self.accepts)\nnonaccepts = filter(lambda x: x not in self.accepts, self.states)\nif nonaccepts != []:\n    classes.append(nonaccepts)\nwhile changed:\n    changed = False\n    for cl in classes:\n        local_change = False\n        for alpha in self.alphabet:\n            next_class = None\n            new_class = []\n            for state in cl:\n                next = self.delta(state, alpha)\n                if next_class == None:\n                    for c in classes:\n                        if next in c:\n                            next_class = c\n                elif next not in next_class:\n                    new_class.append(state)\n                    changed = True\n                    local_change = True\n            if local_change == True:\n                old_class = []\n                for c in cl:\n                    if c not in new_class:\n                        old_class.append(c)\n                classes.remove(cl)\n                classes.append(old_class)\n                classes.append(new_class)\n                break\nreturn classes", "path": "DFA.py", "repo_name": "reverie/python-automata", "stars": 27, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Returns a DFA that accepts all binary numbers equal to 0 mod n. Use the optional\nparameter \"base\" if you want something other than binary. The empty string is also \nincluded in the DFA's language.\n\"\"\"\n", "func_signal": "def modular_zero(n, base=2):\n", "code": "states = range(n)\nalphabet = map(str, range(base))\ndelta = lambda q, c: ((q*base+int(c)) % n)\nstart = 0\naccepts = [0]\nreturn DFA(states=states, alphabet=alphabet, delta=delta, start=start, accepts=accepts)", "path": "DFA.py", "repo_name": "reverie/python-automata", "stars": 27, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Returns the partition of the state-set into the preamble and \nkernel as a 2-tuple. A state is in the preamble iff there \nare finitely many strings that reach it from the start state.\n\nSee \"The DFAs of Finitely Different Regular Languages\" for context.\n\"\"\"\n#O(n^2): can this be improved?\n", "func_signal": "def preamble_and_kernel(self):\n", "code": "reachable = {}\nfor q in self.states:\n    reachable[q] = self.reachable_from(q, inclusive=False)\nin_fin = self.state_hash(True)\nfor q in reachable[self.start]:\n    if q in reachable[q]:\n        for next in reachable[q]:\n            in_fin[next] = False\npreamble = filter(lambda x: in_fin[x], self.states)\nkernel = filter(lambda x: not in_fin[x], self.states)\nreturn (preamble, kernel)", "path": "DFA.py", "repo_name": "reverie/python-automata", "stars": 27, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Constructs an unminimized DFA recognizing the intersection of the languages of two given DFAs.\"\"\"\n", "func_signal": "def intersection(D1, D2):\n", "code": "f = bool.__and__\nreturn cross_product(D1, D2, f)", "path": "DFA.py", "repo_name": "reverie/python-automata", "stars": 27, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"DFCA minimization\"\nInput: \"self\" is a DFA accepting a finite language\nResult: \"self\" is DFCA-minimized, and the returned value is the length of the longest\n        word accepted by the original DFA\n\nSee \"Minimal cover-automata for finite languages\" for context on DFCAs, and\n\"An O(n^2) Algorithm for Constructing Minimal Cover Automata for Finite Languages\"\nfor the source of this algorithm (Campeanu, Paun, Santean, and Yu). We follow their\nalgorithm exactly, except that \"l\" is optionally calculated for you, and the state-\nordering is automatically created.\n\nThere exists a faster, O(n*logn)-time algorithm due to Korner, from CIAA 2002.\n\"\"\"\n\n", "func_signal": "def DFCA_minimize(self, l=None):\n", "code": "assert(self.is_finite())\n\nself.minimize()\n\n###Step 0: Numbering the states and computing \"l\"\nn = len(self.states) - 1\nstate_order = self.pluck_leaves()\nif l==None:\n    l = self.longest_word_length()\n#We're giving each state a numerical name so that the  algorithm can \n# run on an \"ordered\" DFA -- see the paper for why. These functions\n# allow us to copiously convert between names.\ndef nn(q): # \"numerical name\"\n    return state_order.index(q)\ndef rn(n): # \"real name\"\n    return state_order[n]\n\n###Step 1: Computing the gap function\n# 1.1 -- Step numbering is from the paper\nlevel = self.levels() #holds real names\ngap = {}  #holds numerical names\n# 1.2 \nfor i in range(n):\n    gap[(i, n)] = l\nif level[rn(n)] <= l:\n    for q in self.accepts:\n        gap[(nn(q), n)] = 0\n# 1.3\nfor i in range(n-1):\n    for j in range(i+1, n):\n        if (rn(i) in self.accepts)^(rn(j) in self.accepts):\n            gap[(i,j)] = 0\n        else:\n            gap[(i,j)] = l\n# 1.4\ndef level_range(i, j):\n    return l - max(level[rn(i)], level[rn(j)])\nfor i in range(n-2, -1, -1):\n    for j in range(n, i, -1):\n        for char in self.alphabet:\n            i2 = nn(self.delta(rn(i), char))\n            j2 = nn(self.delta(rn(j), char))\n            if i2 != j2:\n                if i2 < j2:\n                    g = gap[(i2, j2)]\n                else:\n                    g = gap[(j2, i2)]\n                if g+1 <= level_range(i, j):\n                    gap[(i,j)] = min(gap[(i,j)], g+1)\n\n###Step 2: Merging states\n# 2.1\nP = {}\nfor i in range(n+1):\n    P[i] = False\n# 2.2\nfor i in range(n):\n    if P[i] == False:\n        for j in range(i+1, n+1):\n            if (P[j] == False) and (gap[(i,j)] == l):\n                self.state_merge(rn(j), rn(i))\n                P[j] = True\nreturn l", "path": "DFA.py", "repo_name": "reverie/python-automata", "stars": 27, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Merges q1 into q2. All transitions to q1 are moved to q2.\nIf q1 was the start or current state, those are also moved to q2.\n\"\"\"\n", "func_signal": "def state_merge(self, q1, q2):\n", "code": "self.states.remove(q1)\nif q1 in self.accepts:\n    self.accepts.remove(q1)\nif self.current_state == q1:\n    self.current_state = q2\nif self.start == q1:\n    self.start = q2\ntransitions = {}\nfor state in self.states: #without q1\n    transitions[state] = {}\n    for char in self.alphabet:\n        next = self.delta(state, char)\n        if next == q1:\n            next = q2\n        transitions[state][char] = next\nself.delta = (lambda s, c: transitions[s][c])", "path": "DFA.py", "repo_name": "reverie/python-automata", "stars": 27, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Indicates whether the DFA's language is a finite set.\"\"\"\n", "func_signal": "def is_finite(self):\n", "code": "D2 = self.copy()\nD2.minimize()\nplucked = D2.pluck_leaves()\nreturn (D2.start in plucked)", "path": "DFA.py", "repo_name": "reverie/python-automata", "stars": 27, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"\nShortcut to create a token with random key/secret.\n\"\"\"\n", "func_signal": "def create_token(self, consumer, token_type, timestamp, user=None):\n", "code": "token, created = self.get_or_create(consumer=consumer, \n                                    token_type=token_type, \n                                    timestamp=timestamp,\n                                    user=user)\n\nif created:\n    token.key, token.secret = self.generate_random_codes()\n    token.save()\n\nreturn token", "path": "lib\\piston\\managers.py", "repo_name": "sandyarmstrong/snowy", "stars": 16, "license": "agpl-3.0", "language": "python", "size": 2301}
{"docstring": "\"\"\"\nGets a function ref to deserialize content\nfor a certain mimetype.\n\"\"\"\n", "func_signal": "def loader_for_type(self, ctype):\n", "code": "for loadee, mimes in Mimer.TYPES.iteritems():\n    for mime in mimes:\n        if ctype.startswith(mime):\n            return loadee", "path": "lib\\piston\\utils.py", "repo_name": "sandyarmstrong/snowy", "stars": 16, "license": "agpl-3.0", "language": "python", "size": 2301}
{"docstring": "\"\"\"Return whether the given ``value`` can be used as a JSON-P callback.\"\"\"\n\n", "func_signal": "def is_valid_jsonp_callback_value(value):\n", "code": "for identifier in value.split(u'.'):\n    while '[' in identifier:\n        if not has_valid_array_index(identifier):\n            return False\n        identifier = replace_array_index(u'', identifier)\n    if not is_valid_javascript_identifier(identifier):\n        return False\n\nreturn True", "path": "lib\\piston\\validate_jsonp.py", "repo_name": "sandyarmstrong/snowy", "stars": 16, "license": "agpl-3.0", "language": "python", "size": 2301}
{"docstring": "\"\"\"\nAdd cache if you use a default resource.\n\"\"\"\n", "func_signal": "def get_default_resource(self, name):\n", "code": "if not self._default_resource:\n    self._default_resource = self.get(name=name)\n\nreturn self._default_resource", "path": "lib\\piston\\managers.py", "repo_name": "sandyarmstrong/snowy", "stars": 16, "license": "agpl-3.0", "language": "python", "size": 2301}
{"docstring": "\"\"\"\nDiffs the two provided Meta definitions (dicts).\n\"\"\"\n\n# First, diff unique_together\n", "func_signal": "def meta_diff(old, new):\n", "code": "old_unique_together = eval(old.get('unique_together', \"[]\"))\nnew_unique_together = eval(new.get('unique_together', \"[]\"))\n\nadded_uniques = set()\nremoved_uniques = set()\n\nfor entry in old_unique_together:\n    if entry not in new_unique_together:\n        removed_uniques.add(tuple(entry))\n\nfor entry in new_unique_together:\n    if entry not in old_unique_together:\n        added_uniques.add(tuple(entry))\n\nreturn added_uniques, removed_uniques", "path": "lib\\south\\management\\commands\\startmigration.py", "repo_name": "sandyarmstrong/snowy", "stars": 16, "license": "agpl-3.0", "language": "python", "size": 2301}
{"docstring": "\"\"\"\nDecorator requiring a certain mimetype. There's a nifty\nhelper called `require_extended` below which requires everything\nwe support except for post-data via form.\n\"\"\"\n", "func_signal": "def require_mime(*mimes):\n", "code": "@decorator\ndef wrap(f, self, request, *args, **kwargs):\n    m = Mimer(request)\n    realmimes = set()\n\n    rewrite = { 'json':   'application/json',\n                'yaml':   'application/x-yaml',\n                'xml':    'text/xml',\n                'pickle': 'application/python-pickle' }\n\n    for idx, mime in enumerate(mimes):\n        realmimes.add(rewrite.get(mime, mime))\n\n    if not m.content_type() in realmimes:\n        return rc.BAD_REQUEST\n\n    return f(self, request, *args, **kwargs)\nreturn wrap", "path": "lib\\piston\\utils.py", "repo_name": "sandyarmstrong/snowy", "stars": 16, "license": "agpl-3.0", "language": "python", "size": 2301}
{"docstring": "\"\"\"\nChecks whether an OpenID user has given all of his user details\n\"\"\"\n", "func_signal": "def registration_complete(self):\n", "code": "if self.openid_user:\n    if self.user.username[:10] == \"openiduser\" or self.user.email == \"\":\n        return False\nreturn True", "path": "accounts\\models.py", "repo_name": "sandyarmstrong/snowy", "stars": 16, "license": "agpl-3.0", "language": "python", "size": 2301}
{"docstring": "\"\"\"\nCreate a UserProfile object in response to a new User being created.\n\"\"\"\n", "func_signal": "def _create_profile(sender, instance, created, **kwargs):\n", "code": "if not created: return\nUserProfile.objects.create(user=instance)", "path": "accounts\\models.py", "repo_name": "sandyarmstrong/snowy", "stars": 16, "license": "agpl-3.0", "language": "python", "size": 2301}
{"docstring": "\"\"\"Return whether the given ``id`` is a valid Javascript identifier.\"\"\"\n\n", "func_signal": "def is_valid_javascript_identifier(identifier, escape=r'\\u', ucd_cat=category):\n", "code": "if not identifier:\n    return False\n\nif not isinstance(identifier, unicode):\n    try:\n        identifier = unicode(identifier, 'utf-8')\n    except UnicodeDecodeError:\n        return False\n\nif escape in identifier:\n\n    new = []; add_char = new.append\n    split_id = identifier.split(escape)\n    add_char(split_id.pop(0))\n\n    for segment in split_id:\n        if len(segment) < 4:\n            return False\n        try:\n            add_char(unichr(int('0x' + segment[:4], 16)))\n        except Exception:\n            return False\n        add_char(segment[4:])\n        \n    identifier = u''.join(new)\n\nif is_reserved_js_word(identifier):\n    return False\n\nfirst_char = identifier[0]\n\nif not ((first_char in valid_jsid_chars) or\n        (ucd_cat(first_char) in valid_jsid_categories_start)):\n    return False\n\nfor char in identifier[1:]:\n    if not ((char in valid_jsid_chars) or\n            (ucd_cat(char) in valid_jsid_categories)):\n        return False\n\nreturn True", "path": "lib\\piston\\validate_jsonp.py", "repo_name": "sandyarmstrong/snowy", "stars": 16, "license": "agpl-3.0", "language": "python", "size": 2301}
{"docstring": "\"\"\"\nGiven the name of something that needs orm. stuck on the front and\na python eval-able string, possibly add orm. to it.\n\"\"\"\n", "func_signal": "def poss_ormise(default_app, rel_to, arg):\n", "code": "orig_arg = arg\n# If it's not a relative field, short-circuit out\nif not rel_to:\n    return arg\n# Get the name of the other model\nrel_name = rel_to._meta.object_name\n# Is it in a different app? If so, use proper addressing.\nif rel_to._meta.app_label != default_app:\n    real_name = \"orm['%s.%s']\" % (rel_to._meta.app_label, rel_name)\nelse:\n    real_name = \"orm.%s\" % rel_name\n# If it's surrounded by quotes, get rid of those\nfor quote_type in QUOTES:\n    l = len(quote_type)\n    if arg[:l] == quote_type and arg[-l:] == quote_type:\n        arg = arg[l:-l]\n        break\n# Now see if we can replace it.\nif arg.lower() == rel_name.lower():\n    return real_name\n# Or perhaps it's app.model?\nif arg.lower() == rel_to._meta.app_label.lower() + \".\" + rel_name.lower():\n    return real_name\n# Or perhaps it's 'self'?\nif arg == RECURSIVE_RELATIONSHIP_CONSTANT:\n    return real_name\nreturn orig_arg", "path": "lib\\south\\management\\commands\\startmigration.py", "repo_name": "sandyarmstrong/snowy", "stars": 16, "license": "agpl-3.0", "language": "python", "size": 2301}
{"docstring": "\"\"\"\nReturns a fresh `HttpResponse` when getting \nan \"attribute\". This is backwards compatible\nwith 0.2, which is important.\n\"\"\"\n", "func_signal": "def __getattr__(self, attr):\n", "code": "try:\n    (r, c) = self.CODES.get(attr)\nexcept TypeError:\n    raise AttributeError(attr)\n\nclass HttpResponseWrapper(HttpResponse):\n    \"\"\"\n    Wrap HttpResponse and make sure that the internal _is_string \n    flag is updated when the _set_content method (via the content \n    property) is called\n    \"\"\"\n    def _set_content(self, content):\n        \"\"\"\n        Set the _container and _is_string properties based on the \n        type of the value parameter. This logic is in the construtor\n        for HttpResponse, but doesn't get repeated when setting \n        HttpResponse.content although this bug report (feature request)\n        suggests that it should: http://code.djangoproject.com/ticket/9403 \n        \"\"\"\n        if not isinstance(content, basestring) and hasattr(content, '__iter__'):\n            self._container = content\n            self._is_string = False\n        else:\n            self._container = [content]\n            self._is_string = True\n\n    content = property(HttpResponse._get_content, _set_content)            \n\nreturn HttpResponseWrapper(r, content_type='text/plain', status=c)", "path": "lib\\piston\\utils.py", "repo_name": "sandyarmstrong/snowy", "stars": 16, "license": "agpl-3.0", "language": "python", "size": 2301}
{"docstring": "\"\"\"\nReturns the difference between the old and new sets of models as a 5-tuple:\nadded_models, deleted_models, added_fields, deleted_fields, changed_fields\n\"\"\"\n\n", "func_signal": "def models_diff(old, new):\n", "code": "added_models = set()\ndeleted_models = set()\nignored_models = set() # Stubs for backwards\ncontinued_models = set() # Models that existed before and after\nadded_fields = set()\ndeleted_fields = set()\nchanged_fields = []\n\n# See if anything's vanished\nfor key in old:\n    if key not in new:\n        if \"_stub\" not in old[key]:\n            deleted_models.add(key)\n        else:\n            ignored_models.add(key)\n\n# Or appeared\nfor key in new:\n    if key not in old:\n        added_models.add(key)\n\n# Now, for every model that's stayed the same, check its fields.\nfor key in old:\n    if key not in deleted_models and key not in ignored_models:\n        continued_models.add(key)\n        still_there = set()\n        # Find fields that have vanished.\n        for fieldname in old[key]:\n            if fieldname != \"Meta\" and fieldname not in new[key]:\n                deleted_fields.add((key, fieldname))\n            else:\n                still_there.add(fieldname)\n        # And ones that have appeared\n        for fieldname in new[key]:\n            if fieldname != \"Meta\" and fieldname not in old[key]:\n                added_fields.add((key, fieldname))\n        # For the ones that exist in both models, see if they were changed\n        for fieldname in still_there:\n            if fieldname != \"Meta\" and \\\n               remove_useless_attributes(new[key][fieldname], True) != \\\n               remove_useless_attributes(old[key][fieldname], True):\n                changed_fields.append((key, fieldname, old[key][fieldname], new[key][fieldname]))\n\nreturn added_models, deleted_models, continued_models, added_fields, deleted_fields, changed_fields", "path": "lib\\south\\management\\commands\\startmigration.py", "repo_name": "sandyarmstrong/snowy", "stars": 16, "license": "agpl-3.0", "language": "python", "size": 2301}
{"docstring": "\"\"\"\nTruncates a string after a certain number of characters, but respects word boundaries.\n\nArgument: Number of characters to truncate after.\n\"\"\"\n", "func_signal": "def truncatechars(value, arg):\n", "code": "try:\n    length = int(arg)\nexcept ValueError: # If the argument is not a valid integer.\n    return value # Fail silently.\nreturn truncate_chars(value, length)", "path": "core\\templatetags\\truncate.py", "repo_name": "sandyarmstrong/snowy", "stars": 16, "license": "agpl-3.0", "language": "python", "size": 2301}
{"docstring": "\"\"\"\nShortcut to create a consumer with random key/secret.\n\"\"\"\n", "func_signal": "def create_consumer(self, name, description=None, user=None):\n", "code": "consumer, created = self.get_or_create(name=name)\n\nif user:\n    consumer.user = user\n\nif description:\n    consumer.description = description\n\nif created:\n    consumer.key, consumer.secret = self.generate_random_codes()\n    consumer.save()\n\nreturn consumer", "path": "lib\\piston\\managers.py", "repo_name": "sandyarmstrong/snowy", "stars": 16, "license": "agpl-3.0", "language": "python", "size": 2301}
{"docstring": "\"\"\"\nDjango doesn't particularly understand REST.\nIn case we send data over PUT, Django won't\nactually look at the data and load it. We need\nto twist its arm here.\n\nThe try/except abominiation here is due to a bug\nin mod_python. This should fix it.\n\"\"\"\n", "func_signal": "def coerce_put_post(request):\n", "code": "if request.method == \"PUT\":\n    # Bug fix: if _load_post_and_files has already been called, for\n    # example by middleware accessing request.POST, the below code to\n    # pretend the request is a POST instead of a PUT will be too late\n    # to make a difference. Also calling _load_post_and_files will result \n    # in the following exception:\n    #   AttributeError: You cannot set the upload handlers after the upload has been processed.\n    # The fix is to check for the presence of the _post field which is set \n    # the first time _load_post_and_files is called (both by wsgi.py and \n    # modpython.py). If it's set, the request has to be 'reset' to redo\n    # the query value parsing in POST mode.\n    if hasattr(request, '_post'):\n        del request._post\n        del request._files\n    \n    try:\n        request.method = \"POST\"\n        request._load_post_and_files()\n        request.method = \"PUT\"\n    except AttributeError:\n        request.META['REQUEST_METHOD'] = 'POST'\n        request._load_post_and_files()\n        request.META['REQUEST_METHOD'] = 'PUT'\n        \n    request.PUT = request.POST", "path": "lib\\piston\\utils.py", "repo_name": "sandyarmstrong/snowy", "stars": 16, "license": "agpl-3.0", "language": "python", "size": 2301}
{"docstring": "\"\"\"\nReturns a set of models this one depends on to be defined; things like\nOneToOneFields as ID, ForeignKeys everywhere, etc.\n\"\"\"\n", "func_signal": "def model_dependencies(model, last_models=None):\n", "code": "depends = {}\nfor field in model._meta.fields + model._meta.many_to_many:\n    depends.update(field_dependencies(field, last_models))\nreturn depends", "path": "lib\\south\\management\\commands\\startmigration.py", "repo_name": "sandyarmstrong/snowy", "stars": 16, "license": "agpl-3.0", "language": "python", "size": 2301}
{"docstring": "\"\"\"\nSimple throttling decorator, caches\nthe amount of requests made in cache.\n\nIf used on a view where users are required to\nlog in, the username is used, otherwise the\nIP address of the originating request is used.\n\nParameters::\n - `max_requests`: The maximum number of requests\n - `timeout`: The timeout for the cache entry (default: 1 hour)\n\"\"\"\n", "func_signal": "def throttle(max_requests, timeout=60*60, extra=''):\n", "code": "@decorator\ndef wrap(f, self, request, *args, **kwargs):\n    if request.user.is_authenticated():\n        ident = request.user.username\n    else:\n        ident = request.META.get('REMOTE_ADDR', None)\n\n    if hasattr(request, 'throttle_extra'):\n        \"\"\"\n        Since we want to be able to throttle on a per-\n        application basis, it's important that we realize\n        that `throttle_extra` might be set on the request\n        object. If so, append the identifier name with it.\n        \"\"\"\n        ident += ':%s' % str(request.throttle_extra)\n    \n    if ident:\n        \"\"\"\n        Preferrably we'd use incr/decr here, since they're\n        atomic in memcached, but it's in django-trunk so we\n        can't use it yet. If someone sees this after it's in\n        stable, you can change it here.\n        \"\"\"\n        ident += ':%s' % extra\n\n        now = time.time()\n        count, expiration = cache.get(ident, (1, None))\n\n        if expiration is None:\n            expiration = now + timeout\n\n        if count >= max_requests and expiration > now:\n            t = rc.THROTTLED\n            wait = int(expiration - now)\n            t.content = 'Throttled, wait %d seconds.' % wait\n            t['Retry-After'] = wait\n            return t\n\n        cache.set(ident, (count+1, expiration), (expiration - now))\n\n    return f(self, request, *args, **kwargs)\nreturn wrap", "path": "lib\\piston\\utils.py", "repo_name": "sandyarmstrong/snowy", "stars": 16, "license": "agpl-3.0", "language": "python", "size": 2301}
{"docstring": "\"\"\"\nConvert a note into a html string\n\"\"\"\n", "func_signal": "def note_to_html(note, author):\n", "code": "from lxml import etree\nimport os.path\n\n# Extension function for XSL. Called twice per link,\n# so we keep a little cache to save on lookups\nlink_cache = {}\ndef get_url_for_title(dummy, link_text):\n    link_text = unicode(link_text)\n    if link_text in link_cache:\n        return link_cache[link_text]\n    try:\n        note = Note.objects.get(author=author, title=link_text)\n        note_url = note.get_absolute_url()\n        link_cache[link_text] = note_url\n        return note_url\n    except ObjectDoesNotExist:\n        return None\n\nns = etree.FunctionNamespace(\"http://tomboy-online.org/stuff\")\nns.prefix = \"tomboyonline\"\nns['get_url_for_title'] = get_url_for_title\n\nstyle = etree.parse(os.path.join(settings.PROJECT_ROOT,\n                                 'data/note2xhtml.xsl'))\ntransform = etree.XSLT(style)\n\ntemplate = CONTENT_TEMPLATES.get(note.content_version, DEFAULT_CONTENT_TEMPLATE)\ncomplete_xml = template.replace('%%%CONTENT%%%', note.content)\ndoc = etree.fromstring(complete_xml)\n\nresult = transform(doc)\nbody = str(result)\nreturn body", "path": "notes\\utils.py", "repo_name": "sandyarmstrong/snowy", "stars": 16, "license": "agpl-3.0", "language": "python", "size": 2301}
{"docstring": "\"\"\"\nGiven the defualt app, the field class,\nand the defn triple (or string), make the defition string.\n\"\"\"\n# It might be a defn string already...\n", "func_signal": "def make_field_constructor(default_app, field, triple):\n", "code": "if isinstance(triple, (str, unicode)):\n    return triple\n# OK, do it the hard way\nif hasattr(field, \"rel\") and hasattr(field.rel, \"to\") and field.rel.to:\n    rel_to = field.rel.to\nelse:\n    rel_to = None\nargs = [poss_ormise(default_app, rel_to, arg) for arg in triple[1]]\nkwds = [\"%s=%s\" % (k, poss_ormise(default_app, rel_to, v)) for k,v in triple[2].items()]\nreturn \"%s(%s)\" % (triple[0], \", \".join(args+kwds))", "path": "lib\\south\\management\\commands\\startmigration.py", "repo_name": "sandyarmstrong/snowy", "stars": 16, "license": "agpl-3.0", "language": "python", "size": 2301}
{"docstring": "\"\"\"\nTemplate filter to truncate a string to at most num characters respecting word\nboundaries.\n\"\"\"\n", "func_signal": "def truncate_chars(s, num):\n", "code": "s = force_unicode(s)\nlength = int(num)\nif len(s) > length:\n    length = length - 3\n    if s[length-1] == ' ' or s[length] == ' ':\n        s = s[:length].strip()\n    else:\n        words = s[:length].split()\n        if len(words) > 1:\n            del words[-1]\n        s = u' '.join(words)\n    s += '...'\nreturn s", "path": "core\\templatetags\\truncate.py", "repo_name": "sandyarmstrong/snowy", "stars": 16, "license": "agpl-3.0", "language": "python", "size": 2301}
{"docstring": "# choices can be any iterable\n", "func_signal": "def __init__(self, attrs=None, choices=()):\n", "code": "self.attrs = attrs or {}\nself.choices = choices", "path": "lib\\django\\django\\newforms\\widgets.py", "repo_name": "jchris/portable-google-app-engine-sdk", "stars": 23, "license": "other", "language": "python", "size": 2214}
{"docstring": "# Format:\n#   !!python/object:module.name { ... state ... }\n", "func_signal": "def construct_python_object(self, suffix, node):\n", "code": "instance = self.make_python_instance(suffix, node, newobj=True)\nyield instance\ndeep = hasattr(instance, '__setstate__')\nstate = self.construct_mapping(node, deep=deep)\nself.set_python_instance_state(instance, state)", "path": "lib\\yaml\\lib\\yaml\\constructor.py", "repo_name": "jchris/portable-google-app-engine-sdk", "stars": 23, "license": "other", "language": "python", "size": 2214}
{"docstring": "# Convenience function using get_model avoids a circular import when using this model\n", "func_signal": "def get_content_type(self, obj):\n", "code": "ContentType = get_model(\"contenttypes\", \"contenttype\")\nreturn ContentType.objects.get_for_model(obj)", "path": "lib\\django\\django\\db\\models\\fields\\generic.py", "repo_name": "jchris/portable-google-app-engine-sdk", "stars": 23, "license": "other", "language": "python", "size": 2214}
{"docstring": "# RadioSelect is represented by multiple <input type=\"radio\"> fields,\n# each of which has a distinct ID. The IDs are made distinct by a \"_X\"\n# suffix, where X is the zero-based index of the radio field. Thus,\n# the label for a RadioSelect should reference the first one ('_0').\n", "func_signal": "def id_for_label(self, id_):\n", "code": "if id_:\n    id_ += '_0'\nreturn id_", "path": "lib\\django\\django\\newforms\\widgets.py", "repo_name": "jchris/portable-google-app-engine-sdk", "stars": 23, "license": "other", "language": "python", "size": 2214}
{"docstring": "# Make sure the fields exist (these raise FieldDoesNotExist, \n# which is a fine error to raise here)\n", "func_signal": "def contribute_to_class(self, cls, name):\n", "code": "self.name = name\nself.model = cls\nself.cache_attr = \"_%s_cache\" % name\n\n# For some reason I don't totally understand, using weakrefs here doesn't work.\ndispatcher.connect(self.instance_pre_init, signal=signals.pre_init, sender=cls, weak=False)\n\n# Connect myself as the descriptor for this field\nsetattr(cls, name, self)", "path": "lib\\django\\django\\db\\models\\fields\\generic.py", "repo_name": "jchris/portable-google-app-engine-sdk", "stars": 23, "license": "other", "language": "python", "size": 2214}
{"docstring": "\"\"\"\nFactory function for a manager that subclasses 'superclass' (which is a\nManager) and adds behavior for generic related objects.\n\"\"\"\n\n", "func_signal": "def create_generic_related_manager(superclass):\n", "code": "class GenericRelatedObjectManager(superclass):\n    def __init__(self, model=None, core_filters=None, instance=None, symmetrical=None,\n                 join_table=None, source_col_name=None, target_col_name=None, content_type=None,\n                 content_type_field_name=None, object_id_field_name=None):\n        \n        super(GenericRelatedObjectManager, self).__init__()\n        self.core_filters = core_filters or {}\n        self.model = model\n        self.content_type = content_type\n        self.symmetrical = symmetrical\n        self.instance = instance\n        self.join_table = join_table\n        self.join_table = model._meta.db_table\n        self.source_col_name = source_col_name\n        self.target_col_name = target_col_name\n        self.content_type_field_name = content_type_field_name\n        self.object_id_field_name = object_id_field_name\n        self.pk_val = self.instance._get_pk_val()\n                    \n    def get_query_set(self):\n        query = {\n            '%s__pk' % self.content_type_field_name : self.content_type.id, \n            '%s__exact' % self.object_id_field_name : self.pk_val,\n        }\n        return superclass.get_query_set(self).filter(**query)\n\n    def add(self, *objs):\n        for obj in objs:\n            setattr(obj, self.content_type_field_name, self.content_type)\n            setattr(obj, self.object_id_field_name, self.pk_val)\n            obj.save()\n    add.alters_data = True\n\n    def remove(self, *objs):\n        for obj in objs:\n            obj.delete()\n    remove.alters_data = True\n\n    def clear(self):\n        for obj in self.all():\n            obj.delete()\n    clear.alters_data = True\n\n    def create(self, **kwargs):\n        kwargs[self.content_type_field_name] = self.content_type\n        kwargs[self.object_id_field_name] = self.pk_val\n        obj = self.model(**kwargs)\n        obj.save()\n        return obj\n    create.alters_data = True\n\nreturn GenericRelatedObjectManager", "path": "lib\\django\\django\\db\\models\\fields\\generic.py", "repo_name": "jchris/portable-google-app-engine-sdk", "stars": 23, "license": "other", "language": "python", "size": 2214}
{"docstring": "# Construct and return the next document.\n", "func_signal": "def get_data(self):\n", "code": "if self.check_node():\n    return self.construct_document(self.get_node())", "path": "lib\\yaml\\lib\\yaml\\constructor.py", "repo_name": "jchris/portable-google-app-engine-sdk", "stars": 23, "license": "other", "language": "python", "size": 2214}
{"docstring": "# check_test is a callable that takes a value and returns True\n# if the checkbox should be checked for that value.\n", "func_signal": "def __init__(self, attrs=None, check_test=bool):\n", "code": "self.attrs = attrs or {}\nself.check_test = check_test", "path": "lib\\django\\django\\newforms\\widgets.py", "repo_name": "jchris/portable-google-app-engine-sdk", "stars": 23, "license": "other", "language": "python", "size": 2214}
{"docstring": "# Note: we do not check for duplicate keys, because it's too\n# CPU-expensive.\n", "func_signal": "def construct_yaml_omap(self, node):\n", "code": "omap = []\nyield omap\nif not isinstance(node, SequenceNode):\n    raise ConstructorError(\"while constructing an ordered map\", node.start_mark,\n            \"expected a sequence, but found %s\" % node.id, node.start_mark)\nfor subnode in node.value:\n    if not isinstance(subnode, MappingNode):\n        raise ConstructorError(\"while constructing an ordered map\", node.start_mark,\n                \"expected a mapping of length 1, but found %s\" % subnode.id,\n                subnode.start_mark)\n    if len(subnode.value) != 1:\n        raise ConstructorError(\"while constructing an ordered map\", node.start_mark,\n                \"expected a single mapping item, but found %d items\" % len(subnode.value),\n                subnode.start_mark)\n    key_node, value_node = subnode.value[0]\n    key = self.construct_object(key_node)\n    value = self.construct_object(value_node)\n    omap.append((key, value))", "path": "lib\\yaml\\lib\\yaml\\constructor.py", "repo_name": "jchris/portable-google-app-engine-sdk", "stars": 23, "license": "other", "language": "python", "size": 2214}
{"docstring": "# See the comment for RadioSelect.id_for_label()\n", "func_signal": "def id_for_label(self, id_):\n", "code": "if id_:\n    id_ += '_0'\nreturn id_", "path": "lib\\django\\django\\newforms\\widgets.py", "repo_name": "jchris/portable-google-app-engine-sdk", "stars": 23, "license": "other", "language": "python", "size": 2214}
{"docstring": "\"\"\"\nReturns the 1-based index of the last object on the given page,\nrelative to total objects found (hits).\n\"\"\"\n", "func_signal": "def last_on_page(self, page_number):\n", "code": "page_number = self.validate_page_number(page_number)\npage_number += 1   # 1-base\nif page_number == self.pages:\n    return self.hits\nreturn page_number * self.num_per_page", "path": "lib\\django\\django\\core\\paginator.py", "repo_name": "jchris/portable-google-app-engine-sdk", "stars": 23, "license": "other", "language": "python", "size": 2214}
{"docstring": "# value is a list of values, each corresponding to a widget\n# in self.widgets.\n", "func_signal": "def render(self, name, value, attrs=None):\n", "code": "if not isinstance(value, list):\n    value = self.decompress(value)\noutput = []\nfor i, widget in enumerate(self.widgets):\n    try:\n        widget_value = value[i]\n    except KeyError:\n        widget_value = None\n    output.append(widget.render(name + '_%s' % i, widget_value, attrs))\nreturn self.format_output(output)", "path": "lib\\django\\django\\newforms\\widgets.py", "repo_name": "jchris/portable-google-app-engine-sdk", "stars": 23, "license": "other", "language": "python", "size": 2214}
{"docstring": "# Handle initalizing an object with the generic FK instaed of \n# content-type/object-id fields.        \n", "func_signal": "def instance_pre_init(self, signal, sender, args, kwargs):\n", "code": "if kwargs.has_key(self.name):\n    value = kwargs.pop(self.name)\n    kwargs[self.ct_field] = self.get_content_type(value)\n    kwargs[self.fk_field] = value._get_pk_val()", "path": "lib\\django\\django\\db\\models\\fields\\generic.py", "repo_name": "jchris/portable-google-app-engine-sdk", "stars": 23, "license": "other", "language": "python", "size": 2214}
{"docstring": "\"\"\"Constructor.\n\nArgs:\n  # email is optional. it defaults to the current user.\n  email: string\n\"\"\"\n", "func_signal": "def __init__(self, email=None, _auth_domain=None, nickname=None):\n", "code": "if _auth_domain is None:\n  _auth_domain = os.environ.get('AUTH_DOMAIN')\nelse:\n  assert email is not None\n\nassert _auth_domain\n\nif nickname is None:\n  assert 'USER_NICKNAME' in os.environ\n  nickname = os.environ['USER_NICKNAME']\n\nif email is None:\n  assert 'USER_EMAIL' in os.environ\n  email = os.environ['USER_EMAIL']\n\nif not email:\n  raise UserNotFoundError\n\nself.__nickname = nickname\nself.__email = email\nself.__auth_domain = _auth_domain", "path": "google\\appengine\\api\\users.py", "repo_name": "jchris/portable-google-app-engine-sdk", "stars": 23, "license": "other", "language": "python", "size": 2214}
{"docstring": "# Format:\n#   !!python/object/apply       # (or !!python/object/new)\n#   args: [ ... arguments ... ]\n#   kwds: { ... keywords ... }\n#   state: ... state ...\n#   listitems: [ ... listitems ... ]\n#   dictitems: { ... dictitems ... }\n# or short format:\n#   !!python/object/apply [ ... arguments ... ]\n# The difference between !!python/object/apply and !!python/object/new\n# is how an object is created, check make_python_instance for details.\n", "func_signal": "def construct_python_object_apply(self, suffix, node, newobj=False):\n", "code": "if isinstance(node, SequenceNode):\n    args = self.construct_sequence(node, deep=True)\n    kwds = {}\n    state = {}\n    listitems = []\n    dictitems = {}\nelse:\n    value = self.construct_mapping(node, deep=True)\n    args = value.get('args', [])\n    kwds = value.get('kwds', {})\n    state = value.get('state', {})\n    listitems = value.get('listitems', [])\n    dictitems = value.get('dictitems', {})\ninstance = self.make_python_instance(suffix, node, args, kwds, newobj)\nif state:\n    self.set_python_instance_state(instance, state)\nif listitems:\n    instance.extend(listitems)\nif dictitems:\n    for key in dictitems:\n        instance[key] = dictitems[key]\nreturn instance", "path": "lib\\yaml\\lib\\yaml\\constructor.py", "repo_name": "jchris/portable-google-app-engine-sdk", "stars": 23, "license": "other", "language": "python", "size": 2214}
{"docstring": "# Note: the same code as `construct_yaml_omap`.\n", "func_signal": "def construct_yaml_pairs(self, node):\n", "code": "pairs = []\nyield pairs\nif not isinstance(node, SequenceNode):\n    raise ConstructorError(\"while constructing pairs\", node.start_mark,\n            \"expected a sequence, but found %s\" % node.id, node.start_mark)\nfor subnode in node.value:\n    if not isinstance(subnode, MappingNode):\n        raise ConstructorError(\"while constructing pairs\", node.start_mark,\n                \"expected a mapping of length 1, but found %s\" % subnode.id,\n                subnode.start_mark)\n    if len(subnode.value) != 1:\n        raise ConstructorError(\"while constructing pairs\", node.start_mark,\n                \"expected a single mapping item, but found %d items\" % len(subnode.value),\n                subnode.start_mark)\n    key_node, value_node = subnode.value[0]\n    key = self.construct_object(key_node)\n    value = self.construct_object(value_node)\n    pairs.append((key, value))", "path": "lib\\yaml\\lib\\yaml\\constructor.py", "repo_name": "jchris/portable-google-app-engine-sdk", "stars": 23, "license": "other", "language": "python", "size": 2214}
{"docstring": "\"\"\"\nReturns context variables required by apps that use Django's authentication\nsystem.\n\"\"\"\n", "func_signal": "def auth(request):\n", "code": "return {\n    'user': request.user,\n    'messages': request.user.get_and_delete_messages(),\n    'perms': PermWrapper(request.user),\n}", "path": "lib\\django\\django\\core\\context_processors.py", "repo_name": "jchris/portable-google-app-engine-sdk", "stars": 23, "license": "other", "language": "python", "size": 2214}
{"docstring": "\"\"\"Return this user's nickname.\n\nThe nickname will be a unique, human readable identifier for this user\nwith respect to this application. It will be an email address for some\nusers, but not all.\n\"\"\"\n", "func_signal": "def nickname(self):\n", "code": "if self.__nickname:\n  return self.__nickname\nif (self.__email and self.__auth_domain and\n    self.__email.endswith('@' + self.__auth_domain)):\n  suffix_len = len(self.__auth_domain) + 1\n  return self.__email[:-suffix_len]\nelse:\n  return self.__email", "path": "google\\appengine\\api\\users.py", "repo_name": "jchris/portable-google-app-engine-sdk", "stars": 23, "license": "other", "language": "python", "size": 2214}
{"docstring": "\"\"\"\nReturns the 1-based index of the first object on the given page,\nrelative to total objects found (hits).\n\"\"\"\n", "func_signal": "def first_on_page(self, page_number):\n", "code": "page_number = self.validate_page_number(page_number)\nreturn (self.num_per_page * page_number) + 1", "path": "lib\\django\\django\\core\\paginator.py", "repo_name": "jchris/portable-google-app-engine-sdk", "stars": 23, "license": "other", "language": "python", "size": 2214}
{"docstring": "\"\"\"\nDisplays the template validator form, which finds and displays template\nsyntax errors.\n\"\"\"\n# get a dict of {site_id : settings_module} for the validator\n", "func_signal": "def template_validator(request):\n", "code": "settings_modules = {}\nfor mod in settings.ADMIN_FOR:\n    settings_module = __import__(mod, {}, {}, [''])\n    settings_modules[settings_module.SITE_ID] = settings_module\nmanipulator = TemplateValidator(settings_modules)\nnew_data, errors = {}, {}\nif request.POST:\n    new_data = request.POST.copy()\n    errors = manipulator.get_validation_errors(new_data)\n    if not errors:\n        request.user.message_set.create(message='The template is valid.')\nreturn render_to_response('admin/template_validator.html', {\n    'title': 'Template validator',\n    'form': oldforms.FormWrapper(manipulator, new_data, errors),\n}, context_instance=template.RequestContext(request))", "path": "lib\\django\\django\\contrib\\admin\\views\\template.py", "repo_name": "jchris/portable-google-app-engine-sdk", "stars": 23, "license": "other", "language": "python", "size": 2214}
{"docstring": "\"\"\"Close the socket underlying this connection.\"\"\"\n", "func_signal": "def close(self):\n", "code": "self.rfile.close()\n\nif not self.linger:\n    # Python's socket module does NOT call close on the kernel socket\n    # when you call socket.close(). We do so manually here because we\n    # want this server to send a FIN TCP segment immediately. Note this\n    # must be called *before* calling socket.close(), because the latter\n    # drops its reference to the kernel socket.\n    if hasattr(self.socket, '_sock'):\n        self.socket._sock.close()\n    self.socket.close()\nelse:\n    # On the other hand, sometimes we want to hang around for a bit\n    # to make sure the client has a chance to read our entire\n    # response. Skipping the close() calls here delays the FIN\n    # packet until the socket object is garbage-collected later.\n    # Someday, perhaps, we'll do the full lingering_close that\n    # Apache does, but not today.\n    pass", "path": "dutils\\wsgiserver.py", "repo_name": "amitu/dutils", "stars": 18, "license": "None", "language": "python", "size": 2276}
{"docstring": "\"\"\" Calls kvds, get dict and makes from the dict \"\"\"\n", "func_signal": "def get(cls, **kw):\n", "code": "o = cls.get_dict(**kw)\nm = dict_to_model(cls, o)\nm.is_saved = True\nreturn m", "path": "dutils\\kvds\\models.py", "repo_name": "amitu/dutils", "stars": 18, "license": "None", "language": "python", "size": 2276}
{"docstring": "\"\"\"WSGI callable to write unbuffered data to the client.\n\nThis method is also used internally by start_response (to write\ndata from the iterable returned by the WSGI application).\n\"\"\"\n", "func_signal": "def write(self, chunk):\n", "code": "if not self.started_response:\n    raise AssertionError(\"WSGI write called before start_response.\")\n\nif not self.req.sent_headers:\n    self.req.sent_headers = True\n    self.req.send_headers()\n\nself.req.write(chunk)", "path": "dutils\\wsgiserver.py", "repo_name": "amitu/dutils", "stars": 18, "license": "None", "language": "python", "size": 2276}
{"docstring": "\"\"\"Parse the next HTTP request start-line and message-headers.\"\"\"\n", "func_signal": "def parse_request(self):\n", "code": "self.rfile = SizeCheckWrapper(self.conn.rfile,\n                              self.server.max_request_header_size)\ntry:\n    self._parse_request()\nexcept MaxSizeExceeded:\n    self.simple_response(\"413 Request Entity Too Large\")\n    return", "path": "dutils\\wsgiserver.py", "repo_name": "amitu/dutils", "stars": 18, "license": "None", "language": "python", "size": 2276}
{"docstring": "\"\"\"Create (or recreate) the actual socket object.\"\"\"\n", "func_signal": "def bind(self, family, type, proto=0):\n", "code": "self.socket = socket.socket(family, type, proto)\nprevent_socket_inheritance(self.socket)\nself.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\nif self.nodelay and not isinstance(self.bind_addr, str):\n    self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n\nif self.ssl_adapter is not None:\n    self.socket = self.ssl_adapter.bind(self.socket)\n\n# If listening on the IPV6 any address ('::' = IN6ADDR_ANY),\n# activate dual-stack. See http://www.cherrypy.org/ticket/871.\nif (family == socket.AF_INET6\n    and self.bind_addr[0] in ('::', '::0', '::0.0.0.0')):\n    try:\n        self.socket.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 0)\n    except (AttributeError, socket.error):\n        # Apparently, the socket option is not available in\n        # this machine's TCP stack\n        pass\n\nself.socket.bind(self.bind_addr)", "path": "dutils\\wsgiserver.py", "repo_name": "amitu/dutils", "stars": 18, "license": "None", "language": "python", "size": 2276}
{"docstring": "\"\"\"Spawn new worker threads (not above self.max).\"\"\"\n", "func_signal": "def grow(self, amount):\n", "code": "for i in range(amount):\n    if self.max > 0 and len(self._threads) >= self.max:\n        break\n    worker = WorkerThread(self.server)\n    worker.setName(\"CP Server \" + worker.getName())\n    self._threads.append(worker)\n    worker.start()", "path": "dutils\\wsgiserver.py", "repo_name": "amitu/dutils", "stars": 18, "license": "None", "language": "python", "size": 2276}
{"docstring": "\"\"\"Like print_exc() but return a string. Backport for Python 2.3.\"\"\"\n", "func_signal": "def format_exc(limit=None):\n", "code": "try:\n    etype, value, tb = sys.exc_info()\n    return ''.join(traceback.format_exception(etype, value, tb, limit))\nfinally:\n    etype = value = tb = None", "path": "dutils\\wsgiserver.py", "repo_name": "amitu/dutils", "stars": 18, "license": "None", "language": "python", "size": 2276}
{"docstring": "\"\"\"Call the gateway and write its iterable output.\"\"\"\n", "func_signal": "def respond(self):\n", "code": "mrbs = self.server.max_request_body_size\nif self.chunked_read:\n    self.rfile = ChunkedRFile(self.conn.rfile, mrbs)\nelse:\n    cl = int(self.inheaders.get(\"Content-Length\", 0))\n    if mrbs and mrbs < cl:\n        if not self.sent_headers:\n            self.simple_response(\"413 Request Entity Too Large\")\n        return\n    self.rfile = KnownLengthRFile(self.conn.rfile, cl)\n\nself.server.gateway(self).respond()\n\nif (self.ready and not self.sent_headers):\n    self.sent_headers = True\n    self.send_headers()\nif self.chunked_write:\n    self.conn.wfile.sendall(\"0\\r\\n\\r\\n\")", "path": "dutils\\wsgiserver.py", "repo_name": "amitu/dutils", "stars": 18, "license": "None", "language": "python", "size": 2276}
{"docstring": "# create new class\n", "func_signal": "def __new__(cls, name,bases,attrs):\n", "code": "super_new = super(ModelBase, cls).__new__\nmodule = attrs.pop('__module__')\nnew_class = super_new(cls, name, bases, {'__module__': module})\n\n# adding most needed attributes to save, meta info about instance\nattrs['id'] = Field(primary_index=True)\nattrs['modelname'] = Field(initval=name)\nattrs['key_prefix'] = Field(initval=attrs.get('key_prefix',name))\n\n# _meta, a dict which saves all fields of one type into array, \n# with the field meta_name as the key and the array as value\nnew_class.add_to_class('_meta', {})\n# field_prefix, a dict which saves fields prefix as keys and\n# the instance as value\nnew_class.add_to_class('field_prefix', {})\nfor obj_name, obj in attrs.items():\n    if isinstance(obj, Field):\n        obj.name = obj_name\n        if not new_class._meta.get(obj.meta_name):\n            new_class._meta[obj.meta_name] = {}\n        new_class._meta[obj.meta_name].update({obj.name:obj})\n        #print obj.meta_name, obj.name, obj.field_key , obj\n        new_class.field_prefix.update({obj.field_key:obj})\n        new_class.add_to_class(obj_name, obj)\n    else:\n        new_class.add_to_class(obj_name, obj)\n    \nreturn new_class", "path": "dutils\\kvds\\models.py", "repo_name": "amitu/dutils", "stars": 18, "license": "None", "language": "python", "size": 2276}
{"docstring": "#print \"Field getting obj:\", obj, \"objtype:\", objtype\n", "func_signal": "def __get__(self, obj, objtype):\n", "code": "fo = getattr(obj, self.meta_name) \nreturn fo[self.name].val", "path": "dutils\\kvds\\models.py", "repo_name": "amitu/dutils", "stars": 18, "license": "None", "language": "python", "size": 2276}
{"docstring": "\"\"\" Saves the model in datastore serialized as json \"\"\"\n", "func_signal": "def save(self):\n", "code": "for fattrs in self._meta.keys():\n    mfields = getattr(self,fattrs)\n    klass = mfields.values()[0].__class__\n    klass.pre_save(self)\ndata = self.__data__()\nprimary_index_keys = []\nfor k,f in self.fields.items():\n    if f.primary_index:\n        v = getattr(self, k)\n        key = construct_key(self.key_prefix, k, v)\n        primary_index_keys.append(key)\n        #print key, \"=>\", simplejson.dumps(data)\n        dutils.kvds.utils.kvds(key=key, value=simplejson.dumps(data))\nfor k,f in self.fields.items():\n    if f.index:\n        v = getattr(self, k)\n        index_key = construct_key(self.key_prefix, k, v)\n        for pik in primary_index_keys:\n            #print index_key, \"=>\", pik\n            dutils.kvds.utils.kvds(key=index_key, value=pik)\nself.is_saved = True", "path": "dutils\\kvds\\models.py", "repo_name": "amitu/dutils", "stars": 18, "license": "None", "language": "python", "size": 2276}
{"docstring": "\"\"\"Assert, process, and send the HTTP response message-headers.\n\nYou must set self.status, and self.outheaders before calling this.\n\"\"\"\n", "func_signal": "def send_headers(self):\n", "code": "hkeys = [key.lower() for key, value in self.outheaders]\nstatus = int(self.status[:3])\n\nif status == 413:\n    # Request Entity Too Large. Close conn to avoid garbage.\n    self.close_connection = True\nelif \"content-length\" not in hkeys:\n    # \"All 1xx (informational), 204 (no content),\n    # and 304 (not modified) responses MUST NOT\n    # include a message-body.\" So no point chunking.\n    if status < 200 or status in (204, 205, 304):\n        pass\n    else:\n        if (self.response_protocol == 'HTTP/1.1'\n            and self.method != 'HEAD'):\n            # Use the chunked transfer-coding\n            self.chunked_write = True\n            self.outheaders.append((\"Transfer-Encoding\", \"chunked\"))\n        else:\n            # Closing the conn is the only way to determine len.\n            self.close_connection = True\n\nif \"connection\" not in hkeys:\n    if self.response_protocol == 'HTTP/1.1':\n        # Both server and client are HTTP/1.1 or better\n        if self.close_connection:\n            self.outheaders.append((\"Connection\", \"close\"))\n    else:\n        # Server and/or client are HTTP/1.0\n        if not self.close_connection:\n            self.outheaders.append((\"Connection\", \"Keep-Alive\"))\n\nif (not self.close_connection) and (not self.chunked_read):\n    # Read any remaining request body data on the socket.\n    # \"If an origin server receives a request that does not include an\n    # Expect request-header field with the \"100-continue\" expectation,\n    # the request includes a request body, and the server responds\n    # with a final status code before reading the entire request body\n    # from the transport connection, then the server SHOULD NOT close\n    # the transport connection until it has read the entire request,\n    # or until the client closes the connection. Otherwise, the client\n    # might not reliably receive the response message. However, this\n    # requirement is not be construed as preventing a server from\n    # defending itself against denial-of-service attacks, or from\n    # badly broken client implementations.\"\n    remaining = getattr(self.rfile, 'remaining', 0)\n    if remaining > 0:\n        self.rfile.read(remaining)\n\nif \"date\" not in hkeys:\n    self.outheaders.append((\"Date\", rfc822.formatdate()))\n\nif \"server\" not in hkeys:\n    self.outheaders.append((\"Server\", self.server.server_name))\n\nbuf = [self.server.protocol + \" \" + self.status + CRLF]\nfor k, v in self.outheaders:\n    buf.append(k + \": \" + v + CRLF)\nbuf.append(CRLF)\nself.conn.wfile.sendall(\"\".join(buf))", "path": "dutils\\wsgiserver.py", "repo_name": "amitu/dutils", "stars": 18, "license": "None", "language": "python", "size": 2276}
{"docstring": "\"\"\"Read headers from the given stream into the given header dict.\n\nIf hdict is None, a new header dict is created. Returns the populated\nheader dict.\n\nHeaders which are repeated are folded together using a comma if their\nspecification so dictates.\n\nThis function raises ValueError when the read bytes violate the HTTP spec.\nYou should probably return \"400 Bad Request\" if this happens.\n\"\"\"\n", "func_signal": "def read_headers(rfile, hdict=None):\n", "code": "if hdict is None:\n    hdict = {}\n\nwhile True:\n    line = rfile.readline()\n    if not line:\n        # No more data--illegal end of headers\n        raise ValueError(\"Illegal end of headers.\")\n    \n    if line == CRLF:\n        # Normal end of headers\n        break\n    if not line.endswith(CRLF):\n        raise ValueError(\"HTTP requires CRLF terminators\")\n    \n    if line[0] in ' \\t':\n        # It's a continuation line.\n        v = line.strip()\n    else:\n        try:\n            k, v = line.split(\":\", 1)\n        except ValueError:\n            raise ValueError(\"Illegal header line.\")\n        # TODO: what about TE and WWW-Authenticate?\n        k = k.strip().title()\n        v = v.strip()\n        hname = k\n    \n    if k in comma_separated_headers:\n        existing = hdict.get(hname)\n        if existing:\n            v = \", \".join((existing, v))\n    hdict[hname] = v\n\nreturn hdict", "path": "dutils\\wsgiserver.py", "repo_name": "amitu/dutils", "stars": 18, "license": "None", "language": "python", "size": 2276}
{"docstring": "\"\"\"Write unbuffered data to the client.\"\"\"\n", "func_signal": "def write(self, chunk):\n", "code": "if self.chunked_write and chunk:\n    buf = [hex(len(chunk))[2:], CRLF, chunk, CRLF]\n    self.conn.wfile.sendall(\"\".join(buf))\nelse:\n    self.conn.wfile.sendall(chunk)", "path": "dutils\\wsgiserver.py", "repo_name": "amitu/dutils", "stars": 18, "license": "None", "language": "python", "size": 2276}
{"docstring": "\"\"\"Start the pool of threads.\"\"\"\n", "func_signal": "def start(self):\n", "code": "for i in range(self.min):\n    self._threads.append(WorkerThread(self.server))\nfor worker in self._threads:\n    worker.setName(\"CP Server \" + worker.getName())\n    worker.start()\nfor worker in self._threads:\n    while not worker.ready:\n        time.sleep(.1)", "path": "dutils\\wsgiserver.py", "repo_name": "amitu/dutils", "stars": 18, "license": "None", "language": "python", "size": 2276}
{"docstring": "# Use max, disallow tiny reads in a loop as they are very inefficient.\n# We never leave read() with any leftover data from a new recv() call\n# in our internal buffer.\n", "func_signal": "def read(self, size=-1):\n", "code": "rbufsize = max(self._rbufsize, self.default_bufsize)\n# Our use of StringIO rather than lists of string objects returned by\n# recv() minimizes memory usage and fragmentation that occurs when\n# rbufsize is large compared to the typical return value of recv().\nbuf = self._rbuf\nbuf.seek(0, 2)  # seek end\nif size < 0:\n    # Read until EOF\n    self._rbuf = StringIO.StringIO()  # reset _rbuf.  we consume it via buf.\n    while True:\n        data = self.recv(rbufsize)\n        if not data:\n            break\n        buf.write(data)\n    return buf.getvalue()\nelse:\n    # Read until size bytes or EOF seen, whichever comes first\n    buf_len = buf.tell()\n    if buf_len >= size:\n        # Already have size bytes in our buffer?  Extract and return.\n        buf.seek(0)\n        rv = buf.read(size)\n        self._rbuf = StringIO.StringIO()\n        self._rbuf.write(buf.read())\n        return rv\n\n    self._rbuf = StringIO.StringIO()  # reset _rbuf.  we consume it via buf.\n    while True:\n        left = size - buf_len\n        # recv() will malloc the amount of memory given as its\n        # parameter even though it often returns much less data\n        # than that.  The returned data string is short lived\n        # as we copy it into a StringIO and free it.  This avoids\n        # fragmentation issues on many platforms.\n        data = self.recv(left)\n        if not data:\n            break\n        n = len(data)\n        if n == size and not buf_len:\n            # Shortcut.  Avoid buffer data copies when:\n            # - We have no data in our buffer.\n            # AND\n            # - Our call to recv returned exactly the\n            #   number of bytes we were asked to read.\n            return data\n        if n == left:\n            buf.write(data)\n            del data  # explicit free\n            break\n        assert n <= left, \"recv(%d) returned %d bytes\" % (left, n)\n        buf.write(data)\n        buf_len += n\n        del data  # explicit free\n        #assert buf_len == buf.tell()\n    return buf.getvalue()", "path": "dutils\\wsgiserver.py", "repo_name": "amitu/dutils", "stars": 18, "license": "None", "language": "python", "size": 2276}
{"docstring": "\"\"\"Return a new environ dict targeting the given wsgi.version\"\"\"\n", "func_signal": "def get_environ(self):\n", "code": "req = self.req\nenv = {\n    # set a non-standard environ entry so the WSGI app can know what\n    # the *real* server protocol is (and what features to support).\n    # See http://www.faqs.org/rfcs/rfc2145.html.\n    'ACTUAL_SERVER_PROTOCOL': req.server.protocol,\n    'PATH_INFO': req.path,\n    'QUERY_STRING': req.qs,\n    'REMOTE_ADDR': req.conn.remote_addr or '',\n    'REMOTE_PORT': str(req.conn.remote_port or ''),\n    'REQUEST_METHOD': req.method,\n    'REQUEST_URI': req.uri,\n    'SCRIPT_NAME': '',\n    'SERVER_NAME': req.server.server_name,\n    # Bah. \"SERVER_PROTOCOL\" is actually the REQUEST protocol.\n    'SERVER_PROTOCOL': req.request_protocol,\n    'SERVER_SOFTWARE': \"%s WSGI Server\" % req.server.version,\n    'wsgi.errors': sys.stderr,\n    'wsgi.input': req.rfile,\n    'wsgi.multiprocess': False,\n    'wsgi.multithread': True,\n    'wsgi.run_once': False,\n    'wsgi.url_scheme': req.scheme,\n    'wsgi.version': (1, 0),\n    }\n\nif isinstance(req.server.bind_addr, basestring):\n    # AF_UNIX. This isn't really allowed by WSGI, which doesn't\n    # address unix domain sockets. But it's better than nothing.\n    env[\"SERVER_PORT\"] = \"\"\nelse:\n    env[\"SERVER_PORT\"] = str(req.server.bind_addr[1])\n\n# CONTENT_TYPE/CONTENT_LENGTH\nfor k, v in req.inheaders.iteritems():\n    env[\"HTTP_\" + k.upper().replace(\"-\", \"_\")] = v\nct = env.pop(\"HTTP_CONTENT_TYPE\", None)\nif ct is not None:\n    env[\"CONTENT_TYPE\"] = ct\ncl = env.pop(\"HTTP_CONTENT_LENGTH\", None)\nif cl is not None:\n    env[\"CONTENT_LENGTH\"] = cl\n\nif req.conn.ssl_env:\n    env.update(req.conn.ssl_env)\n\nreturn env", "path": "dutils\\wsgiserver.py", "repo_name": "amitu/dutils", "stars": 18, "license": "None", "language": "python", "size": 2276}
{"docstring": "\"\"\"Return a new environ dict targeting the given wsgi.version\"\"\"\n", "func_signal": "def get_environ(self):\n", "code": "req = self.req\nenv_10 = WSGIGateway_10.get_environ(self)\nenv = dict([(k.decode('ISO-8859-1'), v) for k, v in env_10.iteritems()])\nenv[u'wsgi.version'] = ('u', 0)\n\n# Request-URI\nenv.setdefault(u'wsgi.url_encoding', u'utf-8')\ntry:\n    for key in [u\"PATH_INFO\", u\"SCRIPT_NAME\", u\"QUERY_STRING\"]:\n        env[key] = env_10[str(key)].decode(env[u'wsgi.url_encoding'])\nexcept UnicodeDecodeError:\n    # Fall back to latin 1 so apps can transcode if needed.\n    env[u'wsgi.url_encoding'] = u'ISO-8859-1'\n    for key in [u\"PATH_INFO\", u\"SCRIPT_NAME\", u\"QUERY_STRING\"]:\n        env[key] = env_10[str(key)].decode(env[u'wsgi.url_encoding'])\n\nfor k, v in sorted(env.items()):\n    if isinstance(v, str) and k not in ('REQUEST_URI', 'wsgi.input'):\n        env[k] = v.decode('ISO-8859-1')\n\nreturn env", "path": "dutils\\wsgiserver.py", "repo_name": "amitu/dutils", "stars": 18, "license": "None", "language": "python", "size": 2276}
{"docstring": "# Shamelessly stolen from StringIO\n", "func_signal": "def __iter__(self):\n", "code": "total = 0\nline = self.readline(sizehint)\nwhile line:\n    yield line\n    total += len(line)\n    if 0 < sizehint <= total:\n        break\n    line = self.readline(sizehint)", "path": "dutils\\wsgiserver.py", "repo_name": "amitu/dutils", "stars": 18, "license": "None", "language": "python", "size": 2276}
{"docstring": "\"\"\"Mark the given socket fd as non-inheritable (Windows).\"\"\"\n", "func_signal": "def prevent_socket_inheritance(sock):\n", "code": "if not windll.kernel32.SetHandleInformation(sock.fileno(), 1, 0):\n    raise WinError()", "path": "dutils\\wsgiserver.py", "repo_name": "amitu/dutils", "stars": 18, "license": "None", "language": "python", "size": 2276}
{"docstring": "''' Read an indirect object.  If it has already\n    been read, return it from the cache.\n'''\n\n", "func_signal": "def readindirect(self, objnum, gennum):\n", "code": "def setobj(obj):\n    # Store the new object in the dictionary\n    # once we have its value\n    record[1] = obj\n\ndef ordinary(source, setobj, obj):\n    # Deal with an ordinary (non-array, non-dict) object\n    setobj(obj)\n    return obj\n\nfdata, objnum, gennum = self.fdata, int(objnum), int(gennum)\nrecord = self.indirect_objects[fdata, objnum, gennum]\nif record[1] is not self.unresolved:\n    return record[1]\n\n# Read the object header and validate it\nsource = PdfTokens(fdata, record[0])\nobjid = source.multiple(3)\nassert int(objid[0]) == objnum, objid\nassert int(objid[1]) == gennum, objid\nassert objid[2] == 'obj', objid\n\n# Read the object, and call special code if it starts\n# an array or dictionary\nobj = source.next()\nobj = self.special.get(obj, ordinary)(source, setobj, obj)\nself.readstream(obj, source)\nobj.indirect = True\nreturn obj", "path": "mat\\pdfrw\\pdfreader.py", "repo_name": "4ZM/Metadata-Anonymisation-Toolkit", "stars": 26, "license": "gpl-2.0", "language": "python", "size": 636}
{"docstring": "'''\n    Remove the \"metadata\" block from the file\n'''\n", "func_signal": "def remove_all(self):\n", "code": "if self.backup is True:\n    shutil.copy2(self.filename, self.output)\n    self.filename = self.output\n\nmfile = FLAC(self.filename)\nmfile.delete()\nmfile.clear_pictures()\nmfile.save()", "path": "mat\\audio.py", "repo_name": "4ZM/Metadata-Anonymisation-Toolkit", "stars": 26, "license": "gpl-2.0", "language": "python", "size": 636}
{"docstring": "''' Read optional stream following a dictionary\n    object.\n'''\n", "func_signal": "def readstream(obj, source):\n", "code": "tok = source.next()\nif tok == 'endobj':\n    return  # No stream\n\nassert isinstance(obj, PdfDict)\nassert tok == 'stream', tok\nfdata = source.fdata\nfloc = fdata.rindex(tok, 0, source.floc) + len(tok)\nch = fdata[floc]\nif ch == '\\r':\n    floc += 1\n    ch = fdata[floc]\nassert ch == '\\n'\nstartstream = floc + 1\nendstream = startstream + int(obj.Length)\nobj._stream = fdata[startstream:endstream]\nsource = PdfTokens(fdata, endstream)\nendit = source.multiple(2)\nif endit != 'endstream endobj'.split():\n    # /Length attribute is broken, try to read stream\n    # anyway disregarding the specified value\n    # TODO: issue warning here once we have some kind of\n    # logging\n    endstream = fdata.index('endstream', startstream)\n    if fdata[endstream-2:endstream] == '\\r\\n':\n        endstream -= 2\n    elif fdata[endstream-1] in ['\\n', '\\r']:\n        endstream -= 1\n    source = PdfTokens(fdata, endstream)\n    endit = source.multiple(2)\n    assert endit == 'endstream endobj'.split()\n    obj.Length = str(endstream-startstream)\n    obj._stream = fdata[startstream:endstream]", "path": "mat\\pdfrw\\pdfreader.py", "repo_name": "4ZM/Metadata-Anonymisation-Toolkit", "stars": 26, "license": "gpl-2.0", "language": "python", "size": 636}
{"docstring": "'''\n    Return a dict with all the meta of the file\n'''\n", "func_signal": "def get_meta(self):\n", "code": "zipin = zipfile.ZipFile(self.filename, 'r')\nmetadata = {}\nfor item in zipin.namelist():\n    if item.startswith('docProps/'):\n        metadata[item] = 'harmful content'\nzipin.close()\nreturn metadata", "path": "mat\\office.py", "repo_name": "4ZM/Metadata-Anonymisation-Toolkit", "stars": 26, "license": "gpl-2.0", "language": "python", "size": 636}
{"docstring": "'''\n    Check if the file is clean from harmful metadatas\n'''\n", "func_signal": "def is_clean(self):\n", "code": "zipin = zipfile.ZipFile(self.filename, 'r')\nfor item in zipin.namelist():\n    if item.startswith('docProps/'):\n        return False\nzipin.close()\nczf = archive.ZipStripper(self.filename, self.parser,\n        'application/zip', self.backup, self.add2archive)\nif not czf.is_clean():\n    return False\nelse:\n    return True", "path": "mat\\office.py", "repo_name": "4ZM/Metadata-Anonymisation-Toolkit", "stars": 26, "license": "gpl-2.0", "language": "python", "size": 636}
{"docstring": "'''\n    Check if the file is clean from harmful metadatas\n'''\n", "func_signal": "def is_clean(self):\n", "code": "for key in self.meta_list:\n    if key == 'creation-date' or key == 'mod-date':\n        if self.document.get_property(key) != -1:\n            return False\n    elif self.document.get_property(key) is not None and \\\n        self.document.get_property(key) != '':\n        return False\nreturn True", "path": "mat\\office.py", "repo_name": "4ZM/Metadata-Anonymisation-Toolkit", "stars": 26, "license": "gpl-2.0", "language": "python", "size": 636}
{"docstring": "'''\n    Check if the file is clean from harmful metadatas\n'''\n", "func_signal": "def is_clean(self):\n", "code": "zipin = zipfile.ZipFile(self.filename, 'r')\ntry:\n    zipin.getinfo('meta.xml')\nexcept KeyError:  # no meta.xml in the file\n    czf = archive.ZipStripper(self.filename, self.parser,\n        'application/zip', self.backup, self.add2archive)\n    if czf.is_clean():\n        zipin.close()\n        return True\nzipin.close()\nreturn False", "path": "mat\\office.py", "repo_name": "4ZM/Metadata-Anonymisation-Toolkit", "stars": 26, "license": "gpl-2.0", "language": "python", "size": 636}
{"docstring": "'''\n    Check if the \"metadata\" block is present in the file\n'''\n", "func_signal": "def is_clean(self):\n", "code": "mfile = FLAC(self.filename)\nif mfile.tags is None and mfile.pictures == []:\n    return True\nelse:\n    return False", "path": "mat\\audio.py", "repo_name": "4ZM/Metadata-Anonymisation-Toolkit", "stars": 26, "license": "gpl-2.0", "language": "python", "size": 636}
{"docstring": "'''\n    Return the content of the metadata block if present\n'''\n", "func_signal": "def get_meta(self):\n", "code": "metadata = {}\nmfile = FLAC(self.filename)\nif mfile.tags is not None:\n    if mfile.pictures != []:\n        metadata['picture :'] = 'yes'\n    for key, value in mfile.tags:\n        metadata[key] = value\nreturn metadata", "path": "mat\\audio.py", "repo_name": "4ZM/Metadata-Anonymisation-Toolkit", "stars": 26, "license": "gpl-2.0", "language": "python", "size": 636}
{"docstring": "'''\n    Called when entering into xml balise\n'''\n", "func_signal": "def startElement(self, name, attrs):\n", "code": "self.between = True\nself.key = name\nself.content = ''", "path": "mat\\mat.py", "repo_name": "4ZM/Metadata-Anonymisation-Toolkit", "stars": 26, "license": "gpl-2.0", "language": "python", "size": 636}
{"docstring": "'''\n    Return a dict with all the meta of the file\n'''\n", "func_signal": "def get_meta(self):\n", "code": "metadata = {}\nfor key in self.meta_list:\n    if key == 'creation-date' or key == 'mod-date':\n        #creation and modification are set to -1\n        if self.document.get_property(key) != -1:\n            metadata[key] = self.document.get_property(key)\n    elif self.document.get_property(key) is not None and \\\n        self.document.get_property(key) != '':\n        metadata[key] = self.document.get_property(key)\nreturn metadata", "path": "mat\\office.py", "repo_name": "4ZM/Metadata-Anonymisation-Toolkit", "stars": 26, "license": "gpl-2.0", "language": "python", "size": 636}
{"docstring": "'''\n    Opening the pdf with poppler, then doing a render\n    on a cairo pdfsurface for each pages.\n    Thanks to Lunar^for the idea.\n    http://cairographics.org/documentation/pycairo/2/\n    python-poppler is not documented at all : have fun ;)\n'''\n", "func_signal": "def remove_all(self):\n", "code": "page = self.document.get_page(0)\npage_width, page_height = page.get_size()\nsurface = cairo.PDFSurface(self.output, page_width, page_height)\ncontext = cairo.Context(surface)  # context draws on the surface\nlogging.debug('Pdf rendering of %s' % self.filename)\nfor pagenum in xrange(self.document.get_n_pages()):\n    page = self.document.get_page(pagenum)\n    context.translate(0, 0)\n    page.render(context)  # render the page on context\n    context.show_page()  # draw context on surface\nsurface.finish()\n\n#For now, poppler cannot write meta, so we must use pdfrw\nlogging.debug('Removing %s\\'s superficial metadata' % self.filename)\ntrailer = pdfrw.PdfReader(self.output)\ntrailer.Info.Producer = trailer.Info.Creator = None\nwriter = pdfrw.PdfWriter()\nwriter.trailer = trailer\nwriter.write(self.output)\nself.do_backup()", "path": "mat\\office.py", "repo_name": "4ZM/Metadata-Anonymisation-Toolkit", "stars": 26, "license": "gpl-2.0", "language": "python", "size": 636}
{"docstring": "'''\n    Return a dict with all the meta of the file\n'''\n", "func_signal": "def get_meta(self):\n", "code": "metadata = {}\nwith open(self.filename, 'r') as f:\n    decoded = bencode.bdecode(f.read())\nfor key in self.fields:\n    try:\n        if decoded[key] != '':\n            metadata[key] = decoded[key]\n    except:\n        pass\nreturn metadata", "path": "mat\\misc.py", "repo_name": "4ZM/Metadata-Anonymisation-Toolkit", "stars": 26, "license": "gpl-2.0", "language": "python", "size": 636}
{"docstring": "'''\n    Return the content of the metadata block if present\n'''\n", "func_signal": "def get_meta(self):\n", "code": "metadata = {}\nmfile = OggVorbis(self.filename)\nfor key, value in mfile.tags:\n    metadata[key] = value\nreturn metadata", "path": "mat\\audio.py", "repo_name": "4ZM/Metadata-Anonymisation-Toolkit", "stars": 26, "license": "gpl-2.0", "language": "python", "size": 636}
{"docstring": "'''\n    Return a dict with all the meta of the file by\n    trying to read the meta.xml file.\n'''\n", "func_signal": "def get_meta(self):\n", "code": "zipin = zipfile.ZipFile(self.filename, 'r')\nmetadata = {}\ntry:\n    content = zipin.read('meta.xml')\n    zipin.close()\n    metadata[self.filename] = 'harmful meta'\nexcept KeyError:  # no meta.xml file found\n    logging.debug('%s has no opendocument metadata' % self.filename)\nreturn metadata", "path": "mat\\office.py", "repo_name": "4ZM/Metadata-Anonymisation-Toolkit", "stars": 26, "license": "gpl-2.0", "language": "python", "size": 636}
{"docstring": "'''\n    return a $FILETYPEStripper() class,\n    corresponding to the filetype of the given file\n'''\n", "func_signal": "def create_class_file(name, backup, add2archive):\n", "code": "if not os.path.isfile(name):\n    # check if the file exists\n    logging.error('%s is not a valid file' % name)\n    return\n\nfilename = ''\ntry:\n    filename = hachoir_core.cmd_line.unicodeFilename(name)\nexcept TypeError:  # get rid of \"decoding Unicode is not supported\"\n    filename = name\n\nparser = hachoir_parser.createParser(filename)\nif not parser:\n    logging.info('Unable to parse %s' % filename)\n    return\n\nmime = parser.mime_type\n\nif mime == 'application/zip':  # some formats are zipped stuff\n    mime = mimetypes.guess_type(name)[0]\n\nif mime.startswith('application/vnd.oasis.opendocument'):\n    mime = 'application/opendocument'  # opendocument fileformat\nelif mime.startswith('application/vnd.openxmlformats-officedocument'):\n    mime = 'application/officeopenxml'  # office openxml\n\ntry:\n    stripper_class = STRIPPERS[mime]\nexcept KeyError:\n    logging.info('Don\\'t have stripper for %s format' % mime)\n    return\n\nreturn stripper_class(filename, parser, mime, backup, add2archive)", "path": "mat\\mat.py", "repo_name": "4ZM/Metadata-Anonymisation-Toolkit", "stars": 26, "license": "gpl-2.0", "language": "python", "size": 636}
{"docstring": "'''\n    Remove all the files that are compromizing\n'''\n", "func_signal": "def remove_all(self):\n", "code": "with open(self.filename, 'r') as f:\n    decoded = bencode.bdecode(f.read())\nfor key in self.fields:\n    try:\n        decoded[key] = ''\n    except:\n        pass\nwith open(self.output, 'w') as f:  # encode the decoded torrent\n    f.write(bencode.bencode(decoded))  # and write it in self.output\nself.do_backup()", "path": "mat\\misc.py", "repo_name": "4ZM/Metadata-Anonymisation-Toolkit", "stars": 26, "license": "gpl-2.0", "language": "python", "size": 636}
{"docstring": "# PDFs can have arbitrarily nested Pages/Page\n# dictionary structures.\n", "func_signal": "def readpages(self, node):\n", "code": "if node.Type == self.pagename:\n    return [node]\nassert node.Type == self.pagesname, node.Type\nresult = []\nfor node in node.Kids:\n    result.extend(self.readpages(node))\nreturn result", "path": "mat\\pdfrw\\pdfreader.py", "repo_name": "4ZM/Metadata-Anonymisation-Toolkit", "stars": 26, "license": "gpl-2.0", "language": "python", "size": 636}
{"docstring": "'''\n    Check if the file is clean from harmful metadatas\n'''\n", "func_signal": "def is_clean(self):\n", "code": "with open(self.filename, 'r') as f:\n    decoded = bencode.bdecode(f.read())\nfor key in self.fields:\n    try:\n        if decoded[key] != '':\n            return False\n    except:\n        pass\nreturn True", "path": "mat\\misc.py", "repo_name": "4ZM/Metadata-Anonymisation-Toolkit", "stars": 26, "license": "gpl-2.0", "language": "python", "size": 636}
{"docstring": "'''\n    FIXME ?\n    There is a patch implementing the Zipfile.remove()\n    method here : http://bugs.python.org/issue6818\n'''\n", "func_signal": "def _remove_all(self, method):\n", "code": "zipin = zipfile.ZipFile(self.filename, 'r')\nzipout = zipfile.ZipFile(self.output, 'w',\n    allowZip64=True)\nfor item in zipin.namelist():\n    name = os.path.join(self.tempdir, item)\n    _, ext = os.path.splitext(name)\n    if item.startswith('docProps/'):  # metadatas\n        pass\n    elif ext in parser.NOMETA or item == '.rels':\n        #keep parser.NOMETA files, and the file named \".rels\"\n        zipin.extract(item, self.tempdir)\n        zipout.write(name, item)\n    else:\n        zipin.extract(item, self.tempdir)\n        if os.path.isfile(name):  # don't care about folders\n            try:\n                cfile = mat.create_class_file(name, False,\n                    self.add2archive)\n                if method == 'normal':\n                    cfile.remove_all()\n                else:\n                    cfile.remove_all_ugly()\n                logging.debug('Processing %s from %s' % (item,\n                    self.filename))\n                zipout.write(name, item)\n            except:\n                logging.info('%s\\' fileformat is not supported' % item)\n                if self.add2archive:\n                    zipout.write(name, item)\nzipout.comment = ''\nlogging.info('%s treated' % self.filename)\nzipin.close()\nzipout.close()\nself.do_backup()", "path": "mat\\office.py", "repo_name": "4ZM/Metadata-Anonymisation-Toolkit", "stars": 26, "license": "gpl-2.0", "language": "python", "size": 636}
{"docstring": "'''\u53d1\u9001\u7fa4\u901a\u544a\u3002\u53ea\u4f1a\u53d1\u7ed9\u5728\u7ebf\u7684\u4eba\uff0c\u5305\u62ec snoozing \u8005\u3002'''\n", "func_signal": "def do_notice(self, arg):\n", "code": "if not arg:\n  self.msg.reply('\u8bf7\u7ed9\u51fa\u7fa4\u901a\u544a\u7684\u5185\u5bb9\u3002')\n  return\n\nmsg = self.msg.body[len(self.sender.prefix):].split(None, 1)[-1]\n\nl = User.gql('where avail != :1', OFFLINE)\nlog_admin(self.sender, NOTICE % msg)\nfor u in l:\n  try:\n    xmpp.send_message(u.jid, u'\u901a\u544a\uff1a' + msg)\n  except xmpp.InvalidJidError:\n    pass", "path": "gaetalk.py", "repo_name": "lilydjwg/gaetalk", "stars": 22, "license": "None", "language": "python", "size": 341}
{"docstring": "'''\u663e\u793a\u672c\u5e2e\u52a9\u3002\u53c2\u6570 long \u663e\u793a\u8be6\u7ec6\u5e2e\u52a9\uff0c\u4e5f\u53ef\u6307\u5b9a\u547d\u4ee4\u540d\u3002'''\n", "func_signal": "def do_help(self, args=()):\n", "code": "doc = []\nprefix = self.sender.prefix\n\nif len(args) > 1:\n  self.msg.reply('\u53c2\u6570\u9519\u8bef\u3002')\n  return\narg = args[0] if args else None\n\nif arg is None or arg == 'long':\n  for b in self.__class__.__bases__ + (self.__class__,):\n    for c, f in b.__dict__.items():\n      if c.startswith('do_'):\n        if arg is None:\n          doc.append(u'%s%s:\\t%s' % (prefix, c[3:], f.__doc__.decode('utf-8').\\\n                                     split(u'\uff0c', 1)[0].split(u'\u3002', 1)[0]))\n        else:\n          doc.append(u'%s%s:\\t%s' % (prefix, c[3:], f.__doc__.decode('utf-8')))\n  doc.sort()\n  if arg is None:\n    doc.insert(0, u'** \u547d\u4ee4\u6307\u5357 **\\n(\u5f53\u524d\u547d\u4ee4\u524d\u7f00 %s\uff0c\u53ef\u8bbe\u7f6e\u3002\u4f7f\u7528 %shelp long \u663e\u793a\u8be6\u7ec6\u5e2e\u52a9)' % (prefix, prefix))\n  else:\n    doc.insert(0, u'** \u547d\u4ee4\u6307\u5357 **\\n(\u5f53\u524d\u547d\u4ee4\u524d\u7f00 %s\uff0c\u53ef\u8bbe\u7f6e)' % prefix)\n  doc.append(u'\u8981\u79bb\u5f00\uff0c\u76f4\u63a5\u5220\u6389\u597d\u53cb\u5373\u53ef\u3002')\n  doc.append(u'Gtalk \u5ba2\u6237\u7aef\u7528\u6237\u8981\u79bb\u5f00\u8bf7\u4f7f\u7528 quit \u547d\u4ee4\u3002')\n  self.msg.reply(u'\\n'.join(doc).encode('utf-8'))\nelse:\n  try:\n    handle = getattr(self, 'do_' + arg)\n  except AttributeError:\n    self.msg.reply(u'\u9519\u8bef\uff1a\u672a\u77e5\u547d\u4ee4 %s' % arg)\n  except UnicodeEncodeError:\n    self.msg.reply(u'\u9519\u8bef\uff1a\u547d\u4ee4\u540d\u89e3\u7801\u5931\u8d25\u3002\u6b64\u95ee\u9898\u5728 GAE \u5347\u7ea7\u5176 Python \u5230 3.x \u540e\u65b9\u80fd\u89e3\u51b3\u3002')\n  else:\n    self.msg.reply(u'%s%s:\\t%s' % (prefix, arg, handle.__doc__.decode('utf-8')))", "path": "gaetalk.py", "repo_name": "lilydjwg/gaetalk", "stars": 22, "license": "None", "language": "python", "size": 341}
{"docstring": "'''\u8bbe\u7f6e\u53c2\u6570\u3002\u53c2\u6570\u683c\u5f0f key=value\uff1b\u4e0d\u5e26\u53c2\u6570\u4ee5\u67e5\u770b\u8bf4\u660e\u3002'''\n#\u6ce8\u610f\uff1a\u9009\u9879\u540d/\u503c\u4e2d\u4e0d\u80fd\u5305\u542b\u7a7a\u683c\n", "func_signal": "def do_set(self, args):\n", "code": "if len(args) != 1:\n  doc = []\n  for c, f in self.__class__.__dict__.items():\n    if c.startswith('set_'):\n      doc.append(u'* %s:\\t%s' % (c[4:], f.__doc__.decode('utf-8')))\n  for b in self.__class__.__bases__:\n    for c, f in b.__dict__.items():\n      if c.startswith('set_'):\n        doc.append(u'* %s:\\t%s' % (c[4:], f.__doc__.decode('utf-8')))\n  doc.sort()\n  doc.insert(0, u'\u8bbe\u7f6e\u9009\u9879\uff1a')\n  self.msg.reply(u'\\n'.join(doc).encode('utf-8'))\nelse:\n  msg = self.msg\n  cmd = args[0].split('=', 1)\n  if len(cmd) == 1 or cmd[1] == '':\n    msg.reply(u'\u9519\u8bef\uff1a\u8bf7\u7ed9\u51fa\u9009\u9879\u503c')\n    return\n  try:\n    handle = getattr(self, 'set_' + cmd[0])\n  except AttributeError:\n    msg.reply(u'\u9519\u8bef\uff1a\u672a\u77e5\u9009\u9879 %s' % cmd[0])\n  except IndexError:\n    msg.reply(u'\u9519\u8bef\uff1a\u65e0\u9009\u9879')\n  except UnicodeEncodeError:\n    msg.reply(u'\u9519\u8bef\uff1a\u9009\u9879\u540d\u89e3\u7801\u5931\u8d25\u3002\u6b64\u95ee\u9898\u5728 GAE \u5347\u7ea7\u5176 Python \u5230 3.x \u540e\u65b9\u80fd\u89e3\u51b3\u3002')\n  else:\n    handle(cmd[1])", "path": "gaetalk.py", "repo_name": "lilydjwg/gaetalk", "stars": 22, "license": "None", "language": "python", "size": 341}
{"docstring": "'''\u8bbe\u7f6e\u7fa4\u72b6\u6001'''\n", "func_signal": "def do_groupstatus(self, arg):\n", "code": "grp = get_group_info()\nif grp is None:\n  grp = Group()\ngrp.status = self.msg.body[len(self.sender.prefix):].split(None, 1)[-1]\ngrp.put()\nfor u in User.all():\n  xmpp.send_presence(u.jid, status=grp.status)\nself.msg.reply(u'\u8bbe\u7f6e\u6210\u529f\uff01')", "path": "gaetalk.py", "repo_name": "lilydjwg/gaetalk", "stars": 22, "license": "None", "language": "python", "size": 341}
{"docstring": "'''\u5c06\u65f6\u95f4\u8f6c\u6362\u4e3a\u5b57\u7b26\u4e32\uff0c\u8003\u8651\u65f6\u533a\uff0c\u53ef\u80fd\u5e26\u65e5\u671f'''\n", "func_signal": "def strftime(time, timezone, show_date=False):\n", "code": "if not show_date:\n  format = '%H:%M:%S'\nelse:\n  format = '%m-%d %H:%M:%S'\nreturn (time + timezone).strftime(format)", "path": "utils.py", "repo_name": "lilydjwg/gaetalk", "stars": 22, "license": "None", "language": "python", "size": 341}
{"docstring": "'''show \u53ef\u4ee5\u662f away\u3001dnd\uff08\u5fd9\u788c\uff09\u6216\u7a7a\uff08\u5728\u7ebf\uff09'''\n", "func_signal": "def post(self):\n", "code": "jid, resource = self.request.get('from').split('/', 1)\nstatus = self.request.get('status')\nshow = self.request.get('show')\nlogging.debug(u'%s \u7684\u72b6\u6001: %s (%s)' % (jid, status, show))\ntry:\n  show = gaetalk.STATUS_CODE[show]\nexcept KeyError:\n  logging.error('%s has sent an incorrect show code %s' % (jid, show))\n  return\ntry:\n  gaetalk.send_status(self.request.get('from'))\nexcept xmpp.Error:\n  logging.error('Error while sending presence to %s' % jid)\n  return\nu = gaetalk.get_user_by_jid(jid)\nif u is not None:\n  modified = False\n  if resource not in u.resources:\n    u.resources.append(resource)\n    modified = True\n  if u.avail != show:\n    if u.avail == gaetalk.OFFLINE:\n      u.last_online_date = datetime.datetime.now()\n    u.avail = show\n    modified = True\n  if modified:\n    gaetalk.log_onoff(u, show, resource)\n    u.put()\n  if config.warnGtalk105 and resource.startswith('Talk.v105'):\n    xmpp.send_message(jid, u'\u60a8\u7684\u5ba2\u6237\u7aef\u4f7f\u7528\u660e\u6587\u4f20\u8f93\u6570\u636e\uff0c\u4e3a\u4e86\u5927\u5bb6\u7684\u5b89\u5168\uff0c\u8bf7\u4f7f\u7528Gtalk\u82f1\u6587\u7248\u6216\u8005\u5176\u5b83\u4f7f\u7528SSL\u52a0\u5bc6\u7684\u5ba2\u6237\u7aef\u3002')\nelse:\n  gaetalk.try_add_user(jid, show, resource)", "path": "chatmain.py", "repo_name": "lilydjwg/gaetalk", "stars": 22, "license": "None", "language": "python", "size": 341}
{"docstring": "'''\u89e3\u5c01\u67d0\u4e2a ID'''\n", "func_signal": "def do_unblock(self, args):\n", "code": "if len(args) != 1:\n  self.msg.reply(u'\u8bf7\u7ed9\u51fa\u8981\u89e3\u5c01\u7528\u6237\u7684 JID\u3002')\n  return\n\ntarget = get_blocked_user(args[0])\nif target is None:\n  self.msg.reply(u'\u5c01\u7981\u5217\u8868\u4e2d\u6ca1\u6709\u8fd9\u4e2a JID\u3002')\n  return\n\ntarget.delete()\nsend_to_all((u'%s \u5df2\u88ab\u89e3\u9664\u5c01\u7981\u3002' % args[0]) \\\n            .encode('utf-8'))\nlog_admin(self.sender, UNBLOCK % args[0])", "path": "gaetalk.py", "repo_name": "lilydjwg/gaetalk", "stars": 22, "license": "None", "language": "python", "size": 341}
{"docstring": "'''\u67e5\u770b\u6216\u8bbe\u7f6e\u7fa4\u4e3b\u9898'''\n", "func_signal": "def do_topic(self, args=()):\n", "code": "grp = get_group_info()\nif not args:\n  if grp is None or not grp.topic:\n    self.msg.reply(u'\u6ca1\u6709\u8bbe\u7f6e\u7fa4\u4e3b\u9898\u3002')\n  else:\n    self.msg.reply(grp.topic)\nelse:\n  grp = get_group_info()\n  if grp is None:\n    grp = Group()\n  grp.topic = self.msg.body[len(self.sender.prefix):].split(None, 1)[-1]\n  grp.put()\n  self.msg.reply(u'\u7fa4\u4e3b\u9898\u5df2\u66f4\u65b0\u3002')", "path": "gaetalk.py", "repo_name": "lilydjwg/gaetalk", "stars": 22, "license": "None", "language": "python", "size": 341}
{"docstring": "'''\u6e05\u9664\u591a\u4f59\u7684\u94fe\u63a5\u6587\u672c'''\n", "func_signal": "def removelinks(msg):\n", "code": "links = linkre.findall(msg)\nif len(links) != 1:\n  msg = linkre.sub('', msg)\nmsg = linkjsre.sub('', msg)\nreturn msg", "path": "utils.py", "repo_name": "lilydjwg/gaetalk", "stars": 22, "license": "None", "language": "python", "size": 341}
{"docstring": "'''\u5217\u51fa\u88ab\u5c01\u7981\u7528\u6237\u540d\u5355'''\n", "func_signal": "def do_lsblocked(self, args):\n", "code": "r = []\nl = BlockedUser.all()\nfor u in l:\n  r.append(unicode('* %s (%s, %s)' % (u.jid,\n                                      utils.strftime(u.add_date, timezone),\n                                      u.reason)\n                  )\n          )\nr.sort()\nn = len(r)\nr.insert(0, u'\u5c01\u7981\u5217\u8868:')\nr.append(u'\u5171 %d \u4e2a JID \u88ab\u5c01\u7981\u3002' % n)\nself.msg.reply(u'\\n'.join(r).encode('utf-8'))", "path": "gaetalk.py", "repo_name": "lilydjwg/gaetalk", "stars": 22, "license": "None", "language": "python", "size": 341}
{"docstring": "'''\u8bbe\u7f6e\u662f\u5426\u63a5\u6536\u79c1\u4fe1\uff0c\u53c2\u6570\u4e3a y\uff08\u63a5\u6536\uff09\u6216\u8005 n\uff08\u62d2\u7edd\uff09'''\n", "func_signal": "def set_allowpm(self, arg):\n", "code": "if arg not in 'yn':\n  self.msg.reply(u'\u9519\u8bef\u7684\u53c2\u6570\u3002')\n  return\n\nif arg == 'y':\n  self.sender.reject_pm = False\nelse:\n  self.sender.reject_pm = True\nself.sender.put()\nself.msg.reply(u'\u8bbe\u7f6e\u6210\u529f\uff01')", "path": "gaetalk.py", "repo_name": "lilydjwg/gaetalk", "stars": 22, "license": "None", "language": "python", "size": 341}
{"docstring": "'''\u5047\u88c5\u79bb\u7ebf\uff0c\u8ba9\u7a0b\u5e8f\u8ba4\u4e3a\u4f60\u7684\u6240\u6709\u8d44\u6e90\u5df2\u79bb\u7ebf\u3002\u5982\u5728\u4f60\u79bb\u7ebf\u65f6\u7a0b\u5e8f\u4ecd\u8ba4\u4e3a\u4f60\u5728\u7ebf\uff0c\u8bf7\u4f7f\u7528\u6b64\u547d\u4ee4\u3002'''\n", "func_signal": "def do_offline(self, args):\n", "code": "del self.sender.resources[:]\nself.sender.avail = OFFLINE\nself.sender.last_offline_date = datetime.datetime.now()\nself.sender.put()\nself.msg.reply('OK\uff0c\u5728\u4e0b\u6b21\u4f60\u8bf4\u4f60\u5728\u7ebf\u4e4b\u524d\u6211\u90fd\u8ba4\u4e3a\u4f60\u5df2\u79bb\u7ebf\u3002')", "path": "gaetalk.py", "repo_name": "lilydjwg/gaetalk", "stars": 22, "license": "None", "language": "python", "size": 341}
{"docstring": "'''\u5224\u65ad\u4e00\u4e2a\u6635\u79f0\u662f\u5426\u5408\u6cd5'''\n", "func_signal": "def checkNick(nick):\n", "code": "if len(nick.encode('utf-8')) > config.nick_maxlen:\n  return False\nfor i in nick:\n  cat = unicodedata.category(i)\n  # Lt & Lm are special chars\n  if (not (cat.startswith('L') or cat.startswith('N')) or cat in ('Lm', 'Lt')) \\\n     and i not in config.allowedSymbolInNick:\n    return False\nreturn True", "path": "utils.py", "repo_name": "lilydjwg/gaetalk", "stars": 22, "license": "None", "language": "python", "size": 341}
{"docstring": "'''\u5c01\u7981\u67d0\u4e2a ID\uff0c\u53c2\u6570\u4e3a\u7528\u6237\u6635\u79f0\u6216\u8005 ID\uff08\u5982\u679c\u4e0d\u662f\u5df2\u7ecf\u52a0\u5165\u7684 ID \u7684\u8bdd\uff09\uff0c\u4ee5\u53ca\u5c01\u7981\u539f\u56e0'''\n", "func_signal": "def do_block(self, args):\n", "code": "if len(args) < 2:\n  self.msg.reply(u'\u8bf7\u7ed9\u51fa\u8981\u5c01\u7981\u7684\u7528\u6237\u548c\u539f\u56e0\u3002')\n  return\n\ntarget = get_user_by_nick(args[0])\nreason = self.msg.body[len(self.sender.prefix):].split(None, 2)[-1]\nif target is None:\n  jid = args[0]\n  name = jid\n  fullname = name\nelse:\n  jid = target.jid\n  name = target.nick\n  fullname = '%s (%s)' % (name, jid)\nu = BlockedUser.gql('where jid = :1', jid).get()\nif u is not None:\n  self.msg.reply(u'\u6b64 JID \u5df2\u7ecf\u88ab\u5c01\u7981\u3002')\n  return\n\nif jid == config.root:\n  self.msg.reply('\u4e0d\u80fd\u5c01\u7981 root \u7528\u6237')\n  return\n\nif target:\n  target.delete()\nu = BlockedUser(jid=jid, reason=reason)\nu.put()\n\nsend_to_all_except(self.sender.jid,\n                   (u'%s \u5df2\u88ab\u672c\u7fa4\u5c01\u7981\uff0c\u7406\u7531\u4e3a %s\u3002' % (name, reason)) \\\n                   .encode('utf-8'))\nself.msg.reply(u'%s \u5df2\u88ab\u672c\u7fa4\u5c01\u7981\uff0c\u7406\u7531\u4e3a %s\u3002' % (fullname, reason))\nxmpp.send_message(jid, u'\u4f60\u5df2\u88ab\u672c\u7fa4\u5c01\u7981\uff0c\u7406\u7531\u4e3a %s\u3002' % reason)\nxmpp.send_presence(jid, status=u'\u60a8\u5df2\u7ecf\u88ab\u672c\u7fa4\u5c01\u7981')\nlog_admin(self.sender, BLOCK % (fullname, reason))", "path": "gaetalk.py", "repo_name": "lilydjwg/gaetalk", "stars": 22, "license": "None", "language": "python", "size": 341}
{"docstring": "'''\u8bbe\u7f6e\u6635\u79f0\u663e\u793a\u683c\u5f0f\uff0c\u7528 %s \u8868\u793a\u6635\u79f0\u7684\u4f4d\u7f6e'''\n", "func_signal": "def set_nickpattern(self, arg):\n", "code": "try:\n  arg % 'test'\nexcept (TypeError, ValueError):\n  self.msg.reply(u'\u9519\u8bef\uff1a\u4e0d\u6b63\u786e\u7684\u683c\u5f0f')\n  return\n\nself.sender.nick_pattern = arg\nself.sender.put()\nself.msg.reply(u'\u8bbe\u7f6e\u6210\u529f\uff01')", "path": "gaetalk.py", "repo_name": "lilydjwg/gaetalk", "stars": 22, "license": "None", "language": "python", "size": 341}
{"docstring": "'''\u8bbe\u7f6e\u81ea\u6211\u4ecb\u7ecd\u4fe1\u606f'''\n", "func_signal": "def do_intro(self, arg):\n", "code": "if not arg:\n  self.msg.reply('\u8bf7\u7ed9\u51fa\u81ea\u6211\u4ecb\u7ecd\u7684\u5185\u5bb9\u3002')\n  return\n\nmsg = self.get_msg_part(1)\nu = self.sender\ntry:\n  u.intro = msg\nexcept db.BadValueError:\n  # \u8fc7\u957f\u6587\u672c\u5df2\u5728 handle_message \u4e2d\u88ab\u62e6\u622a\n  self.msg.reply('\u9519\u8bef\uff1a\u81ea\u6211\u4ecb\u7ecd\u5185\u5bb9\u53ea\u80fd\u4e3a\u4e00\u884c\u3002')\n  return\n\nu.put()\nself.msg.reply(u'\u8bbe\u7f6e\u6210\u529f\uff01')", "path": "gaetalk.py", "repo_name": "lilydjwg/gaetalk", "stars": 22, "license": "None", "language": "python", "size": 341}
{"docstring": "'''\u8bbe\u7f6e\u547d\u4ee4\u524d\u7f00'''\n", "func_signal": "def set_prefix(self, arg):\n", "code": "self.sender.prefix = arg\nself.sender.put()\nself.msg.reply(u'\u8bbe\u7f6e\u6210\u529f\uff01')", "path": "gaetalk.py", "repo_name": "lilydjwg/gaetalk", "stars": 22, "license": "None", "language": "python", "size": 341}
{"docstring": "'''\u5220\u9664\u67d0\u4eba\u3002\u4ed6\u4ecd\u53ef\u4ee5\u91cd\u65b0\u52a0\u5165\u3002'''\n", "func_signal": "def do_kick(self, args):\n", "code": "if len(args) != 1:\n  self.msg.reply('\u8bf7\u7ed9\u51fa\u6635\u79f0\u3002')\n  return\n\ntarget = get_user_by_nick(args[0])\nif target is None:\n  self.msg.reply('Sorry\uff0c\u67e5\u65e0\u6b64\u4eba\u3002')\n  return\n\nif target.jid == config.root:\n  self.msg.reply('\u4e0d\u80fd\u5220\u9664 root \u7528\u6237')\n  return\n\ntargetjid = target.jid\ntargetnick = target.nick\ntarget.delete()\nself.msg.reply((u'OK\uff0c\u5220\u9664 %s\u3002' % target.nick).encode('utf-8'))\nsend_to_all_except(self.sender.jid, (u'%s \u5df2\u88ab\u5220\u9664\u3002' % target.nick) \\\n                   .encode('utf-8'))\nxmpp.send_message(targetjid, u'\u4f60\u5df2\u88ab\u7ba1\u7406\u5458\u4ece\u6b64\u7fa4\u4e2d\u5220\u9664\uff0c\u8bf7\u5220\u9664\u8be5\u597d\u53cb\u3002')\nlog_admin(self.sender, KICK % (targetnick, targetjid))", "path": "gaetalk.py", "repo_name": "lilydjwg/gaetalk", "stars": 22, "license": "None", "language": "python", "size": 341}
{"docstring": "'''\u5728\u7ebf\u6210\u5458\u5217\u8868\u3002\u53ef\u5e26\u4e00\u4e2a\u53c2\u6570\uff0c\u6307\u5b9a\u5728\u540d\u5b57\u4e2d\u51fa\u73b0\u7684\u4e00\u4e2a\u5b50\u4e32\u3002'''\n", "func_signal": "def do_online(self, args):\n", "code": "r = []\npat = args[0] if args else None\nnow = datetime.datetime.now()\nl = User.gql('where avail != :1', OFFLINE)\nfor u in l:\n  m = u.nick\n  if pat and m.find(pat) == -1:\n    continue\n  status = u.avail\n  if status != ONLINE:\n    m += u' (%s)' % status\n  if u.snooze_before is not None and u.snooze_before > now:\n    m += u' (snoozing)'\n  if u.black_before is not None and u.black_before > now:\n    m += u' (\u5df2\u7981\u8a00)'\n  r.append(unicode('* ' + m))\nr.sort()\nn = len(r)\nif pat:\n  r.insert(0, u'\u5728\u7ebf\u6210\u5458\u5217\u8868\uff08\u5305\u542b\u5b50\u4e32 %s\uff09:' % pat)\n  r.append(u'\u5171 %d \u4eba\u3002' % n)\nelse:\n  r.insert(0, u'\u5728\u7ebf\u6210\u5458\u5217\u8868:')\n  r.append(u'\u5171 %d \u4eba\u5728\u7ebf\u3002' % n)\nself.msg.reply(u'\\n'.join(r).encode('utf-8'))", "path": "gaetalk.py", "repo_name": "lilydjwg/gaetalk", "stars": 22, "license": "None", "language": "python", "size": 341}
{"docstring": "'''\u53cb\u597d\u5730\u663e\u793a\u65f6\u95f4'''\n", "func_signal": "def displayTime(t):\n", "code": "r = []\nfor i in timeZhUnitMap:\n  r.append(t % i)\n  t = t // i\n  if t == 0:\n    break\nreturn u''.join(reversed(map(lambda x, y: unicode(x)+y if x else u'', r, timeZhUnits)))", "path": "utils.py", "repo_name": "lilydjwg/gaetalk", "stars": 22, "license": "None", "language": "python", "size": 341}
{"docstring": "'''Processess information needed for graphical elements to enter city view.'''\n# We need to send list of tiles for terrain manager\n", "func_signal": "def enterCity(self, ident):\n", "code": "tiles = []\nxsum = 0\nysum = 0\nn = 0\nfor tile in self.tiles:\n    if tile.cityid is ident:\n        tiles.append(tile)\n        # We need to compute center of city to target camera there\n        xsum += tile.coords[0]\n        ysum += tile.coords[1]\n        n += 1\nxavg = xsum/n\nyavg = ysum/n\nposition = (xavg, yavg)\n# We need to send city info so gui elements can be drawn\ncity = self.cities[ident]\nmessenger.send('enterCityView', [ident, city, position, tiles])", "path": "client\\region.py", "repo_name": "croxis/CityMania", "stars": 28, "license": "None", "language": "python", "size": 3387}
{"docstring": "'''Returns tile by coordinate. \nThankfully smart enough to find a way to not iterate\n'''\n", "func_signal": "def getTile(self, x, y):\n", "code": "value = y * self.region_size[0] + x\nreturn self.tiles[value]", "path": "client\\region.py", "repo_name": "croxis/CityMania", "stars": 28, "license": "None", "language": "python", "size": 3387}
{"docstring": "'''Loads a new region, usually from connecting to a server\nOr starting a new or previously saved region.\n'''\n", "func_signal": "def load(self, container, name=\"New Region\"):\n", "code": "import base64\nself.heightmap = PNMImage()\nimageString = base64.b64decode(container.heightmap)\nself.heightmap.read(StringStream(imageString))\nself.region_size = (self.heightmap.getXSize()-1, self.heightmap.getYSize()-1)\n\nposition = 0\ntileid = 0\ntotal_tiles = self.region_size[0] * self.region_size[1]\nranges = []\ntiles = []\n\nfor tile in container.tiles:\n    tiles.append((tile.id, tile.cityid))\nfor n in range(len(tiles)):\n    try:\n        ranges.append((tiles[n][0], tiles[n+1][0]-1, tiles[n][1]))\n    except:\n        ranges.append((tiles[n][0], total_tiles, tiles[n][1]))\nfor r in ranges:\n    for x in range(r[0], r[1]+1):\n        #print \"r0, r1, x\", r[0], r[1], x\n        self.tiles.append(Tile(tileid, r[2]))\n        #print \"Len\", len(self.tiles)\n        tileid += 1\n\nposition = 0\nfor y in range(self.region_size[1]):\n    for x in range(self.region_size[0]):\n        self.tiles[position].coords = (x,y)\n        position += 1\n        \nfor city in container.cities:\n    self.newCity(city)\nmessenger.send(\"generateRegion\", [self.heightmap, self.tiles, self.cities, container])", "path": "client\\region.py", "repo_name": "croxis/CityMania", "stars": 28, "license": "None", "language": "python", "size": 3387}
{"docstring": "'''Checks permissions and, if all good, unfounds city.'''\n# Can't unfound the region or a city that doesn't exist\n", "func_signal": "def unfoundCity(self, peer, ident):\n", "code": "logging.debug(\"Requesting to unfound city %s by %s\" %(ident, peer))\ncontainer = proto.Container()\nif not ident or ident not in self.cities:\n    container.response = \"Can not unfound imaginary city.\"\n    messenger.send(\"sendData\", [peer, container])\n    return\n\nuser = users.getNameFromPeer(peer)\naccess = users.getType(user)\n\nif access > 1 or self.cities[ident].mayor == user:\n    for tile in self.tiles:\n        if tile.cityid is ident:\n            tile.cityid = 0\n            self.updateTile(container, tile)\n    del self.cities[ident]\n    container.unfoundCity = ident\n    messenger.send(\"broadcastData\", [container])\nelse:\n    container.response = \"Lack permissions for unfounding this city.\"\n    messenger.send(\"sendData\", [peer, container])\nlogging.info(\"City %s unfounded. New city db: %s\" %(ident, self.cities))", "path": "server\\region.py", "repo_name": "croxis/CityMania", "stars": 28, "license": "None", "language": "python", "size": 3387}
{"docstring": "'''Sets the user access level and name'''\n", "func_signal": "def setSelf(self, level, name):\n", "code": "access.level = level\naccess.username = name", "path": "client\\main.py", "repo_name": "croxis/CityMania", "stars": 28, "license": "None", "language": "python", "size": 3387}
{"docstring": "'''Returns tiles for a particular city.'''\n# Itterate, yuck and slow. This should only be called when a city loads\n", "func_signal": "def getCityTiles(self, cityid):\n", "code": "cityTiles = []\nfor tile in self.tiles:\n    if tile.cityid is cityid:\n        cityTiles.append(tile)\nreturn cityTiles", "path": "server\\region.py", "repo_name": "croxis/CityMania", "stars": 28, "license": "None", "language": "python", "size": 3387}
{"docstring": "\"\"\"initialize\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self.server = None\nself.language = 'english'\nself.singlePlayer = False\nself.accept('exit', self.exit)\nself.accept('setSelfAccess', self.setSelf)\nself.accept(\"finishedTerrainGen\", self.setupRig)\n\nbase.disableMouse()\n\nbase.setFrameRateMeter(True)\nself.keys()\n\n# Initialize classes\nlights = environment.Lights(lightsOn = True, showLights = True)\n\nself.terrainManager = environment.TerrainManager()", "path": "client\\main.py", "repo_name": "croxis/CityMania", "stars": 28, "license": "None", "language": "python", "size": 3387}
{"docstring": "\"\"\"\nPeeks into the chat container for what to do\nThe chat container is then resent, intact,\nto the message target\n\"\"\"\n", "func_signal": "def processChat(self, peer, chat):\n", "code": "if not chat.HasField(\"to\"):\n    chat.to = \"#region\"\n# We inject who the chat is from, so we know who is talking\nfor username, value in self.users.items():\n    if value == peer:\n        chat.sender = username\nif chat.to.startswith(\"#\") and chat.to in self.channels:\n    for user in self.users:\n        peerTo = self.users[user]\n        self.sendChat(peerTo, chat)\nelif container.to in self.users:\n    peerTo = self.users[chat.to]\n    self.sendChat(peerTo, chat)\nelse:\n    pass\n    #return error", "path": "server\\chat.py", "repo_name": "croxis/CityMania", "stars": 28, "license": "None", "language": "python", "size": 3387}
{"docstring": "\"\"\"\nGenerates region\nheightmap is a grayscale bitmap for height\ncolormap is color bitmap for terrain texture\nterrainTextureDB is data on texture to use for color map\n\"\"\"\n", "func_signal": "def generate(self, name, heightmap):\n", "code": "self.heightmap = heightmap\nheightmapBuffer = StringIO.StringIO(self.heightmap)\nheightmapImage = Image.open(heightmapBuffer)\nself.width, self.height = heightmapImage.size\n# Image is 1 px more than number of tiles\nself.width -= 1\nself.height -= 1\nself.name = name\n# Generate the simulation tiles\ntileid = 0\nfor y in range(self.height):\n    for x in range(self.width):\n        tile = Tile(tileid, coords=(x,y))\n        self.tiles.append(tile)\n        tileid+= 1\n# Other generations such as initial roads, etc, is done here\nself.sendGameState()", "path": "server\\region.py", "repo_name": "croxis/CityMania", "stars": 28, "license": "None", "language": "python", "size": 3387}
{"docstring": "\"\"\"\nLogs in player to the server\n\"\"\"\n", "func_signal": "def login(self, peer, login):\n", "code": "self.lock.acquire()\ncontainer = proto.Container()\ncontainer.loginResponse.type = 1\nif login.regionPassword != self.password:\n    container.loginResponse.type = 0\n    container.loginResponse.message = \"Region password incorrect\"\nif login.name not in users.userdb:\n    users.addUser(login.name, login.password)\nloginType = users.login(login.name, login.password, peer)\nif not loginType:\n    container.loginResponse.type = 0\n    container.loginResponse.message = \"Player password incorrect\"\nif container.loginResponse.type:\n    container.loginResponse.usertype = users.getType(login.name)\n    container.loginResponse.username = login.name\n    messenger.send(\"loggedIn\", [peer, login.name])\n    logger.info(\"Logged in: %s %s\" %(login.name, peer) )\nmessenger.send(\"sendData\", [peer, container])\nself.lock.release()", "path": "server\\server.py", "repo_name": "croxis/CityMania", "stars": 28, "license": "None", "language": "python", "size": 3387}
{"docstring": "\"\"\"\nBreaks a message from an IRC server into its prefix, command, and arguments.\nTheved from twisted\n\"\"\"\n", "func_signal": "def parsemsg(self, s):\n", "code": "prefix = ''\ntrailing = []\nif not s:\n    #raise IRCBadMessage(\"Empty line.\")\n    return None, None, None\nif s[0] == ':':\n    prefix, s = s[1:].split(' ', 1)\nif s.find(' :') != -1:\n    s, trailing = s.split(' :', 1)\n    args = s.split()\n    args.append(trailing)\nelse:\n    args = s.split()\ncommand = args.pop(0)\nreturn prefix, command, args", "path": "server\\chat.py", "repo_name": "croxis/CityMania", "stars": 28, "license": "None", "language": "python", "size": 3387}
{"docstring": "\"\"\"\nLoads the designated mod into memory, will require some helper functions in other classes\n\"\"\"\n\n", "func_signal": "def loadMod(name):\n", "code": "\n# Create the directories\n#filesystem.home(oo = True)\n#print \"Path:\", filesystem.home()\n\n#LOG_FILENAME = 'client.log'\n#logging.basicConfig(filename=LOG_FILENAME,level=logging.DEBUG,)\nvfs.createVFS()\nsys.stdout = Logger()\n\nconnection = network.ServerSocket()\n\nscript = gui.Script()\n#messenger.toggleVerbose()\n\n#audioManager = Audio.AudioManager()\n\nworld=World()\nguiController = gui.GUIController(script)\nguiController.makeMainMenu()\nserverHost = 'localhost'\nserverPort = 52003\nreg = region.Region()\nkmcontroller = controllers.KMController()\nrun()", "path": "client\\main.py", "repo_name": "croxis/CityMania", "stars": 28, "license": "None", "language": "python", "size": 3387}
{"docstring": "'''Checks if user can enter city and if so, what permissions.'''\n# TODO: Check if city is active, if not send tiles to be simulated\n# TODO: Flag city as active\n", "func_signal": "def checkEnterCity(self, peer, ident):\n", "code": "userName = users.getNameFromPeer(peer)\ncity = self.cities[ident]\ncontainer = proto.Container()\nif users.isAdmin(userName) or userName == city.mayor:\n    container.enterCity = ident\nelif users.canUser(userName, ident, 'viewCity'):\n    container.enterCity = ident\nelse:\n    container.response = \"You lack permission to enter city.\"\nmessenger.send(\"sendData\", [peer, container])", "path": "server\\region.py", "repo_name": "croxis/CityMania", "stars": 28, "license": "None", "language": "python", "size": 3387}
{"docstring": "\"\"\"keys\"\"\"\n", "func_signal": "def keys(self):\n", "code": "base.accept(\"f\", base.toggleWireframe)\nself.accept('t',self.toggleTexture)\n#self.accept('s',self.snapShot)", "path": "client\\main.py", "repo_name": "croxis/CityMania", "stars": 28, "license": "None", "language": "python", "size": 3387}
{"docstring": "'''Sends game state. Requires full simulation pause while in progress.\nIn this region package tiles are sent by changes in city id. No other data needs to be sent until a city is activated.\n'''\n", "func_signal": "def sendGameState(self, peer=None):\n", "code": "messenger.send(\"setServerState\", [1])\ncontainer = proto.Container()\ncontainer.gameState.name = self.name\ncontainer.gameState.heightmap = base64.b64encode(self.heightmap)\n# Used to check for change\ntilecityid = -1\nfor tile in self.tiles:\n    if tile.cityid != tilecityid:\n        tilecityid = tile.cityid\n        t = container.gameState.tiles.add()\n        t.id = tile.id\n        t.positionx = tile.coords[0]\n        t.positiony = tile.coords[1]\n        t.cityid = tile.cityid\n\nfor ident, city in self.cities.items():\n    c = container.gameState.cities.add()\n    c.id = ident\n    c.name = city.name\n    c.mayor = city.mayor\n    c.population = city.population\n    c.funds = city.funds\n\nif peer:\n    messenger.send(\"sendData\", [peer, container])\nelse:\n    messenger.send(\"broadcastData\", [container])\n# TODO: Create method to return to previous server state after we finished sending", "path": "server\\region.py", "repo_name": "croxis/CityMania", "stars": 28, "license": "None", "language": "python", "size": 3387}
{"docstring": "\"\"\"\nServer states:\n0:  No region loaded.\n1:  Region loaded, but full simulation pause\n\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self.serverState = 0\nself.accept(\"requestServerState\", self.getServerState)\nself.accept(\"requestGameState\", self.fullPause)\nself.accept(\"setServerState\", self.setServerState)", "path": "server\\server.py", "repo_name": "croxis/CityMania", "stars": 28, "license": "None", "language": "python", "size": 3387}
{"docstring": "#print \"Step1\"\n", "func_signal": "def step(self):\n", "code": "if self.commandQueue:\n    #self.lock.acquire()\n    peer, data = self.commandQueue.pop()\n    self.processData(peer, data)\n    #self.lock.release()", "path": "server\\server.py", "repo_name": "croxis/CityMania", "stars": 28, "license": "None", "language": "python", "size": 3387}
{"docstring": "#return\n", "func_signal": "def write(self, message):\n", "code": "self.terminal.write(message)\nself.log.write(message)", "path": "client\\main.py", "repo_name": "croxis/CityMania", "stars": 28, "license": "None", "language": "python", "size": 3387}
{"docstring": "# If serverstate is 0 then can't change it to 1!\n", "func_signal": "def fullPause(self, var1=None):\n", "code": "if self.serverState:\n    self.serverState = 1", "path": "server\\server.py", "repo_name": "croxis/CityMania", "stars": 28, "license": "None", "language": "python", "size": 3387}
{"docstring": "'''Checks for city in given cell for region gui display'''\n", "func_signal": "def checkCity(self, cell):\n", "code": "if not cell: return\ntile = self.getTile(cell[0], cell[1])\nif tile.cityid:\n    messenger.send(\"showRegionCityWindow\", [tile.cityid, self.cities[tile.cityid]])", "path": "client\\region.py", "repo_name": "croxis/CityMania", "stars": 28, "license": "None", "language": "python", "size": 3387}
{"docstring": "\"\"\"Format the given partition, specified by its mount point (eg,\n\"/system\").\"\"\"\n\n", "func_signal": "def FormatPartition(self, partition):\n", "code": "reserve_size = 0\nfstab = self.info.get(\"fstab\", None)\nif fstab:\n  p = fstab[partition]\n  self.script.append('format(\"%s\", \"%s\", \"%s\", \"%s\");' %\n                     (p.fs_type, common.PARTITION_TYPES[p.fs_type],\n                      p.device, p.length))", "path": "releasetools\\triumph_edify_generator.py", "repo_name": "ikarosdev/android_device_motorola_triumph", "stars": 20, "license": "None", "language": "python", "size": 42272}
{"docstring": "\"\"\"Check that the given file (or MTD reference) has one of the\ngiven *sha1 hashes, checking the version saved in cache if the\nfile does not match.\"\"\"\n", "func_signal": "def PatchCheck(self, filename, *sha1):\n", "code": "self.script.append('assert(apply_patch_check(\"%s\"' % (filename,) +\n                   \"\".join([', \"%s\"' % (i,) for i in sha1]) +\n                   '));')", "path": "releasetools\\triumph_edify_generator.py", "repo_name": "ikarosdev/android_device_motorola_triumph", "stars": 20, "license": "None", "language": "python", "size": 42272}
{"docstring": "\"\"\"Recursively set path ownership and permissions.\"\"\"\n", "func_signal": "def SetPermissionsRecursive(self, fn, uid, gid, dmode, fmode):\n", "code": "self.script.append('set_perm_recursive(%d, %d, 0%o, 0%o, \"%s\");'\n                   % (uid, gid, dmode, fmode, fn))", "path": "releasetools\\triumph_edify_generator.py", "repo_name": "ikarosdev/android_device_motorola_triumph", "stars": 20, "license": "None", "language": "python", "size": 42272}
{"docstring": "\"\"\"Assert that the current system build fingerprint is one of *fp.\"\"\"\n", "func_signal": "def AssertSomeFingerprint(self, *fp):\n", "code": "if not fp:\n  raise ValueError(\"must specify some fingerprints\")\ncmd = ('assert(' +\n       ' ||\\0'.join([('file_getprop(\"/system/build.prop\", '\n                     '\"ro.build.fingerprint\") == \"%s\"')\n                    % i for i in fp]) +\n       ');')\nself.script.append(self._WordWrap(cmd))", "path": "releasetools\\triumph_edify_generator.py", "repo_name": "ikarosdev/android_device_motorola_triumph", "stars": 20, "license": "None", "language": "python", "size": 42272}
{"docstring": "\"\"\"Assert that the device identifier is the given string.\"\"\"\n", "func_signal": "def AssertDevice(self, device):\n", "code": "cmd = ('assert(' + \n       ' || \\0'.join(['getprop(\"ro.product.device\") == \"%s\" || getprop(\"ro.build.product\") == \"%s\" || getprop(\"ro.product.board\") == \"%s\"'\n                     % (i, i, i) for i in device.split(\",\")]) + \n       ');')\nself.script.append(self._WordWrap(cmd))", "path": "releasetools\\triumph_edify_generator.py", "repo_name": "ikarosdev/android_device_motorola_triumph", "stars": 20, "license": "None", "language": "python", "size": 42272}
{"docstring": "\"\"\"Asert that the bootloader version is one of *bootloaders.\"\"\"\n", "func_signal": "def AssertSomeBootloader(self, *bootloaders):\n", "code": "cmd = (\"assert(\" +\n       \" ||\\0\".join(['getprop(\"ro.bootloader\") == \"%s\"' % (b,)\n                     for b in bootloaders]) +\n       \");\")\nself.script.append(self._WordWrap(cmd))", "path": "releasetools\\triumph_edify_generator.py", "repo_name": "ikarosdev/android_device_motorola_triumph", "stars": 20, "license": "None", "language": "python", "size": 42272}
{"docstring": "\"\"\"Read and parse the META/misc_info.txt key/value pairs from the\ninput target files and return a dict.\"\"\"\n\n", "func_signal": "def LoadInfoDict(zip):\n", "code": "d = {}\ntry:\n  for line in zip.read(\"META/misc_info.txt\").split(\"\\n\"):\n    line = line.strip()\n    if not line or line.startswith(\"#\"): continue\n    k, v = line.split(\"=\", 1)\n    d[k] = v\nexcept KeyError:\n  # ok if misc_info.txt doesn't exist\n  pass\n\n# backwards compatibility: These values used to be in their own\n# files.  Look for them, in case we're processing an old\n# target_files zip.\n\nif \"mkyaffs2_extra_flags\" not in d:\n  try:\n    d[\"mkyaffs2_extra_flags\"] = zip.read(\"META/mkyaffs2-extra-flags.txt\").strip()\n  except KeyError:\n    # ok if flags don't exist\n    pass\n\nif \"recovery_api_version\" not in d:\n  try:\n    d[\"recovery_api_version\"] = zip.read(\"META/recovery-api-version.txt\").strip()\n  except KeyError:\n    raise ValueError(\"can't find recovery API version in input target-files\")\n\nif \"tool_extensions\" not in d:\n  try:\n    d[\"tool_extensions\"] = zip.read(\"META/tool-extensions.txt\").strip()\n  except KeyError:\n    # ok if extensions don't exist\n    pass\n\ntry:\n  data = zip.read(\"META/imagesizes.txt\")\n  for line in data.split(\"\\n\"):\n    if not line: continue\n    name, value = line.split(\" \", 1)\n    if not value: continue\n    if name == \"blocksize\":\n      d[name] = value\n    else:\n      d[name + \"_size\"] = value\nexcept KeyError:\n  pass\n\ndef makeint(key):\n  if key in d:\n    d[key] = int(d[key], 0)\n\nmakeint(\"recovery_api_version\")\nmakeint(\"blocksize\")\nmakeint(\"system_size\")\nmakeint(\"userdata_size\")\nmakeint(\"recovery_size\")\nmakeint(\"boot_size\")\n\nd[\"fstab\"] = LoadRecoveryFSTab(zip)\nreturn d", "path": "releasetools\\triumph_common.py", "repo_name": "ikarosdev/android_device_motorola_triumph", "stars": 20, "license": "None", "language": "python", "size": 42272}
{"docstring": "\"\"\"Assert that the build on the device is older (or the same as)\nthe given timestamp.\"\"\"\n", "func_signal": "def AssertOlderBuild(self, timestamp):\n", "code": "self.script.append(('assert(!less_than_int(%s, '\n                    'getprop(\"ro.build.date.utc\")));') % (timestamp,))", "path": "releasetools\\triumph_edify_generator.py", "repo_name": "ikarosdev/android_device_motorola_triumph", "stars": 20, "license": "None", "language": "python", "size": 42272}
{"docstring": "\"\"\"Check that the given file (or MTD reference) has one of the\ngiven *sha1 hashes.\"\"\"\n", "func_signal": "def FileCheck(self, filename, *sha1):\n", "code": "self.script.append('assert(sha1_check(read_file(\"%s\")' % (filename,) +\n                   \"\".join([', \"%s\"' % (i,) for i in sha1]) +\n                   '));')", "path": "releasetools\\triumph_edify_generator.py", "repo_name": "ikarosdev/android_device_motorola_triumph", "stars": 20, "license": "None", "language": "python", "size": 42272}
{"docstring": "\"\"\"Create symlinks, given a list of (dest, link) pairs.\"\"\"\n", "func_signal": "def MakeSymlinks(self, symlink_list):\n", "code": "by_dest = {}\nfor d, l in symlink_list:\n  by_dest.setdefault(d, []).append(l)\n\nfor dest, links in sorted(by_dest.iteritems()):\n  cmd = ('symlink(\"%s\", ' % (dest,) +\n         \",\\0\".join(['\"' + i + '\"' for i in sorted(links)]) + \");\")\n  self.script.append(self._WordWrap(cmd))", "path": "releasetools\\triumph_edify_generator.py", "repo_name": "ikarosdev/android_device_motorola_triumph", "stars": 20, "license": "None", "language": "python", "size": 42272}
{"docstring": "\"\"\"Given a target_files ZipFile, parse the META/apkcerts.txt file\nand return a {package: cert} dict.\"\"\"\n", "func_signal": "def ReadApkCerts(tf_zip):\n", "code": "certmap = {}\nfor line in tf_zip.read(\"META/apkcerts.txt\").split(\"\\n\"):\n  line = line.strip()\n  if not line: continue\n  m = re.match(r'^name=\"(.*)\"\\s+certificate=\"(.*)\"\\s+'\n               r'private_key=\"(.*)\"$', line)\n  if m:\n    name, cert, privkey = m.groups()\n    if cert in SPECIAL_CERT_STRINGS and not privkey:\n      certmap[name] = cert\n    elif (cert.endswith(\".x509.pem\") and\n          privkey.endswith(\".pk8\") and\n          cert[:-9] == privkey[:-4]):\n      certmap[name] = cert[:-9]\n    else:\n      raise ValueError(\"failed to parse line from apkcerts.txt:\\n\" + line)\nreturn certmap", "path": "releasetools\\triumph_common.py", "repo_name": "ikarosdev/android_device_motorola_triumph", "stars": 20, "license": "None", "language": "python", "size": 42272}
{"docstring": "\"\"\"Take a kernel, cmdline, and ramdisk directory from the input (in\n'sourcedir'), and turn them into a boot image.  Return the image\ndata, or None if sourcedir does not appear to contains files for\nbuilding the requested image.\"\"\"\n\n", "func_signal": "def BuildBootableImage(sourcedir):\n", "code": "if (not os.access(os.path.join(sourcedir, \"RAMDISK\"), os.F_OK) or\n    not os.access(os.path.join(sourcedir, \"kernel\"), os.F_OK)):\n  return None\n\nramdisk_img = tempfile.NamedTemporaryFile()\nimg = tempfile.NamedTemporaryFile()\n\np1 = Run([\"mkbootfs\", os.path.join(sourcedir, \"RAMDISK\")],\n         stdout=subprocess.PIPE)\np2 = Run([\"minigzip\"],\n         stdin=p1.stdout, stdout=ramdisk_img.file.fileno())\n\np2.wait()\np1.wait()\nassert p1.returncode == 0, \"mkbootfs of %s ramdisk failed\" % (targetname,)\nassert p2.returncode == 0, \"minigzip of %s ramdisk failed\" % (targetname,)\n\n\"\"\"check if uboot is requested\"\"\"\nfn = os.path.join(sourcedir, \"ubootargs\")\nif os.access(fn, os.F_OK):\n  cmd = [\"mkimage\"]\n  for argument in open(fn).read().rstrip(\"\\n\").split(\" \"):\n    cmd.append(argument)\n  cmd.append(\"-d\")\n  cmd.append(os.path.join(sourcedir, \"kernel\")+\":\"+ramdisk_img.name)\n  cmd.append(img.name)\n\nelse:\n  cmd = [\"mkbootimg\", \"--kernel\", os.path.join(sourcedir, \"kernel\")]\n\n  fn = os.path.join(sourcedir, \"cmdline\")\n  if os.access(fn, os.F_OK):\n    cmd.append(\"--cmdline\")\n    cmd.append(open(fn).read().rstrip(\"\\n\"))\n\n  fn = os.path.join(sourcedir, \"base\")\n  if os.access(fn, os.F_OK):\n    cmd.append(\"--base\")\n    cmd.append(open(fn).read().rstrip(\"\\n\"))\n\n  fn = os.path.join(sourcedir, \"pagesize\")\n  if os.access(fn, os.F_OK):\n    cmd.append(\"--pagesize\")\n    cmd.append(open(fn).read().rstrip(\"\\n\"))\n\n  cmd.extend([\"--ramdisk\", ramdisk_img.name,\n              \"--output\", img.name])\n\np = Run(cmd, stdout=subprocess.PIPE)\np.communicate()\nassert p.returncode == 0, \"mkbootimg of %s image failed\" % (\n    os.path.basename(sourcedir),)\n\nimg.seek(os.SEEK_SET, 0)\ndata = img.read()\n\nramdisk_img.close()\nimg.close()\n\nreturn data", "path": "releasetools\\triumph_common.py", "repo_name": "ikarosdev/android_device_motorola_triumph", "stars": 20, "license": "None", "language": "python", "size": 42272}
{"docstring": "\"\"\"Write a comment into the update script.\"\"\"\n", "func_signal": "def Comment(self, comment):\n", "code": "self.script.append(\"\")\nfor i in comment.split(\"\\n\"):\n  self.script.append(\"# \" + i)\nself.script.append(\"\")", "path": "releasetools\\triumph_edify_generator.py", "repo_name": "ikarosdev/android_device_motorola_triumph", "stars": 20, "license": "None", "language": "python", "size": 42272}
{"docstring": "\"\"\" Gmake in MAC OS has file descriptor (PIPE) leak. We close those fds\nbefore doing other work.\"\"\"\n", "func_signal": "def CloseInheritedPipes():\n", "code": "if platform.system() != \"Darwin\":\n  return\nfor d in range(3, 1025):\n  try:\n    stat = os.fstat(d)\n    if stat is not None:\n      pipebit = stat[0] & 0x1000\n      if pipebit != 0:\n        os.close(d)\n  except OSError:\n    pass", "path": "releasetools\\triumph_common.py", "repo_name": "ikarosdev/android_device_motorola_triumph", "stars": 20, "license": "None", "language": "python", "size": 42272}
{"docstring": "\"\"\"Apply binary patches (in *patchpairs) to the given srcfile to\nproduce tgtfile (which may be \"-\" to indicate overwriting the\nsource file.\"\"\"\n", "func_signal": "def ApplyPatch(self, srcfile, tgtfile, tgtsize, tgtsha1, *patchpairs):\n", "code": "if len(patchpairs) % 2 != 0 or len(patchpairs) == 0:\n  raise ValueError(\"bad patches given to ApplyPatch\")\ncmd = ['apply_patch(\"%s\",\\0\"%s\",\\0%s,\\0%d'\n       % (srcfile, tgtfile, tgtsha1, tgtsize)]\nfor i in range(0, len(patchpairs), 2):\n  cmd.append(',\\0%s, package_extract_file(\"%s\")' % patchpairs[i:i+2])\ncmd.append(');')\ncmd = \"\".join(cmd)\nself.script.append(self._WordWrap(cmd))", "path": "releasetools\\triumph_edify_generator.py", "repo_name": "ikarosdev/android_device_motorola_triumph", "stars": 20, "license": "None", "language": "python", "size": 42272}
{"docstring": "\"\"\"Unzip the given archive into a temporary directory and return the name.\"\"\"\n\n", "func_signal": "def UnzipTemp(filename, pattern=None):\n", "code": "tmp = tempfile.mkdtemp(prefix=\"targetfiles-\")\nOPTIONS.tempfiles.append(tmp)\ncmd = [\"unzip\", \"-o\", \"-q\", filename, \"-d\", tmp]\nif pattern is not None:\n  cmd.append(pattern)\np = Run(cmd, stdout=subprocess.PIPE)\np.communicate()\nif p.returncode != 0:\n  raise ExternalError(\"failed to unzip input target-files \\\"%s\\\"\" %\n                      (filename,))\nreturn tmp", "path": "releasetools\\triumph_common.py", "repo_name": "ikarosdev/android_device_motorola_triumph", "stars": 20, "license": "None", "language": "python", "size": 42272}
{"docstring": "\"\"\"Make a temporary script object whose commands can latter be\nappended to the parent script with AppendScript().  Used when the\ncaller wants to generate script commands out-of-order.\"\"\"\n", "func_signal": "def MakeTemporary(self):\n", "code": "x = EdifyGenerator(self.version, self.info)\nx.mounts = self.mounts\nreturn x", "path": "releasetools\\triumph_edify_generator.py", "repo_name": "ikarosdev/android_device_motorola_triumph", "stars": 20, "license": "None", "language": "python", "size": 42272}
{"docstring": "\"\"\"'cmd' should be a function call with null characters after each\nparameter (eg, \"somefun(foo,\\0bar,\\0baz)\").  This function wraps cmd\nto a given line length, replacing nulls with spaces and/or newlines\nto format it nicely.\"\"\"\n", "func_signal": "def _WordWrap(cmd, linelen=80):\n", "code": "indent = cmd.index(\"(\")+1\nout = []\nfirst = True\nx = re.compile(\"^(.{,%d})\\0\" % (linelen-indent,))\nwhile True:\n  if not first:\n    out.append(\" \" * indent)\n  first = False\n  m = x.search(cmd)\n  if not m:\n    parts = cmd.split(\"\\0\", 1)\n    out.append(parts[0]+\"\\n\")\n    if len(parts) == 1:\n      break\n    else:\n      cmd = parts[1]\n      continue\n  out.append(m.group(1)+\"\\n\")\n  cmd = cmd[m.end():]\n\nreturn \"\".join(out).replace(\"\\0\", \" \").rstrip(\"\\n\")", "path": "releasetools\\triumph_edify_generator.py", "repo_name": "ikarosdev/android_device_motorola_triumph", "stars": 20, "license": "None", "language": "python", "size": 42272}
{"docstring": "\"\"\"Call the named function in the device-specific module, passing\nthe given args and kwargs.  The first argument to the call will be\nthe DeviceSpecific object itself.  If there is no module, or the\nmodule does not define the function, return the value of the\n'default' kwarg (which itself defaults to None).\"\"\"\n", "func_signal": "def _DoCall(self, function_name, *args, **kwargs):\n", "code": "if self.module is None or not hasattr(self.module, function_name):\n  return kwargs.get(\"default\", None)\nreturn getattr(self.module, function_name)(*((self,) + args), **kwargs)", "path": "releasetools\\triumph_common.py", "repo_name": "ikarosdev/android_device_motorola_triumph", "stars": 20, "license": "None", "language": "python", "size": 42272}
{"docstring": "\"\"\"Prompt the user to enter a value (password) for each key in\n'current' whose value is fales.  Returns a new dict with all the\nvalues.\n\"\"\"\n", "func_signal": "def PromptResult(self, current):\n", "code": "result = {}\nfor k, v in sorted(current.iteritems()):\n  if v:\n    result[k] = v\n  else:\n    while True:\n      result[k] = getpass.getpass(\"Enter password for %s key> \"\n                                  % (k,)).strip()\n      if result[k]: break\nreturn result", "path": "releasetools\\triumph_common.py", "repo_name": "ikarosdev/android_device_motorola_triumph", "stars": 20, "license": "None", "language": "python", "size": 42272}
{"docstring": "\"\"\"\nReturn the name of the form for the request or None.\n\"\"\"\n", "func_signal": "def form_in_request(request):\n", "code": "if request.method == 'POST':\n    return request.POST.get('__formish_form__')\nif request.method == 'GET':\n    return request.GET.get('__formish_form__')\nreturn None", "path": "formish\\util.py", "repo_name": "ish/formish", "stars": 16, "license": "other", "language": "python", "size": 3882}
{"docstring": "\"\"\"\n:arg options: either a list of values ``[value,]`` where value is used for the label or a list of tuples of the form ``[(value, label),]``\n:arg none_option: a tuple of ``(value, label)`` to use as the unselected option\n:arg css_class: a css class to apply to the field\n\"\"\"\n", "func_signal": "def __init__(self, options, **k):\n", "code": "none_option = k.pop('none_option', UNSET)\nif none_option is not UNSET:\n    self.none_option = none_option\nif self.none_option and self.none_option[0] and 'none_value' not in k:\n    k['none_value'] = self.none_option[0]\nelse:\n    k['none_value'] = ''\nWidget.__init__(self, **k)\nself.options = _normalise_options(options)", "path": "formish\\widgets.py", "repo_name": "ish/formish", "stars": 16, "license": "other", "language": "python", "size": 3882}
{"docstring": "\"\"\"\nWSGI application wrapper factory for extending the WSGI environ with\napplication-specific keys.\n\"\"\"\n\n# Create any objects that should exist for the lifetime of the application\n# here. Don't forget to actually include them in the environ though! For\n# example:\n", "func_signal": "def setup_environ(app, global_conf, app_conf):\n", "code": "renderer = MakoRenderer(\n    directories=[pkg_resources.resource_filename('testish', 'templates'),\n    pkg_resources.resource_filename('formish', 'templates/mako')],\n    module_directory=os.path.join(app_conf['cache_dir'], 'templates'),\n    input_encoding='utf-8', output_encoding='utf-8',\n    default_filters=['unicode', 'h'])\n\ndef application(environ, start_response):\n\n    # Add additional keys to the environ here. For example:\n    #\n    environ['restish.templating'] = Templating(renderer)\n\n    return app(environ, start_response)\n\nreturn application", "path": "formish\\tests\\testish\\testish\\wsgiapp.py", "repo_name": "ish/formish", "stars": 16, "license": "other", "language": "python", "size": 3882}
{"docstring": "\"\"\"\nReturn a sequence of (value, label) pairs for all options where each option\ncan be a scalar value or a (value, label) tuple.\n\"\"\"\n", "func_signal": "def _normalise_options(options):\n", "code": "out = []\nif hasattr(options, '__call__'):\n    options = options()\nfor option in options:\n    if isinstance(option, tuple):\n        out.append( option )\n    else:\n        out.append( (option, str(option)) )\nreturn out", "path": "formish\\widgets.py", "repo_name": "ish/formish", "stars": 16, "license": "other", "language": "python", "size": 3882}
{"docstring": "\"\"\"\nSome widgets (at the moment only files) need to have their data massaged in\norder to make sure that data->request and request->data is symmetric\n\nThis pre parsing is a null operation for most widgets\n\"\"\"\n", "func_signal": "def pre_parse_incoming_request_data(self, field, request_data):\n", "code": "data = {}\n\nfor f in field.fields:\n    if request_data is None:\n        r = None\n    else:\n        r = request_data.get(f.nodename, None)\n    d = f.widget.pre_parse_incoming_request_data(f, r)\n    data[f.nodename] = d\nreturn data", "path": "formish\\widgets.py", "repo_name": "ish/formish", "stars": 16, "license": "other", "language": "python", "size": 3882}
{"docstring": "\"\"\"\nafter the form has been submitted, the request data is converted into\nto the schema type.\n\"\"\"\n", "func_signal": "def from_request_data(self, field, request_data):\n", "code": "if request_data == self.none_value_as_request_data(field):\n    data = self.empty\nelse:\n    data = string_converter(field.attr).to_type(request_data[0], converter_options=self.converter_options)\nreturn data", "path": "formish\\widgets.py", "repo_name": "ish/formish", "stars": 16, "license": "other", "language": "python", "size": 3882}
{"docstring": "\"\"\"\nWe use the url factory to get an identifier for the file which we use\nas the name. We also store it in the 'default' field so we can check if\nsomething has been uploaded (the identifier doesn't match the name)\n\"\"\"\n", "func_signal": "def to_request_data(self, field, data):\n", "code": "filename = ''\nmimetype = ''\nif isinstance(data, SchemaFile):\n    default = util.encode_file_resource_path(None, self.url_ident_factory(data))\n    mimetype = data.mimetype\n    filename = data.filename\nelif data is not None:\n    default = util.encode_file_resource_path(None, data)\nelse:\n    default = ''\nreturn {'name': [default], 'default':[default], 'mimetype':[mimetype],\n        'filename': [filename]}", "path": "formish\\widgets.py", "repo_name": "ish/formish", "stars": 16, "license": "other", "language": "python", "size": 3882}
{"docstring": "\"\"\"\nBefore the widget is rendered, the data is converted to a string\nformat.If the data is None then we return an empty string. The sequence\nis request data representation.\n\"\"\"\n", "func_signal": "def to_request_data(self, field, data):\n", "code": "if data is None or (data == self.empty and self.roundtrip_empty is True):\n    return self.none_value_as_request_data(field)\nstring_data = string_converter(field.attr).from_type(data, converter_options=self.converter_options)\nreturn [string_data]", "path": "formish\\widgets.py", "repo_name": "ish/formish", "stars": 16, "license": "other", "language": "python", "size": 3882}
{"docstring": "\"\"\"\nEncode a file store name and key to a file resource path.\n\"\"\"\n", "func_signal": "def encode_file_resource_path(name, key):\n", "code": "if name:\n    path = '@%s/%s' % (name, key)\nelse:\n    if key[:1] == '@':\n        path = ''.join(['@@', key[1:]])\n    else:\n        path = key\nreturn urllib.quote(path, '/@')", "path": "formish\\util.py", "repo_name": "ish/formish", "stars": 16, "license": "other", "language": "python", "size": 3882}
{"docstring": "\"\"\"\nReturn all of the options for the widget\n\"\"\"\n", "func_signal": "def get_options(self, field):\n", "code": "options = []\nfor value, label in self.options:\n    options.append( (string_converter(field.attr).from_type(value),label) )\nreturn options", "path": "formish\\widgets.py", "repo_name": "ish/formish", "stars": 16, "license": "other", "language": "python", "size": 3882}
{"docstring": "\"\"\"\nIterate over the data, converting each one\n\"\"\"\n", "func_signal": "def to_request_data(self, field, data):\n", "code": "if data is None:\n    data = []\nreturn [string_converter(field.attr.attr).from_type(d) for d in data]", "path": "formish\\widgets.py", "repo_name": "ish/formish", "stars": 16, "license": "other", "language": "python", "size": 3882}
{"docstring": "\"\"\"\nCheck the value passed matches the actual value\n\"\"\"\n\n# Don't mark additional none option as selected.\n", "func_signal": "def selected(self, option, field):\n", "code": "if self.none_option and option[0] == self.none_option[0]:\n    return ''\n\nif option[0] == field.value[0] and option[0] != self.empty:\n    return ' selected=\"selected\"'\nelse:\n    return ''", "path": "formish\\widgets.py", "repo_name": "ish/formish", "stars": 16, "license": "other", "language": "python", "size": 3882}
{"docstring": "\"\"\"\nFor each value, convert it and check to see if it matches the input data\n\"\"\"\n", "func_signal": "def checked(self, field):\n", "code": "if field.value and field.value[0] and string_converter(field.attr).to_type(field.value[0]) == self.checked_value:\n    return ' checked=\"checked\"'\nelse:\n    return ''", "path": "formish\\widgets.py", "repo_name": "ish/formish", "stars": 16, "license": "other", "language": "python", "size": 3882}
{"docstring": "\"\"\"\nFile uploads are wierd; in out case this means assymetric. We store the\nfile in a temporary location and just store an identifier in the field.\nThis at least makes the file look symmetric.\n\"\"\"\n", "func_signal": "def pre_parse_incoming_request_data(self, field, data):\n", "code": "if data is None:\n    data = {}\nif data.get('remove', [None])[0] is not None:\n    data['name'] = ['']\n    data['mimetype'] = ['']\n    return data\n\nfieldstorage = data.get('file', [''])[0]\nif getattr(fieldstorage,'file',None):\n    # XXX Can we reuse the key from a previous temp upload to avoid\n    # creating an additional temp file?\n    key = uuid.uuid4().hex\n    cache_tag = uuid.uuid4().hex\n    self.filestore.put(key, fieldstorage.file, cache_tag,\n                       [('Content-Type', fieldstorage.type),\n                        ('Filename', fieldstorage.filename)])\n    data['name'] = [util.encode_file_resource_path('tmp', key)]\n    data['mimetype'] = [fieldstorage.type]\nreturn data", "path": "formish\\widgets.py", "repo_name": "ish/formish", "stars": 16, "license": "other", "language": "python", "size": 3882}
{"docstring": "\"\"\"\npopulate the other choice if needed\n\"\"\"\n", "func_signal": "def to_request_data(self, field, data):\n", "code": "string_data = string_converter(field.attr).from_type(data)\nif string_data is None:\n    return self.none_value_as_request_data(field)\nif string_data in [value for value, label in self.options]:\n    return {'select': [string_data], 'other': ['']}\nreturn {'select': [self.other_option[0]], 'other': [string_data]}", "path": "formish\\widgets.py", "repo_name": "ish/formish", "stars": 16, "license": "other", "language": "python", "size": 3882}
{"docstring": "\"\"\"\nSome widgets (at the moment only files) need to have their data massaged in\norder to make sure that data->request and request->data is symmetric\n\nThis pre parsing is a null operation for most widgets\n\"\"\"\n", "func_signal": "def pre_parse_incoming_request_data(self, field, request_data):\n", "code": "data = {}\n\nfor f in field.fields:\n    try:\n        r = request_data[int(f.nodename)]\n    except (TypeError, KeyError):\n        r = None\n    d = f.widget.pre_parse_incoming_request_data(f, r)\n    data[f.nodename] = d\n\nreturn data", "path": "formish\\widgets.py", "repo_name": "ish/formish", "stars": 16, "license": "other", "language": "python", "size": 3882}
{"docstring": "\"\"\"\nrecursively applies ``convert_sequences``\n\"\"\"\n", "func_signal": "def recursive_convert_sequences(data):\n", "code": "if not hasattr(data,'keys'):\n    return data\nif len(data.keys()) == 0:\n    return data\ntry:\n    int(data.keys()[0])\nexcept ValueError:\n    tmp = {}\n    for key, value in data.items():\n        tmp[key] = recursive_convert_sequences(value)\n    return tmp\nintkeys = []\nfor key in data.keys():\n    intkeys.append(int(key))\nintkeys.sort()\nout = []\nfor key in intkeys:\n    out.append(recursive_convert_sequences(data[str(key)]))\nreturn out", "path": "formish\\widgets.py", "repo_name": "ish/formish", "stars": 16, "license": "other", "language": "python", "size": 3882}
{"docstring": "\"\"\"\nBefore the widget is rendered, the data is converted to a string\nformat.If the data is None then we return an empty string. The sequence\nis request data representation.\n\"\"\"\n", "func_signal": "def to_request_data(self, field, data):\n", "code": "if data is None or (data == self.empty and self.roundtrip_empty is True):\n    return self.none_value_as_request_data(field)\nstring_data = string_converter(field.attr).from_type(data, converter_options=self.converter_options)\nreturn [string_data]", "path": "formish\\widgets.py", "repo_name": "ish/formish", "stars": 16, "license": "other", "language": "python", "size": 3882}
{"docstring": "\"\"\"\nConvert to date parts\n\"\"\"\n", "func_signal": "def to_request_data(self, field, data):\n", "code": "dateparts = datetuple_converter(field.attr).from_type(data)\nif dateparts is None:\n    return self.none_value_as_request_data(field)\nreturn {'year': [dateparts[0]],\n        'month': [dateparts[1]],\n        'day': [dateparts[2]]}", "path": "formish\\widgets.py", "repo_name": "ish/formish", "stars": 16, "license": "other", "language": "python", "size": 3882}
{"docstring": "\"\"\"\nCreate a title from an attribute name.\n\"\"\"\n", "func_signal": "def title_from_name(name):\n", "code": "def _():\n    \"\"\"\n    Generator to convert parts of title\n    \"\"\"\n    try:\n        int(name)\n        yield 'Item #%s'% name\n        return\n    except ValueError:\n        pass\n\n    it = iter(name)\n    last = None\n\n    while 1:\n        ch = it.next()\n        if ch == '_':\n            if last != '_':\n                yield ' '\n        elif last in (None,'_'):\n            yield ch.upper()\n        elif ch.isupper() and not last.isupper():\n            yield ' '\n            yield ch.upper()\n        else:\n            yield ch\n        last = ch\nreturn ''.join(_())", "path": "formish\\util.py", "repo_name": "ish/formish", "stars": 16, "license": "other", "language": "python", "size": 3882}
{"docstring": "\"\"\"Trains the tagger from the tagged sentences provided\n\"\"\"\n", "func_signal": "def train(self, sentence_list):\n", "code": "noun_fallback = DefaultTagger('NN')\naffix_fallback = AffixTagger(sentence_list,\n    backoff=noun_fallback)\nunigram_fallback = UnigramTagger(sentence_list,\n    backoff=affix_fallback)\nbigram_fallback = BigramTagger(sentence_list,\n    backoff=unigram_fallback)\ntrigram_fallback = TrigramTagger(sentence_list,\n    backoff=bigram_fallback)\ntemplates = [\n    brill.SymmetricProximateTokensTemplate(\n        brill.ProximateTagsRule, (1, 1)),\n    brill.SymmetricProximateTokensTemplate(\n        brill.ProximateTagsRule, (2, 2)),\n    brill.SymmetricProximateTokensTemplate(\n        brill.ProximateTagsRule, (1, 2)),\n    brill.SymmetricProximateTokensTemplate(\n        brill.ProximateTagsRule, (1, 3)),\n    brill.SymmetricProximateTokensTemplate(\n        brill.ProximateWordsRule, (1, 1)),\n    brill.SymmetricProximateTokensTemplate(\n        brill.ProximateWordsRule, (2, 2)),\n    brill.SymmetricProximateTokensTemplate(\n        brill.ProximateWordsRule, (1, 2)),\n    brill.SymmetricProximateTokensTemplate(\n        brill.ProximateWordsRule, (1, 3)),\n    brill.ProximateTokensTemplate(\n        brill.ProximateTagsRule, (-1, -1), (1, 1)),\n    brill.ProximateTokensTemplate(\n        brill.ProximateWordsRule, (-1, -1), (1, 1))]\n\ntrainer = brill.FastBrillTaggerTrainer(trigram_fallback, templates)\nself.tagger = trainer.train(sentence_list, max_rules=100, min_score=3)", "path": "src\\collective\\classification\\taggers\\taggers.py", "repo_name": "ggozad/collective.classification", "stars": 25, "license": "None", "language": "python", "size": 1355}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def action_clusterize(self, action, data):\n", "code": "catalog = getToolByName(self.context, 'portal_catalog')\nclusterer = KMeans()\nclusters = clusterer.clusterize(\n    data['no_clusters'],\n    data['no_noun_ranks'],\n    repeats=data['repeats'])\nresult = []\nfor cluster in clusters.values():\n    clusterlist = []\n    for uid in cluster:\n        item = catalog.unrestrictedSearchResults(UID=uid)[0]\n        clusterlist.append(\n            (item.getURL(),\n             item.Title,\n             item.Description))\n    result.append(clusterlist)\nself.clusters = result", "path": "src\\collective\\classification\\browser\\clusterize.py", "repo_name": "ggozad/collective.classification", "stars": 25, "license": "None", "language": "python", "size": 1355}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def classify(self, doc_id):\n", "code": "if not self.classifier:\n    return []\npresentNouns = dict()\ncatalog = getToolByName(self, 'portal_catalog')\nallNouns = catalog.uniqueValuesFor('noun_terms')\nfor item in allNouns:\n    presentNouns.setdefault(item, 0)\n\nresults = catalog.unrestrictedSearchResults(UID=doc_id)\nif not results:\n    return []\nimportantNouns = results[0]['noun_terms']\nfor noun in importantNouns:\n    if noun in presentNouns.keys():\n        presentNouns[noun] = 1\nreturn self.classifier.classify(presentNouns)", "path": "src\\collective\\classification\\classifiers\\nounbayesclassifier.py", "repo_name": "ggozad/collective.classification", "stars": 25, "license": "None", "language": "python", "size": 1355}
{"docstring": "\"\"\"Tests the n-gram tagger.\n\"\"\"\n", "func_signal": "def test_ngram_tagger(self):\n", "code": "tagger = TriGramTagger()\ntagger.train(self.tagged_sents)\nself.failUnless(tagger.tag(self.tokens) ==\n    [('The', 'AT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'),\n     ('jumped', 'VBD'), ('over', 'RP'), ('the', 'AT'), ('lazy', 'JJ'),\n     ('dog', 'NN'), ('.', '.')])", "path": "src\\collective\\classification\\tests\\test_tagger.py", "repo_name": "ggozad/collective.classification", "stars": 25, "license": "None", "language": "python", "size": 1355}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def __init__(self, tagger=None):\n", "code": "self.classifier = None\nself.trainAfterUpdate = True", "path": "src\\collective\\classification\\classifiers\\nounbayesclassifier.py", "repo_name": "ggozad/collective.classification", "stars": 25, "license": "None", "language": "python", "size": 1355}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def train(self):\n", "code": "catalog = getToolByName(self, 'portal_catalog')\npresentNouns = dict()\ntrainingData = []\nallNouns = catalog.uniqueValuesFor('noun_terms')\nfor item in allNouns:\n    presentNouns.setdefault(item, 0)\n\nsubjectIndex = catalog._catalog.getIndex('Subject')\nnounTermsIndex = catalog._catalog.getIndex('noun_terms')\n\n# The internal catalog ids of the objects\n# that have noun terms in the catalog\nnounTermIndexIds = IISet(nounTermsIndex._unindex.keys())\n\n# The internal catalog ids of the objects\n# that have subjects in the catalog\nsubjectIndexIds = IISet(subjectIndex._unindex.keys())\ncommonIds = intersection(subjectIndexIds, nounTermIndexIds)\n\nfor cid in commonIds:\n    nounPresence = presentNouns.copy()\n    nouns = nounTermsIndex._unindex[cid]\n    tags = subjectIndex._unindex[cid]\n    for noun in nouns:\n        nounPresence[noun] = 1\n    for tag in tags:\n        trainingData.append((nounPresence, tag, ))\nif trainingData:\n    self.classifier = NaiveBayesClassifier.train(trainingData)", "path": "src\\collective\\classification\\classifiers\\nounbayesclassifier.py", "repo_name": "ggozad/collective.classification", "stars": 25, "license": "None", "language": "python", "size": 1355}
{"docstring": "\"\"\"Tests the default english tagger shipped with\ncollective.classification\n\"\"\"\n", "func_signal": "def test_default_english_tagger(self):\n", "code": "tagger = getUtility(IPOSTagger, name=\"en\")\nself.failUnless(tagger.tag(self.tokens) ==\n    [('The', 'DT'), ('quick', 'JJ'), ('brown', 'VBN'), ('fox', 'NN'),\n     ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'NN'),\n     ('dog', 'NN'), ('.', '.')])\n# Test normalization\nself.failUnless(tagger.normalize(\"axes\", \"NNS\") == \"axis\")", "path": "src\\collective\\classification\\tests\\test_tagger.py", "repo_name": "ggozad/collective.classification", "stars": 25, "license": "None", "language": "python", "size": 1355}
{"docstring": "\"\"\"Setup the tokenized test text and select the training set for the\nn-gram tagger.\n\"\"\"\n", "func_signal": "def setUp(self):\n", "code": "tokenizer = getUtility(ITokenizer, name=\"en\")\ntext = \"The quick brown fox jumped over the lazy dog.\"\nself.tokens = tokenizer.tokenize(text)\nself.tagged_sents = brown.tagged_sents(categories='news')", "path": "src\\collective\\classification\\tests\\test_tagger.py", "repo_name": "ggozad/collective.classification", "stars": 25, "license": "None", "language": "python", "size": 1355}
{"docstring": "\"\"\"Tests the Pen TreeBank tagger.\n\"\"\"\n", "func_signal": "def test_pentreebank_tagger(self):\n", "code": "tagger = PennTreebankTagger()\nself.failUnless(tagger.tag(self.tokens) ==\n    [('The', 'DT'), ('quick', 'NN'), ('brown', 'NN'), ('fox', 'NN'),\n     ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'NN'),\n     ('dog', 'NN'), ('.', '.')])", "path": "src\\collective\\classification\\tests\\test_tagger.py", "repo_name": "ggozad/collective.classification", "stars": 25, "license": "None", "language": "python", "size": 1355}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def extract(self, text, locale='en'):\n", "code": "tokenizer = queryUtility(ITokenizer, name=locale)\ntagger = queryUtility(IPOSTagger, name=locale)\nif not tagger or not tokenizer:\n    #Non-supported language\n    return\ntokens = tokenizer.tokenize(text)\ntagged_terms = tagger.tag(tokens)\nterms = {}\nnp_terms = {}\nnoun_phrases = [\n    node\n    for node in tagger.np_grammar.parse(tagged_terms)\n    if not isinstance(node, tuple)]\nfor node in noun_phrases:\n    coll_tag = tree2conlltags(node)\n    if len(coll_tag) > 1:\n        mterm = [\n            term.lower()\n            for (term, tag, temp) in coll_tag\n            if len(term)>1]\n        mterm = ' '.join(mterm)\n        if mterm:\n            self._add(mterm, np_terms)\n    for (term, tag, temp) in coll_tag:\n        if tag.startswith('N') and len(term)>1:\n            term = tagger.normalize(term, tag)\n            self._add(term.lower(), terms)\nfor term in terms.keys():\n    if not self.filter(term, terms[term]):\n        del terms[term]\nfor term in np_terms.keys():\n    if not self.filter(term, np_terms[term]):\n        del np_terms[term]\nreturn (terms, np_terms)", "path": "src\\collective\\classification\\classifiers\\npextractor.py", "repo_name": "ggozad/collective.classification", "stars": 25, "license": "None", "language": "python", "size": 1355}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def tag(self, words):\n", "code": "if not self.tagger:\n    raise Exception(\"Brill Tagger not trained.\")\nreturn self.tagger.tag(words)", "path": "src\\collective\\classification\\taggers\\taggers.py", "repo_name": "ggozad/collective.classification", "stars": 25, "license": "None", "language": "python", "size": 1355}
{"docstring": "\"\"\" return a file object from the test data folder \"\"\"\n", "func_signal": "def getFile(filename):\n", "code": "filename = join(dirname(tests.__file__), 'data', filename)\nreturn open(filename, 'r')", "path": "src\\collective\\classification\\tests\\util.py", "repo_name": "ggozad/collective.classification", "stars": 25, "license": "None", "language": "python", "size": 1355}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def action_apply(self, action, data):\n", "code": "self.informativeFeatures = \\\n    self.classifier.informativeFeatures(data['no_features'])", "path": "src\\collective\\classification\\browser\\classificationstats.py", "repo_name": "ggozad/collective.classification", "stars": 25, "license": "None", "language": "python", "size": 1355}
{"docstring": "\"\"\"Reads a review of Alice in Wonderland and extracts the most\nfrequent nouns found in the text as well as the most frequent\n'noun phrases'.\n\"\"\"\n\n", "func_signal": "def test_extractor(self):\n", "code": "text = readData('alicereview.txt')\nextractor = getUtility(ITermExtractor)\n(simple_terms, np_terms) = extractor.extract(text, locale=\"en\")\n\ntop_5_nouns = sorted(\n    simple_terms.items(),\n    key = itemgetter(1),\n    reverse=True)[:5]\ntop_5_nouns = [term for (term, rank) in top_5_nouns]\nfor word in ['alice', 'rabbit', 'hatter', 'door', 'cat']:\n    self.failUnless(word in top_5_nouns)\n\ntop_5_nps = sorted(\n    np_terms.items(),\n    key = itemgetter(1),\n    reverse=True)[:5]\ntop_5_nps = [term for (term, rank) in top_5_nps]\nfor np in ['white rabbit', 'mock turtle', 'mad hatter', 'march hare']:\n    self.failUnless(np in top_5_nps)", "path": "src\\collective\\classification\\tests\\test_extractor.py", "repo_name": "ggozad/collective.classification", "stars": 25, "license": "None", "language": "python", "size": 1355}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def probabilityClassify(self, doc_id):\n", "code": "if not self.classifier:\n    return []\npresentNouns = dict()\ncatalog = getToolByName(self, 'portal_catalog')\nallNouns = catalog.uniqueValuesFor('noun_terms')\nfor item in allNouns:\n    presentNouns.setdefault(item, 0)\n\nresults = catalog.unrestrictedSearchResults(UID=doc_id)\nif not results:\n    return []\nimportantNouns = results[0]['noun_terms']\nfor noun in importantNouns:\n    if noun in presentNouns.keys():\n        presentNouns[noun] = 1\nreturn self.classifier.prob_classify(presentNouns)", "path": "src\\collective\\classification\\classifiers\\nounbayesclassifier.py", "repo_name": "ggozad/collective.classification", "stars": 25, "license": "None", "language": "python", "size": 1355}
{"docstring": "\"\"\"Creates an IATContentType and tests the catalog index\n\"\"\"\n", "func_signal": "def test_indexer(self):\n", "code": "text = readData('alicereview.txt')\nself.folder.invokeFactory('Document', 'test',\n                          text=text,\n                          subject=\"A Subject\")\ncatalog = getToolByName(self.folder, 'portal_catalog')\ncr = catalog.searchResults(noun_terms=\"alice\")\nself.failUnless(cr[0]['noun_terms'][:5] ==\n    ['alice', 'rabbit', 'door', 'cat', 'hatter'])\nself.failUnless(cr[0]['noun_phrase_terms'][:5] ==\n    ['mock turtle', 'white rabbit', 'march hare', 'mad hatter'])", "path": "src\\collective\\classification\\tests\\test_indexer.py", "repo_name": "ggozad/collective.classification", "stars": 25, "license": "None", "language": "python", "size": 1355}
{"docstring": "# Simple sums\n", "func_signal": "def pearson(v1, v2):\n", "code": "sum1=sum(v1)\nsum2=sum(v2)\n# Sums of the squares\nsum1Sq=sum([pow(v, 2) for v in v1])\nsum2Sq=sum([pow(v, 2) for v in v2])\n# Sum of the products\npSum=sum([v1[i]*v2[i] for i in range(len(v1))])\n# Calculate r (Pearson score)\nnum=pSum-(sum1*sum2/len(v1))\nden=sqrt((sum1Sq-pow(sum1, 2)/len(v1))*(sum2Sq-pow(sum2, 2)/len(v1)))\nif den==0:\n    return 0\nreturn 1.0-num/den", "path": "src\\collective\\classification\\classifiers\\utils.py", "repo_name": "ggozad/collective.classification", "stars": 25, "license": "None", "language": "python", "size": 1355}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "super(ClassificationStatsView, self).__init__(*args,**kwargs)\ncatalog = getToolByName(self.context, 'portal_catalog')\nself.classifier = getUtility(IContentClassifier)\nself.informativeFeatures = self.classifier.informativeFeatures()\nself.parsedDocs = len(catalog._catalog.getIndex('noun_terms')._unindex)", "path": "src\\collective\\classification\\browser\\classificationstats.py", "repo_name": "ggozad/collective.classification", "stars": 25, "license": "None", "language": "python", "size": 1355}
{"docstring": "\"\"\"Tests the default english tagger shipped with\ncollective.classification\n\"\"\"\n\n", "func_signal": "def test_default_dutch_tagger(self):\n", "code": "tokenizer = getUtility(ITokenizer, name=\"nl\")\ntext = \"De snelle bruine vos sprong over de luie hond.\"\ntokens = tokenizer.tokenize(text)\ntagger = getUtility(IPOSTagger, name=\"nl\")\nself.failUnless(tagger.tag(tokens) ==\n    [('De', 'DET'), ('snelle', 'ADJ'), ('bruine', 'ADJ'),\n     ('vos', 'N'), ('sprong', 'V'), ('over', 'P'), ('de', 'DET'),\n     ('luie', 'NN'), ('hond', 'N'), ('.', '.')])", "path": "src\\collective\\classification\\tests\\test_tagger.py", "repo_name": "ggozad/collective.classification", "stars": 25, "license": "None", "language": "python", "size": 1355}
{"docstring": "\"\"\"Determines and returns the most relevant features\n\"\"\"\n", "func_signal": "def informativeFeatures(self, n=10):\n", "code": "if not self.classifier:\n    return []\ncpdist = self.classifier._feature_probdist\nresult = []\nfor (fname, fval) in self.classifier.most_informative_features(n):\n\n    def labelprob(l):\n        return cpdist[l, fname].prob(fval)\n    labels = sorted([l for l in self.classifier._labels\n                     if fval in cpdist[l, fname].samples()],\n                    key=labelprob)\n    if len(labels) == 1:\n        continue\n    l0 = labels[0]\n    l1 = labels[-1]\n    if cpdist[l0, fname].prob(fval) == 0:\n        ratio = 'INF'\n    else:\n        ratio = '%8.1f' % (cpdist[l1, fname].prob(fval) /\n                           cpdist[l0, fname].prob(fval))\n    result.append((fname, bool(fval), l1, l0, ratio))\nreturn result", "path": "src\\collective\\classification\\classifiers\\nounbayesclassifier.py", "repo_name": "ggozad/collective.classification", "stars": 25, "license": "None", "language": "python", "size": 1355}
{"docstring": "\"\"\"Real object initialisation is made here, because now we've got the arguments.\"\"\"\n", "func_signal": "def __call__(self, *args, **kwargs) :\n", "code": "if not self._initdone :\n    self.__class__._number = self.__class__._number + 1\n    methodname = self._postinit(*args,**kwargs)\n    self._parent._PyWrite(\"\\n    # create PDF%sObject number %i\\n    %s = %s.%s(%s)\" % (methodname[5:], self.__class__._number, self._name, self._parent._name, methodname, buildargs(*args,**kwargs)))\n    self._initdone = 1\nreturn self", "path": "src\\reportlab\\pdfgen\\pycanvas.py", "repo_name": "mattjmorrison/ReportLab", "stars": 25, "license": "bsd-3-clause", "language": "python", "size": 2060}
{"docstring": "\"\"\"Initialize and begins source code.\"\"\"\n", "func_signal": "def __init__(self, *args, **kwargs) :\n", "code": "self._parent = self     # nice trick, isn't it ?\nself._in = 0\nself._contextlevel = 0\nself._pagenumber = 1\nself._formnumber = 0\nself._footerpresent = 0\nself._object = canvas.Canvas(*args,**kwargs)\nself._enforceColorSpace = self._object._enforceColorSpace\nself._pyfile = cStringIO.StringIO()\nself._PyWrite(PyHeader)\ntry :\n    del kwargs[\"filename\"]\nexcept KeyError :\n    pass\nself._PyWrite(\"    # create the PDF document\\n    %s = Canvas(file, %s)\\n\\n    # Begins page 1\" % (self._name, buildargs(*args[1:], **kwargs)))", "path": "src\\reportlab\\pdfgen\\pycanvas.py", "repo_name": "mattjmorrison/ReportLab", "stars": 25, "license": "bsd-3-clause", "language": "python", "size": 2060}
{"docstring": "#loads one\n", "func_signal": "def start_drawing(self, args):\n", "code": "moduleName = args[\"module\"]\nfuncName = args[\"constructor\"]\nshowBoundary = int(args.get(\"showBoundary\", \"0\"))\nhAlign = args.get(\"hAlign\", \"CENTER\")\n\n\n# the path for the imports should include:\n# 1. document directory\n# 2. python path if baseDir not given, or\n# 3. baseDir if given\ntry:\n    dirName = sdict[\"baseDir\"]\nexcept:\n    dirName = None\nimportPath = [os.getcwd()]\nif dirName is None:\n    importPath.extend(sys.path)\nelse:\n    importPath.insert(0, dirName)\n\nmodul = recursiveImport(moduleName, baseDir=importPath)\nfunc = getattr(modul, funcName)\ndrawing = func()\n\ndrawing.hAlign = hAlign\nif showBoundary:\n    drawing._showBoundary = 1\n\nself._curDrawing = pythonpoint.PPDrawing()\nself._curDrawing.drawing = drawing", "path": "tools\\pythonpoint\\stdparser.py", "repo_name": "mattjmorrison/ReportLab", "stars": 25, "license": "bsd-3-clause", "language": "python", "size": 2060}
{"docstring": "#just append to current paragraph text, so we can quote XML\n", "func_signal": "def handle_cdata(self, data):\n", "code": "if self._curPara:\n    self._curPara.rawtext = self._curPara.rawtext + data\nelif self._curPrefmt:\n    self._curPrefmt.rawtext = self._curPrefmt.rawtext + data\nelif self._curPyCode:\n    self._curPyCode.rawtext = self._curPyCode.rawtext + data\nelif  self._curString:\n    self._curString.text = self._curString.text + data\nelif self._curTable:\n    self._curTable.rawBlocks.append(data)\nelif self._curAuthor != None:\n    self._curAuthor = self._curAuthor + data\nelif self._curSubject != None:\n    self._curSubject = self._curSubject + data", "path": "tools\\pythonpoint\\stdparser.py", "repo_name": "mattjmorrison/ReportLab", "stars": 25, "license": "bsd-3-clause", "language": "python", "size": 2060}
{"docstring": "\"\"\"Constructs a printable list of arguments suitable for use in source function calls.\"\"\"\n", "func_signal": "def buildargs(*args, **kwargs) :\n", "code": "arguments = \"\"\nfor arg in args :\n    arguments = arguments + (\"%s, \" % repr(arg))\nfor (kw, val) in kwargs.items() :\n    arguments = arguments+ (\"%s=%s, \" % (kw, repr(val)))\nif arguments[-2:] == \", \" :\n    arguments = arguments[:-2]\nreturn arguments", "path": "src\\reportlab\\pdfgen\\pycanvas.py", "repo_name": "mattjmorrison/ReportLab", "stars": 25, "license": "bsd-3-clause", "language": "python", "size": 2060}
{"docstring": "\"\"\"Outputs comments after the method call.\"\"\"\n", "func_signal": "def _postcomment(self) :\n", "code": "if self._action == \"showPage\" :\n    self._parent._pagenumber = self._parent._pagenumber + 1\n    self._parent._PyWrite(\"\\n    # Begins page %i\" % self._parent._pagenumber)\nelif self._action in [ \"endForm\", \"drawPath\", \"clipPath\" ] :\n    self._parent._PyWrite(\"\")", "path": "src\\reportlab\\pdfgen\\pycanvas.py", "repo_name": "mattjmorrison/ReportLab", "stars": 25, "license": "bsd-3-clause", "language": "python", "size": 2060}
{"docstring": "\"\"\"Method or attribute access.\"\"\"\n", "func_signal": "def __getattr__(self, name) :\n", "code": "if name == \"beginPath\" :\n    return self.PathObject(self)\nelif name == \"beginText\" :\n    return self.TextObject(self)\nelse :\n    return self.Action(self, name)", "path": "src\\reportlab\\pdfgen\\pycanvas.py", "repo_name": "mattjmorrison/ReportLab", "stars": 25, "license": "bsd-3-clause", "language": "python", "size": 2060}
{"docstring": "\"\"\"Saves a pointer to the parent Canvas.\"\"\"\n", "func_signal": "def __init__(self, parent) :\n", "code": "self._parent = parent\nself._initdone = 0", "path": "src\\reportlab\\pdfgen\\pycanvas.py", "repo_name": "mattjmorrison/ReportLab", "stars": 25, "license": "bsd-3-clause", "language": "python", "size": 2060}
{"docstring": "\"\"\"Saves a pointer to the parent object, and the method name.\"\"\"\n", "func_signal": "def __init__(self, parent, action) :\n", "code": "self._parent = parent\nself._action = action", "path": "src\\reportlab\\pdfgen\\pycanvas.py", "repo_name": "mattjmorrison/ReportLab", "stars": 25, "license": "bsd-3-clause", "language": "python", "size": 2060}
{"docstring": "#makes it the current style sheet.\n", "func_signal": "def start_stylesheet(self, args):\n", "code": "path = self._arg('stylesheet',args,'path')\nif path=='None': path = []\nif not isSeqType(path): path = [path]\npath.append('styles')\npath.append(os.getcwd())\nmodulename = self._arg('stylesheet', args, 'module')\nfuncname = self._arg('stylesheet', args, 'function')\ntry:\n    found = imp.find_module(modulename, path)\n    (file, pathname, description) = found\n    mod = imp.load_module(modulename, file, pathname, description)\nexcept ImportError:\n    #last gasp\n    mod = getModule(modulename)\n\n#now get the function\nfunc = getattr(mod, funcname)\npythonpoint.setStyles(func())", "path": "tools\\pythonpoint\\stdparser.py", "repo_name": "mattjmorrison/ReportLab", "stars": 25, "license": "bsd-3-clause", "language": "python", "size": 2060}
{"docstring": "\"\"\"Outputs comments before the method call.\"\"\"\n", "func_signal": "def _precomment(self) :\n", "code": "if self._action == \"showPage\" :\n    self._parent._PyWrite(\"\\n    # Ends page %i\" % self._parent._pagenumber)\nelif self._action == \"saveState\" :\n    state = {}\n    d = self._parent._object.__dict__\n    for name in self._parent._object.STATE_ATTRIBUTES:\n        state[name] = d[name]\n    self._parent._PyWrite(\"\\n    # Saves context level %i %s\" % (self._parent._contextlevel, state))\n    self._parent._contextlevel = self._parent._contextlevel + 1\nelif self._action == \"restoreState\" :\n    self._parent._contextlevel = self._parent._contextlevel - 1\n    self._parent._PyWrite(\"\\n    # Restores context level %i %s\" % (self._parent._contextlevel, self._parent._object.state_stack[-1]))\nelif self._action == \"beginForm\" :\n    self._parent._formnumber = self._parent._formnumber + 1\n    self._parent._PyWrite(\"\\n    # Begins form %i\" % self._parent._formnumber)\nelif self._action == \"endForm\" :\n    self._parent._PyWrite(\"\\n    # Ends form %i\" % self._parent._formnumber)\nelif self._action == \"save\" :\n    self._parent._PyWrite(\"\\n    # Saves the PDF document to disk\")", "path": "src\\reportlab\\pdfgen\\pycanvas.py", "repo_name": "mattjmorrison/ReportLab", "stars": 25, "license": "bsd-3-clause", "language": "python", "size": 2060}
{"docstring": "#loads one\n", "func_signal": "def start_customshape(self, args):\n", "code": "path = self._arg('customshape',args,'path')\nif path=='None':\n    path = []\nelse:\n    path=[path]\n\n# add package root folder and input file's folder to path\npath.append(os.path.dirname(self.sourceFilename))\npath.append(os.path.dirname(pythonpoint.__file__))\n\nmodulename = self._arg('customshape',args,'module')\nfuncname = self._arg('customshape',args,'class')\ntry:\n    found = imp.find_module(modulename, path)\n    (file, pathname, description) = found\n    mod = imp.load_module(modulename, file, pathname, description)\nexcept ImportError:\n    mod = getModule(modulename)\n\n#now get the function\n\nfunc = getattr(mod, funcname)\ninitargs = self.ceval('customshape',args,'initargs')\nself._curCustomShape = func(*initargs)", "path": "tools\\pythonpoint\\stdparser.py", "repo_name": "mattjmorrison/ReportLab", "stars": 25, "license": "bsd-3-clause", "language": "python", "size": 2060}
{"docstring": "#the only data should be paragraph text, preformatted para\n#text, 'string text' for a fixed string on the page,\n#or table data\n", "func_signal": "def handle_data(self, data):\n", "code": "if self._curPara:\n    self._curPara.rawtext = self._curPara.rawtext + data\nelif self._curPrefmt:\n    self._curPrefmt.rawtext = self._curPrefmt.rawtext + data\nelif self._curPyCode:\n    self._curPyCode.rawtext = self._curPyCode.rawtext + data\nelif  self._curString:\n    self._curString.text = self._curString.text + data\nelif self._curTable:\n    self._curTable.rawBlocks.append(data)\nelif self._curTitle != None:  # need to allow empty strings,\n    # hence explicitly testing for None\n    self._curTitle = self._curTitle + data\nelif self._curAuthor != None:\n    self._curAuthor = self._curAuthor + data\nelif self._curSubject != None:\n    self._curSubject = self._curSubject + data", "path": "tools\\pythonpoint\\stdparser.py", "repo_name": "mattjmorrison/ReportLab", "stars": 25, "license": "bsd-3-clause", "language": "python", "size": 2060}
{"docstring": "\"\"\"Returns the equivalent Python source code.\"\"\"\n", "func_signal": "def __str__(self) :\n", "code": "if not self._footerpresent :\n    self._PyWrite(PyFooter)\n    self._footerpresent = 1\nreturn self._pyfile.getvalue()", "path": "src\\reportlab\\pdfgen\\pycanvas.py", "repo_name": "mattjmorrison/ReportLab", "stars": 25, "license": "bsd-3-clause", "language": "python", "size": 2060}
{"docstring": "#controller should have set the text\n", "func_signal": "def end_string(self):\n", "code": "if self._curSlide:\n    self._curSlide.graphics.append(self._curString)\nelif self._curSection:\n    self._curSection.graphics.append(self._curString)\nself._curString = None", "path": "tools\\pythonpoint\\stdparser.py", "repo_name": "mattjmorrison/ReportLab", "stars": 25, "license": "bsd-3-clause", "language": "python", "size": 2060}
{"docstring": "\"\"\"PTO stands for Please Turn Over and is a means for\nspecifying content to be inserted when stuff goes over a page.\nThis makes one long multi-page paragraph.\"\"\"\n\n# Build story.\n", "func_signal": "def _ptoTestCase(self):\n", "code": "story = []\ndef fbreak(story=story):\n    story.append(FrameBreak())\n\nstyleSheet = getSampleStyleSheet()\nH1 = styleSheet['Heading1']\nH1.pageBreakBefore = 0\nH1.keepWithNext = 0\n\nbt = styleSheet['BodyText']\npto = ParagraphStyle('pto',parent=bt)\npto.alignment = TA_RIGHT\npto.fontSize -= 1\ndef ColorParagraph(c,text,style):\n    return Paragraph('<para color=\"%s\">%s</para>' % (c,text),style)\n\ndef ptoblob(blurb,content,trailer=None,header=None, story=story, H1=H1):\n    if type(content) not in (type([]),type(())): content = [content]\n    story.append(PTOContainer([Paragraph(blurb,H1)]+list(content),trailer,header))\n\nt0 = [ColorParagraph('blue','Please turn over', pto )]\nh0 = [ColorParagraph('blue','continued from previous page', pto )]\nt1 = [ColorParagraph('red','Please turn over(inner)', pto )]\nh1 = [ColorParagraph('red','continued from previous page(inner)', pto )]\nptoblob('First Try at a PTO',[Paragraph(text0,bt)],t0,h0)\nfbreak()\nc1 = Table([('alignment', 'align\\012alignment'),\n            ('bulletColor', 'bulletcolor\\012bcolor'),\n            ('bulletFontName', 'bfont\\012bulletfontname'),\n            ('bulletFontSize', 'bfontsize\\012bulletfontsize'),\n            ('bulletIndent', 'bindent\\012bulletindent'),\n            ('firstLineIndent', 'findent\\012firstlineindent'),\n            ('fontName', 'face\\012fontname\\012font'),\n            ('fontSize', 'size\\012fontsize'),\n            ('leading', 'leading'),\n            ('leftIndent', 'leftindent\\012lindent'),\n            ('rightIndent', 'rightindent\\012rindent'),\n            ('spaceAfter', 'spaceafter\\012spacea'),\n            ('spaceBefore', 'spacebefore\\012spaceb'),\n            ('textColor', 'fg\\012textcolor\\012color')],\n        style = [\n            ('VALIGN',(0,0),(-1,-1),'TOP'),\n            ('INNERGRID', (0,0), (-1,-1), 0.25, black),\n            ('BOX', (0,0), (-1,-1), 0.25, black),\n            ],\n        )\nptoblob('PTO with a table inside',c1,t0,h0)\nfbreak()\nptoblob('A long PTO',[Paragraph(text0+' '+text1,bt)],t0,h0)\nfbreak()\nptoblob('2 PTO (inner split)',[ColorParagraph('pink',text0,bt),PTOContainer([ColorParagraph(black,'Inner Starts',H1),ColorParagraph('yellow',text2,bt),ColorParagraph('black','Inner Ends',H1)],t1,h1),ColorParagraph('magenta',text1,bt)],t0,h0)\n_showDoc('test_platypus_pto.pdf',story)", "path": "tests\\test_platypus_pleaseturnover.py", "repo_name": "mattjmorrison/ReportLab", "stars": 25, "license": "bsd-3-clause", "language": "python", "size": 2060}
{"docstring": "\"\"\"Returns a dictionary of styles based on Helvetica\"\"\"\n\n", "func_signal": "def getParagraphStyles():\n", "code": "stylesheet = {}\nParagraphStyle = styles.ParagraphStyle\n\npara = ParagraphStyle('Normal', None)   #the ancestor of all\npara.fontName = 'Helvetica-Bold'\npara.fontSize = 24\npara.leading = 28\npara.textColor = white\nstylesheet['Normal'] = para\n\npara = ParagraphStyle('BodyText', stylesheet['Normal'])\npara.spaceBefore = 12\nstylesheet['BodyText'] = para\n\npara = ParagraphStyle('BigCentered', stylesheet['Normal'])\npara.spaceBefore = 12\npara.alignment = TA_CENTER\nstylesheet['BigCentered'] = para\n\npara = ParagraphStyle('Italic', stylesheet['BodyText'])\npara.fontName = 'Helvetica-Oblique'\npara.textColor = white\nstylesheet['Italic'] = para\n\npara = ParagraphStyle('Title', stylesheet['Normal'])\npara.fontName = 'Helvetica'\npara.fontSize = 48\npara.Leading = 58\npara.spaceAfter = 36\npara.alignment = TA_CENTER\nstylesheet['Title'] = para\n\npara = ParagraphStyle('Heading1', stylesheet['Normal'])\npara.fontName = 'Helvetica-Bold'\npara.fontSize = 48# 36\npara.leading = 44\npara.spaceAfter = 36\npara.textColor = green\npara.alignment = TA_LEFT\nstylesheet['Heading1'] = para\n\npara = ParagraphStyle('Heading2', stylesheet['Normal'])\npara.fontName = 'Helvetica-Bold'\npara.fontSize = 28\npara.leading = 34\npara.spaceBefore = 24\npara.spaceAfter = 12\nstylesheet['Heading2'] = para\n\npara = ParagraphStyle('Heading3', stylesheet['Normal'])\npara.fontName = 'Helvetica-BoldOblique'\npara.spaceBefore = 24\npara.spaceAfter = 12\nstylesheet['Heading3'] = para\n\npara = ParagraphStyle('Bullet', stylesheet['Normal'])\npara.firstLineIndent = -18\npara.leftIndent = 72\npara.spaceBefore = 6\npara.bulletFontName = 'Symbol'\npara.bulletFontSize = 24\npara.bulletIndent = 36\nstylesheet['Bullet'] = para\n\npara = ParagraphStyle('Definition', stylesheet['Normal'])\n#use this for definition lists\npara.firstLineIndent = 0\npara.leftIndent = 72\npara.bulletIndent = 0\npara.spaceBefore = 12\npara.bulletFontName = 'Helvetica-BoldOblique'\nstylesheet['Definition'] = para\n\npara = ParagraphStyle('Code', stylesheet['Normal'])\npara.fontName = 'Courier-Bold'\npara.fontSize = 16\npara.leading = 18\npara.leftIndent = 36\npara.textColor = chartreuse\nstylesheet['Code'] = para\n\nreturn stylesheet", "path": "tools\\pythonpoint\\styles\\projection.py", "repo_name": "mattjmorrison/ReportLab", "stars": 25, "license": "bsd-3-clause", "language": "python", "size": 2060}
{"docstring": "\"\"\"No contents so deal with it here.\"\"\"\n", "func_signal": "def start_spacer(self, args):\n", "code": "sp = pythonpoint.PPSpacer()\nsp.height = eval(args['height'])\nself._curFrame.content.append(sp)", "path": "tools\\pythonpoint\\stdparser.py", "repo_name": "mattjmorrison/ReportLab", "stars": 25, "license": "bsd-3-clause", "language": "python", "size": 2060}
{"docstring": "\"\"\"The fake method is called, print it then call the real one.\"\"\"\n", "func_signal": "def __call__(self, *args, **kwargs) :\n", "code": "if not self._parent._parent._in :\n    self._precomment()\n    self._parent._parent._PyWrite(\"    %s.%s(%s)\" % (self._parent._name, self._action, buildargs(*args, **kwargs)))\n    self._postcomment()\nself._parent._parent._in = self._parent._parent._in + 1\nretcode = getattr(self._parent._object, self._action)(*args,**kwargs)\nself._parent._parent._in = self._parent._parent._in - 1\nreturn retcode", "path": "src\\reportlab\\pdfgen\\pycanvas.py", "repo_name": "mattjmorrison/ReportLab", "stars": 25, "license": "bsd-3-clause", "language": "python", "size": 2060}
{"docstring": "\"\"\"Returns a dictionary of styles to get you started.\n\nWe will provide a way to specify a module of these.  Note that\nthis just includes TableStyles as well as ParagraphStyles for any\ntables you wish to use.\n\"\"\"\n\n", "func_signal": "def getParagraphStyles():\n", "code": "stylesheet = {}\nParagraphStyle = styles.ParagraphStyle\n\npara = ParagraphStyle('Normal', None)   #the ancestor of all\npara.fontName = 'Times-Roman'\npara.fontSize = 24\npara.leading = 28\nstylesheet['Normal'] = para\n\n#This one is spaced out a bit...\npara = ParagraphStyle('BodyText', stylesheet['Normal'])\npara.spaceBefore = 12\nstylesheet['BodyText'] = para\n\n#Indented, for lists\npara = ParagraphStyle('Indent', stylesheet['Normal'])\npara.leftIndent = 36\npara.firstLineIndent = 0\nstylesheet['Indent'] = para\n\npara = ParagraphStyle('Centered', stylesheet['Normal'])\npara.alignment = TA_CENTER\nstylesheet['Centered'] = para\n\npara = ParagraphStyle('BigCentered', stylesheet['Normal'])\npara.spaceBefore = 12\npara.alignment = TA_CENTER\nstylesheet['BigCentered'] = para\n\npara = ParagraphStyle('Italic', stylesheet['BodyText'])\npara.fontName = 'Times-Italic'\nstylesheet['Italic'] = para\n\npara = ParagraphStyle('Title', stylesheet['Normal'])\npara.fontName = 'Times-Roman'\npara.fontSize = 48\npara.leading = 58\npara.alignment = TA_CENTER\nstylesheet['Title'] = para\n\npara = ParagraphStyle('Heading1', stylesheet['Normal'])\npara.fontName = 'Times-Bold'\npara.fontSize = 36\npara.leading = 44\npara.alignment = TA_CENTER\nstylesheet['Heading1'] = para\n\npara = ParagraphStyle('Heading2', stylesheet['Normal'])\npara.fontName = 'Times-Bold'\npara.fontSize = 28\npara.leading = 34\npara.spaceBefore = 24\nstylesheet['Heading2'] = para\n\npara = ParagraphStyle('Heading3', stylesheet['Normal'])\npara.fontName = 'Times-BoldItalic'\npara.spaceBefore = 24\nstylesheet['Heading3'] = para\n\npara = ParagraphStyle('Heading4', stylesheet['Normal'])\npara.fontName = 'Times-BoldItalic'\npara.spaceBefore = 6\nstylesheet['Heading4'] = para\n\npara = ParagraphStyle('Bullet', stylesheet['Normal'])\npara.firstLineIndent = 0\npara.leftIndent = 56\npara.spaceBefore = 6\npara.bulletFontName = 'Symbol'\npara.bulletFontSize = 24\npara.bulletIndent = 20\nstylesheet['Bullet'] = para\n\npara = ParagraphStyle('Definition', stylesheet['Normal'])\n#use this for definition lists\npara.firstLineIndent = 0\npara.leftIndent = 72\npara.bulletIndent = 0\npara.spaceBefore = 12\npara.bulletFontName = 'Helvetica-BoldOblique'\npara.bulletFontSize = 24\nstylesheet['Definition'] = para\n\npara = ParagraphStyle('Code', stylesheet['Normal'])\npara.fontName = 'Courier'\npara.fontSize = 16\npara.leading = 18\npara.leftIndent = 36\nstylesheet['Code'] = para\n\npara = ParagraphStyle('PythonCode', stylesheet['Normal'])\npara.fontName = 'Courier'\npara.fontSize = 16\npara.leading = 18\npara.leftIndent = 36\nstylesheet['PythonCode'] = para\n\npara = ParagraphStyle('Small', stylesheet['Normal'])\npara.fontSize = 12\npara.leading = 14\nstylesheet['Small'] = para\n\n#now for a table\nts = TableStyle([\n     ('FONT', (0,0), (-1,-1), 'Times-Roman', 24),\n     ('LINEABOVE', (0,0), (-1,0), 2, colors.green),\n     ('LINEABOVE', (0,1), (-1,-1), 0.25, colors.black),\n     ('LINEBELOW', (0,-1), (-1,-1), 2, colors.green),\n     ('LINEBEFORE', (-1,0), (-1,-1), 2, colors.black),\n     ('ALIGN', (1,1), (-1,-1), 'RIGHT'),   #all numeric cells right aligned\n     ('TEXTCOLOR', (0,1), (0,-1), colors.red),\n     ('BACKGROUND', (0,0), (-1,0), colors.Color(0,0.7,0.7))\n     ])\nstylesheet['table1'] = ts\n\nreturn stylesheet", "path": "tools\\pythonpoint\\styles\\standard.py", "repo_name": "mattjmorrison/ReportLab", "stars": 25, "license": "bsd-3-clause", "language": "python", "size": 2060}
{"docstring": "## \n", "func_signal": "def getAcl(self,aclid):\n", "code": "theAcl = self.getSession().query(Acl).filter(Acl.id == aclid)[0]\nreturn theAcl", "path": "pyffle_data.py", "repo_name": "pyffle/Pyffle-BBS", "stars": 28, "license": "None", "language": "python", "size": 224}
{"docstring": "## Delete messages first\n", "func_signal": "def deleteBoardByBoardid(self, boardid):\n", "code": "msgids = self.getMessagesByBoardid(boardid)\nfor msgid in msgids:\n\tself.deleteMessage(msgid)\n\n## Get the board\nboard = self.getBoard(boardid)\nif not board == None:\n\tboardname = board.name\n\t\n\t\n\t## remove ourselves\n\tself.getSession().delete(board)\n\tself.getSession().commit()\t\t\n\t\n\t## Finally delete the ACL\n\t## self.deleteAcl(board.aclid)\n\t\n\n\tself.logEntry(self.LOGNORMAL,\"DELETE/BOARD\",\"BOARDID:\" + str(boardid),\"Board deleted: %s \" % (str(boardname)))", "path": "pyffle_data.py", "repo_name": "pyffle/Pyffle-BBS", "stars": 28, "license": "None", "language": "python", "size": 224}
{"docstring": "## \n", "func_signal": "def getMessagesAuthoredByUser(self,username,checkSrm=True):\n", "code": "rv = []\nfor instance in self.getSession().query(Message).filter(Message.fromname == username).order_by(Message.sentdate): \n\tif checkSrm:\n\t\tif self.srmcheck(instance.aclid,username,\"READ\"):\n\t\t\trv.append(instance.id)\n\telse:\n\t\trv.append(instance.id)\t\nreturn rv", "path": "pyffle_data.py", "repo_name": "pyffle/Pyffle-BBS", "stars": 28, "license": "None", "language": "python", "size": 224}
{"docstring": "## val     10 | val chiswell       25    | 23-Oct-10   13 | No comment..\n", "func_signal": "def listUsers(self):\n", "code": "users = self.data.getUsers()\n\n## Userlist @ waffle.uuhec.net, 23 total\n\nself.data.util.println(\"\\nUserlist @ %n, \" + str(users.count()) + \" total\\n\")\nfor user in users:\n\tusername = user.username\n\tidentity = user.fullidentity\n\tlastlogin = user.datefastlogin\n\tlastlogin = self.data.util.formatDateString(lastlogin)\n\tcomment = user.comment\n\tself.data.util.println(\"{0:10} | \".format(username) + \"{0:25} | \".format(identity) + \"{0:13} | \".format(lastlogin) + comment)\nself.data.util.println(\"\")", "path": "pyffle_userlist.py", "repo_name": "pyffle/Pyffle-BBS", "stars": 28, "license": "None", "language": "python", "size": 224}
{"docstring": "## \n", "func_signal": "def deny(self,acl, subject, object):\n", "code": "if self.isDenied(acl,subject,object):\n\treturn;\t\t## This ACE already exists, we don't need to do anything\nmyAce = Ace()\nmyAce.aclid = acl.id\nmyAce.subjectid = subject\nmyAce.permission = object\nmyAce.grantordeny = \"DENY\"\nself.getSession().add(myAce)\nself.getSession().commit()\nself.logEntry(self.LOGINFO,\"ACE/DENY\",str(subject),\"Denied %s to %s\" % (str(object),str(subject)))", "path": "pyffle_data.py", "repo_name": "pyffle/Pyffle-BBS", "stars": 28, "license": "None", "language": "python", "size": 224}
{"docstring": "## Returns true if we can process this kind of address\n", "func_signal": "def matchAddress(self,s):\n", "code": "if (s.find(\"!\")) > 0 or (s.find(\"@\") > 0):\n\treturn True\nelse:\n\treturn False", "path": "pyffle_mta_uucp.py", "repo_name": "pyffle/Pyffle-BBS", "stars": 28, "license": "None", "language": "python", "size": 224}
{"docstring": "### single mta = [module.py, mta name, mta description]\n", "func_signal": "def loadMtaList(self,mtas):\n", "code": "self.mtas = mtas\nfor mta in mtas:\n\tself.mtaModules[mta[1]] = (__import__(mta[0]))", "path": "pyffle_data.py", "repo_name": "pyffle/Pyffle-BBS", "stars": 28, "license": "None", "language": "python", "size": 224}
{"docstring": "## \n", "func_signal": "def getBoardByName(self,name):\n", "code": "for theBoard in self.getSession().query(Board).filter(Board.name == name):\n\treturn theBoard\nreturn None", "path": "pyffle_data.py", "repo_name": "pyffle/Pyffle-BBS", "stars": 28, "license": "None", "language": "python", "size": 224}
{"docstring": "## \n", "func_signal": "def getBoardids(self):\n", "code": "rv = []\nfor theBoard in self.getBoards():\n\trv.append(theBoard.id)\nreturn rv", "path": "pyffle_data.py", "repo_name": "pyffle/Pyffle-BBS", "stars": 28, "license": "None", "language": "python", "size": 224}
{"docstring": "## FIXME: add checking of pre-existing ACE\n\t\t## \n", "func_signal": "def grant(self,acl, subject, object):\n", "code": "\t\tif self.isGranted(acl,subject,object):\n\t\t\treturn;\t\t## This ACE already exists, we don't need to do anything\n\t\tmyAce = Ace()\n\t\tmyAce.aclid = acl.id\n\t\tmyAce.subjectid = subject\n\t\tmyAce.permission = object\n\t\tmyAce.grantordeny = \"GRANT\"\n\t\tself.getSession().add(myAce)\n\t\tself.getSession().commit()\n\t\tself.logEntry(self.LOGINFO,\"ACE/GRANT\",str(subject),\"Granted %s to %s\" % (str(object),str(subject)))", "path": "pyffle_data.py", "repo_name": "pyffle/Pyffle-BBS", "stars": 28, "license": "None", "language": "python", "size": 224}
{"docstring": "## \n", "func_signal": "def getMessagesByBoardid(self,boardid):\n", "code": "rv = []\nfor instance in self.getSession().query(Message).filter(Message.boardid == boardid).order_by(Message.sentdate): \n\trv.append(instance.id)\nreturn rv", "path": "pyffle_data.py", "repo_name": "pyffle/Pyffle-BBS", "stars": 28, "license": "None", "language": "python", "size": 224}
{"docstring": "## \n", "func_signal": "def getUser(self,username):\n", "code": "rv = None\nfor theUser in self.getSession().query(Users).filter(Users.username == username.lower()):\n\trv = theUser\nreturn rv", "path": "pyffle_data.py", "repo_name": "pyffle/Pyffle-BBS", "stars": 28, "license": "None", "language": "python", "size": 224}
{"docstring": "## \n", "func_signal": "def getBoards(self):\n", "code": "rv = []\nfor theBoard in self.getSession().query(Board).order_by(Board.id):\n\trv.append(theBoard)\nreturn rv", "path": "pyffle_data.py", "repo_name": "pyffle/Pyffle-BBS", "stars": 28, "license": "None", "language": "python", "size": 224}
{"docstring": "## FIXME - args\n## FIXME - join\n\n## Remember the currently selected board \n", "func_signal": "def new(self):\n", "code": "oldCurrent = self.data.getCurrentBoard()\n\n## Get a list of all boards FIXME: Should only get the subscribed ones\nboards = self.data.getBoards()\nself.data.stateChange(\"board_newstart\")\t\n\nif self.currentUser.datelastnewscan == None:\n\tself.data.stateChange(\"board_newnodatestart\")\t\n\tself.data.util.println(\"No last scan, setting to epoch..\")\n\tself.resetScan()\n\tself.data.stateChange(\"board_newnodateend\")\n\n## Loop through the boards\n\nself.data.util.println(\"\\nScanning for new messages..\\n\")\nself.data.stateChange(\"board_newloop\")\nscannedBoards = 0\nfor board in boards:\n\tif not board.name.startswith('__') and  (board.id in self.data.getJoinedBoardids()):   ## ignore any internal boards\n\t\t## Check that we're allowed to read this board\n\t\tif self.data.srmcheck(board.aclid,self.currentUser.username,\"READ\",minlevel=board.minreadlevel):\n\t\t\t## Get the messages\n\t\t\tscannedBoards = scannedBoards + 1 \n\t\t\tmsgids = self.data.getMessagesSince(board,self.currentUser.datelastnewscan) \n\t\t\tif not msgids == []:\n\t\t\t\tself.data.stateChange(\"board_newfoundstart\")\n\t\t\t\tself.data.util.println(\"  {0:30}\".format(board.name) + \" \" + str(len(msgids)) + \" new message(s)\")\n\t\t\t\tself.data.stateChange(\"board_newfoundend\")\n\t\t\telse:\n\t\t\t\tself.data.stateChange(\"board_newemptystart\")\n\t\t\t\tself.data.util.println(\"  {0:30}\".format(board.name) + \" No new messages\")\n\t\t\t\tself.data.stateChange(\"board_newemptyend\")\nself.data.stateChange(\"board_newloopend\")\nif scannedBoards == 0:\n\tself.data.stateChange(\"board_newjoinwarnstart\")\n\tself.data.util.println(\"No boards actually scanned, check your JOIN settings..\")\n\tself.data.stateChange(\"board_newjoinwarnend\")\nelse:\n\tself.data.stateChange(\"board_newsummarystart\")\n\tself.data.util.println(\"\\nScanned %s boards\\n\" % (str(scannedBoards)))\n\tself.data.stateChange(\"board_newsummaryend\")\n\nself.data.stateChange(\"board_newend\")\nreturn", "path": "pyffle_boards.py", "repo_name": "pyffle/Pyffle-BBS", "stars": 28, "license": "None", "language": "python", "size": 224}
{"docstring": "## Returns tuple with the local user and system specified by this incoming address\n", "func_signal": "def parseIncomingAddress(self,s):\n", "code": "elements = s.split(\"!\")\nusername = elements[len(elements)-1]\nusername = username.strip()\nsystem = elements[len(elements)-2]\nsystem = system.strip()\nreturn username,system", "path": "pyffle_mta_uucp.py", "repo_name": "pyffle/Pyffle-BBS", "stars": 28, "license": "None", "language": "python", "size": 224}
{"docstring": "## \n", "func_signal": "def getBoard(self,boardid):\n", "code": "for theBoard in self.getSession().query(Board).filter(Board.id == boardid):\n\treturn theBoard\nreturn None", "path": "pyffle_data.py", "repo_name": "pyffle/Pyffle-BBS", "stars": 28, "license": "None", "language": "python", "size": 224}
{"docstring": "## \n## let's get the message first so that we can delete it's components\n", "func_signal": "def deleteUser(self,username):\n", "code": "user = self.getUser(username)\nif not user == None:\t\t\n\t## remember the acl id for the step after this\n\taclid = user.aclid\n\t\n\t## delete the user itself\n\tself.getSession().delete(user)\n\tself.getSession().commit()\n\t\n\t## finally we delete the ACL\n\tself.deleteAcl(aclid)\n\tself.logEntry(self.LOGINFO,\"DELETE/USER\",str(username),\"Deleted user: %s\" % (str(username)))\n\t\n\treturn True", "path": "pyffle_data.py", "repo_name": "pyffle/Pyffle-BBS", "stars": 28, "license": "None", "language": "python", "size": 224}
{"docstring": "## Loop through board objects, display one numbered line / board\n", "func_signal": "def listBoards(self, boards):\n", "code": "self.data.stateChange(\"board_listboardsstart\")\nself.data.util.println(\"\\nBoards available:\")\t\ni = 1\nfor board in boards:\n\tif not board.name.startswith('__'):\n\t\tif self.data.srmcheck(board.aclid,self.currentUser.username,\"READ\",minlevel=board.minreadlevel):\n\t\t\tself.data.util.println(\" [^\" + str(i) + \"^] \" + str(board.name) + \" - (\" + board.description + \") \") \n\ti = i + 1\nself.data.stateChange(\"board_listboardsend\")", "path": "pyffle_boards.py", "repo_name": "pyffle/Pyffle-BBS", "stars": 28, "license": "None", "language": "python", "size": 224}
{"docstring": "## Fri, 19 Nov 82 16:14:55 GMT\n", "func_signal": "def formatNntpDate(self, date):\n", "code": "rv = date.strftime(\"%a, %d %b %Y %H:%M:%S %Z\")\t\t\nreturn rv", "path": "pyffle_sysman.py", "repo_name": "pyffle/Pyffle-BBS", "stars": 28, "license": "None", "language": "python", "size": 224}
{"docstring": "## \n", "func_signal": "def isDenied(self,acl, subject, object):\n", "code": "denied = False\nfor myAce in self.getSession().query(Ace).filter(Ace.aclid==acl.id).filter(Ace.subjectid==subject).filter(Ace.permission == object).filter(Ace.grantordeny == \"DENY\"):\n\tdenied = True\nreturn denied", "path": "pyffle_data.py", "repo_name": "pyffle/Pyffle-BBS", "stars": 28, "license": "None", "language": "python", "size": 224}
{"docstring": "\"\"\"Return an ASCII-only JSON representation of a Python string\n\n\"\"\"\n", "func_signal": "def py_encode_basestring_ascii(s):\n", "code": "if isinstance(s, str) and HAS_UTF8.search(s) is not None:\n    s = s.decode('utf-8')\ndef replace(match):\n    s = match.group(0)\n    try:\n        return ESCAPE_DCT[s]\n    except KeyError:\n        n = ord(s)\n        if n < 0x10000:\n            #return '\\\\u{0:04x}'.format(n)\n            return '\\\\u%04x' % (n,)\n        else:\n            # surrogate pair\n            n -= 0x10000\n            s1 = 0xd800 | ((n >> 10) & 0x3ff)\n            s2 = 0xdc00 | (n & 0x3ff)\n            #return '\\\\u{0:04x}\\\\u{1:04x}'.format(s1, s2)\n            return '\\\\u%04x\\\\u%04x' % (s1, s2)\nreturn '\"' + str(ESCAPE_ASCII.sub(replace, s)) + '\"'", "path": "fetcher\\simplejson\\encoder.py", "repo_name": "kkszysiu/lifepress", "stars": 19, "license": "None", "language": "python", "size": 334}
{"docstring": "'''Parse a string according to the MS SQL date format'''\n", "func_signal": "def _parse_date_mssql(dateString):\n", "code": "m = _mssql_date_re.match(dateString)\nif not m: return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': m.group(4), 'minute': m.group(5), 'second': m.group(6),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('MS SQL date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "feedparser.py", "repo_name": "kkszysiu/lifepress", "stars": 19, "license": "None", "language": "python", "size": 334}
{"docstring": "# Check if\n# - server requires digest auth, AND\n# - we tried (unsuccessfully) with basic auth, AND\n# - we're using Python 2.3.3 or later (digest auth is irreparably broken in earlier versions)\n# If all conditions hold, parse authentication information\n# out of the Authorization header we sent the first time\n# (for the username and password) and the WWW-Authenticate\n# header the server sent back (for the realm) and retry\n# the request with the appropriate digest auth headers instead.\n# This evil genius hack has been brought to you by Aaron Swartz.\n", "func_signal": "def http_error_401(self, req, fp, code, msg, headers):\n", "code": "host = urlparse.urlparse(req.get_full_url())[1]\ntry:\n    assert sys.version.split()[0] >= '2.3.3'\n    assert base64 != None\n    user, passw = base64.decodestring(req.headers['Authorization'].split(' ')[1]).split(':')\n    realm = re.findall('realm=\"([^\"]*)\"', headers['WWW-Authenticate'])[0]\n    self.add_password(realm, host, user, passw)\n    retry = self.http_error_auth_reqed('www-authenticate', host, req, headers)\n    self.reset_retry_count()\n    return retry\nexcept:\n    return self.http_error_default(req, fp, code, msg, headers)", "path": "feedparser.py", "repo_name": "kkszysiu/lifepress", "stars": 19, "license": "None", "language": "python", "size": 334}
{"docstring": "'''Return the Time Zone Designator as an offset in seconds from UTC.'''\n", "func_signal": "def __extract_tzd(m):\n", "code": "if not m:\n    return 0\ntzd = m.group('tzd')\nif not tzd:\n    return 0\nif tzd == 'Z':\n    return 0\nhours = int(m.group('tzdhours'))\nminutes = m.group('tzdminutes')\nif minutes:\n    minutes = int(minutes)\nelse:\n    minutes = 0\noffset = (hours*60 + minutes) * 60\nif tzd[0] == '+':\n    return -offset\nreturn offset", "path": "feedparser.py", "repo_name": "kkszysiu/lifepress", "stars": 19, "license": "None", "language": "python", "size": 334}
{"docstring": "'''Parse a variety of ISO-8601-compatible formats like 20040105'''\n", "func_signal": "def _parse_date_iso8601(dateString):\n", "code": "m = None\nfor _iso8601_match in _iso8601_matches:\n    m = _iso8601_match(dateString)\n    if m: break\nif not m: return\nif m.span() == (0, 0): return\nparams = m.groupdict()\nordinal = params.get('ordinal', 0)\nif ordinal:\n    ordinal = int(ordinal)\nelse:\n    ordinal = 0\nyear = params.get('year', '--')\nif not year or year == '--':\n    year = time.gmtime()[0]\nelif len(year) == 2:\n    # ISO 8601 assumes current century, i.e. 93 -> 2093, NOT 1993\n    year = 100 * int(time.gmtime()[0] / 100) + int(year)\nelse:\n    year = int(year)\nmonth = params.get('month', '-')\nif not month or month == '-':\n    # ordinals are NOT normalized by mktime, we simulate them\n    # by setting month=1, day=ordinal\n    if ordinal:\n        month = 1\n    else:\n        month = time.gmtime()[1]\nmonth = int(month)\nday = params.get('day', 0)\nif not day:\n    # see above\n    if ordinal:\n        day = ordinal\n    elif params.get('century', 0) or \\\n             params.get('year', 0) or params.get('month', 0):\n        day = 1\n    else:\n        day = time.gmtime()[2]\nelse:\n    day = int(day)\n# special case of the century - is the first year of the 21st century\n# 2000 or 2001 ? The debate goes on...\nif 'century' in params.keys():\n    year = (int(params['century']) - 1) * 100 + 1\n# in ISO 8601 most fields are optional\nfor field in ['hour', 'minute', 'second', 'tzhour', 'tzmin']:\n    if not params.get(field, None):\n        params[field] = 0\nhour = int(params.get('hour', 0))\nminute = int(params.get('minute', 0))\nsecond = int(params.get('second', 0))\n# weekday is normalized by mktime(), we can ignore it\nweekday = 0\n# daylight savings is complex, but not needed for feedparser's purposes\n# as time zones, if specified, include mention of whether it is active\n# (e.g. PST vs. PDT, CET). Using -1 is implementation-dependent and\n# and most implementations have DST bugs\ndaylight_savings_flag = 0\ntm = [year, month, day, hour, minute, second, weekday,\n      ordinal, daylight_savings_flag]\n# ISO 8601 time zone adjustments\ntz = params.get('tz')\nif tz and tz != 'Z':\n    if tz[0] == '-':\n        tm[3] += int(params.get('tzhour', 0))\n        tm[4] += int(params.get('tzmin', 0))\n    elif tz[0] == '+':\n        tm[3] -= int(params.get('tzhour', 0))\n        tm[4] -= int(params.get('tzmin', 0))\n    else:\n        return None\n# Python's time.mktime() is a wrapper around the ANSI C mktime(3c)\n# which is guaranteed to normalize d/m/y/h/m/s.\n# Many implementations have bugs, but we'll pretend they don't.\nreturn time.localtime(time.mktime(tm))", "path": "feedparser.py", "repo_name": "kkszysiu/lifepress", "stars": 19, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"\nReturns either a variable by itself or a non-boolean operation (such as\n``x == 0`` or ``x < 0``).\n\nThis is needed to keep correct precedence for boolean operations (i.e.\n``x or x == 0`` should be ``x or (x == 0)``, not ``(x or x) == 0``).\n\"\"\"\n", "func_signal": "def get_bool_var(self):\n", "code": "var = self.get_var()\nif not self.at_end():\n    op_token = self.get_token(lookahead=True)[0]\n    if isinstance(op_token, basestring) and (op_token not in\n                                             BOOL_OPERATORS):\n        op, negate = self.get_operator()\n        return op(var, self.get_var(), negate=negate)\nreturn var", "path": "lajfstrim\\templatetags\\smart_if.py", "repo_name": "kkszysiu/lifepress", "stars": 19, "license": "None", "language": "python", "size": 334}
{"docstring": "'''Changes an XML data stream on the fly to specify a new encoding\n\ndata is a raw sequence of bytes (not Unicode) that is presumed to be in %encoding already\nencoding is a string recognized by encodings.aliases\n'''\n", "func_signal": "def _toUTF8(data, encoding):\n", "code": "if _debug: sys.stderr.write('entering _toUTF8, trying encoding %s\\n' % encoding)\n# strip Byte Order Mark (if present)\nif (len(data) >= 4) and (data[:2] == '\\xfe\\xff') and (data[2:4] != '\\x00\\x00'):\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-16be':\n            sys.stderr.write('trying utf-16be instead\\n')\n    encoding = 'utf-16be'\n    data = data[2:]\nelif (len(data) >= 4) and (data[:2] == '\\xff\\xfe') and (data[2:4] != '\\x00\\x00'):\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-16le':\n            sys.stderr.write('trying utf-16le instead\\n')\n    encoding = 'utf-16le'\n    data = data[2:]\nelif data[:3] == '\\xef\\xbb\\xbf':\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-8':\n            sys.stderr.write('trying utf-8 instead\\n')\n    encoding = 'utf-8'\n    data = data[3:]\nelif data[:4] == '\\x00\\x00\\xfe\\xff':\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-32be':\n            sys.stderr.write('trying utf-32be instead\\n')\n    encoding = 'utf-32be'\n    data = data[4:]\nelif data[:4] == '\\xff\\xfe\\x00\\x00':\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-32le':\n            sys.stderr.write('trying utf-32le instead\\n')\n    encoding = 'utf-32le'\n    data = data[4:]\nnewdata = unicode(data, encoding)\nif _debug: sys.stderr.write('successfully converted %s data to unicode\\n' % encoding)\ndeclmatch = re.compile('^<\\?xml[^>]*?>')\nnewdecl = '''<?xml version='1.0' encoding='utf-8'?>'''\nif declmatch.search(newdata):\n    newdata = declmatch.sub(newdecl, newdata)\nelse:\n    newdata = newdecl + u'\\n' + newdata\nreturn newdata.encode('utf-8')", "path": "feedparser.py", "repo_name": "kkszysiu/lifepress", "stars": 19, "license": "None", "language": "python", "size": 334}
{"docstring": "# called for each end tag, e.g. for </pre>, tag will be 'pre'\n# Reconstruct the original end tag.\n", "func_signal": "def unknown_endtag(self, tag):\n", "code": "if tag not in self.elements_no_end_tag:\n    self.pieces.append(\"</%(tag)s>\" % locals())", "path": "feedparser.py", "repo_name": "kkszysiu/lifepress", "stars": 19, "license": "None", "language": "python", "size": 334}
{"docstring": "# test in/out equivalence and parsing\n", "func_signal": "def test_parse(self):\n", "code": "res = json.loads(JSON)\nout = json.dumps(res)\nself.assertEquals(res, json.loads(out))\ntry:\n    json.dumps(res, allow_nan=False)\nexcept ValueError:\n    pass\nelse:\n    self.fail(\"23456789012E666 should be out of range\")", "path": "simplejson\\tests\\test_pass1.py", "repo_name": "kkszysiu/lifepress", "stars": 19, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"Return a JSON string representation of a Python data structure.\n\n>>> JSONEncoder().encode({\"foo\": [\"bar\", \"baz\"]})\n'{\"foo\": [\"bar\", \"baz\"]}'\n\n\"\"\"\n# This is for extremely simple cases and benchmarks.\n", "func_signal": "def encode(self, o):\n", "code": "if isinstance(o, basestring):\n    if isinstance(o, str):\n        _encoding = self.encoding\n        if (_encoding is not None\n                and not (_encoding == 'utf-8')):\n            o = o.decode(_encoding)\n    if self.ensure_ascii:\n        return encode_basestring_ascii(o)\n    else:\n        return encode_basestring(o)\n# This doesn't pass the iterator directly to ''.join() because the\n# exceptions aren't as detailed.  The list call should be roughly\n# equivalent to the PySequence_Fast that ''.join() would do.\nchunks = self.iterencode(o, _one_shot=True)\nif not isinstance(chunks, (list, tuple)):\n    chunks = list(chunks)\nreturn ''.join(chunks)", "path": "fetcher\\simplejson\\encoder.py", "repo_name": "kkszysiu/lifepress", "stars": 19, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"Return a JSON representation of a Python string\n\n\"\"\"\n", "func_signal": "def encode_basestring(s):\n", "code": "def replace(match):\n    return ESCAPE_DCT[match.group(0)]\nreturn '\"' + ESCAPE.sub(replace, s) + '\"'", "path": "fetcher\\simplejson\\encoder.py", "repo_name": "kkszysiu/lifepress", "stars": 19, "license": "None", "language": "python", "size": 334}
{"docstring": "# override internal declaration handler to handle CDATA blocks\n", "func_signal": "def parse_declaration(self, i):\n", "code": "if _debug: sys.stderr.write('entering parse_declaration\\n')\nif self.rawdata[i:i+9] == '<![CDATA[':\n    k = self.rawdata.find(']]>', i)\n    if k == -1: k = len(self.rawdata)\n    self.handle_data(_xmlescape(self.rawdata[i+9:k]), 0)\n    return k+3\nelse:\n    k = self.rawdata.find('>', i)\n    return k+1", "path": "feedparser.py", "repo_name": "kkszysiu/lifepress", "stars": 19, "license": "None", "language": "python", "size": 334}
{"docstring": "# called for each block of plain text, i.e. outside of any tag and\n# not containing any character or entity references\n", "func_signal": "def handle_data(self, text, escape=1):\n", "code": "if not self.elementstack: return\nif escape and self.contentparams.get('type') == 'application/xhtml+xml':\n    text = _xmlescape(text)\nself.elementstack[-1][2].append(text)", "path": "feedparser.py", "repo_name": "kkszysiu/lifepress", "stars": 19, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"Decode a JSON document from ``s`` (a ``str`` or ``unicode`` beginning\nwith a JSON document) and return a 2-tuple of the Python\nrepresentation and the index in ``s`` where the document ended.\n\nThis can be used to decode a JSON document from a string that may\nhave extraneous data at the end.\n\n\"\"\"\n", "func_signal": "def raw_decode(self, s, idx=0):\n", "code": "try:\n    obj, end = self.scan_once(s, idx)\nexcept StopIteration:\n    raise ValueError(\"No JSON object could be decoded\")\nreturn obj, end", "path": "simplejson\\decoder.py", "repo_name": "kkszysiu/lifepress", "stars": 19, "license": "None", "language": "python", "size": 334}
{"docstring": "'''Parse a string according to the Nate 8-bit date format'''\n", "func_signal": "def _parse_date_nate(dateString):\n", "code": "m = _korean_nate_date_re.match(dateString)\nif not m: return\nhour = int(m.group(5))\nampm = m.group(4)\nif (ampm == _korean_pm):\n    hour += 12\nhour = str(hour)\nif len(hour) == 1:\n    hour = '0' + hour\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': hour, 'minute': m.group(6), 'second': m.group(7),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('Nate date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "feedparser.py", "repo_name": "kkszysiu/lifepress", "stars": 19, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"Scan the string s for a JSON string. End is the index of the\ncharacter in s after the quote that started the JSON string.\nUnescapes all valid JSON string escape sequences and raises ValueError\non attempt to decode an invalid string. If strict is False then literal\ncontrol characters are allowed in the string.\n\nReturns a tuple of the decoded string and the index of the character in s\nafter the end quote.\"\"\"\n", "func_signal": "def py_scanstring(s, end, encoding=None, strict=True, _b=BACKSLASH, _m=STRINGCHUNK.match):\n", "code": "if encoding is None:\n    encoding = DEFAULT_ENCODING\nchunks = []\n_append = chunks.append\nbegin = end - 1\nwhile 1:\n    chunk = _m(s, end)\n    if chunk is None:\n        raise ValueError(\n            errmsg(\"Unterminated string starting at\", s, begin))\n    end = chunk.end()\n    content, terminator = chunk.groups()\n    # Content is contains zero or more unescaped string characters\n    if content:\n        if not isinstance(content, unicode):\n            content = unicode(content, encoding)\n        _append(content)\n    # Terminator is the end of string, a literal control character,\n    # or a backslash denoting that an escape sequence follows\n    if terminator == '\"':\n        break\n    elif terminator != '\\\\':\n        if strict:\n            msg = \"Invalid control character %r at\" % (terminator,)\n            #msg = \"Invalid control character {0!r} at\".format(terminator)\n            raise ValueError(errmsg(msg, s, end))\n        else:\n            _append(terminator)\n            continue\n    try:\n        esc = s[end]\n    except IndexError:\n        raise ValueError(\n            errmsg(\"Unterminated string starting at\", s, begin))\n    # If not a unicode escape sequence, must be in the lookup table\n    if esc != 'u':\n        try:\n            char = _b[esc]\n        except KeyError:\n            msg = \"Invalid \\\\escape: \" + repr(esc)\n            raise ValueError(errmsg(msg, s, end))\n        end += 1\n    else:\n        # Unicode escape sequence\n        esc = s[end + 1:end + 5]\n        next_end = end + 5\n        if len(esc) != 4:\n            msg = \"Invalid \\\\uXXXX escape\"\n            raise ValueError(errmsg(msg, s, end))\n        uni = int(esc, 16)\n        # Check for surrogate pair on UCS-4 systems\n        if 0xd800 <= uni <= 0xdbff and sys.maxunicode > 65535:\n            msg = \"Invalid \\\\uXXXX\\\\uXXXX surrogate pair\"\n            if not s[end + 5:end + 7] == '\\\\u':\n                raise ValueError(errmsg(msg, s, end))\n            esc2 = s[end + 7:end + 11]\n            if len(esc2) != 4:\n                raise ValueError(errmsg(msg, s, end))\n            uni2 = int(esc2, 16)\n            uni = 0x10000 + (((uni - 0xd800) << 10) | (uni2 - 0xdc00))\n            next_end += 6\n        char = unichr(uni)\n        end = next_end\n    # Append the unescaped character\n    _append(char)\nreturn u''.join(chunks), end", "path": "simplejson\\decoder.py", "repo_name": "kkszysiu/lifepress", "stars": 19, "license": "None", "language": "python", "size": 334}
{"docstring": "# called for each entity reference, e.g. for '&copy;', ref will be 'copy'\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "if not self.elementstack: return\nif _debug: sys.stderr.write('entering handle_entityref with %s\\n' % ref)\nif ref in ('lt', 'gt', 'quot', 'amp', 'apos'):\n    text = '&%s;' % ref\nelse:\n    # entity resolution graciously donated by Aaron Swartz\n    def name2cp(k):\n        import htmlentitydefs\n        if hasattr(htmlentitydefs, 'name2codepoint'): # requires Python 2.3\n            return htmlentitydefs.name2codepoint[k]\n        k = htmlentitydefs.entitydefs[k]\n        if k.startswith('&#') and k.endswith(';'):\n            return int(k[2:-1]) # not in latin-1\n        return ord(k)\n    try: name2cp(ref)\n    except KeyError: text = '&%s;' % ref\n    else: text = unichr(name2cp(ref)).encode('utf-8')\nself.elementstack[-1][2].append(text)", "path": "feedparser.py", "repo_name": "kkszysiu/lifepress", "stars": 19, "license": "None", "language": "python", "size": 334}
{"docstring": "'''Parse a string according to the OnBlog 8-bit date format'''\n", "func_signal": "def _parse_date_onblog(dateString):\n", "code": "m = _korean_onblog_date_re.match(dateString)\nif not m: return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': m.group(4), 'minute': m.group(5), 'second': m.group(6),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('OnBlog date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "feedparser.py", "repo_name": "kkszysiu/lifepress", "stars": 19, "license": "None", "language": "python", "size": 334}
{"docstring": "'''Parse an RFC822, RFC1123, RFC2822, or asctime-style date'''\n", "func_signal": "def _parse_date_rfc822(dateString):\n", "code": "data = dateString.split()\nif data[0][-1] in (',', '.') or data[0].lower() in rfc822._daynames:\n    del data[0]\nif len(data) == 4:\n    s = data[3]\n    i = s.find('+')\n    if i > 0:\n        data[3:] = [s[:i], s[i+1:]]\n    else:\n        data.append('')\n    dateString = \" \".join(data)\nif len(data) < 5:\n    dateString += ' 00:00:00 GMT'\ntm = rfc822.parsedate_tz(dateString)\nif tm:\n    return time.gmtime(rfc822.mktime_tz(tm))", "path": "feedparser.py", "repo_name": "kkszysiu/lifepress", "stars": 19, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"\nTest a calculation is True, also checking the inverse \"negate\" case.\n\"\"\"\n", "func_signal": "def assertCalc(self, calc, context=None):\n", "code": "context = context or {}\nself.assert_(calc.resolve(context))\ncalc.negate = not calc.negate\nself.assertFalse(calc.resolve(context))", "path": "lajfstrim\\templatetags\\smart_if.py", "repo_name": "kkszysiu/lifepress", "stars": 19, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"Sends a result to the server, returning a Deferred that fires with\na bool to indicate whether or not the work was accepted.\n\"\"\"\n\n# Must be a 128-byte response, but the last 48 are typically ignored.\n", "func_signal": "def sendResult(self, result):\n", "code": "result += '\\x00'*48\n\nd = self.agent.request(\n    'POST',\n    self.baseURL + self.basePath,\n    Headers(\n        {'User-Agent': [self.version],\n        'Authorization': [self.auth],\n        'Content-Type': ['application/json'],\n        }),\n    GetWorkProducer(result))\n\ndef callback(response):\n    d = defer.Deferred()\n    response.deliverBody(BodyLoader(d))\n    return d\nd.addCallback(callback)\nd.addCallback(self._processSubmissionResponse)\nreturn d", "path": "src\\minerutil\\RPCProtocol.py", "repo_name": "CFSworks/multiminer", "stars": 27, "license": "None", "language": "python", "size": 250}
{"docstring": "\"\"\"Writes a configuration variable to the database.\n\nThe value is type-converted to a string for storage.\n\"\"\"\n# There might be an old definition, so take it out if so.\n", "func_signal": "def setConfig(self, var, value):\n", "code": "self.db.execute('DELETE FROM config WHERE var=?;', (var,))\nif value is not None: # Setting to None means the variable gets cleared.\n    self.db.execute('INSERT INTO config (var,value) VALUES (?,?);',\n                   (var, str(value)))\n\n# Now inform any waiting callbacks...\nfor callback in self.configCallbacks.get(var, []):\n    callback()", "path": "src\\ClusterServer.py", "repo_name": "CFSworks/multiminer", "stars": 27, "license": "None", "language": "python", "size": 250}
{"docstring": "\"\"\"Retrieves an up-to-date WorkUnit from the provider. The unit is not\nreturned directly, but as a Deferred.\n\nThis function tries to return work with a mask of desiredMask, but it\ndoes not guarantee the mask size (it could be smaller, if the provider\nhas a shortage of queued work units)\n\"\"\"\n\n", "func_signal": "def getWork(self, desiredMask):\n", "code": "if not self.work:\n    # Completely out of work, must defer.\n    d = defer.Deferred()\n    self.deferreds.append((d, desiredMask))\n    return d\n\n# Strategy #1: Iterate through work list, find the first (that is,\n# newest and smallest) available WorkUnit that is big enough, then\n# subdivide it until it matches the size of desiredMask.\nfor unit in self.work:\n    if unit.mask < desiredMask:\n        continue\n    \n    # Found a unit big enough, pull it off the list and start splitting\n    self.work.remove(unit)\n    while unit.mask > desiredMask:\n        unit, other = unit.split()\n        self.work.append(other)\n    self.work.sort() # TODO: Insert work in place so we don't have to\n                     # sort afterward.\n    self.checkWork()\n    return defer.succeed(unit)\n\n# Strategy #2: There are no big enough units left, so just get the\n# biggest (and newest, if there is a tie for biggest) unit.\nworkBySize = sorted(self.work, key=lambda x: x.mask, reverse=True)\nself.work.remove(workBySize[0])\nself.checkWork()\nreturn defer.succeed(workBySize[0])", "path": "src\\WorkProvider.py", "repo_name": "CFSworks/multiminer", "stars": 27, "license": "None", "language": "python", "size": 250}
{"docstring": "\"\"\"Sets up the server to listen on a port and starts all subsystems.\"\"\"\n", "func_signal": "def start(self):\n", "code": "port = self.getConfig('server_port', int, 8880)\nip = self.getConfig('server_ip', str, '')\nreactor.listenTCP(port, self, interface=ip)\n\nself.web = WebServer(self)\nself.web.start()\n\nself.workProvider.start()", "path": "src\\ClusterServer.py", "repo_name": "CFSworks/multiminer", "stars": 27, "license": "None", "language": "python", "size": 250}
{"docstring": "\"\"\"Something just worked right, tell the application if we haven't\nalready.\n\"\"\"\n", "func_signal": "def _success(self):\n", "code": "if self.connected or not self.active:\n    return\n# We got the connection back, but if the user doesn't want an askrate,\n# stop asking...\nif self.polling.running and not self.askrate:\n    self.polling.stop()\nself.runCallback('connect')\nself.connected = True", "path": "src\\minerutil\\RPCProtocol.py", "repo_name": "CFSworks/multiminer", "stars": 27, "license": "None", "language": "python", "size": 250}
{"docstring": "\"\"\"Check a result against a specified target. If no target is\nspecified, the WorkUnit's own target is used.\n\nThis function also verifies that the result is related to this WorkUnit\nbefore checking the hash.\n\"\"\"\n\n", "func_signal": "def checkResult(self, result, target=None):\n", "code": "if target is None:\n    target = self.target\n\nif len(result) != len(self.data):\n    return False\n\nif result[:76] != self.data[:76]:\n    return False\n\nmaskBits = (1<<self.mask)-1\nresultNonce, = struct.unpack('<I', result[76:80])\n\nif (self.getNonce() | maskBits) != (resultNonce | maskBits):\n    return False\n\n# Swap the result now; Bitcoin treats SHA-256 as if it loads words\n# in little-endian, but Python's (true) implementation of SHA-256\n# will load the words big-endian.\nswappedResult = ''\nfor i in range(80):\n    swappedResult += result[i^3]\n\nhash = hashlib.sha256(hashlib.sha256(swappedResult).digest()).digest()\n\n# Compare every byte in the target and hash in little-endian order\nfor t,h in zip(target[::-1], hash[::-1]):\n    if ord(t) > ord(h):\n        return True\n    elif ord(t) < ord(h):\n        return False\nreturn True", "path": "src\\WorkUnit.py", "repo_name": "CFSworks/multiminer", "stars": 27, "license": "None", "language": "python", "size": 250}
{"docstring": "\"\"\"Gets a connection by its session ID.\"\"\"\n", "func_signal": "def getConnection(self, sessionno):\n", "code": "for w in self.workers:\n    if w.transport.sessionno == sessionno:\n        return w", "path": "src\\ClusterServer.py", "repo_name": "CFSworks/multiminer", "stars": 27, "license": "None", "language": "python", "size": 250}
{"docstring": "\"\"\"Called by the backend when it receives a new WorkUnit.\"\"\"\n\n", "func_signal": "def onWork(self, wu):\n", "code": "work = WorkUnit(self, wu.data, wu.target, wu.mask)\n\nself.workRequested = False\n\n# Check if this work is similar (that is, same prev. block) to the\n# template. The template is the WorkUnit to which all buffered work\n# must be similar.\nif self.template is not None and self.template.isSimilarTo(work):\n    self.work.append(work)\n    self.work.sort()\nelse:\n    # Not similar. Reset the buffer, and inform every connected worker\n    # that it needs to send new work.\n    self.template = work\n    self.work = [work]\n    for worker in self.server.workers:\n        worker.sendWork()\n\nself.checkWork()\n\n# If there are deferreds, take care of them (unless the work buffer\n# runs dry again)\nwhile self.deferreds and self.work:\n    d, mask = self.deferreds.pop(0)\n    self.getWork(mask).chainDeferred(d)", "path": "src\\WorkProvider.py", "repo_name": "CFSworks/multiminer", "stars": 27, "license": "None", "language": "python", "size": 250}
{"docstring": "\"\"\"Retrieves all configuration values from the database.\"\"\"\n", "func_signal": "def getAllConfig(self):\n", "code": "config = {}\nfor var, value in self.db.execute('SELECT var, value FROM config;'):\n    config[var] = value\nreturn config", "path": "src\\ClusterServer.py", "repo_name": "CFSworks/multiminer", "stars": 27, "license": "None", "language": "python", "size": 250}
{"docstring": "\"\"\"Handle an unparsed JSON-RPC response to submitting work.\"\"\"\n\n", "func_signal": "def _processSubmissionResponse(self, body):\n", "code": "result = self._parseJSONResult(body)\nif result is None:\n    return False\n\nreturn bool(result)", "path": "src\\minerutil\\RPCProtocol.py", "repo_name": "CFSworks/multiminer", "stars": 27, "license": "None", "language": "python", "size": 250}
{"docstring": "\"\"\"Reads a configuration variable out of the database.\n\nWill attempt to convert it into the specified type. If the variable\nis not found, or type conversion fails, returns the default.\n\"\"\"\n# Take care of the callback first, before encountering any returns...\n", "func_signal": "def getConfig(self, var, type=str, default=None, callback=None):\n", "code": "if callable(callback):\n    callbacks = self.configCallbacks.setdefault(var, [])\n    if callback not in callbacks:\n        callbacks.append(callback)\n\n# This should only loop once.\nfor value, in self.db.execute('SELECT value FROM config WHERE var=? '\n                              'LIMIT 1;', (var,)):\n    try:\n        value = type(value)\n    except (TypeError, ValueError):\n        return default\n    else:\n        return value # Type-converted\n\n# Variable is not present in the database.\nreturn default", "path": "src\\ClusterServer.py", "repo_name": "CFSworks/multiminer", "stars": 27, "license": "None", "language": "python", "size": 250}
{"docstring": "\"\"\"Starts the WorkProvider; creates and establishes the backend\nconnection.\n\"\"\"\n\n", "func_signal": "def start(self):\n", "code": "url = self.server.getConfig('backend_url', str,\n    'http://bitcoin:bitcoin@127.0.0.1:8332/', callback=self.start)\n\n# If there is already a backend, be sure to disconnect it first,\n# because this function is also called as a callback when backend_url\n# changes.\nif self.backend:\n    self.backend.disconnect()\n\nself.backend = openURL(url, self)\nself.backend.setVersion('multiminer', 'Multiminer Server', '%d.%d' %\n    self.server.versionNumber)\nself.backend.connect()", "path": "src\\WorkProvider.py", "repo_name": "CFSworks/multiminer", "stars": 27, "license": "None", "language": "python", "size": 250}
{"docstring": "\"\"\"Compare implemented so that WorkUnits are sorted with the newest\nat the beginning of a list. If two WorkUnits share an equal age, then\nthe smaller WorkUnit comes first.\n\"\"\"\n# The timestamps are swapped so they get sorted in descending order.\n", "func_signal": "def __cmp__(self, other):\n", "code": "comparison = cmp(other.getTimestamp(), self.getTimestamp())\n\n# If the user wants the work buffer to be a fifo, swap the comparison\n# back to ascending order.\nif self.provider.server.getConfig('work_fifo', int, 0):\n    comparison = -comparison\n\nif comparison != 0:\n    return comparison\nelse:\n    return cmp(self.mask, other.mask) # Not swapped: Ascending order.", "path": "src\\WorkUnit.py", "repo_name": "CFSworks/multiminer", "stars": 27, "license": "None", "language": "python", "size": 250}
{"docstring": "\"\"\"Called by the backend when it successfully connects.\n\nIt is not necessarily logged in yet. After this callback, it will\nattempt to log in, and then call onWork when it gets its initial work.\n\"\"\"\n", "func_signal": "def onConnect(self):\n", "code": "self.work = []\nself.template = None", "path": "src\\WorkProvider.py", "repo_name": "CFSworks/multiminer", "stars": 27, "license": "None", "language": "python", "size": 250}
{"docstring": "\"\"\"Fires a getwork request at the server. The rpc argument indicates\nwhether a JSONRPC request is needed or not.\n\"\"\"\n\n# Only one RPC request should run at a time... Long-poll requests are\n# limited outside of this function.\n", "func_signal": "def _startRequest(self, rpc=True):\n", "code": "if rpc:\n    if self.requesting:\n        return\n    self.requesting = True\n\n# There are some differences between long-poll and RPC, which are\n# sorted out here.\nmethod = 'POST' if rpc else 'GET'\ncontentType = ['application/json'] if rpc else []\nbody = GetWorkProducer() if rpc else None\nif rpc:\n    url = self.baseURL + self.basePath\nelse:\n    parsedLP = urlparse.urlparse(self.longPollPath)\n    parsedBase = urlparse.urlparse(self.baseURL)\n    scheme = parsedLP.scheme or parsedBase.scheme\n    netloc = parsedLP.netloc or parsedBase.netloc\n    path = parsedLP.path\n    query = parsedLP.query\n    url = urlparse.urlunparse((scheme, netloc, path, '', query, ''))\n\nd = self.agent.request(\n    method,\n    url,\n    Headers(\n        {'User-Agent': [self.version],\n        'Authorization': [self.auth],\n        'Content-Type': contentType,\n        }),\n    body)\n\n\ndef callback(response):\n    d = defer.Deferred()\n    response.deliverBody(BodyLoader(d))\n    d.addCallback(lambda x: self._processResponse(x, not rpc))\n    # Headers are not read until after the response is processed,\n    # so that application callbacks are in a sensible order.\n    d.addCallback(lambda x: self._readHeaders(response, rpc) or x)\n    return d\nd.addCallback(callback)\nd.addErrback(lambda x: self._failure())\nif rpc:\n    def both(ignored):\n        self.requesting = False\n        return ignored\n    d.addBoth(both)\nreturn d", "path": "src\\minerutil\\RPCProtocol.py", "repo_name": "CFSworks/multiminer", "stars": 27, "license": "None", "language": "python", "size": 250}
{"docstring": "\"\"\"New block on the Bitcoin network; inform connected workers.\"\"\"\n\n", "func_signal": "def onBlock(self, block):\n", "code": "if block == self.block:\n    return\n\nself.block = block\nfor worker in self.server.workers:\n    worker.sendBlock()", "path": "src\\WorkProvider.py", "repo_name": "CFSworks/multiminer", "stars": 27, "license": "None", "language": "python", "size": 250}
{"docstring": "\"\"\"Tells the RPCClient that it's time to start communicating with the\nserver.\n\"\"\"\n\n", "func_signal": "def connect(self):\n", "code": "if self.active:\n    return\nself.active = True\n\nif not self.polling.running and self.askrate:\n    self.polling.start(self.askrate, True)\nelse:\n    self._startRequest()", "path": "src\\minerutil\\RPCProtocol.py", "repo_name": "CFSworks/multiminer", "stars": 27, "license": "None", "language": "python", "size": 250}
{"docstring": "\"\"\"Shuts down the RPCClient. It should not be connect()ed again.\"\"\"\n\n", "func_signal": "def disconnect(self):\n", "code": "if not self.active:\n    return\nself.active = False\n\nif self.connected:\n    self.connected = False\n    self.runCallback('disconnect')\n\nif self.polling.running:\n    self.polling.stop()\nself._setLongPollingPath(None)", "path": "src\\minerutil\\RPCProtocol.py", "repo_name": "CFSworks/multiminer", "stars": 27, "license": "None", "language": "python", "size": 250}
{"docstring": "\"\"\"Called by a worker connection when it finds a full-difficulty work\nsolution.\n\"\"\"\n\n", "func_signal": "def sendResult(self, result):\n", "code": "if self.backend:\n    self.backend.sendResult(result)", "path": "src\\WorkProvider.py", "repo_name": "CFSworks/multiminer", "stars": 27, "license": "None", "language": "python", "size": 250}
{"docstring": "\"\"\"List every connected, logged-in worker using the specified username.\nThe username is case-sensitive.\n\"\"\"\n", "func_signal": "def listAccountConnections(self, username):\n", "code": "return filter(lambda x: x.account and x.account.username == username,\n              self.workers)", "path": "src\\ClusterServer.py", "repo_name": "CFSworks/multiminer", "stars": 27, "license": "None", "language": "python", "size": 250}
{"docstring": "\"\"\"\nInitialise the lexical unit.\n\"\"\"\n\n", "func_signal": "def __init__(self):\n", "code": "dict.__init__(self)\n\nself['ID'] = None\nself['name'] = None\nself['pos'] = None\nself['definition'] = None\nself['frame'] = None\nself['incorporatedFE'] = None\n\nself['lexemes'] = []\nself['subcorpora'] = {}\n\nself['annotationSets'] = {}\n\nself['layers'] = {}", "path": "framenet\\lexical_unit.py", "repo_name": "dasmith/FrameNet-python", "stars": 26, "license": "mit", "language": "python", "size": 68114}
{"docstring": "\"\"\"\nLoads annotation set.\n\"\"\"\n", "func_signal": "def loadAnnotationSet(self, annoNode, corpusName):\n", "code": "annotation = {}\ntry:\n    annotation['ID'] = int(annoNode.attributes['ID'].value)\nexcept:\n    print >> sys.stderr, '>>> Unable to read the annoNode id'\n    return False\n\ntry:\n    annotation['status'] = str(annoNode.attributes['status'].value)\nexcept:\n    print >> sys.stderr, '>>> Unable to read the annoNode status'\n    return False\n\ngoodNodes = filter(lambda x:x.nodeType != x.TEXT_NODE, annoNode.childNodes)\n\nfor node in goodNodes:\n    if node.nodeName == 'layers':\n        if annotation.has_key('layers'):\n            print >> sys.stderr, '>>>> Already has a Layers key!'\n            return False\n        elif not self.loadLayers(node, annotation):\n            return False\n    elif node.nodeName == 'sentence':\n        if annotation.has_key('sentence'):\n            print >> sys.stderr, '>>>> Already has a sentences key!'\n            return False\n        elif not self.loadSentence(node, annotation):\n            return False\n\nself['annotationSets'][annotation['ID']] = annotation\nreturn True", "path": "framenet\\lexical_unit.py", "repo_name": "dasmith/FrameNet-python", "stars": 26, "license": "mit", "language": "python", "size": 68114}
{"docstring": "\"\"\"\nThis function will look up a given word by its pos. The word must be already\nlemmatised and in lower case. The pos must also be in lower case and it can be\none of the following:\n(1) v  -- for verb\n(2) n  -- for noun\n(3) a  -- for adjective\n(4) adv -- for adverb\n(5) prep -- for preposition\n(6) num -- for numbers\n(7) intj -- for interjections\n\nThis function will return a dictionary of lexical units which match the (headWord, pos)\npair. The keys to the dictionary will be the IDs of the lexical units, and the values of\nthe dictionary will be LexicalUnit objects.\n\"\"\"\n\n", "func_signal": "def lookupLexicalUnit(self, headWord, pos):\n", "code": "pickledLUPath = FRAMENET_PATH + '/' + LU_DIR_ENV\n\nw = headWord + '.' + pos\n\nif self['luCache'].has_key(w):\n    return self['luCache'][w]\n\nif not self['luIndex'].has_key(w):\n    return {}\n\nobjects = {}\nfor _id in self['luIndex'][w].keys():\n    inputFile = pickledLUPath + '/lu' + str(_id) + '.xml'\n    lu = LexicalUnit()\n    lu.loadXML(inputFile)\n    objects[lu['ID']] = lu\n\nself['luCache'][w] = objects\nreturn objects", "path": "framenet\\framenet.py", "repo_name": "dasmith/FrameNet-python", "stars": 26, "license": "mit", "language": "python", "size": 68114}
{"docstring": "\"\"\"\nLoads the sentence\n\"\"\"\n", "func_signal": "def loadSentence(self, sentNode, annotation):\n", "code": "goodNodes = filter(lambda x:x.nodeType != x.TEXT_NODE, sentNode.childNodes)\n\nif len(goodNodes) != 1:\n    print >> sys.stderr, '>> Sent node has:', len(goodNodes), 'good nodes'\n    return False\n\nsent = {}\ntry:\n    sent = loadXMLAttributes(sent, sentNode.attributes)\nexcept:\n    print >> sys.stderr, '>> Unable to get one of ID or aPos'\n    return False\n\nsent['text'] = goodNodes[0].childNodes[0].nodeValue\n\nif not isinstance(sent['text'], unicode):\n    print >> sys.stderr, '>> Unable to get the sentence text from', goodNodes[0].childNodes[0]\n    return False\n\ntry:\n    sent['text'] = str(sent['text'])\nexcept:\n    pass\n\nannotation['sentence'] = sent\n\nreturn True", "path": "framenet\\lexical_unit.py", "repo_name": "dasmith/FrameNet-python", "stars": 26, "license": "mit", "language": "python", "size": 68114}
{"docstring": "\"\"\"\n\"\"\"\n\n", "func_signal": "def loadXML(self, fileName):\n", "code": "doc = xml.dom.minidom.parse(fileName)\n\nrelationTypeNodes = getNoneTextChildNodes(doc.childNodes[1]) #the actual frame-relation-type nodes\n\nfor rtn in relationTypeNodes:\n    rt = {} #dictionary for frame-relation-type\n    loadXMLAttributes(rt, rtn.attributes)\n    relationNodes = getNoneTextChildNodes(rtn) #the actual frame-relationS nodes\n    if len(relationNodes) != 1:\n        print >> sys.stderr, 'Got more than one frame-relations node in type:', rt['name']\n        return False\n    singleRelationNodes = getNoneTextChildNodes(relationNodes[0]) # the actual frame-relation nodes\n\n    singleRelations = {}\n    i = 0\n    for srn in singleRelationNodes:\n        tmp = self.loadSingleRelation(srn)\n        if tmp == None:\n            print >> sys.stderr, 'Unable to load relation No.' + str(i), 'for type', rt['name']\n            return False\n        singleRelations[tmp['ID']] = tmp\n\n    rt['frame-relations'] = singleRelations\n    self['relation-types'][rt['ID']] = rt\n\nreturn True", "path": "framenet\\frame_relation.py", "repo_name": "dasmith/FrameNet-python", "stars": 26, "license": "mit", "language": "python", "size": 68114}
{"docstring": "\"\"\"\nInitialises all the frames\n\"\"\"\n\n", "func_signal": "def _generatePickledFrames(self):\n", "code": "framePath = FRAMENET_PATH+ '/' + FRAME_DIR_ENV + '/' + FRAME_FILE\npickledFramePath = FRAMENET_PATH + '/' + PICKLED_FRAME_FILE\n\nprint >> sys.stderr, 'Loading xml for frames ...',\ndoc = xml.dom.minidom.parse(framePath)\nframeNodes = getNoneTextChildNodes(doc.childNodes[1])\nprint >> sys.stderr, 'done',\n\nframes = {}\nprint >> sys.stderr, 'parsing each frame ...',\nfor fn in frameNodes:\n    f = Frame()\n    f.loadXMLNode(fn)\n    frames[f['ID']] = f\nprint >> sys.stderr, 'done',\n\nprint >> sys.stderr, 'saving the frames ...',\ncPickle.dump(frames, open(pickledFramePath, 'w'), cPickle.HIGHEST_PROTOCOL)\nprint >> sys.stderr, 'done'\n\npass", "path": "framenet\\framenet.py", "repo_name": "dasmith/FrameNet-python", "stars": 26, "license": "mit", "language": "python", "size": 68114}
{"docstring": "\"\"\"\nLoads the ID, name, frame, POS and incorporateFE attributes of the main document node\n\"\"\"\n\n# Load the ID\n", "func_signal": "def loadMainAttributes(self, doc):\n", "code": "if doc.attributes.has_key('ID'):\n    self['ID'] = int(doc.attributes['ID'].value)\nelse:\n    print >> sys.stderr, '>> Unable to load the ID node'\n    return False\n\n# Load the HeadWord\nif doc.attributes.has_key('name'):\n    n = doc.attributes['name'].value\n    self['name'] = str(n)\nelse:\n    print >> sys.stderr, '>> Unable to load the name'\n    return False\n\n# Load the POS\nif doc.attributes.has_key('pos'):\n    self['pos'] = str(doc.attributes['pos'].value)\nelse:\n    print >> sys.stderr, '>> Unable to load the POS'\n    return False\n\n# Load the frame\nif doc.attributes.has_key('frame'):\n    self['frame'] = str(doc.attributes['frame'].value)\nelse:\n    print >> sys.stderr, '>> Unable to load the frame'\n    return False\n\n# Load the incorporatedFE\nif doc.attributes.has_key('incorporatedFE'):\n    self['incorporatedFE'] = str(doc.attributes['incorporatedFE'].value)\nelse:\n    print >> sys.stderr, '>> Unable to load the incorporatedFE'\n    return False\n\nreturn True", "path": "framenet\\lexical_unit.py", "repo_name": "dasmith/FrameNet-python", "stars": 26, "license": "mit", "language": "python", "size": 68114}
{"docstring": "\"\"\"\nLoads the lexemes\n\"\"\"\n\n", "func_signal": "def loadLexemes(self, lexemeNodes):\n", "code": "lexemeChildNodes = filter(lambda x:x.nodeType != x.TEXT_NODE, lexemeNodes.childNodes)\n\n# initialize default list\nself['lexemes'] = []\n    \nfor lexemeNode in lexemeChildNodes:\n    lexeme = {}\n    try:\n        lexeme['lexeme'] = lexemeNode.childNodes[0].nodeValue\n    except:\n        print >> sys.stderr, '>> Unable to load the lexemeNode lexeme:', lexemeNode\n        return False\n    try:\n        lexeme['ID'] = int(lexemeNode.attributes['ID'].value)\n    except:\n        print >> sys.stderr, '>> Unable to read the lexemeNode id'\n        return False\n    try:\n        lexeme['pos'] = str(lexemeNode.attributes['pos'].value)\n    except:\n        print >> sys.stderr, '>> Unable to read the lexemeNode pos'\n        return False\n    try:\n        lexeme['breakBefore'] = bool(lexemeNode.attributes['breakBefore'].value)\n    except:\n        print >> sys.stderr, '>> Unable to read the lexemeNode breakBefore'\n        return False\n    try:\n        lexeme['headword'] = bool(lexemeNode.attributes['headword'].value)\n    except:\n        print >> sys.stderr, '>> Unable to read the lexemeNode headword'\n        return False  \n    self['lexemes'].append(lexeme)        \n\nreturn True", "path": "framenet\\lexical_unit.py", "repo_name": "dasmith/FrameNet-python", "stars": 26, "license": "mit", "language": "python", "size": 68114}
{"docstring": "\"\"\"\nLoads the subcorpus node\n\"\"\"\n\n", "func_signal": "def loadSubcorpus(self, subCorpusNode):\n", "code": "try:\n    corpusName = str(subCorpusNode.attributes['name'].value)\nexcept:\n    print >> sys.stderr, '>> Unable to load the name of the subcorpus node', \n    return False\n\nself['subcorpora'][corpusName] = {}\n\nannotationNodes = filter(lambda x:x.nodeType != x.TEXT_NODE, subCorpusNode.childNodes)\n\nfor annoNode in annotationNodes:\n    if not self.loadAnnotationSet(annoNode, corpusName):\n        print >> sys.stderr, '>> Unable to load an annotation set'\n        return False\nreturn True", "path": "framenet\\lexical_unit.py", "repo_name": "dasmith/FrameNet-python", "stars": 26, "license": "mit", "language": "python", "size": 68114}
{"docstring": "\"\"\"\nInitialises all the frames\n\"\"\"\n\n", "func_signal": "def _generatePickledLexicalUnitsIndex(self):\n", "code": "baseDir = FRAMENET_PATH + '/' + LU_DIR_ENV\npickledLUPath = FRAMENET_PATH + '/' + PICKLED_LU_FILE\n\nlexicalUnitsIndexByName = {}\nfor _f in os.listdir(baseDir):\n    if _f.lower().startswith('lu') and _f.lower().endswith('.xml'):\n        print >> sys.stderr, 'Loading:', _f, \": \",\n        lu = LexicalUnit()\n        lu.loadXML(baseDir + '/' + _f)\n        print >> sys.stderr, lu['name'], '...', \n        print >> sys.stderr, 'done'\n\n        if lexicalUnitsIndexByName.has_key(lu['name']):\n            lexicalUnitsIndexByName[lu['name']][lu['ID']] = 1\n        else:\n            lexicalUnitsIndexByName[lu['name']] = {lu['ID']:1}\n\nprint >> sys.stderr, 'Saving the pickled lu files ...',\ncPickle.dump(lexicalUnitsIndexByName, open(pickledLUPath, 'w'), cPickle.HIGHEST_PROTOCOL)\nprint >> sys.stderr, 'done'\npass", "path": "framenet\\framenet.py", "repo_name": "dasmith/FrameNet-python", "stars": 26, "license": "mit", "language": "python", "size": 68114}
{"docstring": "\"\"\"\nLoads the definition node\n\"\"\"\n", "func_signal": "def loadDefinition(self, defNode):\n", "code": "if len(defNode.childNodes) > 0:\n    try:\n        self['definition'] = defNode.childNodes[0].nodeValue\n    except:\n        print >> sys.stderr, '>> Unable to load the definition node of:', defNode\n        return False\nreturn True", "path": "framenet\\lexical_unit.py", "repo_name": "dasmith/FrameNet-python", "stars": 26, "license": "mit", "language": "python", "size": 68114}
{"docstring": "\"\"\"\nLoads the layers <layers>\n\"\"\"\n\n", "func_signal": "def loadLayers(self, layersNode, annotation):\n", "code": "layers = filter(lambda x:x.nodeType != x.TEXT_NODE, layersNode.childNodes)\n\nsingleLayers = {}\n\nfor layer in layers:\n    loaded = self.loadSingleLayer(layer)\n\n    if loaded == None:\n        print >> sys.stderr, '>>> Unable to load a layer!'\n        return False\n    singleLayers[loaded['ID']] = loaded\n\nannotation['layers'] = singleLayers\n\nreturn True", "path": "framenet\\lexical_unit.py", "repo_name": "dasmith/FrameNet-python", "stars": 26, "license": "mit", "language": "python", "size": 68114}
{"docstring": "\"\"\"\nConstructor, doesn't do much\n\"\"\"\n\n", "func_signal": "def __init__(self):\n", "code": "dict.__init__(self)\n\nself['relation-types'] = {}", "path": "framenet\\frame_relation.py", "repo_name": "dasmith/FrameNet-python", "stars": 26, "license": "mit", "language": "python", "size": 68114}
{"docstring": "\"\"\"\n\"\"\"\n\n", "func_signal": "def loadSingleRelation(self, relationNode):\n", "code": "frRelation = {}\n\nloadXMLAttributes(frRelation, relationNode.attributes)\n\nfeNodes = getNoneTextChildNodes(relationNode)\nfor fn in feNodes:\n    tmp = {}\n    loadXMLAttributes(tmp, fn.attributes)\n    frRelation[tmp['ID']] = tmp\n\nreturn frRelation", "path": "framenet\\frame_relation.py", "repo_name": "dasmith/FrameNet-python", "stars": 26, "license": "mit", "language": "python", "size": 68114}
{"docstring": "\"\"\"\n\"\"\"\n\n", "func_signal": "def initialize():\n", "code": "fn = FrameNet()\nfn.initialize()\n\npass", "path": "framenet\\framenet.py", "repo_name": "dasmith/FrameNet-python", "stars": 26, "license": "mit", "language": "python", "size": 68114}
{"docstring": "\"\"\"\nInitialises all the frames\n\"\"\"\n\n", "func_signal": "def _generatePickledFrameRelations(self):\n", "code": "frRelationPath = FRAMENET_PATH + '/' + FRAME_DIR_ENV + '/' + FRAME_RELATION_FILE\npickledFrameRelationPath = FRAMENET_PATH + '/' + PICKLED_FRAME_RELATIONS_FILE\n\nprint >> sys.stderr, 'Loading xml for frame relations ...',\nfrRelations = FrameRelation()\nfrRelations.loadXML(frRelationPath)\nprint >> sys.stderr, 'done',\n\nprint >> sys.stderr, 'saving the frame relationships ...',\ncPickle.dump(frRelations, open(pickledFrameRelationPath, 'w'), cPickle.HIGHEST_PROTOCOL)\nprint >> sys.stderr, 'done'\n\npass", "path": "framenet\\framenet.py", "repo_name": "dasmith/FrameNet-python", "stars": 26, "license": "mit", "language": "python", "size": 68114}
{"docstring": "\"\"\"\nThis function takes a string as input, the string should be the name\nof the Frame that you want to lookup, and its case sensitive. If not\nfound, None will be returned.\n\"\"\"\n\n", "func_signal": "def lookupFrame(self, frame):\n", "code": "if self['frameIndex'].has_key(frame):\n    return self['frameIndex'][frame]\n\nreturn None", "path": "framenet\\framenet.py", "repo_name": "dasmith/FrameNet-python", "stars": 26, "license": "mit", "language": "python", "size": 68114}
{"docstring": "\"\"\"\nConstructor, loads the frames and the frame relations.\n\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "pickledFramePath = FRAMENET_PATH + '/' + PICKLED_FRAME_FILE\npickledFrameRelationPath = FRAMENET_PATH  + '/' + PICKLED_FRAME_RELATIONS_FILE\npickledLUPath = FRAMENET_PATH + '/' + PICKLED_LU_FILE\n    \ntry:\n    print >> sys.stderr, 'Loading the frames ...',\n    frames = cPickle.load(open(pickledFramePath))\n    self['frames'] = frames\n    print >> sys.stderr, 'done'\n    \n    print >> sys.stderr, 'Loading the frame relations ...',\n    frRelations = cPickle.load(open(pickledFrameRelationPath))\n    self['frameRelations'] = frRelations\n    print >> sys.stderr, 'done'\n\n    print >> sys.stderr, 'Loading the lexical units data ...',\n    luData = cPickle.load(open(pickledLUPath))\n    print >> sys.stderr, 'done'\n    self['luIndex'] = luData\n    \nexcept:\n    print >> sys.stderr, 'Framenet not initialized, doing it now'\n    self.initialize()\n    print >> sys.stderr, 'Loading the frames ...',\n    frames = cPickle.load(open(pickledFramePath))\n    self['frames'] = frames\n    print >> sys.stderr, 'done'\n    \n    print >> sys.stderr, 'Loading the frame relations ...',\n    frRelations = cPickle.load(open(pickledFrameRelationPath))\n    self['frameRelations'] = frRelations\n    print >> sys.stderr, 'done'\n    \n    print >> sys.stderr, 'Loading the lexical units data ...',\n    luData = cPickle.load(open(pickledLUPath))\n    print >> sys.stderr, 'done'\n    self['luIndex'] = luData\n\nself['luCache'] = {}\nself._generateFrameIndex()\npass", "path": "framenet\\framenet.py", "repo_name": "dasmith/FrameNet-python", "stars": 26, "license": "mit", "language": "python", "size": 68114}
{"docstring": "\"\"\"\nGenerate an index from frame name to Frame objects\n\"\"\"\n\n", "func_signal": "def _generateFrameIndex(self):\n", "code": "self['frameIndex'] = {}\n\nfor _id in self['frames'].keys():\n    f = self['frames'][_id]\n    name = f['name']\n    if self['frameIndex'].has_key(name):\n        print >> sys.stderr, 'Error, multiple frame name:', name, 'found'\n        sys.exit()\n\n    self['frameIndex'][name] = f\n\npass", "path": "framenet\\framenet.py", "repo_name": "dasmith/FrameNet-python", "stars": 26, "license": "mit", "language": "python", "size": 68114}
{"docstring": "\"\"\" Returns a list of the tokens that evoke this lexical unit, given\nan optional pos tag to filter the lexemes by. \"\"\"\n", "func_signal": "def getLexemes(self,pos=None):\n", "code": "lex = {}\nfor l in self['lexemes']:\n    if pos != None and l['pos'] != pos: continue\n    lex[l['lexeme']] = l        \nreturn lex", "path": "framenet\\lexical_unit.py", "repo_name": "dasmith/FrameNet-python", "stars": 26, "license": "mit", "language": "python", "size": 68114}
{"docstring": "'''\nGiven a fb query, looks up all of the properties associated with that object. \nThese properties can become result_args fields.\n\nFormat: /api/freebaselookupprops?args={arg1:val1,arg2:val1,...}\n\ntested:\ncurl \"http://127.0.0.1:8000/api/freebaselookupprops?args=id:/en/the_beatles\"\ncurl \"http://127.0.0.1:8000/api/freebaselookupprops?args=id:/en/the_beatles,type:/music/artist\"\n\n'''\n\n", "func_signal": "def freebaseLookupProps(self, request, obj_url):\n", "code": "query_args={}\nquery_args_str = request.GET['args']\nfor a in query_args_str.split(','):\n    query_args[a.split(':')[0]]=a.split(':')[1]\n\nreturn '{ The result set has the following properties: '+str(MQLQuery.view_props(query_args))+'}'", "path": "conceptdb\\api\\handlers.py", "repo_name": "commonsense/conceptdb", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 6108}
{"docstring": "\"\"\"\nDo a random walk from a given node. The direction of the initial step\nmay be specified or chosen randomly.\n\nIf it is not possible to walk in the given direction, the walk will\nterminate.\n\"\"\"\n", "func_signal": "def corona_random_walk(graph, node, direction=None):\n", "code": "if direction is None:\n    direction = random.choice([UP, DOWN])\n\nif direction == UP:\n    choices = graph.predecessors(node)\n    accumulator = 'steps_up'\nelif direction == DOWN:\n    choices = graph.successors(node)\n    accumulator = 'steps_down'\nelse:\n    raise ValueError(\"Invalid direction\")\n\nprobability_target = random.random()\ntotal_weight = 0.0\n\nfor next_node in choices:\n    total_weight += edge_weight(graph, node, next_node, direction)\nif total_weight == 0:\n    # Terminate the walk here.\n    return\n\nprobability_used = 0.0\nfor next_node in choices:\n    probability_used += edge_weight(graph, node, next_node, direction) / total_weight\n    if probability_target < probability_used:\n        graph.node[next_node][accumulator] += 1\n        graph.graph[accumulator] += 1\n        return corona_random_walk(graph, next_node)\nassert False, \"Ran out of probability\"", "path": "experiments\\inference\\corona.py", "repo_name": "commonsense/conceptdb", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 6108}
{"docstring": "\"\"\"Method called to look up an assertion by its attributes, when the id \nnumber is not known.  Accessed by going to URL /api/assertionfind.  Must\ninclude parameters for the dataset, relation, concepts, polarity, and context.  \n\n/api/assertionfind?dataset={datasetname}&rel={relation}&concepts={concept1,concept2,etc}\n&polarity={polarity}&context={context}\n\nPolarity and context are optional parameters, defaulting to polarity = 1 and context = None\n\"\"\"\n\n", "func_signal": "def assertionFind(self, request, obj_url):\n", "code": "dataset = request.GET['dataset']\nrelation = request.GET['rel']\nargstr = request.GET['concepts']\npolarity = float(request.GET.get('polarity',1))\ncontext = request.GET.get('context','None')\n\nif context == 'None':\n    context = None\n\ntry:\n    return Assertion.objects.get(\n        dataset = dataset,\n        relation = relation,\n        argstr = argstr,\n        polarity = polarity,\n        context = context).serialize()\nexcept DoesNotExist:\n    return rc.NOT_FOUND", "path": "conceptdb\\api\\handlers.py", "repo_name": "commonsense/conceptdb", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 6108}
{"docstring": "\"\"\"\nTake in an object and get a string representing its ID. If it's already\nan ID string, leave it alone.\n\"\"\"\n", "func_signal": "def ensure_reference(obj):\n", "code": "if isinstance(obj, basestring):\n    return obj\nelif isinstance(obj, ConceptDBDocument):\n    return obj.name\nelse:\n    raise ValueError(\"Don't know how to reference %s\" % obj)", "path": "conceptdb\\util.py", "repo_name": "commonsense/conceptdb", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 6108}
{"docstring": "\"\"\"Given a factor in a ReasonConjunction object, returns all of the things\nthat the reason has been used to justify. Currently returns a list of the things\nthat use it in form {assertions: [list of assertions], sentence: [list of sentences],\nexpression: [list of expressions]}.  If the reason has also been used to \njustify things that are not in the database (for instance Users), it will\ninform you but not return the other items.  I might change this later.  \n\nURL must take the form /api/factorusedfor/{reason id}\"\"\"\n\n#must look for the reason being used in Assertion, Sentence, and Expression\n#TODO: should there be a limit on the number of things returned,  maybe also\n#sorted by confidence scores?  \n\n", "func_signal": "def factorUsedFor(self, obj_url):\n", "code": "factorName = obj_url.replace('/factorusedfor', '')\nassertions = [] #list of assertion id's with obj_url as justification\nexpressions = [] #list of expressions with obj_url as justification\nsentences = [] #list of sentences with obj_url as justification\nother = False #\ncursor = ReasonConjunction.objects._collection.find({'factors':factorName})\nwhile(True):\n    try:\n        next_item = cursor.next()['target']\n        \n        #if target was the document itself, change to document name\n        if isinstance(next_item, ConceptDBDocument):\n            next_item = next_item.name\n\n        if isinstance(next_item, basestring) == False:\n            #not a ConceptDBDocument.\n            other = True\n            continue;\n\n        #go through and add assertion ids to assertion list,\n        #expression ids to expression list,\n        #sentence ids to sentence list.  \n        if next_item.startswith('/assertion'):\n            assertions.append(next_item.replace('/assertion/', ''))\n        elif next_item.startswith('/expression'):\n            expressions.append(next_item.replace('/expression/', ''))\n        elif next_item.startswith('/sentence/'):\n            sentences.append(next_item.replace('/sentence/', ''))\n        else: #not a database item\n            other = True\n    except StopIteration:\n        break\n\n\nif (len(sentences) == len(assertions) == len(expressions) == 0) and (other == False):\n    #not used to justify anything\n    return rc.NOT_FOUND\n\nret = \"{'assertions':\" + str(assertions) + \", 'sentences':\" + str(sentences) + \", 'expressions':\" + str(expressions) + \"}\"\n\nif other:\n    ret = ret + \"\\nThis reason is also used to justify non-database items.\"\n\nreturn ret", "path": "conceptdb\\api\\handlers.py", "repo_name": "commonsense/conceptdb", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 6108}
{"docstring": "\"\"\"This method takes the unique identifiers of an assertion as its arguments:\ndataset, relation, concepts, context, and polarity.  It checks to see if this\nassertion exists.  If it does not, it creates it and adds the submitting user's\nvote as a justification.  If it exists already, it adds the submitting user's\nvote as a justification.\n\nAccessed by going to the URL\n/api/assertionmake?dataset={dataset}&rel={relation}&concepts={concept1,concept2,etc}&\npolarity={polarity}&context={context}&user={username}&password={password}\n\nPolarity and context are optional, defaulting to polarity = 1 context = None\n\"\"\"\n", "func_signal": "def assertionMake(self, request, obj_url):\n", "code": "dataset = request.POST['dataset']\nrelation = request.POST['rel']\nargstr = request.POST['concepts']\narguments = argstr.split(',')\npolarity = int(request.POST.get('polarity','1'))\ncontext = request.POST.get('context','None')\nuser = request.POST['user']\npassword = request.POST['password']\n\nif context == \"None\":\n    context = None\n# TODO: uncomment to take into account user and password?\nif User.objects.get(username=user).check_password(password):\n    #the user's password is correct.  Get their reason and add\n    \n    try:\n        user_reason = ReasonConjunction.objects.get(target=dataset + '/contributor/' + user)\n    except DoesNotExist:\n        return rc.FORBIDDEN\nelse:\n    #incorrect password\n    return rc.FORBIDDEN\n\ntry:\n    assertion = Assertion.objects.get(\n        dataset = dataset,\n        relation = relation,\n        argstr = argstr,\n        polarity = polarity,\n        context = context)\n    \n    assertion.add_support([dataset + '/contributor/' + user]) \n    return \"The assertion you created already exists.  Your vote for this \\\n    assertion has been counted.\\n\" + str(assertion.serialize())\n\nexcept DoesNotExist:\n    assertion = Assertion.make(dataset = dataset,\n                arguments = arguments,\n                relation = relation,\n                polarity = polarity,\n                context = context)\n    \n    assertion.add_support([dataset + '/contributor/' + user]) \n\n    return assertion.serialize()", "path": "conceptdb\\api\\handlers.py", "repo_name": "commonsense/conceptdb", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 6108}
{"docstring": "\"\"\"\nDeletes the database with a given name. Requires admin access.\n\"\"\"\n", "func_signal": "def drop_mongodb(dbname, host=None, username=None, password=None):\n", "code": "if dbname in IMPORTANT_DATABASES:\n    raise ValueError(\"I'm sorry, Dave, I can't let you do that.\")\nhost = host or db_config.MONGODB_HOST\nusername = username or db_config.MONGODB_USER\npassword = password or db_config.MONGODB_PASSWORD\n\nconn = pymongo.Connection(host=host)\nconn.admin.authenticate(username, password)\nconn.drop_database(dbname)", "path": "conceptdb\\__init__.py", "repo_name": "commonsense/conceptdb", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 6108}
{"docstring": "\"\"\"\nGet the confidence value for a given ID.\n\"\"\"\n", "func_signal": "def get(object_id):\n", "code": "entry = ConfidenceValue.objects.with_id(object_id)\nif entry is None:\n    return ConfidenceValue.DEFAULT_CONFIDENCE\nelse:\n    return entry.confidence", "path": "conceptdb\\justify.py", "repo_name": "commonsense/conceptdb", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 6108}
{"docstring": "\"\"\"Method allows you to look up a ReasonConjunction by its id.  \nAccessed by going to URL /api/reason/{id}\n\"\"\"\n", "func_signal": "def reasonLookup(self, obj_url):\n", "code": "try:\n    return ReasonConjunction.objects.get(obj_url.replace('/reason/', '')).serialize()\nexcept DoesNotExist:\n    return rc.NOT_FOUND", "path": "conceptdb\\api\\handlers.py", "repo_name": "commonsense/conceptdb", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 6108}
{"docstring": "# TODO: more consistency checks\n", "func_signal": "def check_consistency(self):\n", "code": "assert (self.polarity == 1 or self.polarity == 0 or self.polarity == -1) #valid polarity\nassert (self.complete == 1 or self.complete == 0) #valid boolean value\n\n#maybe there should be checks with relation to # of arguments\n#how will more than 2 concepts as arguments work? 1 specific\n#example was VSO, where 3 concepts would map to a relation\n#I would put in a check which makes sure that there are the\n#correct number of concepts for a given relation.", "path": "conceptdb\\assertion.py", "repo_name": "commonsense/conceptdb", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 6108}
{"docstring": "\"\"\"\nGet the weight of the edge from `n1` to `n2`, taking conjunctions\ninto account.\n\"\"\"\n", "func_signal": "def edge_weight(graph, n1, n2, direction=DOWN):\n", "code": "if direction == UP:\n    n1, n2 = n2, n1\nedge_data = graph[n1][n2]\nmultiplier = 1.0\nif 'dependencies' in edge_data:\n    baseline = node_weight(graph, n1)\n    inv_parallel = 0.0\n    for n3 in edge_data['dependencies']:\n        weight = node_weight(graph, n3)\n        if weight <= 0:\n            # This depends on a conjunction that cannot be satisfied.\n            return 0.0\n        inv_parallel += 1.0/weight\n    multiplier = 1.0/inv_parallel/baseline\nreturn edge_data['weight'] * multiplier", "path": "experiments\\inference\\corona.py", "repo_name": "commonsense/conceptdb", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 6108}
{"docstring": "\"\"\"Assertion vote is called whenever someone is voting on an assertion.  It can \nbe accessed in one of 2 ways: voting on an assertion identified directly by its id\nor voting on an assertion identified by its unique attributes. To add a positive vote,\nthe user should make vote=1.  A negative vote is vote=-1.  Any other values will result\nin no action being taken.  \n\nCan be accessed through either of the following URLS:\n/api/assertionvote?dataset={dataset}&rel={relation}&concept={concept1,concept2,etc}\n&polarity={polarity}&context={context}&vote={vote}&user={username}&password={password}\n\npolarity and context are optional values, defaulting to polarity = 1 and context = None\n\n/api/assertionidvote?id={id}&vote={vote}\n\"\"\"\n\n", "func_signal": "def assertionVote(self, request, obj_url):\n", "code": "user = request.POST['user']\npassword = request.POST['password']\n\n \nif obj_url.startswith('/assertionvote'):\n    dataset = request.POST['dataset']\n    relation = request.POST['rel']\n    argstr = request.POST['concepts']\n    polarity = int(request.POST.get('polarity','1'))\n    context = request.POST.get('context','None')\n\n    if context == \"None\":\n        context = None\n\n    \n    try:\n         assertion = Assertion.objects.get(\n             dataset = dataset,\n             relation = relation,\n             argstr = argstr,\n             polarity = polarity,\n             context = context)\n    except DoesNotExist:\n        return rc.NOT_FOUND\nelse:\n    id = request.POST['id']\n\n    try:\n        assertion = Assertion.get(id)\n        dataset = assertion.dataset\n    except DoesNotExist:\n        return rc.NOT_FOUND\n\nif User.objects.get(username=user).check_password(password):\n\n    #the user's password is correct.  Get their reason and add\n    try:\n      ReasonConjunction.objects.get(target = dataset + '/contributor/' + user)\n    except DoesNotExist:\n      return rc.FORBIDDEN\nelse:\n    #incorrect password\n    return rc.FORBIDDEN\n \nvote = request.POST['vote']\nif vote == \"1\": #vote in favor\n    assertion.add_support([dataset + '/contributor/' + user]) \nelif vote == \"-1\": #vote against\n    assertion.add_oppose([dataset + '/contributor/' + user])\nelse: #invalid vote\n    return rc.BAD_REQUEST\n\nreturn assertion.serialize()", "path": "conceptdb\\api\\handlers.py", "repo_name": "commonsense/conceptdb", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 6108}
{"docstring": "# updated to corona2 form.\n", "func_signal": "def make(target, factors, vote):\n", "code": "target = ensure_reference(target)\nfactors = [ensure_reference(f) for f in factors]\nr, _ = ReasonConjunction.objects.get_or_create(\n    target=target,\n    factors__all=factors,\n    defaults={'factors': factors, 'vote': vote}\n)\nif r.id is not None:\n    # FIXME: this minimizes the number of factors at all costs.\n    # This may not be the correct rule, but it's hard to think of\n    # cases where it goes wrong.\n    r.factors = factors\n    r.vote = vote\nreturn r", "path": "conceptdb\\justify.py", "repo_name": "commonsense/conceptdb", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 6108}
{"docstring": "\"\"\"\nGet the conductance from root to each node by solving the electrical\nsystem.\n\"\"\"\n", "func_signal": "def solve_system(self, known_conductances, current_source):\n", "code": "current = np.zeros((len(self.nodes),))\ncurrent[current_source] = 1.0\n\nsystem = self.get_system_matrix(known_conductances)\nA = self.get_edge_matrix()\n\n# Solve the sparse system of linear equations using cg\nnew_potentials = sparse_linalg.cg(system, current)[0]\n\n# A = edges by nodes\ncurrents = -A.dot(new_potentials)\n\npotential_differences = new_potentials[current_source] - new_potentials\ncurrent_magnitude = np.abs(self.get_edge_matrix_transpose()).dot(currents)\nconductance = 1.0/potential_differences\nreturn conductance", "path": "experiments\\inference\\belief_network.py", "repo_name": "commonsense/conceptdb", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 6108}
{"docstring": "\"\"\"\nCalculates the Hamacher product of a list of numbers. The Hamacher product\nis a product-like norm that scales approximately linearly with its\ninput values, so that its outputs are in the same units as its inputs.\n\nWe use it for conjunctions in ConceptDB/CORONA.\n\"\"\"\n", "func_signal": "def hamacher(values):\n", "code": "result = 1.0\nfor val in values:\n    result = (result*val) / (result + val - result*val)\nreturn result", "path": "conceptdb\\justify.py", "repo_name": "commonsense/conceptdb", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 6108}
{"docstring": "# Set up a simple belief network and run it until it converges to\n# something.\n", "func_signal": "def demo():\n", "code": "bn = BeliefNetwork()\nbn.add_nodes(('root', 'A', 'B', 'C', 'D', 'E', 'F', 'G'))\nbn.initialize_matrices()\nbn.add_edge('root', 'A', 0.25)\nbn.add_edge('root', 'B', 0.75)\nbn.add_edge('A', 'C', -1.0)\nbn.add_edge('B', 'C', 1.0)\nbn.add_conjunction(('A', 'B'), 'D', 1.0)\nbn.add_edge('A', 'E', 1.0)\nbn.add_edge('B', 'E', 1.0)\nbn.add_conjunction(('C', 'G'), 'F', 1.0)\nbn.add_edge('B', 'G', -1.0)\nbn.iterate(10000)\nreturn bn", "path": "experiments\\inference\\incremental_belief.py", "repo_name": "commonsense/conceptdb", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 6108}
{"docstring": "\"\"\"Method called to look up an assertion by its id number.  Accessed\nby going to URL /api/assertion/{id}.  Returns a serialized version \nof the assertion.\"\"\"\n", "func_signal": "def assertionLookup(self, obj_url):\n", "code": "try:\n    return Assertion.get(obj_url.replace('/assertion/', '')).serialize()    \nexcept DoesNotExist:\n    return rc.NOT_FOUND\nexcept ValidationError: #raised if input is not a valid id\n    return rc.NOT_FOUND", "path": "conceptdb\\api\\handlers.py", "repo_name": "commonsense/conceptdb", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 6108}
{"docstring": "'''\nGiven a fb query, and a property, looks up all of the possible values that can \nbe inserted for that property, i.e. type='/common/topic','/music/artist', ...\n\nFormat: /api/freebaselookupentities?args={arg1:val1,arg2:val1,...}&property={prop}\n\ntested:\ncurl \"http://127.0.0.1:8000/api/freebaselookupentities?args=id:/en/the_beatles&property=type\"\n'''\n\n", "func_signal": "def freebaseLookupEntities(self, request, obj_url):\n", "code": "query_args={}\nquery_args_str = request.GET['args']\nfor a in query_args_str.split(','):\n    query_args[a.split(':')[0]]=a.split(':')[1]\n\nproperty = request.GET['property']\n\nreturn '{ The property %s can be assigned the following entities: %s}'%(property,str(MQLQuery.view_entities(query_args, property)))", "path": "conceptdb\\api\\handlers.py", "repo_name": "commonsense/conceptdb", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 6108}
{"docstring": "\"\"\"\nStore the confidence value for a given ID.\n\"\"\"\n", "func_signal": "def set(object_id, confidence):\n", "code": "query = ConfidenceValue.objects(object_id=object_id)\nquery.update_one(upsert=True, safe_update=False,\n                 set__confidence=confidence)\nobj = dereference(object_id)\nif obj is not None:\n    obj.confidence = confidence", "path": "conceptdb\\justify.py", "repo_name": "commonsense/conceptdb", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 6108}
{"docstring": "\"\"\"\nUpdate and return the confidence value of this object.\n\"\"\"\n", "func_signal": "def update_confidence(self):\n", "code": "self.confidence = ConfidenceValue.calculate(self.name)\nreturn self.confidence", "path": "conceptdb\\justify.py", "repo_name": "commonsense/conceptdb", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 6108}
{"docstring": "\"\"\"\nTest that registration still functions properly when\n``django.contrib.sites`` is not installed; the fallback will\nbe a ``RequestSite`` instance.\n\n\"\"\"\n", "func_signal": "def test_registration_no_sites(self):\n", "code": "Site._meta.installed = False\n\nnew_user = self.backend.register(_mock_request(),\n                                 username='bob',\n                                 email='bob@example.com',\n                                 password1='secret')\n\nself.assertEqual(new_user.username, 'bob')\nself.failUnless(new_user.check_password('secret'))\nself.assertEqual(new_user.email, 'bob@example.com')\n\nself.failIf(new_user.is_active)\n\nself.assertEqual(RegistrationProfile.objects.count(), 1)\nself.assertEqual(len(mail.outbox), 1)\n\nSite._meta.installed = True", "path": "ATP_Performance_Test\\ext\\django_registration\\registration\\tests\\backends.py", "repo_name": "tkopczuk/ATP_Performance_Test", "stars": 27, "license": "None", "language": "python", "size": 300}
{"docstring": "\"\"\"\nTest that asking for a post-activation redirect from this\nbackend is an error.\n\n\"\"\"\n", "func_signal": "def test_post_activation_redirect(self):\n", "code": "self.assertRaises(NotImplementedError, self.backend.post_activation_redirect,\n                  request=_mock_request(), user=User())", "path": "ATP_Performance_Test\\ext\\django_registration\\registration\\tests\\backends.py", "repo_name": "tkopczuk/ATP_Performance_Test", "stars": 27, "license": "None", "language": "python", "size": 300}
{"docstring": "\"\"\"\nTest the activation process: trying to activate outside the\npermitted window fails, and leaves the account inactive.\n\n\"\"\"\n", "func_signal": "def test_invalid_activation(self):\n", "code": "expired_user = self.backend.register(_mock_request(),\n                                     username='bob',\n                                     email='bob@example.com',\n                                     password1='secret')\n\nexpired_user.date_joined = expired_user.date_joined - datetime.timedelta(days=settings.ACCOUNT_ACTIVATION_DAYS)\nexpired_user.save()\nexpired_profile = RegistrationProfile.objects.get(user=expired_user)\nself.failIf(self.backend.activate(_mock_request(),\n                                  expired_profile.activation_key))\nself.failUnless(expired_profile.activation_key_expired())", "path": "ATP_Performance_Test\\ext\\django_registration\\registration\\tests\\backends.py", "repo_name": "tkopczuk/ATP_Performance_Test", "stars": 27, "license": "None", "language": "python", "size": 300}
{"docstring": "\"\"\"\nCreate and immediately log in a new user.\n\n\"\"\"\n", "func_signal": "def register(self, request, **kwargs):\n", "code": "username, email, password = kwargs['username'], kwargs['email'], kwargs['password1']\nUser.objects.create_user(username, email, password)\n\n# authenticate() always has to be called before login(), and\n# will return the user we just created.\nnew_user = authenticate(username=username, password=password)\nlogin(request, new_user)\nsignals.user_registered.send(sender=self.__class__,\n                             user=new_user,\n                             request=request)\nreturn new_user", "path": "ATP_Performance_Test\\ext\\django_registration\\registration\\backends\\simple\\__init__.py", "repo_name": "tkopczuk/ATP_Performance_Test", "stars": 27, "license": "None", "language": "python", "size": 300}
{"docstring": "\"\"\"\nTest that a backend module which exists but does not have a\nclass of the specified name raises the correct exception.\n\n\"\"\"\n", "func_signal": "def test_backend_attribute_error(self):\n", "code": "self.assertRaises(ImproperlyConfigured, get_backend,\n                  'registration.backends.default.NonexistentBackend')", "path": "ATP_Performance_Test\\ext\\django_registration\\registration\\tests\\backends.py", "repo_name": "tkopczuk/ATP_Performance_Test", "stars": 27, "license": "None", "language": "python", "size": 300}
{"docstring": "\"\"\"\nDetermine whether this ``RegistrationProfile``'s activation\nkey has expired, returning a boolean -- ``True`` if the key\nhas expired.\n\nKey expiration is determined by a two-step process:\n\n1. If the user has already activated, the key will have been\n   reset to the string constant ``ACTIVATED``. Re-activating\n   is not permitted, and so this method returns ``True`` in\n   this case.\n\n2. Otherwise, the date the user signed up is incremented by\n   the number of days specified in the setting\n   ``ACCOUNT_ACTIVATION_DAYS`` (which should be the number of\n   days after signup during which a user is allowed to\n   activate their account); if the result is less than or\n   equal to the current date, the key has expired and this\n   method returns ``True``.\n\n\"\"\"\n", "func_signal": "def activation_key_expired(self):\n", "code": "expiration_date = datetime.timedelta(days=settings.ACCOUNT_ACTIVATION_DAYS)\nreturn self.activation_key == self.ACTIVATED or \\\n       (self.user.date_joined + expiration_date <= datetime.datetime.now())", "path": "ATP_Performance_Test\\ext\\django_registration\\registration\\models.py", "repo_name": "tkopczuk/ATP_Performance_Test", "stars": 27, "license": "None", "language": "python", "size": 300}
{"docstring": "\"\"\"\nCreate a ``RegistrationProfile`` for a given\n``User``, and return the ``RegistrationProfile``.\n\nThe activation key for the ``RegistrationProfile`` will be a\nSHA1 hash, generated from a combination of the ``User``'s\nusername and a random salt.\n\n\"\"\"\n", "func_signal": "def create_profile(self, user):\n", "code": "salt = sha_constructor(str(random.random())).hexdigest()[:5]\nusername = user.username\nif isinstance(username, unicode):\n    username = username.encode('utf-8')\nactivation_key = sha_constructor(salt+username).hexdigest()\nreturn self.create(user=user,\n                   activation_key=activation_key)", "path": "ATP_Performance_Test\\ext\\django_registration\\registration\\models.py", "repo_name": "tkopczuk/ATP_Performance_Test", "stars": 27, "license": "None", "language": "python", "size": 300}
{"docstring": "\"\"\"\nRemove expired instances of ``RegistrationProfile`` and their\nassociated ``User``s.\n\nAccounts to be deleted are identified by searching for\ninstances of ``RegistrationProfile`` with expired activation\nkeys, and then checking to see if their associated ``User``\ninstances have the field ``is_active`` set to ``False``; any\n``User`` who is both inactive and has an expired activation\nkey will be deleted.\n\nIt is recommended that this method be executed regularly as\npart of your routine site maintenance; this application\nprovides a custom management command which will call this\nmethod, accessible as ``manage.py cleanupregistration``.\n\nRegularly clearing out accounts which have never been\nactivated serves two useful purposes:\n\n1. It alleviates the ocasional need to reset a\n   ``RegistrationProfile`` and/or re-send an activation email\n   when a user does not receive or does not act upon the\n   initial activation email; since the account will be\n   deleted, the user will be able to simply re-register and\n   receive a new activation key.\n\n2. It prevents the possibility of a malicious user registering\n   one or more accounts and never activating them (thus\n   denying the use of those usernames to anyone else); since\n   those accounts will be deleted, the usernames will become\n   available for use again.\n\nIf you have a troublesome ``User`` and wish to disable their\naccount while keeping it in the database, simply delete the\nassociated ``RegistrationProfile``; an inactive ``User`` which\ndoes not have an associated ``RegistrationProfile`` will not\nbe deleted.\n\n\"\"\"\n", "func_signal": "def delete_expired_users(self):\n", "code": "for profile in self.all():\n    if profile.activation_key_expired():\n        user = profile.user\n        if not user.is_active:\n            user.delete()", "path": "ATP_Performance_Test\\ext\\django_registration\\registration\\models.py", "repo_name": "tkopczuk/ATP_Performance_Test", "stars": 27, "license": "None", "language": "python", "size": 300}
{"docstring": "\"\"\"\nTest the registration process: registration creates a new\ninactive account and a new profile with activation key,\npopulates the correct account data and sends an activation\nemail.\n\n\"\"\"\n", "func_signal": "def test_registration(self):\n", "code": "new_user = self.backend.register(_mock_request(),\n                                 username='bob',\n                                 email='bob@example.com',\n                                 password1='secret')\n\n# Details of the returned user must match what went in.\nself.assertEqual(new_user.username, 'bob')\nself.failUnless(new_user.check_password('secret'))\nself.assertEqual(new_user.email, 'bob@example.com')\n\n# New user must not be active.\nself.failIf(new_user.is_active)\n\n# A registration profile was created, and an activation email\n# was sent.\nself.assertEqual(RegistrationProfile.objects.count(), 1)\nself.assertEqual(len(mail.outbox), 1)", "path": "ATP_Performance_Test\\ext\\django_registration\\registration\\tests\\backends.py", "repo_name": "tkopczuk/ATP_Performance_Test", "stars": 27, "license": "None", "language": "python", "size": 300}
{"docstring": "\"\"\"\nTest that ``RegistrationFormUniqueEmail`` validates uniqueness\nof email addresses.\n\n\"\"\"\n# Create a user so we can verify that duplicate addresses\n# aren't permitted.\n", "func_signal": "def test_registration_form_unique_email(self):\n", "code": "User.objects.create_user('alice', 'alice@example.com', 'secret')\n\nform = forms.RegistrationFormUniqueEmail(data={'username': 'foo',\n                                               'email': 'alice@example.com',\n                                               'password1': 'foo',\n                                               'password2': 'foo'})\nself.failIf(form.is_valid())\nself.assertEqual(form.errors['email'],\n                 [u\"This email address is already in use. Please supply a different email address.\"])\n\nform = forms.RegistrationFormUniqueEmail(data={'username': 'foo',\n                                               'email': 'foo@example.com',\n                                               'password1': 'foo',\n                                               'password2': 'foo'})\nself.failUnless(form.is_valid())", "path": "ATP_Performance_Test\\ext\\django_registration\\registration\\tests\\forms.py", "repo_name": "tkopczuk/ATP_Performance_Test", "stars": 27, "license": "None", "language": "python", "size": 300}
{"docstring": "\"\"\"\nVerify that ``get_backend()`` returns the correct value when\npassed a valid backend.\n\n\"\"\"\n", "func_signal": "def test_get_backend(self):\n", "code": "self.failUnless(isinstance(get_backend('registration.backends.default.DefaultBackend'),\n                           DefaultBackend))", "path": "ATP_Performance_Test\\ext\\django_registration\\registration\\tests\\backends.py", "repo_name": "tkopczuk/ATP_Performance_Test", "stars": 27, "license": "None", "language": "python", "size": 300}
{"docstring": "\"\"\"\nTest that the default post-registration redirect is the public\nURL of the new user account.\n\n\"\"\"\n", "func_signal": "def test_post_registration_redirect(self):\n", "code": "new_user = self.backend.register(_mock_request(),\n                                 username='bob',\n                                 email='bob@example.com',\n                                 password1='secret')\n\nself.assertEqual(self.backend.post_registration_redirect(_mock_request(), new_user),\n                 (new_user.get_absolute_url(), (), {}))", "path": "ATP_Performance_Test\\ext\\django_registration\\registration\\tests\\backends.py", "repo_name": "tkopczuk/ATP_Performance_Test", "stars": 27, "license": "None", "language": "python", "size": 300}
{"docstring": "\"\"\"\nSend an activation email to the user associated with this\n``RegistrationProfile``.\n\nThe activation email will make use of two templates:\n\n``registration/activation_email_subject.txt``\n    This template will be used for the subject line of the\n    email. Because it is used as the subject line of an email,\n    this template's output **must** be only a single line of\n    text; output longer than one line will be forcibly joined\n    into only a single line.\n\n``registration/activation_email.txt``\n    This template will be used for the body of the email.\n\nThese templates will each receive the following context\nvariables:\n\n``activation_key``\n    The activation key for the new account.\n\n``expiration_days``\n    The number of days remaining during which the account may\n    be activated.\n\n``site``\n    An object representing the site on which the user\n    registered; depending on whether ``django.contrib.sites``\n    is installed, this may be an instance of either\n    ``django.contrib.sites.models.Site`` (if the sites\n    application is installed) or\n    ``django.contrib.sites.models.RequestSite`` (if\n    not). Consult the documentation for the Django sites\n    framework for details regarding these objects' interfaces.\n\n\"\"\"\n", "func_signal": "def send_activation_email(self, site):\n", "code": "ctx_dict = {'activation_key': self.activation_key,\n            'expiration_days': settings.ACCOUNT_ACTIVATION_DAYS,\n            'site': site}\nsubject = render_to_string('registration/activation_email_subject.txt',\n                           ctx_dict)\n# Email subject *must not* contain newlines\nsubject = ''.join(subject.splitlines())\n\nmessage = render_to_string('registration/activation_email.txt',\n                           ctx_dict)\n\nself.user.email_user(subject, message, settings.DEFAULT_FROM_EMAIL)", "path": "ATP_Performance_Test\\ext\\django_registration\\registration\\models.py", "repo_name": "tkopczuk/ATP_Performance_Test", "stars": 27, "license": "None", "language": "python", "size": 300}
{"docstring": "\"\"\"\nTest that registering a user sends the ``user_registered``\nsignal.\n\n\"\"\"\n", "func_signal": "def test_registration_signal(self):\n", "code": "def receiver(sender, **kwargs):\n    self.failUnless('user' in kwargs)\n    self.assertEqual(kwargs['user'].username, 'bob')\n    self.failUnless('request' in kwargs)\n    self.failUnless(isinstance(kwargs['request'], WSGIRequest))\n    received_signals.append(kwargs.get('signal'))\n\nreceived_signals = []\nsignals.user_registered.connect(receiver, sender=self.backend.__class__)\n\nself.backend.register(_mock_request(),\n                      username='bob',\n                      email='bob@example.com',\n                      password1='secret')\n\nself.assertEqual(len(received_signals), 1)\nself.assertEqual(received_signals, [signals.user_registered])", "path": "ATP_Performance_Test\\ext\\django_registration\\registration\\tests\\backends.py", "repo_name": "tkopczuk/ATP_Performance_Test", "stars": 27, "license": "None", "language": "python", "size": 300}
{"docstring": "\"\"\"\nTest that the setting ``REGISTRATION_OPEN`` appropriately\ncontrols whether registration is permitted.\n\n\"\"\"\n", "func_signal": "def test_allow(self):\n", "code": "old_allowed = getattr(settings, 'REGISTRATION_OPEN', True)\nsettings.REGISTRATION_OPEN = True\nself.failUnless(self.backend.registration_allowed(_mock_request()))\n\nsettings.REGISTRATION_OPEN = False\nself.failIf(self.backend.registration_allowed(_mock_request()))\nsettings.REGISTRATION_OPEN = old_allowed", "path": "ATP_Performance_Test\\ext\\django_registration\\registration\\tests\\backends.py", "repo_name": "tkopczuk/ATP_Performance_Test", "stars": 27, "license": "None", "language": "python", "size": 300}
{"docstring": "\"\"\"\nTest that a nonexistent/unimportable backend raises the\ncorrect exception.\n\n\"\"\"\n", "func_signal": "def test_backend_error_invalid(self):\n", "code": "self.assertRaises(ImproperlyConfigured, get_backend,\n                  'registration.backends.doesnotexist.NonExistentBackend')", "path": "ATP_Performance_Test\\ext\\django_registration\\registration\\tests\\backends.py", "repo_name": "tkopczuk/ATP_Performance_Test", "stars": 27, "license": "None", "language": "python", "size": 300}
{"docstring": "\"\"\"\nRather than issuing a request and returning the response, this\nsimply constructs an ``HttpRequest`` object and returns it.\n\n\"\"\"\n", "func_signal": "def request(self, **request):\n", "code": "environ = {\n    'HTTP_COOKIE':      self.cookies,\n    'PATH_INFO':         '/',\n    'QUERY_STRING':      '',\n    'REMOTE_ADDR':       '127.0.0.1',\n    'REQUEST_METHOD':    'GET',\n    'SCRIPT_NAME':       '',\n    'SERVER_NAME':       'testserver',\n    'SERVER_PORT':       '80',\n    'SERVER_PROTOCOL':   'HTTP/1.1',\n    'wsgi.version':      (1,0),\n    'wsgi.url_scheme':   'http',\n    'wsgi.errors':       self.errors,\n    'wsgi.multiprocess': True,\n    'wsgi.multithread':  False,\n    'wsgi.run_once':     False,\n    }\nenviron.update(self.defaults)\nenviron.update(request)\nrequest = WSGIRequest(environ)\n\n# We have to manually add a session since we'll be bypassing\n# the middleware chain.\nsession_middleware = SessionMiddleware()\nsession_middleware.process_request(request)\nreturn request", "path": "ATP_Performance_Test\\ext\\django_registration\\registration\\tests\\backends.py", "repo_name": "tkopczuk/ATP_Performance_Test", "stars": 27, "license": "None", "language": "python", "size": 300}
{"docstring": "\"\"\"\nTest that the default post-registration redirect is the named\npattern ``registration_complete``.\n\n\"\"\"\n", "func_signal": "def test_post_registration_redirect(self):\n", "code": "self.assertEqual(self.backend.post_registration_redirect(_mock_request(), User()),\n                 ('registration_complete', (), {}))", "path": "ATP_Performance_Test\\ext\\django_registration\\registration\\tests\\backends.py", "repo_name": "tkopczuk/ATP_Performance_Test", "stars": 27, "license": "None", "language": "python", "size": 300}
{"docstring": "\"\"\"\nTest that the setting ``REGISTRATION_OPEN`` appropriately\ncontrols whether registration is permitted.\n\n\"\"\"\n", "func_signal": "def test_allow(self):\n", "code": "old_allowed = getattr(settings, 'REGISTRATION_OPEN', True)\nsettings.REGISTRATION_OPEN = True\nself.failUnless(self.backend.registration_allowed(_mock_request()))\n\nsettings.REGISTRATION_OPEN = False\nself.failIf(self.backend.registration_allowed(_mock_request()))\nsettings.REGISTRATION_OPEN = old_allowed", "path": "ATP_Performance_Test\\ext\\django_registration\\registration\\tests\\backends.py", "repo_name": "tkopczuk/ATP_Performance_Test", "stars": 27, "license": "None", "language": "python", "size": 300}
{"docstring": "\"\"\"\nTest that ``RegistrationFormNoFreeEmail`` disallows\nregistration with free email addresses.\n\n\"\"\"\n", "func_signal": "def test_registration_form_no_free_email(self):\n", "code": "base_data = {'username': 'foo',\n             'password1': 'foo',\n             'password2': 'foo'}\nfor domain in forms.RegistrationFormNoFreeEmail.bad_domains:\n    invalid_data = base_data.copy()\n    invalid_data['email'] = u\"foo@%s\" % domain\n    form = forms.RegistrationFormNoFreeEmail(data=invalid_data)\n    self.failIf(form.is_valid())\n    self.assertEqual(form.errors['email'],\n                     [u\"Registration using free email addresses is prohibited. Please supply a different email address.\"])\n\nbase_data['email'] = 'foo@example.com'\nform = forms.RegistrationFormNoFreeEmail(data=base_data)\nself.failUnless(form.is_valid())", "path": "ATP_Performance_Test\\ext\\django_registration\\registration\\tests\\forms.py", "repo_name": "tkopczuk/ATP_Performance_Test", "stars": 27, "license": "None", "language": "python", "size": 300}
{"docstring": "\"\"\"Save this object to the database.  Behaves very similarly to\nwhatever collection.save(document) would, ie. does upserts on _id\npresence.  If methods ``pre_save`` or ``post_save`` are defined, those\nare called.  If there is a spec document, then the document is\nvalidated against it after the ``pre_save`` hook but before the save.\"\"\"\n", "func_signal": "def save(self):\n", "code": "if hasattr(self, 'pre_save'):\n    self.pre_save()\ndatabase, collection = self._collection_key.split('.')\nself.validate()\n_id = current()[database][collection].save(dict(self))\nif _id: self._id = _id\nif hasattr(self, 'post_save'):\n    self.post_save()", "path": "micromongo\\models.py", "repo_name": "jmoiron/micromongo", "stars": 26, "license": "mit", "language": "python", "size": 132}
{"docstring": "\"\"\"Test that cursors can be re-iterated.\"\"\"\n", "func_signal": "def test_reiteration(self):\n", "code": "c = connect(*from_env())\ncol = c.test_db.test_collection\n\nclass Foo(Model):\n    collection = col.full_name\n\ncol.save({'foo': [{'one': 1}, 'two', {'three': {'3':4}}]})\ncol.save({'foo': [{'one': [1, 2, {'three': 3, 'four': [1,2,3, {\n    'five': [5]}]}]}]})\n\nfoos = Foo.find()\n\nself.assertEqual(len(list(foos)), 2)\nself.assertEqual(len(list(foos)), 2)", "path": "tests\\test_models.py", "repo_name": "jmoiron/micromongo", "stars": 26, "license": "mit", "language": "python", "size": 132}
{"docstring": "\"\"\"An alternate to ``sort`` which allows you to specify a list\nof fields and use a leading - (minus) to specify DESCENDING.\"\"\"\n", "func_signal": "def order_by(self, *fields):\n", "code": "doc = []\nfor field in fields:\n    if field.startswith('-'):\n        doc.append((field.strip('-'), pymongo.DESCENDING))\n    else:\n        doc.append((field, pymongo.ASCENDING))\nreturn self.sort(doc)", "path": "micromongo\\backend.py", "repo_name": "jmoiron/micromongo", "stars": 26, "license": "mit", "language": "python", "size": 132}
{"docstring": "\"\"\"Test micromongo.VERSION.\"\"\"\n", "func_signal": "def test_version(self):\n", "code": "import setup\nimport micromongo\nself.assertTrue(bool(micromongo.VERSION))\n# make sure that it's the same as the version in setup.py\nself.assertEqual('.'.join(map(str, micromongo.VERSION)), setup.version)", "path": "tests\\test_models.py", "repo_name": "jmoiron/micromongo", "stars": 26, "license": "mit", "language": "python", "size": 132}
{"docstring": "\"\"\"Test basic model wrapping.\"\"\"\n", "func_signal": "def test_basic_wrapping(self):\n", "code": "c = connect(*from_env())\ncol = c.test_db.test_collection\nuuids = [uuid(), uuid(), uuid()]\ncol.save({'docid': 0, 'uuid': uuids[0]})\ncol.save({'docid': 1, 'uuid': uuids[1]})\ncol.save({'docid': 2, 'uuid': uuids[2]})\n\nd1 = col.find_one({'docid': 0})\nself.assertEqual(type(d1), dict)\nself.assertEqual(d1['uuid'], uuids[d1['docid']])\n\nclass Foo(Model):\n    collection = col.full_name\n\n# sanity check the correctness of these documents\nd1 = col.find_one({'docid': 0})\nself.assertEqual(type(d1), Foo)\nself.assertEqual(d1.docid, 0)\nfor i in range(3):\n    d = col.find_one({'docid': i})\n    self.assertEqual(d.uuid, uuids[d.docid])\n\nd2 = col.find_one({'docid': 1})\nd2.uuid = uuid()\nd2.save()\n\nd3 = col.find_one({'docid': 1})\nself.assertEqual(d3.uuid, d2.uuid)\nself.assertTrue(d3.uuid != uuids[d3.docid])", "path": "tests\\test_models.py", "repo_name": "jmoiron/micromongo", "stars": 26, "license": "mit", "language": "python", "size": 132}
{"docstring": "\"\"\"Test the SONManipulator, only with compatible pymongo versions.\"\"\"\n", "func_signal": "def test_son_manipulator(self):\n", "code": "from micromongo.backend import require_manipulator\nif not require_manipulator:\n    return\nc = connect(*from_env())\ncol = c.test_db.test_collection\ncol.save({'docid': 17, 'subdoc': {'test': 1}})\n\nclass Foo(Model):\n    collection = col.full_name\n\n# first, test that saving this mess works once we hvae the class\ncol.save({'docid': 18, 'subdoc': {'test': 1}})\n\nd = col.find_one({'docid': 18})\nd.subdoc.test = 3\nd.save()\n\nd2 = col.find_one({'docid': 18})\nself.assertEqual(d2.subdoc.test, 3)", "path": "tests\\test_models.py", "repo_name": "jmoiron/micromongo", "stars": 26, "license": "mit", "language": "python", "size": 132}
{"docstring": "\"\"\"Create a new instance of this model based on its spec and either\na map or the provided kwargs.\"\"\"\n", "func_signal": "def new(cls, *args, **kwargs):\n", "code": "new = cls(make_default(getattr(cls, 'spec', {})))\nnew.update(args[0] if args and not kwargs else kwargs)\nreturn new", "path": "micromongo\\models.py", "repo_name": "jmoiron/micromongo", "stars": 26, "license": "mit", "language": "python", "size": 132}
{"docstring": "\"\"\"Connect to the database.  Passes arguments along to\n``pymongo.connection.Connection`` unmodified.\n\nThe Connection returned by this proxy method will be used by micromongo\nfor all of its queries.  Micromongo will alter the behavior of this\nconneciton object in some subtle ways;  if you want a clean one, call\n``micromongo.clean_connection`` after connecting.\"\"\"\n", "func_signal": "def connect(*args, **kwargs):\n", "code": "global __connection, __connection_args\n__connection_args = (args, dict(kwargs))\n# inject our class_router\nkwargs['class_router'] = class_router\n__connection = Connection(*args, **kwargs)\nreturn __connection", "path": "micromongo\\models.py", "repo_name": "jmoiron/micromongo", "stars": 26, "license": "mit", "language": "python", "size": 132}
{"docstring": "\"\"\"Create a typecheck from some value ``t``.  This behaves differently\ndepending on what ``t`` is.  It should take a value and return True if\nthe typecheck passes, or False otherwise.  Override ``pre_validate``\nin a child class to do type coercion.\n\n* If ``t`` is a type, like basestring, int, float, *or* a tuple of base\n  types, then a simple isinstance typecheck is returned.\n\n* If ``t`` is a list or tuple of instances, such as a tuple or list of\n  integers or of strings, it's treated as the definition of an enum\n  and a simple \"in\" check is returned.\n\n* If ``t`` is callable, ``t`` is assumed to be a valid typecheck.\n\n* If ``t`` is None, a typecheck that always passes is returned.\n\nIf none of these conditions are met, a TypeError is raised.\n\"\"\"\n", "func_signal": "def typecheck(self, t):\n", "code": "if t is None:\n    return lambda x: True\n\ndef _isinstance(types, value):\n    return isinstance(value, types)\n\ndef _enum(values, value):\n    return value in values\n\nif t.__class__ is type:\n    return partial(_isinstance, t)\nelif isinstance(t, (tuple, list)):\n    if all([x.__class__ is type for x in t]):\n        return partial(_isinstance, t)\n    return partial(_enum, t)\nelif callable(t):\n    return t\nraise TypeError('%r is not a valid field type' % r)", "path": "micromongo\\spec.py", "repo_name": "jmoiron/micromongo", "stars": 26, "license": "mit", "language": "python", "size": 132}
{"docstring": "\"\"\"Test that nested lists get un-documented.\"\"\"\n# NOTE, the saves here would raise an exception and fail the test\n# if the underlying manipulator was not functioning properly\n", "func_signal": "def test_lists(self):\n", "code": "c = connect(*from_env())\ncol = c.test_db.test_collection\n\nclass Foo(Model):\n    collection = col.full_name\n\ncol.save({'foo': [{'one': 1}, 'two', {'three': {'3':4}}]})\ncol.save({'foo': [{'one': [1, 2, {'three': 3, 'four': [1,2,3, {\n    'five': [5]}]}]}]})\n\nself.assertEqual(Foo.find().count(), 2)\n\nfor f in Foo.find():\n    f.save()", "path": "tests\\test_models.py", "repo_name": "jmoiron/micromongo", "stars": 26, "license": "mit", "language": "python", "size": 132}
{"docstring": "\"\"\"Memoizing function.  Potentially not thread-safe, since it will return\nresuts across threads.  Make sure this is okay with callers.\"\"\"\n", "func_signal": "def memoize(function):\n", "code": "_cache = {}\n@wraps(function)\ndef wrapper(*args, **kwargs):\n    key = str(args) + str(kwargs)\n    if key not in _cache:\n        _cache[key] = function(*args, **kwargs)\n    return _cache[key]\nreturn wrapper", "path": "micromongo\\utils.py", "repo_name": "jmoiron/micromongo", "stars": 26, "license": "mit", "language": "python", "size": 132}
{"docstring": "\"\"\"Run a find on this model's collection.  The arguments to ``Model.find``\nare the same as to ``pymongo.Collection.find``.\"\"\"\n", "func_signal": "def find(cls, *args, **kwargs):\n", "code": "database, collection = cls._collection_key.split('.')\nreturn current()[database][collection].find(*args, **kwargs)", "path": "micromongo\\models.py", "repo_name": "jmoiron/micromongo", "stars": 26, "license": "mit", "language": "python", "size": 132}
{"docstring": "\"\"\"Create an empty document that follows spec.  Any field with a default\nwill take that value, required or not.  Required fields with no default\nwill get a value of None.  If your default value does not match your\ntype or otherwise customized Field class, this can create a spec that\nfails validation.\"\"\"\n", "func_signal": "def make_default(spec):\n", "code": "doc = {}\nfor key, field in spec.iteritems():\n    if field.default is not no_default:\n        doc[key] = field.default\nreturn doc", "path": "micromongo\\spec.py", "repo_name": "jmoiron/micromongo", "stars": 26, "license": "mit", "language": "python", "size": 132}
{"docstring": "\"\"\"Test that nested dicts get un-documented.\"\"\"\n# NOTE, the saves here would raise an exception and fail the test\n# if the underlying manipulator was not functioning properly\n", "func_signal": "def test_dicts(self):\n", "code": "c = connect(*from_env())\ncol = c.test_db.test_collection\n\nclass Foo(Model):\n    collection = col.full_name\n\ncol.save({'foo': {'bar': {'baz': 1}}})\ncol.save({'foo': 1, 'bar': {'baz': 1}})\n\nself.assertEqual(Foo.find().count(), 2)\n\nfor f in Foo.find():\n    f.save()", "path": "tests\\test_models.py", "repo_name": "jmoiron/micromongo", "stars": 26, "license": "mit", "language": "python", "size": 132}
{"docstring": "\"\"\"Get host/port settings from the environment.\"\"\"\n", "func_signal": "def from_env():\n", "code": "if 'MICROMONGO_URI' in os.environ:\n    return (os.environ['MICROMONGO_URI'],)\nhost = os.environ.get('MICROMONGO_HOST', 'localhost')\nport = int(os.environ.get('MICROMONGO_PORT', 27017))\nreturn (host, port)", "path": "micromongo\\backend.py", "repo_name": "jmoiron/micromongo", "stars": 26, "license": "mit", "language": "python", "size": 132}
{"docstring": "\"\"\"Run a find_one on this model's collection.  The arguments to\n``Model.find_one`` are the same as to ``pymongo.Collection.find_one``.\"\"\"\n", "func_signal": "def find_one(cls, *args, **kwargs):\n", "code": "database, collection = cls._collection_key.split('.')\nreturn current()[database][collection].find_one(*args, **kwargs)", "path": "micromongo\\models.py", "repo_name": "jmoiron/micromongo", "stars": 26, "license": "mit", "language": "python", "size": 132}
{"docstring": "\"\"\"Test the un-camel-casing routines in util.\"\"\"\n", "func_signal": "def test_uncamel(self):\n", "code": "from micromongo.utils import uncamel\nself.assertEquals(uncamel('CamelCase'), 'camel_case')\nself.assertEquals(uncamel('CamelCamelCase'), 'camel_camel_case')\nself.assertEquals(uncamel('already_un'), 'already_un')\nself.assertEquals(uncamel('Capitalized'), 'capitalized')\nself.assertEquals(uncamel('getHTTPResponseCode'), 'get_http_response_code')", "path": "tests\\test_utils.py", "repo_name": "jmoiron/micromongo", "stars": 26, "license": "mit", "language": "python", "size": 132}
{"docstring": "\"\"\"Convert a CamelCase name to a lower_underscore one.  From:\n    http://stackoverflow.com/questions/1175208/\n\"\"\"\n", "func_signal": "def uncamel(name):\n", "code": "s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\nreturn re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()", "path": "micromongo\\utils.py", "repo_name": "jmoiron/micromongo", "stars": 26, "license": "mit", "language": "python", "size": 132}
{"docstring": "\"\"\"Validate a value for this field.  If the field is invalid, this\nwill raise a ValueError.  Runs ``pre_validate`` hook prior to\nvalidation, and returns value if validation passes.\"\"\"\n", "func_signal": "def validate(self, value):\n", "code": "value = self.pre_validate(value)\nif not self._typecheck(value):\n    raise ValueError('%r failed type check' % value)\nreturn value", "path": "micromongo\\spec.py", "repo_name": "jmoiron/micromongo", "stars": 26, "license": "mit", "language": "python", "size": 132}
{"docstring": "\"\"\"A `next` that caches the returned results.  Together with the\nslightly different `__iter__`, these cursors can be iterated over\nmore than once.\"\"\"\n", "func_signal": "def next(self):\n", "code": "if self.__tailable:\n    return PymongoCursor.next(self)\ntry:\n    ret = PymongoCursor.next(self)\nexcept StopIteration:\n    self.__fullcache = True\n    raise\nself.__itercache.append(ret)\nreturn ret", "path": "micromongo\\backend.py", "repo_name": "jmoiron/micromongo", "stars": 26, "license": "mit", "language": "python", "size": 132}
{"docstring": "\"\"\" Determine if a given string is also a docstring. \nAlso, detect if docstring is also the module's docsting.\"\"\"\n", "func_signal": "def doDocString(self, tok):\n", "code": "result = False\nif self.docString and tok.type == token.STRING:\n    tok.semtype = DOCSTRING\n    if self.checkForModuleDocString:  # found the module's doc string\n        self.metrics['numModuleDocStrings'] += 1\n        self.checkForModuleDocString = False\n    self.__postToken( tok )\n    self.docString = False\n    result = True\nreturn result", "path": "PyMetrics\\compute.py", "repo_name": "ipmb/PyMetrics", "stars": 25, "license": "gpl-2.0", "language": "python", "size": 229}
{"docstring": "\"\"\" Place single quotes around strings and escaping existing single quotes.\"\"\"\n", "func_signal": "def sqlQ( s ):\n", "code": "a = s.replace( \"\\\\\",\"\\\\\\\\\" )\na = a.replace( \"'\", \"\\\\'\" )\na = a.replace( '\"', '\\\\\"' )\nreturn '\"'+a+'\"'", "path": "PyMetrics\\utils.py", "repo_name": "ipmb/PyMetrics", "stars": 25, "license": "gpl-2.0", "language": "python", "size": 229}
{"docstring": "\"\"\" Open output file and generate header line, if desired.\"\"\"\n", "func_signal": "def __init__( self, fileName, genHdrSw=True, genNewSw=False ):\n", "code": "self.fileName = fileName\nself.quotedFileName = '\"'+self.fileName+'\"'\nself.IDDateTime = '\"'+time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime())+'\"'\nself.toknum = 0\n\nmode = \"a\"\nif genNewSw:\n    mode = \"w\"\ntry:\n    if self.fileName:\n        self.fd = open( fileName, mode )\n    else:\n        self.fd = sys.stdout\nexcept IOError:\n    raise\n    \nself.writeHdr( genNewSw, genHdrSw )", "path": "PyMetrics\\csvout.py", "repo_name": "ipmb/PyMetrics", "stars": 25, "license": "gpl-2.0", "language": "python", "size": 229}
{"docstring": "\"\"\" Keep track of the number of each operator. Also, handle the\ncase of the colon (:) terminating a class or def header.\"\"\"\n", "func_signal": "def doOperators(self, tok):\n", "code": "result = False\nif tok.type == OP:\n    if tok.text == ':':\n        if self.findFcnHdrEnd:\n            self.findFcnHdrEnd = False\n            self.docString = True\n            self.fcnDepthIncr = 1\n        elif self.findClassHdrEnd:\n            self.findClassHdrEnd = False\n            self.docString = True\n            self.classDepthIncr = 1\n        result = True\n    elif tok.text == '(':\n        self.parenDepth += 1\n    elif tok.text == ')':\n        self.parenDepth -= 1\n    elif tok.text == '[':\n        self.bracketDepth += 1\n    elif tok.text == ']':\n        self.bracketDepth -= 1\n    elif tok.text == '{':\n        self.braceDepth += 1\n    elif tok.text == '}':\n        self.braceDepth -= 1\n    result = True\nreturn result", "path": "PyMetrics\\compute.py", "repo_name": "ipmb/PyMetrics", "stars": 25, "license": "gpl-2.0", "language": "python", "size": 229}
{"docstring": "\"\"\" Initialize all the local variables that will be \nneeded for analysing tokens.:\"\"\"\n", "func_signal": "def __initMetrics( self, metricInstance ):\n", "code": "metricList = []\nfor m in metricInstance.keys():\n    if metricInstance[m]:       # only append valid instances\n        metricList.append( metricInstance[m] )\n# clear out any old data while leaving reference to same \n# thing (ie., pointers to these lists are always valid\ndel self.processSrcLineSubscribers[:]\ndel self.processTokenSubscribers[:]\ndel self.processStmtSubscribers[:]\ndel self.processBlockSubscribers[:]\ndel self.processFunctionSubscribers[:]\ndel self.processClassSubscribers[:]\ndel self.processModuleSubscribers[:]\ndel self.processRunSubscribers[:]\n# since all metric classes are derived from MetricBase,\n# we can assign all the processX functions to all the\n# metrics\nself.processSrcLineSubscribers.extend( metricList )\nself.processTokenSubscribers.extend( metricList )\nself.processStmtSubscribers.extend( metricList )\nself.processBlockSubscribers.extend( metricList )\nself.processFunctionSubscribers.extend( metricList )\nself.processClassSubscribers.extend( metricList )\nself.processModuleSubscribers.extend( metricList )\nself.processRunSubscribers.extend( metricList )\n\nself.numSrcLines = 0\nself.blockDepth = 0\nself.numBlocks = 0\nself.parenDepth = 0\nself.bracketDepth = 0\nself.braceDepth = 0\nself.numNestedClasses = 0\nself.numKeywords = 0\nself.numComments = 0\nself.numEmpty = 0\nself.classDepth = 0\nself.fcnDepth = 0\nself.classDepthIncr = 0\nself.fcnDepthIncr = 0\nself.maxBlockDepth = -1\nself.maxClassDepth = -1\nself.fqnName = []\nself.defFunction = False\nself.defClass = False\nself.docString = True\nself.findFcnHdrEnd = False\nself.findClassHdrEnd = False\nself.inClass = False\nself.inFunction = False\nself.metrics['numSrcLines'] = 0\nself.metrics['numTokens'] = 0\nself.metrics['numComments'] = 0\nself.metrics['numCommentsInline'] = 0\nself.metrics['numModuleDocStrings'] = 0\nself.metrics['numBlocks'] = 0\nself.metrics['numFunctions'] = 0\nself.metrics['numClasses'] = 0\nself.className = None\nself.fcnName = None\nself.saveTok = None\nself.skipUntil = None # used to skip invalid chars until valid char found\nself.invalidToken = None\nself.checkForModuleDocString = True\n\nreturn metricList", "path": "PyMetrics\\compute.py", "repo_name": "ipmb/PyMetrics", "stars": 25, "license": "gpl-2.0", "language": "python", "size": 229}
{"docstring": "\"\"\" Initialize general computational object.\"\"\"\n", "func_signal": "def __init__( self, metricInstance, context, runMetrics, metrics, pa, so, co ):\n", "code": "self.metricInstance = metricInstance\nself.context = context\nself.runMetrics = runMetrics\nself.metrics = metrics\nself.pa = pa\nself.so = so\nself.co = co\n\nself.fqnFunction = []\nself.fqnClass = []\nself.fcnExits = []\n\nself.token = None\nself.stmt = []\nself.block = []\nself.fcn = []\nself.cls = []\nself.mod = []\nself.run = []\n\nself.processSrcLineSubscribers = []\nself.processTokenSubscribers = []\nself.processStmtSubscribers = []\nself.processBlockSubscribers = []\nself.processFunctionSubscribers = []\nself.processClassSubscribers = []\nself.processModuleSubscribers = []\nself.processRunSubscribers = []\n    \nself.__initMetrics( metricInstance )", "path": "PyMetrics\\compute.py", "repo_name": "ipmb/PyMetrics", "stars": 25, "license": "gpl-2.0", "language": "python", "size": 229}
{"docstring": "\"\"\" Handle processing at end of function. \"\"\"\n", "func_signal": "def processFunction( self ):\n", "code": "msg = self.__checkNumberOfExits()\nfcnName = self.__extractFQN( self.fqnFunction )\nclassName = self.__extractFQN( self.fqnClass, None )\nfor subscriber in self.processFunctionSubscribers:\n    subscriber.processFunction( fcnName, className, self.fcn )\n    \nself.fcn[:] = [] # clear out function list\n    \nreturn msg", "path": "PyMetrics\\compute.py", "repo_name": "ipmb/PyMetrics", "stars": 25, "license": "gpl-2.0", "language": "python", "size": 229}
{"docstring": "\"\"\" Handle processing at end of block.\"\"\"\n", "func_signal": "def processBlock( self ):\n", "code": "fcnName = self.__extractFQN( self.fqnFunction )\nclassName = self.__extractFQN( self.fqnClass, None )\nfor subscriber in self.processBlockSubscribers:\n    subscriber.processBlock( fcnName, className, self.block )\n\nself.block[:] = [] # clear out block list", "path": "PyMetrics\\compute.py", "repo_name": "ipmb/PyMetrics", "stars": 25, "license": "gpl-2.0", "language": "python", "size": 229}
{"docstring": "\"\"\" Check for comments and distingish inline comments from\nnormal comments.\"\"\"\n", "func_signal": "def doInlineComment(self, tok, prevTok):\n", "code": "result = False\nif tok.type == COMMENT:\n    self.metrics['numComments'] += 1\n    # compensate for older tokenize including newline\n    # symbols in token when only thing on line is comment\n    # this patch makes all comments consistent\n    if tok.text[-1] == '\\n':\n        tok.text = tok.text[:-1]\n        \n    if prevTok and prevTok.type != NEWLINE and prevTok.type != EMPTY:\n        tok.semtype = INLINE\n        self.metrics['numCommentsInline'] += 1\n    self.__postToken( tok )\n    result = True\n    \nreturn result", "path": "PyMetrics\\compute.py", "repo_name": "ipmb/PyMetrics", "stars": 25, "license": "gpl-2.0", "language": "python", "size": 229}
{"docstring": "\"\"\" Count keywords and check if keyword 'return' used more than\nonce in a given function/method.\"\"\"\n", "func_signal": "def doKeywords(self, tok, prevTok):\n", "code": "if tok.semtype == KEYWORD:\n    self.__incr( 'numKeywords')\n    if tok.text == 'def':\n        self.defFunction = True\n    elif tok.text == 'class':\n        self.defClass = True\n    elif tok.text == 'return':\n        assert self.fcnDepth == len( self.fqnFunction )\n        if self.fcnDepth == 0:       # not in any function\n            if not self.pa.quietSw:  # report on these types of errors\n                print ((\"Module %s contains the return statement at \"+\n                       \"line %d that is outside any function\") % \n                       (self.context['inFile'],tok.row)\n                      )\n        if prevTok.text == ':': \n            # this return on same line as conditional, \n            # so it must be an extra return\n            self.fcnExits.append( tok.row )\n        elif self.blockDepth > 1:\n            # Let fcnBlockDepth be the block depth of the function body.\n            # We are trying to count the number of return statements\n            # in this function. Only one is allowed at the fcnBlockDepth \n            # for the function. If the self.blockDepth is greater than \n            # fcnBlockDepth, then this is a conditional return - i.e., \n            # an additional return\n            fcnBlockDepth = self.fqnFunction[-1][1] + 1\n            if self.blockDepth > fcnBlockDepth:\n                self.fcnExits.append( tok.row )", "path": "PyMetrics\\compute.py", "repo_name": "ipmb/PyMetrics", "stars": 25, "license": "gpl-2.0", "language": "python", "size": 229}
{"docstring": "\"\"\" Write header information for CSV file.\"\"\"\n", "func_signal": "def writeHdr( self, genNewSw, genHdrSw ):\n", "code": "if genNewSw and genHdrSw:\n    fldNames = [\n        '\"IDDateTime\"',\n        '\"tokNum\"',\n        '\"inFile\"',\n        '\"line\"',\n        '\"col\"',\n        '\"tokType\"',\n        '\"semType\"',\n        '\"tokLen\"',\n        '\"token\"',\n        '\"fqnFunction\"',\n        '\"fqnClass\"',\n        '\"blockNum\"',\n        '\"blockDepth\"',\n        '\"fcnDepth\"',\n        '\"classDepth\"',\n        '\"parenDepth\"',\n        '\"bracketDepth\"',\n        '\"braceDepth\"'\n    ]\n    self.fd.write( ','.join( fldNames ) )\n    self.fd.write( '\\n' )", "path": "PyMetrics\\csvout.py", "repo_name": "ipmb/PyMetrics", "stars": 25, "license": "gpl-2.0", "language": "python", "size": 229}
{"docstring": "\"\"\" Handle processing of each physical source line.\n\nThe fcnName and className are meaningless until the\ntokens are evaluated.\n\"\"\"\n", "func_signal": "def processSrcLines( self, srcLine ):\n", "code": "fcnName = self.__extractFQN( self.fqnFunction )\nclassName = self.__extractFQN( self.fqnClass, None )\n\nfor subscriber in self.processSrcLineSubscribers:\n    subscriber.processSrcLines( srcLine )", "path": "PyMetrics\\compute.py", "repo_name": "ipmb/PyMetrics", "stars": 25, "license": "gpl-2.0", "language": "python", "size": 229}
{"docstring": "\"\"\" Close output file if it is not stdout. \"\"\"\n", "func_signal": "def close( self ):\n", "code": "if self.fileName:\n    self.fd.flush()\n    self.fd.close()", "path": "PyMetrics\\csvout.py", "repo_name": "ipmb/PyMetrics", "stars": 25, "license": "gpl-2.0", "language": "python", "size": 229}
{"docstring": "\"\"\" Handle processing at end of class. \"\"\"\n", "func_signal": "def processModule( self ):\n", "code": "moduleName = self.context['inFile']\nmod = self\nfor subscriber in self.processModuleSubscribers:\n    subscriber.processModule( moduleName, mod )\n\nself.mod[:] = []", "path": "PyMetrics\\compute.py", "repo_name": "ipmb/PyMetrics", "stars": 25, "license": "gpl-2.0", "language": "python", "size": 229}
{"docstring": "\"\"\" Handle processing at end of class. \"\"\"\n", "func_signal": "def processRun( self ):\n", "code": "for subsriber in self.processRunSubscribers:\n    subsriber.processRun( self.run )\n    \nself.run[:] = []", "path": "PyMetrics\\compute.py", "repo_name": "ipmb/PyMetrics", "stars": 25, "license": "gpl-2.0", "language": "python", "size": 229}
{"docstring": "\"\"\" Generate the CSV data line.\"\"\"\n", "func_signal": "def write( self, context, tok, fqnFunction, fqnClass ):\n", "code": "self.toknum += 1\ntxt = tok.text\ntt = tok.type\ntn = token.tok_name[tt]\n\nsn = ''\nif tok.semtype:\n    sn = token.tok_name[tok.semtype]\nif tt == token.NEWLINE or tt == tokenize.NL:\n    txt = r'\\n'\n\nsArgs = ','.join( (\n    self.IDDateTime,\n    str( self.toknum ),\n    '\"'+str( context['inFile'] )+'\"',\n    str( tok.row ),\n    str( tok.col ),\n    '\"'+tn+'\"',\n    '\"'+sn+'\"',\n    str( len( txt ) ),\n    csvQ( txt ),\n    '\"'+str( toTypeName( context, fqnFunction ) )+'\"',\n    '\"'+str( toTypeName( context, fqnClass ) )+'\"',\n    str( context['blockNum'] ),\n    str( context['blockDepth'] ),\n    str( context['fcnDepth'] ),\n    str( context['classDepth'] ),\n    str( context['parenDepth'] ),\n    str( context['bracketDepth'] ),\n    str( context['braceDepth'] )\n    ) )\nself.fd.write( sArgs )\nself.fd.write( '\\n' )", "path": "PyMetrics\\csvout.py", "repo_name": "ipmb/PyMetrics", "stars": 25, "license": "gpl-2.0", "language": "python", "size": 229}
{"docstring": "\"\"\" Quote a string using rules for CSV data.\"\"\"\n", "func_signal": "def csvQ( s ):\n", "code": "a = s.replace(\"\\\\\",\"\\\\\\\\\")\nb = a.replace( \"'\", \"\\\\'\" )\nc = b.replace( \"\\n\", \"\\\\n\" )\nd = c.replace( '\"', '\"\"' )\nreturn '\"'+d+'\"'", "path": "PyMetrics\\utils.py", "repo_name": "ipmb/PyMetrics", "stars": 25, "license": "gpl-2.0", "language": "python", "size": 229}
{"docstring": "\"\"\" Truncate long tokens to MAXDISPLAY length.\nAlso, newlines are replace with '\\\\n' so text fits on a line.\"\"\"\n#tmpText = tok.text[:].replace( '\\n', '\\\\n' )\n", "func_signal": "def __fitIn( self, tok ):\n", "code": "tmpText = tok.text[:]\nif len( tmpText ) > MAXDISPLAY:\n    tmpText = tmpText[:10].strip() + ' ... ' + \\\n              tmpText[-10:].strip()\nreturn tmpText", "path": "PyMetrics\\compute.py", "repo_name": "ipmb/PyMetrics", "stars": 25, "license": "gpl-2.0", "language": "python", "size": 229}
{"docstring": "\"\"\" Handle processing after each token processed.\"\"\"\n", "func_signal": "def processToken( self, tok ):\n", "code": "self.token = tok\nself.stmt.append( tok )     # we are always in a statememt\nself.block.append( tok )    # we are always in a block\nif self.fqnFunction:        # we are inside some function\n    self.fcn.append( tok )\nif self.fqnClass:           # we are inside some class\n    self.cls.append( tok )\nself.mod.append( tok )      # we are always in some module\nself.run.append( tok )      # we are always in some run\n\nfcnName = self.__extractFQN( self.fqnFunction )\nclassName = self.__extractFQN( self.fqnClass, None )\nfor subscriber in self.processTokenSubscribers:\n    subscriber.processToken( fcnName, className, self.token )", "path": "PyMetrics\\compute.py", "repo_name": "ipmb/PyMetrics", "stars": 25, "license": "gpl-2.0", "language": "python", "size": 229}
{"docstring": "\"\"\" Increment indent count and record if maximum depth.\"\"\"\n", "func_signal": "def __doIndent(self):\n", "code": "self.__incr( 'numBlocks' )\nself.blockDepth += 1\nself.metrics['blockDepth'] = self.blockDepth\nif self.metrics.get('maxBlockDepth',0) < self.blockDepth:\n    self.metrics['maxBlockDepth'] = self.blockDepth\nreturn True", "path": "PyMetrics\\compute.py", "repo_name": "ipmb/PyMetrics", "stars": 25, "license": "gpl-2.0", "language": "python", "size": 229}
{"docstring": "\"\"\"\nAdd a (doc, labels) pair to the dataset\n\n'labels' can be either a sequence (e.g. [1,2,5],\nor a single value (e.g. True or False)\n\"\"\"\n", "func_signal": "def add(self, doc, labels):\n", "code": "self.docs.append(doc)\nself.labels.append(labels)\nself.digested = False", "path": "mekano\\Dataset.py", "repo_name": "alad/Mekano", "stars": 16, "license": "other", "language": "python", "size": 223}
{"docstring": "\"\"\"Convert an L{AtomVector} from one AtomFactory to another.\n\n@param oldAF            : The old AtomFactory to which AtomVector av belongs\n@param newAF            : The new AtomFactory\n@param av               : The AtomVector to convert\n@return                 : The converted AtomVector\n\"\"\"\n", "func_signal": "def convertAtomVector(oldAF, newAF, av):\n", "code": "new_av = AtomVector(av.name)\nfor a, v in av.iteritems():\n    try:\n        a = convertAtom(oldAF, newAF, a)\n        new_av[a] = v\n    except Exception:       # todo: why are we suppressing the exception ?!\n        pass\nreturn new_av", "path": "mekano\\atoms\\atomfactory.py", "repo_name": "alad/Mekano", "stars": 16, "license": "other", "language": "python", "size": 223}
{"docstring": "# a human readable name\n", "func_signal": "def __init__(self, name=\"\"):\n", "code": "self.name = name\n# the set of all labels; this should have one label if this is\n# a binary label dataset\nself.labelset = set()\n# the list of AtomVectors-like objects\nself.docs = []\n# the list of Labels corresponding to data[]\nself.labels = []\nself.digested = True\n\n# Not all datasets have these\nself.cs = None\nself.catfactory = None\nself.tokenfactory = None", "path": "mekano\\Dataset.py", "repo_name": "alad/Mekano", "stars": 16, "license": "other", "language": "python", "size": 223}
{"docstring": "\"\"\"Create an AtomVector from a string.\n\nTokenizes string 's' using tokenizer, creating\natoms using AtomFactory 'af'.\n\"\"\"\n", "func_signal": "def Vectorize(s, af, tokenizer = WordRegexTokenizer):\n", "code": "av = AtomVector()\nfor word in tokenizer(s):\n    atom = af[word]\n    av[atom] += 1\nreturn av", "path": "mekano\\Textual.py", "repo_name": "alad/Mekano", "stars": 16, "license": "other", "language": "python", "size": 223}
{"docstring": "\"\"\"Write a binary dataset to fout in SVM format.\n\nReturns the byte positions of the labels, which can be used\nby L{toSVMSubsequent}() to overwrite the labels with something\nelse.\n\"\"\"\n", "func_signal": "def toSVM(self, fout):\n", "code": "assert(self.isBinary())\npositions = []\nfor doc, label in self:\n    if label: svm_label = \"+1\"\n    else: svm_label = \"-1\"\n    positions.append(fout.tell())\n    fout.write(\"%s %s\\n\" % (svm_label,\n                            \" \".join([\"%d:%-7.4f\" % (a,v) for a,v in sorted(doc.iteritems())])))\nreturn positions", "path": "mekano\\Dataset.py", "repo_name": "alad/Mekano", "stars": 16, "license": "other", "language": "python", "size": 223}
{"docstring": "\"\"\"Create and return binary datasets.\n\n@return: A C{{k:v}} dictionary where k is a category name, and v is a binary dataset.\n\"\"\"\n\n", "func_signal": "def binarize(self):\n", "code": "self.digest()\n \nassert not self.isBinary(), \"Dataset is already binary\"\n\nname = self.name\nall_labels = self.labelset\n\n# create a dictionary of datasets that we will return\nret = dict([(l,Dataset(\"%s.%s\" % (name, str(l)))) for l in all_labels])\n\nfor doc, doclabels in self:\n    doclabels = set(doclabels)\n    for label in all_labels:\n        if label in doclabels:\n            ret[label].labels.append(True)\n        else:\n            ret[label].labels.append(False)\n\nfor ds in ret.values():\n    # docs are shared!\n    ds.docs = self.docs\n    ds.digest(force=True)\n    ds.catfactory = self.catfactory\n    ds.tokenfactory = self.tokenfactory\n    ds.cs = self.cs\n\nreturn ret", "path": "mekano\\Dataset.py", "repo_name": "alad/Mekano", "stars": 16, "license": "other", "language": "python", "size": 223}
{"docstring": "\"\"\"Extract .c and .j from classifier object and return cmd-line options for SVM.\n\nFor example: -c 1.0 -j 2.0\"\"\"\n\n", "func_signal": "def _svm_params(classifier):\n", "code": "ret = \"-c %f\" % classifier.c\nif classifier.j is not None:\n    ret += \" -j %f\" % classifier.j\nreturn ret", "path": "mekano\\ml\\svm.py", "repo_name": "alad/Mekano", "stars": 16, "license": "other", "language": "python", "size": 223}
{"docstring": "\"\"\"Split on any non-word letter.\n\nWords need not start with [a-z]\n\"\"\"\n", "func_signal": "def BasicTokenizer(s, minlen=1):\n", "code": "for token in wordsplitter_rex.split(s.lower()):\n    if len(token) >= minlen:\n      yield token", "path": "mekano\\Textual.py", "repo_name": "alad/Mekano", "stars": 16, "license": "other", "language": "python", "size": 223}
{"docstring": "\"\"\"Get a dictionary of labels and respective document counts.\n\nThis is an O(n) operation!\n\"\"\"\n", "func_signal": "def getCategoryCounts(self):\n", "code": "self.digest()\ncounts = dict([(l,0) for l in self.labelset])\nfor labels in self.labels:\n    for label in labels:\n        counts[label] += 1\n\nreturn counts", "path": "mekano\\Dataset.py", "repo_name": "alad/Mekano", "stars": 16, "license": "other", "language": "python", "size": 223}
{"docstring": "\"\"\"Run a query against an Indri index.\n\n@param index            : A directory pointing to an Indri index\n@param query            : A string query\n@param printDocuments   : Whether to include document text (available in C{.text} attribute of the returned objects)\n@param printSnippets    : Whether to include document snippets (available in C{.text} attribute) (Only one of these should be true)\n@return                 : A list of L{Result} objects.\n\"\"\"\n", "func_signal": "def runquery(index, query, printDocuments = False, printSnippets = False):\n", "code": "multiline = False\nret = []\nif binaryLocation: cmd = \"%s/runquery\"\nelse: cmd = \"runquery\"\ncmd += \" -index=%s -query='%s'\" % (index, query)\nif printDocuments:\n    cmd += \" -printDocuments=1\"\n    multiline = True\nelif printSnippets:\n    cmd += \" -printSnippets=1\"\n    multiline = True\np = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\nstdout, stderr = p.communicate()\nif not multiline:\n    for line in stdout.split(\"\\n\"):\n        if not line: break\n        score, docid, _, _ = line.split()\n        result = Result()\n        result.score = float(score)\n        result.docid = docid\n        ret.append(result)\nelse:\n    pat = \"-([0-9.]+)\\s+([^\\s]+)\\s+([0-9]+)\\s+([0-9]+)$\"\n    result = None\n    for line in stdout.split(\"\\n\"):\n        line = line.rstrip()\n        if re.match(pat, line):\n            result = Result()\n            score, docid, _, _ = line.split()\n            result.score = float(score)\n            result.docid = docid\n            result.text = \"\"\n            ret.append(result)\n        else:\n            result.text += line + \" \"\nreturn ret", "path": "mekano\\indri.py", "repo_name": "alad/Mekano", "stars": 16, "license": "other", "language": "python", "size": 223}
{"docstring": "\"\"\"Create cross-validation folds.\n\nThe dataset is broken into `count` pieces, each fold (i.e. train-test pair)\nis created by assigning 1 piece to `train`, and `count-1` pieces to `test`.\n\n@param count        : Number of folds\n@return             : A list of [train,test] datasets\n\"\"\"\n", "func_signal": "def kfold(self, count):\n", "code": "subsets = self.subset(count)\nfolds = [[Dataset(), Dataset()] for i in range(count)]\nfor i in range(count):\n    for j in range(count):\n        if i == j:\n            folds[i][1] = subsets[j]\n        else:\n            folds[i][0] += subsets[j]\nreturn folds", "path": "mekano\\Dataset.py", "repo_name": "alad/Mekano", "stars": 16, "license": "other", "language": "python", "size": 223}
{"docstring": "\"\"\"Add two datasets.\n\nIf both datasets are non-empty, then they must be 'compatible',\ni.e., share the same factories and corpus stats.\n\nThe resulting dataset combines the docs and labels, and inherits\nthe factories and corpus stats of the non-empty parent dataset.\n\nIf both parents were L{digest}ed, the resulting dataset is also digested.\n\"\"\"\n", "func_signal": "def __add__(self, other):\n", "code": "result = Dataset()\n# do not add incompatible datasets, unless one of them is empty.\nif len(self.docs) > 0 and len(other.docs) > 0:\n    if self.catfactory != other.catfactory or self.tokenfactory != other.tokenfactory or self.cs != other.cs:\n        raise Exception(\"Incompatible datasets\")\n\nif len(self.docs) > 0:\n    reference_ds = self\nelse:\n    reference_ds = other\n    \nresult.docs = self.docs + other.docs\nresult.labels = self.labels + other.labels\nif self.digested and other.digested:\n    result.labelset = self.labelset | other.labelset\n    result.digested = True\nelse:\n    result.digested = False\n\nresult.catfactory = reference_ds.catfactory\nresult.tokenfactory = reference_ds.tokenfactory\nresult.cs = reference_ds.cs\n\nresult.name = self.name + \"+\" + other.name\n    \nreturn result", "path": "mekano\\Dataset.py", "repo_name": "alad/Mekano", "stars": 16, "license": "other", "language": "python", "size": 223}
{"docstring": "\"\"\"Create a dataset from rainbow's output.\n\n$ rainbow -d model --index 20news/train/*\n$ rainbow -d model --print-matrix=siw > train.txt\n\n    >>> ds = from_rainbow(\"train.txt\")\n\nC{ds.catfactory} holds the L{AtomFactory} for category names.\nC{ds.tokenfactory} holds the L{AtomFactory} for the tokens.\n\nA test set should share its factories with a training set.\nTherefore, read is like so:\n\n    >>> ds2 = from_rainbow(\"testfile.txt\", linkto = ds)\n\n@param filename     : File containing rainbow's output\n@param linkto       : Another dataset whose L{AtomFactory} we should borrow.\n@return             : A brand new dataset.\n\"\"\"\n\n# cdef AtomVector.AtomVector av\n# cdef int i, l, atom\n# cdef double count\n\n", "func_signal": "def from_rainbow(filename, linkto=None):\n", "code": "ds = Dataset(filename)\nif linkto is None:\n    catfactory = AtomFactory(\"cats\")\n    tokenfactory = AtomFactory(\"tokens\")\nelse:\n    catfactory = linkto.catfactory\n    tokenfactory = linkto.tokenfactory\nfin = open(filename, \"r\")\nfor line in fin:\n    a = line.split(None, 2)\n    catatom = catfactory[a[1]]\n    a0 = a[0]\n    p = a0.rfind(\"/\")\n    if p != -1:\n        docname = a0[p+1:]\n    else:\n        docname = a0\n    a = a[2].split()\n    l = len(a)\n    av = AtomVector(name=docname)\n    #for i from 0 <= i < l by 2:\n    for i in range(0,l,2):\n        atom = tokenfactory[a[i]]\n        count = float(a[i+1])\n        av.set(atom, count)\n    ds.add(av,[catatom])\nds.digest()\nds.catfactory = catfactory\nds.tokenfactory = tokenfactory\nreturn ds", "path": "mekano\\Dataset.py", "repo_name": "alad/Mekano", "stars": 16, "license": "other", "language": "python", "size": 223}
{"docstring": "\"\"\"Write a multi-class dataset to fout in SVM format.\n\nThis can be directly consumed by LIBSVM.\n\"\"\"\n", "func_signal": "def toMultiClassSVM(self, fout):\n", "code": "for doc, labels in self:\n    svm_label = labels[0]\n    fout.write(\"%s %s\\n\" % (svm_label,\n                            \" \".join([\"%d:%-7.4f\" % (a,v) for a,v in sorted(doc.iteritems())])))", "path": "mekano\\Dataset.py", "repo_name": "alad/Mekano", "stars": 16, "license": "other", "language": "python", "size": 223}
{"docstring": "\"\"\"Returns a new AtomFactory with the given objects removed.\n\"\"\"\n", "func_signal": "def remove(self, objects):\n", "code": "objects = set(objects)\nnew_af = AtomFactory(self.name)\nfor obj in self.atom_to_obj:\n    if obj not in objects:\n        new_af[obj]\nreturn new_af", "path": "mekano\\atoms\\atomfactory.py", "repo_name": "alad/Mekano", "stars": 16, "license": "other", "language": "python", "size": 223}
{"docstring": "\"\"\"Convert to a weighted (e.g. LTC) dataset\n\n@param cs       : An optional L{CorpusStats} object, otherwise it will be created\n                  an associated with the dataset.\n\n\"\"\"\n\n", "func_signal": "def makeWeighted(self, cs = None):\n", "code": "if cs is None:\n    cs = CorpusStats()\n\nfor doc, doclabels in self:\n    cs.add(doc)\n\nwvc = WeightVectors(cs)\nfor i in range(len(self.docs)):\n    self.docs[i] = wvc[self.docs[i]]\n\nself.cs = cs", "path": "mekano\\Dataset.py", "repo_name": "alad/Mekano", "stars": 16, "license": "other", "language": "python", "size": 223}
{"docstring": "\"\"\"Analyze the data and generate an internal list of labels.\n\nUseful for binarizing etc.\n\"\"\"\n", "func_signal": "def digest(self, force=False):\n", "code": "if self.digested and force==False: return\n\nself.labelset = set()\nfor labels in self.labels:\n    if hasattr(labels, \"__iter__\"):\n        for label in labels:\n            self.labelset.add(label)\n    elif labels:\n        self.labelset.add(labels)\nself.digested = True", "path": "mekano\\Dataset.py", "repo_name": "alad/Mekano", "stars": 16, "license": "other", "language": "python", "size": 223}
{"docstring": "\"\"\"Save each object on a line.\n\nThis should be enough to reconstruct the AtomFactory,\nand is also useful for things like LDA's vocabulary file.\n\"\"\"\n", "func_signal": "def savetxt(self, filename):\n", "code": "with open(filename, \"w\") as fout:\n    for obj in self.atom_to_obj:\n        fout.write(\"%s\\n\" % obj)", "path": "mekano\\atoms\\atomfactory.py", "repo_name": "alad/Mekano", "stars": 16, "license": "other", "language": "python", "size": 223}
{"docstring": "\"\"\"Find 4 or more letter words or numbers/currencies.\n\nWords must start with [a-z]\n\"\"\"\n", "func_signal": "def WordNumberRegexTokenizer(s):\n", "code": "for match in word_number_regex.finditer(s.lower()):\n    yield match.group()", "path": "mekano\\Textual.py", "repo_name": "alad/Mekano", "stars": 16, "license": "other", "language": "python", "size": 223}
{"docstring": "\"\"\"Find 3 or more letter words.\n\nWords must start with [a-z]\n\"\"\"\n", "func_signal": "def WordRegexTokenizer(s):\n", "code": "for match in word_regex.finditer(s.lower()):\n    yield match.group()", "path": "mekano\\Textual.py", "repo_name": "alad/Mekano", "stars": 16, "license": "other", "language": "python", "size": 223}
{"docstring": "\"\"\"\nReset results.\n\"\"\"\n", "func_signal": "def reset( self ):\n", "code": "self.iterations = numpy.int32( 0 )\nself.minimal_error = numpy.float32( 1e12 )\nself.optimal_weights = None\nself.total_time = 0.0\nself.opencl_time = 0.0", "path": "nn\\training.py", "repo_name": "reven86/gpgpu-neuralnet", "stars": 25, "license": "gpl-3.0", "language": "python", "size": 174}
{"docstring": "\"\"\"\nInitialize training method by neural network.\n\n@param context\n    Execution context.\n\"\"\"\n\n", "func_signal": "def prepare_training( self, context ):\n", "code": "self._weights_delta_buf = pyopencl.Buffer( \n    context.opencl.context, pyopencl.mem_flags.READ_WRITE | pyopencl.mem_flags.COPY_HOST_PTR,\n    hostbuf = numpy.zeros( [ context._weights_buf_size ], numpy.float32 )\n    )", "path": "nn\\training.py", "repo_name": "reven86/gpgpu-neuralnet", "stars": 25, "license": "gpl-3.0", "language": "python", "size": 174}
{"docstring": "\"\"\"\nAdjust weights of neural network by certain direction.\n\"\"\"\n", "func_signal": "def adjust_weights( self, context ):\n", "code": "context.opencl.kernel_adjust_weights_quickprop( \n    context.opencl.queue, ( int( context._weights_buf_size ), ),\n    context._gradient_buf,\n    self.prev_direction_buf,\n    self.n, self.alpha,\n    self._weights_delta_buf,\n    context._weights_buf\n    )\n\npyopencl.enqueue_copy_buffer( context.opencl.queue, context._gradient_buf, self.prev_direction_buf )", "path": "nn\\training.py", "repo_name": "reven86/gpgpu-neuralnet", "stars": 25, "license": "gpl-3.0", "language": "python", "size": 174}
{"docstring": "\"\"\"\nInitialize weights of layer by random values\n\"\"\"\n\n", "func_signal": "def randomize_weights( self, context ):\n", "code": "weights = numpy.random.rand( context._weights_buf_size ).astype( numpy.float32 )\nweights -= 0.5\nweights *= 4.0 / numpy.sqrt( numpy.float32( context._weights_buf_size / context._neurons_buf_size ) )\n\npyopencl.enqueue_write_buffer( context.opencl.queue, context._weights_buf, weights, is_blocking = True )", "path": "nn\\training.py", "repo_name": "reven86/gpgpu-neuralnet", "stars": 25, "license": "gpl-3.0", "language": "python", "size": 174}
{"docstring": "\"\"\"\nWait for outputs.\n\"\"\"\n", "func_signal": "def get_outputs( self ):\n", "code": "outputs = numpy.ndarray( [ self.neuron_count ], numpy.float32 )\npyopencl.enqueue_read_buffer( \n    self.opencl.queue, self.context._outputs_buf, outputs,\n    device_offset = int( self._neurons_offset * 4 ), is_blocking = True\n    )\nreturn outputs", "path": "nn\\layer.py", "repo_name": "reven86/gpgpu-neuralnet", "stars": 25, "license": "gpl-3.0", "language": "python", "size": 174}
{"docstring": "\"\"\"\nCreate additional buffers to store learning rate for each weight.\n\n@param layer\n    Input layer.\n\"\"\"\n", "func_signal": "def prepare_training( self, context ):\n", "code": "super( RPROP, self ).prepare_training( context )\n\nself.n_buf = pyopencl.Buffer( \n    context.opencl.context, pyopencl.mem_flags.READ_WRITE | pyopencl.mem_flags.COPY_HOST_PTR,\n    hostbuf = numpy.array( [ self.n ] * context._weights_buf_size, numpy.float32 )\n    )\nself.prev_gradient_buf = pyopencl.Buffer( \n    context.opencl.context, pyopencl.mem_flags.READ_ONLY | pyopencl.mem_flags.COPY_HOST_PTR,\n    hostbuf = numpy.zeros( [ context._weights_buf_size ], numpy.float32 )\n    )", "path": "nn\\training.py", "repo_name": "reven86/gpgpu-neuralnet", "stars": 25, "license": "gpl-3.0", "language": "python", "size": 174}
{"docstring": "\"\"\"\nDumps gathered kernel statistics to file\n\"\"\"\n", "func_signal": "def flush_stats( self, filename ):\n", "code": "if self.profiling_enabled:\n    with open( filename, 'wb' ) as f:\n        writer = csv.writer( f, delimiter = \";\" )\n        for k, e in self._event_times_by_kernel.iteritems():\n            writer.writerow( [k] + list( e ) )", "path": "nn\\opencl.py", "repo_name": "reven86/gpgpu-neuralnet", "stars": 25, "license": "gpl-3.0", "language": "python", "size": 174}
{"docstring": "\"\"\"\nSet target outputs of this layer and calculate errors for output layer and mean error.\n\"\"\"\n", "func_signal": "def _set_outputs_and_calc_errors( self, outputs, total_error_buf ):\n", "code": "o_buf = pyopencl.Buffer( \n    self.opencl.context, pyopencl.mem_flags.READ_ONLY | pyopencl.mem_flags.COPY_HOST_PTR,\n    hostbuf = outputs\n    )\n\nself.opencl.kernel_setup_training_data.set_args( \n    self.context._neurons_buf_size, self.context._outputs_buf, self.context.output_layer._neurons_offset,\n    self.context.output_layer.neuron_count, o_buf, pyopencl.LocalMemory( 32 * 4 ),\n    self.context._errors_backpropagation_buf, total_error_buf )\n\npyopencl.enqueue_nd_range_kernel( \n    self.opencl.queue,\n    self.opencl.kernel_setup_training_data,\n    ( 32, ), ( 32, ),\n    None, None\n    ).wait()", "path": "nn\\layer.py", "repo_name": "reven86/gpgpu-neuralnet", "stars": 25, "license": "gpl-3.0", "language": "python", "size": 174}
{"docstring": "\"\"\"\nCreate additional buffers to store previous gradient vector.\n\n@param layer\n    Input layer.\n\"\"\"\n", "func_signal": "def prepare_training( self, context ):\n", "code": "super( ConjugateGradient, self ).prepare_training( context )\n\nself.direction_buf = pyopencl.Buffer( \n    context.opencl.context, pyopencl.mem_flags.READ_WRITE | pyopencl.mem_flags.COPY_HOST_PTR,\n    hostbuf = numpy.zeros( [ context._weights_buf_size ], numpy.float32 )\n    )\nself.prev_gradient_buf = pyopencl.Buffer( \n    context.opencl.context, pyopencl.mem_flags.READ_WRITE | pyopencl.mem_flags.COPY_HOST_PTR,\n    hostbuf = numpy.array( [ 0.01 ] * context._weights_buf_size, numpy.float32 )\n    )\n\n#1 float beta coefficient\nself.beta_buf = pyopencl.Buffer( context.opencl.context, pyopencl.mem_flags.READ_WRITE, 4 )\n\nself.iteration_count = 0", "path": "nn\\training.py", "repo_name": "reven86/gpgpu-neuralnet", "stars": 25, "license": "gpl-3.0", "language": "python", "size": 174}
{"docstring": "\"\"\"\nDoes nothing. Calls calc_weights_gradient on following layers.\n\"\"\"\n", "func_signal": "def calc_weights_gradient( self ):\n", "code": "super( InputLayer, self ).calc_weights_gradient()\n\nself.reset_processed()", "path": "nn\\layer.py", "repo_name": "reven86/gpgpu-neuralnet", "stars": 25, "license": "gpl-3.0", "language": "python", "size": 174}
{"docstring": "\"\"\"\nGets weights gradient vector of this layer.\n\"\"\"\n", "func_signal": "def _get_gradient( self ):\n", "code": "gradient = numpy.ndarray( [ self.weights_count ], numpy.float32 )\npyopencl.enqueue_read_buffer( \n    self.opencl.queue, self.context._gradient_buf, gradient,\n    device_offset = int( self._weights_offset * 4 ), is_blocking = True\n    )\nreturn gradient", "path": "nn\\layer.py", "repo_name": "reven86/gpgpu-neuralnet", "stars": 25, "license": "gpl-3.0", "language": "python", "size": 174}
{"docstring": "\"\"\"\nStores list of layers weights for entire neural network.\n\"\"\"\n\n# iterate over all layers and apply weights to them\n", "func_signal": "def store_weights( self, context ):\n", "code": "self.optimal_weights = []\ndef _store_weights( l ):\n    self.optimal_weights.append( l.get_weights() )\n\nself._iterate_over_layers( context, _store_weights )", "path": "nn\\training.py", "repo_name": "reven86/gpgpu-neuralnet", "stars": 25, "license": "gpl-3.0", "language": "python", "size": 174}
{"docstring": "\"\"\"\nCreates all necessary buffers.\n\n@param input_layer\n    Input layer of neural network.\n    \n@param output_layer\n    Output layer of neural network.\n    \n@param allow_training\n    if True then some buffers would have read-write access to allow store training data\n\"\"\"\n", "func_signal": "def __init__( self, input_layer, output_layer, allow_training = False ):\n", "code": "self._opencl = input_layer.opencl\nself._input_layer = input_layer\nself._output_layer = output_layer\nself._training_allowed = allow_training\nself._total_neurons = numpy.int32( 0 )\nself._total_weights = numpy.int32( 0 )\nself._total_inputs = numpy.int32( 0 )      # total inputs to neurons, without polarization link\n\n# following variables define actual items count in buffers and may differ from\n# total_* variables due to alignment issues\nself._neurons_buf_size = numpy.int32( 0 )\nself._weights_buf_size = numpy.int32( 0 )\nself._inputs_buf_size = numpy.int32( 0 )\n\ndef align( value, threshold ):\n    return ( value + threshold - 1 ) & ~( threshold - 1 )\n\nll = [ input_layer ]\nwhile ll:\n    l = ll.pop()\n\n    #process layer\n    l._weights_count = l.neuron_count * l.inputs_per_neuron\n    l._weights_offset = self._weights_buf_size\n    l._neurons_offset = self._neurons_buf_size\n    l._inputs_offset = self._inputs_buf_size\n    l.context = self\n    l._processed = True\n\n    self._total_weights += l.weights_count\n    self._total_neurons += l.neuron_count\n    self._total_inputs += l.inputs_per_neuron - 1\n\n    l._neurons_buf_size = align( l.neuron_count, 32 )\n    l._weights_buf_size = align( l.weights_count, self.opencl.max_local_size[ 0 ] )\n    l._inputs_buf_size = align( l.inputs_per_neuron - 1, 16 )\n\n    self._neurons_buf_size += l._neurons_buf_size\n    self._weights_buf_size += l._weights_buf_size\n    self._inputs_buf_size += l._inputs_buf_size\n\n    for k in l._next_layers:\n        if k[0].processed == False:\n            ll.append( k[0] )\n\ninput_layer.reset_processed()\n\nif allow_training:\n    fl = pyopencl.mem_flags.READ_WRITE\nelse:\n    fl = pyopencl.mem_flags.READ_ONLY\n\nself._inputs_buf = pyopencl.Buffer( \n    self.opencl.context,\n    pyopencl.mem_flags.READ_ONLY | pyopencl.mem_flags.COPY_HOST_PTR,\n    hostbuf = numpy.zeros( [self._inputs_buf_size], numpy.float32 )\n    )\nself._outputs_buf = pyopencl.Buffer( \n    self.opencl.context,\n    pyopencl.mem_flags.WRITE_ONLY | pyopencl.mem_flags.COPY_HOST_PTR,\n    hostbuf = numpy.zeros( [self._neurons_buf_size], numpy.float32 )\n    )\nself._weights_buf = pyopencl.Buffer( \n    self.opencl.context, fl | pyopencl.mem_flags.COPY_HOST_PTR,\n    hostbuf = numpy.zeros( [self._weights_buf_size], numpy.float32 )\n    )\n\nif allow_training:\n    self._gradient_buf = pyopencl.Buffer( \n        self.opencl.context,\n        pyopencl.mem_flags.READ_WRITE | pyopencl.mem_flags.COPY_HOST_PTR,\n        hostbuf = numpy.zeros( [self._weights_buf_size], numpy.float32 )\n        )\n    self._errors_backpropagation_buf = pyopencl.Buffer( \n        self.opencl.context,\n        pyopencl.mem_flags.READ_WRITE | pyopencl.mem_flags.COPY_HOST_PTR,\n        hostbuf = numpy.zeros( [self._neurons_buf_size], numpy.float32 )\n        )", "path": "nn\\layer.py", "repo_name": "reven86/gpgpu-neuralnet", "stars": 25, "license": "gpl-3.0", "language": "python", "size": 174}
{"docstring": "\"\"\"\nSet weights for entire layer.\n\n@param weights\n    NumPy.NDArray of float32 values, size equals to inputs_per_neuron * neuron_count\n\"\"\"\n", "func_signal": "def set_weights( self, weights ):\n", "code": "pyopencl.enqueue_write_buffer( \n    self.opencl.queue, self.context._weights_buf, weights,\n    device_offset = int( self._weights_offset * 4 ), is_blocking = True\n    )", "path": "nn\\layer.py", "repo_name": "reven86/gpgpu-neuralnet", "stars": 25, "license": "gpl-3.0", "language": "python", "size": 174}
{"docstring": "\"\"\"\nApply optimal weights to neural network.\n\"\"\"\n", "func_signal": "def apply_weights( self, context ):\n", "code": "if not isinstance( self.optimal_weights, list ):\n    #old format, convert optimal_weights to list\n\n    class _gather_weights():\n        def __init__( self, w ):\n            self.ofs = 0\n            self.optimal_weights = w\n            self.new_weights = []\n        def __call__( self, l ):\n            self.new_weights.append( self.optimal_weights[self.ofs:self.ofs + l.weights_count] )\n            self.ofs += 16 * ( 1 + l.weights_count // 16 )   # old style alignment...\n\n    g = _gather_weights( self.optimal_weights )\n    self._iterate_over_layers( context, g )\n    self.optimal_weights = g.new_weights\n\n# iterate over all layers and apply weights to them\nclass _apply_weights():\n    def __init__( self, w ):\n        self.i = 0\n        self.optimal_weights = w\n    def __call__( self, l ):\n        l.set_weights( self.optimal_weights[ self.i ] )\n        self.i += 1\n\nself._iterate_over_layers( context, _apply_weights( self.optimal_weights ) )", "path": "nn\\training.py", "repo_name": "reven86/gpgpu-neuralnet", "stars": 25, "license": "gpl-3.0", "language": "python", "size": 174}
{"docstring": "\"\"\"\nProcess for InputLayer does nothing. Simple invokes process for next layers.\n\"\"\"\n", "func_signal": "def process( self ):\n", "code": "self.opencl.kernel_process_layer.set_arg( 0, self.context._inputs_buf )\nself.opencl.kernel_process_layer.set_arg( 1, self.context._weights_buf )\nself.opencl.kernel_process_layer.set_arg( 7, pyopencl.LocalMemory( 64 * 4 ) )\nself.opencl.kernel_process_layer.set_arg( 8, self.context._outputs_buf )\n\nif self.context.training_allowed:\n    self.opencl.kernel_calc_layer_gradient.set_arg( 0, self.context._inputs_buf )\n    self.opencl.kernel_calc_layer_gradient.set_arg( 1, self.context._errors_backpropagation_buf )\n    self.opencl.kernel_calc_layer_gradient.set_arg( 6, self.context._gradient_buf )\n\n    self.opencl.kernel_propagate_errors.set_arg( 0, self.context._errors_backpropagation_buf )\n    self.opencl.kernel_propagate_errors.set_arg( 1, self.context._weights_buf )\n    self.opencl.kernel_propagate_errors.set_arg( 8, pyopencl.LocalMemory( 256 ) )\n    self.opencl.kernel_propagate_errors.set_arg( 9, self.context._outputs_buf )\n\nsuper( InputLayer, self ).process()\n\nself.reset_processed()", "path": "nn\\layer.py", "repo_name": "reven86/gpgpu-neuralnet", "stars": 25, "license": "gpl-3.0", "language": "python", "size": 174}
{"docstring": "\"\"\"\nRecursively reset processed flag on all linked layers.\n\"\"\"\n", "func_signal": "def reset_processed( self ):\n", "code": "self._processed = False\nfor l in self._next_layers:\n    l[0].reset_processed()", "path": "nn\\layer.py", "repo_name": "reven86/gpgpu-neuralnet", "stars": 25, "license": "gpl-3.0", "language": "python", "size": 174}
{"docstring": "\"\"\"\nSetup inputs to input layer.\n\n@param inputs\n    NumPy.NDArray of float32 values, size equals to neuron count\n\"\"\"\n", "func_signal": "def set_inputs( self, inputs, is_blocking = True, wait_for = None ):\n", "code": "return pyopencl.enqueue_write_buffer( \n    self.opencl.queue, self.context._inputs_buf, inputs,\n    device_offset = int( self._inputs_offset * 4 ), is_blocking = is_blocking,\n    wait_for = wait_for\n    )", "path": "nn\\layer.py", "repo_name": "reven86/gpgpu-neuralnet", "stars": 25, "license": "gpl-3.0", "language": "python", "size": 174}
{"docstring": "\"\"\"\nReturns time in seconds spent by OpenCL since last call to gather_opencl_time.\n\"\"\"\n", "func_signal": "def gather_opencl_stats( self ):\n", "code": "if not self.profiling_enabled:\n    return 0.0\n\n# make sure all events are finished\nself.queue.finish()\nres = numpy.float32( 0.0 )\n\n# for each event and kernel compute time for each event state\nfor k, e in self._event_list:\n    times = numpy.array( ( \n        1,\n        # event.profile works too slow\n        1e-9 * ( e.get_profiling_info( pyopencl.profiling_info.SUBMIT ) - e.get_profiling_info( pyopencl.profiling_info.QUEUED ) ),\n        1e-9 * ( e.get_profiling_info( pyopencl.profiling_info.START ) - e.get_profiling_info( pyopencl.profiling_info.SUBMIT ) ),\n        1e-9 * ( e.get_profiling_info( pyopencl.profiling_info.END ) - e.get_profiling_info( pyopencl.profiling_info.START ) ),\n        ), numpy.float32 )\n    if k in self._event_times_by_kernel:\n        self._event_times_by_kernel[k] += times\n    else:\n        self._event_times_by_kernel[k] = times\n    res += times[3]\n\ndel self._event_list[:]\nreturn res", "path": "nn\\opencl.py", "repo_name": "reven86/gpgpu-neuralnet", "stars": 25, "license": "gpl-3.0", "language": "python", "size": 174}
{"docstring": "\"\"\"\nAdjust weights of neural network by certain direction.\n\"\"\"\n", "func_signal": "def adjust_weights( self, context ):\n", "code": "dir = self.get_weights_direction_buf( context ) #this call should always return opposite direction\n\ncontext.opencl.kernel_adjust_weights( \n    context.opencl.queue, ( int( context._weights_buf_size ), ),\n    dir,\n    self.n, self.alpha,\n    self._weights_delta_buf,\n    context._weights_buf,\n    wait_for = ( context.input_layer._calc_gradient_event, ),\n    )", "path": "nn\\training.py", "repo_name": "reven86/gpgpu-neuralnet", "stars": 25, "license": "gpl-3.0", "language": "python", "size": 174}
{"docstring": "\"\"\"\nDelete an access key associated with a user.\n\nIf the user_name is not specified, it is determined implicitly based\non the AWS Access Key ID used to sign the request.\n\n:type access_key_id: string\n:param access_key_id: The ID of the access key to be deleted.\n\n:type user_name: string\n:param user_name: The username of the new user\n\n\"\"\"\n", "func_signal": "def delete_access_key(self, access_key_id, user_name=None):\n", "code": "params = {'AccessKeyId' : access_key_id}\nif user_name:\n    params['UserName'] = user_name\nreturn self.get_response('DeleteAccessKey', params)", "path": "boto\\iam\\__init__.py", "repo_name": "apetresc/awskeyserver", "stars": 18, "license": "None", "language": "python", "size": 189}
{"docstring": "\"\"\"\nRetrieve information about the specified user.\n\nIf the user_name is not specified, the user_name is determined\nimplicitly based on the AWS Access Key ID used to sign the request.\n\n:type user_name: string\n:param user_name: The name of the user to delete.\n                  If not specified, defaults to user making\n                  request.\n\n\"\"\"\n", "func_signal": "def get_user(self, user_name=None):\n", "code": "params = {}\nif user_name:\n    params['UserName'] = user_name\nreturn self.get_response('GetUser', params)", "path": "boto\\iam\\__init__.py", "repo_name": "apetresc/awskeyserver", "stars": 18, "license": "None", "language": "python", "size": 189}
{"docstring": "\"\"\"\nAdd an environemnt variable\nFor Ubuntu, the best place is /etc/environment.  Values placed here do\nnot need to be exported.\n\"\"\"\n", "func_signal": "def add_env(self, key, value):\n", "code": "boto.log.info('Adding env variable: %s=%s' % (key, value))\nif not os.path.exists(\"/etc/environment.orig\"):\n    self.run('cp /etc/environment /etc/environment.orig', notify=False, exit_on_error=False)\nfp = open('/etc/environment', 'a')\nfp.write('\\n%s=\"%s\"' % (key, value))\nfp.close()\nos.environ[key] = value", "path": "boto\\pyami\\installers\\ubuntu\\installer.py", "repo_name": "apetresc/awskeyserver", "stars": 18, "license": "None", "language": "python", "size": 189}
{"docstring": "\"\"\"\nDelete a user including the user's path, GUID and ARN.\n\nIf the user_name is not specified, the user_name is determined\nimplicitly based on the AWS Access Key ID used to sign the request.\n\n:type user_name: string\n:param user_name: The name of the user to delete.\n\n\"\"\"\n", "func_signal": "def delete_user(self, user_name):\n", "code": "params = {'UserName' : user_name}\nreturn self.get_response('DeleteUser', params)", "path": "boto\\iam\\__init__.py", "repo_name": "apetresc/awskeyserver", "stars": 18, "license": "None", "language": "python", "size": 189}
{"docstring": "\"\"\"\nWrite a file to /etc/cron.d to schedule a command\n    env is a dict containing environment variables you want to set in the file\n    name will be used as the name of the file\n\"\"\"\n", "func_signal": "def add_cron(self, name, command, minute=\"*\", hour=\"*\", mday=\"*\", month=\"*\", wday=\"*\", who=\"root\", env=None):\n", "code": "if minute == 'random':\n    minute = str(random.randrange(60))\nif hour == 'random':\n    hour = str(random.randrange(24))\nfp = open('/etc/cron.d/%s' % name, \"w\")\nif env:\n    for key, value in env.items():\n        fp.write('%s=%s\\n' % (key, value))\nfp.write('%s %s %s %s %s %s %s\\n' % (minute, hour, mday, month, wday, who, command))\nfp.close()", "path": "boto\\pyami\\installers\\ubuntu\\installer.py", "repo_name": "apetresc/awskeyserver", "stars": 18, "license": "None", "language": "python", "size": 189}
{"docstring": "\"\"\"\nCreate a group.\n\n:type group_name: string\n:param group_name: The name of the new group\n\n:type path: string\n:param path: The path to the group (Optional).  Defaults to /.\n\n\"\"\"\n", "func_signal": "def create_group(self, group_name, path='/'):\n", "code": "params = {'GroupName' : group_name,\n          'Path' : path}\nreturn self.get_response('CreateGroup', params)", "path": "boto\\iam\\__init__.py", "repo_name": "apetresc/awskeyserver", "stars": 18, "license": "None", "language": "python", "size": 189}
{"docstring": "\"\"\"\nCreate a user.\n\n:type user_name: string\n:param user_name: The name of the new user\n\n:type path: string\n:param path: The path in which the user will be created.\n             Defaults to /.\n\n\"\"\"\n", "func_signal": "def create_user(self, user_name, path='/'):\n", "code": "params = {'UserName' : user_name,\n          'Path' : path}\nreturn self.get_response('CreateUser', params)", "path": "boto\\iam\\__init__.py", "repo_name": "apetresc/awskeyserver", "stars": 18, "license": "None", "language": "python", "size": 189}
{"docstring": "\"\"\"\nRetrieves the specified policy document for the specified user.\n\n:type user_name: string\n:param user_name: The name of the user the policy is associated with.\n\n:type policy_name: string\n:param policy_name: The policy document to get.\n\n\"\"\"\n", "func_signal": "def get_user_policy(self, user_name, policy_name):\n", "code": "params = {'UserName' : user_name,\n          'PolicyName' : policy_name}\nreturn self.get_response('GetUserPolicy', params, verb='POST')", "path": "boto\\iam\\__init__.py", "repo_name": "apetresc/awskeyserver", "stars": 18, "license": "None", "language": "python", "size": 189}
{"docstring": "\"\"\"\nAdds or updates the specified policy document for the specified user.\n\n:type user_name: string\n:param user_name: The name of the user the policy is associated with.\n\n:type policy_name: string\n:param policy_name: The policy document to get.\n\n:type policy_json: string\n:param policy_json: The policy document.\n\n\"\"\"\n", "func_signal": "def put_user_policy(self, user_name, policy_name, policy_json):\n", "code": "params = {'UserName' : user_name,\n          'PolicyName' : policy_name,\n          'PolicyDocument' : policy_json}\nreturn self.get_response('PutUserPolicy', params, verb='POST')", "path": "boto\\iam\\__init__.py", "repo_name": "apetresc/awskeyserver", "stars": 18, "license": "None", "language": "python", "size": 189}
{"docstring": "\"\"\"\nCreate a new AWS Secret Access Key and corresponding AWS Access Key ID\nfor the specified user.  The default status for new keys is Active\n\nIf the user_name is not specified, the user_name is determined\nimplicitly based on the AWS Access Key ID used to sign the request.\n\n:type user_name: string\n:param user_name: The username of the new user\n\n\"\"\"\n", "func_signal": "def create_access_key(self, user_name=None):\n", "code": "params = {'UserName' : user_name}\nreturn self.get_response('CreateAccessKey', params)", "path": "boto\\iam\\__init__.py", "repo_name": "apetresc/awskeyserver", "stars": 18, "license": "None", "language": "python", "size": 189}
{"docstring": "\"\"\"\nRetrieves the specified policy document for the specified group.\n\n:type group_name: string\n:param group_name: The name of the group the policy is associated with.\n\n:type policy_name: string\n:param policy_name: The policy document to get.\n\n\"\"\"\n", "func_signal": "def get_group_policy(self, group_name, policy_name):\n", "code": "params = {'GroupName' : group_name,\n          'PolicyName' : policy_name}\nreturn self.get_response('GetGroupPolicy', params, verb='POST')", "path": "boto\\iam\\__init__.py", "repo_name": "apetresc/awskeyserver", "stars": 18, "license": "None", "language": "python", "size": 189}
{"docstring": "\"\"\"\nAdd a user to a group\n\n:type group_name: string\n:param group_name: The name of the new group\n\n:type user_name: string\n:param user_name: The to be added to the group.\n\n\"\"\"\n", "func_signal": "def add_user_to_group(self, group_name, user_name):\n", "code": "params = {'GroupName' : group_name,\n          'UserName' : user_name}\nreturn self.get_response('AddUserToGroup', params)", "path": "boto\\iam\\__init__.py", "repo_name": "apetresc/awskeyserver", "stars": 18, "license": "None", "language": "python", "size": 189}
{"docstring": "\"\"\"\nChanges the status of the specified access key from Active to Inactive\nor vice versa.  This action can be used to disable a user's key as\npart of a key rotation workflow.\n\nIf the user_name is not specified, the user_name is determined\nimplicitly based on the AWS Access Key ID used to sign the request.\n\n:type access_key_id: string\n:param access_key_id: The ID of the access key.\n\n:type status: string\n:param status: Either Active or Inactive.\n\n:type user_name: string\n:param user_name: The username of user (optional).\n\n\"\"\"\n", "func_signal": "def update_access_key(self, access_key_id, status, user_name=None):\n", "code": "params = {'AccessKeyId' : access_key_id,\n          'Status' : status}\nif user_name:\n    params['UserName'] = user_name\nreturn self.get_response('UpdateAccessKey', params)", "path": "boto\\iam\\__init__.py", "repo_name": "apetresc/awskeyserver", "stars": 18, "license": "None", "language": "python", "size": 189}
{"docstring": "\"\"\"\nUpdate a group by adding or removing a user to/from it.\n\n:type group_name: string\n:param group_name: The name of the new group\n\n:type new_group_name: string\n:param new_group_name: If provided, the name of the group will be\n                       changed to this name.\n\n:type new_path: string\n:param new_path: If provided, the path of the group will be\n                 changed to this path.\n\n\"\"\"\n", "func_signal": "def update_group(self, group_name, new_group_name=None, new_path=None):\n", "code": "params = {'GroupName' : group_name}\nif new_group_name:\n    params['NewGroupName'] = new_group_name\nif new_path:\n    params['NewPath'] = new_path\nreturn self.get_response('UpdateGroup', params)", "path": "boto\\iam\\__init__.py", "repo_name": "apetresc/awskeyserver", "stars": 18, "license": "None", "language": "python", "size": 189}
{"docstring": "\"\"\"\nList the users that have the specified path prefix.\n\n:type path_prefix: string\n:param path_prefix: If provided, only users whose paths match\n                    the provided prefix will be returned.\n\n:type marker: string\n:param marker: Use this only when paginating results and only in\n               follow-up request after you've received a response\n               where the results are truncated.  Set this to the\n               value of the Marker element in the response you\n               just received.\n\n:type max_items: int\n:param max_items: Use this only when paginating results to indicate\n                  the maximum number of groups you want in the\n                  response.\n\"\"\"\n", "func_signal": "def get_all_users(self, path_prefix='/', marker=None, max_items=None):\n", "code": "params = {'PathPrefix' : path_prefix}\nif marker:\n    params['Marker'] = marker\nif max_items:\n    params['MaxItems'] = max_items\nreturn self.get_response('ListUsers', params, list_marker='Users')", "path": "boto\\iam\\__init__.py", "repo_name": "apetresc/awskeyserver", "stars": 18, "license": "None", "language": "python", "size": 189}
{"docstring": "\"\"\"\nAdds or updates the specified policy document for the specified group.\n\n:type group_name: string\n:param group_name: The name of the group the policy is associated with.\n\n:type policy_name: string\n:param policy_name: The policy document to get.\n\n:type policy_json: string\n:param policy_json: The policy document.\n\n\"\"\"\n", "func_signal": "def put_group_policy(self, group_name, policy_name, policy_json):\n", "code": "params = {'GroupName' : group_name,\n          'PolicyName' : policy_name,\n          'PolicyDocument' : policy_json}\nreturn self.get_response('PutGroupPolicy', params, verb='POST')", "path": "boto\\iam\\__init__.py", "repo_name": "apetresc/awskeyserver", "stars": 18, "license": "None", "language": "python", "size": 189}
{"docstring": "\"\"\"\nResets the password associated with the user's login profile.\n\n:type user_name: string\n:param user_name: The name of the user\n\n:type password: string\n:param password: The new password for the user\n\n\"\"\"\n", "func_signal": "def update_login_profile(self, user_name, password):\n", "code": "params = {'UserName' : user_name,\n          'Password' : password}\nreturn self.get_response('UpdateLoginProfile', params)", "path": "boto\\iam\\__init__.py", "repo_name": "apetresc/awskeyserver", "stars": 18, "license": "None", "language": "python", "size": 189}
{"docstring": "\"\"\"\nGet all MFA devices associated with an account.\n\n:type user_name: string\n:param user_name: The username of the user\n\n:type marker: string\n:param marker: Use this only when paginating results and only in\n               follow-up request after you've received a response\n               where the results are truncated.  Set this to the\n               value of the Marker element in the response you\n               just received.\n\n:type max_items: int\n:param max_items: Use this only when paginating results to indicate\n                  the maximum number of groups you want in the\n                  response.\n                  \n\"\"\"\n", "func_signal": "def get_all_mfa_devices(self, user_name, marker=None, max_items=None):\n", "code": "params = {'UserName' : user_name}\nif marker:\n    params['Marker'] = marker\nif max_items:\n    params['MaxItems'] = max_items\nreturn self.get_response('ListMFADevices',\n                         params, list_marker='MFADevices')", "path": "boto\\iam\\__init__.py", "repo_name": "apetresc/awskeyserver", "stars": 18, "license": "None", "language": "python", "size": 189}
{"docstring": "\"\"\"\nDeactivates the specified MFA device and removes it from\nassociation with the user.\n\n:type user_name: string\n:param user_name: The username of the user\n\n:type serial_number: string\n:param seriasl_number: The serial number which uniquely identifies\n                       the MFA device.\n\n\"\"\"\n", "func_signal": "def deactivate_mfa_device(self, user_name, serial_number):\n", "code": "params = {'UserName' : user_name,\n          'SerialNumber' : serial_number}\nreturn self.get_response('DeactivateMFADevice', params)", "path": "boto\\iam\\__init__.py", "repo_name": "apetresc/awskeyserver", "stars": 18, "license": "None", "language": "python", "size": 189}
{"docstring": "# First, find and attach the volume\n", "func_signal": "def install(self):\n", "code": "self.attach()\n\n# Install the xfs tools\nself.run('apt-get -y install xfsprogs xfsdump')\n\n# Check to see if the filesystem was created or not\nself.make_fs()\n\n# create the /ebs directory for mounting\nself.handle_mount_point()\n\n# create the backup script\nself.create_backup_script()\n\n# Set up the backup script\nminute = boto.config.get('EBS', 'backup_cron_minute', '0')\nhour = boto.config.get('EBS', 'backup_cron_hour', '4,16')\nself.add_cron(\"ebs_backup\", \"/usr/local/bin/ebs_backup\", minute=minute, hour=hour)\n\n# Set up the backup cleanup script\nminute = boto.config.get('EBS', 'backup_cleanup_cron_minute')\nhour = boto.config.get('EBS', 'backup_cleanup_cron_hour')\nif (minute != None) and (hour != None):\n    self.create_backup_cleanup_script();\n    self.add_cron(\"ebs_backup_cleanup\", \"/usr/local/bin/ebs_backup_cleanup\", minute=minute, hour=hour)\n\n# Set up the fstab\nself.update_fstab()", "path": "boto\\pyami\\installers\\ubuntu\\ebs.py", "repo_name": "apetresc/awskeyserver", "stars": 18, "license": "None", "language": "python", "size": 189}
{"docstring": "# Basic setup\n", "func_signal": "def group_metric(collection, group, metric):\n", "code": "period = '-4hours'\ncname = collection\ncollection = config.collections[collection]\ngname = group\ngroup = collection['groups'][group]\n# Slug => metric object\nmobj = None\nfor m in group['metrics']:\n    if m.name == metric:\n        mobj = m\n        break\nif mobj is None:\n    flask.abort(404)\n# Metric-based nav\nmetric_groups = map(\n    lambda x: (x, flask.url_for('group_metric', collection=cname,\n        group=gname, metric=x)),\n    [x.name for x in group['metrics']]\n)\nparent = flask.url_for('group', collection=cname, group=gname)\n# Grid setup\nper_row = 5\ncol_size = (16 / per_row)\n# Thumbnails\nthumbnail_opts = {\n    'height': 100,\n    'width': 200,\n    'hideLegend': True,\n    'hideGrid': True,\n    'yBoundsOnly': True,\n    'hideXAxis': True,\n    'from': period,\n}\nreturn flask.render_template(\n    'group.html',\n    collection=collection,\n    group=group,\n    metric=mobj,\n    metric_groups=metric_groups,\n    current_mgroup=metric,\n    per_row=per_row,\n    col_size=col_size,\n    thumbnail_opts=thumbnail_opts,\n    period=period,\n    parent=parent,\n    gname=gname\n)", "path": "fullerene\\__init__.py", "repo_name": "bitprophet/fullerene", "stars": 28, "license": "None", "language": "python", "size": 256}
{"docstring": "\"\"\"\nList items in groups collections should honor custom metric names\n\"\"\"\n", "func_signal": "def test_metric_aliases(self):\n", "code": "config = conf(\"basic\")\naliased_metric = config.groups['group1']['metric1']\neq_(aliased_metric.path, \"foo.bar\")", "path": "tests\\test_main.py", "repo_name": "bitprophet/fullerene", "stars": 28, "license": "None", "language": "python", "size": 256}
{"docstring": "\"\"\"\nReturn expanded metric list from our path and the given ``hostname``.\n\nE.g. if self.path == foo.*.bar, this might return [foo.1.bar,\nfoo.2.bar].\n\nThis method does not take into account any filtering or exclusions; it\nreturns the largest possible expansion (basically what Graphite's\n/metrics/expand/ endpoint gives you.)\n\n``hostname`` is used solely for giving context to the expansion; the\nreturned metric paths will still be host-agnostic (in order to blend in\nwith non-expanded metric paths.)\n\"\"\"\n", "func_signal": "def expand(self, hostname=\"\"):\n", "code": "sep = '.'\nif hostname:\n    path = sep.join([hostname, self.path])\n    func = lambda x: sep.join(x.split(sep)[1:])\nelse:\n    path = self.path\n    func = lambda x: x\nreturn map(func, self.config.graphite.query(path))", "path": "fullerene\\metric.py", "repo_name": "bitprophet/fullerene", "stars": 28, "license": "None", "language": "python", "size": 256}
{"docstring": "\"\"\"\ncombine(include_raw=True) without expansions\n\"\"\"\n", "func_signal": "def test_include_raw_no_expansions(self):\n", "code": "result = combine([\"foo.bar\", \"foo.biz\"], [], True)\neq_(result, {\"foo.{bar,biz}\": [\"foo.bar\", \"foo.biz\"]})", "path": "tests\\test_main.py", "repo_name": "bitprophet/fullerene", "stars": 28, "license": "None", "language": "python", "size": 256}
{"docstring": "# Setup\n", "func_signal": "def group(collection, group):\n", "code": "cname = collection\ngname = group\ncollection = config.collections[collection]\ngroup = collection['groups'][group]\nreturn flask.render_template(\n    'collection_group.html',\n    cname=cname,\n    group=group,\n    gname=gname,\n    metrics=group['metrics']\n)", "path": "fullerene\\__init__.py", "repo_name": "bitprophet/fullerene", "stars": 28, "license": "None", "language": "python", "size": 256}
{"docstring": "\"\"\"\nAttributes which are straight-up imported from the YAML\n\"\"\"\n", "func_signal": "def test_basic_attributes(self):\n", "code": "for attr, expected in (\n    ('defaults', {'height': 250, 'width': 400, 'from': '-2hours'}),\n    ('periods', {'day': '-24hours', 'week': '-7days'}),\n    ('graphite.uri', 'whatever'),\n    ('graphite.exclude_hosts', ['a', 'b']),\n):\n    eq_.description = \"Config.%s = YAML '%s' value\" % (attr, attr)\n    result = recursive_getattr(conf(\"basic\"), attr.split('.'))\n    yield eq_, result, expected\n    del eq_.description", "path": "tests\\test_main.py", "repo_name": "bitprophet/fullerene", "stars": 28, "license": "None", "language": "python", "size": 256}
{"docstring": "\"\"\"\nFill in self.wildcards from self.parts\n\"\"\"\n# Discover wildcard locations (so we can tell, while walking a split\n# string, \"which\" wildcard we may be looking at (the 0th, 1st, Nth)\n", "func_signal": "def find_wildcards(self):\n", "code": "wildcards = []\nfor index, part in enumerate(self.parts):\n    if '*' in part:\n        wildcards.append(index)\nreturn wildcards", "path": "fullerene\\metric.py", "repo_name": "bitprophet/fullerene", "stars": 28, "license": "None", "language": "python", "size": 256}
{"docstring": "# Remember that expansion indexes apply only to wildcard slots,\n# which here are slots which differ from path to path and would thus\n# get combined by default.\n", "func_signal": "def test_expansions(self):\n", "code": "for desc, inputs, expansions, results in (\n    (\"Expand second part\",\n        [\"foo.bar\", \"foo.biz\"], [1], [\"foo.bar\", \"foo.biz\"]),\n    (\"Expand both parts\",\n        [\"1.2\", \"3.4\"], [0, 1], [\"1.2\", \"3.4\"]),\n):\n    eq_.description = desc\n    yield eq_, set(map(str, combine(inputs, expansions))), set(results)\n    del eq_.description", "path": "tests\\test_main.py", "repo_name": "bitprophet/fullerene", "stars": 28, "license": "None", "language": "python", "size": 256}
{"docstring": "\"\"\"\nWe don't typically want this to accept overrides -- if somebody wants to go\nto the composer view, many e.g. thumbnail/presentational options should\nprobably get turned off so they get a more normal view. Useful things like\ntimeperiod/from will typically be preserved.\n\"\"\"\n", "func_signal": "def composer(graph):\n", "code": "if config.external_graphite:\n    return config.external_graphite + \"/composer/\" + graph.querystring", "path": "fullerene\\__init__.py", "repo_name": "bitprophet/fullerene", "stars": 28, "license": "None", "language": "python", "size": 256}
{"docstring": "# Get metric objects for this group\n", "func_signal": "def host(domain, host, metric_group, period):\n", "code": "raw_metrics = config.groups[metric_group].values()\n# Filter period value through defined aliases\nkwargs = {'from': config.periods.get(period, period)}\n# Generate graph objects from each metric, based on hostname context\ngraphite_host = host + '_' + domain.replace('.', '_')\ngraphs = map(lambda m: m.graphs(graphite_host, **kwargs), raw_metrics)\nmerged = reduce(operator.add, graphs, [])\n# Set up metric group nav\nmetric_groups = map(\n    lambda x: (x, flask.url_for('host', domain=domain, metric_group=x,\n        host=host, period=period)),\n    config.metric_groups\n)\nreturn flask.render_template(\n    'host.html',\n    domain=domain,\n    host=host,\n    metrics=merged,\n    metric_groups=metric_groups,\n    periods=config.periods.keys(),\n    current_mgroup=metric_group,\n    current_period=period,\n)", "path": "fullerene\\__init__.py", "repo_name": "bitprophet/fullerene", "stars": 28, "license": "None", "language": "python", "size": 256}
{"docstring": "\"\"\"\nTake a list of paths and combine into fewer using brace-expressions.\n\nE.g. [\"foo.bar\", \"foo.biz\"] => \"foo.{bar,biz}\"\n\nWhen ``include_raw`` is ``True``, returns a mapping instead of a list,\nwhere the key the brace-expression string and the value is the list of\npaths making up that particular brace-expression. (Which, with no\nexpansion, will always be the same as the input list; with expansion it's\nusually a subset.)\n\nWith expansions & include_raw, you'd get e.g. {\"foo.bar\": [\"foo.bar\"],\n\"foo.biz\": [\"foo.biz\"]} for the same input as above and an expansion list\nof [1]. Pretty tautological.\n\nA more complex example would be partial expansion. Calling\ncombine([\"a.1.b.1\", \"a.1.b.2\", \"a.2.b.1\", \"a.2.b.2\"], expansions=[1],\ninclude_raw=True) would result in:\n\n    {\n        \"a.{1,2}.b.1\": [\"a.1.b.1\", \"a.2.b.1\"],\n        \"a.{1,2}.b.2\": [\"a.1.b.2\", \"a.2.b.2\"]\n    }\n\nbecause the first \"overlapping\" segment (a.1 vs a.2) is expanded, but the\nsecond (b.1 vs b.2) is not, and thus we get two keys whose values split the\nincoming 4-item list in half.\n\"\"\"\n# Preserve input\n", "func_signal": "def combine(paths, expansions=[], include_raw=False):\n", "code": "original_paths = paths[:]\nbuckets = defaultdict(list)\n# Divvy up paths into per-segment buckets\nfor path in paths:\n    for i, part in enumerate(path.split('.')):\n        if part not in buckets[i]:\n            buckets[i].append(part)\n# \"Zip\" up buckets as needed depending on expansions\nret = [[]]\nfor key in sorted(buckets.keys()):\n    value = list(buckets[key])\n    # Only one value for this index: everybody gets a copy\n    if len(value) == 1:\n        for x in ret:\n            x.append(value[0])\n    else:\n        # If this index is to be expanded, branch out: all existing results\n        # up to this point get cloned, one per matching item\n        if key in expansions:\n            previous = ret[:]\n            ret = []\n            for x in value:\n                for y in previous:\n                    ret.append(y + [x])\n        # No expansion = just drop in the iterable, no conversion to string\n        else:\n            for x in ret:\n                x.append(value)\n# Now that we're done, merge the chains into strings\nmapping = {}\n# TODO: This is so dumb. Must be a way to merge with the nearly-identical\n# shit above.  I suck at algorithms.\nfor expr in ret:\n    key_parts = []\n    paths = [[]]\n    for part in expr:\n        if isinstance(part, list):\n            # Update paths\n            previous = paths[:]\n            paths = []\n            for subpart in part:\n                for x in previous:\n                    paths.append(x + [subpart])\n            # Update key parts\n            part = \"{\" + \",\".join(part) + \"}\"\n        else:\n            for path in paths:\n                path.append(part)\n        key_parts.append(part)\n    # New final key/value pair\n    # (Strip out any incorrectly expanded paths not present in the input.)\n    # (TODO: figure out how not to expand combinatorically when that's not\n    # correct. sigh)\n    raw_paths = filter(\n        lambda x: x in original_paths,\n        map(lambda x: \".\".join(x), paths)\n    )\n    # Do the same for keys, when expanding; have to search substrings.\n    merged_path = \".\".join(key_parts)\n    merged_path_good = False\n    lcd, _, rest = merged_path.partition('{')\n    for original in original_paths:\n        if original.startswith(lcd):\n            merged_path_good = True\n            break\n    if merged_path_good:\n        mapping[merged_path] = raw_paths\nreturn mapping if include_raw else mapping.keys()", "path": "fullerene\\metric.py", "repo_name": "bitprophet/fullerene", "stars": 28, "license": "None", "language": "python", "size": 256}
{"docstring": "\"\"\"\nPrints out query string for easy appending to URLs.\n\nE.g. \"?target=foo&from=blah&height=xxx\"\n\"\"\"\n", "func_signal": "def querystring(self):\n", "code": "pairs = map(lambda x: \"%s=%s\" % (x[0], x[1]), self.kwargs.items())\nreturn \"?\" + \"&\".join(pairs)", "path": "fullerene\\graph.py", "repo_name": "bitprophet/fullerene", "stars": 28, "license": "None", "language": "python", "size": 256}
{"docstring": "\"\"\"\nReturn 1+ Graph objects, optionally using ``hostname`` for context.\n\nUses the initial ``excludes`` and ``expands`` options to determine\nwhich graphs to return. See ``metric.Metric.expand`` and\n``metric.combine`` for details.\n\nAny kwargs passed in are passed into the Graph objects, so e.g.\n``.graphs('foo.hostname', **{'from': '-24hours'})`` is a convenient way\nto get a collection of graphs for this metric all set to draw a 24 hour\nperiod.\n\nThe kwargs will be used to override any defaults from the config\nobject.\n\"\"\"\n", "func_signal": "def graphs(self, hostname=\"\", **kwargs):\n", "code": "hostname = hostname.replace('.', '_')\n# If %-expressions in path, or raw=True, just insert hostname and skip\n# parsing\ngroup = kwargs.pop('group', \"\")\nparameterized = \"%s\" in self.path or \"%g\" in self.path\nif parameterized or self.raw:\n    if parameterized:\n        path = (self.path\n            .replace(\"%s\", \"%(hostname)s\")\n            .replace(\"%g\", \"%(group)s\")\n        )\n        results = [path % {'hostname': hostname, 'group': group}]\n    else:\n        results = [self.path]\n    return self._graphs(results, kwargs)\n# Expand out to full potential list of paths, apply filters\nmatches = []\nexpanded = self.expand(hostname)\nfor item in expanded:\n    parts = item.split('.')\n    good = True\n    # Exclude any exclusions\n    for location, part in enumerate(parts):\n        # We only care about wildcard slots\n        if location not in self.wildcards:\n            continue\n        # Which wildcard slot is this?\n        wildcard_index = self.wildcards.index(location)\n        # Is this substring listed for exclusion in this slot?\n        if part in self.excludes.get(wildcard_index, []):\n            good = False\n            break # move on to next metric/item\n    if good:\n        matches.append(item)\n# Perform any necessary combining into brace-expressions & return\nresult = combine(matches, self.to_expand)\nresult = map(lambda x: \"%s.%s\" % (hostname, x), result)\nreturn self._graphs(result, kwargs)", "path": "fullerene\\metric.py", "repo_name": "bitprophet/fullerene", "stars": 28, "license": "None", "language": "python", "size": 256}
{"docstring": "\"\"\"\nReturn list of metric paths based on one or more search queries.\n\nBasically just a wrapper around Graphite's /metrics/expand/ endpoint.\n\nSpecify ``leaves_only=True`` to filter out any non-leaf results.\n\"\"\"\n", "func_signal": "def query(self, *paths, **kwargs):\n", "code": "query = \"?\" + \"&\".join(map(lambda x: \"query=%s\" % x, paths))\nuri = self.uri + \"/metrics/expand/%s\" % query\nif kwargs.get('leaves_only', False):\n    uri += \"&leavesOnly=1\"\nresponse = requests.get(uri)\nstruct = json.loads(response.content)['results']\nfiltered = filter(\n    lambda x: x not in self.exclude_hosts,\n    struct\n)\nreturn filtered", "path": "fullerene\\graphite.py", "repo_name": "bitprophet/fullerene", "stars": 28, "license": "None", "language": "python", "size": 256}
{"docstring": "\"\"\"\nTakes a Graph as input, prints out full render URL.\n\"\"\"\n", "func_signal": "def _render(graph, **overrides):\n", "code": "params = dict(graph.kwargs, **overrides)\nreturn flask.url_for(\"render\", **params)", "path": "fullerene\\__init__.py", "repo_name": "bitprophet/fullerene", "stars": 28, "license": "None", "language": "python", "size": 256}
{"docstring": "# Handle just-a-string options\n", "func_signal": "def __init__(self, options, config, name=\"\"):\n", "code": "if not hasattr(options, 'pop'):\n    options = {'path': options}\nself.name = name\nself.path = options.pop('path')\nself.title = options.pop('title', self.path)\nself.title_param = options.pop('title_param', None)\nself.config = config\nself.raw = options.pop('raw', False)\n# Generate split version of our path, and note any wildcards\nself.parts = self.path.split('.')\nself.wildcards = self.find_wildcards()\n# Normalize/clean up options\nself.excludes = self.set_excludes(options.pop('exclude', ()))\nself.to_expand = self.set_expansions(options.pop('expand', ()))\n# Everything else given in the YAML config is a graphite override\nself.extra_options = options", "path": "fullerene\\metric.py", "repo_name": "bitprophet/fullerene", "stars": 28, "license": "None", "language": "python", "size": 256}
{"docstring": "\"\"\"\ncombine(include_raw=True) with complex input\n\"\"\"\n", "func_signal": "def test_include_raw_complex(self):\n", "code": "paths = [\"a.1.b.1\", \"a.1.b.2\", \"a.2.b.1\", \"a.2.b.2\"]\nresult = combine(paths, [3], True)\neq_(result,\n    {\n        \"a.{1,2}.b.1\": [\"a.1.b.1\", \"a.2.b.1\"],\n        \"a.{1,2}.b.2\": [\"a.1.b.2\", \"a.2.b.2\"]\n    }\n)", "path": "tests\\test_main.py", "repo_name": "bitprophet/fullerene", "stars": 28, "license": "None", "language": "python", "size": 256}
{"docstring": "\"\"\"\ncombine(include_raw=True) with expansions\n\"\"\"\n", "func_signal": "def test_include_raw_expansions(self):\n", "code": "result = combine([\"foo.bar\", \"foo.biz\"], [1], True)\neq_(result, {\"foo.bar\": [\"foo.bar\"], \"foo.biz\": [\"foo.biz\"]})", "path": "tests\\test_main.py", "repo_name": "bitprophet/fullerene", "stars": 28, "license": "None", "language": "python", "size": 256}
{"docstring": "\"\"\"\nReturn *all* metrics starting with the given ``base`` pattern/string.\n\nAssumes maximum realistic depth of ``max_depth``, due to the method\nrequired to get multiple levels of metric paths out of Graphite.\n\nIf run with ``base=\"*\"`` be prepared to wait a very long time for any\nnontrivial Graphite installation to come back with the answer...\n\"\"\"\n", "func_signal": "def query_all(base, max_depth=7):\n", "code": "queries = []\nfor num in range(1, max_depth + 1):\n    query = \"%s.%s\" % (base, \".\".join(['*'] * num))\n    queries.append(query)\nreturn self.query(queries, leaves_only=True)", "path": "fullerene\\graphite.py", "repo_name": "bitprophet/fullerene", "stars": 28, "license": "None", "language": "python", "size": 256}
{"docstring": "# Precedence: defaults => overridden by extra_options => kwargs\n", "func_signal": "def _graphs(self, paths, kwargs):\n", "code": "first_merge = dict(self.extra_options, **kwargs)\nmerged_kwargs = dict(self.config.defaults, **first_merge)\nreturn [\n    Graph(path, self.config, self.title, self.title_param, **merged_kwargs)\n    for path in paths\n]", "path": "fullerene\\metric.py", "repo_name": "bitprophet/fullerene", "stars": 28, "license": "None", "language": "python", "size": 256}
{"docstring": "\"\"\"Turns a list of maps, lists, or scalars into a single map.\nUsed to build the SELF_CLOSING_TAGS, NESTABLE_TAGS, and\nNESTING_RESET_TAGS maps out of lists and partial maps.\"\"\"\n", "func_signal": "def buildTagMap(default, *args):\n", "code": "built = {}\nfor portion in args:\n    if hasattr(portion, 'items'):\n        #It's a map. Merge it.\n        for k,v in portion.items():\n            built[k] = v\n    elif isList(portion):\n        #It's a list. Map each item to the default.\n        for k in portion:\n            built[k] = default\n    else:\n        #It's a scalar. Map it to the default.\n        built[portion] = default\nreturn built", "path": "BeautifulSoup.py", "repo_name": "vimjolts/joltserver", "stars": 16, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"Recursively destroys the contents of this tree.\"\"\"\n", "func_signal": "def decompose(self):\n", "code": "contents = [i for i in self.contents]\nfor i in contents:\n    if isinstance(i, Tag):\n        i.decompose()\n    else:\n        i.extract()\nself.extract()", "path": "BeautifulSoup.py", "repo_name": "vimjolts/joltserver", "stars": 16, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"This method routes method call requests to either the SGMLParser\nsuperclass or the Tag superclass, depending on the method name.\"\"\"\n#print \"__getattr__ called on %s.%s\" % (self.__class__, methodName)\n\n", "func_signal": "def __getattr__(self, methodName):\n", "code": "if methodName.find('start_') == 0 or methodName.find('end_') == 0 \\\n       or methodName.find('do_') == 0:\n    return SGMLParser.__getattr__(self, methodName)\nelif methodName.find('__') != 0:\n    return Tag.__getattr__(self, methodName)\nelse:\n    raise AttributeError", "path": "BeautifulSoup.py", "repo_name": "vimjolts/joltserver", "stars": 16, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"Returns true iff this tag has the same name, the same attributes,\nand the same contents (recursively) as the given tag.\n\nNOTE: right now this will return false if two tags have the\nsame attributes in a different order. Should this be fixed?\"\"\"\n", "func_signal": "def __eq__(self, other):\n", "code": "if not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):\n    return False\nfor i in range(0, len(self.contents)):\n    if self.contents[i] != other.contents[i]:\n        return False\nreturn True", "path": "BeautifulSoup.py", "repo_name": "vimjolts/joltserver", "stars": 16, "license": "None", "language": "python", "size": 276}
{"docstring": "# Read JSON POST input to jsonfilter.json if matching mime type\n", "func_signal": "def __call__(self, environ, start_response):\n", "code": "response = {'status': '200 OK', 'headers': []}\ndef json_start_response(status, headers):\n    response['status'] = status\n    response['headers'].extend(headers)\nenviron['jsonfilter.mime_type'] = self.mime_type\nif environ.get('REQUEST_METHOD', '') == 'POST':\n    if environ.get('CONTENT_TYPE', '') == self.mime_type:\n        args = [_ for _ in [environ.get('CONTENT_LENGTH')] if _]\n        data = environ['wsgi.input'].read(*map(int, args))\n        environ['jsonfilter.json'] = simplejson.loads(data)\nres = simplejson.dumps(self.app(environ, json_start_response))\njsonp = cgi.parse_qs(environ.get('QUERY_STRING', '')).get('jsonp')\nif jsonp:\n    content_type = 'text/javascript'\n    res = ''.join(jsonp + ['(', res, ')'])\nelif 'Opera' in environ.get('HTTP_USER_AGENT', ''):\n    # Opera has bunk XMLHttpRequest support for most mime types\n    content_type = 'text/plain'\nelse:\n    content_type = self.mime_type\nheaders = [\n    ('Content-type', content_type),\n    ('Content-length', len(res)),\n]\nheaders.extend(response['headers'])\nstart_response(response['status'], headers)\nreturn [res]", "path": "simplejson\\jsonfilter.py", "repo_name": "vimjolts/joltserver", "stars": 16, "license": "None", "language": "python", "size": 276}
{"docstring": "# Convert the document to Unicode.\n", "func_signal": "def _feed(self, inDocumentEncoding=None):\n", "code": "markup = self.markup\nif isinstance(markup, unicode):\n    if not hasattr(self, 'originalEncoding'):\n        self.originalEncoding = None\nelse:\n    dammit = UnicodeDammit\\\n             (markup, [self.fromEncoding, inDocumentEncoding],\n              smartQuotesTo=self.smartQuotesTo)\n    markup = dammit.unicode\n    self.originalEncoding = dammit.originalEncoding\nif markup:\n    if self.markupMassage:\n        if not isList(self.markupMassage):\n            self.markupMassage = self.MARKUP_MASSAGE\n        for fix, m in self.markupMassage:\n            markup = fix.sub(m, markup)\n        # TODO: We get rid of markupMassage so that the\n        # soup object can be deepcopied later on. Some\n        # Python installations can't copy regexes. If anyone\n        # was relying on the existence of markupMassage, this\n        # might cause problems.\n        del(self.markupMassage)\nself.reset()\n\nSGMLParser.feed(self, markup)\n# Close out any unfinished strings and close all the open tags.\nself.endData()\nwhile self.currentTag.name != self.ROOT_TAG_NAME:\n    self.popTag()", "path": "BeautifulSoup.py", "repo_name": "vimjolts/joltserver", "stars": 16, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"This method fixes a bug in Python's SGMLParser.\"\"\"\n", "func_signal": "def convert_charref(self, name):\n", "code": "try:\n    n = int(name)\nexcept ValueError:\n    return\nif not 0 <= n <= 127 : # ASCII ends at 127, not 255\n    return\nreturn self.convert_codepoint(n)", "path": "BeautifulSoup.py", "repo_name": "vimjolts/joltserver", "stars": 16, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"\ndemjson, python-cjson compatibility hook. Use dumps(s) instead.\n\"\"\"\n", "func_signal": "def encode(obj):\n", "code": "import warnings\nwarnings.warn(\"simplejson.dumps(s) should be used instead of encode(s)\",\n    DeprecationWarning)\nreturn dumps(obj)", "path": "simplejson\\__init__.py", "repo_name": "vimjolts/joltserver", "stars": 16, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"Adds a certain piece of text to the tree as a NavigableString\nsubclass.\"\"\"\n", "func_signal": "def _toStringSubclass(self, text, subclass):\n", "code": "self.endData()\nself.handle_data(text)\nself.endData(subclass)", "path": "BeautifulSoup.py", "repo_name": "vimjolts/joltserver", "stars": 16, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"Setting tag[key] sets the value of the 'key' attribute for the\ntag.\"\"\"\n", "func_signal": "def __setitem__(self, key, value):\n", "code": "self._getAttrMap()\nself.attrMap[key] = value\nfound = False\nfor i in range(0, len(self.attrs)):\n    if self.attrs[i][0] == key:\n        self.attrs[i] = (key, value)\n        found = True\nif not found:\n    self.attrs.append((key, value))\nself._getAttrMap()[key] = value", "path": "BeautifulSoup.py", "repo_name": "vimjolts/joltserver", "stars": 16, "license": "None", "language": "python", "size": 276}
{"docstring": "#print \"Push\", tag.name\n", "func_signal": "def pushTag(self, tag):\n", "code": "if self.currentTag:\n    self.currentTag.contents.append(tag)\nself.tagStack.append(tag)\nself.currentTag = self.tagStack[-1]", "path": "BeautifulSoup.py", "repo_name": "vimjolts/joltserver", "stars": 16, "license": "None", "language": "python", "size": 276}
{"docstring": "#print \"End tag %s\" % name\n", "func_signal": "def unknown_endtag(self, name):\n", "code": "if self.quoteStack and self.quoteStack[-1] != name:\n    #This is not a real end tag.\n    #print \"</%s> is not real!\" % name\n    self.handle_data('</%s>' % name)\n    return\nself.endData()\nself._popToTag(name)\nif self.quoteStack and self.quoteStack[-1] == name:\n    self.quoteStack.pop()\n    self.literal = (len(self.quoteStack) > 0)", "path": "BeautifulSoup.py", "repo_name": "vimjolts/joltserver", "stars": 16, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"\nReturn a JSON string representation of a Python data structure.\n\n>>> JSONEncoder().encode({\"foo\": [\"bar\", \"baz\"]})\n'{\"foo\": [\"bar\", \"baz\"]}'\n\"\"\"\n# This is for extremely simple cases and benchmarks...\n", "func_signal": "def encode(self, o):\n", "code": "if isinstance(o, basestring):\n    if isinstance(o, str):\n        _encoding = self.encoding\n        if (_encoding is not None \n                and not (_encoding == 'utf-8' and _need_utf8)):\n            o = o.decode(_encoding)\n    return encode_basestring_ascii(o)\n# This doesn't pass the iterator directly to ''.join() because it\n# sucks at reporting exceptions.  It's going to do this internally\n# anyway because it uses PySequence_Fast or similar.\nchunks = list(self.iterencode(o))\nreturn ''.join(chunks)", "path": "simplejson\\encoder.py", "repo_name": "vimjolts/joltserver", "stars": 16, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"Returns the parents of this Tag that match the given\ncriteria.\"\"\"\n\n", "func_signal": "def findParents(self, name=None, attrs={}, limit=None, **kwargs):\n", "code": "return self._findAll(name, attrs, None, limit, self.parentGenerator,\n                     **kwargs)", "path": "BeautifulSoup.py", "repo_name": "vimjolts/joltserver", "stars": 16, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"\ndemjson, python-cjson API compatibility hook. Use loads(s) instead.\n\"\"\"\n", "func_signal": "def decode(s):\n", "code": "import warnings\nwarnings.warn(\"simplejson.loads(s) should be used instead of decode(s)\",\n    DeprecationWarning)\nreturn loads(s)", "path": "simplejson\\__init__.py", "repo_name": "vimjolts/joltserver", "stars": 16, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"Handle entity references as data, possibly converting known\nHTML and/or XML entity references to the corresponding Unicode\ncharacters.\"\"\"\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "data = None\nif self.convertHTMLEntities:\n    try:\n        data = unichr(name2codepoint[ref])\n    except KeyError:\n        pass\n\nif not data and self.convertXMLEntities:\n        data = self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref)\n\nif not data and self.convertHTMLEntities and \\\n    not self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref):\n        # TODO: We've got a problem here. We're told this is\n        # an entity reference, but it's not an XML entity\n        # reference or an HTML entity reference. Nonetheless,\n        # the logical thing to do is to pass it through as an\n        # unrecognized entity reference.\n        #\n        # Except: when the input is \"&carol;\" this function\n        # will be called with input \"carol\". When the input is\n        # \"AT&T\", this function will be called with input\n        # \"T\". We have no way of knowing whether a semicolon\n        # was present originally, so we don't know whether\n        # this is an unknown entity or just a misplaced\n        # ampersand.\n        #\n        # The more common case is a misplaced ampersand, so I\n        # escape the ampersand and omit the trailing semicolon.\n        data = \"&amp;%s\" % ref\nif not data:\n    # This case is different from the one above, because we\n    # haven't already gone through a supposedly comprehensive\n    # mapping of entities to Unicode characters. We might not\n    # have gone through any mapping at all. So the chances are\n    # very high that this is a real entity, and not a\n    # misplaced ampersand.\n    data = \"&%s;\" % ref\nself.handle_data(data)", "path": "BeautifulSoup.py", "repo_name": "vimjolts/joltserver", "stars": 16, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"\nReturn a JSON representation of a Python string\n\"\"\"\n", "func_signal": "def encode_basestring(s):\n", "code": "def replace(match):\n    return ESCAPE_DCT[match.group(0)]\nreturn '\"' + ESCAPE.sub(replace, s) + '\"'", "path": "simplejson\\encoder.py", "repo_name": "vimjolts/joltserver", "stars": 16, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"Destructively rips this element out of the tree.\"\"\"\n", "func_signal": "def extract(self):\n", "code": "if self.parent:\n    try:\n        self.parent.contents.remove(self)\n    except ValueError:\n        pass\n\n#Find the two elements that would be next to each other if\n#this element (and any children) hadn't been parsed. Connect\n#the two.\nlastChild = self._lastRecursiveChild()\nnextElement = lastChild.next\n\nif self.previous:\n    self.previous.next = nextElement\nif nextElement:\n    nextElement.previous = self.previous\nself.previous = None\nlastChild.next = None\n\nself.parent = None\nif self.previousSibling:\n    self.previousSibling.nextSibling = self.nextSibling\nif self.nextSibling:\n    self.nextSibling.previousSibling = self.previousSibling\nself.previousSibling = self.nextSibling = None\nreturn self", "path": "BeautifulSoup.py", "repo_name": "vimjolts/joltserver", "stars": 16, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"Returns the closest sibling to this Tag that matches the\ngiven criteria and appears after this Tag in the document.\"\"\"\n", "func_signal": "def findNextSibling(self, name=None, attrs={}, text=None, **kwargs):\n", "code": "return self._findOne(self.findNextSiblings, name, attrs, text,\n                     **kwargs)", "path": "BeautifulSoup.py", "repo_name": "vimjolts/joltserver", "stars": 16, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"Given a document, tries to detect its XML encoding.\"\"\"\n", "func_signal": "def _detectEncoding(self, xml_data):\n", "code": "xml_encoding = sniffed_xml_encoding = None\ntry:\n    if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n        # EBCDIC\n        xml_data = self._ebcdic_to_ascii(xml_data)\n    elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n        # UTF-16BE\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') \\\n             and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16BE with BOM\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n        # UTF-16LE\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and \\\n             (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16LE with BOM\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n        # UTF-32BE\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n        # UTF-32LE\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n        # UTF-32BE with BOM\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n        # UTF-32LE with BOM\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n    elif xml_data[:3] == '\\xef\\xbb\\xbf':\n        # UTF-8 with BOM\n        sniffed_xml_encoding = 'utf-8'\n        xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n    else:\n        sniffed_xml_encoding = 'ascii'\n        pass\n    xml_encoding_match = re.compile \\\n                         ('^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>')\\\n                         .match(xml_data)\nexcept:\n    xml_encoding_match = None\nif xml_encoding_match:\n    xml_encoding = xml_encoding_match.groups()[0].lower()\n    if sniffed_xml_encoding and \\\n       (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',\n                         'iso-10646-ucs-4', 'ucs-4', 'csucs4',\n                         'utf-16', 'utf-32', 'utf_16', 'utf_32',\n                         'utf16', 'u16')):\n        xml_encoding = sniffed_xml_encoding\nreturn xml_data, xml_encoding, sniffed_xml_encoding", "path": "BeautifulSoup.py", "repo_name": "vimjolts/joltserver", "stars": 16, "license": "None", "language": "python", "size": 276}
{"docstring": "#123 is a made up type\n", "func_signal": "def test_unknown_type(self):\n", "code": "\t\tself.assertRaises(TypeError,self.create_invalid_prepinfocolumn,123,0)\n\t\t#696 is LONGVARBYTE which is real, but not supported\n\t\tself.assertRaises(ValueError,self.create_invalid_prepinfocolumn,696,0)", "path": "dwhwrapper\\tests\\test_tdcli.py", "repo_name": "xlfe/dwhwrapper", "stars": 24, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"returns a byte string in indicdata format\"\"\"\n\n", "func_signal": "def pack(self,nulls):\n", "code": "n = 0\nbytes = []\n\nfor i in range(0,self.indic_data_len):\n\t\n\tbyte = 0\n\t\n\tfor octet in range( 0, min(8,len(nulls)-n) ):\n\t\toctet = 7 - octet\n\t\tbyte = byte + (nulls[n] << (octet))\n\t\tn += 1\n\t\t\n\tbytes.append(byte)\n\t\nreturn b''.join(chr(b) for b in bytes)", "path": "dwhwrapper\\tdcli.py", "repo_name": "xlfe/dwhwrapper", "stars": 24, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"take a single byte character and insert it at offset\"\"\"\n", "func_signal": "def insert_byte(self,offset,byte):\n", "code": "assert isinstance(byte,type(b''))\nassert len(byte) == 1\nself.format.insert(offset,'B')\nself.data.insert(offset,ord(byte))\nself.row_len += 1", "path": "dwhwrapper\\tdcli.py", "repo_name": "xlfe/dwhwrapper", "stars": 24, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"binary safe conversion from binary fast-export format to csv\n\nddf - contains definitions of all fields required for conversion\nfexp_file - filename of binary fastexp file we read from\nargs - arguments passed by the user (eg verbosity, output file)\n\"\"\"\n\n", "func_signal": "def fexp_to_csv(ddf,fexp_file,args):\n", "code": "if args.use_column_titles is True:\n\theader_nm = 'Title'\nelse:\n\theader_nm = 'Name'\n\ncols=[]\ntd_types = []\n\nfor fd in ddf:\n\n\tcols.append(fd[header_nm])\n\ttry:\n\t\t#define each td_type with appropriate type_TYPE handler\n\t\ttd_types.append(\n\t\t\tglobals()['type_{0}'.format(fd['Type'].lower())](fd)\n\t\t)\n\texcept KeyError:\n\t\traise Exception(\"Unable to find handler class '{0}' for '{1}'\".format(\n\t\t\tfd['Type'],fd['Title']))\n\nwith file(args.output,'w') as out_file:\n\tout = csv.writer(out_file,quoting=csv.QUOTE_MINIMAL)\n\t\n\tout.writerow(cols)\n\t\n\twith open(fexp_file,'rb') as input:\n\t\t\n\t\twhile True:\n\t\t\t\n\t\t\theader = input.read(2)\n\t\t\tif len(header) < 2:\n\t\t\t\t#EOF\n\t\t\t\tbreak\n\t\t\t\n\t\t\trow_len = struct.unpack('H',header)[0]\n\t\t\trow_data = input.read(row_len)\n\t\t\t\n\t\t\truh = row_unpack_handler(row_data,len(td_types))\n\t\t\t\n\t\t\trow_items = ruh.unpack_row(td_types,row_data,row_len) #fexpb.row_unpack(row_data,row_len[0])\n\t\t\t\t\n\t\t\tout.writerow(row_items)\n\t\t\tinput.read(1) #End of record indicator is a single newline char..", "path": "dwhwrapper\\tdcli.py", "repo_name": "xlfe/dwhwrapper", "stars": 24, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"float\"\"\"\n\n", "func_signal": "def gen_floats(self):\n", "code": "float_size = 8\n\nddf = self.dummy_ddf('FLOAT',float_size)\n\nfloats = { 'min':float('4.9406564584124654E-324')\n ,'max':float('1.7976931348623157E+308')\n ,'zero':float('0')\n ,'neg_z':float('-0')\n ,'inf':float('inf')\n ,'neginf':float('-inf')\n #,'nan':float('nan')\n \n }\n\nfloats['random'] = random.uniform(floats['min'],floats['max'])\n\nreturn ddf,floats", "path": "dwhwrapper\\tests\\test_tdcli.py", "repo_name": "xlfe/dwhwrapper", "stars": 24, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"byteint\"\"\"\n", "func_signal": "def test_byteint(self):\n", "code": "ddf = self.dummy_ddf('BYTEINT',1)\nself.int_range(ddf,-128,127)", "path": "dwhwrapper\\tests\\test_tdcli.py", "repo_name": "xlfe/dwhwrapper", "stars": 24, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"PrepInfoColumn\"\"\"\n\n", "func_signal": "def test_varchar_decode(self):\n", "code": "data_type=449\t\t#Varchar\ndata_len=256\ncol_name='TestingColumn this is a test'\ncolumn_format='TestingFormat a very simple test'\ncolumn_title='TestingTitle let us hope it passes!'\n\ndata = struct.pack('=HHH{0}sH{1}sH{2}s'. \\\n\t\t\t\t   format(len(col_name),\n\t\t\t\t\t\tlen(column_format),\n\t\t\t\t\t\tlen(column_title)\n\t\t\t\t\t\t),\n\t\t\t\t\tdata_type,data_len,\n\t\t\t\t\tlen(col_name),col_name,\n\t\t\t\t\tlen(column_format),column_format,\n\t\t\t\t\tlen(column_title),column_title\n\t\t\t\t\t)\n\n#raise Exception('\\n'.join(d for d in data))\npic = tdcli.PrepInfoColumn(data)\n\nself.assertEqual(pic.data_type_name,'VARCHAR')\nself.assertNotEqual(pic.data_type_allows_nulls,False)\nself.assertEqual(pic.pic_length,10 + len(col_name)+len(column_format)+len(column_title))\nself.assertEqual(pic.col_name,col_name)\nself.assertEqual(pic.column_format,column_format)\nself.assertEqual(pic.column_title,column_title)", "path": "dwhwrapper\\tests\\test_tdcli.py", "repo_name": "xlfe/dwhwrapper", "stars": 24, "license": "None", "language": "python", "size": 115}
{"docstring": "#\n# WARNING - we assume we're working with LATIN character set ISO8859-1\n#\n#\n", "func_signal": "def gen_varchar(self):\n", "code": "length = random.randint(0,64000)\t\t\nstring = ISO8859(length)\n\nddf = self.dummy_ddf('VARCHAR',length)\n\nreturn ddf,string", "path": "dwhwrapper\\tests\\test_tdcli.py", "repo_name": "xlfe/dwhwrapper", "stars": 24, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"wrapper to call the DBCHCL function -\ndon't need to pass much because it's all setup in dbcarea\"\"\"\n\n", "func_signal": "def dbchcl(self,type):\n", "code": "self.result = ctypes.c_int(self.EM_OK)\nself.dbcarea.func=type\n\nself.cli.DBCHCL(ctypes.byref(self.result), ctypes.byref(self.cnta), ctypes.byref(self.dbcarea))\n\nreturn self.result.value", "path": "dwhwrapper\\tdcli.py", "repo_name": "xlfe/dwhwrapper", "stars": 24, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"floats\"\"\"\n\n", "func_signal": "def test_float(self):\n", "code": "ddf,floats = self.gen_floats()\nfor f in floats:\n\tself.pack_unpack(ddf,floats[f])", "path": "dwhwrapper\\tests\\test_tdcli.py", "repo_name": "xlfe/dwhwrapper", "stars": 24, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"binary safe conversion from csv to fast-export binary format\n\nddf - contains definitions of all fields in the csv (and potential fields not in the csv)\ncsv_file - filename of csv we read from\nfexp_file - filename of fexp file we write to\nargs - arguments passed by the user (eg verbosity)\n\n\nreturns the actual ddfs used\n\"\"\"\n\n\n", "func_signal": "def csv_to_fexp(ddf,csv_file,fexp_file,args):\n", "code": "with open(csv_file,'r') as f:\n\t\n\tdict_reader = csv.DictReader(f)\n\t\n\tif args.use_column_titles is True:\n\t\tcolumn = 'Title'\n\telse:\n\t\tcolumn = 'Name'\n\t\n\t#td_types for use\n\ttd_types=[]\n\t\n\t#available field definitions\n\tdefined_fields = [fd[column] for fd in ddf]\n\t\n\t#compare available_fields with the headers from the csv file being read\n\tfor field in dict_reader.fieldnames:\n\t\tif field not in defined_fields:\n\t\t\traise Exception(\"'{0}' not defined in '{1}' - unable to continue\".format(\n\t\t\t\tfield,args.dest))\n\t\telse:\n\t\t\tfor fd in ddf:\n\t\t\t\tif fd[column] == field:\n\t\t\t\t\ttry:\n\t\t\t\t\t\t#define each td_type with appropriate type_TYPE handler\n\t\t\t\t\t\ttd_types.append(\n\t\t\t\t\t\t\tglobals()['type_{0}'.format(fd['Type'].lower())](fd)\n\t\t\t\t\t\t\t)\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\traise Exception(\"Unable to find handler class '{0}' for '{1}'\".format(\n\t\t\t\t\t\t\tfd['Type'],fd['Title']))\n\t\t\t\t\t\t\t\n\t\t\t\t\t\n\twith open(fexp_file,'wb') as out:\n\t\n\t\tfor row in dict_reader:\n\t\t\t\n\t\t\tif len(row) < len(td_types):\n\t\t\t\traise Exception('Row is missing columns')\n\t\t\t\t\n\t\t\telif len(row) > len(td_types):\n\t\t\t\traise Exception('Row has too many columns')\n\t\t\t\t\n\t\t\trph = row_pack_handler()\n\t\t\t\n\t\t\tfor td_type in td_types:\n\t\t\t\t\n\t\t\t\tr = row[td_type.fd[column]]\n\t\t\t\t\n\t\t\t\tif r is None or len(r) == 0:\n\t\t\t\t\tif td_type.fd['Type'] in ['CHAR','VARCHAR']:\n\t\t\t\t\t\t\n\t\t\t\t\t\tr=''\n\t\t\t\t\t\trph.define_null(False)\n\t\t\t\t\telse:\n\t\t\t\t\t\tif td_type.fd['Nulls'] is False:\n\t\t\t\t\t\t\traise ValueError('{0} has an empty value, but is defined as NON NULL'.format(\n\t\t\t\t\t\t\t\ttd_type.fd['Name']))\n\t\t\t\t\t\t\n\t\t\t\t\t\trph.add_data(td_type=td_type,data=0)\n\t\t\t\t\t\trph.define_null(True)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t\n\t\t\t\telse:\n\t\t\t\t\trph.define_null(False)\n\t\t\t\t\t\n\t\t\t\t#depending on the type, this calls type_TYPE.pack()\n\t\t\t\trph.pack(td_type=td_type,data=r)\n\t\t\t\n\t\t\tout.write(rph.pack_row(len(td_types)))\n\t\t\nreturn [td_type.fd for td_type in td_types]", "path": "dwhwrapper\\tdcli.py", "repo_name": "xlfe/dwhwrapper", "stars": 24, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"cliv2 lib\"\"\"\n\n", "func_signal": "def test_dbc_init(self):\n", "code": "self.assertEqual(self.dbc.dbcarea.total_len,640)\n\nself.assertEqual(self.dbc.result.value,self.dbc.EM_OK)\n\n#Check a few options that are setup as defaults\nself.assertEqual(self.dbc.dbcarea.req_proc_opt,'P')\nself.assertEqual(self.dbc.dbcarea.parcel_mode,'Y')\nself.assertEqual(self.dbc.dbcarea.resp_mode,'I')\n\nself.assertNotEqual(self.dbc.dbcarea.tell_about_crash,'N')", "path": "dwhwrapper\\tests\\test_tdcli.py", "repo_name": "xlfe/dwhwrapper", "stars": 24, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"smallint\"\"\"\n", "func_signal": "def test_smallint(self):\n", "code": "ddf = self.dummy_ddf('SMALLINT',2)\nself.int_range(ddf,-32768,32767)", "path": "dwhwrapper\\tests\\test_tdcli.py", "repo_name": "xlfe/dwhwrapper", "stars": 24, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"extra type testing for edge cases\"\"\"\n\n", "func_signal": "def test_xtra(self):\n", "code": "for type in tdcli.SUPPORTED_TYPES:\n\ttype_class = getattr(tdcli,'type_{0}'.format(type.lower()))\n\tfor i in range(0,random.randint(10,30)):\n\t\tgetattr(self,'test_{0}'.format(type.lower()))()", "path": "dwhwrapper\\tests\\test_tdcli.py", "repo_name": "xlfe/dwhwrapper", "stars": 24, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"add data in format specified by data_type and data_length in td_type\"\"\"\n", "func_signal": "def add_data(self,td_type,data):\n", "code": "self.format.append(td_type.data_type)\nself.data.append(data)\nself.row_len += td_type.data_length", "path": "dwhwrapper\\tdcli.py", "repo_name": "xlfe/dwhwrapper", "stars": 24, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"pack/unpack multiple rows and columns\"\"\"\n\n", "func_signal": "def test_multirow(self):\n", "code": "n_rows = random.randint(1,10)\nn_columns = random.randint(1,100)\ni=0\ncols=[]\nprev_type= type=None\n\nfor c in range(0,n_columns):\n\t\n\twhile type == prev_type:\n\t\ttype = random.choice(tdcli.SUPPORTED_TYPES)\n\t\n\tcols.append(type)\n\tprev_type = type\n\t\n\nwhile i < n_rows:\n\t\n\trph = tdcli.row_pack_handler()\n\ttd_types=[]\n\titems=[]\n\trow_size = 0\n\t\n\tfor c in range(0,n_columns):\n\t\t\n\t\tddf,data = getattr(self,'gen_{0}'.format(cols[c].lower()))()\n\t\t\n\t\ttd_type = getattr(tdcli,'type_{0}'.format(cols[c].lower()))\n\t\tnull = bool(random.randint(0,1))\n\t\tinstance = td_type(ddf)\n\t\t\n\t\tif ddf['Type'] in ['CHAR','VARCHAR']:\n\t\t\tl = len(data)\n\t\telse:\n\t\t\tl = 10\n\t\t\n\t\tif rph.row_len + l + instance.data_length + len(items) > 65534:\n\t\t\tself.assertRaises(OverflowError,rph.pack,instance,data)\n\t\telse:\n\t\t\trph.pack(instance,data)\n\t\t\ttd_types.append(instance)\n\t\t\trph.define_null(null)\n\t\t\t\n\t\t\tif null is True:\n\t\t\t\titems.append(None)\n\t\t\telse:\n\t\t\t\titems.append(data)\n\t\t\n\trow_data = rph.pack_row(len(items))\n\t\t\n\trow_length = struct.unpack('H',row_data[:2])[0]\n\t\n\tself.assertTrue(row_length < 65535)\n\t\n\truh = tdcli.row_unpack_handler(row_data[2:],len(items))\n\trow_items = ruh.unpack_row(td_types,row_data[2:],row_length)\n\t\n\tfor input in items:\n\t\toutput = row_items.pop(0)\n\t\tself.assertEqual(input,output)\n\t\t\n\ti += 1", "path": "dwhwrapper\\tests\\test_tdcli.py", "repo_name": "xlfe/dwhwrapper", "stars": 24, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"fetches responses from the dbc -\ncontinues until one of the parcels in return_on_parcels is seen\"\"\"\n\n", "func_signal": "def fetch_request(self,return_on_parcels=[]):\n", "code": "while True:\n\t\n\tself.result = self.dbchcl(self.DBFFET)\n\t\n\tif self.result == self.REQEXHAUST:\n\t\treturn self.REQEXHAUST\n\t\n\telif self.result != self.EM_OK:\n\t\traise Exception(\"Fetch failed {0}\".format(self.dbcarea.msg_text))\n\t\t\n\tif self.dbcarea.fet_parcel_flavor in return_on_parcels:\n\t\treturn self.dbcarea.fet_parcel_flavor\n\t\n\telif self.dbcarea.fet_parcel_flavor == self.PclFAILURE or \\\n\t\t\t\tself.dbcarea.fet_parcel_flavor == self.PclERROR:\t\t\t\t\t\t\n\t\t\n\t\tcf = ctypes.cast(self.dbcarea.fet_data_ptr,ctypes.POINTER(cli_failure))[0]\n\t\traise Exception(\"STATEMENT:{0} ERR:{1} {2}\".format(cf.StatementNo,cf.Code,cf.Msg[:cf.Length]))\n\t\n\t#print 'Parcel Flavour {0}'.format(self.dbcarea.fet_parcel_flavor)", "path": "dwhwrapper\\tdcli.py", "repo_name": "xlfe/dwhwrapper", "stars": 24, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"packs a completed row and returns the binary data\"\"\"\n\n", "func_signal": "def pack_row(self,column_count):\n", "code": "id = indic_data(column_count)\nid_bytes = id.pack(nulls=self.nulls)\n\nassert(self.row_len < 65535)\n\nfor id_byte_n in range(0,len(id_bytes)):\n\tself.insert_byte(id_byte_n,id_bytes[id_byte_n])\n\t\nassert(self.row_len < 65535)\n\nself.format.insert(0,'H')\nself.data.insert(0,self.row_len)\n\nself.format.append('B')\nself.data.append(ord('\\n'))\n\n#Specify native byte order and no alignment\nself.format.insert(0,'=')\t\t\n\nfmt = ''.join(f for f in self.format)\nreturn struct.pack(fmt,*self.data)", "path": "dwhwrapper\\tdcli.py", "repo_name": "xlfe/dwhwrapper", "stars": 24, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"add data in a format different from that specified in td_type\"\"\"\n", "func_signal": "def add_custom_data(self,data_type,data_length,data):\n", "code": "self.format.append(data_type)\nself.data.append(data)\nself.row_len += data_length", "path": "dwhwrapper\\tdcli.py", "repo_name": "xlfe/dwhwrapper", "stars": 24, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"Unpack a byte-string of indicator format data\nand return an array of boolean values\"\"\"\n\t\n", "func_signal": "def unpack(self,indic_data):\n", "code": "assert isinstance(indic_data,type(b''))\nnulls = []\n\t\nfor col in range(0,self.column_count):\n\t\n\tbyte = ord(indic_data[col / 8])\n\tbit = 7 - (col % 8)\n\t\n\tis_null = (byte >> bit) & 1\n\t\n\tnulls.append(bool(is_null))\n\nreturn nulls", "path": "dwhwrapper\\tdcli.py", "repo_name": "xlfe/dwhwrapper", "stars": 24, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"\nRecord a user's vote on a given object. Only allows a given user\nto vote once, though that vote may be changed.\n\nA zero vote indicates that any existing vote should be removed.\n\"\"\"\n", "func_signal": "def record_vote(self, obj, user, vote):\n", "code": "if vote not in (+1, 0, -1):\n    raise ValueError('Invalid vote (must be +1/0/-1)')\nctype = ContentType.objects.get_for_model(obj)\ntry:\n    v = self.get(user=user, content_type=ctype,\n                 object_id=obj._get_pk_val())\n    \n    # previous vote exists, so remove the old one from the object\n    if v.vote == 1:\n        obj.yeas -= 1\n    if v.vote == -1:\n        obj.nays -= 1\n    \n    if vote == 0:\n        v.delete()\n        # the user wants to clear their vote so\n        obj.votes -= 1\n    else:\n        v.vote = vote\n        v.save()\nexcept models.ObjectDoesNotExist:\n    if vote != 0:\n        self.create(user=user, content_type=ctype,\n                    object_id=obj._get_pk_val(), vote=vote)\n        # we just created a new vote so \n        obj.votes += 1\n\n# update the denorm fields\nif vote == 1:\n    obj.yeas += 1\nif vote == -1:\n    obj.nays += 1\n\nobj.save()", "path": "colab\\apps\\voting\\managers.py", "repo_name": "caseywstark/colab", "stars": 25, "license": "mit", "language": "python", "size": 1255}
{"docstring": "\"\"\"\nGet a dictionary mapping object ids to total score and number\nof votes for each object.\n\"\"\"\n", "func_signal": "def get_scores_in_bulk(self, objects):\n", "code": "object_ids = [o._get_pk_val() for o in objects]\nif not object_ids:\n    return {}\n\nctype = ContentType.objects.get_for_model(objects[0])\nctype = ContentType.objects.get_for_model(objects[0])\n\nif supports_aggregates:\n    queryset = self.filter(\n        object_id__in = object_ids,\n        content_type = ctype,\n    ).values(\n        'object_id',\n    ).annotate(\n        score = CoalesceSum('vote', default='0'),\n        num_votes = CoalesceCount('vote', default='0'),\n    )\nelse:\n    queryset = self.filter(\n        object_id__in = object_ids,\n        content_type = ctype,\n        ).extra(\n            select = {\n                'score': 'COALESCE(SUM(vote), 0)',\n                'num_votes': 'COALESCE(COUNT(vote), 0)',\n            }\n        ).values('object_id', 'score', 'num_votes')\n    queryset.query.group_by.append('object_id')\n\nvote_dict = {}\nfor row in queryset:\n    vote_dict[row['object_id']] = {\n        'score': int(row['score']),\n        'num_votes': int(row['num_votes']),\n    }\n\nreturn vote_dict", "path": "colab\\apps\\voting\\managers.py", "repo_name": "caseywstark/colab", "stars": 25, "license": "mit", "language": "python", "size": 1255}
{"docstring": "\"\"\"\nProvides the credentials required to authenticate the user after\nsign-up is completed.\n\"\"\"\n", "func_signal": "def user_credentials(self):\n", "code": "credentials = {}\nif EMAIL_AUTHENTICATION:\n    credentials[\"email\"] = self.cleaned_data[\"email\"]\nelse:\n    credentials[\"username\"] = self.cleaned_data[\"username\"]\ncredentials[\"password\"] = self.cleaned_data[\"password1\"]\nreturn credentials", "path": "colab\\apps\\account\\forms.py", "repo_name": "caseywstark/colab", "stars": 25, "license": "mit", "language": "python", "size": 1255}
{"docstring": "\"\"\"\nThe part that's the least straightforward about views in this module is how they \ndetermine their redirects after they have finished computation.\n\nIn short, they will try and determine the next place to go in the following order:\n\n1. If there is a variable named ``next`` in the *POST* parameters, the view will\nredirect to that variable's value.\n2. If there is a variable named ``next`` in the *GET* parameters, the view will\nredirect to that variable's value.\n3. If Django can determine the previous page from the HTTP headers, the view will\nredirect to that previous page.\n4. Otherwise, the view raise a 404 Not Found.\n\"\"\"\n", "func_signal": "def _get_next(request):\n", "code": "next = request.POST.get('next', request.GET.get('next', request.META.get('HTTP_REFERER', None)))\nif not next or next == request.path:\n    raise Http404 # No next url was supplied in GET or POST.\nreturn next", "path": "colab\\apps\\threadedcomments\\views.py", "repo_name": "caseywstark/colab", "stars": 25, "license": "mit", "language": "python", "size": 1255}
{"docstring": "# remember provided (validated!) OpenID to attach it to the new user\n# later.\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "self.openid = kwargs.pop(\"openid\", None)\n# pop these off since they are passed to this method but we can\"t\n# pass them to forms.Form.__init__\nkwargs.pop(\"reserved_usernames\", [])\nkwargs.pop(\"no_duplicate_emails\", False)\n\nsuper(OpenIDSignupForm, self).__init__(*args, **kwargs)\n\nif REQUIRED_EMAIL or EMAIL_VERIFICATION or EMAIL_AUTHENTICATION:\n    self.fields[\"email\"].label = ugettext(\"E-mail\")\n    self.fields[\"email\"].required = True\nelse:\n    self.fields[\"email\"].label = ugettext(\"E-mail (optional)\")\n    self.fields[\"email\"].required = False", "path": "colab\\apps\\account\\forms.py", "repo_name": "caseywstark/colab", "stars": 25, "license": "mit", "language": "python", "size": 1255}
{"docstring": "\"\"\"\nDefault callback function to determine wether the given user has the\nability to delete the given comment.\n\"\"\"\n", "func_signal": "def can_delete_comment(comment, user):\n", "code": "if user.is_staff or user.is_superuser:\n    return True\nif hasattr(comment, 'user') and comment.user == user:\n    return True\nreturn False", "path": "colab\\apps\\threadedcomments\\views.py", "repo_name": "caseywstark/colab", "stars": 25, "license": "mit", "language": "python", "size": 1255}
{"docstring": "# @@@ use bridge\n", "func_signal": "def group_context(group, bridge):\n", "code": "return {\n    \"group\": group,\n}", "path": "colab\\apps\\account\\views.py", "repo_name": "caseywstark/colab", "stars": 25, "license": "mit", "language": "python", "size": 1255}
{"docstring": "\"\"\"\nDeletes the specified comment, which can be either a ``FreeThreadedComment`` or a\n``ThreadedComment``.  If it is a POST request, then the comment will be deleted\noutright, however, if it is a GET request, a confirmation page will be shown.\n\"\"\"\n", "func_signal": "def comment_delete(request, object_id, model=ThreadedComment, extra_context = {}, context_processors = [], permission_callback=can_delete_comment):\n", "code": "tc = get_object_or_404(model, id=int(object_id))\nif not permission_callback(tc, request.user):\n    login_url = settings.LOGIN_URL\n    current_url = urlquote(request.get_full_path())\n    return HttpResponseRedirect(\"%s?next=%s\" % (login_url, current_url))\nif request.method == \"POST\":\n    tc.delete()\n    return HttpResponseRedirect(_get_next(request))\nelse:\n    if model == ThreadedComment:\n        is_free_threaded_comment = False\n        is_threaded_comment = True\n    else:\n        is_free_threaded_comment = True\n        is_threaded_comment = False\n    return render_to_response(\n        'threadedcomments/confirm_delete.html',\n        extra_context, \n        context_instance = RequestContext(\n            request, \n            {\n                'comment' : tc, \n                'is_free_threaded_comment' : is_free_threaded_comment,\n                'is_threaded_comment' : is_threaded_comment,\n                'next' : _get_next(request),\n            },\n            context_processors\n        )\n    )", "path": "colab\\apps\\threadedcomments\\views.py", "repo_name": "caseywstark/colab", "stars": 25, "license": "mit", "language": "python", "size": 1255}
{"docstring": "\"\"\"\nGet the top N scored objects for a given model.\n\nYields (object, score) tuples.\n\"\"\"\n", "func_signal": "def get_top(self, Model, limit=10, reversed=False):\n", "code": "ctype = ContentType.objects.get_for_model(Model)\nquery = \"\"\"\nSELECT object_id, SUM(vote) as %s\nFROM %s\nWHERE content_type_id = %%s\nGROUP BY object_id\"\"\" % (\n    connection.ops.quote_name('score'),\n    connection.ops.quote_name(self.model._meta.db_table),\n)\n\n# MySQL has issues with re-using the aggregate function in the\n# HAVING clause, so we alias the score and use this alias for\n# its benefit.\nif settings.DATABASE_ENGINE == 'mysql':\n    having_score = connection.ops.quote_name('score')\nelse:\n    having_score = 'SUM(vote)'\nif reversed:\n    having_sql = ' HAVING %(having_score)s < 0 ORDER BY %(having_score)s ASC LIMIT %%s'\nelse:\n    having_sql = ' HAVING %(having_score)s > 0 ORDER BY %(having_score)s DESC LIMIT %%s'\nquery += having_sql % {\n    'having_score': having_score,\n}\n\ncursor = connection.cursor()\ncursor.execute(query, [ctype.id, limit])\nresults = cursor.fetchall()\n\n# Use in_bulk() to avoid O(limit) db hits.\nobjects = Model.objects.in_bulk([id for id, score in results])\n\n# Yield each object, score pair. Because of the lazy nature of generic\n# relations, missing objects are silently ignored.\nfor id, score in results:\n    if id in objects:\n        yield objects[id], int(score)", "path": "colab\\apps\\voting\\managers.py", "repo_name": "caseywstark/colab", "stars": 25, "license": "mit", "language": "python", "size": 1255}
{"docstring": "\"\"\"\n    {% blog_sections as blog_sections %}\n\"\"\"\n", "func_signal": "def blog_sections(parser, token):\n", "code": "bits = token.split_contents()\nreturn BlogSectionsNode(bits[2])", "path": "colab\\apps\\biblion\\templatetags\\biblion_tags.py", "repo_name": "caseywstark/colab", "stars": 25, "license": "mit", "language": "python", "size": 1255}
{"docstring": "# don\"t assume a username is available. it is a common removal if\n# site developer wants to use e-mail authentication.\n", "func_signal": "def save(self, request=None):\n", "code": "username = self.cleaned_data.get(\"username\")\nemail = self.cleaned_data[\"email\"]\n\nif self.cleaned_data[\"confirmation_key\"]:\n    from friends.models import JoinInvitation # @@@ temporary fix for issue 93\n    try:\n        join_invitation = JoinInvitation.objects.get(confirmation_key=self.cleaned_data[\"confirmation_key\"])\n        confirmed = True\n    except JoinInvitation.DoesNotExist:\n        confirmed = False\nelse:\n    confirmed = False\n\n# @@@ clean up some of the repetition below -- DRY!\n\nif confirmed:\n    if email == join_invitation.contact.email:\n        new_user = self.create_user(username)\n        join_invitation.accept(new_user) # should go before creation of EmailAddress below\n        if request:\n            messages.add_message(request, messages.INFO,\n                ugettext(u\"Your email address has already been verified\")\n            )\n        # already verified so can just create\n        EmailAddress(user=new_user, email=email, verified=True, primary=True).save()\n    else:\n        new_user = self.create_user(username)\n        join_invitation.accept(new_user) # should go before creation of EmailAddress below\n        if email:\n            if request:\n                messages.add_message(request, messages.INFO,\n                    ugettext(u\"Confirmation email sent to %(email)s\") % {\n                        \"email\": email,\n                    }\n                )\n            EmailAddress.objects.add_email(new_user, email)\nelse:\n    new_user = self.create_user(username)\n    if email:\n        if request and not EMAIL_VERIFICATION:\n            messages.add_message(request, messages.INFO,\n                ugettext(u\"Confirmation email sent to %(email)s\") % {\n                    \"email\": email,\n                }\n            )\n        EmailAddress.objects.add_email(new_user, email)\n\nif EMAIL_VERIFICATION:\n    new_user.is_active = False\n    new_user.save()\n\n### CoLab mods here ###\nname = self.cleaned_data[\"name\"]\nnew_researcher, created = Researcher.objects.get_or_create(user=new_user)\nnew_researcher.name = name\nnew_researcher.save()\n\nreturn self.user_credentials() # required for authenticate()", "path": "colab\\apps\\account\\forms.py", "repo_name": "caseywstark/colab", "stars": 25, "license": "mit", "language": "python", "size": 1255}
{"docstring": "\"\"\"\nReturns a preview of the comment so that the user may decide if he or she wants to\nedit it before submitting it permanently.\n\"\"\"\n", "func_signal": "def _preview(request, context_processors, extra_context, form_class=ThreadedCommentForm):\n", "code": "_adjust_max_comment_length(form_class)\nform = form_class(request.POST or None)\ncontext = {\n    'next' : _get_next(request),\n    'form' : form,\n}\nif form.is_valid():\n    new_comment = form.save(commit=False)\n    context['comment'] = new_comment\nelse:\n    context['comment'] = None\nreturn render_to_response(\n    'threadedcomments/preview_comment.html',\n    extra_context, \n    context_instance = RequestContext(request, context, context_processors)\n)", "path": "colab\\apps\\threadedcomments\\views.py", "repo_name": "caseywstark/colab", "stars": 25, "license": "mit", "language": "python", "size": 1255}
{"docstring": "\"\"\"\nGet a dictionary containing the total score for ``obj`` and\nthe number of votes it's received.\n\"\"\"\n", "func_signal": "def get_score(self, obj):\n", "code": "ctype = ContentType.objects.get_for_model(obj)\nresult = self.filter(object_id=obj._get_pk_val(),\n                     content_type=ctype).extra(\n    select={\n        'score': 'COALESCE(SUM(vote), 0)',\n        'num_votes': 'COALESCE(COUNT(vote), 0)',\n}).values_list('score', 'num_votes')[0]\n\nreturn {\n    'score': int(result[0]),\n    'num_votes': int(result[1]),\n}", "path": "colab\\apps\\voting\\managers.py", "repo_name": "caseywstark/colab", "stars": 25, "license": "mit", "language": "python", "size": 1255}
{"docstring": "\"\"\"\nThin wrapper around free_comment which adds login_required status and also assigns\nthe ``model`` to be ``ThreadedComment``.\n\"\"\"\n", "func_signal": "def comment(*args, **kwargs):\n", "code": "kwargs['model'] = ThreadedComment\nkwargs['form_class'] = ThreadedCommentForm\nreturn free_comment(*args, **kwargs)", "path": "colab\\apps\\threadedcomments\\views.py", "repo_name": "caseywstark/colab", "stars": 25, "license": "mit", "language": "python", "size": 1255}
{"docstring": "\"\"\"\n    {% latest_section_post \"articles\" as latest_article_post %}\n\"\"\"\n", "func_signal": "def latest_section_post(parser, token):\n", "code": "bits = token.split_contents()\nreturn LatestSectionPostNode(bits[1], bits[3])", "path": "colab\\apps\\biblion\\templatetags\\biblion_tags.py", "repo_name": "caseywstark/colab", "stars": 25, "license": "mit", "language": "python", "size": 1255}
{"docstring": "\"\"\"\nA wrapper for update_set used in order to distinguish between object\nfeeds and researcher feeds (they act differently).\n\n\"\"\"\n", "func_signal": "def updates(self):\n", "code": "from people.models import Researcher\nresearcher_type = ContentType.objects.get_for_model(Researcher)\nif self.content_type == researcher_type:\n    return Update.objects.filter(user__id=self.object_id)\nelse:\n    return self.update_set", "path": "colab\\apps\\object_feeds\\models.py", "repo_name": "caseywstark/colab", "stars": 25, "license": "mit", "language": "python", "size": 1255}
{"docstring": "# set the new user password\n", "func_signal": "def save(self):\n", "code": "user = self.user\nuser.set_password(self.cleaned_data[\"password1\"])\nuser.save()\n# mark password reset object as reset\nPasswordReset.objects.filter(temp_key=self.temp_key).update(reset=True)", "path": "colab\\apps\\account\\forms.py", "repo_name": "caseywstark/colab", "stars": 25, "license": "mit", "language": "python", "size": 1255}
{"docstring": "\"\"\"\nGiven kwargs from the view (with view specific keys popped) pull out the\nbridge and fetch group from database.\n\"\"\"\n\n", "func_signal": "def group_and_bridge(kwargs):\n", "code": "bridge = kwargs.pop(\"bridge\", None)\n\nif bridge:\n    try:\n        group = bridge.get_group(**kwargs)\n    except ObjectDoesNotExist:\n        raise Http404\nelse:\n    group = None\n\nreturn group, bridge", "path": "colab\\apps\\account\\views.py", "repo_name": "caseywstark/colab", "stars": 25, "license": "mit", "language": "python", "size": 1255}
{"docstring": "\"\"\"\nGet a dictionary mapping object ids to votes made by the given\nuser on the corresponding objects.\n\"\"\"\n", "func_signal": "def get_for_user_in_bulk(self, objects, user):\n", "code": "vote_dict = {}\nif len(objects) > 0:\n    ctype = ContentType.objects.get_for_model(objects[0])\n    votes = list(self.filter(content_type__pk=ctype.id,\n                             object_id__in=[obj._get_pk_val() \\\n                                            for obj in objects],\n                             user__pk=user.id))\n    vote_dict = dict([(vote.object_id, vote) for vote in votes])\nreturn vote_dict", "path": "colab\\apps\\voting\\managers.py", "repo_name": "caseywstark/colab", "stars": 25, "license": "mit", "language": "python", "size": 1255}
{"docstring": "\"\"\"\nProvides the credentials required to authenticate the user for\nlogin.\n\"\"\"\n", "func_signal": "def user_credentials(self):\n", "code": "credentials = {}\nif EMAIL_AUTHENTICATION:\n    credentials[\"email\"] = self.cleaned_data[\"email\"]\nelse:\n    credentials[\"username\"] = self.cleaned_data[\"username\"]\ncredentials[\"password\"] = self.cleaned_data[\"password\"]\nreturn credentials", "path": "colab\\apps\\account\\forms.py", "repo_name": "caseywstark/colab", "stars": 25, "license": "mit", "language": "python", "size": 1255}
{"docstring": "\"\"\" Start project as group \"\"\"\n", "func_signal": "def start_project(request, slug):\n", "code": "group = get_object_or_404(Group, slug__iexact=slug)\ndata = {'members': [request.user.id] }\nform = GroupProjectForm(group, request.POST or None, initial=data)\n\nif request.method == \"POST\" and form.is_valid():\n    project = form.save(request.user)\n\n    return HttpResponseRedirect(reverse('project-detail',\n                                        kwargs = {'slug': project.slug}))\n\nreturn direct_to_template(request,\n                          template='blgroup/form_project.html',\n                          extra_context={'form': form,\n                                         'group': group})", "path": "blgroup\\views.py", "repo_name": "bread-and-pepper/busylissy", "stars": 31, "license": "None", "language": "python", "size": 1230}
{"docstring": "\"\"\"\nChecks if user is allowed to see the contacts\n\"\"\"\n", "func_signal": "def get_contact(view):\n", "code": "def wrapper(request, project=None, company=None, *args, **kwargs):\n    if project:\n        try:\n            project = Project.objects.get(slug__iexact=project, members=request.user)\n        except Project.DoesNotExist:\n            raise Http404\n        return view(request, project=project, company=None, *args, **kwargs)\n    else:\n        #try:\n        #    company = Company.objects.get(slug__iexact=company, members=request.user)\n        #except Company.DoesNotExist:\n        raise Http404\n        return view(request, project=None, company=None, *args, **kwargs)\nreturn wraps(view)(wrapper)", "path": "blcontact\\decorators.py", "repo_name": "bread-and-pepper/busylissy", "stars": 31, "license": "None", "language": "python", "size": 1230}
{"docstring": "# In case of \"/rss/beats/0613/foo/bar/baz/\", or other such clutter,\n# check that bits has only one member.\n", "func_signal": "def get_object(self, bits):\n", "code": "if len(bits) != 2:\n    raise ObjectDoesNotExist\nelse:\n    user = get_object_or_404(User, username=bits[0])\n\n    salt = 'bltoken'\n    hash_obj = md5_constructor(str(user.username) + salt + str(user.pk))\n    if bits[1] != hash_obj.hexdigest():\n        raise Object.DoesNotExist\nreturn user", "path": "blactivity\\feeds.py", "repo_name": "bread-and-pepper/busylissy", "stars": 31, "license": "None", "language": "python", "size": 1230}
{"docstring": "\"\"\" Return calendar \"\"\"\n", "func_signal": "def week_cal(request, project_slug, direction, today=None):\n", "code": "now = datetime.datetime.now()\n\nif today is None:\n    today = now\nelse: today = datetime.datetime.strptime(today + '-' + str(now.year), \"%d-%m-%Y\")\nif direction == \"forw\":\n    today = today + datetime.timedelta(days=1)\nelse: today -= datetime.timedelta(days=8)\n\nif today.month < now.month or (today.year - now.year) != 0:\n    today = today.replace(year=today.year + 1)\n\nend_of_week = today + datetime.timedelta(days=7)\n\nmilestone_list = Task.objects.filter(project__slug=project_slug, completed=False, due_date__lte=end_of_week).order_by(\"-due_date\")\nproject = get_object_or_404(Project, slug=project_slug)\n\ndays = []\n\nday = today\nwhile day <= end_of_week:\n    cal_day = {}\n    cal_day['day'] = day\n    cal_day['milestone'] = False\n\n    for item in milestone_list:\n        if item.due_date and (day.date() == item.due_date):\n            cal_day['milestone'] = True\n            cal_day['event'] = item\n            \n    days.append(cal_day)\n    day += datetime.timedelta(days=1)\n\nreturn direct_to_template(request,\n                          template='bltask/cal.html',\n                          extra_context={'days':days, 'project': project})", "path": "bltask\\views.py", "repo_name": "bread-and-pepper/busylissy", "stars": 31, "license": "None", "language": "python", "size": 1230}
{"docstring": "\"\"\" Add/Edit contact \"\"\"\n", "func_signal": "def add_edit_for_model(request, model, model_slug, contact_slug=None, template='blcontact/includes/form.html'):\n", "code": "model = get_model(*model.split('.'))\nmodel_object = get_object_or_404(model, slug=model_slug, members=request.user)\nmodel_type = ContentType.objects.get_for_model(model_object)  \n\nif isinstance(model_object, Group):\n    redirect_reverse = 'group-contact-detail'\n    extra_context = {'group': model_object}\n\nelif isinstance(model_object, Project):\n    redirect_reverse = 'project-contact-detail'\n    extra_context = {'project': model_object}\n\nform = ContactForm(request.POST or None,\n                   instance=contact_slug and Contact.objects.get(slug__iexact=contact_slug,\n                                                                 content_type__pk=model_type.id))\n\nif request.method == \"POST\" and form.is_valid():\n    contact = form.save(model_object)\n        \n    return HttpResponseRedirect(reverse(redirect_reverse,\n                                        kwargs = {'model_slug': model_slug,\n                                                  'contact_slug': contact.slug}))\nextra_context['form'] = form\nreturn direct_to_template(request,\n                          template=template,\n                          extra_context=extra_context)", "path": "blcontact\\views.py", "repo_name": "bread-and-pepper/busylissy", "stars": 31, "license": "None", "language": "python", "size": 1230}
{"docstring": "\"\"\" Returns details about the contact \"\"\"\n", "func_signal": "def detail(request, id):\n", "code": "contact = get_object_or_404(Contact, pk=id)\nmodel = contact.content_type.model_class()\nobject = get_object_or_404(model, members=request.user, id=contact.object_id)\n\nreturn direct_to_template(request,\n                          template='blcontact/detail.html',\n                          extra_context={'contact': contact,\n                                         'object': object,\n                                         'model': model._meta.verbose_name})", "path": "blcontact\\views.py", "repo_name": "bread-and-pepper/busylissy", "stars": 31, "license": "None", "language": "python", "size": 1230}
{"docstring": "\"\"\" List all the contacts for this user \"\"\"\n", "func_signal": "def list(request, model=None, slug=None):\n", "code": "if not model:\n    models = [Project, Group]\nelif model in ['projects', 'groups']:\n    models = [Project, ] if model == 'projects' else [Group, ]\nelse: raise Http404\n\nall_contacts = []\nall_ids = []\nfor model in models:\n    ctype = ContentType.objects.get_for_model(model)\n    if not slug:\n        object_ids = [id['id'] for id in model.objects.filter(members=request.user).values('id')]\n    else:\n        object_ids = [id['id'] for id in model.objects.filter(members=request.user, slug__iexact=slug).values('id')]\n    \n    contacts = Contact.objects.filter(content_type__pk=ctype.id).filter(object_id__in=object_ids)\n    # Sort the contacts\n    all_contacts.extend(contacts)\n    all_ids.extend(object_ids)\n\nall_contacts = sorted(all_contacts, key=lambda x: x.last_name.lower())\n\nqueryset = Contact.objects.filter(Q(object_id__in=all_ids),\n                                  Q(content_type__pk=14)|Q(content_type__pk=15))\n\ntags = Tag.objects.usage_for_queryset(queryset, counts=True)\n\nreturn direct_to_template(request,\n                          template='blcontact/contact_list.html',\n                          extra_context={'contact_list': all_contacts,\n                                         'contact_tags': tags})", "path": "blcontact\\views.py", "repo_name": "bread-and-pepper/busylissy", "stars": 31, "license": "None", "language": "python", "size": 1230}
{"docstring": "\"\"\" Add/Edit contact \"\"\"\n", "func_signal": "def add_edit(request, id=None):\n", "code": "form = ContactForm(request.POST or None,\n                   instance=id and Contact.objects.get(pk=id))\n\nif request.method == \"POST\" and form.is_valid():\n    contact = form.save()\n\n    return HttpResponseRedirect(reverse('contact-detail',\n                                        kwargs = {'id': contact.id, }))\nreturn direct_to_template(request,\n                          template='blcontact/includes/form.html',\n                          extra_context={'form': form,})", "path": "blcontact\\views.py", "repo_name": "bread-and-pepper/busylissy", "stars": 31, "license": "None", "language": "python", "size": 1230}
{"docstring": "\"\"\" Add/Edit group \"\"\"\n", "func_signal": "def add_edit(request, slug=None):\n", "code": "form = GroupForm(request.POST or None,\n                 instance=slug and Group.objects.get(slug__iexact=slug))\n\nif slug:\n    check = GroupPermission(request.user)\n    group = Group.objects.get(slug__iexact=slug)\n    if not check.change_group(group):\n        raise Http404\n    \nif request.method == \"POST\" and form.is_valid():\n    group = form.save(request.user, slug)\n    form.save_m2m()\n\n    # Notification\n    if slug: msg = _(\"Group '%(group)s' has been updated\" % {'group': group.name })\n    else: msg = _(\"Group '%(group)s' has been created\" % {'group': group.name })\n    request.user.message_set.create(message=msg)\n\n    return HttpResponseRedirect(reverse('group-detail', kwargs = {'slug': group.slug }))\n\nreturn direct_to_template(request,\n                          template='blgroup/form.html',\n                          extra_context={'form': form, })", "path": "blgroup\\views.py", "repo_name": "bread-and-pepper/busylissy", "stars": 31, "license": "None", "language": "python", "size": 1230}
{"docstring": "\"\"\" Returns the details for a contact \"\"\"\n", "func_signal": "def detail_for_model(request, model, model_slug, contact_slug, template='blcontact/detail.html'):\n", "code": "model = get_model(*model.split('.'))\nmodel_object = get_object_or_404(model, slug=model_slug)\nmodel_type = ContentType.objects.get_for_model(model_object)\n\nq = Contact.objects.filter(content_type__pk=model_type.id,\n                           object_id=model_object.id)\n\nif isinstance(model_object, Group):\n    extra = {'group': model_object}\nelif isinstance(model_object, Project):\n    extra = {'project': model_object}\n\nreturn list_detail.object_detail(request,\n                                 queryset=q,\n                                 slug=contact_slug,\n                                 template_name=template,\n                                 extra_context=extra)", "path": "blcontact\\views.py", "repo_name": "bread-and-pepper/busylissy", "stars": 31, "license": "None", "language": "python", "size": 1230}
{"docstring": "# In case of \"/rss/beats/0613/foo/bar/baz/\", or other such clutter,\n# check that bits has only one member.\n", "func_signal": "def get_object(self, bits):\n", "code": "if len(bits) != 2:\n    raise ObjectDoesNotExist\nelse:\n    project = get_object_or_404(Project, slug=bits[0])\n    \n    salt = 'bltoken'\n    hash_obj = md5_constructor(str(project.slug) + salt + str(project.pk))\n    \nreturn project", "path": "blactivity\\feeds.py", "repo_name": "bread-and-pepper/busylissy", "stars": 31, "license": "None", "language": "python", "size": 1230}
{"docstring": "\"\"\" Returns the contacts for a model \"\"\"\n", "func_signal": "def list_for_model(request, model, slug, template='blcontact/contact_list.html'):\n", "code": "model = get_model(*model.split('.'))\nq = model.objects.filter(members=request.user)\nobject = get_object_or_404(model, slug=slug)\n\nif isinstance(object, Group):\n    extra = {'group': object}\nelif isinstance(object, Project):\n    extra = {'project': object}\n\nreturn list_detail.object_detail(request,\n                                 queryset=q,\n                                 slug=slug,\n                                 template_name=template,\n                                 extra_context=extra)", "path": "blcontact\\views.py", "repo_name": "bread-and-pepper/busylissy", "stars": 31, "license": "None", "language": "python", "size": 1230}
{"docstring": "\"\"\" Delete a node \n\nTODO:\n    - Confirmation of delete\n    - Notification of delete\n\n\"\"\"\n", "func_signal": "def delete(request, project_slug, id):\n", "code": "node = get_object_or_404(Task, id=id, project__members=request.user)\nnode.create_activity(request.user, Activity.DELETE)\nnode.delete()\nrequest.user.message_set.create(message=_('Task \\'%(name)s\\' has been deleted' % {'name': node.name}))\nreturn HttpResponseRedirect(reverse('task-list', kwargs={'project_slug': project_slug}))", "path": "bltask\\views.py", "repo_name": "bread-and-pepper/busylissy", "stars": 31, "license": "None", "language": "python", "size": 1230}
{"docstring": "\"\"\" Delete the contact \"\"\"\n", "func_signal": "def delete(request, id):\n", "code": "contact = get_object_or_404(Contact, pk=id)\nmodel = contact.content_type.model_class()\nobject = get_object_or_404(model, members=request.user, id=contact.object_id)\n\nif object:\n    contact.delete()\n    \nreturn HttpResponseRedirect(reverse('contact-list'))", "path": "blcontact\\views.py", "repo_name": "bread-and-pepper/busylissy", "stars": 31, "license": "None", "language": "python", "size": 1230}
{"docstring": "\"\"\" Week calendar for dashboard \"\"\"\n\n", "func_signal": "def week_cal(project):\n", "code": "year, month = datetime.datetime.now().year, datetime.datetime.now().month\n\noccurrences = Occurrence.objects.select_related().filter(start_time__year=year, start_time__month=month, event__project__slug=project.slug)\nproject = get_object_or_404(Project, pk=project.pk)\n\nby_day = dict([\n    (dom, list(items)) \n    for dom,items in itertools.groupby(occurrences, lambda o: o.start_time.day)\n])\n\ntoday = datetime.datetime.now().date()\nend_of_week = today + datetime.timedelta(days=7)\n\ndays = []\nday = today\nwhile day <= end_of_week:\n    days.append(day)\n    day += datetime.timedelta(days=1)\n\ndata = dict(\n    today=datetime.datetime.now(),\n    project=project,\n    calendar=[(d, by_day.get(d.day, [])) for d in days],\n    MEDIA_URL=settings.MEDIA_URL,\n    this_month=datetime.datetime.now(),\n)\n\nreturn data", "path": "bltask\\templatetags\\milestones.py", "repo_name": "bread-and-pepper/busylissy", "stars": 31, "license": "None", "language": "python", "size": 1230}
{"docstring": "\"\"\" Add or edit a task to a project \"\"\"\n", "func_signal": "def add_edit(request, project_slug, instance=None, parent=None):\n", "code": "project = get_object_or_404(Project, slug=project_slug, members=request.user)\ntags = Tag.objects.usage_for_queryset(Task.objects.select_related().filter(project=project),\n                                      counts=True)\nif instance: instance = get_object_or_404(Task, id=instance)\nif parent: parent = get_object_or_404(Task, id=parent)\ninitial = {'assigned_to': request.user.id }\n\nform = TaskModelForm(project,\n                     request.user,\n                     parent,\n                     instance=instance,\n                     initial=initial if not instance else None)\n\nif request.method == \"POST\":\n    form = TaskModelForm(project,\n                         request.user,\n                         parent,\n                         request.POST,\n                         instance=instance)\n    if form.is_valid():\n        task = form.save()\n        \n        # Message notification\n        if instance:\n            msg = _('Task \\'%(name)s\\' has been updated' % {'name': task.name})\n            task.create_activity(request.user, Activity.UPDATE) \n        else:\n            msg = _('Task \\'%(name)s\\' has been created' % {'name': task.name})\n            task.create_activity(request.user, Activity.CREATE)\n            # If the task has siblings, move to the top\n            sibling = task.get_siblings()[0]\n            if sibling:\n                task.move(sibling, 'first-sibling')\n\n        request.user.message_set.create(message=msg)\n\n        return HttpResponseRedirect(reverse('task-list', \n                                            kwargs={'project_slug': project_slug}) + \\\n                                    \"#task-%s\" % task.id)\n\nreturn direct_to_template(request,\n                          template='bltask/task_form.html',\n                          extra_context={\n                              'form': form,\n                              'project': project,\n                              'tags': tags,})", "path": "bltask\\views.py", "repo_name": "bread-and-pepper/busylissy", "stars": 31, "license": "None", "language": "python", "size": 1230}
{"docstring": "\"\"\" Delete group \"\"\"\n", "func_signal": "def delete(request, slug):\n", "code": "group = get_object_or_404(Group, slug__iexact=slug)\ncheck = GroupPermission(request.user)\n\nif check.delete_group(group):\n    group.delete()\n\n    # Notification\n    request.user.message_set.create(message=_(\"Group '%(group)s' has been deleted\" % {'group': group.name }))\n    \n    return HttpResponseRedirect(reverse('group-list'))\nelse:\n    raise Http404", "path": "blgroup\\views.py", "repo_name": "bread-and-pepper/busylissy", "stars": 31, "license": "None", "language": "python", "size": 1230}
{"docstring": "\"\"\" Toggles a task as complete or incomplete \"\"\"\n", "func_signal": "def toggle(request, project_slug, id):\n", "code": "task = get_object_or_404(Task, id=id, project__members=request.user)\nif task.toggle():\n    if task.completed:\n        msg = _('Task \\'%(name)s\\' has been completed' % {'name': task.name})\n        task.create_activity(request.user, Activity.CLOSE)\n        # Move the node to the correct position\n        try:\n            last_completed = task.get_siblings().filter(completed=True).exclude(pk=task.pk)[0]\n        except IndexError:\n            task.move(task, 'last-sibling')\n        else:\n            task.move(last_completed, 'left')\n    else:\n        msg = _('Task \\'%(name)s\\' has been marked as incomplete' % {'name': task.name})\n        # Move it to the correct position\n        task.move(task, 'first-sibling')\n\n    request.user.message_set.create(message=msg)\nreturn HttpResponseRedirect(reverse('task-list', kwargs={'project_slug': project_slug}) + \\\n                           '#task-%s' % task.id)", "path": "bltask\\views.py", "repo_name": "bread-and-pepper/busylissy", "stars": 31, "license": "None", "language": "python", "size": 1230}
{"docstring": "\"\"\" Contacts by tag \"\"\"\n", "func_signal": "def tagged_contacts(request, tags=None):\n", "code": "models = [Project, Group]\n\nall_ids = []\nfor model in models:\n    ctype = ContentType.objects.get_for_model(model)\n    object_ids = [id['id'] for id in model.objects.filter(members=request.user).values('id')]\n\n    all_ids.extend(object_ids)\n\ntag_list = tags.split(\"+\")\ncontact_queryset = Contact.objects.filter(Q(object_id__in=all_ids),\n                                          Q(content_type__pk=14)|Q(content_type__pk=15))\n\nqueryset = TaggedItem.objects.get_intersection_by_model(contact_queryset, tag_list)\ntags = Tag.objects.usage_for_queryset(queryset, counts=True)\n\nselected_tags = []\nfor item in tag_list:\n    name = Tag.objects.get(name=item)\n    tags.remove(name)\n    selected_tags.append(name)\n    tag_url = item + '+'\n\nreturn list_detail.object_list(request,\n                               queryset=queryset,\n                               template_name='blcontact/contact_list.html',\n                               template_object_name='contact',\n                               extra_context={'contact_tags': tags,\n                                              'tag_url': tag_url,\n                                              'selected_tags': selected_tags})", "path": "blcontact\\views.py", "repo_name": "bread-and-pepper/busylissy", "stars": 31, "license": "None", "language": "python", "size": 1230}
{"docstring": "\"\"\" List tagged objects \"\"\"\n", "func_signal": "def tagged_groups(request, tags=None):\n", "code": "tag_list = tags.split(\"+\")\n\nqueryset = TaggedItem.objects.get_intersection_by_model(Group.objects.filter(members=request.user), tag_list)\ntags = Tag.objects.usage_for_queryset(queryset, counts=True)\n\nselected_tags = []\nfor item in tag_list:\n    name = Tag.objects.get(name=item)\n    tags.remove(name)\n    selected_tags.append(name)\n    tag_url = item + '+'\n\nreturn list_detail.object_list(request,\n                               queryset=queryset,\n                               template_name='blgroup/group_list.html',\n                               template_object_name='group',\n                               extra_context={'group_tags': tags,\n                                              'tag_url': tag_url,\n                                              'selected_tags': selected_tags})", "path": "blgroup\\views.py", "repo_name": "bread-and-pepper/busylissy", "stars": 31, "license": "None", "language": "python", "size": 1230}
{"docstring": "\"\"\"Ensure an add command returns the current CAS.\"\"\"\n", "func_signal": "def testAddReturnsCas(self):\n", "code": "vals=self.mc.add('x', 5, 19, 'some val')\nself.assertValidCas('x', vals[1])", "path": "testClient.py", "repo_name": "dustin/memcached-test", "stars": 21, "license": "mit", "language": "python", "size": 226}
{"docstring": "\"\"\"Ensure an incr command returns the current CAS.\"\"\"\n", "func_signal": "def testIncrReturnsCAS(self):\n", "code": "val, cas, something=self.mc.set(\"x\", 5, 19, '4')\nval, cas=self.mc.incr(\"x\", init=5)\nself.assertEquals(5, val)\nself.assertValidCas('x', cas)", "path": "testClient.py", "repo_name": "dustin/memcached-test", "stars": 21, "license": "mit", "language": "python", "size": 226}
{"docstring": "\"\"\"invoked for any unknown command.\"\"\"\n", "func_signal": "def handle_unknown(self, cmd, hdrs, key, cas, data):\n", "code": "return self._error(memcacheConstants.ERR_UNKNOWN_CMD,\n    \"The command %d is unknown\" % cmd)", "path": "testServer.py", "repo_name": "dustin/memcached-test", "stars": 21, "license": "mit", "language": "python", "size": 226}
{"docstring": "\"\"\"Flush all storage in a memcached instance.\"\"\"\n", "func_signal": "def flush(self, timebomb=0):\n", "code": "return self._doCmd(memcacheConstants.CMD_FLUSH, '', '',\n    struct.pack(memcacheConstants.FLUSH_PKT_FMT, timebomb))", "path": "mc_bin_client.py", "repo_name": "dustin/memcached-test", "stars": 21, "license": "mit", "language": "python", "size": 226}
{"docstring": "\"\"\"Get the supported SASL methods.\"\"\"\n", "func_signal": "def sasl_mechanisms(self):\n", "code": "return set(self._doCmd(memcacheConstants.CMD_SASL_LIST_MECHS,\n                       '', '')[2].split(' '))", "path": "mc_bin_client.py", "repo_name": "dustin/memcached-test", "stars": 21, "license": "mit", "language": "python", "size": 226}
{"docstring": "\"\"\"Get the value for a given key within the memcached server.\"\"\"\n", "func_signal": "def get(self, key):\n", "code": "parts=self._doCmd(memcacheConstants.CMD_GET, key, '')\nreturn self.__parseGet(parts)", "path": "mc_bin_client.py", "repo_name": "dustin/memcached-test", "stars": 21, "license": "mit", "language": "python", "size": 226}
{"docstring": "\"\"\"Test the version command returns something.\"\"\"\n", "func_signal": "def testVersion(self):\n", "code": "v=self.mc.version()\nself.assertTrue(len(v) > 0, \"Bad version:  ``\" + str(v) + \"''\")", "path": "testClient.py", "repo_name": "dustin/memcached-test", "stars": 21, "license": "mit", "language": "python", "size": 226}
{"docstring": "\"\"\"Test prepend functionality.\"\"\"\n", "func_signal": "def testPrepend(self):\n", "code": "val, cas, something=self.mc.set(\"x\", 5, 19, \"some\")\nval, cas, something=self.mc.prepend(\"x\", \"thing\")\nself.assertGet((19, 'thingsome'), self.mc.get(\"x\"))", "path": "testClient.py", "repo_name": "dustin/memcached-test", "stars": 21, "license": "mit", "language": "python", "size": 226}
{"docstring": "\"\"\"Get values for any available keys in the given iterable.\n\nReturns a dict of matched keys to their values.\"\"\"\n", "func_signal": "def getMulti(self, keys):\n", "code": "opaqued=dict(enumerate(keys))\nterminal=len(opaqued)+10\n# Send all of the keys in quiet\nfor k,v in opaqued.iteritems():\n    self._sendCmd(memcacheConstants.CMD_GETQ, v, '', k)\n\nself._sendCmd(memcacheConstants.CMD_NOOP, '', '', terminal)\n\n# Handle the response\nrv={}\ndone=False\nwhile not done:\n    opaque, cas, data=self._handleSingleResponse(None)\n    if opaque != terminal:\n        rv[opaqued[opaque]]=self.__parseGet((opaque, cas, data))\n    else:\n        done=True\n\nreturn rv", "path": "mc_bin_client.py", "repo_name": "dustin/memcached-test", "stars": 21, "license": "mit", "language": "python", "size": 226}
{"docstring": "\"\"\"Test a set, get, delete, get sequence.\"\"\"\n", "func_signal": "def testDelete(self):\n", "code": "self.mc.set(\"x\", 5, 19, \"somevalue\")\nself.assertGet((19, \"somevalue\"), self.mc.get(\"x\"))\nself.mc.delete(\"x\")\nself.assertNotExists(\"x\")", "path": "testClient.py", "repo_name": "dustin/memcached-test", "stars": 21, "license": "mit", "language": "python", "size": 226}
{"docstring": "\"\"\"Test a flush with a time bomb.\"\"\"\n", "func_signal": "def testTimeBombedFlush(self):\n", "code": "val, cas, something=self.mc.set(\"x\", 5, 19, \"some\")\nself.mc.flush(2)\nself.assertGet((19, 'some'), self.mc.get(\"x\"))\ntime.sleep(2.1)\nself.assertNotExists('x')", "path": "testClient.py", "repo_name": "dustin/memcached-test", "stars": 21, "license": "mit", "language": "python", "size": 226}
{"docstring": "\"\"\"Testing multiget functionality\"\"\"\n", "func_signal": "def testMultiGet(self):\n", "code": "self.mc.add(\"x\", 5, 1, \"ex\")\nself.mc.add(\"y\", 5, 2, \"why\")\nvals=self.mc.getMulti('xyz')\nself.assertGet((1, 'ex'), vals['x'])\nself.assertGet((2, 'why'), vals['y'])\nself.assertEquals(2, len(vals))", "path": "testClient.py", "repo_name": "dustin/memcached-test", "stars": 21, "license": "mit", "language": "python", "size": 226}
{"docstring": "\"\"\"Simple decr test.\"\"\"\n", "func_signal": "def testDecr(self):\n", "code": "val, cas=self.mc.incr(\"x\", init=5)\nself.assertEquals(5, val)\nval, cas=self.mc.decr(\"x\")\nself.assertEquals(4, val)\nval, cas=self.mc.decr(\"x\", 211)\nself.assertEquals(0, val)", "path": "testClient.py", "repo_name": "dustin/memcached-test", "stars": 21, "license": "mit", "language": "python", "size": 226}
{"docstring": "\"\"\"Ensure a set command returns the current CAS.\"\"\"\n", "func_signal": "def testSetReturnsCas(self):\n", "code": "vals=self.mc.set('x', 5, 19, 'some val')\nself.assertValidCas('x', vals[1])", "path": "testClient.py", "repo_name": "dustin/memcached-test", "stars": 21, "license": "mit", "language": "python", "size": 226}
{"docstring": "\"\"\"Replace a value in the memcached server iff it already exists.\"\"\"\n", "func_signal": "def replace(self, key, exp, flags, val):\n", "code": "return self._mutate(memcacheConstants.CMD_REPLACE, key, exp, flags, 0,\n    val)", "path": "mc_bin_client.py", "repo_name": "dustin/memcached-test", "stars": 21, "license": "mit", "language": "python", "size": 226}
{"docstring": "\"\"\"Testing decr when a value doesn't exist (and we make a new one)\"\"\"\n", "func_signal": "def testDecrDoesntExistCreate(self):\n", "code": "self.assertNotExists(\"x\")\nself.assertEquals(19, self.mc.decr(\"x\", init=19)[0])", "path": "testClient.py", "repo_name": "dustin/memcached-test", "stars": 21, "license": "mit", "language": "python", "size": 226}
{"docstring": "\"\"\"Ensure an decr command returns the current CAS.\"\"\"\n", "func_signal": "def testDecrReturnsCAS(self):\n", "code": "val, cas, something=self.mc.set(\"x\", 5, 19, '4')\nval, cas=self.mc.decr(\"x\", init=5)\nself.assertEquals(3, val)\nself.assertValidCas('x', cas)", "path": "testClient.py", "repo_name": "dustin/memcached-test", "stars": 21, "license": "mit", "language": "python", "size": 226}
{"docstring": "\"\"\"CAS in a new value for the given key and comparison value.\"\"\"\n", "func_signal": "def cas(self, key, exp, flags, oldVal, val):\n", "code": "self._mutate(memcacheConstants.CMD_SET, key, exp, flags,\n    oldVal, val)", "path": "mc_bin_client.py", "repo_name": "dustin/memcached-test", "stars": 21, "license": "mit", "language": "python", "size": 226}
{"docstring": "\"\"\"Ensure zero-expiration sets work properly.\"\"\"\n", "func_signal": "def testZeroExpiration(self):\n", "code": "self.mc.set(\"x\", 0, 19, \"somevalue\")\ntime.sleep(1.1)\nself.assertGet((19, \"somevalue\"), self.mc.get(\"x\"))", "path": "testClient.py", "repo_name": "dustin/memcached-test", "stars": 21, "license": "mit", "language": "python", "size": 226}
{"docstring": "\"\"\"Get stats.\"\"\"\n", "func_signal": "def stats(self, sub=''):\n", "code": "opaque=self.r.randint(0, 2**32)\nself._sendCmd(memcacheConstants.CMD_STAT, sub, '', opaque)\ndone = False\nrv = {}\nwhile not done:\n    cmd, opaque, cas, klen, extralen, data = self._handleKeyedResponse(None)\n    if klen:\n        rv[data[0:klen]] = data[klen:]\n    else:\n        done = True\nreturn rv", "path": "mc_bin_client.py", "repo_name": "dustin/memcached-test", "stars": 21, "license": "mit", "language": "python", "size": 226}
{"docstring": "\"\"\"\nCreates an object with the about tag given.\nIf the object already exists, returns the object instead.\n\nReturns: the object returned if successful, wrapped up in\nan (O) object whose class variables correspond to the\nvalues returned by FluidDB, in particular, o.id and o.URL.\nIf there's a failure, the return value is an integer error code.\n\"\"\"\n", "func_signal": "def create_object(self, about=None):\n", "code": "if about:\n    body = json.dumps({u'about': about})\nelse:\n    body = None\n(status, o) = self.call(u'POST', u'/objects', body)\nreturn O(o) if status == STATUS.CREATED else status", "path": "fdblib.py", "repo_name": "njr0/fdb", "stars": 25, "license": "mit", "language": "python", "size": 1461}
{"docstring": "\"\"\"Returns the absolute tag path (see above), prefixed with /tag.\n\n   Examples: (assuming the user credentials username is njr):\n        full_tag_path ('rating') = '/tags/njr/rating'\n        full_tag_path ('/njr/rating') = '/tags/njr/rating'\n        full_tag_path ('/tags/njr/rating') = '/tags/njr/rating'\n        full_tag_path('foo/rating') = '/tags/njr/foo/rating'\n        full_tag_path('/njr/foo/rating') = '/tags/njr/foo/rating'\n        full_tag_path('/tags/njr/foo/rating') = '/tags/njr/foo/rating'\n\"\"\"\n", "func_signal": "def full_tag_path(self, tag):\n", "code": "if tag.startswith(u'/tags/'):\n    return tag\nelse:\n    return u'/tags%s' % self.abs_tag_path(tag)", "path": "fdblib.py", "repo_name": "njr0/fdb", "stars": 25, "license": "mit", "language": "python", "size": 1461}
{"docstring": "# this might turn into a cache that gets dumped to file and\n# supports more than two fixed hosts in time.\n", "func_signal": "def id(about, host):\n", "code": "cache = IDS_MAIN if host == FLUIDDB_PATH else IDS_SAND\nreturn cache[about]", "path": "fdblib.py", "repo_name": "njr0/fdb", "stars": 25, "license": "mit", "language": "python", "size": 1461}
{"docstring": "\"\"\"\nCalls FluidDB with the attributes given.\nThis function was lifted nearly verbatim from fluiddb.py,\nby Sanghyeon Seo, with additions by Nicholas Tollervey.\n\nReturns: a 2-tuple consisting of the status and result\n\"\"\"\n", "func_signal": "def call(self, method, path, body=None, hash=None, **kw):\n", "code": "headers = self.headers.copy()\nif body:\n    headers[u'content-type'] = u'application/json'\n\nk2 = {}\nfor k in kw:\n    k2[k] = (kw[k].decode('UTF-8')\n             if type(kw[k]) == types.StringType else kw[k])\nkw = k2\nurl = self._get_url(self.host, path, hash, kw)\n\nif self.debug:\n    Print(u'\\nmethod: %r\\nurl: %r\\nbody: %s\\nheaders:' %\n           (method, url, body))\n    for k in headers:\n        if not k == u'Authorization':\n            Print(u'  %s=%s' % (k, headers[k]))\nbody8 = body.encode('UTF-8') if type(body) == unicode else body\n\nhttp = _get_http(self.timeout)\nresponse, content = http.request(url, method, body8, headers)\nstatus = response.status\nif response[u'content-type'].startswith(u'application/json'):\n    result = json.loads(content)\nelse:\n    result = content\nif self.debug:\n    Print(u'status: %d; content: %s' % (status, toStr(result)))\n    if status >= 400:\n        for header in response:\n            if header.lower().startswith(u'x-fluiddb-'):\n                Print(u'\\t%s=%s' % (header.decode('UTF-8'),\n                                    response[header].decode('UTF-8')))\n\nreturn status, result", "path": "fdblib.py", "repo_name": "njr0/fdb", "stars": 25, "license": "mit", "language": "python", "size": 1461}
{"docstring": "\"\"\"\nSets one or more tags on objects that match a query.\n\ndb         is an instantiated FluidDB instance.\n\nquery      is a unicode string representing a valid Fluidinfo query.\n           e.g. 'has njr/rating'\n\ntagsToSet  is a dictionary containing tag names (as keys)\n           and values to be set.   (Use None to set a tag with no value.)\n\nExample:\n\n    db = FluidDB()\n    tag_by_query(db, u'has njr/rating', {'njr/rated': True})\n\nsets an njr/rated tag to True for every object having an njr/rating.\n\nNOTE: Unlike in much of the rest of fdb.py, tags need to be full paths\nwithout a leading slash.   (This will change.)\n\nNOTE: Tags must exist before being used.   (This will change.)\n\nNOTE: All strings must be (and will be) unicode.\n\n\n\"\"\"\n", "func_signal": "def tag_by_query(db, query, tagsToSet):\n", "code": "strHash = u'{%s}' % u', '.join(u'\"%s\": {\"value\": %s}'\n                               % (tag, format_val(tagsToSet[tag]))\n                               for tag in tagsToSet)\n(v, r) = db.call(u'PUT', u'/values', strHash, {u'query': query})\nassert_status(v, STATUS.NO_CONTENT)", "path": "fdblib.py", "repo_name": "njr0/fdb", "stars": 25, "license": "mit", "language": "python", "size": 1461}
{"docstring": "\"\"\"Gets the value of a tag on an object identified by the\n   object's ID or about value..\n\n   spec is the id or about tag for the object, and with\n   with byAbout being true if it is an about tag.\n\n   Returns  returns a 2-tuple, in which the first component\n   is the status, and the second is either the tag value,\n   if the return stats is STATUS.OK, or None otherwise.\n\"\"\"\n", "func_signal": "def get_tag_value(self, spec, tag, byAbout, inPref=False):\n", "code": "objTagParts = self.path_parts(byAbout, spec, tag, inPref)\nstatus, (value, value_type) = self._get_tag_value(objTagParts)\nreturn status, (value if status == STATUS.OK else None)", "path": "fdblib.py", "repo_name": "njr0/fdb", "stars": 25, "license": "mit", "language": "python", "size": 1461}
{"docstring": "\"\"\"\nSets the host on the basis of the global variable flags,\nif that exists.   Used to enable the tests to run against\nalternate hosts.\n\"\"\"\n", "func_signal": "def set_connection_from_global(self):\n", "code": "self.host = choose_host()\nself.debug = choose_debug_mode()\nself.timeout = choose_http_timeout()", "path": "fdblib.py", "repo_name": "njr0/fdb", "stars": 25, "license": "mit", "language": "python", "size": 1461}
{"docstring": "\"\"\"Runs the query to get the IDs of objects satisfying the query.\n   If the query is successful, the list of ids is returned, as a list;\n   otherwise, an error code is returned.\n\"\"\"\n", "func_signal": "def query(self, query):\n", "code": "(status, o) = self.call(u'GET', u'/objects', query=query)\nreturn status if status != STATUS.OK else o[u'ids']", "path": "fdblib.py", "repo_name": "njr0/fdb", "stars": 25, "license": "mit", "language": "python", "size": 1461}
{"docstring": "\"\"\"Quote a unicode string s using %-encoding.\n\n   If s is a list, each part is quoted then return, joined by slashes.\n\n   Returns UTF-8.\n\"\"\"\n", "func_signal": "def quote_u_8(s):\n", "code": "if type(s) in (list, tuple):\n    u8parts = (part.encode('UTF-8') for part in s)\n    return '/'.join(urllib.quote(p, safe='') for p in u8parts)\n                    \nelse:\n    return urllib.quote(s.encode('UTF-8'))", "path": "fdblib.py", "repo_name": "njr0/fdb", "stars": 25, "license": "mit", "language": "python", "size": 1461}
{"docstring": "\"\"\"Applies urllib.urlencode to a hash that may contain unicode values.\"\"\"\n", "func_signal": "def urlencode_hash_u_8(hash):\n", "code": "h8 = {}\nfor key in hash:\n    v = hash[key]\n    h8[key] = v.encode('UTF-8') if type(v) == unicode else v\nreturn urllib.urlencode(h8, True)", "path": "fdblib.py", "repo_name": "njr0/fdb", "stars": 25, "license": "mit", "language": "python", "size": 1461}
{"docstring": "\"\"\"\nFormats a value for json (unicode).\n\"\"\"\n", "func_signal": "def format_val(s):\n", "code": "if type(s) == type('s'):\n    raise NonUnicodeStringError\nelif type(s) == unicode:\n    if s.startswith(u'\"') and s.endsswith(u'\"'):\n        return s\n    else:\n        return u'\"%s\"' % s\nelif type(s) == bool:\n    return unicode(s).lower()\nelif s is None:\n    return u'null'\nelse:\n    return unicode(s)", "path": "fdblib.py", "repo_name": "njr0/fdb", "stars": 25, "license": "mit", "language": "python", "size": 1461}
{"docstring": "\"\"\"Deletes an abstract tag, removing all of its concrete\n   instances from objects.   Use with care.\n   So db.delete_abstract_tag('njr/rating') removes\n   the njr/rating from ALL OBJECTS IN FLUIDDB.\n\n   Returns 0 if successful; otherwise returns an integer error code.\n\"\"\"\n", "func_signal": "def delete_abstract_tag(self, tag):\n", "code": "fullTag = self.full_tag_path(tag)\n(status, o) = self.call('DELETE', fullTag)\nreturn 0 if status == STATUS.NO_CONTENT else status", "path": "fdblib.py", "repo_name": "njr0/fdb", "stars": 25, "license": "mit", "language": "python", "size": 1461}
{"docstring": "\"\"\"Uses some simple rules to extract simple typed values from strings.\n    Specifically:\n       true and t (any case) return True (boolean)\n       false and f (any case) return False (boolean)\n       simple integers (possibly signed) are returned as ints\n       simple floats (possibly signed) are returned as floats\n            (supports '.' and ',' as floating-point separator,\n             subject to locale)\n       Everything else is returned as a string, with matched\n            enclosing quotes stripped.\n\"\"\"\n", "func_signal": "def get_typed_tag_value(v):\n", "code": "if v.lower() in (u'true', u't'):\n    return True\nelif v.lower() in (u'false', u'f'):\n    return False\nelif re.match(INTEGER_RE, v):\n    return int(v)\nelif re.match(DECIMAL_RE, v) or re.match(DECIMAL_RE2, v):\n    try:\n        r = float(v)\n    except ValueError:\n        return toStr(v)\n    return r\nelif len(v) > 1 and v[0] == v[-1] and v[0] in (u'\"\\''):\n    return v[1:-1]\nelse:\n    return toStr(v)", "path": "fdblib.py", "repo_name": "njr0/fdb", "stars": 25, "license": "mit", "language": "python", "size": 1461}
{"docstring": "\"\"\"\nTurns json-formatted string into python value.\nUnicode.\n\"\"\"\n", "func_signal": "def to_typed(v):\n", "code": "L = v.lower()\nif v.startswith(u'\"') and v.startswith(u'\"') and len(v) >= 2:\n    return v[1:-1]\nelif v.startswith(u\"'\") and v.startswith(u\"'\") and len(v) >= 2:\n    return v[1:-1]\nelif L == u'true':\n    return True\nelif L == u'false':\n    return False\nelif re.match(INTEGER_RE, v):\n    return int(v)\nelif re.match(DECIMAL_RE, v) or re.match(DECIMAL_RE2, v):\n    try:\n        r = float(v)\n    except ValueError:\n        return unicode(v)\n    return r\nelse:\n    return unicode(v)", "path": "fdblib.py", "repo_name": "njr0/fdb", "stars": 25, "license": "mit", "language": "python", "size": 1461}
{"docstring": "\"\"\"returns URL as unicode\n\"\"\"\n", "func_signal": "def _get_url(self, host, path, hash, kw):\n", "code": "url = host.encode('UTF-8') + quote_u_8(path)\nif hash:\n    url = '%s?%s' % (url, urlencode_hash_u_8(hash))\nelif kw:\n    kwds = '&'.join('%s=%s' % (k.encode('UTF-8'),\n                               format_param(kw[k])) for k in kw)\n\n    url = '%s?%s' % (url, kwds)\nreturn url.decode('UTF-8')", "path": "fdblib.py", "repo_name": "njr0/fdb", "stars": 25, "license": "mit", "language": "python", "size": 1461}
{"docstring": "\"\"\"Returns an object describing the namespace specified by the path.\n\n   The path, as usual in FDB, is considered absolute if it starts\n   with a slash, and relative to the user's namespace otherwise.\n\n   The object contains attributes tagNames, namespaceNames and\n   path.\n\n   If the call is unsuccessful, an error code is returned instead.\n\"\"\"\n", "func_signal": "def describe_namespace(self, path):\n", "code": "absPath = self.abs_tag_path(path)\nfullPath = u'/namespaces' + absPath\nif fullPath.endswith(u'/'):\n    fullPath = fullPath[:-1]\nstatus, result = self.call(u'GET', fullPath, returnDescription=True,\n                           returnTags=True, returnNamespaces=True)\nreturn O(result) if status == STATUS.OK else status", "path": "fdblib.py", "repo_name": "njr0/fdb", "stars": 25, "license": "mit", "language": "python", "size": 1461}
{"docstring": "\"\"\"\nReturns the absolute path for the tag nominated,\nusually in the form\n    /namespace/.../shortTagName\nIf the already tag starts with a '/', no action is taken;\nif it doesn't, the username from the current credentials\nis added.\n\nif /tags/ is present at the start of the path,\n/tags is stripped off (which might be a problem if there's\na user called tags...\n\nAlways returns unicode.\n\nExamples: (assuming the user credentials username is njr):\n    abs_tag_path('rating') = u'/njr/rating'\n    abs_tag_path('/njr/rating') = u'/njr/rating'\n    abs_tag_path('/tags/njr/rating') = u'/njr/rating'\n\n    abs_tag_path('foo/rating') = u'/njr/foo/rating'\n    abs_tag_path('/njr/foo/rating') = u'/njr/foo/rating'\n    abs_tag_path('/tags/njr/foo/rating') = u'/njr/foo/rating'\n\nThe behaviour is modified if inPref or outPref is set to True.\n\nSetting inPref to True will change the way the input is handled\nif the self.unixStyle is False.   In this case, the input will\nbe assume to be a FluidDB-style path already, i.e. it will\nbe assumed to be a full path with no leading slash.\n\nSetting outPref to True will change the way the input is handled\nif the self.unixStyle is False.   In this case, the output will\nnot have a leading slash.\n\"\"\"\n", "func_signal": "def abs_tag_path(self, tag, inPref=False, outPref=False):\n", "code": "inUnix = self.unixStyle if inPref else True\noutUnix = self.unixStyle if outPref else True\noutPrefix = u'/' if outUnix else u''\nif inUnix:\n    if tag == u'/about':     # special case\n        return u'%sfluiddb/about' % outPrefix\n    if tag.startswith(u'/'):\n        if tag.startswith(u'/tags/'):\n            return u'%s%s' % (outPrefix, tag[6:])\n        else:\n            return u'%s%s' % (outPrefix, tag[1:])\n    else:\n        return u'%s%s/%s' % (outPrefix,\n                             self.credentials.username,\n                             tag)\nelse:\n    return u'%s%s' % (outPrefix, tag)", "path": "fdblib.py", "repo_name": "njr0/fdb", "stars": 25, "license": "mit", "language": "python", "size": 1461}
{"docstring": "\"\"\"Quote a unicode string s using %-encoding.\n\n   If s is a list, each part is quoted then return, joined by slashes.\n\n   Returns unicode.\n\"\"\"\n", "func_signal": "def quote_u_u(s):\n", "code": "if type(s) in (list, tuple):\n    u8parts = (part.encode('UTF-8') for part in s)\n    return u'/'.join(urllib.quote(p, safe='').decode('UTF-8')\n                     for p in u8parts)\nelse:\n    return urllib.quote(s.encode('UTF-8')).decode('UTF-8')", "path": "fdblib.py", "repo_name": "njr0/fdb", "stars": 25, "license": "mit", "language": "python", "size": 1461}
{"docstring": "\"\"\"A bit like os.path.split, this splits any old kind of a FluidDB\n   tag path into a user, a subnamespace (if there is one) and a tag.\n   But unlike os.path.split, if no namespace is given,\n   the one from the user credentials is returned.\n\n   Any /tags/ prefix is discarded and the namespace is returned\n   with no leading '/'.\n\n   Examples: (assuming the user credentials username is njr):\n        tag_path_split('rating') = (u'njr', u'', u'rating')\n        tag_path_split('/njr/rating') = (u'njr', u'', u'rating')\n        tag_path_split('/tags/njr/rating') = (u'njr', u'', u'rating')\n        tag_path_split('foo/rating') = (u'njr', u'foo', u'rating')\n        tag_path_split('/njr/foo/rating') = (u'njr', u'foo', u'rating')\n        tag_path_split('/tags/njr/foo/rating') = (u'njr', u'foo',\n                                                          u'rating')\n        tag_path_split('foo/bar/rating') = (u'njr', u'foo/bar',\n                                            u'rating')\n        tag_path_split('/njr/foo/bar/rating') = (u'njr', u'foo/bar',\n                                                         u'rating')\n        tag_path_split('/tags/njr/foo/bar/rating') = (u'njr',\n                                                      u'foo/bar',\n                                                      u'rating')\n\n   Returns (user, subnamespace, tagname)\n\"\"\"\n", "func_signal": "def tag_path_split(self, tag):\n", "code": "if tag in (u'', u'/'):\n    raise TagPathError(u'%s is not a valid tag path' % tag)\ntag = self.abs_tag_path(tag)\nparts = tag.split(u'/')\nsubnamespace = u''\ntagname = parts[-1]\nif len(parts) < 3:\n    raise TagPathError(u'%s is not a valid tag path' % tag)\nuser = parts[1]\nif len(parts) > 3:\n    subnamespace = u'/'.join(parts[2:-1])\nreturn (user, subnamespace, tagname)", "path": "fdblib.py", "repo_name": "njr0/fdb", "stars": 25, "license": "mit", "language": "python", "size": 1461}
{"docstring": "\"\"\"\nGets the values of a set of tags satisfying a given query.\nReturns them as a dictionary (hash) keyed on object ID.\nThe values in the dictionary are simple objects with each tag\nvalue in the object's dictionary (__dict__).\n\ndb         is an instantiated FluidDB instance.\n\nquery      is a unicode string representing a valid Fluidinfo query.\n           e.g. 'has njr/rating'\n\ntags       is a list (or tuple) containing the tags whose values are\n           required.\n\nExample:\n\n    db = FluidDB()\n    tag_by_query(db, u'has njr/rating < 3', ('fluiddb/about',))\n\nNOTE: Unlike in much of the rest of fdb.py, tags need to be full paths\nwithout a leading slash.   (This will change.)\n\nNOTE: All strings must be (and will be) unicode.\n\n\"\"\"\n", "func_signal": "def get_values(db, query, tags):\n", "code": "(v, r) = db.call(u'GET', u'/values', None, {u'query': query,\n                                            u'tag': tags})\nassert_status(v, STATUS.OK)\nH = r[u'results'][u'id']\nresults = []\nfor id in H:\n    o = O()\n    o.__dict__[u'id'] = id\n    for tag in tags:\n        o.__dict__[tag] = H[id][tag][u'value']\n    results.append(o)\nreturn results      # hash of objects, keyed on ID, with attributes\n                    # corresponding to tags, inc id.", "path": "fdblib.py", "repo_name": "njr0/fdb", "stars": 25, "license": "mit", "language": "python", "size": 1461}
{"docstring": "\"\"\"\nCreate a list of form field instances from the passed in 'attrs', plus any\nsimilar fields on the base classes (in 'bases'). This is used by both the\nForm and ModelForm metclasses.\n\nIf 'with_base_fields' is True, all fields from the bases are used.\nOtherwise, only fields in the 'declared_fields' attribute on the bases are\nused. The distinction is useful in ModelForm subclassing.\nAlso integrates any additional media definitions\n\"\"\"\n", "func_signal": "def get_declared_fields(bases, attrs, with_base_fields=True):\n", "code": "fields = [(field_name, attrs.pop(field_name)) for field_name, obj in attrs.items() if isinstance(obj, Field)]\nfields.sort(key=lambda x: x[1].creation_counter)\n\n# If this class is subclassing another Form, add that Form's fields.\n# Note that we loop over the bases in *reverse*. This is necessary in\n# order to preserve the correct order of fields.\nif with_base_fields:\n    for base in bases[::-1]:\n        if hasattr(base, 'base_fields'):\n            fields = base.base_fields.items() + fields\nelse:\n    for base in bases[::-1]:\n        if hasattr(base, 'declared_fields'):\n            fields = base.declared_fields.items() + fields\n\nreturn SortedDict(fields)", "path": "django\\forms\\forms.py", "repo_name": "hunch/hunch-sample-app", "stars": 17, "license": "mit", "language": "python", "size": 8094}
{"docstring": "\"\"\"\nSaves the new password.\n\"\"\"\n", "func_signal": "def save(self, commit=True):\n", "code": "self.user.set_password(self.cleaned_data[\"password1\"])\nif commit:\n    self.user.save()\nreturn self.user", "path": "django\\contrib\\auth\\forms.py", "repo_name": "hunch/hunch-sample-app", "stars": 17, "license": "mit", "language": "python", "size": 8094}
{"docstring": "\"\"\"\nReturns True if the form needs to be multipart-encrypted, i.e. it has\nFileInput. Otherwise, False.\n\"\"\"\n", "func_signal": "def is_multipart(self):\n", "code": "for field in self.fields.values():\n    if field.widget.needs_multipart_form:\n        return True\nreturn False", "path": "django\\forms\\forms.py", "repo_name": "hunch/hunch-sample-app", "stars": 17, "license": "mit", "language": "python", "size": 8094}
{"docstring": "\"\"\"\nBuilds an absolute URI from the location and the variables available in\nthis request. If no location is specified, the absolute URI is built on\n``request.get_full_path()``.\n\"\"\"\n", "func_signal": "def build_absolute_uri(self, location=None):\n", "code": "if not location:\n    location = self.get_full_path()\nif not absolute_http_url_re.match(location):\n    current_uri = '%s://%s%s' % (self.is_secure() and 'https' or 'http',\n                                 self.get_host(), self.path)\n    location = urljoin(current_uri, location)\nreturn iri_to_uri(location)", "path": "django\\http\\__init__.py", "repo_name": "hunch/hunch-sample-app", "stars": 17, "license": "mit", "language": "python", "size": 8094}
{"docstring": "# returns the column name for name or a chain of column names if name contains\n# '__' in the case of JOINs\n", "func_signal": "def get_column_name(start_model, name):\n", "code": "denormalized_model = start_model\ncolumn_name = ''\nfor value in name.split('__')[:-1]:\n    column_name += denormalized_model._meta.get_field(value).column + '__'\n    denormalized_model = denormalized_model._meta.get_field(value).rel.to\n\nreturn column_name + denormalized_model._meta.get_field(name.split('__')[-1]).column", "path": "dbindexer\\api.py", "repo_name": "hunch/hunch-sample-app", "stars": 17, "license": "mit", "language": "python", "size": 8094}
{"docstring": "\"\"\"\nCalculates and returns the ID attribute for this BoundField, if the\nassociated Form has specified auto_id. Returns an empty string otherwise.\n\"\"\"\n", "func_signal": "def _auto_id(self):\n", "code": "auto_id = self.form.auto_id\nif auto_id and '%s' in smart_unicode(auto_id):\n    return smart_unicode(auto_id) % self.html_name\nelif auto_id:\n    return self.html_name\nreturn ''", "path": "django\\forms\\forms.py", "repo_name": "hunch/hunch-sample-app", "stars": 17, "license": "mit", "language": "python", "size": 8094}
{"docstring": "# Populates self._post and self._files\n", "func_signal": "def _load_post_and_files(self):\n", "code": "if self.method != 'POST':\n    self._post, self._files = QueryDict('', encoding=self._encoding), MultiValueDict()\n    return\nif self._read_started:\n    self._mark_post_parse_error()\n    return\n\nif self.META.get('CONTENT_TYPE', '').startswith('multipart'):\n    self._raw_post_data = ''\n    try:\n        self._post, self._files = self.parse_file_upload(self.META, self)\n    except:\n        # An error occured while parsing POST data.  Since when\n        # formatting the error the request handler might access\n        # self.POST, set self._post and self._file to prevent\n        # attempts to parse POST data again.\n        # Mark that an error occured.  This allows self.__repr__ to\n        # be explicit about it instead of simply representing an\n        # empty POST\n        self._mark_post_parse_error()\n        raise\nelse:\n    self._post, self._files = QueryDict(self.raw_post_data, encoding=self._encoding), MultiValueDict()", "path": "django\\http\\__init__.py", "repo_name": "hunch/hunch-sample-app", "stars": 17, "license": "mit", "language": "python", "size": 8094}
{"docstring": "# Some browsers do not support quoted-string from RFC 2109,\n# including some versions of Safari and Internet Explorer.\n# These browsers split on ';', and some versions of Safari\n# are known to split on ', '. Therefore, we encode ';' and ','\n\n# SimpleCookie already does the hard work of encoding and decoding.\n# It uses octal sequences like '\\\\012' for newline etc.\n# and non-ASCII chars.  We just make use of this mechanism, to\n# avoid introducing two encoding schemes which would be confusing\n# and especially awkward for javascript.\n\n# NB, contrary to Python docs, value_encode returns a tuple containing\n# (real val, encoded_val)\n", "func_signal": "def value_encode(self, val):\n", "code": "val, encoded = super(CompatCookie, self).value_encode(val)\n\nencoded = encoded.replace(\";\", \"\\\\073\").replace(\",\",\"\\\\054\")\n# If encoded now contains any quoted chars, we need double quotes\n# around the whole string.\nif \"\\\\\" in encoded and not encoded.startswith('\"'):\n    encoded = '\"' + encoded + '\"'\n\nreturn val, encoded", "path": "django\\http\\__init__.py", "repo_name": "hunch/hunch-sample-app", "stars": 17, "license": "mit", "language": "python", "size": 8094}
{"docstring": "\"\"\"\nSets the encoding used for GET/POST accesses. If the GET or POST\ndictionary has already been created, it is removed and recreated on the\nnext access (so that it is decoded correctly).\n\"\"\"\n", "func_signal": "def _set_encoding(self, val):\n", "code": "self._encoding = val\nif hasattr(self, '_get'):\n    del self._get\nif hasattr(self, '_post'):\n    del self._post", "path": "django\\http\\__init__.py", "repo_name": "hunch/hunch-sample-app", "stars": 17, "license": "mit", "language": "python", "size": 8094}
{"docstring": "\"\"\"Converts 'first_name' to 'First name'\"\"\"\n", "func_signal": "def pretty_name(name):\n", "code": "if not name:\n    return u''\nreturn name.replace('_', ' ').capitalize()", "path": "django\\forms\\forms.py", "repo_name": "hunch/hunch-sample-app", "stars": 17, "license": "mit", "language": "python", "size": 8094}
{"docstring": "\"\"\"\nRenders the field by rendering the passed widget, adding any HTML\nattributes passed as attrs.  If no widget is specified, then the\nfield's default widget will be used.\n\"\"\"\n", "func_signal": "def as_widget(self, widget=None, attrs=None, only_initial=False):\n", "code": "if not widget:\n    widget = self.field.widget\n\nattrs = attrs or {}\nauto_id = self.auto_id\nif auto_id and 'id' not in attrs and 'id' not in widget.attrs:\n    if not only_initial:\n        attrs['id'] = auto_id\n    else:\n        attrs['id'] = self.html_initial_id\n\nif not self.form.is_bound:\n    data = self.form.initial.get(self.name, self.field.initial)\n    if callable(data):\n        data = data()\nelse:\n    data = self.field.bound_data(\n        self.data, self.form.initial.get(self.name, self.field.initial))\ndata = self.field.prepare_value(data)\n\nif not only_initial:\n    name = self.html_name\nelse:\n    name = self.html_initial_name\nreturn widget.render(name, data, attrs=attrs)", "path": "django\\forms\\forms.py", "repo_name": "hunch/hunch-sample-app", "stars": 17, "license": "mit", "language": "python", "size": 8094}
{"docstring": "\"\"\"\nReturns the raw_value for a particular field name. This is just a\nconvenient wrapper around widget.value_from_datadict.\n\"\"\"\n", "func_signal": "def _raw_value(self, fieldname):\n", "code": "field = self.fields[fieldname]\nprefix = self.add_prefix(fieldname)\nreturn field.widget.value_from_datadict(self.data, self.files, prefix)", "path": "django\\forms\\forms.py", "repo_name": "hunch/hunch-sample-app", "stars": 17, "license": "mit", "language": "python", "size": 8094}
{"docstring": "\"\"\"\nProvide a description of all media required to render the widgets on this form\n\"\"\"\n", "func_signal": "def _get_media(self):\n", "code": "media = Media()\nfor field in self.fields.values():\n    media = media + field.widget.media\nreturn media", "path": "django\\forms\\forms.py", "repo_name": "hunch/hunch-sample-app", "stars": 17, "license": "mit", "language": "python", "size": 8094}
{"docstring": "\"\"\"Full HTTP message, including headers.\"\"\"\n", "func_signal": "def __str__(self):\n", "code": "return '\\n'.join(['%s: %s' % (key, value)\n    for key, value in self._headers.values()]) \\\n    + '\\n\\n' + self.content", "path": "django\\http\\__init__.py", "repo_name": "hunch/hunch-sample-app", "stars": 17, "license": "mit", "language": "python", "size": 8094}
{"docstring": "\"\"\"\nReturns a string of space-separated CSS classes for this field.\n\"\"\"\n", "func_signal": "def css_classes(self, extra_classes=None):\n", "code": "if hasattr(extra_classes, 'split'):\n    extra_classes = extra_classes.split()\nextra_classes = set(extra_classes or [])\nif self.errors and hasattr(self.form, 'error_css_class'):\n    extra_classes.add(self.form.error_css_class)\nif self.field.required and hasattr(self.form, 'required_css_class'):\n    extra_classes.add(self.form.required_css_class)\nreturn ' '.join(extra_classes)", "path": "django\\forms\\forms.py", "repo_name": "hunch/hunch-sample-app", "stars": 17, "license": "mit", "language": "python", "size": 8094}
{"docstring": "\"\"\"Renders this field as an HTML widget.\"\"\"\n", "func_signal": "def __unicode__(self):\n", "code": "if self.field.show_hidden_initial:\n    return self.as_widget() + self.as_hidden(only_initial=True)\nreturn self.as_widget()", "path": "django\\forms\\forms.py", "repo_name": "hunch/hunch-sample-app", "stars": 17, "license": "mit", "language": "python", "size": 8094}
{"docstring": "\"\"\"\nIf request is passed in, the form will validate that cookies are\nenabled. Note that the request (a HttpRequest object) must have set a\ncookie with the key TEST_COOKIE_NAME and value TEST_COOKIE_VALUE before\nrunning this validation.\n\"\"\"\n", "func_signal": "def __init__(self, request=None, *args, **kwargs):\n", "code": "self.request = request\nself.user_cache = None\nsuper(AuthenticationForm, self).__init__(*args, **kwargs)", "path": "django\\contrib\\auth\\forms.py", "repo_name": "hunch/hunch-sample-app", "stars": 17, "license": "mit", "language": "python", "size": 8094}
{"docstring": "\"\"\"\nAdd the data to the StringIO file.\n\"\"\"\n", "func_signal": "def receive_data_chunk(self, raw_data, start):\n", "code": "if not self.active:\n    return raw_data", "path": "djangoappengine\\storage.py", "repo_name": "hunch/hunch-sample-app", "stars": 17, "license": "mit", "language": "python", "size": 8094}
{"docstring": "\"\"\"\nReturn a file object if we're activated.\n\"\"\"\n", "func_signal": "def file_complete(self, file_size):\n", "code": "if not self.active:\n    return\n\nreturn BlobstoreUploadedFile(\n    blobinfo=BlobInfo(self.blobkey),\n    charset=self.charset)", "path": "djangoappengine\\storage.py", "repo_name": "hunch/hunch-sample-app", "stars": 17, "license": "mit", "language": "python", "size": 8094}
{"docstring": "\"\"\"\nValidates that a user exists with the given e-mail address.\n\"\"\"\n", "func_signal": "def clean_email(self):\n", "code": "email = self.cleaned_data[\"email\"]\nself.users_cache = User.objects.filter(email__iexact=email)\nif len(self.users_cache) == 0:\n    raise forms.ValidationError(_(\"That e-mail address doesn't have an associated user account. Are you sure you've registered?\"))\nreturn email", "path": "django\\contrib\\auth\\forms.py", "repo_name": "hunch/hunch-sample-app", "stars": 17, "license": "mit", "language": "python", "size": 8094}
{"docstring": "\"\"\"Returns the node of matches to be processed\"\"\"\n\n", "func_signal": "def _retreive_page(self, page_index):\n", "code": "params = self._get_params()\nparams[\"page\"] = str(page_index)\ndoc = self._request(self._ws_prefix + \".search\", True, params)\n\nreturn doc.getElementsByTagName(self._ws_prefix + \"matches\")[0]", "path": "service\\pylast.py", "repo_name": "steffentchr/legacy-stalkify", "stars": 22, "license": "None", "language": "python", "size": 354}
{"docstring": "\"\"\"Returns the user's gender. Either USER_MALE or USER_FEMALE.\"\"\"\n\n", "func_signal": "def get_gender(self):\n", "code": "doc = self._request(\"user.getInfo\", True)\n\nvalue = _extract(doc, \"gender\")\n\nif value == 'm':\n    return USER_MALE\nelif value == 'f':\n    return USER_FEMALE\n\nreturn None", "path": "service\\pylast.py", "repo_name": "steffentchr/legacy-stalkify", "stars": 22, "license": "None", "language": "python", "size": 354}
{"docstring": "\"\"\"Returns the language code of the language used by the user.\"\"\"\n\n", "func_signal": "def get_language(self):\n", "code": "doc = self._request(\"user.getInfo\", True)\n\nreturn _extract(doc, \"lang\")", "path": "service\\pylast.py", "repo_name": "steffentchr/legacy-stalkify", "stars": 22, "license": "None", "language": "python", "size": 354}
{"docstring": "\"\"\"Retrieves a token from the network for web authentication.\nThe token then has to be authorized from getAuthURL before creating session.\n\"\"\"\n\n", "func_signal": "def _get_web_auth_token(self):\n", "code": "request = _Request(self.network, 'auth.getToken')\n\n# default action is that a request is signed only when\n# a session key is provided.\nrequest.sign_it()\n\ndoc = request.execute()\n\ne = doc.getElementsByTagName('token')[0]\nreturn e.firstChild.data", "path": "service\\pylast.py", "repo_name": "steffentchr/legacy-stalkify", "stars": 22, "license": "None", "language": "python", "size": 354}
{"docstring": "\"\"\"Returns the next page of results as a sequence of Tag objects.\"\"\"\n\n", "func_signal": "def get_next_page(self):\n", "code": "master_node = self._retrieve_next_page()\n\nseq = []\nfor node in master_node.getElementsByTagName(\"tag\"):\n    seq.append(Tag(_extract(node, \"name\"), self.network))\n\nreturn seq", "path": "service\\pylast.py", "repo_name": "steffentchr/legacy-stalkify", "stars": 22, "license": "None", "language": "python", "size": 354}
{"docstring": "\"\"\"Shares this track (sends out recommendations). \n  * users: A list that can contain usernames, emails, User objects, or all of them.\n  * message: A message to include in the recommendation message. \n\"\"\"\n\n#last.fm currently accepts a max of 10 recipient at a time\n", "func_signal": "def share(self, users, message = None):\n", "code": "while(len(users) > 10):\n    section = users[0:9]\n    users = users[9:]\n    self.share(section, message)\n\nnusers = []\nfor user in users:\n    if isinstance(user, User):\n        nusers.append(user.get_name())\n    else:\n        nusers.append(user)\n\nparams = self._get_params()\nrecipients = ','.join(nusers)\nparams['recipient'] = recipients\nif message: params['message'] = _unicode(message)\n\nself._request('track.share', False, params)", "path": "service\\pylast.py", "repo_name": "steffentchr/legacy-stalkify", "stars": 22, "license": "None", "language": "python", "size": 354}
{"docstring": "\"\"\"Returns a list of the user's friends.\"\"\"\n\n", "func_signal": "def get_neighbours(self, limit = 50):\n", "code": "params = self._get_params()\nif limit:\n    params['limit'] = _unicode(limit)\n\ndoc = self._request('user.getNeighbours', True, params)\n\nseq = []\nnames = _extract_all(doc, 'name')\n\nfor name in names:\n    seq.append(User(name, self.network))\n\nreturn seq", "path": "service\\pylast.py", "repo_name": "steffentchr/legacy-stalkify", "stars": 22, "license": "None", "language": "python", "size": 354}
{"docstring": "\"\"\"Extracts a sequence of items from a sequence of TopItem or LibraryItem objects.\"\"\"\n\n", "func_signal": "def extract_items(topitems_or_libraryitems):\n", "code": "seq = []\nfor i in topitems_or_libraryitems:\n    seq.append(i.item)\n\nreturn seq", "path": "service\\pylast.py", "repo_name": "steffentchr/legacy-stalkify", "stars": 22, "license": "None", "language": "python", "size": 354}
{"docstring": "\"\"\"Shares this event (sends out recommendations). \n  * users: A list that can contain usernames, emails, User objects, or all of them.\n  * message: A message to include in the recommendation message. \n\"\"\"\n\n#last.fm currently accepts a max of 10 recipient at a time\n", "func_signal": "def share(self, users, message = None):\n", "code": "while(len(users) > 10):\n    section = users[0:9]\n    users = users[9:]\n    self.share(section, message)\n\nnusers = []\nfor user in users:\n    if isinstance(user, User):\n        nusers.append(user.get_name())\n    else:\n        nusers.append(user)\n\nparams = self._get_params()\nrecipients = ','.join(nusers)\nparams['recipient'] = recipients\nif message: params['message'] = _unicode(message)\n\nself._request('event.share', False, params)", "path": "service\\pylast.py", "repo_name": "steffentchr/legacy-stalkify", "stars": 22, "license": "None", "language": "python", "size": 354}
{"docstring": "\"\"\"Returns the track id on the network.\"\"\"\n\n", "func_signal": "def get_id(self):\n", "code": "doc = self._request(\"track.getInfo\", True)\n\nreturn _extract(doc, \"id\")", "path": "service\\pylast.py", "repo_name": "steffentchr/legacy-stalkify", "stars": 22, "license": "None", "language": "python", "size": 354}
{"docstring": "\"\"\"Add an artist to this library.\"\"\"\n\n", "func_signal": "def add_artist(self, artist):\n", "code": "params = self._get_params()\nparams[\"artist\"] = artist.get_name()\n\nself._request(\"library.addArtist\", False, params)", "path": "service\\pylast.py", "repo_name": "steffentchr/legacy-stalkify", "stars": 22, "license": "None", "language": "python", "size": 354}
{"docstring": "\"\"\"Returns a list of the most frequently used Tags on this object.\"\"\"\n\n", "func_signal": "def get_top_tags(self, limit = None):\n", "code": "doc = self._request(self.ws_prefix + '.getTopTags', True)\n\nelements = doc.getElementsByTagName('tag')\nseq = []\n\nfor element in elements:\n    if limit and len(seq) >= limit:\n        break\n    tag_name = _extract(element, 'name')\n    tagcount = _extract(element, 'count')\n    \n    seq.append(TopItem(Tag(tag_name, self.network), tagcount))\n\nreturn seq", "path": "service\\pylast.py", "repo_name": "steffentchr/legacy-stalkify", "stars": 22, "license": "None", "language": "python", "size": 354}
{"docstring": "\"\"\"Extracts all the values from the xml string. returning a list.\"\"\"\n\n", "func_signal": "def _extract_all(node, name, limit_count = None):\n", "code": "seq = []\n\nfor i in range(0, len(node.getElementsByTagName(name))):\n    if len(seq) == limit_count:\n        break\n    \n    seq.append(_extract(node, name, i))\n\nreturn seq", "path": "service\\pylast.py", "repo_name": "steffentchr/legacy-stalkify", "stars": 22, "license": "None", "language": "python", "size": 354}
{"docstring": "\"\"\"\n    Returns a sequqence of Shout objects\n\"\"\"\n\n", "func_signal": "def get_shouts(self, limit=50):\n", "code": "shouts = []\nfor node in _collect_nodes(limit, self, \"artist.getShouts\", False):\n    shouts.append(Shout(\n                        _extract(node, \"body\"),\n                        User(_extract(node, \"author\"), self.network),\n                        _extract(node, \"date\")\n                        )\n                    )\nreturn shouts", "path": "service\\pylast.py", "repo_name": "steffentchr/legacy-stalkify", "stars": 22, "license": "None", "language": "python", "size": 354}
{"docstring": "\"\"\"Returns the summary of the wiki.\"\"\"\n\n", "func_signal": "def get_wiki_summary(self):\n", "code": "doc = self._request(\"track.getInfo\", True)\n\nif len(doc.getElementsByTagName(\"wiki\")) == 0:\n    return\n\nnode = doc.getElementsByTagName(\"wiki\")[0]\n\nreturn _extract(node, \"summary\")", "path": "service\\pylast.py", "repo_name": "steffentchr/legacy-stalkify", "stars": 22, "license": "None", "language": "python", "size": 354}
{"docstring": "\"\"\"Compare this user with another Last.fm user.\nReturns a sequence (tasteometer_score, (shared_artist1, shared_artist2, ...))\nuser: A User object or a username string/unicode object.\n\"\"\"\n\n", "func_signal": "def compare_with_user(self, user, shared_artists_limit = None):\n", "code": "if isinstance(user, User):\n    user = user.get_name()\n\nparams = self._get_params()\nif shared_artists_limit:\n    params['limit'] = _unicode(shared_artists_limit)\nparams['type1'] = 'user'\nparams['type2'] = 'user'\nparams['value1'] = self.get_name()\nparams['value2'] = user\n\ndoc = self._request('tasteometer.compare', False, params)\n\nscore = _extract(doc, 'score')\n\nartists = doc.getElementsByTagName('artists')[0]\nshared_artists_names = _extract_all(artists, 'name')\n\nshared_artists_seq = []\n\nfor name in shared_artists_names:\n    shared_artists_seq.append(Artist(name, self.network))\n\nreturn (score, shared_artists_seq)", "path": "service\\pylast.py", "repo_name": "steffentchr/legacy-stalkify", "stars": 22, "license": "None", "language": "python", "size": 354}
{"docstring": "\"\"\"Returns a sequence of the top tags used by this user with their counts as (Tag, tagcount). \n* limit: The limit of how many tags to return. \n\"\"\"\n\n", "func_signal": "def get_top_tags(self, limit = None):\n", "code": "doc = self._request(\"user.getTopTags\", True)\n\nseq = []\nfor node in doc.getElementsByTagName(\"tag\"):\n    if len(seq) < limit:\n        seq.append(TopItem(Tag(_extract(node, \"name\"), self.network), _extract(node, \"count\")))\n\nreturn seq", "path": "service\\pylast.py", "repo_name": "steffentchr/legacy-stalkify", "stars": 22, "license": "None", "language": "python", "size": 354}
{"docstring": "\"\"\"Returns the url of the artist page on the network. \n# Parameters:\n* domain_name: The network's language domain. Possible values:\n  o DOMAIN_ENGLISH\n  o DOMAIN_GERMAN\n  o DOMAIN_SPANISH\n  o DOMAIN_FRENCH\n  o DOMAIN_ITALIAN\n  o DOMAIN_POLISH\n  o DOMAIN_PORTUGUESE\n  o DOMAIN_SWEDISH\n  o DOMAIN_TURKISH\n  o DOMAIN_RUSSIAN\n  o DOMAIN_JAPANESE\n  o DOMAIN_CHINESE \n\"\"\"\n\n", "func_signal": "def get_url(self, domain_name = DOMAIN_ENGLISH):\n", "code": "artist = _url_safe(self.get_name())\n\nreturn self.network._get_url(domain_name, \"artist\") %{'artist': artist}", "path": "service\\pylast.py", "repo_name": "steffentchr/legacy-stalkify", "stars": 22, "license": "None", "language": "python", "size": 354}
{"docstring": "\"\"\"Returns the headliner of the event. \"\"\"\n\n", "func_signal": "def get_headliner(self):\n", "code": "doc = self._request(\"event.getInfo\", True)\n\nreturn Artist(_extract(doc, \"headliner\"), self.network)", "path": "service\\pylast.py", "repo_name": "steffentchr/legacy-stalkify", "stars": 22, "license": "None", "language": "python", "size": 354}
{"docstring": "\"\"\"Returns the weekly album charts for the week starting from the from_date value to the to_date value.\"\"\"\n\n", "func_signal": "def get_weekly_album_charts(self, from_date = None, to_date = None):\n", "code": "params = self._get_params()\nif from_date and to_date:\n    params[\"from\"] = from_date\n    params[\"to\"] = to_date\n\ndoc = self._request(\"user.getWeeklyAlbumChart\", True, params)\n\nseq = []\nfor node in doc.getElementsByTagName(\"album\"):\n    item = Album(_extract(node, \"artist\"), _extract(node, \"name\"), self.network)\n    weight = _number(_extract(node, \"playcount\"))\n    seq.append(TopItem(item, weight))\n\nreturn seq", "path": "service\\pylast.py", "repo_name": "steffentchr/legacy-stalkify", "stars": 22, "license": "None", "language": "python", "size": 354}
{"docstring": "'''\ntable : '{' object_id table_inner '}'\n      | '{' '-' object_id table_inner '}'\n'''\n", "func_signal": "def p_table(p):\n", "code": "if len(p) == 6:\n    trunc = True\n    objid, inner = p[3:5]\nelse:\n    trunc = False\n    objid, inner = p[2:4]\n\np[0] = morkast.Table(objid, inner['rows'], inner['meta'], trunc)", "path": "src\\MorkDB\\morkyacc.py", "repo_name": "KevinGoodsell/mork-converter", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 467}
{"docstring": "'''\nrow : '[' object_id row_inner ']'\n    | '[' '-' object_id row_inner ']'\n'''\n", "func_signal": "def p_row(p):\n", "code": "if len(p) == 6:\n    trunc = True\n    objid, inner = p[3:5]\nelse:\n    trunc = False\n    objid, inner = p[2:4]\n\np[0] = morkast.Row(objid, inner['cells'], inner['meta'], trunc = trunc)", "path": "src\\MorkDB\\morkyacc.py", "repo_name": "KevinGoodsell/mork-converter", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 467}
{"docstring": "'''\nrow_update : general_row\n           | '+' general_row\n           | '-' general_row\n'''\n", "func_signal": "def p_row_update(p):\n", "code": "if len(p) == 3:\n    p[0] = morkast.RowUpdate(p[2], p[1])\nelse:\n    p[0] = p[1]", "path": "src\\MorkDB\\morkyacc.py", "repo_name": "KevinGoodsell/mork-converter", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 467}
{"docstring": "'''\nitem_list :\n          | item_list item\n'''\n", "func_signal": "def p_item_list(p):\n", "code": "if len(p) == 1:\n    p[0] = []\nelse:\n    p[0] = p[1] + [ p[2] ]", "path": "src\\MorkDB\\morkyacc.py", "repo_name": "KevinGoodsell/mork-converter", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 467}
{"docstring": "'''\ndict_inner : dict_inner meta_dict\n'''\n", "func_signal": "def p_dict_inner_meta(p):\n", "code": "p[1].meta.append(p[2])\np[0] = p[1]", "path": "src\\MorkDB\\morkyacc.py", "repo_name": "KevinGoodsell/mork-converter", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 467}
{"docstring": "'''\ntable_inner :\n            | table_inner row_update\n'''\n", "func_signal": "def p_table_inner_row(p):\n", "code": "if len(p) == 1:\n    p[0] = { 'rows': [], 'meta': [] }\nelse:\n    p[1]['rows'].append(p[2])\n    p[0] = p[1]", "path": "src\\MorkDB\\morkyacc.py", "repo_name": "KevinGoodsell/mork-converter", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 467}
{"docstring": "'''\nobject_reference : CARET HEX\n                 | CARET HEX COLON NAME\n'''\n", "func_signal": "def p_object_reference(p):\n", "code": "if len(p) == 3:\n    obj = morkast.ObjectId(p[2])\nelse:\n    obj = morkast.ObjectId(p[2], p[4])\n\np[0] = morkast.ObjectRef(obj)", "path": "src\\MorkDB\\morkyacc.py", "repo_name": "KevinGoodsell/mork-converter", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 467}
{"docstring": "'''\nmork : MAGIC item_group_list\n     | item_group_list\n'''\n", "func_signal": "def p_mork_db(p):\n", "code": "if len(p) == 2:\n    # No magic found\n    warnings.warn('File may not be a supported mork version')\n    items = p[1]\nelse:\n    items = p[2]\n\np[0] = morkast.Database(items)", "path": "src\\MorkDB\\morkyacc.py", "repo_name": "KevinGoodsell/mork-converter", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 467}
{"docstring": "'''\nrow_inner : row_inner meta_row\n'''\n", "func_signal": "def p_row_inner_meta(p):\n", "code": "p[1]['meta'].append(p[2])\np[0] = p[1]", "path": "src\\MorkDB\\morkyacc.py", "repo_name": "KevinGoodsell/mork-converter", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 467}
{"docstring": "'''\ntable_inner : table_inner meta_table\n'''\n", "func_signal": "def p_table_inner_meta(p):\n", "code": "p[1]['meta'].append(p[2])\np[0] = p[1]", "path": "src\\MorkDB\\morkyacc.py", "repo_name": "KevinGoodsell/mork-converter", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 467}
{"docstring": "'''\ncell_row_list :\n              | cell_row_list cell\n'''\n", "func_signal": "def p_cell_row_list_cell(p):\n", "code": "if len(p) == 1:\n    p[0] = { 'cells': [], 'rows': [] }\nelse:\n    p[1]['cells'].append(p[2])\n    p[0] = p[1]", "path": "src\\MorkDB\\morkyacc.py", "repo_name": "KevinGoodsell/mork-converter", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 467}
{"docstring": "'''\ncell : LPAREN cell_column cell_value RPAREN\n     | '-' LPAREN cell_column cell_value RPAREN\n'''\n", "func_signal": "def p_cell(p):\n", "code": "if len(p) == 6:\n    p[0] = morkast.Cell(p[3], p[4], cut = True)\nelse:\n    p[0] = morkast.Cell(p[2], p[3])", "path": "src\\MorkDB\\morkyacc.py", "repo_name": "KevinGoodsell/mork-converter", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 467}
{"docstring": "'''\nobject_id : HEX\n          | HEX COLON NAME\n'''\n", "func_signal": "def p_object_id(p):\n", "code": "if len(p) == 2:\n    p[0] = morkast.ObjectId(p[1])\nelse:\n    p[0] = morkast.ObjectId(p[1], p[3])", "path": "src\\MorkDB\\morkyacc.py", "repo_name": "KevinGoodsell/mork-converter", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 467}
{"docstring": "'''\ncell_row_list : cell_row_list general_row\n'''\n", "func_signal": "def p_cell_row_list_row(p):\n", "code": "p[1]['rows'].append(p[2])\np[0] = p[1]", "path": "src\\MorkDB\\morkyacc.py", "repo_name": "KevinGoodsell/mork-converter", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 467}
{"docstring": "'''\ncell_list :\n          | cell_list cell\n'''\n", "func_signal": "def p_cell_list(p):\n", "code": "if len(p) == 1:\n    p[0] = []\nelse:\n    p[1].append(p[2])\n    p[0] = p[1]", "path": "src\\MorkDB\\morkyacc.py", "repo_name": "KevinGoodsell/mork-converter", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 467}
{"docstring": "'''\nrow_inner :\n          | row_inner cell\n'''\n", "func_signal": "def p_row_inner_cell(p):\n", "code": "if len(p) == 1:\n    p[0] = { 'cells': [], 'meta': [] }\nelse:\n    p[1]['cells'].append(p[2])\n    p[0] = p[1]", "path": "src\\MorkDB\\morkyacc.py", "repo_name": "KevinGoodsell/mork-converter", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 467}
{"docstring": "'''\nitem_group_list :\n                | item_group_list item_group\n'''\n", "func_signal": "def p_item_group_list(p):\n", "code": "if len(p) == 1:\n    p[0] = []\nelse:\n    p[0] = p[1] + [ p[2] ]", "path": "src\\MorkDB\\morkyacc.py", "repo_name": "KevinGoodsell/mork-converter", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 467}
{"docstring": "'''\nFormat value as as CSV field.\n'''\n", "func_signal": "def _format_csv_value(self, value):\n", "code": "match = self._needs_quotes.search(value)\nif match:\n    # Add surrounding double-quotes and double internal double-quotes.\n    value = '\"%s\"' % value.replace('\"', '\"\"')\n\nreturn value", "path": "src\\MorkDB\\filters\\csv_output.py", "repo_name": "KevinGoodsell/mork-converter", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 467}
{"docstring": "'''\ndict_inner :\n           | dict_inner alias\n'''\n", "func_signal": "def p_dict_inner_alias(p):\n", "code": "if len(p) == 1:\n    p[0] = morkast.Dict()\nelse:\n    p[1].aliases.append(p[2])\n    p[0] = p[1]", "path": "src\\MorkDB\\morkyacc.py", "repo_name": "KevinGoodsell/mork-converter", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 467}
{"docstring": "'''\ngroup : GROUPSTART item_list GROUPCOMMIT\n      | GROUPSTART item_list GROUPABORT\n'''\n", "func_signal": "def p_group(p):\n", "code": "m = _groupId.match(p[1])\nif m is None:\n    raise ValueError('no ID found in group token: %s' % p[1])\n\ncommit = p[3].find('~') == -1\n\np[0] = morkast.Group(m.group('id'), p[2], commit)", "path": "src\\MorkDB\\morkyacc.py", "repo_name": "KevinGoodsell/mork-converter", "stars": 24, "license": "gpl-2.0", "language": "python", "size": 467}
{"docstring": "''' Change sequence node value to Sequence() expr. '''\n# example: \ta (b/c)\n# --> \t\tsequence:[a  Choice(b, c)]\n# --> \t\tSequence(a, Choice(b, c))\n#\n# new node value\n# original format expr for pattern output\n", "func_signal": "def sequenceCode(node):\n", "code": "expr = repr(node.snippet)\n# collect wrapped patterns\npatterns = [p.value for p in node.value]\nargList = \", \".join(patterns)\nnode.value = \"Sequence([%s], expression=%s)\" % (argList,expr)", "path": "generator\\pijnuActions.py", "repo_name": "erikrose/pijnu", "stars": 22, "license": "None", "language": "python", "size": 267}
{"docstring": "''' Change choice node value to Choice() expr. '''\n# example: \t(x y) / z\n# --> \t\tchoice:[Sequence(x, y)  z]\n# --> \t\tChoice(Sequence(x, y), z)\n#\n# new node value\n# original format expr for pattern output\n", "func_signal": "def choiceCode(node):\n", "code": "expr = repr(node.snippet)\n# collect wrapped patterns\npatterns = [p.value for p in node.value]\nargList = \", \".join(patterns)\nnode.value = \"Choice([%s], expression=%s)\" % (argList,expr)", "path": "generator\\pijnuActions.py", "repo_name": "erikrose/pijnu", "stars": 22, "license": "None", "language": "python", "size": 267}
{"docstring": "''' Change klass node value to Klass() expr. '''\n# example: \t[a..e  0..9  _ -  !!d0]\n# --> \t\tklass:\"a..e  0..9  _ -  !!d0\"\n# --> \t\tklass:\"abcde0123456789_ -!!d0\"\t(rangeToCharset)\n# --> \t\tklass:\"abce123456789_ -\"\t\t(klassToCharset)\n# --> \t\tKlass('abce123456789_ -', '[a..e  0..9  _ -  !!d0]')\n#\n# A class node can hold: range, \"!!\" EXCLUSION code, \"  \" KALSSSEP separator,\n# single character expr (litChat, codedChar, decChar, hexChar)\n#\n# original format for output, full charset for matching\n# (use repr() to avoid control character mess)\n", "func_signal": "def klassCode(node):\n", "code": "expr = repr(node.snippet)\ncharset = repr(node.value)\n# new node value\n# !!! format comes first here !!!\nnode.value = \"Klass(u%s, expression=%s)\" % (charset, expr)", "path": "generator\\pijnuActions.py", "repo_name": "erikrose/pijnu", "stars": 22, "license": "None", "language": "python", "size": 267}
{"docstring": "''' Change repetition suffix node value to expression of repetition. '''\n# examples:\n# *\t\t\t--> numMin=False,numMax=False\n# +\t\t\t--> numMin=1,numMax=False\n# {3}\t\t--> numMin=3,numMax=3\n# {3..9}\t--> numMin=3,numMax=9\n# TODO:\n# {..9}\t\t--> numMin=False,numMax=9\n# {3..}\t\t--> numMin=3,numMax=False\n#\n# data\n", "func_signal": "def repetSuffixCode(node):\n", "code": "repetTyp = node.tag\n# case sign: '*' or '+'\nif repetTyp == \"ZEROORMORE\":\n\tnode.value = \"numMin=False, numMax=False\"\nelif repetTyp == \"ONEORMORE\":\n\tnode.value = \"numMin=1, numMax=False\"\n# case numbering\nelif repetTyp == \"number\":\n\tn = node.value\n\tnode.value = \"numMin=%s, numMax=%s\" % (n,n)\nelif repetTyp == \"numRanj\":\n\t(m,n) = (node[0].value,node[1].value)\n\tnode.value = \"numMin=%s, numMax=%s\" % (m,n)", "path": "generator\\pijnuActions.py", "repo_name": "erikrose/pijnu", "stars": 22, "license": "None", "language": "python", "size": 267}
{"docstring": "''' Change ranj node value to expanded range charset. '''\n# example: \t\"a..e\"\n# --> \t\tranj:\"a..e\"\n# --> \t\tranj:\"abcde\"\n#\n# range borders (right-side included)\n", "func_signal": "def ranjToCharset(node):\n", "code": "(c1,c2) = (node[0].value,node[1].value)\n(n1,n2) = (ord(c1),ord(c2))\n# chars in range\nchars = [chr(n) for n in range(n1, n2+1)]\n# new node value\nnode.value = ''.join(chars)", "path": "generator\\pijnuActions.py", "repo_name": "erikrose/pijnu", "stars": 22, "license": "None", "language": "python", "size": 267}
{"docstring": "''' Write title line and record id. '''\n", "func_signal": "def titleCode(node):\n", "code": "node.id = node.value.strip()\n# new node value\nnode.value = \"### title: %s ###\\n\" % node.id", "path": "generator\\pijnuActions.py", "repo_name": "erikrose/pijnu", "stars": 22, "license": "None", "language": "python", "size": 267}
{"docstring": "''' Change word node value to Word() expr. '''\n# example: \t\"xyz\"\n# --> \t\tword:\"xyz\"\n# --> \t\tWord('xyz')\n#\n# original format expr for pattern output\n", "func_signal": "def wordCode(node):\n", "code": "expr = repr(node.snippet)\n# ~ use repr() to avoid trouble of control characters\nword = repr(node.value)\nnode.value = \"Word(%s, expression=%s)\" % (word,expr)", "path": "generator\\pijnuActions.py", "repo_name": "erikrose/pijnu", "stars": 22, "license": "None", "language": "python", "size": 267}
{"docstring": "''' Change klass node value to charset.\n\t~ Actually, there's only to remove possible excluded characters\n\t (ranges have already been expanded) '''\n# example:\t[a..e  0..9 *+  !!c0..6]\n# -->\t\tklass:\"abcde0123456789*+!!c0123456\"\t\t(rangeToCharset)\n# --> \t\tklass:'abde789+*'\n", "func_signal": "def klassToCharset(node):\n", "code": "klass = node.value\nexclusion_count = klass.count(EXCLUSION)\n# case no EXCLUSION: nothing to do\nif exclusion_count == 0:\n\tpass\n# case 1 EXCLUSION: exclude right side from charset\nelif exclusion_count == 1:\n\t(included,excluded) = klass.split(EXCLUSION)\n\tchars = [c for c in included if c not in excluded]\n\tnode.value = ''.join(chars)\n# case more than 1 EXCLUSION: error\nelse:\n\tmessage = (\t\"Class expr cannot hold EXCLUSION code '!!'\"\n\t\t\t\t\" more than once.\\n   %s\" % klass)\n\traise ValueError(message)", "path": "generator\\pijnuActions.py", "repo_name": "erikrose/pijnu", "stars": 22, "license": "None", "language": "python", "size": 267}
{"docstring": "''' Change hexadecimal ordinal format to character  '''\n# '\\x61' --> 'a'\n", "func_signal": "def hexToChar(node):\n", "code": "ord = int(node[2:], 16)\nnode.value = chr(ord)", "path": "generator\\pijnuActions.py", "repo_name": "erikrose/pijnu", "stars": 22, "license": "None", "language": "python", "size": 267}
{"docstring": "''' Change introduction node value to introduction in code. '''\n# introduction node: [intro lines] or NIL\n# where intro lines are comment or blank lines\n#\n# record title\n", "func_signal": "def introductionCode(node):\n", "code": "for line in node:\n\tif line.tag == \"title\":\n\t\tnode.title = line.id\n\t\tbreak\n# new node value\nintroLines = '\\n'.join(line.value for line in node.value)\nnode.value = \"%s\\n\\n\" % introLines", "path": "generator\\pijnuActions.py", "repo_name": "erikrose/pijnu", "stars": 22, "license": "None", "language": "python", "size": 267}
{"docstring": "''' Change string node value to expr of repetition.\n\t* When the base pattern is a klass, or a simple char,\n\t  a String pattern will be yielded instead;\n\t  else a general Repetition pattern.\n\t  This is managed in Repetition's __new__ method. '''\n# example: \t(x/y)*\n# --> \t\trepetition:[choice:[name:x  name:y]  ZEROORMORE:*]\n# --> \t\tRepetition(x, numMin=False,numMax=False)\n# example: \t[1..9]{1..3}\n# --> \t\trepetition:[klass:[1..9]  numRanj:[1  3]]\n# --> \t\tRepetition(Klass(\"123456789\"), numMin=1,numMax=3)\n# --> \t\tString(Klass(\"123456789\"), numMin=1,numMax=3)\n#\n# original format expr for pattern output\n", "func_signal": "def repetitionCode(node):\n", "code": "expr = repr(node.snippet)\n# repeted pattern, repetition suffix\n(pattern,suffix) = (node[0].value,node[1].value)\n# new node value\nnode.value = \"Repetition(%s, %s, expression=%s)\" \\\n\t\t\t\t% (pattern, suffix, expr)", "path": "generator\\pijnuActions.py", "repo_name": "erikrose/pijnu", "stars": 22, "license": "None", "language": "python", "size": 267}
{"docstring": "''' Change preprocess node value to preprocess section in code.\n\t~ just copy lines for now '''\n", "func_signal": "def preprocessCode(node):\n", "code": "PREPROCESS = \"### PREPROCESS ###\\n\"\n# new node value\nif node.value == Node.NIL:\n\tnode.tag = \"preprocess\"\n\tnode.value = \"\"\nelse:\n\tpreprocessLines = '\\n'.join(line.value for line in node.value)\n\tnode.value = \"%s\\n\\n\" % preprocessLines", "path": "generator\\pijnuActions.py", "repo_name": "erikrose/pijnu", "stars": 22, "license": "None", "language": "python", "size": 267}
{"docstring": "\"\"\"Return a parser.\n\nThe parser's toolset functions are (optionally) augmented (or overridden)\nby a map of additional ones passed in.\n\n\"\"\"\n", "func_signal": "def make_parser(actions=None):\n", "code": "if actions is None:\n    actions = {}\n\n# Start off with the imported pijnu library functions:\ntoolset = globals().copy()\n\nparser = Parser()\nstate = parser.state\n\n\n### title: testParser ###\n\n\n\n\ntoolset.update(actions)\n\n###   <definition>\nw1 = Word('foo', expression='\"foo\"', name='w1')\nw2 = Word('bar', expression='\"bar\"', name='w2')\nch = Choice([w1, w2], expression='w1 / w2', name='ch')\n\nsymbols = locals().copy()\nsymbols.update(actions)\nparser._recordPatterns(symbols)\nparser._setTopPattern(\"ch\")\nparser.grammarTitle = \"testParser\"\nparser.filename = \"testParserParser.py\"\n\nreturn parser", "path": "library\\testParserParser.py", "repo_name": "erikrose/pijnu", "stars": 22, "license": "None", "language": "python", "size": 267}
{"docstring": "''' Change lookahead node value to lookahead expr. '''\n# example: \t!\"foo\"\n# --> \t\tnextNot:Word(\"foo\")\n# --> \t\tNextNot(Word(\"foo\"))\n#\n# next or nextNot?\n", "func_signal": "def lookaheadCode(node):\n", "code": "lookaheadTyp = \"Next\" if node.tag == \"next\" else \"NextNot\"\npattern = node.value\n# new node value\n# original format expr for pattern output\nexpr = repr(node.snippet)\nnode.value = \"%s(%s, expression=%s)\" % (lookaheadTyp,pattern,expr)", "path": "generator\\pijnuActions.py", "repo_name": "erikrose/pijnu", "stars": 22, "license": "None", "language": "python", "size": 267}
{"docstring": "''' Change option node value to option expr. '''\n# example: \t\"foo\"?\n# --> \t\toption:[Word(\"foo\")]\n# --> \t\tOption(Word(\"foo\"))\n#\n# new node value\n# original format expr for pattern output\n", "func_signal": "def optionCode(node):\n", "code": "expr = repr(node.snippet)\npattern = node.value[0].value\nnode.value = \"Option(%s, expression=%s)\" % (pattern,expr)", "path": "generator\\pijnuActions.py", "repo_name": "erikrose/pijnu", "stars": 22, "license": "None", "language": "python", "size": 267}
{"docstring": "''' Change transform node to expr of transform.\n\t~ Ensure the node has a '' value case no transform.\n\t~ Flag as recursive if tagged. '''\n# example: \tjoin real\n# --> \t\t(join, real)\n# example: \t@ extract\n# --> \t\t(extract)   & 'isRecursive' flag\n#\n", "func_signal": "def transformCode(node):\n", "code": "node.isRecursive = False\n# case NIL value (no transform)\nif node.value == Node.NIL:\n\tnode.value = \"\"\n# case recursive tag and/or transforms required\nelse:\n\ttransforms = node.value\n\t# -- take care of possible RECURSIVE flag\n\tif len(transforms)>0 and transforms[0].tag==\"RECURSIVE\":\n\t\tdel transforms[0]\n\t\tnode.isRecursive = True\n\t# -- new node value\n\tif len(transforms) == 0:\n\t\tnode.value = \"\"\n\telse:\n\t\tdef dispatchExpression(name):\n\t\t\treturn \"toolset['%s']\" % name\n\t\ttransformNames = [tr.value for tr in transforms]\n\t\ttransformList = \", \".join(map(dispatchExpression, transformNames))\n\t\tnode.value = \"(%s)\" % transformList", "path": "generator\\pijnuActions.py", "repo_name": "erikrose/pijnu", "stars": 22, "license": "None", "language": "python", "size": 267}
{"docstring": "''' Change char node value to Char() expr. '''\n# example: \t'a'\n# --> \t\tchar:'a'\n# --> \t\tChar('a')\n#\n# new node value\n# original format expr for pattern output\n", "func_signal": "def charCode(node):\n", "code": "expr = repr(node.snippet)\n# ~ use repr() to avoid trouble of control characters\nchar = repr(node.value)\nnode.value = \"Char(%s, expression=%s)\" % (char,expr)", "path": "generator\\pijnuActions.py", "repo_name": "erikrose/pijnu", "stars": 22, "license": "None", "language": "python", "size": 267}
{"docstring": "''' Change decimal ordinal format to character  '''\n# '\\097' --> 'a'\n", "func_signal": "def decToChar(node):\n", "code": "ord = int(node[1:])\nnode.value = chr(ord)", "path": "generator\\pijnuActions.py", "repo_name": "erikrose/pijnu", "stars": 22, "license": "None", "language": "python", "size": 267}
{"docstring": "''' Change pattern definition to pattern expr code:\n\t~ Append possible transforms.\n\t~ Flag it as recursive if ever.\n\t~ (Original format expr is already given in call).\n\tNote: This will be used for single pattern generation. '''\n# example: \tx y : join\n# --> \t\tpattern[format:Word('foo', '\"foo\"')  transforms:drop]\n# --> \t\tWord('foo', '\"foo\"')(drop)\n#\n# pattern definition: format, transforms\n", "func_signal": "def patternCode(node):\n", "code": "(format,transform) = node\nnode.isRecursive =  transform.isRecursive\nnode.isName =  format.tag == \"name\"\n# store data to allow rewriting (with name) in patternDefCode\n(format,transform) = (format.value, transform.value)\n(node.format,node.transform) = (format,transform)\n# new node value\nnode.value = \"%s%s\" % (format, transform)", "path": "generator\\pijnuActions.py", "repo_name": "erikrose/pijnu", "stars": 22, "license": "None", "language": "python", "size": 267}
{"docstring": "''' Change grammar node value to whole grammar code. '''\n# grammar node: [introduction title definition]\n# Note: grammar node's (source) grammar is 'text' attribute ;-)\n#\n# grammar sections: intro? titleLine toolset? preprocess? definition\n", "func_signal": "def grammarCode(node):\n", "code": "(introduction,toolset,definition) = node\n# grammar's own title,\n# \"top pattern\" name (last one)\n# & original text (definition only)\nnode.title = introduction.title\nnode.topPatternName = definition.topPatternName\nnode.definition = definition.snippet\n# new node value\nnode.value = introduction.value + toolset.value + definition.value", "path": "generator\\pijnuActions.py", "repo_name": "erikrose/pijnu", "stars": 22, "license": "None", "language": "python", "size": 267}
{"docstring": "# Add additional Twitter info tags to the user's object. We do this\n# asynchronously, and all taggings are launched at once (not\n# sequentially).  No-one is waiting on us, so we don't return anything.\n\n", "func_signal": "def addExtraTwitterTags(endpoint, userObject, user):\n", "code": "screenname = user['screen_name']\n\ndef _err(failure, attr):\n    log.err('Failed to add %s tag to user %r:' % (attr, screenname))\n    log.err(failure)\n    # Return None\n\ndef _done(_):\n    log.msg('Added extra tags to user %r.' % screenname)\n\ndeferreds = []\n\nfor attr, tag in extraTags.iteritems():\n    d = userObject.set(endpoint, tag, user[attr])\n    d.addErrback(_err, attr)\n    deferreds.append(d)\n\nd = defer.DeferredList(deferreds)\nd.addCallback(_done)\nreturn d", "path": "tickery\\ftwitter.py", "repo_name": "fluidinfo/Tickery", "stars": 18, "license": "apache-2.0", "language": "python", "size": 295}
{"docstring": "\"\"\" Returns a token from something like:\noauth_token_secret=xxx&oauth_token=xxx\n\"\"\"\n", "func_signal": "def from_string(s):\n", "code": "params = cgi.parse_qs(s, keep_blank_values=False)\nkey = params['oauth_token'][0]\nsecret = params['oauth_token_secret'][0]\ntoken = OAuthToken(key, secret)\ntry:\n    token.callback_confirmed = params['oauth_callback_confirmed'][0]\nexcept KeyError:\n    pass  # 1.0, no callback confirmed.\nreturn token", "path": "tickery\\oauth.py", "repo_name": "fluidinfo/Tickery", "stars": 18, "license": "apache-2.0", "language": "python", "size": 295}
{"docstring": "\"\"\"Builds the base signature string.\"\"\"\n", "func_signal": "def build_signature(self, oauth_request, consumer, token):\n", "code": "key, raw = self.build_signature_base_string(oauth_request, consumer,\n    token)\n\n# HMAC object.\ntry:\n    import hashlib  # 2.5\n    hashed = hmac.new(key, raw, hashlib.sha1)\nexcept:\n    import sha  # Deprecated\n    hashed = hmac.new(key, raw, sha)\n\n# Calculate the digest base 64.\nreturn binascii.b2a_base64(hashed.digest())[:-1]", "path": "tickery\\oauth.py", "repo_name": "fluidinfo/Tickery", "stars": 18, "license": "apache-2.0", "language": "python", "size": 295}
{"docstring": "\"\"\"Convert unicode to utf-8.\"\"\"\n", "func_signal": "def _utf8_str(s):\n", "code": "if isinstance(s, unicode):\n    return s.encode(\"utf-8\")\nelse:\n    return str(s)", "path": "tickery\\oauth.py", "repo_name": "fluidinfo/Tickery", "stars": 18, "license": "apache-2.0", "language": "python", "size": 295}
{"docstring": "# We must at least create the user.\n", "func_signal": "def addUserByScreenname(cache, endpoint, userJob):\n", "code": "userJob.workToDo = WORK_TO_CREATE_A_FRIEND\nuserJob.workDone = 0\nscreenname = userJob.screenname\nlog.msg('Adding user %r' % screenname)\n\ndef catchUnknownScreenname(fail):\n    fail.trap(error.Error)\n    if int(fail.value.status) != http.NOT_FOUND:\n        return fail\n    return defer.fail(UnknownScreenname(screenname))\n\ndef catchProtectedScreenname(fail):\n    fail.trap(error.Error)\n    if int(fail.value.status) != http.UNAUTHORIZED:\n        return fail\n    return defer.fail(ProtectedScreenname(screenname))\n\nd = cache.friendsIdCache[screenname]\nd.addErrback(catchUnknownScreenname)\nd.addErrback(catchProtectedScreenname)\nfriendUids = yield d\nlog.msg('Got %d friends for user %r' % (len(friendUids), screenname))\n\n# Make a tag for this new user to mark their friends with.\nns = Namespace(TWITTER_USERNAME, TWITTER_FRIENDS_NAMESPACE_NAME)\nd = ns.createTag(endpoint, screenname.lower(),\n    \"A tag used to mark %s's Twitter friends.\" % screenname, False)\n# TODO: check the X-FluidDB-Error-Class header in the errback to make\n# sure it really got a namespace already exists error.\nd.addErrback(_ignoreHTTPStatus, http.PRECONDITION_FAILED)\nyield d\n# Note: the call to createTag (above) will return a Tag instance when\n# txFluidDB gets fixed.\nfriendTag = Tag(TWITTER_USERNAME, TWITTER_FRIENDS_NAMESPACE_NAME,\n                screenname.lower())\nfriendTagPath = friendTag.getPath()\nlog.msg('Created Twitter friends tag %s' % friendTagPath)\n\ndef _madeUserDone(userObject, user):\n    userJob.workDone += WORK_TO_CREATE_A_FRIEND\n    cache.extraTwitterTagsPool.add(\n        addExtraTwitterTags(endpoint, userObject, user))\n    return userObject\n\ndef _madeUserErr(failure):\n    userJob.workDone += WORK_TO_CREATE_A_FRIEND\n    return failure\n\ndef _tagFriendDone():\n    userJob.workDone += WORK_TO_TAG_A_FRIEND\n\ndef makeUser(user, thisIndex=None, totalToAdd=None):\n    newName = user['screen_name']\n    if thisIndex is not None:\n        log.msg('Making user %r, friend %d/%d of %r.' %\n                (newName, thisIndex, totalToAdd, screenname))\n    else:\n        log.msg('Making user %r.' % newName)\n    d = cache.oidUidScreennameCache.objectByUid(user['id'], newName)\n    d.addCallbacks(_madeUserDone, _madeUserErr, callbackArgs=(user,))\n    return d\n\ndef _ignore404uid(fail, uid):\n    fail.trap(error.Error)\n    if int(fail.value.status) == http.NOT_FOUND:\n        log.msg('Twitter uid %d is no longer found (404). Ignoring.' % uid)\n        cache.userCache.removeUid(uid)\n        cache.oidUidScreennameCache.removeUid(uid)\n    else:\n        log.msg('Failure fetching Twitter uid %d:' % uid)\n        log.err(fail)\n\ndef makeCreateUserJobs(friendsToAdd):\n    nToAdd = len(friendsToAdd)\n    for i, friendUid in enumerate(friendsToAdd):\n        if userJob.canceled():\n            log.msg('Detected cancelation of screenname %r.' % screenname)\n            raise StopIteration\n        d = cache.userCache.userByUid(friendUid)\n        d.addCallbacks(makeUser, _ignore404uid,\n                       callbackArgs=(i + 1, nToAdd),\n                       errbackArgs=(friendUid,))\n        yield d\n\n@defer.inlineCallbacks\ndef addFriend(friendName, thisIndex, totalToAdd):\n    log.msg('About to mark user %r as a friend %d/%d of %r.' %\n            (friendName, thisIndex, totalToAdd, screenname))\n    d = cache.oidUidScreennameCache.objectIdByScreenname(friendName)\n    d.addErrback(log.err)\n    objectId = yield d\n    log.msg('Marking user %r as a friend %d/%d of %r' %\n            (friendName, thisIndex, totalToAdd, screenname))\n    if objectId is not None:\n        o = Object(objectId)\n        yield o.set(endpoint, friendTag, None)\n        log.msg('Marked user %r as a friend %d/%d of %r' %\n                (friendName, thisIndex, totalToAdd, screenname))\n    _tagFriendDone()\n\ndef makeTagFriendsJobs():\n    nFriendUids = len(friendUids)\n    for i, friendUid in enumerate(friendUids):\n        if userJob.canceled():\n            log.msg('Detected cancelation of screenname %r.' % screenname)\n            raise StopIteration\n        d = cache.userCache.screennameByUid(friendUid)\n        d.addCallbacks(addFriend, _ignore404uid,\n                       callbackArgs=(i + 1, nFriendUids),\n                       errbackArgs=(friendUid,))\n        yield d\n\n# Get screename's id and add them as a Twitter user.\nuser = yield cache.userCache.userByScreenname(screenname)\nuserObject = yield makeUser(user)\nlog.msg('User object for %r is %r' % (screenname, userObject))\n\n# Add the amount of work will it be to tag all friends.\nuserJob.workToDo += (len(friendUids) * WORK_TO_TAG_A_FRIEND)\n\n# Figure out the work will it be to create whatever friends are needed.\nfriendsToAdd = [fid for fid in friendUids\n                if not cache.oidUidScreennameCache.knownUid(fid)]\nnFriendsToAdd = len(friendsToAdd)\nlog.msg('Must create %d new user objects as friends of %r.' %\n        (nFriendsToAdd, screenname))\n\nif nFriendsToAdd and not userJob.canceled():\n    userJob.workToDo += (nFriendsToAdd * WORK_TO_CREATE_A_FRIEND)\n    start = time.time()\n\n    # Create Fluidinfo objects for all the friends that we don't yet know\n    # about.\n    jobs = makeCreateUserJobs(friendsToAdd)\n    deferreds = []\n    coop = task.Cooperator()\n    for i in xrange(MAX_SIMULTANEOUS_REQUESTS):\n        d = coop.coiterate(jobs)\n        d.addErrback(log.err)\n        deferreds.append(d)\n    yield defer.DeferredList(deferreds)\n\n    if not userJob.canceled():\n        elapsed = time.time() - start\n        log.msg('Created %d new friend (of %r) objects in %.2f seconds. '\n                'Mean %.4f' % (nFriendsToAdd, screenname, elapsed,\n                               float(elapsed / nFriendsToAdd)))\n\nif friendUids and not userJob.canceled():\n    # Tag all friends.\n    start = time.time()\n    jobs = makeTagFriendsJobs()\n    deferreds = []\n    coop = task.Cooperator()\n    for i in xrange(MAX_SIMULTANEOUS_REQUESTS):\n        d = coop.coiterate(jobs)\n        d.addErrback(log.err)\n        deferreds.append(d)\n    log.msg('About to yield friend tagging DL for %r' % screenname)\n    yield defer.DeferredList(deferreds)\n    log.msg('Friend tagging DL finished for %r' % screenname)\n\n    if not userJob.canceled():\n        elapsed = time.time() - start\n        nFriendsUids = len(friendUids)\n        log.msg('Tagged %d objects as being a friend of %r in %.2f '\n                'seconds. Mean = %.4f' % (nFriendsUids, screenname,\n                elapsed, float(elapsed / nFriendsUids)))\n\nif userJob.canceled():\n    log.msg('Canceled addUserByScreenname for %r.' % screenname)\n    raise Canceled(screenname)\nelse:\n    # Add the updated tag to the user's object.\n    log.msg('Adding updated tag to user object for %r' % screenname)\n    yield userObject.set(endpoint, updatedTag, int(time.time()))\n    log.msg('Successfully added screenname %r.' % (screenname,))\n\nuserJob.workDone = userJob.workToDo", "path": "tickery\\ftwitter.py", "repo_name": "fluidinfo/Tickery", "stars": 18, "license": "apache-2.0", "language": "python", "size": 295}
{"docstring": "\"\"\"Set the signature parameter to the result of build_signature.\"\"\"\n# Set the signature method.\n", "func_signal": "def sign_request(self, signature_method, consumer, token):\n", "code": "self.set_parameter('oauth_signature_method',\n    signature_method.get_name())\n# Set the signature.\nself.set_parameter('oauth_signature',\n    self.build_signature(signature_method, consumer, token))", "path": "tickery\\oauth.py", "repo_name": "fluidinfo/Tickery", "stars": 18, "license": "apache-2.0", "language": "python", "size": 295}
{"docstring": "\"\"\"Serialize as post data for a POST request.\"\"\"\n", "func_signal": "def to_postdata(self):\n", "code": "return '&'.join(['%s=%s' % (escape(str(k)), escape(str(v))) \\\n    for k, v in self.parameters.iteritems()])", "path": "tickery\\oauth.py", "repo_name": "fluidinfo/Tickery", "stars": 18, "license": "apache-2.0", "language": "python", "size": 295}
{"docstring": "\"\"\"Return a string that contains the parameters that must be signed.\"\"\"\n", "func_signal": "def get_normalized_parameters(self):\n", "code": "params = self.parameters\ntry:\n    # Exclude the signature if it exists.\n    del params['oauth_signature']\nexcept:\n    pass\n# Escape key values before sorting.\nkey_values = [(escape(_utf8_str(k)), escape(_utf8_str(v))) \\\n    for k, v in params.items()]\n# Sort lexicographically, first after key, then after value.\nkey_values.sort()\n# Combine key value pairs into a string.\nreturn '&'.join(['%s=%s' % (k, v) for k, v in key_values])", "path": "tickery\\oauth.py", "repo_name": "fluidinfo/Tickery", "stars": 18, "license": "apache-2.0", "language": "python", "size": 295}
{"docstring": "\"\"\"Turn Authorization: header into parameters.\"\"\"\n", "func_signal": "def _split_header(header):\n", "code": "params = {}\nparts = header.split(',')\nfor param in parts:\n    # Ignore realm parameter.\n    if param.find('realm') > -1:\n        continue\n    # Remove whitespace.\n    param = param.strip()\n    # Split key-value.\n    param_parts = param.split('=', 1)\n    # Remove quotes and unescape the value.\n    params[param_parts[0]] = urllib.unquote(param_parts[1].strip('\\\"'))\nreturn params", "path": "tickery\\oauth.py", "repo_name": "fluidinfo/Tickery", "stars": 18, "license": "apache-2.0", "language": "python", "size": 295}
{"docstring": "\"\"\"Verify the correct version request for this server.\"\"\"\n", "func_signal": "def _get_version(self, oauth_request):\n", "code": "try:\n    version = oauth_request.get_parameter('oauth_version')\nexcept:\n    version = VERSION\nif version and version != self.version:\n    raise OAuthError('OAuth version %s not supported.' % str(version))\nreturn version", "path": "tickery\\oauth.py", "repo_name": "fluidinfo/Tickery", "stars": 18, "license": "apache-2.0", "language": "python", "size": 295}
{"docstring": "# Keep the ids in the order we received them. The 'seen' set is to\n# avoid duplicates.\n", "func_signal": "def _deDup(self):\n", "code": "seen = set()\nresult = []\nfor uid in self.results:\n    if uid not in seen:\n        result.append(uid)\n        seen.add(uid)\nreturn result", "path": "tickery\\twitter.py", "repo_name": "fluidinfo/Tickery", "stars": 18, "license": "apache-2.0", "language": "python", "size": 295}
{"docstring": "\"\"\"Parses the URL and rebuilds it to be scheme://host/path.\"\"\"\n", "func_signal": "def get_normalized_http_url(self):\n", "code": "parts = urlparse.urlparse(self.http_url)\nscheme, netloc, path = parts[:3]\n# Exclude default port numbers.\nif scheme == 'http' and netloc[-3:] == ':80':\n    netloc = netloc[:-3]\nelif scheme == 'https' and netloc[-4:] == ':443':\n    netloc = netloc[:-4]\nreturn '%s://%s%s' % (scheme, netloc, path)", "path": "tickery\\oauth.py", "repo_name": "fluidinfo/Tickery", "stars": 18, "license": "apache-2.0", "language": "python", "size": 295}
{"docstring": "# Does the object for the user screenname2 have a screenname1\n# follows tag on it?\n\n", "func_signal": "def friendOf(cache, endpoint, screenname1, screenname2):\n", "code": "def filter404(fail):\n    fail.trap(error.Error)\n    if int(fail.value.status) == http.NOT_FOUND:\n        return False\n    else:\n        return fail\n\ndef checkTagOnObject(objectId):\n    o = Object(objectId)\n    tag = Tag(TWITTER_USERNAME, TWITTER_FRIENDS_NAMESPACE_NAME,\n              screenname1.lower())\n    d = o.get(endpoint, tag)\n    d.addCallback(lambda _: True)\n    return d\n\nd = cache.oidUidScreennameCache.objectIdByScreenname(screenname2)\nd.addCallback(checkTagOnObject)\nd.addErrback(filter404)\nreturn d", "path": "tickery\\ftwitter.py", "repo_name": "fluidinfo/Tickery", "stars": 18, "license": "apache-2.0", "language": "python", "size": 295}
{"docstring": "\"\"\"Concatenates the consumer key and secret.\"\"\"\n", "func_signal": "def build_signature_base_string(self, oauth_request, consumer, token):\n", "code": "sig = '%s&' % escape(consumer.secret)\nif token:\n    sig = sig + escape(token.secret)\nreturn sig, sig", "path": "tickery\\oauth.py", "repo_name": "fluidinfo/Tickery", "stars": 18, "license": "apache-2.0", "language": "python", "size": 295}
{"docstring": "\"\"\"Verifies an api call and checks all the parameters.\"\"\"\n# -> consumer and token\n", "func_signal": "def verify_request(self, oauth_request):\n", "code": "self._get_version(oauth_request)\nconsumer = self._get_consumer(oauth_request)\n# Get the access token.\ntoken = self._get_token(oauth_request, 'access')\nself._check_signature(oauth_request, consumer, token)\nparameters = oauth_request.get_nonoauth_parameters()\nreturn consumer, token, parameters", "path": "tickery\\oauth.py", "repo_name": "fluidinfo/Tickery", "stars": 18, "license": "apache-2.0", "language": "python", "size": 295}
{"docstring": "\"\"\"Turn URL string into parameters.\"\"\"\n", "func_signal": "def _split_url_string(param_str):\n", "code": "parameters = cgi.parse_qs(param_str, keep_blank_values=False)\nfor k, v in parameters.iteritems():\n    parameters[k] = urllib.unquote(v[0])\nreturn parameters", "path": "tickery\\oauth.py", "repo_name": "fluidinfo/Tickery", "stars": 18, "license": "apache-2.0", "language": "python", "size": 295}
{"docstring": "# Note: only use the friends cache if the user is not protected. That's\n# because the cache only contains public users, to prevent people who\n# are not authorized followers of a private user from doing queries\n# on that person using our cache.\n#\n# We could still cache (elsewhere) the friends of private users, but\n# let's leave that for later, since most users are public and it's a\n# bit sensitive.\n", "func_signal": "def friendsIdFetcher(cookie, cache, screenname):\n", "code": "user, accessToken = _lookupCookie(cookie, cache, 'friendsIdFetcher')\nif user['protected']:\n    fetcher = twitter.FriendsIdFetcher(screenname, accessToken)\n    d = fetcher.fetch()\nelse:\n    d = cache.friendsIdCache[screenname]\nreturn d", "path": "tickery\\ftwitter.py", "repo_name": "fluidinfo/Tickery", "stars": 18, "license": "apache-2.0", "language": "python", "size": 295}
{"docstring": "# The cookie cache is actually only available once the cache\n# service has started. But that should happen before we get any\n# requests.\n", "func_signal": "def render_GET(self, request):\n", "code": "log.err('Callback received: %s' % request)\n\noauthToken = request.args['oauth_token']\nif oauthToken:\n    oauthToken = oauthToken[0]\nelse:\n    log.err('Received callback with no oauth_token: %s' % request)\n    raise Exception('Received callback with no oauth_token.')\n\noauthVerifier = request.args['oauth_verifier']\nif oauthVerifier:\n    oauthVerifier = oauthVerifier[0]\nelse:\n    log.err('Received callback with no oauth_verifier: %s' % request)\n    raise Exception('Received callback with no oauth_verifier.')\n\ntry:\n    token = self.cache.oauthTokenDict.pop(oauthToken)\nexcept KeyError:\n    log.err('Received callback with unknown oauth_token: %s' %\n            oauthToken)\n    raise Exception('Received callback with unknown oauth_token.')\n\noaRequest = oauth.OAuthRequest.from_consumer_and_token(\n    consumer.consumer, token=token, verifier=oauthVerifier,\n    http_url=twitter.ACCESS_TOKEN_URL)\noaRequest.sign_request(\n    signature.hmac_sha1, consumer.consumer, token)\nlog.msg('Requesting access token.')\nr = RetryingCall(\n    client.getPage, oaRequest.to_url(), headers=oaRequest.to_header())\nd = r.start()\nd.addCallback(self._storeAccessToken, request)\nd.addErrback(log.err)\nreturn server.NOT_DONE_YET", "path": "tickery\\callback.py", "repo_name": "fluidinfo/Tickery", "stars": 18, "license": "apache-2.0", "language": "python", "size": 295}
{"docstring": "# From http://code.activestate.com/recipes/498181/\n", "func_signal": "def splitthousands(n, sep=','):\n", "code": "s = '%s' % n  # This to keep MSIE quiet?\nif len(s) <= 3:\n    return s\nelse:\n    return splitthousands(s[:-3], sep) + sep + s[-3:]", "path": "tickery\\www\\utils.py", "repo_name": "fluidinfo/Tickery", "stars": 18, "license": "apache-2.0", "language": "python", "size": 295}
{"docstring": "\"\"\"Serialize as a header for an HTTPAuth request.\"\"\"\n", "func_signal": "def to_header(self, realm=''):\n", "code": "auth_header = 'OAuth realm=\"%s\"' % realm\n# Add the oauth parameters.\nif self.parameters:\n    for k, v in self.parameters.iteritems():\n        if k[:6] == 'oauth_':\n            auth_header += ', %s=\"%s\"' % (k, escape(str(v)))\nreturn {'Authorization': auth_header}", "path": "tickery\\oauth.py", "repo_name": "fluidinfo/Tickery", "stars": 18, "license": "apache-2.0", "language": "python", "size": 295}
{"docstring": "\"\"\" Set the output format for the class instance. \"\"\"\n", "func_signal": "def set_output_format(self, format):\n", "code": "try:\n    self.serializer = self.output_formats[format.lower()]\nexcept KeyError:\n    message(CRITICAL, 'Invalid Output Format: \"%s\". Use one of %s.' \\\n                       % (format, self.output_formats.keys()))", "path": "markdown\\__init__.py", "repo_name": "pfibiger/grumblechat", "stars": 22, "license": "apache-2.0", "language": "python", "size": 1085}
{"docstring": "#import code; code.interact(local=locals())\n#from pydbgr.api import debug; debug()\n#import pdb; pdb.set_trace()\n", "func_signal": "def main():\n", "code": "webapp.template.register_template_library('tags.filters')\nrun_wsgi_app(application)", "path": "room.py", "repo_name": "pfibiger/grumblechat", "stars": 22, "license": "apache-2.0", "language": "python", "size": 1085}
{"docstring": "# escape attribute value\n", "func_signal": "def _escape_attrib_html(text, encoding):\n", "code": "try:\n    if \"&\" in text:\n        text = text.replace(\"&\", \"&amp;\")\n    if \">\" in text:\n        text = text.replace(\">\", \"&gt;\")\n    if \"\\\"\" in text:\n        text = text.replace(\"\\\"\", \"&quot;\")\n    return text.encode(encoding, \"xmlcharrefreplace\")\nexcept (TypeError, AttributeError):\n    _raise_serialization_error(text)", "path": "markdown\\html4.py", "repo_name": "pfibiger/grumblechat", "stars": 22, "license": "apache-2.0", "language": "python", "size": 1085}
{"docstring": "\"\"\"Converts a markdown file and returns the HTML as a unicode string.\n\nDecodes the file using the provided encoding (defaults to utf-8),\npasses the file content to markdown, and outputs the html to either\nthe provided stream or the file with provided name, using the same\nencoding as the source file.\n\n**Note:** This is the only place that decoding and encoding of unicode\ntakes place in Python-Markdown.  (All other code is unicode-in /\nunicode-out.)\n\nKeyword arguments:\n\n* input: Name of source text file.\n* output: Name of output file. Writes to stdout if `None`.\n* encoding: Encoding of input and output files. Defaults to utf-8.\n\n\"\"\"\n\n", "func_signal": "def convertFile(self, input=None, output=None, encoding=None):\n", "code": "encoding = encoding or \"utf-8\"\n\n# Read the source\ninput_file = codecs.open(input, mode=\"r\", encoding=encoding)\ntext = input_file.read()\ninput_file.close()\ntext = text.lstrip(u'\\ufeff') # remove the byte-order mark\n\n# Convert\nhtml = self.convert(text)\n\n# Write to file or stdout\nif isinstance(output, (str, unicode)):\n    output_file = codecs.open(output, \"w\", encoding=encoding)\n    output_file.write(html)\n    output_file.close()\nelse:\n    output.write(html.encode(encoding))", "path": "markdown\\__init__.py", "repo_name": "pfibiger/grumblechat", "stars": 22, "license": "apache-2.0", "language": "python", "size": 1085}
{"docstring": "# identify namespaces used in this tree\n\n# maps qnames to *encoded* prefix:local names\n", "func_signal": "def _namespaces(elem, encoding, default_namespace=None):\n", "code": "qnames = {None: None}\n\n# maps uri:s to prefixes\nnamespaces = {}\nif default_namespace:\n    namespaces[default_namespace] = \"\"\n\ndef encode(text):\n    return text.encode(encoding)\n\ndef add_qname(qname):\n    # calculate serialized qname representation\n    try:\n        if qname[:1] == \"{\":\n            uri, tag = qname[1:].split(\"}\", 1)\n            prefix = namespaces.get(uri)\n            if prefix is None:\n                prefix = _namespace_map.get(uri)\n                if prefix is None:\n                    prefix = \"ns%d\" % len(namespaces)\n                if prefix != \"xml\":\n                    namespaces[uri] = prefix\n            if prefix:\n                qnames[qname] = encode(\"%s:%s\" % (prefix, tag))\n            else:\n                qnames[qname] = encode(tag) # default element\n        else:\n            if default_namespace:\n                # FIXME: can this be handled in XML 1.0?\n                raise ValueError(\n                    \"cannot use non-qualified names with \"\n                    \"default_namespace option\"\n                    )\n            qnames[qname] = encode(qname)\n    except TypeError:\n        _raise_serialization_error(qname)\n\n# populate qname and namespaces table\ntry:\n    iterate = elem.iter\nexcept AttributeError:\n    iterate = elem.getiterator # cET compatibility\nfor elem in iterate():\n    tag = elem.tag\n    if isinstance(tag, QName) and tag.text not in qnames:\n        add_qname(tag.text)\n    elif isinstance(tag, basestring):\n        if tag not in qnames:\n            add_qname(tag)\n    elif tag is not None and tag is not Comment and tag is not PI:\n        _raise_serialization_error(tag)\n    for key, value in elem.items():\n        if isinstance(key, QName):\n            key = key.text\n        if key not in qnames:\n            add_qname(key)\n        if isinstance(value, QName) and value.text not in qnames:\n            add_qname(value.text)\n    text = elem.text\n    if isinstance(text, QName) and text.text not in qnames:\n        add_qname(text.text)\nreturn qnames, namespaces", "path": "markdown\\html4.py", "repo_name": "pfibiger/grumblechat", "stars": 22, "license": "apache-2.0", "language": "python", "size": 1085}
{"docstring": "\"\"\" Setup configs. \"\"\"\n", "func_signal": "def __init__ (self, configs):\n", "code": "self.config = {'PLACE_MARKER':\n               [\"///Footnotes Go Here///\",\n                \"The text string that marks where the footnotes go\"]}\n\nfor key, value in configs:\n    self.config[key][0] = value\n    \nself.reset()", "path": "markdown\\extensions\\footnotes.py", "repo_name": "pfibiger/grumblechat", "stars": 22, "license": "apache-2.0", "language": "python", "size": 1085}
{"docstring": "\"\"\"\nResets all state variables so that we can start with a new text.\n\"\"\"\n", "func_signal": "def reset(self):\n", "code": "self.htmlStash.reset()\nself.references.clear()\n\nfor extension in self.registeredExtensions:\n    extension.reset()", "path": "markdown\\__init__.py", "repo_name": "pfibiger/grumblechat", "stars": 22, "license": "apache-2.0", "language": "python", "size": 1085}
{"docstring": "\"\"\" Return a setting for the given key or an empty string. \"\"\"\n", "func_signal": "def getConfig(self, key):\n", "code": "if key in self.config:\n    return self.config[key][0]\nelse:\n    return \"\"", "path": "markdown\\__init__.py", "repo_name": "pfibiger/grumblechat", "stars": 22, "license": "apache-2.0", "language": "python", "size": 1085}
{"docstring": "\"\"\" Return div of footnotes as et Element. \"\"\"\n\n", "func_signal": "def makeFootnotesDiv(self, root):\n", "code": "if not self.footnotes.keys():\n    return None\n\ndiv = etree.Element(\"div\")\ndiv.set('class', 'footnote')\nhr = etree.SubElement(div, \"hr\")\nol = etree.SubElement(div, \"ol\")\n\nfor id in self.footnotes.keys():\n    li = etree.SubElement(ol, \"li\")\n    li.set(\"id\", self.makeFootnoteId(id))\n    self.parser.parseChunk(li, self.footnotes[id])\n    backlink = etree.Element(\"a\")\n    backlink.set(\"href\", \"#\" + self.makeFootnoteRefId(id))\n    backlink.set(\"rev\", \"footnote\")\n    backlink.set(\"title\", \"Jump back to footnote %d in the text\" % \\\n                    (self.footnotes.index(id)+1))\n    backlink.text = FN_BACKLINK_TEXT\n\n    if li.getchildren():\n        node = li[-1]\n        if node.tag == \"p\":\n            node.text = node.text + NBSP_PLACEHOLDER\n            node.append(backlink)\n        else:\n            p = etree.SubElement(li, \"p\")\n            p.append(backlink)\nreturn div", "path": "markdown\\extensions\\footnotes.py", "repo_name": "pfibiger/grumblechat", "stars": 22, "license": "apache-2.0", "language": "python", "size": 1085}
{"docstring": "# escape character data\n", "func_signal": "def _escape_cdata(text, encoding):\n", "code": "try:\n    # it's worth avoiding do-nothing calls for strings that are\n    # shorter than 500 character, or so.  assume that's, by far,\n    # the most common case in most applications.\n    if \"&\" in text:\n        text = text.replace(\"&\", \"&amp;\")\n    if \"<\" in text:\n        text = text.replace(\"<\", \"&lt;\")\n    if \">\" in text:\n        text = text.replace(\">\", \"&gt;\")\n    return text.encode(encoding, \"xmlcharrefreplace\")\nexcept (TypeError, AttributeError):\n    _raise_serialization_error(text)", "path": "markdown\\html4.py", "repo_name": "pfibiger/grumblechat", "stars": 22, "license": "apache-2.0", "language": "python", "size": 1085}
{"docstring": "\"\"\"Run Markdown from the command line.\"\"\"\n\n# Parse options and adjust logging level if necessary\n", "func_signal": "def run():\n", "code": "options, logging_level = parse_options()\nif not options: sys.exit(0)\nif logging_level: logging.getLogger('MARKDOWN').setLevel(logging_level)\n\n# Run\nmarkdown.markdownFromFile(**options)", "path": "markdown\\commandline.py", "repo_name": "pfibiger/grumblechat", "stars": 22, "license": "apache-2.0", "language": "python", "size": 1085}
{"docstring": "\"\"\"Load extension by name, then return the module.\n\nThe extension name may contain arguments as part of the string in the\nfollowing format: \"extname(key1=value1,key2=value2)\"\n\n\"\"\"\n\n# Parse extensions config params (ignore the order)\n", "func_signal": "def load_extension(ext_name, configs = []):\n", "code": "configs = dict(configs)\npos = ext_name.find(\"(\") # find the first \"(\"\nif pos > 0:\n    ext_args = ext_name[pos+1:-1]\n    ext_name = ext_name[:pos]\n    pairs = [x.split(\"=\") for x in ext_args.split(\",\")]\n    configs.update([(x.strip(), y.strip()) for (x, y) in pairs])\n\n# Setup the module names\next_module = 'markdown.extensions'\nmodule_name_new_style = '.'.join([ext_module, ext_name])\nmodule_name_old_style = '_'.join(['mdx', ext_name])\n\n# Try loading the extention first from one place, then another\ntry: # New style (markdown.extensons.<extension>)\n    module = __import__(module_name_new_style, {}, {}, [ext_module])\nexcept ImportError:\n    try: # Old style (mdx.<extension>)\n        module = __import__(module_name_old_style)\n    except ImportError:\n       message(WARN, \"Failed loading extension '%s' from '%s' or '%s'\"\n           % (ext_name, module_name_new_style, module_name_old_style))\n       # Return None so we don't try to initiate none-existant extension\n       return None\n\n# If the module is loaded successfully, we expect it to define a\n# function called makeExtension()\ntry:\n    return module.makeExtension(configs.items())\nexcept AttributeError:\n    message(CRITICAL, \"Failed to initiate extension '%s'\" % ext_name)", "path": "markdown\\__init__.py", "repo_name": "pfibiger/grumblechat", "stars": 22, "license": "apache-2.0", "language": "python", "size": 1085}
{"docstring": "\"\"\" Find indented text and remove indent before further proccesing.\n\nKeyword arguments:\n\n* lines: an array of strings\n\nReturns: a list of post processed items and the unused\nremainder of the original list\n\n\"\"\"\n", "func_signal": "def detectTabbed(self, lines):\n", "code": "items = []\nitem = -1\ni = 0 # to keep track of where we are\n\ndef detab(line):\n    match = TABBED_RE.match(line)\n    if match:\n       return match.group(4)\n\nfor line in lines:\n    if line.strip(): # Non-blank line\n        line = detab(line)\n        if line:\n            items.append(line)\n            i += 1\n            continue\n        else:\n            return items, lines[i:]\n\n    else: # Blank line: _maybe_ we are done.\n        i += 1 # advance\n\n        # Find the next non-blank line\n        for j in range(i, len(lines)):\n            if lines[j].strip():\n                next_line = lines[j]; break\n        else:\n            break # There is no more text; we are done.\n\n        # Check if the next non-blank line is tabbed\n        if detab(next_line): # Yes, more work to do.\n            items.append(\"\")\n            continue\n        else:\n            break # No, we are done.\nelse:\n    i += 1\n\nreturn items, lines[i:]", "path": "markdown\\extensions\\footnotes.py", "repo_name": "pfibiger/grumblechat", "stars": 22, "license": "apache-2.0", "language": "python", "size": 1085}
{"docstring": "\"\"\"\nRecursively find all footnote definitions in lines.\n\nKeywords:\n\n* lines: A list of lines of text\n\nReturn: A list of lines with footnote definitions removed.\n\n\"\"\"\n", "func_signal": "def _handleFootnoteDefinitions(self, lines):\n", "code": "i, id, footnote = self._findFootnoteDefinition(lines)\n\nif id :\n    plain = lines[:i]\n    detabbed, theRest = self.detectTabbed(lines[i+1:])\n    self.footnotes.setFootnote(id,\n                               footnote + \"\\n\"\n                               + \"\\n\".join(detabbed))\n    more_plain = self._handleFootnoteDefinitions(theRest)\n    return plain + [\"\"] + more_plain\nelse :\n    return lines", "path": "markdown\\extensions\\footnotes.py", "repo_name": "pfibiger/grumblechat", "stars": 22, "license": "apache-2.0", "language": "python", "size": 1085}
{"docstring": "\"\"\" Basic html escaping \"\"\"\n", "func_signal": "def escape(self, html):\n", "code": "html = html.replace('&', '&amp;')\nhtml = html.replace('<', '&lt;')\nhtml = html.replace('>', '&gt;')\nreturn html.replace('\"', '&quot;')", "path": "markdown\\postprocessors.py", "repo_name": "pfibiger/grumblechat", "stars": 22, "license": "apache-2.0", "language": "python", "size": 1085}
{"docstring": "\"\"\" Return ElementTree Element that contains Footnote placeholder. \"\"\"\n", "func_signal": "def findFootnotesPlaceholder(self, root):\n", "code": "def finder(element):\n    for child in element:\n        if child.text:\n            if child.text.find(self.getConfig(\"PLACE_MARKER\")) > -1:\n                return child, True\n        if child.tail:\n            if child.tail.find(self.getConfig(\"PLACE_MARKER\")) > -1:\n                return (child, element), False\n        finder(child)\n    return None\n        \nres = finder(root)\nreturn res", "path": "markdown\\extensions\\footnotes.py", "repo_name": "pfibiger/grumblechat", "stars": 22, "license": "apache-2.0", "language": "python", "size": 1085}
{"docstring": "\"\"\"\nRegister extensions with this instance of Markdown.\n\nKeyword aurguments:\n\n* extensions: A list of extensions, which can either\n   be strings or objects.  See the docstring on Markdown.\n* configs: A dictionary mapping module names to config options.\n\n\"\"\"\n", "func_signal": "def registerExtensions(self, extensions, configs):\n", "code": "for ext in extensions:\n    if isinstance(ext, basestring):\n        ext = load_extension(ext, configs.get(ext, []))\n    try:\n        ext.extendMarkdown(self, globals())\n    except AttributeError:\n        message(ERROR, \"Incorrect type! Extension '%s' is \"\n                       \"neither a string or an Extension.\" %(repr(ext)))", "path": "markdown\\__init__.py", "repo_name": "pfibiger/grumblechat", "stars": 22, "license": "apache-2.0", "language": "python", "size": 1085}
{"docstring": "\"\"\" Iterate over html stash and restore \"safe\" html. \"\"\"\n", "func_signal": "def run(self, text):\n", "code": "for i in range(self.markdown.htmlStash.html_counter):\n    html, safe  = self.markdown.htmlStash.rawHtmlBlocks[i]\n    if self.markdown.safeMode and not safe:\n        if str(self.markdown.safeMode).lower() == 'escape':\n            html = self.escape(html)\n        elif str(self.markdown.safeMode).lower() == 'remove':\n            html = ''\n        else:\n            html = markdown.HTML_REMOVED_TEXT\n    if safe or not self.markdown.safeMode:\n        text = text.replace(\"<p>%s</p>\" % \n                    (markdown.preprocessors.HTML_PLACEHOLDER % i),\n                    html + \"\\n\")\n    text =  text.replace(markdown.preprocessors.HTML_PLACEHOLDER % i, \n                         html)\nreturn text", "path": "markdown\\postprocessors.py", "repo_name": "pfibiger/grumblechat", "stars": 22, "license": "apache-2.0", "language": "python", "size": 1085}
{"docstring": "# escape attribute value\n", "func_signal": "def _escape_attrib(text, encoding):\n", "code": "try:\n    if \"&\" in text:\n        text = text.replace(\"&\", \"&amp;\")\n    if \"<\" in text:\n        text = text.replace(\"<\", \"&lt;\")\n    if \">\" in text:\n        text = text.replace(\">\", \"&gt;\")\n    if \"\\\"\" in text:\n        text = text.replace(\"\\\"\", \"&quot;\")\n    if \"\\n\" in text:\n        text = text.replace(\"\\n\", \"&#10;\")\n    return text.encode(encoding, \"xmlcharrefreplace\")\nexcept (TypeError, AttributeError):\n    _raise_serialization_error(text)", "path": "markdown\\html4.py", "repo_name": "pfibiger/grumblechat", "stars": 22, "license": "apache-2.0", "language": "python", "size": 1085}
{"docstring": "\"\"\"\nConvert markdown to serialized XHTML or HTML.\n\nKeyword arguments:\n\n* source: Source text as a Unicode string.\n\n\"\"\"\n\n# Fixup the source text\n", "func_signal": "def convert(self, source):\n", "code": "if not source.strip():\n    return u\"\"  # a blank unicode string\ntry:\n    source = unicode(source)\nexcept UnicodeDecodeError:\n    message(CRITICAL, 'UnicodeDecodeError: Markdown only accepts unicode or ascii input.')\n    return u\"\"\n\nsource = source.replace(STX, \"\").replace(ETX, \"\")\nsource = source.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\") + \"\\n\\n\"\nsource = re.sub(r'\\n\\s+\\n', '\\n\\n', source)\nsource = source.expandtabs(TAB_LENGTH)\n\n# Split into lines and run the line preprocessors.\nself.lines = source.split(\"\\n\")\nfor prep in self.preprocessors.values():\n    self.lines = prep.run(self.lines)\n\n# Parse the high-level elements.\nroot = self.parser.parseDocument(self.lines).getroot()\n\n# Run the tree-processors\nfor treeprocessor in self.treeprocessors.values():\n    newRoot = treeprocessor.run(root)\n    if newRoot:\n        root = newRoot\n\n# Serialize _properly_.  Strip top-level tags.\noutput, length = codecs.utf_8_decode(self.serializer(root, encoding=\"utf8\"))\nif self.stripTopLevelTags:\n    start = output.index('<%s>'%DOC_TAG)+len(DOC_TAG)+2\n    end = output.rindex('</%s>'%DOC_TAG)\n    output = output[start:end].strip()\n\n# Run the text post-processors\nfor pp in self.postprocessors.values():\n    output = pp.run(output)\n\nreturn output.strip()", "path": "markdown\\__init__.py", "repo_name": "pfibiger/grumblechat", "stars": 22, "license": "apache-2.0", "language": "python", "size": 1085}
{"docstring": "\"\"\"\nReturns a full html link to incoming_user\n\"\"\"\n", "func_signal": "def create_user_link(incoming_user):\n", "code": "user_link = \"<a href=\\\"%s%s\\\">%s</a>\" % (localsettings.my_URL_BASE, incoming_user, incoming_user)\nreturn user_link", "path": "django\\flowgram\\core\\controller.py", "repo_name": "reverie/flowgram.com", "stars": 22, "license": "None", "language": "python", "size": 67618}
{"docstring": "\"\"\"\nStores Flowgram Favorited event in UserHistory\n\"\"\"\n", "func_signal": "def store_fgfaved_event(data):\n", "code": "try:\n    # Unpack dict\n    currentUser = data['current_user']\n    eventCode = data['eventCode']\n    flowgramIdStr = data['fg_id']\n    fg = Flowgram.objects.get(pk=flowgramIdStr) \n\n    # if the fg is private, we need to kill this here\n    if fg.public == True:\n\n        # Get Flowgram info from flowgramIdStr\n        flowgramTitle = fg.title\n        targetUser = fg.owner\n    \n        # HTML CODE BUILD\n        iconHtmlCode = get_icon_code(eventCode) # img icon\n        currentUserLink = create_user_link(currentUser) # user link\n        targetUserLink = create_user_link(targetUser) # user link\n        flowgramLink = create_fg_details_link(flowgramIdStr, flowgramTitle) # fg link\n    \n        # Determine proper message and add html code sets\n        secondPersonPresent = \"%s <span>You favorited %s's Flowgram named %s.</span>\" % (iconHtmlCode,targetUserLink,flowgramLink) \n        thirdPersonPast = \"%s <span>%s favorited %s's Flowgram named %s.</span>\" % (iconHtmlCode,currentUserLink,targetUserLink,flowgramLink) \n        secondPersonPast = \"%s <span>%s favorited your Flowgram named %s.</span>\" % (iconHtmlCode,currentUserLink,flowgramLink)             \n    \n        # save event obj to UserHistory\n        UserHistory.objects.create(currentUser=currentUser,targetUser=targetUser,eventCode=eventCode,flowgramId=flowgramIdStr,thirdPersonPast=thirdPersonPast,secondPersonPast=secondPersonPast,secondPersonPresent=secondPersonPresent)\n\nexcept:\n    log.debug(\"Attempted to create FG_FAVED UserHistory obj FAILED\")# log error", "path": "django\\flowgram\\core\\controller.py", "repo_name": "reverie/flowgram.com", "stars": 22, "license": "None", "language": "python", "size": 67618}
{"docstring": "\"\"\"Expects a page object with title and source_url already set.\"\"\"\n# TODO(andrew): move functionality into or out of this awkward function?\n\n# put it at the end of the flowgram:\n", "func_signal": "def create_page_to_flowgram(flowgram, page, html, do_make_thumbnail=True, set_position=True):\n", "code": "if set_position:\n    page.position = get_next_position(flowgram)\n\nlog.debug(\"create_page_to_flowgram: \" + str(flowgram))\npage.flowgram = flowgram\npage.save()\n\n# save the file:\nfilename = page_filename(page)\nadd_file_to_flowgram(flowgram, filename, html, PAGEFILE_ENCODING)\n\n# Set the title of the flowgram if this is the first page:\nif flowgram.title == DEFAULT_FG_TITLE:\n    flowgram.title = page.title\n    flowgram.save()  \n            \n\nif do_make_thumbnail:\n    make_thumbnail(page)\nadd_default_time(page)\n\nreturn page", "path": "django\\flowgram\\core\\controller.py", "repo_name": "reverie/flowgram.com", "stars": 22, "license": "None", "language": "python", "size": 67618}
{"docstring": "\"\"\"Overrides the save method so that an ID can be manually generated.\"\"\"\n", "func_signal": "def save(self):\n", "code": "new = not self.id\n\nsuper(Comment, self).save()\n\nif new: \n    # Increment flowgram comment count\n    fg = self.flowgram\n    fg.num_comments += 1\n    fg.save(invalidate_cache=False)\n    import mail\n    mail.announce_new_comment(self)", "path": "django\\flowgram\\core\\models.py", "repo_name": "reverie/flowgram.com", "stars": 22, "license": "None", "language": "python", "size": 67618}
{"docstring": "\"\"\"Gets the specified user's rating for the flowgram.\"\"\"\n", "func_signal": "def get_users_rating(self, user):\n", "code": "if not user.is_authenticated():\n    return -1\nrating = Rating.objects.filter(flowgram=self, user=user)\nif len(rating) > 0:\n    return float(rating[0].value) / 2\nelse:\n    return -1", "path": "django\\flowgram\\core\\models.py", "repo_name": "reverie/flowgram.com", "stars": 22, "license": "None", "language": "python", "size": 67618}
{"docstring": "\"\"\"\nStores Flowgram Commented event in UserHistory\n\"\"\"\n\n", "func_signal": "def store_fgcomented_event(data):\n", "code": "try:\n    # Unpack dict\n    currentUser = data['commentor']\n    eventCode = data['eventCode']\n    flowgramIdStr = data['fg_id']\n    fg = Flowgram.objects.get(pk=flowgramIdStr)# Get Flowgram info from flowgramID\n    \n    # if the fg is private, we need to kill this here\n    if fg.public == True:\n        flowgramTitle = fg.title\n        targetUser = fg.owner\n    \n        # HTML CODE BUILD\n        iconHtmlCode = get_icon_code(eventCode) # img icon\n        currentUserLink = create_user_link(currentUser) # user link\n        targetUserLink = create_user_link(targetUser) # user link\n        flowgramLink = create_fg_details_link(flowgramIdStr, flowgramTitle) # fg link\n    \n        # Determine proper message and add html code sets\n        secondPersonPresent = \"%s <span>You commented on %s's Flowgram named %s.</span>\" % (iconHtmlCode,targetUserLink,flowgramLink) \n        secondPersonPast = \"%s <span>%s commented on your Flowgram named %s.</span>\" % (iconHtmlCode,currentUserLink,flowgramLink)          \n        thirdPersonPast = \"%s <span>%s commented on %s's Flowgram named %s.</span>\" % (iconHtmlCode,currentUserLink,targetUserLink,flowgramLink) \n    \n        # save event obj to UserHistory\n        #UserHistory.objects.create(user=commentor,targetUser=target_user,ThirdPersonMsg=ThirdPerson,ThirdPersonYouMsg=ThirdPersonYou,FirstPersonMsg=FirstPerson,rawData=raw)\n        UserHistory.objects.create(currentUser=currentUser,targetUser=targetUser,eventCode=eventCode,flowgramId=flowgramIdStr,thirdPersonPast=thirdPersonPast,secondPersonPast=secondPersonPast,secondPersonPresent=secondPersonPresent)\nexcept:\n    log.debug(\"Attempted to create FG_COMMENTED event FAILED\")# log error", "path": "django\\flowgram\\core\\controller.py", "repo_name": "reverie/flowgram.com", "stars": 22, "license": "None", "language": "python", "size": 67618}
{"docstring": "\"\"\"The 1-indexed page position that will become the last position in flowgram\"\"\"\n", "func_signal": "def get_next_position(flowgram):\n", "code": "position = 1 + max([p.position for p in Page.objects.filter(flowgram=flowgram)]+[0])\nreturn position", "path": "django\\flowgram\\core\\controller.py", "repo_name": "reverie/flowgram.com", "stars": 22, "license": "None", "language": "python", "size": 67618}
{"docstring": "\"\"\"\nDetermines if the new flowgram view count causes total views to surpass a set increment \nif so, store the event in UserHistory\n\"\"\"\n# increments taken from views/show_user\n", "func_signal": "def check_fgviews_landmark(user, new_count):\n", "code": "if (new_count == 100) or (new_count == 500) or (new_count == 1000)  or (new_count == 5000) or (new_count == 10000):\n\ttry: # UserHistory: BADGE_FGVIEWS\n\t    if new_count >= 10000:\n\t        eventCode = \"BADGE_FGVIEWS_10k\"\n\t    elif new_count >= 5000:\n\t        eventCode = \"BADGE_FGVIEWS_5k\"\n\t    elif new_count >= 1000:\n\t        eventCode = \"BADGE_FGVIEWS_1k\"\n\t    elif new_count >= 500:\n\t        eventCode = \"BADGE_FGVIEWS_500\"\n\t    elif new_count >= 100:\n\t        eventCode = \"BADGE_FGVIEWS_100\"\n\t    else:\n\t        eventCode = \"BADGE_FGVIEWS_generic\"\n\t\n\t    data = {'current_user':user,'fg_views':new_count,'eventCode':eventCode} # Build dict for UserHistory Event\n\t    store_badge_fgviews_event(data)\n\n\t\n\texcept:\n\t\tlog.debug(\"attempted to send BADGE_FGVIEWS event to store_badge_fgviews_event -- FAILED\")# log error", "path": "django\\flowgram\\core\\controller.py", "repo_name": "reverie/flowgram.com", "stars": 22, "license": "None", "language": "python", "size": 67618}
{"docstring": "\"\"\"Determines whether or not the site should be down for maintenance.\"\"\"\n", "func_signal": "def is_down_for_maintenance(request):\n", "code": "return localsettings.fg_DOWN_FOR_MAINTENANCE and \\\n       not request.META.get('REMOTE_ADDR', '') in localsettings.fg_INTERNAL_IPS", "path": "django\\flowgram\\core\\helpers.py", "repo_name": "reverie/flowgram.com", "stars": 22, "license": "None", "language": "python", "size": 67618}
{"docstring": "\"\"\"Returns the average rating, divided by 2, to make it on a 5 point\nscale instead of a 10 point scale.\"\"\"\n", "func_signal": "def get_rating(self):\n", "code": "if self.num_ratings == 0:\n    return 0\nelse:\n    return self.avg_rating", "path": "django\\flowgram\\core\\models.py", "repo_name": "reverie/flowgram.com", "stars": 22, "license": "None", "language": "python", "size": 67618}
{"docstring": "\"\"\"An HTML document that alerts `msg`, then redirects to `next`. Used for quick-add response. \n\"\"\"\n", "func_signal": "def redirect(next, msg=None):\n", "code": "meta_tag = '<meta http-equiv=\"REFRESH\" content=\"0;url=%s\">' % next\nif msg:\n    msg = msg.replace(\"'\", \"\\\\'\")\n    return HttpResponse(\"<html><head>%s</head><body><script>alert('%s');</script></body></html>\" % (meta_tag, msg))\nreturn HttpResponse(\"<html><head>%s</head><body></body></html>\" % (meta_tag))", "path": "django\\flowgram\\core\\helpers.py", "repo_name": "reverie/flowgram.com", "stars": 22, "license": "None", "language": "python", "size": 67618}
{"docstring": "\"\"\"\nInput: a response returned from urllib2.urlopen\nOutput: the HTML as a correctly decoded unicode object\n\"\"\"\n", "func_signal": "def html_from_urlopen(r, use_unicode=True):\n", "code": "html = r.read()\n\n# Try using header info    \nif r.info().has_key('content-type'):\n    ct = r.info().get('content-type')\n    if ct:\n        null, opts = cgi.parse_header(ct)\n        enc = opts.get('charset')\n        if enc:\n            try:\n                return unicode(html, enc)\n            except UnicodeDecodeError:\n                pass\n\n# Try using info from HTML\nmatch = enc_re.search(html)\nif match:\n    enc = match.groups()[0]\n    try:\n        return unicode(html, enc)\n    except UnicodeDecodeError:\n        pass\n            \n# Default            \nreturn unicode(html, \"windows-1252\", errors='replace') if use_unicode else html", "path": "django\\flowgram\\core\\helpers.py", "repo_name": "reverie/flowgram.com", "stars": 22, "license": "None", "language": "python", "size": 67618}
{"docstring": "\"\"\"Returns file contents.\"\"\"\n#\"\"\"Returns open file handle.\"\"\"\n", "func_signal": "def get_file_from_flowgram(flowgram, filename, encoding=None):\n", "code": "dir = flowgram.directory()\npath = dir + filename\nif encoding is None:\n    f = open(path, 'rb')\nelse:\n    f = codecs.open(path, 'rb', encoding)\ncontent = f.read()\nf.close()\nreturn content", "path": "django\\flowgram\\core\\controller.py", "repo_name": "reverie/flowgram.com", "stars": 22, "license": "None", "language": "python", "size": 67618}
{"docstring": "\"\"\"Must be called before you unset page.flowgram\"\"\"\n", "func_signal": "def delete_page_files(page):\n", "code": "flowgram = page.flowgram\n# Move original file\nsource_orig = flowgram.directory() + page_filename(page)\ndest_orig = deleted_pages_dir() + page_filename(page)\ntry:\n    shutil.move(source_orig, dest_orig)\nexcept IOError:\n    pass\n\n# Move highlighted file\nsource_hl = flowgram.directory() + highlighted_page_filename(page)\ndest_hl = deleted_pages_dir() + highlighted_page_filename(page)\ntry: \n    shutil.move(source_hl, dest_hl)\nexcept IOError:\n    pass", "path": "django\\flowgram\\core\\controller.py", "repo_name": "reverie/flowgram.com", "stars": 22, "license": "None", "language": "python", "size": 67618}
{"docstring": "\"\"\"INACTIVE\"\"\"\n\n", "func_signal": "def store_kudos_event(data):\n", "code": "\n\"\"\"\nReturns a full html link to flowgram\n\"\"\"\nfg_link = \"<a href=\\\"%sp/%s\\\">%s</a>\" % (localsettings.my_URL_BASE, fg_id, fg_title)\nreturn fg_link", "path": "django\\flowgram\\core\\controller.py", "repo_name": "reverie/flowgram.com", "stars": 22, "license": "None", "language": "python", "size": 67618}
{"docstring": "\"\"\"\nReturn full icon html img code associated with EventType 'code', if fail, returns default icon \n\"\"\"\n", "func_signal": "def get_icon_code(code):\n", "code": "try:\n\teventType = EventType.objects.get(eventCode__exact=code) \n\ticonPath = \"<img src=\\\"%s%s\\\" />\" % (localsettings.my_URL_BASE, eventType.icon_path)\n\treturn iconPath\nexcept:\n\ticonPath = \"<img src=\\\"%smedia/images/hearts/small_heart_red.png\\\" />\" % localsettings.my_URL_BASE # default icon\n\treturn iconPath", "path": "django\\flowgram\\core\\controller.py", "repo_name": "reverie/flowgram.com", "stars": 22, "license": "None", "language": "python", "size": 67618}
{"docstring": "# not really case in-sensitive, just upper and lower\n", "func_signal": "def ifind(s, sub):\n", "code": "lower = s.find(sub.lower())\nupper = s.find(sub.upper())\nif ((lower < upper) and (lower != -1)) or upper == -1:\n    return lower\nreturn upper", "path": "django\\flowgram\\core\\helpers.py", "repo_name": "reverie/flowgram.com", "stars": 22, "license": "None", "language": "python", "size": 67618}
{"docstring": "\"\"\"\nStores Flowgram Made event in UserHistory\n\"\"\"\n", "func_signal": "def store_fgmade_event(data):\n", "code": "eventCode = data['eventCode']\nflowgramIdStr = data['fg_id']\nfgAction = data['active']\nfg = Flowgram.objects.get(pk=flowgramIdStr) \n\nif fgAction == \"make_active\":\n    flowgramTitle = fg.title\n    currentUser = fg.owner # in this case, current user is the flowgram owner \n    targetUser = fg.owner # there is no target user in this case, populating with current_user as placeholder\n        \n    # Get HTML code\n    iconHtmlCode = get_icon_code(eventCode)\n    currentUserLink = create_user_link(currentUser)\n    flowgramLink = create_fg_details_link(flowgramIdStr, flowgramTitle) # Get fg html link\n    \n    # Determine proper message and add html code sets\n    secondPersonPresent = \"%s <span>Congrats! Your Flowgram named %s was made public.</span>\" % (iconHtmlCode,flowgramLink) \n    thirdPersonPast = \"%s <span>%s's Flowgram named %s was made public.</span>\" % (iconHtmlCode,currentUserLink,flowgramLink) \n    secondPersonPast = \"\" # no case for this msg type\n    historyItem, created = UserHistory.objects.get_or_create(flowgramId=flowgramIdStr, eventCode=eventCode, defaults={'currentUser':currentUser,'targetUser':targetUser,'thirdPersonPast':thirdPersonPast,'secondPersonPast':secondPersonPast,'secondPersonPresent':secondPersonPresent})       \n    if not created:\n        historyItem.eventActive = True\n        historyItem.save()              \nelse: # assumes 'make_inactive' \n    try:\n        historyItem = UserHistory.objects.get(flowgramId=flowgramIdStr, eventCode=eventCode)        \n        # make inactive / else do nothing, no worries \n        historyItem.eventActive = False\n        historyItem.save()  \n    except UserHistory.DoesNotExist:\n        return", "path": "django\\flowgram\\core\\controller.py", "repo_name": "reverie/flowgram.com", "stars": 22, "license": "None", "language": "python", "size": 67618}
{"docstring": "\"\"\" \nStores subscription event in UserHisotry\n\"\"\"\n", "func_signal": "def store_subscription_event(data):\n", "code": "try:\n\t# Unpack dict\n\tcurrentUser = data['user']\n\ttargetUser = data['target_user']\n\teventCode = data['eventCode']\n\t#raw = \"%s\" % (data)\n\tflowgramId = \"none\"\n\t\n\t# HTML CODE BUILD\n\ticonHtmlCode = get_icon_code(eventCode) # Get icon image path + html based on eventCode\n\tcurrentUserLink = create_user_link(currentUser) # returns html link to user\n\ttargetUserLink = create_user_link(targetUser) # returns html link to user\n\t\n\t# Determine proper message and add html code sets\n\tif eventCode == \"SUB\":\n\t\tsecondPersonPresent = \"%s <span>You subscribed to %s.</span>\" % (iconHtmlCode,targetUserLink) \n\t\tthirdPersonPast = \"%s <span>%s subscribed to %s.</span>\" % (iconHtmlCode,currentUserLink,targetUserLink) \n\t\tsecondPersonPast = \"%s %s subscribed to you.\" % (iconHtmlCode,currentUserLink) \n\telse: #assumes eventCode == \"UNSUB\"\n\t\tsecondPersonPresent = \"%s <span>You unsubscribed from %s.</span>\" % (iconHtmlCode,targetUserLink) \n\t\tthirdPersonPast = \"%s <span>%s unsubscribed from %s.</span>\" % (iconHtmlCode,currentUserLink,targetUserLink) \n\t\tsecondPersonPast = \"%s <span>%s unsubscribed from you.</span>\" % (iconHtmlCode,currentUserLink) \n\t\n\t# save event obj to UserHistory\n\tUserHistory.objects.create(currentUser=currentUser,targetUser=targetUser,eventCode=eventCode,flowgramId=flowgramId,thirdPersonPast=thirdPersonPast,secondPersonPast=secondPersonPast,secondPersonPresent=secondPersonPresent)\nexcept:\n\tlog.debug(\"Attempted to create (UN)SUB UserHistory event FAILED\") # log error", "path": "django\\flowgram\\core\\controller.py", "repo_name": "reverie/flowgram.com", "stars": 22, "license": "None", "language": "python", "size": 67618}
{"docstring": "\"\"\"\nStores Badge event in UserHistory\n\"\"\"\n", "func_signal": "def store_badge_fgviews_event(data):\n", "code": "try:\n\t# Unpack dict & Get User / Flowgram info\n\teventCode = data['eventCode']\n\tflowgramIdStr = \"none\"\n\tflowgramViewsCount = data['fg_views']\n\tcurrentUser = data['current_user'] \n\ttargetUser = currentUser\n\t\n\t# Get HTML code\n\ticonHtmlCode = get_icon_code(eventCode)\n\tcurrentUserLink = create_user_link(currentUser)\n\t\n\t# Determine proper message and add html code sets\n\tsecondPersonPresent = \"%s <span>Congrats! Your Flowgrams have reached %s views!</span>\" % (iconHtmlCode,flowgramViewsCount) \n\tthirdPersonPast = \"%s <span>Wow! %s's Flowgrams have reached %s views.</span>\" % (iconHtmlCode,currentUserLink,flowgramViewsCount) \n\tsecondPersonPast = \"\" # no case for this msg type\n\t\n\t# save event obj to UserHistory\n\tUserHistory.objects.create(currentUser=currentUser,targetUser=targetUser,eventCode=eventCode,flowgramId=flowgramIdStr,thirdPersonPast=thirdPersonPast,secondPersonPast=secondPersonPast,secondPersonPresent=secondPersonPresent)\nexcept Exception:\n\tlog.debug(\"Attempted to create BADGE_FGVIEWS UserHistory event FAILED\")# log error", "path": "django\\flowgram\\core\\controller.py", "repo_name": "reverie/flowgram.com", "stars": 22, "license": "None", "language": "python", "size": 67618}
{"docstring": "\"\"\"Handle entity references as data, possibly converting known\nHTML and/or XML entity references to the corresponding Unicode\ncharacters.\"\"\"\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "data = None\nif self.soup.convertHTMLEntities:\n    try:\n        data = unichr(name2codepoint[ref])\n    except KeyError:\n        pass\n\nif not data and self.soup.convertXMLEntities:\n        data = self.soup.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref)\n\nif not data and self.soup.convertHTMLEntities and \\\n    not self.soup.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref):\n        # TODO: We've got a problem here. We're told this is\n        # an entity reference, but it's not an XML entity\n        # reference or an HTML entity reference. Nonetheless,\n        # the logical thing to do is to pass it through as an\n        # unrecognized entity reference.\n        #\n        # Except: when the input is \"&carol;\" this function\n        # will be called with input \"carol\". When the input is\n        # \"AT&T\", this function will be called with input\n        # \"T\". We have no way of knowing whether a semicolon\n        # was present originally, so we don't know whether\n        # this is an unknown entity or just a misplaced\n        # ampersand.\n        #\n        # The more common case is a misplaced ampersand, so I\n        # escape the ampersand and omit the trailing semicolon.\n        data = \"&amp;%s\" % ref\nif not data:\n    # This case is different from the one above, because we\n    # haven't already gone through a supposedly comprehensive\n    # mapping of entities to Unicode characters. We might not\n    # have gone through any mapping at all. So the chances are\n    # very high that this is a real entity, and not a\n    # misplaced ampersand.\n    data = \"&%s;\" % ref\nself.handle_data(data)", "path": "BeautifulSoup.py", "repo_name": "cmaul/MetroTransit-API", "stars": 22, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\"Adds a certain piece of text to the tree as a NavigableString\nsubclass.\"\"\"\n", "func_signal": "def _toStringSubclass(self, text, subclass):\n", "code": "self.soup.endData()\nself.handle_data(text)\nself.soup.endData(subclass)", "path": "BeautifulSoup.py", "repo_name": "cmaul/MetroTransit-API", "stars": 22, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\"Create a new NavigableString.\n\nWhen unpickling a NavigableString, this method is called with\nthe string in DEFAULT_OUTPUT_ENCODING. That encoding needs to be\npassed in to the superclass's __new__ or the superclass won't know\nhow to handle non-ASCII characters.\n\"\"\"\n", "func_signal": "def __new__(cls, value):\n", "code": "if isinstance(value, unicode):\n    return unicode.__new__(cls, value)\nreturn unicode.__new__(cls, value, DEFAULT_OUTPUT_ENCODING)", "path": "BeautifulSoup.py", "repo_name": "cmaul/MetroTransit-API", "stars": 22, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\"Returns true iff this tag has the same name, the same attributes,\nand the same contents (recursively) as the given tag.\n\nNOTE: right now this will return false if two tags have the\nsame attributes in a different order. Should this be fixed?\"\"\"\n", "func_signal": "def __eq__(self, other):\n", "code": "if not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):\n    return False\nfor i in range(0, len(self.contents)):\n    if self.contents[i] != other.contents[i]:\n        return False\nreturn True", "path": "BeautifulSoup.py", "repo_name": "cmaul/MetroTransit-API", "stars": 22, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\"Returns the parents of this Tag that match the given\ncriteria.\"\"\"\n\n", "func_signal": "def findParents(self, name=None, attrs={}, limit=None, **kwargs):\n", "code": "return self._findAll(name, attrs, None, limit, self.parentGenerator,\n                     **kwargs)", "path": "BeautifulSoup.py", "repo_name": "cmaul/MetroTransit-API", "stars": 22, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\"Returns the closest parent of this Tag that matches the given\ncriteria.\"\"\"\n# NOTE: We can't use _findOne because findParents takes a different\n# set of arguments.\n", "func_signal": "def findParent(self, name=None, attrs={}, **kwargs):\n", "code": "r = None\nl = self.findParents(name, attrs, 1)\nif l:\n    r = l[0]\nreturn r", "path": "BeautifulSoup.py", "repo_name": "cmaul/MetroTransit-API", "stars": 22, "license": "None", "language": "python", "size": 159}
{"docstring": "#print \"End tag %s\" % name\n", "func_signal": "def unknown_endtag(self, name):\n", "code": "if self.quoteStack and self.quoteStack[-1] != name:\n    #This is not a real end tag.\n    #print \"</%s> is not real!\" % name\n    self.handle_data('</%s>' % name)\n    return\nself.endData()\nself._popToTag(name)\nif self.quoteStack and self.quoteStack[-1] == name:\n    self.quoteStack.pop()\n    self.literal = (len(self.quoteStack) > 0)", "path": "BeautifulSoup.py", "repo_name": "cmaul/MetroTransit-API", "stars": 22, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\"Returns true iff the given string is the name of a\nself-closing tag according to this parser.\"\"\"\n", "func_signal": "def isSelfClosingTag(self, name):\n", "code": "return self.SELF_CLOSING_TAGS.has_key(name) \\\n       or self.instanceSelfClosingTags.has_key(name)", "path": "BeautifulSoup.py", "repo_name": "cmaul/MetroTransit-API", "stars": 22, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\"Returns either the given Unicode string or its encoding.\"\"\"\n", "func_signal": "def sob(unicode, encoding):\n", "code": "if encoding is None:\n    return unicode\nelse:\n    return unicode.encode(encoding)", "path": "BeautifulSoup.py", "repo_name": "cmaul/MetroTransit-API", "stars": 22, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\"Given a document, tries to detect its XML encoding.\"\"\"\n", "func_signal": "def _detectEncoding(self, xml_data, isHTML=False):\n", "code": "xml_encoding = sniffed_xml_encoding = None\ntry:\n    if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n        # EBCDIC\n        xml_data = self._ebcdic_to_ascii(xml_data)\n    elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n        # UTF-16BE\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') \\\n             and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16BE with BOM\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n        # UTF-16LE\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and \\\n             (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16LE with BOM\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n        # UTF-32BE\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n        # UTF-32LE\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n        # UTF-32BE with BOM\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n        # UTF-32LE with BOM\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n    elif xml_data[:3] == '\\xef\\xbb\\xbf':\n        # UTF-8 with BOM\n        sniffed_xml_encoding = 'utf-8'\n        xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n    else:\n        sniffed_xml_encoding = 'ascii'\n        pass\nexcept:\n    xml_encoding_match = None\nxml_encoding_re = '^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>'.encode()\nxml_encoding_match = re.compile(xml_encoding_re).match(xml_data)\nif not xml_encoding_match and isHTML:\n    meta_re = '<\\s*meta[^>]+charset=([^>]*?)[;\\'\">]'.encode()\n    regexp = re.compile(meta_re, re.I)\n    xml_encoding_match = regexp.search(xml_data)\nif xml_encoding_match is not None:\n    xml_encoding = xml_encoding_match.groups()[0].decode(\n        'ascii').lower()\n    if isHTML:\n        self.declaredHTMLEncoding = xml_encoding\n    if sniffed_xml_encoding and \\\n       (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',\n                         'iso-10646-ucs-4', 'ucs-4', 'csucs4',\n                         'utf-16', 'utf-32', 'utf_16', 'utf_32',\n                         'utf16', 'u16')):\n        xml_encoding = sniffed_xml_encoding\nreturn xml_data, xml_encoding, sniffed_xml_encoding", "path": "BeautifulSoup.py", "repo_name": "cmaul/MetroTransit-API", "stars": 22, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\"Destructively rips this element out of the tree.\"\"\"\n", "func_signal": "def extract(self):\n", "code": "if self.parent:\n    try:\n        self.parent.contents.remove(self)\n    except ValueError:\n        pass\n\n#Find the two elements that would be next to each other if\n#this element (and any children) hadn't been parsed. Connect\n#the two.\nlastChild = self._lastRecursiveChild()\nnextElement = lastChild.next\n\nif self.previous:\n    self.previous.next = nextElement\nif nextElement:\n    nextElement.previous = self.previous\nself.previous = None\nlastChild.next = None\n\nself.parent = None\nif self.previousSibling:\n    self.previousSibling.nextSibling = self.nextSibling\nif self.nextSibling:\n    self.nextSibling.previousSibling = self.previousSibling\nself.previousSibling = self.nextSibling = None\nreturn self", "path": "BeautifulSoup.py", "repo_name": "cmaul/MetroTransit-API", "stars": 22, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\"Convenience method that works with all 2.x versions of Python\nto determine whether or not something is stringlike.\"\"\"\n", "func_signal": "def isString(s):\n", "code": "try:\n    return isinstance(s, unicode) or isinstance(s, basestring)\nexcept NameError:\n    return isinstance(s, str)", "path": "BeautifulSoup.py", "repo_name": "cmaul/MetroTransit-API", "stars": 22, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\"Pops the tag stack up to and including the most recent\ninstance of the given tag. If inclusivePop is false, pops the tag\nstack up to but *not* including the most recent instqance of\nthe given tag.\"\"\"\n#print \"Popping to %s\" % name\n", "func_signal": "def _popToTag(self, name, inclusivePop=True):\n", "code": "if name == self.ROOT_TAG_NAME:\n    return\n\nnumPops = 0\nmostRecentTag = None\nfor i in range(len(self.tagStack)-1, 0, -1):\n    if name == self.tagStack[i].name:\n        numPops = len(self.tagStack)-i\n        break\nif not inclusivePop:\n    numPops = numPops - 1\n\nfor i in range(0, numPops):\n    mostRecentTag = self.popTag()\nreturn mostRecentTag", "path": "BeautifulSoup.py", "repo_name": "cmaul/MetroTransit-API", "stars": 22, "license": "None", "language": "python", "size": 159}
{"docstring": "#print \"Matching %s against %s\" % (markup, matchAgainst)\n", "func_signal": "def _matches(self, markup, matchAgainst):\n", "code": "result = False\nif matchAgainst == True and type(matchAgainst) == types.BooleanType:\n    result = markup != None\nelif callable(matchAgainst):\n    result = matchAgainst(markup)\nelse:\n    #Custom match methods take the tag as an argument, but all\n    #other ways of matching match the tag name as a string.\n    if isinstance(markup, Tag):\n        markup = markup.name\n    if markup is not None and not isString(markup):\n        markup = unicode(markup)\n    #Now we know that chunk is either a string, or None.\n    if hasattr(matchAgainst, 'match'):\n        # It's a regexp object.\n        result = markup and matchAgainst.search(markup)\n    elif (isList(matchAgainst)\n          and (markup is not None or not isString(matchAgainst))):\n        result = markup in matchAgainst\n    elif hasattr(matchAgainst, 'items'):\n        result = markup.has_key(matchAgainst)\n    elif matchAgainst and isString(markup):\n        if isinstance(markup, unicode):\n            matchAgainst = unicode(matchAgainst)\n        else:\n            matchAgainst = str(matchAgainst)\n\n    if not result:\n        result = matchAgainst == markup\nreturn result", "path": "BeautifulSoup.py", "repo_name": "cmaul/MetroTransit-API", "stars": 22, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\"Recursively destroys the contents of this tree.\"\"\"\n", "func_signal": "def decompose(self):\n", "code": "contents = [i for i in self.contents]\nfor i in contents:\n    if isinstance(i, Tag):\n        i.decompose()\n    else:\n        i.extract()\nself.extract()", "path": "BeautifulSoup.py", "repo_name": "cmaul/MetroTransit-API", "stars": 22, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\"Setting tag[key] sets the value of the 'key' attribute for the\ntag.\"\"\"\n", "func_signal": "def __setitem__(self, key, value):\n", "code": "self._getAttrMap()\nself.attrMap[key] = value\nfound = False\nfor i in range(0, len(self.attrs)):\n    if self.attrs[i][0] == key:\n        self.attrs[i] = (key, value)\n        found = True\nif not found:\n    self.attrs.append((key, value))\nself._getAttrMap()[key] = value", "path": "BeautifulSoup.py", "repo_name": "cmaul/MetroTransit-API", "stars": 22, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\"Handle a processing instruction as a ProcessingInstruction\nobject, possibly one with a %SOUP-ENCODING% slot into which an\nencoding will be plugged later.\"\"\"\n", "func_signal": "def handle_pi(self, text):\n", "code": "if text[:3] == \"xml\":\n    text = u\"xml version='1.0' encoding='%SOUP-ENCODING%'\"\nself._toStringSubclass(text, ProcessingInstruction)", "path": "BeautifulSoup.py", "repo_name": "cmaul/MetroTransit-API", "stars": 22, "license": "None", "language": "python", "size": 159}
{"docstring": "#print \"Push\", tag.name\n", "func_signal": "def pushTag(self, tag):\n", "code": "if self.currentTag:\n    self.currentTag.contents.append(tag)\nself.tagStack.append(tag)\nself.currentTag = self.tagStack[-1]", "path": "BeautifulSoup.py", "repo_name": "cmaul/MetroTransit-API", "stars": 22, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\"Beautiful Soup can detect a charset included in a META tag,\ntry to convert the document to that charset, and re-parse the\ndocument from the beginning.\"\"\"\n", "func_signal": "def extractCharsetFromMeta(self, attrs):\n", "code": "httpEquiv = None\ncontentType = None\ncontentTypeIndex = None\ntagNeedsEncodingSubstitution = False\n\nfor i in range(0, len(attrs)):\n    key, value = attrs[i]\n    key = key.lower()\n    if key == 'http-equiv':\n        httpEquiv = value\n    elif key == 'content':\n        contentType = value\n        contentTypeIndex = i\n\nif httpEquiv and contentType: # It's an interesting meta tag.\n    match = self.CHARSET_RE.search(contentType)\n    if match:\n        if (self.declaredHTMLEncoding is not None or\n            self.originalEncoding == self.fromEncoding):\n            # An HTML encoding was sniffed while converting\n            # the document to Unicode, or an HTML encoding was\n            # sniffed during a previous pass through the\n            # document, or an encoding was specified\n            # explicitly and it worked. Rewrite the meta tag.\n            def rewrite(match):\n                return match.group(1) + \"%SOUP-ENCODING%\"\n            newAttr = self.CHARSET_RE.sub(rewrite, contentType)\n            attrs[contentTypeIndex] = (attrs[contentTypeIndex][0],\n                                       newAttr)\n            tagNeedsEncodingSubstitution = True\n        else:\n            # This is our first pass through the document.\n            # Go through it again with the encoding information.\n            newCharset = match.group(3)\n            if newCharset and newCharset != self.originalEncoding:\n                self.declaredHTMLEncoding = newCharset\n                self._feed(self.declaredHTMLEncoding)\n                raise StopParsing\n            pass\ntag = self.unknown_starttag(\"meta\", attrs)\nif tag and tagNeedsEncodingSubstitution:\n    tag.containsSubstitutions = True", "path": "BeautifulSoup.py", "repo_name": "cmaul/MetroTransit-API", "stars": 22, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\"Returns the closest sibling to this Tag that matches the\ngiven criteria and appears after this Tag in the document.\"\"\"\n", "func_signal": "def findNextSibling(self, name=None, attrs={}, text=None, **kwargs):\n", "code": "return self._findOne(self.findNextSiblings, name, attrs, text,\n                     **kwargs)", "path": "BeautifulSoup.py", "repo_name": "cmaul/MetroTransit-API", "stars": 22, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\"Checks that a specific keychain exists\n\nRationalises keychain strings as to whether they end with .keychain\nand looks the keychain them up in the dictionary of keychains created\nat instantiation. Returns a tuple containing True if successful and\nFalse + error message if keychain is not found\n\n\"\"\"\n", "func_signal": "def check_keychain_exists(self, keychain):\n", "code": "keychain_name = self.normalise_keychain_name(keychain)\nkeychain_list = self.list_keychains()\n\nif keychain_name in keychain_list:\n    return True\nelse:\n    raise KeychainException(\n        \"%s.keychain  doesn't exist\" % keychain\n    )", "path": "keychain.py", "repo_name": "spjwebster/keychain.py", "stars": 18, "license": "None", "language": "python", "size": 79}
{"docstring": "\"\"\"Returns account information from specified keychain item \"\"\"\n\n", "func_signal": "def get_generic_password(self, keychain, account, servicename=None):\n", "code": "if self.check_keychain_exists(keychain):\n    account = account and '-a %s' % (account,) or ''\n    servicename = servicename and '-s %s' % (servicename,) or ''\n    result = commands.getstatusoutput(\n        \"security find-generic-password -g %s %s %s.keychain\" % \\\n            (account, servicename, keychain)\n    )\n    if result[0]:\n        return False, 'The specified item could not be found'\n    else:\n        account = RXACCOUNT.search(result[1])\n        password = RXPASS.search(result[1])\n        service = RXSERVICE.search(result[1])\n        \n        if account and password:\n            data = {\n                \"account\":account.group(1),\n                \"password\":password.group(1),\n            }\n            if service:\n                data.update({\"service\": service.group(1)})\n            return data\n        else:\n            return False, 'The specified item could not be found'", "path": "keychain.py", "repo_name": "spjwebster/keychain.py", "stars": 18, "license": "None", "language": "python", "size": 79}
{"docstring": "\"\"\"test keychain name when already ending with .keychain\"\"\"\n", "func_signal": "def test_keychain_name_validation2(self):\n", "code": "keychain_name = self.keychain.normalise_keychain_name(\"test.keychain\")\nself.assert_equal(keychain_name, \"test\")", "path": "tests.py", "repo_name": "spjwebster/keychain.py", "stars": 18, "license": "None", "language": "python", "size": 79}
{"docstring": "\"\"\"Deletes a Keychain\n\nReturns True on success and False if it fails\n\n\"\"\"\n\n", "func_signal": "def delete_keychain(self, keychain):\n", "code": "if self.check_keychain_exists(keychain):\n    result = commands.getstatusoutput(\n        \"security delete-keychain %s.keychain\" % keychain\n    )\n\n    if result[0]:\n        return False, 'Keychain deletion failed'\n    elif result[0] is 0:\n        return True, 'Keychain deleted successfully'", "path": "keychain.py", "repo_name": "spjwebster/keychain.py", "stars": 18, "license": "None", "language": "python", "size": 79}
{"docstring": "\"\"\"test listing generic passwords\"\"\"\n", "func_signal": "def test_list_generic_passwords(self):\n", "code": "self.keychain.create_keychain(self.test_keychain_name, 'testpass')\nself.keychain.set_generic_password(\n    self.test_keychain_name, \n    'test-account',\n    'test-password',\n    servicename=\"awesome1\",\n)\nself.keychain.set_generic_password(\n    self.test_keychain_name, \n    'test-account2',\n    'test-password2',\n)\npass_list = self.keychain.list_keychain_accounts(\n    self.test_keychain_name\n)\nself.assert_equal(pass_list, [\n    {\n        'account': 'test-account', \n        'password': 'test-password', \n        'service': 'awesome1'\n    }, \n    {\n        'account': 'test-account2', \n        'password': 'test-password2', \n    }\n])", "path": "tests.py", "repo_name": "spjwebster/keychain.py", "stars": 18, "license": "None", "language": "python", "size": 79}
{"docstring": "\"\"\"test removing generic password\"\"\"\n\n", "func_signal": "def test_removing_generic_password(self):\n", "code": "self.keychain.create_keychain(self.test_keychain_name, 'testpass')\nself.keychain.set_generic_password(\n    self.test_keychain_name, \n    'test-account',\n    'test-password',\n    servicename=\"awesome1\",\n)\nself.keychain.set_generic_password(\n    self.test_keychain_name, \n    'test-account2',\n    'test-password2',\n)\nself.keychain.remove_generic_password(\n    self.test_keychain_name, \n    \"testpass\", \"test-account2\"\n)\nresult = self.keychain.list_keychain_accounts(self.test_keychain_name)\nself.assert_equal(result, [\n    {\n        'account': 'test-account', \n        'password': 'test-password', \n        'service': 'awesome1'\n    }\n])", "path": "tests.py", "repo_name": "spjwebster/keychain.py", "stars": 18, "license": "None", "language": "python", "size": 79}
{"docstring": "\"\"\"test displaying the default keychain info\"\"\"\n", "func_signal": "def test_show_keychain_info(self):\n", "code": "self.keychain.create_keychain(self.test_keychain_name, 'testpass')\nresult = self.keychain.show_keychain_info(self.test_keychain_name,)\nself.assert_equal(int(result['timeout']), 300)\nself.assert_equal(result['lock-on-sleep'], True)", "path": "tests.py", "repo_name": "spjwebster/keychain.py", "stars": 18, "license": "None", "language": "python", "size": 79}
{"docstring": "\"\"\"Unlocks a locked Keychain\"\"\"\n\n", "func_signal": "def unlock_keychain(self, keychain, password=None):\n", "code": "if self.check_keychain_exists(keychain):\n    if not password:\n        from getpass import getpass\n        password = getpass('Password:')\n    result = commands.getstatusoutput(\"security unlock-keychain -p %s \"\\\n        \"%s.keychain\" % (password, keychain,))\n\n    if result[0]:\n        return False, 'Keychain could not be unlocked'\n    elif result[0] is 0:\n        return True, 'Keychain unlocked successfully'", "path": "keychain.py", "repo_name": "spjwebster/keychain.py", "stars": 18, "license": "None", "language": "python", "size": 79}
{"docstring": "\"\"\"Returns a dictionary containing the keychain settings\"\"\"\n\n", "func_signal": "def show_keychain_info(self, keychain):\n", "code": "if self.check_keychain_exists(keychain):\n\n    result = commands.getstatusoutput(\n        \"security show-keychain-info %s.keychain\" % (keychain,)\n    )\n\n    if result[0]:\n        return False, 'Keychain could not be found'\n    elif result[0] is 0:\n        info = {}\n        info['keychain'] = keychain\n\n        if result[1].find('lock-on-sleep') > -1:\n            info['lock-on-sleep'] = True\n\n        if result[1].find('no-timeout') > -1:\n            info['timeout'] = 0\n        else:\n            timeout_rx = re.compile(r'timeout=(\\d+)s', re.S)\n            match = timeout_rx.search(result[1])\n            info['timeout'] = match.group(1)\n        return info", "path": "keychain.py", "repo_name": "spjwebster/keychain.py", "stars": 18, "license": "None", "language": "python", "size": 79}
{"docstring": "\"\"\"Testing that keychain gets successfully deleted\"\"\"\n\n", "func_signal": "def test_keychain_deletion(self):\n", "code": "self.keychain.create_keychain(self.test_keychain_name, 'testpass')\ndeletion = self.keychain.delete_keychain(self.test_keychain_name)\nself.assert_equal(deletion, (True, 'Keychain deleted successfully'))", "path": "tests.py", "repo_name": "spjwebster/keychain.py", "stars": 18, "license": "None", "language": "python", "size": 79}
{"docstring": "\"\"\"test keychain name\"\"\"\n", "func_signal": "def test_keychain_name_validation(self):\n", "code": "keychain_name = self.keychain.normalise_keychain_name(\"test\")\nself.assert_equal(keychain_name, \"test\")", "path": "tests.py", "repo_name": "spjwebster/keychain.py", "stars": 18, "license": "None", "language": "python", "size": 79}
{"docstring": "\"\"\"Creates a Keychain\n\nreturns True on success and False if it fails\n\n\"\"\"\n\n", "func_signal": "def create_keychain(self, keychain, password=None):\n", "code": "if not password:\n    from getpass import getpass\n    password = getpass('Password:')\n\nkeychain = self.normalise_keychain_name(keychain)\nresult = commands.getstatusoutput(\n    \"security create-keychain -p %s %s.keychain\" % (password, keychain,)\n)\nif result[0] == 12288:\n    return False, 'A Keychain already exists with this name'\nelif result[0]:\n    return False, 'Keychain creation failed'\nelif result[0] is 0:\n    return True, 'Keychain created successfully'", "path": "keychain.py", "repo_name": "spjwebster/keychain.py", "stars": 18, "license": "None", "language": "python", "size": 79}
{"docstring": "\"\"\" Drop a generic password.\n\nThis is carried out by recreating the old keychain minus the\nspecified account.\n\n\"\"\"\n\n", "func_signal": "def remove_generic_password(self, keychain, keychainpassword, account):\n", "code": "if self.check_keychain_exists(keychain):\n\n    if not keychainpassword:\n        raise KeychainException(\"Password must be supplied\")\n\n    old_keychain = self.show_keychain_info(keychain)\n    old_accounts = self.list_keychain_accounts(keychain)\n\n    if self.delete_keychain(keychain)[0] is False:\n        return False, 'Keychain could not be modified'\n\n    self.create_keychain(keychain, keychainpassword)\n    self.set_keychain_settings(\n        keychain,\n        old_keychain[\"lock-on-sleep\"],\n        old_keychain[\"timeout\"]\n    )\n\n    for old_account in old_accounts:\n        if old_account.get(\"account\", None) and \\\n            old_account[\"account\"] != account:\n\n            servicename = old_account.get('service', None)\n            self.set_generic_password(\n                keychain,\n                old_account[\"account\"],\n                old_account[\"password\"],\n                servicename=servicename,\n            )\n\n    return (True, \"Generic Password removed Successfully\")", "path": "keychain.py", "repo_name": "spjwebster/keychain.py", "stars": 18, "license": "None", "language": "python", "size": 79}
{"docstring": "\"\"\"Allows setting the keychain configuration.\n\nIf lock is True the keychain will be locked on sleep. If the timeout is\nset to anything other than 0 the keychain will be set to lock after\ntimeout seconds of inactivity\n\n\"\"\"\n\n", "func_signal": "def set_keychain_settings(self, keychain, lock=True, timeout=0):\n", "code": "if self.check_keychain_exists(keychain):\n    lock = lock and \"-l \" or \"\"\n    timeout = timeout and '-u -t %s' % (timeout,) or ''\n    result = commands.getstatusoutput(\n        \"security set-keychain-settings %s %s %s.keychain\" % \\\n            (lock, timeout, keychain,)\n    )\n\n    if result[0]:\n        return False, 'Keychain settings failed'\n    elif result[0] is 0:\n        return True, 'Keychain updated successfully'", "path": "keychain.py", "repo_name": "spjwebster/keychain.py", "stars": 18, "license": "None", "language": "python", "size": 79}
{"docstring": "\"\"\" Returns account + password list from specified keychain \"\"\"\n\n", "func_signal": "def list_keychain_accounts(self, keychain):\n", "code": "if self.check_keychain_exists(keychain):\n    result = commands.getstatusoutput(\n        \"security dump-keychain -d %s.keychain\" % (keychain)\n    )\n    data = []\n    for kc in result[1].split(\"keychain:\"):\n        account = RXACCOUNT.search(kc)\n        password = RXDATA.search(kc)\n        service = RXSERVICE.search(kc)\n        if account and password:\n            data.append({\n                \"account\":account.group(1),\n                \"password\":password.group(1)\n            })\n        if service:\n            data[-1].update({\"service\": service.group(1)})\n\n    return data", "path": "keychain.py", "repo_name": "spjwebster/keychain.py", "stars": 18, "license": "None", "language": "python", "size": 79}
{"docstring": "\"\"\"Set's the message on instantiation\"\"\"\n", "func_signal": "def __init__(self, value=\"\"):\n", "code": "self.message_prefix = \"Keychain Error:\"\nself.message = \"%s %s\" % (self.message_prefix, value)", "path": "keychain.py", "repo_name": "spjwebster/keychain.py", "stars": 18, "license": "None", "language": "python", "size": 79}
{"docstring": "\"\"\"test changing the default keychain info\"\"\"\n", "func_signal": "def test_changing_keychain_info(self):\n", "code": "self.keychain.create_keychain(self.test_keychain_name, 'testpass')\nself.keychain.set_keychain_settings(\n    self.test_keychain_name, False, timeout=100\n)\nresult = self.keychain.show_keychain_info(self.test_keychain_name,)\nself.assert_equal(int(result['timeout']), 100)\nself.assert_equal(result.get('lock-on-sleep', None), None)", "path": "tests.py", "repo_name": "spjwebster/keychain.py", "stars": 18, "license": "None", "language": "python", "size": 79}
{"docstring": "\"\"\"Wraps assertEqual for convenience\"\"\"\n", "func_signal": "def assert_equal(self, result, expected):\n", "code": "return self.assertEqual(\n    result, \n    expected,\n    \"%s should be %s\" % (result, expected,),\n)", "path": "tests.py", "repo_name": "spjwebster/keychain.py", "stars": 18, "license": "None", "language": "python", "size": 79}
{"docstring": "\"\"\"Deletes the test keychain\"\"\"\n", "func_signal": "def tearDown(self):\n", "code": "try:\n    self.keychain.delete_keychain(self.test_keychain_name)\nexcept KeychainException:\n    pass\nself.keychain = None", "path": "tests.py", "repo_name": "spjwebster/keychain.py", "stars": 18, "license": "None", "language": "python", "size": 79}
{"docstring": "\"\"\"Locks an un-locked Keychain\"\"\"\n\n", "func_signal": "def lock_keychain(self, keychain):\n", "code": "if self.check_keychain_exists(keychain):\n    result = commands.getstatusoutput(\n        \"security lock-keychain %s.keychain\" % (keychain,)\n    )\n    if result[0]:\n        return False, 'Keychain: %s could not be locked' % keychain\n    elif result[0] is 0:\n        return True, 'Keychain: %s locked successfully' % keychain", "path": "keychain.py", "repo_name": "spjwebster/keychain.py", "stars": 18, "license": "None", "language": "python", "size": 79}
{"docstring": "'''Return string, the table as a LaTeX tabular environment.\nNote: will equire the booktabs package.'''\n#fetch the text format, override with fmt_dict\n", "func_signal": "def as_latex_tabular(self, **fmt_dict):\n", "code": "fmt = self._get_fmt('latex', **fmt_dict)\naligns = self[-1].get_aligns('latex', **fmt)\nformatted_rows = [ r'\\begin{tabular}{%s}' % aligns ]\n\ntable_dec_above = fmt['table_dec_above']\nif table_dec_above:\n\tformatted_rows.append(table_dec_above)\n\nformatted_rows.extend(\n\trow.as_string(output_format='latex', **fmt) for row in self )\n\ntable_dec_below = fmt['table_dec_below']\nif table_dec_below:\n\tformatted_rows.append(table_dec_below)\n\nformatted_rows.append(r'\\end{tabular}')\n#tabular does not support caption, but make it available for figure environment\nif self.title:\n\ttitle = r'%%\\caption{%s}' % self.title\n\tformatted_rows.append(title)\nreturn '\\n'.join(formatted_rows)\n\"\"\"\nif fmt_dict['strip_backslash']:\n\tltx_stubs = [stub.replace('\\\\',r'$\\backslash$') for stub in self.stubs]\n\tltx_headers = [header.replace('\\\\',r'$\\backslash$') for header in self.headers]\n\tltx_headers = self.format_headers(fmt_dict, ltx_headers)\nelse:\n\tltx_headers = self.format_headers(fmt_dict)\nltx_stubs = self.format_stubs(fmt_dict, ltx_stubs)\n\"\"\"", "path": "table.py", "repo_name": "rncarpio/bellman", "stars": 18, "license": "None", "language": "python", "size": 244}
{"docstring": "\"\"\"Return string: the formatted row.\nThis is the default formatter for rows.\nOverride this to get different formatting.\nA row formatter must accept as arguments\na row (self) and an output format,\none of ('html', 'txt', 'csv', 'latex').\n\"\"\"\n", "func_signal": "def as_string(self, output_format='txt', **fmt_dict):\n", "code": "fmt = self._get_fmt(output_format, **fmt_dict)\n\n#get column widths\ntry:\n\tcolwidths = self.table.get_colwidths(output_format, **fmt)\nexcept AttributeError:\n\tcolwidths = fmt.get('colwidths')\nif colwidths is None:\n\tcolwidths = (0,) * len(self)\n\ncolsep = fmt['colsep']\nrow_pre = fmt.get('row_pre','')\nrow_post = fmt.get('row_post','')\nformatted_cells = []\nfor cell, width in zip(self, colwidths):\n\tcontent = cell.format(width, output_format=output_format, **fmt)\n\tformatted_cells.append(content)\nformatted_row = row_pre + colsep.join(formatted_cells) + row_post\nformatted_row = self._decorate_below(formatted_row, output_format, **fmt)\nreturn formatted_row", "path": "table.py", "repo_name": "rncarpio/bellman", "stars": 18, "license": "None", "language": "python", "size": 244}
{"docstring": "\"\"\"Return string, sequence of column alignments.\nEnsure comformable data_aligns in `fmt_dict`.\"\"\"\n", "func_signal": "def get_aligns(self, output_format, **fmt_dict):\n", "code": "fmt = self._get_fmt(output_format, **fmt_dict)\nreturn ''.join( cell.alignment(output_format, **fmt) for cell in self )", "path": "table.py", "repo_name": "rncarpio/bellman", "stars": 18, "license": "None", "language": "python", "size": 244}
{"docstring": "\"\"\"Return dict, the formatting options.\n\"\"\"\n", "func_signal": "def _get_fmt(self, output_format, **fmt_dict):\n", "code": "output_format = get_output_format(output_format)\n#first get the default formatting\ntry:\n\tfmt = self.output_formats[output_format].copy()\nexcept KeyError:\n\traise ValueError('Unknown format: %s' % output_format)\n#then, add formatting specific to this call\nfmt.update(fmt_dict)\nreturn fmt", "path": "table.py", "repo_name": "rncarpio/bellman", "stars": 18, "license": "None", "language": "python", "size": 244}
{"docstring": "\"\"\"Return string.\nThis is the default formatter for HTML tables.\nAn HTML table formatter must accept as arguments\na table and a format dictionary.\n\"\"\"\n#fetch the text format, override with fmt_dict\n", "func_signal": "def as_html(self, **fmt_dict):\n", "code": "fmt = self._get_fmt('html', **fmt_dict)\nformatted_rows = ['<table class=\"simpletable\">']\nif self.title:\n\ttitle = '<caption>%s</caption>' % self.title\n\tformatted_rows.append(title)\nformatted_rows.extend( row.as_string('html', **fmt) for row in self )\nformatted_rows.append('</table>')\nreturn '\\n'.join(formatted_rows)", "path": "table.py", "repo_name": "rncarpio/bellman", "stars": 18, "license": "None", "language": "python", "size": 244}
{"docstring": "\"\"\"Return SimpleTable instance,\ncreated from the data in `csvfile`,\nwhich is in comma separated values format.\nThe first row may contain headers: set headers=True.\nThe first column may contain stubs: set stubs=True.\nCan also supply headers and stubs as tuples of strings.\n\"\"\"\n", "func_signal": "def csv2st(csvfile, headers=False, stubs=False, title=None):\n", "code": "rows = list()\nwith open(csvfile,'r') as fh:\n\treader = csv.reader(fh)\n\tif headers is True:\n\t\ttry:\n\t\t\theaders = next(reader)\n\t\texcept AttributeError: #must be Python 2.5 or earlier\n\t\t\theaders = reader.next()\n\telif headers is False:\n\t\theaders=()\n\tif stubs is True:\n\t\tstubs = list()\n\t\tfor row in reader:\n\t\t\tif row:\n\t\t\t\tstubs.append(row[0])\n\t\t\t\trows.append(row[1:])\n\telse: #no stubs, or stubs provided\n\t\tfor row in reader:\n\t\t\tif row:\n\t\t\t\trows.append(row)\n\tif stubs is False:\n\t\tstubs = ()\nnrows = len(rows)\nncols = len(rows[0])\nif any(len(row)!=ncols for row in rows):\n\traise IOError('All rows of CSV file must have same length.')\nreturn SimpleTable(data=rows, headers=headers, stubs=stubs)", "path": "table.py", "repo_name": "rncarpio/bellman", "stars": 18, "license": "None", "language": "python", "size": 244}
{"docstring": "\"\"\"This really only makes sense for the text and latex output formats.\"\"\"\n", "func_signal": "def _decorate_below(self, row_as_string, output_format, **fmt_dict):\n", "code": "dec_below = fmt_dict.get(self.dec_below, None)\nif dec_below is None:\n\tresult = row_as_string\nelse:\n\toutput_format = get_output_format(output_format)\n\tif output_format == 'txt':\n\t\trow0len = len(row_as_string)\n\t\tdec_len = len (dec_below)\n\t\trepeat, addon = divmod(row0len, dec_len)\n\t\tresult = row_as_string + \"\\n\" + (dec_below * repeat + dec_below[:addon])\n\telif output_format == 'latex':\n\t\tresult = row_as_string + \"\\n\" + dec_below\n\telse:\n\t\traise ValueError(\"I can't decorate a %s header.\"%output_format)\nreturn result", "path": "table.py", "repo_name": "rncarpio/bellman", "stars": 18, "license": "None", "language": "python", "size": 244}
{"docstring": "# sometimes fminbound doesn't catch the boundaries.\n", "func_signal": "def maximizer1(h, a, b):\n", "code": "result = float(fminbound(lambda x: -h(x), a, b))\nif (h(a) > h(result) and h(a) > h(b)):\n\treturn a\nif (h(b) > h(result) and h(b) > h(a)):\n\treturn b\nreturn result", "path": "gamblers_ruin2.py", "repo_name": "rncarpio/bellman", "stars": 18, "license": "None", "language": "python", "size": 244}
{"docstring": "\"\"\"Return None.  Insert a row of headers,\nwhere ``headers`` is a sequence of strings.\n(The strings may contain newlines, to indicated multiline headers.)\n\"\"\"\n", "func_signal": "def insert_header_row(self, rownum, headers, dec_below='header_dec_below'):\n", "code": "header_rows = [header.split('\\n') for header in headers]\n#rows in reverse order\nrows = list(izip_longest(*header_rows, fillvalue=''))\nrows.reverse()\nfor i, row in enumerate(rows):\n\tself.insert(rownum, row, datatype='header')\n\tif i == 0:\n\t\tself[rownum].dec_below = dec_below\n\telse:\n\t\tself[rownum].dec_below = None", "path": "table.py", "repo_name": "rncarpio/bellman", "stars": 18, "license": "None", "language": "python", "size": 244}
{"docstring": "\"\"\"Return None.  Inserts a stub cell\nin the row at `loc`.\n\"\"\"\n", "func_signal": "def insert_stub(self, loc, stub):\n", "code": "_Cell = self._Cell\nif not isinstance(stub, _Cell):\n\tstub = stub \n\tstub = _Cell(stub, datatype='stub', row=self)\nself.insert(loc, stub)", "path": "table.py", "repo_name": "rncarpio/bellman", "stars": 18, "license": "None", "language": "python", "size": 244}
{"docstring": "# all consumption, no savings -> use U'(c)\n", "func_signal": "def expUtilPrime(w, wPrime, M, opt_s, env_interp, envprime_interp):\n", "code": "if (opt_s == 0):\n\treturn Uprime(M)\n# all savings, no consumption -> use  beta (P(s+z>0) E[w'(s,z)] + dP E[w]]", "path": "gamblers_ruin2.py", "repo_name": "rncarpio/bellman", "stars": 18, "license": "None", "language": "python", "size": 244}
{"docstring": "# this is a hack, should find a better way.\n", "func_signal": "def __call__(self, z):\n", "code": "arg = z\nif (not isinstance(z, types.FloatType)):\n  arg = z[0]\nreturn _myfuncs.interp1d(self.m_grid1, self.m_FArray, arg)", "path": "lininterp2.py", "repo_name": "rncarpio/bellman", "stars": 18, "license": "None", "language": "python", "size": 244}
{"docstring": "\"\"\"Return dict, the formatting options.\n\"\"\"\n", "func_signal": "def _get_fmt(self, output_format, **fmt_dict):\n", "code": "output_format = get_output_format(output_format)\n#first get the default formatting\ntry:\n\tfmt = default_fmts[output_format].copy()\nexcept KeyError:\n\traise ValueError('Unknown format: %s' % output_format)\n#second get table specific formatting (if possible)\ntry:\n\tfmt.update(self.table.output_formats[output_format])\nexcept AttributeError:\n\tpass\n#finally, add formatting for this row and this call\nfmt.update(self._fmt)\nfmt.update(fmt_dict)\nspecial_fmt = self.special_fmts.get(output_format, None)\nif special_fmt is not None:\n\tfmt.update(special_fmt)\nreturn fmt", "path": "table.py", "repo_name": "rncarpio/bellman", "stars": 18, "license": "None", "language": "python", "size": 244}
{"docstring": "\"\"\"\nReturn None. Adds row-instance specific formatting\nfor the specified output format.\nExample: myrow.add_format('txt', row_dec_below='+-') \n\"\"\"\n", "func_signal": "def add_format(self, output_format, **fmt_dict):\n", "code": "output_format = get_output_format(output_format)\nif output_format not in self.special_fmts:\n\tself.special_fmts[output_format] = dict()\nself.special_fmts[output_format].update(fmt_dict)", "path": "table.py", "repo_name": "rncarpio/bellman", "stars": 18, "license": "None", "language": "python", "size": 244}
{"docstring": "\"\"\"Return list of Row,\nthe raw data as rows of cells.\n\"\"\"\n", "func_signal": "def _data2rows(self, raw_data):\n", "code": "logging.debug('Enter SimpleTable.data2rows.')\n_Cell = self._Cell\n_Row = self._Row\nrows = []\nfor datarow in raw_data:\n\tdtypes = cycle(self._datatypes)\n\tnewrow = _Row(datarow, datatype='data', table=self, celltype=_Cell)\n\tfor cell in newrow:\n\t\ttry:\n\t\t\tcell.datatype = next(dtypes)\n\t\texcept AttributeError: #Python 2.5 or earlier\n\t\t\tcell.datatype = dtypes.next()\n\t\tcell.row = newrow  #a cell knows its row\n\trows.append(newrow)\nlogging.debug('Exit SimpleTable.data2rows.')\nreturn rows", "path": "table.py", "repo_name": "rncarpio/bellman", "stars": 18, "license": "None", "language": "python", "size": 244}
{"docstring": "\"\"\"Return dict, the formatting options.\n\"\"\"\n", "func_signal": "def _get_fmt(self, output_format, **fmt_dict):\n", "code": "output_format = get_output_format(output_format)\n#first get the default formatting\ntry:\n\tfmt = default_fmts[output_format].copy()\nexcept KeyError:\n\traise ValueError('Unknown format: %s' % output_format)\n#then get any table specific formtting\ntry:\n\tfmt.update(self.row.table.output_formats[output_format])\nexcept AttributeError:\n\tpass\n#then get any row specific formtting\ntry:\n\tfmt.update(self.row._fmt)\nexcept AttributeError:\n\tpass\n#finally add formatting for this instance and call\nfmt.update(self._fmt)\nfmt.update(fmt_dict)\nreturn fmt", "path": "table.py", "repo_name": "rncarpio/bellman", "stars": 18, "license": "None", "language": "python", "size": 244}
{"docstring": "\"\"\"Return None.  Adds headers and stubs to table,\nif these were provided at initialization.\nParameters\n----------\nheaders : list of strings\n\tK strings, where K is number of columns\nstubs : list of strings\n\tR strings, where R is number of non-header rows\n\n:note: a header row does not receive a stub!\n\"\"\"\n", "func_signal": "def _add_headers_stubs(self, headers, stubs):\n", "code": "if headers:\n\tself.insert_header_row(0, headers, dec_below='header_dec_below')\nif stubs:\n\tself.insert_stubs(0, stubs)", "path": "table.py", "repo_name": "rncarpio/bellman", "stars": 18, "license": "None", "language": "python", "size": 244}
{"docstring": "'''\ninterpolate a cubic 3D grid defined by x,y,z,cd at the point\n(xi,yi,zi)\n'''\n\n# assume regular grid.\n# returns the index of the _cell_ (between grid points!)\n# return -1 or len(vector)-2 if outside grid\n", "func_signal": "def interp3d(self, xv,yv,zv,cd,xi,yi,zi):\n", "code": "def get_cell_index(value,vector):\n\tdx = vector[1] - vector[0]\n\tif (value < vector[0]):\n\t\treturn -1\n\tif (value >= vector[-1]):\n\t\treturn len(vector)-2\n\treturn int((value - vector[0]) / dx)\n\ndef force_to_grid(x, vector):\n\tif (x < vector[0]):\n\t\treturn vector[0]\n\tif (x > vector[-1]):\n\t\treturn vector[-1]\n\treturn x\n\t\n#xv = x[:,0,0]\n#yv = y[0,:,0]\n#zv = z[0,0,:]\n\na = force_to_grid(xi, xv)\nb = force_to_grid(yi, yv)\nc = force_to_grid(zi, zv)\n\ni = get_cell_index(a,xv)\nj = get_cell_index(b,yv)\nk = get_cell_index(c,zv)\n\n#x1 = x[i,j,k]\n#x2 = x[i+1,j,k]\nx1 = xv[i]\nx2 = xv[i+1]\n#y1 = y[i,j,k]\n#y2 = y[i,j+1,k]\ny1 = yv[j]\ny2 = yv[j+1]\t\n#z1 = z[i,j,k]\n#z2 = z[i,j,k+1]\nz1 = zv[k]\nz2 = zv[k+1]\n\n\nu1 = cd[i, j, k]\nu2 = cd[i+1, j, k]\nu3 = cd[i, j+1, k]\nu4 = cd[i+1, j+1, k]\nu5 = cd[i, j, k+1]\nu6 = cd[i+1, j, k+1]\nu7 = cd[i, j+1, k+1]\nu8 = cd[i+1, j+1, k+1]\n\nw1 = u2 + (u2-u1)/(x2-x1)*(a-x2)\nw2 = u4 + (u4-u3)/(x2-x1)*(a-x2)\nw3 = w2 + (w2-w1)/(y2-y1)*(b-y2)\nw4 = u5 + (u6-u5)/(x2-x1)*(a-x1)\nw5 = u7 + (u8-u7)/(x2-x1)*(a-x1)\nw6 = w4 + (w5-w4)/(y2-y1)*(b-y1)\nw7 = w3 + (w6-w3)/(z2-z1)*(c-z1)\nu = w7\n\nreturn u", "path": "lininterp.py", "repo_name": "rncarpio/bellman", "stars": 18, "license": "None", "language": "python", "size": 244}
{"docstring": "\"\"\"Return string, the table in CSV format.\nCurrently only supports comma separator.\"\"\"\n#fetch the format, which may just be default_csv_format\n", "func_signal": "def as_csv(self, **fmt_dict):\n", "code": "fmt = self._get_fmt('csv', **fmt_dict)\nreturn self.as_text(**fmt)", "path": "table.py", "repo_name": "rncarpio/bellman", "stars": 18, "license": "None", "language": "python", "size": 244}
{"docstring": "\"\"\"Return None.\nExtend each row of `self` with corresponding row of `table`.\nDoes **not** import formatting from ``table``.\nThis generally makes sense only if the two tables have\nthe same number of rows, but that is not enforced.\n:note: To extend append a table below, just use `extend`,\nwhich is the ordinary list method.  This generally makes sense\nonly if the two tables have the same number of columns,\nbut that is not enforced.\n\"\"\"\n", "func_signal": "def extend_right(self, table):\n", "code": "for row1, row2 in zip(self, table):\n\trow1.extend(row2)", "path": "table.py", "repo_name": "rncarpio/bellman", "stars": 18, "license": "None", "language": "python", "size": 244}
{"docstring": "\"\"\"\nA private method meant to be overridden by subclasses, which returns\na (lat,lon) indicating the estimated position of the vehicle at the\ngiven time, provided that time lives between pt0 and pt1.\npt0 and pt1 are each (lat,lon,time) tuples.\n\nThe implementation below is a simple linear interpolation.\n\"\"\"\n", "func_signal": "def _interpolate(self,pt0,pt1,time):\n", "code": "(lat0,lon0,time0) = pt0\n(lat1,lon1,time1) = pt1\n# same time different place?\nif time1 == time0:\n  return (lat1+lat0)/2,(lon1+lon0)/2\nratio = float(time-time0)/(time1-time0)\ndlat,dlon = (lat1-lat0),(lon1-lon0)\nreturn (lat0 + dlat*ratio, lon0 + dlon*ratio)", "path": "core\\src\\BusTrack.py", "repo_name": "cbick/gps2gtfs", "stars": 19, "license": "mit", "language": "python", "size": 924}
{"docstring": "\"\"\"\nGiven a latitude (in degrees), returns (mperlat,mperlon)\nwhere mperlat = meters per degree latitude,\nmperlon = meters per degree longitude.\nThese calculations are based on a spherical earth, and were taken from\nhttp://www.nga.mil/MSISiteContent/StaticFiles/Calculators/degree.html\n\"\"\"\n", "func_signal": "def latlon_distance_conversion(lat):\n", "code": "lat_rad = lat * 2.0 * math.pi / 360.0;\nm1 = 111132.92;\nm2 = -559.82;\nm3 = 1.175;\nm4 = -0.0023;\np1 = 111412.84;\np2 = -93.5;\np3 = 0.118;\n\nlatlen = m1 + (m2 * math.cos(2 * lat)) + (m3 * math.cos(4 * lat)) + \\\n    (m4 * math.cos(6 * lat));\nlonlen = (p1 * math.cos(lat)) + (p2 * math.cos(3 * lat)) + \\\n    (p3 * math.cos(5 * lat));\n\nreturn (latlen,lonlen);", "path": "common\\src\\gisutils.py", "repo_name": "cbick/gps2gtfs", "stars": 19, "license": "mit", "language": "python", "size": 924}
{"docstring": "\"\"\"\nGiven a function 'tester' which, given a row, returns either\nTrue or False, and a collection of rows, returns\n      (trues,falses)\nwhere trues is a list of the rows where tester returned True,\nand falses is a list of the rows where tester returned False.\n\"\"\"\n", "func_signal": "def split_on_attribute_values(tester,rows):\n", "code": "ts,fs = [],[]\ndef helper(r):\n  if tester(r): ts.append(r)\n  else: fs.append(r)\n\nmap(helper,rows)\nreturn ts,fs", "path": "postprocessing\\src\\DataMining.py", "repo_name": "cbick/gps2gtfs", "stars": 19, "license": "mit", "language": "python", "size": 924}
{"docstring": "\"\"\"\nGiven a time, returns a (lat,lon) location of the (estimated)\nlocation of the bus at that time. If the time is before or after\nthe bounding time of the route, then None is returned.\n\"\"\"\n", "func_signal": "def getLocationAtTime(self,time):\n", "code": "if time < self.min_time or time > self.max_time:\n  return None\nif time < self.interpolation[self.cached_index][2]:\n  self.cached_index = 0    \n\n# Since time is within our bounds, this won't violate \n# the bounds of our list\nwhile time > self.interpolation[self.cached_index+1][2]:\n  self.cached_index += 1\n\npt0 = self.interpolation[self.cached_index]\npt1 = self.interpolation[self.cached_index+1]\n\nreturn self._interpolate( pt0, pt1, time );", "path": "core\\src\\BusTrack.py", "repo_name": "cbick/gps2gtfs", "stars": 19, "license": "mit", "language": "python", "size": 924}
{"docstring": "\"\"\"\nCreates a schedule matchup based on the specified tracked segment.\nIf trip_id is specified, uses the GTFS schedule for that trip ID,\notherwise uses the trip ID specified in the database.\nIf offset is specified, then that offset is applied against GTFS\ndata, otherwise the offset specified in the database is used.\nsegment can be either the segment_id in the tracked_routes table\nstored in the database, or it can be a GPSBusTrack object.\n\"\"\"\n\n", "func_signal": "def __init__(self,segment,trip_id=None,offset=None):\n", "code": "if isinstance(segment,basestring) or isinstance(segment,long):\n  self.segment = gpstool.TrackedVehicleSegment(segment,\n                                               useCorrectedGTFS=False);\nelse:\n  self.segment = segment\n\nif offset is not None: \n  self.segment.offset = offset\n  \nif trip_id is not None:\n  self.segment.trip_id = trip_id;\n  self.segment.schedule = GTFSBusSchedule(trip_id,self.segment.offset);\n  \nself.schedule = self.segment.schedule;\n\nif isinstance(segment,basestring) or isinstance(segment,long):\n  self.bustrack = GPSBusTrack(self.segment);\nelse:\n  self.bustrack = segment\n\nself.corrected_schedule = None; #don't make it unless someone wants it\nself.__matchSchedule();", "path": "core\\src\\GPSBusTrack.py", "repo_name": "cbick/gps2gtfs", "stars": 19, "license": "mit", "language": "python", "size": 924}
{"docstring": "\"\"\"\nCreates a ServiceDateHandler using the database from dbconn.\nIf autoFill is True, then any missing service combinations\nare added to the database. If autoCommit is True, these\nchanges will be committed immediately.\n\"\"\"\n", "func_signal": "def __init__(self,dbconn,autoFill=False,autoCommit=False):\n", "code": "cur = db.get_cursor()\n\n## Prepare calendar data\n\ndb.SQLExec(cur,\"\"\"select monday,tuesday,wednesday,thursday,friday,saturday,\n             sunday, service_id, start_date, end_date from gtf_calendar\"\"\");\nself.calendar_rows = cur.fetchall();\ndb.SQLExec(cur,\"\"\"select * from gtf_calendar_dates\"\"\");\nself.calendar_date_rows = cur.fetchall();\n\n## Load existing combos\n\ndb.SQLExec(cur,\"\"\"select * from service_combinations \n               order by combination_id, service_id\"\"\");\nservice_combo_rows = cur.fetchall();\n\nself.combos = {};  # map from combo_id to combo\n# note that combo lists are sorted by service_id\nfor row in service_combo_rows:\n  service_id, combo_id = row['service_id'],int(row['combination_id']);\n  if not self.combos.has_key(combo_id):\n    self.combos[combo_id] = []\n  self.combos[combo_id].append(service_id)\n\n# map from combo to combo_id (reverse of self.combos)\nself.existing_combos = {}; \nfor combo_id in self.combos:\n  self.existing_combos[tuple(self.combos[combo_id])] = int(combo_id);\n\ncur.close()\n\n## Fill in missing combos\n\nif autoFill:\n  self.fill_unique_service_combinations(dbconn,autoCommit);", "path": "common\\src\\ServiceDateHandler.py", "repo_name": "cbick/gps2gtfs", "stars": 19, "license": "mit", "language": "python", "size": 924}
{"docstring": "\"\"\"\nGiven a tuple of column names 'keys', and a collection of dict-like\nrows, returns a dictionary where every unique value as defined by keys\nis a key in the dictionary, and the value under that key is a list\ncontaining the corresponding rows.\n\"\"\"\n", "func_signal": "def split_on_attributes(keys,rows):\n", "code": "ret = {}\n\nfor row in rows:\n  key = tuple([row[k] for k in keys])\n  vals = ret.get(key)\n  if vals is None:\n    vals = []\n    ret[key] = vals\n  vals.append(row)\n\nreturn ret", "path": "postprocessing\\src\\DataMining.py", "repo_name": "cbick/gps2gtfs", "stars": 19, "license": "mit", "language": "python", "size": 924}
{"docstring": "\"\"\"\nGiven a day (date object), returns a collection of service IDs\nthat are in effect on that day\n\"\"\"\n", "func_signal": "def effective_service_ids(self,day):\n", "code": "service_ids = set()\ndayOfWeek = day.weekday() #0-6 for Mon-Sun\nfor row in self.calendar_rows:     \n  # recall we selected mon,tues,... first in the query\n  if int(row[dayOfWeek]):\n    #this service runs on this day of the week\n    if row['start_date'] <= day and row['end_date'] >= day: \n      #this service applies to this date\n      service_ids.add(row['service_id']);\n\nfor row in self.calendar_date_rows:\n  rowDate = row['date'];\n  if rowDate == day:\n    if int(row['exception_type'])-2: # 1 means added, 2 means removed\n      #so this means added\n      service_ids.add(row['service_id']);\n    else:\n      #and this means removed\n      service_ids.discard(row['service_id']);\n      \nret = list(service_ids);\nret.sort(); # to prevent permuted duplicates\nreturn tuple(ret);", "path": "common\\src\\ServiceDateHandler.py", "repo_name": "cbick/gps2gtfs", "stars": 19, "license": "mit", "language": "python", "size": 924}
{"docstring": "\"\"\"\nLoads segment_id's gps schedule from database\n\"\"\"\n", "func_signal": "def __init__(self,segment_id):\n", "code": "self.arrival_schedule = db.load_gps_schedule(segment_id);\nself.trip_id, self.trip_date, self.vehicle_id, self.schedule_error, \\\n    self.schedule_offset = db.load_gps_segment_header(segment_id);", "path": "core\\src\\GPSBusTrack.py", "repo_name": "cbick/gps2gtfs", "stars": 19, "license": "mit", "language": "python", "size": 924}
{"docstring": "\"\"\"\nA private method which returns the tuple \n(min_lon, max_lon, min_lat, max_lat) \nrepresenting the latitudinal and longitudinal bounds \nthat contain all points along this route.\n\"\"\"\n", "func_signal": "def _findBoundingBox(self):\n", "code": "if self.interpolation is None:\n  return None\ninitial = (1000,-1000,1000,-1000);\ndef helper(bounds,point):\n  lat,lon = map(float,(point[0],point[1]))\n  return (min(bounds[0],lat), max(bounds[1], lat), \n          min(bounds[2],lon), max(bounds[3], lon));\n\nreturn reduce(helper, self.interpolation, initial);", "path": "core\\src\\BusTrack.py", "repo_name": "cbick/gps2gtfs", "stars": 19, "license": "mit", "language": "python", "size": 924}
{"docstring": "\"\"\"\nGiven two (lat,lon) points ll1 and ll2, returns the distance\nbetween them in nautical miles.\n\"\"\"\n", "func_signal": "def distance_nautical_miles(ll1,ll2):\n", "code": "lat1,lon1=ll1\nlat2,lon2=ll2\nrad = math.pi / 180.0\n  \nyDistance = (lat2 - lat1) #* nauticalMilePerLat\nxDistance = (math.cos(lat1 * rad) + math.cos(lat2 * rad)) * (lon2 - lon1)# * (nauticalMilePerLongitude / 2)\n  \ndistance = math.sqrt( yDistance**2 + xDistance**2 )\n\nreturn distance# * milesPerNauticalMile", "path": "common\\src\\gisutils.py", "repo_name": "cbick/gps2gtfs", "stars": 19, "license": "mit", "language": "python", "size": 924}
{"docstring": "\"\"\"\nReturns (mu_e,mu_l)\nwhere mu_e = mean of all early arrival times\nand   mu_l = mean of all late arrival times.\n\nCan optionally specify a static arrival schedule to calculate.\n\"\"\"\n", "func_signal": "def getEarlyAndLateMeans(self,arrival_schedule=None):\n", "code": "mu_e,mu_l=0.0,0.0\nn_e,n_l=0,0\nif arrival_schedule is None:\n  arrival_schedule = self.arrival_schedule\n\nfor arrival in arrival_schedule:\n  arrival_time=arrival['actual_arrival_time_seconds'];\n  if arrival_time is not None:\n    sched_time = arrival['arrival_time_seconds']\n    diff = arrival_time - sched_time\n    if diff < 0: \n      mu_e -= diff\n      n_e += 1\n    elif diff > 0:\n      mu_l += diff\n      n_l += 1\n\nn_e = max(n_e,1)\nn_l = max(n_l,1)\nreturn mu_e/n_e, mu_l/n_l", "path": "core\\src\\GPSBusTrack.py", "repo_name": "cbick/gps2gtfs", "stars": 19, "license": "mit", "language": "python", "size": 924}
{"docstring": "\"\"\"\nGiven two (lat,lon) points ll1 and ll2, returns the distance\nbetween them in meters. Note that this only works for points\nthat are relatively close.\n\"\"\"\n", "func_signal": "def distance_meters(ll1, ll2):\n", "code": "lat1,lon1 = map(float,ll1)\nlat2,lon2 = map(float,ll2)\nmidlat = (lat1+lat2)/2\nm_per_lat,m_per_lon = latlon_distance_conversion(midlat);\n\nlon_dist_m = (lon2-lon1)*m_per_lon;\nlat_dist_m = (lat2-lat1)*m_per_lat;\n\nreturn math.sqrt(lon_dist_m**2 + lat_dist_m**2);", "path": "common\\src\\gisutils.py", "repo_name": "cbick/gps2gtfs", "stars": 19, "license": "mit", "language": "python", "size": 924}
{"docstring": "#print \"Uploading...\"\n\n", "func_signal": "def handle_write(self):\n", "code": "bytes = self.send(self.xml)\nself.xml = self.xml[bytes:]\nif self.xml == \"\":\n  #print \"done.\"\n  self.close()", "path": "realtime\\src\\test.py", "repo_name": "cbick/gps2gtfs", "stars": 19, "license": "mit", "language": "python", "size": 924}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def getNumTimePoints(self,arrival_schedule=None):\n", "code": "if arrival_schedule is None:\n  arrival_schedule = self.arrival_schedule\ndef help(a,b):\n  if b['actual_arrival_time_seconds'] is None:\n    return a\n  return a+1\nreturn reduce(help,arrival_schedule,0)", "path": "core\\src\\GPSBusTrack.py", "repo_name": "cbick/gps2gtfs", "stars": 19, "license": "mit", "language": "python", "size": 924}
{"docstring": "\"\"\"\nReturns (distance,fraction)\nwhere distance = distance of p1 from ll1-ll2 segment, and \nfraction = fractional progress of the minimum-distance point\nalong the segment ll1->ll2\n\"\"\"\n", "func_signal": "def interp_helper(ll1,ll2,pt):\n", "code": "D_ps = distance_from_segment_meters(ll1,ll2,pt);\nD_12 = distance_meters(ll1,ll2);\nD_p1 = distance_meters(ll1,pt);\nif D_12 == 0:\n  return D_p1,0.0\nif D_p1 < D_ps:\n  return D_p1,0.0\nfrac = math.sqrt( int(D_p1)**2 - int(D_ps)**2 ) / D_12;      \nreturn (D_ps,frac);", "path": "common\\src\\gisutils.py", "repo_name": "cbick/gps2gtfs", "stars": 19, "license": "mit", "language": "python", "size": 924}
{"docstring": "\"\"\"Sets attributes as, for each key in dictlike,\nself.key = dictlike[key]\n\"\"\"\n", "func_signal": "def set_attributes(self,dictlike):\n", "code": "for key in dictlike.keys():\n  self.__setattr__(key,dictlike[key]);", "path": "core\\src\\BusTrack.py", "repo_name": "cbick/gps2gtfs", "stars": 19, "license": "mit", "language": "python", "size": 924}
{"docstring": "\"\"\"\nGiven two (lat,lon) endpoints of a line pt1 and pt2,\nand a third (lat,lon) point ll, returns the distance\nin meters of ll from the segment.\n\"\"\"\n## First convert to meter coordinates for proper distance\n", "func_signal": "def distance_from_segment_meters(pt1,pt2,ll):\n", "code": "lat1,lon1 = map(float,pt1)\nlat2,lon2 = map(float,pt2)\nllat,llon = map(float,ll)\nmidlat = (lat1+lat2+llat)/3.\nmidlon = (lon1+lon2+llon)/3.\nm_per_lat,m_per_lon = latlon_distance_conversion(midlat);\n## Translate to the weighted center\ny1,x1 = (lat1-midlat)*m_per_lat,(lon1-midlon)*m_per_lon\ny2,x2 = (lat2-midlat)*m_per_lat,(lon2-midlon)*m_per_lon\nly,lx = (llat-midlat)*m_per_lat,(llon-midlon)*m_per_lon\n## Compute and return answer\nsh_line = geom.LineString(( (x1,y1) , (x2,y2) ))\nsh_pt = geom.Point(( lx,ly ))\nreturn sh_pt.distance(sh_line);", "path": "common\\src\\gisutils.py", "repo_name": "cbick/gps2gtfs", "stars": 19, "license": "mit", "language": "python", "size": 924}
{"docstring": "\"\"\"\nGiven a VehicleSegment, builds the interpolation.\n\"\"\"\n", "func_signal": "def __init__(self,vehicle_segment):\n", "code": "self.__load(vehicle_segment);\nBusTrack.__init__(self);", "path": "core\\src\\GPSBusTrack.py", "repo_name": "cbick/gps2gtfs", "stars": 19, "license": "mit", "language": "python", "size": 924}
{"docstring": "\"\"\"\n\n\"\"\"\n", "func_signal": "def __init__(self,sum1,sum2,length,id,left=None,right=None):\n", "code": "self.sum1,self.sum2,self.length = sum1,sum2,length\nself.mean = float(sum1)/length\nself.std = sqrt( float(sum2)/length - self.mean**2 )\nself.id = id\nself.left,self.right = left,right", "path": "postprocessing\\src\\DataMining.py", "repo_name": "cbick/gps2gtfs", "stars": 19, "license": "mit", "language": "python", "size": 924}
{"docstring": "\"\"\"write the contents of the loop manager to a Rosetta3 loop file\"\"\"\n", "func_signal": "def write(self,filename):\n", "code": "loop_file = fileutil.universal_open(filename,\"w\")\nfor loop in self.looplist:\n    loop_file.write(loop.to_string()+\"\\n\")\nloop_file.close()", "path": "rosettautil\\rosetta\\loops.py", "repo_name": "decarboxy/py_protein_utils", "stars": 16, "license": "mit", "language": "python", "size": 175}
{"docstring": "\"\"\"read a rosetta 3 loop file into the loop manager.\nif append=True, add the contents of the loop file to the existing loops\"\"\"\n", "func_signal": "def read(self,filename,append=False):\n", "code": "if not append:\n    self.looplist = []\nloop_file = fileutil.universal_open(filename,\"rU\")\nfor line in loop_file:\n    fields = line.split()\n    if line[0] == '#':\n        continue #this is a comment\n    if len(fields) <1 :\n        continue #this is a blank line\n    if fields[0] != \"LOOP\":\n        continue #this is something that is not a loop line\n    loop = RosettaLoop()\n    loop.set_loop_from_string(line)\n    self.looplist.append(loop)\nloop_file.close()", "path": "rosettautil\\rosetta\\loops.py", "repo_name": "decarboxy/py_protein_utils", "stars": 16, "license": "mit", "language": "python", "size": 175}
{"docstring": "\"\"\"return true if the score of a given sequence position and residue name is greater than 0\"\"\"\n", "func_signal": "def conserved(self,seqpos,resname):\n", "code": "score = self.get_score(seqpos,resname)\nif(score >=0):\n\treturn True\nelse:\n\treturn False", "path": "rosettautil\\protein\\PSSM.py", "repo_name": "decarboxy/py_protein_utils", "stars": 16, "license": "mit", "language": "python", "size": 175}
{"docstring": "\"\"\"return percent pssm recovery fro residues within a range of b factors\"\"\"\n", "func_signal": "def pssm_recovery_range(struct,pssm_map,min,max):\n", "code": "pssm_recovery = 0.0;\nstruct_size = 0.0;\nfor residue in struct.get_residues():\n\tscore= residue.get_list()[1].get_bfactor()\n\t#print score\n\tif score >= min and score <= max:\n\t\tresidue_name = residue.get_resname()\n\t\tresidue_num = residue.get_id()[1]\n\t\tstatus = pssm_map.conserved(residue_num,residue_name)\n\t\tif status:\n\t\t\tpssm_recovery += 1.0\n\t\tstruct_size += 1.0\n\t\t\nreturn pssm_recovery/struct_size", "path": "rosettautil\\protein\\pdbStat.py", "repo_name": "decarboxy/py_protein_utils", "stars": 16, "license": "mit", "language": "python", "size": 175}
{"docstring": "\"\"\"output a loop as a line of a rosetta3 loop file\"\"\"\n", "func_signal": "def to_string(self):\n", "code": "if self.extend:\n    return \"LOOP \"+str(self.start)+\" \"+str(self.end)+\" \"+str(self.cutpoint)+\" \"+str(self.skip)+\" \"+str(1)\nelse:\n    return \"LOOP \"+str(self.start)+\" \"+str(self.end)+\" \"+str(self.cutpoint)+\" \"+str(self.skip)+\" \"+str(0)", "path": "rosettautil\\rosetta\\loops.py", "repo_name": "decarboxy/py_protein_utils", "stars": 16, "license": "mit", "language": "python", "size": 175}
{"docstring": "#tag = path.split(\"/\")[-1]\n", "func_signal": "def add_file(self,path):\n", "code": "tag=path\nself.table_map[tag]=ScoreTable(path)", "path": "rosettautil\\rosetta\\rosettaScore.py", "repo_name": "decarboxy/py_protein_utils", "stars": 16, "license": "mit", "language": "python", "size": 175}
{"docstring": "\"\"\" return raw pssm total for each residue\"\"\"\n", "func_signal": "def pssm_scores(struct,pssm_map):\n", "code": "struct_residues = struct.get_residues()\npssm_scores = {}\nsize = 0\nfor residue in struct_residues:\n\tsize += 1\n\tresidue_name = residue.get_resname()\n\tresidue_num = residue.get_id()[1]\n\ttry:\n\t\tpssm_scores[residue_name] += pssm_map.get_score(residue_num,residue_name)\n\texcept KeyError:\n\t\tpssm_scores[residue_name] = pssm_map.get_score(residue_num,residue_name)\nfor key in pssm_scores:\n\tpssm_scores[key] = pssm_scores[key]/size\nreturn pssm_scores", "path": "rosettautil\\protein\\pdbStat.py", "repo_name": "decarboxy/py_protein_utils", "stars": 16, "license": "mit", "language": "python", "size": 175}
{"docstring": "\"\"\"Parse a PSSM file from BLAST into a usable datastructure\"\"\"\n", "func_signal": "def __init__(self,path,mode=\"score\"):\n", "code": "self.pssmmap = {}\nself.native_sequence = {}\npssmfile = fileutil.universal_open(path,'r')\n\n\npssmfile.readline()\npssmfile.readline()\n\t\nheader = pssmfile.readline()\nheader = header.split()\nheader = header[0:21]\nfor line in pssmfile:\n\t#print line\n\tline = line.split()\n\t#self.native_sequence.append(\n\tif len(line) == 0:\n\t\tbreak\n\n\tres_num = int(line[0])\n\tres_id = line[1]\n\tself.native_sequence[res_num] = res_id\n\tline_map = {}\n\t\n\tif mode == \"score\":\n\t\tdata = line[2:23]\n\tif mode == \"percent\":\n\t\tdata = line[22:42]\n\t\t#print data\n\tfor resname,score in zip(header,data):\n\t\tline_map[resname] = int(score)\n\tself.pssmmap[res_num] = line_map\n\npssmfile.close()", "path": "rosettautil\\protein\\PSSM.py", "repo_name": "decarboxy/py_protein_utils", "stars": 16, "license": "mit", "language": "python", "size": 175}
{"docstring": "\"\"\"calculate percent sequence recovery between a native and designed\nstruct for all residues between a minimum and maximum b factor \"\"\"\n", "func_signal": "def sequence_recovery_range(native_struct,designed_struct,min,max):\n", "code": "native_residues = native_struct.get_residues()\ndesigned_residues = designed_struct.get_residues()\ntotal = 0.0\nrecovered = 0.0\nfor native,designed in zip(native_residues,designed_residues):\n\tscore = native.get_list()[1].get_bfactor()\n\tif score >= min and score <= max:\n\t\tif native.get_resname() == designed.get_resname():\n\t\t\trecovered += 1\n\t\ttotal += 1\nreturn recovered/total", "path": "rosettautil\\protein\\pdbStat.py", "repo_name": "decarboxy/py_protein_utils", "stars": 16, "license": "mit", "language": "python", "size": 175}
{"docstring": "\"\"\" copy the b factors from one structure to another\"\"\"\n", "func_signal": "def copy_b_factor(native_pdb,designed_pdb):\n", "code": "native_atoms = native_pdb.get_atoms()\ndesigned_atoms = designed_pdb.get_atoms()\nfor native_atom, designed_atom in zip(native_atoms, designed_atoms):\n\tdesigned_atom.set_bfactor(native_atom.get_bfactor)\nreturn designed_pdb", "path": "rosettautil\\protein\\pdbStat.py", "repo_name": "decarboxy/py_protein_utils", "stars": 16, "license": "mit", "language": "python", "size": 175}
{"docstring": "\"\"\"return percent pssm recovery\"\"\"\n", "func_signal": "def pssm_recovery(struct,pssm_map):\n", "code": "struct_residues = struct.get_residues()\npssm_recovery = 0.0;\nstruct_size = 0.0;\nfor residue in struct_residues:\n\tresidue_name = residue.get_resname()\n\tresidue_num = residue.get_id()[1]\n\tstatus = pssm_map.conserved(residue_num,residue_name)\n\tif status:\n\t\tpssm_recovery += 1.0\n\tstruct_size += 1.0\nreturn pssm_recovery/struct_size", "path": "rosettautil\\protein\\pdbStat.py", "repo_name": "decarboxy/py_protein_utils", "stars": 16, "license": "mit", "language": "python", "size": 175}
{"docstring": "\"\"\"return the score table from the bottom of a PDB as a list of lines\"\"\"\n", "func_signal": "def get_table(path):\n", "code": "raw_table = []\ninfile = fileutil.universal_open(path,'r')\ntable = False\nfor line in infile:\n    line_split = line.split()\n    if len(line_split) <1:\n        break\n    if line_split[0] == \"#BEGIN_POSE_ENERGIES_TABLE\":\n        table =True\n        raw_table.append(line)\n    #elif table and line_split[0] == \"#END_POSE_ENERGIES_TABLE\":\n    #    raw_table.append(line)\n    #    break\n    elif table:\n        raw_table.append(line)\ninfile.close()\nreturn raw_table", "path": "rosettautil\\rosetta\\rosettaScore.py", "repo_name": "decarboxy/py_protein_utils", "stars": 16, "license": "mit", "language": "python", "size": 175}
{"docstring": "\"\"\"calculate percent sequence recovery between a native and designed struct\"\"\"\n", "func_signal": "def sequence_recovery(native_struct,designed_struct):\n", "code": "native_residues = native_struct.get_residues()\ndesigned_residues = designed_struct.get_residues()\ntotal = 0.0;\nrecovered = 0.0;\nfor native,designed in zip(native_residues,designed_residues):\n\tif native.get_resname() == designed.get_resname():\n\t\trecovered += 1\n\ttotal += 1\n#print recovered, total\nreturn recovered/total", "path": "rosettautil\\protein\\pdbStat.py", "repo_name": "decarboxy/py_protein_utils", "stars": 16, "license": "mit", "language": "python", "size": 175}
{"docstring": "\"\"\"calculate the CA rmsd of two structures using the residues in residue_list\"\"\"\n", "func_signal": "def ca_rms_only(struct_a,struct_b,residue_list):\n", "code": "residues_a = struct_a.get_residues();\nresidues_b = struct_b.get_residues();\n\nd_2_sum = 0.0\nresn = 0\nfor (res_a, res_b) in zip(residues_a, residues_b):\n\tif res_a.get_id()[1] not in residue_list:\n\t\tcontinue\n\tCA_a = res_a['CA']\n\tCA_b = res_b['CA']\n\n\tdistance_2 = (CA_a-CA_b)**2\n\td_2_sum += distance_2\n\tresn += 1\n\t\nrmsd = math.sqrt(d_2_sum/resn)\nreturn rmsd", "path": "rosettautil\\protein\\pdbStat.py", "repo_name": "decarboxy/py_protein_utils", "stars": 16, "license": "mit", "language": "python", "size": 175}
{"docstring": "\"\"\"calculate sequence composition by residue\"\"\"\n", "func_signal": "def sequence_composition(struct):\n", "code": "struct_residues = struct.get_residues()\ncomposition = {}\nfor residue in struct_residues:\n\tresidue_name = residue.get_resname()\n\ttry:\n\t\tcomposition[residue_name]+=1\n\texcept KeyError:\n\t\tcomposition[residue_name] = 1 \nreturn composition", "path": "rosettautil\\protein\\pdbStat.py", "repo_name": "decarboxy/py_protein_utils", "stars": 16, "license": "mit", "language": "python", "size": 175}
{"docstring": "\"\"\"return a biopython structure object given a pdb file path\"\"\"\n", "func_signal": "def load_pdb(path):\n", "code": "parser = PDBParser(PERMISSIVE=1)\npdb_file = fileutil.universal_open(path,'rU')\nstructure = parser.get_structure(path[0:4],pdb_file)\npdb_file.close()\nreturn structure", "path": "rosettautil\\protein\\util.py", "repo_name": "decarboxy/py_protein_utils", "stars": 16, "license": "mit", "language": "python", "size": 175}
{"docstring": "\"\"\"load a param file into a data structure\"\"\"\n", "func_signal": "def __init__(self,filename):\n", "code": "self.atoms = {}\nself.bonds = []\nself.icoors = {}\ninfile = fileutil.universal_open(filename,'r')\nfor line in infile:\n    if len(line)==0:\n        continue\n    line = line.split()\n    tag = line[0]\n    if tag ==\"NAME\":\n        self.name = line[1]\n    elif tag ==\"IO_STRING\":\n        self.io_string = line[1]\n    elif tag == \"TYPE\":\n        self.type = line[1]\n    elif tag == \"AA\":\n        self.aa = line[1]\n    elif tag == \"ATOM\":\n        current_atom = Atom(line[1],line[2],line[3],line[4])\n        self.atoms[line[1]]= current_atom\n    elif tag == \"BOND\":\n        current_bond = Bond(line[1],line[2])\n        self.bonds.append(current_bond)\n        self.atoms[line[1]].add_bond(current_bond)\n        self.atoms[line[2]].add_bond(current_bond)\n    elif tag == \"NBR_ATOM\":\n        self.nbr_atom = line[1]\n    elif tag == \"NBR_RADIUS\":\n        self.nbr_radius = line[1]\n    elif tag == \"ICOOR_INTERNAL\":\n        current_icoor = icoor(line[1],line[2],line[3],line[4],line[5],line[6],line[7])\n        self.icoors[line[1]]=current_icoor\ninfile.close()", "path": "rosettautil\\rosetta\\params.py", "repo_name": "decarboxy/py_protein_utils", "stars": 16, "license": "mit", "language": "python", "size": 175}
{"docstring": "\"\"\"calculate the all atom rmsd given two lists of atoms and a list of residues\"\"\"\n", "func_signal": "def atom_rms(atoms_a,atoms_b,residue_list=\"\"):\n", "code": "d_2_sum = 0.0\nresn = 0\nfor(atom_a,atom_b) in zip(atoms_a,atoms_b):\n\tparent_a = atom_a.get_parent()\n\tif parent_a.get_id()[1] in residue_list or residue_list == \"\":\n\t\t#print \"calculating for\",parent_a.get_id()[1]\n\t\tdistance_2 = (atom_a-atom_b)**2\n\t\td_2_sum += distance_2\n\t\tresn +=1\n\telse:\n\t\tcontinue\nrmsd = math.sqrt(d_2_sum/resn)\nreturn rmsd", "path": "rosettautil\\protein\\pdbStat.py", "repo_name": "decarboxy/py_protein_utils", "stars": 16, "license": "mit", "language": "python", "size": 175}
{"docstring": "\"\"\"calculate the pssm recovery within a range of b factors given a\nstructure and a pssm map\"\"\"\n", "func_signal": "def pssm_recovery_map_range(struct,pssm_map,min,max):\n", "code": "struct_residues = struct.get_residues()\nrecovery_map = {}\nfor residue in struct_residues:\n\tscore = residue.get_list()[1].get_bfactor()\n\tif score >= min and score <= max:\n\t\tresidue_name = residue.get_resname()\n\t\tresidue_num = residue.get_id()[1]\n\t\tstatus = pssm_map.conserved(residue_num, residue_name)\n\t\tif status:\n\t\t\ttry:\n\t\t\t\trecovery_map[residue_name]+= 1\n\t\t\texcept KeyError:\n\t\t\t\trecovery_map[residue_name] = 1\nreturn recovery_map", "path": "rosettautil\\protein\\pdbStat.py", "repo_name": "decarboxy/py_protein_utils", "stars": 16, "license": "mit", "language": "python", "size": 175}
{"docstring": "\"\"\"calculate the pssm recovery given a structure and a pssm map\"\"\"\n", "func_signal": "def pssm_recovery_map(struct,pssm_map):\n", "code": "struct_residues = struct.get_residues()\n#pssm_recovery = 0.0;\n#struct_size = 0.0;\nrecovery_map = {}\nfor residue in struct_residues:\n\tresidue_name = residue.get_resname()\n\tresidue_num = residue.get_id()[1]\n\tstatus = pssm_map.conserved(residue_num,residue_name)\n\tif status:\n\t\ttry:\n\t\t\trecovery_map[residue_name]+=1\n\t\texcept KeyError:\n\t\t\trecovery_map[residue_name] = 1\nreturn recovery_map", "path": "rosettautil\\protein\\pdbStat.py", "repo_name": "decarboxy/py_protein_utils", "stars": 16, "license": "mit", "language": "python", "size": 175}
{"docstring": "# jsmin default\n", "func_signal": "def test_default_compression():\n", "code": "merger = compression_setup()\noutfiles = merger.run()\nsfb = open(outfiles[0]).read().strip()\nassert sfb == jsmin_result.strip(), sfb", "path": "tests\\test_jsbuild.py", "repo_name": "whitmo/jstools", "stars": 18, "license": "None", "language": "python", "size": 997}
{"docstring": "#@@ this function needs to be decomposed into smaller testable bits\n", "func_signal": "def extract_deps(self, cfg, depmap=None):\n", "code": "sourcedirs = cfg['root']\n\n# assemble all files in source directory according to config\ninclude = cfg.get('include', tuple())\nexclude = [Exclude(e) for e in cfg['exclude']]\nall_inc = cfg['first'] + cfg['include'] + cfg['last']\nfiles = {}\nimplicit = False\nif not len(include):            \n    # implicit file inclusion\n    implicit = True\n\nfor sourcedir in sourcedirs:\n    newfiles = []\n    for filepath in jsfiles_for_dir(os.path.join(self.root_dir, sourcedir)):\n        fitem = filepath, srcfile = filepath, self.make_sourcefile(sourcedir, filepath, exclude),\n        if implicit and not filepath in exclude:\n            all_inc.append(filepath)\n        newfiles.append(fitem)\n\n    newfiles = list((filepath, sf) for filepath, sf in newfiles if filepath in all_inc and filepath not in exclude)\n    files.update(dict(newfiles))\n\n# ensure all @include and @requires references are in\ncomplete = False\nwhile not complete:\n    complete = True\n    for filepath, info in files.items():\n        for path in info.include + info.requires:\n            if path not in exclude and not files.has_key(path):\n                complete = False\n                for sourcedir in sourcedirs:\n                    if os.path.exists(os.path.join(self.root_dir, sourcedir, path)):\n                        files[path] = self.make_sourcefile(sourcedir, path, exclude)\n                        break\n                else:\n                    raise MissingImport(\"File '%s' not found in root directories\" % path)\n\n# create list of dependencies\ndependencies = {}\nfor filepath, info in files.items():\n    dependencies[filepath] = info.requires\n\n\n# get tuple of files ordered by dependency\nself.printer.debug(\"Sorting dependencies.\")\norder = [x for x in tsort.sort(dependencies)]\n\n# move forced first and last files to the required position\nself.printer.debug(\"Re-ordering files.\")\norder = cfg['first'] + [item\n             for item in order\n             if ((item not in cfg['first']) and\n                 (item not in cfg['last']))] + cfg['last']\n\nparts = ('first', 'include', 'last')\nrequired_files = []\n\n## Make sure all imports are in files dictionary\nfor part in parts:\n    for fp in cfg[part]:\n        if not fp in exclude and not files.has_key(fp):\n            raise MissingImport(\"File from '%s' not found: %s\" % (part, fp))\n\n## Output the files in the determined order\nresult = []\nfor fp in order:\n    result.append(files[fp])\n\nreturn result", "path": "jstools\\merge.py", "repo_name": "whitmo/jstools", "stars": 18, "license": "None", "language": "python", "size": 997}
{"docstring": "\"\"\"\nExclude should trump all other declarations of dependency\n\"\"\"\n", "func_signal": "def test_exclude():\n", "code": "merger = file_tree(conf='exclude')\nout = merger.run(uncompressed=True, single='Output.js', strip_deps=True)\nresfile = open(out[0])\nresults = resfile.readlines()\nresfile.close()\nfor ln in range(len(results)):\n    # required\n    assert 'prototype.js' not in results[ln], ValueError(results[ln])\n    # included\n    assert 'api.js' not in results[ln], ValueError(results[ln])", "path": "tests\\test_jsbuild.py", "repo_name": "whitmo/jstools", "stars": 18, "license": "None", "language": "python", "size": 997}
{"docstring": "\"\"\"\nRunning against 'data/basic.cfg' w/out any first entries, all files\nshould be picked up, but 'core/api.js' should be last.\n\"\"\"\n", "func_signal": "def test_implicit_just_last():\n", "code": "merger = file_tree()\nmerger.remove_option('Output.js', 'first')\n\nout = merger.run(uncompressed=True, single='Output.js', strip_deps=True)\nresults = results_from_uncompressed_outfile(out[0])\nassert_basic_order(results)\n\nassert results[-1] == 'core/api.js'\nassert not '3rd/logger.js' in results", "path": "tests\\test_jsbuild.py", "repo_name": "whitmo/jstools", "stars": 18, "license": "None", "language": "python", "size": 997}
{"docstring": "\"\"\"Load up a list of config filenames in our merger\"\"\"\n", "func_signal": "def from_fn(cls, fn, output_dir=None, root_dir=None, defaults=None, printer=logger):\n", "code": "merger = cls(output_dir, root_dir, defaults=defaults, printer=printer)\nif isinstance(fn, basestring):\n    fn = fn,\nfns = merger.read(fn)\nassert fns, ValueError(\"No valid config files: %s\" %fns)\nreturn merger", "path": "jstools\\merge.py", "repo_name": "whitmo/jstools", "stars": 18, "license": "None", "language": "python", "size": 997}
{"docstring": "\"\"\"Load up config files in our parser.\"\"\"\n", "func_signal": "def from_fn(cls, fn, defaults=None):\n", "code": "parser = cls(defaults)\nif isinstance(fn, basestring):\n    fn = fn,\nfns = parser.read(fn)\nassert fns, ValueError(\"No valid config files: %s\" % fns)\nreturn parser", "path": "jstools\\jst.py", "repo_name": "whitmo/jstools", "stars": 18, "license": "None", "language": "python", "size": 997}
{"docstring": "\"\"\"\nProvides lazy reading of the source file\n\"\"\"\n", "func_signal": "def source(self):\n", "code": "if self._source is None:\n    self._source = open(self.abspath, \"U\").read()\nreturn self._source", "path": "jstools\\merge.py", "repo_name": "whitmo/jstools", "stars": 18, "license": "None", "language": "python", "size": 997}
{"docstring": "## Header inserted at the start of each file in the output\n", "func_signal": "def merge(self, cfg, depmap=None):\n", "code": "HEADER = \"/* \" + \"=\" * 70 + \"\\n    %s\\n\" + \"   \" + \"=\" * 70 + \" */\\n\\n\"\nresult = []\nfiles = self.extract_deps(cfg, depmap)\nfor f in files:\n    self.printer.debug(\"Exporting: \" + f.filepath)\n    result.append(HEADER % f.filepath)\n    source = f.source\n    result.append(source)\n    if not source.endswith(\"\\n\"):\n        result.append(\"\\n\")\n\nself.printer.debug(\"\\nTotal files merged: %d \" % len(files))\nmerged = \"\".join(result)\nif cfg['closure']:\n    merged = '(function(){%s})();' % merged\nreturn merged", "path": "jstools\\merge.py", "repo_name": "whitmo/jstools", "stars": 18, "license": "None", "language": "python", "size": 997}
{"docstring": "\"\"\"\nRunning against 'data/basic.cfg' w/out any last entries, all files\nshould be picked up, but '3rd/prototype.js' should be first and\n'core/params.js' should be third.\n\"\"\"\n", "func_signal": "def test_implicit_just_first():\n", "code": "merger = file_tree()\nmerger.remove_option('Output.js', 'last')\n\nout = merger.run(uncompressed=True, single='Output.js', strip_deps=True)\nresults = results_from_uncompressed_outfile(out[0])\n\n#@@ sorting actually hides valuable info here\nassert_basic_order(results)\nassert results[0] == '3rd/prototype.js'\nassert results[2] == 'core/params.js'\nassert not '3rd/logger.js' in results", "path": "tests\\test_jsbuild.py", "repo_name": "whitmo/jstools", "stars": 18, "license": "None", "language": "python", "size": 997}
{"docstring": "#@@ add a dep->out map option\n", "func_signal": "def file_tree(conf='basic', depcfg='data/deps1.cfg', output_files=('Output.js',)):\n", "code": "tempdir, libdir = testutils.setup_temp_dir()\nmerger = testutils.load_config(conf, libdir)\nhandles = testutils.setup_dir(merger, prefix=libdir)\ndepmap = deps.DepMap.from_resource(depcfg)\nfiles = [x for x in testutils.inject_deps(handles, libdir, depmap)]\nset_faux_files(merger, libdir, *output_files)\nreturn merger", "path": "tests\\test_jsbuild.py", "repo_name": "whitmo/jstools", "stars": 18, "license": "None", "language": "python", "size": 997}
{"docstring": "# walk up from current directory\n# check virtualenv\n# check user directory\n# return either a ConfigParser or a SectionMap\n", "func_signal": "def retrieve_config(section=None, strict=False):\n", "code": "from paver.easy import path\nfn = \".jstools.cfg\"\nconf = None\ndirectory = path(os.path.abspath(os.curdir))\nsection_or_parser = load_return(section)\nwhile conf is None and directory:\n    if os.path.exists(directory / fn):\n        return section_or_parser(directory / fn)\n    directory = directory.parent\n\nvenv = os.environ.get(\"VIRTUAL_ENV\")\nif venv and (path(venv) / fn).exists():\n    return section_or_parser(path(venv) / fn)\n\nuser = path(os.path.expanduser(\"~/\"))\nif (user / fn).exists():\n    return section_or_parser(user / fn)", "path": "jstools\\utils.py", "repo_name": "whitmo/jstools", "stars": 18, "license": "None", "language": "python", "size": 997}
{"docstring": "\"\"\"\nwhen deps are 'data/deps1.cfg' and first or last is specified as\nbasic.cfg, output order should be:\n\n['3rd/prototype.js',\n'core/application.js',\n'core/params.js',\n'core/api.js']\n\n\"\"\"\n", "func_signal": "def assert_basic_order(results):\n", "code": "assert results == ['3rd/prototype.js',\n                   'core/application.js',\n                   'core/params.js',\n                   'core/api.js'], results", "path": "tests\\test_jsbuild.py", "repo_name": "whitmo/jstools", "stars": 18, "license": "None", "language": "python", "size": 997}
{"docstring": "\"\"\"\nPassing Merger.run a concatenate kwarg should create a single file\n\"\"\"\n", "func_signal": "def test_concatenate():\n", "code": "merger = file_tree(depcfg=\"data/concat-dep.cfg\", conf=\"meta-concatenate\", output_files=(\"Output1.js\", \"Output2.js\", \"Output3.js\"))\n\n# set o2 to an alternate license\nlp = merger.get('Output2.js', 'license') + \"2\"\nmerger.set('Output2.js', 'license', lp)\nopen(lp, 'w').write(license2)\n\noutfiles = merger.run(concatenate=\"sfb.js\")\nsfb = open(outfiles[0])\nout = sfb.read()\nfiles_found = R_FILES.findall(out)\nassert files_found == ['core2/lib2.js', 'core3/lib3.js', 'core1/lib1.js']\n\nassert license in out\nassert license2 in out\n\nmerger.add_section(\"meta\")\nmerger.set(\"meta\", \"order\", \"\\n\".join(['Output1.js', 'Output2.js', 'Output3.js']))\noutfiles = merger.run(concatenate=\"sfb2.js\")\nsfb = open(outfiles[0])\nfiles_found = R_FILES.findall(sfb.read())\nassert files_found == ['core1/lib1.js', 'core2/lib2.js', 'core3/lib3.js']", "path": "tests\\test_jsbuild.py", "repo_name": "whitmo/jstools", "stars": 18, "license": "None", "language": "python", "size": 997}
{"docstring": "\"\"\"\nExtracts the dependencies specified in the source code and returns\na list of them.\n\"\"\"\n", "func_signal": "def requires(self):\n", "code": "req = getattr(self, '_requires', None)\nif req is _marker:\n    self._requires = [x.strip() for x in RE_REQUIRE.findall(self.source)\\\n                      if x not in self.exclude]\nreturn self._requires", "path": "jstools\\merge.py", "repo_name": "whitmo/jstools", "stars": 18, "license": "None", "language": "python", "size": 997}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def __init__(self, root_dir, sourcedir, filepath, exclude, depmap=None):\n", "code": "self.filepath = filepath\nself.fullpath = os.path.join(sourcedir, filepath)\nself.abspath = os.path.join(root_dir, self.fullpath)\nself.exclude = exclude\nself._source = None\nself._requires = _marker\nself._include = _marker\nself.depmap = depmap", "path": "jstools\\merge.py", "repo_name": "whitmo/jstools", "stars": 18, "license": "None", "language": "python", "size": 997}
{"docstring": "\"\"\"\nLicenses unwrapped by comment should get wrapped\n\"\"\"\n", "func_signal": "def test_license_wrapping():\n", "code": "merger = compression_setup()\nlp = merger.get('Output.js', 'license')\nopen(lp, 'w').write(license2)\noutfiles = merger.run()\nsfb = open(outfiles[0]).read().strip()\nassert sfb.startswith(\"/*\")", "path": "tests\\test_jsbuild.py", "repo_name": "whitmo/jstools", "stars": 18, "license": "None", "language": "python", "size": 997}
{"docstring": "\"\"\"\nIf not an 'include' directive is not given, all files in the\nsource directly should be added if not specified in the 'exclude'\nsection.\n\"\"\"\n", "func_signal": "def test_implicit():\n", "code": "merger = file_tree()\nmerger.remove_option('Output.js', 'first')\nmerger.remove_option('Output.js', 'last')\n\nout = merger.run(uncompressed=True, single='Output.js', strip_deps=True)\nresults = results_from_uncompressed_outfile(out[0])\nassert results == ['3rd/prototype.js', 'core/application.js', 'core/api.js', 'core/params.js']\nassert not '3rd/logger.js' in results", "path": "tests\\test_jsbuild.py", "repo_name": "whitmo/jstools", "stars": 18, "license": "None", "language": "python", "size": 997}
{"docstring": "\"\"\"\nwrites input to a tempfile, then runs yuicompressor on the tempfile\n\"\"\"\n", "func_signal": "def compress(input, args, cfg):\n", "code": "paths = list(find_paths(args, cfg))\n\nf, tpath = tempfile.mkstemp(suffix=\".js\")\nopen(tpath, \"w+\").write(input)\n\narg_string = \"java -jar %s --type=js %s\" %(paths.pop(0), tpath)\n\nnew_env = dict(os.environ)\nif len(paths):\n    new_env['CLASSPATH'] = paths.pop()\nelif not new_env.has_key(\"CLASSPATH\"):\n    info(\"No CLASSPATH found in environment or configuration\")\n\nproc = subprocess.Popen(arg_string.split(\" \"), stdout=subprocess.PIPE, env=new_env)\nout, err = proc.communicate()\nif err:\n    raise OSError(err)\npath(tpath).unlink()\nreturn out", "path": "jstools\\yuicompressor.py", "repo_name": "whitmo/jstools", "stars": 18, "license": "None", "language": "python", "size": 997}
{"docstring": "\"\"\"\ncascading lookup, first non null match wins\n\narg: jarpath, None (assume environmental variable)\narg: jarpath, classpath\nbuild config: jarpath, classpath\nuser config: jarpath, classpath\n\n@@ set up doctest runner\n\n>>> from jstools import yuicompressor as yc\n>>> from ConfigParser import ConfigParser\n>>> cp = ConfigParser()\n>>> find_paths(\"yui\", cp, limit=True)\nNone, None,\n\n>>> find_paths(\"yui:/my/yui/jar\", cp, limit=True)\nassert ret == \"/my/yui/jar\", None,\n\n>>> find_paths(\"yui:/my/yui/jar:/my/lib/jars\", cp, limit=True)\n\"/my/yui/jar\", \"/my/lib/jars\",\n\n>>> find_paths(\"yui:/my/yui/jar:/my/lib/jars:/more/lib/jars\", cp, limit=True)\n\"/my/yui/jar\", \"/my/lib/jars:/more/lib/jars\",\n\n>>> cp.add_section(\"meta\")\n>>> cp.set(\"meta\", \"jarpath\", \"/conf/jarpath\")\n>>> cp.set(\"meta\", \"classpath\", \"/conf/classpath\")\n\n>>> find_paths(\"yui:/my/yui/jar\", cp, limit=True)\n\"/my/yui/jar\", \"/conf/classpath\",\n\n>>> find_paths(\"yui\", cp, limit=True)\n\"/conf/classpath\", \"/conf/classpath\",\n\nLastly, if no jar or classpath is found in the build config or\ncommand line, we look for a global config file.  Paver's yui\ninstall must be run to insure this is setup.\n \n>>> find_paths(\"yui\", ConfigParser(), limit=True)\n\"/conf/classpath\", \"/conf/classpath\",\n\"\"\"\n\n", "func_signal": "def find_paths(args, cfg, limit=False):\n", "code": "path = None\npath = args.split(\":\")\npaths = dict(jarpath=None, classpath=None)\nif len(path) == 2:\n    paths['jarpath']=path[1]\nelif len(path)==3:\n    del path[0]\n    jarpath = path.pop(0)\n    classpath = \":\".join(path)\n    paths.update(dict(jarpath=jarpath,\n                      classpath=classpath))\n    \nif not all(paths.values()) and cfg.has_section(\"meta\"):\n    paths = nondestructive_populate(utils.SectionMap(cfg, \"meta\"), paths)\n\nif limit:\n    # mainly for testing purposes\n    return paths\n\n# move to implicit options\nif not all(paths.values()):\n    gc = utils.retrieve_config(\"yui_compressor\")\n    if gc is not None:\n        paths = nondestructive_populate(gc, paths)\n\nreturn paths['jarpath'], paths['classpath'],", "path": "jstools\\yuicompressor.py", "repo_name": "whitmo/jstools", "stars": 18, "license": "None", "language": "python", "size": 997}
{"docstring": "\"\"\"\nExtracts the list of files to be included before or after this one.\n\"\"\"\n", "func_signal": "def include(self):\n", "code": "req = getattr(self, '_include', None)\nif req is _marker:\n    self._include = [x.strip() for x in RE_INCLUDE.findall(self.source) \\\n                     if x not in self.exclude]\n                           \nreturn self._include", "path": "jstools\\merge.py", "repo_name": "whitmo/jstools", "stars": 18, "license": "None", "language": "python", "size": 997}
{"docstring": "# unpickle\n", "func_signal": "def __setstate__(self, state):\n", "code": "self.lock = threading.Lock()\nself._entries = state['entries']\nself.timeout = state['timeout']", "path": "weibopy\\cache.py", "repo_name": "wuyuntao/weibopy", "stars": 30, "license": "None", "language": "python", "size": 156}
{"docstring": "\"\"\" Returns a token from something like:\noauth_token_secret=xxx&oauth_token=xxx\n\"\"\"\n", "func_signal": "def from_string(s):\n", "code": "params = cgi.parse_qs(s, keep_blank_values=False)\nkey = params['oauth_token'][0]\nsecret = params['oauth_token_secret'][0]\ntoken = OAuthToken(key, secret)\ntry:\n    token.callback_confirmed = params['oauth_callback_confirmed'][0]\nexcept KeyError:\n    pass # 1.0, no callback confirmed.\nreturn token", "path": "weibopy\\oauth.py", "repo_name": "wuyuntao/weibopy", "stars": 30, "license": "None", "language": "python", "size": 156}
{"docstring": "\"\"\"Return a string that contains the parameters that must be signed.\"\"\"\n", "func_signal": "def get_normalized_parameters(self):\n", "code": "params = self.parameters\ntry:\n    # Exclude the signature if it exists.\n    del params['oauth_signature']\nexcept:\n    pass\n# Escape key values before sorting.\nkey_values = [(escape(_utf8_str(k)), escape(_utf8_str(v))) \\\n    for k,v in params.items()]\n# Sort lexicographically, first after key, then after value.\nkey_values.sort()\n# Combine key value pairs into a string.\nreturn '&'.join(['%s=%s' % (k, v) for k, v in key_values])", "path": "weibopy\\oauth.py", "repo_name": "wuyuntao/weibopy", "stars": 30, "license": "None", "language": "python", "size": 156}
{"docstring": "\"\"\"Verify that timestamp is recentish.\"\"\"\n", "func_signal": "def _check_timestamp(self, timestamp):\n", "code": "timestamp = int(timestamp)\nnow = int(time.time())\nlapsed = abs(now - timestamp)\nif lapsed > self.timestamp_threshold:\n    raise OAuthError('Expired timestamp: given %d and now %s has a '\n        'greater difference than threshold %d' %\n        (timestamp, now, self.timestamp_threshold))", "path": "weibopy\\oauth.py", "repo_name": "wuyuntao/weibopy", "stars": 30, "license": "None", "language": "python", "size": 156}
{"docstring": "\"\"\"Parses the URL and rebuilds it to be scheme://host/path.\"\"\"\n", "func_signal": "def get_normalized_http_url(self):\n", "code": "parts = urlparse.urlparse(self.http_url)\nscheme, netloc, path = parts[:3]\n# Exclude default port numbers.\nif scheme == 'http' and netloc[-3:] == ':80':\n    netloc = netloc[:-3]\nelif scheme == 'https' and netloc[-4:] == ':443':\n    netloc = netloc[:-4]\nreturn '%s://%s%s' % (scheme, netloc, path)", "path": "weibopy\\oauth.py", "repo_name": "wuyuntao/weibopy", "stars": 30, "license": "None", "language": "python", "size": 156}
{"docstring": "\"\"\"Serialize as post data for a POST request.\"\"\"\n", "func_signal": "def to_postdata(self):\n", "code": "return '&'.join(['%s=%s' % (escape(str(k)), escape(str(v))) \\\n    for k, v in self.parameters.iteritems()])", "path": "weibopy\\oauth.py", "repo_name": "wuyuntao/weibopy", "stars": 30, "license": "None", "language": "python", "size": 156}
{"docstring": "\"\"\"Serialize as a header for an HTTPAuth request.\"\"\"\n", "func_signal": "def to_header(self, realm=''):\n", "code": "auth_header = 'OAuth realm=\"%s\"' % realm\n# Add the oauth parameters.\nif self.parameters:\n    for k, v in self.parameters.iteritems():\n        if k[:6] == 'oauth_':\n            auth_header += ', %s=\"%s\"' % (k, escape(str(v)))\nreturn {'Authorization': auth_header}", "path": "weibopy\\oauth.py", "repo_name": "wuyuntao/weibopy", "stars": 30, "license": "None", "language": "python", "size": 156}
{"docstring": "\"\"\"Convert unicode to utf-8.\"\"\"\n", "func_signal": "def _utf8_str(s):\n", "code": "if isinstance(s, unicode):\n    return s.encode(\"utf-8\")\nelse:\n    return str(s)", "path": "weibopy\\oauth.py", "repo_name": "wuyuntao/weibopy", "stars": 30, "license": "None", "language": "python", "size": 156}
{"docstring": "\"\"\"Builds the base signature string.\"\"\"\n", "func_signal": "def build_signature(self, oauth_request, consumer, token):\n", "code": "key, raw = self.build_signature_base_string(oauth_request, consumer,\n    token)\n\n# HMAC object.\ntry:\n    import hashlib # 2.5\n    hashed = hmac.new(key, raw, hashlib.sha1)\nexcept:\n    import sha # Deprecated\n    hashed = hmac.new(key, raw, sha)\n\n# Calculate the digest base 64.\nreturn binascii.b2a_base64(hashed.digest())[:-1]", "path": "weibopy\\oauth.py", "repo_name": "wuyuntao/weibopy", "stars": 30, "license": "None", "language": "python", "size": 156}
{"docstring": "\"\"\"Processes an access_token request and returns the\naccess token on success.\n\"\"\"\n", "func_signal": "def fetch_access_token(self, oauth_request):\n", "code": "version = self._get_version(oauth_request)\nconsumer = self._get_consumer(oauth_request)\ntry:\n    verifier = self._get_verifier(oauth_request)\nexcept OAuthError:\n    verifier = None\n# Get the request token.\ntoken = self._get_token(oauth_request, 'request')\nself._check_signature(oauth_request, consumer, token)\nnew_token = self.data_store.fetch_access_token(consumer, token, verifier)\nreturn new_token", "path": "weibopy\\oauth.py", "repo_name": "wuyuntao/weibopy", "stars": 30, "license": "None", "language": "python", "size": 156}
{"docstring": "\"\"\"Turn Authorization: header into parameters.\"\"\"\n", "func_signal": "def _split_header(header):\n", "code": "params = {}\nparts = header.split(',')\nfor param in parts:\n    # Ignore realm parameter.\n    if param.find('realm') > -1:\n        continue\n    # Remove whitespace.\n    param = param.strip()\n    # Split key-value.\n    param_parts = param.split('=', 1)\n    # Remove quotes and unescape the value.\n    params[param_parts[0]] = urllib.unquote(param_parts[1].strip('\\\"'))\nreturn params", "path": "weibopy\\oauth.py", "repo_name": "wuyuntao/weibopy", "stars": 30, "license": "None", "language": "python", "size": 156}
{"docstring": "\"\"\"Turn URL string into parameters.\"\"\"\n", "func_signal": "def _split_url_string(param_str):\n", "code": "parameters = cgi.parse_qs(param_str, keep_blank_values=False)\nfor k, v in parameters.iteritems():\n    parameters[k] = urllib.unquote(v[0])\nreturn parameters", "path": "weibopy\\oauth.py", "repo_name": "wuyuntao/weibopy", "stars": 30, "license": "None", "language": "python", "size": 156}
{"docstring": "\"\"\"Processes a request_token request and returns the\nrequest token on success.\n\"\"\"\n", "func_signal": "def fetch_request_token(self, oauth_request):\n", "code": "try:\n    # Get the request token for authorization.\n    token = self._get_token(oauth_request, 'request')\nexcept OAuthError:\n    # No token required for the initial token request.\n    version = self._get_version(oauth_request)\n    consumer = self._get_consumer(oauth_request)\n    try:\n        callback = self.get_callback(oauth_request)\n    except OAuthError:\n        callback = None # 1.0, no callback specified.\n    self._check_signature(oauth_request, consumer, None)\n    # Fetch a new token.\n    token = self.data_store.fetch_request_token(consumer, callback)\nreturn token", "path": "weibopy\\oauth.py", "repo_name": "wuyuntao/weibopy", "stars": 30, "license": "None", "language": "python", "size": 156}
{"docstring": "\"\"\"Try to find the token for the provided request token key.\"\"\"\n", "func_signal": "def _get_token(self, oauth_request, token_type='access'):\n", "code": "token_field = oauth_request.get_parameter('oauth_token')\ntoken = self.data_store.lookup_token(token_type, token_field)\nif not token:\n    raise OAuthError('Invalid %s token: %s' % (token_type, token_field))\nreturn token", "path": "weibopy\\oauth.py", "repo_name": "wuyuntao/weibopy", "stars": 30, "license": "None", "language": "python", "size": 156}
{"docstring": "\"\"\"Verifies an api call and checks all the parameters.\"\"\"\n# -> consumer and token\n", "func_signal": "def verify_request(self, oauth_request):\n", "code": "version = self._get_version(oauth_request)\nconsumer = self._get_consumer(oauth_request)\n# Get the access token.\ntoken = self._get_token(oauth_request, 'access')\nself._check_signature(oauth_request, consumer, token)\nparameters = oauth_request.get_nonoauth_parameters()\nreturn consumer, token, parameters", "path": "weibopy\\oauth.py", "repo_name": "wuyuntao/weibopy", "stars": 30, "license": "None", "language": "python", "size": 156}
{"docstring": "\"\"\"Return iterator for items in each page\"\"\"\n", "func_signal": "def items(self, limit=0):\n", "code": "i = ItemIterator(self.iterator)\ni.limit = limit\nreturn i", "path": "weibopy\\cursor.py", "repo_name": "wuyuntao/weibopy", "stars": 30, "license": "None", "language": "python", "size": 156}
{"docstring": "# If authentication is required and no credentials\n# are provided, throw an error.\n", "func_signal": "def __init__(self, api, args, kargs):\n", "code": "if self.require_auth and not api.auth:\n    raise WeibopError('Authentication required!')\n\nself.api = api\nself.post_data = kargs.pop('post_data', None)\nself.retry_count = kargs.pop('retry_count', api.retry_count)\nself.retry_delay = kargs.pop('retry_delay', api.retry_delay)\nself.retry_errors = kargs.pop('retry_errors', api.retry_errors)\nself.headers = kargs.pop('headers', {})\nself.build_parameters(args, kargs)\n# Pick correct URL root to use\nif self.search_api:\n    self.api_root = api.search_root\nelse:\n    self.api_root = api.api_root\n\n# Perform any path variable substitution\nself.build_path()\n\nif api.secure:\n    self.scheme = 'https://'\nelse:\n    self.scheme = 'http://'\n\nif self.search_api:\n    self.host = api.search_host\nelse:\n    self.host = api.host\n\n# Manually set Host header to fix an issue in python 2.5\n# or older where Host is set including the 443 port.\n# This causes Twitter to issue 301 redirect.\n# See Issue http://github.com/joshthecoder/tweepy/issues/#issue/12\nself.headers['Host'] = self.host", "path": "weibopy\\binder.py", "repo_name": "wuyuntao/weibopy", "stars": 30, "license": "None", "language": "python", "size": 156}
{"docstring": "\"\"\"Return iterator for pages\"\"\"\n", "func_signal": "def pages(self, limit=0):\n", "code": "if limit > 0:\n    self.iterator.limit = limit\nreturn self.iterator", "path": "weibopy\\cursor.py", "repo_name": "wuyuntao/weibopy", "stars": 30, "license": "None", "language": "python", "size": 156}
{"docstring": "\"\"\"Get any non-OAuth parameters.\"\"\"\n", "func_signal": "def get_nonoauth_parameters(self):\n", "code": "parameters = {}\nfor k, v in self.parameters.iteritems():\n    # Ignore oauth parameters.\n    if k.find('oauth_') < 0:\n        parameters[k] = v\nreturn parameters", "path": "weibopy\\oauth.py", "repo_name": "wuyuntao/weibopy", "stars": 30, "license": "None", "language": "python", "size": 156}
{"docstring": "\"\"\"Verify that the nonce is uniqueish.\"\"\"\n", "func_signal": "def _check_nonce(self, consumer, token, nonce):\n", "code": "nonce = self.data_store.lookup_nonce(consumer, token, nonce)\nif nonce:\n    raise OAuthError('Nonce already used: %s' % str(nonce))", "path": "weibopy\\oauth.py", "repo_name": "wuyuntao/weibopy", "stars": 30, "license": "None", "language": "python", "size": 156}
{"docstring": "\"\"\"Continually handle events until an error occurs\"\"\"\n", "func_signal": "def run(self):\n", "code": "while True:\n    # Look for new chat requests and notify all online\n    # localusers of each one\n    for chat in self._getchatswithstatus(self.STATUS_WAITING):\n        availableusers = self._getavailablelocalusers()\n        if availableusers:\n            message = \"Remote user '%s' wants to start a conversation.\" % chat.remoteuser\n            if chat.startmessage:\n                message += \" The starting message is: '%s'\" % chat.startmessage\n            message += \" To accept this request, reply with the message '!ACCEPT %d'.\" % chat.chatid\n            for localuser in availableusers:\n                basemessage = message\n                if len(availableusers) > 1:\n                    message += \" Requests were also sent to: %s.\" % ', '.join(user for user in availableusers if user != localuser)\n                self._localsend(localuser, message)\n                MODULELOG.info(\"A chat request from %s was sent to %s\" % (chat.remoteuser, localuser))\n                message = basemessage\n            self._setchatstatus(chat.chatid, self.STATUS_NOTIFIED)\n        else:\n            self._queueremote(chat.chatid, \"No one is available to answer your chat request right now.\")\n            self._setchatstatus(chat.chatid, self.STATUS_FAILED)\n\n    # Look for new queued messages for localusers and send them\n    for message in self._getallqueuedlocalmessages():\n        self._localsend(message.localuser, message.message)\n        MODULELOG.info(\"%s said to %s in chat #%d: '%s'\", message.remoteuser, message.localuser, message.chatid, message.message)\n        self._markmessagesent(message.messageid)\n    \n    if self.client.Process(1) == 0:\n        MODULELOG.info(\"Disconnected from the server. Reconnecting soon.\")\n        time.sleep(20)\n        self._connect()", "path": "seshat\\server.py", "repo_name": "kstrauser/seshat", "stars": 18, "license": "None", "language": "python", "size": 114}
{"docstring": "\"\"\"Issue a new chat request and return its chatid\"\"\"\n", "func_signal": "def _openchat(self, remoteuser, message=''):\n", "code": "self.dbconn.execute(\"INSERT INTO chat (remoteuser, starttime, status, startmessage) VALUES (?, ?, ?, ?)\",\n                    (remoteuser, time.time(), self.STATUS_WAITING, message))\nchatid = self.dbconn.execute(\"SELECT last_insert_rowid()\").fetchone()[0]\nself.dbconn.commit()\nself._queueremote(chatid, \"Your chat request has been sent. Please wait while it is answered.\")\nreturn chatid", "path": "seshat\\sqlitebackend.py", "repo_name": "kstrauser/seshat", "stars": 18, "license": "None", "language": "python", "size": 114}
{"docstring": "\"\"\"See: sqlitebackend.ChatInfo.__init__.__doc__\"\"\"\n", "func_signal": "def __init__(self, function, pattern, helptext):\n", "code": "self.function = function\nself.pattern = pattern\nself.helptext = helptext", "path": "seshat\\server.py", "repo_name": "kstrauser/seshat", "stars": 18, "license": "None", "language": "python", "size": 114}
{"docstring": "\"\"\"See: ChatInfo.__init__.__doc__\"\"\"\n", "func_signal": "def __init__(self, chatid, localuser, remoteuser, messageid, message):\n", "code": "self.chatid = chatid\nself.localuser = localuser\nself.remoteuser = remoteuser\nself.messageid = messageid\nself.message = message", "path": "seshat\\sqlitebackend.py", "repo_name": "kstrauser/seshat", "stars": 18, "license": "None", "language": "python", "size": 114}
{"docstring": "\"\"\"Open a chat and set its localuser to the given value\"\"\"\n", "func_signal": "def _acceptchat(self, chatid, localuser):\n", "code": "self.dbconn.execute(\"UPDATE chat SET status = ?, localuser = ? WHERE chatid = ?\",\n                    (self.STATUS_OPEN, localuser, chatid))\nself.dbconn.commit()", "path": "seshat\\sqlitebackend.py", "repo_name": "kstrauser/seshat", "stars": 18, "license": "None", "language": "python", "size": 114}
{"docstring": "\"\"\"Establish a database connection and create the tables\nnecessary tables if they don't already exist\"\"\"\n", "func_signal": "def __init__(self, sqlitedb):\n", "code": "self.dbconn = sqlite3.connect(sqlitedb)\ninitdb = False\ntry:\n    versionquery = self.dbconn.execute('SELECT version FROM dbversion WHERE versionid = 1')\nexcept sqlite3.OperationalError:\n    # If the 'version' table doesn't exist, then this is a new\n    # database and needs to be initialized\n    MODULELOG.info('Unable to find the database version table')\n    initdb = True\nelse:\n    dbversionrow = versionquery.fetchone()\n    if dbversionrow is None:\n        MODULELOG.info('Unable to read the database version')\n        # A previous initialization didn't get as far as\n        # setting the database version. Wierd, but let's\n        # handle it rationally.\n        initdb = True\n    else:\n        dbversion = dbversionrow[0]\n        if dbversion < CURRENTDBVERSION:\n            localqueuesize = self.dbconn.execute('SELECT count(1) FROM localmessagequeue WHERE sendtime IS NULL').fetchone()[0]\n            remotequeuesize = self.dbconn.execute('SELECT count(1) FROM remotemessagequeue WHERE sendtime IS NULL').fetchone()[0]\n            unclosedchatcount = self.dbconn.execute('SELECT count(1) FROM chat WHERE STATUS IN (?, ?, ?)',\n                                                    (self.STATUS_WAITING, self.STATUS_NOTIFIED, self.STATUS_OPEN)).fetchone()[0]\n            message = 'The Seshat database (%s) is out of date. It has %d queued incoming message(s), %d queued outgoing message(s), and %s unclosed chats.' % (\n                sqlitedb,\n                localqueuesize,\n                remotequeuesize,\n                unclosedchatcount)\n            if localqueuesize or remotequeuesize or unclosedchatcount:\n                message += ' Only delete the database file if you are willing to lose the unsent messages and open chats.'\n            else:\n                message += ' You may safely delete the current database file. It will be created automatically the next time you launch the server.'\n            MODULELOG.critical(message)\n            sys.exit(-1)\nif initdb:\n    MODULELOG.info('Creating and populating the database')\n    for query in CREATEQUERIES:\n        try:\n            self.dbconn.execute(query)\n        except sqlite3.OperationalError:\n            pass\n        else:\n            MODULELOG.debug('Executed: %s', query)", "path": "seshat\\sqlitebackend.py", "repo_name": "kstrauser/seshat", "stars": 18, "license": "None", "language": "python", "size": 114}
{"docstring": "\"\"\"Send a web message to the chat's remoteuser\"\"\"\n", "func_signal": "def _queueremote(self, chatid, message):\n", "code": "self.dbconn.execute(\"INSERT INTO remotemessagequeue (posttime, chatid, message) VALUES (?, ?, ?)\",\n                    (time.time(), chatid, message))\nself.dbconn.commit()", "path": "seshat\\sqlitebackend.py", "repo_name": "kstrauser/seshat", "stars": 18, "license": "None", "language": "python", "size": 114}
{"docstring": "\"\"\"Launch a broker bot using settings found in the named\nconfiguration (.ini) file, in the specified section\"\"\"\n", "func_signal": "def main(configfile, section):\n", "code": "import ConfigParser\n\nlogging.basicConfig()\nlogging.getLogger('').setLevel(logging.DEBUG)\nconfig = ConfigParser.ConfigParser()\nconfig.read(configfile)\nsetting = {}\n\n# Prefer keys named like \"seshat_username\", but fall back to\n# \"username\" if they don't exist. This is so that Seshat settings\n# can be embedded in Pyramid config files with little risk of\n# conflicts.\nfor key in ('username', 'password', 'localusers', 'sqlitedb'):\n    try:\n        setting[key] = config.get(section, 'seshat_%s' % key)\n    except ConfigParser.NoOptionError:\n        setting[key] = config.get(section, '%s' % key)\nsetting['localusers'] = [localuser.strip() for localuser in setting['localusers'].split(',')]\nSeshatServer(setting['username'], setting['password'], setting['localusers'], setting['sqlitedb']).run()", "path": "seshat\\server.py", "repo_name": "kstrauser/seshat", "stars": 18, "license": "None", "language": "python", "size": 114}
{"docstring": "\"\"\"Establish a connection to a Jabber server and prepare to\nmanage it\"\"\"\n", "func_signal": "def __init__(self, username, password, localusers, sqlitedb):\n", "code": "super(SeshatServer, self).__init__(sqlitedb)\n\nself.localusers = localusers\nself.password = password\nself.jid = xmpp.protocol.JID(username)\nself.xmppserver = self.jid.getDomain()\n\n# Set all users to offline. Their real status will be updated\n# by _presencehandler as soon as we connect and send our\n# presence.\nself._clearonlineusers()\nself.onlineresource = {}\nfor localuser in self.localusers:\n    self.onlineresource[localuser] = {}\n\n# Establish a Jabber connection\nself.client = xmpp.Client(self.xmppserver, debug=[])\nself._connect()\nself.client.RegisterHandler('message', self._messagehandler)\nself.client.RegisterHandler('presence', self._presencehandler)", "path": "seshat\\server.py", "repo_name": "kstrauser/seshat", "stars": 18, "license": "None", "language": "python", "size": 114}
{"docstring": "\"\"\"Update (or store) the number of accounts where the\nlocaluser is online. For example, they might be online with\nboth their desktop and laptop.\"\"\"\n", "func_signal": "def _setonlinestatus(self, localuser, resource, online):\n", "code": "online = int(online)\ntry:\n    self.dbconn.execute(\"INSERT INTO onlinestatus (localuser, resource, online) VALUES (?, ?, ?)\", (localuser, resource, online))\nexcept sqlite3.IntegrityError:\n    self.dbconn.execute(\"UPDATE onlinestatus SET online = ? WHERE localuser = ? AND resource = ?\", (online, localuser, resource))\nself.dbconn.commit()", "path": "seshat\\sqlitebackend.py", "repo_name": "kstrauser/seshat", "stars": 18, "license": "None", "language": "python", "size": 114}
{"docstring": "\"\"\"This would be a namedtuple, but those aren't available in\nall the versions of Python that Pyramid supports and I'd hate\nto leave someone out over something so trivial.\"\"\"\n", "func_signal": "def __init__(self, chatid, localuser, remoteuser, starttime, endtime, status, startmessage):\n", "code": "self.chatid = chatid\nself.localuser = localuser\nself.remoteuser = remoteuser\nself.starttime = starttime\nself.endtime = endtime\nself.status = status\nself.startmessage = startmessage", "path": "seshat\\sqlitebackend.py", "repo_name": "kstrauser/seshat", "stars": 18, "license": "None", "language": "python", "size": 114}
{"docstring": "\"\"\"!HELP - Show available commands\"\"\"\n", "func_signal": "def _command_help(self, localuser):\n", "code": "self._localsend(localuser,\n                \"Available options:\\n\" + '\\n'.join(sorted(pattern.helptext for pattern in COMMANDPATTERNS)))", "path": "seshat\\server.py", "repo_name": "kstrauser/seshat", "stars": 18, "license": "None", "language": "python", "size": 114}
{"docstring": "\"\"\"!FINISH - Close your current chat\"\"\"\n", "func_signal": "def _command_finish(self, localuser):\n", "code": "currentchat = self._getlocaluserchat(localuser)\nif currentchat is None:\n    self._replywithhelp(localuser, \"You are not currently active in a chat.\")\n    return\nself._closechat(currentchat.chatid, self.STATUS_CLOSED)\nself._localsend(localuser, \"The chat is now closed.\")\nself._queueremote(currentchat.chatid, \"The chat is now closed.\")", "path": "seshat\\server.py", "repo_name": "kstrauser/seshat", "stars": 18, "license": "None", "language": "python", "size": 114}
{"docstring": "\"\"\"!CANCEL n - Cancel chat request #n\"\"\"\n", "func_signal": "def _command_cancel(self, localuser, chatid):\n", "code": "chatid = int(chatid)\nchatinfo = self._getchatinfo(chatid)\nif chatinfo is None:\n    self._replywithhelp(localuser, \"Chat #%d does not exist.\" % chatid)\n    return\nif chatinfo.status == self.STATUS_OPEN:\n    if chatinfo.localuser == localuser:\n        self._replywithhelp(localuser, \"You have already accepted chat #%d. Send '!FINISH' to close it.\" % chatid)\n    else:\n        self._replywithhelp(localuser, \"Chat #%d has already been accepted by %s.\" % (chatid, chatinfo.localuser))\n    return\nif chatinfo.status == self.STATUS_CLOSED:\n    self._replywithhelp(localuser, \"Chat #%d is already closed.\" % chatid)\n    return\nself._closechat(chatid, self.STATUS_CANCELEDLOCALLY)\nself._localsend(localuser, \"You canceled chat #%d.\" % chatid)\nself._queueremote(chatid, \"Your chat was canceled.\")\nMODULELOG.info(\"%s canceled chat #%d\" % (localuser, chatid))", "path": "seshat\\server.py", "repo_name": "kstrauser/seshat", "stars": 18, "license": "None", "language": "python", "size": 114}
{"docstring": "\"\"\"Return information about the localuser's current open chat,\nif any (otherwise None)\"\"\"\n", "func_signal": "def _getlocaluserchat(self, localuser):\n", "code": "row = self.dbconn.execute(\"SELECT chatid, localuser, remoteuser, starttime, endtime, status, startmessage FROM chat WHERE localuser = ? AND status = ?\",\n                          (localuser, self.STATUS_OPEN)).fetchone()\nif row is None:\n    return None\nreturn ChatInfo(*row)", "path": "seshat\\sqlitebackend.py", "repo_name": "kstrauser/seshat", "stars": 18, "license": "None", "language": "python", "size": 114}
{"docstring": "\"\"\"Return all the stored information about a chat\"\"\"\n", "func_signal": "def _getchatinfo(self, chatid):\n", "code": "row = self.dbconn.execute(\"SELECT chatid, localuser, remoteuser, starttime, endtime, status, startmessage FROM chat WHERE chatid = ?\",\n                          (chatid,)).fetchone()\nif row is None:\n    return None\nreturn ChatInfo(*row)", "path": "seshat\\sqlitebackend.py", "repo_name": "kstrauser/seshat", "stars": 18, "license": "None", "language": "python", "size": 114}
{"docstring": "\"\"\"Return the (possibly empty) list of messages queued for\ndelivery to localusers\"\"\"\n", "func_signal": "def _getallqueuedlocalmessages(self):\n", "code": "rows = self.dbconn.execute(\"SELECT chat.chatid, chat.localuser, chat.remoteuser, localmessagequeue.messageid, localmessagequeue.message FROM chat JOIN localmessagequeue ON chat.chatid = localmessagequeue.chatid WHERE localmessagequeue.sendtime IS NULL AND chat.localuser IS NOT NULL ORDER BY localmessagequeue.messageid\").fetchall()\nif rows is None:\n    return []\nreturn [QueuedMessage(*row) for row in rows]", "path": "seshat\\sqlitebackend.py", "repo_name": "kstrauser/seshat", "stars": 18, "license": "None", "language": "python", "size": 114}
{"docstring": "\"\"\"Record the time that the given message was sent\"\"\"\n", "func_signal": "def _markmessagesent(self, messageid):\n", "code": "self.dbconn.execute(\"UPDATE localmessagequeue SET sendtime = ? WHERE messageid = ?\", (time.time(), messageid))\nself.dbconn.commit()", "path": "seshat\\sqlitebackend.py", "repo_name": "kstrauser/seshat", "stars": 18, "license": "None", "language": "python", "size": 114}
{"docstring": "\"\"\"Connect to the Jabber server\"\"\"\n", "func_signal": "def _connect(self):\n", "code": "self.client.connect()\nself.client.auth(self.jid.getNode(), self.password)\nself.client.sendInitPresence()", "path": "seshat\\server.py", "repo_name": "kstrauser/seshat", "stars": 18, "license": "None", "language": "python", "size": 114}
{"docstring": "\"\"\"Mark the chat as closed with the given status code\"\"\"\n", "func_signal": "def _closechat(self, chatid, status):\n", "code": "self.dbconn.execute(\"UPDATE chat SET status = ?, endtime = ? WHERE chatid = ?\", (status, time.time(), chatid))\nself.dbconn.commit()\nchatinfo = self._getchatinfo(chatid)\nMODULELOG.info(\"Chat #%d between %s and %s is closed.\" % (chatid, chatinfo.localuser, chatinfo.remoteuser))", "path": "seshat\\sqlitebackend.py", "repo_name": "kstrauser/seshat", "stars": 18, "license": "None", "language": "python", "size": 114}
{"docstring": "\"\"\" Break cyclic dependancies to let python's GC free memory right now.\"\"\"\n", "func_signal": "def _destroy_socket(self):\n", "code": "self.Stream.dispatch=None\nself.Stream.stream_footer_received=None\nself.Stream.stream_header_received=None\nself.Stream.destroy()\nself._sock.close()\nself.set_socket_state(SOCKET_DEAD)", "path": "howie\\frontends\\xmpp\\session.py", "repo_name": "linzhp/Howie", "stars": 29, "license": "None", "language": "python", "size": 6806}
{"docstring": "\"\"\" Initialise internal variables. \"\"\"\n", "func_signal": "def __init__(self):\n", "code": "PlugIn.__init__(self)\nself.DBG_LINE='ibb'\nself._exported_methods=[self.OpenStream]\nself._streams={}\nself._ampnode=Node(NS_AMP+' amp',payload=[Node('rule',{'condition':'deliver-at','value':'stored','action':'error'}),Node('rule',{'condition':'match-resource','value':'exact','action':'error'})])", "path": "howie\\frontends\\xmpp\\filetransfer.py", "repo_name": "linzhp/Howie", "stars": 29, "license": "None", "language": "python", "size": 6806}
{"docstring": "\"\"\" Handle stream closure due to all data transmitted.\n    Raise xmpppy event specifying successfull data receive. \"\"\"\n", "func_signal": "def StreamCloseHandler(self,conn,stanza):\n", "code": "sid=stanza.getTagAttr('close','sid')\nself.DEBUG('StreamCloseHandler called sid->%s'%sid,'info')\nif sid in self._streams.keys():\n    conn.send(stanza.buildReply('result'))\n    conn.Event(self.DBG_LINE,'SUCCESSFULL RECEIVE',self._streams[sid])\n    del self._streams[sid]\nelse: conn.send(Error(stanza,ERR_ITEM_NOT_FOUND))", "path": "howie\\frontends\\xmpp\\filetransfer.py", "repo_name": "linzhp/Howie", "stars": 29, "license": "None", "language": "python", "size": 6806}
{"docstring": "\"\"\" Caches proxy and target addresses.\n    'proxy' argument is a dictionary with mandatory keys 'host' and 'port' (proxy address)\n    and optional keys 'user' and 'password' to use for authentication.\n    'server' argument is a tuple of host and port - just like TCPsocket uses. \"\"\"\n", "func_signal": "def __init__(self,proxy,server,use_srv=True):\n", "code": "TCPsocket.__init__(self,server,use_srv)\nself.DBG_LINE=DBG_CONNECT_PROXY\nself._proxy=proxy", "path": "howie\\frontends\\xmpp\\transports.py", "repo_name": "linzhp/Howie", "stars": 29, "license": "None", "language": "python", "size": 6806}
{"docstring": "\"\"\" Handles streams state change. Used internally. \"\"\"\n", "func_signal": "def IqHandler(self,conn,stanza):\n", "code": "typ=stanza.getType()\nself.DEBUG('IqHandler called typ->%s'%typ,'info')\nif typ=='set' and stanza.getTag('open',namespace=NS_IBB): self.StreamOpenHandler(conn,stanza)\nelif typ=='set' and stanza.getTag('close',namespace=NS_IBB): self.StreamCloseHandler(conn,stanza)\nelif typ=='result': self.StreamCommitHandler(conn,stanza)\nelif typ=='error': self.StreamOpenReplyHandler(conn,stanza)\nelse: conn.send(Error(stanza,ERR_BAD_REQUEST))\nraise NodeProcessed", "path": "howie\\frontends\\xmpp\\filetransfer.py", "repo_name": "linzhp/Howie", "stars": 29, "license": "None", "language": "python", "size": 6806}
{"docstring": "\"\"\" This callback is used to detect the stream namespace of incoming stream. Used internally. \"\"\"\n", "func_signal": "def _catch_stream_id(self,ns=None,tag='stream',attrs={}):\n", "code": "if not attrs.has_key('id') or not attrs['id']:\n    return self.terminate_stream(STREAM_INVALID_XML)\nself.ID=attrs['id']\nif not attrs.has_key('version'): self._owner.Dialback(self)", "path": "howie\\frontends\\xmpp\\session.py", "repo_name": "linzhp/Howie", "stars": 29, "license": "None", "language": "python", "size": 6806}
{"docstring": "\"\"\" Used to analyse server <features/> tag for TLS support.\n    If TLS is supported starts the encryption negotiation. Used internally\"\"\"\n", "func_signal": "def FeaturesHandler(self, conn, feats):\n", "code": "if not feats.getTag('starttls',namespace=NS_TLS):\n    self.DEBUG(\"TLS unsupported by remote server.\",'warn')\n    return\nself.DEBUG(\"TLS supported by remote server. Requesting TLS start.\",'ok')\nself._owner.RegisterHandlerOnce('proceed',self.StartTLSHandler,xmlns=NS_TLS)\nself._owner.RegisterHandlerOnce('failure',self.StartTLSHandler,xmlns=NS_TLS)\nself._owner.Connection.send('<starttls xmlns=\"%s\"/>'%NS_TLS)\nraise NodeProcessed", "path": "howie\\frontends\\xmpp\\transports.py", "repo_name": "linzhp/Howie", "stars": 29, "license": "None", "language": "python", "size": 6806}
{"docstring": "\"\"\" Change the session state.\n    Session starts with SESSION_NOT_AUTHED state\n    and then comes through \n    SESSION_AUTHED, SESSION_BOUND, SESSION_OPENED and SESSION_CLOSED states.\n\"\"\"\n", "func_signal": "def set_session_state(self,newstate):\n", "code": "if self._session_state<newstate:\n    if self._session_state<SESSION_AUTHED and \\\n       newstate>=SESSION_AUTHED: self._stream_pos_queued=self._stream_pos_sent\n    self._session_state=newstate", "path": "howie\\frontends\\xmpp\\session.py", "repo_name": "linzhp/Howie", "stars": 29, "license": "None", "language": "python", "size": 6806}
{"docstring": "\"\"\" Start new stream. You should provide stream id 'sid', the endpoind jid 'to',\n    the file object containing info for send 'fp'. Also the desired blocksize can be specified.\n    Take into account that recommended stanza size is 4k and IBB uses base64 encoding\n    that increases size of data by 1/3.\"\"\"\n", "func_signal": "def OpenStream(self,sid,to,fp,blocksize=3000):\n", "code": "if sid in self._streams.keys(): return\nif not JID(to).getResource(): return\nself._streams[sid]={'direction':'|>'+to,'block-size':blocksize,'fp':fp,'seq':0}\nself._owner.RegisterCycleHandler(self.SendHandler)\nsyn=Protocol('iq',to,'set',payload=[Node(NS_IBB+' open',{'sid':sid,'block-size':blocksize})])\nself._owner.send(syn)\nself._streams[sid]['syn_id']=syn.getID()\nreturn self._streams[sid]", "path": "howie\\frontends\\xmpp\\filetransfer.py", "repo_name": "linzhp/Howie", "stars": 29, "license": "None", "language": "python", "size": 6806}
{"docstring": "\"\"\" Write the closing stream tag and destroy the underlaying socket. Used internally. \"\"\"\n", "func_signal": "def _stream_close(self,unregister=1):\n", "code": "if self._stream_state>=STREAM__CLOSED: return\nself.set_stream_state(STREAM__CLOSING)\nself.sendnow('</stream:stream>')\nself.set_stream_state(STREAM__CLOSED)\nself.push_queue()       # decompose queue really since STREAM__CLOSED\nself._owner.flush_queues()\nif unregister: self._owner.unregistersession(self)\nself._destroy_socket()", "path": "howie\\frontends\\xmpp\\session.py", "repo_name": "linzhp/Howie", "stars": 29, "license": "None", "language": "python", "size": 6806}
{"docstring": "\"\"\" This method is used to initialise the internal xml expat parser\n    and to send initial stream header (in case of client connection).\n    Should be used after initial connection and after every stream restart.\"\"\"\n", "func_signal": "def StartStream(self):\n", "code": "self._stream_state=STREAM__NOT_OPENED\nself.Stream=simplexml.NodeBuilder()\nself.Stream._dispatch_depth=2\nself.Stream.dispatch=self._dispatch\nself.Parse=self.Stream.Parse\nself.Stream.stream_footer_received=self._stream_close\nif self.TYP=='client':\n    self.Stream.stream_header_received=self._catch_stream_id\n    self._stream_open()\nelse:\n    self.Stream.stream_header_received=self._stream_open", "path": "howie\\frontends\\xmpp\\session.py", "repo_name": "linzhp/Howie", "stars": 29, "license": "None", "language": "python", "size": 6806}
{"docstring": "\"\"\" Handle remote side reply about is it agree or not to receive our datastream.\n    Used internally. Raises xmpppy event specfiying if the data transfer\n    is agreed upon.\"\"\"\n", "func_signal": "def StreamOpenReplyHandler(self,conn,stanza):\n", "code": "syn_id=stanza.getID()\nself.DEBUG('StreamOpenReplyHandler called syn_id->%s'%syn_id,'info')\nfor sid in self._streams.keys():\n    stream=self._streams[sid]\n    if stream['syn_id']==syn_id:\n        if stanza.getType()=='error':\n            if stream['direction'][0]=='<': conn.Event(self.DBG_LINE,'ERROR ON RECEIVE',stream)\n            else: conn.Event(self.DBG_LINE,'ERROR ON SEND',stream)\n            del self._streams[sid]\n        elif stanza.getType()=='result':\n            if stream['direction'][0]=='|':\n                stream['direction']=stream['direction'][1:]\n                conn.Event(self.DBG_LINE,'STREAM COMMITTED',stream)\n            else: conn.send(Error(stanza,ERR_UNEXPECTED_REQUEST))", "path": "howie\\frontends\\xmpp\\filetransfer.py", "repo_name": "linzhp/Howie", "stars": 29, "license": "None", "language": "python", "size": 6806}
{"docstring": "\"\"\" Takes Protocol instance as argument.\n    Puts stanza into \"send\" fifo queue. Items into the send queue are hold until\n    stream authenticated. After that this method is effectively the same as \"sendnow\" method.\"\"\"\n", "func_signal": "def enqueue(self,stanza):\n", "code": "if isinstance(stanza,Protocol):\n    self.stanza_queue.append(stanza)\nelse: self.sendbuffer+=stanza\nif self._socket_state>=SOCKET_ALIVE: self.push_queue()", "path": "howie\\frontends\\xmpp\\session.py", "repo_name": "linzhp/Howie", "stars": 29, "license": "None", "language": "python", "size": 6806}
{"docstring": "\"\"\" If the 'now' argument is true then starts using encryption immidiatedly.\n    If 'now' in false then starts encryption as soon as TLS feature is\n    declared by the server (if it were already declared - it is ok).\n\"\"\"\n", "func_signal": "def PlugIn(self,owner,now=0):\n", "code": "if owner.__dict__.has_key('TLS'): return  # Already enabled.\nPlugIn.PlugIn(self,owner)\nDBG_LINE='TLS'\nif now: return self._startSSL()\nif self._owner.Dispatcher.Stream.features:\n    try: self.FeaturesHandler(self._owner.Dispatcher,self._owner.Dispatcher.Stream.features)\n    except NodeProcessed: pass\nelse: self._owner.RegisterHandlerOnce('features',self.FeaturesHandler,xmlns=NS_STREAMS)\nself.starttls=None", "path": "howie\\frontends\\xmpp\\transports.py", "repo_name": "linzhp/Howie", "stars": 29, "license": "None", "language": "python", "size": 6806}
{"docstring": "\"\"\" Declare some feature as \"negotiating now\" to prevent other features from start negotiating. \"\"\"\n", "func_signal": "def start_feature(self,f):\n", "code": "if self.feature_in_process: raise \"Starting feature %s over %s !\"%(f,self.feature_in_process)\nself.feature_in_process=f", "path": "howie\\frontends\\xmpp\\session.py", "repo_name": "linzhp/Howie", "stars": 29, "license": "None", "language": "python", "size": 6806}
{"docstring": "\"\"\" Notify the peer about stream closure.\n    Ensure that xmlstream is not brokes - i.e. if the stream isn't opened yet -\n    open it before closure.\n    If the error condition is specified than create a stream error and send it along with\n    closing stream tag.\n    Emulate receiving 'unavailable' type presence just before stream closure.\n\"\"\"\n", "func_signal": "def terminate_stream(self,error=None,unregister=1):\n", "code": "if self._stream_state>=STREAM__CLOSING: return\nif self._stream_state<STREAM__OPENED:\n    self.set_stream_state(STREAM__CLOSING)\n    self._stream_open()\nelse:\n    self.set_stream_state(STREAM__CLOSING)\n    p=Presence(typ='unavailable')\n    p.setNamespace(NS_CLIENT)\n    self._dispatch(p,trusted=1)\nif error:\n    if isinstance(error,Node): self.sendnow(error)\n    else: self.sendnow(ErrorNode(error))\nself._stream_close(unregister=unregister)\nif self.slave_session:\n    self.slave_session.terminate_stream(STREAM_REMOTE_CONNECTION_FAILED)", "path": "howie\\frontends\\xmpp\\session.py", "repo_name": "linzhp/Howie", "stars": 29, "license": "None", "language": "python", "size": 6806}
{"docstring": "\"\"\" Closes the socket. \"\"\"\n", "func_signal": "def disconnect(self):\n", "code": "self.DEBUG(\"Closing socket\",'stop')\nself._sock.close()", "path": "howie\\frontends\\xmpp\\transports.py", "repo_name": "linzhp/Howie", "stars": 29, "license": "None", "language": "python", "size": 6806}
{"docstring": "\"\"\" Unregisters TLS handler's from owner's dispatcher. Take note that encription\n    can not be stopped once started. You can only break the connection and start over.\"\"\"\n", "func_signal": "def plugout(self,now=0):\n", "code": "self._owner.UnregisterHandler('features',self.FeaturesHandler,xmlns=NS_STREAMS)\nself._owner.UnregisterHandlerOnce('proceed',self.StartTLSHandler,xmlns=NS_TLS)\nself._owner.UnregisterHandlerOnce('failure',self.StartTLSHandler,xmlns=NS_TLS)", "path": "howie\\frontends\\xmpp\\transports.py", "repo_name": "linzhp/Howie", "stars": 29, "license": "None", "language": "python", "size": 6806}
{"docstring": "\"\"\" Declare some stream feature as activated one. \"\"\"\n", "func_signal": "def feature(self,feature):\n", "code": "if feature not in self.features: self.features.append(feature)\nself.unfeature(feature)", "path": "howie\\frontends\\xmpp\\session.py", "repo_name": "linzhp/Howie", "stars": 29, "license": "None", "language": "python", "size": 6806}
{"docstring": "\"\"\" Disconnect from the remote server and unregister self.disconnected method from\n    the owner's dispatcher. \"\"\"\n", "func_signal": "def plugout(self):\n", "code": "self._sock.close()\nif self._owner.__dict__.has_key('Connection'):\n    del self._owner.Connection\n    self._owner.UnregisterDisconnectHandler(self.disconnected)", "path": "howie\\frontends\\xmpp\\transports.py", "repo_name": "linzhp/Howie", "stars": 29, "license": "None", "language": "python", "size": 6806}
{"docstring": "\"\"\"\nReturn the value of the partition function at the specified temperatures\n`Tlist` in K. The formula is\n\n.. math:: q_\\\\mathrm{vib}(T) = \\\\prod_i \\\\frac{1}{1 - e^{-\\\\xi_i}}\n\nwhere :math:`\\\\xi_i \\\\equiv h \\\\nu_i / k_\\\\mathrm{B} T`,\n:math:`T` is temperature, :math:`\\\\nu_i` is the frequency of vibration\n:math:`i`, :math:`k_\\\\mathrm{B}` is the Boltzmann constant, :math:`h`\nis the Planck constant, and :math:`R` is the gas law constant. Note\nthat we have chosen our zero of energy to be at the zero-point energy\nof the molecule, *not* the bottom of the potential well.\n\"\"\"\n", "func_signal": "def getPartitionFunction(self, T):\n", "code": "cython.declare(Q=cython.double, freq=cython.double)\nQ = 1.0\nfor freq in self.frequencies:\n    Q = Q / (1 - numpy.exp(-freq / (0.695039 * T)))  # kB = 0.695039 cm^-1/K\nreturn Q", "path": "chempy\\states.py", "repo_name": "jwallen/ChemPy", "stars": 22, "license": "mit", "language": "python", "size": 447}
{"docstring": "\"\"\"\nReturn the contribution to the entropy due to vibration in J/mol*K at\nthe specified temperatures `Tlist` in K. The formula is\n\n.. math:: \\\\frac{S^\\\\mathrm{vib}(T)}{R} = \\\\sum_i \\\\left[ - \\\\ln \\\\left(1 - e^{-\\\\xi_i} \\\\right) + \\\\frac{\\\\xi_i}{e^{\\\\xi_i} - 1} \\\\right]\n\nwhere :math:`\\\\xi_i \\\\equiv h \\\\nu_i / k_\\\\mathrm{B} T`,\n:math:`T` is temperature, :math:`\\\\nu_i` is the frequency of vibration\n:math:`i`, :math:`k_\\\\mathrm{B}` is the Boltzmann constant, :math:`h`\nis the Planck constant, and :math:`R` is the gas law constant.\n\"\"\"\n", "func_signal": "def getEntropy(self, T):\n", "code": "cython.declare(S=cython.double, freq=cython.double)\ncython.declare(x=cython.double, exp_x=cython.double)\nS = numpy.log(self.getPartitionFunction(T))\nfor freq in self.frequencies:\n    x = freq / (0.695039 * T)\t# kB = 0.695039 cm^-1/K\n    exp_x = numpy.exp(x)\n    S = S + x / (exp_x - 1)\nreturn S * constants.R", "path": "chempy\\states.py", "repo_name": "jwallen/ChemPy", "stars": 22, "license": "mit", "language": "python", "size": 447}
{"docstring": "\"\"\"\nReturn the density of states at the specified energlies `Elist` in J/mol\nabove the ground state. For the cosine potential, the formula is\n\n.. math:: \\\\rho(E) = \\\\frac{2 q_\\\\mathrm{1f}}{\\\\pi^{3/2} V_0^{1/2}} \\\\mathcal{K}(E / V_0) \\\\hspace{20pt} E < V_0\n\nand\n\n.. math:: \\\\rho(E) = \\\\frac{2 q_\\\\mathrm{1f}}{\\\\pi^{3/2} E^{1/2}} \\\\mathcal{K}(V_0 / E) \\\\hspace{20pt} E > V_0\n\nwhere\n\n.. math:: q_\\\\mathrm{1f} = \\\\frac{\\\\pi^{1/2}}{\\\\sigma} \\\\left( \\\\frac{8 \\\\pi^2 I}{h^2} \\\\right)^{1/2}\n\n:math:`E` is energy, :math:`V_0` is barrier height, and\n:math:`\\\\mathcal{K}(x)` is the complete elliptic integral of the first\nkind. There is currently no functionality for using the Fourier series\npotential.\n\"\"\"\n", "func_signal": "def getDensityOfStates(self, Elist):\n", "code": "cython.declare(rho=numpy.ndarray, q1f=cython.double, pre=cython.double, V0=cython.double, i=cython.int)\nrho = numpy.zeros_like(Elist)\nq1f = math.sqrt(8 * math.pi * math.pi * math.pi * self.inertia / constants.h / constants.h / constants.Na) / self.symmetry\nV0 = self.barrier\npre = 2.0 * q1f / math.sqrt(math.pi * math.pi * math.pi * V0)\n# The following is only valid in the classical limit\n# Note that cellipk(1) = infinity, so we must skip that value\nfor i in range(len(Elist)):\n    if Elist[i] / V0 < 1:\n        rho[i] = pre * cellipk(Elist[i] / V0)\n    elif Elist[i] / V0 > 1:\n        rho[i] = pre * math.sqrt(V0 / Elist[i]) * cellipk(V0 / Elist[i])\nreturn rho", "path": "chempy\\states.py", "repo_name": "jwallen/ChemPy", "stars": 22, "license": "mit", "language": "python", "size": 447}
{"docstring": "\"\"\"\nReturn the entropy in J/mol*K at the specified temperatures `Tlist` in\nK.\n\"\"\"\n", "func_signal": "def getEntropy(self, T):\n", "code": "cython.declare(S=cython.double)\nS = 0.0\nfor mode in self.modes:\n    S += mode.getEntropy(T)\nreturn S", "path": "chempy\\states.py", "repo_name": "jwallen/ChemPy", "stars": 22, "license": "mit", "language": "python", "size": 447}
{"docstring": "\"\"\"\nCheck the graph isomorphism functions.\n\"\"\"\n\n", "func_signal": "def testIsomorphism(self):\n", "code": "vertices1 = [Vertex() for i in range(6)]\nedges1 = [Edge() for i in range(5)]\nvertices2 = [Vertex() for i in range(6)]\nedges2 = [Edge() for i in range(5)]\n\ngraph1 = Graph()\nfor vertex in vertices1: graph1.addVertex(vertex)\ngraph1.edges[vertices1[0]] = {                          vertices1[1]: edges1[0] }\ngraph1.edges[vertices1[1]] = { vertices1[0]: edges1[0], vertices1[2]: edges1[1] }\ngraph1.edges[vertices1[2]] = { vertices1[1]: edges1[1], vertices1[3]: edges1[2] }\ngraph1.edges[vertices1[3]] = { vertices1[2]: edges1[2], vertices1[4]: edges1[3] }\ngraph1.edges[vertices1[4]] = { vertices1[3]: edges1[3], vertices1[5]: edges1[4] }\ngraph1.edges[vertices1[5]] = { vertices1[4]: edges1[4] }\n\ngraph2 = Graph()\nfor vertex in vertices2: graph2.addVertex(vertex)\ngraph2.edges[vertices2[0]] = {                          vertices2[1]: edges2[4] }\ngraph2.edges[vertices2[1]] = { vertices2[0]: edges2[4], vertices2[2]: edges2[3] }\ngraph2.edges[vertices2[2]] = { vertices2[1]: edges2[3], vertices2[3]: edges2[2] }\ngraph2.edges[vertices2[3]] = { vertices2[2]: edges2[2], vertices2[4]: edges2[1] }\ngraph2.edges[vertices2[4]] = { vertices2[3]: edges2[1], vertices2[5]: edges2[0] }\ngraph2.edges[vertices2[5]] = { vertices2[4]: edges2[0] }\n\nself.assertTrue(graph1.isIsomorphic(graph2))\nself.assertTrue(graph1.isSubgraphIsomorphic(graph2))\nself.assertTrue(graph2.isIsomorphic(graph1))\nself.assertTrue(graph2.isSubgraphIsomorphic(graph1))", "path": "unittest\\graphTest.py", "repo_name": "jwallen/ChemPy", "stars": 22, "license": "mit", "language": "python", "size": 447}
{"docstring": "\"\"\"\nReturn the values of the hindered rotor potential :math:`V(\\\\phi)`\nin J/mol at the angles `phi` in radians.\n\"\"\"\n", "func_signal": "def getPotential(self, phi):\n", "code": "cython.declare(V=numpy.ndarray, k=cython.int)\nV = numpy.zeros_like(phi)\nif self.fourier is not None:\n    for k in range(self.fourier.shape[1]):\n        V += self.fourier[0,k] * numpy.cos((k+1) * phi) + self.fourier[1,k] * numpy.sin((k+1) * phi)\n    V -= numpy.sum(self.fourier[0,:])\nelse:\n    V = 0.5 * self.barrier * (1 - numpy.cos(self.symmetry * phi))\nreturn V", "path": "chempy\\states.py", "repo_name": "jwallen/ChemPy", "stars": 22, "license": "mit", "language": "python", "size": 447}
{"docstring": "\"\"\"\nReturn the value of the partition function at the specified temperatures\n`Tlist` in K. The formula is\n\n.. math:: q_\\\\mathrm{rot}(T) = \\\\frac{8 \\\\pi^2 I k_\\\\mathrm{B} T}{\\\\sigma h^2}\n\nfor linear rotors and\n\n.. math:: q_\\\\mathrm{rot}(T) = \\\\frac{\\\\sqrt{\\\\pi}}{\\\\sigma} \\\\left( \\\\frac{8 \\\\pi^2 k_\\\\mathrm{B} T}{h^2} \\\\right)^{3/2} \\\\sqrt{I_\\\\mathrm{A} I_\\\\mathrm{B} I_\\\\mathrm{C}}\n\nfor nonlinear rotors. Above, :math:`T` is temperature, :math:`\\\\sigma`\nis the symmetry number, :math:`I` is the moment of inertia,\n:math:`k_\\\\mathrm{B}` is the Boltzmann constant, and :math:`h` is the\nPlanck constant.\n\"\"\"\n", "func_signal": "def getPartitionFunction(self, T):\n", "code": "cython.declare(theta=cython.double, inertia=cython.double)\nif self.linear:\n    theta = constants.h * constants.h / (8 * constants.pi * constants.pi * self.inertia[0] * constants.kB)\n    return T / theta / self.symmetry\nelse:\n    theta = 1.0\n    for inertia in self.inertia:\n        theta *= constants.h * constants.h / (8 * constants.pi * constants.pi * inertia * constants.kB)\n    return numpy.sqrt(constants.pi * T**len(self.inertia) / theta) / self.symmetry", "path": "chempy\\states.py", "repo_name": "jwallen/ChemPy", "stars": 22, "license": "mit", "language": "python", "size": 447}
{"docstring": "\"\"\"\nReturn the value of the zeroth-order modified Bessel function at `x`.\n\"\"\"\n", "func_signal": "def besseli0(x):\n", "code": "import scipy.special\nreturn scipy.special.i0(x)", "path": "chempy\\states.py", "repo_name": "jwallen/ChemPy", "stars": 22, "license": "mit", "language": "python", "size": 447}
{"docstring": "\"\"\"\nCheck the subgraph isomorphism functions.\n\"\"\"\n\n", "func_signal": "def testSubgraphIsomorphism(self):\n", "code": "vertices1 = [Vertex() for i in range(6)]\nedges1 = [Edge() for i in range(5)]\nvertices2 = [Vertex() for i in range(2)]\nedges2 = [Edge() for i in range(1)]\n\ngraph1 = Graph()\nfor vertex in vertices1: graph1.addVertex(vertex)\ngraph1.edges[vertices1[0]] = {                          vertices1[1]: edges1[0] }\ngraph1.edges[vertices1[1]] = { vertices1[0]: edges1[0], vertices1[2]: edges1[1] }\ngraph1.edges[vertices1[2]] = { vertices1[1]: edges1[1], vertices1[3]: edges1[2] }\ngraph1.edges[vertices1[3]] = { vertices1[2]: edges1[2], vertices1[4]: edges1[3] }\ngraph1.edges[vertices1[4]] = { vertices1[3]: edges1[3], vertices1[5]: edges1[4] }\ngraph1.edges[vertices1[5]] = { vertices1[4]: edges1[4] }\n\ngraph2 = Graph()\nfor vertex in vertices2: graph2.addVertex(vertex)\ngraph2.edges[vertices2[0]] = { vertices2[1]: edges2[0] }\ngraph2.edges[vertices2[1]] = { vertices2[0]: edges2[0] }\n\n\nself.assertFalse(graph1.isIsomorphic(graph2))\nself.assertFalse(graph2.isIsomorphic(graph1))\nself.assertTrue(graph1.isSubgraphIsomorphic(graph2))\n\nismatch, mapList = graph1.findSubgraphIsomorphisms(graph2)\nself.assertTrue(ismatch)\nself.assertTrue(len(mapList) == 10)", "path": "unittest\\graphTest.py", "repo_name": "jwallen/ChemPy", "stars": 22, "license": "mit", "language": "python", "size": 447}
{"docstring": "\"\"\"\nReturn the contribution to the entropy due to rigid rotation in J/mol*K\nat the specified temperatures `Tlist` in K. The formula is\n\n.. math:: \\\\frac{S^\\\\mathrm{rot}(T)}{R} = \\\\ln Q^\\\\mathrm{rot} + 1\n\nfor linear rotors and\n\n.. math:: \\\\frac{S^\\\\mathrm{rot}(T)}{R} = \\\\ln Q^\\\\mathrm{rot} + \\\\frac{3}{2}\n\nfor nonlinear rotors, where :math:`Q^\\\\mathrm{rot}` is the partition\nfunction for a rigid rotor and :math:`R` is the gas law constant.\n\"\"\"\n", "func_signal": "def getEntropy(self, T):\n", "code": "if self.linear:\n    return (numpy.log(self.getPartitionFunction(T)) + 1.0) * constants.R\nelse:\n    return (numpy.log(self.getPartitionFunction(T)) + 1.5) * constants.R", "path": "chempy\\states.py", "repo_name": "jwallen/ChemPy", "stars": 22, "license": "mit", "language": "python", "size": 447}
{"docstring": "\"\"\"\nReturn the value of the density of states in mol/J at the specified\nenergies `Elist` in J/mol above the ground state. An active K-rotor is\nautomatically included if there are no external rotational modes.\n\"\"\"\n", "func_signal": "def getDensityOfStates(self, Elist):\n", "code": "cython.declare(rho=numpy.ndarray, i=cython.int, E=cython.double)\nrho = numpy.zeros_like(Elist)\n# Active K-rotor\nrotors = [mode for mode in self.modes if isinstance(mode, RigidRotor)]\nif len(rotors) == 0:\n    rho0 = numpy.zeros_like(Elist)\n    for i, E in enumerate(Elist):\n        if E > 0: rho0[i] = 1.0 / math.sqrt(1.0 * E)\n    rho = convolve(rho, rho0, Elist)\n# Other non-vibrational modes\nfor mode in self.modes:\n    if not isinstance(mode, HarmonicOscillator):\n        rho = convolve(rho, mode.getDensityOfStates(Elist), Elist)\n# Vibrational modes\nfor mode in self.modes:\n    if isinstance(mode, HarmonicOscillator):\n        rho = mode.getDensityOfStates(Elist, rho)\nreturn rho * self.spinMultiplicity", "path": "chempy\\states.py", "repo_name": "jwallen/ChemPy", "stars": 22, "license": "mit", "language": "python", "size": 447}
{"docstring": "\"\"\"\nReturn the density of states at the specified energies `Elist` in J/mol\nabove the ground state. The Beyer-Swinehart method is used to\nefficiently convolve the vibrational density of states into the\ndensity of states of other modes. To be accurate, this requires a small\n(:math:`1-10 \\\\ \\\\mathrm{cm^{-1}}` or so) energy spacing.\n\"\"\"\n", "func_signal": "def getDensityOfStates(self, Elist, rho0=None):\n", "code": "cython.declare(rho=numpy.ndarray, freq=cython.double)\ncython.declare(dE=cython.double, nE=cython.int, dn=cython.int, n=cython.int)\nif rho0 is not None:\n    rho = rho0\nelse:\n    rho = numpy.zeros_like(Elist)\ndE = Elist[1] - Elist[0]\nnE = len(Elist)\nfor freq in self.frequencies:\n    dn = int(freq * constants.h * constants.c * 100 * constants.Na / dE)\n    for n in range(dn+1, nE):\n        rho[n] = rho[n] + rho[n-dn]\nreturn rho", "path": "chempy\\states.py", "repo_name": "jwallen/ChemPy", "stars": 22, "license": "mit", "language": "python", "size": 447}
{"docstring": "\"\"\"\nReturn the constant-pressure heat capacity in J/mol*K at the specified\ntemperatures `Tlist` in K.\n\"\"\"\n", "func_signal": "def getHeatCapacity(self, T):\n", "code": "cython.declare(Cp=cython.double)\nCp = constants.R\nfor mode in self.modes:\n    Cp += mode.getHeatCapacity(T)\nreturn Cp", "path": "chempy\\states.py", "repo_name": "jwallen/ChemPy", "stars": 22, "license": "mit", "language": "python", "size": 447}
{"docstring": "\"\"\"\nReturn the contribution to the enthalpy due to vibration in J/mol at\nthe specified temperatures `Tlist` in K. The formula is\n\n.. math:: \\\\frac{H^\\\\mathrm{vib}(T)}{RT} = \\\\sum_i \\\\frac{\\\\xi_i}{e^{\\\\xi_i} - 1}\n\nwhere :math:`\\\\xi_i \\\\equiv h \\\\nu_i / k_\\\\mathrm{B} T`,\n:math:`T` is temperature, :math:`\\\\nu_i` is the frequency of vibration\n:math:`i`, :math:`k_\\\\mathrm{B}` is the Boltzmann constant, :math:`h`\nis the Planck constant, and :math:`R` is the gas law constant.\n\"\"\"\n", "func_signal": "def getEnthalpy(self, T):\n", "code": "cython.declare(H=cython.double, freq=cython.double)\ncython.declare(x=cython.double, exp_x=cython.double)\nH = 0.0\nfor freq in self.frequencies:\n    x = freq / (0.695039 * T)\t# kB = 0.695039 cm^-1/K\n    exp_x = numpy.exp(x)\n    H = H + x / (exp_x - 1)\nreturn H * constants.R * T", "path": "chempy\\states.py", "repo_name": "jwallen/ChemPy", "stars": 22, "license": "mit", "language": "python", "size": 447}
{"docstring": "\"\"\"\nReturn the enthalpy in J/mol at the specified temperatures `Tlist` in K.\n\"\"\"\n", "func_signal": "def getEnthalpy(self, T):\n", "code": "cython.declare(H=cython.double)\nH = constants.R * T\nfor mode in self.modes:\n    H += mode.getEnthalpy(T)\nreturn H", "path": "chempy\\states.py", "repo_name": "jwallen/ChemPy", "stars": 22, "license": "mit", "language": "python", "size": 447}
{"docstring": "\"\"\"\nReturn a string representation that can be used to reconstruct the\nobject.\n\"\"\"\n", "func_signal": "def __repr__(self):\n", "code": "frequencies = ', '.join(['%g' % freq for freq in self.frequencies])\nreturn 'HarmonicOscillator(frequencies=[%s])' % (frequencies)", "path": "chempy\\states.py", "repo_name": "jwallen/ChemPy", "stars": 22, "license": "mit", "language": "python", "size": 447}
{"docstring": "\"\"\"\nReturn the value of the partition function at the specified temperatures\n`Tlist` in K. The formula is\n\n.. math:: q_\\\\mathrm{trans}(T) = \\\\left( \\\\frac{2 \\\\pi m k_\\\\mathrm{B} T}{h^2} \\\\right)^{3/2} \\\\frac{k_\\\\mathrm{B} T}{P}\n\nwhere :math:`T` is temperature, :math:`V` is volume, :math:`m` is mass,\n:math:`d` is dimensionality, :math:`k_\\\\mathrm{B}` is the Boltzmann\nconstant, and :math:`h` is the Planck constant.\n\"\"\"\n", "func_signal": "def getPartitionFunction(self, T):\n", "code": "cython.declare(qt=cython.double)\nqt = ((2 * constants.pi * self.mass / constants.Na) / (constants.h * constants.h))**1.5 / 1e5\nreturn qt * (constants.kB * T)**2.5", "path": "chempy\\states.py", "repo_name": "jwallen/ChemPy", "stars": 22, "license": "mit", "language": "python", "size": 447}
{"docstring": "\"\"\"\nReturn the density of states at the specified energlies `Elist` in J/mol\nabove the ground state. The formula is\n\n.. math:: \\\\rho(E) = \\\\left( \\\\frac{2 \\\\pi m}{h^2} \\\\right)^{3/2} \\\\frac{E^{3/2}}{\\\\Gamma(5/2)} \\\\frac{1}{P}\n\nwhere :math:`E` is energy, :math:`m` is mass, :math:`k_\\\\mathrm{B}` is\nthe Boltzmann constant, and :math:`R` is the gas law constant.\n\"\"\"\n", "func_signal": "def getDensityOfStates(self, Elist):\n", "code": "cython.declare(rho=numpy.ndarray, qt=cython.double)\nrho = numpy.zeros_like(Elist)\nqt = ((2 * constants.pi * self.mass / constants.Na / constants.Na) / (constants.h * constants.h))**(1.5) / 1e5\nrho = qt * Elist**1.5 / (numpy.sqrt(math.pi) * 0.25) / constants.Na\nreturn rho", "path": "chempy\\states.py", "repo_name": "jwallen/ChemPy", "stars": 22, "license": "mit", "language": "python", "size": 447}
{"docstring": "\"\"\"\nReturn the contribution to the heat capacity due to hindered rotation\nin J/mol*K at the specified temperatures `Tlist` in K. For the cosine\npotential, this is calculated numerically from the partition function.\nFor the Fourier series potential, we solve the corresponding 1D\nSchrodinger equation to obtain the energy levels of the rotor and\nutilize the expression\n\n.. math:: S^\\\\mathrm{hind}(T) = R \\\\left( \\\\ln q_\\\\mathrm{hind}(T) + \\\\frac{\\\\sum_i E_i e^{-\\\\beta E_i}}{RT \\\\sum_i e^{-\\\\beta E_i}} \\\\right)\n\nto obtain the entropy.\n\"\"\"\n", "func_signal": "def getEntropy(self, T):\n", "code": "if self.fourier is not None:\n    cython.declare(S=cython.double, E=numpy.ndarray, e_kT=numpy.ndarray, i=cython.int)\n    E = self.energies\n    S = constants.R * numpy.log(self.getPartitionFunction(T))\n    e_kT = numpy.exp(-E / constants.R / T)\n    S += numpy.sum(E*e_kT) / (T * numpy.sum(e_kT))\n    return S\nelse:\n    Tlow = T * 0.999\n    Thigh = T * 1.001\n    return (numpy.log(self.getPartitionFunction(Thigh)) +\n        T * (numpy.log(self.getPartitionFunction(Thigh)) -\n        numpy.log(self.getPartitionFunction(Tlow))) /\n        (Thigh - Tlow)) * constants.R", "path": "chempy\\states.py", "repo_name": "jwallen/ChemPy", "stars": 22, "license": "mit", "language": "python", "size": 447}
{"docstring": "\"\"\"\nReturn the contribution to the enthalpy due to rigid rotation in J/mol\nat the specified temperatures `Tlist` in K. The formula is\n\n.. math:: \\\\frac{H^\\\\mathrm{rot}(T)}{RT} = 1\n\nfor linear rotors and\n\n.. math:: \\\\frac{H^\\\\mathrm{rot}(T)}{RT} = \\\\frac{3}{2}\n\nfor nonlinear rotors, where :math:`T` is temperature and :math:`R` is\nthe gas law constant.\n\"\"\"\n", "func_signal": "def getEnthalpy(self, T):\n", "code": "if self.linear:\n    return constants.R * T\nelse:\n    return 1.5 * constants.R * T", "path": "chempy\\states.py", "repo_name": "jwallen/ChemPy", "stars": 22, "license": "mit", "language": "python", "size": 447}
{"docstring": "\"\"\"\nReturns True if all subforms are either valid or\nempty and not required. False otherwise.\n\"\"\"\n# first check if we're bound ...\n", "func_signal": "def is_valid(self):\n", "code": "if self.is_bound:\n    # then check every subform ...\n    for form in self.forms.values():\n        if not form.is_valid():\n            return False\nelse:\n    return False\nreturn True", "path": "kikola\\forms\\superforms.py", "repo_name": "playpauseandstop/kikola", "stars": 21, "license": "bsd-3-clause", "language": "python", "size": 184}
{"docstring": "# Test work with empty or corrupted HttpRequest instance\n", "func_signal": "def test_path(self):\n", "code": "request = HttpRequest()\nrequest.META = {'SERVER_NAME': 'www.google.com',\n                'SERVER_PORT': 80}\ncontext = path(request)\nself.assertEqual(context,\n                 {'REQUEST_ABSOLUTE_URI': 'http://www.google.com',\n                  'REQUEST_FULL_PATH': '',\n                  'REQUEST_PATH': ''})\n\n# Check how it works on real request\nurl = reverse('context_processors_path')\nresponse = self.client.get(url)\nself.assertContains(response,\n                    '\"REQUEST_ABSOLUTE_URI\": \"http://testserver%s' % \\\n                    url)\nself.assertContains(response,\n                    '\"REQUEST_FULL_PATH\": \"%s\"' % url)\nself.assertContains(response,\n                    '\"REQUEST_PATH\": \"%s\"' % url)\n\nresponse = self.client.get(url, {'q': 'Query'})\nself.assertContains(response,\n                    '\"REQUEST_ABSOLUTE_URI\": \"http://testserver' \\\n                    '%s?q=Query\"' % url)\nself.assertContains(response,\n                    '\"REQUEST_FULL_PATH\": \"%s?q=Query\"' % url)\nself.assertContains(response,\n                    '\"REQUEST_PATH\": \"%s\"' % url)", "path": "testproject\\core\\tests.py", "repo_name": "playpauseandstop/kikola", "stars": 21, "license": "bsd-3-clause", "language": "python", "size": 184}
{"docstring": "\"\"\"\nReturns this form rendered as HTML <tr>s -- excluding the\n<table></table>.\n\"\"\"\n", "func_signal": "def as_table(self):\n", "code": "subs = []\nfor f in self.forms.values():\n    subs.append(f.as_table())\nreturn \"\\n\".join(subs)", "path": "kikola\\forms\\superforms.py", "repo_name": "playpauseandstop/kikola", "stars": 21, "license": "bsd-3-clause", "language": "python", "size": 184}
{"docstring": "\"\"\"\nRender output from view function as JSON response.\n\"\"\"\n", "func_signal": "def render_to_json(*args, **json_kwargs):\n", "code": "def json_decorator(func, **json_kwargs):\n    @wraps(func)\n    def wrapper(request, *args, **kwargs):\n        # Execute view function\n        output = func(request, *args, **kwargs)\n\n        # Prepare JSON kwargs\n        defaults = {'cls': DjangoJSONEncoder,\n                    'ensure_ascii': False}\n        defaults.update(json_kwargs)\n\n        # Dumps view function output into JSON response\n        return HttpResponse(json.dumps(output, **defaults),\n                            mimetype='application/json')\n    return wrapper\n\nif not args and not json_kwargs:\n    return json_decorator\n\nif args and callable(args[0]):\n    return json_decorator(args[0], **json_kwargs)\n\ndef decorator(func):\n    return json_decorator(func, **json_kwargs)\nreturn decorator", "path": "kikola\\core\\decorators.py", "repo_name": "playpauseandstop/kikola", "stars": 21, "license": "bsd-3-clause", "language": "python", "size": 184}
{"docstring": "\"\"\"\nReturn kikola version number in human readable form.\n\"\"\"\n", "func_signal": "def get_version(version=None):\n", "code": "version = version or VERSION\nif len(version) > 2 and version[2] is not None:\n    if isinstance(version[2], int):\n        return '%d.%d.%d' % version\n    return '%d.%d-%s' % version\nreturn '%d.%d' % version[:2]", "path": "kikola\\__init__.py", "repo_name": "playpauseandstop/kikola", "stars": 21, "license": "bsd-3-clause", "language": "python", "size": 184}
{"docstring": "\"\"\"\nConvert any Python object to ``int``.\n\nVery useful function to get integer values from GET or POST requests and\nprevent corrupted data.\n\"\"\"\n", "func_signal": "def force_int(value, default=None):\n", "code": "try:\n    return int(value)\nexcept (TypeError, ValueError):\n    matched = integer_re.match(smart_str(value).strip())\n    if matched:\n        return force_int(matched.group())\n    return default", "path": "kikola\\utils\\digits.py", "repo_name": "playpauseandstop/kikola", "stars": 21, "license": "bsd-3-clause", "language": "python", "size": 184}
{"docstring": "\"\"\"\nShortcut to fast getting settings var.\n\nIf settings ``name`` does not exist and ``default`` value is ``None``,\n``AttributeError`` would be raised.\n\"\"\"\n", "func_signal": "def conf(name, default=None):\n", "code": "if default is not None:\n    return getattr(settings, name, default)\nreturn getattr(settings, name)", "path": "kikola\\shortcuts\\__init__.py", "repo_name": "playpauseandstop/kikola", "stars": 21, "license": "bsd-3-clause", "language": "python", "size": 184}
{"docstring": "\"\"\"\nReturns this form rendered as HTML <li>s -- excluding the <ul></ul>.\n\"\"\"\n", "func_signal": "def as_ul(self):\n", "code": "subs = []\nfor f in self.forms.values():\n    subs.append(f.as_ul())\nreturn \"\\n\".join(subs)", "path": "kikola\\forms\\superforms.py", "repo_name": "playpauseandstop/kikola", "stars": 21, "license": "bsd-3-clause", "language": "python", "size": 184}
{"docstring": "\"\"\"\nFormat ``datetime.datetime`` or compatible object returned by ``func`` with\n``django.templates.defaultfilters.date`` filter.\n\nIf this date is same day to today - use ``time_format`` format called with\ndecorator or \"H:i\" by default.\n\nElse - use ``datetime_format`` called with decorator or \"F d, H:i\" by\ndefault.\n\nAlso, you can to compare returned datetime with other date, not today,\nby sending it as ``compare_date`` arg.\n\"\"\"\n", "func_signal": "def smart_datetime(datetime_format=None, time_format=None, compare_date=None):\n", "code": "def datetime_decorator(func, datetime_format=None, time_format=None,\n                       compare_date=None):\n    compare_date = compare_date or TODAY()\n    datetime_format = \\\n        datetime_format or conf('DATETIME_FORMAT', 'F d, H:i')\n    time_format = time_format or conf('TIME_FORMAT', 'H:i')\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        value = func(*args, **kwargs)\n\n        if not hasattr(value, 'date'):\n            return value\n\n        if value.date() == compare_date:\n            format = time_format\n        else:\n            format = datetime_format\n\n        return date_filter(value, format)\n    return wrapper\n\nif datetime_format and callable(datetime_format):\n    return datetime_decorator(datetime_format)\n\nif datetime_format is None and \\\n   time_format is None and \\\n   compare_date is None:\n    return datetime_decorator\n\ndef decorator(func):\n    return datetime_decorator(func,\n                              datetime_format,\n                              time_format,\n                              compare_date)\nreturn decorator", "path": "kikola\\core\\decorators.py", "repo_name": "playpauseandstop/kikola", "stars": 21, "license": "bsd-3-clause", "language": "python", "size": 184}
{"docstring": "\"\"\"\nReturns this form rendered as HTML <tr>s -- excluding the\n<table></table>.\n\"\"\"\n", "func_signal": "def as_table(self):\n", "code": "subs = []\nfor f in self._forms:\n    subs.append(f.as_table())\nreturn \"\\n\".join(subs)", "path": "kikola\\forms\\superforms.py", "repo_name": "playpauseandstop/kikola", "stars": 21, "license": "bsd-3-clause", "language": "python", "size": 184}
{"docstring": "\"\"\"\nReturns the field name with a prefix appended, if this Form has a\nprefix set.\n\nSubclasses may wish to override.\n\"\"\"\n", "func_signal": "def add_prefix(self, field_name):\n", "code": "return self.prefix and \\\n       ('%s-%s' % (self.prefix, field_name)) or \\\n       field_name", "path": "kikola\\forms\\superforms.py", "repo_name": "playpauseandstop/kikola", "stars": 21, "license": "bsd-3-clause", "language": "python", "size": 184}
{"docstring": "# Urlize present links\n", "func_signal": "def twitterize(status, autoescape=None):\n", "code": "status = urlize(status, nofollow=True, autoescape=autoescape)\n\n# Wraps \"RT\" text in status\nstatus = re.sub(r'^RT @', '<span class=\"retweeted\">RT</span> @', status)\n\n# Add links to twitter usernames\nstatus = re.sub(r'(\\s|^)@([a-zA-Z0-9_]+)',\n                '\\\\1@<a href=\"http://twitter.com/\\\\2\" rel=\"nofollow\">' \\\n                '\\\\2</a>',\n                status)\n\n# Add links to hash tags\nstatus = re.sub(r'(\\s|^)#([a-zA-Z0-9_]+)',\n                '\\\\1<a href=\"http://twitter.com/search?q=%23\\\\2\" ' \\\n                'rel=\"nofollow\">#\\\\2</a>',\n                status)\n\nreturn mark_safe(status)", "path": "kikola\\templatetags\\twitter_tags.py", "repo_name": "playpauseandstop/kikola", "stars": 21, "license": "bsd-3-clause", "language": "python", "size": 184}
{"docstring": "\"\"\"\nReturns an ErrorDict for self.data\n\"\"\"\n", "func_signal": "def _errors(self):\n", "code": "if self.__errors is None:\n    error_dict = self.forms['_self'].errors\n    for k,f in self.forms.items():\n        if k == '_self':\n            continue\n        error_dict[k] = f.errors\n    self.__errors = error_dict\nreturn self.__errors", "path": "kikola\\forms\\superforms.py", "repo_name": "playpauseandstop/kikola", "stars": 21, "license": "bsd-3-clause", "language": "python", "size": 184}
{"docstring": "\"\"\"\nCreate new virtualenv and install all pip requirements there.\n\"\"\"\n# Change directory to current\n", "func_signal": "def main():\n", "code": "os.chdir(DIRNAME)\n\n# Read configuration values from ``bootstrap.cfg`` file if possible\nread_config('bootstrap.cfg')\n\n# Parse destination directory for new virtual environment\ncreate_environment()\n\n# Install all requirements to this virtual environment if possible\ninstall_requirements()", "path": "testproject\\bootstrap.py", "repo_name": "playpauseandstop/kikola", "stars": 21, "license": "bsd-3-clause", "language": "python", "size": 184}
{"docstring": "\"\"\"\nReturns this form rendered as HTML <p>s.\n\"\"\"\n", "func_signal": "def as_p(self):\n", "code": "subs = []\nfor f in self.forms.values():\n    subs.append(f.as_p())\nreturn \"\\n\".join(subs)", "path": "kikola\\forms\\superforms.py", "repo_name": "playpauseandstop/kikola", "stars": 21, "license": "bsd-3-clause", "language": "python", "size": 184}
{"docstring": "\"\"\"\nExpect the dict from view. Render returned dict with RequestContext.\n\"\"\"\n", "func_signal": "def render_to(template_path, mimetype=None):\n", "code": "def decorator(func):\n    @wraps(func)\n    def wrapper(request, *args, **kwargs):\n        output = func(request, *args, **kwargs)\n\n        if not isinstance(output, dict):\n            return output\n\n        kwargs = {'context_instance': RequestContext(request)}\n        output['request'] = request\n\n        if 'MIME_TYPE' in output:\n            kwargs['mimetype'] = output.pop('MIME_TYPE')\n        elif 'MIMETYPE' in output:\n            kwargs['mimetype'] = output.pop('MIMETYPE')\n        elif mimetype:\n            kwargs['mimetype'] = mimetype\n\n        if 'TEMPLATE' in output:\n            template = output.pop('TEMPLATE')\n        else:\n            template = template_path\n\n        return render_to_response(template, output, **kwargs)\n    return wrapper\nreturn decorator", "path": "kikola\\core\\decorators.py", "repo_name": "playpauseandstop/kikola", "stars": 21, "license": "bsd-3-clause", "language": "python", "size": 184}
{"docstring": "\"\"\"\nAdds current absolute URI, path and full path variables to templates.\n\nTo enable, adds ``kikola.core.context_processors.path`` to your project's\n``settings`` ``TEMPLATE_CONTEXT_PROCESSORS`` var.\n\n**Note:** Django has ``django.core.context_processors.request`` context\nprocessor that adding whole ``HttpRequest`` object to templates.\n\"\"\"\n", "func_signal": "def path(request):\n", "code": "return {'REQUEST_ABSOLUTE_URI': request.build_absolute_uri(),\n        'REQUEST_FULL_PATH': request.get_full_path(),\n        'REQUEST_PATH': request.path}", "path": "kikola\\core\\context_processors.py", "repo_name": "playpauseandstop/kikola", "stars": 21, "license": "bsd-3-clause", "language": "python", "size": 184}
{"docstring": "# Convert string time to timedelta instance and then convert to\n# seconds value\n", "func_signal": "def clean(self, value):\n", "code": "if isinstance(value, basestring):\n    value = str_to_timedelta(value)\n\n    if value:\n        value = timedelta_seconds(value)\n\nreturn super(TimeDeltaField, self).clean(value)", "path": "kikola\\forms\\fields.py", "repo_name": "playpauseandstop/kikola", "stars": 21, "license": "bsd-3-clause", "language": "python", "size": 184}
{"docstring": "\"\"\"\nReturns a BoundField with the given name.\n\"\"\"\n", "func_signal": "def __getitem__(self, name):\n", "code": "try:\n    return self.forms[name]\nexcept KeyError:\n    return self.forms['_self'][name]", "path": "kikola\\forms\\superforms.py", "repo_name": "playpauseandstop/kikola", "stars": 21, "license": "bsd-3-clause", "language": "python", "size": 184}
{"docstring": "\"\"\"\nDetermines if there's data submitted for this subform\n\"\"\"\n", "func_signal": "def _got_data(self, form):\n", "code": "for k in self.data.keys():\n    if k.startswith(form.prefix):\n        return True\nreturn False", "path": "kikola\\forms\\superforms.py", "repo_name": "playpauseandstop/kikola", "stars": 21, "license": "bsd-3-clause", "language": "python", "size": 184}
{"docstring": "# the special argument \"-\" means sys.std{in,out}\n", "func_signal": "def __call__(self, string):\n", "code": "if string == '-':\n    if 'r' in self._mode:\n        return _sys.stdin\n    elif 'w' in self._mode:\n        return _sys.stdout\n    else:\n        msg = _('argument \"-\" with mode %r' % self._mode)\n        raise ValueError(msg)\n\n# all other arguments are used as file names\nif self._bufsize:\n    return open(string, self._mode, self._bufsize)\nelse:\n    return open(string, self._mode)", "path": "argparse.py", "repo_name": "frxxbase-xx/jgd", "stars": 17, "license": "None", "language": "python", "size": 4248}
{"docstring": "# in all examples below, we have to allow for '--' args\n# which are represented as '-' in the pattern\n", "func_signal": "def _get_nargs_pattern(self, action):\n", "code": "nargs = action.nargs\n\n# the default (None) is assumed to be a single argument\nif nargs is None:\n    nargs_pattern = '(-*A-*)'\n\n# allow zero or one arguments\nelif nargs == OPTIONAL:\n    nargs_pattern = '(-*A?-*)'\n\n# allow zero or more arguments\nelif nargs == ZERO_OR_MORE:\n    nargs_pattern = '(-*[A-]*)'\n\n# allow one or more arguments\nelif nargs == ONE_OR_MORE:\n    nargs_pattern = '(-*A[A-]*)'\n\n# allow any number of options or arguments\nelif nargs == REMAINDER:\n    nargs_pattern = '([-AO]*)'\n\n# allow one argument followed by any number of options or arguments\nelif nargs == PARSER:\n    nargs_pattern = '(-*A[-AO]*)'\n\n# all others should be integers\nelse:\n    nargs_pattern = '(-*%s-*)' % '-*'.join('A' * nargs)\n\n# if this is an optional action, -- is not allowed\nif action.option_strings:\n    nargs_pattern = nargs_pattern.replace('-*', '')\n    nargs_pattern = nargs_pattern.replace('-', '')\n\n# return the pattern\nreturn nargs_pattern", "path": "argparse.py", "repo_name": "frxxbase-xx/jgd", "stars": 17, "license": "None", "language": "python", "size": 4248}
{"docstring": "# match the pattern for this action to the arg strings\n", "func_signal": "def _match_argument(self, action, arg_strings_pattern):\n", "code": "nargs_pattern = self._get_nargs_pattern(action)\nmatch = _re.match(nargs_pattern, arg_strings_pattern)\n\n# raise an exception if we weren't able to find a match\nif match is None:\n    nargs_errors = {\n        None: _('expected one argument'),\n        OPTIONAL: _('expected at most one argument'),\n        ONE_OR_MORE: _('expected at least one argument'),\n    }\n    default = _('expected %s argument(s)') % action.nargs\n    msg = nargs_errors.get(action.nargs, default)\n    raise ArgumentError(action, msg)\n\n# return the number of arguments matched\nreturn len(match.group(1))", "path": "argparse.py", "repo_name": "frxxbase-xx/jgd", "stars": 17, "license": "None", "language": "python", "size": 4248}
{"docstring": "# determine function from conflict handler string\n", "func_signal": "def _get_handler(self):\n", "code": "handler_func_name = '_handle_conflict_%s' % self.conflict_handler\ntry:\n    return getattr(self, handler_func_name)\nexcept AttributeError:\n    msg = _('invalid conflict_resolution value: %r')\n    raise ValueError(msg % self.conflict_handler)", "path": "argparse.py", "repo_name": "frxxbase-xx/jgd", "stars": 17, "license": "None", "language": "python", "size": 4248}
{"docstring": "# make sure required is not specified\n", "func_signal": "def _get_positional_kwargs(self, dest, **kwargs):\n", "code": "if 'required' in kwargs:\n    msg = _(\"'required' is an invalid argument for positionals\")\n    raise TypeError(msg)\n\n# mark positional arguments as required if at least one is\n# always required\nif kwargs.get('nargs') not in [OPTIONAL, ZERO_OR_MORE]:\n    kwargs['required'] = True\nif kwargs.get('nargs') == ZERO_OR_MORE and 'default' not in kwargs:\n    kwargs['required'] = True\n\n# return the keyword arguments with no option strings\nreturn dict(kwargs, dest=dest, option_strings=[])", "path": "argparse.py", "repo_name": "frxxbase-xx/jgd", "stars": 17, "license": "None", "language": "python", "size": 4248}
{"docstring": "# set prog from the existing prefix\n", "func_signal": "def add_parser(self, name, **kwargs):\n", "code": "if kwargs.get('prog') is None:\n    kwargs['prog'] = '%s %s' % (self._prog_prefix, name)\n\n# create a pseudo-action to hold the choice help\nif 'help' in kwargs:\n    help = kwargs.pop('help')\n    choice_action = self._ChoicesPseudoAction(name, help)\n    self._choices_actions.append(choice_action)\n\n# create the parser and add it to the map\nparser = self._parser_class(**kwargs)\nself._name_parser_map[name] = parser\nreturn parser", "path": "argparse.py", "repo_name": "frxxbase-xx/jgd", "stars": 17, "license": "None", "language": "python", "size": 4248}
{"docstring": "# collect groups by titles\n", "func_signal": "def _add_container_actions(self, container):\n", "code": "title_group_map = {}\nfor group in self._action_groups:\n    if group.title in title_group_map:\n        msg = _('cannot merge actions - two groups are named %r')\n        raise ValueError(msg % (group.title))\n    title_group_map[group.title] = group\n\n# map each action to its group\ngroup_map = {}\nfor group in container._action_groups:\n\n    # if a group with the title exists, use that, otherwise\n    # create a new group matching the container's group\n    if group.title not in title_group_map:\n        title_group_map[group.title] = self.add_argument_group(\n            title=group.title,\n            description=group.description,\n            conflict_handler=group.conflict_handler)\n\n    # map the actions to their new group\n    for action in group._group_actions:\n        group_map[action] = title_group_map[group.title]\n\n# add container's mutually exclusive groups\n# NOTE: if add_mutually_exclusive_group ever gains title= and\n# description= then this code will need to be expanded as above\nfor group in container._mutually_exclusive_groups:\n    mutex_group = self.add_mutually_exclusive_group(\n        required=group.required)\n\n    # map the actions to their new mutex group\n    for action in group._group_actions:\n        group_map[action] = mutex_group\n\n# add all actions to this container or their group\nfor action in container._actions:\n    group_map.get(action, self)._add_action(action)", "path": "argparse.py", "repo_name": "frxxbase-xx/jgd", "stars": 17, "license": "None", "language": "python", "size": 4248}
{"docstring": "# determine short and long option strings\n", "func_signal": "def _get_optional_kwargs(self, *args, **kwargs):\n", "code": "option_strings = []\nlong_option_strings = []\nfor option_string in args:\n    # error on strings that don't start with an appropriate prefix\n    if not option_string[0] in self.prefix_chars:\n        msg = _('invalid option string %r: '\n                'must start with a character %r')\n        tup = option_string, self.prefix_chars\n        raise ValueError(msg % tup)\n\n    # strings starting with two prefix characters are long options\n    option_strings.append(option_string)\n    if option_string[0] in self.prefix_chars:\n        if len(option_string) > 1:\n            if option_string[1] in self.prefix_chars:\n                long_option_strings.append(option_string)\n\n# infer destination, '--foo-bar' -> 'foo_bar' and '-x' -> 'x'\ndest = kwargs.pop('dest', None)\nif dest is None:\n    if long_option_strings:\n        dest_option_string = long_option_strings[0]\n    else:\n        dest_option_string = option_strings[0]\n    dest = dest_option_string.lstrip(self.prefix_chars)\n    if not dest:\n        msg = _('dest= is required for options like %r')\n        raise ValueError(msg % option_string)\n    dest = dest.replace('-', '_')\n\n# return the updated keyword arguments\nreturn dict(kwargs, dest=dest, option_strings=option_strings)", "path": "argparse.py", "repo_name": "frxxbase-xx/jgd", "stars": 17, "license": "None", "language": "python", "size": 4248}
{"docstring": "\"\"\"error(message: string)\n\nPrints a usage message incorporating the message to stderr and\nexits.\n\nIf you override this in a subclass, it should not return -- it\nshould either exit or raise an exception.\n\"\"\"\n", "func_signal": "def error(self, message):\n", "code": "self.print_usage(_sys.stderr)\nself.exit(2, _('%s: error: %s\\n') % (self.prog, message))", "path": "argparse.py", "repo_name": "frxxbase-xx/jgd", "stars": 17, "license": "None", "language": "python", "size": 4248}
{"docstring": "# expand arguments referencing files\n", "func_signal": "def _read_args_from_files(self, arg_strings):\n", "code": "new_arg_strings = []\nfor arg_string in arg_strings:\n\n    # for regular arguments, just add them back into the list\n    if arg_string[0] not in self.fromfile_prefix_chars:\n        new_arg_strings.append(arg_string)\n\n    # replace arguments referencing files with the file content\n    else:\n        try:\n            args_file = open(arg_string[1:])\n            try:\n                arg_strings = []\n                for arg_line in args_file.read().splitlines():\n                    for arg in self.convert_arg_line_to_args(arg_line):\n                        arg_strings.append(arg)\n                arg_strings = self._read_args_from_files(arg_strings)\n                new_arg_strings.extend(arg_strings)\n            finally:\n                args_file.close()\n        except IOError:\n            err = _sys.exc_info()[1]\n            self.error(str(err))\n\n# return the modified argument list\nreturn new_arg_strings", "path": "argparse.py", "repo_name": "frxxbase-xx/jgd", "stars": 17, "license": "None", "language": "python", "size": 4248}
{"docstring": "# replace arg strings that are file references\n", "func_signal": "def _parse_known_args(self, arg_strings, namespace):\n", "code": "if self.fromfile_prefix_chars is not None:\n    arg_strings = self._read_args_from_files(arg_strings)\n\n# map all mutually exclusive arguments to the other arguments\n# they can't occur with\naction_conflicts = {}\nfor mutex_group in self._mutually_exclusive_groups:\n    group_actions = mutex_group._group_actions\n    for i, mutex_action in enumerate(mutex_group._group_actions):\n        conflicts = action_conflicts.setdefault(mutex_action, [])\n        conflicts.extend(group_actions[:i])\n        conflicts.extend(group_actions[i + 1:])\n\n# find all option indices, and determine the arg_string_pattern\n# which has an 'O' if there is an option at an index,\n# an 'A' if there is an argument, or a '-' if there is a '--'\noption_string_indices = {}\narg_string_pattern_parts = []\narg_strings_iter = iter(arg_strings)\nfor i, arg_string in enumerate(arg_strings_iter):\n\n    # all args after -- are non-options\n    if arg_string == '--':\n        arg_string_pattern_parts.append('-')\n        for arg_string in arg_strings_iter:\n            arg_string_pattern_parts.append('A')\n\n    # otherwise, add the arg to the arg strings\n    # and note the index if it was an option\n    else:\n        option_tuple = self._parse_optional(arg_string)\n        if option_tuple is None:\n            pattern = 'A'\n        else:\n            option_string_indices[i] = option_tuple\n            pattern = 'O'\n        arg_string_pattern_parts.append(pattern)\n\n# join the pieces together to form the pattern\narg_strings_pattern = ''.join(arg_string_pattern_parts)\n\n# converts arg strings to the appropriate and then takes the action\nseen_actions = _set()\nseen_non_default_actions = _set()\n\ndef take_action(action, argument_strings, option_string=None):\n    seen_actions.add(action)\n    argument_values = self._get_values(action, argument_strings)\n\n    # error if this argument is not allowed with other previously\n    # seen arguments, assuming that actions that use the default\n    # value don't really count as \"present\"\n    if argument_values is not action.default:\n        seen_non_default_actions.add(action)\n        for conflict_action in action_conflicts.get(action, []):\n            if conflict_action in seen_non_default_actions:\n                msg = _('not allowed with argument %s')\n                action_name = _get_action_name(conflict_action)\n                raise ArgumentError(action, msg % action_name)\n\n    # take the action if we didn't receive a SUPPRESS value\n    # (e.g. from a default)\n    if argument_values is not SUPPRESS:\n        action(self, namespace, argument_values, option_string)\n\n# function to convert arg_strings into an optional action\ndef consume_optional(start_index):\n\n    # get the optional identified at this index\n    option_tuple = option_string_indices[start_index]\n    action, option_string, explicit_arg = option_tuple\n\n    # identify additional optionals in the same arg string\n    # (e.g. -xyz is the same as -x -y -z if no args are required)\n    match_argument = self._match_argument\n    action_tuples = []\n    while True:\n\n        # if we found no optional action, skip it\n        if action is None:\n            extras.append(arg_strings[start_index])\n            return start_index + 1\n\n        # if there is an explicit argument, try to match the\n        # optional's string arguments to only this\n        if explicit_arg is not None:\n            arg_count = match_argument(action, 'A')\n\n            # if the action is a single-dash option and takes no\n            # arguments, try to parse more single-dash options out\n            # of the tail of the option string\n            chars = self.prefix_chars\n            if arg_count == 0 and option_string[1] not in chars:\n                action_tuples.append((action, [], option_string))\n                for char in self.prefix_chars:\n                    option_string = char + explicit_arg[0]\n                    explicit_arg = explicit_arg[1:] or None\n                    optionals_map = self._option_string_actions\n                    if option_string in optionals_map:\n                        action = optionals_map[option_string]\n                        break\n                else:\n                    msg = _('ignored explicit argument %r')\n                    raise ArgumentError(action, msg % explicit_arg)\n\n            # if the action expect exactly one argument, we've\n            # successfully matched the option; exit the loop\n            elif arg_count == 1:\n                stop = start_index + 1\n                args = [explicit_arg]\n                action_tuples.append((action, args, option_string))\n                break\n\n            # error if a double-dash option did not use the\n            # explicit argument\n            else:\n                msg = _('ignored explicit argument %r')\n                raise ArgumentError(action, msg % explicit_arg)\n\n        # if there is no explicit argument, try to match the\n        # optional's string arguments with the following strings\n        # if successful, exit the loop\n        else:\n            start = start_index + 1\n            selected_patterns = arg_strings_pattern[start:]\n            arg_count = match_argument(action, selected_patterns)\n            stop = start + arg_count\n            args = arg_strings[start:stop]\n            action_tuples.append((action, args, option_string))\n            break\n\n    # add the Optional to the list and return the index at which\n    # the Optional's string args stopped\n    assert action_tuples\n    for action, args, option_string in action_tuples:\n        take_action(action, args, option_string)\n    return stop\n\n# the list of Positionals left to be parsed; this is modified\n# by consume_positionals()\npositionals = self._get_positional_actions()\n\n# function to convert arg_strings into positional actions\ndef consume_positionals(start_index):\n    # match as many Positionals as possible\n    match_partial = self._match_arguments_partial\n    selected_pattern = arg_strings_pattern[start_index:]\n    arg_counts = match_partial(positionals, selected_pattern)\n\n    # slice off the appropriate arg strings for each Positional\n    # and add the Positional and its args to the list\n    for action, arg_count in zip(positionals, arg_counts):\n        args = arg_strings[start_index: start_index + arg_count]\n        start_index += arg_count\n        take_action(action, args)\n\n    # slice off the Positionals that we just parsed and return the\n    # index at which the Positionals' string args stopped\n    positionals[:] = positionals[len(arg_counts):]\n    return start_index\n\n# consume Positionals and Optionals alternately, until we have\n# passed the last option string\nextras = []\nstart_index = 0\nif option_string_indices:\n    max_option_string_index = max(option_string_indices)\nelse:\n    max_option_string_index = -1\nwhile start_index <= max_option_string_index:\n\n    # consume any Positionals preceding the next option\n    next_option_string_index = min([\n        index\n        for index in option_string_indices\n        if index >= start_index])\n    if start_index != next_option_string_index:\n        positionals_end_index = consume_positionals(start_index)\n\n        # only try to parse the next optional if we didn't consume\n        # the option string during the positionals parsing\n        if positionals_end_index > start_index:\n            start_index = positionals_end_index\n            continue\n        else:\n            start_index = positionals_end_index\n\n    # if we consumed all the positionals we could and we're not\n    # at the index of an option string, there were extra arguments\n    if start_index not in option_string_indices:\n        strings = arg_strings[start_index:next_option_string_index]\n        extras.extend(strings)\n        start_index = next_option_string_index\n\n    # consume the next optional and any arguments for it\n    start_index = consume_optional(start_index)\n\n# consume any positionals following the last Optional\nstop_index = consume_positionals(start_index)\n\n# if we didn't consume all the argument strings, there were extras\nextras.extend(arg_strings[stop_index:])\n\n# if we didn't use all the Positional objects, there were too few\n# arg strings supplied.\nif positionals:\n    self.error(_('too few arguments'))\n\n# make sure all required actions were present\nfor action in self._actions:\n    if action.required:\n        if action not in seen_actions:\n            name = _get_action_name(action)\n            self.error(_('argument %s is required') % name)\n\n# make sure all required groups had one option present\nfor group in self._mutually_exclusive_groups:\n    if group.required:\n        for action in group._group_actions:\n            if action in seen_non_default_actions:\n                break\n\n        # if no actions were used, report the error\n        else:\n            names = [_get_action_name(action)\n                     for action in group._group_actions\n                     if action.help is not SUPPRESS]\n            msg = _('one of the arguments %s is required')\n            self.error(msg % ' '.join(names))\n\n# return the updated namespace and the extra arguments\nreturn namespace, extras", "path": "argparse.py", "repo_name": "frxxbase-xx/jgd", "stars": 17, "license": "None", "language": "python", "size": 4248}
{"docstring": "# remove all conflicting options\n", "func_signal": "def _handle_conflict_resolve(self, action, conflicting_actions):\n", "code": "        for option_string, action in conflicting_actions:\n    # remove the conflicting option\n            action.option_strings.remove(option_string)\n            self._option_string_actions.pop(option_string, None)\n    # if the option now has no option string, remove it from the\n            # container holding it\n            if not action.option_strings:\n                action.container._remove_action(action)", "path": "argparse.py", "repo_name": "frxxbase-xx/jgd", "stars": 17, "license": "None", "language": "python", "size": 4248}
{"docstring": "\"\"\"\nadd_argument(dest, ..., name=value, ...)\nadd_argument(option_string, option_string, ..., name=value, ...)\n\"\"\"\n\n# if no positional args are supplied or only one is supplied and\n# it doesn't look like an option string, parse a positional\n# argument\n", "func_signal": "def add_argument(self, *args, **kwargs):\n", "code": "chars = self.prefix_chars\nif not args or len(args) == 1 and args[0][0] not in chars:\n    if args and 'dest' in kwargs:\n        raise ValueError('dest supplied twice for positional argument')\n    kwargs = self._get_positional_kwargs(*args, **kwargs)\n\n# otherwise, we're adding an optional argument\nelse:\n    kwargs = self._get_optional_kwargs(*args, **kwargs)\n\n# if no default was supplied, use the parser-level default\nif 'default' not in kwargs:\n    dest = kwargs['dest']\n    if dest in self._defaults:\n        kwargs['default'] = self._defaults[dest]\n    elif self.argument_default is not None:\n        kwargs['default'] = self.argument_default\n\n# create the action object, and add it to the parser\naction_class = self._pop_action_class(kwargs)\nif not _callable(action_class):\n    raise ValueError('unknown action \"%s\"' % action_class)\naction = action_class(**kwargs)\n\n# raise an error if the action type is not callable\ntype_func = self._registry_get('type', action.type, action.type)\nif not _callable(type_func):\n    raise ValueError('%r is not callable' % type_func)\n\nreturn self._add_action(action)", "path": "argparse.py", "repo_name": "frxxbase-xx/jgd", "stars": 17, "license": "None", "language": "python", "size": 4248}
{"docstring": "# find all options that conflict with this option\n", "func_signal": "def _check_conflict(self, action):\n", "code": "        confl_optionals = []\n        for option_string in action.option_strings:\n            if option_string in self._option_string_actions:\n                confl_optional = self._option_string_actions[option_string]\n                confl_optionals.append((option_string, confl_optional))\n# resolve any conflicts\n        if confl_optionals:\n            conflict_handler = self._get_handler()\n            conflict_handler(action, confl_optionals)", "path": "argparse.py", "repo_name": "frxxbase-xx/jgd", "stars": 17, "license": "None", "language": "python", "size": 4248}
{"docstring": "# add any missing keyword arguments by checking the container\n", "func_signal": "def __init__(self, container, title=None, description=None, **kwargs):\n", "code": "update = kwargs.setdefault\nupdate('conflict_handler', container.conflict_handler)\nupdate('prefix_chars', container.prefix_chars)\nupdate('argument_default', container.argument_default)\nsuper_init = super(_ArgumentGroup, self).__init__\nsuper_init(description=description, **kwargs)\n\n# group attributes\nself.title = title\nself._group_actions = []\n\n# share most attributes with the container\nself._registries = container._registries\nself._actions = container._actions\nself._option_string_actions = container._option_string_actions\nself._defaults = container._defaults\nself._has_negative_number_optionals = \\\n    container._has_negative_number_optionals", "path": "argparse.py", "repo_name": "frxxbase-xx/jgd", "stars": 17, "license": "None", "language": "python", "size": 4248}
{"docstring": "# converted value must be one of the choices (if specified)\n", "func_signal": "def _check_value(self, action, value):\n", "code": "if action.choices is not None and value not in action.choices:\n    tup = value, ', '.join(map(repr, action.choices))\n    msg = _('invalid choice: %r (choose from %s)') % tup\n    raise ArgumentError(action, msg)", "path": "argparse.py", "repo_name": "frxxbase-xx/jgd", "stars": 17, "license": "None", "language": "python", "size": 4248}
{"docstring": "# args default to the system args\n", "func_signal": "def parse_known_args(self, args=None, namespace=None):\n", "code": "if args is None:\n    args = _sys.argv[1:]\n\n# default Namespace built from parser defaults\nif namespace is None:\n    namespace = Namespace()\n\n# add any action defaults that aren't present\nfor action in self._actions:\n    if action.dest is not SUPPRESS:\n        if not hasattr(namespace, action.dest):\n            if action.default is not SUPPRESS:\n                default = action.default\n                if isinstance(action.default, _basestring):\n                    default = self._get_value(action, default)\n                setattr(namespace, action.dest, default)\n\n# add any parser defaults that aren't present\nfor dest in self._defaults:\n    if not hasattr(namespace, dest):\n        setattr(namespace, dest, self._defaults[dest])\n\n# parse the arguments and exit if there are any errors\ntry:\n    return self._parse_known_args(args, namespace)\nexcept ArgumentError:\n    err = _sys.exc_info()[1]\n    self.error(str(err))", "path": "argparse.py", "repo_name": "frxxbase-xx/jgd", "stars": 17, "license": "None", "language": "python", "size": 4248}
{"docstring": "# progressively shorten the actions list by slicing off the\n# final actions until we find a match\n", "func_signal": "def _match_arguments_partial(self, actions, arg_strings_pattern):\n", "code": "result = []\nfor i in range(len(actions), 0, -1):\n    actions_slice = actions[:i]\n    pattern = ''.join([self._get_nargs_pattern(action)\n                       for action in actions_slice])\n    match = _re.match(pattern, arg_strings_pattern)\n    if match is not None:\n        result.extend([len(string) for string in match.groups()])\n        break\n\n# return the list of arg string counts\nreturn result", "path": "argparse.py", "repo_name": "frxxbase-xx/jgd", "stars": 17, "license": "None", "language": "python", "size": 4248}
{"docstring": "# resolve any conflicts\n", "func_signal": "def _add_action(self, action):\n", "code": "self._check_conflict(action)\n\n# add to actions list\nself._actions.append(action)\naction.container = self\n\n# index the action by any option strings it has\nfor option_string in action.option_strings:\n    self._option_string_actions[option_string] = action\n\n# set the flag if any option strings look like negative numbers\nfor option_string in action.option_strings:\n    if self._negative_number_matcher.match(option_string):\n        if not self._has_negative_number_optionals:\n            self._has_negative_number_optionals.append(True)\n\n# return the created action\nreturn action", "path": "argparse.py", "repo_name": "frxxbase-xx/jgd", "stars": 17, "license": "None", "language": "python", "size": 4248}
{"docstring": "# determine the required width and the entry label\n", "func_signal": "def _format_action(self, action):\n", "code": "help_position = min(self._action_max_length + 2,\n                    self._max_help_position)\nhelp_width = self._width - help_position\naction_width = help_position - self._current_indent - 2\naction_header = self._format_action_invocation(action)\n\n# ho nelp; start on same line and add a final newline\nif not action.help:\n    tup = self._current_indent, '', action_header\n    action_header = '%*s%s\\n' % tup\n\n# short action name; start on the same line and pad two spaces\nelif len(action_header) <= action_width:\n    tup = self._current_indent, '', action_width, action_header\n    action_header = '%*s%-*s  ' % tup\n    indent_first = 0\n\n# long action name; start on the next line\nelse:\n    tup = self._current_indent, '', action_header\n    action_header = '%*s%s\\n' % tup\n    indent_first = help_position\n\n# collect the pieces of the action help\nparts = [action_header]\n\n# if there was help for the action, add lines of help text\nif action.help:\n    help_text = self._expand_help(action)\n    help_lines = self._split_lines(help_text, help_width)\n    parts.append('%*s%s\\n' % (indent_first, '', help_lines[0]))\n    for line in help_lines[1:]:\n        parts.append('%*s%s\\n' % (help_position, '', line))\n\n# or add a newline if the description doesn't end with one\nelif not action_header.endswith('\\n'):\n    parts.append('\\n')\n\n# if there are any sub-actions, add their help as well\nfor subaction in self._iter_indented_subactions(action):\n    parts.append(self._format_action(subaction))\n\n# return a single string\nreturn self._join_parts(parts)", "path": "argparse.py", "repo_name": "frxxbase-xx/jgd", "stars": 17, "license": "None", "language": "python", "size": 4248}
{"docstring": "\"\"\"Flips the bit order, because that's what Fossil wants.\"\"\"\n", "func_signal": "def flip(self,c):\n", "code": "l=[0, 8, 4, 12, 2, 10, 6, 14, 1, 9, 5, 13, 3, 11, 7, 15];\nreturn ((l[c&0x0F]) << 4) + l[(c & 0xF0) >> 4];", "path": "pymw.py", "repo_name": "travisgoodspeed/PyMetaWatch", "stars": 28, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"returns 0 on success, -1 on failure\"\"\"\n# save current filter\n", "func_signal": "def write_inquiry_mode(sock, mode):\n", "code": "old_filter = sock.getsockopt( bluez.SOL_HCI, bluez.HCI_FILTER, 14)\n\n# Setup socket filter to receive only events related to the\n# write_inquiry_mode command\nflt = bluez.hci_filter_new()\nopcode = bluez.cmd_opcode_pack(bluez.OGF_HOST_CTL, \n        bluez.OCF_WRITE_INQUIRY_MODE)\nbluez.hci_filter_set_ptype(flt, bluez.HCI_EVENT_PKT)\nbluez.hci_filter_set_event(flt, bluez.EVT_CMD_COMPLETE);\nbluez.hci_filter_set_opcode(flt, opcode)\nsock.setsockopt( bluez.SOL_HCI, bluez.HCI_FILTER, flt )\n\n# send the command!\nbluez.hci_send_cmd(sock, bluez.OGF_HOST_CTL, \n        bluez.OCF_WRITE_INQUIRY_MODE, struct.pack(\"B\", mode) )\n\npkt = sock.recv(255)\n\nstatus = struct.unpack(\"xxxxxxB\", pkt)[0]\n\n# restore old filter\nsock.setsockopt( bluez.SOL_HCI, bluez.HCI_FILTER, old_filter )\nif status != 0: return -1\nreturn 0", "path": "inquiry-with-rssi.py", "repo_name": "travisgoodspeed/PyMetaWatch", "stars": 28, "license": "None", "language": "python", "size": 204}
{"docstring": "#crcval=0;\n", "func_signal": "def checksum(self,str):\n", "code": "        crcval=0xFFFF;\n        for c in str:\n            crcval=self.update_crc(crcval, ord(c));\n        return crcval;", "path": "pymw.py", "repo_name": "travisgoodspeed/PyMetaWatch", "stars": 28, "license": "None", "language": "python", "size": 204}
{"docstring": "#crcval=0;\n", "func_signal": "def checksum(self,str):\n", "code": "        crcval=0xFFFF;\n        for c in str:\n            crcval=self.update_crc(crcval, ord(c));\n        return crcval;", "path": "medic.py", "repo_name": "travisgoodspeed/PyMetaWatch", "stars": 28, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"Returns the hex decoded version of a byte string.\"\"\"\n", "func_signal": "def hex(str):\n", "code": "toret=\"\";\nif str==None: return \"none\";\nfor c in str:\n    toret=\"%s %02x\" % (toret,ord(c));\nreturn toret;", "path": "pymw.py", "repo_name": "travisgoodspeed/PyMetaWatch", "stars": 28, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"Get the information string.\"\"\"\n", "func_signal": "def getinfostr(self,item=0x00):\n", "code": "string=self.tx(\"\\x03\\x00%s\" % chr(item));\nreturn string[:len(string)-2];  #Don't include the checksum.\n\ndef getinfo(self):\n    \"\"\"Get all the information strings.\"\"\";\nmodel=self.getinfostr(0);\nversion=self.getinfostr(1);\nreturn \"%s %s\" % (model,version);", "path": "medic.py", "repo_name": "travisgoodspeed/PyMetaWatch", "stars": 28, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"Writes image data to the Draw Buffer.\nYou'll need to send activatedisplay() to swap things over when done.\"\"\"\n", "func_signal": "def writebuffer(self, mode, row1, data1, row2=None, data2=None):\n", "code": "option=mode; #idle screen, single row.\nif row2 != None:\n    option = option | 0x10;\n\npacket=\"\\x40%s%s%s\\x00\" % (\n        chr(option),\n        chr(row1),\n        data1[0:12]\n        )\nif row2!=None:\n    packet=\"%s%s%s\" % (\n            packet,\n            chr(row2),\n            data2[0:11]);\nself.tx(packet,rx=False);", "path": "pymw.py", "repo_name": "travisgoodspeed/PyMetaWatch", "stars": 28, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"Writes image data to the Draw Buffer.\nYou'll need to send activatedisplay() to swap things over when done.\"\"\"\n", "func_signal": "def writebuffer(self, mode, row1, data1, row2=None, data2=None):\n", "code": "option=mode; #idle screen, single row.\nif row2 != None:\n    option = option | 0x10;\n\npacket=\"\\x40%s%s%s\\x00\" % (\n        chr(option),\n        chr(row1),\n        data1[0:12]\n        )\nif row2!=None:\n    packet=\"%s%s%s\" % (\n            packet,\n            chr(row2),\n            data2[0:11]);\nself.tx(packet,rx=False);", "path": "medic.py", "repo_name": "travisgoodspeed/PyMetaWatch", "stars": 28, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"returns 0 on success, -1 on failure\"\"\"\n# save current filter\n", "func_signal": "def write_inquiry_mode(sock, mode):\n", "code": "old_filter = sock.getsockopt( bluez.SOL_HCI, bluez.HCI_FILTER, 14)\n\n# Setup socket filter to receive only events related to the\n# write_inquiry_mode command\nflt = bluez.hci_filter_new()\nopcode = bluez.cmd_opcode_pack(bluez.OGF_HOST_CTL, \n        bluez.OCF_WRITE_INQUIRY_MODE)\nbluez.hci_filter_set_ptype(flt, bluez.HCI_EVENT_PKT)\nbluez.hci_filter_set_event(flt, bluez.EVT_CMD_COMPLETE);\nbluez.hci_filter_set_opcode(flt, opcode)\nsock.setsockopt( bluez.SOL_HCI, bluez.HCI_FILTER, flt )\n\n# send the command!\nbluez.hci_send_cmd(sock, bluez.OGF_HOST_CTL, \n        bluez.OCF_WRITE_INQUIRY_MODE, struct.pack(\"B\", mode) )\n\npkt = sock.recv(255)\n\nstatus = struct.unpack(\"xxxxxxB\", pkt)[0]\n\n# restore old filter\nsock.setsockopt( bluez.SOL_HCI, bluez.HCI_FILTER, old_filter )\nif status != 0: return -1\nreturn 0", "path": "medic.py", "repo_name": "travisgoodspeed/PyMetaWatch", "stars": 28, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"returns the current mode, or -1 on failure\"\"\"\n# save current filter\n", "func_signal": "def read_inquiry_mode(sock):\n", "code": "old_filter = sock.getsockopt( bluez.SOL_HCI, bluez.HCI_FILTER, 14)\n\n# Setup socket filter to receive only events related to the\n# read_inquiry_mode command\nflt = bluez.hci_filter_new()\nopcode = bluez.cmd_opcode_pack(bluez.OGF_HOST_CTL, \n        bluez.OCF_READ_INQUIRY_MODE)\nbluez.hci_filter_set_ptype(flt, bluez.HCI_EVENT_PKT)\nbluez.hci_filter_set_event(flt, bluez.EVT_CMD_COMPLETE);\nbluez.hci_filter_set_opcode(flt, opcode)\nsock.setsockopt( bluez.SOL_HCI, bluez.HCI_FILTER, flt )\n\n# first read the current inquiry mode.\nbluez.hci_send_cmd(sock, bluez.OGF_HOST_CTL, \n        bluez.OCF_READ_INQUIRY_MODE )\n\npkt = sock.recv(255)\n\nstatus,mode = struct.unpack(\"xxxxxxBB\", pkt)\nif status != 0: mode = -1\n\n# restore old filter\nsock.setsockopt( bluez.SOL_HCI, bluez.HCI_FILTER, old_filter )\nreturn mode", "path": "medic.py", "repo_name": "travisgoodspeed/PyMetaWatch", "stars": 28, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"Set the date and time of the watch to the system time.\"\"\"\n", "func_signal": "def setclock(self):\n", "code": "ltime=time.localtime();\n#Year in BIG ENDIAN, not LITTLE.\nstr=\"\\x07\\xdb%s%s%s%s%s%s\\x01\\x01\" % (\n    chr(ltime.tm_mon),  #month\n    chr(ltime.tm_mday), #day of month\n    chr((ltime.tm_wday+1) % 7), #day of week\n    chr(ltime.tm_hour), #hour\n    chr(ltime.tm_min),  #min\n    chr(ltime.tm_sec),  #sec\n    );\nself.tx(\"\\x26\\x00\"+str, rx=False);", "path": "pymw.py", "repo_name": "travisgoodspeed/PyMetaWatch", "stars": 28, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"Buzz the buzzer.\"\"\"\n\n", "func_signal": "def buzz(self, ms_on=500, ms_off=500, cycles=1):\n", "code": "ms_on = min(ms_on, 65535)\nms_off = min(ms_off, 65535)\ncycles = min(cycles, 256)\n\nmessage = []\nmessage.append(\"\\x23\\x00\\x01\")\nmessage.append(chr(ms_on % 256))\nmessage.append(chr(ms_on / 256))\nmessage.append(chr(ms_off % 256))\nmessage.append(chr(ms_off / 256))\nmessage.append(chr(cycles))\nself.tx(''.join(message), False)", "path": "pymw.py", "repo_name": "travisgoodspeed/PyMetaWatch", "stars": 28, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"Update the display to a particular mode.\"\"\"\n", "func_signal": "def updatedisplay(self,mode=0,activate=0):\n", "code": "time.sleep(0.1)\nif activate:\n    mode=mode|0x10\n#time.sleep(0.1); #rate limiting is necessary here.\nself.tx(\"\\x43%s\" % chr(mode),rx=False)", "path": "pymw.py", "repo_name": "travisgoodspeed/PyMetaWatch", "stars": 28, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"Get the information string.\"\"\"\n", "func_signal": "def getinfostr(self,item=0x00):\n", "code": "string=self.tx(\"\\x03\\x00%s\" % chr(item));\nreturn string[:len(string)-2];  #Don't include the checksum.\n\ndef getinfo(self):\n    \"\"\"Get all the information strings.\"\"\";\nmodel=self.getinfostr(0);\nversion=self.getinfostr(1);\nreturn \"%s %s\" % (model,version);", "path": "pymw.py", "repo_name": "travisgoodspeed/PyMetaWatch", "stars": 28, "license": "None", "language": "python", "size": 204}
{"docstring": "#address is D0:37:61:C3:1A:EE\n", "func_signal": "def main():\n", "code": "watchaddr=\"D0:37:61:C3:1A:EE\";\nif len(sys.argv)>1: watchaddr=sys.argv[1];\nmw=MetaWatch(watchaddr,debug=False);\n\n#mode=MODE_IDLE;\nmode=MODE_NOTIFICATION;\n\nmw.getBatteryVoltage()\n\n#mw.getButtonConfiguration(mode,0)\n\nmw.configureWatchMode(mode=mode, invertDisplay=True)\n\n# First, clear the draw buffer to a filled template.\nmw.clearbuffer(mode=mode,filled=False);\n\nimgfile=\"test.bmp\";\nif len(sys.argv)>2:\n    imgfile=sys.argv[2];\n\nmw.writeText(mode,\"Medic Mode\")\n\n#mw.enableButton(mode,BUTTON_A, BUTTON_TYPE_IMMEDIATE)\n#mw.enableButton(mode,BUTTON_B, BUTTON_TYPE_IMMEDIATE)\n#mw.enableButton(mode,BUTTON_C, BUTTON_TYPE_IMMEDIATE)\n#mw.enableButton(mode,BUTTON_D, BUTTON_TYPE_IMMEDIATE)\n#mw.enableButton(mode,BUTTON_E, BUTTON_TYPE_IMMEDIATE)\n#mw.enableButton(mode,BUTTON_F, BUTTON_TYPE_IMMEDIATE)\n\n#mw.disableButton(mode,BUTTON_A, BUTTON_TYPE_PRESSANDRELEASE)\n#mw.disableButton(mode,BUTTON_B, BUTTON_TYPE_PRESSANDRELEASE)\n#mw.disableButton(mode,BUTTON_C, BUTTON_TYPE_PRESSANDRELEASE)", "path": "medic.py", "repo_name": "travisgoodspeed/PyMetaWatch", "stars": 28, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"Flips the bit order, because that's what Fossil wants.\"\"\"\n", "func_signal": "def flip(self,c):\n", "code": "l=[0, 8, 4, 12, 2, 10, 6, 14, 1, 9, 5, 13, 3, 11, 7, 15];\nreturn ((l[c&0x0F]) << 4) + l[(c & 0xF0) >> 4];", "path": "medic.py", "repo_name": "travisgoodspeed/PyMetaWatch", "stars": 28, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"Set the date and time of the watch to the system time.\"\"\"\n", "func_signal": "def setclock(self):\n", "code": "ltime=time.localtime();\n#Year in BIG ENDIAN, not LITTLE.\nstr=\"\\x07\\xdb%s%s%s%s%s%s\\x01\\x01\" % (\n    chr(ltime.tm_mon),  #month\n    chr(ltime.tm_mday), #day of month\n    chr((ltime.tm_wday+1) % 7), #day of week\n    chr(ltime.tm_hour), #hour\n    chr(ltime.tm_min),  #min\n    chr(ltime.tm_sec),  #sec\n    );\nself.tx(\"\\x26\\x00\"+str, rx=False);", "path": "medic.py", "repo_name": "travisgoodspeed/PyMetaWatch", "stars": 28, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"returns the current mode, or -1 on failure\"\"\"\n# save current filter\n", "func_signal": "def read_inquiry_mode(sock):\n", "code": "old_filter = sock.getsockopt( bluez.SOL_HCI, bluez.HCI_FILTER, 14)\n\n# Setup socket filter to receive only events related to the\n# read_inquiry_mode command\nflt = bluez.hci_filter_new()\nopcode = bluez.cmd_opcode_pack(bluez.OGF_HOST_CTL, \n        bluez.OCF_READ_INQUIRY_MODE)\nbluez.hci_filter_set_ptype(flt, bluez.HCI_EVENT_PKT)\nbluez.hci_filter_set_event(flt, bluez.EVT_CMD_COMPLETE);\nbluez.hci_filter_set_opcode(flt, opcode)\nsock.setsockopt( bluez.SOL_HCI, bluez.HCI_FILTER, flt )\n\n# first read the current inquiry mode.\nbluez.hci_send_cmd(sock, bluez.OGF_HOST_CTL, \n        bluez.OCF_READ_INQUIRY_MODE )\n\npkt = sock.recv(255)\n\nstatus,mode = struct.unpack(\"xxxxxxBB\", pkt)\nif status != 0: mode = -1\n\n# restore old filter\nsock.setsockopt( bluez.SOL_HCI, bluez.HCI_FILTER, old_filter )\nreturn mode", "path": "inquiry-with-rssi.py", "repo_name": "travisgoodspeed/PyMetaWatch", "stars": 28, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"Returns the hex decoded version of a byte string.\"\"\"\n", "func_signal": "def hex(str):\n", "code": "toret=\"\";\nif str==None: return \"none\";\nfor c in str:\n    toret=\"%s %02x\" % (toret,ord(c));\nreturn toret;", "path": "medic.py", "repo_name": "travisgoodspeed/PyMetaWatch", "stars": 28, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"Set whether the watch shows the time at top.\"\"\"\n\n", "func_signal": "def showtime(self, watch_controls_top):\n", "code": "message = []\nmessage.append(\"\\x42\\x00\")\nif watch_controls_top:\n    message.append(\"\\x00\")\nelse:\n    message.append(\"\\x01\")\n\nself.tx(''.join(message), False)", "path": "medic.py", "repo_name": "travisgoodspeed/PyMetaWatch", "stars": 28, "license": "None", "language": "python", "size": 204}
{"docstring": "#print \"End tag %s\" % name\n", "func_signal": "def unknown_endtag(self, name):\n", "code": "if self.quoteStack and self.quoteStack[-1] != name:\n    #This is not a real end tag.\n    #print \"</%s> is not real!\" % name\n    self.handle_data('</%s>' % name)\n    return\nself.endData()\nself._popToTag(name)\nif self.quoteStack and self.quoteStack[-1] == name:\n    self.quoteStack.pop()\n    self.literal = (len(self.quoteStack) > 0)", "path": "appmedia\\BeautifulSoup.py", "repo_name": "divio/django-appmedia", "stars": 18, "license": "None", "language": "python", "size": 125}
{"docstring": "\"\"\"Adds a certain piece of text to the tree as a NavigableString\nsubclass.\"\"\"\n", "func_signal": "def _toStringSubclass(self, text, subclass):\n", "code": "self.endData()\nself.handle_data(text)\nself.endData(subclass)", "path": "appmedia\\BeautifulSoup.py", "repo_name": "divio/django-appmedia", "stars": 18, "license": "None", "language": "python", "size": 125}
{"docstring": "\"\"\"Handle a processing instruction as a ProcessingInstruction\nobject, possibly one with a %SOUP-ENCODING% slot into which an\nencoding will be plugged later.\"\"\"\n", "func_signal": "def handle_pi(self, text):\n", "code": "if text[:3] == \"xml\":\n    text = u\"xml version='1.0' encoding='%SOUP-ENCODING%'\"\nself._toStringSubclass(text, ProcessingInstruction)", "path": "appmedia\\BeautifulSoup.py", "repo_name": "divio/django-appmedia", "stars": 18, "license": "None", "language": "python", "size": 125}
{"docstring": "\"\"\"This method fixes a bug in Python's SGMLParser.\"\"\"\n", "func_signal": "def convert_charref(self, name):\n", "code": "try:\n    n = int(name)\nexcept ValueError:\n    return\nif not 0 <= n <= 127 : # ASCII ends at 127, not 255\n    return\nreturn self.convert_codepoint(n)", "path": "appmedia\\BeautifulSoup.py", "repo_name": "divio/django-appmedia", "stars": 18, "license": "None", "language": "python", "size": 125}
{"docstring": "\"\"\"Returns the parents of this Tag that match the given\ncriteria.\"\"\"\n\n", "func_signal": "def findParents(self, name=None, attrs={}, limit=None, **kwargs):\n", "code": "return self._findAll(name, attrs, None, limit, self.parentGenerator,\n                     **kwargs)", "path": "appmedia\\BeautifulSoup.py", "repo_name": "divio/django-appmedia", "stars": 18, "license": "None", "language": "python", "size": 125}
{"docstring": "\"\"\"Handle entity references as data, possibly converting known\nHTML and/or XML entity references to the corresponding Unicode\ncharacters.\"\"\"\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "data = None\nif self.convertHTMLEntities:\n    try:\n        data = unichr(name2codepoint[ref])\n    except KeyError:\n        pass\n\nif not data and self.convertXMLEntities:\n        data = self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref)\n\nif not data and self.convertHTMLEntities and \\\n    not self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref):\n        # TODO: We've got a problem here. We're told this is\n        # an entity reference, but it's not an XML entity\n        # reference or an HTML entity reference. Nonetheless,\n        # the logical thing to do is to pass it through as an\n        # unrecognized entity reference.\n        #\n        # Except: when the input is \"&carol;\" this function\n        # will be called with input \"carol\". When the input is\n        # \"AT&T\", this function will be called with input\n        # \"T\". We have no way of knowing whether a semicolon\n        # was present originally, so we don't know whether\n        # this is an unknown entity or just a misplaced\n        # ampersand.\n        #\n        # The more common case is a misplaced ampersand, so I\n        # escape the ampersand and omit the trailing semicolon.\n        data = \"&amp;%s\" % ref\nif not data:\n    # This case is different from the one above, because we\n    # haven't already gone through a supposedly comprehensive\n    # mapping of entities to Unicode characters. We might not\n    # have gone through any mapping at all. So the chances are\n    # very high that this is a real entity, and not a\n    # misplaced ampersand.\n    data = \"&%s;\" % ref\nself.handle_data(data)", "path": "appmedia\\BeautifulSoup.py", "repo_name": "divio/django-appmedia", "stars": 18, "license": "None", "language": "python", "size": 125}
{"docstring": "\"\"\"Initializes a map representation of this tag's attributes,\nif not already initialized.\"\"\"\n", "func_signal": "def _getAttrMap(self):\n", "code": "if not getattr(self, 'attrMap'):\n    self.attrMap = {}\n    for (key, value) in self.attrs:\n        self.attrMap[key] = value\nreturn self.attrMap", "path": "appmedia\\BeautifulSoup.py", "repo_name": "divio/django-appmedia", "stars": 18, "license": "None", "language": "python", "size": 125}
{"docstring": "\"\"\"Sets up the initial relations between this element and\nother elements.\"\"\"\n", "func_signal": "def setup(self, parent=None, previous=None):\n", "code": "self.parent = parent\nself.previous = previous\nself.next = None\nself.previousSibling = None\nself.nextSibling = None\nif self.parent and self.parent.contents:\n    self.previousSibling = self.parent.contents[-1]\n    self.previousSibling.nextSibling = self", "path": "appmedia\\BeautifulSoup.py", "repo_name": "divio/django-appmedia", "stars": 18, "license": "None", "language": "python", "size": 125}
{"docstring": "\"\"\"Turns a list of maps, lists, or scalars into a single map.\nUsed to build the SELF_CLOSING_TAGS, NESTABLE_TAGS, and\nNESTING_RESET_TAGS maps out of lists and partial maps.\"\"\"\n", "func_signal": "def buildTagMap(default, *args):\n", "code": "built = {}\nfor portion in args:\n    if hasattr(portion, 'items'):\n        #It's a map. Merge it.\n        for k,v in portion.items():\n            built[k] = v\n    elif isList(portion):\n        #It's a list. Map each item to the default.\n        for k in portion:\n            built[k] = default\n    else:\n        #It's a scalar. Map it to the default.\n        built[portion] = default\nreturn built", "path": "appmedia\\BeautifulSoup.py", "repo_name": "divio/django-appmedia", "stars": 18, "license": "None", "language": "python", "size": 125}
{"docstring": "#print \"Push\", tag.name\n", "func_signal": "def pushTag(self, tag):\n", "code": "if self.currentTag:\n    self.currentTag.contents.append(tag)\nself.tagStack.append(tag)\nself.currentTag = self.tagStack[-1]", "path": "appmedia\\BeautifulSoup.py", "repo_name": "divio/django-appmedia", "stars": 18, "license": "None", "language": "python", "size": 125}
{"docstring": "\"\"\"Returns the closest parent of this Tag that matches the given\ncriteria.\"\"\"\n# NOTE: We can't use _findOne because findParents takes a different\n# set of arguments.\n", "func_signal": "def findParent(self, name=None, attrs={}, **kwargs):\n", "code": "r = None\nl = self.findParents(name, attrs, 1)\nif l:\n    r = l[0]\nreturn r", "path": "appmedia\\BeautifulSoup.py", "repo_name": "divio/django-appmedia", "stars": 18, "license": "None", "language": "python", "size": 125}
{"docstring": "\"\"\"Returns the closest sibling to this Tag that matches the\ngiven criteria and appears after this Tag in the document.\"\"\"\n", "func_signal": "def findNextSibling(self, name=None, attrs={}, text=None, **kwargs):\n", "code": "return self._findOne(self.findNextSiblings, name, attrs, text,\n                     **kwargs)", "path": "appmedia\\BeautifulSoup.py", "repo_name": "divio/django-appmedia", "stars": 18, "license": "None", "language": "python", "size": 125}
{"docstring": "\"\"\"Encodes an object to a string in some encoding, or to Unicode.\n.\"\"\"\n", "func_signal": "def toEncoding(self, s, encoding=None):\n", "code": "if isinstance(s, unicode):\n    if encoding:\n        s = s.encode(encoding)\nelif isinstance(s, str):\n    if encoding:\n        s = s.encode(encoding)\n    else:\n        s = unicode(s)\nelse:\n    if encoding:\n        s  = self.toEncoding(str(s), encoding)\n    else:\n        s = unicode(s)\nreturn s", "path": "appmedia\\BeautifulSoup.py", "repo_name": "divio/django-appmedia", "stars": 18, "license": "None", "language": "python", "size": 125}
{"docstring": "#print \"Matching %s against %s\" % (markup, matchAgainst)\n", "func_signal": "def _matches(self, markup, matchAgainst):\n", "code": "result = False\nif matchAgainst == True and type(matchAgainst) == types.BooleanType:\n    result = markup != None\nelif callable(matchAgainst):\n    result = matchAgainst(markup)\nelse:\n    #Custom match methods take the tag as an argument, but all\n    #other ways of matching match the tag name as a string.\n    if isinstance(markup, Tag):\n        markup = markup.name\n    if markup and not isString(markup):\n        markup = unicode(markup)\n    #Now we know that chunk is either a string, or None.\n    if hasattr(matchAgainst, 'match'):\n        # It's a regexp object.\n        result = markup and matchAgainst.search(markup)\n    elif isList(matchAgainst):\n        result = markup in matchAgainst\n    elif hasattr(matchAgainst, 'items'):\n        result = markup.has_key(matchAgainst)\n    elif matchAgainst and isString(markup):\n        if isinstance(markup, unicode):\n            matchAgainst = unicode(matchAgainst)\n        else:\n            matchAgainst = str(matchAgainst)\n\n    if not result:\n        result = matchAgainst == markup\nreturn result", "path": "appmedia\\BeautifulSoup.py", "repo_name": "divio/django-appmedia", "stars": 18, "license": "None", "language": "python", "size": 125}
{"docstring": "\"\"\"Recursively destroys the contents of this tree.\"\"\"\n", "func_signal": "def decompose(self):\n", "code": "contents = [i for i in self.contents]\nfor i in contents:\n    if isinstance(i, Tag):\n        i.decompose()\n    else:\n        i.extract()\nself.extract()", "path": "appmedia\\BeautifulSoup.py", "repo_name": "divio/django-appmedia", "stars": 18, "license": "None", "language": "python", "size": 125}
{"docstring": "\"\"\"Pops the tag stack up to and including the most recent\ninstance of the given tag. If inclusivePop is false, pops the tag\nstack up to but *not* including the most recent instqance of\nthe given tag.\"\"\"\n#print \"Popping to %s\" % name\n", "func_signal": "def _popToTag(self, name, inclusivePop=True):\n", "code": "if name == self.ROOT_TAG_NAME:\n    return\n\nnumPops = 0\nmostRecentTag = None\nfor i in range(len(self.tagStack)-1, 0, -1):\n    if name == self.tagStack[i].name:\n        numPops = len(self.tagStack)-i\n        break\nif not inclusivePop:\n    numPops = numPops - 1\n\nfor i in range(0, numPops):\n    mostRecentTag = self.popTag()\nreturn mostRecentTag", "path": "appmedia\\BeautifulSoup.py", "repo_name": "divio/django-appmedia", "stars": 18, "license": "None", "language": "python", "size": 125}
{"docstring": "\"\"\"Returns true iff this tag has the same name, the same attributes,\nand the same contents (recursively) as the given tag.\n\nNOTE: right now this will return false if two tags have the\nsame attributes in a different order. Should this be fixed?\"\"\"\n", "func_signal": "def __eq__(self, other):\n", "code": "if not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):\n    return False\nfor i in range(0, len(self.contents)):\n    if self.contents[i] != other.contents[i]:\n        return False\nreturn True", "path": "appmedia\\BeautifulSoup.py", "repo_name": "divio/django-appmedia", "stars": 18, "license": "None", "language": "python", "size": 125}
{"docstring": "# Convert the document to Unicode.\n", "func_signal": "def _feed(self, inDocumentEncoding=None):\n", "code": "markup = self.markup\nif isinstance(markup, unicode):\n    if not hasattr(self, 'originalEncoding'):\n        self.originalEncoding = None\nelse:\n    dammit = UnicodeDammit\\\n             (markup, [self.fromEncoding, inDocumentEncoding],\n              smartQuotesTo=self.smartQuotesTo)\n    markup = dammit.unicode\n    self.originalEncoding = dammit.originalEncoding\nif markup:\n    if self.markupMassage:\n        if not isList(self.markupMassage):\n            self.markupMassage = self.MARKUP_MASSAGE\n        for fix, m in self.markupMassage:\n            markup = fix.sub(m, markup)\n        # TODO: We get rid of markupMassage so that the\n        # soup object can be deepcopied later on. Some\n        # Python installations can't copy regexes. If anyone\n        # was relying on the existence of markupMassage, this\n        # might cause problems.\n        del(self.markupMassage)\nself.reset()\n\nSGMLParser.feed(self, markup)\n# Close out any unfinished strings and close all the open tags.\nself.endData()\nwhile self.currentTag.name != self.ROOT_TAG_NAME:\n    self.popTag()", "path": "appmedia\\BeautifulSoup.py", "repo_name": "divio/django-appmedia", "stars": 18, "license": "None", "language": "python", "size": 125}
{"docstring": "\"\"\"Used in a call to re.sub to replace HTML, XML, and numeric\nentities with the appropriate Unicode characters. If HTML\nentities are being converted, any unrecognized entities are\nescaped.\"\"\"\n", "func_signal": "def _convertEntities(self, match):\n", "code": "x = match.group(1)\nif self.convertHTMLEntities and x in name2codepoint:\n    return unichr(name2codepoint[x])\nelif x in self.XML_ENTITIES_TO_SPECIAL_CHARS:\n    if self.convertXMLEntities:\n        return self.XML_ENTITIES_TO_SPECIAL_CHARS[x]\n    else:\n        return u'&%s;' % x\nelif len(x) > 0 and x[0] == '#':\n    # Handle numeric entities\n    if len(x) > 1 and x[1] == 'x':\n        return unichr(int(x[2:], 16))\n    else:\n        return unichr(int(x[1:]))\n\nelif self.escapeUnrecognizedEntities:\n    return u'&amp;%s;' % x\nelse:\n    return u'&%s;' % x", "path": "appmedia\\BeautifulSoup.py", "repo_name": "divio/django-appmedia", "stars": 18, "license": "None", "language": "python", "size": 125}
{"docstring": "\"\"\"This method routes method call requests to either the SGMLParser\nsuperclass or the Tag superclass, depending on the method name.\"\"\"\n#print \"__getattr__ called on %s.%s\" % (self.__class__, methodName)\n\n", "func_signal": "def __getattr__(self, methodName):\n", "code": "if methodName.find('start_') == 0 or methodName.find('end_') == 0 \\\n       or methodName.find('do_') == 0:\n    return SGMLParser.__getattr__(self, methodName)\nelif methodName.find('__') != 0:\n    return Tag.__getattr__(self, methodName)\nelse:\n    raise AttributeError", "path": "appmedia\\BeautifulSoup.py", "repo_name": "divio/django-appmedia", "stars": 18, "license": "None", "language": "python", "size": 125}
{"docstring": "# Within this algorithm the order of steps described in the\n# specification is not quite the same as the order of steps in the\n# code. It should still do the same though.\n\n# Step 1: stop the algorithm when there's nothing to do.\n", "func_signal": "def reconstructActiveFormattingElements(self):\n", "code": "if not self.activeFormattingElements:\n    return\n\n# Step 2 and step 3: we start with the last element. So i is -1.\ni = len(self.activeFormattingElements) - 1\nentry = self.activeFormattingElements[i]\nif entry == Marker or entry in self.openElements:\n    return\n\n# Step 6\nwhile entry != Marker and entry not in self.openElements:\n    if i == 0:\n        #This will be reset to 0 below\n        i = -1\n        break\n    i -= 1\n    # Step 5: let entry be one earlier in the list.\n    entry = self.activeFormattingElements[i]\n\nwhile True:\n    # Step 7\n    i += 1\n\n    # Step 8\n    entry = self.activeFormattingElements[i]\n    clone = entry.cloneNode() #Mainly to get a new copy of the attributes\n\n    # Step 9\n    element = self.insertElement({\"type\":\"StartTag\", \n                                  \"name\":clone.name, \n                                  \"namespace\":clone.namespace, \n                                  \"data\":clone.attributes})\n\n    # Step 10\n    self.activeFormattingElements[i] = element\n\n    # Step 11\n    if element == self.activeFormattingElements[-1]:\n        break", "path": "lib\\html5lib\\treebuilders\\_base.py", "repo_name": "mhausenblas/web.instata", "stars": 26, "license": "None", "language": "python", "size": 678}
{"docstring": "\"\"\"Insert text data.\"\"\"\n", "func_signal": "def insertText(self, data, parent=None):\n", "code": "if parent is None:\n    parent = self.openElements[-1]\n\nif (not self.insertFromTable or (self.insertFromTable and\n                                 self.openElements[-1].name \n                                 not in tableInsertModeElements)):\n    parent.insertText(data)\nelse:\n    # We should be in the InTable mode. This means we want to do\n    # special magic element rearranging\n    parent, insertBefore = self.getTableMisnestedNodePosition()\n    parent.insertText(data, insertBefore)", "path": "lib\\html5lib\\treebuilders\\_base.py", "repo_name": "mhausenblas/web.instata", "stars": 26, "license": "None", "language": "python", "size": 678}
{"docstring": "#Put us back in the right whitespace handling mode\n", "func_signal": "def endTagBlock(self, token):\n", "code": "if token[\"name\"] == \"pre\":\n    self.processSpaceCharacters = self.processSpaceCharactersNonPre\ninScope = self.tree.elementInScope(token[\"name\"])\nif inScope:\n    self.tree.generateImpliedEndTags()\nif self.tree.openElements[-1].name != token[\"name\"]:\n     self.parser.parseError(\"end-tag-too-early\", {\"name\": token[\"name\"]})\nif inScope:\n    node = self.tree.openElements.pop()\n    while node.name != token[\"name\"]:\n        node = self.tree.openElements.pop()", "path": "lib\\html5lib\\html5parser.py", "repo_name": "mhausenblas/web.instata", "stars": 26, "license": "None", "language": "python", "size": 678}
{"docstring": "\"\"\"Generate the literal the C{@property}, taking into account datatype, etc.\nNote: this method is called only if the C{@property} is indeed present, no need to check.\n\nThis method is an encoding of the algorithm documented\nU{task force's wiki page<http://www.w3.org/2006/07/SWD/wiki/RDFa/LiteralObject>}.\n\nThe method returns a value whether the literal is a 'normal' literal (regardless of its datatype)\nor an XML Literal. The return value is True or False, respectively. This value is used to control whether\nthe parser should stop recursion. This also means that that if the literal is generated from @content,\nthe return value is False, regardless of the possible @datatype value.\n\n@param node: DOM element node\n@param graph: the (RDF) graph to add the properies to\n@param subject: the RDFLib URIRef serving as a subject for the generated triples\n@param state: the current state to be used for the CURIE-s\n@type state: L{State.ExecutionContext}\n@return: whether the literal is a 'normal' or an XML Literal (return value is True or False, respectively). Note that if the literal is generated from @content, the return value is False, regardless of the possible @datatype value.\n@rtype: Boolean\n\"\"\"\n", "func_signal": "def generate_literal(node, graph, subject, state):\n", "code": "def _get_literal(Pnode):\n    \"\"\"\n    Get (recursively) the full text from a DOM Node.\n\n    @param Pnode: DOM Node\n    @return: string\n    \"\"\"\n    rc = \"\"\n    for node in Pnode.childNodes:\n        if node.nodeType == node.TEXT_NODE:\n            rc = rc + node.data\n        elif node.nodeType == node.ELEMENT_NODE:\n            rc = rc + _get_literal(node)\n\n    # The decision of the group in February 2008 is not to normalize the result by default.\n    # This is reflected in the default value of the option\n    if state.options.space_preserve:\n        return rc\n    else:\n        return re.sub(r'(\\r| |\\n|\\t)+', \" \", rc).strip()\n# end getLiteral\n\ndef _get_XML_literal(Pnode):\n    \"\"\"\n    Get (recursively) the XML Literal content of a DOM Node. (Most of the processing is done\n    via a C{node.toxml} call of the xml minidom implementation.)\n\n    @param Pnode: DOM Node\n    @return: string\n    \"\"\"\n    def collectPrefixes(prefixes, node):\n        def addPf(prefx, string):\n            pf = string.split(':')[0]\n            if pf != string and pf not in prefx : prefx.append(pf)\n        # edn addPf\n\n        # first the local name of the node\n        addPf(prefixes, node.tagName)\n        # get all the attributes and children\n        for child in node.childNodes:\n            if child.nodeType == node.ELEMENT_NODE:\n                collectPrefixes(prefixes, child)\n            elif child.nodeType == node.ATTRIBUTE_NODE:\n                addPf(prefixes, node.child.name)\n    # end collectPrefixes\n\n    rc = \"\"\n    prefixes = []\n    for node in Pnode.childNodes:\n        if node.nodeType == node.ELEMENT_NODE:\n            collectPrefixes(prefixes, node)\n\n    for node in Pnode.childNodes:\n        if node.nodeType == node.TEXT_NODE:\n            rc = rc + __putBackEntities(node.data)\n        elif node.nodeType == node.ELEMENT_NODE:\n            # Decorate the element with namespaces and lang values\n            for prefix in prefixes:\n                if prefix in state.ns and not node.hasAttribute(\"xmlns:%s\" % prefix):\n                    node.setAttribute(\"xmlns:%s\" % prefix, \"%s\" % state.ns[prefix])\n            # Set the default namespace, if not done (and is available)\n            if not node.getAttribute(\"xmlns\") and state.defaultNS != None:\n                node.setAttribute(\"xmlns\", state.defaultNS)\n            # Get the lang, if necessary\n            if not node.getAttribute(\"xml:lang\") and state.lang != None:\n                node.setAttribute(\"xml:lang\", state.lang)\n            rc = rc + node.toxml()\n    return rc\n    # If XML Literals must be canonicalized for space, then this is the return line:\n    #return re.sub(r'(\\r| |\\n|\\t)+', \" \", rc).strip()\n# end getXMLLiteral\n\n# Most of the times the literal is a 'normal' one, ie, not an XML Literal\nretval = True\n\n# Get the Property URI-s\nprops = state.get_resources(node.getAttribute(\"property\"), prop=True)\n\n# Get, if exists, the value of @datatype, and figure out the language\ndatatype = None\ndtset    = False\nlang     = state.lang\nif node.hasAttribute(\"datatype\"):\n    dtset = True\n    dt = node.getAttribute(\"datatype\")\n    if dt != \"\":\n        datatype = state.get_resource(dt)\n        lang = None\n\n# The simple case: separate @content attribute\nif node.hasAttribute(\"content\"):\n    val = node.getAttribute(\"content\")\n    object = Literal(node.getAttribute(\"content\"), datatype=datatype, lang=lang)\n    # The value of datatype has been set, and the keyword paramaters take care of the rest\nelse:\n    # see if there *is* a datatype (even if it is empty!)\n    if dtset:\n        # yep. The Literal content is the pure text part of the current element:\n        # We have to check whether the specified datatype is, in fact, and\n        # explicit XML Literal\n        if datatype == XMLLiteral:\n            object = Literal(_get_XML_literal(node), datatype=XMLLiteral)\n            retval = False\n        else:\n            object = Literal(_get_literal(node), datatype=datatype, lang=lang)\n    else:\n        # no controlling @datatype. We have to see if there is markup in the contained\n        # element\n        if True in [ n.nodeType == node.ELEMENT_NODE for n in node.childNodes ]:\n            # yep, and XML Literal should be generated\n            object = Literal(_get_XML_literal(node), datatype=XMLLiteral)\n            retval = False\n        else:\n            val = _get_literal(node)\n            # At this point, there might be entities in the string that are returned as real characters by the dom\n            # implementation. That should be turned back\n            object = Literal(_get_literal(node), lang=lang)\n\n# NOTE: rdflib<2.5 didn't equal Literal with lang=\"\", hence this check\n# proably always passed?\n# All tests pass with this check removed; going with that..\n## The object may be empty, for example in an ill-defined <meta> element...\nif True:#object != \"\":\n    for prop in props:\n        graph.add((subject, prop, object))\n\nreturn retval", "path": "lib\\rdflib\\plugins\\parsers\\rdfa\\literal.py", "repo_name": "mhausenblas/web.instata", "stars": 26, "license": "None", "language": "python", "size": 678}
{"docstring": "\"\"\"Set the base URI for the comment triples.\n\nNote that this method I{must} be called at some point to complete the triples. Without it the triples\nadded via L{add_warning<CommentGraph.add_warning>}, L{add_info<CommentGraph.add_info>}, etc, will not be added to the final graph.\n\n@param URI: URIRef for the subject of the comments\n\"\"\"\n", "func_signal": "def set_base_URI(self, URI):\n", "code": "self.baseURI = URI\nif self.graph != None:\n    for obj, prop in self.accumulated_literals:\n        _add_to_comment_graph(self.graph, obj, prop, self.baseURI)\nself.accumulated_literals = []", "path": "lib\\rdflib\\plugins\\parsers\\rdfa\\options.py", "repo_name": "mhausenblas/web.instata", "stars": 26, "license": "None", "language": "python", "size": 678}
{"docstring": "# \"clear the stack back to a table context\"\n", "func_signal": "def clearStackToTableContext(self):\n", "code": "while self.tree.openElements[-1].name not in (\"table\", \"html\"):\n    #self.parser.parseError(\"unexpected-implied-end-tag-in-table\",\n    #  {\"name\":  self.tree.openElements[-1].name})\n    self.tree.openElements.pop()\n# When the current node is <html> it's an innerHTML case", "path": "lib\\html5lib\\html5parser.py", "repo_name": "mhausenblas/web.instata", "stars": 26, "license": "None", "language": "python", "size": 678}
{"docstring": "# XXX The idea is to make errorcode mandatory.\n", "func_signal": "def parseError(self, errorcode=\"XXX-undefined-error\", datavars={}):\n", "code": "self.errors.append((self.tokenizer.stream.position(), errorcode, datavars))\nif self.strict:\n    raise ParseError", "path": "lib\\html5lib\\html5parser.py", "repo_name": "mhausenblas/web.instata", "stars": 26, "license": "None", "language": "python", "size": 678}
{"docstring": "#If we get here there must be at least one non-whitespace character\n# Do the table magic!\n", "func_signal": "def insertText(self, token):\n", "code": "self.tree.insertFromTable = True\nself.parser.phases[\"inBody\"].processCharacters(token)\nself.tree.insertFromTable = False", "path": "lib\\html5lib\\html5parser.py", "repo_name": "mhausenblas/web.instata", "stars": 26, "license": "None", "language": "python", "size": 678}
{"docstring": "\"\"\" HTML5 specific normalizations to the token stream \"\"\"\n\n", "func_signal": "def normalizeToken(self, token):\n", "code": "if token[\"type\"] == tokenTypes[\"StartTag\"]:\n    token[\"data\"] = dict(token[\"data\"][::-1])\n\nreturn token", "path": "lib\\html5lib\\html5parser.py", "repo_name": "mhausenblas/web.instata", "stars": 26, "license": "None", "language": "python", "size": 678}
{"docstring": "\"\"\"Get a TreeBuilder class for various types of tree with built-in support\n\ntreeType - the name of the tree type required (case-insensitive). Supported\n           values are \"simpletree\", \"dom\", \"etree\" and \"beautifulsoup\"\n           \n           \"simpletree\" - a built-in DOM-ish tree type with support for some\n                          more pythonic idioms.\n            \"dom\" - A generic builder for DOM implementations, defaulting to\n                    a xml.dom.minidom based implementation for the sake of\n                    backwards compatibility (as releases up until 0.10 had a\n                    builder called \"dom\" that was a minidom implemenation).\n            \"etree\" - A generic builder for tree implementations exposing an\n                      elementtree-like interface (known to work with\n                      ElementTree, cElementTree and lxml.etree).\n            \"beautifulsoup\" - Beautiful soup (if installed)\n           \nimplementation - (Currently applies to the \"etree\" and \"dom\" tree types). A\n                  module implementing the tree type e.g.\n                  xml.etree.ElementTree or lxml.etree.\"\"\"\n\n", "func_signal": "def getTreeBuilder(treeType, implementation=None, **kwargs):\n", "code": "treeType = treeType.lower()\nif treeType not in treeBuilderCache:\n    if treeType == \"dom\":\n        import dom\n        # XXX: Keep backwards compatibility by using minidom if no implementation is given\n        if implementation == None:\n            from xml.dom import minidom\n            implementation = minidom\n        # XXX: NEVER cache here, caching is done in the dom submodule\n        return dom.getDomModule(implementation, **kwargs).TreeBuilder\n    elif treeType == \"simpletree\":\n        import simpletree\n        treeBuilderCache[treeType] = simpletree.TreeBuilder\n    elif treeType == \"beautifulsoup\":\n        import soup\n        treeBuilderCache[treeType] = soup.TreeBuilder\n    elif treeType == \"lxml\":\n        import etree_lxml\n        treeBuilderCache[treeType] = etree_lxml.TreeBuilder\n    elif treeType == \"etree\":\n        # Come up with a sane default\n        if implementation == None:\n            try:\n                import xml.etree.cElementTree as ET\n            except ImportError:\n                try:\n                    import xml.etree.ElementTree as ET\n                except ImportError:\n                    try:\n                        import cElementTree as ET\n                    except ImportError:\n                        import elementtree.ElementTree as ET\n            implementation = ET\n        import etree\n        # NEVER cache here, caching is done in the etree submodule\n        return etree.getETreeModule(implementation, **kwargs).TreeBuilder\n    else:\n        raise ValueError(\"\"\"Unrecognised treebuilder \"%s\" \"\"\"%treeType)\nreturn treeBuilderCache.get(treeType)", "path": "lib\\html5lib\\treebuilders\\__init__.py", "repo_name": "mhausenblas/web.instata", "stars": 26, "license": "None", "language": "python", "size": 678}
{"docstring": "\"\"\"Utility method for adding a complete simple element\"\"\"\n", "func_signal": "def element(self, uri, content, attributes={}):\n", "code": "self.push(uri)\nfor k, v in attributes.iteritems():\n    self.attribute(k,v)\nself.text(content)\nself.pop()", "path": "lib\\rdflib\\plugins\\serializers\\xmlwriter.py", "repo_name": "mhausenblas/web.instata", "stars": 26, "license": "None", "language": "python", "size": 678}
{"docstring": "\"\"\"Get the foster parent element, and sibling to insert before\n(or None) when inserting a misnested table node\"\"\"\n# The foster parent element is the one which comes before the most\n# recently opened table element\n# XXX - this is really inelegant\n", "func_signal": "def getTableMisnestedNodePosition(self):\n", "code": "lastTable=None\nfosterParent = None\ninsertBefore = None\nfor elm in self.openElements[::-1]:\n    if elm.name == \"table\":\n        lastTable = elm\n        break\nif lastTable:\n    # XXX - we should really check that this parent is actually a\n    # node here\n    if lastTable.parent:\n        fosterParent = lastTable.parent\n        insertBefore = lastTable\n    else:\n        fosterParent = self.openElements[\n            self.openElements.index(lastTable) - 1]\nelse:\n    fosterParent = self.openElements[0]\nreturn fosterParent, insertBefore", "path": "lib\\html5lib\\treebuilders\\_base.py", "repo_name": "mhausenblas/web.instata", "stars": 26, "license": "None", "language": "python", "size": 678}
{"docstring": "#We repeat the test for the body end tag token being ignored here\n", "func_signal": "def endTagHtml(self, token):\n", "code": "if self.tree.elementInScope(\"body\"):\n    self.endTagBody(impliedTagToken(\"body\"))\n    return token", "path": "lib\\html5lib\\html5parser.py", "repo_name": "mhausenblas/web.instata", "stars": 26, "license": "None", "language": "python", "size": 678}
{"docstring": "# XXX AT Any ideas on how to share this with endTagTable?\n", "func_signal": "def startTagTableOther(self, token):\n", "code": "if (self.tree.elementInScope(\"tbody\", variant=\"table\") or\n    self.tree.elementInScope(\"thead\", variant=\"table\") or\n    self.tree.elementInScope(\"tfoot\", variant=\"table\")):\n    self.clearStackToTableBodyContext()\n    self.endTagTableRowGroup(\n        impliedTagToken(self.tree.openElements[-1].name))\n    return token\nelse:\n    # innerHTML case\n    assert self.parser.innerHTML\n    self.parser.parseError()", "path": "lib\\html5lib\\html5parser.py", "repo_name": "mhausenblas/web.instata", "stars": 26, "license": "None", "language": "python", "size": 678}
{"docstring": "# We need to imply </option> if <option> is the current node.\n", "func_signal": "def startTagOption(self, token):\n", "code": "if self.tree.openElements[-1].name == \"option\":\n    self.tree.openElements.pop()\nself.tree.insertElement(token)", "path": "lib\\html5lib\\html5parser.py", "repo_name": "mhausenblas/web.instata", "stars": 26, "license": "None", "language": "python", "size": 678}
{"docstring": "# http://groups.google.com/group/html5lib-discuss/browse_frm/thread/f4f00e4a2f26d5c0\n", "func_signal": "def test_line_counter(self):\n", "code": "parser = html5parser.HTMLParser(tree=dom.TreeBuilder)\nparser.parse(\"<pre>\\nx\\n&gt;\\n</pre>\")", "path": "lib\\html5lib\\tests\\test_parser2.py", "repo_name": "mhausenblas/web.instata", "stars": 26, "license": "None", "language": "python", "size": 678}
{"docstring": "\"\"\"Parse a HTML document into a well-formed tree\n\nstream - a filelike object or string containing the HTML to be parsed\n\nThe optional encoding parameter must be a string that indicates\nthe encoding.  If specified, that encoding will be used,\nregardless of any BOM or later declaration (such as in a meta\nelement)\n\"\"\"\n", "func_signal": "def parse(self, stream, encoding=None, parseMeta=True, useChardet=True):\n", "code": "self._parse(stream, innerHTML=False, encoding=encoding, \n            parseMeta=parseMeta, useChardet=useChardet)\nreturn self.tree.getDocument()", "path": "lib\\html5lib\\html5parser.py", "repo_name": "mhausenblas/web.instata", "stars": 26, "license": "None", "language": "python", "size": 678}
{"docstring": "\"\"\"Check if an element exists between the end of the active\nformatting elements and the last marker. If it does, return it, else\nreturn false\"\"\"\n\n", "func_signal": "def elementInActiveFormattingElements(self, name):\n", "code": "for item in self.activeFormattingElements[::-1]:\n    # Check for Marker first because if it's a Marker it doesn't have a\n    # name attribute.\n    if item == Marker:\n        break\n    elif item.name == name:\n        return item\nreturn False", "path": "lib\\html5lib\\treebuilders\\_base.py", "repo_name": "mhausenblas/web.instata", "stars": 26, "license": "None", "language": "python", "size": 678}
{"docstring": "# No really...\n", "func_signal": "def startTagImage(self, token):\n", "code": "self.parser.parseError(\"unexpected-start-tag-treated-as\",\n  {\"originalName\": \"image\", \"newName\": \"img\"})\nself.processStartTag(impliedTagToken(\"img\", \"StartTag\",\n                                     attributes=token[\"data\"],\n                                     selfClosing=token[\"selfClosing\"]))", "path": "lib\\html5lib\\html5parser.py", "repo_name": "mhausenblas/web.instata", "stars": 26, "license": "None", "language": "python", "size": 678}
{"docstring": "# The name of this method is mostly historical. (It's also used in the\n# specification.)\n", "func_signal": "def resetInsertionMode(self):\n", "code": "last = False\nnewModes = {\n    \"select\":\"inSelect\",\n    \"td\":\"inCell\",\n    \"th\":\"inCell\",\n    \"tr\":\"inRow\",\n    \"tbody\":\"inTableBody\",\n    \"thead\":\"inTableBody\",\n    \"tfoot\":\"inTableBody\",\n    \"caption\":\"inCaption\",\n    \"colgroup\":\"inColumnGroup\",\n    \"table\":\"inTable\",\n    \"head\":\"inBody\",\n    \"body\":\"inBody\",\n    \"frameset\":\"inFrameset\",\n    \"html\":\"beforeHead\"\n}\nfor node in self.tree.openElements[::-1]:\n    nodeName = node.name\n    new_phase = None\n    if node == self.tree.openElements[0]:\n        assert self.innerHTML\n        last = True\n        nodeName = self.innerHTML\n    # Check for conditions that should only happen in the innerHTML\n    # case\n    if nodeName in (\"select\", \"colgroup\", \"head\", \"html\"):\n        assert self.innerHTML\n\n    if nodeName in newModes:\n        new_phase = self.phases[newModes[nodeName]]\n        break\n    elif node.namespace in (namespaces[\"mathml\"], namespaces[\"svg\"]):\n        new_phase = self.phases[\"inForeignContent\"]\n        break\n    elif last:\n        new_phase = self.phases[\"inBody\"]\n        break\n\nself.phase = new_phase", "path": "lib\\html5lib\\html5parser.py", "repo_name": "mhausenblas/web.instata", "stars": 26, "license": "None", "language": "python", "size": 678}
{"docstring": "\"\"\"exists : EXISTS idlist COLON ID COMMA expr\n\"\"\"\n", "func_signal": "def p_exists(p):\n", "code": "p[0] = dict(quantifier=p[1],\n            identifiers=p[2],\n            type=p[4],\n            expr=p[6])", "path": "coqd\\parser\\gram.py", "repo_name": "dcolish/Cockerel", "stars": 25, "license": "other", "language": "python", "size": 1274}
{"docstring": "\"\"\"Parse coqtop results and serialize\"\"\"\n", "func_signal": "def do_parse(self, output):\n", "code": "logging.debug(\"Unformatted Output: %s\" % output)\noutput = ' '.join(output.splitlines())\nresult = parser.parse(output)\nreturn result", "path": "coqd\\connserv.py", "repo_name": "dcolish/Cockerel", "stars": 25, "license": "other", "language": "python", "size": 1274}
{"docstring": "\"\"\"Close out sessions and remove the userid from the tracking table\"\"\"\n", "func_signal": "def cleanup_session(self, userid):\n", "code": "active_sess = self.active_conns.get(userid)\nif active_sess:\n    active_sess.quit()\n    del self.active_conns[userid]", "path": "coqd\\connserv.py", "repo_name": "dcolish/Cockerel", "stars": 25, "license": "other", "language": "python", "size": 1274}
{"docstring": "\"\"\"When a command is received send it to the specific backend for the\ngiven userid\"\"\"\n", "func_signal": "def handle_command(self, userid, command):\n", "code": "if not command:\n    resp_dict = {'userid': userid,\n                 'response': ''}\nelif 'quit' == command:\n    self.cleanup_session(userid)\n    resp_dict = {'userid': userid,\n                 'response': 'Exited'}\nelse:\n    active_sess = self.active_conns.get(userid)\n\n    if not active_sess:\n        active_sess = self.ActiveConn(userid, self.modules)\n        self.active_conns.update({active_sess.userid: active_sess})\n\n    if not active_sess.send(command):\n        logging.error(\"Command could not be sent\")\n        self.cleanup_session(userid)\n\n    resp_dict = active_sess.read()\n\nreturn resp_dict", "path": "coqd\\connserv.py", "repo_name": "dcolish/Cockerel", "stars": 25, "license": "other", "language": "python", "size": 1274}
{"docstring": "\"\"\"proverstate : thmname LARRW NUMBER PIPE thmlist NUMBER LARRW\"\"\"\n", "func_signal": "def p_proverstate(p):\n", "code": "p[0] = dict(proverline=p[2],\n            thmname=p[3],\n            thmlin=p[4])", "path": "coqd\\parser\\gram.py", "repo_name": "dcolish/Cockerel", "stars": 25, "license": "other", "language": "python", "size": 1274}
{"docstring": "\"\"\"\nParse data we've recieved and send to proof engine\nExpects JSON will be in the schema {\"command\":\"\", \"userid\":\"\"}\n\"\"\"\n", "func_signal": "def dataReceived(self, data):\n", "code": "req_data = JSONDecoder().decode(data)\nlogging.debug(\"Got: %s\", req_data)\ncommand = req_data.get('command')\nuserid = req_data.get('userid')\nresp_data = self.handle_command(userid, command)\n\ndata = resp_data['response']\n\nif self.serialize:\n    resp_data['response'] = self.do_parse(resp_data['response'])\n    data = JSONEncoder().encode(resp_data)\n\nself.transport.write(data)\nself.transport.loseConnection()", "path": "coqd\\connserv.py", "repo_name": "dcolish/Cockerel", "stars": 25, "license": "other", "language": "python", "size": 1274}
{"docstring": "\"\"\"Attempts to connect with the fork and send a command\"\"\"\n", "func_signal": "def run(self, conn):\n", "code": "cmd = ''\ntry:\n    if conn.poll():\n        cmd = conn.recv()\n        self.process.send(cmd + \"\\n\")\n    self.process.expect('\\<\\/prompt\\>')\n    # Strip out the cmd sent from the output\n    result = self.process.before[len(cmd):] + self.process.after + \" \"\n    conn.send(result)\nexcept EOF:\n    self.process.close()\n    conn.send(\"Closing Coqtop\\n\")", "path": "coqd\\base.py", "repo_name": "dcolish/Cockerel", "stars": 25, "license": "other", "language": "python", "size": 1274}
{"docstring": "\"\"\"Have the child terminate\"\"\"\n", "func_signal": "def terminate(self, Force=False):\n", "code": "if Force:\n    return self.process.terminate(True)\nelse:\n    return self.process.terminate(True)", "path": "coqd\\base.py", "repo_name": "dcolish/Cockerel", "stars": 25, "license": "other", "language": "python", "size": 1274}
{"docstring": "\"\"\"Chew up blank lines, and be a little lame about it\"\"\"\n", "func_signal": "def readscript(script):\n", "code": "return [x.strip() + '.' for x in script.strip().rsplit(r'.')\n        if not x == u'']", "path": "cockerel\\webapp\\prover\\prover.py", "repo_name": "dcolish/Cockerel", "stars": 25, "license": "other", "language": "python", "size": 1274}
{"docstring": "\"\"\"expr : expr IMPL expr\n        | expr AND expr\n        | expr OR expr\n        | expr LARRW expr\n        | expr RARRW expr\n        | LPAREN expr RPAREN\n        | LBRKT expr RBRKT\n        | idlist\n        \"\"\"\n", "func_signal": "def p_expr(p):\n", "code": "if len(p) == 2 and p[1] is not None:\n    p[0] = str(p[1])\nelif len(p) == 4:\n    # We only care about the middle expr\n    if p[1] == '(' or p[1] == '{':\n        p[0] = dict(expr=p[2])\n    else:\n        p[0] = dict(operator=p[2],\n                    expr=(p[1], p[3]))", "path": "coqd\\parser\\gram.py", "repo_name": "dcolish/Cockerel", "stars": 25, "license": "other", "language": "python", "size": 1274}
{"docstring": "\"\"\"idlist : ID idlist\n          | TILDE idlist\n          | PLING idlist\n          | TRUE\n          | FALSE\n          | ID\n          \"\"\"\n", "func_signal": "def p_idlist(p):\n", "code": "if len(p) == 3:\n    p[0] = ' '.join((str(p[1]), str(p[2])))\nelse:\n    p[0] = str(p[1])", "path": "coqd\\parser\\gram.py", "repo_name": "dcolish/Cockerel", "stars": 25, "license": "other", "language": "python", "size": 1274}
{"docstring": "\"\"\"thmlist : thmname PIPE thmlist\n           | thmname PIPE\n           | PIPE\n           \"\"\"\n", "func_signal": "def p_thmlist(p):\n", "code": "if len(p) == 4:\n    p[0] = dict(names=' ,'.join((p[1], p[3]['names'])))\nelse:\n    p[0] = dict(names=p[1])", "path": "coqd\\parser\\gram.py", "repo_name": "dcolish/Cockerel", "stars": 25, "license": "other", "language": "python", "size": 1274}
{"docstring": "\"\"\"send results to our coqtop instance\"\"\"\n", "func_signal": "def send(self, data):\n", "code": "if self.proc.alive:\n    logging.debug(\"sending stuff\")\n    self.here.send(data + \" \")\n    return True\nelse:\n    return False", "path": "coqd\\connserv.py", "repo_name": "dcolish/Cockerel", "stars": 25, "license": "other", "language": "python", "size": 1274}
{"docstring": "\"\"\"Execs a new child for coqtop\"\"\"\n", "func_signal": "def start(self, modules):\n", "code": "args = ['coqtop']\nfor module in modules:\n    args += ['-include %s' % module]\nargs += ['-emacs-U']\n\nself.process = spawn(' '.join(args))", "path": "coqd\\base.py", "repo_name": "dcolish/Cockerel", "stars": 25, "license": "other", "language": "python", "size": 1274}
{"docstring": "\"\"\"When the connection closes log the exit status\"\"\"\n", "func_signal": "def connectionLost(self, reason):\n", "code": "if 'ConnectionDone' in ''.join(reason.parents):\n    logging.debug(\"Connection closed cleanly\")\nelse:\n    logging.debug(\"Connection didn't close correctly\")\n    raise reason", "path": "coqd\\connserv.py", "repo_name": "dcolish/Cockerel", "stars": 25, "license": "other", "language": "python", "size": 1274}
{"docstring": "# Ensure that reserved words are not overwritten with ID's\n", "func_signal": "def t_ID(t):\n", "code": "r'[a-zA-Z_][a-zA-Z_0-9\\']*'\nt.type = reserved.get(t.value, 'ID')    # Check for reserved words\nreturn t", "path": "coqd\\parser\\lexer.py", "repo_name": "dcolish/Cockerel", "stars": 25, "license": "other", "language": "python", "size": 1274}
{"docstring": "\"\"\"poll results from the coqtop backend\"\"\"\n", "func_signal": "def read(self):\n", "code": "res = None\nself.proc.run(self.there)\nif self.here.poll():\n    res = self.here.recv()\n    logging.debug(\"Received content from process\")\nreturn {'userid': self.userid, 'response': res}", "path": "coqd\\connserv.py", "repo_name": "dcolish/Cockerel", "stars": 25, "license": "other", "language": "python", "size": 1274}
{"docstring": "\"\"\"\nSetup and run coqd\n\"\"\"\n", "func_signal": "def main(modules, serialize):\n", "code": "logging.info(\"Coq is starting... hold on\")\nfactory = CoqFactory(modules, serialize)\nreactor.listenTCP(8003, factory)\nreactor.run()", "path": "coqd\\runner.py", "repo_name": "dcolish/Cockerel", "stars": 25, "license": "other", "language": "python", "size": 1274}
{"docstring": "\"\"\"hyp : ID COLON PROP hyp\n       | ID COLON expr hyp\n       | ID COLON PROP\n       | ID COLON expr\n       \"\"\"\n", "func_signal": "def p_hyp(p):\n", "code": "hyp = [dict(name=p[1],\n            type=p[3])]\nif len(p) == 5:\n    p[0] = hyp + [p[4]]\nelse:\n    p[0] = dict(name=p[1],\n           type=p[3])", "path": "coqd\\parser\\gram.py", "repo_name": "dcolish/Cockerel", "stars": 25, "license": "other", "language": "python", "size": 1274}
{"docstring": "\"\"\"thmname : ID\n           | ID NUMBER\"\"\"\n", "func_signal": "def p_thmname(p):\n", "code": "if len(p) == 2:\n    p[0] = p[1]\nelse:\n    p[0] = ''.join([p[1], str(p[2])])", "path": "coqd\\parser\\gram.py", "repo_name": "dcolish/Cockerel", "stars": 25, "license": "other", "language": "python", "size": 1274}
{"docstring": "\"\"\"Makes a verification OAuth request.\"\"\"\n\n", "func_signal": "def _make_verification_request(self, token, secret, verifier, params):\n", "code": "token = oauth.OAuthToken(token, secret)\nrequest = oauth.OAuthRequest.from_consumer_and_token(\n    self._consumer,\n    token=token,\n    verifier=verifier,\n    http_url=self._config['access_token_url'],\n)\nrequest.sign_request(\n    self._signature_method,\n    self._consumer,\n    token,\n)\n\nreturn urlfetch.fetch(request.to_url())", "path": "src\\ao\\social\\oauth_.py", "repo_name": "attilaolah/ao.social", "stars": 18, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Store the token for the user.\"\"\"\n\n", "func_signal": "def set_token(self, method, token=None):\n", "code": "if not hasattr(self, 'tokens'):\n    raise NotImplementedError('User object has no `tokens` attribute.')\n\ntokens = json.loads(self.tokens or '{}')\nif token is None:\n    del tokens[method]\nelse:\n    tokens[method] = token\n\nself.tokens = json.dumps(tokens, separators=(',', ':'))\n\nself.save_user()", "path": "src\\ao\\social\\__init__.py", "repo_name": "attilaolah/ao.social", "stars": 18, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Get Authorization URL.\"\"\"\n\n", "func_signal": "def get_authorization_url(self, callback_url):\n", "code": "return '%s?oauth_token=%s' % (self._config['authorize_url'],\n    self._get_auth_token(callback_url))", "path": "src\\ao\\social\\oauth_.py", "repo_name": "attilaolah/ao.social", "stars": 18, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Get a user for the corresponding method & external id.\"\"\"\n\n", "func_signal": "def lookup_user(cls, uid):\n", "code": "raise NotImplementedError('You must overload the `lookup_user` '\\\n    'method.')", "path": "src\\ao\\social\\__init__.py", "repo_name": "attilaolah/ao.social", "stars": 18, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Get User Info.\n\nExchanges the auth token for an access token and returns a dictionary\nof information about the authenticated user.\n\n\"\"\"\n\n", "func_signal": "def get_user_info(self, auth_token, auth_verifier):\n", "code": "auth_token = urllib.unquote(auth_token)\nauth_verifier = urllib.unquote(auth_verifier)\n\nauth_secret = memcache.get(self._get_memcache_auth_key(auth_token))\nauth_secret = auth_secret or ''  # memcache might return None\n\nresponse = self._make_verification_request(auth_token, auth_secret,\n    auth_verifier, {'oauth_verifier': auth_verifier})\n\n# Extract the access token/secret from the response.\nresult = self._extract_credentials(response)\n\n# Try to collect some information about this user from the service.\nuser_info = self.lookup_user_info(result['token'], result['secret'])\nuser_info.update(result)\n\nreturn user_info", "path": "src\\ao\\social\\oauth_.py", "repo_name": "attilaolah/ao.social", "stars": 18, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Return the token for the given method.\"\"\"\n\n", "func_signal": "def get_token(self, method):\n", "code": "if not hasattr(self, 'tokens'):\n    raise NotImplementedError('User object has no `tokens` attribute.')\n\nreturn json.loads(self.tokens or '{}')[method]", "path": "src\\ao\\social\\__init__.py", "repo_name": "attilaolah/ao.social", "stars": 18, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Constructs an absolute URI.\"\"\"\n\n", "func_signal": "def _build_absolute_uri(environ, path='/'):\n", "code": "root = '%s://%s' % (environ['wsgi.url_scheme'], environ['HTTP_HOST'])\npath = not path.startswith('/') and environ['PATH_INFO'] + path or path\n\nreturn root + path", "path": "src\\ao\\social\\middleware.py", "repo_name": "attilaolah/ao.social", "stars": 18, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Connects the account to the current user.\"\"\"\n\n", "func_signal": "def _connect_user(self, request, method, credentials):\n", "code": "id = str(credentials['id'])\nuser = request.environ['ao.social.user']\nsession = request.environ['beaker.session']\n\n# Save the user's details and any associated tokens\nif method == 'facebook':\n    user.set_token('facebook', {\n        'uid': id,\n        'token': credentials['token'],\n        'secret': credentials['secret'],\n    })\nelif method == 'twitter':\n    user.set_token('twitter', {\n        'uid': id,\n        'token': credentials['token'],\n        'secret': credentials['secret'],\n    })\nelif method == 'google':\n    user.set_token('google', {\n        'uid': id,\n    })\nelif method == 'linkedin':\n    user.set_token('linkedin', {\n        'uid': id,\n        'token': credentials['token'],\n        'secret': credentials['secret'],\n    })\n\n# Prepare the response\npostlogin = ''\nif 'postlogin' in session:\n    postlogin = session['postlogin']\n    del session['postlogin']\nbody = self._popup_html % {'postlogin': postlogin}\nresponse = webob.Response(body=body)\n\n# Save changes to the user object\nuser.save_user()\n\nreturn response", "path": "src\\ao\\social\\middleware.py", "repo_name": "attilaolah/ao.social", "stars": 18, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Configure the middleware.\"\"\"\n\n", "func_signal": "def __init__(self, app, config={}):\n", "code": "self._app = app\n\nself._user_class = self._import_user(config['user_class'])\nself._login_path = config['login_path']\n\nself._clients = {}\nfor method in ('facebook', 'google', 'twitter', 'linkedin'):\n    if method in config:\n        self._clients[method] = social.registerClient(method,\n            config[method])", "path": "src\\ao\\social\\middleware.py", "repo_name": "attilaolah/ao.social", "stars": 18, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Handles authentication for the user.\n\nIf `mode` is set to 'connect', it will assume that a user is already\nlogged in and connects the new account to the logged in user.\nOtherwise, simply logs in the user.\n\n\"\"\"\n\n", "func_signal": "def _handle_user(self, request, method, mode='login'):\n", "code": "user = request.environ['ao.social.user']\nsession = request.environ['beaker.session']\n\n# Save the post login action in the session.\nif 'postlogin' in request.GET:\n    session['postlogin'] = request.GET['postlogin']\n    session.save()\n\n# Check if the user has logged in via Facebook Connect.\nif method == 'facebook':\n    facebook_user = self._clients[method].get_user(request)\n    if facebook_user is None:\n        raise social.Unauthorized('Facebook Connect authentication '\\\n            'failed.')\n    # OK, Facebook user is verified.\n    if user is None:\n        return self._login_user(request, method, facebook_user)\n    return self._connect_user(request, method, facebook_user)\n\n# Check if the user has logged in via Twitter's Oauth.\nif method == 'twitter':\n    keys = ('oauth_token', 'oauth_verifier')\n    if  not all(key in request.GET for key in keys):\n        # Redirect the user to Twitter's authorization URL.\n        auth_url = self._login_path % method\n        auth_url = self._clients[method].get_authorization_url(\n            self._build_absolute_uri(request.environ, auth_url))\n        return webob.Response(status=302, headers={\n            'Location': auth_url,\n        })\n    try:\n        twitter_user = self._clients[method].get_user_info(\n            request.GET['oauth_token'],\n            request.GET['oauth_verifier'],\n        )\n        # OK, Twitter user is verified.\n        if user is None:\n            return self._login_user(request, method, twitter_user)\n        return self._connect_user(request, method, twitter_user)\n    except oauth.OAuthError:\n        raise social.Unauthorized('Twitter OAuth authentication '\\\n            'failed.')\n\n# Check if the user has logged in via Google OpenID/OAuth.\nif method == 'google':\n    if len(request.GET) < 2:\n        # Create a custom auth request and redirect the user.\n        return webob.exc.HTTPTemporaryRedirect(\n            location=self._clients[method].redirect(),\n        )\n    # Hopefully the user has come back from the auth request url.\n    callback = self._build_absolute_uri(request.environ,\n        self._login_path % method)\n    google_user = self._clients[method].get_user(request.GET, callback)\n    if google_user is None:\n        raise social.Unauthorized('Google OpenID authentication '\\\n            'failed.')\n    # OK, Google user is verified.\n    if user is None:\n        return self._login_user(request, method, google_user)\n    return self._connect_user(request, method, google_user)\n\n# Check if the user has logged in via LinkedIn's Oauth.\nif method == 'linkedin':\n    keys = ('oauth_token', 'oauth_verifier')\n    if  not all(key in request.GET for key in keys):\n        # Redirect the user to LinkedIn's authorization URL.\n        auth_url = self._login_path % method\n        auth_url = self._clients[method].get_authorization_url(\n            self._build_absolute_uri(request.environ, auth_url))\n        return webob.Response(status=302, headers={\n            'Location': auth_url,\n        })\n    try:\n        linkedin_user = self._clients[method].get_user_info(\n            request.GET['oauth_token'],\n            request.GET['oauth_verifier'],\n        )\n        # OK, LinkedIn user is verified.\n        if user is None:\n            return self._login_user(request, method, linkedin_user)\n        return self._connect_user(request, method, linkedin_user)\n    except oauth.OAuthError:\n        raise social.Unauthorized('LinkedIn OAuth authentication '\\\n            'failed.')", "path": "src\\ao\\social\\middleware.py", "repo_name": "attilaolah/ao.social", "stars": 18, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Looks up the user and initiates a session.\"\"\"\n\n", "func_signal": "def _login_user(self, request, method, credentials):\n", "code": "id = str(credentials['id'])\nuid = ':'.join((method, id))\nsession = request.environ['beaker.session']\n\n# Get the user from the database backend (or create a new user)\nuser = self._user_class.lookup_user(uid)\n\n# Save the user's details and any associated tokens\nif method == 'facebook':\n    info = ['name', 'first_name', 'last_name', 'email', 'pic_square']\n    data = self._clients[method].users.getInfo(id, info)[0]\n    user.update_details({\n        'name': data['name'],\n        'first_name': data['first_name'],\n        'last_name': data['last_name'],\n        'avatar': data['pic_square'],\n        'email': data['email'],\n    })\n    user.set_token('facebook', {\n        'uid': id,\n        'token': credentials['token'],\n        'secret': credentials['secret'],\n    })\nelif method == 'twitter':\n    first_name, _, last_name = credentials['name'].partition(' ')\n    user.update_details({\n        'name': credentials['name'],\n        'first_name': first_name,\n        'last_name': last_name,\n        'avatar': credentials['profile_image_url'],\n    })\n    user.set_token('twitter', {\n        'uid': id,\n        'token': credentials['token'],\n        'secret': credentials['secret'],\n    })\nelif method == 'google':\n    user.update_details({\n        'name': '%s %s' % (credentials['first_name'],\n            credentials['last_name']),\n        'first_name': credentials['first_name'],\n        'last_name': credentials['last_name'],\n        'email': credentials['email'],\n    })\n    user.set_token('google', {\n        'uid': id,\n    })\nelif method == 'linkedin':\n    user.update_details({\n        'name': '%s %s' % (credentials['first_name'],\n            credentials['last_name']),\n        'first_name': credentials['first_name'],\n        'last_name': credentials['last_name'],\n    })\n    user.set_token('linkedin', {\n        'uid': id,\n        'token': credentials['token'],\n        'secret': credentials['secret'],\n    })\n\n# Prepare the response\npostlogin = ''\nif 'postlogin' in session:\n    postlogin = session['postlogin']\n    del session['postlogin']\nbody = self._popup_html % {'postlogin': postlogin}\nresponse = webob.Response(body=body)\n\n# Store the user's key in the session\nsession['ao.social.user'] = str(user.get_key())\nsession.save()\n\n# Save changes to the user object\nuser.save_user()\n\nreturn response", "path": "src\\ao\\social\\middleware.py", "repo_name": "attilaolah/ao.social", "stars": 18, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Update the user's details.\"\"\"\n\n", "func_signal": "def update_details(self, details):\n", "code": "raise NotImplementedError('You must overload the `update_details` '\\\n    'method.')", "path": "src\\ao\\social\\__init__.py", "repo_name": "attilaolah/ao.social", "stars": 18, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Return the client for the given method.\"\"\"\n\n", "func_signal": "def getClient(method):\n", "code": "global clients\n\ntry:\n    return clients[method]\nexcept KeyError:\n    raise ImproperlyConfigured('The requested client (%s) is not '\\\n        'initialized (available clients: %s).' % (method, clients.keys()))", "path": "src\\ao\\social\\__init__.py", "repo_name": "attilaolah/ao.social", "stars": 18, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Import the provided `User` class.\"\"\"\n\n", "func_signal": "def _import_user(cls):\n", "code": "modstr, _, cls = cls.rpartition('.')\n\nmod = __import__(modstr)\nfor sub in modstr.split('.')[1:]:\n    mod = getattr(mod, sub)\n\nreturn getattr(mod, cls)", "path": "src\\ao\\social\\middleware.py", "repo_name": "attilaolah/ao.social", "stars": 18, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Register a Twitter client.\"\"\"\n\n", "func_signal": "def registerClient(method, config={}):\n", "code": "client_classes = {\n    'twitter': twitter.TwitterClient,\n    'google': google.GoogleClient,\n    'facebook': facebook.FacebookClient,\n    'linkedin': linkedin.LinkedInClient,\n}\n\nglobal clients\nclients[method] = client_classes[method](config)\n\nreturn clients[method]", "path": "src\\ao\\social\\__init__.py", "repo_name": "attilaolah/ao.social", "stars": 18, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Adds the session user to the `RequestContext`.\n\nThis is a Django `template context processor`. You shouldn't need to use\nthis function directly.\n\n\"\"\"\n\n", "func_signal": "def user(request):\n", "code": "return {\n    'user': request.environ['ao.social.user'],\n}", "path": "src\\ao\\social\\__init__.py", "repo_name": "attilaolah/ao.social", "stars": 18, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Makes a generic OAuth request.\"\"\"\n\n", "func_signal": "def _make_callback_request(self, callback_url):\n", "code": "request = oauth.OAuthRequest.from_consumer_and_token(\n    self._consumer,\n    callback=callback_url,\n    http_url=self._config['request_token_url'],\n)\nrequest.sign_request(\n    self._signature_method,\n    self._consumer,\n    None,\n)\n\nreturn urlfetch.fetch(request.to_url())", "path": "src\\ao\\social\\oauth_.py", "repo_name": "attilaolah/ao.social", "stars": 18, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Put the user object into the WSGI environment.\"\"\"\n\n# Create our own request object\n", "func_signal": "def __call__(self, environ, start_response):\n", "code": "request = webob.Request(environ, charset='utf-8')\n\n# Save the login path, we might require it in template tags\nenviron['ao.social.login'] = self._build_absolute_uri(environ,\n    self._login_path)\n\n# We need beaker sessions for this middleware\nsession = environ['beaker.session']\n\n# Check if the user already has a session\nenviron['ao.social.user'] = None\nif 'ao.social.user' in session:\n    environ['ao.social.user'] = self._user_class.get_user(\n        session['ao.social.user'])\n\n# Try to log in the user\nfor method in ('facebook', 'twitter', 'google', 'linkedin'):\n    if request.path_info == self._login_path % method:\n        response = self._handle_user(request, method, 'login')\n        if response is not None:\n            return response(environ, start_response)\n\n# Call the downstream application\nreturn self._app(environ, start_response)", "path": "src\\ao\\social\\middleware.py", "repo_name": "attilaolah/ao.social", "stars": 18, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Do a stream post or status update to the given service.\"\"\"\n\n", "func_signal": "def post(self, method, text, *args, **kw):\n", "code": "token = self.get_token(method)\nif method == 'facebook':\n    kw['uid'] = token['uid']\nelif method in ('twitter', 'linkedin'):\n    kw['token'] = token['token']\n    kw['secret'] = token['secret']\n\nclient = getClient(method)\n\nreturn client.post(text, *args, **kw)", "path": "src\\ao\\social\\__init__.py", "repo_name": "attilaolah/ao.social", "stars": 18, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Get Authorization Token.\n\nActually gets the authorization token and secret from the service. The\ntoken and secret are stored in memcache, and the auth token is\nreturned.\n\n\"\"\"\n\n# Create an OAuth request and extract the credentials.\n", "func_signal": "def _get_auth_token(self, callback_url):\n", "code": "result = self._extract_credentials(\n    self._make_callback_request(callback_url))\n\nauth_token = result['token']\nauth_secret = result['secret']\n\n# Add the secret to memcache.\nmemcache.set(self._get_memcache_auth_key(auth_token), auth_secret,\n    time=1200)\n\nreturn auth_token", "path": "src\\ao\\social\\oauth_.py", "repo_name": "attilaolah/ao.social", "stars": 18, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Checks for invalid increment *count++.\n\nFor example following function:\nvoid increment_counter(int* count) {\n  *count++;\n}\nis invalid, because it effectively does count++, moving pointer, and should\nbe replaced with ++*count, (*count)++ or *count += 1.\n\nArgs:\n  filename: The name of the current file.\n  clean_lines: A CleansedLines instance containing the file.\n  linenum: The number of the line to check.\n  error: The function to call with any errors found.\n\"\"\"\n", "func_signal": "def CheckInvalidIncrement(filename, clean_lines, linenum, error):\n", "code": "line = clean_lines.elided[linenum]\nif _RE_PATTERN_INVALID_INCREMENT.match(line):\n  error(filename, linenum, 'runtime/invalid_increment', 5,\n        'Changing pointer instead of value (or unused value of operator*).')", "path": "cpplint.py", "repo_name": "mstone/vscan", "stars": 16, "license": "bsd-3-clause", "language": "python", "size": 567}
{"docstring": "\"\"\"Checks for common mistakes in TODO comments.\n\nArgs:\n  comment: The text of the comment from the line in question.\n  filename: The name of the current file.\n  linenum: The number of the line to check.\n  error: The function to call with any errors found.\n\"\"\"\n", "func_signal": "def CheckComment(comment, filename, linenum, error):\n", "code": "match = _RE_PATTERN_TODO.match(comment)\nif match:\n  # One whitespace is correct; zero whitespace is handled elsewhere.\n  leading_whitespace = match.group(1)\n  if len(leading_whitespace) > 1:\n    error(filename, linenum, 'whitespace/todo', 2,\n          'Too many spaces before TODO')\n\n  username = match.group(2)\n  if not username:\n    error(filename, linenum, 'readability/todo', 2,\n          'Missing username in TODO; it should look like '\n          '\"// TODO(my_username): Stuff.\"')\n\n  middle_whitespace = match.group(3)\n  # Comparisons made explicit for correctness -- pylint: disable-msg=C6403\n  if middle_whitespace != ' ' and middle_whitespace != '':\n    error(filename, linenum, 'whitespace/todo', 2,\n          'TODO(my_username) should be followed by a space')", "path": "cpplint.py", "repo_name": "mstone/vscan", "stars": 16, "license": "bsd-3-clause", "language": "python", "size": 567}
{"docstring": "\"\"\"Collapses strings and chars on a line to simple \"\" or '' blocks.\n\nWe nix strings first so we're not fooled by text like '\"http://\"'\n\nArgs:\n  elided: The line being processed.\n\nReturns:\n  The line with collapsed strings.\n\"\"\"\n", "func_signal": "def _CollapseStrings(elided):\n", "code": "if not _RE_PATTERN_INCLUDE.match(elided):\n  # Remove escaped characters first to make quote/single quote collapsing\n  # basic.  Things that look like escaped characters shouldn't occur\n  # outside of strings and chars.\n  elided = _RE_PATTERN_CLEANSE_LINE_ESCAPES.sub('', elided)\n  elided = _RE_PATTERN_CLEANSE_LINE_SINGLE_QUOTES.sub(\"''\", elided)\n  elided = _RE_PATTERN_CLEANSE_LINE_DOUBLE_QUOTES.sub('\"\"', elided)\nreturn elided", "path": "cpplint.py", "repo_name": "mstone/vscan", "stars": 16, "license": "bsd-3-clause", "language": "python", "size": 567}
{"docstring": "\"\"\"Checks that the file contains a header guard.\n\nLogs an error if no #ifndef header guard is present.  For other\nheaders, checks that the full pathname is used.\n\nArgs:\n  filename: The name of the C++ header file.\n  lines: An array of strings, each representing a line of the file.\n  error: The function to call with any errors found.\n\"\"\"\n\n", "func_signal": "def CheckForHeaderGuard(filename, lines, error):\n", "code": "cppvar = GetHeaderGuardCPPVariable(filename)\n\nifndef = None\nifndef_linenum = 0\ndefine = None\nendif = None\nendif_linenum = 0\nfor linenum, line in enumerate(lines):\n  linesplit = line.split()\n  if len(linesplit) >= 2:\n    # find the first occurrence of #ifndef and #define, save arg\n    if not ifndef and linesplit[0] == '#ifndef':\n      # set ifndef to the header guard presented on the #ifndef line.\n      ifndef = linesplit[1]\n      ifndef_linenum = linenum\n    if not define and linesplit[0] == '#define':\n      define = linesplit[1]\n  # find the last occurrence of #endif, save entire line\n  if line.startswith('#endif'):\n    endif = line\n    endif_linenum = linenum\n\nif not ifndef or not define or ifndef != define:\n  error(filename, 0, 'build/header_guard', 5,\n        'No #ifndef header guard found, suggested CPP variable is: %s' %\n        cppvar)\n  return\n\n# The guard should be PATH_FILE_H_, but we also allow PATH_FILE_H__\n# for backward compatibility.\nif ifndef != cppvar:\n  error_level = 0\n  if ifndef != cppvar + '_':\n    error_level = 5\n\n  ParseNolintSuppressions(filename, lines[ifndef_linenum], ifndef_linenum,\n                          error)\n  error(filename, ifndef_linenum, 'build/header_guard', error_level,\n        '#ifndef header guard has wrong style, please use: %s' % cppvar)\n\nif endif != ('#endif  // %s' % cppvar):\n  error_level = 0\n  if endif != ('#endif  // %s' % (cppvar + '_')):\n    error_level = 5\n\n  ParseNolintSuppressions(filename, lines[endif_linenum], endif_linenum,\n                          error)\n  error(filename, endif_linenum, 'build/header_guard', error_level,\n        '#endif line should be \"#endif  // %s\"' % cppvar)", "path": "cpplint.py", "repo_name": "mstone/vscan", "stars": 16, "license": "bsd-3-clause", "language": "python", "size": 567}
{"docstring": "\"\"\"Determines if the given filename has a suffix that identifies it as a test.\n\nArgs:\n  filename: The input filename.\n\nReturns:\n  True if 'filename' looks like a test, False otherwise.\n\"\"\"\n", "func_signal": "def _IsTestFilename(filename):\n", "code": "if (filename.endswith('_test.cc') or\n    filename.endswith('_unittest.cc') or\n    filename.endswith('_regtest.cc')):\n  return True\nelse:\n  return False", "path": "cpplint.py", "repo_name": "mstone/vscan", "stars": 16, "license": "bsd-3-clause", "language": "python", "size": 567}
{"docstring": "\"\"\"Logs an error for each line containing Unicode replacement characters.\n\nThese indicate that either the file contained invalid UTF-8 (likely)\nor Unicode replacement characters (which it shouldn't).  Note that\nit's possible for this to throw off line numbering if the invalid\nUTF-8 occurred adjacent to a newline.\n\nArgs:\n  filename: The name of the current file.\n  lines: An array of strings, each representing a line of the file.\n  error: The function to call with any errors found.\n\"\"\"\n", "func_signal": "def CheckForUnicodeReplacementCharacters(filename, lines, error):\n", "code": "for linenum, line in enumerate(lines):\n  if u'\\ufffd' in line:\n    error(filename, linenum, 'readability/utf8', 5,\n          'Line contains invalid UTF-8 (or Unicode replacement character).')", "path": "cpplint.py", "repo_name": "mstone/vscan", "stars": 16, "license": "bsd-3-clause", "language": "python", "size": 567}
{"docstring": "\"\"\"Returns the CPP variable that should be used as a header guard.\n\nArgs:\n  filename: The name of a C++ header file.\n\nReturns:\n  The CPP variable that should be used as a header guard in the\n  named file.\n\n\"\"\"\n\n# Restores original filename in case that cpplint is invoked from Emacs's\n# flymake.\n", "func_signal": "def GetHeaderGuardCPPVariable(filename):\n", "code": "filename = re.sub(r'_flymake\\.h$', '.h', filename)\n\nfileinfo = FileInfo(filename)\nreturn re.sub(r'[-./\\s]', '_', fileinfo.RepositoryName()).upper() + '_'", "path": "cpplint.py", "repo_name": "mstone/vscan", "stars": 16, "license": "bsd-3-clause", "language": "python", "size": 567}
{"docstring": "\"\"\"Performs lint checks and reports any errors to the given error function.\n\nArgs:\n  filename: Filename of the file that is being processed.\n  file_extension: The extension (dot not included) of the file.\n  lines: An array of strings, each representing a line of the file, with the\n         last element being empty if the file is termined with a newline.\n  error: A callable to which errors are reported, which takes 4 arguments:\n\"\"\"\n", "func_signal": "def ProcessFileData(filename, file_extension, lines, error):\n", "code": "lines = (['// marker so line numbers and indices both start at 1'] + lines +\n         ['// marker so line numbers end in a known way'])\n\ninclude_state = _IncludeState()\nfunction_state = _FunctionState()\nclass_state = _ClassState()\n\nResetNolintSuppressions()\n\nCheckForCopyright(filename, lines, error)\n\nif file_extension == 'h':\n  CheckForHeaderGuard(filename, lines, error)\n\nRemoveMultiLineComments(filename, lines, error)\nclean_lines = CleansedLines(lines)\nfor line in xrange(clean_lines.NumLines()):\n  ProcessLine(filename, file_extension, clean_lines, line,\n              include_state, function_state, class_state, error)\nclass_state.CheckFinished(filename, error)\n\nCheckForIncludeWhatYouUse(filename, clean_lines, include_state, error)\n\n# We check here rather than inside ProcessLine so that we see raw\n# lines rather than \"cleaned\" lines.\nCheckForUnicodeReplacementCharacters(filename, lines, error)\n\nCheckForNewlineAtEOF(filename, lines, error)", "path": "cpplint.py", "repo_name": "mstone/vscan", "stars": 16, "license": "bsd-3-clause", "language": "python", "size": 567}
{"docstring": "\"\"\"Figures out what kind of header 'include' is.\n\nArgs:\n  fileinfo: The current file cpplint is running over. A FileInfo instance.\n  include: The path to a #included file.\n  is_system: True if the #include used <> rather than \"\".\n\nReturns:\n  One of the _XXX_HEADER constants.\n\nFor example:\n  >>> _ClassifyInclude(FileInfo('foo/foo.cc'), 'stdio.h', True)\n  _C_SYS_HEADER\n  >>> _ClassifyInclude(FileInfo('foo/foo.cc'), 'string', True)\n  _CPP_SYS_HEADER\n  >>> _ClassifyInclude(FileInfo('foo/foo.cc'), 'foo/foo.h', False)\n  _LIKELY_MY_HEADER\n  >>> _ClassifyInclude(FileInfo('foo/foo_unknown_extension.cc'),\n  ...                  'bar/foo_other_ext.h', False)\n  _POSSIBLE_MY_HEADER\n  >>> _ClassifyInclude(FileInfo('foo/foo.cc'), 'foo/bar.h', False)\n  _OTHER_HEADER\n\"\"\"\n# This is a list of all standard c++ header files, except\n# those already checked for above.\n", "func_signal": "def _ClassifyInclude(fileinfo, include, is_system):\n", "code": "is_stl_h = include in _STL_HEADERS\nis_cpp_h = is_stl_h or include in _CPP_HEADERS\n\nif is_system:\n  if is_cpp_h:\n    return _CPP_SYS_HEADER\n  else:\n    return _C_SYS_HEADER\n\n# If the target file and the include we're checking share a\n# basename when we drop common extensions, and the include\n# lives in . , then it's likely to be owned by the target file.\ntarget_dir, target_base = (\n    os.path.split(_DropCommonSuffixes(fileinfo.RepositoryName())))\ninclude_dir, include_base = os.path.split(_DropCommonSuffixes(include))\nif target_base == include_base and (\n    include_dir == target_dir or\n    include_dir == os.path.normpath(target_dir + '/../public')):\n  return _LIKELY_MY_HEADER\n\n# If the target and include share some initial basename\n# component, it's possible the target is implementing the\n# include, so it's allowed to be first, but we'll never\n# complain if it's not there.\ntarget_first_component = _RE_FIRST_COMPONENT.match(target_base)\ninclude_first_component = _RE_FIRST_COMPONENT.match(include_base)\nif (target_first_component and include_first_component and\n    target_first_component.group(0) ==\n    include_first_component.group(0)):\n  return _POSSIBLE_MY_HEADER\n\nreturn _OTHER_HEADER", "path": "cpplint.py", "repo_name": "mstone/vscan", "stars": 16, "license": "bsd-3-clause", "language": "python", "size": 567}
{"docstring": "\"\"\"Clears a range of lines for multi-line comments.\"\"\"\n# Having // dummy comments makes the lines non-empty, so we will not get\n# unnecessary blank line warnings later in the code.\n", "func_signal": "def RemoveMultiLineCommentsFromRange(lines, begin, end):\n", "code": "for i in range(begin, end):\n  lines[i] = '// dummy'", "path": "cpplint.py", "repo_name": "mstone/vscan", "stars": 16, "license": "bsd-3-clause", "language": "python", "size": 567}
{"docstring": "\"\"\"Returns true if the specified error category is suppressed on this line.\n\nConsults the global error_suppressions map populated by\nParseNolintSuppressions/ResetNolintSuppressions.\n\nArgs:\n  category: str, the category of the error.\n  linenum: int, the current line number.\nReturns:\n  bool, True iff the error should be suppressed due to a NOLINT comment.\n\"\"\"\n", "func_signal": "def IsErrorSuppressedByNolint(category, linenum):\n", "code": "return (linenum in _error_suppressions.get(category, set()) or\n        linenum in _error_suppressions.get(None, set()))", "path": "cpplint.py", "repo_name": "mstone/vscan", "stars": 16, "license": "bsd-3-clause", "language": "python", "size": 567}
{"docstring": "\"\"\"Sets the module's verbosity, and returns the previous setting.\"\"\"\n", "func_signal": "def SetVerboseLevel(self, level):\n", "code": "last_verbose_level = self.verbose_level\nself.verbose_level = level\nreturn last_verbose_level", "path": "cpplint.py", "repo_name": "mstone/vscan", "stars": 16, "license": "bsd-3-clause", "language": "python", "size": 567}
{"docstring": "\"\"\"Prints a brief usage string and exits, optionally with an error message.\n\nArgs:\n  message: The optional error message.\n\"\"\"\n", "func_signal": "def PrintUsage(message):\n", "code": "sys.stderr.write(_USAGE)\nif message:\n  sys.exit('\\nFATAL ERROR: ' + message)\nelse:\n  sys.exit(1)", "path": "cpplint.py", "repo_name": "mstone/vscan", "stars": 16, "license": "bsd-3-clause", "language": "python", "size": 567}
{"docstring": "\"\"\"Logs an error if no Copyright message appears at the top of the file.\"\"\"\n\n# We'll say it should occur by line 10. Don't forget there's a\n# dummy line at the front.\n", "func_signal": "def CheckForCopyright(filename, lines, error):\n", "code": "for line in xrange(1, min(len(lines), 11)):\n  if re.search(r'Copyright', lines[line], re.I): break\nelse:                       # means no copyright line was found\n  error(filename, 0, 'legal/copyright', 5,\n        'No copyright message found.  '\n        'You should have a line: \"Copyright [year] <Copyright Owner>\"')", "path": "cpplint.py", "repo_name": "mstone/vscan", "stars": 16, "license": "bsd-3-clause", "language": "python", "size": 567}
{"docstring": "\"\"\"Check rules that are applicable to #include lines.\n\nStrings on #include lines are NOT removed from elided line, to make\ncertain tasks easier. However, to prevent false positives, checks\napplicable to #include lines in CheckLanguage must be put here.\n\nArgs:\n  filename: The name of the current file.\n  clean_lines: A CleansedLines instance containing the file.\n  linenum: The number of the line to check.\n  include_state: An _IncludeState instance in which the headers are inserted.\n  error: The function to call with any errors found.\n\"\"\"\n", "func_signal": "def CheckIncludeLine(filename, clean_lines, linenum, include_state, error):\n", "code": "fileinfo = FileInfo(filename)\n\nline = clean_lines.lines[linenum]\n\n# \"include\" should use the new style \"foo/bar.h\" instead of just \"bar.h\"\nif _RE_PATTERN_INCLUDE_NEW_STYLE.search(line):\n  error(filename, linenum, 'build/include', 4,\n        'Include the directory when naming .h files')\n\n# we shouldn't include a file more than once. actually, there are a\n# handful of instances where doing so is okay, but in general it's\n# not.\nmatch = _RE_PATTERN_INCLUDE.search(line)\nif match:\n  include = match.group(2)\n  is_system = (match.group(1) == '<')\n  if include in include_state:\n    error(filename, linenum, 'build/include', 4,\n          '\"%s\" already included at %s:%s' %\n          (include, filename, include_state[include]))\n  else:\n    include_state[include] = linenum\n\n    # We want to ensure that headers appear in the right order:\n    # 1) for foo.cc, foo.h  (preferred location)\n    # 2) c system files\n    # 3) cpp system files\n    # 4) for foo.cc, foo.h  (deprecated location)\n    # 5) other google headers\n    #\n    # We classify each include statement as one of those 5 types\n    # using a number of techniques. The include_state object keeps\n    # track of the highest type seen, and complains if we see a\n    # lower type after that.\n    error_message = include_state.CheckNextIncludeOrder(\n        _ClassifyInclude(fileinfo, include, is_system))\n    if error_message:\n      error(filename, linenum, 'build/include_order', 4,\n            '%s. Should be: %s.h, c system, c++ system, other.' %\n            (error_message, fileinfo.BaseName()))\n    if not include_state.IsInAlphabeticalOrder(include):\n      error(filename, linenum, 'build/include_alpha', 4,\n            'Include \"%s\" not in alphabetical order' % include)\n\n# Look for any of the stream classes that are part of standard C++.\nmatch = _RE_PATTERN_INCLUDE.match(line)\nif match:\n  include = match.group(2)\n  if Match(r'(f|ind|io|i|o|parse|pf|stdio|str|)?stream$', include):\n    # Many unit tests use cout, so we exempt them.\n    if not _IsTestFilename(filename):\n      error(filename, linenum, 'readability/streams', 3,\n            'Streams are highly discouraged.')", "path": "cpplint.py", "repo_name": "mstone/vscan", "stars": 16, "license": "bsd-3-clause", "language": "python", "size": 567}
{"docstring": "\"\"\"Checks for the correctness of various spacing around function calls.\n\nArgs:\n  filename: The name of the current file.\n  line: The text of the line to check.\n  linenum: The number of the line to check.\n  error: The function to call with any errors found.\n\"\"\"\n\n# Since function calls often occur inside if/for/while/switch\n# expressions - which have their own, more liberal conventions - we\n# first see if we should be looking inside such an expression for a\n# function call, to which we can apply more strict standards.\n", "func_signal": "def CheckSpacingForFunctionCall(filename, line, linenum, error):\n", "code": "fncall = line    # if there's no control flow construct, look at whole line\nfor pattern in (r'\\bif\\s*\\((.*)\\)\\s*{',\n                r'\\bfor\\s*\\((.*)\\)\\s*{',\n                r'\\bwhile\\s*\\((.*)\\)\\s*[{;]',\n                r'\\bswitch\\s*\\((.*)\\)\\s*{'):\n  match = Search(pattern, line)\n  if match:\n    fncall = match.group(1)    # look inside the parens for function calls\n    break\n\n# Except in if/for/while/switch, there should never be space\n# immediately inside parens (eg \"f( 3, 4 )\").  We make an exception\n# for nested parens ( (a+b) + c ).  Likewise, there should never be\n# a space before a ( when it's a function argument.  I assume it's a\n# function argument when the char before the whitespace is legal in\n# a function name (alnum + _) and we're not starting a macro. Also ignore\n# pointers and references to arrays and functions coz they're too tricky:\n# we use a very simple way to recognize these:\n# \" (something)(maybe-something)\" or\n# \" (something)(maybe-something,\" or\n# \" (something)[something]\"\n# Note that we assume the contents of [] to be short enough that\n# they'll never need to wrap.\nif (  # Ignore control structures.\n    not Search(r'\\b(if|for|while|switch|return|delete)\\b', fncall) and\n    # Ignore pointers/references to functions.\n    not Search(r' \\([^)]+\\)\\([^)]*(\\)|,$)', fncall) and\n    # Ignore pointers/references to arrays.\n    not Search(r' \\([^)]+\\)\\[[^\\]]+\\]', fncall)):\n  if Search(r'\\w\\s*\\(\\s(?!\\s*\\\\$)', fncall):      # a ( used for a fn call\n    error(filename, linenum, 'whitespace/parens', 4,\n          'Extra space after ( in function call')\n  elif Search(r'\\(\\s+(?!(\\s*\\\\)|\\()', fncall):\n    error(filename, linenum, 'whitespace/parens', 2,\n          'Extra space after (')\n  if (Search(r'\\w\\s+\\(', fncall) and\n      not Search(r'#\\s*define|typedef', fncall)):\n    error(filename, linenum, 'whitespace/parens', 4,\n          'Extra space before ( in function call')\n  # If the ) is followed only by a newline or a { + newline, assume it's\n  # part of a control statement (if/while/etc), and don't complain\n  if Search(r'[^)]\\s+\\)\\s*[^{\\s]', fncall):\n    error(filename, linenum, 'whitespace/parens', 2,\n          'Extra space before )')", "path": "cpplint.py", "repo_name": "mstone/vscan", "stars": 16, "license": "bsd-3-clause", "language": "python", "size": 567}
{"docstring": "\"\"\"Checks the use of CHECK and EXPECT macros.\n\nArgs:\n  filename: The name of the current file.\n  clean_lines: A CleansedLines instance containing the file.\n  linenum: The number of the line to check.\n  error: The function to call with any errors found.\n\"\"\"\n\n# Decide the set of replacement macros that should be suggested\n", "func_signal": "def CheckCheck(filename, clean_lines, linenum, error):\n", "code": "raw_lines = clean_lines.raw_lines\ncurrent_macro = ''\nfor macro in _CHECK_MACROS:\n  if raw_lines[linenum].find(macro) >= 0:\n    current_macro = macro\n    break\nif not current_macro:\n  # Don't waste time here if line doesn't contain 'CHECK' or 'EXPECT'\n  return\n\nline = clean_lines.elided[linenum]        # get rid of comments and strings\n\n# Encourage replacing plain CHECKs with CHECK_EQ/CHECK_NE/etc.\nfor operator in ['==', '!=', '>=', '>', '<=', '<']:\n  if ReplaceableCheck(operator, current_macro, line):\n    error(filename, linenum, 'readability/check', 2,\n          'Consider using %s instead of %s(a %s b)' % (\n              _CHECK_REPLACEMENT[current_macro][operator],\n              current_macro, operator))\n    break", "path": "cpplint.py", "repo_name": "mstone/vscan", "stars": 16, "license": "bsd-3-clause", "language": "python", "size": 567}
{"docstring": "\"\"\"If input points to ( or { or [, finds the position that closes it.\n\nIf lines[linenum][pos] points to a '(' or '{' or '[', finds the the\nlinenum/pos that correspond to the closing of the expression.\n\nArgs:\n  clean_lines: A CleansedLines instance containing the file.\n  linenum: The number of the line to check.\n  pos: A position on the line.\n\nReturns:\n  A tuple (line, linenum, pos) pointer *past* the closing brace, or\n  (line, len(lines), -1) if we never find a close.  Note we ignore\n  strings and comments when matching; and the line we return is the\n  'cleansed' line at linenum.\n\"\"\"\n\n", "func_signal": "def CloseExpression(clean_lines, linenum, pos):\n", "code": "line = clean_lines.elided[linenum]\nstartchar = line[pos]\nif startchar not in '({[':\n  return (line, clean_lines.NumLines(), -1)\nif startchar == '(': endchar = ')'\nif startchar == '[': endchar = ']'\nif startchar == '{': endchar = '}'\n\nnum_open = line.count(startchar) - line.count(endchar)\nwhile linenum < clean_lines.NumLines() and num_open > 0:\n  linenum += 1\n  line = clean_lines.elided[linenum]\n  num_open += line.count(startchar) - line.count(endchar)\n# OK, now find the endchar that actually got us back to even\nendpos = len(line)\nwhile num_open >= 0:\n  endpos = line.rfind(')', 0, endpos)\n  num_open -= 1                 # chopped off another )\nreturn (line, linenum, endpos + 1)", "path": "cpplint.py", "repo_name": "mstone/vscan", "stars": 16, "license": "bsd-3-clause", "language": "python", "size": 567}
{"docstring": "\"\"\"Report if too many lines in function body.\n\nArgs:\n  error: The function to call with any errors found.\n  filename: The name of the current file.\n  linenum: The number of the line to check.\n\"\"\"\n", "func_signal": "def Check(self, error, filename, linenum):\n", "code": "if Match(r'T(EST|est)', self.current_function):\n  base_trigger = self._TEST_TRIGGER\nelse:\n  base_trigger = self._NORMAL_TRIGGER\ntrigger = base_trigger * 2**_VerboseLevel()\n\nif self.lines_in_function > trigger:\n  error_level = int(math.log(self.lines_in_function / base_trigger, 2))\n  # 50 => 0, 100 => 1, 200 => 2, 400 => 3, 800 => 4, 1600 => 5, ...\n  if error_level > 5:\n    error_level = 5\n  error(filename, linenum, 'readability/fn_size', error_level,\n        'Small and focused functions are preferred:'\n        ' %s has %d non-comment lines'\n        ' (error triggered by exceeding %d lines).'  % (\n            self.current_function, self.lines_in_function, trigger))", "path": "cpplint.py", "repo_name": "mstone/vscan", "stars": 16, "license": "bsd-3-clause", "language": "python", "size": 567}
{"docstring": "\"\"\"Return the most recent non-blank line and its line number.\n\nArgs:\n  clean_lines: A CleansedLines instance containing the file contents.\n  linenum: The number of the line to check.\n\nReturns:\n  A tuple with two elements.  The first element is the contents of the last\n  non-blank line before the current line, or the empty string if this is the\n  first non-blank line.  The second is the line number of that line, or -1\n  if this is the first non-blank line.\n\"\"\"\n\n", "func_signal": "def GetPreviousNonBlankLine(clean_lines, linenum):\n", "code": "prevlinenum = linenum - 1\nwhile prevlinenum >= 0:\n  prevline = clean_lines.elided[prevlinenum]\n  if not IsBlankLine(prevline):     # if not a blank line...\n    return (prevline, prevlinenum)\n  prevlinenum -= 1\nreturn ('', -1)", "path": "cpplint.py", "repo_name": "mstone/vscan", "stars": 16, "license": "bsd-3-clause", "language": "python", "size": 567}
{"docstring": "\"\"\"\nBuilds the field and prepares it to access to related data.\n\nThe ``to`` argument should point to a ``Resource`` class, NOT\nto a ``Model``. Required.\n\nThe ``attribute`` argument should specify what field/callable points to\nthe related data on the instance object. Required.\n\nOptionally accepts a ``related_name`` argument. Currently unused, as\nunlike Django's ORM layer, reverse relations between ``Resource``\nclasses are not automatically created. Defaults to ``None``.\n\nOptionally accepts a ``null``, which indicated whether or not a\n``None`` is allowable data on the field. Defaults to ``False``.\n\nOptionally accepts a ``blank``, which indicated whether or not\ndata may be omitted on the field. Defaults to ``False``.\n\nOptionally accepts a ``readonly``, which indicates whether the field\nis used during the ``hydrate`` or not. Defaults to ``False``.\n\nOptionally accepts a ``full``, which indicates how the related\n``Resource`` will appear post-``dehydrate``. If ``False``, the\nrelated ``Resource`` will appear as a URL to the endpoint of that\nresource. If ``True``, the result of the sub-resource's\n``dehydrate`` will be included in full.\n\nOptionally accepts a ``unique``, which indicates if the field is a\nunique identifier for the object.\n\nOptionally accepts ``help_text``, which lets you provide a\nhuman-readable description of the field exposed at the schema level.\nDefaults to the per-Field definition.\n\"\"\"\n", "func_signal": "def __init__(self, to, attribute, related_name=None, default=NOT_PROVIDED, null=False, blank=False, readonly=False, full=False, unique=False, help_text=None):\n", "code": "self.instance_name = None\nself._resource = None\nself.to = to\nself.attribute = attribute\nself.related_name = related_name\nself._default = default\nself.null = null\nself.blank = blank\nself.readonly = readonly\nself.full = full\nself.api_name = None\nself.resource_name = None\nself.unique = unique\nself._to_class = None\n\nif self.to == 'self':\n    self.self_referential = True\n    self._to_class = self.__class__\n\nif help_text:\n    self.help_text = help_text", "path": "piecrust\\fields.py", "repo_name": "toastdriven/piecrust", "stars": 16, "license": "other", "language": "python", "size": 258}
{"docstring": "\"\"\"\nGiven a URI is provided, the related resource is attempted to be\nloaded based on the identifiers in the URI.\n\"\"\"\n", "func_signal": "def resource_from_uri(self, fk_resource, uri, request=None, related_obj=None, related_name=None):\n", "code": "try:\n    obj = fk_resource.get_via_uri(uri, request=request)\n    bundle = fk_resource.build_bundle(obj=obj, request=request)\n    return fk_resource.full_dehydrate(bundle)\nexcept NotFound:\n    raise ApiFieldError(\"Could not find the provided object via resource URI '%s'.\" % uri)", "path": "piecrust\\fields.py", "repo_name": "toastdriven/piecrust", "stars": 16, "license": "other", "language": "python", "size": 258}
{"docstring": "\"\"\"Import a module.\n\nThe 'package' argument is required when performing a relative import. It\nspecifies the package to use as the anchor point from which to resolve the\nrelative import to an absolute import.\n\n\"\"\"\n", "func_signal": "def import_module(name, package=None):\n", "code": "if name.startswith('.'):\n    if not package:\n        raise TypeError(\"relative imports require the 'package' argument\")\n    level = 0\n    for character in name:\n        if character != '.':\n            break\n        level += 1\n    name = _resolve_name(name[level:], package, level)\n__import__(name)\nreturn sys.modules[name]", "path": "piecrust\\utils\\importlib.py", "repo_name": "toastdriven/piecrust", "stars": 16, "license": "other", "language": "python", "size": 258}
{"docstring": "# Do the least we can here so that we don't hate ourselves in the\n# morning.\n", "func_signal": "def contribute_to_class(self, cls, name):\n", "code": "self.instance_name = name\nself._resource = cls", "path": "piecrust\\fields.py", "repo_name": "toastdriven/piecrust", "stars": 16, "license": "other", "language": "python", "size": 258}
{"docstring": "\"\"\"\nTakes data stored in the bundle for the field and returns it. Used for\ntaking simple data and building a instance object.\n\"\"\"\n", "func_signal": "def hydrate(self, bundle):\n", "code": "if self.readonly:\n    return None\n\nif not bundle.data.has_key(self.instance_name):\n    if getattr(self, 'is_related', False) and not getattr(self, 'is_m2m', False):\n        # We've got an FK (or alike field) & a possible parent object.\n        # Check for it.\n        if bundle.related_obj and bundle.related_name in (self.attribute, self.instance_name):\n            return bundle.related_obj\n\n    if self.blank:\n        return None\n    elif self.attribute and getattr(bundle.obj, self.attribute, None):\n        return getattr(bundle.obj, self.attribute)\n    elif self.instance_name and hasattr(bundle.obj, self.instance_name):\n        return getattr(bundle.obj, self.instance_name)\n    elif self.has_default():\n        if callable(self._default):\n            return self._default()\n\n        return self._default\n    elif self.null:\n        return None\n    else:\n        raise ApiFieldError(\"The '%s' field has no data and doesn't allow a default or null value.\" % self.instance_name)\n\nreturn bundle.data[self.instance_name]", "path": "piecrust\\fields.py", "repo_name": "toastdriven/piecrust", "stars": 16, "license": "other", "language": "python", "size": 258}
{"docstring": "\"\"\"\nBased on the ``full_resource``, returns either the endpoint or the data\nfrom ``full_dehydrate`` for the related resource.\n\"\"\"\n", "func_signal": "def dehydrate_related(self, bundle, related_resource):\n", "code": "if not self.full:\n    # Be a good netizen.\n    return related_resource.get_resource_uri(bundle)\nelse:\n    # ZOMG extra data and big payloads.\n    bundle = related_resource.build_bundle(obj=related_resource.instance, request=bundle.request)\n    return related_resource.full_dehydrate(bundle)", "path": "piecrust\\fields.py", "repo_name": "toastdriven/piecrust", "stars": 16, "license": "other", "language": "python", "size": 258}
{"docstring": "# Automatically shove in the user.\n", "func_signal": "def alter_deserialized_detail_data(self, request, bundle_or_list):\n", "code": "if isinstance(bundle_or_list, Bundle):\n    # Handle the detail.\n    bundle_or_list.data['user'] = request.user\nelif isinstance(bundle_or_list, list):\n    # Handle the list.\n    for obj_data in bundle_or_list:\n        obj_data['user'] = request.user\n\nreturn bundle_or_list", "path": "tests\\core\\tests\\resources.py", "repo_name": "toastdriven/piecrust", "stars": 16, "license": "other", "language": "python", "size": 258}
{"docstring": "\"\"\"\nGiven a dictionary-like structure is provided, a fresh related\nresource is created using that data.\n\"\"\"\n# Try to hydrate the data provided.\n", "func_signal": "def resource_from_data(self, fk_resource, data, request=None, related_obj=None, related_name=None):\n", "code": "data = dict_strip_unicode_keys(data)\nfk_bundle = fk_resource.build_bundle(data=data, request=request)\n\nif related_obj:\n    fk_bundle.related_obj = related_obj\n    fk_bundle.related_name = related_name\n\n# We need to check to see if updates are allowed on the FK\n# resource. If not, we'll just return a populated bundle instead\n# of mistakenly updating something that should be read-only.\nif not fk_resource.can_update():\n    return fk_resource.full_hydrate(fk_bundle)\n\ntry:\n    return fk_resource.obj_update(fk_bundle, **data)\nexcept NotFound:\n    try:\n        # Attempt lookup by primary key\n        lookup_kwargs = dict((k, v) for k, v in data.iteritems() if getattr(fk_resource, k).unique)\n\n        if not lookup_kwargs:\n            raise NotFound()\n\n        return fk_resource.obj_update(fk_bundle, **lookup_kwargs)\n    except NotFound:\n        return fk_resource.full_hydrate(fk_bundle)\nexcept:\n    # Something else unexpected happened. Just build a fresh one.\n    return fk_resource.full_hydrate(fk_bundle)", "path": "piecrust\\fields.py", "repo_name": "toastdriven/piecrust", "stars": 16, "license": "other", "language": "python", "size": 258}
{"docstring": "\"\"\"\nGiven an object with a ``pk`` attribute, the related resource\nis attempted to be loaded via that PK.\n\"\"\"\n", "func_signal": "def resource_from_pk(self, fk_resource, obj, request=None, related_obj=None, related_name=None):\n", "code": "bundle = fk_resource.build_bundle(obj=obj, request=request)\nreturn fk_resource.full_dehydrate(bundle)", "path": "piecrust\\fields.py", "repo_name": "toastdriven/piecrust", "stars": 16, "license": "other", "language": "python", "size": 258}
{"docstring": "\"\"\"\nReturns a bundle of data built by the related resource, usually via\n``hydrate`` with the data provided.\n\nAccepts either a URI, a data dictionary (or dictionary-like structure)\nor an object with a ``pk``.\n\"\"\"\n", "func_signal": "def build_related_resource(self, value, request=None, related_obj=None, related_name=None):\n", "code": "self.fk_resource = self.to_class()\nkwargs = {\n    'request': request,\n    'related_obj': related_obj,\n    'related_name': related_name,\n}\n\nif isinstance(value, basestring):\n    # We got a URI. Load the object and assign it.\n    return self.resource_from_uri(self.fk_resource, value, **kwargs)\nelif hasattr(value, 'items'):\n    # We've got a data dictionary.\n    # Since this leads to creation, this is the only one of these\n    # methods that might care about \"parent\" data.\n    return self.resource_from_data(self.fk_resource, value, **kwargs)\nelif hasattr(value, 'pk'):\n    # We've got an object with a primary key.\n    return self.resource_from_pk(self.fk_resource, value, **kwargs)\nelse:\n    raise ApiFieldError(\"The '%s' field has was given data that was not a URI, not a dictionary-alike and does not have a 'pk' attribute: %s.\" % (self.instance_name, value))", "path": "piecrust\\fields.py", "repo_name": "toastdriven/piecrust", "stars": 16, "license": "other", "language": "python", "size": 258}
{"docstring": "\"\"\"Returns the default value for the field.\"\"\"\n", "func_signal": "def default(self):\n", "code": "if callable(self._default):\n    return self._default()\n\nreturn self._default", "path": "piecrust\\fields.py", "repo_name": "toastdriven/piecrust", "stars": 16, "license": "other", "language": "python", "size": 258}
{"docstring": "# Very minimal & stock.\n", "func_signal": "def test_init(self):\n", "code": "resource_1 = NoteResource()\nself.assertEqual(len(resource_1.fields), 8)\nself.assertNotEqual(resource_1._meta.queryset, None)\nself.assertEqual(resource_1._meta.resource_name, 'notes')\nself.assertEqual(resource_1._meta.limit, 20)\nself.assertEqual(resource_1._meta.list_allowed_methods, ['get', 'post', 'put', 'delete', 'patch'])\nself.assertEqual(resource_1._meta.detail_allowed_methods, ['get', 'post', 'put', 'delete', 'patch'])\nself.assertEqual(isinstance(resource_1._meta.serializer, Serializer), True)\n\n# Lightly custom.\nresource_2 = LightlyCustomNoteResource()\nself.assertEqual(len(resource_2.fields), 8)\nself.assertNotEqual(resource_2._meta.queryset, None)\nself.assertEqual(resource_2._meta.resource_name, 'noteish')\nself.assertEqual(resource_2._meta.limit, 20)\nself.assertEqual(resource_2._meta.list_allowed_methods, ['get'])\nself.assertEqual(resource_2._meta.detail_allowed_methods, ['get'])\nself.assertEqual(isinstance(resource_2._meta.serializer, Serializer), True)\n\n# Highly custom.\nresource_3 = VeryCustomNoteResource()\nself.assertEqual(len(resource_3.fields), 7)\nself.assertNotEqual(resource_3._meta.queryset, None)\nself.assertEqual(resource_3._meta.resource_name, 'notey')\nself.assertEqual(resource_3._meta.limit, 50)\nself.assertEqual(resource_3._meta.list_allowed_methods, ['get'])\nself.assertEqual(resource_3._meta.detail_allowed_methods, ['get', 'post', 'put'])\nself.assertEqual(isinstance(resource_3._meta.serializer, CustomSerializer), True)\n\n# Note - automatic resource naming.\nresource_4 = NoUriNoteResource()\nself.assertEqual(resource_4._meta.resource_name, 'nourinote')\n\n# Test to make sure that, even with a mix of basic & advanced\n# configuration, options are set right.\nresource_5 = TestOptionsResource()\nself.assertEqual(resource_5._meta.allowed_methods, ['post'])\n# Should be the overridden values.\nself.assertEqual(resource_5._meta.list_allowed_methods, ['post', 'put'])\n# Should inherit from the basic configuration.\nself.assertEqual(resource_5._meta.detail_allowed_methods, ['post'])\n\nresource_6 = CustomPageNoteResource()\nself.assertEqual(resource_6._meta.paginator_class, CustomPaginator)", "path": "tests\\core\\tests\\resources.py", "repo_name": "toastdriven/piecrust", "stars": 16, "license": "other", "language": "python", "size": 258}
{"docstring": "# Does a per-object check that \"can't\" be expressed as part of a\n# ``QuerySet``. This helps test that all objects in the ``QuerySet``\n# aren't loaded & evaluated, only results that match the request.\n", "func_signal": "def apply_limits(self, request, object_list):\n", "code": "final_list = []\n\nfor obj in object_list:\n    # Only match ``Note`` objects with 'post' in the title.\n    if 'post' in obj.title.lower():\n        final_list.append(obj)\n\nreturn final_list", "path": "tests\\core\\tests\\resources.py", "repo_name": "toastdriven/piecrust", "stars": 16, "license": "other", "language": "python", "size": 258}
{"docstring": "# Different from the ``ResourceTestCase.test_fields``, we're checking\n# some related bits here & self-referential bits later on.\n", "func_signal": "def test_fields(self):\n", "code": "resource_1 = RelatedNoteResource()\nself.assertEqual(len(resource_1.fields), 8)\nself.assert_('author' in resource_1.fields)\nself.assertTrue(isinstance(resource_1.fields['author'], fields.ToOneField))\nself.assertEqual(resource_1.fields['author']._resource, resource_1.__class__)\nself.assertEqual(resource_1.fields['author'].instance_name, 'author')\nself.assertTrue('subjects' in resource_1.fields)\nself.assertTrue(isinstance(resource_1.fields['subjects'], fields.ToManyField))\nself.assertEqual(resource_1.fields['subjects']._resource, resource_1.__class__)\nself.assertEqual(resource_1.fields['subjects'].instance_name, 'subjects')", "path": "tests\\core\\tests\\resources.py", "repo_name": "toastdriven/piecrust", "stars": 16, "license": "other", "language": "python", "size": 258}
{"docstring": "\"\"\"\nSets up the field. This is generally called when the containing\n``Resource`` is initialized.\n\nOptionally accepts an ``attribute``, which should be a string of\neither an instance attribute or callable off the object during the\n``dehydrate`` or push data onto an object during the ``hydrate``.\nDefaults to ``None``, meaning data will be manually accessed.\n\nOptionally accepts a ``default``, which provides default data when the\nobject being ``dehydrated``/``hydrated`` has no data on the field.\nDefaults to ``NOT_PROVIDED``.\n\nOptionally accepts a ``null``, which indicated whether or not a\n``None`` is allowable data on the field. Defaults to ``False``.\n\nOptionally accepts a ``blank``, which indicated whether or not\ndata may be omitted on the field. Defaults to ``False``.\n\nOptionally accepts a ``readonly``, which indicates whether the field\nis used during the ``hydrate`` or not. Defaults to ``False``.\n\nOptionally accepts a ``unique``, which indicates if the field is a\nunique identifier for the object.\n\nOptionally accepts ``help_text``, which lets you provide a\nhuman-readable description of the field exposed at the schema level.\nDefaults to the per-Field definition.\n\"\"\"\n# Track what the index thinks this field is called.\n", "func_signal": "def __init__(self, attribute=None, default=NOT_PROVIDED, null=False, blank=False, readonly=False, unique=False, help_text=None):\n", "code": "self.instance_name = None\nself._resource = None\nself.attribute = attribute\nself._default = default\nself.null = null\nself.blank = blank\nself.readonly = readonly\nself.value = None\nself.unique = unique\n\nif help_text:\n    self.help_text = help_text", "path": "piecrust\\fields.py", "repo_name": "toastdriven/piecrust", "stars": 16, "license": "other", "language": "python", "size": 258}
{"docstring": "# The common case, where the ``Api`` specifies the name.\n", "func_signal": "def test_urls(self):\n", "code": "resource = NoteResource(api_name='v1')\npatterns = resource.urls\nself.assertEqual(len(patterns), 4)\nself.assertEqual([pattern.name for pattern in patterns], ['api_dispatch_list', 'api_get_schema', 'api_get_multiple', 'api_dispatch_detail'])\nself.assertEqual(reverse('api_dispatch_list', kwargs={\n    'api_name': 'v1',\n    'resource_name': 'notes',\n}), '/api/v1/notes/')\nself.assertEqual(reverse('api_dispatch_detail', kwargs={\n    'api_name': 'v1',\n    'resource_name': 'notes',\n    'pk': 1,\n}), '/api/v1/notes/1/')\n\n# Start over.\nresource = NoteResource()\npatterns = resource.urls\nself.assertEqual(len(patterns), 4)\nself.assertEqual([pattern.name for pattern in patterns], ['api_dispatch_list', 'api_get_schema', 'api_get_multiple', 'api_dispatch_detail'])\nself.assertEqual(reverse('api_dispatch_list', urlconf='core.tests.manual_urls', kwargs={\n    'resource_name': 'notes',\n}), '/notes/')\nself.assertEqual(reverse('api_dispatch_detail', urlconf='core.tests.manual_urls', kwargs={\n    'resource_name': 'notes',\n    'pk': 1,\n}), '/notes/1/')", "path": "tests\\core\\tests\\resources.py", "repo_name": "toastdriven/piecrust", "stars": 16, "license": "other", "language": "python", "size": 258}
{"docstring": "\"\"\"Return the absolute name of the module to be imported.\"\"\"\n", "func_signal": "def _resolve_name(name, package, level):\n", "code": "if not hasattr(package, 'rindex'):\n    raise ValueError(\"'package' not set to a string\")\ndot = len(package)\nfor x in xrange(level, 1, -1):\n    try:\n        dot = package.rindex('.', 0, dot)\n    except ValueError:\n        raise ValueError(\"attempted relative import beyond top-level \"\n                          \"package\")\nreturn \"%s.%s\" % (package[:dot], name)", "path": "piecrust\\utils\\importlib.py", "repo_name": "toastdriven/piecrust", "stars": 16, "license": "other", "language": "python", "size": 258}
{"docstring": "# We're going to heavily jack with settings. :/\n", "func_signal": "def setUp(self):\n", "code": "super(BustedResourceTestCase, self).setUp()\nself.old_debug = settings.DEBUG\nself.old_full_debug = getattr(settings, 'TASTYPIE_FULL_DEBUG', False)\nself.old_canned_error = getattr(settings, 'TASTYPIE_CANNED_ERROR', \"Sorry, this request could not be processed. Please try again later.\")\nself.old_broken_links = getattr(settings, 'SEND_BROKEN_LINK_EMAILS', False)\n\nself.resource = BustedResource()\nself.request = HttpRequest()\nself.request.GET = {'format': 'json'}\nself.request.method = 'GET'", "path": "tests\\core\\tests\\resources.py", "repo_name": "toastdriven/piecrust", "stars": 16, "license": "other", "language": "python", "size": 258}
{"docstring": "\"\"\"\nTakes data from the provided object and prepares it for the\nresource.\n\"\"\"\n", "func_signal": "def dehydrate(self, bundle):\n", "code": "if self.attribute is not None:\n    # Check for `__` in the field for looking through the relation.\n    attrs = self.attribute.split('__')\n    current_object = bundle.obj\n\n    for attr in attrs:\n        previous_object = current_object\n        current_object = getattr(current_object, attr, None)\n\n        if current_object is None:\n            if self.has_default():\n                current_object = self._default\n                # Fall out of the loop, given any further attempts at\n                # accesses will fail miserably.\n                break\n            elif self.null:\n                current_object = None\n                # Fall out of the loop, given any further attempts at\n                # accesses will fail miserably.\n                break\n            else:\n                raise ApiFieldError(\"The object '%r' has an empty attribute '%s' and doesn't allow a default or null value.\" % (previous_object, attr))\n\n    if callable(current_object):\n        current_object = current_object()\n\n    return self.convert(current_object)\n\nif self.has_default():\n    return self.convert(self.default)\nelse:\n    return None", "path": "piecrust\\fields.py", "repo_name": "toastdriven/piecrust", "stars": 16, "license": "other", "language": "python", "size": 258}
{"docstring": "# Regression: You have a FK field that's required on the model\n# but you want to optionally allow the user to omit it and use\n# custom ``hydrate_*`` method to populate it if it's not\n# present.\n", "func_signal": "def test_optional_required_data(self):\n", "code": "nmbr = NullableMediaBitResource()\n\nbundle_1 = Bundle(data={\n    'title': \"Foo\",\n})\n\ntry:\n    # This is where things blow up, because you can't assign\n    # ``None`` to a required FK.\n    hydrated1 = nmbr.full_hydrate(bundle_1)\n    self.fail()\nexcept Note.DoesNotExist:\n    pass\n\n# So we introduced ``blank=True``.\nbmbr = BlankMediaBitResource()\nhydrated1 = bmbr.full_hydrate(bundle_1)\nself.assertEqual(hydrated1.obj.title, \"Foo\")\nself.assertEqual(hydrated1.obj.note.pk, 1)", "path": "tests\\core\\tests\\resources.py", "repo_name": "toastdriven/piecrust", "stars": 16, "license": "other", "language": "python", "size": 258}
{"docstring": "\"\"\"a utility method to write a dictionary of tasks to the TODO file\"\"\"\n", "func_signal": "def writeTasks(taskDict):\n", "code": "keys = taskDict.keys()\nkeys.sort()\nbackup(TODO_FILE, TODO_BACKUP)\nf = open(TODO_FILE, \"w\")\nfor key in keys:\n    f.write(taskDict[key] + os.linesep)\nf.close()", "path": "todo.py", "repo_name": "abztrakt/ya-todo-py", "stars": 23, "license": "None", "language": "python", "size": 339}
{"docstring": "\"\"\"color replacement function used when highlighting overdue items\"\"\"\n", "func_signal": "def highlightLate(matchobj):\n", "code": "due = date(int(matchobj.group(2)), int(matchobj.group(3)), int(matchobj.group(4)))\nif due <= date.today():\n    return matchobj.group(1) + LATE + \"{due: \" + matchobj.group(2) + \"-\" + matchobj.group(3) + \"-\" \\\n    \t+ matchobj.group(4) + \"}\" + DEFAULT + matchobj.group(5)\nelse:\n    return DEFAULT + matchobj.group(0) + DEFAULT", "path": "todo.py", "repo_name": "abztrakt/ya-todo-py", "stars": 23, "license": "None", "language": "python", "size": 339}
{"docstring": "\"\"\" Use openanything to open a file or url and return a dictionary of info about it \"\"\"\n", "func_signal": "def getFile(source):\n", "code": "file = openanything.fetch(source)\nreturn file", "path": "chooser.py", "repo_name": "abztrakt/ya-todo-py", "stars": 23, "license": "None", "language": "python", "size": 339}
{"docstring": "\"\"\"Month Day Warning - add Warning days before the date eg. {Nov 22 +5}\"\"\"\n", "func_signal": "def hasWarning(rem, today):\n", "code": "re_rem = re.compile(r\" \\+(\\d+)$\")\nmatch = re.search(re_rem, rem)\nif match:\n    rem = re.sub(re_rem, \"\", rem)\n    return match.group(1), rem\nelse:\n    return False, False", "path": "todo_cron.py", "repo_name": "abztrakt/ya-todo-py", "stars": 23, "license": "None", "language": "python", "size": 339}
{"docstring": "\"\"\"Multiple DayOfWeek - recur each day that matches\neg. {Mon Wed} or {Mon Tue Wed} or {Mon Tue Wed Thu Fri}\"\"\"\n", "func_signal": "def multiDoW(rem, today):\n", "code": "words = rem.split()\nfor day in words:\n    type, now = singleDoW(day, today)\n    if not type:\n        # If one fails - they all fail\n        return False, False\n    if now:\n        return True, True\nreturn True, False", "path": "todo_cron.py", "repo_name": "abztrakt/ya-todo-py", "stars": 23, "license": "None", "language": "python", "size": 339}
{"docstring": "\"\"\"color replacement function used when highlighting priorities\"\"\"\n", "func_signal": "def highlightPriority(matchobj):\n", "code": "if (matchobj.group(1) == \"(A)\"):\n    return PRI_A + matchobj.group(0) + DEFAULT\nelif (matchobj.group(1) == \"(B)\"):\n    return PRI_B + matchobj.group(0) + DEFAULT\nelif (matchobj.group(1) == \"(C)\"):\n    return PRI_C + matchobj.group(0) + DEFAULT\nelse:\n    return PRI_X + matchobj.group(0) + DEFAULT", "path": "todo.py", "repo_name": "abztrakt/ya-todo-py", "stars": 23, "license": "None", "language": "python", "size": 339}
{"docstring": "\"\"\"sorting function to sort tasks alphabetically\"\"\"\n", "func_signal": "def alphaSort(a, b):\n", "code": "if sortIgnoreCase:\n    if (a[5:].lower() > b[5:].lower()): return 1\n    elif (a[5:].lower() < b[5:].lower()): return -1\n    else: return 0\nelse:\n    if (a[5:] > b[5:]): return 1\n    elif (a[5:] < b[5:]): return -1\n    else: return 0", "path": "todo.py", "repo_name": "abztrakt/ya-todo-py", "stars": 23, "license": "None", "language": "python", "size": 339}
{"docstring": "\"\"\"parses REM style date strings - returns True if event is today\"\"\"\n\n", "func_signal": "def parseREM(rem):\n", "code": "today = time.localtime()\n\nwarnDays, newrem = hasWarning(rem, today)\nif warnDays:\n    warnDays = int(warnDays)\n    rem = newrem\n\nrepeatDays, newrem = hasRepeat(rem, today)\nif repeatDays:\n    repeatDays = int(repeatDays)\n    rem = newrem\n\ntype, now = singleDay(rem, today)\nif type and now: return True\nif type and not now: return False\n\ntype, now = multiDay(rem, today)\nif type and now: return True\nif type and not now: return False\n\ntype, now = singleDoW(rem, today)\nif type and now: return True\nif type and not now: return False\n\ntype, now = multiDoW(rem, today)\nif type and now: return True\nif type and not now: return False\n\ntype, now = monthDay(rem, today, warn=warnDays, rep=repeatDays)\nif type and now: return True\nif type and not now: return False\n\ntype, now = monthDayYear(rem, today, warn=warnDays, rep=repeatDays)\nif type and now: return True\nif type and not now: return False\n\n# 1st DayOfWeek after date\n# {Mon 15}\n\n# xth DayOfWeek after date\n# {Mon 15}\n\n# OMIT\n\n#### Sub day --- no support planned\n# Times -- AT 5:00PM\n# {AT 5:00PM}", "path": "todo_cron.py", "repo_name": "abztrakt/ya-todo-py", "stars": 23, "license": "None", "language": "python", "size": 339}
{"docstring": "\"\"\"Set colors for use when printing text\"\"\"\n", "func_signal": "def setTheme(theme):\n", "code": "global PRI_A, PRI_B, PRI_C, PRI_X, DEFAULT, LATE\n\n# Set the theme from BGCOL environment variable\n# only set if not set by cmdline flag\nif not theme and os.environ.has_key('BGCOL'):\n    if os.environ['BGCOL'] == 'light':\n        theme = 'light'\n    elif os.environ['BGCOL'] == 'dark':\n        theme = 'dark'\n\n# If no theme from cmdline or environment then use default\nif not theme: theme = defaultTheme\n\nif theme == \"light\":\n    PRI_A = RED\n    PRI_B = GREEN\n    PRI_C = LIGHT_BLUE\n    PRI_X = PURPLE\n    LATE  = LIGHT_RED\nelif theme == \"dark\":\n    PRI_A = YELLOW\n    PRI_B = LIGHT_GREEN\n    PRI_C = LIGHT_PURPLE\n    PRI_X = WHITE\n    LATE  = LIGHT_RED\nelif theme == \"windark\" and os.name == 'nt':\n    PRI_A = WIN_YELLOW\n    PRI_B = WIN_GREEN\n    PRI_C = WIN_LBLUE\n    PRI_X = WIN_GREY\n    DEFAULT = WIN_WHITE\n    LATE  = WIN_RED\nelse:\n    PRI_A = NONE\n    PRI_B = NONE\n    PRI_C = NONE\n    PRI_X = NONE\n    DEFAULT = NONE\n    LATE = NONE", "path": "todo.py", "repo_name": "abztrakt/ya-todo-py", "stars": 23, "license": "None", "language": "python", "size": 339}
{"docstring": "\"\"\"append an extension attribute\"\"\"\n#rmExtension(item, attribute)\n", "func_signal": "def addExtension(item, attribute, value):\n", "code": "newextension = \"{\" + attribute + \": \" + value + \"}\"\nappend(item, newextension)", "path": "todo.py", "repo_name": "abztrakt/ya-todo-py", "stars": 23, "license": "None", "language": "python", "size": 339}
{"docstring": "\"\"\"Match tasks with a dates in a date list\"\"\"\n", "func_signal": "def addDates(tasks, dates):\n", "code": "temp = {}\nfor k,v in tasks.iteritems():\n    match = False\n    for date in dates:\n        if (re.search(date, v, re.IGNORECASE)): match = True\n\n    if (match == True): temp[k] = v\nreturn temp", "path": "todo.py", "repo_name": "abztrakt/ya-todo-py", "stars": 23, "license": "None", "language": "python", "size": 339}
{"docstring": "\"\"\" set_wincolor(FOREGROUND_GREEN | FOREGROUND_INTENSITY)\"\"\"\n", "func_signal": "def set_wincolor(color):\n", "code": "stdhandle = ctypes.windll.kernel32.GetStdHandle(-11)\nbool = ctypes.windll.kernel32.SetConsoleTextAttribute(stdhandle, color)\nreturn bool", "path": "todo.py", "repo_name": "abztrakt/ya-todo-py", "stars": 23, "license": "None", "language": "python", "size": 339}
{"docstring": "\"\"\"Month Day Repeat - add for Repeat days after the date eg. {Nov 22 *5}\"\"\"\n", "func_signal": "def hasRepeat(rem, today):\n", "code": "re_rem = re.compile(r\" \\*(\\d+)$\")\nmatch = re.search(re_rem, rem)\nif match:\n    rem = re.sub(re_rem, \"\", rem)\n    return match.group(1), rem\nelse:\n    return False, False", "path": "todo_cron.py", "repo_name": "abztrakt/ya-todo-py", "stars": 23, "license": "None", "language": "python", "size": 339}
{"docstring": "\"\"\"Return a task list based on a pattern - use remove for negate\"\"\"\n", "func_signal": "def findPatterns(tasks, patterns, escape=True, matchAny=False, remove=False):\n", "code": "temp = {}\nfor k,v in tasks.iteritems():\n    # Match any or all tasks using matchAny switch\n    match = True\n    if matchAny: match = False\n    for pattern in patterns:\n        if escape: pattern = re.escape(pattern)\n        if matchAny:\n            if (re.search(pattern, v, re.IGNORECASE)): match = True\n        else:\n            if (not re.search(pattern, v, re.IGNORECASE)): match = False\n\n    if remove:\n        # remove adds tasks if not matched\n        if not match: temp[k] = v\n    else:\n        # remove=False add if matched\n        if match: temp[k] = v\nreturn temp", "path": "todo.py", "repo_name": "abztrakt/ya-todo-py", "stars": 23, "license": "None", "language": "python", "size": 339}
{"docstring": "\"\"\"Check for existing tasks in the TODO file\"\"\"\n", "func_signal": "def taskExists(rem):\n", "code": "tasks = todo.getTaskDict()\ntheSet = set(tasks.values())\nif rem in theSet:\n    return True\nelse:\n    return False", "path": "todo_cron.py", "repo_name": "abztrakt/ya-todo-py", "stars": 23, "license": "None", "language": "python", "size": 339}
{"docstring": "\"\"\"Check if an item is a 'done' item\"\"\"\n", "func_signal": "def isDone(text):\n", "code": "if text.startswith(\"x \"): \n    return True\nelse: \n    return False", "path": "todo.py", "repo_name": "abztrakt/ya-todo-py", "stars": 23, "license": "None", "language": "python", "size": 339}
{"docstring": "\"\"\"report open and closed tasks - airchive first\"\"\"\n", "func_signal": "def report():\n", "code": "archive()\n\nactive = getTaskDict()\nclosed = getDoneDict()\n\ndate = time.strftime(\"%Y-%m-%d-%T\", time.localtime())\nf = open(REPORT_FILE, 'a')\nstring = \"%s %d %d\" % (date, len(active), len(closed))\nf.write(string + os.linesep)\nf.close()", "path": "todo.py", "repo_name": "abztrakt/ya-todo-py", "stars": 23, "license": "None", "language": "python", "size": 339}
{"docstring": "\"\"\"Remove #INCLUDE statements from task list\"\"\"\n", "func_signal": "def hideInclude(tasks):\n", "code": "patterns = ['#INCLUDE']\nreturn removePatterns(tasks, patterns, escape=False)", "path": "todo.py", "repo_name": "abztrakt/ya-todo-py", "stars": 23, "license": "None", "language": "python", "size": 339}
{"docstring": "\"\"\" Process the text and return todo items that can be written to the todo.txt data file \"\"\"\n# Create a tempfile and write our data into it\n", "func_signal": "def parseFile(file):\n", "code": "tfile = tempfile.mkstemp()[1]\nthandle = open(tfile, 'w')\nthandle.write(file['data'])\nthandle.close()\n\n# Open our tempfile and edit it\nsubprocess.call([editor, tfile])\n\n# Read the tempfile back in as todos\nthandle = open(tfile, 'r')\ntodos = thandle.read()\nthandle.close()\n\n# Clean up the tempfile, since mkstemp doesn't do that for us.\ncleanup = subprocess.Popen(\"rm \" + tfile, shell=True)\nsts = os.waitpid(cleanup.pid, 0)\n\nreturn todos", "path": "chooser.py", "repo_name": "abztrakt/ya-todo-py", "stars": 23, "license": "None", "language": "python", "size": 339}
{"docstring": "\"\"\"Handle priority if exisiting in supplied text\"\"\"\n\n", "func_signal": "def setPriority(text):\n", "code": "if singleLetterPriority:\n    re_pri = re.compile(r\"^[A-Z] \")\n    if re.search(re_pri, text):\n    # A single capital letter was found, convert it to a priority!\n        newpriority = \"(\" + text[0] + \") \"\n        text = re.sub(re_pri, newpriority, text)\nre_pri = re.compile(r\"^[\\(\\[\\{]([a-zA-Z])[\\]\\}\\)]\") # [a] or {a} or ()\nprio = re.search(re_pri, text)\nif prio:\n    newpriority = \"(\" + prio.group(1).upper() + \")\"\n    text = re.sub(re_pri, newpriority, text)\nreturn text", "path": "todo.py", "repo_name": "abztrakt/ya-todo-py", "stars": 23, "license": "None", "language": "python", "size": 339}
{"docstring": "\"\"\"Get sequences of height and weight.\n\nArgs:\n    jitter: float magnitude of random noise added to heights\n\nReturns:\n    tuple of sequences (heights, weights)\n\"\"\"\n", "func_signal": "def GetHeightWeight(self, jitter=0.0):\n", "code": "heights = []\nweights = []\nfor r in self.records:\n    if r.wtkg2 == 'NA' or r.htm3 == 'NA':\n        continue\n    \n    height = r.htm3 + random.uniform(-jitter, jitter)\n    \n    heights.append(height)\n    weights.append(r.wtkg2)\n    \nreturn heights, weights", "path": "thinkstats_resources\\brfss_scatter.py", "repo_name": "nierman/thinkstats", "stars": 18, "license": "None", "language": "python", "size": 50340}
{"docstring": "\"\"\"Computes a table suitable for use with chi-squared stats.\n\nThere are three uses of this function:\n\n1) To compute observed values, use probs=None and row_func=ComputeRow\n\n2) To compute expected values, provide probs from the pooled data,\n    and row_func=ComputeRow\n    \n3) To generate random values, provide pooled probs,\n    and row_func=SimulateRow\n    \nReturns:\n  row of rows of float values\n\"\"\"\n", "func_signal": "def ComputeRows(firsts, others, funcs, probs=None, row_func=ComputeRow):\n", "code": "rows = []\nfor table in [firsts, others]:\n    n = len(table)\n    row_probs = probs or [func(table.pmf) for func in funcs]\n    row = row_func(n, row_probs)\n    rows.append(row)\n\nreturn rows", "path": "thinkstats_resources\\chi.py", "repo_name": "nierman/thinkstats", "stars": 18, "license": "None", "language": "python", "size": 50340}
{"docstring": "\"\"\"Get sequences of mother's age, birth weight, and first baby flag.\n\nArgs:\n    table: Table object\n\nReturns:\n    tuple of sequences (ages, weights, first_bool)\n\"\"\"\n", "func_signal": "def GetAgeWeightFirst(table):\n", "code": "ages = []\nweights = []\nfirst_bool = []\nfor r in table.records:\n    if 'NA' in [r.agepreg, r.totalwgt_oz, r.birthord]:\n        continue\n\n    # first is 1.0 for first babies; 0.0 for others\n    if r.birthord == 1:\n        first = 1.0\n    else:\n        first = 0.0\n    \n    ages.append(r.agepreg)\n    weights.append(r.totalwgt_oz)\n    first_bool.append(first)\n\nreturn ages, weights, first_bool", "path": "thinkstats_resources\\age_lm.py", "repo_name": "nierman/thinkstats", "stars": 18, "license": "None", "language": "python", "size": 50340}
{"docstring": "\"\"\"Generates a sequence of points suitable for plotting.\n\nAn empirical CDF is a step function; linear interpolation\ncan be misleading.\n\nReturns:\n    tuple of (xs, ps)\n\"\"\"\n", "func_signal": "def Render(self):\n", "code": "xs = [self.xs[0]]\nps = [0.0]\nfor i, p in enumerate(self.ps):\n    xs.append(self.xs[i])\n    ps.append(p)\n\n    try:\n        xs.append(self.xs[i+1])\n        ps.append(p)\n    except IndexError:\n        pass\nreturn xs, ps", "path": "thinkstats_resources\\Cdf.py", "repo_name": "nierman/thinkstats", "stars": 18, "license": "None", "language": "python", "size": 50340}
{"docstring": "\"\"\"Makes a cdf from an unsorted sequence of (value, frequency) pairs.\n\nArgs:\n    items: unsorted sequence of (value, frequency) pairs\n    name: string name for this CDF\n\nReturns:\n    cdf: list of (value, fraction) pairs\n\"\"\"\n", "func_signal": "def MakeCdfFromItems(items, name=''):\n", "code": "runsum = 0\nxs = []\ncs = []\n\nfor value, count in sorted(items):\n    runsum += count\n    xs.append(value)\n    cs.append(runsum)\n\ntotal = float(runsum)\nps = [c/total for c in cs]\n\ncdf = Cdf(xs, ps, name)\nreturn cdf", "path": "thinkstats_resources\\Cdf.py", "repo_name": "nierman/thinkstats", "stars": 18, "license": "None", "language": "python", "size": 50340}
{"docstring": "\"\"\"Returns the Pmf with oversampling proportional to value.\n\nIf pmf is the distribution of true values, the result is the\ndistribution that would be seen if values are oversampled in\nproportion to their values; for example, if you ask students\nhow big their classes are, large classes are oversampled in\nproportion to their size.\n\nIf invert=True, computes in inverse operation; for example,\nunbiasing a sample collected from students.\n\nArgs:\n  pmf: Pmf object.\n  invert: boolean\n\n Returns:\n   Pmf object\n\"\"\"\n", "func_signal": "def BiasPmf(pmf, name, invert=False):\n", "code": "new_pmf = pmf.Copy()\nnew_pmf.name = name\n\nfor x, p in pmf.Items():\n    if invert:\n        new_pmf.Mult(x, 1.0/x)\n    else:\n        new_pmf.Mult(x, x)\n    \nnew_pmf.Normalize()\nreturn new_pmf", "path": "thinkstats_resources\\social.py", "repo_name": "nierman/thinkstats", "stars": 18, "license": "None", "language": "python", "size": 50340}
{"docstring": "\"\"\"Creates a CDF from an unsorted sequence.\n\nArgs:\n    seq: unsorted sequence of sortable values\n    name: string name for the cdf\n\nReturns:\n   Cdf object\n\"\"\"\n", "func_signal": "def MakeCdfFromList(seq, name=''):\n", "code": "hist = Pmf.MakeHistFromList(seq)\nreturn MakeCdfFromHist(hist, name)", "path": "thinkstats_resources\\Cdf.py", "repo_name": "nierman/thinkstats", "stars": 18, "license": "None", "language": "python", "size": 50340}
{"docstring": "\"\"\"Submits model to r.lm and returns the result.\"\"\"\n", "func_signal": "def RunModel(model, print_flag=True):\n", "code": "model = r(model)\nres = r.lm(model)\nif print_flag:\n    PrintSummary(res)\nreturn res", "path": "thinkstats_resources\\age_lm.py", "repo_name": "nierman/thinkstats", "stars": 18, "license": "None", "language": "python", "size": 50340}
{"docstring": "\"\"\"Generates a random row of a table.\n\nChooses all but the last element at random, then chooses the\nlast element to make the sums work.\n\nArgs:\n  n: sum of the elements in the row\n  prob: sequence of float probabilities\n\"\"\"\n", "func_signal": "def SimulateRow(n, probs):\n", "code": "row = [Binomial(n, prob) for prob in probs]\nrow[-1] += n - sum(row)\nreturn row", "path": "thinkstats_resources\\chi.py", "repo_name": "nierman/thinkstats", "stars": 18, "license": "None", "language": "python", "size": 50340}
{"docstring": "\"\"\"Returns a new PDF representing speeds observed at a given speed.\n\nThe chance of observing a runner is proportional to the difference\nin speed.\n\nArgs:\n    pmf: distribution of actual speeds\n    speed: speed of the observing runner\n    name: string name for the new dist\n\nReturns:\n    Pmf object\n\"\"\"\n", "func_signal": "def BiasPmf(pmf, speed, name=None):\n", "code": "new = pmf.Copy(name=name)\nfor val, prob in new.Items():\n    diff = abs(val - speed)\n    new.Mult(val, diff)\nnew.Normalize()\nreturn new", "path": "thinkstats_resources\\relay_soln.py", "repo_name": "nierman/thinkstats", "stars": 18, "license": "None", "language": "python", "size": 50340}
{"docstring": "\"\"\"Generates CDFs and normal prob plots for weights and log weights.\"\"\"\n", "func_signal": "def MakeFigures(self):\n", "code": "weights = [record.wtkg2 for record in self.records\n           if record.wtkg2 != 'NA']\nself.MakeNormalModel(weights, root='brfss_weight_model')\nrankit.MakeNormalPlot(weights,\n                      root='brfss_weight_normal',\n                      title='Adult weight',\n                      ylabel='Weight (kg)')\n\nlog_weights = [math.log(weight) for weight in weights]\nxmax = math.log(175.0)\naxis = [3.5, 5.2, 0, 1]\nself.MakeNormalModel(log_weights, \n                     root='brfss_weight_log',\n                     xmax=xmax,\n                     xlabel='adult weight (log kg)',\n                     axis=axis)\nrankit.MakeNormalPlot(log_weights, \n                      root='brfss_weight_lognormal',\n                      title='Adult weight',\n                      ylabel='Weight (log kg)')", "path": "thinkstats_resources\\brfss_figs.py", "repo_name": "nierman/thinkstats", "stars": 18, "license": "None", "language": "python", "size": 50340}
{"docstring": "\"\"\"Multiplies out a row of a table.\n\nArgs:\n  n: sum of the elements in the row\n  prob: sequence of float probabilities\n\"\"\"\n", "func_signal": "def ComputeRow(n, probs):\n", "code": "row = [n * prob for prob in probs]\nreturn row", "path": "thinkstats_resources\\chi.py", "repo_name": "nierman/thinkstats", "stars": 18, "license": "None", "language": "python", "size": 50340}
{"docstring": "\"\"\"Add an (x, p) pair to the end of this CDF.\n\nNote: this us normally used to build a CDF from scratch, not\nto modify existing CDFs.  It is up to the caller to make sure\nthat the result is a legal CDF.\n\"\"\"\n", "func_signal": "def Append(self, x, p):\n", "code": "self.xs.append(x)\nself.ps.append(p)", "path": "thinkstats_resources\\Cdf.py", "repo_name": "nierman/thinkstats", "stars": 18, "license": "None", "language": "python", "size": 50340}
{"docstring": "\"\"\"Returns a random sample from a binomial distribution.\n\nArgs:\n  n: int number of trials\n  prob: float probability\n  \nReturns:\n  int: number of successes\n\"\"\"\n", "func_signal": "def Binomial(n, prob):\n", "code": "t = [1 for _ in range(n) if random.random() < prob]\nreturn sum(t)", "path": "thinkstats_resources\\chi.py", "repo_name": "nierman/thinkstats", "stars": 18, "license": "None", "language": "python", "size": 50340}
{"docstring": "\"\"\"Reads a compressed data file.\n\nArgs:\n    filename: string name of the file to read\n\"\"\"\n", "func_signal": "def ReadFile(filename='soc-Slashdot0902.txt.gz', n=None):\n", "code": "if filename.endswith('gz'):\n    fp = gzip.open(filename)\nelse:\n    fp = open(filename)\n\nsrcs = {}\nfor i, line in enumerate(fp):\n    if i == n:\n        break\n\n    if line.startswith('#'):\n        continue\n\n    src, dest = line.split()\n\n    srcs.setdefault(src, []).append(dest)\n\nfp.close()\n\nreturn srcs", "path": "thinkstats_resources\\social.py", "repo_name": "nierman/thinkstats", "stars": 18, "license": "None", "language": "python", "size": 50340}
{"docstring": "\"\"\"Probability that a value from pmf1 is less than a value from pmf2.\n\nArgs:\n    pmf1: Pmf object\n    pmf2: Pmf object\n\nReturns:\n    float\n\"\"\"\n", "func_signal": "def PmfProbLess(pmf1, pmf2):\n", "code": "total = 0.0\nfor v1, p1 in pmf1.Items():\n    for v2, p2 in pmf2.Items():\n        if v1 < v2:\n            total += p1 * p2\nreturn total", "path": "thinkstats_resources\\social.py", "repo_name": "nierman/thinkstats", "stars": 18, "license": "None", "language": "python", "size": 50340}
{"docstring": "\"\"\"Makes a CDF from a Pmf object.\n\nArgs:\n   d: dictionary that maps values to frequencies.\n   name: string name for the data.\n\nReturns:\n    Cdf object\n\"\"\"\n", "func_signal": "def MakeCdfFromPmf(pmf, name=None):\n", "code": "if name == None:\n    name = pmf.name\nreturn MakeCdfFromItems(pmf.Items(), name)", "path": "thinkstats_resources\\Cdf.py", "repo_name": "nierman/thinkstats", "stars": 18, "license": "None", "language": "python", "size": 50340}
{"docstring": "\"\"\"Compute the Chi-squared statistic for two tables.\n\nArgs:\n  expected: row of rows of values\n  observed: row of rows of values\n  \nReturns:\n  float chi-squared statistic\n\"\"\"\n", "func_signal": "def ChiSquared(expected, observed):\n", "code": "it = zip(itertools.chain(*expected), \n         itertools.chain(*observed))\nt = [(obs - exp)**2 / exp for exp, obs in it]\nreturn sum(t)", "path": "thinkstats_resources\\chi.py", "repo_name": "nierman/thinkstats", "stars": 18, "license": "None", "language": "python", "size": 50340}
{"docstring": "\"\"\"Returns CDF(x), the probability that corresponds to value x.\n\nArgs:\n    x: number\n\nReturns:\n    float probability\n\"\"\"\n", "func_signal": "def Prob(self, x):\n", "code": "if x < self.xs[0]: return 0.0\nindex = bisect.bisect(self.xs, x)\np = self.ps[index-1]\nreturn p", "path": "thinkstats_resources\\Cdf.py", "repo_name": "nierman/thinkstats", "stars": 18, "license": "None", "language": "python", "size": 50340}
{"docstring": "# get the data\n", "func_signal": "def main():\n", "code": "pool, firsts, others = descriptive.MakeTables()\nTest(pool, firsts, others, num_trials=1000)", "path": "thinkstats_resources\\chi.py", "repo_name": "nierman/thinkstats", "stars": 18, "license": "None", "language": "python", "size": 50340}
{"docstring": "# Make an exception from an entry in ERRORARRAY.\n", "func_signal": "def _make_excp(self, error):\n", "code": "if \"ERRORCODE\" not in error or \"ERRORMESSAGE\" not in error:\n    return None\nif error[\"ERRORCODE\"] ==  4:\n    return InvalidCredsException(error[\"ERRORMESSAGE\"])\nreturn LinodeException(error[\"ERRORCODE\"], error[\"ERRORMESSAGE\"])", "path": "libcloud\\drivers\\linode.py", "repo_name": "secondstory/dewpoint", "stars": 19, "license": "apache-2.0", "language": "python", "size": 433}
{"docstring": "\"\"\"\nReboot the node by passing in the node object\n\"\"\"\n", "func_signal": "def reboot_node(self, node):\n", "code": "params = {'method': 'voxel.devices.power',\n          'device_id': node.id,\n          'power_action': 'reboot'}\nreturn self._getstatus(self.connection.request('/', params=params).object)", "path": "libcloud\\drivers\\voxel.py", "repo_name": "secondstory/dewpoint", "stars": 19, "license": "apache-2.0", "language": "python", "size": 433}
{"docstring": "# Given a response object, slurp the information from it.\n", "func_signal": "def __init__(self, response):\n", "code": "self.body = response.read()\nself.status = response.status\nself.headers = dict(response.getheaders())\nself.error = response.reason\nself.invalid = LinodeException(0xFF,\n                               \"Invalid JSON received from server\")\n\n# Move parse_body() to here;  we can't be sure of failure until we've\n# parsed the body into JSON.\nself.action, self.object, self.errors = self.parse_body()\n\nif self.error == \"Moved Temporarily\":\n    raise LinodeException(0xFA,\n                          \"Redirected to error page by API.  Bug?\")\n\nif not self.success():\n    # Raise the first error, as there will usually only be one\n    raise self.errors[0]", "path": "libcloud\\drivers\\linode.py", "repo_name": "secondstory/dewpoint", "stars": 19, "license": "apache-2.0", "language": "python", "size": 433}
{"docstring": "# Convert a returned Linode instance into a Node instance.\n", "func_signal": "def _to_node(self, obj):\n", "code": "lid = obj[\"LINODEID\"]\n\n# Get the IP addresses for a Linode\nparams = { \"api_action\": \"linode.ip.list\", \"LinodeID\": lid }\nreq = self.connection.request(LINODE_ROOT, params=params)\nif not req.success() or len(req.object) == 0:\n    return None\n\npublic_ip = []\nprivate_ip = []\nfor ip in req.object:\n    if ip[\"ISPUBLIC\"]:\n        public_ip.append(ip[\"IPADDRESS\"])\n    else:\n        private_ip.append(ip[\"IPADDRESS\"])\n\nn = Node(id=lid, name=obj[\"LABEL\"],\n    state=self.LINODE_STATES[obj[\"STATUS\"]], public_ip=public_ip,\n    private_ip=private_ip, driver=self.connection.driver)\nn.extra = copy(obj)\nreturn n", "path": "libcloud\\drivers\\linode.py", "repo_name": "secondstory/dewpoint", "stars": 19, "license": "apache-2.0", "language": "python", "size": 433}
{"docstring": "# NOTE: country currently hardcoded\n", "func_signal": "def _to_location(self, location):\n", "code": "return NodeLocation(id = location.findtext('ID'),\n                    name = location.findtext('Name'),\n                    country = 'US',\n                    driver = self.connection.driver)", "path": "libcloud\\drivers\\ibm_sbc.py", "repo_name": "secondstory/dewpoint", "stars": 19, "license": "apache-2.0", "language": "python", "size": 433}
{"docstring": "\"\"\"Gets a driver\n@param provider: Id of provider to get driver\n@type provider: L{libcloud.types.Provider}\n\"\"\"\n", "func_signal": "def get_driver(provider):\n", "code": "if provider in DRIVERS:\n    mod_name, driver_name = DRIVERS[provider]\n    _mod = __import__(mod_name, globals(), locals(), [driver_name])\n    return getattr(_mod, driver_name)", "path": "libcloud\\providers.py", "repo_name": "secondstory/dewpoint", "stars": 19, "license": "apache-2.0", "language": "python", "size": 433}
{"docstring": "\"\"\"\nCreates a node in the IBM Developer Cloud.\n\nSee L{NodeDriver.create_node} for more keyword args.\n\n@keyword    ex_configurationData: Image-specific configuration parameters.\n                               Configuration parameters are defined in\n                               the parameters.xml file.  The URL to\n                               this file is defined in the NodeImage\n                               at extra[parametersURL].\n@type       ex_configurationData: C{dict}\n\"\"\"\n\n# Compose headers for message body\n", "func_signal": "def create_node(self, **kwargs):\n", "code": "data = {}\ndata.update({'name': kwargs['name']})\ndata.update({'imageID': kwargs['image'].id})\ndata.update({'instanceType': kwargs['size'].id})\nif 'location' in kwargs:\n    data.update({'location': kwargs['location'].id})\nelse:\n    data.update({'location': '1'})\nif 'auth' in kwargs and isinstance(kwargs['auth'], NodeAuthSSHKey):\n    data.update({'publicKey': kwargs['auth'].pubkey})\nif 'ex_configurationData' in kwargs:\n    configurationData = kwargs['ex_configurationData']\n    for key in configurationData.keys():\n        data.update({key: configurationData.get(key)})\n\n# Send request!\nresp = self.connection.request(action = REST_BASE + 'instances',\n                               headers = {'Content-Type': 'application/x-www-form-urlencoded'},\n                               method = 'POST',\n                               data = data).object\nreturn self._to_nodes(resp)[0]", "path": "libcloud\\drivers\\ibm_sbc.py", "repo_name": "secondstory/dewpoint", "stars": 19, "license": "apache-2.0", "language": "python", "size": 433}
{"docstring": "# IBM Developer Cloud instances currently support SMALL, MEDIUM, and\n# LARGE.  Storage also supports SMALL, MEDIUM, and LARGE.\n", "func_signal": "def list_sizes(self, location = None):\n", "code": "return [ NodeSize('SMALL', 'SMALL', None, None, None, None, self.connection.driver),\n         NodeSize('MEDIUM', 'MEDIUM', None, None, None, None, self.connection.driver),\n         NodeSize('LARGE', 'LARGE', None, None, None, None, self.connection.driver) ]", "path": "libcloud\\drivers\\ibm_sbc.py", "repo_name": "secondstory/dewpoint", "stars": 19, "license": "apache-2.0", "language": "python", "size": 433}
{"docstring": "# Set the datacenter for create requests.\n#\n# Create will try to guess, based on where all of the API key's\n# Linodes are located; if they are all in one location, Create will\n# make a new node there.  If there are NO Linodes on the account or\n# Linodes are in multiple locations, it is imperative to set this or\n# creates will fail.\n", "func_signal": "def linode_set_datacenter(self, did):\n", "code": "params = { \"api_action\": \"avail.datacenters\" }\ndata = self.connection.request(LINODE_ROOT, params=params).object\nfor dc in data:\n    if did == dc[\"DATACENTERID\"]:\n        self.datacenter = did\n        return\n\ndcs = \", \".join([d[\"DATACENTERID\"] for d in data])\nself.datacenter = None\nraise LinodeException(0xFD, \"Invalid datacenter (use one of %s)\" % dcs)", "path": "libcloud\\drivers\\linode.py", "repo_name": "secondstory/dewpoint", "stars": 19, "license": "apache-2.0", "language": "python", "size": 433}
{"docstring": "\"\"\"Create a new GoGird node\n\nSee L{NodeDriver.create_node} for more keyword args.\n\n@keyword    ex_description: Description of a Node\n@type       ex_description: C{string}\n\"\"\"\n", "func_signal": "def create_node(self, **kwargs):\n", "code": "name = kwargs['name']\nimage = kwargs['image']\nsize = kwargs['size']\nfirst_ip = self._get_first_ip()\nparams = {'name': name,\n          'image': image.id,\n          'description': kwargs.get('ex_description',''),\n          'server.ram': size.id,\n          'ip':first_ip}\n\nobject = self.connection.request('/api/grid/server/add',\n                                 params=params).object\nnode = self._to_node(object['list'][0])\n\nreturn node", "path": "libcloud\\drivers\\gogrid.py", "repo_name": "secondstory/dewpoint", "stars": 19, "license": "apache-2.0", "language": "python", "size": 433}
{"docstring": "# List Images\n# Retrieve all available Linux distributions.\n", "func_signal": "def list_images(self, location=None):\n", "code": "params = { \"api_action\": \"avail.distributions\" }\ndata = self.connection.request(LINODE_ROOT, params=params).object\ndistros = []\nfor obj in data:\n    i = NodeImage(id=obj[\"DISTRIBUTIONID\"],\n                  name=obj[\"LABEL\"],\n                  driver=self.connection.driver,\n                  extra={'pvops': obj['REQUIRESPVOPSKERNEL'],\n                         '64bit': obj['IS64BIT']})\n    distros.append(i)\nreturn distros", "path": "libcloud\\drivers\\linode.py", "repo_name": "secondstory/dewpoint", "stars": 19, "license": "apache-2.0", "language": "python", "size": 433}
{"docstring": "# List\n# Provide a list of all nodes that this API key has access to.\n", "func_signal": "def list_nodes(self):\n", "code": "params = { \"api_action\": \"linode.list\" }\ndata = self.connection.request(LINODE_ROOT, params=params).object\nreturn [self._to_node(n) for n in data]", "path": "libcloud\\drivers\\linode.py", "repo_name": "secondstory/dewpoint", "stars": 19, "license": "apache-2.0", "language": "python", "size": 433}
{"docstring": "# Parse the body of the response into JSON.  Will return None if the\n# JSON response chokes the parser.  Returns a triple:\n#    (action, data, errorarray)\n", "func_signal": "def parse_body(self):\n", "code": "try:\n    js = json.loads(self.body)\n    if (\"DATA\" not in js\n        or \"ERRORARRAY\" not in js\n        or \"ACTION\" not in js):\n\n        return (None, None, [self.invalid])\n    errs = [self._make_excp(e) for e in js[\"ERRORARRAY\"]]\n    return (js[\"ACTION\"], js[\"DATA\"], errs)\nexcept:\n    # Assume invalid JSON, and use an error code unused by Linode API.\n    return (None, None, [self.invalid])", "path": "libcloud\\drivers\\linode.py", "repo_name": "secondstory/dewpoint", "stars": 19, "license": "apache-2.0", "language": "python", "size": 433}
{"docstring": "# Reboot\n# Execute a shutdown and boot job for the given Node.\n", "func_signal": "def reboot_node(self, node):\n", "code": "params = { \"api_action\": \"linode.reboot\", \"LinodeID\": node.id }\nself.connection.request(LINODE_ROOT, params=params)\nreturn True", "path": "libcloud\\drivers\\linode.py", "repo_name": "secondstory/dewpoint", "stars": 19, "license": "apache-2.0", "language": "python", "size": 433}
{"docstring": "# Obtain the errors from the response.  Will always return a list.\n", "func_signal": "def parse_error(self):\n", "code": "try:\n    js = json.loads(self.body)\n    if \"ERRORARRAY\" not in js:\n        return [self.invalid]\n    return [self._make_excp(e) for e in js[\"ERRORARRAY\"]]\nexcept:\n    return [self.invalid]", "path": "libcloud\\drivers\\linode.py", "repo_name": "secondstory/dewpoint", "stars": 19, "license": "apache-2.0", "language": "python", "size": 433}
{"docstring": "# List Sizes\n# Retrieve all available Linode plans.\n# FIXME: Prices get mangled due to 'float'.\n", "func_signal": "def list_sizes(self, location=None):\n", "code": "params = { \"api_action\": \"avail.linodeplans\" }\ndata = self.connection.request(LINODE_ROOT, params=params).object\nsizes = []\nfor obj in data:\n    n = NodeSize(id=obj[\"PLANID\"], name=obj[\"LABEL\"], ram=obj[\"RAM\"],\n            disk=(obj[\"DISK\"] * 1024), bandwidth=obj[\"XFER\"],\n            price=obj[\"PRICE\"], driver=self.connection.driver)\n    sizes.append(n)\nreturn sizes", "path": "libcloud\\drivers\\linode.py", "repo_name": "secondstory/dewpoint", "stars": 19, "license": "apache-2.0", "language": "python", "size": 433}
{"docstring": "# Destroy\n# Terminates a Node.  With prejudice.\n", "func_signal": "def destroy_node(self, node):\n", "code": "params = { \"api_action\": \"linode.delete\", \"LinodeID\": node.id,\n    \"skipChecks\": True }\nself.connection.request(LINODE_ROOT, params=params)\nreturn True", "path": "libcloud\\drivers\\linode.py", "repo_name": "secondstory/dewpoint", "stars": 19, "license": "apache-2.0", "language": "python", "size": 433}
{"docstring": "\"\"\" create sig from md5 of key + secret + time \"\"\"\n", "func_signal": "def get_signature(self, key, secret):\n", "code": "m = hashlib.md5(key+secret+str(int(time.time())))\nreturn m.hexdigest()", "path": "libcloud\\drivers\\gogrid.py", "repo_name": "secondstory/dewpoint", "stars": 19, "license": "apache-2.0", "language": "python", "size": 433}
{"docstring": "# power in ['start', 'stop', 'restart']\n", "func_signal": "def _server_power(self, id, power):\n", "code": "params = {'id': id, 'power': power}\nreturn self.connection.request(\"/api/grid/server/power\", params)", "path": "libcloud\\drivers\\gogrid.py", "repo_name": "secondstory/dewpoint", "stars": 19, "license": "apache-2.0", "language": "python", "size": 433}
{"docstring": "\"\"\"Create a new linode instance\n\nSee L{NodeDriver.create_node} for more keyword args.\n\n@keyword    ex_swap: Size of the swap partition in MB (128).\n@type       ex_swap: C{number}\n\n@keyword    ex_rsize: Size of the root partition (plan size - swap).\n@type       ex_rsize: C{number}\n\n@keyword    ex_kernel: A kernel ID from avail.kernels (Latest 2.6).\n@type       ex_kernel: C{number}\n\n@keyword    ex_payment: One of 1, 12, or 24; subscription length (1)\n@type       ex_payment: C{number}\n\n@keyword    ex_comment: Comments to store with the config\n@type       ex_comment: C{str}\n\"\"\"\n#    Labels to override what's generated (default on right):\n#       lconfig      [%name] Instance\n#       lrecovery    [%name] Finnix Recovery Configuration\n#       lroot        [%name] %distro\n#       lswap        [%name] Swap Space\n#\n# Datacenter logic:\n#\n#   As Linode requires choosing a datacenter, a little logic is done.\n#\n#   1. If the API key in use has all its Linodes in one DC, that DC will\n#      be chosen (and can be overridden with linode_set_datacenter).\n#\n#   2. Otherwise (for both the \"No Linodes\" and \"different DC\" cases), a\n#      datacenter must explicitly be chosen using linode_set_datacenter.\n#\n# Please note that for safety, only 5 Linodes can be created per hour.\n\n", "func_signal": "def create_node(self, **kwargs):\n", "code": "name = kwargs[\"name\"]\nchosen = kwargs[\"location\"].id\nimage = kwargs[\"image\"]\nsize = kwargs[\"size\"]\nauth = kwargs[\"auth\"]\n\n# Step 0: Parameter validation before we purchase\n# We're especially careful here so we don't fail after purchase, rather\n# than getting halfway through the process and having the API fail.\n\n# Plan ID\nplans = self.list_sizes()\nif size.id not in [p.id for p in plans]:\n    raise LinodeException(0xFB, \"Invalid plan ID -- avail.plans\")\n\n# Payment schedule\npayment = \"1\" if \"ex_payment\" not in kwargs else str(kwargs[\"ex_payment\"])\nif payment not in [\"1\", \"12\", \"24\"]:\n    raise LinodeException(0xFB, \"Invalid subscription (1, 12, 24)\")\n\nssh = None\nroot = None\n# SSH key and/or root password\nif isinstance(auth, NodeAuthSSHKey):\n    ssh = auth.pubkey\nelif isinstance(auth, NodeAuthPassword):\n    root = auth.password\n\nif not ssh and not root:\n    raise LinodeException(0xFB, \"Need SSH key or root password\")\nif not root is None and len(root) < 6:\n    raise LinodeException(0xFB, \"Root password is too short\")\n\n# Swap size\ntry: swap = 128 if \"ex_swap\" not in kwargs else int(kwargs[\"ex_swap\"])\nexcept: raise LinodeException(0xFB, \"Need an integer swap size\")\n\n# Root partition size\nimagesize = (size.disk - swap) if \"ex_rsize\" not in kwargs else \\\n    int(kwargs[\"ex_rsize\"])\nif (imagesize + swap) > size.disk:\n    raise LinodeException(0xFB, \"Total disk images are too big\")\n\n# Distribution ID\ndistros = self.list_images()\nif image.id not in [d.id for d in distros]:\n    raise LinodeException(0xFB,\n                          \"Invalid distro -- avail.distributions\")\n\n# Kernel\nif \"ex_kernel\" in kwargs:\n    kernel = kwargs[\"ex_kernel\"]\nelse:\n    if image.extra['64bit']:\n        kernel = 111 if image.extra['pvops'] else 107\n    else:\n        kernel = 110 if image.extra['pvops'] else 60\nparams = { \"api_action\": \"avail.kernels\" }\nkernels = self.connection.request(LINODE_ROOT, params=params).object\nif kernel not in [z[\"KERNELID\"] for z in kernels]:\n    raise LinodeException(0xFB, \"Invalid kernel -- avail.kernels\")\n\n# Comments\ncomments = \"Created by libcloud <http://www.libcloud.org>\" if \\\n    \"ex_comment\" not in kwargs else kwargs[\"ex_comment\"]\n\n# Labels\nlabel = {\n    \"lconfig\": \"[%s] Configuration Profile\" % name,\n    \"lrecovery\": \"[%s] Finnix Recovery Configuration\" % name,\n    \"lroot\": \"[%s] %s Disk Image\" % (name, image.name),\n    \"lswap\": \"[%s] Swap Space\" % name\n}\nfor what in [\"lconfig\", \"lrecovery\", \"lroot\", \"lswap\"]:\n    if what in kwargs:\n        label[what] = kwargs[what]\n\n# Step 1: linode.create\nparams = {\n    \"api_action\":   \"linode.create\",\n    \"DatacenterID\": chosen,\n    \"PlanID\":       size.id,\n    \"PaymentTerm\":  payment\n}\ndata = self.connection.request(LINODE_ROOT, params=params).object\nlinode = { \"id\": data[\"LinodeID\"] }\n\n# Step 2: linode.disk.createfromdistribution\nif not root:\n    root = os.urandom(8).encode('hex')\nparams = {\n    \"api_action\":       \"linode.disk.createfromdistribution\",\n    \"LinodeID\":         linode[\"id\"],\n    \"DistributionID\":   image.id,\n    \"Label\":            label[\"lroot\"],\n    \"Size\":             imagesize,\n    \"rootPass\":         root,\n}\nif ssh: params[\"rootSSHKey\"] = ssh\ndata = self.connection.request(LINODE_ROOT, params=params).object\nlinode[\"rootimage\"] = data[\"DiskID\"]\n\n# Step 3: linode.disk.create for swap\nparams = {\n    \"api_action\":       \"linode.disk.create\",\n    \"LinodeID\":         linode[\"id\"],\n    \"Label\":            label[\"lswap\"],\n    \"Type\":             \"swap\",\n    \"Size\":             swap\n}\ndata = self.connection.request(LINODE_ROOT, params=params).object\nlinode[\"swapimage\"] = data[\"DiskID\"]\n\n# Step 4: linode.config.create for main profile\ndisks = \"%s,%s,,,,,,,\" % (linode[\"rootimage\"], linode[\"swapimage\"])\nparams = {\n    \"api_action\":       \"linode.config.create\",\n    \"LinodeID\":         linode[\"id\"],\n    \"KernelID\":         kernel,\n    \"Label\":            label[\"lconfig\"],\n    \"Comments\":         comments,\n    \"DiskList\":         disks\n}\ndata = self.connection.request(LINODE_ROOT, params=params).object\nlinode[\"config\"] = data[\"ConfigID\"]\n\n# TODO: Recovery image (Finnix)\n\n# Step 5: linode.boot\nparams = {\n    \"api_action\":       \"linode.boot\",\n    \"LinodeID\":         linode[\"id\"],\n    \"ConfigID\":         linode[\"config\"]\n}\ndata = self.connection.request(LINODE_ROOT, params=params).object\n\n# Make a node out of it and hand it back\nparams = { \"api_action\": \"linode.list\", \"LinodeID\": linode[\"id\"] }\ndata = self.connection.request(LINODE_ROOT, params=params).object\nreturn self._to_node(data[0])", "path": "libcloud\\drivers\\linode.py", "repo_name": "secondstory/dewpoint", "stars": 19, "license": "apache-2.0", "language": "python", "size": 433}
{"docstring": "\"\"\" Gets the confirmation page \"\"\"\n", "func_signal": "def getConfirmationPage (current_page):\n", "code": "form = getFormData (current_page, \"orderSummaryForm\")\nformdata = form.form_data\n\nsetFormField (formdata, 'orderSummaryForm:_idcl', 'orderSummaryForm:osCheckout')\n#next_url = urllib2.urlparse.urljoin (TEST_ROOT, form.form_action)\n#print next_url\n#newpage = getPage (next_url, formdata)\nnewpage = getPage (CHECKOUT_URL, formdata)\n#dumpPage (newpage)\nreturn newpage", "path": "pizza_py_party.py", "repo_name": "Ryochan7/pizza-py-party", "stars": 18, "license": "None", "language": "python", "size": 121}
{"docstring": "\"\"\" Generic function that makes requests for pages \"\"\"\n", "func_signal": "def getPage (url, data=None):\n", "code": "if data != None:\n    data = urlencode (data)\ntry:\n    req = urllib2.Request (url, data, USER_AGENT)\n    handle = urllib2.urlopen (req)\nexcept:\n    raise Exception (\"Could not get page %s.\" % url)\n\npage = handle.read ()\nhandle.close ()\nreturn page", "path": "pizza_py_party.py", "repo_name": "Ryochan7/pizza-py-party", "stars": 18, "license": "None", "language": "python", "size": 121}
{"docstring": "\"\"\" Merge attributes specified in the config file and any attributes\n    specified on the command line \"\"\"\n", "func_signal": "def mergeAttributes (parsed_conf, username, password, pizza):\n", "code": "if not username:\n    username = parsed_conf[\"username\"]\nif not password:\n    password = parsed_conf[\"password\"]\nif not pizza.crust:\n    pizza.setCrust(parsed_conf[\"default_crust\"])\nif not pizza.size:\n    pizza.setSize (parsed_conf[\"default_size\"])\nif not pizza.quantity:\n    pizza.setQuantity (parsed_conf[\"default_quantity\"])\nif not pizza.toppings:\n    for new_topping in parsed_conf[\"default_toppings\"]:\n        pizza.addTopping (new_topping)\nreturn username, password", "path": "pizza_py_party.py", "repo_name": "Ryochan7/pizza-py-party", "stars": 18, "license": "None", "language": "python", "size": 121}
{"docstring": "# If working outside of desired form, ignore\n", "func_signal": "def end_select (self):\n", "code": "if not self.getform:\n    return\n\n# If no option is selected (should not happen),\n# make first grabbed value the selected value\nif not self.option_value:\n    self.option_value = self.first_option_value", "path": "pizza_py_party.py", "repo_name": "Ryochan7/pizza-py-party", "stars": 18, "license": "None", "language": "python", "size": 121}
{"docstring": "# If working outside of list item, ignore\n", "func_signal": "def start_a (self, attrs):\n", "code": "if not self.getcoupon:\n    return\n\nfor item, value in attrs:\n    if item == 'onclick':\n        match = self.pattern.search (value)\n        if not match:\n            raise Exception (\"Necessary item 'couponCode' was not found on the page\")\n        temp = match.group (1)\n        self.id = temp\n    elif item == \"title\":\n        self.description = value", "path": "pizza_py_party.py", "repo_name": "Ryochan7/pizza-py-party", "stars": 18, "license": "None", "language": "python", "size": 121}
{"docstring": "\"\"\" Gets the page that holds the main form for specifying a pizza \"\"\"\n", "func_signal": "def startBuildPizza (current_page):\n", "code": "form = getFormData (current_page, \"choose_pizza\")\nformdata = form.form_data\n\nsetFormField (formdata, 'choose_pizza:_idcl', 'choose_pizza:goToBuildOwn')\n#next_url = urllib2.urlparse.urljoin (TEST_ROOT, form.form_action)\n#print next_url\n#newpage = getPage (next_url, formdata)\nnewpage = getPage (BUILD_PIZZA_URL, formdata)\n#dumpPage (newpage)\nreturn newpage", "path": "pizza_py_party.py", "repo_name": "Ryochan7/pizza-py-party", "stars": 18, "license": "None", "language": "python", "size": 121}
{"docstring": "\"\"\" Add a topping to a pizza \"\"\"\n", "func_signal": "def addTopping (self, topping):\n", "code": "if not isinstance(topping, Topping):\n    if topping in short2topping:\n        topping = short2topping[topping]\n    elif topping in long2topping:\n        topping = long2topping[topping]\n    else:\n        print >> sys.stderr, \"'%s' is not a valid topping choice. Exiting.\" % topping\n        sys.exit (42)\nself.order[topping.long_name] = ['W', '1']\nself.toppings.add (topping)", "path": "pizza_py_party.py", "repo_name": "Ryochan7/pizza-py-party", "stars": 18, "license": "None", "language": "python", "size": 121}
{"docstring": "# If working outside of desired form, ignore\n", "func_signal": "def do_option (self, attrs):\n", "code": "if not self.getform:\n    return\n\nattrs = dict(attrs)\nvalue = attrs.get(\"value\", None)\nif \"selected\" in attrs:\n    if value is None:\n        raise Exception(\"TODO: Support options w/o value attrs\")\n    self.option_value = value\n\nif not self.first_option_value:\n    self.first_option_value = value\n#self.last_option_value = value", "path": "pizza_py_party.py", "repo_name": "Ryochan7/pizza-py-party", "stars": 18, "license": "None", "language": "python", "size": 121}
{"docstring": "# If working outside of main coupon display, ignore\n", "func_signal": "def start_li (self, attrs):\n", "code": "if not self.in_coupon_menu or not self.in_ul:\n    return\n\nfor item, value in attrs:\n    if item == \"class\" and value == \"coupon-item\" or value == \"coupon-item first-item\":\n        self.getcoupon = True # Only set True when in \"menutype seeall\" div scope", "path": "pizza_py_party.py", "repo_name": "Ryochan7/pizza-py-party", "stars": 18, "license": "None", "language": "python", "size": 121}
{"docstring": "\"\"\"Pretty print a dictionary.\"\"\"\n", "func_signal": "def pretty_print(d):\n", "code": "return \"{\\n%s\\n}\" % \"\\n\".join(\"    %r: %r,\" % (k, v)\n                              for k, v in sorted(d.items()))", "path": "pizza_py_party.py", "repo_name": "Ryochan7/pizza-py-party", "stars": 18, "license": "None", "language": "python", "size": 121}
{"docstring": "# If working outside of main coupon display, ignore\n", "func_signal": "def end_div (self):\n", "code": "if not self.in_coupon_menu:\n    return\n\n# Get price from text buffer\nif self.type == \"coupon-price\":\n    text = self.save_end ()\n    self.price = text.decode (\"utf8\", \"ignore\")\n    self.type = \"\"", "path": "pizza_py_party.py", "repo_name": "Ryochan7/pizza-py-party", "stars": 18, "license": "None", "language": "python", "size": 121}
{"docstring": "\"\"\" Calls the main Parser class to obtain the form data of a page \"\"\"\n", "func_signal": "def getFormData (scan_page, target_form):\n", "code": "parsed_form = Parser (target_form)\n#try:\nparsed_form.feed (scan_page)\n#except Exception:\n#    dumpPage (scan_page)\nparsed_form.close ()", "path": "pizza_py_party.py", "repo_name": "Ryochan7/pizza-py-party", "stars": 18, "license": "None", "language": "python", "size": 121}
{"docstring": "\"\"\" Gets the coupon page \"\"\"\n", "func_signal": "def getCouponsPage (current_page):\n", "code": "form = getFormData (current_page, \"choose_pizza\")\nformdata = form.form_data\n\nsetFormField (formdata, 'choose_pizza:_idcl', 'choose_pizza:couponsButton')\n# Needed for me since no Domino's store delivers to me\n#setFormField (formdata, 'startOrder:deliveryOrPickup', 'Pickup')\n\n#next_url = urllib2.urlparse.urljoin (TEST_ROOT, form.form_action)\n#newpage = getPage (next_url, formdata)\nnewpage = getPage (ADD_COUPON_URL, formdata)\nstoreClosed (newpage)\nreturn newpage", "path": "pizza_py_party.py", "repo_name": "Ryochan7/pizza-py-party", "stars": 18, "license": "None", "language": "python", "size": 121}
{"docstring": "# If working outside of desired form, ignore\n", "func_signal": "def start_select (self, attrs):\n", "code": "if not self.getform:\n    return\n\nfor item, value in attrs:\n    if item == \"name\":\n        self.select_name = value", "path": "pizza_py_party.py", "repo_name": "Ryochan7/pizza-py-party", "stars": 18, "license": "None", "language": "python", "size": 121}
{"docstring": "\"\"\" Check that a particular entry exists in the form data\n    and updates the entry with the value passed \"\"\"\n", "func_signal": "def setFormField (current_data, name, new_value):\n", "code": "if not name in current_data:\n    raise Exception (\"Necessary item \\\"%s\\\" was not found in the form.\\n\"\n                     \"Current form data: %s\" %\n                     (name, pretty_print(current_data)))\n\ncurrent_data.update ({name: new_value})", "path": "pizza_py_party.py", "repo_name": "Ryochan7/pizza-py-party", "stars": 18, "license": "None", "language": "python", "size": 121}
{"docstring": "\"\"\" Add the coupon code to the user's order \"\"\"\n", "func_signal": "def addCoupon (current_page, coupon, coupon_data):\n", "code": "form = getFormData (current_page, \"couponsForm\")\nformdata = form.form_data\n\n# Take the coupon_data list, place the coupon ids in a temporary list,\n# and find out if the coupon code specified is valid\ntemp = []\nfor id, desc, price in coupon_data:\n    temp.append (id)\nif not coupon in temp:\n    raise Exception (\"'%s' is not a valid coupon code.\" % coupon)\n\nsetFormField (formdata, 'couponCode', coupon)\nsetFormField (formdata, 'couponsForm:_idcl', 'couponsForm:addCouponLink')\n\nnewpage = getPage (ADD_COUPON_URL, formdata)\n#next_url = urllib2.urlparse.urljoin (TEST_ROOT, form.form_action)\n#newpage = getPage (next_url, formdata)\nreturn newpage", "path": "pizza_py_party.py", "repo_name": "Ryochan7/pizza-py-party", "stars": 18, "license": "None", "language": "python", "size": 121}
{"docstring": "# If working outside of desired form, ignore\n", "func_signal": "def do_input (self, attrs):\n", "code": "if not self.getform:\n    return\n\nname = tmp_value = None\nradio_field = checked = False\nfor item, value in attrs:\n    if item == \"name\":\n        name = value\n    elif item == \"value\":\n        tmp_value = value\n    if item == \"type\" and value == \"radio\":\n        radio_field = True\n    elif item == \"checked\":\n        checked = True\n\nif name and tmp_value != None:\n    # Check if a radio field needs to be updated\n    if radio_field and checked:\n        self.formdata.update ({name: tmp_value})\n    # Update non-radio fields\n    elif not radio_field:\n        self.formdata.update ({name: tmp_value})\n# Include any input with no specified value\n# (in some needed hidden fields). Ex. login:_idcl\nelif name:\n    self.formdata.update ({name: \"\"})", "path": "pizza_py_party.py", "repo_name": "Ryochan7/pizza-py-party", "stars": 18, "license": "None", "language": "python", "size": 121}
{"docstring": "\"\"\" Gets the total for an order. Needed to run prior to\n    going to the confirmation page or the total cannot be obtained\n    due to the use of AJAX \"\"\"\n", "func_signal": "def calculateTotal ():\n", "code": "newpage = getPage (CALCULATE_TOTAL_URL, CALCULATE_TOTAL_URL_POST_VARS)\n#dumpPage (newpage)\na = dom.parseString (newpage)\norder_total = a.getElementsByTagName ('total')[0].firstChild.data\nreturn order_total", "path": "pizza_py_party.py", "repo_name": "Ryochan7/pizza-py-party", "stars": 18, "license": "None", "language": "python", "size": 121}
{"docstring": "\"\"\" Find the first attribute that is blank, print what the missing\n    attribute is, and then exit \"\"\"\n", "func_signal": "def findMissingAttributes (data_list):\n", "code": "index = data_list.index ('')\n\nfailed_value = ''\nif index == 0:\n    failed_value = 'username'\nelif index == 1:\n    failed_value = 'password'\nelif index == 2:\n    failed_value = 'crust'\nelif index == 3:\n    failed_value = 'size'\nelif index == 4:\n    failed_value = 'quantity'\nprint >> sys.stderr, \"A value for '%s' was not specified. Exiting.\" % failed_value\nsys.exit (42)", "path": "pizza_py_party.py", "repo_name": "Ryochan7/pizza-py-party", "stars": 18, "license": "None", "language": "python", "size": 121}
{"docstring": "\"\"\" Add the user's custom pizza onto the order \"\"\"\n", "func_signal": "def addPizza (current_page, pizza, check_coupon=''):\n", "code": "form = getFormData (current_page, \"build_own\")\nformdata = form.form_data\n\n# Delete any unknown toppings\nformdata_temp = formdata.copy ()\ntopping_re = re.compile (r\"topping(?!Side|Amount).*\")\ntemp_topping_keys = [t.cryptic_name for t in TOPPINGS]\nfor key, value in formdata_temp.iteritems ():\n    # If key is cheese or sauce, skip\n    if key == TOPPING_CHEESE or key == TOPPPING_SAUCE:\n        continue\n    elif topping_re.match (key) and key not in temp_topping_keys:\n        del formdata[key]\ndel formdata_temp\n\nfor topping in TOPPINGS:\n    if topping in pizza.toppings:\n        # Fill field with topping settings.\n        setFormField (formdata, topping.cryptic_num,\n                      pizza.order[topping.long_name][0])\n    else:\n        # Unused toppings must be removed from form data\n        topping_field = topping.cryptic_name\n        if topping_field in formdata:\n            del formdata[topping_field]\n\nsetFormField (formdata, \"builderCrust\", pizza.order[\"crust\"])\nsetFormField (formdata, \"builderSize\", pizza.order[\"size\"])\nsetFormField (formdata, \"builderQuantity\", pizza.order[\"quantity\"])\nsetFormField (formdata, \"build_own:_idcl\", \"build_own:doAdd\")\n#print formdata\n\nnewpage = getPage (ADD_PIZZA_URL, formdata)\n#next_url = urllib2.urlparse.urljoin (TEST_ROOT, form.form_action)\n#print next_url\n#newpage = getPage (next_url, formdata)\n#dumpPage (newpage)\nreturn newpage", "path": "pizza_py_party.py", "repo_name": "Ryochan7/pizza-py-party", "stars": 18, "license": "None", "language": "python", "size": 121}
{"docstring": "'''Adds a component to the component dictionary. A\nActorDuplicateComponent exception will be raised if a component of the\nsame type already exitsts.\n'''\n", "func_signal": "def add_component(self, component):\n", "code": "t = component.component_type\n\nif t in self.components:\n    raise ActorException('Cannot add duplicate component of \\\n            type %s to actor %s' % (t, self.name))\n\n# Add new component\nself.components[t] = component\ncomponent.attach(self)", "path": "game\\actor\\actor.py", "repo_name": "davexunit/PyPlatformer", "stars": 17, "license": "None", "language": "python", "size": 962}
{"docstring": "'''Loads Tiled object XML.\nMap height and tile height need to be passed because OpenGL uses the\nbottom-left of the screen as the origin where as Tiled uses the upper-left.\nThis means that we have to invert the Y coordinate.\n'''\n# Get object properties\n", "func_signal": "def load_object(tag, tiledmap):\n", "code": "properties = dict()\n\n# Every tiled object has these properties\nproperties['name'] = tag.get('name')\nproperties['type'] = tag.get('type')\nproperties['x'] = int(tag.get('x'))\nproperties['y'] = height * tiledmap.tile_height - int(tag.get('y')) - \\\n                    tiledmap.tile_height\nproperties['width'] = int(tag.get('width'))\nproperties['height'] = int(tag.get('height'))\n\n# Read custom properties\nfor p in tag.find('properties'):\n    properties[p.get('name')] = p.get('value')\n\n# Load object\nobj = factories[actor_type](properties)\n\nreturn obj", "path": "game\\tiled\\tiled.py", "repo_name": "davexunit/PyPlatformer", "stars": 17, "license": "None", "language": "python", "size": 962}
{"docstring": "# Avoid not-a-number errors\n", "func_signal": "def on_move(self, x, y, rel_x, rel_y):\n", "code": "if math.isnan(x) or math.isnan(y):\n    return\n\nself.sprite.position = (x, y)", "path": "game\\components.py", "repo_name": "davexunit/PyPlatformer", "stars": 17, "license": "None", "language": "python", "size": 962}
{"docstring": "# Get image properties\n", "func_signal": "def load_image(tag):\n", "code": "source = tag.get('source')\nwidth = int(tag.get('width'))\nheight = int(tag.get('height'))\nreturn pyglet.resource.image(source), width, height", "path": "game\\tiled\\tiled.py", "repo_name": "davexunit/PyPlatformer", "stars": 17, "license": "None", "language": "python", "size": 962}
{"docstring": "'''Checks for the existence of given component of given component_type\nand optionally by class instance.\n'''\n", "func_signal": "def has_component(self, component_type, component_class=None):\n", "code": "exists = component_type in self.components\n\nif exists and component_class != None:\n    return isinstance(self.components[component_type], component_class)\n\nreturn exists", "path": "game\\actor\\actor.py", "repo_name": "davexunit/PyPlatformer", "stars": 17, "license": "None", "language": "python", "size": 962}
{"docstring": "'''Sets the owner of this component to the given actor. An exception\nwill be raised if this component is already owned by someone else.\n'''\n", "func_signal": "def attach(self, actor):\n", "code": "if self.owner != None:\n    raise Exception('Component of type %s already has an owner' %\n            self.type)\nself.owner = actor", "path": "game\\actor\\component.py", "repo_name": "davexunit/PyPlatformer", "stars": 17, "license": "None", "language": "python", "size": 962}
{"docstring": "'''Enforces inter-component dependencies. Returns instance to component\nif it exists. An ActorException is raised if a component is not found.\n'''\n", "func_signal": "def require(self, component_type, component_class=None):\n", "code": "if not self.has_component(component_type, component_class):\n    error_message = 'Actor %s is missing dependency: %s' % (self.name, component_type)\n    if component_class != None:\n        error_message += ' of type %s' % component_clas.__name__\n\n    raise ActorException(error_message)\n\nreturn self.components[component_type]", "path": "game\\actor\\actor.py", "repo_name": "davexunit/PyPlatformer", "stars": 17, "license": "None", "language": "python", "size": 962}
{"docstring": "# Open xml file\n", "func_signal": "def load_animset(filename):\n", "code": "root = ElementTree.parse(filename).getroot()\nif root.tag != 'animset':\n    raise MapException('Expected <animset> tag, found <%s> tag' % root.tag)\n\n# Get animset properties\nimage = pyglet.resource.image('anims/' + root.get('image'))\ntile_width = int(root.get('tilewidth'))\ntile_height = int(root.get('tileheight'))\n\n# Create image sequence of tiles\ngrid = pyglet.image.ImageGrid(image, image.width / tile_width, image.height / tile_height)\nsequence = grid.get_texture_sequence()\nanims = AnimSet()\n\n# Loop through all animations\nfor child in root.findall('anim'):\n    anim_name = child.get('name')\n    anim_duration = float(child.get('duration'))\n    frame_indices = [int(x) for x in child.text.split(',')]\n    frames = list()\n    for f in frame_indices:\n        frames.append(sequence[f])\n    anims[anim_name] = pyglet.image.Animation.from_image_sequence(frames, anim_duration, loop=True)\nreturn anims", "path": "game\\util\\anim.py", "repo_name": "davexunit/PyPlatformer", "stars": 17, "license": "None", "language": "python", "size": 962}
{"docstring": "# Get tileset properties\n", "func_signal": "def load_tileset(tag):\n", "code": "firstgid = int(tag.get('firstgid'))\nname = tag.get('name')\ntile_width = int(tag.get('tilewidth'))\ntile_height = int(tag.get('tileheight'))\nspacing = int(tag.get('spacing', 0))\nmargin = int(tag.get('margin', 0))\n\nchild = tag.find('image')\n# Raise an exception if child tag is not <image>\nif child.tag != 'image':\n    raise MapException('Unsupported tag in tileset: %s' % child.tag)\n# Load image\nimage, image_width, image_height  = load_image(child)\n\n# Construct tileset\ntileset = TileSet()\nfor y in range(margin, image_height - tile_height - spacing, tile_height + spacing):\n    for x in range(margin, image_width - spacing, tile_width + spacing):\n        tile = image.get_region(x, image_height - y - tile_height, tile_width, tile_height)\n        tileset.append(cocos.tiles.Tile(y * (image_width / tile_width) + x, None, tile))\n        # set texture clamping to avoid mis-rendering subpixel edges\n        # Borrowed from cocos2d sources - tiles.py\n        tile.texture.id\n        glBindTexture(tile.texture.target, tile.texture.id)\n        glTexParameteri(tile.texture.target, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE)\n        glTexParameteri(tile.texture.target, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE)\nreturn tileset", "path": "game\\tiled\\tiled.py", "repo_name": "davexunit/PyPlatformer", "stars": 17, "license": "None", "language": "python", "size": 962}
{"docstring": "# Open xml file\n", "func_signal": "def from_xml(filename):\n", "code": "tree = ElementTree.parse(filename)\nroot = tree.getroot()\n\n# Root level tag is expected to be <geometry> so raise an exception if it's not\nif root.tag != 'physics':\n    raise Exception('%s root level tag is %s rather than <physics>' % (filename, root.tag))\n\nphysics = Physics()\n\nfor p in root.findall('polygon'):\n    vertices = []\n    for v in p.findall('vertex'):\n        vertices.append((int(v.get('x')), int(v.get('y'))))\n    physics.space.add(make_static_polygon(vertices))\n\nreturn physics", "path": "game\\physics.py", "repo_name": "davexunit/PyPlatformer", "stars": 17, "license": "None", "language": "python", "size": 962}
{"docstring": "'''Returns a list of all actors whose bounding boxes intersect with the\ngiven rectangle.\n'''\n", "func_signal": "def get_in_region(self, rect):\n", "code": "actors = weakref.WeakSet()\nfor a in self.actors.values():\n    actor_rect = cocos.rect.Rect(a.x, a.y, a.width, a.height)\n    if actor_rect.intersects(rect):\n        actors.add(a)\nreturn actors", "path": "game\\actor\\actorlayer.py", "repo_name": "davexunit/PyPlatformer", "stars": 17, "license": "None", "language": "python", "size": 962}
{"docstring": "'''Refreshing the components gives each component the chance to hook\ninto the events of other components that belong to the Actor.\nYou should call this once during the initial creation of the Actor,\nafter all of the components have been added.\nIf you add/remove components later, make sure to refresh.\n'''\n", "func_signal": "def refresh_components(self):\n", "code": "for component in self.components.values():\n    component.on_refresh()", "path": "game\\actor\\actor.py", "repo_name": "davexunit/PyPlatformer", "stars": 17, "license": "None", "language": "python", "size": 962}
{"docstring": "# Get layer properties\n", "func_signal": "def load_layer(tag, tiledmap):\n", "code": "name = tag.get('name')\nwidth = int(tag.get('width'))\nheight = int(tag.get('height'))\n\nchild = tag.find('data')\n# Raise exception if there is no <data> tag because that's fucked up\nif child == None:\n    raise MapException('No <data> tag in layer')\n\n# Load layer data\ndata = load_data(child)\n# Construct layer\ncolumns = []\nfor i in range(0, width):\n    row = []\n    columns.append(row)\n    for j in range(0, height):\n        index = j * width + i\n        tile = tiledmap.tileset[data[index] - 1]\n        if data[index] == 0:\n            tile = None\n        row.insert(0, cocos.tiles.RectCell(i, height - j - 1,\n            tiledmap.tile_width, tiledmap.tile_height, None, tile))\n\nreturn cocos.tiles.RectMapLayer(name, tiledmap.tile_width, \n                                tiledmap.tile_height, columns, (0,0,0),\n                                None)", "path": "game\\tiled\\tiled.py", "repo_name": "davexunit/PyPlatformer", "stars": 17, "license": "None", "language": "python", "size": 962}
{"docstring": "# Open xml file\n", "func_signal": "def load_map(filename):\n", "code": "tree = ElementTree.parse(filename)\nroot = tree.getroot()\n\n# Root level tag is expected to be <map> so raise an exception if it's not\nif root.tag != 'map':\n    raise MapException('%s root level tag is %s rather than <map>' % (filename, root.tag))\n# We can only load orthogonal maps here because that's all I care about :P\nif root.get('orientation') != 'orthogonal':\n    raise MapException('Map orientation %s not supported. Orthogonal maps only' % root.get('orientation'))\n\n# Initialize map\ntiledmap = TiledMap()\ntiledmap.orientation = root.get('orientation')\ntiledmap.version = root.get('version')\ntiledmap.width = int(root.get('width'))\ntiledmap.height = int(root.get('height'))\ntiledmap.tile_width = int(root.get('tilewidth'))\ntiledmap.tile_height = int(root.get('tileheight'))\n\n# Load map properties\nproperties = root.find('properties')\nfor p in properties.findall('property'):\n    tiledmap.properties[p.get('name')] = p.get('value')\n\n# Load tilesets\nfor tag in root.findall('tileset'):\n    tiledmap.tileset += load_tileset(tag)\n\n# Load layers\nfor tag in root.findall('layer'):\n    layer = load_layer(tag, tiledmap)\n    tiledmap.layers[layer.id] = layer\n\n# Load object layers\nfor tag in root.findall('objectgroup'):\n    layer = load_object_group(tag, tiledmap)\n    tiledmap.object_groups[layer.name] = layer\n\nreturn tiledmap", "path": "game\\tiled\\tiled.py", "repo_name": "davexunit/PyPlatformer", "stars": 17, "license": "None", "language": "python", "size": 962}
{"docstring": "'''Detaches component of the given type and calls the necessary\nclean-up routines. A KeyError will be raised if a component of given\ntype is not attached.\n'''\n", "func_signal": "def remove_component(self, component_type):\n", "code": "del self.components[component_type]\ncomponent.detach()", "path": "game\\actor\\actor.py", "repo_name": "davexunit/PyPlatformer", "stars": 17, "license": "None", "language": "python", "size": 962}
{"docstring": "# Scroll map with WASD\n", "func_signal": "def _step(self, dt):\n", "code": "move_inc = 20\ndx = (self.keys[pyglet.window.key.D] - self.keys[pyglet.window.key.A]) * move_inc\ndy = (self.keys[pyglet.window.key.W] - self.keys[pyglet.window.key.S]) * move_inc\nscroller = self.parent.scroller\nscroller.set_focus(scroller.fx + dx, scroller.fy + dy)", "path": "game\\editor.py", "repo_name": "davexunit/PyPlatformer", "stars": 17, "license": "None", "language": "python", "size": 962}
{"docstring": "# Avoid not-a-number errors\n", "func_signal": "def on_rotate(self, angle):\n", "code": "if math.isnan(angle):\n    return\n\n# Convert that shit to degrees because chipmunk uses radians\nself.sprite.rotation = math.degrees(angle)", "path": "game\\components.py", "repo_name": "davexunit/PyPlatformer", "stars": 17, "license": "None", "language": "python", "size": 962}
{"docstring": "# Get data properties\n", "func_signal": "def load_data(tag):\n", "code": "encoding = tag.get('encoding')\ncompression = tag.get('compression')\ndata = tag.text\n\n# Only base64 encoding supported as of now\nif encoding != 'base64':\n    raise MapException('Encoding type %s not supported' % encoding)\n# Only zlib compression supported as of now\nif compression != 'zlib':\n    raise MapException('Compression type %s not supported' % compression)\n\n# Uncompress data\ndecoded_data = zlib.decompress(base64.b64decode(data))\n\n# decoded_data is a string made of 64 bit integers now\n# Turn that string into an array of 64 bit integers\n# TODO: Use encoding 'L' or 'I' accordingly based upon CPU architecture.\n#       32 bit = 'L' and 64 bit = 'I'\n\nreturn array.array('I', decoded_data)", "path": "game\\tiled\\tiled.py", "repo_name": "davexunit/PyPlatformer", "stars": 17, "license": "None", "language": "python", "size": 962}
{"docstring": "'''This method is to be called only by the Actor class. If you are\nto call this method manually, you must also manually remove the\ncomponent from the Actor's dictionary. In other words, don't do\nthis. Use Actor.remove_component instead.\n'''\n", "func_signal": "def detach(self):\n", "code": "self.owner = None\nself.on_detach()", "path": "game\\actor\\component.py", "repo_name": "davexunit/PyPlatformer", "stars": 17, "license": "None", "language": "python", "size": 962}
{"docstring": "'''Handles characters that jump and land on static geometry.\n'''\n", "func_signal": "def on_character_jump_land(self, space, arbiter):\n", "code": "debug.msg(\"%s, %s\" % (arbiter.contacts[0].normal, len(arbiter.contacts)))\n\n#if arbiter.contacts[0].normal.dot(arbiter.contacts[1].normal) > 0:\nactor = arbiter.shapes[1].actor()\nphysics = actor.get_component('physics')\nphysics.on_jump_land()\nreturn True", "path": "game\\physics.py", "repo_name": "davexunit/PyPlatformer", "stars": 17, "license": "None", "language": "python", "size": 962}
{"docstring": "\"\"\"\nAn undefined name warning is emitted if the subscript used as the\ntarget of a C{with} statement is not defined.\n\"\"\"\n", "func_signal": "def test_withStatementSubscriptUndefined(self):\n", "code": "self.flakes('''\nfrom __future__ import with_statement\nimport foo\nwith open('foo') as foo[bar]:\n    pass\n''', m.UndefinedName)", "path": "ftplugin\\python\\pyflakes\\pyflakes\\test\\test_other.py", "repo_name": "mitechie/pyflakes-pathogen", "stars": 18, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"\nDon't warn when a variable in a list comprehension is assigned to but\nnot used.\n\"\"\"\n", "func_signal": "def test_assignInListComprehension(self):\n", "code": "self.flakes('''\ndef f():\n    [None for i in range(10)]\n''')", "path": "ftplugin\\python\\pyflakes\\pyflakes\\test\\test_other.py", "repo_name": "mitechie/pyflakes-pathogen", "stars": 18, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"\nIf a class name is used in the body of that class's definition and\nthe name is not already defined, a warning is emitted.\n\"\"\"\n", "func_signal": "def test_classNameUndefinedInClassBody(self):\n", "code": "self.flakes('''\nclass foo:\n    foo\n''', m.UndefinedName)", "path": "ftplugin\\python\\pyflakes\\pyflakes\\test\\test_other.py", "repo_name": "mitechie/pyflakes-pathogen", "stars": 18, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"\nbreak and continue statements are supported.\n\"\"\"\n", "func_signal": "def test_loopControl(self):\n", "code": "self.flakes('''\nfor x in [1, 2]:\n    break\n''')\nself.flakes('''\nfor x in [1, 2]:\n    continue\n''')", "path": "ftplugin\\python\\pyflakes\\pyflakes\\test\\test_other.py", "repo_name": "mitechie/pyflakes-pathogen", "stars": 18, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"\nIf the target of a C{with} statement uses any or all of the valid forms\nfor that part of the grammar (See\nU{http://docs.python.org/reference/compound_stmts.html#the-with-statement}),\nthe names involved are checked both for definedness and any bindings\ncreated are respected in the suite of the statement and afterwards.\n\"\"\"\n", "func_signal": "def test_withStatementComplicatedTarget(self):\n", "code": "self.flakes('''\nfrom __future__ import with_statement\nc = d = e = g = h = i = None\nwith open('foo') as [(a, b), c[d], e.f, g[h:i]]:\n    a, b, c, d, e, g, h, i\na, b, c, d, e, g, h, i\n''')", "path": "ftplugin\\python\\pyflakes\\pyflakes\\test\\test_other.py", "repo_name": "mitechie/pyflakes-pathogen", "stars": 18, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"\nNo warnings are emitted for using any of the list of names defined by a\nC{with} statement within the suite or afterwards.\n\"\"\"\n", "func_signal": "def test_withStatementListNames(self):\n", "code": "self.flakes('''\nfrom __future__ import with_statement\nwith open('foo') as [bar, baz]:\n    bar, baz\nbar, baz\n''')", "path": "ftplugin\\python\\pyflakes\\pyflakes\\test\\test_other.py", "repo_name": "mitechie/pyflakes-pathogen", "stars": 18, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"\nIf a name in the base list of a class definition is undefined, a\nwarning is emitted.\n\"\"\"\n", "func_signal": "def test_undefinedBaseClass(self):\n", "code": "self.flakes('''\nclass foo(foo):\n    pass\n''', m.UndefinedName)", "path": "ftplugin\\python\\pyflakes\\pyflakes\\test\\test_other.py", "repo_name": "mitechie/pyflakes-pathogen", "stars": 18, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"\nAssigning to a member of another object and then not using that member\nvariable is perfectly acceptable. Do not mistake it for an unused\nlocal variable.\n\"\"\"\n# XXX: Adding this test didn't generate a failure. Maybe not\n# necessary?\n", "func_signal": "def test_assignToMember(self):\n", "code": "self.flakes('''\nclass b:\n    pass\ndef a():\n    b.foo = 1\n''')", "path": "ftplugin\\python\\pyflakes\\pyflakes\\test\\test_other.py", "repo_name": "mitechie/pyflakes-pathogen", "stars": 18, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"\nTest that shadowing a function definition with a decorated version of\nthat function does not raise a warning.\n\"\"\"\n", "func_signal": "def test_functionDecorator(self):\n", "code": "self.flakes('''\nfrom somewhere import somedecorator\n\ndef a(): pass\na = somedecorator(a)\n''')", "path": "ftplugin\\python\\pyflakes\\pyflakes\\test\\test_other.py", "repo_name": "mitechie/pyflakes-pathogen", "stars": 18, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"\nIf a defined name is used on either side of a containment test, no\nwarning is emitted.\n\"\"\"\n", "func_signal": "def test_containment(self):\n", "code": "self.flakes('''\nx = 10\ny = 20\nx in y\nx not in y\n''')", "path": "ftplugin\\python\\pyflakes\\pyflakes\\test\\test_other.py", "repo_name": "mitechie/pyflakes-pathogen", "stars": 18, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"\nA name defined in the body of a C{with} statement can be used after\nthe body ends without warning.\n\"\"\"\n", "func_signal": "def test_withStatementNameDefinedInBody(self):\n", "code": "self.flakes('''\nfrom __future__ import with_statement\nwith open('foo') as bar:\n    baz = 10\nbaz\n''')", "path": "ftplugin\\python\\pyflakes\\pyflakes\\test\\test_other.py", "repo_name": "mitechie/pyflakes-pathogen", "stars": 18, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"\nCheck names used in a class definition, including its decorators, base\nclasses, and the body of its definition.  Additionally, add its name to\nthe current scope.\n\"\"\"\n# decorator_list is present as of Python 2.6\n", "func_signal": "def CLASSDEF(self, node):\n", "code": "for deco in getattr(node, 'decorator_list', []):\n    self.handleNode(deco, node)\nfor baseNode in node.bases:\n    self.handleNode(baseNode, node)\nself.pushClassScope()\nfor stmt in node.body:\n    self.handleNode(stmt, node)\nself.popScope()\nself.addBinding(node, Binding(node.name, node))", "path": "ftplugin\\python\\pyflakes\\pyflakes\\checker.py", "repo_name": "mitechie/pyflakes-pathogen", "stars": 18, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"\nCheck to see if any assignments have not been used.\n\"\"\"\n", "func_signal": "def checkUnusedAssignments():\n", "code": "for name, binding in self.scope.iteritems():\n    if (not binding.used and not name in self.scope.globals\n        and isinstance(binding, Assignment)):\n        self.report(messages.UnusedVariable,\n                    binding.source, name)", "path": "ftplugin\\python\\pyflakes\\pyflakes\\checker.py", "repo_name": "mitechie/pyflakes-pathogen", "stars": 18, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"\nDon't warn when the assignment is used in an inner function, even if\nthat inner function itself is in an inner function.\n\"\"\"\n", "func_signal": "def test_doubleClosedOver(self):\n", "code": "self.flakes('''\ndef barMaker():\n    foo = 5\n    def bar():\n        def baz():\n            return foo\n    return bar\n''')", "path": "ftplugin\\python\\pyflakes\\pyflakes\\test\\test_other.py", "repo_name": "mitechie/pyflakes-pathogen", "stars": 18, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"\nReturn a list of the names referenced by this binding.\n\"\"\"\n", "func_signal": "def names(self):\n", "code": "names = []\nif isinstance(self.source, _ast.List):\n    for node in self.source.elts:\n        if isinstance(node, _ast.Str):\n            names.append(node.s)\nreturn names", "path": "ftplugin\\python\\pyflakes\\pyflakes\\checker.py", "repo_name": "mitechie/pyflakes-pathogen", "stars": 18, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"\nDon't warn when a variable in a for loop is assigned to but not used.\n\"\"\"\n", "func_signal": "def test_assignInForLoop(self):\n", "code": "self.flakes('''\ndef f():\n    for i in range(10):\n        pass\n''')", "path": "ftplugin\\python\\pyflakes\\pyflakes\\test\\test_other.py", "repo_name": "mitechie/pyflakes-pathogen", "stars": 18, "license": "None", "language": "python", "size": 137}
{"docstring": "# the decorators attribute is called decorator_list as of Python 2.6\n", "func_signal": "def FUNCTIONDEF(self, node):\n", "code": "if hasattr(node, 'decorators'):\n    for deco in node.decorators:\n        self.handleNode(deco, node)\nelse:\n    for deco in node.decorator_list:\n        self.handleNode(deco, node)\nself.addBinding(node, FunctionDefinition(node.name, node))\nself.LAMBDA(node)", "path": "ftplugin\\python\\pyflakes\\pyflakes\\checker.py", "repo_name": "mitechie/pyflakes-pathogen", "stars": 18, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"\nExtended slices are supported.\n\"\"\"\n", "func_signal": "def test_extendedSlice(self):\n", "code": "self.flakes('''\nx = 3\n[1, 2][x,:]\n''')", "path": "ftplugin\\python\\pyflakes\\pyflakes\\test\\test_other.py", "repo_name": "mitechie/pyflakes-pathogen", "stars": 18, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"\nProcess bindings for loop variables.\n\"\"\"\n", "func_signal": "def FOR(self, node):\n", "code": "vars = []\ndef collectLoopVars(n):\n    if isinstance(n, _ast.Name):\n        vars.append(n.id)\n    elif isinstance(n, _ast.expr_context):\n        return\n    else:\n        for c in iter_child_nodes(n):\n            collectLoopVars(c)\n\ncollectLoopVars(node.target)\nfor varn in vars:\n    if (isinstance(self.scope.get(varn), Importation)\n            # unused ones will get an unused import warning\n            and self.scope[varn].used):\n        self.report(messages.ImportShadowedByLoopVar,\n                    node, varn, self.scope[varn].source)\n\nself.handleChildren(node)", "path": "ftplugin\\python\\pyflakes\\pyflakes\\checker.py", "repo_name": "mitechie/pyflakes-pathogen", "stars": 18, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"\nSet comprehensions are properly handled.\n\"\"\"\n", "func_signal": "def test_setComprehensionAndLiteral(self):\n", "code": "self.flakes('''\na = {1, 2, 3}\nb = {x for x in range(10)}\n''')", "path": "ftplugin\\python\\pyflakes\\pyflakes\\test\\test_other.py", "repo_name": "mitechie/pyflakes-pathogen", "stars": 18, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"\nReturns a list of tuples for each of the fields specified.  This\nworks by wrapping the SQLAlchemyQuerySet in a \nSQLAlchemyValuesListQuerySet that modifies the iterator behavior.\nThe flat option is only available with one column and flattens\nout the tuples into a simple list.\n\"\"\"\n", "func_signal": "def values_list(self, *fields, **kwargs):\n", "code": "flat = kwargs.pop('flat', False)\nif kwargs:\n    raise TypeError('Unexpected keyword arguments to values_list: %s'\n            % (kwargs.keys(),))\nif flat and len(fields) > 1:\n    raise TypeError(\"'flat' is not valid when values_list is called with more than one field.\")\nfrom django_sqlalchemy.models.query import SQLAlchemyValuesListQuerySet\nreturn self._clone(klass=SQLAlchemyValuesListQuerySet, setup=True, flat=flat,\n        _fields=fields)", "path": "django_sqlalchemy\\models\\query.py", "repo_name": "brosner/django-sqlalchemy", "stars": 17, "license": "bsd-3-clause", "language": "python", "size": 464}
{"docstring": "\"\"\"\nSave the current instance. We force a flush so it mimics Django's \nbehavior.\n\"\"\"\n", "func_signal": "def save(self):\n", "code": "obj = session.add(self)\nsession.commit()\nreturn obj", "path": "django_sqlalchemy\\models\\base.py", "repo_name": "brosner/django-sqlalchemy", "stars": 17, "license": "bsd-3-clause", "language": "python", "size": 464}
{"docstring": "\"\"\"\nTODO:add support for related fields\nParse the order_by clause and return the modified query. \nThis does not consider related tables at this time.\n\"\"\"\n", "func_signal": "def parse_order_by(queryset, *field_names):\n", "code": "for field in field_names:\n    if field == '?':\n        queryset.query = queryset.query.order_by(func.random())\n        continue\n    if isinstance(field, int):\n        if field < 0:\n            order = desc\n            field = -field\n        else:\n            order = asc\n        queryset.query = queryset.query.order_by(order(field))\n        continue\n    # evaluate the descending condition\n    if '-' in field:\n        order = desc\n        field = field[1:]\n    else:\n        order = asc\n    # old school django style for related fields\n    if '.' in field:\n        #TODO: this is not accurate\n        queryset.query = queryset.query.order_by(order(condition))\n    else:\n        # normal order by\n        queryset, parts = parse_joins(queryset, field)\n        condition = reduce(lambda x, y: getattr(x, y), parts)\n        queryset.query = queryset.query.order_by(order(condition))\nreturn queryset", "path": "django_sqlalchemy\\models\\query_utils.py", "repo_name": "brosner/django-sqlalchemy", "stars": 17, "license": "bsd-3-clause", "language": "python", "size": 464}
{"docstring": "\"\"\"\nReturns the latest object, according to the model's 'get_latest_by'\noption or optional given field_name.\n\"\"\"\n", "func_signal": "def latest(self, field_name=None):\n", "code": "latest_by = field_name or self.model._meta.get_latest_by\nassert bool(latest_by), \"latest() requires either a field_name parameter or 'get_latest_by' in the model\"\nreturn self.order_by('-%s' % latest_by).first()", "path": "django_sqlalchemy\\models\\query.py", "repo_name": "brosner/django-sqlalchemy", "stars": 17, "license": "bsd-3-clause", "language": "python", "size": 464}
{"docstring": "\"\"\"Return a new QuerySet object, applying the given list of\nSQLAlchemy MapperOptions.\n\"\"\"\n", "func_signal": "def options(self, *args):\n", "code": "obj = self._clone()\nobj.query.options(*args)\nreturn obj", "path": "django_sqlalchemy\\models\\query.py", "repo_name": "brosner/django-sqlalchemy", "stars": 17, "license": "bsd-3-clause", "language": "python", "size": 464}
{"docstring": "\"\"\"\nReturns a new QuerySet instance that will select only distinct results.\n\"\"\"\n", "func_signal": "def distinct(self, true_or_false=True):\n", "code": "clone = self._clone()\nclone.query._distinct = true_or_false\nreturn clone", "path": "django_sqlalchemy\\models\\query.py", "repo_name": "brosner/django-sqlalchemy", "stars": 17, "license": "bsd-3-clause", "language": "python", "size": 464}
{"docstring": "\"\"\"\nConstructs the field_names list that the values query will be\nretrieving.\n\nCalled by the _clone() method after initialising the rest of the\ninstance.\n\"\"\"\n", "func_signal": "def _setup_query(self):\n", "code": "if self._fields:\n    self.field_names = list(self._fields) + self.field_names\nelse:\n    # Default to all fields.\n    self.field_names = [f.attname for f in self.model._meta.fields]\n\nfield_names = utils.fields_to_sa_columns(self, *self.field_names)\nself.query = self.query.values(*field_names)", "path": "django_sqlalchemy\\models\\query.py", "repo_name": "brosner/django-sqlalchemy", "stars": 17, "license": "bsd-3-clause", "language": "python", "size": 464}
{"docstring": "\"\"\"\nReverses the ordering of the queryset.\n\"\"\"\n", "func_signal": "def reverse(self):\n", "code": "clone = self._clone()\nfor field in clone.query._order_by:\n    if field.modifier == operators.desc_op: \n        field.modifier = operators.asc_op\n    else:\n        field.modifier = operators.desc_op\nreturn clone", "path": "django_sqlalchemy\\models\\query.py", "repo_name": "brosner/django-sqlalchemy", "stars": 17, "license": "bsd-3-clause", "language": "python", "size": 464}
{"docstring": "\"\"\"\nUpdates all elements in the current QuerySet, setting all the given\nfields to the appropriate values.\n\"\"\"\n", "func_signal": "def update(self, **kwargs):\n", "code": "values = self._parse_update_values(**kwargs)\nself.model.__table__.update(self.query.compile()._whereclause, values).execute()", "path": "django_sqlalchemy\\models\\query.py", "repo_name": "brosner/django-sqlalchemy", "stars": 17, "license": "bsd-3-clause", "language": "python", "size": 464}
{"docstring": "\"\"\"\nTODO:need to map complex_filter\nReturns a new QuerySet instance with filter_obj added to the filters.\nfilter_obj can be a Q object (or anything with an add_to_query()\nmethod) or a dictionary of keyword lookup arguments.\n\nThis exists to support framework features such as 'limit_choices_to',\nand usually it will be more natural to use other methods.\n\"\"\"\n", "func_signal": "def complex_filter(self, filter_obj):\n", "code": "if isinstance(filter_obj, Q) or hasattr(filter_obj, 'add_to_query'):\n    return self._filter_or_exclude(None, filter_obj)\nelse:\n    return self._filter_or_exclude(None, **filter_obj)", "path": "django_sqlalchemy\\models\\query.py", "repo_name": "brosner/django-sqlalchemy", "stars": 17, "license": "bsd-3-clause", "language": "python", "size": 464}
{"docstring": "\"\"\"Given a class, configure the class declaratively,\nusing the given registry (any dictionary) and MetaData object.\nThis operation does not assume any kind of class hierarchy.\n\n\"\"\"\n", "func_signal": "def instrument_declarative(cls, registry, metadata):\n", "code": "if '_decl_class_registry' in cls.__dict__:\n    raise exceptions.InvalidRequestError(\"Class %r already has been instrumented declaratively\" % cls)\ncls._decl_class_registry = registry\ncls.metadata = metadata\n_as_declarative(cls, cls.__name__, cls.__dict__)", "path": "django_sqlalchemy\\models\\base.py", "repo_name": "brosner/django-sqlalchemy", "stars": 17, "license": "bsd-3-clause", "language": "python", "size": 464}
{"docstring": "\"\"\"\nTODO: Need to map dates\nReturns a list of datetime objects representing all available dates\nfor the given field_name, scoped to 'kind'.\n\"\"\"\n", "func_signal": "def dates(self, field_name, kind, order='ASC'):\n", "code": "assert kind in (\"month\", \"year\", \"day\"), \\\n        \"'kind' must be one of 'year', 'month' or 'day'.\"\nassert order in ('ASC', 'DESC'), \\\n        \"'order' must be either 'ASC' or 'DESC'.\"\n# Let the FieldDoesNotExist exception propagate.\nfield = self.model._meta.get_field(field_name, many_to_many=False)\nassert isinstance(field, DateField), \"%r isn't a DateField.\" \\\n        % field_name\nreturn self._clone(klass=DateQuerySet, setup=True, _field=field,\n        _kind=kind, _order=order)", "path": "django_sqlalchemy\\models\\query.py", "repo_name": "brosner/django-sqlalchemy", "stars": 17, "license": "bsd-3-clause", "language": "python", "size": 464}
{"docstring": "# TODO: can't handle init in inherited classes\n# need to handle auto_now and auto_now_add\n# kwargs = dict(onupdate=self.onupdate)\n", "func_signal": "def sa_column_kwargs(self):\n", "code": "base = super(DSDateField, self).sa_column_kwargs()\n# base.update(kwargs)\nreturn base", "path": "django_sqlalchemy\\models\\fields\\__init__.py", "repo_name": "brosner/django-sqlalchemy", "stars": 17, "license": "bsd-3-clause", "language": "python", "size": 464}
{"docstring": "\"\"\"\nAdd a single filter to the query. The 'filter_expr' is a pair:\n(filter_string, value). E.g. ('name__contains', 'fred')\n\nIf 'negate' is True, this is an exclude() filter. If 'trim' is True, we\nautomatically trim the final join group (used internally when\nconstructing nested queries).\n\"\"\"\n", "func_signal": "def parse_filter(queryset, exclude, **kwargs):\n", "code": "query = queryset._clone()\n\nfor filter_expr in [(k, v) for k, v in kwargs.items()]:\n    arg, value = filter_expr\n    parts = arg.split(LOOKUP_SEP)\n    \n    # Work out the lookup type and remove it from 'parts', if necessary.\n    if len(parts) == 1 or parts[-1] not in QUERY_TERMS:\n        lookup_type = 'exact'\n    else:\n        lookup_type = parts.pop()\n            \n    if callable(value):\n        value = value()\n    \n    # parse and create the joins\n    query, parts = parse_joins(queryset, parts)\n\n    field = reduce(lambda x, y: getattr(x, lookup_attname(parts[0]._meta, y)), parts)\n    op = _lookup_query_expression(lookup_type, field, value)\n    expression = op()\n    if exclude:\n        expression = ~(expression)\n    query.query = query.query.filter(expression)\nreturn query", "path": "django_sqlalchemy\\models\\query_utils.py", "repo_name": "brosner/django-sqlalchemy", "stars": 17, "license": "bsd-3-clause", "language": "python", "size": 464}
{"docstring": "\"\"\"Adjust the incoming callable such that a 'self' argument is not required.\"\"\"\n\n", "func_signal": "def unbound_method_to_callable(func_or_cls):\n", "code": "if isinstance(func_or_cls, types.MethodType) and not func_or_cls.im_self:\n    return func_or_cls.im_func\nelse:\n    return func_or_cls", "path": "tests\\class_replace\\fields.py", "repo_name": "brosner/django-sqlalchemy", "stars": 17, "license": "bsd-3-clause", "language": "python", "size": 464}
{"docstring": "# fk field needs to know the related_name it belongs to \n# for create_column to work properly.\n", "func_signal": "def contribute_to_related_class(self, cls, related):\n", "code": "self._original.contribute_to_related_class(self, cls, related)\nself.related = related\nself.related_name = related.get_accessor_name()", "path": "django_sqlalchemy\\models\\fields\\related.py", "repo_name": "brosner/django-sqlalchemy", "stars": 17, "license": "bsd-3-clause", "language": "python", "size": 464}
{"docstring": "# m2m fields do not have a column\n", "func_signal": "def create_column(self):\n", "code": "self.sa_column = None\nself.__table__ = sa.Table(self.m2m_db_table(), metadata,\n    sa.Column(self.m2m_column_name(), self.model._meta.pk.sa_column.type),\n    sa.Column(self.m2m_reverse_name(), self.rel.to._meta.pk.sa_column.type))\nreturn self.sa_column", "path": "django_sqlalchemy\\models\\fields\\related.py", "repo_name": "brosner/django-sqlalchemy", "stars": 17, "license": "bsd-3-clause", "language": "python", "size": 464}
{"docstring": "\"\"\"\nReturns a dictionary mapping each of the given IDs to the object with\nthat ID.\n\"\"\"\n", "func_signal": "def in_bulk(self, id_list):\n", "code": "assert isinstance(id_list, (tuple,  list)), \\\n        \"in_bulk() must be provided with a list of IDs.\"\nif not id_list:\n    return {}\nqs = self.filter(**{'pk__in': id_list})\nreturn dict([(obj._get_pk_val(), obj) for obj in qs.iterator()])", "path": "django_sqlalchemy\\models\\query.py", "repo_name": "brosner/django-sqlalchemy", "stars": 17, "license": "bsd-3-clause", "language": "python", "size": 464}
{"docstring": "\"\"\"\nThis looks up the correct attribute name if the attribute is the\npk attribute.\n\"\"\"\n", "func_signal": "def lookup_attname(meta, value):\n", "code": "if value == 'pk':\n    return meta.pk.attname\nelse:\n    try:\n        field, model, direct, m2m = meta.get_field_by_name(value)\n        return field.attname\n    except models.FieldDoesNotExist:\n        return value", "path": "django_sqlalchemy\\models\\query_utils.py", "repo_name": "brosner/django-sqlalchemy", "stars": 17, "license": "bsd-3-clause", "language": "python", "size": 464}
{"docstring": "\"\"\"\nThis does the actual filtering, either combined filtering or \nexcluding depending on the exclude flag.\n\"\"\"\n", "func_signal": "def _filter_or_exclude(self, exclude, *args, **kwargs):\n", "code": "from django_sqlalchemy.models import query_utils\nreturn query_utils.parse_filter(self, exclude, **kwargs)", "path": "django_sqlalchemy\\models\\query.py", "repo_name": "brosner/django-sqlalchemy", "stars": 17, "license": "bsd-3-clause", "language": "python", "size": 464}
{"docstring": "# Each neuron is put in its own cluster.\n", "func_signal": "def clusterize (neurons, data_dimension, clusters, dataset, nb_clusters=0):\n", "code": "for neuron in neurons:\n    c = cluster.Cluster(data_dimension)\n    c.add_neuron(neuron)\n    c.compute_average()\n    clusters.append(c)\n\n# Every element from the dataset is classified and the corresponding\n# cluster stores its label.\nfor z in dataset:\n    nearest_cluster = clusters[0]\n    best_distance = distance.euclidian(clusters[0], z.data())\n    for i in xrange(1, len(clusters)):\n        tmp_distance = distance.euclidian(clusters[i], z.data())\n        if tmp_distance < best_distance:\n            nearest_cluster = clusters[i]\n            best_distance = tmp_distance\n    nearest_cluster.add_label(z.labels())\n    nearest_cluster._neurons[0]._labels.append(z.labels())\n\n# Here we reduce the number of cluster to the one specified. To do\n# that we look for the two closest clusters and we merge them until\n#  we have the good number of clusters.\nif nb_clusters > 0:\n    while len(clusters) > nb_clusters:\n        best_distance = clusters[0].distance_with(clusters[1].get_center())\n        cluster1 = 0\n        cluster2 = 1\n        for i in xrange(len(clusters)):\n            for j in xrange(len(clusters)):\n                if i != j:\n                    tmp_distance = clusters[i].distance_with(clusters[j].get_center())\n                    if tmp_distance < best_distance:\n                        cluster1 = i\n                        cluster2 = j\n                        best_distance = tmp_distance\n        clusters[i].merge_with(clusters[j])\n        clusters.pop(j)", "path": "algorithms\\kohonen\\algorithm.py", "repo_name": "jlauron/kohonen", "stars": 25, "license": "None", "language": "python", "size": 247}
{"docstring": "#export(\"Classe A\", points)\n", "func_signal": "def main2gaussians ():\n", "code": "points2 = generate_gaussian(1000, 2, 0, 2, center=(5, 5))\npylab.plot (points2[:,0], points2[:,1], 'b+')\n#export(\"Classe C\", points)\npoints4 = generate_gaussian(1000, 2, 0, 2, center=(0, 0))\npylab.plot (points4[:,0], points4[:,1], 'g+')\npylab.axis([-10, 20, -10, 20])\npylab.show()\n\npoints = scipy.concatenate ((points2,points4))\n\nlabels = []\nfor i in xrange(len(points2)):\n    labels.append(0)\nfor i in xrange(len(points4)):\n    labels.append(1)\ndata = dataset.Dataset (points, labels)\ndata.random ()\n\ndataset.save (data, \"../datasets/2gaussians1k.data\")", "path": "base\\dataset_generator.py", "repo_name": "jlauron/kohonen", "stars": 25, "license": "None", "language": "python", "size": 247}
{"docstring": "\"\"\" Classify a given point by finding its closest Voronoi vector and\nreturning the corresponding class \"\"\"\n", "func_signal": "def classify (self, point):\n", "code": "_vector = self.__get_closest_voronoi_vector(point)\n\nreturn _vector.get_label()", "path": "algorithms\\lvq\\algorithm.py", "repo_name": "jlauron/kohonen", "stars": 25, "license": "None", "language": "python", "size": 247}
{"docstring": "\"\"\" Draw centers with pylab \"\"\"\n", "func_signal": "def draw_center (self, color=None):\n", "code": "if color == \"green\":\n    pylab.plot([self._center[0]], [self._center[1]], \"go\")\nelif color == \"black\":\n    pylab.plot([self._center[0]], [self._center[1]], \"ko\")\nelif color == \"blue\":\n    pylab.plot([self._center[0]], [self._center[1]], \"bo\")\nelse:\n    pylab.plot([self._center[0]], [self._center[1]], \"ro\")", "path": "base\\cluster.py", "repo_name": "jlauron/kohonen", "stars": 25, "license": "None", "language": "python", "size": 247}
{"docstring": "\"\"\" Get the cluster main label \"\"\"\n\n", "func_signal": "def get_label (self):\n", "code": "most_frequent = None\n\nfrequences = {}\nmost_frequent = None\nfor label in self._labels:\n    if frequences.has_key(label):\n        frequences[label] += 1\n    else:\n        frequences[label] = 0\n\n    freq = None\n    for label in frequences:\n        if most_frequent is None:\n            most_frequent = label\n            freq = frequences[label]\n        else:\n            if frequences[label] > freq:\n                most_frequent = label\n                freq = frequences[label]\n\nreturn most_frequent", "path": "base\\cluster.py", "repo_name": "jlauron/kohonen", "stars": 25, "license": "None", "language": "python", "size": 247}
{"docstring": "\"\"\" Computes the standard deviation of the last cross validation. This\nvalue is computed as sqrt(1/N * sum((x_i - mean(x_i)) ^ 2)). \"\"\"\n\n", "func_signal": "def get_standard_deviation(self):\n", "code": "mean = self.get_expected_value()\n\nstandard_deviation = 0.0\nfor res in self.results:\n    standard_deviation += (res - mean) ** 2\nstandard_deviation /= len(self.results)\n\nreturn math.sqrt(standard_deviation)", "path": "base\\crossvalidation.py", "repo_name": "jlauron/kohonen", "stars": 25, "license": "None", "language": "python", "size": 247}
{"docstring": "\"\"\" Get the cluster main label \"\"\"\n\n", "func_signal": "def get_label (self):\n", "code": "most_frequent = None\n\nfrequences = {}\nmost_frequent = None\nfor label in self._labels:\n    if frequences.has_key(label):\n        frequences[label] += 1\n    else:\n        frequences[label] = 0\n\n    freq = None\n    for label in frequences:\n        if most_frequent is None:\n            most_frequent = label\n            freq = frequences[label]\n        else:\n            if frequences[label] > freq:\n                most_frequent = label\n                freq = frequences[label]\n\nreturn most_frequent", "path": "algorithms\\kohonen\\neuron.py", "repo_name": "jlauron/kohonen", "stars": 25, "license": "None", "language": "python", "size": 247}
{"docstring": "\"\"\" Computes the probability that the nearest cluster of a given point\nto have a given label. The probability is given as 0 < p < 1. This\nprobability is independant of any treshold. \"\"\"\n\n# We first get the closest cluster.\n", "func_signal": "def class_probability(self, point, c):\n", "code": "nearest_cluster = get_nearest_cluster(self._clusters, point)\n\n# We store every occurence of a label in a dictionary.\nfrequences = {0: 0.0, 1: 0.0}\nfor label in nearest_cluster._labels:\n    if frequences.has_key(label):\n        frequences[label] += 1.0\n    else:\n        frequences[label] = 0.0\n\nsum = 0\nfor k in frequences:\n    sum += frequences[k]\n\nif sum == 0:\n    return 0\nelse:\n    return frequences[c] / sum", "path": "algorithms\\kohonen\\algorithm.py", "repo_name": "jlauron/kohonen", "stars": 25, "license": "None", "language": "python", "size": 247}
{"docstring": "\"\"\" Classify a given point by finding its closest Voronoi vector and\nreturning the corresponding class \"\"\"\n", "func_signal": "def classify (self, point):\n", "code": "_vector = self.__get_closest_voronoi_vector(point)\n\nreturn _vector.get_label()", "path": "algorithms\\lvq\\algorithm.py", "repo_name": "jlauron/kohonen", "stars": 25, "license": "None", "language": "python", "size": 247}
{"docstring": "\"\"\" Computes the probability that the  vector of a given point\nto have a given label. The probability is given as 0 < p < 1. This\nprobability is independant of any treshold. \"\"\"\n\n# We first get the closest Voronoi vector\n", "func_signal": "def class_probability (self, point, c):\n", "code": "_vector = self.__get_closest_voronoi_vector(point)\n\n# We store every occurence of a label in a dictionary.\nfrequences = {0: 0.0, 1: 0.0}\nfor label in _vector._labels:\n    if frequences.has_key(label):\n        frequences[label] += 1.0\n    else:\n        frequences[label] = 0.0\n\nsum = 0\nfor k in frequences:\n    sum += frequences[k]\n\nif sum == 0:\n    return 0\nelse:\n    return frequences[c] / sum", "path": "algorithms\\lvq\\algorithm.py", "repo_name": "jlauron/kohonen", "stars": 25, "license": "None", "language": "python", "size": 247}
{"docstring": "\"\"\" Build indexes from list of dimensions \"\"\"\n", "func_signal": "def generate_positions (dimensions, acc=[]):\n", "code": "if len(dimensions) == 0:\n    yield acc\nelse:\n    index = dimensions.pop(0)\n    for i in xrange(index):\n        mydimensions = acc[:]\n        mydimensions.append(i)\n        for result in generate_positions(dimensions[:], mydimensions):\n            yield result", "path": "algorithms\\kohonen\\algorithm.py", "repo_name": "jlauron/kohonen", "stars": 25, "license": "None", "language": "python", "size": 247}
{"docstring": "\"\"\" Computes the probability that the nearest cluster of a given point\nto have a given label. The probability is given as 0 < p < 1. This\nprobability is independant of any treshold. \"\"\"\n\n# We first get the closest cluster.\n", "func_signal": "def class_probability(self, point, c):\n", "code": "nearest_cluster = som.get_nearest_cluster(self._clusters, point)\n\n# We store every occurence of a label in a dictionary.\n# TODO: this dictionary should be in the cluster.\nfrequences = {0: 0.0, 1: 0.0}\nfor label in nearest_cluster._labels:\n    frequences[label] += 1.0\n\nsum = frequences[0] + frequences[1]\nif sum == 0:\n    return 0\nelse:\n    return frequences[c] / (frequences[0] + frequences[1])", "path": "algorithms\\recsom\\algorithm.py", "repo_name": "jlauron/kohonen", "stars": 25, "license": "None", "language": "python", "size": 247}
{"docstring": "\"\"\" Return the closest Voronoi vector for a given point \"\"\"\n", "func_signal": "def __get_closest_voronoi_vector (self, point):\n", "code": "_vector = self._voronoi_vectors[0]\nbest_distance = self._voronoi_vectors[0].distance_with(point)\nfor i in xrange(1, len(self._voronoi_vectors)):\n    tmp_distance = self._voronoi_vectors[i].distance_with(point)\n    if tmp_distance < best_distance:\n        _vector = self._voronoi_vectors[i]\n        best_distance = tmp_distance\n\nreturn _vector", "path": "algorithms\\lvq\\algorithm.py", "repo_name": "jlauron/kohonen", "stars": 25, "license": "None", "language": "python", "size": 247}
{"docstring": "\"\"\" Retrieves the closest cluster of a point. \"\"\"\n\n# Initially the result is set to the first neuron.\n", "func_signal": "def get_nearest_cluster(self, point):\n", "code": "nearest_cluster = self._clusters[0]\nbest_distance = distance.euclidian(self._clusters[0], point)\n\n# We look for a closer neuron in the neuron list.\nfor i in xrange(1, len(self._clusters)):\n    tmp_distance = distance.euclidian(self._clusters[i], point)\n    if tmp_distance < best_distance:\n        nearest_cluster = self._clusters[i]\n        best_distance = tmp_distance\n\nreturn nearest_cluster", "path": "algorithms\\kohonen\\algorithm.py", "repo_name": "jlauron/kohonen", "stars": 25, "license": "None", "language": "python", "size": 247}
{"docstring": "\"\"\" Generate Gaussian array of nb_points in dimension where mu is the mean\nand sigma the standard deviation \"\"\"\n", "func_signal": "def generate_gaussian (nb_points, dimension, mu, sigma, center=(0, 0)):\n", "code": "points = scipy.zeros((nb_points, dimension))\nfor i in xrange(0, nb_points):\n    for j in xrange(0, dimension):\n        points[i, j] = random.gauss(mu, sigma) + center[j]\n\nreturn points", "path": "base\\dataset_generator.py", "repo_name": "jlauron/kohonen", "stars": 25, "license": "None", "language": "python", "size": 247}
{"docstring": "\"\"\" Computes the guessed label of a given point. \"\"\"\n\n# TODO: We should have the possibility of defining a treshold for that.\n", "func_signal": "def classify (self, point):\n", "code": "if self.class_probability(point, 1) > 0.5:\n    return 1\nelse:\n    return 0", "path": "algorithms\\recsom\\algorithm.py", "repo_name": "jlauron/kohonen", "stars": 25, "license": "None", "language": "python", "size": 247}
{"docstring": "\"\"\" Computes the expected value of the last cross validation. This\nvalue is computed as sum(x_i * p_i). \"\"\"\n\n", "func_signal": "def get_expected_value(self):\n", "code": "mean = 0.0\nfor res in self.results:\n    mean += res\nmean /= len(self.results)\n\nreturn mean", "path": "base\\crossvalidation.py", "repo_name": "jlauron/kohonen", "stars": 25, "license": "None", "language": "python", "size": 247}
{"docstring": "\"\"\" Compute average point of all points affected to the current cluster,\nand move the center of the cluster to this new point \"\"\"\n", "func_signal": "def compute_average (self, distance_fun=distance.euclidian):\n", "code": "if len(self._points) <= 0:\n    return 0\n\nif self._dimension != len(self._points[0]):\n    raise Exception()\n\n# Initialize new center coords\nnew_center = []\nfor dim in xrange(self._dimension):\n    new_center.append(0)\n\n# Compute average of all points coords\nfor i in xrange(len(self._points)):\n    for dim in xrange(self._dimension):\n        new_center[dim] += self._points[i][dim]\nfor dim in xrange(self._dimension):\n    new_center[dim] = new_center[dim] / len(self._points)\n\nif self.distance_with(new_center) > 0:\n    self._center = new_center\n    return 1\nelse:\n    return 0", "path": "base\\cluster.py", "repo_name": "jlauron/kohonen", "stars": 25, "license": "None", "language": "python", "size": 247}
{"docstring": "#datas = dataset.load (\"donut.data\")\n", "func_signal": "def test_kfold ():\n", "code": "do = donut.Donut (1000, 0.1)\ndatas, labels = do.get_toyproblem ()\ndatas = dataset.Dataset (datas, labels)\nkfold = cv.KFold (10)\nkfold.load_dataset (datas)\nkfold.estimate (algorithm.KNN (3))\nkfold.print_results ()", "path": "algorithms\\knn\\main.py", "repo_name": "jlauron/kohonen", "stars": 25, "license": "None", "language": "python", "size": 247}
{"docstring": "\"\"\" Computes the probability that the  vector of a given point\nto have a given label. The probability is given as 0 < p < 1. This\nprobability is independant of any treshold. \"\"\"\n\n# We first get the closest Voronoi vector\n", "func_signal": "def class_probability (self, point, c):\n", "code": "_vector = self.__get_closest_voronoi_vector(point)\n\n# We store every occurence of a label in a dictionary.\nfrequences = {0: 0.0, 1: 0.0}\nfor label in _vector._labels:\n    if frequences.has_key(label):\n        frequences[label] += 1.0\n    else:\n        frequences[label] = 0.0\n\nsum = 0\nfor k in frequences:\n    sum += frequences[k]\n\nif sum == 0:\n    return 0\nelse:\n    return frequences[c] / sum", "path": "algorithms\\lvq\\algorithm.py", "repo_name": "jlauron/kohonen", "stars": 25, "license": "None", "language": "python", "size": 247}
{"docstring": "# NSObject doesn't implement the copying protocol\n#o = super(MyCopyClass, self).copyWithZone_(zone)\n", "func_signal": "def copyWithZone_(self, zone):\n", "code": "o = self.__class__.alloc().init()\no.foobar = 2\nreturn o", "path": "pyobjc\\pyobjc-core\\PyObjCTest\\test_subclass.py", "repo_name": "orestis/pyobjc", "stars": 17, "license": "None", "language": "python", "size": 18178}
{"docstring": "# This is jus a round-about way of testing that the right proxy\n# object is created\n", "func_signal": "def testFormattingForDateTime(self):\n", "code": "formatter = NSDateFormatter.alloc().initWithDateFormat_allowNaturalLanguage_(\n        \"%Y-%m-%d %H:%M:%S\", True)\n\ndate = datetime.datetime.now()\n\nvalue = formatter.stringFromDate_(date)\nself.assertEquals(value, date.strftime('%Y-%m-%d %H:%M:%S'))", "path": "pyobjc\\pyobjc-core\\PyObjCTest\\test_nsdate_proxy.py", "repo_name": "orestis/pyobjc", "stars": 17, "license": "None", "language": "python", "size": 18178}
{"docstring": "# Helper function for creating a buffer, needed until we write the\n# manual wrappers for creating a buffer without a pool\n", "func_signal": "def makeBuffer(self):\n", "code": "rv, pool = CVPixelBufferPoolCreate(None, {\n        kCVPixelBufferPoolMinimumBufferCountKey: 1,\n        kCVPixelBufferPoolMaximumBufferAgeKey: 300,\n    }, {\n        kCVPixelBufferWidthKey: 100,\n        kCVPixelBufferHeightKey: 100,\n        kCVPixelBufferPixelFormatTypeKey: kCVPixelFormatType_32ARGB,\n    }, None)\nself.failUnlessEqual(rv, 0)\nself.failUnlessIsInstance(pool, CVPixelBufferPoolRef)\n\nrv, image = CVPixelBufferPoolCreatePixelBuffer(None, pool, None)\nself.failUnlessEqual(rv, 0)\nreturn image", "path": "pyobjc\\pyobjc-framework-Quartz\\PyObjCTest\\test_cvpixelbuffer.py", "repo_name": "orestis/pyobjc", "stars": 17, "license": "None", "language": "python", "size": 18178}
{"docstring": "\"\"\"\nRegister 'type' as a list-like type that will be proxied\nas an NSMutableArray subclass. \n\"\"\"\n", "func_signal": "def registerListType(type):\n", "code": "OC_PythonArray = lookUpClass('OC_PythonArray')\nOC_PythonArray.depythonifyTable().append(type)", "path": "pyobjc\\pyobjc-core\\Lib\\objc\\_bridges.py", "repo_name": "orestis/pyobjc", "stars": 17, "license": "None", "language": "python", "size": 18178}
{"docstring": "# Python to Obj-C\n", "func_signal": "def _bridgePythonTypes():\n", "code": "OC_PythonObject = lookUpClass('OC_PythonObject')\ntry:\n    if BRIDGED_TYPES:\n        OC_PythonObject.depythonifyTable().extend(BRIDGED_TYPES)\n    if BRIDGED_STRUCTURES:\n        OC_PythonObject.pythonifyStructTable().update(BRIDGED_STRUCTURES)\nexcept AttributeError:\n    pass", "path": "pyobjc\\pyobjc-core\\Lib\\objc\\_bridges.py", "repo_name": "orestis/pyobjc", "stars": 17, "license": "None", "language": "python", "size": 18178}
{"docstring": "# should do checks to see if the libs exist\n", "func_signal": "def transform_libs(self, libs):\n", "code": "new_libs = []\nfor lib in libs:\n\tif self.lib_aliases.has_key(lib):\n\t\tfor new_lib in self.lib_aliases[lib]:\n\t\t\tif new_lib not in new_libs:\n\t\t\t\tnew_libs.append(new_lib)\n\telse:\n\t\tif lib not in new_libs:\n\t\t\tnew_libs.append(lib)\nreturn new_libs", "path": "pyobjc\\PyOpenGL-2.0.2.01\\setup\\dist.py", "repo_name": "orestis/pyobjc", "stars": 17, "license": "None", "language": "python", "size": 18178}
{"docstring": "# New-style class mixin\n", "func_signal": "def testMultipleInheritance1(self):\n", "code": "class MixinClass1 (object):\n    def mixinMethod(self):\n        return \"foo\"\n\nclass MITestClass1 (NSObject, MixinClass1):\n    def init(self):\n        return NSObject.pyobjc_instanceMethods.init(self)\n\nself.assert_(hasattr(MITestClass1, 'mixinMethod'))\n\no = MITestClass1.alloc().init()\nself.assertEquals(o.mixinMethod(), \"foo\")", "path": "pyobjc\\pyobjc-core\\PyObjCTest\\test_subclass.py", "repo_name": "orestis/pyobjc", "stars": 17, "license": "None", "language": "python", "size": 18178}
{"docstring": "\"\"\" check that staticmethod()-s are not converted to selectors \"\"\"\n\n", "func_signal": "def testStaticMethod(self):\n", "code": "class StaticMethodTest (NSObject):\n    def stMeth(self):\n        return \"hello\"\n    stMeth = staticmethod(stMeth)\n\nself.assert_(isinstance(StaticMethodTest.stMeth, types.FunctionType))", "path": "pyobjc\\pyobjc-core\\PyObjCTest\\test_subclass.py", "repo_name": "orestis/pyobjc", "stars": 17, "license": "None", "language": "python", "size": 18178}
{"docstring": "# ***** Part 1 *****\n", "func_signal": "def doAlphaRects(context):\n", "code": "ourRect = CGRectMake(0.0, 0.0, 130.0, 100.0)\nnumRects = 6\nrotateAngle = 2 * math.pi / numRects\ntintAdjust = 1./numRects\n\n# ***** Part 2 *****\nCGContextTranslateCTM(context, 2*ourRect.size.width, \n                                2*ourRect.size.height)\n\n# ***** Part 3 *****\ntint = 1.0\nfor i in range(numRects):\n    CGContextSetRGBFillColor(context, tint, 0.0, 0.0, tint)\n    CGContextFillRect(context, ourRect)\n    CGContextRotateCTM(context, rotateAngle)  # cumulative\n    tint -= tintAdjust", "path": "pyobjc\\pyobjc-framework-Quartz\\Examples\\Programming with Quartz\\CocoaDrawingShell\\MyView.py", "repo_name": "orestis/pyobjc", "stars": 17, "license": "None", "language": "python", "size": 18178}
{"docstring": "\"\"\"Setup light 0 and enable lighting\"\"\"\n", "func_signal": "def light():\n", "code": "glLightfv(GL_LIGHT0, GL_AMBIENT, [0.0, 1.0, 0.0, 1.0])\nglLightfv(GL_LIGHT0, GL_DIFFUSE, [1.0, 1.0, 1.0, 1.0])\nglLightfv(GL_LIGHT0, GL_SPECULAR, [1.0, 1.0, 1.0, 1.0])\nglLightfv(GL_LIGHT0, GL_POSITION, [1.0, 1.0, 1.0, 0.0]);   \nglLightModelfv(GL_LIGHT_MODEL_AMBIENT, [0.2, 0.2, 0.2, 1.0])\nglEnable(GL_LIGHTING)\nglEnable(GL_LIGHT0)", "path": "pyobjc\\PyOpenGL-2.0.2.01\\OpenGL\\Demo\\GLUT\\tom\\text.py", "repo_name": "orestis/pyobjc", "stars": 17, "license": "None", "language": "python", "size": 18178}
{"docstring": "\"\"\"\nRegister 'type' as a list-like type that will be proxied\nas an NSMutableArray subclass. \n\"\"\"\n", "func_signal": "def registerMappingType(type):\n", "code": "OC_PythonDictionary = lookUpClass('OC_PythonDictionary')\nOC_PythonDictionary.depythonifyTable().append(type)", "path": "pyobjc\\pyobjc-core\\Lib\\objc\\_bridges.py", "repo_name": "orestis/pyobjc", "stars": 17, "license": "None", "language": "python", "size": 18178}
{"docstring": "# Always use this delegate method instead of using the action to do something\n# when the selection changed: the action method is only called when the selection\n# changed by means of a mouse click; this method is also called when the\n# user uses the arrow keys.\n", "func_signal": "def tableViewSelectionDidChange_(self, notification):\n", "code": "if self.tableView.numberOfSelectedRows() == 0:\n    if self.tableView.numberOfSelectedColumns() == 0:\n        self.label.setStringValue_(\"\")\n    else:\n        items = list(self.tableView.selectedColumnEnumerator())\n        word = \"Column\"\nelse:\n    items = list(self.tableView.selectedRowEnumerator())\n    word = \"Row\"\nlabel = \"%s%s: %s\" % (word, (\"s\", \"\")[len(items) == 1], \", \".join([str(x) for x in items]))\nself.label.setStringValue_(label)", "path": "pyobjc\\pyobjc-framework-Cocoa\\Examples\\AppKit\\TableModel\\TableModel.py", "repo_name": "orestis/pyobjc", "stars": 17, "license": "None", "language": "python", "size": 18178}
{"docstring": "# Defining a method whose name is a keyword followed by two underscores\n# should define the method name without underscores in the runtime,\n# and this method should be accesible both with and without the\n# underscores.\n\n", "func_signal": "def testMethodRaise(self):\n", "code": "class RaiseClass (NSObject):\n    def raise__(self):\n        pass\n\nself.assert_(not hasattr(NSObject, 'raise__'))\nself.assert_(not hasattr(NSObject, 'raise'))\n\nself.assert_(hasattr(RaiseClass, 'raise__'))\nself.assert_(hasattr(RaiseClass, 'raise'))\nself.assertEquals(RaiseClass.raise__.selector, 'raise')\nself.assertEquals(getattr(RaiseClass, 'raise').selector, 'raise')", "path": "pyobjc\\pyobjc-core\\PyObjCTest\\test_subclass.py", "repo_name": "orestis/pyobjc", "stars": 17, "license": "None", "language": "python", "size": 18178}
{"docstring": "# This is jus a round-about way of testing that the right proxy\n# object is created\n", "func_signal": "def testFormattingForDate(self):\n", "code": "formatter = NSDateFormatter.alloc().initWithDateFormat_allowNaturalLanguage_(\n        \"%Y-%m-%d\", True)\n\ndate = datetime.date.today()\n\nvalue = formatter.stringFromDate_(NSDate.date())\nself.assertEquals(value, date.strftime('%Y-%m-%d'))\n\nvalue = formatter.stringFromDate_(date)\nself.assertEquals(value, date.strftime('%Y-%m-%d'))", "path": "pyobjc\\pyobjc-core\\PyObjCTest\\test_nsdate_proxy.py", "repo_name": "orestis/pyobjc", "stars": 17, "license": "None", "language": "python", "size": 18178}
{"docstring": "\"\"\"Callback function for displaying the scene\n\nThis defines a unit-square environment in which to draw,\ni.e. width is one drawing unit, as is height\n\"\"\"\n", "func_signal": "def display( swap=1, clear=1):\n", "code": "if clear:\n\tglClearColor(0.5, 0.5, 0.5, 0)\n\tglClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)\n\n# establish the projection matrix (perspective)\nglMatrixMode(GL_PROJECTION)\nglLoadIdentity()\nx,y,width,height = glGetDoublev(GL_VIEWPORT)\ngluPerspective(\n\t45, # field of view in degrees\n\twidth/float(height or 1), # aspect ratio\n\t.25, # near clipping plane\n\t200, # far clipping plane\n)\n\n# and then the model view matrix\nglMatrixMode(GL_MODELVIEW)\nglLoadIdentity()\ngluLookAt(\n\t0,1,30, # eyepoint\n\t10,0,0, # center-of-view\n\t0,1,0, # up-vector\n)\nlight()\nrotation()\n\nglFrontFace(GL_CCW)\nglEnable(GL_CULL_FACE)  # added by jfp to use with new logo.py\nglEnable(GL_DEPTH_TEST)\nglMaterialfv(GL_FRONT, GL_DIFFUSE, [1., 1., 0., 0.])", "path": "pyobjc\\PyOpenGL-2.0.2.01\\OpenGL\\Demo\\GLUT\\tom\\text.py", "repo_name": "orestis/pyobjc", "stars": 17, "license": "None", "language": "python", "size": 18178}
{"docstring": "# NSFloatingWindowLevel is a define in Objective-C, up-to 1.0rc1\n# we didn't correctly pick up this define due to a bug in the code.\n", "func_signal": "def testNSFloatingWindowLevel(self):\n", "code": "self.assert_(hasattr(AppKit, 'NSFloatingWindowLevel'))\nself.assert_(isinstance(AppKit.NSFloatingWindowLevel, int))", "path": "pyobjc\\pyobjc-framework-Cocoa\\PyObjCTest\\test_constants.py", "repo_name": "orestis/pyobjc", "stars": 17, "license": "None", "language": "python", "size": 18178}
{"docstring": "\"\"\"Create display list for the text\"\"\"\n", "func_signal": "def createList( ):\n", "code": "newList = glGenLists(1);\nglNewList(newList, GL_COMPILE);\ntry:\n\tdefine_logo()\nfinally:\n\tglEndList()\nreturn newList", "path": "pyobjc\\PyOpenGL-2.0.2.01\\OpenGL\\Demo\\GLUT\\tom\\text.py", "repo_name": "orestis/pyobjc", "stars": 17, "license": "None", "language": "python", "size": 18178}
{"docstring": "\"\"\"Do rotation of the scene at given rate\"\"\"\n", "func_signal": "def rotation( period = 10):\n", "code": "angle = (((time.time()-starttime)%period)/period)* 360\nglRotate( angle, 0,1,0)\nreturn angle", "path": "pyobjc\\PyOpenGL-2.0.2.01\\OpenGL\\Demo\\GLUT\\tom\\text.py", "repo_name": "orestis/pyobjc", "stars": 17, "license": "None", "language": "python", "size": 18178}
{"docstring": "# old-style class mixin\n", "func_signal": "def testMultipleInheritance2(self):\n", "code": "class MixinClass2:\n    def mixinMethod(self):\n        return \"foo\"\n\nclass MITestClass2 (NSObject, MixinClass2):\n    def init(self):\n        return NSObject.pyobjc_instanceMethods.init(self)\n\nself.assert_(hasattr(MITestClass2, 'mixinMethod'))\n\no = MITestClass2.alloc().init()\nself.assertEquals(o.mixinMethod(), \"foo\")", "path": "pyobjc\\pyobjc-core\\PyObjCTest\\test_subclass.py", "repo_name": "orestis/pyobjc", "stars": 17, "license": "None", "language": "python", "size": 18178}
{"docstring": "\"\"\"\nReturns a Protocol object for the named protocol. This is the\nequivalent of @protocol(name) in Objective-C.\nRaises objc.ProtocolError when the protocol does not exist.\n\"\"\"\n", "func_signal": "def protocolNamed(name):\n", "code": "name = unicode(name)\ntry:\n    return PROTOCOL_CACHE[name]\nexcept KeyError:\n    pass\nfor p in _objc.protocolsForProcess():\n    pname = p.__name__\n    PROTOCOL_CACHE.setdefault(pname, p)\n    if pname == name:\n        return p\nfor cls in _objc.getClassList():\n    for p in _objc.protocolsForClass(cls):\n        pname = p.__name__\n        PROTOCOL_CACHE.setdefault(pname, p)\n        if pname == name:\n            return p\nraise ProtocolError(\"protocol %r does not exist\" % (name,), name)", "path": "pyobjc\\pyobjc-core\\Lib\\objc\\_protocols.py", "repo_name": "orestis/pyobjc", "stars": 17, "license": "None", "language": "python", "size": 18178}
{"docstring": "\"\"\"Initializes a JavaScript token stream state tracker.\n\nArgs:\n  closurized_namespaces: An optional list of namespace prefixes used for\n      testing of goog.provide/require.\n\"\"\"\n", "func_signal": "def __init__(self, closurized_namespaces=''):\n", "code": "statetracker.StateTracker.__init__(self, JsDocFlag)\nself.__closurized_namespaces = closurized_namespaces", "path": "deps\\closure-linter\\closure_linter\\javascriptstatetracker.py", "repo_name": "kof/node-linter", "stars": 22, "license": "None", "language": "python", "size": 4214}
{"docstring": "\"\"\"Determine the block type given a START_BLOCK token.\n\nCode blocks come after parameters, keywords  like else, and closing parens.\n\nArgs:\n  token: The current token. Can be assumed to be type START_BLOCK\nReturns:\n  Code block type for current token.\n\"\"\"\n", "func_signal": "def GetBlockType(self, token):\n", "code": "last_code = tokenutil.SearchExcept(token, Type.NON_CODE_TYPES, None,\n                                   True)\nif last_code.type in (Type.END_PARAMETERS, Type.END_PAREN,\n                      Type.KEYWORD) and not last_code.IsKeyword('return'):\n  return self.CODE\nelse:\n  return self.OBJECT_LITERAL", "path": "deps\\closure-linter\\closure_linter\\javascriptstatetracker.py", "repo_name": "kof/node-linter", "stars": 22, "license": "None", "language": "python", "size": 4214}
{"docstring": "\"\"\"Initializes to prepare to check a file.\n\nArgs:\n  checker: Class to report errors to.\n  limited_doc_checks: Whether doc checking is relaxed for this file.\n  is_html: Whether the file is an HTML file with extracted contents.\n\"\"\"\n", "func_signal": "def Initialize(self, checker, limited_doc_checks, is_html):\n", "code": "self.__checker = checker\nself._limited_doc_checks = limited_doc_checks\nself._is_html = is_html", "path": "deps\\closure-linter\\closure_linter\\checkerbase.py", "repo_name": "kof/node-linter", "stars": 22, "license": "None", "language": "python", "size": 4214}
{"docstring": "\"\"\"Create a single file lint test case.\n\nArgs:\n  filename: Filename to test.\n  runner: Object implementing the LintRunner interface that lints a file.\n  converter: Function taking an error string and returning an error code.\n\"\"\"\n\n", "func_signal": "def __init__(self, filename, runner, converter):\n", "code": "googletest.TestCase.__init__(self, 'runTest')\nself._filename = filename\nself._messages = []\nself._runner = runner\nself._converter = converter", "path": "deps\\closure-linter\\closure_linter\\common\\filetestcase.py", "repo_name": "kof/node-linter", "stars": 22, "license": "None", "language": "python", "size": 4214}
{"docstring": "\"\"\"Internal handler for the start of a script tag.\n\nArgs:\n  attrs: The attributes of the script tag, as a list of tuples.\n\"\"\"\n", "func_signal": "def start_script(self, attrs):\n", "code": "for attribute in attrs:\n  if attribute[0].lower() == 'src':\n    # Skip script tags with a src specified.\n    return\nself._in_script = True", "path": "deps\\closure-linter\\closure_linter\\common\\htmlutil.py", "repo_name": "kof/node-linter", "stars": 22, "license": "None", "language": "python", "size": 4214}
{"docstring": "\"\"\"Handles the given token and updates state.\n\nArgs:\n  token: The token to handle.\n  last_non_space_token:\n\"\"\"\n", "func_signal": "def HandleToken(self, token, last_non_space_token):\n", "code": "super(JavaScriptStateTracker, self).HandleToken(token,\n                                                last_non_space_token)\n\nif token.IsType(Type.IDENTIFIER):\n  if token.string == 'goog.require':\n    class_token = tokenutil.Search(token, Type.STRING_TEXT)\n    self.__goog_require_tokens.append(class_token)\n\n  elif token.string == 'goog.provide':\n    class_token = tokenutil.Search(token, Type.STRING_TEXT)\n    self.__goog_provide_tokens.append(class_token)\n\n  elif self.__closurized_namespaces:\n    self.__AddUsedNamespace(token.string)\nif token.IsType(Type.SIMPLE_LVALUE) and not self.InFunction():\n  identifier = token.values['identifier']\n\n  if self.__closurized_namespaces:\n    namespace = self.GetClosurizedNamespace(identifier)\n    if namespace and identifier == namespace:\n      self.__provided_namespaces.add(namespace)\nif (self.__closurized_namespaces and\n    token.IsType(Type.DOC_FLAG) and\n    token.attached_object.flag_type == 'implements'):\n  # Interfaces should be goog.require'd.\n  doc_start = tokenutil.Search(token, Type.DOC_START_BRACE)\n  interface = tokenutil.Search(doc_start, Type.COMMENT)\n  self.__AddUsedNamespace(interface.string)", "path": "deps\\closure-linter\\closure_linter\\javascriptstatetracker.py", "repo_name": "kof/node-linter", "stars": 22, "license": "None", "language": "python", "size": 4214}
{"docstring": "\"\"\"Trap gpylint's output parse it to get messages added.\"\"\"\n", "func_signal": "def _ProcessFileAndGetMessages(self, filename):\n", "code": "errors = erroraccumulator.ErrorAccumulator()\nself._runner.Run([filename], errors)\n\nerrors = errors.GetErrors()\nerrors.sort()\nreturn errors", "path": "deps\\closure-linter\\closure_linter\\common\\filetestcase.py", "repo_name": "kof/node-linter", "stars": 22, "license": "None", "language": "python", "size": 4214}
{"docstring": "\"\"\"Given an identifier, returns the namespace that identifier is from.\n\nArgs:\n  identifier: The identifier to extract a namespace from.\n\nReturns:\n  The namespace the given identifier resides in, or None if one could not\n  be found.\n\"\"\"\n", "func_signal": "def GetClosurizedNamespace(self, identifier):\n", "code": "parts = identifier.split('.')\nfor part in parts:\n  if part.endswith('_'):\n    # Ignore private variables / inner classes.\n    return None\n\nif identifier.startswith('goog.global'):\n  # Ignore goog.global, since it is, by definition, global.\n  return None\n\nfor namespace in self.__closurized_namespaces:\n  if identifier.startswith(namespace + '.'):\n    last_part = parts[-1]\n    if not last_part:\n      # TODO(robbyw): Handle this: it's a multi-line identifier.\n      return None\n\n    if last_part in ('apply', 'inherits', 'call'):\n      # Calling one of Function's methods usually indicates use of a\n      # superclass.\n      parts.pop()\n      last_part = parts[-1]\n\n    for i in xrange(1, len(parts)):\n      part = parts[i]\n      if part.isupper():\n        # If an identifier is of the form foo.bar.BAZ.x or foo.bar.BAZ,\n        # the namespace is foo.bar.\n        return '.'.join(parts[:i])\n      if part == 'prototype':\n        # If an identifier is of the form foo.bar.prototype.x, the\n        # namespace is foo.bar.\n        return '.'.join(parts[:i])\n\n    if last_part.isupper() or not last_part[0].isupper():\n      # Strip off the last part of an enum or constant reference.\n      parts.pop()\n\n    return '.'.join(parts)\n\nreturn None", "path": "deps\\closure-linter\\closure_linter\\javascriptstatetracker.py", "repo_name": "kof/node-linter", "stars": 22, "license": "None", "language": "python", "size": 4214}
{"docstring": "\"\"\"Parse a file and get a sorted list of expected messages.\"\"\"\n", "func_signal": "def _GetExpectedMessages(self, stream):\n", "code": "messages = []\nfor i, line in enumerate(stream):\n  match = self._EXPECTED_RE.search(line)\n  if match:\n    line = match.group('line')\n    msg_ids = match.group('msgs')\n    if line is None:\n      line = i + 1\n    elif line.startswith('+') or line.startswith('-'):\n      line = i + 1 + int(line)\n    else:\n      line = int(line)\n    for msg_id in msg_ids.split(','):\n      # Ignore a spurious message from the license preamble.\n      if msg_id != 'WITHOUT':\n        messages.append((line, self._converter(msg_id.strip())))\nstream.seek(0)\nmessages.sort()\nreturn messages", "path": "deps\\closure-linter\\closure_linter\\common\\filetestcase.py", "repo_name": "kof/node-linter", "stars": 22, "license": "None", "language": "python", "size": 4214}
{"docstring": "\"\"\"Initialize an JavaScriptStyleChecker object.\n\nArgs:\n  error_handler: Error handler to pass all errors to\n\"\"\"\n", "func_signal": "def __init__(self, error_handler):\n", "code": "checkerbase.CheckerBase.__init__(\n    self,\n    error_handler=error_handler,\n    lint_rules=javascriptlintrules.JavaScriptLintRules(),\n    state_tracker=javascriptstatetracker.JavaScriptStateTracker(\n        closurized_namespaces=flags.FLAGS.closurized_namespaces),\n    metadata_pass=ecmametadatapass.EcmaMetaDataPass(),\n    limited_doc_files=flags.FLAGS.limited_doc_files)", "path": "deps\\closure-linter\\closure_linter\\checker.py", "repo_name": "kof/node-linter", "stars": 22, "license": "None", "language": "python", "size": 4214}
{"docstring": "\"\"\"Initialize a ScriptExtractor.\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "htmllib.HTMLParser.__init__(self, formatter.NullFormatter())\nself._in_script = False\nself._text = ''", "path": "deps\\closure-linter\\closure_linter\\common\\htmlutil.py", "repo_name": "kof/node-linter", "stars": 22, "license": "None", "language": "python", "size": 4214}
{"docstring": "\"\"\"Creates a new Token object.\n\nArgs:\n  string: The string of input the token contains.\n  token_type: The type of token.\n  line: The text of the line this token is in.\n  line_number: The line number of the token.\n  values: A dict of named values within the token.  For instance, a\n    function declaration may have a value called 'name' which captures the\n    name of the function.\n\"\"\"\n", "func_signal": "def __init__(self, string, token_type, line, line_number, values=None):\n", "code": "self.type = token_type\nself.string = string\nself.length = len(string)\nself.line = line\nself.line_number = line_number\nself.values = values\n\n# These parts can only be computed when the file is fully tokenized\nself.previous = None\nself.next = None\nself.start_index = None\n\n# This part is set in statetracker.py\n# TODO(robbyw): Wrap this in to metadata\nself.attached_object = None\n\n# This part is set in *metadatapass.py\nself.metadata = None", "path": "deps\\closure-linter\\closure_linter\\common\\tokens.py", "repo_name": "kof/node-linter", "stars": 22, "license": "None", "language": "python", "size": 4214}
{"docstring": "\"\"\"Internal handler for character data.\n\nArgs:\n  data: The character data from the HTML file.\n\"\"\"\n", "func_signal": "def handle_data(self, data):\n", "code": "if self._in_script:\n  # If the last line contains whitespace only, i.e. is just there to\n  # properly align a </script> tag, strip the whitespace.\n  if data.rstrip(' \\t') != data.rstrip(' \\t\\n\\r\\f'):\n    data = data.rstrip(' \\t')\n  self._text += data\nelse:\n  self._AppendNewlines(data)", "path": "deps\\closure-linter\\closure_linter\\common\\htmlutil.py", "repo_name": "kof/node-linter", "stars": 22, "license": "None", "language": "python", "size": 4214}
{"docstring": "\"\"\"Tests if this token is any of the given types.\n\nArgs:\n  token_types: The types to check.  Also accepts a single array.\n\nReturns:\n  True if the type of this token is any of the types passed in.\n\"\"\"\n", "func_signal": "def IsAnyType(self, *token_types):\n", "code": "if not isinstance(token_types[0], basestring):\n  return self.type in token_types[0]\nelse:\n  return self.type in token_types", "path": "deps\\closure-linter\\closure_linter\\common\\tokens.py", "repo_name": "kof/node-linter", "stars": 22, "license": "None", "language": "python", "size": 4214}
{"docstring": "\"\"\"Resets the state tracker to prepare for processing a new page.\"\"\"\n", "func_signal": "def Reset(self):\n", "code": "super(JavaScriptStateTracker, self).Reset()\n\nself.__goog_require_tokens = []\nself.__goog_provide_tokens = []\nself.__provided_namespaces = set()\nself.__used_namespaces = []", "path": "deps\\closure-linter\\closure_linter\\javascriptstatetracker.py", "repo_name": "kof/node-linter", "stars": 22, "license": "None", "language": "python", "size": 4214}
{"docstring": "\"\"\"Run GJsLint on the given filenames.\n\nArgs:\n  filenames: The filenames to check\n  error_handler: An optional ErrorHandler object, an ErrorPrinter is used if\n    none is specified.\n\nReturns:\n  error_count, file_count: The number of errors and the number of files that\n      contain errors.\n\"\"\"\n", "func_signal": "def Run(self, filenames, error_handler=None):\n", "code": "if not error_handler:\n  error_handler = errorprinter.ErrorPrinter(errors.NEW_ERRORS)\n\nchecker = JavaScriptStyleChecker(error_handler)\n\n# Check the list of files.\nfor filename in filenames:\n  checker.Check(filename)\n\nreturn error_handler", "path": "deps\\closure-linter\\closure_linter\\checker.py", "repo_name": "kof/node-linter", "stars": 22, "license": "None", "language": "python", "size": 4214}
{"docstring": "\"\"\"Checks the file, printing warnings and errors as they are found.\n\nArgs:\n  filename: The name of the file to check.\n  source: Optional. The contents of the file.  Can be either a string or\n      file-like object.  If omitted, contents will be read from disk from\n      the given filename.\n\"\"\"\n\n", "func_signal": "def Check(self, filename, source=None):\n", "code": "if source is None:\n  try:\n    f = open(filename)\n  except IOError:\n    self.__error_handler.HandleFile(filename, None)\n    self.HandleError(errors.FILE_NOT_FOUND, 'File not found', None)\n    self.__error_handler.FinishFile()\n    return\nelse:\n  if type(source) in [str, unicode]:\n    f = StringIO.StringIO(source)\n  else:\n    f = source\n\ntry:\n  if filename.endswith('.html') or filename.endswith('.htm'):\n    self.CheckLines(filename, htmlutil.GetScriptLines(f), True)\n  else:\n    self.CheckLines(filename, f, False)\nfinally:\n  f.close()", "path": "deps\\closure-linter\\closure_linter\\checkerbase.py", "repo_name": "kof/node-linter", "stars": 22, "license": "None", "language": "python", "size": 4214}
{"docstring": "\"\"\"Extract script tag contents from the given HTML file.\n\nArgs:\n  f: The HTML file.\n\nReturns:\n  Lines in the HTML file that are from script tags.\n\"\"\"\n", "func_signal": "def GetScriptLines(f):\n", "code": "extractor = ScriptExtractor()\n\n# The HTML parser chokes on text like Array.<!string>, so we patch\n# that bug by replacing the < with &lt; - escaping all text inside script\n# tags would be better but it's a bit of a catch 22.\ncontents = f.read()\ncontents = re.sub(r'<([^\\s\\w/])',\n       lambda x: '&lt;%s' % x.group(1),\n       contents)\n\nextractor.feed(contents)\nextractor.close()\nreturn extractor.GetScriptLines()", "path": "deps\\closure-linter\\closure_linter\\common\\htmlutil.py", "repo_name": "kof/node-linter", "stars": 22, "license": "None", "language": "python", "size": 4214}
{"docstring": "\"\"\"Count the number of newlines in the given string and append them.\n\nThis ensures line numbers are correct for reported errors.\n\nArgs:\n  data: The data to count newlines in.\n\"\"\"\n# We append 'x' to both sides of the string to ensure that splitlines\n# gives us an accurate count.\n", "func_signal": "def _AppendNewlines(self, data):\n", "code": "for i in xrange(len(('x' + data + 'x').splitlines()) - 1):\n  self._text += '\\n'", "path": "deps\\closure-linter\\closure_linter\\common\\htmlutil.py", "repo_name": "kof/node-linter", "stars": 22, "license": "None", "language": "python", "size": 4214}
{"docstring": "\"\"\"Adds the namespace of an identifier to the list of used namespaces.\n\nArgs:\n  identifier: An identifier which has been used.\n\"\"\"\n", "func_signal": "def __AddUsedNamespace(self, identifier):\n", "code": "namespace = self.GetClosurizedNamespace(identifier)\n\nif namespace:\n  # We add token.string as a 'namespace' as it is something that could\n  # potentially be provided to satisfy this dependency.\n  self.__used_namespaces.append([namespace, identifier])", "path": "deps\\closure-linter\\closure_linter\\javascriptstatetracker.py", "repo_name": "kof/node-linter", "stars": 22, "license": "None", "language": "python", "size": 4214}
