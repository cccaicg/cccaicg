{"docstring": "\"\"\"\nTest response to incoming version request.\n\"\"\"\n", "func_signal": "def test_onVersion(self):\n", "code": "self.stub = XmlStreamStub()\nself.protocol = generic.VersionHandler('Test', '0.1.0')\nself.protocol.xmlstream = self.stub.xmlstream\nself.protocol.send = self.stub.xmlstream.send\nself.protocol.connectionInitialized()\n\niq = domish.Element((None, 'iq'))\niq['from'] = 'user@example.org/Home'\niq['to'] = 'example.org'\niq['type'] = 'get'\nquery = iq.addElement((NS_VERSION, 'query'))\nself.stub.send(iq)\n\nresponse = self.stub.output[-1]\nself.assertEquals('user@example.org/Home', response['to'])\nself.assertEquals('example.org', response['from'])\nself.assertEquals('result', response['type'])\nself.assertEquals(NS_VERSION, response.query.uri)\nelements = list(domish.generateElementsQNamed(response.query.children,\n                                              'name', NS_VERSION))\nself.assertEquals(1, len(elements))\nself.assertEquals('Test', unicode(elements[0]))\nelements = list(domish.generateElementsQNamed(response.query.children,\n                                              'version', NS_VERSION))\nself.assertEquals(1, len(elements))\nself.assertEquals('0.1.0', unicode(elements[0]))", "path": "wokkel\\test\\test_generic.py", "repo_name": "gnunicorn/wokkel", "stars": 1, "license": "mit", "language": "python", "size": 224}
{"docstring": "\"\"\"\nDisconnect from the router and handlers.\n\"\"\"\n", "func_signal": "def stopService(self):\n", "code": "service.Service.stopService(self)\n\nfor domain in self.domains:\n    self._router.removeRoute(domain, self._pipe.sink)\n\nself._pipe = None\nself.xmlstream = None\n\nfor e in self:\n    e.connectionLost(None)", "path": "wokkel\\component.py", "repo_name": "gnunicorn/wokkel", "stars": 1, "license": "mit", "language": "python", "size": 224}
{"docstring": "\"\"\"\nTest send after XML stream disconnection.\n\nThe data should be cached until a new XML stream has been established\nand initialized.\n\"\"\"\n", "func_signal": "def test_sendDisconnected(self):\n", "code": "factory = xmlstream.XmlStreamFactory(xmlstream.Authenticator())\nsm = subprotocols.StreamManager(factory)\nhandler = DummyXMPPHandler()\nsm.addHandler(handler)\n\nxs = factory.buildProtocol(None)\nxs.connectionMade()\nxs.transport = proto_helpers.StringTransport()\nxs.connectionLost(None)\n\nsm.send(\"<presence/>\")\nself.assertEquals(\"\", xs.transport.value())\nself.assertEquals(\"<presence/>\", sm._packetQueue[0])", "path": "wokkel\\test\\test_subprotocols.py", "repo_name": "gnunicorn/wokkel", "stars": 1, "license": "mit", "language": "python", "size": 224}
{"docstring": "\"\"\"\nTest send when the stream has been initialized.\n\nThe data should be sent directly over the XML stream.\n\"\"\"\n", "func_signal": "def test_sendInitialized(self):\n", "code": "factory = xmlstream.XmlStreamFactory(xmlstream.Authenticator())\nsm = subprotocols.StreamManager(factory)\nxs = factory.buildProtocol(None)\nxs.transport = proto_helpers.StringTransport()\nxs.connectionMade()\nxs.dataReceived(\"<stream:stream xmlns='jabber:client' \"\n                \"xmlns:stream='http://etherx.jabber.org/streams' \"\n                \"from='example.com' id='12345'>\")\nxs.dispatch(xs, \"//event/stream/authd\")\nsm.send(\"<presence/>\")\nself.assertEquals(\"<presence/>\", xs.transport.value())", "path": "wokkel\\test\\test_subprotocols.py", "repo_name": "gnunicorn/wokkel", "stars": 1, "license": "mit", "language": "python", "size": 224}
{"docstring": "\"\"\"\nGet a Mood instance from an XML representation.\n\nThis class method parses the given XML document into a L{Mood}\ninstances.\n\n@param element: The XML mood document.\n@type element: object providing\n               L{IElement<twisted.words.xish.domish.IElement>}\n@return: A L{Mood} instance or C{None} if C{element} was not a mood\n         document or if there was no mood value element.\n\"\"\"\n", "func_signal": "def fromXml(self, element):\n", "code": "if element.uri != NS_MOOD or element.name != 'mood':\n    return None\n\nvalue = None\ntext = None\n\nfor child in element.elements():\n    if child.uri != NS_MOOD:\n        continue\n\n    if child.name == 'text':\n        text = unicode(child)\n    else:\n        value = child.name\n\nif value:\n    return Mood(value, text)\nelse:\n    return None", "path": "wokkel\\formats.py", "repo_name": "gnunicorn/wokkel", "stars": 1, "license": "mit", "language": "python", "size": 224}
{"docstring": "\"\"\"\nTest that protocol handlers have their connectionLost method\ncalled when the XML stream is disconnected.\n\"\"\"\n", "func_signal": "def test_disconnected(self):\n", "code": "sm = self.streamManager\nhandler = DummyXMPPHandler()\nhandler.setHandlerParent(sm)\nxs = xmlstream.XmlStream(xmlstream.Authenticator())\nsm._disconnected(xs)\nself.assertEquals(0, handler.doneMade)\nself.assertEquals(0, handler.doneInitialized)\nself.assertEquals(1, handler.doneLost)", "path": "wokkel\\test\\test_subprotocols.py", "repo_name": "gnunicorn/wokkel", "stars": 1, "license": "mit", "language": "python", "size": 224}
{"docstring": "\"\"\"\nCalled by the stream when it has started.\n\nThis examines the default namespace of the incoming stream and whether\nthere is a requested hostname for the component. Then it generates a\nstream identifier, sends a response header and adds an observer for\nthe first incoming element, triggering L{onElement}.\n\"\"\"\n\n", "func_signal": "def streamStarted(self, rootElement):\n", "code": "xmlstream.ListenAuthenticator.streamStarted(self, rootElement)\n\n# Compatibility fix for pre-8.2 implementations of ListenAuthenticator\nif not self.xmlstream.sid:\n    from twisted.python import randbytes\n    self.xmlstream.sid = randbytes.secureRandom(8).encode('hex')\n\nif rootElement.defaultUri != self.namespace:\n    exc = error.StreamError('invalid-namespace')\n    self.xmlstream.sendStreamError(exc)\n    return\n\n# self.xmlstream.thisEntity is set to the address the component\n# wants to assume.\nif not self.xmlstream.thisEntity:\n    exc = error.StreamError('improper-addressing')\n    self.xmlstream.sendStreamError(exc)\n    return\n\nself.xmlstream.sendHeader()\nself.xmlstream.addOnetimeObserver('/*', self.onElement)", "path": "wokkel\\component.py", "repo_name": "gnunicorn/wokkel", "stars": 1, "license": "mit", "language": "python", "size": 224}
{"docstring": "\"\"\"\nTest correct initialization and setup of factory observers.\n\"\"\"\n", "func_signal": "def test_basic(self):\n", "code": "sm = self.streamManager\nself.assertIdentical(None, sm.xmlstream)\nself.assertEquals([], sm.handlers)\nself.assertEquals(sm._connected,\n                  sm.factory.callbacks['//event/stream/connected'])\nself.assertEquals(sm._authd,\n                  sm.factory.callbacks['//event/stream/authd'])\nself.assertEquals(sm._disconnected,\n                  sm.factory.callbacks['//event/stream/end'])\nself.assertEquals(sm.initializationFailed,\n                  sm.factory.callbacks['//event/xmpp/initfailed'])", "path": "wokkel\\test\\test_subprotocols.py", "repo_name": "gnunicorn/wokkel", "stars": 1, "license": "mit", "language": "python", "size": 224}
{"docstring": "\"\"\"\nTest send when the stream is connected but not yet initialized.\n\nThe data should be cached until the XML stream has been initialized.\n\"\"\"\n", "func_signal": "def test_sendNotInitialized(self):\n", "code": "factory = xmlstream.XmlStreamFactory(xmlstream.Authenticator())\nsm = subprotocols.StreamManager(factory)\nxs = factory.buildProtocol(None)\nxs.transport = proto_helpers.StringTransport()\nxs.connectionMade()\nxs.dataReceived(\"<stream:stream xmlns='jabber:client' \"\n                \"xmlns:stream='http://etherx.jabber.org/streams' \"\n                \"from='example.com' id='12345'>\")\nsm.send(\"<presence/>\")\nself.assertEquals(\"\", xs.transport.value())\nself.assertEquals(\"<presence/>\", sm._packetQueue[0])", "path": "wokkel\\test\\test_subprotocols.py", "repo_name": "gnunicorn/wokkel", "stars": 1, "license": "mit", "language": "python", "size": 224}
{"docstring": "\"\"\"\nTest removal of a protocol handler.\n\"\"\"\n", "func_signal": "def test_removeHandler(self):\n", "code": "handler = DummyXMPPHandler()\nhandler.setHandlerParent(self.collection)\nhandler.disownHandlerParent(self.collection)\nself.assertNotIn(handler, self.collection)\nself.assertIdentical(None, handler.parent)", "path": "wokkel\\test\\test_subprotocols.py", "repo_name": "gnunicorn/wokkel", "stars": 1, "license": "mit", "language": "python", "size": 224}
{"docstring": "\"\"\"\nTest that connectionLost forgets the XML stream.\n\"\"\"\n", "func_signal": "def test_connectionLost(self):\n", "code": "handler = subprotocols.XMPPHandler()\nxs = xmlstream.XmlStream(xmlstream.Authenticator())\nhandler.makeConnection(xs)\nhandler.connectionLost(Exception())\nself.assertIdentical(None, handler.xmlstream)", "path": "wokkel\\test\\test_subprotocols.py", "repo_name": "gnunicorn/wokkel", "stars": 1, "license": "mit", "language": "python", "size": 224}
{"docstring": "\"\"\"\nSend an element from the sink and observe it from the source.\n\"\"\"\n", "func_signal": "def test_sendFromSink(self):\n", "code": "def cb(obj):\n    called.append(obj)\n\ncalled = []\nself.pipe.source.addObserver('/test[@xmlns=\"testns\"]', cb)\nelement = domish.Element(('testns', 'test'))\nself.pipe.sink.send(element)\nself.assertEquals([element], called)", "path": "wokkel\\test\\test_generic.py", "repo_name": "gnunicorn/wokkel", "stars": 1, "license": "mit", "language": "python", "size": 224}
{"docstring": "\"\"\"\nTest the addition of a protocol handler after the stream\nhave been initialized.\n\nMake sure that the handler will have the connected stream\npassed via C{makeConnection} and have C{connectionInitialized}\ncalled.\n\"\"\"\n", "func_signal": "def test_addHandlerInitialized(self):\n", "code": "sm = self.streamManager\nxs = xmlstream.XmlStream(xmlstream.Authenticator())\nsm._connected(xs)\nsm._authd(xs)\nhandler = DummyXMPPHandler()\nhandler.setHandlerParent(sm)\n\nself.assertEquals(1, handler.doneMade)\nself.assertEquals(1, handler.doneInitialized)\nself.assertEquals(0, handler.doneLost)", "path": "wokkel\\test\\test_subprotocols.py", "repo_name": "gnunicorn/wokkel", "stars": 1, "license": "mit", "language": "python", "size": 224}
{"docstring": "\"\"\"\nTest response when the request is handled successfully with payload.\n\"\"\"\n\n", "func_signal": "def test_successPayload(self):\n", "code": "class Handler(DummyIQHandler):\n    payload = domish.Element(('testns', 'foo'))\n\n    def onGet(self, iq):\n        return self.payload\n\niq = domish.Element((None, 'iq'))\niq['type'] = 'get'\niq['id'] = 'r1'\nhandler = Handler()\nhandler.handleRequest(iq)\nresponse = handler.output[-1]\nself.assertEquals(None, response.uri)\nself.assertEquals('iq', response.name)\nself.assertEquals('result', response['type'])\npayload = response.elements().next()\nself.assertEqual(handler.payload, payload)", "path": "wokkel\\test\\test_subprotocols.py", "repo_name": "gnunicorn/wokkel", "stars": 1, "license": "mit", "language": "python", "size": 224}
{"docstring": "\"\"\"\nAdd a new handler and connect it to the stream.\n\"\"\"\n", "func_signal": "def addHandler(self, handler):\n", "code": "XMPPHandlerCollection.addHandler(self, handler)\n\nif self.xmlstream:\n    handler.makeConnection(self.xmlstream)\n    handler.connectionInitialized()", "path": "wokkel\\component.py", "repo_name": "gnunicorn/wokkel", "stars": 1, "license": "mit", "language": "python", "size": 224}
{"docstring": "\"\"\"\nTest response when the request handler raises a non-stanza-error.\n\"\"\"\n\n", "func_signal": "def test_failureUnknown(self):\n", "code": "class TestError(Exception):\n    pass\n\nclass Handler(DummyIQHandler):\n    def onGet(self, iq):\n        raise TestError()\n\niq = domish.Element((None, 'iq'))\niq['type'] = 'get'\niq['id'] = 'r1'\nhandler = Handler()\nhandler.handleRequest(iq)\nresponse = handler.output[-1]\nself.assertEquals(None, response.uri)\nself.assertEquals('iq', response.name)\nself.assertEquals('error', response['type'])\ne = error.exceptionFromStanza(response)\nself.assertEquals('internal-server-error', e.condition)\nself.assertEquals(1, len(self.flushLoggedErrors(TestError)))", "path": "wokkel\\test\\test_subprotocols.py", "repo_name": "gnunicorn/wokkel", "stars": 1, "license": "mit", "language": "python", "size": 224}
{"docstring": "\"\"\"\nRoute a stanza.\n\n@param stanza: The stanza to be routed.\n@type stanza: L{domish.Element}.\n\"\"\"\n", "func_signal": "def route(self, stanza):\n", "code": "destination = JID(stanza['to'])\n\nlog.msg(\"Routing to %s: %r\" % (destination.full(), stanza.toXml()))\n\nif destination.host in self.routes:\n    self.routes[destination.host].send(stanza)\nelse:\n    self.routes[None].send(stanza)", "path": "wokkel\\component.py", "repo_name": "gnunicorn/wokkel", "stars": 1, "license": "mit", "language": "python", "size": 224}
{"docstring": "\"\"\"\nCalled when a component has succesfully authenticated.\n\nAdd the component to the routing table and establish a handler\nfor a closed connection.\n\"\"\"\n", "func_signal": "def connectionInitialized(self, xs):\n", "code": "destination = xs.thisEntity.host\n\nself.router.addRoute(destination, xs)\nxs.addObserver(xmlstream.STREAM_END_EVENT, self.connectionLost, 0,\n                                           destination, xs)", "path": "wokkel\\component.py", "repo_name": "gnunicorn/wokkel", "stars": 1, "license": "mit", "language": "python", "size": 224}
{"docstring": "\"\"\"\nCalled when stream initialization has failed.\n\nStop the service (thereby disconnecting the current stream) and\nraise the exception.\n\"\"\"\n", "func_signal": "def initializationFailed(self, reason):\n", "code": "self.stopService()\nreason.raiseException()", "path": "wokkel\\component.py", "repo_name": "gnunicorn/wokkel", "stars": 1, "license": "mit", "language": "python", "size": 224}
{"docstring": "\"\"\"\nTest that data is passed on for sending by the stream manager.\n\"\"\"\n", "func_signal": "def test_send(self):\n", "code": "class DummyStreamManager(object):\n    def __init__(self):\n        self.outlist = []\n\n    def send(self, data):\n        self.outlist.append(data)\n\nhandler = subprotocols.XMPPHandler()\nhandler.parent = DummyStreamManager()\nhandler.send('<presence/>')\nself.assertEquals(['<presence/>'], handler.parent.outlist)", "path": "wokkel\\test\\test_subprotocols.py", "repo_name": "gnunicorn/wokkel", "stars": 1, "license": "mit", "language": "python", "size": 224}
{"docstring": "#logging.debug('emitting %s' % signal)\n#logging.debug('emit map %s' % repr(self.slots))\n", "func_signal": "def emit(self,signal,*args):\n", "code": "for slot in self.slots.get(signal,[]):\n\tlogging.debug( 'Emitting: %s, for  %s slot' % ( signal, repr( slot )) )\n\ttry:\n\t\tif not slot(*args):\n\t\t\tlogging.debug( 'Emit %s: FALSE' % signal )\n\t\t\treturn False\n\texcept:\n\t\tlogging.error('PLUGIN ERROR: %s' % trace())\nlogging.debug( 'Emit %s: True' % signal )\nreturn True", "path": "viperpeers.py", "repo_name": "mayson/viperpeers", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 128}
{"docstring": "\"\"\"\nCalled when the connection times out.\n\"\"\"\n", "func_signal": "def timeoutConnection(self):\n", "code": "logging.debug('timeout: %s' % self._addr)\nself.write('<HUB> Login timeout!|')\nself.transport.loseConnection()", "path": "viperpeers.py", "repo_name": "mayson/viperpeers", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 128}
{"docstring": "# Params should be: 'plugin'\n", "func_signal": "def LoadPlugin(self,addr,params=[]):\n", "code": "if len(params)==1:\n\tlogging.debug('loading plugin %s' % params[0])\n\tif params[0] not in self.plugs:\n\t\ttry:\n\t\t\tif not '.' in sys.path:\n\t\t\t\tsys.path.append('.')\n\t\t\tif 'plugins.'+params[0] not in sys.modules:\n\t\t\t\tplugins=__import__('plugins.'+params[0])\n\t\t\t\tplugin=getattr(plugins,params[0])\n\t\t\telse:\n\t\t\t\tplugin=reload(sys.modules['plugins.'+params[0]])\n\t\t\tlogging.getLogger().setLevel(self.settings['core']['loglevel'])\n\t\t\tlogging.debug('loaded plugin file success')\n\t\t\tcls=getattr(plugin,params[0]+'_plugin')\n\t\t\tobj=cls(self)\n\t\t\tself.plugs[params[0]]=obj\n\t\t\tself.commands.update(obj.commands)\n\t\t\t#self.usercommands.update(obj.usercommands)\n\t\t\tlogging.debug( 'Plugin %s slots: %s' % (params[0], repr( obj.slots ) ) )\n\t\t\tfor key,value in obj.slots.iteritems():\n\t\t\t\tlogging.debug( 'Activating Slot: %s, on plugin %s' % ( key, params[0] ) )\n\n\n\t\t\t\tif key in self.slots:\n\t\t\t\t\tself.slots[key].append(value)\n\t\t\t\t\t\n\t\t\t\telse:\n\t\t\t\t\tself.slots[key]=[value]\n\t\t\tlogging.debug( 'MessageMap: %s' % repr( self.slots ))\n\n\t\t\tself.Gen_UC()\n\t\t\tself.send_usercommands_to_all()\n\t\t\treturn self._('Success')\n\t\texcept:\n\t\t\te=trace()\n\t\t\tlogging.debug( 'Plugin load error: %s' % (e,) )\n\t\t\treturn self._( 'Plugin load error: %s' % (e,) )\n\telse:\n\t\treturn self._('Plugin already loaded')\nelse:\n\treturn self._('Params error')", "path": "viperpeers.py", "repo_name": "mayson/viperpeers", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 128}
{"docstring": "# Params should be: 'nick' 'level'\n", "func_signal": "def SetLevel(self,addr,params=[]):\n", "code": "if len(params)==2:\n\tif params[0] in self.reglist:\n\t\tself.reglist[params[0]]['level']=yaml.load(params[1])\n\t\treturn self._('Success')\n\telse:\n\t\treturn self._('No such user')\nelse:\n\treturn self._('Params error.')", "path": "viperpeers.py", "repo_name": "mayson/viperpeers", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 128}
{"docstring": "# params: 'nick'\n\n", "func_signal": "def UI(self,addr,params=[]):\n", "code": "if len(params)==1:\n\tuser=self.nicks.get(params[0],None)\n\tif user!=None:\n\t\treturn self._(' -- USER %s INFO --\\n addres: %s\\n level: %s\\n is op?: %s\\n is protected?: %s') % (user.nick, user.addr, user.level, repr(user.level in self.oplevels), repr(user.level in self.protected))\n\telse:\n\t\treturn self._('No such user')\nelse:\n\t\treturn self._('Params error')", "path": "viperpeers.py", "repo_name": "mayson/viperpeers", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 128}
{"docstring": "# Params should be: 'nick'\n", "func_signal": "def Kick (self, addr, params=[]):\n", "code": "if len(params)>=1:\n\tif params[0] in self.nicks:\n\t\tif self.nicks[params[0]].level in self.protected:\n\t\t\treturn self._('User protected!')\n\t\tmsg = '<%s> is kicking %s because: ' % (self.addrs[addr].nick, params[0])\n\t\tif len(params)>1:\n\t\t\tfnick = self.core_settings['hubname'].replace(' ','_')\n\t\t\treason = ' '.join(params[1:])\n\t\t\tself.send_pm_to_nick(fnick, params[0], reason)\n\t\t\tmsg += reason\n\t\telse:\n\t\t\tmsg += '-'\n\t\tself.drop_user_by_nick(params[0])\n\t\tself.send_to_all(msg)\n\t\treturn self._('Success')\n\telse:\n\t\treturn self._('No such user')\nelse:\n\treturn self._('Usage: !Kick <Username> [<reason>]')", "path": "viperpeers.py", "repo_name": "mayson/viperpeers", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 128}
{"docstring": "# Params should be 'nick'\n", "func_signal": "def DelReg(self,addr,params=[]):\n", "code": "if len(params)==1:\n\t# Check if 'nick' registred\n\tif params[0] in self.reglist:\n\t\tif params[0] not in self.protected:\n\t\t\tdel self.reglist[params[0]]\n\t\t\treturn self._('User deleted')\n\t\telse:\n\t\t\treturn self._('User protected!')\n\n\telse:\n\t\treturn self._('User not registred')\nelse:\n\treturn self._('Params error')", "path": "viperpeers.py", "repo_name": "mayson/viperpeers", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 128}
{"docstring": "# Params 'nick'\n", "func_signal": "def Passwd(self,addr,params=[]):\n", "code": "if len(params)>0:\n\tnewpass=\" \".join(params)\n\tnick=self.addrs[addr].nick\n\tif nick in self.reglist:\n\t\tself.reglist[nick]['passwd']=newpass\n\t\treturn self._('Your password updated')\n\telse:\n\t\treturn self._('You are not registred')\nelse:\n\treturn self._('Params error')", "path": "viperpeers.py", "repo_name": "mayson/viperpeers", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 128}
{"docstring": "# Params: 'nick' 'newpass'\n", "func_signal": "def PasswdTo(self,addr,params=[]):\n", "code": "if len(params)>1:\n\tnick=params[0]\n\tnewpass=\" \".join(params[1:])\n\tif nick in self.reglist:\n\t\tif self.nicks[nick].level in self.protected:\n\t\t\t\treturn self._('User protected!')\n\t\tself.reglist[nick]['passwd']=newpass\n\t\treturn self._('User password updated')\n\telse:\n\t\t\treturn self._('User not registred')\n\nelse:\n\t\treturn self._('Params error')", "path": "viperpeers.py", "repo_name": "mayson/viperpeers", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 128}
{"docstring": "''' return string with ASCII 0, 5, 36, 96, 124, 126 unmasked from: &# ; mask. e.g. &#5; -> chr(5) '''\n", "func_signal": "def unmasksyms(self, str):\n", "code": "cds=[0, 5, 36, 96, 124, 126]\nfor i in cds:\n\tstr=str.replace('&#%s;' % i, chr(i))\n\nreturn str", "path": "viperpeers.py", "repo_name": "mayson/viperpeers", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 128}
{"docstring": "#params 'nick' 'message'\n", "func_signal": "def say(self,addr,params):\n", "code": "if len(params)>1:\n\tself.hub.send_to_all('<%s> %s|' % (params[0], \" \".join(params[1:])))\n\treturn ''\nelse:\n\treturn self.hub._('params error')", "path": "plugins\\say.py", "repo_name": "mayson/viperpeers", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 128}
{"docstring": "# Params can be empty or 'command'\n", "func_signal": "def Help(self,addr,params=\"\"):\n", "code": "if len(params)==1:\n\tif self.check_rights(self.addrs[addr], params[0]):\n\t\treturn self.help[params[0]]\n\telse:\n\t\treturn self._('Premission denied')\n\t\t\nelif len(params)==0:\n\tans=self._(' -- Aviable commands for you--\\n')\n\tfor cmd in self.commands.keys():\n\t\tif self.check_rights(self.addrs[addr],cmd):\n\t\t\tans+='%s\\n' % self.help.get(cmd,cmd)\n\treturn ans\nelse:\n\treturn self._('Params error')", "path": "viperpeers.py", "repo_name": "mayson/viperpeers", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 128}
{"docstring": "''' return string with ASCII 0, 5, 36, 96, 124, 126 masked with: &# ;. e.g. chr(5) -> &#5; '''\n", "func_signal": "def masksyms(self, str):\n", "code": "cds=[0, 5, 36, 96, 124, 126]\nfor i in cds:\n\tstr=str.replace(chr(i),'&#%s;' % i)\n\nreturn str", "path": "viperpeers.py", "repo_name": "mayson/viperpeers", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 128}
{"docstring": "\"\"\"\nif user registred, and passwd is correct \nwe should connect it even if it's already connected (drop & connect)\n\"\"\"\n", "func_signal": "def parse_validate_cmd(self, cmd):\n", "code": "acmd = cmd.split(' ', 1)\nif acmd[0] == '$MyPass':\n\tlogging.debug('MyPass %s' % acmd[1])\n\tif acmd[1] != self.factory.reglist[self._nick]['passwd']:\n\t\tlogging.info('wrong pass')\n\t\tself.write(('<HUB> %s|$BadPass|' % (self._('Password incorrect. Provided: %s') % str(acmd[1]),)).encode(self.factory.charset))\n\t\tlogging.debug('not validated nick. dropping.')\n\t\tself.transport.loseConnection()\n\t\treturn\n\telse:\n\t\tif self._nick in self.factory.nicks:\n\t\t\tlogging.debug('reconnecting identified user')\n\t\t\ttry:\n\t\t\t\tself.factory.nicks[self._nick].descr.write('<HUB> You are connecting from different machine. Bye.|')\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\tself.factory.drop_user_by_nick(self._nick)\n\t\tself.send_negotiate_cmd()\n\t\treturn\n#else:\n#\tlogging.debug('received wrong cmd: %s' % cmd)", "path": "viperpeers.py", "repo_name": "mayson/viperpeers", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 128}
{"docstring": "#repeat some code for faster access\n", "func_signal": "def get_op_list(self):\n", "code": "oplist=\"$OpList \"\nfor user in self.nicks.values():\n\tif user.level in self.oplevels:\n\t\toplist+=user.nick+\"$$\"\nreturn oplist+'|'", "path": "viperpeers.py", "repo_name": "mayson/viperpeers", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 128}
{"docstring": "# params: 'message'\n\n", "func_signal": "def SetMotd(self,addr, params=[]):\n", "code": "if len(params)<=0:\n        return self.hub._('Params error')\n\nself.hub.settings['motd']['message']=' '.join(params)\n\nreturn self.hub._('Success')", "path": "plugins\\motd.py", "repo_name": "mayson/viperpeers", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 128}
{"docstring": "#params: ['topic']\n", "func_signal": "def SetTopic(self,addr,params=[]):\n", "code": "if len(params)>=1:\n\ttopic=' '.join(params)\n\tself.core_settings['topic']=topic\n\tself.send_to_all('$HubTopic %s|' % topic)\n\treturn self._('Success')\nelse:\n\treturn self._('Params error')", "path": "viperpeers.py", "repo_name": "mayson/viperpeers", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 128}
{"docstring": "# Params should be: 'nick' 'level' 'passwd'\n", "func_signal": "def AddReg(self,addr,params=[]):\n", "code": "if len(params)==3:\n\t# Check if 'nick' already registred\n\tif params[0] not in self.reglist:\n\t\tself.reglist[params[0]]={'level': params[1],'passwd':params[2]}\n\t\treturn self._('User Registred:\\n nick: %s\\n level: %s\\n passwd:%s') % (params[0],params[1],params[2])\n\telse:\n\t\treturn self._('User already registred')\nelse:\n\treturn self._('Params error.')", "path": "viperpeers.py", "repo_name": "mayson/viperpeers", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 128}
{"docstring": "# Params should be: 'plugin'\n", "func_signal": "def UnloadPlugin(self,addr,params=[]):\n", "code": "logging.debug('unloading plugin')\nif len(params)==1:\n\ttry:\n\t\tif params[0] in self.plugs:\n\t\t\tplug=self.plugs.pop(params[0])\n\t\t\tplug.unload()\n\t\t\tfor key in plug.commands.keys():\n\t\t\t\tself.commands.pop(key,None)\n\t\t\tfor key in plug.usercommands.keys():\n\t\t\t\tself.usercommands.pop(key,None)\n\t\t\tfor key, value in plug.slots.iteritems():\n\t\t\t\tif key in self.slots:\n\t\t\t\t\tif value in self.slots[key]:\n\t\t\t\t\t\tself.slots[key].remove(value)\n\t\t\tself.Gen_UC()\n\t\t\tself.send_usercommands_to_all()\n\t\t\treturn self._('Success')\n\t\telse:\n\t\t\treturn self._('Plugin not loaded')\n\texcept:\n\t\treturn self._('Plugin unload error: %s' % trace())\nelse:\n\treturn self._('Params error')", "path": "viperpeers.py", "repo_name": "mayson/viperpeers", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 128}
{"docstring": "\"\"\"\nReturns a formatted-for-humans file size.\n\n``precision``\nThe level of precision, defaults to 1\n\nExamples::\n\n>>> number_to_human_size(123)\n'123 Bytes'\n>>> number_to_human_size(1234)\n'1.2 KB'\n>>> number_to_human_size(12345)\n'12.1 KB'\n>>> number_to_human_size(1234567)\n'1.2 MB'\n>>> number_to_human_size(1234567890)\n'1.1 GB'\n>>> number_to_human_size(1234567890123)\n'1.1 TB'\n>>> number_to_human_size(1234567, 2)\n'1.18 MB'\n\"\"\"\n", "func_signal": "def number_to_human_size(size, precision=1):\n", "code": "if size == 1:\n\treturn \"1 Byte\"\nelif size < 1024:\n\treturn \"%d Bytes\" % size\nelif size < (1024**2):\n\treturn (\"%%.%if KB\" % precision) % (size / 1024.00)\nelif size < (1024**3):\n\treturn (\"%%.%if MB\" % precision) % (size / 1024.00**2)\nelif size < (1024**4):\n\t return (\"%%.%if GB\" % precision) % (size / 1024.00**3)\nelif size < (1024**5):\n\treturn (\"%%.%if TB\" % precision) % (size / 1024.00**4)\n\n\nreturn \"\"", "path": "viperpeers.py", "repo_name": "mayson/viperpeers", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 128}
{"docstring": "\"\"\"\nIf the event concerns a directory and the auto_add flag of the\ntargetted watch is set to True, a new watch is added on this\nnew directory, with the same attributes's values than those of\nthis watch.\n\"\"\"\n", "func_signal": "def process_IN_CREATE(self, raw_event):\n", "code": "if raw_event.mask & IN_ISDIR:\n    watch_ = self._watch_manager._wmd.get(raw_event.wd)\n    if watch_.auto_add:\n        addw = self._watch_manager.add_watch\n        newwd = addw(os.path.join(watch_.path, raw_event.name),\n                     watch_.mask, proc_fun=watch_.proc_fun,\n                     rec=False, auto_add=watch_.auto_add)\n\n        # Trick to handle mkdir -p /t1/t2/t3 where t1 is watched and\n        # t2 and t3 are created.\n        # Since the directory is new, then everything inside it\n        # must also be new.\n        base = os.path.join(watch_.path, raw_event.name)\n        if newwd[base] > 0:\n            for name in os.listdir(base):\n                inner = os.path.join(base, name)\n                if (os.path.isdir(inner) and\n                    self._watch_manager.get_wd(inner) is None):\n                    # Generate (simulate) creation event for sub\n                    # directories.\n                    rawevent = _RawEvent(newwd[base],\n                                         IN_CREATE | IN_ISDIR,\n                                         0, name)\n                    self._notifier._eventq.append(rawevent)\nreturn self.process_default(raw_event)", "path": "evserver\\other\\pyinotify.py", "repo_name": "brosner/evserver", "stars": 0, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"\nGet every wd from self._wmd if its path is under the path of\none (at least) of those in lpath. Doesn't follow symlinks.\n\n@param lpath: list of watch descriptor\n@type lpath: list of int\n@return: list of watch descriptor\n@rtype: list of int\n\"\"\"\n", "func_signal": "def __get_sub_rec(self, lpath):\n", "code": "for d in lpath:\n    root = self.get_path(d)\n    if root:\n        # always keep root\n        yield d\n    else:\n        # if invalid\n        continue\n\n    # nothing else to expect\n    if not os.path.isdir(root):\n        continue\n\n    # normalization\n    root = os.path.normpath(root)\n    # recursion\n    lend = len(root)\n    for iwd in self._wmd.items():\n        cur = iwd[1].path\n        pref = os.path.commonprefix([root, cur])\n        if root == os.sep or (len(pref) == lend and \\\n                              len(cur) > lend and \\\n                              cur[lend] == os.sep):\n            yield iwd[1].wd", "path": "evserver\\other\\pyinotify.py", "repo_name": "brosner/evserver", "stars": 0, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"\nSTATUS: the following bug has been fixed in the recent kernels (fixme:\nwhich version ?). Now it raises IN_DELETE_SELF instead.\n\nOld kernels are bugged, this event is raised when the watched item\nwas moved, so we must update its path, but under some circumstances it\ncan be impossible: if its parent directory and its destination\ndirectory aren't watched. The kernel (see include/linux/fsnotify.h)\ndoesn't bring us enough informations like the destination path of\nmoved items.\n\"\"\"\n", "func_signal": "def process_IN_MOVE_SELF(self, raw_event):\n", "code": "watch_ = self._watch_manager._wmd.get(raw_event.wd)\nsrc_path = watch_.path\nmv_ = self._mv.get(src_path)\nif mv_:\n    watch_.path = mv_[0]\nelse:\n    log.error(\"The path %s of this watch %s must not \"\n              \"be trusted anymore\" % (watch_.path, watch_))\n    if not watch_.path.endswith('-wrong-path'):\n        watch_.path += '-wrong-path'\n# FIXME: should we pass the cookie even if this is not standart?\nreturn self.process_default(raw_event)", "path": "evserver\\other\\pyinotify.py", "repo_name": "brosner/evserver", "stars": 0, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"\nThread's main loop. don't meant to be called by user directly.\nCall start() instead.\n\nEvents are read only once time every min(read_freq, timeout)\nseconds at best and only if the size to read is >= treshold.\n\"\"\"\n# Read and process events while _stop_event condition\n# remains unset.\n", "func_signal": "def loop(self):\n", "code": "while not self._stop_event.isSet():\n    self.process_events()\n    ref_time = time.time()\n    # There is a timeout here because without that poll() could\n    # block until an event is received and therefore\n    # _stop_event.isSet() would not be evaluated until then, thus\n    # this thread won't be able to stop its execution.\n    if self.check_events():\n        self._sleep(ref_time)\n        self.read_events()", "path": "evserver\\other\\pyinotify.py", "repo_name": "brosner/evserver", "stars": 0, "license": "None", "language": "python", "size": 576}
{"docstring": "''' make sure that the object will never be garbage-collected, to avoid mem problems with ctypes'''\n", "func_signal": "def inc_ref(obj):\n", "code": "references[id(obj)] = obj\nreturn obj", "path": "evserver\\utils.py", "repo_name": "brosner/evserver", "stars": 0, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"\nRemoves watch(s).\n\n@param wd: Watch Descriptor of the file or directory to unwatch.\n           Also accepts a list of WDs.\n@type wd: int or list of int.\n@param rec: Recursively removes watches on every already watched\n            subdirectories and subfiles.\n@type rec: bool\n@param quiet: if True raise an WatchManagerError exception on\n              error. See example not_quiet.py\n@type quiet: bool\n@return: dict of watch descriptors associated to booleans values.\n         True if the corresponding wd has been successfully\n         removed, False otherwise.\n@rtype: dict of int: bool\n\"\"\"\n", "func_signal": "def rm_watch(self, wd, rec=False, quiet=True):\n", "code": "lwd = self.__format_param(wd)\nif rec:\n    lwd = self.__get_sub_rec(lwd)\n\nret_ = {}  # return {wd: bool, ...}\nfor awd in lwd:\n    # remove watch\n    wd_ = LIBC.inotify_rm_watch(self._fd, awd)\n    if wd_ < 0:\n        ret_[awd] = False\n        err = 'rm_watch: cannot remove WD=%d' % awd\n        if quiet:\n            log.error(err)\n            continue\n        raise WatchManagerError(err, ret_)\n\n    ret_[awd] = True\n    log.debug('watch WD=%d (%s) removed' % (awd, self.get_path(awd)))\nreturn ret_", "path": "evserver\\other\\pyinotify.py", "repo_name": "brosner/evserver", "stars": 0, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"\nReturns the watch descriptor associated to path. This method\nhas an prohibitive cost, always prefer to keep the WD.\nIf path is unknown None is returned.\n\n@param path: path.\n@type path: str\n@return: WD or None.\n@rtype: int or None\n\"\"\"\n", "func_signal": "def get_wd(self, path):\n", "code": "path = os.path.normpath(path)\nfor iwd in self._wmd.iteritems():\n    if iwd[1].path == path:\n        return iwd[0]\nlog.debug('get_wd: unknown path %s' % path)", "path": "evserver\\other\\pyinotify.py", "repo_name": "brosner/evserver", "stars": 0, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"Count the number of instances for each type tracked by the GC.\n\nNote that the GC does not track simple objects like int or str.\n\nNote that classes with the same name but defined in different modules\nwill be lumped together.\n\"\"\"\n", "func_signal": "def typestats():\n", "code": "stats = {}\nfor o in gc.get_objects():\n    stats.setdefault(type(o).__name__, 0)\n    stats[type(o).__name__] += 1\nreturn stats", "path": "evserver\\other\\objgraph.py", "repo_name": "brosner/evserver", "stars": 0, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"\n\n@param wm: Watch Manager.\n@type wm: WatchManager instance\n@param notifier: notifier.\n@type notifier: Instance of Notifier.\n\"\"\"\n", "func_signal": "def __init__(self, wm, notifier):\n", "code": "self._watch_manager = wm  # watch manager\nself._notifier = notifier  # notifier\nself._mv_cookie = {}  # {cookie(int): (src_path(str), date), ...}\nself._mv = {}  # {src_path(str): (dst_path(str), date), ...}", "path": "evserver\\other\\pyinotify.py", "repo_name": "brosner/evserver", "stars": 0, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"\n@return: stored value.\n@rtype: int\n\"\"\"\n", "func_signal": "def get_val(self):\n", "code": "oldv = ctypes.c_int(0)\nsize = ctypes.c_int(ctypes.sizeof(oldv))\nLIBC.sysctl(self._attr, 3,\n            ctypes.c_voidp(ctypes.addressof(oldv)),\n            ctypes.addressof(size),\n            None, 0)\nreturn oldv.value", "path": "evserver\\other\\pyinotify.py", "repo_name": "brosner/evserver", "stars": 0, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"\nAttach attributes (contained in dict_) to self.\n\"\"\"\n", "func_signal": "def __init__(self, dict_):\n", "code": "for tpl in dict_.iteritems():\n    setattr(self, *tpl)", "path": "evserver\\other\\pyinotify.py", "repo_name": "brosner/evserver", "stars": 0, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"\nWatch a transient file, which will be created and deleted frequently\nover time (e.g. pid file).\n\n@param filename: Filename.\n@type filename: string\n@param mask: Bitmask of events, should contain IN_CREATE and IN_DELETE.\n@type mask: int\n@param proc_class: ProcessEvent (or of one of its subclass), beware of\n                   accepting a ProcessEvent's instance as argument into\n                   __init__, see transient_file.py example for more\n                   details.\n@type proc_class: ProcessEvent's instance or of one of its subclasses.\n@return: See add_watch().\n@rtype: See add_watch().\n\"\"\"\n", "func_signal": "def watch_transient_file(self, filename, mask, proc_class):\n", "code": "dirname = os.path.dirname(filename)\nif dirname == '':\n    return {}  # Maintains coherence with add_watch()\nbasename = os.path.basename(filename)\n# Assuming we are watching at least for IN_CREATE and IN_DELETE\nmask |= IN_CREATE | IN_DELETE\n\ndef cmp_name(event):\n    return basename == event.name\nreturn self.add_watch(dirname, mask,\n                      proc_fun=proc_class(ChainIf(func=cmp_name)),\n                      rec=False,\n                      auto_add=False, do_glob=False)", "path": "evserver\\other\\pyinotify.py", "repo_name": "brosner/evserver", "stars": 0, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"\nEnable chaining of ProcessEvent instances.\n\n@param pevent: optional callable object, will be called on event\n               processing (before self).\n@type pevent: callable\n@param kargs: optional arguments delagated to template method my_init\n@type kargs: dict\n\"\"\"\n", "func_signal": "def __init__(self, pevent=None, **kargs):\n", "code": "self.pevent = pevent\nself.my_init(**kargs)", "path": "evserver\\other\\pyinotify.py", "repo_name": "brosner/evserver", "stars": 0, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"Return an object at a given memory address.\n\nThe reverse of id(obj):\n\n    >>> at(id(obj)) is obj\n    True\n\nNote that this function does not work on objects that are not tracked by\nthe GC (e.g. ints or strings).\n\"\"\"\n", "func_signal": "def at(addr):\n", "code": "for o in gc.get_objects():\n    if id(o) == addr:\n        return o\nreturn None", "path": "evserver\\other\\objgraph.py", "repo_name": "brosner/evserver", "stars": 0, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"\nReturns the path associated to WD, if WD is unknown\nNone is returned.\n\n@param wd: watch descriptor.\n@type wd: int\n@return: path or None.\n@rtype: string or None\n\"\"\"\n", "func_signal": "def get_path(self, wd):\n", "code": "watch_ = self._wmd.get(wd)\nif watch_:\n    return watch_.path\nlog.debug('get_path: unknown WD %d' % wd)", "path": "evserver\\other\\pyinotify.py", "repo_name": "brosner/evserver", "stars": 0, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"\n@return: String representation.\n@rtype: str\n\"\"\"\n", "func_signal": "def __repr__(self):\n", "code": "s = ' '.join(['%s%s%s' % (color_theme.field_name(attr),\n                          color_theme.punct('='),\n                          color_theme.field_value(getattr(self,\n                                                          attr))) \\\n              for attr in self.__dict__ if not attr.startswith('_')])\n\ns = '%s%s %s %s' % (color_theme.punct('<'),\n                    color_theme.class_name(self.__class__.__name__),\n                    s,\n                    color_theme.punct('>'))\nreturn s", "path": "evserver\\other\\pyinotify.py", "repo_name": "brosner/evserver", "stars": 0, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"\nMap the source path with the destination path (+ date for\ncleaning).\n\"\"\"\n", "func_signal": "def process_IN_MOVED_TO(self, raw_event):\n", "code": "watch_ = self._watch_manager._wmd.get(raw_event.wd)\npath_ = watch_.path\ndst_path = os.path.normpath(os.path.join(path_, raw_event.name))\nmv_ = self._mv_cookie.get(raw_event.cookie)\nif mv_:\n    self._mv[mv_[0]] = (dst_path, datetime.now())\nreturn self.process_default(raw_event, {'cookie': raw_event.cookie})", "path": "evserver\\other\\pyinotify.py", "repo_name": "brosner/evserver", "stars": 0, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"\nCleanup (delete) old (>1mn) records contained in self._mv_cookie\nand self._mv.\n\"\"\"\n", "func_signal": "def cleanup(self):\n", "code": "date_cur_ = datetime.now()\nfor seq in [self._mv_cookie, self._mv]:\n    for k in seq.keys():\n       if (date_cur_ - seq[k][1]) > timedelta(minutes=1):\n           log.debug('cleanup: deleting entry %s' % seq[k][0])\n           del seq[k]", "path": "evserver\\other\\pyinotify.py", "repo_name": "brosner/evserver", "stars": 0, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"\nClose the inotify's instance (close its file descriptor).\nIt destroys all existing watches, pending events,...\n\"\"\"\n", "func_signal": "def stop(self):\n", "code": "self._pollobj.unregister(self._fd)\nos.close(self._fd)", "path": "evserver\\other\\pyinotify.py", "repo_name": "brosner/evserver", "stars": 0, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"\nReturn the event name associated to mask. IN_ISDIR is appended when\nappropriate. Note: only one event is returned, because only one is\nraised once at a time.\n\n@param mask: mask.\n@type mask: int\n@return: event name.\n@rtype: str\n\"\"\"\n", "func_signal": "def maskname(mask):\n", "code": "ms = mask\nname = '%s'\nif mask & IN_ISDIR:\n    ms = mask - IN_ISDIR\n    name = '%s|IN_ISDIR'\nreturn name % EventsCodes.ALL_VALUES[ms]", "path": "evserver\\other\\pyinotify.py", "repo_name": "brosner/evserver", "stars": 0, "license": "None", "language": "python", "size": 576}
{"docstring": "''' Initalizes the field print name lookup\n:return: The built name cache\n'''\n", "func_signal": "def init_name_map(self):\n", "code": "self._name_map = [f.name for f in self._fields()]\nreturn self._name_map", "path": "rosetta\\core\\options.py", "repo_name": "bashwork/rosetta", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "''' Convert a type to XML text\n:param input: The type do serialize\n:return: The input serialized to xml\n'''\n", "func_signal": "def serialize(input):\n", "code": "data = input.__dict__\nname = input.__class__.__name__\nmod  = input.__class__.__module__\n\nroot = etree.Element(name, module=mod)\nfor value in data.iteritems():\n    node = etree.SubElement(root, value[0],\n        type=type(value[1]).__name__)\n    node.text = \"%s\" % value[1]\nreturn etree.tostring(root, xml_declaration=True)", "path": "rosetta\\format\\xml.py", "repo_name": "bashwork/rosetta", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "''' command runner '''\n", "func_signal": "def run(self):\n", "code": "old_cwd = os.getcwd()\nfor entry in os.listdir('./api'):\n    os.chdir('./api/%s' % entry)\n    os.system('python build.py')\n    os.chdir(old_cwd)", "path": "setup.py", "repo_name": "bashwork/rosetta", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "''' Convert serialized yaml back to a type\n:param input: The serialized yaml string\n:return: The initialized type\n'''\n", "func_signal": "def deserialize(input):\n", "code": "result = yaml.load(input)\nmodule, type = result['name'].split('//')\nhandle = _create(module, type)\nhandle.__dict__ = result['data']\nreturn handle", "path": "rosetta\\format\\soap.py", "repo_name": "bashwork/rosetta", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "''' Convert a type to yaml text\n:param input: The type do serialize\n:return: The input serialized to yaml\n'''\n", "func_signal": "def serialize(input):\n", "code": "handle = input.__class__\ndata   = input.__dict__\nname   = '%s//%s' % (handle.__module__, handle.__name__)\nresult = {'name':name, 'data':data}\nreturn yaml.dump(result)", "path": "rosetta\\format\\soap.py", "repo_name": "bashwork/rosetta", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "''' Check to see if an executable exists\n:param program: The program to check for\n:return: The full path of the executable or None if not found\n'''\n", "func_signal": "def which(program):\n", "code": "fpath, name = os.path.split(program)\nif fpath:\n    if is_exe(program):\n        return program\nelse:\n    for path in os.environ[\"PATH\"].split(os.pathsep):\n        exe_file = os.path.join(path, program)\n        if is_exe(exe_file):\n            return exe_file\nreturn None", "path": "api\\doxygen\\build.py", "repo_name": "bashwork/rosetta", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "''' ...hello...\n:param cls: The class calling down to us\n:param name: The attribute to add\n'''\n", "func_signal": "def contribute_to_class(self, cls, name):\n", "code": "cls._meta = self\nself.object_name = cls.__name__\nself.module_name = self.object_name.lower()\n\n# apply overriden meta values\nif self.meta:\n    meta_attrs = self.meta.__dict__.copy()\n    for attr_name in DEFAULT_NAMES:\n        if attr_name in meta_attrs:\n            setattr(self, attr_name, meta_attrs.pop(attr_name))\n        elif hasattr(self.meta, attr_name): # I don't know why this is done\n            setattr(self, attr_name, getattr(self.meta, attr_name))", "path": "rosetta\\core\\options.py", "repo_name": "bashwork/rosetta", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "''' Convert serialized XML back to a type\n:param input: The serialized XML string\n:return: The initialized type\n'''\n", "func_signal": "def deserialize(input):\n", "code": "root   = etree.fromstring(input)\nhandle = _create(root.attrib['module'], root.tag)\nresult = {}\nfor child in root.iterchildren():\n    result[child.tag] = eval('%s(\"%s\")' %\n        (child.attrib['type'], child.text))\nhandle.__dict__ = result\nreturn handle", "path": "rosetta\\format\\xml.py", "repo_name": "bashwork/rosetta", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "''' Appends a field to the field collection\nWe use the order column to insert the fields in the correct\norder that they were declared as.\n\n:param field: The field to append to the collection\n'''\n", "func_signal": "def add_field(self, field):\n", "code": "self.local_fields.insert(bisect(self.local_fields, field), field)\n# invalidate the cache\nif hasattr(self, '_name_map'):\n    del self._name_map", "path": "rosetta\\core\\options.py", "repo_name": "bashwork/rosetta", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "\"\"\"\nRegister a set of models as belonging to an app.\n\"\"\"\n", "func_signal": "def register_models(self, app_label, *models):\n", "code": "for model in models:\n    # Store as 'name: model' pair in a dictionary\n    # in the app_models dictionary\n    model_name = model._meta.object_name.lower()\n    model_dict = self.app_models.setdefault(app_label, SortedDict())\n    if model_name in model_dict:\n        # The same model may be imported via different paths (e.g.\n        # appname.models and project.appname.models). We use the source\n        # filename as a means to detect identity.\n        fname1 = os.path.abspath(sys.modules[model.__module__].__file__)\n        fname2 = os.path.abspath(sys.modules[model_dict[model_name].__module__].__file__)\n        # Since the filename extension could be .py the first time and\n        # .pyc or .pyo the second time, ignore the extension when\n        # comparing.\n        if os.path.splitext(fname1)[0] == os.path.splitext(fname2)[0]:\n            continue\n    model_dict[model_name] = model", "path": "rosetta\\core\\loading.py", "repo_name": "bashwork/rosetta", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "''' Return a list of all the field print names\n:return: List of all the field names\n'''\n", "func_signal": "def get_all_field_names(self):\n", "code": "try:\n    cache = self._name_map\nexcept AttributeError:\n    cache = self.init_name_map()\nnames = cache\nnames.sort()\nreturn names", "path": "rosetta\\core\\options.py", "repo_name": "bashwork/rosetta", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "''' Initialize a new instance of the type\n'''\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "self.__count  = 0\nself._fields = {}\n\nfor key, val in kwargs.iteritems():\n    self._add_enum(key, val)\nfor key in args:\n    self._add_enum(key)\n\nself.__readonly = True", "path": "rosetta\\utils\\enum.py", "repo_name": "bashwork/rosetta", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "''' Initialize a new instance\n:param meta: The base meta object\n'''\n", "func_signal": "def __init__(self, meta):\n", "code": "self.local_fields = []\nself.meta = meta", "path": "rosetta\\core\\options.py", "repo_name": "bashwork/rosetta", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "\"\"\"\nFill in all the cache information. This method is threadsafe, in the\nsense that every caller will see the same state upon return, and if the\ncache is already initialised, it does no work.\n\"\"\"\n", "func_signal": "def _populate(self):\n", "code": "if self.loaded:\n    return\nself.write_lock.acquire()\ntry:\n    if self.loaded:\n        return\n    for app_name in settings.INSTALLED_APPS:\n        if app_name in self.handled:\n            continue\n        self.load_app(app_name, True)\n    if not self.nesting_level:\n        for app_name in self.postponed:\n            self.load_app(app_name)\n        self.loaded = True\nfinally:\n    self.write_lock.release()", "path": "rosetta\\core\\loading.py", "repo_name": "bashwork/rosetta", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "''' Helper to block writing to the fields\n'''\n", "func_signal": "def __setattr__(self, key, value):\n", "code": "if self.__readonly:\n    self._exception()\nsuper(Enum, self).__setattr__(key, value)", "path": "rosetta\\utils\\enum.py", "repo_name": "bashwork/rosetta", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "\"\"\"\nReturns the model matching the given app_label and case-insensitive\nmodel_name.\n\nReturns None if no model is found.\n\"\"\"\n", "func_signal": "def get_model(self, app_label, model_name, seed_cache=True):\n", "code": "if seed_cache:\n    self._populate()\nreturn self.app_models.get(app_label, SortedDict()).get(model_name.lower())", "path": "rosetta\\core\\loading.py", "repo_name": "bashwork/rosetta", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "''' Returns the requested field\n:param name: The field name to retrieve\n:return: The requested field or Exception if field does not exist\n'''\n", "func_signal": "def get_field(self, name):\n", "code": "for field in self.fields:\n    if field.name == name:\n        return field\nraise FieldDoesNotExist('no field named %s' % name)", "path": "rosetta\\core\\options.py", "repo_name": "bashwork/rosetta", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "''' Returns a field by its print name.\n:param name: The field to retrieve\n:return: The requested field\n\nUses a cache internally, so after the first access, this is very fast.\n'''\n", "func_signal": "def get_field_by_name(self, name):\n", "code": "try:\n    try:\n        return self._name_map[name]\n    except AttributeError:\n        cache = self.init_name_map()\n        return cache[name]\nexcept KeyError:\n    raise FieldDoesNotExist('has no field named %s' % (name))", "path": "rosetta\\core\\options.py", "repo_name": "bashwork/rosetta", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "\"\"\"\nGiven a module containing models, returns a list of the models.\nOtherwise returns a list of all installed models.\n\"\"\"\n", "func_signal": "def get_models(self, app_mod=None):\n", "code": "self._populate()\nif app_mod:\n    return self.app_models.get(app_mod.__name__.split('.')[-2], SortedDict()).values()\nelse:\n    model_list = []\n    for app_entry in self.app_models.itervalues():\n        model_list.extend(app_entry.values())\n    return model_list", "path": "rosetta\\core\\loading.py", "repo_name": "bashwork/rosetta", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "\"\"\"\nLoads the app with the provided fully qualified name, and returns the\nmodel module.\n\"\"\"\n", "func_signal": "def load_app(self, app_name, can_postpone=False):\n", "code": "self.handled[app_name] = None\nself.nesting_level += 1\nmod = __import__(app_name, {}, {}, ['models'])\nself.nesting_level -= 1\nif not hasattr(mod, 'models'):\n    if can_postpone:\n        # Either the app has no models, or the package is still being\n        # imported by Python and the model module isn't available yet.\n        # We will check again once all the recursion has finished (in\n        # populate).\n        self.postponed.append(app_name)\n    return None\nif mod.models not in self.app_store:\n    self.app_store[mod.models] = len(self.app_store)\nreturn mod.models", "path": "rosetta\\core\\loading.py", "repo_name": "bashwork/rosetta", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "'''In the dictionary d, it adds subkey:subitem to the dictionary found in kword. If\nkword isn't found, then it is created, if the subkey already exists, then it is\noverwritten.\n'''\n", "func_signal": "def _addsubkeys(d,kword,subkey,subitem):\n", "code": "tempdict=d.pop(kword,{})\ntempdict.update({subkey:subitem})\nd.update({kword:tempdict})\nreturn d", "path": "bloks\\skatolo.py", "repo_name": "SimonTite/skatolo", "stars": 1, "license": "None", "language": "python", "size": 1592}
{"docstring": "# Retrieve the author stored in the current session\n# None if not defined\n", "func_signal": "def index(self):\n", "code": "author = cherrypy.session.get('author', None)\n\npage = [_header]\n\nif author:\n    page.append(\"\"\"\n    <div><span>Hello %s, please leave us a note.\n    <a href=\"author\">Switch identity</a>.</span></div>\"\"\" % (author,))\n    page.append(_note_form)\nelse:\n    page.append(\"\"\"<div><a href=\"author\">Set your identity</a></span></div>\"\"\")\n\nnotes = _notes[:]\nnotes.reverse()\nfor note in notes:\n    page.append(self._render_note(note))\n\npage.append(_footer)\n# Returns to the CherryPy server the page to render\nreturn page", "path": "note.py", "repo_name": "SimonTite/skatolo", "stars": 1, "license": "None", "language": "python", "size": 1592}
{"docstring": "# Retrieve the note attached to the given id\n", "func_signal": "def note(self, id):\n", "code": "try:\n    note = _notes[int(id)]\nexcept:\n    # If the ID was not valid, let's tell the\n    # client we did not find it\n    raise cherrypy.NotFound\nreturn [_header, self._render_note(note), _footer]", "path": "sample code\\note.py", "repo_name": "SimonTite/skatolo", "stars": 1, "license": "None", "language": "python", "size": 1592}
{"docstring": "# Increase the silly hit counter\n", "func_signal": "def index(self):\n", "code": "count = cherrypy.session.get('count', 0) + 1\n\n# Store the new value in the session dictionary\ncherrypy.session['count'] = count\n\n# And display a silly hit count message!\nreturn '''\n    During your current session, you've viewed this\n    page %s times! Your life is a patio of fun!\n''' % count", "path": "tut07_sessions.py", "repo_name": "SimonTite/skatolo", "stars": 1, "license": "None", "language": "python", "size": 1592}
{"docstring": "#NB, note no content is allowed\n", "func_signal": "def __init__(self,picname,alt=\"\",**kwords):\n", "code": "_addsubkeys(kwords,\"atts\",\"src\",\"/img/\"+picname)\n_addsubkeys(kwords,\"atts\",\"alt\",alt)\n_addsubkeys(kwords,\"styles\",\"width\",bc.TN_DEFAULT)\nsuper(Picture,self).__init__(**kwords)", "path": "bloks\\skatolo.py", "repo_name": "SimonTite/skatolo", "stars": 1, "license": "None", "language": "python", "size": 1592}
{"docstring": "# Note that we call the header and footer methods inherited\n# from the Page class!\n", "func_signal": "def index(self):\n", "code": "return self.header() + '''\n    <p>\n    Isn't this exciting? There's\n    <a href=\"./another/\">another page</a>, too!\n    </p>\n''' + self.footer()", "path": "tut05_derived_objects.py", "repo_name": "SimonTite/skatolo", "stars": 1, "license": "None", "language": "python", "size": 1592}
{"docstring": "# Retrieve the author stored in the current session\n# None if not defined\n", "func_signal": "def index(self):\n", "code": "author = cherrypy.session.get('author', None)\n\npage = [_header]\n\nif author:\n    page.append(\"\"\"\n    <div><span>Hello %s, please leave us a note.\n    <a href=\"author\">Switch identity</a>.</span></div>\"\"\" % (author,))\n    page.append(_note_form)\nelse:\n    page.append(\"\"\"<div><a href=\"author\">Set your identity</a></span></div>\"\"\")\n\nnotes = _notes[:]\nnotes.reverse()\nfor note in notes:\n    page.append(self._render_note(note))\n\npage.append(_footer)\n# Returns to the CherryPy server the page to render\nreturn page", "path": "sample code\\note.py", "repo_name": "SimonTite/skatolo", "stars": 1, "license": "None", "language": "python", "size": 1592}
{"docstring": "# Here we react depending on the virtualPath -- the part of the\n# path that could not be mapped to an object method. In a real\n# application, we would probably do some database lookups here\n# instead of the silly if/elif/else construct.\n", "func_signal": "def default(self, user):\n", "code": "if user == 'remi':\n    out = \"Remi Delon, CherryPy lead developer\"\nelif user == 'hendrik':\n    out = \"Hendrik Mans, CherryPy co-developer & crazy German\"\nelif user == 'lorenzo':\n    out = \"Lorenzo Lamas, famous actor and singer!\"\nelse:\n    out = \"Unknown user. :-(\"\n\nreturn '%s (<a href=\"./\">back</a>)' % out", "path": "tut06_default_method.py", "repo_name": "SimonTite/skatolo", "stars": 1, "license": "None", "language": "python", "size": 1592}
{"docstring": "# Let's make up a list of users for presentation purposes\n", "func_signal": "def index(self):\n", "code": "users = ['Remi', 'Carlos', 'Hendrik', 'Lorenzo Lamas']\n\n# Every yield line adds one part to the total result body.\nyield self.header()\nyield \"<h3>List of users:</h3>\"\n\nfor user in users:\n    yield \"%s<br/>\" % user\n\nyield self.footer()", "path": "tut08_generators_and_yield.py", "repo_name": "SimonTite/skatolo", "stars": 1, "license": "None", "language": "python", "size": 1592}
{"docstring": "\"\"\"Helper to render a note into HTML\"\"\"\n", "func_signal": "def _render_note(self, note):\n", "code": "return _note_view % (note,\n                     note.author,\n                     time.strftime(\"%a, %d %b %Y %H:%M:%S\",\n                                   note.timestamp),\n                     note.id,\n                     note.id)", "path": "sample code\\note.py", "repo_name": "SimonTite/skatolo", "stars": 1, "license": "None", "language": "python", "size": 1592}
{"docstring": "\"\"\"Return the backup file name based on a generation number.\n\nExamples:\nget_backup_name(\"myfile.txt\") returns (\"myfile.txt\")\nget_backup_name(\"myfile.txt\",1) returns (\"myfile.B1.txt\")\nget_backup_name(\"myfile.txt\",2) returns (\"myfile.B2.txt\")\n...and so on.\n\n\"\"\"\n", "func_signal": "def get_backup_name(filename, generation=0, infix=\"B\"):\n", "code": "base, ext = splitext(filename)\nif generation == 0:\n    return filename\nelse:\n    return base + \".\" + infix + str(generation) + ext", "path": "py-utils\\age.py", "repo_name": "SimonTite/skatolo", "stars": 1, "license": "None", "language": "python", "size": 1592}
{"docstring": "'''In the dictionary d, it adds an item to the list found under kword. If the kword\nisn't found, then it is created.\n'''\n", "func_signal": "def _addsublist(d,kword,item):\n", "code": "templist=d.pop(kword,[])\nif item not in templist:\n    templist.append(item)\nd.update({kword:templist})\nreturn d", "path": "bloks\\skatolo.py", "repo_name": "SimonTite/skatolo", "stars": 1, "license": "None", "language": "python", "size": 1592}
{"docstring": "# Retrieve the author stored in the current session, None if not defined.\n", "func_signal": "def index(self):\n", "code": "author = cherrypy.session.get('author', None)\npage = [_header]\nif author:\n    page.append(\"\"\"\n    <div><span>Hello %s, please leave us a note.\n    <a href=\"author\">Switch identity</a>.</span></div>\"\"\"\n    %(author,))\n    page.append(_note_form)\nelse:\n    page.append(\"\"\"<div><span><a href=\"author\">Set your identity</a>\n    </span></div>\"\"\")\nnotes = _notes[:]\nnotes.reverse()\nfor note in notes:\n    page.append(self._render_note(note))\npage.append(_footer)\n# Returns to the CherryPy server to render the page\nreturn page", "path": "note.B1.py", "repo_name": "SimonTite/skatolo", "stars": 1, "license": "None", "language": "python", "size": 1592}
{"docstring": "# simple function to toggle tracebacks on and off\n", "func_signal": "def toggleTracebacks(self):\n", "code": "tracebacks = cherrypy.request.show_tracebacks\ncherrypy.config.update({'request.show_tracebacks': not tracebacks})\n\n# redirect back to the index\nraise cherrypy.HTTPRedirect('/')", "path": "tut10_http_errors.py", "repo_name": "SimonTite/skatolo", "stars": 1, "license": "None", "language": "python", "size": 1592}
{"docstring": "# display some links that will result in errors\n", "func_signal": "def index(self):\n", "code": "tracebacks = cherrypy.request.show_tracebacks\nif tracebacks:\n    trace = 'off'\nelse:\n    trace = 'on'\n\nreturn \"\"\"\n<html><body>\n    <h2><a href=\"toggleTracebacks\">Toggle tracebacks %s</a></h2>\n    <p><a href=\"/doesNotExist\">Click me; I'm a broken link!</a></p>\n    <p><a href=\"/error?code=403\">Use a custom an error page from a file.</a></p>\n    <p>These errors are explicitly raised by the application:</p>\n    <ul>\n        <li><a href=\"/error?code=400\">400</a></li>\n        <li><a href=\"/error?code=401\">401</a></li>\n        <li><a href=\"/error?code=402\">402</a></li>\n        <li><a href=\"/error?code=500\">500</a></li>\n    </ul>\n    <p><a href=\"/messageArg\">You can also set the response body\n    when you raise an error.</a></p>\n</body></html>\n\"\"\" % trace", "path": "tut10_http_errors.py", "repo_name": "SimonTite/skatolo", "stars": 1, "license": "None", "language": "python", "size": 1592}
{"docstring": "\"\"\"Helper to render a note into HTML\"\"\"\n", "func_signal": "def _render_note(self, note):\n", "code": "return _note_view % (note,\n                     note.author,\n                     time.strftime(\"%a, %d %b %Y %H:%M:%S\",\n                                   note.timestamp),\n                     note.id,\n                     note.id)", "path": "note.py", "repo_name": "SimonTite/skatolo", "stars": 1, "license": "None", "language": "python", "size": 1592}
{"docstring": "# Retrieve the note attached to the given id\n", "func_signal": "def note(self, id):\n", "code": "try:\n    note = _notes[int(id)]\nexcept:\n    # if id not valid, tell client not found\n    raise cherrypy.NotFound\nreturn [_header, self._render_note(note), _footer]", "path": "note.B1.py", "repo_name": "SimonTite/skatolo", "stars": 1, "license": "None", "language": "python", "size": 1592}
{"docstring": "\"\"\"Helper to render a note into HTML\"\"\"\n", "func_signal": "def _render_note(self, note):\n", "code": "return _note_view % (note, note.author,\n    time.strftime(\"%a, %d %b %Y %H:%M:%S\",note.timestamp),\n    note.id, note.id)", "path": "note.B1.py", "repo_name": "SimonTite/skatolo", "stars": 1, "license": "None", "language": "python", "size": 1592}
{"docstring": "# Retrieve the note attached to the given id\n", "func_signal": "def note(self, id):\n", "code": "try:\n    note = _notes[int(id)]\nexcept:\n    # If the ID was not valid, let's tell the\n    # client we did not find it\n    raise cherrypy.NotFound\nreturn [_header, self._render_note(note), _footer]", "path": "note.py", "repo_name": "SimonTite/skatolo", "stars": 1, "license": "None", "language": "python", "size": 1592}
{"docstring": "# CherryPy passes all GET and POST variables as method parameters.\n# It doesn't make a difference where the variables come from, how\n# large their contents are, and so on.\n#\n# You can define default parameter values as usual. In this\n# example, the \"name\" parameter defaults to None so we can check\n# if a name was actually specified.\n\n", "func_signal": "def greetUser(self, name = None):\n", "code": "if name:\n    # Greet the user!\n    return \"Hey %s, what's up?\" % name\nelse:\n    if name is None:\n        # No name was specified\n        return 'Please enter your name <a href=\"./\">here</a>.'\n    else:\n        return 'No, really, enter your name <a href=\"./\">here</a>.'", "path": "tut03_get_and_post.py", "repo_name": "SimonTite/skatolo", "stars": 1, "license": "None", "language": "python", "size": 1592}
{"docstring": "# Calling set_continue unconditionally would break unit test\n# coverage reporting, as Bdb.set_continue calls sys.settrace(None).\n", "func_signal": "def set_continue(self):\n", "code": "if self.__debugger_used:\n    pdb.Pdb.set_continue(self)", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "rc/femhub", "stars": 1, "license": "other", "language": "python", "size": 1416}
{"docstring": "\"\"\"\nReturn the compiler-flags associated with the future features that\nhave been imported into the given namespace (globs).\n\"\"\"\n", "func_signal": "def _extract_future_flags(globs):\n", "code": "flags = 0\nfor fname in __future__.all_feature_names:\n    feature = globs.get(fname, None)\n    if feature is getattr(__future__, fname):\n        flags |= feature.compiler_flag\nreturn flags", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "rc/femhub", "stars": 1, "license": "other", "language": "python", "size": 1416}
{"docstring": "\"\"\"\nReturn a list of the DocTests that are defined by the given\nobject's docstring, or by any of its contained objects'\ndocstrings.\n\nThe optional parameter `module` is the module that contains\nthe given object.  If the module is not specified or is None, then\nthe test finder will attempt to automatically determine the\ncorrect module.  The object's module is used:\n\n    - As a default namespace, if `globs` is not specified.\n    - To prevent the DocTestFinder from extracting DocTests\n      from objects that are imported from other modules.\n    - To find the name of the file containing the object.\n    - To help find the line number of the object within its\n      file.\n\nContained objects whose module does not match `module` are ignored.\n\nIf `module` is False, no attempt to find the module will be made.\nThis is obscure, of use mostly in tests:  if `module` is False, or\nis None but cannot be found automatically, then all objects are\nconsidered to belong to the (non-existent) module, so all contained\nobjects will (recursively) be searched for doctests.\n\nThe globals for each DocTest is formed by combining `globs`\nand `extraglobs` (bindings in `extraglobs` override bindings\nin `globs`).  A new copy of the globals dictionary is created\nfor each DocTest.  If `globs` is not specified, then it\ndefaults to the module's `__dict__`, if specified, or {}\notherwise.  If `extraglobs` is not specified, then it defaults\nto {}.\n\n\"\"\"\n# If name was not specified, then extract it from the object.\n", "func_signal": "def find(self, obj, name=None, module=None, globs=None, extraglobs=None):\n", "code": "if name is None:\n    name = getattr(obj, '__name__', None)\n    if name is None:\n        raise ValueError(\"DocTestFinder.find: name must be given \"\n                \"when obj.__name__ doesn't exist: %r\" %\n                         (type(obj),))\n\n# Find the module that contains the given object (if obj is\n# a module, then module=obj.).  Note: this may fail, in which\n# case module will be None.\nif module is False:\n    module = None\nelif module is None:\n    module = inspect.getmodule(obj)\n\n# Read the module's source code.  This is used by\n# DocTestFinder._find_lineno to find the line number for a\n# given object's docstring.\ntry:\n    file = inspect.getsourcefile(obj) or inspect.getfile(obj)\n    source_lines = linecache.getlines(file)\n    if not source_lines:\n        source_lines = None\nexcept TypeError:\n    source_lines = None\n\n# Initialize globals, and merge in extraglobs.\nif globs is None:\n    if module is None:\n        globs = {}\n    else:\n        globs = module.__dict__.copy()\nelse:\n    globs = globs.copy()\nif extraglobs is not None:\n    globs.update(extraglobs)\n\n# Recursively expore `obj`, extracting DocTests.\ntests = []\nself._find(tests, obj, name, module, source_lines, globs, {})\n# Sort the tests by alpha order of names, for consistency in\n# verbose-mode output.  This was a feature of doctest in Pythons\n# <= 2.3 that got lost by accident in 2.4.  It was repaired in\n# 2.4.4 and 2.5.\ntests.sort()\nreturn tests", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "rc/femhub", "stars": 1, "license": "other", "language": "python", "size": 1416}
{"docstring": "\"\"\"Debug a single doctest docstring.\n\nProvide the module (or dotted name of the module) containing the\ntest to be debugged and the name (within the module) of the object\nwith the docstring with tests to be debugged.\n\"\"\"\n", "func_signal": "def debug(module, name, pm=False):\n", "code": "module = _normalize_module(module)\ntestsrc = testsource(module, name)\ndebug_script(testsrc, pm, module.__dict__)", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "rc/femhub", "stars": 1, "license": "other", "language": "python", "size": 1416}
{"docstring": "\"\"\"\nReturn true if the given object is defined in the given\nmodule.\n\"\"\"\n", "func_signal": "def _from_module(self, module, object):\n", "code": "if module is None:\n    return True\nelif inspect.isfunction(object):\n    return module.__dict__ is object.func_globals\nelif inspect.isclass(object):\n    return module.__name__ == object.__module__\nelif inspect.getmodule(object) is not None:\n    return module is inspect.getmodule(object)\nelif hasattr(object, '__module__'):\n    return module.__name__ == object.__module__\nelif isinstance(object, property):\n    return True # [XX] no way not be sure.\nelse:\n    raise ValueError(\"object must be a class or function\")", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "rc/femhub", "stars": 1, "license": "other", "language": "python", "size": 1416}
{"docstring": "\"\"\"\nEssentially the only subtle case:\n>>> _ellipsis_match('aa...aa', 'aaa')\nFalse\n\"\"\"\n", "func_signal": "def _ellipsis_match(want, got):\n", "code": "if ELLIPSIS_MARKER not in want:\n    return want == got\n\n# Find \"the real\" strings.\nws = want.split(ELLIPSIS_MARKER)\nassert len(ws) >= 2\n\n# Deal with exact matches possibly needed at one or both ends.\nstartpos, endpos = 0, len(got)\nw = ws[0]\nif w:   # starts with exact match\n    if got.startswith(w):\n        startpos = len(w)\n        del ws[0]\n    else:\n        return False\nw = ws[-1]\nif w:   # ends with exact match\n    if got.endswith(w):\n        endpos -= len(w)\n        del ws[-1]\n    else:\n        return False\n\nif startpos > endpos:\n    # Exact end matches required more characters than we have, as in\n    # _ellipsis_match('aa...aa', 'aaa')\n    return False\n\n# For the rest, we only need to find the leftmost non-overlapping\n# match for each piece.  If there's no overall match that way alone,\n# there's no overall match period.\nfor w in ws:\n    # w may be '' at times, if there are consecutive ellipses, or\n    # due to an ellipsis at the start or end of `want`.  That's OK.\n    # Search for an empty string succeeds, and doesn't change startpos.\n    startpos = got.find(w, startpos, endpos)\n    if startpos < 0:\n        return False\n    startpos += len(w)\n\nreturn True", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "rc/femhub", "stars": 1, "license": "other", "language": "python", "size": 1416}
{"docstring": "\"\"\"\nReport that the given example raised an unexpected exception.\n\"\"\"\n", "func_signal": "def report_unexpected_exception(self, out, test, example, exc_info):\n", "code": "out(self._failure_header(test, example) +\n    'Exception raised:\\n' + _indent(_exception_traceback(exc_info)))", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "rc/femhub", "stars": 1, "license": "other", "language": "python", "size": 1416}
{"docstring": "\"\"\"\nCreates the temporary directory for the worker.\n\n:type job: sage.dsage.database.job.Job\n:param job: a Job object\n\n\"\"\"\n\n", "func_signal": "def setup_tmp_dir(self, job):\n", "code": "cur_dir = os.getcwd() # keep a reference to the current directory\ntmp_job_dir = os.path.join(TMP_WORKER_FILES, job.job_id)\nif not os.path.isdir(TMP_WORKER_FILES):\n    os.mkdir(TMP_WORKER_FILES)\nif not os.path.isdir(tmp_job_dir):\n    os.mkdir(tmp_job_dir)\nos.chdir(tmp_job_dir)\nself.sage.eval(\"os.chdir('%s')\" % tmp_job_dir)\n\nreturn tmp_job_dir", "path": "spkg\\standard\\sage_scripts\\dsage_worker.py", "repo_name": "rc/femhub", "stars": 1, "license": "other", "language": "python", "size": 1416}
{"docstring": "\"\"\"A unittest suite for one or more doctest files.\n\nThe path to each doctest file is given as a string; the\ninterpretation of that string depends on the keyword argument\n\"module_relative\".\n\nA number of options may be provided as keyword arguments:\n\nmodule_relative\n  If \"module_relative\" is True, then the given file paths are\n  interpreted as os-independent module-relative paths.  By\n  default, these paths are relative to the calling module's\n  directory; but if the \"package\" argument is specified, then\n  they are relative to that package.  To ensure os-independence,\n  \"filename\" should use \"/\" characters to separate path\n  segments, and may not be an absolute path (i.e., it may not\n  begin with \"/\").\n\n  If \"module_relative\" is False, then the given file paths are\n  interpreted as os-specific paths.  These paths may be absolute\n  or relative (to the current working directory).\n\npackage\n  A Python package or the name of a Python package whose directory\n  should be used as the base directory for module relative paths.\n  If \"package\" is not specified, then the calling module's\n  directory is used as the base directory for module relative\n  filenames.  It is an error to specify \"package\" if\n  \"module_relative\" is False.\n\nsetUp\n  A set-up function.  This is called before running the\n  tests in each file. The setUp function will be passed a DocTest\n  object.  The setUp function can access the test globals as the\n  globs attribute of the test passed.\n\ntearDown\n  A tear-down function.  This is called after running the\n  tests in each file.  The tearDown function will be passed a DocTest\n  object.  The tearDown function can access the test globals as the\n  globs attribute of the test passed.\n\nglobs\n  A dictionary containing initial global variables for the tests.\n\noptionflags\n  A set of doctest option flags expressed as an integer.\n\nparser\n  A DocTestParser (or subclass) that should be used to extract\n  tests from the files.\n\nencoding\n  An encoding that will be used to convert the files to unicode.\n\"\"\"\n", "func_signal": "def DocFileSuite(*paths, **kw):\n", "code": "suite = unittest.TestSuite()\n\n# We do this here so that _normalize_module is called at the right\n# level.  If it were called in DocFileTest, then this function\n# would be the caller and we might guess the package incorrectly.\nif kw.get('module_relative', True):\n    kw['package'] = _normalize_module(kw.get('package'))\n\nfor path in paths:\n    suite.addTest(DocFileTest(path, **kw))\n\nreturn suite", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "rc/femhub", "stars": 1, "license": "other", "language": "python", "size": 1416}
{"docstring": "\"\"\"\nChecks for signs of exceptions or errors in the output.\n\n:type sage_output: string\n:param sage_output: output from the sage instance\n\n\"\"\"\n\n", "func_signal": "def check_failure(self, sage_output):\n", "code": "if sage_output == None:\n    return False\nelse:\n    sage_output = ''.join(sage_output)\n\nif 'Traceback' in sage_output:\n    return True\nelif 'Error' in sage_output:\n    return True\nelse:\n    return False", "path": "spkg\\standard\\sage_scripts\\dsage_worker.py", "repo_name": "rc/femhub", "stars": 1, "license": "other", "language": "python", "size": 1416}
{"docstring": "\"\"\"\nRecord the fact that the given DocTest (`test`) generated `f`\nfailures out of `t` tried examples.\n\"\"\"\n", "func_signal": "def __record_outcome(self, test, f, t):\n", "code": "f2, t2 = self._name2ft.get(test.name, (0,0))\nself._name2ft[test.name] = (f+f2, t+t2)\nself.failures += f\nself.tries += t", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "rc/femhub", "stars": 1, "license": "other", "language": "python", "size": 1416}
{"docstring": "\"\"\"\nReport that the test runner is about to process the given\nexample.  (Only displays a message if verbose=True)\n\"\"\"\n", "func_signal": "def report_start(self, out, test, example):\n", "code": "if self._verbose:\n    if example.want:\n        out('Trying:\\n' + _indent(example.source) +\n            'Expecting:\\n' + _indent(example.want))\n    else:\n        out('Trying:\\n' + _indent(example.source) +\n            'Expecting nothing\\n')", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "rc/femhub", "stars": 1, "license": "other", "language": "python", "size": 1416}
{"docstring": "\"\"\"\nclean_output attempts to clean up the output string from sage. \n\n:type sage_output: string\n:param sage_output: sys.stdout output from the child sage instance\n\n\"\"\"\n\n", "func_signal": "def clean_output(self, sage_output):\n", "code": "begin = sage_output.find(START_MARKER)\nif begin != -1:\n    self.got_output = True\n    begin += len(START_MARKER)\nelse:\n    begin = 0\nend = sage_output.find(END_MARKER)\nif end != -1:\n    end -= 1\nelse:\n    if not self.got_output:\n        end = 0\n    else:\n        end = len(sage_output)\noutput = sage_output[begin:end]\noutput = output.strip()\noutput = output.replace('\\r', '')\n\nif ('execfile' in output or 'load' in output) and self.got_output:\n    output = ''           \n    \nreturn output", "path": "spkg\\standard\\sage_scripts\\dsage_worker.py", "repo_name": "rc/femhub", "stars": 1, "license": "other", "language": "python", "size": 1416}
{"docstring": "\"\"\"\nExtract all doctest examples from the given string, and return\nthem as a list of `Example` objects.  Line numbers are\n0-based, because it's most common in doctests that nothing\ninteresting appears on the same line as opening triple-quote,\nand so the first interesting line is called \\\"line 1\\\" then.\n\nThe optional argument `name` is a name identifying this\nstring, and is only used for error messages.\n\"\"\"\n", "func_signal": "def get_examples(self, string, name='<string>'):\n", "code": "return [x for x in self.parse(string, name)\n        if isinstance(x, Example)]", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "rc/femhub", "stars": 1, "license": "other", "language": "python", "size": 1416}
{"docstring": "\"\"\"Sets the unittest option flags.\n\nThe old flag is returned so that a runner could restore the old\nvalue if it wished to:\n\n  >>> import doctest\n  >>> old = doctest._unittest_reportflags\n  >>> doctest.set_unittest_reportflags(REPORT_NDIFF |\n  ...                          REPORT_ONLY_FIRST_FAILURE) == old\n  True\n\n  >>> doctest._unittest_reportflags == (REPORT_NDIFF |\n  ...                                   REPORT_ONLY_FIRST_FAILURE)\n  True\n\nOnly reporting flags can be set:\n\n  >>> doctest.set_unittest_reportflags(ELLIPSIS)\n  Traceback (most recent call last):\n  ...\n  ValueError: ('Only reporting flags allowed', 8)\n\n  >>> doctest.set_unittest_reportflags(old) == (REPORT_NDIFF |\n  ...                                   REPORT_ONLY_FIRST_FAILURE)\n  True\n\"\"\"\n", "func_signal": "def set_unittest_reportflags(flags):\n", "code": "global _unittest_reportflags\n\nif (flags & REPORTING_FLAGS) != flags:\n    raise ValueError(\"Only reporting flags allowed\", flags)\nold = _unittest_reportflags\n_unittest_reportflags = flags\nreturn old", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "rc/femhub", "stars": 1, "license": "other", "language": "python", "size": 1416}
{"docstring": "\"\"\"\nReturn a DocTest for the given object, if it defines a docstring;\notherwise, return None.\n\"\"\"\n# Extract the object's docstring.  If it doesn't have one,\n# then return None (no test for this object).\n", "func_signal": "def _get_test(self, obj, name, module, globs, source_lines):\n", "code": "if isinstance(obj, basestring):\n    docstring = obj\nelse:\n    try:\n        if obj.__doc__ is None:\n            docstring = ''\n        else:\n            docstring = obj.__doc__\n            if not isinstance(docstring, basestring):\n                docstring = str(docstring)\n    except (TypeError, AttributeError):\n        docstring = ''\n\n# Find the docstring's location in the file.\nlineno = self._find_lineno(obj, source_lines)\n\n# Don't bother if the docstring is empty.\nif self._exclude_empty and not docstring:\n    return None\n\n# Return a DocTest for this object.\nif module is None:\n    filename = None\nelse:\n    filename = getattr(module, '__file__', module.__name__)\n    if filename[-4:] in (\".pyc\", \".pyo\"):\n        filename = filename[:-1]\nreturn self._parser.get_doctest(docstring, globs, name,\n                                filename, lineno)", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "rc/femhub", "stars": 1, "license": "other", "language": "python", "size": 1416}
{"docstring": "\"\"\"\nDivide the given string into examples and intervening text,\nand return them as a list of alternating Examples and strings.\nLine numbers for the Examples are 0-based.  The optional\nargument `name` is a name identifying this string, and is only\nused for error messages.\n\"\"\"\n", "func_signal": "def parse(self, string, name='<string>'):\n", "code": "string = string.expandtabs()\n# If all lines begin with the same indentation, then strip it.\nmin_indent = self._min_indent(string)\nif min_indent > 0:\n    string = '\\n'.join([l[min_indent:] for l in string.split('\\n')])\n\noutput = []\ncharno, lineno = 0, 0\n# Find all doctest examples in the string:\nfor m in self._EXAMPLE_RE.finditer(string):\n    # Add the pre-example text to `output`.\n    output.append(string[charno:m.start()])\n    # Update lineno (lines before this example)\n    lineno += string.count('\\n', charno, m.start())\n    # Extract info from the regexp match.\n    (source, options, want, exc_msg) = \\\n             self._parse_example(m, name, lineno)\n    # Create an Example, and add it to the list.\n    if not self._IS_BLANK_OR_COMMENT(source):\n        output.append( Example(source, want, exc_msg,\n                            lineno=lineno,\n                            indent=min_indent+len(m.group('indent')),\n                            options=options) )\n    # Update lineno (lines inside this example)\n    lineno += string.count('\\n', m.start(), m.end())\n    # Update charno.\n    charno = m.end()\n# Add any remaining post-example text to `output`.\noutput.append(string[charno:])\nreturn output", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "rc/femhub", "stars": 1, "license": "other", "language": "python", "size": 1416}
{"docstring": "\"\"\"\nQuickly decreases the number of times a worker checks for output\n\n\"\"\"\n\n", "func_signal": "def increase_checker_task_timeout(self):\n", "code": "if self.checker_task.running:\n    self.checker_task.stop()\n\nself.checker_timeout = self.checker_timeout * 1.1\nif self.checker_timeout > self.checker_limit:\n    self.checker_timeout = self.checker_limit\nself.checker_task = task.LoopingCall(self.check_work)\nself.checker_task.start(self.checker_timeout, now=False)\nif self.log_level > 0:\n    msg = 'Checking output again in %s' % self.checker_timeout\n    log.msg(LOG_PREFIX % self.id + msg)", "path": "spkg\\standard\\sage_scripts\\dsage_worker.py", "repo_name": "rc/femhub", "stars": 1, "license": "other", "language": "python", "size": 1416}
{"docstring": "\"\"\"\nReturn True iff the actual output from an example (`got`)\nmatches the expected output (`want`).  These strings are\nalways considered to match if they are identical; but\ndepending on what option flags the test runner is using,\nseveral non-exact match types are also possible.  See the\ndocumentation for `TestRunner` for more information about\noption flags.\n\"\"\"\n# Handle the common case first, for efficiency:\n# if they're string-identical, always return true.\n", "func_signal": "def check_output(self, want, got, optionflags):\n", "code": "if got == want:\n    return True\n\n# The values True and False replaced 1 and 0 as the return\n# value for boolean comparisons in Python 2.3.\nif not (optionflags & DONT_ACCEPT_TRUE_FOR_1):\n    if (got,want) == (\"True\\n\", \"1\\n\"):\n        return True\n    if (got,want) == (\"False\\n\", \"0\\n\"):\n        return True\n\n# <BLANKLINE> can be used as a special sequence to signify a\n# blank line, unless the DONT_ACCEPT_BLANKLINE flag is used.\nif not (optionflags & DONT_ACCEPT_BLANKLINE):\n    # Replace <BLANKLINE> in want with a blank line.\n    want = re.sub('(?m)^%s\\s*?$' % re.escape(BLANKLINE_MARKER),\n                  '', want)\n    # If a line in got contains only spaces, then remove the\n    # spaces.\n    got = re.sub('(?m)^\\s*?$', '', got)\n    if got == want:\n        return True\n\n# This flag causes doctest to ignore any differences in the\n# contents of whitespace strings.  Note that this can be used\n# in conjunction with the ELLIPSIS flag.\nif optionflags & NORMALIZE_WHITESPACE:\n    got = ' '.join(got.split())\n    want = ' '.join(want.split())\n    if got == want:\n        return True\n\n# The ELLIPSIS flag says to let the sequence \"...\" in `want`\n# match any substring in `got`.\nif optionflags & ELLIPSIS:\n    if _ellipsis_match(want, got):\n        return True\n\n# We didn't find any match; return false.\nreturn False", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "rc/femhub", "stars": 1, "license": "other", "language": "python", "size": 1416}
{"docstring": "\"\"\"\nResets the output/result checker for the worker.\n\n\"\"\"\n\n", "func_signal": "def reset_checker(self):\n", "code": "if self.checker_task.running:\n    self.checker_task.stop()\nself.checker_timeout = 0.1\nself.checker_task = task.LoopingCall(self.check_work)", "path": "spkg\\standard\\sage_scripts\\dsage_worker.py", "repo_name": "rc/femhub", "stars": 1, "license": "other", "language": "python", "size": 1416}
{"docstring": "\"\"\"Returns true iff the given string is the name of a\nself-closing tag according to this parser.\"\"\"\n", "func_signal": "def isSelfClosingTag(self, name):\n", "code": "return self.SELF_CLOSING_TAGS.has_key(name) \\\n       or self.instanceSelfClosingTags.has_key(name)", "path": "tracker\\jpexpress\\package\\BeautifulSoup.py", "repo_name": "nayutaya/packtrack", "stars": 1, "license": "None", "language": "python", "size": 384}
{"docstring": "\"\"\"Returns the parents of this Tag that match the given\ncriteria.\"\"\"\n\n", "func_signal": "def findParents(self, name=None, attrs={}, limit=None, **kwargs):\n", "code": "return self._findAll(name, attrs, None, limit, self.parentGenerator,\n                     **kwargs)", "path": "tracker\\jpexpress\\package\\BeautifulSoup.py", "repo_name": "nayutaya/packtrack", "stars": 1, "license": "None", "language": "python", "size": 384}
{"docstring": "'''Given a string and its encoding, decodes the string into Unicode.\n%encoding is a string recognized by encodings.aliases'''\n\n# strip Byte Order Mark (if present)\n", "func_signal": "def _toUnicode(self, data, encoding):\n", "code": "if (len(data) >= 4) and (data[:2] == '\\xfe\\xff') \\\n       and (data[2:4] != '\\x00\\x00'):\n    encoding = 'utf-16be'\n    data = data[2:]\nelif (len(data) >= 4) and (data[:2] == '\\xff\\xfe') \\\n         and (data[2:4] != '\\x00\\x00'):\n    encoding = 'utf-16le'\n    data = data[2:]\nelif data[:3] == '\\xef\\xbb\\xbf':\n    encoding = 'utf-8'\n    data = data[3:]\nelif data[:4] == '\\x00\\x00\\xfe\\xff':\n    encoding = 'utf-32be'\n    data = data[4:]\nelif data[:4] == '\\xff\\xfe\\x00\\x00':\n    encoding = 'utf-32le'\n    data = data[4:]\nnewdata = unicode(data, encoding)\nreturn newdata", "path": "tracker\\jpexpress\\package\\BeautifulSoup.py", "repo_name": "nayutaya/packtrack", "stars": 1, "license": "None", "language": "python", "size": 384}
{"docstring": "\"\"\"Initializes a map representation of this tag's attributes,\nif not already initialized.\"\"\"\n", "func_signal": "def _getAttrMap(self):\n", "code": "if not getattr(self, 'attrMap'):\n    self.attrMap = {}\n    for (key, value) in self.attrs:\n        self.attrMap[key] = value\nreturn self.attrMap", "path": "tracker\\jpexpress\\package\\BeautifulSoup.py", "repo_name": "nayutaya/packtrack", "stars": 1, "license": "None", "language": "python", "size": 384}
{"docstring": "\"\"\"Returns true iff this tag has the same name, the same attributes,\nand the same contents (recursively) as the given tag.\n\nNOTE: right now this will return false if two tags have the\nsame attributes in a different order. Should this be fixed?\"\"\"\n", "func_signal": "def __eq__(self, other):\n", "code": "if not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):\n    return False\nfor i in range(0, len(self.contents)):\n    if self.contents[i] != other.contents[i]:\n        return False\nreturn True", "path": "tracker\\jpexpress\\package\\BeautifulSoup.py", "repo_name": "nayutaya/packtrack", "stars": 1, "license": "None", "language": "python", "size": 384}
{"docstring": "\"\"\"Pops the tag stack up to and including the most recent\ninstance of the given tag. If inclusivePop is false, pops the tag\nstack up to but *not* including the most recent instqance of\nthe given tag.\"\"\"\n#print \"Popping to %s\" % name\n", "func_signal": "def _popToTag(self, name, inclusivePop=True):\n", "code": "if name == self.ROOT_TAG_NAME:\n    return\n\nnumPops = 0\nmostRecentTag = None\nfor i in range(len(self.tagStack)-1, 0, -1):\n    if name == self.tagStack[i].name:\n        numPops = len(self.tagStack)-i\n        break\nif not inclusivePop:\n    numPops = numPops - 1\n\nfor i in range(0, numPops):\n    mostRecentTag = self.popTag()\nreturn mostRecentTag", "path": "tracker\\jpexpress\\package\\BeautifulSoup.py", "repo_name": "nayutaya/packtrack", "stars": 1, "license": "None", "language": "python", "size": 384}
{"docstring": "\"\"\"Treat a bogus SGML declaration as raw data. Treat a CDATA\ndeclaration as a CData object.\"\"\"\n", "func_signal": "def parse_declaration(self, i):\n", "code": "j = None\nif self.rawdata[i:i+9] == '<![CDATA[':\n     k = self.rawdata.find(']]>', i)\n     if k == -1:\n         k = len(self.rawdata)\n     data = self.rawdata[i+9:k]\n     j = k+3\n     self._toStringSubclass(data, CData)\nelse:\n    try:\n        j = HTMLParser.parse_declaration(self, i)\n    except HTMLParseError:\n        toHandle = self.rawdata[i:]\n        self.handle_data(toHandle)\n        j = i + len(toHandle)\nreturn j", "path": "tracker\\jpexpress\\package\\BeautifulSoup.py", "repo_name": "nayutaya/packtrack", "stars": 1, "license": "None", "language": "python", "size": 384}
{"docstring": "# Convert the document to Unicode.\n", "func_signal": "def _feed(self, inDocumentEncoding=None, isHTML=False):\n", "code": "markup = self.markup\nif isinstance(markup, unicode):\n    if not hasattr(self, 'originalEncoding'):\n        self.originalEncoding = None\nelse:\n    dammit = UnicodeDammit\\\n             (markup, [self.fromEncoding, inDocumentEncoding],\n              smartQuotesTo=self.smartQuotesTo, isHTML=isHTML)\n    markup = dammit.unicode\n    self.originalEncoding = dammit.originalEncoding\n    self.declaredHTMLEncoding = dammit.declaredHTMLEncoding\nif markup:\n    if self.markupMassage:\n        if not isList(self.markupMassage):\n            self.markupMassage = self.MARKUP_MASSAGE\n        for fix, m in self.markupMassage:\n            markup = fix.sub(m, markup)\n        # TODO: We get rid of markupMassage so that the\n        # soup object can be deepcopied later on. Some\n        # Python installations can't copy regexes. If anyone\n        # was relying on the existence of markupMassage, this\n        # might cause problems.\n        del(self.markupMassage)\nself.builder.reset()\n\nself.builder.feed(markup)\n# Close out any unfinished strings and close all the open tags.\nself.endData()\nwhile self.currentTag.name != self.ROOT_TAG_NAME:\n    self.popTag()", "path": "tracker\\jpexpress\\package\\BeautifulSoup.py", "repo_name": "nayutaya/packtrack", "stars": 1, "license": "None", "language": "python", "size": 384}
{"docstring": "\"\"\"Convenience method that works with all 2.x versions of Python\nto determine whether or not something is stringlike.\"\"\"\n", "func_signal": "def isString(s):\n", "code": "try:\n    return isinstance(s, unicode) or isinstance(s, basestring)\nexcept NameError:\n    return isinstance(s, str)", "path": "tracker\\jpexpress\\package\\BeautifulSoup.py", "repo_name": "nayutaya/packtrack", "stars": 1, "license": "None", "language": "python", "size": 384}
{"docstring": "\"\"\"Returns either the given Unicode string or its encoding.\"\"\"\n", "func_signal": "def sob(unicode, encoding):\n", "code": "if encoding is None:\n    return unicode\nelse:\n    return unicode.encode(encoding)", "path": "tracker\\jpexpress\\package\\BeautifulSoup.py", "repo_name": "nayutaya/packtrack", "stars": 1, "license": "None", "language": "python", "size": 384}
{"docstring": "\"\"\"Destructively rips this element out of the tree.\"\"\"\n", "func_signal": "def extract(self):\n", "code": "if self.parent:\n    try:\n        self.parent.contents.remove(self)\n    except ValueError:\n        pass\n\n#Find the two elements that would be next to each other if\n#this element (and any children) hadn't been parsed. Connect\n#the two.\nlastChild = self._lastRecursiveChild()\nnextElement = lastChild.next\n\nif self.previous:\n    self.previous.next = nextElement\nif nextElement:\n    nextElement.previous = self.previous\nself.previous = None\nlastChild.next = None\n\nself.parent = None\nif self.previousSibling:\n    self.previousSibling.nextSibling = self.nextSibling\nif self.nextSibling:\n    self.nextSibling.previousSibling = self.previousSibling\nself.previousSibling = self.nextSibling = None\nreturn self", "path": "tracker\\jpexpress\\package\\BeautifulSoup.py", "repo_name": "nayutaya/packtrack", "stars": 1, "license": "None", "language": "python", "size": 384}
{"docstring": "\"\"\"Turns a list of maps, lists, or scalars into a single map.\nUsed to build the SELF_CLOSING_TAGS, NESTABLE_TAGS, and\nNESTING_RESET_TAGS maps out of lists and partial maps.\"\"\"\n", "func_signal": "def buildTagMap(default, *args):\n", "code": "built = {}\nfor portion in args:\n    if hasattr(portion, 'items'):\n        #It's a map. Merge it.\n        for k,v in portion.items():\n            built[k] = v\n    elif isList(portion) and not isString(portion):\n        #It's a list. Map each item to the default.\n        for k in portion:\n            built[k] = default\n    else:\n        #It's a scalar. Map it to the default.\n        built[portion] = default\nreturn built", "path": "tracker\\jpexpress\\package\\BeautifulSoup.py", "repo_name": "nayutaya/packtrack", "stars": 1, "license": "None", "language": "python", "size": 384}
{"docstring": "\"\"\"Recursively destroys the contents of this tree.\"\"\"\n", "func_signal": "def decompose(self):\n", "code": "contents = [i for i in self.contents]\nfor i in contents:\n    if isinstance(i, Tag):\n        i.decompose()\n    else:\n        i.extract()\nself.extract()", "path": "tracker\\jpexpress\\package\\BeautifulSoup.py", "repo_name": "nayutaya/packtrack", "stars": 1, "license": "None", "language": "python", "size": 384}
{"docstring": "#print \"Push\", tag.name\n", "func_signal": "def pushTag(self, tag):\n", "code": "if self.currentTag:\n    self.currentTag.contents.append(tag)\nself.tagStack.append(tag)\nself.currentTag = self.tagStack[-1]", "path": "tracker\\jpexpress\\package\\BeautifulSoup.py", "repo_name": "nayutaya/packtrack", "stars": 1, "license": "None", "language": "python", "size": 384}
{"docstring": "\"\"\"Beautiful Soup can detect a charset included in a META tag,\ntry to convert the document to that charset, and re-parse the\ndocument from the beginning.\"\"\"\n", "func_signal": "def extractCharsetFromMeta(self, attrs):\n", "code": "httpEquiv = None\ncontentType = None\ncontentTypeIndex = None\ntagNeedsEncodingSubstitution = False\n\nfor i in range(0, len(attrs)):\n    key, value = attrs[i]\n    key = key.lower()\n    if key == 'http-equiv':\n        httpEquiv = value\n    elif key == 'content':\n        contentType = value\n        contentTypeIndex = i\n\nif httpEquiv and contentType: # It's an interesting meta tag.\n    match = self.CHARSET_RE.search(contentType)\n    if match:\n        if (self.declaredHTMLEncoding is not None or\n            self.originalEncoding == self.fromEncoding):\n            # An HTML encoding was sniffed while converting\n            # the document to Unicode, or an HTML encoding was\n            # sniffed during a previous pass through the\n            # document, or an encoding was specified\n            # explicitly and it worked. Rewrite the meta tag.\n            def rewrite(match):\n                return match.group(1) + \"%SOUP-ENCODING%\"\n            newAttr = self.CHARSET_RE.sub(rewrite, contentType)\n            attrs[contentTypeIndex] = (attrs[contentTypeIndex][0],\n                                       newAttr)\n            tagNeedsEncodingSubstitution = True\n        else:\n            # This is our first pass through the document.\n            # Go through it again with the encoding information.\n            newCharset = match.group(3)\n            if newCharset and newCharset != self.originalEncoding:\n                self.declaredHTMLEncoding = newCharset\n                self._feed(self.declaredHTMLEncoding)\n                raise StopParsing\n            pass\ntag = self.unknown_starttag(\"meta\", attrs)\nif tag and tagNeedsEncodingSubstitution:\n    tag.containsSubstitutions = True", "path": "tracker\\jpexpress\\package\\BeautifulSoup.py", "repo_name": "nayutaya/packtrack", "stars": 1, "license": "None", "language": "python", "size": 384}
{"docstring": "\"\"\"Sets up the initial relations between this element and\nother elements.\"\"\"\n", "func_signal": "def setup(self, parent=None, previous=None):\n", "code": "self.parent = parent\nself.previous = previous\nself.next = None\nself.previousSibling = None\nself.nextSibling = None\nif self.parent and self.parent.contents:\n    self.previousSibling = self.parent.contents[-1]\n    self.previousSibling.nextSibling = self", "path": "tracker\\jpexpress\\package\\BeautifulSoup.py", "repo_name": "nayutaya/packtrack", "stars": 1, "license": "None", "language": "python", "size": 384}
{"docstring": "\"\"\"Create a new NavigableString.\n\nWhen unpickling a NavigableString, this method is called with\nthe string in DEFAULT_OUTPUT_ENCODING. That encoding needs to be\npassed in to the superclass's __new__ or the superclass won't know\nhow to handle non-ASCII characters.\n\"\"\"\n", "func_signal": "def __new__(cls, value):\n", "code": "if isinstance(value, unicode):\n    return unicode.__new__(cls, value)\nreturn unicode.__new__(cls, value, DEFAULT_OUTPUT_ENCODING)", "path": "tracker\\jpexpress\\package\\BeautifulSoup.py", "repo_name": "nayutaya/packtrack", "stars": 1, "license": "None", "language": "python", "size": 384}
{"docstring": "\"\"\"Returns the closest sibling to this Tag that matches the\ngiven criteria and appears after this Tag in the document.\"\"\"\n", "func_signal": "def findNextSibling(self, name=None, attrs={}, text=None, **kwargs):\n", "code": "return self._findOne(self.findNextSiblings, name, attrs, text,\n                     **kwargs)", "path": "tracker\\jpexpress\\package\\BeautifulSoup.py", "repo_name": "nayutaya/packtrack", "stars": 1, "license": "None", "language": "python", "size": 384}
{"docstring": "\"\"\"Handle entity references as data, possibly converting known\nHTML and/or XML entity references to the corresponding Unicode\ncharacters.\"\"\"\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "data = None\nif self.soup.convertHTMLEntities:\n    try:\n        data = unichr(name2codepoint[ref])\n    except KeyError:\n        pass\n\nif not data and self.soup.convertXMLEntities:\n        data = self.soup.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref)\n\nif not data and self.soup.convertHTMLEntities and \\\n    not self.soup.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref):\n        # TODO: We've got a problem here. We're told this is\n        # an entity reference, but it's not an XML entity\n        # reference or an HTML entity reference. Nonetheless,\n        # the logical thing to do is to pass it through as an\n        # unrecognized entity reference.\n        #\n        # Except: when the input is \"&carol;\" this function\n        # will be called with input \"carol\". When the input is\n        # \"AT&T\", this function will be called with input\n        # \"T\". We have no way of knowing whether a semicolon\n        # was present originally, so we don't know whether\n        # this is an unknown entity or just a misplaced\n        # ampersand.\n        #\n        # The more common case is a misplaced ampersand, so I\n        # escape the ampersand and omit the trailing semicolon.\n        data = \"&amp;%s\" % ref\nif not data:\n    # This case is different from the one above, because we\n    # haven't already gone through a supposedly comprehensive\n    # mapping of entities to Unicode characters. We might not\n    # have gone through any mapping at all. So the chances are\n    # very high that this is a real entity, and not a\n    # misplaced ampersand.\n    data = \"&%s;\" % ref\nself.handle_data(data)", "path": "tracker\\jpexpress\\package\\BeautifulSoup.py", "repo_name": "nayutaya/packtrack", "stars": 1, "license": "None", "language": "python", "size": 384}
{"docstring": "\"\"\"Returns the closest parent of this Tag that matches the given\ncriteria.\"\"\"\n# NOTE: We can't use _findOne because findParents takes a different\n# set of arguments.\n", "func_signal": "def findParent(self, name=None, attrs={}, **kwargs):\n", "code": "r = None\nl = self.findParents(name, attrs, 1)\nif l:\n    r = l[0]\nreturn r", "path": "tracker\\jpexpress\\package\\BeautifulSoup.py", "repo_name": "nayutaya/packtrack", "stars": 1, "license": "None", "language": "python", "size": 384}
{"docstring": "\"\"\"\nExtract all doctest examples from the given string, and return\nthem as a list of `Example` objects.  Line numbers are\n0-based, because it's most common in doctests that nothing\ninteresting appears on the same line as opening triple-quote,\nand so the first interesting line is called \\\"line 1\\\" then.\n\nThe optional argument `name` is a name identifying this\nstring, and is only used for error messages.\n\"\"\"\n", "func_signal": "def get_examples(self, string, name='<string>'):\n", "code": "return [x for x in self.parse(string, name)\n        if isinstance(x, Example)]", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "matthewturk/raft", "stars": 1, "license": "other", "language": "python", "size": 1481}
{"docstring": "\"\"\"\nReturn the module specified by `module`.  In particular:\n  - If `module` is a module, then return module.\n  - If `module` is a string, then import and return the\n    module with that name.\n  - If `module` is None, then return the calling module.\n    The calling module is assumed to be the module of\n    the stack frame at the given depth in the call stack.\n\"\"\"\n", "func_signal": "def _normalize_module(module, depth=2):\n", "code": "if inspect.ismodule(module):\n    return module\nelif isinstance(module, (str, unicode)):\n    return __import__(module, globals(), locals(), [\"*\"])\nelif module is None:\n    return sys.modules[sys._getframe(depth).f_globals['__name__']]\nelse:\n    raise TypeError(\"Expected a module, string, or None\")", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "matthewturk/raft", "stars": 1, "license": "other", "language": "python", "size": 1481}
{"docstring": "\"\"\"\nCreates the temporary directory for the worker.\n\n:type job: dsage.database.job.Job\n:param job: a Job object\n\n\"\"\"\n\n", "func_signal": "def setup_tmp_dir(self, job):\n", "code": "cur_dir = os.getcwd() # keep a reference to the current directory\ntmp_job_dir = os.path.join(TMP_WORKER_FILES, job.job_id)\nif not os.path.isdir(TMP_WORKER_FILES):\n    os.mkdir(TMP_WORKER_FILES)\nif not os.path.isdir(tmp_job_dir):\n    os.mkdir(tmp_job_dir)\nos.chdir(tmp_job_dir)\nself.sage.eval(\"os.chdir('%s')\" % tmp_job_dir)\n\nreturn tmp_job_dir", "path": "spkg\\standard\\sage_scripts\\dsage_worker.py", "repo_name": "matthewturk/raft", "stars": 1, "license": "other", "language": "python", "size": 1481}
{"docstring": "\"\"\"\nQuickly decreases the number of times a worker checks for output\n\n\"\"\"\n\n", "func_signal": "def increase_checker_task_timeout(self):\n", "code": "if self.checker_task.running:\n    self.checker_task.stop()\n\nself.checker_timeout = self.checker_timeout * 1.1\nif self.checker_timeout > self.checker_limit:\n    self.checker_timeout = self.checker_limit\nself.checker_task = task.LoopingCall(self.check_work)\nself.checker_task.start(self.checker_timeout, now=False)\nif self.log_level > 0:\n    msg = 'Checking output again in %s' % self.checker_timeout\n    log.msg(LOG_PREFIX % self.id + msg)", "path": "spkg\\standard\\sage_scripts\\dsage_worker.py", "repo_name": "matthewturk/raft", "stars": 1, "license": "other", "language": "python", "size": 1481}
{"docstring": "\"\"\"Debug a single doctest docstring.\n\nProvide the module (or dotted name of the module) containing the\ntest to be debugged and the name (within the module) of the object\nwith the docstring with tests to be debugged.\n\"\"\"\n", "func_signal": "def debug(module, name, pm=False):\n", "code": "module = _normalize_module(module)\ntestsrc = testsource(module, name)\ndebug_script(testsrc, pm, module.__dict__)", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "matthewturk/raft", "stars": 1, "license": "other", "language": "python", "size": 1481}
{"docstring": "\"\"\"\nExtract all doctest examples from the given string, and\ncollect them into a `DocTest` object.\n\n`globs`, `name`, `filename`, and `lineno` are attributes for\nthe new `DocTest` object.  See the documentation for `DocTest`\nfor more information.\n\"\"\"\n", "func_signal": "def get_doctest(self, string, globs, name, filename, lineno):\n", "code": "return DocTest(self.get_examples(string, name), globs,\n               name, filename, lineno, string)", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "matthewturk/raft", "stars": 1, "license": "other", "language": "python", "size": 1481}
{"docstring": "\"\"\"\nReturn true if the given object is defined in the given\nmodule.\n\"\"\"\n", "func_signal": "def _from_module(self, module, object):\n", "code": "if module is None:\n    return True\nelif inspect.isfunction(object):\n    return module.__dict__ is object.func_globals\nelif inspect.isclass(object):\n    return module.__name__ == object.__module__\nelif inspect.getmodule(object) is not None:\n    return module is inspect.getmodule(object)\nelif hasattr(object, '__module__'):\n    return module.__name__ == object.__module__\nelif isinstance(object, property):\n    return True # [XX] no way not be sure.\nelse:\n    raise ValueError(\"object must be a class or function\")", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "matthewturk/raft", "stars": 1, "license": "other", "language": "python", "size": 1481}
{"docstring": "# Not unless they asked for a fancy diff.\n", "func_signal": "def _do_a_fancy_diff(self, want, got, optionflags):\n", "code": "if not optionflags & (REPORT_UDIFF |\n                      REPORT_CDIFF |\n                      REPORT_NDIFF):\n    return False\n\n# If expected output uses ellipsis, a meaningful fancy diff is\n# too hard ... or maybe not.  In two real-life failures Tim saw,\n# a diff was a major help anyway, so this is commented out.\n# [todo] _ellipsis_match() knows which pieces do and don't match,\n# and could be the basis for a kick-ass diff in this case.\n##if optionflags & ELLIPSIS and ELLIPSIS_MARKER in want:\n##    return False\n\n# ndiff does intraline difference marking, so can be useful even\n# for 1-line differences.\nif optionflags & REPORT_NDIFF:\n    return True\n\n# The other diff types need at least a few lines to be helpful.\nreturn want.count('\\n') > 2 and got.count('\\n') > 2", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "matthewturk/raft", "stars": 1, "license": "other", "language": "python", "size": 1481}
{"docstring": "\"\"\"\nReports failure of a job.\n\n:type failure: twisted.python.failure\n:param failure: A twisted failure object\n\n\"\"\"\n\n", "func_signal": "def report_failure(self, failure):\n", "code": "msg = 'Job %s failed!' % (self.job.job_id)\nimport shutil\nfailed_dir = self.tmp_job_dir + '_failed'\nif os.path.exists(failed_dir):\n    shutil.rmtree(failed_dir)\nshutil.move(self.tmp_job_dir, failed_dir)\nlog.msg(LOG_PREFIX % self.id + msg)\nlog.msg('Traceback: \\n%s' % failure)\nd = self.remoteobj.callRemote('job_failed', self.job.job_id, failure)\nd.addErrback(self._catch_failure)\n\nreturn d", "path": "spkg\\standard\\sage_scripts\\dsage_worker.py", "repo_name": "matthewturk/raft", "stars": 1, "license": "other", "language": "python", "size": 1481}
{"docstring": "\"\"\"A unittest suite for one or more doctest files.\n\nThe path to each doctest file is given as a string; the\ninterpretation of that string depends on the keyword argument\n\"module_relative\".\n\nA number of options may be provided as keyword arguments:\n\nmodule_relative\n  If \"module_relative\" is True, then the given file paths are\n  interpreted as os-independent module-relative paths.  By\n  default, these paths are relative to the calling module's\n  directory; but if the \"package\" argument is specified, then\n  they are relative to that package.  To ensure os-independence,\n  \"filename\" should use \"/\" characters to separate path\n  segments, and may not be an absolute path (i.e., it may not\n  begin with \"/\").\n\n  If \"module_relative\" is False, then the given file paths are\n  interpreted as os-specific paths.  These paths may be absolute\n  or relative (to the current working directory).\n\npackage\n  A Python package or the name of a Python package whose directory\n  should be used as the base directory for module relative paths.\n  If \"package\" is not specified, then the calling module's\n  directory is used as the base directory for module relative\n  filenames.  It is an error to specify \"package\" if\n  \"module_relative\" is False.\n\nsetUp\n  A set-up function.  This is called before running the\n  tests in each file. The setUp function will be passed a DocTest\n  object.  The setUp function can access the test globals as the\n  globs attribute of the test passed.\n\ntearDown\n  A tear-down function.  This is called after running the\n  tests in each file.  The tearDown function will be passed a DocTest\n  object.  The tearDown function can access the test globals as the\n  globs attribute of the test passed.\n\nglobs\n  A dictionary containing initial global variables for the tests.\n\noptionflags\n  A set of doctest option flags expressed as an integer.\n\nparser\n  A DocTestParser (or subclass) that should be used to extract\n  tests from the files.\n\nencoding\n  An encoding that will be used to convert the files to unicode.\n\"\"\"\n", "func_signal": "def DocFileSuite(*paths, **kw):\n", "code": "suite = unittest.TestSuite()\n\n# We do this here so that _normalize_module is called at the right\n# level.  If it were called in DocFileTest, then this function\n# would be the caller and we might guess the package incorrectly.\nif kw.get('module_relative', True):\n    kw['package'] = _normalize_module(kw.get('package'))\n\nfor path in paths:\n    suite.addTest(DocFileTest(path, **kw))\n\nreturn suite", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "matthewturk/raft", "stars": 1, "license": "other", "language": "python", "size": 1481}
{"docstring": "\"\"\"\nReturn a DocTest for the given object, if it defines a docstring;\notherwise, return None.\n\"\"\"\n# Extract the object's docstring.  If it doesn't have one,\n# then return None (no test for this object).\n", "func_signal": "def _get_test(self, obj, name, module, globs, source_lines):\n", "code": "if isinstance(obj, basestring):\n    docstring = obj\nelse:\n    try:\n        if obj.__doc__ is None:\n            docstring = ''\n        else:\n            docstring = obj.__doc__\n            if not isinstance(docstring, basestring):\n                docstring = str(docstring)\n    except (TypeError, AttributeError):\n        docstring = ''\n\n# Find the docstring's location in the file.\nlineno = self._find_lineno(obj, source_lines)\n\n# Don't bother if the docstring is empty.\nif self._exclude_empty and not docstring:\n    return None\n\n# Return a DocTest for this object.\nif module is None:\n    filename = None\nelse:\n    filename = getattr(module, '__file__', module.__name__)\n    if filename[-4:] in (\".pyc\", \".pyo\"):\n        filename = filename[:-1]\nreturn self._parser.get_doctest(docstring, globs, name,\n                                filename, lineno)", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "matthewturk/raft", "stars": 1, "license": "other", "language": "python", "size": 1481}
{"docstring": "\"\"\"\nReturn a dictionary containing option overrides extracted from\noption directives in the given source string.\n\n`name` is the string's name, and `lineno` is the line number\nwhere the example starts; both are used for error messages.\n\"\"\"\n", "func_signal": "def _find_options(self, source, name, lineno):\n", "code": "options = {}\n# (note: with the current regexp, this will match at most once:)\nfor m in self._OPTION_DIRECTIVE_RE.finditer(source):\n    option_strings = m.group(1).replace(',', ' ').split()\n    for option in option_strings:\n        if (option[0] not in '+-' or\n            option[1:] not in OPTIONFLAGS_BY_NAME):\n            raise ValueError('line %r of the doctest for %s '\n                             'has an invalid option: %r' %\n                             (lineno+1, name, option))\n        flag = OPTIONFLAGS_BY_NAME[option[1:]]\n        options[flag] = (option[0] == '+')\nif options and self._IS_BLANK_OR_COMMENT(source):\n    raise ValueError('line %r of the doctest for %s has an option '\n                     'directive on a line with no example: %r' %\n                     (lineno, name, source))\nreturn options", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "matthewturk/raft", "stars": 1, "license": "other", "language": "python", "size": 1481}
{"docstring": "\"\"\"\n:type remoteobj: remoteobj\n:param remoteobj: Reference to the remote dsage server\n\n:type id: integer\n:param id: numerical identifier of worker\n\n:type log_level: integer\n:param log_level: log level, higher means more verbose\n\n:type poll: integer\n:param poll: rate (in seconds) a worker talks to the server\n\n\"\"\"\n\n", "func_signal": "def __init__(self, remoteobj, id, log_level=0, poll=1.0):\n", "code": "self.remoteobj = remoteobj\nself.id = id\nself.free = True\nself.job = None\nself.log_level = log_level\nself.poll_rate = poll\nself.checker_task = task.LoopingCall(self.check_work)\nself.checker_timeout = 0.1\nself.checker_limit = 0.1\nself.got_output = False\nself.job_start_time = None\nself.orig_poll = poll\nself.get_pending = False\nself.start()", "path": "spkg\\standard\\sage_scripts\\dsage_worker.py", "repo_name": "matthewturk/raft", "stars": 1, "license": "other", "language": "python", "size": 1481}
{"docstring": "\"\"\"\nReturn a list of the DocTests that are defined by the given\nobject's docstring, or by any of its contained objects'\ndocstrings.\n\nThe optional parameter `module` is the module that contains\nthe given object.  If the module is not specified or is None, then\nthe test finder will attempt to automatically determine the\ncorrect module.  The object's module is used:\n\n    - As a default namespace, if `globs` is not specified.\n    - To prevent the DocTestFinder from extracting DocTests\n      from objects that are imported from other modules.\n    - To find the name of the file containing the object.\n    - To help find the line number of the object within its\n      file.\n\nContained objects whose module does not match `module` are ignored.\n\nIf `module` is False, no attempt to find the module will be made.\nThis is obscure, of use mostly in tests:  if `module` is False, or\nis None but cannot be found automatically, then all objects are\nconsidered to belong to the (non-existent) module, so all contained\nobjects will (recursively) be searched for doctests.\n\nThe globals for each DocTest is formed by combining `globs`\nand `extraglobs` (bindings in `extraglobs` override bindings\nin `globs`).  A new copy of the globals dictionary is created\nfor each DocTest.  If `globs` is not specified, then it\ndefaults to the module's `__dict__`, if specified, or {}\notherwise.  If `extraglobs` is not specified, then it defaults\nto {}.\n\n\"\"\"\n# If name was not specified, then extract it from the object.\n", "func_signal": "def find(self, obj, name=None, module=None, globs=None, extraglobs=None):\n", "code": "if name is None:\n    name = getattr(obj, '__name__', None)\n    if name is None:\n        raise ValueError(\"DocTestFinder.find: name must be given \"\n                \"when obj.__name__ doesn't exist: %r\" %\n                         (type(obj),))\n\n# Find the module that contains the given object (if obj is\n# a module, then module=obj.).  Note: this may fail, in which\n# case module will be None.\nif module is False:\n    module = None\nelif module is None:\n    module = inspect.getmodule(obj)\n\n# Read the module's source code.  This is used by\n# DocTestFinder._find_lineno to find the line number for a\n# given object's docstring.\ntry:\n    file = inspect.getsourcefile(obj) or inspect.getfile(obj)\n    source_lines = linecache.getlines(file)\n    if not source_lines:\n        source_lines = None\nexcept TypeError:\n    source_lines = None\n\n# Initialize globals, and merge in extraglobs.\nif globs is None:\n    if module is None:\n        globs = {}\n    else:\n        globs = module.__dict__.copy()\nelse:\n    globs = globs.copy()\nif extraglobs is not None:\n    globs.update(extraglobs)\n\n# Recursively expore `obj`, extracting DocTests.\ntests = []\nself._find(tests, obj, name, module, source_lines, globs, {})\n# Sort the tests by alpha order of names, for consistency in\n# verbose-mode output.  This was a feature of doctest in Pythons\n# <= 2.3 that got lost by accident in 2.4.  It was repaired in\n# 2.4.4 and 2.5.\ntests.sort()\nreturn tests", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "matthewturk/raft", "stars": 1, "license": "other", "language": "python", "size": 1481}
{"docstring": "\"\"\"\nGiven a regular expression match from `_EXAMPLE_RE` (`m`),\nreturn a pair `(source, want)`, where `source` is the matched\nexample's source code (with prompts and indentation stripped);\nand `want` is the example's expected output (with indentation\nstripped).\n\n`name` is the string's name, and `lineno` is the line number\nwhere the example starts; both are used for error messages.\n\"\"\"\n# Get the example's indentation level.\n", "func_signal": "def _parse_example(self, m, name, lineno):\n", "code": "indent = len(m.group('indent'))\n\n# Divide source into lines; check that they're properly\n# indented; and then strip their indentation & prompts.\nsource_lines = m.group('source').split('\\n')\nself._check_prompt_blank(source_lines, indent, name, lineno)\nself._check_prefix(source_lines[1:], ' '*indent + '.', name, lineno)\nsource = '\\n'.join([sl[indent+4:] for sl in source_lines])\n\n# Divide want into lines; check that it's properly indented; and\n# then strip the indentation.  Spaces before the last newline should\n# be preserved, so plain rstrip() isn't good enough.\nwant = m.group('want')\nwant_lines = want.split('\\n')\nif len(want_lines) > 1 and re.match(r' *$', want_lines[-1]):\n    del want_lines[-1]  # forget final newline & spaces after it\nself._check_prefix(want_lines, ' '*indent, name,\n                   lineno + len(source_lines))\nwant = '\\n'.join([wl[indent:] for wl in want_lines])\n\n# If `want` contains a traceback message, then extract it.\nm = self._EXCEPTION_RE.match(want)\nif m:\n    exc_msg = m.group('msg')\nelse:\n    exc_msg = None\n\n# Extract options from the source.\noptions = self._find_options(source, name, lineno)\n\nreturn source, options, want, exc_msg", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "matthewturk/raft", "stars": 1, "license": "other", "language": "python", "size": 1481}
{"docstring": "\"\"\"\nExecutes a job\n\n:type job: dsage.database.job.Job\n:param job: A Job object\n\n\"\"\"\n\n", "func_signal": "def doJob(self, job):\n", "code": "log.msg(LOG_PREFIX % self.id + 'Starting job %s ' % job.job_id)\n    \nself.free = False\nself.got_output = False\nd = defer.Deferred()\n\ntry:\n    self.checker_task.start(self.checker_timeout, now=False)\nexcept AssertionError:\n    self.checker_task.stop()\n    self.checker_task.start(self.checker_timeout, now=False)\nif self.log_level > 2:\n    log.msg(LOG_PREFIX % self.id + 'Starting checker task...')\n\nself.tmp_job_dir = self.setup_tmp_dir(job)\nself.extract_and_load_job_data(job)\n\njob_filename = self.write_job_file(job)\n\nf = os.path.join(self.tmp_job_dir, job_filename)\nself.sage._send(\"execfile('%s')\" % (f))\nself.job_start_time = datetime.datetime.now()\nif self.log_level > 2:\n    msg = 'File to execute: %s' % f\n    log.msg(LOG_PREFIX % self.id + msg)\n\nd.callback(True)", "path": "spkg\\standard\\sage_scripts\\dsage_worker.py", "repo_name": "matthewturk/raft", "stars": 1, "license": "other", "language": "python", "size": 1481}
{"docstring": "\"\"\"square() -> square TestClass's associated value\n\n>>> _TestClass(13).square().get()\n169\n\"\"\"\n\n", "func_signal": "def square(self):\n", "code": "self.val = self.val ** 2\nreturn self", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "matthewturk/raft", "stars": 1, "license": "other", "language": "python", "size": 1481}
{"docstring": "\"\"\"\nCreate a new DocTest containing the given examples.  The\nDocTest's globals are initialized with a copy of `globs`.\n\"\"\"\n", "func_signal": "def __init__(self, examples, globs, name, filename, lineno, docstring):\n", "code": "assert not isinstance(examples, basestring), \\\n       \"DocTest no longer accepts str; use DocTestParser instead\"\nself.examples = examples\nself.docstring = docstring\nself.globs = globs.copy()\nself.name = name\nself.filename = filename\nself.lineno = lineno", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "matthewturk/raft", "stars": 1, "license": "other", "language": "python", "size": 1481}
{"docstring": "\"\"\"\nCallback for connect.\n\n:type remoteobj: remote object\n:param remoteobj: remote obj\n\n\"\"\"\n\n", "func_signal": "def _connected(self, remoteobj):\n", "code": "self.remoteobj = remoteobj\nself.remoteobj.notifyOnDisconnect(self._disconnected)\nself.connected = True\n\nif self.worker_pool == None: # Only pool workers the first time\n    self.pool_workers(self.remoteobj)\nelse:\n    for worker in self.worker_pool:\n        worker.remoteobj = self.remoteobj # Update workers\n        if worker.job == None:\n            worker.restart()", "path": "spkg\\standard\\sage_scripts\\dsage_worker.py", "repo_name": "matthewturk/raft", "stars": 1, "license": "other", "language": "python", "size": 1481}
{"docstring": "\"\"\"\nPrints usage help.\n\n\"\"\"\n\n", "func_signal": "def usage():\n", "code": "from optparse import OptionParser\n\nusage = ['usage: %prog [options]\\n',\n          'Bug reports to <yqiang@gmail.com>']\nparser = OptionParser(usage=''.join(usage))\nparser.add_option('-s', '--server',\n                  dest='server',\n                  default='localhost',\n                  help='hostname. Default is localhost')\nparser.add_option('-p', '--port', \n                  dest='port', \n                  type='int',\n                  default=8081,\n                  help='port to connect to. default=8081')\nparser.add_option('--poll',\n                  dest='poll',\n                  type='float',\n                  default=5.0,\n                  help='poll rate before checking for new job. default=5')\nparser.add_option('-a', '--authenticate',\n                  dest='authenticate',\n                  default=False,\n                  action='store_true',\n                  help='Connect as authenticate worker. default=True')\nparser.add_option('-f', '--logfile',\n                  dest='logfile',\n                  default=os.path.join(DSAGE_DIR, 'worker.log'),\n                  help='log file')\nparser.add_option('-l', '--loglevel',\n                  dest='loglevel',\n                  type='int',\n                  default=0,\n                  help='log level. default=0')\nparser.add_option('--ssl',\n                  dest='ssl',\n                  action='store_true',\n                  default=False,\n                  help='enable or disable ssl')\nparser.add_option('--privkey',\n                  dest='privkey_file',\n                  default=os.path.join(DSAGE_DIR, 'dsage_key'),\n                  help='private key file. default = ' + \n                       '~/.sage/dsage/dsage_key')\nparser.add_option('--pubkey',\n                  dest='pubkey_file',\n                  default=os.path.join(DSAGE_DIR, 'dsage_key.pub'),\n                  help='public key file. default = ' +\n                       '~/.sage/dsage/dsage_key.pub')\nparser.add_option('-w', '--workers',\n                  dest='workers',\n                  type='int',\n                  default=2,\n                  help='number of workers. default=2')\nparser.add_option('--priority',\n                  dest='priority',\n                  type='int',\n                  default=20,\n                  help='priority of workers. default=20')\nparser.add_option('-u', '--username',\n                  dest='username',\n                  default=getuser(),\n                  help='username')\nparser.add_option('--noblock',\n                  dest='noblock',\n                  action='store_true',\n                  default=False,\n                  help='tells that the server was ' + \n                       'started in blocking mode')\n(options, args) = parser.parse_args()\n\nreturn options", "path": "spkg\\standard\\sage_scripts\\dsage_worker.py", "repo_name": "matthewturk/raft", "stars": 1, "license": "other", "language": "python", "size": 1481}
{"docstring": "\"\"\"Show a relationship between two users taking screenname as an argument\nReturns a dict with the relationship\n\nArguments:\n- `target_screenname`: Screenname of the user in question\n- `source_screenname`: Screenname of the originating user, if None will use the Authenticated user as source\n\"\"\"\n\n", "func_signal": "def friendship_show_by_screenname(self, target_screenname, source_screenname = None):\n", "code": "input_data = {'target_screen_name': target_screenname}\nif not source_screenname == None:\n    input_data.update({'source_screen_name': source_screenname})\nreturn self.__get_data('friendships/show', input_data)", "path": "berd.py", "repo_name": "docblades/Berd", "stars": 1, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Sets the user's status.\nMUST BE AUTHENTICATED\n\nReturns a Status object with the new status\n\nArguments:\n- `self`:\n- `status`: String with the status message. 140 chars max\n- `in_reply_to_status_id`: Status.id of message this is a @reply to\n\"\"\"\n", "func_signal": "def set_status(self, status, in_reply_to_status_id=None):\n", "code": "if in_reply_to_status_id == None:\n    input_data = {'status': status}\nelse:\n    input_data = {'status': status, 'in_reply_to_status_id': in_reply_to_status_id}\n\nreturn Status(self.__get_data('statuses/update', input_data))", "path": "berd.py", "repo_name": "docblades/Berd", "stars": 1, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Retrieves a Status by ID\n\nArguments:\n- `status_id`: Status ID\n\"\"\"\n", "func_signal": "def get_status(self, status_id):\n", "code": "data = self.__get_data(\"/statuses/show/%s\" % status_id)\nstatus = Status(data)\nreturn status", "path": "berd.py", "repo_name": "docblades/Berd", "stars": 1, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Sets status as a favorite of the Authenticated user\n\nReturns a Status object with the new favorite\n\nArguments:\n- `id`: ID of the status to favorite\n\"\"\"\n\n", "func_signal": "def favorite_create(self, id):\n", "code": "return Status(\n    self.__get_data('favorites/create/%s' % id)\n    )", "path": "berd.py", "repo_name": "docblades/Berd", "stars": 1, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Enables DEVICE notifications for updates from the specified user\n\nMust be Authenticated\n\nReturns a User object\nArguments:\n- `id`: Screenname or User ID\n\"\"\"\n\n", "func_signal": "def notifications_follow(self, id):\n", "code": "return User(\n    self.__get_data('notifications/follow/%s' % id)\n    )", "path": "berd.py", "repo_name": "docblades/Berd", "stars": 1, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Returns an array of favorites for the specified screenname (or if None, then the Authenticated user\n\n20 Per page\n\nArguments:\n- `page`: Which page to return. Defaults to the first page.\n- `screenname`: User whose favorites will be returned (if None, then the Authenticated user\n\"\"\"\n\n", "func_signal": "def favorites(self, page=1, screenname=None):\n", "code": "input_data = {'page': page}\nif screenname == None:\n    data = self.__get_data('favorites', input_data)\nelse:\n    data = self.__get_data('favorites/%s' % screenname, input_data)\n\nfor x in data:\n    yield Status(x)", "path": "berd.py", "repo_name": "docblades/Berd", "stars": 1, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Destroys the direct message. From Twitter, not the python object\n\nArguments:\n- `id`: DirectMessage id\n\"\"\"\n\n", "func_signal": "def destroy_direct_message(self, id):\n", "code": "return DirectMessage(\n    self.__get_data('direct_messages/destroy/%s' % id)\n    )", "path": "berd.py", "repo_name": "docblades/Berd", "stars": 1, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def __init__(self, method, opener):\n", "code": "self._method = method\nself._opener = opener\nself._page = 1\nself._count = 20\nself._last_id = None\nself._url = _base_url + self._method + '.json'", "path": "berd.py", "repo_name": "docblades/Berd", "stars": 1, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\" Get an arbitrary page from this Twitter method\n\"\"\"\n\n", "func_signal": "def retrieve_page(self, page):\n", "code": "if self._last_id == None:\n    input_data = {'count': self._count, 'page': page}\nelse:\n    input_data = {'count': self._count, 'page': page, 'max_id': self._last_id}\n\ndata = self.__get_data(input_data)\n\nreturn self.to_status(data)\n# for x in data:\n#     yield Status(x)", "path": "berd.py", "repo_name": "docblades/Berd", "stars": 1, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"\n\nArguments:\n- `user_dict`: Dictionary from a twitter method\n\"\"\"\n", "func_signal": "def __init__(self, user_dict):\n", "code": "self.user_dict = user_dict\n\nif 'user' in user_dict:\n    user_dict = user_dict['user']\n    \nself.id = user_dict['id']\nself.name = user_dict['name']\nself.screen_name = user_dict['screen_name']\nself.url = user_dict['url']\nself.profile_image_url = user_dict['profile_image_url']\nself.description = user_dict['description']\nself.location = user_dict['location']\n# HACK! Fix for apostrophe error in Python\ntry:\n    self.description = self.description.replace(u'\\u2019', u'\\u0027')\n    self.location = self.location.replace(u'\\u2019', u'\\u0027')\nexcept AttributeError:\n    pass\n# End Hack\nself.followers_count = user_dict['followers_count']\nself.friend_count = user_dict['friends_count']\nself.statuses_count = user_dict['statuses_count']\nself.created_at = user_dict['created_at']\nself.protected = user_dict['protected']\nself.utc_offset = user_dict['utc_offset']", "path": "berd.py", "repo_name": "docblades/Berd", "stars": 1, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\" Use this is get a list of tweets for this method\nWill get self._count tweets no matter what, (ignores last_id)\nIf you want to get the next set of tweets, use next_tweets\n\"\"\"\n\n", "func_signal": "def get_tweets(self):\n", "code": "self._page = 1\ninput_data = {'count': self._count}\ndata = self.__get_data(input_data)\nself._last_id = data[0]['id']\n\nreturn self.to_status(data)\n# for x in data:\n#     yield Status(x)", "path": "berd.py", "repo_name": "docblades/Berd", "stars": 1, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Show a relationship between two users taking user_id as an argument\nReturns a dict with the relationship\n\nArguments:\n- `target_user_id`: ID of the user in question\n- `source_user_id`: ID of the originating user, if None will use the Authenticated user as source\n\"\"\"\n\n", "func_signal": "def friendship_show_by_id(self, target_user_id, source_user_id = None):\n", "code": "input_data = {'target_id': target_user_id}\nif not source_user_id == None:\n    input_data.update({'source_id': source_user_id})\nreturn __get_data('friendships/show', input_data)", "path": "berd.py", "repo_name": "docblades/Berd", "stars": 1, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\" Uses a generator to yield a set of Statuses\nThis method is to shave off a couple lines of code and make life easier when extending this class\n\"\"\"\n", "func_signal": "def to_status(self, data):\n", "code": "for x in data:\n    yield Status(x)", "path": "berd.py", "repo_name": "docblades/Berd", "stars": 1, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Destroy a friendship/following\n\nReturns a new User object with the deleted person\n\nArguments:\n- `id`: Screenname or ID\n\"\"\"\n\n", "func_signal": "def friendship_destroy(self, id):\n", "code": "return User(\n    self.__get_data('friendships/destroy/%s' % id)\n    )", "path": "berd.py", "repo_name": "docblades/Berd", "stars": 1, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\" This should be your main method for working with this class\nReturns all the tweets for this twitter method since the last call\nOR if this is the first time, just returns self._count tweets\n\"\"\"\n\n", "func_signal": "def next_tweets(self):\n", "code": "self._page = 1\nif self._last_id == None:\n    input_data = {'count': self._count}\nelse:\n    input_data = {'count': self._count, 'since_id': self._last_id}\n\ndata = self.__get_data(input_data)\n# if there is a blank list, just output nothing\ntry:\n    last_id = data[0]['id']\nexcept IndexError:\n    pass\nelse:\n    self._last_id = last_id\n    # self._last_id = data[0]['id']\n\nreturn self.to_status(data)\n# for x in data:\n#     yield Status(x)", "path": "berd.py", "repo_name": "docblades/Berd", "stars": 1, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Destroys a Status\nReturns a Status object containing the destroyed status\n\nArguments:\n- `status_id`: ID of the status item to destroy\n\"\"\"\n", "func_signal": "def destroy_status(self, status_id):\n", "code": "data = self.__get_data(\"/statuses/destroy/%s\" % status_id)\nstatus = Status(data)\nreturn status", "path": "berd.py", "repo_name": "docblades/Berd", "stars": 1, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Returns a Paginated object containing this user's timeline\n\nCalled with an Authenticated Twitter object will potentially return protected statuses\n\nArguments:\n- `twitter`: An Authenticated Twitter object (if None, will use an unauthenticated request\n\"\"\"\n", "func_signal": "def get_timeline(self, twitter = None):\n", "code": "if twitter == None:\n    return Twitter.user_timeline(self.id)\nelse:\n    return twitter.user_timeline(self.id)", "path": "berd.py", "repo_name": "docblades/Berd", "stars": 1, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Send a direct message to User\nReturns a Direct Message object\n\nArguments:\n- `user`: User ID or screen name\n- `text`: Text of message\n\"\"\"\n", "func_signal": "def new_direct_message(self, user, text):\n", "code": "input_data = {'user': user, 'text': text}\nreturn DirectMessage(\n    self.__get_data('direct_messages/new', input_data)\n    )", "path": "berd.py", "repo_name": "docblades/Berd", "stars": 1, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\" Sets the number of results per page\n\"\"\"\n\n# Do some checking to make sure we don't go over Twitter's limit\n", "func_signal": "def set_count(self, count):\n", "code": "if count > 3200:\n    self.count = 3200\nelif count < 1:\n    self.count = 1\nelse:\n    self._count = count\nreturn self._count", "path": "berd.py", "repo_name": "docblades/Berd", "stars": 1, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Disables DEVICE notifications for updates from the specified user\n\nMust be Authenticated\n\nReturns a User object\n\nArguments:\n- `id`: Screenname or User ID\n\"\"\"\n\n", "func_signal": "def notifications_leave(self, id):\n", "code": "return User(\n    self.__get_data('notifications/leave/%s' % id)\n    )", "path": "berd.py", "repo_name": "docblades/Berd", "stars": 1, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\" to close cleanly databases in a multithreaded environment \"\"\"\n\n", "func_signal": "def close_all_instances(action):\n", "code": "sql_locker.acquire()\npid = thread.get_ident()\nif pid in SQLDB._folders:\n    del SQLDB._folders[pid]\nif pid in SQLDB._instances:\n    instances = SQLDB._instances[pid]\n    while instances:\n        instance = instances.pop()\n        sql_locker.release()\n        action(instance)\n        sql_locker.acquire()\n\n        # ## if you want pools recycle this connection\n        really = True\n        if instance._pool_size:\n            pool = SQLDB._connection_pools[instance._uri]\n            if len(pool) < instance._pool_size:\n                pool.append(instance._connection)\n                really = False\n        if really:\n            sql_locker.release()\n            instance._connection.close()\n            sql_locker.acquire()\n    del SQLDB._instances[pid]\nsql_locker.release()\nreturn", "path": "pygnite\\sql.py", "repo_name": "szarak/pygnite", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 184}
{"docstring": "\"\"\"    \n\nCreate a table with all possible field types\n'sqlite://test.db'\n'mysql://root:none@localhost/test'\n'postgres://mdipierro:none@localhost/test'\n'mssql://web2py:none@A64X2/web2py_test'\n'firebird://user:password@server:3050/database'\n'db2://DSN=dsn;UID=user;PWD=pass'\n\n>>> if len(sys.argv)<2: db=SQLDB(\\\"sqlite://test.db\\\")\n>>> if len(sys.argv)>1: db=SQLDB(sys.argv[1])\n>>> tmp=db.define_table('users',\\\n          SQLField('stringf','string',length=32,required=True),\\\n          SQLField('booleanf','boolean',default=False),\\\n          SQLField('passwordf','password',notnull=True),\\\n          SQLField('blobf','blob'),\\\n          SQLField('uploadf','upload'),\\\n          SQLField('integerf','integer',unique=True),\\\n          SQLField('doublef','double',unique=True,notnull=True),\\\n          SQLField('datef','date',default=datetime.date.today()),\\\n          SQLField('timef','time'),\\\n          SQLField('datetimef','datetime'),\\\n          migrate='test_user.table')\n\n   Insert a field\n\n>>> db.users.insert(stringf='a',booleanf=True,passwordf='p',blobf='0A',\\\n                   uploadf=None, integerf=5,doublef=3.14,\\\n                   datef=datetime.date(2001,1,1),\\\n                   timef=datetime.time(12,30,15),\\\n                   datetimef=datetime.datetime(2002,2,2,12,30,15))\n1\n\nDrop the table   \n\n>>> db.users.drop()\n\nExamples of insert, select, update, delete\n\n>>> tmp=db.define_table('person',\\\n          SQLField('name'),\\\n          SQLField('birth','date'),\\\n          migrate='test_person.table')\n>>> person_id=db.person.insert(name=\\\"Marco\\\",birth='2005-06-22')\n>>> person_id=db.person.insert(name=\\\"Massimo\\\",birth='1971-12-21')\n>>> len(db().select(db.person.ALL))\n2\n>>> me=db(db.person.id==person_id).select()[0] # test select\n>>> me.name\n'Massimo'\n>>> db(db.person.name=='Massimo').update(name='massimo') # test update\n1\n>>> db(db.person.name=='Marco').delete() # test delete\n1\n\nUpdate a single record\n\n>>> me.update_record(name=\\\"Max\\\")\n>>> me.name\n'Max'\n\nExamples of complex search conditions\n\n>>> len(db((db.person.name=='Max')&(db.person.birth<'2003-01-01')).select())\n1\n>>> len(db((db.person.name=='Max')&(db.person.birth<datetime.date(2003,01,01))).select())\n1\n>>> len(db((db.person.name=='Max')|(db.person.birth<'2003-01-01')).select())\n1\n>>> me=db(db.person.id==person_id).select(db.person.name)[0] \n>>> me.name\n'Max'\n  \nExamples of search conditions using extract from date/datetime/time      \n\n>>> len(db(db.person.birth.month()==12).select())\n1\n>>> len(db(db.person.birth.year()>1900).select())\n1\n\nExample of usage of NULL\n\n>>> len(db(db.person.birth==None).select()) ### test NULL\n0\n>>> len(db(db.person.birth!=None).select()) ### test NULL\n1\n\nExamples of search consitions using lower, upper, and like\n\n>>> len(db(db.person.name.upper()=='MAX').select())\n1\n>>> len(db(db.person.name.like('%ax')).select())\n1\n>>> len(db(db.person.name.upper().like('%AX')).select())\n1\n>>> len(db(~db.person.name.upper().like('%AX')).select())\n0\n\norderby, groupby and limitby \n\n>>> people=db().select(db.person.name,orderby=db.person.name)\n>>> order=db.person.name|~db.person.birth\n>>> people=db().select(db.person.name,orderby=order)\n\n>>> people=db().select(db.person.name,orderby=db.person.name,groupby=db.person.name)\n\n>>> people=db().select(db.person.name,orderby=order,limitby=(0,100))\n\nExample of one 2 many relation\n\n>>> tmp=db.define_table('dog',\\\n           SQLField('name'),\\\n           SQLField('birth','date'),\\\n           SQLField('owner',db.person),\\\n           migrate='test_dog.table')\n>>> db.dog.insert(name='Snoopy',birth=None,owner=person_id)\n1\n\nA simple JOIN\n\n>>> len(db(db.dog.owner==db.person.id).select())\n1\n\n>>> len(db().select(db.person.ALL,db.dog.name,left=db.dog.on(db.dog.owner==db.person.id)))\n1\n\nDrop tables\n\n>>> db.dog.drop()\n>>> db.person.drop()\n\nExample of many 2 many relation and SQLSet\n \n>>> tmp=db.define_table('author',SQLField('name'),\\\n                        migrate='test_author.table')\n>>> tmp=db.define_table('paper',SQLField('title'),\\\n                        migrate='test_paper.table')\n>>> tmp=db.define_table('authorship',\\\n        SQLField('author_id',db.author),\\\n        SQLField('paper_id',db.paper),\\\n        migrate='test_authorship.table')\n>>> aid=db.author.insert(name='Massimo')\n>>> pid=db.paper.insert(title='QCD')\n>>> tmp=db.authorship.insert(author_id=aid,paper_id=pid)\n\nDefine a SQLSet\n\n>>> authored_papers=db((db.author.id==db.authorship.author_id)&(db.paper.id==db.authorship.paper_id))\n>>> rows=authored_papers.select(db.author.name,db.paper.title)\n>>> for row in rows: print row.author.name, row.paper.title\nMassimo QCD\n\nExample of search condition using  belongs\n\n>>> set=(1,2,3)\n>>> rows=db(db.paper.id.belongs(set)).select(db.paper.ALL)\n>>> print rows[0].title\nQCD\n\nExample of search condition using nested select\n\n>>> nested_select=db()._select(db.authorship.paper_id)\n>>> rows=db(db.paper.id.belongs(nested_select)).select(db.paper.ALL)\n>>> print rows[0].title\nQCD\n\nExample of expressions\n\n>>> mynumber=db.define_table('mynumber',SQLField('x','integer'))\n>>> db(mynumber.id>0).delete()\n0\n>>> for i in range(10): tmp=mynumber.insert(x=i)\n>>> db(mynumber.id>0).select(mynumber.x.sum())[0]._extra[mynumber.x.sum()]\n45\n>>> db(mynumber.x+2==5).select(mynumber.x+2)[0]._extra[mynumber.x+2]\n5\n\nOutput in csv\n\n>>> print str(authored_papers.select(db.author.name,db.paper.title)).strip()\nauthor.name,paper.title\\r\nMassimo,QCD\n\nDelete all leftover tables\n\n# >>> SQLDB.distributed_transaction_commit(db)\n\n>>> db.mynumber.drop()\n>>> db.authorship.drop()\n>>> db.author.drop()\n>>> db.paper.drop()\n\"\"\"\n\n\n", "func_signal": "def test_all():\n", "code": "\nimport doctest\ndoctest.testmod()", "path": "pygnite\\sql.py", "repo_name": "szarak/pygnite", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 184}
{"docstring": "\"\"\"\nRoute decorator.\n\nexample::\n\n    @url('/')\n    def foo(request):\n        pass\n\nwill return foo function when address is /\n\n:param regex: Regexp for url path.\n:param methods: Lists of methods, if ['*'] match all. \n:param content_type: Content Type of returned text. \n\n\"\"\"\n", "func_signal": "def url(regex, methods=['*'], content_type='text/html'):\n", "code": "def wrap(f):\n    if methods[0] == '*':\n        _methods = routes.keys()\n    else:\n        _methods = methods\n    wildcards = { '*' : '.+', '@' : '\\w+', '#' : '\\d+', '$' : '[^/]+' }\n\n    for method in _methods:\n        if method in routes:\n            if not routes[method].has_key(regex):\n                if isinstance(regex, str):\n                    # If regex is string, convert it to regexp...\n                    pattern = re.compile('([%s]+):?(\\w+)?(\\?)?' % ''.join(wildcards.keys()))\n                    u = '^'\n                    for part in [part for part in regex.split('/') if part]:\n                        match = pattern.match(part)\n                        u += '/'\n                        if match:\n                            (wildcard, name, is_optional) = match.groups()\n                            wildcard = wildcards.get(wildcard, '.*')\n                            tmp = ''\n                            if name:\n                                tmp += '(?P<%s>%s)' % (name, wildcard)\n                            else:\n                                tmp += '(%s)' % wildcard\n                            if is_optional is not None:\n                                tmp = '?%s?' % tmp\n                            u += tmp\n                        else:\n                            u += part\n                    u += '/?$'\n                else:\n                    # Else u = regex (allow regexp in @url())\n                    u = regex\n\n                route = { re.compile(u) : (f, content_type) }\n                routes[method].update(route)\n    return f\nreturn wrap", "path": "pygnite\\http.py", "repo_name": "szarak/pygnite", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 184}
{"docstring": "\"\"\"\nShortcut for:\n    \n    >>> @url(regex, methods=['POST'])\n\"\"\"\n", "func_signal": "def post(regex, **kwds):\n", "code": "def wrap(f):\n    return url(regex, methods=['POST'], **kwds)(f)\nreturn wrap", "path": "pygnite\\http.py", "repo_name": "szarak/pygnite", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 184}
{"docstring": "'''\nConverts a unicode string into US-ASCII, using a simple conversion scheme. Each unicode character that\ndoes not have a US-ASCII equivalent is converted into a URL escaped form based on its hexadecimal value.\nFor example, the unicode character '\\u4e86' will become the string '%4e%86'\n\n@param string: unicode string, the unicode string to convert into an escaped US-ASCII form\n@return: string, the US-ASCII escaped form of the inputed string \n\n'''\n", "func_signal": "def escape_unicode(string):\n", "code": "returnValue = StringIO()\n\nfor character in string:\n    code = ord(character)\n    if code > 0x7F:\n        hexCode = hex(code)\n        returnValue.write('%' + hexCode[2:4] + '%' + hexCode[4:6])\n    else:\n        returnValue.write(character)\n\nreturn returnValue.getvalue()", "path": "pygnite\\validators.py", "repo_name": "szarak/pygnite", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 184}
{"docstring": "\"\"\"\nShortcut for:\n\n    >>> @url(regex, methods=['PUT'])\n\"\"\"\n", "func_signal": "def put(regex, **kwds):\n", "code": "def wrap(f):\n    return url(regex, methods=['PUT'], **kwds)(f)\nreturn wrap", "path": "pygnite\\http.py", "repo_name": "szarak/pygnite", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 184}
{"docstring": "\"\"\"\nShortcut for:\n\n    >>> @url(regex, methods=['GET'])\n\"\"\"\n", "func_signal": "def get(regex, **kwds):\n", "code": "def wrap(f):\n    return url(regex, methods=['GET'], **kwds)(f)\nreturn wrap", "path": "pygnite\\http.py", "repo_name": "szarak/pygnite", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 184}
{"docstring": "\"\"\"\nReturn hashed string by ``digest_alg``.\n\"\"\"\n\n", "func_signal": "def hash(value, digest_alg='md5'):\n", "code": "h = hashlib.new(digest_alg)\nh.update(value)\nreturn h.hexdigest()", "path": "pygnite\\utils.py", "repo_name": "szarak/pygnite", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 184}
{"docstring": "'''\nFollows the steps in RFC 3490, Section 4 to convert a unicode authority string into its ASCII equivalent.\nFor example, u'www.Alliancefran\\xe7aise.nu' will be converted into 'www.xn--alliancefranaise-npb.nu'\n\n@param authority: unicode string, the URL authority component to convert,\n                  e.g. u'www.Alliancefran\\xe7aise.nu'\n@return: string, the US-ASCII character equivalent to the inputed authority,\n         e.g. 'www.xn--alliancefranaise-npb.nu'\n@raise Exception: if the function is not able to convert the inputed authority\n\n@author: Jonathan Benn \n'''\n#RFC 3490, Section 4, Step 1\n#The encodings.idna Python module assumes that AllowUnassigned == True\n\n#RFC 3490, Section 4, Step 2\n", "func_signal": "def unicode_to_ascii_authority(authority):\n", "code": "labels = label_split_regex.split(authority)\n\n#RFC 3490, Section 4, Step 3\n#The encodings.idna Python module assumes that UseSTD3ASCIIRules == False\n\n#RFC 3490, Section 4, Step 4\n#We use the ToASCII operation because we are about to put the authority into an IDN-unaware slot\nasciiLabels = []\nfor label in labels:\n    if label:\n        asciiLabels.append(encodings.idna.ToASCII(label))\n    else:\n        #encodings.idna.ToASCII does not accept an empty string, but it is necessary for us to\n        #allow for empty labels so that we don't modify the URL\n        asciiLabels.append('')\n\n#RFC 3490, Section 4, Step 5\nreturn str(reduce(lambda x, y: x + unichr(0x002E) + y, asciiLabels))", "path": "pygnite\\validators.py", "repo_name": "szarak/pygnite", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 184}
{"docstring": "\"\"\"\nMain pygnite function which lunches supported server.\n\n:param mode: Server mode (see server module for supported)\n:param host: Hostname.\n:param port: Port.\n:param server_conf: Extra server configuration.\n:param templates_path: Path to templates.\n:param session_key: Session key.\n:param session_secret: Session secret.\n:param debug: if debug is True, show traceback in console and www, if console - only console, if www - only www. Default: True.\n\"\"\"\n", "func_signal": "def pygnite(**conf):\n", "code": "global debug\n\n## Conf to var assignment\n# Serving conf\nmode = conf.get('mode', 'dev')\nhost = conf.get('host', '127.0.0.1')\nport = conf.get('port', 6060)\nserver_conf = conf.get('server_conf', {})\n# templates path. ofc you can add it manually by template.append_path\ntemplates_path = conf.get('templates_path', os.path.join(sys.path[0], 'templates'))\nappend_path(templates_path)\n# Session config\nsession_key = conf.get('session_key', 'mysession')\nsession_secret = conf.get('session_secret', 'randomsecret')\n# debug:\ndebug = conf.get('debug', True)\n\nif not mode in server.SERVERS:\n    # if mode not supported, choose dev\n    mode = 'dev'\n\n## Session middleware:\napp = SessionMiddleware(create_app, key=session_key, secret=session_secret)\n\nif mode == 'dev' and not server_conf.has_key('auto_reload'):\n    server_conf['auto_reload'] = True\n\nreturn getattr(server, mode)(app, host, port, **server_conf)", "path": "pygnite\\main.py", "repo_name": "szarak/pygnite", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 184}
{"docstring": "# ## deal with particular case first:\n\n", "func_signal": "def _pool_connection(self, f):\n", "code": "        if not self._pool_size:\n            self._connection = f()\n            return\n        uri = self._uri\n        sql_locker.acquire()\n        if not uri in self._connection_pools:\n            self._connection_pools[uri] = []\n        if self._connection_pools[uri]:\n            self._connection = self._connection_pools[uri].pop()\n            sql_locker.release()\n        else:\n            sql_locker.release()\n            self._connection = f()", "path": "pygnite\\sql.py", "repo_name": "szarak/pygnite", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 184}
{"docstring": "# # this only changes value, not _value\n\n", "func_signal": "def _validate(self):\n", "code": "        name = self['_name']\n        if name == None or name == '':\n            return True\n        name = str(name)\n        if self['_type'] != 'checkbox':\n            self['old_value'] = self['value'] or self['_value'] or ''\n            value = self.request_vars.get(name, '')\n        else:\n            self['old_value'] = self['value'] or False\n            value = self.request_vars.get(name, False)\n        self['value'] = value\n        requires = self['requires']\n        if requires:\n            if not isinstance(requires, (list, tuple)):\n                requires = [requires]\n            for validator in requires:\n                (value, errors) = validator(value)\n                if errors != None:\n                    self.vars[name] = value\n                    self.errors[name] = errors\n                    break\n        if not name in self.errors:\n            self.vars[name] = value\n            return True\n        return False", "path": "pygnite\\html.py", "repo_name": "szarak/pygnite", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 184}
{"docstring": "\"\"\"\nController for serving static files.\n\n:param static_path: Path to static files.\n:param indexes: List files (True/False).\n:param f: File.\n\"\"\"\n\n", "func_signal": "def serve_static(static_path, indexes=False, f=None):\n", "code": "path = os.path.join(static_path, f or '')\n\nif not os.path.exists(path):\n    return _404()\n\nif f and os.path.isfile(path):\n    from mimetypes import guess_type\n\n    f = open(path, 'rb')\n\n    response = Response(body=f.read(), content_type=guess_type(path)[0] or 'text/plain')\n    f.close()\n\n    return response\nelse:\n    if indexes:\n        files = os.listdir(path)\n\n        return render('list_files.html', files=[f for f in files if os.path.isfile(os.path.join(path, f))], \n                                         dirs=[d for d in files if os.path.isdir(os.path.join(path, d))])\n    else:\n        return _status(403, '403.html')", "path": "pygnite\\http.py", "repo_name": "szarak/pygnite", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 184}
{"docstring": "\"\"\"\nAlways returns a SQLRows object, even if it may be empty\n\"\"\"\n\n", "func_signal": "def select(self, *fields, **attributes):\n", "code": "def response(query):\n    self._db['_lastsql'] = query\n    self._db._execute(query)\n    return self._db._cursor.fetchall()\n\nif not attributes.get('cache', None):\n    query = self._select(*fields, **attributes)\n    r = response(query)\nelse:\n    (cache_model, time_expire) = attributes['cache']\n    del attributes['cache']\n    query = self._select(*fields, **attributes)\n    key = self._db._uri + '/' + query\n    r = cache_model(key, lambda : response(query), time_expire)\nif self._db._dbname in ['mssql', 'mssql2', 'db2']:\n    r = r[(attributes.get('limitby', None) or (0,))[0]:]\nreturn SQLRows(self._db, r, *self.colnames)", "path": "pygnite\\sql.py", "repo_name": "szarak/pygnite", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 184}
{"docstring": "\"\"\"This function is wrapper which returns ``SQLDB`` known from web2py.\n\n:param engine: Database engine, e.g. ``sqlite3`` or ``mysql``. \n:param db: If engine is sqlite, path to database, else database name. \n:param host: Host. Default: `localhost`. \n:param username: Usename.\n:param password: Password.\n:param port: Port.\n\"\"\"\n", "func_signal": "def database(engine='sqlite3', db='database.db', host=None, username=None, password=None, port=None):\n", "code": "if engine == 'sqlite3':\n    if not db.startswith('/'):\n        root = sys.path[0]\n        db_path = os.path.join(root, db)\n        db_folder = os.path.dirname(db_path)\n\n        if not os.path.isdir(db_folder):\n            os.makedirs(db_folder)\n    else:\n        db_path = db\n\n    return SQLDB('sqlite://%s' % db_path)\nelif engine == 'oracle':\n    return SQLDB('oracle://%s/%s@%s' % (username, password, db))\nelse:\n    if port is not None:\n        return SQLDB('%s://%s:%s@%s:%s/%s' % (engine, username, password, host or 'localhost', port, db))\n    else:\n        return SQLDB('%s://%s:%s@%s/%s' % (engine, username, password, host or 'localhost', db))", "path": "pygnite\\sql.py", "repo_name": "szarak/pygnite", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 184}
{"docstring": "\"\"\"\nserializes the table into a csv file\n\"\"\"\n\n", "func_signal": "def __str__(self):\n", "code": "s = cStringIO.StringIO()\nself.export_to_csv_file(s)\nreturn s.getvalue()", "path": "pygnite\\sql.py", "repo_name": "szarak/pygnite", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 184}
{"docstring": "\"\"\"\n@param value: a string, the URL to validate\n@return: a tuple, where tuple[0] is the inputed value (possible prepended with prepend_scheme),\n         and tuple[1] is either None (success!) or the string error_message  \n\"\"\"\n\n", "func_signal": "def __call__(self, value):\n", "code": "try:\n\n    # if the URL does not mis-use the '%' character\n\n    if not re.compile(r\"%[^0-9A-Fa-f]{2}|%[^0-9A-Fa-f][0-9A-Fa-f]|%[0-9A-Fa-f][^0-9A-Fa-f]|%$|%[0-9A-Fa-f]$|%[^0-9A-Fa-f]$\"\n                      ).search(value):\n\n        # if the URL is only composed of valid characters\n\n        if re.compile(r\"[A-Za-z0-9;/?:@&=+$,\\-_\\.!~*'\\(\\)%#]+$\"\n                      ).match(value):\n\n            # Then split up the URL into its components and check on the scheme\n\n            scheme = url_split_regex.match(value).group(2)\n\n            # Clean up the scheme before we check it\n\n            if scheme != None:\n                scheme = urllib.unquote(scheme).lower()\n\n            # If the scheme really exists\n\n            if scheme in self.allowed_schemes:\n\n                # Then the URL is valid\n\n                return (value, None)\n            else:\n\n                # else, for the possible case of abbreviated URLs with ports, check to see if adding a valid\n                # scheme fixes the problem (but only do this if it doesn't have one already!)\n\n                if not re.compile('://').search(value) and None\\\n                     in self.allowed_schemes:\n                    schemeToUse = self.prepend_scheme or 'http'\n                    prependTest = self.__call__(schemeToUse\n                             + '://' + value)\n\n                    # if the prepend test succeeded\n\n                    if prependTest[1] == None:\n\n                        # if prepending in the output is enabled\n\n                        if self.prepend_scheme:\n                            return prependTest\n                        else:\n\n                            # else return the original, non-prepended value\n\n                            return (value, None)\nexcept:\n    pass\n\n# else the URL is not valid\n\nreturn (value, self.error_message)", "path": "pygnite\\validators.py", "repo_name": "szarak/pygnite", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 184}
{"docstring": "\"\"\"\nShortcut for:\n\n    >>> @url(regex, methods=['DELETE'])\n\"\"\"\n", "func_signal": "def delete(regex, **kwds):\n", "code": "def wrap(f):\n    return url(regex, methods=['DELETE'], **kwds)(f)\nreturn wrap", "path": "pygnite\\http.py", "repo_name": "szarak/pygnite", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 184}
{"docstring": "\"\"\"\niterator over records\n\"\"\"\n\n", "func_signal": "def __iter__(self):\n", "code": "for i in xrange(len(self)):\n    yield self[i]", "path": "pygnite\\sql.py", "repo_name": "szarak/pygnite", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 184}
{"docstring": "\"\"\"\n@param value: a string, the URL to validate\n@return: a tuple, where tuple[0] is the inputed value (possible prepended with prepend_scheme),\n         and tuple[1] is either None (success!) or the string error_message\n\"\"\"\n\n", "func_signal": "def __call__(self, value):\n", "code": "try:\n\n    # if the URL passes generic validation\n\n    x = IS_GENERIC_URL(error_message=self.error_message,\n                       allowed_schemes=self.allowed_schemes,\n                       prepend_scheme=self.prepend_scheme)\n    if x(value)[1] == None:\n        componentsMatch = url_split_regex.match(value)\n        authority = componentsMatch.group(4)\n\n        # if there is an authority component\n\n        if authority:\n\n            # if authority is a valid IP address\n\n            if re.compile('\\d+\\.\\d+\\.\\d+\\.\\d+(:\\d*)*$'\n                          ).match(authority):\n\n                # Then this HTTP URL is valid\n\n                return (value, None)\n            else:\n\n                # else if authority is a valid domain name\n\n                domainMatch = \\\n                    re.compile('(([A-Za-z0-9]+[A-Za-z0-9\\-]*[A-Za-z0-9]+\\.)*([A-Za-z0-9]+\\.)*)*([A-Za-z]+[A-Za-z0-9\\-]*[A-Za-z0-9]+)\\.?(:\\d*)*$'\n                        ).match(authority)\n                if domainMatch:\n\n                    # if the top-level domain really exists\n\n                    if domainMatch.group(4).lower()\\\n                         in official_top_level_domains:\n\n                        # Then this HTTP URL is valid\n\n                        return (value, None)\n        else:\n\n            # else this is a relative/abbreviated URL, which will parse into the URL's path component\n\n            path = componentsMatch.group(5)\n\n            # relative case: if this is a valid path (if it starts with a slash)\n\n            if re.compile('/').match(path):\n\n                # Then this HTTP URL is valid\n\n                return (value, None)\n            else:\n\n                # abbreviated case: if we haven't already, prepend a scheme and see if it fixes the problem\n\n                if not re.compile('://').search(value):\n                    schemeToUse = self.prepend_scheme or 'http'\n                    prependTest = self.__call__(schemeToUse\n                             + '://' + value)\n\n                    # if the prepend test succeeded\n\n                    if prependTest[1] == None:\n\n                        # if prepending in the output is enabled\n\n                        if self.prepend_scheme:\n                            return prependTest\n                        else:\n\n                            # else return the original, non-prepended value\n\n                            return (value, None)\nexcept:\n    pass\n\n# else the HTTP URL is not valid\n\nreturn (value, self.error_message)", "path": "pygnite\\validators.py", "repo_name": "szarak/pygnite", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Inits this event with JSON data.\n\nArgs:\n  json: JSON data from Wave server.\n\"\"\"\n", "func_signal": "def __init__(self, json):\n", "code": "self.modifiedBy = json.get('modifiedBy')\nself.properties = json.get('properties', {})\nself.timestamp = json.get('timestamp', 0)\nself.type = json.get('type')\nself.raw_data = json", "path": "waveapi\\model.py", "repo_name": "scottferg/sa-emote", "stars": 1, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"Create a new wave with the initial participants on it.\"\"\"\n# we shouldn't need a wave/wavelet id here, but we do\n", "func_signal": "def NewWave(context, participants=None):\n", "code": "wavelet = context.GetRootWavelet()\nreturn context.builder.WaveletCreate(wavelet.GetWaveId(), wavelet.GetId(), participants)", "path": "waveapi\\robot_abstract.py", "repo_name": "scottferg/sa-emote", "stars": 1, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"Parse a JSON string and return a context and an event list.\"\"\"\n", "func_signal": "def ParseJSONBody(json_body):\n", "code": "json = simplejson.loads(json_body)\n# TODO(davidbyttow): Remove this once no longer needed.\ndata = util.CollapseJavaCollections(json)\ncontext = ops.CreateContext(data)\nevent_list = [model.Event(event_data) for event_data in data['events']]\nreturn context, event_list", "path": "waveapi\\robot_abstract.py", "repo_name": "scottferg/sa-emote", "stars": 1, "license": "None", "language": "python", "size": 173}
{"docstring": "# Read JSON POST input to jsonfilter.json if matching mime type\n", "func_signal": "def __call__(self, environ, start_response):\n", "code": "response = {'status': '200 OK', 'headers': []}\ndef json_start_response(status, headers):\n    response['status'] = status\n    response['headers'].extend(headers)\nenviron['jsonfilter.mime_type'] = self.mime_type\nif environ.get('REQUEST_METHOD', '') == 'POST':\n    if environ.get('CONTENT_TYPE', '') == self.mime_type:\n        args = [_ for _ in [environ.get('CONTENT_LENGTH')] if _]\n        data = environ['wsgi.input'].read(*map(int, args))\n        environ['jsonfilter.json'] = simplejson.loads(data)\nres = simplejson.dumps(self.app(environ, json_start_response))\njsonp = cgi.parse_qs(environ.get('QUERY_STRING', '')).get('jsonp')\nif jsonp:\n    content_type = 'text/javascript'\n    res = ''.join(jsonp + ['(', res, ')'])\nelif 'Opera' in environ.get('HTTP_USER_AGENT', ''):\n    # Opera has bunk XMLHttpRequest support for most mime types\n    content_type = 'text/plain'\nelse:\n    content_type = self.mime_type\nheaders = [\n    ('Content-type', content_type),\n    ('Content-length', len(res)),\n]\nheaders.extend(response['headers'])\nstart_response(response['status'], headers)\nreturn [res]", "path": "waveapi\\simplejson\\jsonfilter.py", "repo_name": "scottferg/sa-emote", "stars": 1, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"Registers all event handlers exported by the given object.\n\nArgs:\n  listener: an object with methods corresponding to wave events.\n    Methods should be named either in camel case, e.g. 'OnBlipSubmitted',\n    or in lowercase, e.g. 'on_blip_submitted', with names corresponding\n    to the event names in the events module.\n\"\"\"\n", "func_signal": "def RegisterListener(self, listener):\n", "code": "for event in dir(events):\n  if event.startswith('_'):\n    continue\n  lowercase_method_name = 'on_' + event.lower()\n  camelcase_method_name = 'On' + util.ToUpperCamelCase(event)\n  if hasattr(listener, lowercase_method_name):\n    handler = getattr(listener, lowercase_method_name)\n  elif hasattr(listener, camelcase_method_name):\n    handler = getattr(listener, camelcase_method_name)\n  else:\n    continue\n  if callable(handler):\n    self.RegisterHandler(event, handler)", "path": "waveapi\\robot_abstract.py", "repo_name": "scottferg/sa-emote", "stars": 1, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"Return the (first) gadget that has the specified url.\n\nIf no matching gadget can be found, return None. If url\nis None, return the first gadget that can be found.\n\"\"\"\n", "func_signal": "def GetGadgetByUrl(self, url):\n", "code": "for el in self.elements.values():\n  if (el.type == document.ELEMENT_TYPE.GADGET\n      and getattr(el, 'url', None) == url):\n    return el\nreturn None", "path": "waveapi\\model.py", "repo_name": "scottferg/sa-emote", "stars": 1, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"Sets up the webapp handlers for this robot and starts listening.\n\nArgs:\n  debug: Optional variable that defaults to False and is passed through\n      to the webapp application to determine if it should show debug info.\n\"\"\"\n# App Engine expects to construct a class with no arguments, so we\n# pass a lambda that constructs the appropriate handler with\n# arguments from the enclosing scope.\n", "func_signal": "def Run(self, debug=False):\n", "code": "app = webapp.WSGIApplication([\n    ('/_wave/capabilities.xml', lambda: RobotCapabilitiesHandler(self)),\n    ('/_wave/robot/profile', lambda: RobotProfileHandler(self)),\n    ('/_wave/robot/jsonrpc', lambda: RobotEventHandler(self)),\n], debug=debug)\nrun_wsgi_app(app)", "path": "waveapi\\robot.py", "repo_name": "scottferg/sa-emote", "stars": 1, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"Initializes self with robot information.\"\"\"\n", "func_signal": "def __init__(self, name, version, image_url='', profile_url=''):\n", "code": "self._handlers = {}\nself.name = name\nself.version = version\nself.image_url = image_url\nself.profile_url = profile_url\nself.cron_jobs = []", "path": "waveapi\\robot_abstract.py", "repo_name": "scottferg/sa-emote", "stars": 1, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"Return this robot's capabilities as an XML string.\"\"\"\n", "func_signal": "def GetCapabilitiesXml(self):\n", "code": "lines = ['<w:version>%s</w:version>' % self.version]\n\nlines.append('<w:capabilities>')\nfor capability in self._handlers:\n  lines.append('  <w:capability name=\"%s\"/>' % capability)\nlines.append('</w:capabilities>')\n\nif self.cron_jobs:\n  lines.append('<w:crons>')\n  for job in self.cron_jobs:\n    lines.append('  <w:cron path=\"%s\" timerinseconds=\"%s\"/>' % job)\n  lines.append('</w:crons>')\n\nrobot_attrs = ' name=\"%s\"' % self.name\nif self.image_url:\n  robot_attrs += ' imageurl=\"%s\"' % self.image_url\nif self.profile_url:\n  robot_attrs += ' profileurl=\"%s\"' % self.profile_url\nlines.append('<w:profile%s/>' % robot_attrs)\nreturn ('<?xml version=\"1.0\"?>\\n'\n        '<w:robot xmlns:w=\"http://wave.google.com/extensions/robots/1.0\">\\n'\n        '%s\\n</w:robot>\\n') % ('\\n'.join(lines))", "path": "waveapi\\robot_abstract.py", "repo_name": "scottferg/sa-emote", "stars": 1, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"Return a JSON string representing the given context.\"\"\"\n", "func_signal": "def SerializeContext(context, version):\n", "code": "context_dict = util.Serialize(context)\ncontext_dict['version'] = str(version)\nreturn simplejson.dumps(context_dict)", "path": "waveapi\\robot_abstract.py", "repo_name": "scottferg/sa-emote", "stars": 1, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"Inits this wavelet with JSON data.\n\nArgs:\n  json: JSON data dictionary from Wave server.\n\"\"\"\n", "func_signal": "def __init__(self, json):\n", "code": "self.creator = json.get('creator')\nself.creationTime = json.get('creationTime', 0)\nself.dataDocuments = json.get('dataDocuments', {})\nself.lastModifiedTime = json.get('lastModifiedTime')\nself.participants = set(json.get('participants', []))\nself.rootBlipId = json.get('rootBlipId')\nself.title = json.get('title', '')\nself.waveId = json.get('waveId')\nself.waveletId = json.get('waveletId')\nself.raw_data = json", "path": "waveapi\\model.py", "repo_name": "scottferg/sa-emote", "stars": 1, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"Handles HTTP GET request.\"\"\"\n", "func_signal": "def get(self):\n", "code": "xml = self._robot.GetCapabilitiesXml()\nself.response.headers['Content-Type'] = 'text/xml'\nself.response.out.write(xml)", "path": "waveapi\\robot.py", "repo_name": "scottferg/sa-emote", "stars": 1, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"Handles the get event for debugging. Ops usually too long.\"\"\"\n", "func_signal": "def get(self):\n", "code": "ops = self.request.get('ops')\nlogging.info('get: ' + ops)\nif ops:\n  self.request.body = ops\n  self.post()\n  self.response.headers['Content-Type'] = 'text/html'", "path": "waveapi\\robot.py", "repo_name": "scottferg/sa-emote", "stars": 1, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"Calls all of the handlers associated with an event.\"\"\"\n", "func_signal": "def HandleEvent(self, event, context):\n", "code": "for handler in self._handlers.get(event.type, []):\n  # TODO(jacobly): pass the event in to the handlers directly\n  # instead of passing the properties dictionary.\n  handler(event.properties, context)", "path": "waveapi\\robot_abstract.py", "repo_name": "scottferg/sa-emote", "stars": 1, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"Inits this wave with JSON data.\n\nArgs:\n  json: JSON data dictionary from Wave server.\n\nAttributes:\n  raw_data: Dictionary of incoming raw JSON data.\n  waveId: String id of this wave.\n  waveletId: String id of this wavelet.\n\"\"\"\n", "func_signal": "def __init__(self, json):\n", "code": "self.waveId = json.get('waveId')\nself.waveletIds = set(json.get('waveletIds', []))\nself.raw_data = json", "path": "waveapi\\model.py", "repo_name": "scottferg/sa-emote", "stars": 1, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"Handles HTTP GET request.\"\"\"\n", "func_signal": "def get(self):\n", "code": "self.response.headers['Content-Type'] = 'application/json'\nself.response.out.write(self._robot.GetProfileJson())", "path": "waveapi\\robot.py", "repo_name": "scottferg/sa-emote", "stars": 1, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"Handles HTTP POST requests.\"\"\"\n", "func_signal": "def post(self):\n", "code": "json_body = self.request.body\nif not json_body:\n  # TODO(davidbyttow): Log error?\n  return\n\njson_body = unicode(json_body, 'utf8')\nlogging.info('Incoming: ' + json_body)\n\ncontext, events = robot_abstract.ParseJSONBody(json_body)\nfor event in events:\n  try:\n    self._robot.HandleEvent(event, context)\n  except:\n    logging.error(traceback.format_exc())\n\njson_response = robot_abstract.SerializeContext(context,\n                                                self._robot.version)\nlogging.info('Outgoing: ' + json_response)\n\n# Build the response.\nself.response.headers['Content-Type'] = 'application/json; charset=utf-8'\nself.response.out.write(json_response.encode('utf-8'))", "path": "waveapi\\robot.py", "repo_name": "scottferg/sa-emote", "stars": 1, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"Returns a data document for this wavelet based on key name.\"\"\"\n", "func_signal": "def GetDataDocument(self, name, default=None):\n", "code": "if self.dataDocuments:\n  return self.dataDocuments.get(name, default)\nreturn default", "path": "waveapi\\model.py", "repo_name": "scottferg/sa-emote", "stars": 1, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"Inits this blip with JSON data.\n\nArgs:\n  json: JSON data dictionary from Wave server.\n\"\"\"\n", "func_signal": "def __init__(self, json):\n", "code": "self.blipId = json.get('blipId')\nself.childBlipIds = set(json.get('childBlipIds', []))\nself.content = json.get('content', '')\nself.contributors = set(json.get('contributors', []))\nself.creator = json.get('creator')\nself.lastModifiedTime = json.get('lastModifiedTime', 0)\nself.parentBlipId = json.get('parentBlipId')\nself.waveId = json.get('waveId')\nself.waveletId = json.get('waveletId')\nself.annotations = []\nfor annotation in json.get('annotations', []):\n  r = document.Range(annotation['range']['start'],\n                     annotation['range']['end'])\n  self.annotations.append(document.Annotation(\n      annotation['name'], annotation['value'], r=r))\nself.document = Document(self)\nself.elements = {}\njson_elements = json.get('elements', {})\nfor elem in json_elements:\n  self.elements[elem] = document.ElementFromJson(json_elements[elem])\nself.raw_data = json", "path": "waveapi\\model.py", "repo_name": "scottferg/sa-emote", "stars": 1, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"Returns the root wavelet or None if it is not in this context.\"\"\"\n", "func_signal": "def GetRootWavelet(self):\n", "code": "for wavelet_id, wavelet in self.wavelets.items():\n  if wavelet_id.endswith(ROOT_WAVELET_ID_SUFFIX):\n    return wavelet\nlogging.warning('Could not retrieve root wavelet.')\nreturn None", "path": "waveapi\\model.py", "repo_name": "scottferg/sa-emote", "stars": 1, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"Handle a msgid.\"\"\"\n", "func_signal": "def handle_mi(self):\n", "code": "if self.current_state in ['MC', 'MS', 'MX']:\n    _listappend(self.instance, self.current_entry)\n    self.current_entry = POEntry()\nself.current_entry.obsolete = self.entry_obsolete\nself.current_entry.msgid = unquote(self.current_token[7:-1])\nreturn True", "path": "inlinetrans\\polib.py", "repo_name": "hyperweek/django-inlinetrans", "stars": 1, "license": "lgpl-3.0", "language": "python", "size": 110}
{"docstring": "\"\"\"\nRun the state machine, parse the file line by line and call process()\nwith the current matched symbol.\n\"\"\"\n", "func_signal": "def parse(self):\n", "code": "i, lastlen = 1, 0\nfor line in self.fhandle:\n    line = _strstrip(line)\n    if line == '':\n        i = i+1\n        continue\n    if line[:3] == '#~ ':\n        line = line[3:]\n        self.entry_obsolete = 1\n    else:\n        self.entry_obsolete = 0\n    self.current_token = line\n    if line[:2] == '#:':\n        # we are on a occurrences line\n        self.process('OC', i)\n    elif line[:7] == 'msgid \"':\n        # we are on a msgid\n        self.process('MI', i)\n    elif line[:8] == 'msgstr \"':\n        # we are on a msgstr\n        self.process('MS', i)\n    elif line[:1] == '\"':\n        # we are on a continuation line or some metadata\n        self.process('MC', i)\n    elif line[:14] == 'msgid_plural \"':\n        # we are on a msgid plural\n        self.process('MP', i)\n    elif line[:7] == 'msgstr[':\n        # we are on a msgstr plural\n        self.process('MX', i)\n    elif line[:3] == '#, ':\n        # we are on a flags line\n        self.process('FL', i)\n    elif line[:2] == '# ' or line == '#':\n        if line == '#': line = line + ' '\n        # we are on a translator comment line\n        self.process('TC', i)\n    elif line[:2] == '#.':\n        # we are on a generated comment line\n        self.process('GC', i)\n    i = i+1\n\nif self.current_entry:\n    # since entries are added when another entry is found, we must add\n    # the last entry here (only if there are lines)\n    _listappend(self.instance, self.current_entry)\n# before returning the instance, check if there's metadata and if \n# so extract it in a dict\nfirstentry = self.instance[0]\nif firstentry.msgid == '': # metadata found\n    # remove the entry\n    firstentry = _listpop(self.instance, 0)\n    self.instance.metadata_is_fuzzy = firstentry.flags\n    key = None\n    for msg in firstentry.msgstr.splitlines():\n        try:\n            key, val = _strsplit(msg, ':', 1)\n            self.instance.metadata[key] = _strstrip(val)\n        except:\n            if key is not None:\n                self.instance.metadata[key] += '\\n'+_strstrip(msg)\n# close opened file\nself.fhandle.close()\nreturn self.instance", "path": "inlinetrans\\polib.py", "repo_name": "hyperweek/django-inlinetrans", "stars": 1, "license": "lgpl-3.0", "language": "python", "size": 110}
{"docstring": "\"\"\"Handle a msgid plural.\"\"\"\n", "func_signal": "def handle_mp(self):\n", "code": "self.current_entry.msgid_plural = unquote(self.current_token[14:-1])\nreturn True", "path": "inlinetrans\\polib.py", "repo_name": "hyperweek/django-inlinetrans", "stars": 1, "license": "lgpl-3.0", "language": "python", "size": 110}
{"docstring": "\"\"\"Return the string representation of the po file\"\"\"\n", "func_signal": "def __str__(self):\n", "code": "ret, headers = '', _strsplit(self.header, '\\n')\nfor header in headers:\n    if header[:1] in [',', ':']:\n        ret += '#%s\\n' % header\n    else:\n        ret += '# %s\\n' % header\nreturn ret + _BaseFile.__str__(self)", "path": "inlinetrans\\polib.py", "repo_name": "hyperweek/django-inlinetrans", "stars": 1, "license": "lgpl-3.0", "language": "python", "size": 110}
{"docstring": "\"\"\"\nConstructor.\n\n**Keyword argument**:\n  - *fpath*: string, path to the po file\n\"\"\"\n", "func_signal": "def __init__(self, fpath):\n", "code": "self.fhandle = open(fpath, 'r')\nself.instance = POFile(fpath=fpath)\nself.transitions = {}\nself.current_entry = POEntry()\nself.current_state = 'ST'\nself.current_token = None\n# two memo flags used in handlers\nself.msgstr_index = 0\nself.entry_obsolete = 0\n# Configure the state machine, by adding transitions.\n# Signification of symbols:\n#     * ST: Beginning of the file (start)\n#     * HE: Header\n#     * TC: a translation comment\n#     * GC: a generated comment\n#     * OC: a file/line occurence\n#     * FL: a flags line\n#     * MI: a msgid\n#     * MP: a msgid plural\n#     * MS: a msgstr\n#     * MX: a msgstr plural\n#     * MC: a msgid or msgstr continuation line\nall_ = ['ST', 'HE', 'GC', 'OC', 'FL', 'TC', 'MS', 'MP', 'MX', 'MI']\n\nself.add('TC', ['ST', 'HE'],                                     'HE')\nself.add('TC', ['GC', 'OC', 'FL', 'TC', 'MS', 'MP', 'MX', 'MI'], 'TC')\nself.add('GC', all_,                                             'GC')\nself.add('OC', all_,                                             'OC')\nself.add('FL', all_,                                             'FL')\nself.add('MI', ['ST', 'HE', 'GC', 'OC', 'FL', 'TC', 'MS', 'MX'], 'MI')\nself.add('MP', ['TC', 'GC', 'MI'],                               'MP')\nself.add('MS', ['MI', 'MP', 'TC'],                               'MS')\nself.add('MX', ['MI', 'MX', 'MP', 'TC'],                         'MX')\nself.add('MC', ['MI', 'MP', 'MS', 'MX'],                         'MC')", "path": "inlinetrans\\polib.py", "repo_name": "hyperweek/django-inlinetrans", "stars": 1, "license": "lgpl-3.0", "language": "python", "size": 110}
{"docstring": "\"\"\"_MOFileParser constructor.\"\"\"\n", "func_signal": "def __init__(self, fpath):\n", "code": "self.fhandle = open(fpath, 'rb')\nself.instance = MOFile(fpath)", "path": "inlinetrans\\polib.py", "repo_name": "hyperweek/django-inlinetrans", "stars": 1, "license": "lgpl-3.0", "language": "python", "size": 110}
{"docstring": "\"\"\"\nAdd a transition to the state machine.\nKeywords arguments:\n\nsymbol     -- string, the matched token (two chars symbol)\nstates     -- list, a list of states (two chars symbols)\nnext_state -- the next state the fsm will have after the action\n\"\"\"\n", "func_signal": "def add(self, symbol, states, next_state):\n", "code": "for state in states:\n    action = getattr(self, 'handle_%s' % next_state.lower())\n    self.transitions[(symbol, state)] = (action, next_state)", "path": "inlinetrans\\polib.py", "repo_name": "hyperweek/django-inlinetrans", "stars": 1, "license": "lgpl-3.0", "language": "python", "size": 110}
{"docstring": "\"\"\"\nUnquote and return the given string *st*.\n\n**Examples**:\n\n>>> unquote('\\\\\\\\t and \\\\\\\\n and \\\\\\\\r and \\\\\\\\\" and \\\\\\\\\\\\\\\\')\n'\\\\t and \\\\n and \\\\r and \" and \\\\\\\\'\n\"\"\"\n# unquote {{{\n", "func_signal": "def unquote(st):\n", "code": "st = _strreplace(st, r'\\\"', '\"')\nst = _strreplace(st, r'\\n', '\\n')\nst = _strreplace(st, r'\\r', '\\r')\nst = _strreplace(st, r'\\t', '\\t')\nst = _strreplace(st, r'\\\\', '\\\\')\nreturn st\n# }}}", "path": "inlinetrans\\polib.py", "repo_name": "hyperweek/django-inlinetrans", "stars": 1, "license": "lgpl-3.0", "language": "python", "size": 110}
{"docstring": "'''\nCalled by comparison operations if rich comparison is not defined.\n\n**Tests**:\n>>> a  = POEntry(msgid='a', occurrences=[('b.py', 1), ('b.py', 3)])\n>>> b  = POEntry(msgid='b', occurrences=[('b.py', 1), ('b.py', 3)])\n>>> c1 = POEntry(msgid='c1', occurrences=[('a.py', 1), ('b.py', 1)])\n>>> c2 = POEntry(msgid='c2', occurrences=[('a.py', 1), ('a.py', 3)])\n>>> po = POFile()\n>>> po.append(a)\n>>> po.append(b)\n>>> po.append(c1)\n>>> po.append(c2)\n>>> po.sort()\n>>> print po\n# \nmsgid \"\"\nmsgstr \"\"\n<BLANKLINE>\n#: a.py:1 a.py:3\nmsgid \"c2\"\nmsgstr \"\"\n<BLANKLINE>\n#: a.py:1 b.py:1\nmsgid \"c1\"\nmsgstr \"\"\n<BLANKLINE>\n#: b.py:1 b.py:3\nmsgid \"a\"\nmsgstr \"\"\n<BLANKLINE>\n#: b.py:1 b.py:3\nmsgid \"b\"\nmsgstr \"\"\n<BLANKLINE>\n'''\n", "func_signal": "def __cmp__(self, other):\n", "code": "def compare_occurrences(a, b):\n    \"\"\"\n    Compare an entry occurrence with another one.\n    \"\"\"\n    if a[0] != b[0]:\n        return a[0] < b[0]\n    if a[1] != b[1]:\n        return a[1] < b[1]\n    return 0\n\n# First: Obsolete test\nif self.obsolete != other.obsolete:\n    if self.obsolete:\n        return -1\n    else:\n        return 1\n# Work on a copy to protect original\nocc1 = self.occurrences[:]\nocc2 = other.occurrences[:]\n# Sorting using compare method\nocc1.sort(compare_occurrences)\nocc2.sort(compare_occurrences)\n# Comparing sorted occurrences\npos = 0\nfor entry1 in occ1:\n    try:\n        entry2 = occ2[pos]\n    except IndexError:\n        return 1\n    pos = pos + 1\n    if entry1[0] != entry2[0]:\n        if entry1[0] > entry2[0]:\n            return 1\n        else:\n            return -1\n    if entry1[1] != entry2[1]:\n        if entry1[1] > entry2[1]:\n            return 1\n        else:\n            return -1\n# Finally: Compare message ID\nif self.msgid > other.msgid: return 1\nelse: return -1", "path": "inlinetrans\\polib.py", "repo_name": "hyperweek/django-inlinetrans", "stars": 1, "license": "lgpl-3.0", "language": "python", "size": 110}
{"docstring": "\"\"\"\norganizes multiple extensions that are separated with commas or passed by\nusing --extension/-e multiple times.\n\nfor example: running 'django-admin makemessages -e js,txt -e xhtml -a'\nwould result in a extension list: ['.js', '.txt', '.xhtml']\n\n>>> handle_extensions(['.html', 'html,js,py,py,py,.py', 'py,.py'])\n['.html', '.js']\n>>> handle_extensions(['.html, txt,.tpl'])\n['.html', '.tpl', '.txt']\n\"\"\"\n", "func_signal": "def handle_extensions(extensions=('html',)):\n", "code": "ext_list = []\nfor ext in extensions:\n    ext_list.extend(ext.replace(' ','').split(','))\nfor i, ext in enumerate(ext_list):\n    if not ext.startswith('.'):\n        ext_list[i] = '.%s' % ext_list[i]\n\n# we don't want *.py files here because of the way non-*.py files\n# are handled in make_messages() (they are copied to file.ext.py files to\n# trick xgettext to parse them as Python files)\nreturn set([x for x in ext_list if x != '.py'])", "path": "inlinetrans\\management\\commands\\inline_makemessages.py", "repo_name": "hyperweek/django-inlinetrans", "stars": 1, "license": "lgpl-3.0", "language": "python", "size": 110}
{"docstring": "\"\"\"\nTry to detect the encoding used by the file *fpath*. The function will\nreturn polib default *encoding* if it's unable to detect it.\n\n**Keyword argument**:\n  - *fpath*: string, full or relative path to the mo file to parse.\n\n**Examples**:\n\n>>> print detect_encoding('tests/test_noencoding.po')\nutf-8\n>>> print detect_encoding('tests/test_utf8.po')\nUTF-8\n>>> print detect_encoding('tests/test_utf8.mo')\nUTF-8\n>>> print detect_encoding('tests/test_iso-8859-15.po')\nISO_8859-15\n>>> print detect_encoding('tests/test_iso-8859-15.mo')\nISO_8859-15\n\"\"\"\n# detect_encoding {{{\n", "func_signal": "def detect_encoding(fpath):\n", "code": "import re\nrx = re.compile(r'\"?Content-Type:.+? charset=([\\w_\\-:\\.]+)')\nf = open(fpath)\nfor l in f:\n    match = rx.search(l)\n    if match:\n        f.close()\n        return _strstrip(match.group(1))\nf.close()\nreturn default_encoding\n# }}}", "path": "inlinetrans\\polib.py", "repo_name": "hyperweek/django-inlinetrans", "stars": 1, "license": "lgpl-3.0", "language": "python", "size": 110}
{"docstring": "\"\"\"\nSave the po file to file *fpath* if no file handle exists for\nthe object. If there's already an open file and no fpath is\nprovided, then the existing file is rewritten with the modified\ndata.\n\n**Keyword arguments**:\n  - *fpath*: string, full or relative path to the file.\n  - *repr_method*: string, the method to use for output.\n\"\"\"\n", "func_signal": "def save(self, fpath=None, repr_method='__str__'):\n", "code": "if self.fpath is None and fpath is None:\n    raise IOError('You must provide a file path to save() method')\ncontents = getattr(self, repr_method)()\nif fpath is None:\n    fpath = self.fpath\nmode = 'w'\nif repr_method == 'to_binary':\n    mode += 'b'\nfhandle = open(fpath, mode)\nfhandle.write(contents)\nfhandle.close()", "path": "inlinetrans\\polib.py", "repo_name": "hyperweek/django-inlinetrans", "stars": 1, "license": "lgpl-3.0", "language": "python", "size": 110}
{"docstring": "\"\"\"\nConvenience function that parse the po/pot file *fpath* and return\na POFile instance.\n\n**Keyword arguments**:\n  - *fpath*: string, full or relative path to the po/pot file to parse\n  - *wrapwidth*: integer, the wrap width, only useful when -w option was\n    passed to xgettext (optional, default to 78)\n  - *autodetect_encoding*: boolean, if set to False the function will\n    not try to detect the po file encoding (optional, default to True)\n  - *encoding*: string, an encoding, only relevant if autodetect_encoding\n    is set to False\n\n**Example**:\n\n>>> import polib\n>>> po = polib.pofile('tests/test_utf8.po')\n>>> po #doctest: +ELLIPSIS\n<POFile instance at ...>\n>>> import os, tempfile\n>>> for fname in ['test_iso-8859-15.po', 'test_utf8.po']:\n...     orig_po = polib.pofile('tests/'+fname)\n...     tmpf = tempfile.NamedTemporaryFile().name\n...     orig_po.save(tmpf)\n...     try:\n...         new_po = polib.pofile(tmpf)\n...         for old, new in zip(orig_po, new_po):\n...             if old.msgid != new.msgid:\n...                 old.msgid\n...                 new.msgid\n...             if old.msgstr != new.msgstr:\n...                 old.msgid\n...                 new.msgid\n...     finally:\n...         os.unlink(tmpf)\n\"\"\"\n# pofile {{{\n", "func_signal": "def pofile(fpath, **kwargs):\n", "code": "if _dictget(kwargs, 'autodetect_encoding', True) == True:\n    enc = detect_encoding(fpath)\nelse:\n    enc = _dictget(kwargs, 'encoding', default_encoding)\nparser = _POFileParser(fpath)\ninstance = parser.parse()\ninstance.wrapwidth = _dictget(kwargs, 'wrapwidth', 78)\ninstance.encoding  = enc\nreturn instance\n# }}}", "path": "inlinetrans\\polib.py", "repo_name": "hyperweek/django-inlinetrans", "stars": 1, "license": "lgpl-3.0", "language": "python", "size": 110}
{"docstring": "\"\"\"\nQuote and return the given string *st*.\n\n**Examples**:\n\n>>> quote('\\\\t and \\\\n and \\\\r and \" and \\\\\\\\')\n'\\\\\\\\t and \\\\\\\\n and \\\\\\\\r and \\\\\\\\\" and \\\\\\\\\\\\\\\\'\n\"\"\"\n# quote {{{\n", "func_signal": "def quote(st):\n", "code": "st = _strreplace(st, '\\\\', r'\\\\')\nst = _strreplace(st, '\\t', r'\\t')\nst = _strreplace(st, '\\r', r'\\r')\nst = _strreplace(st, '\\n', r'\\n')\nst = _strreplace(st, '\\\"', r'\\\"')\nreturn st\n# }}}", "path": "inlinetrans\\polib.py", "repo_name": "hyperweek/django-inlinetrans", "stars": 1, "license": "lgpl-3.0", "language": "python", "size": 110}
{"docstring": "\"\"\"Handle a msgstr.\"\"\"\n", "func_signal": "def handle_ms(self):\n", "code": "self.current_entry.msgstr = unquote(self.current_token[8:-1])\nreturn True", "path": "inlinetrans\\polib.py", "repo_name": "hyperweek/django-inlinetrans", "stars": 1, "license": "lgpl-3.0", "language": "python", "size": 110}
{"docstring": "\"\"\"\nMOFile constructor.\nSee _BaseFile.__construct.\n\"\"\"\n", "func_signal": "def __init__(self, fpath=None, wrapwidth=78):\n", "code": "_BaseFile.__init__(self, fpath, wrapwidth)\nself.magic_number = None\nself.version = 0", "path": "inlinetrans\\polib.py", "repo_name": "hyperweek/django-inlinetrans", "stars": 1, "license": "lgpl-3.0", "language": "python", "size": 110}
{"docstring": "\"\"\"Base Entry constructor.\"\"\"\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "self.msgid = _dictget(kwargs, 'msgid', '')\nself.msgstr = _dictget(kwargs, 'msgstr', '')\nself.msgid_plural = _dictget(kwargs, 'msgid_plural', '')\nself.msgstr_plural = _dictget(kwargs, 'msgstr_plural', {})\nself.obsolete = _dictget(kwargs, 'obsolete', False)\nself.encoding = _dictget(kwargs, 'encoding', default_encoding)", "path": "inlinetrans\\polib.py", "repo_name": "hyperweek/django-inlinetrans", "stars": 1, "license": "lgpl-3.0", "language": "python", "size": 110}
{"docstring": "\"\"\"\nParse the magic number and raise an exception if not valid.\n\"\"\"\n", "func_signal": "def parse_magicnumber(self):\n", "code": "magic_number = self._readbinary(fmt='4s')\n# magic number must be 0xde120495 or 0x950412de\nif magic_number not in ['\\xde\\x12\\x04\\x95', '\\x95\\x04\\x12\\xde']:\n    raise IOError('Invalid mo file, magic number is incorrect !')\nself.instance.magic_number = magic_number", "path": "inlinetrans\\polib.py", "repo_name": "hyperweek/django-inlinetrans", "stars": 1, "license": "lgpl-3.0", "language": "python", "size": 110}
{"docstring": "\"\"\"\nConvenience method that return the metadata ordered. The return\nvalue is list of tuples (metadata name, metadata_value).\n\"\"\"\n# copy the dict first\n", "func_signal": "def ordered_metadata(self):\n", "code": "metadata = self.metadata.copy()\ndata_order = [\n    'Project-Id-Version',\n    'Report-Msgid-Bugs-To',\n    'POT-Creation-Date',\n    'PO-Revision-Date',\n    'Last-Translator',\n    'Language-Team',\n    'MIME-Version',\n    'Content-Type',\n    'Content-Transfer-Encoding'\n]\nordered_data = []\nfor data in data_order:\n    try:\n        value = metadata.pop(data)\n        _listappend(ordered_data, (data, value))\n    except KeyError:\n        pass\n# the rest of the metadata won't be ordered there are no specs for this\nkeys = metadata.keys()\nkeys.sort()\nfor data in keys:\n    value = metadata[data]\n    _listappend(ordered_data, (data, value))\nreturn ordered_data", "path": "inlinetrans\\polib.py", "repo_name": "hyperweek/django-inlinetrans", "stars": 1, "license": "lgpl-3.0", "language": "python", "size": 110}
{"docstring": "\"\"\"Handle a msgstr plural.\"\"\"\n", "func_signal": "def handle_mx(self):\n", "code": "index, value = self.current_token[7], self.current_token[11:-1]\nself.current_entry.msgstr_plural[index] = unquote(value)\nself.msgstr_index = index\nreturn True", "path": "inlinetrans\\polib.py", "repo_name": "hyperweek/django-inlinetrans", "stars": 1, "license": "lgpl-3.0", "language": "python", "size": 110}
{"docstring": "\"\"\"Return the Python representation of ``s`` (a ``str`` or ``unicode``\ninstance containing a JSON document)\n\n\"\"\"\n", "func_signal": "def decode(self, s, _w=WHITESPACE.match):\n", "code": "obj, end = self.raw_decode(s, idx=_w(s, 0).end())\nend = _w(s, end).end()\nif end != len(s):\n    raise JSONDecodeError(\"Extra data\", s, end, len(s))\nreturn obj", "path": "waveapi\\simplejson\\decoder.py", "repo_name": "cameronneylon/ChemGadget", "stars": 1, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\"Return a JSON representation of a Python string\n\n\"\"\"\n", "func_signal": "def encode_basestring(s):\n", "code": "if isinstance(s, str) and HAS_UTF8.search(s) is not None:\n    s = s.decode('utf-8')\ndef replace(match):\n    return ESCAPE_DCT[match.group(0)]\nreturn u'\"' + ESCAPE.sub(replace, s) + u'\"'", "path": "waveapi\\simplejson\\encoder.py", "repo_name": "cameronneylon/ChemGadget", "stars": 1, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\"Inits this event with JSON data.\n\nArgs:\n  json: JSON data from Wave server.\n\"\"\"\n", "func_signal": "def __init__(self, json):\n", "code": "self.modifiedBy = json.get('modifiedBy')\nself.properties = json.get('properties', {})\nself.timestamp = json.get('timestamp', 0)\nself.type = json.get('type')\nself.raw_data = json", "path": "waveapi\\model.py", "repo_name": "cameronneylon/ChemGadget", "stars": 1, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\"Initializes the range with a start and end position.\n\nArgs:\n  start: Start index of the range.\n  end: End index of the range.\n\nRaises:\n  ValueError: Value error if the range is invalid (less than zero).\n\"\"\"\n", "func_signal": "def __init__(self, start=0, end=1):\n", "code": "self.start = start\nself.end = end\nif self.end - self.start < 0:\n  raise ValueError('Range cannot be less than 0')", "path": "waveapi\\document.py", "repo_name": "cameronneylon/ChemGadget", "stars": 1, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\"Return the (first) gadget that has the specified url.\n\nIf no matching gadget can be found, return None. If url\nis None, return the first gadget that can be found.\n\"\"\"\n", "func_signal": "def GetGadgetByUrl(self, url):\n", "code": "for el in self.elements.values():\n  if (el.type == document.ELEMENT_TYPE.GADGET\n      and getattr(el, 'url', None) == url):\n    return el\nreturn None", "path": "waveapi\\model.py", "repo_name": "cameronneylon/ChemGadget", "stars": 1, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\"Returns the root wavelet or None if it is not in this context.\"\"\"\n", "func_signal": "def GetRootWavelet(self):\n", "code": "for wavelet_id, wavelet in self.wavelets.items():\n  if wavelet_id.endswith(ROOT_WAVELET_ID_SUFFIX):\n    return wavelet\nlogging.warning('Could not retrieve root wavelet.')\nreturn None", "path": "waveapi\\model.py", "repo_name": "cameronneylon/ChemGadget", "stars": 1, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\"Submits the passed delta to the gadget.\n\nThis does not send the delta to the server, but only modifies the\nlocal state. The send the delto the server, go through the\ndocument.GadgetSubmitDelta interface.\n\"\"\"\n", "func_signal": "def SubmitDelta(self, delta):\n", "code": "for k, v in delta.items():\n  setattr(self, k, v)", "path": "waveapi\\document.py", "repo_name": "cameronneylon/ChemGadget", "stars": 1, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\"Return an ASCII-only JSON representation of a Python string\n\n\"\"\"\n", "func_signal": "def py_encode_basestring_ascii(s):\n", "code": "if isinstance(s, str) and HAS_UTF8.search(s) is not None:\n    s = s.decode('utf-8')\ndef replace(match):\n    s = match.group(0)\n    try:\n        return ESCAPE_DCT[s]\n    except KeyError:\n        n = ord(s)\n        if n < 0x10000:\n            #return '\\\\u{0:04x}'.format(n)\n            return '\\\\u%04x' % (n,)\n        else:\n            # surrogate pair\n            n -= 0x10000\n            s1 = 0xd800 | ((n >> 10) & 0x3ff)\n            s2 = 0xdc00 | (n & 0x3ff)\n            #return '\\\\u{0:04x}\\\\u{1:04x}'.format(s1, s2)\n            return '\\\\u%04x\\\\u%04x' % (s1, s2)\nreturn '\"' + str(ESCAPE_ASCII.sub(replace, s)) + '\"'", "path": "waveapi\\simplejson\\encoder.py", "repo_name": "cameronneylon/ChemGadget", "stars": 1, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\"Decode a JSON document from ``s`` (a ``str`` or ``unicode``\nbeginning with a JSON document) and return a 2-tuple of the Python\nrepresentation and the index in ``s`` where the document ended.\n\nThis can be used to decode a JSON document from a string that may\nhave extraneous data at the end.\n\n\"\"\"\n", "func_signal": "def raw_decode(self, s, idx=0):\n", "code": "try:\n    obj, end = self.scan_once(s, idx)\nexcept StopIteration:\n    raise JSONDecodeError(\"No JSON object could be decoded\", s, idx)\nreturn obj, end", "path": "waveapi\\simplejson\\decoder.py", "repo_name": "cameronneylon/ChemGadget", "stars": 1, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\"Custom serializer for Elements.\n\nElement need their non standard attributes returned in a dict named\nproperties.\n\"\"\"\n", "func_signal": "def Serialize(self):\n", "code": "props = {}\ndata = {}\nfor attr in dir(self):\n  if attr.startswith('_'):\n    continue\n  val = getattr(self, attr)\n  if val is None or callable(val):\n    continue\n  val = util.Serialize(val)\n  if attr == 'type' or attr == 'java_class':\n    data[attr] = val\n  else:\n    props[attr] = val\ndata['properties'] = util.Serialize(props)\nreturn data", "path": "waveapi\\document.py", "repo_name": "cameronneylon/ChemGadget", "stars": 1, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\"Return a JSON string representation of a Python data structure.\n\n>>> from simplejson import JSONEncoder\n>>> JSONEncoder().encode({\"foo\": [\"bar\", \"baz\"]})\n'{\"foo\": [\"bar\", \"baz\"]}'\n\n\"\"\"\n# This is for extremely simple cases and benchmarks.\n", "func_signal": "def encode(self, o):\n", "code": "if isinstance(o, basestring):\n    if isinstance(o, str):\n        _encoding = self.encoding\n        if (_encoding is not None\n                and not (_encoding == 'utf-8')):\n            o = o.decode(_encoding)\n    if self.ensure_ascii:\n        return encode_basestring_ascii(o)\n    else:\n        return encode_basestring(o)\n# This doesn't pass the iterator directly to ''.join() because the\n# exceptions aren't as detailed.  The list call should be roughly\n# equivalent to the PySequence_Fast that ''.join() would do.\nchunks = self.iterencode(o, _one_shot=True)\nif not isinstance(chunks, (list, tuple)):\n    chunks = list(chunks)\nif self.ensure_ascii:\n    return ''.join(chunks)\nelse:\n    return u''.join(chunks)", "path": "waveapi\\simplejson\\encoder.py", "repo_name": "cameronneylon/ChemGadget", "stars": 1, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\"Encode the given object and yield each string\nrepresentation as available.\n\nFor example::\n\n    for chunk in JSONEncoder().iterencode(bigobject):\n        mysocket.write(chunk)\n\n\"\"\"\n", "func_signal": "def iterencode(self, o, _one_shot=False):\n", "code": "if self.check_circular:\n    markers = {}\nelse:\n    markers = None\nif self.ensure_ascii:\n    _encoder = encode_basestring_ascii\nelse:\n    _encoder = encode_basestring\nif self.encoding != 'utf-8':\n    def _encoder(o, _orig_encoder=_encoder, _encoding=self.encoding):\n        if isinstance(o, str):\n            o = o.decode(_encoding)\n        return _orig_encoder(o)\n\ndef floatstr(o, allow_nan=self.allow_nan,\n        _repr=FLOAT_REPR, _inf=PosInf, _neginf=-PosInf):\n    # Check for specials. Note that this type of test is processor\n    # and/or platform-specific, so do tests which don't depend on\n    # the internals.\n\n    if o != o:\n        text = 'NaN'\n    elif o == _inf:\n        text = 'Infinity'\n    elif o == _neginf:\n        text = '-Infinity'\n    else:\n        return _repr(o)\n\n    if not allow_nan:\n        raise ValueError(\n            \"Out of range float values are not JSON compliant: \" +\n            repr(o))\n\n    return text\n\n\nif (_one_shot and c_make_encoder is not None\n        and not self.indent and not self.sort_keys):\n    _iterencode = c_make_encoder(\n        markers, self.default, _encoder, self.indent,\n        self.key_separator, self.item_separator, self.sort_keys,\n        self.skipkeys, self.allow_nan)\nelse:\n    _iterencode = _make_iterencode(\n        markers, self.default, _encoder, self.indent, floatstr,\n        self.key_separator, self.item_separator, self.sort_keys,\n        self.skipkeys, _one_shot)\nreturn _iterencode(o, 0)", "path": "waveapi\\simplejson\\encoder.py", "repo_name": "cameronneylon/ChemGadget", "stars": 1, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\"Standard get interface for gadgets\"\"\"\n", "func_signal": "def get(self, key, default=None):\n", "code": "if hasattr(self, key):\n  return getattr(self, key)\nelse:\n  return default", "path": "waveapi\\document.py", "repo_name": "cameronneylon/ChemGadget", "stars": 1, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\"Construct one of the type of elements given a json object.\"\"\"\n", "func_signal": "def ElementFromJson(json):\n", "code": "etype = json['type']\nlogging.info('constructing: ' + str(json))\nprops = json['properties'].copy()\n\nif etype == ELEMENT_TYPE.GADGET:\n  url = props['url']\n  del props['url']\n  return Gadget(url=url, props=props)\nelif etype == ELEMENT_TYPE.IMAGE:\n  return Image(url=props.get('url', ''),\n               width=props.get('width'),\n               height=props.get('height'),\n               attachment_id=props.get('attachmentId'),\n               caption=props.get('caption'))\n\nreturn FormElement(element_type=etype,\n                   name=props.get('name', ''),\n                   value=props.get('value', ''),\n                   default_value=props.get('defaultValue', ''),\n                   label=props.get('label', ''))", "path": "waveapi\\document.py", "repo_name": "cameronneylon/ChemGadget", "stars": 1, "license": "None", "language": "python", "size": 140}
{"docstring": "# Note that this function is called from _speedups\n", "func_signal": "def errmsg(msg, doc, pos, end=None):\n", "code": "lineno, colno = linecol(doc, pos)\nif end is None:\n    #fmt = '{0}: line {1} column {2} (char {3})'\n    #return fmt.format(msg, lineno, colno, pos)\n    fmt = '%s: line %d column %d (char %d)'\n    return fmt % (msg, lineno, colno, pos)\nendlineno, endcolno = linecol(doc, end)\n#fmt = '{0}: line {1} column {2} - line {3} column {4} (char {5} - {6})'\n#return fmt.format(msg, lineno, colno, endlineno, endcolno, pos, end)\nfmt = '%s: line %d column %d - line %d column %d (char %d - %d)'\nreturn fmt % (msg, lineno, colno, endlineno, endcolno, pos, end)", "path": "waveapi\\simplejson\\decoder.py", "repo_name": "cameronneylon/ChemGadget", "stars": 1, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\"Initializes this annotation with a name and value pair and a range.\n\nArgs:\n  name: Key name for this annotation.\n  value: Value of this annotation.\n  r: Range that this annotation is valid over.\n\"\"\"\n", "func_signal": "def __init__(self, name, value, r=None):\n", "code": "self.name = name\nself.value = value\nself.range = r or Range()", "path": "waveapi\\document.py", "repo_name": "cameronneylon/ChemGadget", "stars": 1, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\"Returns a data document for this wavelet based on key name.\"\"\"\n", "func_signal": "def GetDataDocument(self, name, default=None):\n", "code": "if self.dataDocuments:\n  return self.dataDocuments.get(name, default)\nreturn default", "path": "waveapi\\model.py", "repo_name": "cameronneylon/ChemGadget", "stars": 1, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\"Inits this wavelet with JSON data.\n\nArgs:\n  json: JSON data dictionary from Wave server.\n\"\"\"\n", "func_signal": "def __init__(self, json):\n", "code": "self.creator = json.get('creator')\nself.creationTime = json.get('creationTime', 0)\nself.dataDocuments = json.get('dataDocuments', {})\nself.lastModifiedTime = json.get('lastModifiedTime')\nself.participants = set(json.get('participants', []))\nself.rootBlipId = json.get('rootBlipId')\nself.title = json.get('title', '')\nself.waveId = json.get('waveId')\nself.waveletId = json.get('waveletId')\nself.raw_data = json", "path": "waveapi\\model.py", "repo_name": "cameronneylon/ChemGadget", "stars": 1, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\"Invoked when the robot has been added.\"\"\"\n", "func_signal": "def OnRobotAdded(properties, context):\n", "code": "root_wavelet = context.GetRootWavelet()\nroot_wavelet.CreateBlip().GetDocument().SetText(\"Hello, I'm ChemGadget, I will insert a 3d chemical gadget where you have placed the text  chem[chemicalName;image].  This is Version 0.1 of ChemGadget\")", "path": "ChemGadget.py", "repo_name": "cameronneylon/ChemGadget", "stars": 1, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\"Initializes self with the specified type and any properties.\n\nArgs:\n  element_type: string typed member of ELEMENT_TYPE\n  properties: either a dictionary of initial properties, or a dictionary\n      with just one member properties that is itself a dictionary of\n      properties. This allows us to both use\n      e = Element(atype, prop1=val1, prop2=prop2...)\n      and\n      e = Element(atype, properties={prop1:val1, prop2:prop2..})\n\"\"\"\n", "func_signal": "def __init__(self, element_type, **properties):\n", "code": "if len(properties) == 1 and 'properties' in properties:\n  properties = properties['properties']\nself.type = element_type\nfor key, val in properties.items():\n  setattr(self, key, val)", "path": "waveapi\\document.py", "repo_name": "cameronneylon/ChemGadget", "stars": 1, "license": "None", "language": "python", "size": 140}
{"docstring": "# remove all conflicting options\n", "func_signal": "def _handle_conflict_resolve(self, action, conflicting_actions):\n", "code": "        for option_string, action in conflicting_actions:\n    # remove the conflicting option\n            action.option_strings.remove(option_string)\n            self._option_string_actions.pop(option_string, None)\n    # if the option now has no option string, remove it from the\n            # container holding it\n            if not action.option_strings:\n                action.container._remove_action(action)", "path": "alien\\argparse.py", "repo_name": "yaniv-aknin/yalib", "stars": 1, "license": "None", "language": "python", "size": 96}
{"docstring": "# determine the required width and the entry label\n", "func_signal": "def _format_action(self, action):\n", "code": "help_position = min(self._action_max_length + 2,\n                    self._max_help_position)\nhelp_width = self._width - help_position\naction_width = help_position - self._current_indent - 2\naction_header = self._format_action_invocation(action)\n\n# ho nelp; start on same line and add a final newline\nif not action.help:\n    tup = self._current_indent, '', action_header\n    action_header = '%*s%s\\n' % tup\n\n# short action name; start on the same line and pad two spaces\nelif len(action_header) <= action_width:\n    tup = self._current_indent, '', action_width, action_header\n    action_header = '%*s%-*s  ' % tup\n    indent_first = 0\n\n# long action name; start on the next line\nelse:\n    tup = self._current_indent, '', action_header\n    action_header = '%*s%s\\n' % tup\n    indent_first = help_position\n\n# collect the pieces of the action help\nparts = [action_header]\n\n# if there was help for the action, add lines of help text\nif action.help:\n    help_text = self._expand_help(action)\n    help_lines = self._split_lines(help_text, help_width)\n    parts.append('%*s%s\\n' % (indent_first, '', help_lines[0]))\n    for line in help_lines[1:]:\n        parts.append('%*s%s\\n' % (help_position, '', line))\n\n# or add a newline if the description doesn't end with one\nelif not action_header.endswith('\\n'):\n    parts.append('\\n')\n\n# if there are any sub-actions, add their help as well\nfor subaction in self._iter_indented_subactions(action):\n    parts.append(self._format_action(subaction))\n\n# return a single string\nreturn self._join_parts(parts)", "path": "alien\\argparse.py", "repo_name": "yaniv-aknin/yalib", "stars": 1, "license": "None", "language": "python", "size": 96}
{"docstring": "# make sure required is not specified\n", "func_signal": "def _get_positional_kwargs(self, dest, **kwargs):\n", "code": "if 'required' in kwargs:\n    msg = _(\"'required' is an invalid argument for positionals\")\n    raise TypeError(msg)\n\n# mark positional arguments as required if at least one is\n# always required\nif kwargs.get('nargs') not in [OPTIONAL, ZERO_OR_MORE]:\n    kwargs['required'] = True\nif kwargs.get('nargs') == ZERO_OR_MORE and 'default' not in kwargs:\n    kwargs['required'] = True\n\n# return the keyword arguments with no option strings\nreturn dict(kwargs, dest=dest, option_strings=[])", "path": "alien\\argparse.py", "repo_name": "yaniv-aknin/yalib", "stars": 1, "license": "None", "language": "python", "size": 96}
{"docstring": "\"\"\"\nadd_argument(dest, ..., name=value, ...)\nadd_argument(option_string, option_string, ..., name=value, ...)\n\"\"\"\n\n# if no positional args are supplied or only one is supplied and\n# it doesn't look like an option string, parse a positional\n# argument\n", "func_signal": "def add_argument(self, *args, **kwargs):\n", "code": "chars = self.prefix_chars\nif not args or len(args) == 1 and args[0][0] not in chars:\n    kwargs = self._get_positional_kwargs(*args, **kwargs)\n\n# otherwise, we're adding an optional argument\nelse:\n    kwargs = self._get_optional_kwargs(*args, **kwargs)\n\n# if no default was supplied, use the parser-level default\nif 'default' not in kwargs:\n    dest = kwargs['dest']\n    if dest in self._defaults:\n        kwargs['default'] = self._defaults[dest]\n    elif self.argument_default is not None:\n        kwargs['default'] = self.argument_default\n\n# create the action object, and add it to the parser\naction_class = self._pop_action_class(kwargs)\naction = action_class(**kwargs)\nreturn self._add_action(action)", "path": "alien\\argparse.py", "repo_name": "yaniv-aknin/yalib", "stars": 1, "license": "None", "language": "python", "size": 96}
{"docstring": "# find group indices and identify actions in groups\n", "func_signal": "def _format_actions_usage(self, actions, groups):\n", "code": "group_actions = set()\ninserts = {}\nfor group in groups:\n    start = actions.index(group._group_actions[0])\n    if start != -1:\n        end = start + len(group._group_actions)\n        if actions[start:end] == group._group_actions:\n            for action in group._group_actions:\n                group_actions.add(action)\n            if not group.required:\n                inserts[start] = '['\n                inserts[end] = ']'\n            else:\n                inserts[start] = '('\n                inserts[end] = ')'\n            for i in xrange(start + 1, end):\n                inserts[i] = '|'\n\n# collect all actions format strings\nparts = []\nfor i, action in enumerate(actions):\n\n    # suppressed arguments are marked with None\n    # remove | separators for suppressed arguments\n    if action.help is SUPPRESS:\n        parts.append(None)\n        if inserts.get(i) == '|':\n            inserts.pop(i)\n        elif inserts.get(i + 1) == '|':\n            inserts.pop(i + 1)\n\n    # produce all arg strings\n    elif not action.option_strings:\n        part = self._format_args(action, action.dest)\n\n        # if it's in a group, strip the outer []\n        if action in group_actions:\n            if part[0] == '[' and part[-1] == ']':\n                part = part[1:-1]\n\n        # add the action string to the list\n        parts.append(part)\n\n    # produce the first way to invoke the option in brackets\n    else:\n        option_string = action.option_strings[0]\n\n        # if the Optional doesn't take a value, format is:\n        #    -s or --long\n        if action.nargs == 0:\n            part = '%s' % option_string\n\n        # if the Optional takes a value, format is:\n        #    -s ARGS or --long ARGS\n        else:\n            default = action.dest.upper()\n            args_string = self._format_args(action, default)\n            part = '%s %s' % (option_string, args_string)\n\n        # make it look optional if it's not required or in a group\n        if not action.required and action not in group_actions:\n            part = '[%s]' % part\n\n        # add the action string to the list\n        parts.append(part)\n\n# insert things at the necessary indices\nfor i in sorted(inserts, reverse=True):\n    parts[i:i] = [inserts[i]]\n\n# join all the action items with spaces\ntext = ' '.join(item for item in parts if item is not None)\n\n# clean up separators for mutually exclusive groups\nopen = r'[\\[(]'\nclose = r'[\\])]'\ntext = _re.sub(r'(%s) ' % open, r'\\1', text)\ntext = _re.sub(r' (%s)' % close, r'\\1', text)\ntext = _re.sub(r'%s *%s' % (open, close), r'', text)\ntext = _re.sub(r'\\(([^|]*)\\)', r'\\1', text)\ntext = text.strip()\n\n# return the text\nreturn text", "path": "alien\\argparse.py", "repo_name": "yaniv-aknin/yalib", "stars": 1, "license": "None", "language": "python", "size": 96}
{"docstring": "# collect groups by titles\n", "func_signal": "def _add_container_actions(self, container):\n", "code": "title_group_map = {}\nfor group in self._action_groups:\n    if group.title in title_group_map:\n        msg = _('cannot merge actions - two groups are named %r')\n        raise ValueError(msg % (group.title))\n    title_group_map[group.title] = group\n\n# map each action to its group\ngroup_map = {}\nfor group in container._action_groups:\n\n    # if a group with the title exists, use that, otherwise\n    # create a new group matching the container's group\n    if group.title not in title_group_map:\n        title_group_map[group.title] = self.add_argument_group(\n            title=group.title,\n            description=group.description,\n            conflict_handler=group.conflict_handler)\n\n    # map the actions to their new group\n    for action in group._group_actions:\n        group_map[action] = title_group_map[group.title]\n\n# add all actions to this container or their group\nfor action in container._actions:\n    group_map.get(action, self)._add_action(action)", "path": "alien\\argparse.py", "repo_name": "yaniv-aknin/yalib", "stars": 1, "license": "None", "language": "python", "size": 96}
{"docstring": "# if it doesn't start with a prefix, it was meant to be positional\n", "func_signal": "def _parse_optional(self, arg_string):\n", "code": "if not arg_string[0] in self.prefix_chars:\n    return None\n\n# if it's just dashes, it was meant to be positional\nif not arg_string.strip('-'):\n    return None\n\n# if the option string is present in the parser, return the action\nif arg_string in self._option_string_actions:\n    action = self._option_string_actions[arg_string]\n    return action, arg_string, None\n\n# search through all possible prefixes of the option string\n# and all actions in the parser for possible interpretations\noption_tuples = self._get_option_tuples(arg_string)\n\n# if multiple actions match, the option string was ambiguous\nif len(option_tuples) > 1:\n    options = ', '.join(opt_str for _, opt_str, _ in option_tuples)\n    tup = arg_string, options\n    self.error(_('ambiguous option: %s could match %s') % tup)\n\n# if exactly one action matched, this segmentation is good,\n# so return the parsed action\nelif len(option_tuples) == 1:\n    option_tuple, = option_tuples\n    return option_tuple\n\n# if it was not found as an option, but it looks like a negative\n# number, it was meant to be positional\n# unless there are negative-number-like options\nif self._negative_number_matcher.match(arg_string):\n    if not self._has_negative_number_optionals:\n        return None\n\n# it was meant to be an optional but there is no such option\n# in this parser (though it might be a valid option in a subparser)\nreturn None, arg_string, None", "path": "alien\\argparse.py", "repo_name": "yaniv-aknin/yalib", "stars": 1, "license": "None", "language": "python", "size": 96}
{"docstring": "# resolve any conflicts\n", "func_signal": "def _add_action(self, action):\n", "code": "self._check_conflict(action)\n\n# add to actions list\nself._actions.append(action)\naction.container = self\n\n# index the action by any option strings it has\nfor option_string in action.option_strings:\n    self._option_string_actions[option_string] = action\n\n# set the flag if any option strings look like negative numbers\nfor option_string in action.option_strings:\n    if self._negative_number_matcher.match(option_string):\n        if not self._has_negative_number_optionals:\n            self._has_negative_number_optionals.append(True)\n\n# return the created action\nreturn action", "path": "alien\\argparse.py", "repo_name": "yaniv-aknin/yalib", "stars": 1, "license": "None", "language": "python", "size": 96}
{"docstring": "# set prog from the existing prefix\n", "func_signal": "def add_parser(self, name, **kwargs):\n", "code": "if kwargs.get('prog') is None:\n    kwargs['prog'] = '%s %s' % (self._prog_prefix, name)\n\n# create a pseudo-action to hold the choice help\nif 'help' in kwargs:\n    help = kwargs.pop('help')\n    choice_action = self._ChoicesPseudoAction(name, help)\n    self._choices_actions.append(choice_action)\n\n# create the parser and add it to the map\nparser = self._parser_class(**kwargs)\nself._name_parser_map[name] = parser\nreturn parser", "path": "alien\\argparse.py", "repo_name": "yaniv-aknin/yalib", "stars": 1, "license": "None", "language": "python", "size": 96}
{"docstring": "# the special argument \"-\" means sys.std{in,out}\n", "func_signal": "def __call__(self, string):\n", "code": "if string == '-':\n    if 'r' in self._mode:\n        return _sys.stdin\n    elif 'w' in self._mode:\n        return _sys.stdout\n    else:\n        msg = _('argument \"-\" with mode %r' % self._mode)\n        raise ValueError(msg)\n\n# all other arguments are used as file names\nif self._bufsize:\n    return open(string, self._mode, self._bufsize)\nelse:\n    return open(string, self._mode)", "path": "alien\\argparse.py", "repo_name": "yaniv-aknin/yalib", "stars": 1, "license": "None", "language": "python", "size": 96}
{"docstring": "# in all examples below, we have to allow for '--' args\n# which are represented as '-' in the pattern\n", "func_signal": "def _get_nargs_pattern(self, action):\n", "code": "nargs = action.nargs\n\n# the default (None) is assumed to be a single argument\nif nargs is None:\n    nargs_pattern = '(-*A-*)'\n\n# allow zero or one arguments\nelif nargs == OPTIONAL:\n    nargs_pattern = '(-*A?-*)'\n\n# allow zero or more arguments\nelif nargs == ZERO_OR_MORE:\n    nargs_pattern = '(-*[A-]*)'\n\n# allow one or more arguments\nelif nargs == ONE_OR_MORE:\n    nargs_pattern = '(-*A[A-]*)'\n\n# allow one argument followed by any number of options or arguments\nelif nargs is PARSER:\n    nargs_pattern = '(-*A[-AO]*)'\n\n# all others should be integers\nelse:\n    nargs_pattern = '(-*%s-*)' % '-*'.join('A' * nargs)\n\n# if this is an optional action, -- is not allowed\nif action.option_strings:\n    nargs_pattern = nargs_pattern.replace('-*', '')\n    nargs_pattern = nargs_pattern.replace('-', '')\n\n# return the pattern\nreturn nargs_pattern", "path": "alien\\argparse.py", "repo_name": "yaniv-aknin/yalib", "stars": 1, "license": "None", "language": "python", "size": 96}
{"docstring": "# for everything but PARSER args, strip out '--'\n", "func_signal": "def _get_values(self, action, arg_strings):\n", "code": "if action.nargs is not PARSER:\n    arg_strings = [s for s in arg_strings if s != '--']\n\n# optional argument produces a default when not present\nif not arg_strings and action.nargs == OPTIONAL:\n    if action.option_strings:\n        value = action.const\n    else:\n        value = action.default\n    if isinstance(value, basestring):\n        value = self._get_value(action, value)\n        self._check_value(action, value)\n\n# when nargs='*' on a positional, if there were no command-line\n# args, use the default if it is anything other than None\nelif (not arg_strings and action.nargs == ZERO_OR_MORE and\n      not action.option_strings):\n    if action.default is not None:\n        value = action.default\n    else:\n        value = arg_strings\n    self._check_value(action, value)\n\n# single argument or optional argument produces a single value\nelif len(arg_strings) == 1 and action.nargs in [None, OPTIONAL]:\n    arg_string, = arg_strings\n    value = self._get_value(action, arg_string)\n    self._check_value(action, value)\n\n# PARSER arguments convert all values, but check only the first\nelif action.nargs is PARSER:\n    value = list(self._get_value(action, v) for v in arg_strings)\n    self._check_value(action, value[0])\n\n# all other types of nargs produce a list\nelse:\n    value = list(self._get_value(action, v) for v in arg_strings)\n    for v in value:\n        self._check_value(action, v)\n\n# return the converted value\nreturn value", "path": "alien\\argparse.py", "repo_name": "yaniv-aknin/yalib", "stars": 1, "license": "None", "language": "python", "size": 96}
{"docstring": "# format the indented section\n", "func_signal": "def format_help(self):\n", "code": "if self.parent is not None:\n    self.formatter._indent()\njoin = self.formatter._join_parts\nfor func, args in self.items:\n    func(*args)\nitem_help = join(func(*args) for func, args in self.items)\nif self.parent is not None:\n    self.formatter._dedent()\n\n# return nothing if the section was empty\nif not item_help:\n    return ''\n\n# add the heading if the section was non-empty\nif self.heading is not SUPPRESS and self.heading is not None:\n    current_indent = self.formatter._current_indent\n    heading = '%*s%s:\\n' % (current_indent, '', self.heading)\nelse:\n    heading = ''\n\n# join the section-initial newline, the heading and the help\nreturn join(['\\n', heading, item_help, '\\n'])", "path": "alien\\argparse.py", "repo_name": "yaniv-aknin/yalib", "stars": 1, "license": "None", "language": "python", "size": 96}
{"docstring": "# find all options that conflict with this option\n", "func_signal": "def _check_conflict(self, action):\n", "code": "        confl_optionals = []\n        for option_string in action.option_strings:\n            if option_string in self._option_string_actions:\n                confl_optional = self._option_string_actions[option_string]\n                confl_optionals.append((option_string, confl_optional))\n# resolve any conflicts\n        if confl_optionals:\n            conflict_handler = self._get_handler()\n            conflict_handler(action, confl_optionals)", "path": "alien\\argparse.py", "repo_name": "yaniv-aknin/yalib", "stars": 1, "license": "None", "language": "python", "size": 96}
{"docstring": "# map all mutually exclusive arguments to the other arguments\n# they can't occur with\n", "func_signal": "def _parse_args(self, arg_strings, namespace):\n", "code": "action_conflicts = {}\nfor mutex_group in self._mutually_exclusive_groups:\n    group_actions = mutex_group._group_actions\n    for i, mutex_action in enumerate(mutex_group._group_actions):\n        conflicts = action_conflicts.setdefault(mutex_action, [])\n        conflicts.extend(group_actions[:i])\n        conflicts.extend(group_actions[i + 1:])\n\n# find all option indices, and determine the arg_string_pattern\n# which has an 'O' if there is an option at an index,\n# an 'A' if there is an argument, or a '-' if there is a '--'\noption_string_indices = {}\narg_string_pattern_parts = []\narg_strings_iter = iter(arg_strings)\nfor i, arg_string in enumerate(arg_strings_iter):\n\n    # all args after -- are non-options\n    if arg_string == '--':\n        arg_string_pattern_parts.append('-')\n        for arg_string in arg_strings_iter:\n            arg_string_pattern_parts.append('A')\n\n    # otherwise, add the arg to the arg strings\n    # and note the index if it was an option\n    else:\n        option_tuple = self._parse_optional(arg_string)\n        if option_tuple is None:\n            pattern = 'A'\n        else:\n            option_string_indices[i] = option_tuple\n            pattern = 'O'\n        arg_string_pattern_parts.append(pattern)\n\n# join the pieces together to form the pattern\narg_strings_pattern = ''.join(arg_string_pattern_parts)\n\n# converts arg strings to the appropriate and then takes the action\nseen_actions = set()\nseen_non_default_actions = set()\ndef take_action(action, argument_strings, option_string=None):\n    seen_actions.add(action)\n    argument_values = self._get_values(action, argument_strings)\n\n    # error if this argument is not allowed with other previously\n    # seen arguments, assuming that actions that use the default\n    # value don't really count as \"present\"\n    if argument_values is not action.default:\n        seen_non_default_actions.add(action)\n        for conflict_action in action_conflicts.get(action, []):\n            if conflict_action in seen_non_default_actions:\n                msg = _('not allowed with argument %s')\n                action_name = _get_action_name(conflict_action)\n                raise ArgumentError(action, msg % action_name)\n\n    # take the action if we didn't receive a SUPPRESS value\n    # (e.g. from a default)\n    if argument_values is not SUPPRESS:\n        action(self, namespace, argument_values, option_string)\n\n# function to convert arg_strings into an optional action\ndef consume_optional(start_index):\n\n    # get the optional identified at this index\n    option_tuple = option_string_indices[start_index]\n    action, option_string, explicit_arg = option_tuple\n\n    # identify additional optionals in the same arg string\n    # (e.g. -xyz is the same as -x -y -z if no args are required)\n    match_argument = self._match_argument\n    action_tuples = []\n    while True:\n\n        # if we found no optional action, raise an error\n        if action is None:\n            self.error(_('no such option: %s') % option_string)\n\n        # if there is an explicit argument, try to match the\n        # optional's string arguments to only this\n        if explicit_arg is not None:\n            arg_count = match_argument(action, 'A')\n\n            # if the action is a single-dash option and takes no\n            # arguments, try to parse more single-dash options out\n            # of the tail of the option string\n            chars = self.prefix_chars\n            if arg_count == 0 and option_string[1] not in chars:\n                action_tuples.append((action, [], option_string))\n                for char in self.prefix_chars:\n                    option_string = char + explicit_arg[0]\n                    explicit_arg = explicit_arg[1:] or None\n                    optionals_map = self._option_string_actions\n                    if option_string in optionals_map:\n                        action = optionals_map[option_string]\n                        break\n                else:\n                    msg = _('ignored explicit argument %r')\n                    raise ArgumentError(action, msg % explicit_arg)\n\n            # if the action expect exactly one argument, we've\n            # successfully matched the option; exit the loop\n            elif arg_count == 1:\n                stop = start_index + 1\n                args = [explicit_arg]\n                action_tuples.append((action, args, option_string))\n                break\n\n            # error if a double-dash option did not use the\n            # explicit argument\n            else:\n                msg = _('ignored explicit argument %r')\n                raise ArgumentError(action, msg % explicit_arg)\n\n        # if there is no explicit argument, try to match the\n        # optional's string arguments with the following strings\n        # if successful, exit the loop\n        else:\n            start = start_index + 1\n            selected_patterns = arg_strings_pattern[start:]\n            arg_count = match_argument(action, selected_patterns)\n            stop = start + arg_count\n            args = arg_strings[start:stop]\n            action_tuples.append((action, args, option_string))\n            break\n\n    # add the Optional to the list and return the index at which\n    # the Optional's string args stopped\n    assert action_tuples\n    for action, args, option_string in action_tuples:\n        take_action(action, args, option_string)\n    return stop\n\n# the list of Positionals left to be parsed; this is modified\n# by consume_positionals()\npositionals = self._get_positional_actions()\n\n# function to convert arg_strings into positional actions\ndef consume_positionals(start_index):\n    # match as many Positionals as possible\n    match_partial = self._match_arguments_partial\n    selected_pattern = arg_strings_pattern[start_index:]\n    arg_counts = match_partial(positionals, selected_pattern)\n\n    # slice off the appropriate arg strings for each Positional\n    # and add the Positional and its args to the list\n    for action, arg_count in zip(positionals, arg_counts):\n        args = arg_strings[start_index: start_index + arg_count]\n        start_index += arg_count\n        take_action(action, args)\n\n    # slice off the Positionals that we just parsed and return the\n    # index at which the Positionals' string args stopped\n    positionals[:] = positionals[len(arg_counts):]\n    return start_index\n\n# consume Positionals and Optionals alternately, until we have\n# passed the last option string\nstart_index = 0\nif option_string_indices:\n    max_option_string_index = max(option_string_indices)\nelse:\n    max_option_string_index = -1\nwhile start_index <= max_option_string_index:\n\n    # consume any Positionals preceding the next option\n    next_option_string_index = min(\n        index\n        for index in option_string_indices\n        if index >= start_index)\n    if start_index != next_option_string_index:\n        positionals_end_index = consume_positionals(start_index)\n\n        # only try to parse the next optional if we didn't consume\n        # the option string during the positionals parsing\n        if positionals_end_index > start_index:\n            start_index = positionals_end_index\n            continue\n        else:\n            start_index = positionals_end_index\n\n    # if we consumed all the positionals we could and we're not\n    # at the index of an option string, there were unparseable\n    # arguments\n    if start_index not in option_string_indices:\n        msg = _('extra arguments found: %s')\n        extras = arg_strings[start_index:next_option_string_index]\n        self.error(msg % ' '.join(extras))\n\n    # consume the next optional and any arguments for it\n    start_index = consume_optional(start_index)\n\n# consume any positionals following the last Optional\nstop_index = consume_positionals(start_index)\n\n# if we didn't consume all the argument strings, there were too\n# many supplied\nif stop_index != len(arg_strings):\n    extras = arg_strings[stop_index:]\n    self.error(_('extra arguments found: %s') % ' '.join(extras))\n\n# if we didn't use all the Positional objects, there were too few\n# arg strings supplied.\nif positionals:\n    self.error(_('too few arguments'))\n\n# make sure all required actions were present\nfor action in self._actions:\n    if action.required:\n        if action not in seen_actions:\n            name = _get_action_name(action)\n            self.error(_('argument %s is required') % name)\n\n# make sure all required groups had one option present\nfor group in self._mutually_exclusive_groups:\n    if group.required:\n        for action in group._group_actions:\n            if action in seen_non_default_actions:\n                break\n\n        # if no actions were used, report the error\n        else:\n            names = [_get_action_name(action)\n                     for action in group._group_actions\n                     if action.help is not SUPPRESS]\n            msg = _('one of the arguments %s is required')\n            self.error(msg % ' '.join(names))\n\n# return the updated namespace\nreturn namespace", "path": "alien\\argparse.py", "repo_name": "yaniv-aknin/yalib", "stars": 1, "license": "None", "language": "python", "size": 96}
{"docstring": "# converted value must be one of the choices (if specified)\n", "func_signal": "def _check_value(self, action, value):\n", "code": "if action.choices is not None and value not in action.choices:\n    tup = value, ', '.join(map(repr, action.choices))\n    msg = _('invalid choice: %r (choose from %s)') % tup\n    raise ArgumentError(action, msg)", "path": "alien\\argparse.py", "repo_name": "yaniv-aknin/yalib", "stars": 1, "license": "None", "language": "python", "size": 96}
{"docstring": "\"\"\"\nAdd exception handling. I've read the CPython code of sendto and am not sure it can create an exception\n on a nonblocking datagram socket, but better safe than sorry.\n\"\"\"\n", "func_signal": "def send(self, s):\n", "code": "if self.sock is None:\n    self.createSocket()\ntry:\n    self.sock.sendto(s, (self.host, self.port))\nexcept socket.error:\n    self.sock.close()\n    # Note: so we can call createSocket next time\n    self.sock = None", "path": "logutils\\__init__.py", "repo_name": "yaniv-aknin/yalib", "stars": 1, "license": "None", "language": "python", "size": 96}
{"docstring": "# match the pattern for this action to the arg strings\n", "func_signal": "def _match_argument(self, action, arg_strings_pattern):\n", "code": "nargs_pattern = self._get_nargs_pattern(action)\nmatch = _re.match(nargs_pattern, arg_strings_pattern)\n\n# raise an exception if we weren't able to find a match\nif match is None:\n    nargs_errors = {\n        None:_('expected one argument'),\n        OPTIONAL:_('expected at most one argument'),\n        ONE_OR_MORE:_('expected at least one argument')\n    }\n    default = _('expected %s argument(s)') % action.nargs\n    msg = nargs_errors.get(action.nargs, default)\n    raise ArgumentError(action, msg)\n\n# return the number of arguments matched\nreturn len(match.group(1))", "path": "alien\\argparse.py", "repo_name": "yaniv-aknin/yalib", "stars": 1, "license": "None", "language": "python", "size": 96}
{"docstring": "# determine short and long option strings\n", "func_signal": "def _get_optional_kwargs(self, *args, **kwargs):\n", "code": "option_strings = []\nlong_option_strings = []\nfor option_string in args:\n    # error on one-or-fewer-character option strings\n    if len(option_string) < 2:\n        msg = _('invalid option string %r: '\n                'must be at least two characters long')\n        raise ValueError(msg % option_string)\n\n    # error on strings that don't start with an appropriate prefix\n    if not option_string[0] in self.prefix_chars:\n        msg = _('invalid option string %r: '\n                'must start with a character %r')\n        tup = option_string, self.prefix_chars\n        raise ValueError(msg % tup)\n\n    # error on strings that are all prefix characters\n    if not (set(option_string) - set(self.prefix_chars)):\n        msg = _('invalid option string %r: '\n                'must contain characters other than %r')\n        tup = option_string, self.prefix_chars\n        raise ValueError(msg % tup)\n\n    # strings starting with two prefix characters are long options\n    option_strings.append(option_string)\n    if option_string[0] in self.prefix_chars:\n        if option_string[1] in self.prefix_chars:\n            long_option_strings.append(option_string)\n\n# infer destination, '--foo-bar' -> 'foo_bar' and '-x' -> 'x'\ndest = kwargs.pop('dest', None)\nif dest is None:\n    if long_option_strings:\n        dest_option_string = long_option_strings[0]\n    else:\n        dest_option_string = option_strings[0]\n    dest = dest_option_string.lstrip(self.prefix_chars)\n    dest = dest.replace('-', '_')\n\n# return the updated keyword arguments\nreturn dict(kwargs, dest=dest, option_strings=option_strings)", "path": "alien\\argparse.py", "repo_name": "yaniv-aknin/yalib", "stars": 1, "license": "None", "language": "python", "size": 96}
{"docstring": "# progressively shorten the actions list by slicing off the\n# final actions until we find a match\n", "func_signal": "def _match_arguments_partial(self, actions, arg_strings_pattern):\n", "code": "result = []\nfor i in xrange(len(actions), 0, -1):\n    actions_slice = actions[:i]\n    pattern = ''.join(self._get_nargs_pattern(action)\n                      for action in actions_slice)\n    match = _re.match(pattern, arg_strings_pattern)\n    if match is not None:\n        result.extend(len(string) for string in match.groups())\n        break\n\n# return the list of arg string counts\nreturn result", "path": "alien\\argparse.py", "repo_name": "yaniv-aknin/yalib", "stars": 1, "license": "None", "language": "python", "size": 96}
{"docstring": "# Extended venn diagram based on int list, scale the data to the max value\n", "func_signal": "def test_venn(self):\n", "code": "G = Venn( [100,80,60,30,30,30,10], encoding='text')\nG.scale(0,100)\nself._test_a_chart(repr(self), G.checksum())\nreturn G", "path": "src\\GChartWrapper\\tests.py", "repo_name": "mettadore/update-rose", "stars": 1, "license": "None", "language": "python", "size": 192}
{"docstring": "# Fill the chart/background using chf, add axes to show bg \n", "func_signal": "def test_fill(self):\n", "code": "G = Line( ['pqokeYONOMEBAKPOQVTXZdecaZcglprqxuux393ztpoonkeggjp'] )\nG.color('red')\nG.line(4,3,0)\nG.axes('xy') \nG.axes.label(0, 1,2,3,4,5)\nG.axes.label(1, None,50,100)\nG.fill('c','lg',45,'white',0,'76A4FB',0.75)\nG.fill('bg','s','EFEFEF')\nself._test_a_chart(repr(self), G.checksum())    \nreturn G", "path": "src\\GChartWrapper\\tests.py", "repo_name": "mettadore/update-rose", "stars": 1, "license": "None", "language": "python", "size": 192}
{"docstring": "# Title using name with optional color and size\n", "func_signal": "def test_title(self):\n", "code": "G = Line( ['GurMrabsClgubaolGvzCrgrefOrnhgvshyvforggregunahtyl'] )\nG.title('The Zen of Python','00cc00',36)\nG.color('00cc00')\nself._test_a_chart(repr(self), G.checksum())\nreturn G", "path": "src\\GChartWrapper\\tests.py", "repo_name": "mettadore/update-rose", "stars": 1, "license": "None", "language": "python", "size": 192}
{"docstring": "# Map pin w/ cool icon\n", "func_signal": "def test_adv_icon_pin(self):\n", "code": "G = Pin('xpin_icon','star','home','aqua','red')\nself._test_a_chart(repr(self), G.checksum())\nreturn G", "path": "src\\GChartWrapper\\tests.py", "repo_name": "mettadore/update-rose", "stars": 1, "license": "None", "language": "python", "size": 192}
{"docstring": "# Using text markers in a bar chart\n", "func_signal": "def test_bar_text(self):\n", "code": "G = HorizontalBarGroup([[40,60],[50,30]], encoding='text')\nG.size(200,125)\nG.marker('tApril mobile hits','black',0,0,13)\nG.marker('tMay mobile hits','black',0,1,13,-1)\nG.marker('tApril desktop hits','black',1,0,13)\nG.marker('tMay desktop hits', 'black',1,1,13)\nG.color('FF9900','FFCC33')\nself._test_a_chart(repr(self), G.checksum())\nreturn G", "path": "src\\GChartWrapper\\tests.py", "repo_name": "mettadore/update-rose", "stars": 1, "license": "None", "language": "python", "size": 192}
{"docstring": "# Add a left aligned legend to the chart\n", "func_signal": "def test_legend2(self):\n", "code": "G = Line( ['abcde','FGHIJ','09876'] )  \nG.color('red','lime','blue')\nG.legend('Animals','Vegetables','Minerals')\nG.legend_pos('l')\nG.axes('y') \nself._test_a_chart(repr(self), G.checksum())\nreturn G", "path": "src\\GChartWrapper\\tests.py", "repo_name": "mettadore/update-rose", "stars": 1, "license": "None", "language": "python", "size": 192}
{"docstring": "# Larger bubble marker\n", "func_signal": "def test_large_bubble_icon(self):\n", "code": "G = Bubble('icon_text_big','snack','bb','$2.99','ffbb00','black')\nself._test_a_chart(repr(self), G.checksum())   \nreturn G", "path": "src\\GChartWrapper\\tests.py", "repo_name": "mettadore/update-rose", "stars": 1, "license": "None", "language": "python", "size": 192}
{"docstring": "# Add red line 6 thick\n# with 5 line segments with 2 blank segments\n", "func_signal": "def test_line(self):\n", "code": "G = Line( ['hX1xPj'] )\nG.axes('xy')\nG.axes.label(0, 'Mar', 'Apr', 'May', 'June', 'July')\nG.axes.label(1, None, '50+Kb')        \nG.color('red')\nG.line(6,5,2)\nself._test_a_chart(repr(self), G.checksum())\nreturn G", "path": "src\\GChartWrapper\\tests.py", "repo_name": "mettadore/update-rose", "stars": 1, "license": "None", "language": "python", "size": 192}
{"docstring": "# Make a text chart label w/ any text you like\n# Google automagically ignores white space and spaces text correctly\n", "func_signal": "def test_text(self):\n", "code": "text = '''\n1600 Ampitheatre Parkway\nMountain View, CA\n(650)+253-0000\n'''\nG = Text('darkred',16,'h','red','b',text)\nself._test_a_chart(repr(self), G.checksum())\nreturn G", "path": "src\\GChartWrapper\\tests.py", "repo_name": "mettadore/update-rose", "stars": 1, "license": "None", "language": "python", "size": 192}
{"docstring": "# Draw multiple lines with markers on an lxy chart\n", "func_signal": "def test_multiline(self):\n", "code": "G = LineXY( [ \n    [0,30,60,70,90,95,100], # x values\n    [20,30,40,50,60,70,80], # y values, etc.\n    [10,30,40,45,52],\n    [100,90,40,20,10],\n    ['-1'], # domain not found, interpolated\n    [5,33,50,55,7],\n])\nG.scale(0,100)\nG.color('3072F3','red','00aaaa')\nG.marker('s','red',0,-1,5)\nG.marker('s','blue',1,-1,5)\nG.marker('s','00aa00',2,-1,5)   \nG.line(2,4,1)   \nself._test_a_chart(repr(self), G.checksum())\nreturn G", "path": "src\\GChartWrapper\\tests.py", "repo_name": "mettadore/update-rose", "stars": 1, "license": "None", "language": "python", "size": 192}
{"docstring": "# http://code.google.com/apis/chart/#sparkline  \n", "func_signal": "def test_guide_sparkline(self):\n", "code": "G = Sparkline([27,25,25,25,25,27,100,31,25,36,25,25,39,\n    25,31,25,25,25,26,26,25,25,28,25,25,100,28,27,31,25,\n    27,27,29,25,27,26,26,25,26,26,35,33,34,25,26,25,36,25,\n    26,37,33,33,37,37,39,25,25,25,25], encoding='text')\nG.color('0077CC')\nG.size(200,40)\nG.marker('B', 'E6F2FA',0,0,0)\nG.line(1,0,0)\nself._test_a_chart(repr(self), G.checksum())\nreturn G", "path": "src\\GChartWrapper\\tests.py", "repo_name": "mettadore/update-rose", "stars": 1, "license": "None", "language": "python", "size": 192}
{"docstring": "# Submitted by anedvedicky\n", "func_signal": "def test_czech_and_unicode(self):\n", "code": "G = VerticalBarStack( [[10], [20], [30]], encoding = 'text')\nG.color('green','lime','red')\nG.label('\u0161\u00fd\u017d\u011b\u016f\u010d\u00e1\u0159...')\nG.legend('\u0161\u00fd\u017d\u011b\u016f\u010d\u00e1\u0159...','\u222b\u00b5\u2264','\u00b4\u00ae\u2020\u00a5\u00a8\u02c6\u00f8\u03c0\u00ac\u02da\u2264\u00b5\u02dc')\nself._test_a_chart(repr(self), G.checksum())\nreturn G", "path": "src\\GChartWrapper\\tests.py", "repo_name": "mettadore/update-rose", "stars": 1, "license": "None", "language": "python", "size": 192}
{"docstring": "#http://chart.apis.google.com/chart?cht=v&chs=200x100&chd=t:100,20,20,20,20,0,0&chdl=First|Second|Third&chco=ff0000,00ff00,0000ff&chdlp=t\n", "func_signal": "def test_legend_position(self):\n", "code": "G = Venn([100,20,20,20,20,0,0])\nG.legend('First','Second','Third')\nG.legend_pos('t')\nG.color('red','lime','blue')\nself._test_a_chart(repr(self), G.checksum())\nreturn G", "path": "src\\GChartWrapper\\tests.py", "repo_name": "mettadore/update-rose", "stars": 1, "license": "None", "language": "python", "size": 192}
{"docstring": "# Call type first with the chxt\n# then call label and style in order, \n# label can contain None(s)\n", "func_signal": "def test_axes(self):\n", "code": "G = Line( ['foobarbaz'] )\nG.color('76A4FB') \nG.axes('xyrx')\nG.axes.label(0,'Foo', 'Bar', 'Baz')\nG.axes.style(0, '0000dd', 14)\nG.axes.label(1, None, '20K', '60K', '100K')  \nG.axes.label(2, 'A', 'B', 'C')  \nG.axes.label(3, None,'20','40','60','80')      \nself._test_a_chart(repr(self), G.checksum())  \nreturn G", "path": "src\\GChartWrapper\\tests.py", "repo_name": "mettadore/update-rose", "stars": 1, "license": "None", "language": "python", "size": 192}
{"docstring": "# Output a QR code graph that allows 15% restore with 0 margin\n# *Defaults to UTF-8 encoding \n", "func_signal": "def test_qr_code(self):\n", "code": "G = QRCode('''To the human eye QR Codes look like hieroglyphics, \n    but they can be read by any device that has \n    the appropriate software installed.''')\n# or use output_encoding method\nG.output_encoding('UTF-8')\n# level_data(error_correction,margin_size)\nG.level_data('M',0)\nself._test_a_chart(repr(self), G.checksum())\nreturn G", "path": "src\\GChartWrapper\\tests.py", "repo_name": "mettadore/update-rose", "stars": 1, "license": "None", "language": "python", "size": 192}
{"docstring": "# Create a radar chart w/ multiple lines\n", "func_signal": "def test_guide_radar(self):\n", "code": "G = Radar([ [77,66,15,0,31,48,100,77],[20,36,100,2,0,100] ], encoding='text')  \nG.size(200,200)\nG.color('red','FF9900')\nG.line(2,4,0)\nG.line(2,4,0)        \nG.axes('x')\nG.axes.label(0, 0,45,90,135,180,225,270,315)\nG.axes.range(0, 0,360)\nself._test_a_chart(repr(self), G.checksum())\nreturn G", "path": "src\\GChartWrapper\\tests.py", "repo_name": "mettadore/update-rose", "stars": 1, "license": "None", "language": "python", "size": 192}
{"docstring": "# Large bubble marker with just text\n", "func_signal": "def test_large_bubble_texts(self):\n", "code": "G = Bubble('texts_big','bb','teal','khaki',\"Joe\\'s Restaurant\\n123 Long St\\n92745 Mountain View\")\nself._test_a_chart(repr(self), G.checksum())\nreturn G", "path": "src\\GChartWrapper\\tests.py", "repo_name": "mettadore/update-rose", "stars": 1, "license": "None", "language": "python", "size": 192}
{"docstring": "# multiple axis with label positions specified\n# values between 0 and 100 - use text encoding\n", "func_signal": "def test_axes_position(self):\n", "code": "data = [[4.6, 6.0, 7.4, 11.6, 12.0, 14.8, 18.1, 25.1, \n         27.9, 28.3, 30.6, 34.4, 43.7, 48.3, 57.6, 64.6, \n         72.5, 74.4, 76.2, 77.2, 86.0, 86.9, 93.9, 96.7, 99.0], \n        [80.5, 100.0, 95.4, 93.7, 96.3, 91.7, 71.5, 63.0, \n         65.2, 65.5, 66.0, 75.9, 65.8, 64.4, 64.2, 62.5, 37.2, \n         35.3, 32.4, 35.2, 38.4, 37.9, 69.8, 38.0, 64.5]]\n\n# positions between 0 and 100\naxis = [ [0, 13, 28, 42, 56, 71, 84, 100],\n         ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] ]\n\n# don't do integer arithmetic\nmin_value = float(min(data[1]))\nmax_value = float(max(data[1]))\nlast_value = float(data[1][-1])\n\nG = LineXY(data, encoding='text')\nG.color('76A4FB')\nG.marker('o', '0077CC',0,-1,5)\nG.marker('r', 'E6F2FA',0,(min_value/max_value),1.0) # 0 to 1.0\nG.axes(\"xyr\")    \nG.axes.label(0, *axis[1])\nG.axes.position(0, *axis[0])\nG.axes.label(1, '%d'%min_value, '%d'%max_value)    \nG.axes.position(1, int(100*min_value/max_value),100) # 0 to 100\nG.axes.label(2, '%d'%last_value)\nG.axes.position(2, int(100*last_value/max_value)) # 0 to 100\nself._test_a_chart(repr(self), G.checksum())        \nreturn G", "path": "src\\GChartWrapper\\tests.py", "repo_name": "mettadore/update-rose", "stars": 1, "license": "None", "language": "python", "size": 192}
{"docstring": "# Map pin w/ a certain icon\n", "func_signal": "def test_icon_pin(self):\n", "code": "G = Pin('pin_icon','home','yellow')\nself._test_a_chart(repr(self), G.checksum())\nreturn G", "path": "src\\GChartWrapper\\tests.py", "repo_name": "mettadore/update-rose", "stars": 1, "license": "None", "language": "python", "size": 192}
{"docstring": "# Using concentric pie charts\n", "func_signal": "def test_concentric_pie(self):\n", "code": "G = PieC(['Helo','Wrld'], encoding='simple')\nG.size(200,100)\nself._test_a_chart(repr(self), G.checksum())\nreturn G", "path": "src\\GChartWrapper\\tests.py", "repo_name": "mettadore/update-rose", "stars": 1, "license": "None", "language": "python", "size": 192}
{"docstring": "\"\"\"The back-redirect target for this form.\"\"\"\n", "func_signal": "def redirect_target(self):\n", "code": "return get_redirect_target(self.invalid_redirect_targets,\n                           self.request)", "path": "hugh\\forms.py", "repo_name": "mgood/hugh", "stars": 1, "license": "other", "language": "python", "size": 152}
{"docstring": "\"\"\"Return a li item.\"\"\"\n", "func_signal": "def as_li(self, **attrs):\n", "code": "rv = [self.render(**attrs)]\nif self.label:\n    rv.append(u' ' + self.label())\nif self.help_text:\n    rv.append(html.div(self.help_text, class_='explanation'))\nrv.append(self.errors())\nreturn html.li(u''.join(rv))", "path": "hugh\\forms.py", "repo_name": "mgood/hugh", "stars": 1, "license": "other", "language": "python", "size": 152}
{"docstring": "\"\"\"Checks if a choice is selected.  If the field is a multi select\nfield it's checked if the choice is in the passed iterable of values,\notherwise it's checked if the value matches the choice.\n\"\"\"\n", "func_signal": "def _is_choice_selected(field, value, choice):\n", "code": "if field.multiple_choices:\n    for value in value:\n        if _value_matches_choice(value, choice):\n            return True\n    return False\nreturn _value_matches_choice(value, choice)", "path": "hugh\\forms.py", "repo_name": "mgood/hugh", "stars": 1, "license": "other", "language": "python", "size": 152}
{"docstring": "\"\"\"Similar to `_force_list` but always succeeds and never drops data.\"\"\"\n", "func_signal": "def _to_list(value):\n", "code": "if value is None:\n    return []\nif isinstance(value, basestring):\n    return [value]\ntry:\n    return list(value)\nexcept TypeError:\n    return [value]", "path": "hugh\\forms.py", "repo_name": "mgood/hugh", "stars": 1, "license": "other", "language": "python", "size": 152}
{"docstring": "\"\"\"True if the form has changed.\"\"\"\n", "func_signal": "def has_changed(self):\n", "code": "return self._root_field.to_primitive(self.initial) != \\\n       self._root_field.to_primitive(self.data)", "path": "hugh\\forms.py", "repo_name": "mgood/hugh", "stars": 1, "license": "other", "language": "python", "size": 152}
{"docstring": "\"\"\"Add an ID to the attrs if there is none.\"\"\"\n", "func_signal": "def _attr_setdefault(self, attrs):\n", "code": "if 'id' not in attrs and self.id is not None:\n    attrs['id'] = self.id", "path": "hugh\\forms.py", "repo_name": "mgood/hugh", "stars": 1, "license": "other", "language": "python", "size": 152}
{"docstring": "\"\"\"Return one or multiple hidden fields for the current value.  This\nalso handles subwidgets.  This is useful for transparent form data\npassing.\n\"\"\"\n", "func_signal": "def hidden(self):\n", "code": "fields = []\n\ndef _add_field(name, value):\n    fields.append(html.input(type='hidden', name=name, value=value))\n\ndef _to_hidden(value, name):\n    if isinstance(value, list):\n        for idx, value in enumerate(value):\n            _to_hidden(value, _make_name(name, idx))\n    elif isinstance(value, dict):\n        for key, value in value.iteritems():\n            _to_hidden(value, _make_name(name, key))\n    else:\n        _add_field(name, value)\n\n_to_hidden(self.value, self.name)\nreturn u'\\n'.join(fields)", "path": "hugh\\forms.py", "repo_name": "mgood/hugh", "stars": 1, "license": "other", "language": "python", "size": 152}
{"docstring": "\"\"\"If the value is not a dict, raise an exception.\"\"\"\n", "func_signal": "def _force_dict(value):\n", "code": "if value is None or not isinstance(value, dict):\n    return {}\nreturn value", "path": "hugh\\forms.py", "repo_name": "mgood/hugh", "stars": 1, "license": "other", "language": "python", "size": 152}
{"docstring": "\"\"\"Method that binds a field to a form. If `form` is None, a copy of\nthe field is returned.\"\"\"\n", "func_signal": "def _bind(self, form, memo):\n", "code": "if form is not None and self.bound:\n    raise TypeError('%r already bound' % type(obj).__name__)\nrv = object.__new__(self.__class__)\nrv.__dict__.update(self.__dict__)\nrv.validators = self.validators[:]\nrv.messages = self.messages.copy()\nif form is not None:\n    rv.form = form\nreturn rv", "path": "hugh\\forms.py", "repo_name": "mgood/hugh", "stars": 1, "license": "other", "language": "python", "size": 152}
{"docstring": "\"\"\"The unique CSRF security token for this form.\"\"\"\n", "func_signal": "def csrf_token(self):\n", "code": "if self.request is None:\n    raise AttributeError('no csrf token because form not bound '\n                         'to request')\npath = self.request.path\nuser_id = -1\nif self.request.user.is_somebody:\n    user_id = self.request.user.id\nlogin_time = self.request.session.get('lt', -1)\nkey = self.request.app.cfg['secret_key']\nreturn sha1(('%s|%s|%s|%s' % (path, login_time, user_id, key))\n             .encode('utf-8')).hexdigest()", "path": "hugh\\forms.py", "repo_name": "mgood/hugh", "stars": 1, "license": "other", "language": "python", "size": 152}
{"docstring": "\"\"\"Iterate over choices.\"\"\"\n", "func_signal": "def _iter_choices(choices):\n", "code": "if choices is not None:\n    for choice in choices:\n        if not isinstance(choice, tuple):\n            choice = (choice, choice)\n        yield choice", "path": "hugh\\forms.py", "repo_name": "mgood/hugh", "stars": 1, "license": "other", "language": "python", "size": 152}
{"docstring": "\"\"\"Helper for the field binding.  This is inspired by the way `deepcopy`\nis implemented.\n\"\"\"\n", "func_signal": "def _bind(obj, form, memo):\n", "code": "if memo is None:\n    memo = {}\nobj_id = id(obj)\nif obj_id in memo:\n    return memo[obj_id]\nrv = obj._bind(form, memo)\nmemo[obj_id] = rv\nreturn rv", "path": "hugh\\forms.py", "repo_name": "mgood/hugh", "stars": 1, "license": "other", "language": "python", "size": 152}
{"docstring": "\"\"\"Return the form as widget. If an id_prefix is provided this will\nbe added to the id attributes of all fields. This can be used to avoid\nid conflicts between multiple forms on the same page.\n\nThe default rendering of a form field includes only the name of the\nfield, so if multiple forms on the same page both had a field named\n\"the_field\" their ids would conflict.\n\n>>> class MyForm(Form):\n...     the_field = TextField()\n>>> MyForm().as_widget()['the_field'].render()\nu'<input type=\"text\" name=\"the_field\" value=\"\" id=\"f_the_field\">'\n\nIf an id_prefix is provided it is inserted before the field name so\nthat each form's fields will have unique ids.\n\n>>> MyForm().as_widget(id_prefix=\"my_form\")['the_field'].render()\nu'<input type=\"text\" name=\"the_field\" value=\"\" id=\"f_my_form_the_field\">'\n\"\"\"\n# if there is submitted data, use that for the widget\n", "func_signal": "def as_widget(self, id_prefix=None):\n", "code": "if self.raw_data is not None:\n    data = self.raw_data\n# otherwise go with the data from the source (eg: database)\nelse:\n    data = self.data\nreturn _make_widget(self._root_field, None, data, self.errors,\n                    id_prefix)", "path": "hugh\\forms.py", "repo_name": "mgood/hugh", "stars": 1, "license": "other", "language": "python", "size": 152}
{"docstring": "\"\"\"A helper to fill the dict passed with the items passed as keyword\narguments if they are not yet in the dict.  If the dict passed was\n`None` a new dict is created and returned.\n\nThis can be used to prepopulate initial dicts in overriden constructors:\n\n    class MyForm(forms.Form):\n        foo = forms.TextField()\n        bar = forms.TextField()\n\n        def __init__(self, initial=None):\n            forms.Form.__init__(self, forms.fill_dict(initial,\n                foo=\"nothing\",\n                bar=\"nothing\"\n            ))\n\"\"\"\n", "func_signal": "def fill_dict(_dict, **kwargs):\n", "code": "if _dict is None:\n    return kwargs\nfor key, value in kwargs.iteritems():\n    if key not in _dict:\n        _dict[key] = value\nreturn _dict", "path": "hugh\\forms.py", "repo_name": "mgood/hugh", "stars": 1, "license": "other", "language": "python", "size": 152}
{"docstring": "\"\"\"Resets the form.\"\"\"\n", "func_signal": "def reset(self):\n", "code": "self.data = self.initial.copy()\nself.errors = {}\nself.raw_data = None", "path": "hugh\\forms.py", "repo_name": "mgood/hugh", "stars": 1, "license": "other", "language": "python", "size": 152}
{"docstring": "\"\"\"The help text of the widget.\"\"\"\n", "func_signal": "def help_text(self):\n", "code": "if self._field.help_text is not None:\n    return unicode(self._field.help_text)", "path": "hugh\\forms.py", "repo_name": "mgood/hugh", "stars": 1, "license": "other", "language": "python", "size": 152}
{"docstring": "\"\"\"Return the next position hint.\"\"\"\n", "func_signal": "def _next_position_hint():\n", "code": "global _last_position_hint\n_position_hint_lock.acquire()\ntry:\n    _last_position_hint += 1\n    return _last_position_hint\nfinally:\n    _position_hint_lock.release()", "path": "hugh\\forms.py", "repo_name": "mgood/hugh", "stars": 1, "license": "other", "language": "python", "size": 152}
{"docstring": "\"\"\"The current errors and the errors of all child widgets.\"\"\"\n", "func_signal": "def all_errors(self):\n", "code": "items = sorted(self._all_errors.items())\nif self.name is None:\n    return ErrorList(chain(*(item[1] for item in items)))\nresult = ErrorList()\nfor key, value in items:\n    if key == self.name or key.startswith(self.name + '.'):\n        result.extend(value)\nreturn result", "path": "hugh\\forms.py", "repo_name": "mgood/hugh", "stars": 1, "license": "other", "language": "python", "size": 152}
{"docstring": "\"\"\"This method is called by the `hidden_fields` property to return\na list of (key, value) pairs for the special hidden fields.\n\"\"\"\n", "func_signal": "def get_hidden_fields(self):\n", "code": "fields = []\nif self._field.form.request is not None:\n    if self._field.form.csrf_protected:\n        fields.append(('_csrf_token', self.csrf_token))\n    if self._field.form.redirect_tracking:\n        target = self.redirect_target\n        if target is not None:\n            fields.append(('_redirect_target', target))\nreturn fields", "path": "hugh\\forms.py", "repo_name": "mgood/hugh", "stars": 1, "license": "other", "language": "python", "size": 152}
{"docstring": "\"\"\"The label for the widget.\"\"\"\n", "func_signal": "def label(self):\n", "code": "if self._field.label is not None:\n    return Label(unicode(self._field.label), self.id)", "path": "hugh\\forms.py", "repo_name": "mgood/hugh", "stars": 1, "license": "other", "language": "python", "size": 152}
{"docstring": "\"\"\"Filter sequence of receivers to get resolved, live receivers.\n\nThis is a generator which will iterate over the passed sequence,\nchecking for weak references and resolving them, then returning\nall live receivers.\n\"\"\"\n", "func_signal": "def live_receivers(receivers):\n", "code": "for receiver in receivers:\n    if isinstance(receiver, WEAKREF_TYPES):\n        # Dereference the weak reference.\n        receiver = receiver()\n    if receiver is not None:\n        # Check installed plugins to make sure this receiver is\n        # live.\n        live = True\n        for plugin in plugins:\n            if not plugin.is_live(receiver):\n                live = False\n                break\n        if live:\n            yield receiver", "path": "louie\\dispatcher.py", "repo_name": "enoki/bandleader", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Send ``signal`` from ``sender`` to all connected receivers.\n\n- ``signal``: (Hashable) signal value; see ``connect`` for details.\n\n- ``sender``: The sender of the signal.\n\n  If ``Any``, only receivers registered for ``Any`` will receive the\n  message.\n\n  If ``Anonymous``, only receivers registered to receive messages\n  from ``Anonymous`` or ``Any`` will receive the message.\n\n  Otherwise can be any Python object (normally one registered with\n  a connect if you actually want something to occur).\n\n- ``arguments``: Positional arguments which will be passed to *all*\n  receivers. Note that this may raise ``TypeError`` if the receivers\n  do not allow the particular arguments.  Note also that arguments\n  are applied before named arguments, so they should be used with\n  care.\n\n- ``named``: Named arguments which will be filtered according to the\n  parameters of the receivers to only provide those acceptable to\n  the receiver.\n\nReturn a list of tuple pairs ``[(receiver, response), ...]``\n\nIf any receiver raises an error, the error propagates back through\nsend, terminating the dispatch loop, so it is quite possible to\nnot have all receivers called if a raises an error.\n\"\"\"\n# Call each receiver with whatever arguments it can accept.\n# Return a list of tuple pairs [(receiver, response), ... ].\n", "func_signal": "def send(signal=All, sender=Anonymous, *arguments, **named):\n", "code": "responses = []\nfor receiver in live_receivers(get_all_receivers(sender, signal)):\n    # Wrap receiver using installed plugins.\n    original = receiver\n    for plugin in plugins:\n        receiver = plugin.wrap_receiver(receiver)\n    response = robustapply.robust_apply(\n        receiver, original,\n        signal=signal,\n        sender=sender,\n        *arguments,\n        **named\n        )\n    responses.append((receiver, response))\n# Update stats.\nif __debug__:\n    global sends\n    sends += 1\nreturn responses", "path": "louie\\dispatcher.py", "repo_name": "enoki/bandleader", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Remove all back-references to this ``senderkey``.\"\"\"\n", "func_signal": "def _remove_back_refs(senderkey):\n", "code": "try:\n    signals = connections[senderkey]\nexcept KeyError:\n    signals = None\nelse:\n    for signal, receivers in signals.iteritems():\n        for receiver in receivers:\n            _kill_back_ref(receiver, senderkey)", "path": "louie\\dispatcher.py", "repo_name": "enoki/bandleader", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Return a strong reference to the bound method.\n\nIf the target cannot be retrieved, then will return None,\notherwise returns a bound instance method for our object and\nfunction.\n\nNote: You may call this method any number of times, as it does\nnot invalidate the reference.\n\"\"\"\n", "func_signal": "def __call__(self):\n", "code": "target = self.weak_self()\nif target is not None:\n    function = self.weak_func()\n    if function is not None:\n        return function.__get__(target)\nreturn None", "path": "louie\\saferef.py", "repo_name": "enoki/bandleader", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Remove ``senderkey`` from connections.\"\"\"\n", "func_signal": "def _remove_sender(senderkey):\n", "code": "_remove_back_refs(senderkey)\ntry:\n    del connections[senderkey]\nexcept KeyError:\n    pass\n# Senderkey will only be in senders dictionary if sender \n# could be weakly referenced.\ntry:\n    del senders[senderkey]\nexcept:\n    pass", "path": "louie\\dispatcher.py", "repo_name": "enoki/bandleader", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"If receiver is a method on a QWidget, only return True if\nit hasn't been destroyed.\"\"\"\n", "func_signal": "def is_live(self, receiver):\n", "code": "if (hasattr(receiver, 'im_self') and\n    isinstance(receiver.im_self, self.qt.QWidget)\n    ):\n    try:\n        receiver.im_self.x()\n    except RuntimeError:\n        return False\nreturn True", "path": "louie\\plugin.py", "repo_name": "enoki/bandleader", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Like ``send``, but does not attach ``signal`` and ``sender``\narguments to the call to the receiver.\"\"\"\n# Call each receiver with whatever arguments it can accept.\n# Return a list of tuple pairs [(receiver, response), ... ].\n", "func_signal": "def send_minimal(signal=All, sender=Anonymous, *arguments, **named):\n", "code": "responses = []\nfor receiver in live_receivers(get_all_receivers(sender, signal)):\n    # Wrap receiver using installed plugins.\n    original = receiver\n    for plugin in plugins:\n        receiver = plugin.wrap_receiver(receiver)\n    response = robustapply.robust_apply(\n        receiver, original,\n        *arguments,\n        **named\n        )\n    responses.append((receiver, response))\n# Update stats.\nif __debug__:\n    global sends\n    sends += 1\nreturn responses", "path": "louie\\dispatcher.py", "repo_name": "enoki/bandleader", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Get function-like callable object for given receiver.\n\nreturns (function_or_method, codeObject, fromMethod)\n\nIf fromMethod is true, then the callable already has its first\nargument bound.\n\"\"\"\n", "func_signal": "def function(receiver):\n", "code": "if hasattr(receiver, '__call__'):\n    # receiver is a class instance; assume it is callable.\n    # Reassign receiver to the actual method that will be called.\n    c = receiver.__call__\n    if hasattr(c, 'im_func') or hasattr(c, 'im_code'):\n        receiver = c\nif hasattr(receiver, 'im_func'):\n    # receiver is an instance-method.\n    return receiver, receiver.im_func.func_code, 1\nelif not hasattr(receiver, 'func_code'):\n    raise ValueError(\n        'unknown reciever type %s %s' % (receiver, type(receiver)))\nreturn receiver, receiver.func_code, 0", "path": "louie\\robustapply.py", "repo_name": "enoki/bandleader", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "# Don't import reactor ourselves, but make access to it\n# easier.\n", "func_signal": "def __init__(self):\n", "code": "from twisted import internet\nfrom twisted.internet.defer import Deferred\nself._internet = internet\nself._Deferred = Deferred", "path": "louie\\plugin.py", "repo_name": "enoki/bandleader", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Create new instance or return current instance.\n\nBasically this method of construction allows us to\nshort-circuit creation of references to already- referenced\ninstance methods.  The key corresponding to the target is\ncalculated, and if there is already an existing reference,\nthat is returned, with its deletion_methods attribute updated.\nOtherwise the new instance is created and registered in the\ntable of already-referenced methods.\n\"\"\"\n", "func_signal": "def __new__(cls, target, on_delete=None, *arguments, **named):\n", "code": "key = cls.calculate_key(target)\ncurrent = cls._all_instances.get(key)\nif current is not None:\n    current.deletion_methods.append(on_delete)\n    return current\nelse:\n    base = super(BoundMethodWeakref, cls).__new__(cls)\n    cls._all_instances[key] = base\n    base.__init__(target, on_delete, *arguments, **named)\n    return base", "path": "louie\\saferef.py", "repo_name": "enoki/bandleader", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Kill old ``senders_back`` references from ``receiver``.\n\nThis guards against multiple registration of the same receiver for\na given signal and sender leaking memory as old back reference\nrecords build up.\n\nAlso removes old receiver instance from receivers.\n\"\"\"\n", "func_signal": "def _remove_old_back_refs(senderkey, signal, receiver, receivers):\n", "code": "try:\n    index = receivers.index(receiver)\n    # need to scan back references here and remove senderkey\nexcept ValueError:\n    return False\nelse:\n    old_receiver = receivers[index]\n    del receivers[index]\n    found = 0\n    signals = connections.get(signal)\n    if signals is not None:\n        for sig, recs in connections.get(signal, {}).iteritems():\n            if sig != signal:\n                for rec in recs:\n                    if rec is old_receiver:\n                        found = 1\n                        break\n    if not found:\n        _kill_back_ref(old_receiver, senderkey)\n        return True\n    return False", "path": "louie\\dispatcher.py", "repo_name": "enoki/bandleader", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Delete empty signals for ``senderkey``. Delete ``senderkey`` if\nempty.\"\"\"\n", "func_signal": "def _cleanup_connections(senderkey, signal):\n", "code": "try:\n    receivers = connections[senderkey][signal]\nexcept:\n    pass\nelse:\n    if not receivers:\n        # No more connected receivers. Therefore, remove the signal.\n        try:\n            signals = connections[senderkey]\n        except KeyError:\n            pass\n        else:\n            del signals[signal]\n            if not signals:\n                # No more signal connections. Therefore, remove the sender.\n                _remove_sender(senderkey)", "path": "louie\\dispatcher.py", "repo_name": "enoki/bandleader", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Remove ``receiver`` from connections.\"\"\"\n", "func_signal": "def _remove_receiver(receiver):\n", "code": "if not senders_back:\n    # During module cleanup the mapping will be replaced with None.\n    return False\nbackKey = id(receiver)\nfor senderkey in senders_back.get(backKey, ()):\n    try:\n        signals = connections[senderkey].keys()\n    except KeyError:\n        pass\n    else:\n        for signal in signals:\n            try:\n                receivers = connections[senderkey][signal]\n            except KeyError:\n                pass\n            else:\n                try:\n                    receivers.remove(receiver)\n                except Exception:\n                    pass\n            _cleanup_connections(senderkey, signal)\ntry:\n    del senders_back[backKey]\nexcept KeyError:\n    pass", "path": "louie\\dispatcher.py", "repo_name": "enoki/bandleader", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Disconnect ``receiver`` from ``sender`` for ``signal``.\n\n- ``receiver``: The registered receiver to disconnect.\n\n- ``signal``: The registered signal to disconnect.\n\n- ``sender``: The registered sender to disconnect.\n\n- ``weak``: The weakref state to disconnect.\n\n``disconnect`` reverses the process of ``connect``, the semantics for\nthe individual elements are logically equivalent to a tuple of\n``(receiver, signal, sender, weak)`` used as a key to be deleted\nfrom the internal routing tables.  (The actual process is slightly\nmore complex but the semantics are basically the same).\n\nNote: Using ``disconnect`` is not required to cleanup routing when\nan object is deleted; the framework will remove routes for deleted\nobjects automatically.  It's only necessary to disconnect if you\nwant to stop routing to a live object.\n    \nReturns ``None``, may raise ``DispatcherTypeError`` or\n``DispatcherKeyError``.\n\"\"\"\n", "func_signal": "def disconnect(receiver, signal=All, sender=Any, weak=True):\n", "code": "if signal is None:\n    raise error.DispatcherTypeError(\n        'Signal cannot be None (receiver=%r sender=%r)'\n        % (receiver, sender))\nif weak:\n    receiver = saferef.safe_ref(receiver)\nsenderkey = id(sender)\ntry:\n    signals = connections[senderkey]\n    receivers = signals[signal]\nexcept KeyError:\n    raise error.DispatcherKeyError(\n        'No receivers found for signal %r from sender %r' \n        % (signal, sender)\n        )\ntry:\n    # also removes from receivers\n    _remove_old_back_refs(senderkey, signal, receiver, receivers)\nexcept ValueError:\n    raise error.DispatcherKeyError(\n        'No connection to receiver %s for signal %s from sender %s'\n        % (receiver, signal, sender)\n        )\n_cleanup_connections(senderkey, signal)\n# Update stats.\nif __debug__:\n    global disconnects\n    disconnects += 1", "path": "louie\\dispatcher.py", "repo_name": "enoki/bandleader", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Reset the state of Louie.\n\nUseful during unit testing.  Should be avoided otherwise.\n\"\"\"\n", "func_signal": "def reset():\n", "code": "global connections, senders, senders_back, plugins\nconnections = {}\nsenders = {}\nsenders_back = {}\nplugins = []", "path": "louie\\dispatcher.py", "repo_name": "enoki/bandleader", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Give a friendly representation of the object.\"\"\"\n", "func_signal": "def __str__(self):\n", "code": "return \"%s(%s.%s)\" % (\n    self.__class__.__name__,\n    self.self_name,\n    self.func_name,\n    )", "path": "louie\\saferef.py", "repo_name": "enoki/bandleader", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Get list of receivers from global tables.\n\nThis function allows you to retrieve the raw list of receivers\nfrom the connections table for the given sender and signal pair.\n\nNote: There is no guarantee that this is the actual list stored in\nthe connections table, so the value should be treated as a simple\niterable/truth value rather than, for instance a list to which you\nmight append new records.\n\nNormally you would use ``live_receivers(get_receivers(...))`` to\nretrieve the actual receiver objects as an iterable object.\n\"\"\"\n", "func_signal": "def get_receivers(sender=Any, signal=All):\n", "code": "try:\n    return connections[id(sender)][signal]\nexcept KeyError:\n    return []", "path": "louie\\dispatcher.py", "repo_name": "enoki/bandleader", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Get list of all receivers from global tables.\n\nThis gets all receivers which should receive the given signal from\nsender, each receiver should be produced only once by the\nresulting generator.\n\"\"\"\n", "func_signal": "def get_all_receivers(sender=Any, signal=All):\n", "code": "yielded = set()\nfor receivers in (\n    # Get receivers that receive *this* signal from *this* sender.\n    get_receivers(sender, signal),\n    # Add receivers that receive *all* signals from *this* sender.\n    get_receivers(sender, All),\n    # Add receivers that receive *this* signal from *any* sender.\n    get_receivers(Any, signal),\n    # Add receivers that receive *all* signals from *any* sender.\n    get_receivers(Any, All),\n    ):\n    for receiver in receivers:\n        if receiver: # filter out dead instance-method weakrefs\n            try:\n                if not receiver in yielded:\n                    yielded.add(receiver)\n                    yield receiver\n            except TypeError:\n                # dead weakrefs raise TypeError on hash...\n                pass", "path": "louie\\dispatcher.py", "repo_name": "enoki/bandleader", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Do actual removal of back reference from ``receiver`` to\n``senderkey``.\"\"\"\n", "func_signal": "def _kill_back_ref(receiver, senderkey):\n", "code": "receiverkey = id(receiver)\nsenders = senders_back.get(receiverkey, ())\nwhile senderkey in senders:\n    try:\n        senders.remove(senderkey)\n    except:\n        break\nif not senders:\n    try:\n        del senders_back[receiverkey]\n    except KeyError:\n        pass\nreturn True", "path": "louie\\dispatcher.py", "repo_name": "enoki/bandleader", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Call receiver with arguments and appropriate subset of named.\n``signature`` is the callable used to determine the call signature\nof the receiver, in case ``receiver`` is a callable wrapper of the\nactual receiver.\"\"\"\n", "func_signal": "def robust_apply(receiver, signature, *arguments, **named):\n", "code": "signature, code_object, startIndex = function(signature)\nacceptable = code_object.co_varnames[\n    startIndex + len(arguments):\n    code_object.co_argcount\n    ]\nfor name in code_object.co_varnames[\n    startIndex:startIndex + len(arguments)\n    ]:\n    if named.has_key(name):\n        raise TypeError(\n            'Argument %r specified both positionally '\n            'and as a keyword for calling %r'\n            % (name, signature)\n            )\nif not (code_object.co_flags & 8):\n    # fc does not have a **kwds type parameter, therefore \n    # remove unacceptable arguments.\n    for arg in named.keys():\n        if arg not in acceptable:\n            del named[arg]\nreturn receiver(*arguments, **named)", "path": "louie\\robustapply.py", "repo_name": "enoki/bandleader", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "# Python markdown note: the HTML tokenization here differs from\n# that in Markdown.pl, hence the behaviour for subtle cases can\n# differ (I believe the tokenizer here does a better job because\n# it isn't susceptible to unmatched '<' and '>' in HTML tags).\n# Note, however, that '>' is not allowed in an auto-link URL\n# here.\n", "func_signal": "def _escape_special_chars(self, text):\n", "code": "escaped = []\nis_html_markup = False\nfor token in self._sorta_html_tokenize_re.split(text):\n    if is_html_markup:\n        # Within tags/HTML-comments/auto-links, encode * and _\n        # so they don't conflict with their use in Markdown for\n        # italics and strong.  We're replacing each such\n        # character with its corresponding MD5 checksum value;\n        # this is likely overkill, but it should prevent us from\n        # colliding with the escape values by accident.\n        escaped.append(token.replace('*', g_escape_table['*'])\n                            .replace('_', g_escape_table['_']))\n    else:\n        escaped.append(self._encode_backslash_escapes(token))\n    is_html_markup = not is_html_markup\nreturn ''.join(escaped)", "path": "markdown.py", "repo_name": "aruneinstein/groups", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "# Swap back in all the special characters we've hidden.\n", "func_signal": "def _unescape_special_chars(self, text):\n", "code": "for ch, hash in g_escape_table.items():\n    text = text.replace(hash, ch)\nreturn text", "path": "markdown.py", "repo_name": "aruneinstein/groups", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "# Process the contents of a single ordered or unordered list,\n# splitting it into individual list items.\n    \n# The $g_list_level global keeps track of when we're inside a list.\n# Each time we enter a list, we increment it; when we leave a list,\n# we decrement. If it's zero, we're not in a list anymore.\n#\n# We do this because when we're not inside a list, we want to treat\n# something like this:\n#\n#       I recommend upgrading to version\n#       8. Oops, now this line is treated\n#       as a sub-list.\n#\n# As a single paragraph, despite the fact that the second line starts\n# with a digit-period-space sequence.\n#\n# Whereas when we're inside a list (or sub-list), that line will be\n# treated as the start of a sub-list. What a kludge, huh? This is\n# an aspect of Markdown's syntax that's hard to parse perfectly\n# without resorting to mind-reading. Perhaps the solution is to\n# change the syntax rules such that sub-lists must start with a\n# starting cardinal number; e.g. \"1.\" or \"a.\".\n", "func_signal": "def _process_list_items(self, list_str):\n", "code": "self.list_level += 1\nself._last_li_endswith_two_eols = False\nlist_str = list_str.rstrip('\\n') + '\\n'\nlist_str = self._list_item_re.sub(self._list_item_sub, list_str)\nself.list_level -= 1\nreturn list_str", "path": "markdown.py", "repo_name": "aruneinstein/groups", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "\"\"\"Standalone XML processing instruction regex.\"\"\"\n", "func_signal": "def _xml_oneliner_re_from_tab_width(tab_width):\n", "code": "return re.compile(r\"\"\"\n    (?:\n        (?<=\\n\\n)       # Starting after a blank line\n        |               # or\n        \\A\\n?           # the beginning of the doc\n    )\n    (                           # save in $1\n        [ ]{0,%d}\n        (?:\n            <\\?\\w+\\b\\s+.*?\\?>   # XML processing instruction\n            |\n            <\\w+:\\w+\\b\\s+.*?/>  # namespaced single tag\n        )\n        [ \\t]*\n        (?=\\n{2,}|\\Z)       # followed by a blank line or end of document\n    )\n    \"\"\" % (tab_width - 1), re.X)", "path": "markdown.py", "repo_name": "aruneinstein/groups", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "\"\"\"Ensure that Python interactive shell sessions are put in\ncode blocks -- even if not properly indented.\n\"\"\"\n", "func_signal": "def _prepare_pyshell_blocks(self, text):\n", "code": "if \">>>\" not in text:\n    return text\n\nless_than_tab = self.tab_width - 1\n_pyshell_block_re = re.compile(r\"\"\"\n    ^([ ]{0,%d})>>>[ ].*\\n   # first line\n    ^(\\1.*\\S+.*\\n)*         # any number of subsequent lines\n    ^\\n                     # ends with a blank line\n    \"\"\" % less_than_tab, re.M | re.X)\n\nreturn _pyshell_block_re.sub(self._pyshell_block_sub, text)", "path": "markdown.py", "repo_name": "aruneinstein/groups", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "\"\"\"A function for use in a Pygments Formatter which\nwraps in <code> tags.\n\"\"\"\n", "func_signal": "def _wrap_code(self, inner):\n", "code": "yield 0, \"<code>\"\nfor tup in inner:\n    yield tup \nyield 0, \"</code>\"", "path": "markdown.py", "repo_name": "aruneinstein/groups", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "\"\"\"_dedent(text, tabsize=8, skip_first_line=False) -> dedented text\n\n    \"text\" is the text to dedent.\n    \"tabsize\" is the tab width to use for indent width calculations.\n    \"skip_first_line\" is a boolean indicating if the first line should\n        be skipped for calculating the indent width and for dedenting.\n        This is sometimes useful for docstrings and similar.\n\ntextwrap.dedent(s), but don't expand tabs to spaces\n\"\"\"\n", "func_signal": "def _dedent(text, tabsize=8, skip_first_line=False):\n", "code": "lines = text.splitlines(1)\n_dedentlines(lines, tabsize=tabsize, skip_first_line=skip_first_line)\nreturn ''.join(lines)", "path": "markdown.py", "repo_name": "aruneinstein/groups", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "# These are all the transformations that occur *within* block-level\n# tags like paragraphs, headers, and list items.\n    \n", "func_signal": "def _run_span_gamut(self, text):\n", "code": "text = self._do_code_spans(text)\n    \ntext = self._escape_special_chars(text)\n    \n# Process anchor and image tags.\ntext = self._do_links(text)\n    \n# Make links out of things like `<http://example.com/>`\n# Must come after _do_links(), because you can use < and >\n# delimiters in inline links like [this](<url>).\ntext = self._do_auto_links(text)\n\nif \"link-patterns\" in self.extras:\n    text = self._do_link_patterns(text)\n    \ntext = self._encode_amps_and_angles(text)\n    \ntext = self._do_italics_and_bold(text)\n    \n# Do hard breaks:\ntext = re.sub(r\" {2,}\\n\", \" <br%s\\n\" % self.empty_element_suffix, text)\n    \nreturn text", "path": "markdown.py", "repo_name": "aruneinstein/groups", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "# These are all the transformations that form block-level\n# tags like paragraphs, headers, and list items.\n\n", "func_signal": "def _run_block_gamut(self, text):\n", "code": "text = self._do_headers(text)\n\n# Do Horizontal Rules:\nhr = \"\\n<hr\"+self.empty_element_suffix+\"\\n\"\nfor hr_re in self._hr_res:\n    text = hr_re.sub(hr, text)\n\ntext = self._do_lists(text)\n\nif \"pyshell\" in self.extras:\n    text = self._prepare_pyshell_blocks(text)\n\ntext = self._do_code_blocks(text)\n\ntext = self._do_block_quotes(text)\n\n# We already ran _HashHTMLBlocks() before, in Markdown(), but that\n# was to escape raw HTML in the original Markdown source. This time,\n# we're escaping the markup we've just created, so that we don't wrap\n# <p> tags around block-level tags.\ntext = self._hash_html_blocks(text)\n\ntext = self._form_paragraphs(text)\n\nreturn text", "path": "markdown.py", "repo_name": "aruneinstein/groups", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "\"\"\"A footnote definition looks like this:\n\n    [^note-id]: Text of the note.\n\n        May include one or more indented paragraphs.\n\nWhere,\n- The 'note-id' can be pretty much anything, though typically it\n  is the number of the footnote.\n- The first paragraph may start on the next line, like so:\n    \n    [^note-id]:\n        Text of the note.\n\"\"\"\n", "func_signal": "def _strip_footnote_definitions(self, text):\n", "code": "less_than_tab = self.tab_width - 1\nfootnote_def_re = re.compile(r'''\n    ^[ ]{0,%d}\\[\\^(.+)\\]:   # id = \\1\n    [ \\t]*\n    (                       # footnote text = \\2\n      # First line need not start with the spaces.\n      (?:\\s*.*\\n+)\n      (?:\n        (?:[ ]{%d} | \\t)  # Subsequent lines must be indented.\n        .*\\n+\n      )*\n    )\n    # Lookahead for non-space at line-start, or end of doc.\n    (?:(?=^[ ]{0,%d}\\S)|\\Z)\n    ''' % (less_than_tab, self.tab_width, self.tab_width),\n    re.X | re.M)\nreturn footnote_def_re.sub(self._extract_footnote_def_sub, text)", "path": "markdown.py", "repo_name": "aruneinstein/groups", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "\"\"\"Convert the given text.\"\"\"\n# Main function. The order in which other subs are called here is\n# essential. Link and image substitutions need to happen before\n# _EscapeSpecialChars(), so that any *'s or _'s in the <a>\n# and <img> tags get encoded.\n\n# Clear the global hashes. If we don't clear these, you get conflicts\n# from other articles when generating a page which contains more than\n# one article (e.g. an index page that shows the N most recent\n# articles):\n", "func_signal": "def convert(self, text):\n", "code": "self.reset()\n\nif not isinstance(text, unicode):\n    #TODO: perhaps shouldn't presume UTF-8 for string input?\n    text = unicode(text, 'utf-8')\n\nif self.use_file_vars:\n    # Look for emacs-style file variable hints.\n    emacs_vars = self._get_emacs_vars(text)\n    if \"markdown-extras\" in emacs_vars:\n        splitter = re.compile(\"[ ,]+\")\n        for e in splitter.split(emacs_vars[\"markdown-extras\"]):\n            if '=' in e:\n                ename, earg = e.split('=', 1)\n                try:\n                    earg = int(earg)\n                except ValueError:\n                    pass\n            else:\n                ename, earg = e, None\n            self.extras[ename] = earg\n\n# Standardize line endings:\ntext = re.sub(\"\\r\\n|\\r\", \"\\n\", text)\n\n# Make sure $text ends with a couple of newlines:\ntext += \"\\n\\n\"\n\n# Convert all tabs to spaces.\ntext = self._detab(text)\n\n# Strip any lines consisting only of spaces and tabs.\n# This makes subsequent regexen easier to write, because we can\n# match consecutive blank lines with /\\n+/ instead of something\n# contorted like /[ \\t]*\\n+/ .\ntext = self._ws_only_line_re.sub(\"\", text)\n\nif self.safe_mode:\n    text = self._hash_html_spans(text)\n\n# Turn block-level HTML blocks into hash entries\ntext = self._hash_html_blocks(text, raw=True)\n\n# Strip link definitions, store in hashes.\nif \"footnotes\" in self.extras:\n    # Must do footnotes first because an unlucky footnote defn\n    # looks like a link defn:\n    #   [^4]: this \"looks like a link defn\"\n    text = self._strip_footnote_definitions(text)\ntext = self._strip_link_definitions(text)\n\ntext = self._run_block_gamut(text)\n\nif \"footnotes\" in self.extras:\n    text = self._add_footnotes(text)\n\ntext = self._unescape_special_chars(text)\n\nif self.safe_mode:\n    text = self._unhash_html_spans(text)\n\ntext += \"\\n\"\nreturn text", "path": "markdown.py", "repo_name": "aruneinstein/groups", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "# Form HTML ordered (numbered) and unordered (bulleted) lists.\n\n", "func_signal": "def _do_lists(self, text):\n", "code": "for marker_pat in (self._marker_ul, self._marker_ol):\n    # Re-usable pattern to match any entire ul or ol list:\n    less_than_tab = self.tab_width - 1\n    whole_list = r'''\n        (                   # \\1 = whole list\n          (                 # \\2\n            [ ]{0,%d}\n            (%s)            # \\3 = first list item marker\n            [ \\t]+\n          )\n          (?:.+?)\n          (                 # \\4\n              \\Z\n            |\n              \\n{2,}\n              (?=\\S)\n              (?!           # Negative lookahead for another list item marker\n                [ \\t]*\n                %s[ \\t]+\n              )\n          )\n        )\n    ''' % (less_than_tab, marker_pat, marker_pat)\n\n    # We use a different prefix before nested lists than top-level lists.\n    # See extended comment in _process_list_items().\n    #\n    # Note: There's a bit of duplication here. My original implementation\n    # created a scalar regex pattern as the conditional result of the test on\n    # $g_list_level, and then only ran the $text =~ s{...}{...}egmx\n    # substitution once, using the scalar as the pattern. This worked,\n    # everywhere except when running under MT on my hosting account at Pair\n    # Networks. There, this caused all rebuilds to be killed by the reaper (or\n    # perhaps they crashed, but that seems incredibly unlikely given that the\n    # same script on the same server ran fine *except* under MT. I've spent\n    # more time trying to figure out why this is happening than I'd like to\n    # admit. My only guess, backed up by the fact that this workaround works,\n    # is that Perl optimizes the substition when it can figure out that the\n    # pattern will never change, and when this optimization isn't on, we run\n    # afoul of the reaper. Thus, the slightly redundant code to that uses two\n    # static s/// patterns rather than one conditional pattern.\n\n    if self.list_level:\n        sub_list_re = re.compile(\"^\"+whole_list, re.X | re.M | re.S)\n        text = sub_list_re.sub(self._list_sub, text)\n    else:\n        list_re = re.compile(r\"(?:(?<=\\n\\n)|\\A\\n?)\"+whole_list,\n                             re.X | re.M | re.S)\n        text = list_re.sub(self._list_sub, text)\n\nreturn text", "path": "markdown.py", "repo_name": "aruneinstein/groups", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "\"\"\"Encode/escape certain characters inside Markdown code runs.\nThe point is that in code, these characters are literals,\nand lose their special Markdown meanings.\n\"\"\"\n", "func_signal": "def _encode_code(self, text):\n", "code": "replacements = [\n    # Encode all ampersands; HTML entities are not\n    # entities within a Markdown code span.\n    ('&', '&amp;'),\n    # Do the angle bracket song and dance:\n    ('<', '&lt;'),\n    ('>', '&gt;'),\n    # Now, escape characters that are magic in Markdown:\n    ('*', g_escape_table['*']),\n    ('_', g_escape_table['_']),\n    ('{', g_escape_table['{']),\n    ('}', g_escape_table['}']),\n    ('[', g_escape_table['[']),\n    (']', g_escape_table[']']),\n    ('\\\\', g_escape_table['\\\\']),\n]\nfor before, after in replacements:\n    text = text.replace(before, after)\nreturn text", "path": "markdown.py", "repo_name": "aruneinstein/groups", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "#  Input: an email address, e.g. \"foo@example.com\"\n#\n#  Output: the email address as a mailto link, with each character\n#      of the address encoded as either a decimal or hex entity, in\n#      the hopes of foiling most address harvesting spam bots. E.g.:\n#\n#    <a href=\"&#x6D;&#97;&#105;&#108;&#x74;&#111;:&#102;&#111;&#111;&#64;&#101;\n#       x&#x61;&#109;&#x70;&#108;&#x65;&#x2E;&#99;&#111;&#109;\">&#102;&#111;&#111;\n#       &#64;&#101;x&#x61;&#109;&#x70;&#108;&#x65;&#x2E;&#99;&#111;&#109;</a>\n#\n#  Based on a filter by Matthew Wickline, posted to the BBEdit-Talk\n#  mailing list: <http://tinyurl.com/yu7ue>\n", "func_signal": "def _encode_email_address(self, addr):\n", "code": "chars = [_xml_encode_email_char_at_random(ch)\n         for ch in \"mailto:\" + addr]\n# Strip the mailto: from the visible part.\naddr = '<a href=\"%s\">%s</a>' \\\n       % (''.join(chars), ''.join(chars[7:]))\nreturn addr", "path": "markdown.py", "repo_name": "aruneinstein/groups", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "# Strips link definitions from text, stores the URLs and titles in\n# hash references.\n", "func_signal": "def _strip_link_definitions(self, text):\n", "code": "less_than_tab = self.tab_width - 1\n    \n# Link defs are in the form:\n#   [id]: url \"optional title\"\n_link_def_re = re.compile(r\"\"\"\n    ^[ ]{0,%d}\\[(.+)\\]: # id = \\1\n      [ \\t]*\n      \\n?               # maybe *one* newline\n      [ \\t]*\n    <?(.+?)>?           # url = \\2\n      [ \\t]*\n    (?:\n        \\n?             # maybe one newline\n        [ \\t]*\n        (?<=\\s)         # lookbehind for whitespace\n        ['\"(]\n        ([^\\n]*)        # title = \\3\n        ['\")]\n        [ \\t]*\n    )?  # title is optional\n    (?:\\n+|\\Z)\n    \"\"\" % less_than_tab, re.X | re.M | re.U)\nreturn _link_def_re.sub(self._extract_link_def_sub, text)", "path": "markdown.py", "repo_name": "aruneinstein/groups", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "# <strong> must go first:\n", "func_signal": "def _do_italics_and_bold(self, text):\n", "code": "if \"code-friendly\" in self.extras:\n    text = self._code_friendly_strong_re.sub(r\"<strong>\\1</strong>\", text)\n    text = self._code_friendly_em_re.sub(r\"<em>\\1</em>\", text)\nelse:\n    text = self._strong_re.sub(r\"<strong>\\2</strong>\", text)\n    text = self._em_re.sub(r\"<em>\\2</em>\", text)\nreturn text", "path": "markdown.py", "repo_name": "aruneinstein/groups", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "\"\"\"Turn Markdown link shortcuts into XHTML <a> and <img> tags.\n\nThis is a combination of Markdown.pl's _DoAnchors() and\n_DoImages(). They are done together because that simplified the\napproach. It was necessary to use a different approach than\nMarkdown.pl because of the lack of atomic matching support in\nPython's regex engine used in $g_nested_brackets.\n\"\"\"\n", "func_signal": "def _do_links(self, text):\n", "code": "MAX_LINK_TEXT_SENTINEL = 3000  # markdown2 issue 24\n\n# `anchor_allowed_pos` is used to support img links inside\n# anchors, but not anchors inside anchors. An anchor's start\n# pos must be `>= anchor_allowed_pos`.\nanchor_allowed_pos = 0\n\ncurr_pos = 0\nwhile True: # Handle the next link.\n    # The next '[' is the start of:\n    # - an inline anchor:   [text](url \"title\")\n    # - a reference anchor: [text][id]\n    # - an inline img:      ![text](url \"title\")\n    # - a reference img:    ![text][id]\n    # - a footnote ref:     [^id]\n    #   (Only if 'footnotes' extra enabled)\n    # - a footnote defn:    [^id]: ...\n    #   (Only if 'footnotes' extra enabled) These have already\n    #   been stripped in _strip_footnote_definitions() so no\n    #   need to watch for them.\n    # - a link definition:  [id]: url \"title\"\n    #   These have already been stripped in\n    #   _strip_link_definitions() so no need to watch for them.\n    # - not markup:         [...anything else...\n    try:\n        start_idx = text.index('[', curr_pos)\n    except ValueError:\n        break\n    text_length = len(text)\n\n    # Find the matching closing ']'.\n    # Markdown.pl allows *matching* brackets in link text so we\n    # will here too. Markdown.pl *doesn't* currently allow\n    # matching brackets in img alt text -- we'll differ in that\n    # regard.\n    bracket_depth = 0\n    for p in range(start_idx+1, min(start_idx+MAX_LINK_TEXT_SENTINEL, \n                                    text_length)):\n        ch = text[p]\n        if ch == ']':\n            bracket_depth -= 1\n            if bracket_depth < 0:\n                break\n        elif ch == '[':\n            bracket_depth += 1\n    else:\n        # Closing bracket not found within sentinel length.\n        # This isn't markup.\n        curr_pos = start_idx + 1\n        continue\n    link_text = text[start_idx+1:p]\n\n    # Possibly a footnote ref?\n    if \"footnotes\" in self.extras and link_text.startswith(\"^\"):\n        normed_id = re.sub(r'\\W', '-', link_text[1:])\n        if normed_id in self.footnotes:\n            self.footnote_ids.append(normed_id)\n            result = '<sup class=\"footnote-ref\" id=\"fnref-%s\">' \\\n                     '<a href=\"#fn-%s\">%s</a></sup>' \\\n                     % (normed_id, normed_id, len(self.footnote_ids))\n            text = text[:start_idx] + result + text[p+1:]\n        else:\n            # This id isn't defined, leave the markup alone.\n            curr_pos = p+1\n        continue\n\n    # Now determine what this is by the remainder.\n    p += 1\n    if p == text_length:\n        return text\n\n    # Inline anchor or img?\n    if text[p] == '(': # attempt at perf improvement\n        match = self._tail_of_inline_link_re.match(text, p)\n        if match:\n            # Handle an inline anchor or img.\n            is_img = start_idx > 0 and text[start_idx-1] == \"!\"\n            if is_img:\n                start_idx -= 1\n\n            url, title = match.group(\"url\"), match.group(\"title\")\n            if url and url[0] == '<':\n                url = url[1:-1]  # '<url>' -> 'url'\n            # We've got to encode these to avoid conflicting\n            # with italics/bold.\n            url = url.replace('*', g_escape_table['*']) \\\n                     .replace('_', g_escape_table['_'])\n            if title:\n                title_str = ' title=\"%s\"' \\\n                    % title.replace('*', g_escape_table['*']) \\\n                           .replace('_', g_escape_table['_']) \\\n                           .replace('\"', '&quot;')\n            else:\n                title_str = ''\n            if is_img:\n                result = '<img src=\"%s\" alt=\"%s\"%s%s' \\\n                    % (url, link_text.replace('\"', '&quot;'),\n                       title_str, self.empty_element_suffix)\n                curr_pos = start_idx + len(result)\n                text = text[:start_idx] + result + text[match.end():]\n            elif start_idx >= anchor_allowed_pos:\n                result_head = '<a href=\"%s\"%s>' % (url, title_str)\n                result = '%s%s</a>' % (result_head, link_text)\n                # <img> allowed from curr_pos on, <a> from\n                # anchor_allowed_pos on.\n                curr_pos = start_idx + len(result_head)\n                anchor_allowed_pos = start_idx + len(result)\n                text = text[:start_idx] + result + text[match.end():]\n            else:\n                # Anchor not allowed here.\n                curr_pos = start_idx + 1\n            continue\n\n    # Reference anchor or img?\n    else:\n        match = self._tail_of_reference_link_re.match(text, p)\n        if match:\n            # Handle a reference-style anchor or img.\n            is_img = start_idx > 0 and text[start_idx-1] == \"!\"\n            if is_img:\n                start_idx -= 1\n            link_id = match.group(\"id\").lower()\n            if not link_id:\n                link_id = link_text.lower()  # for links like [this][]\n            if link_id in self.urls:\n                url = self.urls[link_id]\n                # We've got to encode these to avoid conflicting\n                # with italics/bold.\n                url = url.replace('*', g_escape_table['*']) \\\n                         .replace('_', g_escape_table['_'])\n                title = self.titles.get(link_id)\n                if title:\n                    title = title.replace('*', g_escape_table['*']) \\\n                                 .replace('_', g_escape_table['_'])\n                    title_str = ' title=\"%s\"' % title\n                else:\n                    title_str = ''\n                if is_img:\n                    result = '<img src=\"%s\" alt=\"%s\"%s%s' \\\n                        % (url, link_text.replace('\"', '&quot;'),\n                           title_str, self.empty_element_suffix)\n                    curr_pos = start_idx + len(result)\n                    text = text[:start_idx] + result + text[match.end():]\n                elif start_idx >= anchor_allowed_pos:\n                    result = '<a href=\"%s\"%s>%s</a>' \\\n                        % (url, title_str, link_text)\n                    result_head = '<a href=\"%s\"%s>' % (url, title_str)\n                    result = '%s%s</a>' % (result_head, link_text)\n                    # <img> allowed from curr_pos on, <a> from\n                    # anchor_allowed_pos on.\n                    curr_pos = start_idx + len(result_head)\n                    anchor_allowed_pos = start_idx + len(result)\n                    text = text[:start_idx] + result + text[match.end():]\n                else:\n                    # Anchor not allowed here.\n                    curr_pos = start_idx + 1\n            else:\n                # This id isn't defined, leave the markup alone.\n                curr_pos = match.end()\n            continue\n\n    # Otherwise, it isn't markup.\n    curr_pos = start_idx + 1\n\nreturn text", "path": "markdown.py", "repo_name": "aruneinstein/groups", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "# Smart processing for ampersands and angle brackets that need\n# to be encoded.\n", "func_signal": "def _encode_amps_and_angles(self, text):\n", "code": "text = self._ampersand_re.sub('&amp;', text)\n    \n# Encode naked <'s\ntext = self._naked_lt_re.sub('&lt;', text)\n\n# Encode naked >'s\n# Note: Other markdown implementations (e.g. Markdown.pl, PHP\n# Markdown) don't do this.\ntext = self._naked_gt_re.sub('&gt;', text)\nreturn text", "path": "markdown.py", "repo_name": "aruneinstein/groups", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "\"\"\"Process Markdown `<pre><code>` blocks.\"\"\"\n", "func_signal": "def _do_code_blocks(self, text):\n", "code": "code_block_re = re.compile(r'''\n    (?:\\n\\n|\\A)\n    (               # $1 = the code block -- one or more lines, starting with a space/tab\n      (?:\n        (?:[ ]{%d} | \\t)  # Lines must start with a tab or a tab-width of spaces\n        .*\\n+\n      )+\n    )\n    ((?=^[ ]{0,%d}\\S)|\\Z)   # Lookahead for non-space at line-start, or end of doc\n    ''' % (self.tab_width, self.tab_width),\n    re.M | re.X)\n\nreturn code_block_re.sub(self._code_block_sub, text)", "path": "markdown.py", "repo_name": "aruneinstein/groups", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "# Used for safe_mode.\n\n", "func_signal": "def _hash_html_spans(self, text):\n", "code": "def _is_auto_link(s):\n    if ':' in s and self._auto_link_re.match(s):\n        return True\n    elif '@' in s and self._auto_email_link_re.match(s):\n        return True\n    return False\n\ntokens = []\nis_html_markup = False\nfor token in self._sorta_html_tokenize_re.split(text):\n    if is_html_markup and not _is_auto_link(token):\n        sanitized = self._sanitize_html(token)\n        key = _hash_text(sanitized)\n        self.html_spans[key] = sanitized\n        tokens.append(key)\n    else:\n        tokens.append(token)\n    is_html_markup = not is_html_markup\nreturn ''.join(tokens)", "path": "markdown.py", "repo_name": "aruneinstein/groups", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "\"\"\"call str or unicode depending on current mode\"\"\"\n", "func_signal": "def xstr(*args):\n", "code": "if _use_unicode:\n    return unicode(*args)\nelse:\n    return str(*args)", "path": "sympy\\printing\\pretty\\pretty_symbology.py", "repo_name": "jcockayne/sympy-rkern", "stars": 0, "license": "other", "language": "python", "size": 18016}
{"docstring": "# python2.4 unicodedata lacks some definitions\n\n", "func_signal": "def fixup_tables():\n", "code": "for d in sub, sup:\n    for k in d.keys():\n        if d[k] is None:\n            del d[k]", "path": "sympy\\printing\\pretty\\pretty_symbology.py", "repo_name": "jcockayne/sympy-rkern", "stars": 0, "license": "other", "language": "python", "size": 18016}
{"docstring": "\"\"\"Computes reduced Groebner basis for a set of polynomials.\n\n   Given a set of multivariate polynomials F, find another set G,\n   such that Ideal F = Ideal G and G is a reduced Groebner basis.\n\n   The resulting basis is unique and has monic generators.\n\n   Groebner bases can be used to choose specific generators for a\n   polynomial ideal. Because these bases are unique you can check\n   for ideal equality by comparing the Groebner bases.  To see if\n   one polynomial lies in an ideal, divide by the elements in the\n   base and see if the remainder vanishes.\n\n   They can also be used to  solve systems of polynomial equations\n   as,  by choosing lexicographic ordering,  you can eliminate one\n   variable at a time, provided that the ideal is zero-dimensional\n   (finite number of solutions).\n\n   >>> from sympy import *\n   >>> x,y = symbols('xy')\n\n   >>> G = poly_groebner([x**2 + y**3, y**2-x], x, y, order='lex')\n\n   >>> [ g.as_basic() for g in G ]\n   [x - y**2, y**3 + y**4]\n\n   For more information on the implemented algorithm refer to:\n\n   [1] N.K. Bose, B. Buchberger, J.P. Guiver, Multidimensional\n       Systems Theory and Applications, Springer, 2003, pp. 98+\n\n   [2] A. Giovini, T. Mora, \"One sugar cube, please\" or Selection\n       strategies in Buchberger algorithm, Proc. ISSAC '91, ACM\n\n   [3] I.A. Ajwa, Z. Liu, P.S. Wang, Groebner Bases Algorithm,\n       http://citeseer.ist.psu.edu/ajwa95grbner.html, 1995\n\n   [4] D. Cox, J. Little, D. O'Shea, Ideals, Varieties and\n       Algorithms, Springer, Second Edition, 1997, pp. 62\n\n\"\"\"\n", "func_signal": "def poly_groebner(f, *symbols, **flags):\n", "code": "if isinstance(f, (tuple, list, set)):\n    f, g = f[0], list(f[1:])\n\n    if not isinstance(f, Poly):\n        f = Poly(f, *symbols, **flags)\n    elif symbols or flags:\n        raise SymbolsError(\"Redundant symbols or flags were given\")\n\n    f, g = f.unify_with(g)\n\n    symbols, flags = f.symbols, f.flags\nelse:\n    if not isinstance(f, Poly):\n        f = Poly(f, *symbols, **flags)\n    elif symbols or flags:\n        raise SymbolsError(\"Redundant symbols or flags were given\")\n\n    return [f.as_monic()]\n\ncompare = monomial_cmp(flags.get('order'))\n\nf = [ h for h in [f] + g if h ]\n\nif not f:\n    return [Poly((), *symbols, **flags)]\n\nR, P, G, B, F = set(), set(), set(), {}, {}\n\nfor i, h in enumerate(f):\n    F[h] = i; R.add(i)\n\ndef normal(g, H):\n    h = poly_div(g, [ f[i] for i in H ])[1]\n\n    if h.is_zero:\n        return None\n    else:\n        if not F.has_key(h):\n            F[h] = len(f)\n            f.append(h)\n\n        return F[h], h.LM\n\ndef generate(R, P, G, B):\n    while R:\n        h = normal(f[R.pop()], G | P)\n\n        if h is not None:\n            k, LM = h\n\n            G0 = set(g for g in G if monomial_div(f[g].LM, LM))\n            P0 = set(p for p in P if monomial_div(f[p].LM, LM))\n\n            G, P, R = G - G0, P - P0 | set([k]), R | G0 | P0\n\n            for i, j in set(B):\n                if i in G0 or j in G0:\n                    del B[(i, j)]\n\n    G |= P\n\n    for i in G:\n        for j in P:\n            if i == j:\n                continue\n\n            if i < j:\n               k = (i, j)\n            else:\n               k = (j, i)\n\n            if not B.has_key(k):\n                B[k] = monomial_lcm(f[i].LM, f[j].LM)\n\n    G = set([ normal(f[g], G - set([g]))[0] for g in G ])\n\n    return R, P, G, B\n\nR, P, G, B = generate(R, P, G, B)\n\nwhile B:\n    k, M = B.items()[0]\n\n    for l, N in B.iteritems():\n        if compare(M, N) == 1:\n            k, M = l, N\n\n    del B[k]\n\n    i, j = k[0], k[1]\n    p, q = f[i], f[j]\n\n    p_LM, q_LM = p.LM, q.LM\n\n    if M == monomial_mul(p_LM, q_LM):\n        continue\n\n    criterion = False\n\n    for g in G:\n        if g == i or g == j:\n            continue\n\n        if not B.has_key((min(i, g), max(i, g))):\n            continue\n\n        if not B.has_key((min(j, g), max(j, g))):\n            continue\n\n        if not monomial_div(M, f[g].LM):\n            continue\n\n        criterion = True\n        break\n\n    if criterion:\n        continue\n\n    p = p.mul_term(1/p.LC, monomial_div(M, p_LM))\n    q = q.mul_term(1/q.LC, monomial_div(M, q_LM))\n\n    h = normal(p - q, G)\n\n    if h is not None:\n        k, LM = h\n\n        G0 = set(g for g in G if monomial_div(f[g].LM, LM))\n\n        R, P, G = G0, set([k]), G - G0\n\n        for i, j in set(B):\n            if i in G0 or j in G0:\n                del B[(i, j)]\n\n        R, P, G, B = generate(R, P, G, B)\n\nG = [ f[g].as_monic() for g in G ]\n\nG = sorted(G, compare, lambda p: p.LM)\n\nreturn list(reversed(G))", "path": "sympy\\polys\\algorithms.py", "repo_name": "jcockayne/sympy-rkern", "stars": 0, "license": "other", "language": "python", "size": 18016}
{"docstring": "\"\"\"Computes least common multiple of two polynomials.\n\n   Given two univariate polynomials,  the LCM is computed  via\n   f*g = gcd(f, g)*lcm(f, g) formula. In multivariate case, we\n   compute the unique generator of the intersection of the two\n   ideals, generated by f and g.  This is done by computing  a\n   Groebner basis, with respect to any lexicographic ordering,\n   of t*f and (1 - t)*g, where t is an unrelated symbol and\n   filtering out solution that does not contain t.\n\n   For more information on the implemented algorithm refer to:\n\n   [1] D. Cox, J. Little, D. O'Shea, Ideals, Varieties and\n       Algorithms, Springer, Second Edition, 1997, pp. 187\n\n\"\"\"\n", "func_signal": "def poly_lcm(f, g, *symbols):\n", "code": "if not isinstance(f, Poly):\n    f = Poly(f, *symbols)\nelif symbols:\n    raise SymbolsError(\"Redundant symbols were given\")\n\nf, g = f.unify_with(g)\n\nsymbols, flags = f.symbols, f.flags\n\nif f.is_monomial and g.is_monomial:\n    monom = monomial_lcm(f.LM, g.LM)\n\n    fc, gc = f.LC, g.LC\n\n    if fc.is_Rational and gc.is_Rational:\n        coeff = Integer(ilcm(fc.p, gc.p))\n    else:\n        coeff = S.One\n\n    return Poly((coeff, monom), *symbols, **flags)\n\nfc, f = f.as_primitive()\ngc, g = g.as_primitive()\n\nlcm = ilcm(int(fc), int(gc))\n\nif f.is_multivariate:\n    t = Symbol('t', dummy=True)\n    lex = { 'order' : 'lex' }\n\n    f_monoms = [ (1,) + monom for monom in f.monoms ]\n\n    F = Poly((f.coeffs, f_monoms), t, *symbols, **lex)\n\n    g_monoms = [ (0,) + monom for monom in g.monoms ] + \\\n               [ (1,) + monom for monom in g.monoms ]\n\n    g_coeffs = list(g.coeffs) + [ -coeff for coeff in g.coeffs ]\n    G = Poly(dict(zip(g_monoms, g_coeffs)), t, *symbols, **lex)\n\n    def independent(h):\n        return all(not monom[0] for monom in h.monoms)\n\n    H = [ h for h in poly_groebner((F, G)) if independent(h) ]\n\n    if lcm != 1:\n        h_coeffs = [ coeff*lcm for coeff in H[0].coeffs ]\n    else:\n        h_coeffs = H[0].coeffs\n\n    h_monoms = [ monom[1:] for monom in H[0].monoms ]\n\n    return Poly(dict(zip(h_monoms, h_coeffs)), *symbols, **flags)\nelse:\n    h = poly_div(f * g, poly_gcd(f, g))[0]\n\n    if lcm != 1:\n        return h.mul_term(lcm / h.LC)\n    else:\n        return h.as_monic()", "path": "sympy\\polys\\algorithms.py", "repo_name": "jcockayne/sympy-rkern", "stars": 0, "license": "other", "language": "python", "size": 18016}
{"docstring": "\"\"\"\nReturns the current modelview matrix.\n\"\"\"\n", "func_signal": "def get_projection_matrix(array_type=c_float, glGetMethod=glGetFloatv):\n", "code": "m = (array_type*16)()\nglGetMethod(GL_PROJECTION_MATRIX, m)\nreturn m", "path": "sympy\\plotting\\util.py", "repo_name": "jcockayne/sympy-rkern", "stars": 0, "license": "other", "language": "python", "size": 18016}
{"docstring": "\"\"\"\nReturns the current modelview matrix.\n\"\"\"\n", "func_signal": "def get_model_matrix(array_type=c_float, glGetMethod=glGetFloatv):\n", "code": "m = (array_type*16)()\nglGetMethod(GL_MODELVIEW_MATRIX, m)\nreturn m", "path": "sympy\\plotting\\util.py", "repo_name": "jcockayne/sympy-rkern", "stars": 0, "license": "other", "language": "python", "size": 18016}
{"docstring": "\"\"\"See if unicode output is available and leverage it if possible\"\"\"\n\n", "func_signal": "def pretty_try_use_unicode():\n", "code": "try:\n    symbols = []\n\n    # see, if we can represent greek alphabet\n    for g,G in greek.itervalues():\n        symbols.append(g)\n        symbols.append(G)\n\n    # and atoms\n    symbols += atoms_table.values()\n\n    for s in symbols:\n        if s is None:\n            return  # common symbols not present!\n\n        encoding = getattr(sys.stdout, 'encoding', None)\n\n        # this happens when e.g. stdout is redirected through a pipe, or is\n        # e.g. a cStringIO.StringO\n        if encoding is None:\n            return  # sys.stdout has no encoding\n\n        # try to encode\n        s.encode(encoding)\n\nexcept UnicodeEncodeError:\n    pass\nelse:\n    pretty_use_unicode(True)", "path": "sympy\\printing\\pretty\\pretty_symbology.py", "repo_name": "jcockayne/sympy-rkern", "stars": 0, "license": "other", "language": "python", "size": 18016}
{"docstring": "\"\"\"Half extended Euclidean algorithm.\n\n   Efficiently computes gcd(f, g)  and one of the coefficients\n   in extended Euclidean algorithm. Formally, given univariate\n   polynomials f and g over an Euclidean domain, computes s\n   and h, such that h = gcd(f, g) and s*f = h (mod g).\n\n   For more information on the implemented algorithm refer to:\n\n   [1] M. Bronstein, Symbolic Integration I: Transcendental\n       Functions, Second Edition, Springer-Verlang, 2005\n\n\"\"\"\n", "func_signal": "def poly_half_gcdex(f, g, *symbols):\n", "code": "if not isinstance(f, Poly):\n    f = Poly(f, *symbols)\nelif symbols:\n    raise SymbolsError(\"Redundant symbols were given\")\n\nf, g = f.unify_with(g)\n\nif f.is_multivariate:\n    raise MultivariatePolyError(f)\n\nsymbols, flags = f.symbols, f.flags\n\na = Poly(S.One, *symbols, **flags)\nb = Poly((), *symbols, **flags)\n\nwhile not g.is_zero:\n    q, r = poly_div(f, g)\n\n    f, g = g, r\n    c = a - q*b\n    a, b = b, c\n\nreturn a, f", "path": "sympy\\polys\\algorithms.py", "repo_name": "jcockayne/sympy-rkern", "stars": 0, "license": "other", "language": "python", "size": 18016}
{"docstring": "\"\"\"Construct spatial object of given length.\n\nreturn: [] of equal-length strings\n\"\"\"\n\n", "func_signal": "def xobj(symb, length):\n", "code": "assert length > 0\n\n# TODO robustify when no unicodedat available\nif _use_unicode:\n    _xobj = _xobj_unicode\nelse:\n    _xobj = _xobj_ascii\n\nvinfo = _xobj[symb]\n\nc1 = top = bot = mid = None\n\nif not isinstance(vinfo, tuple):        # 1 entry\n    ext = vinfo\nelse:\n    if isinstance(vinfo[0], tuple):     # (vlong), c1\n        vlong = vinfo[0]\n        c1    = vinfo[1]\n    else:                               # (vlong), c1\n        vlong = vinfo\n\n    ext = vlong[0]\n\n    try:\n        top = vlong[1]\n        bot = vlong[2]\n        mid = vlong[3]\n    except IndexError:\n        pass\n\nif c1  is None:  c1  = ext\nif top is None:  top = ext\nif bot is None:  bot = ext\nif mid is not None:\n    if (length % 2) == 0:\n        # even height, but we have to print it somehow anyway...\n        # XXX is it ok?\n        length += 1\n\nelse:\n    mid = ext\n\nif length == 1:\n    return c1\n\n\nres = []\nnext= (length-2)//2\nnmid= (length-2) - next*2\n\nres += [top]\nres += [ext]*next\nres += [mid]*nmid\nres += [ext]*next\nres += [bot]\n\nreturn res", "path": "sympy\\printing\\pretty\\pretty_symbology.py", "repo_name": "jcockayne/sympy-rkern", "stars": 0, "license": "other", "language": "python", "size": 18016}
{"docstring": "\"\"\"get symbology for a 'character'\"\"\"\n", "func_signal": "def xsym(sym):\n", "code": "op = _xsym[sym]\n\nif _use_unicode:\n    return op[1]\nelse:\n    return op[0]", "path": "sympy\\printing\\pretty\\pretty_symbology.py", "repo_name": "jcockayne/sympy-rkern", "stars": 0, "license": "other", "language": "python", "size": 18016}
{"docstring": "\"\"\"Solves a system of polynomial equations.\n\n   Returns all possible solutions over C[x_1, x_2, ..., x_m] of a\n   set F = { f_1, f_2, ..., f_n } of polynomial equations,  using\n   Groebner basis approach. For now only zero-dimensional systems\n   are supported, which means F can have at most a finite number\n   of solutions.\n\n   The algorithm works by the fact that, supposing G is the basis\n   of F with respect to an elimination order  (here lexicographic\n   order is used), G and F generate the same ideal, they have the\n   same set of solutions. By the elimination property,  if G is a\n   reduced, zero-dimensional Groebner basis, then there exists an\n   univariate polynomial in G (in its last variable). This can be\n   solved by computing its roots. Substituting all computed roots\n   for the last (eliminated) variable in other elements of G, new\n   polynomial system is generated. Applying the above procedure\n   recursively, a finite number of solutions can be found.\n\n   The ability of finding all solutions by this procedure depends\n   on the root finding algorithms. If no solutions were found, it\n   means only that roots() failed, but the system is solvable. To\n   overcome this difficulty use numerical algorithms instead.\n\n   >>> from sympy import *\n   >>> x,y = symbols('xy')\n\n   >>> solve_poly_system([x*y - 2*y, 2*y**2 - x**2], x, y)\n   [(0, 0), (2, -2**(1/2)), (2, 2**(1/2))]\n\n   For more information on the implemented algorithm refer to:\n\n   [1] B. Buchberger, Groebner Bases: A Short Introduction for\n       Systems Theorists,  In: R. Moreno-Diaz,  B. Buchberger,\n       J.L. Freire, Proceedings of EUROCAST'01, February, 2001\n\n   [2] D. Cox, J. Little, D. O'Shea, Ideals, Varieties and\n       Algorithms, Springer, Second Edition, 1997, pp. 112\n\n\"\"\"\n", "func_signal": "def solve_poly_system(system, *symbols):\n", "code": "def is_univariate(f):\n    \"\"\"Returns True if 'f' is univariate in its last variable. \"\"\"\n    for monom in f.iter_monoms():\n        if any(exp > 0 for exp in monom[:-1]):\n            return False\n\n    return True\n\ndef solve_reduced_system(system, entry=False):\n    \"\"\"Recursively solves reduced polynomial systems. \"\"\"\n    basis = poly_groebner(system)\n\n    if len(basis) == 1 and basis[0].is_one:\n        if not entry:\n            return []\n        else:\n            return None\n\n    univariate = filter(is_univariate, basis)\n\n    if len(univariate) == 1:\n        f = univariate.pop()\n    else:\n        raise PolynomialError(\"Not a zero-dimensional system\")\n\n    zeros = roots(Poly(f, f.symbols[-1])).keys()\n\n    if not zeros:\n        return []\n\n    if len(basis) == 1:\n        return [ [zero] for zero in zeros ]\n\n    solutions = []\n\n    for zero in zeros:\n        new_system = []\n\n        for poly in basis[:-1]:\n            eq = poly.evaluate((poly.symbols[-1], zero))\n\n            if not eq.is_zero:\n                new_system.append(eq)\n\n        for solution in solve_reduced_system(new_system):\n            solutions.append(solution + [zero])\n\n    return solutions\n\nif hasattr(system, \"__iter__\"):\n    system = list(system)\nelse:\n    raise TypeError(\"Expected iterable container, got %s\" % system)\n\nf = system.pop(0)\n\nif not isinstance(f, Poly):\n    f = LexPoly(f, *symbols)\nelse:\n    if not symbols:\n        f = LexPoly(f)\n    else:\n        raise SymbolsError(\"Redundant symbols were given\")\n\nhead, tail = f.unify_with(system)\n\nsolutions = solve_reduced_system([head] + tail, True)\n\nif solutions is None:\n    return None\nelse:\n    return sorted(tuple(s) for s in solutions)", "path": "sympy\\solvers\\polysys.py", "repo_name": "jcockayne/sympy-rkern", "stars": 0, "license": "other", "language": "python", "size": 18016}
{"docstring": "\"\"\"Computes functional decomposition of an univariate polynomial.\n\n   Besides factorization and square-free decomposition, functional\n   decomposition is another important, but very different,  way of\n   breaking down polynomials into simpler parts.\n\n   Formally given an univariate polynomial f with coefficients in a\n   field of characteristic zero, returns tuple (f_1, f_2, ..., f_n)\n   where f = f_1 o f_2 o ... f_n = f_1(f_2(... f_n)) and f_2, ...,\n   f_n are monic and homogeneous polynomials of degree at least 2.\n\n   Unlike factorization, complete functional decompositions of\n   polynomials are not unique, consider examples:\n\n    [1] f o g = f(x + b) o (g - b)\n    [2] x**n o x**m = x**m o x**n\n    [3] T_n o T_m = T_m o T_n\n\n   where T_n and T_m are Chebyshev polynomials.\n\n   >>> from sympy import *\n   >>> x,y = symbols('xy')\n\n   >>> p, q = poly_decompose(x**4+2*x**2 + y, x)\n\n   >>> p.as_basic()\n   y + 2*x + x**2\n   >>> q.as_basic()\n   x**2\n\n   For more information on the implemented algorithm refer to:\n\n   [1] D. Kozen, S. Landau, Polynomial decomposition algorithms,\n       Journal of Symbolic Computation 7 (1989), pp. 445-456\n\n\"\"\"\n", "func_signal": "def poly_decompose(f, *symbols):\n", "code": "if not isinstance(f, Poly):\n    f = Poly(f, *symbols)\nelif symbols:\n    raise SymbolsError(\"Redundant symbols were given\")\n\nif f.is_multivariate:\n    raise MultivariatePolyError(f)\n\nsymbols = f.symbols\nflags = f.flags\n\ndef right_factor(f, s):\n    n, lc = f.degree, f.LC\n\n    f = f.as_uv_dict()\n    q = { s : S.One }\n\n    r = n // s\n\n    for k in xrange(1, s):\n        coeff = S.Zero\n\n        for j in xrange(0, k):\n            if not f.has_key(n+j-k):\n                continue\n\n            if not q.has_key(s-j):\n                continue\n\n            fc, qc = f[n+j-k], q[s-j]\n\n            coeff += (k - r*j)*fc*qc\n\n        if coeff is not S.Zero:\n            q[s-k] = coeff / (k*r*lc)\n\n    return Poly(q, *symbols, **flags)\n\ndef left_factor(f, h):\n    g, i = {}, 0\n\n    while not f.is_zero:\n        q, r = poly_div(f, h)\n\n        if not r.is_constant:\n            return None\n        else:\n            if r.LC is not S.Zero:\n                g[i] = r.LC\n\n            f, i = q, i + 1\n\n    return Poly(g, *symbols, **flags)\n\ndef decompose(f):\n    deg = f.degree\n\n    for s in xrange(2, deg):\n        if deg % s != 0:\n            continue\n\n        h = right_factor(f, s)\n\n        if h is not None:\n            g = left_factor(f, h)\n\n            if g is not None:\n                return (g, h)\n\n    return None\n\nF = []\n\nwhile True:\n    result = decompose(f)\n\n    if result is not None:\n        f, h = result\n        F = [h] + F\n    else:\n        break\n\nreturn [f] + F", "path": "sympy\\polys\\algorithms.py", "repo_name": "jcockayne/sympy-rkern", "stars": 0, "license": "other", "language": "python", "size": 18016}
{"docstring": "\"\"\"return pretty representation of a symbol\"\"\"\n# let's split symb_name into symbol + index\n# UC: beta1\n# UC: f_beta\n\n", "func_signal": "def pretty_symbol(symb_name):\n", "code": "if not _use_unicode:\n    return symb_name\n\n    #          name       ^        sup     _    sub or digit\nm = re.match('([^^_\\d]+)(\\^)?((?(2)[^_]+))(_)?((?(4).+|\\d*))$', symb_name)\nif m is None:\n    #print 'DON\\'T MATCH'\n    return symb_name\n\n#print m.groups()\nname, ssup, isup, ssub, isub = m.groups('')\n\n# let's prettify name\ngG = greek.get(name.lower())\nif gG is not None:\n    greek_name = name.islower() and gG[0] or gG[1]\n\n    # some lettrs may not be available\n    if greek_name is not None:\n        name = greek_name\n\n# let's pretty sup/sub\npsup = sup.get(isup)    # exact match\nif psup is None:\n    try:                # match by separate characters\n        psup = ''.join( [sup[c] for c in isup] )\n    except KeyError:\n        pass\n\npsub = sub.get(isub)\nif psub is None:\n    try:\n        psub = ''.join( [sub[c] for c in isub] )\n    except KeyError:\n        pass\n\nres = name\nif psup is not None:\n    res += psup\nelse:\n    res += '%s%s' % (ssup, isup)\n\nif psub is not None:\n    res += psub\nelse:\n    res += '%s%s' % (ssub, isub)\n\nreturn res", "path": "sympy\\printing\\pretty\\pretty_symbology.py", "repo_name": "jcockayne/sympy-rkern", "stars": 0, "license": "other", "language": "python", "size": 18016}
{"docstring": "\"\"\"Computes subresultant PRS of two univariate polynomials.\n\n   Polynomial remainder sequence (PRS) is a fundamental tool in\n   computer algebra as it gives as a sub-product the polynomial\n   greatest common divisor (GCD), provided that the coefficient\n   domain is an unique factorization domain.\n\n   There are several methods for computing PRS, eg.: Euclidean\n   PRS, where the most famous algorithm is used, primitive PRS\n   and, finally, subresultants which are implemented here.\n\n   The Euclidean approach is reasonably efficient but suffers\n   severely from coefficient growth.  The primitive algorithm\n   avoids this but requires a lot of coefficient computations.\n\n   Subresultants solve both problems and so it is efficient and\n   have moderate coefficient growth. The current implementation\n   uses pseudo-divisions  which is well suited for coefficients\n   in integral domains or number fields.\n\n   Formally,  given univariate polynomials f and g over an UFD,\n   then a sequence (R_0, R_1, ..., R_k, 0, ...) is a polynomial\n   remainder sequence where R_0 = f, R_1 = g, R_k != 0 and R_k\n   is similar to gcd(f, g).\n\n   For more information on the implemented algorithm refer to:\n\n   [1] M. Bronstein, Symbolic Integration I: Transcendental\n       Functions, Second Edition, Springer-Verlang, 2005\n\n   [2] M. Keber, Division-Free computation of subresultants\n       using Bezout matrices, Tech. Report MPI-I-2006-1-006,\n       Saarbrucken, 2006\n\n\"\"\"\n", "func_signal": "def poly_subresultants(f, g, *symbols):\n", "code": "if not isinstance(f, Poly):\n    f = Poly(f, *symbols)\nelif symbols:\n    raise SymbolsError(\"Redundant symbols were given\")\n\nf, g = f.unify_with(g)\n\nif f.is_multivariate:\n    raise MultivariatePolyError(f)\n\nsymbols, flags = f.symbols, f.flags\n\nn, m = f.degree, g.degree\n\nif n < m:\n    f, g = g, f\n    n, m = m, n\n\nprs = [f, g]\n\nd = n - m\n\nb = (-1)**(d + 1)\n\nh = poly_pdiv(f, g)[1]\nh = h.mul_term(b)\n\nk = h.degree\n\nc = S.NegativeOne\n\nwhile not h.is_zero:\n    prs.append(h)\n\n    coeff = g.LC\n\n    c = (-coeff)**d / c**(d-1)\n\n    b = -coeff * c**(m-k)\n\n    f, g, m, d = g, h, k, m-k\n\n    h = poly_pdiv(f, g)[1]\n    h = h.div_term(b)\n\n    k = h.degree\n\nreturn prs", "path": "sympy\\polys\\algorithms.py", "repo_name": "jcockayne/sympy-rkern", "stars": 0, "license": "other", "language": "python", "size": 18016}
{"docstring": "\"\"\"Computes resultant of two univariate polynomials.\n\n   Resultants are a classical algebraic tool for determining if\n   a  system of n polynomials in n-1 variables have common root\n   without explicitly solving for the roots.\n\n   They are efficiently represented as  determinants of Bezout\n   matrices whose entries are computed using O(n**2) additions\n   and multiplications where n = max(deg(f), deg(g)).\n\n   >>> from sympy import *\n   >>> x,y = symbols('xy')\n\n   Polynomials x**2-1 and (x-1)**2 have common root:\n\n   >>> poly_resultant(x**2-1, (x-1)**2, x)\n   0\n\n   For more information on the implemented algorithm refer to:\n\n   [1] Eng-Wee Chionh, Fast Computation of the Bezout and Dixon\n       Resultant Matrices, Journal of Symbolic Computation, ACM,\n       Volume 33, Issue 1, January 2002, Pages 13-29\n\n\"\"\"\n", "func_signal": "def poly_resultant(f, g, *symbols):\n", "code": "if not isinstance(f, Poly):\n    f = Poly(f, *symbols)\nelif symbols:\n    raise SymbolsError(\"Redundant symbols were given\")\n\nf, g = f.unify_with(g)\n\nif f.is_multivariate:\n    raise MultivariatePolyError(f)\n\nn, m = f.degree, g.degree\n\nN = max(n, m)\n\nif n < m:\n    p = f.as_uv_dict()\n    q = g.as_uv_dict()\nelse:\n    q = f.as_uv_dict()\n    p = g.as_uv_dict()\n\nimport sympy.matrices\n\nB = sympy.matrices.zeros(N)\n\nfor i in xrange(N):\n    for j in xrange(i, N):\n        if p.has_key(i) and q.has_key(j+1):\n            B[i, j] += p[i] * q[j+1]\n\n        if p.has_key(j+1) and q.has_key(i):\n            B[i, j] -= p[j+1] * q[i]\n\nfor i in xrange(1, N-1):\n    for j in xrange(i, N-1):\n        B[i, j] += B[i-1, j+1]\n\nfor i in xrange(N):\n    for j in xrange(i+1, N):\n        B[j, i] = B[i, j]\n\ndet = B.det()\n\nif not det:\n    return det\nelse:\n    if n >= m:\n        det /= f.LC**(n-m)\n    else:\n        det /= g.LC**(m-n)\n\n    sign = (-1)**(n*(n-1)//2)\n\n    if det.is_Atom:\n        return sign * det\n    else:\n        return sign * Poly.cancel(det)", "path": "sympy\\polys\\algorithms.py", "repo_name": "jcockayne/sympy-rkern", "stars": 0, "license": "other", "language": "python", "size": 18016}
{"docstring": "\"\"\"\nReturns the current viewport.\n\"\"\"\n", "func_signal": "def get_viewport():\n", "code": "m = (c_int*4)()\nglGetIntegerv(GL_VIEWPORT, m)\nreturn m", "path": "sympy\\plotting\\util.py", "repo_name": "jcockayne/sympy-rkern", "stars": 0, "license": "other", "language": "python", "size": 18016}
{"docstring": "\"\"\"Compute square-free decomposition of an univariate polynomial.\n\n   Given an univariate polynomial f over an unique factorization domain\n   returns tuple (f_1, f_2, ..., f_n),  where all  A_i are co-prime and\n   square-free polynomials and f = f_1 * f_2**2 * ... * f_n**n.\n\n   >>> from sympy import *\n   >>> x,y = symbols('xy')\n\n   >>> p, q = poly_sqf(x*(x+1)**2, x)\n\n   >>> p.as_basic()\n   x\n   >>> q.as_basic()\n   1 + x\n\n   For more information on the implemented algorithm refer to:\n\n   [1] M. Bronstein, Symbolic Integration I: Transcendental\n       Functions, Second Edition, Springer-Verlang, 2005\n\n   [2] J. von zur Gathen, J. Gerhard, Modern Computer Algebra,\n       Second Edition, Cambridge University Press, 2003\n\n\"\"\"\n", "func_signal": "def poly_sqf(f, *symbols):\n", "code": "if not isinstance(f, Poly):\n    f = Poly(f, *symbols)\nelif symbols:\n    raise SymbolsError(\"Redundant symbols were given\")\n\nif f.is_multivariate:\n    raise MultivariatePolyError(f)\n\ncoeff, f = f.as_primitive()\n\nsqf = []\n\nh = f.diff()\n\ng = poly_gcd(f, h)\n\np = poly_div(f, g)[0]\nq = poly_div(h, g)[0]\n\nwhile True:\n    h = q - p.diff()\n\n    if h.is_zero:\n        break\n\n    g = poly_gcd(p, h)\n\n    sqf.append(g)\n\n    p = poly_div(p, g)[0]\n    q = poly_div(h, g)[0]\n\nsqf.append(p)\n\nhead, tail = sqf[0], sqf[1:]\nhead = head.mul_term(coeff)\n\nreturn [head] + tail", "path": "sympy\\polys\\algorithms.py", "repo_name": "jcockayne/sympy-rkern", "stars": 0, "license": "other", "language": "python", "size": 18016}
{"docstring": "\"\"\"unicode character by name or None if not found\"\"\"\n", "func_signal": "def U(name):\n", "code": "try:\n    u = unicodedata.lookup(name)\nexcept KeyError:\n    u = unicodedata_missing.get(name)\n\n    if u is None:\n        global warnings\n        warnings += 'W: no \\'%s\\' in unocodedata\\n' % name\n\nreturn u", "path": "sympy\\printing\\pretty\\pretty_symbology.py", "repo_name": "jcockayne/sympy-rkern", "stars": 0, "license": "other", "language": "python", "size": 18016}
{"docstring": "\"\"\"Extended Euclidean algorithm.\n\n   Given univariate polynomials f and g over an Euclidean domain,\n   computes polynomials s, t and h,  such that h = gcd(f, g) and\n   s*f + t*g = h.\n\n   For more information on the implemented algorithm refer to:\n\n   [1] M. Bronstein, Symbolic Integration I: Transcendental\n       Functions, Second Edition, Springer-Verlang, 2005\n\n\"\"\"\n", "func_signal": "def poly_gcdex(f, g, *symbols):\n", "code": "s, h = poly_half_gcdex(f, g, *symbols)\nreturn s, poly_div(h - s*f, g)[0], h", "path": "sympy\\polys\\algorithms.py", "repo_name": "jcockayne/sympy-rkern", "stars": 0, "license": "other", "language": "python", "size": 18016}
{"docstring": "\"\"\"Generalized polynomial division with remainder.\n\n   Given polynomial f and a set of polynomials g = (g_1, ..., g_n)\n   compute a set of quotients q = (q_1, ..., q_n) and remainder r\n   such that f = q_1*f_1 + ... + q_n*f_n + r, where r = 0 or r is\n   a completely reduced polynomial with respect to g.\n\n   In particular g can be a tuple, list or a singleton. All g_i\n   and f can be given as Poly class instances or as expressions.\n\n   For more information on the implemented algorithm refer to:\n\n   [1] D. Cox, J. Little, D. O'Shea, Ideals, Varieties and\n       Algorithms, Springer, Second Edition, 1997, pp. 62\n\n   [2] I.A. Ajwa, Z. Liu, P.S. Wang, Groebner Bases Algorithm,\n       http://citeseer.ist.psu.edu/ajwa95grbner.html, 1995\n\n\"\"\"\n", "func_signal": "def poly_div(f, g, *symbols):\n", "code": "if not isinstance(f, Poly):\n    f = Poly(f, *symbols)\nelif symbols:\n    raise SymbolsError(\"Redundant symbols were given\")\n\nf, g = f.unify_with(g)\n\nsymbols, flags = f.symbols, f.flags\n\nr = Poly((), *symbols, **flags)\n\nif isinstance(g, Basic):\n    if g.is_constant:\n        if g.is_zero:\n            raise ZeroDivisionError\n        elif g.is_one:\n            return f, r\n        else:\n            return f.div_term(g.LC), r\n\n    if g.is_monomial:\n        LC, LM = g.lead_term\n\n        q_coeffs, q_monoms = [], []\n        r_coeffs, r_monoms = [], []\n\n        for coeff, monom in f.iter_terms():\n            quotient = monomial_div(monom, LM)\n\n            if quotient is not None:\n                coeff /= LC\n\n                q_coeffs.append(Poly.cancel(coeff))\n                q_monoms.append(quotient)\n            else:\n                r_coeffs.append(coeff)\n                r_monoms.append(monom)\n\n        return (Poly((q_coeffs, q_monoms), *symbols, **flags),\n                Poly((r_coeffs, r_monoms), *symbols, **flags))\n\n    g, q = [g], [r]\nelse:\n    q = [r] * len(g)\n\nwhile not f.is_zero:\n    for i, h in enumerate(g):\n        monom = monomial_div(f.LM, h.LM)\n\n        if monom is not None:\n            coeff = Poly.cancel(f.LC / h.LC)\n\n            q[i] = q[i].add_term(coeff, monom)\n            f -= h.mul_term(coeff, monom)\n\n            break\n    else:\n        r = r.add_term(*f.LT)\n        f = f.kill_lead_term()\n\nif len(q) != 1:\n    return q, r\nelse:\n    return q[0], r", "path": "sympy\\polys\\algorithms.py", "repo_name": "jcockayne/sympy-rkern", "stars": 0, "license": "other", "language": "python", "size": 18016}
{"docstring": "\"\"\"Produce status, headers, body for a critical error.\n\nReturns a triple without calling any other questionable functions,\nso it should be as error-free as possible. Call it from an HTTP server\nif you get errors outside of the request.\n\nIf extrabody is None, a friendly but rather unhelpful error message\nis set in the body. If extrabody is a string, it will be appended\nas-is to the body.\n\"\"\"\n\n# The whole point of this function is to be a last line-of-defense\n# in handling errors. That is, it must not raise any errors itself;\n# it cannot be allowed to fail. Therefore, don't add to it!\n# In particular, don't call any other CP functions.\n\n", "func_signal": "def bare_error(extrabody=None):\n", "code": "body = \"Unrecoverable error in the server.\"\nif extrabody is not None:\n    body += \"\\n\" + extrabody\n\nreturn (\"500 Internal Server Error\",\n        [('Content-Type', 'text/plain'),\n         ('Content-Length', str(len(body)))],\n        [body])", "path": "cherrypy\\_cperror.py", "repo_name": "jonathanduty/fbfoodie", "stars": 1, "license": "None", "language": "python", "size": 900}
{"docstring": "# Test with no header provided\n", "func_signal": "def test_Accept_Tool(self):\n", "code": "self.getPage('/accept/feed')\nself.assertStatus(200)\nself.assertInBody('<title>Unknown Blog</title>')\n\n# Specify exact media type\nself.getPage('/accept/feed', headers=[('Accept', 'application/atom+xml')])\nself.assertStatus(200)\nself.assertInBody('<title>Unknown Blog</title>')\n\n# Specify matching media range\nself.getPage('/accept/feed', headers=[('Accept', 'application/*')])\nself.assertStatus(200)\nself.assertInBody('<title>Unknown Blog</title>')\n\n# Specify all media ranges\nself.getPage('/accept/feed', headers=[('Accept', '*/*')])\nself.assertStatus(200)\nself.assertInBody('<title>Unknown Blog</title>')\n\n# Specify unacceptable media types\nself.getPage('/accept/feed', headers=[('Accept', 'text/html')])\nself.assertErrorPage(406,\n                     \"Your client sent this Accept header: text/html. \"\n                     \"But this resource only emits these media types: \"\n                     \"application/atom+xml.\")\n\n# Test resource where tool is 'on' but media is None (not set).\nself.getPage('/accept/')\nself.assertStatus(200)\nself.assertBody('<a href=\"feed\">Atom feed</a>')", "path": "cherrypy\\test\\test_misc_tools.py", "repo_name": "jonathanduty/fbfoodie", "stars": 1, "license": "None", "language": "python", "size": 900}
{"docstring": "# Try both our expected media types\n", "func_signal": "def test_accept_selection(self):\n", "code": "self.getPage('/accept/select', [('Accept', 'text/html')])\nself.assertStatus(200)\nself.assertBody('<h2>Page Title</h2>')\nself.getPage('/accept/select', [('Accept', 'text/plain')])\nself.assertStatus(200)\nself.assertBody('PAGE TITLE')\nself.getPage('/accept/select', [('Accept', 'text/plain, text/*;q=0.5')])\nself.assertStatus(200)\nself.assertBody('PAGE TITLE')\n\n# text/* and */* should prefer text/html since it comes first\n# in our 'media' argument to tools.accept\nself.getPage('/accept/select', [('Accept', 'text/*')])\nself.assertStatus(200)\nself.assertBody('<h2>Page Title</h2>')\nself.getPage('/accept/select', [('Accept', '*/*')])\nself.assertStatus(200)\nself.assertBody('<h2>Page Title</h2>')\n\n# Try unacceptable media types\nself.getPage('/accept/select', [('Accept', 'application/xml')])\nself.assertErrorPage(406,\n                     \"Your client sent this Accept header: application/xml. \"\n                     \"But this resource only emits these media types: \"\n                     \"text/html, text/plain.\")", "path": "cherrypy\\test\\test_misc_tools.py", "repo_name": "jonathanduty/fbfoodie", "stars": 1, "license": "None", "language": "python", "size": 900}
{"docstring": "# We could also write this: mtype = cherrypy.lib.accept.accept(...)\n", "func_signal": "def select(self):\n", "code": "mtype = tools.accept.callable(['text/html', 'text/plain'])\nif mtype == 'text/html':\n    return \"<h2>Page Title</h2>\"\nelse:\n    return \"PAGE TITLE\"", "path": "cherrypy\\test\\test_misc_tools.py", "repo_name": "jonathanduty/fbfoodie", "stars": 1, "license": "None", "language": "python", "size": 900}
{"docstring": "# Delete the specified contact\n", "func_signal": "def delete(self, id):\n", "code": "contact = Contact.get(int(id))\ncontact.destroySelf()\nreturn 'Deleted. <a href=\"./\">Return to Index</a>'", "path": "cherrypy\\tutorial\\bonus-sqlobject.py", "repo_name": "jonathanduty/fbfoodie", "stars": 1, "license": "None", "language": "python", "size": 900}
{"docstring": "# Test that NotFound will then try dynamic handlers (see [878]).\n", "func_signal": "def test_fallthrough(self):\n", "code": "self.getPage(\"/static/dynamic\")\nself.assertBody(\"This is a DYNAMIC page\")\n\n# Check a directory via fall-through to dynamic handler.\nself.getPage(\"/static/\")\nself.assertStatus('200 OK')\nself.assertHeader('Content-Type', 'text/html')\nself.assertBody('You want the Baron? You can have the Baron!')", "path": "cherrypy\\test\\test_static.py", "repo_name": "jonathanduty/fbfoodie", "stars": 1, "license": "None", "language": "python", "size": 900}
{"docstring": "\"\"\"Modify cherrypy.response status, headers, and body to represent self.\n\nCherryPy uses this internally, but you can also use it to create an\nHTTPError object and set its output without *raising* the exception.\n\"\"\"\n", "func_signal": "def set_response(self):\n", "code": "import cherrypy\n\nresponse = cherrypy.response\n\nclean_headers(self.status)\n\n# In all cases, finalize will be called after this method,\n# so don't bother cleaning up response values here.\nresponse.status = self.status\ntb = None\nif cherrypy.request.show_tracebacks:\n    tb = format_exc()\nresponse.headers['Content-Type'] = \"text/html\"\n\ncontent = self.get_error_page(self.status, traceback=tb,\n                              message=self._message)\nresponse.body = content\nresponse.headers['Content-Length'] = len(content)\n\n_be_ie_unfriendly(self.status)", "path": "cherrypy\\_cperror.py", "repo_name": "jonathanduty/fbfoodie", "stars": 1, "license": "None", "language": "python", "size": 900}
{"docstring": "# Drop existing table\n", "func_signal": "def reset(self):\n", "code": "Contact.dropTable(True)\n\n# Create new table\nContact.createTable()\n\n# Create some sample data\nContact(\n    firstName = 'Hendrik',\n    lastName = 'Mans',\n    email = 'hendrik@mans.de',\n    phone = '++49 89 12345678',\n    url = 'http://www.mornography.de')\n\nreturn \"reset completed!\"", "path": "cherrypy\\tutorial\\bonus-sqlobject.py", "repo_name": "jonathanduty/fbfoodie", "stars": 1, "license": "None", "language": "python", "size": 900}
{"docstring": "\"\"\"Return a Python object compiled from a string.\"\"\"\n", "func_signal": "def unrepr(s):\n", "code": "if not s:\n    return s\n\ntry:\n    import compiler\nexcept ImportError:\n    # Fallback to eval when compiler package is not available,\n    # e.g. IronPython 1.0.\n    return eval(s)\n\np = compiler.parse(\"__tempvalue__ = \" + s)\nobj = p.getChildren()[1].getChildren()[0].getChildren()[1]\n\nreturn _Builder().build(obj)", "path": "cherrypy\\lib\\__init__.py", "repo_name": "jonathanduty/fbfoodie", "stars": 1, "license": "None", "language": "python", "size": 900}
{"docstring": "\"\"\"Modify cherrypy.response status, headers, and body to represent self.\n\nCherryPy uses this internally, but you can also use it to create an\nHTTPRedirect object and set its output without *raising* the exception.\n\"\"\"\n", "func_signal": "def set_response(self):\n", "code": "import cherrypy\nresponse = cherrypy.response\nresponse.status = status = self.status\n\nif status in (300, 301, 302, 303, 307):\n    response.headers['Content-Type'] = \"text/html\"\n    # \"The ... URI SHOULD be given by the Location field\n    # in the response.\"\n    response.headers['Location'] = self.urls[0]\n    \n    # \"Unless the request method was HEAD, the entity of the response\n    # SHOULD contain a short hypertext note with a hyperlink to the\n    # new URI(s).\"\n    msg = {300: \"This resource can be found at <a href='%s'>%s</a>.\",\n           301: \"This resource has permanently moved to <a href='%s'>%s</a>.\",\n           302: \"This resource resides temporarily at <a href='%s'>%s</a>.\",\n           303: \"This resource can be found at <a href='%s'>%s</a>.\",\n           307: \"This resource has moved temporarily to <a href='%s'>%s</a>.\",\n           }[status]\n    response.body = \"<br />\\n\".join([msg % (u, u) for u in self.urls])\n    # Previous code may have set C-L, so we have to reset it\n    # (allow finalize to set it).\n    response.headers.pop('Content-Length', None)\nelif status == 304:\n    # Not Modified.\n    # \"The response MUST include the following header fields:\n    # Date, unless its omission is required by section 14.18.1\"\n    # The \"Date\" header should have been set in Response.__init__\n    \n    # \"...the response SHOULD NOT include other entity-headers.\"\n    for key in ('Allow', 'Content-Encoding', 'Content-Language',\n                'Content-Length', 'Content-Location', 'Content-MD5',\n                'Content-Range', 'Content-Type', 'Expires',\n                'Last-Modified'):\n        if key in response.headers:\n            del response.headers[key]\n    \n    # \"The 304 response MUST NOT contain a message-body.\"\n    response.body = None\n    # Previous code may have set C-L, so we have to reset it.\n    response.headers.pop('Content-Length', None)\nelif status == 305:\n    # Use Proxy.\n    # self.urls[0] should be the URI of the proxy.\n    response.headers['Location'] = self.urls[0]\n    response.body = None\n    # Previous code may have set C-L, so we have to reset it.\n    response.headers.pop('Content-Length', None)\nelse:\n    raise ValueError(\"The %s status code is unknown.\" % status)", "path": "cherrypy\\_cperror.py", "repo_name": "jonathanduty/fbfoodie", "stars": 1, "license": "None", "language": "python", "size": 900}
{"docstring": "\"\"\"Load a module and retrieve an attribute of that module.\"\"\"\n\n# Parse out the path, module, and attribute\n", "func_signal": "def attributes(full_attribute_name):\n", "code": "last_dot = full_attribute_name.rfind(u\".\")\nattr_name = full_attribute_name[last_dot + 1:]\nmod_path = full_attribute_name[:last_dot]\n\nmod = modules(mod_path)\n# Let an AttributeError propagate outward.\ntry:\n    attr = getattr(mod, attr_name)\nexcept AttributeError:\n    raise AttributeError(\"'%s' object has no attribute '%s'\"\n                         % (mod_path, attr_name))\n\n# Return a reference to the attribute.\nreturn attr", "path": "cherrypy\\lib\\__init__.py", "repo_name": "jonathanduty/fbfoodie", "stars": 1, "license": "None", "language": "python", "size": 900}
{"docstring": "\"\"\"Return exc (or sys.exc_info if None), formatted.\"\"\"\n", "func_signal": "def format_exc(exc=None):\n", "code": "if exc is None:\n    exc = _exc_info()\nif exc == (None, None, None):\n    return \"\"\nimport traceback\nreturn \"\".join(traceback.format_exception(*exc))", "path": "cherrypy\\_cperror.py", "repo_name": "jonathanduty/fbfoodie", "stars": 1, "license": "None", "language": "python", "size": 900}
{"docstring": "# we really want id as an integer. Since GET/POST parameters\n# are always passed as strings, let's convert it.\n", "func_signal": "def edit(self, id = 0):\n", "code": "id = int(id)\n\nif id > 0:\n    # if an id is specified, we're editing an existing contact.\n    contact = Contact.get(id)\n    title = \"Edit Contact\"\nelse:\n    # if no id is specified, we're entering a new contact.\n    contact = None\n    title = \"New Contact\"\n\n\n# In the following template code, please note that we use\n# Cheetah's $getVar() construct for the form values. We have\n# to do this because contact may be set to None (see above).\ntemplate = Template('''\n    <h2>$title</h2>\n\n    <form action=\"./store\" method=\"POST\">\n        <input type=\"hidden\" name=\"id\" value=\"$id\" />\n        Last Name: <input name=\"lastName\" value=\"$getVar('contact.lastName', '')\" /><br/>\n        First Name: <input name=\"firstName\" value=\"$getVar('contact.firstName', '')\" /><br/>\n        Phone: <input name=\"phone\" value=\"$getVar('contact.phone', '')\" /><br/>\n        Email: <input name=\"email\" value=\"$getVar('contact.email', '')\" /><br/>\n        URL: <input name=\"url\" value=\"$getVar('contact.url', '')\" /><br/>\n        <input type=\"submit\" value=\"Store\" />\n    </form>\n''', [locals(), globals()])\n\nreturn template.respond()", "path": "cherrypy\\tutorial\\bonus-sqlobject.py", "repo_name": "jonathanduty/fbfoodie", "stars": 1, "license": "None", "language": "python", "size": 900}
{"docstring": "# Assert that config overrides tool constructor args. Above, we set\n# the favicon in the page handler to be '../favicon.ico',\n# but then overrode it in config to be './static/dirback.jpg'.\n", "func_signal": "def testHandlerToolConfigOverride(self):\n", "code": "self.getPage(\"/favicon.ico\")\nself.assertBody(open(os.path.join(localDir, \"static/dirback.jpg\"),\n                     \"rb\").read())", "path": "cherrypy\\test\\test_config.py", "repo_name": "jonathanduty/fbfoodie", "stars": 1, "license": "None", "language": "python", "size": 900}
{"docstring": "# Check that we get an error if no .file or .dir\n", "func_signal": "def test_config_errors(self):\n", "code": "self.getPage(\"/error/thing.html\")\nself.assertErrorPage(500)\nself.assertInBody(\"TypeError: staticdir() takes at least 2 \"\n                  \"arguments (0 given)\")", "path": "cherrypy\\test\\test_static.py", "repo_name": "jonathanduty/fbfoodie", "stars": 1, "license": "None", "language": "python", "size": 900}
{"docstring": "# Check a directory via \"staticdir.index\".\n", "func_signal": "def test_index(self):\n", "code": "self.getPage(\"/docroot/\")\nself.assertStatus('200 OK')\nself.assertHeader('Content-Type', 'text/html')\nself.assertBody('Hello, world\\r\\n')\n# The same page should be returned even if redirected.\nself.getPage(\"/docroot\")\nself.assertStatus((302, 303))\nself.assertHeader('Location', '%s/docroot/' % self.base())\nself.assertMatchesBody(\"This resource .* at <a href='%s/docroot/'>\"\n                       \"%s/docroot/</a>.\" % (self.base(), self.base()))", "path": "cherrypy\\test\\test_static.py", "repo_name": "jonathanduty/fbfoodie", "stars": 1, "license": "None", "language": "python", "size": 900}
{"docstring": "# Test up-level security\n", "func_signal": "def test_security(self):\n", "code": "self.getPage(\"/static/../../test/style.css\")\nself.assertStatus((400, 403))", "path": "cherrypy\\test\\test_static.py", "repo_name": "jonathanduty/fbfoodie", "stars": 1, "license": "None", "language": "python", "size": 900}
{"docstring": "# Let's display a list of all stored contacts.\n", "func_signal": "def index(self):\n", "code": "contacts = Contact.select()\n\ntemplate = Template('''\n    <h2>All Contacts</h2>\n\n    #for $contact in $contacts\n        <a href=\"mailto:$contact.email\">$contact.lastName, $contact.firstName</a>\n        [<a href=\"./edit?id=$contact.id\">Edit</a>]\n        [<a href=\"./delete?id=$contact.id\">Delete</a>]\n        <br/>\n    #end for\n\n    <p>[<a href=\"./edit\">Add new contact</a>]</p>\n''', [locals(), globals()])\n\nreturn template.respond()", "path": "cherrypy\\tutorial\\bonus-sqlobject.py", "repo_name": "jonathanduty/fbfoodie", "stars": 1, "license": "None", "language": "python", "size": 900}
{"docstring": "\"\"\"Yield the given file object in chunks, stopping after `count`\nbytes has been emitted.  Default chunk size is 64kB. (Core)\n\"\"\"\n", "func_signal": "def file_generator_limited(fileobj, count, chunk_size=65536):\n", "code": "remaining = count\nwhile remaining > 0:\n    chunk = fileobj.read(min(chunk_size, remaining))\n    chunklen = len(chunk)\n    if chunklen == 0:\n        return\n    remaining -= chunklen\n    yield chunk", "path": "cherrypy\\lib\\__init__.py", "repo_name": "jonathanduty/fbfoodie", "stars": 1, "license": "None", "language": "python", "size": 900}
{"docstring": "\"\"\"Yield the given input (a file object) in chunks (default 64k). (Core)\"\"\"\n", "func_signal": "def file_generator(input, chunkSize=65536):\n", "code": "chunk = input.read(chunkSize)\nwhile chunk:\n    yield chunk\n    chunk = input.read(chunkSize)\ninput.close()", "path": "cherrypy\\lib\\__init__.py", "repo_name": "jonathanduty/fbfoodie", "stars": 1, "license": "None", "language": "python", "size": 900}
{"docstring": "\"\"\"Discards all bytes from the output or input buffer\"\"\"\n", "func_signal": "def flush(self):\n", "code": "PurgeComm(self.__handle, PURGE_TXABORT|PURGE_RXABORT|PURGE_TXCLEAR|\n          PURGE_RXCLEAR)", "path": "SerialPort_win.py", "repo_name": "sharms/evi-d30", "stars": 1, "license": "other", "language": "python", "size": 84}
{"docstring": "\"\"\" J.Grauheding \"\"\"\n", "func_signal": "def cts(self):\n", "code": "rbuf = fcntl.ioctl(self.__handle, termios.TIOCMGET, self.buf)\nreturn ord(rbuf[3]) & termios.TIOCM_CTS", "path": "SerialPort_darwin.py", "repo_name": "sharms/evi-d30", "stars": 1, "license": "other", "language": "python", "size": 84}
{"docstring": "\"\"\" J.Grauheding \"\"\"\n", "func_signal": "def cd(self):\n", "code": "rbuf = fcntl.ioctl(self.__handle, termios.TIOCMGET, self.buf)\nreturn ord(rbuf[3]) & termios.TIOCM_CAR", "path": "SerialPort_darwin.py", "repo_name": "sharms/evi-d30", "stars": 1, "license": "other", "language": "python", "size": 84}
{"docstring": "\"\"\"Configure the serial port.\n\nPrivate method called in the class constructor that configure the \nserial port with the characteristics given in the constructor.\n\"\"\"\n", "func_signal": "def __configure(self):\n", "code": "if not self.__speed:\n    self.__speed=9600\n\n# Save the initial port configuration\nself.__oldmode=termios.tcgetattr(self.__handle)\nif not self.__params:\n    # self.__params is a list of attributes of the file descriptor\n    # self.__handle as follows:\n    # [c_iflag, c_oflag, c_cflag, c_lflag, c_ispeed, c_ospeed, cc]\n    # where cc is a list of the tty special characters.\n    self.__params=[]\n    # c_iflag\n    self.__params.append(termios.IGNPAR)           \n    # c_oflag\n    self.__params.append(0)                \n    # c_cflag\n    self.__params.append(termios.CS8|termios.CLOCAL|termios.CREAD) \n    # c_lflag\n    self.__params.append(0)                \n    # c_ispeed\n    self.__params.append(SerialPort.BaudRatesDic[self.__speed]) \n    # c_ospeed\n    self.__params.append(SerialPort.BaudRatesDic[self.__speed]) \n    # XXX FIX: Theorically, it should be better to put:\n    # cc=[0]*termios.NCCS \n    # but it doesn't work because NCCS is 19 and self.__oldmode[6]\n    # is 32  Any help ??????????????\n    cc=[0]*len(self.__oldmode[6])\n    if self.__timeout==None:\n        # A reading is only complete when VMIN characters have\n        # been received (blocking reading)\n        cc[termios.VMIN]=1\n        cc[termios.VTIME]=0\n    elif self.__timeout==0:\n        # Non-blocking reading. The reading operation returns\n        # inmeditately, returning the characters waiting to \n        # be read.\n        cc[termios.VMIN]=0\n        cc[termios.VTIME]=0\n    else:\n        # Time-out reading. For a reading to be correct\n        # a character must be recieved in VTIME*100 seconds.\n        cc[termios.VMIN]=0\n        cc[termios.VTIME]=self.__timeout/100\n    self.__params.append(cc)               # c_cc\n\ntermios.tcsetattr(self.__handle, termios.TCSANOW, self.__params)", "path": "SerialPort_darwin.py", "repo_name": "sharms/evi-d30", "stars": 1, "license": "other", "language": "python", "size": 84}
{"docstring": "\"\"\"Returns the number of bytes waiting to be write\n   mod. by J.Grauheding\n\"\"\"\n", "func_signal": "def outWaiting(self):\n", "code": "rbuf=fcntl.ioctl(self.__handle, termios.TIOCOUTQ, self.buf)\nreturn rbuf", "path": "SerialPort_darwin.py", "repo_name": "sharms/evi-d30", "stars": 1, "license": "other", "language": "python", "size": 84}
{"docstring": "\"\"\"Read num bytes from the serial port.\n\nUses the private method __read1 to read num bytes. If an exception\nis generated in any of the calls to __read1 the exception is reraised.\n\"\"\"\n", "func_signal": "def read(self, num=1):\n", "code": "s=''\nfor i in range(num):\n    s=s+SerialPort.__read1(self)\n\nreturn s", "path": "SerialPort_darwin.py", "repo_name": "sharms/evi-d30", "stars": 1, "license": "other", "language": "python", "size": 84}
{"docstring": "\"\"\" J.Grauheding \"\"\"\n", "func_signal": "def dtr_on(self):\n", "code": "rbuf = fcntl.ioctl(self.__handle, termios.TIOCMGET, SerialPort.buf)\nSerialPort.buf[1] = ord(rbuf[3]) | termios.TIOCM_DTR", "path": "SerialPort_darwin.py", "repo_name": "sharms/evi-d30", "stars": 1, "license": "other", "language": "python", "size": 84}
{"docstring": "\"\"\"Close the serial port and restore its initial configuration\n\nTo close the serial port we have to do explicity: del s\n(where s is an instance of SerialPort)\n\"\"\"\n\t\n", "func_signal": "def __del__(self):\n", "code": "\n\t\ntry:\n    os.close(self.__handle)\nexcept IOError:\n    raise SerialPortException('Unable to close port')", "path": "SerialPort_darwin.py", "repo_name": "sharms/evi-d30", "stars": 1, "license": "other", "language": "python", "size": 84}
{"docstring": "\"\"\"Read a line from the serial port.  Returns input once a '\\n'\ncharacter is found.\nDouglas Jones (dfj23@drexel.edu) 09/09/2005.\n\"\"\"\n\n", "func_signal": "def readline(self):\n", "code": "s = ''\nwhile not '\\n' in s:\n    s = s+SerialPort.__read1(self)\n\nreturn s", "path": "SerialPort_darwin.py", "repo_name": "sharms/evi-d30", "stars": 1, "license": "other", "language": "python", "size": 84}
{"docstring": "\"\"\" J.Grauheding \"\"\"\n", "func_signal": "def rts_off(self):\n", "code": "rbuf = fcntl.ioctl(self.__handle, termios.TIOCMGET, self.buf)\nself.buf[1]=ord(rbuf[3]) & ~termios.TIOCM_RTS\nrbuf = fcntl.ioctl(self.__handle, termios.TIOCMSET, self.buf)\nreturn rbuf", "path": "SerialPort_darwin.py", "repo_name": "sharms/evi-d30", "stars": 1, "license": "other", "language": "python", "size": 84}
{"docstring": "\"\"\" J.Grauheding \"\"\"\n", "func_signal": "def ri(self):\n", "code": "rbuf = fcntl.ioctl(self.__handle, termios.TIOCMGET, self.buf)\nreturn ord(rbuf[3]) & termios.TIOCM_RNG", "path": "SerialPort_darwin.py", "repo_name": "sharms/evi-d30", "stars": 1, "license": "other", "language": "python", "size": 84}
{"docstring": "\"\"\"Open the serial port named by the string 'dev'\n\n'dev' can be any of the following strings: 'COM1', 'COM2', ... 'COMX'\n\n'timeout' specifies the inter-byte timeout or first byte timeout\n(in miliseconds) for all subsequent reads on SerialPort.\nIf we specify None time-outs are not used for reading operations\n(blocking reading).\nIf 'timeout' is 0 then reading operations are non-blocking. It\nspecifies that the reading operation is to return inmediately\nwith the bytes that have already been received, even if\nno bytes have been received.\n\n'speed' is an integer that specifies the input and output baud rate to\nuse. Possible values are: 110, 300, 600, 1200, 2400, 4800, 9600,\n19200, 38400, 57600 and 115200.\nIf None a default speed of 9600 bps is selected.\n\n'mode' specifies if we are using RS-232 or RS-485. The RS-485 mode\nis half duplex and use the RTS signal to indicate the\ndirection of the communication (transmit or recive).\nDefault to RS232 mode (at moment, only the RS-232 mode is\nimplemented).\n\n'params' is a list that specifies properties of the serial \ncommunication.\nIf params=None it uses default values for the number of bits\nper byte (8), the parity (NOPARITY) and the number of stop bits (1)\nelse params must be a list with three items setting up the \nthese values in this order.\n\n\"\"\"\n", "func_signal": "def __init__(self, dev, timeout=None, speed=None, mode='232', params=None):\n", "code": "self.__devName, self.__timeout, self.__speed=dev, timeout, speed\nself.__mode=mode\nself.__params=params\ntry:\n    self.__handle=CreateFile (dev,\n                          win32con.GENERIC_READ|win32con.GENERIC_WRITE,\n                          0, # exclusive access\n                          None, # no security\n                          win32con.OPEN_EXISTING,\n                          win32con.FILE_ATTRIBUTE_NORMAL,\n                          None)\nexcept:\n    raise SerialPortException('Unable to open port')\n\nself.__configure()", "path": "SerialPort_win.py", "repo_name": "sharms/evi-d30", "stars": 1, "license": "other", "language": "python", "size": 84}
{"docstring": "\"\"\"Configure the serial port.\n\nPrivate method called in the class constructor that configure the \nserial port with the characteristics given in the constructor.\n\"\"\"\n", "func_signal": "def __configure(self):\n", "code": "if not self.__speed:\n    self.__speed=9600\n# Tell the port we want a notification on each char\nSetCommMask(self.__handle, EV_RXCHAR)\n# Setup a 4k buffer\nSetupComm(self.__handle, 4096, 4096)\n# Remove anything that was there\nPurgeComm(self.__handle, PURGE_TXABORT|PURGE_RXABORT|PURGE_TXCLEAR|\n          PURGE_RXCLEAR)\n\n# Setup the timeouts parameters for the port\n# timeouts is a tuple with the following items:\n# [0] int : ReadIntervalTimeout\n# [1] int : ReadTotalTimeoutMultiplier\n# [2] int : ReadTotalTimeoutConstant\n# [3] int : WriteTotalTimeoutMultiplier\n# [4] int : WriteTotalTimeoutConstant\n\nif self.__timeout==None:\n    timeouts= 0, 0, 0, 0, 0\nelif self.__timeout==0:\n    timeouts = win32con.MAXDWORD, 0, 0, 0, 1000\nelse:\n    timeouts= self.__timeout, 0, self.__timeout, 0 , 1000\nSetCommTimeouts(self.__handle, timeouts)\n\n# Setup the connection info\ndcb=GetCommState(self.__handle)\ndcb.BaudRate=SerialPort.BaudRatesDic[self.__speed]\nif not self.__params:\n    dcb.ByteSize=8\n    dcb.Parity=NOPARITY\n    dcb.StopBits=ONESTOPBIT\nelse:\n    dcb.ByteSize, dcb.Parity, dcb.StopBits=self.__params\nSetCommState(self.__handle, dcb)", "path": "SerialPort_win.py", "repo_name": "sharms/evi-d30", "stars": 1, "license": "other", "language": "python", "size": 84}
{"docstring": "\"\"\"Close the serial port\n\nTo close the serial port we have to do explicity: del s\n(where s is an instance of SerialPort)\n\"\"\"\n", "func_signal": "def __del__(self):\n", "code": "try:\n    CloseHandle(self.__handle)\nexcept IOError:\n    raise SerialPortException('Unable to close port')", "path": "SerialPort_win.py", "repo_name": "sharms/evi-d30", "stars": 1, "license": "other", "language": "python", "size": 84}
{"docstring": "\"\"\"Read 1 byte from the serial port.\n\nGenerate an exception if no byte is read and self.timeout!=0 \nbecause a timeout has expired.\n\"\"\"\n", "func_signal": "def __read1(self):\n", "code": "byte = os.read(self.__handle, 1)\nif len(byte)==0 and self.__timeout!=0: # Time-out\n    raise SerialPortException('Timeout')\nelse:\n    return byte", "path": "SerialPort_darwin.py", "repo_name": "sharms/evi-d30", "stars": 1, "license": "other", "language": "python", "size": 84}
{"docstring": "\"\"\" J.Grauheding \"\"\"\n", "func_signal": "def dsr(self):\n", "code": "rbuf = fcntl.ioctl(self.__handle, termios.TIOCMGET, self.buf)\nreturn ord(rbuf[2]) & (termios.TIOCM_DSR>>8)", "path": "SerialPort_darwin.py", "repo_name": "sharms/evi-d30", "stars": 1, "license": "other", "language": "python", "size": 84}
{"docstring": "\"\"\"Write the string s to the serial port\"\"\"\n", "func_signal": "def write(self, s):\n", "code": "overlapped=OVERLAPPED()\noverlapped.hEvent=CreateEvent(None, 0,0, None)\nWriteFile(self.__handle, s, overlapped)\n# Wait for the write to complete\nWaitForSingleObject(overlapped.hEvent, INFINITE)", "path": "SerialPort_win.py", "repo_name": "sharms/evi-d30", "stars": 1, "license": "other", "language": "python", "size": 84}
{"docstring": "\"\"\"Read a line from the serial port.  Returns input once a '\\n'\ncharacter is found.\n\n\"\"\"\n\n", "func_signal": "def readline(self):\n", "code": "s = ''\nwhile not '\\n' in s:\n    s = s+SerialPort.read1(self,1)\n\nreturn s", "path": "SerialPort_win.py", "repo_name": "sharms/evi-d30", "stars": 1, "license": "other", "language": "python", "size": 84}
{"docstring": "\"\"\"Returns the number of bytes waiting to be read\"\"\"\n", "func_signal": "def inWaiting(self):\n", "code": "flags, comstat = ClearCommError(self.__handle)\nreturn comstat.cbInQue", "path": "SerialPort_win.py", "repo_name": "sharms/evi-d30", "stars": 1, "license": "other", "language": "python", "size": 84}
{"docstring": "\"\"\"Returns the number of bytes waiting to be read \n   mod. by J.Grauheding\n\"\"\"\n", "func_signal": "def inWaiting(self):\n", "code": "rbuf=fcntl.ioctl(self.__handle, termios.TIOCINQ, self.buf)\nreturn rbuf", "path": "SerialPort_darwin.py", "repo_name": "sharms/evi-d30", "stars": 1, "license": "other", "language": "python", "size": 84}
{"docstring": "\"\"\"\nProvides a more efficient way to tail the (stdout) log than\nreadProcessStdoutLog().  Use readProcessStdoutLog() to read\nchunks and tailProcessStdoutLog() to tail.\n\nRequests (length) bytes from the (name)'s log, starting at\n(offset).  If the total log size is greater than (offset +\nlength), the overflow flag is set and the (offset) is\nautomatically increased to position the buffer at the end of\nthe log.  If less than (length) bytes are available, the\nmaximum number of available bytes will be returned.  (offset)\nreturned is always the last offset in the log +1.\n\n@param string name         the name of the process (or 'group:name')\n@param int offset          offset to start reading from\n@param int length          maximum number of bytes to return\n@return array result       [string bytes, int offset, bool overflow]\n\"\"\"\n", "func_signal": "def tailProcessStdoutLog(self, name, offset, length):\n", "code": "self._update('tailProcessStdoutLog')\nreturn self._tailProcessLog(name, offset, length, 'stdout')", "path": "src\\supervisor\\rpcinterface.py", "repo_name": "GunioRobot/supervisor", "stars": 1, "license": "other", "language": "python", "size": 1063}
{"docstring": "# Thanks to mwm@contessa.phone.net (Mike Meyer)\n", "func_signal": "def status (self):\n", "code": "r = [\n        producers.simple_producer (\n                '<li>Authorization Extension : '\n                '<b>Unauthorized requests:</b> %s<ul>' % self.fail_count\n                )\n        ]\nif hasattr (self.handler, 'status'):\n    r.append (self.handler.status())\nr.append (\n        producers.simple_producer ('</ul>')\n        )\nreturn producers.composite_producer(r)", "path": "src\\supervisor\\medusa\\auth_handler.py", "repo_name": "GunioRobot/supervisor", "stars": 1, "license": "other", "language": "python", "size": 1063}
{"docstring": "\"\"\"\nProvides a more efficient way to tail the (stderr) log than\nreadProcessStderrLog().  Use readProcessStderrLog() to read\nchunks and tailProcessStderrLog() to tail.\n\nRequests (length) bytes from the (name)'s log, starting at\n(offset).  If the total log size is greater than (offset +\nlength), the overflow flag is set and the (offset) is\nautomatically increased to position the buffer at the end of\nthe log.  If less than (length) bytes are available, the\nmaximum number of available bytes will be returned.  (offset)\nreturned is always the last offset in the log +1.\n\n@param string name         the name of the process (or 'group:name')\n@param int offset          offset to start reading from\n@param int length          maximum number of bytes to return\n@return array result       [string bytes, int offset, bool overflow]\n\"\"\"\n", "func_signal": "def tailProcessStderrLog(self, name, offset, length):\n", "code": "self._update('tailProcessStderrLog')\nreturn self._tailProcessLog(name, offset, length, 'stderr')", "path": "src\\supervisor\\rpcinterface.py", "repo_name": "GunioRobot/supervisor", "stars": 1, "license": "other", "language": "python", "size": 1063}
{"docstring": "# other code depends on this raising EPIPE if the pipe is closed\n", "func_signal": "def flush(self):\n", "code": "sent = self.process.config.options.write(self.fd,\n                                         self.input_buffer)\nself.input_buffer = self.input_buffer[sent:]", "path": "src\\supervisor\\dispatchers.py", "repo_name": "GunioRobot/supervisor", "stars": 1, "license": "other", "language": "python", "size": 1063}
{"docstring": "\"\"\" Return the PID of supervisord\n\n@return int PID\n\"\"\"\n", "func_signal": "def getPID(self):\n", "code": "self._update('getPID')\nreturn self.supervisord.options.get_pid()", "path": "src\\supervisor\\rpcinterface.py", "repo_name": "GunioRobot/supervisor", "stars": 1, "license": "other", "language": "python", "size": 1063}
{"docstring": "\"\"\" Stop all processes in the process list\n\n@param boolean wait    Wait for each process to be fully stopped\n@return boolean result Always return true unless error.\n\"\"\"\n", "func_signal": "def stopAllProcesses(self, wait=True):\n", "code": "self._update('stopAllProcesses')\n\nprocesses = self._getAllProcesses()\n\nkillall = make_allfunc(processes, isRunning, self.stopProcess,\n                       wait=wait)\n\nkillall.delay = 0.05\nkillall.rpcinterface = self\nreturn killall # deferred", "path": "src\\supervisor\\rpcinterface.py", "repo_name": "GunioRobot/supervisor", "stars": 1, "license": "other", "language": "python", "size": 1063}
{"docstring": "\"\"\"Update our built-in md5 registry\"\"\"\n\n", "func_signal": "def update_md5(filenames):\n", "code": "import re\nfrom md5 import md5\n\nfor name in filenames:\n    base = os.path.basename(name)\n    f = open(name,'rb')\n    md5_data[base] = md5(f.read()).hexdigest()\n    f.close()\n\ndata = [\"    %r: %r,\\n\" % it for it in md5_data.items()]\ndata.sort()\nrepl = \"\".join(data)\n\nimport inspect\nsrcfile = inspect.getsourcefile(sys.modules[__name__])\nf = open(srcfile, 'rb'); src = f.read(); f.close()\n\nmatch = re.search(\"\\nmd5_data = {\\n([^}]+)}\", src)\nif not match:\n    print >>sys.stderr, \"Internal error!\"\n    sys.exit(2)\n\nsrc = src[:match.start(1)] + repl + src[match.end(1):]\nf = open(srcfile,'w')\nf.write(src)\nf.close()", "path": "ez_setup.py", "repo_name": "GunioRobot/supervisor", "stars": 1, "license": "other", "language": "python", "size": 1063}
{"docstring": "\"\"\" Remove a stopped process from the active configuration.\n\n@param string name         name of process group to remove\n@return boolean result     Indicates wether the removal was successful\n\"\"\"\n", "func_signal": "def removeProcessGroup(self, name):\n", "code": "self._update('removeProcessGroup')\nif name not in self.supervisord.process_groups:\n    raise RPCError(Faults.BAD_NAME, name)\n\nresult = self.supervisord.remove_process_group(name)\nif not result:\n    raise RPCError(Faults.STILL_RUNNING)\nreturn True", "path": "src\\supervisor\\rpcinterface.py", "repo_name": "GunioRobot/supervisor", "stars": 1, "license": "other", "language": "python", "size": 1063}
{"docstring": "\"\"\" Clear the main log.\n\n@return boolean result always returns True unless error\n\"\"\"\n", "func_signal": "def clearLog(self):\n", "code": "self._update('clearLog')\n\nlogfile = self.supervisord.options.logfile\nif  logfile is None or not self.supervisord.options.exists(logfile):\n    raise RPCError(Faults.NO_FILE)\n\n# there is a race condition here, but ignore it.\ntry:\n    self.supervisord.options.remove(logfile)\nexcept (OSError, IOError):\n    raise RPCError(Faults.FAILED)\n\nfor handler in self.supervisord.options.logger.handlers:\n    if hasattr(handler, 'reopen'):\n        self.supervisord.options.logger.info('reopening log file')\n        handler.reopen()\nreturn True", "path": "src\\supervisor\\rpcinterface.py", "repo_name": "GunioRobot/supervisor", "stars": 1, "license": "other", "language": "python", "size": 1063}
{"docstring": "\"\"\" Read length bytes from name's stdout log starting at offset\n\n@param string name        the name of the process (or 'group:name')\n@param int offset         offset to start reading from.\n@param int length         number of bytes to read from the log.\n@return string result     Bytes of log\n\"\"\"\n", "func_signal": "def readProcessStdoutLog(self, name, offset, length):\n", "code": "self._update('readProcessStdoutLog')\nreturn self._readProcessLog(name, offset, length, 'stdout')", "path": "src\\supervisor\\rpcinterface.py", "repo_name": "GunioRobot/supervisor", "stars": 1, "license": "other", "language": "python", "size": 1063}
{"docstring": "# get process to start from name\n", "func_signal": "def _getGroupAndProcess(self, name):\n", "code": "group_name, process_name = split_namespec(name)\n\ngroup = self.supervisord.process_groups.get(group_name)\nif group is None:\n    raise RPCError(Faults.BAD_NAME, name)\n\nif process_name is None:\n    return group, None\n\nprocess = group.processes.get(process_name)\nif process is None:\n    raise RPCError(Faults.BAD_NAME, name)\n\nreturn group, process", "path": "src\\supervisor\\rpcinterface.py", "repo_name": "GunioRobot/supervisor", "stars": 1, "license": "other", "language": "python", "size": 1063}
{"docstring": "# if lexical is true, return processes sorted in lexical order,\n# otherwise, sort in priority order\n", "func_signal": "def _getAllProcesses(self, lexical=False):\n", "code": "all_processes = []\n\nif lexical:\n    group_names = self.supervisord.process_groups.keys()\n    group_names.sort()\n    for group_name in group_names:\n        group = self.supervisord.process_groups[group_name]\n        process_names = group.processes.keys()\n        process_names.sort()\n        for process_name in process_names:\n            process = group.processes[process_name]\n            all_processes.append((group, process))\nelse:\n    groups = self.supervisord.process_groups.values()\n    groups.sort() # asc by priority\n\n    for group in groups:\n        processes = group.processes.values()\n        processes.sort() # asc by priority\n        for process in processes:\n            all_processes.append((group, process))\n\nreturn all_processes", "path": "src\\supervisor\\rpcinterface.py", "repo_name": "GunioRobot/supervisor", "stars": 1, "license": "other", "language": "python", "size": 1063}
{"docstring": "\"\"\" Return the version of the RPC API used by supervisord\n\n@return string version version id\n\"\"\"\n", "func_signal": "def getAPIVersion(self):\n", "code": "self._update('getAPIVersion')\nreturn API_VERSION", "path": "src\\supervisor\\rpcinterface.py", "repo_name": "GunioRobot/supervisor", "stars": 1, "license": "other", "language": "python", "size": 1063}
{"docstring": "\"\"\" Start all processes in the group named 'name'\n\n@param string name        The group name\n@param boolean wait       Wait for each process to be fully started\n@return struct result     A structure containing start statuses\n\"\"\"\n", "func_signal": "def startProcessGroup(self, name, wait=True):\n", "code": "self._update('startProcessGroup')\n\ngroup = self.supervisord.process_groups.get(name)\n\nif group is None:\n    raise RPCError(Faults.BAD_NAME, name)\n\nprocesses = group.processes.values()\nprocesses.sort()\nprocesses = [ (group, process) for process in processes ]\n\nstartall = make_allfunc(processes, isNotRunning, self.startProcess,\n                        wait=wait)\n    \nstartall.delay = 0.05\nstartall.rpcinterface = self\nreturn startall # deferred", "path": "src\\supervisor\\rpcinterface.py", "repo_name": "GunioRobot/supervisor", "stars": 1, "license": "other", "language": "python", "size": 1063}
{"docstring": "\"\"\"\nRemove all ANSI color escapes from the given string.\n\"\"\"\n", "func_signal": "def stripEscapes(string):\n", "code": "result = ''\nshow = 1\ni = 0\nL = len(string)\nwhile i < L:\n    if show == 0 and string[i] in ANSI_TERMINATORS:\n        show = 1\n    elif show:\n        n = string.find(ANSI_ESCAPE_BEGIN, i)\n        if n == -1:\n            return result + string[i:]\n        else:\n            result = result + string[i:n]\n            i = n\n            show = 0\n    i = i + 1\nreturn result", "path": "src\\supervisor\\dispatchers.py", "repo_name": "GunioRobot/supervisor", "stars": 1, "license": "other", "language": "python", "size": 1063}
{"docstring": "# http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.6.1\n# trailer        = *(entity-header CRLF)\n", "func_signal": "def trailer(self):\n", "code": "line = self.buffer\nif line==CRLF:\n    self.done()\n    self.close()", "path": "src\\supervisor\\http_client.py", "repo_name": "GunioRobot/supervisor", "stars": 1, "license": "other", "language": "python", "size": 1063}
{"docstring": "\"\"\" Return the version of the supervisor package in use by supervisord\n\n@return string version version id\n\"\"\"\n", "func_signal": "def getSupervisorVersion(self):\n", "code": "self._update('getSupervisorVersion')\nimport options\nreturn options.VERSION", "path": "src\\supervisor\\rpcinterface.py", "repo_name": "GunioRobot/supervisor", "stars": 1, "license": "other", "language": "python", "size": 1063}
{"docstring": "\"\"\" Clear the stdout and stderr logs for the named process and\nreopen them.\n\n@param string name   The name of the process (or 'group:name')\n@return boolean result      Always True unless error\n\"\"\"\n", "func_signal": "def clearProcessLogs(self, name):\n", "code": "self._update('clearProcessLogs')\n\ngroup, process = self._getGroupAndProcess(name)\n\ntry:\n    # implies a reopen\n    process.removelogs()\nexcept (IOError, OSError):\n    raise RPCError(Faults.FAILED, name)\n\nreturn True", "path": "src\\supervisor\\rpcinterface.py", "repo_name": "GunioRobot/supervisor", "stars": 1, "license": "other", "language": "python", "size": 1063}
{"docstring": "\"\"\" Restart the supervisor process\n\n@return boolean result  always return True unless error\n\"\"\"\n", "func_signal": "def restart(self):\n", "code": "self._update('restart')\n\nself.supervisord.options.mood = SupervisorStates.RESTARTING\nreturn True", "path": "src\\supervisor\\rpcinterface.py", "repo_name": "GunioRobot/supervisor", "stars": 1, "license": "other", "language": "python", "size": 1063}
{"docstring": "\"\"\"Report a change in reference counts, with extra detail.\"\"\"\n", "func_signal": "def detailed_refcounts(self, rc, prev):\n", "code": "print (\"  sum detail refcount=%-8d\"\n       \" sys refcount=%-8d\"\n       \" change=%-6d\"\n       % (self.n, rc, rc - prev))\nself.output()", "path": "src\\supervisor\\tests\\trackrefs.py", "repo_name": "GunioRobot/supervisor", "stars": 1, "license": "other", "language": "python", "size": 1063}
{"docstring": "\"\"\"generic obtain data from svn ls\n\t(TODO: refactor, overlaps with DownloadInstaller get_installers)\"\"\"\n\n", "func_signal": "def svnlist(self,dir):\n", "code": "command=['svn','ls',dir]\nprocess=subprocess.Popen(command,stdout=subprocess.PIPE, stderr=subprocess.PIPE)\nfailp=process.stderr.read()\nif failp:\n\treturn None\nl=list(process.stdout)\n# tidy l in place\nfor i in range(len(l)):\n\tl[i]=l[i].strip()\n\tif l[i].endswith(\"/\"):\n\t\tl[i]=l[i][:-1]\nreturn l", "path": "installer\\tags.py", "repo_name": "cmelbye/mediawiki-testing", "stars": 1, "license": "None", "language": "python", "size": 760}
{"docstring": "\"\"\"If value can be converted to int, return\ntrue, else return false\"\"\"\n", "func_signal": "def isanint(value):\n", "code": "try:\n\tnum=int(value)\nexcept ValueError:\n\treturn False\nreturn True", "path": "installer\\installer_util.py", "repo_name": "cmelbye/mediawiki-testing", "stars": 1, "license": "None", "language": "python", "size": 760}
{"docstring": "\"\"\"try systems, if any system can_exec, we exec.\nshort circuits: Only first system with capability\nwill exec\"\"\"\n", "func_signal": "def exec_task(self, installer_name, task, env=None):\n", "code": "for system in self.systemlist:\n\tif system.can_exec(task):\n\t\tsystem.exec_task(installer_name, task, env)\n\t\tbreak", "path": "installer\\combined_installer.py", "repo_name": "cmelbye/mediawiki-testing", "stars": 1, "license": "None", "language": "python", "size": 760}
{"docstring": "# rm -f (fail silently)\n", "func_signal": "def uninstall_settings(self,installer_name):\n", "code": "try:\n\tos.unlink(self._settings_filepath(installer_name))\nexcept OSError:\n\tpass", "path": "installer\\naive_installer.py", "repo_name": "cmelbye/mediawiki-testing", "stars": 1, "license": "None", "language": "python", "size": 760}
{"docstring": "\"\"\"initialize with a list of pre-initialized\n   installers\n\"\"\"\n", "func_signal": "def __init__(self, systemlist):\n", "code": "Installation_System.__init__(self)\nself.systemlist=systemlist", "path": "installer\\combined_installer.py", "repo_name": "cmelbye/mediawiki-testing", "stars": 1, "license": "None", "language": "python", "size": 760}
{"docstring": "\"\"\"generic replace function, takes a dictionary of search/replace\nstrings (replacements), and applies them to the input file \nspecified by infilename, and saves the results to the \noutput file at outfilename\"\"\"\n\n", "func_signal": "def replace_generic(replacements,infilename,outfilename):\n", "code": "infile=file(infilename)\noutfile=file(outfilename,\"w\")\nfor line in infile:\n\tfor search,replace in replacements.iteritems():\n\t\tline=line.replace(search,replace)\n\toutfile.write(line)\noutfile.close()\ninfile.close()", "path": "installer\\installer_util.py", "repo_name": "cmelbye/mediawiki-testing", "stars": 1, "license": "None", "language": "python", "size": 760}
{"docstring": "\"\"\"When an extension fails to uninstall cleanly, uninstall the entire wiki, and \ncopy the backup back in its place\"\"\"\n", "func_signal": "def clear():\n", "code": "mwinstaller=Mediawiki_Installer()\nmwinstaller.uninstall(target_wiki)\nmwinstaller.duplicate(target_wiki2, target_wiki)", "path": "util\\scripted_clean.py", "repo_name": "cmelbye/mediawiki-testing", "stars": 1, "license": "None", "language": "python", "size": 760}
{"docstring": "\"\"\"obtain revision number for installer itself\"\"\"\n\n", "func_signal": "def revision():\n", "code": "revision_string=None\nos.chdir(settings.installerdir)\ninfo=os.popen(\"svn info .\")\nfor line in info:\n\tif line.startswith(\"Revision:\"):\n\t\trevision_string=line.strip()\n\t\tbreak\ninfo.close()\nif revision_string==None:\n\trevision=\"unknown\"\nelse:\n\trevision=revision_string.replace(\"Revision:\",\"\")\n\nreturn revision", "path": "installer\\installer_util.py", "repo_name": "cmelbye/mediawiki-testing", "stars": 1, "license": "None", "language": "python", "size": 760}
{"docstring": "\"\"\"also set any attributes for subsystems\nwe also set the same attribute locally.\nBe careful when reading back!\"\"\"\n", "func_signal": "def __setattr__(self,name,value):\n", "code": "self.__dict__[name]=value\nif name==\"systemlist\":\t# prevent infinite recursion\n\treturn\nif self.systemlist:\n\tfor system in self.systemlist:\n\t\tsystem.__setattr__(name,value)", "path": "installer\\combined_installer.py", "repo_name": "cmelbye/mediawiki-testing", "stars": 1, "license": "None", "language": "python", "size": 760}
{"docstring": "\"\"\"reply if we can execute a task, (allows nested\ncombined_installers, fwiw. Short circuits (stops\ntrying once it finds one system that supports the task))\"\"\"\n", "func_signal": "def can_exec(self, installer_name, task):\n", "code": "for system in self.systemlist:\n\tif system.can_exec(self, installer_name, task):\n\t\treturn True\n\nreturn False", "path": "installer\\combined_installer.py", "repo_name": "cmelbye/mediawiki-testing", "stars": 1, "license": "None", "language": "python", "size": 760}
{"docstring": "\"\"\"print out information about the target from the info file, short circuits\"\"\"\n", "func_signal": "def get_info(self, installer_name):\n", "code": "for system in self.systemlist:\n\tret=system.get_info()\n\tif ret:\n\t\treturn ret\nreturn None", "path": "installer\\combined_installer.py", "repo_name": "cmelbye/mediawiki-testing", "stars": 1, "license": "None", "language": "python", "size": 760}
{"docstring": "\"\"\"use the maintenance/sql.php included with a particular mediawiki instance to run a sql file. instancedir is the directory containing the mediawiki instance (eg. /home/pete/public_html/revisions/petesfirstwiki) , sqlfilename is the name of the file to run. \"\"\"\n", "func_signal": "def sqldotphp(instancedir,sqlfilename):\n", "code": "if not os.path.exists(sqlfilename):\n\traise Exception(\"File '\"+sqlfilename+\"' not found.\")\nsqldotphp=os.path.join(instancedir,\"maintenance\",\"sql.php\")\nif not os.path.exists(sqldotphp):\n\traise Exception(\"sql.php file not found at \"+sqldotphp+\"'.\")\n\ncommand=settings.phpcommand+\" \"+sqldotphp+\" \"+sqlfilename\nrv=os.system(command)>>8\nif rv:\n\traise Exception(\"Failed to execute \"+command)", "path": "installer\\installer_util.py", "repo_name": "cmelbye/mediawiki-testing", "stars": 1, "license": "None", "language": "python", "size": 760}
{"docstring": "\"\"\"set instance on all items\"\"\"\n", "func_signal": "def set_instance(self,instance):\n", "code": "for system in self.systemlist:\n\tsystem.set_instance(instance)", "path": "installer\\combined_installer.py", "repo_name": "cmelbye/mediawiki-testing", "stars": 1, "license": "None", "language": "python", "size": 760}
{"docstring": "\"\"\"load tags from cache\"\"\"\n", "func_signal": "def load_cache(self):\n", "code": "if not os.path.isfile(settings.tagcache):\n\treturn None\n\ncache=pickle.load(file(settings.tagcache))\nreturn cache", "path": "installer\\tags.py", "repo_name": "cmelbye/mediawiki-testing", "stars": 1, "license": "None", "language": "python", "size": 760}
{"docstring": "\"\"\"return true if any of the installers finds that the particular item is already installed. (short circuits)\"\"\"\n", "func_signal": "def is_installed(self,installer_name):\n", "code": "for system in self.systemlist:\n\tif system.is_installed(installer_name):\n\t\treturn True", "path": "installer\\combined_installer.py", "repo_name": "cmelbye/mediawiki-testing", "stars": 1, "license": "None", "language": "python", "size": 760}
{"docstring": "# try to load preexisting cache\n", "func_signal": "def __init__(self):\n", "code": "self.cache=self.load_cache()\n# No joy? Let's generate one.\nif not self.cache:\n\tself.cache=self.update_cache()\n# Still no joy? \nif not self.cache:\n\traise Exception(\"Internal error: cannot obtain a tag cache\")", "path": "installer\\tags.py", "repo_name": "cmelbye/mediawiki-testing", "stars": 1, "license": "None", "language": "python", "size": 760}
{"docstring": "\"\"\"check whether our database settings actually work\nby logging in to mysql with an empty line.\nreturns true if database works,  else returns false.\"\"\"\n", "func_signal": "def db_works():\n", "code": "rv=os.system(\"echo | \"+settings.mysql_command)\nreturn not rv", "path": "installer\\installer_util.py", "repo_name": "cmelbye/mediawiki-testing", "stars": 1, "license": "None", "language": "python", "size": 760}
{"docstring": "\"\"\"update cache to disk and to memory\"\"\"\n", "func_signal": "def update_cache(self):\n", "code": "self.cache=self.update_cache_file()\nreturn self.cache", "path": "installer\\tags.py", "repo_name": "cmelbye/mediawiki-testing", "stars": 1, "license": "None", "language": "python", "size": 760}
{"docstring": "\"\"\"return a list of installed items (items installed by all the installers)\"\"\"\n", "func_signal": "def get_installed(self):\n", "code": "installers=set()\nfor system in self.systemlist:\n\ttry:\n\t\tinstallers.update(system.get_installed())\n\texcept Exception:\n\t\tpass\n\ninstallers2=list(installers)\ninstallers2.sort()\nreturn installers2", "path": "installer\\combined_installer.py", "repo_name": "cmelbye/mediawiki-testing", "stars": 1, "license": "None", "language": "python", "size": 760}
{"docstring": "\"\"\"perform uninstallation of things that need uninstalling, in the case of the naive installer, we uninstall\"\"\"\n", "func_signal": "def _uninstall(self, installer_name, destination_dir):\n", "code": "schemafilename=os.path.join(destination_dir, installer_name, \"schema.sql\")\nif os.path.exists(schemafilename):\n\tunschema.unschema(self.instancedir(), schemafilename)\nsuper(Naive_Installer,self)._uninstall(installer_name, destination_dir)", "path": "installer\\naive_installer.py", "repo_name": "cmelbye/mediawiki-testing", "stars": 1, "license": "None", "language": "python", "size": 760}
{"docstring": "'''Check if current line starts a test'''\n", "func_signal": "def scanLineForTest( suite, lineNo, line ):\n", "code": "m = test_re.search( line )\nif m:\n    addTest( suite, m.group(2), lineNo )", "path": "python\\cxxtest\\cxxtest_parser.py", "repo_name": "danielrh/cxxtest", "stars": 1, "license": "lgpl-2.1", "language": "python", "size": 899}
{"docstring": "'''Add current suite to list'''\n", "func_signal": "def rememberSuite(suite):\n", "code": "global suites\nsuites.append( suite )", "path": "python\\cxxtest\\cxxtest_parser.py", "repo_name": "danielrh/cxxtest", "stars": 1, "license": "lgpl-2.1", "language": "python", "size": 899}
{"docstring": "'''Set input files specified on command line'''\n", "func_signal": "def setFiles(patterns ):\n", "code": "files = expandWildcards( patterns )\nreturn files", "path": "python\\cxxtest\\cxxtestgen.py", "repo_name": "danielrh/cxxtest", "stars": 1, "license": "lgpl-2.1", "language": "python", "size": 899}
{"docstring": "'''The main program'''\n#\n# Reset global state\n#\n", "func_signal": "def main(args=None):\n", "code": "global wrotePreamble\nwrotePreamble=0\nglobal wroteWorld\nwroteWorld=0\nglobal lastIncluded\nlastIncluded = ''\n\nglobal suites\nglobal options\nfiles = parseCommandline(args)\nif imported_fog and options.fog:\n    [options,suites] = cxxtest_fog.scanInputFiles( files, options )\nelse:\n    [options,suites] = cxxtest_parser.scanInputFiles( files, options )\nwriteOutput()", "path": "python\\cxxtest\\cxxtestgen.py", "repo_name": "danielrh/cxxtest", "stars": 1, "license": "lgpl-2.1", "language": "python", "size": 899}
{"docstring": "'''Create output not based on template'''\n", "func_signal": "def writeSimpleOutput():\n", "code": "output = startOutputFile()\nwritePreamble( output )\nif options.root or not options.part:\n    writeMain( output )\n\nif len(suites) > 0:\n    print >>output, \"bool \"+suites[0]['name']+\"_init = false;\"\n\nwriteWorld( output )\noutput.close()", "path": "python\\cxxtest\\cxxtestgen.py", "repo_name": "danielrh/cxxtest", "stars": 1, "license": "lgpl-2.1", "language": "python", "size": 899}
{"docstring": "'''Create static suite pointer object for dynamic suites'''\n", "func_signal": "def writeSuitePointer( output, suite ):\n", "code": "if options.noStaticInit:\n    output.write( 'static %s *%s;\\n\\n' % (suite['name'], suite['object']) )\nelse:\n    output.write( 'static %s *%s = 0;\\n\\n' % (suite['name'], suite['object']) )", "path": "python\\cxxtest\\cxxtestgen.py", "repo_name": "danielrh/cxxtest", "stars": 1, "license": "lgpl-2.1", "language": "python", "size": 899}
{"docstring": "'''Append the line to the current CXXTEST_CODE() block'''\n", "func_signal": "def addLineToBlock( suite, lineNo, line ):\n", "code": "line = fixBlockLine( suite, lineNo, line )\nline = re.sub( r'^.*\\{\\{', '', line )\n\ne = re.search( r'\\}\\}', line )\nif e:\n    line = line[:e.start()]\nsuite['lines'].append( line )\nreturn e is None", "path": "python\\cxxtest\\cxxtest_parser.py", "repo_name": "danielrh/cxxtest", "stars": 1, "license": "lgpl-2.1", "language": "python", "size": 899}
{"docstring": "'''Change all [E]TS_ macros used in a line to _[E]TS_ macros with the correct file/line'''\n", "func_signal": "def fixBlockLine( suite, lineNo, line):\n", "code": "return re.sub( r'\\b(E?TSM?_(ASSERT[A-Z_]*|FAIL))\\s*\\(',\n               r'_\\1(%s,%s,' % (suite['cfile'], lineNo),\n               line, 0 )", "path": "python\\cxxtest\\cxxtest_parser.py", "repo_name": "danielrh/cxxtest", "stars": 1, "license": "lgpl-2.1", "language": "python", "size": 899}
{"docstring": "'''Write CxxTest::initialize(), which replaces static initialization'''\n", "func_signal": "def writeInitialize(output):\n", "code": "output.write( 'namespace CxxTest {\\n' )\noutput.write( ' void initialize()\\n' )\noutput.write( ' {\\n' )\nfor suite in suites:\n    output.write( '  %s.initialize();\\n' % suite['tlist'] )\n    if isDynamic(suite):\n        output.write( '  %s = 0;\\n' % suite['object'] )\n        output.write( '  %s.initialize( %s, %s, \"%s\", %s, %s, %s, %s );\\n' %\n                      (suite['dobject'], suite['cfile'], suite['line'], suite['name'],\n                       suite['tlist'], suite['object'], suite['create'], suite['destroy']) )\n    else:\n        output.write( '  %s.initialize( %s, %s, \"%s\", %s, %s );\\n' %\n                      (suite['dobject'], suite['cfile'], suite['line'], suite['name'],\n                       suite['object'], suite['tlist']) )\n\n    for test in suite['tests']:\n        output.write( '  %s.initialize( %s, %s, %s, \"%s\" );\\n' %\n                      (test['object'], suite['tlist'], suite['dobject'], test['line'], test['name']) )\n\noutput.write( ' }\\n' )\noutput.write( '}\\n' )", "path": "python\\cxxtest\\cxxtestgen.py", "repo_name": "danielrh/cxxtest", "stars": 1, "license": "lgpl-2.1", "language": "python", "size": 899}
{"docstring": "'''Print CxxTest version and exit'''\n", "func_signal": "def printVersion():\n", "code": "sys.stdout.write( \"This is CxxTest version INSERT_VERSION_HERE.\\n\" )\nsys.exit(0)", "path": "python\\cxxtest\\cxxtestgen.py", "repo_name": "danielrh/cxxtest", "stars": 1, "license": "lgpl-2.1", "language": "python", "size": 899}
{"docstring": "'''Check if current line defines a createSuite() function'''\n", "func_signal": "def scanLineForCreate( suite, lineNo, line ):\n", "code": "if create_re.search( line ):\n    addSuiteCreateDestroy( suite, 'create', lineNo )", "path": "python\\cxxtest\\cxxtest_parser.py", "repo_name": "danielrh/cxxtest", "stars": 1, "license": "lgpl-2.1", "language": "python", "size": 899}
{"docstring": "'''Expand all wildcards in an array (glob)'''\n", "func_signal": "def expandWildcards( patterns ):\n", "code": "fileNames = []\nfor pathName in patterns:\n    patternFiles = glob.glob( pathName )\n    for fileName in patternFiles:\n        fileNames.append( fixBackslashes( fileName ) )\nreturn fileNames", "path": "python\\cxxtest\\cxxtestgen.py", "repo_name": "danielrh/cxxtest", "stars": 1, "license": "lgpl-2.1", "language": "python", "size": 899}
{"docstring": "'''Scan all input files for test suites'''\n", "func_signal": "def scanInputFiles(files, _options):\n", "code": "global options\noptions=_options\nfor file in files:\n    scanInputFile(file)\nglobal suites\nif len(suites) is 0 and not options.root:\n    abort( 'No tests defined' )\n\n#print \"INFO\\n\"\n#for suite in suites:\n    #for key in suite:\n        #print key,suite[key]\n    #print \"\"\n\nreturn [options,suites]", "path": "python\\cxxtest\\cxxtest_parser.py", "repo_name": "danielrh/cxxtest", "stars": 1, "license": "lgpl-2.1", "language": "python", "size": 899}
{"docstring": "'''Check if current line defines a destroySuite() function'''\n", "func_signal": "def scanLineForDestroy( suite, lineNo, line ):\n", "code": "if destroy_re.search( line ):\n    addSuiteCreateDestroy( suite, 'destroy', lineNo )", "path": "python\\cxxtest\\cxxtest_parser.py", "repo_name": "danielrh/cxxtest", "stars": 1, "license": "lgpl-2.1", "language": "python", "size": 899}
{"docstring": "'''Write the static name of the world name'''\n", "func_signal": "def writeWorldDescr( output ):\n", "code": "if options.noStaticInit:\n    output.write( 'const char* static CxxTest::RealDescriptions::_worldName;\\n' )\nelse:\n    output.write( 'const char* CxxTest::RealWorldDescription::_worldName = \"cxxtest\";\\n' )", "path": "python\\cxxtest\\cxxtestgen.py", "repo_name": "danielrh/cxxtest", "stars": 1, "license": "lgpl-2.1", "language": "python", "size": 899}
{"docstring": "'''Scan single input file for test suites'''\n", "func_signal": "def scanInputFile(fileName):\n", "code": "file = open(fileName)\nlineNo = 0\nwhile 1:\n    line = file.readline()\n    if not line:\n        break\n    lineNo = lineNo + 1\n\n    scanInputLine( fileName, lineNo, line )\ncloseSuite()\nfile.close()", "path": "python\\cxxtest\\cxxtest_parser.py", "repo_name": "danielrh/cxxtest", "stars": 1, "license": "lgpl-2.1", "language": "python", "size": 899}
{"docstring": "'''Returns whether current line is part of the current suite.\nThis can be false when we are in a generated suite outside of CXXTEST_CODE() blocks\nIf the suite is generated, adds the line to the list of lines'''\n", "func_signal": "def lineBelongsToSuite( suite, lineNo, line ):\n", "code": "if not suite['generated']:\n    return 1\n\nglobal inBlock\nif not inBlock:\n    inBlock = lineStartsBlock( line )\nif inBlock:\n    inBlock = addLineToBlock( suite, lineNo, line )\nreturn inBlock", "path": "python\\cxxtest\\cxxtest_parser.py", "repo_name": "danielrh/cxxtest", "stars": 1, "license": "lgpl-2.1", "language": "python", "size": 899}
{"docstring": "'''Write a suite declared with CXXTEST_SUITE()'''\n", "func_signal": "def generateSuite( output, suite ):\n", "code": "output.write( 'class %s : public CxxTest::TestSuite {\\n' % suite['name'] )\noutput.write( 'public:\\n' )\nfor line in suite['lines']:\n    output.write(line)\noutput.write( '};\\n\\n' )", "path": "python\\cxxtest\\cxxtestgen.py", "repo_name": "danielrh/cxxtest", "stars": 1, "license": "lgpl-2.1", "language": "python", "size": 899}
{"docstring": "'''Add #include \"file\" statement'''\n", "func_signal": "def writeInclude(output, file):\n", "code": "global lastIncluded\nif file == lastIncluded: return\noutput.writelines( [ '#include \"', file, '\"\\n\\n' ] )\nlastIncluded = file", "path": "python\\cxxtest\\cxxtestgen.py", "repo_name": "danielrh/cxxtest", "stars": 1, "license": "lgpl-2.1", "language": "python", "size": 899}
{"docstring": "'''Write the world definitions'''\n", "func_signal": "def writeWorld( output ):\n", "code": "global wroteWorld\nif wroteWorld: return\nwritePreamble( output )\nwriteSuites( output )\nif options.root or not options.part:\n    writeRoot( output )\n    writeWorldDescr( output )\nif options.noStaticInit:\n    writeInitialize( output )\nwroteWorld = 1", "path": "python\\cxxtest\\cxxtestgen.py", "repo_name": "danielrh/cxxtest", "stars": 1, "license": "lgpl-2.1", "language": "python", "size": 899}
{"docstring": "\"\"\"\nRetrieve a list of instances of the specified model which share\ntags with the model instance ``obj``, ordered by the number of\nshared tags in descending order.\n\nIf ``num`` is given, a maximum of ``num`` instances will be\nreturned.\n\"\"\"\n", "func_signal": "def get_related(self, obj, queryset_or_model, num=None):\n", "code": "queryset, model = get_queryset_and_model(queryset_or_model)\nmodel_table = qn(model._meta.db_table)\ncontent_type = ContentType.objects.get_for_model(obj)\nrelated_content_type = ContentType.objects.get_for_model(model)\nquery = \"\"\"\nSELECT %(model_pk)s, COUNT(related_tagged_item.object_id) AS %(count)s\nFROM %(model)s, %(tagged_item)s, %(tag)s, %(tagged_item)s related_tagged_item\nWHERE %(tagged_item)s.object_id = %%s\n  AND %(tagged_item)s.content_type_id = %(content_type_id)s\n  AND %(tag)s.id = %(tagged_item)s.tag_id\n  AND related_tagged_item.content_type_id = %(related_content_type_id)s\n  AND related_tagged_item.tag_id = %(tagged_item)s.tag_id\n  AND %(model_pk)s = related_tagged_item.object_id\"\"\"\nif content_type.pk == related_content_type.pk:\n    # Exclude the given instance itself if determining related\n    # instances for the same model.\n    query += \"\"\"\n  AND related_tagged_item.object_id != %(tagged_item)s.object_id\"\"\"\nquery += \"\"\"\nGROUP BY %(model_pk)s\nORDER BY %(count)s DESC\n%(limit_offset)s\"\"\"\nquery = query % {\n    'model_pk': '%s.%s' % (model_table, qn(model._meta.pk.column)),\n    'count': qn('count'),\n    'model': model_table,\n    'tagged_item': qn(self.model._meta.db_table),\n    'tag': qn(self.model._meta.get_field('tag').rel.to._meta.db_table),\n    'content_type_id': content_type.pk,\n    'related_content_type_id': related_content_type.pk,\n    'limit_offset': num is not None and connection.ops.limit_offset_sql(num) or '',\n}\n\ncursor = connection.cursor()\ncursor.execute(query, [obj.pk])\nobject_ids = [row[0] for row in cursor.fetchall()]\nif len(object_ids) > 0:\n    # Use in_bulk here instead of an id__in lookup, because id__in would\n    # clobber the ordering.\n    object_dict = queryset.in_bulk(object_ids)\n    return [object_dict[object_id] for object_id in object_ids \\\n            if object_id in object_dict]\nelse:\n    return []", "path": "tagging\\models.py", "repo_name": "KartashAlex/django_social", "stars": 0, "license": "None", "language": "python", "size": 12740}
{"docstring": "\"\"\"\nTag getter. Returns an instance's tags if accessed on an instance, and\nall of a model's tags if called on a class. That is, this model::\n\n   class Link(models.Model):\n       ...\n       tags = TagField()\n\nLets you do both of these::\n\n   >>> l = Link.objects.get(...)\n   >>> l.tags\n   'tag1 tag2 tag3'\n\n   >>> Link.tags\n   'tag1 tag2 tag3 tag4'\n\n\"\"\"\n# Handle access on the model (i.e. Link.tags)\n", "func_signal": "def __get__(self, instance, owner=None):\n", "code": "if instance is None:\n    return edit_string_for_tags(Tag.objects.usage_for_model(owner))\n\ntags = self._get_instance_tag_cache(instance)\nif tags is None:\n    if instance.pk is None:\n        self._set_instance_tag_cache(instance, '')\n    else:\n        self._set_instance_tag_cache(\n            instance, edit_string_for_tags(Tag.objects.get_for_object(instance)))\nreturn self._get_instance_tag_cache(instance)", "path": "contrib\\tagging-trunk\\tagging\\fields.py", "repo_name": "KartashAlex/django_social", "stars": 0, "license": "None", "language": "python", "size": 12740}
{"docstring": "\"\"\"\nCreates the new ``User`` and ``RegistrationProfile``, and\nreturns the ``User``.\n\nThis is essentially a light wrapper around\n``RegistrationProfile.objects.create_inactive_user()``,\nfeeding it the form data and a profile callback (see the\ndocumentation on ``create_inactive_user()`` for details) if\nsupplied.\n\n\"\"\"\n", "func_signal": "def save(self, profile_callback=None):\n", "code": "new_user = RegistrationProfile.objects.create_inactive_user(username=self.cleaned_data['username'],\n                                                            password=self.cleaned_data['password1'],\n                                                            email=self.cleaned_data['email'],\n                                                            profile_callback=profile_callback)\nreturn new_user", "path": "contrib\\registration\\forms.py", "repo_name": "KartashAlex/django_social", "stars": 0, "license": "None", "language": "python", "size": 12740}
{"docstring": "\"\"\"\nValidates that the username is alphanumeric and is not already\nin use.\n\n\"\"\"\n", "func_signal": "def clean_username(self):\n", "code": "if not alnum_re.search(self.cleaned_data['username']):\n    raise forms.ValidationError(_(u'Usernames can only contain letters, numbers and underscores'))\ntry:\n    user = User.objects.get(username__exact=self.cleaned_data['username'])\nexcept User.DoesNotExist:\n    return self.cleaned_data['username']\nraise forms.ValidationError(_(u'This username is already taken. Please choose another.'))", "path": "contrib\\registration\\forms.py", "repo_name": "KartashAlex/django_social", "stars": 0, "license": "None", "language": "python", "size": 12740}
{"docstring": "\"\"\"\nSet an object's tags.\n\"\"\"\n", "func_signal": "def __set__(self, instance, value):\n", "code": "if instance is None:\n    raise AttributeError(_('%s can only be set on instances.') % self.name)\nif settings.FORCE_LOWERCASE_TAGS and value is not None:\n    value = value.lower()\nself._set_instance_tag_cache(instance, value)", "path": "contrib\\tagging-trunk\\tagging\\fields.py", "repo_name": "KartashAlex/django_social", "stars": 0, "license": "None", "language": "python", "size": 12740}
{"docstring": "\"\"\"\nValidates that the supplied email address is unique for the\nsite.\n\n\"\"\"\n", "func_signal": "def clean_email(self):\n", "code": "try:\n    user = User.objects.get(email__exact=self.cleaned_data['email'])\nexcept User.DoesNotExist:\n    return self.cleaned_data['email']\nraise forms.ValidationError(_(u'This email address is already in use. Please supply a different email address.'))", "path": "contrib\\registration\\forms.py", "repo_name": "KartashAlex/django_social", "stars": 0, "license": "None", "language": "python", "size": 12740}
{"docstring": "\"\"\"\nCreate a ``QuerySet`` containing instances of the specified\nmodel associated with *any* of the given list of tags.\n\"\"\"\n", "func_signal": "def get_union_by_model(self, queryset_or_model, tags):\n", "code": "tags = get_tag_list(tags)\ntag_count = len(tags)\nqueryset, model = get_queryset_and_model(queryset_or_model)\n\nif not tag_count:\n    return model._default_manager.none()\n\nmodel_table = qn(model._meta.db_table)\n# This query selects the ids of all objects which have any of\n# the given tags.\nquery = \"\"\"\nSELECT %(model_pk)s\nFROM %(model)s, %(tagged_item)s\nWHERE %(tagged_item)s.content_type_id = %(content_type_id)s\n  AND %(tagged_item)s.tag_id IN (%(tag_id_placeholders)s)\n  AND %(model_pk)s = %(tagged_item)s.object_id\nGROUP BY %(model_pk)s\"\"\" % {\n    'model_pk': '%s.%s' % (model_table, qn(model._meta.pk.column)),\n    'model': model_table,\n    'tagged_item': qn(self.model._meta.db_table),\n    'content_type_id': ContentType.objects.get_for_model(model).pk,\n    'tag_id_placeholders': ','.join(['%s'] * tag_count),\n}\n\ncursor = connection.cursor()\ncursor.execute(query, [tag.pk for tag in tags])\nobject_ids = [row[0] for row in cursor.fetchall()]\nif len(object_ids) > 0:\n    return queryset.filter(pk__in=object_ids)\nelse:\n    return model._default_manager.none()", "path": "tagging\\models.py", "repo_name": "KartashAlex/django_social", "stars": 0, "license": "None", "language": "python", "size": 12740}
{"docstring": "\"\"\"\nObtain a list of tags related to a given list of tags - that\nis, other tags used by items which have all the given tags.\n\nIf ``counts`` is True, a ``count`` attribute will be added to\neach tag, indicating the number of items which have it in\naddition to the given list of tags.\n\nIf ``min_count`` is given, only tags which have a ``count``\ngreater than or equal to ``min_count`` will be returned.\nPassing a value for ``min_count`` implies ``counts=True``.\n\"\"\"\n", "func_signal": "def related_for_model(self, tags, model, counts=False, min_count=None):\n", "code": "if min_count is not None: counts = True\ntags = get_tag_list(tags)\ntag_count = len(tags)\ntagged_item_table = qn(TaggedItem._meta.db_table)\nquery = \"\"\"\nSELECT %(tag)s.id, %(tag)s.name%(count_sql)s\nFROM %(tagged_item)s INNER JOIN %(tag)s ON %(tagged_item)s.tag_id = %(tag)s.id\nWHERE %(tagged_item)s.content_type_id = %(content_type_id)s\n  AND %(tagged_item)s.object_id IN\n  (\n      SELECT %(tagged_item)s.object_id\n      FROM %(tagged_item)s, %(tag)s\n      WHERE %(tagged_item)s.content_type_id = %(content_type_id)s\n        AND %(tag)s.id = %(tagged_item)s.tag_id\n        AND %(tag)s.id IN (%(tag_id_placeholders)s)\n      GROUP BY %(tagged_item)s.object_id\n      HAVING COUNT(%(tagged_item)s.object_id) = %(tag_count)s\n  )\n  AND %(tag)s.id NOT IN (%(tag_id_placeholders)s)\nGROUP BY %(tag)s.id, %(tag)s.name\n%(min_count_sql)s\nORDER BY %(tag)s.name ASC\"\"\" % {\n    'tag': qn(self.model._meta.db_table),\n    'count_sql': counts and ', COUNT(%s.object_id)' % tagged_item_table or '',\n    'tagged_item': tagged_item_table,\n    'content_type_id': ContentType.objects.get_for_model(model).pk,\n    'tag_id_placeholders': ','.join(['%s'] * tag_count),\n    'tag_count': tag_count,\n    'min_count_sql': min_count is not None and ('HAVING COUNT(%s.object_id) >= %%s' % tagged_item_table) or '',\n}\n\nparams = [tag.pk for tag in tags] * 2\nif min_count is not None:\n    params.append(min_count)\n\ncursor = connection.cursor()\ncursor.execute(query, params)\nrelated = []\nfor row in cursor.fetchall():\n    tag = self.model(*row[:2])\n    if counts is True:\n        tag.count = row[2]\n    related.append(tag)\nreturn related", "path": "tagging\\models.py", "repo_name": "KartashAlex/django_social", "stars": 0, "license": "None", "language": "python", "size": 12740}
{"docstring": "\"\"\"\nDelete all files within a path which match the thumbnails pattern.\n\nBy default, matching files from all sub-directories are also removed. To\nonly remove from the path directory, set recursive=False.\n\"\"\"\n", "func_signal": "def delete_all_thumbnails(path, recursive=True):\n", "code": "total = 0\nfor thumbs in all_thumbnails(path, recursive=recursive).values():\n    total += _delete_using_thumbs_list(thumbs)\nreturn total", "path": "sorl\\thumbnail\\utils.py", "repo_name": "KartashAlex/django_social", "stars": 0, "license": "None", "language": "python", "size": 12740}
{"docstring": "\"\"\"\nObtain a list of tags associated with instances of the given\nModel class.\n\nIf ``counts`` is True, a ``count`` attribute will be added to\neach tag, indicating how many times it has been used against\nthe Model class in question.\n\nIf ``min_count`` is given, only tags which have a ``count``\ngreater than or equal to ``min_count`` will be returned.\nPassing a value for ``min_count`` implies ``counts=True``.\n\nTo limit the tags (and counts, if specified) returned to those\nused by a subset of the Model's instances, pass a dictionary\nof field lookups to be applied to the given Model as the\n``filters`` argument.\n\"\"\"\n", "func_signal": "def usage_for_model(self, model, counts=False, min_count=None, filters=None):\n", "code": "if filters is None: filters = {}\n\nqueryset = model._default_manager.filter()\nfor f in filters.items():\n    queryset.query.add_filter(f)\nusage = self.usage_for_queryset(queryset, counts, min_count)\n\nreturn usage", "path": "tagging\\models.py", "repo_name": "KartashAlex/django_social", "stars": 0, "license": "None", "language": "python", "size": 12740}
{"docstring": "\"\"\"\nValidates that the user accepted the Terms of Service.\n\n\"\"\"\n", "func_signal": "def clean_tos(self):\n", "code": "if self.cleaned_data.get('tos', False):\n    return self.cleaned_data['tos']\nraise forms.ValidationError(_(u'You must agree to the terms to register'))", "path": "contrib\\registration\\forms.py", "repo_name": "KartashAlex/django_social", "stars": 0, "license": "None", "language": "python", "size": 12740}
{"docstring": "\"\"\"\nCreate a ``QuerySet`` containing instances of the specified\nmodel associated with a given tag or list of tags.\n\"\"\"\n", "func_signal": "def get_by_model(self, queryset_or_model, tags):\n", "code": "tags = get_tag_list(tags)\ntag_count = len(tags)\nif tag_count == 0:\n    # No existing tags were given\n    queryset, model = get_queryset_and_model(queryset_or_model)\n    return model._default_manager.none()\nelif tag_count == 1:\n    # Optimisation for single tag - fall through to the simpler\n    # query below.\n    tag = tags[0]\nelse:\n    return self.get_intersection_by_model(queryset_or_model, tags)\n\nqueryset, model = get_queryset_and_model(queryset_or_model)\ncontent_type = ContentType.objects.get_for_model(model)\nopts = self.model._meta\ntagged_item_table = qn(opts.db_table)\nreturn queryset.extra(\n    tables=[opts.db_table],\n    where=[\n        '%s.content_type_id = %%s' % tagged_item_table,\n        '%s.tag_id = %%s' % tagged_item_table,\n        '%s.%s = %s.object_id' % (qn(model._meta.db_table),\n                                  qn(model._meta.pk.column),\n                                  tagged_item_table)\n    ],\n    params=[content_type.pk, tag.pk],\n)", "path": "tagging\\models.py", "repo_name": "KartashAlex/django_social", "stars": 0, "license": "None", "language": "python", "size": 12740}
{"docstring": "\"\"\"\nTag getter. Returns an instance's tags if accessed on an instance, and\nall of a model's tags if called on a class. That is, this model::\n\n   class Link(models.Model):\n       ...\n       tags = TagField()\n\nLets you do both of these::\n\n   >>> l = Link.objects.get(...)\n   >>> l.tags\n   'tag1 tag2 tag3'\n\n   >>> Link.tags\n   'tag1 tag2 tag3 tag4'\n\n\"\"\"\n# Handle access on the model (i.e. Link.tags)\n", "func_signal": "def __get__(self, instance, owner=None):\n", "code": "if instance is None:\n    return edit_string_for_tags(Tag.objects.usage_for_model(owner))\n\ntags = self._get_instance_tag_cache(instance)\nif tags is None:\n    if instance.pk is None:\n        self._set_instance_tag_cache(instance, '')\n    else:\n        self._set_instance_tag_cache(\n            instance, edit_string_for_tags(Tag.objects.get_for_object(instance)))\nreturn self._get_instance_tag_cache(instance)", "path": "tagging\\fields.py", "repo_name": "KartashAlex/django_social", "stars": 0, "license": "None", "language": "python", "size": 12740}
{"docstring": "\"\"\"\nSave tags back to the database\n\"\"\"\n", "func_signal": "def _save(self, signal, sender, instance):\n", "code": "tags = self._get_instance_tag_cache(instance)\nif tags is not None:\n    Tag.objects.update_tags(instance, tags)", "path": "contrib\\tagging-trunk\\tagging\\fields.py", "repo_name": "KartashAlex/django_social", "stars": 0, "license": "None", "language": "python", "size": 12740}
{"docstring": "\"\"\"\nAssociates the given object with a tag.\n\"\"\"\n", "func_signal": "def add_tag(self, obj, tag_name):\n", "code": "tag_names = parse_tag_input(tag_name)\nif not len(tag_names):\n    raise AttributeError(_('No tags were given: \"%s\".') % tag_name)\nif len(tag_names) > 1:\n    raise AttributeError(_('Multiple tags were given: \"%s\".') % tag_name)\ntag_name = tag_names[0]\nif settings.FORCE_LOWERCASE_TAGS:\n    tag_name = tag_name.lower()\ntag, created = self.get_or_create(name=tag_name)\nctype = ContentType.objects.get_for_model(obj)\nTaggedItem._default_manager.get_or_create(\n    tag=tag, content_type=ctype, object_id=obj.pk)", "path": "tagging\\models.py", "repo_name": "KartashAlex/django_social", "stars": 0, "license": "None", "language": "python", "size": 12740}
{"docstring": "\"\"\"\nChecks the supplied email address against a list of known free\nwebmail domains.\n\n\"\"\"\n", "func_signal": "def clean_email(self):\n", "code": "email_domain = self.cleaned_data['email'].split('@')[1]\nif email_domain in self.bad_domains:\n    raise forms.ValidationError(_(u'Registration using free email addresses is prohibited. Please supply a different email address.'))\nreturn self.cleaned_data['email']", "path": "contrib\\registration\\forms.py", "repo_name": "KartashAlex/django_social", "stars": 0, "license": "None", "language": "python", "size": 12740}
{"docstring": "\"\"\"\nPerform the custom SQL query for ``usage_for_model`` and\n``usage_for_queryset``.\n\"\"\"\n", "func_signal": "def _get_usage(self, model, counts=False, min_count=None, extra_joins=None, extra_criteria=None, params=None):\n", "code": "if min_count is not None: counts = True\n\nmodel_table = qn(model._meta.db_table)\nmodel_pk = '%s.%s' % (model_table, qn(model._meta.pk.column))\nquery = \"\"\"\nSELECT DISTINCT %(tag)s.id, %(tag)s.name%(count_sql)s\nFROM\n    %(tag)s\n    INNER JOIN %(tagged_item)s\n        ON %(tag)s.id = %(tagged_item)s.tag_id\n    INNER JOIN %(model)s\n        ON %(tagged_item)s.object_id = %(model_pk)s\n    %%s\nWHERE %(tagged_item)s.content_type_id = %(content_type_id)s\n    %%s\nGROUP BY %(tag)s.id, %(tag)s.name\n%%s\nORDER BY %(tag)s.name ASC\"\"\" % {\n    'tag': qn(self.model._meta.db_table),\n    'count_sql': counts and (', COUNT(%s)' % model_pk) or '',\n    'tagged_item': qn(TaggedItem._meta.db_table),\n    'model': model_table,\n    'model_pk': model_pk,\n    'content_type_id': ContentType.objects.get_for_model(model).pk,\n}\n\nmin_count_sql = ''\nif min_count is not None:\n    min_count_sql = 'HAVING COUNT(%s) >= %%s' % model_pk\n    params.append(min_count)\n\ncursor = connection.cursor()\ncursor.execute(query % (extra_joins, extra_criteria, min_count_sql), params)\ntags = []\nfor row in cursor.fetchall():\n    t = self.model(*row[:2])\n    if counts:\n        t.count = row[2]\n    tags.append(t)\nreturn tags", "path": "tagging\\models.py", "repo_name": "KartashAlex/django_social", "stars": 0, "license": "None", "language": "python", "size": 12740}
{"docstring": "\"\"\"\nReturn a dictionary referencing all files which match the thumbnail format.\n\nEach key is a source image filename, relative to path.\nEach value is a list of dictionaries as explained in `thumbnails_for_file`.\n\"\"\"\n", "func_signal": "def all_thumbnails(path, recursive=True, prefix=None, subdir=None):\n", "code": "prefix = get_thumbnail_setting('PREFIX', prefix)\nsubdir = get_thumbnail_setting('SUBDIR', subdir)\nthumbnail_files = {}\nif not path.endswith('/'):\n    path = '%s/' % path\nlen_path = len(path)\nif recursive:\n    all = os.walk(path)\nelse:\n    files = []\n    for file in os.listdir(path):\n        if os.path.isfile(os.path.join(path, file)):\n            files.append(file)\n    all = [(path, [], files)]\nfor dir_, subdirs, files in all:\n    rel_dir = dir_[len_path:]\n    for file in files:\n        thumb = re_thumbnail_file.match(file)\n        if not thumb:\n            continue\n        d = thumb.groupdict()\n        source_filename = d.pop('source_filename')\n        if prefix:\n            source_path, source_filename = os.path.split(source_filename)\n            if not source_filename.startswith(prefix):\n                continue\n            source_filename = os.path.join(source_path,\n                source_filename[len(prefix):])\n        d['options'] = d['options'] and d['options'].split('_') or []\n        if subdir and rel_dir.endswith(subdir):\n            rel_dir = rel_dir[:-len(subdir)] \n        # Corner-case bug: if the filename didn't have an extension but did\n        # have an underscore, the last underscore will get converted to a\n        # '.'.\n        m = re.match(r'(.*)_(.*)', source_filename)\n        if m:\n             source_filename = '%s.%s' % m.groups()\n        filename = os.path.join(rel_dir, source_filename)\n        thumbnail_file = thumbnail_files.setdefault(filename, [])\n        d['filename'] = os.path.join(dir_, file)\n        thumbnail_file.append(d)\nreturn thumbnail_files", "path": "sorl\\thumbnail\\utils.py", "repo_name": "KartashAlex/django_social", "stars": 0, "license": "None", "language": "python", "size": 12740}
{"docstring": "\"\"\"\nObtain a list of tags associated with instances of a model\ncontained in the given queryset.\n\nIf ``counts`` is True, a ``count`` attribute will be added to\neach tag, indicating how many times it has been used against\nthe Model class in question.\n\nIf ``min_count`` is given, only tags which have a ``count``\ngreater than or equal to ``min_count`` will be returned.\nPassing a value for ``min_count`` implies ``counts=True``.\n\"\"\"\n\n", "func_signal": "def usage_for_queryset(self, queryset, counts=False, min_count=None):\n", "code": "extra_joins = ' '.join(queryset.query.get_from_clause()[0][1:])\nwhere, params = queryset.query.where.as_sql()\nif where:\n    extra_criteria = 'AND %s' % where\nelse:\n    extra_criteria = ''\nreturn self._get_usage(queryset.model, counts, min_count, extra_joins, extra_criteria, params)", "path": "tagging\\models.py", "repo_name": "KartashAlex/django_social", "stars": 0, "license": "None", "language": "python", "size": 12740}
{"docstring": "\"\"\"\nCreate a ``QuerySet`` containing instances of the specified\nmodel associated with *all* of the given list of tags.\n\"\"\"\n", "func_signal": "def get_intersection_by_model(self, queryset_or_model, tags):\n", "code": "tags = get_tag_list(tags)\ntag_count = len(tags)\nqueryset, model = get_queryset_and_model(queryset_or_model)\n\nif not tag_count:\n    return model._default_manager.none()\n\nmodel_table = qn(model._meta.db_table)\n# This query selects the ids of all objects which have all the\n# given tags.\nquery = \"\"\"\nSELECT %(model_pk)s\nFROM %(model)s, %(tagged_item)s\nWHERE %(tagged_item)s.content_type_id = %(content_type_id)s\n  AND %(tagged_item)s.tag_id IN (%(tag_id_placeholders)s)\n  AND %(model_pk)s = %(tagged_item)s.object_id\nGROUP BY %(model_pk)s\nHAVING COUNT(%(model_pk)s) = %(tag_count)s\"\"\" % {\n    'model_pk': '%s.%s' % (model_table, qn(model._meta.pk.column)),\n    'model': model_table,\n    'tagged_item': qn(self.model._meta.db_table),\n    'content_type_id': ContentType.objects.get_for_model(model).pk,\n    'tag_id_placeholders': ','.join(['%s'] * tag_count),\n    'tag_count': tag_count,\n}\n\ncursor = connection.cursor()\ncursor.execute(query, [tag.pk for tag in tags])\nobject_ids = [row[0] for row in cursor.fetchall()]\nif len(object_ids) > 0:\n    return queryset.filter(pk__in=object_ids)\nelse:\n    return model._default_manager.none()", "path": "tagging\\models.py", "repo_name": "KartashAlex/django_social", "stars": 0, "license": "None", "language": "python", "size": 12740}
{"docstring": "\"\"\"Return a list of rooms on the server\"\"\"\n", "func_signal": "def ROOMS(self):\n", "code": "house = self.factory.house\nnames = [ x for x in house.lookup_namespace('room') ]\nself.sendMsg('ROOMS', *names)", "path": "haver\\server\\talker.py", "repo_name": "dylanwh/haver-server-python", "stars": 1, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Destroy a room. Only the owner can do this.\"\"\"\n", "func_signal": "def CLOSE(self, name):\n", "code": "assert_name(name)\nassert_name_unreserved(name)\n\nhouse = self.factory.house\nroom = house.lookup('room', name)\nif room['owner'] != self.user.name:\n\traise Fail('access.owner', room.name, room['owner'], self.user.name)\n\nfor user in room:\n\tuser.part(room.name, 'close', self.user.name)\nhouse.remove(room)\nself.sendMsg('CLOSE', name)", "path": "haver\\server\\talker.py", "repo_name": "dylanwh/haver-server-python", "stars": 1, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Disconnect from the server.\"\"\"\n", "func_signal": "def BYE(self, detail = None):\n", "code": "if detail is None:\n\tself.disconnect('bye')\nelse:\n\tself.disconnect('bye', detail)", "path": "haver\\server\\talker.py", "repo_name": "dylanwh/haver-server-python", "stars": 1, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"DEPRECATED. Get a listing of information for a particular thing.\"\"\"\n", "func_signal": "def INFO(self, ns, name):\n", "code": "house = self.factory.house\nthing = house.lookup(ns, name)\nself.sendMsg('INFO', ns, name, *thing.info())", "path": "haver\\server\\talker.py", "repo_name": "dylanwh/haver-server-python", "stars": 1, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Query metadata about a user\"\"\"\n", "func_signal": "def ROOMINFO(self, name):\n", "code": "house = self.factory.house\nroom = house.lookup(\"room\", name)\nself.sendMsg('ROOMINFO', name, *room.info())", "path": "haver\\server\\talker.py", "repo_name": "dylanwh/haver-server-python", "stars": 1, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Create an SSL context.\n\nThis is a sample implementation that loads a certificate from a file\ncalled 'server.pem'.\"\"\"\n", "func_signal": "def getContext(self):\n", "code": "ctx = SSL.Context(SSL.SSLv23_METHOD)\nctx.use_certificate_file('server.pem')\nctx.use_privatekey_file('server.pem')\nreturn ctx", "path": "haver\\server\\ssl.py", "repo_name": "dylanwh/haver-server-python", "stars": 1, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Return info about a client command\"\"\"\n", "func_signal": "def command(self, name):\n", "code": "cmd = getattr(self.talker, name)\n\ntry: exten = cmd.extension\nexcept AttributeError: exten = 'core'\n\ntry: failures = cmd.failures\nexcept AttributeError: failures = []\ndesc = inspect.getdoc(cmd)\nif desc is None: desc = \"<none>\"\nif len(failures) == 0:\n\tfailures = ['<none>']\nreturn dict(\n\tNAME      = cmd.__name__.replace('_', ':'),\n\tEXTENSION = exten,\n\tFAILURES  = \",\".join(failures),\n\tDESC      = desc,\n\tARGS      = getargs(cmd)\n)", "path": "haver\\server\\help.py", "repo_name": "dylanwh/haver-server-python", "stars": 1, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Query metadata about a user\"\"\"\n", "func_signal": "def USERINFO(self, name):\n", "code": "house = self.factory.house\nuser = house.lookup(\"user\", name)\nself.sendMsg('USERINFO', name, *user.info())", "path": "haver\\server\\talker.py", "repo_name": "dylanwh/haver-server-python", "stars": 1, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Join room {name}\"\"\"\n", "func_signal": "def JOIN(self, name):\n", "code": "assert_name(name)\nhouse = self.factory.house\nroom  = house.lookup('room', name)\nif room['secure'] == 'yes' and self.user['secure'] != 'yes':\n\traise Fail('insecure')\nroom.join(self.user)", "path": "haver\\server\\talker.py", "repo_name": "dylanwh/haver-server-python", "stars": 1, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Associate a client connection with name {name}. May not be used after the server sends HELLO.\"\"\"\n", "func_signal": "def IDENT(self, name):\n", "code": "house = self.factory.house\nassert_name(name)\nassert_name_unreserved(name)\nuser = User(name, self)\nhouse.add(user)\nself.sendMsg('HELLO', name, str(self.addr.host))\n\nself.init(user)\nreturn 'normal'", "path": "haver\\server\\talker.py", "repo_name": "dylanwh/haver-server-python", "stars": 1, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Respond to a PING.\"\"\"\n", "func_signal": "def PONG(self, token):\n", "code": "if self.tardy is None:\n\traise Bork(\"You already did that.\")\nelse:\n\tself.tardy = None", "path": "haver\\server\\talker.py", "repo_name": "dylanwh/haver-server-python", "stars": 1, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Part room {name}\"\"\"\n", "func_signal": "def PART(self, name):\n", "code": "assert_name(name)\nhouse = self.factory.house\nroom  = house.lookup('room', name)\nroom.part(self.user, 'normal')", "path": "haver\\server\\talker.py", "repo_name": "dylanwh/haver-server-python", "stars": 1, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Clients must issue this message before any others.\"\"\"\n", "func_signal": "def HAVER(self, version, extensions = '', *rest):\n", "code": "self.version = version\nself.extensions = set(extensions.split(','))\nver = \"%s/%s\" % (haver.server.name, haver.server.version)\nprint(type(help.extensions))\nself.sendMsg('HAVER', self.factory.house.host, ver, \",\".join(help.extensions))\nreturn 'login'", "path": "haver\\server\\talker.py", "repo_name": "dylanwh/haver-server-python", "stars": 1, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Return a list of users on the server.\"\"\"\n", "func_signal": "def USERS(self):\n", "code": "house = self.factory.house\nnames = [ x for x in house.lookup_namespace('user') ]\nself.sendMsg('USERS', *names)", "path": "haver\\server\\talker.py", "repo_name": "dylanwh/haver-server-python", "stars": 1, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Send a public message\"\"\"\n", "func_signal": "def IN(self, name, kind, msg, *rest):\n", "code": "assert_name(name)\nhouse = self.factory.house\nroom  = house.lookup('room', name)\nroom.sendMsg('IN', room.name, self.user.name, kind, msg, *rest)\nself.user.updateIdle()", "path": "haver\\server\\talker.py", "repo_name": "dylanwh/haver-server-python", "stars": 1, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Send a private message\"\"\"\n", "func_signal": "def TO(self, name, kind, msg, *rest):\n", "code": "assert_name(name)\nhouse = self.factory.house\nuser  = house.lookup('user', name)\nuser.sendMsg('FROM', self.user.name, kind, msg, *rest)\nself.user.updateIdle()", "path": "haver\\server\\talker.py", "repo_name": "dylanwh/haver-server-python", "stars": 1, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Called every once and a while. Issues a ping if this client hasn't sent a command recently\"\"\"\n", "func_signal": "def checkPing(self):\n", "code": "now      = time.time()\nduration = int (now - self.lastCmd)\n\nif self.phase != 'normal':\n\tself.tardy = None\n\treturn\n\nif self.tardy is not None:\n\tself.sendMsg('BYE', 'ping')\n\tself.disconnect('ping')\n\treturn\n\nif duration > self.pingTime:\n\tself.sendMsg('PING', 'foo')\n\tself.tardy = 'foo'", "path": "haver\\server\\talker.py", "repo_name": "dylanwh/haver-server-python", "stars": 1, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Create a new room. This command may be restricted to server-admins only.\"\"\"\n", "func_signal": "def OPEN(self, name):\n", "code": "assert_name(name)\nassert_name_unreserved(name)\n\nhouse = self.factory.house\nroom  = Room(name, owner = self.user.name)\nhouse.add(room)\nself.sendMsg('OPEN', name)", "path": "haver\\server\\talker.py", "repo_name": "dylanwh/haver-server-python", "stars": 1, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Return a list of users in the channel $name\"\"\"\n", "func_signal": "def USERSOF(self, name):\n", "code": "house = self.factory.house\nroom = house.lookup('room', name)\nnames = [ x.name for x in room.users ]\nself.sendMsg('USERSOF', room.name,  *names)", "path": "haver\\server\\talker.py", "repo_name": "dylanwh/haver-server-python", "stars": 1, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"ehird's tagging thing\"\"\"\n", "func_signal": "def TAG(self, tag, cmd, *args):\n", "code": "self.tag = tag\nself.invoke(cmd, args)", "path": "haver\\server\\talker.py", "repo_name": "dylanwh/haver-server-python", "stars": 1, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Returns the parents of this Tag that match the given\ncriteria.\"\"\"\n\n", "func_signal": "def findParents(self, name=None, attrs={}, limit=None, **kwargs):\n", "code": "return self._findAll(name, attrs, None, limit, self.parentGenerator,\n                     **kwargs)", "path": "BeautifulSoup.py", "repo_name": "jwheare/bizparse", "stars": 1, "license": "bsd-3-clause", "language": "python", "size": 128}
{"docstring": "#print \"Push\", tag.name\n", "func_signal": "def pushTag(self, tag):\n", "code": "if self.currentTag:\n    self.currentTag.contents.append(tag)\nself.tagStack.append(tag)\nself.currentTag = self.tagStack[-1]", "path": "BeautifulSoup.py", "repo_name": "jwheare/bizparse", "stars": 1, "license": "bsd-3-clause", "language": "python", "size": 128}
{"docstring": "\"\"\"Changes a MS smart quote character to an XML or HTML\nentity.\"\"\"\n", "func_signal": "def _subMSChar(self, match):\n", "code": "orig = match.group(1)\nsub = self.MS_CHARS.get(orig)\nif type(sub) == types.TupleType:\n    if self.smartQuotesTo == 'xml':\n        sub = '&#x'.encode() + sub[1].encode() + ';'.encode()\n    else:\n        sub = '&'.encode() + sub[0].encode() + ';'.encode()\nelse:\n    sub = sub.encode()\nreturn sub", "path": "BeautifulSoup.py", "repo_name": "jwheare/bizparse", "stars": 1, "license": "bsd-3-clause", "language": "python", "size": 128}
{"docstring": "\"\"\"Encodes an object to a string in some encoding, or to Unicode.\n.\"\"\"\n", "func_signal": "def toEncoding(self, s, encoding=None):\n", "code": "if isinstance(s, unicode):\n    if encoding:\n        s = s.encode(encoding)\nelif isinstance(s, str):\n    if encoding:\n        s = s.encode(encoding)\n    else:\n        s = unicode(s)\nelse:\n    if encoding:\n        s  = self.toEncoding(str(s), encoding)\n    else:\n        s = unicode(s)\nreturn s", "path": "BeautifulSoup.py", "repo_name": "jwheare/bizparse", "stars": 1, "license": "bsd-3-clause", "language": "python", "size": 128}
{"docstring": "\"\"\"Returns true iff this tag has the same name, the same attributes,\nand the same contents (recursively) as the given tag.\n\nNOTE: right now this will return false if two tags have the\nsame attributes in a different order. Should this be fixed?\"\"\"\n", "func_signal": "def __eq__(self, other):\n", "code": "if not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):\n    return False\nfor i in range(0, len(self.contents)):\n    if self.contents[i] != other.contents[i]:\n        return False\nreturn True", "path": "BeautifulSoup.py", "repo_name": "jwheare/bizparse", "stars": 1, "license": "bsd-3-clause", "language": "python", "size": 128}
{"docstring": "\"\"\"Given a document, tries to detect its XML encoding.\"\"\"\n", "func_signal": "def _detectEncoding(self, xml_data, isHTML=False):\n", "code": "xml_encoding = sniffed_xml_encoding = None\ntry:\n    if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n        # EBCDIC\n        xml_data = self._ebcdic_to_ascii(xml_data)\n    elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n        # UTF-16BE\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') \\\n             and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16BE with BOM\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n        # UTF-16LE\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and \\\n             (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16LE with BOM\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n        # UTF-32BE\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n        # UTF-32LE\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n        # UTF-32BE with BOM\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n        # UTF-32LE with BOM\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n    elif xml_data[:3] == '\\xef\\xbb\\xbf':\n        # UTF-8 with BOM\n        sniffed_xml_encoding = 'utf-8'\n        xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n    else:\n        sniffed_xml_encoding = 'ascii'\n        pass\nexcept:\n    xml_encoding_match = None\nxml_encoding_re = '^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>'.encode()\nxml_encoding_match = re.compile(xml_encoding_re).match(xml_data)\nif not xml_encoding_match and isHTML:\n    meta_re = '<\\s*meta[^>]+charset=([^>]*?)[;\\'\">]'.encode()\n    regexp = re.compile(meta_re, re.I)\n    xml_encoding_match = regexp.search(xml_data)\nif xml_encoding_match is not None:\n    xml_encoding = xml_encoding_match.groups()[0].decode(\n        'ascii').lower()\n    if isHTML:\n        self.declaredHTMLEncoding = xml_encoding\n    if sniffed_xml_encoding and \\\n       (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',\n                         'iso-10646-ucs-4', 'ucs-4', 'csucs4',\n                         'utf-16', 'utf-32', 'utf_16', 'utf_32',\n                         'utf16', 'u16')):\n        xml_encoding = sniffed_xml_encoding\nreturn xml_data, xml_encoding, sniffed_xml_encoding", "path": "BeautifulSoup.py", "repo_name": "jwheare/bizparse", "stars": 1, "license": "bsd-3-clause", "language": "python", "size": 128}
{"docstring": "\"\"\"Returns the closest sibling to this Tag that matches the\ngiven criteria and appears after this Tag in the document.\"\"\"\n", "func_signal": "def findNextSibling(self, name=None, attrs={}, text=None, **kwargs):\n", "code": "return self._findOne(self.findNextSiblings, name, attrs, text,\n                     **kwargs)", "path": "BeautifulSoup.py", "repo_name": "jwheare/bizparse", "stars": 1, "license": "bsd-3-clause", "language": "python", "size": 128}
{"docstring": "# Convert the document to Unicode.\n", "func_signal": "def _feed(self, inDocumentEncoding=None, isHTML=False):\n", "code": "markup = self.markup\nif isinstance(markup, unicode):\n    if not hasattr(self, 'originalEncoding'):\n        self.originalEncoding = None\nelse:\n    dammit = UnicodeDammit\\\n             (markup, [self.fromEncoding, inDocumentEncoding],\n              smartQuotesTo=self.smartQuotesTo, isHTML=isHTML)\n    markup = dammit.unicode\n    self.originalEncoding = dammit.originalEncoding\n    self.declaredHTMLEncoding = dammit.declaredHTMLEncoding\nif markup:\n    if self.markupMassage:\n        if not isList(self.markupMassage):\n            self.markupMassage = self.MARKUP_MASSAGE\n        for fix, m in self.markupMassage:\n            markup = fix.sub(m, markup)\n        # TODO: We get rid of markupMassage so that the\n        # soup object can be deepcopied later on. Some\n        # Python installations can't copy regexes. If anyone\n        # was relying on the existence of markupMassage, this\n        # might cause problems.\n        del(self.markupMassage)\nself.builder.reset()\n\nself.builder.feed(markup)\n# Close out any unfinished strings and close all the open tags.\nself.endData()\nwhile self.currentTag.name != self.ROOT_TAG_NAME:\n    self.popTag()", "path": "BeautifulSoup.py", "repo_name": "jwheare/bizparse", "stars": 1, "license": "bsd-3-clause", "language": "python", "size": 128}
{"docstring": "\"\"\"Returns either the given Unicode string or its encoding.\"\"\"\n", "func_signal": "def sob(unicode, encoding):\n", "code": "if encoding is None:\n    return unicode\nelse:\n    return unicode.encode(encoding)", "path": "BeautifulSoup.py", "repo_name": "jwheare/bizparse", "stars": 1, "license": "bsd-3-clause", "language": "python", "size": 128}
{"docstring": "\"\"\"Treat a bogus SGML declaration as raw data. Treat a CDATA\ndeclaration as a CData object.\"\"\"\n", "func_signal": "def parse_declaration(self, i):\n", "code": "j = None\nif self.rawdata[i:i+9] == '<![CDATA[':\n     k = self.rawdata.find(']]>', i)\n     if k == -1:\n         k = len(self.rawdata)\n     data = self.rawdata[i+9:k]\n     j = k+3\n     self._toStringSubclass(data, CData)\nelse:\n    try:\n        j = HTMLParser.parse_declaration(self, i)\n    except HTMLParseError:\n        toHandle = self.rawdata[i:]\n        self.handle_data(toHandle)\n        j = i + len(toHandle)\nreturn j", "path": "BeautifulSoup.py", "repo_name": "jwheare/bizparse", "stars": 1, "license": "bsd-3-clause", "language": "python", "size": 128}
{"docstring": "\"\"\"Sets up the initial relations between this element and\nother elements.\"\"\"\n", "func_signal": "def setup(self, parent=None, previous=None):\n", "code": "self.parent = parent\nself.previous = previous\nself.next = None\nself.previousSibling = None\nself.nextSibling = None\nif self.parent and self.parent.contents:\n    self.previousSibling = self.parent.contents[-1]\n    self.previousSibling.nextSibling = self", "path": "BeautifulSoup.py", "repo_name": "jwheare/bizparse", "stars": 1, "license": "bsd-3-clause", "language": "python", "size": 128}
{"docstring": "\"\"\"Handle entity references as data, possibly converting known\nHTML and/or XML entity references to the corresponding Unicode\ncharacters.\"\"\"\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "data = None\nif self.soup.convertHTMLEntities:\n    try:\n        data = unichr(name2codepoint[ref])\n    except KeyError:\n        pass\n\nif not data and self.soup.convertXMLEntities:\n        data = self.soup.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref)\n\nif not data and self.soup.convertHTMLEntities and \\\n    not self.soup.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref):\n        # TODO: We've got a problem here. We're told this is\n        # an entity reference, but it's not an XML entity\n        # reference or an HTML entity reference. Nonetheless,\n        # the logical thing to do is to pass it through as an\n        # unrecognized entity reference.\n        #\n        # Except: when the input is \"&carol;\" this function\n        # will be called with input \"carol\". When the input is\n        # \"AT&T\", this function will be called with input\n        # \"T\". We have no way of knowing whether a semicolon\n        # was present originally, so we don't know whether\n        # this is an unknown entity or just a misplaced\n        # ampersand.\n        #\n        # The more common case is a misplaced ampersand, so I\n        # escape the ampersand and omit the trailing semicolon.\n        data = \"&amp;%s\" % ref\nif not data:\n    # This case is different from the one above, because we\n    # haven't already gone through a supposedly comprehensive\n    # mapping of entities to Unicode characters. We might not\n    # have gone through any mapping at all. So the chances are\n    # very high that this is a real entity, and not a\n    # misplaced ampersand.\n    data = \"&%s;\" % ref\nself.handle_data(data)", "path": "BeautifulSoup.py", "repo_name": "jwheare/bizparse", "stars": 1, "license": "bsd-3-clause", "language": "python", "size": 128}
{"docstring": "\"\"\"Handle a processing instruction as a ProcessingInstruction\nobject, possibly one with a %SOUP-ENCODING% slot into which an\nencoding will be plugged later.\"\"\"\n", "func_signal": "def handle_pi(self, text):\n", "code": "if text[:3] == \"xml\":\n    text = u\"xml version='1.0' encoding='%SOUP-ENCODING%'\"\nself._toStringSubclass(text, ProcessingInstruction)", "path": "BeautifulSoup.py", "repo_name": "jwheare/bizparse", "stars": 1, "license": "bsd-3-clause", "language": "python", "size": 128}
{"docstring": "\"\"\"Adds a certain piece of text to the tree as a NavigableString\nsubclass.\"\"\"\n", "func_signal": "def _toStringSubclass(self, text, subclass):\n", "code": "self.soup.endData()\nself.handle_data(text)\nself.soup.endData(subclass)", "path": "BeautifulSoup.py", "repo_name": "jwheare/bizparse", "stars": 1, "license": "bsd-3-clause", "language": "python", "size": 128}
{"docstring": "\"\"\"Setting tag[key] sets the value of the 'key' attribute for the\ntag.\"\"\"\n", "func_signal": "def __setitem__(self, key, value):\n", "code": "self._getAttrMap()\nself.attrMap[key] = value\nfound = False\nfor i in range(0, len(self.attrs)):\n    if self.attrs[i][0] == key:\n        self.attrs[i] = (key, value)\n        found = True\nif not found:\n    self.attrs.append((key, value))\nself._getAttrMap()[key] = value", "path": "BeautifulSoup.py", "repo_name": "jwheare/bizparse", "stars": 1, "license": "bsd-3-clause", "language": "python", "size": 128}
{"docstring": "\"\"\"Initializes a map representation of this tag's attributes,\nif not already initialized.\"\"\"\n", "func_signal": "def _getAttrMap(self):\n", "code": "if not getattr(self, 'attrMap'):\n    self.attrMap = {}\n    for (key, value) in self.attrs:\n        self.attrMap[key] = value\nreturn self.attrMap", "path": "BeautifulSoup.py", "repo_name": "jwheare/bizparse", "stars": 1, "license": "bsd-3-clause", "language": "python", "size": 128}
{"docstring": "\"\"\"Returns the closest parent of this Tag that matches the given\ncriteria.\"\"\"\n# NOTE: We can't use _findOne because findParents takes a different\n# set of arguments.\n", "func_signal": "def findParent(self, name=None, attrs={}, **kwargs):\n", "code": "r = None\nl = self.findParents(name, attrs, 1)\nif l:\n    r = l[0]\nreturn r", "path": "BeautifulSoup.py", "repo_name": "jwheare/bizparse", "stars": 1, "license": "bsd-3-clause", "language": "python", "size": 128}
{"docstring": "\"\"\"Pops the tag stack up to and including the most recent\ninstance of the given tag. If inclusivePop is false, pops the tag\nstack up to but *not* including the most recent instqance of\nthe given tag.\"\"\"\n#print \"Popping to %s\" % name\n", "func_signal": "def _popToTag(self, name, inclusivePop=True):\n", "code": "if name == self.ROOT_TAG_NAME:\n    return\n\nnumPops = 0\nmostRecentTag = None\nfor i in range(len(self.tagStack)-1, 0, -1):\n    if name == self.tagStack[i].name:\n        numPops = len(self.tagStack)-i\n        break\nif not inclusivePop:\n    numPops = numPops - 1\n\nfor i in range(0, numPops):\n    mostRecentTag = self.popTag()\nreturn mostRecentTag", "path": "BeautifulSoup.py", "repo_name": "jwheare/bizparse", "stars": 1, "license": "bsd-3-clause", "language": "python", "size": 128}
{"docstring": "#print \"End tag %s\" % name\n", "func_signal": "def unknown_endtag(self, name):\n", "code": "if self.quoteStack and self.quoteStack[-1] != name:\n    #This is not a real end tag.\n    #print \"</%s> is not real!\" % name\n    self.handle_data('</%s>' % name)\n    return\nself.endData()\nself._popToTag(name)\nif self.quoteStack and self.quoteStack[-1] == name:\n    self.quoteStack.pop()\n    self.literal = (len(self.quoteStack) > 0)", "path": "BeautifulSoup.py", "repo_name": "jwheare/bizparse", "stars": 1, "license": "bsd-3-clause", "language": "python", "size": 128}
{"docstring": "\"\"\"Convenience method that works with all 2.x versions of Python\nto determine whether or not something is stringlike.\"\"\"\n", "func_signal": "def isString(s):\n", "code": "try:\n    return isinstance(s, unicode) or isinstance(s, basestring)\nexcept NameError:\n    return isinstance(s, str)", "path": "BeautifulSoup.py", "repo_name": "jwheare/bizparse", "stars": 1, "license": "bsd-3-clause", "language": "python", "size": 128}
{"docstring": "\"\"\"Changes an existing item attribute's value.\"\"\"\n\n", "func_signal": "def SetItemAttribute(self, name, value):\n", "code": "for attrib in self.item_attributes:\n  if attrib.name == name:\n    attrib.text = value\n    return", "path": "src\\gdata\\base\\__init__.py", "repo_name": "buger/music_video_aggregator", "stars": 1, "license": "None", "language": "python", "size": 1648}
{"docstring": "# Check if any tasks need to be run\n", "func_signal": "def __init__(self):\n", "code": "query = _AppEngineUtilities_Cron.all()\nquery.filter('next_run <= ', datetime.datetime.now())\nresults = query.fetch(1000)\nif len(results) > 0:\n    one_second = datetime.timedelta(seconds = 1)\n    before  = datetime.datetime.now()\n    for r in results:\n        if re.search(':' + APPLICATION_PORT, r.url):\n            r.url = re.sub(':' + APPLICATION_PORT, ':' + CRON_PORT, r.url)\n        #result = urlfetch.fetch(r.url)\n        diff = datetime.datetime.now() - before\n        if int(diff.seconds) < 1:\n            if memcache.add(str(r.key), \"running\"):\n                result = urlfetch.fetch(r.url)\n                r.next_run = self._get_next_run(pickle.loads(r.cron_compiled))\n                r.put()\n                memcache.delete(str(r.key))\n        else:\n            break", "path": "src\\appengine_utilities\\cron.py", "repo_name": "buger/music_video_aggregator", "stars": 1, "license": "None", "language": "python", "size": 1648}
{"docstring": "\"\"\"Extracts the AuthSub token from an HTTP body string.\n\nUsed to find the new session token after making a request to upgrade a \nsingle use AuthSub token.\n\nArgs:\n  http_body: str The repsonse from the server which contains the AuthSub \n      key. For example, this function would find the new session token\n      from the server's response to an upgrade token request.\n\nReturns:\n  The header value to use for Authorization which contains the AuthSub\n  token.\n\"\"\"\n", "func_signal": "def AuthSubTokenFromHttpBody(http_body):\n", "code": "for response_line in http_body.splitlines():\n  if response_line.startswith('Token='):\n    # Strip off Token= and construct the Authorization value.\n    auth_token = response_line[6:]\n    return 'AuthSub token=%s' % auth_token\nreturn None", "path": "src\\gdata\\auth.py", "repo_name": "buger/music_video_aggregator", "stars": 1, "license": "None", "language": "python", "size": 1648}
{"docstring": "\"\"\"Convert a dictionary of URL arguments into a URL parameter string.\n\nArgs:\n  url_parameters: The dictionaty of key-value pairs which will be converted\n                  into URL parameters. For example,\n                  {'dry-run': 'true', 'foo': 'bar'}\n                  will become ['dry-run=true', 'foo=bar'].\n\nReturns:\n  A list which contains a string for each key-value pair. The strings are\n  ready to be incorporated into a URL by using '&'.join([] + parameter_list)\n\"\"\"\n# Choose which function to use when modifying the query and parameters.\n# Use quote_plus when escape_params is true.\n", "func_signal": "def DictionaryToParamList(url_parameters, escape_params=True):\n", "code": "transform_op = [str, urllib.quote_plus][bool(escape_params)]\n# Create a list of tuples containing the escaped version of the\n# parameter-value pairs.\nparameter_tuples = [(transform_op(param), transform_op(value))\n                   for param, value in (url_parameters or {}).items()]\n# Turn parameter-value tuples into a list of strings in the form\n# 'PARAMETER=VALUE'.\n\nreturn ['='.join(x) for x in parameter_tuples]", "path": "src\\atom\\service.py", "repo_name": "buger/music_video_aggregator", "stars": 1, "license": "None", "language": "python", "size": 1648}
{"docstring": "\"\"\"Extracts the AuthSub token from the URL. \n\nUsed after the AuthSub redirect has sent the user to the 'next' page and\nappended the token to the URL.\n\nArgs:\n  url: str The URL of the current page which contains the AuthSub token as\n      a URL parameter.\n\"\"\"\n", "func_signal": "def AuthSubTokenFromUrl(url):\n", "code": "m = AUTH_SUB_KEY_PATTERN.match(url)\nif m:\n  return 'AuthSub token=%s' % m.group(1)\nreturn None", "path": "src\\gdata\\auth.py", "repo_name": "buger/music_video_aggregator", "stars": 1, "license": "None", "language": "python", "size": 1648}
{"docstring": "\"\"\"\nCheck to see if a valid session token exists, if not,\nthen use a cookie only session. It's up to the application\nto convert the session to a datastore session. Once this\nhas been done, the session will continue to use the datastore\nunless the writer is set to \"cookie\".\n\nSetting the session to use the datastore is as easy as resetting\nrequest.session anywhere if your application.\n\nExample:\n    from common.appengine_utilities import sessions\n    request.session = sessions.Session()\n\"\"\"\n", "func_signal": "def process_request(self, request):\n", "code": "self.request = request\nif sessions.Session.check_token():\n    request.session = sessions.Session()\nelse:\n    request.session = sessions.Session(writer=\"cookie\")\nrequest.session.set_test_cookie = self.set_test_cookie\nrequest.session.test_cookie_worked = self.test_cookie_worked\nrequest.session.delete_test_cookie = self.delete_test_cookie\nrequest.session.save = self.save", "path": "src\\appengine_utilities\\django-middleware\\middleware.py", "repo_name": "buger/music_video_aggregator", "stars": 1, "license": "None", "language": "python", "size": 1648}
{"docstring": "\"\"\"Returns a list of all item attributes which have the desired name.\n\nArgs:\n  name: str The tag of the desired base attributes. For example, calling\n      this method with 'rating' would return a list of ItemAttributes\n      represented by a 'g:rating' tag.\n\nReturns:\n  A list of matching ItemAttribute objects.\n\"\"\"\n", "func_signal": "def GetItemAttributes(self, name):\n", "code": "result = []\nfor attrib in self.item_attributes:\n  if attrib.name == name:\n    result.append(attrib)\nreturn result", "path": "src\\gdata\\base\\__init__.py", "repo_name": "buger/music_video_aggregator", "stars": 1, "license": "None", "language": "python", "size": 1648}
{"docstring": "\"\"\"Get the contents of the first Base item attribute which matches name.\n\nThis method is deprecated, please use GetItemAttributes instead.\n\nArgs: \n  name: str The tag of the desired base attribute. For example, calling\n      this method with name = 'rating' would search for a tag rating\n      in the GBase namespace in the item attributes. \n\nReturns:\n  The text contents of the item attribute, or none if the attribute was\n  not found.\n\"\"\"\n  \n", "func_signal": "def FindItemAttribute(self, name):\n", "code": "for attrib in self.item_attributes:\n  if attrib.name == name:\n    return attrib.text\nreturn None", "path": "src\\gdata\\base\\__init__.py", "repo_name": "buger/music_video_aggregator", "stars": 1, "license": "None", "language": "python", "size": 1648}
{"docstring": "\"\"\"Query the APP server with the given URI\n\nThe uri is the portion of the URI after the server value \n(server example: 'www.google.com').\n\nExample use:\nTo perform a query against Google Base, set the server to \n'base.google.com' and set the uri to '/base/feeds/...', where ... is \nyour query. For example, to find snippets for all digital cameras uri \nshould be set to: '/base/feeds/snippets?bq=digital+camera'\n\nArgs:\n  uri: string The query in the form of a URI. Example:\n       '/base/feeds/snippets?bq=digital+camera'.\n  extra_headers: dicty (optional) Extra HTTP headers to be included\n                 in the GET request. These headers are in addition to \n                 those stored in the client's additional_headers property.\n                 The client automatically sets the Content-Type and \n                 Authorization headers.\n  url_params: dict (optional) Additional URL parameters to be included\n              in the query. These are translated into query arguments\n              in the form '&dict_key=value&...'.\n              Example: {'max-results': '250'} becomes &max-results=250\n  escape_params: boolean (optional) If false, the calling code has already\n                 ensured that the query will form a valid URL (all\n                 reserved characters have been escaped). If true, this\n                 method will escape the query and any URL parameters\n                 provided.\n\nReturns:\n  httplib.HTTPResponse The server's response to the GET request.\n\"\"\"\n", "func_signal": "def Get(self, uri, extra_headers=None, url_params=None, escape_params=True):\n", "code": "return HttpRequest(self, 'GET', None, uri, extra_headers=extra_headers, \n    url_params=url_params, escape_params=escape_params)", "path": "src\\atom\\service.py", "repo_name": "buger/music_video_aggregator", "stars": 1, "license": "None", "language": "python", "size": 1648}
{"docstring": "# Find the element's tag in this class's list of child members\n", "func_signal": "def _ConvertElementTreeToMember(self, child_tree):\n", "code": "if self.__class__._children.has_key(child_tree.tag):\n  member_name = self.__class__._children[child_tree.tag][0]\n  member_class = self.__class__._children[child_tree.tag][1]\n  # If the class member is supposed to contain a list, make sure the\n  # matching member is set to a list, then append the new member\n  # instance to the list.\n  if isinstance(member_class, list):\n    if getattr(self, member_name) is None:\n      setattr(self, member_name, [])\n    getattr(self, member_name).append(atom._CreateClassFromElementTree(\n        member_class[0], child_tree))\n  else:\n    setattr(self, member_name, \n            atom._CreateClassFromElementTree(member_class, child_tree))\nelif child_tree.tag.find('{%s}' % GBASE_NAMESPACE) == 0:\n  # If this is in the gbase namespace, make it into an extension element.\n  name = child_tree.tag[child_tree.tag.index('}')+1:]\n  value = child_tree.text\n  if child_tree.attrib.has_key('type'):\n    value_type = child_tree.attrib['type']\n  else:\n    value_type = None\n  self.AddItemAttribute(name, value, value_type)\nelse:\n  atom.ExtensionContainer._ConvertElementTreeToMember(self, child_tree)", "path": "src\\gdata\\base\\__init__.py", "repo_name": "buger/music_video_aggregator", "stars": 1, "license": "None", "language": "python", "size": 1648}
{"docstring": "\"\"\"Creates a new AtomService client.\n\nArgs:\n  server: string (optional) The start of a URL for the server\n          to which all operations should be directed. Example: \n          'www.google.com'\n  additional_headers: dict (optional) Any additional HTTP headers which\n                      should be included with CRUD operations.\n\"\"\"\n\n", "func_signal": "def __init__(self, server=None, additional_headers=None):\n", "code": "self.server = server\nself.additional_headers = additional_headers or {}\n\nself.additional_headers['User-Agent'] = 'Python Google Data Client Lib'", "path": "src\\atom\\service.py", "repo_name": "buger/music_video_aggregator", "stars": 1, "license": "None", "language": "python", "size": 1648}
{"docstring": "\"\"\"Returns the token value to use in Authorization headers.\n\nReads the token from the server's response to a Client Login request and\ncreates header value to use in requests.\n\nArgs:\n  http_body: str The body of the server's HTTP response to a Client Login\n      request\n \nReturns:\n  The value half of an Authorization header.\n\"\"\"\n", "func_signal": "def GenerateClientLoginAuthToken(http_body):\n", "code": "for response_line in http_body.splitlines():\n  if response_line.startswith('Auth='):\n    # Strip off the leading Auth= and return the Authorization value.\n    return 'GoogleLogin auth=%s' % response_line[5:]\nreturn None", "path": "src\\gdata\\auth.py", "repo_name": "buger/music_video_aggregator", "stars": 1, "license": "None", "language": "python", "size": 1648}
{"docstring": "\"\"\"Attempts to determine the length of the data to send. \n\nThis method will respond with a length only if the data is a string or\nand ElementTree element.\n\nArgs:\n  data: object If this is not a string or ElementTree element this funtion\n      will return None.\n\"\"\"\n", "func_signal": "def __CalculateDataLength(data):\n", "code": "if isinstance(data, str):\n  return len(data)\nelif isinstance(data, list):\n  return None\nelif ElementTree.iselement(data):\n  return len(ElementTree.tostring(data))\nelif hasattr(data, 'read'):\n  # If this is a file-like object, don't try to guess the length.\n  return None\nelse:\n  return len(str(data))", "path": "src\\atom\\service.py", "repo_name": "buger/music_video_aggregator", "stars": 1, "license": "None", "language": "python", "size": 1648}
{"docstring": "\"\"\"Adds a new item attribute tag containing the value.\n\nCreates a new extension element in the GBase namespace to represent a\nGoogle Base item attribute.\n\nArgs:\n  name: str The tag name for the new attribute. This must be a valid xml\n    tag name. The tag will be placed in the GBase namespace.\n  value: str Contents for the item attribute\n  value_type: str (optional) The type of data in the vlaue, Examples: text\n      float\n  access: str (optional) Used to hide attributes. The attribute is not \n      exposed in the snippets feed if access is set to 'private'.\n\"\"\"\n\n", "func_signal": "def AddItemAttribute(self, name, value, value_type=None, access=None):\n", "code": "new_attribute =  ItemAttribute(name, text=value, \n    text_type=value_type, access=access)\nself.item_attributes.append(new_attribute)", "path": "src\\gdata\\base\\__init__.py", "repo_name": "buger/music_video_aggregator", "stars": 1, "license": "None", "language": "python", "size": 1648}
{"docstring": "\"\"\"Processes a passed URL.  If the URL does not begin with https?, then\nthe default value for server is used\"\"\"\n\n", "func_signal": "def ProcessUrl(service, url, for_proxy=False):\n", "code": "server = service.server\nif for_proxy:\n  port = 80\n  ssl = False\nelse:\n  port = service.port\n  ssl = service.ssl\nuri = url\n\nm = URL_REGEX.match(url)\n\nif m is None:\n  return (server, port, ssl, uri)\nelse:\n  if m.group(1) is not None:\n    port = 443\n    ssl = True\n  if m.group(3) is None:\n    server = m.group(2)\n  else:\n    server = m.group(2)\n    port = int(m.group(4))\n  if m.group(5) is not None:\n    uri = m.group(5)\n  else:\n    uri = '/'\n  return (server, port, ssl, uri)", "path": "src\\atom\\service.py", "repo_name": "buger/music_video_aggregator", "stars": 1, "license": "None", "language": "python", "size": 1648}
{"docstring": "\"\"\"\nParse the field to determine whether it is an integer or lists,\nalso converting strings to integers where necessary. If passed bad\nvalues, raises a ValueError.\n\"\"\"\n", "func_signal": "def _validate_cron(self, cron):\n", "code": "parsers = {\n    'dow': self._validate_dow,\n    'mon': self._validate_mon,\n    'day': self._validate_day,\n    'hour': self._validate_hour,\n    'min': self._validate_min,\n    'url': self. _validate_url,\n}\nfor el in cron:\n    parse = parsers[el]\n    cron[el] = parse(cron[el])\nreturn cron", "path": "src\\appengine_utilities\\cron.py", "repo_name": "buger/music_video_aggregator", "stars": 1, "license": "None", "language": "python", "size": 1648}
{"docstring": "# kludge for issue 842, right now we use request headers\n# to set the host.\n", "func_signal": "def _validate_url(self, url):\n", "code": "if url[0] is not \"/\":\n    url = \"/\" + url\nurl = 'http://' + str(os.environ['HTTP_HOST']) + url\nreturn url\n# content below is for when that issue gets fixed\n#regex = re.compile(\"^(http|https):\\/\\/([a-z0-9-]\\.+)*\", re.IGNORECASE)\n#if regex.match(url) is not None:\n#    return url\n#else:\n#    raise ValueError, \"Invalid url \" + url", "path": "src\\appengine_utilities\\cron.py", "repo_name": "buger/music_video_aggregator", "stars": 1, "license": "None", "language": "python", "size": 1648}
{"docstring": "\"\"\"Deletes the first extension element which matches name.\n\nDeletes the first extension element which matches name. \n\"\"\"\n\n", "func_signal": "def RemoveItemAttribute(self, name):\n", "code": "for i in xrange(len(self.item_attributes)):\n  if self.item_attributes[i].name == name:\n    del self.item_attributes[i]\n    return", "path": "src\\gdata\\base\\__init__.py", "repo_name": "buger/music_video_aggregator", "stars": 1, "license": "None", "language": "python", "size": 1648}
{"docstring": "# start with dow as per cron if dow and day are set\n# then dow is used if it comes before day. If dow\n# is *, then ignore it.\n", "func_signal": "def _calc_day(self, next_run, cron):\n", "code": "if str(cron[\"dow\"]) != str(\"*\"):\n    # convert any integers to lists in order to easily compare values\n    m = next_run.month\n    while True:\n        if next_run.month is not m:\n            next_run = next_run.replace(hour=0, minute=0)\n            next_run = self._calc_month(next_run, cron)\n        if next_run.weekday() in cron[\"dow\"] or next_run.day in cron[\"day\"]:\n            return next_run\n        else:\n            one_day = datetime.timedelta(days=1)\n            next_run = next_run + one_day\nelse:\n    m = next_run.month\n    while True:\n        if next_run.month is not m:\n            next_run = next_run.replace(hour=0, minute=0)\n            next_run = self._calc_month(next_run, cron)\n        # if cron[\"dow\"] is next_run.weekday() or cron[\"day\"] is next_run.day:\n        if next_run.day in cron[\"day\"]:\n            return next_run\n        else:\n            one_day = datetime.timedelta(days=1)\n            next_run = next_run + one_day", "path": "src\\appengine_utilities\\cron.py", "repo_name": "buger/music_video_aggregator", "stars": 1, "license": "None", "language": "python", "size": 1648}
{"docstring": "# Convert the members of this class which are XML child nodes. \n# This uses the class's _children dictionary to find the members which\n# should become XML child nodes.\n", "func_signal": "def _AddMembersToElementTree(self, tree):\n", "code": "member_node_names = [values[0] for tag, values in \n                                   self.__class__._children.iteritems()]\nfor member_name in member_node_names:\n  member = getattr(self, member_name)\n  if member is None:\n    pass\n  elif isinstance(member, list):\n    for instance in member:\n      instance._BecomeChildElement(tree)\n  else:\n    member._BecomeChildElement(tree)\n# Convert the members of this class which are XML attributes.\nfor xml_attribute, member_name in self.__class__._attributes.iteritems():\n  member = getattr(self, member_name)\n  if member is not None:\n    tree.attrib[xml_attribute] = member\n# Convert all special custom item attributes to nodes\nfor attribute in self.item_attributes:\n  attribute._BecomeChildElement(tree)\n# Lastly, call the ExtensionContainers's _AddMembersToElementTree to \n# convert any extension attributes.\natom.ExtensionContainer._AddMembersToElementTree(self, tree)", "path": "src\\gdata\\base\\__init__.py", "repo_name": "buger/music_video_aggregator", "stars": 1, "license": "None", "language": "python", "size": 1648}
{"docstring": "\"\"\"Deal with a comment.\"\"\"\n", "func_signal": "def _handle_comment(self, comment):\n", "code": "if not comment:\n    return ''\nstart = self.indent_type\nif not comment.startswith('#'):\n    start += self._a_to_u(' # ')\nreturn (start + comment)", "path": "necrolib\\configobj.py", "repo_name": "non7top/necroposter", "stars": 1, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Extract the value, where we are in a multiline situation.\"\"\"\n", "func_signal": "def _multiline(self, value, infile, cur_index, maxline):\n", "code": "quot = value[:3]\nnewvalue = value[3:]\nsingle_line = self._triple_quote[quot][0]\nmulti_line = self._triple_quote[quot][1]\nmat = single_line.match(value)\nif mat is not None:\n    retval = list(mat.groups())\n    retval.append(cur_index)\n    return retval\nelif newvalue.find(quot) != -1:\n    # somehow the triple quote is missing\n    raise SyntaxError()\n#\nwhile cur_index < maxline:\n    cur_index += 1\n    newvalue += '\\n'\n    line = infile[cur_index]\n    if line.find(quot) == -1:\n        newvalue += line\n    else:\n        # end of multiline, process it\n        break\nelse:\n    # we've got to the end of the config, oops...\n    raise SyntaxError()\nmat = multi_line.match(line)\nif mat is None:\n    # a badly formed line\n    raise SyntaxError()\n(value, comment) = mat.groups()\nreturn (newvalue + value, comment, cur_index)", "path": "necrolib\\configobj.py", "repo_name": "non7top/necroposter", "stars": 1, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Returns a help string listing available options. Automatically assigned to the \"help\" command.\"\"\"\n", "func_signal": "def help_callback( self, mess, args):\n", "code": "usage = '\\n'.join( [ '%s: %s' % ( name, command.__doc__ or '(undocumented)' ) for ( name, command ) in self.commands.items() if name != 'help' ])\n\nif self.__doc__:\n    description = self.__doc__.strip()\nelse:\n    description = 'Available commands:'\n\nreturn '%s\\n\\n%s' % ( description, usage, )", "path": "web\\jabberbot.py", "repo_name": "non7top/necroposter", "stars": 1, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"A version of setdefault that sets sequence if appropriate.\"\"\"\n", "func_signal": "def setdefault(self, key, default=None):\n", "code": "try:\n    return self[key]\nexcept KeyError:\n    self[key] = default\n    return self[key]", "path": "necrolib\\configobj.py", "repo_name": "non7top/necroposter", "stars": 1, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"\nReturn a deepcopy of self as a dictionary.\n\nAll members that are ``Section`` instances are recursively turned to\nordinary dictionaries - by calling their ``dict`` method.\n\n>>> n = a.dict()\n>>> n == a\n1\n>>> n is a\n0\n\"\"\"\n", "func_signal": "def dict(self):\n", "code": "newdict = {}\nfor entry in self:\n    this_entry = self[entry]\n    if isinstance(this_entry, Section):\n        this_entry = this_entry.dict()\n    elif isinstance(this_entry, list):\n        # create a copy rather than a reference\n        this_entry = list(this_entry)\n    elif isinstance(this_entry, tuple):\n        # create a copy rather than a reference\n        this_entry = tuple(this_entry)\n    newdict[entry] = this_entry\nreturn newdict", "path": "necrolib\\configobj.py", "repo_name": "non7top/necroposter", "stars": 1, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"\nHandle any BOM, and decode if necessary.\n\nIf an encoding is specified, that *must* be used - but the BOM should\nstill be removed (and the BOM attribute set).\n\n(If the encoding is wrongly specified, then a BOM for an alternative\nencoding won't be discovered or removed.)\n\nIf an encoding is not specified, UTF8 or UTF16 BOM will be detected and\nremoved. The BOM attribute will be set. UTF16 will be decoded to\nunicode.\n\nNOTE: This method must not be called with an empty ``infile``.\n\nSpecifying the *wrong* encoding is likely to cause a\n``UnicodeDecodeError``.\n\n``infile`` must always be returned as a list of lines, but may be\npassed in as a single string.\n\"\"\"\n", "func_signal": "def _handle_bom(self, infile):\n", "code": "if ((self.encoding is not None) and\n    (self.encoding.lower() not in BOM_LIST)):\n    # No need to check for a BOM\n    # the encoding specified doesn't have one\n    # just decode\n    return self._decode(infile, self.encoding)\n\nif isinstance(infile, (list, tuple)):\n    line = infile[0]\nelse:\n    line = infile\nif self.encoding is not None:\n    # encoding explicitly supplied\n    # And it could have an associated BOM\n    # TODO: if encoding is just UTF16 - we ought to check for both\n    # TODO: big endian and little endian versions.\n    enc = BOM_LIST[self.encoding.lower()]\n    if enc == 'utf_16':\n        # For UTF16 we try big endian and little endian\n        for BOM, (encoding, final_encoding) in BOMS.items():\n            if not final_encoding:\n                # skip UTF8\n                continue\n            if infile.startswith(BOM):\n                ### BOM discovered\n                ##self.BOM = True\n                # Don't need to remove BOM\n                return self._decode(infile, encoding)\n            \n        # If we get this far, will *probably* raise a DecodeError\n        # As it doesn't appear to start with a BOM\n        return self._decode(infile, self.encoding)\n    \n    # Must be UTF8\n    BOM = BOM_SET[enc]\n    if not line.startswith(BOM):\n        return self._decode(infile, self.encoding)\n    \n    newline = line[len(BOM):]\n    \n    # BOM removed\n    if isinstance(infile, (list, tuple)):\n        infile[0] = newline\n    else:\n        infile = newline\n    self.BOM = True\n    return self._decode(infile, self.encoding)\n\n# No encoding specified - so we need to check for UTF8/UTF16\nfor BOM, (encoding, final_encoding) in BOMS.items():\n    if not line.startswith(BOM):\n        continue\n    else:\n        # BOM discovered\n        self.encoding = final_encoding\n        if not final_encoding:\n            self.BOM = True\n            # UTF8\n            # remove BOM\n            newline = line[len(BOM):]\n            if isinstance(infile, (list, tuple)):\n                infile[0] = newline\n            else:\n                infile = newline\n            # UTF8 - don't decode\n            if isinstance(infile, StringTypes):\n                return infile.splitlines(True)\n            else:\n                return infile\n        # UTF16 - have to decode\n        return self._decode(infile, encoding)\n    \n# No BOM discovered and no encoding specified, just return\nif isinstance(infile, StringTypes):\n    # infile read from a file will be a single string\n    return infile.splitlines(True)\nreturn infile", "path": "necrolib\\configobj.py", "repo_name": "non7top/necroposter", "stars": 1, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"\nRecursively restore default values to all members\nthat have them.\n\nThis method will only work for a ConfigObj that was created\nwith a configspec and has been validated.\n\nIt doesn't delete or modify entries without default values.\n\"\"\"\n", "func_signal": "def restore_defaults(self):\n", "code": "for key in self.default_values:\n    self.restore_default(key)\n    \nfor section in self.sections:\n    self[section].restore_defaults()", "path": "necrolib\\configobj.py", "repo_name": "non7top/necroposter", "stars": 1, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"x.__repr__() <==> repr(x)\"\"\"\n", "func_signal": "def __repr__(self):\n", "code": "return '{%s}' % ', '.join([('%s: %s' % (repr(key), repr(self[key])))\n    for key in (self.scalars + self.sections)])", "path": "necrolib\\configobj.py", "repo_name": "non7top/necroposter", "stars": 1, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"\nEncode all strings and values from unicode,\nusing the specified encoding.\n\nWorks with subsections and list values.\nUses the ``walk`` method.\n\"\"\"\n", "func_signal": "def encode(self, encoding):\n", "code": "warn('use of ``encode`` is deprecated.', DeprecationWarning)\ndef encode(section, key, encoding=encoding):\n    \"\"\" \"\"\"\n    val = section[key]\n    if isinstance(val, (list, tuple)):\n        newval = []\n        for entry in val:\n            newval.append(entry.encode(encoding))\n    elif isinstance(val, dict):\n        newval = val\n    else:\n        newval = val.encode(encoding)\n    newkey = key.encode(encoding)\n    section.rename(key, newkey)\n    section[newkey] = newval\nself.walk(encode, call_on_sections=True)", "path": "necrolib\\configobj.py", "repo_name": "non7top/necroposter", "stars": 1, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"\nDecode infile to unicode. Using the specified encoding.\n\nif is a string, it also needs converting to a list.\n\"\"\"\n", "func_signal": "def _decode(self, infile, encoding):\n", "code": "if isinstance(infile, StringTypes):\n    # can't be unicode\n    # NOTE: Could raise a ``UnicodeDecodeError``\n    return infile.decode(encoding).splitlines(True)\nfor i, line in enumerate(infile):\n    if not isinstance(line, unicode):\n        # NOTE: The isinstance test here handles mixed lists of unicode/string\n        # NOTE: But the decode will break on any non-string values\n        # NOTE: Or could raise a ``UnicodeDecodeError``\n        infile[i] = line.decode(encoding)\nreturn infile", "path": "necrolib\\configobj.py", "repo_name": "non7top/necroposter", "stars": 1, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Write a section marker line\"\"\"\n", "func_signal": "def _write_marker(self, indent_string, depth, entry, comment):\n", "code": "return '%s%s%s%s%s' % (indent_string,\n                       self._a_to_u('[' * depth),\n                       self._quote(self._decode_element(entry), multiline=False),\n                       self._a_to_u(']' * depth),\n                       self._decode_element(comment))", "path": "necrolib\\configobj.py", "repo_name": "non7top/necroposter", "stars": 1, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"\n'D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised'\n\"\"\"\n", "func_signal": "def pop(self, key, *args):\n", "code": "val = dict.pop(self, key, *args)\nif key in self.scalars:\n    del self.comments[key]\n    del self.inline_comments[key]\n    self.scalars.remove(key)\nelif key in self.sections:\n    del self.comments[key]\n    del self.inline_comments[key]\n    self.sections.remove(key)\nif self.main.interpolation and isinstance(val, StringTypes):\n    return self._interpolate(key, val)\nreturn val", "path": "necrolib\\configobj.py", "repo_name": "non7top/necroposter", "stars": 1, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Helper function to fetch values from owning section.\n\nReturns a 2-tuple: the value, and the section where it was found.\n\"\"\"\n# switch off interpolation before we try and fetch anything !\n", "func_signal": "def _fetch(self, key):\n", "code": "save_interp = self.section.main.interpolation\nself.section.main.interpolation = False\n\n# Start at section that \"owns\" this InterpolationEngine\ncurrent_section = self.section\nwhile True:\n    # try the current section first\n    val = current_section.get(key)\n    if val is not None:\n        break\n    # try \"DEFAULT\" next\n    val = current_section.get('DEFAULT', {}).get(key)\n    if val is not None:\n        break\n    # move up to parent and try again\n    # top-level's parent is itself\n    if current_section.parent is current_section:\n        # reached top level, time to give up\n        break\n    current_section = current_section.parent\n\n# restore interpolation to previous value before returning\nself.section.main.interpolation = save_interp\nif val is None:\n    raise MissingInterpolationOption(key)\nreturn val, current_section", "path": "necrolib\\configobj.py", "repo_name": "non7top/necroposter", "stars": 1, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Initializes the jabber bot and sets up commands.\"\"\"\n", "func_signal": "def __init__( self, jid, password, res = None):\n", "code": "self.jid = xmpp.JID( jid)\nself.password = password\nself.res = (res or self.__class__.__name__)\nself.conn = None\nself.__finished = False\n\nself.commands = { 'help': self.help_callback, }\nfor (name, value) in inspect.getmembers( self):\n    if inspect.ismethod( value) and name.startswith( self.command_prefix):\n        self.commands[name[len(self.command_prefix):]] = value", "path": "web\\jabberbot.py", "repo_name": "non7top/necroposter", "stars": 1, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"\nDecode all strings and values to unicode, using the specified encoding.\n\nWorks with subsections and list values.\n\nUses the ``walk`` method.\n\nTesting ``encode`` and ``decode``.\n>>> m = ConfigObj(a)\n>>> m.decode('ascii')\n>>> def testuni(val):\n...     for entry in val:\n...         if not isinstance(entry, unicode):\n...             print >> sys.stderr, type(entry)\n...             raise AssertionError, 'decode failed.'\n...         if isinstance(val[entry], dict):\n...             testuni(val[entry])\n...         elif not isinstance(val[entry], unicode):\n...             raise AssertionError, 'decode failed.'\n>>> testuni(m)\n>>> m.encode('ascii')\n>>> a == m\n1\n\"\"\"\n", "func_signal": "def decode(self, encoding):\n", "code": "warn('use of ``decode`` is deprecated.', DeprecationWarning)\ndef decode(section, key, encoding=encoding, warn=True):\n    \"\"\" \"\"\"\n    val = section[key]\n    if isinstance(val, (list, tuple)):\n        newval = []\n        for entry in val:\n            newval.append(entry.decode(encoding))\n    elif isinstance(val, dict):\n        newval = val\n    else:\n        newval = val.decode(encoding)\n    newkey = key.decode(encoding)\n    section.rename(key, newkey)\n    section[newkey] = newval\n# using ``call_on_sections`` allows us to modify section names\nself.walk(decode, call_on_sections=True)", "path": "necrolib\\configobj.py", "repo_name": "non7top/necroposter", "stars": 1, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Decode ASCII strings to unicode if a self.encoding is specified.\"\"\"\n", "func_signal": "def _a_to_u(self, aString):\n", "code": "if self.encoding:\n    return aString.decode('ascii')\nelse:\n    return aString", "path": "necrolib\\configobj.py", "repo_name": "non7top/necroposter", "stars": 1, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"\nWrite the current ConfigObj as a file\n\ntekNico: FIXME: use StringIO instead of real files\n\n>>> filename = a.filename\n>>> a.filename = 'test.ini'\n>>> a.write()\n>>> a.filename = filename\n>>> a == ConfigObj('test.ini', raise_errors=True)\n1\n\"\"\"\n", "func_signal": "def write(self, outfile=None, section=None):\n", "code": "if self.indent_type is None:\n    # this can be true if initialised from a dictionary\n    self.indent_type = DEFAULT_INDENT_TYPE\n    \nout = []\ncs = self._a_to_u('#')\ncsp = self._a_to_u('# ')\nif section is None:\n    int_val = self.interpolation\n    self.interpolation = False\n    section = self\n    for line in self.initial_comment:\n        line = self._decode_element(line)\n        stripped_line = line.strip()\n        if stripped_line and not stripped_line.startswith(cs):\n            line = csp + line\n        out.append(line)\n        \nindent_string = self.indent_type * section.depth\nfor entry in (section.scalars + section.sections):\n    if entry in section.defaults:\n        # don't write out default values\n        continue\n    for comment_line in section.comments[entry]:\n        comment_line = self._decode_element(comment_line.lstrip())\n        if comment_line and not comment_line.startswith(cs):\n            comment_line = csp + comment_line\n        out.append(indent_string + comment_line)\n    this_entry = section[entry]\n    comment = self._handle_comment(section.inline_comments[entry])\n    \n    if isinstance(this_entry, dict):\n        # a section\n        out.append(self._write_marker(\n            indent_string,\n            this_entry.depth,\n            entry,\n            comment))\n        out.extend(self.write(section=this_entry))\n    else:\n        out.append(self._write_line(\n            indent_string,\n            entry,\n            this_entry,\n            comment))\n        \nif section is self:\n    for line in self.final_comment:\n        line = self._decode_element(line)\n        stripped_line = line.strip()\n        if stripped_line and not stripped_line.startswith(cs):\n            line = csp + line\n        out.append(line)\n    self.interpolation = int_val\n    \nif section is not self:\n    return out\n\nif (self.filename is None) and (outfile is None):\n    # output a list of lines\n    # might need to encode\n    # NOTE: This will *screw* UTF16, each line will start with the BOM\n    if self.encoding:\n        out = [l.encode(self.encoding) for l in out]\n    if (self.BOM and ((self.encoding is None) or\n        (BOM_LIST.get(self.encoding.lower()) == 'utf_8'))):\n        # Add the UTF8 BOM\n        if not out:\n            out.append('')\n        out[0] = BOM_UTF8 + out[0]\n    return out\n\n# Turn the list to a string, joined with correct newlines\nnewline = self.newlines or os.linesep\noutput = self._a_to_u(newline).join(out)\nif self.encoding:\n    output = output.encode(self.encoding)\nif self.BOM and ((self.encoding is None) or match_utf8(self.encoding)):\n    # Add the UTF8 BOM\n    output = BOM_UTF8 + output\n    \nif not output.endswith(newline):\n    output += newline\nif outfile is not None:\n    outfile.write(output)\nelse:\n    h = open(self.filename, 'wb')\n    h.write(output)\n    h.close()", "path": "necrolib\\configobj.py", "repo_name": "non7top/necroposter", "stars": 1, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Remove items from the sequence when deleting.\"\"\"\n", "func_signal": "def __delitem__(self, key):\n", "code": "dict. __delitem__(self, key)\nif key in self.scalars:\n    self.scalars.remove(key)\nelse:\n    self.sections.remove(key)\ndel self.comments[key]\ndel self.inline_comments[key]", "path": "necrolib\\configobj.py", "repo_name": "non7top/necroposter", "stars": 1, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Messages sent to the bot will arrive here. Command handling + routing is done in this function.\"\"\"\n", "func_signal": "def callback_message( self, conn, mess):\n", "code": "text = mess.getBody()\n    \n# If a message format is not supported (eg. encrypted), txt will be None\nif not text:\n    return\n\nif ' ' in text:\n    command, args = text.split(' ',1)\nelse:\n    command, args = text,''\n    \ncmd = command.lower()\n    \nif self.commands.has_key(cmd):\n    reply = self.commands[cmd]( mess, args)\nelse:\n    unk_str = 'Unknown command: \"%s\". Type \"help\" for available commands.' % cmd\n    reply = self.unknown_command( mess, cmd, args) or unk_str\nif reply:\n    self.send( mess.getFrom(), reply, mess)", "path": "web\\jabberbot.py", "repo_name": "non7top/necroposter", "stars": 1, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Sends a simple message to the specified user.\"\"\"\n", "func_signal": "def send( self, user, text, in_reply_to = None):\n", "code": "mess = xmpp.Message( user, text)\nif in_reply_to:\n    mess.setThread( in_reply_to.getThread())\n    mess.setType( in_reply_to.getType())\n\nself.connect().send( mess)", "path": "web\\jabberbot.py", "repo_name": "non7top/necroposter", "stars": 1, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"True if nick has the rights\"\"\"\n", "func_signal": "def is_admin(self, nick):\n", "code": "for n, u, h, m in self.users:\n    if nick == n:\n        if m == self.modes['admin']:\n            return True\n        else:\n            return False\nreturn False", "path": "bot.py", "repo_name": "jeanbon/botounet", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\" Retourne un tuple contenant les lignes du fichier.\"\"\"\n", "func_signal": "def read_file(self, fichier):\n", "code": "if os.path.exists(fichier):\n    with open(fichier, \"r\") as file:\n        lignes = file.readlines()\n    return lignes\nelse:\n    self.erreur(_(\"Le fichier %s n'existe pas !\", fichier))\n    return \"\"", "path": "bot.py", "repo_name": "jeanbon/botounet", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"/freeze : Freeze the screen so you can type some long command \"\"\"\n", "func_signal": "def do_freeze(self, args):\n", "code": "self.bot.action(\"Affichage des messages d\u00e9sactiv\u00e9.\",\n        event_type=\"info\")\nself.bot.freezed = True", "path": "bot.py", "repo_name": "jeanbon/botounet", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"/unfreeze : unfreeze a frozen screen with /freeze \"\"\"\n", "func_signal": "def do_unfreeze(self, args):\n", "code": "self.bot.freezed = False\nself.bot.action(\"Affichage des messages activ\u00e9.\", event_type=\"info\")", "path": "bot.py", "repo_name": "jeanbon/botounet", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Check if line is a command or not. \"\"\"\n", "func_signal": "def precmd(self, line):\n", "code": "if line == \"EOF\":\n    return line\nif not self.bot.alive or self.parent.arret:\n    self.do_exit(None)\n    return \"\"\nif line and line[0]==\"/\":\n    return line[1:]\nelse:\n    self.msg(line)\n    return \"\"\n#}}}", "path": "bot.py", "repo_name": "jeanbon/botounet", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Renvoie le type de l'option sp\u00e9cifi\u00e9e. \"\"\"\n", "func_signal": "def __get_type(self, option):\n", "code": "for opt, defarg, desc in self.config_skel:\n    if opt == option:\n        return type(defarg)\nreturn None", "path": "bot.py", "repo_name": "jeanbon/botounet", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"/ignore bliblu : ignore bliblu by his host \"\"\"\n", "func_signal": "def do_ignore(self, args):\n", "code": "if args:\n    self.bot.ignore(args.split(\" \")[0])", "path": "bot.py", "repo_name": "jeanbon/botounet", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"R\u00e9cup\u00e8re la valeur d'une option et renvoie le bon type.\"\"\"\n", "func_signal": "def getopt(self, option):\n", "code": "option = string.lower(option)\ndefval = [val for sect, opt, val, help in self.config.config_skel\\\n        if opt==option]\nif defval:\n    type_val = type(defval[0])\n    if type_val == bool:\n        return self.config.get(option)[0].lower() == \"t\"\n    else:\n        return type(defval[0])(self.config.get(option))\nelse:\n    return self.config.get(option)", "path": "bot.py", "repo_name": "jeanbon/botounet", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"/quit [msg] : leave the game.\"\"\"\n", "func_signal": "def do_quit(self, args):\n", "code": "self.bot.disconnect(args)\nself.do_exit(None)\nreturn -1", "path": "bot.py", "repo_name": "jeanbon/botounet", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"/ignore bliblu : unignore bliblu \"\"\"\n", "func_signal": "def do_unignore(self, args):\n", "code": "if args:\n    self.bot.unignore(args.split(\" \")[0])", "path": "bot.py", "repo_name": "jeanbon/botounet", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"\nThere's two way to ban:\n    - ban a mask (like *!*@foo.bar)\n    - ban the user 'nick' with mode:\n        - nick : nick!*@*\n        - user : *!user@*\n        - host : *!*@host\n\"\"\"\n", "func_signal": "def ban(self, mask=\"\", nick=\"\", mode=\"nick\"):\n", "code": "if (not mask and not nick) or (nick and not mode):\n    return\nif mask:\n    self.mode(self.channel, \"+b %s\" % mask)\nelse:\n    nick, user, host, m = self.get_user_info(nick)\n    if not nick:\n        return\n    if mode == \"user\":\n        cmd = \"+b *!*%s@*\" % user\n    elif mode == \"host\":\n        cmd = \"+b *!*@%s\" % host\n    else:\n        cmd = \"+b %s!*@*\" % nick\n    self.mode(self.channel, cmd)", "path": "bot.py", "repo_name": "jeanbon/botounet", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\" Check if the host is authorized \"\"\"\n", "func_signal": "def __is_admin(self, host):\n", "code": "lines = self.read_file(self.getopt(\"fichier_admins\"))\nfor line in lines:\n    if line.split(' ')[1].strip('\\n') == host:\n        return True\nreturn False", "path": "bot.py", "repo_name": "jeanbon/botounet", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Return a tuple with the user's informations, or None\"\"\"\n", "func_signal": "def get_user_info(self, nick):\n", "code": "for info in self.users:\n    if info[0] == nick:\n        return info\nreturn (None, None, None, None)", "path": "bot.py", "repo_name": "jeanbon/botounet", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\" When a key is pressed\"\"\"\n", "func_signal": "def key_press_event_callback(self, widget, event, entry):\n", "code": "if not self.console: return\nkey = gtk.gdk.keyval_name(event.keyval)\n# L'index 0 de l'historique correspond au texte en cours.\nself.history[self.n_history] = entry.get_text()\nmove = False\nif key == \"Up\":\n    if self.n_history < len(self.history)-1:\n        self.n_history += 1\n        move = True\nelif key == \"Down\":\n    if self.n_history > 0:\n        self.n_history -= 1\n        move = True\ntry:\n    entry.set_text(self.history[self.n_history])\nexcept:\n    # Si on appuie sur 'Ctrl' par exemple, \u00e7a foire.\n    # Wtf gtk ?\n    pass\nif move:\n    entry.set_position(len(entry.get_text()))", "path": "gtkwin.py", "repo_name": "jeanbon/botounet", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\" Clean the admin list \"\"\"\n", "func_signal": "def __clean_admins(self):\n", "code": "lines = self.read_file(self.getopt(\"fichier_admins\"))\nlines = [ line for line in lines if int(line.split(' ')[0]) +\n        self.getopt(\"admin_retention\") < time.time() ]\nwith open(self.getopt(\"fichier_admins\")) as f:\n    f.writelines(lines)", "path": "bot.py", "repo_name": "jeanbon/botounet", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Stocke les informations dans le tableau 'names' \"\"\"\n", "func_signal": "def store_users(self, nick, user=\"\", host=\"\", mode=\"\", remove = False):\n", "code": "if remove:\n    self.users = set( [u for u in self.users if u[0] != nick] )\nelse:\n    if self.users and nick in zip(*self.users)[0]:\n        # Si le nick est d\u00e9j\u00e0 dedans, on modifie juste.\n        self.users = set( [u if u[0] != nick else (nick, user, host,\n            mode) for u in self.users] )\n    else:\n        self.users.add( (nick, user, host, mode) )\nlignes = self.read_file(self.getopt('FICHIER_IGNORE'))\nif host in lignes or user in lignes:\n    self.ignoring.append(nick)", "path": "bot.py", "repo_name": "jeanbon/botounet", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Ignore une personne\"\"\"\n", "func_signal": "def ignore(self, pseudo):\n", "code": "t = [n for n in self.users if n[0] == pseudo]\nif t:\n    pseudo, user, host, mode = t[0]\nelse:\n    return\nself.ignoring.add(pseudo) # On ajoute le pseudo chez les sales.\nself.store(self.getopt('FICHIER_IGNORE'), host)\nself.action(\"Ignore : %s (%s@%s)\" % (pseudo, user, host),\n        event_type=\"info\")\nself.privmsg(self.channel, \"/me ignore %s, d\u00e9sormais.\" % pseudo)", "path": "bot.py", "repo_name": "jeanbon/botounet", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"\nRejoint le canal sp\u00e9cifi\u00e9, et si aucun n'est sp\u00e9cifi\u00e9, rejoint\ncelui d\u00e9fini dans la configuration.\n\"\"\"\n", "func_signal": "def join(self, channel = \"\"):\n", "code": "if not channel:\n    channel = self.getopt(\"channel\")\nself.action(\"Je rejoins le canal %s.\" % channel, event_type=\"info\")\nself.server.join(channel)\nself.server.who(\"*\") # Bah oui, \u00e7a parait tr\u00e8s moche, mais c'est la\n    # seule solution que j'ai trouv\u00e9 pour l'instant :\n    # La premi\u00e8re requ\u00e8te \"who\" renvoie \"\". TODO voir RFC1459 si normal\n    # ou si je est juste mauvais.\nself.channel = channel", "path": "bot.py", "repo_name": "jeanbon/botounet", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "# Juste un truc que j'ai utilis\u00e9 pour r\u00e9cup\u00e9rer un maximum de phrases\n# au hasard, pour le filtre bayesien\n", "func_signal": "def triviallogger(connexion, event):\n", "code": "return 1\nwith open(\"trivialog\", \"a\") as fichier:\n    fichier.write(event.arguments()[0]+\"\\n\")", "path": "outils.py", "repo_name": "jeanbon/botounet", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Console(parent)\"\"\"\n", "func_signal": "def __init__(self, parent):\n", "code": "Cmd.__init__(self)\nself.prompt = '> '\nself.doc_header = \"Commandes document\u00e9es :\"\nself.undoc_header = \"Commandes non document\u00e9es :\"\nself.nohelp = \"*** Pas d'aide pour %s.\"\nself.ruler = \"~\"\nself.parent = parent\nself.bot = parent.bot", "path": "bot.py", "repo_name": "jeanbon/botounet", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Edit the photos in this set.\n\nphotos - photos for set\nprimary - primary photo (if None will used current)\n\"\"\"\n", "func_signal": "def editPhotos(self, photos, primary=None):\n", "code": "method = 'flickr.photosets.editPhotos'\n\nif primary is None:\n    primary = self.primary\n    \nids = [photo.id for photo in photos]\nif primary.id not in ids:\n    ids.append(primary.id)\n\n_dopost(method, auth=True, photoset_id=self.id,\\\n        primary_photo_id=primary.id,\n        photo_ids=ids)\nself.__count = len(ids)\nreturn True", "path": "django\\imageModelling\\taging\\flickr.py", "repo_name": "gmateu/imagemodelling", "stars": 1, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Adds a Photo to the group\"\"\"\n", "func_signal": "def add(self, photo):\n", "code": "method = 'flickr.groups.pools.add'\n_dopost(method, auth=True, photo_id=photo.id, group_id=self.id)\nreturn True", "path": "django\\imageModelling\\taging\\flickr.py", "repo_name": "gmateu/imagemodelling", "stars": 1, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Returns list of Photos.\"\"\"\n", "func_signal": "def getPhotos(self):\n", "code": "method = 'flickr.photosets.getPhotos'\ndata = _doget(method, photoset_id=self.id)\nphotos = data.rsp.photoset.photo\np = []\nfor photo in photos:\n    p.append(Photo(photo.id, title=photo.title, secret=photo.secret, \\\n                   server=photo.server))\nreturn p", "path": "django\\imageModelling\\taging\\flickr.py", "repo_name": "gmateu/imagemodelling", "stars": 1, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Returns a frob that is used in authentication\"\"\"\n", "func_signal": "def getFrob(self):\n", "code": "method = 'flickr.auth.getFrob'\nsig_str = API_SECRET + 'api_key' + API_KEY + 'method' + method\nsignature_hash = hashlib.md5(sig_str).hexdigest()\ndata = _doget(method, auth=False, api_sig=signature_hash)\nreturn data.rsp.frob.text", "path": "django\\imageModelling\\taging\\flickr.py", "repo_name": "gmateu/imagemodelling", "stars": 1, "license": "None", "language": "python", "size": 100}
{"docstring": "#uncomment to check you aren't killing the flickr server\n#print \"***** do post %s\" % method\n\n", "func_signal": "def _dopost(method, auth=False, **params):\n", "code": "params = _prepare_params(params)\nurl = '%s%s/%s' % (HOST, API, _get_auth_url_suffix(method, auth, params))\npayload = 'api_key=%s&method=%s&%s'% \\\n      (API_KEY, method, urlencode(params))\n\n#another useful debug print statement\n#print url\n#print payload\n\nreturn _get_data(minidom.parse(urlopen(url, payload)))", "path": "django\\imageModelling\\taging\\flickr.py", "repo_name": "gmateu/imagemodelling", "stars": 1, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"This token is what needs to be used in future API calls\"\"\"\n", "func_signal": "def getToken(self, frob):\n", "code": "method = 'flickr.auth.getToken'\nsig_str = API_SECRET + 'api_key' + API_KEY + 'frob' + frob + 'method' + method\nsignature_hash = hashlib.md5(sig_str).hexdigest()\ndata = _doget(method, auth=False, api_sig=signature_hash, \n              api_key=API_KEY, frob=frob)\nreturn data.rsp.auth.token.text", "path": "django\\imageModelling\\taging\\flickr.py", "repo_name": "gmateu/imagemodelling", "stars": 1, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Returns user URL in an array (to access, use array[1])\"\"\"\n", "func_signal": "def getUserPhotosURL(userid):\n", "code": "method = 'flickr.urls.getUserPhotos'\ndata = _doget(method, user_id=userid)\nreturn [data.rsp.user.nsid,data.rsp.user.url]", "path": "django\\imageModelling\\taging\\flickr.py", "repo_name": "gmateu/imagemodelling", "stars": 1, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Gets the contacts (Users) for the user_id\"\"\"\n", "func_signal": "def contacts_getPublicList(user_id):\n", "code": "method = 'flickr.contacts.getPublicList'\ndata = _doget(method, auth=False, user_id=user_id)\n\ntry:\n  if isinstance(data.rsp.contacts.contact, list):\n      return [User(user.nsid, username=user.username) \\\n              for user in data.rsp.contacts.contact]\n\nexcept AttributeError:\n  return \"No users in the list\"\nexcept:\n  return \"Unknown error\"", "path": "django\\imageModelling\\taging\\flickr.py", "repo_name": "gmateu/imagemodelling", "stars": 1, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Returns User object.\"\"\"\n", "func_signal": "def people_findByEmail(email):\n", "code": "method = 'flickr.people.findByEmail'\ndata = _doget(method, find_email=email)\nuser = User(data.rsp.user.id, username=data.rsp.user.username.text)\nreturn user", "path": "django\\imageModelling\\taging\\flickr.py", "repo_name": "gmateu/imagemodelling", "stars": 1, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Returns list of Photo objects.\"\"\"\n", "func_signal": "def favorites_getList(user_id='', per_page='', page=''):\n", "code": "method = 'flickr.favorites.getList'\ndata = _doget(method, auth=True, user_id=user_id, per_page=per_page,\\\n              page=page)\nphotos = []\nif isinstance(data.rsp.photos.photo, list):\n    for photo in data.rsp.photos.photo:\n        photos.append(_parse_photo(photo))\nelse:\n    photos = [_parse_photo(data.rsp.photos.photo)]\nreturn photos", "path": "django\\imageModelling\\taging\\flickr.py", "repo_name": "gmateu/imagemodelling", "stars": 1, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Generic get attribute function.\"\"\"\n", "func_signal": "def _general_getattr(self, var):\n", "code": "if getattr(self, \"_%s__%s\" % (self.__class__.__name__, var)) is None \\\n   and not self.__loaded:\n    self._load_properties()\nreturn getattr(self, \"_%s__%s\" % (self.__class__.__name__, var))", "path": "django\\imageModelling\\taging\\flickr.py", "repo_name": "gmateu/imagemodelling", "stars": 1, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Create a Photo object from photo data.\"\"\"\n", "func_signal": "def _parse_photo(photo):\n", "code": "owner = User(photo.owner)\ntitle = photo.title\nispublic = photo.ispublic\nisfriend = photo.isfriend\nisfamily = photo.isfamily\nsecret = photo.secret\nserver = photo.server\np = Photo(photo.id, owner=owner, title=title, ispublic=ispublic,\\\n          isfriend=isfriend, isfamily=isfamily, secret=secret, \\\n          server=server)        \nreturn p", "path": "django\\imageModelling\\taging\\flickr.py", "repo_name": "gmateu/imagemodelling", "stars": 1, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Load User properties from Flickr.\"\"\"\n", "func_signal": "def _load_properties(self):\n", "code": "method = 'flickr.people.getInfo'\ndata = _doget(method, user_id=self.__id)\n\nself.__loaded = True\n\nperson = data.rsp.person\n\nself.__isadmin = person.isadmin\nself.__ispro = person.ispro\nself.__icon_server = person.iconserver\nif int(person.iconserver) > 0:\n    self.__icon_url = 'http://photos%s.flickr.com/buddyicons/%s.jpg' \\\n                      % (person.iconserver, self.__id)\nelse:\n    self.__icon_url = 'http://www.flickr.com/images/buddyicon.jpg'\n\nself.__username = person.username.text\nself.__realname = getattr((getattr(person,  'realname',  u'')), 'text', u'')\nself.__location = getattr((getattr(person,  'location',  u'')), 'text', u'')\nself.__photos_count = getattr((getattr(getattr(person,  'photos',  None),  'count',  u'')), 'text', u'')\nif self.__photos_count:\n    self.__photos_firstdate = person.photos.firstdate.text\n    self.__photos_firstdatetaken = person.photos.firstdatetaken.text\nelse:\n    self.__photos_firstdate = None\n    self.__photos_firstdatetaken = None", "path": "django\\imageModelling\\taging\\flickr.py", "repo_name": "gmateu/imagemodelling", "stars": 1, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Remove a photo from the user's favorites.\"\"\"\n", "func_signal": "def favorites_remove(photo_id):\n", "code": "method = 'flickr.favorites.remove'\n_dopost(method, auth=True, photo_id=photo_id)\nreturn True", "path": "django\\imageModelling\\taging\\flickr.py", "repo_name": "gmateu/imagemodelling", "stars": 1, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Adds the list of tags to current tags. (flickr.photos.addtags)\n\"\"\"\n", "func_signal": "def addTags(self, tags):\n", "code": "method = 'flickr.photos.addTags'\nif isinstance(tags, list):\n    tags = uniq(tags)\n\n_dopost(method, auth=True, photo_id=self.id, tags=tags)\n#load properties again\nself._load_properties()", "path": "django\\imageModelling\\taging\\flickr.py", "repo_name": "gmateu/imagemodelling", "stars": 1, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Returns list of Photo objects.\"\"\"\n", "func_signal": "def favorites_getPublicList(user_id, per_page='', page=''):\n", "code": "method = 'flickr.favorites.getPublicList'\ndata = _doget(method, auth=False, user_id=user_id, per_page=per_page,\\\n              page=page)\nphotos = []\nif isinstance(data.rsp.photos.photo, list):\n    for photo in data.rsp.photos.photo:\n        photos.append(_parse_photo(photo))\nelse:\n    photos = [_parse_photo(data.rsp.photos.photo)]\nreturn photos", "path": "django\\imageModelling\\taging\\flickr.py", "repo_name": "gmateu/imagemodelling", "stars": 1, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Returns a list of Photosets.\"\"\"\n", "func_signal": "def getPhotosets(self):\n", "code": "method = 'flickr.photosets.getList'\ndata = _doget(method, user_id=self.id)\n\nsets = []\nif not getattr(data.rsp.photosets,  'photoset',None):\n    return sets        #N.B. returns an empty set\nif isinstance(data.rsp.photosets.photoset, list):\n    for photoset in data.rsp.photosets.photoset:\n        sets.append(Photoset(photoset.id, photoset.title.text,\\\n                             Photo(photoset.primary),\\\n                             secret=photoset.secret, \\\n                             server=photoset.server, \\\n                             description=photoset.description.text,\n                             photos=photoset.photos))\nelse:\n    photoset = data.rsp.photosets.photoset\n    sets.append(Photoset(photoset.id, photoset.title.text,\\\n                             Photo(photoset.primary),\\\n                             secret=photoset.secret, \\\n                             server=photoset.server, \\\n                             description=photoset.description.text,\n                             photos=photoset.photos))\nreturn sets", "path": "django\\imageModelling\\taging\\flickr.py", "repo_name": "gmateu/imagemodelling", "stars": 1, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Returns user URL in an array (to access, use array[1])\"\"\"\n# This addition has been added upon request of\n# nsteinmetz. It will be \"cleaned up\" at another\n# time.\n", "func_signal": "def getUserPhotosURL(userid):\n", "code": "method = 'flickr.urls.getUserPhotos'\ndata = _doget(method, user_id=userid)\nuserurl = [data.rsp.user.nsid,data.rsp.user.url]\nreturn userurl", "path": "django\\imageModelling\\taging\\flickr.py", "repo_name": "gmateu/imagemodelling", "stars": 1, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Loads the properties from Flickr.\"\"\"\n", "func_signal": "def _load_properties(self):\n", "code": "method = 'flickr.groups.getInfo'\ndata = _doget(method, group_id=self.id)\n\nself.__loaded = True\n\ngroup = data.rsp.group\n\nself.__name = photo.name.text\nself.__members = photo.members.text\nself.__online = photo.online.text\nself.__privacy = photo.privacy.text\nself.__chatid = photo.chatid.text\nself.__chatcount = photo.chatcount.text", "path": "django\\imageModelling\\taging\\flickr.py", "repo_name": "gmateu/imagemodelling", "stars": 1, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Figure out whether we want to authorize, and if so, construct a suitable\nURL suffix to pass to the Flickr API.\"\"\"\n", "func_signal": "def _get_auth_url_suffix(method, auth, params):\n", "code": "authentication = False\n\n# auth may be passed in via the API, AUTH may be set globally (in the same\n# manner as API_KEY, etc). We do a few more checks than may seem necessary\n# because we allow the 'auth' parameter to actually contain the\n# authentication token, not just True/False.\nif auth or AUTH:\n    token = userToken()\n    authentication = True;\nelif auth != False:\n    token = auth;\n    authentication = True;\nelif AUTH != False:\n    token = AUTH;\n    authentication = True;\n\n# If we're not authenticating, no suffix is required.\nif not authentication:\n    return ''\n\nparamaters = ['API_KEY', 'method', 'auth_token']\nfor item in params.items():\n    paramaters.append(item[0])\nparamaters.sort()\n\napi_string = [API_SECRET]\nfor item in paramaters:\n    for chocolate in params.items():\n        if item == chocolate[0]:\n            api_string.append(item)\n            api_string.append(str(chocolate[1]))\n    if item == 'method':\n        api_string.append('method')\n        api_string.append(method)\n    if item == 'API_KEY':\n        api_string.append('api_key')\n        api_string.append(API_KEY)\n    if item == 'auth_token':\n        api_string.append('auth_token')\n        api_string.append(token)\n            \napi_signature = hashlib.md5(''.join(api_string)).hexdigest()\n\nreturn '&auth_token=%s&api_sig=%s' % (token, api_signature)", "path": "django\\imageModelling\\taging\\flickr.py", "repo_name": "gmateu/imagemodelling", "stars": 1, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Creates a class-wide instance property with a thread-specific value.\"\"\"\n", "func_signal": "def make_tls_property(default=None):\n", "code": "class TLSProperty(object):\n    def __init__(self):\n        self.local = local()\n\n    def __get__(self, instance, cls):\n        if not instance:\n            return self\n        return self.value\n\n    def __set__(self, instance, value):\n        self.value = value\n\n    def _get_value(self):\n        return getattr(self.local, 'value', default)\n    def _set_value(self, value):\n        self.local.value = value\n    value = property(_get_value, _set_value)\n\nreturn TLSProperty()", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "ShawTim/gogogo", "stars": 1, "license": "None", "language": "python", "size": 6961}
{"docstring": "\"\"\"\nRemove expired instances of ``RegistrationProfile`` and their\nassociated ``User``s.\n\nAccounts to be deleted are identified by searching for\ninstances of ``RegistrationProfile`` with expired activation\nkeys, and then checking to see if their associated ``User``\ninstances have the field ``is_active`` set to ``False``; any\n``User`` who is both inactive and has an expired activation\nkey will be deleted.\n\nIt is recommended that this method be executed regularly as\npart of your routine site maintenance; this application\nprovides a custom management command which will call this\nmethod, accessible as ``manage.py cleanupregistration``.\n\nRegularly clearing out accounts which have never been\nactivated serves two useful purposes:\n\n1. It alleviates the ocasional need to reset a\n   ``RegistrationProfile`` and/or re-send an activation email\n   when a user does not receive or does not act upon the\n   initial activation email; since the account will be\n   deleted, the user will be able to simply re-register and\n   receive a new activation key.\n\n2. It prevents the possibility of a malicious user registering\n   one or more accounts and never activating them (thus\n   denying the use of those usernames to anyone else); since\n   those accounts will be deleted, the usernames will become\n   available for use again.\n\nIf you have a troublesome ``User`` and wish to disable their\naccount while keeping it in the database, simply delete the\nassociated ``RegistrationProfile``; an inactive ``User`` which\ndoes not have an associated ``RegistrationProfile`` will not\nbe deleted.\n\n\"\"\"\n", "func_signal": "def delete_expired_users(self):\n", "code": "for profile in RegistrationProfile.all():\n    if profile.activation_key_expired():\n        user = profile.user\n        if not user.is_active:\n            user.delete()\n            profile.delete()", "path": "registration\\models.py", "repo_name": "ShawTim/gogogo", "stars": 1, "license": "None", "language": "python", "size": 6961}
{"docstring": "\"\"\"\nTest that activation email can be disabled.\n\n\"\"\"\n", "func_signal": "def test_activation_email_disable(self):\n", "code": "RegistrationProfile.objects.create_inactive_user(username='noemail',\n                                                 password='foo',\n                                                 email='nobody@example.com',\n                                                 send_email=False)\nself.assertEqual(len(mail.outbox), 2)", "path": "registration\\tests.py", "repo_name": "ShawTim/gogogo", "stars": 1, "license": "None", "language": "python", "size": 6961}
{"docstring": "\"\"\"\nValidate an activation key and activate the corresponding\n``User`` if valid.\n\nIf the key is valid and has not expired, return the ``User``\nafter activating.\n\nIf the key is not valid or has expired, return ``False``.\n\nIf the key is valid but the ``User`` is already active,\nreturn ``False``.\n\nTo prevent reactivation of an account which has been\ndeactivated by site administrators, the activation key is\nreset to the string constant ``RegistrationProfile.ACTIVATED``\nafter successful activation.\n\nTo execute customized logic when a ``User`` is activated,\nconnect a function to the signal\n``registration.signals.user_activated``; this signal will be\nsent (with the ``User`` as the value of the keyword argument\n``user``) after a successful activation.\n\n\"\"\"\n", "func_signal": "def activate_user(self, activation_key):\n", "code": "from registration.signals import user_activated\n\n# Make sure the key we're trying conforms to the pattern of a\n# SHA1 hash; if it doesn't, no point trying to look it up in\n# the database.\nif SHA1_RE.search(activation_key):\n    profile = RegistrationProfile.get_by_key_name(\"key_\"+activation_key)\n    if not profile:\n        return False\n    if not profile.activation_key_expired():\n        user = profile.user\n        user.is_active = True\n        user.put()\n        profile.activation_key = RegistrationProfile.ACTIVATED\n        profile.put()\n        user_activated.send(sender=self.model, user=user)\n        return user\nreturn False", "path": "registration\\models.py", "repo_name": "ShawTim/gogogo", "stars": 1, "license": "None", "language": "python", "size": 6961}
{"docstring": "\"\"\"Like getattr(), but can go down a hierarchy like 'attr.subattr'\"\"\"\n", "func_signal": "def getattr_by_path(obj, attr, *default):\n", "code": "value = obj\nfor part in attr.split('.'):\n    if not hasattr(value, part) and len(default):\n        return default[0]\n    value = getattr(value, part)\n    if callable(value):\n        value = value()\nreturn value", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "ShawTim/gogogo", "stars": 1, "license": "None", "language": "python", "size": 6961}
{"docstring": "# Allow for using a RegisterVars instance as a context processor\n", "func_signal": "def __call__(self, item=None, name=None):\n", "code": "if isinstance(item, HttpRequest):\n    return self\nif name is None and isinstance(item, basestring):\n    # @register('as_name') # first param (item) contains the name\n    name, item = item, name\nelif name is None and isinstance(item, dict):\n    # register(somedict)  or  register(othermodule.register)\n    return self.update(item)\nif item is None and isinstance(name, basestring):\n    # @register(name='as_name')\n    return lambda func: self(func, name)\nself[name or item.__name__] = item\nreturn item", "path": "common\\appenginepatch\\ragendja\\registervars.py", "repo_name": "ShawTim/gogogo", "stars": 1, "license": "None", "language": "python", "size": 6961}
{"docstring": "\"\"\"\nConverts objects to table-style list of rows with heading:\n\nExample:\nx.a = 1\nx.b = 2\nx.c = 3\ny.a = 11\ny.b = 12\ny.c = 13\nobject_list_to_table(('a', 'b', 'c'), [x, y])\nresults in the following (dict keys reordered for better readability):\n[\n    ('a', 'b', 'c'),\n    (1, 2, 3),\n    (11, 12, 13),\n]\n\"\"\"\n", "func_signal": "def object_list_to_table(headings, dict_list):\n", "code": "return [headings] + [tuple([getattr_by_path(row, heading, None)\n                            for heading in headings])\n                     for row in dict_list]", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "ShawTim/gogogo", "stars": 1, "license": "None", "language": "python", "size": 6961}
{"docstring": "\"\"\"\nTest that ``RegistrationForm`` enforces username constraints\nand matching passwords.\n\n\"\"\"\n", "func_signal": "def test_registration_form(self):\n", "code": "invalid_data_dicts = [\n    # Non-alphanumeric username.\n    {\n    'data':\n    { 'username': 'foo/bar',\n      'email': 'foo@example.com',\n      'password1': 'foo',\n      'password2': 'foo' },\n    'error':\n    ('username', [u\"Enter a valid value.\"])\n    },\n    # Already-existing username.\n    {\n    'data':\n    { 'username': 'alice',\n      'email': 'alice@example.com',\n      'password1': 'secret',\n      'password2': 'secret' },\n    'error':\n    ('username', [u\"This username is already taken. Please choose another.\"])\n    },\n    # Mismatched passwords.\n    {\n    'data':\n    { 'username': 'foo',\n      'email': 'foo@example.com',\n      'password1': 'foo',\n      'password2': 'bar' },\n    'error':\n    ('__all__', [u\"You must type the same password each time\"])\n    },\n    ]\n\nfor invalid_dict in invalid_data_dicts:\n    form = forms.RegistrationForm(data=invalid_dict['data'])\n    self.failIf(form.is_valid())\n    self.assertEqual(form.errors[invalid_dict['error'][0]], invalid_dict['error'][1])\n\nform = forms.RegistrationForm(data={ 'username': 'foo',\n                                     'email': 'foo@example.com',\n                                     'password1': 'foo',\n                                     'password2': 'foo' })\nself.failUnless(form.is_valid())", "path": "registration\\tests.py", "repo_name": "ShawTim/gogogo", "stars": 1, "license": "None", "language": "python", "size": 6961}
{"docstring": "\"\"\"\nTest that ``RegistrationFormTermsOfService`` requires\nagreement to the terms of service.\n\n\"\"\"\n", "func_signal": "def test_registration_form_tos(self):\n", "code": "form = forms.RegistrationFormTermsOfService(data={ 'username': 'foo',\n                                                   'email': 'foo@example.com',\n                                                   'password1': 'foo',\n                                                   'password2': 'foo' })\nself.failIf(form.is_valid())\nself.assertEqual(form.errors['tos'], [u\"You must agree to the terms to register\"])\n\nform = forms.RegistrationFormTermsOfService(data={ 'username': 'foo',\n                                                   'email': 'foo@example.com',\n                                                   'password1': 'foo',\n                                                   'password2': 'foo',\n                                                   'tos': 'on' })\nself.failUnless(form.is_valid())", "path": "registration\\tests.py", "repo_name": "ShawTim/gogogo", "stars": 1, "license": "None", "language": "python", "size": 6961}
{"docstring": "\"\"\"\nTest that the registration view rejects invalid submissions,\nand creates a new user and redirects after a valid submission.\n\n\"\"\"\n# Invalid data fails.\n", "func_signal": "def test_registration_view(self):\n", "code": "alice = User.all().filter('username =', 'alice').get()\nalice.is_active = True\nalice.put()\nresponse = self.client.post(reverse('registration_register'),\n                            data={ 'username': 'alice', # Will fail on username uniqueness.\n                                   'email': 'foo@example.com',\n                                   'password1': 'foo',\n                                   'password2': 'foo' })\nself.assertEqual(response.status_code, 200)\nself.failUnless(response.context[0]['form'])\nself.failUnless(response.context[0]['form'].errors)\n\nresponse = self.client.post(reverse('registration_register'),\n                            data={ 'username': 'foo',\n                                   'email': 'foo@example.com',\n                                   'password1': 'foo',\n                                   'password2': 'foo' })\nself.assertEqual(response.status_code, 302)\nself.assertEqual(response['Location'], 'http://testserver%s' % reverse('registration_complete'))\nself.assertEqual(RegistrationProfile.all().count(), 3)", "path": "registration\\tests.py", "repo_name": "ShawTim/gogogo", "stars": 1, "license": "None", "language": "python", "size": 6961}
{"docstring": "\"\"\"\nTest that user activation actually activates the user and\nproperly resets the activation key, and fails for an\nalready-active or expired user, or an invalid key.\n\n\"\"\"\n# Activating a valid user returns the user.\n", "func_signal": "def test_activation(self):\n", "code": "self.failUnlessEqual(RegistrationProfile.objects.activate_user(RegistrationProfile.all().filter('user =', self.sample_user).get().activation_key).key(),\n                     self.sample_user.key())\n\n# The activated user must now be active.\nself.failUnless(User.get(self.sample_user.key()).is_active)\n\n# The activation key must now be reset to the \"already activated\" constant.\nself.failUnlessEqual(RegistrationProfile.all().filter('user =', self.sample_user).get().activation_key,\n                     RegistrationProfile.ACTIVATED)\n\n# Activating an expired user returns False.\nself.failIf(RegistrationProfile.objects.activate_user(RegistrationProfile.all().filter('user =', self.expired_user).get().activation_key))\n\n# Activating from a key that isn't a SHA1 hash returns False.\nself.failIf(RegistrationProfile.objects.activate_user('foo'))\n\n# Activating from a key that doesn't exist returns False.\nself.failIf(RegistrationProfile.objects.activate_user(sha.new('foo').hexdigest()))", "path": "registration\\tests.py", "repo_name": "ShawTim/gogogo", "stars": 1, "license": "None", "language": "python", "size": 6961}
{"docstring": "\"\"\"\nTest that the ``user_registered`` and ``user_activated``\nsignals are sent, and that they send the ``User`` as an\nargument.\n\n\"\"\"\n", "func_signal": "def test_signals(self):\n", "code": "def receiver(sender, **kwargs):\n    self.assert_('user' in kwargs)\n    self.assertEqual(kwargs['user'].username, u'signal_test')\n    received_signals.append(kwargs.get('signal'))\n\nreceived_signals = []\nexpected_signals = [signals.user_registered, signals.user_activated]\nfor signal in expected_signals:\n    signal.connect(receiver)\n\nRegistrationProfile.objects.create_inactive_user(username='signal_test',\n                                                 password='foo',\n                                                 email='nobody@example.com',\n                                                 send_email=False)\nRegistrationProfile.objects.activate_user(RegistrationProfile.all().filter('user =', db.Key.from_path(User.kind(), 'key_signal_test')).get().activation_key)\n\nself.assertEqual(received_signals, expected_signals)", "path": "registration\\tests.py", "repo_name": "ShawTim/gogogo", "stars": 1, "license": "None", "language": "python", "size": 6961}
{"docstring": "\"\"\"\nTest that ``RegistrationFormNoFreeEmail`` disallows\nregistration with free email addresses.\n\n\"\"\"\n", "func_signal": "def test_registration_form_no_free_email(self):\n", "code": "base_data = { 'username': 'foo',\n              'password1': 'foo',\n              'password2': 'foo' }\nfor domain in ('aim.com', 'aol.com', 'email.com', 'gmail.com',\n               'googlemail.com', 'hotmail.com', 'hushmail.com',\n               'msn.com', 'mail.ru', 'mailinator.com', 'live.com'):\n    invalid_data = base_data.copy()\n    invalid_data['email'] = u\"foo@%s\" % domain\n    form = forms.RegistrationFormNoFreeEmail(data=invalid_data)\n    self.failIf(form.is_valid())\n    self.assertEqual(form.errors['email'], [u\"Registration using free email addresses is prohibited. Please supply a different email address.\"])\n\nbase_data['email'] = 'foo@example.com'\nform = forms.RegistrationFormNoFreeEmail(data=base_data)\nself.failUnless(form.is_valid())", "path": "registration\\tests.py", "repo_name": "ShawTim/gogogo", "stars": 1, "license": "None", "language": "python", "size": 6961}
{"docstring": "\"\"\"\nTest that ``manage.py cleanupregistration`` functions\ncorrectly.\n\n\"\"\"\n", "func_signal": "def test_management_command(self):\n", "code": "management.call_command('cleanupregistration')\nself.assertEqual(RegistrationProfile.all().count(), 1)", "path": "registration\\tests.py", "repo_name": "ShawTim/gogogo", "stars": 1, "license": "None", "language": "python", "size": 6961}
{"docstring": "\"\"\"\nCreate a ``RegistrationProfile`` for a given\n``User``, and return the ``RegistrationProfile``.\n\nThe activation key for the ``RegistrationProfile`` will be a\nSHA1 hash, generated from a combination of the ``User``'s\nusername and a random salt.\n\n\"\"\"\n", "func_signal": "def create_profile(self, user):\n", "code": "salt = sha.new(str(random.random())).hexdigest()[:5]\nactivation_key = sha.new(salt+user.username).hexdigest()", "path": "registration\\models.py", "repo_name": "ShawTim/gogogo", "stars": 1, "license": "None", "language": "python", "size": 6961}
{"docstring": "\"\"\"Returns a subset of the keys of a dictionary.\"\"\"\n", "func_signal": "def subdict(data, *attrs):\n", "code": "result = {}\nresult.update([(key, data[key]) for key in attrs])\nreturn result", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "ShawTim/gogogo", "stars": 1, "license": "None", "language": "python", "size": 6961}
{"docstring": "\"\"\"\nTest that the activation view activates the user from a valid\nkey and fails if the key is invalid or has expired.\n       \n\"\"\"\n# Valid user puts the user account into the context.\n", "func_signal": "def test_activation_view(self):\n", "code": "response = self.client.get(reverse('registration_activate',\n                                   kwargs={ 'activation_key': RegistrationProfile.all().filter('user =', self.sample_user).get().activation_key }))\nself.assertEqual(response.status_code, 200)\nself.assertEqual(response.context[0]['account'].key(), self.sample_user.key())\n\n# Expired user sets the account to False.\nresponse = self.client.get(reverse('registration_activate',\n                                   kwargs={ 'activation_key': RegistrationProfile.all().filter('user =', self.expired_user).get().activation_key }))\nself.failIf(response.context[0]['account'])\n\n# Invalid key gets to the view, but sets account to False.\nresponse = self.client.get(reverse('registration_activate',\n                                   kwargs={ 'activation_key': 'foo' }))\nself.failIf(response.context[0]['account'])\n\n# Nonexistent key sets the account to False.\nresponse = self.client.get(reverse('registration_activate',\n                                   kwargs={ 'activation_key': sha.new('foo').hexdigest() }))\nself.failIf(response.context[0]['account'])", "path": "registration\\tests.py", "repo_name": "ShawTim/gogogo", "stars": 1, "license": "None", "language": "python", "size": 6961}
{"docstring": "\"\"\"\nCompares two lists and returs True if they contain the same elements, but\ndoesn't require that they have the same order.\n\"\"\"\n", "func_signal": "def equal_lists(left, right):\n", "code": "right = list(right)\nif len(left) != len(right):\n    return False\nfor item in left:\n    if item in right:\n        del right[right.index(item)]\n    else:\n        return False\nreturn True", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "ShawTim/gogogo", "stars": 1, "license": "None", "language": "python", "size": 6961}
{"docstring": "\"\"\"\nDetermine whether this ``RegistrationProfile``'s activation\nkey has expired, returning a boolean -- ``True`` if the key\nhas expired.\n\nKey expiration is determined by a two-step process:\n\n1. If the user has already activated, the key will have been\n   reset to the string constant ``ACTIVATED``. Re-activating\n   is not permitted, and so this method returns ``True`` in\n   this case.\n\n2. Otherwise, the date the user signed up is incremented by\n   the number of days specified in the setting\n   ``ACCOUNT_ACTIVATION_DAYS`` (which should be the number of\n   days after signup during which a user is allowed to\n   activate their account); if the result is less than or\n   equal to the current date, the key has expired and this\n   method returns ``True``.\n\n\"\"\"\n", "func_signal": "def activation_key_expired(self):\n", "code": "expiration_date = datetime.timedelta(days=settings.ACCOUNT_ACTIVATION_DAYS)\nreturn self.activation_key == RegistrationProfile.ACTIVATED or \\\n       (self.user.date_joined + expiration_date <= datetime.datetime.now())", "path": "registration\\models.py", "repo_name": "ShawTim/gogogo", "stars": 1, "license": "None", "language": "python", "size": 6961}
{"docstring": "\"\"\"\nTest that ``RegistrationFormUniqueEmail`` validates uniqueness\nof email addresses.\n\n\"\"\"\n", "func_signal": "def test_registration_form_unique_email(self):\n", "code": "form = forms.RegistrationFormUniqueEmail(data={ 'username': 'foo',\n                                                'email': 'alice@example.com',\n                                                'password1': 'foo',\n                                                'password2': 'foo' })\nself.failIf(form.is_valid())\nself.assertEqual(form.errors['email'], [u\"This email address is already in use. Please supply a different email address.\"])\n\nform = forms.RegistrationFormUniqueEmail(data={ 'username': 'foo',\n                                                'email': 'foo@example.com',\n                                                'password1': 'foo',\n                                                'password2': 'foo' })\nself.failUnless(form.is_valid())", "path": "registration\\tests.py", "repo_name": "ShawTim/gogogo", "stars": 1, "license": "None", "language": "python", "size": 6961}
{"docstring": "\"\"\"Get string pair (n bytes, n bytes which are 2 integers just \nbefore pair)\"\"\"\n", "func_signal": "def get_strpair(self):\n", "code": "klen = self.get_int()\nvlen = self.get_int()\nreturn self.recv(klen), self.recv(vlen)", "path": "pyrant\\protocol.py", "repo_name": "clofresh/pyrant", "stars": 1, "license": "apache-2.0", "language": "python", "size": 92}
{"docstring": "\"\"\"Get the number of records in the database\n\"\"\"\n", "func_signal": "def rnum(self):\n", "code": "self._sock.send(self.RNUM)\nreturn self._sock.get_long()", "path": "pyrant\\protocol.py", "repo_name": "clofresh/pyrant", "stars": 1, "license": "apache-2.0", "language": "python", "size": 92}
{"docstring": "\"\"\"Get an integer for given key. Must been added by addint\"\"\"\n", "func_signal": "def getint(self, key):\n", "code": "self._sock.send(self.GET, _ulen(key), key)\nval = self._sock.get_str()\nreturn struct.unpack('I', val)[0]", "path": "pyrant\\protocol.py", "repo_name": "clofresh/pyrant", "stars": 1, "license": "apache-2.0", "language": "python", "size": 92}
{"docstring": "# Craft string that we'll use to send data based on args type and content\n", "func_signal": "def _pack(code, *args):\n", "code": "buf = ''\nfmt = '>BB'\nlargs = []\nfor arg in args:\n    if isinstance(arg, int):\n        fmt += 'I'\n        largs.append(arg)\n\n    elif isinstance(arg, str):\n        buf += arg\n\n    elif isinstance(arg, unicode):\n        buf += arg.encode(ENCODING)\n    \n    elif isinstance(arg, long):\n        fmt += 'Q'\n        largs.append(arg)\n\n    elif isinstance(arg, (list, tuple)):\n        for v in arg:\n            v = str(v)\n            buf += \"%s%s\" % (struct.pack(\">I\", len(v)), v)\n\nreturn \"%s%s\" % (struct.pack(fmt, MAGIC_NUMBER, code, *largs), buf)", "path": "pyrant\\protocol.py", "repo_name": "clofresh/pyrant", "stars": 1, "license": "apache-2.0", "language": "python", "size": 92}
{"docstring": "\"\"\"Set key to value without waiting for a server response\n\"\"\"\n", "func_signal": "def putnr(self, key, value):\n", "code": "self._sock.send(self.PUTNR, _ulen(key), _ulen(value), key, value, \n                 sync=False)", "path": "pyrant\\protocol.py", "repo_name": "clofresh/pyrant", "stars": 1, "license": "apache-2.0", "language": "python", "size": 92}
{"docstring": "\"\"\"Get 2 long numbers (16 bytes) from socket\"\"\"\n", "func_signal": "def get_double(self):\n", "code": "intpart, fracpart = struct.unpack('>QQ', self.recv(16))\nreturn intpart + (fracpart * 1e-12)", "path": "pyrant\\protocol.py", "repo_name": "clofresh/pyrant", "stars": 1, "license": "apache-2.0", "language": "python", "size": 92}
{"docstring": "\"\"\"Get a double for given key. Must been added by adddouble\"\"\"\n", "func_signal": "def getdouble(self, key):\n", "code": "self._sock.send(self.GET, _ulen(key), key)\nval = self._sock.get_str()\nintpart, fracpart = struct.unpack('>QQ', val)\nreturn intpart + (fracpart * 1e-12)", "path": "pyrant\\protocol.py", "repo_name": "clofresh/pyrant", "stars": 1, "license": "apache-2.0", "language": "python", "size": 92}
{"docstring": "\"\"\"Get given bytes from socket\"\"\"\n", "func_signal": "def recv(self, bytes):\n", "code": "d = ''\nwhile len(d) < bytes:\n    d += self._sock.recv(min(8192, bytes - len(d)))\nreturn d", "path": "pyrant\\protocol.py", "repo_name": "clofresh/pyrant", "stars": 1, "license": "apache-2.0", "language": "python", "size": 92}
{"docstring": "\"\"\"Get the size of a value for key\n\"\"\"\n", "func_signal": "def vsiz(self, key):\n", "code": "self._sock.send(self.VSIZ, _ulen(key), key)\nreturn self._sock.get_int()", "path": "pyrant\\protocol.py", "repo_name": "clofresh/pyrant", "stars": 1, "license": "apache-2.0", "language": "python", "size": 92}
{"docstring": "\"\"\"Sum given double to existing one\n\"\"\"\n", "func_signal": "def adddouble(self, key, num):\n", "code": "fracpart, intpart = math.modf(num)\nfracpart, intpart = int(fracpart * 1e12), int(intpart)\nself._sock.send(self.ADDDOUBLE, _ulen(key), long(intpart), \n                long(fracpart), key)\nreturn self._sock.get_double()", "path": "pyrant\\protocol.py", "repo_name": "clofresh/pyrant", "stars": 1, "license": "apache-2.0", "language": "python", "size": 92}
{"docstring": "\"\"\"Get the next key after iterinit\n\"\"\"\n", "func_signal": "def iternext(self):\n", "code": "self._sock.send(self.ITERNEXT)\nreturn self._sock.get_unicode()", "path": "pyrant\\protocol.py", "repo_name": "clofresh/pyrant", "stars": 1, "license": "apache-2.0", "language": "python", "size": 92}
{"docstring": "\"\"\"Get the size of the database\n\"\"\"\n", "func_signal": "def size(self):\n", "code": "self._sock.send(self.SIZE)\nreturn self._sock.get_long()", "path": "pyrant\\protocol.py", "repo_name": "clofresh/pyrant", "stars": 1, "license": "apache-2.0", "language": "python", "size": 92}
{"docstring": "\"\"\"All databases support \"putlist\", \"outlist\", and \"getlist\".\n\"putlist\" is to store records. It receives keys and values one after\nthe other, and returns an empty list.\n\"outlist\" is to remove records. It receives keys, and returns an empty\nlist.\n\"getlist\" is to retrieve records. It receives keys, and returns values\n\nTable database supports \"setindex\", \"search\", \"genuid\".\nopts is a bitflag that can be:\n    RDBMONOULOG to prevent writing to the update log\n\"\"\"\n", "func_signal": "def misc(self, func, args, opts=0):\n", "code": "try:\n    self._sock.send(self.MISC, len(func), opts, len(args), func, args)\nfinally:\n    numrecs = self._sock.get_int()\n\nreturn [self._sock.get_unicode() for i in xrange(numrecs)]", "path": "pyrant\\protocol.py", "repo_name": "clofresh/pyrant", "stars": 1, "license": "apache-2.0", "language": "python", "size": 92}
{"docstring": "\"\"\"Pack arguments and send the buffer to the socket\"\"\"\n", "func_signal": "def send(self, *args, **kwargs):\n", "code": "sync = kwargs.pop('sync', True)\n# Send message to socket, then check for errors as needed.\nself._sock.sendall(_pack(*args))\nif not sync:\n    return\n\nfail_code = ord(self.get_byte())\nif fail_code:\n    raise TyrantError(fail_code)", "path": "pyrant\\protocol.py", "repo_name": "clofresh/pyrant", "stars": 1, "license": "apache-2.0", "language": "python", "size": 92}
{"docstring": "\"\"\"Call func(key, value) with opts\n\nopts is a bitflag that can be RDBXOLCKREC for record locking\nand/or RDBXOLCKGLB for global locking\"\"\"\n", "func_signal": "def ext(self, func, opts, key, value):\n", "code": "self._sock.send(self.EXT, len(func), opts, _ulen(key), _ulen(value),\n                func, key, value)\nreturn self._sock.get_unicode()", "path": "pyrant\\protocol.py", "repo_name": "clofresh/pyrant", "stars": 1, "license": "apache-2.0", "language": "python", "size": 92}
{"docstring": "\"\"\"Get the value of a key from the server\n\"\"\"\n", "func_signal": "def get(self, key, literal=False):\n", "code": "self._sock.send(self.GET, _ulen(key), key)\nreturn self._sock.get_str() if literal else self._sock.get_unicode()", "path": "pyrant\\protocol.py", "repo_name": "clofresh/pyrant", "stars": 1, "license": "apache-2.0", "language": "python", "size": 92}
{"docstring": "\"\"\"Get key,value pairs from the server for the given list of keys\n\"\"\"\n", "func_signal": "def mget(self, klst):\n", "code": "self._sock.send(self.MGET, len(klst), klst)\nnumrecs = self._sock.get_int()\nreturn [self._sock.get_strpair() for i in xrange(numrecs)]", "path": "pyrant\\protocol.py", "repo_name": "clofresh/pyrant", "stars": 1, "license": "apache-2.0", "language": "python", "size": 92}
{"docstring": "\"\"\"Equivalent to::\n\n    self.putcat(key, value)\n    self.put(key, self.get(key)[-width:])\n\"\"\"\n", "func_signal": "def putshl(self, key, value, width):\n", "code": "self._sock.send(self.PUTSHL, _ulen(key), _ulen(value), width, key,\n                value)", "path": "pyrant\\protocol.py", "repo_name": "clofresh/pyrant", "stars": 1, "license": "apache-2.0", "language": "python", "size": 92}
{"docstring": "\"\"\"Get some statistics about the database\n\"\"\"\n", "func_signal": "def stat(self):\n", "code": "self._sock.send(self.STAT)\nreturn self._sock.get_unicode()", "path": "pyrant\\protocol.py", "repo_name": "clofresh/pyrant", "stars": 1, "license": "apache-2.0", "language": "python", "size": 92}
{"docstring": "\"\"\"Sum given integer to existing one\n\"\"\"\n", "func_signal": "def addint(self, key, num):\n", "code": "self._sock.send(self.ADDINT, _ulen(key), num, key)\nreturn self._sock.get_int()", "path": "pyrant\\protocol.py", "repo_name": "clofresh/pyrant", "stars": 1, "license": "apache-2.0", "language": "python", "size": 92}
{"docstring": "\"\"\"Debug a single doctest docstring.\n\nProvide the module (or dotted name of the module) containing the\ntest to be debugged and the name (within the module) of the object\nwith the docstring with tests to be debugged.\n\"\"\"\n", "func_signal": "def debug(module, name, pm=False):\n", "code": "module = _normalize_module(module)\ntestsrc = testsource(module, name)\ndebug_script(testsrc, pm, module.__dict__)", "path": "setuptools\\tests\\doctest.py", "repo_name": "cournape/ensetuptools", "stars": 1, "license": "other", "language": "python", "size": 1374}
{"docstring": "\"\"\"\nInstall a package by fetching it into a temporary directory and then\ncalling easy_install with the appropriate args.\n\"\"\"\n", "func_signal": "def install(self, *args):\n", "code": "tmpdir = mkdtemp(prefix=\"ensetuptools-\")\ntry:\n    location = self.fetch(tmpdir, source, develop)\n    # XXX more sophistication here?\n    info(\"Installing %s\" % self.name)\n    sys.command([\"easy_install\", location] + args)\nfinally:\n    # remove the tmpdir\n    rmtree(tmpdir)", "path": "ensetuptools\\project.py", "repo_name": "cournape/ensetuptools", "stars": 1, "license": "other", "language": "python", "size": 1374}
{"docstring": "\"\"\"\nFind all easy_install repositories on sys.path\n\"\"\"\n", "func_signal": "def get_local_repos():\n", "code": "repos = []\nfor dirname in remove_eggs_from_path(sys.path):\n    if os.path.exists(os.path.join(dirname, \"easy-install.pth\")):\n        repo = EasyInstallRepository(location=dirname)\n        repos.append(repo)\nreturn repos", "path": "ensetuptools\\requirements.py", "repo_name": "cournape/ensetuptools", "stars": 1, "license": "other", "language": "python", "size": 1374}
{"docstring": "\"\"\"\nReport that the given example raised an unexpected exception.\n\"\"\"\n", "func_signal": "def report_unexpected_exception(self, out, test, example, exc_info):\n", "code": "out(self._failure_header(test, example) +\n    'Exception raised:\\n' + _indent(_exception_traceback(exc_info)))", "path": "setuptools\\tests\\doctest.py", "repo_name": "cournape/ensetuptools", "stars": 1, "license": "other", "language": "python", "size": 1374}
{"docstring": "\"\"\"\nReport that the given example failed.\n\"\"\"\n", "func_signal": "def report_failure(self, out, test, example, got):\n", "code": "out(self._failure_header(test, example) +\n    self._checker.output_difference(example, got, self.optionflags))", "path": "setuptools\\tests\\doctest.py", "repo_name": "cournape/ensetuptools", "stars": 1, "license": "other", "language": "python", "size": 1374}
{"docstring": "\"\"\"\nCreate a new test runner.\n\nOptional keyword arg `checker` is the `OutputChecker` that\nshould be used to compare the expected outputs and actual\noutputs of doctest examples.\n\nOptional keyword arg 'verbose' prints lots of stuff if true,\nonly failures if false; by default, it's true iff '-v' is in\nsys.argv.\n\nOptional argument `optionflags` can be used to control how the\ntest runner compares expected output to actual output, and how\nit displays failures.  See the documentation for `testmod` for\nmore information.\n\"\"\"\n", "func_signal": "def __init__(self, checker=None, verbose=None, optionflags=0):\n", "code": "self._checker = checker or OutputChecker()\nif verbose is None:\n    verbose = '-v' in sys.argv\nself._verbose = verbose\nself.optionflags = optionflags\nself.original_optionflags = optionflags\n\n# Keep track of the examples we've run.\nself.tries = 0\nself.failures = 0\nself._name2ft = {}\n\n# Create a fake output target for capturing doctest output.\nself._fakeout = _SpoofOut()", "path": "setuptools\\tests\\doctest.py", "repo_name": "cournape/ensetuptools", "stars": 1, "license": "other", "language": "python", "size": 1374}
{"docstring": "\"\"\"\nRecord the fact that the given DocTest (`test`) generated `f`\nfailures out of `t` tried examples.\n\"\"\"\n", "func_signal": "def __record_outcome(self, test, f, t):\n", "code": "f2, t2 = self._name2ft.get(test.name, (0,0))\nself._name2ft[test.name] = (f+f2, t+t2)\nself.failures += f\nself.tries += t", "path": "setuptools\\tests\\doctest.py", "repo_name": "cournape/ensetuptools", "stars": 1, "license": "other", "language": "python", "size": 1374}
{"docstring": "\"\"\"Extract the test sources from a doctest docstring as a script.\n\nProvide the module (or dotted name of the module) containing the\ntest to be debugged and the name (within the module) of the object\nwith the doc string with tests to be debugged.\n\"\"\"\n", "func_signal": "def testsource(module, name):\n", "code": "module = _normalize_module(module)\ntests = DocTestFinder().find(module)\ntest = [t for t in tests if t.name == name]\nif not test:\n    raise ValueError(name, \"not found in tests\")\ntest = test[0]\ntestsrc = script_from_examples(test.docstring)\nreturn testsrc", "path": "setuptools\\tests\\doctest.py", "repo_name": "cournape/ensetuptools", "stars": 1, "license": "other", "language": "python", "size": 1374}
{"docstring": "\"\"\"\nReturn a DocTest for the given object, if it defines a docstring;\notherwise, return None.\n\"\"\"\n# Extract the object's docstring.  If it doesn't have one,\n# then return None (no test for this object).\n", "func_signal": "def _get_test(self, obj, name, module, globs, source_lines):\n", "code": "if isinstance(obj, basestring):\n    docstring = obj\nelse:\n    try:\n        if obj.__doc__ is None:\n            docstring = ''\n        else:\n            docstring = obj.__doc__\n            if not isinstance(docstring, basestring):\n                docstring = str(docstring)\n    except (TypeError, AttributeError):\n        docstring = ''\n\n# Find the docstring's location in the file.\nlineno = self._find_lineno(obj, source_lines)\n\n# Don't bother if the docstring is empty.\nif self._exclude_empty and not docstring:\n    return None\n\n# Return a DocTest for this object.\nif module is None:\n    filename = None\nelse:\n    filename = getattr(module, '__file__', module.__name__)\n    if filename[-4:] in (\".pyc\", \".pyo\"):\n        filename = filename[:-1]\nreturn self._parser.get_doctest(docstring, globs, name,\n                                filename, lineno)", "path": "setuptools\\tests\\doctest.py", "repo_name": "cournape/ensetuptools", "stars": 1, "license": "other", "language": "python", "size": 1374}
{"docstring": "\"\"\"\nThe two inputs are time tuples for dates and this function will return a simple date\ndisplay difference between the two time tuples.\n\"\"\"\n# Construct simple datetime.date objects which only contain year/month/day\n# and from this we can subtact these objects, which yields a\n# datetime.timedelta object.  The datetime.timedelta object only keeps\n# track of days in difference so we need to calculate the years, months,\n# weeks and day difference.\n# Note: This makes an assumption based on the average number of days\n#       in a month being 30.  We don't need to be exactly precise since\n#       this display date is just meant to offer a simple display.\n", "func_signal": "def date_display_diff(recent, old):\n", "code": "date_diff = (datetime.date(recent[0], recent[1], recent[2])\n             - datetime.date(old[0], old[1], old[2]))\nyears, days = divmod(date_diff.days, 365)\nmonths, days = divmod(days, 30)\nweeks, days = divmod(days, 7)\n\n# Determine the simple display date based on the years, months, weeks,\n# and days difference that was calculated.\ndate_display = \"\"\nif years:\n    if years == 1:\n        date_display = \"Last year\"\n    else:\n        date_display = \"%s years ago\" % years\nelif months:\n    if months == 1:\n        date_display = \"Last month\"\n    else:\n        date_display = \"%s months ago\" % months\nelif weeks:\n    if weeks == 1:\n        date_display = \"Last week\"\n    else:\n        date_display = \"%s weeks ago\" % weeks\nelif days:\n    if days == 1:\n        date_display = \"Yesterday\"\n    else:\n        date_display = \"%s days ago\" % days\nelse:\n    date_display = \"Today\"\n\n# Finally, return the calculated display date.\nreturn date_display", "path": "ensetuptools\\cmdline.py", "repo_name": "cournape/ensetuptools", "stars": 1, "license": "other", "language": "python", "size": 1374}
{"docstring": "\"\"\"\nReport that the test runner is about to process the given\nexample.  (Only displays a message if verbose=True)\n\"\"\"\n", "func_signal": "def report_start(self, out, test, example):\n", "code": "if self._verbose:\n    if example.want:\n        out('Trying:\\n' + _indent(example.source) +\n            'Expecting:\\n' + _indent(example.want))\n    else:\n        out('Trying:\\n' + _indent(example.source) +\n            'Expecting nothing\\n')", "path": "setuptools\\tests\\doctest.py", "repo_name": "cournape/ensetuptools", "stars": 1, "license": "other", "language": "python", "size": 1374}
{"docstring": "\"\"\"\nReturn a dictionary containing option overrides extracted from\noption directives in the given source string.\n\n`name` is the string's name, and `lineno` is the line number\nwhere the example starts; both are used for error messages.\n\"\"\"\n", "func_signal": "def _find_options(self, source, name, lineno):\n", "code": "options = {}\n# (note: with the current regexp, this will match at most once:)\nfor m in self._OPTION_DIRECTIVE_RE.finditer(source):\n    option_strings = m.group(1).replace(',', ' ').split()\n    for option in option_strings:\n        if (option[0] not in '+-' or\n            option[1:] not in OPTIONFLAGS_BY_NAME):\n            raise ValueError('line %r of the doctest for %s '\n                             'has an invalid option: %r' %\n                             (lineno+1, name, option))\n        flag = OPTIONFLAGS_BY_NAME[option[1:]]\n        options[flag] = (option[0] == '+')\nif options and self._IS_BLANK_OR_COMMENT(source):\n    raise ValueError('line %r of the doctest for %s has an option '\n                     'directive on a line with no example: %r' %\n                     (lineno, name, source))\nreturn options", "path": "setuptools\\tests\\doctest.py", "repo_name": "cournape/ensetuptools", "stars": 1, "license": "other", "language": "python", "size": 1374}
{"docstring": "\"\"\"\nReturn a string describing the differences between the\nexpected output for a given example (`example`) and the actual\noutput (`got`).  `optionflags` is the set of option flags used\nto compare `want` and `got`.\n\"\"\"\n", "func_signal": "def output_difference(self, example, got, optionflags):\n", "code": "want = example.want\n# If <BLANKLINE>s are being used, then replace blank lines\n# with <BLANKLINE> in the actual output string.\nif not (optionflags & DONT_ACCEPT_BLANKLINE):\n    got = re.sub('(?m)^[ ]*(?=\\n)', BLANKLINE_MARKER, got)\n\n# Check if we should use diff.\nif self._do_a_fancy_diff(want, got, optionflags):\n    # Split want & got into lines.\n    want_lines = want.splitlines(True)  # True == keep line ends\n    got_lines = got.splitlines(True)\n    # Use difflib to find their differences.\n    if optionflags & REPORT_UDIFF:\n        diff = difflib.unified_diff(want_lines, got_lines, n=2)\n        diff = list(diff)[2:] # strip the diff header\n        kind = 'unified diff with -expected +actual'\n    elif optionflags & REPORT_CDIFF:\n        diff = difflib.context_diff(want_lines, got_lines, n=2)\n        diff = list(diff)[2:] # strip the diff header\n        kind = 'context diff with expected followed by actual'\n    elif optionflags & REPORT_NDIFF:\n        engine = difflib.Differ(charjunk=difflib.IS_CHARACTER_JUNK)\n        diff = list(engine.compare(want_lines, got_lines))\n        kind = 'ndiff with -expected +actual'\n    else:\n        assert 0, 'Bad diff option'\n    # Remove trailing whitespace on diff output.\n    diff = [line.rstrip() + '\\n' for line in diff]\n    return 'Differences (%s):\\n' % kind + _indent(''.join(diff))\n\n# If we're not using diff, then simply list the expected\n# output followed by the actual output.\nif want and got:\n    return 'Expected:\\n%sGot:\\n%s' % (_indent(want), _indent(got))\nelif want:\n    return 'Expected:\\n%sGot nothing\\n' % _indent(want)\nelif got:\n    return 'Expected nothing\\nGot:\\n%s' % _indent(got)\nelse:\n    return 'Expected nothing\\nGot nothing\\n'", "path": "setuptools\\tests\\doctest.py", "repo_name": "cournape/ensetuptools", "stars": 1, "license": "other", "language": "python", "size": 1374}
{"docstring": "\"\"\"prefix, base -> true iff name prefix + \".\" + base is \"private\".\n\nPrefix may be an empty string, and base does not contain a period.\nPrefix is ignored (although functions you write conforming to this\nprotocol may make use of it).\nReturn true iff base begins with an (at least one) underscore, but\ndoes not both begin and end with (at least) two underscores.\n\n>>> is_private(\"a.b\", \"my_func\")\nFalse\n>>> is_private(\"____\", \"_my_func\")\nTrue\n>>> is_private(\"someclass\", \"__init__\")\nFalse\n>>> is_private(\"sometypo\", \"__init_\")\nTrue\n>>> is_private(\"x.y.z\", \"_\")\nTrue\n>>> is_private(\"_x.y.z\", \"__\")\nFalse\n>>> is_private(\"\", \"\")  # senseless but consistent\nFalse\n\"\"\"\n", "func_signal": "def is_private(prefix, base):\n", "code": "warnings.warn(\"is_private is deprecated; it wasn't useful; \"\n              \"examine DocTestFinder.find() lists instead\",\n              DeprecationWarning, stacklevel=2)\nreturn base[:1] == \"_\" and not base[:2] == \"__\" == base[-2:]", "path": "setuptools\\tests\\doctest.py", "repo_name": "cournape/ensetuptools", "stars": 1, "license": "other", "language": "python", "size": 1374}
{"docstring": "\"\"\"square() -> square TestClass's associated value\n\n>>> _TestClass(13).square().get()\n169\n\"\"\"\n\n", "func_signal": "def square(self):\n", "code": "self.val = self.val ** 2\nreturn self", "path": "setuptools\\tests\\doctest.py", "repo_name": "cournape/ensetuptools", "stars": 1, "license": "other", "language": "python", "size": 1374}
{"docstring": "\"\"\"\nExtract all doctest examples from the given string, and\ncollect them into a `DocTest` object.\n\n`globs`, `name`, `filename`, and `lineno` are attributes for\nthe new `DocTest` object.  See the documentation for `DocTest`\nfor more information.\n\"\"\"\n", "func_signal": "def get_doctest(self, string, globs, name, filename, lineno):\n", "code": "return DocTest(self.get_examples(string, name), globs,\n               name, filename, lineno, string)", "path": "setuptools\\tests\\doctest.py", "repo_name": "cournape/ensetuptools", "stars": 1, "license": "other", "language": "python", "size": 1374}
{"docstring": "\"\"\"\nReturn true if the given object should not be examined.\n\"\"\"\n", "func_signal": "def _filter(self, obj, prefix, base):\n", "code": "return (self._namefilter is not None and\n        self._namefilter(prefix, base))", "path": "setuptools\\tests\\doctest.py", "repo_name": "cournape/ensetuptools", "stars": 1, "license": "other", "language": "python", "size": 1374}
{"docstring": "\"\"\"\nFind the site-packages directory\n\"\"\"\n", "func_signal": "def get_site_packages():\n", "code": "site_packages = sysconfig.get_python_lib()\nif site_packages:\n    return EasyInstallRepository(location=site_packages)\nelse:\n    error(\"Can't locate site-packages directory in path.\")", "path": "ensetuptools\\requirements.py", "repo_name": "cournape/ensetuptools", "stars": 1, "license": "other", "language": "python", "size": 1374}
{"docstring": "\"\"\"\nDivide the given string into examples and intervening text,\nand return them as a list of alternating Examples and strings.\nLine numbers for the Examples are 0-based.  The optional\nargument `name` is a name identifying this string, and is only\nused for error messages.\n\"\"\"\n", "func_signal": "def parse(self, string, name='<string>'):\n", "code": "string = string.expandtabs()\n# If all lines begin with the same indentation, then strip it.\nmin_indent = self._min_indent(string)\nif min_indent > 0:\n    string = '\\n'.join([l[min_indent:] for l in string.split('\\n')])\n\noutput = []\ncharno, lineno = 0, 0\n# Find all doctest examples in the string:\nfor m in self._EXAMPLE_RE.finditer(string):\n    # Add the pre-example text to `output`.\n    output.append(string[charno:m.start()])\n    # Update lineno (lines before this example)\n    lineno += string.count('\\n', charno, m.start())\n    # Extract info from the regexp match.\n    (source, options, want, exc_msg) = \\\n             self._parse_example(m, name, lineno)\n    # Create an Example, and add it to the list.\n    if not self._IS_BLANK_OR_COMMENT(source):\n        output.append( Example(source, want, exc_msg,\n                            lineno=lineno,\n                            indent=min_indent+len(m.group('indent')),\n                            options=options) )\n    # Update lineno (lines inside this example)\n    lineno += string.count('\\n', m.start(), m.end())\n    # Update charno.\n    charno = m.end()\n# Add any remaining post-example text to `output`.\noutput.append(string[charno:])\nreturn output", "path": "setuptools\\tests\\doctest.py", "repo_name": "cournape/ensetuptools", "stars": 1, "license": "other", "language": "python", "size": 1374}
{"docstring": "\"\"\"\nEssentially the only subtle case:\n>>> _ellipsis_match('aa...aa', 'aaa')\nFalse\n\"\"\"\n", "func_signal": "def _ellipsis_match(want, got):\n", "code": "if want.find(ELLIPSIS_MARKER)==-1:\n    return want == got\n\n# Find \"the real\" strings.\nws = want.split(ELLIPSIS_MARKER)\nassert len(ws) >= 2\n\n# Deal with exact matches possibly needed at one or both ends.\nstartpos, endpos = 0, len(got)\nw = ws[0]\nif w:   # starts with exact match\n    if got.startswith(w):\n        startpos = len(w)\n        del ws[0]\n    else:\n        return False\nw = ws[-1]\nif w:   # ends with exact match\n    if got.endswith(w):\n        endpos -= len(w)\n        del ws[-1]\n    else:\n        return False\n\nif startpos > endpos:\n    # Exact end matches required more characters than we have, as in\n    # _ellipsis_match('aa...aa', 'aaa')\n    return False\n\n# For the rest, we only need to find the leftmost non-overlapping\n# match for each piece.  If there's no overall match that way alone,\n# there's no overall match period.\nfor w in ws:\n    # w may be '' at times, if there are consecutive ellipses, or\n    # due to an ellipsis at the start or end of `want`.  That's OK.\n    # Search for an empty string succeeds, and doesn't change startpos.\n    startpos = got.find(w, startpos, endpos)\n    if startpos < 0:\n        return False\n    startpos += len(w)\n\nreturn True", "path": "setuptools\\tests\\doctest.py", "repo_name": "cournape/ensetuptools", "stars": 1, "license": "other", "language": "python", "size": 1374}
{"docstring": "# We may want to load an alternate configuration file, or\n# specify alternate values for certain configuration\n# variables. We cannot control them in the usual way (through\n# an environment variable) because we are running in a system\n# environment, so we pass the values as command-line\n# arguments.\n", "func_signal": "def process_arglist(self, args):\n", "code": "evt_log = log_level = admin_address = None\nif len(args) > 1:\n    import getopt\n    try:\n        opts, more = getopt.getopt(args[1:], None,\n                                   ['p4dti-config=',\n                                    'p4dti-evtlog=',\n                                    'p4dti-loglevel=',\n                                    'p4dti-adminaddr=',\n                                    ])\n        for opt, val in opts:\n            if opt == '--p4dti-config':\n                os.environ['P4DTI_CONFIG'] = val\n            if opt == '--p4dti-evtlog':\n                evt_log = 1\n            if opt == '--p4dti-loglevel':\n                log_level = val\n            if opt == '--p4dti-adminaddr':\n                admin_address = val\n        args = [args[0]] + more\n    except:\n        pass\n\n# Now we can load the configuration...\nfrom config_loader import config\n# ... and reconfigure the configuration...\nif evt_log is not None:\n    # Pass any value, to enable logging to NT Event Log\n    config.use_windows_event_log = 1\nif log_level is not None:\n    config.log_level = int(log_level)\nif admin_address is not None:\n    # Pass empty string, to register administrator_address None\n    # (i.e. no mail will be sent).\n    config.administrator_address = admin_address or None\n# When running as an NT service, stdout goes nowhere.\nconfig.use_stdout_log = 0\n# ... and keep a handle on it.\nself.config = config\n\n# Return remaining args\nreturn args", "path": "service.py", "repo_name": "realalien/hs4itd4p", "stars": 1, "license": "other", "language": "python", "size": 1620}
{"docstring": "# The quote_table will make sure that the password is encrypted\n# before being written to the database.\n", "func_signal": "def add_user(self, dict):\n", "code": "dict['cryptpassword'] = dict['password']\ndel dict['password']\nif self.features.has_key('bitset groups'):\n    dict['groupset'] = self.groups_groupset(dict['groups'])\nelse:\n    groups = dict['groups']\ndel dict['groups']\nself.insert_row('profiles', dict)\nu = self.fetch_one_row_as_dictionary('select * from profiles'\n                                     ' where userid = last_insert_id();',\n                                     'user just created')\nuserid = u['userid']\nif self.cache.has_key('users'):\n    self.cache['users'][userid] = u\nif not self.features.has_key('bitset groups'):\n    self.add_user_groups(userid, groups)\nreturn userid", "path": "bugzilla.py", "repo_name": "realalien/hs4itd4p", "stars": 1, "license": "other", "language": "python", "size": 1620}
{"docstring": "# Attempt to log fatal errors before raising them. It's worth\n# doing this because full tracebacks in the Windows Event log\n# can be fairly unreadable.\n", "func_signal": "def run_logging_errors(self):\n", "code": "try:\n    self.run()\nexcept:\n    type, value = sys.exc_info()[:2]\n    self.log_fatal_error(type, value)\n    raise", "path": "service.py", "repo_name": "realalien/hs4itd4p", "stars": 1, "license": "other", "language": "python", "size": 1620}
{"docstring": "# \"The P4DTI service has started.\"\n", "func_signal": "def SvcDoRun(self):\n", "code": "self.log(catalog.msg(1011))\ntry:\n    self.run_logging_errors()\nfinally:\n    # \"The P4DTI service has halted.\"\n    self.log(catalog.msg(1012))", "path": "service.py", "repo_name": "realalien/hs4itd4p", "stars": 1, "license": "other", "language": "python", "size": 1620}
{"docstring": "# Deduce a reporter for the issue, unless we have one already.\n", "func_signal": "def prepare_issue_advanced(config, bz, p4, dict, job):\n", "code": "if dict.get('reporter', \"\") == \"\":\n    for field in ['P4DTI-user', config.job_owner_field, 'User']:\n        if job.has_key(field):\n            p4_user = job[field]\n            # note: this is a lax translator\n            bz_user = config.user_translator.translate_1_to_0(\n                job[field], bz, p4)\n            if bz_user != config.user_translator.bugzilla_id:\n                dict['reporter'] = bz_user\n                break\n\n# Set creation_ts to the 'Date' field, suitably translated.\n# (otherwise creation_ts gets now()).\nif job.has_key(config.job_date_field) and dict.get('creation_ts', '') == '':\n    dict['creation_ts'] = config.date_translator.translate_1_to_0(\n        job[config.job_date_field], bz, p4)\n\n# If no summary, get short description from the first line of the\n# long description.\nif dict.get('short_desc','') == '':\n    short_desc = string.strip(job.get('Description', ''))\n    newline_pos = string.find(short_desc, '\\n')\n    if newline_pos >= 0:\n        short_desc = short_desc[:newline_pos]\n    if short_desc == '':\n        short_desc = 'No description'\n    dict['short_desc'] =  short_desc\n\nbugzilla_resolved_statuses = ['RESOLVED',\n                              'VERIFIED',\n                              'CLOSED']\n\n# Must fill in resolution for new jobs.\nif (dict.has_key('bug_status')\n    and dict['bug_status'] in bugzilla_resolved_statuses\n    and dict.get('resolution','') == ''):\n    if job.get(config.job_status_field, '') == 'suspended':\n        dict['resolution'] = 'LATER'\n    else:\n        dict['resolution'] = 'FIXED'\n\n\n# Supply default values for product, component and version as\n# promised in [GDR 2001-11-14, 3].\nbz.new_issue_defaults(dict)\n\n# Call user hook (see [GDR 2001-11-14, 3]).\nconfig.prepare_issue(dict, job)", "path": "configure_bugzilla.py", "repo_name": "realalien/hs4itd4p", "stars": 1, "license": "other", "language": "python", "size": 1620}
{"docstring": "# \"MySQLdb version '%s' (release '%s') detected.\n# This release is not supported by the P4DTI, but may work.\"\n", "func_signal": "def unsupported(version, release, config):\n", "code": "msg = catalog.msg(1006, (version, release))\nconfig.logger.log(msg)", "path": "mysqldb_support.py", "repo_name": "realalien/hs4itd4p", "stars": 1, "license": "other", "language": "python", "size": 1620}
{"docstring": "# \"User\"\n", "func_signal": "def format_email_table(self, user_dict):\n", "code": "user_header = catalog.msg(868).text\n# \"E-mail address\"\nemail_header = catalog.msg(869).text\nlongest_user = len(user_header)\nlongest_email = len(email_header)\nusers = user_dict.keys()\nusers.sort()\nfor u in users:\n    if len(u) > longest_user:\n        longest_user = len(u)\n    if len(user_dict[u]) > longest_email:\n        longest_email = len(user_dict[u])\nspaces = longest_user + 2 - len(user_header)\ntable = [ \"  %s%s%s\" % (user_header, ' ' * spaces,\n                        email_header),\n          \"  \" + \"-\" * (longest_user + 2 + longest_email) ]\nfor u in users:\n    email = user_dict[u]\n    if email == '':\n        email = '<none>'\n    spaces = longest_user + 2 - len(u)\n    table.append(\"  %s%s%s\" % (u, ' ' * spaces, email))\nreturn string.join(table, \"\\n\")", "path": "replicator.py", "repo_name": "realalien/hs4itd4p", "stars": 1, "license": "other", "language": "python", "size": 1620}
{"docstring": "# Extract any configuration information from the argument\n# list; then load the configuration.\n", "func_signal": "def __init__(self, args):\n", "code": "args = self.process_arglist(args)\n# Initialize ServiceFramework.\nwin32serviceutil.ServiceFramework.__init__(self, args)\n# Create an event which we will use to wait on. The \"service\n# stop\" event request will set this event.\nself.hWaitStop = win32event.CreateEvent(None, 0, 0, None)", "path": "service.py", "repo_name": "realalien/hs4itd4p", "stars": 1, "license": "other", "language": "python", "size": 1620}
{"docstring": "# Figure out Unicode support from p4 info output,\n# if we're using a P4D which provides it there.\n# Otherwise we can look at the \"unicode\" counter\n# (which we can only inspect if we \n", "func_signal": "def check_unicode(self):\n", "code": "info = self.run(\"info\")[0]\nif self.supports('info_unicode'):\n    self.unicode = info.has_key('unicode')\nelse:\n    self.unicode = True # Force -C utf8 on first attempt\n    unicode_counter = self.counter_value('unicode')\n    self.unicode = (unicode_counter == '1')\nif self.unicode:\n    self.encoding = 'utf8'\nelse:\n    (_, self.encoding) = locale.getdefaultlocale()\n    try:\n        codec = codecs.lookup(self.encoding)\n    except LookupError:\n        # default to Latin-1 encoding, which is\n        # at least defined for every octet.\n        self.encoding = 'latin-1'", "path": "p4.py", "repo_name": "realalien/hs4itd4p", "stars": 1, "license": "other", "language": "python", "size": 1620}
{"docstring": "# Do we have a schema from before release 1.0.2?\n", "func_signal": "def ensure_schema_version_0(self):\n", "code": "replications_indexes = self.fetch_rows_as_list_of_dictionaries(\n    \"show index from p4dti_replications\",\n    \"Getting indexes for the p4dti_replications table.\")\nfor i in replications_indexes:\n    if i['Column_name'] == 'end':\n        # We're in release 1.0.2 or later.\n        return\n# \"Your P4DTI/Bugzilla schema is prior to release 1.0.2.\n# Altering tables to upgrade schema to release 1.0.2.\"\nself.log(121)\nfor alteration in [\n    'alter table p4dti_bugs'\n    '  add index(bug_id)',\n    'alter table p4dti_fixes'\n    '  drop index bug_id,'\n    '  drop index changelist,'\n    '  add unique (bug_id, changelist, rid, sid),'\n    '  add index (bug_id)',\n    'alter table p4dti_replications'\n    '  drop index rid,'\n    '  add unique (start, rid, sid),'\n    '  add index (rid, sid),'\n    '  add index (end)'\n    ]:\n    self.execute(alteration)", "path": "bugzilla.py", "repo_name": "realalien/hs4itd4p", "stars": 1, "license": "other", "language": "python", "size": 1620}
{"docstring": "# Create and initialize an instance of replicator.replicator.\n", "func_signal": "def run(self):\n", "code": "from init import r\n# Event loop analagous to that of run() in replicator.py.\nr.prepare_to_run()\nwhile 1:\n    r.carefully_poll_databases()\n    timeout = r.poll_period * 1000            # in milliseconds\n    rc = win32event.WaitForSingleObject(self.hWaitStop, timeout)\n    # Test return code to see whether our Event was signalled.\n    if rc == win32event.WAIT_OBJECT_0:\n        # We've been asked to halt. Bail out of loop:\n        break", "path": "service.py", "repo_name": "realalien/hs4itd4p", "stars": 1, "license": "other", "language": "python", "size": 1620}
{"docstring": "# Before we do anything, tell the Service Manager that we\n# are intending to halt.\n", "func_signal": "def SvcStop(self):\n", "code": "self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)\n# Then set the event.\nwin32event.SetEvent(self.hWaitStop)", "path": "service.py", "repo_name": "realalien/hs4itd4p", "stars": 1, "license": "other", "language": "python", "size": 1620}
{"docstring": "# Is this the first time this logger has run into problems?\n", "func_signal": "def advise_failed(self, err):\n", "code": "if not self.has_failed:\n    self.has_failed = 1\n    context = self.failure_context()\n    self.log_failed_hook(err, context)", "path": "logger.py", "repo_name": "realalien/hs4itd4p", "stars": 1, "license": "other", "language": "python", "size": 1620}
{"docstring": "# get the SID for this user on this system.\n", "func_signal": "def protect_file(filename):\n", "code": "userName = win32api.GetUserName()\nsystemName = '\\\\\\\\' + win32api.GetComputerName()\nuserSID = win32security.LookupAccountName(systemName, userName)[0]\n\n# Initialize an SD.\nsd = win32security.SECURITY_DESCRIPTOR()\n\n# Initialize an ACL.  The magic number 128 is a buffer size,\n# based on some experimental code which suggested that 52\n# would be sufficient.  NB 2003-09-17.\nacl = win32security.ACL(128)\n\n# Add the user to the ACL.\nacl.AddAccessAllowedAce(win32file.FILE_ALL_ACCESS, userSID)\n\n# Set the DACL of the SD to the ACL we've created.\nsd.SetDacl(1, acl, 0)\n\n# Set the DACL of the file to the SD's DACL.\ntry:\n    win32security.SetFileSecurity(filename,\n                                  win32security.DACL_SECURITY_INFORMATION,\n                                  sd)\nexcept:\n    # In some builds of the Python Extensions for Windows,\n    # SetFileSecurity raises a bogus exception!\n    pass", "path": "portable.py", "repo_name": "realalien/hs4itd4p", "stars": 1, "license": "other", "language": "python", "size": 1620}
{"docstring": "# the set of keys which we explictly use in this class.\n", "func_signal": "def __init__(self, bug, dt):\n", "code": "for key in ['bug_id',\n            'reporter',\n            'qa_contact',\n            'everconfirmed',\n            'assigned_to',\n            'groups',\n            'bug_status',\n            'resolution']:\n    assert bug.has_key(key)\nassert isinstance(dt, dt_bugzilla)\nself.dt = dt\nself.bug = bug\nself.p4dti_bug = self.dt.bugzilla.bug_p4dti_bug(bug)", "path": "dt_bugzilla.py", "repo_name": "realalien/hs4itd4p", "stars": 1, "license": "other", "language": "python", "size": 1620}
{"docstring": "# Find bugs new, touched, or changed (by someone other than\n# this replicator) since the given date, which are not\n# being replicated by any other replicator.\n\n# We exclude changes which have the same timestamp as the\n# current replication; they will get picked up by the next\n# replication. This avoids these changes being replicated by\n# two consecutive replications (which causes an overwrite).\n# See job000235.  NB 2001-03-01.  However, it causes\n# job000337.\n\n# We do this by combining the results of three SELECTs.\n# These results are disjoint.  We could almost certainly\n# do it in a smaller number of SELECTs.\n\n# First, bugs which have been created since the date (but not\n# by migration by me from a new Perforce job), which are not\n# being replicated by any other replicator.\n\n", "func_signal": "def changed_bugs_since(self, date):\n", "code": "new_ids = self.fetch_rows_as_list_of_sequences(\n    (\"select bugs.bug_id from bugs \"\n     \"  left join p4dti_bugs using (bug_id) \" # what replication\n     \"  where bugs.creation_ts >= %s \"        # recent timestamp\n     \"    and bugs.creation_ts < %s \"         # NOT just now\n     \"    and (p4dti_bugs.rid is null \"       # NOT replicated\n     \"         or (p4dti_bugs.rid = %s \"      # or replicated by me.\n     \"             and p4dti_bugs.sid = %s \"\n     \"             and p4dti_bugs.migrated is null))\" %\n                                              # but not migrated by me.\n     (self.quote_string(date),\n      self.quote_string(self.replication),\n      self.quote_string(self.rid),\n      self.quote_string(self.sid))),\n    \"new bugs since '%s'\" % date)\n\n# Next, bugs which are not new but have been touched since the\n# date, but not changed, (no matching rows in bugs_activity),\n# which are not being replicated by any other replicator.\n#\n# Note that we have to specifically exclude bugs which we have\n# just migrated, as the migration might set creation_ts.\n\ntouched_ids = self.fetch_rows_as_list_of_sequences(\n    (\"select bugs.bug_id from bugs \"\n     \"  left join p4dti_bugs using (bug_id) \" # what replication\n     \"  left join bugs_activity \"             # what activity\n     \"    on (bugs_activity.bug_when >= %s and \" # since 'date'\n     \"        bugs_activity.bug_when < %s and \" # and NOT just now\n     \"        bugs.bug_id = bugs_activity.bug_id) \" # on this bug\n     \"  where bugs.delta_ts >= %s \"           # since 'date'\n     \"    and bugs.delta_ts < %s \"            # NOT just now\n     \"    and creation_ts < %s \"              # NOT brand new\n     \"    and bugs_activity.fieldid is null\"  # NO recent activity\n     \"    and (p4dti_bugs.rid is null \"       # NOT replicated\n     \"         or (p4dti_bugs.rid = %s \"      # or replicated by me.\n     \"             and p4dti_bugs.sid = %s)) \"\n     \"    and (p4dti_bugs.migrated is null \"  # NOT migrated lately\n     \"         or p4dti_bugs.migrated < %s) \" %\n     (self.quote_string(date),\n      self.quote_string(self.replication),\n      self.quote_string(date),\n      self.quote_string(self.replication),\n      self.quote_string(date),\n      self.quote_string(self.rid),\n      self.quote_string(self.sid),\n      self.quote_string(date))),\n    \"bugs touched since '%s'\" % date)\n\n# Next, bugs which have been changed since the date, by\n# someone other than me, which are not being replicated by\n# any other replicator.\n\nchanged_ids = self.fetch_rows_as_list_of_sequences(\n    (\"select bugs.bug_id from bugs, bugs_activity ba \"  # bug activity\n     \"left join p4dti_bugs using (bug_id) \"        # what replication\n     \"left join p4dti_bugs_activity pba \"   # what replication activity\n     \"  on (ba.bug_id = pba.bug_id and \"    # by me\n     \"      ba.bug_when = pba.bug_when and \"\n     \"      ba.who = pba.who and \"\n     \"      ba.fieldid = pba.fieldid and \"\n     \"      ba.removed = pba.oldvalue and \"\n     \"      ba.added = pba.newvalue and \"\n     \"      pba.rid = %s and \"\n     \"      pba.sid = %s) \"\n     \"  where ba.bug_when >= %s \"        # recent bug activity\n     \"    and ba.bug_when < %s \"         # but not too recent\n     \"    and bugs.bug_id = ba.bug_id \"  # on this bug\n     \"    and pba.rid is null \"          # NO recent activity by me\n     \"    and (p4dti_bugs.rid is null \"  # NOT replicated\n     \"         or (p4dti_bugs.rid = %s \" # or replicated by me\n     \"             and p4dti_bugs.sid =  %s))\"\n     \"    and (bugs.creation_ts < %s or \" # NOT new, or newly\n     \"         p4dti_bugs.migrated is not null) \" # migrated\n     \"  group by bugs.bug_id \" %         # each bug only once\n     (self.quote_string(self.rid),\n      self.quote_string(self.sid),\n      self.quote_string(date),\n      self.quote_string(self.replication),\n      self.quote_string(self.rid),\n      self.quote_string(self.sid),\n      self.quote_string(date))),\n    \"changed bugs since '%s'\" % date)\n\nbug_ids = new_ids + touched_ids + changed_ids\nreturn map(self.bug_from_bug_id, map(lambda b: b[0], bug_ids))", "path": "bugzilla.py", "repo_name": "realalien/hs4itd4p", "stars": 1, "license": "other", "language": "python", "size": 1620}
{"docstring": "# \"Poll starting.\"\n", "func_signal": "def poll_databases(self):\n", "code": "self.log(911)\nif hasattr(self.dt, 'poll_start'):\n    self.dt.poll_start()\ntry:\n    # Get the changed issues (ignore changed changelists if any\n    # since we only replicate changelists from Perforce to the\n    # defect tracker).\n    changed_issues, _, dt_marker = self.dt.changed_entities()\n    # Support old changed_entities specification [GDR\n    # 2000-10-16, 13.1].\n    if not hasattr(changed_issues, 'fetchone'):\n        changed_issues = list_cursor(changed_issues)\n    changed_jobs, changelists, p4_marker = self.changed_entities()\n\n    # Replicate the issues and the jobs.\n    self.replicate_many(changed_issues, changed_jobs)\n\n    # Replicate the affected changelists.\n    if self.feature['fixes']:\n        for c in changelists:\n            self.replicate_changelist_p4_to_dt(c)\n\n    # Tell the defect tracker and Perforce that we've finished\n    # replicating these changes.\n    self.dt.mark_changes_done(dt_marker)\n    self.mark_changes_done(p4_marker)\nfinally:\n    if hasattr(self.dt, 'poll_end'):\n        self.dt.poll_end()\n# \"Poll finished.\"\nself.log(912)", "path": "replicator.py", "repo_name": "realalien/hs4itd4p", "stars": 1, "license": "other", "language": "python", "size": 1620}
{"docstring": "# Find all bugs replicated by this replicator, and all\n# unreplicated bugs new, touched, or changed since the given\n# date.\n\n", "func_signal": "def all_bugs_since(self, date):\n", "code": "bug_ids = self.fetch_rows_as_list_of_sequences(\n    (\"select bugs.bug_id from bugs \"\n     \"  left join p4dti_bugs using (bug_id) \" # what replication\n     \"  where (bugs.delta_ts >= %s \"          # (recently changed\n     \"         or bugs.creation_ts >= %s \"    #  or recently created\n     \"         and p4dti_bugs.rid is null) \"  #  and not replicated)\n     \"     or (p4dti_bugs.rid = %s \"          # or replicated by me.\n     \"         and p4dti_bugs.sid = %s)\" %\n     (self.quote_string(date),\n      self.quote_string(date),\n      self.quote_string(self.rid),\n      self.quote_string(self.sid))),\n    \"all bugs since '%s'\" % date)\nreturn map(self.bug_from_bug_id, map(lambda b: b[0], bug_ids))", "path": "bugzilla.py", "repo_name": "realalien/hs4itd4p", "stars": 1, "license": "other", "language": "python", "size": 1620}
{"docstring": "# all the tables which have per-bug information, and the\n# column name of the bug number in that table.\n", "func_signal": "def delete_bug(self, bug_id):\n", "code": "column_names = {\n    'attachments': 'bug_id',\n    'bug_group_map': 'bug_id',\n    'bugs': 'bug_id',\n    'bugs_activity': 'bug_id',\n    'cc': 'bug_id',\n    'dependencies': 'blocked',\n    'dependencies': 'dependson',\n    'duplicates': 'dupe',\n    'duplicates': 'dupeof',\n    'flags': 'bug_id',\n    'keywords': 'bug_id',\n    'longdescs': 'bug_id',\n    'votes': 'bug_id',\n    'p4dti_bugs': 'bug_id',\n    'p4dti_bugs_activity': 'bug_id',\n    'p4dti_fixes': 'bug_id',\n    'p4dti_filespecs': 'bug_id',\n    }\ntables = self.table_names()\nfor (table, column) in column_names.items():\n    if table in tables:\n        self.delete_rows(table, '%s = %d' % (column, bug_id))\nif self.cache.has_key(('bugs', bug_id)):\n    del self.cache[('bugs', bug_id)]", "path": "bugzilla.py", "repo_name": "realalien/hs4itd4p", "stars": 1, "license": "other", "language": "python", "size": 1620}
{"docstring": "# Check that the p4dti_bugzilla_parameters table exists.  Its\n# presense depends on a new configuration step; its absense is\n# not an error as most users do not need the information\n# stored in it.\n", "func_signal": "def fetch_bugzilla_config(self):\n", "code": "if not self.table_present(\"p4dti_bugzilla_parameters\"):\n    # \"The Bugzilla configuration parameters are missing from\n    # the Bugzilla database.  This means that the P4DTI won't\n    # support Bugzilla features like 'emailsuffix'.  If you need\n    # these features, edit your Bugzilla configuration\n    # parameters and restart the P4DTI.  See section 5.3.3 of\n    # the P4DTI Administrator's Guide.\"\n    self.log(129)\n    self.params = {}\nelse:\n    self.params = self.fetch_simple_rows_as_dictionary(\n        \"select parameter_name, parameter_value \"\n        \"from p4dti_bugzilla_parameters;\",\n        \"bugzilla parameters\")\n    if self.params['p4dti'] == '0':\n        # \"Bugzilla configuration parameter 'p4dti' is turned\n        # off.  You won't see Perforce fixes in Bugzilla until\n        # you turn it on.  See section 5.3.3 of the P4DTI\n        # Administrator's Guide.\"\n        self.log(130)\n    if self.params.get('utf8','0') == '0':\n        # \"Bugzilla is not configured to store text in UTF-8\n        # encoding.  Replication of non-ASCII text data from\n        # Bugzilla may be incorrect.\"\n        self.log(133)", "path": "bugzilla.py", "repo_name": "realalien/hs4itd4p", "stars": 1, "license": "other", "language": "python", "size": 1620}
{"docstring": "\"\"\"\nEither publishes the current container to the CDN or updates its\nCDN attributes.  Requires CDN be enabled on the account.\n\n>>> container.make_public(ttl=604800) # expire in 1 week\n\n@param ttl: cache duration in seconds of the CDN server\n@type ttl: number\n\"\"\"\n", "func_signal": "def make_public(self, ttl=consts.default_cdn_ttl):\n", "code": "if not self.conn.cdn_enabled:\n    raise CDNNotEnabled()\nif self.cdn_uri:\n    request_method = 'POST'\nelse:\n    request_method = 'PUT'\nhdrs = {'X-TTL': str(ttl), 'X-CDN-Enabled': 'True'}\nresponse = self.conn.cdn_request(request_method, [self.name], hdrs=hdrs)\nif (response.status < 200) or (response.status >= 300):\n    raise ResponseError(response.status, response.reason)\nself.cdn_ttl = ttl\nfor hdr in response.getheaders():\n    if hdr[0].lower() == 'x-cdn-uri':\n        self.cdn_uri = hdr[1]", "path": "cloudfiles\\container.py", "repo_name": "jdunck/python-cloudfiles", "stars": 1, "license": "None", "language": "python", "size": 557}
{"docstring": "\"\"\"\nIterate an ObjectResults and verify that it returns Object instances.\nValidate that the count() and index() methods work as expected.\n\"\"\"\n", "func_signal": "def test_get_objects(self):\n", "code": "objects = self.container.get_objects()\nfor storage_object in objects:\n    self.assert_(isinstance(storage_object, Object))\nself.assert_(objects.count('object1') == 1)\nself.assert_(objects.index('object3') == 2)", "path": "tests\\container_test.py", "repo_name": "jdunck/python-cloudfiles", "stars": 1, "license": "None", "language": "python", "size": 557}
{"docstring": "\"\"\"\nFetch the object's CDN data from the CDN service\n\"\"\"\n", "func_signal": "def _fetch_cdn_data(self):\n", "code": "response = self.conn.cdn_request('HEAD', [self.name])\nif (response.status >= 200) and (response.status < 300):\n    for hdr in response.getheaders():\n        if hdr[0].lower() == 'x-cdn-uri':\n            self.cdn_uri = hdr[1]\n        if hdr[0].lower() == 'x-ttl':\n            self.cdn_ttl = int(hdr[1])", "path": "cloudfiles\\container.py", "repo_name": "jdunck/python-cloudfiles", "stars": 1, "license": "None", "language": "python", "size": 557}
{"docstring": "\"\"\"\nIterate an ObjectResults and verify that it returns Object instances.\nValidate that the count() and index() methods work as expected.\n\"\"\"\n", "func_signal": "def test_get_objects_parametrized(self):\n", "code": "objects = self.container.get_objects(prefix='object', limit=3, \n                                     offset=3, path='/')\nfor storage_object in objects:\n    self.assert_(isinstance(storage_object, Object))\nself.assert_(objects.count('object4') == 1)\nself.assert_(objects.index('object6') == 2)", "path": "tests\\container_test.py", "repo_name": "jdunck/python-cloudfiles", "stars": 1, "license": "None", "language": "python", "size": 557}
{"docstring": "\"\"\"\nVerify that limit & order query parameters work.\n\"\"\"\n", "func_signal": "def test_list_objects_limited(self):\n", "code": "self.assert_(len(self.container.list_objects(limit=3)) == 3)\nself.assert_(len(self.container.list_objects(limit=3, offset=3)) == 3)", "path": "tests\\container_test.py", "repo_name": "jdunck/python-cloudfiles", "stars": 1, "license": "None", "language": "python", "size": 557}
{"docstring": "\"\"\"\nPermanently remove a storage object.\n\n>>> container.list_objects()\n['new_object', 'old_object']\n>>> container.delete_object('old_object')\n>>> container.list_objects()\n['new_object']\n\n@param object_name: the name of the object to retrieve\n@type object_name: str\n\"\"\"\n", "func_signal": "def delete_object(self, object_name):\n", "code": "if isinstance(object_name, Object):\n    object_name = object_name.name\nif not object_name:\n    raise InvalidObjectName(object_name)\nresponse = self.conn.make_request('DELETE', [self.name, object_name])\nif (response.status < 200) or (response.status > 299):\n    buff = response.read()\n    raise ResponseError(response.status, response.reason)\nbuff = response.read()", "path": "cloudfiles\\container.py", "repo_name": "jdunck/python-cloudfiles", "stars": 1, "license": "None", "language": "python", "size": 557}
{"docstring": "\"\"\"\nVerify that Container.get_object() returns an Object instance.\n\"\"\"\n", "func_signal": "def test_get_object(self):\n", "code": "storage_object = self.container.get_object('object1')\nself.assert_(isinstance(storage_object, Object))", "path": "tests\\container_test.py", "repo_name": "jdunck/python-cloudfiles", "stars": 1, "license": "None", "language": "python", "size": 557}
{"docstring": "\"\"\"\nEnsure headers are being set.\n\"\"\"\n", "func_signal": "def test_headers(self):\n", "code": "self.assert_(self.auth.headers['x-auth-user'] == 'jsmith', \\\n       \"storage user header not properly assigned\")\nself.assert_(self.auth.headers['x-auth-key'] == 'xxxxxxxx', \\\n       \"storage password header not properly assigned\")", "path": "tests\\authentication_test.py", "repo_name": "jdunck/python-cloudfiles", "stars": 1, "license": "None", "language": "python", "size": 557}
{"docstring": "\"\"\"\nDisables CDN access to this container.\nIt may continue to be available until its TTL expires.\n\n>>> container.make_private()\n\"\"\"\n", "func_signal": "def make_private(self):\n", "code": "if not self.conn.cdn_enabled:\n    raise CDNNotEnabled()\nhdrs = {'X-CDN-Enabled': 'False'}\nself.cdn_uri = None\nresponse = self.conn.cdn_request('POST', [self.name], hdrs=hdrs)\nif (response.status < 200) or (response.status >= 300):\n    raise ResponseError(response.status, response.reason)", "path": "cloudfiles\\container.py", "repo_name": "jdunck/python-cloudfiles", "stars": 1, "license": "None", "language": "python", "size": 557}
{"docstring": "# slashes make for invalid names\n", "func_signal": "def __set_name(self, name):\n", "code": "if isinstance(name, (str, unicode)) and \\\n        ('/' in name or len(name) > consts.container_name_limit):\n    raise InvalidContainerName(name)\nself._name = name", "path": "cloudfiles\\container.py", "repo_name": "jdunck/python-cloudfiles", "stars": 1, "license": "None", "language": "python", "size": 557}
{"docstring": "\"\"\"\nContainers will rarely if ever need to be instantiated directly by the\nuser.\n\nInstead, use the L{create_container<Connection.create_container>},\nL{get_container<Connection.get_container>},\nL{list_containers<Connection.list_containers>} and\nother methods on a valid Connection object.\n\"\"\"\n", "func_signal": "def __init__(self, connection=None, name=None, count=None, size=None):\n", "code": "self._name = None\nself.name = name\nself.conn = connection\nself.object_count = count\nself.size_used = size\nself.cdn_uri = None\nself.cdn_ttl = None\nif connection.cdn_enabled:\n    self._fetch_cdn_data()", "path": "cloudfiles\\container.py", "repo_name": "jdunck/python-cloudfiles", "stars": 1, "license": "None", "language": "python", "size": 557}
{"docstring": "\"\"\"\nInitiates authentication with the remote service and returns a \ntwo-tuple containing the storage system URL and session token.\n\"\"\"\n", "func_signal": "def authenticate(self):\n", "code": "conn = self.conn_class(self.host, self.port)\nconn.request('GET', self.authurl, '', self.headers)\nresponse = conn.getresponse()\nbuff = response.read()\n\n# A status code of 401 indicates that the supplied credentials\n# were not accepted by the authentication service.\nif response.status == 401:\n    raise AuthenticationFailed()\n\nif response.status != 204:\n    raise ResponseError(response.status, response.reason)\n\nstorage_url = cdn_url = auth_token = None\n\nfor hdr in response.getheaders():\n    if hdr[0].lower() == \"x-storage-url\":\n        storage_url = hdr[1]\n    if hdr[0].lower() == \"x-cdn-management-url\":\n        cdn_url = hdr[1]\n    if hdr[0].lower() == \"x-storage-token\":\n        auth_token = hdr[1]\n    if hdr[0].lower() == \"x-auth-token\":\n        auth_token = hdr[1]\n\nconn.close()\n\nif not (auth_token and storage_url):\n    raise AuthenticationError(\"Invalid response from the \" \\\n            \"authentication service.\")\n\nreturn (storage_url, cdn_url, auth_token)", "path": "cloudfiles\\authentication.py", "repo_name": "jdunck/python-cloudfiles", "stars": 1, "license": "None", "language": "python", "size": 557}
{"docstring": "\"\"\"\nVerify that the prefix query parameter works.\n\"\"\"\n", "func_signal": "def test_list_objects_path(self):\n", "code": "self.assert_(isinstance(\n        self.container.list_objects(path='/'), list))", "path": "tests\\container_test.py", "repo_name": "jdunck/python-cloudfiles", "stars": 1, "license": "None", "language": "python", "size": 557}
{"docstring": "\"\"\"\nSanity check authentication method stub (lame).\n\"\"\"\n", "func_signal": "def test_authenticate(self):\n", "code": "self.assert_(self.auth.authenticate() == (None, None, None), \\\n       \"authenticate() did not return a two-tuple\")", "path": "tests\\authentication_test.py", "repo_name": "jdunck/python-cloudfiles", "stars": 1, "license": "None", "language": "python", "size": 557}
{"docstring": "\"\"\"\nVerify that Container.create_object() returns an Object instance.\n\"\"\"\n", "func_signal": "def test_create_object(self):\n", "code": "storage_object = self.container.create_object('object1')\nself.assert_(isinstance(storage_object, Object))", "path": "tests\\container_test.py", "repo_name": "jdunck/python-cloudfiles", "stars": 1, "license": "None", "language": "python", "size": 557}
{"docstring": "\"\"\"\nReturn the URI for this container, if it is publically\naccessible via the CDN.\n\n>>> connection['container1'].public_uri()\n'http://cdn.cloudfiles.mosso.com/c61'\n\n@rtype: str\n@return: the public URI for this container\n\"\"\"\n", "func_signal": "def public_uri(self):\n", "code": "if not self.is_public():\n    raise ContainerNotPublic()\nreturn self.cdn_uri", "path": "cloudfiles\\container.py", "repo_name": "jdunck/python-cloudfiles", "stars": 1, "license": "None", "language": "python", "size": 557}
{"docstring": "\"\"\"\nVerify that the prefix query parameter works.\n\"\"\"\n", "func_signal": "def test_list_objects_prefixed(self):\n", "code": "self.assert_(isinstance(\n        self.container.list_objects(prefix='object'), list))", "path": "tests\\container_test.py", "repo_name": "jdunck/python-cloudfiles", "stars": 1, "license": "None", "language": "python", "size": 557}
{"docstring": "\"\"\"\nValidate authentication uri construction.\n\"\"\"\n", "func_signal": "def test_get_uri(self):\n", "code": "self.assert_(self.auth.uri == \"auth\", \\\n       \"authentication URL was not properly constructed\")", "path": "tests\\authentication_test.py", "repo_name": "jdunck/python-cloudfiles", "stars": 1, "license": "None", "language": "python", "size": 557}
{"docstring": "\"\"\"\nReturns a boolean indicating whether or not this container is\npublically accessible via the CDN.\n\n>>> container.is_public()\nFalse\n>>> container.make_public()\n>>> container.is_public()\nTrue\n\n@rtype: bool\n@return: whether or not this container is published to the CDN\n\"\"\"\n", "func_signal": "def is_public(self):\n", "code": "if not self.conn.cdn_enabled:\n    raise CDNNotEnabled()\nreturn self.cdn_uri is not None", "path": "cloudfiles\\container.py", "repo_name": "jdunck/python-cloudfiles", "stars": 1, "license": "None", "language": "python", "size": 557}
{"docstring": "\"\"\"\nEnsure you can't assign an invalid name.\n\"\"\"\n", "func_signal": "def test_bad_name_assignment(self):\n", "code": "basket = Container(self.conn) \ntry:\n    basket.name = 'yougivelove/abadname'\n    self.fail(\"InvalidContainerName exception not raised!\")\nexcept InvalidContainerName:\n    pass\n\ntry:\n    basket.name = 'a'*(container_name_limit+1)\n    self.fail(\"InvalidContainerName exception not raised!\")\nexcept InvalidContainerName:\n    pass", "path": "tests\\container_test.py", "repo_name": "jdunck/python-cloudfiles", "stars": 1, "license": "None", "language": "python", "size": 557}
{"docstring": "''' each individual lives on its own without competition with other individuals '''\n", "func_signal": "def no_competition_survival(world, individuals):\n", "code": "for individual in individuals:\n    individual.raw_fitness = individual.base_fitness", "path": "ga.py", "repo_name": "tneumann/skeletal_structures_ga", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "# fitness\n", "func_signal": "def _init_plots(self):\n", "code": "self.fitness_plot_data = ArrayPlotData(generation=N.zeros(1), best=N.zeros(1), average=N.zeros(1))\nself.fitness_plot = Plot(self.fitness_plot_data)\nself.fitness_plot.legend.visible = True\nself.fitness_plot.set(padding_top = 5, padding_right = 5, padding_bottom = 20, padding_left = 40)\nself.fitness_plot.plot(('generation', 'best'), color='green', line_width=2, name='best')\nself.fitness_plot.plot(('generation', 'average'), color='black', name='avg')\n## stability\n#self.stability_plot_data = ArrayPlotData(generation=N.zeros(1), min=N.zeros(1), max=N.zeros(1), average=N.zeros(1))\n#self.stability_plot = Plot(self.stability_plot_data, height=100)\n#self.stability_plot.legend.visible = True\n#self.stability_plot.set(padding_top = 15, padding_right = 5, padding_bottom = 20, padding_left = 40, title='Stability')\n#self.stability_plot.plot(('generation', 'min'), color='red', line_width=2, name='min')\n#self.stability_plot.plot(('generation', 'average'), color='black', name='avg')\n#self.stability_plot.plot(('generation', 'max'), color='green', line_width=2, name='max')\n## feasible\n#self.feasible_plot_data = ArrayPlotData(generation=N.zeros(1), num=N.zeros(1))\n#self.feasible_plot = Plot(self.feasible_plot_data)\n#self.feasible_plot.set(padding_top = 15, padding_right = 5, padding_bottom = 20, padding_left = 40, title = 'Unfeasible Individuals')\n#self.feasible_plot.plot(('generation', 'num'), color='red', line_width=2)", "path": "editor.py", "repo_name": "tneumann/skeletal_structures_ga", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "# TODO: check if construction compatible to base construction\n", "func_signal": "def individual_from_construction(self, construction=None, individual=None):\n", "code": "if construction is None:\n    construction = self.construction\nif individual is None:\n    individual = ConstructionIndividual(joint_positions_gene = N.zeros(self.joint_position_gene_size, N.double), \n        element_material_gene = N.zeros(self.element_material_gene_size, N.uint))\ni = 0\nfor joint in construction.joints:\n    if joint.mutatable_x:\n        individual.joint_positions_gene[i] = joint.x\n        i += 1\n    if joint.mutatable_y:\n        individual.joint_positions_gene[i] = joint.y\n        i += 1\ni = 0\nfor element in construction.elements:\n    if element.mutatable_material:\n        individual.element_material_gene[i] = self.gene_index_for_element_material[element.material]\n        i += 1\nreturn individual", "path": "ga_truss.py", "repo_name": "tneumann/skeletal_structures_ga", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "#app.bw_mode()\n", "func_signal": "def screenshot(app, path='/tmp/sc.png', magnification=2):\n", "code": "app._text_actor.text_property.font_size *= magnification\napp._label_actor.mapper.label_text_property.font_size = 14*magnification\napp.scene.magnification = magnification\napp.scene.save(path)\napp._label_actor.mapper.label_text_property.font_size = 14\napp._text_actor.text_property.font_size /= magnification", "path": "screenshot.py", "repo_name": "tneumann/skeletal_structures_ga", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "# TODO: check if construction compatible to base construction\n", "func_signal": "def construction_from_individual(self, individual, construction=None):\n", "code": "if construction is None:\n    old_construction = self.construction\n    construction = self.construction.clone_traits()\n    construction.element_deleted_material = old_construction.element_deleted_material\nfor joint,(x,y) in zip(construction.joints, self.joint_positions_for_individual(individual)):\n    joint.x = x\n    joint.y = y\nfor element, mat in zip(construction.elements, self.materials_for_individual(individual)):\n    element.material = mat\nconstruction.from_individual = individual\nreturn construction", "path": "ga_truss.py", "repo_name": "tneumann/skeletal_structures_ga", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "''' initialize empty population from initializer function and evaluate first population '''\n", "func_signal": "def init_evolution(self):\n", "code": "cur_pop = Population(individuals = [self.init_individual(self.world) for i in xrange(self.population_size)], number=1)\nself.populations = [cur_pop]\n# calculate base fitness for each of the individuals\nfor individual in cur_pop.individuals:\n    self._call_birth_callback(individual)\n    individual.base_fitness = self.fitness_function(self.world, individual)\nself._calc_base_fitness(cur_pop)\nself.survival(self.world, cur_pop.individuals)\nself.num_steps = 0\nself.inited = True\n#self.best_raw_fitness_history = []\n#self.average_raw_fitness_history = []\n#self.worst_raw_fitness_history = []\nself.on_init = True", "path": "ga.py", "repo_name": "tneumann/skeletal_structures_ga", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "# genotype -> phenotype ;-)\n#self.construction = world.construction_from_individual(self)\n", "func_signal": "def on_birth(self, world):\n", "code": "self.joint_positions = world.joint_positions_for_individual(self)\nself.element_materials = world.materials_for_individual(self)\n# TODO: can be faster via lookup for next 4 calls\nelement_E = N.array( [material.E for material in self.element_materials], N.double)\nelement_r = N.array( [material.radius for material in self.element_materials], N.double)\nelement_p = N.array( [material.density for material in self.element_materials], N.double)\njfreedom = world.construction.get_adjusted_joint_freedom_array(self.element_materials)\n# analyze the individual\nself.joint_displacements, self.element_strains, self.element_stresses, self.mass, self.simulation_status, \\\n    (time_preparation, time_solution, time_postprocessing) = \\\n            physics.analyse_truss(self.joint_positions, jfreedom, world.construction.loads_array, \n                    world.construction.element_index_table, element_E, element_r, element_p)\nself.stability = 1 - (N.max(N.abs(self.element_stresses) / world.construction.max_element_stress_array))\n# calculate constraints\nself.stability_constraint = N.max( N.abs(self.element_stresses) / world.construction.max_element_stress_array ) - 1\n# calculate displacement constraints\nself.displacement_constraint = N.max( N.abs(self.joint_displacements) / world.construction.max_joint_displacement_array ) - 1\n# combine constraints into feasibility\nself.feasability = max(self.stability_constraint, self.displacement_constraint)\nself.feasible = self.stability_constraint <= 0 and self.displacement_constraint <= 0\nself.got_penalized = False", "path": "ga_truss.py", "repo_name": "tneumann/skeletal_structures_ga", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "# pin joint if only deleted members are connected to the joint in question\n# to do this check for each joint if it has at least on non-deleted element\n", "func_signal": "def get_adjusted_joint_freedom_array(self, new_element_materials):\n", "code": "has_non_deleted_elements = N.zeros(len(self.joints), N.bool)\nfor (j1,j2), mat in zip(self.element_index_table, new_element_materials):\n    if mat != self.element_deleted_material:\n        has_non_deleted_elements[j1] = True\n        has_non_deleted_elements[j2] = True\n# joint is discarded as beeing \"deleted\" if it has no non-deleted element ( -> only deleted elements connected)\njfreedom = self.joint_freedom_array.copy()\njfreedom[~has_non_deleted_elements] = (False, False)\nreturn jfreedom", "path": "model.py", "repo_name": "tneumann/skeletal_structures_ga", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "# need to disable the handle widgets?\n", "func_signal": "def _interaction_mode_changed(self):\n", "code": "for h in self._handle_by_joint.values():\n    h.process_events = self.interaction_mode in ['select', 'add_element']\n# cursor for adding\nif self.interaction_mode == 'add_joint':\n    self.scene.render_window.current_cursor = 10", "path": "editor.py", "repo_name": "tneumann/skeletal_structures_ga", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "# apply crossover to joint positions gene\n", "func_signal": "def __call__(self, crossover_rate, world, mummy, daddy, sister, brother):\n", "code": "sister.joint_positions_gene, brother.joint_positions_gene = \\\n    self.joint_position_crossover(crossover_rate, world, \n            mummy.joint_positions_gene, daddy.joint_positions_gene)\n# apply crossover to element material gene\nsister.element_material_gene, brother.element_material_gene = \\\n    self.element_material_crossover(crossover_rate, world, \n            mummy.element_material_gene, daddy.element_material_gene)", "path": "ga_truss.py", "repo_name": "tneumann/skeletal_structures_ga", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "# check if addable\n", "func_signal": "def _handle_elements(self, removed, added):\n", "code": "for a in added:\n    if self._is_duplicate_element(a):\n        raise DuplicateElement\n    #self._check_element_material_change(a, a.material)\n    #a.on_trait_change(self._check_element_material_change, 'material')", "path": "model.py", "repo_name": "tneumann/skeletal_structures_ga", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "# scalar bar for strain\n", "func_signal": "def _setup_scene(self):\n", "code": "lut_strain = tvtk.LookupTable(hue_range=(0.66, 0.0))\nlut_strain.build()\nself._scalar_bar_strain = tvtk.ScalarBarActor(lookup_table=lut_strain, orientation='horizontal',\n        text_position='succeed_scalar_bar', maximum_number_of_colors=256, number_of_labels=9,\n        position=(0.1, 0.01), position2=(0.8, 0.08), title='element strain (%)', visibility=False)\nself.scene.add_actor(self._scalar_bar_strain)\n# scalar bar for stress\n# lookup table from green to yellow, and last 2 values dark red\nlut_stress = tvtk.LookupTable(hue_range=(0.33, 0.1), number_of_table_values=256)\nlut_stress.build()\nlut_stress.set_table_value(254, (1.0, 0.4, 0.0, 1.0))\nlut_stress.set_table_value(255, (1.0, 0.0, 0.0, 1.0))\nself._scalar_bar_stress = tvtk.ScalarBarActor(lookup_table=lut_stress, orientation='horizontal',\n        text_position='succeed_scalar_bar', maximum_number_of_colors=256, number_of_labels=9,\n        position=(0.1, 0.01), position2=(0.8, 0.08), title='element stress', visibility=False)\nself.scene.add_actor(self._scalar_bar_stress)\n# setup elements visualization\nself._elements_polydata = tvtk.PolyData()\nself._tubes = tvtk.TubeFilter(input=self._elements_polydata, number_of_sides=6, \n        vary_radius='vary_radius_by_absolute_scalar', radius_factor=1.5)\nmapper = tvtk.PolyDataMapper(input=self._tubes.output, lookup_table=lut_strain, \n        interpolate_scalars_before_mapping=True, scalar_mode='use_cell_data')\nself._elements_actor = tvtk.Actor(mapper=mapper)\nself.scene.add_actor(self._elements_actor)\n# show elements in deformed state as wireframe\nself._deformed_elements_polydata = tvtk.PolyData()\nself._deformed_elements_actor = tvtk.Actor(mapper=tvtk.PolyDataMapper(input=self._deformed_elements_polydata))\nself._deformed_elements_actor.property.set(opacity = 0.2, representation = 'wireframe')\nself.scene.add_actor(self._deformed_elements_actor)\n# highlight one element via a ribbon outline\nself._hl_element_ribbons = tvtk.RibbonFilter(input=tvtk.PolyData(), use_default_normal=True, width=1.0)\nself._hl_element_actor = tvtk.Actor(mapper=tvtk.PolyDataMapper(input=self._hl_element_ribbons.output), visibility=False)\nself._hl_element_actor.property.set(ambient=1, ambient_color=(1,1,1), diffuse=0)\nself.scene.add_actor(self._hl_element_actor)\n# cross sectional radius labels\nself._elements_label_polydata = tvtk.PolyData()\nself._label_cellcenters = tvtk.CellCenters(input=self._elements_label_polydata)\nself._label_visps = tvtk.SelectVisiblePoints(renderer=self.scene.renderer, input=self._label_cellcenters.output, tolerance=10000)\nself._label_actor = tvtk.Actor2D(mapper=tvtk.Dynamic2DLabelMapper(input=self._label_visps.output, label_mode='label_scalars'))\nself._label_actor.mapper.label_text_property.set(bold=True, italic=False, justification='centered', font_size=14)\n#self.scene.add_actor(self._label_actor)\n# force glyphs (use arrows for that)\nself._force_glyphs = tvtk.Glyph3D(scale_mode='scale_by_vector', vector_mode='use_vector', \n        color_mode='color_by_vector', scale_factor=1.)\nself._force_glyphs.set_source(0, tvtk.ArrowSource(shaft_radius=0.04, tip_resolution=8, tip_radius=0.2).output)\nself._force_polydata = tvtk.PolyData()\nself._force_glyphs.set_input(0, self._force_polydata)\nself._force_actor = tvtk.Actor(mapper=tvtk.PolyDataMapper(input=self._force_glyphs.output, scalar_range=(0,10)))\nself._force_actor.mapper.lookup_table.hue_range = (0.33, 0.0)\nself.scene.add_actor(self._force_actor)\n# current status display\nself._text_actor = tvtk.TextActor(position=(0.5,0.95))\nself._text_actor.position_coordinate.coordinate_system = 'normalized_display'\nself._text_actor.text_property.set(font_size=14, justification='center')\nself.scene.add_actor(self._text_actor)\n# a nice gradient background\nself.scene.renderer.set(background2=(0.28, 0.28, 0.28), background=(0.01, 0.01, 0.02), gradient_background=True)\n# setup events\nself.interaction_mode = 'select'\nself.scene.interactor.add_observer('MouseMoveEvent', self._mouse_move)\nself.scene.interactor.add_observer('LeftButtonPressEvent', self._mouse_press)\nself.scene.renderer.add_observer('StartEvent', self._before_render)\nself._on_init = True\nself._reset_zoom_needed = True", "path": "editor.py", "repo_name": "tneumann/skeletal_structures_ga", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "#print \"redraw_joint_forces\"\n", "func_signal": "def _redraw_joint_forces(self):\n", "code": "if self.construction:\n    points = []\n    vectors = []\n    for joint in self.construction.joints:\n        if joint.force_x != 0 or joint.force_y != 0:\n            points.append((joint.x, joint.y, 0))\n            vectors.append((joint.force_x, joint.force_y, 0))\n    if len(points) > 0:\n        self._force_polydata.points = points\n        self._force_polydata.point_data.vectors = vectors\n    else:\n        self._force_polydata.points = None\n        self._force_polydata.point_data.vectors = None", "path": "editor.py", "repo_name": "tneumann/skeletal_structures_ga", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "# find out worst fitness from all individuals that are feasible (no constraint violated)\n", "func_signal": "def __call__(self, world, individuals):\n", "code": "base_fitnesses_feasible = [i.base_fitness for i in individuals if i.feasible]\nif len(base_fitnesses_feasible) > 0:\n    worst_feasible_fitness = max(base_fitnesses_feasible)\n    for individual in individuals:\n        individual.raw_fitness = individual.base_fitness\n        if self.unfeasible_constraint_penalty and not individual.feasible:\n            #print \"penalty \", individual.feasability\n            individual.raw_fitness = individual.raw_fitness + worst_feasible_fitness * (individual.feasability+1)\nelse:\n    #print \"all unfeasible, choosing stability fitness function!\"\n    worst_fitness = max([i.base_fitness for i in individuals])\n    for individual in individuals:\n        individual.raw_fitness = (individual.feasability+1) + worst_fitness\n\n# TODO: document/cleanup/paremetrize\n# variety reward\nif self.force_variety_topology and len(individuals[0].element_material_gene) > 0:\n    # topology (1=member present, 0=member absent)\n    all_element_genes = N.array([i.element_material_gene < 0 for i in individuals], N.int)\n    all_fitness = N.array([i.raw_fitness for i in individuals])\n    bad_individual = N.zeros(len(individuals), N.bool)\n    for i,individual in enumerate(individuals):\n        # find duplicates to that topology\n        duplicates = N.flatnonzero(N.abs(all_element_genes[i] - all_element_genes).sum(axis=1) < self.topology_variety_distance) # indices to duplicates\n        # select worst (highest fitness) individuals from those duplicaates\n        if len(duplicates) > self.num_similar_individuals:\n            worst_duplicates = duplicates[all_fitness[duplicates].argsort()[self.num_similar_individuals:]]\n            bad_individual[worst_duplicates] = True\n    for individual_index in N.flatnonzero(bad_individual):\n        individual = individuals[individual_index]\n        individual.raw_fitness = individual.raw_fitness + 1e+6\n        individual.got_penalized = True\n\nif self.force_variety_size and len(individuals[0].element_material_gene) > 0:\n    all_element_genes = N.maximum(N.array([i.element_material_gene for i in individuals]), -1) # max(..,-1) because deleted elements are all < 0\n    all_fitness = N.array([i.raw_fitness for i in individuals])\n    bad_individual = N.zeros(len(individuals), N.bool)\n    #print all_element_genes\n    for i,individual in enumerate(individuals):\n        # find duplicates to that topology\n        duplicates = N.flatnonzero((all_element_genes[i] == all_element_genes).all(axis=1)) # indices to duplicates\n        # select worst (highest fitness) individuals from those duplicaates\n        if len(duplicates) > self.num_similar_individuals:\n            worst_duplicates = duplicates[all_fitness[duplicates].argsort()[self.num_similar_individuals:]]\n            bad_individual[worst_duplicates] = True\n    for individual_index in N.flatnonzero(bad_individual):\n        individual = individuals[individual_index]\n        individual.raw_fitness = individual.raw_fitness + 1e+6\n        individual.got_penalized = True\n        #print \"bad: \", individual.element_material_gene\n\nif self.force_variety_shape and len(individuals[0].joint_positions_gene) > 0:\n    all_jp_genes = N.array([i.joint_positions_gene for i in individuals])\n    all_fitness = N.array([i.raw_fitness for i in individuals])\n    bad_individual = N.zeros(len(individuals), N.bool)\n    for i,individual in enumerate(individuals):\n        duplicates = N.flatnonzero((N.abs(individual.joint_positions_gene - all_jp_genes).sum(axis=1) < self.shape_variety_distance)) # indices to duplicates\n        # select worst (highest fitness) individuals from those duplicaates\n        if len(duplicates) > self.num_similar_individuals:\n            worst_duplicates = duplicates[all_fitness[duplicates].argsort()[self.num_similar_individuals:]]\n            bad_individual[worst_duplicates] = True\n    for individual_index in N.flatnonzero(bad_individual):\n        individual = individuals[individual_index]\n        individual.raw_fitness = individual.raw_fitness + 1e+6\n        individual.got_penalized = True\n        #print \"bad: \", individual.element_material_gene", "path": "ga_truss.py", "repo_name": "tneumann/skeletal_structures_ga", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "#print \"getit\"\n", "func_signal": "def _get_gene_index_for_element_material(self):\n", "code": "d = {}\nfor i,mat in enumerate(self.construction.available_element_materials):\n    d[mat] = i\nd[self.construction.element_deleted_material] = -1\nreturn d", "path": "ga_truss.py", "repo_name": "tneumann/skeletal_structures_ga", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "''' complete genetic algorithm cycle including selection and reproduction '''\n# this is the core of the genetic algorithm\n# this algorithm tries to minimize the fitness function\n", "func_signal": "def evolution_step(self):\n", "code": "cur_pop = self.current_population\n# make a new empty population\nnew_pop = Population(number=cur_pop.number+1)\n# make love until new population is full, always get 2 children\nwhile len(new_pop.individuals) < self.population_size:\n    # select 2 individuals for reproduction\n    mummy = self.selection_operator(self.world, cur_pop)\n    daddy = self.selection_operator(self.world, cur_pop)\n    # make 2 children, which are clones of mummy and daddy first, change genes through crossover/mutation later\n    sister = mummy.clone()\n    brother = daddy.clone()\n    # apply crossover operator\n    self.crossover_operator(self.crossover_rate, self.world, mummy, daddy, sister, brother)\n    # apply mutation operator\n    self.mutation_operator(self.mutation_rate, self.world, sister)\n    self.mutation_operator(self.mutation_rate, self.world, brother)\n    # call birth callback for individuals if available\n    self._call_birth_callback(sister)\n    self._call_birth_callback(brother)\n    # put children into new population if they are valid\n    if self.validator is None or self.validator(self.world, sister):\n        new_pop.individuals.append(sister)\n    if self.validator is None or self.validator(self.world, brother):\n        new_pop.individuals.append(brother)\n# if population size is odd then we may have made one child too much, so just randomly kill one\nif len(new_pop.individuals) > len(cur_pop.individuals):\n    del new_pop.individuals[rand_list_index(len(cur_pop.individuals))]\n# calculate base fitness for each new child\nself._calc_base_fitness(new_pop)\n# now put back the parents into the new population when elitism is enabled\nif self.elitism:\n    new_pop.individuals += cur_pop.individuals\n    for individual in cur_pop.individuals:\n        individual.age += 1\nself.survival(self.world, new_pop.individuals)\n# sort individuals by fitness\nnew_pop.individuals.sort(key=lambda i: i.raw_fitness)\n# kill individuals so that population size stays the same\nnew_pop.individuals = new_pop.individuals[:self.population_size]\n# done with this generation :-)\nself.populations.append(new_pop)\nif self.num_keep_generations != -1 and len(self.populations) > self.num_keep_generations:\n    self.populations = self.populations[-self.num_keep_generations:]\nself.num_steps += 1\n#if self.collect_history_stats:\n#    self.best_raw_fitness_history.append(float(self.current_population.best.raw_fitness))\n#    self.average_raw_fitness_history.append(float(self.current_population.mean_fitness))\n#    self.worst_raw_fitness_history.append(float(self.current_population.worst.raw_fitness))\nself.on_step = new_pop", "path": "ga.py", "repo_name": "tneumann/skeletal_structures_ga", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "# no object selected, show general ui\n", "func_signal": "def _edit_selected_object_properties(self):\n", "code": "if self.selected_object == None:\n    self.property_view_contents = \\\n            self.edit_traits(kind='subpanel', parent=self.properties_panel, \n                    view=ui.general_edit_view([self.construction.element_deleted_material] + self.construction.available_element_materials))\n# joint selected\nelif isinstance(self.selected_object, model.Joint):\n    w, h = self.construction.width, self.construction.height\n    self.property_view_contents = \\\n            self.edit_traits(view=ui.joint_edit_view(w, h), kind='subpanel', parent=self.properties_panel,\n                context={'joint': self.selected_object, 'object': self})\n# element selected\nelif isinstance(self.selected_object, model.Element):\n    self.property_view_contents = \\\n            self.edit_traits( view = ui.element_edit_view([self.construction.element_deleted_material] + self.construction.available_element_materials), \n                kind = 'subpanel', parent = self.properties_panel,\n                context = {'element': self.selected_object, 'object': self} )", "path": "editor.py", "repo_name": "tneumann/skeletal_structures_ga", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "''' do a 2 point crossover on 2 1-dimensional numpy arrays, return the 2 children arrays '''\n# select crossover points\n", "func_signal": "def numpy_array_2point_crossover(mummy, daddy):\n", "code": "index1_ = rand_list_index(len(mummy))\nindex2_ = rand_list_index(len(mummy))\nindex1 = min(index1_, index2_)\nindex2 = max(index1_, index2_)\n# do crossover\nsister  = N.concatenate((mummy[:index1], daddy[index1:index2], mummy[index2:]))\nbrother = N.concatenate((daddy[:index1], mummy[index1:index2], daddy[index2:]))\nreturn sister, brother", "path": "ga.py", "repo_name": "tneumann/skeletal_structures_ga", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "# apply mutation to joint positions gene\n", "func_signal": "def __call__(self, mutation_rate, world, individual):\n", "code": "individual.joint_positions_gene = self.joint_position_mutation(mutation_rate, world, individual.joint_positions_gene)\n# apply mutation to element material gene\nindividual.element_material_gene = self.element_material_mutation(mutation_rate, world, individual.element_material_gene)", "path": "ga_truss.py", "repo_name": "tneumann/skeletal_structures_ga", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "#for i in xrange(5):\n", "func_signal": "def _evolve_it(self):\n", "code": "self.ga.evolution_step()\nself.gui.process_events()\nif self.interaction_mode == 'evolve':\n    self.gui.invoke_later(self._evolve_it)", "path": "editor.py", "repo_name": "tneumann/skeletal_structures_ga", "stars": 1, "license": "None", "language": "python", "size": 120}
{"docstring": "\"\"\"\n@return: the list M{Tr}, where M{Tr[r]} is the total count in\n    C{heldout_fdist} for all samples that occur M{r}\n    times in C{base_fdist}.\n@rtype: C{list} of C{float}\n\"\"\"\n", "func_signal": "def _calculate_Tr(self):\n", "code": "Tr = [0.0] * (self._max_r+1)\nfor sample in self._heldout_fdist:\n    r = self._base_fdist[sample]\n    Tr[r] += self._heldout_fdist[sample]\nreturn Tr", "path": "src\\src\\probability.py", "repo_name": "kjmikkel/AuthorAttribution", "stars": 1, "license": "None", "language": "python", "size": 1220}
{"docstring": "\"\"\"\nGiven two numbers C{logx}=M{log(x)} and C{logy}=M{log(y)}, return\nM{log(x+y)}.  Conceptually, this is the same as returning\nM{log(2**(C{logx})+2**(C{logy}))}, but the actual implementation\navoids overflow errors that could result from direct computation.\n\"\"\"\n", "func_signal": "def add_logs(logx, logy):\n", "code": "if (logx < logy + _ADD_LOGS_MAX_DIFF):\n    return logy\nif (logy < logx + _ADD_LOGS_MAX_DIFF):\n    return logx\nbase = min(logx, logy)\nreturn base + math.log(2**(logx-base) + 2**(logy-base), 2)", "path": "src\\src\\probability.py", "repo_name": "kjmikkel/AuthorAttribution", "stars": 1, "license": "None", "language": "python", "size": 1220}
{"docstring": "\"\"\"\nIncrement this C{FreqDist}'s count for the given\nsample.\n\n@param sample: The sample whose count should be incremented.\n@type sample: any\n@param count: The amount to increment the sample's count by.\n@type count: C{int}\n@rtype: None\n@raise NotImplementedError: If C{sample} is not a\n       supported sample type.\n\"\"\"\n", "func_signal": "def inc(self, sample, count=1):\n", "code": "if count == 0: return\n\nself._N += count\nself[sample] = self.get(sample,0) + count\n\n# Invalidate the Nr cache and max cache.\nself._Nr_cache = None\nself._max_cache = None\nself._item_cache = None", "path": "src\\src\\probability.py", "repo_name": "kjmikkel/AuthorAttribution", "stars": 1, "license": "None", "language": "python", "size": 1220}
{"docstring": "\"\"\"\n@return: the list M{estimate}, where M{estimate[r]} is the\n    probability estimate for any sample that occurs M{r} times\n    in the base frequency distribution.  In particular,\n    M{estimate[r]} is M{Tr[r]/(N[r]*N)}.  In the special case\n    that M{N[r]=0}, M{estimate[r]} will never be used; so we\n    define M{estimate[r]=None} for those cases.\n@rtype: C{list} of C{float}\n@type Tr: C{list} of C{float}\n@param Tr: the list M{Tr}, where M{Tr[r]} is the total count in\n    the heldout distribution for all samples that occur M{r}\n    times in base distribution.\n@type Nr: C{list} of C{float}\n@param Nr: The list M{Nr}, where M{Nr[r]} is the number of\n    samples that occur M{r} times in the base distribution.\n@type N: C{int}\n@param N: The total number of outcomes recorded by the heldout\n    frequency distribution. \n\"\"\"\n", "func_signal": "def _calculate_estimate(self, Tr, Nr, N):\n", "code": "estimate = []\nfor r in range(self._max_r+1):\n    if Nr[r] == 0: estimate.append(None)\n    else: estimate.append(Tr[r]/(Nr[r]*N))\nreturn estimate", "path": "src\\src\\probability.py", "repo_name": "kjmikkel/AuthorAttribution", "stars": 1, "license": "None", "language": "python", "size": 1220}
{"docstring": "\"\"\"\nInitialize this object's probability.  This initializer should\nbe called by subclass constructors.  C{prob} should generally be\nthe first argument for those constructors.\n\n@kwparam prob: The probability associated with the object.\n@type prob: C{float}\n@kwparam logprob: The log of the probability associated with\n    the object.\n@type logprob: C{float}\n\"\"\"\n", "func_signal": "def __init__(self, **kwargs):\n", "code": "if 'prob' in kwargs:\n    if 'logprob' in kwargs:\n        raise TypeError('Must specify either prob or logprob '\n                        '(not both)')\n    else:\n        ProbabilisticMixIn.set_prob(self, kwargs['prob'])\nelif 'logprob' in kwargs:\n    ProbabilisticMixIn.set_logprob(self, kwargs['logprob'])\nelse:\n    self.__prob = self.__logprob = None", "path": "src\\src\\probability.py", "repo_name": "kjmikkel/AuthorAttribution", "stars": 1, "license": "None", "language": "python", "size": 1220}
{"docstring": "\"\"\"\nConstruct a new empty conditional frequency distribution.  In\nparticular, the count for every sample, under every condition,\nis zero.\n\n@param cond_samples: The samples to initialize the conditional frequency distribution with\n@type cond_samples: Sequence of (condition, sample) tuples\n\"\"\"\n", "func_signal": "def __init__(self, cond_samples=None):\n", "code": "self._fdists = {}\nif cond_samples:\n    for (cond, sample) in cond_samples:\n        self[cond].inc(sample)", "path": "src\\src\\probability.py", "repo_name": "kjmikkel/AuthorAttribution", "stars": 1, "license": "None", "language": "python", "size": 1220}
{"docstring": "# inherit docs from ProbDistI\n", "func_signal": "def prob(self, sample):\n", "code": "c = self._freqdist[sample]\nif c == 0:\n    return self._P0\nelse:\n    return c / float(self._N + self._T)", "path": "src\\src\\probability.py", "repo_name": "kjmikkel/AuthorAttribution", "stars": 1, "license": "None", "language": "python", "size": 1220}
{"docstring": "\"\"\"\nUse the heldout estimate to create a probability distribution\nfor the experiment used to generate C{base_fdist} and\nC{heldout_fdist}.\n\n@type base_fdist: C{FreqDist}\n@param base_fdist: The base frequency distribution.\n@type heldout_fdist: C{FreqDist}\n@param heldout_fdist: The heldout frequency distribution.\n@type bins: C{int}\n@param bins: The number of sample values that can be generated\n    by the experiment that is described by the probability\n    distribution.  This value must be correctly set for the\n    probabilities of the sample values to sum to one.  If\n    C{bins} is not specified, it defaults to C{freqdist.B()}.\n\"\"\"\n\n", "func_signal": "def __init__(self, base_fdist, heldout_fdist, bins=None):\n", "code": "self._base_fdist = base_fdist\nself._heldout_fdist = heldout_fdist\n\n# The max number of times any sample occurs in base_fdist.\nself._max_r = base_fdist[base_fdist.max()]\n\n# Calculate Tr, Nr, and N.\nTr = self._calculate_Tr()\nNr = [base_fdist.Nr(r, bins) for r in range(self._max_r+1)]\nN = heldout_fdist.N()\n\n# Use Tr, Nr, and N to compute the probability estimate for\n# each value of r.\nself._estimate = self._calculate_estimate(Tr, Nr, N)", "path": "src\\src\\probability.py", "repo_name": "kjmikkel/AuthorAttribution", "stars": 1, "license": "None", "language": "python", "size": 1220}
{"docstring": "\"\"\"\nReturn the samples sorted in decreasing order of frequency.\n\n@return: A list of samples, in sorted order\n@rtype: C{list} of any\n\"\"\"\n", "func_signal": "def keys(self):\n", "code": "self._sort_keys_by_value()\nreturn map(itemgetter(0), self._item_cache)", "path": "src\\src\\probability.py", "repo_name": "kjmikkel/AuthorAttribution", "stars": 1, "license": "None", "language": "python", "size": 1220}
{"docstring": "# Use our precomputed probability estimate.\n", "func_signal": "def prob(self, sample):\n", "code": "r = self._base_fdist[sample]\nreturn self._estimate[r]", "path": "src\\src\\probability.py", "repo_name": "kjmikkel/AuthorAttribution", "stars": 1, "license": "None", "language": "python", "size": 1220}
{"docstring": "\"\"\"\nConstruct a new uniform probability distribution, that assigns\nequal probability to each sample in C{samples}.\n\n@param samples: The samples that should be given uniform\n    probability.\n@type samples: C{list}\n@raise ValueError: If C{samples} is empty.\n\"\"\"\n", "func_signal": "def __init__(self, samples):\n", "code": "if len(samples) == 0:\n    raise ValueError('A Uniform probability distribution must '+\n                     'have at least one sample.')\nself._sampleset = set(samples)\nself._prob = 1.0/len(self._sampleset)\nself._samples = list(self._sampleset)", "path": "src\\src\\probability.py", "repo_name": "kjmikkel/AuthorAttribution", "stars": 1, "license": "None", "language": "python", "size": 1220}
{"docstring": "\"\"\"\nReturn the items sorted in decreasing order of frequency.\n\n@return: A list of items, in sorted order\n@rtype: C{list} of C{tuple}\n\"\"\"\n", "func_signal": "def items(self):\n", "code": "self._sort_keys_by_value()\nreturn self._item_cache[:]", "path": "src\\src\\probability.py", "repo_name": "kjmikkel/AuthorAttribution", "stars": 1, "license": "None", "language": "python", "size": 1220}
{"docstring": "\"\"\"\nReturn the cumulative frequencies of the specified samples.\nIf no samples are specified, all counts are returned, starting\nwith the largest.\n\n@return: The cumulative frequencies of the given samples.\n@rtype: C{list} of C{float}\n@param samples: the samples whose frequencies should be returned.\n@type sample: any.\n\"\"\"\n", "func_signal": "def _cumulative_frequencies(self, samples=None):\n", "code": "cf = 0.0\nfor sample in samples:\n    cf += self[sample]\n    yield cf", "path": "src\\src\\probability.py", "repo_name": "kjmikkel/AuthorAttribution", "stars": 1, "license": "None", "language": "python", "size": 1220}
{"docstring": "\"\"\"\nReturn the items sorted in decreasing order of frequency.\n\n@return: An iterator over the items, in sorted order\n@rtype: C{iter} of any\n\"\"\"\n", "func_signal": "def iteritems(self):\n", "code": "self._sort_keys_by_value()\nreturn iter(self._item_cache)", "path": "src\\src\\probability.py", "repo_name": "kjmikkel/AuthorAttribution", "stars": 1, "license": "None", "language": "python", "size": 1220}
{"docstring": "\"\"\"\nUpdate the probability for the given sample. This may cause the object\nto stop being the valid probability distribution - the user must\nensure that they update the sample probabilities such that all samples\nhave probabilities between 0 and 1 and that all probabilities sum to\none.\n\n@param sample: the sample for which to update the probability\n@type sample: C{any}\n@param prob: the new probability\n@type prob: C{float}\n@param log: is the probability already logged\n@type log: C{bool}\n\"\"\"\n", "func_signal": "def update(self, sample, prob, log=True):\n", "code": "i = self._sample_dict.get(sample)\nassert i != None\nif self._logs:\n    if log: self._data[i] = prob\n    else:   self._data[i] = math.log(prob, 2)\nelse:\n    if log: self._data[i] = 2**(prob)\n    else:   self._data[i] = prob", "path": "src\\src\\probability.py", "repo_name": "kjmikkel/AuthorAttribution", "stars": 1, "license": "None", "language": "python", "size": 1220}
{"docstring": "\"\"\"\nSet the probability associated with this object to C{prob}.\n@param prob: The new probability\n@type prob: C{float}\n\"\"\"\n", "func_signal": "def set_prob(self, prob):\n", "code": "self.__prob = prob\nself.__logprob = None", "path": "src\\src\\probability.py", "repo_name": "kjmikkel/AuthorAttribution", "stars": 1, "license": "None", "language": "python", "size": 1220}
{"docstring": "\"\"\"\nConstruct a new frequency distribution.  If C{samples} is\ngiven, then the frequency distribution will be initialized\nwith the count of each object in C{samples}; otherwise, it\nwill be initialized to be empty.\n\nIn particular, C{FreqDist()} returns an empty frequency\ndistribution; and C{FreqDist(samples)} first creates an empty\nfrequency distribution, and then calls C{inc} for each element\nin the list C{samples}.\n\n@param samples: The samples to initialize the frequency\ndistribution with.\n@type samples: Sequence\n\"\"\"\n", "func_signal": "def __init__(self, samples=None):\n", "code": "dict.__init__(self)\nself._N = 0\nself._Nr_cache = None\nself._max_cache = None\nself._item_cache = None\nif samples:\n    for sample in samples:\n        self.inc(sample)", "path": "src\\src\\probability.py", "repo_name": "kjmikkel/AuthorAttribution", "stars": 1, "license": "None", "language": "python", "size": 1220}
{"docstring": "\"\"\"\n@rtype: C{string}\n@return: A string representation of this C{ProbDist}.\n\"\"\"\n", "func_signal": "def __repr__(self):\n", "code": "s = '<HeldoutProbDist: %d base samples; %d heldout samples>'\nreturn s % (self._base_fdist.N(), self._heldout_fdist.N())", "path": "src\\src\\probability.py", "repo_name": "kjmikkel/AuthorAttribution", "stars": 1, "license": "None", "language": "python", "size": 1220}
{"docstring": "\"\"\"\nSet the log probability associated with this object to\nC{logprob}.  I.e., set the probability associated with this\nobject to C{2**(logprob)}.\n@param logprob: The new log probability\n@type logprob: C{float}\n\"\"\"\n", "func_signal": "def set_logprob(self, logprob):\n", "code": "self.__logprob = prob\nself.__prob = None", "path": "src\\src\\probability.py", "repo_name": "kjmikkel/AuthorAttribution", "stars": 1, "license": "None", "language": "python", "size": 1220}
{"docstring": "\"\"\"\nReturn the samples sorted in decreasing order of frequency.\n\n@return: A list of samples, in sorted order\n@rtype: C{list} of any\n\"\"\"\n", "func_signal": "def values(self):\n", "code": "self._sort_keys_by_value()\nreturn map(itemgetter(1), self._item_cache)", "path": "src\\src\\probability.py", "repo_name": "kjmikkel/AuthorAttribution", "stars": 1, "license": "None", "language": "python", "size": 1220}
{"docstring": "\"\"\"Clears all of the annotations within a given range with a given key.\n\nArgs:\n  r: A Range specifying the range to delete.\n  name: Annotation key type to clear.\n\"\"\"\n", "func_signal": "def DeleteAnnotationsInRange(self, r, name):\n", "code": "self.__context.builder.DocumentAnnotationDelete(self._blip.waveId,\n                                                self._blip.waveletId,\n                                                self._blip.blipId,\n                                                r.start, r.end,\n                                                name)\nres = []\nfor a in self._blip.annotations:\n  if a.name != name or r.start > a.range.end or r.end < a.range.start:\n    res.append(a)\n  elif r.start < a.range.start and r.end > a.range.end:\n    continue\n  else:\n    if a.range.start < r.start:\n      res.append(document.Annotation(\n          name, a.value, document.Range(a.range.start, r.start)))\n    if a.range.end > r.end:\n      a.range.start = r.end\n      res.append(a)\nself._blip.annotations = res", "path": "waveapi\\ops.py", "repo_name": "IanMulvany/janey-robot", "stars": 1, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"Requests to delete content in a given range.\n\nArgs:\n  wave_id: The wave id owning that this operation is applied to.\n  wavelet_id: The wavelet id that this operation is applied to.\n  blip_id: The blip id that this operation is applied to.\n  start: Start of the range.\n  end: End of the range.\n\"\"\"\n", "func_signal": "def DocumentDelete(self, wave_id, wavelet_id, blip_id, start, end):\n", "code": "range = None\nif start != end:\n  range = document.Range(start, end)\nself.AddNewOperation(DOCUMENT_DELETE, wave_id, wavelet_id, blip_id,\n                     prop=range)", "path": "waveapi\\ops.py", "repo_name": "IanMulvany/janey-robot", "stars": 1, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"Deletes all annotations with a given key name.\n\nArgs:\n  name: A string as the key for the annotation to delete.\n\"\"\"\n", "func_signal": "def DeleteAnnotationsByName(self, name):\n", "code": "size = len(self._blip.content)\nself.__context.builder.DocumentAnnotationDelete(self._blip.waveId,\n                                                self._blip.waveletId,\n                                                self._blip.blipId,\n                                                0, size, name)\nself._blip.annotations = [a\n    for a in self._blip.annotations if a.name != name]", "path": "waveapi\\ops.py", "repo_name": "IanMulvany/janey-robot", "stars": 1, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"Initializes this document with its owning blip and session context.\"\"\"\n", "func_signal": "def __init__(self, blip, context):\n", "code": "super(OpBasedDocument, self).__init__(blip)\nself.__context = context", "path": "waveapi\\ops.py", "repo_name": "IanMulvany/janey-robot", "stars": 1, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"Creates and appends a blip to this wavelet and returns it.\n\nReturns:\n  A transient version of the blip that was created.\n\"\"\"\n", "func_signal": "def CreateBlip(self):\n", "code": "blip_data = self.__context.builder.WaveletAppendBlip(self.GetWaveId(),\n                                                     self.GetId())\nreturn self.__context.AddBlip(blip_data)", "path": "waveapi\\ops.py", "repo_name": "IanMulvany/janey-robot", "stars": 1, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"Adds a transient wave based on the data supplied.\n\nArgs:\n  wave_data: JSON data describing this wave.\n\nReturns:\n  An OpBasedWave that may have operations applied to it.\n\"\"\"\n", "func_signal": "def AddWave(self, wave_data):\n", "code": "wave = OpBasedWave(wave_data, self)\nself.waves[wave.GetId()] = wave\nreturn wave", "path": "waveapi\\ops.py", "repo_name": "IanMulvany/janey-robot", "stars": 1, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"Annotates the entire document.\n\nArgs:\n  name: A string as the key for this annotation.\n  value: The value of this annotation.\n\"\"\"\n", "func_signal": "def AnnotateDocument(self, name, value):\n", "code": "b = self.__context.builder\nb.DocumentAnnotationSetNoRange(self._blip.waveId,\n                               self._blip.waveletId,\n                               self._blip.blipId,\n                               name, value)\nr = document.Range(0, len(self._blip.content))\nself._blip.annotations.append(document.Annotation(name, value, r))", "path": "waveapi\\ops.py", "repo_name": "IanMulvany/janey-robot", "stars": 1, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"Requests to append a blip to a wavelet.\n\nArgs:\n  wave_id: The wave id owning the containing wavelet.\n  wavelet_id: The wavelet id that this blip should be appended to.\n\nReturns:\n  JSON representing the id information of the new blip.\n\"\"\"\n", "func_signal": "def WaveletAppendBlip(self, wave_id, wavelet_id):\n", "code": "blip_data = self.__CreateNewBlipData(wave_id, wavelet_id)\nself.AddNewOperation(WAVELET_APPEND_BLIP, wave_id, wavelet_id,\n                     prop=blip_data)\nreturn blip_data", "path": "waveapi\\ops.py", "repo_name": "IanMulvany/janey-robot", "stars": 1, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"Adds a participant to a wavelet.\n\nArgs:\n  participant_id: Id of the participant that is to be added.\n\"\"\"\n", "func_signal": "def AddParticipant(self, participant_id):\n", "code": "self.__context.builder.WaveletAddParticipant(self.GetWaveId(), self.GetId(),\n                                             participant_id)\nself.participants.add(participant_id)", "path": "waveapi\\ops.py", "repo_name": "IanMulvany/janey-robot", "stars": 1, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"Iterate through the ranges defined for name.\n\nArgs:\n  name: The name of the annotation.\n\nReturns:\n  the matching ranges.\n\"\"\"\n", "func_signal": "def RangesForAnnotation(self, name):\n", "code": "for annotation in self._blip.annotations:\n  if annotation.name == name:\n    yield annotation.range", "path": "waveapi\\ops.py", "repo_name": "IanMulvany/janey-robot", "stars": 1, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"Appends text to the end of this document.\n\nArgs:\n  text: The text to append.\n\"\"\"\n", "func_signal": "def AppendText(self, text):\n", "code": "self.__context.builder.DocumentAppend(self._blip.waveId,\n                                      self._blip.waveletId,\n                                      self._blip.blipId,\n                                      text)\nself._blip.content += text", "path": "waveapi\\ops.py", "repo_name": "IanMulvany/janey-robot", "stars": 1, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"Requests to insert an inline blip at a specific location.\n\nArgs:\n  wave_id: The wave id owning that this operation is applied to.\n  wavelet_id: The wavelet id that this operation is applied to.\n  blip_id: The blip id that this operation is applied to.\n  position: The position in the document to insert the blip.\n\nReturns:\n  JSON data for the blip that was created for further operations.\n\"\"\"\n", "func_signal": "def DocumentInlineBlipInsert(self, wave_id, wavelet_id, blip_id, position):\n", "code": "inline_blip_data = self.__CreateNewBlipData(wave_id, wavelet_id)\ninline_blip_data['parentBlipId'] = blip_id\nself.AddNewOperation(DOCUMENT_INLINE_BLIP_INSERT, wave_id, wavelet_id,\n                     blip_id=blip_id,\n                     index=position,\n                     prop=inline_blip_data)\nreturn inline_blip_data", "path": "waveapi\\ops.py", "repo_name": "IanMulvany/janey-robot", "stars": 1, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"Clears the content of this document.\"\"\"\n", "func_signal": "def Clear(self):\n", "code": "self.__context.builder.DocumentDelete(self._blip.waveId,\n                                      self._blip.waveletId,\n                                      self._blip.blipId,\n                                      0, len(self._blip.content))\nself._blip.content = ''", "path": "waveapi\\ops.py", "repo_name": "IanMulvany/janey-robot", "stars": 1, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"Sets a key/value pair on the wavelet data document.\n\nArgs:\n  name: The string key.\n  data: The value associated with this key.\n\"\"\"\n", "func_signal": "def SetDataDocument(self, name, data):\n", "code": "self.__context.builder.WaveletSetDataDoc(self.GetWaveId(), self.GetId(),\n                                         name, data)\nself.dataDocuments[name] = data", "path": "waveapi\\ops.py", "repo_name": "IanMulvany/janey-robot", "stars": 1, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"Appends an inline blip to this blip.\n\nReturns:\n  The local blip that was appended.\n\"\"\"\n", "func_signal": "def AppendInlineBlip(self):\n", "code": "blip_data = self.__context.builder.DocumentInlineBlipAppend(\n    self._blip.waveId, self._blip.waveletId,\n    self._blip.blipId)\nreturn self.__context.AddBlip(blip_data)", "path": "waveapi\\ops.py", "repo_name": "IanMulvany/janey-robot", "stars": 1, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"Requests to create a wavelet in a wave.\n\nNot yet implemented.\n\nArgs:\n  participants: initial participants on this wavelet or None if none\n\n\"\"\"\n", "func_signal": "def WaveletCreate(self, wave_id, wavelet_id, participants=None):\n", "code": "if participants is None:\n  participants = []\nwavelet_data = self.__CreateNewWaveletData(participants)\nblip_data = self.__CreateNewBlipData(\n    wavelet_data.waveId, wavelet_data.waveletId)\nself.__context.AddBlip(blip_data)\nwavelet_data.SetRootBlipId(blip_data.blipId)\nlogging.info('rootblip=' + blip_data.blipId)\nwavelet = self.__context.AddWavelet(wavelet_data)\nop = Operation(WAVELET_CREATE, wave_id, wavelet_id, prop=wavelet_data)\nself.__context.AddOperation(op)\nreturn wavelet", "path": "waveapi\\ops.py", "repo_name": "IanMulvany/janey-robot", "stars": 1, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"Initializes this blip with the session context.\"\"\"\n", "func_signal": "def __init__(self, json, context):\n", "code": "super(OpBasedBlip, self).__init__(json)\nself.__context = context\nself.document = OpBasedDocument(self, context)", "path": "waveapi\\ops.py", "repo_name": "IanMulvany/janey-robot", "stars": 1, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"Removes a wave locally.\"\"\"\n", "func_signal": "def RemoveWave(self, wave_id):\n", "code": "if wave_id in self.waves:\n  del self.waves[wave_id]", "path": "waveapi\\ops.py", "repo_name": "IanMulvany/janey-robot", "stars": 1, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"Replaces an Element at a given position with a new element.\n\nArgs:\n  position: Position of the element to replace.\n  element: The Element to replace with.\n\"\"\"\n", "func_signal": "def ReplaceElement(self, position, element):\n", "code": "self.__context.builder.DocumentElementReplace(self._blip.waveId,\n                                              self._blip.waveletId,\n                                              self._blip.blipId,\n                                              position, element)", "path": "waveapi\\ops.py", "repo_name": "IanMulvany/janey-robot", "stars": 1, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"Inserts an Element at a given position.\n\nArgs:\n  position: Position of the element to replace.\n  element: The Element to replace with.\n\"\"\"\n", "func_signal": "def InsertElement(self, position, element):\n", "code": "self.__context.builder.DocumentElementInsert(self._blip.waveId,\n                                             self._blip.waveletId,\n                                             self._blip.blipId,\n                                             position, element)", "path": "waveapi\\ops.py", "repo_name": "IanMulvany/janey-robot", "stars": 1, "license": "None", "language": "python", "size": 576}
{"docstring": "\"\"\"Removes a joint from the space\"\"\"\n", "func_signal": "def _remove_joint(self, joint):\n", "code": "self._joints.remove(joint)\ncp.cpSpaceRemoveJoint(self._space, joint._joint)", "path": "pymunk\\__init__.py", "repo_name": "irskep/Gluball", "stars": 1, "license": "None", "language": "python", "size": 32032}
{"docstring": "\"\"\"Adds a body to the space\"\"\"\n", "func_signal": "def _add_body(self, body):\n", "code": "self._bodies.add(body)\ncp.cpSpaceAddBody(self._space, body._body)", "path": "pymunk\\__init__.py", "repo_name": "irskep/Gluball", "stars": 1, "license": "None", "language": "python", "size": 32032}
{"docstring": "\"\"\"Adds a shape to the space. Static shapes should be be attached to \na rigid body with an infinite mass and moment of inertia. Also, don't \nadd the rigid body used to the space, as that will cause it to fall \nunder the effects of gravity.\"\"\"\n", "func_signal": "def _add_static_shape(self, static_shape):\n", "code": "self._static_shapes[static_shape.id] = static_shape\ncp.cpSpaceAddStaticShape(self._space, static_shape._shape)", "path": "pymunk\\__init__.py", "repo_name": "irskep/Gluball", "stars": 1, "license": "None", "language": "python", "size": 32032}
{"docstring": "\"\"\"Convert body local to world coordinates\"\"\"\n#TODO: Test me\n", "func_signal": "def local_to_world(self, v):\n", "code": "v = Vec2d(v)\nreturn self.position + v.cpvrotate(self.rotation_vector)", "path": "pymunk\\__init__.py", "repo_name": "irskep/Gluball", "stars": 1, "license": "None", "language": "python", "size": 32032}
{"docstring": "\"\"\"Adds a joint to the space\"\"\"\n", "func_signal": "def _add_joint(self, joint):\n", "code": "self._joints.add(joint)\ncp.cpSpaceAddJoint(self._space, joint._joint)", "path": "pymunk\\__init__.py", "repo_name": "irskep/Gluball", "stars": 1, "license": "None", "language": "python", "size": 32032}
{"docstring": "\"\"\"body is the body attach the circle to, offset is the offset from the\nbody's center of gravity in body local coordinates.\"\"\"\n", "func_signal": "def __init__(self, body, radius, offset):\n", "code": "self._body = body\nself._shape = cp.cpCircleShapeNew(body._body, radius, offset)\nself._shapecontents = self._shape.contents", "path": "pymunk\\__init__.py", "repo_name": "irskep/Gluball", "stars": 1, "license": "None", "language": "python", "size": 32032}
{"docstring": "\"\"\"Remove one or many static shapes from the space\"\"\"\n", "func_signal": "def remove_static(self, *os):\n", "code": "for o in os:\n    if isinstance(o, Shape):\n        self._remove_static_shape(o)\n    else:\n        for oo in o:\n            self.remove_static(oo)", "path": "pymunk\\__init__.py", "repo_name": "irskep/Gluball", "stars": 1, "license": "None", "language": "python", "size": 32032}
{"docstring": "\"\"\"a and b are the two bodies to connect, anchr1 and anchr2 are the\nanchor points on those bodies, and min and max define the allowed\ndistances of the anchor points.\n\"\"\"\n", "func_signal": "def __init__(self, a, b, anchr1, anchr2, min, max):\n", "code": "self._a = a\nself._b = b\nself._joint = cp.cpSlideJointNew(a._body, b._body, anchr1, anchr2, min, max)", "path": "pymunk\\__init__.py", "repo_name": "irskep/Gluball", "stars": 1, "license": "None", "language": "python", "size": 32032}
{"docstring": "\"\"\"Adds a shape to the space\"\"\"\n", "func_signal": "def _add_shape(self, shape):\n", "code": "self._shapes[shape.id] = shape\ncp.cpSpaceAddShape(self._space, shape._shape)", "path": "pymunk\\__init__.py", "repo_name": "irskep/Gluball", "stars": 1, "license": "None", "language": "python", "size": 32032}
{"docstring": "\"\"\"a and b are the two bodies to conenct, \ngroove_a and groove_b is two points or vectors or something.\nanchr2 is an anchor point\n\"\"\"\n", "func_signal": "def __init__(self, a, b, groove_a, groove_b, anchr2):\n", "code": "self._a = a \nself._b = b\nself._joint = cp.cpGrooveJointNew(a._body, b._body, groove_a, groove_b, anchr2)", "path": "pymunk\\__init__.py", "repo_name": "irskep/Gluball", "stars": 1, "license": "None", "language": "python", "size": 32032}
{"docstring": "\"\"\"Removes a static shape from the space.\"\"\"\n", "func_signal": "def _remove_static_shape(self, static_shape):\n", "code": "del self._static_shapes[static_shape.id]\ncp.cpSpaceRemoveStaticShape(self._space, static_shape._shape)", "path": "pymunk\\__init__.py", "repo_name": "irskep/Gluball", "stars": 1, "license": "None", "language": "python", "size": 32032}
{"docstring": "\"\"\"a and b are the two bodies to connect, and pivot is the point in\nworld coordinates of the pivot. Because the pivot location is given in\nworld coordinates, you must have the bodies moved into the correct\npositions already.\n\"\"\"\n", "func_signal": "def __init__(self, a, b, pivot):\n", "code": "self._a = a\nself._b = b\nself._joint = cp.cpPivotJointNew(a._body, b._body, pivot)", "path": "pymunk\\__init__.py", "repo_name": "irskep/Gluball", "stars": 1, "license": "None", "language": "python", "size": 32032}
{"docstring": "\"\"\"Calculate the moment of inertia for a polygon\"\"\"\n", "func_signal": "def moment_for_poly(mass, vertices,  offset):\n", "code": "verts = (Vec2d * len(vertices))\nverts = verts(Vec2d(0, 0))\nfor (i, vertex) in enumerate(vertices):\n    verts[i].x = vertex[0]\n    verts[i].y = vertex[1]\nreturn cp.cpMomentForPoly(mass, len(verts), verts, offset)", "path": "pymunk\\__init__.py", "repo_name": "irskep/Gluball", "stars": 1, "license": "None", "language": "python", "size": 32032}
{"docstring": "### Translate chipmunk shapes to Shapes.\n", "func_signal": "def cf (cpShapeA, cpShapeB, cpContacts, numContacts, normal_coef, _data):\n", "code": "if cpShapeA.contents.id in self._shapes:\n    shapeA = self._shapes[cpShapeA.contents.id]\nelse:\n    shapeA = self._static_shapes[cpShapeA.contents.id]\nif cpShapeB.contents.id in self._shapes:\n    shapeB = self._shapes[cpShapeB.contents.id]\nelse:\n    shapeB = self._static_shapes[cpShapeB.contents.id]\nreturn func(shapeA, shapeB, [Contact(cpContacts[i]) for i in xrange(numContacts)], normal_coef, data)", "path": "pymunk\\__init__.py", "repo_name": "irskep/Gluball", "stars": 1, "license": "None", "language": "python", "size": 32032}
{"docstring": "\"\"\"Sets the default collsion pair function. Passing None as func will \nreset it to default. See ``add_collisionpair_func`` for a details on func\n\"\"\"\n", "func_signal": "def set_default_collisionpair_func(self, func, data=None):\n", "code": "if func is None:\n    self._default_callback = None\n    cp.cpSpaceSetDefaultCollisionPairFunc(self._space, ct.cast(ct.POINTER(ct.c_int)(), cp.cpCollFunc), None)\nelse:\n    f = self._get_cf(func, data)\n    self._default_callback = f\n    cp.cpSpaceSetDefaultCollisionPairFunc(self._space, f, None)", "path": "pymunk\\__init__.py", "repo_name": "irskep/Gluball", "stars": 1, "license": "None", "language": "python", "size": 32032}
{"docstring": "\"\"\"Convert world to body local coordinates\"\"\"\n#TODO: Test me\n", "func_signal": "def world_to_local(self, v):\n", "code": "v = Vec2d(v)\nreturn (v - self.position).cpvunrotate(self.rotation_vector)", "path": "pymunk\\__init__.py", "repo_name": "irskep/Gluball", "stars": 1, "license": "None", "language": "python", "size": 32032}
{"docstring": "#shape = ct.cast(self._shape, ct.POINTER(cp.cpPolyShape))\n#num = shape.contents.numVerts\n#verts = shape.contents.verts\n#TODO: Fix for polygons with offset!!!\n", "func_signal": "def get_points(self):\n", "code": "points = []\nrv = self._body.rotation_vector\nbp = self._body.position\nvs = self.verts\no = self.offset\nfor i in xrange(len(vs)):\n    p = vs[i].cpvrotate(rv)+bp+o\n    points.append(Vec2d(p))\n    \nreturn points", "path": "pymunk\\__init__.py", "repo_name": "irskep/Gluball", "stars": 1, "license": "None", "language": "python", "size": 32032}
{"docstring": "\"\"\"Removes a body from the space\"\"\"\n", "func_signal": "def _remove_body(self, body):\n", "code": "self._bodies.remove(body)\ncp.cpSpaceRemoveBody(self._space, body._body)", "path": "pymunk\\__init__.py", "repo_name": "irskep/Gluball", "stars": 1, "license": "None", "language": "python", "size": 32032}
{"docstring": "\"\"\"Create a polygon\n\n    body : `Body`\n        The body to attach the poly to\n    vertices : [(x,y)] or [`Vec2d`]\n        Define a convex hull of the polygon with a counterclockwise\n        winding.\n    offset : (x,y) or `Vec2d`\n        The offset from the body's center of gravity in body local \n        coordinates. \n    auto_order_vertices : bool \n        Set to True to automatically order the vertices. Currently \n        not supported.\n\"\"\"\n", "func_signal": "def __init__(self, body, vertices, offset, auto_order_vertices=False):\n", "code": "if auto_order_vertices: \n    raise Exception(NotImplemented)\nself._body = body\nself.offset = offset\n#self.verts = (Vec2d * len(vertices))(*vertices)\nself.verts = (Vec2d * len(vertices))\nself.verts = self.verts(Vec2d(0, 0))\nfor (i, vertex) in enumerate(vertices):\n    self.verts[i].x = vertex[0]\n    self.verts[i].y = vertex[1]\n    \nself._shape = cp.cpPolyShapeNew(body._body, len(vertices), self.verts, offset)\nself._shapecontents = self._shape.contents", "path": "pymunk\\__init__.py", "repo_name": "irskep/Gluball", "stars": 1, "license": "None", "language": "python", "size": 32032}
{"docstring": "\"\"\"Create a new instace of the Space\n\n:Parameters:\n    iterations : int\n        Number of iterations to use in the impulse solver to solve \n        contacts.\n\"\"\"\n", "func_signal": "def __init__(self, iterations=10):\n", "code": "self._space = cp.cpSpaceNew(iterations)\nself._callbacks = {} # To prevent the gc to collect the callbacks.\nself._default_callback = None\nself._shapes = {}\nself._static_shapes = {}\nself._bodies = set()\nself._joints = set()", "path": "pymunk\\__init__.py", "repo_name": "irskep/Gluball", "stars": 1, "license": "None", "language": "python", "size": 32032}
{"docstring": "\"\"\"Close the socket underlying this connection.\"\"\"\n", "func_signal": "def close(self):\n", "code": "self.rfile.close()\n\n# Python's socket module does NOT call close on the kernel socket\n# when you call socket.close(). We do so manually here because we\n# want this server to send a FIN TCP segment immediately. Note this\n# must be called *before* calling socket.close(), because the latter\n# drops its reference to the kernel socket.\nself.socket._sock.close()\n\nself.socket.close()", "path": "web\\wsgiserver\\__init__.py", "repo_name": "sirsean/Scorekeeper", "stars": 1, "license": "None", "language": "python", "size": 292}
{"docstring": "# Assuming all templates are html\n", "func_signal": "def __getattr__(self, name):\n", "code": "path = name + \".html\"\nt = self._lookup.get_template(path)\nreturn t.render", "path": "web\\contrib\\template.py", "repo_name": "sirsean/Scorekeeper", "stars": 1, "license": "None", "language": "python", "size": 292}
{"docstring": "\"\"\"Parse the next HTTP request start-line and message-headers.\"\"\"\n", "func_signal": "def parse_request(self):\n", "code": "self.rfile.maxlen = self.max_request_header_size\nself.rfile.bytes_read = 0\n\ntry:\n    self._parse_request()\nexcept MaxSizeExceeded:\n    self.simple_response(\"413 Request Entity Too Large\")\n    return", "path": "web\\wsgiserver\\__init__.py", "repo_name": "sirsean/Scorekeeper", "stars": 1, "license": "None", "language": "python", "size": 292}
{"docstring": "# Assuming all templates end with .html\n", "func_signal": "def __getattr__(self, name):\n", "code": "path = name + '.html'\nt = self._lookup.get_template(path)\nreturn t.render", "path": "web\\contrib\\template.py", "repo_name": "sirsean/Scorekeeper", "stars": 1, "license": "None", "language": "python", "size": 292}
{"docstring": "\"\"\"Return error numbers for all errors in errnames on this platform.\n\nThe 'errno' module contains different global constants depending on\nthe specific platform (OS). This function will return the list of\nnumeric values for a given list of potential names.\n\"\"\"\n", "func_signal": "def plat_specific_errors(*errnames):\n", "code": "errno_names = dir(errno)\nnums = [getattr(errno, k) for k in errnames if k in errno_names]\n# de-dupe the list\nreturn dict.fromkeys(nums).keys()", "path": "web\\wsgiserver\\__init__.py", "repo_name": "sirsean/Scorekeeper", "stars": 1, "license": "None", "language": "python", "size": 292}
{"docstring": "\"\"\"Create (or recreate) the actual socket object.\"\"\"\n", "func_signal": "def bind(self, family, type, proto=0):\n", "code": "self.socket = socket.socket(family, type, proto)\nprevent_socket_inheritance(self.socket)\nself.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\nif self.nodelay:\n    self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\nif self.ssl_certificate and self.ssl_private_key:\n    if SSL is None:\n        raise ImportError(\"You must install pyOpenSSL to use HTTPS.\")\n    \n    # See http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/442473\n    ctx = SSL.Context(SSL.SSLv23_METHOD)\n    ctx.use_privatekey_file(self.ssl_private_key)\n    ctx.use_certificate_file(self.ssl_certificate)\n    self.socket = SSLConnection(ctx, self.socket)\n    self.populate_ssl_environ()\n    \n    # If listening on the IPV6 any address ('::' = IN6ADDR_ANY),\n    # activate dual-stack. See http://www.cherrypy.org/ticket/871.\n    if (not isinstance(self.bind_addr, basestring)\n        and self.bind_addr[0] == '::' and family == socket.AF_INET6):\n        try:\n            self.socket.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 0)\n        except (AttributeError, socket.error):\n            # Apparently, the socket option is not available in\n            # this machine's TCP stack\n            pass\n\nself.socket.bind(self.bind_addr)", "path": "web\\wsgiserver\\__init__.py", "repo_name": "sirsean/Scorekeeper", "stars": 1, "license": "None", "language": "python", "size": 292}
{"docstring": "# Assuming all templates are html\n", "func_signal": "def __getattr__(self, name):\n", "code": "path = name + \".html\"\n\nif self._type == \"text\":\n    from genshi.template import TextTemplate\n    cls = TextTemplate\n    type = \"text\"\nelse:\n    cls = None\n    type = None\n\nt = self._loader.load(path, cls=cls)\ndef template(**kw):\n    stream = t.generate(**kw)\n    if type:\n        return stream.render(type)\n    else:\n        return stream.render()\nreturn template", "path": "web\\contrib\\template.py", "repo_name": "sirsean/Scorekeeper", "stars": 1, "license": "None", "language": "python", "size": 292}
{"docstring": "\"\"\"Mark the given socket fd as non-inheritable (Windows).\"\"\"\n", "func_signal": "def prevent_socket_inheritance(sock):\n", "code": "if not windll.kernel32.SetHandleInformation(sock.fileno(), 1, 0):\n    raise WinError()", "path": "web\\wsgiserver\\__init__.py", "repo_name": "sirsean/Scorekeeper", "stars": 1, "license": "None", "language": "python", "size": 292}
{"docstring": "# Shamelessly stolen from StringIO\n", "func_signal": "def readlines(self, sizehint=0):\n", "code": "total = 0\nlines = []\nline = self.readline()\nwhile line:\n    lines.append(line)\n    total += len(line)\n    if 0 < sizehint <= total:\n        break\n    line = self.readline()\nreturn lines", "path": "web\\wsgiserver\\__init__.py", "repo_name": "sirsean/Scorekeeper", "stars": 1, "license": "None", "language": "python", "size": 292}
{"docstring": "\"\"\"Kill off worker threads (not below self.min).\"\"\"\n# Grow/shrink the pool if necessary.\n# Remove any dead threads from our list\n", "func_signal": "def shrink(self, amount):\n", "code": "for t in self._threads:\n    if not t.isAlive():\n        self._threads.remove(t)\n        amount -= 1\n\nif amount > 0:\n    for i in xrange(min(amount, len(self._threads) - self.min)):\n        # Put a number of shutdown requests on the queue equal\n        # to 'amount'. Once each of those is processed by a worker,\n        # that worker will terminate and be culled from our list\n        # in self.put.\n        self._queue.put(_SHUTDOWNREQUEST)", "path": "web\\wsgiserver\\__init__.py", "repo_name": "sirsean/Scorekeeper", "stars": 1, "license": "None", "language": "python", "size": 292}
{"docstring": "\"\"\"Start the pool of threads.\"\"\"\n", "func_signal": "def start(self):\n", "code": "for i in xrange(self.min):\n    self._threads.append(WorkerThread(self.server))\nfor worker in self._threads:\n    worker.setName(\"CP WSGIServer \" + worker.getName())\n    worker.start()\nfor worker in self._threads:\n    while not worker.ready:\n        time.sleep(.1)", "path": "web\\wsgiserver\\__init__.py", "repo_name": "sirsean/Scorekeeper", "stars": 1, "license": "None", "language": "python", "size": 292}
{"docstring": "\"\"\"Mark the given socket fd as non-inheritable (POSIX).\"\"\"\n", "func_signal": "def prevent_socket_inheritance(sock):\n", "code": "fd = sock.fileno()\nold_flags = fcntl.fcntl(fd, fcntl.F_GETFD)\nfcntl.fcntl(fd, fcntl.F_SETFD, old_flags | fcntl.FD_CLOEXEC)", "path": "web\\wsgiserver\\__init__.py", "repo_name": "sirsean/Scorekeeper", "stars": 1, "license": "None", "language": "python", "size": 292}
{"docstring": "\"\"\"Call the appropriate WSGI app and write its iterable output.\"\"\"\n# Set rfile.maxlen to ensure we don't read past Content-Length.\n# This will also be used to read the entire request body if errors\n# are raised before the app can read the body.\n", "func_signal": "def respond(self):\n", "code": "if self.chunked_read:\n    # If chunked, Content-Length will be 0.\n    self.rfile.maxlen = self.max_request_body_size\nelse:\n    cl = int(self.environ.get(\"CONTENT_LENGTH\", 0))\n    if self.max_request_body_size:\n        self.rfile.maxlen = min(cl, self.max_request_body_size)\n    else:\n        self.rfile.maxlen = cl\nself.rfile.bytes_read = 0\n\ntry:\n    self._respond()\nexcept MaxSizeExceeded:\n    if not self.sent_headers:\n        self.simple_response(\"413 Request Entity Too Large\")\n    return", "path": "web\\wsgiserver\\__init__.py", "repo_name": "sirsean/Scorekeeper", "stars": 1, "license": "None", "language": "python", "size": 292}
{"docstring": "\"\"\"Assert, process, and send the HTTP response message-headers.\"\"\"\n", "func_signal": "def send_headers(self):\n", "code": "hkeys = [key.lower() for key, value in self.outheaders]\nstatus = int(self.status[:3])\n\nif status == 413:\n    # Request Entity Too Large. Close conn to avoid garbage.\n    self.close_connection = True\nelif \"content-length\" not in hkeys:\n    # \"All 1xx (informational), 204 (no content),\n    # and 304 (not modified) responses MUST NOT\n    # include a message-body.\" So no point chunking.\n    if status < 200 or status in (204, 205, 304):\n        pass\n    else:\n        if (self.response_protocol == 'HTTP/1.1'\n            and self.environ[\"REQUEST_METHOD\"] != 'HEAD'):\n            # Use the chunked transfer-coding\n            self.chunked_write = True\n            self.outheaders.append((\"Transfer-Encoding\", \"chunked\"))\n        else:\n            # Closing the conn is the only way to determine len.\n            self.close_connection = True\n\nif \"connection\" not in hkeys:\n    if self.response_protocol == 'HTTP/1.1':\n        # Both server and client are HTTP/1.1 or better\n        if self.close_connection:\n            self.outheaders.append((\"Connection\", \"close\"))\n    else:\n        # Server and/or client are HTTP/1.0\n        if not self.close_connection:\n            self.outheaders.append((\"Connection\", \"Keep-Alive\"))\n\nif (not self.close_connection) and (not self.chunked_read):\n    # Read any remaining request body data on the socket.\n    # \"If an origin server receives a request that does not include an\n    # Expect request-header field with the \"100-continue\" expectation,\n    # the request includes a request body, and the server responds\n    # with a final status code before reading the entire request body\n    # from the transport connection, then the server SHOULD NOT close\n    # the transport connection until it has read the entire request,\n    # or until the client closes the connection. Otherwise, the client\n    # might not reliably receive the response message. However, this\n    # requirement is not be construed as preventing a server from\n    # defending itself against denial-of-service attacks, or from\n    # badly broken client implementations.\"\n    size = self.rfile.maxlen - self.rfile.bytes_read\n    if size > 0:\n        self.rfile.read(size)\n\nif \"date\" not in hkeys:\n    self.outheaders.append((\"Date\", rfc822.formatdate()))\n\nif \"server\" not in hkeys:\n    self.outheaders.append((\"Server\", self.environ['SERVER_SOFTWARE']))\n\nbuf = [self.environ['ACTUAL_SERVER_PROTOCOL'], \" \", self.status, \"\\r\\n\"]\ntry:\n    buf += [k + \": \" + v + \"\\r\\n\" for k, v in self.outheaders]\nexcept TypeError:\n    if not isinstance(k, str):\n        raise TypeError(\"WSGI response header key %r is not a string.\")\n    if not isinstance(v, str):\n        raise TypeError(\"WSGI response header value %r is not a string.\")\n    else:\n        raise\nbuf.append(\"\\r\\n\")\nself.wfile.sendall(\"\".join(buf))", "path": "web\\wsgiserver\\__init__.py", "repo_name": "sirsean/Scorekeeper", "stars": 1, "license": "None", "language": "python", "size": 292}
{"docstring": "\"\"\"Read header lines from the incoming stream.\"\"\"\n", "func_signal": "def read_headers(self):\n", "code": "environ = self.environ\n\nwhile True:\n    line = self.rfile.readline()\n    if not line:\n        # No more data--illegal end of headers\n        raise ValueError(\"Illegal end of headers.\")\n    \n    if line == '\\r\\n':\n        # Normal end of headers\n        break\n    \n    if line[0] in ' \\t':\n        # It's a continuation line.\n        v = line.strip()\n    else:\n        k, v = line.split(\":\", 1)\n        k, v = k.strip().upper(), v.strip()\n        envname = \"HTTP_\" + k.replace(\"-\", \"_\")\n    \n    if k in comma_separated_headers:\n        existing = environ.get(envname)\n        if existing:\n            v = \", \".join((existing, v))\n    environ[envname] = v\n\nct = environ.pop(\"HTTP_CONTENT_TYPE\", None)\nif ct is not None:\n    environ[\"CONTENT_TYPE\"] = ct\ncl = environ.pop(\"HTTP_CONTENT_LENGTH\", None)\nif cl is not None:\n    environ[\"CONTENT_LENGTH\"] = cl", "path": "web\\wsgiserver\\__init__.py", "repo_name": "sirsean/Scorekeeper", "stars": 1, "license": "None", "language": "python", "size": 292}
{"docstring": "# give error if Chetah is not installed\n", "func_signal": "def __init__(self, path):\n", "code": "from Cheetah.Template import Template\nself.path = path", "path": "web\\contrib\\template.py", "repo_name": "sirsean/Scorekeeper", "stars": 1, "license": "None", "language": "python", "size": 292}
{"docstring": "\"\"\"WSGI callable to write unbuffered data to the client.\n\nThis method is also used internally by start_response (to write\ndata from the iterable returned by the WSGI application).\n\"\"\"\n", "func_signal": "def write(self, chunk):\n", "code": "if not self.started_response:\n    raise AssertionError(\"WSGI write called before start_response.\")\n\nif not self.sent_headers:\n    self.sent_headers = True\n    self.send_headers()\n\nif self.chunked_write and chunk:\n    buf = [hex(len(chunk))[2:], \"\\r\\n\", chunk, \"\\r\\n\"]\n    self.wfile.sendall(\"\".join(buf))\nelse:\n    self.wfile.sendall(chunk)", "path": "web\\wsgiserver\\__init__.py", "repo_name": "sirsean/Scorekeeper", "stars": 1, "license": "None", "language": "python", "size": 292}
{"docstring": "\"\"\"Like print_exc() but return a string. Backport for Python 2.3.\"\"\"\n", "func_signal": "def format_exc(limit=None):\n", "code": "try:\n    etype, value, tb = sys.exc_info()\n    return ''.join(traceback.format_exception(etype, value, tb, limit))\nfinally:\n    etype = value = tb = None", "path": "web\\wsgiserver\\__init__.py", "repo_name": "sirsean/Scorekeeper", "stars": 1, "license": "None", "language": "python", "size": 292}
{"docstring": "# Use max, disallow tiny reads in a loop as they are very inefficient.\n# We never leave read() with any leftover data from a new recv() call\n# in our internal buffer.\n", "func_signal": "def read(self, size=-1):\n", "code": "rbufsize = max(self._rbufsize, self.default_bufsize)\n# Our use of StringIO rather than lists of string objects returned by\n# recv() minimizes memory usage and fragmentation that occurs when\n# rbufsize is large compared to the typical return value of recv().\nbuf = self._rbuf\nbuf.seek(0, 2)  # seek end\nif size < 0:\n    # Read until EOF\n    self._rbuf = StringIO.StringIO()  # reset _rbuf.  we consume it via buf.\n    while True:\n        data = self.recv(rbufsize)\n        if not data:\n            break\n        buf.write(data)\n    return buf.getvalue()\nelse:\n    # Read until size bytes or EOF seen, whichever comes first\n    buf_len = buf.tell()\n    if buf_len >= size:\n        # Already have size bytes in our buffer?  Extract and return.\n        buf.seek(0)\n        rv = buf.read(size)\n        self._rbuf = StringIO.StringIO()\n        self._rbuf.write(buf.read())\n        return rv\n\n    self._rbuf = StringIO.StringIO()  # reset _rbuf.  we consume it via buf.\n    while True:\n        left = size - buf_len\n        # recv() will malloc the amount of memory given as its\n        # parameter even though it often returns much less data\n        # than that.  The returned data string is short lived\n        # as we copy it into a StringIO and free it.  This avoids\n        # fragmentation issues on many platforms.\n        data = self.recv(left)\n        if not data:\n            break\n        n = len(data)\n        if n == size and not buf_len:\n            # Shortcut.  Avoid buffer data copies when:\n            # - We have no data in our buffer.\n            # AND\n            # - Our call to recv returned exactly the\n            #   number of bytes we were asked to read.\n            return data\n        if n == left:\n            buf.write(data)\n            del data  # explicit free\n            break\n        assert n <= left, \"recv(%d) returned %d bytes\" % (left, n)\n        buf.write(data)\n        buf_len += n\n        del data  # explicit free\n        #assert buf_len == buf.tell()\n    return buf.getvalue()", "path": "web\\wsgiserver\\__init__.py", "repo_name": "sirsean/Scorekeeper", "stars": 1, "license": "None", "language": "python", "size": 292}
{"docstring": "\"\"\"Spawn new worker threads (not above self.max).\"\"\"\n", "func_signal": "def grow(self, amount):\n", "code": "for i in xrange(amount):\n    if self.max > 0 and len(self._threads) >= self.max:\n        break\n    worker = WorkerThread(self.server)\n    worker.setName(\"CP WSGIServer \" + worker.getName())\n    self._threads.append(worker)\n    worker.start()", "path": "web\\wsgiserver\\__init__.py", "repo_name": "sirsean/Scorekeeper", "stars": 1, "license": "None", "language": "python", "size": 292}
{"docstring": "\"\"\"Send a packet to the server.\"\"\"\n", "func_signal": "def write(self, sock):\n", "code": "self.length = len(self.data)\nself._sendall(sock, CONTAINER_PREFIX + struct.pack('>H', self.length))\nif self.length:\n    self._sendall(sock, self.data)", "path": "flup\\server\\ajp_base.py", "repo_name": "b1tr0t/Instrumented-FLUP", "stars": 1, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nBegin the HTTP response. This must only be called once and it\nmust be called before any calls to write().\n\nstatusCode is the integer status code (e.g. 200). statusMsg\nis the associated reason message (e.g.'OK'). headers is a list\nof 2-tuples - header name/value pairs. (Both header name and value\nmust be strings.)\n\"\"\"\n", "func_signal": "def startResponse(self, statusCode, statusMsg, headers):\n", "code": "assert not self._headersSent, 'Headers already sent!'\n\npkt = Packet()\npkt.data = PKTTYPE_SEND_HEADERS + \\\n           struct.pack('>H', statusCode) + \\\n           encodeString(statusMsg) + \\\n           struct.pack('>H', len(headers)) + \\\n           ''.join([encodeResponseHeader(name, value)\n                    for name,value in headers])\n\nself._conn.writePacket(pkt)\n\nself._headersSent = True", "path": "flup\\server\\ajp_base.py", "repo_name": "b1tr0t/Instrumented-FLUP", "stars": 1, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Handle an FCGI_GET_VALUES request from the web server.\"\"\"\n", "func_signal": "def _do_get_values(self, inrec):\n", "code": "outrec = Record(FCGI_GET_VALUES_RESULT)\n\npos = 0\nwhile pos < inrec.contentLength:\n    pos, (name, value) = decode_pair(inrec.contentData, pos)\n    cap = self.server.capability.get(name)\n    if cap is not None:\n        outrec.contentData += encode_pair(name, str(cap))\n\noutrec.contentLength = len(outrec.contentData)\nself.writeRecord(outrec)", "path": "flup\\server\\fcgi_base.py", "repo_name": "b1tr0t/Instrumented-FLUP", "stars": 1, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Encode a response header/value pair.\"\"\"\n", "func_signal": "def encodeResponseHeader(name, value):\n", "code": "lname = name.lower()\nif lname in responseHeaderTable:\n    # Use table\n    i = responseHeaderTable.index(lname)\n    out = '\\xa0' + chr(i)\nelse:\n    out = encodeString(name)\nout += encodeString(value)\nreturn out", "path": "flup\\server\\ajp_base.py", "repo_name": "b1tr0t/Instrumented-FLUP", "stars": 1, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nProtected convenience method for subclasses to force an exit. Not\nreally thread-safe, which is why it isn't public.\n\"\"\"\n", "func_signal": "def _exit(self, reload=False):\n", "code": "if self._keepGoing:\n    self._keepGoing = False\n    self._hupReceived = reload", "path": "flup\\server\\threadedserver.py", "repo_name": "b1tr0t/Instrumented-FLUP", "stars": 1, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Ensure certain values are present, if required by WSGI.\"\"\"\n", "func_signal": "def _sanitizeEnv(self, environ):\n", "code": "if not environ.has_key('SCRIPT_NAME'):\n    environ['SCRIPT_NAME'] = ''\n\nreqUri = None\nif environ.has_key('REQUEST_URI'):\n    reqUri = environ['REQUEST_URI'].split('?', 1)\n\nif not environ.has_key('PATH_INFO') or not environ['PATH_INFO']:\n    if reqUri is not None:\n        environ['PATH_INFO'] = reqUri[0]\n    else:\n        environ['PATH_INFO'] = ''\nif not environ.has_key('QUERY_STRING') or not environ['QUERY_STRING']:\n    if reqUri is not None and len(reqUri) > 1:\n        environ['QUERY_STRING'] = reqUri[1]\n    else:\n        environ['QUERY_STRING'] = ''\n\n# If any of these are missing, it probably signifies a broken\n# server...\nfor name,default in [('REQUEST_METHOD', 'GET'),\n                     ('SERVER_NAME', 'localhost'),\n                     ('SERVER_PORT', '80'),\n                     ('SERVER_PROTOCOL', 'HTTP/1.0')]:\n    if not environ.has_key(name):\n        environ['wsgi.errors'].write('%s: missing FastCGI param %s '\n                                     'required by WSGI!\\n' %\n                                     (self.__class__.__name__, name))\n        environ[name] = default", "path": "flup\\server\\fcgi_base.py", "repo_name": "b1tr0t/Instrumented-FLUP", "stars": 1, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nWrite data (which comprises the response body). Note that due to\nrestrictions on AJP packet size, we limit our writes to 8185 bytes\neach packet.\n\"\"\"\n", "func_signal": "def write(self, data):\n", "code": "assert self._headersSent, 'Headers must be sent first!'\n\nbytesLeft = len(data)\nwhile bytesLeft:\n    toWrite = min(bytesLeft, self._maxWrite)\n\n    pkt = Packet()\n    pkt.data = PKTTYPE_SEND_BODY + \\\n               struct.pack('>H', toWrite) + \\\n               data[:toWrite] + '\\x00' # Undocumented\n    self._conn.writePacket(pkt)\n\n    data = data[toWrite:]\n    bytesLeft -= toWrite", "path": "flup\\server\\ajp_base.py", "repo_name": "b1tr0t/Instrumented-FLUP", "stars": 1, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Gets rid of already read data (since we can't rewind).\"\"\"\n", "func_signal": "def _shrinkBuffer(self):\n", "code": "if self._pos >= self._shrinkThreshold:\n    self._buf = self._buf[self._pos:]\n    self._avail -= self._pos\n    self._length -= self._pos\n    self._pos = 0\n\n    assert self._avail >= 0 and self._length >= 0", "path": "flup\\server\\ajp_base.py", "repo_name": "b1tr0t/Instrumented-FLUP", "stars": 1, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nThe main loop. Exits on SIGHUP, SIGINT, SIGTERM. Returns True if\nSIGHUP was received, False otherwise.\n\"\"\"\n", "func_signal": "def run(self):\n", "code": "self._web_server_addrs = os.environ.get('FCGI_WEB_SERVER_ADDRS')\nif self._web_server_addrs is not None:\n    self._web_server_addrs = map(lambda x: x.strip(),\n                                 self._web_server_addrs.split(','))\n\nsock = self._setupSocket()\n\nret = PreforkServer.run(self, sock)\n\nself._cleanupSocket(sock)\n\nreturn ret", "path": "flup\\server\\fcgi_fork.py", "repo_name": "b1tr0t/Instrumented-FLUP", "stars": 1, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Handle the FCGI_DATA stream.\"\"\"\n", "func_signal": "def _do_data(self, inrec):\n", "code": "req = self._requests.get(inrec.requestId)\nif req is not None:\n    req.data.add_data(inrec.contentData)", "path": "flup\\server\\fcgi_base.py", "repo_name": "b1tr0t/Instrumented-FLUP", "stars": 1, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Restores previous signal handlers.\"\"\"\n", "func_signal": "def _restoreSignalHandlers(self):\n", "code": "for signum,handler in self._oldSIGs:\n    signal.signal(signum, handler)", "path": "flup\\server\\preforkserver.py", "repo_name": "b1tr0t/Instrumented-FLUP", "stars": 1, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nHandle an FCGI_ABORT_REQUEST from the web server.\n\nWe just mark a flag in the associated Request.\n\"\"\"\n", "func_signal": "def _do_abort_request(self, inrec):\n", "code": "req = self._requests.get(inrec.requestId)\nif req is not None:\n    req.aborted = True", "path": "flup\\server\\fcgi_base.py", "repo_name": "b1tr0t/Instrumented-FLUP", "stars": 1, "license": "None", "language": "python", "size": 128}
{"docstring": "# Only need to flush if this OutputStream is actually buffered.\n", "func_signal": "def flush(self):\n", "code": "if self._buffered:\n    data = ''.join(self._bufList)\n    self._bufList = []\n    self._write(data)", "path": "flup\\server\\fcgi_base.py", "repo_name": "b1tr0t/Instrumented-FLUP", "stars": 1, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Attempt to read a single Record from the socket and process it.\"\"\"\n# Currently, any children Request threads notify this Connection\n# that it is no longer needed by closing the Connection's socket.\n# We need to put a timeout on select, otherwise we might get\n# stuck in it indefinitely... (I don't like this solution.)\n", "func_signal": "def process_input(self):\n", "code": "while self._keepGoing:\n    try:\n        r, w, e = select.select([self._sock], [], [], 1.0)\n    except ValueError:\n        # Sigh. ValueError gets thrown sometimes when passing select\n        # a closed socket.\n        raise EOFError\n    if r: break\nif not self._keepGoing:\n    return\nrec = Record()\nrec.read(self._sock)\n\nif rec.type == FCGI_GET_VALUES:\n    self._do_get_values(rec)\nelif rec.type == FCGI_BEGIN_REQUEST:\n    self._do_begin_request(rec)\nelif rec.type == FCGI_ABORT_REQUEST:\n    self._do_abort_request(rec)\nelif rec.type == FCGI_PARAMS:\n    self._do_params(rec)\nelif rec.type == FCGI_STDIN:\n    self._do_stdin(rec)\nelif rec.type == FCGI_DATA:\n    self._do_data(rec)\nelif rec.requestId == FCGI_NULL_REQUEST_ID:\n    self._do_unknown_type(rec)\nelse:\n    # Need to complain about this.\n    pass", "path": "flup\\server\\fcgi_base.py", "repo_name": "b1tr0t/Instrumented-FLUP", "stars": 1, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"shutdown all workers.\"\"\"\n", "func_signal": "def shutdown(self):\n", "code": "self._lock.acquire()\nself._stop = True\nself._lock.notifyAll()\nself._lock.release()\n\n# wait for all threads to finish\nfor t in self._threads:\n    t.join()", "path": "flup\\server\\threadpool.py", "repo_name": "b1tr0t/Instrumented-FLUP", "stars": 1, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nHandles a body chunk from the server by appending it to the\nInputStream.\n\"\"\"\n", "func_signal": "def _processBody(self, pkt):\n", "code": "if pkt.length:\n    length = struct.unpack('>H', pkt.data[:2])[0]\n    self._request.input.addData(pkt.data[2:2+length])\nelse:\n    # Shouldn't really ever get here.\n    self._request.input.addData('')", "path": "flup\\server\\ajp_base.py", "repo_name": "b1tr0t/Instrumented-FLUP", "stars": 1, "license": "None", "language": "python", "size": 128}
{"docstring": "# Must use locking to prevent intermingling of Records from different\n# threads.\n", "func_signal": "def writeRecord(self, rec):\n", "code": "self._lock.acquire()\ntry:\n    # Probably faster than calling super. ;)\n    rec.write(self._sock)\nfinally:\n    self._lock.release()", "path": "flup\\server\\fcgi_base.py", "repo_name": "b1tr0t/Instrumented-FLUP", "stars": 1, "license": "None", "language": "python", "size": 128}
{"docstring": "# These are normally filled in by Connection.\n", "func_signal": "def __init__(self, server):\n", "code": "self.requestId = 1\nself.role = FCGI_RESPONDER\nself.flags = 0\nself.aborted = False\n\nself.server = server\nself.params = dict(os.environ)\nself.stdin = sys.stdin\nself.stdout = StdoutWrapper(sys.stdout) # Oh, the humanity!\nself.stderr = sys.stderr\nself.data = StringIO.StringIO()", "path": "flup\\server\\fcgi_base.py", "repo_name": "b1tr0t/Instrumented-FLUP", "stars": 1, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Encode and write a Record to a socket.\"\"\"\n", "func_signal": "def write(self, sock):\n", "code": "self.paddingLength = -self.contentLength & 7\n\nif __debug__: _debug(9, 'write: fd = %d, type = %d, requestId = %d, '\n                     'contentLength = %d' %\n                     (sock.fileno(), self.type, self.requestId,\n                      self.contentLength))\n\nheader = struct.pack(FCGI_Header, self.version, self.type,\n                     self.requestId, self.contentLength,\n                     self.paddingLength)\nself._sendall(sock, header)\nif self.contentLength:\n    self._sendall(sock, self.contentData)\nif self.paddingLength:\n    self._sendall(sock, '\\x00'*self.paddingLength)", "path": "flup\\server\\fcgi_base.py", "repo_name": "b1tr0t/Instrumented-FLUP", "stars": 1, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Fill-in/deduce missing values in environ.\"\"\"\n", "func_signal": "def _sanitizeEnv(self, environ):\n", "code": "reqUri = None\nif environ.has_key('REQUEST_URI'):\n    reqUri = environ['REQUEST_URI'].split('?', 1)\n\n# Ensure QUERY_STRING exists\nif not environ.has_key('QUERY_STRING') or not environ['QUERY_STRING']:\n    if reqUri is not None and len(reqUri) > 1:\n        environ['QUERY_STRING'] = reqUri[1]\n    else:\n        environ['QUERY_STRING'] = ''\n\n# Check WSGI_SCRIPT_NAME\nscriptName = environ.get('WSGI_SCRIPT_NAME')\nif scriptName is None:\n    scriptName = self.scriptName\nelse:\n    warnings.warn('WSGI_SCRIPT_NAME environment variable for scgi '\n                  'servers is deprecated',\n                  DeprecationWarning)\n    if scriptName.lower() == 'none':\n        scriptName = None\n\nif scriptName is None:\n    # Do nothing (most likely coming from cgi2scgi)\n    return\n\nif scriptName is NoDefault:\n    # Pull SCRIPT_NAME/PATH_INFO from environment, with empty defaults\n    if not environ.has_key('SCRIPT_NAME'):\n        environ['SCRIPT_NAME'] = ''\n    if not environ.has_key('PATH_INFO') or not environ['PATH_INFO']:\n        if reqUri is not None:\n            environ['PATH_INFO'] = reqUri[0]\n        else:\n            environ['PATH_INFO'] = ''\nelse:\n    # Configured scriptName\n    warnings.warn('Configured SCRIPT_NAME is deprecated\\n'\n                  'Do not use WSGI_SCRIPT_NAME or the scriptName\\n'\n                  'keyword parameter -- they will be going away',\n                  DeprecationWarning)\n\n    value = environ['SCRIPT_NAME']\n    value += environ.get('PATH_INFO', '')\n    if not value.startswith(scriptName):\n        self.logger.warning('scriptName does not match request URI')\n\n    environ['PATH_INFO'] = value[len(scriptName):]\n    environ['SCRIPT_NAME'] = scriptName", "path": "flup\\server\\scgi_base.py", "repo_name": "b1tr0t/Instrumented-FLUP", "stars": 1, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nConverts objects to table-style list of rows with heading:\n\nExample:\nx.a = 1\nx.b = 2\nx.c = 3\ny.a = 11\ny.b = 12\ny.c = 13\nobject_list_to_table(('a', 'b', 'c'), [x, y])\nresults in the following (dict keys reordered for better readability):\n[\n    ('a', 'b', 'c'),\n    (1, 2, 3),\n    (11, 12, 13),\n]\n\"\"\"\n", "func_signal": "def object_list_to_table(headings, dict_list):\n", "code": "return [headings] + [tuple([getattr_by_path(row, heading, None)\n                            for heading in headings])\n                     for row in dict_list]", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "hilarycheng/gae-junkcode2", "stars": 0, "license": "None", "language": "python", "size": 6980}
{"docstring": "\"\"\"\nAppends '?' or '&' to an url, so you can easily add extra GET parameters.\n\"\"\"\n", "func_signal": "def urlquerybase(url):\n", "code": "if url:\n    if '?' in url:\n        url += '&'\n    else:\n        url += '?'\nreturn url", "path": "common\\appenginepatch\\ragendja\\templatetags\\ragendjatags.py", "repo_name": "hilarycheng/gae-junkcode2", "stars": 0, "license": "None", "language": "python", "size": 6980}
{"docstring": "\"\"\"\nCheck the supplied email address against a list of known free\nwebmail domains.\n\n\"\"\"\n", "func_signal": "def clean_email(self):\n", "code": "email_domain = self.cleaned_data['email'].split('@')[1]\nif email_domain in self.bad_domains:\n    raise forms.ValidationError(_(u'Registration using free email addresses is prohibited. Please supply a different email address.'))\nreturn self.cleaned_data['email']", "path": "registration\\forms.py", "repo_name": "hilarycheng/gae-junkcode2", "stars": 0, "license": "None", "language": "python", "size": 6980}
{"docstring": "\"\"\"\nCreate the new ``User`` and ``RegistrationProfile``, and\nreturns the ``User`` (by calling\n``RegistrationProfile.objects.create_inactive_user()``).\n\n\"\"\"\n", "func_signal": "def save(self, domain_override=\"\"):\n", "code": "new_user = RegistrationProfile.objects.create_inactive_user(username=self.cleaned_data['username'],\n                                                            password=self.cleaned_data['password1'],\n                                                            email=self.cleaned_data['email'],\n                                                            domain_override=domain_override,\n                                                            )\nreturn new_user", "path": "registration\\forms.py", "repo_name": "hilarycheng/gae-junkcode2", "stars": 0, "license": "None", "language": "python", "size": 6980}
{"docstring": "\"\"\"\nValidate that the supplied email address is unique for the\nsite.\n\n\"\"\"\n", "func_signal": "def clean_email(self):\n", "code": "email = self.cleaned_data['email'].lower()\nif User.all().filter('email =', email).filter(\n        'is_active =', True).count(1):\n    raise forms.ValidationError(__(u'This email address is already in use. Please supply a different email address.'))\nreturn email", "path": "myapp\\forms.py", "repo_name": "hilarycheng/gae-junkcode2", "stars": 0, "license": "None", "language": "python", "size": 6980}
{"docstring": "\"\"\"\nValidate that the username is alphanumeric and is not already\nin use.\n\n\"\"\"\n", "func_signal": "def clean_username(self):\n", "code": "user = User.get_by_key_name(\"key_\"+self.cleaned_data['username'].lower())\nif user:\n    raise forms.ValidationError(_(u'This username is already taken. Please choose another.'))\nreturn self.cleaned_data['username']", "path": "registration\\forms.py", "repo_name": "hilarycheng/gae-junkcode2", "stars": 0, "license": "None", "language": "python", "size": 6980}
{"docstring": "\"\"\"\nValidate that the supplied email address is unique for the\nsite.\n\n\"\"\"\n", "func_signal": "def clean_email(self):\n", "code": "email = self.cleaned_data['email'].lower()\nif User.all().filter('email =', email).count(1):\n    raise forms.ValidationError(_(u'This email address is already in use. Please supply a different email address.'))\nreturn email", "path": "registration\\forms.py", "repo_name": "hilarycheng/gae-junkcode2", "stars": 0, "license": "None", "language": "python", "size": 6980}
{"docstring": "\"\"\"\nConverts dict to table-style list of rows with heading:\n\nExample:\ndict_list_to_table(('a', 'b', 'c'),\n    [{'a': 1, 'b': 2, 'c': 3}, {'a': 11, 'b': 12, 'c': 13}])\nresults in the following (dict keys reordered for better readability):\n[\n    ('a', 'b', 'c'),\n    (1, 2, 3),\n    (11, 12, 13),\n]\n\"\"\"\n", "func_signal": "def dict_list_to_table(headings, dict_list):\n", "code": "return [headings] + [tuple([row[heading] for heading in headings])\n                     for row in dict_list]", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "hilarycheng/gae-junkcode2", "stars": 0, "license": "None", "language": "python", "size": 6980}
{"docstring": "\"\"\"\n * @param int mode\n * @param float $d1\n * @param float $d2\n\"\"\"\n", "func_signal": "def SMER (mode, d1, d2) :\n", "code": "if mode == 1:\n\td3 = 6378388\n\td4 = 0.0033670033670033669\nelse:\n\td3 = 6378137\n\td4 = 0.0033528106647429845\n\nd5 = 2.0 * d4 - pow (d4, 2);\n\nd6 = 1.0 \\\n\t+ 0.75 * d5 \\\n\t+ 0.703125 * pow (d5, 2) \\\n\t+ 0.68359375 * pow (d5, 3);\nd7 = 0.75 * d5 \\\n\t+ 0.9375 * pow (d5, 2) \\\n\t+ 1.025390625 * pow (d5, 3);\nd8 = 0.234375 * pow (d5, 2) \\\n\t+ 0.41015625 * pow (d5, 3);\nd9 = 0.068359375 * pow (d5, 3);\n\nd10 = d2 - d1;\nd11 = math.sin (d2 * 2) - math.sin (d1 * 2);\nd12 = math.sin (d2 * 4) - math.sin (d1 * 4);\nd13 = math.sin (d2 * 6) - math.sin (d1 * 6);\n\nd14 = d3 * (1.0 - d5);\nd14 *= (d6 * d10) \\\n\t- (d7 * d11 / 2.0) \\\n\t+ (d8 * d12 / 4.0) \\\n\t- (d9 * d13 / 6.0);\n\nreturn d14;", "path": "gogogo\\utils\\HK1980.py", "repo_name": "hilarycheng/gae-junkcode2", "stars": 0, "license": "None", "language": "python", "size": 6980}
{"docstring": "\"\"\"\n  Converts HK1980 data to WGS84 data\n \n  @param int mode 1 = Hayford, 2 = WGS84\n  @return A tuple of HK1980 grid reference system (North , East )\n\"\"\"\n\n", "func_signal": "def wgs84_to_hk1980 (lat,lon,mode=2):\n", "code": "assert(mode == 1 or mode == 2 )\n\nassert( lat >= 22 and lat < 23)\n\nassert( lon >= 113 and lon < 115) \n\nif mode == 1:\n\td4 = 22.312133333333335 * M_PI_180\n\td5 = 114.17855555555556 * M_PI_180\nelse:\n\td4 = 22.310602777777778 * M_PI_180;\n\td5 = 114.1810138888889 * M_PI_180;\n\nd6 = lat * M_PI_180;\nd7 = lon * M_PI_180;\n\nd8 = SMER (mode, 0, d4);\nd9 = SMER (mode, 0, d6);\n\n(d34, d35) = RADIUS (mode, d6);\n\nd10 = (d7 - d5) * math.cos (d6);\nd11 = math.tan (d6);\nd12 = pow (d11, 2);\nd13 = pow (d11, 4);\nd14 = pow (d11, 6);\n\nd15 = d35 / d34;\nd16 = pow (d15, 2);\nd17 = pow (d15, 3);\nd18 = pow (d15, 4);\n\nd19 = d9 - d8;\n\nd20 = (d35 / 2) * pow (d10, 2) * d11;\nd21 = (d20 / 12) * pow (d10, 2) * (d16 * 4 + d15 - d12);\nd22 = (d21 / 30) * pow (d10, 2);\nd22 *=   d18 * (88 - d12 * 192) \\\n\t- d17 * (28 - d12 * 168) \\\n\t+ d16 * (1 - d12 * 32) \\\n\t- d15 * d12 * 2 \\\n\t+ d13;\nd23 = d22 / 56 * pow (d10, 2) \\\n\t* (1385 - 3111 * d12 + d13 * 543 - d14);\nd24 = d35 * d10;\nd25 = d24 / 6  * pow (d10, 2);\nd26 = d25 / 20 * pow (d10, 2);\n\nd27 = d26 / 42 * pow (d10, 2); \n# The original line from Abel's code should be \n#  d27 = d27 / 42 * pow (d10, 2);\n\nd25 *= d15 - d12;\nd26 *= d17 * 4 * (1 - d12 * 6) \\\n\t+ d16 * (1 + d12 * 8) \\\n\t- d15 * d12 * 2 + d13;\nd27 *= 61 - d12 * 479 + d13 * 179 - d14;\n\nd36 = d19 + d20 + d21 + d22 + d23 + 819069.80000000005;\nd37 = d24 + d25 + d26 + d27 + 836694.05000000005;\n\nif mode == 2:\n\td28 = d36;\n\td29 = d37;\n\td30 = 0.99999983729999997;\n\td31 = -2.7858E-005;\n\n\td36 = (d28 * d30 - d29 * d31) - 23.098331000000002;\n\td37 = (d28 * d31 + d29 * d30) + 23.149764999999999;\n\nnorth = d36;\neast = d37;\n\nreturn (north , east)", "path": "gogogo\\utils\\HK1980.py", "repo_name": "hilarycheng/gae-junkcode2", "stars": 0, "license": "None", "language": "python", "size": 6980}
{"docstring": "\"\"\"Returns a subset of the keys of a dictionary.\"\"\"\n", "func_signal": "def subdict(data, *attrs):\n", "code": "result = {}\nresult.update([(key, data[key]) for key in attrs])\nreturn result", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "hilarycheng/gae-junkcode2", "stars": 0, "license": "None", "language": "python", "size": 6980}
{"docstring": "\"\"\"\nVerifiy that the values entered into the two password fields\nmatch. Note that an error here will end up in\n``non_field_errors()`` because it doesn't apply to a single\nfield.\n\n\"\"\"\n", "func_signal": "def clean(self):\n", "code": "if 'password1' in self.cleaned_data and 'password2' in self.cleaned_data:\n    if self.cleaned_data['password1'] != self.cleaned_data['password2']:\n        raise forms.ValidationError(__(u'You must type the same password each time'))\nreturn self.cleaned_data", "path": "myapp\\forms.py", "repo_name": "hilarycheng/gae-junkcode2", "stars": 0, "license": "None", "language": "python", "size": 6980}
{"docstring": "\"\"\"\nVerifiy that the values entered into the two password fields\nmatch. Note that an error here will end up in\n``non_field_errors()`` because it doesn't apply to a single\nfield.\n\n\"\"\"\n", "func_signal": "def clean(self):\n", "code": "if 'password1' in self.cleaned_data and 'password2' in self.cleaned_data:\n    if self.cleaned_data['password1'] != self.cleaned_data['password2']:\n        raise forms.ValidationError(_(u'You must type the same password each time'))\nreturn self.cleaned_data", "path": "registration\\forms.py", "repo_name": "hilarycheng/gae-junkcode2", "stars": 0, "license": "None", "language": "python", "size": 6980}
{"docstring": "\"\"\"\n Converts HK1980 data to WGS84 data\n \n @param mode 1 = Hayford, 2 = WGS84\n @return A tuple of (lat,lon)\n\"\"\"\n", "func_signal": "def hk1980_to_wgs84 (north,east,mode=2):\n", "code": "assert(mode == 1 or mode == 2 )\t\n\nd1 = north;\nd2 = east;\n\nif mode == 1:\n\td4 = 22.312133333333335 * M_PI_180\n\td5 = 114.17855555555556 * M_PI_180\nelse:\n\td4 = 22.310602777777778 * M_PI_180;\n\td5 = 114.1810138888889 * M_PI_180;\n\nif (mode == 2):\n\td6 = 1.0000001619000001;\n\td7 = 2.7858E-05;\n\td8 = 23.098979;\n\td9 = -23.149125000000002;\n\td10 = d6 * d1 - d7 * d2 + d8;\n\td11 = d7 * d1 + d6 * d2 + d9;\n\td1 = d10;\n\td2 = d11;\n\nd28 = d1 - 819069.80000000005;\nd37 = d2 - 836694.05000000005;\nd12 = 6.8535615239999998;\nd13 = 110736.3925;\nd14 = (math.sqrt (d28 * d12 * 4 + pow (d13, 2)) - d13) / 2 / d12 * M_PI_180;\nd15 = d4 + d14;\nd16 = 0;\nd18 = 0;\n\nwhile True:\n\td15 += d16;\n\td17 = SMER (mode, d4, d15);\n\td18 = d28 - d17;\n\td39, d41 = RADIUS (mode, d15);\n\td16 = d18 / d39;\n\tif not (abs(d18) > 1E-06):\n\t\tbreak;\n\n(d40, d42) = RADIUS (mode, d15);\nd19 = math.tan (d15);\nd20 = pow (d19, 2);\nd21 = pow (d19, 4);\nd23 = d42 / d40;\nd24 = pow (d23, 2);\nd25 = pow (d23, 3);\nd26 = pow (d23, 4);\nd28 = d37 / d42;\nd38 = pow (d28, 2);\nd29 = d37 / d40 * d28 * d19 / 2;\nd30 = d29 / 12 * d38 * ((9 * d23 * (1 - d20) - 4 * d24) + 12 * d20);\nd31 = d29 / 360 * pow (d38, 2);\nd31 *= 8 * d26 * (11 - 24 * d20) \\\n\t- 12 * d25 * (21 - 71 * d20) \\\n\t+ 15 * d24 * (15 - 98 * d20 + 15 * d21) \\\n\t+ 180 * d23 * (5 * d20 - 3 * d21) \\\n\t+ 360 * d21;\nd32 = d29 / 20160 * pow (d38, 3) * (1385 + 3633 * d20 + 4095 * d21 + 1575 * d20 * d21);\nd44 = d15 - d29 + d30 - d31 + d32;\nd33 = d28 / math.cos (d15);\nd34 = d33 * d38 / 6 * (d23 + 2 * d20);\nd35 = d33 * pow (d38, 2) / 120;\nd35 *= d24 * (9 - 68 * d20) - 4 * d25 * (1 - 6 * d20) + 72 * d23 * d20 + 24 * d21;\nd36 = d33 * pow (d38, 3) / 5040 * (61 + 662 * d20 + 1320 * d21 + 720 * d20 * d21);\nd43 = d5 + d33 - d34 + d35 - d36;\nd44 /= M_PI_180;\nd43 /= M_PI_180;\n\nlat = d44;\nlon = d43;\n\nreturn (lat,lon)", "path": "gogogo\\utils\\HK1980.py", "repo_name": "hilarycheng/gae-junkcode2", "stars": 0, "license": "None", "language": "python", "size": 6980}
{"docstring": "\"\"\"\nCreate the new ``User`` and ``RegistrationProfile``, and\nreturns the ``User``.\n\nThis is essentially a light wrapper around\n``RegistrationProfile.objects.create_inactive_user()``,\nfeeding it the form data and a profile callback (see the\ndocumentation on ``create_inactive_user()`` for details) if\nsupplied.\n\n\"\"\"\n", "func_signal": "def save(self, domain_override=\"\"):\n", "code": "new_user = RegistrationProfile.objects.create_inactive_user(\n    username=self.cleaned_data['username'],\n    password=self.cleaned_data['password1'],\n    email=self.cleaned_data['email'],\n    domain_override=domain_override)\nself.instance = new_user\nreturn super(UserRegistrationForm, self).save()", "path": "myapp\\forms.py", "repo_name": "hilarycheng/gae-junkcode2", "stars": 0, "license": "None", "language": "python", "size": 6980}
{"docstring": "\"\"\"\nValidate that the username is alphanumeric and is not already\nin use.\n\n\"\"\"\n", "func_signal": "def clean_username(self):\n", "code": "user = User.get_by_key_name(\"key_\"+self.cleaned_data['username'].lower())\nif user and user.is_active:\n    raise forms.ValidationError(__(u'This username is already taken. Please choose another.'))\nreturn self.cleaned_data['username']", "path": "myapp\\forms.py", "repo_name": "hilarycheng/gae-junkcode2", "stars": 0, "license": "None", "language": "python", "size": 6980}
{"docstring": "\"\"\"\nCompares two lists and returs True if they contain the same elements, but\ndoesn't require that they have the same order.\n\"\"\"\n", "func_signal": "def equal_lists(left, right):\n", "code": "right = list(right)\nif len(left) != len(right):\n    return False\nfor item in left:\n    if item in right:\n        del right[right.index(item)]\n    else:\n        return False\nreturn True", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "hilarycheng/gae-junkcode2", "stars": 0, "license": "None", "language": "python", "size": 6980}
{"docstring": "\"\"\"Like getattr(), but can go down a hierarchy like 'attr.subattr'\"\"\"\n", "func_signal": "def getattr_by_path(obj, attr, *default):\n", "code": "value = obj\nfor part in attr.split('.'):\n    if not hasattr(value, part) and len(default):\n        return default[0]\n    value = getattr(value, part)\n    if callable(value):\n        value = value()\nreturn value", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "hilarycheng/gae-junkcode2", "stars": 0, "license": "None", "language": "python", "size": 6980}
{"docstring": "\"\"\"\n  @access private\n  @param mode\n  @param d1\n \"\"\"\n", "func_signal": "def RADIUS (mode, d1):\n", "code": "if (mode == 1):\n\td2 = 6378388\n\td3 = 0.0033670033670033669\nelse:\n\td2 = 6378137\n\td3 = 0.0033528106647429845\n\nd4 = d3 * 2 - pow (d3, 2);\nd5 = 1.0 - d4 * pow (math.sin (d1), 2);\nd6 = (d2 * (1.0 - d4)) / pow (d5, 1.5);\nd7 = d2 / math.sqrt (d5);\n\nreturn (d6, d7);", "path": "gogogo\\utils\\HK1980.py", "repo_name": "hilarycheng/gae-junkcode2", "stars": 0, "license": "None", "language": "python", "size": 6980}
{"docstring": "\"\"\"Creates a class-wide instance property with a thread-specific value.\"\"\"\n", "func_signal": "def make_tls_property(default=None):\n", "code": "class TLSProperty(object):\n    def __init__(self):\n        self.local = local()\n\n    def __get__(self, instance, cls):\n        if not instance:\n            return self\n        return self.value\n\n    def __set__(self, instance, value):\n        self.value = value\n\n    def _get_value(self):\n        return getattr(self.local, 'value', default)\n    def _set_value(self, value):\n        self.local.value = value\n    value = property(_get_value, _set_value)\n\nreturn TLSProperty()", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "hilarycheng/gae-junkcode2", "stars": 0, "license": "None", "language": "python", "size": 6980}
{"docstring": "\"\"\"\nPickling support.\n\"\"\"\n", "func_signal": "def __getstate__(self):\n", "code": "obj_dict = self.__dict__.copy()\nobj_dict['related_select_fields'] = []\nobj_dict['related_select_cols'] = []\ndel obj_dict['connection']\nreturn obj_dict", "path": "db\\models\\sql\\query.py", "repo_name": "myles/django", "stars": 1, "license": "None", "language": "python", "size": 5868}
{"docstring": "\"\"\"\nReturn two independent iterators from a single iterable.\n\nBased on http://www.python.org/doc/2.3.5/lib/itertools-example.html\n\"\"\"\n# Note: Using a dictionary and a list as the default arguments here is\n# deliberate and safe in this instance.\n", "func_signal": "def compat_tee(iterable):\n", "code": "def gen(next, data={}, cnt=[0]):\n    dpop = data.pop\n    for i in itertools.count():\n        if i == cnt[0]:\n            item = data[i] = next()\n            cnt[0] += 1\n        else:\n            item = dpop(i)\n        yield item\nnext = iter(iterable).next\nreturn gen(next), gen(next)", "path": "utils\\itercompat.py", "repo_name": "myles/django", "stars": 1, "license": "None", "language": "python", "size": 5868}
{"docstring": "\"\"\"\nYields blocks of rows from a cursor. We use this iterator in the special\ncase when extra output columns have been added to support ordering\nrequirements. We must trim those extra columns before anything else can use\nthe results, since they're only needed to make the SQL valid.\n\"\"\"\n", "func_signal": "def order_modified_iter(cursor, trim, sentinel):\n", "code": "for rows in iter((lambda: cursor.fetchmany(GET_ITERATOR_CHUNK_SIZE)),\n        sentinel):\n    yield [r[:-trim] for r in rows]", "path": "db\\models\\sql\\query.py", "repo_name": "myles/django", "stars": 1, "license": "None", "language": "python", "size": 5868}
{"docstring": "\"\"\"\nWalks along a chain of aliases, promoting the first nullable join and\nany joins following that. If 'must_promote' is True, all the aliases in\nthe chain are promoted.\n\"\"\"\n", "func_signal": "def promote_alias_chain(self, chain, must_promote=False):\n", "code": "for alias in chain:\n    if self.promote_alias(alias, must_promote):\n        must_promote = True", "path": "db\\models\\sql\\query.py", "repo_name": "myles/django", "stars": 1, "license": "None", "language": "python", "size": 5868}
{"docstring": "\"\"\"\nReturns the query as a string of SQL with the parameter values\nsubstituted in.\n\nParameter values won't necessarily be quoted correctly, since that is\ndone by the database interface at execution time.\n\"\"\"\n", "func_signal": "def __str__(self):\n", "code": "sql, params = self.as_sql()\nreturn sql % params", "path": "db\\models\\sql\\query.py", "repo_name": "myles/django", "stars": 1, "license": "None", "language": "python", "size": 5868}
{"docstring": "\"\"\"\nRemoves any ordering settings. If 'force_empty' is True, there will be\nno ordering in the resulting query (not even the model's default).\n\"\"\"\n", "func_signal": "def clear_ordering(self, force_empty=False):\n", "code": "self.order_by = []\nself.extra_order_by = ()\nif force_empty:\n    self.default_ordering = False", "path": "db\\models\\sql\\query.py", "repo_name": "myles/django", "stars": 1, "license": "None", "language": "python", "size": 5868}
{"docstring": "\"\"\"\nUnpickling support.\n\"\"\"\n", "func_signal": "def __setstate__(self, obj_dict):\n", "code": "self.__dict__.update(obj_dict)\n# XXX: Need a better solution for this when multi-db stuff is\n# supported. It's the only class-reference to the module-level\n# connection variable.\nself.connection = connection", "path": "db\\models\\sql\\query.py", "repo_name": "myles/django", "stars": 1, "license": "None", "language": "python", "size": 5868}
{"docstring": "\"\"\"\nMerge the 'rhs' query into the current one (with any 'rhs' effects\nbeing applied *after* (that is, \"to the right of\") anything in the\ncurrent query. 'rhs' is not modified during a call to this function.\n\nThe 'connector' parameter describes how to connect filters from the\n'rhs' query.\n\"\"\"\n", "func_signal": "def combine(self, rhs, connector):\n", "code": "assert self.model == rhs.model, \\\n        \"Cannot combine queries on two different base models.\"\nassert self.can_filter(), \\\n        \"Cannot combine queries once a slice has been taken.\"\nassert self.distinct == rhs.distinct, \\\n    \"Cannot combine a unique query with a non-unique query.\"\n\n# Work out how to relabel the rhs aliases, if necessary.\nchange_map = {}\nused = set()\nconjunction = (connector == AND)\nfirst = True\nfor alias in rhs.tables:\n    if not rhs.alias_refcount[alias]:\n        # An unused alias.\n        continue\n    promote = (rhs.alias_map[alias][JOIN_TYPE] == self.LOUTER)\n    new_alias = self.join(rhs.rev_join_map[alias],\n            (conjunction and not first), used, promote, not conjunction)\n    used.add(new_alias)\n    change_map[alias] = new_alias\n    first = False\n\n# So that we don't exclude valid results in an \"or\" query combination,\n# the first join that is exclusive to the lhs (self) must be converted\n# to an outer join.\nif not conjunction:\n    for alias in self.tables[1:]:\n        if self.alias_refcount[alias] == 1:\n            self.promote_alias(alias, True)\n            break\n\n# Now relabel a copy of the rhs where-clause and add it to the current\n# one.\nif rhs.where:\n    w = deepcopy(rhs.where)\n    w.relabel_aliases(change_map)\n    if not self.where:\n        # Since 'self' matches everything, add an explicit \"include\n        # everything\" where-constraint so that connections between the\n        # where clauses won't exclude valid results.\n        self.where.add(EverythingNode(), AND)\nelif self.where:\n    # rhs has an empty where clause.\n    w = self.where_class()\n    w.add(EverythingNode(), AND)\nelse:\n    w = self.where_class()\nself.where.add(w, connector)\n\n# Selection columns and extra extensions are those provided by 'rhs'.\nself.select = []\nfor col in rhs.select:\n    if isinstance(col, (list, tuple)):\n        self.select.append((change_map.get(col[0], col[0]), col[1]))\n    else:\n        item = deepcopy(col)\n        item.relabel_aliases(change_map)\n        self.select.append(item)\nself.select_fields = rhs.select_fields[:]\n\nif connector == OR:\n    # It would be nice to be able to handle this, but the queries don't\n    # really make sense (or return consistent value sets). Not worth\n    # the extra complexity when you can write a real query instead.\n    if self.extra_select and rhs.extra_select:\n        raise ValueError(\"When merging querysets using 'or', you \"\n                \"cannot have extra(select=...) on both sides.\")\n    if self.extra_where and rhs.extra_where:\n        raise ValueError(\"When merging querysets using 'or', you \"\n                \"cannot have extra(where=...) on both sides.\")\nself.extra_select.update(rhs.extra_select)\nself.extra_tables += rhs.extra_tables\nself.extra_where += rhs.extra_where\nself.extra_params += rhs.extra_params\n\n# Ordering uses the 'rhs' ordering, unless it has none, in which case\n# the current ordering is used.\nself.order_by = rhs.order_by and rhs.order_by[:] or self.order_by\nself.extra_order_by = rhs.extra_order_by or self.extra_order_by", "path": "db\\models\\sql\\query.py", "repo_name": "myles/django", "stars": 1, "license": "None", "language": "python", "size": 5868}
{"docstring": "\"\"\"\nRender a given template with any extra URL parameters in the context as\n``{{ params }}``.\n\"\"\"\n", "func_signal": "def direct_to_template(request, template, extra_context=None, mimetype=None, **kwargs):\n", "code": "if extra_context is None: extra_context = {}\ndictionary = {'params': kwargs}\nfor key, value in extra_context.items():\n    if callable(value):\n        dictionary[key] = value()\n    else:\n        dictionary[key] = value\nc = RequestContext(request, dictionary)\nt = loader.get_template(template)\nreturn HttpResponse(t.render(c), mimetype=mimetype)", "path": "views\\generic\\simple.py", "repo_name": "myles/django", "stars": 1, "license": "None", "language": "python", "size": 5868}
{"docstring": "\"\"\"\nAdds the given (model) fields to the select set. The field names are\nadded in the order specified.\n\"\"\"\n", "func_signal": "def add_fields(self, field_names, allow_m2m=True):\n", "code": "alias = self.get_initial_alias()\nopts = self.get_meta()\ntry:\n    for name in field_names:\n        field, target, u2, joins, u3, u4 = self.setup_joins(\n                name.split(LOOKUP_SEP), opts, alias, False, allow_m2m,\n                True)\n        final_alias = joins[-1]\n        col = target.column\n        if len(joins) > 1:\n            join = self.alias_map[final_alias]\n            if col == join[RHS_JOIN_COL]:\n                self.unref_alias(final_alias)\n                final_alias = join[LHS_ALIAS]\n                col = join[LHS_JOIN_COL]\n                joins = joins[:-1]\n        self.promote_alias_chain(joins[1:])\n        self.select.append((final_alias, col))\n        self.select_fields.append(field)\nexcept MultiJoin:\n    raise FieldError(\"Invalid field name: '%s'\" % name)\nexcept FieldError:\n    names = opts.get_all_field_names() + self.extra_select.keys()\n    names.sort()\n    raise FieldError(\"Cannot resolve keyword %r into field. \"\n            \"Choices are: %s\" % (name, \", \".join(names)))", "path": "db\\models\\sql\\query.py", "repo_name": "myles/django", "stars": 1, "license": "None", "language": "python", "size": 5868}
{"docstring": "\"\"\"\nAdds a Q-object to the current filter.\n\nCan also be used to add anything that has an 'add_to_query()' method.\n\"\"\"\n", "func_signal": "def add_q(self, q_object, used_aliases=None):\n", "code": "if used_aliases is None:\n    used_aliases = self.used_aliases\nif hasattr(q_object, 'add_to_query'):\n    # Complex custom objects are responsible for adding themselves.\n    q_object.add_to_query(self, used_aliases)\nelse:\n    if self.where and q_object.connector != AND and len(q_object) > 1:\n        self.where.start_subtree(AND)\n        subtree = True\n    else:\n        subtree = False\n    connector = AND\n    for child in q_object.children:\n        if connector == OR:\n            refcounts_before = self.alias_refcount.copy()\n        if isinstance(child, Node):\n            self.where.start_subtree(connector)\n            self.add_q(child, used_aliases)\n            self.where.end_subtree()\n        else:\n            self.add_filter(child, connector, q_object.negated,\n                    can_reuse=used_aliases)\n        if connector == OR:\n            # Aliases that were newly added or not used at all need to\n            # be promoted to outer joins if they are nullable relations.\n            # (they shouldn't turn the whole conditional into the empty\n            # set just because they don't match anything).\n            self.promote_unused_aliases(refcounts_before, used_aliases)\n        connector = q_object.connector\n    if q_object.negated:\n        self.where.negate()\n    if subtree:\n        self.where.end_subtree()\nif self.filter_is_sticky:\n    self.used_aliases = used_aliases", "path": "db\\models\\sql\\query.py", "repo_name": "myles/django", "stars": 1, "license": "None", "language": "python", "size": 5868}
{"docstring": "\"\"\"\nReturns a tuple representing the SQL elements in the \"group by\" clause.\n\"\"\"\n", "func_signal": "def get_grouping(self):\n", "code": "qn = self.quote_name_unless_alias\nresult = []\nfor col in self.group_by:\n    if isinstance(col, (list, tuple)):\n        result.append('%s.%s' % (qn(col[0]), qn(col[1])))\n    elif hasattr(col, 'as_sql'):\n        result.append(col.as_sql(qn))\n    else:\n        result.append(str(col))\nreturn result", "path": "db\\models\\sql\\query.py", "repo_name": "myles/django", "stars": 1, "license": "None", "language": "python", "size": 5868}
{"docstring": "\"\"\"\nReturn the list of columns to use in the select statement. If no\ncolumns have been specified, returns all columns relating to fields in\nthe model.\n\nIf 'with_aliases' is true, any column names that are duplicated\n(without the table names) are given unique aliases. This is needed in\nsome cases to avoid ambiguitity with nested queries.\n\"\"\"\n", "func_signal": "def get_columns(self, with_aliases=False):\n", "code": "qn = self.quote_name_unless_alias\nqn2 = self.connection.ops.quote_name\nresult = ['(%s) AS %s' % (col[0], qn2(alias)) for alias, col in self.extra_select.iteritems()]\naliases = set(self.extra_select.keys())\nif with_aliases:\n    col_aliases = aliases.copy()\nelse:\n    col_aliases = set()\nif self.select:\n    for col in self.select:\n        if isinstance(col, (list, tuple)):\n            r = '%s.%s' % (qn(col[0]), qn(col[1]))\n            if with_aliases and col[1] in col_aliases:\n                c_alias = 'Col%d' % len(col_aliases)\n                result.append('%s AS %s' % (r, c_alias))\n                aliases.add(c_alias)\n                col_aliases.add(c_alias)\n            else:\n                result.append(r)\n                aliases.add(r)\n                col_aliases.add(col[1])\n        else:\n            result.append(col.as_sql(quote_func=qn))\n            if hasattr(col, 'alias'):\n                aliases.add(col.alias)\n                col_aliases.add(col.alias)\nelif self.default_cols:\n    cols, new_aliases = self.get_default_columns(with_aliases,\n            col_aliases)\n    result.extend(cols)\n    aliases.update(new_aliases)\nfor table, col in self.related_select_cols:\n    r = '%s.%s' % (qn(table), qn(col))\n    if with_aliases and col in col_aliases:\n        c_alias = 'Col%d' % len(col_aliases)\n        result.append('%s AS %s' % (r, c_alias))\n        aliases.add(c_alias)\n        col_aliases.add(c_alias)\n    else:\n        result.append(r)\n        aliases.add(r)\n        col_aliases.add(col)\n\nself._select_aliases = aliases\nreturn result", "path": "db\\models\\sql\\query.py", "repo_name": "myles/django", "stars": 1, "license": "None", "language": "python", "size": 5868}
{"docstring": "\"\"\"\nCreates the SQL for this query. Returns the SQL string and list of\nparameters.\n\nIf 'with_limits' is False, any limit/offset information is not included\nin the query.\n\"\"\"\n", "func_signal": "def as_sql(self, with_limits=True, with_col_aliases=False):\n", "code": "self.pre_sql_setup()\nout_cols = self.get_columns(with_col_aliases)\nordering = self.get_ordering()\n\n# This must come after 'select' and 'ordering' -- see docstring of\n# get_from_clause() for details.\nfrom_, f_params = self.get_from_clause()\n\nwhere, w_params = self.where.as_sql(qn=self.quote_name_unless_alias)\nparams = []\nfor val in self.extra_select.itervalues():\n    params.extend(val[1])\n\nresult = ['SELECT']\nif self.distinct:\n    result.append('DISTINCT')\nresult.append(', '.join(out_cols + self.ordering_aliases))\n\nresult.append('FROM')\nresult.extend(from_)\nparams.extend(f_params)\n\nif where:\n    result.append('WHERE %s' % where)\n    params.extend(w_params)\nif self.extra_where:\n    if not where:\n        result.append('WHERE')\n    else:\n        result.append('AND')\n    result.append(' AND '.join(self.extra_where))\n\nif self.group_by:\n    grouping = self.get_grouping()\n    result.append('GROUP BY %s' % ', '.join(grouping))\n\nif self.having:\n    having, h_params = self.get_having()\n    result.append('HAVING %s' % ', '.join(having))\n    params.extend(h_params)\n\nif ordering:\n    result.append('ORDER BY %s' % ', '.join(ordering))\n\nif with_limits:\n    if self.high_mark is not None:\n        result.append('LIMIT %d' % (self.high_mark - self.low_mark))\n    if self.low_mark:\n        if self.high_mark is None:\n            val = self.connection.ops.no_limit_value()\n            if val:\n                result.append('LIMIT %d' % val)\n        result.append('OFFSET %d' % self.low_mark)\n\nparams.extend(self.extra_params)\nreturn ' '.join(result), tuple(params)", "path": "db\\models\\sql\\query.py", "repo_name": "myles/django", "stars": 1, "license": "None", "language": "python", "size": 5868}
{"docstring": "\"\"\"\nReturns a tuple representing the SQL elements in the \"having\" clause.\nBy default, the elements of self.having have their as_sql() method\ncalled or are returned unchanged (if they don't have an as_sql()\nmethod).\n\"\"\"\n", "func_signal": "def get_having(self):\n", "code": "result = []\nparams = []\nfor elt in self.having:\n    if hasattr(elt, 'as_sql'):\n        sql, params = elt.as_sql()\n        result.append(sql)\n        params.extend(params)\n    else:\n        result.append(elt)\nreturn result, params", "path": "db\\models\\sql\\query.py", "repo_name": "myles/django", "stars": 1, "license": "None", "language": "python", "size": 5868}
{"docstring": "\"\"\"\nCreates a copy of the current instance. The 'kwargs' parameter can be\nused by clients to update attributes after copying has taken place.\n\"\"\"\n", "func_signal": "def clone(self, klass=None, **kwargs):\n", "code": "obj = Empty()\nobj.__class__ = klass or self.__class__\nobj.model = self.model\nobj.connection = self.connection\nobj.alias_refcount = self.alias_refcount.copy()\nobj.alias_map = self.alias_map.copy()\nobj.table_map = self.table_map.copy()\nobj.join_map = self.join_map.copy()\nobj.rev_join_map = self.rev_join_map.copy()\nobj.quote_cache = {}\nobj.default_cols = self.default_cols\nobj.default_ordering = self.default_ordering\nobj.standard_ordering = self.standard_ordering\nobj.ordering_aliases = []\nobj.start_meta = self.start_meta\nobj.select_fields = self.select_fields[:]\nobj.related_select_fields = self.related_select_fields[:]\nobj.dupe_avoidance = self.dupe_avoidance.copy()\nobj.select = self.select[:]\nobj.tables = self.tables[:]\nobj.where = deepcopy(self.where)\nobj.where_class = self.where_class\nobj.group_by = self.group_by[:]\nobj.having = self.having[:]\nobj.order_by = self.order_by[:]\nobj.low_mark, obj.high_mark = self.low_mark, self.high_mark\nobj.distinct = self.distinct\nobj.select_related = self.select_related\nobj.related_select_cols = []\nobj.max_depth = self.max_depth\nobj.extra_select = self.extra_select.copy()\nobj.extra_tables = self.extra_tables\nobj.extra_where = self.extra_where\nobj.extra_params = self.extra_params\nobj.extra_order_by = self.extra_order_by\nif self.filter_is_sticky and self.used_aliases:\n    obj.used_aliases = self.used_aliases.copy()\nelse:\n    obj.used_aliases = set()\nobj.filter_is_sticky = False\nobj.__dict__.update(kwargs)\nif hasattr(obj, '_setup_query'):\n    obj._setup_query()\nreturn obj", "path": "db\\models\\sql\\query.py", "repo_name": "myles/django", "stars": 1, "license": "None", "language": "python", "size": 5868}
{"docstring": "\"\"\"\nSets up the select_related data structure so that we only select\ncertain related models (as opposed to all models, when\nself.select_related=True).\n\"\"\"\n", "func_signal": "def add_select_related(self, fields):\n", "code": "field_dict = {}\nfor field in fields:\n    d = field_dict\n    for part in field.split(LOOKUP_SEP):\n        d = d.setdefault(part, {})\nself.select_related = field_dict\nself.related_select_cols = []\nself.related_select_fields = []", "path": "db\\models\\sql\\query.py", "repo_name": "myles/django", "stars": 1, "license": "None", "language": "python", "size": 5868}
{"docstring": "\"\"\"\nTaken from http://docs.python.org/lib/itertools-functions.html\n\"\"\"\n", "func_signal": "def groupby(iterable, keyfunc=None):\n", "code": "if keyfunc is None:\n    keyfunc = lambda x:x\niterable = iter(iterable)\nl = [iterable.next()]\nlastkey = keyfunc(l[0])\nfor item in iterable:\n    key = keyfunc(item)\n    if key != lastkey:\n        yield lastkey, l\n        lastkey = key\n        l = [item]\n    else:\n        l.append(item)\nyield lastkey, l", "path": "utils\\itercompat.py", "repo_name": "myles/django", "stars": 1, "license": "None", "language": "python", "size": 5868}
{"docstring": "\"\"\"\nChanges the aliases in change_map (which maps old-alias -> new-alias),\nrelabelling any references to them in select columns and the where\nclause.\n\"\"\"\n", "func_signal": "def change_aliases(self, change_map):\n", "code": "assert set(change_map.keys()).intersection(set(change_map.values())) == set()\n\n# 1. Update references in \"select\" and \"where\".\nself.where.relabel_aliases(change_map)\nfor pos, col in enumerate(self.select):\n    if isinstance(col, (list, tuple)):\n        old_alias = col[0]\n        self.select[pos] = (change_map.get(old_alias, old_alias), col[1])\n    else:\n        col.relabel_aliases(change_map)\n\n# 2. Rename the alias in the internal table/alias datastructures.\nfor old_alias, new_alias in change_map.iteritems():\n    alias_data = list(self.alias_map[old_alias])\n    alias_data[RHS_ALIAS] = new_alias\n\n    t = self.rev_join_map[old_alias]\n    data = list(self.join_map[t])\n    data[data.index(old_alias)] = new_alias\n    self.join_map[t] = tuple(data)\n    self.rev_join_map[new_alias] = t\n    del self.rev_join_map[old_alias]\n    self.alias_refcount[new_alias] = self.alias_refcount[old_alias]\n    del self.alias_refcount[old_alias]\n    self.alias_map[new_alias] = tuple(alias_data)\n    del self.alias_map[old_alias]\n\n    table_aliases = self.table_map[alias_data[TABLE_NAME]]\n    for pos, alias in enumerate(table_aliases):\n        if alias == old_alias:\n            table_aliases[pos] = new_alias\n            break\n    for pos, alias in enumerate(self.tables):\n        if alias == old_alias:\n            self.tables[pos] = new_alias\n            break\n\n# 3. Update any joins that refer to the old alias.\nfor alias, data in self.alias_map.iteritems():\n    lhs = data[LHS_ALIAS]\n    if lhs in change_map:\n        data = list(data)\n        data[LHS_ALIAS] = change_map[lhs]\n        self.alias_map[alias] = tuple(data)", "path": "db\\models\\sql\\query.py", "repo_name": "myles/django", "stars": 1, "license": "None", "language": "python", "size": 5868}
{"docstring": "\"\"\"\nAdds items from the 'ordering' sequence to the query's \"order by\"\nclause. These items are either field names (not column names) --\npossibly with a direction prefix ('-' or '?') -- or ordinals,\ncorresponding to column positions in the 'select' list.\n\nIf 'ordering' is empty, all ordering is cleared from the query.\n\"\"\"\n", "func_signal": "def add_ordering(self, *ordering):\n", "code": "errors = []\nfor item in ordering:\n    if not ORDER_PATTERN.match(item):\n        errors.append(item)\nif errors:\n    raise FieldError('Invalid order_by arguments: %s' % errors)\nif ordering:\n    self.order_by.extend(ordering)\nelse:\n    self.default_ordering = False", "path": "db\\models\\sql\\query.py", "repo_name": "myles/django", "stars": 1, "license": "None", "language": "python", "size": 5868}
{"docstring": "\"\"\" Returns a Short/Abbreviated Algebraic Notation string of a move \n    The board should be prior to the move \"\"\"\n\n# Has to be importet at calltime, as lmovegen imports lmove\n", "func_signal": "def toSAN (board, move, localRepr=False):\n", "code": "from lmovegen import genAllMoves\n\nflag = move >> 12\n\nif flag == KING_CASTLE:\n    return \"O-O\"\nelif flag == QUEEN_CASTLE:\n    return \"O-O-O\"\n\nfcord = (move >> 6) & 63\ntcord = move & 63\n\nfpiece = board.arBoard[fcord]\ntpiece = board.arBoard[tcord]\n\npart0 = \"\"\npart1 = \"\"\n\nif fpiece != PAWN:\n    if localRepr:\n        part0 += localReprSign[fpiece]\n    else:\n        part0 += reprSign[fpiece]\n\npart1 = reprCord[tcord]\n\nif not fpiece in (PAWN, KING):\n    xs = []\n    ys = []\n    \n    board.lock.acquire()\n    try:\n        for altmove in genAllMoves(board):\n            mfcord = FCORD(altmove)\n            if board.arBoard[mfcord] == fpiece and \\\n                    mfcord != fcord and \\\n                    TCORD(altmove) == tcord:\n                board.applyMove(altmove)\n                if not board.opIsChecked():\n                    xs.append(FILE(mfcord))\n                    ys.append(RANK(mfcord))\n                board.popMove()\n    finally:\n        board.lock.release()\n    \n    x = FILE(fcord)\n    y = RANK(fcord)\n    \n    if ys or xs:\n        if y in ys and not x in xs:\n            # If we share rank with another piece, but not file\n            part0 += reprFile[x]\n        elif x in xs and not y in ys:\n            # If we share file with another piece, but not rank\n            part0 += reprRank[y]\n        elif x in xs and y in ys:\n            # If we share both file and rank with other pieces\n            part0 += reprFile[x] + reprRank[y]\n        else:\n            # If we doesn't share anything, it is standard to put file\n            part0 += reprFile[x]\n\nif tpiece != EMPTY or flag == ENPASSANT:\n    part1 = \"x\" + part1\n    if fpiece == PAWN:\n        part0 += reprFile[FILE(fcord)]\n\nnotat = part0 + part1\nif flag in PROMOTIONS:\n    if localRepr:\n        notat += \"=\"+localReprSign[PROMOTE_PIECE(flag)]\n    else:\n        notat += \"=\"+reprSign[PROMOTE_PIECE(flag)]\n\nboard.lock.acquire()\ntry:\n    board.applyMove(move)\n    if board.isChecked():\n        for altmove in genAllMoves (board):\n            board.applyMove(altmove)\n            if board.opIsChecked():\n                board.popMove()\n                continue\n            board.popMove()\n            notat += \"+\"\n            break\n        else:\n            notat += \"#\"\n    board.popMove()\nfinally:\n    board.lock.release()\n\nreturn notat", "path": "icsbot\\thirdparty\\pychess\\Utils\\lutils\\lmove.py", "repo_name": "xaosfiftytwo/veinsbot", "stars": 0, "license": "None", "language": "python", "size": 448}
{"docstring": "\"\"\"Use always_trigger_ending to get an game ending event even if bot\ndid not see game start. Note then in this case _variant_, start_time\nand rated are set to None (we cannot get the info directly).\n\"\"\"\n", "func_signal": "def __init__(self, icsbot, always_trigger_ending=True, get_games=True):\n", "code": "self._ate = always_trigger_ending\nreg_start = re.compile('^\\{Game (?P<game>\\d+) \\((?P<white>[a-zA-Z]{3,17}) vs. (?P<black>[a-zA-Z]{3,17})\\) (Creating|Continuing) (?P<rated>[^ ]*) (?P<variant>[^ ]*) match.\\}$')\nreg_end   = re.compile('^\\{Game (?P<game>\\d+) \\((?P<white>[a-zA-Z]{3,17}) vs. (?P<black>[a-zA-Z]{3,17})\\) (?P<longresult>.*)\\} (?P<result>.*)$')\n\nself._icsbot = icsbot\n\nself._icsbot.send('set gin 1')\nself._icsbot.send('iset allresults 1')\n\n# We will use games to save a set of for all objects, keeping them\n# in memory.\nself._games = set()\n\nself._sgames = self._icsbot['sgames', 'game', 0]\n\nself.status = self._sgames['__status__']\nself.status['got_all'] = False\n\nself._icsbot.reg_comm(reg_start, self._start)\nself._icsbot.reg_comm(reg_end, self._end)\nif get_games:\n    self._icsbot.execute('games', self._grab_games)", "path": "icsbot\\parser\\gamelist.py", "repo_name": "xaosfiftytwo/veinsbot", "stars": 0, "license": "None", "language": "python", "size": 448}
{"docstring": "\"\"\"Pending\n======\nThis command allows you to view pending games of yourself or a user.\nAlso you can use -t tourney_name if you want all games from that tourney\n\nEXAMPLES:\n    o pending -> your pending games\n    o pending seberg -> seberg's pending games\n    o pending -t test -> pending games for tourney test \n\"\"\"\n", "func_signal": "def pending_command(self, usr, args, tags):\n", "code": "args = parse(args)\n\nif args[1].has_key('t') and len(args[1]['t']) > 0:\n    tourn = args[1]['t'][0]\nelse:\n    tourn = None\n\nif args[0]:\n    user = self.icsbot['users'][args[0][0]]\n    if not user['rowid']:\n        return self.icsbot.qtell.split(usr, 'The user %s is not registered with me.' % user['handle'])\nelse:\n    if not usr['rowid'] and not tourn:\n        return self.icsbot.qtell.split(usr, 'As you are not registered with me, you must give a handle or a tourney.')\n    user = usr\n\nif not tourn:\n    self.games.sql.dcursor.execute('SELECT rowid FROM games WHERE (white_id=? or black_id=?) and (result=\"-\" or result=\"*\" or result=\"ADJ\")', (user['rowid'],user['rowid']))\n    gs = self.games.sql.dcursor.fetchall()\n    if not gs:\n        return self.icsbot.qtell.split(usr, '%s has no unifinished games.' % user['handle'])\n    else:\n        to_send = ['Unfinished games of %s:' % user['handle']]\n\nelse:\n    self.games.sql.dcursor.execute('SELECT rowid, * FROM tourneys WHERE name=?', (tourn,))\n    tourney = self.games.sql.dcursor.fetchall()\n    if len(tourney) == 0:\n        return self.icsbot.qtell.split(usr, 'The tourney \"%s\" does not exist.' % tourn)\n    elif len(tourney) > 1:\n        return self.icsbot.qtell.split(usr, 'More then one tourney matches. This _should_ be impossible.')\n\n    tourney = tourney[0]\n    self.games.sql.dcursor.execute('SELECT rowid FROM games WHERE tourney_id=? and (result=\"-\" or result=\"*\")', (tourney['rowid'],))\n    gs = self.games.sql.dcursor.fetchall()\n    if not gs:\n        return self.icsbot.qtell.split(usr, 'There are no unifinished games in tourney %s.' % tourney['name'])\n    else:\n        to_send = ['Unfinished in tourney %s:' % tourney['name']]\n  \nto_send.append('+---------+-------------------+-------------------+-------+---------+')\nto_send.append('| Tourney |             white | black             | Round |  Result |')\nto_send.append('+---------+-------------------+-------------------+-------+---------+')\nfor g in gs:\n    # Yes, this does do some more SQLs then necessary :).\n    g = self.games[g['rowid']]\n    to_send.append('| %7s | %17s | %-17s | %5s | %s |' % (g['tourney'], g['white'], g['black'], g['round'], g['result'].center(7)))\nto_send.append('+---------+-------------------+-------------------+-------+---------+')\nreturn self.icsbot.qtell.send_list(usr, to_send)", "path": "games.py", "repo_name": "xaosfiftytwo/veinsbot", "stars": 0, "license": "None", "language": "python", "size": 448}
{"docstring": "\"\"\" Returns a Algebraic Notation string of a move\n    board should be prior to the move \"\"\"\n\n", "func_signal": "def toAN (board, move):\n", "code": "s = reprCord[FCORD(move)] + reprCord[TCORD(move)]\nif FLAG(move) in PROMOTIONS:\n    s += reprSign[PROMOTE_PIECE(FLAG(move))]\nreturn s", "path": "icsbot\\thirdparty\\pychess\\Utils\\lutils\\lmove.py", "repo_name": "xaosfiftytwo/veinsbot", "stars": 0, "license": "None", "language": "python", "size": 448}
{"docstring": "\"\"\"Reload\n======\nThis command will reload all games data in the bot, making sure that\nthere is no bogus data.\n\"\"\"\n# Lets recreate the list of games that should be ending:\n", "func_signal": "def reload_games(self, usr, args, tags):\n", "code": "self.games.sql.cursor.execute('SELECT rowid FROM games WHERE result=\"*\" or result=\"ADJ\"')\nto_end = self.games.sql.cursor.fetchall()\nself.games.to_end = set()\nfor spam in to_end:\n    self.games.to_end.add(games[spam[0]])\n\n# Now, lets force a reload on all stored items:\nfor g in self.games.itervalues():\n    g.load('rowid')\n\n# At this point, the only thing that still can be bogus is the list of\n# games to start. Lets just check if there is none there that should\n# be finished already:\nfor g_s in self.games.to_start.copy():\n    if g_s['result'] != '*' or g_s['result'] != '-' or g_s['result'] != 'ADJ':\n        self.games.to_start.remove(g_s)", "path": "games.py", "repo_name": "xaosfiftytwo/veinsbot", "stars": 0, "license": "None", "language": "python", "size": 448}
{"docstring": "\"\"\"Grab a games move/result when it was not grabbed automatically.\nThis command is not finished at the moment, as it will not set the\nresult to * itself, and can thus only print games that were seen\nstarting. The argument must be a correct input for FICS smoves command.\n\nEXAMPLE:\n    o grab 123 seberg -1\n\"\"\"\n", "func_signal": "def grab(self, user, args, tags):\n", "code": "try: game_id, args = args.split(None, 1)\nexcept: return self.icsbot.qtell.split(user, 'USAGE: grab game_id <FICS game identifier for smoves command>, see help.')\ntry: game_id = int(game_id)\nexcept: return self.icsbot.qtell.split(user, 'USAGE: grab game_id <FICS game identifier for smoves command>, game_id must be an Integer.')\n\nif not self.games[game_id]:\n    return self.icsbot.qtell.split(user, 'Invalid game identification number, please check the database.')\n\ngame = self.games[game_id]\n\n# reload, in case of change in the database.\ngame.load('rowid')\nif game not in self.games.to_end:\n    self.games.to_end.add('game')\n\nmoves = self.icsbot['moves']['smoves %s -1' % game['white']]\nmoves['__games_should_be'] = game\nmoves['__games_try'] = None\nmoves['__games_initiator'] = user\nmoves.register('loaded', self.store_info)\nmoves.load('loaded')\nreturn self.icsbot.qtell.split(user, 'Grabbing game from FICS command \"smoves %s\" if possible.' % args)", "path": "games.py", "repo_name": "xaosfiftytwo/veinsbot", "stars": 0, "license": "None", "language": "python", "size": 448}
{"docstring": "\"\"\"play [tourney] [-o opponent] [-r round]\nAsk the bot to issue a match command for you and start looking for the\ngame to start. This commands assumes the first game that fits to be\nthe right game.\nEXAMPLES:\n    o play\n    o play test\n    o play -o seberg\n    o play test -o seberg -r 2\n-t and -r can be given in any order at any point.        \n\"\"\"\n", "func_signal": "def play(self, user, args, tags):\n", "code": "args = parse(args)\nif args[0]:\n    tourney = args[0][0]\nelse:\n    tourney = None\nif args[1].has_key('o') and len(args[1]['o']) > 0:\n    opp_handle = args[1]['o'][0].lower()\nelse:\n    opp_handle = None\nif args[1].has_key('r') and len(args[1]['r']) > 0:\n    try: round_ = int(args[1]['r'][0])\n    except: return self.icsbot.qtell.split(usr, 'Round must be an integer.')\nelse:\n    round_ = None\n\nhandle_n = user['handle']\nhandle = handle_n.lower()\nfor g_p in self.pending():\n    if g_p['white'].lower() == handle or g_p['black'].lower() == handle:\n        if tourney and tourney.lower() != g_p['tourney'].lower():\n            continue\n        \n        if round_ is not None and round_ != g_p['round']:\n            continue\n        \n        if opp_handle is not None and (g_p['white'].lower() != opp_handle and g_p['black'].lower() != opp_handle):\n            continue\n        \n        if g_p['black'].lower() == handle:\n            if not self.icsbot['users'][g_p['white']]['online']:\n                return self.icsbot.qtell.split(user, 'Your opponent %s for the game in the tourney \"%s\" appears not to be online.' % (g_p['white'], g_p['tourney']))\n            l = [g_p['black'], g_p['white'], g_p['controls'], 'black']\n        else:\n            if not self.icsbot['users'][g_p['black']]['online']:\n                return self.icsbot.qtell.split(user, 'Your opponent %s for the game in the tourney \"%s\" appears not to be online.' % (g_p['black'], g_p['tourney']))\n            l = [g_p['white'], g_p['black'], g_p['controls'], 'white']\n\n        if opp_handle and opp_handle.lower() != l[1].lower():\n            continue\n        \n        if not ' r' in l[2] or ' u' in l[2]:\n            l[2] = l[2] + ' r'\n        \n        if self.no_td:\n            self.icsbot.send('tell %s Please \"match %s %s %s\"' % tuple(l))\n        else:\n            self.icsbot.send('rmatch %s %s %s %s' % tuple(l))\n        opponent = self.icsbot['users'][l[1]]\n        self.icsbot.send(self.icsbot.qtell.split(opponent, 'You have recieved a match request for your game in tourney %s against %s, please accept or decline it.' % (g_p['tourney'], handle_n)))\n        self.icsbot.send(self.icsbot.qtell.split(user, 'A match request for your game in tourney %s against %s has been sent. If this is the wrong game, please withdraw the match request and use: play tourney_name or play -o opponent' % (g_p['tourney'], handle_n)))     \n        self.games.to_start.add(g_p)\n        return\nreturn self.icsbot.qtell.split(user, 'I have not found a game to start for you.')", "path": "games.py", "repo_name": "xaosfiftytwo/veinsbot", "stars": 0, "license": "None", "language": "python", "size": 448}
{"docstring": "\"\"\" Parse a Long/Expanded Algebraic Notation string \"\"\"\n\n# To parse LAN pawn moves like \"e2-e4\" as SAN moves, we have to remove a few\n# fields\n", "func_signal": "def parseLAN (board, lan):\n", "code": "if len(lan) == 5:\n    if \"x\" in lan:\n        # e4xd5 -> exd5\n        return parseSAN (board, lan[0]+lan[3:])\n    else:\n        # e2-e4 -> e4\n        return parseSAN (board, lan[3:])\n\n# We want to use the SAN parser for LAN moves like \"Nb1-c3\" or \"Rd3xd7\"\n# The san parser should be able to handle most stuff, as long as we remove\n# the slash\nif not lan.upper().startswith(\"O-O\"):\n    lan = lan.replace(\"-\",\"\")\nreturn parseSAN (board, lan)", "path": "icsbot\\thirdparty\\pychess\\Utils\\lutils\\lmove.py", "repo_name": "xaosfiftytwo/veinsbot", "stars": 0, "license": "None", "language": "python", "size": 448}
{"docstring": "# Dummy check in case I change internal API of the _data.Item.\n", "func_signal": "def _item_changed(self, data_set, item, old, new):\n", "code": "if old == new:\n    return\n\nif item in self.joined_values:\n    TypeError('The item is not a native Item for the database, and thus cannot be changed.')\n\nif item == self.main_key:\n    if self.lower:\n        if old.lower() != new.lower():\n            TypeError('The main key of a table may not be changed.')\n    else:\n        TypeError('The main key of a table may not be changed.')\n\n# This is some fun. If someone changes something that the database should\n# be storing, we assume the item should be added.\n# THIS CAN DO SERIOUS CRAP IF YOU ARE NOT CAREFUL, ie in status\n# there was user['handle'] = handle. Creating an event and storing here.\nif not self.lower:\n    self.cursor.execute('SELECT count(*) FROM %s WHERE %s.%s=?' % (self.table_joins, self.table, self.main_key), (data_set[self.main_key],))\nelse:\n    self.cursor.execute('SELECT count(*) FROM %s WHERE %s.%s=?' % (self.table_joins, self.table, self.main_key_lower), (data_set[self.main_key].lower(),))\nc = self.cursor.fetchone()\n\nif c[0] == 0:\n    if not self.lower:\n        self.cursor.execute('INSERT INTO %s (%s, %s) VALUES (?, ?)' % (self.table, self.main_key, item), (data_set[self.main_key], new))\n    else:\n        self.cursor.execute('INSERT INTO %s (%s, %s, %s) VALUES (?, ?, ?)' % (self.table, self.main_key_lower, self.main_key, item), (data_set[self.main_key].lower(), data_set[self.main_key], new))\n    # we need to reload rowid, etc. becuase we got now default values.\n    self._load_all(data_set)\n    return False\nelif c[0] != 1:\n    assert 'More then one match in database, this should be impossible.'\n\nif not self.lower:\n    self.cursor.execute('UPDATE %s SET %s=? WHERE %s=?' % (self.table, item, self.main_key), (new, data_set[self.main_key]))\nelse:\n    self.cursor.execute('UPDATE %s SET %s=? WHERE %s=?' % (self.table, item, self.main_key_lower), (new, data_set[self.main_key].lower()))\n# We reload all the info, just in case.\nself._load_all(data_set)", "path": "icsbot\\misc\\sqldata.py", "repo_name": "xaosfiftytwo/veinsbot", "stars": 0, "license": "None", "language": "python", "size": 448}
{"docstring": "\"\"\" Inits a new highlevel Move object.\n    The object can be initialized in the follow ways:\n        Move(cord0, cord1, board, [promotionPiece])\n        Move(lovLevelMoveInt) \"\"\"\n\n", "func_signal": "def __init__ (self, cord0, cord1=None, board=None, promotion=None):\n", "code": "if not cord1:\n    self.move = cord0\n    self.flag = self.move >> 12\n    if self.flag in PROMOTIONS:\n        self.promotion = lmove.PROMOTE_PIECE (self.move)\n    else: self.promotion = QUEEN\n    self.cord0 = Cord(lmove.FCORD(self.move))\n    self.cord1 = Cord(lmove.TCORD(self.move))\n    \nelse:\n    self.cord0 = cord0\n    self.cord1 = cord1\n    \n    self.flag = NORMAL_MOVE\n    \n    if board[self.cord0].piece == PAWN and  self.cord1.y in (0,7):\n        if promotion == None: promotion = QUEEN\n        self.flag = FLAG_PIECE(promotion)\n    \n    elif board[self.cord0].piece == KING:\n        if self.cord0.x - self.cord1.x == 2:\n            self.flag = QUEEN_CASTLE\n        elif self.cord0.x - self.cord1.x == -2:\n            self.flag = KING_CASTLE\n    \n    elif board[self.cord0].piece == PAWN and \\\n            board[self.cord1] == None and \\\n            self.cord0.x != self.cord1.x and \\\n            self.cord0.y != self.cord1.y:\n        self.flag = ENPASSANT\n    \n    self.move = lmove.newMove(self.cord0.cord, self.cord1.cord, self.flag)", "path": "icsbot\\thirdparty\\pychess\\Utils\\Move.py", "repo_name": "xaosfiftytwo/veinsbot", "stars": 0, "license": "None", "language": "python", "size": 448}
{"docstring": "\"\"\"Run function to create the necessary games things.\n\"\"\"\n", "func_signal": "def __init__(self, icsbot, database, announce_channel=None, announce_times=(00, 15, 30, 45), no_td=False):\n", "code": "from icsbot.misc import sqldata\n\nself.icsbot = icsbot\nself.no_td = no_td\n\ngames = icsbot['games', 'rowid']\n\nsqldata.SqlData(database=database, dataset=games, table='games', joins='LEFT JOIN users as white ON white.rowid=white_id LEFT JOIN users as black ON black.rowid=black_id LEFT JOIN tourneys ON tourneys.rowid=tourney_id', columns='start_time, wrating, brating, games.round as round, result, longresult, moves, times, controls, white_id, black_id, tourney_id', join_columns='white.handle as white, black.handle as black, tourneys.name as tourney, tourneys.description as tourney_description, controls')\n\nself.games = games\n\ngames.sql.cursor.execute('SELECT rowid FROM games WHERE result=\"*\" or result=\"ADJ\"')\nto_end = games.sql.cursor.fetchall()\ngames.to_end = set()\nfor spam in to_end:\n    games.to_end.add(games[spam[0]])\n\nself.games.to_start = set()\n\n# A set of games we need to store info on. This is just privateself.\nself.to_store = set()\n\ngames.register('result', self.result_change)\nicsbot['sgames'].register('start_time', self.game_start)\nicsbot['sgames'].register('update_time', self.game_might_run)\nicsbot['sgames'].register('end_time', self.game_end)\n\nicsbot['sgames'].register('got_all', self.all_games_were_gotten)\n\n# We will do this on the seperate item in the case the bot gets bigger\n# and uses it in other ways. That way we will not get as much overhead.\n#icsbot['moves'].register('loaded', self.store_info)\n\nicsbot.reg_tell('play', self.play, lambda usr, tags: usr['rowid'])\nicsbot.reg_tell('grab', self.grab, lambda usr, tags: usr['admin'])\nicsbot.reg_tell('reload', self.reload_games, lambda usr, tags: usr['admin'])\nicsbot.reg_tell('pending', self.pending_command)        \nicsbot.reg_tell('games', self.list_games)\n\nself.announce_times = list(announce_times)\nself.announce_times.sort()\nself.announce_channel = announce_channel\n\n# Careful using return here, everything below this would not work.\nl = list(time.localtime())\nl[5:] = [0]*4\nfor i, m in enumerate(announce_times):\n    if m > l[4]:\n        l[4] = m\n        self.icsbot.timer(time.mktime(l), self.announce, i)\n        return\n        \nl[4] = announce_times[0]\nself.icsbot.timer(time.mktime(l), self.announce, 0)", "path": "games.py", "repo_name": "xaosfiftytwo/veinsbot", "stars": 0, "license": "None", "language": "python", "size": 448}
{"docstring": "# Remove the game if needed:\n", "func_signal": "def result_change(self, game, item, old, new):\n", "code": "if old == '-':\n    self.games.to_start.remove(game)\nelif old == 'ADJ' or old == '*':\n    self.games.to_end.remove(game)\n    \nif new == '-':\n    game['gamenumber'] = None\nelif new == 'ADJ':\n    game['gamenumber'] = None\n    self.games.to_end.add(game)\nelif new == '*':\n    self.games.to_end.add(game)", "path": "games.py", "repo_name": "xaosfiftytwo/veinsbot", "stars": 0, "license": "None", "language": "python", "size": 448}
{"docstring": "\"\"\" Returns a Long/Expanded Algebraic Notation string of a move\n    board should be prior to the move \"\"\"\n\n", "func_signal": "def toLAN (board, move):\n", "code": "fcord = FCORD(move)\ntcord = TCORD(move)\n\ns = \"\"\nif board.arBoard[fcord] != PAWN:\n    s = reprSign[board.arBoard[fcord]]\ns += reprCord[FCORD(move)]\n\nif board.arBoard[tcord] == EMPTY:\n    s += \"-\"\nelse: s += \"x\"\n\ns += reprCord[tcord]\n\nflag = FLAG(move)\n\nif flag in PROMOTIONS:\n    s += \"=\" + reprSign[PROMOTE_PIECE(flag)]\n\nreturn s", "path": "icsbot\\thirdparty\\pychess\\Utils\\lutils\\lmove.py", "repo_name": "xaosfiftytwo/veinsbot", "stars": 0, "license": "None", "language": "python", "size": 448}
{"docstring": "\"\"\" Returns a Figurine Algebraic Notation string of a move \"\"\"\n\n", "func_signal": "def toFAN (board, move):\n", "code": "lan = toSAN (board, move)\n\nif not lan[0] in (\"K\", \"Q\", \"R\", \"B\", \"N\", \"O\"):\n    \n    # We generally don't want notations like Pexd4, as the from file - in\n    # this case 'e' could be cut out. However in some cases where two pawns\n    # can attack the same cord, we'll still need it.\n    i = lan.find(\"x\")\n    if i >= 0:\n        fcord = FCORD(move)\n        tcord = TCORD(move)\n        fileNecessary = False\n        \n        from lmovegen import genAllMoves\n        board.lock.acquire()\n        try:\n            for altmove in genAllMoves(board):\n                mfcord = FCORD(altmove)\n                if board.arBoard[mfcord] == PAWN and \\\n                        mfcord != fcord and \\\n                        TCORD(altmove) == tcord:\n                    board.applyMove(altmove)\n                    if not board.opIsChecked():\n                        fileNecessary = True\n                    board.popMove()\n                    # If we found a pawn, that is not us, which can move to our\n                    # tcord, there is no point in looking further, as we can\n                    # never have more than two pawn pointing at the same cord\n                    break\n        finally:\n            board.lock.release()\n        \n        if not fileNecessary:\n            lan = lan[i:]\n    \n    # We add the pawn sign by appending instead of replacing, as SAN\n    # notation has no equal\n    if board.color == WHITE:\n        lan = FAN_PIECES[WHITE][PAWN] + lan\n    else: lan = FAN_PIECES[BLACK][PAWN] + lan\n\nif board.color == WHITE:\n    lan = san2WhiteFanRegex.sub(san2WhiteFanFunc, lan)\nelse: lan = san2BlackFanRegex.sub(san2BlackFanFunc, lan)\n\nreturn lan", "path": "icsbot\\thirdparty\\pychess\\Utils\\lutils\\lmove.py", "repo_name": "xaosfiftytwo/veinsbot", "stars": 0, "license": "None", "language": "python", "size": 448}
{"docstring": "# We could save this loop here by registering with the game only, but\n# I don't feel like changing this right now. There is no performance\n# trouble anyways.\n", "func_signal": "def game_end(self, game, item, old, new):\n", "code": "for g_e in self.games.to_end:\n    if g_e['white'].lower() == game['white'].lower() and g_e['black'].lower() == game['black'].lower():\n        # We do not yet store the information. We will get the information with moves with the\n        # moves command first. We store the result only there.\n        result = game['result']\n        if result == '*':\n            if 'adjourned' in game['longresult']:\n                result = 'ADJ'\n                g_e['result'] = 'ADJ'\n            else:\n                result = '-'\n                g_e['result'] = '-'\n                \n        # We put it here because we want it to also work on aborts, as well as not on just game sets.\n        self.icsbot.send(['tell %s The game end was noted.' % g_e['white'], 'tell %s The game end was noted.' % g_e['black']])\n        if self.announce_channel is not None:\n            self.icsbot.send('tell %s Game: %s vs. %s in tourney %s round %s ended as %s' % (self.announce_channel, g_e['white'], g_e['black'], g_e['tourney'], g_e['round'], result))\n        \n        # No need to grab something then:\n        if g_e['result'] == '-' or g_e['result'] == 'ADJ':\n            return\n\n        moves = self.icsbot['moves']['smoves %s -1' % game['white']]\n        moves['__games_should_be'] = g_e\n        moves['__games_try'] = ('w', -1)\n        moves.register('loaded', self.store_info)\n        moves.load('loaded')\n        return", "path": "games.py", "repo_name": "xaosfiftytwo/veinsbot", "stars": 0, "license": "None", "language": "python", "size": 448}
{"docstring": "\"\"\"Iterator over all pending games. Fetches them from the database.\n\"\"\"\n", "func_signal": "def pending(self):\n", "code": "self.games.sql.cursor.execute('SELECT rowid FROM games WHERE result=\"-\" or result=\"ADJ\" or result=\"*\"')\npending = self.games.sql.cursor.fetchall()\nfor spam in pending:\n    yield self.games[spam[0]]", "path": "games.py", "repo_name": "xaosfiftytwo/veinsbot", "stars": 0, "license": "None", "language": "python", "size": 448}
{"docstring": "\"\"\"Initialize a class giving it a database='icsbot.db', the dataset\n(an icsbot[dataset] object.) the table=tablename, which should\nbe the datasets name for simplicity.\nThe class will make sure that all values gotten are gotten from the\ndb, and all values changed, are stored to the db.\n\n'rowid' is the always the row of the database entry. To get all\nentries loaded at once, load 'rowid' and it will happen.\n\nKWARGS:\n    database = Database to use. Either string, or open connection.\n    dataset  = dataset to work with. (IcsBot[dataset] dataset.)\n    table    = name of the table.\n    load_all = If we should always load all columns. (default: True\n                 Other not tested.)\n\nAlso defines icsbot[dataset].sql to be this connection class making\nicsbot[dataset].sql.cursor and icsbot[dataset].sql.cursor.dcursor\navailable to you. (dcursor is a dictionary cursor)\n\nNOTE: \n    o If there is a column named <main_key>_lower this is used for\n        comparisons (or some other lower_ext)\n    o You only use this for simple things like strings or numbers\n        stored on the user.\n    o The column/item rowid is reserved.\n    o Load all ONLY makes a rowid event, others won't get events!!!\n    o I will automatically get all the necessary items from the table.\n    o The class will never create a new entry in the database when\n        something is set to a _new_ value.\n    o You can define join_columns and joins. Joins will be inserted\n        into the SQL and should be \"LEFT JOIN ...\", join_columns should\n        be all the columns you want to extract from joined tables.\n        These are read only! If you give joins, you must also\n        give columns.\n\"\"\"\n\n", "func_signal": "def __init__(self, database=None, dataset=None, table=None, joins='', columns='*', join_columns='', lower_ext='_lower', load_all=True):\n", "code": "assert dataset, 'No dataset=icsbot dataset item given.'\nassert table, 'Must give table=tablename string.'\nif joins != '' and (columns == '*' or join_columns==''):\n        assert False, 'If joins are given, columns and join_columns must be specified.'\n\nif join_columns:\n    self.all_columns = columns + ', ' + join_columns\nelse:\n    self.all_columns = columns\nself.columns = columns\nself.join_columns = join_columns\n\nself.table = table\nself.table_joins = table + ' ' + joins\n\nself.dataset = dataset\nself.main_key = dataset.main_key\n\nif type(database) is str or type(database) is unicode: \n    self.db = db.connect(database)\nelse:\n    self.db = database\n\n# Setting autocommit. No idea if this is actually good or not :)\nself.db.isolation_level = None\n\nself.cursor = self.db.cursor()\n\nstored = self.cursor.execute('SELECT %s.rowid as rowid, %s FROM %s LIMIT 0' % (self.table, self.all_columns, self.table_joins))\nself.store_values = set([col[0] for col in stored.description])\n\nif self.main_key + lower_ext in self.store_values:\n    self.main_key_lower = self.main_key + lower_ext\n    self.store_values.remove(self.main_key_lower)\n    self.lower = True\nelse:\n    self.lower = False\n\nif self.join_columns:\n    stored = self.cursor.execute('SELECT %s FROM %s LIMIT 0' % (self.join_columns, self.table_joins))\n    self.joined_values = [col[0] for col in stored.description]\nelse:\n    self.joined_values = []\n\n# Snipplet from the pysqlite user guide so I can have a dictionary\n# cursor:\ndef dict_factory(cursor, row):\n    d = {}\n    for idx, col in enumerate(cursor.description):\n        d[col[0]] = row[idx]\n    return d\n    \nself.db.row_factory = dict_factory\nself.dcursor = self.db.cursor()\n\n# Register with the values:\nif not load_all:\n    for value in self.store_values:\n        self.dataset.register(value, self._item_changed)\n        self.dataset.register(value, self._load_item, loader=True)\nelse:\n    for value in self.store_values:\n        self.dataset.register(value, self._item_changed)\n        self.dataset.register(value, self._load_all, loader=True)        \n\nself.store_values.remove(self.main_key)\nself.dataset.register('rowid', self._load_all, loader=True)\nself.dataset.sql = self", "path": "icsbot\\misc\\sqldata.py", "repo_name": "xaosfiftytwo/veinsbot", "stars": 0, "license": "None", "language": "python", "size": 448}
{"docstring": "# copied from list_games:\n", "func_signal": "def announce(self, time_idx):\n", "code": "g_r = []\nfor g in self.games.to_end:\n    if g['gamenumber']:\n        # We sort them by gamenumber for lack of something better:\n        g_r.append(g)\n\nif len(g_r) == 0:\n    return\ng_r.sort(lambda x, y: x['gamenumber']<y['gamenumber'])\n\nfor g in g_r:\n    self.icsbot.send('tell %s Game: %s vs. %s in the %s round %s running. \"observe %s\"' % (self.announce_channel, g['white'], g['black'], g['tourney'], g['round'], g['gamenumber']))\n\nnext = (time_idx + 1) % len(self.announce_times)\nl = list(time.localtime())\nif l[4] < self.announce_times[next]:\n    l[4] = self.announce_times[next]\nelse:\n    l[4] = self.announce_times[next] + 60\n\nl[5:] = [0]*4\nself.icsbot.timer(time.mktime(l), self.announce, next)", "path": "games.py", "repo_name": "xaosfiftytwo/veinsbot", "stars": 0, "license": "None", "language": "python", "size": 448}
{"docstring": "# Just for added features:\n", "func_signal": "def game_start(self, game, *args, **kwargs):\n", "code": "if kwargs.has_key('already_running'):\n    already_running = kwargs['already_running']\nelse:\n    already_running = False\n    \nto_remove = set()\nfor g_s in self.games.to_start:                                 \n    if g_s['white'].lower() == game['white'].lower() and g_s['black'].lower() == game['black'].lower():\n        # Store the gamenumber (before the result is set, so that\n        # an even for the result set to * can access it):\n        g_s['gamenumber'] = game['game']\n        \n        g_s['result'] = '*' # There is some magic here if the result\n                            # was not already * because result_change\n                            # gets executed.\n        self.icsbot.send(['tell %s Good luck in your game, the start has been noted.' % game['white'], 'tell %s Good luck in your game, the start has been noted.' % game['black']])\n        if self.announce_channel is not None:\n            self.icsbot.send('tell %s Game started: %s vs. %s in %s, round %s. \"observe %s\"' % (self.announce_channel, g_s['white'], g_s['black'], g_s['tourney'], g_s['round'], g_s['gamenumber']))\n        return\n    \n    # There is a chance, that we are waiting for it to start, but the players won't start at all:\n    elif g_s['white'].lower() == game['white'].lower() and g_s['black'].lower() == game['white'].lower()\\\n       or g_s['white'].lower() == game['black'].lower() and g_s['black'].lower() == game['black'].lower():\n        to_remove.add(g_s)\n    \n    for g in to_remove:\n        self.games.to_start.remove(g)\n    \n    \n# Now, if the game was not one that we wanted to see starting, there\n# is a chance we are waiting for it to end. This might be if it got\n# adjourned before, or if we lost connection ourselfes.\nfor g_e in self.games.to_end:\n    if g_e['gamenumber'] is not None:\n        continue\n    if g_e['white'].lower() == game['white'].lower() and g_e['black'].lower() == game['black'].lower():\n        # No announce, the game might have been running for a while.\n        g_e['gamenumber'] = game['game']\n        g_e['result'] = '*'\n        self.icsbot.send(['tell %s Good luck in your game, the start has been noted.' % game['white'], 'tell %s Good luck in your game, the start has been noted.' % game['black']])\n        if self.announce_channel is not None:\n            self.icsbot.send('tell %s Game started: %s vs. %s in tourney %s round %s. \"observe %s\"' % (self.announce_channel, g_e['white'], g_e['black'], g_e['tourney'], g_e['round'], g_e['gamenumber']))\n        return", "path": "games.py", "repo_name": "xaosfiftytwo/veinsbot", "stars": 0, "license": "None", "language": "python", "size": 448}
{"docstring": "\"\"\" Parse a Long/Expanded Algebraic Notation string \"\"\"\n\n", "func_signal": "def parseFAN (board, fan):\n", "code": "san = fan2SanRegex.sub(fan2SanFunc, fan)\n\npawnFan = FAN_PIECES[board.color][PAWN]\nif san[0] == pawnFan:\n    san = san.replace(pawnFan, \"\")\n    # If the pawn file has been omitted from a capture fan notation, it\n    # means that there was only one pawn able to move to the end cord. We\n    # just need to find it.\n    if san[0] == \"x\":\n        # We need to find the endcord ourselves. Can't wait for parseSAN\n        i = san.find(\"=\")\n        if i >= 0:\n            tocord = san[1:i]\n        else: tocord = san[1:]\n        tcord = cordDic[tocord]\n        \n        from lmovegen import genAllMoves\n        board.lock.acquire()\n        try:\n            for altmove in genAllMoves(board):\n                if board.arBoard[FCORD(altmove)] == PAWN and \\\n                        TCORD(altmove) == tcord:\n                    board.applyMove(altmove)\n                    if not board.opIsChecked():\n                        san = reprFile(mfcord) + san\n                    board.popMove()\n                    # We know there is only one pawn which can move to tcord, so\n                    # we stop work here\n                    break\n        finally:\n            board.lock.release()\n\nreturn parseSAN (board, san)", "path": "icsbot\\thirdparty\\pychess\\Utils\\lutils\\lmove.py", "repo_name": "xaosfiftytwo/veinsbot", "stars": 0, "license": "None", "language": "python", "size": 448}
{"docstring": "\"\"\"\nArgs:\n  section_name: name given as an argument to the section\n\"\"\"\n", "func_signal": "def __init__(self, section_name=None):\n", "code": "self.section_name = section_name\n\n# Pairs of func, args, or a literal string\nself.current_clause = []\nself.statements = {'default': self.current_clause}", "path": "src\\slingshot\\jsontemplate.py", "repo_name": "zaach/slingshot", "stars": 1, "license": "mit", "language": "python", "size": 96}
{"docstring": "\"\"\"\nThe user's formatters are consulted first, then the default formatters.\n\"\"\"\n", "func_signal": "def _GetFormatter(self, format_str):\n", "code": "formatter = (\n    self.more_formatters(format_str) or _DEFAULT_FORMATTERS.get(format_str))\n\nif formatter:\n  return formatter\nelse:\n  raise BadFormatter('%r is not a valid formatter' % format_str)", "path": "src\\slingshot\\jsontemplate.py", "repo_name": "zaach/slingshot", "stars": 1, "license": "mit", "language": "python", "size": 96}
{"docstring": "\"\"\"Helper function for matching certain directives.\"\"\"\n\n", "func_signal": "def _MatchDirective(token):\n", "code": "if token in ('.or', '.alternates with'):\n  return CLAUSE_TOKEN, token[1:]\n\nif token == '.end':\n  return END_TOKEN, None\n\nmatch = _SECTION_RE.match(token[1:])\nif match:\n  repeated, section_name = match.groups()\n  if repeated:\n    return REPEATED_SECTION_TOKEN, section_name\n  else:\n    return SECTION_TOKEN, section_name\n\nreturn None, None  # no match", "path": "src\\slingshot\\jsontemplate.py", "repo_name": "zaach/slingshot", "stars": 1, "license": "mit", "language": "python", "size": 96}
{"docstring": "\"\"\"Loads default JSON data found in home slinghot directory,\nor another if specified.\n\"\"\"\n\n", "func_signal": "def loaddefaults(startpath=slingshothomedir):\n", "code": "defaultjson = os.path.join(startpath, 'defaults.json')\n\nif os.path.exists(defaultjson):\n  data = json.loads(_readfile(defaultjson))\nelse:\n  data = {}\n\nreturn data", "path": "src\\slingshot\\generator.py", "repo_name": "zaach/slingshot", "stars": 1, "license": "mit", "language": "python", "size": 96}
{"docstring": "\"\"\"{repeated section foo}\"\"\"\n\n", "func_signal": "def _DoRepeatedSection(args, context, callback):\n", "code": "block = args\n\nif block.section_name == '@':\n  # If the name is @, we stay in the enclosing context, but assume it's a\n  # list, and repeat this block many times.\n  items = context.CursorValue()\n  if type(items) is not list:\n    raise EvaluationError('Expected a list; got %s' % type(items))\n  pushed = False\nelse:\n  items = context.PushSection(block.section_name)\n  pushed = True\n\n# TODO: what if items is a dictionary?\n\nif items:\n  last_index = len(items) - 1\n  statements = block.Statements()\n  alt_statements = block.Statements('alternates with')\n  # NOTE: Iteration mutates the context!\n  for i, _ in enumerate(context):\n    # Execute the statements in the block for every item in the list.  Execute\n    # the alternate block on every iteration except the last.\n    # Each item could be an atom (string, integer, etc.) or a dictionary.\n    _Execute(statements, context, callback)\n    if i != last_index:\n      _Execute(alt_statements, context, callback)\n\nelse:\n  _Execute(block.Statements('or'), context, callback)\n\nif pushed:\n  context.Pop()", "path": "src\\slingshot\\jsontemplate.py", "repo_name": "zaach/slingshot", "stars": 1, "license": "mit", "language": "python", "size": 96}
{"docstring": "\"\"\"{section foo}\"\"\"\n\n", "func_signal": "def _DoSection(args, context, callback):\n", "code": "block = args\n# If a section isn't present in the dictionary, or is None, then don't show it\n# at all.\nif context.PushSection(block.section_name):\n  _Execute(block.Statements(), context, callback)\n  context.Pop()\nelse:  # Empty list, None, False, etc.\n  context.Pop()\n  _Execute(block.Statements('or'), context, callback)", "path": "src\\slingshot\\jsontemplate.py", "repo_name": "zaach/slingshot", "stars": 1, "license": "mit", "language": "python", "size": 96}
{"docstring": "\"\"\"\nArgs:\n  template_dir: The base directory of the template files\n\"\"\"\n", "func_signal": "def __init__(self, template_dir):\n", "code": "self.template_dir = template_dir\nself.project_dir = os.path.abspath(os.curdir)\nself.templates = []", "path": "src\\slingshot\\generator.py", "repo_name": "zaach/slingshot", "stars": 1, "license": "mit", "language": "python", "size": 96}
{"docstring": "\"\"\"Low level method to expands the template piece by piece.\n\nArgs:\n  data_dict: The JSON data dictionary.\n  callback: A callback which should be called with each expanded token.\n\nExample: You can pass 'f.write' as the callback to write directly to a file\nhandle.\n\"\"\"\n", "func_signal": "def render(self, data_dict, callback):\n", "code": "context = _ScopedContext(data_dict, self.undefined_str)\n_Execute(self._program.Statements(), context, callback)", "path": "src\\slingshot\\jsontemplate.py", "repo_name": "zaach/slingshot", "stars": 1, "license": "mit", "language": "python", "size": 96}
{"docstring": "\"\"\"\nGet the value associated with a name in the current context.  The current\ncontext could be an dictionary in a list, or a dictionary outside a list.\n\"\"\"\n", "func_signal": "def Lookup(self, name):\n", "code": "i = len(self.stack) - 1\nwhile 1:\n  context = self.stack[i]\n\n  if not hasattr(context, 'get'):  # Can't look up names in a list or atom\n    i -= 1\n  else:\n    value = context.get(name)\n    if value is None:  # A key of None or a missing key are treated the same\n      i -= 1\n    else:\n      return value\n\n  if i <= -1:  # Couldn't find it anywhere\n    if self.undefined_str is None:\n      raise UndefinedVariable('%r is not defined' % name)\n    else:\n      return self.undefined_str", "path": "src\\slingshot\\jsontemplate.py", "repo_name": "zaach/slingshot", "stars": 1, "license": "mit", "language": "python", "size": 96}
{"docstring": "\"\"\"Yields a list of tokens resulting from expansion.\n\nThis may be useful for WSGI apps.  NOTE: In the current implementation, the\nentire expanded template must be stored memory.\n\nNOTE: This is a generator, but JavaScript doesn't have generators.\n\"\"\"\n", "func_signal": "def tokenstream(self, data_dict):\n", "code": "tokens = []\nself.render(data_dict, tokens.append)\nfor token in tokens:\n  yield token", "path": "src\\slingshot\\jsontemplate.py", "repo_name": "zaach/slingshot", "stars": 1, "license": "mit", "language": "python", "size": 96}
{"docstring": "\"\"\"Return a (compiled) regular expression for tokenization.\n\nArgs:\n  meta_left, meta_right: e.g. '{' and '}'\n\n- The regular expressions are memoized.\n- This function is public so the syntax highlighter can use it.\n\"\"\"\n", "func_signal": "def MakeTokenRegex(meta_left, meta_right):\n", "code": "key = meta_left, meta_right\nif key not in _token_re_cache:\n  # - Need () grouping for re.split\n  # - For simplicity, we allow all characters except newlines inside\n  #   metacharacters ({} / [])\n  _token_re_cache[key] = re.compile(\n      r'(' +\n      re.escape(meta_left) +\n      r'.+?' +\n      re.escape(meta_right) +\n      r')')\nreturn _token_re_cache[key]", "path": "src\\slingshot\\jsontemplate.py", "repo_name": "zaach/slingshot", "stars": 1, "license": "mit", "language": "python", "size": 96}
{"docstring": "\"\"\"Free function to expands a template string with a data dictionary.\n\nThis is useful for cases where you don't care about saving the result of\ncompilation (similar to re.match('.*', s) vs DOT_STAR.match(s))\n\"\"\"\n", "func_signal": "def expand(template_str, dictionary, **kwargs):\n", "code": "t = Template(template_str, **kwargs)\nreturn t.expand(dictionary)", "path": "src\\slingshot\\jsontemplate.py", "repo_name": "zaach/slingshot", "stars": 1, "license": "mit", "language": "python", "size": 96}
{"docstring": "\"\"\"Assumes that the top of the stack is a list.\"\"\"\n\n# The top of the stack is always the current context.\n", "func_signal": "def __iter__(self):\n", "code": "self.stack.append(None)\nfor item in self.stack[-2]:\n  self.stack[-1] = item\n  yield item\nself.stack.pop()", "path": "src\\slingshot\\jsontemplate.py", "repo_name": "zaach/slingshot", "stars": 1, "license": "mit", "language": "python", "size": 96}
{"docstring": "\"\"\"Update our built-in md5 registry\"\"\"\n\n", "func_signal": "def update_md5(filenames):\n", "code": "import re\n\nfor name in filenames:\n    base = os.path.basename(name)\n    f = open(name,'rb')\n    md5_data[base] = md5(f.read()).hexdigest()\n    f.close()\n\ndata = [\"    %r: %r,\\n\" % it for it in md5_data.items()]\ndata.sort()\nrepl = \"\".join(data)\n\nimport inspect\nsrcfile = inspect.getsourcefile(sys.modules[__name__])\nf = open(srcfile, 'rb'); src = f.read(); f.close()\n\nmatch = re.search(\"\\nmd5_data = {\\n([^}]+)}\", src)\nif not match:\n    print >>sys.stderr, \"Internal error!\"\n    sys.exit(2)\n\nsrc = src[:match.start(1)] + repl + src[match.end(1):]\nf = open(srcfile,'w')\nf.write(src)\nf.close()", "path": "ez_setup.py", "repo_name": "zaach/slingshot", "stars": 1, "license": "mit", "language": "python", "size": 96}
{"docstring": "\"\"\"\nArgs:\n  context: The root context\n  undefined_str: See Template() constructor.\n\"\"\"\n", "func_signal": "def __init__(self, context, undefined_str):\n", "code": "self.stack = [context]\nself.undefined_str = undefined_str", "path": "src\\slingshot\\jsontemplate.py", "repo_name": "zaach/slingshot", "stars": 1, "license": "mit", "language": "python", "size": 96}
{"docstring": "\"\"\"For sections or repeated sections.\"\"\"\n\n", "func_signal": "def NewSection(self, repeated, section_name):\n", "code": "new_block = _Section(section_name)\nif repeated:\n  func = _DoRepeatedSection\nelse:\n  func = _DoSection\n\nself.current_block.Append((func, new_block))\nself.stack.append(new_block)\nself.current_block = new_block", "path": "src\\slingshot\\jsontemplate.py", "repo_name": "zaach/slingshot", "stars": 1, "license": "mit", "language": "python", "size": 96}
{"docstring": "\"\"\"Yields tokens, which are 2-tuples (TOKEN_TYPE, token_string).\"\"\"\n\n", "func_signal": "def _Tokenize(template_str, meta_left, meta_right):\n", "code": "trimlen = len(meta_left)\n\ntoken_re = MakeTokenRegex(meta_left, meta_right)\n\nfor line in template_str.splitlines(True):  # retain newlines\n  tokens = token_re.split(line)\n\n  # Check for a special case first.  If a comment or \"block\" directive is on a\n  # line by itself (with only space surrounding it), then the space is\n  # omitted.  For simplicity, we don't handle the case where we have 2\n  # directives, say '{.end} # {#comment}' on a line.\n\n  if len(tokens) == 3:\n    # ''.isspace() == False, so work around that\n    if (tokens[0].isspace() or not tokens[0]) and \\\n       (tokens[2].isspace() or not tokens[2]):\n      token = tokens[1][trimlen : -trimlen]\n\n      if token.startswith('#'):\n        continue  # The whole line is omitted\n\n      token_type, token = _MatchDirective(token)\n      if token_type is not None:\n        yield token_type, token  # Only yield the token, not space\n        continue\n\n  # The line isn't special; process it normally.\n\n  for i, token in enumerate(tokens):\n    if i % 2 == 0:\n      yield LITERAL_TOKEN, token\n\n    else:  # It's a \"directive\" in metachracters\n\n      assert token.startswith(meta_left), repr(token)\n      assert token.endswith(meta_right), repr(token)\n      token = token[trimlen : -trimlen]\n\n      # It's a comment\n      if token.startswith('#'):\n        continue\n\n      if token.startswith('.'):\n\n        literal = {\n            '.meta-left': meta_left,\n            '.meta-right': meta_right,\n            '.space': ' ',\n            '.tab': '\\t',\n            '.newline': '\\n',\n            }.get(token)\n\n        if literal is not None:\n          yield LITERAL_TOKEN, literal\n          continue\n\n        token_type, token = _MatchDirective(token)\n        if token_type is not None:\n          yield token_type, token\n\n      else:  # Now we know the directive is a substitution.\n        yield SUBSTITUTION_TOKEN, token", "path": "src\\slingshot\\jsontemplate.py", "repo_name": "zaach/slingshot", "stars": 1, "license": "mit", "language": "python", "size": 96}
{"docstring": "\"\"\"Split and validate metacharacters.\n\nExample: '{}' -> ('{', '}')\n\nThis is public so the syntax highlighter and other tools can use it.\n\"\"\"\n", "func_signal": "def SplitMeta(meta):\n", "code": "n = len(meta)\nif n % 2 == 1:\n  raise ConfigurationError(\n      '%r has an odd number of metacharacters' % meta)\nreturn meta[:n/2], meta[n/2:]", "path": "src\\slingshot\\jsontemplate.py", "repo_name": "zaach/slingshot", "stars": 1, "license": "mit", "language": "python", "size": 96}
{"docstring": "\"\"\"This helps people debug their templates.\n\nIf a variable isn't defined, then some context is shown in the traceback.\nTODO: Attach context for other errors.\n\"\"\"\n", "func_signal": "def __str__(self):\n", "code": "if hasattr(self, 'near'):\n  return '%s\\n\\nNear: %s' % (self.args[0], pprint.pformat(self.near))\nelse:\n  return self.args[0]", "path": "src\\slingshot\\jsontemplate.py", "repo_name": "zaach/slingshot", "stars": 1, "license": "mit", "language": "python", "size": 96}
{"docstring": "\"\"\"\nArgs:\n  more_formatters: A function which returns a function to apply to the\n      value, given a format string.  It can return None, in which case the\n      _DEFAULT_FORMATTERS dictionary is consulted.\n\"\"\"\n", "func_signal": "def __init__(self, more_formatters):\n", "code": "self.current_block = _Section()\nself.stack = [self.current_block]\nself.more_formatters = more_formatters", "path": "src\\slingshot\\jsontemplate.py", "repo_name": "zaach/slingshot", "stars": 1, "license": "mit", "language": "python", "size": 96}
{"docstring": "\"\"\"\nAppends '?' or '&' to an url, so you can easily add extra GET parameters.\n\"\"\"\n", "func_signal": "def urlquerybase(url):\n", "code": "if url:\n    if '?' in url:\n        url += '&'\n    else:\n        url += '?'\nreturn url", "path": "server\\common\\appenginepatch\\ragendja\\templatetags\\ragendjatags.py", "repo_name": "lacostej/mycitybikes-server", "stars": 1, "license": "None", "language": "python", "size": 4172}
{"docstring": "\"\"\"\nRemove expired instances of ``RegistrationProfile`` and their\nassociated ``User``s.\n\nAccounts to be deleted are identified by searching for\ninstances of ``RegistrationProfile`` with expired activation\nkeys, and then checking to see if their associated ``User``\ninstances have the field ``is_active`` set to ``False``; any\n``User`` who is both inactive and has an expired activation\nkey will be deleted.\n\nIt is recommended that this method be executed regularly as\npart of your routine site maintenance; this application\nprovides a custom management command which will call this\nmethod, accessible as ``manage.py cleanupregistration``.\n\nRegularly clearing out accounts which have never been\nactivated serves two useful purposes:\n\n1. It alleviates the ocasional need to reset a\n   ``RegistrationProfile`` and/or re-send an activation email\n   when a user does not receive or does not act upon the\n   initial activation email; since the account will be\n   deleted, the user will be able to simply re-register and\n   receive a new activation key.\n\n2. It prevents the possibility of a malicious user registering\n   one or more accounts and never activating them (thus\n   denying the use of those usernames to anyone else); since\n   those accounts will be deleted, the usernames will become\n   available for use again.\n\nIf you have a troublesome ``User`` and wish to disable their\naccount while keeping it in the database, simply delete the\nassociated ``RegistrationProfile``; an inactive ``User`` which\ndoes not have an associated ``RegistrationProfile`` will not\nbe deleted.\n\n\"\"\"\n", "func_signal": "def delete_expired_users(self):\n", "code": "for profile in RegistrationProfile.all():\n    if profile.activation_key_expired():\n        user = profile.user\n        if not user.is_active:\n            user.delete()\n            profile.delete()", "path": "server\\registration\\models.py", "repo_name": "lacostej/mycitybikes-server", "stars": 1, "license": "None", "language": "python", "size": 4172}
{"docstring": "\"\"\"Copy all media files directly\"\"\"\n", "func_signal": "def add_uncombined_app_media(app):\n", "code": "if on_production_server:\n    return\npath = os.path.join(\n    os.path.dirname(__import__(app, {}, {}, ['']).__file__), 'media')\napp = app.rsplit('.', 1)[-1]\nfor root, dirs, files in os.walk(path):\n    for file in files:\n        if file.endswith(('.css', '.js')):\n            base = os.path.join(root, file)[len(path):].replace(os.sep,\n                '/').lstrip('/')\n            target = '%s/%s' % (app, base)\n            add_app_media(target, target)", "path": "server\\common\\appenginepatch\\ragendja\\settings_post.py", "repo_name": "lacostej/mycitybikes-server", "stars": 1, "license": "None", "language": "python", "size": 4172}
{"docstring": "\"\"\"\nTest that ``RegistrationFormNoFreeEmail`` disallows\nregistration with free email addresses.\n\n\"\"\"\n", "func_signal": "def test_registration_form_no_free_email(self):\n", "code": "base_data = { 'username': 'foo',\n              'password1': 'foo',\n              'password2': 'foo' }\nfor domain in ('aim.com', 'aol.com', 'email.com', 'gmail.com',\n               'googlemail.com', 'hotmail.com', 'hushmail.com',\n               'msn.com', 'mail.ru', 'mailinator.com', 'live.com'):\n    invalid_data = base_data.copy()\n    invalid_data['email'] = u\"foo@%s\" % domain\n    form = forms.RegistrationFormNoFreeEmail(data=invalid_data)\n    self.failIf(form.is_valid())\n    self.assertEqual(form.errors['email'], [u\"Registration using free email addresses is prohibited. Please supply a different email address.\"])\n\nbase_data['email'] = 'foo@example.com'\nform = forms.RegistrationFormNoFreeEmail(data=base_data)\nself.failUnless(form.is_valid())", "path": "server\\registration\\tests.py", "repo_name": "lacostej/mycitybikes-server", "stars": 1, "license": "None", "language": "python", "size": 4172}
{"docstring": "\"\"\"\nDetermine whether this ``RegistrationProfile``'s activation\nkey has expired, returning a boolean -- ``True`` if the key\nhas expired.\n\nKey expiration is determined by a two-step process:\n\n1. If the user has already activated, the key will have been\n   reset to the string constant ``ACTIVATED``. Re-activating\n   is not permitted, and so this method returns ``True`` in\n   this case.\n\n2. Otherwise, the date the user signed up is incremented by\n   the number of days specified in the setting\n   ``ACCOUNT_ACTIVATION_DAYS`` (which should be the number of\n   days after signup during which a user is allowed to\n   activate their account); if the result is less than or\n   equal to the current date, the key has expired and this\n   method returns ``True``.\n\n\"\"\"\n", "func_signal": "def activation_key_expired(self):\n", "code": "expiration_date = datetime.timedelta(days=settings.ACCOUNT_ACTIVATION_DAYS)\nreturn self.activation_key == RegistrationProfile.ACTIVATED or \\\n       (self.user.date_joined + expiration_date <= datetime.datetime.now())", "path": "server\\registration\\models.py", "repo_name": "lacostej/mycitybikes-server", "stars": 1, "license": "None", "language": "python", "size": 4172}
{"docstring": "\"\"\"Return the totalSlots as a String\"\"\"\n", "func_signal": "def totalLocks(self):\n", "code": "if (self.readyBikes == None):\n  return None\nreturn unicode(int(self.readyBikes) + int(self.emptyLocks))", "path": "data-clients\\mcb-python-client\\import\\mycitybikes_oslo_clearchannel.py", "repo_name": "lacostej/mycitybikes-server", "stars": 1, "license": "None", "language": "python", "size": 4172}
{"docstring": "\"\"\"\nTest that ``RegistrationProfile.activation_key_expired()``\nreturns ``True`` for expired users and for active users, and\n``False`` otherwise.\n\n\"\"\"\n# Unexpired user returns False.\n", "func_signal": "def test_account_expiration_condition(self):\n", "code": "self.failIf(RegistrationProfile.all().filter('user =', self.sample_user).get().activation_key_expired())\n\n# Expired user returns True.\nself.failUnless(RegistrationProfile.all().filter('user =', self.expired_user).get().activation_key_expired())\n\n# Activated user returns True.\nRegistrationProfile.objects.activate_user(RegistrationProfile.all().filter('user =', self.sample_user).get().activation_key)\nself.failUnless(RegistrationProfile.all().filter('user =', self.sample_user).get().activation_key_expired())", "path": "server\\registration\\tests.py", "repo_name": "lacostej/mycitybikes-server", "stars": 1, "license": "None", "language": "python", "size": 4172}
{"docstring": "\"\"\"\nValidate that the supplied email address is unique for the\nsite.\n\n\"\"\"\n", "func_signal": "def clean_email(self):\n", "code": "email = self.cleaned_data['email'].lower()\nif User.all().filter('email =', email).filter(\n        'is_active =', True).count(1):\n    raise forms.ValidationError(__(u'This email address is already in use. Please supply a different email address.'))\nreturn email", "path": "server\\myapp\\forms.py", "repo_name": "lacostej/mycitybikes-server", "stars": 1, "license": "None", "language": "python", "size": 4172}
{"docstring": "\"\"\"\nTest that ``RegistrationFormTermsOfService`` requires\nagreement to the terms of service.\n\n\"\"\"\n", "func_signal": "def test_registration_form_tos(self):\n", "code": "form = forms.RegistrationFormTermsOfService(data={ 'username': 'foo',\n                                                   'email': 'foo@example.com',\n                                                   'password1': 'foo',\n                                                   'password2': 'foo' })\nself.failIf(form.is_valid())\nself.assertEqual(form.errors['tos'], [u\"You must agree to the terms to register\"])\n\nform = forms.RegistrationFormTermsOfService(data={ 'username': 'foo',\n                                                   'email': 'foo@example.com',\n                                                   'password1': 'foo',\n                                                   'password2': 'foo',\n                                                   'tos': 'on' })\nself.failUnless(form.is_valid())", "path": "server\\registration\\tests.py", "repo_name": "lacostej/mycitybikes-server", "stars": 1, "license": "None", "language": "python", "size": 4172}
{"docstring": "\"\"\"\nTest that user activation actually activates the user and\nproperly resets the activation key, and fails for an\nalready-active or expired user, or an invalid key.\n\n\"\"\"\n# Activating a valid user returns the user.\n", "func_signal": "def test_activation(self):\n", "code": "self.failUnlessEqual(RegistrationProfile.objects.activate_user(RegistrationProfile.all().filter('user =', self.sample_user).get().activation_key).key(),\n                     self.sample_user.key())\n\n# The activated user must now be active.\nself.failUnless(User.get(self.sample_user.key()).is_active)\n\n# The activation key must now be reset to the \"already activated\" constant.\nself.failUnlessEqual(RegistrationProfile.all().filter('user =', self.sample_user).get().activation_key,\n                     RegistrationProfile.ACTIVATED)\n\n# Activating an expired user returns False.\nself.failIf(RegistrationProfile.objects.activate_user(RegistrationProfile.all().filter('user =', self.expired_user).get().activation_key))\n\n# Activating from a key that isn't a SHA1 hash returns False.\nself.failIf(RegistrationProfile.objects.activate_user('foo'))\n\n# Activating from a key that doesn't exist returns False.\nself.failIf(RegistrationProfile.objects.activate_user(sha.new('foo').hexdigest()))", "path": "server\\registration\\tests.py", "repo_name": "lacostej/mycitybikes-server", "stars": 1, "license": "None", "language": "python", "size": 4172}
{"docstring": "\"\"\"\nValidate that the username is alphanumeric and is not already\nin use.\n\n\"\"\"\n", "func_signal": "def clean_username(self):\n", "code": "user = User.get_by_key_name(\"key_\"+self.cleaned_data['username'].lower())\nif user and user.is_active:\n    raise forms.ValidationError(__(u'This username is already taken. Please choose another.'))\nreturn self.cleaned_data['username']", "path": "server\\myapp\\forms.py", "repo_name": "lacostej/mycitybikes-server", "stars": 1, "license": "None", "language": "python", "size": 4172}
{"docstring": "\"\"\"\nValidate an activation key and activate the corresponding\n``User`` if valid.\n\nIf the key is valid and has not expired, return the ``User``\nafter activating.\n\nIf the key is not valid or has expired, return ``False``.\n\nIf the key is valid but the ``User`` is already active,\nreturn ``False``.\n\nTo prevent reactivation of an account which has been\ndeactivated by site administrators, the activation key is\nreset to the string constant ``RegistrationProfile.ACTIVATED``\nafter successful activation.\n\nTo execute customized logic when a ``User`` is activated,\nconnect a function to the signal\n``registration.signals.user_activated``; this signal will be\nsent (with the ``User`` as the value of the keyword argument\n``user``) after a successful activation.\n\n\"\"\"\n", "func_signal": "def activate_user(self, activation_key):\n", "code": "from registration.signals import user_activated\n\n# Make sure the key we're trying conforms to the pattern of a\n# SHA1 hash; if it doesn't, no point trying to look it up in\n# the database.\nif SHA1_RE.search(activation_key):\n    profile = RegistrationProfile.get_by_key_name(\"key_\"+activation_key)\n    if not profile:\n        return False\n    if not profile.activation_key_expired():\n        user = profile.user\n        user.is_active = True\n        user.put()\n        profile.activation_key = RegistrationProfile.ACTIVATED\n        profile.put()\n        user_activated.send(sender=self.model, user=user)\n        return user\nreturn False", "path": "server\\registration\\models.py", "repo_name": "lacostej/mycitybikes-server", "stars": 1, "license": "None", "language": "python", "size": 4172}
{"docstring": "\"\"\"Return the index of the first item in seq where f(item) == True.\"\"\"\n", "func_signal": "def index(seq, f):\n", "code": "for index in (i for i in xrange(len(seq)) if f(seq[i])):\n    return index", "path": "data-clients\\mcb-python-client\\src\\mycitybikes\\utils.py", "repo_name": "lacostej/mycitybikes-server", "stars": 1, "license": "None", "language": "python", "size": 4172}
{"docstring": "\"\"\"Return a list of ids (strings) representing \"\"\"\n", "func_signal": "def getStationIds(xml):\n", "code": "stringNode = ET.XML(xml)\nif not stringNode.tag == \"{http://smartbikeportal.clearchannel.no/public/mobapp/}string\":\n  raise McbException(\"Incorrect format: expected 'string' unexpected root tree '\" + stringNode.tag + \"'\")\n\nstationsXml = \"<stations>\" + stringNode.text + \"</stations>\"\nstationsNode = ET.XML(stationsXml)\n\nids = []\nfor stationNode in stationsNode:\n  if not stationNode.tag == \"station\":\n    raise McbException(\"Incorrect format: expected 'station' sub node but got '\" + stationNode.tag + \"'\")\n  ids.append(stationNode.text)\nreturn ids", "path": "data-clients\\mcb-python-client\\import\\mycitybikes_oslo_clearchannel.py", "repo_name": "lacostej/mycitybikes-server", "stars": 1, "license": "None", "language": "python", "size": 4172}
{"docstring": "\"\"\"\nTest that the activation view activates the user from a valid\nkey and fails if the key is invalid or has expired.\n       \n\"\"\"\n# Valid user puts the user account into the context.\n", "func_signal": "def test_activation_view(self):\n", "code": "response = self.client.get(reverse('registration_activate',\n                                   kwargs={ 'activation_key': RegistrationProfile.all().filter('user =', self.sample_user).get().activation_key }))\nself.assertEqual(response.status_code, 200)\nself.assertEqual(response.context[0]['account'].key(), self.sample_user.key())\n\n# Expired user sets the account to False.\nresponse = self.client.get(reverse('registration_activate',\n                                   kwargs={ 'activation_key': RegistrationProfile.all().filter('user =', self.expired_user).get().activation_key }))\nself.failIf(response.context[0]['account'])\n\n# Invalid key gets to the view, but sets account to False.\nresponse = self.client.get(reverse('registration_activate',\n                                   kwargs={ 'activation_key': 'foo' }))\nself.failIf(response.context[0]['account'])\n\n# Nonexistent key sets the account to False.\nresponse = self.client.get(reverse('registration_activate',\n                                   kwargs={ 'activation_key': sha.new('foo').hexdigest() }))\nself.failIf(response.context[0]['account'])", "path": "server\\registration\\tests.py", "repo_name": "lacostej/mycitybikes-server", "stars": 1, "license": "None", "language": "python", "size": 4172}
{"docstring": "\"\"\"\nCreate a ``RegistrationProfile`` for a given\n``User``, and return the ``RegistrationProfile``.\n\nThe activation key for the ``RegistrationProfile`` will be a\nSHA1 hash, generated from a combination of the ``User``'s\nusername and a random salt.\n\n\"\"\"\n", "func_signal": "def create_profile(self, user):\n", "code": "salt = sha.new(str(random.random())).hexdigest()[:5]\nactivation_key = sha.new(salt+user.username).hexdigest()", "path": "server\\registration\\models.py", "repo_name": "lacostej/mycitybikes-server", "stars": 1, "license": "None", "language": "python", "size": 4172}
{"docstring": "# Ignore port if it's 80 or 443\n", "func_signal": "def process_request(self, request):\n", "code": "if ':' in request.get_host():\n    domain, port = request.get_host().split(':')\n    if int(port) not in (80, 443):\n        domain = request.get_host()\nelse:\n    domain = request.get_host().split(':')[0]\n\n# We cache the SITE_ID\ncache_key = 'Site:domain:%s' % domain\nsite = cache.get(cache_key)\nif site:\n    SITE_ID.value = site\nelse:\n    site = Site.all().filter('domain =', domain).get()\n    if not site:\n        # Fall back to with/without 'www.'\n        if domain.startswith('www.'):\n            fallback_domain = domain[4:]\n        else:\n            fallback_domain = 'www.' + domain\n        site = Site.all().filter('domain =', fallback_domain).get()\n\n    # Add site if it doesn't exist\n    if not site and getattr(settings, 'CREATE_SITES_AUTOMATICALLY',\n                            True):\n        site = db_create(Site, domain=domain, name=domain)\n        site.put()\n\n    # Set SITE_ID for this thread/request\n    if site:\n        SITE_ID.value = str(site.key())\n    else:\n        SITE_ID.value = _default_site_id\n\n    cache.set(cache_key, SITE_ID.value, 5*60)", "path": "server\\common\\appenginepatch\\ragendja\\sites\\dynamicsite.py", "repo_name": "lacostej/mycitybikes-server", "stars": 1, "license": "None", "language": "python", "size": 4172}
{"docstring": "\"\"\"\nTest that\n``RegistrationProfile.objects.delete_expired_users()`` deletes\nonly inactive users whose activation window has expired.\n\n\"\"\"\n", "func_signal": "def test_expired_user_deletion(self):\n", "code": "RegistrationProfile.objects.delete_expired_users()\nself.assertEqual(RegistrationProfile.all().count(), 1)", "path": "server\\registration\\tests.py", "repo_name": "lacostej/mycitybikes-server", "stars": 1, "license": "None", "language": "python", "size": 4172}
{"docstring": "\"\"\"Return the station object for Oslo\"\"\"\n", "func_signal": "def getStation(externalId, xml):\n", "code": "stringNode = ET.XML(xml)\nif not stringNode.tag == \"{http://smartbikeportal.clearchannel.no/public/mobapp/}string\":\n  raise McbException(\"Incorrect format: expected 'string' unexpected root tree '\" + stringNode.tag + \"'\")\n\ntext = stringNode.text.replace(\"&\", \"&amp;\").encode(\"utf-8\")\nstationNode = ET.XML(text)\n\nif not stationNode.tag == \"station\":\n  raise McbException(\"Unexpected root node in station: '\" + stationNode.tag + \"'\")\n\nreadyBikes = None\nfor subNode in stationNode:\n  if subNode.tag == \"description\":\n    description = subNode.text.strip().replace(\"&\", \"&amp;\").encode(\"utf-8\")\n    array = description.partition(\"-\")\n    if not len(array[1]) == 0:\n      description = array[2]\n    description = unicode(description, \"utf-8\")\n  elif subNode.tag == \"longitute\": # Sic...\n    longitude = subNode.text\n  elif subNode.tag == \"latitude\":\n    latitude = subNode.text\n  elif subNode.tag == \"ready_bikes\":\n    readyBikes = subNode.text\n  elif subNode.tag == \"empty_locks\":\n    emptyLocks = subNode.text\n  elif subNode.tag == \"online\":\n    online = subNode.text\n  else:\n    raise McbException(\"Unexpected node in station: '\" + subNode.tag + \"'\")\nif (readyBikes == None):\n  status = OsloStationStatus(None, None, \"0\")\nelse:\n  status = OsloStationStatus(readyBikes, emptyLocks, online)\nreturn OsloStation(externalId, description, latitude, longitude, status)", "path": "data-clients\\mcb-python-client\\import\\mycitybikes_oslo_clearchannel.py", "repo_name": "lacostej/mycitybikes-server", "stars": 1, "license": "None", "language": "python", "size": 4172}
{"docstring": "\"\"\"\nTest that the ``user_registered`` and ``user_activated``\nsignals are sent, and that they send the ``User`` as an\nargument.\n\n\"\"\"\n", "func_signal": "def test_signals(self):\n", "code": "def receiver(sender, **kwargs):\n    self.assert_('user' in kwargs)\n    self.assertEqual(kwargs['user'].username, u'signal_test')\n    received_signals.append(kwargs.get('signal'))\n\nreceived_signals = []\nexpected_signals = [signals.user_registered, signals.user_activated]\nfor signal in expected_signals:\n    signal.connect(receiver)\n\nRegistrationProfile.objects.create_inactive_user(username='signal_test',\n                                                 password='foo',\n                                                 email='nobody@example.com',\n                                                 send_email=False)\nRegistrationProfile.objects.activate_user(RegistrationProfile.all().filter('user =', db.Key.from_path(User.kind(), 'key_signal_test')).get().activation_key)\n\nself.assertEqual(received_signals, expected_signals)", "path": "server\\registration\\tests.py", "repo_name": "lacostej/mycitybikes-server", "stars": 1, "license": "None", "language": "python", "size": 4172}
{"docstring": "# We should have UNIQUE (location) and UNIQUE (name),\n# not just a unique-together.\n", "func_signal": "def register():\n", "code": "restype = ResourceType('Rails', Rails,\n  id_type={\n    'name': AttrType(\n      pytype=str),\n    'location': RefAttrType(\n      rtype='Directory'),\n    },\n  state_type={\n    # More privileged\n    'maint_user': RefAttrType(\n      valid_condition=is_valid_user,\n      rtype='User'),\n    # Less privileged\n    'run_user': RefAttrType(\n      valid_condition=is_valid_user,\n      rtype='User'),\n    'hostname': AttrType(\n      default_value='localhost',\n      pytype=str),\n    'cluster': RefAttrType(\n      rtype='PgCluster'),\n    })\nget_registry().resource_types.register(restype)", "path": "lib\\systems\\plugins\\rails\\rails.py", "repo_name": "g2p/systems", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 380}
{"docstring": "\"\"\"\nWraps a function and its preconditions.\n\nThis couldn't be implemented with callable objects,\nbecause that would hijack the 'self' argument.\n\"\"\"\n\n", "func_signal": "def wrap(checked_fn, precond_fns):\n", "code": "def with_checks(self, *args, **kargs):\n  for precond_fn in precond_fns:\n    precond_fn(self, *args, **kargs)\n  return checked_fn(self, *args, **kargs)\nwith_checks._func = checked_fn\nwith_checks._preconds = precond_fns\nwith_checks._is_a_wrapped_function = None\nwith_checks.__name__ = checked_fn.__name__\nwith_checks.__doc__ = checked_fn.__doc__\nreturn with_checks", "path": "lib\\systems\\util\\contracts.py", "repo_name": "g2p/systems", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 380}
{"docstring": "# This is incomplete.\n", "func_signal": "def to_yaml(cls, dumper, rg):\n", "code": "pred_rels = [{'node': node, 'depends': list(depends), }\n    for (node, depends) in rg._iter_pred_rels()]\nreturn dumper.represent_mapping(cls.yaml_tag, {\n  'nodes': pred_rels,\n  })", "path": "lib\\systems\\context.py", "repo_name": "g2p/systems", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 380}
{"docstring": "\"\"\"\nrtype: a resource type.\n\"\"\"\n\n", "func_signal": "def __init__(self, **kargs):\n", "code": "self.__rtypename = kargs.pop('rtype')\nif not isinstance(self.rtypename, str):\n  raise ValueError(self.rtypename)\nif 'pytype' in kargs:\n  raise ValueError(kargs)\nkargs['pytype'] = ResourceRef\nsuper(RefAttrType, self).__init__(**kargs)", "path": "lib\\systems\\typesystem.py", "repo_name": "g2p/systems", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 380}
{"docstring": "\"\"\"\nReplace an iterable of resources with one new resource.\n\nMay break the acyclic invariant, caveat emptor.\n\"\"\"\n\n# The invariant is kept iff the r0s don't have paths linking them.\n# For our use case (collectors), we could allow paths provided they are\n# internal to r0s. This introduces self-loops that we would then remove.\n\n", "func_signal": "def collect_resources(self, r0s, r1):\n", "code": "for r0 in r0s:\n  r0 = self._intern(r0)\n  if r0 in self.__processed:\n    raise RuntimeError\n\nif r1 in self._graph:\n  raise ValueError(r1)\nr1 = self._add_aggregate(r1)\n\nfor r0 in r0s:\n  r0 = self._intern(r0)\n  self._move_edges(r0, r1)\n  self.__processed.add(r0)\nself.require_acyclic()", "path": "lib\\systems\\context.py", "repo_name": "g2p/systems", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 380}
{"docstring": "\"\"\"\nBuild the finished dependency graph.\n\nMerge identical realizables, collect what can be.\n\"\"\"\n\n", "func_signal": "def ensure_frozen(self):\n", "code": "if self.__state == 'frozen':\n  return\n# Order is important\nself.require_state('init')\nself.__expandable.expand_into(self.__resources)\n#self.__resources.draw('/tmp/freezing')\nself._expand()\n#self.__resources.draw('/tmp/pre-collect')\nself._collect()\nself._expand_aggregates()\nassert not bool(list(self.__resources.iter_unprocessed()))\nself.__state = 'frozen'\n#self.__resources.draw('/tmp/frozen')", "path": "lib\\systems\\context.py", "repo_name": "g2p/systems", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 380}
{"docstring": "# XXX pygraphviz has steep dependencies (x11 libs)\n# and recommends (texlive) for a headless box.\n\n# We duplicate the graph, otherwise networkx / pygraphviz\n# would make a lossy conversion (sometimes refusing to convert), by adding\n# nodes as their string representation. Madness, I know.\n", "func_signal": "def draw_agraph(self, fname):\n", "code": "gr2 = NX.create_empty_copy(self._graph, False)\nfor node in self._graph.nodes_iter():\n  gr2.add_node(id(node))\nfor (n0, n1) in self._graph.edges_iter():\n  gr2.add_edge(id(n0), id(n1))\nnames = dict((id(node), { 'label': describe(node)})\n    for node in self._graph.nodes_iter())\ngr2.delete_node(id(self._first))\ngr2.delete_node(id(self._last))\ng = NX.to_agraph(gr2, {\n    'graph': {\n      'nodesep': '0.2',\n      'rankdir': 'TB',\n      'ranksep': '0.5',\n      },\n    'node': {\n      'shape': 'box',\n      },\n    },\n    names)\ng.write(fname + '.dot')\n# Dot is good for DAGs.\ng.layout(prog='dot')\ng.draw(fname + '.svg')\nwith open(fname + '.yaml', 'w') as f:\n  yaml.dump(self, f)\n# Fails with the expanded graph, due to instancemethod\n#yaml.load(yaml.dump(self))", "path": "lib\\systems\\context.py", "repo_name": "g2p/systems", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 380}
{"docstring": "\"\"\"\nAdd a resource.\n\nIf an identical resource exists, it is returned.\n\"\"\"\n\n", "func_signal": "def add_resource(self, resource, depends=()):\n", "code": "if not isinstance(resource, (CResource, EResource)):\n  raise TypeError(resource, (CResource, EResource))\n\nif resource.identity in self.__expandables:\n  # We have this id already.\n  # Either it's the exact same resource, or a KeyError is thrown.\n  resource = self._intern(resource)\n  # XXX Need to bypass _intern for already expanded.\n  # XXX When we use add_to_top, we sometimes have to deal\n  # with a resource that's already been expanded.\n  # Those are not in the graph anymore. How do we refer to them?\nelse:\n  self.__expandables[resource.identity] = resource\n# Even if already there, we need to add the depends.\nresource = self._add_node(resource, depends)\n# If already there, notice we aliase it.\nreturn self.make_ref(resource)", "path": "lib\\systems\\context.py", "repo_name": "g2p/systems", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 380}
{"docstring": "\"\"\"\nValidates the valdict, without requiring all values\nand without adding default values.\n\"\"\"\n\n", "func_signal": "def prepare_partial_valdict(self, valdict):\n", "code": "for k in valdict:\n  if k not in self.atypes:\n    raise KeyError(u'Invalid attribute \u00ab%s\u00bb' % k)\n  self.atypes[k].require_valid_value(valdict[k])\nreturn valdict", "path": "lib\\systems\\typesystem.py", "repo_name": "g2p/systems", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 380}
{"docstring": "# To get sizes: gcc -D_GNU_SOURCE -E /usr/include/sys/stat.h -o sysstat.i\n", "func_signal": "def fchmod(fd, mode):\n", "code": "fd = ctypes.c_int(fd)\nmode = ctypes.c_uint(mode)\nres = _libc.fchmod(fd, mode)\nif res:\n  raise OSError(_errno.value, os.strerror(_errno.value))", "path": "lib\\systems\\util\\syscalls.py", "repo_name": "g2p/systems", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 380}
{"docstring": "\"\"\"\nValidates the valdict, adding default values explicitly.\n\"\"\"\n\n", "func_signal": "def prepare_valdict(self, valdict):\n", "code": "for k in valdict:\n  if k not in self.atypes:\n    raise KeyError(u'Invalid attribute \u00ab%s\u00bb' % k)\n\nfor (n, a) in self.atypes.iteritems():\n  if not n in valdict:\n    if a.has_default_value:\n      valdict[n] = a.default_value\n    else:\n      raise KeyError(u'Attribute \u00ab%s\u00bb is unset' % n)\n  a.require_valid_value(valdict[n])\nreturn valdict", "path": "lib\\systems\\typesystem.py", "repo_name": "g2p/systems", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 380}
{"docstring": "# Chicken and egg problem:\n# this won't work until we have installed rubygems.\n", "func_signal": "def read_present(id_attrs):\n", "code": "with open('/dev/null') as nullf:\n  r = subprocess.call(['/usr/bin/gem',\n    'query',\n    '--quiet',\n    '--installed',\n    '--name-matches', id_attrs['name'],\n    '--version', id_attrs['version'],\n    ], stdout=nullf)\nif r == 0:\n  return True\nelif r == 1:\n  return False\nelse:\n  raise subprocess.CalledProcessError(r, '/usr/bin/gem')", "path": "lib\\systems\\plugins\\packages\\rubygem.py", "repo_name": "g2p/systems", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 380}
{"docstring": "\"\"\"\nGet the \"singleton\" instance.\n\"\"\"\n\n", "func_signal": "def global_instance(cls):\n", "code": "if not hasattr(cls, '_global_instance'):\n  setattr(cls, '_global_instance', cls())\nreturn getattr(cls, '_global_instance')", "path": "lib\\systems\\registry.py", "repo_name": "g2p/systems", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 380}
{"docstring": "\"\"\"\nUsage::\n  class foo(ContractSupportBase):\n    def _precond(self, foo):\n      if foo < 0:\n        raise ValueError\n    @precondition(_precond)\n    def method(self):\n      pass\n\"\"\"\n\n", "func_signal": "def precondition(precond_fn):\n", "code": "def partial_application(checked_fn):\n  return wrap(checked_fn, [precond_fn])\nreturn partial_application", "path": "lib\\systems\\util\\contracts.py", "repo_name": "g2p/systems", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 380}
{"docstring": "\"\"\"\nRaise an exception if we are not in the required state.\n\"\"\"\n\n", "func_signal": "def require_state(self, state):\n", "code": "if self.__state != state:\n  raise RuntimeError(u'Realizer state should be \u00ab%s\u00bb' % state)", "path": "lib\\systems\\context.py", "repo_name": "g2p/systems", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 380}
{"docstring": "# From lintian's _valid_version\n# XXX can pass dashes. Check in aptitude source code if it matches.\n# http://www.debian.org/doc/debian-policy/ch-controlfields.html#s-f-Version\n", "func_signal": "def is_valid_version(cls, version):\n", "code": "return bool(\n    re.match('^(\\d+:)?([-\\.+:~a-z0-9]+?)(-[\\.+~a-z0-9]+)?$', version))", "path": "lib\\systems\\plugins\\packages\\aptitudepackage.py", "repo_name": "g2p/systems", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 380}
{"docstring": "\"\"\"\nReplace res by a small resource graph.\n\nThe resource_graph is inserted in the main graph\nbetween the sentinels that represent the resource.\n\"\"\"\n\n", "func_signal": "def expand_resource(self, res):\n", "code": "res = self._intern(res)\n\n# We're processing from the outside in.\nif res in self.__processed:\n  raise RuntimeError\n\nresource_graph = ResourceGraph(self.__top)\nif isinstance(res, EResource):\n  for (name, ref) in res.iter_passed_by_ref():\n    # ref will be present in both graphs.\n    self._pass_by_ref(resource_graph, name, ref)\nelif isinstance(res, Aggregate):\n  pass\nelse:\n  raise TypeError(res)\nres.expand_into(resource_graph)\n# We expand from the outside in\nif bool(resource_graph.__processed):\n  raise RuntimeError\n\n# Do not skip sentinels.\nfor n in resource_graph._graph.nodes_iter():\n  self._add_node(n)\nfor (n0, n1) in resource_graph._graph.edges_iter():\n  self._add_node_dep(n0, n1)\n\nfor (id1, res1) in resource_graph.__expandables.iteritems():\n  # We expand from the outside in.\n  assert res1 not in self.__processed\n  if id1 in self.__expandables:\n    # Pass by reference if you must use the same resource\n    # in different contexts.\n    raise RuntimeError('ResourceBase collision.', res, res1)\n  else:\n    self.__expandables[id1] = res1\n\nbefore, after = self._split_node(res)\nself.__processed.add(res)\nself._move_edges(resource_graph._first, before)\nself._move_edges(resource_graph._last, after)\n# What may break the invariant:\n# Passing a ref to res, and making res depend on ref.\n# ref ends up on both sides of ref.before.\nself.require_acyclic()", "path": "lib\\systems\\context.py", "repo_name": "g2p/systems", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 380}
{"docstring": "# Collects compatible nodes into merged nodes.\n\n", "func_signal": "def _collect(self):\n", "code": "def can_merge(part0, part1):\n  for n0 in part0:\n    for n1 in part1:\n      if self.__resources.resources_connected(n0, n1):\n        return False\n  return True\n\ndef possibly_merge(partition):\n  # Merge once if possible. Return true if did merge.\n  e = dict(enumerate(partition))\n  n = len(partition)\n  # Loop over the triangle of unordered pairs\n  for i in xrange(n):\n    for j in xrange(i + 1, n):\n      part0, part1 = e[i], e[j]\n      if can_merge(part0, part1):\n        partition.add(part0.union(part1))\n        partition.remove(part0)\n        partition.remove(part1)\n        return True\n  return False\n\nreg = get_registry()\nfor collector in reg.collectors:\n  # Pre-partition is made of parts acceptable for the collector.\n  pre_partition = collector.partition(\n      [r for r in self.__resources.iter_uncollected_resources()\n        if collector.filter(r)])\n  for part in pre_partition:\n    # Collector parts are split again, the sub-parts are merged\n    # when dependencies allow.\n    # Not a particularly efficient algorithm, just simple.\n    # Gives one solution among many possibilities.\n    partition = set(frozenset((r, ))\n        for r in part\n        for part in pre_partition)\n    while possibly_merge(partition):\n      pass\n\n    # Let the collector handle the rest\n    for part in partition:\n      if not bool(part):\n        # Test for emptiness.\n        # Aggregate even singletons.\n        continue\n      merged = collector.collect(part)\n      self.__resources.collect_resources(part, merged)\nassert not bool(list(self.__resources.iter_uncollected_resources()))", "path": "lib\\systems\\context.py", "repo_name": "g2p/systems", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 380}
{"docstring": "# Pyplot is stateful and awkward to use.\n", "func_signal": "def draw_matplotlib(self, fname):\n", "code": "import matplotlib.pyplot as P\n# Disable hold or it definitely won't work (probably a bug).\nP.hold(False)\nNX.draw(self._graph)\nP.savefig(fname)", "path": "lib\\systems\\context.py", "repo_name": "g2p/systems", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 380}
{"docstring": "\"\"\"\nRead the current state.\n\"\"\"\n\n", "func_signal": "def read_value(self, id_attrs):\n", "code": "v = self.__reader(id_attrs)\nself.require_valid_value(v)\nreturn v", "path": "lib\\systems\\typesystem.py", "repo_name": "g2p/systems", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 380}
{"docstring": "\"\"\"\nRecord whether or not this rule parsed the input at this position\n\tsuccessfully.\n\t\"\"\"\n\n", "func_signal": "def memoize(self, input, ruleIndex, ruleStartIndex):\n", "code": "if self.failed:\n    stopTokenIndex = self.MEMO_RULE_FAILED\nelse:\n    stopTokenIndex = input.index() - 1\n\nif ruleIndex in self.ruleMemo:\n    self.ruleMemo[ruleIndex][ruleStartIndex] = stopTokenIndex", "path": "antlr3\\recognizers.py", "repo_name": "cheshire/antlr3-python-example", "stars": 1, "license": "None", "language": "python", "size": 2195}
{"docstring": "\"\"\"\nGiven a rule number and a start token index number, return\nMEMO_RULE_UNKNOWN if the rule has not parsed input starting from\nstart index.  If this rule has parsed input starting from the\nstart index before, then return where the rule stopped parsing.\nIt returns the index of the last token matched by the rule.\n\nFor now we use a hashtable and just the slow Object-based one.\nLater, we can make a special one for ints and also one that\ntosses out data after we commit past input position i.\n\t\"\"\"\n\n", "func_signal": "def getRuleMemoization(self, ruleIndex, ruleStartIndex):\n", "code": "if ruleIndex not in self.ruleMemo:\n    self.ruleMemo[ruleIndex] = {}\n\t\t\nstopIndex = self.ruleMemo[ruleIndex].get(ruleStartIndex, None)\nif stopIndex is None:\n    return self.MEMO_RULE_UNKNOWN\n\nreturn stopIndex", "path": "antlr3\\recognizers.py", "repo_name": "cheshire/antlr3-python-example", "stars": 1, "license": "None", "language": "python", "size": 2195}
{"docstring": "\"\"\"Set the char stream and reset the lexer\"\"\"\n", "func_signal": "def setCharStream(self, input):\n", "code": "self.input = None\nself.reset()\nself.input = input", "path": "antlr3\\recognizers.py", "repo_name": "cheshire/antlr3-python-example", "stars": 1, "license": "None", "language": "python", "size": 2195}
{"docstring": "\"\"\"\nHow should a token be displayed in an error message? The default\nis to display just the text, but during development you might\nwant to have a lot of information spit out.  Override in that case\nto use t.toString() (which, for CommonToken, dumps everything about\nthe token). This is better than forcing you to override a method in\nyour token objects because you don't have to go modify your lexer\nso that it creates a new Java type.\n\"\"\"\n\n", "func_signal": "def getTokenErrorDisplay(self, t):\n", "code": "s = t.text\nif s is None:\n    if t.type == EOF:\n        s = \"<EOF>\"\n    else:\n        s = \"<\"+t.type+\">\"\n\nreturn repr(s)", "path": "antlr3\\recognizers.py", "repo_name": "cheshire/antlr3-python-example", "stars": 1, "license": "None", "language": "python", "size": 2195}
{"docstring": "\"\"\"\nConsume tokens until one matches the given token or token set\n\ntokenTypes can be a single token type or a set of token types\n\n\"\"\"\n\n", "func_signal": "def consumeUntil(self, input, tokenTypes):\n", "code": "if not isinstance(tokenTypes, (set, frozenset)):\n    tokenTypes = frozenset([tokenTypes])\n\nttype = input.LA(1)\nwhile ttype != EOF and ttype not in tokenTypes:\n    input.consume()\n    ttype = input.LA(1)", "path": "antlr3\\recognizers.py", "repo_name": "cheshire/antlr3-python-example", "stars": 1, "license": "None", "language": "python", "size": 2195}
{"docstring": "\"\"\"\nA more general version of getRuleInvocationStack where you can\npass in, for example, a RecognitionException to get it's rule\nstack trace.  This routine is shared with all recognizers, hence,\nstatic.\n\nTODO: move to a utility class or something; weird having lexer call\nthis\n\"\"\"\n\n# mmmhhh,... perhaps look at the first argument\n# (f_locals[co_varnames[0]]?) and test if it's a (sub)class of\n# requested recognizer...\n\n", "func_signal": "def _getRuleInvocationStack(cls, module):\n", "code": "rules = []\nfor frame in reversed(inspect.stack()):\n    code = frame[0].f_code\n    codeMod = inspect.getmodule(code)\n    if codeMod is None:\n        continue\n\n    # skip frames not in requested module\n    if codeMod.__name__ != module:\n        continue\n\n    # skip some unwanted names\n    if code.co_name in ('nextToken', '<module>'):\n        continue\n\n    rules.append(code.co_name)\n\nreturn rules", "path": "antlr3\\recognizers.py", "repo_name": "cheshire/antlr3-python-example", "stars": 1, "license": "None", "language": "python", "size": 2195}
{"docstring": "# TODO do single token deletion like above for Token mismatch\n", "func_signal": "def recoverFromMismatchedSet(self, input, e, follow):\n", "code": "if not self.recoverFromMismatchedElement(input, e, follow):\n    raise e", "path": "antlr3\\recognizers.py", "repo_name": "cheshire/antlr3-python-example", "stars": 1, "license": "None", "language": "python", "size": 2195}
{"docstring": "\"\"\"Return the token type or char of the unexpected input element\"\"\"\n\n", "func_signal": "def getUnexpectedType(self):\n", "code": "from antlr3.streams import TokenStream\nfrom antlr3.tree import TreeNodeStream\n\nif isinstance(self.input, TokenStream):\n    return self.token.type\n\nelif isinstance(self.input, TreeNodeStream):\n    adaptor = self.input.treeAdaptor\n    return adaptor.getType(self.node)\n\nelse:\n    return self.c", "path": "antlr3\\exceptions.py", "repo_name": "cheshire/antlr3-python-example", "stars": 1, "license": "None", "language": "python", "size": 2195}
{"docstring": "\"\"\"\nRecover from an error found on the input stream.  Mostly this is\nNoViableAlt exceptions, but could be a mismatched token that\nthe match() routine could not recover from.\n\"\"\"\n\n# PROBLEM? what if input stream is not the same as last time\n# perhaps make lastErrorIndex a member of input\n", "func_signal": "def recover(self, input, re):\n", "code": "if self.lastErrorIndex == input.index():\n    # uh oh, another error at same token index; must be a case\n    # where LT(1) is in the recovery token set so nothing is\n    # consumed; consume a single token so at least to prevent\n    # an infinite loop; this is a failsafe.\n    input.consume()\n\nself.lastErrorIndex = input.index()\nfollowSet = self.computeErrorRecoverySet()\n\nself.beginResync()\nself.consumeUntil(input, followSet)\nself.endResync()", "path": "antlr3\\recognizers.py", "repo_name": "cheshire/antlr3-python-example", "stars": 1, "license": "None", "language": "python", "size": 2195}
{"docstring": "\"\"\"\nWhat error message should be generated for the various\nexception types?\n\nNot very object-oriented code, but I like having all error message\ngeneration within one method rather than spread among all of the\n\texception classes. This also makes it much easier for the exception\n\thandling because the exception classes do not have to have pointers back\n\tto this object to access utility routines and so on. Also, changing\n\tthe message for an exception type would be difficult because you\n\twould have to subclassing exception, but then somehow get ANTLR\n\tto make those kinds of exception objects instead of the default.\n\tThis looks weird, but trust me--it makes the most sense in terms\n\tof flexibility.\n\nFor grammar debugging, you will want to override this to add\n\tmore information such as the stack frame with\n\tgetRuleInvocationStack(e, this.getClass().getName()) and,\n\tfor no viable alts, the decision description and state etc...\n\nOverride this to change the message generated for one or more\n\texception types.\n\"\"\"\n\n", "func_signal": "def getErrorMessage(self, e, tokenNames):\n", "code": "msg = None\nif isinstance(e, MismatchedTokenException):\n    tokenName = \"<unknown>\"\n    if e.expecting == EOF:\n        tokenName = \"EOF\"\n    else:\n        tokenName = self.tokenNames[e.expecting]\n\n    msg = \"mismatched input \" \\\n          + self.getTokenErrorDisplay(e.token) \\\n          + \" expecting \" \\\n          + tokenName\n\nelif isinstance(e, MismatchedTreeNodeException):\n    tokenName = \"<unknown>\"\n    if e.expecting == EOF:\n        tokenName = \"EOF\"\n    else:\n        tokenName = self.tokenNames[e.expecting]\n\n    msg = \"mismatched tree node: %s expecting %s\" \\\n          % (e.node, tokenName)\n\nelif isinstance(e, NoViableAltException):\n    msg = \"no viable alternative at input \" \\\n          + self.getTokenErrorDisplay(e.token)\n\nelif isinstance(e, EarlyExitException):\n    msg = \"required (...)+ loop did not match anything at input \" \\\n          + self.getTokenErrorDisplay(e.token)\n\nelif isinstance(e, MismatchedSetException):\n    msg = \"mismatched input \" \\\n          + self.getTokenErrorDisplay(e.token) \\\n          + \" expecting set \" \\\n          + repr(e.expecting)\n\nelif isinstance(e, MismatchedNotSetException):\n    msg = \"mismatched input \" \\\n          + self.getTokenErrorDisplay(e.token) \\\n          + \" expecting set \" \\\n          + repr(e.expecting)\n\nelif isinstance(e, FailedPredicateException):\n    msg = \"rule \" \\\n          + e.ruleName \\\n          + \" failed predicate: {\" \\\n          + e.predicateText \\\n          + \"}?\"\n\n\nreturn msg", "path": "antlr3\\recognizers.py", "repo_name": "cheshire/antlr3-python-example", "stars": 1, "license": "None", "language": "python", "size": 2195}
{"docstring": "\"\"\"Attempt to recover from a single missing or extra token.\n\nEXTRA TOKEN\n\nLA(1) is not what we are looking for.  If LA(2) has the right token,\nhowever, then assume LA(1) is some extra spurious token.  Delete it\nand LA(2) as if we were doing a normal match(), which advances the\ninput.\n\nMISSING TOKEN\n\nIf current token is consistent with what could come after\nttype then it is ok to \"insert\" the missing token, else throw\nexception For example, Input \"i=(3;\" is clearly missing the\n')'.  When the parser returns from the nested call to expr, it\nwill have call chain:\n\n  stat -> expr -> atom\n\nand it will be trying to match the ')' at this point in the\nderivation:\n\n     => ID '=' '(' INT ')' ('+' atom)* ';'\n                        ^\nmatch() will see that ';' doesn't match ')' and report a\nmismatched token error.  To recover, it sees that LA(1)==';'\nis in the set of tokens that can follow the ')' token\nreference in rule atom.  It can assume that you forgot the ')'.\n\"\"\"\n                                 \n# if next token is what we are looking for then \"delete\" this token\n", "func_signal": "def recoverFromMismatchedToken(self, input, e, ttype, follow):\n", "code": "if input.LA(2) == ttype:\n    self.reportError(e)\n\n    self.beginResync()\n    input.consume() # simply delete extra token\n    self.endResync()\n    input.consume()  # move past ttype token as if all were ok\n    return\n\nif not self.recoverFromMismatchedElement(input, e, follow):\n    raise e", "path": "antlr3\\recognizers.py", "repo_name": "cheshire/antlr3-python-example", "stars": 1, "license": "None", "language": "python", "size": 2195}
{"docstring": "\"\"\"\nMatch current input symbol against ttype.  Upon error, do one token\ninsertion or deletion if possible.  You can override to not recover\n\there and bail out of the current production to the normal error\n\texception catch (at the end of the method) by just throwing\n\tMismatchedTokenException upon input.LA(1)!=ttype.\n\"\"\"\n\n", "func_signal": "def match(self, input, ttype, follow):\n", "code": "if self.input.LA(1) == ttype:\n    self.input.consume()\n    self.errorRecovery = False\n    self.failed = False\n    return\n\nif self.backtracking > 0:\n    self.failed = True\n    return\n\nself.mismatch(input, ttype, follow)\nreturn", "path": "antlr3\\recognizers.py", "repo_name": "cheshire/antlr3-python-example", "stars": 1, "license": "None", "language": "python", "size": 2195}
{"docstring": "#return \"MismatchedTokenException(\"+self.expecting+\")\"\n", "func_signal": "def __str__(self):\n", "code": "return \"MismatchedTokenException(%r!=%r)\" % (\n    self.getUnexpectedType(), self.expecting\n    )", "path": "antlr3\\exceptions.py", "repo_name": "cheshire/antlr3-python-example", "stars": 1, "license": "None", "language": "python", "size": 2195}
{"docstring": "\"\"\"\nHas this rule already parsed input at the current index in the\ninput stream?  Return the stop token index or MEMO_RULE_UNKNOWN.\nIf we attempted but failed to parse properly before, return\nMEMO_RULE_FAILED.\n\nThis method has a side-effect: if we have seen this input for\nthis rule and successfully parsed before, then seek ahead to\n1 past the stop token matched for this rule last time.\n\"\"\"\n\n", "func_signal": "def alreadyParsedRule(self, input, ruleIndex):\n", "code": "stopIndex = self.getRuleMemoization(ruleIndex, input.index())\nif stopIndex == self.MEMO_RULE_UNKNOWN:\n    return False\n\nif stopIndex == self.MEMO_RULE_FAILED:\n    self.failed = True\n\nelse:\n    input.seek(stopIndex + 1)\n\nreturn True", "path": "antlr3\\recognizers.py", "repo_name": "cheshire/antlr3-python-example", "stars": 1, "license": "None", "language": "python", "size": 2195}
{"docstring": "\"\"\"\nReturn the text matched so far for the current token or any\ntext override.\n\"\"\"\n", "func_signal": "def getText(self):\n", "code": "if self._text is not None:\n    return self._text\n\nreturn self.input.substring(\n    self.tokenStartCharIndex,\n    self.getCharIndex()-1\n    )", "path": "antlr3\\recognizers.py", "repo_name": "cheshire/antlr3-python-example", "stars": 1, "license": "None", "language": "python", "size": 2195}
{"docstring": "\"\"\"\nThis code is factored out from mismatched token and mismatched set\nrecovery.  It handles \"single token insertion\" error recovery for\nboth.  No tokens are consumed to recover from insertions.  Return\ntrue if recovery was possible else return false.\n\"\"\"\n\n", "func_signal": "def recoverFromMismatchedElement(self, input, e, follow):\n", "code": "if follow is None:\n    # we have no information about the follow; we can only consume\n    # a single token and hope for the best\n    return False\n\n# compute what can follow this grammar element reference\nif EOR_TOKEN_TYPE in follow:\n    viableTokensFollowingThisRule = \\\n        self.computeContextSensitiveRuleFOLLOW()\n    \n    follow = (follow | viableTokensFollowingThisRule) \\\n             - set([EOR_TOKEN_TYPE])\n\n# if current token is consistent with what could come after set\n# then it is ok to \"insert\" the missing token, else throw exception\nif input.LA(1) in follow:\n    self.reportError(e)\n    return True\n\n# nothing to do; throw exception\nreturn False", "path": "antlr3\\recognizers.py", "repo_name": "cheshire/antlr3-python-example", "stars": 1, "license": "None", "language": "python", "size": 2195}
{"docstring": "\"\"\"\nfactor out what to do upon token mismatch so tree parsers can behave\ndifferently.  Override this method in your parser to do things\n\tlike bailing out after the first error; just throw the mte object\n\tinstead of calling the recovery method.\n\"\"\"\n\n", "func_signal": "def mismatch(self, input, ttype, follow):\n", "code": "mte = MismatchedTokenException(ttype, input)\nself.recoverFromMismatchedToken(input, mte, ttype, follow)", "path": "antlr3\\recognizers.py", "repo_name": "cheshire/antlr3-python-example", "stars": 1, "license": "None", "language": "python", "size": 2195}
{"docstring": "\"\"\"Report a recognition problem.\n    \nThis method sets errorRecovery to indicate the parser is recovering\nnot parsing.  Once in recovery mode, no errors are generated.\nTo get out of recovery mode, the parser must successfully match\na token (after a resync).  So it will go:\n\n-# error occurs\n-# enter recovery mode, report error\n-# consume until token found in resynch set\n-# try to resume parsing\n-# next match() will reset errorRecovery mode\n.\n\n\"\"\"\n\n# if we've already reported an error and have not matched a token\n# yet successfully, don't report any errors.\n", "func_signal": "def reportError(self, e):\n", "code": "if self.errorRecovery:\n    return\n\nself.errorRecovery = True\n\nself.displayRecognitionError(self.tokenNames, e)", "path": "antlr3\\recognizers.py", "repo_name": "cheshire/antlr3-python-example", "stars": 1, "license": "None", "language": "python", "size": 2195}
{"docstring": "\"\"\"\nThe standard method called to automatically emit a token at the\n\toutermost lexical rule.  The token object should point into the\n\tchar buffer start..stop.  If there is a text override in 'text',\n\tuse that to set the token's text.\n\t\"\"\"\n\n", "func_signal": "def emit(self, token=None):\n", "code": "if token is None:\n    token = CommonToken(\n        input=self.input,\n        type=self.type,\n        channel=self.channel,\n        start=self.tokenStartCharIndex,\n        stop=self.getCharIndex()-1\n        )\n    token.line = self.tokenStartLine\n    token.text = self.text\n    token.charPositionInLine = self.tokenStartCharPositionInLine\n\nself.token = token\n\nreturn token", "path": "antlr3\\recognizers.py", "repo_name": "cheshire/antlr3-python-example", "stars": 1, "license": "None", "language": "python", "size": 2195}
{"docstring": "\"\"\"Set the token stream and reset the parser\"\"\"\n\n", "func_signal": "def setTokenStream(self, input):\n", "code": "self.input = None\nself.reset()\nself.input = input", "path": "antlr3\\recognizers.py", "repo_name": "cheshire/antlr3-python-example", "stars": 1, "license": "None", "language": "python", "size": 2195}
{"docstring": "\"\"\" If the router went to an invalid action \"\"\"\n", "func_signal": "def invalid_action(self):\n", "code": "self.hnd.error(404)\nself.render(text=\"Invalid action\")", "path": "gaeo\\controller\\__init__.py", "repo_name": "chihchun/bebebe", "stars": 1, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\" according to self.params['format'] to respond appropriate stuff\n\"\"\"\n", "func_signal": "def respond_to(self, **blk):\n", "code": "if self.params.has_key('format') and blk.has_key(self.params['format']):\n    logging.error(self.params['format'])\n    blk[self.params['format']]()", "path": "gaeo\\controller\\__init__.py", "repo_name": "chihchun/bebebe", "stars": 1, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\ncreate a many-to-one relation\n\"\"\"\n", "func_signal": "def belongs_to(klz, col_name = \"\"):\n", "code": "if col_name:\n    return db.Reference(klz, collection_name=col_name)\nelse:\n    return db.Reference(klz)", "path": "gaeo\\model\\__init__.py", "repo_name": "chihchun/bebebe", "stars": 1, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"set the specified properties, but not update\"\"\"\n\n# Issue #3\n", "func_signal": "def set_attributes(self, kwd_dict = {}, **kwds):\n", "code": "if kwd_dict:\n    kwd_dict.update(kwds)\n    kwds = kwd_dict\n\nprops = self.properties()\nfor prop in props.values():\n    if prop.name in kwds:\n        prop.__set__(self, kwds[prop.name])", "path": "gaeo\\model\\__init__.py", "repo_name": "chihchun/bebebe", "stars": 1, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\" Resolve the url to the correct mapping \"\"\"\n\n", "func_signal": "def resolve(self, url):\n", "code": "if url == '/':\n    return self.__routing_root\n\nret = self.__resolve_by_table(url, self.__routing_table)\nif ret is None: # fallback\n    ret = self.__resolve_by_table(url, self.__routing_table_fallback)\nreturn ret", "path": "gaeo\\dispatch\\router.py", "repo_name": "chihchun/bebebe", "stars": 1, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\" Return a link that contains onclick scripts \"\"\"\n", "func_signal": "def link_to_function(title, script, html = {}):\n", "code": "html_part = ''\nif html:\n    for key, val in html.items():\n        space = ' ' if html_part else ''\n        html_part = '%s%s=\"%s\"' % (space, key, val)\n    \nreturn '<a href=\"#\" %s onclick=\"%s;return false;\">%s</a>' % (html_part, script, title)", "path": "gaeo\\view\\helper\\ajax.py", "repo_name": "chihchun/bebebe", "stars": 1, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\" Periodically call a remote action. (jQuery-based) \"\"\"\n", "func_signal": "def periodically_call_remote(url, **opts):\n", "code": "if not url:\n    raise Exception('Missing remote action url.')\n\n# frequency, default 1000 ms\nfreq = opts.get('frequency', 1000)\nscript = \"setInterval(function(){%s}, %s)\" % (remote_script(url, **opts), freq)\nreturn javascript_tag(script)", "path": "gaeo\\view\\helper\\ajax.py", "repo_name": "chihchun/bebebe", "stars": 1, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"Update the specified properties\"\"\"\n", "func_signal": "def update_attributes(self, kwd_dict = {}, **kwds):\n", "code": "need_change = False\n\n# if user passed a dict, merge to kwds (Issue #3)\nif kwd_dict:\n    kwd_dict.update(kwds)\n    kwds = kwd_dict\n\nprops = self.properties()\nfor prop in props.values():\n    if prop.name in kwds:\n        if not need_change:\n            need_change = True\n        prop.__set__(self, kwds[prop.name])\n\nif need_change:\n    self.update()", "path": "gaeo\\model\\__init__.py", "repo_name": "chihchun/bebebe", "stars": 1, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\" Create a ajax request script \"\"\"\n", "func_signal": "def remote_script(url, **opts):\n", "code": "method = opts.get('method', 'get');\n# request parameters\ndata = opts.get('data', '')\n# callback function\ncallback = opts.get('callback', '')    \ndataType = opts.get('dataType', '')\n\nparams = \"'%s'\" % data if data else ''\nif callback:\n    cbf = 'function(data, textStatus){%s}' % callback\n    params = params + ', %s' % cbf if params else cbf\nparams = params + \", '%s'\" % dataType if dataType else params\nif params:\n    params = ', ' + params\n\nreturn \"\"\"$.%s('%s'%s)\"\"\" % (method, url, params)", "path": "gaeo\\view\\helper\\ajax.py", "repo_name": "chihchun/bebebe", "stars": 1, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\" Convert a dict to JSON. Inspired from SimpleJSON \"\"\"\n", "func_signal": "def to_json(self, obj, **kwds):\n", "code": "from gaeo.controller.jsonencoder import JSONEncoder\n\nif not kwds:\n    return JSONEncoder(skipkeys=False, ensure_ascii=True, check_circular=True,\n        allow_nan=True, indent=None, separators=None, encoding='utf-8',\n        default=None).encode(obj)\nelse:\n    return JSONEncoder(\n        skipkeys=False, ensure_ascii=True, check_circular=True,\n        allow_nan=True, indent=None, separators=None,\n        encoding='utf-8', default=None, **kwds).encode(obj)", "path": "gaeo\\controller\\__init__.py", "repo_name": "chihchun/bebebe", "stars": 1, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\" Set the root (/) routing... \"\"\"\n", "func_signal": "def root(self, **map):\n", "code": "self.__routing_root['controller'] = \\\n    map.get('controller', self.__routing_root['controller'])\nself.__routing_root['action'] = \\\n    map.get('action', self.__routing_root['action'])", "path": "gaeo\\dispatch\\router.py", "repo_name": "chihchun/bebebe", "stars": 1, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\" Create a link that request a remote action (jQuery-based) \"\"\"\n# check if the title is specified\n", "func_signal": "def link_to_remote(title, url, **opts):\n", "code": "if not title:\n    raise Exception('Missing title of the link.')\n\n# remote action\nif not url:\n    raise Exception('Missing remote action.')\n\nscript = remote_script(url, **opts)    \nreturn link_to_function(title, script, opts.get('html', {}))", "path": "gaeo\\view\\helper\\ajax.py", "repo_name": "chihchun/bebebe", "stars": 1, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\" for detecting android \"\"\"\n", "func_signal": "def __detect_android(self):\n", "code": "ua = self.request.headers.get('User-Agent')\nif ua:\n    ua = ua.lower();\n    return ua.find('android') > -1\nelse:\n    return False", "path": "gaeo\\controller\\__init__.py", "repo_name": "chihchun/bebebe", "stars": 1, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nCreate a named query.\n\"\"\"\n", "func_signal": "def named_query(order_by=None, **conds):\n", "code": "cond_str = \"WHERE \"\nfor cond in conds.iterkeys():\n    if len(cond_str) > 6:\n        cond_str += ' AND '\n    cond_str += '%s %s' % (cond, conds[cond])\n\nif order_by:\n    cond_str += ' ORDER BY %s' % order_by\n\nreturn property(lambda self: cls.gql(cond_str))", "path": "gaeo\\model\\__init__.py", "repo_name": "chihchun/bebebe", "stars": 1, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\" for detecting iPhone/iPod \"\"\"\n", "func_signal": "def __detect_iphone(self):\n", "code": "ua = self.request.headers.get('User-Agent')\nif ua:\n    ua = ua.lower();\n    return ua.find('iphone') > -1 or ua.find('ipod') > -1\nelse:\n    return False", "path": "gaeo\\controller\\__init__.py", "repo_name": "chihchun/bebebe", "stars": 1, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\" Resolve url by the given table \"\"\"\n", "func_signal": "def __resolve_by_table(self, url, rules):\n", "code": "for r in rules:\n    ret = r.match_url(url)\n    if ret:\n        return ret\nreturn None", "path": "gaeo\\dispatch\\router.py", "repo_name": "chihchun/bebebe", "stars": 1, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"The Session's constructor.\n\n@param hnd      The webapp.ReqeustHanlder object.\n@param name     The session name.\n@param timeout  The time interval (in sec) that the session expired.\n\"\"\"\n\n", "func_signal": "def __init__(self, hnd, name, timeout):\n", "code": "dict.__init__(self)\nself._name = name\nself._hnd = hnd\nself._timeout = timeout\nself._id = ''.join([ choice(POOL) for i in range(SESSIONID_LEN) ])\n\nself._invalidated = False", "path": "gaeo\\session\\__init__.py", "repo_name": "chihchun/bebebe", "stars": 1, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\" Add routing pattern \"\"\"\n\n", "func_signal": "def connect(self, pattern, **tbl):\n", "code": "rule = Rule(pattern, **tbl)\nif rule not in self.__routing_table:\n    self.__routing_table.append(rule)", "path": "gaeo\\dispatch\\router.py", "repo_name": "chihchun/bebebe", "stars": 1, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\" Create a link that load data from a remote action (jQuery-based) \"\"\"\n# check if the title is specified\n", "func_signal": "def load_from_remote(title, url, target, **opts):\n", "code": "if not title:\n    raise Exception('Missing title of the link.')\n\n# remote action url\nif not url:\n    raise Exception('Missing remote action.')\n    \n# load target #id\nif not target:\n    raise Exception('Missing the id of loaded data target.')\n    \ndata = opts.get('data', '')\ncallback = opts.get('callback', '')    \nparams = data\nif callback:\n    cbf = 'function(){%s}' % callback\n    params = '%s,%s' % (params, cbf) if params else cbf\n\nparams = ', ' + params if params else ''\nscript = \"\"\"$('#%s').load('%s'%s);\"\"\" % (target, url, params)\n\nreturn link_to_function(title, script, opts.get('html', {}))", "path": "gaeo\\view\\helper\\ajax.py", "repo_name": "chihchun/bebebe", "stars": 1, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\" Declare a many-to-one relationship \"\"\"\n", "func_signal": "def belongs_to(cls, ref_cls, reference_name = \"\", collection_name = \"\"):\n", "code": "if ref_cls is None:\n    raise Exception('No referenced class')\n\nref_name = reference_name or ref_cls.__name__.lower()\nif ref_name not in cls._properties:\n    col_name = collection_name or pluralize(cls.__name__.lower())\n    attr = db.ReferenceProperty(ref_cls, collection_name=col_name)\n    cls._properties[ref_name] = attr\n    attr.__property_config__(cls, ref_name)", "path": "gaeo\\model\\__init__.py", "repo_name": "chihchun/bebebe", "stars": 1, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"Overridden. Return this row's data for a certain column, with\ncustom handling for model tables.\n\"\"\"\n\n# find the column for the requested field, for reference\n", "func_signal": "def __getitem__(self, name):\n", "code": "boundcol = self.table._columns[name]\n\n# If the column has a name override (we know then that is was also\n# used for access, e.g. if the condition is true, then\n# ``boundcol.column.name == name``), we need to make sure we use the\n# declaration name to access the model field.\nif boundcol.column.data:\n    if callable(boundcol.column.data):\n        result = boundcol.column.data(self)\n        if not result:\n            if boundcol.column.default is not None:\n                return boundcol.get_default(self)\n        return result\n    else:\n        name = boundcol.column.data\nelse:\n    name = boundcol.declared_name\n\n\n# try to resolve relationships spanning attributes\nbits = name.split('__')\ncurrent = self.data\nfor bit in bits:\n    # note the difference between the attribute being None and not\n    # existing at all; assume \"value doesn't exist\" in the former\n    # (e.g. a relationship has no value), raise error in the latter.\n    # a more proper solution perhaps would look at the model meta\n    # data instead to find out whether a relationship is valid; see\n    # also ``_validate_column_name``, where such a mechanism is\n    # already implemented).\n    if not hasattr(current, bit):\n        raise ValueError(\"Could not resolve %s from %s\" % (bit, name))\n\n    current = getattr(current, bit)\n    if callable(current):\n        current = current()\n    # important that we break in None case, or a relationship\n    # spanning across a null-key will raise an exception in the\n    # next iteration, instead of defaulting.\n    if current is None:\n        break\n\nif current is None:\n    # ...the whole name (i.e. the last bit) resulted in None\n    if boundcol.column.default is not None:\n        return boundcol.get_default(self)\nreturn current", "path": "django_tables\\models.py", "repo_name": "dantium/django-tables", "stars": 1, "license": "bsd-2-clause", "language": "python", "size": 148}
{"docstring": "\"\"\"Specific tests for sortable table meta option.\"\"\"\n\n", "func_signal": "def test_meta_sortable():\n", "code": "def mktable(default_sortable):\n    class BookTable(tables.Table):\n        id = tables.Column(sortable=True)\n        name = tables.Column(sortable=False)\n        author = tables.Column()\n        class Meta:\n            sortable = default_sortable\n    return BookTable([])\n\nglobal_table = mktable(None)\nfor default_sortable, results in (\n    (None,      (True, False, True)),    # last bool is global default\n    (True,      (True, False, True)),    # last bool is table default\n    (False,     (True, False, False)),   # last bool is table default\n):\n    books = mktable(default_sortable)\n    assert [c.sortable for c in books.columns] == list(results)\n\n    # it also works if the meta option is manually changed after\n    # class and instance creation\n    global_table._meta.sortable = default_sortable\n    assert [c.sortable for c in global_table.columns] == list(results)", "path": "tests\\test_basic.py", "repo_name": "dantium/django-tables", "stars": 1, "license": "bsd-2-clause", "language": "python", "size": 148}
{"docstring": "\"\"\"Test relationship spanning.\"\"\"\n\n", "func_signal": "def test_relationships():\n", "code": "class CountryTable(tables.ModelTable):\n    # add relationship spanning columns (using different approaches)\n    capital_name = tables.Column(data='capital__name')\n    capital__population = tables.Column(name=\"capital_population\")\n    invalid = tables.Column(data=\"capital__invalid\")\n    class Meta:\n        model = Country\ncountries = CountryTable(Country.objects.select_related('capital'))\n\n# ordering and field access works\ncountries.order_by = 'capital_name'\nassert [row['capital_name'] for row in countries.rows] == \\\n    [None, None, 'Amsterdam', 'Berlin']\n\ncountries.order_by = 'capital_population'\nassert [row['capital_population'] for row in countries.rows] == \\\n    [None, None, None, None]\n\n# ordering by a column with an invalid relationship fails silently\ncountries.order_by = 'invalid'\nassert countries.order_by == ()", "path": "tests\\test_models.py", "repo_name": "dantium/django-tables", "stars": 1, "license": "bsd-2-clause", "language": "python", "size": 148}
{"docstring": "\"\"\"Use the queryset count() method to get the length, instead of\nloading all results into memory. This allows, for example,\nsmart paginators that use len() to perform better.\n\"\"\"\n", "func_signal": "def __len__(self):\n", "code": "if getattr(self, '_length', None) is None:\n    self._length = self.table.data.count()\nreturn self._length", "path": "django_tables\\models.py", "repo_name": "dantium/django-tables", "stars": 1, "license": "bsd-2-clause", "language": "python", "size": 148}
{"docstring": "\"\"\"Pretty much the same as static table pagination, but make sure we\nprovide the capability, at least for paginators that use it, to not\nhave the complete queryset loaded (by use of a count() query).\n\nNote: This test changes the available cities, make sure it is last,\nor that tests that follow are written appropriately.\n\"\"\"\n", "func_signal": "def test_pagination():\n", "code": "from django.db import connection\n\nclass CityTable(tables.ModelTable):\n    class Meta:\n        model = City\n        columns = ['name']\ncities = CityTable()\n\n# add some sample data\nCity.objects.all().delete()\nfor i in range(1,101):\n    City.objects.create(name=\"City %d\"%i)\n\n# for query logging\nsettings.DEBUG = True\n\n# external paginator\nstart_querycount = len(connection.queries)\npaginator = Paginator(cities.rows, 10)\nassert paginator.num_pages == 10\npage = paginator.page(1)\nassert len(page.object_list) == 10\nassert page.has_previous() == False\nassert page.has_next() == True\n# Make sure the queryset is not loaded completely - there must be two\n# queries, one a count(). This check is far from foolproof...\nassert len(connection.queries)-start_querycount == 2\n\n# using a queryset paginator is possible as well (although unnecessary)\npaginator = QuerySetPaginator(cities.rows, 10)\nassert paginator.num_pages == 10\n\n# integrated paginator\nstart_querycount = len(connection.queries)\ncities.paginate(Paginator, 10, page=1)\n# rows is now paginated\nassert len(list(cities.rows.page())) == 10\nassert len(list(cities.rows.all())) == 100\n# new attributes\nassert cities.paginator.num_pages == 10\nassert cities.page.has_previous() == False\nassert cities.page.has_next() == True\nassert len(connection.queries)-start_querycount == 2\n\n# reset\nsettings.DEBUG = False", "path": "tests\\test_models.py", "repo_name": "dantium/django-tables", "stars": 1, "license": "bsd-2-clause", "language": "python", "size": 148}
{"docstring": "\"\"\"Some of the callable code is reimplemented for modeltables, so\ntest some specifics again.\n\"\"\"\n\n", "func_signal": "def test_callable():\n", "code": "class CountryTable(tables.ModelTable):\n    null = tables.Column(default=lambda s: s['example_domain'])\n    example_domain = tables.Column()\n    class Meta:\n        model = Country\ncountries = CountryTable(Country)\n\n# model method is called\nassert [row['example_domain'] for row in countries] == \\\n                ['example.'+row['tld'] for row in countries]\n\n# column default method is called\nassert [row['example_domain'] for row in countries] == \\\n                [row['null'] for row in countries]", "path": "tests\\test_models.py", "repo_name": "dantium/django-tables", "stars": 1, "license": "bsd-2-clause", "language": "python", "size": 148}
{"docstring": "\"\"\"Data fields, ``default`` and ``data`` options can be callables.\n\"\"\"\n\n", "func_signal": "def test_callable():\n", "code": "class MathTable(tables.Table):\n    lhs = tables.Column()\n    rhs = tables.Column()\n    op = tables.Column(default='+')\n    sum = tables.Column(default=lambda d: calc(d['op'], d['lhs'], d['rhs']))\n    sqrt = tables.Column(data=lambda d: int(sqrt(d['sum'])))\n\nmath = MathTable([\n    {'lhs': 1, 'rhs': lambda x: x['lhs']*3},              # 1+3\n    {'lhs': 9, 'rhs': lambda x: x['lhs'], 'op': '/'},     # 9/9\n    {'lhs': lambda x: x['rhs']+3, 'rhs': 4, 'op': '-'},   # 7-4\n])\n\n# function is called when queried\ndef calc(op, lhs, rhs):\n    if op == '+': return lhs+rhs\n    elif op == '/': return lhs/rhs\n    elif op == '-': return lhs-rhs\nassert [calc(row['op'], row['lhs'], row['rhs']) for row in math] == [4,1,3]\n\n# field function is called while sorting\nmath.order_by = ('-rhs',)\nassert [row['rhs'] for row in math] == [9,4,3]\n\n# default function is called while sorting\nmath.order_by = ('sum',)\nassert [row['sum'] for row in math] == [1,3,4]\n\n# data function is called while sorting\nmath.order_by = ('sqrt',)\nassert [row['sqrt'] for row in math] == [1,1,2]", "path": "tests\\test_basic.py", "repo_name": "dantium/django-tables", "stars": 1, "license": "bsd-2-clause", "language": "python", "size": 148}
{"docstring": "\"\"\"Test Table.columns container functionality.\n\"\"\"\n\n", "func_signal": "def test_columns():\n", "code": "class BookTable(tables.Table):\n    id = tables.Column(sortable=False, visible=False)\n    name = tables.Column(sortable=True)\n    pages = tables.Column(sortable=True)\n    language = tables.Column(sortable=False)\nbooks = BookTable([])\n\nassert list(books.columns.sortable()) == [c for c in books.columns if c.sortable]\n\n# .columns iterator only yields visible columns\nassert len(list(books.columns)) == 3\n# visiblity of columns can be changed at instance-time\nbooks.columns['id'].visible = True\nassert len(list(books.columns)) == 4", "path": "tests\\test_basic.py", "repo_name": "dantium/django-tables", "stars": 1, "license": "bsd-2-clause", "language": "python", "size": 148}
{"docstring": "# so we don't depend on setuptools; from the Storm ORM setup.py\n", "func_signal": "def find_packages(root):\n", "code": "packages = []\nfor directory, subdirectories, files in os.walk(root):\n    if '__init__.py' in files:\n        packages.append(directory.replace(os.sep, '.'))\nreturn packages", "path": "setup.py", "repo_name": "dantium/django-tables", "stars": 1, "license": "bsd-2-clause", "language": "python", "size": 148}
{"docstring": "\"\"\"\nReturns a ``SortedDict`` containing form columns for the given model.\n\n``columns`` is an optional list of field names. If provided, only the\nnamed model fields will be included in the returned column list.\n\n``exclude`` is an optional list of field names. If provided, the named\nmodel fields will be excluded from the returned list of columns, even\nif they are listed in the ``fields`` argument.\n\"\"\"\n\n", "func_signal": "def columns_for_model(model, columns=None, exclude=None):\n", "code": "field_list = []\nopts = model._meta\nfor f in opts.fields + opts.many_to_many:\n    if (columns and not f.name in columns) or \\\n       (exclude and f.name in exclude):\n        continue\n    column = Column() # TODO: chose correct column type, with right options\n    if column:\n        field_list.append((f.name, column))\nreturn SortedDict(field_list)", "path": "django_tables\\models.py", "repo_name": "dantium/django-tables", "stars": 1, "license": "bsd-2-clause", "language": "python", "size": 148}
{"docstring": "\"\"\"\nTest defining tables by declaration.\n\"\"\"\n\n", "func_signal": "def test_declaration():\n", "code": "class GeoAreaTable(tables.Table):\n    name = tables.Column()\n    population = tables.Column()\n\nassert len(GeoAreaTable.base_columns) == 2\nassert 'name' in GeoAreaTable.base_columns\nassert not hasattr(GeoAreaTable, 'name')\n\nclass CountryTable(GeoAreaTable):\n    capital = tables.Column()\n\nassert len(CountryTable.base_columns) == 3\nassert 'capital' in CountryTable.base_columns\n\n# multiple inheritance\nclass AddedMixin(tables.Table):\n    added = tables.Column()\nclass CityTable(GeoAreaTable, AddedMixin):\n    mayer = tables.Column()\n\nassert len(CityTable.base_columns) == 4\nassert 'added' in CityTable.base_columns\n\n# modelforms: support switching from a non-model table hierarchy to a\n# modeltable hierarchy (both base class orders)\nclass StateTable1(tables.ModelTable, GeoAreaTable):\n    motto = tables.Column()\nclass StateTable2(GeoAreaTable, tables.ModelTable):\n    motto = tables.Column()\n\nassert len(StateTable1.base_columns) == len(StateTable2.base_columns) == 3\nassert 'motto' in StateTable1.base_columns\nassert 'motto' in StateTable2.base_columns", "path": "tests\\test_basic.py", "repo_name": "dantium/django-tables", "stars": 1, "license": "bsd-2-clause", "language": "python", "size": 148}
{"docstring": "\"\"\"Ensure the various caches are effective.\n\"\"\"\n\n", "func_signal": "def test_caches():\n", "code": "class BookTable(tables.Table):\n    name = tables.Column()\n    answer = tables.Column(default=42)\nbooks = BookTable([\n    {'name': 'Foo: Bar'},\n])\n\nassert id(list(books.columns)[0]) == id(list(books.columns)[0])\n# TODO: row cache currently not used\n#assert id(list(books.rows)[0]) == id(list(books.rows)[0])\n\n# test that caches are reset after an update()\nold_column_cache = id(list(books.columns)[0])\nold_row_cache = id(list(books.rows)[0])\nbooks.update()\nassert id(list(books.columns)[0]) != old_column_cache\nassert id(list(books.rows)[0]) != old_row_cache", "path": "tests\\test_basic.py", "repo_name": "dantium/django-tables", "stars": 1, "license": "bsd-2-clause", "language": "python", "size": 148}
{"docstring": "\"\"\"Overridden. The snapshot in this case is simply a queryset\nwith the necessary filters etc. attached.\n\"\"\"\n\n# reset caches\n", "func_signal": "def _build_snapshot(self):\n", "code": "self._columns._reset()\nself._rows._reset()\n\nqueryset = self.queryset\nif self.order_by:\n    actual_order_by = self._resolve_sort_directions(self.order_by)\n    queryset = queryset.order_by(*self._cols_to_fields(actual_order_by))\nself._snapshot = queryset", "path": "django_tables\\models.py", "repo_name": "dantium/django-tables", "stars": 1, "license": "bsd-2-clause", "language": "python", "size": 148}
{"docstring": "\"\"\"Further test the ``data`` column property in a ModelTable scenario.\nOther tests already touched on this, for example ``test_relationships``.\n\"\"\"\n\n", "func_signal": "def test_column_data():\n", "code": "class CountryTable(tables.ModelTable):\n    name = tables.Column(data=lambda d: \"hidden\")\n    tld = tables.Column(data='example_domain', name=\"domain\")\n    default_and_data = tables.Column(data=lambda d: None, default=4)\n    class Meta:\n        model = Country\ncountries = CountryTable(Country)\n\n# callable data works, even with a default set\nassert [row['default_and_data'] for row in countries] == [4,4,4,4]\n\n# neato trick: a callable data= column is sortable, if otherwise refers\n# to correct model column; can be used to rewrite what is displayed\ncountries.order_by = 'name'\nassert countries.order_by == ('name',)\n# [bug 282964] this trick also works if the callable is an attribute\n# and we refer to it per string, rather than giving a function object\ncountries.order_by = 'domain'\nassert countries.order_by == ('domain',)", "path": "tests\\test_models.py", "repo_name": "dantium/django-tables", "stars": 1, "license": "bsd-2-clause", "language": "python", "size": 148}
{"docstring": "\"\"\"Make sure the caches work for model tables as well (parts are\nreimplemented).\n\"\"\"\n", "func_signal": "def test_caches():\n", "code": "class CountryTable(tables.ModelTable):\n    class Meta:\n        model = Country\n        exclude = ('id',)\ncountries = CountryTable()\n\nassert id(list(countries.columns)[0]) == id(list(countries.columns)[0])\n# TODO: row cache currently not used\n#assert id(list(countries.rows)[0]) == id(list(countries.rows)[0])\n\n# test that caches are reset after an update()\nold_column_cache = id(list(countries.columns)[0])\nold_row_cache = id(list(countries.rows)[0])\ncountries.update()\nassert id(list(countries.columns)[0]) != old_column_cache\nassert id(list(countries.rows)[0]) != old_row_cache", "path": "tests\\test_models.py", "repo_name": "dantium/django-tables", "stars": 1, "license": "bsd-2-clause", "language": "python", "size": 148}
{"docstring": "\"\"\"Overridden. Only allow model-based fields and valid model\nspanning relationships to be sorted.\"\"\"\n\n# let the base class sort out the easy ones\n", "func_signal": "def _validate_column_name(self, name, purpose):\n", "code": "result = super(BaseModelTable, self)._validate_column_name(name, purpose)\nif not result:\n    return False\n\nif purpose == 'order_by':\n    column = self.columns[name]\n\n    # \"data\" can really be used in two different ways. It is\n    # slightly confusing and potentially should be changed.\n    # It can either refer to an attribute/field which the table\n    # column should represent, or can be a callable (or a string\n    # pointing to a callable attribute) that is used to render to\n    # cell. The difference is that in the latter case, there may\n    # still be an actual source model field behind the column,\n    # stored in \"declared_name\". In other words, we want to filter\n    # out column names that are not oderable, and the column name\n    # we need to check may either be stored in \"data\" or in\n    # \"declared_name\", depending on if and what kind of value is\n    # in \"data\". This is the reason why we try twice.\n    #\n    # See also bug #282964.\n    #\n    # TODO: It might be faster to try to resolve the given name\n    # manually recursing the model metadata rather than\n    # constructing a queryset.\n    for lookup in (column.column.data, column.declared_name):\n        if not lookup or callable(lookup):\n            continue\n        try:\n            # let django validate the lookup\n            _temp = self.queryset.order_by(lookup)\n            _temp.query.as_sql()\n            break\n        except FieldError:\n            pass\n    else:\n        return False\n\n# if we haven't failed by now, the column should be valid\nreturn True", "path": "django_tables\\models.py", "repo_name": "dantium/django-tables", "stars": 1, "license": "bsd-2-clause", "language": "python", "size": 148}
{"docstring": "\"\"\"Some tests here are copied from ``test_basic.py`` but need to be\nrerun with a ModelTable, as the implementation is different.\"\"\"\n\n", "func_signal": "def test_basic():\n", "code": "class CountryTable(tables.ModelTable):\n    null = tables.Column(default=\"foo\")\n    tld = tables.Column(name=\"domain\")\n    class Meta:\n        model = Country\n        exclude = ('id',)\ncountries = CountryTable()\n\ndef test_country_table(table):\n    for r in table.rows:\n        # \"normal\" fields exist\n        assert 'name' in r\n        # unknown fields are removed/not accessible\n        assert not 'does-not-exist' in r\n        # ...so are excluded fields\n        assert not 'id' in r\n        # [bug] access to data that might be available, but does not\n        # have a corresponding column is denied.\n        assert_raises(Exception, \"r['id']\")\n        # missing data is available with default values\n        assert 'null' in r\n        assert r['null'] == \"foo\"   # note: different from prev. line!\n        # if everything else fails (no default), we get None back\n        assert r['null2'] is None\n\n        # all that still works when name overrides are used\n        assert not 'tld' in r\n        assert 'domain' in r\n        assert len(r['domain']) == 2   # valid country tld\ntest_country_table(countries)\n\n# repeat the avove tests with a table that is not associated with a\n# model, and all columns being created manually.\nclass CountryTable(tables.ModelTable):\n    name = tables.Column()\n    population = tables.Column()\n    capital = tables.Column()\n    system = tables.Column()\n    null = tables.Column(default=\"foo\")\n    null2 = tables.Column()\n    tld = tables.Column(name=\"domain\")\ncountries = CountryTable(Country)\ntest_country_table(countries)", "path": "tests\\test_models.py", "repo_name": "dantium/django-tables", "stars": 1, "license": "bsd-2-clause", "language": "python", "size": 148}
{"docstring": "# Let the default form meta class get the declared columns; store\n# those in a separate attribute so that ModelTable inheritance with\n# differing models works as expected (the behaviour known from\n# ModelForms).\n", "func_signal": "def __new__(cls, name, bases, attrs):\n", "code": "self = super(ModelTableMetaclass, cls).__new__(\n    cls, name, bases, attrs, parent_cols_from='declared_columns')\nself.declared_columns = self.base_columns\n\nopts = self._meta = ModelTableOptions(getattr(self, 'Meta', None))\n# if a model is defined, then build a list of default columns and\n# let the declared columns override them.\nif opts.model:\n    columns = columns_for_model(opts.model, opts.columns, opts.exclude)\n    columns.update(self.declared_columns)\n    self.base_columns = columns\nreturn self", "path": "django_tables\\models.py", "repo_name": "dantium/django-tables", "stars": 1, "license": "bsd-2-clause", "language": "python", "size": 148}
{"docstring": "\"\"\"Test the order functionality of bound columns.\n\"\"\"\n\n", "func_signal": "def test_column_order():\n", "code": "class BookTable(tables.Table):\n    id = tables.Column()\n    name = tables.Column()\n    pages = tables.Column()\n    language = tables.Column()\nbooks = BookTable([])\n\n# the basic name property is a no-brainer\nbooks.order_by = ''\nassert [c.name for c in books.columns] == ['id','name','pages','language']\n\n# name_reversed will always reverse, no matter what\nfor test in ['', 'name', '-name']:\n    books.order_by = test\n    assert [c.name_reversed for c in books.columns] == ['-id','-name','-pages','-language']\n\n# name_toggled will always toggle\nbooks.order_by = ''\nassert [c.name_toggled for c in books.columns] == ['id','name','pages','language']\nbooks.order_by = 'id'\nassert [c.name_toggled for c in books.columns] == ['-id','name','pages','language']\nbooks.order_by = '-name'\nassert [c.name_toggled for c in books.columns] == ['id','name','pages','language']\n# other columns in an order_by will be dismissed\nbooks.order_by = '-id,name'\nassert [c.name_toggled for c in books.columns] == ['id','-name','pages','language']\n\n# with multi-column order, this is slightly more complex\nbooks.order_by =  ''\nassert [str(c.order_by) for c in books.columns] == ['id','name','pages','language']\nassert [str(c.order_by_reversed) for c in books.columns] == ['-id','-name','-pages','-language']\nassert [str(c.order_by_toggled) for c in books.columns] == ['id','name','pages','language']\nbooks.order_by =  'id'\nassert [str(c.order_by) for c in books.columns] == ['id','id,name','id,pages','id,language']\nassert [str(c.order_by_reversed) for c in books.columns] == ['-id','id,-name','id,-pages','id,-language']\nassert [str(c.order_by_toggled) for c in books.columns] == ['-id','id,name','id,pages','id,language']\nbooks.order_by =  '-pages,id'\nassert [str(c.order_by) for c in books.columns] == ['-pages,id','-pages,id,name','pages,id','-pages,id,language']\nassert [str(c.order_by_reversed) for c in books.columns] == ['-pages,-id','-pages,id,-name','-pages,id','-pages,id,-language']\nassert [str(c.order_by_toggled) for c in books.columns] == ['-pages,-id','-pages,id,name','pages,id','-pages,id,language']\n\n# querying whether a column is ordered is possible\nbooks.order_by = ''\nassert [c.is_ordered for c in books.columns] == [False, False, False, False]\nbooks.order_by = 'name'\nassert [c.is_ordered for c in books.columns] == [False, True, False, False]\nassert [c.is_ordered_reverse for c in books.columns] == [False, False, False, False]\nassert [c.is_ordered_straight for c in books.columns] == [False, True, False, False]\nbooks.order_by = '-pages'\nassert [c.is_ordered for c in books.columns] == [False, False, True, False]\nassert [c.is_ordered_reverse for c in books.columns] == [False, False, True, False]\nassert [c.is_ordered_straight for c in books.columns] == [False, False, False, False]\n# and even works with multi-column ordering\nbooks.order_by = 'id,-pages'\nassert [c.is_ordered for c in books.columns] == [True, False, True, False]\nassert [c.is_ordered_reverse for c in books.columns] == [False, False, True, False]\nassert [c.is_ordered_straight for c in books.columns] == [True, False, False, False]", "path": "tests\\test_basic.py", "repo_name": "dantium/django-tables", "stars": 1, "license": "bsd-2-clause", "language": "python", "size": 148}
{"docstring": "\"\"\"\nTest defining tables by declaration.\n\"\"\"\n\n", "func_signal": "def test_declaration():\n", "code": "class GeoAreaTable(tables.Table):\n    name = tables.Column()\n    population = tables.Column()\n\nassert len(GeoAreaTable.base_columns) == 2\nassert 'name' in GeoAreaTable.base_columns\nassert not hasattr(GeoAreaTable, 'name')\n\nclass CountryTable(GeoAreaTable):\n    capital = tables.Column()\n\nassert len(CountryTable.base_columns) == 3\nassert 'capital' in CountryTable.base_columns\n\n# multiple inheritance\nclass AddedMixin(tables.Table):\n    added = tables.Column()\nclass CityTable(GeoAreaTable, AddedMixin):\n    mayer = tables.Column()\n\nassert len(CityTable.base_columns) == 4\nassert 'added' in CityTable.base_columns\n\n# modelforms: support switching from a non-model table hierarchy to a\n# modeltable hierarchy (both base class orders)\nclass StateTable1(tables.ModelTable, GeoAreaTable):\n    motto = tables.Column()\nclass StateTable2(GeoAreaTable, tables.ModelTable):\n    motto = tables.Column()\n\nassert len(StateTable1.base_columns) == len(StateTable2.base_columns) == 3\nassert 'motto' in StateTable1.base_columns\nassert 'motto' in StateTable2.base_columns\n\n# test more with actual models", "path": "tests\\test.py", "repo_name": "dantium/django-tables", "stars": 1, "license": "bsd-2-clause", "language": "python", "size": 148}
{"docstring": "\"\"\" Test for issue 3 (navigation disappearing in flat page views) \"\"\"\n", "func_signal": "def test_flatpage_navigation(self):\n", "code": "response = self.client.get(\"/about/\")\nself.assertContains(response, \"FAQ\")", "path": "signup\\unit_tests\\test_views.py", "repo_name": "tfgg/cvn.org", "stars": 1, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\" Send a valid invite to an email address, then send an invite again to the same email address \"\"\"\n", "func_signal": "def test_send_invite(self):\n", "code": "invite_form = {'honeypot': '', 'email': m8r_addr['barry'], 'message': 'flump'}\nresponse = self.client.post(\"/invite/\", invite_form, follow=True)\nself.assertTrue(strings.INVITE_NOTICE_SUCCESS in response.content)\nself.assertEquals(len(mail.outbox),1)\nself.assertEquals(mail.outbox[0].subject, strings.INVITE_SUBJECT % m8r_addr['frank'])\nself.assertEquals(Invitation.objects.filter(email=m8r_addr['barry']).count(), 1)\n\ninvite_form = {'honeypot': '', 'email': m8r_addr['barry'], 'message': 'flump'}\nresponse = self.client.post(\"/invite/\", invite_form, follow=True)\nself.assertTrue(strings.INVITE_ERROR_INVITED % m8r_addr['barry'] in response.content)", "path": "invite\\tests.py", "repo_name": "tfgg/cvn.org", "stars": 1, "license": "None", "language": "python", "size": 160}
{"docstring": "# user could put garbage in the search box. this should not explode\n", "func_signal": "def test_postcode_garbage(self):\n", "code": "response = self.client.get(\"/add_constituency/#search\", {\"q\": u\"\\u2603\"})\n# user is still registered in Crewe\nself.assertContains(response, \"Crewe\")\n# there is an error message\nself.assertContains(response, \"can&#39;t find\")", "path": "signup\\unit_tests\\test_views.py", "repo_name": "tfgg/cvn.org", "stars": 1, "license": "None", "language": "python", "size": 160}
{"docstring": "# User's can leave constituencies after they have joined them,\n# potentually leaving them in no constituencies at all. \n\n# user will sign up for Glasgow North\n", "func_signal": "def runTest(self):\n", "code": "Constituency.objects.create(\n    name = \"Glasgow North\",\n    year = this_year)\n\n# user signs up\nresponse = self.client.post(\"/\",\n    {'email':'foo@mailinator.com',\n     'postcode':'G206BT',\n     'can_cc':True,\n     'first_name':'foo',\n     'last_name':'bar'})\nself.assertRedirects(response, \"/\")\n\n# they have a constituency\nuser = CustomUser.objects.get(email=\"foo@mailinator.com\")\nself.assertEquals(1, len(user.current_constituencies))\nresponse = self.client.get(\"/add_constituency/\")\nself.assertContains(response, \"Glasgow North\")\n\n# leaves their constituency\nresponse = self.client.get(\"/delete_constituency/glasgow-north/\")\nself.assertEquals(0, len(user.current_constituencies)) \n\n# should not go boom\nself.client.get(\"/add_constituency/\")", "path": "signup\\unit_tests\\test_views.py", "repo_name": "tfgg/cvn.org", "stars": 1, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"\nCentre and bounding box of constituencies\n\nDon't provide any argument to get all constituencies\n\"\"\"\n", "func_signal": "def getGeometry(name=None):\n", "code": "if name:\n    params = dict(name=name)\nelse:\n    params = dict()\n\nheaders, result = fetch(svcurl(\"getGeometry\", params))\ndata = json.loads(result, encoding=charset(headers))[\"data\"]\nreturn data", "path": "signup\\twfy.py", "repo_name": "tfgg/cvn.org", "stars": 1, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"\nList of constituency's neighbors.\n\nconstituency - name of the constituency you want the neighbors of\nlimit - max number of neighbors to return\n_data - the data to use for calculations (used for testing)\n\"\"\"\n", "func_signal": "def neighbors(constituency, limit=5, _data=None):\n", "code": "if _data == None:\n    _data = twfy.getGeometry()\n\ndata = dict((k, v) for k,v in _data.iteritems()\n            if v.has_key(\"centre_lon\") and v.has_key(\"centre_lat\"))\n\nclat, clng = center(data, constituency)\ndist_to = lambda c: haversine((clat, clng), center(data, c))\ndistance = dict((c, dist_to(c)) for c in data)\nnearest = sorted((c for c in data), key=lambda c: distance[c])\nreturn nearest[1:limit+1]", "path": "signup\\geo.py", "repo_name": "tfgg/cvn.org", "stars": 1, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"\nReturn the twfy api url for a method\n\"\"\"\n", "func_signal": "def svcurl(method, sparams):\n", "code": "p = params.copy()\np.update(sparams)\nreturn service_url + method + \"?\" + urllib.urlencode(p)", "path": "signup\\twfy.py", "repo_name": "tfgg/cvn.org", "stars": 1, "license": "None", "language": "python", "size": 160}
{"docstring": "# rough way of getting the HTML inside the div tag #page-content\n", "func_signal": "def page_content(whole_content):\n", "code": "content = whole_content.split('id=\"main\"')[1]\nreturn content", "path": "signup\\unit_tests\\test_views.py", "repo_name": "tfgg/cvn.org", "stars": 1, "license": "None", "language": "python", "size": 160}
{"docstring": "# there are postcodes with valid formats that are not valid,\n# eg there are no postcodes that begin with D\n", "func_signal": "def test_invalid_postcode(self):\n", "code": "response = self.client.get(\"/add_constituency/#search\", {\"q\": u\"D7 4XP\"})\n# user is still registered in Crewe\nself.assertContains(response, \"Crewe\")\n# there is an error message\nself.assertContains(response, \"can&#39;t find\")", "path": "signup\\unit_tests\\test_views.py", "repo_name": "tfgg/cvn.org", "stars": 1, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\" Prime the fetch cache \"\"\"\n", "func_signal": "def setUp(self):\n", "code": "params = {\"date\": CONSTITUENCY_YEAR.strftime(\"%Y\")}\nurl = twfy.svcurl(\"getConstituencies\", params)\ndata = pickle.load(open(\"signup/unit_tests/twfy.getConstituencies\"))\ntwfy.fetch.prime(url, data)", "path": "signup\\unit_tests\\test_management.py", "repo_name": "tfgg/cvn.org", "stars": 1, "license": "None", "language": "python", "size": 160}
{"docstring": "# user can also search for a placename\n", "func_signal": "def test_placename(self):\n", "code": "east_devon = Constituency.objects.create(\n    name = \"East Devon\",\n    year = this_year)\nresponse = self.client.get(\"/add_constituency/#search\", {\"q\": u\"Sidmouth\"})\n# user is still registered in Crewe\nself.assertContains(response, \"Crewe\")\nself.assertContains(response, \"Devon\")", "path": "signup\\unit_tests\\test_views.py", "repo_name": "tfgg/cvn.org", "stars": 1, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\" Send an invite to an already registered user \"\"\"\n", "func_signal": "def test_send_registered_invite(self):\n", "code": "invite_form = {'honeypot': '', 'email': m8r_addr['frank'], 'message': 'flump'}\nresponse = self.client.post(\"/invite/\", invite_form, follow=True) # Send the invite        \nself.assertTrue(strings.INVITE_ERROR_REGISTERED % m8r_addr['frank'] in response.content)", "path": "invite\\tests.py", "repo_name": "tfgg/cvn.org", "stars": 1, "license": "None", "language": "python", "size": 160}
{"docstring": "# user can enter a postcode into the search box to find a\n# constituency.\n# Example: a user in Crewe may search for a postcode in Hendon\n", "func_signal": "def test_postcode_search(self):\n", "code": "newcastle = Constituency.objects.create(\n    name = \"Hendon\",\n    year = this_year)\nself.assert_(self.client.login(username=\"Frank\", password=\"\"))\n# NW4 is in Hendon\nresponse = self.client.get(\"/add_constituency/#search\", {\"q\":\"NW4 3AS\"})\nself.assertContains(response, \"Hendon\")", "path": "signup\\unit_tests\\test_views.py", "repo_name": "tfgg/cvn.org", "stars": 1, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"\nValidate an activation key and activate the corresponding\n``User`` if valid.\n\"\"\"\n# Make sure the key we're trying conforms to the pattern of a\n# SHA1 hash; if it doesn't, no point trying to look it up in\n# the database.\n", "func_signal": "def activate_user(self, activation_key):\n", "code": "profile = self.get_user(activation_key,\n                        only_activated=False)\nif profile and not profile.activated and \\\n       not profile.activation_key_expired():\n    user = profile.user\n    user.is_active = True\n    user.save()\n    profile.activated = True\n    profile.save()\n    return profile\nelse:\n    return False", "path": "signup\\models.py", "repo_name": "tfgg/cvn.org", "stars": 1, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"\nValidate that the email is not already used by a registered \nuser and has not already been sent an invite.\n\"\"\"\n", "func_signal": "def clean_email(self):\n", "code": "addresses = rfc822.AddressList(self.cleaned_data['email'])\n\nfor (name, email) in addresses.addresslist:\n    user = CustomUser.objects.all()\\\n           .filter(email=email.lower())\n    if user:\n        raise forms.ValidationError(strings.INVITE_ERROR_REGISTERED % email)\n    \n    invite = Invitation.objects.all()\\\n            .filter(email=email.lower())\n    if invite:\n        raise forms.ValidationError(strings.INVITE_ERROR_INVITED % email)\n    \nreturn addresses.addresslist", "path": "invite\\forms.py", "repo_name": "tfgg/cvn.org", "stars": 1, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"\nA list of constituencies\n\nargs - either:\n  date - the list of constituencies as it was on this date\n\nor:\n  latitude, longitude, distance - list of constituencies\n                             within distance of lat, lng\n\"\"\"\n", "func_signal": "def getConstituencies(**kw):\n", "code": "geoargs = (\"latitude\", \"longitude\", \"distance\")\nvalidargs = (\"date\", ) + geoargs\n\ninvalid_args = list(k for k in kw.keys() if k not in validargs)\nif len(invalid_args) > 0:\n    raise ValueError(\"Invalid args %r\" % \",\".join(invalid_args))\n\nif any(kw.has_key(k) for k in geoargs):\n    if not all(kw.has_key(k) for k in geoargs):\n        raise ValueError(\"Need all geoargs\")\n\nparams = dict((k, v) for k,v in kw.items() if v != None)\nheaders, result = fetch(svcurl(\"getConstituencies\", params))\nreturn [x['name'] for x in json.loads(result, encoding=charset(headers))]", "path": "signup\\twfy.py", "repo_name": "tfgg/cvn.org", "stars": 1, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\" Send a multi-address email \"\"\"\n", "func_signal": "def test_send_invite_multi(self):\n", "code": "invite_form = {'honeypot': '', 'email': 'Barry <%s>, Carl <%s>' % (m8r_addr['barry'], m8r_addr['carl']), 'message': 'flump'}\nresponse = self.client.post(\"/invite/\", invite_form, follow=True)\nself.assertTrue(strings.INVITE_NOTICE_SUCCESS in response.content)\nself.assertEquals(len(mail.outbox),2)\nself.assertEquals(mail.outbox[0].subject, strings.INVITE_SUBJECT % m8r_addr['frank'])\nself.assertEquals(Invitation.objects.all().count(), 2)\nself.assertEquals(Invitation.objects.filter(email=m8r_addr['barry']).count(), 1)\nself.assertEquals(Invitation.objects.filter(email=m8r_addr['carl']).count(), 1)", "path": "invite\\tests.py", "repo_name": "tfgg/cvn.org", "stars": 1, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\" Test loading the constituencies \"\"\"\n", "func_signal": "def test_load(self):\n", "code": "command = constituencies.Command()\ncommand.handle('load', silent=True)", "path": "signup\\unit_tests\\test_management.py", "repo_name": "tfgg/cvn.org", "stars": 1, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\" Checks that flatpages work \"\"\"\n", "func_signal": "def test_flatpage(self):\n", "code": "response = self.client.get(\"/about/\")\nself.assertContains(response, \"About\")\nself.assertContains(response, \"This is a flat page\")", "path": "signup\\unit_tests\\test_views.py", "repo_name": "tfgg/cvn.org", "stars": 1, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\" Test the bot honeypot on the invitation form \"\"\"\n", "func_signal": "def test_honeypot(self):\n", "code": "invite_form = {'honeypot': 'BLAH BLAH IM A BOT', 'email': m8r_addr['barry'], 'message': 'flump'}\nresponse = self.client.post(\"/invite/\", invite_form, follow=True)\nself.assertTrue(strings.INVITE_ERROR_HONEYPOT in response.content)", "path": "invite\\tests.py", "repo_name": "tfgg/cvn.org", "stars": 1, "license": "None", "language": "python", "size": 160}
{"docstring": "\"\"\"Send a batch of requests.\n\nBatches are only useful when RPC is supported. Otherwise, all requests\nare sent synchronously. May throw a BadRequest, BadResponse or\nUnauthorizedRequest exceptions.\n\nArgs:\n  batch: The RequestBatch object.\n  use_rest: bool (optional) If True, will just use the REST protocol.\n\n\"\"\"\n", "func_signal": "def send_request_batch(self, batch, use_rest=False):\n", "code": "if not use_rest and self.supports_rpc():\n  self._send_rpc_requests(batch)\nelse:\n  \"\"\"REST protocol does not support batching, so just process each\n  request individually.\n  \"\"\"\n  for key, request in batch.requests.iteritems():\n    result = self._send_rest_request(request)\n    batch._set_data(key, result)", "path": "opensocial\\__init__.py", "repo_name": "iMax-pp/pintme", "stars": 1, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\"Fetches a person by user id.\n\nArgs:\n  user_id: str The person's container-specific id.\n  fields: list (optional) List of fields to retrieve.\n  \nReturns: A Person object representing the specified user id.\n\n\"\"\"\n", "func_signal": "def fetch_person(self, user_id='@me', fields=None):\n", "code": "request = FetchPersonRequest(user_id, fields=fields)\nreturn self.send_request(request)", "path": "opensocial\\__init__.py", "repo_name": "iMax-pp/pintme", "stars": 1, "license": "other", "language": "python", "size": 366}
{"docstring": "# Toggle following a user or not\n# Query: User's account & friend's nickname\n\n", "func_signal": "def post(self, follow_nick):\n", "code": "user_account = AccountIO()\nuser_account.getFromSession()\n\n# Does that user exist?\nfollow_account = AccountIO()\nfollow_account.getFromSession()\nif follow_account.account and user_account.account:\n\tif user_account.isFollowing( follow_account.account ):\n\t\tuser_account.account.following.remove(follow_account.account.key())\n\telse:\n\t\tuser_account.account.following.append(follow_account.account.key())\n\tuser_account.account.put()\n\nself.redirect('/user/'+follow_nick)", "path": "pint\\profile.py", "repo_name": "iMax-pp/pintme", "stars": 1, "license": "other", "language": "python", "size": 366}
{"docstring": "# build the base signature string\n", "func_signal": "def build_signature(self, oauth_request, consumer, token):\n", "code": "key, raw = self.build_signature_base_string(oauth_request, consumer, token)\n\n# hmac object\ntry:\n    import hashlib # 2.5\n    hashed = hmac.new(key, raw, hashlib.sha1)\nexcept:\n    import sha # deprecated\n    hashed = hmac.new(key, raw, sha)\n\n# calculate the digest base 64\nreturn base64.b64encode(hashed.digest())", "path": "opensocial\\oauth\\__init__.py", "repo_name": "iMax-pp/pintme", "stars": 1, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\"Constructor for ContainerContext.\n\nIf a UrlFetch implementation is not given, will attempt to construct\nthe default implementation based on the environment.\n\nArgs:\n  config: The ContainerConfig to use for this connection.\n  url_fetch: (optional) An implementation of the UrlFetch interface.\n\n\"\"\"\n", "func_signal": "def __init__(self, config, url_fetch=None):\n", "code": "self.config = config\nif not self.config:\n  raise ConfigError('Invalid ContainerConfig.')\nself.url_fetch = url_fetch or http.get_default_urlfetch()\nself.oauth_signature_method = oauth.OAuthSignatureMethod_HMAC_SHA1() \nself.oauth_consumer = None\nself.allow_rpc = True\nif self.config.oauth_consumer_key and self.config.oauth_consumer_secret:\n  self.oauth_consumer = oauth.OAuthConsumer(\n      self.config.oauth_consumer_key,\n      self.config.oauth_consumer_secret)", "path": "opensocial\\__init__.py", "repo_name": "iMax-pp/pintme", "stars": 1, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\"\nReturn a JSON string representation of a Python data structure.\n\n>>> JSONEncoder().encode({\"foo\": [\"bar\", \"baz\"]})\n'{\"foo\":[\"bar\", \"baz\"]}'\n\"\"\"\n# This doesn't pass the iterator directly to ''.join() because it\n# sucks at reporting exceptions.  It's going to do this internally\n# anyway because it uses PySequence_Fast or similar.\n", "func_signal": "def encode(self, o):\n", "code": "chunks = list(self.iterencode(o))\nreturn ''.join(chunks)", "path": "opensocial\\simplejson\\encoder.py", "repo_name": "iMax-pp/pintme", "stars": 1, "license": "other", "language": "python", "size": 366}
{"docstring": "# Read JSON POST input to jsonfilter.json if matching mime type\n", "func_signal": "def __call__(self, environ, start_response):\n", "code": "response = {'status': '200 OK', 'headers': []}\ndef json_start_response(status, headers):\n    response['status'] = status\n    response['headers'].extend(headers)\nenviron['jsonfilter.mime_type'] = self.mime_type\nif environ.get('REQUEST_METHOD', '') == 'POST':\n    if environ.get('CONTENT_TYPE', '') == self.mime_type:\n        args = [_ for _ in [environ.get('CONTENT_LENGTH')] if _]\n        data = environ['wsgi.input'].read(*map(int, args))\n        environ['jsonfilter.json'] = simplejson.loads(data)\nres = simplejson.dumps(self.app(environ, json_start_response))\njsonp = cgi.parse_qs(environ.get('QUERY_STRING', '')).get('jsonp')\nif jsonp:\n    content_type = 'text/javascript'\n    res = ''.join(jsonp + ['(', res, ')'])\nelif 'Opera' in environ.get('HTTP_USER_AGENT', ''):\n    # Opera has bunk XMLHttpRequest support for most mime types\n    content_type = 'text/plain'\nelse:\n    content_type = self.mime_type\nheaders = [\n    ('Content-type', content_type),\n    ('Content-length', len(res)),\n]\nheaders.extend(response['headers'])\nstart_response(response['status'], headers)\nreturn [res]", "path": "opensocial\\simplejson\\jsonfilter.py", "repo_name": "iMax-pp/pintme", "stars": 1, "license": "other", "language": "python", "size": 366}
{"docstring": "# -> consumer and token\n", "func_signal": "def verify_request(self, oauth_request):\n", "code": "version = self._get_version(oauth_request)\nconsumer = self._get_consumer(oauth_request)\n# get the access token\ntoken = self._get_token(oauth_request, 'access')\nself._check_signature(oauth_request, consumer, token)\nparameters = oauth_request.get_nonoauth_parameters()\nreturn consumer, token, parameters", "path": "opensocial\\oauth\\__init__.py", "repo_name": "iMax-pp/pintme", "stars": 1, "license": "other", "language": "python", "size": 366}
{"docstring": "# verify that the nonce is uniqueish\n", "func_signal": "def _check_nonce(self, consumer, token, nonce):\n", "code": "nonce = self.data_store.lookup_nonce(consumer, token, nonce)\nif nonce:\n    raise OAuthError('Nonce already used: %s' % str(nonce))", "path": "opensocial\\oauth\\__init__.py", "repo_name": "iMax-pp/pintme", "stars": 1, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\"\nReturn the Python representation of ``s`` (a ``str`` or ``unicode``\ninstance containing a JSON document)\n\"\"\"\n", "func_signal": "def decode(self, s, _w=WHITESPACE.match):\n", "code": "obj, end = self.raw_decode(s, idx=_w(s, 0).end())\nend = _w(s, end).end()\nif end != len(s):\n    raise ValueError(errmsg(\"Extra data\", s, end, len(s)))\nreturn obj", "path": "opensocial\\simplejson\\decoder.py", "repo_name": "iMax-pp/pintme", "stars": 1, "license": "other", "language": "python", "size": 366}
{"docstring": "# concatenate the consumer key and secret\n", "func_signal": "def build_signature_base_string(self, oauth_request, consumer, token):\n", "code": "sig = escape(consumer.secret) + '&'\nif token:\n    sig = sig + escape(token.secret)\nreturn sig", "path": "opensocial\\oauth\\__init__.py", "repo_name": "iMax-pp/pintme", "stars": 1, "license": "other", "language": "python", "size": 366}
{"docstring": "# verify that timestamp is recentish\n", "func_signal": "def _check_timestamp(self, timestamp):\n", "code": "timestamp = int(timestamp)\nnow = int(time.time())\nlapsed = now - timestamp\nif lapsed > self.timestamp_threshold:\n    raise OAuthError('Expired timestamp: given %d and now %s has a greater difference than threshold %d' % (timestamp, now, self.timestamp_threshold))", "path": "opensocial\\oauth\\__init__.py", "repo_name": "iMax-pp/pintme", "stars": 1, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\"\n``encoding`` determines the encoding used to interpret any ``str``\nobjects decoded by this instance (utf-8 by default).  It has no\neffect when decoding ``unicode`` objects.\n\nNote that currently only encodings that are a superset of ASCII work,\nstrings of other encodings should be passed in as ``unicode``.\n\n``object_hook``, if specified, will be called with the result\nof every JSON object decoded and its return value will be used in\nplace of the given ``dict``.  This can be used to provide custom\ndeserializations (e.g. to support JSON-RPC class hinting).\n\"\"\"\n", "func_signal": "def __init__(self, encoding=None, object_hook=None):\n", "code": "self.encoding = encoding\nself.object_hook = object_hook", "path": "opensocial\\simplejson\\decoder.py", "repo_name": "iMax-pp/pintme", "stars": 1, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\"\nReturn a JSON representation of a Python string\n\"\"\"\n", "func_signal": "def encode_basestring(s):\n", "code": "def replace(match):\n    return ESCAPE_DCT[match.group(0)]\nreturn '\"' + ESCAPE.sub(replace, s) + '\"'", "path": "opensocial\\simplejson\\encoder.py", "repo_name": "iMax-pp/pintme", "stars": 1, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\"\nDecode a JSON document from ``s`` (a ``str`` or ``unicode`` beginning\nwith a JSON document) and return a 2-tuple of the Python\nrepresentation and the index in ``s`` where the document ended.\n\nThis can be used to decode a JSON document from a string that may\nhave extraneous data at the end.\n\"\"\"\n", "func_signal": "def raw_decode(self, s, **kw):\n", "code": "kw.setdefault('context', self)\ntry:\n    obj, end = self._scanner.iterscan(s, **kw).next()\nexcept StopIteration:\n    raise ValueError(\"No JSON object could be decoded\")\nreturn obj, end", "path": "opensocial\\simplejson\\decoder.py", "repo_name": "iMax-pp/pintme", "stars": 1, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\"Sends the request.\n\nMay throw a BadRequestError, BadResponseError or \nUnauthorizedRequestError exceptions.\n\nArgs:\n  request: A Request object.\n  use_rest: bool (optional) If True, will just use the REST protocol.\n  \nReturns: The OpenSocial object returned from the container.\n\n\"\"\"\n", "func_signal": "def send_request(self, request, use_rest=False):\n", "code": "if not use_rest and self.supports_rpc():\n  batch = RequestBatch()\n  batch.add_request(0, request)\n  batch.send(self)\n  return batch.get(0)\nelse:\n  return self._send_rest_request(request)", "path": "opensocial\\__init__.py", "repo_name": "iMax-pp/pintme", "stars": 1, "license": "other", "language": "python", "size": 366}
{"docstring": "# combine multiple parameter sources\n", "func_signal": "def from_request(http_method, http_url, headers=None, parameters=None, query_string=None):\n", "code": "if parameters is None:\n    parameters = {}\n\n# headers\nif headers and 'Authorization' in headers:\n    auth_header = headers['Authorization']\n    # check that the authorization header is OAuth\n    if auth_header.index('OAuth') > -1:\n        try:\n            # get the parameters from the header\n            header_params = OAuthRequest._split_header(auth_header)\n            parameters.update(header_params)\n        except:\n            raise OAuthError('Unable to parse OAuth parameters from Authorization header.')\n\n# GET or POST query string\nif query_string:\n    query_params = OAuthRequest._split_url_string(query_string)\n    parameters.update(query_params)\n\n# URL parameters\nparam_str = urlparse.urlparse(http_url)[4] # query\nurl_params = OAuthRequest._split_url_string(param_str)\nparameters.update(url_params)\n\nif parameters:\n    return OAuthRequest(http_method, http_url, parameters)\n\nreturn None", "path": "opensocial\\oauth\\__init__.py", "repo_name": "iMax-pp/pintme", "stars": 1, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\" If status code \"OK\", then we can safely inspect the returned JSON.\"\"\"\n", "func_signal": "def _handle_response(self, http_response):\n", "code": "if http_response.status == httplib.OK:\n  json = simplejson.loads(http_response.content)\n  # Check for any JSON-RPC 2.0 errors.\n  if 'code' in json:\n    code = json.get('code')\n    if code == httplib.UNAUTHORIZED:\n      raise UnauthorizedRequestError(http_response)\n    else:\n      raise BadResponseError(http_response)\n  return json\nelse:\n  raise BadRequestError(http_response)", "path": "opensocial\\__init__.py", "repo_name": "iMax-pp/pintme", "stars": 1, "license": "other", "language": "python", "size": 366}
{"docstring": "# NOTE(kgibbs): This line was modified to replace the use of\n# basestring, to make this file work under Python 2.2, which is\n# commonly used at Google.\n", "func_signal": "def _iterencode(self, o, markers=None):\n", "code": "if isinstance(o, (str, unicode)):\n# NOTE(kgibbs): End changes.\n    if self.ensure_ascii:\n        encoder = encode_basestring_ascii\n    else:\n        encoder = encode_basestring\n    yield encoder(o)\nelif o is None:\n    yield 'null'\nelif o is True:\n    yield 'true'\nelif o is False:\n    yield 'false'\nelif isinstance(o, (int, long)):\n    yield str(o)\nelif isinstance(o, float):\n    yield floatstr(o, self.allow_nan)\nelif isinstance(o, (list, tuple)):\n    for chunk in self._iterencode_list(o, markers):\n        yield chunk\nelif isinstance(o, dict):\n    for chunk in self._iterencode_dict(o, markers):\n        yield chunk\nelse:\n    if markers is not None:\n        markerid = id(o)\n        if markerid in markers:\n            raise ValueError(\"Circular reference detected\")\n        markers[markerid] = o\n    for chunk in self._iterencode_default(o, markers):\n        yield chunk\n    if markers is not None:\n        del markers[markerid]", "path": "opensocial\\simplejson\\encoder.py", "repo_name": "iMax-pp/pintme", "stars": 1, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\"\nEncode the given object and yield each string\nrepresentation as available.\n\nFor example::\n    \n    for chunk in JSONEncoder().iterencode(bigobject):\n        mysocket.write(chunk)\n\"\"\"\n", "func_signal": "def iterencode(self, o):\n", "code": "if self.check_circular:\n    markers = {}\nelse:\n    markers = None\nreturn self._iterencode(o, markers)", "path": "opensocial\\simplejson\\encoder.py", "repo_name": "iMax-pp/pintme", "stars": 1, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\"\nRegister rendering node\n\"\"\"\n\n", "func_signal": "def registerNode(self, uri):\n", "code": "nodeData = self.uriMap.get(uri)\nif (nodeData == None or self.reusable(uri)):\n\tdebug('spawning new thread for %s' % uri)\n\trt = self.factory(uri, self.scenename, self.context,\n\t\t\t\t\tself.name, self.fqueue,\n\t\t\t\t\tself.squeue,*self.args)\n\tself.r.append(rt)\n\tself.uriMap[uri] = {'renderThread': rt}\n\trt.start()", "path": "bpymodules\\NetworkRender\\Listener.py", "repo_name": "sergeyvfx/NetworkRender", "stars": 1, "license": "None", "language": "python", "size": 116}
{"docstring": "\"\"\"\nInitialize a Renderer\n\n@param uri: uri of remote render server OR 'localhost'\n@param scenename: name of current scene\n@param context: current rendering context\n@param name: the filename of the saved .blendfile\n@param fqueue: worklist\n@param squeue: rendering statistics\n@param imageType: type of output image\n\"\"\"\n\n", "func_signal": "def __init__(self, uri, scenename, context, name, fqueue, squeue, imageType):\n", "code": "RenderThread.__init__(self, uri, scenename, context, fqueue, squeue)\nRendererImpl.__init__(self, uri, scenename, context, name, imageType)", "path": "bpymodules\\NetworkRender\\Renderer.py", "repo_name": "sergeyvfx/NetworkRender", "stars": 1, "license": "None", "language": "python", "size": 116}
{"docstring": "\"\"\"\n@param uri: uri of remote renderserver OR 'localhost'\n@type uri: string\n@param scenename: name of current scene\n@type scenename: string\n@param context: current render context\n@type context: Scene.Renderdata\n@param name: filename of saved .blend file\n@type name: string\n@param imageType: type of output image\n@type imageType: int\n\"\"\"\n\n", "func_signal": "def __init__(self, uri, scenename, context, name, imageType):\n", "code": "self.configurer = Configurer()\nself.uri = uri\nself.scenename = scenename\nself.context = context\nself.name = name\nself.rpcserver = None\nself.blendSent = False\nself.imageType = imageType\n\nif uri == 'localhost':\n\tself.blendSent = True\n\tself.scn = Scene.Get(scenename)\nelse:\n\tself.rpcserver = Server(uri)\n\nself._reset()", "path": "bpymodules\\NetworkRender\\RendererImpl.py", "repo_name": "sergeyvfx/NetworkRender", "stars": 1, "license": "None", "language": "python", "size": 116}
{"docstring": "\"\"\"\nRead variables' values from registry to dictionary\n\"\"\"\n", "func_signal": "def readRegistry(self):\n", "code": "dict = Registry.GetKey('NetworkRender', False)\nif dict:\n\ttry:\n\t\tfor name in self.variables:\n\t\t\tself.variables[name].val = dict[name]\n\texcept:\n\t\t# Error in stored registry. Rewrite it.\n\t\tself.writeRegistry()", "path": "bpymodules\\NetworkRender\\Configurer.py", "repo_name": "sergeyvfx/NetworkRender", "stars": 1, "license": "None", "language": "python", "size": 116}
{"docstring": "\"\"\"\nInitialize a AnimRenderThread\n@param uri: uri of remote render server OR 'localhost'\n@type uri: string\n@param scenename: name of current scene\n@type scenename: string\n@param context: current rendering context\n@type context: Scene.Renderdata\n@param name: the filename of the saved .blendfile\n@type name: string\n@param fqueue: worklist\n@type fqueue: Queue.queue\n@param squeue: rendering statistics\n@type squeue: Queue.queue\n@param imageType: type of output image\n@type imageType: int\n\"\"\"\n\n", "func_signal": "def __init__(self, uri, scenename, context, name, fqueue, squeue, imageType):\n", "code": "Renderer.__init__(self, uri, scenename, context, name,\n\t\t\t\tfqueue, squeue, imageType)", "path": "bpymodules\\NetworkRender\\AnimRenderThread.py", "repo_name": "sergeyvfx/NetworkRender", "stars": 1, "license": "None", "language": "python", "size": 116}
{"docstring": "\"\"\"\n@param uri: uri of remote render server OR 'localhost'\n@type uri: string\n@param scenename: name of current scene\n@type scenename: string\n@param context: current rendering context\n@type context: Scene.Renderdata\n@param fqueue: worklist\n@type fqueue: Queue.queue\n@param squeue: rendering statistics\n@type squeue: Queue.queue\n\"\"\"\n\n", "func_signal": "def __init__(self, uri, scenename, context, fqueue, squeue):\n", "code": "Thread.__init__(self,name = 'thread' + uri)\nself.frames = fqueue\nself.stats = squeue\nself.failure = False\nself.stop = False", "path": "bpymodules\\NetworkRender\\RenderThread.py", "repo_name": "sergeyvfx/NetworkRender", "stars": 1, "license": "None", "language": "python", "size": 116}
{"docstring": "# allow fast restart of the server after it's killed\n", "func_signal": "def server_bind(self):\n", "code": "self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\nSimpleXMLRPCServer.server_bind(self)", "path": "NetworkRenderServer.py", "repo_name": "sergeyvfx/NetworkRender", "stars": 1, "license": "None", "language": "python", "size": 116}
{"docstring": "\"\"\"\nDestroy stored instance of object\n\"\"\"\n", "func_signal": "def destroy(self):\n", "code": "Configurer.__instance = None\ndel self.__dict__['_Configurer__instance']", "path": "bpymodules\\NetworkRender\\Configurer.py", "repo_name": "sergeyvfx/NetworkRender", "stars": 1, "license": "None", "language": "python", "size": 116}
{"docstring": "\"\"\"\nTransfer a block of data from client to temporary serverside .blend.\n@data: a string of binary data\n\nsee newfile()\n\"\"\"\n\n", "func_signal": "def put(self,data):\n", "code": "self.fd.write(str(data))\nreturn 1", "path": "NetworkRenderServer.py", "repo_name": "sergeyvfx/NetworkRender", "stars": 1, "license": "None", "language": "python", "size": 116}
{"docstring": "\"\"\"\nInitialize a Listener instance.\n@param scenename: name of current scene\n@type scenename: string\n@param context: rendercontext of current scene\n@type context: Blender.Renderdata\n@param name: filename of saved .blend file\n@type name: string\n@param fqueue: worklist queue\n@type fqueue: Queue.queue\n@param squeue: statistics queue\n@type squeue: Queue.queue\n@param threadfactory: a classfactory to spawn worker threads\n@type threadfactory: NetworkRender.Renderthread\n@param args: additional arguments to be given to the threadfactory\n\"\"\"\n\n", "func_signal": "def __init__(self, scenename, context, name, fqueue, squeue, threadfactory, *args):\n", "code": "Thread.__init__(self)\nself.scenename = scenename\nself.context = context\nself.name = name\nself.factory = threadfactory\nself.fqueue = fqueue\nself.squeue = squeue\nself.args = args\nself.configurer = Configurer()\ndebug('UDPlistener starting')\nself.socket = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)\t\ndebug('UDPlistener socket created %s' % self.socket)\nself.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\ndebug('UDPlistener options set')\n#self.ip = socket.gethostbyname(socket.gethostname())\nself.ip = '0.0.0.0'\nself.socket.bind((self.ip, self.configurer.get('ClientPort')))\ndebug('UDPlistener listening on %s'%self.socket)", "path": "bpymodules\\NetworkRender\\Listener.py", "repo_name": "sergeyvfx/NetworkRender", "stars": 1, "license": "None", "language": "python", "size": 116}
{"docstring": "\"\"\"\nforbid requests except from specific client hosts\n\"\"\"\n\n", "func_signal": "def verify_request(self, request, client_address):\n", "code": "global configurer\n\nreturn NetworkRender.allowedAddress(self.ip, client_address[0],\n\t\t\t\t\t\t\t\tconfigurer.get('ServerSecureNets'))", "path": "NetworkRenderServer.py", "repo_name": "sergeyvfx/NetworkRender", "stars": 1, "license": "None", "language": "python", "size": 116}
{"docstring": "\"\"\"\nInitialize configuration singleton\n\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "if Configurer.__instance is None:\n\tConfigurer.__instance = Configurer.__impl()\n\nself.__dict__['_Configurer__instance'] = Configurer.__instance", "path": "bpymodules\\NetworkRender\\Configurer.py", "repo_name": "sergeyvfx/NetworkRender", "stars": 1, "license": "None", "language": "python", "size": 116}
{"docstring": "\"\"\"\nListen and spawn new worker threads if appropriate.\n\nOverridden from Thread. Implements a stoppable loop (stops\nwhen requestStop is called on this thread) and records with\nworkerthreads were spawned.\n\"\"\"\n\n", "func_signal": "def run(self):\n", "code": "import time\nself.stop = False\nself.r = []\nself.uriMap = {}\n\n# Register static list of server\nservers = self.configurer.get('ClientServerList').split(',')\nfor server in servers:\n\tif (server == ''):\n\t\tcontinue\n\tserverData = server.split(':')\n\tif (len (serverData) == 1):\n\t\tserverData.append(str(self.configurer.get('ServerPort')))\n\tself.registerNode('http://' + serverData[0].strip() + ':' +\n\t\t\t\t\tserverData[1].strip())\n\nwhile not self.stop:\n\tdebug('UDPlistener ready for request on %s,%s' % self.socket.getsockname())\n\ttry:\n\t\tself.socket.settimeout(5)\n\t\tdata, addr = self.socket.recvfrom(512)\n\t\tdebug('UDPlistener received request: %s from %s' % (data, addr))\n\t\tself.registerNode(data)\n\texcept (socket.timeout) :\n\t\tdebug('UDPlistener received nothing, will try again')\n\tfinally:\n\t\ttime.sleep(self.configurer.get('ServerBCastInterval'))\ndebug('UDPlistener stopped')", "path": "bpymodules\\NetworkRender\\Listener.py", "repo_name": "sergeyvfx/NetworkRender", "stars": 1, "license": "None", "language": "python", "size": 116}
{"docstring": "\"\"\"\nInitialize list of extensions\n\"\"\"\n", "func_signal": "def initExtensions(self):\n", "code": "self.extensions = {'jpeg': Blender.Scene.Render.JPEG, \\\n\t\t\t\t'png': Blender.Scene.Render.PNG}", "path": "bpymodules\\NetworkRender\\Configurer.py", "repo_name": "sergeyvfx/NetworkRender", "stars": 1, "license": "None", "language": "python", "size": 116}
{"docstring": "\"\"\"\nWrite dictionary variables' values to registry for future usage\n\"\"\"\n", "func_signal": "def writeRegistry(self):\n", "code": "dict = {}\nfor name in self.variables:\n\tdict[name] = self.variables[name].val\nRegistry.SetKey('NetworkRender', dict, False)", "path": "bpymodules\\NetworkRender\\Configurer.py", "repo_name": "sergeyvfx/NetworkRender", "stars": 1, "license": "None", "language": "python", "size": 116}
{"docstring": "\"\"\"\nRender a single frame.\n@param frame: the framenumber to render\n@type frame: int\n\"\"\"\n\n", "func_signal": "def render(self,frame):\n", "code": "self.busy = True\nself.frame = frame\nself.animation = True\nself._sendBlendFile()\nself._renderFrame()\nself._getResult()\nself._reset()", "path": "bpymodules\\NetworkRender\\AnimRenderThread.py", "repo_name": "sergeyvfx/NetworkRender", "stars": 1, "license": "None", "language": "python", "size": 116}
{"docstring": "\"\"\"\nRetrieve a block of data from a serverside rendered image.\n@returns: A xmlrpclib.Binary object\n\nSee getResult().\n\"\"\"\n\n", "func_signal": "def get(self):\n", "code": "global configurer\n\ndata = self.fd2.read(configurer.get('ServerBufferSize'))\n\nif len(data) <= 0:\n\tself.fd2.close()\n\nreturn Binary(data)", "path": "NetworkRenderServer.py", "repo_name": "sergeyvfx/NetworkRender", "stars": 1, "license": "None", "language": "python", "size": 116}
{"docstring": "\"\"\"\nClose temporary serverside .blend.\n\nsee newfile()\n\"\"\"\n\n", "func_signal": "def endfile(self) :\n", "code": "self.fd.close()\nreturn 1", "path": "NetworkRenderServer.py", "repo_name": "sergeyvfx/NetworkRender", "stars": 1, "license": "None", "language": "python", "size": 116}
{"docstring": "\"\"\"\nSent saved .blendfile if needed.\n\nUploads saved .blend file to remote host if not already done so.\n\"\"\"\n\n", "func_signal": "def _sendBlendFile(self):\n", "code": "if self.isLocal() or self.blendSent:\n\tpass\nelse:\n\tself.rpcserver.newfile()\n\tfd = open(self.name, 'rb', self.configurer.get('ServerBufferSize'))\n\tdebug('%s sending .blend: %s'%(self.uri, fd))\n\tn = 0\n\tbuffer = True\n\twhile buffer :\n\t\tbuffer = fd.read(self.configurer.get('ServerBufferSize'))\n\t\tif buffer:\n\t\t\tr = self.rpcserver.put(Binary(buffer))\n\t\t\tdebug('%s put response %s' % (self.uri, r))\n\t\t\tn = n + 1\n\t\t\tdebug('%s %d blocks put'%(self.uri, n))\n\tfd.close()\n\tr = self.rpcserver.endfile()\n\tdebug('%s endfile called, response %s'%(self.uri, r))\nself.blendSent = True", "path": "bpymodules\\NetworkRender\\RendererImpl.py", "repo_name": "sergeyvfx/NetworkRender", "stars": 1, "license": "None", "language": "python", "size": 116}
{"docstring": "\"\"\"\nInitialize variables dictionary with their default values\n\"\"\"\n\n", "func_signal": "def initVariables(self):\n", "code": "self.declareVariable('ServerPort', 8080)\nself.declareVariable('ServerAddr', '0.0.0.0')\nself.declareVariable('ServerBCast', '255.255.255.255')\nself.declareVariable('ServerBCastInterval', 5)\nself.declareVariable('ServerStaticMap', '')\nself.declareVariable('ServerSecureNets', '')\nself.declareVariable('ServerBufferSize', 524288)\nself.declareVariable('ServerRenderPath', '/tmp/')\n\nself.declareVariable('ClientPort', 8082)\nself.declareVariable('ClientLocalRendering', True)\nself.declareVariable('ClientServerList', '')\n\nself.declareVariable('StillParts', 2)\nself.declareVariable('ImageType', Blender.Scene.Render.JPEG)", "path": "bpymodules\\NetworkRender\\Configurer.py", "repo_name": "sergeyvfx/NetworkRender", "stars": 1, "license": "None", "language": "python", "size": 116}
{"docstring": "\"\"\"Decodes a hexadecimal string into it's integer value.\"\"\"\n# We don't use the builtin 'hex' codec in python since it can\n# not handle odd numbers of digits, nor raise the same type\n# of exceptions we want to.\n", "func_signal": "def decode_hex( hexstring ):\n", "code": "n = 0\nfor c in hexstring:\n    if '0' <= c <= '9':\n        d = ord(c) - ord('0')\n    elif 'a' <= c <= 'f':\n        d = ord(c) - ord('a') + 10\n    elif 'A' <= c <= 'F':\n        d = ord(c) - ord('A') + 10\n    else:\n        raise JSONDecodeError('not a hexadecimal number',hexstring)\n    # Could use ((n << 4 ) | d), but python 2.3 issues a FutureWarning.\n    n = (n * 16) + d\nreturn n", "path": "demjson.py", "repo_name": "CarlosGabaldon/tweetstitch", "stars": 0, "license": "None", "language": "python", "size": 124}
{"docstring": "\"\"\"Determines if the given character is considered a line terminator.\n\nRef. ECMAScript section 7.3\n\n\"\"\"\n", "func_signal": "def islineterm(self, c):\n", "code": "if c == '\\r' or c == '\\n':\n    return True\nif c == u'\\u2028' or c == u'\\u2029': # unicodedata.category(c) in  ['Zl', 'Zp']\n    return True\nreturn False", "path": "demjson.py", "repo_name": "CarlosGabaldon/tweetstitch", "stars": 0, "license": "None", "language": "python", "size": 124}
{"docstring": "\"\"\"Encodes a Unicode string into a UTF-32LE encoded byte string.\"\"\"\n", "func_signal": "def utf32le_encode( obj, errors='strict' ):\n", "code": "import struct\ntry:\n    import cStringIO as sio\nexcept ImportError:\n    import StringIO as sio\nf = sio.StringIO()\nwrite = f.write\npack = struct.pack\nfor c in obj:\n    n = ord(c)\n    if 0xD800 <= n <= 0xDFFF: # surrogate codepoints are prohibited by UTF-32\n        if errors == 'ignore':\n            continue\n        elif errors == 'replace':\n            n = ord('?')\n        else:\n            cname = 'U+%04X'%n\n            raise UnicodeError('UTF-32 can not encode surrogate characters',cname)\n    write( pack('<L', n) )\nreturn f.getvalue()", "path": "demjson.py", "repo_name": "CarlosGabaldon/tweetstitch", "stars": 0, "license": "None", "language": "python", "size": 124}
{"docstring": "\"\"\"Takes a string and tries to convert it to a Unicode string.\n\nThis will return a Python unicode string type corresponding to the\ninput string (either str or unicode).  The character encoding is\nguessed by looking for either a Unicode BOM prefix, or by the\nrules specified by RFC 4627.  When in doubt it is assumed the\ninput is encoded in UTF-8 (the default for JSON).\n\n\"\"\"\n", "func_signal": "def auto_unicode_decode( s ):\n", "code": "if isinstance(s, unicode):\n    return s\nif len(s) < 4:\n    return s.decode('utf8')  # not enough bytes, assume default of utf-8\n# Look for BOM marker\nimport codecs\nbom2 = s[:2]\nbom4 = s[:4]\na, b, c, d = map(ord, s[:4])  # values of first four bytes\nif bom4 == codecs.BOM_UTF32_LE:\n    encoding = 'utf-32le'\n    s = s[4:]\nelif bom4 == codecs.BOM_UTF32_BE:\n    encoding = 'utf-32be'\n    s = s[4:]\nelif bom2 == codecs.BOM_UTF16_LE:\n    encoding = 'utf-16le'\n    s = s[2:]\nelif bom2 == codecs.BOM_UTF16_BE:\n    encoding = 'utf-16be'\n    s = s[2:]\n# No BOM, so autodetect encoding used by looking at first four bytes\n# according to RFC 4627 section 3.\nelif a==0 and b==0 and c==0 and d!=0: # UTF-32BE\n    encoding = 'utf-32be'\nelif a==0 and b!=0 and c==0 and d!=0: # UTF-16BE\n    encoding = 'utf-16be'\nelif a!=0 and b==0 and c==0 and d==0: # UTF-32LE\n    encoding = 'utf-32le'\nelif a!=0 and b==0 and c!=0 and d==0: # UTF-16LE\n    encoding = 'utf-16le'\nelse: #if a!=0 and b!=0 and c!=0 and d!=0: # UTF-8\n    # JSON spec says default is UTF-8, so always guess it\n    # if we can't guess otherwise\n    encoding = 'utf8'\n# Make sure the encoding is supported by Python\ntry:\n    cdk = codecs.lookup(encoding)\nexcept LookupError:\n    if encoding.startswith('utf-32') \\\n           or encoding.startswith('ucs4') \\\n           or encoding.startswith('ucs-4'):\n        # Python doesn't natively have a UTF-32 codec, but JSON\n        # requires that it be supported.  So we must decode these\n        # manually.\n        if encoding.endswith('le'):\n            unis = utf32le_decode(s)\n        else:\n            unis = utf32be_decode(s)\n    else:\n        raise JSONDecodeError('this python has no codec for this character encoding',encoding)\nelse:\n    # Convert to unicode using a standard codec\n    unis = s.decode(encoding)\nreturn unis", "path": "demjson.py", "repo_name": "CarlosGabaldon/tweetstitch", "stars": 0, "license": "None", "language": "python", "size": 124}
{"docstring": "\"\"\"This method is used to encode user-defined class objects.\n\nThe object being encoded should have a json_equivalent()\nmethod defined which returns another equivalent object which\nis easily JSON-encoded.  If the object in question has no\njson_equivalent() method available then None is returned\ninstead of a string so that the encoding will attempt the next\nstrategy.\n\nIf a caller wishes to disable the calling of json_equivalent()\nmethods, then subclass this class and override this method\nto just return None.\n\n\"\"\"\n", "func_signal": "def encode_equivalent( self, obj, nest_level=0 ):\n", "code": "if hasattr(obj, 'json_equivalent') \\\n       and callable(getattr(obj,'json_equivalent')):\n    obj2 = obj.json_equivalent()\n    if obj2 is obj:\n        # Try to prevent careless infinite recursion\n        raise JSONEncodeError('object has a json_equivalent() method that returns itself',obj)\n    json2 = self.encode( obj2, nest_level=nest_level )\n    return json2\nelse:\n    return None", "path": "demjson.py", "repo_name": "CarlosGabaldon/tweetstitch", "stars": 0, "license": "None", "language": "python", "size": 124}
{"docstring": "\"\"\"Takes a pair of unicode surrogates and returns the equivalent unicode character.\n\nThe input pair must be a surrogate pair, with c1 in the range\nU+D800 to U+DBFF and c2 in the range U+DC00 to U+DFFF.\n\n\"\"\"\n", "func_signal": "def surrogate_pair_as_unicode( c1, c2 ):\n", "code": "n1, n2 = ord(c1), ord(c2)\nif n1 < 0xD800 or n1 > 0xDBFF or n2 < 0xDC00 or n2 > 0xDFFF:\n    raise JSONDecodeError('illegal Unicode surrogate pair',(c1,c2))\na = n1 - 0xD800\nb = n2 - 0xDC00\nv = (a << 10) | b\nv += 0x10000\nreturn unichr(v)", "path": "demjson.py", "repo_name": "CarlosGabaldon/tweetstitch", "stars": 0, "license": "None", "language": "python", "size": 124}
{"docstring": "\"\"\"Intermediate-level decode for JSON boolean literals.\n\nTakes a string and a starting index, and returns a Python bool\n(True or False) and the index of the next unparsed character.\n\n\"\"\"\n", "func_signal": "def decode_boolean(self, s, i=0):\n", "code": "if s[i:i+4] == 'true':\n    return True, i+4\nelif s[i:i+5] == 'false':\n    return False, i+5\nraise JSONDecodeError('literal value is not a JSON boolean keyword',s)", "path": "demjson.py", "repo_name": "CarlosGabaldon/tweetstitch", "stars": 0, "license": "None", "language": "python", "size": 124}
{"docstring": "\"\"\"Skips an ECMAScript comment, either // or /* style.\n\nThe contents of the comment are returned as a string, as well\nas the index of the character immediately after the comment.\n\n\"\"\"\n", "func_signal": "def skip_comment(self, txt, i=0):\n", "code": "if i+1 >= len(txt) or txt[i] != '/' or txt[i+1] not in '/*':\n    return None, i\nif not self._allow_comments:\n    raise JSONDecodeError('comments are not allowed in strict JSON',txt[i:])\nmultiline = (txt[i+1] == '*')\nistart = i\ni += 2\nwhile i < len(txt):\n    if multiline:\n        if txt[i] == '*' and i+1 < len(txt) and txt[i+1] == '/':\n            j = i+2\n            break\n        elif txt[i] == '/' and i+1 < len(txt) and txt[i+1] == '*':\n            raise JSONDecodeError('multiline /* */ comments may not nest',txt[istart:i+1])\n    else:\n        if self.islineterm(txt[i]):\n            j = i  # line terminator is not part of comment\n            break\n    i += 1\n\nif i >= len(txt):\n    if not multiline:\n        j = len(txt)  # // comment terminated by end of file is okay\n    else:\n        raise JSONDecodeError('comment was never terminated',txt[istart:])\nreturn txt[istart:j], j", "path": "demjson.py", "repo_name": "CarlosGabaldon/tweetstitch", "stars": 0, "license": "None", "language": "python", "size": 124}
{"docstring": "\"\"\"Decodes a UTF-32BE byte string into a Unicode string.\"\"\"\n", "func_signal": "def utf32be_decode( obj, errors='strict' ):\n", "code": "if len(obj) % 4 != 0:\n    raise UnicodeError('UTF-32 decode error, data length not a multiple of 4 bytes')\nimport struct\nunpack = struct.unpack\nchars = []\ni = 0\nfor i in range(0, len(obj), 4):\n    seq = obj[i:i+4]\n    n = unpack('>L',seq)[0]\n    chars.append( unichr(n) )\nreturn u''.join( chars )", "path": "demjson.py", "repo_name": "CarlosGabaldon/tweetstitch", "stars": 0, "license": "None", "language": "python", "size": 124}
{"docstring": "\"\"\"Skips all whitespace, including comments and unicode whitespace\n\nTakes a string and a starting index, and returns the index of the\nnext non-whitespace character.\n\nIf skip_comments is True and not running in strict JSON mode, then\ncomments will be skipped over just like whitespace.\n\n\"\"\"\n", "func_signal": "def skipws_any(self, txt, i=0, imax=None, skip_comments=True):\n", "code": "if imax is None:\n    imax = len(txt)\nwhile i < imax:\n    if txt[i] == '/':\n        cmt, i = self.skip_comment(txt, i)\n    if i < imax and self.isws(txt[i]):\n        i += 1\n    else:\n        break\nreturn i", "path": "demjson.py", "repo_name": "CarlosGabaldon/tweetstitch", "stars": 0, "license": "None", "language": "python", "size": 124}
{"docstring": "\"\"\"Filters out all Unicode format control characters from the string.\n\nECMAScript permits any Unicode \"format control characters\" to\nappear at any place in the source code.  They are to be\nignored as if they are not there before any other lexical\ntokenization occurs.  Note that JSON does not allow them.\n\nRef. ECMAScript section 7.1.\n\n\"\"\"\n", "func_signal": "def strip_format_control_chars(self, txt):\n", "code": "import unicodedata\ntxt2 = filter( lambda c: unicodedata.category(unicode(c)) != 'Cf',\n               txt )\nreturn txt2", "path": "demjson.py", "repo_name": "CarlosGabaldon/tweetstitch", "stars": 0, "license": "None", "language": "python", "size": 124}
{"docstring": "#print 'encode_helper(chunklist=%r, obj=%r, nest_level=%r)'%(chunklist,obj,nest_level)\n", "func_signal": "def encode_helper(self, chunklist, obj, nest_level):\n", "code": "if hasattr(obj, 'json_equivalent'):\n    json = self.encode_equivalent( obj, nest_level=nest_level )\n    if json is not None:\n        chunklist.append( json )\n        return\nif obj is None:\n    chunklist.append( self.encode_null() )\nelif obj is undefined:\n    if self._allow_undefined_values:\n        chunklist.append( self.encode_undefined() )\n    else:\n        raise JSONEncodeError('strict JSON does not permit \"undefined\" values')\nelif isinstance(obj, bool):\n    chunklist.append( self.encode_boolean(obj) )\nelif isinstance(obj, (int,long,float,complex)) or \\\n         (decimal and isinstance(obj, decimal.Decimal)):\n    chunklist.append( self.encode_number(obj) )\nelif isinstance(obj, basestring) or isstringtype(obj):\n    chunklist.append( self.encode_string(obj) )\nelse:\n    self.encode_composite(chunklist, obj, nest_level)", "path": "demjson.py", "repo_name": "CarlosGabaldon/tweetstitch", "stars": 0, "license": "None", "language": "python", "size": 124}
{"docstring": "\"\"\"Skips whitespace.\n\"\"\"\n", "func_signal": "def skipws(self, txt, i=0, imax=None, skip_comments=True):\n", "code": "if not self._allow_comments and not self._allow_unicode_whitespace:\n    if imax is None:\n        imax = len(txt)\n    while i < imax and txt[i] in ' \\r\\n\\t':\n        i += 1\n    return i\nelse:\n    return self.skipws_any(txt, i, imax, skip_comments)", "path": "demjson.py", "repo_name": "CarlosGabaldon/tweetstitch", "stars": 0, "license": "None", "language": "python", "size": 124}
{"docstring": "\"\"\"Try to return the Nan, Infinity, and -Infinity float values.\n\nThis is unnecessarily complex because there is no standard\nplatform- independent way to do this in Python as the language\n(opposed to some implementation of it) doesn't discuss\nnon-numbers.  We try various strategies from the best to the\nworst.\n\nIf this Python interpreter uses the IEEE 754 floating point\nstandard then the returned values will probably be real instances\nof the 'float' type.  Otherwise a custom class object is returned\nwhich will attempt to simulate the correct behavior as much as\npossible.\n\n\"\"\"\n", "func_signal": "def _nonnumber_float_constants():\n", "code": "try:\n    # First, try (mostly portable) float constructor.  Works under\n    # Linux x86 (gcc) and some Unices.\n    nan = float('nan')\n    inf = float('inf')\n    neginf = float('-inf')\nexcept ValueError:\n    try:\n        # Try the AIX (PowerPC) float constructors\n        nan = float('NaNQ')\n        inf = float('INF')\n        neginf = float('-INF')\n    except ValueError:\n        try:\n            # Next, try binary unpacking.  Should work under\n            # platforms using IEEE 754 floating point.\n            import struct, sys\n            xnan = '7ff8000000000000'.decode('hex')  # Quiet NaN\n            xinf = '7ff0000000000000'.decode('hex')\n            xcheck = 'bdc145651592979d'.decode('hex') # -3.14159e-11\n            # Could use float.__getformat__, but it is a new python feature,\n            # so we use sys.byteorder.\n            if sys.byteorder == 'big':\n                nan = struct.unpack('d', xnan)[0]\n                inf = struct.unpack('d', xinf)[0]\n                check = struct.unpack('d', xcheck)[0]\n            else:\n                nan = struct.unpack('d', xnan[::-1])[0]\n                inf = struct.unpack('d', xinf[::-1])[0]\n                check = struct.unpack('d', xcheck[::-1])[0]\n            neginf = - inf\n            if check != -3.14159e-11:\n                raise ValueError('Unpacking raw IEEE 754 floats does not work')\n        except (ValueError, TypeError):\n            # Punt, make some fake classes to simulate.  These are\n            # not perfect though.  For instance nan * 1.0 == nan,\n            # as expected, but 1.0 * nan == 0.0, which is wrong.\n            class nan(float):\n                \"\"\"An approximation of the NaN (not a number) floating point number.\"\"\"\n                def __repr__(self): return 'nan'\n                def __str__(self): return 'nan'\n                def __add__(self,x): return self\n                def __radd__(self,x): return self\n                def __sub__(self,x): return self\n                def __rsub__(self,x): return self\n                def __mul__(self,x): return self\n                def __rmul__(self,x): return self\n                def __div__(self,x): return self\n                def __rdiv__(self,x): return self\n                def __divmod__(self,x): return (self,self)\n                def __rdivmod__(self,x): return (self,self)\n                def __mod__(self,x): return self\n                def __rmod__(self,x): return self\n                def __pow__(self,exp): return self\n                def __rpow__(self,exp): return self\n                def __neg__(self): return self\n                def __pos__(self): return self\n                def __abs__(self): return self\n                def __lt__(self,x): return False\n                def __le__(self,x): return False\n                def __eq__(self,x): return False\n                def __neq__(self,x): return True\n                def __ge__(self,x): return False\n                def __gt__(self,x): return False\n                def __complex__(self,*a): raise NotImplementedError('NaN can not be converted to a complex')\n            if decimal:\n                nan = decimal.Decimal('NaN')\n            else:\n                nan = nan()\n            class inf(float):\n                \"\"\"An approximation of the +Infinity floating point number.\"\"\"\n                def __repr__(self): return 'inf'\n                def __str__(self): return 'inf'\n                def __add__(self,x): return self\n                def __radd__(self,x): return self\n                def __sub__(self,x): return self\n                def __rsub__(self,x): return self\n                def __mul__(self,x):\n                    if x is neginf or x < 0:\n                        return neginf\n                    elif x == 0:\n                        return nan\n                    else:\n                        return self\n                def __rmul__(self,x): return self.__mul__(x)\n                def __div__(self,x):\n                    if x == 0:\n                        raise ZeroDivisionError('float division')\n                    elif x < 0:\n                        return neginf\n                    else:\n                        return self\n                def __rdiv__(self,x):\n                    if x is inf or x is neginf or x is nan:\n                        return nan\n                    return 0.0\n                def __divmod__(self,x):\n                    if x == 0:\n                        raise ZeroDivisionError('float divmod()')\n                    elif x < 0:\n                        return (nan,nan)\n                    else:\n                        return (self,self)\n                def __rdivmod__(self,x):\n                    if x is inf or x is neginf or x is nan:\n                        return (nan, nan)\n                    return (0.0, x)\n                def __mod__(self,x):\n                    if x == 0:\n                        raise ZeroDivisionError('float modulo')\n                    else:\n                        return nan\n                def __rmod__(self,x):\n                    if x is inf or x is neginf or x is nan:\n                        return nan\n                    return x\n                def __pow__(self, exp):\n                    if exp == 0:\n                        return 1.0\n                    else:\n                        return self\n                def __rpow__(self, x):\n                    if -1 < x < 1: return 0.0\n                    elif x == 1.0: return 1.0\n                    elif x is nan or x is neginf or x < 0:\n                        return nan\n                    else:\n                        return self\n                def __neg__(self): return neginf\n                def __pos__(self): return self\n                def __abs__(self): return self\n                def __lt__(self,x): return False\n                def __le__(self,x):\n                    if x is self:\n                        return True\n                    else:\n                        return False\n                def __eq__(self,x):\n                    if x is self:\n                        return True\n                    else:\n                        return False\n                def __neq__(self,x):\n                    if x is self:\n                        return False\n                    else:\n                        return True\n                def __ge__(self,x): return True\n                def __gt__(self,x): return True\n                def __complex__(self,*a): raise NotImplementedError('Infinity can not be converted to a complex')\n            if decimal:\n                inf = decimal.Decimal('Infinity')\n            else:\n                inf = inf()\n            class neginf(float):\n                \"\"\"An approximation of the -Infinity floating point number.\"\"\"\n                def __repr__(self): return '-inf'\n                def __str__(self): return '-inf'\n                def __add__(self,x): return self\n                def __radd__(self,x): return self\n                def __sub__(self,x): return self\n                def __rsub__(self,x): return self\n                def __mul__(self,x):\n                    if x is self or x < 0:\n                        return inf\n                    elif x == 0:\n                        return nan\n                    else:\n                        return self\n                def __rmul__(self,x): return self.__mul__(self)\n                def __div__(self,x):\n                    if x == 0:\n                        raise ZeroDivisionError('float division')\n                    elif x < 0:\n                        return inf\n                    else:\n                        return self\n                def __rdiv__(self,x):\n                    if x is inf or x is neginf or x is nan:\n                        return nan\n                    return -0.0\n                def __divmod__(self,x):\n                    if x == 0:\n                        raise ZeroDivisionError('float divmod()')\n                    elif x < 0:\n                        return (nan,nan)\n                    else:\n                        return (self,self)\n                def __rdivmod__(self,x):\n                    if x is inf or x is neginf or x is nan:\n                        return (nan, nan)\n                    return (-0.0, x)\n                def __mod__(self,x):\n                    if x == 0:\n                        raise ZeroDivisionError('float modulo')\n                    else:\n                        return nan\n                def __rmod__(self,x):\n                    if x is inf or x is neginf or x is nan:\n                        return nan\n                    return x\n                def __pow__(self,exp):\n                    if exp == 0:\n                        return 1.0\n                    else:\n                        return self\n                def __rpow__(self, x):\n                    if x is nan or x is inf or x is inf:\n                        return nan\n                    return 0.0\n                def __neg__(self): return inf\n                def __pos__(self): return self\n                def __abs__(self): return inf\n                def __lt__(self,x): return True\n                def __le__(self,x): return True\n                def __eq__(self,x):\n                    if x is self:\n                        return True\n                    else:\n                        return False\n                def __neq__(self,x):\n                    if x is self:\n                        return False\n                    else:\n                        return True\n                def __ge__(self,x):\n                    if x is self:\n                        return True\n                    else:\n                        return False\n                def __gt__(self,x): return False\n                def __complex__(self,*a): raise NotImplementedError('-Infinity can not be converted to a complex')\n            if decimal:\n                neginf = decimal.Decimal('-Infinity')\n            else:\n                neginf = neginf(0)\nreturn nan, inf, neginf", "path": "demjson.py", "repo_name": "CarlosGabaldon/tweetstitch", "stars": 0, "license": "None", "language": "python", "size": 124}
{"docstring": "\"\"\"Encodes a Unicode string into a UTF-32BE encoded byte string.\"\"\"\n", "func_signal": "def utf32be_encode( obj, errors='strict' ):\n", "code": "import struct\ntry:\n    import cStringIO as sio\nexcept ImportError:\n    import StringIO as sio\nf = sio.StringIO()\nwrite = f.write\npack = struct.pack\nfor c in obj:\n    n = ord(c)\n    if 0xD800 <= n <= 0xDFFF: # surrogate codepoints are prohibited by UTF-32\n        if errors == 'ignore':\n            continue\n        elif errors == 'replace':\n            n = ord('?')\n        else:\n            cname = 'U+%04X'%n\n            raise UnicodeError('UTF-32 can not encode surrogate characters',cname)\n    write( pack('>L', n) )\nreturn f.getvalue()", "path": "demjson.py", "repo_name": "CarlosGabaldon/tweetstitch", "stars": 0, "license": "None", "language": "python", "size": 124}
{"docstring": "\"\"\"Decodes a JSON-encoded string into a Python object.\n\nIf 'strict' is set to True, then those strings that are not\nentirely strictly conforming to JSON will result in a\nJSONDecodeError exception.\n\nThe input string can be either a python string or a python unicode\nstring.  If it is already a unicode string, then it is assumed\nthat no character set decoding is required.\n\nHowever, if you pass in a non-Unicode text string (i.e., a python\ntype 'str') then an attempt will be made to auto-detect and decode\nthe character encoding.  This will be successful if the input was\nencoded in any of UTF-8, UTF-16 (BE or LE), or UTF-32 (BE or LE),\nand of course plain ASCII works too.\n\nNote though that if you know the character encoding, then you\nshould convert to a unicode string yourself, or pass it the name\nof the 'encoding' to avoid the guessing made by the auto\ndetection, as with\n\n    python_object = demjson.decode( input_bytes, encoding='utf8' )\n\nOptional keywords arguments must be of the form\n    allow_xxxx=True/False\nor\n    prevent_xxxx=True/False\nwhere each will allow or prevent the specific behavior, after the\nevaluation of the 'strict' argument.  For example, if strict=True\nthen by also passing 'allow_comments=True' then comments will be\nallowed.  If strict=False then prevent_comments=True will allow\neverything except comments.\n\n\"\"\"\n# Initialize the JSON object\n", "func_signal": "def decode( txt, strict=False, encoding=None, **kw ):\n", "code": "j = JSON( strict=strict )\nfor keyword, value in kw.items():\n    if keyword.startswith('allow_'):\n        behavior = keyword[6:]\n        allow = bool(value)\n    elif keyword.startswith('prevent_'):\n        behavior = keyword[8:]\n        allow = not bool(value)\n    else:\n        raise ValueError('unknown keyword argument', keyword)\n    if allow:\n        j.allow(behavior)\n    else:\n        j.prevent(behavior)\n\n# Convert the input string into unicode if needed.\nif isinstance(txt,unicode):\n    unitxt = txt\nelse:\n    if encoding is None:\n        unitxt = auto_unicode_decode( txt )\n    else:\n        cdk = None # codec\n        decoder = None\n        import codecs\n        try:\n            cdk = codecs.lookup(encoding)\n        except LookupError:\n            encoding = encoding.lower()\n            decoder = None\n            if encoding.startswith('utf-32') \\\n                   or encoding.startswith('ucs4') \\\n                   or encoding.startswith('ucs-4'):\n                # Python doesn't natively have a UTF-32 codec, but JSON\n                # requires that it be supported.  So we must decode these\n                # manually.\n                if encoding.endswith('le'):\n                    decoder = utf32le_decode\n                elif encoding.endswith('be'):\n                    decoder = utf32be_decode\n                else:\n                    if txt.startswith( codecs.BOM_UTF32_BE ):\n                        decoder = utf32be_decode\n                        txt = txt[4:]\n                    elif txt.startswith( codecs.BOM_UTF32_LE ):\n                        decoder = utf32le_decode\n                        txt = txt[4:]\n                    else:\n                        if encoding.startswith('ucs'):\n                            raise JSONDecodeError('UCS-4 encoded string must start with a BOM')\n                        decoder = utf32be_decode # Default BE for UTF, per unicode spec\n            elif encoding.startswith('ucs2') or encoding.startswith('ucs-2'):\n                # Python has no UCS-2, but we can simulate with\n                # UTF-16.  We just need to force us to not try to\n                # encode anything past the BMP.\n                encoding = 'utf-16'\n\n        if decoder:\n            unitxt = decoder(txt)\n        elif encoding:\n            unitxt = txt.decode(encoding)\n        else:\n            raise JSONDecodeError('this python has no codec for this character encoding',encoding)\n\n    # Check that the decoding seems sane.  Per RFC 4627 section 3:\n    #    \"Since the first two characters of a JSON text will\n    #    always be ASCII characters [RFC0020], ...\"\n    #\n    # This check is probably not necessary, but it allows us to\n    # raise a suitably descriptive error rather than an obscure\n    # syntax error later on.\n    #\n    # Note that the RFC requirements of two ASCII characters seems\n    # to be an incorrect statement as a JSON string literal may\n    # have as it's first character any unicode character.  Thus\n    # the first two characters will always be ASCII, unless the\n    # first character is a quotation mark.  And in non-strict\n    # mode we can also have a few other characters too.\n    if len(unitxt) > 2:\n        first, second = unitxt[:2]\n        if first in '\"\\'':\n            pass # second can be anything inside string literal\n        else:\n            if ((ord(first) < 0x20 or ord(first) > 0x7f) or \\\n                (ord(second) < 0x20 or ord(second) > 0x7f)) and \\\n                (not j.isws(first) and not j.isws(second)):\n                # Found non-printable ascii, must check unicode\n                # categories to see if the character is legal.\n                # Only whitespace, line and paragraph separators,\n                # and format control chars are legal here.\n                import unicodedata\n                catfirst = unicodedata.category(unicode(first))\n                catsecond = unicodedata.category(unicode(second))\n                if catfirst not in ('Zs','Zl','Zp','Cf') or \\\n                       catsecond not in ('Zs','Zl','Zp','Cf'):\n                    raise JSONDecodeError('the decoded string is gibberish, is the encoding correct?',encoding)\n# Now ready to do the actual decoding\nobj = j.decode( unitxt )\nreturn obj", "path": "demjson.py", "repo_name": "CarlosGabaldon/tweetstitch", "stars": 0, "license": "None", "language": "python", "size": 124}
{"docstring": "\"\"\"Returns a tuple (significant_digits, max_exponent) for the float type.\n\"\"\"\n", "func_signal": "def determine_float_precision():\n", "code": "import math\n# Just count the digits in pi.  The last two decimal digits\n# may only be partial digits, so discount for them.\nwhole, frac = repr(math.pi).split('.')\nsigdigits = len(whole) + len(frac) - 2\n\n# This is a simple binary search.  We find the largest exponent\n# that the float() type can handle without going infinite or\n# raising errors.\nmaxexp = None\nminv = 0; maxv = 1000\nwhile True:\n    if minv+1 == maxv:\n        maxexp = minv - 1\n        break\n    elif maxv < minv:\n        maxexp = None\n        break\n    m = (minv + maxv) // 2\n    try:\n        f = repr(float( '1e+%d' % m ))\n    except ValueError:\n        f = None\n    else:\n        if not f or f[0] < '0' or f[0] > '9':\n            f = None\n    if not f:\n        # infinite\n        maxv = m\n    else:\n        minv = m\nreturn sigdigits, maxexp", "path": "demjson.py", "repo_name": "CarlosGabaldon/tweetstitch", "stars": 0, "license": "None", "language": "python", "size": 124}
{"docstring": "\"\"\"Determines if the given character is considered as white space.\n\nNote that Javscript is much more permissive on what it considers\nto be whitespace than does JSON.\n\nRef. ECMAScript section 7.2\n\n\"\"\"\n", "func_signal": "def isws(self, c):\n", "code": "if not self._allow_unicode_whitespace:\n    return c in ' \\t\\n\\r'\nelse:\n    if not isinstance(c,unicode):\n        c = unicode(c)\n    if c in u' \\t\\n\\r\\f\\v':\n        return True\n    import unicodedata\n    return unicodedata.category(c) == 'Zs'", "path": "demjson.py", "repo_name": "CarlosGabaldon/tweetstitch", "stars": 0, "license": "None", "language": "python", "size": 124}
{"docstring": "\"\"\"Decodes an octal string into it's integer value.\"\"\"\n", "func_signal": "def decode_octal( octalstring ):\n", "code": "n = 0\nfor c in octalstring:\n    if '0' <= c <= '7':\n        d = ord(c) - ord('0')\n    else:\n        raise JSONDecodeError('not an octal number',octalstring)\n    # Could use ((n << 3 ) | d), but python 2.3 issues a FutureWarning.\n    n = (n * 8) + d\nreturn n", "path": "demjson.py", "repo_name": "CarlosGabaldon/tweetstitch", "stars": 0, "license": "None", "language": "python", "size": 124}
{"docstring": "\"\"\"Encodes a Python string into a JSON string literal.\n\n\"\"\"\n# Must handle instances of UserString specially in order to be\n# able to use ord() on it's simulated \"characters\".\n", "func_signal": "def encode_string(self, s):\n", "code": "import UserString\nif isinstance(s, (UserString.UserString, UserString.MutableString)):\n    def tochar(c):\n        return c.data\nelse:\n    # Could use \"lambda c:c\", but that is too slow.  So we set to None\n    # and use an explicit if test inside the loop.\n    tochar = None\n\nchunks = []\nchunks.append('\"')\nrevesc = self._rev_escapes\nasciiencodable = self._asciiencodable\nencunicode = self._encode_unicode_as_escapes\ni = 0\nimax = len(s)\nwhile i < imax:\n    if tochar:\n        c = tochar(s[i])\n    else:\n        c = s[i]\n    cord = ord(c)\n    if cord < 256 and asciiencodable[cord] and isinstance(encunicode, bool):\n        # Contiguous runs of plain old printable ASCII can be copied\n        # directly to the JSON output without worry (unless the user\n        # has supplied a custom is-encodable function).\n        j = i\n        i += 1\n        while i < imax:\n            if tochar:\n                c = tochar(s[i])\n            else:\n                c = s[i]\n            cord = ord(c)\n            if cord < 256 and asciiencodable[cord]:\n                i += 1\n            else:\n                break\n        chunks.append( unicode(s[j:i]) )\n    elif revesc.has_key(c):\n        # Has a shortcut escape sequence, like \"\\n\"\n        chunks.append(revesc[c])\n        i += 1\n    elif cord <= 0x1F:\n        # Always unicode escape ASCII-control characters\n        chunks.append(r'\\u%04x' % cord)\n        i += 1\n    elif 0xD800 <= cord <= 0xDFFF:\n        # A raw surrogate character!  This should never happen\n        # and there's no way to include it in the JSON output.\n        # So all we can do is complain.\n        cname = 'U+%04X' % cord\n        raise JSONEncodeError('can not include or escape a Unicode surrogate character',cname)\n    elif cord <= 0xFFFF:\n        # Other BMP Unicode character\n        if isinstance(encunicode, bool):\n            doesc = encunicode\n        else:\n            doesc = encunicode( c )\n        if doesc:\n            chunks.append(r'\\u%04x' % cord)\n        else:\n            chunks.append( c )\n        i += 1\n    else: # ord(c) >= 0x10000\n        # Non-BMP Unicode\n        if isinstance(encunicode, bool):\n            doesc = encunicode\n        else:\n            doesc = encunicode( c )\n        if doesc:\n            for surrogate in unicode_as_surrogate_pair(c):\n                chunks.append(r'\\u%04x' % ord(surrogate))\n        else:\n            chunks.append( c )\n        i += 1\nchunks.append('\"')\nreturn ''.join( chunks )", "path": "demjson.py", "repo_name": "CarlosGabaldon/tweetstitch", "stars": 0, "license": "None", "language": "python", "size": 124}
{"docstring": "\"\"\"\nDepending on the state of (so called here) finite state machine, \nthis function sets the widgets on main window form (enables and disables, etc.)\n\"\"\"\n", "func_signal": "def update_ui(self):\n", "code": "if self.state == STOP:\n\tself.actionScan_channels.setText('Scan channels')\n\tself.actionScan_channels.setToolTip('Scan channels')\n\tself.actionScan_channel.setText('Scan channel')\n\tself.actionScan_channel.setToolTip('Scan channel')\n\tself.actionSniff.setText('Sniff')\n\tself.actionSniff.setToolTip('Sniff')\n\tself.actionSniff.setIcon(self.iconStart)\n\tself.enable_controls(self.actionNew, self.actionImport, self.actionScan_channels,\n\t\t\t\t\t\tself.treeChannels)\n\tif self.packets.changed() or self.packets.nodes.changed():\n\t\tself.actionSave.setEnabled(True)\n\telse:\n\t\tself.actionSave.setDisabled(True)\n\tif self.channel: #if there is any channel selected....\n\t\tself.enable_controls(self.actionScan_channel, self.actionSniff)\n\tif int(self.packets.rowCount()) > 0:\n\t\tself.setStatusTip('Stopped, ready. Packets recieved: ' + str(self.packets.rowCount()))\n\t\tself.enable_controls(self.actionStatistics, self.actionName_nodes)\n\t\t\nif self.state in (SCAN_CHANNELS, SCAN_CHANNEL, SNIFF):\n\t#turn off all buttons\n\tself.disable_controls(self.actionNew, self.actionImport, self.actionSave,\n\t\t\t\tself.actionStatistics, self.actionName_nodes, self.actionScan_channels, \n\t\t\t\tself.actionScan_channel, self.actionSniff, self.treeChannels)\n\t\nif self.state == SCAN_CHANNELS:\n\tself.actionScan_channels.setText('Stop scanning channels')\n\tself.actionScan_channels.setToolTip('Stop scanning channels')\n\tself.setStatusTip('Scanning channels...')\n\tself.actionScan_channels.setEnabled(True)\nelif self.state == SCAN_CHANNEL:\n\tself.actionScan_channel.setText('Stop scanning channel %i' % self.channel)\n\tself.actionScan_channel.setToolTip('Stop scanning channel %i' % self.channel)\n\tself.setStatusTip('Scanning channel %i' % self.channel) \n\tself.actionScan_channel.setEnabled(True)\nelif self.state == SNIFF:\n\tself.actionSniff.setIcon(self.iconStop)\n\tself.actionSniff.setText('Stop sniffing')\n\tself.actionSniff.setToolTip('Stop sniffing')\n\tself.setStatusTip('Sniffing on channel %i' % self.channel)\n\tself.actionSniff.setEnabled(True)", "path": "src\\ZigBeeSniffer.py", "repo_name": "grucha/Sniffer", "stars": 1, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nSave collected data into a file.\n\"\"\"\n", "func_signal": "def on_actionSave_activated(self):\n", "code": "fname = '.'\nfname = QtGui.QFileDialog.getSaveFileName(self,\n\t\t\t\t\t'ZigBee Sniffer - save log file', fname,\n\t\t\t\t\t'IEEE 802.15.4 log files (*.154)')\nif fname: # if file dialog not rejected, picked some file\n\tif '.' not in fname: fname += '.154'\n\twith open(fname, 'w') as fh:\n\t\tself.packets.save(fh)\n\t\tself.update_ui()", "path": "src\\ZigBeeSniffer.py", "repo_name": "grucha/Sniffer", "stars": 1, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nThis function fills QTreeWidget with packet detailed information.\n\"\"\"\n#first remove all content\n", "func_signal": "def updatePacketDetails(treeWidget, packet):\n", "code": "fontBold = QFont()\nfontBold.setBold(True)\n\ntreeWidget.clear()\n\nt = QTreeWidgetItem(treeWidget)\nt.setText(0, 'Reception time')\nt.setText(1, '%s:%s:%s.%s' % tuple(packet.time))\n\nif packet.isInvalid:\n\t\treturn\n\ntrInfo =  QTreeWidgetItem(treeWidget)\ntrInfo.setText(0, 'Transmission info')\ntrInfo.setText(1, 'CRC passed: %s,  LQI: %s,  RSSI: %s' % (packet.CRCOk, packet.LQI, packet.RSSI))\n\nPHY =  QTreeWidgetItem(treeWidget)\nPHY.setText(0, 'PHY fields')\nPHY.setFont(0, fontBold)\n\nframeLength = QTreeWidgetItem(PHY)\nframeLength.setText(0, 'Frame length')\nframeLength.setText(1, '%i' % len(packet.load))\n\nMAC =  QTreeWidgetItem(treeWidget)\nMAC.setText(0, 'MAC fields')\nMAC.setFont(0, fontBold)\n\nframeControl = QTreeWidgetItem(MAC)\nframeControl.setText(0, 'Frame control')\nframeControl.setText(1, packet.frameControl)\n\nframeType = QTreeWidgetItem(frameControl)\nframeType.setText(0, 'Frame Type')\nframeType.setText(1, packet.frameType)\n\nsecurityEnabled = QTreeWidgetItem(frameControl)\nsecurityEnabled.setText(0, 'Security enabled')\nsecurityEnabled.setText(1, packet.securityEnabled)\n\nframePending = QTreeWidgetItem(frameControl)\nframePending.setText(0, 'Frame pending')\nframePending.setText(1, packet.framePending)\n\nackRequest = QTreeWidgetItem(frameControl)\nackRequest.setText(0, 'Ack. request')\nackRequest.setText(1, packet.ackRequest)\n\nintraPAN = QTreeWidgetItem(frameControl)\nintraPAN.setText(0, 'Intra-PAN')\nintraPAN.setText(1, packet.intraPAN)\n\ndstAddrMode = QTreeWidgetItem(frameControl)\ndstAddrMode.setText(0, 'Dest. addressing mode')\ndstAddrMode.setText(1, packet.dstAddrMode)\n\nsrcAddrMode = QTreeWidgetItem(frameControl)\nsrcAddrMode.setText(0, 'Source addressing mode')\nsrcAddrMode.setText(1, packet.srcAddrMode)\n\nseqNumber = QTreeWidgetItem(MAC)\nseqNumber.setText(0, 'Sequence number')\nseqNumber.setText(1, packet.seqNumber)\n\nif hasattr(packet, 'dstPANID'):\n\tdstPANID = QTreeWidgetItem(MAC)\n\tdstPANID.setText(0, 'Destination PAN-ID')\n\tdstPANID.setText(1, packet.dstPANID)\n\nif hasattr(packet, 'dstAddr'):\n\tdstAddr = QTreeWidgetItem(MAC)\n\tdstAddr.setText(0, 'Destination address')\n\tdstAddr.setText(1, packet.dstAddr)\n\nif hasattr(packet, 'srcPANID'):\n\tsrcPANID = QTreeWidgetItem(MAC)\n\tsrcPANID.setText(0, 'Source PAN-ID')\n\tsrcPANID.setText(1, packet.srcPANID)\n\t\nif hasattr(packet, 'srcAddr'):\n\tsrcAddr = QTreeWidgetItem(MAC)\n\tsrcAddr.setText(0, 'Source address')\n\tsrcAddr.setText(1, packet.srcAddr)\n\t\nif hasattr(packet, 'payload'):\n\tpayload = QTreeWidgetItem(MAC)\n\tpayload.setText(0, 'Payload')\n\tpayload.setText(1, packet.payload)\n\nif hasattr(packet, 'commandType'):\n\tcommandType = QTreeWidgetItem(MAC)\n\tcommandType.setText(0, 'Command type')\n\tcommandType.setText(1, packet.commandType)\n\nif hasattr(packet, 'commandPayload'):\n\tcommandPayload = QTreeWidgetItem(MAC)\n\tcommandPayload.setText(0, 'Command payload')\n\tcommandPayload.setText(1, packet.commandPayload)\n\nif hasattr(packet, 'superFrameSpec'):\n\tsuperFrameSpec = QTreeWidgetItem(MAC)\n\tsuperFrameSpec.setText(0, 'Superframe specification')\n\tsuperFrameSpec.setText(1, packet.superFrameSpec)\n\n\tbeaconOrder = QTreeWidgetItem(superFrameSpec)\n\tbeaconOrder.setText(0, 'Beacon order')\n\tbeaconOrder.setText(1, packet.beaconOrder)\n\n\tsuperFrameOrder = QTreeWidgetItem(superFrameSpec)\n\tsuperFrameOrder.setText(0, 'Superframe order')\n\tsuperFrameOrder.setText(1, packet.superFrameOrder)\n\n\tfinalCAPSlot = QTreeWidgetItem(superFrameSpec)\n\tfinalCAPSlot.setText(0, 'finalCAPSlot')\n\tfinalCAPSlot.setText(1, packet.finalCAPSlot)\n\n\tbattLifeExt = QTreeWidgetItem(superFrameSpec)\n\tbattLifeExt.setText(0, 'Batt. life extension')\n\tbattLifeExt.setText(1, packet.battLifeExt)\n\n\tPANCoord = QTreeWidgetItem(superFrameSpec)\n\tPANCoord.setText(0, 'PAN Coordinator')\n\tPANCoord.setText(1, packet.PANCoord)\n\n\tassocPermit = QTreeWidgetItem(superFrameSpec)\n\tassocPermit.setText(0, 'Association permit')\n\tassocPermit.setText(1, packet.assocPermit)\n\nif hasattr(packet, 'GTS'):\n\tGTS = QTreeWidgetItem(MAC)\n\tGTS.setText(0, 'GTS specification')\n\tGTS.setText(1, packet.GTS)\n\n\tGTSDescrCount = QTreeWidgetItem(GTS)\n\tGTSDescrCount.setText(0, 'GTS descriptor count')\n\tGTSDescrCount.setText(1, packet.GTSDescrCount)\n\t\n\tGTSPermit = QTreeWidgetItem(GTS)\n\tGTSPermit.setText(0, 'GTS permit')\n\tGTSPermit.setText(1, packet.GTSPermit)\n\t\n\tif int(packet.GTSDescrCount, 16) > 0:\n\t\tGTSDirections = QTreeWidgetItem(GTS)\n\t\tGTSDirections.setText(0, 'GTS directions')\n\t\tGTSDirections.setText(1, packet.GTSDirections)\n\t\n\t\tGTSDescriptors = QTreeWidgetItem(GTS)\n\t\tGTSDescriptors.setText(0, 'GTS descriptors list')\n\t\tdescriptors = []\n\t\tfor i in xrange(int(packet.GTSDescrCount, 16)):\n\t\t\tdescriptor = [QTreeWidgetItem(GTSDescriptors)] * 3\n\t\t\tdescriptor[0].setText(0, 'Device short address')\n\t\t\tdescriptor[0].setText(1, packet.GTSDescriptors[i].deviceShortAddr)\n\t\t\tdescriptor[1].setText(0, 'GTS starting slot')\n\t\t\tdescriptor[1].setText(1, packet.GTSDescriptors[i].GTSStartingSlot)\n\t\t\tdescriptor[2].setText(0, 'GTS length')\n\t\t\tdescriptor[3].setText(1, packet.GTSDescriptors[i].GTSLength)\n\t\t\tdescriptors.append(descriptor)\n\t\n\tif int(packet.numShortAddrPnd, 16) > 0 or int(packet.numShortAddrPnd, 16) > 0:\n\t\tpendingAddr = QTreeWidgetItem(MAC)\n\t\tpendingAddr.setText(0, 'Pending addresses list')\n\t\tpndShort = []\n\t\tpndLong = []\n\t\tfor i in xrange(int(packet.numShortAddrPnd, 16)):\n\t\t\tpndShort.append(QTreeWidgetItem(pendingAddr))\n\t\t\tpndShort[i].setText(0, 'Pending short addr. #%i' % i)\n\t\t\tpndShort[i].setText(1, packet.shortAddrPndList[i])\n\t\tfor i in xrange(int(packet.numLongAddrPnd, 16)):\n\t\t\tpndLong.append(QTreeWidgetItem(pendingAddr))\n\t\t\tpndLong[i].setText(0, 'Pending long addr. #%i' % i)\n\t\t\tpndLong[i].setText(1, packet.longAddrPndList[i])\n\t\t\nif hasattr(packet, 'bcnPayload'):\n\tbcnPayload = QTreeWidgetItem(MAC)\n\tbcnPayload.setText(0, 'Beacon payload')\n\tbcnPayload.setText(1, packet.bcnPayload)", "path": "src\\snifferData\\packetDetails.py", "repo_name": "grucha/Sniffer", "stars": 1, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nThis function is called by timer. Its job is to scan next channel each time it's called.\n\"\"\"\n", "func_signal": "def scan_channels(self):\n", "code": "if self.channel < 26:\n\tself.sniffer.stop_reading()\n\tself.channel += 1\n\tself.sniffer.start_reading()\n\tself.sniffer.cmd_scan_channel(self.channel)\nelse:\n\tif self.state == SCAN_CHANNELS: #check for sure\n\t\t#this should stop the scan:\n\t\tself.actionScan_channels.emit(QtCore.SIGNAL(\"activated()\"))", "path": "src\\ZigBeeSniffer.py", "repo_name": "grucha/Sniffer", "stars": 1, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nThis function is used by Qt to obtain data needed by the view.\nIt is mandatory for the model to work.\n\"\"\"\n", "func_signal": "def data(self, index, role=Qt.DisplayRole):\n", "code": "if not index.isValid() or not (0 <= index.row() < len(self.fields)):\n\treturn QVariant()\n\n\nif role == Qt.DisplayRole:\n\tif hasattr(index.parent, 'isValid'):\n\t\tparent = index.parent.row()\n\telse:\n\t\tparent = None\n\t#create subset of fields:\n\tfields = filter(lambda x: x[2] == parent, self.fields)\n\treturn QVariant(fields[index.row()][index.column()])\nreturn QVariant()", "path": "src\\snifferData\\packetDetails.py", "repo_name": "grucha/Sniffer", "stars": 1, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nAdds one packet at end of the packets list\n\"\"\"\n", "func_signal": "def append(self, channel):\n", "code": "self.beginInsertRows(QModelIndex(), len(self.channels), len(self.channels) + 1)\nself.channels.append(channel)\nself.dataChanged = True\nself.endInsertRows()", "path": "src\\snifferData\\channels.py", "repo_name": "grucha/Sniffer", "stars": 1, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nWe make sure here that there is any channel selected to sniff or to scan.\n\"\"\"\n", "func_signal": "def on_treeChannels_activated(self, index):\n", "code": "self.channel = self.channels.clickedChannel(index)\nif self.state in (NONE, STOP):\n\tself.enable_controls(self.actionSniff, self.actionScan_channel)", "path": "src\\ZigBeeSniffer.py", "repo_name": "grucha/Sniffer", "stars": 1, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nThis function is used by Qt to obtain data needed by the view.\nIt is mandatory for the model to work.\n\"\"\"\n", "func_signal": "def data(self, index, role=Qt.DisplayRole):\n", "code": "if not index.isValid() or not (0 <= index.row() < len(self.channels)):\n\treturn QVariant()\n\t\nchannel = self.channels[index.row()]\ncolumn = index.column()\n\t\nif role == Qt.DisplayRole:\n\t\treturn QVariant(channel[column])\n\nif role == Qt.TextAlignmentRole:\n\treturn QVariant(int(Qt.AlignCenter|Qt.AlignVCenter))\nreturn QVariant()", "path": "src\\snifferData\\channels.py", "repo_name": "grucha/Sniffer", "stars": 1, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nThis 'slot' updates detailed view of current selected packet.\n\"\"\"\n#self.details.setPacket(self.packets.getPacket(index))\n#a = QtGui.QMessageBox.question(self, u'podgl\u0105d',\n#str(index.row()), QtGui.QMessageBox.Yes, QtGui.QMessageBox.No)\n", "func_signal": "def on_treePackets_activated(self, index):\n", "code": "updatePacketDetails(self.treeDetails, \n\t\t\t\t\t\t\t\tself.packets.packets[index.row()])\nself.treeDetails.expandAll()\nself.treeDetails.resizeColumnToContents(0)\nself.treeDetails.resizeColumnToContents(1)", "path": "src\\ZigBeeSniffer.py", "repo_name": "grucha/Sniffer", "stars": 1, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nThis event takes care about unsaved data and running serial transmission while user wants to exit app.\n\"\"\"\n", "func_signal": "def closeEvent(self, event):\n", "code": "if self.state not in (NONE, STOP):\n\treply = QtGui.QMessageBox.question(self, 'Close ZigBee Snifer application',\n\t\t\"Sniffer is working. Are you sure to quit?\", QtGui.QMessageBox.Yes, QtGui.QMessageBox.No)\n\tif reply == QtGui.QMessageBox.Yes:\n\t\tself.sniffer.disconnect_()\n\t\tevent.accept()\n\telse:\n\t\tevent.ignore()\nelif self.packets.changed() or self.packets.nodes.changed():\n\treply = QtGui.QMessageBox.question(self, 'Close ZigBee Snifer application',\n\t\t\"There is unsaved data in current sniff log. Are you sure to quit?\", QtGui.QMessageBox.Yes, QtGui.QMessageBox.No)\n\tif reply == QtGui.QMessageBox.Yes:\n\t\tevent.accept()\n\telse:\n\t\tevent.ignore()\nelse:\n\tevent.accept()", "path": "src\\ZigBeeSniffer.py", "repo_name": "grucha/Sniffer", "stars": 1, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nEnable/disable all widgets given.\nWidgets are in args, value is in kwargs (e.g. value=True)\n\"\"\"\n", "func_signal": "def enable_controls(self, *args, **kwargs):\n", "code": "if not kwargs.has_key('value'):\n\tkwargs['value'] = True\nfor widget in args:\n\twidget.setEnabled(kwargs['value'])", "path": "src\\ZigBeeSniffer.py", "repo_name": "grucha/Sniffer", "stars": 1, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nThis function is used by Qt to obtain columns' headers.\n\"\"\"\n", "func_signal": "def headerData(self, section, orientation, role=Qt.DisplayRole):\n", "code": "if role == Qt.TextAlignmentRole:\n\tif orientation == Qt.Horizontal:\n\t\treturn QVariant(int(Qt.AlignCenter|Qt.AlignVCenter))\n\treturn QVariant(int(Qt.AlignCenter|Qt.AlignVCenter))\n\nif role != Qt.DisplayRole:\n\treturn QVariant()\n\nif orientation == Qt.Horizontal:\n\tif section == FIELD:\n\t\treturn QVariant('Field')\n\telif section == VALUE:\n\t\treturn QVariant('Value')\nreturn QVariant(int(section + 1))", "path": "src\\snifferData\\packetDetails.py", "repo_name": "grucha/Sniffer", "stars": 1, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nClear collected data and start everything from scratch.\n\"\"\"\n", "func_signal": "def on_actionNew_activated(self):\n", "code": "self.packets.clear()\nself.packets.nodes.clear()\nself.state = STOP\nself.update_ui()", "path": "src\\ZigBeeSniffer.py", "repo_name": "grucha/Sniffer", "stars": 1, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nThis function is mandatory for the model to work.\n\"\"\"\n", "func_signal": "def rowCount(self, index=QModelIndex()):\n", "code": "if index.isValid():\n\tparent = index.row()\nelse:\n\tparent = None\n\nreturn len(filter(lambda x: x[2] == parent, self.fields))", "path": "src\\snifferData\\packetDetails.py", "repo_name": "grucha/Sniffer", "stars": 1, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nClears the fields list\n\"\"\"\n", "func_signal": "def clear(self):\n", "code": "self.beginRemoveRows(QModelIndex(), 0, len(self.fields))\nself.fields = []\nself.endRemoveRows()", "path": "src\\snifferData\\packetDetails.py", "repo_name": "grucha/Sniffer", "stars": 1, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nStart/stop sniffing packets on selected channel.\n\"\"\"\n", "func_signal": "def on_actionSniff_activated(self):\n", "code": "if self.state in (STOP, NONE):\n\tself.sniffer.start_reading()\n\tself.sniffer.cmd_sniff(self.channel)\n\tself.state = SNIFF\nelif self.state == SNIFF:\n\tself.sniffer.cmd_stop() \n\tself.sniffer.stop_reading()\n\tself.state = STOP\nself.update_ui()", "path": "src\\ZigBeeSniffer.py", "repo_name": "grucha/Sniffer", "stars": 1, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nThis function is used by Qt to obtain columns' headers.\n\"\"\"\n", "func_signal": "def headerData(self, section, orientation, role=Qt.DisplayRole):\n", "code": "if role == Qt.TextAlignmentRole:\n\treturn QVariant(int(Qt.AlignCenter|Qt.AlignVCenter))\n\nif role != Qt.DisplayRole:\n\treturn QVariant()\nif orientation == Qt.Horizontal:\n\tif section == CHN:\n\t\treturn QVariant('Chn.')\n\telif section == FREQ:\n\t\treturn QVariant('Freq.')\n\telif section == LQI:\n\t\treturn QVariant('LQI')\n\telif section == BCN:\n\t\treturn QVariant(\"Bcn\")\nreturn QVariant(int(section + 1)) # TODO: what's this?????", "path": "src\\snifferData\\channels.py", "repo_name": "grucha/Sniffer", "stars": 1, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nClears the packets list\n\"\"\"\n", "func_signal": "def clear(self):\n", "code": "self.beginRemoveRows(QModelIndex(), 0, len(self.channels))\nself.packets = []\nself.endRemoveRows()", "path": "src\\snifferData\\channels.py", "repo_name": "grucha/Sniffer", "stars": 1, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nStart/stop scanning channels.\n\"\"\"\n", "func_signal": "def on_actionScan_channels_activated(self):\n", "code": "if self.state == SCAN_CHANNELS:\n\tself.timerScan.stop()\n\tself.sniffer.cmd_stop()\n\tself.sniffer.stop_reading()\n\tself.channel = self.rememberChannel\n\tself.state = STOP\nelse:\n\tself.rememberChannel = int(self.channel)\n\tself.channel = 10 # it'll be incremented, so it'll start from 11\n\tself.timerScan = QtCore.QTimer()\n\tself.timerScan.connect(self.timerScan, QtCore.SIGNAL(\"timeout()\"), self.scan_channels)\n\tself.scan_channels()\n\tself.timerScan.start(2500)\n\tself.state = SCAN_CHANNELS\nself.update_ui()", "path": "src\\ZigBeeSniffer.py", "repo_name": "grucha/Sniffer", "stars": 1, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"\nThis function is mandatory for the model to work.\n\"\"\"\n", "func_signal": "def rowCount(self, index=QModelIndex()):\n", "code": "if not index.isValid():\n\treturn len(self.channels)\nreturn 0 # <---channels have no children", "path": "src\\snifferData\\channels.py", "repo_name": "grucha/Sniffer", "stars": 1, "license": "None", "language": "python", "size": 248}
{"docstring": "\"\"\"Write a file into the archive.  The contents is the string\n'bytes'.  'zinfo_or_arcname' is either a ZipInfo instance or\nthe name of the file in the archive.\"\"\"\n", "func_signal": "def writestr(self, zinfo_or_arcname, bytes):\n", "code": "if not isinstance(zinfo_or_arcname, ZipInfo):\n    zinfo = ZipInfo(filename=zinfo_or_arcname,\n                    date_time=time.localtime(time.time()))\n    zinfo.compress_type = self.compression\nelse:\n    zinfo = zinfo_or_arcname\nself._writecheck(zinfo)\nzinfo.file_size = len(bytes)            # Uncompressed size\nzinfo.CRC = binascii.crc32(bytes)       # CRC-32 checksum\nif zinfo.compress_type == ZIP_DEFLATED:\n    co = zlib.compressobj(zlib.Z_DEFAULT_COMPRESSION,\n         zlib.DEFLATED, -15)\n    bytes = co.compress(bytes) + co.flush()\n    zinfo.compress_size = len(bytes)    # Compressed size\nelse:\n    zinfo.compress_size = zinfo.file_size\nzinfo.header_offset = self.fp.tell()    # Start of header bytes\nself.fp.write(zinfo.FileHeader())\nzinfo.file_offset = self.fp.tell()      # Start of file bytes\nself.fp.write(bytes)\nif zinfo.flag_bits & 0x08:\n    # Write CRC and file sizes after the file data\n    self.fp.write(struct.pack(\"<lll\", zinfo.CRC, zinfo.compress_size,\n          zinfo.file_size))\nself.filelist.append(zinfo)\nself.NameToInfo[zinfo.filename] = zinfo", "path": "pygopherd\\zipfile.py", "repo_name": "Almad/pygopherd", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 3639}
{"docstring": "\"\"\"Quickly see if file is a ZIP file by checking the magic number.\"\"\"\n", "func_signal": "def is_zipfile(filename):\n", "code": "try:\n    fpin = open(filename, \"rb\")\n    endrec = _EndRecData(fpin)\n    fpin.close()\n    if endrec:\n        return True                 # file has correct magic number\nexcept IOError:\n    pass\nreturn False", "path": "pygopherd\\zipfile.py", "repo_name": "Almad/pygopherd", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 3639}
{"docstring": "\"\"\"This is an almost exact clone of UMN's GSfromLink function.\"\"\"\n", "func_signal": "def getLinkItem(self, fd, capfilepath = None):\n", "code": "entry = LinkEntry(self.entry.selector, self.config)\nnextstep = 'continue'\n\ndone = {'path' : 0, 'type' : 0, 'name' : 0, 'host' : 0, 'port' : 0}\n\nif capfilepath != None:\n    entry.setselector(capfilepath)\n    done['path'] = 1\n\nwhile 1:\n    line = fd.readline()\n    if not line:\n        nextstep = 'stop'\n        break\n    line = line.strip()\n\n    # Empty.\n    if len(line) == 0:\n        break\n\n    # Comment.\n    if line[0] == '#':\n        if done['path']:\n            break\n        else:\n            continue\n\n    # Type.\n    if line[0:5] == \"Type=\":\n        entry.settype(line[5])\n        # FIXME: handle if line[6] is + or ?\n        done['type'] = 1\n    elif line[0:5] == \"Name=\":\n        entry.setname(line[5:])\n        done['name'] = 1\n    elif line[0:5] == \"Path=\":\n        pathname = line[5:]\n        if len(pathname) and pathname[-1] == '/':\n            pathname = pathname[0:-1]\n        if len(line) >= 7 and (line[5:7] == './' or line[5:7] == '~/'):\n            # Handle ./: make full path.\n            entry.setselector(self.selectorbase + \"/\" + pathname[2:])\n            entry.setneedsmerge(1)\n        elif len(pathname) and pathname[0] != '/':\n            entry.setselector(pathname)\n            entry.setneedsabspath(1)\n        else:\n            entry.setselector(pathname)\n        done['path'] = 1\n    elif line[0:5] == 'Host=':\n        if line[5:] != '+':\n            entry.sethost(line[5:])\n        done['host'] = 1\n    elif line[0:5] == 'Port=':\n        if line[5:] != '+':\n            entry.setport(int(line[5:]))\n        done['port'] = 1\n    elif line[0:5] == 'Numb=':\n        try:            # Don't crash if we can't parse the number\n            entry.setnum(int(line[5:]))\n        except:\n            pass\n    elif line[0:9] == 'Abstract=':\n        abstractstr = \"\"\n        abstractline = line[9:]\n        while len(abstractline) and abstractline[-1] == \"\\\\\":\n            abstractstr += abstractline[0:-1] + \"\\n\"\n            abstractline = fd.readline().strip()\n        abstractstr += abstractline\n\n        if abstractstr:\n            entry.setea('ABSTRACT', abstractstr)\n    elif line[0:6] == 'Admin=' or \\\n         line[0:4] == 'URL=' or \\\n         line[0:4] == 'TTL=':\n        pass\n    else:\n        break\n    ### FIXME: Handle Admin, URL, TTL\n\nif done['path']:\n    if entry.getneedsabspath() and \\\n           entry.gethost() == None and entry.getport() == None:\n        entry.setselector(os.path.normpath(self.selectorbase + \"/\" + entry.getselector()))\n    return (nextstep, entry)\nreturn (nextstep, None)", "path": "pygopherd\\handlers\\UMN.py", "repo_name": "Almad/pygopherd", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 3639}
{"docstring": "\"\"\"Override the parent to process dotfiles and keep them out\nof the list.\"\"\"\n", "func_signal": "def prep_initfiles_canaddfile(self, ignorepatt, pattern, file):\n", "code": "if DirHandler.prep_initfiles_canaddfile(self, ignorepatt, pattern,\n                                         file):\n    # If the parent says it's OK, then let's see if it's\n    # a link file.  If yes, process it and return false.\n    if file[0] == '.':\n        if not self.vfs.isdir(self.selectorbase + '/' + file):\n            self.linkentries.extend(self.processLinkFile(self.selectorbase + '/' + file))\n            return 0\n        else:\n            return 0            # A \"dot dir\" -- ignore.\n    return 1                    # Not a dot file -- return true\nelse:\n    return 0                    # Parent returned 0, do the same.", "path": "pygopherd\\handlers\\UMN.py", "repo_name": "Almad/pygopherd", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 3639}
{"docstring": "\"\"\"Close the file, and for mode \"w\" and \"a\" write the ending\nrecords.\"\"\"\n", "func_signal": "def close(self):\n", "code": "if self.fp is None:\n    return\nif not self._filePassed:\n    self.fp.close()\nself.fp = None", "path": "pygopherd\\zipfile.py", "repo_name": "Almad/pygopherd", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 3639}
{"docstring": "\"\"\"Read all the files and check the CRC.\"\"\"\n", "func_signal": "def testzip(self):\n", "code": "for zinfo in self.filelist:\n    try:\n        self.read(zinfo.filename)       # Check CRC-32\n    except:\n        return zinfo.filename", "path": "pygopherd\\zipfile.py", "repo_name": "Almad/pygopherd", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 3639}
{"docstring": "\"\"\"Return data from the \"End of Central Directory\" record, or None.\n\nThe data is a list of the nine items in the ZIP \"End of central dir\"\nrecord followed by a tenth item, the file seek offset of this record.\"\"\"\n", "func_signal": "def _EndRecData(fpin):\n", "code": "fpin.seek(-22, 2)               # Assume no archive comment.\nfilesize = fpin.tell() + 22     # Get file size\ndata = fpin.read()\nif data[0:4] == stringEndArchive and data[-2:] == \"\\000\\000\":\n    endrec = struct.unpack(structEndArchive, data)\n    endrec = list(endrec)\n    endrec.append(\"\")               # Append the archive comment\n    endrec.append(filesize - 22)    # Append the record start offset\n    return endrec\n# Search the last END_BLOCK bytes of the file for the record signature.\n# The comment is appended to the ZIP file and has a 16 bit length.\n# So the comment may be up to 64K long.  We limit the search for the\n# signature to a few Kbytes at the end of the file for efficiency.\n# also, the signature must not appear in the comment.\nEND_BLOCK = min(filesize, 1024 * 4)\nfpin.seek(filesize - END_BLOCK, 0)\ndata = fpin.read()\nstart = data.rfind(stringEndArchive)\nif start >= 0:     # Correct signature string was found\n    endrec = struct.unpack(structEndArchive, data[start:start+22])\n    endrec = list(endrec)\n    comment = data[start+22:]\n    if endrec[7] == len(comment):     # Comment length checks out\n        # Append the archive comment and start offset\n        endrec.append(comment)\n        endrec.append(filesize - END_BLOCK + start)\n        return endrec\nreturn      # Error, return None", "path": "pygopherd\\zipfile.py", "repo_name": "Almad/pygopherd", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 3639}
{"docstring": "\"\"\"Return the per-file header as a string.\"\"\"\n", "func_signal": "def FileHeader(self):\n", "code": "dt = self.date_time\ndosdate = (dt[0] - 1980) << 9 | dt[1] << 5 | dt[2]\ndostime = dt[3] << 11 | dt[4] << 5 | (dt[5] // 2)\nif self.flag_bits & 0x08:\n    # Set these to zero because we write them after the file data\n    CRC = compress_size = file_size = 0\nelse:\n    CRC = self.CRC\n    compress_size = self.compress_size\n    file_size = self.file_size\nheader = struct.pack(structFileHeader, stringFileHeader,\n         self.extract_version, self.reserved, self.flag_bits,\n         self.compress_type, dostime, dosdate, CRC,\n         compress_size, file_size,\n         len(self.filename), len(self.extra))\nreturn header + self.filename + self.extra", "path": "pygopherd\\zipfile.py", "repo_name": "Almad/pygopherd", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 3639}
{"docstring": "\"\"\"Open the ZIP file with mode read \"r\", write \"w\" or append \"a\".\"\"\"\n", "func_signal": "def __init__(self, file):\n", "code": "self.debug = 0  # Level of printing: 0 through 3\nself.locationmap = {} \n\n# Check if we were passed a file-like object\nif type(file) in _STRING_TYPES:\n    self._filePassed = 0\n    self.filename = file\n    self.fp = open(file, 'rb')\nelse:\n    self._filePassed = 1\n    self.fp = file\n    self.filename = getattr(file, 'name', None)\ntry:\n    self._InitZip()\nexcept BadZipfile:\n    if not self._filePassed:\n        self.fp.close()\n        self.fp = None\n    raise", "path": "pygopherd\\zipfile.py", "repo_name": "Almad/pygopherd", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 3639}
{"docstring": "\"\"\"Read all the files and check the CRC.\"\"\"\n", "func_signal": "def testzip(self):\n", "code": "for name in self.locationmap.iterkeys():\n    try:\n        self.read(name)       # Check CRC-32\n    except:\n        return name", "path": "pygopherd\\zipfile.py", "repo_name": "Almad/pygopherd", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 3639}
{"docstring": "\"\"\"This function implements an exact replica of UMN behavior\nGSqsortcmp() behavior.\"\"\"\n", "func_signal": "def entrycmp(self, entry1, entry2):\n", "code": "if entry1.name == None:\n    return 1\nif entry2.name == None:\n    return -1\ne1num = entry1.getnum(0)\ne2num = entry2.getnum(0)\n\n# Equal numbers or no numbers: sort by title.\nif e1num == e2num:\n    return cmp(entry1.name, entry2.name)\n\n# Same signs: use plain numeric comparison.\nif (self.sgn(e1num) == self.sgn(e2num)):\n    return cmp(e1num, e2num)\n\n# Different signs: other comparison.\nif e1num > e2num:\n    return -1\nelse:\n    return 1", "path": "pygopherd\\handlers\\UMN.py", "repo_name": "Almad/pygopherd", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 3639}
{"docstring": "\"\"\"Override parent to do a few more things and override sort order.\"\"\"\n# Initialize.\n", "func_signal": "def prepare(self):\n", "code": "self.linkentries = []\n\n# Let the parent do the directory walking for us.  Will call\n# prep_initfiles_canaddfile and prep_entriesappend.\nif DirHandler.prepare(self):\n    # Returns 1 if it didn't load from the cache.\n    # Merge and sort.\n    self.MergeLinkFiles()\n    self.fileentries.sort(self.entrycmp)", "path": "pygopherd\\handlers\\UMN.py", "repo_name": "Almad/pygopherd", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 3639}
{"docstring": "\"\"\"Returns -1 if less than 0, 1 if greater than 0, and 0 if\nequal to zero.\"\"\"\n", "func_signal": "def sgn(self, a):\n", "code": "if a == 0:\n    return 0\nif a < 0:\n    return -1\nreturn 1", "path": "pygopherd\\handlers\\UMN.py", "repo_name": "Almad/pygopherd", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 3639}
{"docstring": "\"\"\"Read the directory, making sure we close the file if the format\nis bad.\"\"\"\n", "func_signal": "def _GetContents(self):\n", "code": "try:\n    self._RealGetContents()\nexcept BadZipfile:\n    if not self._filePassed:\n        self.fp.close()\n        self.fp = None\n    raise", "path": "pygopherd\\zipfile.py", "repo_name": "Almad/pygopherd", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 3639}
{"docstring": "\"\"\"Called to merge the files from .Links and .names into the\nobjects obtained by walking the directory.  According to UMN code,\nwe ONLY merge if the Path starts with ./ or ~/ in the file.  This\nis set in the getneedsmerge() attribute.  If that attribute is\nnot set, don't even bother with it -- just add.\"\"\"\n\n# For faster matching, make a dictionary out of the list.\n\n", "func_signal": "def MergeLinkFiles(self):\n", "code": "fileentriesdict = {}\nfor entry in self.fileentries:\n    fileentriesdict[entry.selector] = entry\n\nfor linkentry in self.linkentries:\n    if not linkentry.getneedsmerge():\n        self.fileentries.append(linkentry)\n        continue\n    if fileentriesdict.has_key(linkentry.selector):\n        if linkentry.gettype() == 'X':\n            # It's special code to hide something.\n            self.fileentries.remove(fileentriesdict[linkentry.selector])\n        else:\n            self.mergeentries(fileentriesdict[linkentry.selector],\n                              linkentry)\n    else:\n        self.fileentries.append(linkentry)", "path": "pygopherd\\handlers\\UMN.py", "repo_name": "Almad/pygopherd", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 3639}
{"docstring": "\"\"\"Takes the set fields from new and modifies old to have their\nvalue.\"\"\"\n", "func_signal": "def mergeentries(self, old, new):\n", "code": "for field in ['selector', 'type', 'name', 'host', 'port']:\n    if getattr(new, field):\n        setattr(old, field, getattr(new, field))\n\nfor field in new.geteadict().keys():\n    old.setea(field, new.getea(field))", "path": "pygopherd\\handlers\\UMN.py", "repo_name": "Almad/pygopherd", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 3639}
{"docstring": "\"\"\"Return a list of file names in the archive.\"\"\"\n", "func_signal": "def namelist(self):\n", "code": "l = []\nfor data in self.filelist:\n    l.append(data.filename)\nreturn l", "path": "pygopherd\\zipfile.py", "repo_name": "Almad/pygopherd", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 3639}
{"docstring": "\"\"\"Close the file, and for mode \"w\" and \"a\" write the ending\nrecords.\"\"\"\n", "func_signal": "def close(self):\n", "code": "if self.fp is None:\n    return\nif self.mode in (\"w\", \"a\"):             # write ending records\n    count = 0\n    pos1 = self.fp.tell()\n    for zinfo in self.filelist:         # write central directory\n        count = count + 1\n        dt = zinfo.date_time\n        dosdate = (dt[0] - 1980) << 9 | dt[1] << 5 | dt[2]\n        dostime = dt[3] << 11 | dt[4] << 5 | (dt[5] // 2)\n        centdir = struct.pack(structCentralDir,\n          stringCentralDir, zinfo.create_version,\n          zinfo.create_system, zinfo.extract_version, zinfo.reserved,\n          zinfo.flag_bits, zinfo.compress_type, dostime, dosdate,\n          zinfo.CRC, zinfo.compress_size, zinfo.file_size,\n          len(zinfo.filename), len(zinfo.extra), len(zinfo.comment),\n          0, zinfo.internal_attr, zinfo.external_attr,\n          zinfo.header_offset)\n        self.fp.write(centdir)\n        self.fp.write(zinfo.filename)\n        self.fp.write(zinfo.extra)\n        self.fp.write(zinfo.comment)\n    pos2 = self.fp.tell()\n    # Write end-of-zip-archive record\n    endrec = struct.pack(structEndArchive, stringEndArchive,\n             0, 0, count, count, pos2 - pos1, pos1, 0)\n    self.fp.write(endrec)\n    self.fp.flush()\nif not self._filePassed:\n    self.fp.close()\nself.fp = None", "path": "pygopherd\\zipfile.py", "repo_name": "Almad/pygopherd", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 3639}
{"docstring": "\"\"\"Creates a testing handler with input from rfile.  Fills in\nother stuff with fake values.\"\"\"\n\n", "func_signal": "def gettestinghandler(rfile, wfile, config = None):\n", "code": "config = config or getconfig()\n\n# Kludge to pass to the handler init.\n\nclass requestClass:\n    def __init__(self, rfile, wfile):\n        self.rfile = rfile\n        self.wfile = wfile\n    def makefile(self, mode, bufsize):\n        if mode[0] == 'r':\n            return self.rfile\n        return self.wfile\n\nclass handlerClass(initialization.GopherRequestHandler):\n    def __init__(self, request, client_address, server):\n        self.request = request\n        self.client_address = client_address\n        self.server = server\n        self.setup()\n\ns = gettestingserver(config)\nrhandler = handlerClass(requestClass(rfile, wfile),\n                        ('10.77.77.77', '7777'),\n                        s)\nreturn rhandler", "path": "pygopherd\\testutil.py", "repo_name": "Almad/pygopherd", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 3639}
{"docstring": "\"\"\"Processes a link file.  If capfilepath is set, it should\nbe the equivolent of the Path= in a .names file.\"\"\"\n", "func_signal": "def processLinkFile(self, filename, capfilepath = None):\n", "code": "linkentries = []\nfd = self.vfs.open(filename, \"rt\")\nwhile 1:\n    nextstep, entry = self.getLinkItem(fd, capfilepath)\n    if entry:\n        linkentries.append(entry)\n    if nextstep == 'stop':\n        break\nreturn linkentries", "path": "pygopherd\\handlers\\UMN.py", "repo_name": "Almad/pygopherd", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 3639}
{"docstring": "# The new arguments for this field aren't explicitly defined so that\n# users can still use normal ImageField positional arguments.\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "thumbnail=kwargs.pop('thumbnail', None)\nextra_thumbnails=kwargs.pop('extra_thumbnails', None)\nthumbnail_tag=kwargs.pop('thumbnail_tag', TAG_HTML)\n\nsuper(ImageWithThumbnailsField, self).__init__(*args, **kwargs)\nif thumbnail:\n    _verify_thumbnail_attrs(thumbnail)\nif extra_thumbnails:\n    for extra, attrs in extra_thumbnails.items():\n        name = \"%r of 'extra_thumbnails'\"\n        _verify_thumbnail_attrs(attrs, name)\nself.thumbnail = thumbnail\nself.extra_thumbnails = extra_thumbnails\nself.thumbnail_tag = thumbnail_tag", "path": "jung\\sorl\\thumbnail\\fields.py", "repo_name": "rob-b/Jung", "stars": 0, "license": "None", "language": "python", "size": 1312}
{"docstring": "# Adding model 'TaskType'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_table('schedule_tasktype', (\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(_('Title'), unique=True, max_length=100)),\n        ))\n        db.send_create_signal('schedule', ['TaskType'])\n# Adding model 'Task'\n        db.create_table('schedule_task', (\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(_('Title'), max_length=100)),\n            ('slug', AutoSlugField(_('Slug'), editable=True, populate_from='title')),\n            ('body', MarkdownField(_('Body'), blank=True)),\n            ('author', models.ForeignKey(orm['auth.User'], related_name='tasks_created')),\n            ('user', models.ForeignKey(orm['auth.User'])),\n            ('task_type', models.ForeignKey(orm.TaskType)),\n            ('body_rendered', models.TextField(editable=False)),\n        ))\n        db.send_create_signal('schedule', ['Task'])\n# Adding model 'Occurrence'\n        db.create_table('schedule_occurrence', (\n            ('id', models.AutoField(primary_key=True)),\n            ('start_time', models.DateTimeField()),\n            ('end_time', models.DateTimeField()),\n            ('task', models.ForeignKey(orm.Task)),\n        ))\n        db.send_create_signal('schedule', ['Occurrence'])", "path": "jung\\schedule\\migrations\\0001_initial.py", "repo_name": "rob-b/Jung", "stars": 0, "license": "None", "language": "python", "size": 1312}
{"docstring": "\"\"\"Double bookings should be flagged\"\"\"\n\n# add 3 occurences of a task starting today\n", "func_signal": "def test_conflicts(self):\n", "code": "start = datetime.combine(date.today(), time(10))\nend = start + relativedelta(hours=+4)\nself.task.add_occurrences(start, end, count=3,)\n\ntask_2 = Task.objects.create(\n    title='Secodary task',\n    author=self.alice,\n    user=self.bob,\n    task_type=self.et,\n    project=self.project,\n)\ntask_2.add_occurrences(start, end)\n\n# there should be two occurences that are not conflicted\nself.assertEqual(2, Occurrence.objects.pristine().count())\n\n# and another two that are conflicted\nself.assertEqual(2, Occurrence.objects.conflicted().count())\n\n# the conflicted occurences are from different tasks\nfirst, second = Occurrence.objects.conflicted()\nself.assertNotEqual(first.task.pk, second.task.pk)\n\n# the pristine occurences are from the same tasks\nfirst, second = Occurrence.objects.pristine()\nself.assertEqual(first.task.pk, second.task.pk)", "path": "jung\\schedule\\tests.py", "repo_name": "rob-b/Jung", "stars": 0, "license": "None", "language": "python", "size": 1312}
{"docstring": "\"\"\"\nSet the source filetype. First it tries to use magic and\nif import error it will just use the extension\n\"\"\"\n", "func_signal": "def _get_source_filetype(self):\n", "code": "if not hasattr(self, '_source_filetype'):\n    if not isinstance(self.source, basestring):\n        # Assuming a file-like object - we won't know it's type.\n        return None\n    try:\n        import magic\n    except ImportError:\n        self._source_filetype = splitext(self.source)[1].lower().\\\n           replace('.', '').replace('jpeg', 'jpg')\n    else:\n        m = magic.open(magic.MAGIC_NONE)\n        m.load()\n        ftype = m.file(self.source)\n        if ftype.find('Microsoft Office Document') != -1:\n            self._source_filetype = 'doc'\n        elif ftype.find('PDF document') != -1:\n            self._source_filetype = 'pdf'\n        elif ftype.find('JPEG') != -1:\n            self._source_filetype = 'jpg'\n        else:\n            self._source_filetype = ftype\nreturn self._source_filetype", "path": "jung\\sorl\\thumbnail\\base.py", "repo_name": "rob-b/Jung", "stars": 0, "license": "None", "language": "python", "size": 1312}
{"docstring": "\"\"\"if a decorated context_processor is called on a page the url of which\nbegins with admin_path then return an empty dictionary\"\"\"\n", "func_signal": "def exclude_admin(admin_path='/admin'):\n", "code": "def decorator(f):\n    def wrapper(request):\n        if request.get_full_path().startswith(admin_path):\n            return {}\n        return f(request)\n    return wraps(f)(wrapper)\nreturn decorator", "path": "jung\\hostel\\context_processors.py", "repo_name": "rob-b/Jung", "stars": 0, "license": "None", "language": "python", "size": 1312}
{"docstring": "# Deleting model 'TaskType'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.delete_table('schedule_tasktype')\n# Deleting model 'Task'\n        db.delete_table('schedule_task')\n# Deleting model 'Occurrence'\n        db.delete_table('schedule_occurrence')", "path": "jung\\schedule\\migrations\\0001_initial.py", "repo_name": "rob-b/Jung", "stars": 0, "license": "None", "language": "python", "size": 1312}
{"docstring": "# years is an optional list/tuple of years to use in the \"year\" select box.\n", "func_signal": "def __init__(self, attrs=None, years=None):\n", "code": "self.attrs = attrs or {}\nif years:\n    self.years = years\n    self.years.reverse()\nelse:\n    this_year = datetime.date.today().year\n    self.years = range(this_year, this_year+10)\n    self.years.reverse()", "path": "jung\\hostel\\widgets.py", "repo_name": "rob-b/Jung", "stars": 0, "license": "None", "language": "python", "size": 1312}
{"docstring": "\"\"\"\nEnsure the source file exists. If source is not a string then it is\nassumed to be a file-like instance which \"exists\".\n\"\"\"\n", "func_signal": "def _check_source_exists(self):\n", "code": "if not hasattr(self, '_source_exists'):\n    self._source_exists = (self.source and\n                           (not isinstance(self.source, basestring) or\n                            isfile(self.source)))\nreturn self._source_exists", "path": "jung\\sorl\\thumbnail\\base.py", "repo_name": "rob-b/Jung", "stars": 0, "license": "None", "language": "python", "size": 1312}
{"docstring": "# Build kwargs\n", "func_signal": "def _build_thumbnail(self, args):\n", "code": "kwargs = {}\nfor k, v in args.items():\n    kwargs[ALL_ARGS[k]] = v\n# Return thumbnail\nrelative_source_path = getattr(self.instance, self.field.name).name\nreturn DjangoThumbnail(relative_source_path, **kwargs)", "path": "jung\\sorl\\thumbnail\\fields.py", "repo_name": "rob-b/Jung", "stars": 0, "license": "None", "language": "python", "size": 1312}
{"docstring": "# regardless of how they arrived here we can redirect the user to the\n    # requested site.\n", "func_signal": "def redirect_to_market(request, market):\n", "code": "    site = get_object_or_404(Site, domain__istartswith=market)\n    dest = request.META.get('HTTP_REFERER', None)\n    if dest:\n        dest = dest.replace(RequestSite(request).domain, site.domain)\n    else:\n        dest = site.domain\n    Site.objects.clear_cache()\n    response = HttpResponseRedirect(dest)\n# setting the language on the other hand can only be done via a post request\n    if request.method == 'POST':\n        lang_code = market.split('.')[-1]\n        if check_for_language(lang_code):\n            if hasattr(request, 'session'):\n                request.session['django_language'] = lang_code\n            else:\n                response.set_cookie(settings.LANGUAGE_COOKIE_NAME, lang_code)\n    return response", "path": "jung\\hostel\\utils\\__init__.py", "repo_name": "rob-b/Jung", "stars": 0, "license": "None", "language": "python", "size": 1312}
{"docstring": "\"\"\"djangosnippets.org/snippets/1420\"\"\"\n", "func_signal": "def form_kwargs(request, kwargs=None):\n", "code": "if kwargs is None:\n    kwargs ={}\nif request.method == 'POST':\n    kwargs['data'] = request.POST\n    kwargs['files'] = request.FILES\nreturn kwargs", "path": "jung\\hostel\\utils\\__init__.py", "repo_name": "rob-b/Jung", "stars": 0, "license": "None", "language": "python", "size": 1312}
{"docstring": "\"\"\"Return all occurrences that do not overlap with any other\"\"\"\n", "func_signal": "def pristine(self):\n", "code": "return self.exclude(\n    id__in=self.conflicted().values_list('id', flat=True)\n).extra(select={'conflict': 0})", "path": "jung\\schedule\\managers.py", "repo_name": "rob-b/Jung", "stars": 0, "license": "None", "language": "python", "size": 1312}
{"docstring": "\"\"\"\nGenerates the thumbnail if it doesn't exist or if the file date of the\nsource file is newer than that of the thumbnail.\n\"\"\"\n# Ensure dest(ination) attribute is set\n", "func_signal": "def generate(self):\n", "code": "if not self.dest:\n    raise ThumbnailException(\"No destination filename set.\")\n\nif not isinstance(self.dest, basestring):\n    # We'll assume dest is a file-like instance if it exists but isn't\n    # a string.\n    self._do_generate()\nelif not isfile(self.dest) or (self.source_exists and\n    getmtime(self.source) > getmtime(self.dest)):\n\n    # Ensure the directory exists\n    directory = dirname(self.dest)\n    if not isdir(directory):\n        os.makedirs(directory)\n\n    self._do_generate()", "path": "jung\\sorl\\thumbnail\\base.py", "repo_name": "rob-b/Jung", "stars": 0, "license": "None", "language": "python", "size": 1312}
{"docstring": "\"\"\"Trigget the save method on all objects\"\"\"\n", "func_signal": "def resave(request, queryset):\n", "code": "for obj in queryset:\n    obj.save()", "path": "jung\\hostel\\admin\\actions.py", "repo_name": "rob-b/Jung", "stars": 0, "license": "None", "language": "python", "size": 1312}
{"docstring": "\"\"\"\nGet a thumbnail setting from Django settings module, falling back to the\ndefault.\n\nIf override is not None, it will be used instead of the setting.\n\"\"\"\n", "func_signal": "def get_thumbnail_setting(setting, override=None):\n", "code": "if override is not None:\n    return override\nif hasattr(settings, 'THUMBNAIL_%s' % setting):\n    return getattr(settings, 'THUMBNAIL_%s' % setting)\nelse:\n    return getattr(defaults, setting)", "path": "jung\\sorl\\thumbnail\\main.py", "repo_name": "rob-b/Jung", "stars": 0, "license": "None", "language": "python", "size": 1312}
{"docstring": "'''\nAdd one or more occurences to the task using a comparable API to\n``dateutil.rrule``.\n\nIf ``rrule_params`` does not contain a ``freq``, one will be defaulted\nto ``rrule.DAILY``.\n\nBecause ``rrule.rrule`` returns an iterator that can essentially be\nunbounded, we need to slightly alter the expected behavior here in order\nto enforce a finite number of occurrence creation.\n\nIf both ``count`` and ``until`` entries are missing from ``rrule_params``,\nonly a single ``Occurrence`` instance will be created using the exact\n``start_time`` and ``end_time`` values.\n'''\n", "func_signal": "def add_occurrences(self, start_time, end_time, **rrule_params):\n", "code": "rrule_params.setdefault('freq', rrule.DAILY)\n\nif 'count' not in rrule_params and 'until' not in rrule_params:\n    self.occurrence_set.create(start_time=start_time, end_time=end_time)\nelse:\n    delta = end_time - start_time\n    for ev in rrule.rrule(dtstart=start_time, **rrule_params):\n        self.occurrence_set.create(start_time=ev, end_time=ev + delta)", "path": "jung\\schedule\\models.py", "repo_name": "rob-b/Jung", "stars": 0, "license": "None", "language": "python", "size": 1312}
{"docstring": "\"\"\"Return all occurrences that overlap with another occurrence.\n\nbear in mind that most occurence will overlap and so this is only useful\nwhen filtered with something else\"\"\"\n\n", "func_signal": "def conflicted(self):\n", "code": "sql = \"\"\"SELECT DISTINCT  o1.id\nFROM schedule_occurrence o1, schedule_occurrence o2\nWHERE (o2.start_time BETWEEN o1.start_time AND o1.end_time\nOR o2.end_time BETWEEN o1.start_time AND o1.end_time)\nAND o1.id <> o2.id\"\"\"\ncursor = connection.cursor()\ncursor.execute(sql)\nids = chain(*cursor.fetchall())\nreturn self.filter(id__in=ids).extra(select={'conflict': 1})", "path": "jung\\schedule\\managers.py", "repo_name": "rob-b/Jung", "stars": 0, "license": "None", "language": "python", "size": 1312}
{"docstring": "\"\"\"Times outside of work hours should produce errors in the form\"\"\"\n", "func_signal": "def test_occurrence_form(self):\n", "code": "now = datetime.now()\ndata = {\n    'start_time': now.replace(hour=7)\n}\nform = OccurrenceForm(data)\nself.assertFalse(form.is_valid(), '7am starts are not allowed')\nself.assert_('start_time' in form.errors)\nself.assert_('Work hours are' in form.errors['start_time'][0])\n\ndata = {\n    'end_time': now.replace(hour=19)\n}\nform = OccurrenceForm(data)\nself.assertFalse(form.is_valid(), '7pm finishes are not allowed')\nself.assert_('end_time' in form.errors)\nself.assert_('Work hours are' in form.errors['end_time'][0])\n\n# just to ensure that it does not always return invalid\ndata = {\n    'end_time': now.replace(hour=14)\n}\nform = OccurrenceForm(data)\nself.assertFalse(form.is_valid())\nself.assertFalse('end_time' in form.errors)\n\ndata = {\n    'start_time': now.replace(hour=9)\n}\nform = OccurrenceForm(data)\nself.assertFalse(form.is_valid())\nself.assertFalse('start_time' in form.errors)", "path": "jung\\schedule\\tests.py", "repo_name": "rob-b/Jung", "stars": 0, "license": "None", "language": "python", "size": 1312}
{"docstring": "\"\"\"All occurences that occur during the week that features `dt`\"\"\"\n", "func_signal": "def week_of(self, dt):\n", "code": "try:\n    dt = dt.date()\nexcept AttributeError:\n    pass\ndt = dt + timedelta(days=-dt.weekday())\ntd = dt + timedelta(days=6)\ndt = datetime.combine(dt, time(0))\ntd = datetime.combine(td, time(0))\nreturn self.filter(end_time__gt=dt,\n                   start_time__lt=td).distinct()", "path": "jung\\schedule\\managers.py", "repo_name": "rob-b/Jung", "stars": 0, "license": "None", "language": "python", "size": 1312}
{"docstring": "# Delete any thumbnails too (and not just ones defined here in case\n# the {% thumbnail %} tag was used or the thumbnail sizes changed).\n", "func_signal": "def delete(self, save=True):\n", "code": "relative_source_path = getattr(self.instance, self.field.name).name\ndelete_thumbnails(relative_source_path)\nsuper(ImageWithThumbnailsFieldFile, self).delete(save)", "path": "jung\\sorl\\thumbnail\\fields.py", "repo_name": "rob-b/Jung", "stars": 0, "license": "None", "language": "python", "size": 1312}
{"docstring": "\"\"\"get the next character, excluding comments. peek() is used to see\n   if a '/' is followed by a '/' or '*'.\n\"\"\"\n", "func_signal": "def _next(self):\n", "code": "c = self._get()\nif c == '/':\n    p = self._peek()\n    if p == '/':\n        c = self._get()\n        while c > '\\n':\n            c = self._get()\n        return c\n    if p == '*':\n        c = self._get()\n        while 1:\n            c = self._get()\n            if c == '*':\n                if self._peek() == '/':\n                    self._get()\n                    return ' '\n            if c == '\\000':\n                raise UnterminatedComment()\n\nreturn c", "path": "apps\\compress\\filters\\jsmin\\jsmin.py", "repo_name": "rpribadi/flakeware", "stars": 1, "license": "None", "language": "python", "size": 828}
{"docstring": "\"\"\"\nSet an object's tags.\n\"\"\"\n", "func_signal": "def __set__(self, instance, value):\n", "code": "if instance is None:\n    raise AttributeError(_('%s can only be set on instances.') % self.name)\nif settings.FORCE_LOWERCASE_TAGS and value is not None:\n    value = value.lower()\nself._set_instance_tag_cache(instance, value)", "path": "apps\\tagging\\fields.py", "repo_name": "rpribadi/flakeware", "stars": 1, "license": "None", "language": "python", "size": 828}
{"docstring": "\"\"\"\nReturns a HttpResponse whose content is a Javscript file representing a\nlist of links to flatpages.\n\"\"\"\n", "func_signal": "def flatpages_link_list(request):\n", "code": "from django.contrib.flatpages.models import FlatPage\nlink_list = [(page.title, page.url) for page in FlatPage.objects.all()]\nreturn render_to_link_list(link_list)", "path": "apps\\tinymce\\views.py", "repo_name": "rpribadi/flakeware", "stars": 1, "license": "None", "language": "python", "size": 828}
{"docstring": "\"\"\"\nReturns a HttpResponse whose content is a Javscript file. The template\nis loaded from 'tinymce/<name>_textareas.js' or\n'<name>/tinymce_textareas.js'. Optionally, the lang argument sets the\ncontent language.\n\"\"\"\n", "func_signal": "def textareas_js(request, name, lang=None):\n", "code": "template_files = (\n    'tinymce/%s_textareas.js' % name,\n    '%s/tinymce_textareas.js' % name,\n)\ntemplate = loader.select_template(template_files)\n\nvars = get_language_config(lang)\nvars['content_language'] = lang\ncontext = RequestContext(request, vars)\n\nreturn HttpResponse(template.render(context),\n        content_type=\"application/x-javascript\")", "path": "apps\\tinymce\\views.py", "repo_name": "rpribadi/flakeware", "stars": 1, "license": "None", "language": "python", "size": 828}
{"docstring": "\"\"\"\nUtility function for accepting tag input in a flexible manner.\n\nIf a ``Tag`` object is given, it will be returned in a list as\nits single occupant.\n\nIf given, the tag names in the following will be used to create a\n``Tag`` ``QuerySet``:\n\n   * A string, which may contain multiple tag names.\n   * A list or tuple of strings corresponding to tag names.\n   * A list or tuple of integers corresponding to tag ids.\n\nIf given, the following will be returned as-is:\n\n   * A list or tuple of ``Tag`` objects.\n   * A ``Tag`` ``QuerySet``.\n\n\"\"\"\n", "func_signal": "def get_tag_list(tags):\n", "code": "from tagging.models import Tag\nif isinstance(tags, Tag):\n    return [tags]\nelif isinstance(tags, QuerySet) and tags.model is Tag:\n    return tags\nelif isinstance(tags, types.StringTypes):\n    return Tag.objects.filter(name__in=parse_tag_input(tags))\nelif isinstance(tags, (types.ListType, types.TupleType)):\n    if len(tags) == 0:\n        return tags\n    contents = set()\n    for item in tags:\n        if isinstance(item, types.StringTypes):\n            contents.add('string')\n        elif isinstance(item, Tag):\n            contents.add('tag')\n        elif isinstance(item, (types.IntType, types.LongType)):\n            contents.add('int')\n    if len(contents) == 1:\n        if 'string' in contents:\n            return Tag.objects.filter(name__in=[force_unicode(tag) \\\n                                                for tag in tags])\n        elif 'tag' in contents:\n            return tags\n        elif 'int' in contents:\n            return Tag.objects.filter(id__in=tags)\n    else:\n        raise ValueError(_('If a list or tuple of tags is provided, they must all be tag names, Tag objects or Tag ids.'))\nelse:\n    raise ValueError(_('The tag input given was invalid.'))", "path": "apps\\tagging\\utils.py", "repo_name": "rpribadi/flakeware", "stars": 1, "license": "None", "language": "python", "size": 828}
{"docstring": "\"\"\"Returns a copy of this object.\"\"\"\n# This way of initializing the copy means it works for subclasses, too.\n", "func_signal": "def copy(self):\n", "code": "obj = self.__class__(self)\nobj.keyOrder = self.keyOrder[:]\nreturn obj", "path": "apps\\compress\\filters\\csstidy_python\\tools.py", "repo_name": "rpribadi/flakeware", "stars": 1, "license": "None", "language": "python", "size": 828}
{"docstring": "\"\"\"return the next character from stdin. Watch out for lookahead. If\n   the character is a control character, translate it to a space or\n   linefeed.\n\"\"\"\n", "func_signal": "def _get(self):\n", "code": "c = self.theLookahead\nself.theLookahead = None\nif c == None:\n    c = self.instream.read(1)\nif c >= ' ' or c == '\\n':\n    return c\nif c == '': # EOF\n    return '\\000'\nif c == '\\r':\n    return '\\n'\nreturn ' '", "path": "apps\\compress\\filters\\jsmin\\jsmin.py", "repo_name": "rpribadi/flakeware", "stars": 1, "license": "None", "language": "python", "size": 828}
{"docstring": "\"\"\"\nUtility function for accepting single tag input in a flexible\nmanner.\n\nIf a ``Tag`` object is given it will be returned as-is; if a\nstring or integer are given, they will be used to lookup the\nappropriate ``Tag``.\n\nIf no matching tag can be found, ``None`` will be returned.\n\"\"\"\n", "func_signal": "def get_tag(tag):\n", "code": "from tagging.models import Tag\nif isinstance(tag, Tag):\n    return tag\n\ntry:\n    if isinstance(tag, types.StringTypes):\n        return Tag.objects.get(name=tag)\n    elif isinstance(tag, (types.IntType, types.LongType)):\n        return Tag.objects.get(id=tag)\nexcept Tag.DoesNotExist:\n    pass\n\nreturn None", "path": "apps\\tagging\\utils.py", "repo_name": "rpribadi/flakeware", "stars": 1, "license": "None", "language": "python", "size": 828}
{"docstring": "\"\"\"\nGiven a ``QuerySet`` or a ``Model``, returns a two-tuple of\n(queryset, model).\n\nIf a ``Model`` is given, the ``QuerySet`` returned will be created\nusing its default manager.\n\"\"\"\n", "func_signal": "def get_queryset_and_model(queryset_or_model):\n", "code": "try:\n    return queryset_or_model, queryset_or_model.model\nexcept AttributeError:\n    return queryset_or_model._default_manager.all(), queryset_or_model", "path": "apps\\tagging\\utils.py", "repo_name": "rpribadi/flakeware", "stars": 1, "license": "None", "language": "python", "size": 828}
{"docstring": "\"\"\"Inserts the key, value pair before the item with the given index.\"\"\"\n", "func_signal": "def insert(self, index, key, value):\n", "code": "if key in self.keyOrder:\n    n = self.keyOrder.index(key)\n    del self.keyOrder[n]\n    if n < index:\n        index -= 1\nself.keyOrder.insert(index, key)\nsuper(SortedDict, self).__setitem__(key, value)", "path": "apps\\compress\\filters\\csstidy_python\\tools.py", "repo_name": "rpribadi/flakeware", "stars": 1, "license": "None", "language": "python", "size": 828}
{"docstring": "\"\"\"\nAdd a ``font_size`` attribute to each tag according to the\nfrequency of its use, as indicated by its ``count``\nattribute.\n\n``steps`` defines the range of font sizes - ``font_size`` will\nbe an integer between 1 and ``steps`` (inclusive).\n\n``distribution`` defines the type of font size distribution\nalgorithm which will be used - logarithmic or linear. It must be\none of ``tagging.utils.LOGARITHMIC`` or ``tagging.utils.LINEAR``.\n\"\"\"\n", "func_signal": "def calculate_cloud(tags, steps=4, distribution=LOGARITHMIC):\n", "code": "if len(tags) > 0:\n    counts = [tag.count for tag in tags]\n    min_weight = float(min(counts))\n    max_weight = float(max(counts))\n    thresholds = _calculate_thresholds(min_weight, max_weight, steps)\n    for tag in tags:\n        font_set = False\n        tag_weight = _calculate_tag_weight(tag.count, max_weight, distribution)\n        for i in range(steps):\n            if not font_set and tag_weight <= thresholds[i]:\n                tag.font_size = i + 1\n                font_set = True\nreturn tags", "path": "apps\\tagging\\utils.py", "repo_name": "rpribadi/flakeware", "stars": 1, "license": "None", "language": "python", "size": 828}
{"docstring": "\"\"\"return true if the character is a letter, digit, underscore,\n       dollar sign, or non-ASCII character.\n\"\"\"\n", "func_signal": "def isAlphanum(c):\n", "code": "return ((c >= 'a' and c <= 'z') or (c >= '0' and c <= '9') or\n        (c >= 'A' and c <= 'Z') or c == '_' or c == '$' or c == '\\\\' or (c is not None and ord(c) > 126));", "path": "apps\\compress\\filters\\jsmin\\jsmin.py", "repo_name": "rpribadi/flakeware", "stars": 1, "license": "None", "language": "python", "size": 828}
{"docstring": "\"\"\"\nGiven list of ``Tag`` instances, creates a string representation of\nthe list suitable for editing by the user, such that submitting the\ngiven string representation back without changing it will give the\nsame list of tags.\n\nTag names which contain commas will be double quoted.\n\nIf any tag name which isn't being quoted contains whitespace, the\nresulting string of tag names will be comma-delimited, otherwise\nit will be space-delimited.\n\"\"\"\n", "func_signal": "def edit_string_for_tags(tags):\n", "code": "names = []\nuse_commas = False\nfor tag in tags:\n    name = tag.name\n    if u',' in name:\n        names.append('\"%s\"' % name)\n        continue\n    elif u' ' in name:\n        if not use_commas:\n            use_commas = True\n    names.append(name)\nif use_commas:\n    glue = u', '\nelse:\n    glue = u' '\nreturn glue.join(names)", "path": "apps\\tagging\\utils.py", "repo_name": "rpribadi/flakeware", "stars": 1, "license": "None", "language": "python", "size": 828}
{"docstring": "\"\"\"\nLogarithmic tag weight calculation is based on code from the\n`Tag Cloud`_ plugin for Mephisto, by Sven Fuchs.\n\n.. _`Tag Cloud`: http://www.artweb-design.de/projects/mephisto-plugin-tag-cloud\n\"\"\"\n", "func_signal": "def _calculate_tag_weight(weight, max_weight, distribution):\n", "code": "if distribution == LINEAR or max_weight == 1:\n    return weight\nelif distribution == LOGARITHMIC:\n    return math.log(weight) * max_weight / math.log(max_weight)\nraise ValueError(_('Invalid distribution algorithm specified: %s.') % distribution)", "path": "apps\\tagging\\utils.py", "repo_name": "rpribadi/flakeware", "stars": 1, "license": "None", "language": "python", "size": 828}
{"docstring": "\"\"\"Copy the input to the output, deleting the characters which are\n   insignificant to JavaScript. Comments will be removed. Tabs will be\n   replaced with spaces. Carriage returns will be replaced with linefeeds.\n   Most spaces and linefeeds will be removed.\n\"\"\"\n", "func_signal": "def _jsmin(self):\n", "code": "self.theA = '\\n'\nself._action(3)\n\nwhile self.theA != '\\000':\n    if self.theA == ' ':\n        if isAlphanum(self.theB):\n            self._action(1)\n        else:\n            self._action(2)\n    elif self.theA == '\\n':\n        if self.theB in ['{', '[', '(', '+', '-']:\n            self._action(1)\n        elif self.theB == ' ':\n            self._action(3)\n        else:\n            if isAlphanum(self.theB):\n                self._action(1)\n            else:\n                self._action(2)\n    else:\n        if self.theB == ' ':\n            if isAlphanum(self.theA):\n                self._action(1)\n            else:\n                self._action(3)\n        elif self.theB == '\\n':\n            if self.theA in ['}', ']', ')', '+', '-', '\"', '\\'']:\n                self._action(1)\n            else:\n                if isAlphanum(self.theA):\n                    self._action(1)\n                else:\n                    self._action(3)\n        else:\n            self._action(1)", "path": "apps\\compress\\filters\\jsmin\\jsmin.py", "repo_name": "rpribadi/flakeware", "stars": 1, "license": "None", "language": "python", "size": 828}
{"docstring": "\"\"\"do something! What you do is determined by the argument:\n   1   Output A. Copy B to A. Get the next B.\n   2   Copy B to A. Get the next B. (Delete A).\n   3   Get the next B. (Delete B).\n   action treats a string as a single character. Wow!\n   action recognizes a regular expression if it is preceded by ( or , or =.\n\"\"\"\n", "func_signal": "def _action(self, action):\n", "code": "if action <= 1:\n    self._outA()\n\nif action <= 2:\n    self.theA = self.theB\n    if self.theA == \"'\" or self.theA == '\"':\n        while 1:\n            self._outA()\n            self.theA = self._get()\n            if self.theA == self.theB:\n                break\n            if self.theA <= '\\n':\n                raise UnterminatedStringLiteral()\n            if self.theA == '\\\\':\n                self._outA()\n                self.theA = self._get()\n\n\nif action <= 3:\n    self.theB = self._next()\n    if self.theB == '/' and (self.theA == '(' or self.theA == ',' or\n                             self.theA == '=' or self.theA == ':' or\n                             self.theA == '[' or self.theA == '?' or\n                             self.theA == '!' or self.theA == '&' or\n                             self.theA == '|' or self.theA == ';' or\n                             self.theA == '{' or self.theA == '}' or\n                             self.theA == '\\n'):\n        self._outA()\n        self._outB()\n        while 1:\n            self.theA = self._get()\n            if self.theA == '/':\n                break\n            elif self.theA == '\\\\':\n                self._outA()\n                self.theA = self._get()\n            elif self.theA <= '\\n':\n                raise UnterminatedRegularExpression()\n            self._outA()\n        self.theB = self._next()", "path": "apps\\compress\\filters\\jsmin\\jsmin.py", "repo_name": "rpribadi/flakeware", "stars": 1, "license": "None", "language": "python", "size": 828}
{"docstring": "\"\"\"\nTag getter. Returns an instance's tags if accessed on an instance, and\nall of a model's tags if called on a class. That is, this model::\n\n   class Link(models.Model):\n       ...\n       tags = TagField()\n\nLets you do both of these::\n\n   >>> l = Link.objects.get(...)\n   >>> l.tags\n   'tag1 tag2 tag3'\n\n   >>> Link.tags\n   'tag1 tag2 tag3 tag4'\n\n\"\"\"\n# Handle access on the model (i.e. Link.tags)\n", "func_signal": "def __get__(self, instance, owner=None):\n", "code": "if instance is None:\n    return edit_string_for_tags(Tag.objects.usage_for_model(owner))\n\ntags = self._get_instance_tag_cache(instance)\nif tags is None:\n    if instance.pk is None:\n        self._set_instance_tag_cache(instance, '')\n    else:\n        self._set_instance_tag_cache(\n            instance, edit_string_for_tags(Tag.objects.get_for_object(instance)))\nreturn self._get_instance_tag_cache(instance)", "path": "apps\\tagging\\fields.py", "repo_name": "rpribadi/flakeware", "stars": 1, "license": "None", "language": "python", "size": 828}
{"docstring": "\"\"\"\nReturns a HttpResponse that implements the TinyMCE spellchecker protocol.\n\"\"\"\n", "func_signal": "def spell_check(request):\n", "code": "try:\n    import enchant\n\n    raw = request.raw_post_data\n    input = simplejson.loads(raw)\n    id = input['id']\n    method = input['method']\n    params = input['params']\n    lang = params[0]\n    arg = params[1]\n\n    if not enchant.dict_exists(str(lang)):\n        raise RuntimeError(\"dictionary not found for language '%s'\" % lang)\n    checker = enchant.Dict(lang)\n\n    if method == 'checkWords':\n        result = [word for word in arg if not checker.check(word)]\n    elif method == 'getSuggestions':\n        result = checker.suggest(arg)\n    else:\n        raise RuntimeError(\"Unkown spellcheck method: '%s'\" % method)\n    output = {\n        'id': id,\n        'result': result,\n        'error': None,\n    }\nexcept Exception:\n    logging.exception(\"Error running spellchecker\")\n    return HttpResponse(_(\"Error running spellchecker\"))\nreturn HttpResponse(simplejson.dumps(output),\n        content_type='application/json')", "path": "apps\\tinymce\\views.py", "repo_name": "rpribadi/flakeware", "stars": 1, "license": "None", "language": "python", "size": 828}
{"docstring": "\"\"\"\nSplits ``input`` on ``delimiter``, stripping each resulting string\nand returning a list of non-empty strings.\n\"\"\"\n", "func_signal": "def split_strip(input, delimiter=u','):\n", "code": "if not input:\n    return []\n\nwords = [w.strip() for w in input.split(delimiter)]\nreturn [w for w in words if w]", "path": "apps\\tagging\\utils.py", "repo_name": "rpribadi/flakeware", "stars": 1, "license": "None", "language": "python", "size": 828}
{"docstring": "\"\"\"\nParses tag input, with multiple word input being activated and\ndelineated by commas and double quotes. Quotes take precedence, so\nthey may contain commas.\n\nReturns a sorted list of unique tag names.\n\"\"\"\n", "func_signal": "def parse_tag_input(input):\n", "code": "if not input:\n    return []\n\ninput = force_unicode(input)\n\n# Special case - if there are no commas or double quotes in the\n# input, we don't *do* a recall... I mean, we know we only need to\n# split on spaces.\nif u',' not in input and u'\"' not in input:\n    words = list(set(split_strip(input, u' ')))\n    words.sort()\n    return words\n\nwords = []\nbuffer = []\n# Defer splitting of non-quoted sections until we know if there are\n# any unquoted commas.\nto_be_split = []\nsaw_loose_comma = False\nopen_quote = False\ni = iter(input)\ntry:\n    while 1:\n        c = i.next()\n        if c == u'\"':\n            if buffer:\n                to_be_split.append(u''.join(buffer))\n                buffer = []\n            # Find the matching quote\n            open_quote = True\n            c = i.next()\n            while c != u'\"':\n                buffer.append(c)\n                c = i.next()\n            if buffer:\n                word = u''.join(buffer).strip()\n                if word:\n                    words.append(word)\n                buffer = []\n            open_quote = False\n        else:\n            if not saw_loose_comma and c == u',':\n                saw_loose_comma = True\n            buffer.append(c)\nexcept StopIteration:\n    # If we were parsing an open quote which was never closed treat\n    # the buffer as unquoted.\n    if buffer:\n        if open_quote and u',' in buffer:\n            saw_loose_comma = True\n        to_be_split.append(u''.join(buffer))\nif to_be_split:\n    if saw_loose_comma:\n        delimiter = u','\n    else:\n        delimiter = u' '\n    for chunk in to_be_split:\n        words.extend(split_strip(chunk, delimiter))\nwords = list(set(words))\nwords.sort()\nreturn words", "path": "apps\\tagging\\utils.py", "repo_name": "rpribadi/flakeware", "stars": 1, "license": "None", "language": "python", "size": 828}
{"docstring": "# begin wxGlade: GroupFrame.__do_layout\n", "func_signal": "def __do_layout(self):\n", "code": "sizer_17 = wx.BoxSizer(wx.VERTICAL)\nsizer_23 = wx.BoxSizer(wx.VERTICAL)\nsizer_24 = wx.BoxSizer(wx.HORIZONTAL)\nsizer_25_copy = wx.BoxSizer(wx.VERTICAL)\nsizer_25 = wx.BoxSizer(wx.VERTICAL)\nsizer_23.Add((20, 20), 0, 0, 0)\nsizer_24.Add((20, 20), 0, 0, 0)\nsizer_25.Add(self.label_10, 0, 0, 0)\nsizer_25.Add(self.GroupByCLB, 0, 0, 0)\nsizer_24.Add(sizer_25, 0, wx.EXPAND, 0)\nsizer_24.Add((20, 20), 0, 0, 0)\nsizer_25_copy.Add(self.label_11, 0, 0, 0)\nsizer_25_copy.Add(self.ResultLC, 1, wx.EXPAND, 0)\nsizer_24.Add(sizer_25_copy, 1, wx.EXPAND, 0)\nsizer_24.Add((20, 20), 0, 0, 0)\nsizer_23.Add(sizer_24, 1, wx.EXPAND, 0)\nsizer_23.Add((20, 20), 0, 0, 0)\nself.panel_5.SetSizer(sizer_23)\nsizer_17.Add(self.panel_5, 1, wx.EXPAND, 0)\nself.SetSizer(sizer_17)\nsizer_17.Fit(self)\nself.Layout()\n# end wxGlade", "path": "gui_app.py", "repo_name": "jmurray1022/mptuner", "stars": 1, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\"Returns the pth quantile of a (a numpy array). Interpolation is \ntype 7 described in the R documentation, the default for R's quantile()\n\"\"\"\n", "func_signal": "def quantile(a, p):\n", "code": "x = a[where(~isnan(a))]\nif not x.any():\n    return np.nan\nelse:\n    n = len(x)\n    x.sort()\n    k = p * (n - 1) + 1\n    i = int(k)   #truncates k toward zero, gives us an index\n    g = k - i    #used to weight the 2 closest order statistics\n    if i < n:\n        result = (1 - g) * x[i - 1] + g * x[i]\n    else:\n        result = x[i - 1]\nreturn result", "path": "support.py", "repo_name": "jmurray1022/mptuner", "stars": 1, "license": "None", "language": "python", "size": 112}
{"docstring": "'''\nRead the data in self.fname. Self.colnames contains column names, \nself.data has the data as floats.\n'''\n\n", "func_signal": "def read_data(self):\n", "code": "def missing_data(x):\n    '''\n    Substitute NumPy's nan for missing data\n    '''\n    if x == 'NaN' or x=='NA':\n        return np.nan\n    else:\n        return float(x)\n\nfile = open(self.fname)\nline = file.readline()\nif line[-1] == '\\n':\n    line = line.rstrip()\nif line[0] == '\"':\n    line = line.replace('\"','')\ncn = line.rsplit(\",\")\nself.indname  =  cn[0]\nself.colnames = np.array(cn[1:len(cn)])\nnc = len(self.colnames)\nself.colsettings = np.vstack([name.rsplit(\".\")] for name in self.colnames)\n\nconv = dict((k, missing_data) for k in range(nc+1))\ndf = np.loadtxt(self.fname, delimiter=',', converters=conv, skiprows=1)\nself.ind = df[:,0]\nself.data = df[:,1:len(df[0,])]\ndel df\n#self.data=ma.masked_invalid(df[:,1:len(df[0,])])\n\nself.factors = map(str,range(len(self.colsettings[0])))\nself.settings = dict((self.factors[i], \\\n    sorted(list(set(self.colsettings[:,i])), ncmp)) \\\n    for i in range(len(self.factors)))", "path": "datadef.py", "repo_name": "jmurray1022/mptuner", "stars": 1, "license": "None", "language": "python", "size": 112}
{"docstring": "# begin wxGlade: GroupFrame.__init__\n", "func_signal": "def __init__(self, *args, **kwds):\n", "code": "kwds[\"style\"] = wx.DEFAULT_FRAME_STYLE\nwx.Frame.__init__(self, *args, **kwds)\nself.panel_5 = wx.Panel(self, -1)\nself.label_10 = wx.StaticText(self.panel_5, -1, \"Group By:\")\nself.GroupByCLB = wx.CheckListBox(self.panel_5, -1, choices=[])\nself.label_11 = wx.StaticText(self.panel_5, -1, \"Result\")\nself.ResultLC = wx.ListCtrl(self.panel_5, -1, style=wx.LC_REPORT|wx.SUNKEN_BORDER)\n\nself.__set_properties()\nself.__do_layout()\n# end wxGlade\nself.Bind(wx.EVT_CHECKLISTBOX, self.OnCBCheck, self.GroupByCLB)\nself.Bind(wx.EVT_CLOSE, self.onClose)\nself.PopulateFactors()\nself.SetCols()\nfor i in data.groupon:\n    self.GroupByCLB.Check(i)", "path": "gui_app.py", "repo_name": "jmurray1022/mptuner", "stars": 1, "license": "None", "language": "python", "size": 112}
{"docstring": "# begin wxGlade: GroupFrame.__set_properties\n", "func_signal": "def __set_properties(self):\n", "code": "self.SetTitle(\"Factor groups\")\nself.GroupByCLB.SetMinSize((130, 200))\nself.ResultLC.SetMinSize((365, 200))\n# end wxGlade", "path": "gui_app.py", "repo_name": "jmurray1022/mptuner", "stars": 1, "license": "None", "language": "python", "size": 112}
{"docstring": "# begin wxGlade: PlotOptions.__do_layout\n", "func_signal": "def __do_layout(self):\n", "code": "sizer_6 = wx.BoxSizer(wx.HORIZONTAL)\nsizer_6_copy_1 = wx.BoxSizer(wx.VERTICAL)\nsizer_7 = wx.BoxSizer(wx.HORIZONTAL)\nsizer_8 = wx.BoxSizer(wx.VERTICAL)\nsizer_18 = wx.BoxSizer(wx.HORIZONTAL)\nsizer_11 = wx.BoxSizer(wx.HORIZONTAL)\nsizer_10 = wx.BoxSizer(wx.HORIZONTAL)\nsizer_9 = wx.BoxSizer(wx.HORIZONTAL)\nsizer_6_copy_1.Add((10, 10), 0, 0, 0)\nsizer_7.Add((10, 10), 0, 0, 0)\nsizer_9.Add(self.label_6, 0, 0, 0)\nsizer_9.Add(self.TitleTB, 0, 0, 0)\nsizer_8.Add(sizer_9, 1, wx.EXPAND, 0)\nsizer_10.Add(self.label_7, 0, 0, 0)\nsizer_10.Add(self.XAxis, 0, 0, 0)\nsizer_8.Add(sizer_10, 1, wx.EXPAND, 0)\nsizer_11.Add(self.label_7_copy_1, 0, 0, 0)\nsizer_11.Add(self.YAxis, 0, 0, 0)\nsizer_8.Add(sizer_11, 1, wx.EXPAND, 0)\nsizer_8.Add((10, 10), 0, 0, 0)\nsizer_18.Add((10, 10), 1, 0, 0)\nsizer_18.Add(self.Apply, 0, 0, 0)\nsizer_18.Add((10, 10), 0, wx.ALIGN_CENTER_HORIZONTAL, 0)\nsizer_18.Add(self.Clear, 0, 0, 0)\nsizer_18.Add((10, 10), 1, 0, 0)\nsizer_8.Add(sizer_18, 1, wx.EXPAND, 0)\nsizer_7.Add(sizer_8, 1, wx.EXPAND, 0)\nsizer_7.Add((10, 10), 0, 0, 0)\nsizer_6_copy_1.Add(sizer_7, 1, wx.EXPAND, 0)\nsizer_6_copy_1.Add((10, 10), 0, 0, 0)\nself.panel_7.SetSizer(sizer_6_copy_1)\nsizer_6.Add(self.panel_7, 1, wx.EXPAND, 0)\nself.SetSizer(sizer_6)\nsizer_6.Fit(self)\nself.Layout()\n# end wxGlade", "path": "gui_app.py", "repo_name": "jmurray1022/mptuner", "stars": 1, "license": "None", "language": "python", "size": 112}
{"docstring": "# begin wxGlade: PlotOptions.__init__\n", "func_signal": "def __init__(self, *args, **kwds):\n", "code": "kwds[\"style\"] = wx.DEFAULT_FRAME_STYLE\nwx.Frame.__init__(self, *args, **kwds)\nself.panel_7 = wx.Panel(self, -1)\nself.label_6 = wx.StaticText(self.panel_7, -1, \"Title:\")\nself.TitleTB = wx.TextCtrl(self.panel_7, -1, \"\")\nself.label_7 = wx.StaticText(self.panel_7, -1, \"X axis label:\")\nself.XAxis = wx.TextCtrl(self.panel_7, -1, \"\")\nself.label_7_copy_1 = wx.StaticText(self.panel_7, -1, \"Y axis label:\")\nself.YAxis = wx.TextCtrl(self.panel_7, -1, \"\")\nself.Apply = wx.Button(self.panel_7, -1, \"Apply\")\nself.Clear = wx.Button(self.panel_7, -1, \"Clear\")\n\nself.__set_properties()\nself.__do_layout()\n\nself.Bind(wx.EVT_BUTTON, self.OnApply, self.Apply)\nself.Bind(wx.EVT_BUTTON, self.OnClear, self.Clear)\n# end wxGlade", "path": "gui_app.py", "repo_name": "jmurray1022/mptuner", "stars": 1, "license": "None", "language": "python", "size": 112}
{"docstring": "'''\nReturns the length of thelongest contiguous ordered \n(default is increasing) sequence.\nSequences don't have to be strictly monotonic.\n'''\n", "func_signal": "def longest_contig(x, inc = True):\n", "code": "longest = 1\ntmp = 1\nswitch = 0\nif inc == False:\n    x = -1 * x\nfor i in range(x.size):\n    if i > 0:\n        if x[i] >= x[i-1]:\n            tmp = tmp + 1\n        else:\n            switch = 1\n            if tmp > longest:\n                longest = tmp\n            tmp = 1\nif switch == 0:\n    longest = tmp\nreturn longest", "path": "support.py", "repo_name": "jmurray1022/mptuner", "stars": 1, "license": "None", "language": "python", "size": 112}
{"docstring": "'''Returns the interquartile range of a numpy array x.\n'''\n", "func_signal": "def iqr(x):\n", "code": "result = quantile(x, 0.75) - quantile(x, 0.25)\nreturn result", "path": "support.py", "repo_name": "jmurray1022/mptuner", "stars": 1, "license": "None", "language": "python", "size": 112}
{"docstring": "'''\nReturns the length of the longest monotonic sequence in x\n'''\n", "func_signal": "def longest_contig_max(x):\n", "code": "inc = longest_contig(x)\ndec = longest_contig(x, inc = False)\nreturn max(inc, dec)", "path": "support.py", "repo_name": "jmurray1022/mptuner", "stars": 1, "license": "None", "language": "python", "size": 112}
{"docstring": "'''\nReturns the Cartesian product of the elements in lists\n'''\n", "func_signal": "def cprod(lists, previous_elements = []):\n", "code": "if len(lists) == 1:\n    for elem in lists[0]:\n        yield previous_elements + [elem, ]\nelse:\n    for elem in lists[0]:\n        for x in cprod(lists[1:], previous_elements + [elem, ]):\n            yield x", "path": "support.py", "repo_name": "jmurray1022/mptuner", "stars": 1, "license": "None", "language": "python", "size": 112}
{"docstring": "# begin wxGlade: PlotOptions.__set_properties\n", "func_signal": "def __set_properties(self):\n", "code": "self.SetTitle(\"Plot Options\")\nself.label_6.SetMinSize((100, 13))\nself.TitleTB.SetMinSize((300, 21))\nself.label_7.SetMinSize((100, 13))\nself.XAxis.SetMinSize((300, 21))\nself.label_7_copy_1.SetMinSize((100, 13))\nself.YAxis.SetMinSize((300, 21))\n# end wxGlade", "path": "gui_app.py", "repo_name": "jmurray1022/mptuner", "stars": 1, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\"Handles The wx.EVT_SIZE Events For The StatusBar.\n\nActually, All The Calculations Linked To HorizontalAlignment And\nVerticalAlignment Are Done In This Function.\"\"\"\n\n", "func_signal": "def OnSize(self, event):\n", "code": "for pos, item in self._items.items():\n    widget, horizontalalignment, verticalalignment = item.widget, item.horizontalalignment, item.verticalalignment\n   \n    rect = self.GetFieldRect(pos)\n    widgetpos = widget.GetPosition()\n    widgetsize = widget.GetSize()\n\n    rect = self.GetFieldRect(pos)\n    \n    if horizontalalignment == ESB_EXACT_FIT:\n        \n        if verticalalignment == ESB_EXACT_FIT:\n            widget.SetSize((rect.width-2, rect.height-2))\n            widget.SetPosition((rect.x-1, rect.y-1))\n        elif verticalalignment == ESB_ALIGN_CENTER_VERTICAL:\n            if widgetsize[1] < rect.width - 1:\n                diffs = (rect.height - widgetsize[1])/2\n                widget.SetSize((rect.width-2, widgetsize[1]))\n                widget.SetPosition((rect.x-1, rect.y+diffs))\n            else:\n                widget.SetSize((rect.width-2, widgetsize[1]))\n                widget.SetPosition((rect.x-1, rect.y-1))\n        elif verticalalignment == ESB_ALIGN_TOP:\n            widget.SetSize((rect.width-2, widgetsize[1]))\n            widget.SetPosition((rect.x-1, rect.y))\n        elif verticalalignment == ESB_ALIGN_BOTTOM:\n            widget.SetSize((rect.width-2, widgetsize[1]))\n            widget.SetPosition((rect.x-1, rect.height-widgetsize[1]))\n\n    elif horizontalalignment == ESB_ALIGN_LEFT:\n        \n        xpos = rect.x - 1\n        if verticalalignment == ESB_EXACT_FIT:\n            widget.SetSize((widgetsize[0], rect.height-2))\n            widget.SetPosition((xpos, rect.y-1))\n        elif verticalalignment == ESB_ALIGN_CENTER_VERTICAL:\n            if widgetsize[1] < rect.height - 1:\n                diffs = (rect.height - widgetsize[1])/2\n                widget.SetPosition((xpos, rect.y+diffs))\n            else:\n                widget.SetSize((widgetsize[0], rect.height-2))\n                widget.SetPosition((xpos, rect.y-1))\n        elif verticalalignment == ESB_ALIGN_TOP:\n            widget.SetPosition((xpos, rect.y))\n        elif verticalalignment == ESB_ALIGN_BOTTOM:\n            widget.SetPosition((xpos, rect.height-widgetsize[1]))\n        \n    elif horizontalalignment == ESB_ALIGN_RIGHT:\n        \n        xpos = rect.x + rect.width - widgetsize[0] - 1\n        if verticalalignment == ESB_EXACT_FIT:\n            widget.SetSize((widgetsize[0], rect.height-2))\n            widget.SetPosition((xpos, rect.y-1))\n        elif verticalalignment == ESB_ALIGN_CENTER_VERTICAL:\n            if widgetsize[1] < rect.height - 1:\n                diffs = (rect.height - widgetsize[1])/2\n                widget.SetPosition((xpos, rect.y+diffs))\n            else:\n                widget.SetSize((widgetsize[0], rect.height-2))\n                widget.SetPosition((xpos, rect.y-1))\n        elif verticalalignment == ESB_ALIGN_TOP:\n            widget.SetPosition((xpos, rect.y))\n        elif verticalalignment == ESB_ALIGN_BOTTOM:\n            widget.SetPosition((xpos, rect.height-widgetsize[1]))\n\n    elif horizontalalignment == ESB_ALIGN_CENTER_HORIZONTAL:\n        \n        xpos = rect.x + (rect.width - widgetsize[0])/2 - 1\n        if verticalalignment == ESB_EXACT_FIT:\n            widget.SetSize((widgetsize[0], rect.height))\n            widget.SetPosition((xpos, rect.y))\n        elif verticalalignment == ESB_ALIGN_CENTER_VERTICAL:\n            if widgetsize[1] < rect.height - 1:\n                diffs = (rect.height - widgetsize[1])/2\n                widget.SetPosition((xpos, rect.y+diffs))\n            else:\n                widget.SetSize((widgetsize[0], rect.height-1))\n                widget.SetPosition((xpos, rect.y+1))\n        elif verticalalignment == ESB_ALIGN_TOP:\n            widget.SetPosition((xpos, rect.y))\n        elif verticalalignment == ESB_ALIGN_BOTTOM:\n            widget.SetPosition((xpos, rect.height-widgetsize[1]))\n\n        \nif event is not None:\n    event.Skip()", "path": "EnhancedStatusBar.py", "repo_name": "jmurray1022/mptuner", "stars": 1, "license": "None", "language": "python", "size": 112}
{"docstring": "'''Returns the absolute value of the SLR slope'''\n", "func_signal": "def slope(y, x):\n", "code": "reg = vstack((ones(x.shape), x))\n(beta, res, rank, singular) = linalg.lstsq(reg.transpose(), y)\nreturn beta[1]", "path": "support.py", "repo_name": "jmurray1022/mptuner", "stars": 1, "license": "None", "language": "python", "size": 112}
{"docstring": "# begin wxGlade: DataImportFrame.__set_properties\n", "func_signal": "def __set_properties(self):\n", "code": "self.SetTitle(\"Data Import Settings\")\nself.factor_lb.SetMinSize((130, 200))\nself.setting_lb.SetMinSize((130, 200))\nself.name_tc.SetMinSize((130, 20))\n# end wxGlade", "path": "gui_app.py", "repo_name": "jmurray1022/mptuner", "stars": 1, "license": "None", "language": "python", "size": 112}
{"docstring": "# begin wxGlade: MainFrame.__do_layout\n", "func_signal": "def __do_layout(self):\n", "code": "MainSizer = wx.BoxSizer(wx.VERTICAL)\nsizer_3 = wx.BoxSizer(wx.HORIZONTAL)\nsizer_5 = wx.BoxSizer(wx.VERTICAL)\nsizer_2 = wx.BoxSizer(wx.VERTICAL)\nsizer_21 = wx.BoxSizer(wx.HORIZONTAL)\nsizer_20 = wx.BoxSizer(wx.HORIZONTAL)\nsizer_5_copy = wx.BoxSizer(wx.VERTICAL)\nsizer_6_copy = wx.BoxSizer(wx.HORIZONTAL)\nsizer_7_copy = wx.BoxSizer(wx.HORIZONTAL)\nsizer_8_copy = wx.BoxSizer(wx.VERTICAL)\nsizer_10_copy = wx.BoxSizer(wx.HORIZONTAL)\nsizer_9_copy = wx.BoxSizer(wx.HORIZONTAL)\nsizer_22_copy = wx.BoxSizer(wx.HORIZONTAL)\nsizer_19 = wx.BoxSizer(wx.HORIZONTAL)\nsizer_22 = wx.BoxSizer(wx.HORIZONTAL)\nsizer_13 = wx.BoxSizer(wx.HORIZONTAL)\nsizer_12_copy_copy = wx.BoxSizer(wx.VERTICAL)\nsizer_29 = wx.BoxSizer(wx.HORIZONTAL)\nself.panel_4.SetSizer(sizer_29)\nsizer_13.Add(self.panel_4, 0, wx.EXPAND, 0)\nsizer_12_copy_copy.Add(self.canvas, 1, wx.EXPAND, 0)\nsizer_12_copy_copy.Add(self.toolbar, 0, wx.EXPAND, 0)\nself.panel_3.SetSizer(sizer_12_copy_copy)\nsizer_13.Add(self.panel_3, 1, wx.EXPAND, 0)\nMainSizer.Add(sizer_13, 1, wx.EXPAND, 0)\nsizer_22.Add((20, 10), 0, 0, 0)\nself.panel_12.SetSizer(sizer_22)\nMainSizer.Add(self.panel_12, 0, wx.EXPAND, 0)\nsizer_19.Add(self.panel_13, 1, wx.EXPAND, 0)\nsizer_19.Add(self.label_4, 0, 0, 0)\nsizer_19.Add(self.choice_1, 0, 0, 0)\nsizer_19.Add(self.panel_13_copy, 1, wx.EXPAND, 0)\nself.panel_2.SetSizer(sizer_19)\nMainSizer.Add(self.panel_2, 0, wx.EXPAND, 0)\nsizer_22_copy.Add((20, 10), 0, 0, 0)\nself.panel_12_copy.SetSizer(sizer_22_copy)\nMainSizer.Add(self.panel_12_copy, 0, wx.EXPAND, 0)\nsizer_3.Add(self.panel_9, 1, wx.EXPAND, 0)\nsizer_3.Add((10, 20), 0, 0, 0)\nsizer_9_copy.Add(self.label_7_copy, 0, 0, 0)\nsizer_9_copy.Add(self.FrameCtrl, 0, 0, 0)\nsizer_8_copy.Add(sizer_9_copy, 1, wx.EXPAND, 0)\nsizer_8_copy.Add(sizer_10_copy, 1, wx.EXPAND, 0)\nsizer_3.Add(sizer_8_copy, 0, wx.EXPAND, 0)\nsizer_3.Add((10, 20), 0, 0, 0)\nsizer_7_copy.Add(self.label_6_copy, 0, 0, 0)\nsizer_7_copy.Add(self.csSlider, 0, 0, 0)\nsizer_7_copy.Add(self.csText, 0, 0, 0)\nsizer_5_copy.Add(sizer_7_copy, 1, wx.EXPAND, 0)\nsizer_6_copy.Add(self.label_5_copy, 0, 0, 0)\nsizer_6_copy.Add(self.ceSlider, 0, 0, 0)\nsizer_6_copy.Add(self.ceText, 0, 0, 0)\nsizer_5_copy.Add(sizer_6_copy, 1, wx.EXPAND, 0)\nsizer_3.Add(sizer_5_copy, 0, wx.EXPAND, 0)\nsizer_3.Add((10, 20), 0, 0, 0)\nsizer_20.Add(self.label_5, 0, 0, 0)\nsizer_20.Add(self.incText, 0, 0, 0)\nsizer_2.Add(sizer_20, 1, wx.EXPAND, 0)\nsizer_21.Add(self.label_8, 0, 0, 0)\nsizer_21.Add(self.fitText, 0, 0, 0)\nsizer_2.Add(sizer_21, 1, wx.EXPAND, 0)\nsizer_3.Add(sizer_2, 0, wx.EXPAND, 0)\nsizer_3.Add((10, 20), 0, 0, 0)\nsizer_5.Add(self.Recalc, 0, wx.ALIGN_BOTTOM, 0)\nsizer_3.Add(sizer_5, 0, wx.EXPAND, 0)\nsizer_3.Add(self.panel_10, 1, wx.EXPAND, 0)\nself.panel_6.SetSizer(sizer_3)\nMainSizer.Add(self.panel_6, 0, wx.EXPAND, 0)\nself.SetSizer(MainSizer)\nMainSizer.Fit(self)\nself.Layout()\n# end wxGlade", "path": "gui_app.py", "repo_name": "jmurray1022/mptuner", "stars": 1, "license": "None", "language": "python", "size": 112}
{"docstring": "# begin wxGlade: DataImportFrame.__do_layout\n", "func_signal": "def __do_layout(self):\n", "code": "sizer_4 = wx.BoxSizer(wx.VERTICAL)\nsizer_16 = wx.BoxSizer(wx.VERTICAL)\nsizer_12 = wx.BoxSizer(wx.HORIZONTAL)\nsizer_15 = wx.BoxSizer(wx.VERTICAL)\nsizer_14_copy = wx.BoxSizer(wx.VERTICAL)\nsizer_14 = wx.BoxSizer(wx.VERTICAL)\nsizer_12.Add((20, 20), 0, 0, 0)\nsizer_14.Add(self.label_1, 0, 0, 0)\nsizer_14.Add(self.factor_lb, 0, 0, 0)\nsizer_12.Add(sizer_14, 1, wx.EXPAND, 0)\nsizer_12.Add((20, 20), 0, 0, 0)\nsizer_14_copy.Add(self.label_2, 0, 0, 0)\nsizer_14_copy.Add(self.setting_lb, 0, 0, 0)\nsizer_12.Add(sizer_14_copy, 1, wx.EXPAND, 0)\nsizer_12.Add((20, 20), 0, 0, 0)\nsizer_15.Add(self.label_3, 0, 0, 0)\nsizer_15.Add(self.name_tc, 0, 0, 0)\nsizer_15.Add((20, 20), 0, 0, 0)\nsizer_15.Add(self.button_2, 0, 0, 0)\nsizer_15.Add((20, 10), 0, 0, 0)\nsizer_15.Add(self.Save, 0, 0, 0)\nsizer_12.Add(sizer_15, 1, wx.EXPAND, 0)\nsizer_12.Add((20, 20), 0, 0, 0)\nsizer_16.Add(sizer_12, 1, wx.EXPAND, 0)\nsizer_16.Add((20, 20), 0, 0, 0)\nself.panel_1.SetSizer(sizer_16)\nsizer_4.Add(self.panel_1, 1, wx.EXPAND, 0)\nself.SetSizer(sizer_4)\nsizer_4.Fit(self)\nself.Layout()\n# end wxGlade", "path": "gui_app.py", "repo_name": "jmurray1022/mptuner", "stars": 1, "license": "None", "language": "python", "size": 112}
{"docstring": "# begin wxGlade: MainFrame.__set_properties\n", "func_signal": "def __set_properties(self):\n", "code": "self.SetTitle(\"MPTuner 0.90\")\nself.canvas.SetBackgroundColour(wx.Colour(255, 255, 255))\nself.choice_1.SetMinSize((500, 27))\nself.label_7_copy.SetMinSize((60, 17))\nself.FrameCtrl.SetMinSize((55, 27))\nself.label_6_copy.SetMinSize((76, 17))\nself.csSlider.SetMinSize((140, -1))\nself.csSlider.SetSize((200,-1))\nself.csText.SetMinSize((50, 27))\nself.csText.SetFractionWidth(3)\nself.csText.SetIntegerWidth(2)\nself.csText.SetValue(1.5)\nself.label_5_copy.SetMinSize((76, 17))\nself.ceSlider.SetMinSize((140, -1))\nself.ceText.SetFractionWidth(3)\nself.ceText.SetIntegerWidth(2)\nself.ceText.SetValue(3.0)\nself.label_5.SetMinSize((60, 17))\nself.label_8.SetMinSize((60, 17))\nself.fitText.SetFractionWidth(3)\nself.fitText.SetIntegerWidth(2)\nself.fitText.SetValue(3.0)\n# end wxGlade\nself.fitText.SetValue(0.0)\nself.incText.SetValue(0)\nself.csText.SetValue(0.0)\nself.ceText.SetValue(0.0)\nself.csSlider.SetValue(0)\nself.ceSlider.SetValue(0)\n    \nself.SetTitle(\"MPTuner \" + ver)", "path": "gui_app.py", "repo_name": "jmurray1022/mptuner", "stars": 1, "license": "None", "language": "python", "size": 112}
{"docstring": "'''Returns the covariance of x and y'''\n", "func_signal": "def covar(x, y):\n", "code": "cmat = cov(x, y)\nreturn cmat[0, 1]", "path": "support.py", "repo_name": "jmurray1022/mptuner", "stars": 1, "license": "None", "language": "python", "size": 112}
{"docstring": "# begin wxGlade: DataImportFrame.__init__\n", "func_signal": "def __init__(self, *args, **kwds):\n", "code": "kwds[\"style\"] = wx.DEFAULT_FRAME_STYLE\nwx.Frame.__init__(self, *args, **kwds)\nself.panel_1 = wx.Panel(self, -1)\nself.label_1 = wx.StaticText(self.panel_1, -1, \"Factors\")\nself.factor_lb = wx.ListBox(self.panel_1, -1, choices=[])\nself.label_2 = wx.StaticText(self.panel_1, -1, \"Settings\")\nself.setting_lb = wx.ListBox(self.panel_1, -1, choices=[])\nself.label_3 = wx.StaticText(self.panel_1, -1, \"Name:\")\nself.name_tc = wx.TextCtrl(self.panel_1, -1, \"\")\nself.button_2 = wx.Button(self.panel_1, -1, \"Load factor definitions file\")\nself.Save = wx.Button(self.panel_1, -1, \"Save factor definitions file\")\n\nself.__set_properties()\nself.__do_layout()\n\nself.Bind(wx.EVT_LISTBOX, self.OnFactorSelect, self.factor_lb)\nself.Bind(wx.EVT_TEXT, self.OnNameEdit, self.name_tc)\nself.Bind(wx.EVT_BUTTON, self.OnLoadFactorDefs, self.button_2)\nself.Bind(wx.EVT_BUTTON, self.OnSaveFactorDefs, self.Save)\n# end wxGlade\nif len(data.factors) > 0:\n    self.factor_lb.Clear()\n    self.factor_lb.AppendItems(data.factors)\nelse:\n    factors = range(len(data.colsettings[0,:]))\n    for factor in reversed(factors):\n        #factors[i] = str(factor)\n        self.factor_lb.Insert(str(factor), 0)", "path": "gui_app.py", "repo_name": "jmurray1022/mptuner", "stars": 1, "license": "None", "language": "python", "size": 112}
{"docstring": "# called for each start tag\n# attrs is a list of (attr, value) tuples\n# e.g. for <pre class='screen'>, tag='pre', attrs=[('class', 'screen')]\n", "func_signal": "def unknown_starttag(self, tag, attrs):\n", "code": "if _debug: sys.stderr.write('_BaseHTMLProcessor, unknown_starttag, tag=%s\\n' % tag)\nuattrs = []\n# thanks to Kevin Marks for this breathtaking hack to deal with (valid) high-bit attribute values in UTF-8 feeds\nfor key, value in attrs:\n    if type(value) != type(u''):\n        value = unicode(value, self.encoding)\n    uattrs.append((unicode(key, self.encoding), value))\nstrattrs = u''.join([u' %s=\"%s\"' % (key, value) for key, value in uattrs]).encode(self.encoding)\nif tag in self.elements_no_end_tag:\n    self.pieces.append('<%(tag)s%(strattrs)s />' % locals())\nelse:\n    self.pieces.append('<%(tag)s%(strattrs)s>' % locals())", "path": "feedparser.py", "repo_name": "ahmerkureishi/superfeedr2blog", "stars": 0, "license": "None", "language": "python", "size": 132}
{"docstring": "'''Parse a string according to the Nate 8-bit date format'''\n", "func_signal": "def _parse_date_nate(dateString):\n", "code": "m = _korean_nate_date_re.match(dateString)\nif not m: return\nhour = int(m.group(5))\nampm = m.group(4)\nif (ampm == _korean_pm):\n    hour += 12\nhour = str(hour)\nif len(hour) == 1:\n    hour = '0' + hour\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': hour, 'minute': m.group(6), 'second': m.group(7),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('Nate date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "feedparser.py", "repo_name": "ahmerkureishi/superfeedr2blog", "stars": 0, "license": "None", "language": "python", "size": 132}
{"docstring": "# called for each character reference, e.g. for '&#160;', ref will be '160'\n", "func_signal": "def handle_charref(self, ref):\n", "code": "if not self.elementstack: return\nref = ref.lower()\nif ref in ('34', '38', '39', '60', '62', 'x22', 'x26', 'x27', 'x3c', 'x3e'):\n    text = '&#%s;' % ref\nelse:\n    if ref[0] == 'x':\n        c = int(ref[1:], 16)\n    else:\n        c = int(ref)\n    text = unichr(c).encode('utf-8')\nself.elementstack[-1][2].append(text)", "path": "feedparser.py", "repo_name": "ahmerkureishi/superfeedr2blog", "stars": 0, "license": "None", "language": "python", "size": 132}
{"docstring": "'''Get the character encoding of the XML document\n\nhttp_headers is a dictionary\nxml_data is a raw string (not Unicode)\n\nThis is so much trickier than it sounds, it's not even funny.\nAccording to RFC 3023 ('XML Media Types'), if the HTTP Content-Type\nis application/xml, application/*+xml,\napplication/xml-external-parsed-entity, or application/xml-dtd,\nthe encoding given in the charset parameter of the HTTP Content-Type\ntakes precedence over the encoding given in the XML prefix within the\ndocument, and defaults to 'utf-8' if neither are specified.  But, if\nthe HTTP Content-Type is text/xml, text/*+xml, or\ntext/xml-external-parsed-entity, the encoding given in the XML prefix\nwithin the document is ALWAYS IGNORED and only the encoding given in\nthe charset parameter of the HTTP Content-Type header should be\nrespected, and it defaults to 'us-ascii' if not specified.\n\nFurthermore, discussion on the atom-syntax mailing list with the\nauthor of RFC 3023 leads me to the conclusion that any document\nserved with a Content-Type of text/* and no charset parameter\nmust be treated as us-ascii.  (We now do this.)  And also that it\nmust always be flagged as non-well-formed.  (We now do this too.)\n\nIf Content-Type is unspecified (input was local file or non-HTTP source)\nor unrecognized (server just got it totally wrong), then go by the\nencoding given in the XML prefix of the document and default to\n'iso-8859-1' as per the HTTP specification (RFC 2616).\n\nThen, assuming we didn't find a character encoding in the HTTP headers\n(and the HTTP Content-type allowed us to look in the body), we need\nto sniff the first few bytes of the XML data and try to determine\nwhether the encoding is ASCII-compatible.  Section F of the XML\nspecification shows the way here:\nhttp://www.w3.org/TR/REC-xml/#sec-guessing-no-ext-info\n\nIf the sniffed encoding is not ASCII-compatible, we need to make it\nASCII compatible so that we can sniff further into the XML declaration\nto find the encoding attribute, which will tell us the true encoding.\n\nOf course, none of this guarantees that we will be able to parse the\nfeed in the declared character encoding (assuming it was declared\ncorrectly, which many are not).  CJKCodecs and iconv_codec help a lot;\nyou should definitely install them if you can.\nhttp://cjkpython.i18n.org/\n'''\n\n", "func_signal": "def _getCharacterEncoding(http_headers, xml_data):\n", "code": "def _parseHTTPContentType(content_type):\n    '''takes HTTP Content-Type header and returns (content type, charset)\n\n    If no charset is specified, returns (content type, '')\n    If no content type is specified, returns ('', '')\n    Both return parameters are guaranteed to be lowercase strings\n    '''\n    content_type = content_type or ''\n    content_type, params = cgi.parse_header(content_type)\n    return content_type, params.get('charset', '').replace(\"'\", '')\n\nsniffed_xml_encoding = ''\nxml_encoding = ''\ntrue_encoding = ''\nhttp_content_type, http_encoding = _parseHTTPContentType(http_headers.get('content-type'))\n# Must sniff for non-ASCII-compatible character encodings before\n# searching for XML declaration.  This heuristic is defined in\n# section F of the XML specification:\n# http://www.w3.org/TR/REC-xml/#sec-guessing-no-ext-info\ntry:\n    if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n        # EBCDIC\n        xml_data = _ebcdic_to_ascii(xml_data)\n    elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n        # UTF-16BE\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16BE with BOM\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n        # UTF-16LE\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16LE with BOM\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n        # UTF-32BE\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n        # UTF-32LE\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n        # UTF-32BE with BOM\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n        # UTF-32LE with BOM\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n    elif xml_data[:3] == '\\xef\\xbb\\xbf':\n        # UTF-8 with BOM\n        sniffed_xml_encoding = 'utf-8'\n        xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n    else:\n        # ASCII-compatible\n        pass\n    xml_encoding_match = re.compile('^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>').match(xml_data)\nexcept:\n    xml_encoding_match = None\nif xml_encoding_match:\n    xml_encoding = xml_encoding_match.groups()[0].lower()\n    if sniffed_xml_encoding and (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode', 'iso-10646-ucs-4', 'ucs-4', 'csucs4', 'utf-16', 'utf-32', 'utf_16', 'utf_32', 'utf16', 'u16')):\n        xml_encoding = sniffed_xml_encoding\nacceptable_content_type = 0\napplication_content_types = ('application/xml', 'application/xml-dtd', 'application/xml-external-parsed-entity')\ntext_content_types = ('text/xml', 'text/xml-external-parsed-entity')\nif (http_content_type in application_content_types) or \\\n   (http_content_type.startswith('application/') and http_content_type.endswith('+xml')):\n    acceptable_content_type = 1\n    true_encoding = http_encoding or xml_encoding or 'utf-8'\nelif (http_content_type in text_content_types) or \\\n     (http_content_type.startswith('text/')) and http_content_type.endswith('+xml'):\n    acceptable_content_type = 1\n    true_encoding = http_encoding or 'us-ascii'\nelif http_content_type.startswith('text/'):\n    true_encoding = http_encoding or 'us-ascii'\nelif http_headers and (not http_headers.has_key('content-type')):\n    true_encoding = xml_encoding or 'iso-8859-1'\nelse:\n    true_encoding = xml_encoding or 'utf-8'\nreturn true_encoding, http_encoding, xml_encoding, sniffed_xml_encoding, acceptable_content_type", "path": "feedparser.py", "repo_name": "ahmerkureishi/superfeedr2blog", "stars": 0, "license": "None", "language": "python", "size": 132}
{"docstring": "'''Parse a string according to a Greek 8-bit date format.'''\n", "func_signal": "def _parse_date_greek(dateString):\n", "code": "m = _greek_date_format_re.match(dateString)\nif not m: return\ntry:\n    wday = _greek_wdays[m.group(1)]\n    month = _greek_months[m.group(3)]\nexcept:\n    return\nrfc822date = '%(wday)s, %(day)s %(month)s %(year)s %(hour)s:%(minute)s:%(second)s %(zonediff)s' % \\\n             {'wday': wday, 'day': m.group(2), 'month': month, 'year': m.group(4),\\\n              'hour': m.group(5), 'minute': m.group(6), 'second': m.group(7),\\\n              'zonediff': m.group(8)}\nif _debug: sys.stderr.write('Greek date parsed as: %s\\n' % rfc822date)\nreturn _parse_date_rfc822(rfc822date)", "path": "feedparser.py", "repo_name": "ahmerkureishi/superfeedr2blog", "stars": 0, "license": "None", "language": "python", "size": 132}
{"docstring": "# called for each block of plain text, i.e. outside of any tag and\n# not containing any character or entity references\n", "func_signal": "def handle_data(self, text, escape=1):\n", "code": "if not self.elementstack: return\nif escape and self.contentparams.get('type') == 'application/xhtml+xml':\n    text = _xmlescape(text)\nself.elementstack[-1][2].append(text)", "path": "feedparser.py", "repo_name": "ahmerkureishi/superfeedr2blog", "stars": 0, "license": "None", "language": "python", "size": 132}
{"docstring": "'''Changes an XML data stream on the fly to specify a new encoding\n\ndata is a raw sequence of bytes (not Unicode) that is presumed to be in %encoding already\nencoding is a string recognized by encodings.aliases\n'''\n", "func_signal": "def _toUTF8(data, encoding):\n", "code": "if _debug: sys.stderr.write('entering _toUTF8, trying encoding %s\\n' % encoding)\n# strip Byte Order Mark (if present)\nif (len(data) >= 4) and (data[:2] == '\\xfe\\xff') and (data[2:4] != '\\x00\\x00'):\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-16be':\n            sys.stderr.write('trying utf-16be instead\\n')\n    encoding = 'utf-16be'\n    data = data[2:]\nelif (len(data) >= 4) and (data[:2] == '\\xff\\xfe') and (data[2:4] != '\\x00\\x00'):\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-16le':\n            sys.stderr.write('trying utf-16le instead\\n')\n    encoding = 'utf-16le'\n    data = data[2:]\nelif data[:3] == '\\xef\\xbb\\xbf':\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-8':\n            sys.stderr.write('trying utf-8 instead\\n')\n    encoding = 'utf-8'\n    data = data[3:]\nelif data[:4] == '\\x00\\x00\\xfe\\xff':\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-32be':\n            sys.stderr.write('trying utf-32be instead\\n')\n    encoding = 'utf-32be'\n    data = data[4:]\nelif data[:4] == '\\xff\\xfe\\x00\\x00':\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-32le':\n            sys.stderr.write('trying utf-32le instead\\n')\n    encoding = 'utf-32le'\n    data = data[4:]\nnewdata = unicode(data, encoding)\nif _debug: sys.stderr.write('successfully converted %s data to unicode\\n' % encoding)\ndeclmatch = re.compile('^<\\?xml[^>]*?>')\nnewdecl = '''<?xml version='1.0' encoding='utf-8'?>'''\nif declmatch.search(newdata):\n    newdata = declmatch.sub(newdecl, newdata)\nelse:\n    newdata = newdecl + u'\\n' + newdata\nreturn newdata.encode('utf-8')", "path": "feedparser.py", "repo_name": "ahmerkureishi/superfeedr2blog", "stars": 0, "license": "None", "language": "python", "size": 132}
{"docstring": "'''Parse a string according to a Hungarian 8-bit date format.'''\n", "func_signal": "def _parse_date_hungarian(dateString):\n", "code": "m = _hungarian_date_format_re.match(dateString)\nif not m: return\ntry:\n    month = _hungarian_months[m.group(2)]\n    day = m.group(3)\n    if len(day) == 1:\n        day = '0' + day\n    hour = m.group(4)\n    if len(hour) == 1:\n        hour = '0' + hour\nexcept:\n    return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': month, 'day': day,\\\n             'hour': hour, 'minute': m.group(5),\\\n             'zonediff': m.group(6)}\nif _debug: sys.stderr.write('Hungarian date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "feedparser.py", "repo_name": "ahmerkureishi/superfeedr2blog", "stars": 0, "license": "None", "language": "python", "size": 132}
{"docstring": "\"\"\"URL, filename, or string --> stream\n\nThis function lets you define parsers that take any input source\n(URL, pathname to local or network file, or actual data as a string)\nand deal with it in a uniform manner.  Returned object is guaranteed\nto have all the basic stdio read methods (read, readline, readlines).\nJust .close() the object when you're done with it.\n\nIf the etag argument is supplied, it will be used as the value of an\nIf-None-Match request header.\n\nIf the modified argument is supplied, it must be a tuple of 9 integers\nas returned by gmtime() in the standard Python time module. This MUST\nbe in GMT (Greenwich Mean Time). The formatted date/time will be used\nas the value of an If-Modified-Since request header.\n\nIf the agent argument is supplied, it will be used as the value of a\nUser-Agent request header.\n\nIf the referrer argument is supplied, it will be used as the value of a\nReferer[sic] request header.\n\nIf handlers is supplied, it is a list of handlers used to build a\nurllib2 opener.\n\"\"\"\n\n", "func_signal": "def _open_resource(url_file_stream_or_string, etag, modified, agent, referrer, handlers):\n", "code": "if hasattr(url_file_stream_or_string, 'read'):\n    return url_file_stream_or_string\n\nif url_file_stream_or_string == '-':\n    return sys.stdin\n\nif urlparse.urlparse(url_file_stream_or_string)[0] in ('http', 'https', 'ftp'):\n    if not agent:\n        agent = USER_AGENT\n    # test for inline user:password for basic auth\n    auth = None\n    if base64:\n        urltype, rest = urllib.splittype(url_file_stream_or_string)\n        realhost, rest = urllib.splithost(rest)\n        if realhost:\n            user_passwd, realhost = urllib.splituser(realhost)\n            if user_passwd:\n                url_file_stream_or_string = '%s://%s%s' % (urltype, realhost, rest)\n                auth = base64.encodestring(user_passwd).strip()\n    # try to open with urllib2 (to use optional headers)\n    request = urllib2.Request(url_file_stream_or_string)\n    request.add_header('User-Agent', agent)\n    if etag:\n        request.add_header('If-None-Match', etag)\n    if modified:\n        # format into an RFC 1123-compliant timestamp. We can't use\n        # time.strftime() since the %a and %b directives can be affected\n        # by the current locale, but RFC 2616 states that dates must be\n        # in English.\n        short_weekdays = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n        request.add_header('If-Modified-Since', '%s, %02d %s %04d %02d:%02d:%02d GMT' % (short_weekdays[modified[6]], modified[2], months[modified[1] - 1], modified[0], modified[3], modified[4], modified[5]))\n    if referrer:\n        request.add_header('Referer', referrer)\n    if gzip and zlib:\n        request.add_header('Accept-encoding', 'gzip, deflate')\n    elif gzip:\n        request.add_header('Accept-encoding', 'gzip')\n    elif zlib:\n        request.add_header('Accept-encoding', 'deflate')\n    else:\n        request.add_header('Accept-encoding', '')\n    if auth:\n        request.add_header('Authorization', 'Basic %s' % auth)\n    if ACCEPT_HEADER:\n        request.add_header('Accept', ACCEPT_HEADER)\n    request.add_header('A-IM', 'feed') # RFC 3229 support\n    opener = apply(urllib2.build_opener, tuple([_FeedURLHandler()] + handlers))\n    opener.addheaders = [] # RMK - must clear so we only send our custom User-Agent\n    try:\n        return opener.open(request)\n    finally:\n        opener.close() # JohnD\n\n# try to open with native open function (if url_file_stream_or_string is a filename)\ntry:\n    return open(url_file_stream_or_string)\nexcept:\n    pass\n\n# treat url_file_stream_or_string as string\nreturn _StringIO(str(url_file_stream_or_string))", "path": "feedparser.py", "repo_name": "ahmerkureishi/superfeedr2blog", "stars": 0, "license": "None", "language": "python", "size": 132}
{"docstring": "# override internal declaration handler to handle CDATA blocks\n", "func_signal": "def parse_declaration(self, i):\n", "code": "if _debug: sys.stderr.write('entering parse_declaration\\n')\nif self.rawdata[i:i+9] == '<![CDATA[':\n    k = self.rawdata.find(']]>', i)\n    if k == -1: k = len(self.rawdata)\n    self.handle_data(_xmlescape(self.rawdata[i+9:k]), 0)\n    return k+3\nelse:\n    k = self.rawdata.find('>', i)\n    return k+1", "path": "feedparser.py", "repo_name": "ahmerkureishi/superfeedr2blog", "stars": 0, "license": "None", "language": "python", "size": 132}
{"docstring": "'''Parse a string according to the OnBlog 8-bit date format'''\n", "func_signal": "def _parse_date_onblog(dateString):\n", "code": "m = _korean_onblog_date_re.match(dateString)\nif not m: return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': m.group(4), 'minute': m.group(5), 'second': m.group(6),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('OnBlog date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "feedparser.py", "repo_name": "ahmerkureishi/superfeedr2blog", "stars": 0, "license": "None", "language": "python", "size": 132}
{"docstring": "# called for each block of plain text, i.e. outside of any tag and\n# not containing any character or entity references\n# Store the original text verbatim.\n", "func_signal": "def handle_data(self, text):\n", "code": "if _debug: sys.stderr.write('_BaseHTMLProcessor, handle_text, text=%s\\n' % text)\nself.pieces.append(text)", "path": "feedparser.py", "repo_name": "ahmerkureishi/superfeedr2blog", "stars": 0, "license": "None", "language": "python", "size": 132}
{"docstring": "'''Parse a string according to the MS SQL date format'''\n", "func_signal": "def _parse_date_mssql(dateString):\n", "code": "m = _mssql_date_re.match(dateString)\nif not m: return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': m.group(4), 'minute': m.group(5), 'second': m.group(6),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('MS SQL date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "feedparser.py", "repo_name": "ahmerkureishi/superfeedr2blog", "stars": 0, "license": "None", "language": "python", "size": 132}
{"docstring": "# called for each end tag, e.g. for </pre>, tag will be 'pre'\n# Reconstruct the original end tag.\n", "func_signal": "def unknown_endtag(self, tag):\n", "code": "if tag not in self.elements_no_end_tag:\n    self.pieces.append(\"</%(tag)s>\" % locals())", "path": "feedparser.py", "repo_name": "ahmerkureishi/superfeedr2blog", "stars": 0, "license": "None", "language": "python", "size": 132}
{"docstring": "'''Parse a variety of ISO-8601-compatible formats like 20040105'''\n", "func_signal": "def _parse_date_iso8601(dateString):\n", "code": "m = None\nfor _iso8601_match in _iso8601_matches:\n    m = _iso8601_match(dateString)\n    if m: break\nif not m: return\nif m.span() == (0, 0): return\nparams = m.groupdict()\nordinal = params.get('ordinal', 0)\nif ordinal:\n    ordinal = int(ordinal)\nelse:\n    ordinal = 0\nyear = params.get('year', '--')\nif not year or year == '--':\n    year = time.gmtime()[0]\nelif len(year) == 2:\n    # ISO 8601 assumes current century, i.e. 93 -> 2093, NOT 1993\n    year = 100 * int(time.gmtime()[0] / 100) + int(year)\nelse:\n    year = int(year)\nmonth = params.get('month', '-')\nif not month or month == '-':\n    # ordinals are NOT normalized by mktime, we simulate them\n    # by setting month=1, day=ordinal\n    if ordinal:\n        month = 1\n    else:\n        month = time.gmtime()[1]\nmonth = int(month)\nday = params.get('day', 0)\nif not day:\n    # see above\n    if ordinal:\n        day = ordinal\n    elif params.get('century', 0) or \\\n             params.get('year', 0) or params.get('month', 0):\n        day = 1\n    else:\n        day = time.gmtime()[2]\nelse:\n    day = int(day)\n# special case of the century - is the first year of the 21st century\n# 2000 or 2001 ? The debate goes on...\nif 'century' in params.keys():\n    year = (int(params['century']) - 1) * 100 + 1\n# in ISO 8601 most fields are optional\nfor field in ['hour', 'minute', 'second', 'tzhour', 'tzmin']:\n    if not params.get(field, None):\n        params[field] = 0\nhour = int(params.get('hour', 0))\nminute = int(params.get('minute', 0))\nsecond = int(params.get('second', 0))\n# weekday is normalized by mktime(), we can ignore it\nweekday = 0\n# daylight savings is complex, but not needed for feedparser's purposes\n# as time zones, if specified, include mention of whether it is active\n# (e.g. PST vs. PDT, CET). Using -1 is implementation-dependent and\n# and most implementations have DST bugs\ndaylight_savings_flag = 0\ntm = [year, month, day, hour, minute, second, weekday,\n      ordinal, daylight_savings_flag]\n# ISO 8601 time zone adjustments\ntz = params.get('tz')\nif tz and tz != 'Z':\n    if tz[0] == '-':\n        tm[3] += int(params.get('tzhour', 0))\n        tm[4] += int(params.get('tzmin', 0))\n    elif tz[0] == '+':\n        tm[3] -= int(params.get('tzhour', 0))\n        tm[4] -= int(params.get('tzmin', 0))\n    else:\n        return None\n# Python's time.mktime() is a wrapper around the ANSI C mktime(3c)\n# which is guaranteed to normalize d/m/y/h/m/s.\n# Many implementations have bugs, but we'll pretend they don't.\nreturn time.localtime(time.mktime(tm))", "path": "feedparser.py", "repo_name": "ahmerkureishi/superfeedr2blog", "stars": 0, "license": "None", "language": "python", "size": 132}
{"docstring": "'''Parse an RFC822, RFC1123, RFC2822, or asctime-style date'''\n", "func_signal": "def _parse_date_rfc822(dateString):\n", "code": "data = dateString.split()\nif data[0][-1] in (',', '.') or data[0].lower() in rfc822._daynames:\n    del data[0]\nif len(data) == 4:\n    s = data[3]\n    i = s.find('+')\n    if i > 0:\n        data[3:] = [s[:i], s[i+1:]]\n    else:\n        data.append('')\n    dateString = \" \".join(data)\nif len(data) < 5:\n    dateString += ' 00:00:00 GMT'\ntm = rfc822.parsedate_tz(dateString)\nif tm:\n    return time.gmtime(rfc822.mktime_tz(tm))", "path": "feedparser.py", "repo_name": "ahmerkureishi/superfeedr2blog", "stars": 0, "license": "None", "language": "python", "size": 132}
{"docstring": "# called for each entity reference, e.g. for '&copy;', ref will be 'copy'\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "if not self.elementstack: return\nif _debug: sys.stderr.write('entering handle_entityref with %s\\n' % ref)\nif ref in ('lt', 'gt', 'quot', 'amp', 'apos'):\n    text = '&%s;' % ref\nelse:\n    # entity resolution graciously donated by Aaron Swartz\n    def name2cp(k):\n        import htmlentitydefs\n        if hasattr(htmlentitydefs, 'name2codepoint'): # requires Python 2.3\n            return htmlentitydefs.name2codepoint[k]\n        k = htmlentitydefs.entitydefs[k]\n        if k.startswith('&#') and k.endswith(';'):\n            return int(k[2:-1]) # not in latin-1\n        return ord(k)\n    try: name2cp(ref)\n    except KeyError: text = '&%s;' % ref\n    else: text = unichr(name2cp(ref)).encode('utf-8')\nself.elementstack[-1][2].append(text)", "path": "feedparser.py", "repo_name": "ahmerkureishi/superfeedr2blog", "stars": 0, "license": "None", "language": "python", "size": 132}
{"docstring": "# Check if\n# - server requires digest auth, AND\n# - we tried (unsuccessfully) with basic auth, AND\n# - we're using Python 2.3.3 or later (digest auth is irreparably broken in earlier versions)\n# If all conditions hold, parse authentication information\n# out of the Authorization header we sent the first time\n# (for the username and password) and the WWW-Authenticate\n# header the server sent back (for the realm) and retry\n# the request with the appropriate digest auth headers instead.\n# This evil genius hack has been brought to you by Aaron Swartz.\n", "func_signal": "def http_error_401(self, req, fp, code, msg, headers):\n", "code": "host = urlparse.urlparse(req.get_full_url())[1]\ntry:\n    assert sys.version.split()[0] >= '2.3.3'\n    assert base64 != None\n    user, passw = base64.decodestring(req.headers['Authorization'].split(' ')[1]).split(':')\n    realm = re.findall('realm=\"([^\"]*)\"', headers['WWW-Authenticate'])[0]\n    self.add_password(realm, host, user, passw)\n    retry = self.http_error_auth_reqed('www-authenticate', host, req, headers)\n    self.reset_retry_count()\n    return retry\nexcept:\n    return self.http_error_default(req, fp, code, msg, headers)", "path": "feedparser.py", "repo_name": "ahmerkureishi/superfeedr2blog", "stars": 0, "license": "None", "language": "python", "size": 132}
{"docstring": "'''Return the Time Zone Designator as an offset in seconds from UTC.'''\n", "func_signal": "def __extract_tzd(m):\n", "code": "if not m:\n    return 0\ntzd = m.group('tzd')\nif not tzd:\n    return 0\nif tzd == 'Z':\n    return 0\nhours = int(m.group('tzdhours'))\nminutes = m.group('tzdminutes')\nif minutes:\n    minutes = int(minutes)\nelse:\n    minutes = 0\noffset = (hours*60 + minutes) * 60\nif tzd[0] == '+':\n    return -offset\nreturn offset", "path": "feedparser.py", "repo_name": "ahmerkureishi/superfeedr2blog", "stars": 0, "license": "None", "language": "python", "size": 132}
{"docstring": "# utility method to be called by descendants\n", "func_signal": "def normalize_attrs(self, attrs):\n", "code": "attrs = [(k.lower(), v) for k, v in attrs]\nattrs = [(k, k in ('rel', 'type') and v.lower() or v) for k, v in attrs]\nreturn attrs", "path": "feedparser.py", "repo_name": "ahmerkureishi/superfeedr2blog", "stars": 0, "license": "None", "language": "python", "size": 132}
{"docstring": "\"\"\"\nReturns the keys whose values in `dictionary` are `element`\nor, if none exists, [].\n\n    >>> d = {1:4, 3:4}\n    >>> dictfindall(d, 4)\n    [1, 3]\n    >>> dictfindall(d, 5)\n    []\n\"\"\"\n", "func_signal": "def dictfindall(dictionary, element):\n", "code": "res = []\nfor (key, value) in dictionary.iteritems():\n    if element is value:\n        res.append(key)\nreturn res", "path": "web\\utils.py", "repo_name": "antoine/metagenda", "stars": 1, "license": "None", "language": "python", "size": 628}
{"docstring": "\"\"\"Runs a WSGI function as an SCGI server.\"\"\"\n", "func_signal": "def runscgi(func, addr=('localhost', 4000)):\n", "code": "import flup.server.scgi as flups\nreturn flups.WSGIServer(func, bindAddress=addr).run()", "path": "web\\wsgi.py", "repo_name": "antoine/metagenda", "stars": 1, "license": "None", "language": "python", "size": 628}
{"docstring": "\"\"\"\nReturns context_lines before and after lineno from file.\nReturns (pre_context_lineno, pre_context, context_line, post_context).\n\"\"\"\n", "func_signal": "def _get_lines_from_file(filename, lineno, context_lines):\n", "code": "try:\n    source = open(filename).readlines()\n    lower_bound = max(0, lineno - context_lines)\n    upper_bound = lineno + context_lines\n\n    pre_context = \\\n        [line.strip('\\n') for line in source[lower_bound:lineno]]\n    context_line = source[lineno].strip('\\n')\n    post_context = \\\n        [line.strip('\\n') for line in source[lineno + 1:upper_bound]]\n\n    return lower_bound, pre_context, context_line, post_context\nexcept (OSError, IOError):\n    return None, [], None, []", "path": "web\\debugerror.py", "repo_name": "antoine/metagenda", "stars": 1, "license": "None", "language": "python", "size": 628}
{"docstring": "\"\"\"\nReturns a dictionary consisting of the keys in the argument dictionaries.\nIf they share a key, the value from the last argument is used.\n\n    >>> dictadd({1: 0, 2: 0}, {2: 1, 3: 1})\n    {1: 0, 2: 1, 3: 1}\n\"\"\"\n", "func_signal": "def dictadd(*dicts):\n", "code": "result = {}\nfor dct in dicts:\n    result.update(dct)\nreturn result", "path": "web\\utils.py", "repo_name": "antoine/metagenda", "stars": 1, "license": "None", "language": "python", "size": 628}
{"docstring": "\"\"\"\nLike re.sub, but returns the replacement _and_ the match object.\n\n    >>> t, m = re_subm('g(oo+)fball', r'f\\\\1lish', 'goooooofball')\n    >>> t\n    'foooooolish'\n    >>> m.groups()\n    ('oooooo',)\n\"\"\"\n", "func_signal": "def re_subm(pat, repl, string):\n", "code": "compiled_pat = re_compile(pat)\nproxy = _re_subm_proxy()\ncompiled_pat.sub(proxy.__call__, string)\nreturn compiled_pat.sub(repl, string), proxy.match", "path": "web\\utils.py", "repo_name": "antoine/metagenda", "stars": 1, "license": "None", "language": "python", "size": 628}
{"docstring": "\"\"\"\nReturns `lst[ind]` if it exists, `default` otherwise.\n\n    >>> listget(['a'], 0)\n    'a'\n    >>> listget(['a'], 1)\n    >>> listget(['a'], 1, 'b')\n    'b'\n\"\"\"\n", "func_signal": "def listget(lst, ind, default=None):\n", "code": "if len(lst)-1 < ind: \n    return default\nreturn lst[ind]", "path": "web\\utils.py", "repo_name": "antoine/metagenda", "stars": 1, "license": "None", "language": "python", "size": 628}
{"docstring": "#dis = editdist.distance(d1.upper(), d2.upper().encode('utf-8')) \n#editdistance can accept two str or two unicode but if mismatch it will convert using ascii which doesn't work\n#dis = editdist.distance(unicode_to_str(d1.upper()), unicode_to_str(d2.upper()) )\n", "func_signal": "def same(d1, d2, value_of_same=0.1):\n", "code": "dis = editdist.distance(unicode_to_str(remove_uneeded(d1.upper())), unicode_to_str(remove_uneeded(d2.upper())) )\nif len(d2)> len(d1):\n    longest_len = len(d2)\nelse:\n    longest_len =len(d1)\nlevensthein_to_len = (1.0 * dis)/ longest_len\nreturn levensthein_to_len < value_of_same", "path": "utils.py", "repo_name": "antoine/metagenda", "stars": 1, "license": "None", "language": "python", "size": 628}
{"docstring": "\"\"\"Guido van Rossum sez: don't use this function.\"\"\"\n", "func_signal": "def upvars(level=2):\n", "code": "return dictadd(\n  sys._getframe(level).f_globals,\n  sys._getframe(level).f_locals)", "path": "web\\cheetah.py", "repo_name": "antoine/metagenda", "stars": 1, "license": "None", "language": "python", "size": 628}
{"docstring": "\"\"\"\nAutomatically assigns local variables to `self`.\n\n    >>> self = storage()\n    >>> autoassign(self, dict(a=1, b=2))\n    >>> self\n    <Storage {'a': 1, 'b': 2}>\n\nGenerally used in `__init__` methods, as in:\n\n    def __init__(self, foo, bar, baz=1): autoassign(self, locals())\n\"\"\"\n", "func_signal": "def autoassign(self, locals):\n", "code": "for (key, value) in locals.iteritems():\n    if key == 'self': \n        continue\n    setattr(self, key, value)", "path": "web\\utils.py", "repo_name": "antoine/metagenda", "stars": 1, "license": "None", "language": "python", "size": 628}
{"docstring": "\"\"\"\nFormats `string` according to `pattern`, where the letter X gets replaced\nby characters from `string`.\n\n    >>> denumify(\"8005551212\", \"(XXX) XXX-XXXX\")\n    '(800) 555-1212'\n\n\"\"\"\n", "func_signal": "def denumify(string, pattern):\n", "code": "out = []\nfor c in pattern:\n    if c == \"X\":\n        out.append(string[0])\n        string = string[1:]\n    else:\n        out.append(c)\nreturn ''.join(out)", "path": "web\\utils.py", "repo_name": "antoine/metagenda", "stars": 1, "license": "None", "language": "python", "size": 628}
{"docstring": "\"\"\"returns var, literal, paren, dict, or list\"\"\"\n", "func_signal": "def atomr(self):\n", "code": "o = (\n  self.varq() or \n  self.parenq() or\n  self.dictq() or\n  self.listq() or\n  self.literalq()\n)\nif o is False: self.Error('atom')\nreturn o", "path": "web\\template.py", "repo_name": "antoine/metagenda", "stars": 1, "license": "None", "language": "python", "size": 628}
{"docstring": "\"\"\"\nIncrements `element` in `dictionary`, \nsetting it to one if it doesn't exist.\n\n    >>> d = {1:2, 3:4}\n    >>> dictincr(d, 1)\n    3\n    >>> d[1]\n    3\n    >>> dictincr(d, 5)\n    1\n    >>> d[5]\n    1\n\"\"\"\n", "func_signal": "def dictincr(dictionary, element):\n", "code": "dictionary.setdefault(element, 0)\ndictionary[element] += 1\nreturn dictionary[element]", "path": "web\\utils.py", "repo_name": "antoine/metagenda", "stars": 1, "license": "None", "language": "python", "size": 628}
{"docstring": "\"\"\"\nConverts a (UTC) datetime object to a nice string representation.\n\n    >>> from datetime import datetime, timedelta\n    >>> d = datetime(1970, 5, 1)\n    >>> datestr(d, now=d)\n    '0 microseconds ago'\n    >>> for t, v in {\n    ...   timedelta(microseconds=1): '1 microsecond ago',\n    ...   timedelta(microseconds=2): '2 microseconds ago',\n    ...   -timedelta(microseconds=1): '1 microsecond from now',\n    ...   -timedelta(microseconds=2): '2 microseconds from now',\n    ...   timedelta(microseconds=2000): '2 milliseconds ago',\n    ...   timedelta(seconds=2): '2 seconds ago',\n    ...   timedelta(seconds=2*60): '2 minutes ago',\n    ...   timedelta(seconds=2*60*60): '2 hours ago',\n    ...   timedelta(days=2): '2 days ago',\n    ... }.iteritems():\n    ...     assert datestr(d, now=d+t) == v\n    >>> datestr(datetime(1970, 1, 1), now=d)\n    'January  1'\n    >>> datestr(datetime(1969, 1, 1), now=d)\n    'January  1, 1969'\n    >>> datestr(datetime(1970, 6, 1), now=d)\n    'June  1, 1970'\n\"\"\"\n", "func_signal": "def datestr(then, now=None):\n", "code": "def agohence(n, what, divisor=None):\n    if divisor: n = n // divisor\n\n    out = str(abs(n)) + ' ' + what       # '2 day'\n    if abs(n) != 1: out += 's'           # '2 days'\n    out += ' '                           # '2 days '\n    if n < 0:\n        out += 'from now'\n    else:\n        out += 'ago'\n    return out                           # '2 days ago'\n\noneday = 24 * 60 * 60\n\nif not now: now = datetime.datetime.utcnow()\nif type(now).__name__ == \"DateTime\":\n    now = datetime.datetime.fromtimestamp(now)\nif type(then).__name__ == \"DateTime\":\n    then = datetime.datetime.fromtimestamp(then)\ndelta = now - then\ndeltaseconds = int(delta.days * oneday + delta.seconds + delta.microseconds * 1e-06)\ndeltadays = abs(deltaseconds) // oneday\nif deltaseconds < 0: deltadays *= -1 # fix for oddity of floor\n\nif deltadays:\n    if abs(deltadays) < 4:\n        return agohence(deltadays, 'day')\n\n    out = then.strftime('%B %e') # e.g. 'June 13'\n    if then.year != now.year or deltadays < 0:\n        out += ', %s' % then.year\n    return out\n\nif int(deltaseconds):\n    if abs(deltaseconds) > (60 * 60):\n        return agohence(deltaseconds, 'hour', 60 * 60)\n    elif abs(deltaseconds) > 60:\n        return agohence(deltaseconds, 'minute', 60)\n    else:\n        return agohence(deltaseconds, 'second')\n\ndeltamicroseconds = delta.microseconds\nif delta.days: deltamicroseconds = int(delta.microseconds - 1e6) # datetime oddity\nif abs(deltamicroseconds) > 1000:\n    return agohence(deltamicroseconds, 'millisecond', 1000)\n\nreturn agohence(deltamicroseconds, 'microsecond')", "path": "web\\utils.py", "repo_name": "antoine/metagenda", "stars": 1, "license": "None", "language": "python", "size": 628}
{"docstring": "\"\"\"Runs a WSGI function as a FastCGI server.\"\"\"\n", "func_signal": "def runfcgi(func, addr=('localhost', 8000)):\n", "code": "import flup.server.fcgi as flups\nreturn flups.WSGIServer(func, multiplexed=True, bindAddress=addr).run()", "path": "web\\wsgi.py", "repo_name": "antoine/metagenda", "stars": 1, "license": "None", "language": "python", "size": 628}
{"docstring": "\"\"\"\nReturns `integer` as an int or `default` if it can't.\n\n    >>> intget('3')\n    3\n    >>> intget('3a')\n    >>> intget('3a', 0)\n    0\n\"\"\"\n", "func_signal": "def intget(integer, default=None):\n", "code": "try:\n    return int(integer)\nexcept (TypeError, ValueError):\n    return default", "path": "web\\utils.py", "repo_name": "antoine/metagenda", "stars": 1, "license": "None", "language": "python", "size": 628}
{"docstring": "\"\"\"\nRuns a WSGI-compatible `func` using FCGI, SCGI, or a simple web server,\nas appropriate based on context and `sys.argv`.\n\"\"\"\n\n", "func_signal": "def runwsgi(func):\n", "code": "if os.environ.has_key('SERVER_SOFTWARE'): # cgi\n    os.environ['FCGI_FORCE_CGI'] = 'Y'\n\nif (os.environ.has_key('PHP_FCGI_CHILDREN') #lighttpd fastcgi\n  or os.environ.has_key('SERVER_SOFTWARE')):\n    return runfcgi(func, None)\n\nif 'fcgi' in sys.argv or 'fastcgi' in sys.argv:\n    args = sys.argv[1:]\n    if 'fastcgi' in args: args.remove('fastcgi')\n    elif 'fcgi' in args: args.remove('fcgi')\n    if args:\n        return runfcgi(func, validaddr(args[0]))\n    else:\n        return runfcgi(func, None)\n\nif 'scgi' in sys.argv:\n    args = sys.argv[1:]\n    args.remove('scgi')\n    if args:\n        return runscgi(func, validaddr(args[0]))\n    else:\n        return runscgi(func)\n\nreturn httpserver.runsimple(func, validip(listget(sys.argv, 1, '')))", "path": "web\\wsgi.py", "repo_name": "antoine/metagenda", "stars": 1, "license": "None", "language": "python", "size": 628}
{"docstring": "\"\"\"\nReturns a key whose value in `dictionary` is `element` \nor, if none exists, None.\n\n    >>> d = {1:2, 3:4}\n    >>> dictfind(d, 4)\n    3\n    >>> dictfind(d, 5)\n\"\"\"\n", "func_signal": "def dictfind(dictionary, element):\n", "code": "for (key, value) in dictionary.iteritems():\n    if element is value: \n        return key", "path": "web\\utils.py", "repo_name": "antoine/metagenda", "stars": 1, "license": "None", "language": "python", "size": 628}
{"docstring": "\"\"\"\nA replacement for `internalerror` that presents a nice page with lots\nof debug information for the programmer.\n\n(Based on the beautiful 500 page from [Django](http://djangoproject.com/), \ndesigned by [Wilson Miner](http://wilsonminer.com/).)\n\"\"\"\n\n", "func_signal": "def debugerror():\n", "code": "web.ctx.headers = [('Content-Type', 'text/html')]\nweb.ctx.output = djangoerror()", "path": "web\\debugerror.py", "repo_name": "antoine/metagenda", "stars": 1, "license": "None", "language": "python", "size": 628}
{"docstring": "\"\"\"\nConverts text to HTML following the rules of Markdown, but blocking any\noutside HTML input, so that only the things supported by Markdown\ncan be used. Also converts raw URLs to links.\n\n(requires [markdown.py](http://webpy.org/markdown.py))\n\"\"\"\n", "func_signal": "def safemarkdown(text):\n", "code": "from markdown import markdown\nif text:\n    text = text.replace('<', '&lt;')\n    # TODO: automatically get page title?\n    text = r_url.sub(r'<\\1>', text)\n    text = markdown(text)\n    return text", "path": "web\\utils.py", "repo_name": "antoine/metagenda", "stars": 1, "license": "None", "language": "python", "size": 628}
{"docstring": "\"\"\"\nCreates a `storage` object from dictionary `mapping`, raising `KeyError` if\nd doesn't have all of the keys in `requireds` and using the default \nvalues for keys found in `defaults`.\n\nFor example, `storify({'a':1, 'c':3}, b=2, c=0)` will return the equivalent of\n`storage({'a':1, 'b':2, 'c':3})`.\n\nIf a `storify` value is a list (e.g. multiple values in a form submission), \n`storify` returns the last element of the list, unless the key appears in \n`defaults` as a list. Thus:\n\n    >>> storify({'a':[1, 2]}).a\n    2\n    >>> storify({'a':[1, 2]}, a=[]).a\n    [1, 2]\n    >>> storify({'a':1}, a=[]).a\n    [1]\n    >>> storify({}, a=[]).a\n    []\n\nSimilarly, if the value has a `value` attribute, `storify will return _its_\nvalue, unless the key appears in `defaults` as a dictionary.\n\n    >>> storify({'a':storage(value=1)}).a\n    1\n    >>> storify({'a':storage(value=1)}, a={}).a\n    <Storage {'value': 1}>\n    >>> storify({}, a={}).a\n    {}\n\n\"\"\"\n", "func_signal": "def storify(mapping, *requireds, **defaults):\n", "code": "def getvalue(x):\n    if hasattr(x, 'value'):\n        return x.value\n    else:\n        return x\n\nstor = Storage()\nfor key in requireds + tuple(mapping.keys()):\n    value = mapping[key]\n    if isinstance(value, list):\n        if isinstance(defaults.get(key), list):\n            value = [getvalue(x) for x in value]\n        else:\n            value = value[-1]\n    if not isinstance(defaults.get(key), dict):\n        value = getvalue(value)\n    if isinstance(defaults.get(key), list) and not isinstance(value, list):\n        value = [value]\n    setattr(stor, key, value)\n\nfor (key, value) in defaults.iteritems():\n    result = value\n    if hasattr(stor, key): \n        result = stor[key]\n    if value == () and not isinstance(result, tuple): \n        result = (result,)\n    setattr(stor, key, result)\n\nreturn stor", "path": "web\\utils.py", "repo_name": "antoine/metagenda", "stars": 1, "license": "None", "language": "python", "size": 628}
{"docstring": "\"\"\"\nGiven a model class, will return the dict representing the Meta class.\n\"\"\"\n", "func_signal": "def get_model_meta(model):\n", "code": "tree = get_model_tree(model)\n\nresult = {}\n\n# First, try to get any unusual inherited classes\nfor base in model.__bases__:\n    if base is not models.Model:\n        if hasattr(base, '_meta') and not base._meta.abstract:\n            # Abstract models' fields are included anyway, and we don't\n            # want extra dependencies\n            if \"_bases\" not in result:\n                result['_bases'] = []\n            result['_bases'].append(base.__module__ + \".\" + base.__name__)\n\n# Find all classes exactly two levels deep\npossible_meta_classes = set(tree.find(\"classdef classdef\"))\npossible_meta_classes.difference(set(tree.find(\"classdef classdef classdef\")))\n\n# Select only those called 'Meta', and expand all their assignments\npossible_meta_classes = [\n    tree.find(\"^ > suite > stmt > simple_stmt > small_stmt > expr_stmt\")\n    for tree in possible_meta_classes\n    if tree.value[2][1] == \"Meta\"\n]\n\nif possible_meta_classes:\n    # Now, for each possible definition, try it. (Only for last Meta,\n    # since that's how python interprets it)\n    for defn in possible_meta_classes[-1]:\n        bits = defn.flatten()\n        if len(bits) > 1 and bits[1] == token.EQUAL:\n            result[bits[0][1]] = reform(bits[2:])\n\nreturn result or None", "path": "south\\modelsparser.py", "repo_name": "mukulu/django-south", "stars": 1, "license": "None", "language": "python", "size": 275}
{"docstring": "\"\"\"\nReturns a list of migration file names for the given app.\n\"\"\"\n", "func_signal": "def get_migration_names(app):\n", "code": "return sorted([\n    filename[:-3]\n    for filename in os.listdir(os.path.dirname(app.__file__))\n    if filename.endswith(\".py\") and filename != \"__init__.py\" and not filename.startswith(\".\")\n])", "path": "south\\migration.py", "repo_name": "mukulu/django-south", "stars": 1, "license": "None", "language": "python", "size": 275}
{"docstring": "\"\"\"\nGiven a models module, returns a dict mapping all alias imports of models\n(e.g. import Foo as Bar) back to their original names. Bug #134.\n\"\"\"\n", "func_signal": "def aliased_models(module):\n", "code": "aliases = {}\nfor name, obj in module.__dict__.items():\n    if isclass(obj) and issubclass(obj, models.Model) and obj is not models.Model:\n        # Test to see if this has a different name to what it should\n        if name != obj._meta.object_name:\n            aliases[name] = obj._meta.object_name\nreturn aliases", "path": "south\\modelsparser.py", "repo_name": "mukulu/django-south", "stars": 1, "license": "None", "language": "python", "size": 275}
{"docstring": "\"\"\"\nRuns the specified migrations forwards, in order.\n\"\"\"\n\n", "func_signal": "def run_forwards(app, migrations, fake=False, db_dry_run=False, silent=False):\n", "code": "def record(app_name, migration):\n    # Record us as having done this\n    record = MigrationHistory.for_migration(app_name, migration)\n    record.applied = datetime.datetime.utcnow()\n    record.save()\n\nreturn run_migrations(\n    toprint = \" > %s: %s\",\n    torun = \"forwards\",\n    recorder = record,\n    app = app,\n    migrations = migrations,\n    fake = fake,\n    db_dry_run = db_dry_run,\n    silent = silent,\n)", "path": "south\\migration.py", "repo_name": "mukulu/django-south", "stars": 1, "license": "None", "language": "python", "size": 275}
{"docstring": "\"\"\"\nReturns a list of migrations that must be unapplied before (app, name) is,\nin the order they should be unapplied.\nUsed to make sure a migration can be unapplied (and to help unapply up to it).\n\"\"\"\n", "func_signal": "def needed_before_backwards(tree, app, name, sameapp=True):\n", "code": "app_migrations = get_migration_names(app)\nneeded = []\nif sameapp:\n    for aname in reversed(app_migrations[app_migrations.index(name)+1:]):\n        needed += needed_before_backwards(tree, app, aname, False)\n        needed += [(app, aname)]\nfor dapp, dname in tree[app][name].needed_by:\n    needed += needed_before_backwards(tree, dapp, dname)\n    needed += [(dapp, dname)]\nreturn remove_duplicates(needed)", "path": "south\\migration.py", "repo_name": "mukulu/django-south", "stars": 1, "license": "None", "language": "python", "size": 275}
{"docstring": "# Get the source of the model's file\n", "func_signal": "def get_model_tree(model):\n", "code": "source = open(inspect.getsourcefile(model)).read().replace(\"\\r\\n\", \"\\n\").replace(\"\\r\",\"\\n\") + \"\\n\"\ntree = STTree(parser.suite(source).totuple())\n# Now, we have to find it\nfor poss in tree.find(\"compound_stmt\"):\n    if poss.value[1][0] == symbol.classdef and \\\n       poss.value[1][2][1].lower() == model.__name__.lower():\n        # This is the tree\n        return poss", "path": "south\\modelsparser.py", "repo_name": "mukulu/django-south", "stars": 1, "license": "None", "language": "python", "size": 275}
{"docstring": "\"\"\"\nReturns a list of migrations that must be applied before (app, name),\nin the order they should be applied.\nUsed to make sure a migration can be applied (and to help apply up to it).\n\"\"\"\n", "func_signal": "def needed_before_forwards(tree, app, name, sameapp=True):\n", "code": "app_migrations = get_migration_names(app)\nneeded = []\nif sameapp:\n    for aname in app_migrations[:app_migrations.index(name)]:\n        needed += needed_before_forwards(tree, app, aname, False)\n        needed += [(app, aname)]\nfor dapp, dname in tree[app][name].needs:\n    needed += needed_before_forwards(tree, dapp, dname)\n    needed += [(dapp, dname)]\nreturn remove_duplicates(needed)", "path": "south\\migration.py", "repo_name": "mukulu/django-south", "stars": 1, "license": "None", "language": "python", "size": 275}
{"docstring": "\"\"\"\nRenames the table 'old_table_name' to 'table_name'.\n\"\"\"\n", "func_signal": "def rename_table(self, old_table_name, table_name):\n", "code": "if old_table_name == table_name:\n    # No Operation\n    return\nqn = connection.ops.quote_name\nparams = (qn(old_table_name), qn(table_name))\nself.execute('RENAME TABLE %s TO %s;' % params)", "path": "south\\db\\mysql.py", "repo_name": "mukulu/django-south", "stars": 1, "license": "None", "language": "python", "size": 275}
{"docstring": "\"\"\"\nYields (symbol, subtree) for the entire subtree.\nComes out with node 1, node 1's children, node 2, etc.\n\"\"\"\n", "func_signal": "def walk(self, recursive=True):\n", "code": "stack = [self.tree]\ndone_outer = False\nwhile stack:\n    atree = stack.pop()\n    if isinstance(atree, tuple):\n        if done_outer:\n            yield atree[0], STTree(atree)\n        if recursive or not done_outer:\n            for bit in reversed(atree[1:]):\n                stack.append(bit)\n            done_outer = True", "path": "south\\modelsparser.py", "repo_name": "mukulu/django-south", "stars": 1, "license": "None", "language": "python", "size": 275}
{"docstring": "\"\"\"\nRuns the specified migrations backwards, in order, skipping those\nmigrations in 'ignore'.\n\"\"\"\n\n", "func_signal": "def run_backwards(app, migrations, ignore=[], fake=False, db_dry_run=False, silent=False):\n", "code": "def record(app_name, migration):\n    # Record us as having not done this\n    record = MigrationHistory.for_migration(app_name, migration)\n    record.delete()\n\nreturn run_migrations(\n    toprint = \" < %s: %s\",\n    torun = \"backwards\",\n    recorder = record,\n    app = app,\n    migrations = [x for x in migrations if x not in ignore],\n    fake = fake,\n    db_dry_run = db_dry_run,\n    silent = silent,\n)", "path": "south\\migration.py", "repo_name": "mukulu/django-south", "stars": 1, "license": "None", "language": "python", "size": 275}
{"docstring": "# Tweak stuff as needed\n", "func_signal": "def create_table(self, table_name, fields):\n", "code": "for name,f in fields:\n    if isinstance(f, BooleanField):\n        if f.default == True:\n            f.default = 1\n        if f.default == False:\n            f.default = 0\n\n# Run\ngeneric.DatabaseOperations.create_table(self, table_name, fields)", "path": "south\\db\\sql_server\\pyodbc.py", "repo_name": "mukulu/django-south", "stars": 1, "license": "None", "language": "python", "size": 275}
{"docstring": "# Collapses the tree and tries to parse it as a field def\n", "func_signal": "def extract_field(tree):\n", "code": "bits = tree.flatten()\n## Check it looks right:\n# Second token should be equals\nif len(bits) < 2 or bits[1] != token.EQUAL:\n    return\n## Split into meaningful sections\nname = bits[0][1]\ndeclaration = bits[2:]\n# Find the first LPAR; stuff before that is the class.\ntry:\n    lpar_at = declaration.index(token.LPAR)\nexcept ValueError:\n    return\nclsname = reform(declaration[:lpar_at])\n# Now, inside that, find the last RPAR, and we'll take the stuff between\n# them as the arguments\ndeclaration.reverse()\nrpar_at = (len(declaration) - 1) - declaration.index(token.RPAR)\ndeclaration.reverse()\nargs = declaration[lpar_at+1:rpar_at]\n# Now, extract the arguments as a list and dict\ntry:\n    args, kwargs = parse_arguments(reform(args))\nexcept SyntaxError:\n    return\n# OK, extract and reform it\nreturn name, clsname, args, kwargs", "path": "south\\modelsparser.py", "repo_name": "mukulu/django-south", "stars": 1, "license": "None", "language": "python", "size": 275}
{"docstring": "\"\"\"\nReturns all apps with migrations.\n\"\"\"\n", "func_signal": "def get_migrated_apps():\n", "code": "for mapp in models.get_apps():\n    app = get_app(mapp)\n    if app:\n        yield app", "path": "south\\migration.py", "repo_name": "mukulu/django-south", "stars": 1, "license": "None", "language": "python", "size": 275}
{"docstring": "\"\"\"\nSearches the syntax tree with a CSS-like selector syntax.\nYou can use things like 'suite simple_stmt', 'suite, simple_stmt'\nor 'suite > simple_stmt'. Not guaranteed to return in order.\n\"\"\"\n# Split up the overall parts\n", "func_signal": "def find(self, selector):\n", "code": "patterns = [x.strip() for x in selector.split(\",\")]\nresults = []\nfor pattern in patterns:\n    # Split up the parts\n    parts = re.split(r'(?:[\\s]|(>))+', pattern)\n    # Take the first part, use it for results\n    if parts[0] == \"^\":\n        subresults = [self]\n    else:\n        subresults = list(self.findAllType(thing_that_name(parts[0])))\n    recursive = True\n    # For each remaining part, do something\n    for part in parts[1:]:\n        if not subresults:\n            break\n        if part == \">\":\n            recursive = False\n        elif not part:\n            pass\n        else:\n            thing = thing_that_name(part)\n            newresults = [\n                list(tree.findAllType(thing, recursive))\n                for tree in subresults\n            ]\n            subresults = []\n            for stuff in newresults:\n                subresults.extend(stuff)\n            recursive = True\n    results.extend(subresults)\nreturn results", "path": "south\\modelsparser.py", "repo_name": "mukulu/django-south", "stars": 1, "license": "None", "language": "python", "size": 275}
{"docstring": "\"\"\"\nTakes a string representing arguments and returns the positional and \nkeyword argument list and dict respectively.\nAll the entries in these are python source, except the dict keys.\n\"\"\"\n# Get the tree\n", "func_signal": "def parse_arguments(argstr):\n", "code": "tree = STTree(parser.suite(argstr).totuple())\n\n# Initialise the lists\ncurr_kwd = None\nargs = []\nkwds = {}\n\n# Walk through, assigning things\ntestlists = tree.find(\"testlist\")\nfor i, testlist in enumerate(testlists):\n    # BTW: A testlist is to the left or right of an =.\n    items = list(testlist.walk(recursive=False))\n    for j, item in enumerate(items):\n        if item[0] == symbol.test:\n            if curr_kwd:\n                kwds[curr_kwd] = item[1].reform()\n                curr_kwd = None\n            elif j == len(items)-1 and i != len(testlists)-1:\n                # Last item in a group must be a keyword, unless it's last overall\n                curr_kwd = item[1].reform()\n            else:\n                args.append(item[1].reform())\nreturn args, kwds", "path": "south\\modelsparser.py", "repo_name": "mukulu/django-south", "stars": 1, "license": "None", "language": "python", "size": 275}
{"docstring": "# Run ALTER TABLE with no unique column\n", "func_signal": "def add_column(self, table_name, name, field, *args, **kwds):\n", "code": "unique, field._unique, field.db_index = field.unique, False, False\n# If it's not nullable, and has no default, raise an error (SQLite is picky)\nif not field.null and (not field.has_default() or field.get_default() is None):\n    raise ValueError(\"You cannot add a null=False column without a default value.\")\ngeneric.DatabaseOperations.add_column(self, table_name, name, field, *args, **kwds)\n# If it _was_ unique, make an index on it.\nif unique:\n    self.create_index(table_name, [name], unique=True)", "path": "south\\db\\sqlite3.py", "repo_name": "mukulu/django-south", "stars": 1, "license": "None", "language": "python", "size": 275}
{"docstring": "\"\"\"\nReturns a list of migration classes (one for each migration) for the app.\n\"\"\"\n", "func_signal": "def get_migration_classes(app):\n", "code": "for name in get_migration_names(app):\n    yield get_migration(app, name)", "path": "south\\migration.py", "repo_name": "mukulu/django-south", "stars": 1, "license": "None", "language": "python", "size": 275}
{"docstring": "# Model 'Spam'\n", "func_signal": "def forwards(self):\n", "code": "        db.create_table(\"southtest_spam\", (\n            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),\n            ('weight', models.FloatField()),\n            ('expires', models.DateTimeField()),\n            ('name', models.CharField(max_length=255))\n        ))", "path": "south\\tests\\fakeapp\\migrations\\0001_spam.py", "repo_name": "mukulu/django-south", "stars": 1, "license": "None", "language": "python", "size": 275}
{"docstring": "\"\"\"Swaps out various Django calls for fake ones for our own nefarious purposes.\"\"\"\n\n", "func_signal": "def monkeypatch(self):\n", "code": "def new_get_apps():\n    return ['fakeapp']\n\nfrom django.db import models\nfrom django.conf import settings\nmodels.get_apps_old, models.get_apps = models.get_apps, new_get_apps\nsettings.INSTALLED_APPS, settings.OLD_INSTALLED_APPS = (\n    [\"fakeapp\"],\n    settings.INSTALLED_APPS,\n)\nself.redo_app_cache()", "path": "south\\tests\\__init__.py", "repo_name": "mukulu/django-south", "stars": 1, "license": "None", "language": "python", "size": 275}
{"docstring": "\"\"\"Undoes what monkeypatch did.\"\"\"\n\n", "func_signal": "def unmonkeypatch(self):\n", "code": "from django.db import models\nfrom django.conf import settings\nmodels.get_apps = models.get_apps_old\nsettings.INSTALLED_APPS = settings.OLD_INSTALLED_APPS\nself.redo_app_cache()", "path": "south\\tests\\__init__.py", "repo_name": "mukulu/django-south", "stars": 1, "license": "None", "language": "python", "size": 275}
{"docstring": "\"\"\"Accepts an invitation.\n\n    @param ccard: the contactcard service\n    @param contact: the contact to fetch his CCard\n    @param callback: tuple(callable, *args)\n    @param errback: tuple(callable, *args)\n\"\"\"\n", "func_signal": "def __init__(self, ccard, contact, callback, errback):\n", "code": "BaseScenario.__init__(self, callback, errback)\nself.__ccard = ccard\nself.__contact = contact", "path": "papyon\\service\\Spaces\\scenario\\get_contact_card.py", "repo_name": "mobad85/papyon", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 1136}
{"docstring": "\"\"\"Returns the SOAP xml body\"\"\"\n\n", "func_signal": "def soap_body(group_id):\n", "code": "return \"\"\"\n    <ABGroupDelete xmlns=\"http://www.msn.com/webservices/AddressBook\">\n        <abId>\n            00000000-0000-0000-0000-000000000000\n        </abId>\n        <groupFilter>\n            <groupIds>\n                <guid>\n                    %(group_id)s\n                </guid>\n            </groupIds>\n        </groupFilter>\n    </ABGroupDelete>\"\"\" % { 'group_id' : group_id }", "path": "papyon\\service\\description\\AB\\ABGroupDelete.py", "repo_name": "mobad85/papyon", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 1136}
{"docstring": "\"\"\"decorator to be used on decorators, it preserves the docstring and\nfunction attributes of functions to which it is applied.\"\"\"\n", "func_signal": "def decorator(function):\n", "code": "def new_decorator(f):\n    g = function(f)\n    g.__name__ = f.__name__\n    g.__doc__ = f.__doc__\n    g.__dict__.update(f.__dict__)\n    return g\nnew_decorator.__name__ = function.__name__\nnew_decorator.__doc__ = function.__doc__\nnew_decorator.__dict__.update(function.__dict__)\nreturn new_decorator", "path": "papyon\\util\\decorator.py", "repo_name": "mobad85/papyon", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 1136}
{"docstring": "\"\"\"Connect Switchboard Manager signals\"\"\"\n", "func_signal": "def __connect_switchboard_manager_signals(self):\n", "code": "def handler_created(switchboard_manager, handler_class, handler):\n    if handler_class is SwitchboardConversation:\n        if self._dispatch(\"on_invite_conversation\", handler) == 0:\n            logger.warning(\"No event handler attached for conversations\")\n    else:\n        logger.warning(\"Unknown Switchboard Handler class %s\" % handler_class)\n\nself._switchboard_manager.connect(\"handler-created\", handler_created)", "path": "papyon\\client.py", "repo_name": "mobad85/papyon", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 1136}
{"docstring": "# Check to see if it's a signaling message\n", "func_signal": "def _blob_to_session(self, blob):\n", "code": "if blob.session_id == 0:\n    blob.data.seek(0, 0)\n    slp_data = blob.data.read()\n    blob.data.seek(0, 0)\n    try:\n        message = SLPMessage.build(slp_data)\n    except ParseError:\n        logger.warning('Received blob with SessionID=0 and non SLP data')\n        raise SLPError(\"Non SLP data for blob with null sessionID\")\n    session_id = message.body.session_id\n\n    # Backward compatible with older clients that use the call-id\n    # for responses\n    if session_id == 0:\n        call_id = message.call_id\n        for session in self._sessions.itervalues():\n            if session.call_id == call_id:\n                return session\n        # Couldn't find a session for the call id we received\n        return None\n    if session_id in self._sessions:\n        return self._sessions[session_id]\n    # Session doesn't exist\n    return None\nelse:\n    session_id = blob.session_id\n    if session_id in self._sessions:\n        return self._sessions[blob.session_id]\n    else:\n        raise SLPSessionError(\"Unknown session\")", "path": "papyon\\msnp2p\\session_manager.py", "repo_name": "mobad85/papyon", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 1136}
{"docstring": "\"\"\"Logout from the server.\"\"\"\n", "func_signal": "def logout(self):\n", "code": "if self._state == ClientState.CLOSED:\n    logger.warning('alreay logged out')\n    return\nself.__die = True\nself._switchboard_manager.close()\nif self.__state < ClientState.AUTHENTICATING:\n    self._transport.lose_connection()\nelse:\n    self._protocol.signoff()\nself.__state = ClientState.CLOSED", "path": "papyon\\client.py", "repo_name": "mobad85/papyon", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 1136}
{"docstring": "\"\"\"Connect Offline IM signals\"\"\"\n", "func_signal": "def __connect_oim_box_signals(self):\n", "code": "def event(oim_box, *args):\n    method_name = \"on_oim_%s\" % args[-1].replace(\"-\", \"_\")\n    self._dispatch(method_name, *args[:-1])\ndef state_changed(oim_box, pspec):\n    self._dispatch(\"on_oim_state_changed\", oim_box.state)\ndef error(oim_box, error_code):\n    self._dispatch(\"on_client_error\", ClientErrorType.OFFLINE_MESSAGES, error_code)\n\nself.oim_box.connect(\"notify::state\", state_changed)\nself.oim_box.connect('error', error)\n\ndef connect_signal(name):\n    self.oim_box.connect(name, event, name)\nconnect_signal(\"messages-received\")\nconnect_signal(\"messages-fetched\")\nconnect_signal(\"message-sent\")\nconnect_signal(\"messages-deleted\")", "path": "papyon\\client.py", "repo_name": "mobad85/papyon", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 1136}
{"docstring": "\"\"\"Login to the server.\n\n    @param account: the account to use for authentication.\n    @type account: utf-8 encoded string\n\n    @param password: the password needed to authenticate to the account\n    @type password: utf-8 encoded string\n    \"\"\"\n", "func_signal": "def login(self, account, password):\n", "code": "if (self._state != ClientState.CLOSED):\n    logger.warning('login already in progress')\nself.__die = False\nself._state = ClientState.CONNECTING\nself._profile = profile.Profile((account, password), self._protocol)\nself.__connect_profile_signals()\nself._mailbox = msnp.Mailbox(self._protocol)\nself.__connect_mailbox_signals()\nself._transport.establish_connection()", "path": "papyon\\client.py", "repo_name": "mobad85/papyon", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 1136}
{"docstring": "\"\"\"Initializer\"\"\"\n", "func_signal": "def __init__(self, client):\n", "code": "gobject.GObject.__init__(self)\n\nself._client = client\nself._sessions = weakref.WeakValueDictionary() # session_id => session\nself._handlers = []\nself._transport_manager = P2PTransportManager(self._client)\nself._transport_manager.connect(\"blob-received\",\n        lambda tr, blob: self._on_blob_received(blob))\nself._transport_manager.connect(\"blob-sent\",\n        lambda tr, blob: self._on_blob_sent(blob))", "path": "papyon\\msnp2p\\session_manager.py", "repo_name": "mobad85/papyon", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 1136}
{"docstring": "\"\"\"Returns the SOAP xml body\"\"\"\n\n", "func_signal": "def soap_body(group_id, contact_id):\n", "code": "return \"\"\"\n    <ABGroupContactAdd xmlns=\"http://www.msn.com/webservices/AddressBook\">\n        <abId>\n            00000000-0000-0000-0000-000000000000\n        </abId>\n        <groupFilter>\n            <groupIds>\n                <guid>\n                    %(group_id)s\n                </guid>\n            </groupIds>\n        </groupFilter>\n        <contacts>\n            <Contact>\n                <contactId>\n                    %(contact_id)s\n                </contactId>\n            </Contact>\n        </contacts>\n    </ABGroupContactAdd>\"\"\" % { 'group_id' : group_id,\n                                'contact_id' : contact_id }", "path": "papyon\\service\\description\\AB\\ABGroupContactAdd.py", "repo_name": "mobad85/papyon", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 1136}
{"docstring": "\"\"\"Connect profile signals\"\"\"\n", "func_signal": "def __connect_profile_signals(self):\n", "code": "def property_changed(profile, pspec):\n    method_name = \"on_profile_%s_changed\" % pspec.name.replace(\"-\", \"_\")\n    self._dispatch(method_name)\n\nself.profile.connect(\"notify::presence\", property_changed)\nself.profile.connect(\"notify::display-name\", property_changed)\nself.profile.connect(\"notify::personal-message\", property_changed)\nself.profile.connect(\"notify::current-media\", property_changed)\nself.profile.connect(\"notify::msn-object\", property_changed)", "path": "papyon\\client.py", "repo_name": "mobad85/papyon", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 1136}
{"docstring": "\"\"\"Initializer\n\n    @param server: the Notification server to connect to.\n    @type server: tuple(host, port)\n\n    @param proxies: proxies that we can use to connect\n    @type proxies: {type: string => L{gnet.proxy.ProxyInfos}}\n\n    @param transport_class: the transport class to use for the network\n            connection\n    @type transport_class: L{papyon.transport.BaseTransport}\"\"\"\n", "func_signal": "def __init__(self, server, proxies={}, transport_class=DirectConnection):\n", "code": "EventsDispatcher.__init__(self)\n\nself.__state = ClientState.CLOSED\n\nself._proxies = proxies\nself._transport_class = transport_class\nself._proxies = proxies\n\nself._transport = transport_class(server, ServerType.NOTIFICATION,\n        self._proxies)\nself._protocol = msnp.NotificationProtocol(self, self._transport,\n        self._proxies)\n\nself._switchboard_manager = SwitchboardManager(self)\nself._switchboard_manager.register_handler(SwitchboardConversation)\n\nself._p2p_session_manager = P2PSessionManager(self)\nself._webcam_handler = WebcamHandler(self)\nself._p2p_session_manager.register_handler(self._webcam_handler)\n\nself._msn_object_store = MSNObjectStore(self)\nself._p2p_session_manager.register_handler(self._msn_object_store)\n\n\n\nself._external_conversations = {}\n\nself._sso = None\n\nself._profile = None\nself._address_book = None\nself._oim_box = None\nself._mailbox = None\n\nself.__die = False\nself.__connect_transport_signals()\nself.__connect_protocol_signals()\nself.__connect_switchboard_manager_signals()\nself.__connect_webcam_handler_signals()", "path": "papyon\\client.py", "repo_name": "mobad85/papyon", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 1136}
{"docstring": "\"\"\"This is a decorator which can be used to mark functions as unstable API\nwise. It will result in a warning being emitted when the function is used.\"\"\"\n", "func_signal": "def unstable(func):\n", "code": "def new_function(*args, **kwargs):\n    warnings.warn(\"Call to unstable API function %s.\" % func.__name__,\n                  category=FutureWarning)\n    return func(*args, **kwargs)\nreturn new_function", "path": "papyon\\util\\decorator.py", "repo_name": "mobad85/papyon", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 1136}
{"docstring": "\"\"\"Connect contact signals\"\"\"\n", "func_signal": "def __connect_contact_signals(self, contact):\n", "code": "def event(contact, *args):\n    event_name = args[-1]\n    event_args = args[:-1]\n    method_name = \"on_contact_%s\" % event_name.replace(\"-\", \"_\")\n    self._dispatch(method_name, contact, *event_args)\n\ndef property_changed(contact, pspec):\n    method_name = \"on_contact_%s_changed\" % pspec.name.replace(\"-\", \"_\")\n    self._dispatch(method_name, contact)\n\ncontact.connect(\"notify::memberships\", property_changed)\ncontact.connect(\"notify::presence\", property_changed)\ncontact.connect(\"notify::display-name\", property_changed)\ncontact.connect(\"notify::personal-message\", property_changed)\ncontact.connect(\"notify::current-media\", property_changed)\ncontact.connect(\"notify::msn-object\", property_changed)\ncontact.connect(\"notify::client-capabilities\", property_changed)\n\ndef connect_signal(name):\n    contact.connect(name, event, name)\nconnect_signal(\"infos-changed\")", "path": "papyon\\client.py", "repo_name": "mobad85/papyon", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 1136}
{"docstring": "\"\"\"Returns the SOAP xml body\"\"\"\n\n", "func_signal": "def soap_body(contact_id):\n", "code": "return \"\"\"        \n    <ABContactDelete xmlns=\"http://www.msn.com/webservices/AddressBook\">\n        <abId>\n            00000000-0000-0000-0000-000000000000\n        </abId>\n        <contacts>\n            <Contact>\n                <contactId>\n                    %(contact_id)s\n                </contactId>\n            </Contact>\n        </contacts>\n    </ABContactDelete>\"\"\" % { 'contact_id' : contact_id }", "path": "papyon\\service\\description\\AB\\ABContactDelete.py", "repo_name": "mobad85/papyon", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 1136}
{"docstring": "\"\"\"Initializer\n\n    @param client: the main Client instance\"\"\"\n", "func_signal": "def __init__(self, client):\n", "code": "gobject.GObject.__init__(self)\nself._client = weakref.proxy(client)\n\nself._handlers_class = set()\nself._orphaned_handlers = WeakSet()\nself._switchboards = {}\nself._orphaned_switchboards = set()\nself._pending_switchboards = {}\n\nself._client._protocol.connect(\"switchboard-invitation-received\",\n        self._ns_switchboard_invite)", "path": "papyon\\switchboard_manager.py", "repo_name": "mobad85/papyon", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 1136}
{"docstring": "\"\"\"Connect protocol signals\"\"\"\n", "func_signal": "def __connect_protocol_signals(self):\n", "code": "def state_changed(proto, param):\n    state = proto.state\n    if state == msnp.ProtocolState.AUTHENTICATING:\n        self._state = ClientState.AUTHENTICATING\n    elif state == msnp.ProtocolState.AUTHENTICATED:\n        self._state = ClientState.AUTHENTICATED\n    elif state == msnp.ProtocolState.SYNCHRONIZING:\n        self._state = ClientState.SYNCHRONIZING\n    elif state == msnp.ProtocolState.SYNCHRONIZED:\n        self._state = ClientState.SYNCHRONIZED\n    elif state == msnp.ProtocolState.OPEN:\n        self._state = ClientState.OPEN\n        im_contacts = self.address_book.contacts\n        for contact in im_contacts:\n            self.__connect_contact_signals(contact)\n\ndef authentication_failed(proto):\n    self._dispatch(\"on_client_error\", ClientErrorType.AUTHENTICATION,\n                   AuthenticationError.INVALID_USERNAME_OR_PASSWORD)\n    self.__die = True\n    self._transport.lose_connection()\n\ndef disconnected_by_other(proto):\n    self._dispatch(\"on_client_error\", ClientErrorType.PROTOCOL,\n                   ProtocolError.OTHER_CLIENT)\n    self.__die = True\n    self._transport.lose_connection()\n\ndef server_down(proto):\n    self._dispatch(\"on_client_error\", ClientErrorType.PROTOCOL,\n                   ProtocolError.SERVER_DOWN)\n    self.__die = True\n    self._transport.lose_connection()\n\ndef unmanaged_message_received(proto, sender, message):\n    if sender in self._external_conversations:\n        conversation = self._external_conversations[sender]\n        conversation._on_message_received(message)\n    else:\n        conversation = ExternalNetworkConversation(self, [sender])\n        self._register_external_conversation(conversation)\n        if self._dispatch(\"on_invite_conversation\", conversation) == 0:\n            logger.warning(\"No event handler attached for conversations\")\n        conversation._on_message_received(message)\n\nself._protocol.connect(\"notify::state\", state_changed)\nself._protocol.connect(\"authentication-failed\", authentication_failed)\nself._protocol.connect(\"disconnected-by-other\", disconnected_by_other)\nself._protocol.connect(\"server-down\", server_down)\nself._protocol.connect(\"unmanaged-message-received\", unmanaged_message_received)", "path": "papyon\\client.py", "repo_name": "mobad85/papyon", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 1136}
{"docstring": "\"\"\"Initializer\n\n@param method: the method to be called\n@type method: string\n\n@param namespace: the namespace that the method belongs to\n@type namespace: URI\n\n@param encoding_style: the encoding style for this method\n@type encoding_style: URI\n\n@param attr: attributes to be attached to the method\"\"\"\n", "func_signal": "def __init__(self, method, namespace=None, encoding_style=Encoding.SOAP, **attr):\n", "code": "self.header = ElementTree.Element(_SOAPSection.HEADER)\nif namespace is not None:\n    method = \"{\" + namespace + \"}\" + method\nself.method = ElementTree.Element(method)\nif encoding_style is not None:\n    self.method.set(\"{\" + NameSpace.SOAP_ENVELOPE + \"}encodingStyle\", encoding_style)\n\nfor attr_key, attr_value in attr.iteritems():\n    self.method.set(attr_key, attr_value)", "path": "papyon\\gnet\\message\\SOAP.py", "repo_name": "mobad85/papyon", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 1136}
{"docstring": "\"\"\"This is a decorator which can be used to mark functions as deprecated.\nIt will result in a warning being emitted when the function is used.\"\"\"\n", "func_signal": "def deprecated(func):\n", "code": "def new_function(*args, **kwargs):\n    warnings.warn(\"Call to deprecated function %s.\" % func.__name__,\n                  category=DeprecationWarning)\n    return func(*args, **kwargs)\nreturn new_function", "path": "papyon\\util\\decorator.py", "repo_name": "mobad85/papyon", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 1136}
{"docstring": "\"\"\"Connect mailbox signals\"\"\"\n", "func_signal": "def __connect_mailbox_signals(self):\n", "code": "def new_mail_received(mailbox, mail):\n    self._dispatch(\"on_mailbox_new_mail_received\", mail)\n\ndef unread_changed(mailbox, unread_count, initial):\n    method_name = \"on_mailbox_unread_mail_count_changed\"\n    self._dispatch(method_name, unread_count, initial)\n    \nself.mailbox.connect(\"unread-mail-count-changed\", unread_changed)\nself.mailbox.connect(\"new-mail-received\", new_mail_received)", "path": "papyon\\client.py", "repo_name": "mobad85/papyon", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 1136}
{"docstring": "\"\"\" Append a string to the block, as a line \"\"\"\n", "func_signal": "def append(self, strline, *more_strlines):\n", "code": "self.lines.append(strline)\nfor line in more_strlines:\n    self.lines.append(line)", "path": "modules\\TemplateProcessor.py", "repo_name": "goerz/pyala", "stars": 1, "license": "None", "language": "python", "size": 1612}
{"docstring": "\"\"\" Write out all unwritten visits, and create a TOC \"\"\"\n", "func_signal": "def finish(self):\n", "code": "self._writeout(is_last = True)\nself._visits = []\nself._openvisits = 0\nself._write_toc()", "path": "modules\\DetailedStatComponent.py", "repo_name": "goerz/pyala", "stars": 1, "license": "None", "language": "python", "size": 1612}
{"docstring": "\"\"\" Generate a boolean match function for the compiled regex r\n\n    The generated function will have one string parameter and will\n    return True if the string matches the regex r, False otherwise.\n\"\"\"\n", "func_signal": "def rule_regex(r):\n", "code": "import re\nif type(r) == type(re.compile(\"\")):\n    def f(s):\n        try:\n            if r.search(s):\n                return True\n            else:\n                return False\n        except TypeError:\n            warn(\"Option value must be a string. Regex will not match\")\n            return False\n    return f\nelse:\n    warn(\"Couldn't compile %s does not seem to be a regex. The rule will always match\" % r)\n    return lambda s: True", "path": "modules\\OptionManager.py", "repo_name": "goerz/pyala", "stars": 1, "license": "None", "language": "python", "size": 1612}
{"docstring": "\"\"\" Get full country name from two-letter country code \"\"\"\n", "func_signal": "def countryname_from_code(self, code):\n", "code": "try:\n    return self.countrynames[code]\nexcept KeyError:\n    return 'Unknown'", "path": "modules\\GeoIPWrapper.py", "repo_name": "goerz/pyala", "stars": 1, "license": "None", "language": "python", "size": 1612}
{"docstring": "\"\"\" Write the main table of contents file with a calendar\"\"\"\n", "func_signal": "def _write_toc(self):\n", "code": "tp = TemplateProcessor(template = self.get_option('html_toc_template'))\n# build list of the months, so that we can index it\n# we need three lists:\nmonthlist = [] # to keep the dict that maps days to anchors for each month\nmonthnamelist = [] # the names (number) of the corresponding months\nyearnamelist = [] # the names (number) of the corresponding years\nminyear = min(self._anchors.keys())\nmaxyear = max(self._anchors.keys())\nfor year in xrange(minyear, maxyear+1):\n    if self._anchors.has_key(year):\n        # try each month in every year, and see if an entry exists\n        for month in xrange(1,13):\n            if self._anchors[year].has_key(month):\n                monthlist.append(self._anchors[year][month])\n                monthnamelist.append(month)\n                yearnamelist.append(year)\n\ndef calendarloop(index, varname):\n    loopreplacements = {\n        'calendar'  : HTMLCalendar.MonthCal().render(yearnamelist[index], \\\n                                                     monthnamelist[index], \\\n                                                     monthlist[index])\n    }\n    if len(monthlist) > 2:\n        loopreplacements['calendar'] = \"<div style=\\\"float: left; width: 33%\\\">\\n\" \\\n                                       + loopreplacements['calendar']\n    else:\n        loopreplacements['calendar'] = \"<div style=\\\"float: left; width: %s\" % (100/len(monthlist)) \\\n                                       + \"%\\\">\\n\" + loopreplacements['calendar']\n    loopreplacements['calendar'] += \"</div>\\n\"\n    try:\n        return loopreplacements[varname]\n    except:\n        return None\n\nreplacements = {\n    'headrow' : self.get_option('title') + \"&nbsp;&nbsp;-&nbsp;&nbsp;TOC\",\n    'total_visits': str(self._number_of_visits),\n    'link_to_prev': self._outfilename.replace( \".html\", \\\n                    str(self._number_of_pages-1).zfill(6) +\".html\" ),\n    'link_to_next': self._outfilename.replace( \".html\", \\\n                    str(self._number_of_pages+1).zfill(6) +\".html\" )\n}\ntp.replacements = replacements\ntp.loops = {'calendars' : LoopData(\\\n                          length=len(monthlist), \\\n                          datafunction=calendarloop) }\ntp.output = os.path.join(self.get_option('outdir'), self._outfilename)\n\ntp.run()", "path": "modules\\DetailedStatComponent.py", "repo_name": "goerz/pyala", "stars": 1, "license": "None", "language": "python", "size": 1612}
{"docstring": "\"\"\" Get loop data by index and varname\"\"\"\n", "func_signal": "def get(self, index, varname):\n", "code": "if (index < 0) or (index >= self.length):\n    raise TemplateProcessorLoopDataOutOfBounds(index)\nreturn self.datafunction(index, varname)", "path": "modules\\TemplateProcessor.py", "repo_name": "goerz/pyala", "stars": 1, "license": "None", "language": "python", "size": 1612}
{"docstring": "\"\"\" print a warning message to stderr \"\"\"\n", "func_signal": "def warn(msg):\n", "code": "sys.stderr.write(str(msg))\nsys.stderr.write(\"\\n\")", "path": "modules\\GeoIPWrapper.py", "repo_name": "goerz/pyala", "stars": 1, "license": "None", "language": "python", "size": 1612}
{"docstring": "\"\"\" Write out all registered visits and clear the list \"\"\"\n", "func_signal": "def _writeout(self, is_last=False):\n", "code": "self._number_of_pages += 1\nis_first = (self._number_of_pages == 1)\nfilename = self._outfilename.replace( \".html\", \\\n           str(self._number_of_pages).zfill(6) +\".html\" )\ntp = TemplateProcessor(template = self.get_option('html_template'))\ndef visitloop(index, varname):\n    loopreplacements = {\n        'id'                : str(self._visits[index].number),\n        'time'              : ApacheLogParser.stringdate( \\\n                                  self._visits[index].begin_time, \\\n                                  pattern=\"%m/%d/%y %H:%M:%S\", \\\n                                  offset=self._visits[index].pages[-1].serveroffset),\n        'countryicon'       : self._visits[index].country_icon,\n        'countryname'       : self._visits[index].countryname,\n        'countryextension'  : self._visits[index].countryextension,\n        'hostname'          : self._visits[index].hostname,\n        'hostname_short'    : self._visits[index].hostname[-27:],\n        'number_of_pages'   : str(len(self._visits[index].pages)),\n        'visitpage'         : os.path.join(\n                               self._visit_page_folder,\n                               str(self._visits[index].number).zfill(\\\n                                          self._visit_filename_length) + \".html\" ),\n        'os_icon'           : self._visits[index].os_icon,\n        'full_os'           : self._visits[index].os + ' ' + self._visits[index].os_version,\n        'os'                : self._visits[index].os,\n        'browser_icon'      : self._visits[index].browser_icon,\n        'full_browser'      : self._visits[index].browser + ' ' \\\n                              + self._visits[index].browser_version \\\n                              + ' (' +  self._visits[index].fullbrowser + ')',\n        'browser'           : self._visits[index].browser,\n        'referer_url'       : self._visits[index].referer,\n        'referer_site'      : self._visits[index].referer_site,\n        'referer_page'      : self._visits[index].referer_page,\n        'last_page'         : self._visits[index].pages[-1].file,\n        'last_page_short'   : self._visits[index].pages[-1].file[\\\n                                    self._visits[index].pages[-1].file.rfind(\"/\", 0, -2)+1:],\n        'search_term'       : self._visits[index].search.replace(r'\"',r'&quot;'),\n        'search_term_short' : self._visits[index].search[:27],\n        'is_bot'            : str(self._visits[index].is_bot),\n        'anchor_name'       : 'a' + str(self._visits[index].number)\n    }\n    if len(loopreplacements['last_page_short']) > 30:\n        loopreplacements['last_page_short'] = \\\n            loopreplacements['last_page_short'][:27] + \"...\"\n    if len(self._visits[index].search) > 27:\n        loopreplacements['search_term_short'] = loopreplacements['search_term_short'] + \"...\"\n    if len(self._visits[index].hostname) > 27:\n        loopreplacements['hostname_short'] = \"...\" +  loopreplacements['hostname_short']\n    if self._visits[index].is_bot == \"Yes\":\n        loopreplacements['os_icon'] =  self._visits[index].bot_icon\n        loopreplacements['os'] =  self._visits[index].botname\n        loopreplacements['full_os'] =  self._visits[index].botname + self._visits[index].bot_version\n        loopreplacements['browser_icon'] =  self._visits[index].bot_icon\n        loopreplacements['browser'] =  self._visits[index].botname\n        loopreplacements['full_browser'] =  self._visits[index].botname + self._visits[index].bot_version\n    try:\n        return loopreplacements[varname]\n    except:\n        return None\nreplacements = {\n    'headrow' : self.get_option('title'),\n    'visits_per_page': str(self.get_option('visits_per_page')),\n    'link_to_prev': self._outfilename.replace( \".html\", \\\n                    str(self._number_of_pages-1).zfill(6) +\".html\" ),\n    'link_to_next': self._outfilename.replace( \".html\", \\\n                    str(self._number_of_pages+1).zfill(6) +\".html\" ),\n    'link_to_toc' : self._outfilename,\n    'write_visit_pages' : str(self.get_option('write_visit_pages'))\n}\nif is_first:\n    replacements['link_to_prev'] = self._outfilename\nif is_last:\n    replacements['link_to_next'] = self._outfilename\ntp.replacements = replacements\ntp.loops = {'visits' : LoopData(length=len(self._visits), datafunction=visitloop) }\ntp.output = os.path.join(self.get_option('outdir'), filename)\n\ntp.run()", "path": "modules\\DetailedStatComponent.py", "repo_name": "goerz/pyala", "stars": 1, "license": "None", "language": "python", "size": 1612}
{"docstring": "\"\"\" Create a TemplateProcessor object\n\n    Any of the class attributes can be given as optional named parameters\n\"\"\"\n", "func_signal": "def __init__(self, **args):\n", "code": "if args.has_key(\"template\"):\n    self.template = args[\"template\"]\nelse:\n    self.__dict__[\"template\"] = \"\"\nif args.has_key(\"output\"):\n    self.output = args[\"output\"]\nelse:\n    self.__dict__[\"output\"] = \"\"\nif args.has_key(\"replacements\"):\n    self._varreplacements = args[\"replacements\"]\nelse:\n    self.__dict__[\"_varreplacements\"] = {}\nif args.has_key(\"indent\"):\n    self.indent = args[\"indent\"]\nelse:\n    self.indent = \"\"\nif args.has_key(\"loops\"):\n    self.loops = args[\"loops\"]\nelse:\n    self.__dict__[\"loops\"] = {}\nif args.has_key(\"memorytemplate\"):\n    self.memorytemplate = args[\"memorytemplate\"]\nelse:\n    self.memorytemplate = None\n\n# private attributes\nself.__dict__[\"_outfile\"]      = None # the file object for output\nself.__dict__[\"_templatefile\"] = None # the file object for the template file\nself._replacements             = {} # full string replacements, not just varnames\nself._iteration                = {} # keeps track of how many iterations were done in a specific loop\nself._activeoutput             = True; # if this is False, all output is surpressed (used for if-blocks)\nself._if_stack                 = [] # stack for nested if's", "path": "modules\\TemplateProcessor.py", "repo_name": "goerz/pyala", "stars": 1, "license": "None", "language": "python", "size": 1612}
{"docstring": "\"\"\" print a warning message to stderr \"\"\"\n", "func_signal": "def warn(msg):\n", "code": "sys.stderr.write(str(msg))\nsys.stderr.write(\"\\n\")", "path": "modules\\HelperDataRobot.py", "repo_name": "goerz/pyala", "stars": 1, "license": "None", "language": "python", "size": 1612}
{"docstring": "\"\"\"\nInitialization takes two mandatory parameters:\nlength: and integer > 0 that describes how many iterations are in the loop\ndatafunction: a callable that provides the loop data\n\nThe datafunction must have the following specifications:\n- it takes exactly two parameters: index, and varname\n- it returns a string that is the contents of varname for the iteration index\n- if the varname does not exist, it doesn't return anything (None)\n\nThere is a public attribute 'iteration' that serves as a marker of where the\nloop is in an iteration. You have to set the attribute externally\n\"\"\"\n", "func_signal": "def __init__(self, length, datafunction):\n", "code": "if isinstance(length, int) and (length >= 0):\n    self.length = length\nelse:\n    raise TemplateProcessorErrorInLoopVarConstruction(\"length must be an integer >= 0\")\nif callable(datafunction):\n    # run a test of the datafunction\n    try:\n        datafunction(length-1, \"\")\n    except TypeError: # test failed\n        raise TemplateProcessorErrorInLoopVarConstruction(\\\n            \"The data function did not pass the test call. \" + \\\n            \"Try help(LoopData) for the specifications of a valid data function\");\n    else: # test succeeded\n        self.datafunction = datafunction\nelse:\n    raise TemplateProcessorErrorInLoopVarConstruction(\\\n        \"The data function is not callable\")\nself.iteration = 0", "path": "modules\\TemplateProcessor.py", "repo_name": "goerz/pyala", "stars": 1, "license": "None", "language": "python", "size": 1612}
{"docstring": "\"\"\" Create the path d and return True on success or if the directory\n    already exists, return False otherwise.\n\"\"\"\n", "func_signal": "def rule_ensure_dir(d):\n", "code": "import os.path\ntry:\n    if not os.path.isdir(d):\n        os.makedirs(d)\n    return True\nexcept:\n    return False", "path": "modules\\OptionManager.py", "repo_name": "goerz/pyala", "stars": 1, "license": "None", "language": "python", "size": 1612}
{"docstring": "\"\"\" print a warning message to stderr \"\"\"\n", "func_signal": "def warn(msg):\n", "code": "sys.stderr.write(str(msg))\nsys.stderr.write(\"\\n\")", "path": "modules\\TemplateProcessor.py", "repo_name": "goerz/pyala", "stars": 1, "license": "None", "language": "python", "size": 1612}
{"docstring": "\"\"\" Identify the robot from a browserstring\n\n    You get a dict describing the robot with the following fields:\n        'title'       The name of the robot\n        'icon'        An identifier for an icon describing the robot\n        'uri'         A Internet URL that is associated with the robot\n        'version'     An additional string that gives the version info\n\n    If the browser string cannot be identified as any robot, None is\n    returned\n\"\"\"\n\n", "func_signal": "def robot_from_browserstring(self, browserstring):\n", "code": "for robot in self.robots:\n    for rule_tuple in robot[\"rule\"]:\n        rule = rule_tuple[0]\n        versionrule = rule_tuple[1]\n        match = rule.search(browserstring)\n        if match:\n            title = robot[\"title\"]\n            icon = robot[\"icon\"]\n            uri = \"\"\n            if robot.has_key(\"uri\"):\n                uri = robot[\"uri\"]\n            version = \"\"\n            if isinstance(versionrule, int):\n                version = match.group(versionrule)\n            else:\n                version = versionrule\n            # sanity checks to prevent screw-ups from invalid imported data structures:\n            if not isinstance(title, str):\n                warn(\"Internal error in robot_from_browserstring:\")\n                warn(\"Extracted title for bot was not a string.\")\n                warn(\"browserstring was: %s\" % browserstring)\n                title = \"\"\n            if not isinstance(icon, str):\n                warn(\"Internal error in robot_from_browserstring:\")\n                warn(\"Extracted icon for bot was not a string.\")\n                warn(\"browserstring was: %s\" % browserstring)\n                icon = \"\"\n            if not isinstance(uri, str):\n                warn(\"Internal error in robot_from_browserstring:\")\n                warn(\"Extracted uri for bot was not a string.\")\n                warn(\"browserstring was: %s\" % browserstring)\n                uri = \"\"\n            if not isinstance(version, str):\n                warn(\"Internal error in robot_from_browserstring:\")\n                warn(\"Extracted version for bot was not a string.\")\n                warn(\"browserstring was: %s\" % browserstring)\n                version = \"\"\n            return {\n                'title'   : title,\n                'icon'    : icon,\n                'uri'     : uri,\n                'version' : version\n            }\nreturn None", "path": "modules\\HelperDataRobot.py", "repo_name": "goerz/pyala", "stars": 1, "license": "None", "language": "python", "size": 1612}
{"docstring": "\"\"\" Return country information for hostname or IP\n\n    Result is a dict with the following keys:\n        name        the full country name, or \"Unknown\"\n        code        the two-letter country code, or \"N/A\"\n        icon        an icon representing the country\n\"\"\"\n", "func_signal": "def country_from_host(self, host):\n", "code": "host = host.strip()\ncountrycode = \"N/A\"\nif self.ip_pattern.match(host): # it's an ip address\n    countrycode = self.gi.country_code_by_addr(host)\nelse: # it's a hostname\n    countrycode = None\n    # try the pre_try dict first of all\n    for (key, cc_value) in self.pre_try:\n        if host.endswith(key):\n            countrycode = cc_value\n            break\n    if countrycode is None:\n        # try the normal lookup\n        countrycode = self.gi.country_code_by_name(host)\n    if countrycode is None:\n        # try the frequentfailures list\n        for (key, cc_value) in self.frequentfailures:\n            if host.endswith(key):\n                countrycode = cc_value\n                break\n    if countrycode is None:\n        # maybe the country is in the hostname?\n        countrycode = re.search(r'\\.([^.]{2})$', host)\n        if countrycode is not None:\n            countrycode = countrycode.group(1).upper()\n            if self.countryname_from_code(countrycode) == 'Unknown': # failed\n                countrycode = None\n        else:\n            # try just the last part of the hostname\n            host_site = re.search(r'\\.([^.]+\\.[^.]{2,3})$', host)\n            if host_site is not None:\n                host_site = host_site.group(1)\n                countrycode = self.gi.country_code_by_name(host_site)\n    if countrycode is None:\n        # maybe there's an ip address encoded in the hostname\n        ip_guess = re.search(r'^([0-9]{1,3}).([0-9]{1,3}).([0-9]{1,3}).([0-9]{1,3}).', host)\n        if ip_guess is not None:\n            host =   ip_guess.group(1) + \".\" + ip_guess.group(2) + \".\" \\\n                   + ip_guess.group(3) + \".\" + ip_guess.group(4)\n            countrycode = self.gi.country_code_by_addr(host)\nif countrycode is None:\n    # Give up\n    countrycode = \"N/A\"\nreturn {'name':self.countryname_from_code(countrycode), 'code': countrycode, 'icon': self.icon_from_code(countrycode)}", "path": "modules\\GeoIPWrapper.py", "repo_name": "goerz/pyala", "stars": 1, "license": "None", "language": "python", "size": 1612}
{"docstring": "\"\"\" Check if a visit is the first of the day, and mark as anchor position if so \"\"\"\n", "func_signal": "def _test_and_set_anchor(self, index):\n", "code": "visit = self._visits[index]\nvisit_time = ApacheLogParser.extract_from_date(visit.begin_time, visit.pages[0].serveroffset)\nyear = visit_time.tm_year\nmonth = visit_time.tm_mon\nday = visit_time.tm_mday\nday_of_year = visit_time.tm_yday\nprev_visit_day_of_year = day_of_year - 1\nif index > 0:\n    prev_visit_day_of_year = \\\n        ApacheLogParser.extract_from_date(self._visits[index-1].begin_time).tm_yday\nif day_of_year > prev_visit_day_of_year: # there should be an anchor\n    uri_filename = self._outfilename.replace( \".html\", \\\n           str(self._number_of_pages+1).zfill(6) +\".html\" )\n    uri = uri_filename + \"#a\" + str(visit.number)\n    if not self._anchors.has_key(year):\n        self._anchors[year] = {}\n    if not self._anchors[year].has_key(month):\n        self._anchors[year][month] = {}\n    if not self._anchors[year][month].has_key(day):\n        self._anchors[year][month][day] = uri\n        self._anchor_ids.append(visit.number)", "path": "modules\\DetailedStatComponent.py", "repo_name": "goerz/pyala", "stars": 1, "license": "None", "language": "python", "size": 1612}
{"docstring": "\"\"\" Get icon from two-letter country code\"\"\"\n", "func_signal": "def icon_from_code(self, code):\n", "code": "if (code == \"A1\") or (code == \"A2\"):\n    return \"ext_com.png\"\nif code == \"N/A\":\n    return \"ext_unknown.png\"\nreturn \"ext_\" + code.lower() + \".png\"", "path": "modules\\GeoIPWrapper.py", "repo_name": "goerz/pyala", "stars": 1, "license": "None", "language": "python", "size": 1612}
{"docstring": "\"\"\" Return the next line in the block \"\"\"\n", "func_signal": "def readline(self):\n", "code": "try:\n    self.i += 1\n    return self.lines[self.i-1]\nexcept IndexError:\n    return None", "path": "modules\\TemplateProcessor.py", "repo_name": "goerz/pyala", "stars": 1, "license": "None", "language": "python", "size": 1612}
{"docstring": "\"\"\" print a warning message to stderr \"\"\"\n", "func_signal": "def warn(msg):\n", "code": "sys.stderr.write(str(msg))\nsys.stderr.write(\"\\n\")", "path": "modules\\DetailedStatComponent.py", "repo_name": "goerz/pyala", "stars": 1, "license": "None", "language": "python", "size": 1612}
{"docstring": "\"\"\" print a warning message to stderr \"\"\"\n", "func_signal": "def warn(msg):\n", "code": "sys.stderr.write(str(msg))\nsys.stderr.write(\"\\n\")", "path": "modules\\OptionManager.py", "repo_name": "goerz/pyala", "stars": 1, "license": "None", "language": "python", "size": 1612}
{"docstring": "''' Return basic ratings information '''\n", "func_signal": "def host_ratings(self):\n", "code": "host_ratings = select([Host.system, Host.vendor, Host.rating,\n  func.count(Host.rating)], Host.rating != 0).group_by(Host.system).\\\n  order_by(desc(func.count(Host.rating))).limit(50).\\\n  execute().fetchall()", "path": "smoon\\hardware\\controllers\\reports.py", "repo_name": "cgoncalves/smolt-cgoncalves", "stars": 1, "license": "None", "language": "python", "size": 2208}
{"docstring": "''' Return basic ratings information '''\n", "func_signal": "def device_ratings(self):\n", "code": "h = select([HostLink.device_id, HostLink.rating,\n    func.count(HostLink.rating).label('cnt')], HostLink.rating != 0).\\\n    group_by(HostLink.device_id, HostLink.rating).\\\n    order_by(desc('cnt')).limit(500).alias('h')\ndevice_ratings = select([ComputerLogicalDevice.description, h.c.rating, h.c.cnt], ComputerLogicalDevice.id==h.c.device_id).execute().fetchall()\nreturn dict(device_ratings=device_ratings)", "path": "smoon\\hardware\\controllers\\reports.py", "repo_name": "cgoncalves/smolt-cgoncalves", "stars": 1, "license": "None", "language": "python", "size": 2208}
{"docstring": "\"\"\"set the maximum number of stars\"\"\"\n\n", "func_signal": "def set_max_value(self, max_value):\n", "code": "if (self.max_stars != max_value):\n\t\"\"\"Save the old max incase it is less than the\n\tcurrent number of stars, in which case we will\n\thave to redraw\"\"\"\n\t\n\tif (max_value > 0):\n\t\tself.max_stars = max_value\n\t\t#reinit the sizes list (should really be a separate function)\n\t\tself.sizes = []\t\t\n\t\tfor count in range(0,self.max_stars):\n\t\t\tself.sizes.append((count * PIXMAP_SIZE) + BORDER_WIDTH)\n\t\t\"\"\"do we have to change the current number of\n\t\tstars?\"\"\"\t\t\t\n\t\tif (self.stars > self.max_stars):\n\t\t\tself.set_value(self.max_stars)", "path": "client\\starhscale.py", "repo_name": "cgoncalves/smolt-cgoncalves", "stars": 1, "license": "None", "language": "python", "size": 2208}
{"docstring": "\"\"\"From Widget.py: The do_size_request method Gtk+ is calling\n on a widget to ask it the widget how large it wishes to be. \n It's not guaranteed that gtk+ will actually give this size \n to the widget.  So we will send gtk+ the size needed for\n the maximum amount of stars\"\"\"\n\n", "func_signal": "def do_size_request(self, requisition):\n", "code": "requisition.height = PIXMAP_SIZE\nrequisition.width = (PIXMAP_SIZE * self.max_stars) + (BORDER_WIDTH * 2)", "path": "client\\starhscale.py", "repo_name": "cgoncalves/smolt-cgoncalves", "stars": 1, "license": "None", "language": "python", "size": 2208}
{"docstring": "#        return self.vendors[vend].devices[dev].name\n", "func_signal": "def subdevice(self, vend, dev, subvend, subdevice, alt='N/A', bus='pci'):\n", "code": "        try:\n            vend = int(vend)\n        except:\n            pass\n        if vend == 0:\n            return alt\n        try:\n            dev = int(dev)\n        except:\n            pass\n        try:\n            subvend = int(subvend)\n        except:\n            pass\n        try:\n            subdevice = int(subdevice)\n        except:\n            pass\n        try:\n            var = self.vendors[bus][vend].devices[dev].subvendors[subvend].devices[subdevice].name\n            return var\n        except:\n            try:\n                self.vendors[bus][vend].devices[dev].name\n            except:\n                return alt", "path": "smoon\\hardware\\hwdata.py", "repo_name": "cgoncalves/smolt-cgoncalves", "stars": 1, "license": "None", "language": "python", "size": 2208}
{"docstring": "'''Generates a counted view on a single column'''\n", "func_signal": "def simple_counted_view(name, column, desc=False, label=None, distinct=False):\n", "code": "if label:\n    column = column.label(label)\nreturn counted_view(name, [column], column, desc=desc, distinct=distinct)", "path": "smoon\\hardware\\model\\views.py", "repo_name": "cgoncalves/smolt-cgoncalves", "stars": 1, "license": "None", "language": "python", "size": 2208}
{"docstring": "\"\"\"Simple singleton wrapper with lazy initialization\"\"\"\n", "func_signal": "def UuidDb():\n", "code": "global _uuid_db_instance\nif _uuid_db_instance == None:\n    _uuid_db_instance = _UuidDb(os.path.expanduser('~/.smolt/uuiddb.cfg'))\nreturn _uuid_db_instance", "path": "client\\uuiddb.py", "repo_name": "cgoncalves/smolt-cgoncalves", "stars": 1, "license": "None", "language": "python", "size": 2208}
{"docstring": "\"\"\"The button press event virtual method\"\"\"\n\n# make sure it was the first button\n", "func_signal": "def do_button_press_event(self, event):\n", "code": "if event.button == 1:\n\t#check for new stars\n\tself.check_for_new_stars(event.x)\t\t\t\nreturn True", "path": "client\\starhscale.py", "repo_name": "cgoncalves/smolt-cgoncalves", "stars": 1, "license": "None", "language": "python", "size": 2208}
{"docstring": "\"\"\"The do_size_allocate is called by when the actual \nsize is known and the widget is told how much space \ncould actually be allocated Save the allocated space\nself.allocation = allocation. The following code is\nidentical to the widget.py example\"\"\"\n\t\n", "func_signal": "def do_size_allocate(self, allocation):\n", "code": "if self.flags() & gtk.REALIZED:\n\tself.window.move_resize(*allocation)", "path": "client\\starhscale.py", "repo_name": "cgoncalves/smolt-cgoncalves", "stars": 1, "license": "None", "language": "python", "size": 2208}
{"docstring": "''' Shows recently added hosts and devices '''\n", "func_signal": "def recent(self):\n", "code": "recent_pub_uuid = select([Host.pub_uuid, Host.last_modified],\n      Host.last_modified > (date.today() - timedelta(days=90)))\\\n      .order_by(desc(Host.last_modified)).limit(20).execute()\\\n      .fetchall()\nrecent_devices = select([computer_logical_devices.c.description,\n                  computer_logical_devices.c.date_added],\n                  computer_logical_devices.c.date_added > (date.today() -\n                  timedelta(days=90))).order_by(desc(computer_logical_devices\\\n                  .c.date_added)).limit(50).execute().fetchall()\nreturn dict(recent_pub_uuid=recent_pub_uuid,\n            recent_devices=recent_devices)", "path": "smoon\\hardware\\controllers\\reports.py", "repo_name": "cgoncalves/smolt-cgoncalves", "stars": 1, "license": "None", "language": "python", "size": 2208}
{"docstring": "\"\"\"This is where the widget must draw itself.\"\"\"\n\n#Draw the correct number of stars.  Each time you draw another star\n#move over by 22 pixels. which is the size of the star.\n", "func_signal": "def do_expose_event(self, event):\n", "code": "for count in range(0,self.max_stars):\n\tif count < self.stars:\n\t\tself.window.draw_drawable(self.gc, self.star_pixmap, 0, 0\n\t\t\t\t\t  , self.sizes[count] \n\t\t\t\t\t  , 0,-1, -1)\n\telse:\n\t\tself.window.draw_drawable(self.gc, self.bg_pixmap, 0, 0\n\t\t\t\t\t  , self.sizes[count] \n\t\t\t\t\t  , 0,-1, -1)", "path": "client\\starhscale.py", "repo_name": "cgoncalves/smolt-cgoncalves", "stars": 1, "license": "None", "language": "python", "size": 2208}
{"docstring": "\"\"\"Initialization, max_stars is the total number\nof stars that may be visible, and stars is the current\nnumber of stars to draw\"\"\"\n\n#Initialize the Widget\n#gtk.Widget.__init__(self)\n", "func_signal": "def __init__(self, max_stars=5, stars=0):\n", "code": "gtk.GenericCellRenderer.__init__(self)\n\nself.max_stars = max_stars\nself.stars = stars\n\n# Init the list to blank\nself.sizes = []\t\t\nfor count in range(0,self.max_stars):\n\tself.sizes.append((count * PIXMAP_SIZE) + BORDER_WIDTH)", "path": "client\\starhscale.py", "repo_name": "cgoncalves/smolt-cgoncalves", "stars": 1, "license": "None", "language": "python", "size": 2208}
{"docstring": "# if this is a hint, then let's get all the necessary \n# information, if not it's all we need.\n", "func_signal": "def motion_notify_event(self, widget, event):\n", "code": "if event.is_hint:\n\tx, y, state = event.window.get_pointer()\nelse:\n\tx = event.x\n\ty = event.y\n\tstate = event.state\n\nif (state & gtk.gdk.BUTTON1_MASK):\n\t# loop through the sizes and see if the\n\t# number of stars should change\n\tself.check_for_new_stars(event.x)", "path": "client\\starhscale.py", "repo_name": "cgoncalves/smolt-cgoncalves", "stars": 1, "license": "None", "language": "python", "size": 2208}
{"docstring": "\"\"\"finds the the withheld special entry,\n    fixes it's label, and moves it to the end\"\"\"\n", "func_signal": "def handle_withheld_elem(list, attrib_to_check, value_to_check_for):\n", "code": "condition = lambda x: getattr(x, attrib_to_check) == value_to_check_for\ndef modify(x):\n    setattr(x, attrib_to_check, withheld_label)\n    return x\nother_list = [e for e in list if not condition(e)]\nwithheld_list = [modify(e) for e in list if condition(e)]\nreturn other_list + withheld_list", "path": "smoon\\render-stat.py", "repo_name": "cgoncalves/smolt-cgoncalves", "stars": 1, "license": "None", "language": "python", "size": 2208}
{"docstring": "\"\"\"This function will determine how many stars\nwill be show based on an x coordinate. If the\nnumber of stars changes the widget will be invalidated\nand the new number drawn\"\"\"\n\n# loop through the sizes and see if the\n# number of stars should change\n", "func_signal": "def check_for_new_stars(self, xPos):\n", "code": "new_stars = 0\nfor size in self.sizes:\n\tif (xPos < size):\n\t\t# we've reached the star number\n\t\tbreak\n\tnew_stars = new_stars + 1\n\t\n# set the new value\nself.set_value(new_stars)", "path": "client\\starhscale.py", "repo_name": "cgoncalves/smolt-cgoncalves", "stars": 1, "license": "None", "language": "python", "size": 2208}
{"docstring": "\"\"\"Initialization, max_stars is the total number\nof stars that may be visible, and stars is the current\nnumber of stars to draw\"\"\"\n\n#Initialize the Widget\n", "func_signal": "def __init__(self, max_stars=5, stars=0):\n", "code": "gtk.Widget.__init__(self)\n\nself.max_stars = max_stars\nself.stars = stars\n\n# Init the list to blank\nself.sizes = []\t\t\nfor count in range(0,self.max_stars):\n\tself.sizes.append((count * PIXMAP_SIZE) + BORDER_WIDTH)", "path": "client\\starhscale.py", "repo_name": "cgoncalves/smolt-cgoncalves", "stars": 1, "license": "None", "language": "python", "size": 2208}
{"docstring": "\"\"\"Produces final output form from the data returned from a\ncontroller method.\n\n@param tg_format: format of desired output (html or json)\n@param output: the output returned by the controller\n@param template: HTML template to use\n\"\"\"\n", "func_signal": "def _process_output(output, template, format):\n", "code": "if isinstance(output, dict):\n    from turbogears.widgets import js_location\n\n    css = tg_util.setlike()\n    js = dict(izip(js_location, iter(tg_util.setlike, None)))\n    include_widgets = {}\n    include_widgets_lst = config.get(\"tg.include_widgets\", [])\n\n    if config.get(\"tg.mochikit_all\", False):\n        include_widgets_lst.insert(0, 'turbogears.mochikit')\n\n    for i in include_widgets_lst:\n        widget = tg_util.load_class(i)\n        if isclass(widget):\n            widget = widget()\n        include_widgets[\"tg_%s\" % i.split(\".\")[-1]] = widget\n        for script in widget.retrieve_javascript():\n            if hasattr(script, \"location\"):\n                js[script.location].add(script)\n            else:\n                js[js_location.head].add(script)\n        css.add_all(widget.retrieve_css())\n\n    for value in output.itervalues():\n        if hasattr(value, \"retrieve_css\"):\n            retrieve = getattr(value, \"retrieve_css\")\n            if callable(retrieve):\n                css.add_all(value.retrieve_css())\n        if hasattr(value, \"retrieve_javascript\"):\n            retrieve = getattr(value, \"retrieve_javascript\")\n            if callable(retrieve):\n                for script in value.retrieve_javascript():\n                    if hasattr(script, \"location\"):\n                        js[script.location].add(script)\n                    else:\n                        js[js_location.head].add(script)\n    output.update(include_widgets)\n    output[\"tg_css\"] = css\n    #output.update([(\"tg_js_%s\" % str(l), js[l]) for l in js_location])\n    for l in iter(js_location):\n        output[\"tg_js_%s\" % str(l)] = js[l]\n\n    output[\"tg_flash\"] = output.get(\"tg_flash\")\n\n    return engine.render(output, format=format, template=template)", "path": "smoon\\render-stat.py", "repo_name": "cgoncalves/smolt-cgoncalves", "stars": 1, "license": "None", "language": "python", "size": 2208}
{"docstring": "\"\"\"Simple singleton wrapper with lazy initialization\"\"\"\n", "func_signal": "def Gate():\n", "code": "global _gate\nif _gate == None:\n    _gate = _Gate(None)\nreturn _gate", "path": "client\\gate.py", "repo_name": "cgoncalves/smolt-cgoncalves", "stars": 1, "license": "None", "language": "python", "size": 2208}
{"docstring": "\"\"\"Sets the current number of stars that will be \ndrawn.  If the number is different then the current\nnumber the widget will be redrawn\"\"\"\n\n", "func_signal": "def set_value(self, value):\n", "code": "if (value >= 0):\n\tif (self.stars != value):\n\t\tself.stars = value\n\t\t#check for the maximum\n\t\tif (self.stars > self.max_stars):\n\t\t\tself.stars = self.max_stars\n\t\t\n\t\t# redraw the widget\n\t\tself.queue_resize()\n\t\tself.window.move_resize(*self.allocation)", "path": "client\\starhscale.py", "repo_name": "cgoncalves/smolt-cgoncalves", "stars": 1, "license": "None", "language": "python", "size": 2208}
{"docstring": "\"\"\"Simple singleton wrapper with lazy initialization\"\"\"\n", "func_signal": "def GateFromConfig(the_only_config_file):\n", "code": "global _gate\nif _gate == None:\n    _gate = _Gate(the_only_config_file)\nreturn _gate", "path": "client\\gate.py", "repo_name": "cgoncalves/smolt-cgoncalves", "stars": 1, "license": "None", "language": "python", "size": 2208}
{"docstring": "\"\"\" Evaluate all individuals in population, calls the evaluate() method of individuals\n   \n:param args: this params are passed to the evaluation function\n\n\"\"\"\n", "func_signal": "def evaluate(self, **args):\n", "code": "for ind in self.internalPop:\n   ind.evaluate(**args)\nself.__clear_flags()", "path": "src\\pyevolve\\GPopulation.py", "repo_name": "alx21/silevolve", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" Set an individual of population \"\"\"\n", "func_signal": "def __setitem__(self, key, value):\n", "code": "self.internalPop[key] = value\nself.__clear_flags()", "path": "src\\pyevolve\\GPopulation.py", "repo_name": "alx21/silevolve", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" Sort the population \"\"\"\n", "func_signal": "def sort(self):\n", "code": "if self.sorted: return\nrev = (self.minimax == Consts.minimaxType[\"maximize\"])\n\nif self.sortType == Consts.sortType[\"raw\"]:\n   self.internalPop.sort(cmp=cmp_individual_raw, reverse=rev)\nelse:\n   self.scale()\n   self.internalPop.sort(cmp=cmp_individual_scaled, reverse=rev)\n\nself.sorted = True", "path": "src\\pyevolve\\GPopulation.py", "repo_name": "alx21/silevolve", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" Compares two individual fitness scores, used for sorting population\n\nExample:\n   >>> GPopulation.cmp_individual_scaled(a, b)\n\n:param a: the A individual instance\n:param b: the B individual instance\n:rtype: 0 if the two individuals fitness score are the same,\n        -1 if the B individual fitness score is greater than A and\n        1 if the A individual fitness score is greater than B.\n\n.. note:: this function is used to sorte the population individuals\n\n\"\"\"\n", "func_signal": "def cmp_individual_scaled(a, b):\n", "code": "if a.fitness < b.fitness: return -1\nif a.fitness > b.fitness: return 1\nreturn 0", "path": "src\\pyevolve\\GPopulation.py", "repo_name": "alx21/silevolve", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" Return the best raw score individual of population\n\n:rtype: the individual\n\n\"\"\"\n", "func_signal": "def bestRaw(self):\n", "code": "if self.minimax == Consts.minimaxType[\"minimize\"]:\n   return min(self, key=key_raw_score)\nelse:\n   return max(self, key=key_raw_score)", "path": "src\\pyevolve\\GPopulation.py", "repo_name": "alx21/silevolve", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "# number of mutations! will be at least one.\n", "func_signal": "def mutateGene(self):\n", "code": "for i in range(0, int(round(math.ceil(self.pmut * self.length)))):\n    # this can be improved by making it an iterable generator function\n    self.mutateNode(self.pickRandomNode())\nreturn i + 1", "path": "src\\genetree.py", "repo_name": "alx21/silevolve", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" Do statistical analysis of population and set 'statted' to True \"\"\"\n", "func_signal": "def statistics(self):\n", "code": "if self.statted: return\nlogging.debug(\"Running statistical calc.\")\nraw_sum = 0\n\nlen_pop = len(self)\nfor ind in xrange(len_pop):\n   raw_sum += self[ind].score\n\nself.stats[\"rawMax\"] = max(self, key=key_raw_score).score\nself.stats[\"rawMin\"] = min(self, key=key_raw_score).score\nself.stats[\"rawAve\"] = raw_sum / float(len_pop)\n\ntmpvar = 0.0;\nfor ind in xrange(len_pop):\n   s = self[ind].score - self.stats[\"rawAve\"]\n   s*= s\n   tmpvar += s\n\ntmpvar/= float((len(self) - 1))\nself.stats[\"rawDev\"] = math_sqrt(tmpvar)\nself.stats[\"rawVar\"] = tmpvar\n\nself.statted = True", "path": "src\\pyevolve\\GPopulation.py", "repo_name": "alx21/silevolve", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" The GPopulation Class creator \"\"\"\n\n", "func_signal": "def __init__(self, genome):\n", "code": "logging.debug(\"New population instance, %s class genomes.\", genome.__class__.__name__)\nself.oneSelfGenome = genome\nself.internalPop   = []\nself.popSize       = 0\nself.sortType      = Consts.CDefPopSortType\nself.sorted        = False\nself.minimax       = Consts.CDefPopMinimax\nself.scaleMethod   = FunctionSlot(\"Scale Method\")\nself.scaleMethod.set(Consts.CDefPopScale)\nself.allSlots      = [self.scaleMethod]\n\n# Statistics\nself.statted = False\nself.stats   = Statistics()", "path": "src\\pyevolve\\GPopulation.py", "repo_name": "alx21/silevolve", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" Return a brand-new cloned population \"\"\"\n", "func_signal": "def clone(self):\n", "code": "newpop = GPopulation(self.oneSelfGenome.clone())\nself.copy(newpop)\nreturn newpop", "path": "src\\pyevolve\\GPopulation.py", "repo_name": "alx21/silevolve", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" Return a Statistics class for statistics\n\n:rtype: the :class:`Statistics.Statistics` instance\n\n\"\"\"\n", "func_signal": "def getStatistics(self):\n", "code": "self.statistics()\nreturn self.stats", "path": "src\\pyevolve\\GPopulation.py", "repo_name": "alx21/silevolve", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" Remove all individuals from population \"\"\"\n", "func_signal": "def clear(self):\n", "code": "del self.internalPop[:]\nself.__clear_flags()", "path": "src\\pyevolve\\GPopulation.py", "repo_name": "alx21/silevolve", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "# remove one at random! this could destroy great swaths of logic! Oh well! That is evolution at work!\n", "func_signal": "def removeArg(self, node):\n", "code": "arg_index = random.randint(1, len(node)-1)\nself.length -= self.getLength(node[arg_index])\nnode.remove(node[arg_index])", "path": "src\\genetree.py", "repo_name": "alx21/silevolve", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" Compares two individual raw scores\n\nExample:\n   >>> GPopulation.cmp_individual_raw(a, b)\n\n:param a: the A individual instance\n:param b: the B individual instance\n:rtype: 0 if the two individuals raw score are the same,\n        -1 if the B individual raw score is greater than A and\n        1 if the A individual raw score is greater than B.\n\n.. note:: this function is used to sorte the population individuals\n\n\"\"\"\n", "func_signal": "def cmp_individual_raw(a, b):\n", "code": "if a.score < b.score: return -1\nif a.score > b.score: return 1\nreturn 0", "path": "src\\pyevolve\\GPopulation.py", "repo_name": "alx21/silevolve", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" Clone the example genome to fill the population \"\"\"\n", "func_signal": "def create(self, **args):\n", "code": "self.clear()\nself.minimax = args[\"minimax\"]\nfor i in xrange(self.popSize):\n   self.internalPop.append(self.oneSelfGenome.clone())\nself.__clear_flags()", "path": "src\\pyevolve\\GPopulation.py", "repo_name": "alx21/silevolve", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" Scale the population using the scaling method\n\n:param args: this parameter is passed to the scale method\n\n\"\"\"\n", "func_signal": "def scale(self, **args):\n", "code": "for it in self.scaleMethod.applyFunctions(self, **args):\n   pass\n\nfit_sum = 0\nfor ind in xrange(len(self)):\n   fit_sum += self[ind].fitness\n\nself.stats[\"fitMax\"] = max(self, key=key_fitness_score).fitness\nself.stats[\"fitMin\"] = min(self, key=key_fitness_score).fitness\nself.stats[\"fitAve\"] = fit_sum / float(len(self))\n\nself.sorted = False", "path": "src\\pyevolve\\GPopulation.py", "repo_name": "alx21/silevolve", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" Returns the string representation of the population \"\"\"\n", "func_signal": "def __repr__(self):\n", "code": "ret =  \"- GPopulation\\n\"\nret += \"\\tPopulation Size:\\t %d\\n\" % (self.popSize,)\nret += \"\\tSort Type:\\t\\t %s\\n\" % (Consts.sortType.keys()[Consts.sortType.values().index(self.sortType)].capitalize(),)\nret += \"\\tMinimax Type:\\t\\t %s\\n\" % (Consts.minimaxType.keys()[Consts.minimaxType.values().index(self.minimax)].capitalize(),)\nfor slot in self.allSlots:\n   ret+= \"\\t\" + slot.__repr__()\nret+=\"\\n\"\nret+= self.stats.__repr__()\nreturn ret", "path": "src\\pyevolve\\GPopulation.py", "repo_name": "alx21/silevolve", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" Copy current population to 'pop'\n\n:param pop: the destination population\n\n.. warning:: this method do not copy the individuals, only the population logic\n\n\"\"\"\n", "func_signal": "def copy(self, pop):\n", "code": "pop.popSize = self.popSize\npop.sortType = self.sortType\npop.sorted = self.sorted\npop.statted = self.statted\npop.minimax = self.minimax\npop.scaleMethod = self.scaleMethod", "path": "src\\pyevolve\\GPopulation.py", "repo_name": "alx21/silevolve", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" Return the best scaled fitness individual of population\n\n:param index: the *index* best individual\n:rtype: the individual\n\n\"\"\"\n", "func_signal": "def bestFitness(self, index=0):\n", "code": "self.sort()\nreturn self.internalPop[index]", "path": "src\\pyevolve\\GPopulation.py", "repo_name": "alx21/silevolve", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" Initialize all individuals of population,\nthis calls the initialize() of individuals \"\"\"\n", "func_signal": "def initialize(self):\n", "code": "for gen in self.internalPop:\n   gen.initialize()\nself.__clear_flags()", "path": "src\\pyevolve\\GPopulation.py", "repo_name": "alx21/silevolve", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "# add one at random! Hmm, should we just add ints / data, or should we continue down the logic tree?\n# Lets try ints / data for now\n# this code sort of sucks as it relies on int / data inserts being 0 and 1 in the func_list. I don't care to revise it right now.\n# I can't think of a good way to fix it without slowing it down, and it needs to be fast! (Go python go!)\n", "func_signal": "def addArg(self, node):\n", "code": "if random.randint(0,1):\n    new_node = self.createIntNode()\n    self.length += 1\nelse:\n    new_node = self.createDataNode()\n    self.length += 2\n# if we want to continue down the tree; we could add a chance of performing 'mutateNode' on the new node here\nnode.insert(random.randint(1,len(node)),new_node)", "path": "src\\genetree.py", "repo_name": "alx21/silevolve", "stars": 1, "license": "None", "language": "python", "size": 176}
{"docstring": "''' Calculates mutual info and transfer entropy between:\n    - Each pair of neurons (in each direction)\n    - The input signal and each neuron\n    The neuron signal analyzed is the log-Poisson intensity'''\n", "func_signal": "def information_analysis(data1, data2=None, bins=16, lag=2, sp=16):\n", "code": "if data2==None: data2 = data1\nMI = do_pairwise(mutual_information, data1, data2, bins=bins)\nTE = do_pairwise(transfer_entropy_pdf, data1, data2, bins=bins, lag=lag, sp=sp)\nreturn {'MI':MI, 'TE':TE}", "path": "information.py", "repo_name": "caffeine-xx/pyglpp", "stars": 1, "license": "None", "language": "python", "size": 31880}
{"docstring": "''' D_1<-2 '''\n", "func_signal": "def transfer_entropy(ts1,ts2,lag=2,bins=5):\n", "code": "ts1,lts1  = multi_lag(ts1,lag)\nts2,lts2  = multi_lag(ts2,lag)\n\n# P(i_n+1, i_(n), j_(n))\njoint = np.histogramdd([ts1]+lts1+lts2, bins=bins)[0]\njoint = normalize(joint)\n# P(i_n+1, i_(n))\nauto = np.histogramdd([ts1]+lts1, bins=bins)[0]\nauto = normalize(auto)\n# P(i_(n))\nlag1 = np.histogramdd(lts1,bins=bins)[0]\nlag1 = normalize(lag1)\n# P(i_(n), j_(n))\nlag12 = np.histogramdd(lts1+lts2, bins=bins)[0]\nlag12 = normalize(lag12)\n# P(i_n+1 | i_(n), j_(n))\njcond = np.divide(joint.T , lag12.T).T\njcond = clean(jcond) \njcond = do_cpdf(jcond.T, avg_zeros).T\n# P(i_n+1 | i_(n))\nacond = np.divide(auto.T, lag1.T).T\nacond = clean(acond)\nacond = do_cpdf(acond.T, avg_zeros).T\n# E[log P(i_n+1 | i_(n), j_(n)) / P(i_n+1 | i_(n))] \ntransfer = joint * clean(np.log(np.divide(jcond , acond)))\nreturn transfer.sum()", "path": "information.py", "repo_name": "caffeine-xx/pyglpp", "stars": 1, "license": "None", "language": "python", "size": 31880}
{"docstring": "''' Uses the Signals api.  Parameters:\n      - trial: Time dimension of experiment\n      - in_signal: Sparse or dense signal of inputs\n      - out_signal: Sparse or dense signal of outputs (spikes) '''\n", "func_signal": "def set_data(self, trial, in_signal, out_signal):\n", "code": "MultiNeuron.set_data(self, \n  trial.dt, trial.length(),\n  in_signal.signal, out_signal.sparse_bins())", "path": "inference.py", "repo_name": "caffeine-xx/pyglpp", "stars": 1, "license": "None", "language": "python", "size": 31880}
{"docstring": "''' Marginalizes along multiple axes'''\n", "func_signal": "def multi_marginalize(x, axes=[0]):\n", "code": "result = x\nax = axes\nax.reverse()\nfor a in ax:\n  result = marginalize(result, a)\nreturn result", "path": "information.py", "repo_name": "caffeine-xx/pyglpp", "stars": 1, "license": "None", "language": "python", "size": 31880}
{"docstring": "''' For N neurons, results 0-N are lambda monitors, and \n    N+1-2N are inferred intensities '''\n", "func_signal": "def lnp_result_info(result, bins=64, lag=1, sp=8):\n", "code": "return {'Simulator':information_analysis(result.monitors['lambda']), \n        'Inferred':information_analysis(result.intensity, bins=bins, lag=lag, sp=sp),\n        'Sim->Inf':information_analysis(result.intensity,result.monitors['lambda'], bins=bins, lag=lag, sp=sp)}", "path": "information.py", "repo_name": "caffeine-xx/pyglpp", "stars": 1, "license": "None", "language": "python", "size": 31880}
{"docstring": "''' Connector with probability related to inverted square distance of neurons\n    (where 'distance' is a made-up concept based on ID numbers).'''\n", "func_signal": "def taur_connector(P, Q, la=2.0):\n", "code": "W = zeros((P,Q))\nfor i in xrange(P):\n  for j in xrange(Q):\n    if not (i==j): W[i,j] = rand() < taur_dist(i,j,la)\nreturn W", "path": "trials.py", "repo_name": "caffeine-xx/pyglpp", "stars": 1, "license": "None", "language": "python", "size": 31880}
{"docstring": "\"\"\" Private method: recalculates basis versions of spike  \n    between start and end times, using prev. history if\n    available  \"\"\"\n# TODO: Wildly inefficient, needs to be incremental.\n", "func_signal": "def _rebase(self, start, end):\n", "code": "t0 = max(0,start-self.tau.size)\nt1 = end\nspikes = self.spikes[:,t0:t1]\nbspikes = run_bases(self.sp_bas, spikes)\nself.b_spikes[:,t0:t1,:] = bspikes", "path": "stimulus.py", "repo_name": "caffeine-xx/pyglpp", "stars": 1, "license": "None", "language": "python", "size": 31880}
{"docstring": "''' Calulates mutual information for two\n    vectors.  Parameters: vectors data1 & data2\n    Returns: mutual information (in nats) '''\n", "func_signal": "def mutual_information(data1, data2, bins=64):\n", "code": "pdi = pdf_1d(data1,bins=bins)[0]\npdj = pdf_1d(data2,bins=bins)[0]\npdc = pdf_nd([data1,data2],bins=bins)[0]\nreturn pdi._entropy()+pdj._entropy()-pdc._entropy()", "path": "information.py", "repo_name": "caffeine-xx/pyglpp", "stars": 1, "license": "None", "language": "python", "size": 31880}
{"docstring": "''' Takes a conditional PDF in which the last index is the random var,\n    and subsequent indices are conditional vars.  If all entries\n    are zero, turns into a uniform distribution'''\n", "func_signal": "def avg_zeros(pdf):\n", "code": "if pdf.sum() == 0:\n  pdf[:] = 1./len(pdf)\nreturn pdf", "path": "information.py", "repo_name": "caffeine-xx/pyglpp", "stars": 1, "license": "None", "language": "python", "size": 31880}
{"docstring": "''' Utility method - converts sparse representations of\n    spike trains to full representation '''\n", "func_signal": "def sparse_to_spikes(self, sparse):\n", "code": "for i,s in enumerate(sparse):\n  self.spikes[i,s]=1", "path": "inference.py", "repo_name": "caffeine-xx/pyglpp", "stars": 1, "license": "None", "language": "python", "size": 31880}
{"docstring": "\"\"\" Adds zeros as necessary \"\"\"\n", "func_signal": "def _grow(self,t):\n", "code": "while (self.I.shape[1] < t):\n  self.I = np.hstack((self.I, np.zeros((self.I.shape[0], self.T))))\nwhile (self.spikes.shape[1] < t):\n  self.spikes = np.hstack((self.spikes, np.zeros((self.spikes.shape[0], self.T))))\nwhile (self.stim.shape[1] < t):\n  self.stim = np.hstack((self.stim, np.zeros((self.stim.shape[0], self.T))))", "path": "stimulus.py", "repo_name": "caffeine-xx/pyglpp", "stars": 1, "license": "None", "language": "python", "size": 31880}
{"docstring": "''' Performs ML inference on the given simulation result,\nand returns the resulting parameters '''\n\n", "func_signal": "def ml_glpp_parameters(result, model=False):\n", "code": "if not model: model = inf.MultiNeuron()\n\nmodel.set_data(result.trial.dt, result.trial.length(),\n               result.stimulus.signal, result.spike_trains.sparse_bins())\n\nK,H,Mu    = model.max_likelihood()\nintensity = model.logI(K,H,Mu)\n\nparameters = {'K':K, 'H':H, 'Mu':Mu, 'intensity':intensity,\n  'stim_basis':model.stim_basis,'spike_basis':model.spike_basis}\n\nreturn parameters", "path": "analyze.py", "repo_name": "caffeine-xx/pyglpp", "stars": 1, "license": "None", "language": "python", "size": 31880}
{"docstring": "# dimensions\n", "func_signal": "def set_data(self, timestep, duration, stims, sparse):\n", "code": "self.delta   = timestep\nself.T       = duration\nself.N       = len(sparse)\nself.Nx      = stims.shape[0]\n# data\nself.sparse  = sparse\nself.spikes  = np.zeros((self.N,self.T),dtype='float64')\nself.stims   = stims\nself.sparse_to_spikes(sparse)\n# basis\nself.base_spikes = run_bases(self.spike_basis, self.spikes)\nself.base_stims  = run_bases(self.stim_basis, self.stims)", "path": "inference.py", "repo_name": "caffeine-xx/pyglpp", "stars": 1, "license": "None", "language": "python", "size": 31880}
{"docstring": "\"\"\" Private method for simulating forward in time the intensities \"\"\"\n", "func_signal": "def _forward(self,t):\n", "code": "self._grow(t)\nwhile self.T < t:\n  for i in range(0,self.N):\n    for j in range(0,self.Nx):\n      self.I[i,self.T] += np.sum(self.K[i,j,:] * self.b_stims[j,self.T,:])\n    for j in range(0,self.N):\n      self.I[i,self.T] += np.sum(self.H[i,j,:] * self.b_spikes[j,self.T,:])\n    self.I[i,self.T] += self.Mu[i]\n    self.I[i,self.T] = np.ma.exp(self.I[i,self.T])\n    self.spikes[i,self.T] = self._poisson_spike(self.I[i,self.T])\n  self._rebase(self.T,self.T+1)\n  self.T += 1", "path": "stimulus.py", "repo_name": "caffeine-xx/pyglpp", "stars": 1, "license": "None", "language": "python", "size": 31880}
{"docstring": "\"\"\" Run simulation until time T and return the whole spike train\"\"\"\n", "func_signal": "def simulate(self, K, H, Mu, t):\n", "code": "self.K, self.H, self.Mu = K, H, Mu\nself._forward(t)\nreturn self.spikes", "path": "stimulus.py", "repo_name": "caffeine-xx/pyglpp", "stars": 1, "license": "None", "language": "python", "size": 31880}
{"docstring": "''' Map the vector vec through fun, but delayed:\n      out_t = fun(vec_{t-delay}) \n      Size is out.size-delay\n    Returns (resized vec, out) '''\n", "func_signal": "def delay_fn(fun, vec, delay=1):\n", "code": "delayed = roll(vec, -1*delay)[:-delay]\nvecfun = vectorize(fun)\nreturn vec[delay:],vecfun(delayed)", "path": "trials.py", "repo_name": "caffeine-xx/pyglpp", "stars": 1, "license": "None", "language": "python", "size": 31880}
{"docstring": "''' Generates clipped, lagged timeseries from the original x.\nReturns a tuple containing:\n  - newest: unlagged timeseries \n  - lagged: list of L timeseries, where lagged[i] is\n            L-i timesteps lag '''\n", "func_signal": "def multi_lag(x, L=1, sp=1):\n", "code": "newest = np.roll(x,-L*sp)[:-L*sp]\nlagged = [np.roll(x,-l)[:-L*sp] for l in range(0,L*sp,sp)]\nreturn newest,lagged", "path": "information.py", "repo_name": "caffeine-xx/pyglpp", "stars": 1, "license": "None", "language": "python", "size": 31880}
{"docstring": "''' Runs a particular trial. Parameters:\n      - trial: name of a function in trials.py\n      - prefix: file in which the results should be stored.\n      - *args: arguments to the trial function '''\n", "func_signal": "def run_trial(trial, prefix=\"experiment\", *args):\n", "code": "reload(trials)\nresu = getattr(trials, trial)(*args)\nresult.save_result(config.results_dir + prefix, resu)\nreturn resu", "path": "run_trials.py", "repo_name": "caffeine-xx/pyglpp", "stars": 1, "license": "None", "language": "python", "size": 31880}
{"docstring": "''' D_x<-y '''\n", "func_signal": "def transfer_entropy_pdf(x, y, lag=1, bins=64, sp=16):\n", "code": "x1,xt  = multi_lag(x,lag,sp)\ny1,yt  = multi_lag(y,lag,sp)\n# entropies:\n# -X_t + X_t,Y_s + X_t,X_t+1 - X_t,X_t+1,Y_s\npx   = pdf_nd(xt,bins=bins)[0]._entropy()\npxy  = pdf_nd(xt+yt,bins=bins)[0]._entropy()\npxx  = pdf_nd([x1]+xt,bins=bins)[0]._entropy()\npxxy = pdf_nd([x1]+xt+yt,bins=bins)[0]._entropy()\nreturn pxx+pxy-px-pxxy", "path": "information.py", "repo_name": "caffeine-xx/pyglpp", "stars": 1, "license": "None", "language": "python", "size": 31880}
{"docstring": "''' Produces an N-dimensional joint pdf like P(X1=x1, X2=x2,...) \n    Parameters\n      - arr: NxM matrix, where N is dimensions and M is data points\n    Returns: (pdf, edges) where:\n      - pdf: discrete PDF (indexed by bins)\n      - edges: bin edges for discrete PDF values '''\n", "func_signal": "def pdf_nd(arr, bins=4, name=\"nd\"):\n", "code": "histnd,edges = np.histogramdd(arr,bins=bins)\nhistnd = normalize(histnd)\nvalues = np.array(range(histnd.size))\nvalues.reshape(histnd.shape)\npdf = st.rv_discrete(name=name,values=[values,histnd])\nreturn (pdf,edges)", "path": "information.py", "repo_name": "caffeine-xx/pyglpp", "stars": 1, "license": "None", "language": "python", "size": 31880}
{"docstring": "\"\"\"\nCopies the body, in cases where it might be shared with\nanother request object and that is not desired.\n\nThis copies the body in-place, either into a StringIO object\nor a temporary file.\n\"\"\"\n", "func_signal": "def copy_body(self):\n", "code": "length = self.content_length\nif length == 0:\n    # No real need to copy this, but of course it is free\n    self.body_file = StringIO('')\n    return\ntempfile_limit = self.request_body_tempfile_limit\nbody = None\ninput = self.body_file\nif hasattr(input, 'seek'):\n    # Just in case someone has read parts of the body already\n    ## FIXME: Should we use .tell() to try to put the body\n    ## back to its previous position?\n    input.seek(0)\nif length in (-1, None):\n    body = self.body\n    length = len(body)\n    self.content_length = length\nif tempfile_limit and length > tempfile_limit:\n    fileobj = tempfile.TemporaryFile()\n    if body is None:\n        while length:\n            data = input.read(min(length, 4096))\n            fileobj.write(data)\n            length -= len(data)\n    else:\n        fileobj.write(body)\n    fileobj.seek(0)\nelse:\n    if body is None:\n        body = input.read(length)\n    fileobj = StringIO(body)\nself.body_file = fileobj", "path": "lib\\python\\webob-0.9.6.1-py2.5.egg\\webob\\__init__.py", "repo_name": "masyl/mixxim", "stars": 1, "license": "None", "language": "python", "size": 7100}
{"docstring": "\"\"\"\nReturn the content of the request body.\n\"\"\"\n", "func_signal": "def _body__get(self):\n", "code": "try:\n    length = int(self.environ.get('CONTENT_LENGTH', '0'))\nexcept ValueError:\n    return ''\nc = self.body_file.read(length)\nif hasattr(self.body_file, 'seek'):\n    self.body_file.seek(0)\nelse:\n    tempfile_limit = self.request_body_tempfile_limit\n    if tempfile_limit and len(c) > tempfile_limit:\n        fileobj = tempfile.TemporaryFile()\n        fileobj.write(c)\n        fileobj.seek(0)\n    else:\n        fileobj = StringIO(c)\n    # We don't want/need to lose CONTENT_LENGTH here (as setting\n    # self.body_file would do):\n    self.environ['wsgi.input'] = fileobj\nreturn c", "path": "lib\\python\\webob-0.9.6.1-py2.5.egg\\webob\\__init__.py", "repo_name": "masyl/mixxim", "stars": 1, "license": "None", "language": "python", "size": 7100}
{"docstring": "\"\"\"\nReturns a dictionary of all the parameters in the content type.\n\"\"\"\n", "func_signal": "def _content_type_params__get(self):\n", "code": "params = self.headers.get('content-type', '')\nif ';' not in params:\n    return {}\nparams = params.split(';', 1)[1]\nresult = {}\nfor match in _PARAM_RE.finditer(params):\n    result[match.group(1)] = match.group(2) or match.group(3) or ''\nreturn result", "path": "lib\\python\\webob-0.9.6.1-py2.5.egg\\webob\\__init__.py", "repo_name": "masyl/mixxim", "stars": 1, "license": "None", "language": "python", "size": 7100}
{"docstring": "\"\"\"\nResolve other_url relative to the request URL.\n\nIf ``to_application`` is True, then resolve it relative to the\nURL with only SCRIPT_NAME\n\"\"\"\n", "func_signal": "def relative_url(self, other_url, to_application=False):\n", "code": "if to_application:\n    url = self.application_url\n    if not url.endswith('/'):\n        url += '/'\nelse:\n    url = self.path_url\nreturn urlparse.urljoin(url, other_url)", "path": "lib\\python\\webob-0.9.6.1-py2.5.egg\\webob\\__init__.py", "repo_name": "masyl/mixxim", "stars": 1, "license": "None", "language": "python", "size": 7100}
{"docstring": "\"\"\"\nDelete a cookie from the client.  Note that path and domain must match\nhow the cookie was originally set.\n\nThis sets the cookie to the empty string, and max_age=0 so\nthat it should expire immediately.\n\"\"\"\n", "func_signal": "def delete_cookie(self, key, path='/', domain=None):\n", "code": "self.set_cookie(key, '', path=path, domain=domain,\n                max_age=0, expires=timedelta(days=-5))", "path": "lib\\python\\webob-0.9.6.1-py2.5.egg\\webob\\__init__.py", "repo_name": "masyl/mixxim", "stars": 1, "license": "None", "language": "python", "size": 7100}
{"docstring": "\"\"\"\nReturn a MultiDict containing all the variables from a form\nrequest. Returns an empty dict-like object for non-form\nrequests.\n\nForm requests are typically POST requests, however PUT requests\nwith an appropriate Content-Type are also supported.\n\"\"\"\n", "func_signal": "def str_POST(self):\n", "code": "env = self.environ\nif self.method not in ('POST', 'PUT'):\n    return NoVars('Not a form request')\nif 'webob._parsed_post_vars' in env:\n    vars, body_file = env['webob._parsed_post_vars']\n    if body_file is self.body_file:\n        return vars\n# Paste compatibility:\nif 'paste.parsed_formvars' in env:\n    # from paste.request.parse_formvars\n    vars, body_file = env['paste.parsed_formvars']\n    if body_file is self.body_file:\n        # FIXME: is it okay that this isn't *our* MultiDict?\n        return vars\ncontent_type = self.content_type\nif ';' in content_type:\n    content_type = content_type.split(';', 1)[0]\nif (self.method == 'PUT' and not content_type) or \\\n        content_type not in ('', 'application/x-www-form-urlencoded',\n                             'multipart/form-data'):\n    # Not an HTML form submission\n    return NoVars('Not an HTML form submission (Content-Type: %s)'\n                  % content_type)\n# FieldStorage assumes a default CONTENT_LENGTH of -1, but a\n# default of 0 is better:\nenv.setdefault('CONTENT_LENGTH', 0)\nfs_environ = env.copy()\nfs_environ['QUERY_STRING'] = ''\nfs = cgi.FieldStorage(fp=self.body_file,\n                      environ=fs_environ,\n                      keep_blank_values=True)\nvars = MultiDict.from_fieldstorage(fs)\nFakeCGIBody.update_environ(env, vars)\nenv['webob._parsed_post_vars'] = (vars, self.body_file)\nreturn vars", "path": "lib\\python\\webob-0.9.6.1-py2.5.egg\\webob\\__init__.py", "repo_name": "masyl/mixxim", "stars": 1, "license": "None", "language": "python", "size": 7100}
{"docstring": "\"\"\"\nCopies the request and environment object, but turning this request\ninto a GET along the way.  If this was a POST request (or any other verb)\nthen it becomes GET, and the request body is thrown away.\n\"\"\"\n", "func_signal": "def copy_get(self):\n", "code": "env = self.environ.copy()\nenv['wsgi.input'] = StringIO('')\nenv['CONTENT_LENGTH'] = '0'\nif 'CONTENT_TYPE' in env:\n    del env['CONTENT_TYPE']\nenv['REQUEST_METHOD'] = 'GET'\nreturn self.__class__(env)", "path": "lib\\python\\webob-0.9.6.1-py2.5.egg\\webob\\__init__.py", "repo_name": "masyl/mixxim", "stars": 1, "license": "None", "language": "python", "size": 7100}
{"docstring": "\"\"\"\nGenerate an etag for the response object using an MD5 hash of\nthe body (the body parameter, or ``self.body`` if not given)\n\nSets ``self.etag``\nIf ``set_content_md5`` is True sets ``self.content_md5`` as well\nIf ``set_conditional_response`` is True sets ``self.conditional_response`` to True\n\"\"\"\n", "func_signal": "def md5_etag(self, body=None, set_content_md5=False, set_conditional_response=False):\n", "code": "if body is None:\n    body = self.body\ntry:\n    from hashlib import md5\nexcept ImportError:\n    from md5 import md5\nh = md5(body)\nmd5_digest = h.digest().encode('base64').replace('\\n', '').strip('=')\nself.etag = md5_digest\nif set_content_md5:\n    self.content_md5 = md5_digest\nif set_conditional_response:\n    self.conditional_response = True", "path": "lib\\python\\webob-0.9.6.1-py2.5.egg\\webob\\__init__.py", "repo_name": "masyl/mixxim", "stars": 1, "license": "None", "language": "python", "size": 7100}
{"docstring": "\"\"\"\nGet/set the unicode value of the body (using the charset of the Content-Type)\n\"\"\"\n", "func_signal": "def _unicode_body__get(self):\n", "code": "if not self.charset:\n    raise AttributeError(\n        \"You cannot access Response.unicode_body unless charset is set\")\nbody = self.body\nreturn body.decode(self.charset, self.unicode_errors)", "path": "lib\\python\\webob-0.9.6.1-py2.5.egg\\webob\\__init__.py", "repo_name": "masyl/mixxim", "stars": 1, "license": "None", "language": "python", "size": 7100}
{"docstring": "\"\"\"\nCreate a blank request environ (and Request wrapper) with the\ngiven path (path should be urlencoded), and any keys from\nenviron.\n\nThe path will become path_info, with any query string split\noff and used.\n\nAll necessary keys will be added to the environ, but the\nvalues you pass in will take precedence.  If you pass in\nbase_url then wsgi.url_scheme, HTTP_HOST, and SCRIPT_NAME will\nbe filled in from that value.\n\nAny extra keyword will be passed to ``__init__`` (e.g.,\n``decode_param_names``).\n\"\"\"\n", "func_signal": "def blank(cls, path, environ=None, base_url=None, headers=None, **kw):\n", "code": "if _SCHEME_RE.search(path):\n    scheme, netloc, path, qs, fragment = urlparse.urlsplit(path)\n    if fragment:\n        raise TypeError(\n            \"Path cannot contain a fragment (%r)\" % fragment)\n    if qs:\n        path += '?' + qs\n    if ':' not in netloc:\n        if scheme == 'http':\n            netloc += ':80'\n        elif scheme == 'https':\n            netloc += ':443'\n        else:\n            raise TypeError(\"Unknown scheme: %r\" % scheme)\nelse:\n    scheme = 'http'\n    netloc = 'localhost:80'\nif path and '?' in path:\n    path_info, query_string = path.split('?', 1)\n    path_info = urllib.unquote(path_info)\nelse:\n    path_info = urllib.unquote(path)\n    query_string = ''\nenv = {\n    'REQUEST_METHOD': 'GET',\n    'SCRIPT_NAME': '',\n    'PATH_INFO': path_info or '',\n    'QUERY_STRING': query_string,\n    'SERVER_NAME': netloc.split(':')[0],\n    'SERVER_PORT': netloc.split(':')[1],\n    'HTTP_HOST': netloc,\n    'SERVER_PROTOCOL': 'HTTP/1.0',\n    'wsgi.version': (1, 0),\n    'wsgi.url_scheme': scheme,\n    'wsgi.input': StringIO(''),\n    'wsgi.errors': sys.stderr,\n    'wsgi.multithread': False,\n    'wsgi.multiprocess': False,\n    'wsgi.run_once': False,\n    }\nif base_url:\n    scheme, netloc, path, query, fragment = urlparse.urlsplit(base_url)\n    if query or fragment:\n        raise ValueError(\n            \"base_url (%r) cannot have a query or fragment\"\n            % base_url)\n    if scheme:\n        env['wsgi.url_scheme'] = scheme\n    if netloc:\n        if ':' not in netloc:\n            if scheme == 'http':\n                netloc += ':80'\n            elif scheme == 'https':\n                netloc += ':443'\n            else:\n                raise ValueError(\n                    \"Unknown scheme: %r\" % scheme)\n        host, port = netloc.split(':', 1)\n        env['SERVER_PORT'] = port\n        env['SERVER_NAME'] = host\n        env['HTTP_HOST'] = netloc\n    if path:\n        env['SCRIPT_NAME'] = urllib.unquote(path)\nif environ:\n    env.update(environ)\nobj = cls(env, **kw)\nif headers is not None:\n    obj.headers.update(headers)\nreturn obj", "path": "lib\\python\\webob-0.9.6.1-py2.5.egg\\webob\\__init__.py", "repo_name": "masyl/mixxim", "stars": 1, "license": "None", "language": "python", "size": 7100}
{"docstring": "# Again, allow hint but ignore\n", "func_signal": "def readlines(self, hint=None):\n", "code": "body = self._get_body()\nrest = body[self.position:]\nself.position = len(body)\nresult = []\nwhile 1:\n    next = rest.find('\\r\\n')\n    if next == -1:\n        result.append(rest)\n        break\n    result.append(rest[:next+2])\n    rest = rest[next+2:]\nreturn result", "path": "lib\\python\\webob-0.9.6.1-py2.5.egg\\webob\\__init__.py", "repo_name": "masyl/mixxim", "stars": 1, "license": "None", "language": "python", "size": 7100}
{"docstring": "\"\"\"\nLike ``.call_application(application)``, except returns a\nresponse object with ``.status``, ``.headers``, and ``.body``\nattributes.\n\nThis will use ``self.ResponseClass`` to figure out the class\nof the response object to return.\n\"\"\"\n", "func_signal": "def get_response(self, application, catch_exc_info=False):\n", "code": "if catch_exc_info:\n    status, headers, app_iter, exc_info = self.call_application(\n        application, catch_exc_info=True)\n    del exc_info\nelse:\n    status, headers, app_iter = self.call_application(\n        application, catch_exc_info=False)\nreturn self.ResponseClass(\n    status=status, headerlist=headers, app_iter=app_iter,\n    request=self)", "path": "lib\\python\\webob-0.9.6.1-py2.5.egg\\webob\\__init__.py", "repo_name": "masyl/mixxim", "stars": 1, "license": "None", "language": "python", "size": 7100}
{"docstring": "\"\"\"\nLike the normal __call__ interface, but checks conditional headers:\n\n* If-Modified-Since   (304 Not Modified; only on GET, HEAD)\n* If-None-Match       (304 Not Modified; only on GET, HEAD)\n* Range               (406 Partial Content; only on GET, HEAD)\n\"\"\"\n", "func_signal": "def conditional_response_app(self, environ, start_response):\n", "code": "req = self.RequestClass(environ)\nstatus304 = False\nif req.method in self._safe_methods:\n    if req.if_modified_since and self.last_modified and self.last_modified <= req.if_modified_since:\n        status304 = True\n    if req.if_none_match and self.etag:\n        ## FIXME: should a weak match be okay?\n        if self.etag in req.if_none_match:\n            status304 = True\n        else:\n            # Even if If-Modified-Since matched, if ETag doesn't then reject it\n            status304 = False\nif status304:\n    start_response('304 Not Modified', self.headerlist)\n    return EmptyResponse(self.app_iter)\nif req.method == 'HEAD':\n    start_response(self.status, self.headerlist)\n    return EmptyResponse(self.app_iter)\nif (req.range and req.if_range.match_response(self)\n    and self.content_range is None\n    and req.method == 'GET'\n    and self.status_int == 200):\n    content_range = req.range.content_range(self.content_length)\n    if content_range is not None:\n        app_iter = self.app_iter_range(content_range.start, content_range.stop)\n        if app_iter is not None:\n            headers = list(self.headerlist)\n            headers.append(('Content-Range', str(content_range)))\n            start_response('206 Partial Content', headers)\n            return app_iter\nstart_response(self.status, self.headerlist)\nreturn self.app_iter", "path": "lib\\python\\webob-0.9.6.1-py2.5.egg\\webob\\__init__.py", "repo_name": "masyl/mixxim", "stars": 1, "license": "None", "language": "python", "size": 7100}
{"docstring": "\"\"\"\nLike ``.str_cookies``, but may decode values and keys\n\"\"\"\n", "func_signal": "def cookies(self):\n", "code": "vars = self.str_cookies\nif self.charset:\n    vars = UnicodeMultiDict(vars, encoding=self.charset,\n                            errors=self.unicode_errors,\n                            decode_keys=self.decode_param_names)\nreturn vars", "path": "lib\\python\\webob-0.9.6.1-py2.5.egg\\webob\\__init__.py", "repo_name": "masyl/mixxim", "stars": 1, "license": "None", "language": "python", "size": 7100}
{"docstring": "\"\"\"\nLike ``.str_params``, but may decode values and keys\n\"\"\"\n", "func_signal": "def params(self):\n", "code": "params = self.str_params\nif self.charset:\n    params = UnicodeMultiDict(params, encoding=self.charset,\n                              errors=self.unicode_errors,\n                              decode_keys=self.decode_param_names)\nreturn params", "path": "lib\\python\\webob-0.9.6.1-py2.5.egg\\webob\\__init__.py", "repo_name": "masyl/mixxim", "stars": 1, "license": "None", "language": "python", "size": 7100}
{"docstring": "\"\"\"\nRetrieve the Location header of the response, or None if there\nis no header.  If the header is not absolute and this response\nis associated with a request, make the header absolute.\n\nFor more information see `section 14.30\n<http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.30>`_.\n\"\"\"\n", "func_signal": "def _location__get(self):\n", "code": "if 'location' not in self.headers:\n    return None\nlocation = self.headers['location']\nif _SCHEME_RE.search(location):\n    # Absolute\n    return location\nif self.request is not None:\n    base_uri = self.request.url\n    location = urlparse.urljoin(base_uri, location)\nreturn location", "path": "lib\\python\\webob-0.9.6.1-py2.5.egg\\webob\\__init__.py", "repo_name": "masyl/mixxim", "stars": 1, "license": "None", "language": "python", "size": 7100}
{"docstring": "\"\"\" monkey patch for FieldStorage.__repr__\n\nUnbelievely, the default __repr__ on FieldStorage reads\nthe entire file content instead of being sane about it.\nThis is a simple replacement that doesn't do that\n\"\"\"\n", "func_signal": "def _cgi_FieldStorage__repr__patch(self):\n", "code": "if self.file:\n    return \"FieldStorage(%r, %r)\" % (\n            self.name, self.filename)\nreturn \"FieldStorage(%r, %r, %r)\" % (\n         self.name, self.filename, self.value)", "path": "lib\\python\\webob-0.9.6.1-py2.5.egg\\webob\\__init__.py", "repo_name": "masyl/mixxim", "stars": 1, "license": "None", "language": "python", "size": 7100}
{"docstring": "\"\"\"\nThe path of the request, without host but with query string\n\"\"\"\n", "func_signal": "def path_qs(self):\n", "code": "path = self.path\nqs = self.environ.get('QUERY_STRING')\nif qs:\n    path += '?' + qs\nreturn path", "path": "lib\\python\\webob-0.9.6.1-py2.5.egg\\webob\\__init__.py", "repo_name": "masyl/mixxim", "stars": 1, "license": "None", "language": "python", "size": 7100}
{"docstring": "\"\"\"\nReturn the request associated with this response if any.\n\"\"\"\n", "func_signal": "def _request__get(self):\n", "code": "if self._request is None and self._environ is not None:\n    self._request = self.RequestClass(self._environ)\nreturn self._request", "path": "lib\\python\\webob-0.9.6.1-py2.5.egg\\webob\\__init__.py", "repo_name": "masyl/mixxim", "stars": 1, "license": "None", "language": "python", "size": 7100}
{"docstring": "\"\"\"\nReturn a *plain* dictionary of cookies as found in the request.\n\"\"\"\n", "func_signal": "def str_cookies(self):\n", "code": "env = self.environ\nsource = env.get('HTTP_COOKIE', '')\nif 'webob._parsed_cookies' in env:\n    vars, var_source = env['webob._parsed_cookies']\n    if var_source == source:\n        return vars\nvars = {}\nif source:\n    cookies = BaseCookie()\n    cookies.load(source)\n    for name in cookies:\n        value = cookies[name].value\n        unquote_match = self._rx_quotes.match(value)\n        if unquote_match is not None:\n            value = unquote_match.group(1)\n        vars[name] = value\nenv['webob._parsed_cookies'] = (vars, source)\nreturn vars", "path": "lib\\python\\webob-0.9.6.1-py2.5.egg\\webob\\__init__.py", "repo_name": "masyl/mixxim", "stars": 1, "license": "None", "language": "python", "size": 7100}
{"docstring": "\"\"\" GET the message, its headers and contents.\n    (We currently only support staged content, no\n     embedded content)\"\"\"\n", "func_signal": "def GET(self, request):\n", "code": "t = loader.get_template(\"message/get.xml\")\nc = Context({ 'message'     : self.data,\n              'headers'     : self.data.message_header_set.all(),\n              'content_emb' : self.content_set.filter(data__embedded = True),\n              'content_ext' : self.content_set.filter(data__embedded = False),\n              'base_url'    : request.build_absolute_uri('/')})\nreturn HttpResponse(t.render(c))", "path": "Ahkera\\restms\\handlers\\message\\__init__.py", "repo_name": "karthik519/ahkera", "stars": 0, "license": "apache-2.0", "language": "python", "size": 196}
{"docstring": "\"\"\"Simple domain POST test creating a test runner pipe\"\"\"\n", "func_signal": "def testPOSTPipe(self):\n", "code": "r = self.client.post('/restms/domain/Serenity',\n        content_type=\"restms+xml\",\n        data = \"\"\"<?xml version=\"1.0\"?>\n                   <restms xmlns=\"http://www.restms.org/schema/restms\">\n                      <pipe title=\"The Test Runner Pipe\" />\n                   </restms>\n                \"\"\" )\nself.assertEqual(r.status_code, 201)\nloc = r[\"Location\"].split(\"/\")[-1]\nr = self.client.get('/restms/resource/%s' % loc)\nself.assertEqual(r.status_code, 200)", "path": "Ahkera\\restms\\tests\\domain.py", "repo_name": "karthik519/ahkera", "stars": 0, "license": "apache-2.0", "language": "python", "size": 196}
{"docstring": "\"\"\"\nRequests a response from the server using PUT.\n\"\"\"\n", "func_signal": "def put(self, path, data={}, content_type=client.MULTIPART_CONTENT, **extra):\n", "code": "if content_type is client.MULTIPART_CONTENT:\n    put_data = client.encode_multipart(BOUNDARY, data)\nelse:\n    put_data = data\n\nr = {\n    'CONTENT_LENGTH': len(put_data),\n    'CONTENT_TYPE':   content_type,\n    'PATH_INFO':      urllib.unquote(path),\n    'REQUEST_METHOD': 'PUT',\n    'wsgi.input':     client.FakePayload(put_data),\n}\nr.update(extra)\n\nreturn self.request(**r)", "path": "Ahkera\\restms\\tests\\tools.py", "repo_name": "karthik519/ahkera", "stars": 0, "license": "apache-2.0", "language": "python", "size": 196}
{"docstring": "# FIXME: This might be racy.\n", "func_signal": "def delete_if_not_referenced(self):\n", "code": "try:\n    self.message_set.all()\nexcept ObjectDoesNotExist:\n    self.message_header_set.delete()\n    super(message_data, self).delete()", "path": "Ahkera\\restms\\handlers\\message\\__init__.py", "repo_name": "karthik519/ahkera", "stars": 0, "license": "apache-2.0", "language": "python", "size": 196}
{"docstring": "\"\"\"Simple domain POST test posting an existing feed\"\"\"\n", "func_signal": "def testPOSTExistingFeed(self):\n", "code": "r = self.client.post('/restms/domain/Serenity',\n        content_type=\"restms+xml\",\n        data = \"\"\"<?xml version=\"1.0\"?>\n                   <restms xmlns=\"http://www.restms.org/schema/restms\">\n                      <feed name=\"Announcements\" />\n                   </restms>\n                \"\"\" )\nself.assertEqual(r.status_code, 200)", "path": "Ahkera\\restms\\tests\\domain.py", "repo_name": "karthik519/ahkera", "stars": 0, "license": "apache-2.0", "language": "python", "size": 196}
{"docstring": "\"\"\"Simple domain DELETE fail test (not allowed for domains)\"\"\"\n", "func_signal": "def testDELETEFail(self):\n", "code": "client = tools.RESTclient()\nr = client.delete('/restms/domain/Serenity')\nself.assertEqual(r.status_code, 405)", "path": "Ahkera\\restms\\tests\\domain.py", "repo_name": "karthik519/ahkera", "stars": 0, "license": "apache-2.0", "language": "python", "size": 196}
{"docstring": "\"\"\" DELETE a feed and all associated JOINs\"\"\"\n", "func_signal": "def DELETE(self,request):\n", "code": "try: \n    self.join_set.delete()\nexcept: pass\nself.delete()\nreturn HttpResponse()", "path": "Ahkera\\restms\\handlers\\feed\\__init__.py", "repo_name": "karthik519/ahkera", "stars": 0, "license": "apache-2.0", "language": "python", "size": 196}
{"docstring": "\"\"\"\nRequests a response from the server using DELETE.\n\"\"\"\n", "func_signal": "def delete(self, path, **extra):\n", "code": "r = {\n    'PATH_INFO':      urllib.unquote(path),\n    'REQUEST_METHOD': 'DELETE',\n}\nr.update(extra)\n\nreturn self.request(**r)", "path": "Ahkera\\restms\\tests\\tools.py", "repo_name": "karthik519/ahkera", "stars": 0, "license": "apache-2.0", "language": "python", "size": 196}
{"docstring": "\"\"\"Simple join POST fail test (not allowed)\"\"\"\n", "func_signal": "def testPOSTFail(self):\n", "code": "client = tools.RESTclient()\nr = client.post('/restms/resource/join_2',\n        content_type=\"restms+xml\",\n        data = \"\"\"<?xml version=\"1.0\"?>\n                   <restms xmlns=\"http://www.restms.org/schema/restms\">\n                    <message\n                        address=\"alt.rec.misc\"\n                        message_id=\"0815-4711-12345\"\n                        reply_to=\"/dev/null\" />\n                   </restms>\n               \"\"\")\nself.assertEqual(r.status_code, 405)", "path": "Ahkera\\restms\\tests\\join.py", "repo_name": "karthik519/ahkera", "stars": 0, "license": "apache-2.0", "language": "python", "size": 196}
{"docstring": "\"\"\"Simple join PUT fail test (not allowed)\"\"\"\n", "func_signal": "def testPUTFail(self):\n", "code": "client = tools.RESTclient()\nr = client.put('/restms/resource/join_2',\n        content_type=\"restms+xml\",\n        data = \"\"\"<?xml version=\"1.0\"?>\n                   <restms xmlns=\"http://www.restms.org/schema/restms\">\n                      <join address=\"server.*\"\n                        feed=\"http://testserver/restms/feed/Announcements>\"\n                      </join>\n                   </restms>\n               \"\"\")\nself.assertEqual(r.status_code, 405)", "path": "Ahkera\\restms\\tests\\join.py", "repo_name": "karthik519/ahkera", "stars": 0, "license": "apache-2.0", "language": "python", "size": 196}
{"docstring": "\"\"\"GET a pipe, its associated JOINs and all its messages \"\"\"\n", "func_signal": "def GET(self, request):\n", "code": "t = loader.get_template(\"pipe/get.xml\")\nc = Context({ 'pipe'    : self,\n              'joins'   : self.join_set.all(),\n              'messages': self.message_set.all(),\n              'base_url': request.build_absolute_uri('/')})\nreturn HttpResponse(t.render(c))", "path": "Ahkera\\restms\\handlers\\pipe\\__init__.py", "repo_name": "karthik519/ahkera", "stars": 0, "license": "apache-2.0", "language": "python", "size": 196}
{"docstring": "\"\"\"Complex join DELETE test (checks whether the join is really gone)\"\"\"\n", "func_signal": "def testDELETE(self):\n", "code": "client = tools.RESTclient()\nr = client.delete('/restms/resource/join_1')\nself.assertEqual(r.status_code, 200)\nr = client.get('/restms/resource/join_1')\nself.assertEqual(r.status_code, 404)", "path": "Ahkera\\restms\\tests\\join.py", "repo_name": "karthik519/ahkera", "stars": 0, "license": "apache-2.0", "language": "python", "size": 196}
{"docstring": "\"\"\" DELETE a pipe and all associated JOINs\"\"\"\n", "func_signal": "def DELETE(self,request):\n", "code": "try: self.join_set.delete()\nexcept: pass\nself.delete()\nreturn HttpResponse()", "path": "Ahkera\\restms\\handlers\\pipe\\__init__.py", "repo_name": "karthik519/ahkera", "stars": 0, "license": "apache-2.0", "language": "python", "size": 196}
{"docstring": "\"\"\"Simple message GET fail on nonexisting message\"\"\"\n", "func_signal": "def testGETFail(self):\n", "code": "r = self.client.get('/restms/resource/message_99')\nself.assertEqual(r.status_code, 404)", "path": "Ahkera\\restms\\tests\\message.py", "repo_name": "karthik519/ahkera", "stars": 0, "license": "apache-2.0", "language": "python", "size": 196}
{"docstring": "\"\"\"Simple domain GET fail test retrieving a nonexisting domain\"\"\"\n", "func_signal": "def testGETFail(self):\n", "code": "r = self.client.get('/restms/domain/fail')\nself.assertEqual(r.status_code, 404)", "path": "Ahkera\\restms\\tests\\domain.py", "repo_name": "karthik519/ahkera", "stars": 0, "license": "apache-2.0", "language": "python", "size": 196}
{"docstring": "\"\"\"Simple message POST fail test (not allowed)\"\"\"\n", "func_signal": "def testPOSTFail(self):\n", "code": "client = tools.RESTclient()\nr = client.post('/restms/resource/message_1',\n        content_type=\"restms+xml\",\n        data = \"\"\"<?xml version=\"1.0\"?>\n                   <restms xmlns=\"http://www.restms.org/schema/restms\">\n                        <content type=\"text/plain\" encoding=\"ascii\" >Just testing.</content>\n                   </restms>\n               \"\"\")\nself.assertEqual(r.status_code, 405)", "path": "Ahkera\\restms\\tests\\message.py", "repo_name": "karthik519/ahkera", "stars": 0, "license": "apache-2.0", "language": "python", "size": 196}
{"docstring": "\"\"\"GET a domain, its associated public (i.e. named) FEEDs and profiles \n\"\"\"\n", "func_signal": "def GET(self, request):\n", "code": "t = loader.get_template(\"domain/get.xml\")\nc = Context({ 'domain'    : self,\n              'feeds'     : self.feed_set.all(),\n              'profiles'  : self.profile_set.all(),\n              'base_url'  : request.build_absolute_uri('/')})\nreturn HttpResponse(t.render(c))", "path": "Ahkera\\restms\\handlers\\domain\\__init__.py", "repo_name": "karthik519/ahkera", "stars": 0, "license": "apache-2.0", "language": "python", "size": 196}
{"docstring": "\"\"\"Simple domain POST test creating a test runner feed\"\"\"\n", "func_signal": "def testPOSTFeed(self):\n", "code": "r = self.client.post('/restms/domain/Serenity',\n        content_type=\"restms+xml\",\n        data = \"\"\"<?xml version=\"1.0\"?>\n                   <restms xmlns=\"http://www.restms.org/schema/restms\">\n                      <feed name=\"Test Runner Feed\"\n                        title=\"The Test Runner Feed\"\n                        license=\"BSD\" />\n                   </restms>\n                \"\"\" )\nself.assertEqual(r.status_code, 201)\nloc = r[\"Location\"].split(\"/\")[-1]\nr = self.client.get('/restms/feed/%s' % loc)\nself.assertEqual(r.status_code, 200)", "path": "Ahkera\\restms\\tests\\domain.py", "repo_name": "karthik519/ahkera", "stars": 0, "license": "apache-2.0", "language": "python", "size": 196}
{"docstring": "# FIXME: This might be racy.\n", "func_signal": "def delete_if_not_referenced(self):\n", "code": "try:\n    self.content_set.all()\nexcept ObjectDoesNotExist:\n    super(content_data, self).delete()", "path": "Ahkera\\restms\\handlers\\message\\content.py", "repo_name": "karthik519/ahkera", "stars": 0, "license": "apache-2.0", "language": "python", "size": 196}
{"docstring": "\"\"\"Simple message PUT fail test (not allowed)\"\"\"\n", "func_signal": "def testPUTFail(self):\n", "code": "client = tools.RESTclient()\nr = client.put('/restms/resource/message_1',\n        content_type=\"restms+xml\",\n        data = \"\"\"<?xml version=\"1.0\"?>\n                    <restms xmlns=\"http://www.restms.org/schema/restms\">\n                        <message>\n                            <header name=\"funny header\" value=\"123\" />\n                            <content type=\"text/cmd\" encoding=\"ascii\" >Do something! NOW!</content>\n                        </message>\n                    </restms>\n               \"\"\")\nself.assertEqual(r.status_code, 405)", "path": "Ahkera\\restms\\tests\\message.py", "repo_name": "karthik519/ahkera", "stars": 0, "license": "apache-2.0", "language": "python", "size": 196}
{"docstring": "\"\"\"Construct and return tupel with IO objects:\np2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite\n\"\"\"\n", "func_signal": "def _get_handles(self, stdin, stdout, stderr):\n", "code": "if stdin == None and stdout == None and stderr == None:\n    return (None, None, None, None, None, None)\n\np2cread, p2cwrite = None, None\nc2pread, c2pwrite = None, None\nerrread, errwrite = None, None\n\nif stdin == None:\n    p2cread = GetStdHandle(STD_INPUT_HANDLE)\nelif stdin == PIPE:\n    p2cread, p2cwrite = CreatePipe(None, 0)\n    # Detach and turn into fd\n    p2cwrite = p2cwrite.Detach()\n    p2cwrite = msvcrt.open_osfhandle(p2cwrite, 0)\nelif type(stdin) == types.IntType:\n    p2cread = msvcrt.get_osfhandle(stdin)\nelse:\n    # Assuming file-like object\n    p2cread = msvcrt.get_osfhandle(stdin.fileno())\np2cread = self._make_inheritable(p2cread)\n\nif stdout == None:\n    c2pwrite = GetStdHandle(STD_OUTPUT_HANDLE)\nelif stdout == PIPE:\n    c2pread, c2pwrite = CreatePipe(None, 0)\n    # Detach and turn into fd\n    c2pread = c2pread.Detach()\n    c2pread = msvcrt.open_osfhandle(c2pread, 0)\nelif type(stdout) == types.IntType:\n    c2pwrite = msvcrt.get_osfhandle(stdout)\nelse:\n    # Assuming file-like object\n    c2pwrite = msvcrt.get_osfhandle(stdout.fileno())\nc2pwrite = self._make_inheritable(c2pwrite)\n\nif stderr == None:\n    errwrite = GetStdHandle(STD_ERROR_HANDLE)\nelif stderr == PIPE:\n    errread, errwrite = CreatePipe(None, 0)\n    # Detach and turn into fd\n    errread = errread.Detach()\n    errread = msvcrt.open_osfhandle(errread, 0)\nelif stderr == STDOUT:\n    errwrite = c2pwrite\nelif type(stderr) == types.IntType:\n    errwrite = msvcrt.get_osfhandle(stderr)\nelse:\n    # Assuming file-like object\n    errwrite = msvcrt.get_osfhandle(stderr.fileno())\nerrwrite = self._make_inheritable(errwrite)\n\nreturn (p2cread, p2cwrite,\n        c2pread, c2pwrite,\n        errread, errwrite)", "path": "koan\\sub_process.py", "repo_name": "remotesyssupport/koan", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 450}
{"docstring": "\"\"\"\nTranslate a sequence of arguments into a command line\nstring, using the same rules as the MS C runtime:\n\n1) Arguments are delimited by white space, which is either a\n   space or a tab.\n\n2) A string surrounded by double quotation marks is\n   interpreted as a single argument, regardless of white space\n   contained within.  A quoted string can be embedded in an\n   argument.\n\n3) A double quotation mark preceded by a backslash is\n   interpreted as a literal double quotation mark.\n\n4) Backslashes are interpreted literally, unless they\n   immediately precede a double quotation mark.\n\n5) If backslashes immediately precede a double quotation mark,\n   every pair of backslashes is interpreted as a literal\n   backslash.  If the number of backslashes is odd, the last\n   backslash escapes the next double quotation mark as\n   described in rule 3.\n\"\"\"\n\n# See\n# http://msdn.microsoft.com/library/en-us/vccelng/htm/progs_12.asp\n", "func_signal": "def list2cmdline(seq):\n", "code": "result = []\nneedquote = False\nfor arg in seq:\n    bs_buf = []\n\n    # Add a space to separate this argument from the others\n    if result:\n        result.append(' ')\n\n    needquote = (\" \" in arg) or (\"\\t\" in arg)\n    if needquote:\n        result.append('\"')\n\n    for c in arg:\n        if c == '\\\\':\n            # Don't know if we need to double yet.\n            bs_buf.append(c)\n        elif c == '\"':\n            # Double backspaces.\n            result.append('\\\\' * len(bs_buf)*2)\n            bs_buf = []\n            result.append('\\\\\"')\n        else:\n            # Normal char\n            if bs_buf:\n                result.extend(bs_buf)\n                bs_buf = []\n            result.append(c)\n\n    # Add remaining backspaces, if any.\n    if bs_buf:\n        result.extend(bs_buf)\n\n    if needquote:\n        result.extend(bs_buf)\n        result.append('\"')\n\nreturn ''.join(result)", "path": "koan\\sub_process.py", "repo_name": "remotesyssupport/koan", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 450}
{"docstring": "\"\"\"Construct and return tupel with IO objects:\np2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite\n\"\"\"\n", "func_signal": "def _get_handles(self, stdin, stdout, stderr):\n", "code": "p2cread, p2cwrite = None, None\nc2pread, c2pwrite = None, None\nerrread, errwrite = None, None\n\nif stdin == None:\n    pass\nelif stdin == PIPE:\n    p2cread, p2cwrite = os.pipe()\nelif type(stdin) == types.IntType:\n    p2cread = stdin\nelse:\n    # Assuming file-like object\n    p2cread = stdin.fileno()\n\nif stdout == None:\n    pass\nelif stdout == PIPE:\n    c2pread, c2pwrite = os.pipe()\nelif type(stdout) == types.IntType:\n    c2pwrite = stdout\nelse:\n    # Assuming file-like object\n    c2pwrite = stdout.fileno()\n\nif stderr == None:\n    pass\nelif stderr == PIPE:\n    errread, errwrite = os.pipe()\nelif stderr == STDOUT:\n    errwrite = c2pwrite\nelif type(stderr) == types.IntType:\n    errwrite = stderr\nelse:\n    # Assuming file-like object\n    errwrite = stderr.fileno()\n\nreturn (p2cread, p2cwrite,\n        c2pread, c2pwrite,\n        errread, errwrite)", "path": "koan\\sub_process.py", "repo_name": "remotesyssupport/koan", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 450}
{"docstring": "\"\"\"_handle_long_word(chunks : [string],\n                     cur_line : [string],\n                     cur_len : int, width : int)\n\nHandle a chunk of text (most likely a word, not whitespace) that\nis too long to fit in any line.\n\"\"\"\n", "func_signal": "def _handle_long_word(self, chunks, cur_line, cur_len, width):\n", "code": "space_left = max(width - cur_len, 1)\n\n# If we're allowed to break long words, then do so: put as much\n# of the next chunk onto the current line as will fit.\nif self.break_long_words:\n    cur_line.append(chunks[0][0:space_left])\n    chunks[0] = chunks[0][space_left:]\n\n# Otherwise, we have to preserve the long word intact.  Only add\n# it to the current line if there's nothing already there --\n# that minimizes how much we violate the width constraint.\nelif not cur_line:\n    cur_line.append(chunks.pop(0))\n\n# If we're not allowed to break long words, and there's already\n# text on the current line, do nothing.  Next time through the\n# main loop of _wrap_chunks(), we'll wind up here again, but\n# cur_len will be zero, so the next line will be entirely\n# devoted to the long word that we can't handle right now.", "path": "koan\\text_wrap.py", "repo_name": "remotesyssupport/koan", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 450}
{"docstring": "\"\"\"dedent(text : string) -> string\n\nRemove any whitespace than can be uniformly removed from the left\nof every line in `text`.\n\nThis can be used e.g. to make triple-quoted strings line up with\nthe left edge of screen/whatever, while still presenting it in the\nsource code in indented form.\n\nFor example:\n\n    def test():\n        # end first line with \\ to avoid the empty line!\n        s = '''\\\n        hello\n          world\n        '''\n        print repr(s)          # prints '    hello\\n      world\\n    '\n        print repr(dedent(s))  # prints 'hello\\n  world\\n'\n\"\"\"\n", "func_signal": "def dedent(text):\n", "code": "lines = text.expandtabs().split('\\n')\nmargin = None\nfor line in lines:\n    content = line.lstrip()\n    if not content:\n        continue\n    indent = len(line) - len(content)\n    if margin is None:\n        margin = indent\n    else:\n        margin = min(margin, indent)\n\nif margin is not None and margin > 0:\n    for i in range(len(lines)):\n        lines[i] = lines[i][margin:]\n\nreturn '\\n'.join(lines)", "path": "koan\\text_wrap.py", "repo_name": "remotesyssupport/koan", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 450}
{"docstring": "\"\"\"Interact with process: Send data to stdin.  Read data from\nstdout and stderr, until end-of-file is reached.  Wait for\nprocess to terminate.  The optional input argument should be a\nstring to be sent to the child process, or None, if no data\nshould be sent to the child.\n\ncommunicate() returns a tuple (stdout, stderr).\"\"\"\n", "func_signal": "def communicate(self, input=None):\n", "code": "stdout = None # Return\nstderr = None # Return\n\nif self.stdout:\n    stdout = []\n    stdout_thread = threading.Thread(target=self._readerthread,\n                                     args=(self.stdout, stdout))\n    stdout_thread.setDaemon(True)\n    stdout_thread.start()\nif self.stderr:\n    stderr = []\n    stderr_thread = threading.Thread(target=self._readerthread,\n                                     args=(self.stderr, stderr))\n    stderr_thread.setDaemon(True)\n    stderr_thread.start()\n\nif self.stdin:\n    if input != None:\n        self.stdin.write(input)\n    self.stdin.close()\n\nif self.stdout:\n    stdout_thread.join()\nif self.stderr:\n    stderr_thread.join()\n\n# All data exchanged.  Translate lists into strings.\nif stdout != None:\n    stdout = stdout[0]\nif stderr != None:\n    stderr = stderr[0]\n\n# Translate newlines, if requested.  We cannot let the file\n# object do the translation: It is based on stdio, which is\n# impossible to combine with select (unless forcing no\n# buffering).\nif self.universal_newlines and hasattr(open, 'newlines'):\n    if stdout:\n        stdout = self._translate_newlines(stdout)\n    if stderr:\n        stderr = self._translate_newlines(stderr)\n\nself.wait()\nreturn (stdout, stderr)", "path": "koan\\sub_process.py", "repo_name": "remotesyssupport/koan", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 450}
{"docstring": "\"\"\"Wait for child process to terminate.  Returns returncode\nattribute.\"\"\"\n", "func_signal": "def wait(self):\n", "code": "if self.returncode == None:\n    pid, sts = os.waitpid(self.pid, 0)\n    self._handle_exitstatus(sts)\nreturn self.returncode", "path": "koan\\sub_process.py", "repo_name": "remotesyssupport/koan", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 450}
{"docstring": "\"\"\"Find and return absolut path to w9xpopen.exe\"\"\"\n", "func_signal": "def _find_w9xpopen(self):\n", "code": "w9xpopen = os.path.join(os.path.dirname(GetModuleFileName(0)),\n                        \"w9xpopen.exe\")\nif not os.path.exists(w9xpopen):\n    # Eeek - file-not-found - possibly an embedding\n    # situation - see if we can locate it in sys.exec_prefix\n    w9xpopen = os.path.join(os.path.dirname(sys.exec_prefix),\n                            \"w9xpopen.exe\")\n    if not os.path.exists(w9xpopen):\n        raise RuntimeError(\"Cannot locate w9xpopen.exe, which is \"\n                           \"needed for Popen to work with your \"\n                           \"shell or platform.\")\nreturn w9xpopen", "path": "koan\\sub_process.py", "repo_name": "remotesyssupport/koan", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 450}
{"docstring": "\"\"\"Wait for child process to terminate.  Returns returncode\nattribute.\"\"\"\n", "func_signal": "def wait(self):\n", "code": "if self.returncode == None:\n    obj = WaitForSingleObject(self._handle, INFINITE)\n    self.returncode = GetExitCodeProcess(self._handle)\n    _active.remove(self)\nreturn self.returncode", "path": "koan\\sub_process.py", "repo_name": "remotesyssupport/koan", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 450}
{"docstring": "\"\"\"Check if child process has terminated.  Returns returncode\nattribute.\"\"\"\n", "func_signal": "def poll(self):\n", "code": "if self.returncode == None:\n    if WaitForSingleObject(self._handle, 0) == WAIT_OBJECT_0:\n        self.returncode = GetExitCodeProcess(self._handle)\n        _active.remove(self)\nreturn self.returncode", "path": "koan\\sub_process.py", "repo_name": "remotesyssupport/koan", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 450}
{"docstring": "\"\"\"_fix_sentence_endings(chunks : [string])\n\nCorrect for sentence endings buried in 'chunks'.  Eg. when the\noriginal text contains \"... foo.\\nBar ...\", munge_whitespace()\nand split() will convert that to [..., \"foo.\", \" \", \"Bar\", ...]\nwhich has one too few spaces; this method simply changes the one\nspace to two.\n\"\"\"\n", "func_signal": "def _fix_sentence_endings(self, chunks):\n", "code": "i = 0\npat = self.sentence_end_re\nwhile i < len(chunks)-1:\n    if chunks[i+1] == \" \" and pat.search(chunks[i]):\n        chunks[i+1] = \"  \"\n        i += 2\n    else:\n        i += 1", "path": "koan\\text_wrap.py", "repo_name": "remotesyssupport/koan", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 450}
{"docstring": "\"\"\"Return a duplicate of handle, which is inheritable\"\"\"\n", "func_signal": "def _make_inheritable(self, handle):\n", "code": "return DuplicateHandle(GetCurrentProcess(), handle,\n                       GetCurrentProcess(), 0, 1,\n                       DUPLICATE_SAME_ACCESS)", "path": "koan\\sub_process.py", "repo_name": "remotesyssupport/koan", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 450}
{"docstring": "\"\"\"_munge_whitespace(text : string) -> string\n\nMunge whitespace in text: expand tabs and convert all other\nwhitespace characters to spaces.  Eg. \" foo\\tbar\\n\\nbaz\"\nbecomes \" foo    bar  baz\".\n\"\"\"\n", "func_signal": "def _munge_whitespace(self, text):\n", "code": "if self.expand_tabs:\n    text = text.expandtabs()\nif self.replace_whitespace:\n    if isinstance(text, str):\n        text = text.translate(self.whitespace_trans)\n    elif isinstance(text, unicode):\n        text = text.translate(self.unicode_whitespace_trans)\nreturn text", "path": "koan\\text_wrap.py", "repo_name": "remotesyssupport/koan", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 450}
{"docstring": "\"\"\"Interact with process: Send data to stdin.  Read data from\nstdout and stderr, until end-of-file is reached.  Wait for\nprocess to terminate.  The optional input argument should be a\nstring to be sent to the child process, or None, if no data\nshould be sent to the child.\n\ncommunicate() returns a tuple (stdout, stderr).\"\"\"\n", "func_signal": "def communicate(self, input=None):\n", "code": "read_set = []\nwrite_set = []\nstdout = None # Return\nstderr = None # Return\n\nif self.stdin:\n    # Flush stdio buffer.  This might block, if the user has\n    # been writing to .stdin in an uncontrolled fashion.\n    self.stdin.flush()\n    if input:\n        write_set.append(self.stdin)\n    else:\n        self.stdin.close()\nif self.stdout:\n    read_set.append(self.stdout)\n    stdout = []\nif self.stderr:\n    read_set.append(self.stderr)\n    stderr = []\n\nwhile read_set or write_set:\n    rlist, wlist, xlist = select.select(read_set, write_set, [])\n\n    if self.stdin in wlist:\n        # When select has indicated that the file is writable,\n        # we can write up to PIPE_BUF bytes without risk\n        # blocking.  POSIX defines PIPE_BUF >= 512\n        bytes_written = os.write(self.stdin.fileno(), input[:512])\n        input = input[bytes_written:]\n        if not input:\n            self.stdin.close()\n            write_set.remove(self.stdin)\n\n    if self.stdout in rlist:\n        data = os.read(self.stdout.fileno(), 1024)\n        if data == \"\":\n            self.stdout.close()\n            read_set.remove(self.stdout)\n        stdout.append(data)\n\n    if self.stderr in rlist:\n        data = os.read(self.stderr.fileno(), 1024)\n        if data == \"\":\n            self.stderr.close()\n            read_set.remove(self.stderr)\n        stderr.append(data)\n\n# All data exchanged.  Translate lists into strings.\nif stdout != None:\n    stdout = ''.join(stdout)\nif stderr != None:\n    stderr = ''.join(stderr)\n\n# Translate newlines, if requested.  We cannot let the file\n# object do the translation: It is based on stdio, which is\n# impossible to combine with select (unless forcing no\n# buffering).\nif self.universal_newlines and hasattr(open, 'newlines'):\n    if stdout:\n        stdout = self._translate_newlines(stdout)\n    if stderr:\n        stderr = self._translate_newlines(stderr)\n\nself.wait()\nreturn (stdout, stderr)", "path": "koan\\sub_process.py", "repo_name": "remotesyssupport/koan", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 450}
{"docstring": "\"\"\"Wrap a single paragraph of text, returning a list of wrapped lines.\n\nReformat the single paragraph in 'text' so it fits in lines of no\nmore than 'width' columns, and return a list of wrapped lines.  By\ndefault, tabs in 'text' are expanded with string.expandtabs(), and\nall other whitespace characters (including newline) are converted to\nspace.  See TextWrapper class for available keyword args to customize\nwrapping behaviour.\n\"\"\"\n", "func_signal": "def wrap(text, width=70, **kwargs):\n", "code": "w = TextWrapper(width=width, **kwargs)\nreturn w.wrap(text)", "path": "koan\\text_wrap.py", "repo_name": "remotesyssupport/koan", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 450}
{"docstring": "\"\"\"_split(text : string) -> [string]\n\nSplit the text to wrap into indivisible chunks.  Chunks are\nnot quite the same as words; see wrap_chunks() for full\ndetails.  As an example, the text\n  Look, goof-ball -- use the -b option!\nbreaks into the following chunks:\n  'Look,', ' ', 'goof-', 'ball', ' ', '--', ' ',\n  'use', ' ', 'the', ' ', '-b', ' ', 'option!'\n\"\"\"\n", "func_signal": "def _split(self, text):\n", "code": "chunks = self.wordsep_re.split(text)\nchunks = filter(None, chunks)\nreturn chunks", "path": "koan\\text_wrap.py", "repo_name": "remotesyssupport/koan", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 450}
{"docstring": "\"\"\"Fill a single paragraph of text, returning a new string.\n\nReformat the single paragraph in 'text' to fit in lines of no more\nthan 'width' columns, and return a new string containing the entire\nwrapped paragraph.  As with wrap(), tabs are expanded and other\nwhitespace characters converted to space.  See TextWrapper class for\navailable keyword args to customize wrapping behaviour.\n\"\"\"\n", "func_signal": "def fill(text, width=70, **kwargs):\n", "code": "w = TextWrapper(width=width, **kwargs)\nreturn w.fill(text)", "path": "koan\\text_wrap.py", "repo_name": "remotesyssupport/koan", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 450}
{"docstring": "\"\"\"wrap(text : string) -> [string]\n\nReformat the single paragraph in 'text' so it fits in lines of\nno more than 'self.width' columns, and return a list of wrapped\nlines.  Tabs in 'text' are expanded with string.expandtabs(),\nand all other whitespace characters (including newline) are\nconverted to space.\n\"\"\"\n", "func_signal": "def wrap(self, text):\n", "code": "text = self._munge_whitespace(text)\nindent = self.initial_indent\nchunks = self._split(text)\nif self.fix_sentence_endings:\n    self._fix_sentence_endings(chunks)\nreturn self._wrap_chunks(chunks)", "path": "koan\\text_wrap.py", "repo_name": "remotesyssupport/koan", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 450}
{"docstring": "\"\"\"_wrap_chunks(chunks : [string]) -> [string]\n\nWrap a sequence of text chunks and return a list of lines of\nlength 'self.width' or less.  (If 'break_long_words' is false,\nsome lines may be longer than this.)  Chunks correspond roughly\nto words and the whitespace between them: each chunk is\nindivisible (modulo 'break_long_words'), but a line break can\ncome between any two chunks.  Chunks should not have internal\nwhitespace; ie. a chunk is either all whitespace or a \"word\".\nWhitespace chunks will be removed from the beginning and end of\nlines, but apart from that whitespace is preserved.\n\"\"\"\n", "func_signal": "def _wrap_chunks(self, chunks):\n", "code": "lines = []\nif self.width <= 0:\n    raise ValueError(\"invalid width %r (must be > 0)\" % self.width)\n\nwhile chunks:\n\n    # Start the list of chunks that will make up the current line.\n    # cur_len is just the length of all the chunks in cur_line.\n    cur_line = []\n    cur_len = 0\n\n    # Figure out which static string will prefix this line.\n    if lines:\n        indent = self.subsequent_indent\n    else:\n        indent = self.initial_indent\n\n    # Maximum width for this line.\n    width = self.width - len(indent)\n\n    # First chunk on line is whitespace -- drop it, unless this\n    # is the very beginning of the text (ie. no lines started yet).\n    if chunks[0].strip() == '' and lines:\n        del chunks[0]\n\n    while chunks:\n        l = len(chunks[0])\n\n        # Can at least squeeze this chunk onto the current line.\n        if cur_len + l <= width:\n            cur_line.append(chunks.pop(0))\n            cur_len += l\n\n        # Nope, this line is full.\n        else:\n            break\n\n    # The current line is full, and the next chunk is too big to\n    # fit on *any* line (not just this one).\n    if chunks and len(chunks[0]) > width:\n        self._handle_long_word(chunks, cur_line, cur_len, width)\n\n    # If the last chunk on this line is all whitespace, drop it.\n    if cur_line and cur_line[-1].strip() == '':\n        del cur_line[-1]\n\n    # Convert current line back to a string and store it in list\n    # of all lines (return value).\n    if cur_line:\n        lines.append(indent + ''.join(cur_line))\n\nreturn lines", "path": "koan\\text_wrap.py", "repo_name": "remotesyssupport/koan", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 450}
{"docstring": "\"\"\"Check if child process has terminated.  Returns returncode\nattribute.\"\"\"\n", "func_signal": "def poll(self):\n", "code": "if self.returncode == None:\n    try:\n        pid, sts = os.waitpid(self.pid, os.WNOHANG)\n        if pid == self.pid:\n            self._handle_exitstatus(sts)\n    except os.error:\n        pass\nreturn self.returncode", "path": "koan\\sub_process.py", "repo_name": "remotesyssupport/koan", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 450}
{"docstring": "\"\"\"\n\tapp(name=None, id=None, creator=None, pid=None, url=None, terms=True)\n\t\tname : str -- name or path of application, e.g. 'TextEdit', 'TextEdit.app', '/Applications/Textedit.app'\n\t\tid : str -- bundle id of application, e.g. 'com.apple.textedit'\n\t\tcreator : str -- 4-character creator type of application, e.g. 'ttxt'\n\t\tpid : int -- Unix process id, e.g. 955\n\t\turl : str -- eppc:// URL, e.g. eppc://G4.local/TextEdit'\n\t\taemapp : aem.Application\n\t\tterms : module | bool -- if a module, get terminology from it; if True, get terminology from target application; if False, use built-in terminology only\n    \t\t\"\"\"\n", "func_signal": "def __init__(self, name=None, id=None, creator=None, pid=None, url=None, aemapp=None, terms=True):\n", "code": "if len([i for i in [name, id, creator, pid, url, aemapp] if i]) > 1:\n\traise TypeError('app() received more than one of the following arguments: name, id, creator, pid, url, aemapp')\nif name:\n\tconstructor, identifier = 'path', aem.findapp.byname(name)\nelif id:\n\tconstructor, identifier = 'path',  aem.findapp.byid(id)\nelif creator:\n\tconstructor, identifier = 'path',  aem.findapp.bycreator(creator)\nelif pid:\n\tconstructor, identifier = 'pid', pid\nelif url:\n\tconstructor, identifier = 'url', url\nelif aemapp:\n\tconstructor, identifier = 'aemapp', aemapp\nelse:\n\tconstructor, identifier = 'current', None\n# Defer initialisation of AppData until it's needed. This allows user to call launch() on a non-running application without the application being launched by aem.Application, which automatically launches local applications in order to construct an AEAddressDesc of typeProcessSerialNumber.\n# launch()'s usefulness is somewhat limited, since constructing a real app-based reference will also launch the application normally in order to get its terminology. So to actually launch an application, you have to use launch() before constructing any real references to its objects; i.e.:\n#     te = app('TextEdit'); te.launch(); d = app.documents\n# will launch TE without it creating any new documents (i.e. app receives 'ascrnoop' as its first event), but:\n#     te = app('TextEdit'); d = app.documents; te.launch()\n# will launch TE normally (i.e. app receives 'aevtoapp' as its first event), causing it to open a new, empty window.\nReference.__init__(self, AppData(self._Application, constructor, identifier, terms), aem.app)", "path": "lib\\appscript\\reference.py", "repo_name": "laonger/iWork_Auto_Save", "stars": 1, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"Initialises application target and terminology lookup tables.\n\nCalled automatically the first time clients retrieve target, typebycode, typebyname,\nreferencebycode, referencebyname; clients should not need to call it themselves.\n\"\"\"\n# initialise target (by default an aem.Application instance)\n", "func_signal": "def connect(self):\n", "code": "if self.constructor == 'aemapp':\n\tt = self._target = self.identifier\nelif self.constructor == 'current':\n\tt = self._target = self._aemapplicationclass()\nelse:\n\tt = self._target = self._aemapplicationclass(**{self.constructor: self.identifier})\n# initialise translation tables\nif self._terms == True: # obtain terminology from application\n\tself._terms = terminology.tablesforapp(t)\nelif self._terms == False: # use built-in terminology only (e.g. use this when running AppleScript applets)\n\tself._terms = terminology.defaulttables\nelif not isinstance(self._terms, tuple): # use user-supplied terminology module\n\tself._terms = terminology.tablesformodule(self._terms)\nd1, d2, d3, d4 = self._typebycode, self._typebyname, \\\n\t\tself._referencebycode, self._referencebyname = self._terms\nself.target = lambda: t\nself.typebycode = lambda: d1\nself.typebyname = lambda: d2\nself.referencebycode = lambda: d3\nself.referencebyname = lambda: d4", "path": "lib\\appscript\\reference.py", "repo_name": "laonger/iWork_Auto_Save", "stars": 1, "license": "None", "language": "python", "size": 148}
{"docstring": "# (Note: reference.Reference class is passed as argument simply to avoid circular import between that module and this)\n", "func_signal": "def AS_resolve(self, Reference, appdata):\n", "code": "ref = Reference(appdata, {'app':aem.app, 'con':aem.con, 'its':aem.its}[self._call[0]])\nfor method, args, repstr in self._call[1:]:\n\tif method == '__getattr__':\n\t\tref = getattr(ref, args)\n\telif method == '__call__':\n\t\tref = ref(*args[0], **args[1])\n\telse:\n\t\tref = getattr(ref, method)(args)\nreturn ref", "path": "lib\\appscript\\genericreference.py", "repo_name": "laonger/iWork_Auto_Save", "stars": 1, "license": "None", "language": "python", "size": 148}
{"docstring": "# Unpack typeType, typeEnum, typeProperty; replaces default aem decoders to convert types, enums, etc.\n# to Keyword objects instead of AETypes, AEEnums, etc.\n", "func_signal": "def unpackkeyword(self, desc):\n", "code": "aemValue = _lowlevelcodecs.unpack(desc)\nreturn self.typebycode().get(aemValue.code, aemValue)", "path": "lib\\appscript\\reference.py", "repo_name": "laonger/iWork_Auto_Save", "stars": 1, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"Create a new appscript reference from an aem reference.\"\"\"\n", "func_signal": "def AS_newreference(self, ref):\n", "code": "if isinstance(ref, GenericReference):\n\treturn ref.AS_resolve(Reference, self.AS_appdata)\nelif isinstance(ref, aem.Query):\n\treturn Reference(self.AS_appdata, ref)\nelif ref is None:\n\treturn Reference(self.AS_appdata, aem.app)\nelse:\n\treturn Reference(self.AS_appdata, aem.customroot(ref))", "path": "lib\\appscript\\reference.py", "repo_name": "laonger/iWork_Auto_Save", "stars": 1, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"Make File object from file URL.\"\"\"\n", "func_signal": "def makewithurl(klass, url):\n", "code": "obj = klass(_kNoPath)\nobj._desc = newdesc(kae.typeFileURL, url)\nobj._url = url\nobj._path = converturltopath(url, kCFURLPOSIXPathStyle)\nreturn obj", "path": "lib\\aem\\mactypes.py", "repo_name": "laonger/iWork_Auto_Save", "stars": 1, "license": "None", "language": "python", "size": 148}
{"docstring": "# Pack dictionary whose keys are strings (e.g. 'foo'), Keywords (e.g. k.name) or AETypes (e.g. AEType('pnam').\n", "func_signal": "def packdict(self, val):\n", "code": "record = newrecord()\nif self.kClassKeyword in val or self.kClassType in val:\n\t# if hash contains a 'class' property containing a class name, coerce the AEDesc to that class\n\tnewval = val.copy()\n\tif self.kClassKeyword in newval:\n\t\tvalue = newval.pop(self.kClassKeyword)\n\telse:\n\t\tvalue = newval.pop(self.kClassType)\n\tif isinstance(value, Keyword): # get the corresponding AEType (assuming there is one)\n\t\tvalue = self.typebyname().get(value.name, value)\n\tif isinstance(value, aem.AEType): # coerce the record to the desired type\n\t\trecord = record.coerce(value.code)\n\t\tval = newval\nusrf = None\nfor key, value in val.items():\n\tif isinstance(key, Keyword):\n\t\ttry:\n\t\t\tkeyCode = self.typebyname()[key.AS_name].code\n\t\texcept KeyError:\n\t\t\traise KeyError(\"Unknown Keyword: k.%s\" % key.AS_name)\n\t\trecord.setparam(keyCode, self.pack(value))\n\telif isinstance(key, aem.AETypeBase): # AEType/AEProp (AEType is normally used in practice)\n\t\trecord.setparam(key.code, self.pack(value))\n\telse: # user-defined key (normally a string)\n\t\tif not usrf:\n\t\t\tusrf = newlist()\n\t\tusrf.setitem(0, self.pack(key))\n\t\tusrf.setitem(0, self.pack(value))\nif usrf:\n\trecord.setparam('usrf', usrf)\nreturn record", "path": "lib\\appscript\\reference.py", "repo_name": "laonger/iWork_Auto_Save", "stars": 1, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"Dump terminology data to Python module.\n\tapppath : str -- name or path of application\n\tmodulepath : str -- path to generated module\n\t\nGenerates a Python module containing an application's basic terminology \n(names and codes) as used by appscript.\n\nCall the dump() function to dump faulty aetes to Python module, e.g.:\n\n\tdump('MyApp', '/Library/Python/2.5/site-packages/myappglue.py')\n\nPatch any errors by hand, then import the patched module into your script \nand pass it to appscript's app() constructor via its 'terms' argument, e.g.:\n\n\tfrom appscript import *\n\timport myappglue\n\t\n\tmyapp = app('MyApp', terms=myappglue)\n\nNote that dumped terminologies aren't used by appscript's built-in help system.\n\"\"\"\n", "func_signal": "def dump(apppath, modulepath):\n", "code": "from pprint import pprint\nfrom sys import argv\n\napppath = findapp.byname(apppath)\ntables = buildtablesforaetes(ae.getappterminology(apppath))\natts = zip(('classes', 'enums', 'properties', 'elements', 'commands'), tables)\nf = open(modulepath, 'w')\nf.write('version = 1.1\\n')\nf.write('path = %r\\n' % apppath)\nfor key, value in atts:\n\tif key[0] != '_':\n\t\tf.write('\\n%s = \\\\\\n' % key)\n\t\tpprint(value, f)\nf.close()", "path": "lib\\appscript\\terminology.py", "repo_name": "laonger/iWork_Auto_Save", "stars": 1, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"Make Alias object from aem.ae.AEDesc of typeAlias (typeFSS/typeFSRef/typeFileURL are also allowed).\n\"\"\"\n", "func_signal": "def makewithdesc(klass, desc):\n", "code": "if desc.type != kae.typeAlias:\n\tdesc = desc.coerce(kae.typeAlias)\nobj = klass(_kNoPath)\nobj._desc = desc\nreturn obj", "path": "lib\\aem\\mactypes.py", "repo_name": "laonger/iWork_Auto_Save", "stars": 1, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"\n\tvalue : int | float -- the unit value, e.g. 3\n\ttype : str -- the unit type name, e.g. 'inches'\n\"\"\"\n", "func_signal": "def __init__(self, value, type):\n", "code": "self._value = value\nself._type = type", "path": "lib\\aem\\mactypes.py", "repo_name": "laonger/iWork_Auto_Save", "stars": 1, "license": "None", "language": "python", "size": 148}
{"docstring": "# Used for constructing k.keywords\n# Each argument is of format [[name, code], ...]\n", "func_signal": "def _maketypetable(classes, enums, properties):\n", "code": "typebycode = _typebycode.copy()\ntypebyname = _typebyname.copy()\n# note: testing indicates that where name+code clashes occur, classes have highest priority, followed by properties, with enums last (prior to 0.19.0 this code gave higher priority to enums):\nfor klass, table in [(AEEnum, enums), (AEType, properties), (AEType, classes)]: # note: packing properties as AEProp causes problems when the same name is used for both a class and a property, and the property's definition masks the class's one (e.g. Finder's 'file'); if an AEProp is passed where an AEType is expected, it can cause an error as it's not what the receiving app expects. (Whereas they may be more tolerant of an AEType being passed where an AEProp is expected.) Also, note that AppleScript always seems to pack property names as typeType, so we should be ok following its lead here.\n\tfor i, (name, code) in enumerate(table):\n\t\t# If an application-defined name overlaps an existing type name but has a different code, append '_' to avoid collision:\n\t\tif name in _typebyname and _typebyname[name].code != code:\n\t\t\tname += '_'\n\t\ttypebycode[code] = Keyword(name) # to handle synonyms, if same code appears more than once then use name from last definition in list\n\t\tname, code = table[-i - 1]\n\t\tif name in _typebyname and _typebyname[name].code != code:\n\t\t\tname += '_'\n\t\ttypebyname[name] = klass(code) # to handle synonyms, if same name appears more than once then use code from first definition in list\nreturn typebycode, typebyname", "path": "lib\\appscript\\terminology.py", "repo_name": "laonger/iWork_Auto_Save", "stars": 1, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"Make File object from aem.ae.AEDesc of typeFSS/typeFSRef/typeFileURL.\n\tNote: behaviour for other descriptor types is undefined: typeAlias will cause problems, others will probably fail.\n\"\"\"\n", "func_signal": "def makewithdesc(klass, desc):\n", "code": "obj = klass(_kNoPath)\nobj._path = None\nobj._url = None\nif desc.type in [kae.typeFSS, kae.typeFSRef, kae.typeFileURL]:\n\tobj._desc = desc\nelse:\n\tobj._desc = desc.coerce(kae.typeFileURL)\nreturn obj", "path": "lib\\aem\\mactypes.py", "repo_name": "laonger/iWork_Auto_Save", "stars": 1, "license": "None", "language": "python", "size": 148}
{"docstring": "# Unpack typeAERecord,  converting record keys to Keyword objects (not AETypes) where possible.\n", "func_signal": "def unpackaerecord(self, desc):\n", "code": "dct = {}\nfor i in range(desc.count()):\n\tkey, value = desc.getitem(i + 1, kae.typeWildCard)\n\tif key == 'usrf':\n\t\tlst = self.unpack(value)\n\t\tfor i in range(0, len(lst), 2):\n\t\t\tdct[lst[i]] = lst[i+1]\n\telif key in self.typebycode():\n\t\tdct[self.typebycode()[key]] = self.unpack(value)\n\telse:\n\t\tdct[aem.AEType(key)] = self.unpack(value)\nreturn dct", "path": "lib\\appscript\\reference.py", "repo_name": "laonger/iWork_Auto_Save", "stars": 1, "license": "None", "language": "python", "size": 148}
{"docstring": "# need to do some typechecking when unpacking 'contains' comparisons, so have to override the low-level unpacker\n", "func_signal": "def unpackcompdescriptor(self, desc):\n", "code": "rec = self.unpack(desc.coerce(kae.typeAERecord))\noperator = self.kAppscriptTypeCompDescriptorOperators[rec[self.keyAECompOperator].code]\nop1 = rec[self.keyAEObject1]\nop2 = rec[self.keyAEObject2]\nif operator == 'contains':\n\tif isinstance(op1, Reference) and op1.AS_aemreference.AEM_root() == aem.its:\n\t\treturn op1.contains(op2)\n\telif isinstance(op2, Reference) and op2.AS_aemreference.AEM_root() == aem.its:\n\t\treturn op2.isin(op1)\n\telse:\n\t\treturn _lowlevelcodecs.unpack(desc)\nelse:\n\treturn getattr(op1, operator)(op2)", "path": "lib\\appscript\\reference.py", "repo_name": "laonger/iWork_Auto_Save", "stars": 1, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"References may be compared for equality.\"\"\"\n", "func_signal": "def __eq__(self, v):\n", "code": "return self is v or (\n\t\tself.__class__ == v.__class__ and \n\t\tself.AEM_comparable() == v.AEM_comparable())", "path": "lib\\aem\\aemreference.py", "repo_name": "laonger/iWork_Auto_Save", "stars": 1, "license": "None", "language": "python", "size": 148}
{"docstring": "# Pack this Specifier; called by codecs.\n", "func_signal": "def AEM_packself(self, codecs):\n", "code": "desc = self._packself(codecs)\nself.AEM_packself = lambda codecs: desc # once packed, reuse this AEDesc for efficiency\nreturn desc", "path": "lib\\aem\\aemreference.py", "repo_name": "laonger/iWork_Auto_Save", "stars": 1, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"Build terminology tables from a dumped terminology module.\n\tResult : tuple of dict -- (typebycode, typebyname, referencebycode, referencebyname)\n\"\"\"\n", "func_signal": "def tablesformodule(terms):\n", "code": "return _maketypetable(terms.classes, terms.enums, terms.properties) \\\n\t\t+ _makereferencetable(terms.properties, terms.elements, terms.commands)", "path": "lib\\appscript\\terminology.py", "repo_name": "laonger/iWork_Auto_Save", "stars": 1, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"Launch a non-running application in the background and send it a 'launch' event. \n\tNote: this will only launch non-running apps that are specified by name/path/\n\tbundle id/creator type. Apps specified by other means will be still sent a \n\tlaunch event if already running, but an error will occur if they're not.\n\"\"\"\n", "func_signal": "def launch(self):\n", "code": "if not self.isrunning() and self.AS_appdata.constructor == 'path' \\\n\t\tand self.AS_appdata.relaunchmode != 'never':\n\taem.Application.launch(self.AS_appdata.identifier)\n\tself.AS_appdata.target().reconnect() # make sure aem.Application object's AEAddressDesc is up to date\nelse: # send launch event to app (will error if not already running)\n\tCommand(self, 'launch', 'ascrnoop', {})()", "path": "lib\\appscript\\reference.py", "repo_name": "laonger/iWork_Auto_Save", "stars": 1, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"\n\tcommand : Command -- command reference\n\tparameters : tuple -- two-item tuple containing tuple of positional args and dict of keyword args\n\trealerror : Exception -- the original error raised\n\tcodecs : AppData -- used to unpack error data as needed\n\"\"\"\n", "func_signal": "def __init__(self, command, parameters, realerror, codecs):\n", "code": "self.command, self. parameters, self.realerror = command, parameters, realerror\nself._codecs = codecs\nException.__init__(self, command, parameters, realerror)", "path": "lib\\appscript\\reference.py", "repo_name": "laonger/iWork_Auto_Save", "stars": 1, "license": "None", "language": "python", "size": 148}
{"docstring": "# Used for constructing references and commands\n# First two parameters are of format [[name, code], ...]\n# Last parameter is of format [name, code, direct arg type, [[arg code, arg name], ...]]\n", "func_signal": "def _makereferencetable(properties, elements, commands):\n", "code": "referencebycode = _referencebycode.copy()\nreferencebyname = _referencebyname.copy()\nfor kind, table in [(kElement, elements), (kProperty, properties)]:\n\t# note: if property and element names are same (e.g. 'file' in BBEdit), will pack as property specifier unless it's a special case (i.e. see 'text' below). Note that there is currently no way to override this, i.e. to force appscript to pack it as an all-elements specifier instead (in AS, this would be done by prepending the 'every' keyword), so clients would need to use aem for that (but could add an 'all' method to Reference class if there was demand for a built-in workaround)\n\tfor i, (name, code) in enumerate(table):\n\t\treferencebycode[kind+code] = (kind, name) # to handle synonyms, if same code appears more than once then use name from last definition in list\n\t\tname, code = table[-i - 1]\n\t\treferencebyname[name] = (kind, code) # to handle synonyms, if same name appears more than once then use code from first definition in list\nif 'text' in referencebyname: # special case: AppleScript always packs 'text of...' as all-elements specifier\n\treferencebyname['text'] = (kElement, referencebyname['text'][1])\nfor name, code, args in commands[::-1]: # to handle synonyms, if two commands have same name but different codes, only the first definition should be used (iterating over the commands list in reverse ensures this)\n\t# Avoid collisions between default commands and application-defined commands with same name but different code (e.g. 'get' and 'set' in InDesign CS2):\n\tif name in _defaultcommands and code != _defaultcommands[name][1][0]:\n\t\tname += '_'\n\treferencebyname[name] = (kCommand, (code, dict(args)))\nreturn referencebycode, referencebyname", "path": "lib\\appscript\\terminology.py", "repo_name": "laonger/iWork_Auto_Save", "stars": 1, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"Run the test harness (using the given [global] conf).\"\"\"\n", "func_signal": "def run(self, conf=None):\n", "code": "conf = conf or {}\n\n# Start the coverage tool before importing cherrypy,\n# so module-level global statements are covered.\nif self.cover:\n    self.start_coverage()\n\nif self.profile:\n    conf['profiling.on'] = True\n\nif self.validate:\n    conf['validator.on'] = True\n\nif self.conquer:\n    conf['conquer.on'] = True\n\nif self.server == 'cpmodpy':\n    from cherrypy.test import modpy\n    h = modpy.ModPythonTestHarness(self.tests, self.server,\n                                   self.protocol, self.port,\n                                   \"http\", self.interactive)\n    h.use_wsgi = False\nelif self.server == 'modpygw':\n    from cherrypy.test import modpy\n    h = modpy.ModPythonTestHarness(self.tests, self.server,\n                                   self.protocol, self.port,\n                                   \"http\", self.interactive)\n    h.use_wsgi = True\nelif self.server == 'modwsgi':\n    from cherrypy.test import modwsgi\n    h = modwsgi.ModWSGITestHarness(self.tests, self.server,\n                                   self.protocol, self.port,\n                                   \"http\", self.interactive)\n    h.use_wsgi = True\nelif self.server == 'modfcgid':\n    from cherrypy.test import modfcgid\n    h = modfcgid.FCGITestHarness(self.tests, self.server,\n                                 self.protocol, self.port,\n                                 \"http\", self.interactive)\nelse:\n    h = TestHarness(self.tests, self.server, self.protocol,\n                    self.port, self.scheme, self.interactive,\n                    self.host)\n\nsuccess = h.run(conf)\n\nif self.profile:\n    print\n    print (\"run /cherrypy/lib/profiler.py as a script to serve \"\n           \"profiling results on port 8080\")\n\nif self.cover:\n    self.stop_coverage()\n\nreturn success", "path": "cherrypy\\test\\test.py", "repo_name": "jonathanduty/fbfanpick", "stars": 1, "license": "None", "language": "python", "size": 972}
{"docstring": "'''Metaclass for declaring docstrings for class attributes.\n\nBase Python doesn't provide any syntax for setting docstrings on\n'data attributes' (non-callables). This metaclass allows class\ndefinitions to follow the declaration of a data attribute with\na docstring for that attribute; the attribute docstring will be\npopped from the class dict and folded into the class docstring.\n\nThe naming convention for attribute docstrings is:\n    <attrname> + \"__doc\".\nFor example:\n\n    class Thing(object):\n        \"\"\"A thing and its properties.\"\"\"\n        \n        __metaclass__ = cherrypy._AttributeDocstrings\n        \n        height = 50\n        height__doc = \"\"\"The height of the Thing in inches.\"\"\"\n\nIn which case, help(Thing) starts like this:\n\n    >>> help(mod.Thing)\n    Help on class Thing in module pkg.mod:\n    \n    class Thing(__builtin__.object)\n     |  A thing and its properties.\n     |  \n     |  height [= 50]:\n     |      The height of the Thing in inches.\n     | \n\nThe benefits of this approach over hand-edited class docstrings:\n    1. Places the docstring nearer to the attribute declaration.\n    2. Makes attribute docs more uniform (\"name (default): doc\").\n    3. Reduces mismatches of attribute _names_ between\n       the declaration and the documentation.\n    4. Reduces mismatches of attribute default _values_ between\n       the declaration and the documentation.\n\nThe benefits of a metaclass approach over other approaches:\n    1. Simpler (\"less magic\") than interface-based solutions.\n    2. __metaclass__ can be specified at the module global level\n       for classic classes.\n\nFor various formatting reasons, you should write multiline docs\nwith a leading newline and not a trailing one:\n    \n    response__doc = \"\"\"\n    The response object for the current thread. In the main thread,\n    and any threads which are not HTTP requests, this is None.\"\"\"\n\nThe type of the attribute is intentionally not included, because\nthat's not How Python Works. Quack.\n'''\n\n", "func_signal": "def __init__(cls, name, bases, dct):\n", "code": "newdoc = [cls.__doc__ or \"\"]\n\ndctnames = dct.keys()\ndctnames.sort()\n\nfor name in dctnames:\n    if name.endswith(\"__doc\"):\n        # Remove the magic doc attribute.\n        if hasattr(cls, name):\n            delattr(cls, name)\n        \n        # Make a uniformly-indented docstring from it.\n        val = '\\n'.join(['    ' + line.strip()\n                         for line in dct[name].split('\\n')])\n        \n        # Get the default value.\n        attrname = name[:-5]\n        try:\n            attrval = getattr(cls, attrname)\n        except AttributeError:\n            attrval = \"missing\"\n        \n        # Add the complete attribute docstring to our list.\n        newdoc.append(\"%s [= %r]:\\n%s\" % (attrname, attrval, val))\n\n# Add our list of new docstrings to the class docstring.\ncls.__doc__ = \"\\n\\n\".join(newdoc)", "path": "cherrypy\\__init__.py", "repo_name": "jonathanduty/fbfanpick", "stars": 1, "license": "None", "language": "python", "size": 972}
{"docstring": "\"\"\"Expose the function, optionally providing an alias or set of aliases.\"\"\"\n", "func_signal": "def expose(func=None, alias=None):\n", "code": "def expose_(func):\n    func.exposed = True\n    if alias is not None:\n        if isinstance(alias, basestring):\n            parents[alias.replace(\".\", \"_\")] = func\n        else:\n            for a in alias:\n                parents[a.replace(\".\", \"_\")] = func\n    return func\n\nimport sys, types\nif isinstance(func, (types.FunctionType, types.MethodType)):\n    if alias is None:\n        # @expose\n        func.exposed = True\n        return func\n    else:\n        # func = expose(func, alias)\n        parents = sys._getframe(1).f_locals\n        return expose_(func)\nelif func is None:\n    if alias is None:\n        # @expose()\n        parents = sys._getframe(1).f_locals\n        return expose_\n    else:\n        # @expose(alias=\"alias\") or\n        # @expose(alias=[\"alias1\", \"alias2\"])\n        parents = sys._getframe(1).f_locals\n        return expose_\nelse:\n    # @expose(\"alias\") or\n    # @expose([\"alias1\", \"alias2\"])\n    parents = sys._getframe(1).f_locals\n    alias = func\n    return expose_", "path": "cherrypy\\__init__.py", "repo_name": "jonathanduty/fbfanpick", "stars": 1, "license": "None", "language": "python", "size": 972}
{"docstring": "# The 'critical' on_start_resource hook is 'failsafe' (guaranteed\n# to run even if there are failures in other on_start methods).\n# This is NOT true of the other hooks.\n# Here, we have set up a failure in NumerifyTool.numerify_map,\n# but our 'critical' hook should run and set the error to 502.\n", "func_signal": "def testGuaranteedHooks(self):\n", "code": "self.getPage(\"/demo/err_in_onstart\")\nself.assertErrorPage(502)\nself.assertInBody(\"AttributeError: 'str' object has no attribute 'items'\")", "path": "cherrypy\\test\\test_tools.py", "repo_name": "jonathanduty/fbfanpick", "stars": 1, "license": "None", "language": "python", "size": 972}
{"docstring": "# request a page and check for login form\n", "func_signal": "def testSessionAuthenticate(self):\n", "code": "self.getPage('/')\nself.assertInBody('<form method=\"post\" action=\"do_login\">')\n\n# setup credentials\nlogin_body = 'username=test&password=password&from_page=/'\n\n# attempt a login\nself.getPage('/do_login', method='POST', body=login_body)\nself.assertStatus((302, 303))\n\n# get the page now that we are logged in\nself.getPage('/', self.cookies)\nself.assertBody('Hi test, you are logged in')\n\n# do a logout\nself.getPage('/do_logout', self.cookies)\nself.assertStatus((302, 303))\n\n# verify we are logged out\nself.getPage('/', self.cookies)\nself.assertInBody('<form method=\"post\" action=\"do_login\">')", "path": "cherrypy\\test\\test_sessionauthenticate.py", "repo_name": "jonathanduty/fbfanpick", "stars": 1, "license": "None", "language": "python", "size": 972}
{"docstring": "# Shamelessly stolen from StringIO\n", "func_signal": "def readlines(self, sizehint = 0):\n", "code": "total = 0\nlines = []\nline = self.readline()\nwhile line:\n    lines.append(line)\n    total += len(line)\n    if 0 < sizehint <= total:\n        break\n    line = self.readline()\nreturn lines", "path": "cherrypy\\lib\\safemime.py", "repo_name": "jonathanduty/fbfanpick", "stars": 1, "license": "None", "language": "python", "size": 972}
{"docstring": "# Dummy check_username_and_password function\n", "func_signal": "def check(username, password):\n", "code": "if username != 'test' or password != 'password':\n    return u'Wrong login/password'", "path": "cherrypy\\test\\test_sessionauthenticate.py", "repo_name": "jonathanduty/fbfanpick", "stars": 1, "license": "None", "language": "python", "size": 972}
{"docstring": "\"\"\"Check timeout on all responses. (Internal)\"\"\"\n", "func_signal": "def run(self):\n", "code": "for req, resp in self.servings:\n    resp.check_timeout()", "path": "cherrypy\\__init__.py", "repo_name": "jonathanduty/fbfanpick", "stars": 1, "license": "None", "language": "python", "size": 972}
{"docstring": "\"\"\"Wrap request.rfile in a reader that won't crash on no trailing CRLF.\"\"\"\n", "func_signal": "def safe_multipart(flash_only=False):\n", "code": "h = cherrypy.request.headers\nif not h.get('Content-Type','').startswith('multipart/'):\n    return\nif flash_only and not 'Shockwave Flash' in h.get('User-Agent', ''):\n    return\n\nclen = h.get('Content-Length', '0')\ntry:\n    clen = int(clen)\nexcept ValueError:\n    return\ncherrypy.request.rfile = MultipartWrapper(cherrypy.request.rfile, clen)", "path": "cherrypy\\lib\\safemime.py", "repo_name": "jonathanduty/fbfanpick", "stars": 1, "license": "None", "language": "python", "size": 972}
{"docstring": "\"\"\"Start the coverage tool.\n\nTo use this feature, you need to download 'coverage.py',\neither Gareth Rees' original implementation:\nhttp://www.garethrees.org/2001/12/04/python-coverage/\n\nor Ned Batchelder's enhanced version:\nhttp://www.nedbatchelder.com/code/modules/coverage.html\n\nIf neither module is found in PYTHONPATH,\ncoverage is silently(!) disabled.\n\"\"\"\n", "func_signal": "def start_coverage(self):\n", "code": "try:\n    from coverage import the_coverage as coverage\n    c = os.path.join(os.path.dirname(__file__), \"../lib/coverage.cache\")\n    coverage.cache_default = c\n    if c and os.path.exists(c):\n        os.remove(c)\n    coverage.start()\n    import cherrypy\n    from cherrypy.lib import covercp\n    cherrypy.engine.subscribe('start', covercp.start)\n    cherrypy.engine.subscribe('start_thread', covercp.start)\nexcept ImportError:\n    coverage = None\nself.coverage = coverage", "path": "cherrypy\\test\\test.py", "repo_name": "jonathanduty/fbfanpick", "stars": 1, "license": "None", "language": "python", "size": 972}
{"docstring": "\"\"\"\nReturns the Routes RequestConfig object.\n\nTo get the Routes RequestConfig:\n\n>>> from routes import *\n>>> config = request_config()\n\nThe following attributes must be set on the config object every request:\n\nmapper\n    mapper should be a Mapper instance thats ready for use\nhost\n    host is the hostname of the webapp\nprotocol\n    protocol is the protocol of the current request\nmapper_dict\n    mapper_dict should be the dict returned by mapper.match()\nredirect\n    redirect should be a function that issues a redirect, \n    and takes a url as the sole argument\nprefix (optional)\n    Set if the application is moved under a URL prefix. Prefix\n    will be stripped before matching, and prepended on generation\nenviron (optional)\n    Set to the WSGI environ for automatic prefix support if the\n    webapp is underneath a 'SCRIPT_NAME'\n    \n    Setting the environ will use information in environ to try and\n    populate the host/protocol/mapper_dict options if you've already\n    set a mapper.\n\n**Using your own requst local**\n\nIf you have your own request local object that you'd like to use instead of the default\nthread local provided by Routes, you can configure Routes to use it::\n    \n    from routes import request_config()\n    config = request_config()\n    if hasattr(config, 'using_request_local'):\n        config.request_local = YourLocalCallable\n        config = request_config()\n\nOnce you have configured request_config, its advisable you retrieve it again to get the\nobject you wanted. The variable you assign to request_local is assumed to be a callable\nthat will get the local config object you wish.\n\nThis example tests for the presence of the 'using_request_local' attribute which will be\npresent if you haven't assigned it yet. This way you can avoid repeat assignments of the\nrequest specific callable.\n\nShould you want the original object, perhaps to change the callable its using or stop\nthis behavior, call request_config(original=True).\n\"\"\"\n", "func_signal": "def request_config(original=False):\n", "code": "obj = _RequestConfig()\ntry:\n    if obj.request_local and original is False:\n        return getattr(obj, 'request_local')()\nexcept AttributeError:\n    obj.request_local = False\n    obj.using_request_local = False\nreturn _RequestConfig()", "path": "routes\\__init__.py", "repo_name": "jonathanduty/fbfanpick", "stars": 1, "license": "None", "language": "python", "size": 972}
{"docstring": "# Here we react depending on the virtualPath -- the part of the\n# path that could not be mapped to an object method. In a real\n# application, we would probably do some database lookups here\n# instead of the silly if/elif/else construct.\n", "func_signal": "def default(self, user):\n", "code": "if user == 'remi':\n    out = \"Remi Delon, CherryPy lead developer\"\nelif user == 'hendrik':\n    out = \"Hendrik Mans, CherryPy co-developer & crazy German\"\nelif user == 'lorenzo':\n    out = \"Lorenzo Lamas, famous actor and singer!\"\nelse:\n    out = \"Unknown user. :-(\"\n\nreturn '%s (<a href=\"./\">back</a>)' % out", "path": "cherrypy\\tutorial\\tut06_default_method.py", "repo_name": "jonathanduty/fbfanpick", "stars": 1, "license": "None", "language": "python", "size": 972}
{"docstring": "# Place this __file__'s grandparent (../../) at the start of sys.path,\n# so that all cherrypy/* imports are from this __file__'s package.\n", "func_signal": "def prefer_parent_path():\n", "code": "curpath = os.path.normpath(os.path.join(os.getcwd(), localDir))\ngrandparent = os.path.normpath(os.path.join(curpath, '../../'))\nif grandparent not in sys.path:\n    sys.path.insert(0, grandparent)", "path": "cherrypy\\test\\test.py", "repo_name": "jonathanduty/fbfanpick", "stars": 1, "license": "None", "language": "python", "size": 972}
{"docstring": "\"\"\"Create a Tool for safe_multipart and add it to cherrypy.tools.\"\"\"\n", "func_signal": "def init():\n", "code": "cherrypy.tools.safe_multipart = cherrypy.Tool('before_request_body',\n                                               safe_multipart)", "path": "cherrypy\\lib\\safemime.py", "repo_name": "jonathanduty/fbfanpick", "stars": 1, "license": "None", "language": "python", "size": 972}
{"docstring": "\"\"\"Stop the coverage tool, save results, and report.\"\"\"\n", "func_signal": "def stop_coverage(self):\n", "code": "import cherrypy\nfrom cherrypy.lib import covercp\ncherrypy.engine.unsubscribe('start', covercp.start)\ncherrypy.engine.unsubscribe('start_thread', covercp.start)\nif self.coverage:\n    self.coverage.save()\n    self.report_coverage()\n    print (\"run cherrypy/lib/covercp.py as a script to serve \"\n           \"coverage results on port 8080\")", "path": "cherrypy\\test\\test.py", "repo_name": "jonathanduty/fbfanpick", "stars": 1, "license": "None", "language": "python", "size": 972}
{"docstring": "\"\"\"\nLoad the protocol/server info from the environ and store it.\nAlso, match the incoming URL if there's already a mapper, and\nstore the resulting match dict in mapper_dict.\n\"\"\"\n", "func_signal": "def load_wsgi_environ(self, environ):\n", "code": "if 'HTTPS' in environ or environ.get('wsgi.url_scheme') == 'https' \\\n   or environ.get('HTTP_X_FORWARDED_PROTO') == 'https':\n    self.__shared_state.protocol = 'https'\nelse:\n    self.__shared_state.protocol = 'http'\ntry:\n    self.mapper.environ = environ\nexcept AttributeError:\n    pass\n\n# Wrap in try/except as common case is that there is a mapper\n# attached to self\ntry:\n    if 'PATH_INFO' in environ:\n        mapper = self.mapper\n        path = environ['PATH_INFO']\n        result = mapper.routematch(path)\n        if result is not None:\n            self.__shared_state.mapper_dict = result[0]\n            self.__shared_state.route = result[1]\n        else:\n            self.__shared_state.mapper_dict = None\n            self.__shared_state.route = None\nexcept AttributeError:\n    pass\n\nif 'HTTP_X_FORWARDED_HOST' in environ:\n    self.__shared_state.host = environ['HTTP_X_FORWARDED_HOST']\nelif 'HTTP_HOST' in environ:\n    self.__shared_state.host = environ['HTTP_HOST']\nelse:\n    self.__shared_state.host = environ['SERVER_NAME']\n    if environ['wsgi.url_scheme'] == 'https':\n        if environ['SERVER_PORT'] != '443':\n            self.__shared_state.host += ':' + environ['SERVER_PORT']\n    else:\n        if environ['SERVER_PORT'] != '80':\n            self.__shared_state.host += ':' + environ['SERVER_PORT']", "path": "routes\\__init__.py", "repo_name": "jonathanduty/fbfanpick", "stars": 1, "license": "None", "language": "python", "size": 972}
{"docstring": "\"\"\"Create an absolute URL for the given path.\n\nIf 'path' starts with a slash ('/'), this will return\n    (base + script_name + path + qs).\nIf it does not start with a slash, this returns\n    (base + script_name [+ request.path_info] + path + qs).\n\nIf script_name is None, cherrypy.request will be used\nto find a script_name, if available.\n\nIf base is None, cherrypy.request.base will be used (if available).\nNote that you can use cherrypy.tools.proxy to change this.\n\nFinally, note that this function can be used to obtain an absolute URL\nfor the current request path (minus the querystring) by passing no args.\nIf you call url(qs=cherrypy.request.query_string), you should get the\noriginal browser URL (assuming no internal redirections).\n\nIf relative is None or not provided, request.app.relative_urls will\nbe used (if available, else False). If False, the output will be an\nabsolute URL (including the scheme, host, vhost, and script_name).\nIf True, the output will instead be a URL that is relative to the\ncurrent request path, perhaps including '..' atoms. If relative is\nthe string 'server', the output will instead be a URL that is\nrelative to the server root; i.e., it will start with a slash.\n\"\"\"\n", "func_signal": "def url(path=\"\", qs=\"\", script_name=None, base=None, relative=None):\n", "code": "if qs:\n    qs = '?' + qs\n\nif request.app:\n    if not path.startswith(\"/\"):\n        # Append/remove trailing slash from path_info as needed\n        # (this is to support mistyped URL's without redirecting;\n        # if you want to redirect, use tools.trailing_slash).\n        pi = request.path_info\n        if request.is_index is True:\n            if not pi.endswith('/'):\n                pi = pi + '/'\n        elif request.is_index is False:\n            if pi.endswith('/') and pi != '/':\n                pi = pi[:-1]\n        \n        if path == \"\":\n            path = pi\n        else:\n            path = _urljoin(pi, path)\n    \n    if script_name is None:\n        script_name = request.script_name\n    if base is None:\n        base = request.base\n    \n    newurl = base + script_name + path + qs\nelse:\n    # No request.app (we're being called outside a request).\n    # We'll have to guess the base from server.* attributes.\n    # This will produce very different results from the above\n    # if you're using vhosts or tools.proxy.\n    if base is None:\n        base = server.base()\n    \n    path = (script_name or \"\") + path\n    newurl = base + path + qs\n\nif './' in newurl:\n    # Normalize the URL by removing ./ and ../\n    atoms = []\n    for atom in newurl.split('/'):\n        if atom == '.':\n            pass\n        elif atom == '..':\n            atoms.pop()\n        else:\n            atoms.append(atom)\n    newurl = '/'.join(atoms)\n\n# At this point, we should have a fully-qualified absolute URL.\n\nif relative is None:\n    relative = getattr(request.app, \"relative_urls\", False)\n\n# See http://www.ietf.org/rfc/rfc2396.txt\nif relative == 'server':\n    # \"A relative reference beginning with a single slash character is\n    # termed an absolute-path reference, as defined by <abs_path>...\"\n    # This is also sometimes called \"server-relative\".\n    newurl = '/' + '/'.join(newurl.split('/', 3)[3:])\nelif relative:\n    # \"A relative reference that does not begin with a scheme name\n    # or a slash character is termed a relative-path reference.\"\n    old = url().split('/')[:-1]\n    new = newurl.split('/')\n    while old and new:\n        a, b = old[0], new[0]\n        if a != b:\n            break\n        old.pop(0)\n        new.pop(0)\n    new = (['..'] * len(old)) + new\n    newurl = '/'.join(new)\n\nreturn newurl", "path": "cherrypy\\__init__.py", "repo_name": "jonathanduty/fbfanpick", "stars": 1, "license": "None", "language": "python", "size": 972}
{"docstring": "\"\"\"Mount the given root, start the builtin server (and engine), then block.\n\nroot: an instance of a \"controller class\" (a collection of page handler\n    methods) which represents the root of the application.\nscript_name: a string containing the \"mount point\" of the application.\n    This should start with a slash, and be the path portion of the URL\n    at which to mount the given root. For example, if root.index() will\n    handle requests to \"http://www.example.com:8080/dept/app1/\", then\n    the script_name argument would be \"/dept/app1\".\n    \n    It MUST NOT end in a slash. If the script_name refers to the root\n    of the URI, it MUST be an empty string (not \"/\").\nconfig: a file or dict containing application config. If this contains\n    a [global] section, those entries will be used in the global\n    (site-wide) config.\n\"\"\"\n", "func_signal": "def quickstart(root=None, script_name=\"\", config=None):\n", "code": "if config:\n    _global_conf_alias.update(config)\n\nif root is not None:\n    tree.mount(root, script_name, config)\n\nif hasattr(engine, \"signal_handler\"):\n    engine.signal_handler.subscribe()\nif hasattr(engine, \"console_control_handler\"):\n    engine.console_control_handler.subscribe()\n\nengine.start()\nengine.block()", "path": "cherrypy\\__init__.py", "repo_name": "jonathanduty/fbfanpick", "stars": 1, "license": "None", "language": "python", "size": 972}
{"docstring": "\"\"\"\nIf the name is environ, load the wsgi envion with load_wsgi_environ\nand set the environ\n\"\"\"\n", "func_signal": "def __setattr__(self, name, value):\n", "code": "if name == 'environ':\n    self.load_wsgi_environ(value)\n    return self.__shared_state.__setattr__(name, value)\nreturn self.__shared_state.__setattr__(name, value)", "path": "routes\\__init__.py", "repo_name": "jonathanduty/fbfanpick", "stars": 1, "license": "None", "language": "python", "size": 972}
{"docstring": "\"\"\"Given an object or a path to an object, get the object and its name.\"\"\"\n", "func_signal": "def _cherrypy_pydoc_resolve(thing, forceload=0):\n", "code": "if isinstance(thing, _ThreadLocalProxy):\n    thing = getattr(serving, thing.__attrname__)\nreturn _pydoc._builtin_resolve(thing, forceload)", "path": "cherrypy\\__init__.py", "repo_name": "jonathanduty/fbfanpick", "stars": 1, "license": "None", "language": "python", "size": 972}
{"docstring": "\"\"\"Make this an accepting state with the given action. If \nthere is already an action, choose the action with highest\npriority.\"\"\"\n", "func_signal": "def set_action(self, action, priority):\n", "code": "if priority > self.action_priority:\n  self.action = action\n  self.action_priority = priority", "path": "lib\\Plex\\Machines.py", "repo_name": "bluemoon/shovel", "stars": 1, "license": "None", "language": "python", "size": 388}
{"docstring": "''' this prints debugging code in a fashionable way. '''\n", "func_signal": "def debug(self, string, level=NONE):\n", "code": "assert string, \"there is no debug string\"\ndebug(string, level)", "path": "core\\api.py", "repo_name": "bluemoon/shovel", "stars": 1, "license": "None", "language": "python", "size": 388}
{"docstring": "''' puts a global variable in storage so that it\n    can be retrieved from anywhere in the code '''\n", "func_signal": "def putGlobal(self, glbal, value):\n", "code": "assert glbal, \"there was no global input\"\nassert value, \"there was no value\"\n        \nself.config.PutConfig(glbal, value)", "path": "core\\api.py", "repo_name": "bluemoon/shovel", "stars": 1, "license": "None", "language": "python", "size": 388}
{"docstring": "#print \"Destroying\", self ###\n", "func_signal": "def destroy(self):\n", "code": "self.transitions = None\nself.action = None\nself.epsilon_closure = None", "path": "lib\\Plex\\Machines.py", "repo_name": "bluemoon/shovel", "stars": 1, "license": "None", "language": "python", "size": 388}
{"docstring": "''' change to a directory and emit debugging code.\n    arguments: directory -- the directory you want to change to\n'''\n", "func_signal": "def chdir(self, directory):\n", "code": "assert directory, \"no directory variable passed, or value is None\"\n_file.chdir(directory)", "path": "core\\api.py", "repo_name": "bluemoon/shovel", "stars": 1, "license": "None", "language": "python", "size": 388}
{"docstring": "#print \"Destroying\", self ###\n", "func_signal": "def __del__(self):\n", "code": "for state in self.states:\n  state.destroy()", "path": "lib\\Plex\\Machines.py", "repo_name": "bluemoon/shovel", "stars": 1, "license": "None", "language": "python", "size": 388}
{"docstring": "\"\"\"Add a new state to the machine and return it.\"\"\"\n", "func_signal": "def new_state(self):\n", "code": "s = Node()\nn = self.next_state_number\nself.next_state_number = n + 1\ns.number = n\nself.states.append(s)\nreturn s", "path": "lib\\Plex\\Machines.py", "repo_name": "bluemoon/shovel", "stars": 1, "license": "None", "language": "python", "size": 388}
{"docstring": "# Preinitialise the list of empty transitions, because\n# the nfa-to-dfa algorithm needs it\n#self.transitions = {'':[]}\n", "func_signal": "def __init__(self):\n", "code": "self.transitions = TransitionMap()\nself.action_priority = LOWEST_PRIORITY", "path": "lib\\Plex\\Machines.py", "repo_name": "bluemoon/shovel", "stars": 1, "license": "None", "language": "python", "size": 388}
{"docstring": "''' creates a folder if it isnt already present \n    arguments: arguments -- the folder you want to be created in list form\n'''\n\n", "func_signal": "def mkdirIfAbsent(self, *arguments):\n", "code": "assert arguments, \"no argument variable passed, or value is None\"\n_file.mkdirIfAbsent(arguments)", "path": "core\\api.py", "repo_name": "bluemoon/shovel", "stars": 1, "license": "None", "language": "python", "size": 388}
{"docstring": "''' the debug method, a printer for debugging with verbosity control'''\n", "func_signal": "def debug(string, level=DEBUG):\n", "code": "config = configurator()\n## Get the frames so we know who's calling us from what line\ncurrent = inspect.currentframe(0)\n# print inspect.getframeinfo(current)\n# print inspect.stack(current)\nouter   = inspect.getouterframes(current)\n\n## get the debug level from our global class\ndLevel = int(config.getGlobal('debug'))\n## Keep a marker so we dont print the same thing more than once\nhasPrinted = False\n\nif dLevel > NONE:\n    if dLevel >= level and not hasPrinted:\n        hasPrinted = True\n        _dPrint(level, string, outer)\n\n## we must explicitly delete the frame inspector or it will cause \n## unnecessary garbage\ndel current", "path": "core\\debug.py", "repo_name": "bluemoon/shovel", "stars": 1, "license": "None", "language": "python", "size": 388}
{"docstring": "''' assemble a path based on the base path. \n    arguments: \n        base -- the base path \n        *path -- list input that adds the path to the base path\n'''\n", "func_signal": "def buildPath(self, base, *path):\n", "code": "assert base, \"no base variable passed, or value is None\"\nassert path, \"no path variable passed, or value is None\"\n_file.buildPath(base, path)", "path": "core\\api.py", "repo_name": "bluemoon/shovel", "stars": 1, "license": "None", "language": "python", "size": 388}
{"docstring": "''' formats the code so that the string is on the left and \n    the label goes on the right side. with padding in the center\n'''\n", "func_signal": "def pprint(self, string, label, color=None, labelColor=None):\n", "code": "assert string, \"there is no string value, or it is None!\"\nassert lable,  \"there is no label or it is None!\"\npprint(string, label, color, labelColor)", "path": "core\\api.py", "repo_name": "bluemoon/shovel", "stars": 1, "license": "None", "language": "python", "size": 388}
{"docstring": "''' an internal method that converts the numerical value to\n    a usable string. '''\n\n", "func_signal": "def _levelToString(level):\n", "code": "if level == 0:\n    return 'NONE'\nelif level == 1:\n    return 'WARNING'\nelif level == 2:\n    return 'INFO'\nelif level == 3:\n    return 'DEBUG'\n\n## Errors\nelif level == -1:\n    return 'ERROR'", "path": "core\\debug.py", "repo_name": "bluemoon/shovel", "stars": 1, "license": "None", "language": "python", "size": 388}
{"docstring": "''' Create a directory if its not there '''\n", "func_signal": "def mkdirIfAbsent(*args):\n", "code": "for dirName in args:\n    debug(\"ensuring that dir exists: %s\" % dirName,DEBUG)\n    if not os.path.exists(dirName):\n        debug(\"creating dir: %s\" % dirName,DEBUG)\n        os.makedirs(dirName)", "path": "core\\file.py", "repo_name": "bluemoon/shovel", "stars": 1, "license": "None", "language": "python", "size": 388}
{"docstring": "''' touch a file, like the unix command touch\n    arguments: file_ -- the file/path to file to be changed\n'''\n", "func_signal": "def touch(self, file_):\n", "code": "assert file_, \"no file variable passed, or value is None\"\n_file.touch(file_)", "path": "core\\api.py", "repo_name": "bluemoon/shovel", "stars": 1, "license": "None", "language": "python", "size": 388}
{"docstring": "## fancy fancy dictionary comprehension \n", "func_signal": "def dependencyGenRun(self):\n", "code": "lstDict = dict((k, set(self.depList[k])) for k in self.depList)\nR = []\nwhile lstDict:\n    t = set(i for v in lstDict.values() for i in v)-set(lstDict.keys())\n    t.update(k for k, v in lstDict.items() if not v)\n    R.append(t)\n    lstDict = dict(((k, v-t) for k, v in lstDict.items() if v))\nreturn R", "path": "core\\dependencies.py", "repo_name": "bluemoon/shovel", "stars": 1, "license": "None", "language": "python", "size": 388}
{"docstring": "\"\"\"dostring for __init__\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self.splitString = '.'\nself.config = configurator()", "path": "core\\plugin.py", "repo_name": "bluemoon/shovel", "stars": 1, "license": "None", "language": "python", "size": 388}
{"docstring": "''' accepts list input as in ['usr','home'] and ['usr','home','stuff']\n    where the second input directory2 is the longer one.\n'''\n", "func_signal": "def pathDifference(directory1, directory2):\n", "code": "counter = 0\nwhile 1:\n    try:\n        if directory1[counter] == directory2[counter]:\n            pass\n        else:\n            return directory2[counter:]\n            break\n            \n    except IndexError:\n        return directory2[counter:]\n        break\n        \n    counter = counter + 1", "path": "core\\plugin.py", "repo_name": "bluemoon/shovel", "stars": 1, "license": "None", "language": "python", "size": 388}
{"docstring": "''' accepts list input as in ['usr','home'] and ['usr','home','stuff']\n    where the second input directory2 is the longer one.\n'''\n", "func_signal": "def pathDifference(directory1, directory2):\n", "code": "counter = 0\nwhile 1:\n    try:\n        if directory1[counter] == directory2[counter]:\n            pass\n        else:\n            return directory2[counter:]\n            break\n            \n    except IndexError:\n        return directory2[counter:]\n        break\n        \n    counter = counter + 1", "path": "tests\\test_core_plugin.py", "repo_name": "bluemoon/shovel", "stars": 1, "license": "None", "language": "python", "size": 388}
{"docstring": "''' Recursively remove a directory '''\n", "func_signal": "def rmDirectoryRecursive(path):\n", "code": "for i in os.listdir(path):\n    fullPath = path + \"/\" + i\n    if os.path.isdir(fullPath):\n        ## Recursively delete\n        rmDirectoryRecursive(fullPath)\n    \n\n## Finally delete the parent\nos.rmdir(path)", "path": "core\\file.py", "repo_name": "bluemoon/shovel", "stars": 1, "license": "None", "language": "python", "size": 388}
{"docstring": "\"\"\"\nAdd the given list of model field names to the set of fields to\nexclude from loading from the database when automatic column selection\nis done. The new field names are added to any existing field names that\nare deferred (or removed from any existing field names that are marked\nas the only ones for immediate loading).\n\"\"\"\n# Fields on related models are stored in the literal double-underscore\n# format, so that we can use a set datastructure. We do the foo__bar\n# splitting and handling when computing the SQL colum names (as part of\n# get_columns()).\n", "func_signal": "def add_deferred_loading(self, field_names):\n", "code": "existing, defer = self.deferred_loading\nif defer:\n    # Add to existing deferred names.\n    self.deferred_loading = existing.union(field_names), True\nelse:\n    # Remove names from the set of any existing \"immediate load\" names.\n    self.deferred_loading = existing.difference(field_names), False", "path": "lib\\python2.5\\django\\db\\models\\sql\\query.py", "repo_name": "rodericj/Location-Integration", "stars": 1, "license": "None", "language": "python", "size": 12264}
{"docstring": "\"\"\"\nReturns the field name and direction for an order specification. For\nexample, '-foo' is returned as ('foo', 'DESC').\n\nThe 'default' param is used to indicate which way no prefix (or a '+'\nprefix) should sort. The '-' prefix always sorts the opposite way.\n\"\"\"\n", "func_signal": "def get_order_dir(field, default='ASC'):\n", "code": "dirn = ORDER_DIR[default]\nif field[0] == '-':\n    return field[1:], dirn[1]\nreturn field, dirn[0]", "path": "lib\\python2.5\\django\\db\\models\\sql\\query.py", "repo_name": "rodericj/Location-Integration", "stars": 1, "license": "None", "language": "python", "size": 12264}
{"docstring": "\"\"\"\nChanges the aliases in change_map (which maps old-alias -> new-alias),\nrelabelling any references to them in select columns and the where\nclause.\n\"\"\"\n", "func_signal": "def change_aliases(self, change_map):\n", "code": "assert set(change_map.keys()).intersection(set(change_map.values())) == set()\n\n# 1. Update references in \"select\" (normal columns plus aliases),\n# \"group by\", \"where\" and \"having\".\nself.where.relabel_aliases(change_map)\nself.having.relabel_aliases(change_map)\nfor columns in (self.select, self.aggregates.values(), self.group_by or []):\n    for pos, col in enumerate(columns):\n        if isinstance(col, (list, tuple)):\n            old_alias = col[0]\n            columns[pos] = (change_map.get(old_alias, old_alias), col[1])\n        else:\n            col.relabel_aliases(change_map)\n\n# 2. Rename the alias in the internal table/alias datastructures.\nfor old_alias, new_alias in change_map.iteritems():\n    alias_data = list(self.alias_map[old_alias])\n    alias_data[RHS_ALIAS] = new_alias\n\n    t = self.rev_join_map[old_alias]\n    data = list(self.join_map[t])\n    data[data.index(old_alias)] = new_alias\n    self.join_map[t] = tuple(data)\n    self.rev_join_map[new_alias] = t\n    del self.rev_join_map[old_alias]\n    self.alias_refcount[new_alias] = self.alias_refcount[old_alias]\n    del self.alias_refcount[old_alias]\n    self.alias_map[new_alias] = tuple(alias_data)\n    del self.alias_map[old_alias]\n\n    table_aliases = self.table_map[alias_data[TABLE_NAME]]\n    for pos, alias in enumerate(table_aliases):\n        if alias == old_alias:\n            table_aliases[pos] = new_alias\n            break\n    for pos, alias in enumerate(self.tables):\n        if alias == old_alias:\n            self.tables[pos] = new_alias\n            break\nfor key, alias in self.included_inherited_models.items():\n    if alias in change_map:\n        self.included_inherited_models[key] = change_map[alias]\n\n# 3. Update any joins that refer to the old alias.\nfor alias, data in self.alias_map.iteritems():\n    lhs = data[LHS_ALIAS]\n    if lhs in change_map:\n        data = list(data)\n        data[LHS_ALIAS] = change_map[lhs]\n        self.alias_map[alias] = tuple(data)", "path": "lib\\python2.5\\django\\db\\models\\sql\\query.py", "repo_name": "rodericj/Location-Integration", "stars": 1, "license": "None", "language": "python", "size": 12264}
{"docstring": "\"\"\"\nConverts the query to do count(...) or count(distinct(pk)) in order to\nget its size.\n\"\"\"\n", "func_signal": "def add_count_column(self):\n", "code": "if not self.distinct:\n    if not self.select:\n        count = self.aggregates_module.Count('*', is_summary=True)\n    else:\n        assert len(self.select) == 1, \\\n                \"Cannot add count col with multiple cols in 'select': %r\" % self.select\n        count = self.aggregates_module.Count(self.select[0])\nelse:\n    opts = self.model._meta\n    if not self.select:\n        count = self.aggregates_module.Count((self.join((None, opts.db_table, None, None)), opts.pk.column),\n                                 is_summary=True, distinct=True)\n    else:\n        # Because of SQL portability issues, multi-column, distinct\n        # counts need a sub-query -- see get_count() for details.\n        assert len(self.select) == 1, \\\n                \"Cannot add count col with multiple cols in 'select'.\"\n\n        count = self.aggregates_module.Count(self.select[0], distinct=True)\n    # Distinct handling is done in Count(), so don't do it at this\n    # level.\n    self.distinct = False\n\n# Set only aggregate to be the count column.\n# Clear out the select cache to reflect the new unmasked aggregates.\nself.aggregates = {None: count}\nself.set_aggregate_mask(None)\nself.group_by = None", "path": "lib\\python2.5\\django\\db\\models\\sql\\query.py", "repo_name": "rodericj/Location-Integration", "stars": 1, "license": "None", "language": "python", "size": 12264}
{"docstring": "\"\"\"\nAdds a single aggregate expression to the Query\n\"\"\"\n", "func_signal": "def add_aggregate(self, aggregate, model, alias, is_summary):\n", "code": "opts = model._meta\nfield_list = aggregate.lookup.split(LOOKUP_SEP)\nif (len(field_list) == 1 and\n    aggregate.lookup in self.aggregates.keys()):\n    # Aggregate is over an annotation\n    field_name = field_list[0]\n    col = field_name\n    source = self.aggregates[field_name]\n    if not is_summary:\n        raise FieldError(\"Cannot compute %s('%s'): '%s' is an aggregate\" % (\n            aggregate.name, field_name, field_name))\nelif ((len(field_list) > 1) or\n    (field_list[0] not in [i.name for i in opts.fields]) or\n    self.group_by is None or\n    not is_summary):\n    # If:\n    #   - the field descriptor has more than one part (foo__bar), or\n    #   - the field descriptor is referencing an m2m/m2o field, or\n    #   - this is a reference to a model field (possibly inherited), or\n    #   - this is an annotation over a model field\n    # then we need to explore the joins that are required.\n\n    field, source, opts, join_list, last, _ = self.setup_joins(\n        field_list, opts, self.get_initial_alias(), False)\n\n    # Process the join chain to see if it can be trimmed\n    col, _, join_list = self.trim_joins(source, join_list, last, False)\n\n    # If the aggregate references a model or field that requires a join,\n    # those joins must be LEFT OUTER - empty join rows must be returned\n    # in order for zeros to be returned for those aggregates.\n    for column_alias in join_list:\n        self.promote_alias(column_alias, unconditional=True)\n\n    col = (join_list[-1], col)\nelse:\n    # The simplest cases. No joins required -\n    # just reference the provided column alias.\n    field_name = field_list[0]\n    source = opts.get_field(field_name)\n    col = field_name\n\n# Add the aggregate to the query\nalias = truncate_name(alias, self.connection.ops.max_name_length())\naggregate.add_to_query(self, alias, col=col, source=source, is_summary=is_summary)", "path": "lib\\python2.5\\django\\db\\models\\sql\\query.py", "repo_name": "rodericj/Location-Integration", "stars": 1, "license": "None", "language": "python", "size": 12264}
{"docstring": "\"\"\"\nReturns a tuple representing the SQL elements in the \"group by\" clause.\n\"\"\"\n", "func_signal": "def get_grouping(self):\n", "code": "qn = self.quote_name_unless_alias\nresult, params = [], []\nif self.group_by is not None:\n    group_by = self.group_by or []\n\n    extra_selects = []\n    for extra_select, extra_params in self.extra_select.itervalues():\n        extra_selects.append(extra_select)\n        params.extend(extra_params)\n    for col in group_by + self.related_select_cols + extra_selects:\n        if isinstance(col, (list, tuple)):\n            result.append('%s.%s' % (qn(col[0]), qn(col[1])))\n        elif hasattr(col, 'as_sql'):\n            result.append(col.as_sql(qn))\n        else:\n            result.append(str(col))\nreturn result, params", "path": "lib\\python2.5\\django\\db\\models\\sql\\query.py", "repo_name": "rodericj/Location-Integration", "stars": 1, "license": "None", "language": "python", "size": 12264}
{"docstring": "\"\"\"\nReturns the dictionary with the values of the existing aggregations.\n\"\"\"\n", "func_signal": "def get_aggregation(self):\n", "code": "if not self.aggregate_select:\n    return {}\n\n# If there is a group by clause, aggregating does not add useful\n# information but retrieves only the first row. Aggregate\n# over the subquery instead.\nif self.group_by is not None:\n    from subqueries import AggregateQuery\n    query = AggregateQuery(self.model, self.connection)\n\n    obj = self.clone()\n\n    # Remove any aggregates marked for reduction from the subquery\n    # and move them to the outer AggregateQuery.\n    for alias, aggregate in self.aggregate_select.items():\n        if aggregate.is_summary:\n            query.aggregate_select[alias] = aggregate\n            del obj.aggregate_select[alias]\n\n    query.add_subquery(obj)\nelse:\n    query = self\n    self.select = []\n    self.default_cols = False\n    self.extra_select = {}\n    self.remove_inherited_models()\n\nquery.clear_ordering(True)\nquery.clear_limits()\nquery.select_related = False\nquery.related_select_cols = []\nquery.related_select_fields = []\n\nresult = query.execute_sql(SINGLE)\nif result is None:\n    result = [None for q in query.aggregate_select.items()]\n\nreturn dict([\n    (alias, self.resolve_aggregate(val, aggregate))\n    for (alias, aggregate), val\n    in zip(query.aggregate_select.items(), result)\n])", "path": "lib\\python2.5\\django\\db\\models\\sql\\query.py", "repo_name": "rodericj/Location-Integration", "stars": 1, "license": "None", "language": "python", "size": 12264}
{"docstring": "\"\"\"\nGiven a \"before\" copy of the alias_refcounts dictionary (as\n'initial_refcounts') and a collection of aliases that may have been\nchanged or created, works out which aliases have been created since\nthen and which ones haven't been used and promotes all of those\naliases, plus any children of theirs in the alias tree, to outer joins.\n\"\"\"\n# FIXME: There's some (a lot of!) overlap with the similar OR promotion\n# in add_filter(). It's not quite identical, but is very similar. So\n# pulling out the common bits is something for later.\n", "func_signal": "def promote_unused_aliases(self, initial_refcounts, used_aliases):\n", "code": "considered = {}\nfor alias in self.tables:\n    if alias not in used_aliases:\n        continue\n    if (alias not in initial_refcounts or\n            self.alias_refcount[alias] == initial_refcounts[alias]):\n        parent = self.alias_map[alias][LHS_ALIAS]\n        must_promote = considered.get(parent, False)\n        promoted = self.promote_alias(alias, must_promote)\n        considered[alias] = must_promote or promoted", "path": "lib\\python2.5\\django\\db\\models\\sql\\query.py", "repo_name": "rodericj/Location-Integration", "stars": 1, "license": "None", "language": "python", "size": 12264}
{"docstring": "\"\"\"\nSets up the select_related data structure so that we only select\ncertain related models (as opposed to all models, when\nself.select_related=True).\n\"\"\"\n", "func_signal": "def add_select_related(self, fields):\n", "code": "field_dict = {}\nfor field in fields:\n    d = field_dict\n    for part in field.split(LOOKUP_SEP):\n        d = d.setdefault(part, {})\nself.select_related = field_dict\nself.related_select_cols = []\nself.related_select_fields = []", "path": "lib\\python2.5\\django\\db\\models\\sql\\query.py", "repo_name": "rodericj/Location-Integration", "stars": 1, "license": "None", "language": "python", "size": 12264}
{"docstring": "\"\"\"\nReturns the query as a string of SQL with the parameter values\nsubstituted in.\n\nParameter values won't necessarily be quoted correctly, since that is\ndone by the database interface at execution time.\n\"\"\"\n", "func_signal": "def __str__(self):\n", "code": "sql, params = self.as_sql()\nreturn sql % params", "path": "lib\\python2.5\\django\\db\\models\\sql\\query.py", "repo_name": "rodericj/Location-Integration", "stars": 1, "license": "None", "language": "python", "size": 12264}
{"docstring": "\"\"\"\nAdd the given list of model field names to the set of fields to\nretrieve when the SQL is executed (\"immediate loading\" fields). The\nfield names replace any existing immediate loading field names. If\nthere are field names already specified for deferred loading, those\nnames are removed from the new field_names before storing the new names\nfor immediate loading. (That is, immediate loading overrides any\nexisting immediate values, but respects existing deferrals.)\n\"\"\"\n", "func_signal": "def add_immediate_loading(self, field_names):\n", "code": "existing, defer = self.deferred_loading\nif defer:\n    # Remove any existing deferred names from the current set before\n    # setting the new names.\n    self.deferred_loading = set(field_names).difference(existing), False\nelse:\n    # Replace any existing \"immediate load\" field names.\n    self.deferred_loading = set(field_names), False", "path": "lib\\python2.5\\django\\db\\models\\sql\\query.py", "repo_name": "rodericj/Location-Integration", "stars": 1, "license": "None", "language": "python", "size": 12264}
{"docstring": "\"\"\"\nRun the query against the database and returns the result(s). The\nreturn value is a single data item if result_type is SINGLE, or an\niterator over the results if the result_type is MULTI.\n\nresult_type is either MULTI (use fetchmany() to retrieve all rows),\nSINGLE (only retrieve a single row), or None. In this last case, the\ncursor is returned if any query is executed, since it's used by\nsubclasses such as InsertQuery). It's possible, however, that no query\nis needed, as the filters describe an empty set. In that case, None is\nreturned, to avoid any unnecessary database interaction.\n\"\"\"\n", "func_signal": "def execute_sql(self, result_type=MULTI):\n", "code": "try:\n    sql, params = self.as_sql()\n    if not sql:\n        raise EmptyResultSet\nexcept EmptyResultSet:\n    if result_type == MULTI:\n        return empty_iter()\n    else:\n        return\ncursor = self.connection.cursor()\ncursor.execute(sql, params)\n\nif not result_type:\n    return cursor\nif result_type == SINGLE:\n    if self.ordering_aliases:\n        return cursor.fetchone()[:-len(results.ordering_aliases)]\n    return cursor.fetchone()\n\n# The MULTI case.\nif self.ordering_aliases:\n    result = order_modified_iter(cursor, len(self.ordering_aliases),\n            self.connection.features.empty_fetchmany_value)\nelse:\n    result = iter((lambda: cursor.fetchmany(GET_ITERATOR_CHUNK_SIZE)),\n            self.connection.features.empty_fetchmany_value)\nif not self.connection.features.can_use_chunked_reads:\n    # If we are using non-chunked reads, we return the same data\n    # structure as normally, but ensure it is all read into memory\n    # before going any further.\n    return list(result)\nreturn result", "path": "lib\\python2.5\\django\\db\\models\\sql\\query.py", "repo_name": "rodericj/Location-Integration", "stars": 1, "license": "None", "language": "python", "size": 12264}
{"docstring": "\"\"\"\nCreates a copy of the current instance. The 'kwargs' parameter can be\nused by clients to update attributes after copying has taken place.\n\"\"\"\n", "func_signal": "def clone(self, klass=None, **kwargs):\n", "code": "obj = Empty()\nobj.__class__ = klass or self.__class__\nobj.model = self.model\nobj.connection = self.connection\nobj.alias_refcount = self.alias_refcount.copy()\nobj.alias_map = self.alias_map.copy()\nobj.table_map = self.table_map.copy()\nobj.join_map = self.join_map.copy()\nobj.rev_join_map = self.rev_join_map.copy()\nobj.quote_cache = {}\nobj.default_cols = self.default_cols\nobj.default_ordering = self.default_ordering\nobj.standard_ordering = self.standard_ordering\nobj.included_inherited_models = self.included_inherited_models.copy()\nobj.ordering_aliases = []\nobj.select_fields = self.select_fields[:]\nobj.related_select_fields = self.related_select_fields[:]\nobj.dupe_avoidance = self.dupe_avoidance.copy()\nobj.select = self.select[:]\nobj.tables = self.tables[:]\nobj.where = deepcopy(self.where)\nobj.where_class = self.where_class\nif self.group_by is None:\n    obj.group_by = None\nelse:\n    obj.group_by = self.group_by[:]\nobj.having = deepcopy(self.having)\nobj.order_by = self.order_by[:]\nobj.low_mark, obj.high_mark = self.low_mark, self.high_mark\nobj.distinct = self.distinct\nobj.select_related = self.select_related\nobj.related_select_cols = []\nobj.aggregates = deepcopy(self.aggregates)\nif self.aggregate_select_mask is None:\n    obj.aggregate_select_mask = None\nelse:\n    obj.aggregate_select_mask = self.aggregate_select_mask[:]\nif self._aggregate_select_cache is None:\n    obj._aggregate_select_cache = None\nelse:\n    obj._aggregate_select_cache = self._aggregate_select_cache.copy()\nobj.max_depth = self.max_depth\nobj.extra_select = self.extra_select.copy()\nobj.extra_tables = self.extra_tables\nobj.extra_where = self.extra_where\nobj.extra_params = self.extra_params\nobj.extra_order_by = self.extra_order_by\nobj.deferred_loading = deepcopy(self.deferred_loading)\nif self.filter_is_sticky and self.used_aliases:\n    obj.used_aliases = self.used_aliases.copy()\nelse:\n    obj.used_aliases = set()\nobj.filter_is_sticky = False\nobj.__dict__.update(kwargs)\nif hasattr(obj, '_setup_query'):\n    obj._setup_query()\nreturn obj", "path": "lib\\python2.5\\django\\db\\models\\sql\\query.py", "repo_name": "rodericj/Location-Integration", "stars": 1, "license": "None", "language": "python", "size": 12264}
{"docstring": "\"\"\"\nYields blocks of rows from a cursor. We use this iterator in the special\ncase when extra output columns have been added to support ordering\nrequirements. We must trim those extra columns before anything else can use\nthe results, since they're only needed to make the SQL valid.\n\"\"\"\n", "func_signal": "def order_modified_iter(cursor, trim, sentinel):\n", "code": "for rows in iter((lambda: cursor.fetchmany(GET_ITERATOR_CHUNK_SIZE)),\n        sentinel):\n    yield [r[:-trim] for r in rows]", "path": "lib\\python2.5\\django\\db\\models\\sql\\query.py", "repo_name": "rodericj/Location-Integration", "stars": 1, "license": "None", "language": "python", "size": 12264}
{"docstring": "\"\"\"\nSometimes joins at the end of a multi-table sequence can be trimmed. If\nthe final join is against the same column as we are comparing against,\nand is an inner join, we can go back one step in a join chain and\ncompare against the LHS of the join instead (and then repeat the\noptimization). The result, potentially, involves less table joins.\n\nThe 'target' parameter is the final field being joined to, 'join_list'\nis the full list of join aliases.\n\nThe 'last' list contains offsets into 'join_list', corresponding to\neach component of the filter.  Many-to-many relations, for example, add\ntwo tables to the join list and we want to deal with both tables the\nsame way, so 'last' has an entry for the first of the two tables and\nthen the table immediately after the second table, in that case.\n\nThe 'trim' parameter forces the final piece of the join list to be\ntrimmed before anything. See the documentation of add_filter() for\ndetails about this.\n\nReturns the final active column and table alias and the new active\njoin_list.\n\"\"\"\n", "func_signal": "def trim_joins(self, target, join_list, last, trim):\n", "code": "final = len(join_list)\npenultimate = last.pop()\nif penultimate == final:\n    penultimate = last.pop()\nif trim and len(join_list) > 1:\n    extra = join_list[penultimate:]\n    join_list = join_list[:penultimate]\n    final = penultimate\n    penultimate = last.pop()\n    col = self.alias_map[extra[0]][LHS_JOIN_COL]\n    for alias in extra:\n        self.unref_alias(alias)\nelse:\n    col = target.column\nalias = join_list[-1]\nwhile final > 1:\n    join = self.alias_map[alias]\n    if col != join[RHS_JOIN_COL] or join[JOIN_TYPE] != self.INNER:\n        break\n    self.unref_alias(alias)\n    alias = join[LHS_ALIAS]\n    col = join[LHS_JOIN_COL]\n    join_list = join_list[:-1]\n    final -= 1\n    if final == penultimate:\n        penultimate = last.pop()\nreturn col, alias, join_list", "path": "lib\\python2.5\\django\\db\\models\\sql\\query.py", "repo_name": "rodericj/Location-Integration", "stars": 1, "license": "None", "language": "python", "size": 12264}
{"docstring": "\"\"\"\nAdds a Q-object to the current filter.\n\nCan also be used to add anything that has an 'add_to_query()' method.\n\"\"\"\n", "func_signal": "def add_q(self, q_object, used_aliases=None):\n", "code": "if used_aliases is None:\n    used_aliases = self.used_aliases\nif hasattr(q_object, 'add_to_query'):\n    # Complex custom objects are responsible for adding themselves.\n    q_object.add_to_query(self, used_aliases)\nelse:\n    if self.where and q_object.connector != AND and len(q_object) > 1:\n        self.where.start_subtree(AND)\n        subtree = True\n    else:\n        subtree = False\n    connector = AND\n    for child in q_object.children:\n        if connector == OR:\n            refcounts_before = self.alias_refcount.copy()\n        if isinstance(child, Node):\n            self.where.start_subtree(connector)\n            self.add_q(child, used_aliases)\n            self.where.end_subtree()\n        else:\n            self.add_filter(child, connector, q_object.negated,\n                    can_reuse=used_aliases)\n        if connector == OR:\n            # Aliases that were newly added or not used at all need to\n            # be promoted to outer joins if they are nullable relations.\n            # (they shouldn't turn the whole conditional into the empty\n            # set just because they don't match anything).\n            self.promote_unused_aliases(refcounts_before, used_aliases)\n        connector = q_object.connector\n    if q_object.negated:\n        self.where.negate()\n    if subtree:\n        self.where.end_subtree()\nif self.filter_is_sticky:\n    self.used_aliases = used_aliases", "path": "lib\\python2.5\\django\\db\\models\\sql\\query.py", "repo_name": "rodericj/Location-Integration", "stars": 1, "license": "None", "language": "python", "size": 12264}
{"docstring": "\"\"\"\nAdds data to the various extra_* attributes for user-created additions\nto the query.\n\"\"\"\n", "func_signal": "def add_extra(self, select, select_params, where, params, tables, order_by):\n", "code": "if select:\n    # We need to pair any placeholder markers in the 'select'\n    # dictionary with their parameters in 'select_params' so that\n    # subsequent updates to the select dictionary also adjust the\n    # parameters appropriately.\n    select_pairs = SortedDict()\n    if select_params:\n        param_iter = iter(select_params)\n    else:\n        param_iter = iter([])\n    for name, entry in select.items():\n        entry = force_unicode(entry)\n        entry_params = []\n        pos = entry.find(\"%s\")\n        while pos != -1:\n            entry_params.append(param_iter.next())\n            pos = entry.find(\"%s\", pos + 2)\n        select_pairs[name] = (entry, entry_params)\n    # This is order preserving, since self.extra_select is a SortedDict.\n    self.extra_select.update(select_pairs)\nif where:\n    self.extra_where += tuple(where)\nif params:\n    self.extra_params += tuple(params)\nif tables:\n    self.extra_tables += tuple(tables)\nif order_by:\n    self.extra_order_by = order_by", "path": "lib\\python2.5\\django\\db\\models\\sql\\query.py", "repo_name": "rodericj/Location-Integration", "stars": 1, "license": "None", "language": "python", "size": 12264}
{"docstring": "\"\"\"\nUnpickling support.\n\"\"\"\n# Rebuild list of field instances\n", "func_signal": "def __setstate__(self, obj_dict):\n", "code": "obj_dict['select_fields'] = [\n    name is not None and obj_dict['model']._meta.get_field(name) or None\n    for name in obj_dict['select_fields']\n]\n\nself.__dict__.update(obj_dict)\n# XXX: Need a better solution for this when multi-db stuff is\n# supported. It's the only class-reference to the module-level\n# connection variable.\nself.connection = connection", "path": "lib\\python2.5\\django\\db\\models\\sql\\query.py", "repo_name": "rodericj/Location-Integration", "stars": 1, "license": "None", "language": "python", "size": 12264}
{"docstring": "\"\"\"\nAdds the given (model) fields to the select set. The field names are\nadded in the order specified.\n\"\"\"\n", "func_signal": "def add_fields(self, field_names, allow_m2m=True):\n", "code": "alias = self.get_initial_alias()\nopts = self.get_meta()\n\ntry:\n    for name in field_names:\n        field, target, u2, joins, u3, u4 = self.setup_joins(\n                name.split(LOOKUP_SEP), opts, alias, False, allow_m2m,\n                True)\n        final_alias = joins[-1]\n        col = target.column\n        if len(joins) > 1:\n            join = self.alias_map[final_alias]\n            if col == join[RHS_JOIN_COL]:\n                self.unref_alias(final_alias)\n                final_alias = join[LHS_ALIAS]\n                col = join[LHS_JOIN_COL]\n                joins = joins[:-1]\n        self.promote_alias_chain(joins[1:])\n        self.select.append((final_alias, col))\n        self.select_fields.append(field)\nexcept MultiJoin:\n    raise FieldError(\"Invalid field name: '%s'\" % name)\nexcept FieldError:\n    names = opts.get_all_field_names() + self.extra_select.keys() + self.aggregate_select.keys()\n    names.sort()\n    raise FieldError(\"Cannot resolve keyword %r into field. \"\n            \"Choices are: %s\" % (name, \", \".join(names)))\nself.remove_inherited_models()", "path": "lib\\python2.5\\django\\db\\models\\sql\\query.py", "repo_name": "rodericj/Location-Integration", "stars": 1, "license": "None", "language": "python", "size": 12264}
{"docstring": "\"\"\"\nA helper function to add \"value\" to the set of values for \"key\", whether or\nnot \"key\" already exists.\n\"\"\"\n", "func_signal": "def add_to_dict(data, key, value):\n", "code": "if key in data:\n    data[key].add(value)\nelse:\n    data[key] = set([value])", "path": "lib\\python2.5\\django\\db\\models\\sql\\query.py", "repo_name": "rodericj/Location-Integration", "stars": 1, "license": "None", "language": "python", "size": 12264}
{"docstring": "# Gui related stuff\n", "func_signal": "def __init__(self, image_name, description, initial_mask=None, order=None, authors=[]):\n", "code": "self.image_name = image_name\nself.pixbuf = load_image(image_name, (36, 36))\nself.description = description\nself.initial_mask = initial_mask\nself.order = order\nself.authors = []\n# actions\nself.keyboard_actions = []\nself.mouse_actions = []\nself.handler_id = None", "path": "lib\\zeobuilder\\actions\\collections\\interactive.py", "repo_name": "woutersmet/Zeo_thesis", "stars": 1, "license": "gpl-3.0", "language": "python", "size": 1392}
{"docstring": "# A) calling ancestor\n", "func_signal": "def analyze_selection(parameters=None):\n", "code": "if not MoveObjects.analyze_selection(parameters): return False\n# B) validating and initialising\ndestination = context.application.cache.drag_destination\nif not isinstance(destination, GLMixin): return False\nfor Class in context.application.cache.classes:\n    if not issubclass(Class, GLMixin): return False\n# C) passed all tests:\nreturn True", "path": "share\\plugins\\basic\\drag.py", "repo_name": "woutersmet/Zeo_thesis", "stars": 1, "license": "gpl-3.0", "language": "python", "size": 1392}
{"docstring": "# When the selected object is an atom, show the extra button.\n", "func_signal": "def on_cb_vector_changed(self, combo):\n", "code": "if (self.vector_store.get_value(self.cb_vector.get_active_iter(),0)==\"Bond\"):\n    self.cb_bondtype.show()\nelse:\n    self.cb_bondtype.hide()", "path": "share\\plugins\\molecular\\sketch.py", "repo_name": "woutersmet/Zeo_thesis", "stars": 1, "license": "gpl-3.0", "language": "python", "size": 1392}
{"docstring": "# A) calling ancestor\n", "func_signal": "def analyze_selection():\n", "code": "if not Immediate.analyze_selection(): return False\n# B) own tests\nif len(context.application.cache.nodes) == 0: return False\n# C) passed all tests:\nreturn True", "path": "share\\plugins\\basic\\extra.py", "repo_name": "woutersmet/Zeo_thesis", "stars": 1, "license": "gpl-3.0", "language": "python", "size": 1392}
{"docstring": "# A) calling ancestor\n", "func_signal": "def analyze_selection(parameters=None):\n", "code": "if not ImmediateWithMemory.analyze_selection(parameters): return False\n# B) validating and initialising\ndestination = context.application.cache.drag_destination\nif not isinstance(destination, ContainerMixin): return False\nfor Class in context.application.cache.classes:\n    if not destination.check_add(Class): return False\nif context.application.cache.recursive_drag: return False\n# C) passed all tests:\nreturn True", "path": "share\\plugins\\basic\\drag.py", "repo_name": "woutersmet/Zeo_thesis", "stars": 1, "license": "gpl-3.0", "language": "python", "size": 1392}
{"docstring": "# A) calling ancestor\n", "func_signal": "def analyze_selection():\n", "code": "if not UngroupBase.analyze_selection(): return False\n# B) validating\ncache = context.application.cache\nFolder = context.application.plugins.get_node(\"Folder\")\nfor Class in cache.classes:\n    if not issubclass(Class, Folder): return False\nif not isinstance(cache.parent, Folder): return False\n# C) passed all tests\nreturn True", "path": "share\\plugins\\basic\\arrange.py", "repo_name": "woutersmet/Zeo_thesis", "stars": 1, "license": "gpl-3.0", "language": "python", "size": 1392}
{"docstring": "# A) calling ancestor\n", "func_signal": "def analyze_selection():\n", "code": "if not Immediate.analyze_selection(): return False\n# B) validating\ncache = context.application.cache\nif len(cache.nodes) == 0: return False\nfor cls in cache.classes:\n    if issubclass(cls, Reference): return False\n# C) passed all tests:\nreturn True", "path": "share\\plugins\\basic\\object.py", "repo_name": "woutersmet/Zeo_thesis", "stars": 1, "license": "gpl-3.0", "language": "python", "size": 1392}
{"docstring": "# A) collect all extra attributes of the selected nodes\n", "func_signal": "def do(self):\n", "code": "extra_attrs = {}\nfor node in context.application.cache.nodes:\n    for key, value in node.extra.iteritems():\n        other_value = extra_attrs.get(key)\n        if other_value is None:\n            extra_attrs[key] = value\n        elif isinstance(other_value, Undefined):\n            continue\n        elif other_value != value:\n            extra_attrs[key] = Undefined()\nfor key, value in extra_attrs.items():\n    if isinstance(value, Undefined):\n        continue\n    for node in context.application.cache.nodes:\n        other_value = node.extra.get(key)\n        if other_value != value:\n            extra_attrs[key] = Undefined()\n            break\n\n# B) present the extra attributes in a popup window with editing features\nextra_dialog = ExtraDialog()\nmodified_extra_attrs, remove_keys = extra_dialog.run(extra_attrs)\nif len(modified_extra_attrs) > 0 or len(remove_keys) > 0:\n    # C) the modified extra attributes are applied to the select objects\n    for node in context.application.cache.nodes:\n        new_value = node.extra.copy()\n        new_value.update(modified_extra_attrs)\n        for remove_key in remove_keys:\n            new_value.pop(remove_key, None)\n        primitive.SetProperty(node, \"extra\", new_value)", "path": "share\\plugins\\basic\\extra.py", "repo_name": "woutersmet/Zeo_thesis", "stars": 1, "license": "gpl-3.0", "language": "python", "size": 1392}
{"docstring": "# A) calling ancestor\n", "func_signal": "def analyze_selection(NewParentClass):\n", "code": "if not Immediate.analyze_selection(): return False\n# B) validating\ncache = context.application.cache\nold_parent = cache.parent\nif old_parent is None: return False\nif not isinstance(old_parent, ContainerMixin): return False\nif not old_parent.check_add(NewParentClass): return False\nif cache.some_nodes_fixed: return False\nfor Class in cache.classes:\n    if not NewParentClass.check_add(Class): return False\n# C) Passed all tests\nreturn True", "path": "share\\plugins\\basic\\arrange.py", "repo_name": "woutersmet/Zeo_thesis", "stars": 1, "license": "gpl-3.0", "language": "python", "size": 1392}
{"docstring": "# A) calling ancestor\n", "func_signal": "def analyze_selection():\n", "code": "if not Immediate.analyze_selection(): return False\n# B) validating\ncache = context.application.cache\nif cache.node is None: return False\ncontains_atoms = False\nfor cls in cache.child_classes:\n    if issubclass(cls, Atom):\n        contains_atoms = True\n        break\nif not contains_atoms: return False\n# C) passed all tests\nreturn True", "path": "share\\plugins\\molecular\\atom.py", "repo_name": "woutersmet/Zeo_thesis", "stars": 1, "license": "gpl-3.0", "language": "python", "size": 1392}
{"docstring": "# A) calling ancestor\n", "func_signal": "def analyze_selection():\n", "code": "if not Immediate.analyze_selection(): return False\n# B) validating\ncache = context.application.cache\nAtom = context.application.plugins.get_node(\"Atom\")\nif not isinstance(cache.node, GLContainerMixin): return False\nsome_atoms = False\nfor cls in cache.child_classes:\n    if issubclass(cls, Atom):\n        some_atoms = True\n        break\nif not some_atoms: return False\n# C) passed all tests:\nreturn True", "path": "share\\plugins\\molecular\\atom.py", "repo_name": "woutersmet/Zeo_thesis", "stars": 1, "license": "gpl-3.0", "language": "python", "size": 1392}
{"docstring": "# A) calling ancestor\n", "func_signal": "def analyze_selection():\n", "code": "if not OneLevelHigherBase.analyze_selection(): return False\n# B) validating\nif not isinstance(context.application.cache.parent, GLFrameBase): return False\n# C) passed all tests\nreturn True", "path": "share\\plugins\\basic\\arrange.py", "repo_name": "woutersmet/Zeo_thesis", "stars": 1, "license": "gpl-3.0", "language": "python", "size": 1392}
{"docstring": "# A) calling ancestor\n", "func_signal": "def analyze_selection():\n", "code": "if not UngroupBase.analyze_selection(): return False\n# B) validating\ncache = context.application.cache\nFrame = context.application.plugins.get_node(\"Frame\")\nfor Class in cache.classes:\n    if not issubclass(Class, Frame): return False\nif not isinstance(cache.parent, GLContainerMixin): return False\n# C) passed all tests\nreturn True", "path": "share\\plugins\\basic\\arrange.py", "repo_name": "woutersmet/Zeo_thesis", "stars": 1, "license": "gpl-3.0", "language": "python", "size": 1392}
{"docstring": "# A) calling ancestor\n", "func_signal": "def analyze_selection():\n", "code": "if not Immediate.analyze_selection(): return False\n# B) validating\ncache = context.application.cache\nif cache.node is None: return False\ncontains_atoms = False\nfor cls in cache.child_classes:\n    if issubclass(cls, Atom):\n        contains_atoms = True\n        break\nif not contains_atoms: return False\n# C) passed all tests\nreturn True", "path": "share\\plugins\\molecular\\atom.py", "repo_name": "woutersmet/Zeo_thesis", "stars": 1, "license": "gpl-3.0", "language": "python", "size": 1392}
{"docstring": "# A) calling ancestor\n", "func_signal": "def analyze_selection():\n", "code": "if not Immediate.analyze_selection(): return False\n# B) validating\ncache = context.application.cache\nnew_parent = cache.parent\nif new_parent is None: return False\nfor Class in cache.classes:\n    if not issubclass(Class, ContainerMixin): return False\nif cache.some_children_fixed: return False\nfor Class in cache.child_classes:\n    if not new_parent.check_add(Class): return False\n# C) passed all tests\nreturn True", "path": "share\\plugins\\basic\\arrange.py", "repo_name": "woutersmet/Zeo_thesis", "stars": 1, "license": "gpl-3.0", "language": "python", "size": 1392}
{"docstring": "# A) calling ancestor\n", "func_signal": "def analyze_selection():\n", "code": "if not OneLevelHigherRelative.analyze_selection(): return False\n# B) validating\nif not isinstance(context.application.cache.parent.parent, GLContainerMixin): return False\n# C) passed all tests\nreturn True", "path": "share\\plugins\\basic\\arrange.py", "repo_name": "woutersmet/Zeo_thesis", "stars": 1, "license": "gpl-3.0", "language": "python", "size": 1392}
{"docstring": "# A) calling ancestor\n", "func_signal": "def analyze_selection():\n", "code": "if not Immediate.analyze_selection(): return False\n# B) validating\n\n# C) passed all tests:\nreturn True", "path": "share\\plugins\\molecular\\fragment.py", "repo_name": "woutersmet/Zeo_thesis", "stars": 1, "license": "gpl-3.0", "language": "python", "size": 1392}
{"docstring": "# A) calling ancestor\n", "func_signal": "def analyze_selection(parameters=None):\n", "code": "if not ImmediateWithMemory.analyze_selection(parameters): return False\n# B) validating and initialising\ndestination = context.application.cache.drag_destination\nif parameters.child_index != -1: return False\nif not isinstance(destination, Reference): return False\nif not destination.check_target(context.application.cache.node): return False\n# C) passed all tests:\nreturn True", "path": "share\\plugins\\basic\\drag.py", "repo_name": "woutersmet/Zeo_thesis", "stars": 1, "license": "gpl-3.0", "language": "python", "size": 1392}
{"docstring": "# When the selected object is an atom, show the extra button.\n", "func_signal": "def on_cb_object_changed(self, combo):\n", "code": "self.hbox_atoms.hide()\nself.la_current.hide()\nself.hbox_fragments.hide()\nself.doing_fragment = False\nif(self.object_store.get_value(self.cb_object.get_active_iter(),0)==\"Atom\"):\n    self.hbox_atoms.show()\n    self.la_current.show()\nif(self.object_store.get_value(self.cb_object.get_active_iter(),0)==\"Fragment\"):\n    self.doing_fragment = True\n    self.cb_fragment.show()\n    #self.la_fragment.show()\n    self.hbox_fragments.show()", "path": "share\\plugins\\molecular\\sketch.py", "repo_name": "woutersmet/Zeo_thesis", "stars": 1, "license": "gpl-3.0", "language": "python", "size": 1392}
{"docstring": "# Only do something with the right mouse button.\n", "func_signal": "def on_tree_view_button_press_event(self, tree_view, event):\n", "code": "tree_view.grab_focus()\ntemp = tree_view.get_path_at_pos(int(event.x), int(event.y))\nif temp is None: return False\npath, col, cellx, celly = temp\nif event.button == 3:\n    if not self.tree_selection.path_is_selected(path):\n        tree_view.set_cursor(path, col, 0)\n        context.application.cache.emit_invalidate()\n    context.application.menu.popup(event.button, gtk.get_current_event_time())\n    return True\nelif event.button == 1:\n    if event.type == gtk.gdk._2BUTTON_PRESS:\n        context.application.action_manager.default_action(\n            context.application.model[path][0]\n        )\n        return True\nreturn False", "path": "lib\\zeobuilder\\gui\\main.py", "repo_name": "woutersmet/Zeo_thesis", "stars": 1, "license": "gpl-3.0", "language": "python", "size": 1392}
{"docstring": "\"\"\"_parse_like()\"\"\"\n\n", "func_signal": "def test_parse_like(self):\n", "code": "for case, expected in self.like_cases:\n    self.assertEqual(self.api._parse_like(case), expected)", "path": "src\\friendfeed-pyapi\\tests\\friendfeed_tests.py", "repo_name": "pbutler/RoomRanker", "stars": 1, "license": "None", "language": "python", "size": 418}
{"docstring": "\"\"\"_parse_date()\"\"\"\n\n", "func_signal": "def test_parse_date(self):\n", "code": "date = self.api._parse_date('2008-12-18T20:36:49Z')\nself.assertEqual(\n    date,\n    datetime.datetime(2008, 12, 18, 20, 36, 49)\n)", "path": "src\\friendfeed-pyapi\\tests\\friendfeed_tests.py", "repo_name": "pbutler/RoomRanker", "stars": 1, "license": "None", "language": "python", "size": 418}
{"docstring": "\"\"\"minimal __init__()\"\"\"\n\n", "func_signal": "def test_minimal_init(self):\n", "code": "comment = friendfeed.Comment(\n        '411b7034-da96-496d-bec3-e64e2527a997')\nexpected = friendfeed.Comment(\n        id='411b7034-da96-496d-bec3-e64e2527a997',\n        date=None,\n        user=None,\n        body=None,\n        via={}\n        )\nself.assertEqual(comment, expected)", "path": "src\\friendfeed-pyapi\\tests\\friendfeed_tests.py", "repo_name": "pbutler/RoomRanker", "stars": 1, "license": "None", "language": "python", "size": 418}
{"docstring": "\"\"\"_parse_service()\"\"\"\n\n", "func_signal": "def test_parse_service(self):\n", "code": "for case, expected in self.service_cases:\n    self.assertEqual(\n            self.api._parse_service(case),\n            expected\n    )", "path": "src\\friendfeed-pyapi\\tests\\friendfeed_tests.py", "repo_name": "pbutler/RoomRanker", "stars": 1, "license": "None", "language": "python", "size": 418}
{"docstring": "\"\"\"_parse_enclosures()\"\"\"\n\n", "func_signal": "def test_parse_enclosures(self):\n", "code": "enclosures = None\nself.assertEqual(\n        self.api._parse_enclosures(enclosures),\n        []\n)\nenclosures = [{'foo': 'bar'}]\nself.assertEqual(\n        self.api._parse_enclosures(enclosures),\n        enclosures\n)", "path": "src\\friendfeed-pyapi\\tests\\friendfeed_tests.py", "repo_name": "pbutler/RoomRanker", "stars": 1, "license": "None", "language": "python", "size": 418}
{"docstring": "\"\"\"_parse_users() simple\"\"\"\n\n\n", "func_signal": "def test_parse_users_simple(self):\n", "code": "cases, expected = self.separate_cases_and_expected(\n        self.user_profile_cases)\nself.assertEqual(\n        self.api._parse_users(cases),\n        expected\n)", "path": "src\\friendfeed-pyapi\\tests\\friendfeed_tests.py", "repo_name": "pbutler/RoomRanker", "stars": 1, "license": "None", "language": "python", "size": 418}
{"docstring": "\"\"\"_parse_subscriptions_iter()\"\"\"\n\n", "func_signal": "def test_parse_subscriptions_iter(self):\n", "code": "cases = [\n        self.user_profile_cases[0],\n        self.imaginary_friend_cases[0]\n        ]\ncases, expected = self.separate_cases_and_expected(cases)\nself.assertEqual(\n        list(self.api._parse_subscriptions_iter(cases)),\n        expected\n)", "path": "src\\friendfeed-pyapi\\tests\\friendfeed_tests.py", "repo_name": "pbutler/RoomRanker", "stars": 1, "license": "None", "language": "python", "size": 418}
{"docstring": "\"\"\"_check_for_error()\"\"\"\n\n", "func_signal": "def test_check_for_error(self):\n", "code": "error_cases = [\n    ('bad-id-format', friendfeed.BadIdFormatError),\n    ('bad-url-format', friendfeed.BadUrlFormatError),\n    ('entry-not-found', friendfeed.EntryNotFoundError),\n    ('entry-required', friendfeed.EntryRequiredError),\n    ('forbidden', friendfeed.ForbiddenError),\n    ('image-format-not-supported',\n        friendfeed.ImageFormatNotSupportedError),\n    ('internal-server-error',\n        friendfeed.InternalServerErrorError),\n    ('limit-exceeded', friendfeed.LimitExceededError),\n    ('room-not-found', friendfeed.RoomNotFoundError),\n    ('room-required', friendfeed.RoomRequiredError),\n    ('title-required', friendfeed.TitleRequiredError),\n    ('unauthorized', friendfeed.UnauthorizedError),\n    ('user-not-found', friendfeed.UserNotFoundError),\n    ('user-required', friendfeed.UserRequiredError),\n    ('error', friendfeed.FriendFeedError)\n]\n\nfor case, expected_error in error_cases:\n    self.assertRaises(\n            expected_error,\n            self.api._check_for_error,\n            {'errorCode': case}\n    )", "path": "src\\friendfeed-pyapi\\tests\\friendfeed_tests.py", "repo_name": "pbutler/RoomRanker", "stars": 1, "license": "None", "language": "python", "size": 418}
{"docstring": "\"\"\"_parse_user() full\"\"\"\n\n", "func_signal": "def test_parse_user_full(self):\n", "code": "case = self.user_profile_cases[0][0]\nsub_case, sub_expected = self.user_profile_cases[1]\ncase['subscriptions'] = [sub_case]\nservice_case, service_expected = self.service_cases[0]\ncase['services'] = [service_case]\nroom_case, room_expected = self.room_cases[0]\ncase['rooms'] = [room_case]\nsub_list_case, sub_list_expected = self.list_cases[0]\ncase['lists'] = [sub_list_case]\ncase['status'] = 'public'\nexpected = friendfeed.User(\n        id=case['id'],\n        name=case['name'],\n        nickname=case['nickname'],\n        private=False,\n        profile_url=case['profileUrl'],\n        rooms=[room_expected],\n        lists=[sub_list_expected],\n        services=[service_expected],\n        subscriptions=[sub_expected]\n        )\n\nresult = self.api._parse_user(case)\nself.assertEqual(result, expected)", "path": "src\\friendfeed-pyapi\\tests\\friendfeed_tests.py", "repo_name": "pbutler/RoomRanker", "stars": 1, "license": "None", "language": "python", "size": 418}
{"docstring": "\"\"\"_parse_room() full\"\"\"\n\n", "func_signal": "def test_parse_room_full(self):\n", "code": "case = self.room_cases[0][0]\ncase['status'] = 'private'\nuser_case, user_expected = self.user_profile_cases[0]\ncase['administrators'] = [user_case]\ncase['members'] = [user_case]\nexpected = friendfeed.Room(\n        id=case['id'],\n        name=case['name'],\n        nickname=case['nickname'],\n        url=case['url'],\n        private=True,\n        description=case['description'],\n        administrators=[user_expected],\n        members=[user_expected]\n        )\n\nresult = self.api._parse_room(case)\nself.assertEqual(result, expected)", "path": "src\\friendfeed-pyapi\\tests\\friendfeed_tests.py", "repo_name": "pbutler/RoomRanker", "stars": 1, "license": "None", "language": "python", "size": 418}
{"docstring": "\"\"\"_parse_likes()\"\"\"\n\n", "func_signal": "def test_parse_likes(self):\n", "code": "cases, expected = self.separate_cases_and_expected(\n        self.like_cases)\nself.assertEqual(\n        self.api._parse_likes(cases),\n        expected\n)", "path": "src\\friendfeed-pyapi\\tests\\friendfeed_tests.py", "repo_name": "pbutler/RoomRanker", "stars": 1, "license": "None", "language": "python", "size": 418}
{"docstring": "\"\"\"_parse_media_files()\"\"\"\n\n", "func_signal": "def test_parse_media_files(self):\n", "code": "cases, expected = self.separate_cases_and_expected(\n        self.media_cases)\nself.assertEqual(\n        self.api._parse_media_files(cases),\n        expected\n)", "path": "src\\friendfeed-pyapi\\tests\\friendfeed_tests.py", "repo_name": "pbutler/RoomRanker", "stars": 1, "license": "None", "language": "python", "size": 418}
{"docstring": "\"\"\"_parse_entries_iter()\"\"\"\n\n", "func_signal": "def test_parse_entries_iter(self):\n", "code": "cases, expected = self.separate_cases_and_expected(\n        self.entry_cases)\nself.assertEqual(\n        list(self.api._parse_entries_iter(cases)),\n        expected\n)", "path": "src\\friendfeed-pyapi\\tests\\friendfeed_tests.py", "repo_name": "pbutler/RoomRanker", "stars": 1, "license": "None", "language": "python", "size": 418}
{"docstring": "\"\"\"minimal __init__()\"\"\"\n\n", "func_signal": "def test_minimal_init(self):\n", "code": "user = friendfeed.User('gotgenes')\nexpected = friendfeed.User(\n        nickname='gotgenes',\n        id=None,\n        profile_url=None,\n        private=False,\n        services=[],\n        subscriptions=[],\n        rooms=[],\n        lists=[]\n        )\n\nself.assertEqual(user, expected)", "path": "src\\friendfeed-pyapi\\tests\\friendfeed_tests.py", "repo_name": "pbutler/RoomRanker", "stars": 1, "license": "None", "language": "python", "size": 418}
{"docstring": "\"\"\"_make_attrs_dict()\"\"\"\n\n", "func_signal": "def test_make_attrs_dict(self):\n", "code": "case = {\n        'a': 1,\n        'b': [2, 3]\n}\nattrs_map = {'a': 'other_a'}\nmethod_map = {'b': ('other_b', lambda x: sum(x))}\nexpected = {\n        'other_a': 1,\n        'other_b': 5\n}\nself.assertEqual(\n        self.api._make_attrs_dict(case, attrs_map, method_map),\n        expected\n)", "path": "src\\friendfeed-pyapi\\tests\\friendfeed_tests.py", "repo_name": "pbutler/RoomRanker", "stars": 1, "license": "None", "language": "python", "size": 418}
{"docstring": "\"\"\"_parse_media_file()\"\"\"\n\n", "func_signal": "def test_parse_media_file(self):\n", "code": "for case, expected in self.media_cases:\n    self.assertEqual(\n            self.api._parse_media_file(case),\n            expected\n    )", "path": "src\\friendfeed-pyapi\\tests\\friendfeed_tests.py", "repo_name": "pbutler/RoomRanker", "stars": 1, "license": "None", "language": "python", "size": 418}
{"docstring": "\"\"\"_parse_services()\"\"\"\n\n", "func_signal": "def test_parse_services(self):\n", "code": "cases, expected = self.separate_cases_and_expected(\n        self.service_cases)\nself.assertEqual(\n        self.api._parse_services(cases),\n        expected\n)", "path": "src\\friendfeed-pyapi\\tests\\friendfeed_tests.py", "repo_name": "pbutler/RoomRanker", "stars": 1, "license": "None", "language": "python", "size": 418}
{"docstring": "\"\"\"_parse_user() simple\"\"\"\n\n", "func_signal": "def test_parse_user_simple(self):\n", "code": "for case, expected in self.user_profile_cases:\n    self.assertEqual(\n            self.api._parse_user(case),\n            expected\n    )", "path": "src\\friendfeed-pyapi\\tests\\friendfeed_tests.py", "repo_name": "pbutler/RoomRanker", "stars": 1, "license": "None", "language": "python", "size": 418}
{"docstring": "\"\"\"_parse_room() simple\"\"\"\n\n", "func_signal": "def test_parse_room_simple(self):\n", "code": "for case, expected in self.room_cases:\n    self.assertEqual(\n            self.api._parse_room(case),\n            expected\n    )", "path": "src\\friendfeed-pyapi\\tests\\friendfeed_tests.py", "repo_name": "pbutler/RoomRanker", "stars": 1, "license": "None", "language": "python", "size": 418}
{"docstring": "\"\"\"minimal __init__()\"\"\"\n\n", "func_signal": "def test_minimal_init(self):\n", "code": "entry = friendfeed.Entry('6bu83hda-3bc1-55fc-150b-c52c41cd158a')\nexpected = friendfeed.Entry(\n        id='6bu83hda-3bc1-55fc-150b-c52c41cd158a',\n        title=None,\n        link=None,\n        published=None,\n        updated=None,\n        hidden=False,\n        anonymous=False,\n        user=None,\n        service=None,\n        comments=[],\n        likes=[],\n        media=[],\n        via={},\n        room=None,\n        friend_of=None,\n        geo={}\n        )\nself.assertEqual(entry, expected)", "path": "src\\friendfeed-pyapi\\tests\\friendfeed_tests.py", "repo_name": "pbutler/RoomRanker", "stars": 1, "license": "None", "language": "python", "size": 418}
{"docstring": "# test locale support for __format__ code 'n'\n\n", "func_signal": "def test_float__format__locale(self):\n", "code": "for i in range(-10, 10):\n    x = 1234567890.0 * (10.0 ** i)\n    self.assertEqual(locale.format('%g', x, grouping=True), format(x, 'n'))\n    self.assertEqual(locale.format('%.10g', x, grouping=True), format(x, '.10n'))", "path": "py3k\\Lib\\test\\test_types.py", "repo_name": "jcsalterego/py3k-atsign", "stars": 1, "license": "None", "language": "python", "size": 13436}
{"docstring": "# 0-127\n", "func_signal": "def test_codecs_charmap(self):\n", "code": "s = bytes(range(128))\nfor encoding in (\n    'cp037', 'cp1026',\n    'cp437', 'cp500', 'cp737', 'cp775', 'cp850',\n    'cp852', 'cp855', 'cp860', 'cp861', 'cp862',\n    'cp863', 'cp865', 'cp866',\n    'iso8859_10', 'iso8859_13', 'iso8859_14', 'iso8859_15',\n    'iso8859_2', 'iso8859_3', 'iso8859_4', 'iso8859_5', 'iso8859_6',\n    'iso8859_7', 'iso8859_9', 'koi8_r', 'latin_1',\n    'mac_cyrillic', 'mac_latin2',\n\n    'cp1250', 'cp1251', 'cp1252', 'cp1253', 'cp1254', 'cp1255',\n    'cp1256', 'cp1257', 'cp1258',\n    'cp856', 'cp857', 'cp864', 'cp869', 'cp874',\n\n    'mac_greek', 'mac_iceland','mac_roman', 'mac_turkish',\n    'cp1006', 'iso8859_8',\n\n    ### These have undefined mappings:\n    #'cp424',\n\n    ### These fail the round-trip:\n    #'cp875'\n\n    ):\n    self.assertEqual(str(s, encoding).encode(encoding), s)\n\n# 128-255\ns = bytes(range(128, 256))\nfor encoding in (\n    'cp037', 'cp1026',\n    'cp437', 'cp500', 'cp737', 'cp775', 'cp850',\n    'cp852', 'cp855', 'cp860', 'cp861', 'cp862',\n    'cp863', 'cp865', 'cp866',\n    'iso8859_10', 'iso8859_13', 'iso8859_14', 'iso8859_15',\n    'iso8859_2', 'iso8859_4', 'iso8859_5',\n    'iso8859_9', 'koi8_r', 'latin_1',\n    'mac_cyrillic', 'mac_latin2',\n\n    ### These have undefined mappings:\n    #'cp1250', 'cp1251', 'cp1252', 'cp1253', 'cp1254', 'cp1255',\n    #'cp1256', 'cp1257', 'cp1258',\n    #'cp424', 'cp856', 'cp857', 'cp864', 'cp869', 'cp874',\n    #'iso8859_3', 'iso8859_6', 'iso8859_7',\n    #'mac_greek', 'mac_iceland','mac_roman', 'mac_turkish',\n\n    ### These fail the round-trip:\n    #'cp1006', 'cp875', 'iso8859_8',\n\n    ):\n    self.assertEqual(str(s, encoding).encode(encoding), s)", "path": "py3k\\Lib\\test\\test_unicode.py", "repo_name": "jcsalterego/py3k-atsign", "stars": 1, "license": "None", "language": "python", "size": 13436}
{"docstring": "# Testing Unicode contains method\n", "func_signal": "def test_contains(self):\n", "code": "self.assert_('a' in 'abdb')\nself.assert_('a' in 'bdab')\nself.assert_('a' in 'bdaba')\nself.assert_('a' in 'bdba')\nself.assert_('a' not in 'bdb')\nself.assert_('a' in 'bdba')\nself.assert_('a' in ('a',1,None))\nself.assert_('a' in (1,None,'a'))\nself.assert_('a' in ('a',1,None))\nself.assert_('a' in (1,None,'a'))\nself.assert_('a' not in ('x',1,'y'))\nself.assert_('a' not in ('x',1,None))\nself.assert_('abcd' not in 'abcxxxx')\nself.assert_('ab' in 'abcd')\nself.assert_('ab' in 'abc')\nself.assert_('ab' in (1,None,'ab'))\nself.assert_('' in 'abc')\nself.assert_('' in '')\nself.assert_('' in 'abc')\nself.assert_('\\0' not in 'abc')\nself.assert_('\\0' in '\\0abc')\nself.assert_('\\0' in 'abc\\0')\nself.assert_('a' in '\\0abc')\nself.assert_('asdf' in 'asdf')\nself.assert_('asdf' not in 'asd')\nself.assert_('asdf' not in '')\n\nself.assertRaises(TypeError, \"abc\".__contains__)", "path": "py3k\\Lib\\test\\test_unicode.py", "repo_name": "jcsalterego/py3k-atsign", "stars": 1, "license": "None", "language": "python", "size": 13436}
{"docstring": "# Error handling (encoding)\n", "func_signal": "def test_codecs_errors(self):\n", "code": "self.assertRaises(UnicodeError, 'Andr\\202 x'.encode, 'ascii')\nself.assertRaises(UnicodeError, 'Andr\\202 x'.encode, 'ascii','strict')\nself.assertEqual('Andr\\202 x'.encode('ascii','ignore'), b\"Andr x\")\nself.assertEqual('Andr\\202 x'.encode('ascii','replace'), b\"Andr? x\")\n\n# Error handling (decoding)\nself.assertRaises(UnicodeError, str, b'Andr\\202 x', 'ascii')\nself.assertRaises(UnicodeError, str, b'Andr\\202 x', 'ascii', 'strict')\nself.assertEqual(str(b'Andr\\202 x', 'ascii', 'ignore'), \"Andr x\")\nself.assertEqual(str(b'Andr\\202 x', 'ascii', 'replace'), 'Andr\\uFFFD x')\n\n# Error handling (unknown character names)\nself.assertEqual(b\"\\\\N{foo}xx\".decode(\"unicode-escape\", \"ignore\"), \"xx\")\n\n# Error handling (truncated escape sequence)\nself.assertRaises(UnicodeError, b\"\\\\\".decode, \"unicode-escape\")\n\nself.assertRaises(TypeError, b\"hello\".decode, \"test.unicode1\")\nself.assertRaises(TypeError, str, b\"hello\", \"test.unicode2\")\nself.assertRaises(TypeError, \"hello\".encode, \"test.unicode1\")\nself.assertRaises(TypeError, \"hello\".encode, \"test.unicode2\")\n# executes PyUnicode_Encode()\nimport imp\nself.assertRaises(\n    ImportError,\n    imp.find_module,\n    \"non-existing module\",\n    [\"non-existing dir\"]\n)\n\n# Error handling (wrong arguments)\nself.assertRaises(TypeError, \"hello\".encode, 42, 42, 42)\n\n# Error handling (PyUnicode_EncodeDecimal())\nself.assertRaises(UnicodeError, int, \"\\u0200\")", "path": "py3k\\Lib\\test\\test_unicode.py", "repo_name": "jcsalterego/py3k-atsign", "stars": 1, "license": "None", "language": "python", "size": 13436}
{"docstring": "# Ensure that the freelist contains a consistent object, even\n# when a string allocation fails with a MemoryError.\n# This used to crash the interpreter,\n# or leak references when the number was smaller.\n", "func_signal": "def test_raiseMemError(self):\n", "code": "charwidth = 4 if sys.maxunicode >= 0x10000 else 2\n# Note: sys.maxsize is half of the actual max allocation because of\n# the signedness of Py_ssize_t.\nalloc = lambda: \"a\" * (sys.maxsize // charwidth * 2)\nself.assertRaises(MemoryError, alloc)\nself.assertRaises(MemoryError, alloc)", "path": "py3k\\Lib\\test\\test_unicode.py", "repo_name": "jcsalterego/py3k-atsign", "stars": 1, "license": "None", "language": "python", "size": 13436}
{"docstring": "# Make sure unicode objects have an __iter__ method\n", "func_signal": "def test_iterators(self):\n", "code": "it = \"\\u1111\\u2222\\u3333\".__iter__()\nself.assertEqual(next(it), \"\\u1111\")\nself.assertEqual(next(it), \"\\u2222\")\nself.assertEqual(next(it), \"\\u3333\")\nself.assertRaises(StopIteration, next, it)", "path": "py3k\\Lib\\test\\test_unicode.py", "repo_name": "jcsalterego/py3k-atsign", "stars": 1, "license": "None", "language": "python", "size": 13436}
{"docstring": "# make sure we're not accidentally checking ints\n", "func_signal": "def test(i, format_spec, result):\n", "code": "assert type(i) == int\nassert type(format_spec) == str\nself.assertEqual(i.__format__(format_spec), result)", "path": "py3k\\Lib\\test\\test_types.py", "repo_name": "jcsalterego/py3k-atsign", "stars": 1, "license": "None", "language": "python", "size": 13436}
{"docstring": "# Ensure the first 256 integers are shared\n", "func_signal": "def test_normal_integers(self):\n", "code": "a = 256\nb = 128*2\nif a is not b: self.fail('256 is not shared')\nif 12 + 24 != 36: self.fail('int op')\nif 12 + (-24) != -12: self.fail('int op')\nif (-12) + 24 != 12: self.fail('int op')\nif (-12) + (-24) != -36: self.fail('int op')\nif not 12 < 24: self.fail('int op')\nif not -24 < -12: self.fail('int op')\n# Test for a particular bug in integer multiply\nxsize, ysize, zsize = 238, 356, 4\nif not (xsize*ysize*zsize == zsize*xsize*ysize == 338912):\n    self.fail('int mul commutativity')\n# And another.\nm = -sys.maxsize - 1\nfor divisor in 1, 2, 4, 8, 16, 32:\n    j = m // divisor\n    prod = divisor * j\n    if prod != m:\n        self.fail(\"%r * %r == %r != %r\" % (divisor, j, prod, m))\n    if type(prod) is not int:\n        self.fail(\"expected type(prod) to be int, not %r\" %\n                           type(prod))\n# Check for expected * overflow to long.\nfor divisor in 1, 2, 4, 8, 16, 32:\n    j = m // divisor - 1\n    prod = divisor * j\n    if type(prod) is not int:\n        self.fail(\"expected type(%r) to be long, not %r\" %\n                           (prod, type(prod)))\n# Check for expected * overflow to long.\nm = sys.maxsize\nfor divisor in 1, 2, 4, 8, 16, 32:\n    j = m // divisor + 1\n    prod = divisor * j\n    if type(prod) is not int:\n        self.fail(\"expected type(%r) to be long, not %r\" %\n                           (prod, type(prod)))", "path": "py3k\\Lib\\test\\test_types.py", "repo_name": "jcsalterego/py3k-atsign", "stars": 1, "license": "None", "language": "python", "size": 13436}
{"docstring": "# Comparisons:\n", "func_signal": "def test_comparison(self):\n", "code": "self.assertEqual('abc', 'abc')\nself.assertEqual('abc', 'abc')\nself.assertEqual('abc', 'abc')\nself.assert_('abcd' > 'abc')\nself.assert_('abcd' > 'abc')\nself.assert_('abcd' > 'abc')\nself.assert_('abc' < 'abcd')\nself.assert_('abc' < 'abcd')\nself.assert_('abc' < 'abcd')\n\nif 0:\n    # Move these tests to a Unicode collation module test...\n    # Testing UTF-16 code point order comparisons...\n\n    # No surrogates, no fixup required.\n    self.assert_('\\u0061' < '\\u20ac')\n    # Non surrogate below surrogate value, no fixup required\n    self.assert_('\\u0061' < '\\ud800\\udc02')\n\n    # Non surrogate above surrogate value, fixup required\n    def test_lecmp(s, s2):\n        self.assert_(s < s2)\n\n    def test_fixup(s):\n        s2 = '\\ud800\\udc01'\n        test_lecmp(s, s2)\n        s2 = '\\ud900\\udc01'\n        test_lecmp(s, s2)\n        s2 = '\\uda00\\udc01'\n        test_lecmp(s, s2)\n        s2 = '\\udb00\\udc01'\n        test_lecmp(s, s2)\n        s2 = '\\ud800\\udd01'\n        test_lecmp(s, s2)\n        s2 = '\\ud900\\udd01'\n        test_lecmp(s, s2)\n        s2 = '\\uda00\\udd01'\n        test_lecmp(s, s2)\n        s2 = '\\udb00\\udd01'\n        test_lecmp(s, s2)\n        s2 = '\\ud800\\ude01'\n        test_lecmp(s, s2)\n        s2 = '\\ud900\\ude01'\n        test_lecmp(s, s2)\n        s2 = '\\uda00\\ude01'\n        test_lecmp(s, s2)\n        s2 = '\\udb00\\ude01'\n        test_lecmp(s, s2)\n        s2 = '\\ud800\\udfff'\n        test_lecmp(s, s2)\n        s2 = '\\ud900\\udfff'\n        test_lecmp(s, s2)\n        s2 = '\\uda00\\udfff'\n        test_lecmp(s, s2)\n        s2 = '\\udb00\\udfff'\n        test_lecmp(s, s2)\n\n        test_fixup('\\ue000')\n        test_fixup('\\uff61')\n\n# Surrogates on both sides, no fixup required\nself.assert_('\\ud800\\udc02' < '\\ud84d\\udc56')", "path": "py3k\\Lib\\test\\test_unicode.py", "repo_name": "jcsalterego/py3k-atsign", "stars": 1, "license": "None", "language": "python", "size": 13436}
{"docstring": "# test locale support for __format__ code 'n' for integers\n\n", "func_signal": "def test_int__format__locale(self):\n", "code": "x = 123456789012345678901234567890\nfor i in range(0, 30):\n    self.assertEqual(locale.format('%d', x, grouping=True), format(x, 'n'))\n\n    # move to the next integer to test\n    x = x // 10\n\nrfmt = \">20n\"\nlfmt = \"<20n\"\ncfmt = \"^20n\"\nfor x in (1234, 12345, 123456, 1234567, 12345678, 123456789, 1234567890, 12345678900):\n    self.assertEqual(len(format(0, rfmt)), len(format(x, rfmt)))\n    self.assertEqual(len(format(0, lfmt)), len(format(x, lfmt)))\n    self.assertEqual(len(format(0, cfmt)), len(format(x, cfmt)))", "path": "py3k\\Lib\\test\\test_types.py", "repo_name": "jcsalterego/py3k-atsign", "stars": 1, "license": "None", "language": "python", "size": 13436}
{"docstring": "# Make sure __unicode__() works properly\n", "func_signal": "def test_conversion(self):\n", "code": "class Foo0:\n    def __str__(self):\n        return \"foo\"\n\nclass Foo1:\n    def __str__(self):\n        return \"foo\"\n\nclass Foo2(object):\n    def __str__(self):\n        return \"foo\"\n\nclass Foo3(object):\n    def __str__(self):\n        return \"foo\"\n\nclass Foo4(str):\n    def __str__(self):\n        return \"foo\"\n\nclass Foo5(str):\n    def __str__(self):\n        return \"foo\"\n\nclass Foo6(str):\n    def __str__(self):\n        return \"foos\"\n\n    def __str__(self):\n        return \"foou\"\n\nclass Foo7(str):\n    def __str__(self):\n        return \"foos\"\n    def __str__(self):\n        return \"foou\"\n\nclass Foo8(str):\n    def __new__(cls, content=\"\"):\n        return str.__new__(cls, 2*content)\n    def __str__(self):\n        return self\n\nclass Foo9(str):\n    def __str__(self):\n        return \"not unicode\"\n\nself.assertEqual(str(Foo0()), \"foo\")\nself.assertEqual(str(Foo1()), \"foo\")\nself.assertEqual(str(Foo2()), \"foo\")\nself.assertEqual(str(Foo3()), \"foo\")\nself.assertEqual(str(Foo4(\"bar\")), \"foo\")\nself.assertEqual(str(Foo5(\"bar\")), \"foo\")\nself.assertEqual(str(Foo6(\"bar\")), \"foou\")\nself.assertEqual(str(Foo7(\"bar\")), \"foou\")\nself.assertEqual(str(Foo8(\"foo\")), \"foofoo\")\nself.assertEqual(str(Foo9(\"foo\")), \"not unicode\")", "path": "py3k\\Lib\\test\\test_unicode.py", "repo_name": "jcsalterego/py3k-atsign", "stars": 1, "license": "None", "language": "python", "size": 13436}
{"docstring": "# unicode(obj) tests (this maps to PyObject_Unicode() at C level)\n\n", "func_signal": "def test_constructor(self):\n", "code": "self.assertEqual(\n    str('unicode remains unicode'),\n    'unicode remains unicode'\n)\n\nclass UnicodeSubclass(str):\n    pass\n\nself.assertEqual(\n    str(UnicodeSubclass('unicode subclass becomes unicode')),\n    'unicode subclass becomes unicode'\n)\n\nself.assertEqual(\n    str('strings are converted to unicode'),\n    'strings are converted to unicode'\n)\n\nclass StringCompat:\n    def __init__(self, x):\n        self.x = x\n    def __str__(self):\n        return self.x\n\nself.assertEqual(\n    str(StringCompat('__str__ compatible objects are recognized')),\n    '__str__ compatible objects are recognized'\n)\n\n# unicode(obj) is compatible to str():\n\no = StringCompat('unicode(obj) is compatible to str()')\nself.assertEqual(str(o), 'unicode(obj) is compatible to str()')\nself.assertEqual(str(o), 'unicode(obj) is compatible to str()')\n\nfor obj in (123, 123.45, 123):\n    self.assertEqual(str(obj), str(str(obj)))\n\n# unicode(obj, encoding, error) tests (this maps to\n# PyUnicode_FromEncodedObject() at C level)\n\nif not sys.platform.startswith('java'):\n    self.assertRaises(\n        TypeError,\n        str,\n        'decoding unicode is not supported',\n        'utf-8',\n        'strict'\n    )\n\nself.assertEqual(\n    str(b'strings are decoded to unicode', 'utf-8', 'strict'),\n    'strings are decoded to unicode'\n)\n\nif not sys.platform.startswith('java'):\n    self.assertEqual(\n        str(\n            memoryview(b'character buffers are decoded to unicode'),\n            'utf-8',\n            'strict'\n        ),\n        'character buffers are decoded to unicode'\n    )\n\nself.assertRaises(TypeError, str, 42, 42, 42)", "path": "py3k\\Lib\\test\\test_unicode.py", "repo_name": "jcsalterego/py3k-atsign", "stars": 1, "license": "None", "language": "python", "size": 13436}
{"docstring": "# Calling stdcall function as cdecl\n\n", "func_signal": "def test_callconv_2(self):\n", "code": "IsWindow = cdll.user32.IsWindow\n\n# ValueError: Procedure called with not enough arguments (4 bytes missing)\n# or wrong calling convention\nself.assertRaises(ValueError, IsWindow, None)", "path": "py3k\\Lib\\ctypes\\test\\test_win32.py", "repo_name": "jcsalterego/py3k-atsign", "stars": 1, "license": "None", "language": "python", "size": 13436}
{"docstring": "# int, float, and string all share the same format spec\n# mini-language parser.\n\n# Check that we can't ask for too many digits. This is\n# probably a CPython specific test. It tries to put the width\n# into a C long.\n", "func_signal": "def test_format_spec_errors(self):\n", "code": "self.assertRaises(ValueError, format, 0, '1'*10000 + 'd')\n\n# Similar with the precision.\nself.assertRaises(ValueError, format, 0, '.' + '1'*10000 + 'd')\n\n# And may as well test both.\nself.assertRaises(ValueError, format, 0, '1'*1000 + '.' + '1'*10000 + 'd')\n\n# Make sure commas aren't allowed with various type codes\nfor code in 'xXobns':\n    self.assertRaises(ValueError, format, 0, ',' + code)", "path": "py3k\\Lib\\test\\test_types.py", "repo_name": "jcsalterego/py3k-atsign", "stars": 1, "license": "None", "language": "python", "size": 13436}
{"docstring": "# Testing stdcall function\n\n", "func_signal": "def test_callconv_1(self):\n", "code": "IsWindow = windll.user32.IsWindow\n# ValueError: Procedure probably called with not enough arguments (4 bytes missing)\nself.assertRaises(ValueError, IsWindow)\n\n# This one should succeeed...\nself.failUnlessEqual(0, IsWindow(0))\n\n# ValueError: Procedure probably called with too many arguments (8 bytes in excess)\nself.assertRaises(ValueError, IsWindow, 0, 0, 0)", "path": "py3k\\Lib\\ctypes\\test\\test_win32.py", "repo_name": "jcsalterego/py3k-atsign", "stars": 1, "license": "None", "language": "python", "size": 13436}
{"docstring": "# just make sure I'm not accidentally checking longs\n", "func_signal": "def test(i, format_spec, result):\n", "code": "assert type(i) == int\nassert type(format_spec) == str\nself.assertEqual(i.__format__(format_spec), result)", "path": "py3k\\Lib\\test\\test_types.py", "repo_name": "jcsalterego/py3k-atsign", "stars": 1, "license": "None", "language": "python", "size": 13436}
{"docstring": "# Encoding\n", "func_signal": "def test_codecs(self):\n", "code": "self.assertEqual('hello'.encode('ascii'), b'hello')\nself.assertEqual('hello'.encode('utf-7'), b'hello')\nself.assertEqual('hello'.encode('utf-8'), b'hello')\nself.assertEqual('hello'.encode('utf8'), b'hello')\nself.assertEqual('hello'.encode('utf-16-le'), b'h\\000e\\000l\\000l\\000o\\000')\nself.assertEqual('hello'.encode('utf-16-be'), b'\\000h\\000e\\000l\\000l\\000o')\nself.assertEqual('hello'.encode('latin-1'), b'hello')\n\n# Roundtrip safety for BMP (just the first 1024 chars)\nfor c in range(1024):\n    u = chr(c)\n    for encoding in ('utf-7', 'utf-8', 'utf-16', 'utf-16-le',\n                     'utf-16-be', 'raw_unicode_escape',\n                     'unicode_escape', 'unicode_internal'):\n        self.assertEqual(str(u.encode(encoding),encoding), u)\n\n# Roundtrip safety for BMP (just the first 256 chars)\nfor c in range(256):\n    u = chr(c)\n    for encoding in ('latin-1',):\n        self.assertEqual(str(u.encode(encoding),encoding), u)\n\n# Roundtrip safety for BMP (just the first 128 chars)\nfor c in range(128):\n    u = chr(c)\n    for encoding in ('ascii',):\n        self.assertEqual(str(u.encode(encoding),encoding), u)\n\n# Roundtrip safety for non-BMP (just a few chars)\nu = '\\U00010001\\U00020002\\U00030003\\U00040004\\U00050005'\nfor encoding in ('utf-8', 'utf-16', 'utf-16-le', 'utf-16-be',\n                 #'raw_unicode_escape',\n                 'unicode_escape', 'unicode_internal'):\n    self.assertEqual(str(u.encode(encoding),encoding), u)\n\n# UTF-8 must be roundtrip safe for all UCS-2 code points\n# This excludes surrogates: in the full range, there would be\n# a surrogate pair (\\udbff\\udc00), which gets converted back\n# to a non-BMP character (\\U0010fc00)\nu = ''.join(map(chr, list(range(0,0xd800)) +\n                     list(range(0xe000,0x10000))))\nfor encoding in ('utf-8',):\n    self.assertEqual(str(u.encode(encoding),encoding), u)", "path": "py3k\\Lib\\test\\test_unicode.py", "repo_name": "jcsalterego/py3k-atsign", "stars": 1, "license": "None", "language": "python", "size": 13436}
{"docstring": "# This test only affects 32-bit platforms because expandtabs can only take\n# an int as the max value, not a 64-bit C long.  If expandtabs is changed\n# to take a 64-bit long, this test should apply to all platforms.\n", "func_signal": "def test_expandtabs_overflows_gracefully(self):\n", "code": "if sys.maxsize > (1 << 32) or struct.calcsize('P') != 4:\n    return\nself.assertRaises(OverflowError, 't\\tt\\t'.expandtabs, sys.maxsize)", "path": "py3k\\Lib\\test\\test_unicode.py", "repo_name": "jcsalterego/py3k-atsign", "stars": 1, "license": "None", "language": "python", "size": 13436}
{"docstring": "# these work with plain translate()\n", "func_signal": "def test_maketrans_translate(self):\n", "code": "self.checkequalnofix('bbbc', 'abababc', 'translate',\n                     {ord('a'): None})\nself.checkequalnofix('iiic', 'abababc', 'translate',\n                     {ord('a'): None, ord('b'): ord('i')})\nself.checkequalnofix('iiix', 'abababc', 'translate',\n                     {ord('a'): None, ord('b'): ord('i'), ord('c'): 'x'})\nself.checkequalnofix('c', 'abababc', 'translate',\n                     {ord('a'): None, ord('b'): ''})\nself.checkequalnofix('xyyx', 'xzx', 'translate',\n                     {ord('z'): 'yy'})\n# this needs maketrans()\nself.checkequalnofix('abababc', 'abababc', 'translate',\n                     {'b': '<i>'})\ntbl = self.type2test.maketrans({'a': None, 'b': '<i>'})\nself.checkequalnofix('<i><i><i>c', 'abababc', 'translate', tbl)\n# test alternative way of calling maketrans()\ntbl = self.type2test.maketrans('abc', 'xyz', 'd')\nself.checkequalnofix('xyzzy', 'abdcdcbdddd', 'translate', tbl)\n\nself.assertRaises(TypeError, self.type2test.maketrans)\nself.assertRaises(ValueError, self.type2test.maketrans, 'abc', 'defg')\nself.assertRaises(TypeError, self.type2test.maketrans, 2, 'def')\nself.assertRaises(TypeError, self.type2test.maketrans, 'abc', 2)\nself.assertRaises(TypeError, self.type2test.maketrans, 'abc', 'def', 2)\nself.assertRaises(ValueError, self.type2test.maketrans, {'xy': 2})\nself.assertRaises(TypeError, self.type2test.maketrans, {(1,): 2})\n\nself.assertRaises(TypeError, 'hello'.translate)\nself.assertRaises(TypeError, 'abababc'.translate, 'abc', 'xyz')", "path": "py3k\\Lib\\test\\test_unicode.py", "repo_name": "jcsalterego/py3k-atsign", "stars": 1, "license": "None", "language": "python", "size": 13436}
{"docstring": "# these should be rewritten to use both format(x, spec) and\n# x.__format__(spec)\n\n", "func_signal": "def test_float__format__(self):\n", "code": "def test(f, format_spec, result):\n    assert type(f) == float\n    assert type(format_spec) == str\n    self.assertEqual(f.__format__(format_spec), result)\n\ntest(0.0, 'f', '0.000000')\n\n# the default is 'g', except for empty format spec\ntest(0.0, '', '0.0')\ntest(0.01, '', '0.01')\ntest(0.01, 'g', '0.01')\n\n# test for issue 3411\ntest(1.23, '1', '1.23')\ntest(-1.23, '1', '-1.23')\ntest(1.23, '1g', '1.23')\ntest(-1.23, '1g', '-1.23')\n\ntest( 1.0, ' g', ' 1')\ntest(-1.0, ' g', '-1')\ntest( 1.0, '+g', '+1')\ntest(-1.0, '+g', '-1')\ntest(1.1234e200, 'g', '1.1234e+200')\ntest(1.1234e200, 'G', '1.1234E+200')\n\n\ntest(1.0, 'f', '1.000000')\n\ntest(-1.0, 'f', '-1.000000')\n\ntest( 1.0, ' f', ' 1.000000')\ntest(-1.0, ' f', '-1.000000')\ntest( 1.0, '+f', '+1.000000')\ntest(-1.0, '+f', '-1.000000')\n\n# Python versions <= 3.0 switched from 'f' to 'g' formatting for\n# values larger than 1e50.  No longer.\nf = 1.1234e90\nfor fmt in 'f', 'F':\n    # don't do a direct equality check, since on some\n    # platforms only the first few digits of dtoa\n    # will be reliable\n    result = f.__format__(fmt)\n    self.assertEqual(len(result), 98)\n    self.assertEqual(result[-7], '.')\n    self.assert_(result[:12] in ('112340000000', '112339999999'))\nf = 1.1234e200\nfor fmt in 'f', 'F':\n    result = f.__format__(fmt)\n    self.assertEqual(len(result), 208)\n    self.assertEqual(result[-7], '.')\n    self.assert_(result[:12] in ('112340000000', '112339999999'))\n\n\ntest( 1.0, 'e', '1.000000e+00')\ntest(-1.0, 'e', '-1.000000e+00')\ntest( 1.0, 'E', '1.000000E+00')\ntest(-1.0, 'E', '-1.000000E+00')\ntest(1.1234e20, 'e', '1.123400e+20')\ntest(1.1234e20, 'E', '1.123400E+20')\n\n# No format code means use g, but must have a decimal\n# and a number after the decimal.  This is tricky, because\n# a totaly empty format specifier means something else.\n# So, just use a sign flag\ntest(1e200, '+g', '+1e+200')\ntest(1e200, '+', '+1e+200')\n\ntest(1.1e200, '+g', '+1.1e+200')\ntest(1.1e200, '+', '+1.1e+200')\n\n# 0 padding\ntest(1234., '010f', '1234.000000')\ntest(1234., '011f', '1234.000000')\ntest(1234., '012f', '01234.000000')\ntest(-1234., '011f', '-1234.000000')\ntest(-1234., '012f', '-1234.000000')\ntest(-1234., '013f', '-01234.000000')\ntest(-1234.12341234, '013f', '-01234.123412')\ntest(-123456.12341234, '011.2f', '-0123456.12')\n\n# issue 5782, commas with no specifier type\ntest(1.2, '010,.2', '0,000,001.2')\n\n# 0 padding with commas\ntest(1234., '011,f', '1,234.000000')\ntest(1234., '012,f', '1,234.000000')\ntest(1234., '013,f', '01,234.000000')\ntest(-1234., '012,f', '-1,234.000000')\ntest(-1234., '013,f', '-1,234.000000')\ntest(-1234., '014,f', '-01,234.000000')\ntest(-12345., '015,f', '-012,345.000000')\ntest(-123456., '016,f', '-0,123,456.000000')\ntest(-123456., '017,f', '-0,123,456.000000')\ntest(-123456.12341234, '017,f', '-0,123,456.123412')\ntest(-123456.12341234, '013,.2f', '-0,123,456.12')\n\n# % formatting\ntest(-1.0, '%', '-100.000000%')\n\n# format spec must be string\nself.assertRaises(TypeError, 3.0.__format__, None)\nself.assertRaises(TypeError, 3.0.__format__, 0)\n\n# other format specifiers shouldn't work on floats,\n#  in particular int specifiers\nfor format_spec in ([chr(x) for x in range(ord('a'), ord('z')+1)] +\n                    [chr(x) for x in range(ord('A'), ord('Z')+1)]):\n    if not format_spec in 'eEfFgGn%':\n        self.assertRaises(ValueError, format, 0.0, format_spec)\n        self.assertRaises(ValueError, format, 1.0, format_spec)\n        self.assertRaises(ValueError, format, -1.0, format_spec)\n        self.assertRaises(ValueError, format, 1e100, format_spec)\n        self.assertRaises(ValueError, format, -1e100, format_spec)\n        self.assertRaises(ValueError, format, 1e-100, format_spec)\n        self.assertRaises(ValueError, format, -1e-100, format_spec)\n\n# Alternate formatting is not supported\nself.assertRaises(ValueError, format, 0.0, '#')\nself.assertRaises(ValueError, format, 0.0, '#20f')", "path": "py3k\\Lib\\test\\test_types.py", "repo_name": "jcsalterego/py3k-atsign", "stars": 1, "license": "None", "language": "python", "size": 13436}
{"docstring": "\"\"\"Initializes a map representation of this tag's attributes,\nif not already initialized.\"\"\"\n", "func_signal": "def _getAttrMap(self):\n", "code": "if not getattr(self, 'attrMap'):\n    self.attrMap = {}\n    for (key, value) in self.attrs:\n        self.attrMap[key] = value\nreturn self.attrMap", "path": "BeautifulSoup.py", "repo_name": "hylom/wgrabber", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "# Convert the document to Unicode.\n", "func_signal": "def _feed(self, inDocumentEncoding=None, isHTML=False):\n", "code": "markup = self.markup\nif isinstance(markup, unicode):\n    if not hasattr(self, 'originalEncoding'):\n        self.originalEncoding = None\nelse:\n    dammit = UnicodeDammit\\\n             (markup, [self.fromEncoding, inDocumentEncoding],\n              smartQuotesTo=self.smartQuotesTo, isHTML=isHTML)\n    markup = dammit.unicode\n    self.originalEncoding = dammit.originalEncoding\n    self.declaredHTMLEncoding = dammit.declaredHTMLEncoding\nif markup:\n    if self.markupMassage:\n        if not isList(self.markupMassage):\n            self.markupMassage = self.MARKUP_MASSAGE\n        for fix, m in self.markupMassage:\n            markup = fix.sub(m, markup)\n        # TODO: We get rid of markupMassage so that the\n        # soup object can be deepcopied later on. Some\n        # Python installations can't copy regexes. If anyone\n        # was relying on the existence of markupMassage, this\n        # might cause problems.\n        del(self.markupMassage)\nself.builder.reset()\n\nself.builder.feed(markup)\n# Close out any unfinished strings and close all the open tags.\nself.endData()\nwhile self.currentTag.name != self.ROOT_TAG_NAME:\n    self.popTag()", "path": "BeautifulSoup.py", "repo_name": "hylom/wgrabber", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"append url to download list\"\"\"\n\n", "func_signal": "def _append_download_list(self, url):\n", "code": "url = self._rewrite_url(url)\nif not self._check_url(url):\n    return\nif not self._targets.has_key(url):\n    self._targets[url] = 1\n    self._url_lists.append(url)", "path": "wikigrab.py", "repo_name": "hylom/wgrabber", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Encodes an object to a string in some encoding, or to Unicode.\n.\"\"\"\n", "func_signal": "def toEncoding(self, s, encoding=None):\n", "code": "if isinstance(s, unicode):\n    if encoding:\n        s = s.encode(encoding)\nelif isinstance(s, str):\n    if encoding:\n        s = s.encode(encoding)\n    else:\n        s = unicode(s)\nelse:\n    if encoding:\n        s  = self.toEncoding(str(s), encoding)\n    else:\n        s = unicode(s)\nreturn s", "path": "BeautifulSoup.py", "repo_name": "hylom/wgrabber", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "#print \"Push\", tag.name\n", "func_signal": "def pushTag(self, tag):\n", "code": "if self.currentTag:\n    self.currentTag.contents.append(tag)\nself.tagStack.append(tag)\nself.currentTag = self.tagStack[-1]", "path": "BeautifulSoup.py", "repo_name": "hylom/wgrabber", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"grab given url as local html by POST method\"\"\"\n", "func_signal": "def push_by_post(self, url, params):\n", "code": "for key in params:\n    if isinstance(params[key], unicode):\n        params[key] = params[key].encode(\"utf-8\")\nencoded_params = urllib.urlencode(params)\n\nsfjp_opener = sfjp.FancyURLopenerWithCookie()\ntry:\n    sfjp_opener.load_cookie(\"./cookie\")\nexcept IOError:\n    sys.stderr.write(\"cannot use cookie file. create.\\n\")\n\nif sfjp_opener.get_cookie() == \"\":\n    try:\n        uname = raw_input(\"user: \")\n    except KeyboardInterrupt:\n        sys.exit(\"\\nabort.\")\n\n    try:\n        passwd = getpass.getpass(\"login password:\")\n    except KeyboardInterrupt:\n        sys.exit(\"\\nabort.\")\n        if sfjp_opener.login(uname, passwd) != 1:\n            sys.exit(\"login error!\")\n            \n    sfjp_opener.login(uname, passwd)\n    sfjp_opener.save_cookie(\"./cookie\")\nsfjp_opener.regist_cookie()\n\nurllib._urlopener = sfjp_opener\n\nu = urllib.urlopen(url, encoded_params)\ndata = u.read()", "path": "wikipush.py", "repo_name": "hylom/wgrabber", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Used in a call to re.sub to replace HTML, XML, and numeric\nentities with the appropriate Unicode characters. If HTML\nentities are being converted, any unrecognized entities are\nescaped.\"\"\"\n", "func_signal": "def _convertEntities(self, match):\n", "code": "x = match.group(1)\nif self.convertHTMLEntities and x in name2codepoint:\n    return unichr(name2codepoint[x])\nelif x in self.XML_ENTITIES_TO_SPECIAL_CHARS:\n    if self.convertXMLEntities:\n        return self.XML_ENTITIES_TO_SPECIAL_CHARS[x]\n    else:\n        return u'&%s;' % x\nelif len(x) > 0 and x[0] == '#':\n    # Handle numeric entities\n    if len(x) > 1 and x[1] == 'x':\n        return unichr(int(x[2:], 16))\n    else:\n        return unichr(int(x[1:]))\n\nelif self.escapeUnrecognizedEntities:\n    return u'&amp;%s;' % x\nelse:\n    return u'&%s;' % x", "path": "BeautifulSoup.py", "repo_name": "hylom/wgrabber", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"grab given url as local html by POST method\"\"\"\n", "func_signal": "def grab_by_post(self, url, params):\n", "code": "encoded_params = urllib.urlencode(params)\nu = urllib.urlopen(url, encoded_params)\ndata = u.read()\nreturn data", "path": "wikigrab.py", "repo_name": "hylom/wgrabber", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"test for extract_anchors function\"\"\"\n\n", "func_signal": "def test_extract_anchors(self):\n", "code": "ret_ok = [\"http://ffdshow-tryout.sourceforge.net/wiki/feed.php\",\n          \"http://creativecommons.org/licenses/by-nc-sa/3.0/\",\n          \"http://www.dokuwiki.org/donate\",\n          \"http://www.php.net\",\n          \"http://validator.w3.org/check/referer\",\n          \"http://jigsaw.w3.org/css-validator/check/referer?profile=css3\",\n          \"http://dokuwiki.org/\",\n          \"http://ffdshow-tryout.sourceforge.net/wiki/hogehoge\"]\n\nret = self.grabber.extract_anchors(self.test_html, \"http://ffdshow-tryout.sourceforge.net/wiki/home\")\nself.assertEqual(len(ret), len(ret_ok))\nret.sort()\nret_ok.sort()\nfor index in range(len(ret)):\n    self.assertEqual(ret[index], ret_ok[index])", "path": "test_wikigrab.py", "repo_name": "hylom/wgrabber", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Sets up the initial relations between this element and\nother elements.\"\"\"\n", "func_signal": "def setup(self, parent=None, previous=None):\n", "code": "self.parent = parent\nself.previous = previous\nself.next = None\nself.previousSibling = None\nself.nextSibling = None\nif self.parent and self.parent.contents:\n    self.previousSibling = self.parent.contents[-1]\n    self.previousSibling.nextSibling = self", "path": "BeautifulSoup.py", "repo_name": "hylom/wgrabber", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"get content from HTML by id\"\"\"\n# bsp = BeautifulSoup(text, fromEncoding=\"utf_8\")\n", "func_signal": "def extract_anchors(self, html, url):\n", "code": "bsp = BeautifulSoup(html)\nres = bsp.findAll('a', dict(href=True))\nrex_abs = re.compile(r\"^/\")\nrex_rel = re.compile(r\"^(?!http://|https://)\")\nrex_protocol = re.compile(r\"^[a-z]+:\")\nrex_protocol_allow = re.compile(r\"^(http://|https://)\")\n\nif re.search(r\"^http://[^/]+$\", url):\n    url = url + \"/\"\ntry:\n    hostname = re.search(r\"^(http://[^/]+)/\", url).group(1)\nexcept AttributeError:\n    raise WikiGrabberError(\"no_hostname_in_url\", url)\ntry:\n    url_basedir = re.search(r\"^(.*/).*$\", url).group(1)\nexcept AttributeError:\n    raise WikiGrabberError(\"no_basedir_in_url\", url)\n\nanchors = []\nfor item in res:\n    link_url = item[\"href\"]\n\n    # regularize\n    if rex_protocol.search(link_url):\n        if not rex_protocol_allow.search(link_url):\n            continue\n    if rex_abs.search(link_url):\n        link_url = hostname + link_url\n    elif rex_rel.search(link_url):\n        link_url = url_basedir + link_url\n\n    anchors.append(link_url)\nreturn anchors", "path": "wikigrab.py", "repo_name": "hylom/wgrabber", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "'''Given a string and its encoding, decodes the string into Unicode.\n%encoding is a string recognized by encodings.aliases'''\n\n# strip Byte Order Mark (if present)\n", "func_signal": "def _toUnicode(self, data, encoding):\n", "code": "if (len(data) >= 4) and (data[:2] == '\\xfe\\xff') \\\n       and (data[2:4] != '\\x00\\x00'):\n    encoding = 'utf-16be'\n    data = data[2:]\nelif (len(data) >= 4) and (data[:2] == '\\xff\\xfe') \\\n         and (data[2:4] != '\\x00\\x00'):\n    encoding = 'utf-16le'\n    data = data[2:]\nelif data[:3] == '\\xef\\xbb\\xbf':\n    encoding = 'utf-8'\n    data = data[3:]\nelif data[:4] == '\\x00\\x00\\xfe\\xff':\n    encoding = 'utf-32be'\n    data = data[4:]\nelif data[:4] == '\\xff\\xfe\\x00\\x00':\n    encoding = 'utf-32le'\n    data = data[4:]\nnewdata = unicode(data, encoding)\nreturn newdata", "path": "BeautifulSoup.py", "repo_name": "hylom/wgrabber", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "#print \"End tag %s\" % name\n", "func_signal": "def unknown_endtag(self, name):\n", "code": "if self.quoteStack and self.quoteStack[-1] != name:\n    #This is not a real end tag.\n    #print \"</%s> is not real!\" % name\n    self.handle_data('</%s>' % name)\n    return\nself.endData()\nself._popToTag(name)\nif self.quoteStack and self.quoteStack[-1] == name:\n    self.quoteStack.pop()\n    self.literal = (len(self.quoteStack) > 0)", "path": "BeautifulSoup.py", "repo_name": "hylom/wgrabber", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Returns the closest parent of this Tag that matches the given\ncriteria.\"\"\"\n# NOTE: We can't use _findOne because findParents takes a different\n# set of arguments.\n", "func_signal": "def findParent(self, name=None, attrs={}, **kwargs):\n", "code": "r = None\nl = self.findParents(name, attrs, 1)\nif l:\n    r = l[0]\nreturn r", "path": "BeautifulSoup.py", "repo_name": "hylom/wgrabber", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Constructor\"\"\"\n", "func_signal": "def __init__(self, config):\n", "code": "self._config = config\nself._targets = {}\nself._url_lists = []", "path": "wikigrab.py", "repo_name": "hylom/wgrabber", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"grab given url as local html by GET method\"\"\"\n", "func_signal": "def grab_by_get(self, url):\n", "code": "u = urllib.urlopen(url)\ndata = u.read()\nreturn data", "path": "wikigrab.py", "repo_name": "hylom/wgrabber", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"rewrite url\"\"\"\n", "func_signal": "def _rewrite_url(self, url):\n", "code": "for (rule, str) in self.get_config(\"rewrite_rule\"):\n    url = re.sub(rule, str, url)\nreturn url", "path": "wikigrab.py", "repo_name": "hylom/wgrabber", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Run grubbber\"\"\"\n", "func_signal": "def run(self):\n", "code": "target_dir = self.get_config(\"dir\")\nprefix = self.get_config(\"prefix\")\nfor file in dircache.listdir(target_dir):\n    filepath = os.path.join(target_dir, file)\n\n    # print file, filepath\n    if re.search(r\"\\.txt$\", file):\n        if os.path.isfile(filepath):\n            bodytext = self._read(filepath)", "path": "wikipush.py", "repo_name": "hylom/wgrabber", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Given a document, tries to detect its XML encoding.\"\"\"\n", "func_signal": "def _detectEncoding(self, xml_data, isHTML=False):\n", "code": "xml_encoding = sniffed_xml_encoding = None\ntry:\n    if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n        # EBCDIC\n        xml_data = self._ebcdic_to_ascii(xml_data)\n    elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n        # UTF-16BE\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') \\\n             and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16BE with BOM\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n        # UTF-16LE\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and \\\n             (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16LE with BOM\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n        # UTF-32BE\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n        # UTF-32LE\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n        # UTF-32BE with BOM\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n        # UTF-32LE with BOM\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n    elif xml_data[:3] == '\\xef\\xbb\\xbf':\n        # UTF-8 with BOM\n        sniffed_xml_encoding = 'utf-8'\n        xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n    else:\n        sniffed_xml_encoding = 'ascii'\n        pass\nexcept:\n    xml_encoding_match = None\nxml_encoding_re = '^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>'.encode()\nxml_encoding_match = re.compile(xml_encoding_re).match(xml_data)\nif not xml_encoding_match and isHTML:\n    meta_re = '<\\s*meta[^>]+charset=([^>]*?)[;\\'\">]'.encode()\n    regexp = re.compile(meta_re, re.I)\n    xml_encoding_match = regexp.search(xml_data)\nif xml_encoding_match is not None:\n    xml_encoding = xml_encoding_match.groups()[0].decode(\n        'ascii').lower()\n    if isHTML:\n        self.declaredHTMLEncoding = xml_encoding\n    if sniffed_xml_encoding and \\\n       (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',\n                         'iso-10646-ucs-4', 'ucs-4', 'csucs4',\n                         'utf-16', 'utf-32', 'utf_16', 'utf_32',\n                         'utf16', 'u16')):\n        xml_encoding = sniffed_xml_encoding\nreturn xml_data, xml_encoding, sniffed_xml_encoding", "path": "BeautifulSoup.py", "repo_name": "hylom/wgrabber", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Treat a bogus SGML declaration as raw data. Treat a CDATA\ndeclaration as a CData object.\"\"\"\n", "func_signal": "def parse_declaration(self, i):\n", "code": "j = None\nif self.rawdata[i:i+9] == '<![CDATA[':\n     k = self.rawdata.find(']]>', i)\n     if k == -1:\n         k = len(self.rawdata)\n     data = self.rawdata[i+9:k]\n     j = k+3\n     self._toStringSubclass(data, CData)\nelse:\n    try:\n        j = HTMLParser.parse_declaration(self, i)\n    except HTMLParseError:\n        toHandle = self.rawdata[i:]\n        self.handle_data(toHandle)\n        j = i + len(toHandle)\nreturn j", "path": "BeautifulSoup.py", "repo_name": "hylom/wgrabber", "stars": 1, "license": "None", "language": "python", "size": 104}
{"docstring": "''' Add codes to object\n\n\t>>> codes = ((1, 'one'), (2, 'two'))\n\t>>> rc = Recoder(codes)\n>>> rc.value_set() == set((1,2))\nTrue\n>>> rc.add_codes(((3, 'three'), (1, 'first')))\n>>> rc.value_set() == set((1,2,3))\nTrue\n'''\n", "func_signal": "def add_codes(self, codes):\n", "code": "for vals in codes:\n    for val in vals:\n        for ind, name in enumerate(self.fields):\n            self.__dict__[name][val] = vals[ind]", "path": "dipy\\io\\utils.py", "repo_name": "mohini/dipy", "stars": 0, "license": "None", "language": "python", "size": 118}
{"docstring": "''' \n    Calculating the average distances between tracks xyz1 and xyz2 \n    Based on the metrics in Zhang, Correia, Laidlaw 2008 \n    http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=4479455        \n    \n    Parameters:\n    -----------\n    xyz1 : array, shape (N1,3)\n    xyz2 : array, shape (N2,3)\n    arrays representing x,y,z of the N1 and N2 points  of two tracks\n    \n    Returns:\n    ----------\n    \n'''\n", "func_signal": "def mean_closest_distances(xyz1,xyz2):\n", "code": "n1 = xyz1.shape[0]\nn2 = xyz2.shape[0]\n\nd = np.resize(np.tile(xyz1,(n2)),(n1,n2,3)) \\\n    - (np.resize(np.tile(xyz2,(n1)),(n2,n1,3)),(1,0,2)).T\n    \ndm = np.sqrt(np.sum(d**2,axis=2))\nreturn np.average(np.min(dm,axis=0)), np.average(np.min(dm,axis=1))", "path": "dipy\\core\\track_metrics.py", "repo_name": "mohini/dipy", "stars": 0, "license": "None", "language": "python", "size": 118}
{"docstring": "''' Create an actor with the coordinate system axes where  red = x, green = y, blue =z.\n'''\n\n", "func_signal": "def axes(scale=(1,1,1),colorx=(1,0,0),colory=(0,1,0),colorz=(0,0,1),opacity=1):\n", "code": "arrowx=_arrow(color=colorx,scale=scale,opacity=opacity)\narrowy=_arrow(color=colory,scale=scale,opacity=opacity)\narrowz=_arrow(color=colorz,scale=scale,opacity=opacity)\n\narrowy.RotateZ(90)\narrowz.RotateY(-90)\n\nass=vtk.vtkAssembly()\nass.AddPart(arrowx)\nass.AddPart(arrowy)\nass.AddPart(arrowz)\n       \nreturn ass", "path": "dipy\\viz\\fos.py", "repo_name": "mohini/dipy", "stars": 0, "license": "None", "language": "python", "size": 118}
{"docstring": "\"\"\"Combine sets of size n from items\n\nExamples:\n-------------\n>>> ic=generate_combinations(range(3),2)\n>>> for i in ic: print i\n[0, 1]\n[0, 2]\n[1, 2]\n\"\"\"\n\n", "func_signal": "def generate_combinations(items, n):\n", "code": "if n == 0:\n    yield []\nelse:\n    for i in xrange(len(items)):\n        for cc in generate_combinations(items[i+1:], n-1):\n            yield [items[i]] + cc", "path": "dipy\\core\\track_metrics.py", "repo_name": "mohini/dipy", "stars": 0, "license": "None", "language": "python", "size": 118}
{"docstring": "''' Frenet-Serret Space Curve Invarients\n \nCalculates the 3 vector and 2 scaler invarients of a space curve defined\nby vectors x,y and z.  If z is omitted then the curve is only a 2D,\nbut the equations are still valid.\n\nSimilar to\nhttp://www.mathworks.com/matlabcentral/fileexchange/11169\n\n_    r'\nT = ----  (Tangent)\n    |r'|\n \n_    T'\nN = ----  (Normal)\n    |T'|\n_   _   _\nB = T x N (Binormal)\n\nk = |T'|  (Curvature)\n \nt = dot(-B',N) (Torsion)\n\nParameters\n----------\nxyz : array-like shape (N,3)\n   array representing x,y,z of N points in a track\n\n\nReturns\n---------\nT : array shape (N,3)\n    array representing the tangent of the curve xyz\nN : array shape (N,3)\n    array representing the normal of the curve xyz    \nB : array shape (N,3)\n    array representing the binormal of the curve xyz\nk : array shape (N,1)\n    array representing the curvature of the curve xyz\nt : array shape (N,1)\n    array representing the torsion of the curve xyz\n\nExamples\n--------\nCreate a helix and calculate its tangent,normal, binormal, curvature and torsion\n\n>>> from dipy.core import track_metrics as tm\n>>> import numpy as np\n>>> theta = 2*np.pi*np.linspace(0,2,100)\n>>> x=np.cos(theta)\n>>> y=np.sin(theta)\n>>> z=theta/(2*np.pi)\n>>> xyz=np.vstack((x,y,z)).T\n>>> T,N,B,k,t=tm.frenet_serret(xyz)\n\n'''\n", "func_signal": "def frenet_serret(xyz):\n", "code": "xyz = np.asarray(xyz)\nn_pts = xyz.shape[0]\nif n_pts == 0:\n    raise ValueError('xyz array cannot be empty')\n\ndxyz=np.gradient(xyz)[0]        \n\nddxyz=np.gradient(dxyz)[0]\n\n#Tangent        \nT=np.divide(dxyz,magn(dxyz,3))\n\n#Derivative of Tangent\ndT=np.gradient(T)[0]\n\n#Normal\nN = np.divide(dT,magn(dT,3))\n\n#Binormal\nB = np.cross(T,N)\n\n#Curvature \nk = magn(np.cross(dxyz,ddxyz),1)/(magn(dxyz,1)**3)    \n\n#Torsion \n#(In matlab was t=dot(-B,N,2))\nt = np.sum(-B*N,axis=1)\n\n#return T,N,B,k,t,dxyz,ddxyz,dT   \nreturn T,N,B,k,t", "path": "dipy\\core\\track_metrics.py", "repo_name": "mohini/dipy", "stars": 0, "license": "None", "language": "python", "size": 118}
{"docstring": "'''\nCreate one or more 3d dots(points) returns one actor handling all the points\n'''\n\n", "func_signal": "def dots(points,color=(1,0,0),opacity=1):\n", "code": "if points.ndim==2:\n  points_no=points.shape[0]\nelse:\n  points_no=1\n  \npolyVertexPoints = vtk.vtkPoints()\npolyVertexPoints.SetNumberOfPoints(points_no)\naPolyVertex = vtk.vtkPolyVertex()\naPolyVertex.GetPointIds().SetNumberOfIds(points_no)\n\ncnt=0\nif points.ndim>1:\n      for point in points:\n          polyVertexPoints.InsertPoint(cnt, point[0], point[1], point[2])\n          aPolyVertex.GetPointIds().SetId(cnt, cnt)\n          cnt+=1\nelse:\n      polyVertexPoints.InsertPoint(cnt, points[0], points[1], points[2])\n      aPolyVertex.GetPointIds().SetId(cnt, cnt)\n      cnt+=1\n  \n\naPolyVertexGrid = vtk.vtkUnstructuredGrid()\naPolyVertexGrid.Allocate(1, 1)\naPolyVertexGrid.InsertNextCell(aPolyVertex.GetCellType(), aPolyVertex.GetPointIds())\n\naPolyVertexGrid.SetPoints(polyVertexPoints)\naPolyVertexMapper = vtk.vtkDataSetMapper()\naPolyVertexMapper.SetInput(aPolyVertexGrid)\naPolyVertexActor = vtk.vtkActor()\naPolyVertexActor.SetMapper(aPolyVertexMapper)\n\naPolyVertexActor.GetProperty().SetColor(color)\naPolyVertexActor.GetProperty().SetOpacity(opacity)\n\nreturn aPolyVertexActor", "path": "dipy\\viz\\fos.py", "repo_name": "mohini/dipy", "stars": 0, "license": "None", "language": "python", "size": 118}
{"docstring": "''' Create recoder object\n\n\t``codes`` give a sequence of code, alias sequences\n\t``fields`` are names by which the entries in these sequences can be\n\taccessed.\n\n\tBy default ``fields`` gives the first column the name\n\t\"code\".  The first column is the vector of first entries\n\tin each of the sequences found in ``codes``.  Thence you can\n\tget the equivalent first column value with ob.code[value],\n\twhere value can be a first column value, or a value in any of\n\tthe other columns in that sequence. \n\n\tYou can give other columns names too, and access them in the\n\tsame way - see the examples in the class docstring. \n\nParameters\n----------\ncodes : seqence of sequences\n    Each sequence defines values (codes) that are equivalent\nfields : {('code',) string sequence}, optional\n    names by which elements in sequences can be accesssed\n\n'''\n", "func_signal": "def __init__(self, codes, fields=('code',)):\n", "code": "self.fields = fields\nself.field1 = {} # a placeholder for the check below\nfor name in fields:\n    if name in self.__dict__:\n        raise KeyError('Input name %s already in object dict'\n                       % name)\n    self.__dict__[name] = {}\nself.field1 = self.__dict__[fields[0]]\nself.add_codes(codes)", "path": "dipy\\io\\utils.py", "repo_name": "mohini/dipy", "stars": 0, "license": "None", "language": "python", "size": 118}
{"docstring": "''' Generic file-like object open\n\nIf input ``fname`` already looks like a file, pass through.\nIf ``fname`` ends with recognizable compressed types, use python\nlibraries to open as file-like objects (read or write)\nOtherwise, use standard ``open``.\n'''\n", "func_signal": "def allopen(fname, *args, **kwargs):\n", "code": "if hasattr(fname, 'write'):\n    return fname\nif args:\n    mode = args[0]\nelif 'mode' in kwargs:\n    mode = kwargs['mode']\nelse:\n    mode = 'rb'\nif fname.endswith('.gz'):\n    if ('w' in mode and\n        len(args) < 2 and\n        not 'compresslevel' in kwargs):\n        kwargs['compresslevel'] = default_compresslevel\n    import gzip\n    opener = gzip.open\nelif fname.endswith('.bz2'):\n    if ('w' in mode and\n        len(args) < 3 and\n        not 'compresslevel' in kwargs):\n        kwargs['compresslevel'] = default_compresslevel\n    import bz2\n    opener = bz2.BZ2File\nelse:\n    opener = open\nreturn opener(fname, *args, **kwargs)", "path": "dipy\\io\\utils.py", "repo_name": "mohini/dipy", "stars": 0, "license": "None", "language": "python", "size": 118}
{"docstring": "''' Internal function\nCreates a lookup table with given colors.\n\nParameters\n------------\ncolors : array, shape (N,3)\n        Colormap where every triplet is encoding red, green and blue e.g. \n        r1,g1,b1\n        r2,g2,b2\n        ...\n        rN,gN,bN        \n        \n        where\n        0=<r<=1,\n        0=<g<=1,\n        0=<b<=1,\n\nReturns\n----------\nvtkLookupTable\n\n'''\n    \n", "func_signal": "def _lookup(colors):\n", "code": "colors=np.asarray(colors,dtype=np.float32)\n\nif colors.ndim>2:\n    raise ValueError('Incorrect shape of array in colors')\n\nif colors.ndim==1:\n    N=1\n    \nif colors.ndim==2:\n    \n    N=colors.shape[0]    \n\n\nlut=vtk.vtkLookupTable()\nlut.SetNumberOfColors(N)\nlut.Build()\n\nif colors.ndim==2:\n    scalar=0\n    for (r,g,b) in colors:\n        \n        lut.SetTableValue(scalar,r,g,b,1.0)\n        scalar+=1\nif colors.ndim==1:\n    \n    lut.SetTableValue(0,colors[0],colors[1],colors[2],1.0)\n        \nreturn lut", "path": "dipy\\viz\\fos.py", "repo_name": "mohini/dipy", "stars": 0, "license": "None", "language": "python", "size": 118}
{"docstring": "''' Add a specific actor    \n'''\n", "func_signal": "def add(ren,a):\n", "code": "if isinstance(a,vtk.vtkVolume):\n    ren.AddVolume(a)\nelse:    \n    ren.AddActor(a)", "path": "dipy\\viz\\fos.py", "repo_name": "mohini/dipy", "stars": 0, "license": "None", "language": "python", "size": 118}
{"docstring": "''' Calculating the distance between tracks xyz1 and xyz2 \n    Based on the metrics in Zhang,  Correia,   Laidlaw 2008 \n    http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=4479455\n    which in turn are based on those of Corouge et al. 2004\n \nParameters:\n-----------\n    xyz1 : array, shape (N1,3)\n    xyz2 : array, shape (N2,3)\n    arrays representing x,y,z of the N1 and N2 points  of two tracks\n\nReturns:\n--------\n    ave_mcd: float\n                average_mean_closest_distance\n    min_mcd: float\n                minimum_mean_closest_distance\n    max_mcd: float\n                maximum_mean_closest_distance\n'''\n", "func_signal": "def zhang_distances(xyz1,xyz2):\n", "code": "mcd12,mcd21 = mean_closest_distances(xyz1,xyz2)\nreturn (mcd12+mcd21)/2.0, min(mcd12,mcd21), max(mcd12,mcd21)", "path": "dipy\\core\\track_metrics.py", "repo_name": "mohini/dipy", "stars": 0, "license": "None", "language": "python", "size": 118}
{"docstring": "''' Create wx application to show OpenGL output\n'''\n", "func_signal": "def show(trajs=None, colors=None):\n", "code": "app = wx.PySimpleApp()\nframe = make_window_maker(trajs, colors)()\napp.MainLoop()\n\ndel frame   \ndel app", "path": "dipy\\viz\\phos.py", "repo_name": "mohini/dipy", "stars": 0, "license": "None", "language": "python", "size": 118}
{"docstring": "''' Midpoint of track line\n\nParameters\n----------\nxyz : array-like shape (N,3)\n   array representing x,y,z of N points in a track\n\nReturns\n-------\nmp : array shape (3,)\n   Middle point of line, such that, if L is the line length then\n   `np` is the point such that the length xyz[0] to `mp` and from\n   `mp` to xyz[-1] is L/2.  If the middle point is not a point in\n   `xyz`, then we take the interpolation between the two nearest\n   `xyz` points.  If `xyz` is empty, return a ValueError\n\nExamples\n--------\n>>> midpoint([])\nTraceback (most recent call last):\n   ...\nValueError: xyz array cannot be empty\n>>> midpoint([[1, 2, 3]])\narray([1, 2, 3])\n>>> xyz = np.array([[1,1,1],[2,3,4]])\n>>> midpoint(xyz)\narray([ 1.5,  2. ,  2.5])\n>>> xyz = np.array([[0,0,0],[1,1,1],[2,2,2]])\n>>> midpoint(xyz)\narray([ 1.,  1.,  1.])\n>>> xyz = np.array([[0,0,0],[1,0,0],[3,0,0]])\n>>> midpoint(xyz)\narray([ 1.5,  0. ,  0. ])\n>>> xyz = np.array([[0,9,7],[1,9,7],[3,9,7]])\n>>> midpoint(xyz)\narray([ 1.5,  9. ,  7. ])\n'''\n", "func_signal": "def midpoint(xyz):\n", "code": "xyz = np.asarray(xyz)\nn_pts = xyz.shape[0]\nif n_pts == 0:\n    raise ValueError('xyz array cannot be empty')\nif n_pts == 1:\n    return xyz.copy().squeeze()\ncumlen = np.zeros(n_pts)\ncumlen[1:] = length(xyz, along=True)\nmidlen=cumlen[-1]/2.0\nind=np.where((cumlen-midlen)>0)[0][0]\nlen0=cumlen[ind-1]        \nlen1=cumlen[ind]\nDs=midlen-len0\nLambda = Ds/(len1-len0)\nreturn Lambda*xyz[ind]+(1-Lambda)*xyz[ind-1]", "path": "dipy\\core\\track_metrics.py", "repo_name": "mohini/dipy", "stars": 0, "license": "None", "language": "python", "size": 118}
{"docstring": "''' Calculate the average, min and max similarity matrices using Zhang 2008 distances\n\nParameters:\n---------------\nbundle: sequence \n        of tracks as arrays, shape (N1,3) .. (Nm,3)\n    \nReturns:\n----------\nS_avg :\nS_min :\nS_max :\n\nExamples :\n--------------\n\n    \n'''\n\n", "func_signal": "def bundle_similarities_zhang(bundle):\n", "code": "track_pairs = generate_combinations(range(len(bundle)), 2)\n\nS_avg=np.zeros((len(bundle),len(bundle)))\nS_min=np.zeros((len(bundle),len(bundle)))\nS_max=np.zeros((len(bundle),len(bundle)))\n\nfor p in track_pairs:\n    \n    S_avg[p[0],p[1]],S_min[p[0],p[1]],S_max[p[0],p[1]]=zhang_distances(bundle[p[0]],bundle[p[1]])        \n    \nreturn S_avg,S_min,S_max", "path": "dipy\\core\\track_metrics.py", "repo_name": "mohini/dipy", "stars": 0, "license": "None", "language": "python", "size": 118}
{"docstring": "''' Return set of possible returned values for column\n\nBy default, the column is the first column.\n\n\tReturns same values as ``set(obj.field1.values())`` and,\nwith the default initializing``fields`` argument of\nfields=('code',), this will return the same as\n``set(obj.code.values())``\n\nParameters\n----------\nname : {None, string}\n    Where default of none gives result for first column\n\n>>> codes = ((1, 'one'), (2, 'two'), (1, 'repeat value'))\n>>> vs = Recoder(codes).value_set()\n>>> vs == set([1, 2]) # Sets are not ordered, hence this test\nTrue\n>>> rc = Recoder(codes, fields=('code', 'label'))\n>>> rc.value_set('label') == set(('one', 'two', 'repeat value'))\nTrue\n\n'''\n", "func_signal": "def value_set(self, name=None):\n", "code": "if name is None:\n    d = self.field1\nelse:\n    d = self.__dict__[name]\nreturn set(d.values())", "path": "dipy\\io\\utils.py", "repo_name": "mohini/dipy", "stars": 0, "license": "None", "language": "python", "size": 118}
{"docstring": "''' Show window \n\nParameters\n----------\nren : vtkRenderer() object \n        as returned from function ren()\ntitle : string \n        a string for the window title bar\nsize : (int, int) \n        (width,height) of the window\n\nExamples\n--------    \n>>> from dipy.viz import fos\n>>> r=fos.ren()    \n>>> lines=[np.random.rand(10,3),np.random.rand(20,3)]    \n>>> colors=[0.2,0.8]\n>>> c=fos.line(lines,colors)    \n>>> fos.add(r,c)\n>>> l=fos.label(r)\n>>> fos.add(r,l)\n>>> fos.show(r)\n'''\n", "func_signal": "def show(ren,title='Fos',size=(300,300)):\n", "code": "ren.ResetCamera()        \nwindow = vtk.vtkRenderWindow()\nwindow.AddRenderer(ren)\nwindow.SetWindowName(title) \nwindow.SetSize(size)\nstyle=vtk.vtkInteractorStyleTrackballCamera()        \niren = vtk.vtkRenderWindowInteractor()\niren.SetRenderWindow(window)\niren.SetInteractorStyle(style)\niren.Start()", "path": "dipy\\viz\\fos.py", "repo_name": "mohini/dipy", "stars": 0, "license": "None", "language": "python", "size": 118}
{"docstring": "# we may have been given a single line instead of a series\n", "func_signal": "def line(lines,colors=None,opacity=1,linewidth=1):\n", "code": "if isinstance(lines, np.ndarray) and lines.ndim > 1:\n    raise ValueError('Need sequence of lines for lines argument')\n\nscalar=1.0\n\nif colors!=None:        \n    lit=iter(colors)\nelse:\n    colors=np.random.rand(len(lines),3)\n    lit=iter(colors)\n\nGL.glNewList(2, GL.GL_COMPILE)        \nnol=0\nfor Line in lines:\n    \n    inw=True\n    mit=iter(Line)\n    nit=iter(Line)\n    nit.next()\n    \n    scalar=lit.next()\n    GL.glBegin(GL.GL_LINE_STRIP)    \n    GL.glColor3f(scalar[0],scalar[1],scalar[2])\n    while(inw):            \n        try:\n            m=mit.next()                                        \n            GL.glVertex3f(m[0], m[1], m[2]) # point                                                \n        except StopIteration:\n            break\n\n    GL.glEnd()                                \n    \n    nol+=1\n    if nol%1000==0:            \n        print(nol,'Lines Loaded')\n    \nGL.glEndList()", "path": "dipy\\viz\\phos.py", "repo_name": "mohini/dipy", "stars": 0, "license": "None", "language": "python", "size": 118}
{"docstring": "''' Center of mass of streamline\n\nParameters\n----------\nxyz : array-like shape (N,3)\n   array representing x,y,z of N points in a track\n\nReturns\n-------\ncom : array shape (3,)\n   center of mass of streamline\n\nExamples\n--------\n>>> center_of_mass([])\nTraceback (most recent call last):\n   ...\nValueError: xyz array cannot be empty\n>>> center_of_mass([[1,1,1]])\narray([ 1.,  1.,  1.])\n>>> xyz = np.array([[0,0,0],[1,1,1],[2,2,2]])\n>>> center_of_mass(xyz)\narray([ 1.,  1.,  1.])\n'''\n", "func_signal": "def center_of_mass(xyz):\n", "code": "xyz = np.asarray(xyz)\nif xyz.size == 0:\n    raise ValueError('xyz array cannot be empty')\nreturn np.mean(xyz,axis=0)", "path": "dipy\\core\\track_metrics.py", "repo_name": "mohini/dipy", "stars": 0, "license": "None", "language": "python", "size": 118}
{"docstring": "''' Euclidean length of track line\n\nParameters\n----------\nxyz : array-like shape (N,3)\n   array representing x,y,z of N points in a track\nalong : bool, optional\n   If True, return array giving cumulative length along track,\n   otherwise (default) return scalar giving total length.\n\nReturns\n-------\nL : scalar or array shape (N-1,)\n   scalar in case of `along` == False, giving total length, array if\n   `along` == True, giving cumulative lengths.\n\nExamples\n--------\n>>> xyz = np.array([[1,1,1],[2,3,4],[0,0,0]])\n>>> expected_lens = np.sqrt([1+2**2+3**2, 2**2+3**2+4**2])\n>>> length(xyz) == expected_lens.sum()\nTrue\n>>> len_along = length(xyz, along=True)\n>>> np.allclose(len_along, expected_lens.cumsum())\nTrue\n>>> length([])\n0\n>>> length([[1, 2, 3]])\n0\n>>> length([], along=True)\narray([0])\n'''\n", "func_signal": "def length(xyz, along=False):\n", "code": "xyz = np.asarray(xyz)\nif xyz.shape[0] < 2:\n    if along:\n        return np.array([0])\n    return 0\ndists = np.sqrt((np.diff(xyz, axis=0)**2).sum(axis=1))\nif along:\n    return np.cumsum(dists)\nreturn np.sum(dists)", "path": "dipy\\core\\track_metrics.py", "repo_name": "mohini/dipy", "stars": 0, "license": "None", "language": "python", "size": 118}
{"docstring": "''' Convert recarray to dictionary\n\nAlso converts scalar values to scalars\n\nParameters\n----------\nrec : ndarray\n   structured ndarray\n\nReturns\n-------\ndct : dict\n   dict with key, value pairs as for `rec`\n\nExamples\n--------\n>>> r = np.zeros((), dtype = [('x', 'i4'), ('s', 'S10')])\n>>> d = rec2dict(r)\n>>> d == {'x': 0, 's': ''}\nTrue\n'''\n", "func_signal": "def rec2dict(rec):\n", "code": "dct = {}\nfor key in rec.dtype.fields:\n    val = rec[key]\n    try:\n        val = np.asscalar(val)\n    except ValueError:\n        pass\n    dct[key] = val\nreturn dct", "path": "dipy\\io\\utils.py", "repo_name": "mohini/dipy", "stars": 0, "license": "None", "language": "python", "size": 118}
{"docstring": "\"\"\"get_caseid() should return string containing caseid of the transaction\"\"\"\n", "func_signal": "def test_get_caseid(self):\n", "code": "headers = self.reader.get_headers_orig(self.msg)\nheaders = self.reader.get_headers_spl(headers)\nself.assertEqual(self.reader.get_caseid(headers), self.xml.test.caseid.get_text())", "path": "trunk\\main\\python\\test_emailhandler.py", "repo_name": "rgfernandez/MsgHandler", "stars": 1, "license": "None", "language": "python", "size": 1856}
{"docstring": "\"\"\"Send email message using SMTP.\"\"\"\n", "func_signal": "def send(self, contact, msg):\n", "code": "serv = smtplib.SMTP('localhost')\ntry:\n    serv.sendmail(getpass.getuser(), contact, msg)\nexcept smtplib.SMTPSenderRefused:\n    # hack for \"misconfigured\" servers\n    serv.sendmail(getpass.getuser()+self.cfg.get('email', 'server'), contact, msg)\nserv.quit()", "path": "trunk\\main\\python\\emailutil.py", "repo_name": "rgfernandez/MsgHandler", "stars": 1, "license": "None", "language": "python", "size": 1856}
{"docstring": "\"\"\"Return subject with case id information removed.\"\"\"\n", "func_signal": "def get_subject(self, headers):\n", "code": "if 'subject' in headers:\n    subj = headers['subject']\n    cid = re.search('(\\[caseid-.*\\])', subj)\n    if cid:\n        # remove caseid information from subject\n        subj = ' '.join([elem.strip() for elem in subj.split(cid.group())])\n    return ' '.join(filter(self.filt_subject, subj.split()))\nreturn ''", "path": "trunk\\main\\python\\emailutil.py", "repo_name": "rgfernandez/MsgHandler", "stars": 1, "license": "None", "language": "python", "size": 1856}
{"docstring": "\"\"\"get_attachments() should return dictionary containing email attachments\"\"\"\n", "func_signal": "def test_get_attachments(self):\n", "code": "attach_file = self.xml.test.attachments.get_text()\nattach_file = cPickle.loads(str(attach_file))\nself.assertEqual(self.reader.get_attachments(self.msg.get_payload()), attach_file)", "path": "trunk\\main\\python\\test_emailhandler.py", "repo_name": "rgfernandez/MsgHandler", "stars": 1, "license": "None", "language": "python", "size": 1856}
{"docstring": "\"\"\"Get headers used by the system and return as dictionary.\"\"\"\n", "func_signal": "def get_headers_spl(self, headers):\n", "code": "res = dict()\nvals = self.cfg.items('headers')\nfor (item, elem) in vals:\n    for obj in elem.split(','):\n        obj = obj.lower().strip()\n        if obj in headers:\n            res[item] = headers[obj]\n            break\nreturn res", "path": "trunk\\main\\python\\emailutil.py", "repo_name": "rgfernandez/MsgHandler", "stars": 1, "license": "None", "language": "python", "size": 1856}
{"docstring": "\"\"\"get_date() should return string containing when email was sent\"\"\"\n", "func_signal": "def test_get_date(self):\n", "code": "headers = self.reader.get_headers_orig(self.msg)\nheaders = self.reader.get_headers_spl(headers)\nself.assertEqual(self.reader.get_date(headers), self.xml.test.date.get_text())", "path": "trunk\\main\\python\\test_emailhandler.py", "repo_name": "rgfernandez/MsgHandler", "stars": 1, "license": "None", "language": "python", "size": 1856}
{"docstring": "\"\"\"Remove Re and Fwd from subject.\"\"\"\n", "func_signal": "def filt_subject(self, elem):\n", "code": "if elem in ('Re:', 'Fwd:'):\n    return ''\nelse:\n    return elem", "path": "trunk\\main\\python\\emailutil.py", "repo_name": "rgfernandez/MsgHandler", "stars": 1, "license": "None", "language": "python", "size": 1856}
{"docstring": "\"\"\"Return references.\"\"\"\n", "func_signal": "def get_references(self, headers):\n", "code": "if 'references' in headers:\n    refs = headers['references'].split()\n    refs.append(headers['message-id'])\n    return ' '.join(refs)\nelse:\n    return headers['message-id']", "path": "trunk\\main\\python\\emailutil.py", "repo_name": "rgfernandez/MsgHandler", "stars": 1, "license": "None", "language": "python", "size": 1856}
{"docstring": "\"\"\"Send msg response using XSender class.\"\"\"\n", "func_signal": "def respond_to_msg(self, contact, headers, text_content, attachments):\n", "code": "x = MsgSender(self.cfg, headers['mode'])\nx.process(contact, headers, text_content, attachments)", "path": "branches\\test\\python\\main.py", "repo_name": "rgfernandez/MsgHandler", "stars": 1, "license": "None", "language": "python", "size": 1856}
{"docstring": "\"\"\"Construct email message to send and return True.\"\"\"\n# http://snippets.dzone.com/posts/show/757\n# http://python.active-venture.com/lib/node510.html\n", "func_signal": "def send_message(self, contact, headers, text_content, attachments):\n", "code": "if attachments:\n    msg = MIMEMultipart()\n    msg.attach(MIMEText(text_content))\nelse:\n    msg = MIMEText(text_content)\n\nmsg.add_header('To', contact)\nfor (elem, item) in headers.items():\n    if elem.lower() not in ('subject', 'in-reply-to', 'references'):\n        elem = 'X-Eccs-%s' % elem\n    # capitalize words separated by dashes '-'\n    elem = '-'.join([x.capitalize() for x in elem.split('-')])\n    msg.add_header(elem, item)\n\nfor (elem, item) in attachments.items():\n    part = MIMEBase('application', \"octet-stream\")\n    part.set_payload(item)\n    Encoders.encode_base64(part)\n    part.add_header('Content-Disposition', 'attachment; filename=\"%s\"' % elem)\n    msg.attach(part)\n\nself.send(contact, msg.as_string())\nreturn True", "path": "trunk\\main\\python\\emailutil.py", "repo_name": "rgfernandez/MsgHandler", "stars": 1, "license": "None", "language": "python", "size": 1856}
{"docstring": "\"\"\"get_contact() should return string containing sender of email\"\"\"\n", "func_signal": "def test_get_contact(self):\n", "code": "headers = self.reader.get_headers_orig(self.msg)\nheaders = self.reader.get_headers_spl(headers)\nself.assertEqual(self.reader.get_contact(headers), self.xml.test.contact.get_text())", "path": "trunk\\main\\python\\test_emailhandler.py", "repo_name": "rgfernandez/MsgHandler", "stars": 1, "license": "None", "language": "python", "size": 1856}
{"docstring": "\"\"\"Return email address of sender.\"\"\"\n", "func_signal": "def get_contact(self, headers):\n", "code": "if 'from' in headers:\n    return email.utils.parseaddr(headers['from'])[1]\nreturn ''", "path": "trunk\\main\\python\\emailutil.py", "repo_name": "rgfernandez/MsgHandler", "stars": 1, "license": "None", "language": "python", "size": 1856}
{"docstring": "\"\"\"Parse email messages and return as tuple.\"\"\"\n", "func_signal": "def _parse(self, msg):\n", "code": "headers = self.get_headers(msg)\ncontact = self.get_contact(headers)\ntext_content = self.get_text_content(msg.get_payload())\nattachments = self.get_attachments(msg.get_payload())\n\nreturn (contact, headers, text_content, attachments)", "path": "trunk\\main\\python\\emailutil.py", "repo_name": "rgfernandez/MsgHandler", "stars": 1, "license": "None", "language": "python", "size": 1856}
{"docstring": "\"\"\"get_keyword() should return string containing keyword of the transaction\"\"\"\n", "func_signal": "def test_get_keyword(self):\n", "code": "headers = self.reader.get_headers_orig(self.msg)\nheaders = self.reader.get_headers_spl(headers)\nself.assertEqual(self.reader.get_keyword(headers), self.xml.test.keyword.get_text())", "path": "trunk\\main\\python\\test_emailhandler.py", "repo_name": "rgfernandez/MsgHandler", "stars": 1, "license": "None", "language": "python", "size": 1856}
{"docstring": "\"\"\"Return case id.\"\"\"\n", "func_signal": "def get_caseid(self, headers):\n", "code": "if 'caseid' in headers:\n    return headers['caseid']\nif 'subject' in headers:\n    # search for caseid in subject header\n    str_search = re.search('(?<=\\[caseid-).*(?=\\].*)', headers['subject'])\n    if str_search is not None:\n        return str_search.group()\nreturn ''", "path": "trunk\\main\\python\\emailutil.py", "repo_name": "rgfernandez/MsgHandler", "stars": 1, "license": "None", "language": "python", "size": 1856}
{"docstring": "\"\"\"Return body of email as string.\"\"\"\n", "func_signal": "def get_text_content(self, content):\n", "code": "try:\n    return self.get_text_content(content[0].get_payload())\nexcept AttributeError:\n    return content", "path": "trunk\\main\\python\\emailutil.py", "repo_name": "rgfernandez/MsgHandler", "stars": 1, "license": "None", "language": "python", "size": 1856}
{"docstring": "\"\"\"Get email headers and return as dictionary.\"\"\"\n", "func_signal": "def get_headers_orig(self, msg):\n", "code": "headers = [(elem.lower(), item) for (elem, item) in msg.items()]\nlog.info(headers)\nreturn dict(headers)", "path": "trunk\\main\\python\\emailutil.py", "repo_name": "rgfernandez/MsgHandler", "stars": 1, "license": "None", "language": "python", "size": 1856}
{"docstring": "\"\"\"Add special headers to existing email headers.\"\"\"\n", "func_signal": "def get_headers(self, msg):\n", "code": "headers = self.get_headers_orig(msg)\nheaders = self.get_headers_spl(msg)\nheaders['caseid'] = self.get_caseid(headers)\nheaders['subject'] = self.get_subject(headers)\nheaders['references'] = self.get_references(headers)\nheaders['date'] = self.get_date(headers)\nheaders['mode'] = self.mode\n\nreturn headers", "path": "trunk\\main\\python\\emailutil.py", "repo_name": "rgfernandez/MsgHandler", "stars": 1, "license": "None", "language": "python", "size": 1856}
{"docstring": "\"\"\"get_headers should return dictionary containing email headers\"\"\"\n", "func_signal": "def test_get_headers(self):\n", "code": "headers = self.reader.get_headers(self.msg)\n\nheader_file = self.xml.test.headers.get_text()\nheader_file = '<'.join(header_file.split('&lt;'))\nheader_file = '>'.join(header_file.split('&gt;'))\nheader_file = cPickle.loads(str(header_file))\nself.assertEqual(headers, header_file)", "path": "trunk\\main\\python\\test_emailhandler.py", "repo_name": "rgfernandez/MsgHandler", "stars": 1, "license": "None", "language": "python", "size": 1856}
{"docstring": "\"\"\"If test mode, forward message. Else, call appropriate module.\"\"\"\n", "func_signal": "def run(self, emailtext='', test_mode=False):\n", "code": "msg = self.get_message(emailtext)\noutp = self._parse(msg)\nlog.info(outp)\n(contact, headers, text_content, attachments) = outp\n\nx = msgutil.MsgReader(self.cfg, test_mode)\nx.process(*outp)", "path": "trunk\\main\\python\\emailutil.py", "repo_name": "rgfernandez/MsgHandler", "stars": 1, "license": "None", "language": "python", "size": 1856}
{"docstring": "\"\"\"errcheck function for Windows functions that return a HANDLE.\"\"\"\n", "func_signal": "def ErrCheckHandle(result, func, args):\n", "code": "if not result:\n    raise WinError()\nreturn AutoHANDLE(result)", "path": "python-modules\\mozrunner\\winprocess.py", "repo_name": "sid984/firefox-freenet-plugin", "stars": 1, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\"Looks up a dotted task name and imports the module as necessary\nto get at the task.\"\"\"\n", "func_signal": "def _import_task(taskname):\n", "code": "parts = taskname.split('.')\nif len(parts) < 2:\n    return None\nfunc_name = parts[-1]\nfull_mod_name = \".\".join(parts[:-1])\nmod_name = parts[-2]\ntry:\n    module = __import__(full_mod_name, globals(), locals(), [mod_name])\nexcept ImportError:\n    return None\nreturn getattr(module, func_name, None)", "path": "python-modules\\paver\\tasks.py", "repo_name": "sid984/firefox-freenet-plugin", "stars": 1, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\"Runs an external command. If capture is True, the output of the\ncommand will be captured and returned as a string.  If the command \nhas a non-zero return code raise a BuildFailure. You can pass\nignore_error=True to allow non-zero return codes to be allowed to\npass silently, silently into the night.  If you pass cwd='some/path'\npaver will chdir to 'some/path' before exectuting the command.\n\nIf the dry_run option is True, the command will not\nactually be run.\"\"\"\n", "func_signal": "def sh(command, capture=False, ignore_error=False, cwd=None):\n", "code": "def runpipe():\n    kwargs = { 'shell': True, 'stderr': subprocess.PIPE, 'cwd': cwd}\n    if capture:\n        kwargs['stdout'] = subprocess.PIPE\n    p = subprocess.Popen(command, **kwargs)\n    p.wait()\n    if p.returncode and not ignore_error:\n        error(p.stderr.read())\n        raise BuildFailure(\"Subprocess return code: %d\" % p.returncode)\n\n    if capture:\n        return p.stdout.read()\n\nreturn dry(command, runpipe)", "path": "python-modules\\paver\\easy.py", "repo_name": "sid984/firefox-freenet-plugin", "stars": 1, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\"\nReturn a JSON string representation of a Python data structure.\n\n>>> JSONEncoder().encode({\"foo\": [\"bar\", \"baz\"]})\n'{\"foo\": [\"bar\", \"baz\"]}'\n\"\"\"\n# This is for extremely simple cases and benchmarks.\n", "func_signal": "def encode(self, o):\n", "code": "if isinstance(o, basestring):\n    if isinstance(o, str):\n        _encoding = self.encoding\n        if (_encoding is not None \n                and not (_encoding == 'utf-8')):\n            o = o.decode(_encoding)\n    if self.ensure_ascii:\n        return encode_basestring_ascii(o)\n    else:\n        return encode_basestring(o)\n# This doesn't pass the iterator directly to ''.join() because the\n# exceptions aren't as detailed.  The list call should be roughly\n# equivalent to the PySequence_Fast that ''.join() would do.\nchunks = list(self.iterencode(o))\nreturn ''.join(chunks)", "path": "python-modules\\simplejson\\encoder.py", "repo_name": "sid984/firefox-freenet-plugin", "stars": 1, "license": "None", "language": "python", "size": 236}
{"docstring": "# this is where global options should be dealt with\n", "func_signal": "def _parse_global_options(args):\n", "code": "parser = optparse.OptionParser(usage=\n    \"\"\"Usage: %prog [global options] taskname [task options] \"\"\"\n    \"\"\"[taskname [taskoptions]]\"\"\", version=\"Paver %s\" % (VERSION),\n    add_help_option=False)\n\nenvironment.help_function = parser.print_help\n\nparser.add_option('-n', '--dry-run', action='store_true',\n                help=\"don't actually do anything\")\nparser.add_option('-v', \"--verbose\", action=\"store_true\",\n                help=\"display all logging output\")\nparser.add_option('-q', '--quiet', action=\"store_true\",\n                help=\"display only errors\")\nparser.add_option(\"-i\", \"--interactive\", action=\"store_true\",\n                help=\"enable prompting\")\nparser.add_option(\"-f\", \"--file\", metavar=\"FILE\",\n                help=\"read tasks from FILE [%default]\")\nparser.add_option('-h', \"--help\", action=\"store_true\",\n                help=\"display this help information\")\nparser.set_defaults(file=environment.pavement_file)\n\nparser.disable_interspersed_args()\noptions, args = parser.parse_args(args)\nif options.help:\n    args.insert(0, \"help\")\nfor key, value in vars(options).items():\n    setattr(environment, key, value)\n    \nreturn args", "path": "python-modules\\paver\\tasks.py", "repo_name": "sid984/firefox-freenet-plugin", "stars": 1, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\"Sets the command line options that can be set for this task.\nThis uses the same format as the distutils command line option\nparser. It's a list of tuples, each with three elements:\nlong option name, short option, description.\n\nIf the long option name ends with '=', that means that the\noption takes a value. Otherwise the option is just boolean.\nAll of the options will be stored in the options dict with\nthe name of the task. Each value that gets stored in that\ndict will be stored with a key that is based on the long option\nname (the only difference is that - is replaced by _).\"\"\"\n", "func_signal": "def cmdopts(options):\n", "code": "def entangle(func):\n    func = task(func)\n    func.user_options = options\n    return func\nreturn entangle", "path": "python-modules\\paver\\tasks.py", "repo_name": "sid984/firefox-freenet-plugin", "stars": 1, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\" Return the path to which this symbolic link points.\n\nThe result is always an absolute path.\n\"\"\"\n", "func_signal": "def readlinkabs(self):\n", "code": "p = self.readlink()\nif p.isabs():\n    return p\nelse:\n    return (self.parent / p).abspath()", "path": "python-modules\\paver\\path.py", "repo_name": "sid984/firefox-freenet-plugin", "stars": 1, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\" p.splitdrive() -> Return (p.drive, <the rest of p>).\n\nSplit the drive specifier from this path.  If there is\nno drive specifier, p.drive is empty, so the return value\nis simply (path(''), p).  This is always the case on Unix.\n\"\"\"\n", "func_signal": "def splitdrive(self):\n", "code": "drive, rel = os.path.splitdrive(self)\nreturn self.__class__(drive), rel", "path": "python-modules\\paver\\path.py", "repo_name": "sid984/firefox-freenet-plugin", "stars": 1, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\" D.walkdirs() -> iterator over subdirs, recursively.\n\nWith the optional 'pattern' argument, this yields only\ndirectories whose names match the given pattern.  For\nexample, ``mydir.walkdirs('*test')`` yields only directories\nwith names ending in 'test'.\n\nThe errors= keyword argument controls behavior when an\nerror occurs.  The default is 'strict', which causes an\nexception.  The other allowed values are 'warn', which\nreports the error via warnings.warn(), and 'ignore'.\n\"\"\"\n", "func_signal": "def walkdirs(self, pattern=None, errors='strict'):\n", "code": "if errors not in ('strict', 'warn', 'ignore'):\n    raise ValueError(\"invalid errors parameter\")\n\ntry:\n    dirs = self.dirs()\nexcept Exception:\n    if errors == 'ignore':\n        return\n    elif errors == 'warn':\n        warnings.warn(\n            \"Unable to list directory '%s': %s\"\n            % (self, sys.exc_info()[1]),\n            TreeWalkWarning)\n        return\n    else:\n        raise\n\nfor child in dirs:\n    if pattern is None or child.fnmatch(pattern):\n        yield child\n    for subsubdir in child.walkdirs(pattern, errors):\n        yield subsubdir", "path": "python-modules\\paver\\path.py", "repo_name": "sid984/firefox-freenet-plugin", "stars": 1, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\" D.walkfiles() -> iterator over files in D, recursively.\n\nThe optional argument, pattern, limits the results to files\nwith names that match the pattern.  For example,\n``mydir.walkfiles('*.tmp')`` yields only files with the .tmp\nextension.\n\"\"\"\n", "func_signal": "def walkfiles(self, pattern=None, errors='strict'):\n", "code": "if errors not in ('strict', 'warn', 'ignore'):\n    raise ValueError(\"invalid errors parameter\")\n\ntry:\n    childList = self.listdir()\nexcept Exception:\n    if errors == 'ignore':\n        return\n    elif errors == 'warn':\n        warnings.warn(\n            \"Unable to list directory '%s': %s\"\n            % (self, sys.exc_info()[1]),\n            TreeWalkWarning)\n        return\n    else:\n        raise\n\nfor child in childList:\n    try:\n        isfile = child.isfile()\n        isdir = not isfile and child.isdir()\n    except:\n        if errors == 'ignore':\n            continue\n        elif errors == 'warn':\n            warnings.warn(\n                \"Unable to access '%s': %s\"\n                % (self, sys.exc_info()[1]),\n                TreeWalkWarning)\n            continue\n        else:\n            raise\n\n    if isfile:\n        if pattern is None or child.fnmatch(pattern):\n            yield child\n    elif isdir:\n        for f in child.walkfiles(pattern, errors):\n            yield f", "path": "python-modules\\paver\\path.py", "repo_name": "sid984/firefox-freenet-plugin", "stars": 1, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\" Return a list of path objects that match the pattern.\n\npattern - a path relative to this directory, with wildcards.\n\nFor example, path('/users').glob('*/bin/*') returns a list\nof all the files users have in their bin directories.\n\"\"\"\n", "func_signal": "def glob(self, pattern):\n", "code": "cls = self.__class__\nreturn [cls(s) for s in glob.glob(_base(self / pattern))]", "path": "python-modules\\paver\\path.py", "repo_name": "sid984/firefox-freenet-plugin", "stars": 1, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\"errcheck function for Windows functions that return a BOOL True\non success\"\"\"\n", "func_signal": "def ErrCheckBool(result, func, args):\n", "code": "if not result:\n    raise WinError()\nreturn args", "path": "python-modules\\mozrunner\\winprocess.py", "repo_name": "sid984/firefox-freenet-plugin", "stars": 1, "license": "None", "language": "python", "size": 236}
{"docstring": "# Check for specials.  Note that this type of test is processor- and/or\n# platform-specific, so do tests which don't depend on the internals.\n\n", "func_signal": "def floatstr(o, allow_nan=True):\n", "code": "if o != o:\n    text = 'NaN'\nelif o == INFINITY:\n    text = 'Infinity'\nelif o == -INFINITY:\n    text = '-Infinity'\nelse:\n    return FLOAT_REPR(o)\n\nif not allow_nan:\n    raise ValueError(\"Out of range float values are not JSON compliant: %r\"\n        % (o,))\n\nreturn text", "path": "python-modules\\simplejson\\encoder.py", "repo_name": "sid984/firefox-freenet-plugin", "stars": 1, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\"\nEncode the given object and yield each string\nrepresentation as available.\n\nFor example::\n    \n    for chunk in JSONEncoder().iterencode(bigobject):\n        mysocket.write(chunk)\n\"\"\"\n", "func_signal": "def iterencode(self, o):\n", "code": "if self.check_circular:\n    markers = {}\nelse:\n    markers = None\nreturn self._iterencode(o, markers)", "path": "python-modules\\simplejson\\encoder.py", "repo_name": "sid984/firefox-freenet-plugin", "stars": 1, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\" Return a relative path from self to dest.\n\nIf there is no relative path from self to dest, for example if\nthey reside on different drives in Windows, then this returns\ndest.abspath().\n\"\"\"\n", "func_signal": "def relpathto(self, dest):\n", "code": "origin = self.abspath()\ndest = self.__class__(dest).abspath()\n\norig_list = origin.normcase().splitall()\n# Don't normcase dest!  We want to preserve the case.\ndest_list = dest.splitall()\n\nif orig_list[0] != os.path.normcase(dest_list[0]):\n    # Can't get here from there.\n    return dest\n\n# Find the location where the two paths start to differ.\ni = 0\nfor start_seg, dest_seg in zip(orig_list, dest_list):\n    if start_seg != os.path.normcase(dest_seg):\n        break\n    i += 1\n\n# Now i is the point where the two paths diverge.\n# Need a certain number of \"os.pardir\"s to work up\n# from the origin to the point of divergence.\nsegments = [os.pardir] * (len(orig_list) - i)\n# Need to add the diverging part of dest_list.\nsegments += dest_list[i:]\nif len(segments) == 0:\n    # If they happen to be identical, use os.curdir.\n    relpath = os.curdir\nelse:\n    relpath = os.path.join(*segments)\nreturn self.__class__(relpath)", "path": "python-modules\\paver\\path.py", "repo_name": "sid984/firefox-freenet-plugin", "stars": 1, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\"Specifies tasks upon which this task depends.\n\nreq can be a string or a list of strings with the names\nof the tasks. You can call this decorator multiple times\nand the various requirements are added on. You can also\ncall with the requirements as a list of arguments.\n\nThe requirements are called in the order presented in the\nlist.\"\"\"\n", "func_signal": "def needs(*args):\n", "code": "def entangle(func):\n    req = args\n    func = task(func)\n    needs_list = func.needs\n    if len(req) == 1:\n        req = req[0]\n    if isinstance(req, basestring):\n        needs_list.append(req)\n    elif isinstance(req, (list, tuple)):\n        needs_list.extend(req)\n    else:\n        raise PavementError(\"'needs' decorator requires a list or string \"\n                            \"but got %s\" % req)\n    return func\nreturn entangle", "path": "python-modules\\paver\\tasks.py", "repo_name": "sid984/firefox-freenet-plugin", "stars": 1, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\"Specify that this task does not depend on the auto task,\nand don't run the auto task just for this one.\"\"\"\n", "func_signal": "def no_auto(func):\n", "code": "func = task(func)\nfunc.no_auto = True\nreturn func", "path": "python-modules\\paver\\tasks.py", "repo_name": "sid984/firefox-freenet-plugin", "stars": 1, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\"Wraps a function that performs a destructive operation, so that\nnothing will happen when a dry run is requested.\n\nRuns func with the given arguments and keyword arguments. If this\nis a dry run, print the message rather than running the function.\"\"\"\n", "func_signal": "def dry(message, func, *args, **kw):\n", "code": "info(message)\nif tasks.environment.dry_run:\n    return\nreturn func(*args, **kw)", "path": "python-modules\\paver\\easy.py", "repo_name": "sid984/firefox-freenet-plugin", "stars": 1, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\"\nReturn a JSON representation of a Python string\n\"\"\"\n", "func_signal": "def encode_basestring(s):\n", "code": "def replace(match):\n    return ESCAPE_DCT[match.group(0)]\nreturn '\"' + ESCAPE.sub(replace, s) + '\"'", "path": "python-modules\\simplejson\\encoder.py", "repo_name": "sid984/firefox-freenet-plugin", "stars": 1, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\"Any command line arguments that appear after this task on the\ncommand line will be placed in options.args.\"\"\"\n", "func_signal": "def consume_args(func):\n", "code": "func = task(func)\nfunc.consume_args = True\nreturn func", "path": "python-modules\\paver\\tasks.py", "repo_name": "sid984/firefox-freenet-plugin", "stars": 1, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\"Called when a folder sync operation is started.\"\"\"\n", "func_signal": "def skippingfolder(s, folder):\n", "code": "if s.verbose >= 0:\n    s._msg(\"Skipping %s (not changed)\" % folder.getname())", "path": "offlineimap\\ui\\UIBase.py", "repo_name": "brong/brong-offlineimap", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 6608}
{"docstring": "\"\"\"Called when a folder sync operation is started.\"\"\"\n", "func_signal": "def syncingfolder(s, srcrepos, srcfolder, destrepos, destfolder):\n", "code": "if s.verbose >= 0:\n    s._msg(\"Syncing %s: %s -> %s\" % (srcfolder.getname(),\n                                     s.getnicename(srcrepos),\n                                     s.getnicename(destrepos)))", "path": "offlineimap\\ui\\UIBase.py", "repo_name": "brong/brong-offlineimap", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 6608}
{"docstring": "\"\"\"Sleep handler.  Returns same value as UIBase.sleep:\n0 if timeout expired, 1 if there was a request to cancel the timer,\nand 2 if there is a request to abort the program.\n\nAlso, returns 100 if configured to not sleep at all.\"\"\"\n\n", "func_signal": "def sleeper(self, siglistener):\n", "code": "if not self.refreshperiod:\n    return 100\n\nkaobjs = []\n\nif hasattr(self, 'localrepos'):\n    kaobjs.append(self.localrepos)\nif hasattr(self, 'remoterepos'):\n    kaobjs.append(self.remoterepos)\n\nfor item in kaobjs:\n    item.startkeepalive()\n\nrefreshperiod = int(self.refreshperiod * 60)", "path": "offlineimap\\accounts.py", "repo_name": "brong/brong-offlineimap", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 6608}
{"docstring": "\"\"\"Wait until the semaphore gets back to its original state -- all acquired\nresources released.\"\"\"\n", "func_signal": "def semaphorereset(semaphore, originalstate):\n", "code": "for i in range(originalstate):\n    semaphore.acquire()\n# Now release these.\nfor i in range(originalstate):\n    semaphore.release()", "path": "offlineimap\\threadutil.py", "repo_name": "brong/brong-offlineimap", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 6608}
{"docstring": "\"\"\"Recognizes a thread has exited.\"\"\"\n", "func_signal": "def unregisterthread(s, thr):\n", "code": "if s.threadaccounts.has_key(thr):\n    del s.threadaccounts[thr]", "path": "offlineimap\\ui\\UIBase.py", "repo_name": "brong/brong-offlineimap", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 6608}
{"docstring": "\"\"\"Takes a string from an IMAP conversation and returns a list containing\nits components.  One example string is:\n\n(\\\\HasNoChildren) \".\" \"INBOX.Sent\"\n\nThe result from parsing this will be:\n\n['(\\\\HasNoChildren)', '\".\"', '\"INBOX.Sent\"']\"\"\"\n\n", "func_signal": "def imapsplit(imapstring):\n", "code": "debug(\"imapsplit() called with input:\", imapstring)\nif type(imapstring) != types.StringType:\n    debug(\"imapsplit() got a non-string input; working around.\")\n    # Sometimes, imaplib will throw us a tuple if the input\n    # contains a literal.  See Python bug\n    # #619732 at https://sourceforge.net/tracker/index.php?func=detail&aid=619732&group_id=5470&atid=105470\n    # One example is:\n    # result[0] = '() \"\\\\\\\\\" Admin'\n    # result[1] = ('() \"\\\\\\\\\" {19}', 'Folder\\\\2')\n    #\n    # This function will effectively get result[0] or result[1], so\n    # if we get the result[1] version, we need to parse apart the tuple\n    # and figure out what to do with it.  Each even-numbered\n    # part of it should end with the {} number, and each odd-numbered\n    # part should be directly a part of the result.  We'll\n    # artificially quote it to help out.\n    retval = []\n    for i in range(len(imapstring)):\n        if i % 2:                   # Odd: quote then append.\n            arg = imapstring[i]\n            # Quote code lifted from imaplib\n            arg = arg.replace('\\\\', '\\\\\\\\')\n            arg = arg.replace('\"', '\\\\\"')\n            arg = '\"%s\"' % arg\n            debug(\"imapsplit() non-string [%d]: Appending %s\" %\\\n                  (i, arg))\n            retval.append(arg)\n        else:\n            # Even -- we have a string that ends with a literal\n            # size specifier.  We need to strip off that, then run\n            # what remains through the regular imapsplit parser.\n            # Recursion to the rescue.\n            arg = imapstring[i]\n            arg = re.sub('\\{\\d+\\}$', '', arg)\n            debug(\"imapsplit() non-string [%d]: Feeding %s to recursion\" %\\\n                  (i, arg))\n            retval.extend(imapsplit(arg))\n    debug(\"imapsplit() non-string: returning %s\" % str(retval))\n    return retval\n    \nworkstr = imapstring.strip()\nretval = []\nwhile len(workstr):\n    if workstr[0] == '(':\n        rparenc = 1 # count of right parenthesis to match\n        rpareni = 1 # position to examine", "path": "offlineimap\\imaputil.py", "repo_name": "brong/brong-offlineimap", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 6608}
{"docstring": "\"\"\"Called when the UI starts.  Must be called before any other UI\ncall except isusable().  Displays the copyright banner.  This is\nwhere the UI should do its setup -- TK, for instance, would\ncreate the application window here.\"\"\"\n", "func_signal": "def init_banner(s):\n", "code": "if s.verbose >= 0:\n    s._msg(offlineimap.version.banner)", "path": "offlineimap\\ui\\UIBase.py", "repo_name": "brong/brong-offlineimap", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 6608}
{"docstring": "\"\"\"Generic tool called when no other works.\"\"\"\n", "func_signal": "def _msg(s, msg):\n", "code": "s._log(msg)\ns._display(msg)", "path": "offlineimap\\ui\\UIBase.py", "repo_name": "brong/brong-offlineimap", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 6608}
{"docstring": "\"\"\"Called when a thread has terminated with an exception.\nThe argument is the ExitNotifyThread that has so terminated.\"\"\"\n", "func_signal": "def threadException(s, thread):\n", "code": "s._msg(s.getThreadExceptionString(thread))\ns.delThreadDebugLog(thread)\ns.terminate(100)", "path": "offlineimap\\ui\\UIBase.py", "repo_name": "brong/brong-offlineimap", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 6608}
{"docstring": "\"\"\"This function does not actually output anything, but handles\nthe overall sleep, dealing with updates as necessary.  It will,\nhowever, call sleeping() which DOES output something.\n\nReturns 0 if timeout expired, 1 if there is a request to cancel\nthe timer, and 2 if there is a request to abort the program.\"\"\"\n\n", "func_signal": "def sleep(s, sleepsecs, siglistener):\n", "code": "abortsleep = 0\nwhile sleepsecs > 0 and not abortsleep:\n    try:\n        abortsleep = siglistener.get_nowait()\n        # retrieved signal while sleeping: 1 means immediately resynch, 2 means immediately die\n    except Empty:\n        # no signal\n        abortsleep = s.sleeping(1, sleepsecs)\n    sleepsecs -= 1\ns.sleeping(0, 0)               # Done sleeping.\nreturn abortsleep", "path": "offlineimap\\ui\\UIBase.py", "repo_name": "brong/brong-offlineimap", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 6608}
{"docstring": "\"\"\"Called when a thread exits.\"\"\"\n", "func_signal": "def threadexited(thread):\n", "code": "ui = UIBase.getglobalui()\nif thread.getExitCause() == 'EXCEPTION':\n    if isinstance(thread.getExitException(), SystemExit):\n        # Bring a SystemExit into the main thread.\n        # Do not send it back to UI layer right now.\n        # Maybe later send it to ui.terminate?\n        raise SystemExit\n    ui.threadException(thread)      # Expected to terminate\n    sys.exit(100)                   # Just in case...\n    os._exit(100)\nelif thread.getExitMessage() == 'SYNC_WITH_TIMER_TERMINATE':\n    ui.terminate()\n    # Just in case...\n    sys.exit(100)\n    os._exit(100)\nelse:\n    ui.threadExited(thread)", "path": "offlineimap\\threadutil.py", "repo_name": "brong/brong-offlineimap", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 6608}
{"docstring": "\"\"\"Obtain a lock.  Provides nice support for a single\nthread trying to lock it several times -- as may be the case\nif one I/O-using object calls others, while wanting to make it all\nan atomic operation.  Keeps a \"lock request count\" for the current\nthread, and acquires the lock when it goes above zero, releases when\nit goes below one.\n\nThis call is always blocking.\"\"\"\n\n# First, check to see if this thread already has a lock.\n# If so, increment the lock count and just return.\n", "func_signal": "def acquire(self):\n", "code": "self.statuslock.acquire()\ntry:\n    threadid = thread.get_ident()\n\n    if threadid in self.locksheld:\n        self.locksheld[threadid] += 1\n        return\n    else:\n        # This is safe because it is a per-thread structure\n        self.locksheld[threadid] = 1\nfinally:\n    self.statuslock.release()\nself.lock.acquire()", "path": "offlineimap\\threadutil.py", "repo_name": "brong/brong-offlineimap", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 6608}
{"docstring": "\"\"\"Log it to disk.  Returns true if it wrote something; false\notherwise.\"\"\"\n", "func_signal": "def _log(s, msg):\n", "code": "if s.logfile:\n    s.logfile.write(\"%s: %s\\n\" % (threading.currentThread().getName(),\n                                  msg))\n    return 1\nreturn 0", "path": "offlineimap\\ui\\UIBase.py", "repo_name": "brong/brong-offlineimap", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 6608}
{"docstring": "\"\"\"Called when a thread has exited normally.  Many UIs will\njust ignore this.\"\"\"\n", "func_signal": "def threadExited(s, thread):\n", "code": "s.delThreadDebugLog(thread)\ns.unregisterthread(thread)", "path": "offlineimap\\ui\\UIBase.py", "repo_name": "brong/brong-offlineimap", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 6608}
{"docstring": "\"\"\"Enter an infinite \"monitoring\" loop.  The argument, callback,\ndefines the function to call when an ExitNotifyThread has terminated.\nThat function is called with a single argument -- the ExitNotifyThread\nthat has terminated.  The monitor will not continue to monitor for\nother threads until the function returns, so if it intends to perform\nlong calculations, it should start a new thread itself -- but NOT\nan ExitNotifyThread, or else an infinite loop may result.  Furthermore,\nthe monitor will hold the lock all the while the other thread is waiting.\n\"\"\"\n", "func_signal": "def exitnotifymonitorloop(callback):\n", "code": "global exitthreads\nwhile 1:                            # Loop forever.\n    try:\n        thrd = exitthreads.get(False)\n        callback(thrd)\n    except Empty:\n        time.sleep(1)", "path": "offlineimap\\threadutil.py", "repo_name": "brong/brong-offlineimap", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 6608}
{"docstring": "\"\"\"Takes a string which may or may not be quoted and returns it, unquoted.\nThis function does NOT consider parenthised lists to be quoted.\n\"\"\"\n\n", "func_signal": "def dequote(string):\n", "code": "debug(\"dequote() called with input:\", string)\nif not (string[0] == '\"' and string[-1] == '\"'):\n    return string\nstring = string[1:-1]               # Strip off quotes.\nstring = string.replace('\\\\\"', '\"')\nstring = string.replace('\\\\\\\\', '\\\\')\ndebug(\"dequote() returning:\", string)\nreturn string", "path": "offlineimap\\imaputil.py", "repo_name": "brong/brong-offlineimap", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 6608}
{"docstring": "\"\"\"Initialize the instance-limited thread implementation to permit\nup to intancemax threads with the given instancename.\"\"\"\n", "func_signal": "def initInstanceLimit(instancename, instancemax):\n", "code": "instancelimitedlock.acquire()\nif not instancelimitedsems.has_key(instancename):\n    instancelimitedsems[instancename] = BoundedSemaphore(instancemax)\ninstancelimitedlock.release()", "path": "offlineimap\\threadutil.py", "repo_name": "brong/brong-offlineimap", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 6608}
{"docstring": "\"\"\"Takes a configparser object and a boxlist, which is a list of hashes\ncontaining 'accountname' and 'foldername' keys.\"\"\"\n", "func_signal": "def genmbnames():\n", "code": "mblock.acquire()\ntry:\n    localeval = config.getlocaleval()\n    if not config.getdefaultboolean(\"mbnames\", \"enabled\", 0):\n        return\n    file = open(os.path.expanduser(config.get(\"mbnames\", \"filename\")), \"wt\")\n    file.write(localeval.eval(config.get(\"mbnames\", \"header\")))\n    folderfilter = lambda accountname, foldername: 1\n    if config.has_option(\"mbnames\", \"folderfilter\"):\n        folderfilter = localeval.eval(config.get(\"mbnames\", \"folderfilter\"),\n                                      {'re': re})\n    itemlist = []\n    for accountname in boxes.keys():\n        for foldername in boxes[accountname]:\n            if folderfilter(accountname, foldername):\n                itemlist.append(config.get(\"mbnames\", \"peritem\", raw=1) % \\\n                                {'accountname': accountname,\n                                 'foldername': foldername})\n    file.write(localeval.eval(config.get(\"mbnames\", \"sep\")).join(itemlist))\n    file.write(localeval.eval(config.get(\"mbnames\", \"footer\")))\n    file.close()\nfinally:\n    mblock.release()", "path": "offlineimap\\mbnames.py", "repo_name": "brong/brong-offlineimap", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 6608}
{"docstring": "# We don't need an account lock because syncitall() goes through\n# each account once, then waits for all to finish.\n\n", "func_signal": "def sync(self, siglistener):\n", "code": "hook = self.getconf('presynchook', '')\nself.callhook(hook)\n\nquickconfig = self.getconfint('quick', 0)\nif quickconfig < 0:\n    quick = True\nelif quickconfig > 0:\n    if self.quicknum == 0 or self.quicknum > quickconfig:\n        self.quicknum = 1\n        quick = False\n    else:\n        self.quicknum = self.quicknum + 1\n        quick = True\nelse:\n    quick = False\n\ntry:\n    remoterepos = self.remoterepos\n    localrepos = self.localrepos\n    statusrepos = self.statusrepos\n    self.ui.syncfolders(remoterepos, localrepos)\n    remoterepos.syncfoldersto(localrepos, [statusrepos])\n\n    siglistener.addfolders(remoterepos.getfolders(), bool(self.refreshperiod), quick)\n\n    while True:\n        folderthreads = []\n        for remotefolder, quick in siglistener.queuedfolders():\n            thread = InstanceLimitedThread(\\\n                instancename = 'FOLDER_' + self.remoterepos.getname(),\n                target = syncfolder,\n                name = \"Folder sync %s[%s]\" % \\\n                (self.name, remotefolder.getvisiblename()),\n                args = (self.name, remoterepos, remotefolder, localrepos,\n                        statusrepos, quick))\n            thread.setDaemon(1)\n            thread.start()\n            folderthreads.append(thread)\n        threadutil.threadsreset(folderthreads)\n        if siglistener.clearfolders():\n            break\n    mbnames.write()\n    localrepos.forgetfolders()\n    remoterepos.forgetfolders()\n    localrepos.holdordropconnections()\n    remoterepos.holdordropconnections()\nfinally:\n    pass\n\nhook = self.getconf('postsynchook', '')\nself.callhook(hook)", "path": "offlineimap\\accounts.py", "repo_name": "brong/brong-offlineimap", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 6608}
{"docstring": "\"\"\"Sleep for sleepsecs, remainingsecs to go.\nIf sleepsecs is 0, indicates we're done sleeping.\n\nReturn 0 for normal sleep, or 1 to indicate a request\nto sync immediately.\"\"\"\n", "func_signal": "def sleeping(s, sleepsecs, remainingsecs):\n", "code": "s._msg(\"Next refresh in %d seconds\" % remainingsecs)\nif sleepsecs > 0:\n    time.sleep(sleepsecs)\nreturn 0", "path": "offlineimap\\ui\\UIBase.py", "repo_name": "brong/brong-offlineimap", "stars": 1, "license": "gpl-2.0", "language": "python", "size": 6608}
{"docstring": "\"\"\"apply one or more ORDER BY criterion to the query and return the newly resulting ``Query``\"\"\"\n\n", "func_signal": "def order_by(self, *criterion):\n", "code": "criterion = [self._adapt_clause(expression._literal_as_text(o), True, True) for o in criterion]\n\nif self._order_by is False:\n    self._order_by = criterion\nelse:\n    self._order_by = self._order_by + criterion", "path": "lib\\sqlalchemy\\orm\\query.py", "repo_name": "tehasdf/sqlalchemy", "stars": 0, "license": "other", "language": "python", "size": 6836}
{"docstring": "\"\"\"Perform a bulk update query.\n\nUpdates rows matched by this query in the database. The values parameter takes\na dictionary with object attributes as keys and literal values or sql expressions\nas values. The synchronize_session parameter chooses the strategy to update the\nattributes on objects in the session. Valid values are:\n\nFalse\n  don't synchronize the session. Use this when you don't need to use the\n  session after the update or you can be sure that none of the matched objects\n  are in the session.\n\n'expire'\n  performs a select query before the update to find objects that are matched\n  by the update query. The updated attributes are expired on matched objects.\n\n'evaluate'\n  experimental feature. Tries to evaluate the querys criteria in Python\n  straight on the objects in the session. If evaluation of the criteria isn't\n  implemented, the 'expire' strategy will be used as a fallback.\n\n  The expression evaluator currently doesn't account for differing string\n  collations between the database and Python.\n\nReturns the number of rows matched by the update.\n\nWarning - this currently doesn't account for any foreign key/relation cascades.\n\"\"\"\n\n#TODO: value keys need to be mapped to corresponding sql cols and instr.attr.s to string keys\n#TODO: updates of manytoone relations need to be converted to fk assignments\n#TODO: cascades need handling.\n\n", "func_signal": "def update(self, values, synchronize_session='expire'):\n", "code": "if synchronize_session not in [False, 'evaluate', 'expire']:\n    raise sa_exc.ArgumentError(\"Valid strategies for session synchronization are False, 'evaluate' and 'expire'\")\n\ncontext = self._compile_context()\nif len(context.statement.froms) != 1 or not isinstance(context.statement.froms[0], schema.Table):\n    raise sa_exc.ArgumentError(\"Only update via a single table query is currently supported\")\nprimary_table = context.statement.froms[0]\n\nsession = self.session\n\nif synchronize_session == 'evaluate':\n    try:\n        evaluator_compiler = evaluator.EvaluatorCompiler()\n        eval_condition = evaluator_compiler.process(self.whereclause)\n\n        value_evaluators = {}\n        for key,value in values.items():\n            key = expression._column_as_key(key)\n            value_evaluators[key] = evaluator_compiler.process(expression._literal_as_binds(value))\n    except evaluator.UnevaluatableError:\n        synchronize_session = 'expire'\n\nupdate_stmt = sql.update(primary_table, context.whereclause, values)\n\nif synchronize_session == 'expire':\n    select_stmt = context.statement.with_only_columns(primary_table.primary_key)\n    matched_rows = session.execute(select_stmt, params=self._params).fetchall()\n\nif self._autoflush:\n    session._autoflush()\nresult = session.execute(update_stmt, params=self._params)\n\nif synchronize_session == 'evaluate':\n    target_cls = self._mapper_zero().class_\n\n    for (cls, pk),obj in session.identity_map.iteritems():\n        evaluated_keys = value_evaluators.keys()\n\n        if issubclass(cls, target_cls) and eval_condition(obj):\n            state = attributes.instance_state(obj)\n\n            # only evaluate unmodified attributes\n            to_evaluate = state.unmodified.intersection(evaluated_keys)\n            for key in to_evaluate:\n                state.dict[key] = value_evaluators[key](obj)\n\n            state.commit(list(to_evaluate))\n\n            # expire attributes with pending changes (there was no autoflush, so they are overwritten)\n            state.expire_attributes(set(evaluated_keys).difference(to_evaluate))\n\nelif synchronize_session == 'expire':\n    target_mapper = self._mapper_zero()\n\n    for primary_key in matched_rows:\n        identity_key = target_mapper.identity_key_from_primary_key(list(primary_key))\n        if identity_key in session.identity_map:\n            session.expire(session.identity_map[identity_key], values.keys())\n\nfor ext in session.extensions:\n    ext.after_bulk_update(session, self, context, result)\n    \nreturn result.rowcount", "path": "lib\\sqlalchemy\\orm\\query.py", "repo_name": "tehasdf/sqlalchemy", "stars": 0, "license": "other", "language": "python", "size": 6836}
{"docstring": "\"\"\"Return an instance of the object based on the given identifier, or None if not found.\n\nThe `ident` argument is a scalar or tuple of primary key column values\nin the order of the table def's primary key columns.\n\n\"\"\"\n\n# convert composite types to individual args\n", "func_signal": "def get(self, ident):\n", "code": "if hasattr(ident, '__composite_values__'):\n    ident = ident.__composite_values__()\n\nkey = self._only_mapper_zero(\"get() can only be used against a single mapped class.\").identity_key_from_primary_key(ident)\nreturn self._get(key, ident)", "path": "lib\\sqlalchemy\\orm\\query.py", "repo_name": "tehasdf/sqlalchemy", "stars": 0, "license": "other", "language": "python", "size": 6836}
{"docstring": "# this is the most common use case, pickling the results of a\n# database reflection\n", "func_signal": "def test_pickle_via_reflect():\n", "code": "meta2 = MetaData(bind=testing.db)\nt1 = Table('mytable', meta2, autoload=True)\nt2 = Table('othertable', meta2, autoload=True)\nmeta3 = pickle.loads(pickle.dumps(meta2))\nassert meta3.bind is None\nassert meta3.tables['mytable'] is not t1\nreturn (meta3.tables['mytable'], meta3.tables['othertable'])", "path": "test\\engine\\metadata.py", "repo_name": "tehasdf/sqlalchemy", "stars": 0, "license": "other", "language": "python", "size": 6836}
{"docstring": "\"\"\"Return exactly one result or raise an exception.\n\nRaises ``sqlalchemy.orm.exc.NoResultFound`` if the query selects no rows.\nRaises ``sqlalchemy.orm.exc.MultipleResultsFound`` if multiple rows are\nselected.\n\nThis results in an execution of the underlying query.\n\n\"\"\"\n", "func_signal": "def one(self):\n", "code": "if self._statement:\n    raise sa_exc.InvalidRequestError(\n        \"one() not available when from_statement() is used; \"\n        \"use `first()` instead.\")\n\nret = list(self[0:2])\n\nif len(ret) == 1:\n    return ret[0]\nelif len(ret) == 0:\n    raise orm_exc.NoResultFound(\"No row was found for one()\")\nelse:\n    raise orm_exc.MultipleResultsFound(\n        \"Multiple rows were found for one()\")", "path": "lib\\sqlalchemy\\orm\\query.py", "repo_name": "tehasdf/sqlalchemy", "stars": 0, "license": "other", "language": "python", "size": 6836}
{"docstring": "\"\"\"Create a left outer join against this ``Query`` object's criterion\nand apply generatively, retunring the newly resulting ``Query``.\n\nUsage is the same as the ``join()`` method.\n\n\"\"\"\n", "func_signal": "def outerjoin(self, *props, **kwargs):\n", "code": "aliased, from_joinpoint = kwargs.pop('aliased', False), kwargs.pop('from_joinpoint', False)\nif kwargs:\n    raise TypeError(\"unknown arguments: %s\" % ','.join(kwargs.iterkeys()))\nreturn self.__join(props, outerjoin=True, create_aliases=aliased, from_joinpoint=from_joinpoint)", "path": "lib\\sqlalchemy\\orm\\query.py", "repo_name": "tehasdf/sqlalchemy", "stars": 0, "license": "other", "language": "python", "size": 6836}
{"docstring": "\"\"\"addressable 'interface'.\n\nif you really wanted to make a \"generic\" version of this function, it's straightforward.\n\"\"\"\n\n# create_address function, imitaes the rails example.\n# we could probably use property tricks as well to set\n# the Address object's \"addressabletype\" attribute.\n", "func_signal": "def addressable(cls, name, uselist=True):\n", "code": "def create_address(self):\n    a = Address(table.name)\n    if uselist:\n        getattr(self, name).append(a)\n    else:\n        setattr(self, name, a)\n    return a\n\nmapper = class_mapper(cls)\ntable = mapper.local_table\ncls.create_address = create_address\n# no constraints.  therefore define constraints in an ad-hoc fashion.\nprimaryjoin = and_(\n        list(table.primary_key)[0] == addresses.c.addressable_id,\n        addresses.c.addressable_type == table.name\n )\nforeign_keys = [addresses.c.addressable_id]\nmapper.add_property(name, relation(\n        Address,\n        primaryjoin=primaryjoin, uselist=uselist, foreign_keys=foreign_keys,\n        backref=backref('_backref_%s' % table.name, primaryjoin=list(table.primary_key)[0] == addresses.c.addressable_id, foreign_keys=foreign_keys)\n    )\n)", "path": "examples\\poly_assoc\\poly_assoc.py", "repo_name": "tehasdf/sqlalchemy", "stars": 0, "license": "other", "language": "python", "size": 6836}
{"docstring": "\"\"\"Add a join criterion corresponding to a relationship to the given\nparent instance.\n\ninstance\n  a persistent or detached instance which is related to class\n  represented by this query.\n\nproperty\n  string name of the property which relates this query's class to the\n  instance.  if None, the method will attempt to find a suitable\n  property.\n\nCurrently, this method only works with immediate parent relationships,\nbut in the future may be enhanced to work across a chain of parent\nmappers.\n\n\"\"\"\n", "func_signal": "def with_parent(self, instance, property=None):\n", "code": "from sqlalchemy.orm import properties\nmapper = object_mapper(instance)\nif property is None:\n    for prop in mapper.iterate_properties:\n        if isinstance(prop, properties.PropertyLoader) and prop.mapper is self._mapper_zero():\n            break\n    else:\n        raise sa_exc.InvalidRequestError(\"Could not locate a property which relates instances of class '%s' to instances of class '%s'\" % (self._mapper_zero().class_.__name__, instance.__class__.__name__))\nelse:\n    prop = mapper.get_property(property, resolve_synonyms=True)\nreturn self.filter(prop.compare(operators.eq, instance, value_is_parent=True))", "path": "lib\\sqlalchemy\\orm\\query.py", "repo_name": "tehasdf/sqlalchemy", "stars": 0, "license": "other", "language": "python", "size": 6836}
{"docstring": "\"\"\"Assert a target for a test case's function call count.\n\ncount\n  Optional, general target function call count.\n\nversions\n  Optional, a dictionary of Python version strings to counts,\n  for example::\n\n    { '2.5.1': 110,\n      '2.5': 100,\n      '2.4': 150 }\n\n  The best match for the current running python will be used.\n  If none match, 'count' will be used as the fallback.\n\nvariance\n  An +/- deviation percentage, defaults to 5%.\n\"\"\"\n\n# this could easily dump the profile report if --verbose is in effect\n\n", "func_signal": "def function_call_count(count=None, versions={}, variance=0.05):\n", "code": "version_info = list(sys.version_info)\npy_version = '.'.join([str(v) for v in sys.version_info])\n\nwhile version_info:\n    version = '.'.join([str(v) for v in version_info])\n    if version in versions:\n        count = versions[version]\n        break\n    version_info.pop()\n\nif count is None:\n    return lambda fn: fn\n\ndef decorator(fn):\n    def counted(*args, **kw):\n        try:\n            filename = \"%s.prof\" % fn.__name__\n\n            elapsed, stat_loader, result = _profile(\n                filename, fn, *args, **kw)\n\n            stats = stat_loader()\n            calls = stats.total_calls\n\n            if testlib.config.options.verbose:\n                stats.sort_stats('calls', 'cumulative')\n                stats.print_stats()\n                #stats.print_callers()\n            deviance = int(count * variance)\n            if (calls < (count - deviance) or\n                calls > (count + deviance)):\n                raise AssertionError(\n                    \"Function call count %s not within %s%% \"\n                    \"of expected %s. (Python version %s)\" % (\n                    calls, (variance * 100), count, py_version))\n\n            return result\n        finally:\n            if os.path.exists(filename):\n                os.unlink(filename)\n    return _function_named(counted, fn.__name__)\nreturn decorator", "path": "test\\testlib\\profiling.py", "repo_name": "tehasdf/sqlalchemy", "stars": 0, "license": "other", "language": "python", "size": 6836}
{"docstring": "\"\"\"tests reconnect operations at the pool level.  SA's engine/dialect includes another\nlayer of reconnect support for 'database was lost' errors.\"\"\"\n\n", "func_signal": "def test_reconnect(self):\n", "code": "dbapi = MockDBAPI()\np = pool.QueuePool(creator = lambda: dbapi.connect('foo.db'), pool_size = 1, max_overflow = 0, use_threadlocal = False)\nc1 = p.connect()\nc_id = c1.connection.id\nc1.close(); c1=None\n\nc1 = p.connect()\nassert c1.connection.id == c_id\ndbapi.raise_error = True\nc1.invalidate()\nc1 = None\n\nc1 = p.connect()\nassert c1.connection.id != c_id", "path": "test\\engine\\pool.py", "repo_name": "tehasdf/sqlalchemy", "stars": 0, "license": "other", "language": "python", "size": 6836}
{"docstring": "\"\"\"Mark a method as generative.\"\"\"\n\n", "func_signal": "def _generative(*assertions):\n", "code": "@util.decorator\ndef generate(fn, *args, **kw):\n    self = args[0]._clone()\n    for assertion in assertions:\n        assertion(self, fn.func_name)\n    fn(self, *args[1:], **kw)\n    return self\nreturn generate", "path": "lib\\sqlalchemy\\orm\\query.py", "repo_name": "tehasdf/sqlalchemy", "stars": 0, "license": "other", "language": "python", "size": 6836}
{"docstring": "\"\"\"add a mapped entity to the list of result columns to be returned.\"\"\"\n\n", "func_signal": "def add_entity(self, entity, alias=None):\n", "code": "if alias:\n    entity = aliased(entity, alias)\n\nself._entities = list(self._entities)\nm = _MapperEntity(self, entity)\nself.__setup_aliasizers([m])", "path": "lib\\sqlalchemy\\orm\\query.py", "repo_name": "tehasdf/sqlalchemy", "stars": 0, "license": "other", "language": "python", "size": 6836}
{"docstring": "\"\"\"this is a \"flaw\" in the connection pool; since threadlocal uses a single ConnectionFairy per thread\nwith an open/close counter, you can fool the counter into giving you a ConnectionFairy with an\nambiguous counter.  i.e. its not true reference counting.\"\"\"\n", "func_signal": "def test_trick_the_counter(self):\n", "code": "p = pool.QueuePool(creator = mock_dbapi.connect, pool_size = 3, max_overflow = -1, use_threadlocal = True)\nc1 = p.connect()\nc2 = p.connect()\nassert c1 is c2\nc1.close()\nc2 = p.connect()\nc2.close()\nself.assert_(p.checkedout() != 0)\n\nc2.close()\nself.assert_(p.checkedout() == 0)", "path": "test\\engine\\pool.py", "repo_name": "tehasdf/sqlalchemy", "stars": 0, "license": "other", "language": "python", "size": 6836}
{"docstring": "\"\"\"Return a ``MapperOption`` that will indicate to the query that\nthe given attribute will be eagerly loaded.\n\nUsed when feeding SQL result sets directly into ``query.instances()``.\nAlso bundles an ``EagerLazyOption`` to turn on eager loading in case it\nisn't already.\n\n`alias` is the string name of an alias, **or** an ``sql.Alias`` object,\nwhich represents the aliased columns in the query.  This argument is\noptional.\n\n\"\"\"\n", "func_signal": "def contains_eager(*keys, **kwargs):\n", "code": "alias = kwargs.pop('alias', None)\nif kwargs:\n    raise exceptions.ArgumentError(\"Invalid kwargs for contains_eager: %r\" % kwargs.keys())\n\nreturn (strategies.EagerLazyOption(keys, lazy=False), strategies.LoadEagerFromAliasOption(keys, alias=alias))", "path": "lib\\sqlalchemy\\orm\\__init__.py", "repo_name": "tehasdf/sqlalchemy", "stars": 0, "license": "other", "language": "python", "size": 6836}
{"docstring": "\"\"\"Perform a bulk delete query.\n\nDeletes rows matched by this query from the database. The synchronize_session\nparameter chooses the strategy for the removal of matched objects from the\nsession. Valid values are:\n\nFalse\n  don't synchronize the session. Use this when you don't need to use the\n  session after the delete or you can be sure that none of the matched objects\n  are in the session. The behavior of deleted objects still in the session is\n  undefined.\n\n'fetch'\n  performs a select query before the delete to find objects that are matched\n  by the delete query and need to be removed from the session. Matched objects\n  are removed from the session. 'fetch' is the default strategy.\n\n'evaluate'\n  experimental feature. Tries to evaluate the querys criteria in Python\n  straight on the objects in the session. If evaluation of the criteria isn't\n  implemented, the 'fetch' strategy will be used as a fallback.\n\n  The expression evaluator currently doesn't account for differing string\n  collations between the database and Python.\n\nReturns the number of rows deleted, excluding any cascades.\n\nWarning - this currently doesn't account for any foreign key/relation cascades.\n\"\"\"\n#TODO: lots of duplication and ifs - probably needs to be refactored to strategies\n#TODO: cascades need handling.\n\n", "func_signal": "def delete(self, synchronize_session='fetch'):\n", "code": "if synchronize_session not in [False, 'evaluate', 'fetch']:\n    raise sa_exc.ArgumentError(\"Valid strategies for session synchronization are False, 'evaluate' and 'fetch'\")\n\ncontext = self._compile_context()\nif len(context.statement.froms) != 1 or not isinstance(context.statement.froms[0], schema.Table):\n    raise sa_exc.ArgumentError(\"Only deletion via a single table query is currently supported\")\nprimary_table = context.statement.froms[0]\n\nsession = self.session\n\nif synchronize_session == 'evaluate':\n    try:\n        evaluator_compiler = evaluator.EvaluatorCompiler()\n        eval_condition = evaluator_compiler.process(self.whereclause)\n    except evaluator.UnevaluatableError:\n        synchronize_session = 'fetch'\n\ndelete_stmt = sql.delete(primary_table, context.whereclause)\n\nif synchronize_session == 'fetch':\n    #TODO: use RETURNING when available\n    select_stmt = context.statement.with_only_columns(primary_table.primary_key)\n    matched_rows = session.execute(select_stmt, params=self._params).fetchall()\n\nif self._autoflush:\n    session._autoflush()\nresult = session.execute(delete_stmt, params=self._params)\n\nif synchronize_session == 'evaluate':\n    target_cls = self._mapper_zero().class_\n\n    #TODO: detect when the where clause is a trivial primary key match\n    objs_to_expunge = [obj for (cls, pk),obj in session.identity_map.iteritems()\n        if issubclass(cls, target_cls) and eval_condition(obj)]\n    for obj in objs_to_expunge:\n        session._remove_newly_deleted(attributes.instance_state(obj))\nelif synchronize_session == 'fetch':\n    target_mapper = self._mapper_zero()\n    for primary_key in matched_rows:\n        identity_key = target_mapper.identity_key_from_primary_key(list(primary_key))\n        if identity_key in session.identity_map:\n            session._remove_newly_deleted(attributes.instance_state(session.identity_map[identity_key]))\n\nfor ext in session.extensions:\n    ext.after_bulk_delete(session, self, context, result)\n\nreturn result.rowcount", "path": "lib\\sqlalchemy\\orm\\query.py", "repo_name": "tehasdf/sqlalchemy", "stars": 0, "license": "other", "language": "python", "size": 6836}
{"docstring": "\"\"\"Return a new Query with criterion corresponding to a parent instance.\n\nReturn a newly constructed Query object, with criterion corresponding\nto a relationship to the given parent instance.\n\ninstance\n  a persistent or detached instance which is related to class\n  represented by this query.\n\n property\n   string name of the property which relates this query's class to the\n   instance.\n\n \\**kwargs\n   all extra keyword arguments are propagated to the constructor of\n   Query.\n\n\"\"\"\n", "func_signal": "def query_from_parent(cls, instance, property, **kwargs):\n", "code": "mapper = object_mapper(instance)\nprop = mapper.get_property(property, resolve_synonyms=True)\ntarget = prop.mapper\ncriterion = prop.compare(operators.eq, instance, value_is_parent=True)\nreturn Query(target, **kwargs).filter(criterion)", "path": "lib\\sqlalchemy\\orm\\query.py", "repo_name": "tehasdf/sqlalchemy", "stars": 0, "license": "other", "language": "python", "size": 6836}
{"docstring": "\"\"\"Add a SQL ColumnElement to the list of result columns to be returned.\"\"\"\n\n", "func_signal": "def add_column(self, column):\n", "code": "self._entities = list(self._entities)\nl = len(self._entities)\n_ColumnEntity(self, column)\n# _ColumnEntity may add many entities if the\n# given arg is a FROM clause\nself.__setup_aliasizers(self._entities[l:])", "path": "lib\\sqlalchemy\\orm\\query.py", "repo_name": "tehasdf/sqlalchemy", "stars": 0, "license": "other", "language": "python", "size": 6836}
{"docstring": "\"\"\"A many-to-many table gets cleared out with deletion from the backref side\"\"\"\n\n", "func_signal": "def test_delete(self):\n", "code": "mapper(Student, student)\nmapper(Course, course, properties = {\n    'students': relation(Student, enroll, lazy=True,\n                         backref='courses')})\n\nsess = create_session()\ns1 = Student('Student1')\nc1 = Course('Course1')\nc2 = Course('Course2')\nc3 = Course('Course3')\ns1.courses.append(c1)\ns1.courses.append(c2)\nc3.students.append(s1)\nsess.add(s1)\nsess.flush()\nsess.delete(s1)\nsess.flush()\nassert enroll.count().scalar() == 0", "path": "test\\orm\\manytomany.py", "repo_name": "tehasdf/sqlalchemy", "stars": 0, "license": "other", "language": "python", "size": 6836}
{"docstring": "\"\"\"Apply this query's criterion to a SELECT COUNT statement.\n\nIf column expressions or LIMIT/OFFSET/DISTINCT are present,\nthe query \"SELECT count(1) FROM (SELECT ...)\" is issued, \nso that the result matches the total number of rows\nthis query would return.  For mapped entities,\nthe primary key columns of each is written to the \ncolumns clause of the nested SELECT statement.\n\nFor a Query which is only against mapped entities,\na simpler \"SELECT count(1) FROM table1, table2, ... \nWHERE criterion\" is issued.  \n\n\"\"\"\n", "func_signal": "def count(self):\n", "code": "should_nest = [self._should_nest_selectable]\ndef ent_cols(ent):\n    if isinstance(ent, _MapperEntity):\n        return ent.mapper.primary_key\n    else:\n        should_nest[0] = True\n        return [ent.column]\n        \nreturn self._col_aggregate(sql.literal_column('1'), sql.func.count, \n    nested_cols=chain(*[ent_cols(ent) for ent in self._entities]),\n    should_nest = should_nest[0]\n)", "path": "lib\\sqlalchemy\\orm\\query.py", "repo_name": "tehasdf/sqlalchemy", "stars": 0, "license": "other", "language": "python", "size": 6836}
{"docstring": "\"\"\"Compile all mappers that have been defined.\n\nThis is equivalent to calling ``compile()`` on any individual mapper.\n\n\"\"\"\n", "func_signal": "def compile_mappers():\n", "code": "for m in list(_mapper_registry):\n    m.compile()", "path": "lib\\sqlalchemy\\orm\\__init__.py", "repo_name": "tehasdf/sqlalchemy", "stars": 0, "license": "other", "language": "python", "size": 6836}
{"docstring": "# FIXME: have constructor take self instead ?\n", "func_signal": "def setup(self):\n", "code": "pattern = PatternNode(self.state, self.admin, title=_(\"Pattern\"))\nself.nodes['Pattern'] = pattern\nreturn BaseAdminGtk.setup(self)", "path": "flumotion\\component\\producers\\videotest\\admin_gtk.py", "repo_name": "kristofkeppens/UgFlu", "stars": 1, "license": "other", "language": "python", "size": 3324}
{"docstring": "# FIXME: gladify\n", "func_signal": "def render(self):\n", "code": "self.widget = gtk.Table(1, 2)\nlabel = gtk.Label(_(\"Pattern:\"))\nself.widget.attach(label, 0, 1, 0, 1, 0, 0, 6, 6)\nlabel.show()\nself.combobox_pattern = fgtk.FProxyComboBox()\nself.combobox_pattern.set_enum(VideoTestPattern)\nself.pattern_changed_id = self.combobox_pattern.connect('changed',\n    self.cb_pattern_changed)\nself.widget.attach(self.combobox_pattern, 1, 2, 0, 1, 0, 0, 6, 6)\nself.combobox_pattern.show()\nreturn BaseAdminGtkNode.render(self)", "path": "flumotion\\component\\producers\\videotest\\admin_gtk.py", "repo_name": "kristofkeppens/UgFlu", "stars": 1, "license": "other", "language": "python", "size": 3324}
{"docstring": "\"\"\"\nCalled when the PB client wants to send the given feedId to the\ngiven component\n\"\"\"\n# we need to make sure our result goes back, so only stop reading\n", "func_signal": "def perspective_receiveFeed(self, fullFeedId):\n", "code": "t = self.mind.broker.transport\nt.stopReading()\nreactor.callLater(0, self._doReceiveFeed, fullFeedId)", "path": "flumotion\\worker\\feedserver.py", "repo_name": "kristofkeppens/UgFlu", "stars": 1, "license": "other", "language": "python", "size": 3324}
{"docstring": "\"\"\"\nAttempt to deregister a default destination for all requests not\ndirected to a specifically-mapped path. This will only succeed if the\ndefault is currently equal to this avatar.\n\n@param avatar: The avatar being deregistered\n@type  avatar: L{PorterAvatar}\n\"\"\"\n", "func_signal": "def deregisterPrefix(self, prefix, avatar):\n", "code": "if prefix not in self._prefixes:\n    self.warning(\"Mapping not removed: no mapping found\")\n    return\n\nif self._prefixes[prefix] == avatar:\n    self.debug(\"Removing prefix destination from porter\")\n    del self._prefixes[prefix]\nelse:\n    self.warning(\n        \"Not removing prefix destination: expected avatar not found\")", "path": "flumotion\\component\\misc\\porter\\porter.py", "repo_name": "kristofkeppens/UgFlu", "stars": 1, "license": "other", "language": "python", "size": 3324}
{"docstring": "\"\"\"\nRegister a path as being served by a streamer represented by this\navatar. Will remove any previous registration at this path.\n\n@param path:   The path to register\n@type  path:   str\n@param avatar: The avatar representing the streamer to direct this path\n               to\n@type  avatar: L{PorterAvatar}\n\"\"\"\n", "func_signal": "def registerPath(self, path, avatar):\n", "code": "self.debug(\"Registering porter path \\\"%s\\\" to %r\" % (path, avatar))\nif path in self._mappings:\n    self.warning(\"Replacing existing mapping for path \\\"%s\\\"\" % path)\n\nself._mappings[path] = avatar", "path": "flumotion\\component\\misc\\porter\\porter.py", "repo_name": "kristofkeppens/UgFlu", "stars": 1, "license": "other", "language": "python", "size": 3324}
{"docstring": "\"\"\" Calculate the new values to set on capsfilter and videobox. \"\"\"\n", "func_signal": "def _computeAndSetValues(self):\n", "code": "if (self.widthin == -1 or self.heightin == -1 or\n    self.widthout == -1 or self.heightout == -1):\n    # FIXME : should we reset videobox/capsfilter properties here ?\n    self.error(\n        \"We don't have input and output caps, \"\n        \"we can't calculate videobox values\")\n    return\n\nself.log(\"incoming width/height/PAR/DAR : %d/%d/%r/%r\" % (\n    self.widthin, self.heightin,\n    self.parin, self.darin))\nself.log(\"outgoing width/height/PAR/DAR : %d/%d/%r/%r\" % (\n    self.widthout, self.heightout,\n    self.parout, self.darout))\n\nif self.darin == self.darout:\n    self.log(\n        \"We have same input and output caps, \"\n        \"resetting capsfilter and videobox settings\")\n    # same DAR, set inputcaps on capsfilter, reset videobox values\n    caps = gst.caps_new_any()\n    left = 0\n    right = 0\n    top = 0\n    bottom = 0\nelse:\n    par = self.parout\n    dar = self.darin\n    fdarin = float(self.darin.num) / float(self.darin.denom)\n    fdarout = float(self.darout.num) / float(self.darout.denom)\n    if fdarin > fdarout:\n        self.log(\"incoming DAR is greater that ougoing DAR. \"\n                 \"Adding top/bottom borders\")\n        # width, PAR stays the same as output\n        # calculate newheight = (PAR * widthout) / DAR\n        newheight = ((par.num * self.widthout * dar.denom) /\n                     (par.denom * dar.num))\n        self.log(\"newheight should be %d\" % newheight)\n        extra = self.heightout - newheight\n        top = extra / 2\n        bottom = extra - top # compensate for odd extra\n        left = right = 0\n        # calculate filter caps\n        astr = \"width=%d,height=%d\" % (self.widthout, newheight)\n    else:\n        self.log(\"incoming DAR is smaller than outgoing DAR. \"\n                 \"Adding left/right borders\")\n        # height, PAR stays the same as output\n        # calculate newwidth = (DAR * heightout) / PAR\n        newwidth = ((dar.num * self.heightout * par.denom) /\n                    (dar.denom * par.num))\n        self.log(\"newwidth should be %d\" % newwidth)\n        extra = self.widthout - newwidth\n        left = extra / 2\n        right = extra - left # compensate for odd extra\n        top = bottom = 0\n        # calculate filter caps\n        astr = \"width=%d,height=%d\" % (newwidth, self.heightout)\n    caps = gst.caps_from_string(\n        \"video/x-raw-yuv,%s;video/x-raw-rgb,%s\" % (astr, astr))\n\n# set properties on elements\nself.debug(\n    \"About to set left/right/top/bottom : %d/%d/%d/%d\" % (\n    -left, -right, -top, -bottom))\nself.videobox.props.left = -left\nself.videobox.props.right = -right\nself.videobox.props.top = -top\nself.videobox.props.bottom = -bottom\nself.debug(\"Settings filter caps %s\" % caps.to_string())\nself.capsfilter.props.caps = caps\nself.debug(\"done\")", "path": "flumotion\\component\\producers\\playlist\\smartscale.py", "repo_name": "kristofkeppens/UgFlu", "stars": 1, "license": "other", "language": "python", "size": 3324}
{"docstring": "# We maintain a map of path -> avatar (the underlying transport is\n# accessible from the avatar, we need this for FD-passing)\n", "func_signal": "def init(self):\n", "code": "self._mappings = {}\nself._prefixes = {}\n\nself._socketlistener = None\n\nself._socketPath = None\nself._username = None\nself._password = None\nself._port = None\nself._iptablesPort = None\nself._porterProtocol = None\n\nself._interface = ''", "path": "flumotion\\component\\misc\\porter\\porter.py", "repo_name": "kristofkeppens/UgFlu", "stars": 1, "license": "other", "language": "python", "size": 3324}
{"docstring": "\"\"\"\nGenerate a random US-ASCII string of length numchars\n\"\"\"\n", "func_signal": "def generateRandomString(self, numchars):\n", "code": "string = \"\"\nchars = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\nfor _ in range(numchars):\n    string += chars[random.randint(0, len(chars) - 1)]\n\nreturn string", "path": "flumotion\\component\\misc\\porter\\porter.py", "repo_name": "kristofkeppens/UgFlu", "stars": 1, "license": "other", "language": "python", "size": 3324}
{"docstring": "# add volume effect\n", "func_signal": "def configure_pipeline(self, pipeline, properties):\n", "code": "comp_level = pipeline.get_by_name('volumelevel')\nallowVolumeSet = True\nif gst.pygst_version < (0, 10, 7):\n    allowVolumeSet = False\n    m = messages.Info(T_(\n        N_(\"The soundcard volume cannot be changed with this version \"\n            \"of the 'gst-python' library.\\n\")),\n        mid='mixer-track-setting')\n    m.add(T_(N_(\"Please upgrade '%s' to version %s or later \"\n        \"if you require this functionality.\"),\n        'gst-python', '0.10.7'))\n    self.addMessage(m)\n\nvol = volume.Volume('inputVolume', comp_level, pipeline,\n    allowIncrease=False, allowVolumeSet=allowVolumeSet)\nself.addEffect(vol)\nself._srcelement = pipeline.get_by_name(\"src\")", "path": "flumotion\\component\\producers\\soundcard\\soundcard.py", "repo_name": "kristofkeppens/UgFlu", "stars": 1, "license": "other", "language": "python", "size": 3324}
{"docstring": "\"\"\"\nreturns (width, height, par, dar) from given caps.\nIf caps are None, or not negotiated, it will return\n(-1, -1, gst.Fraction(1,1), gst.Fraction(1,1))\n\"\"\"\n", "func_signal": "def _getValuesFromCaps(self, caps, force=False):\n", "code": "width = -1\nheight = -1\npar = gst.Fraction(1, 1)\ndar = gst.Fraction(1, 1)\nif force or (caps and caps.is_fixed()):\n    struc = caps[0]\n    width = struc[\"width\"]\n    height = struc[\"height\"]\n    if struc.has_field('pixel-aspect-ratio'):\n        par = struc['pixel-aspect-ratio']\n    dar = gst.Fraction(width * par.num, height * par.denom)\nreturn (width, height, par, dar)", "path": "flumotion\\component\\producers\\playlist\\smartscale.py", "repo_name": "kristofkeppens/UgFlu", "stars": 1, "license": "other", "language": "python", "size": 3324}
{"docstring": "\"\"\"\nGenerate a socket pathname in an appropriate location\n\"\"\"\n# Also see worker/worker.py:_getSocketPath(), and note that\n# this suffers from the same potential race.\n", "func_signal": "def generateSocketPath(self):\n", "code": "import tempfile\nfd, name = tempfile.mkstemp('.%d' % os.getpid(), 'flumotion.porter.')\nos.close(fd)\n\nreturn name", "path": "flumotion\\component\\misc\\porter\\porter.py", "repo_name": "kristofkeppens/UgFlu", "stars": 1, "license": "other", "language": "python", "size": 3324}
{"docstring": "\"\"\"\nCalled when the PB client wants us to send them the given feed.\n\"\"\"\n# the PB message needs to be sent from the side that has the feeder\n# for proper switching, so we call back as a reply\n", "func_signal": "def perspective_sendFeed(self, fullFeedId):\n", "code": "d = self.mindCallRemote('sendFeedReply', fullFeedId)\nd.addCallback(self._sendFeedReplyCb, fullFeedId)", "path": "flumotion\\worker\\feedserver.py", "repo_name": "kristofkeppens/UgFlu", "stars": 1, "license": "other", "language": "python", "size": 3324}
{"docstring": "\"\"\"\n@param brain: L{flumotion.worker.worker.WorkerBrain}\n\"\"\"\n", "func_signal": "def __init__(self, brain, bouncer, portNum):\n", "code": "self._brain = brain\nself._tport = None\nself.listen(bouncer, portNum)", "path": "flumotion\\worker\\feedserver.py", "repo_name": "kristofkeppens/UgFlu", "stars": 1, "license": "other", "language": "python", "size": 3324}
{"docstring": "\"\"\"\nRegister a destination for all requests directed to anything beginning\nwith a specified prefix. Where there are multiple matching prefixes,\nthe longest is selected.\n\n@param avatar: The avatar being registered\n@type  avatar: L{PorterAvatar}\n\"\"\"\n\n", "func_signal": "def registerPrefix(self, prefix, avatar):\n", "code": "self.debug(\"Setting prefix \\\"%s\\\" for porter\", prefix)\nif prefix in self._prefixes:\n    self.warning(\"Overwriting prefix\")\n\nself._prefixes[prefix] = avatar", "path": "flumotion\\component\\misc\\porter\\porter.py", "repo_name": "kristofkeppens/UgFlu", "stars": 1, "license": "other", "language": "python", "size": 3324}
{"docstring": "\"\"\"\nFind a destination Avatar for this path.\n@returns: The Avatar for this mapping, or None.\n\"\"\"\n\n", "func_signal": "def findDestination(self, path):\n", "code": "if path in self._mappings:\n    return self._mappings[path]\nelse:\n    return self.findPrefixMatch(path)", "path": "flumotion\\component\\misc\\porter\\porter.py", "repo_name": "kristofkeppens/UgFlu", "stars": 1, "license": "other", "language": "python", "size": 3324}
{"docstring": "\"\"\"\nAttempt to deregister the given path. A deregistration will only be\naccepted if the mapping is to the avatar passed.\n\n@param path:   The path to deregister\n@type  path:   str\n@param avatar: The avatar representing the streamer being deregistered\n@type  avatar: L{PorterAvatar}\n\"\"\"\n", "func_signal": "def deregisterPath(self, path, avatar):\n", "code": "if path in self._mappings:\n    if self._mappings[path] == avatar:\n        self.debug(\"Removing porter mapping for \\\"%s\\\"\" % path)\n        del self._mappings[path]\n    else:\n        self.warning(\n            \"Mapping not removed: refers to a different avatar\")\nelse:\n    self.warning(\"Mapping not removed: no mapping found\")", "path": "flumotion\\component\\misc\\porter\\porter.py", "repo_name": "kristofkeppens/UgFlu", "stars": 1, "license": "other", "language": "python", "size": 3324}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def __init__(self, feedServer, avatarId, mind):\n", "code": "fpb.Avatar.__init__(self, avatarId)\nself._transport = None\nself.feedServer = feedServer\nself.avatarId = avatarId\nself.setMind(mind)", "path": "flumotion\\worker\\feedserver.py", "repo_name": "kristofkeppens/UgFlu", "stars": 1, "license": "other", "language": "python", "size": 3324}
{"docstring": "# compare with startStreaming in prototype\n# Remove this from the reactor; we mustn't read or write from it from\n# here on\n", "func_signal": "def _sendFeedReplyCb(self, result, fullFeedId):\n", "code": "t = self.mind.broker.transport\nt.stopReading()\nt.stopWriting()\n\n# hand off the fd to the component\nself.debug(\"Attempting to send FD: %d\", t.fileno())\n\n(flowName, componentName, feedName) = common.parseFullFeedId(\n    fullFeedId)\ncomponentId = common.componentId(flowName, componentName)\n\nif self.feedServer.feedToFD(componentId, feedName, t.fileno(),\n                            self.avatarId):\n    t.keepSocketAlive = True\n\n# We removed the transport from the reactor before sending the\n# FD; now we want the socket cleaned up.\nt.loseConnection()", "path": "flumotion\\worker\\feedserver.py", "repo_name": "kristofkeppens/UgFlu", "stars": 1, "license": "other", "language": "python", "size": 3324}
{"docstring": "\"\"\"\nReturn the location, login username/password, and listening port\nand interface for the porter as a tuple (path, username,\npassword, port, interface).\n\"\"\"\n", "func_signal": "def remote_getPorterDetails(self):\n", "code": "return (self.comp._socketPath, self.comp._username,\n        self.comp._password, self.comp._iptablesPort,\n        self.comp._interface)", "path": "flumotion\\component\\misc\\porter\\porter.py", "repo_name": "kristofkeppens/UgFlu", "stars": 1, "license": "other", "language": "python", "size": 3324}
{"docstring": "\"\"\" set the outgoing caps, because gst.BaseTransform\nis full of CRACK ! \"\"\"\n", "func_signal": "def set_caps(self, caps):\n", "code": "(self.widthout,\n self.heightout,\n self.parout,\n self.darout) = self._getValuesFromCaps(caps, True)", "path": "flumotion\\component\\producers\\playlist\\smartscale.py", "repo_name": "kristofkeppens/UgFlu", "stars": 1, "license": "other", "language": "python", "size": 3324}
{"docstring": "\"\"\"Make a copy of this ParserElement.  Useful for defining different parse actions\n   for the same parsing pattern, using copies of the original parse element.\"\"\"\n", "func_signal": "def copy( self ):\n", "code": "cpy = copy.copy( self )\ncpy.parseAction = self.parseAction[:]\ncpy.ignoreExprs = self.ignoreExprs[:]\nif self.copyDefaultWhiteChars:\n    cpy.whiteChars = ParserElement.DEFAULT_WHITE_CHARS\nreturn cpy", "path": "code\\pybayes\\IO\\pyparsing.py", "repo_name": "izquierdo/kr", "stars": 1, "license": "None", "language": "python", "size": 1136}
{"docstring": "\"\"\"Returns a new copy of a ParseResults object.\"\"\"\n", "func_signal": "def copy( self ):\n", "code": "ret = ParseResults( self.__toklist )\nret.__tokdict = self.__tokdict.copy()\nret.__parent = self.__parent\nret.__accumNames.update( self.__accumNames )\nret.__name = self.__name\nreturn ret", "path": "code\\pybayes\\IO\\pyparsing.py", "repo_name": "izquierdo/kr", "stars": 1, "license": "None", "language": "python", "size": 1136}
{"docstring": "\"\"\"Disables the skipping of whitespace before matching the characters in the \n   ParserElement's defined pattern.  This is normally only used internally by\n   the pyparsing module, but may be needed in some whitespace-sensitive grammars.\n\"\"\"\n", "func_signal": "def leaveWhitespace( self ):\n", "code": "self.skipWhitespace = False\nreturn self", "path": "code\\pybayes\\IO\\pyparsing.py", "repo_name": "izquierdo/kr", "stars": 1, "license": "None", "language": "python", "size": 1136}
{"docstring": "\"\"\"Implementation of + operator - returns And\"\"\"\n", "func_signal": "def __add__(self, other ):\n", "code": "if isinstance( other, basestring ):\n    other = Literal( other )\nif not isinstance( other, ParserElement ):\n    warnings.warn(\"Cannot add element of type %s to ParserElement\" % type(other),\n            SyntaxWarning, stacklevel=2)\nreturn And( [ self, other ] )", "path": "code\\pybayes\\IO\\pyparsing.py", "repo_name": "izquierdo/kr", "stars": 1, "license": "None", "language": "python", "size": 1136}
{"docstring": "\"\"\"Implementation of & operator - returns Each\"\"\"\n", "func_signal": "def __and__(self, other ):\n", "code": "if isinstance( other, basestring ):\n    other = Literal( other )\nif not isinstance( other, ParserElement ):\n    warnings.warn(\"Cannot add element of type %s to ParserElement\" % type(other),\n            SyntaxWarning, stacklevel=2)\nreturn Each( [ self, other ] )", "path": "code\\pybayes\\IO\\pyparsing.py", "repo_name": "izquierdo/kr", "stars": 1, "license": "None", "language": "python", "size": 1136}
{"docstring": "\"\"\"Internal helper to construct opening and closing tag expressions, given a tag name\"\"\"\n", "func_signal": "def _makeTags(tagStr, xml):\n", "code": "if isinstance(tagStr,basestring):\n    resname = tagStr\n    tagStr = Keyword(tagStr, caseless=not xml)\nelse:\n    resname = tagStr.name\n    \ntagAttrName = Word(alphas,alphanums+\"_-:\")\nif (xml):\n    tagAttrValue = dblQuotedString.copy().setParseAction( removeQuotes )\n    openTag = Suppress(\"<\") + tagStr + \\\n            Dict(ZeroOrMore(Group( tagAttrName + Suppress(\"=\") + tagAttrValue ))) + \\\n            Optional(\"/\",default=[False]).setResultsName(\"empty\").setParseAction(lambda s,l,t:t[0]=='/') + Suppress(\">\")\nelse:\n    printablesLessRAbrack = \"\".join( [ c for c in printables if c not in \">\" ] )\n    tagAttrValue = quotedString.copy().setParseAction( removeQuotes ) | Word(printablesLessRAbrack)\n    openTag = Suppress(\"<\") + tagStr + \\\n            Dict(ZeroOrMore(Group( tagAttrName.setParseAction(downcaseTokens) + \\\n            Suppress(\"=\") + tagAttrValue ))) + \\\n            Optional(\"/\",default=[False]).setResultsName(\"empty\").setParseAction(lambda s,l,t:t[0]=='/') + Suppress(\">\")\ncloseTag = Combine(\"</\" + tagStr + \">\")\n\nopenTag = openTag.setResultsName(\"start\"+\"\".join(resname.replace(\":\",\" \").title().split())).setName(\"<%s>\" % tagStr)\ncloseTag = closeTag.setResultsName(\"end\"+\"\".join(resname.replace(\":\",\" \").title().split())).setName(\"</%s>\" % tagStr)\n\nreturn openTag, closeTag", "path": "code\\pybayes\\IO\\pyparsing.py", "repo_name": "izquierdo/kr", "stars": 1, "license": "None", "language": "python", "size": 1136}
{"docstring": "\"\"\"Returns the line of text containing loc within a string, counting newlines as line separators.\n   \"\"\"\n", "func_signal": "def line( loc, strg ):\n", "code": "lastCR = strg.rfind(\"\\n\", 0, loc)\nnextCR = strg.find(\"\\n\", loc)\nif nextCR > 0:\n    return strg[lastCR+1:nextCR]\nelse:\n    return strg[lastCR+1:]", "path": "code\\pybayes\\IO\\pyparsing.py", "repo_name": "izquierdo/kr", "stars": 1, "license": "None", "language": "python", "size": 1136}
{"docstring": "\"\"\"Helper to define a delimited list of expressions - the delimiter defaults to ','.\n   By default, the list elements and delimiters can have intervening whitespace, and \n   comments, but this can be overridden by passing 'combine=True' in the constructor.\n   If combine is set to True, the matching tokens are returned as a single token\n   string, with the delimiters included; otherwise, the matching tokens are returned\n   as a list of tokens, with the delimiters suppressed.\n\"\"\"\n", "func_signal": "def delimitedList( expr, delim=\",\", combine=False ):\n", "code": "dlName = _ustr(expr)+\" [\"+_ustr(delim)+\" \"+_ustr(expr)+\"]...\"\nif combine:\n    return Combine( expr + ZeroOrMore( delim + expr ) ).setName(dlName)\nelse:\n    return ( expr + ZeroOrMore( Suppress( delim ) + expr ) ).setName(dlName)", "path": "code\\pybayes\\IO\\pyparsing.py", "repo_name": "izquierdo/kr", "stars": 1, "license": "None", "language": "python", "size": 1136}
{"docstring": "\"\"\"Extends leaveWhitespace defined in base class, and also invokes leaveWhitespace on\n   all contained expressions.\"\"\"\n", "func_signal": "def leaveWhitespace( self ):\n", "code": "self.skipWhitespace = False\nself.exprs = [ e.copy() for e in self.exprs ]\nfor e in self.exprs:\n    e.leaveWhitespace()\nreturn self", "path": "code\\pybayes\\IO\\pyparsing.py", "repo_name": "izquierdo/kr", "stars": 1, "license": "None", "language": "python", "size": 1136}
{"docstring": "#~  escape these chars: ^-]\n", "func_signal": "def _escapeRegexRangeChars(s):\n", "code": "for c in r\"\\^-]\":\n    s = s.replace(c,\"\\\\\"+c)\ns = s.replace(\"\\n\",r\"\\n\")\ns = s.replace(\"\\t\",r\"\\t\")\nreturn _ustr(s)", "path": "code\\pybayes\\IO\\pyparsing.py", "repo_name": "izquierdo/kr", "stars": 1, "license": "None", "language": "python", "size": 1136}
{"docstring": "\"\"\"Implementation of ^ operator - returns Or\"\"\"\n", "func_signal": "def __xor__(self, other ):\n", "code": "if isinstance( other, basestring ):\n    other = Literal( other )\nif not isinstance( other, ParserElement ):\n    warnings.warn(\"Cannot add element of type %s to ParserElement\" % type(other),\n            SyntaxWarning, stacklevel=2)\nreturn Or( [ self, other ] )", "path": "code\\pybayes\\IO\\pyparsing.py", "repo_name": "izquierdo/kr", "stars": 1, "license": "None", "language": "python", "size": 1136}
{"docstring": "\"\"\"Add parse action to expression's list of parse actions. See L{I{setParseAction}<setParseAction>}.\"\"\"\n", "func_signal": "def addParseAction( self, *fns, **kwargs ):\n", "code": "self.parseAction += map(self.normalizeParseActionArgs, list(fns))\nself.callDuringTry = self.callDuringTry or (\"callDuringTry\" in kwargs and kwargs[\"callDuringTry\"])\nreturn self", "path": "code\\pybayes\\IO\\pyparsing.py", "repo_name": "izquierdo/kr", "stars": 1, "license": "None", "language": "python", "size": 1136}
{"docstring": "\"\"\"Returns the results name for this token expression.\"\"\"\n", "func_signal": "def getName(self):\n", "code": "if self.__name:\n    return self.__name\nelif self.__parent:\n    par = self.__parent()\n    if par:\n        return par.__lookup(self)\n    else:\n        return None\nelif (len(self) == 1 and \n       len(self.__tokdict) == 1 and\n       self.__tokdict.values()[0][0][1] in (0,-1)):\n    return self.__tokdict.keys()[0]\nelse:\n    return None", "path": "code\\pybayes\\IO\\pyparsing.py", "repo_name": "izquierdo/kr", "stars": 1, "license": "None", "language": "python", "size": 1136}
{"docstring": "\"\"\"Implementation of | operator - returns MatchFirst\"\"\"\n", "func_signal": "def __or__(self, other ):\n", "code": "if isinstance( other, basestring ):\n    other = Literal( other )\nif not isinstance( other, ParserElement ):\n    warnings.warn(\"Cannot add element of type %s to ParserElement\" % type(other),\n            SyntaxWarning, stacklevel=2)\nreturn MatchFirst( [ self, other ] )", "path": "code\\pybayes\\IO\\pyparsing.py", "repo_name": "izquierdo/kr", "stars": 1, "license": "None", "language": "python", "size": 1136}
{"docstring": "\"\"\"Implementation of ^= operator\"\"\"\n", "func_signal": "def __rxor__(self, other ):\n", "code": "if isinstance( other, basestring ):\n    other = Literal( other )\nif not isinstance( other, ParserElement ):\n    warnings.warn(\"Cannot add element of type %s to ParserElement\" % type(other),\n            SyntaxWarning, stacklevel=2)\nreturn other ^ self", "path": "code\\pybayes\\IO\\pyparsing.py", "repo_name": "izquierdo/kr", "stars": 1, "license": "None", "language": "python", "size": 1136}
{"docstring": "\"\"\"Define expression to be ignored (e.g., comments) while doing pattern \n   matching; may be called repeatedly, to define multiple comment or other\n   ignorable patterns.\n\"\"\"\n", "func_signal": "def ignore( self, other ):\n", "code": "if isinstance( other, Suppress ):\n    if other not in self.ignoreExprs:\n        self.ignoreExprs.append( other )\nelse:\n    self.ignoreExprs.append( Suppress( other ) )\nreturn self", "path": "code\\pybayes\\IO\\pyparsing.py", "repo_name": "izquierdo/kr", "stars": 1, "license": "None", "language": "python", "size": 1136}
{"docstring": "\"\"\"Extracts the exception line from the input string, and marks \n   the location of the exception with a special symbol.\n\"\"\"\n", "func_signal": "def markInputline( self, markerString = \">!<\" ):\n", "code": "line_str = self.line\nline_column = self.column - 1\nif markerString:\n    line_str = \"\".join( [line_str[:line_column],\n                        markerString, line_str[line_column:]])\nreturn line_str.strip()", "path": "code\\pybayes\\IO\\pyparsing.py", "repo_name": "izquierdo/kr", "stars": 1, "license": "None", "language": "python", "size": 1136}
{"docstring": "\"\"\"Overrides default behavior to expand <TAB>s to spaces before parsing the input string.\n   Must be called before parseString when the input grammar contains elements that \n   match <TAB> characters.\"\"\"\n", "func_signal": "def parseWithTabs( self ):\n", "code": "self.keepTabs = True\nreturn self", "path": "code\\pybayes\\IO\\pyparsing.py", "repo_name": "izquierdo/kr", "stars": 1, "license": "None", "language": "python", "size": 1136}
{"docstring": "\"\"\"Implementation of += operator\"\"\"\n", "func_signal": "def __radd__(self, other ):\n", "code": "if isinstance( other, basestring ):\n    other = Literal( other )\nif not isinstance( other, ParserElement ):\n    warnings.warn(\"Cannot add element of type %s to ParserElement\" % type(other),\n            SyntaxWarning, stacklevel=2)\nreturn other + self", "path": "code\\pybayes\\IO\\pyparsing.py", "repo_name": "izquierdo/kr", "stars": 1, "license": "None", "language": "python", "size": 1136}
{"docstring": "\"\"\"Helper to quickly define a set of alternative Literals, and makes sure to do \n   longest-first testing when there is a conflict, regardless of the input order, \n   but returns a MatchFirst for best performance.  \n   \n   Parameters:\n    - strs - a string of space-delimited literals, or a list of string literals\n    - caseless - (default=False) - treat all literals as caseless\n    - useRegex - (default=True) - as an optimization, will generate a Regex\n      object; otherwise, will generate a MatchFirst object (if caseless=True, or\n      if creating a Regex raises an exception)\n\"\"\"\n", "func_signal": "def oneOf( strs, caseless=False, useRegex=True ):\n", "code": "if caseless:\n    isequal = ( lambda a,b: a.upper() == b.upper() )\n    masks = ( lambda a,b: b.upper().startswith(a.upper()) )\n    parseElementClass = CaselessLiteral\nelse:\n    isequal = ( lambda a,b: a == b )\n    masks = ( lambda a,b: b.startswith(a) )\n    parseElementClass = Literal\n\nif isinstance(strs,(list,tuple)):\n    symbols = strs[:]\nelif isinstance(strs,basestring):\n    symbols = strs.split()\nelse:\n    warnings.warn(\"Invalid argument to oneOf, expected string or list\",\n            SyntaxWarning, stacklevel=2)\n    \ni = 0\nwhile i < len(symbols)-1:\n    cur = symbols[i]\n    for j,other in enumerate(symbols[i+1:]):\n        if ( isequal(other, cur) ):\n            del symbols[i+j+1]\n            break\n        elif ( masks(cur, other) ):\n            del symbols[i+j+1]\n            symbols.insert(i,other)\n            cur = other\n            break\n    else:\n        i += 1\n\nif not caseless and useRegex:\n    #~ print strs,\"->\", \"|\".join( [ _escapeRegexChars(sym) for sym in symbols] )\n    try:\n        if len(symbols)==len(\"\".join(symbols)):\n            return Regex( \"[%s]\" % \"\".join( [ _escapeRegexRangeChars(sym) for sym in symbols] ) )\n        else:\n            return Regex( \"|\".join( [ re.escape(sym) for sym in symbols] ) )\n    except:\n        warnings.warn(\"Exception creating Regex for oneOf, building MatchFirst\",\n                SyntaxWarning, stacklevel=2)\n\n\n# last resort, just use MatchFirst\nreturn MatchFirst( [ parseElementClass(sym) for sym in symbols ] )", "path": "code\\pybayes\\IO\\pyparsing.py", "repo_name": "izquierdo/kr", "stars": 1, "license": "None", "language": "python", "size": 1136}
{"docstring": "\"\"\"\n    Return a number of bytes relative to the start of the current\n    protocol.\n\"\"\"\n", "func_signal": "def _getByteField(self, frm, tlen):\n", "code": "if (tlen == 1):\n    try:\n        return self.packet._data[self.offset+frm]\n    except IndexError:\n        return ''\nelse:\n    return self.packet._data[self.offset+frm:self.offset+frm+tlen].tostring()", "path": "packet\\packet.py", "repo_name": "bytearchive/packetpy", "stars": 0, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"\n    Is this protocol of type \"name\"? Used for dict-like acces from\n    Packet.\n\n    We inspect the inheritance tree of the class, and check the TYPE\n    attribute of each one. This allows us to, for instance, refer to\n    either \"icmp\" and \"icmpechorequest\".\n\"\"\"\n", "func_signal": "def isType(self, name):\n", "code": "for i in self.__class__.__mro__:\n    if hasattr(i, \"TYPE\"):\n        if i.TYPE.lower() == name.lower():\n            return 1\nreturn 0", "path": "packet\\packet.py", "repo_name": "bytearchive/packetpy", "stars": 0, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"\n    Fix packet dimension fields. \n\"\"\"\n", "func_signal": "def _fixDimensions(self):\n", "code": "lst = self.getProtoList()\nfor i in lst:\n    i._fixDimensions()", "path": "packet\\packet.py", "repo_name": "bytearchive/packetpy", "stars": 0, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"\n    The default implementation for ICMPv6 gives the entire message body\n    as payload.\n\"\"\"\n", "func_signal": "def _getPayloadOffsets(self):\n", "code": "offset = 4\ndataLength = len(self.packet) - self.offset - 4\nreturn offset, dataLength", "path": "packet\\packet.py", "repo_name": "bytearchive/packetpy", "stars": 0, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"\n    Packet initialisation. \n        packet      - An instance of the Packet class\n        offset      - The byte offset of the start of this protocol\n        construct  - Create the next protocol in the chain?\n\"\"\"\n", "func_signal": "def __init__(self, packet, offset=0, constructNext=1):\n", "code": "self.packet = packet\nself.offset = offset\nself._next = None\nself._prev = None\nself._printList = []\nif constructNext:\n    self._constructNext()", "path": "packet\\packet.py", "repo_name": "bytearchive/packetpy", "stars": 0, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"\n    Return an integer corresponding to a whole number of bytes,\n    relative to the start of the current protocol header.\n\"\"\"\n", "func_signal": "def _getIntField(self, frm, tlen):\n", "code": "b = self._getByteField(frm, tlen)\nif b and tlen == 1:\n    return ord(b)\nelse:\n    return utils.multiord(b)", "path": "packet\\packet.py", "repo_name": "bytearchive/packetpy", "stars": 0, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"\n    frm  : The offset in bytes from which the field begins. \n    tlen : The length of the field in bytes. \n\"\"\"\n", "func_signal": "def __init__(self, frm, tlen, doc = \"\", options = None):\n", "code": "self.frm = frm\nself.tlen = tlen\nself.options = options\nif self.options:\n    self.__doc__ = \"%s (%s)\"%(doc, \", \".join(self.options.keys()))\nelse:\n    self.__doc__ = doc", "path": "packet\\_packetDescriptors.py", "repo_name": "bytearchive/packetpy", "stars": 0, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"\n    Is this address in the given network?\n\"\"\"\n", "func_signal": "def inNetwork(self, address, mask):\n", "code": "for i in zip(self.bytes, address.bytes, mask.bytes):\n    s, a, m = ord(i[0]), ord(i[1]), ord(i[2])\n    if not (a&m) == (s&m):\n        return False\nreturn True", "path": "packet\\address.py", "repo_name": "bytearchive/packetpy", "stars": 0, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"\n    Retrieve the integer value of a set of bits.\n    byteoffset : offset in bytes of the first byte from the start of data.\n    bitoffset  : offset in bits from byteoffset to the first bit.\n    bitlen     : number of bits to be extracted.\n\"\"\"\n", "func_signal": "def _getBitField(self, frm, bitoffset, bitlen):\n", "code": "byteLen, m = divmod(bitoffset + bitlen, 8)\nif m:\n    byteLen = byteLen+1\nx = self._getIntField(frm, byteLen)\n# Clear the high bits:\nx = x&(pow(2, (byteLen*8 - bitoffset))-1)\n# Shift out the low bits:\nx = x>>(byteLen*8 - bitoffset - bitlen)\nreturn x", "path": "packet\\packet.py", "repo_name": "bytearchive/packetpy", "stars": 0, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"\n    Sends a ping packet, and schedules another alarm for one second\n    from now.\n\"\"\"\n", "func_signal": "def __call__(self, sig, sf):\n", "code": "self.ppack[\"icmp\"].seq_num = self.seq\nself.ppack[\"icmp\"].payload = self._generatePayload()\nself.ppack.finalise()\nself.s.sendto(self.ppack.getRaw(), (self.dst, 0))\nself.numSent += 1\nself.seq += 1\nsignal.alarm(self.wait)", "path": "examples\\ping.py", "repo_name": "bytearchive/packetpy", "stars": 0, "license": "None", "language": "python", "size": 193}
{"docstring": "# We inspect the first byte of the Options to find the initial Option type.\n", "func_signal": "def ipOptionFactory(parent, ipproto, offset):\n", "code": "optionJmpTable = {\n   _IPOption._TypeOptions[\"EndOfList\"]:             IPOptionEndOfList,\n   _IPOption._TypeOptions[\"NOP\"]:                   IPOptionNOP,\n   _IPOption._TypeOptions[\"Security\"]:              IPOptionSecurity,\n   _IPOption._TypeOptions[\"LooseSourceRouting\"]:    IPOptionLooseSourceRouting,\n   _IPOption._TypeOptions[\"StrictSourceRouting\"]:   IPOptionStrictSourceRouting,\n   _IPOption._TypeOptions[\"RecordRoute\"]:           IPOptionRecordRoute,\n   _IPOption._TypeOptions[\"StreamID\"]:              IPOptionStreamID,\n   _IPOption._TypeOptions[\"InternetTimestamp\"]:     IPOptionInternetTimestamp,\n}\nopt = _IPOption(parent, ipproto, offset)\nif optionJmpTable.has_key(opt.optionType):\n    return optionJmpTable[opt.optionType](parent, ipproto, offset)\nelse:\n    return None", "path": "packet\\packet.py", "repo_name": "bytearchive/packetpy", "stars": 0, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"\n    ICMP Factory Function.\n\"\"\"\n", "func_signal": "def ICMP(*args):\n", "code": "stub = ICMPBase(*args)\nif _ICMPJumptable.has_key(stub.itype):\n    return _ICMPJumptable[stub.itype](*args)\nelse:\n    return stub", "path": "packet\\packet.py", "repo_name": "bytearchive/packetpy", "stars": 0, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"\n    Retrieve the cumulative SIZEHINT for this protocol and all\n    protocols it contains.\n\"\"\"\n", "func_signal": "def _getSizehint(self):\n", "code": "sizehint = 0\nfor i in self._getProtoListInclusive():\n    sizehint += i._SIZEHINT\nreturn sizehint", "path": "packet\\packet.py", "repo_name": "bytearchive/packetpy", "stars": 0, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"\n    If 'key' already exists, but in different case, it will be\n    replaced.\n\"\"\"\n", "func_signal": "def _set(self, key, value):\n", "code": "k = key.lower()\nself._ndict[k] = (key, value)\nself._vdict[value] = key", "path": "packet\\_packetDescriptors.py", "repo_name": "bytearchive/packetpy", "stars": 0, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"\n    frm         : The offset from which the field begins. \n    bitoffset   : The offset of the bitfield from the byte specified by frm. \n    bitlen      : The number of bits in the field. \n\"\"\"\n", "func_signal": "def __init__(self, frm, bitoffset, bitlen, doc=\"\"):\n", "code": "self.frm = frm\nself.bitoffset = bitoffset\nself.bitlen = bitlen\nself.__doc__ = doc", "path": "packet\\_packetDescriptors.py", "repo_name": "bytearchive/packetpy", "stars": 0, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"\n    ICMP Factory Function.\n\"\"\"\n", "func_signal": "def ICMP6(*args):\n", "code": "stub = ICMP6Base(*args)\nif _ICMP6Jumptable.has_key(stub.icmp6type):\n    return _ICMP6Jumptable[stub.icmp6type](*args)\nelse:\n    return stub", "path": "packet\\packet.py", "repo_name": "bytearchive/packetpy", "stars": 0, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"\n    Fix all protocol checksums for this packet. \n\"\"\"\n", "func_signal": "def _fixChecksums(self):\n", "code": "lst = self.getProtoList()\nlst.reverse()\nfor i in lst:\n    i._fixChecksums()", "path": "packet\\packet.py", "repo_name": "bytearchive/packetpy", "stars": 0, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"\n    frm  : The offset in bytes from which the field begins. \n    tlen : The length of the field in bytes. \n\"\"\"\n", "func_signal": "def __init__(self, frm, tlen, doc=\"\"):\n", "code": "self.frm = frm\nself.tlen = tlen\nself.__doc__ = doc", "path": "packet\\_packetDescriptors.py", "repo_name": "bytearchive/packetpy", "stars": 0, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"\n    Returns the first and the last addresses in the range defined by\n    this address and the specified mask.\n\"\"\"\n", "func_signal": "def range(self, mask):\n", "code": "f = self.fromInteger(self.integer&mask.integer)\nt = self.fromInteger(f.integer|(~mask.integer&(2**(8*self.WIDTH)-1)))\nreturn f, t", "path": "packet\\address.py", "repo_name": "bytearchive/packetpy", "stars": 0, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"\n    klass   :   A class representing the first protocol in the protocol stack\n    packet  :   The packet._data as a string.\n\"\"\"\n", "func_signal": "def __init__(self, klass, packet, _initialise = 1):\n", "code": "self._data = array.array(\"c\", packet)\nself._klass = klass\nself.protostack = None\nif _initialise:\n    self.initialise()", "path": "packet\\packet.py", "repo_name": "bytearchive/packetpy", "stars": 0, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"Publish the provided activities to Gnip.\n\n@type publisher_name string\n@param publisher_name string The name of the publisher to\n    receive the activities. You must be the owner of the publisher.\n@type activities list of Activity objects\n@param activities The activities to be published\n@return string containing response from the server\n\nThis method allows a publisher to publish activities to the Gnip\nservice. You can only publish activities to a publisher that you own.\n\n\"\"\"\n\n", "func_signal": "def publish_activities(self, publisher_name, activities):\n", "code": "url_path = \"/my/publishers/\" + publisher_name + \"/activity.xml\"\nreturn self.__parse_response(self.__do_http_post(url_path, activities.to_xml()))", "path": "gnip\\__init__.py", "repo_name": "barinek/gnip-python", "stars": 1, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\"Initialize the class.\n\n@type point list of floats\n@param point Point property element containing a pair of coordinates representing latitude then longitude in the WGS84 coordinate reference system\n@type elev float\n@param elev Additional georss property indicating the elevation in meters such as obtained by GPS of the referenced geographic entity.\n@type floor int\n@param floor additional georss property indicating the elevation in building floors of the referenced geographic entity.\n@type feature_type_tag string\n@param feature_type_tag Additional georss property indicating the type of the referenced geographic entity.\n@type feature_name string\n@param feature_name Additional georss property indicating the name or identifier of the referenced geographic entity.\n@type relationship_tag string\n@param relationship_tag Additional georss property indicating the relationship of the accompanying geometric property to the referenced geographic entity.\n\n\"\"\"\n\n", "func_signal": "def __init__(self, point=None, elev=None, floor=None, feature_type_tag=None, feature_name=None, relationship_tag=None):\n", "code": "self.point = point\nself.elev = elev\nself.floor = floor\nself.feature_type_tag = feature_type_tag\nself.feature_name = feature_name\nself.relationship_tag = relationship_tag", "path": "gnip\\place.py", "repo_name": "barinek/gnip-python", "stars": 1, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\"Initialize the class.\n\n@type name string\n@param name The name of the filter\n@type post_url string\n@param post_url The URL to post filter activities to\n@type rules List of rules\n@param rules The rules for the collection\n@type full_data string\n@type full_data Whether or not this filter is for full data\n\nInitializes the class with the proper variables. \n\n\"\"\"\n\n", "func_signal": "def __init__(self, name=\"\", full_data=True, post_url=None, rules=[]):\n", "code": "self.name = name\nself.rules = rules\nself.post_url = post_url\nself.full_data = full_data", "path": "gnip\\filter.py", "repo_name": "barinek/gnip-python", "stars": 1, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\"Return the decoded and uncompressed raw value from a payload\n\n@return string\n\"\"\"\n", "func_signal": "def read_raw(self):\n", "code": "if self.__raw is None:\n    return None\nelse:\n    return self.__decompress_with_gzip(self.__decode(self.__raw))", "path": "gnip\\payload.py", "repo_name": "barinek/gnip-python", "stars": 1, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\"Get Activites (as opposed to Notifications) from a Filter.\n\n@type publisher_scope string\n@param publisher_scope The scope of the publisher (my, public or gnip)  \n@type name string\n@param name The name of the filter you want activities for\n@type publisher_name string\n@param publisher_name The publisher associated with the filter.\n@type date_time datetime\n@param date_time The time for which data should be retrieved\n@return string containing response from the server\n\nGets all of the Activities for a specific Filter. You can specify a time\n(in UTC) from which you would like specific Activities, otherwise the current\ntime will be used.\n\nSee Also: get_filter_notifications()\n\"\"\"\n", "func_signal": "def get_filter_activities(self, publisher_scope, publisher_name, name, date_time=None):\n", "code": "if None == date_time:\n    url_path = \"/\" + publisher_scope + \"/publishers/\" + publisher_name + \"/filters/\" + name + \"/activity/current.xml\"\nelse:\n    corrected_time = self.sync_clock(date_time)\n    time_string = self.time_to_string(corrected_time)\n\n    url_path = \"/\" + publisher_scope + \"/publishers/\" + publisher_name + \"/filters/\" + name + \"/activity/\" + \\\n        time_string + \".xml\"\n\nreturn self.__parse_response(self.__do_http_get(url_path), activities.Activities())", "path": "gnip\\__init__.py", "repo_name": "barinek/gnip-python", "stars": 1, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\"Delete a Gnip filter.\n\n@type publisher_scope string\n@param publisher_scope The scope of the publisher (\"my,\" \"public\" or \"gnip\") \n@type publisher_name string\n@param publisher_name The publisher to create filter for\n@type name string\n@param name The name of the filter to delete\n@return string containing response from the server\n\"\"\"\n\n", "func_signal": "def delete_filter(self, publisher_scope, publisher_name, name):\n", "code": "url_path = \"/\" + publisher_scope + \"/publishers/\" + publisher_name + \"/filters/\" + name + \".xml\"\nreturn self.__parse_response(self.__do_http_delete(url_path))", "path": "gnip\\__init__.py", "repo_name": "barinek/gnip-python", "stars": 1, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\"Delete a Gnip publisher from the \"my\" scope.\n\n@type publisher Publisher\n@param publisher A populated Publisher object\n@return a response object that holds the HTTP response code and an unmarshalled response object\n\nDeletes an existing publisher from the Gnip server within the \"my\" publisher scope.  This operation\nalso deletes all Filters associated with the Publisher.\n\"\"\"\n\n", "func_signal": "def delete_publisher(self, publisher):\n", "code": "url_path = \"/my/publishers/\" + publisher.name + \".xml\"\nreturn self.__parse_response(self.__do_http_delete(url_path))", "path": "gnip\\__init__.py", "repo_name": "barinek/gnip-python", "stars": 1, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\" Populates payload from a payload xml node\n\n@type payload_xml_node Element\n@param payload_xml_node an Element representing the Payload for an Activity.\n\"\"\"\n\n", "func_signal": "def from_xml_node(self, payload_xml_node):\n", "code": "if payload_xml_node is not None:\n\n    title_node = payload_xml_node.find(\"title\")\n    if title_node is not None:\n        self.title = title_node.text\n    else:\n        self.title = None\n\n    body_node = payload_xml_node.find(\"body\")\n    if body_node is not None:\n        self.body = body_node.text\n    else:\n        self.body = None\n\n    media_url_nodes = payload_xml_node.findall(\"mediaURL\")\n    if media_url_nodes is not None:\n        self.media_urls = []\n        for media_url_node in media_url_nodes:\n            media_url = xml_objects.MediaURL(value=media_url_node.text, \n                                             height=media_url_node.get(\"height\"),\n                                             width=media_url_node.get(\"width\"),\n                                             duration=media_url_node.get(\"duration\"),\n                                             type=media_url_node.get(\"type\"),\n                                             mimetype=media_url_node.get(\"mimeType\")\n                                             )\n            self.media_urls.append(media_url)\n    else:\n        self.media_urls = None\n\n    raw_node = payload_xml_node.find(\"raw\")\n    self.__raw = raw_node.text", "path": "gnip\\payload.py", "repo_name": "barinek/gnip-python", "stars": 1, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\"Update our built-in md5 registry\"\"\"\n\n", "func_signal": "def update_md5(filenames):\n", "code": "import re\n\nfor name in filenames:\n    base = os.path.basename(name)\n    f = open(name,'rb')\n    md5_data[base] = md5(f.read()).hexdigest()\n    f.close()\n\ndata = [\"    %r: %r,\\n\" % it for it in md5_data.items()]\ndata.sort()\nrepl = \"\".join(data)\n\nimport inspect\nsrcfile = inspect.getsourcefile(sys.modules[__name__])\nf = open(srcfile, 'rb');\nsrc = f.read();\nf.close()\n\nmatch = re.search(\"\\nmd5_data = {\\n([^}]+)}\", src)\nif not match:\n    print >>sys.stderr, \"Internal error!\"\n    sys.exit(2)\n\nsrc = src[:match.start(1)] + repl + src[match.end(1):]\nf = open(srcfile,'w')\nf.write(src)\nf.close()", "path": "ez_setup.py", "repo_name": "barinek/gnip-python", "stars": 1, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\"Initialize the class.\n\n@type title string\n@param title Activity title\n@type body string\n@param body Activity body\n@type media_urls List of URLs\n@param media_urls MediaURLs associated with the activity\n@type raw raw data of a payload that will be compressed and encoded\n@param raw Raw text of activity\n\n\"\"\"\n\n", "func_signal": "def __init__(self, title=None, body=None, media_urls=None, raw=None):\n", "code": "self.title = title\nself.body = body\nself.media_urls = media_urls\nself.write_raw(raw)", "path": "gnip\\payload.py", "repo_name": "barinek/gnip-python", "stars": 1, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\"Create a Gnip publisher in the \"my\" scope.\n\n@type publisher Publisher\n@param publisher A populated Publisher object\n@return string containing response from the server\n\nCreates a new publisher on the Gnip server within the \"my\" publisher scope.\nThe publisher will be visible only to your account, and only you, can\npublish to it.\n\"\"\"\n\n", "func_signal": "def create_publisher(self, publisher):\n", "code": "url_path = \"/my/publishers\"\nreturn self.__parse_response(self.__do_http_post(url_path, publisher.to_xml()))", "path": "gnip\\__init__.py", "repo_name": "barinek/gnip-python", "stars": 1, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\"Update a Gnip filter.\n\n@type publisher_scope string\n@param publisher_scope The scope of the publisher (my, public or gnip)  \n@type publisher_name string\n@param publisher_name The publisher of the filter\n@type filter Filter\n@param filte A populated Filter object\n@return string containing response from the server\n\nUpdates the Filter on the Gnip service with the Filter provided.\nThe Filter must already exist on the service.\n\n\"\"\"\n", "func_signal": "def update_filter(self, publisher_scope, publisher_name, filter):\n", "code": "url_path = \"/\" + publisher_scope + \"/publishers/\" + publisher_name + \"/filters/\" + filter.name + \".xml\"\nreturn self.__parse_response(self.__do_http_put(url_path, filter.to_xml()))", "path": "gnip\\__init__.py", "repo_name": "barinek/gnip-python", "stars": 1, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\"Adjust a time so that it corresponds with Gnip time\n\n@type datetime datetime\n@param datetime The datetime object to adjust\n@return datetime object representing the corrected time\n\nThis method gets the current time from the Gnip server,\ngets the current local time and determines the difference \nbetween the two. It then adjusts the passed in time to \naccount for the difference. This method can be used to ensure\nyour application's time is in sync with Gnip server time in\norder to prevent clock drift between the two.\n\n\"\"\"\n\n# Do HTTP HEAD request\n", "func_signal": "def sync_clock(self, the_time):\n", "code": "resp, content = self.__do_http_head()\n\n# Get local time, before we do any other processing\n# so that we can get the two times as close as possible\nlocal_time = datetime.datetime.utcnow()\n\n# Get time from headers and parse into python format\ngnip_time = datetime.datetime.strptime(resp[\"date\"], \"%a, %d %b %Y %H:%M:%S %Z\")\n\n# Determine the time difference\ntime_delta = gnip_time - local_time\n\n# Return the corrected time\nreturn the_time + time_delta", "path": "gnip\\__init__.py", "repo_name": "barinek/gnip-python", "stars": 1, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\"Get a Publisher's Notifications (as opposed to Activities).\n\n@type publisher_scope string\n@param publisher_scope The scope of the publisher (\"my,\" \"public\" or \"gnip\") \n@type publisher_name string\n@param publisher_name The publisher you want Notifications for.\n@type date_time datetime\n@param date_time The datetime for which data should be retrieved\n@return List of Activity objects, one for each activity retrieved\n\nGets all of the Notifications for a specific publisher. You can specify a time\n(in UTC) from which you would like specific Notifications, otherwise the current\ntime will be used.\n\nGnip currently represents both \"Activities\" and \"Notifications\" in the same\nobject: Activity. \"Notification\" and \"Activity\" Activity objects are identical, with the\nexception of the \"Activity\" Activity object containing a payload.\n\nSee Also: get_publisher_activities()\n\"\"\"\n", "func_signal": "def get_publisher_notifications(self, publisher_scope, publisher_name, date_time=None):\n", "code": "if None == date_time:\n    url_path = \"/\" + publisher_scope + \"/publishers/\" + publisher_name + \"/notification/current.xml\"\nelse:\n    corrected_time = self.sync_clock(date_time)\n    time_string = self.time_to_string(corrected_time)\n\n    url_path = \"/\" + publisher_scope + \"/publishers/\" + publisher_name + \"/notification/\" + time_string + \".xml\"\n\nreturn self.__parse_response(self.__do_http_get(url_path), activities.Activities())", "path": "gnip\\__init__.py", "repo_name": "barinek/gnip-python", "stars": 1, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\"Get a Gnip publisher.\n\n@type scope string\n@param scope The scope of the publisher (\"my,\" \"public\" or \"gnip\")\n@type name string\n@param name The name of the publisher to get\n@return Publisher object based on response from the server\n\nGets a Publisher from the Gnip server. The Publisher object allows\nyou to determine what capabilities a Publisher supports. These\ncapabilities determine what kind of rules you can use when creating\na Filter.\n\"\"\"\n\n", "func_signal": "def get_publisher(self, scope, name):\n", "code": "url_path = \"/\" + scope + \"/publishers/\" + name + \".xml\"\nreturn self.__parse_response(self.__do_http_get(url_path),publisher.Publisher())", "path": "gnip\\__init__.py", "repo_name": "barinek/gnip-python", "stars": 1, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\"Add a rule to a pre-existing Gnip filter.\n\n@type publisher_scope string\n@param publisher_scope The scope of the publisher (\"my,\" \"public\" or \"gnip\")  \n@type publisher_name string\n@param publisher_name The publisher of the filter to update\n@type filter_name string\n@param filter_name The filter to update\n@type rule string\n@param rule a Rule object to add\n@return string containing response from the server\n\"\"\"\n\n", "func_signal": "def add_rule_to_filter(self, publisher_scope, publisher_name, filter_name, rule):\n", "code": "url_path = \"/\" + publisher_scope + \"/publishers/\" + publisher_name + \"/filters/\" + filter_name + \"/rules.xml\"\nreturn self.__parse_response(self.__do_http_post(url_path, rule.to_xml()))", "path": "gnip\\__init__.py", "repo_name": "barinek/gnip-python", "stars": 1, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\"Set the raw for the payload.\n\n   @type raw string\n   @param raw string will be compressed and encoded.\n\"\"\"\n", "func_signal": "def write_raw(self, raw):\n", "code": "if raw is None:\n    self.__raw = None\nelse:\n    self.__raw = self.__encode(self.__compress_with_gzip(raw))", "path": "gnip\\payload.py", "repo_name": "barinek/gnip-python", "stars": 1, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\"Determine whether or not a given rule exists in an existing Filter.\n\n@type publisher_scope string\n@param publisher_scope The scope of the publisher (\"my,\" \"public\" or \"gnip\") \n@type publisher_name string\n@param publisher_name The publisher of the filter to update\n@type filter_name string\n@param filter_name The filter to check\n@type rule string\n@param rule a Rule to check\n@return boolean as to the existance of the rule, None if existance of the rule can't be determined\n\"\"\"\n\n", "func_signal": "def rule_exists_in_filter(self, publisher_scope, publisher_name, filter_name, rule):\n", "code": "url_path = \"/\" + publisher_scope + \"/publishers/\" + publisher_name + \"/filters/\" + filter_name + \"/rules?\" + rule.to_delete_query_string()\n\nresponse, body = self.__do_http_get(url_path)\nif (response.status == 200):\n    return True\nelif (response.status == 404):\n    return False\nelse:\n    return None", "path": "gnip\\__init__.py", "repo_name": "barinek/gnip-python", "stars": 1, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\"Update a Gnip filter.\n\n@type publisher Publisher\n@param publisher The publisher object to update\n@return string containing response from the server\n\nUpdates a pre-existing Publisher with the Publisher provided.\n\"\"\"\n\n", "func_signal": "def update_publisher(self, publisher):\n", "code": "url_path = \"/my/publishers/\" + publisher.name + \".xml\"\nreturn self.__parse_response(self.__do_http_put(url_path, publisher.to_xml()))", "path": "gnip\\__init__.py", "repo_name": "barinek/gnip-python", "stars": 1, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\" Return a XML representation of the Publisher as a string. \"\"\"\n", "func_signal": "def to_xml(self):\n", "code": "xml = '<publisher name=\"' + self.name + '\">'\nxml += '<supportedRuleTypes>'\n\nfor rule_type in self.rule_types:\n    xml += '<type>' + rule_type + '</type>'\n\nxml += '</supportedRuleTypes></publisher>'\n\nreturn xml", "path": "gnip\\publisher.py", "repo_name": "barinek/gnip-python", "stars": 1, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\" return number of returned results \"\"\"\n", "func_signal": "def count(self):\n", "code": "self._fetch_if_needed()\nreturn len(self._result_cache.get('rows', []))", "path": "couchdbkit\\client.py", "repo_name": "jeremi/couchdbkit", "stars": 1, "license": "isc", "language": "python", "size": 1944}
{"docstring": "\"\"\" bulk save. Modify Multiple Documents With a Single Request\n\n@param docs: list of docs\n@param use_uuids: add _id in doc who don't have it already set.\n@param all_or_nothing: In the case of a power failure, when the database \nrestarts either all the changes will have been saved or none of them. \nHowever, it does not do conflict checking, so the documents will\n@param _raw_json: return raw json instead deserializing it\nbe committed even if this creates conflicts.\n\nWith `_raw_json=True` it return raw response. When False it return anything\nbut update list of docs with new revisions and members (like deleted)\n\n.. seealso:: `HTTP Bulk Document API <http://wiki.apache.org/couchdb/HTTP_Bulk_Document_API>`\n\n\"\"\"\n# we definitely need a list here, not any iterable, or groupby will fail\n", "func_signal": "def bulk_save(self, docs, use_uuids=True, all_or_nothing=False, _raw_json=False):\n", "code": "docs = list(docs)\n        \ndef is_id(doc):\n    return '_id' in doc\n    \nif use_uuids:\n    ids = []\n    noids = []\n    for k, g in groupby(docs, is_id):\n        if not k:\n            noids = list(g)\n        else:\n            ids = list(g)\n    \n    uuid_count = max(len(noids), self.server.uuid_batch_count)\n    for doc in noids:\n        nextid = self.server.next_uuid(count=uuid_count)\n        if nextid:\n            doc['_id'] = nextid\n            \npayload = { \"docs\": docs }\nif all_or_nothing:\n    payload[\"all-or-nothing\"] = True\n    \n# update docs\nresults = self.res.post('/_bulk_docs', payload=payload, _raw_json=_raw_json)\n\nif _raw_json:\n    return results\n\nfor i, res in enumerate(results):\n    docs[i].update({'_id': res['id'], '_rev': res['rev']})", "path": "couchdbkit\\client.py", "repo_name": "jeremi/couchdbkit", "stars": 1, "license": "isc", "language": "python", "size": 1944}
{"docstring": "\"\"\" convert a value to json using appropriate regexp.\nFor Dates we use ISO 8601. Decimal are converted to string.\n\n\"\"\"\n", "func_signal": "def value_to_json(value, item_type=None):\n", "code": "if isinstance(value, datetime.datetime) and is_type_ok(item_type, datetime.datetime):\n    value = value.replace(microsecond=0).isoformat() + 'Z'\nelif isinstance(value, datetime.date) and is_type_ok(item_type, datetime.date):\n    value = value.isoformat()\nelif isinstance(value, datetime.time) and is_type_ok(item_type, datetime.time):\n    value = value.replace(microsecond=0).isoformat()\nelif isinstance(value, decimal.Decimal) and is_type_ok(item_type, decimal.Decimal):\n    value = unicode(value) \nelif isinstance(value, list):\n    value = list_to_json(value, item_type)\nelif isinstance(value, dict):\n    value = dict_to_json(value, item_type)\nreturn value", "path": "couchdbkit\\schema\\properties.py", "repo_name": "jeremi/couchdbkit", "stars": 1, "license": "isc", "language": "python", "size": 1944}
{"docstring": "\"\"\"Default value for list.\n\nBecause the property supplied to 'default' is a static value,\nthat value must be shallow copied to prevent all fields with\ndefault values from sharing the same instance.\n\nReturns:\n  Copy of the default value.\n\"\"\"\n", "func_signal": "def default_value(self):\n", "code": "value = super(DictProperty, self).default_value()\nif value is None:\n    value = {}\nreturn dict(value)", "path": "couchdbkit\\schema\\properties.py", "repo_name": "jeremi/couchdbkit", "stars": 1, "license": "isc", "language": "python", "size": 1944}
{"docstring": "\"\"\" Create a database on CouchDb host\n\n@param dname: str, name of db\n\n@return: Database instance if it's ok or dict message\n\"\"\"\n", "func_signal": "def create_db(self, dbname):\n", "code": "_dbname = url_quote(validate_dbname(dbname), safe=\":\")\nres = self.res.put('/%s/' % _dbname)\nif res['ok']:\n    return Database(self, dbname)\nreturn res['ok']", "path": "couchdbkit\\client.py", "repo_name": "jeremi/couchdbkit", "stars": 1, "license": "isc", "language": "python", "size": 1944}
{"docstring": "\"\"\"\nreturn an available uuid from couchdbkit\n\"\"\"\n", "func_signal": "def next_uuid(self, count=None):\n", "code": "if count is not None:\n    self._uuid_batch_count = count\nelse:\n    self._uuid_batch_count = self.uuid_batch_count\n\nself.uuids = self.uuids or []\nif not self.uuids:\n    self.uuids = self.res.get('/_uuids', count=self._uuid_batch_count)[\"uuids\"]\nreturn self.uuids.pop()", "path": "couchdbkit\\client.py", "repo_name": "jeremi/couchdbkit", "stars": 1, "license": "isc", "language": "python", "size": 1944}
{"docstring": "\"\"\" validate value \"\"\"\n", "func_signal": "def validate(self, value, required=True):\n", "code": "if required and self.empty(value):\n    if self.required:\n        raise BadValueError(\"Property %s is required.\" % self.name)\nelse:\n    if self.choices:\n        match = False\n        for choice in self.choices:\n            if choice == value:\n                match = True\n                break\n        if not match:\n            raise BadValueError('Property %s is %r; must be one of %r' % (\n                self.name, value, self.choices))\nif self.validators:\n    if isinstance(self.validators, (list, tuple,)):\n        for validator in self.validators:\n            if callable(validator):\n                validator(value)\n    elif callable(self.validators):\n        self.validators(value)\nreturn value", "path": "couchdbkit\\schema\\properties.py", "repo_name": "jeremi/couchdbkit", "stars": 1, "license": "isc", "language": "python", "size": 1944}
{"docstring": "\"\"\"Constructor for Database\n\n@param server: Server instance\n@param dbname: str, name of database\n\"\"\"\n\n", "func_signal": "def __init__(self, server, dbname):\n", "code": "if not hasattr(server, 'next_uuid'):\n    raise TypeError('%s is not a couchdbkit.server instance' % \n                    server.__class__.__name__)\n                    \nself.dbname = validate_dbname(dbname)\nself.server = server\nself.res = server.res.clone()\nif \"/\" in dbname:\n    self.res.client.safe = \":/%\"\nself.res.update_uri('/%s' % url_quote(dbname, safe=\":\"))", "path": "couchdbkit\\client.py", "repo_name": "jeremi/couchdbkit", "stars": 1, "license": "isc", "language": "python", "size": 1944}
{"docstring": "\"\"\" bulk delete. \nIt adds '_deleted' member to doc then uses bulk_save to save them.\n\n@param _raw_json: return raw json instead deserializing it\n\nWith `_raw_json=True` it return raw response. When False it return anything\nbut update list of docs with new revisions and members.\n\n\"\"\"\n", "func_signal": "def bulk_delete(self, docs, all_or_nothing=False, _raw_json=False):\n", "code": "for doc in docs:\n    doc['_deleted'] = True\nself.bulk_save(docs, use_uuids=False, all_or_nothing=all_or_nothing,\n            _raw_json=_raw_json)", "path": "couchdbkit\\client.py", "repo_name": "jeremi/couchdbkit", "stars": 1, "license": "isc", "language": "python", "size": 1944}
{"docstring": "\"\"\" copy an existing document to a new id. If dest is None, a new uuid will be requested\n@param doc: dict or string, document or document id\n@param dest: basestring or dict. if _rev is specified in dict it will override the doc\n@param _raw_json: return raw json instead deserializing it\n\"\"\"\n", "func_signal": "def copy_doc(self, doc, dest=None, _raw_json=False):\n", "code": "if isinstance(doc, basestring):\n    docid = doc\nelse:\n    if not '_id' in doc:\n        raise KeyError('_id is required to copy a doc')\n    docid = doc['_id']\n\nif dest is None:\n    destination = self.server.next_uuid(count=1)   \nelif isinstance(dest, basestring):\n    if dest in self:\n        dest = self.get(dest)['_rev']\n        destination = \"%s?rev=%s\" % (dest['_id'], dest['_rev'])\n    else:\n        destination = dest\nelif isinstance(dest, dict):\n    if '_id' in dest and '_rev' in dest and dest['_id'] in self:\n        rev = dest['_rev']\n        destination = \"%s?rev=%s\" % (dest['_id'], dest['_rev'])\n    else:\n        raise KeyError(\"dest doesn't exist or this not a document ('_id' or '_rev' missig).\")\n    \nif destination:\n    result = self.res.copy('/%s' % docid, headers={ \"Destination\": str(destination) },\n                        _raw_json=_raw_json)\n    return result\n    \nresult = { 'ok': False }\nif raw_json:\n    return anyjson.serialize(result)\nreturn result", "path": "couchdbkit\\client.py", "repo_name": "jeremi/couchdbkit", "stars": 1, "license": "isc", "language": "python", "size": 1944}
{"docstring": "\"\"\" render jinja template \"\"\"\n", "func_signal": "def render_template(template_name, _stream=False, **kwargs):\n", "code": "tmpl = template_env.get_template(template_name)\ncontext = kwargs\nif _stream:\n    return tmpl.stream(context)\nreturn tmpl.render(context)", "path": "doc\\couchdbkit.org\\buildweb.py", "repo_name": "jeremi/couchdbkit", "stars": 1, "license": "isc", "language": "python", "size": 1944}
{"docstring": "\"\"\" validate a value. test if value is in supported types \"\"\"\n", "func_signal": "def validate_content(value, item_type=None):\n", "code": "if isinstance(value, list):\n    value = validate_list_content(value, item_type=item_type)\nelif isinstance(value, dict):\n    value = validate_dict_content(value, item_type=item_type)\nelif item_type is not None and type(value) != item_type:\n    raise BadValueError(\n        'Items  must all be in %s' % item_type)\nelif type(value) not in ALLOWED_PROPERTY_TYPES:\n        raise BadValueError(\n            'Items  must all be in %s' %\n                (ALLOWED_PROPERTY_TYPES))\nreturn value", "path": "couchdbkit\\schema\\properties.py", "repo_name": "jeremi/couchdbkit", "stars": 1, "license": "isc", "language": "python", "size": 1944}
{"docstring": "\"\"\" get adhoc view results. Like view it reeturn a ViewResult object.\"\"\"\n", "func_signal": "def temp_view(self, design, obj=None, wrapper=None, **params):\n", "code": "if obj is not None:\n    if not hasattr(obj, 'wrap'):\n        raise AttributeError(\" no 'wrap' method found in obj %s)\" % str(obj))\n    wrapper = obj.wrap\nreturn TempView(self, design, wrapper=wrapper)(**params)", "path": "couchdbkit\\client.py", "repo_name": "jeremi/couchdbkit", "stars": 1, "license": "isc", "language": "python", "size": 1944}
{"docstring": "\"\"\" retrieve revisions of a doc\n    \n@param docid: str, id of document\n@param with_doc: bool, if True return document\ndict with revisions as member, if false return \nonly revisions\n@param _raw_json: return raw json instead deserializing it\n\n@return: dict: '_rev_infos' member if you have set with_doc\nto True :\n\n\n        {\n            \"_revs_info\": [\n                {\"rev\": \"123456\", \"status\": \"disk\"},\n                    {\"rev\": \"234567\", \"status\": \"missing\"},\n                    {\"rev\": \"345678\", \"status\": \"deleted\"},\n            ]\n        }\n    \nIf False, return current revision of the document, but with\nan additional field, _revs, the value being a list of \nthe available revision IDs. \n\"\"\"\n", "func_signal": "def doc_revisions(self, docid, with_doc=True, _raw_json=False):\n", "code": "docid = self.escape_docid(docid)\ntry:\n    if with_doc:\n        doc_with_rev = self.res.get(docid, _raw_json=_raw_json, revs=True)\n    else:\n        doc_with_revs = self.res.get(docid, _raw_json=_raw_json, revs_info=True)\nexcept ResourceNotFound:\n    return None\nreturn doc_with_revs", "path": "couchdbkit\\client.py", "repo_name": "jeremi/couchdbkit", "stars": 1, "license": "isc", "language": "python", "size": 1944}
{"docstring": "\"\"\" convert a value to json from Property._to_json \"\"\"\n", "func_signal": "def convert_property(value):\n", "code": "if type(value) in MAP_TYPES_PROPERTIES:\n    prop = MAP_TYPES_PROPERTIES[type(value)]()\n    value = prop.to_json(value)\nreturn value", "path": "couchdbkit\\schema\\properties.py", "repo_name": "jeremi/couchdbkit", "stars": 1, "license": "isc", "language": "python", "size": 1944}
{"docstring": "\"\"\" Save a document. It will use the `_id` member of the document \nor request a new uuid from CouchDB. IDs are attached to\ndocuments on the client side because POST has the curious property of\nbeing automatically retried by proxies in the event of network\nsegmentation and lost responses. (Idee from `Couchrest <http://github.com/jchris/couchrest/>`)\n\n@param doc: dict.  doc is updated \nwith doc '_id' and '_rev' properties returned \nby CouchDB server when you save.\n@param _raw_json: return raw json instead deserializing it\n@param params, list of optionnal params, like batch=\"ok\"\n\nwith `_raw_json=True` It return raw response. If False it return\nanything but update doc object with new revision (if batch=False).\n\"\"\"\n", "func_signal": "def save_doc(self, doc, _raw_json=False, **params):\n", "code": "if doc is None:\n    doc = {}\n    \nif '_attachments' in doc:\n    doc['_attachments'] = self.encode_attachments(doc['_attachments'])\n    \nif '_id' in doc:\n    docid = self.escape_docid(doc['_id'])\n    res = self.res.put(docid, payload=doc, _raw_json=_raw_json, **params)\nelse:\n    try:\n        doc['_id'] = self.server.next_uuid()\n        res = self.res.put(doc['_id'], payload=doc,\n                    _raw_json=_raw_json, **params)\n    except:\n        res = self.res.post(payload=doc, _raw_json=_raw_json, **params)\n        \nif _raw_json:    \n    return res\n        \nif 'batch' in params and 'id' in res:\n    doc.update({ '_id': res['id']})\nelse:\n    doc.update({ '_id': res['id'], '_rev': res['rev']})", "path": "couchdbkit\\client.py", "repo_name": "jeremi/couchdbkit", "stars": 1, "license": "isc", "language": "python", "size": 1944}
{"docstring": "\"\"\" validate type of values in a dict \"\"\"\n", "func_signal": "def validate_dict_content(value, item_type=None):\n", "code": "return dict([(k, validate_content(v, \n            item_type=item_type)) for k, v in value.iteritems()])", "path": "couchdbkit\\schema\\properties.py", "repo_name": "jeremi/couchdbkit", "stars": 1, "license": "isc", "language": "python", "size": 1944}
{"docstring": "\"\"\"Get document from database\n\nArgs:\n@param docid: str, document id to retrieve \n@param rev: if specified, allows you to retrieve\na specific revision of document\n@param wrapper: callable. function that takes dict as a param. \nUsed to wrap an object.\n@param _raw_json: return raw json instead deserializing it\n\n@return: dict, representation of CouchDB document as\n a dict.\n\"\"\"\n", "func_signal": "def get(self, docid, rev=None, wrapper=None, _raw_json=False):\n", "code": "docid = self.escape_docid(docid)\nif rev is not None:\n    doc = self.res.get(docid, _raw_json=_raw_json, rev=rev)\nelse:\n    doc = self.res.get(docid, _raw_json=_raw_json)\n\nif wrapper is not None:\n    if not callable(wrapper):\n        raise TypeError(\"wrapper isn't a callable\")\n    return wrapper(doc)\n\nreturn doc", "path": "couchdbkit\\client.py", "repo_name": "jeremi/couchdbkit", "stars": 1, "license": "isc", "language": "python", "size": 1944}
{"docstring": "\"\"\" return default value \"\"\"\n\n", "func_signal": "def default_value(self):\n", "code": "default = self.default\nif callable(default):\n    default = default()\nreturn default", "path": "couchdbkit\\schema\\properties.py", "repo_name": "jeremi/couchdbkit", "stars": 1, "license": "isc", "language": "python", "size": 1944}
{"docstring": "\"\"\"\nReturn exactly one result or raise an exception.\n\n\nRaises `couchdbkit.exceptions.MultipleResultsFound` if multiple rows are returned.\nIf except_all is True, raises `couchdbkit.exceptions.NoResultFound` \nif the query selects no rows. \n\nThis results in an execution of the underlying query.\n\"\"\"\n\n", "func_signal": "def one(self, except_all=False):\n", "code": "length = len(self)\nif length > 1:\n    raise MultipleResultsFound(\"%s results found.\" % length)\n\nresult = self.first()\nif result is None and except_all:\n    raise NoResultFound\nreturn result", "path": "couchdbkit\\client.py", "repo_name": "jeremi/couchdbkit", "stars": 1, "license": "isc", "language": "python", "size": 1944}
{"docstring": "# Remove the standard version of Django\n", "func_signal": "def setup_project():\n", "code": "for k in [k for k in sys.modules if k.startswith('django')]:\n    del sys.modules[k]\n\nfrom appenginepatcher import on_production_server\nif on_production_server:\n    # This fixes a pwd import bug for os.path.expanduser()\n    global env_ext\n    env_ext['HOME'] = PROJECT_DIR\n\nos.environ.update(env_ext)\n\n# Add the two parent folders and appenginepatcher's lib folder to sys.path.\n# The current folder has to be added in main.py or setup_env(). This\n# suggests a folder structure where you separate reusable code from project\n# code:\n# project -> common -> appenginepatch\n# You can put a custom Django version into the \"common\" folder, for example.\nEXTRA_PATHS = [\n    PROJECT_DIR,\n    COMMON_DIR,\n]\n\nthis_folder = os.path.abspath(os.path.dirname(__file__))\nEXTRA_PATHS.append(os.path.join(this_folder, 'appenginepatcher', 'lib'))\n\n# We support zipped packages in the common and project folders.\n# The files must be in the packages folder.\nfor packages_dir in ZIP_PACKAGES_DIRS:\n    if os.path.isdir(packages_dir):\n        for zip_package in os.listdir(packages_dir):\n            EXTRA_PATHS.append(os.path.join(packages_dir, zip_package))\n\nsys.path = EXTRA_PATHS + sys.path", "path": "common\\appenginepatch\\aecmd.py", "repo_name": "hokten/gae-site", "stars": 1, "license": "None", "language": "python", "size": 3456}
{"docstring": "\"\"\"\nCreate the new ``User`` and ``RegistrationProfile``, and\nreturns the ``User``.\n\nThis is essentially a light wrapper around\n``RegistrationProfile.objects.create_inactive_user()``,\nfeeding it the form data and a profile callback (see the\ndocumentation on ``create_inactive_user()`` for details) if\nsupplied.\n\n\"\"\"\n", "func_signal": "def save(self, domain_override=\"\"):\n", "code": "new_user = RegistrationProfile.objects.create_inactive_user(\n    username=self.cleaned_data['username'],\n    password=self.cleaned_data['password1'],\n    email=self.cleaned_data['email'],\n    domain_override=domain_override)\nself.instance = new_user\nreturn super(UserRegistrationForm, self).save()", "path": "myapp\\forms.py", "repo_name": "hokten/gae-site", "stars": 1, "license": "None", "language": "python", "size": 3456}
{"docstring": "\"\"\"Decorator that always runs the given function in a transaction.\"\"\"\n", "func_signal": "def transaction(func):\n", "code": "def _transaction(*args, **kwargs):\n    return db.run_in_transaction(func, *args, **kwargs)\n# In case you need to run it without a transaction you can call\n# <func>.non_transactional(...)\n_transaction.non_transactional = func\nreturn _transaction", "path": "common\\appenginepatch\\ragendja\\dbutils.py", "repo_name": "hokten/gae-site", "stars": 1, "license": "None", "language": "python", "size": 3456}
{"docstring": "\"\"\"\nEscapes a string such that it can be used safely as a key_name.\n\nYou can pass multiple values in order to build a path.\n\"\"\"\n", "func_signal": "def generate_key_name(*values):\n", "code": "return KEY_NAME_PREFIX + '/'.join(\n    [value.replace('%', '%1').replace('/', '%2') for value in values])", "path": "common\\appenginepatch\\ragendja\\dbutils.py", "repo_name": "hokten/gae-site", "stars": 1, "license": "None", "language": "python", "size": 3456}
{"docstring": "\"\"\"\nCompares two lists and returs True if they contain the same elements, but\ndoesn't require that they have the same order.\n\"\"\"\n", "func_signal": "def equal_lists(left, right):\n", "code": "right = list(right)\nif len(left) != len(right):\n    return False\nfor item in left:\n    if item in right:\n        del right[right.index(item)]\n    else:\n        return False\nreturn True", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "hokten/gae-site", "stars": 1, "license": "None", "language": "python", "size": 3456}
{"docstring": "\"\"\"Copy all media files directly\"\"\"\n", "func_signal": "def add_uncombined_app_media(env, app):\n", "code": "path = os.path.join(\n    os.path.dirname(__import__(app, {}, {}, ['']).__file__), 'media')\napp = app.rsplit('.', 1)[-1]\nfor root, dirs, files in os.walk(path):\n    for file in files:\n        if file.endswith(('.css', '.js')):\n            base = os.path.join(root, file)[len(path):].replace(os.sep,\n                '/').lstrip('/')\n            target = '%s/%s' % (app, base)\n            add_app_media(env, target, target)", "path": "common\\appenginepatch\\ragendja\\settings_post.py", "repo_name": "hokten/gae-site", "stars": 1, "license": "None", "language": "python", "size": 3456}
{"docstring": "\"\"\"\nConverts dict to table-style list of rows with heading:\n\nExample:\ndict_list_to_table(('a', 'b', 'c'),\n    [{'a': 1, 'b': 2, 'c': 3}, {'a': 11, 'b': 12, 'c': 13}])\nresults in the following (dict keys reordered for better readability):\n[\n    ('a', 'b', 'c'),\n    (1, 2, 3),\n    (11, 12, 13),\n]\n\"\"\"\n", "func_signal": "def dict_list_to_table(headings, dict_list):\n", "code": "return [headings] + [tuple([row[heading] for heading in headings])\n                     for row in dict_list]", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "hokten/gae-site", "stars": 1, "license": "None", "language": "python", "size": 3456}
{"docstring": "\"\"\"Helper method for get_filtered.\"\"\"\n", "func_signal": "def get_filters(*filters):\n", "code": "if len(filters) % 2 == 1:\n    raise ValueError('You must supply an even number of arguments!')\nreturn zip(filters[::2], filters[1::2])", "path": "common\\appenginepatch\\ragendja\\dbutils.py", "repo_name": "hokten/gae-site", "stars": 1, "license": "None", "language": "python", "size": 3456}
{"docstring": "# Fix handling of verbose_name. Google resolves lazy translation objects\n# immedately which of course breaks translation support.\n# http://code.google.com/p/googleappengine/issues/detail?id=583\n", "func_signal": "def fix_app_engine_bugs():\n", "code": "from django import forms\nfrom django.utils.text import capfirst\n# This import is needed, so the djangoforms patch can do its work, first\nfrom google.appengine.ext.db import djangoforms\ndef get_form_field(self, form_class=forms.CharField, **kwargs):\n    defaults = {'required': self.required}\n    defaults['label'] = capfirst(self.verbose_name)\n    if self.choices:\n        choices = []\n        if not self.required or (self.default is None and\n                                 'initial' not in kwargs):\n            choices.append(('', '---------'))\n        for choice in self.choices:\n            choices.append((unicode(choice), unicode(choice)))\n        defaults['widget'] = forms.Select(choices=choices)\n    if self.default is not None:\n        defaults['initial'] = self.default\n    defaults.update(kwargs)\n    return form_class(**defaults)\ndb.Property.get_form_field = get_form_field\n\n# Extend ModelForm with support for EmailProperty\n# http://code.google.com/p/googleappengine/issues/detail?id=880\ndef get_form_field(self, **kwargs):\n    \"\"\"Return a Django form field appropriate for an email property.\"\"\"\n    defaults = {'form_class': forms.EmailField}\n    defaults.update(kwargs)\n    return super(db.EmailProperty, self).get_form_field(**defaults)\ndb.EmailProperty.get_form_field = get_form_field\n\n# Fix DateTimeProperty, so it returns a property even for auto_now and\n# auto_now_add.\n# http://code.google.com/p/googleappengine/issues/detail?id=994\ndef get_form_field(self, **kwargs):\n    defaults = {'form_class': forms.DateTimeField}\n    defaults.update(kwargs)\n    return super(db.DateTimeProperty, self).get_form_field(**defaults)\ndb.DateTimeProperty.get_form_field = get_form_field\ndef get_form_field(self, **kwargs):\n    defaults = {'form_class': forms.DateField}\n    defaults.update(kwargs)\n    return super(db.DateProperty, self).get_form_field(**defaults)\ndb.DateProperty.get_form_field = get_form_field\ndef get_form_field(self, **kwargs):\n    defaults = {'form_class': forms.TimeField}\n    defaults.update(kwargs)\n    return super(db.TimeProperty, self).get_form_field(**defaults)\ndb.TimeProperty.get_form_field = get_form_field\n\n# Fix default value of UserProperty (Google resolves the user too early)\n# http://code.google.com/p/googleappengine/issues/detail?id=879\nfrom django.utils.functional import lazy\nfrom google.appengine.api import users\ndef get_form_field(self, **kwargs):\n    defaults = {'initial': lazy(users.GetCurrentUser, users.User)()}\n    defaults.update(kwargs)\n    return super(db.UserProperty, self).get_form_field(**defaults)\ndb.UserProperty.get_form_field = get_form_field\n\n# Fix file uploads via BlobProperty\ndef get_form_field(self, **kwargs):\n    defaults = {'form_class': forms.FileField}\n    defaults.update(kwargs)\n    return super(db.BlobProperty, self).get_form_field(**defaults)\ndb.BlobProperty.get_form_field = get_form_field\ndef get_value_for_form(self, instance):\n    return getattr(instance, self.name)\ndb.BlobProperty.get_value_for_form = get_value_for_form\nfrom django.core.files.uploadedfile import UploadedFile\ndef make_value_from_form(self, value):\n    if isinstance(value, UploadedFile):\n        return db.Blob(value.read())\n    return super(db.BlobProperty, self).make_value_from_form(value)\ndb.BlobProperty.make_value_from_form = make_value_from_form\n\n# Optimize ReferenceProperty, so it returns the key directly\n# http://code.google.com/p/googleappengine/issues/detail?id=993\ndef get_value_for_form(self, instance):\n    return self.get_value_for_datastore(instance)\ndb.ReferenceProperty.get_value_for_form = get_value_for_form\n# Use our ModelChoiceField instead of Google's\ndef get_form_field(self, **kwargs):\n    defaults = {'form_class': forms.ModelChoiceField,\n                'queryset': self.reference_class.all()}\n    defaults.update(kwargs)\n    return super(db.ReferenceProperty, self).get_form_field(**defaults)\ndb.ReferenceProperty.get_form_field = get_form_field", "path": "common\\appenginepatch\\appenginepatcher\\patch.py", "repo_name": "hokten/gae-site", "stars": 1, "license": "None", "language": "python", "size": 3456}
{"docstring": "\"\"\"\nValidate that the username is alphanumeric and is not already\nin use.\n\n\"\"\"\n", "func_signal": "def clean_username(self):\n", "code": "user = User.get_by_key_name(\"key_\"+self.cleaned_data['username'].lower())\nif user and user.is_active:\n    raise forms.ValidationError(__(u'This username is already taken. Please choose another.'))\nreturn self.cleaned_data['username']", "path": "myapp\\forms.py", "repo_name": "hokten/gae-site", "stars": 1, "license": "None", "language": "python", "size": 3456}
{"docstring": "# Models can define a CLEANUP_REFERENCES attribute if they have\n# reference properties that must get geleted with the model.\n", "func_signal": "def _get_included_cleanup_entities(entities, rels_seen, to_delete, to_put):\n", "code": "include_references = getattr(entities[0], 'CLEANUP_REFERENCES', None)\nif include_references:\n    if not isinstance(include_references, (list, tuple)):\n        include_references = (include_references,)\n    prefetch_references(entities, include_references)\n    for entity in entities:\n        for name in include_references:\n            subentity = getattr(entity, name)\n            to_delete.append(subentity)\n            get_cleanup_entities(subentity, rels_seen=rels_seen,\n                    to_delete=to_delete, to_put=to_put)", "path": "common\\appenginepatch\\ragendja\\dbutils.py", "repo_name": "hokten/gae-site", "stars": 1, "license": "None", "language": "python", "size": 3456}
{"docstring": "\"\"\"\nVerifiy that the values entered into the two password fields\nmatch. Note that an error here will end up in\n``non_field_errors()`` because it doesn't apply to a single\nfield.\n\n\"\"\"\n", "func_signal": "def clean(self):\n", "code": "if 'password1' in self.cleaned_data and 'password2' in self.cleaned_data:\n    if self.cleaned_data['password1'] != self.cleaned_data['password2']:\n        raise forms.ValidationError(__(u'You must type the same password each time'))\nreturn self.cleaned_data", "path": "myapp\\forms.py", "repo_name": "hokten/gae-site", "stars": 1, "license": "None", "language": "python", "size": 3456}
{"docstring": "\"\"\"\nValidate that the supplied email address is unique for the\nsite.\n\n\"\"\"\n", "func_signal": "def clean_email(self):\n", "code": "email = self.cleaned_data['email'].lower()\nif User.all().filter('email =', email).filter(\n        'is_active =', True).count(1):\n    raise forms.ValidationError(__(u'This email address is already in use. Please supply a different email address.'))\nreturn email", "path": "myapp\\forms.py", "repo_name": "hokten/gae-site", "stars": 1, "license": "None", "language": "python", "size": 3456}
{"docstring": "\"\"\"Helper method for get_xxx_or_404.\"\"\"\n", "func_signal": "def get_filtered(data, *filters):\n", "code": "for filter in get_filters(*filters):\n    data.filter(*filter)\nreturn data", "path": "common\\appenginepatch\\ragendja\\dbutils.py", "repo_name": "hokten/gae-site", "stars": 1, "license": "None", "language": "python", "size": 3456}
{"docstring": "\"\"\"Like getattr(), but can go down a hierarchy like 'attr.subattr'\"\"\"\n", "func_signal": "def getattr_by_path(obj, attr, *default):\n", "code": "value = obj\nfor part in attr.split('.'):\n    if not hasattr(value, part) and len(default):\n        return default[0]\n    value = getattr(value, part)\n    if callable(value):\n        value = value()\nreturn value", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "hokten/gae-site", "stars": 1, "license": "None", "language": "python", "size": 3456}
{"docstring": "\"\"\"Creates a class-wide instance property with a thread-specific value.\"\"\"\n", "func_signal": "def make_tls_property(default=None):\n", "code": "class TLSProperty(object):\n    def __init__(self):\n        self.local = local()\n\n    def __get__(self, instance, cls):\n        if not instance:\n            return self\n        return self.value\n\n    def __set__(self, instance, value):\n        self.value = value\n\n    def _get_value(self):\n        return getattr(self.local, 'value', default)\n    def _set_value(self, value):\n        self.local.value = value\n    value = property(_get_value, _set_value)\n\nreturn TLSProperty()", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "hokten/gae-site", "stars": 1, "license": "None", "language": "python", "size": 3456}
{"docstring": "\"\"\"Returns a subset of the keys of a dictionary.\"\"\"\n", "func_signal": "def subdict(data, *attrs):\n", "code": "result = {}\nresult.update([(key, data[key]) for key in attrs])\nreturn result", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "hokten/gae-site", "stars": 1, "license": "None", "language": "python", "size": 3456}
{"docstring": "# Remove modules that we want to override\n", "func_signal": "def patch_python():\n", "code": "for module in ('memcache',):\n    if module in sys.modules:\n        del sys.modules[module]\n\n# For some reason the imp module can't be replaced via sys.path\nfrom appenginepatcher import have_appserver\nif have_appserver:\n    from appenginepatcher import imp\n    sys.modules['imp'] = imp\n\nif have_appserver:\n    def unlink(_):\n        raise NotImplementedError('App Engine does not support FS writes!')\n    os.unlink = unlink", "path": "common\\appenginepatch\\appenginepatcher\\patch.py", "repo_name": "hokten/gae-site", "stars": 1, "license": "None", "language": "python", "size": 3456}
{"docstring": "\"\"\"\nConverts objects to table-style list of rows with heading:\n\nExample:\nx.a = 1\nx.b = 2\nx.c = 3\ny.a = 11\ny.b = 12\ny.c = 13\nobject_list_to_table(('a', 'b', 'c'), [x, y])\nresults in the following (dict keys reordered for better readability):\n[\n    ('a', 'b', 'c'),\n    (1, 2, 3),\n    (11, 12, 13),\n]\n\"\"\"\n", "func_signal": "def object_list_to_table(headings, dict_list):\n", "code": "return [headings] + [tuple([getattr_by_path(row, heading, None)\n                            for heading in headings])\n                     for row in dict_list]", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "hokten/gae-site", "stars": 1, "license": "None", "language": "python", "size": 3456}
{"docstring": "\"\"\"\nThis function creates an object transactionally if it does not exist in\nthe datastore. Otherwise it returns None.\n\"\"\"\n", "func_signal": "def db_add(model, key_name, parent=None, **kwargs):\n", "code": "existing = model.get_by_key_name(key_name, parent=parent)\nif not existing:\n    new_entity = model(parent=parent, key_name=key_name, **kwargs)\n    new_entity.put()\n    return new_entity\nreturn None", "path": "common\\appenginepatch\\ragendja\\dbutils.py", "repo_name": "hokten/gae-site", "stars": 1, "license": "None", "language": "python", "size": 3456}
{"docstring": "\"\"\"Compares self to other using the abstract representations.\n\nThis is not like the standard compare, which use their numerical\nvalue. Note that a total ordering is defined for all possible abstract\nrepresentations.\n\"\"\"\n# if one is negative and the other is positive, it's easy\n", "func_signal": "def compare_total(self, other):\n", "code": "if self._sign and not other._sign:\n    return Dec_n1\nif not self._sign and other._sign:\n    return Dec_p1\nsign = self._sign\n\n# let's handle both NaN types\nself_nan = self._isnan()\nother_nan = other._isnan()\nif self_nan or other_nan:\n    if self_nan == other_nan:\n        if self._int < other._int:\n            if sign:\n                return Dec_p1\n            else:\n                return Dec_n1\n        if self._int > other._int:\n            if sign:\n                return Dec_n1\n            else:\n                return Dec_p1\n        return Dec_0\n\n    if sign:\n        if self_nan == 1:\n            return Dec_n1\n        if other_nan == 1:\n            return Dec_p1\n        if self_nan == 2:\n            return Dec_n1\n        if other_nan == 2:\n            return Dec_p1\n    else:\n        if self_nan == 1:\n            return Dec_p1\n        if other_nan == 1:\n            return Dec_n1\n        if self_nan == 2:\n            return Dec_p1\n        if other_nan == 2:\n            return Dec_n1\n\nif self < other:\n    return Dec_n1\nif self > other:\n    return Dec_p1\n\nif self._exp < other._exp:\n    if sign:\n        return Dec_p1\n    else:\n        return Dec_n1\nif self._exp > other._exp:\n    if sign:\n        return Dec_n1\n    else:\n        return Dec_p1\nreturn Dec_0", "path": "lib\\decimal.py", "repo_name": "aidanf/SimpleICM", "stars": 1, "license": "other", "language": "python", "size": 3896}
{"docstring": "\"\"\"Returns a rotated copy of self, value-of-other times.\"\"\"\n", "func_signal": "def rotate(self, other, context=None):\n", "code": "if context is None:\n    context = getcontext()\n\nans = self._check_nans(other, context)\nif ans:\n    return ans\n\nif other._exp != 0:\n    return context._raise_error(InvalidOperation)\nif not (-context.prec <= int(other) <= context.prec):\n    return context._raise_error(InvalidOperation)\n\nif self._isinfinity():\n    return Decimal(self)\n\n# get values, pad if necessary\ntorot = int(other)\nrotdig = self._int\ntopad = context.prec - len(rotdig)\nif topad:\n    rotdig = '0'*topad + rotdig\n\n# let's rotate!\nrotated = rotdig[torot:] + rotdig[:torot]\nreturn _dec_from_triple(self._sign,\n                        rotated.lstrip('0') or '0', self._exp)", "path": "lib\\decimal.py", "repo_name": "aidanf/SimpleICM", "stars": 1, "license": "other", "language": "python", "size": 3896}
{"docstring": "\"\"\"Normalize- strip trailing 0s, change anything equal to 0 to 0e0\"\"\"\n\n", "func_signal": "def normalize(self, context=None):\n", "code": "if context is None:\n    context = getcontext()\n\nif self._is_special:\n    ans = self._check_nans(context=context)\n    if ans:\n        return ans\n\ndup = self._fix(context)\nif dup._isinfinity():\n    return dup\n\nif not dup:\n    return _dec_from_triple(dup._sign, '0', 0)\nexp_max = [context.Emax, context.Etop()][context._clamp]\nend = len(dup._int)\nexp = dup._exp\nwhile dup._int[end-1] == '0' and exp < exp_max:\n    exp += 1\n    end -= 1\nreturn _dec_from_triple(dup._sign, dup._int[:end], exp)", "path": "lib\\decimal.py", "repo_name": "aidanf/SimpleICM", "stars": 1, "license": "other", "language": "python", "size": 3896}
{"docstring": "\"\"\"Returns an indication of the class of self.\n\nThe class is one of the following strings:\n  sNaN\n  NaN\n  -Infinity\n  -Normal\n  -Subnormal\n  -Zero\n  +Zero\n  +Subnormal\n  +Normal\n  +Infinity\n\"\"\"\n", "func_signal": "def number_class(self, context=None):\n", "code": "if self.is_snan():\n    return \"sNaN\"\nif self.is_qnan():\n    return \"NaN\"\ninf = self._isinfinity()\nif inf == 1:\n    return \"+Infinity\"\nif inf == -1:\n    return \"-Infinity\"\nif self.is_zero():\n    if self._sign:\n        return \"-Zero\"\n    else:\n        return \"+Zero\"\nif context is None:\n    context = getcontext()\nif self.is_subnormal(context=context):\n    if self._sign:\n        return \"-Subnormal\"\n    else:\n        return \"+Subnormal\"\n# just a normal, regular, boring number, :)\nif self._sign:\n    return \"-Normal\"\nelse:\n    return \"+Normal\"", "path": "lib\\decimal.py", "repo_name": "aidanf/SimpleICM", "stars": 1, "license": "other", "language": "python", "size": 3896}
{"docstring": "\"\"\"Returns a copy with the sign inverted.\"\"\"\n", "func_signal": "def copy_negate(self):\n", "code": "if self._sign:\n    return _dec_from_triple(0, self._int, self._exp, self._is_special)\nelse:\n    return _dec_from_triple(1, self._int, self._exp, self._is_special)", "path": "lib\\decimal.py", "repo_name": "aidanf/SimpleICM", "stars": 1, "license": "other", "language": "python", "size": 3896}
{"docstring": "\"\"\"Return self / other.\"\"\"\n", "func_signal": "def __div__(self, other, context=None):\n", "code": "other = _convert_other(other)\nif other is NotImplemented:\n    return NotImplemented\n\nif context is None:\n    context = getcontext()\n\nsign = self._sign ^ other._sign\n\nif self._is_special or other._is_special:\n    ans = self._check_nans(other, context)\n    if ans:\n        return ans\n\n    if self._isinfinity() and other._isinfinity():\n        return context._raise_error(InvalidOperation, '(+-)INF/(+-)INF')\n\n    if self._isinfinity():\n        return Infsign[sign]\n\n    if other._isinfinity():\n        context._raise_error(Clamped, 'Division by infinity')\n        return _dec_from_triple(sign, '0', context.Etiny())\n\n# Special cases for zeroes\nif not other:\n    if not self:\n        return context._raise_error(DivisionUndefined, '0 / 0')\n    return context._raise_error(DivisionByZero, 'x / 0', sign)\n\nif not self:\n    exp = self._exp - other._exp\n    coeff = 0\nelse:\n    # OK, so neither = 0, INF or NaN\n    shift = len(other._int) - len(self._int) + context.prec + 1\n    exp = self._exp - other._exp - shift\n    op1 = _WorkRep(self)\n    op2 = _WorkRep(other)\n    if shift >= 0:\n        coeff, remainder = divmod(op1.int * 10**shift, op2.int)\n    else:\n        coeff, remainder = divmod(op1.int, op2.int * 10**-shift)\n    if remainder:\n        # result is not exact; adjust to ensure correct rounding\n        if coeff % 5 == 0:\n            coeff += 1\n    else:\n        # result is exact; get as close to ideal exponent as possible\n        ideal_exp = self._exp - other._exp\n        while exp < ideal_exp and coeff % 10 == 0:\n            coeff //= 10\n            exp += 1\n\nans = _dec_from_triple(sign, str(coeff), exp)\nreturn ans._fix(context)", "path": "lib\\decimal.py", "repo_name": "aidanf/SimpleICM", "stars": 1, "license": "other", "language": "python", "size": 3896}
{"docstring": "\"\"\"Swaps self/other and returns __floordiv__.\"\"\"\n", "func_signal": "def __rfloordiv__(self, other, context=None):\n", "code": "other = _convert_other(other)\nif other is NotImplemented:\n    return other\nreturn other.__floordiv__(self, context=context)", "path": "lib\\decimal.py", "repo_name": "aidanf/SimpleICM", "stars": 1, "license": "other", "language": "python", "size": 3896}
{"docstring": "\"\"\"Returns a copy with the sign switched.\n\nRounds, if it has reason.\n\"\"\"\n", "func_signal": "def __neg__(self, context=None):\n", "code": "if self._is_special:\n    ans = self._check_nans(context=context)\n    if ans:\n        return ans\n\nif not self:\n    # -Decimal('0') is Decimal('0'), not Decimal('-0')\n    ans = self.copy_abs()\nelse:\n    ans = self.copy_negate()\n\nif context is None:\n    context = getcontext()\nreturn ans._fix(context)", "path": "lib\\decimal.py", "repo_name": "aidanf/SimpleICM", "stars": 1, "license": "other", "language": "python", "size": 3896}
{"docstring": "\"\"\"Compute a lower bound for the adjusted exponent of self.log10().\nIn other words, find r such that self.log10() >= 10**r.\nAssumes that self is finite and positive and that self != 1.\n\"\"\"\n\n# For x >= 10 or x < 0.1 we only need a bound on the integer\n# part of log10(self), and this comes directly from the\n# exponent of x.  For 0.1 <= x <= 10 we use the inequalities\n# 1-1/x <= log(x) <= x-1. If x > 1 we have |log10(x)| >\n# (1-1/x)/2.31 > 0.  If x < 1 then |log10(x)| > (1-x)/2.31 > 0\n\n", "func_signal": "def _log10_exp_bound(self):\n", "code": "adj = self._exp + len(self._int) - 1\nif adj >= 1:\n    # self >= 10\n    return len(str(adj))-1\nif adj <= -2:\n    # self < 0.1\n    return len(str(-1-adj))-1\nop = _WorkRep(self)\nc, e = op.int, op.exp\nif adj == 0:\n    # 1 < self < 10\n    num = str(c-10**-e)\n    den = str(231*c)\n    return len(num) - len(den) - (num < den) + 2\n# adj == -1, 0.1 <= self < 1\nnum = str(10**-e-c)\nreturn len(num) + e - (num < \"231\") - 1", "path": "lib\\decimal.py", "repo_name": "aidanf/SimpleICM", "stars": 1, "license": "other", "language": "python", "size": 3896}
{"docstring": "\"\"\"Return self ** other [ % modulo].\n\nWith two arguments, compute self**other.\n\nWith three arguments, compute (self**other) % modulo.  For the\nthree argument form, the following restrictions on the\narguments hold:\n\n - all three arguments must be integral\n - other must be nonnegative\n - either self or other (or both) must be nonzero\n - modulo must be nonzero and must have at most p digits,\n   where p is the context precision.\n\nIf any of these restrictions is violated the InvalidOperation\nflag is raised.\n\nThe result of pow(self, other, modulo) is identical to the\nresult that would be obtained by computing (self**other) %\nmodulo with unbounded precision, but is computed more\nefficiently.  It is always exact.\n\"\"\"\n\n", "func_signal": "def __pow__(self, other, modulo=None, context=None):\n", "code": "if modulo is not None:\n    return self._power_modulo(other, modulo, context)\n\nother = _convert_other(other)\nif other is NotImplemented:\n    return other\n\nif context is None:\n    context = getcontext()\n\n# either argument is a NaN => result is NaN\nans = self._check_nans(other, context)\nif ans:\n    return ans\n\n# 0**0 = NaN (!), x**0 = 1 for nonzero x (including +/-Infinity)\nif not other:\n    if not self:\n        return context._raise_error(InvalidOperation, '0 ** 0')\n    else:\n        return Dec_p1\n\n# result has sign 1 iff self._sign is 1 and other is an odd integer\nresult_sign = 0\nif self._sign == 1:\n    if other._isinteger():\n        if not other._iseven():\n            result_sign = 1\n    else:\n        # -ve**noninteger = NaN\n        # (-0)**noninteger = 0**noninteger\n        if self:\n            return context._raise_error(InvalidOperation,\n                'x ** y with x negative and y not an integer')\n    # negate self, without doing any unwanted rounding\n    self = self.copy_negate()\n\n# 0**(+ve or Inf)= 0; 0**(-ve or -Inf) = Infinity\nif not self:\n    if other._sign == 0:\n        return _dec_from_triple(result_sign, '0', 0)\n    else:\n        return Infsign[result_sign]\n\n# Inf**(+ve or Inf) = Inf; Inf**(-ve or -Inf) = 0\nif self._isinfinity():\n    if other._sign == 0:\n        return Infsign[result_sign]\n    else:\n        return _dec_from_triple(result_sign, '0', 0)\n\n# 1**other = 1, but the choice of exponent and the flags\n# depend on the exponent of self, and on whether other is a\n# positive integer, a negative integer, or neither\nif self == Dec_p1:\n    if other._isinteger():\n        # exp = max(self._exp*max(int(other), 0),\n        # 1-context.prec) but evaluating int(other) directly\n        # is dangerous until we know other is small (other\n        # could be 1e999999999)\n        if other._sign == 1:\n            multiplier = 0\n        elif other > context.prec:\n            multiplier = context.prec\n        else:\n            multiplier = int(other)\n\n        exp = self._exp * multiplier\n        if exp < 1-context.prec:\n            exp = 1-context.prec\n            context._raise_error(Rounded)\n    else:\n        context._raise_error(Inexact)\n        context._raise_error(Rounded)\n        exp = 1-context.prec\n\n    return _dec_from_triple(result_sign, '1'+'0'*-exp, exp)\n\n# compute adjusted exponent of self\nself_adj = self.adjusted()\n\n# self ** infinity is infinity if self > 1, 0 if self < 1\n# self ** -infinity is infinity if self < 1, 0 if self > 1\nif other._isinfinity():\n    if (other._sign == 0) == (self_adj < 0):\n        return _dec_from_triple(result_sign, '0', 0)\n    else:\n        return Infsign[result_sign]\n\n# from here on, the result always goes through the call\n# to _fix at the end of this function.\nans = None\n\n# crude test to catch cases of extreme overflow/underflow.  If\n# log10(self)*other >= 10**bound and bound >= len(str(Emax))\n# then 10**bound >= 10**len(str(Emax)) >= Emax+1 and hence\n# self**other >= 10**(Emax+1), so overflow occurs.  The test\n# for underflow is similar.\nbound = self._log10_exp_bound() + other.adjusted()\nif (self_adj >= 0) == (other._sign == 0):\n    # self > 1 and other +ve, or self < 1 and other -ve\n    # possibility of overflow\n    if bound >= len(str(context.Emax)):\n        ans = _dec_from_triple(result_sign, '1', context.Emax+1)\nelse:\n    # self > 1 and other -ve, or self < 1 and other +ve\n    # possibility of underflow to 0\n    Etiny = context.Etiny()\n    if bound >= len(str(-Etiny)):\n        ans = _dec_from_triple(result_sign, '1', Etiny-1)\n\n# try for an exact result with precision +1\nif ans is None:\n    ans = self._power_exact(other, context.prec + 1)\n    if ans is not None and result_sign == 1:\n        ans = _dec_from_triple(1, ans._int, ans._exp)\n\n# usual case: inexact result, x**y computed directly as exp(y*log(x))\nif ans is None:\n    p = context.prec\n    x = _WorkRep(self)\n    xc, xe = x.int, x.exp\n    y = _WorkRep(other)\n    yc, ye = y.int, y.exp\n    if y.sign == 1:\n        yc = -yc\n\n    # compute correctly rounded result:  start with precision +3,\n    # then increase precision until result is unambiguously roundable\n    extra = 3\n    while True:\n        coeff, exp = _dpower(xc, xe, yc, ye, p+extra)\n        if coeff % (5*10**(len(str(coeff))-p-1)):\n            break\n        extra += 3\n\n    ans = _dec_from_triple(result_sign, str(coeff), exp)\n\n# the specification says that for non-integer other we need to\n# raise Inexact, even when the result is actually exact.  In\n# the same way, we need to raise Underflow here if the result\n# is subnormal.  (The call to _fix will take care of raising\n# Rounded and Subnormal, as usual.)\nif not other._isinteger():\n    context._raise_error(Inexact)\n    # pad with zeros up to length context.prec+1 if necessary\n    if len(ans._int) <= context.prec:\n        expdiff = context.prec+1 - len(ans._int)\n        ans = _dec_from_triple(ans._sign, ans._int+'0'*expdiff,\n                               ans._exp-expdiff)\n    if ans.adjusted() < context.Emin:\n        context._raise_error(Underflow)\n\n# unlike exp, ln and log10, the power function respects the\n# rounding mode; no need to use ROUND_HALF_EVEN here\nans = ans._fix(context)\nreturn ans", "path": "lib\\decimal.py", "repo_name": "aidanf/SimpleICM", "stars": 1, "license": "other", "language": "python", "size": 3896}
{"docstring": "\"\"\"Returns whether the number is not actually one.\n\n0 if a number\n1 if NaN\n2 if sNaN\n\"\"\"\n", "func_signal": "def _isnan(self):\n", "code": "if self._is_special:\n    exp = self._exp\n    if exp == 'n':\n        return 1\n    elif exp == 'N':\n        return 2\nreturn 0", "path": "lib\\decimal.py", "repo_name": "aidanf/SimpleICM", "stars": 1, "license": "other", "language": "python", "size": 3896}
{"docstring": "\"\"\"Compares one to another.\n\n-1 => a < b\n0  => a = b\n1  => a > b\nNaN => one is NaN\nLike __cmp__, but returns Decimal instances.\n\"\"\"\n", "func_signal": "def compare(self, other, context=None):\n", "code": "other = _convert_other(other, raiseit=True)\n\n# Compare(NaN, NaN) = NaN\nif (self._is_special or other and other._is_special):\n    ans = self._check_nans(other, context)\n    if ans:\n        return ans\n\nreturn Decimal(self.__cmp__(other))", "path": "lib\\decimal.py", "repo_name": "aidanf/SimpleICM", "stars": 1, "license": "other", "language": "python", "size": 3896}
{"docstring": "\"\"\"Quantize self so its exponent is the same as that of exp.\n\nSimilar to self._rescale(exp._exp) but with error checking.\n\"\"\"\n", "func_signal": "def quantize(self, exp, rounding=None, context=None, watchexp=True):\n", "code": "exp = _convert_other(exp, raiseit=True)\n\nif context is None:\n    context = getcontext()\nif rounding is None:\n    rounding = context.rounding\n\nif self._is_special or exp._is_special:\n    ans = self._check_nans(exp, context)\n    if ans:\n        return ans\n\n    if exp._isinfinity() or self._isinfinity():\n        if exp._isinfinity() and self._isinfinity():\n            return Decimal(self)  # if both are inf, it is OK\n        return context._raise_error(InvalidOperation,\n                                'quantize with one INF')\n\n# if we're not watching exponents, do a simple rescale\nif not watchexp:\n    ans = self._rescale(exp._exp, rounding)\n    # raise Inexact and Rounded where appropriate\n    if ans._exp > self._exp:\n        context._raise_error(Rounded)\n        if ans != self:\n            context._raise_error(Inexact)\n    return ans\n\n# exp._exp should be between Etiny and Emax\nif not (context.Etiny() <= exp._exp <= context.Emax):\n    return context._raise_error(InvalidOperation,\n           'target exponent out of bounds in quantize')\n\nif not self:\n    ans = _dec_from_triple(self._sign, '0', exp._exp)\n    return ans._fix(context)\n\nself_adjusted = self.adjusted()\nif self_adjusted > context.Emax:\n    return context._raise_error(InvalidOperation,\n                                'exponent of quantize result too large for current context')\nif self_adjusted - exp._exp + 1 > context.prec:\n    return context._raise_error(InvalidOperation,\n                                'quantize result has too many digits for current context')\n\nans = self._rescale(exp._exp, rounding)\nif ans.adjusted() > context.Emax:\n    return context._raise_error(InvalidOperation,\n                                'exponent of quantize result too large for current context')\nif len(ans._int) > context.prec:\n    return context._raise_error(InvalidOperation,\n                                'quantize result has too many digits for current context')\n\n# raise appropriate flags\nif ans._exp > self._exp:\n    context._raise_error(Rounded)\n    if ans != self:\n        context._raise_error(Inexact)\nif ans and ans.adjusted() < context.Emin:\n    context._raise_error(Subnormal)\n\n# call to fix takes care of any necessary folddown\nans = ans._fix(context)\nreturn ans", "path": "lib\\decimal.py", "repo_name": "aidanf/SimpleICM", "stars": 1, "license": "other", "language": "python", "size": 3896}
{"docstring": "\"\"\"Returns the natural (base e) logarithm of self.\"\"\"\n\n", "func_signal": "def ln(self, context=None):\n", "code": "if context is None:\n    context = getcontext()\n\n# ln(NaN) = NaN\nans = self._check_nans(context=context)\nif ans:\n    return ans\n\n# ln(0.0) == -Infinity\nif not self:\n    return negInf\n\n# ln(Infinity) = Infinity\nif self._isinfinity() == 1:\n    return Inf\n\n# ln(1.0) == 0.0\nif self == Dec_p1:\n    return Dec_0\n\n# ln(negative) raises InvalidOperation\nif self._sign == 1:\n    return context._raise_error(InvalidOperation,\n                                'ln of a negative value')\n\n# result is irrational, so necessarily inexact\nop = _WorkRep(self)\nc, e = op.int, op.exp\np = context.prec\n\n# correctly rounded result: repeatedly increase precision by 3\n# until we get an unambiguously roundable result\nplaces = p - self._ln_exp_bound() + 2 # at least p+3 places\nwhile True:\n    coeff = _dlog(c, e, places)\n    # assert len(str(abs(coeff)))-p >= 1\n    if coeff % (5*10**(len(str(abs(coeff)))-p-1)):\n        break\n    places += 3\nans = _dec_from_triple(int(coeff<0), str(abs(coeff)), -places)\n\ncontext = context._shallow_copy()\nrounding = context._set_rounding(ROUND_HALF_EVEN)\nans = ans._fix(context)\ncontext.rounding = rounding\nreturn ans", "path": "lib\\decimal.py", "repo_name": "aidanf/SimpleICM", "stars": 1, "license": "other", "language": "python", "size": 3896}
{"docstring": "\"\"\"Compares the values numerically with their sign ignored.\"\"\"\n", "func_signal": "def max_mag(self, other, context=None):\n", "code": "other = _convert_other(other, raiseit=True)\n\nif context is None:\n    context = getcontext()\n\nif self._is_special or other._is_special:\n    # If one operand is a quiet NaN and the other is number, then the\n    # number is always returned\n    sn = self._isnan()\n    on = other._isnan()\n    if sn or on:\n        if on == 1 and sn != 2:\n            return self._fix_nan(context)\n        if sn == 1 and on != 2:\n            return other._fix_nan(context)\n        return self._check_nans(other, context)\n\nc = self.copy_abs().__cmp__(other.copy_abs())\nif c == 0:\n    c = self.compare_total(other)\n\nif c == -1:\n    ans = other\nelse:\n    ans = self\n\nreturn ans._fix(context)", "path": "lib\\decimal.py", "repo_name": "aidanf/SimpleICM", "stars": 1, "license": "other", "language": "python", "size": 3896}
{"docstring": "\"\"\"Given integers c, e and p with c > 0, p >= 0, compute an integer\napproximation to 10**p * log10(c*10**e), with an absolute error of\nat most 1.  Assumes that c*10**e is not exactly 1.\"\"\"\n\n# increase precision by 2; compensate for this by dividing\n# final result by 100\n", "func_signal": "def _dlog10(c, e, p):\n", "code": "p += 2\n\n# write c*10**e as d*10**f with either:\n#   f >= 0 and 1 <= d <= 10, or\n#   f <= 0 and 0.1 <= d <= 1.\n# Thus for c*10**e close to 1, f = 0\nl = len(str(c))\nf = e+l - (e+l >= 1)\n\nif p > 0:\n    M = 10**p\n    k = e+p-f\n    if k >= 0:\n        c *= 10**k\n    else:\n        c = _div_nearest(c, 10**-k)\n\n    log_d = _ilog(c, M) # error < 5 + 22 = 27\n    log_10 = _log10_digits(p) # error < 1\n    log_d = _div_nearest(log_d*M, log_10)\n    log_tenpower = f*M # exact\nelse:\n    log_d = 0  # error < 2.31\n    log_tenpower = div_nearest(f, 10**-p) # error < 0.5\n\nreturn _div_nearest(log_tenpower+log_d, 100)", "path": "lib\\decimal.py", "repo_name": "aidanf/SimpleICM", "stars": 1, "license": "other", "language": "python", "size": 3896}
{"docstring": "\"\"\"Return True if self and other have the same exponent; otherwise\nreturn False.\n\nIf either operand is a special value, the following rules are used:\n   * return True if both operands are infinities\n   * return True if both operands are NaNs\n   * otherwise, return False.\n\"\"\"\n", "func_signal": "def same_quantum(self, other):\n", "code": "other = _convert_other(other, raiseit=True)\nif self._is_special or other._is_special:\n    return (self.is_nan() and other.is_nan() or\n            self.is_infinite() and other.is_infinite())\nreturn self._exp == other._exp", "path": "lib\\decimal.py", "repo_name": "aidanf/SimpleICM", "stars": 1, "license": "other", "language": "python", "size": 3896}
{"docstring": "\"\"\"Returns the largest representable number smaller than itself.\"\"\"\n", "func_signal": "def next_minus(self, context=None):\n", "code": "if context is None:\n    context = getcontext()\n\nans = self._check_nans(context=context)\nif ans:\n    return ans\n\nif self._isinfinity() == -1:\n    return negInf\nif self._isinfinity() == 1:\n    return _dec_from_triple(0, '9'*context.prec, context.Etop())\n\ncontext = context.copy()\ncontext._set_rounding(ROUND_FLOOR)\ncontext._ignore_all_flags()\nnew_self = self._fix(context)\nif new_self != self:\n    return new_self\nreturn self.__sub__(_dec_from_triple(0, '1', context.Etiny()-1),\n                    context)", "path": "lib\\decimal.py", "repo_name": "aidanf/SimpleICM", "stars": 1, "license": "other", "language": "python", "size": 3896}
{"docstring": "\"\"\"Returns the smaller value.\n\nLike min(self, other) except if one is not a number, returns\nNaN (and signals if one is sNaN).  Also rounds.\n\"\"\"\n", "func_signal": "def min(self, other, context=None):\n", "code": "other = _convert_other(other, raiseit=True)\n\nif context is None:\n    context = getcontext()\n\nif self._is_special or other._is_special:\n    # If one operand is a quiet NaN and the other is number, then the\n    # number is always returned\n    sn = self._isnan()\n    on = other._isnan()\n    if sn or on:\n        if on == 1 and sn != 2:\n            return self._fix_nan(context)\n        if sn == 1 and on != 2:\n            return other._fix_nan(context)\n        return self._check_nans(other, context)\n\nc = self.__cmp__(other)\nif c == 0:\n    c = self.compare_total(other)\n\nif c == -1:\n    ans = self\nelse:\n    ans = other\n\nreturn ans._fix(context)", "path": "lib\\decimal.py", "repo_name": "aidanf/SimpleICM", "stars": 1, "license": "other", "language": "python", "size": 3896}
{"docstring": "\"\"\"Invert all its digits.\"\"\"\n", "func_signal": "def logical_invert(self, context=None):\n", "code": "if context is None:\n    context = getcontext()\nreturn self.logical_xor(_dec_from_triple(0,'1'*context.prec,0),\n                        context)", "path": "lib\\decimal.py", "repo_name": "aidanf/SimpleICM", "stars": 1, "license": "other", "language": "python", "size": 3896}
{"docstring": "\"\"\"Get session statistics\"\"\"\n", "func_signal": "def session_stats(self):\n", "code": "self._request('session-stats')\nreturn self.session", "path": "lib\\transmissionrpc.py", "repo_name": "brunomlopes/transmission-helpers", "stars": 0, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"\nSet file properties. Takes a dictonary with similar contents as the\nresult of get_files.\n\"\"\"\n", "func_signal": "def set_files(self, items):\n", "code": "if not isinstance(items, dict):\n    raise ValueError('Invalid file description')\nfor tid, files in items.iteritems():\n    if not isinstance(files, dict):\n        continue\n    wanted = []\n    unwanted = []\n    priority_high = []\n    priority_normal = []\n    priority_low = []\n    for fid, file in files.iteritems():\n        if not isinstance(file, dict):\n            continue\n        if 'selected' in file and file['selected']:\n            wanted.append(fid)\n        else:\n            unwanted.append(fid)\n        if 'priority' in file:\n            if file['priority'] == 'high':\n                priority_high.append(fid)\n            elif file['priority'] == 'normal':\n                priority_normal.append(fid)\n            elif file['priority'] == 'low':\n                priority_low.append(fid)\n    self.change([tid], files_wanted = wanted\n                , files_unwanted = unwanted\n                , priority_high = priority_high\n                , priority_normal = priority_normal\n                , priority_low = priority_low)", "path": "lib\\transmissionrpc.py", "repo_name": "brunomlopes/transmission-helpers", "stars": 0, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"\nTests to see if your incoming peer port is accessible from the\noutside world.\n\"\"\"\n", "func_signal": "def port_test(self):\n", "code": "self._rpc_version_warning(5)\nresult = self._request('port-test')\nif 'port-is-open' in result:\n    return result['port-is-open']\nreturn None", "path": "lib\\transmissionrpc.py", "repo_name": "brunomlopes/transmission-helpers", "stars": 0, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Returns the attribute \"eta\" formatted as a string.\"\"\"\n", "func_signal": "def format_eta(self):\n", "code": "eta = self.fields['eta']\nif eta == -1:\n    return 'not available'\nelif eta == -2:\n    return 'unknown'\nelse:\n    return format_timedelta(self.eta)", "path": "lib\\transmissionrpc.py", "repo_name": "brunomlopes/transmission-helpers", "stars": 0, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Get session parameters\"\"\"\n", "func_signal": "def get_session(self):\n", "code": "self._request('session-get')\nreturn self.session", "path": "lib\\transmissionrpc.py", "repo_name": "brunomlopes/transmission-helpers", "stars": 0, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"\nGet list of files for this torrent. This function returns a dictionary with file information for each file.\n\"\"\"\n", "func_signal": "def files(self):\n", "code": "result = {}\nif 'files' in self.fields:\n    indicies = xrange(len(self.fields['files']))\n    files = self.fields['files']\n    priorities = self.fields['priorities']\n    wanted = self.fields['wanted']\n    index = 1\n    for item in zip(indicies, files, priorities, wanted):\n        if item[3]:\n            selected = True\n        else:\n            selected = False\n        priority = PRIORITY[item[2]]\n        result[item[0]] = {\n            'selected': selected,\n            'priority': priority,\n            'size': item[1]['length'],\n            'name': item[1]['name'],\n            'completed': item[1]['bytesCompleted']}\nreturn result", "path": "lib\\transmissionrpc.py", "repo_name": "brunomlopes/transmission-helpers", "stars": 0, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"list all torrents\"\"\"\n", "func_signal": "def list(self):\n", "code": "fields = ['id', 'hashString', 'name', 'sizeWhenDone', 'leftUntilDone'\n    , 'eta', 'status', 'rateUpload', 'rateDownload', 'uploadedEver'\n    , 'downloadedEver']\nreturn self._request('torrent-get', {'fields': fields})", "path": "lib\\transmissionrpc.py", "repo_name": "brunomlopes/transmission-helpers", "stars": 0, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"\nGet list of files for provided torrent id(s).\nThis function returns a dictonary for each requested torrent id holding\nthe information about the files.\n\"\"\"\n", "func_signal": "def get_files(self, ids=[]):\n", "code": "fields = ['id', 'name', 'hashString', 'files', 'priorities', 'wanted']\nrequest_result = self._request('torrent-get', {'fields': fields}, ids)\nresult = {}\nfor id, torrent in request_result.iteritems():\n    result[id] = torrent.files()\nreturn result", "path": "lib\\transmissionrpc.py", "repo_name": "brunomlopes/transmission-helpers", "stars": 0, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Get the upload/download ratio.\"\"\"\n", "func_signal": "def ratio(self):\n", "code": "try:\n    return self.fields['uploadedEver'] / float(self.fields['downloadedEver'])\nexcept ZeroDivisionError:\n    return 0.0", "path": "lib\\transmissionrpc.py", "repo_name": "brunomlopes/transmission-helpers", "stars": 0, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"\nChange torrent parameters. This is the list of parameters that.\n\"\"\"\n", "func_signal": "def change(self, ids, **kwargs):\n", "code": "args = {}\nfor key, value in kwargs.iteritems():\n    argument = make_rpc_name(key)\n    (arg, val) = argument_value_convert('torrent-set'\n                            , argument, value, self.rpc_version)\n    args[arg] = val\n\nif len(args) > 0:\n    self._request('torrent-set', args, ids, True)\nelse:\n    ValueError(\"No arguments to set\")", "path": "lib\\transmissionrpc.py", "repo_name": "brunomlopes/transmission-helpers", "stars": 0, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Update block list. Returns the size of the block list.\"\"\"\n", "func_signal": "def blocklist_update(self):\n", "code": "self._rpc_version_warning(5)\nresult = self._request('blocklist-update')\nif 'blocklist-size' in result:\n    return result['blocklist-size']\nreturn None", "path": "lib\\transmissionrpc.py", "repo_name": "brunomlopes/transmission-helpers", "stars": 0, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Take things and make them valid torrent identifiers\"\"\"\n", "func_signal": "def _format_ids(self, args):\n", "code": "ids = []\n\nif isinstance(args, (int, long)):\n    ids.append(args)\nelif isinstance(args, (str, unicode)):\n    for item in re.split(u'[ ,]+', args):\n        if len(item) == 0:\n            continue\n        addition = None\n        try:\n            # handle index\n            addition = [int(item)]\n        except ValueError:\n            pass\n        if not addition:\n            # handle hashes\n            try:\n                int(item, 16)\n                addition = [item]\n            except:\n                pass\n        if not addition:\n            # handle index ranges i.e. 5:10\n            match = re.match(u'^(\\d+):(\\d+)$', item)\n            if match:\n                try:\n                    idx_from = int(match.group(1))\n                    idx_to = int(match.group(2))\n                    addition = range(idx_from, idx_to + 1)\n                except:\n                    pass\n        if not addition:\n            raise ValueError(u'Invalid torrent id, \\\"%s\\\"' % item)\n        ids.extend(addition)\nelif isinstance(args, (list)):\n    for item in args:\n        ids.extend(self._format_ids(item))\nelse:\n    raise ValueError(u'Invalid torrent id')\nreturn ids", "path": "lib\\transmissionrpc.py", "repo_name": "brunomlopes/transmission-helpers", "stars": 0, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Set session parameters\"\"\"\n", "func_signal": "def set_session(self, **kwargs):\n", "code": "args = {}\nfor key, value in kwargs.iteritems():\n    if key == 'encryption' and value not in ['required', 'preferred', 'tolerated']:\n        raise ValueError('Invalid encryption value')\n    argument = make_rpc_name(key)\n    (arg, val) = argument_value_convert('session-set'\n                        , argument, value, self.rpc_version)\n    args[arg] = val\nif len(args) > 0:\n    self._request('session-set', args)", "path": "lib\\transmissionrpc.py", "repo_name": "brunomlopes/transmission-helpers", "stars": 0, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"reannounce torrent(s) with provided id(s)\"\"\"\n", "func_signal": "def reannounce(self, ids):\n", "code": "self._rpc_version_warning(5)\nself._request('torrent-reannounce', {}, ids, True)", "path": "lib\\transmissionrpc.py", "repo_name": "brunomlopes/transmission-helpers", "stars": 0, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Get the \"eta\" as datetime.timedelta.\"\"\"\n", "func_signal": "def eta(self):\n", "code": "eta = self.fields['eta']\nif eta >= 0:\n    return datetime.timedelta(seconds=eta)\nelse:\n    ValueError('eta not valid')", "path": "lib\\transmissionrpc.py", "repo_name": "brunomlopes/transmission-helpers", "stars": 0, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"\nremove torrent(s) with provided id(s). Local data is removed if\ndelete_data is True, otherwise not.\n\"\"\"\n", "func_signal": "def remove(self, ids, delete_data=False):\n", "code": "self._rpc_version_warning(3)\nself._request('torrent-remove',\n            {'delete-local-data':rpc_bool(delete_data)}, ids, True)", "path": "lib\\transmissionrpc.py", "repo_name": "brunomlopes/transmission-helpers", "stars": 0, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"\nAdd torrent to transfers list. Takes a url to a .torrent file.\nAdditional arguments are:\n\n    * `paused`, boolean, Whether to pause the transfer on add.\n    * `download_dir`, path, The directory where the downloaded\n      contents will be saved in.\n    * `peer_limit`, number, Limits the number of peers for this\n      transfer.\n    * `files_unwanted`,\n    * `files_wanted`,\n    * `priority_high`,\n    * `priority_low`,\n    * `priority_normal`,\n\"\"\"\n", "func_signal": "def add_url(self, torrent_url, **kwargs):\n", "code": "torrent_file = None\nif os.path.exists(torrent_url):\n    torrent_file = open(torrent_url, 'r')\nelse:\n    try:\n        torrent_file = urllib2.urlopen(torrent_url)\n    except:\n        torrent_file = None\n\nif not torrent_file:\n    raise TransmissionError('File does not exist.')\n\ntorrent_data = base64.b64encode(torrent_file.read())\nreturn self.add(torrent_data, **kwargs)", "path": "lib\\transmissionrpc.py", "repo_name": "brunomlopes/transmission-helpers", "stars": 0, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Update the torrent data from a Transmission arguments dictinary\"\"\"\n", "func_signal": "def update(self, other):\n", "code": "fields = None\nif isinstance(other, dict):\n    fields = other\nelif isinstance(other, Torrent):\n    fields = other.fields\nelse:\n    raise ValueError('Cannot update with supplied data')\nfor k, v in fields.iteritems():\n    self.fields[k.replace('-', '_')] = v", "path": "lib\\transmissionrpc.py", "repo_name": "brunomlopes/transmission-helpers", "stars": 0, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Get detailed information for torrent(s) with provided id(s).\"\"\"\n", "func_signal": "def info(self, ids=[], arguments={}):\n", "code": "if not arguments:\n    arguments = self.torrent_get_arguments\nreturn self._request('torrent-get', {'fields': arguments}, ids)", "path": "lib\\transmissionrpc.py", "repo_name": "brunomlopes/transmission-helpers", "stars": 0, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"Get the download progress in percent as float.\"\"\"\n", "func_signal": "def progress(self):\n", "code": "try:\n    return 100.0 * (self.fields['sizeWhenDone'] - self.fields['leftUntilDone']) / float(self.fields['sizeWhenDone'])\nexcept ZeroDivisionError:\n    return 0.0", "path": "lib\\transmissionrpc.py", "repo_name": "brunomlopes/transmission-helpers", "stars": 0, "license": "None", "language": "python", "size": 100}
{"docstring": "\"\"\"\nReturn debug info for the request\n\"\"\"\n", "func_signal": "def debug_info(request, context=None, evalex=True):\n", "code": "if context is None:\n    context = Namespace()\n\nreq_vars = []\nfor item in dir(request):\n    attr = getattr(request, item)\n    if not (item.startswith(\"_\") or inspect.isroutine(attr)):\n        req_vars.append((item, attr))\nreq_vars.sort()\n\ncontext.req_vars = req_vars\nreturn DebugRender(context, evalex).render()", "path": "colubrid\\debug.py", "repo_name": "w495/colubrid", "stars": 1, "license": "None", "language": "python", "size": 105}
{"docstring": "\"\"\"Return the default value if the requested data doesn't exist\"\"\"\n", "func_signal": "def get(self, key, default=None):\n", "code": "try:\n    val = self[key]\nexcept KeyError:\n    return default\nif val == []:\n    return default\nreturn val", "path": "colubrid\\utils.py", "repo_name": "w495/colubrid", "stars": 1, "license": "None", "language": "python", "size": 105}
{"docstring": "\"\"\"Spawn a new Python interpreter with the same arguments as this one,\nbut running the reloader thread.\"\"\"\n", "func_signal": "def restart_with_reloader():\n", "code": "while True:\n    args = [sys.executable] + sys.argv\n    if sys.platform == 'win32':\n        args = ['\"%s\"' % arg for arg in args]\n    new_environ = os.environ.copy()\n    new_environ['RUN_MAIN'] = 'true'\n    exit_code = os.spawnve(os.P_WAIT, sys.executable, args, new_environ)\n    if exit_code != 3:\n        return exit_code", "path": "colubrid\\reloader.py", "repo_name": "w495/colubrid", "stars": 1, "license": "None", "language": "python", "size": 105}
{"docstring": "\"\"\"Get the full URL of the requested page, including query string\n   if any. If append is given, use it as the pathname after the\n   host name.\"\"\"\n\n", "func_signal": "def get_full_url(environ, append=None):\n", "code": "if 'REQUEST_URI' in environ and append is None:\n    return environ['REQUEST_URI']\n\nurl = environ['wsgi.url_scheme']+'://'\nif environ.get('HTTP_HOST'):\n    url += environ['HTTP_HOST']\nelse:\n    url += environ['SERVER_NAME']\n    if environ['wsgi.url_scheme'] == 'https':\n        if environ['SERVER_PORT'] != '443':\n            url += ':' + environ['SERVER_PORT']\n    else:\n        if environ['SERVER_PORT'] != '80':\n            url += ':' + environ['SERVER_PORT']\n\nif append is None:\n    url += quote(environ.get('SCRIPT_NAME', ''))\n    url += quote(environ.get('PATH_INFO', ''))\n    if environ.get('QUERY_STRING'):\n        url += '?' + environ['QUERY_STRING']\nelse:\n    url += append\nreturn url", "path": "colubrid\\utils.py", "repo_name": "w495/colubrid", "stars": 1, "license": "None", "language": "python", "size": 105}
{"docstring": "\"\"\"Call this to initialize the reloader.\"\"\"\n", "func_signal": "def main(main_func, watch=[]):\n", "code": "if os.environ.get('RUN_MAIN') == 'true':\n    thread.start_new_thread(main_func, ())\n    try:\n        reloader_thread(watch)\n    except KeyboardInterrupt:\n        pass\nelse:\n    try:\n        sys.exit(restart_with_reloader())\n    except KeyboardInterrupt:\n        pass", "path": "colubrid\\reloader.py", "repo_name": "w495/colubrid", "stars": 1, "license": "None", "language": "python", "size": 105}
{"docstring": "# simple data types\n", "func_signal": "def var_table(self, var):\n", "code": "if isinstance(var, basestring) or isinstance(var, float)\\\n   or isinstance(var, int) or isinstance(var, long):\n    return ('<table class=\"vars\"><tr><td class=\"value\">%r'\n            '</td></tr></table>' % escape(repr(var)))\n\n# dicts\nif isinstance(var, dict) or hasattr(var, 'items'):\n    items = var.items()\n    items.sort()\n\n    # empty dict\n    if not items:\n        return ('<table class=\"vars\"><tr><th>no data given'\n                '</th></tr></table>')\n\n    result = ['<table class=\"vars\"><tr><th>Name'\n              '</th><th>Value</th></tr>']\n    for key, value in items:\n        try:\n            val = escape(pprint.pformat(value))\n        except:\n            val = '?'\n        result.append('<tr><td class=\"name\">%s</td><td class=\"value\">%s'\n                      '</td></tr>' % (escape(repr(key)), val))\n    result.append('</table>')\n    return '\\n'.join(result)\n\n# lists\nif isinstance(var, list):\n    # empty list\n    if not var:\n        return ('<table class=\"vars\"><tr><th>no data given'\n                '</th></tr></table>')\n\n    result = ['<table class=\"vars\">']\n    for line in var:\n        try:\n            val = escape(pprint.pformat(line))\n        except:\n            val = '?'\n        result.append('<tr><td class=\"value\">%s</td></tr>' % (val))\n    result.append('</table>')\n    return '\\n'.join(result)\n\n# unknown things\ntry:\n    value = escape(repr(var))\nexcept:\n    value = '?'\nreturn '<table class=\"vars\"><tr><th>%s</th></tr></table>' % value", "path": "colubrid\\debug.py", "repo_name": "w495/colubrid", "stars": 1, "license": "None", "language": "python", "size": 105}
{"docstring": "\"\"\"removes count header tuples from the list\nwhere key matches\n\"\"\"\n", "func_signal": "def remove(self, key, count=-1):\n", "code": "removed = 0\ndata = []\nfor _key, _value in self.data:\n    if _key.lower() != key.lower():\n        if count > -1:\n            if removed >= count:\n                break\n            else:\n                removed += 1\n        data.append((_key, _value))\nself.data = data", "path": "colubrid\\utils.py", "repo_name": "w495/colubrid", "stars": 1, "license": "None", "language": "python", "size": 105}
{"docstring": "\"\"\"Return an empty list if the requested data doesn't exist\"\"\"\n", "func_signal": "def getlist(self, key):\n", "code": "try:\n    return dict.__getitem__(self, key)\nexcept KeyError:\n    return []", "path": "colubrid\\utils.py", "repo_name": "w495/colubrid", "stars": 1, "license": "None", "language": "python", "size": 105}
{"docstring": "\"\"\"Returns a copy of this object.\"\"\"\n", "func_signal": "def copy(self):\n", "code": "import copy\nMultiDict.__setitem__ = dict.__setitem__\ncp = copy.deepcopy(self)\nMultiDict.__setitem__ = MultiDict._setitem_list\nreturn cp", "path": "colubrid\\utils.py", "repo_name": "w495/colubrid", "stars": 1, "license": "None", "language": "python", "size": 105}
{"docstring": "\"\"\"return the colubrid and python version.\"\"\"\n", "func_signal": "def get_version():\n", "code": "from colubrid import __version__\nfrom sys import version\nreturn '%s - Python %s' % (__version__, version.split('\\n')[0].strip())", "path": "colubrid\\utils.py", "repo_name": "w495/colubrid", "stars": 1, "license": "None", "language": "python", "size": 105}
{"docstring": "\"\"\"returns matching headers as list\n\nif httpformat is set the result is a HTTP\nheader formatted list.\n\"\"\"\n", "func_signal": "def get(self, key=False, httpformat=False):\n", "code": "if not key:\n    result = self.data\nelif not isinstance(key, basestring):\n    raise TypeError('keys have to be strings')\nelse:\n    result = []\n    for k, v in self.data:\n        if k.lower() == key.lower():\n            result.append((str(k), str(v)))\nif httpformat:\n    return '\\n'.join(['%s: %s' % item for item in result])\nreturn result", "path": "colubrid\\utils.py", "repo_name": "w495/colubrid", "stars": 1, "license": "None", "language": "python", "size": 105}
{"docstring": "\"\"\"remove all header tuples for key and add\na new one\n\"\"\"\n", "func_signal": "def set(self, key, value):\n", "code": "self.remove(key)\nself.add(key, value)", "path": "colubrid\\utils.py", "repo_name": "w495/colubrid", "stars": 1, "license": "None", "language": "python", "size": 105}
{"docstring": "\"\"\"\nReturn the last data value for this key, or [] if it's an empty list;\nraises KeyError if not found.\n\"\"\"\n", "func_signal": "def __getitem__(self, key):\n", "code": "list_ = dict.__getitem__(self, key)\ntry:\n    return list_[-1]\nexcept IndexError:\n    return []", "path": "colubrid\\utils.py", "repo_name": "w495/colubrid", "stars": 1, "license": "None", "language": "python", "size": 105}
{"docstring": "\"\"\"\nFixes the trailing slash in an url.\nIf the user requests an container object without an slash it\nwill appends one.\nRequested an non container object with an traling slash will\nresult in an redirect to the same url without it.\nthe QUERY_STRING won't get lost but post data would. So don't\nforget the slash problem in your form actions ;-)\n\"\"\"\n", "func_signal": "def fix_slash(environ, wantslash):\n", "code": "from colubrid.exceptions import HttpMoved\n#FIXME\n#  argh. never did something that supid\n#  find a better solution for that problem.\nurl = quote(environ.get('SCRIPT_NAME', ''))\nurl += quote(environ.get('PATH_INFO', ''))\nquery = environ.get('QUERY_STRING', '')\noldurl = query and ('%s?%s' % (url, query)) or url\n\nif oldurl and oldurl != '/':\n    if url.endswith('/'):\n        if not wantslash:\n            url = url[:-1]\n    else:\n        if wantslash:\n            url += '/'\n\n    newurl = query and ('%s?%s' % (url, query)) or url\n    if oldurl != newurl:\n        raise HttpMoved(newurl)", "path": "colubrid\\utils.py", "repo_name": "w495/colubrid", "stars": 1, "license": "None", "language": "python", "size": 105}
{"docstring": "\"\"\" Return line generator. \"\"\"\n", "func_signal": "def get_html_output(self):\n", "code": "def html_splitlines(lines):\n    # this cool function was taken from trac.\n    # http://projects.edgewall.com/trac/\n    open_tag_re = re.compile(r'<(\\w+)(\\s.*)?[^/]?>')\n    close_tag_re = re.compile(r'</(\\w+)>')\n    open_tags = []\n    for line in lines:\n        for tag in open_tags:\n            line = tag.group(0) + line\n        open_tags = []\n        for tag in open_tag_re.finditer(line):\n            open_tags.append(tag)\n        open_tags.reverse()\n        for ctag in close_tag_re.finditer(line):\n            for otag in open_tags:\n                if otag.group(1) == ctag.group(1):\n                    open_tags.remove(otag)\n                    break\n        for tag in open_tags:\n            line += '</%s>' % tag.group(1)\n        yield line\n\nreturn list(html_splitlines(self.out.getvalue().splitlines()))", "path": "colubrid\\debug.py", "repo_name": "w495/colubrid", "stars": 1, "license": "None", "language": "python", "size": 105}
{"docstring": "\"\"\"Return the first valuefor key.\"\"\"\n", "func_signal": "def getfirst(self, key, default=None):\n", "code": "if not isinstance(key, basestring):\n    raise TypeError('keys have to be strings')\nfor k, v in self.data:\n    if k.lower() == key.lower():\n        return v\nreturn default", "path": "colubrid\\utils.py", "repo_name": "w495/colubrid", "stars": 1, "license": "None", "language": "python", "size": 105}
{"docstring": "# exec code in open tracebacks\n", "func_signal": "def __call__(self, environ, start_response):\n", "code": "if self.evalex and environ.get('PATH_INFO', '').strip('/').endswith('__traceback__'):\n    parameters = cgi.parse_qs(environ['QUERY_STRING'])\n    try:\n        tb = self.tracebacks[parameters['tb'][0]]\n        frame = parameters['frame'][0]\n        context = tb[frame]\n        code = parameters['code'][0].replace('\\r','')\n    except (IndexError, KeyError):\n        pass\n    else:\n        result = context.exec_expr(code)\n        start_response('200 OK', [('Content-Type', 'text/plain')])\n        yield result\n        return\nappiter = None\ntry:\n    if hasattr(self, 'application'):\n        appiter = self.application(environ, start_response)\n    else:\n        module = __import__(self.module, '', '', [''])\n        app = getattr(module, self.handler)\n        appiter = app(environ, start_response)\n    for line in appiter:\n        yield line\nexcept:\n    ThreadedStream.install(environ)\n    exc_info = sys.exc_info()\n    try:\n        headers = [('Content-Type', 'text/html; charset=utf-8')]\n        start_response('500 INTERNAL SERVER ERROR', headers)\n    except:\n        pass\n    debug_context = self.get_debug_context(exc_info)\n    yield debug_info(environ.get('colubrid.request'), debug_context, self.evalex)\n\nif hasattr(appiter, 'close'):\n    appiter.close()", "path": "colubrid\\debug.py", "repo_name": "w495/colubrid", "stars": 1, "license": "None", "language": "python", "size": 105}
{"docstring": "\"\"\"Append an item to the internal list associated with key.\"\"\"\n", "func_signal": "def appendlist(self, key, value):\n", "code": "self.setlistdefault(key, [])\ndict.__setitem__(self, key, self.getlist(key) + [value])", "path": "colubrid\\utils.py", "repo_name": "w495/colubrid", "stars": 1, "license": "None", "language": "python", "size": 105}
{"docstring": "\"\"\"update() extends rather than replaces existing key lists.\"\"\"\n", "func_signal": "def update(self, other_dict):\n", "code": "if isinstance(other_dict, MultiDict):\n    for key, value_list in other_dict.lists():\n        self.setlistdefault(key, []).extend(value_list)\nelse:\n    for key, value in other_dict.items():\n        self.setlistdefault(key, []).append(value)", "path": "colubrid\\utils.py", "repo_name": "w495/colubrid", "stars": 1, "license": "None", "language": "python", "size": 105}
{"docstring": "\"\"\"This thread watches Python and \"watch\" files and reloads the\napplication if something changes.\"\"\"\n", "func_signal": "def reloader_thread(watch):\n", "code": "mtimes = {}\nwin = sys.platform == 'win32'\n\nwhile RUN_RELOADER:\n    for filename in [getattr(m, '__file__', '') for m\n                     in sys.modules.values()] + watch:\n        if filename[-4:] in ('.pyo', '.pyc'):\n            filename = filename[:-1]\n        if not os.path.exists(filename):\n            continue\n        stat = os.stat(filename)\n        mtime = stat.st_mtime\n        if win:\n            mtime -= stat.st_ctime\n        if filename not in mtimes:\n            mtimes[filename] = mtime\n            continue\n        if mtime != mtimes[filename]:\n            sys.exit(3) # force reload\n    time.sleep(1)", "path": "colubrid\\reloader.py", "repo_name": "w495/colubrid", "stars": 1, "license": "None", "language": "python", "size": 105}
{"docstring": "\"\"\"Send XMPP subscribed message.\n\n@type jid: string\n@param jid: Jabber ID to subscribed\n\"\"\"\n", "func_signal": "def xmppCommandSUBSCRIBED(self, jid):\n", "code": "self.sendToXMPP(Presence(to='%s' % jid,\n                         typ = 'subscribed'))", "path": "clientthread.py", "repo_name": "davux/telepaatti", "stars": 0, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Convert XMPP status to IRC not away\"\"\"\n", "func_signal": "def ircCommandNOWAWAY(self):\n", "code": "nick = self.fixNick(self.nickname)\nmsg = ':localhost 306 %s :%s' % (\n    nick,\n    'You have been marked as being away')\nself.sendToIRC(msg)", "path": "clientthread.py", "repo_name": "davux/telepaatti", "stars": 0, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Handle incoming XMPP with type Iq and version\n\n@type con: Connection\n@type iq: Iq\n@param con: XMPP Connection\n@param iq: XMPP Iq\n\"\"\"\n", "func_signal": "def iqHandlerVersion(self, con, iq):\n", "code": "ch = iq.getTag('query').getChildren()\nself.ircCommandNOTICE('** Software version information for %s **' % self.chanAlias.find(iq.getFrom()))\nfor c in ch:\n    self.ircCommandNOTICE('%s: %s' % (c.getName(), c.getData()))", "path": "clientthread.py", "repo_name": "davux/telepaatti", "stars": 0, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Send XMPP MUC role to MUC room\n\n@type muc: string\n@type nick: string\n@type role: role\n@param muc: Jabber ID of the MUC\n@param nick: users nickname in the MUC\n@param role: role of the user\n\"\"\"\n", "func_signal": "def xmppCommandMUCROLE(self, muc, nick, role):\n", "code": "iq = protocol.Iq(to=muc,\n                 queryNS=NS_MUC_ADMIN,\n                 typ = 'set')\nitem = iq.getTag('query').setTag('item')\nitem.setAttr('nick', nick)\nitem.setAttr('role', role)\nself.sendToXMPP(iq)", "path": "clientthread.py", "repo_name": "davux/telepaatti", "stars": 0, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Fixes strange character nicknames that don't work nicely with\nIRC. This function may cause conflicts and thus unfinished.\n\n@type nick: string\n@param nick: nickname to fix\n@rtype: string\n@return: fixed nick\n\"\"\"\n", "func_signal": "def fixNick(self, nick):\n", "code": "nick = unicode(nick)\nfixednick = nick.replace(' ', '_')\nfixednick = fixednick.replace('!', '_')\nfixednick = fixednick.replace(':', '_')\nself.nickmapper[fixednick] = nick\nreturn fixednick", "path": "clientthread.py", "repo_name": "davux/telepaatti", "stars": 0, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Handle incoming XMPP with type Iq\n\n@type con: Connection\n@type iq: Iq\n@param con: XMPP Connection\n@param iq: XMPP Iq\n\"\"\"\n", "func_signal": "def iqHandler(self, con, iq):\n", "code": "ns = iq.getQueryNS()\nif ns is None:\n    ns = iq.getProperties()[0]\n\nif ns == NS_DISCO and iq.getType() in ['get', 'error']:\n    self.iqHandlerInfo(con, iq)\nelif ns == NS_DISCO_ITEMS and iq.getType() == 'result':\n    self.iqHandlerItems(con, iq)\nelif ns == NS_DISCO_INFO and iq.getType() == 'result':\n    self.iqHandlerInfo(con, iq)\nelif ns == NS_DISCO_INFO and iq.getType() == 'get':\n    self.xmppCommandINFOGET(iq.getFrom())\nelif ns == NS_DISCO_ITEMS and iq.getType() == 'error':\n    self.iqHandlerError(con, iq)\nelif ns == NS_DISCO_INFO and iq.getType() == 'error':\n    self.iqHandlerError(con, iq)\nelif ns == NS_VCARD and iq.getType() == 'result':\n    self.iqHandlerVcard(con, iq)\nelif ns == NS_VCARD and iq.getType() == 'error':\n    self.iqHandlerVcardError(con, iq)\nelif ns == NS_LAST and iq.getType() == 'result':\n    self.iqHandlerLast(con, iq)\nelif ns == NS_LAST and iq.getType() == 'get':\n    self.xmppCommandLASTACTIVITY(iq.getFrom())\nelif ns == NS_LAST and iq.getType() == 'error':\n    self.iqHandlerLastError(con, iq)\nelif ns == NS_VERSION and iq.getType() == 'result':\n    self.iqHandlerVersion(con, iq)\nelif ns == NS_VERSION and iq.getType() == 'error':\n    self.iqHandlerVersionError(con, iq)\nelif ns == NS_VERSION and iq.getType() == 'get':\n    self.xmppCommandSOFTWAREVERSION(iq.getFrom())\nelse:\n    self.printDebug('IQ HANDLER FOR THIS NAMESPACE NOT IMPLEMENTED YET')", "path": "clientthread.py", "repo_name": "davux/telepaatti", "stars": 0, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Convert XMPP vcard query to IRC whois\n\n@type jid: JID\n@param jid: Jabber id of the user whos vcard is quered\n\"\"\"\n", "func_signal": "def ircCommandWHOIS(self, jid):\n", "code": "nick = self.fixNick(self.nickname)\nwhonick = jid\nif self.mucs.has_key(jid.getStripped()):\n    whonick = self.fixNick(jid.getResource())\nlines = [\n    ':localhost 311 %s %s %s %s * : %s' % (nick,\n                                           whonick,\n                                           jid.getNode(),\n                                           jid.getDomain(),\n                                           whonick),\n    ':localhost 312 %s %s localhost : XMPP telepaatti' % (nick, whonick),\n    ':localhost 318 %s %s :End of /WHOIS list.' % (nick, whonick)]\nwhile lines:\n    self.sendToIRC(lines.pop(0))", "path": "clientthread.py", "repo_name": "davux/telepaatti", "stars": 0, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Converts MUC mode to IRC channel mode\n\n@type channel: string\n@type args: string\n@param channel: anme of the channel\n@param args: arguments of the mode\n\"\"\"\n", "func_signal": "def ircCommandMODEMUC(self, channel, args):\n", "code": "nick = self.fixNick(self.nickname)\nmsg = ':localhost 324 %s #%s %s' % (nick, channel, args)\nself.sendToIRC(msg)\nmsg = ':localhost 329 %s #%s %s' % (nick, channel, '1031538353')\nself.sendToIRC(msg)", "path": "clientthread.py", "repo_name": "davux/telepaatti", "stars": 0, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Converts private messages to IRC client\n\n@type nick: string\n@type text: string\n@type timestamp: string\n@param nick: the nickname whois messaging\n@param text: the message\n@param timestamp: timestamp of the message\n\"\"\"\n", "func_signal": "def ircCommandPRIVMSG(self, nick, text, timestamp=''):\n", "code": "nick = self.fixNick(nick)\nnick = unicode(nick)\nlines = text.splitlines()\nmessages = list()\nfor line in lines:\n    action = False\n    if line.upper().startswith('/ME '):\n        line = line[4:]\n        action = True\n    if timestamp:\n        line = \"[%s] %s \" % (timestamp, line)\n    if action:\n        line = self.makeIRCACTION(line)\n\n    msg = ':%s!%s PRIVMSG %s :%s' % (nick, nick, self.fixNick(self.nickname),line)\n    messages.append(msg)\nfor msg in messages:\n    self.sendToIRC(msg)", "path": "clientthread.py", "repo_name": "davux/telepaatti", "stars": 0, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Converts XMPP mode to IRC mode. Unfinished\n\n@type args: string\n@param args: arguments of the mode\n\"\"\"\n# just to keep irssi happy, fix later to more meaningfull\n", "func_signal": "def ircCommandMODE(self, args):\n", "code": "nick = self.fixNick(self.nickname)\nmsg = ':%s MODE %s :%s' % (nick, nick, args)\nself.sendToIRC(msg)", "path": "clientthread.py", "repo_name": "davux/telepaatti", "stars": 0, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Sends message XMPP server\n\n@type msg: string\n@param msg: message to send\n\"\"\"\n", "func_signal": "def sendToXMPP(self, msg):\n", "code": "self.xmppSem.acquire()\nself.client.send(msg)\nself.xmppSem.release()", "path": "clientthread.py", "repo_name": "davux/telepaatti", "stars": 0, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Handle incoming XMPP with type Iq and last activity\n\n@type con: Connection\n@type iq: Iq\n@param con: XMPP Connection\n@param iq: XMPP Iq\n\"\"\"\n", "func_signal": "def iqHandlerLast(self, con, iq):\n", "code": "ch = iq.getTag('query')\nseconds = ch.getAttr('seconds')\nself.ircCommandNOTICE('** Last active information for %s **' % self.chanAlias.find(iq.getFrom()))\nself.ircCommandNOTICE('Idle %s second**' % seconds)", "path": "clientthread.py", "repo_name": "davux/telepaatti", "stars": 0, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Sends message IRC client\n\n@type msg: string\n@param msg: message to send\n\"\"\"\n", "func_signal": "def sendToIRC(self, msg):\n", "code": "msg = msg.encode('utf-8')\nmsg = \"%s\\r\\n\" % msg\nself.printDebug(msg)\ntry:\n    self.socket.send(msg)\nexcept:\n    self.connected = False\n    self.printError('FATAL ERROR WHILE TRYING TO WRITE IRC MESSAGE TO SOCKET, DISCONNECTING')", "path": "clientthread.py", "repo_name": "davux/telepaatti", "stars": 0, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Command handler for commands and text coming in from IRC-client\n\n@type data: string\n@param data: IRC data coming from IRC-client\n\"\"\"\n", "func_signal": "def commandHandler(self, data):\n", "code": "self.printDebug('got ircline: %s' % data)\n# utf-8 test\ntry:\n    unicode(data, 'utf-8')\nexcept exceptions.UnicodeDecodeError:\n    self.printError('GOT ERROR')\n    self.ircCommandERROR('Input form IRC client was not in utf-8. Turn utf-8 support on from your IRC client or input only pure ascii',-1)\n    return\n\nargs = data.split(' ', 1)\narguments = u''\ncommand = args[0]\nif len(args) == 2:\n    arguments = args[1]\ncommand = command.upper()\narguments = arguments.strip()\nMUC = False\nif arguments.startswith('#'):\n    arguments = arguments[1:]\n    MUC = True\n\nif command == 'JOIN':\n    room = arguments\n    muc = self.chanAlias[room]\n    if room == 'roster':\n        if self.joinedRoster: # already in #roster\n            return\n        self.ircCommandROSTERSELFJOIN()\n    else:\n        if muc.find('@') < 1:\n            self.ircCommandERRORMUC(404, 'No such room', muc)\n            return\n        if room in self.mucs.keys(): # already in MUC\n            return\n        self.joinQueue[muc] = {'messages': list(),\n                                     'users': {}}\n        p=Presence(to='%s/%s' % (\n                muc,\n                self.nickname))\n        p.setTag('x',namespace=NS_MUC).setTagData('password','')\n        p.getTag('x').addChild('history',{'maxchars':'10000','maxstanzas':'100'})\n        self.sendToXMPP(p)\n\n\nelif command == 'PART':\n    x = arguments.find(' :')\n    room = arguments.strip()\n    text = ''\n    if x > 0:\n        text = arguments[x+2:]\n        text = text.strip()\n        room = arguments[:x]\n        room = room.strip()\n    printDebug('Leaving %s' % room)\n    if room == 'roster':\n        if not self.joinedRoster: # not in roster\n            return\n        self.ircCommandROSTERPART()\n    else:\n        if room not in self.mucs.keys(): # not in room\n            return\n        printDebug(\"Leaving now %s as %s\" % (self.chanAlias[room], self.newnick))\n        self.sendToXMPP(Presence(to='%s/%s' % (self.chanAlias[room], self.newnick),\n                                 typ='unavailable',\n                                 status=text))\n\nelif command == 'PRIVMSG':\n    x = arguments.find(' :')\n    dest = arguments.strip()\n    text = ''\n    if x > 0:\n        text = arguments[x+2:]\n        text = text.strip()\n        sact = text.find('\\001ACTION ')\n        eact = text.rfind('\\001')\n        if sact > -1 and eact > -1:\n            text = '/me %s' % text[sact+8:eact]\n        dest = arguments[:x]\n        dest = dest.strip()\n    jid = dest\n    type = 'chat'\n    if MUC:\n        type = 'groupchat'\n        jid = self.chanAlias[jid]\n    at = jid.find('@')\n\n    if dest == 'roster':\n        self.ircCommandROSTERPRIVMSGMUC(text)\n        return\n    elif (at < 0) and not MUC: # private msg from muc\n        self.ircCommandERROR('You are trying to send private message someone in MUC room. Jabber can\\'t send messages with nick only. Try to sen message to whole MUC jid, for example if you are in room jabber@conference.jabber.org and are trying to send message to nick petteri use /msg jabber@conference.jabber.org/petteri message!')\n        targetnicks = list()\n        for muc in self.mucs.iterkeys():\n            for mn in self.mucs[muc].keys():\n                mn = unicode(mn)\n                mn = mn.encode('utf-8')\n                if mn[mn.find('/')+1:] == jid:\n                    targetnicks.append(mn)\n        if len(targetnicks) != 1:\n            self.printError('Problems')\n        else:\n            jid = targetnicks[0]\n            self.ircCommandERROR('Telepaatti forwarded your message to JID: %s' % jid)\n    self.sendToXMPP(protocol.Message(jid,\n                                     text,\n                                     typ = type))\n\nelif command == 'NICK':\n    if self.notFirstNick:\n        self.newnick = arguments\n        for muc in self.getMucs():\n            muc = self.chanAlias[muc]\n            self.nickChangeInMucs[muc] = {'checked': False,\n                                          'changed': False}\n        for muc in self.nickChangeInMucs.keys():\n            self.xmppCommandMUCPRESENCE(muc, self.newnick)\n    else:\n        self.notFirstNick = True\n\nelif command == 'TOPIC':\n    x = arguments.find(' :')\n    channel = arguments.strip()\n    text = ''\n    if x > 0:\n        text = arguments[x+2:]\n        text = text.strip()\n        channel = arguments[:x]\n        channel = channel.strip()\n    if channel not in self.mucs.keys():\n        self.ircCommandERROR('', 403)\n        return\n    if channel == 'roster':\n        self.ircCommandERRORMUC(482, 'TOPIC ON ROSTER CANNOT BE CHANGED', channel)\n        return\n\n    self.sendToXMPP(protocol.Message(self.chanAlias[channel],\n                                     typ = 'groupchat',\n                                     subject = text))\n\nelif command == 'MODE':\n    if not arguments:\n        return\n    x = arguments.find(' ')\n    params = ''\n    channel = arguments\n    if x > -1:\n        channel = arguments[:x]\n        params = arguments[x+1:]\n    x = params.find(' ')\n    tonick = ''\n    if x > -1:\n        tonick = params[x+1:]\n        params = params[x:]\n        params.strip()\n\n\n    if channel == 'roster':\n        return\n    elif channel == self.nickname:\n        self.ircCommandMODE(params)\n    else:\n        jid = self.chanAlias[channel]\n        if params.find('b') > -1: # get bandlist\n            self.ircCommandMODEMUCBANLIST(channel)\n            return\n        elif params.find('+o') > -1: # trying to op someone\n            self.xmppCommandMUCROLE(jid, tonick, 'moderator')\n        elif params.find('-o') > -1: # trying to deop someone\n            self.xmppCommandMUCROLE(jid, tonick, 'participant')\n        elif params.find('+v') > -1: # trying to voice someone\n            self.xmppCommandMUCROLE(jid, tonick, 'participant')\n        elif params.find('-v') > -1: # trying to voice someone\n            self.xmppCommandMUCROLE(jid, tonick, 'visitor')\n        else:\n            self.xmppCommandMUCMODE(jid)\n\nelif command == 'WHO':\n    if not arguments:\n        return\n    jid = arguments\n    self.xmppCommandMUCUSERS(jid)\n\nelif command == 'WHOIS':\n    self.ircCommandWHOIS(JID(arguments))\n    self.xmppCommandGETWHOIS(JID(arguments))\n\nelif command == 'AWAY':\n    arguments = arguments[1:] # remove the :\n    show = ''\n    if arguments != '':\n        show = 'away'\n    args = arguments.split(' ',1)\n    status = arguments\n    if args[0].upper() in STATUSSTATES:\n        show = args[0]\n        if len(args) == 2:\n            status = args[1]\n    self.xmppCommandSTATUS(show, status)", "path": "clientthread.py", "repo_name": "davux/telepaatti", "stars": 0, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Reverses nicks fixed with fixNick function. Is also unfinished\n\n@type fixednick: string\n@param fixednick: nickname to unfix\n@rtype: string\n@return: unfixed nick\n\"\"\"\n", "func_signal": "def unfixNick(self, fixednick):\n", "code": "nick = fixednick\ntry:\n    nick = self.nickmapper[fixednick]\nexcept:\n    self.printError('unfixNick did not work')\nreturn nick", "path": "clientthread.py", "repo_name": "davux/telepaatti", "stars": 0, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Convers MUC user mode to IRC channel user mode. Example use cases\nare when someone is granted a voice or admin rights on a MUC.\n\n@type giver: string\n@type taker: string\n@type muc: string\n@type args: string\n@param giver: giver of the new mode\n@param taker: taker of the new mode\n@param muc: MUC on which the mode takes place\n@param args: arguments of the mode\n\"\"\"\n", "func_signal": "def ircCommandMODEMUCUSER(self, giver, taker, muc, args):\n", "code": "givernick = self.fixNick(giver.getResource())\ngiver = self.fixNick(giver)\ntaker = self.fixNick(taker.getResource())\nmsg = ':%s!%s MODE #%s %s %s' % (givernick,\n                                 giver,\n                                 muc,\n                                 args,\n                                 taker)\nself.sendToIRC(msg)", "path": "clientthread.py", "repo_name": "davux/telepaatti", "stars": 0, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Converts MUC topic to IRC channel topic\n\n@type nick: string\n@type channel: string\n@type text: string\n@param channel: name of the channel\n@param nick: the nickname whois messaging\n@param text: the message\n\"\"\"\n", "func_signal": "def ircCommandTOPIC(self, nick, channel, text):\n", "code": "snick = self.fixNick(nick.getResource())\nnick = self.fixNick(nick)\nmsg =':%s!%s TOPIC #%s :%s' % (snick, nick, channel, text)\nself.sendToIRC(msg)", "path": "clientthread.py", "repo_name": "davux/telepaatti", "stars": 0, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Convert XMPP status to IRC away\"\"\"\n", "func_signal": "def ircCommandUNAWAY(self):\n", "code": "nick = self.fixNick(self.nickname)\nmsg = ':localhost 305 %s :%s' % (\n    nick,\n    'You are no longer marked as being away')\nself.sendToIRC(msg)", "path": "clientthread.py", "repo_name": "davux/telepaatti", "stars": 0, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Handle incoming XMPP with type message\n\n@type sess: string\n@type mess: Message\n@param sess: session\n@param mess: XMPP Message\n\"\"\"\n", "func_signal": "def messageHandler(self, sess, mess):\n", "code": "if mess.getType() == 'error':\n    self.messageHandlerError(sess,mess)\n    return\nnick = mess.getFrom()\ntext = mess.getBody()\ntopic = mess.getSubject()\nroom = unicode(mess.getFrom())\nx = room.find('/')\nif x > 0:\n    room = room[:room.find('/')]\nts = ''\nif mess.getTag('x',namespace=NS_DELAY):\n    ts=mess.getTimestamp()\n    if not ts:\n        ts=mess.setTimestamp()\n        ts=mess.getTimestamp()\n    ts = time.strptime(ts,'%Y%m%dT%H:%M:%S')\n    ts = datetime.datetime(*ts[:-3])\nif not text and not topic:\n    return\n\nMUC = False\nif mess.getType() == 'groupchat':\n    MUC = True\n\nroom = self.chanAlias.find(room)\nif not MUC:\n    self.ircCommandPRIVMSG(nick, text, ts)\nelif topic:\n    self.ircCommandTOPIC(nick, room, topic)\nelif not nick.getResource() == self.nickname or ts:\n    self.ircCommandPRIVMSGMUC(nick, room, text, ts)", "path": "clientthread.py", "repo_name": "davux/telepaatti", "stars": 0, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Convert XMPP subscribe message to IRC message\n\n@type pres: presence\n@param pres: presence of which to subscribe\n\"\"\"\n", "func_signal": "def ircCommandSUBSCRIBE(self, pres):\n", "code": "text = \"is making subsciption request to you with message: \\\"%s\\\" You MUST either approve the request or refuse the request. You can approve it by joinin #roster channel andthen type \\\"!subscribed %s\\\" if you wish to subscibe to contact or \\\"!unsubscribed %s\\\" if you wish not to subscibe to contact\" % (pres.getStatus(), pres.getFrom(), pres.getFrom())\ntext = self.makeIRCACTION(text)\nself.printDebug(str(pres))\nself.ircCommandPRIVMSG(pres.getFrom(), text)", "path": "clientthread.py", "repo_name": "davux/telepaatti", "stars": 0, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"\nSet item in session data.\n\nArgs:\n    keyname: They keyname of the mapping.\n    value: The value of mapping.\n\"\"\"\n\n", "func_signal": "def __setitem__(self, keyname, value):\n", "code": "if self.integrate_flash and (keyname == u\"flash\"):\n    self.flash.msg = value\nelse:\n    keyname = self._validate_key(keyname)\n    self.cache[keyname] = value\n    # self._set_memcache() # commented out because this is done in the datestore put\n    return self._put(keyname, value)", "path": "sessions.py", "repo_name": "btbytes/rufflecat", "stars": 1, "license": "None", "language": "python", "size": 432}
{"docstring": "\"\"\"\nReturn size of session.\n\"\"\"\n# check memcache first\n", "func_signal": "def __len__(self):\n", "code": "if hasattr(self, u\"session\"):\n    results = self._get()\n    if results is not None:\n        return len(results) + len(self.cookie_vals)\n    else:\n        return 0\nreturn len(self.cookie_vals)", "path": "sessions.py", "repo_name": "btbytes/rufflecat", "stars": 1, "license": "None", "language": "python", "size": 432}
{"docstring": "\"\"\"\nCreates a unique key for the session.\n\"\"\"\n", "func_signal": "def create_key(self):\n", "code": "self.session_key = time.time()\nvalid = False\nwhile valid == False:\n    # verify session_key is unique\n    if memcache.get(u\"_AppEngineUtilities_Session_\" + unicode(self.session_key)):\n        self.session_key = self.session_key + 0.001\n    else:\n        query = _AppEngineUtilities_Session.all()\n        query.filter(u\"session_key = \", self.session_key)\n        results = query.fetch(1)\n        if len(results) > 0:\n            self.session_key = self.session_key + 0.001\n        else:\n            try:\n                self.put()\n                memcache.set(u\"_AppEngineUtilities_Session_\" + unicode(self.session_key), self)\n            except:\n                self.dirty = True\n                memcache.set(u\"_AppEngineUtilities_Session_\" + unicode(self.session_key), self)\n            valid = True", "path": "sessions.py", "repo_name": "btbytes/rufflecat", "stars": 1, "license": "None", "language": "python", "size": 432}
{"docstring": "\"\"\"\nSet a memcache object with all the session data. Optionally you can\nadd a key and value to the memcache for put operations.\n\"\"\"\n# Pull directly from the datastore in order to ensure that the\n# information is as up to date as possible.\n", "func_signal": "def _set_memcache(self):\n", "code": "if self.writer == \"datastore\":\n    data = {}\n    sessiondata = self._get()\n    if sessiondata is not None:\n        for sd in sessiondata:\n            data[sd.keyname] = pickle.loads(sd.content)\n\n    memcache.set('sid-'+unicode(self.session.key()), data, \\\n        self.session_expire_time)", "path": "sessions.py", "repo_name": "btbytes/rufflecat", "stars": 1, "license": "None", "language": "python", "size": 432}
{"docstring": "\"\"\"\nDelete's the current session, creating a new one.\n\"\"\"\n", "func_signal": "def flush(self):\n", "code": "self._delete_session()\nself.__init__()", "path": "sessions.py", "repo_name": "btbytes/rufflecat", "stars": 1, "license": "None", "language": "python", "size": 432}
{"docstring": "\"\"\"\nUses the passed sid to get a session object from memcache, or datastore\nif a valid one exists.\n\"\"\"\n", "func_signal": "def get_session(cls, session_obj=None):\n", "code": "if session_obj.sid == None:\n    return None\nsession_key = session_obj.sid.split(u'_')[0]\nsession = memcache.get(u\"_AppEngineUtilities_Session_\" + unicode(session_key))\nif session:\n    if session.deleted == True:\n        session.delete()\n        return None\n    if session.dirty == True and session.working != False:\n        # the working bit is used to make sure multiple requests, which can happen\n        # with ajax oriented sites, don't try to put at the same time\n        session.working = True\n        memcache.set(u\"_AppEngineUtilities_Session_\" + unicode(session_key), session)\n        session.put()\n    if session_obj.sid in session.sid:\n        #logging.info('grabbed session from memcache')\n        sessionAge = datetime.datetime.now() - session.last_activity\n        if sessionAge.seconds > session_obj.session_expire_time:\n            session.delete()\n            return None\n        return session\n    else:\n        return None\n \n# Not in memcache, check datastore\nquery = _AppEngineUtilities_Session.all()\nquery.filter(u\"sid = \", session_obj.sid)\nresults = query.fetch(1)\nif len(results) > 0:\n    sessionAge = datetime.datetime.now() - results[0].last_activity\n    if sessionAge.seconds > session_obj.session_expire_time:\n        results[0].delete()\n        return None\n    memcache.set(u\"_AppEngineUtilities_Session_\" + unicode(session_key), results[0])\n    memcache.set(u\"_AppEngineUtilities_SessionData_\" + unicode(session_key), results[0].get_items_ds())\n    #logging.info('grabbed session from datastore')\n    return results[0]\nelse:\n    return None", "path": "sessions.py", "repo_name": "btbytes/rufflecat", "stars": 1, "license": "None", "language": "python", "size": 432}
{"docstring": "\"\"\"\nThis gets all the items straight from the datastore, does not\ninteract with the memcache.\n\"\"\"\n", "func_signal": "def get_items_ds(self):\n", "code": "query = _AppEngineUtilities_SessionData.all()\nquery.filter(u\"session_key\", self.session_key)\nresults = query.fetch(1000)\nreturn results", "path": "sessions.py", "repo_name": "btbytes/rufflecat", "stars": 1, "license": "None", "language": "python", "size": 432}
{"docstring": "\"\"\"\nGet the user's session from the datastore\n\"\"\"\n", "func_signal": "def _get_session(self):\n", "code": "query = _AppEngineUtilities_Session.all()\nquery.filter('sid', self.sid)\nif self.check_user_agent:\n    query.filter('ua', os.environ['HTTP_USER_AGENT'])\nif self.check_ip:\n    query.filter('ip', os.environ['REMOTE_ADDR'])\nresults = query.fetch(1)\nif len(results) is 0:\n    return None\nelse:\n    sessionAge = datetime.datetime.now() - results[0].last_activity\n    if sessionAge.seconds > self.session_expire_time:\n        results[0].delete()\n        return None\n    return results[0]", "path": "sessions.py", "repo_name": "btbytes/rufflecat", "stars": 1, "license": "None", "language": "python", "size": 432}
{"docstring": "\"\"\"\nGet item from session data.\n\nkeyname: The keyname of the mapping.\n\"\"\"\n# flash messages don't go in the datastore\n\n", "func_signal": "def __getitem__(self, keyname):\n", "code": "if self.integrate_flash and (keyname == u\"flash\"):\n    return self.flash.msg\nif keyname in self.cache:\n    # UNPICKLING CACHE return pickle.loads(unicode(self.cache[keyname]))\n    return self.cache[keyname]\nif keyname in self.cookie_vals:\n    return self.cookie_vals[keyname]\nif hasattr(self, u\"session\"):\n    data = self._get(keyname)\n    if data:\n        #UNPICKLING CACHE self.cache[keyname] = data.content\n        self.cache[keyname] = pickle.loads(data.content)\n        return pickle.loads(data.content)\n    else:\n        raise KeyError(unicode(keyname))\nraise KeyError(unicode(keyname))", "path": "sessions.py", "repo_name": "btbytes/rufflecat", "stars": 1, "license": "None", "language": "python", "size": 432}
{"docstring": "\"\"\"\nValidate the keyname, making sure it is set and not a reserved name.\n\"\"\"\n", "func_signal": "def _validate_key(self, keyname):\n", "code": "if keyname is None:\n    raise ValueError(u\"You must pass a keyname for the session\" + \\\n        u\" data content.\")\nelif keyname in (u\"sid\", u\"flash\"):\n    raise ValueError(keyname + u\" is a reserved keyname.\")\n\nif type(keyname) != type([str, unicode]):\n    return unicode(keyname)\nreturn keyname", "path": "sessions.py", "repo_name": "btbytes/rufflecat", "stars": 1, "license": "None", "language": "python", "size": 432}
{"docstring": "\"\"\"\nUpdates with key/value pairs from b, overwriting existing keys, returns None\n\"\"\"\n", "func_signal": "def update(*dicts):\n", "code": "for dict in dicts:\n    for k in dict:\n        self._put(k, dict[k])\nreturn None", "path": "sessions.py", "repo_name": "btbytes/rufflecat", "stars": 1, "license": "None", "language": "python", "size": 432}
{"docstring": "\"\"\"\nA copy list of values.\n\"\"\"\n", "func_signal": "def values(self):\n", "code": "v = []\nfor k in self:\n    v.append(self[k])\nreturn v", "path": "sessions.py", "repo_name": "btbytes/rufflecat", "stars": 1, "license": "None", "language": "python", "size": 432}
{"docstring": "\"\"\"\nList of keys.\n\"\"\"\n", "func_signal": "def keys(self):\n", "code": "l = []\nfor k in self:\n    l.append(k)\nreturn l", "path": "sessions.py", "repo_name": "btbytes/rufflecat", "stars": 1, "license": "None", "language": "python", "size": 432}
{"docstring": "\"\"\"\nDeletes all sessions and session data from the data store and memcache:\n\nNOTE: This is not fully developed. It also will not delete any cookie\ndata as this does not work for each incoming request. Keep this in mind\nif you are using the cookie writer.\n\"\"\"\n", "func_signal": "def delete_all_sessions(cls):\n", "code": "all_sessions_deleted = False\nall_data_deleted = False\n\nwhile not all_sessions_deleted:\n    query = _AppEngineUtilities_Session.all()\n    results = query.fetch(75)\n    if len(results) is 0:\n        all_sessions_deleted = True\n    else:\n        for result in results:\n            result.delete()", "path": "sessions.py", "repo_name": "btbytes/rufflecat", "stars": 1, "license": "None", "language": "python", "size": 432}
{"docstring": "\"\"\"\nChanges the session id.\n\"\"\"\n", "func_signal": "def cycle_key(self):\n", "code": "self.sid = self.new_sid()\nif len(self.session.sid) > 2:\n    self.session.sid.remove(self.session.sid[0])\nself.session.sid.append(self.sid)", "path": "sessions.py", "repo_name": "btbytes/rufflecat", "stars": 1, "license": "None", "language": "python", "size": 432}
{"docstring": "\"\"\"\nA copy of list of (key, value) pairs\n\"\"\"\n", "func_signal": "def items(self):\n", "code": "op = {}\nfor k in self:\n    op[k] = self[k]\nreturn op", "path": "sessions.py", "repo_name": "btbytes/rufflecat", "stars": 1, "license": "None", "language": "python", "size": 432}
{"docstring": "\"\"\"\nAdds a keyname/value for session to the datastore and memcache\n\"\"\"\n# update or insert in datastore\n", "func_signal": "def put(self):\n", "code": "try:\n    db.put(self)\n    self.dirty = False\nexcept:\n    self.dirty = True\n\n# update or insert in memcache\nmc_items = memcache.get(u\"_AppEngineUtilities_SessionData_\" + unicode(self.session_key))\nif mc_items:\n    value_updated = False\n    for item in mc_items:\n        if value_updated == True:\n            break\n        if item.keyname == self.keyname:\n            #logging.info(u\"updating \" + self.keyname)\n            item.content = self.content\n            memcache.set(u\"_AppEngineUtilities_SessionData_\" + unicode(self.session_key), mc_items)\n            value_updated = True\n            break\n    if value_updated == False:\n        #logging.info(\"adding \" + self.keyname)\n        mc_items.append(self)\n        memcache.set(u\"_AppEngineUtilities_SessionData_\" + unicode(self.session_key), mc_items)", "path": "sessions.py", "repo_name": "btbytes/rufflecat", "stars": 1, "license": "None", "language": "python", "size": 432}
{"docstring": "\"\"\"\nReturn all of the SessionData object data from the datastore onlye,\nunless keyname is specified, in which case only that instance of \nSessionData is returned.\nImportant: This does not interact with memcache and pulls directly\nfrom the datastore. This also does not get items from the cookie\nstore.\n\nArgs:\n    keyname: The keyname of the value you are trying to retrieve.\n\"\"\"\n", "func_signal": "def _get(self, keyname=None):\n", "code": "if keyname != None:\n    return self.session.get_item(keyname)\nreturn self.session.get_items()\n\"\"\"\nOLD\nquery = _AppEngineUtilities_SessionData.all()\nquery.filter('session', self.session)\nif keyname != None:\n    query.filter('keyname =', keyname)\nresults = query.fetch(1000)\n\nif len(results) is 0:\n    return None\nif keyname != None:\n    return results[0]\nreturn results\n\"\"\"", "path": "sessions.py", "repo_name": "btbytes/rufflecat", "stars": 1, "license": "None", "language": "python", "size": 432}
{"docstring": "\"\"\"\nCreate a new session id.\n\"\"\"\n", "func_signal": "def new_sid(self):\n", "code": "sid = unicode(self.session.session_key) + \"_\" +md5.new(repr(time.time()) + \\\n        unicode(random.random())).hexdigest()\nreturn sid", "path": "sessions.py", "repo_name": "btbytes/rufflecat", "stars": 1, "license": "None", "language": "python", "size": 432}
{"docstring": "\"\"\"\na[k] if k in a, else x\n\"\"\"\n", "func_signal": "def get(self, keyname, default = None):\n", "code": "try:\n    return self.__getitem__(keyname)\nexcept KeyError:\n    if default is not None:\n        return default\n    return None", "path": "sessions.py", "repo_name": "btbytes/rufflecat", "stars": 1, "license": "None", "language": "python", "size": 432}
{"docstring": "# Special logic to use the enum_map to set the value of the object's value member.\n", "func_signal": "def _ConvertElementAttributeToMember(self, attribute, value):\n", "code": "if attribute == self.attrib_name and value != '':\n  self.value = self.enum_map[value]\n  return\n# Find the attribute in this class's list of attributes.\nif self.__class__._attributes.has_key(attribute):\n  # Find the member of this class which corresponds to the XML attribute\n  # (lookup in current_class._attributes) and set this member to the\n  # desired value (using self.__dict__).\n  setattr(self, self.__class__._attributes[attribute], value)\nelse:\n  # The current class doesn't map this attribute, so try to parent class.\n  atom.ExtensionContainer._ConvertElementAttributeToMember(self, \n                                                           attribute,\n                                                           value)", "path": "gdata\\calendar\\__init__.py", "repo_name": "kevinmarks/addressbooker", "stars": 1, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Populates the content and row_id based on content of the entry.\n\nThis method is used in the Record's contructor.\n\nArgs:\n  entry: gdata.spreadsheet.SpreadsheetsList The Atom entry \n      representing this row in the worksheet.\n\"\"\"\n", "func_signal": "def ExtractContentFromEntry(self, entry):\n", "code": "self.content = {}\nif entry:\n  self.row_id = entry.id.text.split('/')[-1]\n  for label, custom in entry.custom.iteritems():\n    self.content[label] = custom.text", "path": "gdata\\spreadsheet\\text_db.py", "repo_name": "kevinmarks/addressbooker", "stars": 1, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "# Special logic to handle Web Content links\n", "func_signal": "def _ConvertElementTreeToMember(self, child_tree):\n", "code": "if (child_tree.tag == '{%s}link' % atom.ATOM_NAMESPACE and \n    child_tree.attrib['rel'] == WEB_CONTENT_LINK_REL):\n  if self.link is None:\n    self.link = []\n  self.link.append(atom._CreateClassFromElementTree(WebContentLink, \n                                                    child_tree))\n  return\n# Find the element's tag in this class's list of child members\nif self.__class__._children.has_key(child_tree.tag):\n  member_name = self.__class__._children[child_tree.tag][0]\n  member_class = self.__class__._children[child_tree.tag][1]\n  # If the class member is supposed to contain a list, make sure the\n  # matching member is set to a list, then append the new member\n  # instance to the list.\n  if isinstance(member_class, list):\n    if getattr(self, member_name) is None:\n      setattr(self, member_name, [])\n    getattr(self, member_name).append(atom._CreateClassFromElementTree(\n        member_class[0], child_tree))\n  else:\n    setattr(self, member_name,\n            atom._CreateClassFromElementTree(member_class, child_tree))\nelse:\n  atom.ExtensionContainer._ConvertElementTreeToMember(self, child_tree)", "path": "gdata\\calendar\\__init__.py", "repo_name": "kevinmarks/addressbooker", "stars": 1, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Returns a list of all item attributes which have the desired name.\n\nArgs:\n  name: str The tag of the desired base attributes. For example, calling\n      this method with 'rating' would return a list of ItemAttributes\n      represented by a 'g:rating' tag.\n\nReturns:\n  A list of matching ItemAttribute objects.\n\"\"\"\n", "func_signal": "def GetItemAttributes(self, name):\n", "code": "result = []\nfor attrib in self.item_attributes:\n  if attrib.name == name:\n    result.append(attrib)\nreturn result", "path": "gdata\\base\\__init__.py", "repo_name": "kevinmarks/addressbooker", "stars": 1, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "# Convert the members of this class which are XML child nodes. \n# This uses the class's _children dictionary to find the members which\n# should become XML child nodes.\n", "func_signal": "def _AddMembersToElementTree(self, tree):\n", "code": "member_node_names = [values[0] for tag, values in \n                                   self.__class__._children.iteritems()]\nfor member_name in member_node_names:\n  member = getattr(self, member_name)\n  if member is None:\n    pass\n  elif isinstance(member, list):\n    for instance in member:\n      instance._BecomeChildElement(tree)\n  else:\n    member._BecomeChildElement(tree)\n# Convert the members of this class which are XML attributes.\nfor xml_attribute, member_name in self.__class__._attributes.iteritems():\n  member = getattr(self, member_name)\n  if member is not None:\n    tree.attrib[xml_attribute] = member\n# Convert all special custom item attributes to nodes\nfor attribute in self.item_attributes:\n  attribute._BecomeChildElement(tree)\n# Lastly, call the ExtensionContainers's _AddMembersToElementTree to \n# convert any extension attributes.\natom.ExtensionContainer._AddMembersToElementTree(self, tree)", "path": "gdata\\base\\__init__.py", "repo_name": "kevinmarks/addressbooker", "stars": 1, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "# Find the element's tag in this class's list of child members\n", "func_signal": "def _ConvertElementTreeToMember(self, child_tree):\n", "code": "if self.__class__._children.has_key(child_tree.tag):\n  member_name = self.__class__._children[child_tree.tag][0]\n  member_class = self.__class__._children[child_tree.tag][1]\n  # If the class member is supposed to contain a list, make sure the\n  # matching member is set to a list, then append the new member\n  # instance to the list.\n  if isinstance(member_class, list):\n    if getattr(self, member_name) is None:\n      setattr(self, member_name, [])\n    getattr(self, member_name).append(atom._CreateClassFromElementTree(\n        member_class[0], child_tree))\n  else:\n    setattr(self, member_name, \n            atom._CreateClassFromElementTree(member_class, child_tree))\nelif child_tree.tag.find('{%s}' % GBASE_NAMESPACE) == 0:\n  # If this is in the gbase namespace, make it into an extension element.\n  name = child_tree.tag[child_tree.tag.index('}')+1:]\n  value = child_tree.text\n  if child_tree.attrib.has_key('type'):\n    value_type = child_tree.attrib['type']\n  else:\n    value_type = None\n  self.AddItemAttribute(name, value, value_type)\nelse:\n  atom.ExtensionContainer._ConvertElementTreeToMember(self, child_tree)", "path": "gdata\\base\\__init__.py", "repo_name": "kevinmarks/addressbooker", "stars": 1, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Finds spreadsheets which have the unique key or title.\n\nIf querying on the spreadsheet_key there will be at most one result, but\nsearching by name could yield multiple results.\n\nArgs:\n  spreadsheet_key: str The unique key for the spreadsheet, this \n      usually in the the form 'pk23...We' or 'o23...423.12,,,3'.\n  name: str The title of the spreadsheets.\n\nReturns:\n  A list of Database objects representing the desired spreadsheets.\n\"\"\"\n", "func_signal": "def GetDatabases(self, spreadsheet_key=None, name=None):\n", "code": "if spreadsheet_key:\n  db_entry = self.__docs_client.GetDocumentListEntry(\n      r'/feeds/documents/private/full/spreadsheet%3A' + spreadsheet_key)\n  return [Database(spreadsheet_entry=db_entry, database_client=self)]\nelse:\n  title_query = gdata.docs.service.DocumentQuery()\n  title_query['title'] = name\n  db_feed = self.__docs_client.QueryDocumentListFeed(title_query.ToUri())\n  matching_databases = []\n  for entry in db_feed.entry:\n    matching_databases.append(Database(spreadsheet_entry=entry, \n                                       database_client=self))\n  return matching_databases", "path": "gdata\\spreadsheet\\text_db.py", "repo_name": "kevinmarks/addressbooker", "stars": 1, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Query Google Spreadsheets to get the latest data from the server.\n\nFetches the entry for this row and repopulates the content dictionary \nwith the data found in the row.\n\"\"\"\n", "func_signal": "def Pull(self):\n", "code": "if self.row_id:\n  self.entry = self.client._GetSpreadsheetsClient().GetListFeed(\n      self.spreadsheet_key, wksht_id=self.worksheet_id, row_id=self.row_id)\nself.ExtractContentFromEntry(self.entry)", "path": "gdata\\spreadsheet\\text_db.py", "repo_name": "kevinmarks/addressbooker", "stars": 1, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Performs a query against the worksheet to find rows which match.\n\nFor details on query string syntax see the section on sq under\nhttp://code.google.com/apis/spreadsheets/reference.html#list_Parameters\n\nArgs:\n  query_string: str Examples: 'name == john' to find all rows with john\n      in the name column, '(cost < 19.50 and name != toy) or cost > 500'\n\nReturns:\n  RecordResultSet with the first group of matches.\n\"\"\"\n", "func_signal": "def FindRecords(self, query_string):\n", "code": "row_query = gdata.spreadsheet.service.ListQuery()\nrow_query.sq = query_string\nmatching_feed = self.client._GetSpreadsheetsClient().GetListFeed(\n    self.spreadsheet_key, wksht_id=self.worksheet_id, query=row_query)\nreturn RecordResultSet(matching_feed, self.client, \n    self.spreadsheet_key, self.worksheet_id)", "path": "gdata\\spreadsheet\\text_db.py", "repo_name": "kevinmarks/addressbooker", "stars": 1, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Searches for a worksheet with the specified ID or name.\n\nThe list of results should have one table at most, or no results\nif the id or name were not found.\n\nArgs:\n  worksheet_id: str The ID of the worksheet, example: 'od6'\n  name: str The title of the worksheet.\n\nReturns:\n  A list of length 0 or 1 containing the desired Table. A list is returned\n  to make this method feel like GetDatabases and GetRecords.\n\"\"\"\n", "func_signal": "def GetTables(self, worksheet_id=None, name=None):\n", "code": "if worksheet_id:\n  worksheet_entry = self.client._GetSpreadsheetsClient().GetWorksheetsFeed(\n      self.spreadsheet_key, wksht_id=worksheet_id)\n  return [Table(name=worksheet_entry.title.text, \n      worksheet_entry=worksheet_entry, database_client=self.client, \n      spreadsheet_key=self.spreadsheet_key)]\nelse:\n  matching_tables = []\n  query = None\n  if name:\n    query = gdata.spreadsheet.service.DocumentQuery()\n    query.title = name\n \n  worksheet_feed = self.client._GetSpreadsheetsClient().GetWorksheetsFeed(\n      self.spreadsheet_key, query=query)\n  for entry in worksheet_feed.entry:\n    matching_tables.append(Table(name=entry.title.text, \n        worksheet_entry=entry, database_client=self.client, \n        spreadsheet_key=self.spreadsheet_key))\n  return matching_tables", "path": "gdata\\spreadsheet\\text_db.py", "repo_name": "kevinmarks/addressbooker", "stars": 1, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Queries to find the column names in the first row of the worksheet.\n\nUseful when you have retrieved the table from the server and you don't \nknow the column names.\n\"\"\"\n", "func_signal": "def LookupFields(self):\n", "code": "if self.entry:\n  first_row_contents = []\n  query = gdata.spreadsheet.service.CellQuery()\n  query.max_row = '1'\n  query.min_row = '1'\n  feed = self.client._GetSpreadsheetsClient().GetCellsFeed(\n      self.spreadsheet_key, wksht_id=self.worksheet_id, query=query)\n  for entry in feed.entry:\n    first_row_contents.append(entry.content.text)\n  # Get the next set of cells if needed.\n  next_link = feed.GetNextLink()\n  while next_link:\n    feed = self.client._GetSpreadsheetsClient().Get(next_link.href, \n        converter=gdata.spreadsheet.SpreadsheetsCellsFeedFromString)\n    for entry in feed.entry:\n      first_row_contents.append(entry.content.text)\n    next_link = feed.GetNextLink()\n  # Convert the contents of the cells to valid headers.\n  self.fields = ConvertStringsToColumnHeaders(first_row_contents)", "path": "gdata\\spreadsheet\\text_db.py", "repo_name": "kevinmarks/addressbooker", "stars": 1, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Changes the contents of the cells in the first row of this worksheet.\n\nArgs:\n  fields: list of strings The names in the list comprise the\n      first row of the worksheet. These names are converted into XML\n      tags by the server. To avoid changes during the translation\n      process I recommend using all lowercase alphabetic names. For\n      example ['somelongname', 'theothername']\n\"\"\"\n# TODO: If the table already had fields, we might want to clear out the,\n# current column headers.\n", "func_signal": "def SetFields(self, fields):\n", "code": "self.fields = fields\ni = 0\nfor column_name in fields:\n  i = i + 1\n  # TODO: speed this up by using a batch request to update cells.\n  self.client._GetSpreadsheetsClient().UpdateCell(1, i, column_name, \n      self.spreadsheet_key, self.worksheet_id)", "path": "gdata\\spreadsheet\\text_db.py", "repo_name": "kevinmarks/addressbooker", "stars": 1, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Constructor for a Database Client. \n  \nIf the username and password are present, the constructor  will contact\nthe Google servers to authenticate.\n\nArgs:\n  username: str (optional) Example: jo@example.com\n  password: str (optional)\n\"\"\"\n", "func_signal": "def __init__(self, username=None, password=None):\n", "code": "self.__docs_client = gdata.docs.service.DocsService()\nself.__spreadsheets_client = (\n    gdata.spreadsheet.service.SpreadsheetsService())\nself.SetCredentials(username, password)", "path": "gdata\\spreadsheet\\text_db.py", "repo_name": "kevinmarks/addressbooker", "stars": 1, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Finds the first link with rel set to WEB_CONTENT_REL\n\nReturns:\n  A gdata.calendar.WebContentLink or none if none of the links had rel \n  equal to WEB_CONTENT_REL\n\"\"\"\n\n", "func_signal": "def GetWebContentLink(self):\n", "code": "for a_link in self.link:\n  if a_link.rel == WEB_CONTENT_LINK_REL:\n    return a_link\nreturn None", "path": "gdata\\calendar\\__init__.py", "repo_name": "kevinmarks/addressbooker", "stars": 1, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Gets a single record from the worksheet based on row ID or number.\n\nArgs:\n  row_id: The ID for the individual row.\n  row_number: str or int The position of the desired row. Numbering \n      begins at 1, which refers to the second row in the worksheet since\n      the first row is used for column names.\n\nReturns:\n  Record for the desired row.\n\"\"\"\n", "func_signal": "def GetRecord(self, row_id=None, row_number=None):\n", "code": "if row_id:\n  row_entry = self.client._GetSpreadsheetsClient().GetListFeed(\n      self.spreadsheet_key, wksht_id=self.worksheet_id, row_id=row_id)\n  return Record(content=None, row_entry=row_entry, \n       spreadsheet_key=self.spreadsheet_key, \n       worksheet_id=self.worksheet_id, database_client=self.client)\nelse:\n  row_query = gdata.spreadsheet.service.ListQuery()\n  row_query.start_index = str(row_number)\n  row_query.max_results = '1'\n  row_feed = self.client._GetSpreadsheetsClient().GetListFeed(\n      self.spreadsheet_key, wksht_id=self.worksheet_id, query=row_query)\n  if len(row_feed.entry) >= 1:\n    return Record(content=None, row_entry=row_feed.entry[0],\n        spreadsheet_key=self.spreadsheet_key,\n        worksheet_id=self.worksheet_id, database_client=self.client)\n  else:\n    return None", "path": "gdata\\spreadsheet\\text_db.py", "repo_name": "kevinmarks/addressbooker", "stars": 1, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Gets all rows between the start and end row numbers inclusive.\n\nArgs:\n  start_row: str or int\n  end_row: str or int\n\nReturns:\n  RecordResultSet for the desired rows.\n\"\"\"\n", "func_signal": "def GetRecords(self, start_row, end_row):\n", "code": "start_row = int(start_row)\nend_row = int(end_row)\nmax_rows = end_row - start_row + 1\nrow_query = gdata.spreadsheet.service.ListQuery()\nrow_query.start_index = str(start_row)\nrow_query.max_results = str(max_rows)\nrows_feed = self.client._GetSpreadsheetsClient().GetListFeed(\n    self.spreadsheet_key, wksht_id=self.worksheet_id, query=row_query)\nreturn RecordResultSet(rows_feed, self.client, self.spreadsheet_key,\n    self.worksheet_id)", "path": "gdata\\spreadsheet\\text_db.py", "repo_name": "kevinmarks/addressbooker", "stars": 1, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Deletes the entire database spreadsheet from Google Spreadsheets.\"\"\"\n", "func_signal": "def Delete(self):\n", "code": "entry = self.client._GetDocsClient().Get(\n    r'http://docs.google.com/feeds/documents/private/full/spreadsheet%3A' +\n    self.spreadsheet_key)\nself.client._GetDocsClient().Delete(entry.GetEditLink().href)", "path": "gdata\\spreadsheet\\text_db.py", "repo_name": "kevinmarks/addressbooker", "stars": 1, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Deletes the first extension element which matches name.\n\nDeletes the first extension element which matches name. \n\"\"\"\n\n", "func_signal": "def RemoveItemAttribute(self, name):\n", "code": "for i in xrange(len(self.item_attributes)):\n  if self.item_attributes[i].name == name:\n    del self.item_attributes[i]\n    return", "path": "gdata\\base\\__init__.py", "repo_name": "kevinmarks/addressbooker", "stars": 1, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Adds a new item attribute tag containing the value.\n\nCreates a new extension element in the GBase namespace to represent a\nGoogle Base item attribute.\n\nArgs:\n  name: str The tag name for the new attribute. This must be a valid xml\n    tag name. The tag will be placed in the GBase namespace.\n  value: str Contents for the item attribute\n  value_type: str (optional) The type of data in the vlaue, Examples: text\n      float\n  access: str (optional) Used to hide attributes. The attribute is not \n      exposed in the snippets feed if access is set to 'private'.\n\"\"\"\n\n", "func_signal": "def AddItemAttribute(self, name, value, value_type=None, access=None):\n", "code": "new_attribute =  ItemAttribute(name, text=value, \n    text_type=value_type, access=access)\nself.item_attributes.append(new_attribute)", "path": "gdata\\base\\__init__.py", "repo_name": "kevinmarks/addressbooker", "stars": 1, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Attempts to log in to Google APIs using the provided credentials.\n\nIf the username or password are None, the client will not request auth \ntokens.\n\nArgs:\n  username: str (optional) Example: jo@example.com\n  password: str (optional)\n\"\"\"\n", "func_signal": "def SetCredentials(self, username, password):\n", "code": "self.__docs_client.email = username\nself.__docs_client.password = password\nself.__spreadsheets_client.email = username\nself.__spreadsheets_client.password = password\nif username and password:\n  try:\n    self.__docs_client.ProgrammaticLogin()\n    self.__spreadsheets_client.ProgrammaticLogin()\n  except gdata.service.CaptchaRequired:\n    raise CaptchaRequired('Please visit https://www.google.com/accounts/'\n                        'DisplayUnlockCaptcha to unlock your account.')\n  except gdata.service.BadAuthentication:\n    raise BadCredentials('Username or password incorrect.')", "path": "gdata\\spreadsheet\\text_db.py", "repo_name": "kevinmarks/addressbooker", "stars": 1, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Removes a global handler function.\n\nArguments:\n\n    event -- Event type (a string).\n\n    handler -- Callback function.\n\nReturns 1 on success, otherwise 0.\n\"\"\"\n", "func_signal": "def remove_global_handler(self, event, handler):\n", "code": "if not event in self.handlers:\n    return 0\nfor h in self.handlers[event]:\n    if handler == h[1]:\n        self.handlers[event].remove(h)\nreturn 1", "path": "birclib.py", "repo_name": "codelurker/bfirc", "stars": 1, "license": "None", "language": "python", "size": 175}
{"docstring": "\"\"\"Get the (real) server name.\n\nThis method returns the (real) server name, or, more\nspecifically, what the server calls itself.\n\"\"\"\n\n", "func_signal": "def get_server_name(self):\n", "code": "if self.real_server_name:\n    return self.real_server_name\nelse:\n    return \"\"", "path": "birclib.py", "repo_name": "codelurker/bfirc", "stars": 1, "license": "None", "language": "python", "size": 175}
{"docstring": "\"\"\"Creates and returns a DCCConnection object.\n\nArguments:\n\n    dcctype -- \"chat\" for DCC CHAT connections or \"raw\" for\n               DCC SEND (or other DCC types). If \"chat\",\n               incoming data will be split in newline-separated\n               chunks. If \"raw\", incoming data is not touched.\n\"\"\"\n", "func_signal": "def dcc(self, dcctype=\"chat\"):\n", "code": "c = DCCConnection(self, dcctype)\nself.connections.append(c)\nreturn c", "path": "birclib.py", "repo_name": "codelurker/bfirc", "stars": 1, "license": "None", "language": "python", "size": 175}
{"docstring": "\"\"\"[Internal]\"\"\"\n", "func_signal": "def _remove_connection(self, connection):\n", "code": "self.connections.remove(connection)\nif self.fn_to_remove_socket:\n    self.fn_to_remove_socket(connection._get_socket())", "path": "birclib.py", "repo_name": "codelurker/bfirc", "stars": 1, "license": "None", "language": "python", "size": 175}
{"docstring": "\"\"\"[Internal]\"\"\"\n", "func_signal": "def _parse_modes(mode_string, unary_modes=\"\"):\n", "code": "modes = []\narg_count = 0\n\n# State variable.\nsign = \"\"\n\na = mode_string.split()\nif len(a) == 0:\n    return []\nelse:\n    mode_part, args = a[0], a[1:]\n\nif mode_part[0] not in \"+-\":\n    return []\nfor ch in mode_part:\n    if ch in \"+-\":\n        sign = ch\n    elif ch == \" \":\n        collecting_arguments = 1\n    elif ch in unary_modes:\n        if len(args) >= arg_count + 1:\n            modes.append([sign, ch, args[arg_count]])\n            arg_count = arg_count + 1\n        else:\n            modes.append([sign, ch, None])\n    else:\n        modes.append([sign, ch, None])\nreturn modes", "path": "birclib.py", "repo_name": "codelurker/bfirc", "stars": 1, "license": "None", "language": "python", "size": 175}
{"docstring": "\"\"\"Convert an IP address string (e.g. '192.168.0.1') to an IP\nnumber as an integer given in ASCII representation\n(e.g. '3232235521').\"\"\"\n", "func_signal": "def ip_quad_to_numstr(quad):\n", "code": "p = map(long, quad.split(\".\"))\ns = str((p[0] << 24) | (p[1] << 16) | (p[2] << 8) | p[3])\nif s[-1] == \"L\":\n    s = s[:-1]\nreturn s", "path": "birclib.py", "repo_name": "codelurker/bfirc", "stars": 1, "license": "None", "language": "python", "size": 175}
{"docstring": "\"\"\"Listen for connections from a DCC peer.\n\nReturns a DCCConnection instance.\n\"\"\"\n", "func_signal": "def dcc_listen(self, dcctype=\"chat\"):\n", "code": "dcc = self.ircobj.dcc(dcctype)\nself.dcc_connections.append(dcc)\ndcc.listen()\nreturn dcc", "path": "birclib.py", "repo_name": "codelurker/bfirc", "stars": 1, "license": "None", "language": "python", "size": 175}
{"docstring": "\"\"\"Send a PART command.\"\"\"\n", "func_signal": "def part(self, channels, message=\"\"):\n", "code": "if type(channels) == types.StringType:\n    self.send_raw(\"PART \" + channels + (message and (\" \" + message)))\nelse:\n    self.send_raw(\"PART \" + \",\".join(channels) + (message and (\" \" + message)))", "path": "birclib.py", "repo_name": "codelurker/bfirc", "stars": 1, "license": "None", "language": "python", "size": 175}
{"docstring": "\"\"\"Process data from connections once.\n\nArguments:\n\n    timeout -- How long the select() call should wait if no\n               data is available.\n\nThis method should be called periodically to check and process\nincoming data, if there are any.  If that seems boring, look\nat the process_forever method.\n\"\"\"\n", "func_signal": "def process_once(self, timeout=0):\n", "code": "sockets = map(lambda x: x._get_socket(), self.connections)\nsockets = filter(lambda x: x != None, sockets)\nif sockets:\n    (i, o, e) = select.select(sockets, [], [], timeout)\n    self.process_data(i)\nelse:\n    time.sleep(timeout)\nself.process_timeout()", "path": "birclib.py", "repo_name": "codelurker/bfirc", "stars": 1, "license": "None", "language": "python", "size": 175}
{"docstring": "\"\"\"Adds a global handler function for a specific event type.\n\nArguments:\n\n    event -- Event type (a string).  Check the values of the\n    numeric_events dictionary in irclib.py for possible event\n    types.\n\n    handler -- Callback function.\n\n    priority -- A number (the lower number, the higher priority).\n\nThe handler function is called whenever the specified event is\ntriggered in any of the connections.  See documentation for\nthe Event class.\n\nThe handler functions are called in priority order (lowest\nnumber is highest priority).  If a handler function returns\n\\\"NO MORE\\\", no more handlers will be called.\n\"\"\"\n\n", "func_signal": "def add_global_handler(self, event, handler, priority=0):\n", "code": "if not event in self.handlers:\n    self.handlers[event] = []\nbisect.insort(self.handlers[event], ((priority, handler)))", "path": "birclib.py", "repo_name": "codelurker/bfirc", "stars": 1, "license": "None", "language": "python", "size": 175}
{"docstring": "\"\"\"Creates and returns a ServerConnection object.\"\"\"\n\n", "func_signal": "def server(self):\n", "code": "c = ServerConnection(self)\nself.connections.append(c)\nreturn c", "path": "birclib.py", "repo_name": "codelurker/bfirc", "stars": 1, "license": "None", "language": "python", "size": 175}
{"docstring": "\"\"\"Send an SCONNECT command.\"\"\"\n", "func_signal": "def sconnect(self, target, port=\"\", server=\"\"):\n", "code": "self.send_raw(\"CONNECT %s%s%s\" % (target,\n                                  port and (\" \" + port),\n                                  server and (\" \" + server)))", "path": "birclib.py", "repo_name": "codelurker/bfirc", "stars": 1, "license": "None", "language": "python", "size": 175}
{"docstring": "\"\"\"Execute a function after a specified time.\n\nArguments:\n\n    delay -- How many seconds to wait.\n\n    function -- Function to call.\n\n    arguments -- Arguments to give the function.\n\"\"\"\n", "func_signal": "def execute_delayed(self, delay, function, arguments=()):\n", "code": "bisect.insort(self.delayed_commands, (delay+time.time(), function, arguments))\nif self.fn_to_add_timeout:\n    self.fn_to_add_timeout(delay)", "path": "birclib.py", "repo_name": "codelurker/bfirc", "stars": 1, "license": "None", "language": "python", "size": 175}
{"docstring": "\"\"\"Called when a timeout notification is due.\n\nSee documentation for IRC.__init__.\n\"\"\"\n", "func_signal": "def process_timeout(self):\n", "code": "t = time.time()\nwhile self.delayed_commands:\n    if t >= self.delayed_commands[0][0]:\n        self.delayed_commands[0][1](*self.delayed_commands[0][2])\n        del self.delayed_commands[0]\n    else:\n        break", "path": "birclib.py", "repo_name": "codelurker/bfirc", "stars": 1, "license": "None", "language": "python", "size": 175}
{"docstring": "\"\"\"Send a CTCP command.\"\"\"\n", "func_signal": "def ctcp(self, ctcptype, target, parameter=\"\"):\n", "code": "ctcptype = ctcptype.upper()\nself.privmsg(target, \"\\001%s%s\\001\" % (ctcptype, parameter and (\" \" + parameter) or \"\"))", "path": "birclib.py", "repo_name": "codelurker/bfirc", "stars": 1, "license": "None", "language": "python", "size": 175}
{"docstring": "\"\"\"Send a LINKS command.\"\"\"\n", "func_signal": "def links(self, remote_server=\"\", server_mask=\"\"):\n", "code": "command = \"LINKS\"\nif remote_server:\n    command = command + \" \" + remote_server\nif server_mask:\n    command = command + \" \" + server_mask\nself.send_raw(command)", "path": "birclib.py", "repo_name": "codelurker/bfirc", "stars": 1, "license": "None", "language": "python", "size": 175}
{"docstring": "\"\"\"Run an infinite loop, processing data from connections.\n\nThis method repeatedly calls process_once.\n\nArguments:\n\n    timeout -- Parameter to pass to process_once.\n\"\"\"\n", "func_signal": "def process_forever(self, timeout=0.2):\n", "code": "while 1:\n    self.process_once(timeout)", "path": "birclib.py", "repo_name": "codelurker/bfirc", "stars": 1, "license": "None", "language": "python", "size": 175}
{"docstring": "\"\"\"Connect to a DCC peer.\n\nArguments:\n\n    address -- IP address of the peer.\n\n    port -- Port to connect to.\n\nReturns a DCCConnection instance.\n\"\"\"\n", "func_signal": "def dcc_connect(self, address, port, dcctype=\"chat\"):\n", "code": "dcc = self.ircobj.dcc(dcctype)\nself.dcc_connections.append(dcc)\ndcc.connect(address, port)\nreturn dcc", "path": "birclib.py", "repo_name": "codelurker/bfirc", "stars": 1, "license": "None", "language": "python", "size": 175}
{"docstring": "\"\"\"Send a LIST command.\"\"\"\n", "func_signal": "def list(self, channels=None, server=\"\"):\n", "code": "command = \"LIST\"\nif channels:\n    command = command + \" \" + \",\".join(channels)\nif server:\n    command = command + \" \" + server\nself.send_raw(command)", "path": "birclib.py", "repo_name": "codelurker/bfirc", "stars": 1, "license": "None", "language": "python", "size": 175}
{"docstring": "\"\"\"Send a TOPIC command.\"\"\"\n", "func_signal": "def topic(self, channel, new_topic=None):\n", "code": "if new_topic is None:\n    self.send_raw(\"TOPIC \" + channel)\nelse:\n    self.send_raw(\"TOPIC %s :%s\" % (channel, new_topic))", "path": "birclib.py", "repo_name": "codelurker/bfirc", "stars": 1, "license": "None", "language": "python", "size": 175}
{"docstring": "\"\"\"Request connection tear-down.\"\"\"\n\n", "func_signal": "def Disconnect(self):\n", "code": "\n\nself._manager.connection_disconnect(self)\nself.StatusChanged(tp.constants.CONNECTION_STATUS_DISCONNECTED,\n                   tp.constants.CONNECTION_STATUS_REASON_REQUESTED)\nreactor.stop()", "path": "tlen\\connection\\__init__.py", "repo_name": "kkszysiu/telepathy-oxygen", "stars": 1, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Request connection establishment.\"\"\"\n\n", "func_signal": "def Connect(self):\n", "code": "\nreactor.connectTCP('s1.tlen.pl', 443, self.factory)\nself.StatusChanged(tp.CONNECTION_STATUS_CONNECTING,\n                tp.CONNECTION_STATUS_REASON_REQUESTED)\nself._manager.connection_connect(self)\n\nself.factory.addBootstrap('/*', self.lg)\nself.factory.addBootstrap('/presence', self._on_presence_changed)\nself.factory.addBootstrap('/iq[@type=\"result\" and @id=\"GetRoster\"]/*', self._on_roster_received)\nself.factory.addBootstrap('/message/*', self._on_msg_received)\nself.factory.addBootstrap(xmlstream.INIT_FAILED_EVENT, self.err)\n#drobny hack. Ustawia tatus na niewidoczny\nself.factory.sendStanza(self._stanzas['get_roster'])", "path": "tlen\\connection\\__init__.py", "repo_name": "kkszysiu/telepathy-oxygen", "stars": 1, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Set the contact's avatar image path and generate derived\nattributes based on the image itself.\n\nArguments:\navatar_path -- path of the image file (absolute or relative)\npublish -- publish the new avatar immediately (default: True)\n\nExceptions:\norg.freedesktop.Telepathy.Error.InvalidArgument\n\"\"\"\n", "func_signal": "def set_avatar(self, avatar_path, publish=True):\n", "code": "avatar_bin = ''\n\n# will raise tp.errors.InvalidArgument in case of error\navatar_mime = tlen.common.image_filename_to_mime_type(avatar_path)\n\ntry:\n    avatar_file = open(avatar_path, 'r')\n    avatar_bin = avatar_file.read()\n    avatar_file.close()\nexcept IOError:\n    raise tp.errors.InvalidArgument('failed to open avatar file: %s'\n                                    % avatar_path)\n\nif avatar_bin:\n    md5gen = hashlib.md5()\n    md5gen.update(avatar_bin)\n    self._extended_attrs['avatar_bin'] = avatar_bin\n    self._extended_attrs['avatar_mime'] = avatar_mime\n    # FIXME: if the token for this new avatar is the same as the old\n    # one, exit early (and avoid emitting signals)\n    self._extended_attrs['avatar_token'] = md5gen.hexdigest()\n    self._extended_attrs['avatar_path'] = avatar_path\nelse:\n    self._extended_attrs['avatar_token'] = \\\n                            self._extended_attr_defaults['avatar_token']\n    self._extended_attrs['avatar_path'] = \\\n                            self._extended_attr_defaults['avatar_path']\n\nif publish:\n    if self._last_avatar_token_published != \\\n                                self._extended_attrs['avatar_token']:\n\n        self._connection.AvatarUpdated(\n                                self.get_id(),\n                                self._extended_attrs['avatar_token'])\n        self._last_avatar_token_published = \\\n                                self._extended_attrs['avatar_token']", "path": "tlen\\server\\__init__.py", "repo_name": "kkszysiu/telepathy-oxygen", "stars": 1, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Treat this connection as if it disconnected from a real server.\n\nArguments:\nconnection -- connection to no longer track\n\"\"\"\n\n", "func_signal": "def connection_disconnect(self, connection):\n", "code": "\n\nif connection in self._connections:\n    self._connections.remove(connection)", "path": "tlen\\connection_manager.py", "repo_name": "kkszysiu/telepathy-oxygen", "stars": 1, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"\nArguments:\nconnection -- connection this StoredContactList corresponds to\nchannel_handle_obj -- handle object of the channel this list maps to\n\nExceptions:\nIOError -- failed to read contact list file\nIndexError -- contact list file parsing failed\n\"\"\"\n", "func_signal": "def __init__(self, connection, channel_handle_obj):\n", "code": "StoredList.__init__(self, connection, channel_handle_obj,\n                    'contact_lists', 'list')", "path": "tlen\\server\\__init__.py", "repo_name": "kkszysiu/telepathy-oxygen", "stars": 1, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Returns the presence of the given contacts.\n\nArguments:\ncontacts -- iterable of contacts whose presence is requested\n\nReturns:\npresences -- complex list of structs containing the presences\n\nExceptions:\norg.freedesktop.Telepathy.Error.Disconnected\norg.freedesktop.Telepathy.Error.InvalidArgument\norg.freedesktop.Telepathy.Error.InvalidHandle\norg.freedesktop.Telepathy.Error.NetworkError\norg.freedesktop.Telepathy.Error.NotAvailable\n\"\"\"\n", "func_signal": "def GetPresences(self, contacts):\n", "code": "presences = {}\nfor handle_id in contacts:\n    self.check_handle (tp.constants.HANDLE_TYPE_CONTACT, handle_id)\n\n    handle_obj = self._handles[tp.constants.HANDLE_TYPE_CONTACT,\n                               handle_id]\n    presences[handle_id] = handle_obj.get_simple_presence()\n\nreturn presences", "path": "tlen\\bak\\simple_presence.py", "repo_name": "kkszysiu/telepathy-oxygen", "stars": 1, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Request the aliases of any number of contacts.\n\nArguments:\nalias_map -- dictionary mapping contact handles to new aliases\n\nExceptions:\norg.freedesktop.Telepathy.Error.Disconnected\norg.freedesktop.Telepathy.Error.NetworkError\norg.freedesktop.Telepathy.Error.NotAvailable\norg.freedesktop.Telepathy.Error.InvalidArgument\norg.freedesktop.Telepathy.Error.PermissionDenied\n\"\"\"\n", "func_signal": "def SetAliases(self, alias_map):\n", "code": "alias_pairs = []\nfor handle_id in alias_map.keys():\n    self.check_handle (tp.constants.HANDLE_TYPE_CONTACT, handle_id)\n\n    for handle_obj in self._handles.values():\n        if handle_obj.get_id() == handle_id:\n            handle_obj.set_alias(alias_map[handle_id])\n\n            alias_pairs.append((handle_obj.get_id(),\n                               alias_map[handle_id]))\nif len(alias_pairs) >= 1:\n    self.AliasesChanged(alias_pairs)\n    self.save()", "path": "tlen\\connection\\aliasing.py", "repo_name": "kkszysiu/telepathy-oxygen", "stars": 1, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Remove list of contacts from this group.\n\nArguments:\ncontacts -- list of contact handles to add\nmessage -- message to send to server along with request (if supported)\n\"\"\"\n", "func_signal": "def RemoveMembers(self, contacts, message):\n", "code": "self.RemoveMembersWithReason(\n                        contacts, message,\n                        tp.constants.CHANNEL_GROUP_CHANGE_REASON_NONE)", "path": "tlen\\channel\\contact_list.py", "repo_name": "kkszysiu/telepathy-oxygen", "stars": 1, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Request the aliases of any number of contacts.\n\nArguments:\ncontacts -- iterable of contacts whose aliases are requested\n\nReturns:\naliases -- list of requested aliases in the same order\n\nExceptions:\norg.freedesktop.Telepathy.Error.Disconnected\norg.freedesktop.Telepathy.Error.NetworkError\norg.freedesktop.Telepathy.Error.InvalidHandle\norg.freedesktop.Telepathy.Error.NotAvailable\n\"\"\"\n", "func_signal": "def RequestAliases(self, contacts):\n", "code": "aliases = []\nalias_pairs = []\nfor handle_id in contacts:\n    self.check_handle (tp.constants.HANDLE_TYPE_CONTACT, handle_id)\n\n    handle_obj = self._handles[tp.constants.HANDLE_TYPE_CONTACT,\n                               handle_id]\n\n    aliases.append(handle_obj.get_alias())\n    alias_pairs.append((handle_id, aliases[-1]))\n\nreturn aliases", "path": "tlen\\connection\\aliasing.py", "repo_name": "kkszysiu/telepathy-oxygen", "stars": 1, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Returns a handle ID for the given type and name, creating a new\nhandle ID only as necessary (similar to RequestHandles' definition in\nthe specification).\n\nArguments:\nhandle_type -- Telepathy Handle_Type for all the handles\nname -- username for the contact\n\nReturns:\nhandle_id -- ID for the given username\nis_new -- True if the ID was created (did not exist)\n\"\"\"\n\n", "func_signal": "def get_handle_id_idempotent(self, handle_type, name):\n", "code": "is_new = False\nhandle_id = 0\nfor handle in self._handles.values():\n    if handle.get_name() == name:\n        handle_id = handle.get_id()\n        break\n\n# if the handle doesn't already exist, create a new one\nif handle_id <= 0:\n    handle_id = self.get_handle_id()\n    is_new = True", "path": "tlen\\connection\\__init__.py", "repo_name": "kkszysiu/telepathy-oxygen", "stars": 1, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Writes the current contact list, group, and contact state out to a\nnew contacts file.\n\"\"\"\n\n", "func_signal": "def save(self):\n", "code": "\n\ndom_impl = minidom.getDOMImplementation()\nxml_doc = dom_impl.createDocument(None, 'roster', None)\nroster_xml = xml_doc.documentElement\n\n# add newline for human-readability\nnewline_value = xml_doc.createTextNode('\\n')\nroster_xml.appendChild(newline_value)\n\ncontact_channels_map = self.get_contact_channel_membership_info()\nfor handle_obj, lists_groups in contact_channels_map.items():\n    contact_lists, groups = lists_groups\n\n    contact_xml = handle_obj.get_xml(contact_lists, groups)\n    roster_xml.appendChild(contact_xml)\n\n    # add newline for human-readability\n    newline_value = xml_doc.createTextNode('\\n\\n')\n    roster_xml.appendChild(newline_value)\n\n\naccount_id = self.get_account_id()\npin.common.save_roster(xml_doc, account_id)", "path": "tlen\\connection\\__init__.py", "repo_name": "kkszysiu/telepathy-oxygen", "stars": 1, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Returns flag values specifying the behavior of aliases on this\nconnection.\n\nReturns:\nflags -- bitwise OR of Connection_Alias_Flags values specifying the\n         behavior of aliases on this connection\n\nExceptions:\norg.freedesktop.Telepathy.Error.Disconnected\n\"\"\"\n", "func_signal": "def GetAliasFlags(self):\n", "code": "flags = 0\nflags |= tp.constants.CONNECTION_ALIAS_FLAG_USER_SET\n\nreturn flags", "path": "tlen\\bak\\aliasing.py", "repo_name": "kkszysiu/telepathy-oxygen", "stars": 1, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Request the aliases of any number of contacts.\n\nArguments:\nalias_map -- dictionary mapping contact handles to new aliases\n\nExceptions:\norg.freedesktop.Telepathy.Error.Disconnected\norg.freedesktop.Telepathy.Error.NetworkError\norg.freedesktop.Telepathy.Error.NotAvailable\norg.freedesktop.Telepathy.Error.InvalidArgument\norg.freedesktop.Telepathy.Error.PermissionDenied\n\"\"\"\n", "func_signal": "def SetAliases(self, alias_map):\n", "code": "alias_pairs = []\nfor handle_id in alias_map.keys():\n    self.check_handle (tp.constants.HANDLE_TYPE_CONTACT, handle_id)\n\n    for handle_obj in self._handles.values():\n        if handle_obj.get_id() == handle_id:\n            handle_obj.set_alias(alias_map[handle_id])\n\n            alias_pairs.append((handle_obj.get_id(),\n                               alias_map[handle_id]))\nif len(alias_pairs) >= 1:\n    self.AliasesChanged(alias_pairs)\n    self.save()", "path": "tlen\\bak\\aliasing.py", "repo_name": "kkszysiu/telepathy-oxygen", "stars": 1, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Add list of contacts to this group.\n\nArguments:\ncontacts -- list of contact handles to add\nmessage -- message to send to server along with request (if supported)\n\"\"\"\n", "func_signal": "def AddMembers(self, contacts, message):\n", "code": "handle_type = tp.constants.HANDLE_TYPE_CONTACT\n\nfor handle_id in contacts:\n    if (handle_type, handle_id) not in self.parent_connection._handles:\n        raise tp.errors.InvalidHandle('unknown contact handle %d' % \\\n                                      handle_id)\n\nconn_handles = self.parent_connection._handles\nhandle_objs = set([conn_handles[tp.constants.HANDLE_TYPE_CONTACT, x]\n                   for x in contacts])\n\nself.MembersChanged(message, handle_objs, (), (), (),\n                    self.parent_connection._self_handle,\n                    tp.constants.CHANNEL_GROUP_CHANGE_REASON_NONE)\n\nself.parent_connection.save()", "path": "tlen\\channel\\contact_list.py", "repo_name": "kkszysiu/telepathy-oxygen", "stars": 1, "license": "None", "language": "python", "size": 108}
{"docstring": "#        self._conversation.leave()\n#        self._chan_manager_ref().remove_text_channel(self)\n", "func_signal": "def Close(self):\n", "code": "        tp.server.ChannelTypeText.Close(self)\n        self.remove_from_connection()\n\n    # Redefine GetSelfHandle since we use our own handle\n    #  as Butterfly doesn't have channel specific handles\n#    def GetSelfHandle(self):\n#        return self.connection.self_handle\n\n    # papyon.event.ConversationEventInterface\n#    def on_conversation_user_joined(self, contact):\n#        handle = ButterflyHandleFactory(self._conn_ref(), 'contact',\n#                contact.account, contact.network_id)\n#        logger.info(\"User %r joined\" % handle)\n#        self.MembersChanged('', [handle], [], [], [],\n#                handle, tp.CHANNEL_GROUP_CHANGE_REASON_INVITED)\n#\n#    # papyon.event.ConversationEventInterface\n#    def on_conversation_user_left(self, contact):\n#        handle = ButterflyHandleFactory(self._conn_ref(), 'contact',\n#                contact.account, contact.network_id)\n#        logger.info(\"User %r left\" % handle)\n#        if len(self._members) == 1:\n#            self.ChatStateChanged(handle, tp.CHANNEL_CHAT_STATE_GONE)\n#        else:\n#            self.MembersChanged('', [], [handle], [], [],\n#                    handle, tp.CHANNEL_GROUP_CHANGE_REASON_NONE)\n#\n#    # papyon.event.ConversationEventInterface\n#    def on_conversation_user_typing(self, contact):\n#        handle = ButterflyHandleFactory(self._conn_ref(), 'contact',\n#                contact.account, contact.network_id)\n#        logger.info(\"User %r is typing\" % handle)\n#        self.ChatStateChanged(handle, tp.CHANNEL_CHAT_STATE_COMPOSING)\n#\n#    # papyon.event.ConversationEventInterface\n#    def on_conversation_message_received(self, sender, message):\n#        id = self._recv_id\n#        timestamp = int(time.time())\n#        handle = ButterflyHandleFactory(self._conn_ref(), 'contact',\n#                sender.account, sender.network_id)\n#        type = tp.CHANNEL_TEXT_MESSAGE_TYPE_NORMAL\n#        message = message.content\n#        logger.info(\"User %r sent a message\" % handle)\n#        self.Received(id, timestamp, handle, type, 0, message)\n#        self._recv_id += 1\n#\n#    # papyon.event.ConversationEventInterface\n#    def on_conversation_nudge_received(self, sender):\n#        id = self._recv_id\n#        timestamp = int(time.time())\n#        handle = ButterflyHandleFactory(self._conn_ref(), 'contact',\n#                sender.account, sender.network_id)\n#        type = tp.CHANNEL_TEXT_MESSAGE_TYPE_ACTION\n#        text = unicode(_(\"sends you a nudge\"), \"utf-8\")\n#        logger.info(\"User %r sent a nudge\" % handle)\n#        self.Received(id, timestamp, handle, type, 0, text)\n#        self._recv_id += 1\n#\n#    @async\n#    def __add_initial_participants(self):\n#        handles = []\n#        handles.append(self.connection.self_handle)\n#        for participant in self._conversation.participants:\n#            handle = ButterflyHandleFactory(self._conn_ref(), 'contact',\n#                    participant.account, participant.network_id)\n#            handles.append(handle)\n#        self.MembersChanged('', handles, [], [], [],\n#                0, tp.CHANNEL_GROUP_CHANGE_REASON_NONE)\n\n        #        id = self._recv_id\n#        timestamp = int(time.time())\n#        handle = ButterflyHandleFactory(self._conn_ref(), 'contact',\n#                sender.account, sender.network_id)\n#        type = tp.CHANNEL_TEXT_MESSAGE_TYPE_NORMAL\n#        message = message.content\n#        logger.info(\"User %r sent a message\" % handle)\n#        self.Received(id, timestamp, handle, type, 0, message)\n#        self._recv_id += 1", "path": "tlen\\channel\\text.py", "repo_name": "kkszysiu/telepathy-oxygen", "stars": 1, "license": "None", "language": "python", "size": 108}
{"docstring": "# try current symbol locations first\n", "func_signal": "def __init__(self):\n", "code": "\ntry:\n    from dbus.bus import NAME_FLAG_DO_NOT_QUEUE \\\n            as NAME_FLAG_DO_NOT_QUEUE\n    from dbus.bus import REQUEST_NAME_REPLY_EXISTS \\\n            as REQUEST_NAME_REPLY_EXISTS\n# fall back to older locations for older versions of dbus-python\nexcept ImportError:\n    from _dbus_bindings import NAME_FLAG_DO_NOT_QUEUE \\\n            as NAME_FLAG_DO_NOT_QUEUE\n    from _dbus_bindings import REQUEST_NAME_REPLY_EXISTS \\\n            as REQUEST_NAME_REPLY_EXISTS\n\ntp.server.ConnectionManager.__init__(self, 'oxygen')\n\nself._protos[tlen.common.PROTO_DEFAULT] = tlen.connection.Connection\n\nbus = dbus.SessionBus()\n\nrv = bus.request_name(tlen.common.CM_TLEN, NAME_FLAG_DO_NOT_QUEUE)\nif rv == REQUEST_NAME_REPLY_EXISTS:\n    raise dbus.NameExistsException (tlen.common.CM_TLEN)", "path": "tlen\\connection_manager.py", "repo_name": "kkszysiu/telepathy-oxygen", "stars": 1, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Returns flag values specifying the behavior of aliases on this\nconnection.\n\nReturns:\nflags -- bitwise OR of Connection_Alias_Flags values specifying the\n         behavior of aliases on this connection\n\nExceptions:\norg.freedesktop.Telepathy.Error.Disconnected\n\"\"\"\n", "func_signal": "def GetAliasFlags(self):\n", "code": "flags = 0\nflags |= tp.constants.CONNECTION_ALIAS_FLAG_USER_SET\n\nreturn flags", "path": "tlen\\connection\\aliasing.py", "repo_name": "kkszysiu/telepathy-oxygen", "stars": 1, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Reverts to the default contact list for this account and disconnects\nthis Connection. The next instantiation of a Connection for this account\nwill use the default contact list.\n\nNote that disconnecting the Connection will affect any other clients\nusing this Connection.\n\"\"\"\n\n", "func_signal": "def reset_to_default_contacts_file(self):\n", "code": "\n\nself.Disconnect()\n\n# clear out any modified version of the contact list (upon next\n# Connect(), the default contact list will be read in)\naccount_id = self.get_account_id()\nfilename = pin.common.get_contacts_file(account_id,\n                                        pin.common.PREFIX_SAVED)\nif os.path.isfile(filename):\n    os.remove(filename)", "path": "tlen\\connection\\__init__.py", "repo_name": "kkszysiu/telepathy-oxygen", "stars": 1, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"\nArguments:\nconnection -- connection this StoredGroup corresponds to\nchannel_handle_obj -- handle object of the channel this list maps to\n\nExceptions:\nIOError -- failed to read contact list file\nIndexError -- contact list file parsing failed\n\"\"\"\n", "func_signal": "def __init__(self, connection, channel_handle_obj):\n", "code": "StoredList.__init__(self, connection, channel_handle_obj,\n                    'groups', 'group')", "path": "tlen\\server\\__init__.py", "repo_name": "kkszysiu/telepathy-oxygen", "stars": 1, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Request the aliases of any number of contacts.\n\nArguments:\ncontacts -- iterable of contacts whose aliases are requested\n\nReturns:\naliases -- list of requested aliases in the same order\n\nExceptions:\norg.freedesktop.Telepathy.Error.Disconnected\norg.freedesktop.Telepathy.Error.NetworkError\norg.freedesktop.Telepathy.Error.InvalidHandle\norg.freedesktop.Telepathy.Error.NotAvailable\n\"\"\"\n", "func_signal": "def RequestAliases(self, contacts):\n", "code": "aliases = []\nalias_pairs = []\nfor handle_id in contacts:\n    self.check_handle (tp.constants.HANDLE_TYPE_CONTACT, handle_id)\n\n    handle_obj = self._handles[tp.constants.HANDLE_TYPE_CONTACT,\n                               handle_id]\n\n    aliases.append(handle_obj.get_alias())\n    alias_pairs.append((handle_id, aliases[-1]))\n\nreturn aliases", "path": "tlen\\bak\\aliasing.py", "repo_name": "kkszysiu/telepathy-oxygen", "stars": 1, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Posts the given comment to FriendFeed.\n\nSee http://friendfeed.com/api/documentation#write_comment.\nAuthentication is required for this method.\n\"\"\"\n", "func_signal": "def post_comment(self, entry, body, **args):\n", "code": "args.update(entry=entry, body=body)\nreturn self.fetch(\"/comment\", post_args=args)", "path": "friendfeed.py", "repo_name": "kevinmarks/Friendlier-Feed", "stars": 1, "license": "None", "language": "python", "size": 84}
{"docstring": "\"\"\"Returns the Unauthorized Request Token URL for FriendFeed.\n\nSee http://oauth.net/core/1.0/#auth_step1\n\"\"\"\n", "func_signal": "def get_oauth_request_token_url(consumer_token):\n", "code": "url = _FRIENDFEED_OAUTH_BASE + \"/request_token\"\nargs = dict(\n    oauth_consumer_key=consumer_token[\"key\"],\n    oauth_signature_method=\"HMAC-SHA1\",\n    oauth_timestamp=str(int(time.time())),\n    oauth_nonce=binascii.b2a_hex(uuid.uuid4().bytes),\n    oauth_version=\"1.0\",\n)\nsignature = _oauth_signature(consumer_token, \"GET\", url, args)\nargs[\"oauth_signature\"] = signature\nreturn url + \"?\" + urllib.urlencode(args)", "path": "friendfeed.py", "repo_name": "kevinmarks/Friendlier-Feed", "stars": 1, "license": "None", "language": "python", "size": 84}
{"docstring": "\"\"\"Hides the given entry from the authenticated user's FriendFeed.\n\nSee http://friendfeed.com/api/documentation#write_hide.\nAuthentication is required for this method.\n\"\"\"\n", "func_signal": "def hide_entry(self, entry, **args):\n", "code": "args.update(entry=entry)\nreturn self.fetch(\"/hide\", post_args=args)", "path": "friendfeed.py", "repo_name": "kevinmarks/Friendlier-Feed", "stars": 1, "license": "None", "language": "python", "size": 84}
{"docstring": "\"\"\"Returns the installed application Access Token URL for the \ngiven username and password.\n\nSee http://friendfeed.com/api/documentation#authentication\n\"\"\"\n", "func_signal": "def get_installed_app_access_token_url(consumer_token, username, password):\n", "code": "url = _FRIENDFEED_OAUTH_BASE + \"/ia_access_token\"\nargs = dict(\n    oauth_consumer_key=consumer_token[\"key\"],\n    ff_username=username,\n    ff_password=password,\n    oauth_signature_method=\"HMAC-SHA1\",\n    oauth_timestamp=str(int(time.time())),\n    oauth_nonce=binascii.b2a_hex(uuid.uuid4().bytes),\n    oauth_version=\"1.0\",\n)\nsignature = _oauth_signature(consumer_token, \"GET\", url, args)\nargs[\"oauth_signature\"] = signature\nreturn url + \"?\" + urllib.urlencode(args)", "path": "friendfeed.py", "repo_name": "kevinmarks/Friendlier-Feed", "stars": 1, "license": "None", "language": "python", "size": 84}
{"docstring": "\"\"\"Posts the given like to FriendFeed.\n\nSee http://friendfeed.com/api/documentation#write_like.\nAuthentication is required for this method.\n\"\"\"\n", "func_signal": "def post_like(self, entry, **args):\n", "code": "args.update(entry=entry)\nreturn self.fetch(\"/like\", post_args=args)", "path": "friendfeed.py", "repo_name": "kevinmarks/Friendlier-Feed", "stars": 1, "license": "None", "language": "python", "size": 84}
{"docstring": "\"\"\"Updates the name and/or description of the given feed.\n\nIf feed_id is not specified, we update the profile of the\nauthenticated user.\nSee http://friendfeed.com/api/documentation#write_feedinfo.\n\"\"\"\n", "func_signal": "def edit_feed_info(self, feed=None, name=None, description=None, **args):\n", "code": "if feed: args.update(feed=feed)\nif name: args.update(name=name)\nif description: args.update(description=description)\nreturn self.fetch(\"/feedinfo\", post_args=args)", "path": "friendfeed.py", "repo_name": "kevinmarks/Friendlier-Feed", "stars": 1, "license": "None", "language": "python", "size": 84}
{"docstring": "\"\"\"Returns the Access Token URL for the given authorized request token.\n\nThe given request token must have been authorized by sending the user\nto the URL returned by get_oauth_authorization_url() before this URL\nis fetched.\n\nSee http://oauth.net/core/1.0/#auth_step3\n\"\"\"\n", "func_signal": "def get_oauth_access_token_url(consumer_token, request_token):\n", "code": "url = _FRIENDFEED_OAUTH_BASE + \"/access_token\"\nargs = dict(\n    oauth_consumer_key=consumer_token[\"key\"],\n    oauth_token=request_token[\"key\"],\n    oauth_signature_method=\"HMAC-SHA1\",\n    oauth_timestamp=str(int(time.time())),\n    oauth_nonce=binascii.b2a_hex(uuid.uuid4().bytes),\n    oauth_version=\"1.0\",\n)\nsignature = _oauth_signature(consumer_token, \"GET\", url, args,\n                            request_token)\nargs[\"oauth_signature\"] = signature\nreturn url + \"?\" + urllib.urlencode(args)", "path": "friendfeed.py", "repo_name": "kevinmarks/Friendlier-Feed", "stars": 1, "license": "None", "language": "python", "size": 84}
{"docstring": "\"\"\"Subscribes the authenticated user to the given feed.\n\nSee http://friendfeed.com/api/documentation#write_subscribe.\nAuthentication is required for this method.\n\"\"\"\n", "func_signal": "def subscribe(self, feed, **args):\n", "code": "args.update(feed=feed)\nreturn self.fetch(\"/subscribe\", post_args=args)", "path": "friendfeed.py", "repo_name": "kevinmarks/Friendlier-Feed", "stars": 1, "license": "None", "language": "python", "size": 84}
{"docstring": "\"\"\"Calculates the HMAC-SHA1 OAuth signature for the given request.\n\nSee http://oauth.net/core/1.0/#signing_process\n\"\"\"\n", "func_signal": "def _oauth_signature(consumer_token, method, url, parameters={}, token=None):\n", "code": "parts = urlparse.urlparse(url)\nscheme, netloc, path = parts[:3]\nnormalized_url = scheme.lower() + \"://\" + netloc.lower() + path\n\nbase_elems = []\nbase_elems.append(method.upper())\nbase_elems.append(normalized_url)\nbase_elems.append(\"&\".join(\"%s=%s\" % (k, _oauth_escape(str(v)))\n                           for k, v in sorted(parameters.items())))\nbase_string =  \"&\".join(_oauth_escape(e) for e in base_elems)\n\nkey_elems = [consumer_token[\"secret\"]]\nkey_elems.append(token[\"secret\"] if token else \"\")\nkey = \"&\".join(key_elems)\n\nhash = hmac.new(key, base_string, hashlib.sha1)\nreturn binascii.b2a_base64(hash.digest())[:-1]", "path": "friendfeed.py", "repo_name": "kevinmarks/Friendlier-Feed", "stars": 1, "license": "None", "language": "python", "size": 84}
{"docstring": "\"\"\"Fetches an access token for the given username and password.\n\nSee get_installed_app_access_token_url().\n\"\"\"\n", "func_signal": "def fetch_installed_app_access_token(consumer_token, username, password):\n", "code": "url = get_installed_app_access_token_url(consumer_token, username, password)\nrequest = urllib2.urlopen(url)\ntoken = _oauth_parse_response(request.read())\nrequest.close()\nreturn token", "path": "friendfeed.py", "repo_name": "kevinmarks/Friendlier-Feed", "stars": 1, "license": "None", "language": "python", "size": 84}
{"docstring": "\"\"\"Fetches a new, unauthorized request token from FriendFeed.\n\nSee get_oauth_request_token_url().\n\"\"\"\n", "func_signal": "def fetch_oauth_request_token(consumer_token):\n", "code": "url = get_oauth_request_token_url(consumer_token)\nrequest = urllib2.urlopen(url)\ntoken = _oauth_parse_response(request.read())\nrequest.close()\nreturn token", "path": "friendfeed.py", "repo_name": "kevinmarks/Friendlier-Feed", "stars": 1, "license": "None", "language": "python", "size": 84}
{"docstring": "\"\"\"Fetches an access token for the given authorized request token.\n\nSee get_oauth_access_token_url().\n\"\"\"\n", "func_signal": "def fetch_oauth_access_token(consumer_token, request_token):\n", "code": "url = get_oauth_access_token_url(consumer_token, request_token)\nrequest = urllib2.urlopen(url)\ntoken = _oauth_parse_response(request.read())\nrequest.close()\nreturn token", "path": "friendfeed.py", "repo_name": "kevinmarks/Friendlier-Feed", "stars": 1, "license": "None", "language": "python", "size": 84}
{"docstring": "\"\"\"Returns the FriendFeed authorization URL for the given request token.\n\nThe user should be directed to this URL to authorize a request token.\nAfter the user authorizes a token, the user will be redirected to the\ncallback URL you specified when you registered your FriendFeed API\napplication at http://friendfeed.com/api/register. FriendFeed does\nnot support the oauth_callback argument.\n\nSee http://oauth.net/core/1.0/#auth_step2\n\"\"\"\n", "func_signal": "def get_oauth_authorization_url(request_token):\n", "code": "return _FRIENDFEED_OAUTH_BASE + \"/authorize?\" + \\\n    urllib.urlencode(dict(oauth_token=request_token[\"key\"]))", "path": "friendfeed.py", "repo_name": "kevinmarks/Friendlier-Feed", "stars": 1, "license": "None", "language": "python", "size": 84}
{"docstring": "\"\"\"Returns the FriendFeed authentication URL for the given request token.\n\nThe user should be directed to this URL to authorize a request token.\nAfter the user authorizes a token, the user will be redirected to the\ncallback URL you specified when you registered your FriendFeed API\napplication at http://friendfeed.com/api/register. FriendFeed does\nnot support the oauth_callback argument.\n\nSee http://oauth.net/core/1.0/#auth_step2\n\"\"\"\n", "func_signal": "def get_oauth_authentication_url(request_token):\n", "code": "return _FRIENDFEED_OAUTH_BASE + \"/authenticate?\" + \\\n    urllib.urlencode(dict(oauth_token=request_token[\"key\"]))", "path": "friendfeed.py", "repo_name": "kevinmarks/Friendlier-Feed", "stars": 1, "license": "None", "language": "python", "size": 84}
{"docstring": "\"\"\"Edits the given properties on the entry with the given ID.\n\nSee http://friendfeed.com/api/documentation#write_entry.\nAuthentication is required for this method.\n\"\"\"\n", "func_signal": "def edit_entry(self, id, body=None, link=None, **args):\n", "code": "args.update(id=id)\nif body: args.update(body=body)\nif link: args.update(link=link)\nreturn self.fetch(\"/entry\", post_args=args)", "path": "friendfeed.py", "repo_name": "kevinmarks/Friendlier-Feed", "stars": 1, "license": "None", "language": "python", "size": 84}
{"docstring": "\"\"\"Unsubscribes the authenticated user from the given feed.\n\nSee http://friendfeed.com/api/documentation#write_unsubscribe.\nAuthentication is required for this method.\n\"\"\"\n", "func_signal": "def unsubscribe(self, feed, **args):\n", "code": "args.update(feed=feed)\nreturn self.fetch(\"/unsubscribe\", post_args=args)", "path": "friendfeed.py", "repo_name": "kevinmarks/Friendlier-Feed", "stars": 1, "license": "None", "language": "python", "size": 84}
{"docstring": "\"\"\"Fetches the given relative API path, e.g., \"/bret/friends\"\n\nIf the request is a POST, post_args should be provided. Query\nstring arguments should be given as keyword arguments.\n\"\"\"\n", "func_signal": "def fetch(self, path, post_args=None, **args):\n", "code": "url = _FRIENDFEED_API_BASE + path\n\n# Add the OAuth resource request signature if we have credentials\nif self.consumer_token and self.access_token:\n    all_args = {}\n    all_args.update(args)\n    all_args.update(post_args or {})\n    oauth = get_oauth_resource_request_parameters(\n        url, self.consumer_token, self.access_token, all_args,\n        method=\"POST\" if post_args is not None else \"GET\")\n    args.update(oauth)\n\nif args: url += \"?\" + urllib.urlencode(args)\nif post_args is not None:\n    request = urllib2.Request(url, urllib.urlencode(post_args))\nelse:\n    request = urllib2.Request(url)\nstream = urllib2.urlopen(request)\ndata = stream.read()\nstream.close()\nreturn self._parse_dates(_parse_json(data))", "path": "friendfeed.py", "repo_name": "kevinmarks/Friendlier-Feed", "stars": 1, "license": "None", "language": "python", "size": 84}
{"docstring": "\"\"\"Posts the given message to FriendFeed (link and to optional).\n\nSee http://friendfeed.com/api/documentation#write_entry.\nAuthentication is required for this method.\n\"\"\"\n", "func_signal": "def post_entry(self, body, link=None, to=None, **args):\n", "code": "args.update(body=body)\nif link: args.update(link=link)\nif to: args.update(to=to)\nreturn self.fetch(\"/entry\", post_args=args)", "path": "friendfeed.py", "repo_name": "kevinmarks/Friendlier-Feed", "stars": 1, "license": "None", "language": "python", "size": 84}
{"docstring": "\"\"\"Deletes the given entry from FriendFeed.\n\nSee http://friendfeed.com/api/documentation#write_entry.\nAuthentication is required for this method.\n\"\"\"\n", "func_signal": "def delete_entry(self, id, **args):\n", "code": "args.update(id=id)\nreturn self.fetch(\"/entry/delete\", post_args=args)", "path": "friendfeed.py", "repo_name": "kevinmarks/Friendlier-Feed", "stars": 1, "license": "None", "language": "python", "size": 84}
{"docstring": "\"\"\"Sets the OAuth parameters for this session.\"\"\"\n", "func_signal": "def set_oauth(self, consumer_token, access_token):\n", "code": "self.consumer_token = consumer_token\nself.access_token = access_token", "path": "friendfeed.py", "repo_name": "kevinmarks/Friendlier-Feed", "stars": 1, "license": "None", "language": "python", "size": 84}
{"docstring": "# Task has actuals > 0, is completed, and has remaining > 0. \n# All task in us completed. (There are two) \n# should change US to complete and change task to complete, remaining 0\n\n", "func_signal": "def test_editing_task_with_actuals_and_in_complete_state_sets_remaining_to_zero(self):\n", "code": "b = self.browser\nb.click(\"link=Iteration\")\nb.wait()\n\nb.click(\"link=TestUS254 B\")\nb.wait()\n\nb.click(\"xpath=id('task_edit_%d')/img\" % self.task_d.id)\nb.wait()\n\nb.click(\"xpath=id('content')/form/input\")\nb.wait()\n\n# Reload obj task, self.task_c is on a stale state:\nobj = Task.objects.get(id = self.task_d.id)\n# Check that nothing else changed except that which should\nself.assertEqual(obj.name, \"Task D\")\nself.assertEqual(obj.description, '')\nself.assertEqual(obj.estimate, 2)\nself.assertEqual(obj.remaining, 0)\nself.assertEqual(obj.owner, self.user)\nself.assertEqual(obj.user_story, self.story_3)\nself.assertEqual(obj.state, self.task_states['Complete'])\nself.assertEqual(obj.user_story.state, self.user_story_states['Completed'])", "path": "src\\agilito\\tests\\sel\\us254.py", "repo_name": "mahiti/agilito", "stars": 1, "license": "None", "language": "python", "size": 1412}
{"docstring": "\"\"\"\nJust checks if I really if I can change the user password\n\"\"\"\n", "func_signal": "def test_simple_password_change(self):\n", "code": "b = self.browser\nb.click(\"link=Change password\")\nb.wait()\nb.type(\"id_old_password\", \"hi\")\nb.type(\"id_new_password1\", \"bye\")\nb.type(\"id_new_password2\", \"bye\")\nb.click(\"xpath=id('content')/form/p[4]/input\")\nb.wait()\nfor i in xrange(20):\n    b.wait()\n\nself.assertEqual(BASE_URL + 'accounts/changepassword/done/', b.get_location())\nself.assert_(User.objects.get(username = 'User A').check_password('bye'))", "path": "src\\agilito\\tests\\sel\\us320.py", "repo_name": "mahiti/agilito", "stars": 1, "license": "None", "language": "python", "size": 1412}
{"docstring": "\"\"\"\nSimple test to check if the query function works with tasks\n\"\"\"\n", "func_signal": "def test_query_task_name(self):\n", "code": "query = \"name:coding\"\nself.assert_(len(Task.query(query)) > 0)", "path": "src\\agilito\\tests\\unit.py", "repo_name": "mahiti/agilito", "stars": 1, "license": "None", "language": "python", "size": 1412}
{"docstring": "# 1. Align stream on 0x1000 boundary (and therefore on sector boundary)\n", "func_signal": "def pyexcelerator_compounddoc_xlsdoc_save(self, filename, stream):\n", "code": "padding = '\\x00' * (0x1000 - (len(stream) % 0x1000))\nself.book_stream_len = len(stream) + len(padding)\n\nself._XlsDoc__build_directory()\nself._XlsDoc__build_sat()\nself._XlsDoc__build_header()\n\nif isinstance(filename, types.StringTypes):\n    f = file(filename, 'wb')\nelse:\n    f = filename\nf.write(self.header)\nf.write(self.packed_MSAT_1st)\nf.write(stream)\nf.write(padding)\nf.write(self.packed_MSAT_2nd)\nf.write(self.packed_SAT)\nf.write(self.dir_stream)\nf.close()", "path": "src\\agilito\\__init__.py", "repo_name": "mahiti/agilito", "stars": 1, "license": "None", "language": "python", "size": 1412}
{"docstring": "\"\"\"\nCheck if you can search on tasks using a project id\n\"\"\"\n", "func_signal": "def test_query_empty_with_project(self):\n", "code": "result = self._class.query('', project_id = 1)\nself.assertEqual(len(result), 121) # 47", "path": "src\\agilito\\tests\\ttw.py", "repo_name": "mahiti/agilito", "stars": 1, "license": "None", "language": "python", "size": 1412}
{"docstring": "\"\"\"\nValidates that the input is in self.choices.\n\"\"\"\n", "func_signal": "def clean(self, value):\n", "code": "if self.required and value in forms.fields.EMPTY_VALUES:\n    raise forms.ValidationError(self.error_messages['required'])\nif value in forms.fields.EMPTY_VALUES:\n    value = u''\nvalue = smart_unicode(value)\nif value == u'':\n    return value\nvalid_values = []\nchoices = list(self.choices)\nwhile choices:\n    value_or_choices, label = choices.pop(0)\n    try:\n        iter(value_or_choices)\n    except TypeError:\n        is_iter = False\n    else:\n        is_iter = True\n    if is_iter and not isinstance(value_or_choices, basestring):\n        # value_or_choices is choices\n        choices[:0] = list(value_or_choices)\n    else:\n        valid_values.append(smart_unicode(value_or_choices))\nif value not in valid_values:\n    raise forms.ValidationError(self.error_messages['invalid_choice'] % {'value': value})\nreturn value", "path": "src\\agilito\\fields.py", "repo_name": "mahiti/agilito", "stars": 1, "license": "None", "language": "python", "size": 1412}
{"docstring": "\"\"\"\nThis is a helper function used by both 'logged_in_or_basicauth' and\n'has_perm_or_basicauth' that does the nitty of determining if they\nare already logged in or if they have provided proper http-authorization\nand returning the view if all goes well, otherwise responding with a 401.\n\"\"\"\n", "func_signal": "def view_or_basicauth(view, request, test_func, realm = \"\", *args, **kwargs):\n", "code": "if test_func(request.user):\n    # Already logged in, just return the view.\n    #\n    return view(request, *args, **kwargs)\n\n# They are not logged in. See if they provided login credentials\n#\nif 'HTTP_AUTHORIZATION' in request.META:\n    auth = request.META['HTTP_AUTHORIZATION'].split()\n    if len(auth) == 2:\n        # NOTE: We are only support basic authentication for now.\n        #\n        if auth[0].lower() == \"basic\":\n            uname, passwd = base64.b64decode(auth[1]).split(':')\n            user = authenticate(username = uname, password = passwd)\n            if user is not None:\n                if user.is_active:\n                    login(request, user)\n                    request.user = user\n                    return view(request, *args, **kwargs)\n\n# Either they did not provide an authorization header or\n# something in the authorization attempt failed. Send a 401\n# back to them to ask them to authenticate.\n#\nresponse = HttpResponse()\nresponse.status_code = 401\nresponse['WWW-Authenticate'] = 'Basic realm=\"%s\"' % realm\nreturn response", "path": "src\\agilito\\feeds.py", "repo_name": "mahiti/agilito", "stars": 1, "license": "None", "language": "python", "size": 1412}
{"docstring": "\"\"\"\nThis is similar to the above decorator 'logged_in_or_basicauth'\nexcept that it requires the logged in user to have a specific\npermission.\n\nUse:\n\n@logged_in_or_basicauth('asforums.view_forumcollection')\ndef your_view:\n    ...\n\n\"\"\"\n", "func_signal": "def has_perm_or_basicauth(perm, realm = \"\"):\n", "code": "def view_decorator(func):\n    def wrapper(request, *args, **kwargs):\n        return view_or_basicauth(func, request,\n                                 lambda u: u.has_perm(perm),\n                                 realm, *args, **kwargs)\n    return wrapper\nreturn view_decorator", "path": "src\\agilito\\feeds.py", "repo_name": "mahiti/agilito", "stars": 1, "license": "None", "language": "python", "size": 1412}
{"docstring": "# self.__index = 0\n", "func_signal": "def render(self):\n", "code": "self.__rendered = u''\nself.__render(self.hierarchy.children, self.id)\nreturn mark_safe(self.__rendered)", "path": "src\\agilito\\widgets.py", "repo_name": "mahiti/agilito", "stars": 1, "license": "None", "language": "python", "size": 1412}
{"docstring": "# In case of \"/feeds/backlog/9/foo/bar/baz/\", or other such clutter,\n# check that bits has only one member.\n", "func_signal": "def get_object(self, bits):\n", "code": "if len(bits) != 1:\n    raise ObjectDoesNotExist\nbits = [int(b) for b in bits]\nreturn get_project(self.request.user, bits[0])", "path": "src\\agilito\\feeds.py", "repo_name": "mahiti/agilito", "stars": 1, "license": "None", "language": "python", "size": 1412}
{"docstring": "\"\"\"\nA simple decorator that requires a user to be logged in. If they are not\nlogged in the request is examined for a 'authorization' header.\n\nIf the header is present it is tested for basic authentication and\nthe user is logged in with the provided credentials.\n\nIf the header is not present a http 401 is sent back to the\nrequestor to provide credentials.\n\nThe purpose of this is that in several django projects I have needed\nseveral specific views that need to support basic authentication, yet the\nweb site as a whole used django's provided authentication.\n\nThe uses for this are for urls that are access programmatically such as\nby rss feed readers, yet the view requires a user to be logged in.  Many rss\nreaders support supplying the authentication credentials via http basic\nauth (and they do NOT support a redirect to a form where they post a\nusername/password.)\n\nUse is simple:\n\n@logged_in_or_basicauth()\ndef your_view:\n    ...\n\nYou can provide the name of the realm to ask for authentication within.\n\"\"\"\n", "func_signal": "def logged_in_or_basicauth(realm = \"\"):\n", "code": "def view_decorator(func):\n    def wrapper(request, *args, **kwargs):\n        return view_or_basicauth(func, request,\n                                 lambda u: u.is_authenticated(),\n                                 realm, *args, **kwargs)\n    return wrapper\nreturn view_decorator", "path": "src\\agilito\\feeds.py", "repo_name": "mahiti/agilito", "stars": 1, "license": "None", "language": "python", "size": 1412}
{"docstring": "\"\"\"\nJust checks if I really deleted the task\n\"\"\"\n\n", "func_signal": "def test_simple(self):\n", "code": "pre = Task.objects.count()\nb = self.browser\nb.click(\"link=Iteration\")\nb.wait()\nb.click(\"link=ABC\")\nb.wait()\nb.click(\"css=img[title=delete]:first-child\")\nb.wait()\nb.click(\"xpath=id('content')/form/div/input[2]\")\nfor i in xrange(20):\n    b.wait()\nself.assertEqual(Task.objects.count(), pre - 1)", "path": "src\\agilito\\tests\\sel\\us16.py", "repo_name": "mahiti/agilito", "stars": 1, "license": "None", "language": "python", "size": 1412}
{"docstring": "\"\"\"\nCheck if you can search on tasks using a project id\n\"\"\"\n", "func_signal": "def test_query_empty_with_project(self):\n", "code": "result = self._class.query('', project_id = 1)\nself.assertEqual(len(result), 80) #old value 32", "path": "src\\agilito\\tests\\ttw.py", "repo_name": "mahiti/agilito", "stars": 1, "license": "None", "language": "python", "size": 1412}
{"docstring": "# Task has actuals > 0 is on state defined should change state to 'In Progress'\n# Should change story to 'In Progress'\n\n", "func_signal": "def test_editing_task_with_actuals_sets_from_defined_to_complete(self):\n", "code": "b = self.browser\nb.click(\"link=Iteration\")\nb.wait()\n\nb.click(\"link=TestUS254 A\")\nb.wait()\n\nb.click(\"xpath=id('task_edit_%d')/img\" % self.task_c.id)\nb.wait()\n\nb.click(\"xpath=id('content')/form/input\")\nb.wait()\n\n# Reload obj task, self.task_c is on a stale state:\nobj = Task.objects.get(id = self.task_c.id)\n# Check that only that the state changed\nself.assertEqual(obj.name, \"Task C\")\nself.assertEqual(obj.description, '')\nself.assertEqual(obj.estimate, 8)\nself.assertEqual(obj.remaining, 6)\nself.assertEqual(obj.owner, self.user)\nself.assertEqual(obj.user_story, self.story_2)\nself.assertEqual(obj.state, self.task_states['In Progress'])\nself.assertEqual(obj.user_story.state, self.user_story_states['In Progress'])", "path": "src\\agilito\\tests\\sel\\us254.py", "repo_name": "mahiti/agilito", "stars": 1, "license": "None", "language": "python", "size": 1412}
{"docstring": "\"\"\"\nitem_attrs\n    Defines the attributes of each item which will be displayed\n    as a column in each table row, in the order given.\n\n    Any callable attributes specified will be called and have\n    their return value used for display.\n\n    All attribute values will be escaped.\n\"\"\"\n\n", "func_signal": "def __init__(self, item_attrs, grouper, *args, **kwargs):\n", "code": "super(TableSelectMultiple, self).__init__(*args, **kwargs)\n\nself.item_attrs = item_attrs\nif grouper is None:\n    self.grouper = None\n    self.group_label = None\nelse:\n    self.grouper, self.group_label = grouper", "path": "src\\agilito\\widgets.py", "repo_name": "mahiti/agilito", "stars": 1, "license": "None", "language": "python", "size": 1412}
{"docstring": "# Override the default renderer if we were passed one.\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "renderer = kwargs.pop('renderer', None)\nself.hierarchy = kwargs.pop('hierarchy', None)\nsuper(HierarchicRadioSelect, self).__init__(*args, **kwargs)", "path": "src\\agilito\\widgets.py", "repo_name": "mahiti/agilito", "stars": 1, "license": "None", "language": "python", "size": 1412}
{"docstring": "# Task has actuals > 0, is in progress, and has remaining = 0. \n# Not all task in us completed.\n# Should change US to In Progress and change task to complete, remaining 0\n\n", "func_signal": "def test_editing_task_with_actuals_remaining_on_zero_in_in_progress_state_sets_state_to_complete(self):\n", "code": "b = self.browser\nb.click(\"link=Iteration\")\nb.wait()\n\nb.click(\"link=TestUS254 A\")\nb.wait()\n\nb.click(\"xpath=id('task_edit_%d')/img\" % self.task_g.id)\nb.wait()\n\nb.click(\"xpath=id('content')/form/input\")\nb.wait()\n\n# Reload obj task, self.task_c is on a stale state:\nobj = Task.objects.get(id = self.task_g.id)\n# Check that nothing else changed except that which should\nself.assertEqual(obj.name, \"Task G\")\nself.assertEqual(obj.description, '')\nself.assertEqual(obj.estimate, 2)\nself.assertEqual(obj.remaining, 0)\nself.assertEqual(obj.owner, self.user)\nself.assertEqual(obj.user_story, self.story_2)\nself.assertEqual(obj.state, self.task_states['Complete'])\nself.assertEqual(obj.user_story.state, self.user_story_states['In Progress'])", "path": "src\\agilito\\tests\\sel\\us254.py", "repo_name": "mahiti/agilito", "stars": 1, "license": "None", "language": "python", "size": 1412}
{"docstring": "\"\"\"\nCheck if the US30 exists\n\"\"\"\n", "func_signal": "def test_query_user_story_id(self):\n", "code": "query = \"id:30\"\nresult = UserStory.query(query)\nself.assertEqual(len(result), 1)\nself.assertEqual(result[0].id, 30)", "path": "src\\agilito\\tests\\unit.py", "repo_name": "mahiti/agilito", "stars": 1, "license": "None", "language": "python", "size": 1412}
{"docstring": "\"\"\" \nSimple tests just check if there exists a UserStory with a name\ncontaining the word 'owner'\n\"\"\"\n", "func_signal": "def test_query_user_story_name(self):\n", "code": "query = \"name:owner\"\nself.assert_(len(UserStory.query(query)) > 0)", "path": "src\\agilito\\tests\\unit.py", "repo_name": "mahiti/agilito", "stars": 1, "license": "None", "language": "python", "size": 1412}
{"docstring": "\"\"\"\nCheck if you can search on tasks using a project id\n\"\"\"\n", "func_signal": "def test_query_empty_with_project(self):\n", "code": "result = self._class.query('', project_id = 1)\nself.assertEqual(len(result), 87) #old value 53", "path": "src\\agilito\\tests\\ttw.py", "repo_name": "mahiti/agilito", "stars": 1, "license": "None", "language": "python", "size": 1412}
